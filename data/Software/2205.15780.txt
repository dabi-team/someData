A Replication Study on Predicting Metamorphic
Relations at Unit Testing Level

Alejandra Duque-Torres†, Dietmar Pfahl†, Rudolf Ramler‡, and Claus Klammer‡
†Institute of Computer Science , University of Tartu, Tartu, Estonia
E-mail: {duquet, dietmar.pfahl}@ut.ee
‡Software Competence Center Hagenberg (SCCH) GmbH, Hagenberg, Austria
E-mail: {rudolf.ramler, claus.klammer}@scch.at

2
2
0
2

y
a
M
1
3

]
E
S
.
s
c
[

1
v
0
8
7
5
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—Metamorphic Testing (MT) addresses the test oracle
problem by examining the relations between inputs and outputs
of test executions. Such relations are known as Metamorphic
Relations (MRs). In current practice, identifying and selecting
suitable MRs is usually a challenging manual task, requiring a
thorough grasp of the SUT and its application domain. Thus,
Kanewala et al. proposed the Predicting Metamorphic Relations
(PMR) approach to automatically suggest MRs from a list of
six pre-deﬁned MRs for testing newly developed methods. PMR
is based on a classiﬁcation model trained on features extracted
from the control-ﬂow graph (CFG) of 100 Java methods. In our
replication study, we explore the generalizability of PMR. First,
since not all details necessary for a replication are provided, we
rebuild the entire preprocessing and training pipeline and repeat
the original study in a close replication to verify the reported
results and establish the basis for further experiments. Second, we
perform a conceptual replication to explore the reusability of the
PMR model trained on CFGs from Java methods in the ﬁrst step
for functionally identical methods implemented in Python and
C++. Finally, we retrain the model on the CFGs from the Python
and C++ methods to investigate the dependence on programming
language and implementation details. We were able to success-
fully replicate the original study achieving comparable results
for the Java methods set. However, the prediction performance
of the Java-based classiﬁers signiﬁcantly decreases when applied
to functionally equivalent Python and C++ methods despite using
only CFG features to abstract from language details. Since the
performance improved again when the classiﬁers were retrained
on the CFGs of the methods written in Python and C++, we
conclude that the PMR approach can be generalized, but only
when classiﬁers are developed starting from code artefacts in the
used programming language.

Index Terms—Software testing, metamorphic testing, meta-

morphic relations, prediction modelling, replication study

I. INTRODUCTION

Metamorphic Testing (MT) is a software testing approach
proposed by Chen et al. [1] to alleviate the test oracle problem.
A test oracle is a mechanism for detecting whether or not the
outputs of a program are correct [2], [3]. The oracle problem
arises when the SUT lacks an oracle or when developing one
to verify computed outputs is practically impossible [3]. MT
differs from traditional testing approaches in that it examines
the relations between input-output pairs of consecutive SUT
executions rather than the outputs of individual SUT execu-
tions [1]. The relations between SUT inputs and outputs are
known as Metamorphic Relations (MRs). MRs deﬁne how the
outputs should vary in response to a certain change in the

input [4], [5]. In this way, testers may test the SUT indirectly
by looking at whether the inputs and outputs satisfy the MRs.
If an MR is violated for certain test cases, then there is a
high probability that there is a fault in the SUT [4]. The most
challenging task facing MT is determining suitable MRs for a
particular SUT. In current practice, MRs are detected manually
and require an in-depth understanding of the SUT and problem
domain. As a result, the identiﬁcation and selection of high-
quality MRs are recognised as a big challenge.

Recently, an approach supporting unit testing at the method
level was proposed by Kanewala et al., published in STVR [6]
to “predict whether a certain method
and at ISSRE [7],
exhibits a particular MR or not”. The idea behind their
Predicting Metamorphic Relations (PMR) approach is to build
a model that predicts whether a method in a newly developed
SUT can be tested using a speciﬁc MR. The PMR approach
is based on a pre-deﬁned set of six MRs and a classiﬁer
trained on the control-ﬂow graph (CFG) extracted from a pool
of sample methods. While the original study by Kanewala
et al. showed encouraging results, it was performed on a
dataset covering methods implemented in one programming
language (Java) only. In order to see whether and how the
PMR approach could be transferred to other programming
languages, given that using CFGs supports abstracting from
language and implementation details, we decided to perform
further research on this.

Replication studies are required to validate experimental
results achieved in prior research. They are an essential
part of empirical software engineering as they prove that
the observations obtained can hold (or not) under different
situations. There are several forms of replication [8], [9]. An
exact replication aims to replicate the experiments as closely
as possible to the initial procedures. This demonstrates that
uncontrolled random variables did not drive the initial results.
In a conceptual replication, one or more dimensions can be
modiﬁed to see how well the results hold up. It is important to
note that exact and conceptual replications go by the names of
repetition and reproduction, respectively, in the literature [9].
In this paper, we present a conceptual replication of the
study of Kanewala et al. [6]. We follow the guidelines sug-
gested by Carver [10] and the ACM guidelines on repro-
ducibility (different team, different experimental setup) [11]:
“The measurement can be obtained with stated precision by

 
 
 
 
 
 
a different team, a different measuring system, in a different
location on multiple trials. For computational experiments,
this means that an independent group can obtain the same or
similar result using artefacts, which they develop completely
independently”. We decided to replicate the original study in
three steps. In each step, we use the same set of pre-deﬁned
MRs used in the original study.

In the ﬁrst step, we rebuilt the entire pipeline for extracting
CFG information from source code to training and testing
classiﬁcation models in order to repeat the process described
in the original study by Kanewala et al. The authors of the
original study provided the extracted CFG information for
replicating their work but not the Java source code of the
analyzed methods, which we retrieved from the corresponding
project repositories on GitHub. We also had to develop the
classiﬁcation models that we then used for predicting MRs,
because also they were not shared by the authors of the original
paper. We conducted the ﬁrst step to check whether we can
re-create the classiﬁers with as good quality as in the original
study and to prepare for the next steps by building our own
classiﬁers based on features extracted from the CFGs derived
from the source code of methods.

The next

two steps of our study aim at exploring the
transferability and generalizability of the PMR method. In the
second step, we checked whether classiﬁers generated from
Java code perform equally well when applied to Python and
C++ code. Therefore, we created two datasets, one comprising
100 methods in Python and other one comprising 100 methods
in C++. Both sets of methods implemented the exact same
functionality as the 100 Java methods used in the ﬁrst step.
We did this to guarantee that the MRs taken from the original
study would apply in the same way to the Python and C++
methods as they did in the original study using Java. To check
the generalizability of the PMR method to other programming
languages we then also developed individual classiﬁers for
each programming language (Python, C++) starting out from
code in the same programming language to which the classiﬁer
will be applied during evaluation.

The results of our study indicate that PMR classiﬁers
generated based on artefacts implemented in one programming
language do not perform well when applied to artefacts im-
plemented in a different programming language, even though
the classiﬁers are based on CFGs to abstract from program-
ming language and implementation details, the implemented
functionality is exactly identical, and the set of MRs remains
unchanged. On the other hand, it seems to be possible to
generalize the PMR method in the sense that
it can be
applied with good performance on code implemented in a
different programming language as long as the PMR classiﬁers
are redeveloped based on code implemented in the same
programming language.

II. RELATED WORK

MT has been demonstrated to be an effective technique for
testing in a variety of application domains, e.g., autonomous
driving [12], [13], cloud and networking systems [14], [15],

bioinformatic software [16], [17], scientiﬁc software [18],
[19]. However,
the efﬁcacy of MT heavily relies on the
speciﬁc MRs employed. Some research has been done on how
to choose “good” MRs. Chen et al. [20] examined several
MRs discovered for shortest path and critical path programs,
attempting to determine MRs that are useful. Liu et al. [21]
introduced the Composition of MRs (CMRs) technique for
constructing new MRs by mixing multiple existing ones.
Zhang et al. [22] proposed a method in which an algorithm
searches for MRs expressed as linear or quadratic equations.
Chen et al. [23] developed METRIC, a speciﬁcation-based
technique and related tool for identifying MRs based on the
category-choice framework. They also expanded METRIC into
METRIC+ by integrating the information acquired from the
output domain [24]. To our knowledge, Kanewala et al.[6], [7],
were the ﬁrst to show that, in previously unseen methods, MRs
can be predicted using ML techniques. They used features
obtained from CFGs and a set of predeﬁned MR to train
prediction models.

III. PREDICTING METAMORPHIC RELATIONS

This study is a replication of the approach proposed by
Kanewala et al. [6] for predicting suitable MRs for the purpose
the PMR
testing. In this section, we ﬁrst present
of unit
procedure proposed by Kanewala et al. (Section III-A). Then,
we present a detailed description of the set of pre-deﬁned MRs
used in the original and our study (Section III-B) as well as
the labelled dataset used in the original study (Section III-C).
Finally, we summarise the evaluation results reported in the
original study (Section III-D).

A. PMR Procedure

The goal of the PMR approach is to build a model that
predicts whether a method in a newly developed SUT can be
automatically tested by exploiting one or more MRs contained
in a pre-deﬁned set of MRs. Figure 1 shows the PMR
procedure. The PMR procedure consists of three phases. Phase
I is responsible for creating a graph description representation
derived from a method’s control-ﬂow graph (CFG). The output
of this phase is a DOT ﬁle. Phase II is in charge of feature
extraction from the method’s DOT ﬁle. Also, each method
is labelled with elements from the set of pre-deﬁned MRs.
Thus, the output of this phase is a labelled dataset. Phase III
is in charge of training and evaluating the binary classiﬁcation
models that predict whether a speciﬁc MR is applicable to the
unit testing of a speciﬁc method. Below we describe each
phase in detail.

1) Phase I – Graph description representation: This Phase
starts with the creation of the graph representation from the
method’s source code. Then a labelled CFG is created by
annotating each node in the CFG. The process can be split
into the following two steps.

Step 1.1 – CFG generation: This step is responsible for
creating the CFG from the method’s source code. To get the
CFG, Kanewala et al. use Soot[25]. Soot generates CFG rep-
resentations in Jimple format, a typed 3-address intermediate

Fig. 1: PMR procedure

representation, where each CFG node represents an atomic
operation [25]. The left-hand side of Figure 2 shows the CFG
representation of the Algorithm 1 using the soot framework.
The numbering of the nodes has been done manually, i.e., not
with the framework, and serves here only to facilitate a better
understanding of the next phase and its steps.

Algorithm 1 Average of an integer array

1: function AVG(int input[ ])
double sum = 0;
2:
double average = 0;
3:
for (int i = 0; input.length; i++) do
4:
5:

sum + = input[i];

6:
7:

average = sum/input.length;
return average

Fig. 2: On the left, CFG representation of Algorithm 1 using
the soot framework; on the right, its CFG with annotations

Step 1.2 – CFG labelling: In this step a simpliﬁed version
of the CFG is created by replacing the speciﬁc, code-related
information of each node in the CFG by a more general
annotation describing the speciﬁc operations and conditional
jumps in the original code. The right-hand side of Figure 2
shows the CFG representation with the node annotations of
the Algorithm 1. Table I shows examples of annotations that

are assigned depending on the node operation. The annotations
follow the graph description language, i.e., the DOT format.

2) Phase II – Data preparation: This phase is in charge
of extracting a set of features from the annotated CFGs, i.e,
from the Phase I output. Also, to each method’s annotated
CFG zero to six pre-deﬁned MRs are assigned, depending on
their suitability. Like Phase I, Phase II consists of two steps.
Step 2.1 – Feature extraction: Kanewala et al. propose two
approaches for extracting features from CFG representations,
features based on nodes and paths, and features based on
graph similarity measures. In the former, the node features
(hereafter simply NF) follows the form N On − din − dout,
where NOn stands for Node Operation of node n, and din
and dout stand for in-degree and out-degree, respectively. The
number of a speciﬁc NF type, i.e., N On − din − dout, is
tallied and used as the NF value. As an example of NF, let us
consider the annotated CFG of Algorithm 1. As the right-hand
side of Figure 2 shows, the annotated CFG of Algorithm 1
has fourteen nodes. Among these fourteen nodes, there are
seven with the type annotation assi, two with type annotation
add, and ﬁve with unique type annotations, i.e, start, goto, if,
div and exit. For each node, the din and dout are calculated,
too, to derive the complete NF. For instance, the node start
(node 1) will be represented by the NF start-0-1, where start
is N O1, 0 is din and 1 is dout. Each unique NF is tallied to
get the corresponding NF value. For start-0-1, the NF value
is 1. Table II shows the set of NFs and their values extracted
from Algorithm 1 using its CFG representation.

The feature based on path information (hereafter just PF)
refers to the shortest routes from the start node to each node
in the graph, as well as the shortest path from each node in the
graph to the end node. This feature follows the form N O1 −
N O2 − N O... − N On, where N On, as in the NF, denotes a
speciﬁc operation statement in node n. The value of each PF
is the number of occurrences of each path in the CFG. For
instance, let us consider the labelled CFG of Algorithm 1. As
Figure 2 right side shows, the path composed by the nodes the
PF 1-2-3-4-5-6-7 and the nodes 1-2-3-4-5-6-11 can be denoted
as start-assi-assi-goto-assi-if-assi. Therefore, its feature value
is 2 since there are two paths represented by one type of PF.
Table III shows the set of PFs and their associated PF values

Method_nVn,cVn,c...Vn,c1...11Ft2Ft1...FtcMR1...MR2MRcMethod_1V12V11...V1c0...11Method_2V22V21...V2c1...10...........................Labelled	datasetMethod_1CFG GenerationStep	1.1CFG LabellingStep	1.2Method_1.dotPhase	I	-	Graph	description	representationMethod_1.dotStep	2.1MRs LabellingStep	2.2Phase	II	-	Data	preparationFeatureextractorMethod_1.dotTesting dataStep	3.1Training dataData split Step	3.2ModelCreationStep	3.3PerformanceevaluationPhase	III	-	Training	and	testingML model setr0 := @parameter0d0 = 0.0i0 = 0goto label1label1 :$i2 = lenghtof r0if i0<$i2 goto $i1 = r0[i0]$i1=r0[i0]     $d1 = (double)$i1d0 = d0 + $d1i0 = i0 + 1    $i3 = lengthof r0      $d2 = (double)$i3   $d3 = d0/$d2  Return $d31234567891011121314Startassiassigotoassiifassiassiaddaddassiassidivexit1234567891011121314extracted from Algorithm 1 using its CFG representation.

The second approach to extract features from CFG is by
using graph similarity measures. Graph similarity refers to the
process of determining the degree of similarity between two or
more graphs. In particular, Kanewala et al. use Random Walk
Kernel (RWK) and Graphlet Kernel (GK). RWK is the most-
studied family of graph kernels [26]. It provides measure the
similarity between two or more graphs based on the number
of common walks in the graphs. The concept behind GK is to
randomly sample tiny (connected) sub-graphs of size k, and
using them to compare frequency distributions or to construct
graph invariants.

TABLE I: Node operations (NO) in the control ﬂow graph and
corresponding labels (CL) for the annotation

NO

+
||, or
==
<
invoke

CL

add
or
eql
lt
fcall

NO

−
&, and
>=
! =
return

CL

sub
and
geql
neql
return

NO CL

NO

CL

∗
if
>
:=
exit

mul
if
gt
start %
exit

/
div
=
assi
<= leql
rem
goto

goto

Step 2.2 – MR labelling: The key idea of PMR is predicting
whether a given method is suited for a particular MR by using
binary classiﬁers. PMR uses supervising learning classiﬁcation
algorithms, i.e., a labelled dataset is needed to provide exam-
ples for learning. Thus, after Step 2.1 – Feature extraction, the
training dataset is created by manually labelling each method
with applicable MRs. Depending on whether a speciﬁc MR
does or does not satisfy the method, the method is labelled
with 1 or 0 for this MR, respectively.

3) Phase III – Training and testing: This phase involves
the use of one or more supervised machine learning (ML)
algorithms, or a combination of them, to derive knowledge
from the data. Three steps needs to be conducted.

Step 3.1 – Data split: This step is responsible for splitting
the dataset into two subsets: a training set and a test set. The
training set is used to create the prediction model, while the
test set is used to evaluate the performance of the created
prediction model.

Step 3.2 – Model creation refers to the process of building
prediction models. Choosing a good modelling technique is
vital for the training and prediction stage in any ML applica-
tion, including the PMR approach. Kanewala et al. get the best
results using the Support Vector Machine (SVM) technique.
Step 3.3 – Performance evaluation: This step measures
the performance of the created prediction models. Performance

TABLE II: Node Features (NF) extracted from Algorithm 1
related to the nodes of its CFG representation

NF

NF value

NF

NF value

start-0-1
assi-1-1
goto-1-1

1
7
1

if-2-2
add-1-1
div-1-1

1
2
1

TABLE III: Path Features (PF) extracted from Algorithm 1
related to the paths of its CFG representation

PF

PF value

Shortest path from the start node to each node

start
start-assi
start-assi-assi
start-assi-assi-goto
start-assi-assi-goto-assi
start-assi-assi-goto-assi-if
start-assi-assi-goto-assi-if-assi
start-assi-assi-goto-assi-if-assi-assi
start-assi-assi-goto-assi-if-assi-assi-add
start-assi-assi-goto-assi-if-assi-assi-div
start-assi-assi-goto-assi-if-assi-assi-add-add
start-assi-assi-goto-assi-if-assi-assi-div-exit

1
1
1
1
1
1
2
2
1
1
1
1

Shortest path from each node to the end node

assi-assi-goto-assi-if-assi-assi-div-exit
assi-goto-assi-if-assi-assi-div-exit
goto-assi-if-assi-assi-div-exit
assi-if-assi-assi-div-exit
if-assi-assi-div-exit
assi-assi-div-exit
assi-div-exit
div-exit
exit
assi-assi-add-add-if-assi-assi-div-exit
assi-add-add-if-assi-assi-div-exit
add-add-if-assi-assi-div-exit
add-if-assi-assi-div-exit

1
1
1
1
1
1
1
1
1
1
1
1
1

measures are derived from the Confusion Matrix. Let A denote
a classiﬁcation output in which a speciﬁc M Rn satisﬁes the
method m, and let A(cid:48) denote a classiﬁcation output in which
a speciﬁc M Rn does not satisfy the method m, then A can be
seen as the positive class and A(cid:48) as the negative class. Using
this notation, each standard performance measure is expressed
as a function of the counts of elements in the Confusion Matrix
deﬁned as follows:
True Positive (TP): The actual MR of a method was A and the
predicted was A. This represents a successful prediction.
True Negative (TN): The actual MR of a method was A(cid:48), the
predicted was A(cid:48). This represents a successful prediction.
False Positive (FP): The actual MR was A(cid:48) and the predicted
was A. This represents an unsuccessful prediction.
False Negative (FN): The actual MR was A and the predicted
was A(cid:48). This represents an unsuccessful prediction.
Accuracy is the ratio of successful predictions made to both
classes and expressed as:

Accuracy =

T P + T N
T P + T N + F P + F N

(1)

Precision (or positive predictive value) is the ratio of correct
predictions made for class A and is shown in Equation (2):

P recision =

T P
T P + F P

(2)

Recall (or true positive rate, or sensitivity) is the ratio of

successful predictions made to cases of class A

Recall =

T P
T P + F N

The f-measure statistic (or F1 score) is the harmonic mean of
precision and recall:

f-measure = 2 ×

Recall × P recision
Recall + P recision

(4)

C. Dataset

Inputf tc = X1, X2, X3, ..., Xn−1
Then the following output-relation must hold:

Outputf tc(Y ) ≤ Outputstc(X)

MR6: “Invertive” (INV). To get the ftc input, take the inverse

(3)

of each stc input element Xi > 0, i.e.,

Inputf tc = 1/X1, 1/X2, 1/X3, ..., 1/Xn

Then the following output-relation must hold:

Outputf tc(Y ) ≤ Outputstc(X)

In addition to the aforementioned performance measures,
the Area Under Curve (AUC) and the Balanced Success Rate
(BSR) measures are also widely used. The AUC is the area
under the curve that plots the False Positive Rate (FPR) against
the True Positive Rate (TPR) at different points in [0, 1]. In
binary classiﬁcation problems, the BSR measure is calculated
as the average of recall obtained on each class [27].

B. Metamorphic Relations

In the original study, Kanewala et al. use six MRs that
had been suggested previously in other studies [7], [28]–[31].
Below we describe each MR in detail. The acronyms stc
and ftc stand for source test case and follow-up test case,
respectively. The input of the stc is an ordered set of non-
negative numbers:

Inputstc = Xi, ..., Xn where Xi ≥ 0, 0 ≤ i ≤ n

The outputs of the source and follow-up test cases are

written as Outputstc(X) and Outputf tc(Y ), respectively.

The MRs based on these inputs and outputs are:

In their original study, Kanewala et al. relied on a code
corpus containing 100 Java methods that take numerical inputs
and produce numerical outputs. The methods are from the
open-source libraries Colt Project [32], which is an open-
source library written for high-performance scientiﬁc and
technical computing, Apache Mahou [33], which is a machine
learning library, Apache Commons Mathematics [34], which
is a Library of mathematics and statistics components, and
Java Collections [35], which is a framework that provides an
architecture to store and manipulate the group of objects. All
of these libraries are written in Java.

To create a training dataset, Kanewala et al. manually
labelled each method with the set of pre-deﬁned MRs in a
binary manner, i.e., if M Rn matches a method m, then this
method is assigned the label 1 for M Rn, otherwise it is 0.

Table IV reports the total number of methods that do and
do not match a speciﬁc MR. One sees that more than half
of the methods match with MRs denoted as ADD, MUL and
INV, while approximately one third of the methods match with
MRs denoted as PER, INC, and EXC.

MR1: “Addition” (ADD). To get the ftc input, add a positive
constant “C” to each element of the stc input, i.e.,

TABLE IV: Total number of methods that match ((cid:88)) and do
not match ((cid:55)) a speciﬁc MR

Inputf tc = X1 + C, X2 + C, X3 + C..., Xn + C,

Then the following output-relation must hold:

Outputf tc(Y ) ≥ Outputstc(X)

MR2: “Multiplication” (MUL). To get the ftc input, multiply
each stc input element with a positive constant “C”, i.e.,
Inputf tc = X1 ∗ C, X2 ∗ C, ..., Xn ∗ C

Then the following output-relation must hold:

Outputf tc(Y ) ≥ Outputstc(X)

MR3:“Permutation” (PER). To get the ftc input, randomly

permute the stc input elements, e.g., like

Inputf tc = X3, X1, Xn, ..., X2
Then the following output-relation must hold:

Outputf tc(Y ) = Outputstc(X)

MR4: “Inclusive” (INC). To get the ftc input, include a new

element “Xn+1 ≥ 0” to the stc input, e.g., like

Inputf tc = X1, X2, X3, ..., Xn, Xn+1

Then the following output-relation must hold:

Outputf tc(Y ) ≥ Outputstc(X)

MR5: “Exclusive” (EXC). To get the ftc input, remove an

element “Xn−1 ≥ 0” from the stc input, e.g., like

MR

Change in the input

Output expected

ADD
Add a positive constant
MUL Multiply by a positive constant
PER
INC
EXC
INV

Permute the components
Add a new element
Remove an element
Take the inverse of each element

Increase or remain constant
Increase or remain constant
Remain constant
Increase or remain constant
Decrease or remain constant
Decrease or remain constant

(cid:88)

56
66
33
34
32
63

(cid:55)

44
34
67
66
68
37

Table V reports how many methods have 0, 1, 2,

... 6
matching MRs and how those MRs are distributed in each
case. 20 out of 100 methods have no matching MR, and only
9 out of 100 methods match with all six MRs simultaneously.

TABLE V: Total number of methods that have 0, 1, 2, ..., 6
matching MRs and their distributions

No. Met⊥

ADD MUL

PER

INC

EXC

INV

No. MR(cid:45)
0
1
2
3
4
5
6

0
1
3
18
26
6
9
(cid:45)Number of MRs that may apply to certain method, ⊥Number of methods

0
2
1
5
10
7
9

0
3
4
17
26
7
9

0
0
2
5
16
1
9

0
0
1
5
10
7
9

0
2
3
19
16
7
9

20
8
7
23
26
7
9

TABLE VI: Labelled dataset [6]: symbol (cid:88)denotes an MR-method match; symbol (cid:55) denotes that there is no match

ID Method Name

Library

Metamorphic Relation

ID Method Name

Library

Metamorphic Relation

ADD MUL

PER

INC

EXC

INV

ADD MUL

PER

INC

EXC

INV

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50

add values
array calc
array copy
autoCorrelation
average
bi SearchFromTo
bubble
cal AbsoluteDiff
cal Diff
chebyshevDist
checkNonNegative
checkPositive
check equal
check eq tolerance
chiSquare
clip
cnt zeroes
canberraDist
cal DividedDiff
cosineDist
count k
count non zeroes
covariance
dec
dec array
Dist
DistInf
dot product
durbinWatson
ebeAdd
ebeDivide
ebeMultiply
ebeSubtract
elemtWise equal
elemtWise max
elemtWise min
elemtWise not eq
entropy
equals
errorRate
euc Dist
evaluateHoners
eval Internal
evalNewton
evalWeightedProd
ﬁnd diff
ﬁnd euc Dist
ﬁnd magnitude
ﬁnd max
ﬁnd max2

Colle
Colle
Colle
Colt
Colle
Colt
Math
Math
Math
Maho
Math
Math
Colle
Colle
Math
Colle
Colle
Math
Math
Maho
Colle
Colle
Colt
Maho
Colle
Math
Math
Colle
Colt
Math
Math
Math
Math
Colle
Colle
Colle
Colle
Math
Math
Maho
Math
Math
Math
Math
Math
Colle
Colle
Colle
Colle
Colle

(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)

(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100

ﬁnd median
ﬁnd min
g Test
geometric mean
get array value
hamming dist
harmonicMean
insertion sort
kurtosis
lag
manhattanDist
manhattanDist2
max
meanDeviation
mean Diff
mean abs error
min
partition
polevl
pooledMean
pooledVariance
power
product
quantile
reverse
safeNorm
sampleKurtosis
sampleSkew
sampleVariance
sampleWeightedVar
scale
s add
selection sort
sequential search
set min val
shell sort
skew
square
standardize
sum
sumOfLogarithms
sum Power Deviat
sum labeled
tanimotoDist
variance
var Difference
weightedMean
weightedRMS
weighted average
winsorizedMean

Colle
Colle
Math
Colle
Colle
Colle
Colt
Colle
Colt
Colt
Maho
Colle
Colt
Colt
Math
Colle
Colt
Math
Colt
Colt
Colt
Colt
Colt
Colt
Colle
Colle
Math
Colt
Colt
Colt
Colt
Maho
Colle
Colle
Colle
Colle
Colle
Colle
Colle
Maho
Colle
Colt
Colt
Maho
Colle
Colt
Colt
Colt
Colle
Colt

Colle: Java Collection, Maho: Apache Mahout, Math: Apache Commons Mathematics

(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:88)

Table VI shows the names of all methods used in the study
and the library to which they belong. It also shows whether or
not a speciﬁc MR matches the method. For better readability,
we use the symbol (cid:88) to denote that a speciﬁc MR matches,
otherwise, we use the symbol (cid:55). In total, 26 methods stem
from the Colt project, 8 are from Apache Mahout, 25 are from
Apache Commons Mathematics, and 41 methods are from Java
Collections. These methods are provided by Kanewala et al.1
in the form of CFG representation, using the graph description
language format DOT, instead of source code.

D. Achieved Performance in Original Study

Kanewala et al. use features based on nodes and paths, as
well as features based on graph similarity (RWK and GK), to

1http://www.cs.colostate.edu/saxs/MRpred/functions.tar.gz

build 18 binary SVM models, i.e., three models (each using
different feature sets) per any of the six speciﬁc MRs. They use
AUC and BSR to evaluate model performance and consider
AUC > 0.80 to be a good classiﬁcation performance. Among
the eighteen trained SVM models, the most promising one
was when RWK was used. The average performance for the
six models using RWK was 0.87 in terms of AUC.

IV. REPLICATION METHODOLOGY

Our goal is to investigate whether the PMR approach of
Kanewala et al. (i) can be replicated when using our own im-
plementation of the pipeline for developing classiﬁers starting
out from Java source code instead of CFG representations,
(ii) classiﬁers trained on Java source code can be transferred
to Python and C++ methods that have identical functionality,

and (iii) the PMR approach can be applied to Python and
C++ code when classiﬁers are developed from scratch in the
target programming languages. Each of these scenarios gives
rise to a research question that we answer in our study. In all
scenarios, we follow the PMR procedure, Figure 1, and we use
the same set of six pre-deﬁned MRs used in the original study,
i.e., Section III-B. However, we develop our own pipeline and
create new datasets. For performance evaluation, we employ
10-fold stratiﬁed cross-validation., i.e., the dataset is randomly
partitioned into ten subgroups. The classiﬁer is then built using
nine subsets, with the 10th subset being used to evaluate
the predictive model’s performance. This procedure is done
ten times, with each of the ten subgroups being evaluated
separately. The ten folds in stratiﬁed 10-fold cross-validation
are partitioned in such a way that they include about the same
proportion of classes as the original data set. The resulting
overall performance is measured as the average of the ten
cross-validation tests.

Our replication package containing results, scripts, models

and datasets is available online2.

A. Research Questions

We aim at answering the following research questions
• RQ1: [Replicability] How well do classiﬁers predict
matching MRs for Java methods when using our pro-
cessing and training pipeline starting from source code?
• RQ2: [Transferability] How well do classiﬁers developed
on Java code predict matching MRs for functionally
equivalent methods implemented in Python and C++?
• RQ3: [Generalisability] How well do classiﬁers predict
matching methods for Python and C++ methods when
developed from source code in the respective target
languages?

B. RQ1: How well do classiﬁers predict matching MRs for
Java methods when using our pipeline implementation?

In RQ1, we investigate the replicability of the PMR ap-
proach when starting out directly from Java source code (in-
stead of CFG representations), performing all steps of feature
extraction and re-generating the classiﬁers with a different ML
package. In particular, we are interested in checking whether
the classiﬁers developed by us achieve the same performance
as published in [6]. Satisfactory results for RQ1 are the pre-
requisite for tackling RQ2 and RQ3.

To compare with the work of Kanewala et al., we develop
our own artefacts for Phase II - Data preparation, Step 2.1 –
Feature extraction, and all the artefacts needed by Phase III
Training and testing. Then we compare the results of SVM
obtained by Kanewala et al. [6] who used PyML Toolkit [36]
with our results achieved using Python scikit-learn library [37]
with default parameter settings. For the comparison we use
two datasets. The ﬁrst dataset is the one used in the original
study by Kanewala et al., i.e., the methods from Table VI.
This dataset contains the CFG representations of the 100 Java

2https://github.com/aduquet/RENE-PredictingMetamorphicRelations

methods in DOT format. In the following, we call this dataset
DSJK. The second dataset contains the Java source code of
the 100 methods from Table VI as contained in the open-
source libraries. We call this dataset DSJV .

When using DSJK we apply the PMR approach from Phase
I - Step 1.2 through Phase III - Step 3.3, since this dataset
already contains the CFG representation of each method in
DOT format. When using DSJV , we apply our entire pipeline
implementing the PMR approach, i.e., from Phase I - Step
1.1 to Phase III - Step 3.3. To get the CFG representation in
DOT format for the dataset DSJV , we use the soot[25] Java
framework, conﬁgured so that the output matches the CFGs
of the original dataset. Then, we compare the performance of
our classiﬁers against the results published by Kanewala et al.
In particular, we compare BSR and AUC since these are the
measures provided in the original study [6]. In addition, we
provide the performance measures detailed in Section III-A3
- Step 3.3 – Performance evaluation.

C. RQ2: How well do classiﬁers developed on Java code
predict matching MRs for functionally equivalent methods
implemented in Python and C++?

In RQ2, we check whether classiﬁers developed with the
PMR approach from Java methods achieve the same perfor-
mance when applied to methods with identical functionality
but implemented in Python or C++. We chose Python and
C++ because both are popular and widely used programming
languages supporting a broad range of applications [38].

For this experiment, we created two new datasets containing
source code of methods written in Python and in C++. The
methods in each dataset are functionally identical to that of
the 100 Java methods described in Table VI. The correspond-
ing method implementations were either retrieved from the
NumPy package for scientiﬁc computing in case of Python,
the Blinz++ high-performance library for scientiﬁc computing
in case of C++, or they were implemented in Python/C++
by the authors if not present in these libraries. Because the
functionality of the Python and C++ methods is equivalent
to the functionality of the Java methods, we can assume that
exactly the same MRs that match the Java methods match
the corresponding Python and C++ methods. The dataset
named DSP Y contains the Python methods, and the dataset
named DSC++ contains the C++ methods. To get the graph
representation in DOT format, Section III-A1: Phase I - Step
CFG generation, for the methods written in Python, we use
the Python package pycfg [39], and for the methods written
in C++, we use Goblint[40], [41].

D. RQ3: How well do classiﬁers predict matching methods for
Python and C++ methods when developed from source code
in the respective target languages?

Finally, in RQ3, we check whether the PMR approach works
for Python and C++ code similarly well as it does for Java
code, if we develop the classiﬁers for each target language
from scratch. Thus, we train SVM models using each dataset,

i.e., DSP Y and DSC++. We compare the performance of the
new classiﬁers against the results obtained in RQ1 and RQ2.

V. RESULTS AND DISCUSSION
A. RQ1 How well do classiﬁers predict matching MRs for Java
methods when developed from source code using our pipeline?

Table VII shows the performance of our PMR implemen-
tation for both Java datasets, DSJK and DSJV . Overall,
regardless of the feature extraction technique used, the results
are fairly close. This can be seen in the Error column, which
displays the difference in performance between DSJK and
DSJV for each MR. The most negative value is −0.104
(Accuracy of ADD) while the farthest positive value is 0.148
(BSR of INV). This indicates that the classiﬁers developed by
us are consistent for Java code independent from the starting
point of the model development (CFG vs. source code).

Table VIII shows how the performance of our PMR imple-
mentation compares to the performance obtained by Kanewala
et al. in terms of AUC and BSR. As can be seen from the
Error column, our results are close to those obtained in the
original study. The Error range is [-0.093, 0.061] for AUC and
[-0.118, 0.117] for BSR. From combining the results shown
in Table VIII with those shown in Table VII we conclude that
our implementation of PMR achieves similar performance as
reported in [6] even when starting out from source code.

With regards to replicability (RQ1), our results indicate
that we can achieve similar results as Kanewala et al.
when re-implementing the PMR approach no matter
whether we start the modelling process from source
code or from CFG representations.

With regards to transferability (RQ2), our results sug-
gest that classiﬁers trained on a dataset containing
methods in one programming language (Java) have
reduced performance when applied to datasets with
functionally equivalent methods implemented in a dif-
ferent programming language (Python, C++). Hence,
classiﬁcation models built according to the proposed
PMR approach may not be be transferable across
languages.

C. RQ3 How well do classiﬁers predict matching methods for
Python and C++ methods when developed from source code
in the respective target languages?

Table X reports the results of using the PMR approach to
develop classiﬁers separately for each programming language
(Python and C++). Comparing Table IX and Table X indicates
that the performance improves remarkably when using models
that are trained speciﬁcally to also consider the implementa-
tion characteristics stemming from the different programming
languages. Even though the performance has improved by
developing language speciﬁc classiﬁers, the results for Python
and C++ are generally below the results achieved for Java,
with the results for C++ being consistently the worst.

With regards to generalizability (RQ3), our results
suggest the PMR approach can be applied for different
programming languages when the classiﬁers are re-
trained on the speciﬁc target language. The slightly
lower performance, esp. for C++, needs further explo-
ration of the data and the choice of model parameter
settings (tuning).

B. RQ2 How well do classiﬁers developed on Java code
predict matching MRs for functionally equivalent methods
implemented in Python and C++?

Table IX reports on the performance when using classiﬁers,
developed starting out from the DSJV dataset, to predict
matching MRs for methods contained in the DSP Y and
DSC++ datasets. The assumption behind applying a classiﬁer
built on Java code to methods that are functionally equivalent
but implemented in a different programming language is that
the CFG representations from which the features in the SVM
models are taken would be similar enough to achieve similar
classiﬁcation performance as when applied to Java methods.
However, as shown in Table IX, the performance is low for
all performance measures and for both Python and C++. No
measure is greater than 0.689. This result suggests that the
representation of the CFGs of the Python and C++ methods
to which the feature extraction algorithm is applied are more
different from the CFGs of the Java methods than expected.
This can be explained due to the language-speciﬁc CFG
generators that we used as well as differences in the way how
the methods (with identical functionality) are implemented in
different programming languages.

D. Threats to Validity

In the context of our study, two types of threats to validity

are most relevant: threats to internal and external validity.

To achieve internal validity, we used the same set of
methods and of MRs as in Kanewala et al. [6]. For the
Python and C++ datasets, we carefully checked functional
equivalence of the methods with those in the original Java
dataset. Given functional equivalence of the methods, we
assume that the matching MRs are identical for each of the
three chosen programming languages. However, this has not
been veriﬁed. It is unlikely but possible that some methods
have slight differences in the set of matching MRs due to the
programming language. Another potential validity threat in our
study is that we recreated all steps of the PMR approach using
different machine learning libraries with potentially different
parameter settings. However, the performance measures in
RQ1 (Table VII) align well with the results reported in the
original study. This suggests that we have understood how to
correctly build the classiﬁers in our replication.

Regarding external validity, our study uses the same meth-
implemented in different

ods as in the original study but

TABLE VII: PMR performance achieved by our classiﬁers when starting from DSJV and DSJK

Performance measurements

MR

Feat⊥

Accuracy

Precision

DSJK DSJV

Error± DSJK DSJV

Recall
Error± DSJK DSJV

Error± DSJK DSJV

f-measure

AUC
Error± DSJK DSJV

BSR
Error± DSJK DSJV

Error±

ADD

MUL

PER

INC

EXC

INV

NF-NP
GK
RWK

NF-NP
GK
RWK

NF-NP
GK
RWK

PF-NP
GK
RWK

NF-NP
GK
RWK

NF-NP
GK
RWK

0.802
0.712
0.851

0.712
0.663
0.789

0.838
0.834
0.916

0.792
0.752
0.799

0.763
0.816
0.774

0.714
0.778
0.651

0.787
0.816
0.86

0.688
0.641
0.695

0.840
0.826
0.918

0.807
0.788
0.839

0.753
0.787
0.725

0.705
0.759
0.585

0.015
-0.104
-0.009

0.024
0.022
0.094

-0.002
0.008
-0.002

-0.015
-0.036
-0.040

0.010
0.029
0.049

0.009
0.019
0.066

0.786
0.702
0.836

0.672
0.714
0.666

0.860
0.888
0.917

0.847
0.721
0.832

0.772
0.816
0.757

0.674
0.765
0.610

0.751
0.732
0.712

0.689
0.732
0.706

0.883
0.819
0.827

0.822
0.781
0.792

0.783
0.861
0.743

0.659
0.769
0.659

0.035
-0.030
0.124

-0.017
-0.018
-0.040

-0.023
0.069
0.090

0.025
-0.060
0.040

-0.011
-0.045
0.014

0.015
-0.004
-0.049

0.812
0.717
0.771

0.685
0.697
0.693

0.835
0.823
0.878

0.837
0.790
0.800

0.778
0.849
0.757

0.702
0.738
0.643

0.704
0.758
0.791

0.661
0.758
0.797

0.846
0.845
0.835

0.837
0.776
0.773

0.759
0.890
0.741

0.671
0.737
0.639

0.108
-0.041
-0.020

0.024
-0.061
-0.104

-0.011
-0.022
0.043

0.000
0.014
0.027

0.019
-0.041
0.016

0.031
0.001
0.004

0.773
0.744
0.786

0.657
0.676
0.660

0.855
0.864
0.877

0.776
0.783
0.854

0.762
0.871
0.769

0.675
0.760
0.675

0.775
0.712
0.785

0.705
0.733
0.676

0.790
0.79
0.893

0.759
0.718
0.764

0.789
0.790
0.744

0.694
0.721
0.653

-0.002
0.032
0.001

-0.048
-0.057
-0.016

0.065
0.074
-0.016

0.017
0.065
0.090

-0.027
0.081
0.025

-0.019
0.039
0.022

0.837
0.769
0.905

0.742
0.775
0.846

0.945
0.872
0.963

0.845
0.850
0.862

0.768
0.873
0.731

0.905
0.671
0.760

0.827
0.707
0.877

0.734
0.730
0.820

0.925
0.811
0.944

0.852
0.882
0.821

0.755
0.870
0.727

0.917
0.670
0.766

0.010
0.062
0.028

0.008
0.045
0.026

0.020
0.061
0.019

-0.007
-0.032
0.041

0.013
0.003
0.004

-0.012
0.001
-0.006

0.768
0.737
0.843

0.631
0.689
0.774

0.847
0.853
0.757

0.793
0.762
0.673

0.868
0.758
0.79

0.656
0.679
0.787

0.785
0.729
0.829

0.654
0.657
0.739

0.813
0.839
0.793

0.786
0.744
0.654

0.839
0.755
0.757

0.661
0.655
0.639

-0.017
0.008
0.014

-0.023
0.032
0.035

0.034
0.014
-0.036

0.007
0.018
0.019

0.029
0.003
0.033

-0.005
0.024
0.148

⊥Feature extraction approach, ±Error (DSJK − DSJV ), NF-PF: Node Feature - Path Feature, GK: Graphnet Kernel, RWK: Random Walk Kernel

TABLE VIII: Comparison of PMR performance (AUC and
BSR) achieved by Kanewala et al. and when using classiﬁers
developed by us starting from DSJK

MR

Feat⊥

ADD

MUL

PER

INC

EXC

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

[6]

0.81
0.83
0.92

0.73
0.78
0.83

0.93
0.91
0.95

0.84
0.88
0.89

0.78
0.78
0.90

Performance measurements

AUC
DSJK

Error±

[6]

BSR
DSJK

Error±

0.837
0.769
0.905

0.742
0.775
0.846

0.945
0.872
0.963

0.845
0.850
0.862

0.768
0.873
0.731

-0.027
0.061
0.015

-0.012
0.005
-0.016

-0.015
0.038
-0.013

-0.005
0.030
0.028

0.012
-0.093
0.169

0.77
0.79
0.85

0.65
0.69
0.74

0.83
0.83
0.87

0.80
0.75
0.79

0.75
0.74
0.79

0.768
0.737
0.843

0.631
0.689
0.774

0.847
0.853
0.757

0.793
0.762
0.673

0.868
0.758
0.790

0.002
0.053
0.007

0.019
0.001
-0.034

-0.017
-0.023
0.113

0.007
-0.012
0.117

-0.118
-0.018
0.000

INV

0.84
0.68
0.76

0.905
0.671
0.769

NF-PF
GK
RWK

-0.016
-0.019
-0.047
⊥Feature extraction approach, ±Error ( [6]−DSJK ), [6]: Kanewala et al.
NF-PF: Node and Path Feature, GK: Graphnet Kernel
RWK: Random Walk Kernel

-0.065
0.009
-0.009

0.656
0.679
0.787

0.64
0.66
0.74

programming languages. For the sake of generalisability, it
would have been preferable to include additional methods to
overcome any potential bias introduced by the selection of
methods in the original study. As a consequence, our replica-
tion cannot determine the actual scope of the effectiveness of
the PMR approach.

E. Remarks on General Relevance

When assembling the Python and C++ datasets contain-
ing functionally equivalent methods for our replication, we

identiﬁed the issue that such methods tend to be rare and
are usually only found in speciﬁc domains such as libraries
for mathematical computations. In the original study, a fully
labelled dataset was used containing a high number of methods
(80%) with matching MRs. Only 20% of the methods are
not related to any of the supported MRs. How realistic is
this distribution? Since the pre-deﬁned set of MRs is rather
small and only applies to methods with a very speciﬁc
signature (mainly functions that take numerical inputs and
produce numerical outputs), it is unlikely that one will ﬁnd
an equal share of such methods in real-world applications.
In particular since such methods are often already provided as
part of existing, dedicated libraries (e.g., Apache Commons or
NumPy). If, as we assume, the share of matching methods in
newly developed real-world application is very small and given
that the effort for developing language-speciﬁc classiﬁers is
comparably high,
the practical relevance of the proposed
approach seems to be limited.

Furthermore,

the proposed PMR approach uses features
extracted from individual methods and it is therefore tied to the
level of unit testing. A generalisation of the approach beyond
unit testing, e.g., by transferring it to system level testing does
not seem possible.

VI. CONCLUSION

We closely as well as conceptually replicated the study of
Kanewala et al. [6]. First, we reproduced the PMR approach
using our own implementation of the pipeline for feature
extraction and training classiﬁers by starting out from Java
source code and creating corresponding CFGs. We showed
that our classiﬁers perform equally well as in the original
study indicating a successful replication as basis for further
experiments. Second, we checked transferability of classiﬁers
trained on methods implemented in Java to other programming
languages (Python and C++). We found that the performance
decreases too much to consider this approach feasible. This
is caused by programming language-speciﬁc implementation

TABLE IX: Performance of SVM models when trained with DSJV and tested with DSP Y and DSC++

MR

Feat⊥

Accuracy

Precision

Performance measurements
Recall

f-measure

AUC

BSR

DSP Y

DSC++ DSP Y

DSC++ DSP Y

DSC++ DSP Y

DSC++ DSP Y

DSC++ DSP Y

DSC++

ADD

MUL

PER

INC

EXC

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

0.575
0.564
0.544

0.494
0.460
0.499

0.521
0.520
0.515

0.579
0.578
0.536

0.637
0.597
0.568

0.459
0.447
0.468

0.588
0.472
0.388

0.445
0.458
0.424

0.578
0.518
0.521

0.440
0.561
0.548

0.572
0.532
0.526

0.627
0.463
0.499

0.503
0.525
0.515

0.576
0.588
0.525

0.639
0.648
0.579

0.522
0.426
0.474

0.596
0.436
0.388

0.403
0.398
0.540

0.527
0.501
0.444

0.497
0.506
0.564

0.555
0.543
0.593

0.495
0.477
0.492

0.517
0.563
0.503

0.580
0.597
0.508

0.535
0.592
0.613

0.551
0.470
0.403

0.639
0.392
0.387

0.411
0.431
0.471

0.496
0.576
0.528

0.502
0.552
0.610

0.554
0.531
0.500

0.522
0.479
0.488

0.507
0.546
0.517

0.580
0.579
0.536

0.591
0.649
0.579

0.473
0.473
0.483

0.658
0.400
0.393

0.570
0.392
0.496

0.590
0.476
0.526

0.507
0.494
0.627

0.551
0.547
0.550

0.623
0.480
0.490

0.535
0.499
0.516

0.597
0.582
0.512

0.600
0.610
0.570

INV

NF-PF
GK
RWK

0.514
0.468
0.435
⊥Feature extraction approach, NF-PF: Node Feature - Path Feature, GK: Graphnet Kernel, RWK: Random Walk Kernel

0.525
0.473
0.529

0.502
0.477
0.465

0.531
0.472
0.470

0.422
0.395
0.378

0.493
0.411
0.417

0.534
0.478
0.507

0.471
0.421
0.402

0.433
0.401
0.400

0.529
0.452
0.466

0.574
0.431
0.393

0.519
0.399
0.413

0.603
0.558
0.478

0.525
0.467
0.590

0.411
0.403
0.333

0.563
0.561
0.503

0.652
0.475
0.499

0.542
0.535
0.532

0.577
0.594
0.500

0.639
0.658
0.637

0.512
0.469
0.461

0.466
0.427
0.414

0.648
0.478
0.384

0.545
0.454
0.514

0.477
0.587
0.486

0.478
0.492
0.562

0.491
0.459
0.352

TABLE X: Performance of SVM models for DSP Y and DSC++ datasets

MR

Feat⊥

Accuracy

Precision

Performance measurements
Recall

f-measure

AUC

BSR

DSP Y

DSC++ DSP Y

DSC++ DSP Y

DSC++ DSP Y

DSC++ DSP Y

DSC++ DSP Y

DSC++

ADD

MUL

PER

INC

EXC

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

NF-PF
GK
RWK

0.706
0.724
0.737

0.670
0.613
0.727

0.818
0.835
0.869

0.746
0.671
0.785

0.734
0.752
0.791

0.577
0.599
0.738

0.611
0.658
0.795

0.732
0.777
0.824

0.677
0.754
0.760

0.635
0.698
0.805

0.742
0.671
0.653

0.652
0.627
0.732

0.822
0.785
0.856

0.734
0.659
0.793

0.694
0.735
0.805

0.660
0.708
0.720

0.623
0.657
0.742

0.702
0.725
0.853

0.684
0.713
0.721

0.649
0.681
0.834

0.748
0.713
0.699

0.701
0.659
0.640

0.820
0.820
0.811

0.789
0.685
0.808

0.713
0.703
0.782

0.590
0.655
0.797

0.584
0.648
0.685

0.778
0.830
0.877

0.661
0.756
0.784

0.652
0.745
0.794

0.683
0.757
0.668

0.787
0.643
0.726

0.755
0.802
0.862

0.796
0.685
0.746

0.725
0.725
0.788

0.645
0.606
0.730

0.688
0.561
0.715

0.763
0.752
0.755

0.705
0.781
0.682

0.690
0.742
0.786

0.723
0.730
0.693

0.746
0.663
0.686

0.769
0.797
0.796

0.789
0.682
0.804

0.715
0.764
0.808

INV

NF-PF
GK
RWK

0.673
0.604
0.675
⊥Feature extraction approach, NF-PF: Node Feature - Path Feature, GK: Graphnet Kernel, RWK: Random Walk Kernel

0.668
0.589
0.717

0.661
0.558
0.648

0.543
0.620
0.629

0.606
0.582
0.683

0.671
0.598
0.673

0.571
0.595
0.611

0.594
0.579
0.649

0.539
0.586
0.614

0.691
0.652
0.750

0.594
0.607
0.735

0.754
0.717
0.735

0.647
0.772
0.753

0.732
0.760
0.811

0.559
0.633
0.711

0.760
0.798
0.726

0.656
0.663
0.721

0.865
0.829
0.798

0.792
0.681
0.793

0.737
0.734
0.780

0.672
0.599
0.712

0.653
0.667
0.725

0.580
0.625
0.677

0.754
0.694
0.840

0.619
0.672
0.694

0.732
0.673
0.753

0.538
0.624
0.708

details, despite relying only on features extracted from the
abstract CFG representation of the methods. Third, we demon-
strated that the PMR approach can be generalized. When re-
training the classiﬁers from scratch on Python and C++ source
code, the performance we achieved was almost comparable to
those from classiﬁers trained on Java code.

All artefacts created by us as well as all results are available

in a replication package.

ACKNOWLEDGEMENT

This research was partly funded by the Estonian Center
of Excellence in ICT research (EXCITE), the European Re-
gional Development Fund, the IT Academy Programme for
ICT Research Development, the Austrian ministries BMVIT
and BMDW, the State of Upper Austria under the COMET
(Competence Centers for Excellent Technologies) program
managed by FFG, and grant PRG1226 of the Estonian Re-
search Council.

REFERENCES

[1]

T. Y. Chen, S. C. Cheung, and S. M. Yiu, “Metamorphic testing: A
new approach for generating next test cases,” Department of Computer
Science, Hong Kong University of Science and Technology, Hong
Kong, Tech. Rep. HKUST-CS98-01, 1998.

[2] A. Duque-Torres, A. Shalygina, D. Pfahl, and R. Ramler, “Using
rule mining for automatic test oracle generation,” in 8th International
Workshop on Quantitative Approaches to Software Quality (QuASoQ),
ser. QuASoQ’20, 2020.
E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo, “The
oracle problem in software testing: A survey,” IEEE Transactions on
Software Engineering, vol. 41, no. 5, pp. 507–525, 2015.

[3]

[4] H. Liu, F.-C. Kuo, D. Towey, and T. Y. Chen, “How effectively does
metamorphic testing alleviate the oracle problem?” IEEE Transactions
on Software Engineering, vol. 40, no. 1, pp. 4–22, 2014.
Z. Q. Zhou, L. Sun, T. Y. Chen, and D. Towey, “Metamorphic
relations for enhancing system understanding and use,” IEEE Trans-
actions on Software Engineering, vol. 46, no. 10, pp. 1120–1154,
2020.

[5]

[6] U. Kanewala, J. M. Bieman, and A. Ben-Hur, “Predicting meta-
morphic relations for testing scientiﬁc software: A machine learning
approach using graph kernels,” Software testing, veriﬁcation and
reliability, vol. 26, no. 3, pp. 245–269, 2016.

[7] U. Kanewala and J. M. Bieman, “Using machine learning techniques
to detect metamorphic relations for programs without test oracles,”
in IEEE 24th International Symposium on Software Reliability Engi-
neering (ISSRE), 2013, pp. 1–10.

[8] M. Shepperd, N. Ajienka, and S. Counsell, “The role and value of
replication in empirical software engineering results,” Information and
Software Technology, vol. 99, pp. 120–132, 2018.

[10]

[9] O. S. G´omez, N. Juristo, and S. Vegas, “Understanding replication of
experiments in software engineering: A classiﬁcation,” Information
and Software Technology, vol. 56, no. 8, pp. 1033–1048, 2014.
J. C. Carver, “Towards reporting guidelines for experimental repli-
cations: A proposal,” in 1st international workshop on replication in
empirical software engineering, Citeseer, vol. 1, 2010, pp. 1–4.
[11] M. LLC. (2020). “Artifact review and badging - current,” [Online].
Available: https://www.acm.org/publications/policies/artifact-review-
and-badging-current (visited on 09/21/2021).

[13]

[12] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, “Deep-
road: Gan-based metamorphic testing and input validation framework
for autonomous driving systems,” in 33rd IEEE/ACM International
Conference on Automated Software Engineering (ASE), IEEE, 2018,
pp. 132–142.
Z. Q. Zhou and L. Sun, “Metamorphic testing of driverless cars,”
Communications of the ACM, vol. 62, no. 3, pp. 61–67, Feb. 2019.
P. C. Canizares, A. N´unez, J. de Lara, and L. Llana, “MT-EA4Cloud:
A methodology for testing and optimising energy-aware cloud sys-
tems,” Journal of Systems and Software, vol. 163, p. 110 522, 2020.
Z. Zhang, D. Towey, Z. Ying, Y. Zhang, and Z. Q. Zhou, “MT4NS:
Metamorphic testing for network scanning,” in 6th IEEE/ACM In-
ternational Workshop on Metamorphic Testing (MET), ser. MET’21,
2021, pp. 17–23.

[14]

[15]

[16] M. Srinivasan, M. P. Shahri, I. Kahanda, and U. Kanewala, “Qual-
ity assurance of bioinformatics software: A case study of testing
a biomedical
text processing tool using metamorphic testing,” in
IEEE/ACM 3rd International Workshop on Metamorphic Testing
(MET), ser. MET’18, Gothenburg, Sweden: Association for Comput-
ing Machinery, 2018, pp. 26–33.

[17] M. P. Shahri, M. Srinivasan, G. Reynolds, D. Bimczok, I. Kahanda,
and U. Kanewala, “Metamorphic testing for quality assurance of
protein function prediction tools,” in IEEE International Conference
On Artiﬁcial Intelligence Testing (AITest), IEEE, 2019, pp. 140–148.
Z. Peng, U. Kanewala, and N. Niu, “Contextual understanding and
improvement of metamorphic testing in scientiﬁc software develop-
ment,” in 15th ACM/IEEE International Symposium on Empirical
Software Engineering and Measurement (ESEM), 2021, pp. 1–6.

[18]

[19] X. Lin, M. Simon, and N. Niu, “Exploratory metamorphic testing
for scientiﬁc software,” Computing in Science Engineering, vol. 22,
no. 2, pp. 78–87, 2020.
T. Y. Chen, D. Huang, T. Tse, and Z. Q. Zhou, “Case studies
on the selection of useful relations in metamorphic testing,” in 4th
Ibero-American Symposium on Software Engineering and Knowledge
Engineering (JIISIC 2004), Citeseer, 2004, pp. 569–583.

[20]

[22]

[21] H. Liu, X. Liu, and T. Y. Chen, “A new method for constructing
metamorphic relations,” in 12th International Conference on Quality
Software, 2012, pp. 59–68.
J. Zhang, J. Chen, D. Hao, Y. Xiong, B. Xie, L. Zhang, and H. Mei,
“Search-based inference of polynomial metamorphic relations,” in
29th ACM/IEEE International Conference on Automated Software
Engineering, ser. ASE’14, Vasteras, Sweden, 2014, pp. 701–712.
T. Y. Chen, P.-L. Poon, and X. Xie, “METRIC: METamorphic
Relation Identiﬁcation based on the Category-choice framework,”
Journal of Systems and Software, vol. 116, pp. 177–190, 2016.
[24] C.-A. Sun, A. Fu, P.-L. Poon, X. Xie, H. Liu, and T. Y. Chen,
“METRIC+: A metamorphic relation identiﬁcation technique based
on input plus output domains,” IEEE Transactions on Software
Engineering, vol. 47, no. 9, pp. 1764–1785, 2021.

[23]

[25] R. Vall´ee-Rai, P. Co, E. Gagnon, L. Hendren, P. Lam, and V.
Sundaresan, “Soot: A java bytecode optimization framework,” in
CASCON First Decade High Impact Papers, ser. CASCON ’10,
Toronto, Ontario, Canada: IBM Corp., 2010, pp. 214–224.
T. G¨artner, P. Flach, and S. Wrobel, “On graph kernels: Hardness
results and efﬁcient alternatives,” in Learning Theory and Kernel
Machines, Springer Berlin Heidelberg, 2003, pp. 129–143.

[26]

[27] A. Ben-Hur and J. Weston, “A user’s guide to support vector
machines,” in Data Mining Techniques for the Life Sciences. Humana
Press, 2010, pp. 223–239.

[28] U. Kanewala, “Techniques for automatic detection of metamorphic
relations,” in IEEE 7th International Conference on Software Testing,
Veriﬁcation and Validation Workshops (ICSTW), 2014, pp. 237–238.
[29] B. Hardin and U. Kanewala, “Using semi-supervised learning for
predicting metamorphic relations,” in 3rd IEEE/ACM International
Workshop on Metamorphic Testing (MET), ser. MET’18, 2018,
pp. 14–17.

[30] K. Rahman and U. Kanewala, “Predicting metamorphic relations for
matrix calculation programs,” in 3rd IEEE/ACM International Work-
shop on Metamorphic Testing (MET), ser. MET’18, 2018, pp. 10–13.
[31] K. Rahman, I. Kahanda, and U. Kanewala, “MRpredT: Using text
mining for metamorphic relation prediction,” in 42nd IEEE/ACM In-
ternational Conference on Software Engineering Workshops (ICSEW,
2020, pp. 420–424.

[35]

[36]
[37]

[32] Colt project, http://acs.lbl.gov/software/colt/, Accessed: 2021-09-21.
Apache mahout, https://mahout.apache.org/, Accessed: 2021-09-21.
[33]
Apache commons mathematic, http : / / commons . apache . org / proper /
[34]
commons-math/, Accessed: 2021-09-21.
Java collections, https://docs.oracle.com/javase/8/docs/technotes/
guides/collections/overview.html, Accessed: 2021-09-21.
Pyml toolkit, http://pyml.sourceforge.net/, Accessed: 2021-09-21.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J.
Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay, “Scikit-learn: Machine Learning in Python,” in Journal
of Machine Learning Research, vol. 12, 2011, pp. 2825–2830.
S. Cass, “The top programming languages: Our latest rankings put
python on top-again-[careers],” IEEE Spectrum, vol. 57, no. 8, pp. 22–
22, 2020.
Pycfg, https://pypi.org/project/pycfg/, Accessed: 2021-09-21.
[39]
[40] V. Vojdani, K. Apinis, V. R˜otov, H. Seidl, V. Vene, and R. Vogler,
“Static race detection for device drivers: The goblint approach,” in
31st IEEE/ACM International Conference on Automated Software
Engineering (ASE), ser. ASE’16, ACM, 2016, pp. 391–402.
[41] Goblint github, https://github.com/goblint/analyzer, Accessed: 2021-

[38]

09-21.

