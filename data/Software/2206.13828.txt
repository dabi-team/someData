2
2
0
2

n
u
J

8
2

]
E
S
.
s
c
[

1
v
8
2
8
3
1
.
6
0
2
2
:
v
i
X
r
a

Improving Tese Case Generation for Python Native
Libraries Through Constraints on Input Data
Structures

Xin Zhang1,3, Xutong Ma1,3, Jiwei Yan2,3, Baoquan Cui1,3, Jun Yan1,2,3, Jian Zhang1,3,†
1 State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences
2 Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences
3 University of Chinese Academy of Sciences
Email: {zhangxin19, maxt, yanjw, cuibq, yanjun, zj}@ios.ac.cn

Abstract—Modern Python projects execute computational
functions using native libraries and give Python interfaces to
boost execution speed; hence, testing these libraries becomes
critical to the project’s robustness. One challenge is that existing
approaches use coverage to guide generation, but native libraries
run as black boxes to Python code with no execution information.
Another is that dynamic binary instrumentation reduces testing
performance as it needs to monitor both native libraries and the
Python virtual machine.

To address these challenges,

in this paper, we propose an
automated test case generation approach that works at the
Python code layer. Our insight is that many path conditions in
native libraries are for processing input data structures through
interacting with the VM. In our approach, we instrument the
Python Interpreter to monitor the interactions between native
libraries and VM, derive constraints on the structures, and then
use the constraints to guide test case generation. We implement
our approach in a tool named PyCing and apply it to six
widely-used Python projects. The experimental results reveal
that with the structure constraint guidance, PyCing can cover
more execution paths than existing test cases and state-of-the-art
tools. Also, with the checkers in the testing framework Pytest,
PyCing can identify segmentation faults in 10 Python interfaces
and memory leaks in 9. Our instrumentation strategy also has
an acceptable inﬂuence on testing efﬁciency.

Index Terms—Python native library testing, test case genera-

tion, structure constraints

I. INTRODUCTION

Beneﬁtting from its ﬂexible syntax and vast

third-party
libraries, Python has become the dominating programming
language in many popular ﬁelds, including machine learning,
scientiﬁc computation, and image processing [1]. Projects
develop calculating functions utilizing native libraries and
give Python code with interfaces to fulﬁll the performance
expectations in these ﬁelds. These native libraries are mainly
written with statically typed languages such as C, C++,
FORTRAN, and Cython [2] and made available to Python
code through dynamic link libraries. Many high-performance
computing projects have developed their own native libraries,
like NumPy [3] and SciPy [4] for scientiﬁc computing, and
Sklearn [5] for machine learning. In this paper, we refer to
a Python interface provided by native libraries as a native

method, and refer to the type and attributes of one Python
object as the structures of the Python object.

Because data structures change across programming lan-
guages, defect detection mechanisms in Python, such as au-
tomatic memory management, cannot cover these libraries.
While utilizing native libraries has several advantages, such as
better execution performance and interface with the operating
system, they may also result in fatal errors, such as type
conﬂict [6]–[8], performance loss [9], [10], and dependency
issues [11], [12]. Besides, native libraries run on the operating
system, not the Python virtual machine. Some defects in native
libraries may cause VM crashes, like segmentation faults. The
rising popularity of native libraries emphasizes the critical
nature of their testing.

Fuzzing methods are effective in detecting bugs under the
guidance of code coverage [13]–[15]. They get code coverage
by instrumenting the source code, which is not difﬁcult in
cases when just focusing on one programming language.
However, they fail in instrumenting the native libraries in real-
world Python projects since they cannot parse dynamic link
libraries and are incompatible with the complicated compila-
tion process. One option is dynamic binary instrumentation.
Because the Python virtual machine is essential for running
native libraries,
this option needs to instrument both the
VM and native libraries, which can signiﬁcantly lower test
efﬁciency. Moreover, invoking native libraries without a VM
necessitates rewriting all VM-related functions, which is time-
consuming and labor-intensive [16].

Some approaches for other languages, such as Java, test
native libraries through summaries [17]–[20]. They employ
statically analyzing C code to capture constraints in sum-
maries, which they subsequently use to guide test case gen-
eration. Because Java and native libraries are both statically
typed languages,
the input data structures are known, so
these approaches only need to focus on the input value.
However, building summaries for Python native libraries can
be challenging. On the one hand, native libraries in Python
have multiple languages, including C, C++, FORTRAN, and
Cython. Combining summaries for different languages is com-
plex. On the other hand, Python does not limit or validate input

1

 
 
 
 
 
 
types to native methods due to the dynamically typed language
feature. To provide the same functionality for input data of
varying types and attributes, native libraries include lots of
path conditions related to structures [21]. Building summary
necessitates both modeling functions for processing input
data structures and dealing with the situation where native
libraries call Python code. It poses considerable difﬁculties
in producing effective summaries for testing Python native
libraries.

Compared with other test scenarios, testing Python native

libraries has two key challenges.

• Lacking information about structure constraints in
native libraries. If there is no range of structures that
native libraries can handle, it can be hard to improve
test effectiveness by randomly generating input data with
Python built-in types. Besides, going through all the com-
mon types and attributes will result in a combinatorial
explosion.

• Lacking execution information of native libraries. Exe-
cution information is critical not just for test case gen-
eration but also for evaluation. Native libraries operate
as black boxes for Python code, offering no execution
information. Moreover, dynamic binary instrumentation
will dramatically degrade test efﬁciency.

In this paper, we propose an automated test case generation
approach guided by structure constraints in Python native
libraries. Given the overhead of directly invoking native li-
braries, we choose to test native libraries from the Python
code layer. Our insight is that many path conditions in native
libraries are for input data structures, and they must process
the structures using API functions provided by the Python
Interpreter. Thus, we can track the calls to these APIs when
executing the native libraries, and build structure constraints
based on the API call logs. We chose to instrument these
API functions to address the ﬁrst challenge. In this way, we
can retrieve the arguments and return values every time the
native libraries use these functions, then utilize the outputs
to build constraints on input data structures. To address the
second challenge, as the structure constraints are part of path
conditions in native libraries, they can reﬂect the number of
paths explored by test cases. Our approach takes the source
code of a project as input. We ﬁrst retrieve native methods
from the documentation. Then, we execute these methods to
build structure constraints for existing input data. Following
that, constraints are reversed and used to generate new input
data. We can gradually explore paths in native libraries by
utilizing newly created objects as input data.

that PyCing can cover more branch conditions relating to
structures than test cases. Moreover, objects from PyCing can
identify 10 segmentation errors, 26 known memory leaks,
and 9 novel leaks using checkers from the popular testing
framework Pytest [22]. In the comparisons to two state-of-
the-art Python testing tools, the ﬁndings indicate that PyCing
can generate objects with different structures, and these objects
can be treated as seeds for other tools.

In conclusion, we make the following contributions.
• We propose an instrumentation strategy to collect con-
straints on input data structure from native libraries. Com-
pared with existing instrumentation strategies, ours can be
applied to large Python projects and has less impact on the
test efﬁciency.

• We propose an automated test case generation approach
based on structure constraints, and we implement it in a
tool named PyCing. To the best of our knowledge, it is the
ﬁrst tool for automatically testing Python native libraries.
• PyCing is applied to 6 real-world Python projects, and the
results indicate that it can explore more paths than test
cases in the projects. Input data from PyCing can detect
segmentation faults in 10 native methods and memory
leaks in 9.

II. BACKGROUND AND MOTIVATION

First, we will brieﬂy introduce the Python execution envi-
ronment and how Python code interacts with native libraries.
Following that, an example from the NumPy project will be
used to demonstrate the signiﬁcance of input data structures
in testing native libraries.

A. The Python Execution Environment

Python is an interpreted language with many interpreters
available, including CPython [23] written in C/C++, Intel-
Python [24] developed by Intel, and JPython [25] written in
Java. Because CPython is the ofﬁcial version and is commonly
used in the development, all the keywords Python Interpreter
and interpreter in this paper refer to CPython.

The interpreter executes Python code on a stack-based
virtual machine and saves the stack frame throughout execu-
tion using Python objects called PyFrame. Each PyFrame
provides access to the execution context, including the current
local and global variables, the line of executed code, and the
location of the Python ﬁle. Users can acquire PyFrames by
registering a tracer to the code. Many Python debug tools, like
pdb [26], employ this tracer mechanism.

B. Python Native Libraries

We implement our approach in a tool named PyC-
ing(Python/C Testing). To demonstrate the beneﬁts of our
approach, we apply it to native libraries in 6 popular open-
source Python projects. We ﬁrst compare PyCing with test
cases in the projects. Because existing tools cannot obtain
coverage from native libraries, we use a set of handwritten
codes as benchmarks to successfully run these tools and
compare them with PyCing. The experimental results show

Developers can compile functions developed by other pro-
gramming languages into dynamic link libraries, which can
subsequently be invoked from Python code. To establish
correlations between Python methods and native functions for
analysis, some researchers analyze the source code of native
libraries statically [18], [27]–[29]. Because developers can
specify the names of Python interfaces in many ways, these
approaches are inaccurate when applied to real-world projects,

2

making call graphs between Python code and native libraries
hard to construct.

A Python object is handled in the native libraries as a C
struct PyObject, where types and attributes of the object
correspond to the ﬁelds in the PyObject. Thus, in this paper,
we regard all the operations on a PyObject in native libraries
as operations on the attributes and types of the corresponding
Python object.

To help native libraries manage Python objects and access
the Python virtual machine, the interpreter provides a set of
C/C++ functions named Python/C API [30]. These API func-
tions enable native libraries to check types, extract attributes,
allocate and free memory in the Python heap area, and throw
exceptions to Python code.

C. A Motivating Example

The interpreter manages the lifetime of objects with a
reference counting mechanism, relieving developers of the
work of manually freeing objects and reducing the danger of
memory leaks [31]. But this mechanism demands developers
to manage memory manually while processing Python objects
in native libraries. Thus, if the reference count of a Python
object is increased in native libraries, the interpreter will not
free it even though it is no longer in use [32], [33].

Figure 1 shows one of the C functions in the implementation
of a native method numpy.power. This native method takes
two Python objects as input and calculate their powers. The
presented C function receives one PyObject and one double
value as input and returns the power value through the second
argument, where argument o2 points to the second argument
of this native method. In line 4, this function uses a Python/C
API function PyLong_Check to check if o2 is of type int.
On line 9, the API function PyFloat_Check checks for
the type float. API function PyIndex_Check at line 13
checks if o2 includes a member method __index__. Then at
line 14, this function invoke the member method __index__
through API function PyNumber_Index.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

static NPY_SCALARKIND is_scalar_with_conversion(

PyObject *o2, double* out_exponent){

...
if(PyLong_Check(o2)){

...
return NPY_NOSCALAR;

}
...
if(PyFloat_Check(o2)){

...
return NPY_FLOAT_SCALAR;

}
else if(PyIndex_Check(o2)){

PyObject* value = PyNumber_Index(o2);
...
return NPY_INTPOS_SCALAR;

}
...
}

Fig. 1. One of C functions in the implementation of the native method
numpy.power.

The API function PyNumber_Index will raise the return
value’s reference count
this value from being
to prevent
recycled throughout the caller’s procedure. A leak will occur
if the native method ﬁnishes without decreasing the reference
count. To trigger this leak, the second argument of the native
method must make the API functions on lines 4 and 9 return
False and the API function on line 13 return True. Thus,
the input data cannot inherit from the types int and float,
and must have the method __index__.

An example of input data to trigger this leak is as follows,
in which we construct a class and inherit it from type str,
then add the needed method to this class from lines 2 to 4. In
line 5, we instantiate this class and pass it a random string.
However, such input data does not exist in the project’s test
cases. Existing Python testing tools can only mutate values,
and they cannot generate test case to trigger this leak when
their seeds do not contain such structures.

1
2
3
4
5

class self_class(str):

index = 1
def __index__(self):
return self.index

obj2 = self_class(’’)

III. TERMINOLOGY
In this section, we will introduce some conceptions used in

our approach.

A Python object. A Python object can be described as a

pair:

obj = (cid:104)T ype , Attrs(cid:105)

, where T ype denotes the object’s type and Attrs is a
collection of its attributes. One attribute can be a member
method, a member variable, or an element in a collection type
object. Each attribute is a Python object as well.

Constraints on type inheritance. Let set T represent all
Python types, and ∀t1, t2 ∈ T, t1 (cid:22) t2 denotes that types t1
and t2 are the same, or that type t1 inherits from type t2.

If two sets Tbt ⊆ T and Tnbt ⊆ T satisfy ∀t1 ∈ Tbt, t2 ∈
Tnbt, ¬(t1 (cid:22) t2) ∧ ¬(t2 (cid:22) t1) ,we can deﬁne path constraints
on the type inheritance of an object (Type constraints for short)
as:

∀t1 ∈ Tbt, t2 ∈ Tnbt, (obj.T ype (cid:22) t1) ∧ ¬(obj.T ype (cid:22) t2)

Set Tbt represents all the types that an object should be or
should inherit from, whereas the set Tnbt contains all the types
that should not be or should not inherit from

Constraints on attribute existence. Let set A represent
all Python objects, and ∃a ∈ A, a ∈ obj.attrs indicates that
Python object obj contains the attribute a.

If there are two sets Abt ⊆ A and Anbt ⊆ A, and they fulﬁll
Abt ∩ Anbt = ∅, we can express the constraints on attribute
existence of an object (attribute constraints for short) as:

∀a1 ∈ Abt, a2 ∈ Anbt, (a1 ∈ obj.Attrs) ∧ (a2 /∈ obj.Attrs)

Set Abt represents all of the attributes that an object needs
to contain, whereas the set Anbt represents the attributes that
should not possess.

3

Structure Constraint. Every Python object can be mapped
to a quad (Tbt, Tnbt, Abt, Anbt) based on the deﬁnitions of
constraints on type inheritance and attribute existence.

Because distinct attributes in a Python object correspond to
different quads, one structure constraint of a Python object
(SC for short) is a conjunctive normal form of a set of
mappings:

SCobject = map1 ∧ map2 ∧ · · · ∧ mapi

. Each mapi is a mapping connections between an object and
a quad: mapi = obji (cid:55)→ (Ti bt, Ti nbt, Ai bt, Ai nbt), where
obji can be the Python object object or one of its attributes.

IV. THE PROPOSED APPROACH

Figure 2 depicts the workﬂow of our approach, encompass-
ing two phases: Static analysis and Structure constraint-
guided Testing. In the ﬁrst phase, we construct a value seed
to house all of the values used in the projects. Also, we
instrument the Python/C API to capture execution informa-
tion. The second stage uses a loop to iteratively explore the
execution paths, stopping when no new SC appears. In each
loop, we construct SCs for existing input data based on API
call logs. Guided by the SCs, our approach generates new
Python objects as input data for the next loop.

Fig. 2. The workﬂow of our approach.

A. Static analysis

This section illustrates how we collect project values and

instrument the Python Interpreter.

1) Value Set Construction: In addition to SCs, there are
constraints on input values, such as branch conditions that
compare the input value to a certain value. However, ex-
tracting value constraints from native libraries exactly can
be problematic. On the one hand, extracting from source
code requires complete call function graphs and path-sensitive
interprocedural analysis, which are challenging to achieve
with present static analysis tools. On the other hand, it is
tricky to relate the instructions to input data if monitoring
the instructions. We cannot ﬁnd vulnerabilities lying in paths
if we generate values randomly.

To circumvent value constraints, we apply a lightweight
strategy, where we collect all of the values used in the projects
and employ them in the generating phase. We can produce
values that satisfy such branch conditions in this manner.

In detail, we collect values from two aspects: the literals in
the source code and objects passed to native methods in the
projects. We ﬁrst scan Python and native library source code
and store all the literals. Then, to collect objects passed to
the native methods, we utilize the debug tool pdb to insert
breakpoints to each native method call. In this way, we can
stop at each callsite and dump the input data while running
the test suites in the projects. We build the value set ValueSet
to hold all of the data we collect.

2) Python Interpreter Instrumentation: As mentioned in
Section I, native libraries serve as black boxes for Python code.
In our approach, we utilize Python/C APIs to monitor native
library execution. We instrument these API functions at the
source code level and recompile the interpreter.

The interpreter supports roughly a thousand API functions,
with parts implemented using function-like macros. Because
not all the functions are for processing input data, we man-
ually review the documentation and ﬁlter two categories of
functions: one for type checking and another for attribute
extracting. We utilize two separate instrumentation strategies
to output in the same format while their implementation is not
the same.

• API functions for type checking. This category of func-
tions takes a reference as an input and returns a boolean
value indicating whether the referred Python object
is
of the given type. We collect 50 functions from the
documentation and insert code to output the reference, the
veriﬁed type, and the result.

• API functions for attribute extracting. This category
of API functions receives two pointers as input, one to
a Python object and the other to a key, and returns the
attribute if it exists or NULL if it does not. We gather 14
functions and insert code to print the two pointers and the
return value.

Using the output of the instrumented code, we can abstract
each API function call into a 4-tuple: AP I(o, act, valact, ret),
where o stands for the input data, act for the functionality,
such as type checking or attribute extracting, valact for the
argument, and ret for the return value.

Example for instrumentation: From lines 2 to 6 in the
following code, we give the type-checking API function
PyDict_Check implementation, where we instrument code
at line 4 to output the checked type dict, the checking result,
and the argument. Besides, we present the simpliﬁed imple-
mentation of API function PyMapping_GetItemString
from lines 8 to 13. This function is for extracting attributes
from the input data. Lines 10-11 are the instrumented code
that prints the attribute r, the key key, and the input data o.

// Objects/dictobject.c
int PyDict_Check(void *op){

int result = PyType_FastSubclass(
PY_TYPE(op),

Py_TPFLAGS_DICT_SUBCLASS);

printf("%p, Chk, dict, %d \n", op, result);
return result;

}
// Objects/abstract.c

1
2
3
4

5
6
7
8

4

ExecutionReverseGeneration(T'i_bt, T'i_nbt, A'i_bt, A'i_nbt)New Python objectobji→ (Ti_bt, Ti_nbt, Ai_bt, Ai_nbt)Instrumented Python InterpreterNative methodsValue setStructure Constraint-guided TestingStatic analysisInput data: <object, Ø>PyObject * PyMapping_GetItemString(PyObject *o,

char *key){

PyObject * r = PyObject_GetItem(o, key);
printf("%p, Get, %s, %p \n", o, key, r);
return r;

9

10
11
12
13

}

B. Execution

In this part, we construct SCs for existing input data. On a
high level, we use an instrumented Python Interpreter to call
native methods. Then, using the interpreter output, we gather
associated API function call logs and construct the SCs.

We will receive a series of API function calls after execut-
ing. However, we cannot match the C structures processed by
the API functions with the input data just based on logs. This
is because native libraries can also employ API functions to
process locally created objects or one of the arguments passed
to the native method. Without knowing which calls are for
input data, we will construct plenty of constraints that are not
related to the input data, limiting our exploration’s efﬁciency.
Consider the code shown in Figure 1, API function calls to
object o2 are for this native method’s second argument. It
will be hard to explore paths in this C function if we wrongly
assume that these calls are for processing the ﬁrst argument.
Thus, we scan the logs twice to build the SCs. We gather
type and attribute constraints on input data for the ﬁrst time.
The second time, we collect constraints on attributes of input
data to fulﬁll the SCs.

Because all Python objects are stored in Python heap space,
an object’s memory address will not change when transferred
from Python code to native libraries. Thus, we can ﬁlter the
function calls that process the input data using the memory
address. Before invoking a native method, we ﬁrst retrieve
the memory addresses for the input data at the Python code
layer, then append these addresses to the SCs. If the address
in one call appears in SCs, it indicates that this function is
for processing the input data. We can establish the type and
attribute constraints for the input data in the ﬁrst scan.

However, using memory address matching to create con-
straints for attributes is inefﬁcient. Because an attribute is
also a Python object, the Attrs of a Python object is tree-
structured. Acquisition of memory addresses for all the at-
tributes can cause recursive traversal and consumes a lot of
space.

As the addresses of input data were added to the SCs in
the ﬁrst step, we ﬁlter the calls of attribute extraction and
add the address of the extracted attribute to the SC. We can
then use the extracted attribute to ﬁlter API function calls and
progressively generate SCs for the input data.

The two guidelines in Table I deﬁne the building process.
We present example statements, API function call patterns,
and updating operations for each rule. When we process an
API function for type checking, such as the call patterns in
rule R1, we adjust the type constraints of obj based on the
return result if the obj is recorded in the SC. Speciﬁcally,
if the return value is true, we add the valact to the set Tbt
through operation op1.1, or to the set Tnbt by operation op1.2.

When dealing with an API function for attribute extracting, as
described in rule R2, we modify the attribute constraints if the
obj is in the SC. Firstly, we add the attribute with an empty
set to the SCs using the operation op2.1. Then, similar to the
operations in rule R1, if the return result is not null, indicating
that the object obj possesses the attribute, we will add it to
the set Abt in the operation op2.2. Otherwise, we shall add
the attribute to the set Anbt as this object does not contain it.
Example for SC construction. We pass in two Python
objects [1,2,3] and "abc" for
the native method
numpy.power, and we can retrieve call logs when executing
the C function in Figure 1 as the follows.

1
2
3

API(o2, ’Chk’, Long, False)
API(o2, ’Chk’, Float, False)
API(o2.’Get’, ’__index__’, False)

We add the object o2 to the SCs as o1 before calling this
native method. When dealing with the calls on lines 1 and 2,
we use operation op1.2 to update the SC: o2 (cid:55)→ (To2 nbt =
{f loat, long}, Ao2 nbt = {

index }).

For the call on line 3, we ﬁrst add the attribute to SCs with

the operation op2.3. After line 3, SCs are:
o2 (cid:55)→ (To2 nbt = {f loat, long}, Ao2 nbt = {
∧o1.

index

(cid:55)→ ().

index })

C. Reverse

The purpose of reverse is to ﬂip the result of the current
branch condition and then create a new Python object to fulﬁll
the opposite path conditions. We select to build the original
SC and its reverses simultaneously.

As indicated in Section IV-B, we traverse the call logs and
use the operations in Table I to create the original SC. Before
we update the original SC for each operation, we produce
one reversed SC by making a copy and adding the valact
to the opposite set. Thus, a reversed SC can represent the
constraints from the native method entrance to the current
branch condition, and another path to the current branch
condition.

The speciﬁcs of the reverse process are present in Algo-
rithm 1. According to the API call, the get_operates
function on line 4 obtains operations from Table I. From lines
5 through 16, we go over each operation. Line 6 generates a
copy before updating the original SC, while function apply
in line 7 changes the original SC depending on the operation.
Lines 8-15 update the set against the operation in the copied
SC. If this reversed SC does not appear before, we include
it in the result mut_list.

Because each rule only increases one constraint, and we
create one reversed SC for each rule, the number of reversed
SC does not exceed the number of constraints in the original
SC.

An example for the reverse. Consider the API function
calls in the example for SC construction. When we process
the call on line 1, we add long to the T nbt in the original
SC, while building a reverse SC: o1 (cid:55)→ (To1 bt = {long}).

After we process the calls on lines 2 and 3, we can
produce two reversed SCs. The ﬁrst is: o1 (cid:55)→ (To1 bt =

5

TABLE I
RULES FOR CONSTRUCTING STRUCTURE CONSTRAINTS.

Rule Example Statements

Call patterns

OP ID Updating operations

R1

PyDict Chk(obj);

AP I(obj, Chk, valact, ret)

OP1.1

obj (cid:55)→ (Tobj bt ∪ {valact})|ret = T rue ∧ obj ∈
SC

R2

PyList GetItem(obj, ind); AP I(obj, Get, valact, ret)

OP1.2

obj (cid:55)→ (Tobj nbt ∪ {valact})|ret = F alse ∧ obj ∈
SC

OP2.1

SC ∧ (ret (cid:55)→ ())

OP2.2

OP2.3

obj (cid:55)→ (Aobj bt ∪ {valact})|ret (cid:54)= N U LL ∧ obj ∈
SC

obj (cid:55)→ (Aobj nbt ∪{valact})|ret = N U LL∧obj ∈
SC

Algorithm 1: Reverse on one SC
Input: API function call logs api_list
Output: One original SC cons, a set of reversed

SCs mut_list

Algorithm 2: Generation SC
Input: One SC StrucCons, the value set

ValueSet

Output: One Python object obj

1 mut list ← list()
2 cons ← dict()
3 for api ∈ api list do
4

ope list ← get operates(api)
for ope ∈ ope list do

5

6

7

8

9

10

11

12

13

14

15

16

17

18

mut cons ← copy(cons)
apply(cons, ope)
if ope is OP1.1 then

update(mut cons, ope, Tnbt)

else if ope is OP1.2 then

update(mut cons, ope, Tbt)

else if ope is OP2.2 then

update(mut cons, ope, Anbt)

else if ope is OP2.3 then

update(mut cons, ope, Abt)

else

apply(mut cons, op2.1)

mut list.add(mut cons)

19 return cons, mut list

{f loat}, To1 nbt = long),
and the second is: o1.
∧o1 (cid:55)→ (To1 nbt = {long, f loat}, Ao1 bt = {

index

(cid:55)→ ()

index }).

D. Generation

Algorithm 2 depicts the generation process. From a high
level, we ﬁrst produce a piece of class deﬁnition code that
causes the class to inherit from the types in the Tbt but not
in the Tnbt. Then we can instantiate this class and modify the
attributes based on Abt and Anbt.

Because SC imposes constraints on both the object and its
attributes, we must ﬁrst generate attributes before producing

6

1 gen set ← topological sort(StrucCons).keys())
2 attrimap ← dict()
3 for attri ∈ gen set do
4

cons ← StrucCons[attri]
builtins ← get builtin types()
if cons.Tbt (cid:54)= ∅ then

inherits ← (builtins - cons.Tnbt) ∩ cons.Tbt

else

inherits ← (builtins - cons.Tnbt)

self cls ← build class(inherits)
up class(self cls, cons, attrimap, ValueSet)
obj ← self cls()
up object(obj, cons, attrimap, ValueSet)
attrimap[attri] ← obj

5

6

7

8

9

10

11

12

13

14

15 return attrimap[attriset.last]

the object. In line 1, we topologically order all keys in SC
based on their containment relationship. For example, if object
o1 appears in Abt or Anbt of object o2, we consider o2
contains o1 and sort o2 behind o1. We use a map attrimap
in line 2 to store all the attributes we have produced.

The two language features dynamic attribute and dynamic
type cannot be utilized to generate objects since the API
functions only check for types and member methods based
on the class deﬁnition code. Thus, each object generation is
completed in three phases. First, we construct a class that
inherits needed types from lines 5–10. Then,
in line 11,
following the attribute constraints, we change the member
variables and methods in the class deﬁnition code. Finally, if
the object is of collection types, as demonstrated in lines 12 to
13, we instantiate this class and adjust the elements depending
on attribute constraints.

We compute the types that the class needs to inherit from

lines 5 to 9, where we subtract the types in the set cons.Tnbt
from the Python built-in type set builtins, then intersect
the result with cons.Tbt. If the set cons.Tbt is empty, we do
not conduct the intersection operation since the object is not
required to inherit from a speciﬁc type. In line 10, we produce
a piece of class deﬁnition code to make the self-deﬁned class
inherits from the types in the set inherits in the function
build_class.

We change member methods and variables in the self-
deﬁned class depending on cons.Abt and cons.Anbt in the
function up_class on line 11. We extract a member method
or variable from the attrimap and add it to the class deﬁni-
tion code if this attribute is required to exist. For example, if
index }, we can add a method named
the set cons.Abt is {
__index__ and a member variable as this method’s return
value to the class deﬁnition code. The member variable’s name
can be generated randomly while its value comes from the
value set ValueSet.

We instantiate this class on line 12. The object obj is then
updated at line 13 via the function up_object if it is of
collection types and there exist constraints on its elements,
where we add required attributes to this object.

In line 14, we append the obj object into the attriset.
The input object we produced for the SC is the last element
added to the attriset after ﬁnishing the loop from lines 4
to 14.

An example for the generation. The following SC is used

index

index , names})

as an example to demonstrate the generating process:
o1 (cid:55)→ (To1 bt = {dict}, Ao1 bt = {
∧o1.
(cid:55)→ (To1.
∧o1.names (cid:55)→ (To1.names bt = {list}).
We start by sorting the three keys in the SC, and we can
retrieve gen_set as o1.__index__, o1.names, and o1,
which implies we need to produce two attributes before
building object o1.

bt = {f loat})

index

1
2
3
4
5

class self_class(dict):

retvalue = 1.0
def __index__(self): return self.retvalue

obj1 = self_class()
obj1["names"] = []

the

We

index

produce

a method __index__ for

key
o1.__index__. Because To1.
bt contains f loat,
this method must return an float value. Thus, we generate a
member variable of type float and return it in this method.
If the attribute o.names exists in the attriset, we can
utilize the matching value to produce a key-value pair and add
it to the object obj. Or we can randomly generate a value
to produce the key-value pair. We generate an empty list for
constraints on o1.names, as required by To1.names bt.

For constraints on o1, we ﬁrst create a class that inherits
from type dict based on its Tbt, then add member method
and variable to the class deﬁnition code (lines 2-3), and last
instantiate the class and insert the required key-value pair
(lines 4-5). After this, object obj1 is the object we generate
for this SC.

7

V. EVALUATION

A. Evaluation Setup

We instrument Python 3.8.5 and develop PyCing with
Python 3. All of the code and data are open to the public
1.

We traverse a list of the top 50 Python projects on GitHub
by star rating. As the target of PyCing is testing Python native
libraries, We choose projects based on the three criteria listed
below: (1) incorporate native libraries, (2) do not use lazy
evaluation methods [9] since we cannot guarantee that a native
method runs or only returns a symbol, and (3) have bug reports
about native libraries. We remain six projects from the ﬁfty
for our experiments.

In contrast to statically typed languages that determine the
method types and deﬁnitions during compilation, obtaining
native methods can be challenging as Python only determines
the method deﬁnitions when executing. Besides, no type
inference tools support method types [7], [34], [35].

Thus, we opt to collect native methods from two sources:
existing studies and project documents. Several studies have
analyzed native libraries for some Python projects [9], [28],
[36]. We can glean native methods from their publicly avail-
able experimental data. Besides, we review the project doc-
umentation for all available methods and then verify their
implementation to collect native methods.

1) Reserch Questions: We propose three research ques-
tions, and by answering them, we will present the results of
our experiment.

• RQ1. What is the efﬁciency of PyCing?
• RQ2. What is the test effectiveness of PyCing in real-world

Python projects?

• RQ3. What

is the test effectiveness of PyCing when

compared to other fuzzers?

In answer to the ﬁrst question, we evaluate the impact
of PyCing instrumentation and settings on test efﬁciency. To
answer the second question, we collect modern Python projects
from GitHub and compare test cases in these projects with
PyCing. Because existing tools cannot obtain code coverage
from these projects, we compare PyCing against two state-of-
the-art Python fuzzers on a set of handwritten benchmarks to
answer the third question.

2) Evaluation Criteria: As indicated in Section I, native
libraries do not provide code coverage. Thus, we evaluate
testing effectiveness from 2 factors: the number of SCs and
the types of API functions. When the number of test cases
is identical, the number of SCs can indicate the number of
examined execution paths, while the types of API functions
the structure coverage that native libraries can
can reﬂect
handle.

3) Conﬁguration: All experiments run on a PC equipped
with an Intel Core i7 3.60GHz CPU and 32 GByte RAM. The
GitHub projects are built on Ubuntu 18 using Python 3.8.5.
We establish a time restriction of 24 hours for each project,

1https://anonymous.4open.science/r/pycing-present-6BC7

determined by the comparative experiments speciﬁed in papers
or extra materials of existing Python testing tools.

B. Performance of PyCing (RQ1)

Two aspects will inﬂuence the efﬁciency of PyCing: in-
strumentation to the interpreter and conﬁgurations in the
generation phase of PyCing.

We ﬁrst assess the inﬂuence of instrumentation on execution
performance. The instrumented interpreter is compared to a
standard interpreter and a commonly used dynamic binary
instrumentation framework pin [37]. We utilize the test cases
in these projects as benchmarks since they can represent
common usage, and we measure the execution time with
the testing framework Pytest. Because Pytest supports multi-
processing, we can run test cases with different numbers of
processes. We can directly invoke Pytest as a Python module
for the two interpreters. For the framework pin, we use the
default pintool to monitor at the function level and execute on
the standard interpreter inside the framework. Project CPython
is excluded from this experiment as many native methods in
this project deal with suspended execution or multi-processing,
both of which have a substantial impact on the running time
count.

Table II shows the number of test cases and execution
times under a single process (1), two processes (2), and four
processes (4). If one test case runs for more than an hour or
the operating system terminates the Python virtual machine,
we will mark it as NA for this conﬁguration. According
to the data, our instrumentation strategy has an acceptable
inﬂuence on execution performance, while dynamic binary
instrumentation increases the time by 5x to 10x. Increasing
the number of processes might lower the time under dynamic
binary instrumentation. However, testing one native method
requires several rounds of sequential execution, and increasing
the number of processes has little effect on test efﬁciency.
In the SciPy project, the pin fails because of conﬂicts when
instrumenting one binary ﬁle under the multiprocess mode, or
it takes too much memory for storing interim results, causing
the OS to terminate it.

TABLE II
EXECUTION TIME UNDER DIFFERENT INSTRUMENTATION STRATEGIES
(SECONDS).

Project Tests

Standard

Instrumented

1

2

4

1

2

4

1

Pin

2

4

Matplotlib 8K 404 243 160 434

297 183 2,762 1,489 991

NumPy

17K 151 107 73

179

121

89 1,034 667 446

Pandas

132K 1,644 969 787 1,992 1,171 893 9,653 5,476 NA

Pillow

2K

49

25 16

52

29

18

350

217 157

SciPy

35K 477 305 245 545

333 255 NA NA NA

To prevent combinatorial explosions, we restrict the number
of SCs in each loop’s generation phase. We choose a value
every 200 as the upper limit in the range of 200 to 1200 to ﬁnd

an appropriate conﬁguration. PyCing can complete in under 2
hours in each setting. Due to space limitations, we only display
the data from the project NumPy and place the results from
other projects on our website.

Fig. 3. Number of explored SCs under each upper limit.

The results in Figure 3 indicate that increasing the upper
limit of SCs helps us to reach the top bound faster, and the
exploration can ﬁnish in less than 40 loops. However, raising
the maximum limit needs more time to complete one loop.
Even though PyCing can complete exploration in 12 loops
under the 1200 upper limit, this conﬁguration takes longer than
others. Considering the resource usage and execution speed,
we believe 800 SCs per loop is the acceptable conﬁguration.
Conclusion. The execution time comparison reveals that our
instrumentation strategy has less inﬂuence on execution efﬁ-
ciency. Obtaining execution information from native libraries
via dynamic binary instrumentation can be time-consuming or
may fail since it demands monitoring the whole Python virtual
machine.

C. Test Effectiveness of PyCing (RQ2)

In answering this question, we will ﬁrst evaluate the efﬁ-
ciency of test cases in real-world projects, then investigate the
improvement made by PyCing.

To quantify the size of native libraries developed by these
projects, we use the tool cloc to count the lines of source
code in C/C++, FORTRAN, and Cython. Besides, we count
the types and numbers of Python/C APIs employed in the
native libraries. Columns 3-6 of Table III demonstrate that the
six projects develop their native libraries using a variety of
languages and a large number of API functions, some of which
are multilingual, providing challenges for building summaries.

We ﬁrst count the number of SCs explored by existing test
cases. The number of native methods is shown in the second
column of Table IV, and the number of types of input data used
by test cases to the native methods is listed in the third column.
The fourth column shows the number of SC explored by the
test cases. According to the results, the test cases employ a
wide range of input data to test native methods. But when
compared with the number of explored SCs, we can observe
that these test cases fall short of addressing execution paths
with structure constraints.

We compare the number of SC explored by the test cases
to the number identiﬁed by PyCing to explore whether PyCing

8

600070008000900010000110001200013579111315171921232527293133353739Number of SCsTesting LoopUp-200Up-400Up-600Up-800Up-1000Up-1200Figure 5 presents the results. As shown, PyCing can cover
more APIs for type checking and attribute extraction with
the same amount of test cases, particularly for attributes.
Moreover, PyCing can result in more types of return values
for these API calls.

TABLE III
BASIC INFORMATION OF EXPERIMENTED PROJECTS.

Project

Version C/C++ FORTRAN Cython

API

CPython

3.10.1

552K

Matplotlib

3.5.1

69K

0

0

0

0

6,249

56

NumPy

1.22.3

185K

416

3,589

5,885

Pandas

Pillow

SciPy

1.4.2

9.0.1

1.8.0

7,017

33K

135K

0

0

20K

38,943

0

136

74K

11K

31,131

TABLE IV
NUMBER OF SC S COVERED BY EXISTING TEST CASES.

Project

Native Method

Input data

SC

Fig. 5. Number of types of API functions and return value covered by test
cases and PyCing.

Cpython

Matplotlib

NumPy

Pandas

Pillow

SciPy

499

10

194

188

16

224

176K

1,889

185K

467K

1,475

33K

1,043

10

218

334

35

242

can increase test effectiveness. Figure 4 shows the comparing
results. Because PyCing ﬁnds a signiﬁcant number of SCs for
a few native methods, we exhibit the distribution based on the
number of SCs explored by the test cases.

As the results show, PyCing can ﬁnd more SCs than
test cases for most native methods, particularly in the SciPy
project, where test cases examined 242 SCs whereas PyCing
examined 160K totally.

Fig. 4. Number of SCs explored by test cases and by PyCing.

As mentioned in Section IV-A2, we instrument type check-
ing and attribute extraction API functions. To explore which
category of API functions aid PyCing to improve test effec-
tiveness, we count two kinds of API functions covered by
PyCing and test cases. Besides, because the return values of
API functions can cause the code to run alternative branches,
we count the different types of return values of these functions.
In particular, we regard returning true and false as two types
of return values for type checking API methods and returning
NULL and non-NULL to be two types for attribute extraction
API functions.

9

We also consider the inﬂuence of structures not covered by
the test cases on the test effectiveness. The second column
in Table V indicates the new API functions discovered by
PyCing, and the third column gives the number of native
methods that utilize these API functions. In the table, API
functions ending in Exact return true if the argument is of
the checked type but not a subtype, while API functions ending
in get extract attributes. The results in Figure 5 and Table V
demonstrate that PyCing can improve test effectiveness in two
ways. First, PyCing can trigger various API return values for
the API functions examined by the test cases. Second, PyCing
can cover API functions not covered by test cases.

TABLE V
NEW API FUNCTIONS EXPLORED BY PYCING.

Project

Structures

# Met

CPython

set, object getitemstring

Matplotlib

bytes, dict, list, sequence, tuple, tuple-
Exact, sequence getitem,

NumPy

bytearray,
complexExact, dictExact,
list, mapping, mapping getitemstring,
object getitem

Pandas

object getattr, object getitemstring

Pillow

object getattr

SciPy

bytearray, complex, ﬂoatExact, tuple-
Exact, object getattr

2

2

94

5

3

12

1) Bug Detection Results: In this paper, we use PyCing
to detect memory leaks and segmentation faults in native
libraries. The testing framework Pytest provides a checker
pytest-leaks [38] for detecting memory leaks, while segmen-
tation faults will make the interpreter terminated. We ﬁrst
use PyCing to produce test cases for the native methods
in the six projects, and run these test cases via Pytest. We
can produce test cases to trigger memory leaks in 9 native
methods, and segmentation faults in 10 methods. Table VI
shows distributions of bugs among the projects.

Test casesPyCingCpython      Matplotlib         NumPy            Pandas            Pillow            SciPyNumber of SCsCpython        Matplotlib         NumPy              Pandas            Pillow                SciPyNumber of typesAPIs by PyCingAPIs by test casesAPIs & Return value by PyCingAPIs & Return value by test casesTABLE VI
BUGS DETECTED BY PYCING

Projects

Memory Leaks

Segmentation Faults

Cpython

Matplotlib

NumPy

Pandas

Pillow

SciPy

SUM

1

1

4

-

-

3

9

1

-

2

2

3

2

10

Based on leaks we report to the developers, we can ﬁnd
that developers often cause leaks when using API functions to
extract values from input data while ignoring the side effects
of the change in the reference count of the return value. The
following code utilizes two APIs to obtain the ﬁrst element
from an object o of type list. However, the ﬁrst API raises
item1’s reference count. Thus, when developers forget to re-
duce the reference count, a leak occurs. We advise developers
to use uniform API categories to minimize misunderstandings
about the behavior of the returned response.

1

2

PyObject item1 = PySequence_GetItem(o, 0); //

increase

PyObject item2 = PyList_GetItem(o, 0); // not

increase

We cannot pinpoint the source of segmentation issues. It
is due to the lack of debugging information in the project
compilation process. Besides, the OS stops the interpreter
before it outputs error information. As a result, even if we
may produce many test cases that trigger the bugs, we count
these bugs at the native method level.

A case study. The following code shows a leak found by
PyCing in the project CPython and it affects Python 3.9, 3.10,
and 3.11 [39]. When the native libraries _flatten1 from
lines 8 to 12 returns 0, function _tkinter_flatten does
not reduce the reference of obj before returning at line 5.
The input object needs to execute the false branch at line 9
to trigger this leak, and PyCing produces a str object for
it. As test suites do not contain such a type, input data from
existing fuzzers can not cover the satisfying type to trigger it.
The input object generated by PyCing has been added to the
test suites of this project.

1
2
3
4
5
6
7
8
9
10
11

PyObject * _tkinter_flatten(PyObject * obj){

...
if(!_flatten1(obj)){

// fix patch: + Py_XDECREF(obj)
return 0;

}

}
int _flatten1(PyObject * obj){

if(PyTuple_Check(obj) || PyList())

return 1;

return 0;

}

Conclusion. Real-world Python projects include plenty of
native libraries written in multiple statically typed program-

ming languages. These native libraries use a plethora of
Python/C APIs in condition branches to handle various kinds
and attributes of input objects. When comparing the number
of APIs used to the number of APIs covered by original test
cases, we can ﬁnd that native libraries are not well tested, even
though these projects have a considerable number of test cases
and code coverage of 95% in Python.

The results of six real-world experiments reveal that PyC-
ing can effectively explore execution paths, especially those
related to the types and attributes of input objects. PyCing
can aid in the automated generation of test cases for native
library testing and memory leak detection.

D. The Comparison with other Fuzzers (RQ3)

To evaluate the effectiveness of PyCing, we choose to
compare it with two state-of-art Python fuzzers, python-
fuzz [40] and python-aﬂ [41]. pythonfuzz is a coverage-guided
fuzzing tool maintained by Google, which detects unhandled
exceptions and memory limitations in Python libraries, while
python-aﬂ applies American Fuzzy Lop (AFL) for pure Python
code. Currently, python-aﬂ is an experimental module.

test

As indicated in Section I, current

tools cannot ac-
quire code coverage from native libraries in the experimental
projects, and the two Python fuzzers are unable to exploit
structure constraints. To compare two fuzzers with PyCing
fairly, we generate multiple sets of codes that contain the type
and attribute judgments as benchmarks. Because pythonfuzz
and python-aﬂ are for pure Python and PyCing for testing
native libraries, each set contains C and Python code snippets
with the same functionality.

To ensure that Python and C code behave identically, we
use Python/C APIs to process the types and attributes of
input objects in C code, then write code templates with the
same functionality for each API function in Python code.
A type-checking API function is equivalent to the built-in
method isinstance, and an attribute-extracting API func-
tion corresponds to another built-in method in. For simplicity,
we do not process the attributes of the input objects in the
benchmarks.

Thus, we can generate C code by adding multiple branches
and randomly selecting API functions in the branch conditions,
where we ensure each function receives the same object.
Because the attribute-extracting API functions do not return
a boolean variable, we compare the return result to NULL in
the branch condition. Then, in the Python code, we may retain
the form of the if statements while substituting handwritten
templates for the APIs.

Figure 6 presents two pieces of codes. The left piece of
code is for python-aﬂ and pythonfuzz, while the right is for
PyCing. In both Python and C code, lines 1 to 4 check if the
input is of the dict type. Lines 5 to 10 in both sides inspect
if the object includes the key ’names’, and lines 11 to 16
check for the key ’formats’. We use the Python code as
fuzz target for Pythonfuzz and python-aﬂ while compiling the
C code to produce dynamic link libraries for PyCing.

10

can investigate more execution paths in native libraries. The
proposed instrumentation method can also be applied to other
interpreters.

VII. RELATED WORKS

In this section, we brieﬂy describe some related works.
Detecting Memory Leaks via Testing Approaches. Some
studies employ testing techniques such as fuzzing to uncover
new paths and provide methodologies for monitoring memory
usage and detecting memory leaks. Valgrind [42] is a widely
used tool for detecting memory leaks. It monitors the execution
of programs to collect information on memory allocation and
free. Cristian et al. [14] use a solver on the acquired path
conditions to generate new input data. Ma et al. [43] employ
UI testing techniques and dump heap ﬁles to evaluate memory
leaks. For 10 open source apps and 35 commercial apps, they
report more than one leaked action or fragment. They create
test cases by deﬁning test generating methods for two types
of GUI event sequences and following certain typical leak
patterns. Wu et al. [44] combines static analysis and model
checking to detect resource leaks in Android Apps. However,
Their test scenarios do not involve cross-language situations.
Cross-language Analysis. Some researchers attempt
to
provide summaries for native libraries to guide native method
testing. Fourtounis et al. [27] expand the Java call graph to
include both Java and native code. Zhao et al. [18] focus on
the calls to several high-risk system APIs in native libraries
and collect branch prediction data for the execution path that
includes these APIs. To assist fuzzing in Java code,
they
employ path conditions gathered in native libraries. Lee [17]
creates summaries for native libraries, which are subsequently
used in the analysis of Java code.

Testing Approaches in Python. Many studies use test-
ing techniques to identify ﬂaws or wasteful code in Python
projects. Lukasczyk et al. [13] proposed a method for creating
test cases automatically. Holmes et al. [45] put forward a
method for guiding generation that relies on relative lines. The
two methods focus on value generation and require users to
deﬁne the types.

VIII. CONCLUSION

In this paper, we propose a lightweight approach for testing
Python native libraries, and we implement it in a tool called
PyCing. By imposing constraints on the structures of input
data and instrumenting the Python Interpreter, PyCing can
overcome the language barrier and improve test efﬁciency.
The experimental results in six real-world Python projects
show that PyCing explores execution paths more efﬁciently
than state-of-the-art Python fuzzers and can generate seeds for
these fuzzers to boost their test effectiveness. PyCing also ﬁnds
memory leaks in 9 native methods and segmentation faults in
10 methods. Developers have conﬁrmed eight leaks. In the
future, we will introduce value constraints in native libraries
to PyCing to improve test effectiveness.

Fig. 6. Two snippets of handcrafted code for testing Pythonfuzz and Python-
aﬂ.

pythonfuzz and python-aﬂ can only enter the True branch of
the ﬁrst boolean statement. The two tools are not effective in
dealing with constraints on the structures. PyCing can explore
all the execution paths based on the output of the handcrafted
code. When the input objects generated by PyCing are seeded
to these tools, the tools can mutate the values in the objects,
covering the paths in these benchmarks.

Conclusion. Existing fuzzers are more concerned with the
value of the input data than with its structure; as a result,
they are ineffective when an execution path incorporates input
data structures. Adding support for complicated structures and
using feedback other than code coverage to assist generation
can be improvable.

VI. THREATS TO VALIDITY

We will discuss some internal threats and external threats

to our work in this section.

Internal Threats. The native method collection is one of
the most signiﬁcant internal threats. There are some native
methods not found by the relevant research, or not recorded
in the project documentation. But we believe that there are
fewer undocumented native methods, or that some are soon
to become outdated. On the other hand, the project does not
want these methods to be called directly.

Another threat arises with the generation of new input
data. Because we do not employ a constraint solver in the
generation, there exist some constraints that we can not create
new objects. Besides, because PyCing primarily focuses on
the types and attributes, it cannot handle numerical constraints
between two objects. However, embedding the structure con-
straints into a solver can result in deviation, and a solver may
slow down the fuzzing process. Value constraints between
attributes are difﬁcult to ﬁnd and handle even by analyzing
the source code.

External Threats. We exclusively use PyCing to identify
memory leaks, and PyCing may not perform well on other
defects. Besides, because the memory leaks discovered by
us came from four Python projects,
the results may not
apply to other projects. Also, we only build PyCing and
instrument APIs with CPython, and may not work on other
implementations of Python or native libraries. But, the six
projects we choose are essential components in virtually all
Python code, and the experiment results show that PyCing

11

if not isinstance(a, dict):print(“1”)returnif 'names' not in a.keys():print(“2”)returnif 'formats' not in a.keys():print(“3”)returnprint(“4”)return123456789101112131415161718if(!PyDict_Check(obj)){printf(“1”);return NULL;}        PyObjectnames = PyMapping_GetItemString(obj,"names");if(names == NULL){printf(“2");return NULL;}            PyObjectformats =PyMapping_GetItemString(obj,“formats");if(formats == NULL){printf(“3");return NULL;  }printf(“4”)return NULL;Python codeC codeREFERENCES

[1] Aaquib Javed, Monika Zaman, Mohammad Monir Uddin, and Tasnova
Nusrat. An analysis on Python programming language demand and its
recent trend in bangladesh. In ICCPR ’19: 8th International Conference
on Computing and Pattern Recognition, Beijing, China, October 23-25,
2019, pages 458–465. ACM, 2019.

[2] Cython. https://cython.org/.
[3] Numpy. https://github.com/numpy/numpy.
[4] Scipy. https://github.com/scipy/scipy.
[5] Sklearn. https://github.com/scikit-learn/scikit-learn.
[6] Mingzhe Hu and Yu Zhang. The Python/C API: evolution, usage
statistics, and bug patterns. In 27th IEEE International Conference on
Software Analysis, Evolution and Reengineering, SANER 2020, London,
ON, Canada, February 18-21, 2020, pages 532–536. IEEE, 2020.
[7] Faizan Khan, Boqi Chen, Daniel Varro, and Shane Mcintosh. An
IEEE

empirical study of type-related defects in Python projects.
Transactions on Software Engineering, 2021.

[8] Zhifei Chen, Yanhui Li, Bihuan Chen, Wanwangying Ma, Lin Chen, and
Baowen Xu. An empirical study on dynamic typing related practices
In ICPC ’20: 28th International Conference on
in Python systems.
Program Comprehension, Seoul, Republic of Korea, July 13-15, 2020,
pages 83–93. ACM, 2020.

[9] Guoqiang Zhang and Xipeng Shen. Best-effort lazy evaluation for
In 35th European Conference on
Python software built on API.
Object-Oriented Programming, ECOOP 2021, July 11-17, 2021, Aarhus,
Denmark (Virtual Conference), volume 194 of LIPIcs, pages 15:1–15:24.
Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2021.

[10] Weijie Zhou, Yue Zhao, Guoqiang Zhang, and Xipeng Shen. HARP:
holistic analysis for refactoring Python-based analytics programs.
In
ICSE ’20: 42nd International Conference on Software Engineering,
Seoul, South Korea, 27 June - 19 July, 2020, pages 506–517. ACM,
2020.

[11] Suchita Mukherjee, Abigail Almanza, and Cindy Rubio-Gonz´alez. Fix-
ing dependency errors for Python build reproducibility. In ISSTA ’21:
30th ACM SIGSOFT International Symposium on Software Testing and
Analysis, Virtual Event, Denmark, July 11-17, 2021, pages 439–451.
ACM, 2021.

[12] Tim Shaffer, Kyle Chard, and Douglas Thain. An empirical study of
package dependencies and lifetimes in binder Python containers. In 17th
IEEE International Conference on eScience, eScience 2021, Innsbruck,
Austria, September 20-23, 2021, pages 215–224. IEEE, 2021.

[13] Stephan Lukasczyk, Florian Kroiß, and Gordon Fraser. Automated unit
test generation for Python. In Search-Based Software Engineering - 12th
International Symposium, SSBSE 2020, Bari, Italy, October 7-8, 2020,
Proceedings, volume 12420 of Lecture Notes in Computer Science,
pages 9–24. Springer, 2020.

[14] Cristian Cadar, Daniel Dunbar, and Dawson R. Engler. KLEE: unassisted
and automatic generation of high-coverage tests for complex systems
In 8th USENIX Symposium on Operating Systems Design
programs.
and Implementation, OSDI 2008, December 8-10, 2008, San Diego,
California, USA, Proceedings, pages 209–224. USENIX Association,
2008.

[15] Anna Derezinska and Konrad Halas. Experimental evaluation of muta-
tion testing approaches to Python programs. In Seventh IEEE Interna-
tional Conference on Software Testing, Veriﬁcation and Validation, ICST
2014 Workshops Proceedings, March 31 - April 4, 2014, Cleveland,
Ohio, USA, pages 156–164. IEEE Computer Society, 2014.

[16] Fengguo Wei, Xingwei Lin, Xinming Ou, Ting Chen, and Xiaosong
Zhang. JN-SAF: precise and efﬁcient ndk/jni-aware inter-language static
analysis framework for security vetting of android applications with
In Proceedings of the 2018 ACM SIGSAC Conference
native code.
on Computer and Communications Security, CCS 2018, Toronto, ON,
Canada, October 15-19, 2018, pages 1137–1150. ACM, 2018.

[17] Sungho Lee.

JNI program analysis with automatically extracted C
In Proceedings of the 28th ACM SIGSOFT In-
semantic summary.
ternational Symposium on Software Testing and Analysis, ISSTA 2019,
Beijing, China, July 15-19, 2019, pages 448–451. ACM, 2019.

[18] Jinjing Zhao, Yan Wen, Xiang Li, Ling Pang, Xiaohui Kuang,
and Dongxia Wang. A heuristic fuzz test generator for java na-
the 2nd ACM SIGSOFT Inter-
tive interface.
national Workshop on Software Qualities and Their Dependencies,
SQUADE@ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26,
2019, pages 1–7. ACM, 2019.

In Proceedings of

[19] Sungjae Hwang, Sungho Lee, Jihoon Kim, and Sukyoung Ryu. Justgen:
Effective test generation for unspeciﬁed JNI behaviors on jvms. In 43rd
IEEE/ACM International Conference on Software Engineering, ICSE
2021, Madrid, Spain, 22-30 May 2021, pages 1708–1718. IEEE, 2021.
[20] Yunho Kim, Shin Hong, and Moonzoo Kim. Target-driven compositional
concolic testing with function summary reﬁnement for effective bug
In Proceedings of the ACM Joint Meeting on European
detection.
Software Engineering Conference and Symposium on the Foundations
of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,
August 26-30, 2019, pages 16–26. ACM, 2019.

[21] Yun Peng, Yu Zhang, and Mingzhe Hu. An empirical study for common
language features used in Python projects. In 28th IEEE International
Conference on Software Analysis, Evolution and Reengineering, SANER
2021, Honolulu, HI, USA, March 9-12, 2021, pages 24–35. IEEE, 2021.

[22] Pytest. https://docs.pytest.org/.
[23] Cpython. https://www.python.org/.
implemented
interpreter
[24] Python

by

intel.

https://software.

intel.com/content/www/us/en/develop/tools/oneapi/components/
distribution-for-python.html.

[25] Python implemented in java. https://www.jython.org/.
[26] The python debugger. https://docs.python.org/3/library/pdb.html.
[27] George Fourtounis, Leonidas Triantafyllou, and Yannis Smaragdakis.
Identifying java calls in native code via binary scanning. In ISSTA ’20:
29th ACM SIGSOFT International Symposium on Software Testing and
Analysis, Virtual Event, USA, July 18-22, 2020, pages 388–400. ACM,
2020.

[28] Mingzhe Hu, Yu Zhang, Wenchao Huang, and Yan Xiong. Static type
inference for foreign functions of Python. In 32nd IEEE International
Symposium on Software Reliability Engineering, ISSRE 2021, Wuhan,
China, October 25-28, 2021, pages 423–433. IEEE, 2021.

[29] Can Yang, Zhengzi Xu, Hongxu Chen, Yang Liu, Xiaorui Gong, and
Baoxu Liu. Modx: Binary level partial imported third-party library de-
tection through program modularization and semantic matching. CoRR,
abs/2204.08237, 2022.

[30] Python/c API reference manual. https://docs.python.org/3/c-api/index.

html.

[31] Python memory management. https://docs.python.org/3/c-api/memory.

html.

[32] Siliang Li and Gang Tan. Finding reference-counting errors in Python/C
In ECOOP 2014 - Object-Oriented
programs with afﬁne analysis.
Programming - 28th European Conference, Uppsala, Sweden, July 28 -
August 1, 2014. Proceedings, volume 8586 of Lecture Notes in Computer
Science, pages 80–104. Springer, 2014.

[33] Junjie Mao, Yu Chen, Qixue Xiao, and Yuanchun Shi. RID: ﬁnding ref-
erence count bugs with inconsistent path pair checking. In Proceedings
of the Twenty-First International Conference on Architectural Support
for Programming Languages and Operating Systems, ASPLOS 2016,
Atlanta, GA, USA, April 2-6, 2016, pages 531–544. ACM, 2016.
[34] C. M. Khaled Saifullah, Muhammad Asaduzzaman, and Chanchal K.
Roy. Exploring type inference techniques of dynamically typed lan-
guages. In 27th IEEE International Conference on Software Analysis,
Evolution and Reengineering, SANER 2020, London, ON, Canada,
February 18-21, 2020, pages 70–80. IEEE, 2020.

[35] Rapha¨el Monat, Abdelraouf Ouadjaout, and Antoine Min´e.
Static
In 34th
type analysis by abstract interpretation of Python programs.
European Conference on Object-Oriented Programming, ECOOP 2020,
November 15-17, 2020, Berlin, Germany (Virtual Conference), volume
166 of LIPIcs, pages 17:1–17:29. Schloss Dagstuhl - Leibniz-Zentrum
f¨ur Informatik, 2020.

[36] Jialiang Tan, Yu Chen, Zhenming Liu, Bin Ren, Shuaiwen Leon Song,
Xipeng Shen, and Xu Liu. Toward efﬁcient interactions between Python
In ESEC/FSE ’21: 29th ACM Joint European
and native libraries.
Software Engineering Conference and Symposium on the Foundations
of Software Engineering, Athens, Greece, August 23-28, 2021, pages
1117–1128. ACM, 2021.

[37] Chi-Keung Luk, Robert S. Cohn, Robert Muth, Harish Patil, Artur
Klauser, P. Geoffrey Lowney, Steven Wallace, Vijay Janapa Reddi,
and Kim M. Hazelwood. Pin: building customized program analysis
the ACM
tools with dynamic instrumentation.
SIGPLAN 2005 Conference on Programming Language Design and
Implementation, Chicago, IL, USA, June 12-15, 2005, pages 190–200.
ACM, 2005.

In Proceedings of

[38] A pytest plugin to trace resource leaks.

https://pypi.org/project/

pytest-leaks/.

12

[39] A memory leak in CPython. https://bugs.python.org/issue44608.
[40] Pythonfuzz. https://pypi.org/project/pythonfuzz/.
[41] Python-aﬂ. https://github.com/jwilk/python-aﬂ.
[42] Nicholas Nethercote and Julian Seward. Valgrind: a framework for
In Proceedings of the
heavyweight dynamic binary instrumentation.
ACM SIGPLAN 2007 Conference on Programming Language Design
and Implementation, San Diego, California, USA, June 10-13, 2007,
pages 89–100. ACM, 2007.

[43] Jun Ma, Sheng Liu, Shengtao Yue, Xianping Tao, and Jian Lu. Leakdaf:
An automated tool for detecting leaked activities and fragments of
In 41st IEEE Annual Computer Software and
android applications.
Applications Conference, COMPSAC 2017, Turin, Italy, July 4-8, 2017.
Volume 1, pages 23–32. IEEE Computer Society, 2017.

[44] Tianyong Wu, Jierui Liu, Zhenbo Xu, Chaorong Guo, Yanli Zhang, Jun
Yan, and Jian Zhang. Light-weight, inter-procedural and callback-aware
resource leak detection for android apps. IEEE Trans. Software Eng.,
42(11):1054–1076, 2016.

[45] Josie Holmes, Iftekhar Ahmed, Caius Brindescu, Rahul Gopinath,
He Zhang, and Alex Groce. Using relative lines of code to guide auto-
mated test generation for Python. ACM Trans. Softw. Eng. Methodol.,
29(4):28:1–28:38, 2020.

13

