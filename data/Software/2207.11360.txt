Hardware-based Scheduler Implementation for
Dynamic Workloads on Heterogeneous SoCs

Alexander Fusco, Sahil Hassan, Joshua Mack, Ali Akoglu
Electrical and Computer Engineering, The University of Arizona
Tucson, AZ, USA
{afusco1,sahilhassan,jmack2545,akoglu}@email.arizona.edu

2
2
0
2

l
u
J

2
2

]

R
A
.
s
c
[

1
v
0
6
3
1
1
.
7
0
2
2
:
v
i
X
r
a

Abstract—Non-uniform performance and power consumption
across the processing elements (PEs) of heterogeneous SoCs
increase the computation complexity of the task scheduling
problem compared to homogeneous architectures. Latency of a
software-based scheduler with the increased heterogeneity level
in terms of number and types of PEs creates the necessity of
deploying a scheduler as an overlay processor in hardware to be
able to make scheduling decisions rapidly and enable deployment
of real-life applications on heterogeneous SoCs. In this study
we present the design trade-offs involved for implementing and
deploying the runtime variant of the heterogeneous earliest ﬁnish
time algorithm (HEFTRT) on the FPGA. We conduct performance
evaluations on a SoC conﬁguration emulated over the Xilinx Zynq
ZCU102 platform. In a runtime environment we demonstrate
hardware-based HEFTRT’s ability to make scheduling decisions
with 9.144 ns latency on average, process 26.7% more tasks
per second compared to its software counterpart, and reduce
the scheduling latency by up to a factor of 183× based on
workloads composed of mixture of dynamically arriving real-life
signal processing applications.

Index Terms—Scheduling, system on chip, FPGA, hardware

emulation, multiprocessor SoC.

I. INTRODUCTION

Heterogeneous system on chip (SoC) computing platforms
composed of a pool of general-purpose processors and spe-
cialized processors offer the ability to pair each execution
phase of a given application with a compute unit that matches
the processing needs of that phase. Hence, they offer a large
potential for performance and energy-efﬁciency relative to
homogeneous counterparts [1]. However, from task scheduling
point of view, heterogeneous SoCs pose additional challenges
as they provide a variety of compute resources giving tasks
varying execution times that create a larger scheduling search
space to explore. When building runtime systems for het-
to design schedulers that
erogeneous SoCs,
are capable of thoroughly exploring the space of possible
schedules while still returning effective scheduling decisions
within a reasonable amount of time. Effective utilization
of system resources while supporting execution of multiple

is difﬁcult

it

This material is based on research sponsored by Air Force Research Lab-
oratory (AFRL) and Defense Advanced Research Projects Agency (DARPA)
under agreement number FA8650-18-2-7860. The U.S. Government is au-
thorized to reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon. The views and conclusion
contained herein are those of the authors and should not be interpreted as
necessarily representing the ofﬁcial policies or endorsements, either expressed
or implied, of Air Force Research Laboratory (AFRL) and Defence Advanced
Research Projects Agency (DARPA) or the U.S. Government.

interleaving applications requires a dynamic scheduling policy
that considers task dependencies within an application and
demands on the same processing element (PE) by multiple
applications. When systems are highly heterogeneous in terms
of both the number and types of PEs, and when a runtime
system must deal with numerous applications co-existing on
the system at once, the latency of software-based schedulers
can become a major bottleneck to total system throughput.
As a result, there is a strong case to be made that scheduling
algorithms themselves should be subject to hardware-based
acceleration to enable utilization of heterogeneous SoC plat-
forms for practical deployment under scenarios where multiple
applications co-exist in domains such as autonomous vehicles
and communication systems that require low Size, Weight and
Power (SWaP) solutions.

In this study, we utilize the HEFTRT [2] algorithm, a
runtime variant of the Heterogeneous Earliest Finish Time
(HEFT) [3] scheduling heuristic, implement it in hardware
as an overlay processor for an SoC composed of ARM CPU
cores and FFT accelerator that is emulated on the Xilinx Zynq
ZCU102 board. We analyze the design trade-offs involved for
implementing and deploying the HEFTRT on the FPGA. We
perform this analysis with realistic hardware conﬁgurations
coupled with complex signal-processing workloads that give
conﬁdence that the results seen here will generalize across
other applications. We show that for a runtime system with
ready queue size of up to 5 tasks, the software based scheduler
should be preferred. Beyond this point we observe that the data
transfer overhead to and from the hardware-based scheduler
is compensated by the reduced scheduling latency. As the
ready queue size increases, the gap in scheduling overhead
between hardware-based HEFTRT and software-based HEFTRT
grows. Previous literature has found that hardware-accelerated
scheduling is beneﬁcial
in homogeneous systems. In this
study, we demonstrate the ability to accelerate a high quality,
heterogeneity-aware scheduling heuristic on hardware in a
manner that demonstrably reduces scheduling overhead and
consequently improves total system performance. We show
that hardware-based HEFTRT is capable of making up to
183× faster scheduling decisions across the ready queue sizes
used in our evaluations. Before presenting our approach to
implementing HEFTRT in hardware, we start by exploring
related work in the area of task scheduling with emphasis on
hardware based dynamic task scheduling.

 
 
 
 
 
 
II. RELATED WORK

Previous hardware schedulers have implemented deadline-
based scheduling policies [4]–[7], with most of them relying
on earliest deadline ﬁrst (EDF) for real time systems that
has been shown to minimize the maximum lateness and meet
deadlines when a feasible solution exists [8].

The study by Tang and Bergman [4] discuss types of task
queues and their beneﬁts across scheduling policies such as
EDF, deadline based least slack time, and multilevel feedback
queue using static priorities. Derafshi et al. [5] expands
upon Tang and Bergman’s study [4] by targeting a homoge-
neous multi-core processor, and moving more task dependency
management to hardware. The hardware scheduler by Gupta
et al. [6] also targets multi-core homogeneous processors,
and implements Pfair [9], a scheduling policy that uses the
expected execution time to estimate when a task will ﬁnish
in relation to its deadline. Koh´utka et al. [7] implement a
hardware scheduler that aims to improve on the EDF policy
by discarding tasks when the workload becomes unfeasible.

These hardware schedulers do not consider runtime con-
straints regarding how the task-to-PE-mapping decision for
a given application may affect the execution time of other
dynamically arriving applications, and they are limited to
targeting single-core [4], [7] or multi-core homogeneous ar-
chitectures [5], [6]. To the best of our knowledge, the only
previous work on hardware-based scheduler for heterogeneous
architectures is presented by Aliyev et al. [10], which im-
plements the Heterogeneous Earliest Finish Time [3]. This
implementation, unlike EDF based schedulers, is not suitable
for runtime execution as it requires parsing the full directed
acyclic graph (DAG) of an application, and can only perform
the
scheduling on one application at a time. Among all
hardware schedulers, the implementation by Derafshi et al. [5]
is the only one that integrates their scheduler into a runtime
environment, where they perform emulation based analysis
using artiﬁcial tasks. None of these studies present analysis on
the quality of schedules generated by the hardware scheduler
with dynamic workloads. We perform such an analysis by
implementing HEFTRT in hardware and integrating with a
runtime environment.

III. BACKGROUND

A. Runtime Environment

In this study, we utilize the Compiler Integrated Extensible
DSSoC Runtime (CEDR) ecosystem [11] as it enables compi-
lation and development of user applications, evaluation of re-
source management strategies, and validation of heterogeneous
hardware conﬁgurations in a uniﬁed framework. This runtime
system operates as a background Daemon Process in the Linux
user space and allows end users to interact with the hetero-
geneous architecture by launching a workload composed of
any number and combination of distinct applications with user
speciﬁed arrival rates. CEDR launches worker threads, each
tasked with managing a particular PE on the heterogeneous
system in terms of sending data to its respective PE, executing

its assigned tasks, and receiving their output. Meanwhile, the
CEDR management thread parses the incoming application
DAGs, monitors the state of tasks executing on each PE,
maintains a ready queue with tasks whose dependencies are
resolved, and estimates when a PE will become available. A
mapping event occurs when a new task arrives in the ready
queue, which invokes the scheduler. The tasks that are in the
ready queue and the estimated availability times of the PEs
are then passed to the ”Scheduler” at each mapping event.

B. Runtime Variant: HEFTRT

[3]

Heterogeneous Earliest Finish Time (HEFT)

is a
scheduling heuristic that balances runtime complexity and
quality of generated schedules, and has been shown to gen-
erate competitive schedules to this date [12]. List-scheduling
algorithms, such as HEFT, are typically used as purely static
schedulers as they operate on DAGs with known task depen-
dencies and application structures. The study by Mack et al. [2]
introduces a methodology that lifts the barriers of list sched-
ulers from being deployed in a runtime environment through
a series of transformations. They apply this methodology and
develop the runtime variant HEFTRT. At each mapping event,
HEFTRT parses the ready queue, PE state, earliest availability
time collected from the runtime. It sorts all tasks in the ready
queue using a separate priority queue in order of highest
average runtime across all PEs. As tasks are dequeued in
priority order one by one, for each task, the PE that delivers
the earliest ﬁnish time is selected. This selection along with
the updated availability time for that selected PE are ﬁnally
sent to the runtime system. In the following section we discuss
the proposed hardware implementation of this ﬂow.

IV. HEFTRT HARDWARE IMPLEMENTATION

A. AXI Stream Queuing Interface

The implementation of the HEFTRT algorithm is illustrated
in Figure 1. For ﬁgure clarity, the deﬁnitions and expressions
for related parameters are listed in Table I. When interfacing
the hardware based HEFTRT implementation with the runtime
environment and the processing elements of the emulated SoC,
we use direct memory access (DMA) blocks to move data
between the host CPU and the processing elements via the
AXI4-Stream protocol. At each mapping event, the CEDR
sends the updated availability times for each PE (Tavail)
to the scheduler, which are forwarded to its designated PE
Handler. This synchronizes the availability times with the
runtime environment before processing the ready queue. For
each task in the ready queue, the runtime environment feeds
the task information that includes a unique task identiﬁer
(T ID), average execution time across the PEs (AvgT ID), and
execution time on each PE (ExecT ID[P Ei]), to the scheduler
through an AXI-Stream (AXI-S). The task is assigned a
counter-based queue identiﬁer (QID) ranging from 0 to the
depth of the priority queue (D). The mapping of T ID to
QID ensures that each task is identiﬁed within a ﬁxed range.
The QID and AvgT ID are injected into the priority queue.
While the T ID is written into the BRAM, the execution

Fig. 1: Architecture of hardware scheduler.

TABLE I: Hardware Scheduler Parameters

Parameter
P
D
P Ei
T ID
QID
AvgT ID
ExecT ID[P Ei]
Tavail
Tf inish
W (AvgT ID)
W (QID)

Meaning
Number of PEs
Depth of Priority Queue
Identiﬁer of a PE where i ∈ [0, P )
Task identiﬁer
Queue identiﬁer
Average execution time of task across all PEs
Task execution time for a given PE
Tavail[P Ei]
Tf inish[P Ei]
Bit width of AvgT ID
Bit width of QID, which is (cid:100)log2D(cid:101)

times (ExecT ID[P Ei]) are written into the LUT-RAM, both at
addresses speciﬁed by the QID. It takes one cycle to insert the
task information into the Priority Queue and memory blocks
once they are received from the runtime.

B. Priority Queue

At each mapping event, the hardware scheduler sorts the
tasks that are in the Priority Queue. Each cell in the queue, as
illustrated in Figure 2, includes the task information in terms of
QID and AvgT ID, where average execution time represents
the relative importance of each task.

The Priority Queue is implemented as a shift register
priority queue that has also been used in EDF based hardware
schedulers [4], [7]. Each cell in the queue swaps its task infor-
mation with its neighbor when the condition of the comparator
logic is satisﬁed. The comparison operations are carried out
alternately among the odd and even indexed cells following
the odd-even sorting transposition algorithm [13]. Therefore it
takes two clock cycles to complete every comparison. When
there are two cycles in a row without any swaps, the sorting
process is terminated. Then the queue switches to right-shift
register mode and starts dequeuing the cell entries until all
entries have been read. In each clock cycle, the output QID
is used as an index to read out execution times for each PE
from the LUT-RAM to be forwarded to their corresponding
PE Handler along with the T ID to be forwarded to the AXI-
S Scheduling Interface.

C. Processing Element Handler

For each PE in the target hardware conﬁguration there is a
designated PE Handler in the hardware scheduler. It receives
the Tavail from the AXI-S and stores this information in the

Fig. 2: Architecture of shift register priority queue.

availability time register of the PE Handler. The output is the
ﬁnish time (Tf inish), which is the sum of ExecT ID[P Ei] and
Tavail[P Ei]. Finish time represents the estimated time when
that task will complete its execution on that PE. This ﬁnish
time is written into the availability time register only if that
PE is selected by the EFT Selector.

The Earliest Finish Time (EFT) Selector receives the ﬁnish
time for each PE as an input, and outputs an index correspond-
ing to the index of the PE (P Ei) with the lowest ﬁnish time. A
comparator tree is utilized to implement the selection process.
The EFT Selector feeds the index to each of the PE Handlers.
The handler of the selected PE then makes the calculated ﬁnish
time the new availability time of that PE. During this update,
the T ID and the P Ei are sent to the runtime through the
AXI-S Scheduling Interface.

V. EXPERIMENTAL SETUP

All experiments are performed on the Xilinx Zynq Ul-
trascale+ ZCU102 MPSoC development board. This MPSoC
combines general purpose CPUs (4x Arm Cortex A53 proces-
sors) with programmable FPGA fabric. On the FPGA fabric,
we add the Xilinx FFT IP accelerator for Fast Fourier Trans-
form (FFT). We use four representative real-world applications
part of the CEDR release [11] that are Radar Correlator (RC),
Temporal Interference Mitigation (TM), Pulse Doppler (PD),
and WiFi TX (TX). Based on average observed latencies,
RC and TM are low latency applications, whereas relatively
TX and PD are high latency applications. We refer to the
”Frame” as the input data size needed by a single instance
of each of the applications. The low workload consists of
twenty Frames for RC and TM for a 1280Kb per frame. We
compose a high latency workload consisting of ten instances
of PD and TX applications each, with a Frame size of 1037
Kb per frame. We use the low-latency workload for verifying
the functionality of the hardware HEFTRT with respect to its
software counterpart, as low latency workloads will not stress
the scheduler and in turn trends will not be affected by the
hardware based scheduler making faster scheduling decisions.
The high latency workloads on the other hand will allow us
to compare the performance of the two schedulers when the
system becomes oversubscribed. We evaluate the performance
of each scheduler in this study by stressing the scheduler’s
ability to cope with the volume of incoming jobs. We establish
this by setting a ﬁxed ”target” frame rate that deﬁnes the rate of
application instances arriving in the runtime per second. Then,
we monitor if the scheduler can complete jobs at that same
rate or if system performance begins to suffer. We refer to the

Tavail[PEi]Runtime EnvironmentAXI-S QueuingAXI-S SchedulingPriority QueueEFT SelectorAssign IDTIDTfinish[PE2]Tfinish[PE1]PEiExecTID[PEi]AvgTIDExecTID[PEi]QIDQIDTfinish[PE3]Tfinish[PE0]TIDTIDWenDEC0123PE HandlerPE0PE HandlerPE1PE HandlerPE2PE HandlerPE3LUT-RAMwr_addrwr_datard_addrrd_dataBRAMwr_datawr_addrrd_addrrd_dataPriority Queue (D = 4)Cell 1Cell 0AvgTIDQIDQIDAvgTIDCell 3Cell 2AvgTIDQIDQIDAvgTIDAvgTIDQIDQIDAvgTIDDequeue QIDEnqueue QIDAvgTIDA > B ? 1 : 0ABAA > B ? 1 : 0ABA > B ? 1 : 0BTABLE II: Resource usage on Zynq ZCU102 FPGA along with the
total including peripheral resources shown in Figure 1.

Module
Priority Queue
PE Handlers
EFT Selector
Total

Logic LUTs

18,632
404
48
19,603

6.8%
1.5%
0.02%
7.2%

LUT-RAM
0%
0
0%
0
0%
0
0.9% 14,534
1,280

13,433
128
0

Registers

BRAM

0
2.5%
0
0.5%
0%
0
2.7% 0.5

0%
0%
0%
0.05%

empirical frame rate observed as the ”achieved” frame rate,
which indicates the number of applications that the runtime
can process per second. For this, we use 29 different injection
rates, where the injection rate deﬁnes the amount of input data
arriving into the runtime per unit time.

task and the start of the ﬁrst

We use three metrics for performance evaluation. The
cumulative execution time of an application is the sum of
execution times of its individual tasks, ignoring the overhead
associated with scheduling them. Lower cumulative execu-
tion time indicates better scheduling decisions made and
better exploitation of the available heterogeneity at hand.
The scheduling overhead metric captures the time spent by
the runtime in making scheduling decisions. The application
execution time is the difference between the end of the
last
task of an application,
including the overhead of all scheduling decisions in between.
Lower execution times indicate the scheduler’s capability to
manage the workload efﬁciently. To make the cumulative
execution times and application execution times comparable
across different runtime conﬁgurations, we normalize them
with the number of applications (per application). We sweep
all input conﬁgurations, repeat each experiment twenty ﬁve
times, and collect performance metrics listed in the table by
averaging across the twenty ﬁve runs. We then average these
three metrics across twenty ﬁve repetitions.

VI. RESULTS AND ANALYSIS

A. Hardware Analysis

Table II shows resource usage of the HEFTRT scheduler
on the Zynq ZCU102 for a conﬁguration where the system is
composed of 4 PEs, priority queue depth is 512, and bit-width
for representing average execution time (W (AvgT ID)) is set
to 16. We choose 16 bits over 32 bits, since this bit-width
provides sufﬁcient dynamic range and reduces the amount of
LUTs used by the datapath. The design uses 7.15% of the
available LUTs and the majority of the LUTs and registers
in the scheduler are consumed by the Priority Queue. The
amount of LUT-RAM reserved for storing the execution time
information of each DAG node for each PE is a product of
the number of PEs and the depth of the priority queue, since
it stores the execution time for each task on each PE when
the priority queue is full.

The Priority Queue allows insertion and removal of tasks
through the front of the queue, which prevents the depth of the
queue from impacting the number of cycles needed to make
a scheduling decision. Sorting the queue in odd-even phases
allows each cell to only depend on its immediate neighbor,
which makes the path delay independent of the queue depth.
Therefore, the odd-even transposition based sorting prevents

TABLE III: Resource usage and path delay comparison where
HEFTRT1 is based on P = 16, D = 132, W (AvgT ID) = 16, and
HEFTRT2 is based on P = 4, D = 64, W (AvgT ID) = 32. [5] is
based on 16 PEs with a total queue length of 132. [4] is based on 1
PE with a priority queue length of 64 and a priority bit-width of 32.

HEFTRT1

[5]

HEFTRT2

[4]

LUTs

LUT-RAM

Register

Path Delay (ns)

7,598

1,920

6,430

5.91

4,834

N/A

3,968

5.001

4,360

160

3,590

3.035

2,170

N/A

1,158

6.122

the queue depth from impacting the critical path. Due to the
minimum tree structure of the EFT Selector, the number of PEs
impacts the critical path delay, while the queue depth and bit
width parameters affect the LUT and register usage. The EFT
Selector and PE Handlers take relatively fewer resources and
the PE Handler is on the critical path due to the feedback loop
that goes through the adder within the PE Handler, comparator
and multiplexers in the EFT Selector, decoder, and ﬁnally back
to the PE Handler. Since the EFT Selector inputs are fed
through a comparison tree driven by the number of PEs, the
EFT Selector contributes to the path delay but its contribution
to critical path scales logarithmically with the PE count.

In Table III, we show resource usage and path delay reported
by the EDF based schedulers ( [4], [5]) that are implemented
with priority queue lengths of 132 and 64 respectively. We
synthesize two versions of HEFTRT to match the two priority
queue lengths and include in the table as HEFTRT1 and
HEFTRT2. Since bit-width of 32 is used by [4] for priority, we
set W (AvgT ID) = 32 for HEFTRT2. The other implementa-
tions synthesize schedulers with search [4] and heap-based [5]
insertion queues that require less comparison logic compared
to our shift register based queue implementation that on the
other hand offers ability to insert tasks at every cycle.

The scheduler is capable of processing a ready queue of
size n in 3n + 3 cycles, with a turnaround of 2n + 3 cycles
for the task to PE mapping decision of the ﬁrst entry in the
sorted Priority Queue. The Priority Queue can be ﬁlled at a
rate of one task per cycle, meaning it will take n cycles to
ﬁll the priority queue with the tasks that are received from the
ready queue. The odd-even sorting implemented by the shift
register queue takes n cycles to sort for the worst case [13].
Additionally, both sorting phases will need to happen with
no shifts for the queue to be marked as sorted, resulting with
additional two cycles. The priority queue can shift out tasks at
a rate of one cycle per task while reading the execution times
from the LUT-RAM. After dequeuing and reading from LUT-
RAM, it takes the PE Handlers and EFT Selector one cycle
to make the PE mapping decision. The time to ﬁll the queue,
complete sorting, and select the PE accumulates to 2n + 3
cycles to generate the ﬁrst scheduling decision. Shifting out
and mapping the tasks at a rate of one task per cycle adds
another n cycles, meaning the scheduler will take at most
3n + 3 cycles to map a ready queue of size n to each of the
PEs. The average number of cycles it takes to make a single
task to PE mapping decision is then 3n+3
n . Given that latency

TABLE IV: Hardware scheduler resource usage and path delay with
respect to P and D on Zynq ZCU102 FPGA.

P

4
4
4
4
8
16

D

64
128
256
512
512
512

Logic LUTs

LUT-RAM

Registers

2,817
5,190
9,857
19,603
20,471
22,038

160
1.03%
320
1.89%
3.60%
640
7.15% 1,280
7.47% 2,560
8.04% 3,200

2,520
0.11%
4,159
0.22%
0.44%
7,543
0.89% 14,534
1.78% 15,243
2.22% 16,422

0.46%
0.76%
1.38%
2.65%
2.78%
3.00%

Block
RAM
0.5
0.5
0.5
0.5
0.5
3.5

Critical
Path
3.06
3.029
2.976
3.048
4.637
6.875

Fig. 4: Ready queue size versus scheduling overhead for
HEFTRT and HEFTRT HW

Fig. 3: Minimum cumulative execution time of tasks for low
latency workload versus injection rate.

for the design with Priority Queue length of 512 over 4 PE
conﬁguration is 3.048 ns, average time to make a single task
to PE mapping decision is 9.144 ns excluding the data transfer
overhead to the Priority Queue of the scheduler.

In Table IV we ﬁrst ﬁx the number of PEs (P ) on the SoC
to 4 and show the hardware scheduler resource usage and path
delay with respect to increase in Priority Queue length (D)
from 64 to 512. In the same table, we ﬁx D at 512 and increase
P from 4 to 8 and 16. Increasing the D does not affect the
path delay conﬁrming our earlier complexity analysis, and as
expected LUT and LUT-RAM usage increases proportionally
with D. As expected path delay grows less than linearly as
we increase P from 4 to 8 and 16 due to the minimum
tree structure of the EFT Selector unit. The EDF hardware
scheduler [5] that uses 16 PEs with a queue length of 134 has
a critical path delay of 6.122 ns, while our design with 16 PEs
and 512 queue length has a critical path delay of 6.875 ns.

B. Functional Veriﬁcation

Figure 3 shows cumulative execution time between software
and hardware versions of the HEFTRT scheduler for low-
latency workload with respect to change in job injection rate.
We choose low-latency workloads for this scenario due to the
fact that, as noted in Section V, they do not heavily stress
the scheduler. The cumulative execution time is independent
from scheduling overhead, and is completely dependent on
task execution time. If the hardware scheduler were making
to see
different PE mapping decisions, we would expect
a difference in the cumulative execution time, however the
average difference across injection rates between the two
schedulers is only 0.024 ms which corresponds to 0.32%
difference on average between pairwise points on the plot.
This negligible difference conﬁrms that the two schedulers
are making similar task to PE mapping decisions across all
injection rates.

Fig. 5: Average execution time vs. injection rate for HEFTRT
and HEFTRT HW schedulers over high-latency workload.

C. Performance Analysis

In Figure 4 we show the average scheduling overhead
for software (HEFTRT) and hardware (HEFTRT HW) based
implementations with respect to increase in the ready queue
size of the CEDR. We measure the scheduling overhead by
recording the start and completion times of each mapping
event. The rate of increase in scheduling overhead as the
queue size grows is higher with the HEFTRT compared to the
HEFTRT HW. This is expected based on the 3n + 3 cycle count
analysis presented earlier, that offers O(n) complexity, while
the software implementation relies on sorting with O(n log2 n)
complexity. However, we note that in the plot we observe
some outlier points that are not following the expected trend
line for the HEFTRT HW primarily caused by the data transfer
overhead on the Zynq ZCU102. In the same plot we show
the ready queue size in the range of 1 to 10 in magniﬁed
form to illustrate the crossover point where HEFTRT offers less
overhead for queue size of up to 5. Beyond this point, data
transfer overhead to the HEFTRT HW is compensated by its
ability to make faster scheduling decisions. The gap between
the two schedulers grow as the queue size increases, where we
observe a speed up of 2.6× at queue size of 1330. Without the
data transfer overhead, in terms of time to make scheduling
decisions HEFTRT HW achieves a speedup of 183×.

Figure 5 shows the average application execution time per
application along the y-axis, with respect to varying injection
rates along the x-axis, for both HEFTRT and HEFTRT HW
schedulers. Here, we observe that for lower injection rates
(< 250 Mbps),
the application execution times for both
schedulers increase linearly with increasing injection rates.
This is expected as increasing injection rate grows the number

0200400600800Injection Rate (Mbps)1.401.451.501.551.601.651.701.751.80Avg. cumulativeexecution time / app. (ms)HEFTRTHEFTRT_HW020040060080010001200Ready queue size0.00.51.01.52.02.5Average scheduling overhead (ms)0246810120.0020.0040.0060.0080.010HEFTRTHEFTRT_HW025050075010001250150017502000Injection Rate (Mbps)20406080100120140Avg. execution time / app.(ms)HEFTRTHEFTRT_HWVII. CONCLUSION

In this work, we explore the design, implementation, and
evaluation of the HEFTRT [2] in hardware to accelerate
the scheduling of dynamically interleaved applications across
heterogeneous SoCs. We implement HEFTRT as an overlay
processor on the Zynq Ultrascale+ ZCU102 FPGA and inte-
grated it into an open source runtime system (CEDR [11]).
We ﬁnd that, for an SoC composed of 3 ARM cores and 1
FFT accelerator emulated on the same ZCU102 FPGA, we
are able to generate competitive schedules with a 9.144 ns
average latency for making a task-to-PE mapping decision.
We present analysis of this hardware design by exploring the
impact of queue size and number of processing elements on
the latency and resource usage, and we ﬁnd that our design
is overall quite scalable to different system conﬁgurations.
We then stress the scheduler by evaluating it in our runtime
environment with realistic workloads, and we ﬁnd that it can
simultaneously produce high quality scheduling decisions and
maintain correct application execution. As future work, we will
explore acceleration of energy-aware scheduling heuristics in
order to expand our evaluations beyond focusing purely on
optimization of execution time.

REFERENCES

[1] J. Cong, V. Sarkar, G. Reinman, and A. Bui, “Customizable Domain-
Speciﬁc Computing,” IEEE Design & Test of Comput., vol. 28, no. 2,
pp. 6–15, 2011.

[2] J. Mack, S. E. Arda, U. Y. Ogras, and A. Akoglu, “Performant, Multi-
Objective Scheduling of Highly Interleaved Task Graphs on Heteroge-
neous System on Chip Devices,” IEEE Trans. on Parallel and Distrib.
Syst., vol. 33, no. 9, pp. 2148–2162, 2022.

[3] H. Topcuoglu and S. Hariri, “Performance-effective and low-complexity
task scheduling for heterogeneous computing,” IEEE Trans. on Parallel
and Distrib. Syst., vol. 13, no. 3, pp. 260–274, 2002.

[4] Y. Tang and N. W. Bergmann, “A Hardware Scheduler Based on Task
Queues for FPGA-Based Embedded Real-Time Systems,” IEEE Trans.
on Comput., vol. 64, no. 5, pp. 1254–1267, 2015.

[5] D. Derafshi, A. Norollah, M. Khosroanjam, and H. Beitollahi, “HRHS:
A High-Performance Real-Time Hardware Scheduler,” IEEE Trans.
Parallel Distrib. Syst., vol. 31, no. 4, pp. 897–908, 2020.

[6] N. Gupta, S. K. Mandal, J. Malave, A. Mandal, and R. N. Mahapatra,
“A Hardware Scheduler for Real Time Multiprocessor System on Chip,”
in 2010 23rd Int. Conf. on VLSI Design, 2010, pp. 264–269.

[7] L. Koh´utka and V. Stopjakov´a, “ASIC Architecture and Implementation
of RED Scheduler for Mixed-Criticality Real-Time Systems,” in 2020
27th Int. Conf. on Mixed Design of Integr. Circuits and Syst. (MIXDES),
2020, pp. 83–88.

[8] G. C. Buttazzo, Hard Real-Time Computing Systems: Predictable
Scheduling Algorithms and Applications. Springer Science & Business
Media, 2011, vol. 24.

[9] J. Goossens, S. Funk, and S. Baruah, “Priority-Driven Scheduling of
Periodic Task Systems on Multiprocessors,” Real-Time Syst., vol. 25,
no. 2, pp. 187–205, 2003.

[10] I. Aliyev, J. Mack, N. Kumbhare, A. Akoglu, and H. F. Ugurdag,
“FPGA-based Minimal Latency HEFT Scheduler for Heterogeneous
Computing,” in 2021 6th Int. Conf. on Comput. Sci. and Eng. (UBMK),
2021, pp. 1–5.

[11] J. Mack, S. Hassan, N. Kumbhare, M. Gonzales, and A. Akoglu, “CEDR
- A Compiler-integrated, Extensible DSSoC Runtime,” ACM Trans. on
Embedded Comput. Syst., vol. 33, no. 9, pp. 2148–2162, 2022.
[12] A. K. Maurya and A. K. Tripathi, “On benchmarking task scheduling
algorithms for heterogeneous computing systems,” The J. of Supercom-
puting, vol. 74, no. 7, pp. 3039–3070, 2018.

[13] D. Bitton, D. J. DeWitt, D. K. Hsaio, and J. Menon, “A Taxonomy of
Parallel Sorting,” ACM Comput. Surv., vol. 16, no. 3, p. 287–318, 1984.

Fig. 6: Target frame rate vs. average achieved frame rate for
HEFTRT and HEFTRT HW over high-latency workload.

of concurrent jobs in the runtime, and with these jobs sharing
the same set of PEs, their execution times see a linear growth.
The reduction in execution time per application with the
hardware scheduler becomes more dominant as the injection
rate increases. As the injection rate goes beyond a certain level
(> 250 Mbps), the execution time per application starts to
saturate on average, at 131.37 ms and 89.79 ms for HEFTRT
and HEFTRT HW respectively. The saturation is caused by
the runtime reaching to an oversubscribed state, where it is
processing tasks on PEs at the maximum possible rate, and no
further improvement is attainable. Execution time comparison
between both schedulers across all injection rates show that
with the HEFTRT HW this time is consistently lower than
HEFTRT and results with a 31.7% reduction on average in
the saturated region. The improvement in execution time is
achieved due to the ability of HEFTRT HW in making faster
scheduling decisions, which enhances the runtime’s capability
to cope with scenarios where larger number of applications
may arrive. We further discuss this frame processing capability
using Figure 6, where we present the trend in average achieved
frame rate on y-axis with respect to target frame rate on x-axis
for high latency workload.

In Figure 6, for lower values of target frame rates (<250
frames/second), both the HEFTRT and HEFTRT HW schedulers
show similar linear trend of growth in average achieved
frame rate, as the target frame rate increases. For higher
values of target frame rates (>250 frames/second), the average
achieved frame rate saturates for both the schedulers, which
indicates that
the runtime has become oversubscribed and
system resources are fully occupied. The saturation point
on the achieved frame rate with HEFTRT HW scheduler is at
204.62 frames/second (on average), while HEFTRT saturates
at 161.51 frames/second on average. We notice that with
the runtime is able to achieve
the HEFTRT HW scheduler,
saturation at around 43.11 frames/second higher frame rate
on average compared to the HEFTRT scheduler. The ability of
the hardware scheduler to make task-to-PE mapping decisions
faster, provides the runtime with a better capability to process
frequently arriving jobs at a higher rate in the oversubscribed
state and in turn results with 26.7% more frames/second in
average achieved frame rate.

05001000150020002500300035004000Target Frame Rate (# of frames/sec)255075100125150175200225Avg. achieved Frame Rate (# of frames/sec)HEFTRTHEFTRT_HW