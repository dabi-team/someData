On Non-Negative Quadratic Programming
in Geometric Optimization∗

Siu-Wing Cheng†

Man Ting Wong†

Abstract

We present experimental and theoretical results on a method that applies a numerical solver
iteratively to solve several non-negative quadratic programming problems in geometric opti-
mization. The method gains eﬃciency by exploiting the potential sparsity of the intermediate
solutions. We implemented the method to call quadprog of MATLAB iteratively.
In com-
parison with a single call of quadprog, we obtain a 10-fold speedup on two proximity graph
problems in Rd on some public data sets, a 10-fold speedup on the minimum enclosing ball
problem on random points in a unit cube in Rd, and a 5-fold speedup on the polytope distance
problem on random points from a cube in Rd when the input size is signiﬁcantly larger than
the dimension; we also obtain a 2-fold or more speedup on deblurring some gray-scale space and
thermal images via non-negative least square. We compare with two minimum enclosing ball
software by G¨artner and Fischer et al.; for 1000 nearly cospherical points or random points in
a unit cube, the iterative method overtakes the software by G¨artner at 20 dimensions and the
software by Fischer et al. at 170 dimensions. In the image deblurring experiments, the itera-
tive method compares favorably with other software that can solve non-negative least square,
including FISTA with backtracking, SBB, FNNLS, and lsqnonneg of MATLAB. We analyze
theoretically the number of iterations taken by the iterative scheme to reduce the gap between
the current solution value and the optimum by a factor e. Under certain assumptions, we prove
a bound proportional to the square root of the number of variables.

2
2
0
2

l
u
J

6
1

]

G
C
.
s
c
[

1
v
9
3
8
7
0
.
7
0
2
2
:
v
i
X
r
a

∗Research supported by Research Grants Council, Hong Kong, China (project no. 16203718).
†Department of Computer Science and Engineering, HKUST, Hong Kong.

Email:

scheng@cse.ust.hk,

mtwongaf@connect.ust.hk

 
 
 
 
 
 
1

Introduction

We study an iterative method to apply a numerical solver to solve non-negative convex quadratic
programming problems (NNQ problems): min xtAtAx + atx subject to Bx ≥ b, Cx = c, x ≥ 0ν.
The method gains eﬃciency by exploiting the potential sparsity of the intermediate solutions. We
study its performance both theoretically and experimentally on several NNQ problems that arise
in geometric optimization.

The ﬁrst problem is ﬁtting a proximity graph to data points in Rd, which ﬁnds applications
in classiﬁcation, regression, and clustering [19, 27, 28, 42]. Daitch et al. [19] deﬁned a proximity
graph via solving an NNQ problem. The unknown is a vector x ∈ Rn(n−1)/2, specifying the edge
weights in the complete graph on the input points p1, . . . , pn. Let (x)i(cid:77)j denote the coordinate of
x that stores the weight of the edge pipj. Edge weights must be non-negative; each point must
have a total weight of at least 1 over its incident edges; these constraints can be modelled as
Bx ≥ 1n and x ≥ 0n(n−1)/2, where B ∈ Rn×n(n−1)/2 is the incidence matrix for the complete graph,
and 1n is the n-dimensional vector with all coordinates equal to 1. The objective is to minimize
j∈[n]\{i}(x)i(cid:77)j(pi −pj)(cid:13)
(cid:80)n
2, which can be written as xtAtAx for some matrix A ∈ Rdn×n(n−1)/2.
(cid:13)

(cid:80)

i=1

(cid:13)
(cid:13)

We call this the DKSG problem. The one-dimensional case can be solved in O(n2) time [16].

2 (cid:107)Ux − 1n(cid:107)2 + ρ

We also study another proximity graph in Rd deﬁned by Zhang et al. [42] via minimizing
d btx + µ
2 (cid:107)x(cid:107)2 for some constants µ, ρ ≥ 0. The vector x ∈ Rn(n−1)/2 stores the
1
unknown edge weights. For all distinct i, j ∈ [n], the coordinate (b)i(cid:77)j of b is equal to (cid:107)pi − pj(cid:107)2.
The matrix U is the incidence matrix for the complete graph. The objective function can be
written as xtAtAx + atx, where At = (cid:2) µ
d b − µUt1n. The only constraints are
x ≥ 0n(n−1)/2. We call this the ZHLG problem.

(cid:3) and a = 1

2 In(n−1)/2

2 Ut ρ

The third problem is ﬁnding the minimum enclosing ball (MEB) of n points in Rd [21, 23, 39].
It has connections to clustering [10, 14], computation of spatial hierarchies [26], and support vector
machine research [12, 37, 38]. The approximate MEB problem is related to the study of coresets [13,
17, 30, 41]. Let p1, . . . , pn be the input points. In the unknown vector x ∈ Rn, the coordinate (x)i
stores the weight associated with pi in expressing the MEB center as a convex combination of the
input points. The constraints are 1t
nx = 1 and x ≥ 0n. The objective is to minimize xtAtAx + atx,
where A = [ p1 . . . pn ] and a = [ −(cid:107)p1(cid:107)2 . . . − (cid:107)pn(cid:107)2 ]t.

The fourth problem is to deblur some mildly sparse gray-scale images [25, 31]. The pixels in an
image form a vector x of gray scale values in Rd; the blurring can be modelled by the action of a
matrix A ∈ Rd×d that depends on the particular point spread function adopted [31]; and Ax is the
observed blurred image. Working backward, given the matrix A and a blurred image b, we recover
the image by ﬁnding the unknown x ≥ 0d that minimizes (cid:107)Ax − b(cid:107)2, which is a non-negative least
square problem (NNLS) [29, 32]. The problem is often ill-posed, i.e., A is not invertible.

The ﬁfth problem is to ﬁnd the distance between the convex hulls of two point sets in Rd [24, 36,
40]. Wolfe [40] reported that the problem was encountered in the optimization of non-diﬀerentiable
functions, approximation theory, and pattern recognition. We call it the PD problem. Due to space
limitation, we defer the experimental results on the PD problem to Appendix D.4.

The method that we study selects a subset of variables as free variables—the non-free variables
are ﬁxed at zero—and calls the solver to solve a constrained NNQ in each iteration. It is reminiscent
of the active-set method (e.g. [15, 18]) which determines a descent direction and an appropriate
step size in that descent direction in each iteration. In contrast, our main task is selecting free
variables; the descent step is carried out by calling a solver. A version of this method was used
in [19] for solving the DKSG problem. However, it is only brieﬂy mentioned that a small number
of the most negative gradient coordinates are selected as free variables; no further details are given;
there is no analysis of the convergence. We ﬁnd the selection of free variables crucial. The right

1

answer is not some constant number of variables or a constant fraction of them. Also, one cannot
always turn free variables that happen to be zero in an iteration into non-free variables in the next
iteration as suggested in [19]; otherwise, the algorithm may not terminate in some cases.

We implemented the method to call quadprog of MATLAB iteratively. In comparison with a
single call of quadprog, we obtain a 10-fold speedup on DKSG and ZHLG on some public data sets,
a 10-fold speedup on MEB on random points in a unit cube, and a 5-fold speedup on PD on random
points from a cube when the input size is signiﬁcantly larger than the dimension; we also obtain
a 2-fold or more speedup on deblurring some gray-scale space and thermal images via NNLS. We
compare with two MEB software by G¨artner [22] and Fischer et al. [20]; for 1000 nearly cospherical
points or random points in a unit cube, the iterative method overtakes the software by G¨artner
at 20 dimensions and the software by Fischer et al. at 170 dimensions. In the image deblurring
experiments, the iterative method compares favorably with other software that can solve NNLS,
including FISTA with backtracking [5, 11], SBB [29], FNNLS [32], and lsqnonneg of MATLAB.
We emphasize that we do not claim a solution for image deblurring as there are many issues that
we do not address; we only seek to demonstrate the potential of the iterative scheme.

We use f : Rν → R to denote the objective function of the NNQ problem at hand. A unit
direction n ∈ Rν is a descent direction from a feasible solution x if (cid:104)∇f (x), n(cid:105) < 0 and x + sn
lies in the feasible region for some s > 0. We oﬀer a partial theoretical analysis. Let xr be the
solution produced in the (r − 1)-th iteration. Let x∗ be the optimal solution. We prove that one
can ﬁnd a descent direction nr from xr, for which only a few variables need to be freed, such that
if the distance between xr and the minimum in direction nr is assumed to be at least 1
λ (cid:107)xr − x∗(cid:107)
for all r, where λ is some value at least 1, then f (xr+i) − f (x∗) ≤ e−1(f (xr) − f (x∗)) for some
i = O(λ) · square root of the number of variables. An intuitive interpretation is that even if the
descent direction is far from ideal as we free only a few variables for the sake of eﬃciency, as long as
we can move from the current solution by a sizable fraction of its distance from x∗, a geometric-like
convergence can be obtained. We checked this assumption in some of our experiments on DKSG
and ZHLG; it is almost always satisﬁed for λ = 10 by the descent directions taken. Due to space
limitation, the missing proofs can be found in the appendix.

We use uppercase and lowercase letters in typewriter font to denote matrices and vectors,
respectively. Given a vector x, we use (x)i to denote its i-th coordinate. The inner product of two
vectors x and y is written as (cid:104)x, y(cid:105) or xty. The span of a set of vectors W is the linear subspace
spanned by them, and we denote it by span(W ). In Rν, for i ∈ [ν], deﬁne the vector ei to have a
value 1 in the i-th coordinate and zero at other coordinates. Given a set of vectors {w1, w2, . . . , wν},
a conical combination of them is (cid:80)ν

i=1 ciwi for some non-negative real values c1, c2, . . . , cν.

2 Algorithm

Figure 1(a) shows the NNQ problem constrained by the active set Sr that stores the non-free
variables in the (r − 1)-th iteration. Assume that B ∈ Rκ×ν. Let xr be the optimal solution of this
constrained problem. Let u and ˜u be the vectors of Lagrange multipliers for the constraints Bx ≥ b
and Cx = c, respectively. The coordinates of u are required to be non-negative, whereas those of ˜u
are unrestricted. Let v ∈ Rν be the vector of Lagrange multipliers for the constraints x ≥ 0ν. For
each i ∈ [ν] \ Sr, we require (v)i ≥ 0; for i ∈ Sr, (v)i is unrestricted. The dual of the constrained
problem is shown in Figure 1(b), where g(u, ˜u, v, x) = f (x) + ut(b − Bx) + ˜ut(c − Cx) − vt x. In
the dual, x is unrestricted. Let (ur, ˜ur, vr, xr) be the optimal solution of g constrained by Sr. Let
supp(xr) = (cid:8)i ∈ [ν] : (xr)i (cid:54)= 0(cid:9). By optimality, the following KKT conditions must hold: (i) critical
point: ∂g
∂x (xr) = ∇f (xr) − Btur − Ct˜ur − vr = 0ν; (ii) primal feasibility: Bxr ≥ b, Cxr = c, (xr)i = 0

2

min

s.t.

f (x)

Bx ≥ b, Cx = c,
x ≥ 0ν, ∀ i ∈ Sr, (x)i = 0.
(a) Constrained NNQ

max
u,˜u,v
s.t.

min
x

g(u, ˜u, v, x)

u ≥ 0κ,
∀ i (cid:54)∈ Sr, (v)i ≥ 0.

(b) Dual of constrained NNQ

Figure 1: The constrained NNQ and its dual.

for i ∈ Sr, and xr ≥ 0ν; (iii) dual feasibility: ur ≥ 0κ, (vr)i ≥ 0 for i ∈ [ν] \ Sr; (iv) complementary
slackness: (ur)i · (b − Bxr)i = 0 for i ∈ [κ], and (vr)i · (xr)i = 0 for i ∈ [ν].

For eﬃciency, before computing xr in the previous iteration, we eliminated the variables cor-
responding to the coordinates of xr that belong to Sr. So the solver cannot return all Lagrange
multipliers encoded in vr; nevertheless, as long as the solver returns ur, ˜ur, and xr, we can compute
vr using the ﬁrst KKT condition. Thus, an optimal solution of the dual constrained by Sr is fully
characterized by (ur, ˜ur, xr). By strong duality, the optimum of g constrained by Sr is equal to the
optimum of f constrained by Sr. If xr is not an optimal solution, there must exist some j ∈ Sr such
that if j is removed from Sr, then (ur, ˜ur, xr) is non-optimal with respect to the updated active set.
Among the KKT conditions, only the dual feasibility can be violated by removing j from Sr; in case
of violation, we must have (vr)j < 0. For each i ∈ Sr, we check whether (vr)i < 0, or equivalently,
(∇f (xr) − Btur − Ct˜ur)i < 0. We select a subset of the negative coordinates of ∇f (xr) − Btur − Ct˜ur
to update the active set. SolveNNQ in Algorithm 1 describes the iterative method.

Algorithm 1 SolveNNQ

1: τ is an integer threshold much less than ν
2: β0 is a threshold on |Er| such that β0 > τ
3: β1 is a threshold on the number of iterations
4: compute an initial active set S1 and an optimal solution (u1, ˜u1, x1) of g constrained by S1
5: r ← 1
6: loop
7:

Er ← the sequence of indices i ∈ Sr such that (vr)i = (∇f (xr) − Btur − Ct˜ur)i < 0 and Er is
sorted in non-decreasing order of (vr)i
if Er = ∅ then
return xr

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

else

if |Er| < β0 or r > β1 then

Sr+1 ← Sr \ Er

else

r be subset of the ﬁrst τ indices in Er

let E(cid:48)
Sr+1 ← [ν] \ (cid:0)supp(xr) ∪ E(cid:48)

(cid:1)

r

end if

end if
(ur+1, ˜ur+1, xr+1) ← optimal solution of g constrained by the active set Sr+1
r ← r + 1

19:
20: end loop

In line 15 of SolveNNQ, we kick out all free variables that are zero in the current solution xr,
i.e., if i (cid:54)∈ Sr and i (cid:54)∈ supp(xr), move i to Sr+1. This step keeps the number of free variables small
so that the next call of the solver will run fast. When the computation is near its end, moving

3

a free variable wrongly to the active set can be costly; it is preferable to shrink the active set
to stay on course for the optimal solution x∗. Therefore, we have a threshold β0 on |Er| and set
Sr+1 := Sr \ Er in line 12 if |Er| < β0. The threshold β0 gives us control on the number of free
variables. In some rare cases when we are near x∗, the algorithm may alternate between excluding
a primal variable from the active set in one iteration and moving the same variable into the active
set in the next iteration. The algorithm may not terminate. Therefore, we have another threshold
β1 on the number of iterations beyond which the active set will be shrunk monotonically in line 12,
thereby guaranteeing convergence. We will see later that if β1 is not exceeded, the iterations of
lines 7–19 converge quickly to the optimum under some assumption. If β1 is exceeded, we can still
bound the number of remaining iterations by |supp(x∗)| (see Appendix A.1), but we will have no
control on the number of free variables. The threshold β1 is rarely exceeded in our experiments;
setting Sr+1 := Sr \ Er when |Er| < β0 also helps to keep the the algorithm from exceeding the
threshold β1. In our experiments, we set τ = Θ(log2 ν), β0 = Θ(τ ), and β1 to be a constant; more
details are given in Section 5.
Lemma 2.1. Let n ∈ Rν be any unit descent direction from xr. Let y = xr + αn for some α > 0
be the feasible point that minimizes f on the ray from xr in direction n. Then, (i) f (xr) − f (y) ≤
−(cid:107)xr − y(cid:107) · (cid:104)∇f (xr), n(cid:105), and (ii) f (xr) − f (y) ≥ − 1
Corollary 2.1. Let n ∈ Rν be any unit descent direction from xr. Let y = xr + αn for some α > 0
be the feasible point that minimizes f on the ray from xr in direction n. Let S be an active set that
is disjoint from supp(xr) ∪ supp(n). Let xr+1 be the optimal solution for f constrained by S. Then,
f (xr)−f (xr+1)
f (xr)−f (x∗) ≥ f (xr)−f (y)

f (xr)−f (x∗) ≥ (cid:107)xr−y(cid:107)

2 (cid:107)xr − y(cid:107) · (cid:104)∇f (xr), n(cid:105).

(cid:104)∇f (xr),n(cid:105)
(cid:104)∇f (xr),x∗−xr(cid:105) .

2

·

3 Convergence analysis for NNLS and ZHLG

Let L be an aﬃne subspace in Rν. For every x ∈ Rν, deﬁne x ↓ L to be the projection of x to the
linear subspace parallel to L—the translate of L that contains the origin. Note that x ↓ L may not
belong to L; it does if L is a linear subspace. One can verify that (−x) ↓ L = −(x ↓ L), so we will
write −x ↓ L without any bracket. In the pseudocode of SolveNNQ, Er is a sequence of indices i ∈ Sr
such that (vr)i < 0, arranged in non-decreasing order of (vr)i. Deﬁne Jr = span({ei : i ∈ Er}).
Lemma 3.1 below states a key result that the ﬁrst τ indices in Er induce a conical combination
that is a good descent direction. We give the analysis in Appendix B.
Lemma 3.1. The vectors in (cid:8)ei : i among the ﬁrst τ indices in Er
(cid:9) have a unit conical combi-
nation nr such that nr is a descent direction from xr and ∠(−vr ↓ Jr, nr) ≤ arccos(cid:0)(cid:112)α/(2 ln ν)(cid:1),
where α = min{1, τ /|Er|}.

We use Lemma 3.1 to establish a convergence rate under an assumption.

Lemma 3.2. Let nr be a unit descent direction from xr that satisﬁes Lemma 3.1. Let yr be the
feasible point that minimizes f on the ray from xr in direction nr. Let α = min{1, τ /|Er|}. If there
exists λ such that (cid:107)xr − yr(cid:107) ≥ 1

λ (cid:107)xr − x∗(cid:107), then f (xr+1)−f (x∗)

f (xr)−f (x∗) ≤ 1 −

√

.

1
2 ln ν/α

2λ

Irrespective of whether Sr+1 is computed in line 12 or 15 of SolveNNQ, Sr+1 is disjoint

Proof.
from supp(xr) ∪ supp(nr), which makes Corollary 2.1 applicable.

Since ∇f (xr) = vr for an NNLS or ZHLG problem, we have (cid:104)−∇f (xr), nr) = (cid:104)−vr, nr(cid:105) = (cid:104)−vr ↓
Jr, nr(cid:105). The last equality comes from the fact that nr ∈ Jr, so the component of −vr orthogonal
to Jr vanishes in the inner product. By Lemma 3.1, (cid:104)−∇f (xr), nr(cid:105) = (cid:104)−vr ↓ Jr, nr(cid:105) = (cid:107)vr ↓
Jr(cid:107) · cos ∠(−vr ↓ Jr, nr) ≥ (cid:107)vr ↓ Jr(cid:107) ·

1√

.

2 ln ν/α

4

The inequality above takes care of the nominator in the bound in Corollary 2.1 multiplied by −1.
The denominator in the bound in Corollary 2.1 multiplied by −1 is equal to (cid:104)−∇f (xr), x∗ − xr(cid:105) =
(cid:104)−vr, x∗ − xr(cid:105) = (cid:104)−vr ↓ Jr, x∗ − xr(cid:105) + (cid:104)−vr + vr ↓ Jr, x∗ − xr(cid:105).

Recall that (vr ↓ Jr)i is (vr)i for i ∈ Er and zero otherwise. Therefore, (−vr + vr ↓ Jr)i is zero for
i ∈ Er and −(vr)i otherwise. If i (cid:54)∈ Er, then (vr)i ≥ 0, which implies that (−vr+vr ↓ Jr)i = −(vr)i ≤
0. By the complementary slackness, if (vr)i > 0, then (xr)i = 0, which implies that (x∗ − xr)i ≥ 0
as x∗ is non-negative. Altogether, we conclude that for i ∈ [ν], if i ∈ Er or (i (cid:54)∈ Er ∧ (vr)i = 0),
then (−vr + vr ↓ Jr)i · (x∗ − xr)i = 0; otherwise, (−vr + vr ↓ Jr)i · (x∗ − xr)i ≤ 0. Therefore,
(cid:104)−vr +vr ↓ Jr, x∗−xr(cid:105) ≤ 0. As a result, (cid:104)−∇f (xr), x∗−xr(cid:105) ≤ (cid:104)−vr ↓ Jr, x∗−xr(cid:105) ≤ (cid:107)x∗−xr(cid:107)·(cid:107)vr ↓ Jr(cid:107).
.

Substituting the results in the above into Corollary 2.1 gives f (xr)−f (xr+1)

It then follows from the assumption of (cid:107)xr − yr(cid:107) ≥ 1
(cid:0)f (xr) − f (x∗)(cid:1) − (cid:0)f (xr) − f (xr+1)(cid:1) ≤ (cid:0)f (xr) − f (x∗)(cid:1) −

f (xr)−f (x∗) ≥ (cid:107)xr−yr(cid:107)
2(cid:107)x∗−xr(cid:107) ·
2 ln ν/α
λ (cid:107)xr − x∗(cid:107) in the lemma that f (xr+1) − f (x∗) =
(cid:0)f (xr) − f (x∗)(cid:1).

1√

√

1
2 ln ν/α

2λ

Let T (v) be the time complexity of solving a convex quadratic program with v variables, as-
suming that the number of constraints is O(v). It is known that T (v) = O(v3) [35]. Theorem 3.1
below follows from Lemma 3.2; the running time is analyzed in Appendix B.2.

Theorem 3.1. Consider the application of SolveNNQ on an NNLS problem with n constraints in
Rd or a ZHLG problem for n points in Rd. Let ν be n for NNLS or n(n − 1)/2 for ZHLG. The
initialization of SolveNNQ can be done in T (β0) + O(dn2) time for NNLS or T (β0) + O(n4) time
for ZHLG. Suppose that the threshold β1 on the total number of iterations is not exceeded, and
there exists λ such that (cid:107)xr − yr(cid:107) ≥ 1
λ (cid:107)xr − x∗(cid:107) for all r ≥ 1, where yr is the feasible point that
minimizes f on the ray from xr in the descent direction that satisﬁes Lemma 3.1. Then, for all
r ≥ 1, f (xr+i) − f (x∗) ≤ e−1(f (xr) − f (x∗)) for some i = O(cid:0)λ(cid:112)ν log ν/τ (cid:1), and each iteration in
SolveNNQ takes T (k + β0) + O(kν + β2

0) time, where k = maxr≥1 |supp(xr)|.
For ease of presentation, we assume in Theorem 3.1 that (cid:107)xr − yr(cid:107) ≥ 1

λ (cid:107)xr − x∗(cid:107) for all r ≥
1; nevertheless, the gap f (xr) − f (x∗) will still decrease by a factor e after O(λ(cid:112)ν log ν/τ ) not
necessarily consecutive iterations in the future that satisfy the assumption.

4 Convergence analysis for MEB, PD, and DKSG

We ﬁrst present some general results; they will be specialized to MEB, PD, and DKSG. Suppose
that the feasible region is described by a system of equality constraints Mx = m for some matrix
M ∈ Rκ×ν and vector m ∈ Rκ, in addition to the constraints x ≥ 0ν. We use (M)a,i to denote
the entry at the a-th row and i-th column. Recall that (˜ur, vr, xr) form the solution of the dual
problem constrained by the active set Sr, where ˜ur are the dual variables corresponding to the
constraints Mx = m. For a ∈ [κ], the a-th constraint is a hyperplane in Rν with [ (M)a,1 . . . (M)a,ν ]t
as a normal vector. As a result, for any direction k from the current solution xr that is parallel to
the hyperplanes encoded by Mx = m, the vector Mt˜ur is orthogonal to k, which gives

(cid:104)∇f (xr), k(cid:105) = (cid:104)∇f (xr) − Mt˜ur, k(cid:105) = (cid:104)vr, k(cid:105).

Without loss of generality, assume that (vr)1 = mini∈Er (vr)i. Deﬁne the following items:

Kr = {x ∈ Rν : ∀ i (cid:54)∈ Er ∪ supp(xr), (x)i = 0} ,

∀ a ∈ [κ], Ia = supp(xr) ∩ (cid:8)i : (M)a,i (cid:54)= 0(cid:9),

∀ a ∈ [κ], Ha =

(cid:110)

x ∈ Rν :

(cid:88)

(M)a,i(x)i = (m)a ∧ ∀ i (cid:54)∈ {1} ∪ supp(xr), (x)i = 0

(cid:111)
,

i∈{1}∪Ia

5

h =

(cid:92)

Ha,

a∈[κ]

∀ a ∈ [κ], ∀ i ∈ [ν], (ka)i =






(cid:113)(cid:80)

(M)a,i

j∈{1}∪Ia

if i ∈ {1} ∪ Ia,

(M)2
a,j

0

otherwise.

For a ∈ [κ], 1 (cid:54)∈ Ia as Ia ⊆ supp(xr) and 1 ∈ Er. For a ∈ [κ], Ha is an aﬃne subspace of the a-th
hyperplane in Mx = m. The solution xr lies in h. For a ∈ [κ], ka is a unit vector normal to Ha.
Each Ha is a hyperplane in span({ei : i ∈ {1} ∪ supp(xr)}). So h has dimension |supp(xr)| + 1 − κ.
We assume that |supp(xr)| + 1 − κ is positive which will be the case for MEB, PD, and DKSG.
The subspace of span({ei : i ∈ {1} ∪ supp(xr)}) orthogonal to h is κ-dimensional, and it is equal
to span({ka : a ∈ [κ]}). The following equalities follow easily:

e1 ↓ h = e1 −

(cid:88)

a∈[κ]

(cid:104)e1, ka(cid:105) ka = e1 −

(cid:88)

a∈[κ]

(cid:113)(cid:80)

(M)a,1

j∈{1}∪Ia

· ka.

(M)2
a,j

(cid:104)vr ↓ Kr, e1 ↓ h(cid:105) = (cid:104)vr ↓ Kr, e1(cid:105) −

(cid:88)

a∈[κ]

(cid:113)(cid:80)

(M)a,1

j∈{1}∪Ia

(M)2
a,j

· (cid:104)vr ↓ Kr, ka(cid:105).

Lemma 4.1 shows that freeing the variable (x)1 admits a good descent direction; we can show
that (cid:104)vr ↓ Kr, e1 ↓ h(cid:105) < 0 for MEB, PD, and DKSG. Combining Lemmas 4.1 and 4.2 shows a
geometric-like convergence under the assumption that the distance between xr and the minimum
in the descent direction that frees x1 is at least 1

λ (cid:107)xr − x∗(cid:107).

Lemma 4.1. Let S be an active set disjoint from {1} ∪ supp(xr). If (cid:104)vr ↓ Kr, e1 ↓ h(cid:105) < 0, then
nr = e1 ↓ h/(cid:107)e1 ↓ h(cid:107) is a unit descent direction from xr with respect to S and cos ∠(−vr ↓ Kr, nr) ≥
(cid:104)−vr ↓ Kr, e1 ↓ h(cid:105) · (cid:107)vr ↓ Kr(cid:107)−1.
Lemma 4.2. Suppose that there exist positive λ and γ such that cos ∠(−vr ↓ Kr, nr) ≥ 1/γ and
(cid:107)xr − yr(cid:107) ≥ 1
λ (cid:107)xr − x∗(cid:107) for all r ≥ 1, where nr is the unit descent direction that satisﬁes Lemma 4.1
with respect to the active set Sr+1, and yr is the feasible point that minimizes f on the ray from xr
in direction nr. Then, for all r ≥ 1, f (xr+i) − f (x∗) ≤ e−1(f (xr) − f (x∗)) for some i ≤ 2λγ.

MEB. The hyperplane {x ∈ Rn : (cid:80)n
I1 = supp(xr); (M)1,i = 1 for i ∈ [n]. As |supp(xr)| ≥ 1,

i=1(x)i = 1} contains all feasible solution. We have [κ] = {1};
2 . Thus,
(M)2
(cid:104)vr ↓ Kr, k1(cid:105). For i (cid:54)∈ {1} ∪ supp(xr), (k1)i = 0. For

|supp(xr)|+1 ≤ 1

i∈{1}∪I1

(M)2

=

(cid:80)

1,1

1,i

1

(cid:104)vr ↓ Kr, e1 ↓ h(cid:105) = (vr)1 −

1√

|supp(xr)|+1
1√

|supp(xr)|+1

i ∈ {1} ∪ supp(xr), (k1)i =

. By complementary slackness, for all i ∈ supp(xr),

(vr)i = 0 as (xr)i > 0. Thus, (cid:104)vr ↓ Kr, k1(cid:105) = (vr)1 · (k1)1 =
h(cid:105) = (vr)1 · (cid:0)1 −
(cid:1) ≤ 1
(cid:80)
(vr)2

(vr)1 and (cid:104)vr ↓ Kr, e1 ↓
2 (vr)1 < 0. Since (vr)i = 0 for i ∈ supp(xr), we have (cid:107)vr ↓ Kr(cid:107)2 =
2n. Then,

1. By Lemma 4.1, cos ∠(−vr ↓ Kr, nr) ≥ 1/(cid:112)2|Er| ≥ 1/

1
|supp(xr)|+1
i ≤ |Er| · (vr)2

|supp(xr)|+1

√

i∈Er

Lemma 4.2 implies that SolveNNQ reduces f (xr) − f (x∗) by a factor e in O(λ

n) iterations.

√

1√

I1 = supp(xr) ∩ [m]; (M)1,i = 1 for all i ∈ {1} ∪ I1. Thus,

PD. Let the two input point sets be {p1, . . . , pm} and {q1, . . . , qn}. Every feasible solution lies
i=1(x)i = 1 and (cid:80)m+n
in the following two hyperplanes: (cid:80)m
i=m+1(x)i = 1. We have [κ] = {1, 2};
= 1
2 . We have
(cid:104)vr ↓ Kr, k1(cid:105). As in the case of MEB,
(vr ↓ Kr)1 = (vr)1. Thus, (cid:104)vr ↓ Kr, e1 ↓ h(cid:105) = (vr)1 −
cos ∠(−vr ↓ Kr, nr) ≥ 1/(cid:112)2|Er| ≥ 1/(cid:112)2(m + n). So Lemma 4.2 implies that SolveNNQ reduces
f (xr) − f (x∗) by a factor e in O(λ

m + n) iterations.

|I1|+1 ≤ 1

i∈{1}∪I1

1√

|I1|+1

(M)2

(M)2

√

(cid:80)

1,1

1,i

6

DKSG. We follow the approach in the analysis for MEB. To do so, we transform the inequality
constraints to equality constraints by introducing n slack variables. The details are in Appendix C.2.
The conclusion is that SolveNNQ reduces f (xr) − f (x∗) by a factor e in O(λn) iterations.

Theorem 4.1. Consider running SolveNNQ on an MEB, PD, or DKSG problem. Let k =
maxr≥1 |supp(xr)|. For all r ≥ 1, there is a unit descent direction nr from xr that satisﬁes
Lemma 4.1 so that cos ∠(−vr ↓ Kr, nr) ≥ 1/γ, where γ is Θ(
m + n) for PD,
and Θ(n) for DKSG. Suppose that the threshold β1 on the number of iterations is not exceeded,
and that (cid:107)xr − yr(cid:107) ≥ 1
λ (cid:107)xr − x∗(cid:107) for all r ≥ 1, where yr is the feasible point that minimizes f on
the ray from xr in direction nr.

n) for MEB, Θ(

√

√

• MEB: Initialization takes T (d + 1) + O(dn2) time. For r ≥ 1, f (xr+i) − f (x∗) ≤ e−1(f (xr) −

f (x∗)) for some i = O(λ

n). Each iteration takes T (k + β0) + O(kn + β2

0) time.

√

• PD: Initialization takes O(dm2 + dn2) time. For r ≥ 1, f (xr+i) − f (x∗) ≤ e−1(f (xr) − f (x∗))

for some i = O(λ

m + n). Each iteration takes T (k + β0) + O(km + kn + β2

0) time.

√

• DKSG: Initialization takes T (n + β0 − 1) + O(dn4 + β2

e−1(f (xr) − f (x∗)) for some i = O(λn). Each iteration takes T (k + β0) + O(kn2 + β2

0) time. For r ≥ 1, f (xr+i) − f (x∗) ≤
0) time.

As discussed in the case of Theorem 3.1, the gap f (xr) − f (x∗) for MEB, PD, and DKSG will
still reduce by a factor e after O(λγ) not necessarily consecutive iterations in the future that satisfy
the assumption.

5 Experimental results

Our machine conﬁguration is: Intel Core 7-9700K 3.6Hz cpu, 3600 Mhz ram, 8 cores, 8 logical
processors. We use MATLAB version R2020b. We use the version of quadprog that runs the
interior-point-convex algorithm. In each iteration, we should extract the submatrices corresponding
to the free variables; however, doing so incurs a large amount of data movements. Instead, we zero
out the rows and columns corresponding to the variables in the active set. We determine that
τ = 4 ln2 ν is a good setting for the DKSG and ZHLG data sets. We set τ = 4 ln2 ν in the
experiments with MEB, image deblurring, and PD as well without checking whether this is the
best for them. It helps to verify the robustness of this choice of τ . We set β0 to be 3τ and β1 to
be 15. Due to space limitation, the experimental results that show that SolveNNQ often achieves
a 5-fold speedup over a single call of quadprog on PD are deferred to Appendix D.4.

5.1 DKSG and ZHLG

We set µ = 16 and ρ = 2 in the objective function for ZHLG which are good settings according
to [42]. We experimented with the data sets Iris, HCV, Ionosphere, and AAL [9]. For each data
set, we sampled a number of rows that represent the number of points n and a number of attributes
that represent the number of dimensions d. Table 1 shows some average running times of SolveNNQ
and quadprog. When n ≥ 130 and n ≥ 13d, SolveNNQ is often 10 times or more faster. Figure 2
shows the plot of the natural logarithm of the average speedup of SolveNNQ over a single call of
quadprog. For both DKSG and ZHLG and for every ﬁxed d, the average speedup as a function of n
hovers around Θ(n3) for Ionosphere and AAL. In Appendix D.1, we show more results on Iris and
HCV. Figure 3(a) shows the plot of the average f (xr+1)−f (x∗)
f (xr)−f (x∗) against r in some of our experiments.
It shows that the gap f (xr) − f (x∗) decreases by at least a constant factor; most of the runs show

7

n
70

100

130

160

d
2
4
10
2
4
10
2
4
10
2
4
10

Ionosphere

AAL

quadprog
4.57s
5.78s
4.65s
26.67s
43.60s
28.53s
192.41s
261.54s
208.24s
660.25s
888.06s
979.92s

DKSG
ours
1.84s
2.25s
1.55s
5.41s
7.06s
5.46s
11.56s
13.13s
12.02s
25.39s
27.20s
22.03s

nnz
44%
47%
47%
33%
35%
35%
27%
27%
27%
23%
23%
22%

quadprog
2.51s
2.20s
1.86s
16.31s
14.87s
13.34s
144.27s
141.70s
134.95s
614.71s
622.58s
598.55s

ZHLG
ours
1.06s
0.97s
1.67s
3.35s
2.77s
5.42s
5.53s
6.23s
12.43s
9.14s
10.74s
29.97s

nnz
47%
49%
59%
36%
36%
44%
28%
29%
36%
24%
24%
31%

quadprog
3.99s
4.64s
4.89s
22.91s
38.63s
36.58s
175.43s
221.80s
257.41s
550.44s
864.51s
984.85s

DKSG
ours
1.72s
1.94s
1.80s
5.30s
7.81s
6.00s
15.81s
15.89s
14.60s
26.06s
31.71s
20.86s

nnz
44%
46%
48%
34%
37%
35%
27%
29%
28%
24%
24%
22%

quadprog
2.36s
2.32s
1.93s
15.25s
13.38s
13.34s
145.83s
142.01s
135.08s
613.45s
610.71s
601.50s

ZHLG
ours
1.19s
1.27s
1.69s
3.24s
2.68s
4.59s
6.58s
5.88s
10.03s
15.01s
14.62s
22.42s

nnz
52%
50%
58%
37%
35%
43%
28%
30%
35%
26%
26%
30%

Table 1: Ionosphere and AAL: each data shown for a pair (n, d) is the average of the corresponding
data from ﬁve runs; nnz is the percentage of the average number of non-zeros in the ﬁnal solution.

Figure 2: We plot the natural logarithm of the average speedup of SolveNNQ over a single call of
quadprog for DKSG and ZHLG. The horizontal axis is ln n. Data sets of diﬀerent dimensions are
obtained from AAL and Ionosphere.

(cid:107)x∗−xr(cid:107) and cos ∠(vr,xr+1−xr)

an even faster convergence. Our lower bound in Corollary 2.1 leads to a geometric convergence
if both (cid:107)xr+1−xr(cid:107)
cos ∠(vr,x∗−xr) are bounded away from zero. Figure 3(b) shows the plot of
(cid:107)x∗−xr(cid:107) against r. Figure 3(c) the corresponding plot of the median cos ∠(vr,xr+1−xr)
the average (cid:107)xr+1−xr(cid:107)
cos ∠(vr,x∗−xr)
is more than 0.4 and the median cos ∠(vr,xr+1−xr)
against r. The average (cid:107)xr+1−xr(cid:107)
is more than 0.5.
cos ∠(vr,x∗−xr)
(cid:107)x∗−xr(cid:107)

(a)

(b)

(c)

Figure 3: (a) Plot of the average f (xr+1)−f (x∗)
r. (c) Plot of the median cos ∠(vr,xr+1−xr)

cos ∠(vr,x∗−xr) against r.

f (xr)−f (x∗) against r. (b) Plot of the average (cid:107)xr+1−xr(cid:107)

(cid:107)x∗−xr(cid:107) against

5.2 Minimum Enclosing Ball

We experimented with two types of point sets as in previous works [21, 23, 24]. The ﬁrst type is
uniformly random points from a d-dimensional unit cube. The second type is nearly cospherical

8

(a) SolveNNQ vs G99.

(b) SolveNNQ vs FGK03.

Figure 4: Average running times on 1000 random points in a unit cube.

n
1000

2000

4000

6000

8000

d
200
400
1000
200
400
1000
200
400
1000
200
400
1000
200
400
1000

MEB

Random Points

Nearly Cospherical Points

quadprog
0.119s
0.131s
0.116s
0.573s
0.559s
0.599s
3.360s
3.730s
4.261s
10.765s
11.261s
12.020s
20.180s
24.700s
24.776s

ours
0.026s
0.030s
0.155s
0.056s
0.062s
0.161s
0.223s
0.193s
0.304s
0.485s
0.453s
0.546s
0.772s
0.931s
0.918s

nnz
202
401
1000
201
401
1001
203
401
1001
202
401
1001
202
402
1001

quadprog
0.108s
0.106s
0.0931s
0.506s
0.539s
0.554s
2.918s
2.828s
2.862s
9.279s
8.552s
7.934s
19.584s
19.976s
17.719s

ours
0.339s
0.223s
0.095s
0.921s
1.369s
1.082s
2.013s
3.328s
5.332s
3.993s
6.533s
10.323s
5.770s
9.919s
15.208s

nnz
918
958
1000
1301
1499
1814
1803
2211
2796
2185
2753
3516
2526
3082
3937

Figure 5: In the table on the left, each running time is the average over ﬁve runs, and nnz is the
rounded average number of non-zeros in the ﬁnal solution. The right image shows the average
speedup of SolveNNQ over a single call of quadprog on random points in a unit cube.

points in Rd. To generate a point set of the second type, we ﬁrst draw points uniformly at random
from Sd−1. For each point p drawn from Sd−1, we pick a random number ε ∈ [−10−4, 10−4] and
include the point (1 + ε)p in the ﬁnal point set. We compare SolveNNQ, quadprog, an MEB C++
code by G¨artner [22], and an MEB Java library by Fischer et al. [20]. We refer to the latter two
software as G99 and FGK03, respectively. Figure 4(a) and (b) show the average running times of
G99, FGK03, and SolveNNQ on 1000 random points in a unit cube as d increases. Since G99 is
designed for eﬃciency in low dimensions, SolveNNQ is expected to outperform G99 for larger d;
this happens at d = 20. SolveNNQ overtakes FGK03 at 140 dimensions on random points in a
unit cube. Similar trends are observed for nearly cospherical points (see Appendix D.2). In 200
dimensions or higher, SolveNNQ and a single call of quadprog outperform G99 and FGK03 on
both types of point sets. The table in Figure 5 shows the results for SolveNNQ and quadprog in
dimensions from 200 to 1000. When n ≥ 2000 and n ≥ 10d, SolveNNQ is 10 times or more faster
than quadprog on random points in a unit cube; the average speedup increases roughly linearly in
n for each ﬁxed d as shown in the plot in Figure 5. For nearly cospherical point sets, the number
of non-zeros in the ﬁnal solution is much bigger than d, which slows down SolveNNQ signiﬁcantly.
Still, SolveNNQ is signiﬁcantly faster than quadprog when n ≥ 6000 and n ≥ 20d.

5.3 Image deblurring

We experimented with four gray-scale space images bcea, nph, sts, and hst [2, 3, 7, 8]; note that the
iterative scheme is eﬃcient only when the intermediate and ﬁnal solutions are sparse. We adopt the

9

atmospheric turbulence point spread function to generate the blurring matrix A [31], which depends
on a parameter σ; the larger σ is, the blurrier the image. Multiplying A with a vector representing
the original image generates a vector b that represents the blurred image. Working backward, we
solve an NNLS problem to deblur the image: ﬁnd the non-negative x that minimizes (cid:107)Ax − b(cid:107)2. We
compare SolveNNQ with several software that can solve NNLS, including FISTA with backtracking
(FISTABT) [5, 11], SBB [29], FNNLS [32], lsqnonneg of MATLAB, and a single call of quadprog.
Figure 6 shows the results on hst. Table 2 shows some statistics. When the relative mean square
error of an image is well below 0.01, it is visually non-distinguishable from the original. In our
experiments, SolveNNQ is very eﬃcient and it produces a very good output. More details and
more results on thermal images are given in Appendix D.3. We emphasize that we do not claim
a solution for image deblurring as there are many issues that we do not address; we only seek to
demonstrate the potential of the iterative scheme.

hst, original

hst, σ = 2.5

SolveNNQ, 0.002, 21.6s

FISTABT, 0.1, 11.5s

SBB, 0.08, 600s

FNNLS, 2 × 10−7, 694s

lsqnonneg, 7 × 10−12, 1610s

quadprog, 0.002, 72.1s

Figure 6: The upper left two images are the original and the blurred image. Under the output of each
software, the ﬁrst number is the relative mean square error of the intensities of the corresponding
pixels between the output and the original, and the second number is the running time.

Data
bcea

nph

sts

hst

SolveNNQ

FISTABT

SBB

FNNLS

lsqnonneg

quadprog

σ
1
1.5
2
1
1.5
2
1
1.5
2
1
1.5
2

rel. err
4 × 10−5
0.001
0.002
10−5
4 × 10−4
7 × 10−4
2 × 10−5
4 × 10−4
5 × 10−4
2 × 10−5
0.001
0.001

time
2.2s
5.1s
9.2s
3s
5.6s
8.2s
2.1s
4.9s
9.7s
2.5s
6.5s
11.1s

rel. err
0.009
0.05
0.09
0.009
0.04
0.06
0.01
0.06
0.1
0.02
0.06
0.1

time
1.3s
2.8s
4.5s
2s
4.3s
6.8s
2.3s
4.7s
7.1s
2.3s
4.7s
7.4s

rel. err
7 × 10−5
0.008
0.02
3 × 10−5
0.009
0.02
4 × 10−5
0.01
0.04
4 × 10−5
0.02
0.05

time
110s
600s
600s
171s
600s
600s
206s
600s
600s
275s
600s
600s

rel. err
10−10
5 × 10−8
7 × 10−8
7 × 10−11
3 × 10−8
6 × 10−8
8 × 10−11
10−8
3 × 10−8
10−10
10−7
3 × 10−7

time
33s
137s
200s
15.4s
60s
94s
14s
48.9s
80s
79.4s
272s
459s

rel. err
10−13
2 × 10−12
3 × 10−12
10−13
10−12
2 × 10−12
9 × 10−14
2 × 10−12
2 × 10−12
2 × 10−13
3 × 10−12
4 × 10−12

time
59s
267s
420s
25s
105s
181s
22.7s
87s
156s
182s
651s
1060s

rel. err
3 × 10−5
8 × 10−4
0.002
3 × 10−5
5 × 10−4
8 × 10−4
3 × 10−5
4 × 10−4
0.001
3 × 10−5
0.001
0.003

time
5s
11.4s
17.9s
8.8s
17.9s
33.3s
9.4s
20.4s
38.6s
9.4s
20.9s
47.2s

Table 2: Experimental results for some space images. In the column for each software, the number
on the left is the relative mean square error, and the number on the right is the running time.

10

References

[1] https://adas-dataset-v2.ﬂirconservator.com/#downloadguide.

[2] https://github.com/jnagy1/IRtools/blob/master/Extra/test data/HSTgray.jpg.

[3] https://github.com/jnagy1/IRtools/blob/master/Extra/test data/satellite.mat.

[4] https://github.com/QiaoLiuHit/LSOTB-TIR.

[5] https://github.com/tiepvupsu/FISTA.

[6] https://public.roboﬂow.com/object-detection/thermal-dogs-and-people/1.

[7] https://www.skyatnightmagazine.com/advice/how-to-take-a-photo-of-the-iss/.

[8] https://www.space.com/russian-progress-cargo-spacecraft-docks-space-station.

[9] https://archive.ics.uci.edu/ml/datasets.php.

[10] N. Alon, S. Dar, M. Parnas, and D. Ron. Testing of clustering. In Proceedings fo the 41st

Annual Symposium on Foundations of Computer Science, pages 240–250, 2000.

[11] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

[12] A. Ben-Hur, D. Horn, H.T. Siegelmann, and V. Vapnik. Support vector clustering. Journal of

Machine Learning Research, 2:125–137, 2001.

[13] M. B˘adoiu and K.L. Clarkson. Optimal core-sets for balls. Computational Geometry: Theory

and Applications, 40:14–22, 2008.

[14] M. B˘adoiu, S. Har-Peled, and P. Indyk. Approximate clustering via core-sets. In Proceedings

of the 34th Annual ACM Symposium on Theory of Computing, pages 250–257, 2002.

[15] P.H. Calamai and J.J. Mor´e. Projected gradient methods for linearly constrained problems.

Mathematical programming, 39:93–116, 1987.

[16] S.-W. Cheng, O. Cheong, T. Lee, and Z. Ren. Fitting a graph to one-dimensional data.

Theoretical Computer Science, 867:40–49, 2021.

[17] K.L. Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM

Transactions on Algorithms, 6(4), 2010.

[18] T.F. Coleman and L.A. Hulbert. A direct active set algorithm for large sparse quadratic

programs with simple bounds. Mathematical programming, 45:373–406, 1989.

[19] S.I. Daitch, J.A. Kelner, and D.A. Spielman. Fitting a graph to vector data. In Proceedings
of the 26th Annual International Conference on Machine Learning, pages 201–209, 2009.

[20] K. Fischer, B. G¨artner, and M. Kutz. https://github.com/hbf/miniball.

[21] K. Fischer, B. G¨artner, and M. Kutz. Fast smallest-enclosing-ball computation in high dimen-
sions. In Proceedings of the 11th Annual European Symposium on Algorithms, pages 630–641,
2003.

11

[22] B. G¨artner. https://people.inf.ethz.ch/gaertner/subdir/software/miniball.html.

[23] B. G¨artner. Fast and robust smallest enclosing balls. In Proceedings of the 7th Annual European

Symposium on Algorithms, pages 121–139, 1999.

[24] B. G¨artner and S. Sch¨onherr. An eﬃcient, exact, and generic quadratic programming solver
for geometric optimization. In Proceedings of the 16th Annual Symposium on Computational
Geoemtry, pages 110–118, 2000.

[25] M. Hanke, J.G. Nagy, and C. Vogel. Quasi-Newton approach to nonnegative image restorations.

Linear Algebra and Its Applications, 316:223–236, 2000.

[26] P.M. Hubbard. Approximating polyhedra with spheres for time-critical collision detection.

ACM Transactions on Graphics, 15:179–210, 1996.

[27] T. Jebara, J. Wang, and S.-F. Chang. Graph construction and b-matching for semi-supervised
learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 441–448, 2009.

[28] V. Kalofolias. How to learn a graph from smooth signals. In Proceedings of the 19th Interna-

tional Conference on Artiﬁcial Intelligence and Statistics, pages 920–929, 2016.

[29] D. Kim, S. Sra, and I.S. Dhillon. A non-monotonic method for large-scale non-negative least

squares. Optimization Methods and Software, 28(5):1012–1039, 2013.

[30] P. Kumar, J.S.B. Mitchell, and E.A. Yıldırım. Approximate minimum enclosing balls in high

dimensions using core-sets. ACM Journal of Experimental Algorithmics, 8, 2003.

[31] R.L. Lagendijk and J. Biemond. Iterative Identiﬁcation and Restoration of Images. Springer,

1991.

[32] C.L. Lawson and R.J. Hanson. Solving least squares problems. SIAM, 1995.

[33] J. Matouˇsek, M. Sharir, and E. Welzl. A subexponential bound for linear programming.

Algorithmica, 16:498–516, 1996.

[34] B.L. McGlamery. Restoration of turbulence degraded images. Journal of the Optical Society

of America, 57(3):293–297, 1967.

[35] R.D.C. Monteiro and I. Adler. Interior path following primal-dual algorithms. Part II: convex

quadratic programming. Mathematical Programming, 44:43–66, 1989.

[36] K. Sekitani and Y. Yamamoto. A recursive algorithm for ﬁnding the minimum norm point in a
polytope and a pair of closest points in two polytopes. Mathematical Programming, 61:233–249,
1993.

[37] I.W. Tsang, J.T. Kwok, and P.-M. . Core vector machines: fast SVM training on very large

data sets. Journal of Machine Learning Research, 6:363–392, 2005.

[38] I.W. Tsang, J.T. Kwok, and J.M. Zurada. Generalized core vector machines. IEEE Transac-

tions on Neural Networks, 17(5):1126–1140, 2006.

[39] E. Welzl. Smallest enclosing disks (balls and ellipsoids). In New Results and New Trends in
Computer Science, volume 555 of Lecture Notes in Computer Science, pages 359–270. Springer,
1991.

12

[40] P. Wolfe. Finding the nearest point in a polytope. Mathematical Programming, 11:128–149,

1976.

[41] E.A. Yıldırım. Two algorithms for the minimum enclosing ball problem. SIAM Journal on

Optimization, 19(3):1368–1391, 2008.

[42] Y.-M. Zhang, K. Huang, X. Hou, and C.-L. Liu. Learning locality preserving graph from data.

IEEE Tranactions on Cybernetics, 44(11):2088–2098, 2014.

A Missing content in Section 2

A.1 Number of iterations when β1β1β1 is exceeded

Let x∗ denote the optimal solution of the original NNQ problem.

Lemma A.1. If β1 is exceeded, lines 7–19 of SolveNNQ will be iterated at most |supp(x∗)| more
times before x∗ is found.

Proof. By the KKT conditions, ∇f (xr) = Btur + Ct˜ur + vr, so (cid:104)∇f (xr), x∗ − xr(cid:105) = (cid:104)Btur, x∗ −
xr(cid:105) + (cid:104)Ct˜ur, x∗ − xr(cid:105) + (cid:104)vr, x∗ − xr(cid:105). By the complementary slackness, we have ut
r(Bxr − b) = 0 and
vt
rxr = 0. The former equation implies that ut
rb. By primal feasibility, Cx∗ = Cxr = c
which implies that (cid:104)Ct˜ur, x∗ − xr(cid:105) = ˜ut

rBxr = ut
r(Cx∗ − Cxr) = 0. As a result,

(cid:104)∇f (xr), x∗ − xr(cid:105) = ut

r(Bx∗ − b) + vt

rx∗.

(1)

Primal and dual feasibilities imply that Bx∗ ≥ b and ur have non-negative coordinates; therefore,

ut
r(Bx∗ − b) ≥ 0.

r(Bx∗ − b) ≥ 0 and vt

rx∗ ≥ 0 into (1) gives (cid:10)∇f (xr), x∗ − xr

We claim that (vr)i < 0 for some i ∈ supp(x∗) ∩ Sr. Assume to the contrary that (vr)i ≥ 0
for all i ∈ supp(x∗) ∩ Sr. Then, (vr)i ≥ 0 for all i ∈ supp(x∗), which implies that vt
rx∗ ≥ 0.
(cid:11) ≥ 0. However, since
Substituting ut
xr is not optimal, by the convexity of f and the feasible region of the NNQ problem, x∗ − xr is a
descent direction from xr, contradicting the relation (cid:104)∇f (xr), x∗ − xr(cid:105) ≥ 0. This proves our claim.
By our claim, there must exist some index i ∈ supp(x∗) ∩ Sr such that (vr)i < 0. This index
i must be included in Er, which means that i (cid:54)∈ Sr+1 because Sr+1 = Sr \ Er. As a result, each
iteration of lines 11 and 12 sets free at least one more element of supp(x∗). After at most |supp(x∗)|
more iterations, all elements of supp(x∗) are free variables, which means that the solver will ﬁnd
x∗ in at most |supp(x∗)| more iterations.

A.2 Proofs of Lemma 2.1 and Corollary 2.1

We restate Lemma 2.1 and gives its proof.

Lemma 2.1 Let n ∈ Rν be any unit descent direction from xr. Let y = xr + αn for some α > 0
be the feasible point that minimizes f on the ray from xr in direction n. Then, (i) f (xr) − f (y) ≤
−(cid:107)xr − y(cid:107) · (cid:104)∇f (xr), n(cid:105), and (ii) f (xr) − f (y) ≥ − 1

2 (cid:107)xr − y(cid:107) · (cid:104)∇f (xr), n(cid:105).

Proof. For all s ∈ [0, 1], deﬁne ys = xr + s(y − xr). By the chain rule, we have

∂f
∂s

=

(cid:29)

(cid:28) ∂f
∂ys

,

∂ys
∂s

= (cid:104)∇f (ys), y − xr(cid:105) .

13

We integrate along a linear movement from xr to y. Using the fact that ∇f (ys) = 2AtA(xr + s(y −
xr)) + a = ∇f (xr) + 2sAtA(y − xr), we obtain

f (y) = f (xr) +

= f (xr) +

= f (xr) +

(cid:90) 1

0
(cid:90) 1

0

(cid:104)∇f (ys), y − xr(cid:105) ds

(cid:104)∇f (xr), y − xr(cid:105) ds +

(cid:90) 1

0

2s(cid:10)AtA(y − xr), y − xr

(cid:11) ds

(cid:104)
(cid:104)∇f (xr), y − xr(cid:105) · s

(cid:105)1

0

(cid:104)

+

(cid:107)A(y − xr)(cid:107)2 · s2(cid:105)1

0

= f (xr) + (cid:104)∇f (xr), y − xr(cid:105) + (cid:107)A(y − xr)(cid:107)2.

It follows immediately that

f (xr) − f (y) = −(cid:104)∇f (xr), y − xr(cid:105) − (cid:107)A(y − xr)(cid:107)2
≤ −(cid:104)∇f (xr), y − xr(cid:105)
= −(cid:107)xr − y(cid:107) · (cid:104)∇f (xr), n(cid:105).

(2)

This completes the proof of (i).

Consider (ii). Using ys = xr + s(y − xr) = y + (1 − s)(xr − y) and the fact that ∇f (ys) =
2AtA(y + (1 − s)(xr − y)) + a = ∇f (y) + 2(1 − s)AtA(xr − y), we carry out a symmetric derivation:

f (xr) = f (y) +

= f (y) +

(cid:90) 0

1
(cid:90) 0

1

(cid:104)∇f (ys), y − xr(cid:105) ds

(cid:104)∇f (y), y − xr(cid:105) ds +

(cid:90) 0

1

2(1 − s)(cid:104)AtA(xr − y), y − xr(cid:105) ds

= f (y) +

(cid:104)

(cid:104)∇f (y), y − xr(cid:105) · s

(cid:105)0

1

(cid:104)

+

(cid:107)A(y − xr)(cid:107)2 · (1 − s)2(cid:105)0

1

= f (y) − (cid:104)∇f (y), y − xr(cid:105) + (cid:107)A(y − xr)(cid:107)2.

(3)

Combining (2) and (3) gives

f (xr) − f (y) = −

1
2

(cid:104)∇f (xr), y − xr(cid:105) −

1
2

(cid:104)∇f (y), y − xr(cid:105).

As y is the feasible point that minimizes the value of f on the ray from xr in direction n, we have
(cid:104)∇f (y), y − xr(cid:105) ≤ 0. Therefore,

f (xr) − f (y) ≥ −

1
2

(cid:104)∇f (xr), y − xr(cid:105) = −

1
2

(cid:107)xr − y(cid:107) · (cid:104)∇f (xr), n(cid:105).

We restate Corollary 2.1 and gives its proof.

Corollary 2.1 Let n ∈ Rν be any unit descent direction from xr. Let y = xr + αn for some α > 0
be the feasible point that minimizes f on the ray from xr in direction n. Let S be an active set that
is disjoint from supp(xr) ∪ supp(n). Let xr+1 be the optimal solution for f constrained by S. Then,
f (xr)−f (x∗) ≥ f (xr)−f (y)
f (xr)−f (xr+1)

f (xr)−f (x∗) ≥ (cid:107)xr−y(cid:107)

(cid:104)∇f (xr),n(cid:105)
(cid:104)∇f (xr),x∗−xr(cid:105) .

2

·

Proof.
By applying Lemma 2.1(i) to the descent direction x∗ − xr, we get f (xr) − f (x∗) ≤
−(cid:104)∇f (xr), x∗ − xr(cid:105). Since S is disjoint from supp(x∗) ∪ supp(n), the feasible points on the ray from

14

Figure 7: The vector n1,2 bisects the right angle ∠(n1, n2). There are two choices for η, and we
choose the angle that is at least π/2.

xr in direction n are also feasible with respect to S. Therefore, f (xr) − f (xr+1) ≥ f (xr) − f (y).
By applying Lemma 2.1(ii) to the descent direction n, we get f (xr) − f (xr+1) ≥ f (xr) − f (y) ≥
− 1
2 (cid:107)xr − y(cid:107) · (cid:104)∇f (xr), n(cid:105). Dividing the lower bound for f (xr) − f (xr+1) by the upper bound for
f (xr) − f (x∗) proves the corollary.

B Missing content in Section 3

B.1 Analysis for Lemma 3.1

Recall that Jr = span({ei : i ∈ Er}). The next result follows from the deﬁnitions of Er and Jr.

Lemma B.1. For all i ∈ [ν], if i ∈ Er, then (vr ↓ Jr)i = (vr)i; otherwise, (vr ↓ Jr)i = 0. Moreover,
−vr ↓ Jr is a conical combination of {ei : i ∈ Er}.

We show that Er gives a subset {ei} such that cos ∠(vr ↓ Jr, ei) is bounded away from zero.

Lemma B.2. For any α ∈ (0, 1], there exists j in Er with rank at most α|Er| such that for every
i ∈ Er, if i = j or i precedes j in Er, then cos2 ∠(vr ↓ Jr, ei) ≥ α/(cid:0)j ln ν(cid:1).

i∈Er

i∈Er

α/(i ln ν) ≤ (α/ ln ν) · (cid:80)ν

cos2 ∠(vr ↓ Jr, ei). As vr ↓ Jr ∈ Jr, we have (cid:80)

Proof. Consider a histogram H1 of α/(i ln ν) against i ∈ Er. The total length of the vertical
bars in H1 is equal to (cid:80)
i=1 1/i ≤ α. Consider another histogram
H2 of cos2 ∠(vr ↓ Jr, ei) against i ∈ Er. The total length of the vertical bars in H2 is equal to
(cid:80)
cos2 ∠(vr ↓ Jr, ei) = 1. That is, the
total length of the vertical bars in H2 is equal to 1. Recall that the indices of Er are sorted in
non-decreasing order of (vr)i. As (vr ↓ Jr)i = (vr)i < 0 for all i ∈ Er, the ordering in Er is the
same as the non-increasing order of cos2 ∠(vr ↓ Jr, ei) = (vr ↓ Jr)2
i /(cid:107)vr ↓ Jr(cid:107)2. Therefore, the total
length of the vertical bars in H2 for the ﬁrst α|Er| indices is at least α.
It implies that when
we scan the indices in Er from left to right, we must encounter an index j among the ﬁrst α|Er|
indices such that the vertical bar in H2 at j is not shorter than the vertical bar in H1 at j, i.e.,
cos2 ∠(vr ↓ Jr, ej) ≥ α/(j ln ν). This index j satisﬁes the lemma.

i∈Er

Next, we boost the angle bound implied by Lemma B.2 by showing that −vr ↓ Jr makes a much

smaller angle with some conical combination of {ei : i ∈ Er}.

√

2. Let z be a vector in RD for some D ≥ 2. Suppose that there
Lemma B.3. Take any c ≤ 1/
is a set V of unit vectors in RD such that the vectors in V are mutually orthogonal, and for every
n ∈ V , ∠(n, z) ≤ arccos (cid:0)c|V |−1/2(cid:1). There exists a conical combination y of the vectors in V such
that ∠(y, z) ≤ arccos(c/

2).

√

15

zn1n2n1,2≤θφηπ/4π/4√

n1 + 1√
2

Let θ = arccos (cid:0)c|V |−1/2(cid:1).
√

2) for c ≤ 1/

Proof.
If θ ≤ π/3, we can pick any vector n ∈ V as y because
2. Suppose not. Let W be a maximal subset of V whose
π/3 ≤ arccos(c/
size is a power of 2. Arbitrarily label the vectors in W as n1, n2, . . .. Consider the unit vector
n2. Let φ = ∠(n1,2, z). Refer to Figure 7. By assumption, n1 ⊥ n2. We choose
n1,2 = 1√
2
η to be at least π/2 as explained in the caption of Figure 7. By the spherical law of cosines,
cos θ ≤ cos ∠(n2, z) = cos φ cos(π/4) + sin φ sin(π/4) cos η. Note that cos η ≤ 0 as η ≥ π/2. It im-
plies that cos φ ≥ sec(π/4) cos θ =
2 cos θ. The same analysis holds between z and the unit vector
n3,4 = 1√
n4, and so on. In the end, we obtain |W |/2 vectors n2i−1,2i for i = 1, 2, . . . , |W |/2
2
such that ∠(n2i−1,2i, z) ≤ arccos(cid:0)√
2 cos θ(cid:1). Call this the ﬁrst stage. Repeat the above with the
|W |/2 unit vectors n1,2, n3,4, . . . in the second stage and so on. We end up with only one vector in
log2 |W | stages. If we ever produce a vector that makes an angle at most π/3 with z before going
through all log2 |W | stages, the lemma is true. Otherwise, we produce a vector y in the end such
that cos ∠(y, z) ≥ (cid:0)√

2(cid:1)log2 |V |−1 cos θ ≥ (cid:112)|V |/2 · cos θ = c/

2(cid:1)log2 |W | cos θ ≥ (cid:0)√

n3 + 1√
2

√

√

2.

We now are ready to prove Lemma 3.1 which is restated below for convenience.

(cid:9) have a unit conical combi-
Lemma 3.1 The vectors in (cid:8)ei : i among the ﬁrst τ indices in Er
nation nr such that nr is a descent direction from xr and ∠(−vr ↓ Jr, nr) ≤ arccos(cid:0)(cid:112)α/(2 ln ν)(cid:1),
where α = min{1, τ /|Er|}.

Since −vr ↓ Jr is a conical combination of (cid:8)ei

(cid:9), −vr ↓ Jr makes a non-
Proof.
: i ∈ Er}. Then, Lemmas B.2 and B.3 imply that the
obtuse angle with any vector in {ei
vectors in (cid:8)ei : i among the ﬁrst τ indices in Er
(cid:9) have a unit conical combination nr such that
cos ∠(−vr ↓ Jr, nr) ≥ (cid:112)α/(2 ln ν), where α = min{1, τ /|Er|}. For any feasible solution x and any
non-negative values c1, . . . , cν, x + (cid:80)ν
i=1 ciei is also a feasible solution. It follows that xr + snr is
a feasible solution for all s ≥ 0. Recall that ∇f (xr) = vr. Moreover, (cid:104)−vr ↓ Jr, nr(cid:105) = (cid:104)−vr, nr(cid:105)
because nr ∈ Jr and so the component of −vr orthogonal to Jr vanishes in the inner product. Then,
the acuteness of ∠(−vr ↓ Jr, nr) implies that (cid:104)∇f (xr), nr(cid:105) < 0. In all, nr is a descent direction.

: i ∈ Er

B.2 Analysis for Theorem 3.1

We restate Theorem 3.1 and give its proof.

Theorem 3.1 Consider the application of SolveNNQ on an NNLS problem with n constraints in
Rd or a ZHLG problem for n points in Rd. Let ν be n for NNLS or n(n − 1)/2 for ZHLG. The
initialization of SolveNNQ can be done in T (β0) + O(dn2) time for NNLS or T (β0) + O(n4) time
for ZHLG. Suppose that the threshold β1 on the total number of iterations is not exceeded, and
there exists λ such that (cid:107)xr − yr(cid:107) ≥ 1
λ (cid:107)xr − x∗(cid:107) for all r ≥ 1, where yr is the feasible point that
minimizes f on the ray from xr in the descent direction that satisﬁes Lemma 3.1. Then, for all
r ≥ 1, f (xr+i) − f (x∗) ≤ e−1(f (xr) − f (x∗)) for some i = O(cid:0)λ(cid:112)ν log ν/τ (cid:1), and each iteration in
SolveNNQ takes T (k + β0) + O(kν + β2

0) time, where k = maxr≥1 |supp(xr)|.

Proof. For NNLS, A is n × d and we compute AtA in O(dn2) time. For ZHLG, A is n(n + 1)/2 ×
(cid:3), where
n(n − 1)/2 and a is a vector of dimension n(n − 1)/2. Recall that At = (cid:2) µ
U ∈ Rn×n(n−1)/2 is the incidence matrix for the complete graph. Therefore, each row of At has at
most three non-zero entries, meaning that AtA can be computed in O(n4) time.

2 In(n−1)/2

2 Ut ρ

The initial solution is obtained as follows. Sample β0 indices from [ν]. The initial active set

16

contains all indices in the range [ν] except for these sampled indices. We extract the rows and
columns of AtA and coordinates of a that correspond to these β0 sampled indices. Then, we call the
solver in T (β0) time to obtain the initial solution x1.

At the beginning of every iteration, we compute ∇f (xr) and update the active set. This step
can be done in O(kν) time because xr has at most k non-zero entries. At most k + β0 indices are
absent from the updated active set because the threshold β1 is not exceeded. We select the rows
and columns of AtA and coordinates of a that correspond to the indices absent from the active set
in O(k2 + β2
0) time. The subsequent call of the solver takes T (k + β0) time. So each iteration runs
0) time. By Lemma 3.2, we need O(cid:0)λ(cid:112)ν log ν/τ (cid:1) iterations to reduce the
in T (k + β0) + O(kν + β2
gap f (xr) − f (x∗) by a factor e.

C Missing content in Section 4

C.1 Proofs of Lemmas 4.1 and 4.2

We restate Lemma 4.1 and give its proof.

Lemma 4.1 Let S be an active set disjoint from {1} ∪ supp(xr). If (cid:104)vr ↓ Kr, e1 ↓ h(cid:105) < 0, then
nr = e1 ↓ h/(cid:107)e1 ↓ h(cid:107) is a unit descent direction from xr with respect to S and cos ∠(−vr ↓ Kr, nr) ≥
(cid:104)−vr ↓ Kr, e1 ↓ h(cid:105) · (cid:107)vr ↓ Kr(cid:107)−1.

Since nr = e1 ↓ h/(cid:107)e1 ↓ h(cid:107) , we have cos ∠(−vr ↓ Kr, nr) = (cid:104)−vr ↓ Kr, e1 ↓ h(cid:105) · (cid:107)vr ↓
Proof.
Kr(cid:107)−1 · (cid:107)e1 ↓ h(cid:107)−1, which is positive as (cid:104)vr ↓ Kr, e1 ↓ h(cid:105) < 0 by assumption. Clearly, (cid:107)e1 ↓ h(cid:107) ≤ 1.
Therefore, cos ∠(−vr ↓ Kr, nr) satisﬁes the lemma. It remains to argue that e1 ↓ h is a descent di-
rection with respect to the active set S. Since S is disjoint from {1} ∪ supp(xr) by the assumption
of the lemma, any movement in a direction parallel to h is not obstructed by S. As we move from
xr in direction e1 ↓ h, our projection in e1 moves in the positive direction, and so (x)1 increases.
During the movement, for i (cid:54)∈ {1} ∪ supp(xr), the coordinate (x)i remains at zero by the deﬁnition
of h. Some coordinates among (cid:8)(x)i : i ∈ supp(xr)(cid:9), which are positive, may decrease during the
movement. The feasibility constraints are thus preserved for some extent of the movement. Recall
from the preamble of Section 4 that (cid:104)∇f (xr), e1 ↓ h(cid:105) = (cid:104)vr, e1 ↓ h(cid:105), which is equal to (cid:104)vr ↓ Kr, e1 ↓ h(cid:105)
as e1 ↓ h ∈ Kr. By the assumption of the lemma, (cid:104)vr ↓ Kr, e1 ↓ h(cid:105) < 0. Hence, e1 ↓ h is a descent
direction.

Next, we restate Lemma 4.2 and give its proof.

Lemma 4.2 Suppose that there exist positive λ and γ such that cos ∠(−vr ↓ Kr, nr) ≥ 1/γ and
(cid:107)xr − yr(cid:107) ≥ 1
λ (cid:107)xr − x∗(cid:107) for all r ≥ 1, where nr is the unit descent direction that satisﬁes Lemma 4.1
with respect to the active set Sr+1, and yr is the feasible point that minimizes f on the ray from xr
in direction nr. Then, for all r ≥ 1, f (xr+i) − f (x∗) ≤ e−1(f (xr) − f (x∗)) for some i ≤ 2λγ.

Proof. We claim that (cid:104)−vr, x∗ − xr(cid:105) ≤ (cid:104)−vr ↓ Kr, x∗ − xr(cid:105). Note that (−vr + vr ↓ Kr)i is equal to
zero for i ∈ supp(xr) ∪ Er and −(vr)i otherwise. By deﬁnition, for all i (cid:54)∈ supp(xr) ∪ Er, (vr)i ≥ 0
and so (−vr + vr ↓ Kr)i = −(vr)i ≤ 0. By the complementary slackness, if (vr)i > 0, then (xr)i = 0,
which implies that (x∗ − xr)i ≥ 0 as x∗ is non-negative. Altogether, we conclude that

∀ i ∈ [ν], (−vr + vr ↓ Kr)i · (x∗ − xr)i =

(cid:40) 0

if i ∈ supp(xr) ∪ Er,

≤ 0 if i (cid:54)∈ supp(xr) ∪ Er.

17

Therefore, (cid:104)−vr +vr ↓ Kr, x∗−xr(cid:105) ≤ 0, implying that (cid:104)−vr, x∗−xr(cid:105) = (cid:104)−vr ↓ Kr, x∗−xr(cid:105)+(cid:104)−vr +vr ↓
Kr, x∗ − xr(cid:105) ≤ (cid:104)−vr ↓ Kr, x∗ − xr(cid:105). This proves our claim.

Recall from the preamble of Section 4 that (cid:104)∇f (xr), k(cid:105) = (cid:104)vr, k(cid:105) for any direction k that is
parallel to the hyperplanes encodes by Mx = m. As a result, (cid:104)−∇f (xr), nr(cid:105) = (cid:104)−vr, nr(cid:105) = (cid:104)−vr ↓
Kr, nr(cid:105) because nr ∈ Kr. Similarly, (cid:104)−∇f (xr), x∗ − xr(cid:105) = (cid:104)−vr, x∗ − xr(cid:105), which is at most (cid:104)−vr ↓
Kr, x∗ − xr(cid:105) by the claim that we proved earlier.

We apply Corollary 2.1 and the assumptions of cos ∠(−vr ↓ Kr, nr) ≥ 1/γ and (cid:107)xr − yr(cid:107) ≥

1
λ (cid:107)xr − x∗(cid:107). Hence,

f (xr) − f (xr+1)
f (xr) − f (x∗)

≥

(cid:107)xr − yr(cid:107) · (cid:104)−vr ↓ Kr, nr(cid:105)
2(cid:104)−vr ↓ Kr, x∗ − xr(cid:105)

≥

(cid:107)xr − yr(cid:107)
(cid:107)xr − x∗(cid:107)

·

1
2γ

≥

1
2λγ

.

It follows that f (xr+1) − f (x∗) ≤

(cid:16)

1 − 1
2λγ

(cid:17)

· (cid:0)f (xr) − f (x∗)(cid:1). As a result, in at most 2λγ iterations,

f (cid:0)xr+2λγ

(cid:1) − f (x∗) ≤

(cid:19)2λγ

(cid:18)

1 −

1
2λγ

· (cid:0)f (xr) − f (x∗)(cid:1) ≤ e−1 · (cid:0)f (xr) − f (x∗)(cid:1).

C.2 Convergence analysis for DKSG

We add one slack variable per constraint in Bx ≥ 1n. This gives n slack variables and a total of
n(n + 1)/2 variables. That is, the ambient space expands to Rn(n+1)/2.

For every ˆx ∈ Rn(n+1)/2, the slack variables are represented by the last n coordinates of ˆx. The
(cid:3) and the feasibility constraints become ˆBˆx = 1n(n+1)/2.
(cid:3), where 0n,n denotes the n × n zero matrix. Let

constraint matrix becomes ˆB = (cid:2) B − In
Similarly, we expand the matrix A to ˆA = [ A 0n,n
ˆf : Rn(n+1)/2 → R denote the corresponding objective function.

The dual variables u for the constraints Bx ≥ 1n are still the dual variables for ˆBˆx = 1n. We have
a new set of dual variables ˆv ≥ 0n(n+1)/2 for the primal variables ˆx. One can verify that if (u, v, x)
is an optimal solution for the original system, then (u, ˆv, ˆx) with the following setting satisﬁes the
KKT conditions for the extended system, i.e., it is an optimal solution for the extended system:

• For all j ∈ [n(n − 1)/2], (ˆx)j = (x)j.

• For all j ∈ [n(n − 1)/2 + 1, n(n + 1)/2], (ˆx)j = (B)a,∗ · x − 1, where a = j − n(n − 1)/2 and

(B)a,∗ denotes the a-th row of B.

• For all j ∈ [n(n − 1)/2], (ˆv)j = (v)j.

• For all j ∈ [n(n − 1)/2 + 1, n(n + 1)/2], (ˆv)j = (u)a, where a = j − n(n − 1)/2.

Conversely, if (u, ˆv, ˆx) is an optimal solution for the extended system, we verify that (u, v, x)
satisﬁes the KKT conditions for the original system, i.e., (u, v, x) is an optimal solution of the
original system. By the KKT conditions on the extended system, we have ˆx ≥ 0n(n+1)/2, ˆv ≥
0n(n+1)/2, and

(cid:21)

(cid:20)∇f (x)
0n

− ˆBtu − ˆv = 0n(n+1)/2 ⇒ ˆv =

(cid:21)
(cid:20)∇f (x) − Btu
u

.

The fact that ˆv ≥ 0n(n+1)/2 implies that u ≥ 0n. By our setting, x and v consist of the ﬁrst
n(n − 1)/2 coordinates of ˆx and ˆv, respectively; therefore, v = ∇f (x) − Btu, v ≥ 0n(n−1)/2, and

18

x ≥ 0n(n−1)/2. The non-negative slack variables ensure that Bx ≥ 1n. The complementary slackness
on the extended system guarantees that (v)i · (x)i = (ˆv)i · (ˆx)i = 0 for i ∈ [n(n − 1)/2]. We show
that (u)a · ((B)a,∗x − 1) = 0 for a ∈ [n]. For all a ∈ [n], if (B)a,∗x > 1, the a-th slack variable is
positive which forces (u)a = (ˆv)n(n−1)/2+a = 0 by the complementary slackness on the extended
system. In the reverse direction, if (u)a = (ˆv)n(n−1)/2+a > 0, the a-th slack variable is zero by the
complementary slackness on the extended system; this forces (B)a,∗x = 1. We complete the analysis
that (u, v, x) satisﬁes the KKT conditions for the original system.

In the case that some primal variables belong to the active set, we allow the corresponding (ˆv)i

to be unrestricted. The slack variables are always free because SolveNNQ is unaware of them.

Let (ur, ˆvr, ˆxr) be the extended solution corresponding to (ur, vr, xr).
We assume that (vr)1 = mini∈Er (vr)i and that the edge with index 1 is p1p2. For DKSG, the

instantiation of the deﬁnition of Ia for DKSG is as follows:

∀ a ∈ [n], Ia = intersection of supp(ˆxr) and the set (cid:8)n(n − 1)/2 + a(cid:9)∪

(cid:8)i : i is the index of a complete graph edge incident to pa

(cid:9).

Note that 1 (cid:54)∈ Ia for a ∈ [n]. The set Ia is not empty because every node must be incident to some
edge in the graph induced by ˆxr in order that the incident edges of that node have a total weight
of at least 1. There may be some slack variables in supp(ˆxr), but Er does not contain any slack
variable.

Correspondingly, the instantiations of the deﬁnitions of Ha for a ∈ [n] and the aﬃne subspace

h are as follows:

∀ a ∈ [n], Ha =

ˆx ∈ Rn(n+1)/2 :

(cid:40)

(cid:88)

i∈{1}∪Ia

h =

(cid:92)

Ha.

a∈[n]

(ˆB)a,i · (x)i = 1 ∧ ∀ i (cid:54)∈ {1} ∪ supp(ˆxr), (ˆx)i = 0

(cid:41)
,

Regarding the vector ka that is normal to Ha for a ∈ [n], note that (ka)1 = 0 if a (cid:54)∈ {1, 2}. As a
result,

e1 ↓ h = e1 −

(cid:88)

a∈[n]

(cid:104)e1, ka(cid:105) ka = e1 −

2
(cid:88)

a=1

(cid:104)e1, ka(cid:105) ka.

For every a ∈ {1, 2} and every i ∈ {1} ∪ Ia, if i ∈ [n(n − 1)/2], then (ˆB)a,i = 1; otherwise, i must
be equal to n(n − 1)/2 + a which means that (ˆB)a,i = −1. Therefore,

∀ a ∈ {1, 2}, ∀ i ∈ [n(n + 1)/2],

(ˆBa,i)2

(cid:80)

j∈{1}∪Ia

(ˆBa,j)2

=

(ˆBa,i)2
|Ia| + 1

.

Recall that the normal vector ka is deﬁned as:

∀ a ∈ {1, 2}, ∀ i ∈ [n(n + 1)/2], (ka)i =






19

(cid:113)(cid:80)

(ˆB)a,i

j∈{1}∪Ia

if i ∈ {1} ∪ Ia,

(ˆB)2
a,j

0

otherwise.

It follows that:

∀ a ∈ {1, 2}, ∀ i ∈ [n(n + 1)/2],

(ka)i =






1
(cid:112)|Ia| + 1

1
(cid:112)|Ia| + 1

−

1
(cid:112)|Ia| + 1

if i = 1,

if i ∈ [n(n − 1)/2] and i ∈ Ia,

if i = n(n − 1)/2 + a and i ∈ Ia,

0

otherwise.

Therefore,

e1 ↓ h = e1 −

2
(cid:88)

a=1

ka
(cid:112)|Ia| + 1

.

Taking the inner product with ˆvr ↓ Kr results in:

(cid:104)ˆvr ↓ Kr, e1 ↓ h(cid:105) = (cid:104)ˆvr ↓ Kr, e1(cid:105) −

2
(cid:88)

a=1

(cid:104)ˆvr ↓ Kr, ka(cid:105)
(cid:112)|Ia| + 1

= (ˆvr)1 −

2
(cid:88)

a=1

(cid:104)ˆvr ↓ Kr, ka(cid:105)
(cid:112)|Ia| + 1

.

The inner product (cid:104)ˆvr ↓ Kr, ka(cid:105) can be written as:

(cid:104)ˆvr ↓ Kr, ka(cid:105) =

(ˆvr)1
(cid:112)|Ia| + 1

+

(cid:88)

i∈[n(n−1)/2]∩Ia

(ˆvr)i
(cid:112)|Ia| + 1

−

δa · (ur)a
(cid:112)|Ia| + 1

,

(4)

where δa = 1 if n(n−1)/2+a ∈ supp(ˆxr), and δa = 0 otherwise. This setting of δa arises from the fact
that for i > n(n − 1)/2, if i = n(n − 1)/2 + a ∈ supp(ˆxr), then (vr ↓ Kr)i = (ˆvr)i = (ur)a; otherwise,
(vr ↓ Kr)i = 0. If i ∈ [n(n − 1)/2] ∩ Ia, then i ∈ supp(xr), which implies that (ˆvr)i = (vr)i = 0. So
the middle term in (4) is zero. If δa = 1, the presence of the a-th slack variable in supp(ˆxr) means
that the a-th constraint in Bx ≥ 1n is not tight, implying that (ur)a = 0 by the complementary
slackness. We conclude that

(cid:104)ˆvr ↓ Kr, e1 ↓ h(cid:105) = (vr)1 −

2
(cid:88)

a=1

(vr)1
|Ia| + 1

.

(5)

To invoke Lemma 4.1, we need to show that (cid:104)ˆvr ↓ Kr, e1 ↓ h(cid:105) < 0. The technical hurdle is that
both |I1| and |I2| may be equal to 1 in which case the right hand side of (5) becomes zero. That is,
both p1 and p2 are incident to exactly one edge with weight 1 in the graph induced by ˆxr. This is
a degenerate situation in some sense. We perturb the problem as follows so that |I1| ≥ 2. We add
a dummy point pn+1 arbitrarily close to p1 and insert the edge p1pn+1. We add a new constraint
that forces the weight of p1pn+1 to be an arbitrarily small positive value ε. At the same time, we
need to allow the sums of edge weights incident to p1 to be bounded by 1 + ε from below instead
of 1. We do not allow pn+1 to be connected to any point among p2, . . . , pn. The eﬀect is that we
force p1 to be incident to at least two edges in the graph induced by the current solution. There

20

is at least one edge that connects p1 to another point among p2, . . . , pn because the sum of edge
weights between these points and pi has to be at least 1. There is now also the edge p1pn+1, thus
making |I1| ≥ 2. In the linear algebraic formulation, it means that the ambient space expands to
Rn(n+1)/2+1, ˆxr gains a last coordinate (ˆxr)n(n+1)/2+1 that represents the weight ε of p1pn+1. The
vector ˆvr also gains a corresponding coordinate. There is also a new constraint: (ˆx)n(n+1)/2+1 = ε.
The new index n(n + 1)/2 + 1 belongs to supp(ˆxr) because (ˆxr)n(n+1)/2+1 is forced to be ε > 0. As a
result, (ˆvr)n(n+1)/2+1 must be zero due to complementary slackness. We emphasize that SolveNNQ
is oblivious of this dummy point pn+1 and the problem perturbation associated with it; the problem
perturbation is introduced for the sake of analysis only. Given a solution ˆxr ∈ Rn(n+1)/2+1 that
satisﬁes the KKT conditions with respect to the extended system with the addition of pn+1, one
can simply remove the last coordinate to obtain a solution ˆxr ∈ Rn(n+1)/2 for the original extended
system without pn+1 that satisﬁes the KKT conditions too. The addition of p1pn+1 increases the
dimension of Kr by one, but since (ˆvr)n(n+1)/2+1 = 0, the projection ˆvr ↓ Kr remains unchanged by
the increase in the dimension of Kr. After adding p1pn+1, the aﬃne subspace h is merely translated.
Therefore, there is no eﬀect on the inner product (cid:104)ˆvr ↓ Kr, e1 ↓ h(cid:105) by adding pn+1. Hence, we will
abuse the notation slightly by not changing the notation, staying in Rn(n+1)/2 instead of Rn(n+1)/2+1,
and asserting that |I1| ≥ 2. We conclude that:

(cid:104)ˆvr ↓ Kr, e1 ↓ h(cid:105) = (vr)1 −

2
(cid:88)

a=1

(vr)1
|Ia| + 1

≤ (vr)1 −

(cid:18) 1
3

+

(cid:19)

1
2

(vr)1 =

1
6

(vr)1 < 0.

A side eﬀect is that the dimension of h is positive; otherwise, e1 ↓ h would be the zero vector,

rendering the inner product (cid:104)ˆvr ↓ Kr, e1 ↓ h(cid:105) zero, a contradiction.

We are almost ready to invoke Lemma 4.1 except for an upper bound of (cid:107)ˆvr ↓ Kr(cid:107). We have
a, where δa = 1 if n(n − 1)/2 + a ∈ supp(ˆxr), and δa = 0

(cid:107)ˆvr ↓ Kr(cid:107)2 = (cid:80)
otherwise. We have argued previously that (ur)a = 0 if δa = 1. As a result,

a=1 δa · (ur)2

i + (cid:80)n

(vr)2

i∈Er

(cid:107)ˆvr ↓ Kr(cid:107) =

(cid:115)(cid:88)

i∈Er

(vr)2

i ≤ (cid:112)|Er| · (−vr)1.

By Lemma 4.1, cos ∠(−ˆvr ↓ Kr, e1 ↓ h) ≥ − 1
n(n−1) .
Substituting this result into Lemma 4.2 shows that SolveNNQ reduces f (xr) − f (x∗) by a factor e
in O(λn) iterations.

6 (vr)1 · (cid:0)(cid:112)|Er| · (−vr)1

1
√
|Er|
6

(cid:1)−1 =

≥ 1

6 ·

(cid:113) 2

C.3 Proof of Theorem 4.1

We restate Theorem 4.1 and give its proof.

Theorem 4.1 Consider running SolveNNQ on an MEB, PD, or DKSG problem. Let k =
maxr≥1 |supp(xr)|. For all r ≥ 1, there is a unit descent direction nr from xr that satisﬁes
Lemma 4.1 so that cos ∠(−vr ↓ Kr, nr) ≥ 1/γ, where γ is Θ(
m + n) for PD,
and Θ(n) for DKSG. Suppose that the threshold β1 on the number of iterations is not exceeded,
and that (cid:107)xr − yr(cid:107) ≥ 1
λ (cid:107)xr − x∗(cid:107) for all r ≥ 1, where yr is the feasible point that minimizes f on
the ray from xr in direction nr.

n) for MEB, Θ(

√

√

• MEB: Initialization takes T (d + 1) + O(dn2) time. For r ≥ 1, f (xr+i) − f (x∗) ≤ e−1(f (xr) −

f (x∗)) for some i = O(λ

n). Each iteration takes T (k + β0) + O(kn + β2

0) time.

√

• PD: Initialization takes O(dm2 + dn2) time. For r ≥ 1, f (xr+i) − f (x∗) ≤ e−1(f (xr) − f (x∗))

for some i = O(λ

m + n). Each iteration takes T (k + β0) + O(km + kn + β2

0) time.

√

21

• DKSG: Initialization takes T (n + β0 − 1) + O(dn4 + β2

e−1(f (xr) − f (x∗)) for some i = O(λn). Each iteration takes T (k + β0) + O(kn2 + β2

0) time. For r ≥ 1, f (xr+i) − f (x∗) ≤
0) time.

(cid:3) and a = (cid:2)(cid:107)p1(cid:107)2 . . . (cid:107)pn(cid:107)2(cid:3). We compute AtA
Proof. Consider MEB. Recall that A = (cid:2)p1 . . . pn
(cid:80)n
and a in O(dn2) time. We compute the point 1
i=1 pi and ﬁnd the d + 1 farthest input points
n
from it. We set up the initial active set to contain all indices in [n] except for the indices of these
d + 1 farthest points. We extract in O(d2) time the rows and columns of AtA and coordinates of a
that correspond to the indices of those d + 1 farthest points. We then call the solver in T (d + 1)
time to obtain the initial solution (˜u1, x1). In each iteration afterwards, we compute ∇f (xr) − 1t
n˜ur
and update the active set, which can be done in O(kn) time because xr has at most k non-zero
entries. Since the threshold β1 is not exceeded, we select at most β0 new variables to be freed. We
extract in O(k2 + β2
0) time the rows and columns of AtA and coordinates of a that correspond to
the indices absent from the updated active set. Then, we call the solver in T (k + β0) time. So each
iteration takes T (k + β0) + O(kn + β2
n)
iterations to reduce f (xr) − f (x∗) by a factor e.

0) time. By Lemma 4.2 with γ = Θ(

n), it takes O(λ

√

√

The analysis for PD is similar. The matrix A is [ p1, . . . , pm, −q1, . . . , −qn ] (see Appendix D.4).
We ﬁrst compute AtA in O(dm2 + dn2) time. We pick six arbitrary points, three among p1, . . . , pm
and three among q1, . . . , qn. The initial active set includes all indices in [m + n] except those of
these six selected points. The, we call the solver in T (6, 2) = O(1) time to get the initial solution
(˜u1, x1). In each iteration afterwards, we compute ∇f (xr) − Ct˜ur and update the active set, which
can be done in O(km + kn) time because xr has at most k non-zero entries. Since the threshold β1
is not exceeded, we select at most β0 new variables to be freed. We extract in O(k2 + β2
0) time the
rows and columns of AtA that correspond to the indices absent from the updated active set. Then,
we call the solver in T (k + β0) time. So each iteration takes T (k + β0) + O(km + kn + β2
0) time.
By Lemma 4.2 with γ = Θ(
m + n) iterations to reduce f (xr) − f (x∗) by a
factor e.

m + n), it takes O(λ

√

√

Consider DKSG, We ﬁrst compute AtA. Recall that A is dn × n(n − 1)/2, and A resembles the
incidence matrix for the complete graph in the sense that every non-zero entry of the incidence
matrix gives rise to d non-zero entries in the same column in A. There are at most two non-zero
entries in each column of the incidence matrix. Therefore, computing AtA takes O(dn4) time. We
form an initial graph as follows. Randomly include β0 edges by sampling indices from (cid:2)n(n − 1)/2(cid:3).
Also, pick a node and connect it to all other n − 1 nodes in order to guarantee that the initial
graph admits a feasible solution for the DKSG problem. There are at most n + β0 − 1 edges. The
initial active set contains all indices in the range (cid:2)n(n − 1)/2(cid:3) except for the indices of the edges
0) time the rows and columns of AtA that correspond
of the initial graph. We extract in O(n2 + β2
to the edges of the initial graph. Then, we call the solver in T (n + β0 − 1) time to obtain the
initial solution (u1, x1). At the beginning of every iteration, we compute ∇f (xr) and update the
active set. This can be done in O(kn2) time because xr has at most k non-zero entries. Since the
threshold β1 is not exceeded, at most k + β0 indices are absent from the updated active set. We
extract the corresponding rows and columns of AtA in O(k2 + β2
0) time. The subsequent call of the
solver takes T (k + β0) time. So each iteration runs in T (k + β0) + O(kn2 + β2
0) time. By Lemma 4.2
with γ = Θ(n), we need O(cid:0)λn(cid:1) iterations to reduce f (xr) − f (x∗) by a factor e.

22

D More experimental results

D.1 More results on DKSG and ZHLG

Table 3 shows some average running times of SolveNNQ and quadprog on Iris and HCV. When
n ≥ 130 and n ≥ 13d, SolveNNQ is often 10 times or more faster than a single call of quadprog.
Figure 8 shows the plot of the natural logarithm of the average speedup of SolveNNQ over a single
call of quadprog against ln n for diﬀerent dimension d. For both DKSG and ZHLG and for every
ﬁxed d, the average speedup as a function of n is Ω(n2.7).

n
70

100

130

150

n
70

100

130

160

d
2
3
4
2
3
4
2
3
4
2
3
4

d
2
4
10
2
4
10
2
4
10
2
4
10

DKSG
ours
2.00s
2.09s
2.10s
6.55s
5.92s
5.95s
13.29s
12.20s
13.96s
20.15s
19.81s
21.04s

DKSG
ours
1.16s
1.71s
1.35s
3.50s
4.84s
3.65s
6.90s
12.05s
8.16s
17.32s
23.93s
14.48s

IRIS

nnz
43%
44%
45%
33%
33%
33%
26%
27%
27%
23%
24%
24%

HCV

nnz
43%
44%
43%
32%
33%
31%
28%
27%
24%
24%
22%
20%

quadprog
5.61s
5.66s
4.88s
43.87s
37.56s
42.69s
241.59s
199.22s
256.28s
504.21s
510.55s
565.89s

quadprog
3.68s
5.22s
4.55s
13.02s
39.50s
29.79s
108.10s
180.42s
184.51s
551.43s
868.60s
633.97s

quadprog
2.76s
2.61s
2.23s
17.63s
15.92s
15.24s
148.34s
150.54s
151.87s
414.79s
398.06s
390.47s

quadprog
2.65s
2.13s
1.88s
15.20s
13.03s
12.78s
141.20s
138.05s
133.91s
613.29s
613.04s
588.64s

ZHLG
ours
1.21s
1.15s
1.18s
2.13s
2.64s
2.95s
5.62s
5.34s
6.20s
8.38s
7.80s
7.36s

ZHLG
ours
0.84s
0.88s
2.06s
2.00s
2.35s
8.32s
3.07s
3.78s
21.17s
5.22s
8.10s
45.33s

nnz
47%
48%
50%
34%
36%
37%
29%
28%
30%
25%
26%
26%

nnz
45%
45%
62%
32%
32%
48%
24%
24%
38%
20%
21%
32%

Table 3: Iris and HCV: each data shown for a pair (n, d) is the average of the corresponding data
from ﬁve runs; nnz is percentage of the average number of non-zeros in the ﬁnal solution.

Figure 8: Average speedup of SolveNNQ over a single call of quadprog for DKSG and ZHLG.

D.2 More results on MEB

Figure 9 shows the average running times of SolveNNQ, G99, and FGK03 on 1000 nearly cospherical
points. SolveNNQ overtakes G99 at d = 20. FGK03 is faster than SolveNNQ up to 172 dimensions;
afterwards, SolveNNQ runs much faster than FGK03 as d increases.

23

(a) SolveNNQ vs G99.

(b) SolveNNQ vs FGK03.

Figure 9: Average running times of SolveNNQ, G99, and FGK03 on 1000 nearly cospherical points.

D.3 Image deblurring

Let I denote a two-dimensional r0 × c0 pixel array (an image) We call the pixel at the a-th row
and b-th column the (a, b)-pixel. Let I ∗
a,b denote the intensity of the (a, b)-pixel in the original
image; let I (cid:48)
a,b denote its intensity after blurring. The atmospheric turbulence point spread function
determines how to distribute the intensity I ∗
a,b to other pixels [34] . At the same time, the (a, b)-
pixel receives contributions from other pixels. The sum of these contributions and the remaining
intensity at the (a, b)-pixel gives I (cid:48)
a,b. The weight factor with respect to an (a, b)-pixel and a (c, d)-
pixel is K · exp(cid:0)− (a−c)2+(b−d)2
It means that the contribution
of the (a, b)-pixel to I (cid:48)
a,b. The point spread function is spatially
invariant which makes the relation symmetric; therefore, the contribution of the (c, d)-pixel to I (cid:48)
a,b
is K · exp(cid:0)− (a−c)2+(b−d)2

c,d is K · exp(cid:0)− (a−c)2+(b−d)2

(cid:1) for some positive constant K.

(cid:1) · I ∗

(cid:1) · I ∗

2σ2

2σ2

2σ2

c,d.

The atmospheric turbulence point spread function needs to be truncated as its support is inﬁnite.
We truncate it within a (2σ + 1) × (2σ + 1) square. Let B be such a (2σ + 1) × (2σ + 1) square
centered at the (a, b)-pixel. It means that the (a, b)-pixel has no contribution outside B, and no
pixel outside B contributes to the (a, b)-pixel. For any s, t ∈ [−σ, σ], the weight of the entry of B
at the (a + s)-th row and the (b + t)-th column is K · exp(cid:0)− s2+t2
(cid:1). We divide every entry of B
2σ2
by the total weight of all entries in B because the contributions of the (a, b)-pixel to other pixels
should sum to 1. The coeﬃcient K no longer appears in the normalized weights in B, so we do not
need to worry about how to set K.

How do we transform the blurring to a multiplication of a matrix and a vector? We transpose
the rows of I to columns and stack them in the row order to form a long vector x. That is, the
ﬁrst row of I becomes the top c0 entries of x and so on. Let Ma,b be an array with the same row
and column dimensions as I. Assume for now that the square B centered at the (a, b)-entry of
Ma,b is fully contained in Ma,b. The entries of Ma,b are zero if they are outside B; otherwise, the
entries of Ma,b are equal to the normalized weights at the corresponding entries in B. Then, we
concatenate the rows of Ma,b in the row order to form a long row vector; this long row vector is the
((a − 1)c0 + b)-th row of the matrix A. The product of this ((a − 1)c0 + b)-th row of A and x is a
scalar, which is exactly the sum of the contributions of all pixels at the (a, b)-pixel, i.e., the intensity
I (cid:48)
a,b. Consider the case that some entries of B lie outside Ma,b. If an entry of B is outside Ma,b,
we ﬁnd the vertically/horizontally nearest boundary entry of Ma,b and add to it the normalized
weight at that “outside” entry of B. This ensures that the total weight within Ma,b still sums to
1, thus making sure that no pixel intensity is lost. Afterwards, we linearize Ma,b to form a row of
A as before. In all, Ax is the blurred image according to the atmospheric turbulence point spread

24

function.

bcea

nph

sts

hst

Figure 10: Four space images.

We experimented with four space images bcea, nph, sts, and hst from [2, 3, 7, 8]. We down-
sample and sparsify by setting nearly black pixels (with intensities such as 10 or below, or 20 or
below) to black. Figure 10 shows the four resulting space images. The image bcea is 124 × 83; nph
is 149 × 103; both sts and hst are 128 × 128. We generate blurred images using diﬀerent values of σ.
In running SolveNNQ on the space images, we compute the gradient vector at the zero vector
and select the 20τ most negative coordinates. (Recall that τ = 4 ln2 ν and ν is the total number
of pixels in an image in this case.) We call quadprog with these 20τ most negative coordinates
as the only free variables and obtain the initial solution x1. Afterwards, SolveNNQ iterates as
described before. Table 2 in the main text shows the statistics of the relative mean square errors
and the running times. On the next page, some blurred images are shown; the corresponding image
recovered by SolveNNQ is shown below each blurred image; the value of σ used for blurring and the
relative mean square error is shown between each pair of blurred and recovered images. When the
relative mean square error is well below 0.01, the recovered image is visually non-distinguishable
from the original.

We also experimented with some thermal images walk, heli, lion, and dog [1, 4, 6]; such ther-
mal images are typically encountered in surveillance. In each case, we extract the dominant color,
down-sample, and apply thresholding to obtain a gray-scale image without most of irrelevant parts.
Afterwards, we apply the uniform out-of-focus point spread function for blurring [31]—the atmo-
spheric turbulence point spread function is not relevant in these cases. There is a positive parameter
R. Given an (a, b)-pixel and a (c, d)-pixel, if (a − c)2 + (b − d)2 ≤ R2, the weight factor for them
is 1/(πR2); otherwise, the weight factor is zero. The larger R is, the blurrier is the image. The
construction of the matrix A works as in the atmospheric turbulence point spread function. We
take a certain number of the most negative coordinates of the gradient at the zero vector, and then
we call quadprog with these coordinates as the only free variables to obtain the initial solution x1.
Afterwards, SolveNNQ iterates as described previously.

Figure 11 shows the thermal images, the images extracted from them, the blurred images with
R = 3, the images recovered by SolveNNQ, and the relative mean square errors (with respect to the
images extracted). Table 4 shows some statistics of the experiments. All software achieve a relative
mean square error well below 0.01, so their output are visually non-distinguishable from each other.
In these experiments, the running times of SolveNNQ are very competitive when compared with
the other software.

We did another experiment on deblurring the red, green, and blue color panes of a dog thermal
image separately, followed by recombining the deblurred color panes. Figure 12(a) shows the original
thermal image of a dog. Figures 12(b)–(d) show the gray-scale versions of the red, green, and blue
pixels which have been thresholded for sparsiﬁcation. Figure 12(e) shows the image obtained by

25

σ = 1, rel. err = 4 × 10−5

σ = 1.5, rel. err = 0.001

σ = 2, rel. err = 0.002

σ = 2.5, rel. err = 0.001

σ = 1, rel. err = 10−5

σ = 1.5, rel. err = 4 × 10−4

σ = 2, rel. err = 7 × 10−4

σ = 2.5, rel. err = 4 × 10−4

σ = 1, rel. err = 2 × 10−5

σ = 1.5, rel. err = 4 × 10−4

σ = 2, rel. err = 5 × 10−4

σ = 2.5, rel. err = 0.002

σ = 1, rel. err = 2 × 10−5

σ = 1.5, rel. err = 0.001

σ = 2, rel. err = 0.001

σ = 2.5, rel. err = 0.002

26

combining Figures 12(b)–(d); the result is in essence an extraction of the dog in the foreground. We
blur the diﬀerent color panes; Figure 13 shows these blurred color panes and their recombinations.
We deblur the blurred versions of the diﬀerent color panes and then recombine the deblurred color
panes. The rightmost column in Figure 13 shows the recombinations of the deblurred output of
SolveNNQ. As in the case of the images walk, heli, and lion, the outputs of SolveNNQ, FISTABT,
SBB, FNNLS, lsqnonneg, and a single call quadprog are visually non-distinguishable from each
other. Table 5 shows that the total running times of SolveNNQ over the three color panes are very
competitive when compared with the other software.

Image
data
walk

heli

lion

SolveNNQ

FISTABT

SBB

FNNLS

lsqnonneg

quadprog

rel. err
6 × 10−6
6 × 10−6
10−5
3 × 10−6
2 × 10−6
3 × 10−6
10−6
4 × 10−6
9 × 10−6

time
9.6s
19.1s
33.1s
1.9s
4.6s
8.2s
0.9s
1.6s
3.2s

rel. err
2 × 10−11
2 × 10−11
3 × 10−11
2 × 10−10
2 × 10−10
9 × 10−8
6 × 10−11
6 × 10−11
5 × 10−11

time
350s
1340s
2630s
800s
800s
800s
104s
185s
800s

rel. err
10−7
2 × 10−7
2 × 10−7
3 × 10−7
2 × 10−7
5 × 10−7
6 × 10−8
6 × 10−8
2 × 10−7

time
5.4s
16.4s
43.5s
4.9s
8.1s
36.8s
0.9s
1s
3.7s

rel. err
10−13
2 × 10−13
6 × 10−13
2 × 10−13
5 × 10−13
10−12
5 × 10−14
10−13
4 × 10−13

time
144s
270s
414s
57.2s
103s
157s
4.8s
8.9s
14.1s

rel. err
4 × 10−15
7 × 10−15
10−14
5 × 10−15
10−14
2 × 10−14
3 × 10−15
6 × 10−15
10−14

time
220s
447s
748s
93.8s
187s
289s
6.8s
13s
21.2s

rel. err
6 × 10−6
2 × 10−5
10−5
4 × 10−6
6 × 10−6
8 × 10−6
4 × 10−6
9 × 10−6
7 × 10−6

time
26.9s
76.3s
163s
12.3s
37.2s
54.7s
5.4s
11.7s
20.5

R
2
3
4
2
3
4
2
3
4

Table 4: Results for some thermal images. In the column for each software, the number on the left
is the relative mean square error, and the number on the right is the running time.

walk, thermal

walk, extracted

walk, R = 3

SolveNNQ, rel. err = 6 × 10−6

heli, thermal

heli, extracted

heli, R = 3

SolveNNQ, rel. err = 2 × 10−6

lion, thermal

lion, extracted

lion, R = 3

SolveNNQ, rel. err = 7 × 10−6

Figure 11: Thermal images.

SolveNNQ
green
0.6s
1.3s
1.9s

blue
2.6s
5.1s
7.7s

red
0.5s
1s
2s

FISTABT
green
48.1s
176s
583s

blue
160s
301s
800s

red
362s
663s
800s

R
2
3
4

red
2.1s
2.4s
11.5s

SBB
green
0.7s
1.3s
5.5s

blue
1.5s
2.8s
9.8s

red
4.7s
8.4s
14s

FNNLS
green
4.8s
8.1s
13.2s

blue
30.6s
54.1s
78.1s

red
6.5s
12.2s
19.4s

lsqnonneg
green
6.6s
11.3s
18.6s

blue
42.5s
79.2s
130s

red
8.7s
23.1s
38.6s

quadprog
green
9.3s
26.6s
41.7s

blue
10.6s
30.6s
42.3s

Table 5: Running times on the thresholded, blurred color panes of the dog image.

27

(a) Thermal, original.

(b) Red, thresholded.

(c) Green, thresholded.

(d) Blue, thresholded.

(e) Recombined.

Figure 12: Thermal image of a dog, the three thresholded color panes, and their recombinations.

Red, R = 2

Green, R = 2

Blue, R = 2

Recombined, R = 2

SolveNNQ

Red, R = 3

Green, R = 3

Blue, R = 3

Recombined, R = 3

SolveNNQ

Red, R = 4

Green, R = 4

Blue, R = 4

Recombined, R = 4

SolveNNQ

Figure 13: Blurred color panes, their recombinations, and the recombinations of the deblurred
output of SolveNNQ.

28

D.4 The PD problem

Let p1, . . . , pm and q1, . . . , qn be two input point sets in Rd. The PD problem is determine the
distance between the convex hulls of p1, . . . , pm and q1, . . . , qn. Wolfe [40] designed an algorithm
for the special case that one of the two point sets consists of a single point; he also wrote that the
problem was encountered in the optimization of non-diﬀerentiable functions, approximation theory,
and pattern recognition. Sekitani and Yamamoto [36] proposed an algorithm for the general case.
√
The PD problem is an LP-type problem [33]; it can be solved in O(cid:0)d2n + eO(
d ln d)(cid:1) time, but
the running time increases rapidly in d. G¨artner and and Sch¨onherr [24] proposed a simplex-
like algorithm for convex quadratic programming which can be used to solve the PD problem.
However, it has been noted [21] that this algorithm is ineﬃcient in high dimensions due to the use
of arbitrary-precision arithmetic. The distance between the two polytopes is achieved between some
convex combinations of pi’s and qj’s. In the unknown vector x ∈ Rm+n, the ﬁrst m coordinates store
the weights of the pi’s in their convex combination, and the last n coordinates store the weights
i=1(x)i = 1, (cid:80)m+n
of the qj’s in their convex combination. The constraints are (cid:80)m
i=m+1(x)i = 1, and
x ≥ 0m+n. The objective function is xtAtAx, where A = [ p1 . . . pm − q1 . . . − qn ].

Our experiments are set up as follows. Let n be any ﬁxed value. We draw (cid:98)n/2(cid:99) points uniformly
at random from the d-dimensional cube [−1, 1]d to form a point set P . Similarly, we draw (cid:100)n/2(cid:101)
points to form a point set Q. Then, we slide the second cube along the ﬁrst axis in the positive
direction so that it is at distance s from the ﬁrst cube, where s ∈ {0, 4, 8}. Table 6 shows some
average running times of SolveNNQ and a single call of quadprog. The average running times
of quadprog are more or less insensitive to variations in s and d, except when s = 0. It seems
that quadprog can sometimes take substantially longer time when the polytope distance is tiny; for
example, when (n, d) ∈ {(2000, 100), (6000, 3), (6000, 10), (10000, 3), (10000, 10), (10000, 40)}. Since
SolveNNQ calls quadprog, a similar behavior is also observed for SolveNNQ. In general, the average
running time of SolveNNQ increases very slowly in d for each ﬁxed n. Ignoring the case of s = 0,
for each ﬁxed d and each ﬁxed s, the average speedup of SolveNNQ over a single call of quadprog
is an increasing function of n. Ignoring the case of s = 0, for n ≥ 6000, SolveNNQ is on average
more than 5 times faster than a single call of quadprog for d up to 100.

n
1000

2000

6000

10000

s
0
4
8
0
4
8
0
4
8
0
4
8

d = 3

PD

d = 10

d = 40

d = 100

quadprog
0.11s
0.09s
0.10s
0.51s
0.50s
0.49s
77.95s
8.29s
8.74s
45.77s
33.19s
31.23s

ours
0.04s
0.04s
0.04s
0.16s
0.15s
0.14s
1.52s
1.34s
1.42s
4.71s
3.94s
3.95s

quadprog
0.10s
0.09s
0.11s
0.50s
0.53s
0.54s
10.86s
8.31s
9.26s
152.59s
31.80s
31.17s

ours
0.06s
0.04s
0.03s
0.19s
0.14s
0.14s
1.71s
1.42s
1.43s
5.69s
4.93s
4.18s

quadprog
0.09s
0.09s
0.10s
0.59s
0.63s
0.65s
9.27s
8.88s
9.81s
40.83s
34.82s
37.34s

ours
0.08s
0.06s
0.05s
0.30s
0.18s
0.18s
2.55s
1.77s
1.59s
7.69s
4.40s
4.66s

quadprog
0.10s
0.09s
0.10s
0.77s
0.60s
0.66s
9.23s
8.56s
9.78s
33.53s
32.26s
37.01s

ours
0.10s
0.06s
0.05s
0.38s
0.22s
0.17s
2.98s
1.86s
1.66s
9.08s
6.15s
6.04s

Table 6: Average running times of SolveNNQ and a single call of quadprog for the PD problem on
random points from a cube. Each running time is the average over ﬁve runs.

29

