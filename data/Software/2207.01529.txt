2
2
0
2

l
u
J

4

]

C
H
.
s
c
[

1
v
9
2
5
1
0
.
7
0
2
2
:
v
i
X
r
a

Cybersecurity Discussions in Stack Overflow: A Developer-Centred Analysis of
Engagement and Self-Disclosure Behaviour

NICOLÃS E. DÃAZ FERREYRA, Hamburg University of Technology, Germany
MELINA VIDONI, Australian National University, Australia
MARITTA HEISEL, University of Duisburg-Essen, Germany
RICCARDO SCANDARIATO, Hamburg University of Technology, Germany

Stack Overflow (SO) is a popular platform among developers seeking advice on various software-related topics, including privacy and

security. As for many knowledge-sharing websites, the value of SO depends largely on usersâ€™ engagement, namely their willingness to

answer, comment or post technical questions. Still, many of these questions (including cybersecurity-related ones) remain unanswered,

putting the siteâ€™s relevance and reputation into question. Hence, it is important to understand usersâ€™ participation in privacy and

security discussions to promote engagement and foster the exchange of such expertise. Objective: Based on prior findings on online

social networks, this work elaborates on the interplay between usersâ€™ engagement and their privacy practices in SO. Particularly, it

analyses developersâ€™ self-disclosure behaviour regarding profile visibility and their involvement in discussions related to privacy and

security. Method: We followed a mixed-methods approach by (i) analysing SO data from 1239 cybersecurity-tagged questions along

with 7048 user profiles, and (ii) conducting an anonymous online survey (N=64). Results: About 33% of the questions we retrieved

had no answer, whereas more than 50% had no accepted answer. We observed that proactive users tend to disclose significantly less

information in their profiles than reactive and unengaged ones. However, no correlations were found between these engagement

categories and privacy-related constructs such as perceived control or general privacy concerns. Implications: These findings contribute

to (i) a better understanding of developersâ€™ engagement towards privacy and security topics, and (ii) to shape strategies promoting the

exchange of cybersecurity expertise in SO.

CCS Concepts: â€¢ General and reference â†’ Empirical studies; â€¢ Security and privacy â†’ Social aspects of security and privacy.

Additional Key Words and Phrases: stack overflow, usable privacy and security, engagement, self-disclosure, r programming, python

ACM Reference Format:

NicolÃ¡s E. DÃ­az Ferreyra, Melina Vidoni, Maritta Heisel, and Riccardo Scandariato. 2022. Cybersecurity Discussions in Stack Overflow:

A Developer-Centred Analysis of Engagement and Self-Disclosure Behaviour. In Woodstock â€™18: ACM Symposium on Neural Gaze

Detection, June 03â€“05, 2018, Woodstock, NY . ACM, New York, NY, USA, 16 pages. https://doi.org/XXXX

1 INTRODUCTION

The last decade has put privacy in the spotlight of software development, as new legal frameworks emerged to safeguard

peopleâ€™s data protection rights and promote responsible engineering practices. One clear example is the EU General

Data Protection Regulation (GDPR) [24] which has introduced strong legal provisions seeking to enforce software

companies to comply with a set of privacy principles including transparency, fairness, and informed consent. More

recently, as the software industry moves towards the development of Artificial Intelligence (AI) applications, a new

regulatory framework is in sight [12], promising to strengthen the protection and governance of personal data in AI

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

Â© 2022 Association for Computing Machinery.
Manuscript submitted to ACM

1

 
 
 
 
 
 
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

DÃ­az Ferreyra et al.

systems. In turn, companies and organisations have been urged to adopt privacy-by-design practices to comply with

current regulations. Nevertheless, this has also raised questions and concerns among software developers on how to

effectively translate these legal provisions and privacy principles into technical solutions [30].

Question-Answer (Q&A) platforms are a valuable resource for both experienced and junior programmers seeking

support in their software development tasks. Stack Overflow (SO) [33] is among the largest Q&A platforms in which

developers participate in discussions related to performance issues, bugs, and code workarounds [4]. Given the increasing

importance of cybersecurity in software engineering, a large number of questions regarding privacy, security, and

data protection have been posited and addressed by SO users. Particularly, issues related to GDPR compliance, privacy

policies, and access-control are some of the most popular privacy-related discussions in SO [22, 38]. Still, privacy- and

security-related topics receive little attention in comparison to others such as data science, big data, and mobile operating
systems1. Albeit this suggests a low engagement towards cybersecurity discussions within the SO community, it also
reveals an overall tendency among software developers to overlook privacy and security aspects of their code [6, 15, 28].

1.1 Motivation

Developers play a key role in embedding privacy and security principles into the core architecture of information

systems [15]. However, many often fail to create secure software solutions that successfully preserve usersâ€™ privacy

and data protection rights [15, 28]. Over the last years, a growing body of research has leveraged the SOâ€™s dataset to

identify and characterise cybersecurity trends among software practitioners. Prior work has investigated developersâ€™

motivations [22], knowledge gaps [38], and concerns towards privacy and security [21]. However, â€œanswer-hungryâ€

questions are still a common phenomenon and an ongoing issue within Q&A websites (i.e., questions remaining

unanswered or unresolved) [14]. Being SO a community frequented by more than 100 Million developers per month [31],

usersâ€™ commitment towards timely and high-quality answers becomes critical for the platformâ€™s reputation and success.

Former research has sought to understand usersâ€™ motivations (and amotivations) when it comes to participation in

Q&A forums [2, 10, 41]. Yet, little effort has been made to characterise usersâ€™ engagement in cybersecurity discussions

in SO. That is, on providing evidence and actionable information about community members participating actively (or

not) in such exchanges.

Individualsâ€™ engagement in Online Social Networks (OSNs) like Facebook has been extensively investigated from the

perspective of privacy concerns. Such research has analysed the connection between usersâ€™ self-disclosure decisions (e.g.,

the amount of private information they reveal inside profiles and posts) and their engagement in these platforms (e.g.,

number and quality of OSN posts) [9, 17, 36]. Overall, such research has not only contributed to a better understanding of

usersâ€™ privacy concerns and practices but has also paved the road for the development of user-centred technologies. That

is, for the elaboration of methods and tools aiming to support and guide usersâ€™ interaction in OSN environments [27].

However, to the extent of our knowledge, the role of privacy-related behaviour has not been closely investigated within

Q&A platforms like SO. Particularly, the interplay between developersâ€™ self-disclosure practices and their engagement

in discussion threads has not been yet explored under the lens of privacy and security benchmarks.

1.2 Contribution and Research Questions

SO is a valuable resource for developers seeking advice about multiple aspects of software development. Given the

increasing importance of cybersecurity in software engineering, it becomes necessary to foster the engagement among

1By May 2021, the amount of security- and privacy-related questions was around 53.000, whereas for Android and iOS it was over 1.900.000 [34].

2

Cybersecurity Discussions in Stack Overflow

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

its users towards privacy- and security-related discussions. Hence, this work aims at contributing to ongoing research

in SO by investigating the interplay between usersâ€™ self-disclosure decisions and their engagement in cybersecurity

discussions. All in all, the research questions (RQs) this paper seeks to answer are:

â€¢ RQ1: Are usersâ€™ self-disclosure behaviour associated with their engagement in cybersecurity discussions? Prior studies
in OSNs (in general) and Q&A platforms (in particular) have shown correlations between usersâ€™ engagement and

self-disclosure practices (e.g., [2, 17, 39]). Hence, this RQ aims at zooming into developersâ€™ decisions regarding

profile visibility and their participation in discussions about privacy and security. Particularly, it seeks to

investigate whether different self-disclosure patterns exist across SO users who involve themselves actively in

such discussions, and those who do not.

â€¢ RQ2: Are privacy-related constructs associated with usersâ€™ engagement in cybersecurity discussions? As with RQ1,
former studies have delved into the relation between psychological constructs (e.g., perceived risks and control)

and peoplesâ€™ engagement within OSNs (e.g., [16, 36]). The purpose of this RQ is to examine whether such

correlations also take place in SO but regarding usersâ€™ participation in discussions about privacy and security.

To answer these RQs, we have followed a mixed-method approach combining the analysis of data collected from an

online survey and information retrieved from SO user profiles. The results of our analysis show significant differences

in the self-disclosure practices (i.e., with regard to profile visibility) of users contributing actively to discussions

about data protection and information security, and those who do not. These findings not only contribute to a better

understanding of usersâ€™ engagement in such discussions, but also to solutions addressing â€œanswer-hungryâ€ questions

in Q&A platforms. Particularly, for the elaboration of incentive strategies and recommender systems promoting the

exchange of cybersecurity expertise in SO.

Paper Structure. Section 2 discusses related work and gives and overview of the paperâ€™s theoretical background.

Section 3 describes the methodology employed for the study in terms of data collection, aggregation, and survey design.

Section 4 reports the results of our analysis, and Section 5 discusses them. Section 6 summarises limitations and threats

to validity. Section 7 concludes this work.

2 BACKGROUND AND RELATED WORK

A growing amount of literature has zoomed into cybersecurity discussions in SO and engagement patterns in OSNs.

This section summarises related work elaborating on privacy and security insights gathered through SO. Alongside, we

discuss research addressing privacy concerns as a rationale for usersâ€™ engagement and self-disclosure behaviour in

OSNs.

2.1 Cybersecurity Discussions in SO

Given the Q&A affordances available within SO, this platform has been widely used as a proxy for understanding the

cybersecurity concerns and practices of software engineers [13, 21, 22, 38]. For instance, Lopez et al. [22] conducted a

qualitative analysis of SO discussion threads to understand the type of security support developers seek and provide

online. Their findings suggest that security-related discussions in SO are rich in terms of technical help but also regarding

developersâ€™ personal values and attitudes such as trust, fear, and sense of responsibility. In a follow-up article [21], the

authors gathered further insights on how security knowledge is built and fostered within the SO community. Overall,

their results show that developers often tend towards security-related discussions within the context of technical

solutions provided by others. In line with this, Tahaei et al. [38] applied natural language processing techniques to

3

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

DÃ­az Ferreyra et al.

unveil topics emerging within privacy-related questions. The outcome of such an analysis showed that privacy policies,

access-control, and encryption are among the main privacy topics addressed by SO members.

At its core, SO is a peer-production community where knowledge is built from the interaction between developers

seeking to clarify each otherâ€™s technical inquires [29]. Hence, usersâ€™ participation and engagement are of utmost

importance for the sustained development of the platform and the expertise crafted within it. Moreover, timely

answers to questions are critical to the platformâ€™s efficiency and, thus, to its popularity. Nonetheless, prior research has

systematically reported that many questions in SO receive little attention or even remain unanswered/unresolved (up to

30% by May 2022 [32]). As a catalyst for developersâ€™ technical concerns and best practices, it is essential to understand

the factors contributing to or impairing usersâ€™ participation in SO. Prior work has tried to explain why some questions

remain unanswered and even proposed machine learning models for predicting whether specific questions will be

addressed or not [3]. Still, the low engagement and the lack of answers to specific questions (including privacy- and

security-related ones) remain open issues [14]. Hence, there is a call for empirical evidence to (i) help characterise

usersâ€™ engagement in cybersecurity discussions and (ii) elaborate strategies for boosting their participation in such

discussions.

2.2 Insights from Online Social Networks

Factors influencing peopleâ€™s participation in OSNs have been thoroughly investigated through the lens of privacy

concerns. Moreover, prior work has closely analysed usersâ€™ privacy practices, often accounting for correlations between

OSN engagement and self-disclosure behaviour. Staddon et al. [36], for instance, observed strong associations between

privacy concerns and usersâ€™ engagement on Facebook using an online survey. Their findings revealed that individuals

expressing concerns about their privacy also report spending less time on the platform and sharing less content. Hence,

they concluded that privacy concerns might play a significant role in peopleâ€™s engagement in OSNs. In line with this,

a study by Choi and Sung [9] showed that privacy concerns are closely associated with active Instagram use (e.g.,

sharing content and interacting more with others) and peopleâ€™s selection of a particular OSN platform over others (e.g.,

Instagram over Snapchat). Alongside, research has systematically reported evidence on the so-called â€œprivacy paradoxâ€,

showing offsets between usersâ€™ concerns and engagement in OSNs [18]. Such evidence suggests that, despite expressing

privacy concerns, people still join OSNs and disclose significant amounts of personal information.

When it comes to engagement in Q&A platforms, Kayes et al. [17] investigated the interplay between usersâ€™ privacy

concerns and their participation in Yahoo! Answers. By considering changes in profile visibility as manifestations

of privacy concerns, the authors unveiled correlations between usersâ€™ self-disclosure behaviour and their platform

contributions. Overall, they observed that users with a private profile contribute more often and with better content

to the platform than those with a public one. Such findings can contribute substantially to the elaboration of Q&A

recommendation approaches. For instance, one could leverage profile visibility for rooting unresolved questions to those

users who are more likely to answer them [17]. Surprisingly, concerns and practices alike have not been thoroughly

investigated in SO despite its Q&A and social network affordances. Moreover, to the extent of our knowledge, the

relationship between engagement in cybersecurity topics and self-disclosure practices has not been yet explored nor

investigated from a developer-centred perspective.

3 METHODOLOGY

We conducted a two-stage empirical study to identify nuances in the self-disclosure practices of users participating

actively in cybersecurity discussions, and those who do not. For this, we created a dataset from 7048 SO profiles

4

Cybersecurity Discussions in Stack Overflow

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

corresponding to engaged and unengaged users during the first stage of the study. This dataset was then leveraged

on the second stage to conduct an anonymous online survey. Both experimental stages are described in detail in the

following subsections.

3.1 Data Collection

To identify users concerned with cybersecurity topics, we first conducted an analysis of privacy- and security-related

conversations in SO. Such an analysis consisted in the identification of cybersecurity-relevant conversation threads
through their corresponding user-assigned tags. For this, we used SOâ€™s Tag Explorer2 for the definition of tag sets
which were used thereafter to mine relevant conversations. Particularly a set of topic tags plus two language tags were

employed in the identification of cybersecurity-relevant discussions.

We included privacy, privacy-policy, security, code-access-security, data-security, network-security, and
gdprconsentform as topic tags3. Additionally, r and python were used as language tags given the increasing popularity
of these languages within the data science community [23]. Thereby, we sought to narrow down the scope of the study

mainly to data science practitioners as they are prone to handle sensitive data (e.g., medical records, biometric data,

demographics). Furthermore, their cybersecurity practices can have a great impact on automated decision-making

systems (e.g., biases, discrimination).

3.1.1 Discussions Dataset (D1). Each topic tag was explored in combination with each language tag, resulting in 14

tag searches. To maximise the size of the dataset, we did not include additional restrictions such as time of posting,

the existence of an approved answer, upvotes, or downvotes. Both search and extraction were executed through an
R-based mining package included in the StackExchange API4. We conducted fourteen independent searches (i.e., one
per tag combination) using the search/advanced endpoint and a tag filter provided by the API itself. By the end of

the mining process, a total of 1239 questions/posts were retrieved from SO (Figure 1).

Questions posted in SO can be answered or commented on by other platform members. The main difference is that

the latter asks for clarification instead of describing a suitable solution. One question can trigger several answers and

comments (to the main question or to othersâ€™ answers) from other SO users interested in the discussion topic. Therefore,

such comments and answers are also relevant for identifying SO profiles corresponding to individuals who engage in

cybersecurity discussions. Consequently, answers and comments associated with each of the 1239 questions were also
mined and included in a discussions dataset ğ·1. After this additional mining process, ğ·1 contained 1239 questions,
2558 comments to questions, 1811 answers, and 2373 comments to answers.

3.1.2 Profiles Dataset (D2). The information contained in ğ·1 allowed us to identify the SO ids of those users who
have either posted a question, provided an answer, or posted a comment deemed as cybersecurity-relevant. Overall,

3591 unique ids were retrieved, from which only 17 corresponded to users with fully private SO profiles. The remaining

3574 ids were used to mine the public information disclosed in their profiles through the StackExchange API (i.e., via

the users/{ids} endpoint).

The email address of some of them was also mined using the GitHub (GH) URL available in the profiles (email

addresses are never included in SO profile pages). This step was necessary to recruit participants afterwards for the
online survey. This complementary mining process was executed using the R package gh5 resulting in 457 unique e-mail

2See: https://stackoverflow.com/tags
3It is worth mentioning that, at the moment of conducting this study, these were the only cybersecurity-related tags available in SO.
4See: https://api.stackexchange.com/
5See: https://cloud.r-project.org/web/packages/gh/index.html

5

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

DÃ­az Ferreyra et al.

Fig. 1. Mining process followed to extract and generate both datasets.

addresses corresponding to engaged users. Such information was included in the profiles dataset ğ·2 along with the rest
of the profile information extracted from SO.

In order to populate ğ·2 with profile information from unengaged users, we first estimated a representative sample
size for such a subgroup. For this, we run a query to determine how many users have participated on each language
tag6 using Stack Exchangeâ€™s Data Explorer (SEDE). The result of this query gave 46038 users for the r tag, and 777587
for python tag. Next, we mined the profile information from a representative sample of these two groups with a 99%

confidence and a margin of error of 3%. Such information was mined directly from the users/{uids} endpoint, ensuring

that the corresponding SO ids were not already part of the engaged group, and were not repeated across each language.
Overall, we obtained 1830 Python users and 1645 R users (3475 in total). These results were merged into the ğ·2
dataset, using an additional variable to indicate whether this information corresponds to engaged or unengaged users.

Like with the engaged profiles, we collected the e-mail addresses of 413 unengaged users via GH (Figure 1).

3.2 Data Aggregation

We parsed the information collected in both datasets to compute two variables of interest: (i) the amount of information

users disclose in their profiles, and (ii) their engagement in cybersecurity discussions. The following subsections describe

these variables plus an additional analysis we conducted to understand self-disclosure through display names.

3.2.1 Amount of Self-Disclosure. SO allows users to include the following information in their profiles: display

name (with a maximum of 30 characters), location (as a text field), title (available in the profile, but merged into the

display name when using the API), about me (HTML-friendly text box of up to 3000 characters), a website link, links to

Twitter and GitHub profiles, and a profile picture (if not used, the system assigns a randomised avatar). To compute

a metric reflecting the amount of personal information revealed in a profile, we assigned a normalised variable (i.e.,

ranging from 0 to 1) to each field except for the title. The value for each particular variable was estimated as follows:

â€¢ We gave each link (website, Twitter and GitHub) a value of 1 if it was filled in the userâ€™s profile, and 0 if not.

6Query: https://data.stackexchange.com/stackoverflow/query/1392147

6

Cybersecurity Discussions in Stack Overflow

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

â€¢ The location variable was calculated as the links (i.e., 1 if it was completed and 0 if not). Since users can obfuscate
this field (e.g., by using nicknames or aliases), we conducted a card sorting analysis to estimate the reliability of

this coding schema. From this analysis, we concluded that location information could be considered accurate if

present.

â€¢ The variable corresponding to the display name was computed as the proportion of used characters over the
total available (30 characters). As with location, we completed another card sorting analysis to obtain further

reliability insights. Once again, we concluded that the information present in this field could be considered

accurate. Both card-sorting analyses can be found in the paperâ€™s Replication Package.

â€¢ The profile image was retrieved as an URL address during the data collection process. To determine whether an
image corresponds to a custom or a default one we compared its URL against a collection of Gravatar7 URLs
(Gravatar pictures are frequently used as default in SO profiles). Using regular expressions, we assigned a 0 value

to those profile pictures found in the Gravatar database. Otherwise, they were considered as custom and given a

value of 1.

â€¢ The about me field can have up to 3000 characters allowing HTML formatting. The HTML tags were removed
through an R script, and the proportion of used characters was calculated to determine the corresponding

disclosure value of this field. This approach assumes that, as more characters are included, more personal

information is being revealed.

These normalised variables were aggregated into another variable named ğ‘ ğ‘œğ‘ƒğ‘Ÿğ‘œ ğ‘“ ğ·ğ‘–ğ‘ ğ‘ğ‘™ğ‘œğ‘ ğ‘¢ğ‘Ÿğ‘’ quantifying the amount

of personal information disclosed in a SO profile:

ğ‘ ğ‘œğ‘ƒğ‘Ÿğ‘œ ğ‘“ ğ·ğ‘–ğ‘ ğ‘ğ‘™ğ‘œğ‘ ğ‘¢ğ‘Ÿğ‘’ =

ğ‘ğ‘¡ğ‘¡ğ‘ ğ‘‰ ğ‘–ğ‘ ğ‘–ğ‘ğ‘™ğ‘’ğ¼ğ‘›ğ‘ƒğ‘Ÿğ‘œ ğ‘“ ğ‘–ğ‘™ğ‘’
ğ‘šğ‘ğ‘¥ğ´ğ‘šğ‘œğ‘¢ğ‘›ğ‘¡ğ‘‚ ğ‘“ ğ·ğ‘–ğ‘ ğ‘ğ‘™ğ‘œğ‘ ğ‘ğ‘ğ‘™ğ‘’ğ´ğ‘¡ğ‘¡ğ‘ 

where ğ‘šğ‘ğ‘¥ğ´ğ‘šğ‘œğ‘¢ğ‘›ğ‘¡ğ‘‚ ğ‘“ ğ·ğ‘–ğ‘ ğ‘ğ‘™ğ‘œğ‘ ğ‘ğ‘ğ‘™ğ‘’ğ´ğ‘¡ğ‘¡ğ‘  corresponds to the maximum number of disclosable attribute values (7 in

total), and ğ‘ğ‘¡ğ‘¡ğ‘ ğ‘‰ ğ‘–ğ‘ ğ‘–ğ‘ğ‘™ğ‘’ğ¼ğ‘›ğ‘ƒğ‘Ÿğ‘œ ğ‘“ ğ‘–ğ‘™ğ‘’ to the summation of each normalised variable.

3.2.2 Engagement in Cybersecurity Discussions. We classified users into engaged or unengaged, given their
participation by computing the number of cybersecurity-relevant questions a user has posted (#ğ‘„), the number of
answers provided to such questions (#ğ´), and of corresponding comments. This last one was divided into comments to
cybersecurity questions (#ğ¶ğ‘„ ) and comments to cybersecurity answers (#ğ¶ğ´). Overall, if the sum #ğ‘„ + #ğ´ + #ğ¶ğ‘„ + #ğ¶ğ´
was greater than 0, then the user was classified as engaged and, otherwise, as unengaged.

Also, we classified engaged users into proactive and reactive according to their tendency towards starting new

discussion threads. Particularly, we considered proactive users to those who place more questions than comments and
answers. That is, in cases where #ğ‘„ â‰¥ #ğ´ + #ğ¶ğ‘„ + #ğ¶ğ´. Conversely, users posting more comments and answers than
cybersecurity questions were classified as reactive. That is, when #ğ‘„ < #ğ´ + #ğ¶ğ‘„ + #ğ¶ğ´.

3.3 Survey Structure

To complement the analysis of profile information and discussion threads, we conducted an online survey within

a subgroup of SO users. In particular, we aimed at measuring psychological constructs and antecedents to better

understand developersâ€™ concerns and behaviour regarding cybersecurity. The questionnaire consisted of an introductory

part and two main sections:

7See: https://en.gravatar.com/

7

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

DÃ­az Ferreyra et al.

i. The introductory section provided information about the aim of the study along with the conditions for

participation/withdrawal (participation was voluntary, and people were given a chance to withdraw at any time).

We also included the contact details of the authors in case of further questions and enquiries.

ii. After accepting the surveyâ€™s terms and conditions, participants were forwarded to the first part of the questionnaire.

This part included questions eliciting demographic information (e.g., participantsâ€™ gender, education level, and

current work status) along with their prior experience in software development (e.g., years working with R or

Python).

iii. The second part included a set of questions measuring the following constructs: general privacy concerns (GPC),

privacy concerns on social threats (PCS), privacy concerns on organisational threats (PCO), perceived privacy risk

(RSK), perceived control (PC), and self-disclosure (SD). We used well-established constructs and scales previously

elaborated and validated by other authors (i.e., GPC by Buchanan et al. [8] and the rest by Krasnova et al. [19]). All

questions were close-ended and measured using a 6-Point Likert scale to increase the responsesâ€™ reliability. We also

included an attention question by the end of this section to identify careless respondents and preserve the quality

of the results [20].

This survey was assessed and approved by an Ethics Committee, and is also available in the Replication Package.

Population & Sampling. The survey was distributed through Qualtrics in April/May 2021 using the email addresses

collected during the mining process (Section 3.1.2). We gathered 69 responses, out of which five were filtered through

the â€œattention controlâ€ question. The remaining 64 responses were considered for the corresponding analysis.

4 RESULTS

We conducted several statistical analyses over the information collected from SO and the responses obtained through the
online survey. We conducted a ğ‘¡-Test followed by an ANOVA test to identify significant differences in the self-disclosure
practices of engaged and unengaged users. The results of these tests were complemented afterwards with an analysis of

the survey data.

4.1 Privacy and Security Discussions (SO Q&A data)

A total of 1239 cybersecurity-related questions were collected from SO using the StackExchange API (as explained in

Section 3.1.1). As shown in Table 1, around 67% of these questions had at least one answer (answered), and about 47%

received an answer considered adequate by the user who asked the question (accepted). Another 58% had a positive

score (i.e., a positive difference between up-votes and down-votes), whereas 39% of the questions received at least one

comment. SO also allows experienced community members to close questions that are either off-topic or may need

further clarification. We observe that around 8% of the questions in our dataset fall into this category.

Table 1. Question status indicators.

Has Answers Has Accepted Answers Has Score > 0 Has Comments Closed

Frequency
% Total

825
67%

588
47%

719
58%

489
39%

94
8%

8

Cybersecurity Discussions in Stack Overflow

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

4.2 Self-Disclosure Practices (SO profile data)

As mentioned in Section 3.1.2, from the 7049 profiles retrieved from SO, 3574 correspond to engaged users and 3475 to

unengaged ones. Figure 2 illustrates the disclosure frequency of each profile attribute in our sample for each group of

users. We can see that such frequencies are quite even across all attributes for both groups and that â€œdisplay nameâ€

is an attribute everyone discloses. We can also observe that â€œlocationâ€ and â€œabout meâ€ are among the most revealed

profile attributes, whereas â€œhas Twitterâ€ is the least frequent one.

Fig. 2. Profile attributes disclosed by unengaged and engaged users (frequencies).

We ran an independent samples ğ‘¡-Test to identify significant differences in the amount of profile information disclosed
by engaged and unengaged users. Since Leveneâ€™s test for equality of variances resulted significant (ğ¹1,7047 = 6.605,
ğ‘ = 0.10), the corresponding ğ‘¡ statistic was computed without assuming homogeneity of variances. Overall, we found no
significant differences in the average amount of self-disclosure between engaged and unengaged users (ğ‘¡7027.424 = 0.918,
ğ‘ > 0.05). This can be observed in Figure 3-a. Hence, we conducted a follow-up ANOVA test to determine whether
such differences exist among unengaged, proactive, and reactive users.

Fig. 3. Average self-disclosure of (a) unengaged and engaged users, and (b) unengaged, proactive, and reactive users.

From the 3574 concerned profiles, 716 corresponded to reactive users and 2858 to proactive ones. In principle,

we can observe differences in the amount of profile information disclosed across these 3 groups (Figure 3-b). After

9

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

DÃ­az Ferreyra et al.

conducting the ANOVA test (Table 4), we could confirm that such differences were indeed statistically significant
(ğ¹2,7046 = 86.180, ğ‘ < 0.05). To determine where these differences actually occur, we ran an additional non-parametric
posthoc test. We chose a Games-Howell test since Leveneâ€™s statistic suggested no equal variances within the sample
(ğ¹2,7046 = 31.772, ğ‘ < 0.05). This analysis revealed significant differences (ğ‘ > 0.05) in the average amount of
self-disclosure across all paired groups (Table 2). That is, between unengaged-proactive (âˆ’0.107), unengaged-reactive
(âˆ’0.020), and proactive-reactive (âˆ’0.127).

Table 2. Games-Howell Test for Differences of Means

Diff. Levels

Diff. Means

SE

ğ’‘

95% CI

Unengagedâ€”Proactive
Unengagedâ€”Reactive

Proactiveâ€”Unengaged
Proactiveâ€”Reactive

Reactiveâ€”Unengaged
Reactiveâ€”Proactive

0.107*
-0.020*

-0.107*
-0.127*

0.020*
0.127*

0.008
0.006

0.009
0.008

0.006
0.009

0.000
0.002

0.000
0.000

0.002
0.000

(0.086, 0.127)
(-0.034, -0.006)

(-0.127, -0.086)
(-0.148, -0.106)

(0.006, 0.034)
(0.106, 0.148)

* The mean difference is significant for ğ›¼ = 5%.

Finally, we conducted a multinomial logistic regression to obtain further insights on the self-disclosure practices of

SO users. For this, we considered the unengaged users as the baseline category against which the other groups (i.e.,

proactive and reactive) should be compared. The parameter estimates of the resulting model are summarised in Table 3.

As it can be observed, the percentage of information disclosed in a profile (% self-disclosure) is a significant predictor for
both proactive and reactive user categories (ğ‘ < 0.05).

On the one hand, for every one-unit increase on % self-disclosure, the likelihood a user has of falling in the proactive

category decreases by 2.2% (i.e., relative to falling in the unengaged group). Conversely, such a likelihood increases by

0.4% for the reactive category. This model is a significant improvement in fit over an intercept model with no predictors
2 = 181.388, ğ‘ < 0.001). However, it does not fit well to the data, which makes it not adequate for prediction purposes
(ğœ’ 2
(Pearsonâ€™s ğœ’ 2

170 = 256.978, ğ‘ < 0.05).

Table 3. Multinomial Logistic Regression (estimates).

Group

proactive

reactive

B
âˆ’0.991
intercept
% self-disclosure âˆ’0.023
âˆ’0.313
intercept
0.004
% self-disclosure

SE

Sig.

Exp(B)

0.062
0.002
0.043
0.001

0.000
0.000
0.000
0.001

0.978

1.004

4.3 Privacy-Related Constructs (survey data)

As shown in Table 5, 76.56% of the survey respondents worked full time and had more than 10 years of programming

experience. Another 75% reported having more than 5 years of experience working with R or Python, and 56.25% having

a graduate degree. In terms of gender, 61 out of the 64 participants were male, 1 was a woman, 1 non-binary, and 1

preferred not to reveal it.

10

Cybersecurity Discussions in Stack Overflow

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Following the same user categories investigated in Section 4.2, we conducted a one-way ANOVA test to analyse the

privacy-related constructs elicited in the second part of the survey (i.e., GPC, PCS, PCO, RSK, PC, and SD). From the

64 participants, 33 were classified as unengaged, 8 as proactive, and 23 as reactive. Prior to conducting the test, we

assessed the reliability of the employed scales by calculating their corresponding Cronbachâ€™s Alpha coefficient. In all

the cases, such a value was higher than 0.7 suggesting a high internal consistency within each scaleâ€™s items.

Table 4 also summarises the outcome of the one-way ANOVA for each constructs measured. We found no significant

differences in any of these constructs across proactive, reactive, and unengaged users. This was also the case when
conducting a ğ‘¡-Test for a two-group classification (i.e., engaged and unengaged).

Table 4. One-way ANOVA Test (profile and survey data).

Variable

SS

d.f. MS

F

ğ’‘

Profile data

% self-disclosure

9.340

Survey data

GPC
PCST
PCOT
RSK
PC
SD

0.825
0.400
3.860
0.547
2.174
3.082

2

2
2
2
2
2
2

4.670

86.180

0.000

0.412
0.200
1.930
0.274
1.087
1.541

0.333
0.141
1.127
0.384
0.854
1.040

0.718
0.869
0.331
0.682
0.431
0.360

5 DISCUSSION

This section discusses the results of our study and provides answers to the paperâ€™s research questions. We also elaborate

on the implications of our findings within the area of developer-centred security, namely the elaboration of strategies

for boosting the participation of SO users in cybersecurity discussions.

5.1 Engagement and Self-Disclosure Behaviour (RQ1)

Our findings suggest that SO users with a tendency towards starting cybersecurity discussions disclose significantly less

information in their profiles than others who do not (Section 4.2). Similar observations were made by Kayes et al. [17]

in a study about peoplesâ€™ engagement in the Q&A platform Yahoo! Answers. The authors found correlations between

usersâ€™ self-disclosure behaviour (i.e., profile visibility preferences), the frequency, and the quality of their contributions.

Particularly, individuals with a more restrictive profile tend to contribute more and with better content than those

with a public one. Furthermore, such users also showcase higher retention levels (i.e., average time interval between

contributions) and have a higher perception on answer quality.

On the other hand, our results also show that reactive users not only reveal more profile information than proactive

ones, but also more than those unengaged. Such a finding is to some extent aligned with prior research on identity

formation in Q&A platforms. To a certain extent, participation in SO is driven by usersâ€™ need for recognition within the

platform. That is, in terms of points and badges that users can assign to each other based on the perceived quality of their

contributions [41]. For instance, a study conducted by Adaji and Vassileva [2] showed that high-quality questions are

frequently posted by users with complete profile information. Vargo and Matsubara [39] also made similar observations

11

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

DÃ­az Ferreyra et al.

Table 5. Survey Self-Reported Demographic Data.

Demographic Ranges

Freq.

Resp. (%)

Gender

Educational
Level

Employment
Status

Programming
Experience
(R/Python)

Female
Male
Non-Binary
Prefer not to say

Graduate Degree (MSc, PhD)
High School or Less
Some College
Undergrad Degree (BSc, BA)

Currently in School
Currently in University
Unemployed, not looking for work
Unemployed, looking for work
Working full-time
Working part-time
<2 years
2-5 years
5-10 years
>10 years

Other
Programming
Experience

2-5 years
5-10 years
>10 years

1
61
1
1

36
3
11
14

1
5
2
1
49
6

2
14
22
26

3
12
49

1.56%
95.31%
1.56%
1.56%

56.25%
4.69%
17.19%
21.88%

1.56%
7.81%
3.13%
1.56%
76.56%
9.38%

3.13%
21.88%
34.38%
40.63%

4.69%
18.75%
76.56%

and concluded that profile visibility tends to decrease over time. Hence, we could assume that reactive users may also be

driven by reputation or recognition when deciding whether to disclose more personal information inside their profiles.

5.2 Engagement and Privacy-Related Constructs (RQ2)

Unlike the results obtained from the usersâ€™ profile information (Section 4.2), the analysis conducted over the survey

data showed no significant differences in the elicited constructs (i.e., GPC, PCST, PCOT, RSK, and PC) across unengaged,

proactive, and reactive users (Section 4.3). We hypothesise that this can be related to the relatively good reputation of

SO in terms of privacy and data protection, as opposed to OSNs like Facebook. Unlike the latter, SO has not received

the attention of mainstream media due to major data-breach scandals or privacy violations. Hence, the role of privacy

concerns and perceived risks may not be significant for usersâ€™ participation and engagement within the platform.

The differences observed in self-disclosure behaviour were not reflected by its survey counterpart (i.e., the SD

variable). Nevertheless, and despite that such results may look inconsistent, prior research has also found discrepancies

between peopleâ€™s reported and actual privacy behaviour. As mentioned in Section 2.2, this is often referred to as the

â€œprivacy paradoxâ€, a phenomenon frequently observed within users of OSNs. Our findings suggest traces of this paradox

among SO users, especially when contrasting the outcome of the survey analysis with that of the usersâ€™ profiles. Still,

further research is necessary to determine whether the reported privacy behaviour outperforms the actual one across

the three user categories. It would be of special interest to understand whether and up to which extent is the privacy

paradox manifested among SO users, and how does it relate to their overall engagement.

12

Cybersecurity Discussions in Stack Overflow

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

5.3 Implications and Recommendations

As privacy and security flaws in information systems grow steadily, it is very important to promote the exchange of

privacy and security knowledge among software practitioners. To a large extent, the SO community is encompassed by

early-career developers seeking for support and guidance in their engineering practices [22]. Hence, it plays a key role

in the dissemination and synthesis of cybersecurity expertise. However, our results show an apparent deficit in terms of

answers to privacy- and security-related questions (Section 4.1). This can not only cause dissatisfaction to those asking

such questions, but also damage the platformâ€™s value and usefulness in this regard.

Fig. 4. Practical implications (envisaged interface).

Having identified nuances in the self-disclosure behaviour across different user groups can be used to foster the

exchange of privacy and security expertise. For instance, profile information could be leveraged to motivate the

participation in cybersecurity discussions among SO users, by rooting pending questions to those users who are more

likely to answer them (e.g., those with a less visible profile). Moreover, closed questions could be assigned to these

users for further clarification, and thus increase their resolution chances. Such an approach could also contribute to

existing Q&A recommender systems and frameworks (e.g., [40]) seeking to match forthcoming questions to potential

respondents. That is, by incorporating profile visibility as a feature of their question-user matching algorithms.

Similarly, our results could be used to elaborate incentive strategies targeting unengaged individuals. For example, by

delivering cybersecurity suggestions to those SO users having a more visible profile. This approach is illustrated in Fig. 4,

where a (hypothetically) unengaged user receives such suggestions as she seeks for advice about an issue that is not

cybersecurity-related. Here suggestions come in the form of privacy- and security-related entries in the Overflow Blog

[35], a website curated by SO that gathers essays, opinion articles, and podcasts about computer programming. Using

different persuasive styles to approach certain user groups could also improve even more the chances of engagement

and behaviour change [5, 26]. For example, unengaged users could be nudged using a more authoritarian style (e.g.,

â€œMicrosoft and other big tech companies urge developers to engage in cybersecurity training!â€), whereas a consensual one

could be applied to proactive and reactive users (e.g., â€œMany across the SO community agree: Cybersecurity training is

essential for software developers!â€). Likewise, differentiated training content (e.g., access to customised documentation

and software artefacts) could be offered to each user group based on a further assessment of their technical skills.

13

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

DÃ­az Ferreyra et al.

6 LIMITATIONS AND THREATS TO VALIDITY

To a certain extent, the results of our study are subject to limitations related to its experimental design. In particular,

the following construct, external, and internal threats may affect the validity of our findings and conclusions:

Construct threats stem from the degree to which scales, constructs, and instruments measure the properties they

intend to [25]. Within the scope of our study, one construct threat arises from the approach employed to compute

usersâ€™ amount of self-disclosure. Profiles are not the only means to reveal private information in SO as users can also

disclose personal data inside questions, answers, or comments. However, we conducted our analysis exclusively over

SO profiles as they are already adequate and extensive sources of self-disclosure evidence.

Another construct threat relates to the approach we followed to characterise usersâ€™ engagement in SO. Indeed,

engagement can also take a passive form, where a member (often referred as a â€œlurkerâ€) may not contribute actively to

a discussion but may still read it and take advantage of its knowledge. We left passive engagement out of the scope of

this work as it cannot be determined from the information in our dataset. Still, future research will seek to characterise

lurkers and their interaction patterns regarding cybersecurity discussions.

Regarding the psychological constructs elicited during the online survey, we have assessed their reliability by

computing the corresponding Cronbachâ€™s Alpha coefficient. As mentioned in Section 4.3, we obtained values higher
than 0.7 in all the cases, suggesting a high internal consistency of these survey instruments.

External threats refer to conditions that may affect the generalisability of the study results [11]. In our case, this

relates to the discussions and profile samples we extracted from SO. Since the selection of cybersecurity questions was

guided by the tags users assign to them, we may have considered wrongly-tagged questions in our analysis or missed

some untagged ones out. Nonetheless, since the SO community of curators often addresses such problems, we assumed

the posts we retrieved were accurately labelled.

Another external threat to validity stems from the different sample sizes between Python and R discussions. To

minimise this threat, we treated both samples as one without conducting any analysis on each specific language.

Likewise, having gathered the email addresses only through GitHub can also be seen as an external threat since it

directly impacts the surveyâ€™s sample size â€“we sent the survey only to those users from whom we collected their emails

via GitHubâ€“. Nevertheless, we managed to gather enough contact details using this approach and distributed the survey

to a fair amount of potential respondents.

Internal threats relate to factors that may influence the independent variables of the study in terms of causality [11].

In this work, we have analysed the connection between usersâ€™ self-disclosure practices and their engagement in

cybersecurity discussions. However, as mentioned in Section 5.1, both self-disclosure and engagement practices can be

influenced by usersâ€™ need for recognition and popularity within the platform, among other intrinsic and extrinsic factors.

Hence, we acknowledge that our study is observational and, as such, cannot be leveraged to draw casual conclusions

given the lack of controlled experimental ground truth data.

7 CONCLUSIONS AND FUTURE WORK

Secure software development largely depends on practitionersâ€™ abilities to detect and address potential cybersecurity

threats. Still, prior work has shown that many consider security and privacy as secondary aspects of software projects [1].

Given the increasing popularity of Q&A platforms like SO, it is important to characterise and foster the exchange of

cybersecurity expertise of their users in order to shape privacy- and security-savvy communities.

14

Cybersecurity Discussions in Stack Overflow

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

The results of this work confirm that â€œanswer-hungryâ€ questions are still a pending issue in SO. Furthermore, it is an

issue affecting the privacy- and security-related expertise provided by the platform and its community. As discussed

in Section 5.3, having identified different engagement patterns can contribute to elaborating recommender systems

and incentive mechanisms targeting this issue. Considering SOâ€™s size and outreach, these results could also support

the dissemination of privacy- and security-by-design principles among software practitioners. That is, by delivering

personalised training programs and tools through the platform to bridge developersâ€™ knowledge gaps on cybersecurity.

Hence, this work contributes not only to current research in SO but also to ongoing efforts on bringing cybersecurity to

the core of software engineering practices.

As highlighted in Section 6, the results yielded in this work are observational and call for further investigations.

One potential direction for future research is the interplay between privacy concerns and the quality of cybersecurity

feedback provided by SO users. For instance, to determine whether developersâ€™ collective privacy concerns (e.g., their

sense of responsibility and empathy towards end-users) and prior cybersecurity experience play a significant role in the

extent and frequency of their contributions. For this, we plan to extend our analysis with an empirical study about the

factors motivating developers to value and address security and data-protection aspects of their software. For instance,

by using scales and survey instruments that capture their efforts towards secure software development, experiences

with security issues along with extrinsic motivations and deterrents (similar to the ones proposed in [7] and [37]).

ETHICAL CONSIDERATIONS

The methodology used in this manuscript was approved by the Australian National University Human Ethics Research

Committee (HREC), with project code 2021-24127.

REFERENCES

[1] Yasemin Acar, Sascha Fahl, and Michelle L Mazurek. 2016. You are not your developer, either: A research agenda for usable security and privacy

research beyond end users. In Cybersecurity Development. IEEE, Boston, MA, USA, 3â€“8.

[2] Ifeoma Adaji and Julita Vassileva. 2016. Towards Understanding User Participation in Stack Overflow Using Profile Data. In International Conference

on Social Informatics. Springer, USA, 3â€“13.

[3] Arshad Ahmad, Chong Feng, Shi Ge, and Abdallah Yousif. 2018. A survey on mining Stack Overflow: Question and answering (Q&A) community.

Data Technologies and Applications 52, 2 (2018), 190â€“247.

[4] Tanveer Ahmed and Abhishek Srivastava. 2017. Understanding and evaluating the behavior of technical users: A study of developer interaction at

StackOverflow. Human-Centric Computing and Information Sciences 7, 1 (2017), 1â€“18.

[5] Esma Aimeur, Nicolas Emilio Diaz Ferreyra, and Hicham Hage. 2019. Manipulation and Malicious Personalization: Exploring the Self-Disclosure

Biases Exploited by Deceptive Attackers on Social Media. Frontiers in Artificial Intelligence 2 (2019), 26.

[6] Hala Assal and Sonia Chiasson. 2018. Motivations and amotivations for software security. In SOUPS Workshop on Security Information Workers

(WSIW). USENIX Association. USENIX Association, USA, 1â€“12.

[7] Hala Assal and Sonia Chiasson. 2019.

â€™Think secure from the beginningâ€™ A Survey with Software Developers. In Proceedings of the 2019 CHI

conference on human factors in computing systems. 1â€“13.

[8] Tom Buchanan, Carina Paine, Adam N. Joinson, and Ulf-Dietrich Reips. 2007. Development of measures of online privacy concern and protection

for use on the Internet. Journal of the American Society for Information Science and Technology 58, 2 (2007), 157â€“165.

[9] Tae Rang Choi and Yongjun Sung. 2018. Instagram versus Snapchat: Self-expression and privacy concern on social media. Telematics and informatics

35, 8 (2018), 2289â€“2298.

[10] Alton YK Chua and Snehasish Banerjee. 2015. Answers or no answers: Studying question answerability in stack overflow. Journal of Information

Science 41, 5 (2015), 720â€“731.

[11] Daniela Soares Cruzes and Lotfi ben Othmane. 2017. Threats to validity in empirical software security research. Empirical Research for Software

Security: Foundations and Experience 1, 1 (2017), 277â€“302.

[12] European Comission 2021. Proposal for a Regulation Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act). European

Comission.

[13] Felix Fischer, Konstantin BÃ¶ttinger, Huang Xiao, Christian Stransky, Yasemin Acar, Michael Backes, and Sascha Fahl. 2017. Stack overflow considered

Harmful? The Impact of Copy&Paste on Android Application Security. In Symposium on Security and Privacy (SP). IEEE, USA, 121â€“136.

15

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

DÃ­az Ferreyra et al.

[14] Zhipeng Gao, Xin Xia, David Lo, and John Grundy. 2020. Technical Q8A site answer recommendation via question boosting. ACM Transactions on

Software Engineering and Methodology (TOSEM) 30, 1 (2020), 1â€“34.

[15] Irit Hadar, Tomer Hasson, Oshrat Ayalon, Eran Toch, Michael Birnhack, Sofia Sherman, and Arod Balissa. 2018. Privacy by designers: software

developersâ€™ privacy mindset. Empirical Software Engineering 23, 1 (2018), 259â€“289.

[16] Mohsen Jozani, Emmanuel Ayaburi, Myung Ko, and Kim-Kwang Raymond Choo. 2020. Privacy concerns and benefits of engagement with social

media-enabled apps: A privacy calculus perspective. Computers in Human Behavior 107 (2020), 106260.

[17] Imrul Kayes, Nicolas Kourtellis, Francesco Bonchi, and Adriana Iamnitchi. 2015. Privacy Concerns vs. User Behavior in Community Question
Answering. In 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE, IEEE, Boston, MA,
USA, 681â€“688.

[18] Nicole C KrÃ¤mer and Johanna SchÃ¤wel. 2020. Mastering the challenge of balancing self-disclosure and privacy in social media. Current Opinion in

Psychology 31 (February 2020), 67â€“71.

[19] Hanna Krasnova, Oliver GÃ¼nther, Sarah Spiekermann, and Ksenia Koroleva. 2009. Privacy concerns and identity in online social networks. Identity

in the Information Society 2, 1 (01 Dec 2009), 39â€“63.

[20] Franki Y.H. Kung, Navio Kwok, and Douglas J. Brown. 2018. Are Attention Check Questions a Threat to Scale Validity? Applied Psychology 67, 2

(2018), 264â€“283.

[21] T. Lopez, T. Tun, A. Bandara, L. Mark, B. Nuseibeh, and H. Sharp. 2019. An Anatomy of Security Conversations in Stack Overflow. In 41st International

Conference on Software Engineering: Software Engineering in Society. IEEE/ACM, Canada, 31â€“40.

[22] Tamara Lopez, Thein T. Tun, Arosha Bandara, Mark Levine, Bashar Nuseibeh, and Helen Sharp. 2018. An Investigation of Security Conversations in
Stack Overflow: Perceptions of Security and Community Involvement. In 1st International Workshop on Security Awareness from Design to Deployment
(Gothenburg, Sweden) (SEAD â€™18). ACM, USA, 26â€“32.

[23] Iraklis Moutidis and Hywel TP Williams. 2021. Community evolution on Stack Overflow. Plos one 16, 6 (2021), e0253010.
[24] European Parliament and of the Council. 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the
protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46.
Official Journal of the European Union (OJ) 59, 1-88 (April 2016), 294.

[25] Paul Ralph and Ewan Tempero. 2018. Construct Validity in Software Engineering Research and Software Metrics. In 22nd International Conference

on Evaluation and Assessment in Software Engineering (New Zealand) (EASEâ€™18). ACM, USA, 13â€“23.

[26] Johanna SchÃ¤wel. 2019. How to Raise Usersâ€™ Awareness of Online Privacy. Ph. D. Dissertation. University of Duisburg-Essen.
[27] Kent Seamons. 2022. Privacy-Enhancing Technologies. In Modern Socio-Technical Perspectives on Privacy. Springer, Cham, 149â€“170.
[28] Awanthika Senarath and Nalin A. G. Arachchilage. 2018. Why Developers Cannot Embed Privacy into Software Systems? An Empirical Investigation.

In 22nd International Conference on Evaluation and Assessment in Software Engineering 2018 (New Zealand). ACM, USA, 211â€“216.

[29] Subhasree Sengupta and Caroline Haythornthwaite. 2020. Learning with comments: An analysis of comments and community on Stack Overflow.

In Proceedings of the 53rd Hawaii International Conference on System Sciences.

[30] Sean Sirur, Jason R.C. Nurse, and Helena Webb. 2018. Are We There Yet? Understanding the Challenges Faced in Complying with the General Data

Protection Regulation (GDPR). In 2nd International Workshop on Multimedia Privacy and Security (Canada) (MPS â€™18). ACM, USA, 88â€“95.
[31] StackExchange. 2022. How many developers visit Stack Overflow? Retrieved June 7, 2022 from https://stackoverflow.co/advertising/audience/
[32] StackExchange. 2022. Stack Overflow Statistics. Retrieved June 7, 2022 from https://stackexchange.com/sites
[33] StackOverflow. 2022. Stack Overflow - Where Developers Learn, Share, and Build Careers. Retrieved June 7, 2022 from https://stackoverflow.com
[34] StackOverflow. 2022. Stack Overflow Tag Explorer. Retrieved June 7, 2022 from https://stackoverflow.com/tags
[35] StackOverflow. 2022. The Overflow - Essays, opinions, and advice on the act of computer programming from Stack Overflow. Retrieved June 7, 2022

from https://stackoverflow.blog

[36] Jessica Staddon, David Huffaker, Larkin Brown, and Aaron Sedley. 2012. Are Privacy Concerns a Turn-off? Engagement and Privacy in Social
Networks. In Proceedings of the Eighth Symposium on Usable Privacy and Security (Washington, D.C.) (SOUPS â€™12). Association for Computing
Machinery, New York, NY, USA, Article 10, 13 pages. https://doi.org/10.1145/2335356.2335370

[37] Mohammad Tahaei, Alisa Frik, and Kami Vaniea. 2021. Privacy Champions in Software Teams: Understanding Their Motivations, Strategies, and

Challenges. In Conference on Human Factors in Computing Systems. ACM, USA, 1â€“15.

[38] Mohammad Tahaei, Kami Vaniea, and Naomi Saphra. 2020. Understanding Privacy-Related Questions on Stack Overflow. In Conference on Human

Factors in Computing Systems. ACM, USA, 1â€“14.

[39] Andrew W Vargo and Shigeo Matsubara. 2018. Identity and performance in technical Q&A. Behaviour & Information Technology 37, 7 (2018),

658â€“674.

[40] Lin Wang, Bin Wu, Juan Yang, and Shuang Peng. 2016. Personalized recommendation for new questions in community question answering. In 2016
IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE, IEEE, Boston, MA, USA, 901â€“908.
[41] Jie Yang, Ke Tao, Alessandro Bozzon, and Geert-Jan Houben. 2014. Sparrows and Owls: Characterisation of Expert Behaviour in Stack Overflow. In

International Conference on User Modeling, Adaptation, and Personalization. Springer, Denmark, 266â€“277.

16

