2
2
0
2

r
p
A
1

]

R
C
.
s
c
[

1
v
1
1
0
0
0
.
4
0
2
2
:
v
i
X
r
a

Leveraging Privacy Proﬁles to Empower Users in the
Digital Society

Davide Di Ruscioa,∗, Paola Inverardia,
Patrizio Migliarinia, Phuong T. Nguyena

aUniversità degli studi dell’Aquila L’Aquila, Italy
{davide.diruscio, paola.inverardi, patrizio.migliarini, phuong.nguyen}@univaq.it

Abstract

Privacy and ethics of citizens are at the core of the concerns raised by our increas-

ingly digital society. Proﬁling users is standard practice for software applications

triggering the need for users, also enforced by laws, to properly manage privacy

settings. Users need to manage software privacy settings properly to protect

personally identiﬁable information and express personal ethical preferences. AI

technologies that empower users in their interaction with the digital world by

reﬂecting their personal ethical preferences can be key enablers of a trustworthy

digital society. This paper focuses on the privacy dimension and contributes a

step in the above direction through an empirical study on an existing dataset

collected from the ﬁtness domain. The study aims to understand which set of

questions and settings is more appropriate to diﬀerentiate users according to

their privacy preferences. The experimental results reveal that a compact set of

semantic-driven questions (about general privacy preferences) helps distinguish

users better than a complex domain-dependent one (concerning the ﬁtness do-

main). Based on the study outcome, we design and implement a recommender

system to provide users with suitable recommendations with respect to privacy

choices. We then show that the proposed recommender system provides relevant

settings to users, obtaining high prediction accuracy.

Keywords: Privacy proﬁles, Clustering, Recommender systems

∗Corresponding author

Preprint submitted to Elsevier

April 4, 2022

 
 
 
 
 
 
1. Introduction

Privacy and ethics of citizens are at the core of the concerns raised by our

increasingly digital society. Proﬁling users is standard practice for software

applications triggering the need for users, also enforced by laws, to properly

manage privacy settings and moral preferences. This deals with the way users

give their consent to storing, sharing to third parties, as well as disseminating

sensitive personal information and express moral preferences like, for example,

ticking to pay a decarbonization tax. Mobile apps have been becoming increas-

ingly popular as they can provide users with a wide range of functionalities.

For diﬀerent reasons, apps often require access to intimate information about

the users and hosting device, triggering privacy concerns. This requires proper

management of privacy, with the ultimate aim of protecting users’ preferences

as well as personally identiﬁable information.

In this paper, we focus on the privacy dimension of an ordinary user with

little technical knowledge of the privacy mechanisms of the digital systems she

seamlessly uses but with an evident moral character. While choosing strict

settings may help protect her data, this may prevent the complete availability

of the functionalities provided by the software. In contrast, loosening privacy

settings mitigates the restriction on functionalities, but it may come with the

price of compromising her data privacy. In this respect, Artiﬁcial Intelligence

(AI) technologies can empower the user in maintaining a reasonable trade-oﬀ

between accessibility and protection, and reﬂecting the user privacy preferences

can be the key enabler of a trustworthy digital society.

Understanding the commonalities and diﬀerences among users based on pro-

ﬁles has been among the main issues in data privacy research [1, 2, 3]. Categoriz-

ing proﬁles contributes to better identiﬁcation of users’ behaviors and supports

administrators in comprehending privacy choices. At the same time, personal

proﬁles may enable the design of functionalities that help users set privacy pref-

erences of the digital technologies they use. Various proposals to categorize or

group end-users into clusters based on their security or privacy attitudes/be-

2

haviors in speciﬁc domains have been made [4, 5]. Users’ preferences were

analyzed in an extensive study [6] on permission settings from real Android

mobile users to recommend personalized default settings. Sanchez et al. [7] an-

alyzed user-privacy preferences in the ﬁtness domain employing a speciﬁcally

designed questionnaire consisting of both domain-speciﬁc and general questions

to recommend personalized privacy settings for the ﬁtness apps.

Though a lot of achievements have been reported, as discussed in [8], we

believe that there is still the need to understand how to characterize user’s pri-

vacy behavior in a general setting. Indeed privacy is a dimension of ethics and

should be part of the ethical proﬁle of a user and driven by ethical consideration

rather than by contextual attitudes or practices in given domains. For example,

relying on the analysis of current or past users’ preference settings as in [6] does

not guarantee a correspondence between what users believe as their general pri-

vacy proﬁle and what they actually (can) do when setting privacy preferences.

Moreover, data privacy awareness in the digital society is only recently exiting

the specialists’ ﬁelds (legal, ethical, economic, social) to impact the wider so-

ciety. The pandemic has also dramatically advanced the penetration of digital

technologies in the society from market to education [9, 10]. This means that

a large body of collected data on privacy settings may not reﬂect the attitude

and attention to privacy that present users have and will have in the future.

In this work, we explore a diﬀerent research direction by relying on the data

of the study in the ﬁtness domain [7] that were collected by means of a question-

naire and a simulator.1 We analyze both general and domain-speciﬁc questions

with the aim of (i) identifying general questions that reﬂect moral attitudes of

the users; and (ii) recommending privacy preferences accordingly. Moreover, we

design and implement a recommender system [11] to provide users with suitable

recommendations with respect to privacy choices. The experimental results are

positively interesting, revealing that a compact set of general questions helps

1We thank Prof. Dr. Ilaria Torre, University of Genoa (Italy) for providing us with the

privacy dataset [7].

3

distinguish users better than a more complex domain-dependent one. We also

show that the proposed recommender system provides relevant settings to users,

obtaining high prediction accuracy.

The main contributions of our work are summarized as follows.

• We investigate which sets of (general) privacy questions are more relevant

for classifying users with respect to their privacy moral preferences.

• By means of an empirical evaluation, we show that self-assessment about

privacy attitudes given by users does not reﬂect the way they act in prac-

tice.

• We develop PisaRec, a recommender system to provide suitable privacy

settings that reﬂect user preferences. This aims to help users relieve the

burden of setting privacy conﬁgurations when they go online.

We organize the paper into the following sections. In Section 2, we present a

motivating example and a categorization of privacy proﬁles. Section 3 describes

the proposed approach which makes use of both unsupervised and supervised

learning to handle user proﬁles. The methods used to evaluate our approach

are detailed in Section 4. We report and analyze the experimental results in

Section 5. Discussion related to the limitations and threats to validity are

provided in Section 6. We review related work in Section 7. Finally, Section 8

sketches future work and concludes the paper.

2. Background

The following example illustrates the need for personalized automated pri-

vacy assistance that a user interacting with multiple systems at a time may

require. Then we brieﬂy report the most relevant aspects for our research tax-

onomies for privacy proﬁles proposed in the literature.

2.1. Motivating example

After a long day at work, Alice is at the subway station. After the pandemic

outbreak, she will meet pals at the cinema. She is on time but learns that she

4

cannot buy a ticket from the subway station attendant due to rigorous hygiene

regulations. In addition, vending machines are out of commission for contact-

less technology upgrades. Instead, a QR code and simple instructions to buy an

electronic ticket online are posted in front of the vending machines. Her train

is about to arrive, she opens her camera app and frames the QR code. The site

structure appears in a split second, but as Alice scrolls down to ﬁnd the ticket

she needs, a popup asks for her privacy settings. Above a very long list of radial

button options about disclosing GPS position, information about her mobile

phone, consent to save various types of cookies on her device, share her list of

contacts, etc., she is presented with three buttons: accept all, strictly necessary,

decline all.

Alice is very concerned about her privacy, and when not strictly necessary

for the purpose she wants to perform, she does not wish to disclose private

information. Since the service she is asking for is simple as asking for a one-ride

ticket, she clicks decline all. The next page seems to load slowly, images and

structure are shown in a non-adaptive way, so she has to pinch-in to zoom and

scroll to read the text that informs her that a cryptographic key used for her

session management cannot be stored due to her preferences, so the session is

not secure also the page asks her to choose language, timezone, type of device

and the web browser she is using, payment options, etc. While reading, Alice

realizes that her train is about to arrive at the station. So, she decides to click

the back button on her browser, reload the page and click strictly necessary

when prompted. The site then stays fast and steady, adapted to the display

of her device, prompting if she wants to take a one-ride ticket or a full day

one. Her mobile wallet handles the payment instantly, and she receives her

ticket just before the train comes. On time to the cinema, Alice enjoys the

ﬁlm with her friends, soon forgetting the online ticket purchase experience. Her

preferences are saved on her phone, so she will buy train tickets quickly and

easily in the future. Alice does not know that the strictly necessary option,

although excluding third-party tracking and marketing, includes all alternatives

that are strictly essential to all services oﬀered by the booking site, including –

5

Table 1: Privacy categories according to diﬀerent taxonomies (Listed in chronological order).

How

impor-

NOTHING

LITTLE

QUITE

VERY

tant is privacy

to you?

Segmentation [1] Unconcerned

Pragmatists

Fundamentalist

Privacy Per-

Marginally

Amateurs

Technicians

Lazy Ex-

Fundamentalists

sonas [14]

concerned

perts

Philosophies [15] Fatalism

Nothing

Something

Trade-oﬀ

Personal

Moral right

to hide

to hide

Resp.

Privacy Clus-

Unconcerned

Socially active

Health-

focused

Minimal

Anonymous

(Strict)

Conservative

Unconcerned

Fence-Sitter

Advanced Users

tering [7]

Self-

assessment [7]

Our proposed

INATTENTIVE

INVOLVED/ATTENTIVE

SOLICITOUS

categorization

the lower price inter-city ticket that requires GPS tracking, the discounted price

for kids that requires age disclosure, discount for army and state oﬃcials who

must check other installed mobile applications, as well as the train pass app to

see if the ticket is part of a booklet, etc.

Analogously to various studies notably Liu et al. [12], we believe that a

software technology should assist Alice in automatically selecting the options

that, on the one hand, are needed for what she wants to do and, on the other

hand, are compliant with her moral preferences.

In this work, we show that it is possible to protect users by ﬁrst understand-

ing their privacy proﬁles, which can be automatically identiﬁed by considering

a small set of general and domain-independent questions that are shown to be

enough to reﬂect the user’s moral attitude. Thus, our approach is to categorize

personal privacy proﬁles from an ethical perspective [13]. Proﬁles can then be

used to automate app and web settings, leveraging recommender systems like

in this paper or other technologies.

2.2. Categorizations of privacy proﬁles

Table 1 gives a summary of the most notable taxonomies of privacy cate-

gories. Starting from the question: “How important is privacy to you? ” from left

to right of the table, an increasing level of privacy concerns is shown. Westin [1]

proposed the ﬁrst categorization of user proﬁles with three levels, i.e., Uncon-

6

Figure 1: The proposed approach.

cerned, Pragmatists, and Fundamentalist. Since then, there have been other

studies that follow up and develop this initial taxonomy. In particular, Dupre

et al. [14] expanded it proposing ﬁve categories: Marginally concerned, Ama-

teurs, Technicians, Lazy Experts, and Moral right. Schairer et al. [15] came up

with even more, i.e., six categories, where the answer Little is split into Nothing

to hide, and Something to hide; and Quite is made of Trade-oﬀ, and Personal

Resp. Recently, Sanchez et al. [7] proposed a more compact categorization,

where users are grouped into four categories, Privacy concerns, Unconcerns,

Fence-Sitter, and Advanced Users. As it appears from the table, category re-

ﬁnement happens in the middle category and may depend on the application

domain as well as on the amount of input data. Further categories are con-

textual and can be obtained as specialization of personal proﬁles based on the

single user’s experience. Therefore, as starting point three categories provide

three clearly distinguishable moral attitudes.

The categorization we propose names the three clusters as Inattentive, At-

tentive, and Solicitous. While Inattentive means that users do not care about

privacy, Solicitous corresponds to an opposite attitude, where users are com-

pletely aware of privacy issues. The Attentive category is something in between,

and covers both Little and Quite answers to the question “How important is pri-

vacy to you? ”

7

Assisted Selection of Privacy PreferencesFull set of  questionsAutomated creation of user privacy profilesPrivacy ProfilesAutomated assignment of privacy profiles to usersPrivacy settings recommendationSoftware SystemPrivacy SettingsFiltered set of questionsEmpirical Study12343. Proposed Approach

Typically, users specify privacy preferences by directly interacting with the

privacy settings provided by the used software. Similar to other techniques [13,

12, 8] we propose an approach that relies on a software layer that automatically

identiﬁes privacy proﬁles and interacts with the user or the software system to

recommend privacy preferences accordingly.

Concerning what we present in this paper, the assisted selection phase of pri-

vacy preferences started on training data consisting of general, domain-speciﬁc,

and app-speciﬁc answers given to the questions deﬁned in [7] (see “Full set of

questions” in Figure 1). We have empirically analyzed the full set of questions

to identify the corresponding subset (consisting of general questions) that is

suﬃcient to automatically identify our three user privacy proﬁle categories, i.e.,

Inattentive, Attentive, and Solicitous (see activity 1 in Figure 1).

The automated creation of user privacy proﬁles phase (see activity 2 in

Figure 1) relies on an unsupervised clustering module, which can automatically

group users in the training data. The automated assignment of privacy proﬁles

to users phase (see 3 ) relies on a supervised classiﬁer using a feed-forward

neural network to automatically assign to the given user the corresponding

privacy proﬁle among one of those identiﬁed in 2 . Finally, a recommender

system is used to further validate the activities 1 and 2 , and to provide users

with privacy settings recommendations (see activity 4 ) according to the privacy

settings of other users belonging to privacy proﬁles as detected in 3 .

Details on the results obtained from the performed Empirical Study are

given in the Experimental results section, whereas the activities 2 , 3 , and 4

are described as follows.

3.1. Automated creation of user privacy proﬁles

To automatically create user privacy proﬁles, we employed a clustering pro-

cess by relying on the graph-based representation of users and privacy settings

as shown in Figure 2. This representation is also used by the developed neural

network for classifying users presented in Section 3.2.

8

Figure 2: Graph representation of users and privacy settings.

Each user u is represented by a vector φ = (φ1, φ2, .., φF ), where φi is the
weight of term si, computed as the term-frequency inverse document frequency
value as follows:

φi = fsi × log(

|P |
asi

)

(1)

The similarity between two users u and v is computed using their corre-
sponding feature vectors φ = {φi}i=1,..,F and ω = {ωj}j=1,..,F by means of the
cosine similarity function:

sim(u, v) =

n
t=1(φt)2 ×
(cid:80)
where n is the cardinality of all settings that were set to 1 by both u and
v. Intuitively, u and v are characterized by using vectors in an n-dimensional

(cid:112)(cid:80)

(cid:112)(cid:80)

(2)

n
t=1 φt × ωt
n
t=1(ωt)2

space, and Equation 2 measures the cosine of the angle between them. As an

example, in Fig. 2, we see that the two users u2 and u4 are similar since they
both set two settings s1 and s3.

A set of n users is grouped into κ pre-deﬁned number of clusters, with the

aim of maximizing both the similarity among instances within a single cluster,

and the dissimilarity among independent clusters. To this end, we calculate the

distance between every pair of users and feed as the input for the clustering

engine. The K-medoids algorithm [16] has been chosen to group users into

clusters due to its simplicity and eﬃciency.

In the clustering process, the distance scores, computed as dC(u, v) = 1 −
Initially, a set of medoids

simC(u, v), are used to assign users to clusters.

9

u1u2u3u4s2s4s3s5s1(users) is generated randomly, then a medoid is selected as the user in the

cluster that has minimum average distance to all the other users in the cluster.

Afterwards, users are assigned to the cluster with the closest medoid, using a

greedy strategy [16].

3.2. Automated assignment of privacy proﬁles to users

Supervised learning algorithms can simulate humans’ learning activities,

mining knowledge from labeled data and performing predictions for unknown

data [17]. Among others, neural networks have been widely adopted in var-

ious applications, including pattern recognitions [18], or forecasting [19]. A

feed-forward neural network consists of connected layers of neurons, where the

output of a layer is transferred to the next layer’s neurons, except for the output

layer.

Figure 3: A three-layer neural network.

We built a feed-forward neural network to classify users into diﬀerent pri-

vacy groups, using preferences as features. The network consists of three layers

explained as shown in Figure 3. The input layer has L neurons, being equal
to the number of input settings, i.e., X = (x1, x2, ..., xL). The middle layer
consists of M neurons, i.e., H = (h1, h2, ..., hM ), M can be conﬁgured during
the evaluation. There are κ neurons in the output layer, corresponding to κ
output categories, i.e., ˆy = (ˆy1, ˆy2, .., ˆyκ). The predicted value ˆyk for neuron k

10

.........x1x2xLh1h2hMˆy1ˆyNω(1)11ω(1)21ω(2)11ω(2)21of the output layer is computed to minimize the error between the real values

and the predicted ones. As discussed in the Experimental results section, the

conceived neural network has played an important role in the performed analy-

sis, especially to understand to what extent self-declared privacy proﬁles reﬂect

the actual user category.

3.3. Privacy settings recommendation

We conceptualize PisaRec, a Privacy settings assistant running on top of a

Recommender system to provide users with suitable data protection conﬁgura-

tions. PisaRec works based on the assumption that “if users of the same privacy

proﬁle already share some common privacy settings, then they are supposed to

share additional similar settings” [20].

In this way, we utilize the proposed

graph-based representation to model the relationship among users and use a

collaborative-ﬁltering algorithm [21] to recommend missing settings. To feed as

input for the recommendation engine, we adopt the user-item paradigm [22], in

which each user corresponds to one row, a column represents each setting. In

this way, a cell in the matrix dictates the rating given by a user to a setting.

The two values 0 and 1 correspond to deny and allow, respectively. An example

of a user-setting matrix for the set of four users and ﬁve settings is as follows:

u1 (cid:51) s1, s2; u2 (cid:51) s1, s3; u3 (cid:51) s1, s3, s4, s5; u4 (cid:51) s1, s2, s4, s5. Accordingly, the
user-item ratings matrix built to model the occurrence of the settings is depicted

in Figure 4.

Figure 4: A user-setting matrix.

11

s1s2s3s4s5u111000u210100u310111u411011The following collaborative-ﬁltering formula is utilized [20] to predict the

inclusion of a setting si for user u:

ru,si = ru +

v∈topsim(u)(ru,si − rv) · sim(u, v)
v∈topsim(u) sim(u, v)

(cid:80)

(3)

where ru and rv are the mean of the ratings of u and v, respectively; v belongs
to the set of top-k most similar users to u or neighbour users, i.e., topsim(u);
sim(u, v) is the similarity between u and a similar user v, computed using
Equation 2.

(cid:80)

The clusters obtained from the previous section allow us to identify users

with similar privacy preferences. Based on the obtained categorization, given an

input user, the neural network assigns the user to a speciﬁc category. Afterward,

we build a graph only for this category following the paradigm in Figure 2. Such

a sub-graph contains fewer nodes and edges than a full graph for all categories,

aiming to optimize the computation. On top of this, PisaRec recommends

missing settings to users. The outcome of the computation is a ranked list

of probable settings, and we select the top-N of them to present as the ﬁnal

recommendations.

4. Evaluation

To study the proposed approach’s performance, we ﬁrst introduce three re-

search questions. Afterward, we describe the dataset and metrics used in our

evaluation.

4.1. Research questions

The following research questions are considered to evaluate our proposed

approach.

• RQ1: How well does the users’ self-assessment reﬂect their privacy cat-
egory? As users in the considered dataset [23] have been allowed to self-

assess their privacy category, we examine if such a self-evaluation reﬂects

their real category.

12

• RQ2: Which sets of questions are relevant for assessing privacy con-
cerns? We are interested in ﬁnding the set of questions that can better

distinguish between user proﬁles. For this research question, we cluster

the users with diﬀerent sets of features, and identify the one that brings

the best clustering solution. The aim is to ﬁnd a set of privacy questions

that better represents the user proﬁles.

• RQ3: To which extent is PisaRec able to utilize the obtained categorization
in recommending relevant privacy settings to users? We investigate how

well the conceived recommender system learns from existing proﬁles, pro-

viding users with additional conﬁgurations that reﬂect their preferences.

4.2. Dataset

We opted for an existing dataset that has been collected through a domain-

speciﬁc survey about the usage of a ﬁtness app including user privacy preferences

[7]. As shown in Table 2, there are 444 data entries which have been divided

into three main groups as follows:

• Domain speciﬁc: This is the set of questions being explicitly related to
the ﬁtness activity. There are a total of 202 questions in this category.

• App related : These questions are about the use or setting of the app,

consisting of 113 questions.

• Generic: This set of questions consists of generic questions that are not

related to other groups. There are 129 generic questions in total.

4.3. Evaluation metrics

• Compactness. The metric measures how closely relevant the users within

a cluster are [24]. In this respect, a lower value represents a better clus-

tering solution and vice versa.

13

Table 2: Summary of the dataset.

Questions/Data

Alias

Description

# entries

Domain speciﬁc

D

Questions related to the speciﬁc domain (Fit-

202

ness)

D Subset 1

DP1

Subset of the D set consisting of privacy rele-

123

vant questions

App related

A

Questions related to the mobile application

113

A Subset 1

AP1

Subset of the A set consisting of privacy rele-

and the speciﬁc software context

vant questions

A Subset 2

AP2

Subset of the A set that includes only gener-

65

6

D + A Subset 0

Generic

S0

G

alizable questions

Privacy related questions from the D and the

188

A sets (DP1+AP1)

Generic questions not speciﬁcally related to

129

the domain (ﬁtness) or the application/soft-

ware context (mobile app)

G Privacy Subset 1

GP1

Subset of the G set consisting of privacy rele-

110

vant questions

G Subset 1

G1

Subset of the G set consisting of questions re-

35

lated to the disclosure of information about

user’s identity with the app

G Data 2

G2

Data concerning the time spent by the users to

3

answer the questionnaire, play with the simu-

lator, and the sum of the two

G Subset 3

G3

Subset of the G set consisting of questions re-

lated to the user’s identity

G Subset 4

G4

Subset of the G set consisting of questions re-

19

56

lated to the disclosure of private information

with the app

G Subset 5

G5

Subset of the G set consisting of questions re-

16

lated to the concerns about privacy

Full dataset

DATA

Data collected with the questionnaire and the

444

simulator (D+A+G )

• Silhouette. It measures how similar a user u is to all the remaining users

of the same cluster [24], computed using the following formula:

s(u) =

(b(u) − a(u))
max{a(u), b(u)}

(4)

where a(u) is the mean distance between u and the others, b(u) is the
minimum mean distance. A silhouette value falls into the range [-1,..+1],

where a higher score means a better clustering solution.

14

Furthermore, we also use Precision, Recall, ROC curve and AUC to study

the performance of the proposed approach.

First, there are the following deﬁnitions: True positive (TP) is the set-

tings that match with ground-truth data; False positive (FP) is the rec-

ommended settings but do not match with the ground-truth data; False

negative (FN): the settings that should be recommended, but they are

excluded. Then, the metrics are as follows:

• Precision and Recall. Precision measures the fraction of the number

of settings properly classiﬁed to the total number of recommended items

and Recall (or true positive rate – TPR) is the ratio of the number of

correctly classiﬁed items to the total number of items in the ground-truth

data. The metrics are deﬁned as follows:

P =

T P
T P + F P

R =

T P
T P + F N

= T P R

• False positive rate (FPR). This metric measures the ratio of the num-

ber of items that are falsely classiﬁed into a category c, to the total number

of items that are either correctly not classiﬁed, or falsely classiﬁed into the

category:

F P R =

F P
T N + F P

• ROC curve and AUC. The relationship between FPR and TPR is

sketched in a 2D space, using a receiver operating characteristic (ROC)

[25], which spans from (0,0) to (1,1). An ROC close to the upper left

corner represents a better prediction performance.

5. Experimental results

This section reports and analyzes the experimental results by answering the

research questions introduced in Section 4.1.

15

Figure 5: ROC curves with generic questions.

5.1. RQ1: How well does the users’ self-assessment reﬂect their privacy cate-

gory?

In the dataset [7] considered in our evaluation, each user has assigned them-

selves to one of the following four groups: Privacy Conservative (Class 0), Un-

concerned (Class 1), Fence-Sitter (Class 2), and Advanced User (Class 3). We

investigate if the self-assessment is consistent, i.e., if all the users properly per-

ceive their real privacy category. This is important since a proper self-clustering

can be utilized in additional proﬁling activities.

We conducted evaluation using the conceived neural network as the classiﬁer.

Such a technique has been successfully applied to classify various types of data,

e.g., text [26], chemical patterns [27], metamodels [28], to name a few. Similarly,

we use the privacy settings as features, and the labels speciﬁed by humans to

train the classiﬁer. We opt for the ten-fold cross validation technique [29],

where the dataset is split into ten equal parts, and the evaluation is done in ten

rounds. The evaluation metrics are computed on the test set, i.e., for each user

the network predicts a label, which is then compared with the self-assessed label

to evaluate the performance. Finally, ROC curves are sketched by combining

the scores obtained from all the ten folds.

16

Figure 6: ROC curves with domain speciﬁc questions.

Figure 5 and Figure 6 depict the ROC curves obtained from the classiﬁcation

results for generic and domain speciﬁc questions. It is evident that the classiﬁer

achieves very low prediction performance on both conﬁgurations. In particular,

the curves bend over the diagonal line, being close to a random guess. Moreover,

the AUC values of the four categories are always lower than 0.65. In other words,

we encounter negative results, where the neural network fails to predict a proper

category for a user. These results suggest that there are noises in the training

data [30], which could possibly be both in the features and the labels.

To conﬁrm the hypothesis, we measure the similarity between each user and

all the remaining others. Interestingly, we found out that 96.20% of the users

have very similar users in completely diﬀerent self-assessed categories. This

demonstrates that while users share similar preferences, they classify themselves

diﬀerently, causing a low prediction performance for the neural network.

Answer to RQ1. The self-assessment given by users does not reﬂect their
real privacy category: Users with highly similar settings perceive themselves

as completely diﬀerent groups. In practice, this means administrators should

not rely on such a self-categorization, but have to perform privacy proﬁling

on their own.

17

5.2. RQ2: Which sets of questions are relevant for assessing privacy concerns?

As seen in RQ1, the self-assessment given by users is not consistent, thus
it is necessary to ﬁnd another way to group users into clusters. We performed

experiments on diﬀerent subsets of the questionnaire to study the inﬂuence of

each set on the clustering results. The ultimate aim is to identify a set of

questions that helps classify users better.

In particular, we are interested in

analyzing the following groups of questions:

• QS1: It is a set of question sets as follows: Domain speciﬁc (D); App
related (A); Generic (G) and their combination (i.e., D+A+G) named

COM.. Furthermore, we also include the set G+AP2 where AP2 con-

tains generalizable questions like “Do you believe the company providing

this ﬁtness tracker is trustworthy in handling your information?” Indeed,

this is to ask if the company is trustworthy, and therefore we consider it as

a general question. QS1 permits to compare compactness and silhouette
performances between a single set and combinations of all questions.

• QS2: It is a set of questions sets as follows: DP1, AP1, and GP1 that are
the subsets of D, A, and G consisting only of privacy relevant questions,

respectively. COM. is the union of the three subsets, i.e., COM. =
DP1+AP1+GP1. QS2 permits to understand the actual inﬂuence of
the privacy-related questions.

• QS3: It consists of subsets of generic questions G deﬁned as follows: G1
are the questions related to disclosure of information about user’s identity

with the app; G2 the questions related to the time spent by the user in

completing the survey; G3 the questions related to user’s identity; G4 the

questions related to disclosure of private information with the app; G5 the

questions related to concerns about privacy. COM. is the combination
of all the subsets, i.e., COM. = G1+G2+G3+G4+G5. QS3 is to
ascertain the inﬂuence of the generic questions with respect to the overall

set of questions.

18

(a) Compactness for QS1

(b) Silhouette for QS1

(c) Compactness for QS2

(d) Silhouette for QS2.

(e) Compactness for QS3

(f) Silhouette for QS3.

Figure 7: Compactness and silhouette scores.

We compute and report for each set the corresponding compactness and sil-

houette scores. Figure 7(a), Figure 7(c), and Figure 7(e) report the compactness

scores computed for the three question sets.

As it can be seen in Figure 7(a), using A as input yields the most compact

clusters. In particular, most of the scores are smaller than 40. When domain

speciﬁc questions (D) are used as the features, we also obtain low compactness

scores, albeit being larger than using A. If only generic questions, i.e., G, are

utilized, worse clustering solutions are seen. When comparing the results ob-

tained by using G with those of using G+AP2, we can see that adding AP2 to
G contributes to a better clustering. Concerning QS2 where only privacy rele-
vant questions are considered, we see that using domain speciﬁc privacy relevant

questions (DP1) allows us to gain the most discriminative clusters. Using the

subset of privacy relevant questions, i.e., AP1 is also beneﬁcial to the clustering

of user proﬁles.

19

For QS3, there are comparable clustering solutions when using the features

sets G1, G3, G4, and G5. The best clustering is obtained with G2.

The silhouette scores in Figure 7(b), Figure 7(d), and Figure 7(f) further

enforce the compactness ones. A is the feature set that achieves the best sil-
houette for QS1. Adding AP2 to G helps achieve a better clustering solution,
compared to using only G.

Answer to RQ2. According to the performed evaluation, generic questions
plus generalizable ones (i.e., G+AP2) provide the best clustering solution.

5.3. RQ3: To which extent is PisaRec able to utilize the obtained categorization

in recommending relevant privacy settings to users?

An issue with clustering is whenever there is a new user to be classiﬁed,

it is necessary to re-run the whole process. This is a time consuming phase,

especially where there is a large number of users. Thus, we propose a more

feasible way to assign new users to clusters, avoiding repetitive clustering.

Figure 8: ROC curves, three categories.

Given that there is an existing categorization of user proﬁles, the feed-

forward neural network presented in Section 3.2 is used to classify a new user

into a suitable group. Once clusters have been obtained, we feed them as input

to train the neural network and perform the testing using the ten-fold cross-

20

validation procedure. It is worth mentioning that we use three clusters instead

of four as explained in Section 2.2.

The ﬁnal performance measured by means of ROC curves is depicted in

Figure 8. In particular, the AUC values for Class 0, Class 1, and Class 2 are 0.83,
0.85, and 0.76, respectively. The curves representing the three classes reside
near the upper left corner, implying a good prediction performance. Overall,

the curves and the AUC values demonstrate that the obtained performance

is much better compared to that before clustering in Figure 5 and Figure 6.

This suggests that properly clustering user proﬁles can substantially increase

the neural network’s prediction performance.

Next, we validate the performance of PisaRec as follows. We opted for the

ten-fold cross validation technique [29], where the dataset is split into ten equal

folds, and the evaluation is done in ten rounds. By each round, one fold is

utilized as testing, and the other nine folds are merged to create the training

data. In a testing fold, for each user, the features are split into two parts, one

part is fed as query, and the remaining part is removed to be used as ground-

truth data. The ratio of the number of settings used as query to the total

number of settings is called α. This simulates a real scenario, where the user

already speciﬁed some settings, and the system is expected to recommend the

rest, corresponding to the ground-truth data. For each user, PisaRec returns

a ranked list of N settings (N is conﬁgurable), and the evaluation metrics are

computed on the test set as follows. The recommended items are then compared

with the ground-truth data to evaluate the performance. Eventually, we average

out the metrics obtained from the testing folds to produce the ﬁnal results.

We experiment with diﬀerent conﬁgurations by varying α, k: the number

of neighbor users used for the computation, N : the number of recommended
items. In particular, α = {0.1, 0.3, 0.5}; k = {3, 5, 10, 15}; and N is varied from
1 to 50, simulating a real-world scenario where users have to set several settings.

The precision-recall curves are then sketched following these parameters.

As seen in Figure 9, when α = 0.1, i.e., only a small amount of data is
used as query, PisaRec recommends relevant settings to users, however with

21

Figure 9: Conﬁguration C1.

considerably low precision and recall. For instance, when k = 3, a maximum
precision of 0.52 is obtained and the maximum precision is 0.7 when k = 15.
Similarly, the recall scores are low, i.e., smaller than 0.4 by all the conﬁgurations.

Altogether, this implies a mediocre performance which is understandable as the

conﬁguration with α = 0.1 corresponds to the case where the user only speciﬁed
a few settings, and the system has limited context to recommend additional

settings.

Figure 10: Conﬁguration C2.

22

00.10.20.30.40.50.60.70.80.20.30.40.50.60.70.80.9k=3k=5k=10k=15RecallPrecision00.10.20.30.40.50.60.70.80.20.30.40.50.60.70.80.9k=3k=5k=10k=15RecallPrecisionWhen we increase α to 0.3, there is an improvement in both precision and
recall as in Figure 10, compared to the results obtained with α = 0.1 in Figure 9.
Precision scores are always larger than 0.55 in the conﬁgurations, with 0.80

being the maximum value. Similarly, we also see that recall scores are gradually

improved. For instance, a maximum recall of 0.35 is achieved with k = 3, and
the corresponding maximum for k = 15 is 0.48.

Figure 11: Conﬁguration C3.

Such an improvement is more evident when α = 0.50, i.e., a half of the
settings is used as query. In Figure 11, apart from some outliers, most of the

precision scores are larger than 0.70, with 0.85 as the maximum value. Com-

pared to the previous conﬁgurations with α = 0.1 and α = 0.3, the recall scores
are also better, i.e., with a longer list of items, recall increases substantially. In

particular, a recall of 0.73 is seen when k = 10 and k = 15.

Concerning the number of neighbors used for computing recommendations,

i.e., k (see Section 3.3 and Formula 3), by considering Figure 9, Figure 10, and

Figure 11 together, it is evident that adding more users for the computation

contributes to a better prediction performance. For instance, by increasing k

from 3 to 5, 10, and 15, we boost both precision and recall by all the cut-oﬀ
values N .

23

00.10.20.30.40.50.60.70.80.20.30.40.50.60.70.80.9k=3k=5k=10k=15RecallPrecisionAltogether, the experimental results show that even if users perceive their

categories diﬀerently as shown in RQ1, once we have identiﬁed their right pri-
vacy group PisaRec can exploit the categories to provide relevant settings to

users, though the considered dataset is pretty small. We anticipate that its

performance can be further enhanced, if there is more data for training.

Answer to RQ3. PisaRec recommends highly relevant settings to a user,
though there is limited amount of data available for training. The prediction

performance improves alongside the amount of data fed as input.

6. Discussion

This section provides discussion related to the possible extensions of our

work, as well as the threats to validity of our ﬁndings.

6.1. Extendability

Dataset.

In our work, we utilized a small dataset for the evaluation. The

amount of training data may impact the performance of both the clustering

and classiﬁcation phases. Moreover, as PisaRec is a collaborative-ﬁltering rec-

ommender system, its performance is heavily driven by the quality and amount

of data. We anticipate that we may need to calibrate the systems’ parameters

to maintain both timing eﬃciency and eﬀectiveness with more data.

The unsupervised algorithm. In the scope of this paper, we used the K-

Medoids algorithm to cluster the user proﬁles. Such a technique has been cho-

sen due to its simplicity and eﬀectiveness. In fact, several clustering algorithms

could be employed to categorize user proﬁles. Thus, the outcome of a cluster-

ing solution depends heavily on the considered techniques. We plan to extend

our work by considering other clustering algorithms, such as CLARA [31], or

DBSCAN [32].

The supervised classiﬁer. The neural network used to classify user proﬁles

may be suitable only for the considered dataset. For a diﬀerent dataset, it

is necessary to ﬁnd adequate network conﬁgurations employing an empirical

24

evaluation. For instance, the number of hidden layers, or the number of neurons

for each layer, should be considerably increased to deal with a larger number of

user proﬁles.

6.2. Threats to validity

We are aware of the existence of some threats that might harm the validity

of the performed experiments as they are presented as follows.

• Threats to construct validity are related to any factor that can compro-

mise the validity of the given observations. The main threat to construct

validity is related to the size of the analyzed data. The used dataset is in-

deed relatively small but has the advantage of coming from a recent work

[7] thus reﬂecting users’ contemporary privacy behaviors. More extensive

experiments are under planning encompassing other ethical dimensions

beyond privacy.

• Concerning the threats to internal validity, i.e., any confounding factor

that could inﬂuence our ﬁndings, we attempted to avoid any bias in the

automatic creation of user proﬁles and in the way we split the full data

into groups. We tried to mitigate this threat by semantically analyzing

and double-checking the clusters obtained by the proposed approach.

• Concerning the threats to external validity, they are related to the general-

izability of our results. This is about checking the adequacy of our privacy

proﬁles in other contexts, notably, in the traveling or IoT domains. Gener-

alizability is actually our initial driver for extracting privacy proﬁles from

general moral questions. Thus, further experimental evidence is planned

to support the reported paper results.

7. Related work

This section reviews the related work and their main characteristics to posi-

tion our approach in the current scenario for eliciting, proﬁling, and predicting

user privacy preferences.

25

7.1. Overview

The work presented in this paper has been done in the context of the design of

the EXOSOUL research project that aims at providing users with a personalized

software layer that mediates users’ interactions with the digital world according

to user’s ethics, including privacy preferences [33, 13].

According to various studies [34, 35], the vast majority of users do not bother

to read privacy agreements because of the excessive language and confusing

explanations [36, 37, 38, 39]; it is unreasonable also to expect they will read them

on a regularly basis [40]. Resignation from privacy choices may also be a result

of their dissatisfaction with the lack of options and excessive complexity [41].

Privacy proﬁling is at the core of our work therefore, most related studies are

on user clustering, privacy proﬁling, and privacy preferences settings. The more

signiﬁcant part of existing studies about privacy proﬁling develop on the work of

Westin [1]. Based on a series of privacy-related surveys, the author established

“Privacy Indexes” for most of these polls to summarize results, indicate trends in

privacy concerns, and suggest a widely recognized segmentation methodology

of “Privacy Proﬁles.”The methodology he applied classiﬁes people into three

categories: privacy fundamentalists, pragmatists, and unconcerned. Because of

the commercial nature of Westin’s surveys, the methodology and the details

of how privacy indexes were calculated are not fully disclosed, so we rely on

subsequent works [2] that deeply analyzed and reported them.

Westin Segmentation, and particularly the pragmatism adherence of the

consumers were criticized by the work of Hoofnagle and Urban [42, 43]. Their

experimental work investigated customer expectations for privacy safeguards,

showing that many people believe they have greater protection than they really

do due to a lack of knowledge about corporate practices, privacy policies, and

data usage limits. Westin’s methodology has been applied, revised, and ex-

panded in several empirical studies on privacy that include collected data. The

categorizations that are most relevant to our research are reported in Table 1.

26

Dupree et al. [14] analyzed data from surveys and participants interviews, the

authors identiﬁed ﬁve user clusters that emerge from end-user behaviors, includ-

ing Fundamentalists, Lazy Experts, Technicians, Amateurs and the Marginally

Concerned. Schairer et al. [15] presented a model of privacy disposition and

its development based on qualitative research on privacy considerations in the

context of emerging health technologies. The authors identiﬁed six clusters,

including Fatalism, Nothing to hide, Something to hide, Tradeoﬀ, Personal re-

sponsibility, Moral right.

In the research proposed by Sanchez et al. [7] the

authors presented the results of a ﬁtness-related simulation and questionnaire

to classify users according to their privacy-related preferences. They used two

diﬀerent sets of labels for their clusters, one for the computed privacy-proﬁle

assignment consisting of six groups and one for self-assessment they proposed

to the users consisting in four groups. The ﬁrst clusters were labeled as: Uncon-

cerned, Socially Active, Health-focused, Minimal, Anonymous, Strict. The sec-

ond clusters were labeled as: Privacy Conservative, Unconcerned, Fence-Sitter,

Advanced User.

7.2. Proﬁling and Clusterization

Diﬀerent approaches were used in recent works to create user proﬁling start-

ing from data collection and analysis. Lee and Kobsa [5] performed a cluster

analysis on online survey data composed of IoT scenarios and user responses

like reaction parameters. Because all parameters have either categorical or or-

dinal values, the authors utilized K-modes, a variant of the K-means clustering

algorithm. Qin et al. [4] proposed a user’s preferences prediction through clus-

terization of partial preference relations on the MovieLens dataset, which is

commonly used to test collaborative ﬁltering technology. According to Fern-

quist et al. [? ], users may also be identiﬁed by their data proﬁles created

by their device based on time and events: researchers gathered information on

how and when people use their networked devices, recording the time period in

which a user interacts or transmits data and the speciﬁc place. For the sake of

interpreting their ﬁndings, the researchers took into account three distinct sorts

27

of events: voice calls, texts, and data transfers, as well as combinations of these.

The ﬁndings showed that the proﬁles studied may be used to identify the user.

7.3. Automating privacy settings

Concerning automating privacy preferences settings, the closest approach is

by Liu et al. [6, 8] and by the Personalized Privacy Assistant Project team [23].

Their approach employs user categorizations that are obtained by mining exist-

ing privacy settings in the app domain, complemented with an initial dialogue

with the user to select the appropriate proﬁle. Our approach is also based on

privacy proﬁles. However, they are obtained by analyzing data resulting from

questions that relate to the user’s ethics and are not concerned with any speciﬁc

domain.

Wilson et al. [44] identiﬁed the impact of privacy proﬁles on the preferences,

sharing inclinations, and overall satisfaction levels of users of location-sharing

apps. Their ﬁndings demonstrate that privacy proﬁles for location sharing set-

tings can have a long-lasting impact on how users perceive their privacy, even in

the face of ongoing opportunities to reﬂect on the sharing outcomes that result

from their chosen settings. This implies that attempts to simplify privacy set-

tings should be performed with caution since such simplicity may easily impact

the people with whom the settings are intended to interact and educate.

Brandimarte et al. [45] investigated the concept that giving people a greater

sense of control over the release and access to private information – even in-

formation that enables them to be personally identiﬁed – would improve their

willingness to provide sensitive information. If their desire to reveal adequately

rises, this control may, counter-intuitively, result in being more slack.

In their research, participants in a publication were informed that a proﬁle

comprising their information would be produced automatically and published

online once the website was ﬁnished. Other participants were informed that

only half of their proﬁles would be published online. The uncertain publishing

condition was designed to reduce participants’ sense of control over the pub-

lic distribution of their survey responses without actually decreasing access by

28

others. Their theories predicted that decreasing control would limit the desire

to reveal in the uncertain publishing condition, notwithstanding lower external

costs or hazards. According to the researchers’ results, if individuals behave in

enough oﬀsetting manner, devices supposed to safeguard them might instead

end up worsening the hazards they confront.

7.4. Surveys and regulations

Based on previous research in survey technique and related domains, Red-

miles et al. [46] provide a set of important recommendations for conducting

self-report usability studies. There are established criteria and suggestions for

collecting good quality self-report data in other sectors that depend on self-

report data, such as health and social sciences. We used this information as a

guideline for selecting and reﬁning question groups.

As discussed by Emami-Naeini et al. [47], surveys and interviews can be

administered with consolidated methodologies like the Delphi Method. This

method is “a method for the systematic solicitation and collection of judgments

on a particular topic through a set of carefully designed sequential questionnaires

interspersed with summarized information and feedback of opinions derived from

earlier responses” [48]. Using a three-round Delphi process, the authors con-

ducted an expert elicitation study with 22 privacy and security experts to iden-

tify the factors that experts believe are important for consumers to consider

when comparing the privacy and security of IoT devices to inform their pur-

chase decisions. The same methodology could be used to elicit preferences from

the users.

Considering the research theme, we took into account the General Data

Protection Regulation (GDPR) [49], the document that governs the storing,

processing, and use of personal data by the European Union (EU) as of May

25, 2018. Even if not based in the EU, the GDPR applies to all third parties

that operate in the EU market or access the data of EU citizens.

29

8. Conclusion and future work

This paper proposes a holistic approach consisting of both supervised and

unsupervised learning to identify privacy proﬁles. By ﬁnding a set of questions

suitable for assessing privacy proﬁles, we recommend relevant privacy prefer-

ences to users. An empirical study on the proposed system using a ﬁtness

dataset shows that generic questions are suitable for categorizing user proﬁles

and recommending privacy settings. For future work, besides developing fur-

ther experimental evidence supporting the results reported in this paper, we will

work in the direction of building user proﬁles that cover other ethical dimensions

beyond privacy. Last but not least, in the scope of the Exosoul project, we will

deploy the conceived techniques to analyze data collected from users, studying

the characteristics of users’ behaviors and their attitudes in the digital world.

Acknowledgements

The research described in this paper has been carried out as part of the

EXOSOUL project and also partially supported by the Centre of excellence

EX-EMERGE (Centre of EXcellence on Connected, Geo-localized and Cyber-

secure vehicles) of the University of L’Aquila, Italy.

References

[1] A. F. Westin, Bibliography of surveys of the u.s. public, 1970-2003 (2003).

URL https://web.archive.org/web/20051224000944/http://www.

privacyexchange.org/iss/surveys/surveybibliography603.pdf

[2] P. Kumaraguru, L. F. Cranor, Privacy Indexes: A Survey of Westin’s Stud-

ies, Tech. Rep. CMU-ISRI-5-138, Institute for Software Research Inter-

national, School of Computer Science, Carnegie Mellon University, Pitts-

burgh, PA (Dezember 2005).

[3] Y. Zhao, Y. Yu, Y. Li, G. Han, X. Du, Machine learning based privacy-

preserving fair data trading in big data market, Information Sciences 478

30

(2019) 449–460. doi:https://doi.org/10.1016/j.ins.2018.11.028.

URL

https://www.sciencedirect.com/science/article/pii/

S0020025518309174

[4] M. Qin, S. Buﬀett, M. W. Fleming, Predicting user preferences via

similarity-based clustering, in: S. Bergler (Ed.), Advances in Artiﬁcial Intel-

ligence, Springer Berlin Heidelberg, Berlin, Heidelberg, 2008, pp. 222–233.

[5] H. Lee, A. Kobsa, Understanding user privacy in internet of things environ-

ments, in: 2016 IEEE 3rd World Forum on Internet of Things (WF-IoT),

2016, pp. 407–412. doi:10.1109/WF-IoT.2016.7845392.

[6] J. Lin, B. Liu, N. Sadeh, J. I. Hong, Modeling users’ mobile app privacy

preferences: Restoring usability in a sea of permission settings, in: 10th

Symposium On Usable Privacy and Security ({SOUPS} 2014), 2014, pp.

199–212.

[7] O. R. Sanchez, I. Torre, Y. He, B. P. Knijnenburg, A recommenda-

tion approach for user privacy preferences in the ﬁtness domain, User

Modeling and User-Adapted Interaction 30 (3) (2020) 513–565.

doi:

10.1007/s11257-019-09246-3.

URL https://doi.org/10.1007/s11257-019-09246-3

[8] B. Liu, Can Machine Learning Help People Conﬁgure Their Mobile App

Privacy Settings? (1 2020). doi:10.1184/R1/11591340.v1.

URL

https://kilthub.cmu.edu/articles/thesis/Can_Machine_

Learning_Help_People_Configure_Their_Mobile_App_Privacy_

Settings_/11591340

[9] McKinsey, How COVID-19 has pushed companies over the technology tip-

ping point—and transformed business forever (https://mck.co/3trP4OV),

2020.

URL https://mck.co/3trP4OV

31

[10] OECD, OECD Digital Economy Outlook 2020, 2020.

doi:https:

//doi.org/https://doi.org/10.1787/bb167041-en.

URL

https://www.oecd-ilibrary.org/content/publication/

bb167041-en

[11] A. Pujahari, D. S. Sisodia, Modeling side

information in pref-

erence

relation

based

restricted

boltzmann machine

for

rec-

ommender

systems,

Information

Sciences

490

(2019)

126–145.

doi:https://doi.org/10.1016/j.ins.2019.03.064.

URL

https://www.sciencedirect.com/science/article/pii/

S0020025519302774

[12] B. Liu, M. S. Andersen, F. Schaub, H. Almuhimedi, S. Zhang, N. Sadeh,

A. Acquisti, Y. Agarwal, Follow my recommendations: A personalized pri-

vacy assistant for mobile app permissions, in: Proceedings of the Twelfth

USENIX Conference on Usable Privacy and Security, SOUPS ’16, USENIX

Association, 2016, pp. 27–41, event-place: Denver, CO, USA.

[13] M. Autili, D. D. Ruscio, P. Inverardi, P. Pelliccione, M. Tivoli, A software

exoskeleton to protect and support citizen’s ethics and privacy in the digital

world, IEEE Access 7 (2019) 62011–62021. doi:10.1109/ACCESS.2019.

2916203.

URL https://doi.org/10.1109/ACCESS.2019.2916203

[14] J. L. Dupree, R. Devries, D. M. Berry, E. Lank, Privacy personas: Clus-

tering users via attitudes and behaviors toward security practices, in: Pro-

ceedings of the 2016 CHI Conference on Human Factors in Computing

Systems, CHI ’16, Association for Computing Machinery, New York, NY,

USA, 2016, p. 5228–5239. doi:10.1145/2858036.2858214.

URL https://doi.org/10.1145/2858036.2858214

[15] C. E. Schairer, C. Cheung, C. Kseniya Rubanovich, M. Cho, L. F.

Cranor, C. S. Bloss, Disposition toward privacy and information dis-

closure in the context of emerging health technologies, Journal of

32

the American Medical

Informatics Association 26 (7)

(2019) 610–

619.

arXiv:https://academic.oup.com/jamia/article-pdf/26/7/

610/34151415/ocz010.pdf, doi:10.1093/jamia/ocz010.

URL https://doi.org/10.1093/jamia/ocz010

[16] H.-S. Park, C.-H. Jun, A simple and fast algorithm for k-medoids cluster-

ing, Expert Systems with Applications 36 (2, Part 2) (2009) 3336–3341.

doi:https://doi.org/10.1016/j.eswa.2008.01.039.

URL

https://www.sciencedirect.com/science/article/pii/

S095741740800081X

[17] K. Gurney, An Introduction to Neural Networks, Taylor and Francis, Inc.,

USA, 1997.

[18] C. M. Bishop, Neural networks for pattern recognition, Oxford University

Press, Inc., New York, NY, USA, 1995.

[19] G. Zhang, B. Eddy Patuwo, M. Y. Hu, Forecasting with artiﬁcial neural

networks:: The state of the art, International Journal of Forecasting 14 (1)

(1998) 35–62.

[20] J. B. Schafer, D. Frankowski, J. Herlocker, S. Sen, The adaptive web,

Springer-Verlag, Berlin, Heidelberg, 2007, pp. 291–324.

[21] C. Aggarwal, Neighborhood-based collaborative ﬁltering, in: Recommender

systems: The textbook, Springer International Publishing, Cham, 2016, pp.

29–70.

[22] T. D. Noia, V. C. Ostuni, Recommender systems and linked open data,

in: W. Faber, A. Paschke (Eds.), Reasoning Web. Web Logic Rules - 11th

International Summer School 2015, Berlin, Germany, July 31 - August 4,

2015, Tutorial Lectures, Vol. 9203 of Lecture Notes in Computer Science,

Springer, 2015, pp. 88–113. doi:10.1007/978-3-319-21768-0\_4.

URL https://doi.org/10.1007/978-3-319-21768-0_4

33

[23] N. Sadeh, B. Liu, A. Das, M. Degeling, F. Schaub, Personalized privacy

assistant, uS Patent 10,956,586 (Mar. 23 2021).

[24] Y. Liu, Z. Li, H. Xiong, X. Gao, J. Wu, Understanding of internal cluster-

ing validation measures, in: Proceedings of the 2010 IEEE International

Conference on Data Mining, ICDM ’10, IEEE Computer Society, USA,

2010, p. 911–916. doi:10.1109/ICDM.2010.35.

URL https://doi.org/10.1109/ICDM.2010.35

[25] T. Fawcett, An introduction to roc analysis, Pattern Recogn. Lett. 27 (8)

(2006) 861–874. doi:10.1016/j.patrec.2005.10.010.

URL https://doi.org/10.1016/j.patrec.2005.10.010

[26] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, J. Gao,

Deep learning–based text classiﬁcation: A comprehensive review, ACM

Comput. Surv. 54 (3) (Apr. 2021). doi:10.1145/3439726.

URL https://doi.org/10.1145/3439726

[27] J. A. Burns, G. M. Whitesides, Feed-forward neural networks in chemistry:

Mathematical systems for classiﬁcation and pattern recognition, Chem.

Rev. 93 (1993) 2583–2601, 377.

[28] P. T. Nguyen, J. Di Rocco, D. Di Ruscio, A. Pierantonio, L. Iovino, Auto-

mated classiﬁcation of metamodel repositories: A machine learning ap-

proach,

in: 2019 ACM/IEEE 22nd International Conference on Model

Driven Engineering Languages and Systems (MODELS), 2019, pp. 272–

282. doi:10.1109/MODELS.2019.00011.

[29] R. Kohavi, A study of cross-validation and bootstrap for accuracy estima-

tion and model selection, in: Proceedings of the 14th international joint

conference on artiﬁcial intelligence - volume 2, IJCAI’95, Morgan Kauf-

mann Publishers Inc., San Francisco, CA, USA, 1995, pp. 1137–1143.

[30] X. Zhu, X. Wu, Class noise vs. attribute noise: A quantitative study of

their impacts, Artif. Intell. Rev. 22 (3) (2004) 177–210.

34

[31] L. Kaufman, P. J. Rousseeuw, Finding Groups in Data: An Introduction

to Cluster Analysis., John Wiley, 1990.

[32] M. Ester, H.-P. Kriegel, J. Sander, X. Xu, A density-based algorithm for

discovering clusters in large spatial databases with noise, in: Proceedings

of the Second International Conference on Knowledge Discovery and Data

Mining, KDD’96, AAAI Press, 1996, p. 226–231.

[33] Exoskeleton, The software exoskeleton.

URL https://exosoul.disim.univaq.it/

[34] G. R. Milne, M. J. Culnan, Strategies for reducing online privacy risks:

Why consumers read (or don’t read) online privacy notices, Journal of

interactive marketing 18 (3) (2004) 15–29.

[35] J. A. Obar, A. Oeldorf-Hirsch, The biggest lie on the internet: Ignoring the

privacy policies and terms of service policies of social networking services,

Information, Communication & Society 23 (1) (2020) 128–147.

[36] J. Bhatia, T. D. Breaux, J. R. Reidenberg, T. B. Norton, A theory of

vagueness and privacy risk perception, in: 2016 IEEE 24th International

Requirements Engineering Conference (RE), IEEE, 2016, pp. 26–35.

[37] C. Jensen, C. Potts, Privacy policies as decision-making tools: an evalua-

tion of online privacy notices, in: Proceedings of the SIGCHI conference

on Human Factors in Computing Systems, 2004, pp. 471–478.

[38] A. M. McDonald, R. W. Reeder, P. G. Kelley, L. F. Cranor, A comparative

study of online privacy policies and formats, in: International Symposium

on Privacy Enhancing Technologies Symposium, Springer, 2009, pp. 37–55.

[39] J. R. Reidenberg, T. Breaux, L. F. Cranor, B. French, A. Grannis, J. T.

Graves, F. Liu, A. McDonald, T. B. Norton, R. Ramanath, Disagreeable

privacy policies: Mismatches between meaning and users’ understanding,

Berkeley Tech. LJ 30 (2015) 39.

35

[40] A. M. McDonald, L. F. Cranor, The cost of reading privacy policies, Isjlp

4 (2008) 543.

[41] J. Colnago, Y. Feng, T. Palanivel, S. Pearman, M. Ung, A. Acquisti, L. F.

Cranor, N. Sadeh, Informing the design of a personalized privacy assistant

for the internet of things, in: Proceedings of the 2020 CHI Conference on

Human Factors in Computing Systems, 2020, pp. 1–13.

[42] C. J. Hoofnagle, J. M. Urban, Alan westin’s privacy homo economicus,

Wake Forest L. Rev. 49 (2014) 261.

[43] J. M. Urban, C. J. Hoofnagle, The privacy pragmatic as privacy vulnerable,

in: Symposium on Usable Privacy and Security (SOUPS 2014) Workshop

on Privacy Personas and Segmentation (PPS), 2014.

[44] S. Wilson, J. Cranshaw, N. Sadeh, A. Acquisti, L. F. Cranor, J. Springﬁeld,

S. Y. Jeong, A. Balasubramanian, Privacy manipulation and acclimation

in a location sharing application, in: Proceedings of the 2013 ACM inter-

national joint conference on Pervasive and ubiquitous computing, 2013, pp.

549–558.

[45] L. Brandimarte, A. Acquisti, G. Loewenstein, Misplaced conﬁdences: Pri-

vacy and the control paradox, Social psychological and personality science

4 (3) (2013) 340–347.

[46] E. M. Redmiles, Y. Acar, S. Fahl, M. L. Mazurek, A summary of survey

methodology best practices for security and privacy researchers, Tech. rep.

(2017).

[47] P. Emami-Naeini, Y. Agarwal, L. F. Cranor, H. Hibshi, Ask the experts:

What should be on an iot privacy and security label?, in: 2020 IEEE

Symposium on Security and Privacy (SP), IEEE, 2020, pp. 447–464.

[48] C. R. Atherton, Group techniques for program planning: A guide to nomi-

nal group and delphi processes. by andré l. delbecq, andrew h. van de ven,

36

and david h. gustafson. glenview, ill.: Scott, foresman & co., 1975. 174 pp.

4.75 paper and interpersonal conﬂict resolution. by alan c. ﬁlley (1976).

[49] T. E. Parliament, the Council of the European Union, Eu general data

protection regulation (gdpr) - regulation eu 2016/679 of the european par-

liament and of the council of 27 april 2016, Oﬃcial Journal of the European

Union (2016).

37

