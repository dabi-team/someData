On Strong-Scaling and Open-Source Tools for High-Throughput
Quantiﬁcation of Material Point Cloud Data: Composition Gradients,
Microstructural Object Reconstruction, and Spatial Correlations

Markus Kühbach1,2, Vitor Vieira Rielli3, Sophie Primig3, Alaukik Saxena4, David Mayweg4,5,
Benjamin Jenkins5, Stoichko Antonov4, Alexander Reichmann7, Stefan Kardos7, Lorenz Romaner7,
and Sandor Brockhauser1

1Consortium FAIRmat, Humboldt-Universität zu Berlin, Zum Großen Windkanal 2, D-12489 Berlin, Germany.
2Structure Research & Electron Microscopy Group, Department of Physics, Humboldt-Universität zu Berlin, Newtonstraße 15,
D-12489 Berlin, Germany.
3School of Materials Science & Engineering, UNSW Sydney, Kensington, 2052 NSW, Australia.
4Max-Planck-Institut für Eisenforschung GmbH (MPIE), Max-Planck-Straße 1, D-40237 Düsseldorf, Germany.
5Chalmers University of Technology, Department of Physics, Division of Microstructure Physics, Fysikgränd 3, SE-412 96
Göteborg, Sweden.
6Department of Materials, University of Oxford, 16 Parks Road, Oxford, OX1 3PH, United Kingdom.
7Department of Materials Science, Montanuniversität Leoben, Franz Josef-Straße 18, A-8700 Leoben, Austria.

Abstract

Characterizing microstructure-material-property relations calls for software tools which extract point-cloud- and
continuum-scale-based representations of microstructural objects. Application examples include atom probe, electron,
and computational microscopy experiments. Mapping between atomic- and continuum-scale representations of mi-
crostructural objects results often in representations which are sensitive to parameterization; however assessing this
sensitivity is a tedious task in practice.

Here, we show how combining methods from computational geometry, collision analyses, and graph analytics yield
software tools for automated analyses of point cloud data for reconstruction of three-dimensional objects, character-
ization of composition proﬁles, and extraction of multi-parameter correlations via evaluating graph-based relations
between sets of meshed objects. Implemented for point clouds with mark data, we discuss use cases in atom probe
microscopy that focus on interfaces, precipitates, and coprecipitation phenomena observed in diﬀerent alloys. The
methods are expandable for spatio-temporal analyses of grain fragmentation, crystal growth, or precipitation.

2
2
0
2

y
a
M
6
2

]
i
c
s
-
l
r
t

m

.
t
a
m
-
d
n
o
c
[

1
v
0
1
5
3
1
.

5
0
2
2
:
v
i
X
r
a

1

 
 
 
 
 
 
1

Introduction

Microscopy techniques, such as electron microscopy (EM) [1, 2], atom probe microscopy (APM) [3–6], and even computa-
tional microscopy [7–9], are essential tools for characterizing the atomic architecture of materials. Collecting quantitative
evidence in support of or against a set of research hypotheses is the purpose of microscopy research. Microscopy data
are processed into descriptors that can be used in physically- or artiﬁcial-intelligence-based surrogate models to decode
material properties and develop understanding as to how microstructures change with processing and also during service.
In most cases, descriptors are of two kinds: Either they are quantities at the continuum-scale, like (spatial) statistics of
crystal defect or atom ensembles, such as dislocation density, or they are spatially-detailed three-dimensional descriptors
for the static arrangement or oftentimes even dynamics of crystal defect ensembles. Examples include descriptions of
point [10] and line defects [8, 11, 12] or of grains and crystals of diﬀerent thermodynamic phases and descriptions for the
junctions in the network of crystal defects of a microstructure [13–15].

Such a scale-bridging encoding of point clouds into microstructural objects requires assumptions and models which
have often adjustable parameters which control the shape, extent, and topology of the objects’ representation. The
signiﬁcance of this parameter sensitivity should not only be explored but ideally quantiﬁed in detail within a research
study. Enabling and supporting researchers with this quantiﬁcation is one key role of software tools. In the condensed-
matter physics as well as the computational materials science community, a substantial number of simulation tools are not
only open-source [16–18] but are especially build to reduce barriers with management and comparison of results according
to the FAIR data stewardship principles [19–21]. These activities enable and drive the development of programmatically
usable scientiﬁc software around existing (software) tools and instruments to explore and assess the parameter sensitivity
of descriptors, their uncertainty, and the vast set of materials they describe.

For the materials-science-branch of especially the EM community sophisticated open-source tools have been developed
(e.g.
[22–24]) but sophisticated platforms for sharing experimental data remain to be developed. For the atom probe
microscopy community, the situation with respect to data sharing platforms is similar [21] but fewer options of general
enough open-source software exist which could enable domain scientists to take advantage of specialist tools’ and algo-
rithms of other scientiﬁc communities such as computational geometry. Many microscopes are operated with proprietary
software which combines metadata management, data acquisition, and analysis services in a single application through a
graphical user interface (GUI). Provisioning instrument calibrations, abstracting hardware layers, and oﬀering intuitive
GUIs are advantages of these tools for scientists with routine analysis needs.

However, such software can decouple scientists from the capabilities of open-source scientiﬁc software, unless vendors
interface their software via advanced programming interfaces (APIs) or scripting solutions. Here we report a set of tools
which can complement the vendors’ eﬀorts to equip scientists with a diverse set of tools, which should all ideally be
made interoperable. Our proposal is focused on automating analyses, motivating a more detailed understanding of the
functioning of numerical algorithms and the signiﬁcance of parameter sensitivity. With this we serve the improvement
of research processes by reducing needs for eventually unnecessarily ineﬀective or restrictive tools which currently are
barriers to FAIR-compliant research [25].

Atom probe scientists are one community who face this situation [21]. The two main techniques they use, atom
probe tomography (APT) and ﬁeld-ion microscopy (FIM), both indirectly measure positions of atoms. They both use
controlled ﬁeld evaporation to characterize a needle-shaped nanoscale specimen. This is done by holding the specimen in
an electric ﬁeld before superimposing a laser or high-voltage electric pulse to achieve controlled evaporation. For APT,
the atoms are measured as ions which are evaporated via the controlled pulsing and successively measured via position-
sensitive time-of-ﬂight-resolved mass spectrometry. For FIM, imaging gas atoms are used which ﬁeld-ionize speciﬁcally
above individual surface atoms and are then accelerated along the electric ﬁeld lines towards a position-sensitive detector
[26]. Exploitation of the fact that the ﬁeld strength can be changed to enforce also the evaporation of the surface atoms
has blurred the boundary between APT and FIM experiments [27, 28]. Data from such experiments can be processed
into three-dimensional tomographic reconstructions which are point cloud models of the evaporated specimen volume
with ion and atom type information. Despite diﬀerences in the position and ion-type resolution between APT and FIM,
these reconstructions oﬀer a unique combination of isotopic and sub-nanometer spatially-resolved information about the
atomic architecture of materials.

Like every experiment and computational model, the technique and the reconstruction faces limitations [29, 30]:
Limited detector eﬃciencies cause that not every atom is detectable. Limited mass-to-charge-resolving power causes
that not every ion can be decomposed into its atoms. The reconstruction process can result in regions of the dataset
which have a lower positional accuracy and precision as compared to electron microscopy (EM) experiments or especially
to (atom trajectory) data available with molecular dynamics simulations [9, 31]. These diﬃculties have motivated eﬀorts
towards using APM and EM correlatively [32, 33] surplus taking advantage of computational methods [34].

In eﬀect, atom probe microscopy has developed into an experimental technique whose capabilities of measuring a
statistically signiﬁcant number of ions delivered a tool for not only accurate and precise composition analysis [6] but
also for studies of the spatial arrangement of atoms and how they are organized as an ensemble of crystal defects at
diﬀerent length scales [6, 35]. Characterizing these so-called microstructural objects geometrically and developing eﬃcient
methods for assessing the parameter sensitivity of these geometrical descriptions is the focus of this work.

Our work is a continuation of open-source software development eﬀorts to support the scientiﬁc community [36].
Seminal previous work in this regard with relevance for the characterization of microstructural objects were the intro-
duction of iso-surface-based methods [37, 38] for revealing grain and phase boundaries (interfaces), signed-distance-based

2

composition quantiﬁcation, via so-called proxigrams [39, 40], computational-geometry-based methods [41–44] and con-
necting these with composition analyses and interfacial excess mapping [45–47], taking advantage of clustering methods
for segmenting precipitates or dislocations [12, 48–50], and recently the introduction of artiﬁcial intelligence methods
[51, 52]. Most of the associated software tools are proof-of-concept implementations. Their source code is shared openly.
The implementation and maintenance is in most cases decoupled from the commercial tools.

The more frequently the proprietary and open-source tools were used the more apparent became the sensitivity
of the descriptors they delivered as a function of the parameter settings [53, 54]. This revealed unsolved challenges:
Some are conceptual, like the assumption that using a single threshold value suﬃces to segment a dataset into useful iso-
surfaces. As this work supports this is an often inadequate [55–57]. Other challenges, like data accessibility restrictions in
commercial software, pose practical barriers with respect to how completely computational geometry, concentration ﬁeld,
and detailed metadata are exportable [21, 58]; and thus how APM research studies are (programmatically) comparable
to one another and can become FAIR-compliant. Suggestions were made by individual researchers how data from the
GUI of commercial software can be extracted into spreadsheets [59, 60]. As far as tasks like composition, interface-based
analyses, and microstructural-object-centric analyses are considered, we found only few cases where tools other than the
commercial ones were used [61–63] (apart from the case of interfacial excess quantiﬁcation [46, 64].)

To improve the situation we want to substantiate the assumption that numerically more eﬃcient and better, in the
sense of more automated and more functionally capable, tools can be developed by embracing open-source tools from
diﬀerent scientiﬁc communities and blending them into a set of complementary open-source tools that can be coupled
into complex workﬂows and customized without barriers.

In previous studies [65, 66], we substantiated the validity of this assumption for methods including spatial statistics
[65] and crystallography [66]. In continuation of this research, we exemplify that it is also possible to develop eﬃcient
parallelized tools which are programmatically automatable for analyzing point cloud data for composition and object-
based geometrical analyses. Speciﬁcally, in this work we blend tools from computational geometry, robotics, game engine,
and graph analytics communities. We exemplify their beneﬁt when processing point cloud data - here using diﬀerent
datasets from atom probe microscopy. Speciﬁcally, solutions for the following data analysis tasks are developed:

1. Diﬀerent methods are implemented for representing the edge of datasets and compare these representations geomet-
rically and quantitatively. The methods are useful for reducing bias through detection of object-edge-intersections.
These results are useful for quantifying parameter sensitivity and detection of mesh properties such as closure.

2. Methods for distance-based segmentation of datasets are implemented which accept generic triangulated surface
meshes and/or input from clustering analyses, obtained from other software tools, to segment datasets volumetri-
cally into regions and yield corresponding volume composite or surface meshes of three-dimensional regions.

3. We implement an open-source tool for delocalization, i.e. smoothing and discretizing point cloud data and sub-
sequently executed iso-surface-based analyses on the such discretized datasets. We equip this tool with methods
which enable automated computational-geometry-based characterization of surface facets and three-dimensional
objects with respect to object closure, shape, size, and composition (ionic and/or elemental).

4. We revisit previous work of [46, 47] on methods for creating automated triangulated surface patches of interfaces
and show how a more robust numerical protocol can be used to make such analyses even more automated and
FAIR-compliantly documented.

5. We implement algorithms which automate the placing and aligning of an arbitrary number of regions-of-interest
(ROIs) at facets of triangulated surface meshes. Speciﬁcally, we study three important cases: Namely, interfaces
about closed objects (precipitates), surface patches between regions with preferential composition gradients to
exploit, and surface patches which reveal themselves in APM measurements only via their decoration with solutes.
We implement procedures which fully automate the characterization of composition proﬁles and interfacial excess
mapping for these ROIs. These keep automatically track of all metadata and probe the composition proﬁles
consistently where this is numerically possible.

6. We implement a tool which enables to logically relate an arbitrary number of such object-based results via graph
analytics to formulate robust methods for characterizing coprecipitation. We use these tools for studying the
sensitivity of object representations as a function of parameterization.

7. We explore multithreading options for these tools. All of them are applied to analyze experiments using a lap-
top. Finally, we document the scalability of our solution when processing among the nowadays largest known
reconstructions where a total of one billion ions is computed on a single computing node of a computer cluster.

These solutions are implemented as modiﬁed or additional tools respectively into the paraprobe-toolbox. This is a
modularized software toolbox with eﬃcient tools [65, 66] for programmatic analyses of point cloud data and inscribed
sets of other geometric primitives.

3

2 Methods

2.1 The paraprobe-toolbox is modularized

Context This work continues activities on building strong-scaling, open-source, and FAIR-compliant software tools
for processing point cloud data with associated mark data [21, 65–67] (the so-called paraprobe-toolbox). In this work
we improved existent tools and implemented two new ones (paraprobe-nanochem and paraprobe-intersector). Figure
1a summarizes the tools. An analysis is scripted in Python as a workﬂow using a jupyter notebook [68]. This removes
the need for routine users to deal with the C/C++ backend and can serve as a starting point for formulating work-
ﬂows via workﬂow management systems like pyiron [69]. The individual paraprobe-tools, displayed as blue nodes in
Fig. 1a, are instructed through an NeXus/HDF5 conﬁguration ﬁle, represented by the conﬁg.h5 nodes in turquoise of
Fig. 1a.
Internally the tools batch process a list of analysis tasks. Each tool and run returns an HDF5 ﬁle which
gives unrestricted access to all numerical data and metadata. Python convenience functions are available on the input
(paraprobe-parmsetup) and the post-processing side (paraprobe-autoreporter) to support users with creating conﬁgura-
tion ﬁles and with extracting commonly used quantities from the eventually deep data tree in the HDF5 ﬁle. Most of the
tools implement strong-scaling multithreading via Open Multi-Processing (OpenMP). Strategies for process parallelism
have been explored [65].

Figure 1: Sub-ﬁgure a) summarizes the collection of tools (bluish nodes) which the paraprobe-toolbox includes to solve speciﬁc sets of analyses for
three-dimensional point cloud data with ion or atom type marks and geometrical primitive sets. The tool interfaces with commercial APM software
and tools of the community via paraprobe-transcoder. This tool reads community ﬁle formats (turquoise nodes) as input. Each run of a tool has a
dedicated conﬁguration ﬁle (the conﬁg.h5 nodes) which complies with a NeXus application deﬁnition. The results of the tools, numerical data and
metadata, are stored as a set of speciﬁcally-formatted HDF5 binary ﬁles. Sub-ﬁgure b) details the workﬂow which paraprobe-nanochem implements,
representing an open-source code route of performing classical iso-surface-based analyses. The reconstructed volume is discretized into scalar ﬁelds
of element-speciﬁc counts, composition, or concentration, respectively via ion- or atom-type-speciﬁc kernel density estimation. These ﬁelds can be
exported. Subsequently, triangulated iso-surfaces can be approximated and post-processed to identify which triangles are connectable into triangle
clusters. Applying subsequently polygon mesh processing on these clusters enables to detect which triangle clusters represent surface meshes of closed
polyhedra and which represent isolated surface patches. Finally, ROIs can be placed and processed automatically for local composition analyses. All
surface mesh and patch data can be exported. The spatial arrangement of meshes can be analyzed with paraprobe-intersector. Supplementary ﬁles
are written for basic visualization via the eXtensible Data Model and Format (XDMF).

Tool modiﬁcations First, we refactored the tools of [65] to improve their modularity and replaced similar code portions
with more generally applicable functions. Unnecessary restrictions in [65] were also removed which makes possible now
studies with arbitrarily complex molecular ions [21], realized via an adjustable maximum number of atoms per molecular
ion (currently 32) to handle complex ions [70, 71] and a processing of selected elements or isotopes in molecular ions

4

(atomic decomposition). Analyses can now be restricted via applying spatial and/or ion attribute ﬁlters. Ions can be
ﬁltered based on type, hit multiplicity, or evaporation ID. Spatial ﬁlters can be combined with sub-sampling each n-th ion
of the dataset. Spatial ﬁlters are implemented via set operations for geometric primitive ROI(s) M ∈ R3 such as oriented
bounding boxes (OBBs), rotated cylinders, or spheres. These can be combined. Polyhedra could be implemented in the
future. Robust point inclusion tests [72, 73] for each respective primitive were implemented.

Importing reconstructions and ranging deﬁnitions Due to the availability of many community codes (e.g. [74])
for performing reconstruction and creating ranging deﬁnitions and the proprietary nature of reconstruction protocols
in commercial software, paraprobe does not implement an own tool for building reconstructions so far. Instead, each
analysis after having made a new measurement starts with using the paraprobe-transcoder tool to load a dataset as a pair
of tomographic reconstruction and ranging deﬁnitions. Data from third-party software like AP Suite, Inspico, or other
APM community software [36], see Fig. 1a, are supported. Community interaction with vendors would enable to make
this interface even more sophisticated in the future. Paraprobe-transcoder then creates a HDF5 ﬁle with ion positions,
mass-to-charge-state-ratio values, and a set of ranging deﬁnitions. Once transcoded, existent ranging deﬁnitions are
applied using paraprobe-ranger which computes an ion type array. Multiple mappings can be stored in the same HDF5
ﬁle to reﬂect that analyses in atom probe are often sensitive to ranging deﬁnitions and reconstruction parameters which
calls for a pedantic versioning [21].

Geometrical models for the edge of a dataset and distance-based ﬁltering In previous work [21, 65] we
discussed the importance of characterizing the edge of the reconstructed point cloud because atom probe specimens
can contain incompletely measured (truncated) microstructural objects such as precipitates or patches of grain or phase
boundaries. Edge models can be created and characterized with paraprobe-surfacer, paraprobe-distancer, or paraprobe-
tessellator.

We modiﬁed the paraprobe-surfacer tool to support convex hull computations and added diagnostics for α-shapes
[75, 76] such as returning the geometry of interior tetrahedra for given α-values, mesh closure tests, and the option to
characterize the set of possible α-shapes as a function of α-value, also to oﬀer a bridge to related work on α-shapes for
cluster analysis [77]. Paraprobe-surfacer can store the geometry of all created objects as triangle and tetrahedra sets.

The paraprobe-distancer tool implements functionalities for computing the shortest Euclidean distance of points to
an arbitrary inputted set of (non-degenerate) triangles. Practical applications are segmentations of datasets into ions
within a certain distance range to microstructural objects or said mesh of an edge model.

Tessellations and spatial statistics with distance-based ﬁltering Furthermore, we modiﬁed the paraprobe-
tessellator tool compared to [65] to support loading of optional input from paraprobe-distancer computations with
the aim to identify interfaces between adjacent Voronoi cells with speciﬁc cell attributes. First, paraprobe-tessellator
computes a Voronoi tessellation of the entire dataset. Second, the tool optionally reports facets of Voronoi cells only
for these cells whose associated distance attribute value is above dero to those cells whose neighboring cells’ distance is
below dero. This enables to compose surfaces similarly like it was reported for visualizing atomically-resolved slip planes
within molecular dynamics simulations [7, 8].

Also the paraprobe-spatstat tool [65] for computing spatial statistics was modiﬁed. Speciﬁcally, the tool was equipped
with a functionality which enables users to load two optional distance values per ion which can be used for ﬁltering ions
and thus restrict the computation to customizable arbitrarily shaped regions in the dataset in addition to the above-
mentioned spatial ﬁlters. A possible application are spatial statistics for all ions within a (signed) maximum distance to
a set of microstructural objects combined with the ﬁltering of ions for their distance to the edge of the dataset to reduce
bias.

Point-in-primitive inclusion tests and primitive intersection analyses The relative spatial arrangement of
individual points to geometric primitives (sphere, rotated cylinder, oriented bounding box, or polyhedron) is evaluated
with algorithms of the computational geometry community: Inclusion tests are used to identify if a point p ∈ Rd with
d = 2, 3 is lying inside (including the edge of the primitive) or outside given primitives. Self-intersections of polyhedra
[78] are tested for and reported. Paraprobe can handle polyhedra with convex and non-convex surface patches. The
shortest Euclidean distance between a point and a triangle is computed via a function from the Geometrytools [72]. The
shortest Euclidean distance between a point pA on a triangle A to its closest point pB on a triangle B is computed via
a function of the GammaUNC/PQP library [79–81].

Import cluster analysis results from third-party tools We saw the need for a utility tool, paraprobe-clusterer,
with which results from cluster analyses of commercial or other APM tools can be loaded into paraprobe. The tool takes
deﬁnitions of clusters from currently IVAS/AP Suite as input and disentangles the commercial encoding that all ions of
a cluster are stored with the same artiﬁcial mass-to-charge-state-ratio value. Paraprobe reconstructs from these values
the unique cluster IDs surplus recovers the evaporation ID of each ion by comparing ion positions.

Interface modeling A set of algorithms was implemented in paraprobe-nanochem which enables a computation of
triangulated surface meshes to a set of points in R3. These points are the reconstructed positions of the solute ions. Ions

5

are atomically decomposed, i.e. points duplicated with respect to their multiplicity within the ion. Most of the steps
use functionalities of the CGAL library, speciﬁcally algorithms oﬀered by the polygon mesh processing package [82, 83].
First, a set of points is ﬁltered from the dataset. These are the locations of the selected segregating species (eventually
further spatially ﬁltered) which guide the locating of the interface. This subset of the entire point cloud is interpolated
via principal component analysis (PCA) [84]. The resulting plane is taken to slice the axis-aligned bounding box to the
dataset via a polyline whose interior area is subsequently triangulated to yield an initial interface model.

Next, an iterative algorithm is used in which course the mesh is isotropically reﬁned, smoothen, consistent facet and
vertex normals computed, and evolved via DCOM [46]. During DCOM the barycenter of all solute ions within a control
sphere about each vertex is computed. The triangle vertices are then translated in the direction of their vertex normal
towards the respective barycenter positions. Vertices without ions in the control sphere are not touched. After each
DCOM iteration the mesh is inspected for self intersections. The evolution of the mesh is documented with writing
all meshes to the HDF5 results ﬁle. Automated placing and analyzing of ROIs follows the procedures described in the
automated composition proﬁling paragraph below.

NeXus/HDF5 data models and enabling cloud computing Another key modiﬁcation to previous work is the
I/O handling. Speciﬁcally, the conﬁguration ﬁles for each tool follow an NeXus application deﬁnition which deﬁnes what
each parameter conceptually represents, which datatype and unit it has. Each conﬁguration ﬁle is stored as a NeXus-
compliant HDF5 ﬁle [85]. These application deﬁnitions build on recent work in APM research data management [86] to
work towards making interoperable input and output data between diﬀerent software tools via a community-developed
set of open application deﬁnitions, glossary terms, and eventually ontology [87, 88]. Thanks to all modiﬁcations to the
toolbox, it has now become possible to package the toolbox in a Docker container which allows its eﬃcient integration
into FAIR data management platforms, like NOMAD [18]. Here, we can take advantage of paraprobe already because
of the open data model and open ﬁle format. We encourage the atom probe community to contribute to these eﬀorts by
exchanging ideas at meetings and conferences and making suggestions through the online documentation [86]. If fostered
by the community that could lead to the formulation of a common data exchange model to make atom probe microscopy
analyses more interoperable and reproducible.

2.2 High-throughput composition and object analyses

Open-source deconvolution, iso-surfaces, and microstructural object reconstruction Paraprobe-nanochem
implements multiple computational geometry algorithms which, when used in combination, enable users to program-
matically instruct delocalization tasks, iso-surface-based analyses, and subsequent computational geometry processing
to reconstruct microstructural objects and automated ROI-based composition, concentration, proxigram, and interfacial
excess analyses. This work focuses on three-dimensional objects, such as triangulated surface meshes of precipitates.
Unique is that the user can access all intermediate results and objects, including their geometry. Figure 1b summarizes
the individual algorithmic steps of the paraprobe-nanochem tool for quantifying microstructural objects. Key details of
these algorithms are summarized in the following paragraphs.

Delocalization This is a strategy for smearing ion positions into the continuum to enable subsequent approximating
of topologically simpler and smoother features via iso-surfaces [4]. Delocalization can be achieved with deﬁning ﬁrst a
discretization volume (3D voxel grid) and superimposing it on the point cloud. Second, the voxels are scanned with a
delocalization kernel which evaluates how strongly each ion contributes signal intensity to each voxel.

So far, paraprobe implemented [65] a naive approach whereby an ion was binned into the voxel that covered its position
using rectangular binning. More sophisticated methods were proposed [4, 40, 89] but these have so far been accessible
almost exclusively via commercial software [62] where exporting the scalar ﬁelds is, to the best of our knowledge, not
practically possible. Given that delocalization settings are often not reported in the literature in suﬃcient detail or
many users rely on default settings [21], it is often diﬃcult to judge the sensitivity of iso-surface-based results that were
reported in the literature. However, as Larson et al. pointed out [4], applying delocalization always calls for making a
compromise between how strongly one smears positions in an eﬀort to achieve blunter surfaces and smoother composition
gradients but to smear not too strongly to render very diﬀerent or even questionable numerical results. Several authors
reported in fact a strong parameter sensitivity of iso-surfaces [54–56], and suggested improvements but these remained,
with few exceptions [61], at the level of accepting the results from commercial tools oftentimes paired with very tedious
manual inspection. To close this gap was our incentive to implement an open-source alternative. Paraprobe-nanochem
implements a multi-threaded kernel density estimation which uses an anisotropic 3D Gaussian delocalization kernel with
variances σx = σy := σ ∈ R, σ > 0, and σz = 0.5σx. Ion types are decomposed into isotopes of elements and accounted
for via element-speciﬁc scalar ﬁelds. Users can instruct multiple delocalization analyses with diﬀerent settings to study
the sensitivity of iso-surfaces to delocalization (see case study 3.5 speciﬁcally). Further technical details are reported in
the supplementary material.

Iso-surface extraction After each delocalization run, paraprobe-nanochem activates another internal batch queue for
approximating triangulated iso-contour surfaces (ion-count-, composition-, or concentration-based). This enables studies
with a customizable set of iso-surface threshold values {ϕ}. Already computed delocalization results are reused rather
than recomputed. Multiple methods have been reported in the computational geometry community for approximating

6

a continuum description for surfaces. We decided to use an open-source implementation of a topologically more robust
marching cubes (MC) algorithm [90]. The reader is referred to the literature for a detailed overview of the functioning,
the history, and diﬀerences between MC implementations and alternatives [91–95].

The result is a triangle soup, i.e. triangles without connectivity information, representing a complex (set) of iso-
surfaces. Although MC has frequently been applied in the atom probe literature, mostly via its implementation in
commercial software, few atom probers have discussed that the implementation of the topological rule set can diﬀer
between MC implementations [64]. These diﬀerences can result in eventually signiﬁcant eﬀects on the local topology and
closure of the iso-surface, in particular when there is a strong sensitivity on the threshold value ϕ.

Microstructural object (feature) reconstruction Iso-surfaces serve the quantiﬁcation of a key methodology in
material science which is to coarse-grain speciﬁc atomic arrangements into objects, or features, for which descriptors, like
curvature tensor, line or surface energy, can be attributed at the continuum scale. Continuing with the triangle soup,
paraprobe-nanochem performs ﬁrst a proximity clustering of the triangles, using a modiﬁed DBScan algorithm [96] to
cluster nearby triangles. Once clustered, polygon mesh processing is used, including combinatorial steps, to identify which
of the triangle cluster represent individually closed surface meshes of polyhedra and which cluster represent open or free-
standing triangle patches. Shortest Euclidean triangle-to-triangle distances were computed via the GammaUNC/PQP
library [81]. Polygon mesh processing uses functionalities of CGAL [73, 78]. Triangles are clustered sequentially, polygon
mesh processing uses multithreading.

Object characterization After clustering the triangles yet another internal queue can be processed which analyzes
further each identiﬁed closed polyhedron: These analyses can be conﬁgured according to user needs oﬀering the volume
of the polyhedron, an inspection if triangles of each surface mesh intersect with the mesh of the edge of the dataset, a
computation of outer unit normal vectors for each triangle facet, and a shape analysis of the object via computing an
(approximate) oriented bounding box (OBB) [97] or rotated ellipsoid [98] for the object respectively. Furthermore, the
tool inspects which ions, i.e. which evaporation IDs, are located inside or on the surface mesh of each closed polyhedron
[73]. An exact algorithm for computing the optimal OBB has been reported [99, 100]. However, its cubic numerical
complexity in the number of points renders it impractical. Therefore, we opted to interface the tool with a faster
approximating algorithm [97, 101, 102] implemented in CGAL [73]. Point-in-polyhedron intersection tests are evaluated
also via CGAL. All object characterization uses multithreading.

Automated composition proﬁling As another optional step, paraprobe-nanochem implements algorithms for plac-
ing customizable ROIs at each triangle facet of an object or of a free-standing surface mesh and evaluating projected
signed distances of the ion relative to the signed interface facet normal (1D composition proﬁles, Fig. 4e, 6b). Using
the outer unit normal and barycenter of each triangle facet enables to place and align each ROI. The cylinder-triangle
intersection algorithm of [103] enables to identify if ROIs lie completely inside the dataset or not to detect bias or proﬁle
truncation. The result is a set of ROI mesh geometrical metadata and at least a collection of ion and/or element-speciﬁc
sorted lists of projected distances, eventually preprocessed already into cumulated composition proﬁles. If desired, such
analyses can be performed for every run to study sensitivity on delocalization, iso-surface, and object parameterization.
This automates a set of tasks which would otherwise very likely not be performed manually because of the extremely
time consuming number of thousands if not millions of ROI analyses necessary through GUI interaction. Paraprobe
documents all ROI metadata (location, orientation, dimensions) to oﬀer numerical exact reproduction, which is another
clear advantage over hitherto reported manual procedures. ROIs are processed via multithreading.

2.3 Spatial correlation analyses

Motivation These functionalities enable scientists to collect detailed ensembles of sets of mesh data which is useful
for parameter sensitivity studies, uncertainty quantiﬁcation, and planning more targeted or detailed manual analyses
commercial or community software. Despite its value for scripting analyses, we found that the amount of generated
data can be overwhelming because the results report, depending on how the analyses were conﬁgured, the sensitivity
on multiple sets of parameters from the geometrical, the surface approximation, and delocalization, and eventually even
ranging and reconstruction. Being able to investigate this wealth of information is the strength of high-throughput tools.
Therefore, we felt we should develop an automated post-processing tool to support scientists with quantifying the
sensitivity of an object’s representation as a function of parameters used - paraprobe-intersector is the result of this
- the second key novelty of our work. The tool enables generic spatial correlation and location analyses of ensembles
of sets of three-dimensional triangulated surface meshes. Tools for such a characterization have been developed in the
biological structure characterization and ﬂuorescence microscopy communities [104–108] but these work with discretized
objects, i.e. voxel grids. Known as so-called colocalization analysis tools in those communities, these tools have not
found a particular recognition or application in the materials science community though yet. Our implementation works
for objects in continuum space. Here, assuring numerical robustness is a more diﬃcult challenge than for discretized
objects.

Methods With the paraprobe-intersector tool of [66] now becoming replaced by paraprobe-crystalstructure [66] we
changed the scope of the paraprobe-intersector tool and reimplemented it. Now paraprobe-intersector takes a collection S

7

of sets Mk of three-dimensional triangulated surface meshes T l
k ∈ Mk of polyhedra l. Application on the set S = {Mk}
with k, l ∈ N quantiﬁes a number of spatial location, mesh composition, and collision analysis tasks by pair-wise
comparing objects T a
(with i, j values of k and i = j allowed and a, b values of l with a = b allowed). Objects can
be compared for diﬀerent k or l. Speciﬁcally, the tool provides algorithms for answering the following questions:

i , T b
j

1. With which objects does an object intersect? We term this collision analyses.

2. If two objects collide, what is the volume of the intersection (T a
i

these, so-called intersection analyses, are reported in the supplementary material.

(cid:84) T b
j

)? Details of the volume computation of

3. If an object does not collide/intersect (volumetrically) with another object, does it have other objects within a

threshold distance dprx (shortest Euclidean)? We term this proximity analyses.

4. We store individual collisions and proximity relations as directed graphs per set k. In these graphs, nodes represent
said objects (polyhedron l in set k). Edges represent collision- or proximity-type unidirectional relations between
node pairs. This enables to logically relate objects even though they are assigned eventually diﬀerent identiﬁers
throughout the analyses.

5. Using Voronoi-tessellation methods (see [65]), we construct composite objects from Voronoi cells which belong to
speciﬁc reconstructed ion positions inside speciﬁc objects. Resulting in composite meshes of sets of individual
Voronoi cells, where each cell has object ID and evaporation ID attribute data, selected nearest and higher-order
nearest neighbor cell adjacencies are evaluated. These topological analyses enable a segmentation of the cell set into
three-dimensional regions about objects and a representation of interfaces between objects as 3D meshed entities.
An example are ions within the interface between two or more coprecipitating second-phase precipitates. The
approach is equivalent to describing the three-dimensional structure of grain boundaries via Voronoi cells [13, 14].

6. Using attribute data for each object, such as polyhedron volume or element-speciﬁc composition values (from
paraprobe-nanochem), we post-process the resulting directed graphs along the direction k, while traversing forward
and backward, to characterize the evolution of shape and properties of objects as a function of their representation
for diﬀerent k. Essentially this graph traversal yields results that remind of Matryoshka dolls which enables the
desired quantiﬁcation how volume and composition of speciﬁc objects change as a function of iso-surface value ϕ for
instance. The traversal copes with the problem that iso-surface analyses are independent and thus objects will get
diﬀerent identiﬁers assigned throughout the high-throughput analyses. Paraprobe-intersector essentially identiﬁes
the relations between these identiﬁers. Evidently, the variable k denotes a general coordinate in parameter or phase
space, respectively, whose interpretation depends on the use case: Another possible one for which the tool could be
used is spatio-temporal tracking of triangulated surface meshes of crystals from e.g. crystal plasticity simulations
[67]. In such a case, k could be chosen as equivalent to the time step t, strain step (cid:15), or iteration counter. Then,
is equivalent to the re-identiﬁcation (tracking) of the crystal
a comparison of mesh T a
t
(or its fragments) between the previous (t − 1, backward tracking) or the next time step (t + 1, forward tracking)
respectively.

(at time t) with mesh T b

t±1

Further details are given in the supplementary material. These include also details about the implementation, the

computers that were used, and the scheduling of the jobs.

3 Results and discussion

3.1 Comparative study of methods for modeling the edge of a dataset

Although an atom probe reconstruction captures often a statistically signiﬁcant number of ions, it represents a point
cloud within a ﬁnite region. Consequently, datasets often contain incompletely measured but relevant microstructural
features. Tightly-ﬁtting triangulated surface meshes to the edge of the point cloud [109] can be a useful tool in this
regard for quantifying such edge eﬀects and studying the sensitivity of engineering-relevant descriptors (e.g. number of
precipitates or atoms per unit volume).

In the ﬁrst case study we compare how quantitatively diﬀerent such results and descriptors are for diﬀerent edge
meshing strategies. A mesh-based description of microstructural features oﬀers further opportunities. One is to create
distance-based or even tessellation-based volumetrical segmentations of datasets. Another one is to inspect eventual
collisions of meshes, or portions of meshes, and evaluate the proximity of meshes. With these functionalities the ﬁrst
case study also documents how spatial object analyses, like those reported in [41, 110], can be generalized for meshes.
Finally, we use the ﬁrst case study also as a veriﬁcation of the functionalities for the implemented paraprobe-distancer,
paraprobe-surfacer, paraprobe-tessellator, and paraprobe-intersector tools.

As an illustrative example we reanalyze a reconstructed dataset of an neutron-irradiated steel specimen. The dataset
was measured, reconstructed, and ranged originally by Jenkins and coworkers. Compositional information, irradiation
and experimental conditions [111], and details to the heat treatment of the material [112] were reported in the literature.
The specimen contains a dispersion of clusters, which are richer in copper, silicon, nickel, and manganese than the matrix.
The authors characterized these clusters with the core-linkage algorithm [49, 111]. Sharing the original reconstructed

8

ion positions and mass-to-charge-state-ratio values surplus which ions were assigned a cluster, enabled our work. With a
total of 7.57 × 106 detected ions the dataset volume is small but instructive because the smaller a dataset is (in volume)
the relatively more likely its ions lie close to the edge of the dataset. Especially in this case, a quantiﬁcation of edge
eﬀects is more important than it is for larger datasets. We should also mention that acquiring substantially more than a
few million ions is often problematic for materials which are prone to premature fracture due to stress on the specimen.
The reconstructed point cloud was transcoded and ranged. In this and subsequent case studies we perform atom-
type-speciﬁc quantiﬁcation by decomposing (molecular) ion types into atoms and computing respective multiplicity of
each atom. Next, four diﬀerent sets of edge models were computed: The ﬁrst set of meshes are approximations of speciﬁc
iso-contour surfaces or iso-surfaces for short. The here developed paraprobe-nanochem tool was used to delocalize the
position of each ion and compute the total number of atoms per voxel without distinguishing atom types. We refer to
such iso-surfaces as iso-total-atom-count ones. The adjustable parameter is ϕ the total number of atoms per voxel. The
delocalization returns real-valued quantities. The second set of meshes was computed via the paraprobe-surfacer tool
which constructs a set of α-shapes [46, 65, 113]. In principle these are generalizations of convex hulls with α as the
adjustable parameter. The third edge model is the special case of α
∞ which is equivalent to the convex hull [109].
The fourth edge model was created by computing ﬁrst the distance of each ion to an iso-surface from the ﬁrst edge model
(the one for ϕ = 0.1 atoms). Second this model was combined with ion-to-edge distance attribute data and a Voronoi
tessellation of the entire dataset. With these pieces of information an edge model was instantiated which is composed of
all facets between pairs of cells where one cell has a distance larger and the other one a distance that is smaller than an
ion-to-edge distance threshold d. This threshold is the adjustable parameter.

(cid:1)

Figures 2 summarize key results when comparing these edge models. Speciﬁcally, we inspect the surface meshes for
closure and compare their interior volumes. The volume of the dataset is a key descriptor for normalizing many other
descriptors of materials engineering relevance like the density of microstructural features. These engineering descriptors
will be explored in more detail in the ﬁfth case study. The results in Figs. 2 substantiate that parameter settings exist
for which all four edge models yield watertight surface meshes. The respective interior volume, though, diﬀers by 2 % to
10 % between diﬀerent models. Especially, the description of the volume based on α-shapes is sensitive. These results
add quantitative substantiation to previous studies [65, 77, 111, 114].

Figure 2: We compare diﬀerent methods for quantifying surface meshes of the edge to the dataset. Sub-ﬁgure a) shows an exemplar rendering of a
portion of the iso-surface-based edge model (ϕ = 0.1, see also comments to d). Sub-ﬁgure b) shows an exemplar rendering of the same region as in
a) for the tessellation-based edge model. Sub-ﬁgure c) compares the dataset volume as a function of the parameterization. Volume here refers to
either the interior volume of the closed polyhedron (in the case of convex hull, tessellation, and iso-surfaces eventually), or the accumulated volume
of interior tetrahedra (in the case of α-shapes). A numerical analysis of the triangle set to each iso-surface, tessellation, and the convex hull edge
models conﬁrmed these sets represent closed polyhedra. Using the same analysis on all α-shapes, though, identiﬁed that not all of these represent
closed polyhedra. This is distinguished in the respective curve for α-shapes by plotting thick (cid:54) symbols for objects which are closed and thin open
◦ symbols for those objects which are not closed. The volume of the convex hull serves as the reference (dark-bluish-dotted lower horizontal line).
Volume diﬀerences can be 10 % and larger in particular when using α-shapes. The right vertical (green) dotted curve is associated with the dotted
curve of accumulated volume of Voronoi cells (tessellation). The colour change from turquoise to green marks the distance beyond which cells are no
longer aﬀected by edge eﬀects. Sub-ﬁgure d) shows a rendition of the dataset with the ϕ = 0.1 atoms iso-surface in light grey and the set of convex
hulls around all clusters. The corresponding volume in sub-ﬁgure c) is marked with the light grey vertical line. The bluish halo about each cluster
represents the positions of all those ions which lie within 1.0 nm (as an example) to a surface mesh. The unit for the parameter α was plotted as
log10(α) only to remind the reader that α is a length. Plotting the logarithm of α enables a comparison between order of magnitude diﬀerences on
the same x-axis for all three parameters α, ϕ, d.

Our study supports that edge models based on the convex hull have advantages. These are lower numerical costs
than for the other edge models, no parameter sensitivity, and guaranteed watertightness. Returning in most cases a
lower number of triangles compared to α-shapes obtained with α (cid:28) ∞ is an additional advantage when computing

9

ion-to-edge distances. The key disadvantage of convex hulls is that the computed distances of ions in local concavities
are inaccurate. The results for α-shapes document why using previously reported practices of downsampling should be
used very carefully if at all. The results show that α-shapes which are computed for diﬀerently sampled input but the
same α-value have not only diﬀerent shape but also a diﬀerent interior volume evolution curve with changing α.

Thanks to using the Computational Geometry Algorithms Library (CGAL) (see methods section), the numerical
eﬃciency of using paraprobe-surfacer for analyses as compared to those performed by Jenkins et al.
[111] is well one
order of magnitude higher. The authors reported it took sequentially more than two hours to compute an α-shape of the
entire dataset (using a package of the R programming language [115, 116]). Paraprobe-surfacer by contrast computed
an α-shape for the entire dataset in 4 min 45 s (sequential execution, setting α −→ ∞). This substantiates that instead of
using downsampling practices, i.e. take only each n-th ion, it is better to use tools which are numerically more eﬃcient.
We conﬁrm that downsampling reduces indeed trivially the numerical costs. Tested here with repeating the authors’
downsampling experiments conﬁrmed the approximately the same order of magnitude faster processing so that minutes
reduced to seconds. In eﬀect, this enables α-shape-based analyses for a larger number of cases as it was so far considered
in the APM literature. We expect that the practical beneﬁt of our solution becomes even more important when working
with larger datasets because algorithms for computing α-shapes scale worse than linearly.

Frequently it is useful to segment a dataset into regions of diﬀerent distance to microstructural features. Figures 2
verify the tools’ capabilities to compute this segmentation. As an example, we study the same clusters which the authors
identiﬁed via convex hull surface meshes. For this purpose a utility tool (paraprobe-clusterer) was implemented which
imports arbitrarily made deﬁnitions of clusters from third-party tools of the APM community into paraprobe. Here,
we exemplify for clusters computed with IVAS/AP Suite, which is the main commercial software. The ﬁgure visualizes
all ions in the dataset within 1.0 nm distance to these surface meshes (on either side). The method can be extended to
segment also regions in the dataset with speciﬁc distance to spline-based line or tubular features.

Another purpose of the ﬁrst case study is to verify the capabilities of the paraprobe-intersector tool to correctly iden-
tify collisions between and proximity of individual or entire sets of triangles of surface meshes and/or surface patches.
Furthermore, we test the tool functionality that computes Voronoi-tessellation-based volumetric segmentation. Specif-
ically, each convex hull was tested for collisions on neighboring convex hulls in the set. Although this is an empirical
strategy for verifying an implementation, it is expected that detecting eventual numerical diﬀerences could help to iden-
tify if severe implementation errors exist. We expect that each convex hull should collide only with its own copy and on
neighbors eventually.

Figure 2d conﬁrms all of these expectations. All meshes get detected as overlapping with themselves. Interestingly,
also cases of additional collisions on neighboring convex hulls were detected. One random example is shown in Fig. 3
for a collision pair. Using paraprobe-clusterer it was possible to conﬁrm that no ion was coincidentally a member of
two clusters within the authors’ original cluster analysis results. However, the point set which is closer, in an Euclidean
closest distance sense, to all ions of a given cluster (the composite volume of all Voronoi cells to the points in the cluster)
is diﬀerent to the point set that is covered by the respective convex hull of that point set.

Using the meshes that paraprobe-tessellator generated and connecting these with the collision analysis results of
paraprobe-intersector enabled to conﬁrm the situation using Paraview. The tools deliver all data which enable scientists
to inspect all collision situations. As an example we deliver the respective 3D models as supplementary material. This
material documents that also all other cases of collisions on more objects than ones own mesh where cases like the one
which is exemplarily shown in Fig. 3. This veriﬁcation suggests that the paraprobe-intersector can solve also a number
of other collision analyses like those reported for iso-surface-based analyses in [21].

3.2 Automated composition proﬁling for closed objects

The second case study aims to verify the paraprobe-nanochem tool. Furthermore, we use it to present an automated
method for composition proﬁling to support scientists with collecting reliable statistics of composition gradients across
interfaces in an eﬃcient manner. As an example, we process a reconstructed dataset of Cerjak et al. [117] that was taken
from a S690 steel specimen. The dataset contains 23.8 × 106 ions in total. We work with the original reconstructed
ion positions and ranging deﬁnitions of the authors [117] for which data were made public and experimental procedures
explained in previous work [21]. The dataset represents a material volume which contains a partially measured carbide
located at a grain boundary segment which is decorated with phosphorus as solute atoms. We focus ﬁrst on characterizing
a single object, here the carbide, to familiarize the reader with object-based analyses before continuing with results of such
studies for ensembles of objects and ﬁnally for ensembles of such ensembles and their parameter sensitivity. Speciﬁcally,
we computed carbon iso-composition surfaces (ϕC = 1.0 at.% to 30 at.% with 1.0 at.% step) for a delocalization grid with
cubic cells of l = 1.0 nm edge length. The variance of the kernel is σ = 1.0 nm.

Figures 4 document that paraprobe-nanochem yields detailed quantitative and spatially-resolved composition results.
Speciﬁcally, Fig. 4d veriﬁes that objects can be meshed and ions inside these meshes detected correctly. The triangle
facet normals have the correct orientation which is, according to our deﬁnition, positive when pointing out of a closed
object. The 3D models in the supplementary material document also a correct detection of eventual inclusion of, or
intersections between, individual ROIs and an edge model of the dataset Fig. 4c to pinpoint for which ROIs edge eﬀects
have to be considered. Figures like 4c and 4d were creatable because paraprobe stores all numerical data through HDF5.
This enables eﬃcient and programmatic ﬁgure creation, for instance to either explore composition gradients locally Fig.
4e or to script models which consume and post-process the characterized data in the HDF5 ﬁle according to user needs.

10

Figure 3: With combining results from surface meshes, clustering, and tessellations, it is possible to three-dimensionally segment ROIs which represent
portions of interfaces or junctions between interfaces. Sub-ﬁgure a) shows a rendition of the convex hull meshes for two exemplar clusters, object 130,
in blue and object 133, in green, respectively. These objects intersect three-dimensionally. Convex hulls of neighboring clusters are shown in light grey
in the background. The ﬁgure is a visual conﬁrmation of the quantitative analysis and graph-based description which paraprobe-intersector yields.
Sub-ﬁgure b) shows the segmented 3D models of Voronoi cell composites. The orange composite are all Voronoi cells within nearest or second-nearest
neighbour connections to a cell of either cluster. The composite can be used as a volumetric description of the region where the two objects intersect
and help to uniquely assign each ion to either an object or the interface. The blue and green composites show the Voronoi cells to all ions supporting
either cluster 130 (blue) or 133 (green), respectively.

In this example the results conﬁrm there is a decreasing carbon gradient towards the matrix and local diﬀerences of the
carbon concentration proﬁle across the surface mesh.

It is important to mention that the only input from commercial software is the reconstructed dataset with the
ranging deﬁnitions. The novelty is in the combination of giving unrestricted access to algorithms for delocalization,
iso-surface extraction, subsequent polygon mesh processing, and automated ROI analyses. This oﬀers an alternative
for many analyses that would manually be very time consuming to perform with GUIs. Examples are given especially
in the following case studies. Such high-throughput analyses were in the past often neither feasible nor reported by
experimentalists across APM publications, but with now having access to also the 3D atom intensity ﬁelds as data arrays
it is possible to compare results to those from other tools.

Figures 5 summarizes the tools’ capabilities for quantifying the spatial arrangement between diﬀerent objects. The
result suggests an example how the sensitivity of an object’s representation can be described as a function of the
parameterization for the respective geometrical model. Speciﬁcally, Fig. 5a exemplarily shows how diﬀerent choices for
the iso-composition threshold ϕC result in diﬀerent object representations. Speciﬁcally, a composite of 3D meshes of the
carbide is shown for diﬀerent carbon iso-composition ϕC values. Such a qualitative characterization is an established
prerequisite step during exploratory analyses of APM data [21] especially when using commercial software. Our strategy
and built tool, though, enables quantitative studies via logically combined analyses as a function of diﬀerent threshold
values, plus the rigorous documentation of these. This strategy serves not only advanced sensitivity and uncertainty
quantiﬁcation but also repeatability and reproducibility because of the documentation of the steps. The carbide example
documents a sensitivity of the shape and composition to ϕC. These ﬁndings conﬁrm that a single threshold value is in
most cases inadequate to distinguish objects via iso-contour approximation techniques [54–57]. Our work solves how these
inherent limitations of iso-surfaces can at least be studied quantitatively in an exactly reproducible and fully automated
manner.

The suggested delocalization method pinpoints a shortcoming of edge eﬀect handling for microstructural objects in
commercial software that is commonly faced, though, rarely discussed for atom probe data: We expect that delocalizing
and discretizing a ﬁnite point cloud results in edge eﬀects because the delocalization kernel extends beyond the edge of
the point cloud. This aﬀects the representation of surfaces. A more detailed discussion is oﬀered in the supplementary
material.

3.3 Automated meshing of interfaces aided by compositional gradients

Automated composition proﬁling as in the second case study requires orientable meshes to obtain consistently signed
surface normals for each facet. For closed objects such a set of normal vectors can be computed as it was shown in the
previous case study. In the case of polyhedra usually weighting schemes [118] are in use because otherwise discontinuous
changes of surface normals are encountered at vertices and edges. A related atom probe study [62] used these so-called
pseudonormal vectors.

11

Figure 4: Sub-ﬁgure a) documents that paraprobe-nanochem can be used to analyze the entire dataset or portions of it. Automated inspections of
objects via local ROIs to the surface mesh are possible. Sub-ﬁgures a) and b) give an overview of the entire dataset a) and a close-up b) of the carbide
at a curved interface segment. Sub-ﬁgure c) and d) show a close-up of the carbide to document the computational geometry and composition analysis
Iso-surface-based analyses deliver triangulated surface meshes such as the blue wireframe mesh of the carbide in c). This sub-ﬁgure
capabilities.
supports that also incompletely measured microstructural features in contact with the edge of the dataset can be analyzed. For each (closed) object
the tool computes which ions are lying inside or on the surface of the object (green point cloud inside the bluish wireframe in c). Sub-ﬁgure d) shows
the triangles of the surface mesh with individual outer unit normal vectors. These are used to automatically place and orient ROIs with the local
facets (here exempliﬁed for cylinders). To convey this message clearer, we rendered only a random sub-set of ROIs and their guiding normals. 3D
models are available as supplementary material. Paraprobe-nanochem evaluates if ROIs intersect with the edge of the dataset. Thereby, automated
analyses of composition proﬁles, like those shown in sub-ﬁgure e), can be controlled to detect eventually incomplete proﬁles. Here, all 1D cumulated
carbon composition proﬁles are shown for all those ROIs which are fully embedded in the dataset. Sub-ﬁgures like the last one can be created with
scripting using paraprobe-autoreporter, thus oﬀering tailored analyses.

Many iso-surfaces, though, contain portions which are not closed but represent free-standing, so-called surface patches.
In this case, the (global) context of the model and often further assumptions are needed to decide how the normals of
the mesh primitives (here triangles) can be oriented consistently.

One important application is quantiﬁcation of composition variation in the vicinity of grain or phase boundaries. Here,
it is important that the normals point ideally in the same direction of an existent composition gradient instead of being
ﬂipped in an uncontrolled manner. The procedure which is used to compute normals in such situations should be well
documented as to guide users where eventual inconsistencies exist. Given the variety of possible triangle conﬁgurations
there can be inconsistencies when computing so-called proximity diagrams and studying their respective composition
proﬁles [39, 62].

In this (third) case study, we test an automated protocol for orienting triangle normals by taking into account gradient
information of the underlying composition ﬁeld and formulate a quantitative quality descriptor which documents for which
triangles this procedure is locally eventually not reliable. Exemplarily, the case study here reports local composition
proﬁles across an interface protrusion and positions of eventual junctions between microstructural features (dislocations
or patches of phase boundaries) at this protrusion.

Speciﬁcally, the analysis is for a reconstruction of a 100Cr steel specimen from Mayweg et al. [119, 120]. The author
shared the original dataset with 30.7 × 106 ion positions and ranging deﬁnitions. A convex hull edge model was computed
for the entire dataset. Subsequently, diﬀerent carbon iso-composition surfaces (l = σ = 1.0 nm and ϕC = 5.0 at.% to
15 at.% with 1.0 at.% step) were processed. Figure 6a displays the ϕC = 12 at.% iso-composition surface. To orient
the normals of the triangles we ﬁrst compute the gradient of the composition ﬁeld ∇ (cid:126)cC. Second-order accurate central
diﬀerences are computed for interior voxel and ﬁrst order accurate one-sided (forward or backward) diﬀerences at the
boundaries. Next, the voxel with the closest barycenter to the triangle barycenter is taken to guide the direction of the
normal. Speciﬁcally, the magnitude of the gradient (cid:107)∇ (cid:126)cC(cid:107) at this voxel is taken as the quality descriptor to ﬁlter if the

12

Figure 5: Paraprobe-intersector is a tool for studying the parameter sensitivity of individual or ensembles of objects. Sub-ﬁgure a) shows a composite
of 3D meshes of the carbide for diﬀerent iso-surface settings. Reminding of Matryoshka dolls, the meshes for the carbide have diﬀerent size and shapes.
Paraprobe-intersector evaluates which meshes are logically connected via spatial collisions on other or proximity relations to meshes of neighboring
objects. Sub-ﬁgure b) documents how this functionality yields automated graph network descriptions and visualization of the logical relations. These
quantify the successive fragmentation of the iso-surface with increasing ϕC . Sub-ﬁgure c) documents that connecting results from diﬀerent tools, here
individual object properties for each iso-surface value from paraprobe-nanochem (carbon ϕC iso-composition), with graph analytics from paraprobe-
intersector, yields rigorous quantitative characterization how sensitive descriptors like volume or composition of an object are to parameterization.
With scripting such diagrams could be created for each object of a high-throughput analysis in an automated manner.

voxel can guide the eventual ﬂipping of the triangle normal vector. As an example, we ignore triangles with associated
voxels with lower than 0.01 at.% nm−1. For all other triangles of the patch, the respective normals (cid:126)n are evaluated against
the gradient direction and eventually ﬂipped to assure (cid:126)n · ∇ (cid:126)cC ≥ 0.

Note that the magnitude of the gradient and the cosine of the angle between the gradient vector and the triangle
normal can serve as quality descriptors to identify local conﬁgurations where the gradient vector is nearly perpendicular
to the proposed triangle normal. In such a case, the cosine evaluation yields values close to zero regardless from which
side one approaches so that the projection is prone to sign ﬂuctuations of the normal and therefore of lower quality than
compared to a case of a near parallel alignment.

Figures 6 summarize that this approach makes the triangle normals point in a direction which is consistent with
the composition gradient that is here pointing into the ferrite with its lower carbon concentration. With these normals
computed, automated ROIs are placed (similarly like in the second case study). An extension of this work and combination
with results from paraprobe-distancer could use these normals and quality descriptors to ﬁlter out those regions in the

13

Figure 6: The automated placing functionalities for ROIs and composition analyses of paraprobe-nanochem can also be used for quantifying surface
patches which are not part of closed objects. In this case the local triangle normal vector is oriented via an inspection of the gradient of the composition
ﬁeld ∇(cid:126)c. Sub-ﬁgure a) shows an exemplar free-standing surface patch with normal vectors which are aligned with the direction of decreasing carbon
concentration cC . Two exemplar sets of cylindrical ROIs are displayed in transparent colors to show how they overlap and align with the local normals.
Sub-ﬁgure b) shows a collection of all 1D cumulated carbon composition proﬁles for all ROIs which do not intersect with the edge of the dataset. The
results give access to substantial details of the composition gradients from the carbon-rich precipitate towards the ferrite with a lower carbon content.
All proﬁles are stored in the HDF5 result ﬁle to enable further programmatic analyses with Python scripting for example.

dataset with a speciﬁc proximity to the surface patch and exclude for instance those regions in the dataset where the
above-mentioned approach of orienting the triangle normals delivered results with low quality.

3.4 Automated meshing of interfaces aided by chemical decoration

There are many datasets where iso-surface-based analyses of grain and phase boundaries, like those discussed so far, are
unsuccessful. Facing imprecision of current reconstruction models, insigniﬁcant chemical gradients across the interface
or lacking correlative results from electron microscopy methods means insuﬃcient latent crystallographic information to
identify interfaces in atom probe datasets. In some cases features within ﬁeld desorption images can be used to inform
the reconstruction of interfaces inside the reconstructed volume [121].

There are cases, though, where this is a tricky, if not a conceptually questionable approach: A grain or phase
boundary is deﬁned as the three-dimensional region between two adjoining crystals where the long-range crystallographic
symmetry breaks. This region is often spatially correlated but not necessarily located exactly where a local segregation
of a decorating solute is highest.

With the increased usage of site-speciﬁcally prepared atom probe specimens to probe spatial details of solute segre-
[122, 123]), there has been an interest to use this chemical decoration for supporting the
gation at interfaces (see e.g.
construction of triangulated surface meshes to stand in as models of interfaces. Felfer et al. implemented computational
geometry methods for this task [42, 43, 46]. Their so-called DCOM algorithm has inﬂuenced several authors [47, 64,
124]. Implemented in practice, these tools are semi-automated and may or not need manual mesh processing which is
most conveniently performed with GUI-based tools like Blender. A noteworthy subtlety of DCOM is the stability of
the algorithm when it gets applied iteratively. These and how users assure the creation of robust meshes when working
manually has not been covered in substantial detail in the atom probe literature.

DCOM is an iterative algorithm which relocates the triangles of an initial interface model by moving vertices towards
the barycenter of the local cloud of solute atoms about the respective triangle vertices. However, if left unconstrained,
these mesh operations can result in mesh self-intersections. To support this discussion we speciﬁcally revisited the
authors’ implementation [74] and investigated whether a complete automation of the algorithm and adding of established
intermediate steps of polygon mesh processing and mesh quality inspection yields a more frequently applicable automation
of DCOM. We were also interested whether this improves robustness or not.

As an example, we applied our modiﬁed implementation ﬁrst on an interface in a molybdenum-hafnium alloy from
Leitner et al. [125]. The authors shared the original dataset with 4.58 × 106 ion positions surplus the original ranging
deﬁnitions. The dataset details a reconstruction with a single curved interface patch with boron and carbon segregation.
Figures 7 summarize the results of letting paraprobe-nanochem construct a quality triangulated surface mesh based
on the joint point cloud of boron and carbon atoms in the dataset. The initial planar interface model was computed with
principal component analysis (PCA). Subsequently this model was iteratively reﬁned via DCOM. During each iteration
the mesh from the previous iteration was reﬁned and mesh-smoothened to equilibrate interior angles of the triangles,
followed by a DCOM step, with a subsequent check for mesh self intersections. Speciﬁcally, the mesh was reﬁned from
an initial triangle edge length and DCOM radius of 10 nm in steps of 2 nm to a target edge length of 2 nm.

14

Using sequential computing the interface modeling took 0.3 s wall clock time including I/O. For this dataset it was
successful to take naively the entire boron and carbon ion point cloud. The algorithm locates the interface initially and
successive reﬁned the model fully automated.

With a triangulated surface mesh generated, we equipped paraprobe-nanochem with support for the automated ROI
analyses of the previous case studies. For the exemplar dataset here, we performed completely automated and multi-
threaded analyses using four cores on a laptop. Details are available in the supplementary material. ROIs with a height
of 20 nm and a radius of 5 nm were taken. From the total of 4934 triangles/ROIs, 1354 were detected as fully-contained
ROIs. Composition analyses were performed for all of them. This analysis took 18.2 s of which 5 % was spent in I/O
operations.

Figure 7: Paraprobe-nanochem implements a set of robust and automated methods for creating triangulated surface mesh models of individual
interfaces. This is useful in cases when a decoration of solute atoms is the exclusive experimental hint of the interface location. The tool combines
this with a robust implementation of the DCOM method which detects if triangles self intersect during the iterative reﬁnement of the interface model.
Once identiﬁed, triangulated surface meshes are processed via fully-automated methods where ROIs are placed and aligned at each mesh triangle, if
desired, to characterize composition, concentration, proximity diagrams, and interfacial excess mappings a), d), e). The light-gray contour about the
dataset is a mesh of the convex hull. Intersections between ROIs and the convex hull, here representing the edge of the dataset, are detected in this
process to control analysis bias by ignoring those ROIs that are only partially contained in the dataset. Sub-ﬁgure b) shows a composite of all those
ROIs which lie inside the convex hull. For all these ROIs, of which one is shown exemplarily in c), the tool computes the projected distances of each
ion and its respective atoms. The results are stored in the HDF5 results ﬁle. This enables post-processing of diﬀerent types of composition analyses via
the paraprobe-autoreporter Python tool. Exempliﬁed for ROI 1005 these analyses yield e.g. composition d) or interfacial excess e) proﬁles. Sub-ﬁgure
a) summarizes the results of such interfacial excess computations for all interior ROIs, displayed as an interfacial excess mapping. Triangles outside
the dataset or without an associate ROI inside the dataset are drawn as a wireframe mesh.

With additional less than 10 s of sequential Python processing the results yield interfacial excess maps Fig. 7a. This
map and exemplar composition proﬁles Fig. 7d and Fig. 7e conﬁrm that boron and carbon have segregated to a joint
atomic fraction within the interface of approximately 1.5 at.-%.

With an average width of more than 5 nm the proﬁles document that the chemical decoration, as it displays in the
reconstructed volume, is wider than typically observed for structural units in grain boundaries when these are charac-
terized with high-resolution transmission electron microscopy. Such experiments reported 0.5 nm to 2 nm or explored
with atom-resolving simulations [121, 122, 125, 126]. Calculation of segregation proﬁles at grain boundaries within the
coincident site lattice (CSL) approach display widths which are consistently smaller than 0.5 nm [127–129]. Literature
results from early APFIM measurements [130, 131] which relied on a diﬀerent methodological approach than compared
to modern APT measurements also report such narrow segregation proﬁles for W-Re solid solutions. The here proposed
protocol for automated characterization of gradients at interfaces can support research on disentangling the individual
contributions of inaccuracies due to reconstruction models used or the eventual existence of defect phases [132], and/or
precipitates at the interface. Whether such a disentangling is possible with experiments alone or requires the support of
computer simulations is a topic of current research [57]. This warrants a detailed analysis in its own right that is beyond
the scope of this multi-method-reporting paper.

Motivated by this success, we applied the tool to ﬁve other datasets. Each was measured for a similar experimental
design: characterizing a single eventually curved interface, in a site-speciﬁcally specimen to yield a reconstructed volume
with an interface that is decorated with solutes. Studies for these other datasets are parts of ongoing work; and therefore
the following ﬁndings are so far preliminary. We learned that it is the speciﬁc location of the interface plane, its
inclination, the spatial arrangement (relative to the interface patch) and the number of solute atoms on either side of

15

the interface which deﬁnes if creating an interface model without manual interaction is successful.

For one of the ﬁve datasets this was successful using also naively the entire dataset.

In the other cases working
with all ions of the decorating species was unsuccessful. Especially for strong noise the PCA just splitted the dataset
approximately in half uncorrelated to the chemical decoration. In three other cases, using the naive approach delivered
a mesh of only a partial patch of the interface. However, with simply constraining the input to operate only on those
solute ions inside a thin rotated bounding box about the interface, it was possible to mesh all cases across the entire
interface.

Only in one of the ﬁve cases the spatial distribution of the decorating ions was so heterogeneous that local curvature
built up during DCOM operations so that the algorithm warned and stopped when it detected mesh self-intersections. We
learned that here is potential for a further improvement of the method which is to use automated remeshing procedures
which are commonly used in the ﬁeld of ﬁnite-element-based interface evolution solvers. Alternatively one could interpret
the DCOM-relocated vertex positions as a predictor step and apply a subsequent shape smoothening operation which
eﬀectively constraints excessive vertex migration. Algorithms for such tasks have been proposed in the computational
geometry community and are used in the continuum mechanics and the annealing microstructure evolution modeling
communities.

In summary, we consider the work on this use case and DCOM a success because if such a simple spatial ﬁltering
suﬃces it is likely that also other datasets can proﬁt from our faster, more automated, and more comprehensively
documenting tool. Thanks to open-source software, it is possible to containerize the tool, and make it thus accessible
for cloud-based computing in services like NOMAD [25]. This substantiates that much collectively performed research
with sharing atom probe datasets is left to become cooperatively harvested. This would also enable even more careful
analyses of the subject to pinpoint in which cases there is really no alternative to manual mesh processing.

3.5 Automated high-throughput analyses of object ensembles

In the ﬁfth case study we apply the above-mentioned methods in automated high-throughput analyses for character-
izing microstructural objects, precipitates, via computing and segmenting triangulated iso-composition surfaces. The
tomographic reconstruction was taken from a measured atom probe specimen of a nickel base super alloy. This material
contains two diﬀerent types of precipitates. One type are γ(cid:48)(cid:48) (N i3N b) which are niobium-rich precipitates. The other
type are γ(cid:48) precipitates (N i3X) which contain aluminium or titanium atoms representing X. Many of these precipi-
tates show coprecipitation, i.e. (spatial) conﬁgurations where one or multiple precipitates lie in close proximity or make
contact to other precipitates. To rationalize their eﬀect on material properties, it is not only relevant to quantify each
precipitate individually with respect to its volume, shape, and atom composition, but also to understand the detailed
spatial arrangement and relative frequency of diﬀerent coprecipitation conﬁgurations.

The dataset was measured, reconstructed, and ranged by Rielli et al.

[133]. The reconstruction contains a total of
103.2 × 106 ions. Experiments were performed on a CAMECA/AMETEK local electrode atom probe (LEAP) 3000 Si
instrument with a detector eﬃciency of 57 % in pulsed voltage mode. A target evaporation of 1 %, a pulse fraction of
20 %, and a pulse rate of 200 kHz were used. The specimen was measured while maintaining a temperature of 50 K. The
tomographic reconstruction was performed with commercial software IVAS (v3.8.4) and crystallographically calibrated
[134, 135].

In the preprocessing steps we imported the dataset representing the reconstruction and ranging deﬁnitions from IVAS
and applied these (paraprobe-transcoder, paraprobe-ranger). Next, we computed an iso-surface of the total atom count
per voxel (paraprobe-nanochem) and evaluated the shortest Euclidean distance of each ion to the iso-surface at ϕ = 0.1
atoms threshold (paraprobe-distancer). Next, two sets of high-throughput studies were executed: One for characterizing
γ(cid:48)(cid:48) and another one for characterizing γ(cid:48) precipitates. Previous high-resolution transmission electron microscopy studies
[136] on a similar material and processing conditions supports our assumption to restrict the analyses in this work on γ(cid:48)(cid:48)
and γ(cid:48) as the most important precipitate types. We assume that closed objects segmented from niobium iso-composition
surfaces (ϕN b) represent γ(cid:48)(cid:48) precipitates while closed objects segmented from aluminium plus titanium iso-composition
surfaces (ϕAl+T i) are γ(cid:48) precipitates. Having made the same assumption in previous work [59, 60, 133], we continue to
use the brown (γ(cid:48)(cid:48)) and turquoise (γ(cid:48)) coloring scheme of [59] to distinguish these precipitates.

Each of the two high-throughput studies was instructed as a set of paraprobe-nanochem runs with diﬀerent pa-
rameterization. Both probe the sensitivity of the object representations as a function of delocalization settings (grid
resolution l, kernel variance σ, and threshold values ϕN b and ϕAl+T i) respectively. Speciﬁcally, 3D discretized grids
within l = 0.50 nm to 2.0 nm with 0.25 nm step were probed. For each edge length l, anisotropic Gaussian smoothing
kernels with σ = σx = σy = 2σz were used and σ was varied between 0.25 nm to 1.0 nm (with 0.25 nm step). A 73
voxel delocalization kernel was centered at the closest voxel covering each ion. The result of each parameter combination
yields a set of discretized elemental composition ﬁelds (for niobium, and aluminium plus titanium, respectively, reporting
atomic fraction at.%). Iso-composition surfaces were computed on the interval 1.0 at.% to 30.0 at.% with 0.1 at.% step.
In total, each high-throughput study thus probed 28 diﬀerent combinations of delocalization settings with a total of 291
iso-surfaces analyzed per delocalization each.

Compared to classical analyses it would be necessary that an experimentalist performs 8148 runs via manual, GUI-
based analyses and characterizes each object for these runs. This is a highly impractical task already for a single run as
most runs have a complex set of triangulated iso-surfaces [60, 133].

By contrast, paraprobe-nanochem performs automated analyses with post-processing for each iso-surface including

16

all its microstructural objects to extract quantities of interest for materials engineers (number density, volume of each
object, object shape via ﬁtting approximate oriented bounding boxes, and computing which ions lie inside each object
for quantiﬁcation of ion-species- and element-speciﬁc/atom-type-speciﬁc compositions). We assume closed objects can
qualify as so-called interior objects if no point on their surface mesh lies closer to an edge model of the dataset than a
threshold distance dprx ≤ 2.0 nm.

Figure 8: High-throughput quantiﬁcation with paraprobe-nanochem reveals the parameter sensitivity of iso-surface-based descriptors, here exempliﬁed
for the number of interior objects a) and the total volume, respectively b) of γ(cid:48)(cid:48) and γ(cid:48) precipitates as a function of σ and ϕ at ﬁxed l = 1.0 nm.
The volume of the convex hull about the entire dataset is 2.056 × 106 nm3. The upper dotted lines mark those runs, which are discussed exemplarily
in more detail. For γ(cid:48)(cid:48) and γ(cid:48) the respective ϕ values are 7.6 at.% and 6.1 at.%. The results show there are regions in parameter space for which
the descriptors are least sensitive to ϕ. Paraprobe-nanochem can compute detailed statistics with each run. Sub-ﬁgure c) exempliﬁes this with the
distribution of volume of interior objects. Sub-ﬁgure d) exempliﬁes statistics of average object composition, here as an example for niobium for γ(cid:48)(cid:48)
and aluminium plus titanium for γ(cid:48), respectively, as a function of the object volume.

One key result of the study is that all descriptors show a sensitivity on the iso-surface parameterization. The results
also quantitatively document a substantial eﬀect which the delocalization settings have on the resulting descriptors.
These results clearly support previously made comments [4] that such settings should not only be always reported but
also their eﬀect ideally be quantiﬁed in a reproducible and routine manner given now the availability of automated tools.
For example, Figs. 8 display the sensitivity of the number of precipitates Fig. 8a and total volume of such precipitates
Fig. 8b as a function of iso-composition ϕ, i.e. ϕN b and ϕAl+T i respectively, for increasingly higher variance of the
delocalization kernel σ. The curve is practically smooth with two maxima. Diﬀerent qualitative surfaces are obtained:

17

Below ϕ ≤ 3 at.% the iso-surfaces represent a single complex which encloses the majority of the dataset. With increasing
ϕ the complex begins to fragment into pieces with a complicated dependency on the threshold value. Evidently, these
low threshold values result in a percolating network of complexes, which indicates a threshold that most experimentalists
would qualitatively consider as a too small one. With increasing threshold the complexes fragment further, which is
more representative of a dataset that hosts a set of individually separated precipitates. The corresponding ϕ range can
be interpreted as a region of the parameter space (descriptor ϕ) with a lower sensitivity to ϕ. The resulting objects are
in addition sensitive to the delocalization settings. Beyond ϕ ≥ 8 at.% the number density and corresponding volume
of γ(cid:48)(cid:48) or γ(cid:48) precipitates reduces until eventually no interior objects remain. This is an eﬀect of the successively stronger
erosion of the precipitates with increasing ϕ. Traditionally one would ask at this stage what is the “right” threshold to
choose. Our methods enable to quantify the sensitivity to either support speciﬁc user choices or supplement these with
the required sensitivity.

Thanks to automation, it was possible to compute further statistics for each run. As these are part of the sup-
it suﬃces to zoom into key results. Figures 8c and 8d exemplify for the speciﬁc runs with
plementary material,
l = 1.0 nm, σ = 1.0 nm, ϕN b = 7.6 at.% for γ(cid:48)(cid:48), and ϕAl+T i = 6.1 at.% for γ(cid:48) respectively. We ﬁnd that the com-
positional variance of the objects is signiﬁcantly lower the larger it is the object volume (Fig. 8d). This sensitivity
originates from at least two eﬀects: Finite ion counting statistics and a relatively stronger eﬀect of the delocalization for
small objects. The results support that analyses of nanoscale precipitates requires substantial care if not sometimes a
clearly set limit as to what is reliably characterizable with iso-surfaces and APM [6, 137] in general. Especially when no
correlative electron microscopy data are available. Observing a quantization of compositions for objects with less than
a few voxels (here 1 nm3) is evidence of ﬁnite counting eﬀects.

To further support our argumentation that a high-throughput approach is not only applicable to other materials
but also oﬀers signiﬁcant value for scientists, we add another short case study. Speciﬁcally, an example of research
on characterizing the composition of metastable phases in a metastable titanium alloy. The challenge here is that
thermodynamics enable eventually for a range of object compositions, which can oftentimes be very similar, especially
when defect phases [132] are involved and ﬁnite counting eﬀects are relevant [137]. Speciﬁcally, the user story summarizes
practical research experiences from a recent analysis by Zheng and coworkers [138, 139]. They studied diﬀerent transient
phases forming during heating to aging temperature in a β Ti-5Al-5Mo-5V-3Cr alloy. The reconstructed dataset contained
a total of 45 × 106 ions. The authors performed APT experiments, analyzed these via GUI-based processing in commercial
software and studied individual precipitates via high-resolution electron microscopy.

Initially, only one phase (Ti-rich) was identiﬁed in the APT dataset when exploring titanium iso-composition surfaces.
This was in contrast to the electron microscopy work, which clearly showed a high density of two types of precipitates
(with diﬀerent crystal structures) within the matrix. After two weeks manual exploring the APT data, it was possible
to identify two types of precipitates. These showed very similar compositions (both Ti rich but with slight variation
in the Al content) and overlap. Subsequently, it took approximately 5 h of GUI work to sort through the precipitates
based on their composition and relative shape (spherical vs. slightly more elliptical) and assign them diﬀerent colors
for visualizing each precipitate type. This manual approach introduces signiﬁcant bias into the analysis as the user has
to ultimately make a semi-quantitative judgement which precipitate in the reconstruction corresponds to which phase.
Finally, the three most representative precipitates were used to estimate the average composition of each phase. However,
unintentional user bias can skew the results as the most representative precipitates which are taken for the composition
analysis are in reality usually the most extreme cases in terms of composition.

As an example how this user story could be supported, we performed two exemplar high-throughput runs for titanium
(ϕT i = 50 at.% to 100 at.%) and aluminium (ϕAl = 1 at.% to 15 at.%) iso-composition surfaces (0.1 at.% step for both
runs) for the same dataset. For each run the titanium, aluminium, vanadium, and molybdenum composition was
computed for each closed interior object (using a convex hull model for the edge of the dataset). The high-throughput
runs yield sets of tables of atom-type-resolved compositions in an HDF5 ﬁle. Using a laptop and four threads the results
were available after 1 h:39 min wall-clock time without further manual interaction. With all computational geometry and
compositional data accessible and deterministically reproducible in an automated manner there is a clear beneﬁt and
complementary value of using automated analyses for initial assessing a dataset. Such assessment can support users with
oﬀering guidance where making traditional GUI-based analyses are of additional value.

3.6

3D characterization of co-precipitating phases

As the sixth and last case study, we will discuss how the results for the surface meshes of γ(cid:48)(cid:48) and γ(cid:48) precipitates (from
the previous case study) can be further processed via paraprobe-intersector to yield automated and rigorous quantitative
analyses of the precipitates’ spatial arrangement and eventual inclusion in (sets of) so-called coprecipitate conﬁgurations.
In the ﬁrst and especially the second case study we discussed how paraprobe-intersector enables a quantiﬁcation of
an object’s shape and composition dependency as a function of e.g. ϕ. Now, we will extend such analyses to entire
sets of precipitates for collision and proximity respectively. We quantify all cases where γ(cid:48)(cid:48) precipitates are lying in
speciﬁc proximity or are having contact with other γ(cid:48) precipitates. We should mention that the example of studying
surface meshes which here are γ(cid:48)(cid:48) and γ(cid:48) precipitates is again an example that could equally be applied for objects
representing other microstructural features such as other phases, or fragments of grains, crystals or polyhedra-composite-
based descriptions of crystal defect ensembles.

Two high-throughput runs were performed: The ﬁrst took all surface meshes of γ(cid:48)(cid:48) precipitates while the second

18

run took all meshes of γ(cid:48) precipitates. Exemplarily we discuss the conﬁgurations for objects within two speciﬁc of the
runs from the ﬁfth case study. Those for parameter values l = σ = 1.0 nm, ϕN b = 7.6 at.%, ϕAl+T i = 6.1 at.%, and
dprx = 1.0 nm as the maximum proximity threshold distance between objects to qualify still as neighbors.

Figures 9 display examples for the results of such paraprobe-intersector analyses. The ﬁgures report how graph
analytics and 3D visualization can support getting a detailed understanding of coprecipitation (conﬁgurations). For the
selected parameters the analyses returned 2384 γ(cid:48)(cid:48) and 3634 γ(cid:48) interior objects Fig. 9a, respectively.

The ﬁrst case study on edge models of irradiated steel inspected collisions between convex hulls with themselves.
Now, however, pairings of objects from diﬀerent sets were processed. Speciﬁcally, three such set combinations were
processed: First, the γ(cid:48)(cid:48) meshes against themselves, second the γ(cid:48) meshes against themselves, and third the γ(cid:48)(cid:48) meshes
were compared against the γ(cid:48) meshes (all for the above-mentioned parameterization l, σ, ϕN b, ϕAl+T i, dprx).

Individually, each analysis yields a list of disjoint collision pairs and disjoint proximity pairs. Each list of such pairs
can be interpreted as a set of bidirectional relations between object pairs. For example, Fig. 9b and Fig. 9c reveal
that γ(cid:48)(cid:48) object 5395 intersects with γ(cid:48) object 5040. Note that for a given l, σ, ϕN b, ϕAl+T i the list of proximity relations
depends only on the value of dprx in this example. The larger it is the proximity distance the less restrictive one is with
how close two objects are to still be considered as a member of the (same) coprecipitate conﬁguration.

The most interesting case is when all three lists are combined into a single graph Fig. 9b because such a graph
describes all collision or proximity relations between all members of a given pair of object sets for a speciﬁc proximity
threshold dprx. Subsequently, sub-graphs can be extracted to characterize each coprecipitate conﬁguration via graph
analytics.

Figure 9: Spatial analyses of object collisions and proximity with paraprobe-intersector yield a graph-based quantiﬁcation of coprecipitate conﬁgu-
rations. Sub-ﬁgure a) shows a composite of two 3D surface mesh ensembles for one exemplar pair of runs from one of the high-throughput studies
(l = σ = 1.0 nm, ϕN b = 7.6 at.%, ϕAl+T i = 6.1 at.%, dprx ≤ 1.0 nm). The composite shows the precipitates of all γ(cid:48)(cid:48) (brownish color) and γ(cid:48) (turquoise
color) precipitates in the dataset. Sub-ﬁgure b) shows an exemplar sub-graph which resolves all collision and proximity relations between an exemplar
chosen target object (γ(cid:48)(cid:48), ID = 5410) to its neighbors. This is a coprecipitate conﬁguration. Sub-ﬁgure c) visualizes the target and its neighbors via
their surface meshes.

We should explore the beneﬁts and potential limitations of such a description of coprecipitation: A clear beneﬁt
of the graph-based description is that qualifying a conﬁguration is possible through counting disjoint relations. For
instance, an object without any collision or proximity relations is a monolith. An object with only one relation (collision
or proximity) is a partner in a duplet. Evidently, sub-graph analyses are a viable approach to disentangle all possible
intricate conﬁgurations and thus avoid ambiguities or inconsistencies such as double counting of relations.

Figure 9 substantiates that this is beneﬁcial in particular for studying the more complicated conﬁgurations with
multiple objects. However, Fig. 9c pinpoints that there are also challenges: Namely, one can attribute multiple qualitative
types to a given sub-graph: In fact, does object 5410 for instance qualify as a monolith because it does not collide on its
neighbors or does it qualify as a quadruplet, or even septet, given that it has six disjoint objects in proximity? We learn
that qualitative conﬁguration analyses like those in [60, 133] can be formulated more rigorously with our approach.

Furthermore, we learn that analyzing coprecipitation via iso-surface-based meshes faces the challenge that neighboring

19

objects can show internal collision relations. Examples are shown in Fig. 9c ({5395, 5040}, {5397, 5061}, {5455, 5121}).
These internal collisions are a consequence that the analysis logically relates multiple interpretations of iso-surfaces (here
of a region in the dataset to represent either a portion of γ(cid:48)(cid:48) or of γ(cid:48) precipitate volume). This is problematic because
if crystal structure information were available, one would expect that there can be only one thermodynamic phase at
the same location, i.e. inside an overlapping contour about a set of atoms. Evidently, researchers have to make speciﬁc
assumptions how they treat and distinguish such conﬁgurations when all information available is composition based. For
the example here, possible solutions are to consider only collisions, or to treat colliding objects with neighbors of the
target like the above three pairs as a single object; and thus assume object 5410 is of quadruplet type.

In summary, we have substantiated how our approach and paraprobe-intersector, combined with graph analytics, give
researchers additional novel opportunities for making assessments - in an automated manner. We should note that it is
possible to inject further qualitative (or quantitative) pieces of information into the graph, if available. This input can
be based on detailed computational geometry analyses: For instance, we could combine the approach in this case study
with the tessellation-based segmentation of the ﬁrst case study to tessellate the ions in the vicinity of the above-discussed
precipitates.

By inspecting the topology of the resulting labelled 3D Voronoi cell network, it is possible then not only to detect
how objects are arranged but also in which cases they form speciﬁc conﬁgurations. Equipped with such capabilities
it is possible to answer if a triplet {γ(cid:48)(cid:48)/γ(cid:48)/γ(cid:48)(cid:48)} is a sandwich, i.e. a multilayered precipitate and to clarifying how the
adjoining γ(cid:48)(cid:48) objects collide. The Voronoi cell composite would then give a 3D description of the hetero-phase junctions
as accurate as it is possible for point cloud data without having crystal structure information available and when using
experiments with less than ab-initio accuracy and/or precision for resolving true atom positions.

Finally, we want to exemplify that automation of these detailed conﬁguration analyses via scripting and combining
analysis yields high-throughput quantiﬁcation of the sensitivity of how many conﬁgurations (monoliths, duplets, and
others) exist for a given pairing of object sets as a function of parameterization. The idea is to extend the above analyses
of two object sets to combinations of pairs of object sets. Figures 10 show the results as heat maps: Each combination of
ϕN b and ϕAl+T i yields an own collision and proximity graph which can be post-processed to quantify how many disjoint
conﬁgurations exist (monoliths, duplets, and triplets).

Figure 10a shows an island of stability that matches with the low parameter sensitivity of γ(cid:48) and γ(cid:48)(cid:48) for their respective
least sensible regions - compare with Figs. 8a, and 8b. The stronger the combination of ϕN b and ϕAl+T i deviates from
the reference combination (ϕN b = 7.6 at.%, ϕAl+T i = 6.1 at.%) the more likely the respective objects are in diﬀerent
locations in the tomographic reconstruction. In eﬀect, the ensemble is increasingly poorer in registration which results
in a lower number of detectable collision or proximity sub-graphs. Without a rigorous automation such analyses would
be even more tedious to perform than the already very tedious high-throughput characterization of iso-surfaces.

3.7 Scalability

We close the study with exploring the numerical performance of the tools and probe their functioning for large datasets.
All tools are equipped with functionalities for monitoring how much wall-clock time a tool spends in diﬀerent code
sections. The ﬁrst benchmarks are with a dataset which is a tomographic reconstruction of a measured Sm-Co-Fe-Zr-Cu
hard magnet specimen of Polin et al. [140, 141]. The point cloud has 104.9 × 106 ions.

Proﬁling data were collected for paraprobe-ranger (get ion type for each ion), paraprobe-surfacer (compute convex
hull of the dataset), paraprobe-distancer tool (compute shortest Euclidean distance of each ion to the triangle mesh of
the convex hull), and paraprobe-nanochem. Ion positions were delocalized (l = σ = 1 nm, 73 kernel) before collecting
iso-composition surfaces for ϕZr = 5 at.% to 70 at.% with 1 at.% step. Microstructural objects were reconstructed for
each iso-surface, checked for closure and intersection with the edge of the dataset, characterized for their shape, volume,
and which ions they contain. The analyses were executed on a computing node of the TALOS computer cluster (see
e.g. [67] for details). Runs were scheduled exclusively with one MPI process which spawned between 1 to 40 OpenMP
threads. This represents a typical scenario of using a workstation rather than a laptop.

The total wall-clock timings for sequential execution as the reference were as follows (including I/O): paraprobe-ranger
took less than 9 s, the HDF5 ﬁle with the convex hull (5200 triangles) was ready after 2 min and 26 s. Computing the
distances for all ions to the hull took 12 min. Object reconstruction and related pre-processing took 1 h:42 min; of which
the delocalization was the most time-consuming task accounting for 92 % of the total time. Figure 11 documents the
strong-scaling multithreading performance achieved when solving the same tasks with multiple cores. Using 40 threads,
the object reconstruction is ﬁnished in less than 6 min. Ion distances are ready after 30 s.

Finally, we would like to report the successful application of the tools for analyzing a dataset containing one billion ions
(1.074 × 109), which is one among the largest datasets so far measured with APM worldwide. The atom probe specimen
to this dataset was prepared from a Fe-12Cr-0.3C steel sample [142]. We used the same four tools and above-mentioned
questions when analyzing this dataset, but quantiﬁed iron instead of zirconium iso-composition surfaces (5 at.% to 70 at.%
with 1 at.% step). TALOS was used to machine oﬀ the tasks with one MPI process which spawned at most 40 threads.
Nodes were used exclusively.

The reference wall-clock timings for sequential execution were as follows (including I/O): paraprobe-ranger took less
than 2 min, the HDF5 ﬁle with the convex hull (21836 triangles) was ready after 34 min. Computing the distances for
all ions to the hull took 4 h:20 min. Object reconstruction and related pre-processing took 18 h. The delocalization
took 97 % of the total time. Like in previous benchmarks, we see again an order of magnitude improvement thanks to

20

Figure 10: With high-throughput quantiﬁcation workﬂow using paraprobe-intersector and paraprobe-autoreporter, coprecipitate conﬁgurations can be
analyzed to inspect how diﬀerent pairings of runs result in distinguishable collisions and proximity relations between objects. By counting the number
of disjoint relations, it is possible to categorize diﬀerent conﬁgurations. Each such conﬁguration is a sub-graph representing a group of objects with no
collisions on members of other sub-graphs, i.e. distances > dprx to neighbors. The sub-ﬁgures show exemplar results for objects which either collide or
lie within dprx = 1.0 nm proximity. Speciﬁcally, sub-ﬁgure a) shows the total number of disjoint conﬁgurations. Each conﬁguration is a disconnected
sub-graph (one of which is shown in Fig. 9). Sub-ﬁgures b), c) and d) report the fraction how many of these sub-graphs qualify as monoliths, duplets,
and triplets, respectively.

multithreading: When using 40 threads, ranging was accomplished in less than 1 min, all distances were available after
7 min, and the entire analysis with paraprobe-nanochem reduced to 1 h.

We should mention that the main reason for the reduction of the computing time was due to time consuming
tasks like delocalization and distance computations. Figure 11 summarizes that both tasks are eﬃciently strong-scaling
with multithreading. Comparing the execution of the same analysis with 40 threads to sequential analyses indicates
70 % eﬃciency for delocalization and more than 90 % eﬃciency for distancing. We point the interested reader to the
supplementary material where we provide a more detailed discussion about the results of these benchmarks.

In summary, we exempliﬁed how open-source software tools can be combined to deliver a toolbox which gives atom
probers and scientists interested in studying point cloud data eﬃcient, automated, and scriptable opportunities for
reproducible analyses in hitherto seldom explored parameter spaces. The sensitivity of atom probe numerical results
can now be addressed by taking advantage of open-source tools and transparent data management practices to motivate
future work towards a more detailed quantitative understanding and better interoperability between open-source and

21

Figure 11: Strong-scaling multithreading eﬃciency for the paraprobe-distancer and paraprobe-nanochem tool when processing datasets with 108 and
109 ions respectively. Grey staircase curves report which eﬃciency would be naively expected according to Amdahl’s law.

commercial software tools.

4 Data availability

The workﬂows of the use cases are shared. Exemplar notebooks for case studies 3.2, 3.4, 3.5, 3.6, and a generic one
for an oxide-dispersion-strengthened steel [21, 143] are available as jupyter notebooks. These can be used as templates
for getting started with the tools. Notebooks for the other use cases and numerical results in addition to those in the
supplementary material are available from MK upon serious request. Readers who are interested in getting access to
speciﬁc reconstructed point cloud and associated range ﬁles are kindly asked to contact the respective co-authors directly:
BJ (case study 3.1), [21] (case study 3.2), DM (case study 3.3), AR/SK/LR (case study 3.4), VR/SP (nickel base super
alloy part of case studies 3.5, 3.6), SA/AS (titanium part of case study 3.5), and AS (benchmarks 3.7) respectively. A
table of SHA256 checksums for all range and reconstruction ﬁles is provided in the supplementary material (Tab. 1).

5 Code availability

This work modiﬁed previously reported tools [65, 66] and delivers new tools to the paraprobe-toolbox.
The source code and tutorial-style jupyter notebooks are available via the following repository:

• http://gitlab.com/paraprobe/paraprobe-toolbox.git

Only these repositories will be maintained in the future.

6 Acknowledgements

MK gratefully acknowledges the funding and computing time grants (on TALOS) through BiGmax, the Max-Planck-
Society’s Research Network on Big-Data-Driven Materials Science, the Fritz-Haber-Institute of the Max-Planck-Society
(not only for oﬀering a laptop), and the FAIRmat team (for discussions and exchange of ideas on building software tools
for FAIR research data management that motivated the NeXus application deﬁnitions). The work on implementing
software tools for FAIR experimental microscopy methods is funded by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation) – project 460197019. MK and SB are funded by the FAIRmat consortium.

SP’s research is funded by the Australian Research Council Linkage Project LP190101169 and the UNSW Scientia
Fellowship scheme. SP and VR thank Dr. Takanori Sato at the Australian Centre for Microscopy and Microanalysis at
the University of Sydney and acknowledge use of their facilities via Microscopy Australia. BJ would like to acknowledge
funding from the EPSRC program grant MIDAS (EP/S01702X/1) for the study of irradiation damage in zirconium
alloys. Part of this work was supported by Rolls-Royce Plc. and the atom probe facilities at the University of Oxford
are funded by the EPSRC (EP/M022803/1). SA would like to acknowledge the ﬁnancial support by the Alexander von

22

Humboldt Foundation. AR, SK, and LR’s research was funded in whole, or in part, by the Austrian Science Fund (FWF)
[P 34179-N].

We gratefully acknowledge the contribution of the industry partners who enabled the exchange of original datasets in
the course of the study. MK would like to thank Nikita Polin (MPIE) for oﬀering the hard magnet benchmark dataset,
AS for the inspiring discussions during the testing of the toolbox, Christoph Freysoldt (MPIE) for making editorial
suggestions on the manuscript and exchanging experiences on software benchmarking, Sherjeel Shabih for support with
containerizing the toolbox for NOMAD, Christoph Koch for support (both currently at the Humboldt-Universität zu
Berlin), and Alisson Kwiatkowski da Silva, Leigh Stephenson, and Shyam Katnagallu (all three also MPIE) for oﬀering
access to the billion ion benchmark dataset. MK thanks .

7 Author contributions

MK leads the paraprobe project. He designed the tools, implemented the code, performed the analyses, and wrote the
majority of the manuscript. The co-authors inspired the use cases and contributed to the work by oﬀering access to their
datasets, discussing the results, and contributing suggestions as well as editorial advice to the respective case studies:
BJ (case study 3.1), DM (case study 3.3), AR/SK/LR (case study 3.4), VR/SP (nickel base super alloy part of case
studies 3.5, 3.6), and SA/AS (titanium part of case study 3.5. AS contributed to tool testing and ran all benchmarks on
TALOS (case study 3.7). VR contributed additionally by successfully testing the tools on Windows 11 using WSL2, the
Windows Subsystem for Linux.

8 Competing interests statement

The authors declare they have no competing interests.

23

Supplementary Material

Results and discussion

2.2 Automated composition proﬁling for closed objects We implemented two diﬀerent approaches for instructing
the marching cube algorithm to process triangles at the edge of the dataset. These decisions have quantitative eﬀects for
the representation of microstructural features close to and at the edge of the dataset. With the ﬁrst method we process
each voxel of the delocalization grid with the marching cubes algorithm. With the second method we exclude voxels that
are close to the edge of the dataset to spatially restrict the placing of triangles.

The most frequent case is that scientists are interested in composition quantiﬁcation. In this case the composition c is
with a number of target atoms Ntrg over the total number of atoms Ntotal for each voxel (at.%). Delocalization
with a ≥ 0, b > 0, a, b ∈ R and a ≤ b, with a representing Ntrg and b representing

c := Ntrg
Ntotal
converts counts into intensities a
b
Ntotal, respectively.

For voxels inside the dataset a ∧ b ≥ 1 holds from which a

b < a follows. For voxels at the edge or close to holes in the
dataset, though, the opposite holds ( a
b ≥ a). Consequently, using the ﬁrst strategy an object becomes also triangulated
towards the edge of the dataset (see Fig. 3c). This eﬀectively closes in many cases the triangle surface patch of an
objects towards the edge. Thereby, the respective iso-surface facets protrude out of the edge and point cloud (see blue
wireframe in Fig. 3c) though. These parts of an object’s mesh are biased and especially sensitive to the delocalization
parameters. An alternative is to evaluate the intensity (b ≤ 1) for each voxel; and thus inform the marching cubes
algorithm about which voxels are so close to the edge of the dataset that they better should not be processed. This
correction method yields open objects at the edge of the dataset like the surface patch which is shown in Fig. 5a. These
object representations match qualitatively closer to the results from commercial software (IVAS, AP Suite).

2.7 Scalability We learn that the delocalization is the primary target for future code optimization. There are several
possible strategies: We should note that we have not implemented the two-step fast-Fourier-transformation-based ap-
proach that is presumably used in IVAS and AP Suite [4]. Load imbalance is a key reason for limited scalability especially
for the delocalization because we distribute blocks of voxel slabs along the longest axis of the specimen. This requires
compromises, though, as to how equally the ions are distributed across the threads. Load imbalance is especially relevant
when benchmarking the processing of microstructural objects. We found that not only the total number of objects but
also the distribution of geometrical primitives (triangles, points) diﬀers strongly between iso-surfaces. We learned that
further work is necessary before substantiated statements about the strong-scaling eﬃciency of these code portions can
be made. The processing time for each object and task has to be assessed in relation to the cardinality of the object
set, the individual shape and size of the objects and their spatial arrangement. Finally, it should also be mentioned
that modern processors use adaptive protocols for setting the clock cycling to values which guard the processor against
overheating. This can aﬀect with which eﬀective clock cycle especially a series of short running processes gets executed.

Methods

A comment on the usage of graphical user interface (GUI)-based software in atom probe We respect
that GUI-based workﬂows are important for scientists, for most because the usage of such tools is often intuitive.
What is conceptually behind GUI-based software is a set of predeﬁned algorithms and functionalities which the users
compose into a workﬂow as they interact with the software. A common limitation with how this user experience is
currently implemented in many commercial software tools, though, is that the documentation of these, indeed often very
ﬂexible workﬂow compositing system, is that many of the intermediate numerical results and associated metadata are
not exportable if at all tracked. This is relevant, though, for FAIR-compliant research especially with respect to the
repeatability and/or reproducibility of such workﬂows. There are several likely reasons why such limitations of on-the-ﬂy
documentation systems of GUI-based software exist. There are technical intricacies and, given a GUIs ﬂexibility, the
need to have a solution that works ideally for arbitrary usage pattern. Both challenges represent implementation eﬀorts,
and thus increase development costs.

We envision that the role of open-source software tools like the paraprobe-toolbox is in the supporting of established
GUI-based workﬂows within the e.g. atom probe microscopy community and related disciplines. Speciﬁcally, it is the
oﬀering of automation capabilities for making data-driven assessments. These can support scientists with identifying
where investing further time and detail into e.g. traditional GUI-based workﬂows is worthwhile which are then eventually
more conveniently performed via GUIs. Ideally, though, our work is understood in such a way that with making research
more compliant with the FAIR data stewardship principles, there is also an increasing interest, a tremendous set of
opportunities, and need to oﬀer scientists tools which also substantially stronger automate the collecting of metadata.
Ideally, this can incentivize a stronger working together between vendors of proprietary software and developers of (open-
source) software to build eﬀective and eﬃcient tools for a FAIR research data infrastructure for the materials science
communities [25].

Geometrical models for the edge of a dataset and distance-based ﬁltering To enable fast distancing of point
clouds to arbitrarily spatially arranged triangle sets, we modiﬁed the tool with adding an iterative preprocessing stage:
First, the point cloud is coarsely discretized. Second, an extremal distance is computed for each voxel. This distance is

24

successively reﬁned to an eventually shorter extremal distance per voxel. These distances can then guide the subsequent
distancing of each ion through oﬀering on average shorter querying distances so that the bounded volume hierarchy
queries result in less triangle candidates; and thus faster distancing.

Added mesh statistics and mesh inspection functionalities of paraprobe-surfacer, including closure tests delivered also
a more detailed understanding of algorithms for ﬁltering point clouds such as the one reported in [65]. This algorithm
reduces numerical costs when computing α-shapes for large datasets. The key idea is to compute the α-shape for a version
of the point cloud where interior points are ﬁltered out. The resulting α-shapes pick up diﬀerently curved regions in the
dataset but have the disadvantage that the set of (exterior) triangles is neither necessarily watertight nor necessarily free
of regions with double surface patches or other internal structure.

Details depend on the α value. It is possible that the α-shape has double surface patches, i.e. regions with inner and
(oftentimes approximately parallel) outer surface patches. For ions in the proximity, or in between such double surface
patches, the closest triangle of the α-shape can be a triangle of an inner patch. In this case, though, setting a minimum
distance of the ion to the edge of the dataset can cause a stronger than necessary removal of ions. This reduces the
signiﬁcance of the analysis in e.g. spatial statistics applications.

In this work, we learned from inspecting the exported interior tetrahedra for each α-shape as a function of α that this
spatial ﬁltering technique needs improvement. Namely, an additional ﬁltering step so that the eventually created interior
portions of double surface patches are removed. One opportunity is to evaluate the distance of the triangles to the ion
density ﬁeld that gets computed during the spatial ﬁltering. Interior portions of double surface patches have usually a
larger distance to the edge of the dataset than their exterior portions. This could be explored in the future to further
improve the spatial ﬁltering algorithm and thus improve the quality of α-shape-based models for serving as models for
the edge of a point cloud.

Delocalization Paraprobe-nanochem implements a multi-threaded kernel density estimation. Speciﬁcally, a truncated
anisotropic 3D Gaussian delocalization kernel is centered at each reconstructed ion position and Eq. 1 evaluated

(cid:90) +aL

(cid:90) +aL

(cid:90) +aL

A ·

−aL

−aL

−aL

e−[(x2/(2σ2

x))+(y2/(2σ2

x))+(z2/(2(0.5σx)2))]dxdydz = 1

(1)

to quantify the respective delocalized ion intensities in a [(2a) + 1]3 kernel for each voxel. We set µx = µy = µz = 0
and assume that a ∈ N , a > 0 is the kernel half-size, L ∈ R, L > 0 the edge length of (cubic) voxel, and σz = 0.5σx with
σx = σy := σ ∈ R, σ > 0 the kernel width, and x, y, z the reconstructed ion positions. Using the symmetry of the error
function yields analytical solutions for the kernel contributions to each voxel that can be evaluated numerically using
double precision. The implementation could be replaced in the future by a fast-Fourier-transformation-based approach
to improve numerical eﬃciency. The renormalization constant A accounts for ion intensity contributions beyond the tails
of the kernel to prevent a leakage of the total ion intensity.

The implementation enables diﬀerent intensities to be computed, including total number of atoms per voxel, element
composition (at.%), or concentration (in atoms /nm3). A preprocessing identiﬁes the location of the ions based on which
the voxel grid is spatially partitioned along the specimen main axis (z). Subsequently, the delocalization is computed in
such a manner that each thread processes approximately the same total number of ions. Using overlapping halo regions
assures a correct accounting over adjacent regions of the voxel grid.

Spatial correlation analyses - implementation details The above analysis capabilities require robust computa-
tional geometry methods to detect if triangulated polyhedra collide or intersect volumetrically. For convex polyhedra
the Gilbert–Johnson–Keerthi (GJK) algorithm [144] is one such. This algorithm is implemented for instance in the
GammaUNC/PQP library [81] and elsewhere [145–147]. For non-convex polyhedra it is possible to use a tessellation of
the polyhedra into sets of individually all convex smaller polyhedra. Subsequently, these can be evaluated for collisions
via e.g. the GJK algorithm to infer eventual collisions of the original non-convex polyhedra.

Eventually not as eﬃcient as the GJK implementation in GammaUNC/PQP or the study of [147], we tetrahedralize
each surface mesh ﬁrst using TetGen [67, 148]. Subsequently, we evaluate tetrahedron-tetrahedron intersections [145,
148, 149]. Evidently, using such an approach requires that each object (triangulated surface mesh) gets successfully
tetrahedralized as a piecewise-linear complex [148]. Occasionally, we detected cases, though, where portions of the mesh
were locally connected via non-trivial, almost point-like contacts. Although these situations are known to occur for MC
[90] and are characterizable, they can occasionally result in non-trivial-to-debug situations during tetrahedralization.
Therefore, we implemented another strategy for detecting object collisions which checks if two objects share at least one
ion with the same evaporation ID.

To compute the intersection volume of arbitrary polyhedra we use a strategy of tetrahedralizing each polyhedron and
accumulate the volume of individual tetrahedron-tetrahedron-intersections. A numerical robust yet eﬃcient algorithm
for evaluating a de facto analytical intersection volume between two arbitrarily-shaped and -oriented tetrahedra is a
challenging mathematical problem. To the best of our knowledge this has not yet been fully solved [150].

Therefore, we use a more pragmatic approach which is to evaluate NEF tetrahedra intersections using functionalities
of the CGAL library [151, 152]. We are aware that this approach has limitations which are not central to our work but
worth to become improved in future work: One is that computing NEF polyhedra can be very costly. Another one is
that meshed-based processing using ﬂoating point numbers, especially when decomposing a polyhedron into tetrahedra,

25

can result in numerical inaccuracies, even though the NEF-based approach is conceptually capable of being much more
accurate (usage of arbitrary ﬂoating point arithmetics provided).

For the paraprobe-intersector tool we detect intersections between polyhedra via performing object collision tests
rather than proximity analyses [145]. Proximity analyses were evaluated for objects which do not collide by using
the analytical triangle-triangle distance computation capabilities of GammaUNC/PQP [81] and compare the shortest
distance to dprx. A hierarchy of axis-aligned bounding boxes was used to prune the search space of objects, triangles,
and tetrahedra respectively. The post-processing of graph data was implemented in Python.

For this work we made a few assumptions during the implementation of paraprobe-intersector: First, surface meshes
can represent convex or non-convex polyhedra but must not have holes, self-intersections, or be numerically degenerated.
Second, we accept a collection of sets only when it represents results from a linearly sampled sequence of k values. For
instance, a collection of objects from an iso-surface high-throughput characterization (via paraprobe-nanochem) with
increasing values of ϕ in steps of ∆ϕ on [ϕmin, ϕmax] is a suited candidate collection. To begin with, we implemented
pair-wise comparisons between sets k and k + 1 using linearly space probing of parameter values. One could change this
in the future for a non-linear spacing and still use the same k ± 1 traversing. Although we have not implemented it,
allowing for larger jumps in k, to cover for instance relations between the state of an object between k and k ± n with
n > 1 is a possible generalization.

We store all processed directed graphs in the HDF5 output ﬁle of the paraprobe-intersector run. This oﬀers several
beneﬁts, such as programmatic options for automating the visualization of said analyses to support a detailed under-
standing, rendering, debugging, and counting of relations between nodes via graph analytics. Such analytics yield robust
descriptors to qualify which objects are monoliths, duplets, triplets, or arbitrarily complex conﬁgurations as the paper
reports. Recalling our motivation, paraprobe-intersector delivers an automated alternative to otherwise very tedious
manual analyses of coprecipitation phenomena [133] or two-dimensional slicing through images of discretized datasets
[59], thus oﬀering rigorous qualiﬁcation and quantiﬁcation.

Implementation details The above-described tools were implemented in C/C++ and a set of convenience Python
tools. Third-party specialists libraries are called which have open-source licenses for academic usage. These were
CGAL (v5.2.1) [73], which we compiled with the Eigen (v3.3.9) [153] linear algebra, the Boost C/C++ (v1.76.0) [154]
template library, and the GMP (v6.2.1) and MPFR (v4.0.2) numerical libraries. Additionally, we used code from
GammaUNC/PQP (v1.3) [79–81] for computing triangle-triangle intersections. Surface meshes were tessellated with
TetGen (v1.5.1) [67, 148]. Point clouds were tessellated with Voro++ (v0.4.6) [155]. Bounded volume hierarchies were
used to reduce the number of location and intersection queries for geometric primitives [65, 79]. Graph analytics were
instructed via cypher scripting using neo4j [156] (v4.3.1) and Python. Data were written with the HDF5 library (v1.12.0)
[157].

In continuation of our previous work [65, 66], we parallelized the tools via an MPI/OpenMP work partitioning
strategy. At the coarse layer work packages are distributed via the Message Passing interface (MPI) library. At the
ﬁne layer these work packages are split further to be solved via Open Multi-Processing (OpenMP) multithreading. We
conﬁgured paraprobe with cmake build tools (v3.19) and compiled with the GNU C/C++ compiler (v7.5) using in most
cases -O3 -march=native optimization.

Except for the benchmarks and the molybdenum-hafnium case study, all case studies were executed on a laptop.
Speciﬁcally, this was a Dell Latitude 5480 i7-782 with a four hyper-threading core pair CPU with 32 GiB main memory
plus a SanDisk X400 M.2 2280 SSD drive with 512 GiB, all instructed by Ubuntu 18.04.5. We used four OpenMP
threads, OMP_PLACES=cores mapping, and one MPI process. We explore the strong-scaling multithreading eﬃ-
ciency with executing runs on the TALOS system (see [66, 67] for details). Here, nodes were used exclusively with
OMP_PLACES=threads mapping. We compare wall-clock timing data of individual functions (I/O monitored sepa-
rately) while processing the same datasets using from 1 to 40 OpenMP threads.

The molybdenum-hafnium case study was processed with a Dell XPS 15 9510 laptop with an Intel i9-11900H eight-
core CPU with 32 GiB main memory plus a SK Hynix 1TB PC711 NVMe disk, all instructed by Windows11 Pro. We
used the Windows Subsystem for Linux (WSL2) running Ubuntu 20.04.4 LTS. Four of the eight cores and 20 GiB main
memory were allocated for the WSL2 part. Thereby the work also documents that the toolbox can be used not only
on Linux. We would be happy to be contacted by interested individuals who would like to run the toolbox on Mac OS
systems so that we can extend the range of applicable systems for the atom probe community.

26

SHA256 checksums

Table 1: SHA256 checksums for the point cloud and range ﬁles for the datasets of each case study. Benchmarks are marked as (B) cases.

File

Case study

SHA256

2.1
R33_07490-v02.pos
2.1
R33_07490-v02.cluster.rrng
2.1
R33_07490-v02.cluster.indexed.rrng
2.1
R33_07490-v02.cluster.indexed.pos
2.2
R21_07575-v02.rrng
2.2
R21_07575-v02.pos
2.3
R5096_41455-v02.pos
2.3
R5096_41455-v02.rrng
2.4
R21_08680-v02.pos
2.4
R21_08680.rrng
2.5, 2.6
R04_22071.RRNG
2.5, 2.6
R04_22071.pos
R5006_29110_Top_Level_ROI.apt
2.5
R5006_29110_Top_Level_ROI.RRNG 2.5
D1_High_Hc_R5076_52126.apt
D1_High_Hc_R5076_52126.RRNG
R76_20169-v01.pos
20169.RRNG

2.7 (B)
2.7 (B)
2.7 (B)
2.7 (B)

322a24d780b54bdcea132a0014c152aa8a03b408d33ea25fc88ecfa379f2bd12
289153bb0bc29679fd9a362e9acfd856b49b3795c58f8a0c9045d01ed442e621
4b934716e274eb4b9a152aaf2f2452357dae0f1da10e55ac9862408241f6f1bc
74cdde57f0dd34dd153f377898a67e35b62103f76fa7c053193833b548dfac73
fee7128f30ed88f7e115ec30fbbd1f3c0962d55772849d702cc3917056f61c93
66bab07cc649236bc63cec29bb67375e226edc4f92fde00313c6f203d43bf4ca
b1f05fe36c0acee0ad91b46ba5f7b490638ed51f0455ada22df01bbb25b26950
154b4dac9aa8e86191c4746dd92f7faa88af2105494a24d3e3b9fdd04a9ace13
be6287c45233de97f0376fd8e1e078f7cfa029bac8b1a4087936076a2ea0de51
c75aead0e82d7e4149cf07192e734ee9765259259b9e7cba96f284d4a7ec7cea
4cb6dc8da15412e33104fc64c6b2e6480484d08f8ca8641895fd00ea848e9f8f
8040adﬀed5c1f9761cdc2da6b881625c9f9335f3e96c557374f8bae0dc89078
614fdfd8b7dbb3e24decea5493a24df3e7b468062e854682f620a440c8e7dcd0
30f11b1cfa5db8f1b3b22ﬀfed9f922ae8a0c885d4c50d76538d452b8a19c6e9
b803a34aa6f3b87acbf9db152c17453789084278cef37237b898acb2ae405403
3107ae9a46357267eb294d6df5b3819d42c4fe604a819eaa86864f3b7d067d61
9a0c71984acb5290c5cb31e98ddd29e06d08da078817baa305b038cef278c237
db9ad704afe141a1040e72698702bd1ef656c26b4b390aad4d6c6e4d20f58de8

27

References

1. Williams, D. B. & Carter, C. B. Transmission Electron Microscopy 1st ed. (Springer, New York, 1996).

2. Kirkland, E. J. Advanced Computing in Electron Microscopy 3rd ed. (Springer, Cham, 2020).

3. Miller, M. K. Atom Probe Tomography: Analysis at the Atomic Level 1st ed. (Springer, New York, 2000).

4. Larson, D. J., Prosa, T. J., Ulﬁg, R. M., Geiser, B. P. & Kelly, T. F. Local Electrode Atom Probe Tomography

1st ed. (Springer, New York, 2013).

5. Atom Probe Tomography Put Theory Into Practice 1st ed. (Elsevier, Amsterdam, 2016).

6. Gault, B. et al. Atom probe tomography. Nat. Rev. Methods Primers 1 (2021).
7. Stukowski, A. Visualization and analysis of atomistic simulation data with OVITO–the Open Visualization Tool.

Model. Simul. Mater. Sci. Eng. 18, 015012 (2010).

8. Zepeda-Ruiz, L. A. et al. Atomistic insights into metal hardening. Nat. Mater. 20, 315–320.
9. Casalino, L. et al. AI-driven multiscale simulations illuminate mechanisms of SARS-CoV-2 spike dynamics. Int. J.

High Perform. Comput. Appl. 35, 432–451 (2021).

10. Huber, L., Hadian, R., Grabowski, B. & Neugebauer, J. A machine learning approach to model solute grain

boundary segregation. npj Comput. Mater. 4, 64 (2018).

11. Stukowski, A. & Albe, K. Extracting dislocations and non-dislocation crystal defects from atomistic simulation

data. Model. Simul. Mat. Sci. Eng. 18, 085001 (2010).

12. Ghamarian, I., Yu, L.-J. & Marquis, E. A. Morphological classiﬁcation of dense objects in atom probe tomography

data. Ultramicroscopy 215, 112996 (2020).

13. Lazar, E. A., Han, J. & Srolovitz, D. J. Topological framework for local structure analysis in condensed matter.

PNAS 112, E5769–E5776 (2015).

14. Lazar, E. A. VoroTop: Voronoi cell topology visualization and analysis toolkit. Model. Simul. Mater. Sci. Eng. 26,

015011 (2017).

15. Spencer, L. T., Chen, K., Han, J., Purohit, P. K. & Srolovitz, D. J. Reconciling grain growth and shear-coupled

grain boundary migration. Nat. Commun. 8, 1764 (2017).

16. Momma, K. & Izumi, F. VESTA 3 for three-dimensional visualization of crystal, volumetric and morphology data.

J. Appl. Crystallogr. 44, 1272–1276 (2011).

17. Jain, A. et al. The Materials Project: A materials genome approach to accelerating materials innovation. APL

Mater. 1, 011002 (2013).

18. Draxl, C. & Scheﬄer, M. The NOMAD laboratory: from data sharing to artiﬁcial intelligence. J. Phys. Mater. 2,

036001 (2019).

19. Wilkinson, M. D. et al. The FAIR Guiding Principles for scientiﬁc data management and stewardship. Sci. Data

3, 160018 (2016).

20. Draxl, C. & Scheﬄer, M. in Handbook of Materials Modeling: Methods: Theory and Modeling (eds Andreoni, W.

& Yip, S.) 49–73 (Springer International Publishing, Cham, 2020).

21. Kühbach, M. et al. Community-Driven Methods for Open and Reproducible Software Tools for Analyzing Datasets

from Atom Probe Microscopy. Microsc. Microanal., 1–16 (2021).

22. de la Peña, F. et al. hyperspy (last accessed May 19, 2022). 2021.

23. Clausen, A. et al. LiberTEM (last accessed May 19, 2022). 2021.

24. Madsen, J. & Susi, T. The abTEM code: transmission electron microscopy from ﬁrst principles (last accessed May

19, 2022). 2021.

25. Scheﬄer, M. et al. FAIR data enabling new horizons for materials research. Nature 604, 635–642 (2022).
26. Müller, E. W. Resolution of the Atomic Structure of a Metal Surface by the Field Ion Microscope. J. Appl. Phys.

27, 474–476 (1956).

27. Katnagallu, S. et al. Advanced data mining in ﬁeld ion microscopy. Mater. Charact. 146, 307–318 (2018).
28. Morgado, F. F. et al. Revealing atomic-scale vacancy-solute interaction in nickel. Scr. Mater. 203, 114036 (2021).
29. Oberdorfer, C., Eich, S. M., Lütkemeyer, M. & Schmitz, G. Applications of a versatile modelling approach to 3D

atom probe simulations. Ultramicroscopy 159, 184–194 (2015).

30. Gault, B. et al. Reﬂections on the Spatial Performance of Atom Probe Tomography in the Analysis of Atomic

Neighborhoods. Microsc. Microanal., 1–11 (2021).

31. Brehm, M., Thomas, M., Gehrke, S. & Kirchner, B. TRAVIS – A Free Analyzer for Trajectories from Molecular

Simulation. J. Chem. Phys. 152, 164105 (2020).

28

32. Xu, R. et al. Three-dimensional coordinates of individual atoms in materials revealed by electron tomography.

Nat. Mater. 14, 1099–1103 (11 2015).

33. Kelly, T., Dunin-Borkowski, R. & Meyer, J. Project Tomo: Toward Atomic-scale Analytical Tomography. Microsc.

Microanal. 26, 2618–2621 (2020).

34. Ceguerra, A. V. et al. The rise of computational techniques in atom probe microscopy. Curr. Opin. Solid. State.

Mater. Sci. 17, 224–235 (2013).

35. Kontis, P. et al. The eﬀect of chromium and cobalt segregation at dislocations on nickel-based superalloys. Scr.

Mater. 145, 76–80 (2018).

36. Haley, D. et al. List of software tools for atom probe (last accessed May 19, 2022). 2021. http://docs.google.

com/spreadsheets/d/1pGKUYOadY8vtEtvb9cMMZOWg7lHHkJ0SL8ew4CsSDVQ/edit#gid=0.

37. Miller, M. K. & Horton, J. A. An Atom Probe Field-Ion Microscope Study of Boron Decorated Boundaries in

Ni3Al. Scr. Metall. 20, 789–792 (1986).

38. Hellman, O. C., Vandenbroucke, J. A., Rüsing, J., Isheim, D. & Seidman, D. N. Identiﬁcation of 2D Boundaries
from 3D Atom Probe Data, and Spatial Correlation of Atomic Distributions with Interfaces in Proceedings of the
MRS Fall Meeting 1999: Symposia A/C – Multiscale Phenomena in Materials - Experiments in Modeling (eds
Devincre, B., Lassila, D. H., Phillips, R. & Robertson, I. M.) 578 (2000), 395–400.

39. Hellman, O. C., Vandenbroucke, J., Rüsing, J., Isheim, D. & Seidman, D. N. Analysis of Three-dimensional Atom-

probe Data by the Proximity Histogram. Microsc. Microanal. 6, 437–444 (5 2000).

40. Hellman, O. C., du Rivage, J. B. & Seidman, D. N. Eﬃcient sampling for three-dimensional atom probe microscopy

data. Ultramicroscopy 95, 199–205 (2003).

41. Karnesky, R. A., Sudbrack, C. K. & Seidman, D. N. Best-ﬁt ellipsoids of atom-probe tomographic data to study

coalescence of γ(cid:48) (L12) precipitates in Ni–Al–Cr. Scr. Mater. 57, 353–356.

42. Felfer, P. et al. A quantitative atom probe study of the Nb excess at prior austenite grain boundaries in a Nb

microalloyed strip-cast steel. Acta Mater. 60, 5049–5055 (2012).

43. Felfer, P., Ceguerra, A., Ringer, S. & Cairney, J. Applying computational geometry techniques for advanced feature

analysis in atom probe data. Ultramicroscopy 132, 100–106 (2013).

44. Felfer, P., Ceguerra, A. V., Ringer, S. P. & Cairney, J. M. Detecting and extracting clusters in atom probe data:

A simple, automated method using Voronoi cells. Ultramicroscopy 150, 30–36 (2015).

45. Krakauer, B. W. & Seidman, D. N. Absolute atomic-scale measurements of the Gibbsian interfacial excess of solute

at internal interfaces. Phys. Rev. B 48, 6724–6728 (1993).

46. Felfer, P., Scherrer, B., Demeulmeester, J., Vandervoorst, W. & Cairney, J. M. Mapping interfacial excess in atom

probe data. Ultramicroscopy 159, 438–444 (2015).

47. Peng, Z. et al. An Automated Computational Approach for Complete In-Plane Compositional Interface Analysis

by Atom Probe Tomography. Microsc. Microanal. 25, 389–400 (2019).

48. Hyde, J. M. & English, C. A. An Analysis of the Structure of Irradiation induced Cu-enriched Clusters in Low

and High Nickel Welds. MRS Proceedings 650, 6–12 (2000).

49. Stephenson, L. T., Moody, M. P., Liddicoat, P. V. & Ringer, S. P. New Techniques for the Analysis of Fine-Scaled

Clustering Phenomena within Atom Probe Tomography (APT) Data. Microsc. Microanal. 13, 448–463 (2007).

50. Marquis, E. A. et al. On the Use of Density-Based Algorithms for the Analysis of Solute Clustering in Atom Probe
Tomography Data in Proceedings of the 18th International Conference on Environmental Degradation of Materials
in Nuclear Power Systems – Water Reactors (Springer, Cham, 2017), 2097–2113.

51. Sarker, J. et al. A combined approach of atom probe tomography and unsupervised machine learning to understand

phase transformation in (AlxGa1−x)2

O3. Appl. Phys. Lett. 116 (15 2020).

52. Zhou, X. et al. The hidden structure dependence of the chemical life of dislocations. Sci. Adv 7 (2021).
53. Torres, K. L., Daniil, M., Willard, M. A. & Thompson, G. B. The inﬂuence of voxel size on atom probe tomography

data. Ultramicroscopy 111, 464–468 (6 2011).

54. Martin, T. L. et al. Insights into microstructural interfaces in aerospace alloys characterised by atom probe to-

mography. Mater. Sci. Technol. 32, 232–241 (3 2015).

55. Hornbuckle, B., Kapoor, M. & Thompson, G. A procedure to create isoconcentration surfaces in low-chemical-

partitioning, high-solute alloys. Ultramicroscopy 159, 346–353 (2015).

56. Barton, D., Hornbuckle, B., Darling, K. & Thompson, G. The Inﬂuence of Isoconcentration Surface Selection in

Quantitative Outputs from Proximity Histograms. Microsc. Microanal. 25, 401–409 (2 2019).

57. Jenkins, B. M. et al. Reﬂections on the Analysis of Interfaces and Grain Boundaries by Atom Probe Tomography.

Microsc. Microanal. 26, 247–257 (2 2020).

29

58. London, A. J., Haley, D. & Moody, M. Single-Ion Deconvolution of Mass Peak Overlaps for Atom Probe Microscopy.

Microsc. Microanal. 23, 300–306 (2017).

59. Theska, F., Ringer, S. P. & Primig, S. Atom Probe Microscopy of Strengthening Eﬀects in Alloy 718. Microsc.

Microanal. 25, 470–480 (2019).

60. Rielli, V. V. et al. High-Throughput Analysis of Precipitates in a Ni-based superalloy (last accessed May 19, 2022).
2020. http://www.youtube.com/watch?v=YAKjdaxxlZk&list=PL-0gZJOh94SfNCVFaZm6WNGLc2BDkvRex&index=
17.

61. Haley, D. & Ceguerra, A. 3Depict - Visualisation & Analysis for Atom Probe (last accessed on May 19, 2022).

http://threedepict.sourceforge.net (2018).

62. Keutgen, J., London, A. & Cojocaru-Mirédin, O. Solving Peak Overlaps for Proximity Histogram Analysis of

Complex Interfaces for Atom Probe Tomography Data. Microsc. Microanal. 27, 28–35 (1 2020).

63. Heller, M. et al. An open-source software toolbox for FAIR atom probe data analysis in Proceedings of the Atom

Probe Tomography & Microscopy Conference 2021 (Virtual), September 27-30, Portland, U.S. (2021).

64. Zhou, X. et al. Revealing in-plane grain boundary composition features through machine learning from atom probe

tomography data. Acta Mat. 226, 117633 (2022).

65. Kühbach, M. et al. On strong-scaling and open-source tools for analyzing atom probe tomography data. npj

Comput. Mater. 7, 21 (2021).

66. Kühbach, M., Kasemer, M., Gault, B. & Breen, A. On Open and Strong-Scaling Tools for Atom Probe Crystal-
lography: High-Throughput Methods for Indexing Crystal Structure and Orientation. J. Appl. Crystallogr. 54,
1490–1508 (2021).

67. Kühbach, M. & Roters, F. Quantiﬁcation of 3D spatial correlations between state variables and distances to the
grain boundary network in full-ﬁeld crystal plasticity spectral method simulations. Model. Simul. Mat. Sci. Eng.
28, 055005 (2020).

68. Kluyver, T. et al. Jupyter Notebooks – a publishing format for reproducible computational workﬂows in Positioning
and Power in Academic Publishing: Players, Agents and Agendas (eds Loizides, F. & Schmidt, B.) (2016), 87–90.

69. Janßen, J. et al. pyiron: An integrated development environment for computational materials science. Comput.

Mater. Sc. 163, 24–36 (2019).

70. El-Zoka, A. A. et al. Enabling near-atomic–scale analysis of frozen water. Sci. Adv. 6 (2020).
71. Felfer, P. Ion list APT (last accessed May 19, 2022). 2021. http://github.com/peterfelfer/Atom- Probe-

Toolbox/blob/master/IonenlisteAPT.xlsx.

72. Eberly, D. H. Robust and Error-Free Geometric Computing 1st ed. (CRC Press, 2020).
73. CGAL Project, T. CGAL User and Reference Manual 5.3. http://doc.cgal.org/5.3/Manual/packages.html

(CGAL Editorial Board, 2021).

74. Felfer, P. et al. Atom-Probe-Toolbox (last accessed May 19, 2022). 2022. http://github.com/peterfelfer/Atom-

Probe-Toolbox.

75. Edelsbrunner, H. & Mücke, E. P. Three-Dimensional Alpha Shapes. ACM Trans. Graph. 13, 43–72 (1994).
76. Da, T. K. F., Loriot, S. & Yvinec, M. in CGAL User and Reference Manual 5.3 (CGAL Editorial Board, 2021).

http://doc.cgal.org/5.3/Manual/packages.html#PkgAlphaShapes3.

77. Still, E. K., Schreiber, D. K., Wang, J. & Hosemann, P. Alpha Shape Analysis (ASA) Framework for Post-

Clustering Property Determination in Atom Probe Tomographic Data. Microsc. Microanal., 1–21 (2021).

78. Loriot, S., Rouxel-Labbé, M., Tournois, J. & Yaz, I. O. in CGAL User and Reference Manual 5.3 (CGAL Editorial

Board, 2021). http://doc.cgal.org/5.3/Manual/packages.html#PkgPolygonMeshProcessing.

79. Gottschalk, S., Lin, M. C. & Manocha, D. OBBTree: a hierarchical structure for rapid interference detection in
Proceedings of the 23rd annual conference on Computer graphics and interactive techniques - SIGGRAPH '96
(1996).

80. Larsen, E., Gottschalk, S., Lin, M. & Manocha, D. Fast distance queries with rectangular swept sphere volumes
in Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation.
Symposia Proceedings (Cat. No.00CH37065) (2000).

81. Larsen, E. & Gottschalk, S. GammaUNC/PQP library (last accessed May 19, 2022). http : / / github . com /

GammaUNC/PQP.

82. Loriot, S., Rouxel-Labbé, M., Tournois, J. & Yaz, I. O. in CGAL User and Reference Manual 5.4 (CGAL Editorial

Board, 2022). https://doc.cgal.org/5.4/Manual/packages.html#PkgPolygonMeshProcessing.

83. Botsch, M., Kobbelt, L., Pauly, M., Alliez, P. & Levy, B. Polygon Mesh Processing 1st ed. (CRC Press, New York,

2010).

30

84. Alliez, P., Pion, S. & Gupta, A. in CGAL User and Reference Manual 5.4 (CGAL Editorial Board, 2022). https:

//doc.cgal.org/5.4/Manual/packages.html#PkgPrincipalComponentAnalysisD.

85. Könnecke, M. et al. The NeXus data format. J. Appl. Crystallogr. 48, 301–305 (2015).
86. Aeschlimann, M. et al. NeXus FAIRmat proposal (last accessed May 19, 2022). 2022. https : / / fairmat -

experimental.github.io/nexus-fairmat-proposal.

87. Collins, S. P. et al. ExPaNDS ontologies v1.0 (last accessed May 19, 2022). 2021. https://zenodo.org/record/

4806026#.YnwBz1RBy3_.

88. Görzig, H., Jemian, P. R. & Watts, B. NeXusOntology (last accessed May 19, 2022). https : / / github . com /

nexusformat/NeXusOntology.

89. Ulﬁg, R. M., Oltman, E., Larson, D. J. & Smentkowski, V. S. Improvements in Three-Dimensional Compositional

Analysis of Complex Alloys. Microsc. Microanal. 15, 294–295 (2009).

90. Lewiner, T., Lopes, H., Vieria, A. W. & Tavares, G. Eﬃcient implementation of Marching Cubes’ cases with

topological guarantees. J. Graph. Tools 8 (2003).

91. Lorensen, W. E. & Cline, H. E. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. ACM

SIGGRAPH Computer Graphics 21, 163–169 (1987).

92. Newman, T. S. & Yi, H. A survey of the marching cubes algorithm. Comput. Graph. 30, 854–879 (2006).
93. Lorensen, W. E. History of the Marching Cubes Algorithm. IEEE Comput. Graph. 40, 8–15 (2020).
94. Engwirda, D. Source code repository to construct generalised Voronoi-type dual meshes in MATLAB. (last accessed

May 19, 2022). 2020. http://github.com/dengwirda/dual-mesh.

95. Kazhdan, M., Bolitho, M. & Hoppe, H. Poisson Surface Reconstruction in Eurographics Symposium on Geometry

Processing (eds Polthier, K. & Sheﬀer, A.) (2006).

96. Ester, M., Kriegel, H.-P., Sander, J. & Xu, X. A Density-Based Algorithm for Discovering Clusters in Large Spatial

Databases with Noise in (AAAI Press, 1996), 226–231.

97. Katrioplas, K. & Rouxel-Labbé, M. in CGAL User and Reference Manual 5.3 (CGAL Editorial Board, 2021).

http://doc.cgal.org/5.3/Manual/packages.html#PkgOptimalBoundingBox.

98. Fischer, K., Gärtner, B., Herrmann, T., Hoﬀmann, M. & Schönherr, S. in CGAL User and Reference Manual 5.3
(CGAL Editorial Board, 2021). http://doc.cgal.org/5.3/Manual/packages.html#PkgBoundingVolumes.

99. O´Rourke, J. Finding Minimal Enclosing Boxes. Int. J. Comput. Inf. 14, 183–199 (1985).
100. Gorissen, B. & Melchior, S. OptimalOBB (last accessed May 19, 2022). 2018. http://github.com/chadogome/

OptimalOBB.

101. Barequet, G. & Har-Peled, S. Eﬃciently Approximating the Minimum-Volume Bounding Box of a Point Set in

Three Dimensions. J. Algorithms 38, 91–109 (2001).

102. Chang, C.-T., Gorissen, B. & Melchior, S. Fast oriented bounding box optimization on the rotation group SO(3, R).

ACM Trans. Graph. 30, 122 (2011).

103. Eberly, D. Intersection of a Triangle and a Cylinder (last accessed May 19, 2022). 2010. http : / / www .

geometrictools.com/Documentation/IntersectionTriangleCylinder.pdf.

104. Manders, E., Stap, J., Brakenhoﬀ, G., van Driel, R. & Aten, J. Dynamics of three-dimensional replication patterns
during the S-phase, analysed by double labelling of DNA and confocal microscopy. J. Cell Sci. 103, 857–862
(1992).

105. Costes, S. V. et al. Automatic and Quantitative Measurement of Protein-Protein Colocalization in Live Cells.

Biophys. J. 86, 3993–4003 (2004).

106. Bolte, S. & Cordelières, F. P. A guided tour into subcellular colocalization analysis in light microscopy. J. Microsc.

224, 213–232 (2006).

107. Zinchuk, V. & Zinchuk, O. Quantitative Colocalization Analysis of Confocal Fluorescence Microscopy Images.

Curr. Protoc. Cell Biol. 39 (2008).

108. Gilles, J.-F., Dos Santos, M., Boudier, T., Bolte, S. & Heck, N. DiAna, an ImageJ tool for object-based 3D

co-localization and distance analysis. Methods 115, 55–64 (2017).

109. Haley, D., Petersen, T., Barton, G. & Ringer, S. P. Inﬂuence of ﬁeld evaporation on Radial Distribution Functions

in Atom Probe Tomography. Philos. Mag. 89, 925–943 (2009).

110. Karnesky, R. A., Isheim, D. & Seidman, D. N. Direct measurement of two-dimensional and three-dimensional
interprecipitate distance distributions from atom-probe tomographic reconstructions. Appl. Phys. Lett 91 (2007).
111. Jenkins, B. M. et al. Using alpha hulls to automatically and reproducibly detect edge clusters in atom probe

tomography datasets. Mat. Characterization 160, 110078 (2020).

31

112. Jenkins, B. M. et al. The eﬀect of composition variations on the response of steels subjected to high ﬂuence neutron

irradiation. Materialia 11, 100717 (2020).

113. Edelsbrunner, H., Kirkpatrick, D. G. & Seidel, R. On the Shape of a Set of Points in the Plane. IEEE Trans. Inf.

Theory 29, 551–559 (1983).

114. Felfer, P. & Cairney, J. A computational geometry framework for the optimisation of atom probe reconstructions.

Ultramicroscopy 169, 62–68 (2016).

115. London, A. J. & Jenkins, B. Cluster-Alpha-Edge (last accessed May 19, 2022). 2019. https : / / github . com /

andyroo101/Cluster-Alpha-Edge.git.

116. Lafarge, T. & Pateiro-Lopez, B. CRAN R Project Package "alphashape3d" (last accessed May 19, 2022). 2020.

http://cran.r-project.org/web/packages/alphashape3d/index.html.

117. Cerjak, H., Mendez-Martin, F. & Domakova, M. Atom probe investigations on temper embrittlement and reversible

temper embrittlement in S 690 steel weld metal. Sci. Technol. Weld. Joining 23, 140–147 (2 2017).

118. Baerentzen, J. & Aanaes, H. Signed Distance Computation Using the Angle Weighted Pseudonormal. IEEE Trans-

actions on Visualization and Computer Graphics 11, 243–253 (2005).

119. Mayweg, D. Microstructural characterization of white etching cracks in 100Cr6 bearing steel with emphasis on the

role of carbon 1st ed. (Dissertation, RWTH Aachen University, 2021).

120. Mayweg, D., Morsdorf, L., Li, Y. & Herbig, M. Correlation between grain size and carbon content in white etching

areas in bearings. Acta Mat. 215, 117048 (2021).

121. Wei, Y. et al. 3D nanostructural characterisation of grain boundaries in atom probe data utilising machine learning

methods. PLOS One (2019).

122. Grain boundary segregation engineering in metallic alloys: A pathway to the design of interfaces. Curr. Opin. Solid

State Mater. Sci. 18, 253–261 (2014).

123. Kontis, P. Interactions of solutes with crystal defects: A new dynamic design parameter for advanced alloys. Scr.

Mater. 194, 113626 (2021).

124. Felfer, P. & Weiser, M. APT Winterschool day 3 - lecture 4 - (local) interfacial excess and interfacial excess

mapping (last accessed March 12, 2022). 2022. http://www.youtube.com/watch?v=gOwD8z_ZY5Y.

125. Leitner, K. et al. How grain boundary chemistry controls the fracture mode of molybdenum. Mater. Des 142,

36–43 (2018).

126. Leitner, K. et al. On grain boundary segregation in molybdenum materials. Mater. Des. 135, 204–212 (2017).
127. Scheiber, D., Razumovskiy, V. I., Puschnig, P. & nd L. Romaner, R. P. Ab initio description of segregation and

cohesion of grain boundaries in W–25 at.% Re alloys. Acta Mater. 88, 180–189 (2015).

128. Han, J., Vitek, V. & Srolovitz, D. J. Grain-boundary metastability and its statistical properties. Acta Mater. 104,

259–273 (2016).

129. Scheiber, D. & Romaner, L. Impact of the segregation energy spectrum on the enthalpy and entropy of segregation.

Acta Mater. 221, 117393 (2021).

130. Seidman, D. N. Solute-atom segregation at internal interfaces on an atomic scale: atom-probe experiments and

computer simulations. Mater. Sc. Eng. A 137, 57–67 (1991).

131. Relationship of chemical composition and structure on an atomic scale for metal/metal interfaces: The W(Re)

system. Scr. Metall. et Mater. 27, 693–698 (1992).

132. Korte-Kerzel, S. et al. Defect phases – thermodynamics and impact on material properties. Int. Mater. Rev. 0,

1–29 (2021).

133. Rielli, V. V. et al. Evolution of nanoscale precipitates during common Alloy 718 ageing treatments. Mat. & Design

205, 109762.

134. Gault, B., Moody, M. P., Cairney, J. M. & Ringer, S. P. Atom probe crystallography. Mater. Today 15, 378–386

(2012).

135. Ceguerra, A. V., Day, A. C. & Ringer, S. P. Assessing the Spatial Accuracy of the Reconstruction in Atom Probe
Tomography and a New Calibratable Adaptive Reconstruction. Microsc. Microanal. 25, 309–319 (2019).
136. Theska, F. et al. On the early stages of precipitation during direct ageing of Alloy 718. Acta Mater. 188, 492–503

(2020).

137. de Geuser, F. & Gault, B. Metrology of small particles and solute clusters by atom probe tomography. Acta Mater.

188, 406–415 (2020).

138. Zheng, Y., Stoichko, A., Feng, Q., Rajarshi, B. & Hamish, B. D. F. L. Shuﬄe-induced modulated structure and
heating-induced ordering in the metastable β-titanium alloy, Ti-5Al-5Mo-5V-3Cr. Scr. Mater. 176, 7–11 (2020).

32

139. Antonov, S. et al. Nucleation and growth of α phase in a metastable β-Titanium Ti-5Al-5Mo-5V-3Cr alloy: Inﬂuence
from the nano-scale, ordered-orthorhombic O(cid:48)(cid:48) phase and α compositional evolution. Scr. Mater. 194, 113672
(2021).

140. Polin, N. et al. Atomic-scale Insights to Design of High-Performing Hard Magnets Gained by Atom Probe Tomog-
raphy in Proceedings of the Atom Probe Tomography & Microscopy Conference 2021 (Virtual), September 27-30,
Portland, U.S. (2021).

141. Saxena, A., Polin, N., Raabe, D., Gault, B. & Freysoldt, C. A materials informatics framework to discover patterns
in atom probe tomography data in Proceedings of the Atom Probe Tomography & Microscopy Conference 2021
(Virtual), September 27-30, Portland, U.S. (2021).

142. Belde, M., Springer, H. & Raabe, D. Vessel microstructure design: A new approach for site-speciﬁc core-shell

micromechanical tailoring of TRIP-assisted ultra-high strength steels. Acta Mat. 113, 19–31 (2016).

143. Wang, J., Schreiber, D. K., Bailey, N., Hosemann, P. & Toloczko, M. B. The Application of the OPTICS Algorithm

to Cluster Analysis in Atom Probe Tomography Data. Microsc. Microanal. 25, 338–348 (2019).

144. Gilbert, M., Vurpillot, F., Vella, A., Bernas, H. & Deconihout, B. Some aspects of the silicon behaviour under

femtosecond pulsed laser ﬁeld evaporation. Ultramicroscopy 107, 767–772 (2007).

145. Hornus, S. A review of polyhedral intersection detection and new techniques RR-8730, Inria Nancy - Grand Est

(Villers-lès-Nancy, France). http://hal.inria.fr/hal-01157239v1.

146. Magalhães, S. V. MeshIntersection http://github.com/sallesviana/MeshIntersection.git.
147. de Magalhães, S. V. G., Franklin, W. R. & Andrade, M. V. A. in Lecture Notes in Computational Science and

Engineering 1st ed., 365–383 (Springer International Publishing, 2019).

148. Si, H. TetGen, a Delaunay-Based Quality Tetrahedral Mesh Generator. ACM Trans. Math. Softw. 41, (2015).
149. Ganovelli, F., Ponchio, F. & Rocchini, C. Fast Tetrahedron-Tetrahedron Overlap Algorithm. J. Graph. Tools 7,

17–25 (2002).

150. McCoid, C. & Gander, M. J. Intersection of tetrahedra http : / / www . unige . ch / ~mccoid / ongoing /

mccoid2020tetrahedra.pdf.

151. Hachenberger, P., Kettner, L. & Mehlhorn, K. Boolean operations on 3D selective Nef complexes: Data structure,

algorithms, optimized implementation and experiments. Comput. Geometry 38, 64–99 (1-2 2007).

152. Hachenberger, P. & Kettner, L. in CGAL User and Reference Manual 5.3 (CGAL Editorial Board, 2021). https:

//doc.cgal.org/5.3/Manual/packages.html#PkgNef3.

153. Guennebaud, G., Jacob, B. & others. Eigen v3 http://eigen.tuxfamily.org.
154. Schling, B. The Boost C++ Libraries 1st ed. isbn: 0982219199 (XML Press, 2011).

155. Rycroft, C. VORO++: A three-dimensional Voronoi cell library in C++. Chaos 19 (2009).
156. Neo4j. (last accessed May 19, 2022). 2021. http://neo4j.com/docs/.
157. The HDF Group. Hierarchical Data Format, version 5 http://www.hdfgroup.org/HDF5/.

33

