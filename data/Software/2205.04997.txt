2
2
0
2

y
a
M
0
1

]
E
M

.
t
a
t
s
[

1
v
7
9
9
4
0
.
5
0
2
2
:
v
i
X
r
a

Random Forests for Change Point Detection

Malte Londschien1,2, Peter Bühlmann1, Solt Kovács1

1Seminar for Statistics, ETH Zürich, Switzerland
2AI Center, ETH Zürich, Switzerland

May 2022

Abstract

We propose a novel multivariate nonparametric multiple change point detection
method using classiﬁers. We construct a classiﬁer log-likelihood ratio that uses class
probability predictions to compare diﬀerent change point conﬁgurations. We propose
a computationally feasible search method that is particularly well suited for random
forests, denoted by changeforest. However, the method can be paired with any
classiﬁer that yields class probability predictions, which we illustrate by also using a
k-nearest neighbor classiﬁer. We provide theoretical results motivating our choices.
In a large simulation study, our proposed changeforest method achieves improved em-
pirical performance compared to existing multivariate nonparametric change point
detection methods. An eﬃcient implementation of our method is made available for
R, Python, and Rust users in the changeforest software package.

Keywords break point detection, classiﬁers, multivariate time series, nonparametric

1 Introduction

Change point detection considers the localization of abrupt distributional changes in ordered
observations, often time series. We focus on oﬄine problems, retrospectively detecting changes
after all samples have been observed.
Inferring abrupt structural changes has a wide range
of applications, including bioinformatics (Olshen et al., 2004; Picard et al., 2005), neuroscience
(Kaplan et al., 2001), biochemistry (Hotz et al., 2013), climatology (Reeves et al., 2007), and
ﬁnance (Kim et al., 2005). A rich literature has developed around statistical methods that recover
change points in diﬀerent scenarios, see Truong et al. (2020) for a recent review.

Parametric change point detection methods typically assume that observations between
change points stem from a ﬁnite-dimensional family of distributions. Change points can then be
estimated by maximizing a regularized log-likelihood over various segmentations. The classical
scenario of independent univariate Gaussian variables with constant variance and changes in the
mean goes back to the 1950s (Page, 1954, 1955). It has recently been studied by, among others,
Frick et al. (2014), Fryzlewicz (2014), and references therein. Pein et al. (2017) consider a relaxed
version that allows certain changes in the variance in addition to the mean shifts. Generalizations

1

 
 
 
 
 
 
exist for multivariate scenarios. Recently, even high-dimensional scenarios have been studied:
Wang and Samworth (2018) consider multivariate Gaussian observations with sparse mean shifts,
Wang et al. (2021) study the problem of changing covariance matrices of sub-Gaussian random
vectors, Roy et al. (2017) study estimation in Markov random ﬁeld models, and Londschien et al.
(2021) consider time-varying Gaussian graphical models.

Nonparametric change point detection methods try to use measures that do not rely on
parametric forms of the distribution or the nature of change. Proposals for univariate nonpara-
metric change point detection methods include Pettitt (1979), Carlstein (1988), Dümbgen (1991),
and, more recently, Zou et al. (2014) and Madrid–Padilla et al. (2021a). Multivariate setups are
challenging even in parametric scenarios. The few multivariate nonparametric change point
detection methods we are aware of are based on ranks (Lung-Yut-Fong et al., 2015), distances
(Matteson and James, 2014; Chen and Zhang, 2015; Zhang and Chen, 2021), kernel distances
(Arlot et al., 2019; Garreau and Arlot, 2018), kernel densities (Madrid–Padilla et al., 2021b),
and relative density ratio estimation (Liu et al., 2013).

Many of these nonparametric proposals are related to hypothesis testing. For a single change
point, they maximize a test statistic that measures the dissimilarity of distributions.
In the
modern era of statistics and machine learning, many nonparametric methods are available to
learn complex conditional class probability distributions, such as random forests (Breiman, 2001)
and neural networks (McCulloch and Pitts, 1943). These have proven to often outperform simple
rank and distance-based methods. Friedman (2004) proposed to use binary classiﬁers for two-
sample testing. Recently, Lopez-Paz and Oquab (2017) applied this framework in combination
with neural networks and Hediger et al. (2022) with random forests. Similar to such two-sample
testing approaches, we use classiﬁers to construct a novel class of multivariate nonparametric
multiple change point detection methods.

1.1 Our contribution

We propose a novel classiﬁer-based nonparametric change point detection framework. Motivated
by parametric change point detection, we construct a classiﬁer log-likelihood ratio that uses class
probability predictions to compare diﬀerent change point conﬁgurations. Theoretical results for
the population case motivate the development of our algorithm.

We present a novel search method based on binary segmentation that requires a constant
number of classiﬁer ﬁts to ﬁnd a single change point. For multiple change points, the number
of classiﬁer ﬁts required scales linearly with the number of change points, making the algorithm
highly eﬃcient. Our method is implemented for random forests and k-nearest neighbor clas-
siﬁers in the changeforest software package, available for R, Python, and Rust users. The
algorithm achieves improved empirical performance compared to existing multivariate nonpara-
metric change point detection methods.

2 A nonparametric classiﬁer log-likelihood ratio

We are interested in detecting structural breaks in the probability distribution of a time se-
Rp with
ries. More formally, consider a sequence of independent random variables (Xi)n
distributions ˜P1, . . . , ˜Pn such that the map i

˜Pi is piecewise constant. Let

i=1 ⊂

7→

α0 :=

0, n
{

i : ˜Pi

} ∪ {

= ˜Pi+1}

2

6
1, . . . , α0

be the set of segment boundaries and denote with K 0 :=
1 the total number of segments.
We label the elements of α0 by their natural order starting with zero. Then consecutive elements
Pk := ˜Pαk are i.i.d. We
in α0 deﬁne segments (α0
call α0
K 091 the change points of the sequence X1, . . . , Xn. We aim to estimate the change
points (or equivalently α0) of the time series process X1, . . . , Xn upon observing a realization
x1, . . . , xn. We construct a classiﬁer log-likelihood ratio (1) and use it for change point detection.
We motivate this ratio later in Section 2.2, drawing parallels to parametric methods that we
introduce in Section 2.1.

k] for k = 1, . . . , K 0 within which the Xi

k91, α0

α0
|

| −

∼

For any segmentation α, let yα = (yi)n

∈
(αk91, αk]. Consider a classiﬁcation algorithm ˆp that can produce class probability predictions
and denote with ˆpα the classiﬁer trained on covariates X = (xi)n
i=1 and labels yα. Write ˆpα(xi)k
for the trained classiﬁer’s class k probability prediction for observation xi. The classiﬁer log-
likelihood ratio for change point detection is

i=1 be the sequence such that yi = k whenever i

G ((xi)n

i=1 |

α, ˆp) :=

K

αk

k=1
X

i=αk91+1
X

log

αk

(cid:18)

n
αk91

−

ˆpα(xi)k

.

(cid:19)

(1)

0,s,n

Consider a setup with a single change point. If a split candidate s is close to that change point,
to perform better than random at separating x1, . . . xs
we expect the trained classiﬁer ˆp
{
from xs+1, . . . xn and G ((xi)n
, ˆp) will take a positive value. We will later relate
i=1 | {
}
G ((xi)n
, ˆp) to the gain G(0,n](s) from Equation (5) used in binary segmentation,
0, s, n
}
where G(0,n](s) is the reduction in loss when splitting the segment (0, n] at s, used in binary
segmentation style algorithms. For a segmentation α with multiple change points, the classiﬁer
log-likelihood ratio (1) measures the separability of the segments (α0, α1], . . . , (αK91, αK ]. We
expect this separability to be highest for α = α0, which we formalize in Proposition 2.

i=1 | {

}
0, s, n

2.1 The parametric setting

Since ﬁrst proposals from Page (1954), parametric approaches are typically based on maximizing
a parametric log-likelihood, see also Frick et al. (2014) and references therein. For this, one
assumes that the Xi
k] and that the distributions
Pk
i=1
}
can then be parametrized with the tuple (α0, (ϑ0
k=1) such that for all k = 1, . . . , K 0 and
k the Xi are P
i = α0

∈
for some ﬁnite-dimensional parameter space Θ. The distribution of (Xi)n

Pk are i.i.d. within segments i

k91 + 1, . . . , α0

Assuming the Pϑ have densities pϑ, we can express the log-likelihood of observing some

-distributed.

k91, α0

k)K 0

(α0

∈ {

Pϑ

ϑ0
k

Θ

∼

∈

ϑ

|

sequence (xi)n

i=1 in a setup parametrized by (α, (ϑk)K

k=1) as

ℓ

(xi)n

i=1 |

(cid:0)

K

αk

α, (ϑk)K

k=1

=

log(pϑk (xi)).

k=1
X

i=αk91+1
X

(cid:1)

(2)

An estimate of α0 can be obtained by maximizing (2). Since the log-likelihood is increasing in
K =
1, an additional penalty term γ > 0 has to be subtracted whenever the number of
change points is unknown. Let

α
|

| −

ˆαγ := argmax

α

∈A

max
ϑ1,...,ϑK

ℓ

(xi)n

i=1 |

α, (ϑk)K

k=1

γ ,
α
|

− |

(3)

(cid:0)

3

(cid:1)

where
minimum length of δn for some δ > 0.

A

is a set of possible segmentations, typically restricted such that each segment has a

The most popular parametrization of in-segment distributions are Gaussian distributions
with known variance and a changing mean, see for example Yao (1988) and Fryzlewicz (2014).
Next to changes in the mean, Killick et al. (2012) also study changes in the variance, where
the mean of the Gaussian is unknown but constant over all segments. Kim and Siegmund
(1989) study changes in the parameters of a linear regression model. Shen and Zhang (2012)
and Cleynen and Lebarbier (2014) consider changes in the rate of a Poisson and changes in
the success rate in a negative binomial, with applications detecting copy number alterations in
DNA. Frick et al. (2014) consider the general case where in-segment distributions belong to an
exponential family.

2.2 The nonparametric setting

Often, a good parametric model is hard to justify and nonparametric methods can be applied
instead. Again, consider a classiﬁcation algorithm ˆp that yields class probability predictions
after training and recall that yα = (yi)n
(αk91, αk] and that
X = (xi)n
i=1. For any α, training ˆp on (X, yα) yields the function ˆpα := ˆp(X, yα) that as-
signs class k probability estimates ˆpα(x)k corresponding to segments (αk91, αk] to observations
˜Pi for the mixture distribution of Xu+1, . . . , Xv and assume that
x. Write P
densities p(u,v] exist. Assuming a uniform prior on class probabilities, the trained classiﬁer ˆpα
estimates

i=1 with yi = k whenever i

(u,v] := 1
v
−

v
i=u+1

P

∈

u

ˆpα(x)k

≈

P(Y = k

|

X = x) =

dP(X = x

Y = k)

|

dP(X = x)

P(Y = k) =

p(αk91,αk](xi)
p(0,n](xi)

αk

αk91
−
n

for k = 1, . . . K. This yields an approximation

p(αk91,αk](x)

p(0,n](x) ≈

αk

n
αk91

−

ˆpα(x)k ,

which we use in the deﬁnition of the classiﬁer log-likelihood ratio (1):

G ((xi)n

i=1 |

K

αk

α, ˆp) =

log

n
αk91

ˆpα(xi)k

≈

(cid:19)

αk

(cid:18)

K

αk

log

p(αk91,αk](xi)
p(0,n](xi)

.

(cid:19)

k=1
X

i=αk91+1
X
p(0,n](xi)
This resembles the parametric log-likelihood (2), where p(α0
which is independent of α. In Equation (1), the classiﬁcation algorithm ˆp takes the role of the
Θ in Equation (2). Analogously to (3), a nonparametric estimator for α0
parametrization ϑ
could be given by

i=αk91+1
k=1
X
X
k] = pϑ0

n
i=1 log

, up to

k91,α0

P

−

−

∈

(cid:0)

k

(cid:18)

,

(cid:1)

ˆαγ = argmax

α

∈A

G ((xi)n

i=1 |

α, ˆp)

,

γ

α
|
|

−

(4)

where

A

is a suitable set of possible segmentations.

Remark 1. To further motivate (1), assume that ˆp is a generative classiﬁer (as in linear dis-
v
criminant analysis). Write ˆϑ((u, v]) := argmaxϑ
i=u+1 log(pϑ(xi)) and for any segmentation α
let πk := αk91−
. Then the classiﬁer log-likelihood
n

, such that ˆpα(xi)k =

αk

πk p ˆϑ((αk91,αk ])(xi)
P
j=1 πj p ˆϑ((αj91,αj ])(xi)

PK

4

ratio is equal to the parametric log-likelihood ratio between the segment distributions p ˆϑ((αk91,αk])
and the mixture model

K
)j:
j=1 πj p ˆϑ((αj91,αj ])(
·

G ((xi)n

i=1 |

α, ˆp) =

P
K

αk

Xk=1

i=αk91+1
X

(xi)n

i=1 |

= ℓ

(cid:16)

log



πj

p ˆϑ((αk91,αk])(xi)
K
j=1 p ˆϑ((αj91,αj ])(xi) 

log

k=1)

n



P
(α, ( ˆϑ((αk91, αk]))K

−

(cid:17)

n=1
X

K

Xk=1

πk p ˆϑ((αk91,αk])(xi)

!

.

The following proposition suggests that for any estimator approximating the Bayes classiﬁer,

minimizing G over segmentations α yields a reasonable estimator for α0.

Proposition 2. Let

p∞α (x) := P (Y = k

X = x) =

|

αk

αk91 −
n

(cid:18)

p(αk91,αk]
p(0,n] (cid:19)

α

|−

|

1

k=1

be the inﬁnite sample Bayes classiﬁer corresponding to the segmentation α.
segmentations containing α0 and DKL(P
respect to P , then

is a set of
Q) is the Kullback-Leibler divergence of Q with

A

If

k

max
α
∈A

E [G ((Xi)n

i=1 |

α, p∞)] = E

G

(Xi)n

i=1 |

α0, p∞

(cid:2)
K

=

(cid:0)
(α0
k −

(cid:1)(cid:3)
k91)DKL(Pk
α0

k

P

(0,n])

Xk=1

and any maximizer of E [G ((Xi)n
particular, the maximizer α with the smallest cardinality

α, p∞)] is equal to α0 or is an oversegmentation of α0. In
is equal to α0.

i=1 |

α
|
|

3 The changeforest algorithm

Finding a solution to the nonparametric estimator (4) by evaluating the classiﬁer log-likelihood
is infeasible. We present an eﬃcient
ratio (1) for all possible candidate segmentations α
search procedure based on binary segmentation that ﬁnds an approximate solution to (4). We
furthermore motivate the use of random forests as classiﬁers and present a model selection proce-
dure adapted to our search procedure. These building blocks combine to form the changeforest
algorithm, which we summarize in Section 3.5.

∈ A

3.1 Binary segmentation

Binary segmentation (Vostrikova, 1981) is a popular greedy algorithm to obtain an approximate
solution to the parametric maximum log-likelihood estimator (3). For ˆϑ((u, v]) := argmaxϑ
v
i=u+1 log(pϑ(xi)), binary segmentation recursively splits segments at the split s maximizing

the gain
P

s

G(u,v](s) :=

log

i=u+1
X

p ˆϑ((u,s])(xi)
p ˆϑ((u,v])(xi) !

+

v

log

i=s+1
X

p ˆϑ((s,v])(xi)
p ˆϑ((u,v])(xi) !

,

(5)

5

 
 
 
the increase in log-likelihood, a parametric log-likelihood ratio, until a stopping criterion is met.
For changes in the mean, where ˆϑ((u, v]) = 1
ϑ)2,
v
−

−
v
the normalized gain 2
, the square
s
v
−
of the CUSUM statistic, ﬁrst presented by Page (1954). Binary segmentation typically requires
(n log(n)) evaluations of the gain and is typically faster than search methods based on dynamic

v
i=u+1 xi and log(pϑ(x)) = 1

u G(u,v](s) is equal to

s
i=u+1 xi

v
i=s+1 xi

√2π −
2

s
P
−
u
−

1
2 (x

s
−
v
−

(cid:16)q

P

P

q

−

u
s

(cid:17)

u

O
programming such as PELT (Killick et al., 2012).

We replace the parametric gain G(u,v](s) in binary segmentation with the nonparametric

classiﬁer log-likelihood ratio from (1)

(xi)s

i=u+1 | {

u, s, v

G

(cid:0)

s

, ˆp
}

=

(cid:1)

i=u+1
X
s

≈

i=u+1
X

log

log

(cid:18)

(cid:18)

v
s

u
u

−
−

ˆp
{

u,s,v

}

p(u,s](xi)
p(u,v](xi)

v

(xi)1

+

log

(cid:19)

v

i=s+1
X

(cid:18)
p(s,v](xi)
p(u,v](xi)

(cid:19)

+

log

(cid:19)

i=s+1
X

(cid:18)

v
v

ˆp
{

u,s,v

}

(xi)2

(6)

(cid:19)

u
s

−
−

.

In many parametric settings, the expected gain curve G(u,v] will be piecewise convex between
the underlying change points (Kovács et al., 2020c). Proposition 3 shows that the same holds for
the nonparametric variant (6) in the population case. Figure 1 shows the nonparametric gains
for a random forest and a k-nearest neighbor classiﬁer applied to simulated data with two change
points.

Proposition 3. For the Bayes classiﬁer p∞, the expected classiﬁer log-likelihood ratio

i=u+1 | {
is piecewise convex between the underlying change points α0, with strict convexity in the segment
= P
(α0

= P

7→

(cid:1)(cid:3)

(cid:0)

k91, α0

k] if P

(cid:2)
k] or P

(α0

k,v].

(α0

k91,α0

(α0

k91,α0

k] 6

(u,α0

k91] 6

u, s, v

, p∞
}

s

E

G

(Xi)v

20

0

i

n
a
g

−20

−40

40

20

0

100

200

300

400

500

600

0

100

200

300

400

500

600

split

split

Figure 1: Classiﬁer-based gain curves, using a random forest (left) and a k-nearest neighbor
classiﬁer (right). The time series with 600 observations of dimension 5 and change points at
t = 200, 400 (green crosses) was simulated with the challenging change in covariance setup
described in Section 4.2. The maximizer is marked in red.

3.2 The two-step search algorithm

Binary segmentation relies on the full grid search, where G(u,v](s) is evaluated for all s = u + 1, . . . , v,
to ﬁnd the maximizer of the gain G(u,v](s).
change in mean, log-likelihoods of neighboring segments can be recovered using cheap
dates, enabling change point detection with binary segmentation in typically

In many traditional parametric settings, such as
(1) up-
O
(n log(n)) time.

O

6

For many classiﬁers, random forests in particular, such updates are unavailable and the clas-
siﬁers need to be recomputed from scratch, making the computational cost of the grid search
in binary segmentation prohibitive. Similar computational issues arise in high-dimensional re-
gression models. Here, Kaul et al. (2019) propose a two-step approach. Starting with an initial
guess s(0), they ﬁt a single high-dimensional linear regression for each of the segments (u, s(0)]
and (s(0), v] and generate an improved estimate of the optimal split using the resulting residuals.
They repeat the procedure two times and show a result about consistency in the high-dimensional
regression setting with a single change point if the change point is suﬃciently close to the ﬁrst
guess s(0).

We apply a variant of the two-step search paired with the classiﬁer log-likelihood ratio for
multiple change point scenarios. Instead of residuals, we recycle the class probability predictions
and the resulting classiﬁer log-likelihood ratios for individual observations from a single classiﬁer
ﬁt ˆp
u,s(0),v
{

, approximating

}

(xi)v

i=u+1 | {

u, s, v

G

(cid:0)

s

log

i=u+1
X

(cid:18)

, ˆp
}

≈

(cid:1)

v
s(0)

u
u

−
−

v

ˆp
u,s(0),v
{

}

(xi)1

+

log

(cid:19)

i=s+1
X

v
v

u
−
s(0)
−

(cid:18)

ˆp
u,s(0),v
{

}

(xi)2

.

(cid:19)

We call this the approximate gain in the following. Note that the classiﬁer and normalization
factors are ﬁxed, but the summation varies with the split s. Like Kaul et al. (2019), we compute
this twice, using the ﬁrst maximizer of the approximate gain as a second guess s(1). This allows
us to ﬁnd local maxima of the nonparametric gain (6) in a constant number of classiﬁer ﬁts.

Kaul et al. (2019) only apply the two-step search in scenarios with a single change point. In
scenarios with multiple change points, as encountered in binary segmentation, the class probabil-
ity predictions from the initial classiﬁer ﬁt might contain little information if P
(s(0),v].
To avoid this, we start with multiple initial guesses and select the split point corresponding to
the overall highest approximate gain as the second guess. The resulting two-step algorithm, as
implemented in changeforest, using three initial guesses at the segment’s 1
4 -quantiles,
is presented in Algorithm 2. Proposition 4 suggests good estimation performance when coupling
the two-step search with our classiﬁer-based approach in a single change point scenario.

2 and 3

(u,s(0)] ≈

4 , 1

P

Proposition 4. For the Bayes classiﬁer p∞ and any initial guess u < s(0) < v, the expected
approximate gain

s

E

s

7→

log

(cid:18)

"

i=u+1
X

v
s(0)

u
u

−
−

p∞
u,s(0),v
{

}

v

(Xi)1

+

(cid:19)

i=s+1
X

log

v
v

u
−
s(0)
−

(cid:18)

p∞
u,s(0),v
{

}

(Xi)2

(cid:19)#

is piecewise linear between the underlying change points α0. If there is a single change point
a0

(u, v], the expected approximate gain has a unique maximum at s = a0.

∈
Figure 2 shows approximate gain curves and the underlying class probability predictions for
a simulated dataset with two change points. The approximate gain curves have a piecewise
linear shape with kinks at the underlying change points, as predicted by Proposition 4. One
can also observe how the distributions of the probability predictions change at the true change
points. For the second row, the choice of the initial guess s(0) = 300 is precisely such that
(0,300] = P
P
(300,600], and the approximate gain curve is ﬂat. Figure 4 shows approximate gain
curves in diﬀerent stages of binary segmentation for a diﬀerent simulated time series. Again, the
approximate gain curves are roughly piecewise linear with kinks at the underlying change points.

7

i

n
a
g

.
x
o
r
p
p
a

i

n
a
g

.
x
o
r
p
p
a

i

n
a
g

.
x
o
r
p
p
a

i

n
a
g

.
x
o
r
p
p
a

0

−50

−100

0

−50

−100

0

−50

−100

0

−50

−100

s
n
o
i
t
c
i
d
e
r
p

.
a
b
o
r
p

s
n
o
i
t
c
i
d
e
r
p

.
a
b
o
r
p

s
n
o
i
t
c
i
d
e
r
p

.
a
b
o
r
p

s
n
o
i
t
c
i
d
e
r
p

.
a
b
o
r
p

1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

0

100

200

300

400

500

600

0

100

200

300

400

500

600

split

time

Figure 2: Approximate gain curves (left) from single random forest ﬁts for initial guesses s(0) =
150 (top), 300 (second row), and 450 (third row) and the underlying out-of-bag class probability
predictions (right). The last row shows the approximate gain curve for the second guess ˆs(1) =
400. The underlying data is the same as in Figure 1, with true change points at t = 200, 400
(green crosses). The initial guess and the curve’s maximizer are marked in blue and red vertical
dashed lines.

3.3 Choice of classiﬁer

We recommend pairing our classiﬁer-based method with random forests (Breiman, 2001). We
motivate this choice.

A classiﬁer should satisfy three properties when paired with our change point detection
methodology to enjoy good estimation performance and computational feasibility: (i) It needs to
have a low dependence on hyperparameter tuning, (ii) it needs to be able to generate unbiased
class probability predictions, and (iii) the training cost should be reasonable even for a large
number of samples. The hyperparameters optimal for classiﬁcation can vary between segments
encountered in binary segmentation. Retuning them, as is done by Londschien et al. (2021),
is infeasible. Thus, the classiﬁer needs to provide suitable class probability predictions even
for suboptimal hyperparameters. If the class probability predictions have a strong bias due to
overﬁtting, the approximate gain curve will have a kink at the initial guess, see Figure 3. For
a weak underlying signal, the initial guess might even be the maximizer of the curve, causing
the two-step algorithm to fail in ﬁnding a true change point. As for computational cost: Our
optimization algorithm based on binary segmentation and the two-step search massively reduces

8

 
 
 
 
 
 
 
 
the number of classiﬁer ﬁts required for change point estimation. Still, hundreds of ﬁts might
be necessary. For this not to become restrictive, the classiﬁer’s training cost should not explode
when the number of samples gets large.

Random forests are a perfect ﬁt: They are known to be an oﬀ-the-shelf classiﬁer that ﬁts most
data without much hyperparameter tuning, they can generate unbiased probability estimates
through out-of-bag prediction, and their training complexity is nearly linear in the sample size
for a ﬁxed maximal depth.

We highlight another attractive attribute of random forests: They are applicable to a wide
range of types of data. The need for nonparametric change point detection arises primarily when
little is known about the data to be analyzed. Else, a smartly chosen parametric method might
be better suited. In such situations, classiﬁers with broad applicability are optimal. As a tree-
based method, random forests are invariant to feature rescaling and robust to the inclusion of
irrelevant features. Due to these properties, random forests are popular as black box or oﬀ-the-
shelf learning algorithms and thus well suited for many datasets encountered in nonparametric
change point detection.

While random forests are our classiﬁer of choice, we believe that our method, including the
two-step search, can be paired with any classiﬁcation algorithm that can produce unbiased class
probability predictions. As an illustration, we pair our approach with the Euclidean norm k-
nearest neighbor classiﬁer. It has a single hyperparameter k, which we set to the square root
of the segment length. Furthermore, the algorithm can be adapted to recover leave-one-out
cross-validated probability estimates. Our implementation has a computational complexity of
(n2) to compute all pairwise distances once, which we can recycle for all segments. More
O
eﬃcient implementations exist (Arya et al., 1998). However, the quadratic runtime and memory
requirements are not an issue for the reasonably sized datasets we use for benchmarking.

50

0

i

n
a
g

.
x
o
r
p
p
a

−50

s
n
o
i
t
c
i
d
e
r
p

.
a
b
o
r
p

1.0

0.5

0.0

0

100

200

300

400

500

600

0

100

200

300

400

500

600

split

t

Figure 3: In-sample (not out-of-bag) class probability predictions of a random forest (right) and
the corresponding approximate gain curve (left). Predictions are from a single random forest ﬁt
for the initial guess s(0) = 450 (blue line). The underlying data is the same as in Figures 1 and
2, with true change points at t = 200, 400 (green crosses). The maximizer is marked in red. The
predictions show a clear bias, dominating the underlying signal (kink around the true change
point at t = 400), and the approximate gain curve has a peak at the initial guess. Compare this
to the third row of Figure 2, where out-of-bag class probability predictions were used, resulting
in a maximum around a true change point at t = 400.

3.4 Model selection

So far, we have discussed how to accurately and eﬃciently estimate the location of change points
in a signal. We now discuss how to decide whether to keep or discard a candidate split.

9

 
 
Separating true change points from false ones is crucial for good estimation performance and is
a task as complex as ﬁnding candidate splits. Two approaches for model selection are particularly
popular in change point detection: thresholding and permutation tests. For thresholding, a
candidate split is kept if its inclusion results in an increase of the log-likelihood at least as large
as the threshold γ. Thresholding is often applied in parametric settings, where BIC-type criteria
for the minimal gain to split can be derived with asymptotic theory, see for example Yao (1988).
Permutation tests are an alternative to thresholding when such asymptotic theory is unavailable.
Here, a statistic gathered for a candidate split is compared to values of the statistic gathered
after the underlying data was permuted, see for example Matteson and James (2014).

There is no asymptotic theory available to suggest a threshold for the (approximate) non-
parametric classiﬁer log-likelihood ratio presented in Section 2. Meanwhile, permutation tests
require a high number of classiﬁer ﬁts in each splitting step, with a severe eﬀect on the overall
computational cost. We apply a leaner pseudo-permutation test. Instead of permuting the ob-
servations and reﬁtting the classiﬁer, we only permute the classiﬁer’s predictions gathered in the
ﬁrst step of the two-step search, and thus the classiﬁer log-likelihood ratios. We then compute
the approximate gains after shuﬄing and take their maxima. In the absence of change points,
there is no underlying signal for the classiﬁer to learn other than the relative class sizes. We
thus expect the distribution of the classiﬁer log-likelihood ratios to be approximately invariant
under permutations. This allows us to compare the maximal gain obtained in the ﬁrst step of
the two-step search to the maximal gains computed after permuting. Algorithm 3 summarizes
the procedure.

The signiﬁcance threshold for the pseudo-permutation test is a tuning parameter rather than
a valid signiﬁcance level. We choose 0.02 by default. This will typically lead to many false
positives due to multiple testing, as the permutation test is applied to each segment encountered
in binary segmentation. We use 199 permutations. We analyze the empirical false positive rate
of the pseudo-permutation test in Section 4.7.

3.5 Our proposal

We present changeforest in Algorithm 1.

}

u,s,v

u
1
−
1
{i6s}

= v
−
u
s
−
−

instead of v
s

u
u . We introduce logη(x) := log((1
−
−

We comment on two details: We use logη(x) instead of the natural logarithm and rescale prob-
η)x + η)
abilities with 1 / π{
i
to use instead of log(x) in the deﬁnition of the classiﬁer log-likelihood ratio. Class probabil-
ity predictions can take values 0 and 1, prohibiting us from using the logarithm directly to
compute individual classiﬁer log-likelihood ratios. With η = exp(
6), we eﬀectively cap individ-
ual classiﬁer log-likelihood ratios from below by
6. Concerning the rescaling: the out-of-bag
probability predictions of a random forest behave like those from leave-one-out cross-validation.
The prediction for observation xi was generated by a forest that was trained on the v
1
.
observations (xj)j
1 of these observations belong to the
u+1,...,v
u do. Thus, in the absence of any change points, we expect
ﬁrst class, and if i > s, s
s
u,s,v
before
ˆp
{
taking the logarithm to recover the classiﬁer log-likelihood ratio estimates.

and thus scale predictions by the inverse of π{
i

i
}\{
}
−
1 = π{
i

If i 6 s, then s

u
−
v

−
u
−

(xi)

∈{
1

u,s,v

u,s,v

{i6s}

−

−

−

−

−

−

−

≈

u

u

−

}

}

}

Our algorithm is visualized in Figure 4, where we display the approximate gain curves of the

two-step search for segments encountered in binary segmentation.

10

Algorithm 1 changeforest
Input: Observations (xi)n
Output: Change point estimate ˆα

i=1, a classiﬁer ˆp, and a minimum relative segment length δ > 0.

BinarySegmentation((xi)n

i=1, ˆp, δ).

←

function BinarySegmentation((xi)v
u < 2δn then

if v

i=u+1, ˆp, δ)

−
return ∅

end if

ˆs, ((ℓi,k,j)k=1,2
q = ModelSelection(((ℓi,k,j)k=1,2

i=u+1,...,v)j=1,2,3 ←

TwoStepSearch((xi)v
i=u+1,...,v)j=1,2,3, δ)

i=u+1, ˆp, δ)

⊲ c.f. Algorithm 2

⊲ c.f. Algorithm 3

if q 6 0.02 then
ˆαleft ←
ˆαright ←
return ˆαleft ∪ {
ˆs
} ∪
return ∅

else

BinarySegmentation((xi)ˆs
BinarySegmentation((xi)v
ˆαright

i=u+1, ˆp, δ)
i=ˆs+1, ˆp, δ)

end if

end function

3.6 The changeforest software package

A substantial part of this work is the changeforest software package, implementing Algorithm 1
for random forests and k-nearest neighbor classiﬁers. It also implements the parametric change
in mean setup, paired with binary segmentation, which we use as a baseline in the simulations.

The changeforest package is available for Python users on PyPI and conda-forge (conda-forge community,

2015), for R users on conda-forge, and for Rust users on crates.io. Its backend is implemented
in the system programming language Rust (Matsakis and Klock, 2014). For inquiries, installation
instructions, a tutorial, and more information, please visit github.com/mlondschien/changeforest.

11

Algorithm 2 TwoStepSearch
Input: Observations (xi)v
Output:
((ℓi,k,j)k=1,2

A single

i=u+1,...,v)j=1,2,3 for model selection.

i=u+1, a classiﬁer ˆp, and a minimum relative segment length δ > 0.
log-likelihood ratios
change point

ˆs and classiﬁer

estimate

Step 1: Deﬁne (s(0)
η = exp(

1 , s(0)

2 , s(0)

3 ) = (
⌊
u
s
6) and set π{
−
−
}
i
u
v
−
−
i=u+1, (1 + 1
) = ((xi)v
i>s(0)
j }
{

,
4 ⌋
{i6s}
1

−
(X, y
u,s(0)
j
{

3u+v

u,s,v

←

,v

1

}

v+u
,
2 ⌋

u+3v

4 ⌋

) and logη(x) = log((1

η)x + η) for

⌊

⌊
. For j = 1, 2, 3, train binary classiﬁers ˆp
u,s(0)
j
{
)v
i=u+1) and let

−

on

,v

}

ℓi,1,j = logη

ˆp
u,s(0)
j
{

,v

}

(cid:18)

u,s(0)
j

(xi)1/π{
i

,v

}

(cid:19)

, ℓi,2,j = logη

ˆp
u,s(0)
j
{

,v

}

(cid:18)

(xi)2/(1

−

u,s(0)
j

π{
i

,v

}

.

)
(cid:19)

Set

ˆs(1) =

argmax
,...,v
δn
⌉
⌈
Step 2: Train a binary classiﬁer ˆp
u,ˆs(1),v
{
u,s(1),v

s=u+1+

−⌈

δn

}

ℓi,1 = logη

ˆp
u,s(1),v
{

}

(xi)1/π{
i

Set

(cid:16)

s

v

ℓi,1,j +

ℓi,2,j

i=u+1
X

i=s+1
X

max
j=1,2,3

⌉
and let

, ℓi,2 = logη

ˆp
u,s(1),v
{

(cid:16)

(xi)2/(1

}

−

u,s(1),v

π{
i

}

)

.

(cid:17)

}

(cid:17)

ˆs = ˆs(2) =

s=u+1+

argmax
,...,v
δn
⌉
⌈

s

v

ℓi,1 +

ℓi,2.

δn

−⌈

⌉

i=u+1
X

i=s+1
X

Return:

ˆs, ((ℓi,k,g)k=1,2
(cid:16)

i=u+1,...,v)j=1,2,3

(cid:17)

Algorithm 3 ModelSelection

Input: Classiﬁer log-likelihood ratios ((ℓi,k,j)k=1,2
minimum relative segment length δ > 0.
Output: An approximate p-value from the pseudo-permutation test.

i=u+1,...,v)j=1,2,3 from the two-step search and a

Step 1: Recover the maximal gain encountered in the ﬁrst step of the two-step search.

G0 =

s=u+

max
+1,...,v
⌉

δn
⌈

δn

−⌈

max
j=1,2,3

s

v

li,1,j +

li,2,j.

Step 2: For random permutations σl :
compute maximal gains after permuting likelihoods

⌉

i=u+1
X
u + 1, . . . , v
{

} → {

i=s+1
X
u + 1, . . . , v

for l = 1 . . . , 199,

}

Gl = max
j=1,2,3

s=u+

max
+1,...,v
⌉

δn
⌈

δn

−⌈

s

v

lσl(i),1,j +

lσl(i),2,j.

⌉

i=u+1
X

i=s+1
X

Return: #

l = 0, . . . , 199
{

Gl > G0}

|

/ 200

12

i

n
a
g

.
x
o
r
p
p
a

i

n
a
g

.
x
o
r
p
p
a

i

n
a
g

.
x
o
r
p
p
a

i

n
a
g

.
x
o
r
p
p
a

i

n
a
g

.
x
o
r
p
p
a

i

n
a
g

.
x
o
r
p
p
a

0

−500

−1000

0

−1000

−2000

0

−1000

−2000

0

−1000

0

−200

0

−200

−400

0

500

1000

1500

2000

2500

3000

3500

4000

split

Figure 4: Approximate gain curves from the last step of the two-step search in the diﬀer-
ent steps of binary segmentation as encountered in changeforest. The underlying dataset
was simulated with the abalone setup, see Section 4.2 for details. The true change points
at t = 568, 635, 1122, 1225, 1914, 2305, 2508, 2775, 2817, 2875, 3134, 3249, 3306, 3432 are marked
with green crosses. Change point estimates, corresponding to maxima of the approximate
gain curves that are accepted in our model selection procedure, are marked in red. The
second guess ˆs(1) for the two-step search is marked in blue. Current segment boundaries
are marked with black vertical lines. Here, changeforest estimates change points at t =
568, 636, 1122, 1229, 1917, 2305, 2444, 2798, 2875, 3132, 3245, 3306, 3425, corresponding to an ad-
justed Rand index of 0.97.

13

 
 
 
 
 
 
4 Empirical results

We present results from a simulation study comparing the empirical performance of our classiﬁer-
based method changeforest to available multivariate nonparametric competitors. The source
code for the simulations, together with guidance on reproducing tables and ﬁgures, is available at
github.com/mlondschien/changeforest-simulations. The repository can easily be expanded
to benchmark new methods provided by users.

4.1 Competing methods

We are aware of the following nonparametric methods for multivariate multiple change point
detection with existing reasonably eﬃcient implementations: Matteson and James’ (2014) e-
divisive (ECP), Lung-Yut-Fong et al.’s (2015) MultiRank, and kernel change point methods such
as Arlot et al.’s (2019) KCP.

ECP searches for a single change point minimizing energy distances within segments. The
method generalizes to multiple change point scenarios through binary segmentation, and the
signiﬁcance for change points is assessed with a permutation test. An implementation of ECP is
available in the R-package ecp (James and Matteson, 2015), which we use in the simulations.

MultiRank uses a rank-based multivariate homogeneity statistic combined with dynamic
programming. The signiﬁcance of change points is assessed based on asymptotic theory. We
used code made available to us by the authors to run simulations with MultiRank, which is
included in the simulations repository.

Kernel change point methods minimize a within-segment average kernelized dissimilarity
measure. Arlot et al. (2019) propose KCP, an algorithm based on dynamic programming. An
eﬃcient implementation can be found in the Python package ruptures (Truong et al., 2020).
We used the slope heuristic as proposed by Arlot et al. (2019) for model selection.

For all packages, we used default parameters, if available. For ECP, this results in α = 1
and using 199 permutations at signiﬁcance level 0.05. For KCP, we use a Gaussian kernel with
a bandwidth of 0.1, optimal for the simulated scenarios. See Section 4.6 and Table 9 for details.
We supplied information about the minimum relative segment length δ, equal to 0.01 in the main
simulations, to each method, either by informing about the minimum number of observations
or by informing about the maximal number of change points
per segment
Other multivariate nonparametric multiple change point detection methods that we did not
include in our simulations include Liu et al.’s (2013) RuLSIF, Cabrieto et al.’s (2017) Decon,
Madrid–Padilla et al.’s (2021b) MNWBS, and Zhang and Chen’s (2021) gMulti. For gMulti, no
implementation has been made available by the time of writing. Implementations for MNWBS
and Decon exist. However, these ran for over one hour for only medium-sized datasets with 5000
observations. The implementation for RuLSIF is only available in Matlab and does not include
an automatic model selection procedure.

1/δ
⌊

δn
⌈

.
⌋

⌉

4.2 Simulation setups

We evaluate all methods on three parametric and six nonparametric scenarios. We display their
key characteristics below the header in Table 2.

As for parametric setups, we use the change in mean (CIM) and change in covariance (CIC)
setups from (Matteson and James, 2014, Section 4.3) and the Dirichlet setup from (Arlot et al.,
2019, Section 6.1). For both works, the authors present empirical results of their methods on

14

datasets for three sets of parameters. We use the set of parameters corresponding to medium
diﬃculty.

For the change in mean and change in covariance setups, we generate time series of dimension
d = 5 with n = 600 observations and change points at t = 200, 400. Observations in the ﬁrst
and last segment are independently drawn from a standard multivariate Gaussian distribution.
In the change in mean setup, entries in the second segment are i.i.d. Gaussian with a mean shift
of µ = 2 in each coordinate. In the change in covariance setup, entries in the second segment
are i.i.d. normal with mean zero and unit variance, but with a covariance of ρ = 0.7 between
coordinates. Lastly, the Dirichlet dataset consists of n = 1000 observations and has dimension
d = 20. Ten change points are located at t = 100, 130, 220, 320, 370, 520, 620, 740, 790, 870. In
each segment, the observations are distributed according to the Dirichlet distribution, with the
d parameters drawn i.i.d. uniformly from [0, 0.2].

We also generate time series with nonparametric in-segment distributions. For this, we use
popular multiclass classiﬁcation datasets and treat observations corresponding to a single class
as i.i.d. draws from a class-speciﬁc distribution. We discard classes with fewer than δn = n
100
observations and shuﬄe and randomly concatenate the remaining classes. Categorical variables
are dummy encoded. Finally, each covariate gets normalized by the median absolute deviation
of absolute consecutive diﬀerences, as is also used by Fryzlewicz (2014) and further discussed
by Kovács et al. (2020a). This normalization does not aﬀect our random forest-based method
changeforest and rank-based MultiRank but increases performance for distance-based competi-
tors ECP and KCP.

We use the following datasets: The iris ﬂower dataset (Fisher, 1936) contains 150 sam-
ples of four measurements from three species of the iris ﬂower. The glass identiﬁcation dataset
(Evett and Spiehler, 1989), initially motivated by criminological investigation, contains measure-
ments of chemical compounds for diﬀerent types of glass. The wine dataset (Cortez et al., 2009)
is the result of physicochemical analysis of both red and white vinho verde from north-western
Portugal. It includes quality scores from blind testing, which we use as class labels. We encode
the wine color as binary. The Wisconsin breast cancer dataset (Street et al., 1993) contains nom-
inal characteristics of cell nuclei computed from a ﬁne needle aspirate of breast tissue labeled as
malignant or benign. The resulting simulated time series has a single change point. The abalone
dataset (Waugh, 1995) contains easily obtainable physical measurements of snails and their age,
determined by cutting through the shell cone and counting the number of rings. Two categorical
variables are dummy encoded. We use the number of rings as class labels. The dry beans dataset
(Koklu and Ozkan, 2020) contains shape measurements derived from images of seven diﬀerent
types of dry beans.

The change in mean and change in covariance and possibly also the abalone and wine datasets
feature similar or equal distributions in non-neighboring segments. We expect these scenarios to
be challenging for our methodology, see Section 3.2 and Figure 2.

4.3 Performance measures

We mainly use the adjusted Rand index (Hubert and Arabie, 1985), a standard measure to
compare clusterings, to evaluate the goodness of ﬁt of estimated segmentations. Given two
partitionings of n observations, the Rand index (Rand, 1971) is the number of agreements, pairs
of observations either in the same or in diﬀerent subsets for both partitionings, divided by the
. The adjusted Rand index is the normalized diﬀerence between the
total number of pairs

n
2

(cid:0)

(cid:1)

15

Rand index and its expectation when choosing partitions randomly. Consequently, the adjusted
Rand index can take negative values but no values greater than one.
Its expected value is
zero for random class assignments, but the expected value for random segmentations, namely
class assignments consistent with the time structure, is signiﬁcantly higher. We present a set of
segmentations together with their adjusted Rand indices in Table 1.

true segmentation
0, 17, 46, 55, 68, 144, 214

estimated segmentation
0, 17, 46, 55, 68, 144, 214
0, 15, 45, 55, 68, 142, 214
0, 46, 55, 68, 144, 214
0, 17, 46, 55, 144, 214
0, 17, 46, 55, 68, 80, 144, 214
0, 17, 46, 55, 68, 100, 144, 214
0, 50, 100, 150, 214
0, 214

ARI
1.00
0.95
0.95
0.89
0.91
0.83
0.61
0.00

dH comment
perfect ﬁt
almost perfect ﬁt

0.000
0.009
0.079 undersegmentation
0.061 undersegmentation
0.056
0.150
0.150
0.327 no segmentation

oversegmentation
oversegmentation
random segmentation

Table 1: Adjusted Rand indices (ARI) and Hausdorﬀ distances (dH ) for diﬀerent segmentations.
The true segmentation comes from the nonparametric glass setup.

We believe that an adjusted Rand index above 0.95 corresponds to a segmentation that is
close to perfect. An index of 0.9 corresponds to a good segmentation, possibly oversegmenting
with a false positive close to a true change point or missing one of two close change points. On
the other hand, we expect a segmentation with an adjusted Rand index signiﬁcantly below 0.8
not to provide much scientiﬁc value.

We additionally use the Hausdorﬀ distance to evaluate change point estimates. For two
segmentations α, α′, set d(α, α′) := 1
. Then d(α0, ˆα) is the largest
α mina′
n maxa
∈
relative distance of a true change point in α0 to its closest counterpart in ˆα, and d(ˆα, α0) is the
largest relative distance of a found change point in ˆα to the closest entry in α0. Thus d(α0, ˆα)
especially penalizes undersegmentation while d(ˆα, α) penalizes oversegmentation. Deﬁne the
Hausdorﬀ distance as dH (α, α′) := max(d(α, α′), d(α′, α)). Table 1 also includes the Hausdorﬀ
distance for the selected segmentations.

a′|

a
|

−

α′

∈

4.4 Main simulation results

We present the results of our main simulation study in Table 2, where we display average adjusted
Rand indices over 500 simulations.

Our method, changeforest, scores above 0.9 on average for each simulation scenario studied.
All other methods have an average score lower than 0.75 for at least one scenario. changeforest
is the best performing method for four setups: Dirichlet, glass, abalone, and wine. Further-
more, in four further setups, change in mean, iris, breast cancer, and dry beans, our method
changeforest scores above 0.975 on average, corresponding to an almost perfect segmentation.
The only setup for which changeforest does not perform best or above 0.975 is change in co-
variance. Here, changeforest scores second-best behind KCP with an average adjusted Rand
index of 0.925, and no other method scores above 0.5. We compute the average score for each
method to combine the results for the nine simulation scenarios. changeforest scores high-
est. Overall, changeforest is widely applicable, with robust performance on all parametric and
nonparametric setups.

16

CIM

CIC

Dirichlet

iris

glass

n, d, K

600, 5, 3

600, 5, 3

1000, 20, 11

150, 4, 3

214, 8, 6

change in mean
changeforest
changekNN
ECP
KCP
MultiRank

0.490 (0.171)
0.935 (0.082)
0.000 (0.000)
0.000 (0.000)
0.999 (0.003)
0.983 (0.040) 0.923 (0.072)
0.925 (0.122) 0.986 (0.019)
0.987 (0.035)
0.879 (0.089)
0.990 (0.031)
0.688 (0.245)
0.011 (0.077)
0.993 (0.023)
0.608 (0.243)
0.836 (0.080)
0.994 (0.025)
0.991 (0.031)
0.430 (0.317)
0.870 (0.078) 0.994 (0.023)
0.998 (0.012) 0.955 (0.057)
0.744 (0.234)
0.598 (0.093)
0.569 (0.256)
0.966 (0.025)
0.003 (0.036)

1.000 (0.001)

breast cancer

abalone

wine

dry beans

average worst

n, d, K

699, 9, 2

4066, 9, 15

6462, 12, 5

13611, 16, 7

change in mean
changeforest
changekNN
ECP
KCP
MultiRank

0.791 (0.027)
0.983 (0.018)
0.885 (0.055)
0.991 (0.014)
0.000
0.997 (0.010) 0.968 (0.061) 0.923
0.983 (0.074) 0.932 (0.049) 0.993 (0.023)
0.011
0.996 (0.011)
0.972 (0.040)
0.880 (0.097)
0.905 (0.166)
0.982 (0.032) 1.000 (0.002)
0.430
0.865 (0.064)
0.995 (0.021)
0.988 (0.011) 1.000 (0.003)
0.996 (0.033)
0.744
0.916 (0.043)
0.003
0.957 (0.032)
0.813 (0.115)
0.923 (0.119)

0.813 (0.113)
0.854 (0.136)
0.940 (0.087)
0.729 (0.114)

0.675 (0.067)

Table 2: Average adjusted Rand indices over 500 simulations, with standard deviations in
parentheses. Optimal scores are marked in bold. The MultiRank method was unable to produce
results for the dry beans simulation setup. Average and worst denote the average and worst
performances across all setups.

We include Table 5 with median Hausdorﬀ distances in the Appendix. The results are
similar, with changeforest being among the methods with the best score, except for change in
covariance, where it achieves a median Hausdorﬀ distance of 0.017, behind KCP.

Table 6 in the Appendix shows the average number of estimated change points for each
method and simulation setup. changeforest slightly oversegments for most setups, but not for
the glass and wine setups.

Figure 7 in the Appendix shows histograms of change point estimates for the Dirichlet setup,
where the underlying change points are constant. Corresponding to high adjusted Rand indices
of 0.986 and 0.966, change point estimates by changeforest and MultiRank are sharply concen-
trated around the true change point locations. Meanwhile, estimates by changekNN, ECP, and
KCP are visually more spread out, corresponding to worse adjusted Rand indices smaller than
0.9. Lastly, the change in mean method fails to produce any change point estimates.

We present average computation times for each method in Table 3. Simulations were run on
eight Intel Xeon 2.3 GHz cores with 4 GB of RAM available per core (32 GB in total). The simple
change in mean method is blazingly fast. On the largest simulation setup dry beans with n =
13611 and d = 16, the computation time was around 0.002 seconds on average. changeforest
is the fastest nonparametric method on the dry beans dataset, requiring around 2.6 seconds on
average. On the other hand, ECP is prohibitively slow, requiring more than one hour to estimate
change points on the dry beans dataset on average. The code for MultiRank is experimental
and broke for the dry beans dataset due to the multiple dummy encoded columns. We provide
further analysis of the computational eﬃciency of available implementations in Section 4.5.

17

CIM

CIC Dirichlet

iris

glass

breast
cancer

abalone

wine

dry
beans

n, d

600, 5

600, 5

1000, 20

150, 4

214, 8

699, 9

4066, 9

6462, 12

13611, 16

change in mean <0.01 <0.01
0.09
changeforest
0.04
changekNN
2.9
ECP
0.08
KCP
0.76
MultiRank

<0.01
0.89
0.43
0.03
4.7
0.15 <0.01
741
0.37
6.6
0.02
90
0.10
Table 3: Average computation times in seconds on 8 Intel Xeon 2.3 GHz cores.

<0.01 <0.01 <0.01 <0.01
0.07
0.05
4.3
0.10
1.0

<0.01
0.86
1.8
386
2.5
34

0.06
0.01
0.72
0.04
0.17

0.08
0.04
4.2
0.08
0.77

18
0.18
2.1

<0.01
2.6
24
5356
31

4.5 Performance for a varying number of segments and sample size

The performance of change point estimation methods depends on an interplay between the
number of observations, the size of distributional shifts between segments, and the number of
change points. In Table 2 we see that for some datasets, such as CIM, iris, and dry beans, all
methods achieve average adjusted Rand indices above 0.975. Here, we believe that the signal-to-
noise ratio is so high that change point estimation is easy, and the diﬀerence between the average
adjusted Rand index and the perfect score 1.0 is mostly a result of random false positives. On the
other hand, the change in covariance and glass setups appear truly diﬃcult, clearly separating
methods that perform well (above 0.9) and those performing not much better than random
guessing (below 0.8).

Exp(1) i.i.d. for k = 1, . . . , K and deﬁne Ni = 1

We introduce two simulation settings that let us generate time series of any length n with any
number of change points K, allowing us to control the underlying signal-to-noise ratio. We draw
˜Nk
˜Nk, resulting in a minimum
relative segment length of δ = 1
K
k=1 round(nNk) = n.
such that

10K . We use round(nNk) as segment lengths, where we round

10K + 0.9
PK

˜Nj

∼

j=1

P

We present two simulation setups: One based on the Dirichlet distribution and one on the
dry beans dataset. We construct the former as in Section 4.2. For each segment, we draw i.i.d.
observations from a Dirichlet distribution of dimension d = 20, after sampling its parameters
uniformly from [0, 0.2]. To generate time series of arbitrary length based on the dry beans
dataset, we ﬁrst normalize each covariate to have an average within-class variance of one. Then,
for each segment, we draw a class label unequal to that of the previous segment and draw
round(nNk) observations with replacement from the corresponding class. Finally, we add i.i.d.
standard Gaussian noise to each observation. We simulate 500 datasets with K = 20, 80 and
vary n = 250, 354, 500, 707, 1000, . . . , 64000 for each setup. The mean adjusted Rand indices of
change point estimates from our and competing methods are displayed in Figure 5. Each method
was supplied with the minimum relative segment length δ = 1

10K .

The change in mean method does not perform well for either setup. Our method changeforest
performs best, except for very small sample sizes. For the dry beans simulation setup when
n/K < 20, ECP performs best, but not much better than 0.8. For the Dirichlet simulation
setup for K = 20, n = 354 and K = 80, n = 1000, MultiRank performs best. Surprisingly, KCP
performs very badly for the dry beans-based simulation, not selecting any change points at all.
This might be due to the higher number of change points compared to the main simulations, or

18

20 segments

80 segments

Dirichlet

1.00

0.75

1.00

0.75

0.50

0.50

0.25

0.25

0.00

0.00

3

4

3

4

10

10

10

10

1.00

1.00

dry beans

0.75

0.75

0.50

0.50

0.25

0.25

x
e
d
n

i

d
n
a
R

.
j
d
a

.
g
v
a

x
e
d
n

i

d
n
a
R

.
j
d
a

.
g
v
a

change in mean

changeforest

changekNN

ECP

KCP

MultiRank

0.00

0.00

3

4

3

4

10

10

10

10

sample size (n)

sample size (n)

Figure 5: Evolution of the average adjusted Rand index for methods by the number of observa-
tions for 20 and 80 segments. The y-axis was scaled to better pronounce values in [0.6, 1]. Two
times the standard deviation of the mean score is marked with vertical bars. All methods were
run until computational time got prohibitive (ECP) or 32 GB of memory was not suﬃcient for
computation (KCP, MultiRank, changekNN).

the additional Gaussian noise added.

We display the average computational time of the diﬀerent methods in Figure 6, together
with dashed grey lines corresponding to linear and quadratic time complexity. Again, the para-
metric change in mean method is very fast. For both change in mean and changeforest, time
complexity appears to scale approximately linear in the number of observations. Meanwhile,
changekNN, MultiRank, and KCP scale approximately quadratic, while ECP appears to have
worse than quadratic time complexity. changeforest is the quickest nonparametric method for
n & 5000.

4.6 Dependence on hyperparameters

We investigate how the hyperparameters of random forests aﬀect changeforest’s performance.
For this, we vary the three most important parameters for random forests: the number of trees
from 20 to 100 (default) to 500, the maximal depth of individual trees from 2 to 8 (default) to
inﬁnity, and the number of features considered at each split (mtry) from 1 to √d (default, rounded
down) to d. The average adjusted Rand indices for each set of parameters for the simulation
setups introduced in Section 4.2 are in Table 7 in the Appendix. Average computational costs
are in Table 8. We summarize the results here.

We observe that choosing even very extreme parameters, such as number of trees = 20,
maximal depth = 2, and mtry = 1, yields acceptable change point estimation performance for
simple simulation setups. With these parameters, changeforest performs on average better
than ECP, MultiRank, and changekNN, our method paired with a k-nearest neighbor classiﬁer.
It even outperforms the default conﬁguration (100, 8, √d) for the iris, breast cancer, and wine

19

 
 
 
 
 
 
20 segments

80 segments

4

10

2

10

0

10

)
s
(

e
m

i
t

−2

10

−4

10

4

10

2

10

0

10

−2

10

−4

10

change in mean

changeforest

changekNN

ECP

KCP

MultiRank

linear /  uadratic

3

4

3

4

10

10

10

10

sample size (n)

sample size (n)

Figure 6: Average time for change point detection on 8 Intel Xeon 2.3 GHz cores with 4
GB of RAM available per core for the Dirichlet simulation setup. All methods were run until
computational time got prohibitive (ECP) or the in total 32 GB of RAM we not suﬃcient (KCP,
MultiRank, changekNN).

simulation setups, possibly due to selecting fewer false positives because of higher noise in the
predictions. On the other hand, increasing the number of trees from 100 to 500 and splitting
individual trees to purity does not signiﬁcantly increase the performance compared to the default
values while increasing computational cost ﬁvefold.

This low dependence on the choice of tuning parameters is in contrast to KCP, where the
bandwidth of the Gaussian kernel has to be ﬁne-tuned for optimal performance. Table 9 in the
Appendix shows the estimation performance of KCP for diﬀerent kernels and bandwidths.

4.7 False positive rates

We empirically analyze our model selection procedure. For this, we select the largest class of
each dataset from Section 4.2, shuﬄe the observations before applying the change point detection
methods and collect the percentage out of 2500 simulations where at least one change point was
detected. See Table 4 for the results.

dataset

CIM CIC Dirichlet

iris

glass

change in mean
changeforest
changekNN
ECP
KCP
MultiRank

0.12
3.36
2.44
5.76
0.00
0.20

0.12
3.36
2.44
6.20
0.00
0.20

100.0
3.76
1.80
5.08
0.00
12.08

27.04
5.00
3.64
6.80
3.04
93.64

91.44
3.80
2.40
5.72
0.32
90.44

breast
cancer

3.44
3.80
29.80
4.76
5.24
100.0

abalone wine

7.68
3.00
3.28
4.76
11.32
100.0

1.20
3.52
6.80
5.16
0.08
100.0

dry
beans

1.40
2.48
2.04
5.16
0.04

worst

100.00
5.00
29.80
6.80
11.32
100.00

Table 4: Percentage of simulations where at least one change point was detected in a homoge-
neous dataset. For each dataset and method, 2500 simulations were performed.

As our model selection procedure is not a true permutation test, we observe more false posi-
tives than expected, given the approximate signiﬁcance threshold of 0.02 for both changeforest
and the k-nearest neighbor method changekNN. However, changeforest has a similar false pos-
itive rate across all simulation setups. The methods based on heuristics (KCP) or asymptotic

20

 
theory (MultiRank, change in mean) have varying degrees of false positive rates by dataset.
ChangekNN, which uses the same model selection procedure as changeforest, has a false posi-
tive rate of almost 30% for the breast cancer dataset.

5 Conclusion

Our proposed changeforest algorithm uses random forests to eﬃciently and accurately perform
multiple change point detection across a wide range of dataset types. Motivated by parametric
change point detection methodology, the algorithm optimizes a nonparametric classiﬁer-based
log-likelihood ratio. When coupled with random forests, it performs very well or better than
competitors for all simulation setups studied. The computation time of changeforest scales
almost linearly with the number of observations, enabling nonparametric change point detection
even for very large datasets. The changeforest software package is available to R, Python, and
Rust users. See github.com/mlondschien/changeforest for installation instructions.

As argued in Section 3.3, random forests are a natural choice as a classiﬁer to pair with our
methodology due to the possibility of retrieving unbiased out-of-bag class probability predic-
tions, the low dependence on hyperparameters, and almost linear time complexity. However, in
principle, any classiﬁer yielding class probability predictions can be used to generate classiﬁer log-
likelihood ratios, which we demonstrated by pairing our methodology with a k-nearest neighbor
classiﬁer. Other choices, such as neural networks, are also possible. Here, using cross-validation
to generate unbiased class probability predictions would be necessary.

The performance of the changeforest algorithm could be improved by pairing it with wild
binary segmentation of Fryzlewicz (2014) or seeded binary segmentation of Kovács et al. (2020b).
This would help particularly in scenarios with very short segments and equal or similar distri-
butions in surrounding segments. However, this would require changes to the model selection
procedure presented to avoid overﬁtting. Since we already observe very good performance with
binary segmentation, we do not include results from wild or seeded binary segmentation here.

Acknowledgments

Malte Londschien is supported by the ETH Foundations of Data Science and the ETH AI Center.
Solt Kovács and Peter Bühlmann are supported by the European Research Council (ERC) under
the European Union’s Horizon 2020 research and innovation programme (Grant agreement No.
786461 CausalStats - ERC-2017-ADG).

Authors’ Contributions

S.K. conceived and co-supervised the project. M.L. and S.K. developed the methodology. M.L.
designed and implemented the changeforest software package, designed, implemented, and ran
the simulations, developed the theory, and wrote the manuscript. P.B. co-supervised the project
and provided feedback and ideas during the development. S.K. and P.B. provided feedback on
the manuscript.

21

References

Sylvain Arlot, Alain Celisse, and Zaid Harchaoui. A kernel multiple change-point algorithm via

model selection. Journal of Machine Learning Research, 20(162), 2019.

Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth Silverman, and Angela Y. Wu. An
optimal algorithm for approximate nearest neighbor searching ﬁxed dimensions. Journal of
the ACM, 45(6):891–923, 1998.

Leo Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.

Jedelyn Cabrieto, Francis Tuerlinckx, Peter Kuppens, Mariel Grassmann, and Eva Ceulemans.
Detecting correlation changes in multivariate time series: A comparison of four non-parametric
change point detection methods. Behavior Research Methods, 49(3):988–1005, 2017.

Edward Carlstein. Nonparametric change-point estimation. The Annals of Statistics, 16(1):

188–197, 1988.

Hao Chen and Nancy Zhang. Graph-based change-point detection. The Annals of Statistics, 43

(1):139–176, 2015.

Alice Cleynen and Émilie Lebarbier. Segmentation of the Poisson and negative binomial rate

models: a penalized estimator. ESAIM: Probability and Statistics, 18:750–769, 2014.

conda-forge community. The conda-forge Project: Community-based Software Distribution Built

on the conda Package Format and Ecosystem, July 2015.

Paulo Cortez, António Cerdeira, Fernando Almeida, Telmo Matos, and José Reis. Modeling wine
preferences by data mining from physicochemical properties. Decision support systems, 47(4):
547–553, 2009.

Lutz Dümbgen. The asymptotic behavior of some nonparametric change-point estimators. The

Annals of Statistics, 19(3):1471–1495, 1991.

Ian W. Evett and Ernest J. Spiehler. Rule induction in forensic science. In Knowledge Based

Systems, pages 152–160. Online Publications, 1989.

Ronald A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics,

7(2):179–188, 1936.

Klaus Frick, Axel Munk, and Hannes Sieling. Multiscale change point inference. Journal of the

Royal Statistical Society: Series B, 76(3):495–580, 2014.

Jerome Friedman. On multivariate goodness-of-ﬁt and two-sample testing. Technical report,

Stanford University, 2004.

Piotr Fryzlewicz. Wild binary segmentation for multiple change-point detection. The Annals of

Statistics, 42(6):2243–2281, 2014.

Damien Garreau and Sylvain Arlot. Consistent change-point detection with kernels. Electronic

Journal of Statistics, 12(2):4440–4486, 2018.

22

Simon Hediger, Loris Michel, and Jeﬀrey Näf. On the use of random forest for two-sample testing.

Computational Statistics & Data Analysis, 170:170435, 2022.

Thomas Hotz, Ole Mathis Schütte, Hannes Sieling, Tatjana Polupanow, Ulf Diederichsen, Clau-
Idealizing ion channel recordings by a jump segmentation

dia Steinem, and Axel Munk.
multiresolution ﬁlter. IEEE Transactions on NanoBioscience, 12(4):376–386, 2013.

Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classiﬁcation, 2(1):

193–218, 1985.

Nicholas James and David Matteson. ecp: An R package for nonparametric multiple change

point analysis of multivariate data. Journal of Statistical Software, 62(7):1–25, 2015.

Alexander Kaplan, Joachim Röschke, Boris Darkhovsky, and Juergen Fell. Macrostructural
EEG characterization based on nonparametric change point segmentation: application to sleep
analysis. Journal of Neuroscience Methods, 106(1):81–90, 2001.

Abhishek Kaul, Venkata K. Jandhyala, and Stergios B. Fotopoulos. An eﬃcient two step algo-
rithm for high dimensional change point regression models without grid search. Journal of
Machine Learning Research, 20:1–40, 2019.

Rebecca Killick, Paul Fearnhead, and Idris A. Eckley. Optimal detection of changepoints with
a linear computational cost. Journal of the American Statistical Association, 107(500):1590–
1598, 2012.

Chang-Jin Kim, James C. Morley, and Charles R. Nelson. The structural break in the equity

premium. Journal of Business & Economic Statistics, 23(2):181–191, 2005.

Hyune-Ju Kim and David Siegmund. The likelihood ratio test for a change-point in simple linear

regression. Biometrika, 76(3):409–423, 1989.

Murat Koklu and Ilker Ali Ozkan. Multiclass classiﬁcation of dry beans using computer vision
and machine learning techniques. Computers and Electronics in Agriculture, 174:105507, 2020.

Solt Kovács, Housen Li, and Peter Bühlmann. Seeded intervals and noise level estimation in
change point detection: a discussion of Fryzlewicz (2020). Journal of the Korean Statistical
Society, 49(4):1081–1089, 2020a.

Solt Kovács, Housen Li, Peter Bühlmann, and Axel Munk. Seeded binary segmentation: A
general methodology for fast and optimal change point detection. arXiv:2002.06633, 2020b.
To appear in Biometrika.

Solt Kovács, Housen Li, Lorenz Haubner, Axel Munk, and Peter Bühlmann. Optimistic
search strategy: Change point detection for large-scale data via adaptive logarithmic queries.
arXiv:2010.10194, 2020c.

Song Liu, Makoto Yamada, Nigel Collier, and Masashi Sugiyama. Change-point detection in

time-series data by relative density-ratio estimation. Neural Networks, 43:72–83, 2013.

Malte Londschien, Solt Kovács, and Peter Bühlmann. Change-point detection for graphical
models in the presence of missing values. Journal of Computational and Graphical Statistics,
30(3):768–779, 2021.

23

David Lopez-Paz and Maxime Oquab. Revisiting classiﬁer two-sample tests. In International

Conference on Learning Representations, 2017.

Alexandre Lung-Yut-Fong, Céline Lévy-Leduc, and Olivier Cappé. Homogeneity and change-
point detection tests for multivariate data using rank statistics. Journal de la Société Française
de Statistique, 156(4):133–162, 2015.

Oscar–Hernan Madrid–Padilla, Yi Yu, Daren Wang, and Alessandro Rinaldo. Optimal nonpara-

metric change point analysis. Electronic Journal of Statistics, 15(1):1154 – 1201, 2021a.

Oscar–Hernan Madrid–Padilla, Yi Yu, Daren Wang, and Alessandro Rinaldo. Optimal nonpara-
metric multivariate change point detection and localization. IEEE Transactions on Information
Theory, 68(3):1922–1944, 2021b.

Nicholas D. Matsakis and Felix S. Klock. The rust language.

In ACM SIGAda Ada Letters,

volume 34, pages 103–104. ACM, 2014.

David S. Matteson and Nicholas A. James. A nonparametric approach for multiple change
point analysis of multivariate data. Journal of the American Statistical Association, 109(505):
334–345, 2014.

Warren S. McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous

activity. The Bulletin of Mathematical Biophysics, 5(4):115–133, 1943.

Adam B. Olshen, E. Seshan Venkatraman, Robert Lucito, and Michael Wigler. Circular binary
segmentation for the analysis of array-based DNA copy number data. Biostatistics, 5(4):
557–572, 2004.

Ewan S. Page. Continuous inspection schemes. Biometrika, 41(1/2):100–115, 1954.

Ewan S. Page. A test for a change in a parameter occurring at an unknown point. Biometrika,

42(3/4):523–527, 1955.

Florian Pein, Hannes Sieling, and Axel Munk. Heterogeneous change point inference. Journal

of the Royal Statistical Society: Series B, 79(4):1207–1227, 2017.

Anthony N. Pettitt. A non-parametric approach to the change-point problem. Journal of the

Royal Statistical Society: Series C, 28(2):126–135, 1979.

Franck Picard, Stephane Robin, Marc Lavielle, Christian Vaisse, and Jean-Jacques Daudin. A

statistical approach for array CGH data analysis. BMC Bioinformatics, 6(1):1–14, 2005.

William M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the

American Statistical association, 66(336):846–850, 1971.

Jaxk Reeves, Jien Chen, Xiaolan L. Wang, Robert Lund, and Qi Qi Lu. A review and comparison
of changepoint detection techniques for climate data. Journal of Applied Meteorology and
Climatology, 46(6):900–915, 2007.

Sandipan Roy, Yves Atchadé, and George Michailidis. Change point estimation in high dimen-
sional Markov random-ﬁeld models. Journal of the Royal Statistical Society: Series B, 79(4):
1187–1206, 2017.

24

Jeremy J. Shen and Nancy R. Zhang. Change-point model on nonhomogeneous Poisson processes
with application in copy number proﬁling by next-generation DNA sequencing. The Annals
of Applied Statistics, 6(2):476–496, 2012.

Nick Street, William H. Wolberg, and Olvi L. Mangasarian. Nuclear feature extraction for breast
tumor diagnosis. In Biomedical Image Processing and Biomedical Visualization, volume 1905.
SPIE, 1993.

Charles Truong, Laurent Oudre, and Nicolas Vayatis. Selective review of oﬄine change point

detection methods. Signal Processing, 167:107299, 2020.

Lyudmila Y. Vostrikova. Detecting ’disorder’ in multidimensional random processes. Soviet

Mathematics Doklady, 24:55–59, 1981.

Daren Wang, Yi Yu, and Alessandro Rinaldo. Optimal covariance change point localization in

high dimensions. Bernoulli, 27(1):554–575, 2021.

Tengyao Wang and Richard J. Samworth. High dimensional change point estimation via sparse

projection. Journal of the Royal Statistical Society: Series B, 80(1):57–83, 2018.

Samuel G. Waugh. Extending and benchmarking Cascade-Correlation: extensions to the Cascade-
Correlation architecture and benchmarking of feed-forward supervised artiﬁcial neural networks.
PhD thesis, University of Tasmania, 1995.

Yi-Ching Yao. Estimating the number of change-points via Schwarz’ criterion. Statistics &

Probability Letters, 6(3):181–189, 1988.

Yuxuan Zhang and Hao Chen. Graph-based multiple change-point detection. arXiv:2110.01170,

2021.

Changliang Zou, Guosheng Yin, Long Feng, and Zhaojun Wang. Nonparametric maximum
likelihood approach to multiple change-point problems. The Annals of Statistics, 42(3):970–
1002, 2014.

25

A Proofs

Proposition 2. Let

p∞α (x) := P (Y = k

X = x) =

|

αk

αk91 −
n

(cid:18)

p(αk91,αk]
p(0,n] (cid:19)

α

|−

|

1

k=1

be the inﬁnite sample Bayes classiﬁer corresponding to the segmentation α.
segmentations containing α0 and DKL(P
respect to P , then

is a set of
Q) is the Kullback-Leibler divergence of Q with

A

If

k

max
α
∈A

E [G ((Xi)n

i=1 |

α, p∞)] = E

G

(Xi)n

i=1 |

α0, p∞

=

(cid:0)
(α0
k −

(cid:2)
K

Xk=1

(cid:1)(cid:3)
k91)DKL(Pk
α0

k

P

(0,n])

and any maximizer of E [G ((Xi)n
particular, the maximizer α with the smallest cardinality

α, p∞)] is equal to α0 or is an oversegmentation of α0. In
is equal to α0.

i=1 |

Proof. Let u < v and deﬁne κ((u, v]) :=
k
|
{
that α0
k
k, then κ((u, v]) =
}
{

k91 6 u < v 6 α0

α
|
|
(α0
k91, α0
(u, v]
k]
and p(u,v] = p(α0

∩

= ∅
k91,α0

. If there exists some k such
}
k]. Consequently,

v

E

"

i=u+1
X

p(u,v](Xi)
p(0,n](Xi)

log

(cid:18)

(cid:19)#

= (v

−

u)DKL(Pk

P

(0,n]).

k

This implies, that

E

Gn

(Xi)n

i=1 |

(cid:2)

(cid:0)

K

α0, p∞

=

(cid:1)(cid:3)

Xk=1

α0

α0
k −
n

k91

DKL(Pk

P

(0,n]).

k

Now assume that κ((u, v]) contains more than a single element. Set πk((u, v]) := |
Then

(u,v]

(α0

k91,α0
k]
∩
u
v
−

|

.

v

E

"

i=u+1
X

p(u,v](Xi)
p(0,n](Xi)

log

(cid:18)

(cid:19)#

= (v

= (v

< (v

= (v

−

−

−

−

u)EX

P

∼

(u,v]

p(u,v](X)
p(0,n](X)

log

(cid:20)

(cid:18)

(cid:19)(cid:21)

u)EX

P

∼

(u,v]

k

log

"

  P

κ((u,v]) πk((u, v])pk(X)
∈

p(0,n](X)

!#

πk((u, v])EX

Pk

log

∼

(cid:20)
πk((u, v])DKL(Pk

(cid:18)
P

k

(0,n]) ,

pk(X)
p(0,n](X)

(cid:19)(cid:21)

u)

u)

κ((u,v])

Xk
∈

κ((u,v])

Xk
∈

) is strictly convex
where we used the fact that the continuous extension of x
in the third line. Consider any segmentation α that is not an oversegmentation of α0. Then there

x log(x) on [0,

∞

7→

26

6
exists at least one segment (αk′91, αk′] such that κ((αk′91, αk′]) contains more than one element
and thus

E [Gn ((Xi)n

i=1 |

α, p∞)] =

<

=

p(αk91,αk](Xi)
p(0,n](Xi)

log

(cid:18)



(cid:19)

K

E

αk

i=αk91+1
X




(αk

Xk=1
K

k=1
X
K

k=1
X

αk91)

−

κ((αk91,αk])

Xl
∈

α0

α0
k −
n

k91

DKL(Pk

P

(0,n]) ,

k


πl((αk91, αk])DKL(Pl

P

(0,n])

k

applying the above inequality with u = αk′91 and v = αk′.

Proposition 3. For the Bayes classiﬁer p∞, the expected classiﬁer log-likelihood ratio

s

E

G

(Xi)v

i=u+1 | {
is piecewise convex between the underlying change points α0, with strict convexity in the segment
= P
(α0

= P

7→

(cid:1)(cid:3)

(cid:0)

k91, α0

k] if P

(cid:2)
k] or P

(α0

k,v].

(α0

k91,α0

(α0

k91,α0
k] 6

u, s, v

, p∞
}

Proof. Let s

∈

α0. We show that the expected classiﬁer log-likelihood ratio
\

(u,α0

k91] 6
(u, v]

is convex at s, that is, G(s + 1)

−
s

G(s) := E

G

(Xi)v

i=u+1 | {

u, s, v

, p∞
}

(cid:2)

(cid:0)

2G(s) + G(s + 1) > 0. By deﬁnition

(cid:1)(cid:3)

G(s) = E

"

i=u+1
X

p(u,s](Xi)
p(u,v](Xi)

log

(cid:18)

v

+

log

(cid:19)

i=s+1
X

(cid:18)

p(s,v](Xi)
p(u,v](Xi)

(cid:19)#

and

G(s + 1)

−

s+1

G(s) = E

log

p(u,s+1](Xi)
p(u,s](Xi)

(cid:18)
i=u+1
X
u + 1)DKL(P

= (s

"

−

(u,s+1] k

v

+

log

(cid:19)
P

i=s+2
X
(u,s]) + (v

p(s+1,v](Xi)
p(s,v](Xi)

(cid:19)
1)DKL(P

+ log

(cid:18)

(s+1,v] k

p(u,s](Xs+1)
p(s,v](Xs+1)
P

(s,v])+

(cid:19)#

E

log

(cid:18)
with a strict inequality if P

(cid:20)

p(u,v](Xs+1)
p(s,v](Xs+1)

> E

log

(cid:19)(cid:21)

(cid:20)

(cid:18)

(u,s+1] 6

= P

(u,s] or if s < v

1 and P

−

(s+1,v] 6

= P

(s,v]. Similarly,

(cid:18)
s
−
p(u,s](Xs+1)
p(s,v](Xs+1)

−

,

(cid:19)(cid:21)

G(s

1)

−

−

G(s) = E

"

s

1

−

log

1](Xi)
p(u,s
−
p(u,s](Xi)

= (s

i=u+1
X
u

−

−

E

log

(cid:20)

(cid:18)

(cid:18)
1)DKL(P
p(s,v](Xs)
p(u,v](Xs)

(u,s

1] k
−
> E

(cid:19)(cid:21)

v

+

log

(cid:19)
P

i=s+1
X
(u,s]) + (v

(cid:18)

−

−

1,v](Xi)
p(s
p(s,v](Xi)
(cid:19)
s + 1)DKL(P

+ log

(cid:18)

(s

1,v] k

−

1)
1)

−

−

p(s,v](Xs
p(u,s](Xs
P

(s,v])+

(cid:19)#

p(s,v](Xs)
p(u,s](Xs)

,

(cid:19)(cid:21)

log

(cid:20)

(cid:18)

27

with a strict inequality if P
are i.i.d. and thus

(s

1,v] 6

−

= P

(s,v] or if s > u + 1 and P

(u,s

1] 6

−

= P

(s,v]. Note that Xs, Xs+1

E

log

(cid:20)

(cid:18)

p(u,s](Xs+1)
p(s,v](Xs+1)

+ log

(cid:19)

(cid:18)

p(s,v](Xs)
p(u,s](Xs)

= E

log

(cid:19)(cid:21)

(cid:20)

(cid:18)

p(u,s](Xs+1)
p(u,s](Xs)

+ log

(cid:19)

(cid:18)

p(s,v](Xs)
p(s,v](Xs+1)

= 0.

(cid:19)(cid:21)

Combining everything together,

G(s + 1)

2G(s) + G(s

−

1) = (G(s + 1)
(u,s] or P

−
= P

(u,s+1] 6

(s
k]. Then this is equivalent to P

1,v] 6

−

k91, α0

−
= P

G(s)) + (G(s

1)

G(s)) > 0,

−
(s,v]. Choose k such that s
= P

−

(α0

k91,α0

(u,α0

k91]

∈
k] or P

(α0

k91, α0
k].
=
k91,α0
k]

(α0

with strict inequality if P
Then also s + 1
P

(α0

∈

(α0

k,v].

Proposition 4. For the Bayes classiﬁer p∞ and any initial guess u < s(0) < v, the expected
approximate gain

s

E

"

s

7→

log

v
s(0)

u
u

−
−

p∞
u,s(0),v
{

}

v

(Xi)1

+

log

v
v

u
s(0) p∞
−
−

(Xi)2

u,s(0),v
{

}

(cid:19)#

i=u+1
X

(cid:19)
is piecewise linear between the underlying change points α0. If there is a single change point
a0

(u, v], the expected approximate gain has a unique maximum at s = a0.

i=s+1
X

(cid:18)

(cid:18)

∈

Proof. Write

s

G(s) :=

log

i=u+1
X
s

=

log

i=u+1
X

and deﬁne

(cid:18)

u
u

v
s(0)

p∞
u,s(0),v
{

−
−
p(u,s(0)](Xi)
p(u,v](Xi) !

}

v

+

i=s+1
X

v

(Xi)

+

log

v
v

u
−
s(0)
−

(cid:19)

log

(cid:18)

i=s+1
X
p(s(0),v](Xi)
p(u,v](Xi) !

(1

p∞
u,s(0),v
{

}

−

(Xi))

(7)

(cid:19)

(8)

Uj := G(j)

G(j

−

−

1) = log

p(u,s(0)](Xj)
p(s(0),v](Xj) !

,

such that G(s) = G(u) +
segment (u, v] and any s
as the Uα0
points in α0.

k91, αk] intersecting the
k91+1]
k91+1, . . . , Uαk are i.i.d., showing that E[G(s)] is piecewise linear between the change

s
j=u+1 Uj. In particular, for any segment (α0
k91, αk], we have that E[G(s)] = E[G(α0
(α0
P
∈

k91)E[Uα0
α0

k91)] + (s

−

Now assume that there is a single change point a0 in (u, v]. Assume without loss of generality
a0
u p(a0,v] and p(s(0),v] = p(a0,v]. Then, for

u p(u,a0] + s(0)

s(0)

u

that s(0) > a0 such that p(u,s(0)] = a0
s(0)
j 6 a0,

−
−

−
−

E[Uj] = EP

log

p(u,s(0)](Xj)
p(s(0),v](Xj) !#

(u,a0] "
u
u

−
−

=

=

s(0)
a0

s(0)
a0

EP

log

(u,s(0)] "

p(u,s(0)](Xj)
p(s(0),v](Xj) !# −

EP

log

(a0,v] "

p(u,s(0)](Xj)
p(s(0),v](Xj) !#

u
u

−
−

DKL(P

(u,s(0)] k

P

(a0,v]) +

(s(0),v] k

P

(u,s(0)]) > 0.

s(0)
a0

a0
u

−
−
DKL(P

s(0)
a0

a0
u

−
−

28

6
6
 
 
 
 
 
 
Similarly, for j > a0

E[Us] = EP

which shows that s

7→

p(u,s](Xj)
p(s,v](Xj)

log

(a0,v]

(cid:20)
E[G(s)] = E[G(u)] +

(cid:18)

=

DKL(P2 k

−

P

(u,s]) < 0,

(cid:19)(cid:21)

s
i=u+1

E[Ui] has a unique maximum at s = a0.

P

29

B Figures

0.05

0.04

0.03

0.02

0.01

change in mean

changeforest

500

400

300

200

100

0.00

0

0

200

400

600

800

1000

0

200

400

600

800

1000

changekNN

ECP

100

80

60

40

20

150

100

50

0

0

0

200

400

600

800

1000

0

200

400

600

800

1000

KCP

MultiRank

200

150

100

50

500

400

300

200

100

0

0

0

200

400

600

800

1000

0

200

400

600

800

1000

Figure 7: Histograms of cumulative change point estimates for 500 simulation runs on the Dirich-
let simulation setup. The true change point locations are marked with green crosses. Note the
diﬀerent scales on the y-axis.

30

C Tables

dataset

CIM CIC Dirichlet

iris

glass

change in mean
changeforest
changekNN
ECP
KCP
MultiRank

0.0 33.3
0.0
1.7
0.0 33.3
0.0 30.7
0.0
0.7
0.0 33.3

48.0
0.2
6.6
5.1
5.0
3.0

1.3
0.0
0.0
0.0
0.0
15.3

16.8
2.8
7.9
31.1
7.9
15.9

breast
cancer

16.7
0.0
0.0
0.0
0.0
0.0

abalone wine

average

dry
beans

5.0
3.1
5.0
6.2
4.2
9.1

0.4
0.0
1.5
0.3
0.2
3.0

1.3
0.0
0.0
0.0
0.0

13.7
0.9
6.0
8.1
2.0
10.0

Table 5: Median relative Hausdorﬀ distance over 500 simulations (in percent). Optimal scores
are marked in bold.

dataset

CIM CIC Dirichlet

iris

glass

breast
cancer

abalone wine

ground truth

2

2

10

2

5

1

14

4

change in mean
changeforest
changekNN
ECP
KCP
MultiRank

3.65
2.99
4.21
2.15
4.52
2.11
3.71
2.09
3.60
2.05
3.00
13.31
Table 6: Average number of change points estimated over 500 simulations.

22.06
4.93
4.16
2.58
3.44
21.74

0.00
10.54
8.01
7.70
8.39
9.18

16.62
1.08
1.49
1.07
1.01
8.28

11.07
12.09
11.74
9.73
10.62
11.77

0.00
2.36
0.59
1.26
2.38
0.45

2.00
2.13
2.12
2.07
2.03
2.00

dry
beans

6

7.04
6.23
6.21
6.05
6.00

31

trees max. depth mtry CIM

CIC

Dirichlet

iris

glass

20

100

500

2

8

∞

2

8

∞

2

8

∞

1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d

0.987 (0.033)
0.988 (0.032)
0.988 (0.033)
0.992 (0.017)
0.991 (0.016)
0.987 (0.048)
0.871 (0.268)
0.849 (0.296)
0.777 (0.361)
0.983 (0.040)
0.982 (0.042)
0.983 (0.039)
0.987 (0.034)
0.987 (0.035)
0.990 (0.031)
0.984 (0.038)
0.985 (0.035)
0.988 (0.031)
0.977 (0.048)
0.978 (0.045)
0.980 (0.045)
0.982 (0.042)
0.983 (0.040)
0.986 (0.037)
0.983 (0.040)
0.981 (0.042)
0.984 (0.039)

0.133 (0.290)
0.185 (0.339)
0.189 (0.339)
0.286 (0.411)
0.266 (0.403)
0.206 (0.370)
0.000 (0.000)
0.000 (0.000)
0.000 (0.000)
0.498 (0.438)
0.521 (0.439)
0.499 (0.434)
0.929 (0.130)
0.925 (0.122)
0.891 (0.204)
0.880 (0.217)
0.882 (0.222)
0.815 (0.312)
0.529 (0.435)
0.565 (0.432)
0.519 (0.433)
0.948 (0.051)
0.942 (0.068)
0.929 (0.110)
0.941 (0.080)
0.944 (0.068)
0.935 (0.110)

0.972 (0.020)
0.982 (0.017)
0.982 (0.016)
0.982 (0.014)
0.988 (0.012)
0.988 (0.011)
0.979 (0.016)
0.988 (0.012)
0.988 (0.012)
0.983 (0.019)
0.981 (0.021)
0.976 (0.021)
0.985 (0.017)
0.986 (0.019)
0.984 (0.019)
0.986 (0.018)
0.987 (0.019)
0.985 (0.018)
0.979 (0.024)
0.977 (0.025)
0.974 (0.023)
0.986 (0.019)
0.985 (0.019)
0.983 (0.020)
0.988 (0.017)
0.986 (0.018)
0.984 (0.019)

0.984 (0.039)
0.985 (0.038)
0.989 (0.030)
0.991 (0.025)
0.993 (0.021)
0.992 (0.024)
0.993 (0.022)
0.993 (0.022)
0.992 (0.023)
0.980 (0.042)
0.982 (0.040)
0.982 (0.040)
0.981 (0.041)
0.983 (0.040)
0.986 (0.034)
0.983 (0.039)
0.984 (0.039)
0.986 (0.035)
0.977 (0.047)
0.981 (0.042)
0.982 (0.040)
0.980 (0.046)
0.981 (0.044)
0.986 (0.035)
0.981 (0.045)
0.981 (0.043)
0.986 (0.037)

0.889 (0.084)
0.903 (0.081)
0.913 (0.080)
0.892 (0.081)
0.889 (0.098)
0.869 (0.131)
0.855 (0.148)
0.872 (0.121)
0.848 (0.157)
0.899 (0.079)
0.912 (0.071)
0.919 (0.068)
0.923 (0.069)
0.923 (0.072)
0.917 (0.079)
0.917 (0.075)
0.920 (0.073)
0.918 (0.076)
0.910 (0.074)
0.916 (0.072)
0.918 (0.070)
0.925 (0.071)
0.924 (0.072)
0.921 (0.076)
0.921 (0.074)
0.925 (0.072)
0.918 (0.078)

trees max. depth mtry

breast cancer

abalone

wine

dry beans

average

20

100

500

2

8

∞

2

8

∞

2

8

∞

1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d

0.992 (0.050)
0.991 (0.044)
0.988 (0.053)
0.996 (0.026)
0.994 (0.039)
0.993 (0.042)
0.998 (0.006)
0.998 (0.010)
0.998 (0.007)
0.975 (0.089)
0.975 (0.084)
0.975 (0.084)
0.979 (0.075)
0.983 (0.074)
0.984 (0.069)
0.996 (0.017)
0.994 (0.026)
0.996 (0.019)
0.976 (0.085)
0.972 (0.091)
0.973 (0.088)
0.969 (0.098)
0.978 (0.082)
0.982 (0.075)
0.991 (0.030)
0.994 (0.026)
0.994 (0.032)

0.868 (0.083)
0.907 (0.062)
0.923 (0.054)
0.843 (0.106)
0.852 (0.108)
0.809 (0.131)
0.081 (0.139)
0.089 (0.143)
0.070 (0.118)
0.914 (0.052)
0.931 (0.040)
0.937 (0.038)
0.931 (0.047)
0.932 (0.049)
0.927 (0.055)
0.879 (0.096)
0.902 (0.075)
0.892 (0.077)
0.916 (0.046)
0.932 (0.040)
0.938 (0.039)
0.934 (0.045)
0.938 (0.044)
0.933 (0.050)
0.914 (0.062)
0.927 (0.052)
0.919 (0.061)

0.984 (0.022)
0.991 (0.014)
0.991 (0.021)
0.996 (0.009)
0.996 (0.009)
0.997 (0.004)
0.991 (0.010)
0.992 (0.009)
0.992 (0.009)
0.976 (0.037)
0.984 (0.037)
0.984 (0.040)
0.991 (0.031)
0.993 (0.023)
0.995 (0.019)
0.997 (0.005)
0.997 (0.005)
0.997 (0.005)
0.971 (0.056)
0.972 (0.057)
0.972 (0.061)
0.984 (0.043)
0.987 (0.037)
0.994 (0.021)
0.997 (0.006)
0.997 (0.007)
0.996 (0.009)

0.997 (0.008)
0.997 (0.009)
0.997 (0.010)
0.998 (0.004)
0.998 (0.004)
0.998 (0.004)
0.998 (0.004)
0.998 (0.004)
0.998 (0.004)
0.993 (0.017)
0.990 (0.023)
0.987 (0.026)
0.995 (0.015)
0.997 (0.010)
0.996 (0.013)
0.998 (0.006)
0.996 (0.014)
0.992 (0.022)
0.984 (0.033)
0.984 (0.032)
0.982 (0.034)
0.992 (0.020)
0.993 (0.019)
0.995 (0.015)
0.996 (0.012)
0.991 (0.023)
0.994 (0.019)

0.867 (0.107)
0.881 (0.120)
0.884 (0.120)
0.886 (0.145)
0.885 (0.144)
0.871 (0.140)
0.752 (0.113)
0.753 (0.117)
0.740 (0.137)
0.911 (0.154)
0.917 (0.154)
0.916 (0.152)
0.967 (0.061)
0.968 (0.059)
0.963 (0.081)
0.958 (0.085)
0.961 (0.085)
0.952 (0.112)
0.913 (0.154)
0.920 (0.153)
0.915 (0.153)
0.967 (0.053)
0.968 (0.052)
0.968 (0.057)
0.968 (0.048)
0.970 (0.045)
0.968 (0.054)

Table 7: Average adjusted Rand indices of changeforest for diﬀerent tuning parameters over
500 simulations, with standard deviations in parentheses. Optimal scores and those within two
standard deviations are marked in bold. When necessary, mtry, the number of features evaluated
at each split, was rounded down to the next integer.

32

trees max. depth mtry CIM CIC Dirichlet

iris

glass wine breast cancer

abalone

dry beans

20

100

500

2

2

8

8

∞

1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d
1
√d
d

0.64
0.01
0.68
0.01
0.86
0.01
0.84
0.01
0.97
0.01
1.5
0.01
1.2
0.01
1.4
0.01
2.1
0.01
1.3
0.02
1.4
0.02
2.1
0.02
2.1
0.02
2.6
0.02
4.7
0.02
3.6
0.03
4.2
0.02
7.4
0.03
4.3
0.04
5.0
0.04
8.2
0.05
8.4
0.08
11
0.08
21
0.08
15
0.08
18
0.08
33
0.08
Table 8: Average computational times in seconds on 8 Intel Xeon 2.3 GHz cores. When necessary,
mtry, the number of features evaluated at each split, was rounded down to the next integer.

0.02
0.02
0.02
0.03
0.03
0.03
0.03
0.03
0.03
0.04
0.04
0.04
0.07
0.07
0.07
0.09
0.09
0.09
0.10
0.11
0.12
0.22
0.22
0.25
0.30
0.29
0.31

0.02
0.02
0.02
0.02
0.03
0.03
0.03
0.03
0.03
0.03
0.04
0.04
0.06
0.06
0.07
0.06
0.06
0.07
0.10
0.10
0.12
0.20
0.20
0.22
0.22
0.21
0.22

0.01
0.01
0.01
0.02
0.02
0.03
0.02
0.02
0.02
0.03
0.03
0.04
0.08
0.09
0.10
0.10
0.11
0.12
0.08
0.09
0.11
0.27
0.30
0.37
0.36
0.38
0.44

0.23
0.24
0.27
0.32
0.35
0.42
0.23
0.27
0.31
0.43
0.46
0.56
0.77
0.85
1.1
1.3
1.4
1.7
1.2
1.3
1.7
2.7
3.1
4.2
4.9
5.3
6.7

0.10
0.11
0.14
0.16
0.16
0.21
0.18
0.17
0.22
0.22
0.24
0.33
0.43
0.43
0.57
0.50
0.46
0.60
0.67
0.75
1.1
1.6
1.6
2.2
1.9
1.7
2.3

0.03
0.02
0.03
0.03
0.03
0.04
0.04
0.04
0.05
0.04
0.04
0.05
0.07
0.08
0.10
0.09
0.10
0.11
0.11
0.12
0.14
0.25
0.27
0.33
0.32
0.33
0.38

0.24
0.25
0.27
0.34
0.36
0.45
0.57
0.57
0.72
0.45
0.48
0.56
0.79
0.87
1.2
1.6
1.6
2.2
1.4
1.5
1.9
3.0
3.4
5.0
6.8
7.0
9.7

∞

∞

8

2

33

kernel

bandwidth CIM

CIC

Dirichlet

iris

glass

cosine
linear
Gaussian 0.025
Gaussian 0.05
Gaussian 0.1
Gaussian 0.2
Gaussian 0.4
Gaussian 0.8
oracle

0.360 (0.250)
0.801 (0.166)
0.343 (0.280)
0.988 (0.033)
0.376 (0.357)
0.993 (0.025)
0.731 (0.333)
0.996 (0.018)
0.955 (0.057)
0.998 (0.012)
0.999 (0.005)
0.978 (0.032)
1.000 (0.002) 0.983 (0.022) 0.870 (0.079)
0.868 (0.079)
0.973 (0.090)
0.999 (0.002)
0.871 (0.079)
0.983 (0.022)
1.000 (0.002)

0.028 (0.000)
0.858 (0.082)
0.869 (0.078)
0.849 (0.163)
0.943 (0.105)
0.799 (0.112)
0.866 (0.080)
0.977 (0.063)
0.870 (0.078)
0.994 (0.023)
0.871 (0.079) 0.996 (0.013)
0.994 (0.023)
0.831 (0.218)
0.996 (0.013)

0.031 (0.000)
0.367 (0.147)
0.828 (0.157)
0.830 (0.171)
0.744 (0.234)
0.588 (0.258)
0.465 (0.278)
0.153 (0.246)
0.830 (0.171)

kernel

bandwidth

breast cancer

abalone

wine

dry beans

average

cosine
linear
Gaussian 0.025
Gaussian 0.05
Gaussian 0.1
Gaussian 0.2
Gaussian 0.4
Gaussian 0.8
oracle

0.390 (0.037)
0.840 (0.062)
0.892 (0.053)

0.157 (0.024)
0.630 (0.158)
0.986 (0.016)

0.794 (0.078)
0.036 (0.005)
0.965 (0.021)
0.968 (0.054)
1.000 (0.003)
1.000 (0.002)
0.999 (0.003) 0.905 (0.045) 0.987 (0.014) 1.000 (0.003)
0.996 (0.033)
0.989 (0.055)
0.987 (0.055)
0.988 (0.046)
1.000 (0.002)

0.384 (0.108)
0.758 (0.136)
0.868 (0.141)
0.921 (0.130)
0.916 (0.043) 0.988 (0.011) 1.000 (0.003) 0.940 (0.087)
0.988 (0.011) 1.000 (0.000)
0.925 (0.094)
0.918 (0.045)
1.000 (0.000)
0.905 (0.102)
0.970 (0.027)
0.878 (0.076)
0.835 (0.123)
0.967 (0.039)
0.916 (0.026)
0.819 (0.101)
0.954 (0.065)
1.000 (0.003)
0.988 (0.011)
0.918 (0.045)

Table 9: Average adjusted Rand indices of KCP for diﬀerent tuning parameters over 500 simu-
lations, with standard deviations in parentheses. Optimal scores and those within two standard
deviations are marked in bold. The oracle is the best performing hyperparameter combination.
We used a Gaussian kernel with bandwidth 0.1 in the main simulations.

34

