2
2
0
2

y
a
M
6
1

]
E
S
.
s
c
[

1
v
6
3
7
7
0
.
5
0
2
2
:
v
i
X
r
a

Prioritizing Corners in OoD Detectors via
Symbolic String Manipulation

Chih-Hong Cheng1, Changshun Wu2, Emmanouil Seferis1, and Saddek Bensalem2(cid:63)

1 Fraunhofer IKS, Munich, Germany
{chih-hong.cheng,emmanouil.seferis}@iks.frauhofer.de
2 Univ. Grenoble Alpes, Verimag, Grenoble, France
{changshun.wu,saddek.bensalem}@univ-grenoble-alpes.fr

Abstract. For safety assurance of deep neural networks (DNNs), out-
of-distribution (OoD) monitoring techniques are essential as they ﬁlter
spurious input that is distant from the training dataset. This paper stud-
ies the problem of systematically testing OoD monitors to avoid cases
where an input data point is tested as in-distribution by the monitor,
but the DNN produces spurious output predictions. We consider the def-
inition of “in-distribution” characterized in the feature space by a union
of hyperrectangles learned from the training dataset. Thus the testing is
reduced to ﬁnding corners in hyperrectangles distant from the available
training data in the feature space. Concretely, we encode the abstract lo-
cation of every data point as a ﬁnite-length binary string, and the union
of all binary strings is stored compactly using binary decision diagrams
(BDDs). We demonstrate how to use BDDs to symbolically extract cor-
ners distant from all data points within the training set. Apart from test
case generation, we explain how to use the proposed corners to ﬁne-tune
the DNN to ensure that it does not predict overly conﬁdently. The result
is evaluated over examples such as number and traﬃc sign recognition.

Keywords: OoD monitoring · test case prioritization · neural network
· training.

1

Introduction

To cope with practical concerns in autonomous driving where deep neural net-
works (DNN) [7] are operated in an open environment, out-of-distribution (OoD)
monitoring is a commonly used technique that raises a warning if a DNN receives
an input distant from the training dataset. One of the weaknesses with OoD de-
tection is regarding inputs that fall in the OoD detector’s decision boundary
while being distant from the training dataset. These inputs are considered “in-
distribution” by the OoD detector but can impose safety issues due to extensive
extrapolation. In this paper, we are thus addressing this issue by developing a
disciplined method to identify the weakness of OoD detectors and improve the
system accordingly.

(cid:63) The ﬁrst two authors contributed equally to this work.

 
 
 
 
 
 
2

C.-H. Cheng et al.

Precisely, we consider OoD detectors constructed using boxed abstraction-
based approaches [10,3,25], where DNN-generated feature vectors from the train-
ing dataset are clustered and enclosed using hyperrectangles. The OoD detector
raises a warning over an input, provided that its corresponding feature vec-
tor falls outside the boxed abstraction. We focus on analyzing the corners of
the monitor’s hyperrectangle and diﬀerentiate whether a corner is supported or
unsupported depending on having some input in the training dataset generating
feature vectors located in the corner. However, the number of exponentially many
corners in the abstraction reveals two challenges, namely (1) how to enumerate
the unsupported corners and (2) how to prioritize unsupported corners to be
analyzed.

– For (1), we present an encoding technique that, for each feature vector dimen-
sion, decides if an input falls in the border subject to a closeness threshold δ.
This allows encoding for each input in-sample as a binary string and storing
the complete set compactly via Binary decision diagrams (BDDs) [2]. With
an encoding via BDD, one can compute all unsupported corners using set
diﬀerence operations.

– For (2), we further present an algorithm manipulated on the BDDs that
allows ﬁltering all corners that are far from all training data subject to a
minimum constant Hamming distance (which may be further translated into
Euclidean distance). This forms the basis of our corner prioritization tech-
nique for abstractions characterized by a single hyperrectangle. For multiple
boxed-abstraction, we use a lazy approach to omit the corners when the
proposed corner from one box falls inside another box.
With a given corner proposal, we further encounter practical problems to
produce input images that resemble “natural” images. We thus consider an alter-
native approach: it is feasible to have the DNN generate a prediction with low
conﬁdence for any input whose feature vectors resemble unsupported corners.
This requirement leads to a DNN ﬁne-tuning scheme as the ﬁnal contribution of
this paper: The ﬁne-tuning freezes parameters for all network layers before the
monitored layer, thereby keeping the validity of the OoD monitor. However, it
allows all layers after the monitored features to be adjusted. Thus the algorithm
feeds the unsupported corners to the ﬁne-tunable sub-network to ensure that the
modiﬁed DNN reports every class with low conﬁdence, while keeping the same
prediction for existing training data.

We have evaluated our proposed techniques in applications ranging from
standard digit recognition to traﬃc sign detection. For corners inside the moni-
tor while distant from the training data, our experiment indicates that the DNN
indeed acts over-conﬁdently in the corresponding prediction, which is later ad-
justed with our local training method. Altogether the positive evaluation of the
technique oﬀers a rigorous paradigm to align DNN testing, OoD detection, and
DNN repair for safety-critical systems.

The rest of the paper is structured as follows: After reviewing related work
in Section 2, we present in Section 3 the basic notation as well as a concise
deﬁnition on abstraction-based monitors. Subsequently, in Section 4 we present

Prioritizing Corners in OoD Detectors via Symbolic String Manipulation

3

our key results for prioritized corner case proposal in a single-box conﬁguration
and its extension to a multi-box setting. In Section 5 we present how to use the
discovered corners in improving the DNN via local training. Finally, we present
our preliminary evaluation in Section 6 and conclude in Section 7.

2 Related Work

Systematically testing of DNNs has been an active research scheme, where read-
ers may reference Section 5.1 of a recent survey [12] for an overview of existing
results. Overall, the line of attack is by ﬁrst deﬁning a coverage criterion, followed
by concrete test case generation utilizing techniques such as adversarial pertur-
bation [24], constraint solving [13], or model-based exploration [22]. For white
box coverage criteria, neuron coverage [21] and extensions (e.g., SS-coverage [23]
or neuron combinatorial testing [19]) essentially consider the activation pattern
for neurons and demands the set of test inputs to satisfy a pre-deﬁned relative
completeness criterion; the idea is essentially motivated by classical software
testing coverage (e.g., branch coverage) as used in safety standards. For black-
box coverage criteria, multiple results are utilizing combinatorial testing [4,1],
where by ﬁrst deﬁning the human-speciﬁed features in the input space, it is
also possible to argue the relative completeness of the test data. For the above
metrics, one can apply coverage-driven testing, i.e., generate test cases that max-
imally increase coverage. Note that the above test metrics and the associated
test case prioritization techniques are not property-oriented, i.e., prioritizing the
test cases does not have a direct relation with dependability attributes. This is
in contrast to our work on testing the decision boundary of a DNN monitor,
where our test prioritization scheme prefers corners (of the monitor) that have
no input data being close-by. These corners refer to regions where DNN decisions
are largely extrapolated, and it is important to ensure that inputs that may lead
to these corners are properly tested. The second diﬀerentiation is that we also
consider the subsequent DNN repair scheme (via local training) to incorporate
the distant-yet-uncovered corners.

In this paper, we are interested in testing the monitors built from an abstrac-
tion of feature vectors from the training data, where the shape of the abstraction
is a union of hyperrectangles [10,3,25]. There exist also other types of monitors.
The most typical runtime monitoring approach for DNNs is to build a logic on
top of the DNN, where the logic inspects some of the DNN features and tries to
access the decision quality. Popular approaches in this direction are the baseline
of Hendrycks et al. [9] that looks at the output softmax value and ﬂags it as
problematic if lower than a threshold, or the ODIN approach that improves on
it using temperature scaling [18]. Further, [17] looks at intermediate layers of a
DNN and assumes that their features are approximately Gaussian-distributed.
With that, they use the Mahanalobis distance as a conﬁdence score for adversar-
ial or OoD detection. The work of [17] is considered the practical state-of-the-art
in the domain. In another direction, researchers have attempted to measure the
uncertainty of a DNN on its decisions, using Bayesian approaches such as drop

4

C.-H. Cheng et al.

out at runtime [6] and ensemble learning. Deep Ensembles [15] achieve state-
of-the-art uncertainty estimation but at a large computational overhead (since
one needs to train many models), thus recent work attempts to mitigate this
with various ideas [5,8]. Although the above results surely have their beneﬁts,
for complex monitoring techniques, the decision boundary is never a single value
but rather a complex geometric shape. For this, we observe a strong need in
systematic testing over the decision boundaries (for rejecting an input or not),
which is reﬂected in this work by testing or training against unsupported corners
of a monitor.

3 Preliminaries

Let N and R be the sets of natural and real numbers. To refer to integer intervals,
we use [a · · · b] with a, b ∈ N and a ≤ b. To refer to real intervals, we use [a, b]
with a, b ∈ R ∪ {−∞, ∞} and if a, b ∈ R, then a ≤ b. We use square bracket
when both sides are included, and use round bracket to exclude end points (e.g.,
[a, b) for excluding b). For n ∈ N \ {0}, Rn def= R × · · · × R
is the space of real
(cid:125)

(cid:124)

(cid:123)(cid:122)
n times

coordinates of dimension n and its elements are called n-dimensional vectors.
We use x = (x1, . . . , xn) to denote an n-dimensional vector.

Feedforward Neural Networks. A neuron is an elementary mathematical func-
tion. A (forward) neural network f def= (gL, . . . , g1) is a sequential structure of
L ∈ N \ {0} layers, where, for i ∈ [1 · · · L], the i-th layer comprises di neurons
and implements a function gi : Rdi−1 → Rdi. The inputs of neurons at layer i
comprise (1) the outputs of neurons at layer (i − 1) and (2) a bias. The outputs
of neurons at layer i are inputs for neurons at layer i + 1. Given a network input
x ∈ Rd0, the output at the i-th layer is computed by the function composition
f i(x) def= gi(· · · g2(g1(x))). Therefore, f L(x) is the output of the neural network.
We use f i

j (x) to extract the j-th value from the vector f i(x).

Abstraction-based Monitors using Boxes [10,3,25]. In the following, we present
the simplistic deﬁnition of abstraction-based monitors using multiple boxes. The
deﬁnition is simpliﬁed in that we assume the monitor operates on all neurons
within a given layer, but the technique is generic and can be used to monitor a
subset of neurons across multiple layers.

For a neural network f whose weights and bias related to neurons are ﬁxed,
def= {(x, y) | x ∈ Rd0, y ∈ RdL} be the corresponding training dataset.
let Dtrain
We call B def= (cid:2)[a1, b1], · · · , [an, bn](cid:3) an n-dimensional box, where B is the
set of points {(x1, . . . , xn)} ⊆ Rn with ∀i ∈ [1 · · · n] : xi ∈ [ai, bi]. Given a
neural network f and the corresponding training dataset, let k be a positive
def= {B1, . . . , Bk} is a k-boxed
integer constant and l ∈ [1 · · · L]. Then Bk,l,δ
abstraction monitor over layer l with buﬀer vector δ def= (δ1, . . . , δdl ),
provided that Bk,l,δ satisﬁes the following properties.

Prioritizing Corners in OoD Detectors via Symbolic String Manipulation

5

1. ∀i ∈ [1 · · · k], Bi is a dl-dimensional box.
2. ∀(x, y) ∈ Dtrain, there exists i ∈ [1 · · · k] such that f l(x) ∈ Bi.
3. ∀i ∈ [1 · · · k], let Bi be (cid:2)[a1, b1], · · · , [adl , bdl ](cid:3). Then

– for every j ∈ [1 · · · dl], exists (x, y) ∈ Dtrain such that aj ≤ f l

j(x) ≤

– for every j ∈ [1 · · · dl], exists (x(cid:48), y(cid:48)) ∈ Dtrain such that bj −δj ≤ f l

j(x(cid:48)) ≤

aj + δj, and

bj.

The three conditions stated above can be intuitively explained as follows:
Condition (1) ensures that any box is well formed, condition (2) ensures that
for any training data point, its feature vector at the l-th layer falls into one
of the boxes, and (3) the construction of boxes is relatively tight in that for
any dimension, there exists one training data point whose j-th dimension of its
feature vector is close to (subject to δj) the j-th lower-bound of the box; the
same condition also holds for the j-th upper-bound.

Monitoring. Given a neural network f and the boxed abstraction monitor Bk,l,δ,
in runtime, the monitor rejects an input x(cid:48) if (cid:54) ∃i ∈ [1 · · · k] : f l(x(cid:48)) ∈ Bi.
That is, the feature vector of x(cid:48) at the l-th layer is not contained by any box. As
the containment checking f l(x(cid:48)) ∈ Bi simply compares f l(x(cid:48)) against the box’s
lower and upper bounds on each dimension, it can be done in time linear to the
number of neurons being monitored.

Example 1. Consider the set {f l(x) | (x, y) ∈ Dtrain} = {(0.1, 2.9), (0.3, 2.6), (0.6,
2.3), (0.8, 2.8), (0.9, 2.1), (2.1, 0.1), (2.2, 0.7), (2.3, 0.3), (2.6, 0.6), (2.9, 0.2), (2.7, 0.9)}
of feature vectors obtained at layer l that has only two neurons: Fig. 1 shows
B2,l,δ = {(cid:2)[0, 1], [2, 3](cid:3), (cid:2)[2, 3], [0, 1](cid:3)}, a 2-boxed abstraction monitor with δ =
(0.15, 0.15). The area inﬂuenced by δ is visualized in yellow.

Corners within monitors. As a monitor built
from boxed abstraction only rejects an input
if the feature vector falls outside the box,
the borders of the box actually serve as a
proxy for the boundary of the operational
design domain (ODD) - anything inside a
box is considered acceptable. With this con-
cept in mind, we are interested in ﬁnding
test inputs that can lead to corners of
these boxes. As shown in Fig. 1, for the box
(cid:2)[0, 1], [2, 3](cid:3), the bottom left corner is not oc-
cupied by a feature vector produced from any
training data point.

Fig. 1: An example of two-boxes,
where corners are deep-yellow ar-
eas.

We now precise the deﬁnition of corners.

Given a box Bi = (cid:2)[a1, b1], · · · , [adl, bdl ](cid:3) ∈ Bk,l,δ, the set of corners associ-
ated with Bi is CBi

def= {(cid:2)[α1, β1], · · · , [αdl , βdl ](cid:3)} where ∀j ∈ [1 · · · dl], either

– [αj, βj] = [aj, aj + δj], or
– [αj, βj] = [bj − δj, bj].

6

C.-H. Cheng et al.

Fig. 2: Partition the boxed monitor and encode every region using BDDs. A black dot
represents a feature vector generated from a training data point.

Without surprise, the below lemma reminded us the well known problem of
combinatorial explosion, where the number of corners, although linear to the
number of boxes, is exponential to the number of dimensions.
Lemma 1. Given Bk,l,δ, (cid:80)k
with the monitor, equals k · 2dl .

1 |CBi|, i.e., the total number of corners associated

, exists (x, y) ∈ Dtrain such that ∀j ∈ [1 · · · dl] : f l

Given the set CBi of corners associated with Bi, deﬁne C s
Bi

⊆ CBi to be the
(training-data) supported corners where for each (cid:2)[α1, β1], · · · , [αdl , βdl ](cid:3)
in C s
j(x) ∈ [αj, βj]. The
Bi
set of (training-data) unsupported corners C u
is the set complement, i.e.,
Bi
def= CBi \ C s
C u
. As an example, consider the box Bi in Fig. 2(a). The set of
Bi
Bi
is {(cid:2)[a1, a1 +δ1], [b2 −δ2, b2]], (cid:2)[b1 −δ1, b1], [a2, a2 +δ2]]},
unsupported corners C u
Bi
i.e, the top-left corner and the bottom-right corner.

An unsupported corner reﬂects the possibility of having an input xop in op-
eration time, where the DNN-computed lth-layer feature vector f l(xop) falls into
the corner of the monitor. It reﬂects additional risks, as we do not know the pre-
diction result, but the monitor also will not reject the input. The consequence
of Lemma 1 implies that when we only have a ﬁnite budget for testing unsup-
ported corners, we need to develop methods to prioritize them, as detailed in
the following sections.

4 Unsupported Corner Prioritization under Single-Boxed

Abstraction

We ﬁrst consider the special case where only one box is used in the mon-
itoring. That is, we consider B1,l,δ = {B} where B = {(x1, . . . , xdl ) | x1 ∈
[a1, b1], . . . , xdl ∈ [adl , bdl ]}. The workﬂow is to ﬁrst consider encoding feature
vectors at the l-th layer into ﬁxed-length binary strings, in order to derive the
set of unsupported corners. Subsequently, prioritize the unsupported corners via
Hamming distance-based ﬁltering. The algorithm stated in this section serves
as the foundation for the general multi-boxed monitor setting detailed in later
sections.

Prioritizing Corners in OoD Detectors via Symbolic String Manipulation

7

4.1 Encoding feature vectors using binary strings
Given a ﬁnite-length Boolean string b ∈ {0, 1}∗, We use b[i···j] to denote the sub-
string indexed from i to j. For a single-boxed monitor B1,l,δ = {B} constructed
from Dtrain, let the φ-bit encoding (φ ≥ 2) be a function encφ : Rdl → {0, 1}φ·dl
that, for any x ∈ Dtrain, translates the feature vector f l(x) to a Boolean string b
(with length φ · dl) using the following operation: ∀j ∈ [1 · · · dl],

– if f l

j(x) ∈ [αj, αj + δj], then b[φ(j−1)+1···φj] = 0 · · · 0
(cid:124) (cid:123)(cid:122) (cid:125)
φ times

;

– else if f l

j(x) ∈ [βj − δj, βj], then b[φ(j−1)+1···φj] = 1 · · · 1
(cid:124) (cid:123)(cid:122) (cid:125)
φ times

;

– otherwise, b[φ(j−1)+1···φj] = 0 · · · 0
(cid:124) (cid:123)(cid:122) (cid:125)
φ−τ times
j(x) ∈ [aj + δj + (τ −1)(bj −aj −2δj )
f l

φ −1

when

1 · · · 1
(cid:124) (cid:123)(cid:122) (cid:125)
τ times

, aj + δj + (τ )(bj −aj −2δj )

)

φ −1

The φ-bit encoding essentially considers f l(x) in dimension j, assigns the sub-
string with all 0s when f l
j(x) falls in the corner reﬂecting the lower-bound, assigns
with all 1s when f l
j(x) falls in the corner reﬂecting the upper-bound, and ﬁnally,
splits the rest interval of length bj − aj − 2δj into φ − 1 equally sized inter-
vals and assigns each interval with an encoding. Fig. 2 illustrates the result of
2-bit and 3-bit partitioning under a 2-dimensional boxed monitor. For point x
in Fig. 2(b), enc3(x) = 011111. The ﬁrst part “011” comes as when τ = 2,
1(x) ∈ [a1 + δ1 + (2−1)(b1−a1−2δ1)
, a1 + δ1 + (2)(b1−a1−2δ1)
f l
). The second part
“111” comes as f l
2(x) ∈ [β2 − δ2, β2]. Given an input x and its computed feature
vector f l(x), the time required for perfotming φ-bit encoding is in low degree
polynomial with respect to dl and φ.

3 −1

3 −1

4.2 BDD encoding and priortizing the unsupported corners

This section presents Algorithm 1, a BDD-based algorithm for identifying un-
supported corners. To ease understanding, we separate the algorithm into three
parts.

A: Encode the complete training dataset. Given the training dataset Dtrain and
the DNN function f , one can easily compute {b | b = encφ(f l(x)) where x ∈
Dtrain} as be the set of all binary strings characterizing the complete training
dataset. As each element in the set is a ﬁxed-length binary string, the set can
be compactly stored using Binary Decision Diagrams.

Precisely, as the length of a binary string b = encφ(f l(x)) equals φ · dl, in
our encoding we use φ · dl BDD variables, denoted as bv1, . . . , bvφdl , such that
bvi = true iﬀ b[i···i] = 1. Line 1 of Algorithm 1 performs such a declaration.
Lines 2 to 9 perform the BDD encoding and creation of the set Strain containing
all binary strings created from the training set. Initially (line 2) Strain is set to
be an empty set. Subsequently, generate the binary string (line 4), and encode a
set Sb which contains only the binary string (line 5-8). Finally, add Sb to Strain
(line 9).

8

C.-H. Cheng et al.

Algorithm 1 Priortizing unsupported corners using BDD
Input: Dataset Dtrain, DNN f , 1-box monitor B1,l,δ = {B}, φ, distance metric ∆
Output: The set {bu} of binary strings represented in BDD, reﬂecting unsupported
B for box B, with each bu distant to all training data encodings by at

corners C u
least ∆ + 1 bits.

.

(* Initialize to empty set *)

(* Reﬁne the set to contain only b *)

if b[m···m] = 1 then Sb ← BDD.and(Sb, bvm)
else Sb ← BDD.and(Sb, BDD.not(bvm))

(* Add Sb to the set *)

b ← encφ(f l(x))
Sb ← BDD.true
for all m ∈ [1 · · · φdl] do

1: Declare BDD variables bv1, . . . , bvφdl
2: Strain ← BDD.false
3: for all x ∈ Dtrain do
4:
5:
6:
7:
8:
9:
10: Sall.corners ← BDD.true
11: for all j ∈ [1 · · · dl] do
12:
13:
14:
15:

Sj0s ← BDD.true; Sj1s ← BDD.true
for all m ∈ [1 · · · φ] do

Strain ← BDD.or(Strain, Sb)

16:
17: Sunsup ← Sall.corners \ Strain
18: S≤∆
train ← Strain
19: for all n ∈ [1 · · · ∆] do
Slocal ← S≤∆
20:
train
21:
for all m ∈ [1 · · · φdl] do
train ← BDD.or(S≤∆
S≤∆
22:
23: return Sunsup \ S≤∆
train

Sj0s ← BDD.and(Sj0s, BDD.not(bvφ(j−1)+m))
Sj1s ← BDD.and(Sj1s, bvφ(j−1)+m)

Sall.corners ← BDD.and(Sall.corners, BDD.or(Sj0s, Sj1s))

(* BDD.setminus(·, ·) operation *)

train, BDD.exists(Slocal, bvm))

B: Derive the set of unsupported corners. Lines 10 to 17 of Algorithm 1 computes
Sunsup, where each binary string in Sunsup corresponds to an unsupported corner.
The set is computed by a set diﬀerence operation (line 17) between the set of all
corners Sall.corners and Strain. Following the encoding in Section 4.1, we know
}dl. As an example, in
that the set of all corners corresponds to {0 · · · 0
(cid:124) (cid:123)(cid:122) (cid:125)
φ times

, 1 · · · 1
(cid:124) (cid:123)(cid:122) (cid:125)
φ times

Fig. 2(b), the set of all corners equals {000000, 000111, 111000, 111111}. Lines 10
to 16 of Algorithm 1 describe how such a construction can be done symbolically
using BDD, where the number of BDD operations being triggered is linear to
φ·dl. The set Sj0s, after the inner loop (line 13-15), contains the set of all possible
(similarly
Boolean words with the restriction that b[φ(j−1)+1···φj] equals 0 · · · 0
(cid:124) (cid:123)(cid:122) (cid:125)
φ times

Sj1s for having 1s). The “BDD.or” operation at line 16 performs a set union
operation between Sj0s and Sj1s, to explicitly allow two types of possibilities
within b[φ(j−1)+1···φj].

Prioritizing Corners in OoD Detectors via Symbolic String Manipulation

9

C: Filter unsupported corners that are close to training data. Although at line 17
of Algorithm 1, all unsupported corners are stored compactly inside the BDD,
the implication of Lemma 1 suggests that the number of unsupported corners
can still be exponential. Therefore, we are interested in further ﬁltering out
some unsupported corners and only keeping those unsupported corners that are
distant from the training data.

Consider again the example in Fig. 2(b), where Sunsup is the symbolic rep-

resentation of two strings, namely

– 000111 reﬂecting the top-left corner, and
– 111000 reﬂecting the bottom-right corner.

The algorithm thus should keep 000111 and ﬁlter 111000, as the bottom-right
corner has a training data x(cid:48) being close-by.

The ﬁnal part of Algorithm 1 (starting at line 18) describes how to per-
form such an operation symbolically by utilizing the Hamming distance on the
binary string level. Consider again the example in Fig. 2(b), where for train-
ing data x(cid:48), enc3(f l(x(cid:48))) = 011000. The Hamming distance between “011000”
and the bottom-right corner encoding “111000” equals 1. For the top-left corner
having its encoding being 000111, there exists only data points whose encoding
(e.g., x has an encoding of 011111) has a Hamming distance of 2. Therefore, by
ﬁltering out the elements with Hamming distance 1, only the top-left corner is
kept.

Within Algorithm 1, line 18 maintains S≤∆

train as a BDD storing every binary
string that has another binary string in Strain such that the Hamming distance
between these two is at most ∆. Initially, S≤∆
train is set to be Strain, reﬂecting
the case of Hamming distance being 0. The loop of Line 19 is executed ∆ times
to gradually increase S≤∆
train to cover strings with Hamming distance from 1 up
to ∆.

Within the loop, ﬁrst a local copy Slocal is created (line 20). Subsequently,
enlarging the set by a Hamming distance 1 can be done by the inner loop within
line 21-22: for each variable index m, perform existential quantiﬁcation over the
local copy to get the set of binary strings that is insensitive at variable bvm. As
an example, if Slocal = {011000}, then performing existential quantiﬁcation on
the ﬁrst variable generates a set “{θ11000 | θ ∈ 0, 1}”, and performing existential
quantiﬁcation on the second variable generates another set “{0θ1000 | θ ∈ 0, 1}”.
A union over all these newly generated sets returns the set of strings whose
Hamming distance to the original “011000” is less or equal to 1.

Finally, line 23 performs another set diﬀerence to remove elements in Sunsup
train, and the resulting set is returned as the output of the

that is present in S≤∆
algorithm.

4.3 Corner prioritization with multi-boxed abstraction monitors

In the previous section, we focus on ﬁnding corners within a box, where the
corners are distant (by means of Hamming distance) to DNN-computed feature
vectors from the training dataset. Nevertheless, when the monitor uses multiple

10

C.-H. Cheng et al.

boxes, is it possible that the corner being prioritized in one box has been covered
by another box? An example can be found in Fig. 3, where the monitor contains
two boxes B1 and B2. If the algorithm applied on B1 proposes corner c1 to be
tested, it would be a waste as c1 lies inside B2.

We propose a lazy approach to mediate this problem - whenever a corner
proposal is created from one box, use a strengthened condition and check if
some part of the corner is deep inside another box (subject to δ). Precisely, given
Bk,l,δ, provided that Algorithm 1 applied on Bi = (cid:2)[a1, b1], · · · , [adl , bdl ](cid:3) ∈ Bk,l,δ
suggests an unsupported corner c ∈ C u
whose corresponding binary string
Bi
equals b, conduct the following:
1. Given b, ﬁnd a vertex v = (v1, . . . , vdl ) in box Bi that is also in the proposed

corner c. Precisely, for ∀j ∈ [1 · · · dl],

– if b[φ(j−1)+1···φj] = 0 · · · 0
(cid:124) (cid:123)(cid:122) (cid:125)
φ times
– Otherwise, set vj to be bj.

, set vj to be aj.

2. Discard the corner proposal on c, whenever there exists Bi(cid:48) = (cid:2)[a(cid:48)

1], · · · ,
(cid:54)= i, such that the following holds: ∀j ∈ [1 · · · dl] :

1, b(cid:48)

, b(cid:48)
dl

[a(cid:48)
dl
j + δj < vj < b(cid:48)
a(cid:48)

](cid:3) ∈ Bk,l,δ, i(cid:48)
j − δj.
The time complexity for rejecting a corner
proposal is in low degree polynomial:

– For step (1), assigning each vj sums up

the time O(dl).

– For step (2), the containment check is
done on every other box (the number
of boxes equals k) over all dimensions
(size dl), leading to the time complexity
O(k · dl).

Improving the DNN against

5
the Unsupported Corners

Fig. 3: Two overlapping boxes.

As unsupported corners represent regions in
the monitor where no training data is close-by, any input whose feature vector
falls in that corner will not be rejected by the monitor, leading to safety concerns
if the prediction is incorrect. For classiﬁcation tasks, one possible mediation is
to explicitly ensure that any input whose feature vector falls in the unsupported
corner does not cause the DNN to generate a strong prediction over a particular
class.

i

As an example, if the DNN f is used for digit recognition and dL equals 10
with each f (L)
indicating the possibility of the character being i−1, it is desirable
to let an input x, whose feature vector falls inside the unsupported corner, to
produce f (L)
10 (x) ∼= 0.1, i.e., the DNN is not certain
on which class this input belongs to. One can naively retrain the complete DNN
against such an input x. Nevertheless, if the DNN is completely retrained, the

(x) ∼= . . . ∼= f (L)

(x) ∼= f (L)

2

1

Prioritizing Corners in OoD Detectors via Symbolic String Manipulation

11

Algorithm 2 DNN modiﬁcation against unsupported corners under 1-boxed
abstraction monitor (classiﬁcation network with one-hot output encoding)

Input: Dataset Dtrain, DNN f = (gL, . . . , g1), 1-boxed monitor B1,l,δ, S ⊂ Sunsup \
created from Algorithm 1, the number of samples ρ per unsupported corner.

S≤∆
train

def= {(f l(x), y)) | (x, y) ∈ Dtrain}

Output: Updated DNN f (cid:48).
1: Create dataset Dmodif y
2: for all b ∈ S do
3:
4:
5:
6:

Let c def= (cid:2)[α1, β1], · · · , [αdl , βdl ](cid:3) ∈ Rdl be the corresponding corner of b.
Sample ρ points p1, . . . , pρ from c.
for all i ∈ [1 · · · ρ] do

Dmodif y ← Dmodif y ∪ {(pi, ( 1
dL

, . . . , 1
dL

))}

7: Improve gL, . . . gl+1 to ˆgL, . . . ˆgl+1 by training against Dmodif y
8: Return f (cid:48) def= (ˆgL, . . . ˆgl+1, gl, . . . , g1)

created monitor Bk,l,δ is no longer valid, as the parameters before layer l have
been changed due to re-training.

Towards this issue, Algorithm 2 presents a local DNN modiﬁcation scheme3
where the re-training is only done between layers l + 1 and L. As the new DNN
share the same function with the existing one from layer 1 to layer l, previously
constructed 1-boxed monitor remains applicable in the new DNN.

As re-training is only done over a sub-network between layers l +1 and L, the
input for training the sub-network is the output of layer l. Therefore, reﬂected
at line 1, one prepares a new training dataset where the input is f l(x). The
input for Algorithm 2 also contains S, which is a subset of unsupported corners
derived from Algorithm 1. Lines 2 to 6 translate each binary string in S into
an unsupported corner (line 3) and sample ρ points (line 4) to be added to the
new training dataset. As stated in the previous paragraph, we wish the result of
these points to be unbiased for any output class. Therefore stated at line 6, the
corresponding label, under the assumption where Dtrain uses one-hot encoding,
should be ( 1
dL

, . . . , 1
dL

).

6 Evaluation

This section aims to experimentally answer two questions about the unsupported
corners generated by the method in Section 4. The ﬁrst question is regarding the
behavior of feature vectors in the unsupported corners reﬂected in the output
(Section 6.1). The second question is regarding generating inputs that can lead
to these unsupported corners (Section 6.2).

Speciﬁcally, we consider monitors built on the penultimate layer of two neural
networks, trained on benchmarks MNIST [16] and GTSRB [11], respectively, to

3 For simplicity, we only show the algorithm for 1-boxed abstraction monitor, while
extensions for multi-boxed abstraction monitor can follow the same paradigm stated
in Section 4.3.

12

C.-H. Cheng et al.

Table 1: Hyper-parameter setting in the experiments
dataset # of monitored neurons m :# unsupported corners ρ: # collected samples per corner
1000
MNIST
1000
GTSRB

10
10

40
84

classify handwritten digits (0-9) and traﬃc signs. Following Algorithm 1, we ﬁrst
encode the monitors’ supported corners using BDD representation. Subsequently,
compute the unsupported corners using symbolic set diﬀerence operations. We
use Pytorch4 to train the DNN and use the python-based BDD library dd5 for
encoding the binary strings into the BDD.

6.1 Understanding unsupported corners

This subsection focuses on understanding the output softmax (probability) val-
ues for the feature vectors from unsupported corners. We take m unsupported
corners and from each of them uniformly pick ρ samples in the corresponding
corner. The hyper-parameters used in the experiments are shown in Table 1.

We ﬁrst examine if the DNN can output overconﬁdent softmax values for
these samples. From the statistical results, as shown in the left part of Fig. 4, one
can ﬁnd that samples from many unsupported corners (with Hamming distance
larger than 3 from the training dataset) are assigned a high softmax value.
This conﬁrms our conjecture that additional local training is needed to suppress
high-conﬁdent output against unsupported corners. After applying Algorithm 2
for ﬁne-tuning the after-monitored-layer sub-network, these unsupported cases
are all assigned an averaged softmax value of 1
10 , as shown in the right part
of Fig. 4. Interestingly, the ﬁne-tuning does not deteriorate the accuracy of the
neural network on the original training and test sets: We observe a shift from
the original accuracy of 99.34% (98.8%) on the training (test) dataset to a new
one of 99.24% (98.84%).

Remark 1. The repair of the sub-network essentially equips the original network
an additional ability of identifying out-of-distribution samples (around the area
of unsupported corners) by observing whether the softmax value of prediction is
close to 1
dL

or not.

6.2 From test case proposal to test case generation

This subsection explores two possibilities for generating inputs that yield features
in speciﬁc unsupported corners of the monitored layer.

– The ﬁrst method is to verify whether the maximum or minimum activation
value of each monitored neuron is responsible for a particular segment or
local area of the input, hereafter referred to as Neuron-Wise-Excited-Input-
Feature (NWEIF). If such a connection exists, since a corner is a combination

4 https://pytorch.org/
5 https://github.com/tulip-control/dd

Prioritizing Corners in OoD Detectors via Symbolic String Manipulation

13

Fig. 4: Statistics of numbers of samples (picked from unsupported corners) per softmax
value interval on MNIST (left: before re-training; right: after re-training).

of the maximum/minimum activation values of each neuron, then a new
input can be formed by combining the NWEIFs of each neuron.

– The second is to apply optimization techniques. Given an image in the train-
ing dataset, perform gradient descent to ﬁnd a modiﬁcation over the image
such that the modiﬁed image generates a feature within a given unsupported
corner.

Neuron-wise excited input-feature combination We applied the layer-
wise relevance propagation (LRP) [20] technique to interpret the images that
reach the maximum and minimum ﬁve values of a neuron. LRP is one of the
back-propagation techniques used for redistributing neuron activation values at
one layer to its precedent layers (possible up to the input layer). In a nutshell,
it explains which parts of the input contribute to the neuron’s activation and to
what extent.

Discussion The results in Fig. 5 show that it is diﬃcult for humans to compose
new inputs based on NWEIF. The ﬁrst and second rows in each bold-black
block are the original images and corresponding heat maps interpreted by LRP.
Although LRP can help us identify regions or features, it is very diﬃcult to
precisely associate one neuron with one speciﬁc input-feature. We can observe in
Fig. 5 that for the 20km/h speed sign, the area that leads to maximum activation
has considerable overlap with the area that leads to minimum activation. This
makes a precise association between neurons and features diﬃcult, justifying the
need of using other methods such as optimization-based image generation for
testing and Algorithm 2 for local training over unsupported corners.

Optimization-based test case generation Finally, we create images corre-
sponding to corners by using an optimization method, similar to the ones used
for adversarial examples generation [24]. Overall, the generated test case should
allow the DNN to (1) fall inside the box of the unsupported corner and to (2)
be conﬁdent in predicting a wrong class. In our implementation, the previously

14

C.-H. Cheng et al.

Fig. 5: LRP interpretation example: each bold-black block contains the inputs reaching
the minimum 5 (red sub-block) and the maximum 5 (blue sub-block) activation values
of a neuron; the top two and the bottom two are from MNIST and GTSRB, respectively.

mentioned two objectives are integrated as a loss function, which is optimized
(by minimizing the loss) with respect to the input image. We refer readers to the
appendix for details regarding how such a method is implemented. Figure 6 il-
lustrates examples of original and perturbed images, where for the bottom-right
example, the perturbed images not only falls into a particular corner, but the re-
sulting prediction also changes from the initially correct “1” to the incorrect “4”.
We observe that when the buﬀer δ around the box is small, it can be diﬃcult
for the adversarial testing method to generate images that fall into a speciﬁc
corner. However, we are unable to state that it is impossible to generate such an
input; the problem can only be answered using formal veriﬁcation. This further
justiﬁes the need for local DNN training.

7 Concluding Remarks

In this paper, we address the issue of testing OoD monitors built from boxed
abstractions of feature vectors from the training data, and we show how this
testing problem could be reduced to ﬁnding corners in hyperrectangle distant
from the available training data in the feature space. The key novelty lies in a
rigorous method for analyzing the corners of the monitors and detecting whether
a corner is supported or not according to the input in the training data set,
generating feature vectors located in the corner. To the best of our knowledge,
it is the ﬁrst approach for testing the decision boundary of a DNN monitor,
where the test prioritization scheme is based on corners (of the monitor) that
have no input data being close-by. The other important result is the DNN repair
scheme (via local training) to incorporate the distant-yet-uncovered corners. To
this end, we have developed a tool that provides technical solutions for our OoD
detectors based on boxed abstractions. Our experiments show the eﬀectiveness
of our method in diﬀerent applications.

This work raises a new research direction on rigorous engineering of DNN
monitors to be used in safety-critical applications. An important future direction
is the reﬁnement of boxed abstractions: By considering the unrealistic corners,

OriginLRPmin. 5max. 5min. 5max. 5OriginLRPPrioritizing Corners in OoD Detectors via Symbolic String Manipulation

15

Fig. 6: Using adversarial testing to generate images whose feature vectors fall into a
particular corner. The original images are shown on the left, and the perturbed ones on
the right. We also show the predicted classes, and for the perturbed images additionally
the distance of their features from the corner point, as well as the distance from the
unperturbed image.

we can reﬁne the abstraction by adding more boxes to remove them. Another
direction is to use some probability estimation method to prioritize corners rather
than using Hamming distance.

(Acknowledgement) This work is funded by the Bavarian Ministry for Eco-
nomic Aﬀairs, Regional Development and Energy as part of a project to support
the thematic development of the Fraunhofer Institute for Cognitive Systems.
This work is also supported by the European project Horizon 2020 research and
innovation programme under grant agreement No. 956123.

References

1. S. Abrecht, L. Gauerhof, C. Gladisch, K. Groh, C. Heinzemann, and M. Woehrle.
Testing deep learning-based visual perception for automated driving. ACM TCPS,
5(4):1–28, 2021.

2. R. E. Bryant. Symbolic boolean manipulation with ordered binary-decision dia-

grams. CSUR, 24(3):293–318, 1992.

3. C.-H. Cheng, C.-H. Huang, T. Brunner, and V. Hashemi. Towards safety veriﬁ-
cation of direct perception neural networks. In DATE, pages 1640–1643. IEEE,
2020.

4. C.-H. Cheng, C.-H. Huang, and H. Yasuoka. Quantitative projection coverage for
testing ml-enabled autonomous systems. In ATVA, pages 126–142. Springer, 2018.
5. M. Dusenberry, G. Jerfel, Y. Wen, Y. Ma, J. Snoek, K. Heller, B. Lakshmi-
narayanan, and D. Tran. Eﬃcient and scalable bayesian neural nets with rank-1
factors. In ICML, pages 2782–2792. PMLR, 2020.

6. Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing
model uncertainty in deep learning. In ICML, pages 1050–1059. PMLR, 2016.

class: 5class: 5 ||fl(x)pc||2=33.34 ||xx0||2 = 6.4class: 9class: 4 ||fl(x)pc||2=23.98 ||xx0||2 = 6.41class: 4class: 4 ||fl(x)pc||2=33.53 ||xx0||2 = 6.38class: 1class: 4 ||fl(x)pc||2=22.62 ||xx0||2 = 6.616

C.-H. Cheng et al.

7. I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.
8. M. Havasi, R. Jenatton, S. Fort, J. Z. Liu, J. Snoek, B. Lakshminarayanan, A. M.
Dai, and D. Tran. Training independent subnetworks for robust prediction. arXiv
preprint arXiv:2010.06610, 2020.

9. D. Hendrycks and K. Gimpel. A baseline for detecting misclassiﬁed and out-of-
distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
10. T. A. Henzinger, A. Lukina, and C. Schilling. Outside the box: Abstraction-based

monitoring of neural networks. arXiv preprint arXiv:1911.09032, 2019.

11. S. Houben, J. Stallkamp, J. Salmen, M. Schlipsing, and C. Igel. Detection of
traﬃc signs in real-world images: The German Traﬃc Sign Detection Benchmark.
In IJCNN, pages 1–8. IEEE, 2013.

12. X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M. Wu, and
X. Yi. A survey of safety and trustworthiness of deep neural networks: Veriﬁcation,
testing, adversarial attack and defence, and interpretability. Computer Science
Review, 37:100270, 2020.

13. G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Reluplex:
An eﬃcient smt solver for verifying deep neural networks. In CAV, pages 97–117.
Springer, 2017.

14. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980, 2014.

15. B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474,
2016.

16. Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database, 2010.
17. K. Lee, K. Lee, H. Lee, and J. Shin. A simple uniﬁed framework for detecting
out-of-distribution samples and adversarial attacks. In NeurIPS, volume 31, 2018.
18. S. Liang, Y. Li, and R. Srikant. Enhancing the reliability of out-of-distribution
image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.
19. L. Ma, F. Zhang, M. Xue, B. Li, Y. Liu, J. Zhao, and Y. Wang. Combinatorial

testing for deep learning systems. arXiv preprint arXiv:1806.07723, 2018.

20. G. Montavon, A. Binder, S. Lapuschkin, W. Samek, and K.-R. Müller. Layer-wise
relevance propagation: an overview. Explainable AI: interpreting, explaining and
visualizing deep learning, pages 193–209, 2019.

21. K. Pei, Y. Cao, J. Yang, and S. Jana. Deepxplore: Automated whitebox testing of

deep learning systems. In SOSP, pages 1–18. ACM, 2017.

22. V. Riccio and P. Tonella. Model-based exploration of the frontier of behaviours for

deep learning system testing. In FSE, pages 876–888. ACM, 2020.

23. Y. Sun, X. Huang, D. Kroening, J. Sharp, M. Hill, and R. Ashmore. Structural
test coverage criteria for deep neural networks. ACM TECS, 18(5s):1–23, 2019.
24. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
arXiv preprint

Intriguing properties of neural networks.

and R. Fergus.
arXiv:1312.6199, 2013.

25. C. Wu, Y. Falcone, and S. Bensalem. Customizable reference runtime monitoring
of neural networks using resolution boxes. arXiv preprint arXiv:2104.14435, 2021.

Prioritizing Corners in OoD Detectors via Symbolic String Manipulation

17

Appendix

A. Optimization-based test case generation

In this section, we describe our optimization-based method for creating test
cases. This method is similar to techniques used for generating adversarial ex-
amples in DNNs.

An adversarial attack tries to perturb an input x so that its classiﬁcation
changes, while simultaneously staying close to the original input. This is achieved
by an optimization method with minimizing a loss function that tries to enforce
a miss-classiﬁcation. Apart from that, adversarial attacks also try to ensure that
the perturbed input will remain close enough to the original input (since this is
the deﬁnition of adversarial examples).

For our case, consider an input x with class y, DNN f , and let f (l)(x) be the
output of the l-th layer we want to monitor and take features from. Let pc be
a point in the speciﬁed corner c that we want to approximate. We consider the
following loss function: loss(x, y, pc) = −λ · crossentropy(f L(x), y) + ||f (l)(x) −
pc||2. In this loss, the ﬁrst term is the standard cross-entropy loss for classi-
ﬁcation, while the second term is the distance of the l-th layers features from
the corner point pc with respect L2 norm. The number λ ≥ 0 is a parameter
balancing the two objectives. Hence, we can minimize this loss to generate test
cases using the Adam optimizer [14].

To understand this loss function, note that in a standard adversarial attack
only the cross-entropy term is present. The attack tries to minimize the term −λ·
crossentropy(x, y) (equivalently maximizing the cross-entropy), thus resulting
in a miss-classiﬁcation, e.g. a successful adversarial example. With our loss now,
when it is minimized, we achieve the miss-classiﬁcation objective, and at the
other hand, minimization is achieved by keeping the distance term ||f (l)(x)−pc||2
small. Thus, we achieve a miss-classiﬁcation, while f (l)(x) is close to the corner
point pc, which is what we aim to achieve.

