A software toolkit and hardware platform for investigating and
comparing robot autonomy algorithms in simulation and reality

Asher Elmquist1, Aaron Young2, Ishaan Mahajan2, Kyle Fahey2, Abhiraj Dashora2
Sriram Ashokkumar2, Stefan Caldararu2, Victor Freire3, Xiangru Xu4, Radu Serban5, and Dan Negrut6

2
2
0
2

n
u
J

4
1

]

O
R
.
s
c
[

1
v
7
3
5
6
0
.
6
0
2
2
:
v
i
X
r
a

Abstract— We describe a software framework and a hard-
ware platform used in tandem for the design and analysis
of robot autonomy algorithms in simulation and reality. The
software, which is open source, containerized, and operating
system (OS) independent, has three main components: a ROS
2 interface to a C++ vehicle simulation framework (Chrono),
which provides high-ﬁdelity wheeled/tracked vehicle and sensor
simulation; a basic ROS 2-based autonomy stack for algorithm
design and testing; and, a development ecosystem which en-
ables visualization, and hardware-in-the-loop experimentation
in perception, state estimation, path planning, and controls. The
accompanying hardware platform is a 1/6th scale vehicle aug-
mented with reconﬁgurable mountings for computing, sensing,
and tracking. Its purpose is to allow algorithms and sensor
conﬁgurations to be physically tested and improved. Since
this vehicle platform has a digital twin within the simulation
environment, one can test and compare the same algorithms
and autonomy stack in simulation and reality. This platform has
been built with an eye towards characterizing and managing the
simulation-to-reality gap. Herein, we describe how this platform
is set up, deployed, and used to improve autonomy for mobility
applications.

I. INTRODUCTION

A. Motivation

This contribution is concerned with the idea of using
simulation to facilitate the task of developing algorithms that
improve the autonomy of wheeled and tracked robots oper-
ating on rigid or deformable terrains. The value proposition
is tied to the cost-effective manner in which data can be
generated in simulation as well as to the ease and safety with
which candidate solutions can be tested and iterated upon.
Using simulation in robotics requires a dynamics engine and
a model [1]. Should one make the necessary investment to
understand both how the simulator and model should be
conﬁgured, simulation provides insights that are difﬁcult to
obtain in physical testing, e.g., complete state information
and quantitative insights about the interaction between the

1Ph.D.

student with the Department of Mechanical Engineer-
the University of Wisconsin-Madison, Madison WI, USA

ing at
amelmquist@wisc.edu

2Undergraduate Student with the Simulation-Based Engineering Lab at

the University of Wisconsin-Madison, WI, USA

3Masters

student with the Department of Mechanical Engineer-
the University of Wisconsin-Madison, Madison WI, USA

ing at
freiremelgiz@wisc.edu

4Assistant
at
Engineering
xiangru.xu@wisc.edu

Professor

the University

in

the

Department

of Mechanical
of Wisconsin-Madison, WI, USA

5Scientist in the Department of Mechanical Engineering at the University

of Wisconsin-Madison, Madison WI, USA serban@wisc.edu

6Professor in the Department of Mechanical Engineering at the University

of Wisconsin-Madison, Madison WI, USA negrut@wisc.edu

robot and environment it operates in. Unfortunately, these
insights do not always lead to decisions that work well in
reality due to the simulation-to-reality gap [2]. How to close
this sim-to-real gap remains an open problem, and solutions
have been proposed that include randomizing the experience
of the robot inside the simulator [3], [4], using adversarial
learning to capture unknown components of the model via
ghost external perturbations in an adversarial reinforcement
learning framework [5], using ensembles of models [6], using
a mix of simulation-generated and real-world data to train
robots [7], [8], etc.

Despite these and other similarly valuable contributions,
the community lacks an objective understanding of what
exactly produces the sim-to-real gap. Our contribution is
motivated by this observation and inspired by the belief
that an open source autonomy research testbed can be a
catalyst for research in two areas: improvement of algorithms
for autonomy in mobility; and understanding and mitigating
the sim-to-real gap. To that end, we describe herein an
infrastructure, i.e., a physical and software platform, that
allows one to experiment with an autonomy algorithm, e.g.
a sensor-fusion approach, and have the opportunity to do so
ﬁrst in simulation and then quickly on the actual robot. The
principle is to have one ROS 2 [9], [10] autonomy stack
that uses the same collection of perception, state estimation,
planning, and control algorithms both in simulation and in
the real world. This can enable one to perform controlled
tests to understand and evaluate the performance of the
algorithms.

B. Contribution

Our contribution is twofold. First, we established an
autonomy development environment and an accompanying
hardware platform (called ART, from “autonomy research
testbed”) that in tandem facilitate two types of technical pur-
suits: research into algorithms for autonomy in mobility, in
the context of wheeled and tracked vehicles; and quantitative
characterization of the sim-to-real gap in robotics. Second,
we developed an autonomy toolkit (ATK) which is a set of
command line tools for building the container system that
underpins ART.

The autonomy research testbed is containerized [11] to
provide an OS agnostic platform that fosters collaboration
and accelerates the research effort since it: 1) mitigates the
startup time for setting up an autonomy stack by providing
its bridges and interfaces to other system components, e.g.,
sensors and bridge to simulation; 2) abstracts the components

 
 
 
 
 
 
as follows. To contextualize this contribution, we provide
next an overview of similar community efforts. Section
II highlights the container build tools associated with this
project. Section III describes the autonomy research testbed
by outlining the autonomy stack and its structure, the bridge
to the simulator [19], the companion vehicle platform, and
twin. This information, along with the online
its digital
documentation referenced herein should be sufﬁcient for an
interesting party to replicate and deploy ART at a modest
cost should one have access to a 3D printer.

C. State of the Art

in spirit

MuSHR [20], an open-source race-car project, comes
closest
to ART. For simulation, MuSHR draws
on Gazebo [21]. It uses a 1/10th form-factor vehicle, has
one 2D lidar sensor, one stereo camera, and a Jetson Nano
processor. MuSHR is ROS-based and runs off a Linux
Ubuntu distribution. It has comprehensive documentation
and has been used in classes at University of Washington.
Another well known platform is MIT RACECAR [22], [23].
It uses a 1/10th form factor, ZED stereo camera, 2D lidar.
The software stack, which is ROS-anchored, is provided
as a Docker image. Simulation support comes via Gazebo.
MuSHR and RACECAR share the same pros and cons –
open source platform, ROS-backend, proven software stack;
and, respectively, small form factor, indoors use, software
and hardware infrastructure relatively difﬁcult
to expand
beyond the current vehicles. NVIDIA’s JetRacer [24] is a
hobbyist platform that comes in a small form factor (1/18th);
it uses a Raspberry Pi Camera V2, and runs off a Jetson Nano
processor. This is not necessarily for use as an autonomy
research platform, instead it draws on NVIDIA’s Jetpack,
which is a basic toolkit in Python.

The Cat [25] uses only IR sensors and is designed to chase
an IR LED mounted on a remote control car. It has a 1/10th
format, two IR sensors, a PIC18F2455 microcontroller; SSC-
32 servo controller. It has a MATLAB simulator customized
to the hardware platform at hand (changing hardware plat-
form requires re-write of the simulator); no other vehicles
or sensors can be exercised in the simulator. Karr [26] is
designed to only use a depth camera, and requires walls
on either side of the car to guide the vehicle along the
path. It
is a 1/10th form factor, uses NVIDIA’s Jetson
TX1 chip that comes with a developer kit targeting visual
computing running in a pre-ﬂashed Linux environment. Karr
relies on ROS and trains a neural net using Keras with a
Theano backend using real-life videos. There is no simulator
bundled, yet Carla [27] is mentioned as a next logical step
towards the goal of training in simulation.

Donkey Car [28] is a popular hobbyist project, which
is concerned with racing small, 1/16th or 1/10th, format
vehicles that mostly draw on Raspberry Pi 3b+ and a wide-
angle camera. It can be ﬁtted with Jetson Nano or TX2
with access to the NVIDIA developer kit and its ecosystem.
Donkeycar uses Python for controlling the car, and Unity
[29] as a simulator,
the latter building off the NVIDIA
PhysX simulation engine [30]. The Donkeycar infrastructure

Fig. 1: Fully equipped ART 1/6th scale autonomous vehicle
showing 3D printed reconﬁgurable mountings and support of
camera, 3D lidar, and Jetson Xavier NX, the later running
the autonomy stack.

of the autonomy stack thus enabling researchers to quickly
replace the algorithm(s) it embeds; 3) makes available a
proven simulation environment for tracked/wheeled vehicles,
operating on/off-road conditions by allowing communication
with Chrono [12]; and, 4) leverages a 1/6th scale vehicle
for real world testing that anchors the algorithm validation
exercises as well as the sim-to-real
inquiry efforts. The
hardware component of ART (the 1/6th scale vehicle shown
in Fig. 1) has been chosen for its topology, payload capacity,
and off-road ability. In term of its topology, the suspension
and steering mechanism closely mirror those of a full-size
vehicle. The scaled vehicle’s topology and payload capacity
ensure that it can host the same class of sensors encountered
on a full sized-vehicle, e.g. Velodyne VLP-16. Finally, with
a wider wheel base and tire format, the vehicle can operate
in off-road conditions.

From a high vantage point, our main accomplishment is
that a basic autonomy stack has been set up and Docker-
containerized. It can be used either with the associated vehi-
cle, to test autonomy algorithms, or in a simulator using the
Chrono digital twin. This is the core of ART. Additionally,
ATK provides the tools to build the ART ecosystem and
collect, visualize, and analyze data coming from ART, which
others can leverage for custom containerized environments.
ART and ATK allow one to (a) carry out research in the
sim-to-real gap; (b) generate data for data-driven solutions in
robotics; and (c) improve autonomy algorithms (perception,
state estimation, planning, controls) in plug-and-play fashion
by working exclusively on the ART vehicle, or by combining
physical testing and physics-based simulation. The simula-
tor which ART leverages, has the following features: easy
wheeled/tracked vehicle model set up via templates [13];
sensor simulation [14]; terradynamics support [15], [16];
support for multiple agents [17]; Python bindings [18].

The subsequent portions of the contribution are organized

is popular, see, for instance [31] for an offshoot. One project
that stands out in terms of form factor is PARV/MPAD;
at 1/8th is larger than all the rest, but smaller than the
platform discussed herein. While the project has modest
goals on the autonomy component, it is remarkable in that
it is a 3D printed hardware platform [32] that comes with
a basic autonomy stack called MPAD (Modular Package for
Autonomous Driving), which is Python based, uses OpenCV
with a Rasberry Pi camera, has Ultrasonic sensors, an IMU,
and runs off a Rasberry Pi 4 [33]. Vehicle control draws on an
Arduino Mega. A Raspberry Pi-based solution with Arduino
for vehicle control is described in [34]; it is small form
factor and works by neural-net-enabled image recognition in
conjunction with scaled-down trafﬁc signs (data not collected
trained with data collected by
in simulation, neural net
driving car around manually). Another hobbyist solution is
PiCar of SunFounder [35], which at less than 1/10th format is
an inexpensive platform that uses Raspberry Pi 4 and a wide-
angle USB camera, with controls designed in EzBlock [36],
an open platform for building intro-level electronic projects.
All solutions mentioned above are different in at least
one of the following four aspects: (1) project breadth of
scope; (2) scale and ﬂexibility of hardware platform; (3)
degree to which project embraces and promotes a simulation
component; (4) documentation and support. For (1), ART
is a researcher’s (instead of hobbyist’s) platform that uses,
but
the vehicle shown in Fig. 1. Of
the solutions mentioned above, only MuSHR has a similar
research purpose. For (2), we adopted a larger form factor
since the interest is on- and off-road mobility. Moreover, the
goal is enabling sensor fusion, experimenting with various
sensor packages, etc., which requires additional payload
capacity. For (3), this group is interested in simulation ﬁrst
and foremost, and the vehicle is not the goal, but rather a
means to an end. The ends are enabling research in autonomy
for mobility and better understanding and management of
the sim-to-real gap. For (4), a documentation infrastructure
details our solution [37].

limited to,

is not

Finally, this effort does not seek to establish our autonomy
testbed as a competitor to full-blown stacks produced by
companies such as Tesla, Waymo, or Motional, or made
available in Autoware [38] or Apollo [39]. Our goal is to
enable easy, plug-and-play research, that enlists the support
of a basic autonomy stack and a companion platform for
simulation-enabled research in autonomy. The research fa-
cilitated by ATK and ART pertains to advancing the state of
the art in relation to what simulation can and should do in
robotics; the physical platform provides the required reality
check and ground truth.

II. AUTONOMY TOOLKIT

The objective of ATK is to provide a modular and
portable framework for developing, testing, and deploying
autonomous algorithms in simulation and reality. This toolkit
is a Python package that leverages Docker by wrapping
Docker Compose to build and deploy a multi-container
system speciﬁcally designed for autonomy research. ATK

wraps Docker Compose in that all of the functionality of
Compose is still available; however, ATK also provides util-
ities, defaults, documentation, and examples speciﬁcally for
applications relating to autonomous algorithm development.
ATK is open-source, available on GitHub, and has been made
available through the Python Package Index (PyPI) [40].

A requirement of this toolkit is to be OS portable. To that
end, a containerized system, set up to leverage Docker, is
separated into “services” that can be combined to produce a
container network that supports complex interactions across
a variety of projects. To expedite the deployment process
to the physical hardware under test, the same containerized
system is used. The Docker interface, especially on Linux
systems, is extremely lightweight and introduces negligible
overhead for most applications. The development framework
is built into two main components: the ATK Python package
and the Docker containers themselves. The ATK package,
which helps generate the containers, is described in Section
II-A; the container system is described in Section II-B.

A. autonomy-toolkit Package

The package which generates the container system is, in
its simplest form, a wrapper of Docker Compose. Called
autonomy-toolkit, ATK for short,
this Python package is
publicly available and provides a cross-platform command-
line interface. ATK leverages the multi-container architecture
to provide optional, extensible, and customizable containers
to facilitate development of AVs that greatly increases the
generality of the development workﬂow. The only non-
Python requirement for the autonomy-toolkit is Docker. With
Docker available, the toolkit can be installed through typical
python mechanisms like pip. ATK works as follows: YAML
conﬁguration ﬁles are combined with defaults provided
through ATK into a conﬁguration ﬁle readable by Docker
Compose. From there, ATK simply makes calls to Docker
Compose based on the resulting output. All defaults provided
through ATK are customizable. Furthermore,
the generic
Docker Compose commands can be used in conjunction with
the generated conﬁg ﬁle, if desired, without ATK.

ATK was built with modularity and expandability in mind.
This means the functionality of the toolkit itself is agnostic
of the autonomy stack it is used for, i.e. it is not tied to
ART. Individual hardware platforms and control stacks may
implement their own containers and customize the default
conﬁgurations, provided they follow the few requirements
outlined in the documentation [37]. As ATK is agnostic of the
hardware platform and usage, it can also be used to generate
custom containers outside the original scope of the toolkit.
For instance, the simulation container to be discussed further
in Section III-A leverages ATK to create its own container for
Chrono. Other common use cases could include a container
for training data or an HD map, a deployment container for
speciﬁc hardware, an automated testing environment, etc.

B. Container System

Distributed with the ATK package itself are many pre-
deﬁned and customizable utilities that are used to generate

Fig. 2: ART environment, setup used on the physical vehicle.

the images and containers. In general, there are two primary
services that come directly with the ATK package: dev and
vnc.

dev: This is the primary component

that is used for
algorithm development. It is deﬁned by a custom Dockerﬁle
which utilizes build arguments speciﬁed through a conﬁg-
uration ﬁle to generate an image speciﬁc to the project
being developed. By default, dev builds on the ROS 2
Galactic image, the most recent ROS 2 distribution at the
time of writing. By default, dev is assumed to be the
primary container and, when launched, initializes a shell
environment. The directory that holds the conﬁguration ﬁle
is mounted into the container for data persistence upon
container termination. Other utilities and conﬁguration are
preformed to further enhance the generality of the toolkit
and remove OS dependence.

vnc: Since the isolation-based design of Docker results
in difﬁculty visualizing applications, a cross-platform visu-
alization system is used which requires no host setup. The
implemented solution leverages Virtual Network Computing
(VNC) within a separate service, making its usage optional.
To use VNC visualization, dev (or any other container run
using ATK) must simply attach to the same network and
conﬁgure its DISPLAY variable to that of the vnc hostname.
NoVNC, a browser based VNC client, can then be used to
view any displayed windows from dev from any internet
browser. Further explanation of the use, and a short tutorial
are provided with the ATK documentation [37].

is

Note that a networking layer

set up to facili-
tate distributed container interaction. This feature supports
hardware-in-the-loop experiments, where the simulation con-
tainer is deployed on separate hardware from the autonomy
stack. To facilitate this, we leverage Docker and Docker
Compose networking features. When services are deployed
on the same system,
the default network conﬁgurations
are used. However, when the services are deployed in a
distributed fashion, the network is customized to expose the
necessary ports to facilitate inter-computer communication.

Since the environment is containerized, it can be deployed
to a physical vehicle, with optional hardware-speciﬁc opti-
mizations. While ATK is general purpose, an example of the
containerized system can be seen in Figs. 2 and 3, which
illustrate the services when running on the real vehicle vs.
simulation, respectively.

Fig. 3: ART environment, setup used for simulation.

III. AUTONOMY RESEARCH TESTBED

A. Simulation

One of the motivations behind this effort was the use of
simulation as a means of designing and testing autonomous
algorithms. To support this, a bridge to Chrono [19] was
developed to allow direct integration within the container-
ized system for ART. The bridge, along with the custom
Chrono container, is an example of the modular environment
produced by ATK.

The Chrono ROS Bridge is a ROS 2 package that inte-
grates with Chrono to feed messages to and from the simu-
lation scenarios. The interface is built on JSON messages so
users can send any data between ROS and Chrono. Generic
publishers and subscribers (a feature of ROS 2) are leveraged
to allow for user deﬁned message types and topic names to
be used. The package, written in C++, is open source and
available under a BSD3 license.

On the Chrono side, the bridge builds directly on core
functionality available through the utility class ChSocket.
Additional hooks were provided that wrap the JSON gen-
eration from user code. This was done to facilitate Python
wrapping with PyChrono, the Python bindings to the C++
Chrono API. This allows the researchers to leverage the
rapid development process enabled by Python. Additionally,
to allow custom message types to be sent between ROS and
Chrono, custom functors may be implemented to generate
and/or parse custom message formats.

B. Autonomy Stack

The autonomy stack developed for ART is built on top
of ROS 2 and includes basic implementations of autonomy
algorithms to enable autonomy experiments in simulation
and reality. The stack is basic; there is nothing particularly
novel in its implementation. It utilizes publicly available
algorithms and packages typically shipped with ROS 2.
This is because the focus of ART is not on the autonomy
stack itself, but
to which the algorithms are
transferable between sim and reality. Against this backdrop,
the perception algorithm is a custom trained instance of
Faster-RCNN [41] built on a MobileNetV3 [42] network.
MobileNet is intended for mobile phone CPUs, but was
adopted for ART considering the limited compute power
available on the vehicle. For the example used later in this
contribution, in which a vehicle navigates down a lane set up
with cones, the perception algorithm was trained using both
simulated and real images. Based on the bounding box output

the extent

from Faster-RCNN, 3D cone positions are generated at each
timestep and a simple planner and controller are used to keep
the vehicle on the path deﬁned by the cones. To control the
car, throttle, braking, and steering inputs are communicated
to the car via an Arduino processor. ROS 2 stock interface
packages are utilized to communicate with sensors.

C. Vehicle Chassis

The vehicle platform is built upon a 1/6th scale remote
controlled car. With a 47 cm wheel base and a 34 cm track
width, the vehicle is large enough to carry multiple cameras
and an automotive lidar such as the VLP-16 at 0.83 kg.
This is critical since research into the difference between
simulation and reality when applied to sensor simulation
must perform analysis on the speciﬁc sensors of interest in
full-scale applications. The base vehicle includes a double
wishbone independent suspension at the front and rear. The
kinematics of this suspension allow for use in off-road mobil-
ity. For actuation, the vehicle includes a 1300 KV brushless
motor and a 15 kg-cm servo for steering which we upgraded
to a 25 kg-cm servo for durability. The servo controls the
steering through a Pitman arm steering mechanism.

D. Electronic Hardware

The base RC car was augmented to allow for direct
control of the steering and throttle via an onboard computer.
The modiﬁcations were minimal to allow for duplication
of the system. First, the dual-battery setup powering the
electronic speed controller (ESC) was reduced to a single
battery, which with an eye towards safety, lowers the top
speed of what would otherwise be a very fast car. The use
of a dedicated battery for electronics ensures that voltage
spikes and drops induced by the motor do not affect any
sensitive compute or sensing electronics. To prevent ﬂoating
voltage issues, all ground connections are wired to a common
ground rail. A power distribution and control wiring diagram
along with a bill of material are provided with the hardware
documentation online [37].

The computer used on the ART vehicle is a Jetson Xavier
NX, although other compute platforms could also be used.
Indeed, the design of the vehicle and development environ-
ment are compute system agnostic. To facilitate this, direct
control of the servo and ESC is performed through PWM
signals from an Arduino chip. On the vehicle herein, an Ar-
duino Uno was leveraged, but an Arduino Nano is considered
in the documentation. The Arduino is used exclusively as a
device driver, performing no autonomy. The ART vehicle is
equipped with a USB camera and a VLP-16 lidar. Further
sensors can be mounted on the vehicle to facilitate additional
research needs.

E. Mounting, Expansion, and Reconﬁgurability

The ability to reconﬁgure the vehicle platform was a high
priority given the “research-platform mission” this solution
must fulﬁll. To that end, all mounting components were
designed to be 3D printed, with a base plate for electronics
that could be laser cut or 3D printed. The ﬁnal setup is shown

in Fig. 1 with the electronics mounted above the motor and
ESC on the base RC car. While most electronics are mounted
directly to the electronics board, the camera and lidar must be
mounted in unobstructed locations. To achieve this, a custom
mount allows a lidar to be placed above and clear of the
rest of the vehicle, and a front 3D printed bumper allows
for camera mounting. Future mountings will be designed to
hold multiple cameras and tracking markers on the front and
rear bumpers. In addition to electronics and sensors, motion
capture tracking targets can be mounted to the car using a
scattering of holes across the vehicle.

F. Vehicle Digital Twin

The vehicle described in this contribution has a digital
counterpart modeled in Chrono using Chrono::Vehicle [13]
and Chrono::Sensor [14]. The vehicle model is implemented
with a double wishbone suspension and a linear spring-
damper with parameters estimated from the vehicle mass.
Further calibration and measurements will allow for more
accurate values of spring stiffness and damping coefﬁcient.
The Chrono model of the car is rendered in Fig. 4 using
Chrono::Sensor and highlights the mesh representation of the
car along with the double wishbone conﬁguration.

(a) Front view of ART model.

(b) ISO view of ART model.

Fig. 4: Chrono::Vehicle model of ART as visualized with
Chrono::Sensor.

The steering model uses the Chrono::Vehicle Pitman arm
template, allowing actuation of the steering arm. The maxi-
mum steering angle was calibrated from a set of minimum
radius turn tests using motion tracking. The motor model is
a simple linear torque-speed curve, with decreasing power
with motor speed. While this does not accurately model
the brushless motor in a standalone conﬁguration, a lumped
model of the ESC and motor must be considered. Calibration
of the motor model remains part of future work and will be
critical as more dynamic simulations come into focus.

IV. DEMONSTRATION

To demonstrate ART at work, a path of cones was setup in
our motion tracking lab. The cone locations were recorded
and injected into a simulation of the same motion. The
vehicle then drove along the path in simulation and reality.
Images of this setup from a similar location for real and sim
are provided in Fig. 5, which shows the same cone paths and
the vehicle in a similar location on the path. To navigate,
the vehicle exclusively uses its front facing camera. This
demonstration is included in the supporting video and can
be found online [43].

(a) Real setup

(a) Perception on a simulated image

(b) Simulated setup

Fig. 5: Real and simulated scenarios using same cone loca-
tions measured by a motion capture system.

To further demonstrate the algorithms used by ART, we
show examples of the intermediate output from the percep-
tion and planning stages. Perception results from simulation
and reality are show in Fig. 6, and results from the planning
stage are shown in Fig. 7.

V. CONCLUSION AND FUTURE WORK

This contribution outlines an open-source autonomy re-
search testbed (ART) whose purpose is twofold: conduct
research in autonomy for wheeled/tracked vehicles, in on/off-
road conditions; and,
investigate the sim-to-real gap in
robotics – understand what causes it, and how it can be
controlled. Looking ahead, we will equip ART with addi-
tional sensors (e.g. GPS, IMU, and extra cameras including
a stereo camera). Given that we can track the position of
the vehicle in reality with millimeter accuracy via a motion
capture system, this richer family of sensors will allow us
to investigate more expeditiously better sensor models, as
well as perception, state estimation, planning, and controls
algorithms, and understand how inaccuracy in different com-
ponents of the autonomy stack propagate downstream and
cause the sim-to-real gap. Another future direction is tied to
setting up ART to work with tracked vehicles and in off-road
conditions.

Finally, ATK can be used to facilitate autonomy algorithm
development by allowing researchers to conﬁgure a custom
set of containers to host their development, deployment, sim-
ulation, and visualization needs. For instance, this platform
will assist UW-Madison in their SAE-sponsored AutoDrive
Challenge II.

ACKNOWLEDGMENT

This work was carried out

in part with support from
National Science Foundation project CPS1739869. Special
thanks to the Society of Automotive Engineers (SAE) and

(b) Perception on a real image

Fig. 6: Intermediate perception results from the autonomy
stack shown for simulation and reality. Overlaid on the image
are the detected bounding boxes, classes, conﬁdence, and
estimated 3D location relative to the vehicle.

General Motors for their support
Challenge Series II competition.

through the AutoDrive

REFERENCES

[1] K. Liu and D. Negrut, “The role of physics-based simulators in
robotics,” Annual Review of Control, Robotics, and Autonomous Sys-
tems - accepted, 2020.

[2] N. Jakobi, P. Husbands, and I. Harvey, “Noise and the reality gap: The
use of simulation in evolutionary robotics,” in European Conference
on Artiﬁcial Life, pp. 704–720, Springer, 1995.

[3] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,
“Domain randomization for transferring deep neural networks from
simulation to the real world,” in 2017 IEEE/RSJ international con-
ference on intelligent robots and systems (IROS), pp. 23–30, IEEE,
2017.

[4] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to-
real transfer of robotic control with dynamics randomization,” in 2018
IEEE International Conference on Robotics and Automation (ICRA),
pp. 1–8, May 2018.

[5] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, “Robust adversar-
ial reinforcement learning,” in International Conference on Machine
Learning, pp. 2817–2826, PMLR, 2017.

[6] A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine, “EPOpt:
Learning robust neural network policies using model ensembles,”
2016.

[7] A. Farchy, S. Barrett, P. MacAlpine, and P. Stone, “Humanoid robots
learning to walk faster: From the real world to simulation and back,”
in Proceedings of the 2013 international conference on Autonomous
agents and multi-agent systems, pp. 39–46, 2013.

[20] S. S. Srinivasa, P. Lancaster, J. Michalove, M. Schmittle, C. Sum-
mers, M. Rockett, J. R. Smith, S. Chouhury, C. Mavrogiannis, and
F. Sadeghi, “MuSHR: A low-cost, open-source robotic racecar for
education and research,” CoRR, vol. abs/1908.08031, 2019.

[21] Open-Source-Robotics-Foundation, “A 3D multi-robot simulator with

dynamics.” http://gazebosim.org/. Accessed: 2022-03-01.

[22] RACECAR Team, “The racecar project.” https://racecar.

mit.edu, 2022.

[23] RACECAR Team, “Github repo,

racecar project.” https://

[24] NVIDIA,

github.com/mit-racecar, 2022.
jet
NVIDIA-AI-IOT/jetracer, 2022.
cat

“The

“The

[25] J. Tse,

racer project.” https://github.com/

platform.” https://www.jontse.com/

portfolio/autonomous_rc_car.html, 2022.

[26] M. Dziubi´nski, “The karr platform.” https://tinyurl.com/

2p8s35rt, 2019.

[27] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
“CARLA: An open urban driving simulator,” in Proceedings of the
1st Annual Conference on Robot Learning, pp. 1–16, 2017.

[28] Donkey Car Community, “The donkey car.” https://docs.

donkeycar.com, 2022.

[29] Unity3D, “Main Website.” https://unity3d.com/, 2016. Ac-

cessed: 2021-11-23.

[30] NVIDIA, “PhysX simulation engine.” Available online at http://
developer.nvidia.com/object/physx.html, 2019.
[31] I. Orsolic, “Donkey car-based self-driving vehicle.” https://

tinyurl.com/56wb2ue4, 2020.

[32] D. Czuba, R. Dandekar, S. Gordon, and E. Stultz, “Parv: A 3d
printed autonomous roboticvehicle platform.” https://digital.
wpi.edu/pdfviewer/cj82kb092, 2021. Worcester Polytechnic
Institute.

[33] E. G. de Azevedo, A. Jeanlys, C. Mercer, J. Mugabo, E. Reardon,
and T. Sel, “Modular package for autonomous driving (mpad).”
https://digital.wpi.edu/downloads/pc289n23j,
2020. Worcester Polytechnic Institute.

[34] S. Sharma and J. White, “Self driving rc car.” https://github.

com/sidroopdaska/SelfDrivingRCCar, 2021.

[35] SunFounder, “PiCar.” https://tinyurl.com/4sabb5bk, 2021.
[36] EzBlock Studio, “EzBlock.” https://ezblock.cc/, 2022.
[37] UW-Madison

Engineering

Based
Laboratory,
http://projects.sbel.org/

Simulation
Toolkit.”

“Autonomy
autonomy-toolkit/, 2022.

[38] S. Kato, S. Tokunaga, Y. Maruyama, S. Maeda, M. Hirabayashi,
Y. Kitsukawa, A. Monrroy, T. Ando, Y. Fujii, and T. Azumi, “Autoware
on board: Enabling autonomous vehicles with embedded systems,”
in 2018 ACM/IEEE 9th International Conference on Cyber-Physical
Systems (ICCPS), pp. 287–296, IEEE, 2018.

[39] Apollo Development Team, “Apollo: A Smart Transportation Solu-

tion.” https://apollo.auto/, 2022.

[40] UW-Madison

Simulation

“PyPI: Autonomy-ToolKit.”
autonomy-toolkit/, 2022.

Based
Laboratory,
https://pypi.org/project/

Engineering

[41] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards
real-time object detection with region proposal networks,” IEEE
transactions on pattern analysis and machine intelligence, vol. 39,
no. 6, pp. 1137–1149, 2016.

[42] A. Howard, M. Sandler, B. Chen, W. Wang, L. Chen, M. Tan, G. Chu,
V. Vasudevan, Y. Zhu, R. Pang, H. Adam, and Q. Le, “Searching
for mobilenetv3,” in 2019 IEEE/CVF International Conference on
Computer Vision (ICCV), (Los Alamitos, CA, USA), pp. 1314–1324,
IEEE Computer Society, nov 2019.

[43] UW-Madison Simulation Based Engineering Laboratory, “ART
https://uwmadison.box.com/s/

Supporting
kfpxah1j5hrvnp5np6119ehe5rc0tzkn, 2022.

Video.”

Fig. 7: An example of intermediate results from the planning
stage. Shown here are the locations of the cones in 2D
(circles), the estimated curve of the boundary, and the target
point used for control (blue).

[8] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac,
N. Ratliff, and D. Fox, “Closing the sim-to-real loop: Adapting simula-
tion randomization with real world experience,” in 2019 International
Conference on Robotics and Automation (ICRA), pp. 8973–8979,
IEEE, 2019.

[9] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, and A. Y. Ng, “ROS: an open-source Robot Operating
System,” in ICRA workshop on open source software, vol. 3, p. 5,
Kobe, Japan, 2009.
2

documentation.” https://index.ros.org/doc/

[10] “ROS

ros2/.

[11] D. Merkel, “Docker:

lightweight Linux containers for consistent
development and deployment,” Linux journal, vol. 2014, no. 239, p. 2,
2014.

[12] Project Chrono, “Chrono: An open source framework for the physics-
based simulation of dynamic systems.” http://projectchrono.
org, 2020. Accessed: 2020-03-03.

[13] R. Serban, M. Taylor, D. Negrut, and A. Tasora, “Chrono::Vehicle
template-based ground vehicle modeling and simulation,” Intl. J. Veh.
Performance, vol. 5, no. 1, pp. 18–39, 2019.

[14] A. Elmquist, R. Serban, and D. Negrut, “A sensor simulation frame-
work for training and testing robots and autonomous vehicles,” ASME
Journal of Autonomous Vehicles and Systems, vol. 1, no. 2, p. 021001,
2021.

[15] A. Tasora, D. Mangoni, D. Negrut, R. Serban, and P. Jayakumar,
“Deformable soil with adaptive level of detail for tracked and wheeled
vehicles,” International Journal of Vehicle Performance, vol. 5, no. 1,
pp. 60–76, 2019.

[16] W. Hu, Z. Zhou, S. Chandler, D. Apostolopoulos, K. Kamrin, R. Ser-
ban, and D. Negrut, “Traction control design for off-road mobility
using an SPH-DAE co-simulation framework,” Multibody System Dy-
namics, vol.
, no. https://doi.org/10.1007/s11044-022-09815-2, 2022.
[17] J. Taves, A. Elmquist, A. Young, R. Serban, and D. Negrut, “Syn-
chrono: A scalable, physics-based simulation platform for testing
groups of autonomous vehicles and/or robots,” in Proceedings of 2020
International Conference on Intelligent Robots and Systems (IROS) –
Las Vegas, USA, 2020.

[18] Project Chrono Development Team, “PyChrono: A python wrapper
for the chrono multi-physics library.” https://anaconda.org/
projectchrono/pychrono. Accessed: 2021-11-19.

[19] A. Tasora, R. Serban, H. Mazhar, A. Pazouki, D. Melanz, J. Fleis-
chmann, M. Taylor, H. Sugiyama, and D. Negrut, “Chrono: An open
source multi-physics dynamics engine,” in High Performance Comput-
ing in Science and Engineering – Lecture Notes in Computer Science
(T. Kozubek, ed.), pp. 19–49, Springer International Publishing, 2016.

