2
2
0
2

r
p
A
7

]
E
S
.
s
c
[

2
v
3
5
6
6
1
.
3
0
2
2
:
v
i
X
r
a

Error Identification Strategies for Python Jupyter Notebooks

Derek Robinson
Department of Computer Science
University of Victoria
Victoria, Canada
drobinson@uvic.ca

Enrique Larios Vargas
Department of Computer Science
University of Victoria
Victoria, Canada
elariosvargas@uvic.ca

Neil A. Ernst
Department of Computer Science
University of Victoria
Victoria, Canada
nernst@uvic.ca

Margaret-Anne D. Storey
Department of Computer Science
University of Victoria
Victoria, Canada
mstorey@uvic.ca

ABSTRACT
Computational notebooks—such as Jupyter or Colab—combine text
and data analysis code. They have become ubiquitous in the world
of data science and exploratory data analysis. Since these note-
books present a diﬀerent programming paradigm than conventional
IDE-driven programming, it is plausible that debugging in compu-
tational notebooks might also be diﬀerent. More speciﬁcally, since
creating notebooks blends domain knowledge, statistical analysis,
and programming, the ways in which notebook users ﬁnd and ﬁx
errors in these diﬀerent forms might be diﬀerent. In this paper,
we present an exploratory, observational study on how Python
Jupyter notebook users ﬁnd and understand potential errors in
notebooks. Through a conceptual replication of study design in-
vestigating the error identiﬁcation strategies of R notebook users,
we presented users with Python Jupyter notebooks pre-populated
with common notebook errors—errors rooted in either the statisti-
cal data analysis, the knowledge of domain concepts, or in the pro-
gramming. We then analyzed the strategies our study participants
used to ﬁnd these errors and determined how successful each strat-
egy was at identifying errors. Our ﬁndings indicate that while the
notebook programming environment is diﬀerent from the environ-
ments used for traditional programming, debugging strategies re-
main quite similar. It is our hope that the insights presented in this
paper will help both notebook tool designers and educators make
changes to improve how data scientists discover errors more easily
in the notebooks they write.

1 INTRODUCTION
Jupyter Notebook1 is an open-source, browser-based programming
environment that allows users to weave rich text, code, equations,
and visualizations into a single human-readable document. Jupyter
Notebooks and other computational notebooks, such as Google
Colab2, RMarkdown Notebooks3, and Azure Notebooks4, have be-
come immensely popular for anyone who wishes to perform data
analysis or exploration tasks. The Jupyter platform speciﬁcally has
grown exponentially since 2015, with over 6 million publicly avail-
able Jupyter Notebooks currently residing on GitHub alone [15].

1https://jupyter.org/
2https://colab.research.google.com/
3https://rmarkdown.rstudio.com/
4https://notebooks.azure.com/

Despite its popularity, Jupyter Notebook users have mixed opin-
ions about the process of debugging [6]. Some praise its ability to
help them ﬁnd errors quickly, but others complain about a poor de-
bugging experience [6]. This has given us some insight into how
users feel about debugging Jupyter Notebooks, but not much about
their actual debugging processes.

Debugging involves two phases. The ﬁrst phase involves identi-
fying what and where the error is, and the second phase involves
determining how best to ﬁx the error [29]. We have many insights
about the process of debugging software systems and programs in
general [2, 14, 26], but we lack similar insights for computational
notebooks. To the best of our knowledge, there are a few studies
which examine how computational notebooks are debugged. For
instance, Yang et al. [28], focused on a tool for improving code un-
derstanding with program synthesis but did not synthesize strate-
gies used in debugging. Without knowing how computational note-
books are debugged, it is diﬃcult to support—whether with tools
or processes—the debugging of computational notebooks.

Motivated by the sheer popularity [15] of the Jupyter Notebook
platform across many disciplines, we aimed to identify strategies
adopted by users of Jupyter Notebooks in the ﬁrst phase of de-
bugging, i.e., strategies for identifying errors in data analysis note-
books. Understanding debugging strategies may provide educators
and students with valuable knowledge about how errors are found
in computational notebooks, improving how they teach or learn
error-ﬁnding methods. Although our ﬁndings are not aimed at any
speciﬁc community of Jupyter Notebook users, as the platform is
used across many diﬀerent domains [5, 20, 25], we hope that our
results may serve as recommendations for users of Jupyter Note-
books who may not have a strong background in the process of
debugging.

Our work focused on this research question:
What strategies do data scientists use to ﬁnd statistical,
data, and programming errors in Python Jupyter Notebooks?
Our study is a conceptual replication of a work-in-progress study
of how experts and novices debug RMarkdown documents [13]. All
source materials [3] were provided by the authors of the R study.
We replicated the R study design and performed an observational
study with 14 participants tasked with ﬁnding and understand-
ing errors in one of four Jupyter Notebooks. We translated the
RMarkdown study materials [3] into Python, retaining the same

 
 
 
 
 
 
pre-populated errors in the statistics, data, and programming code
and the same supporting analysis text as designed in the original
study materials [13]. The participants could use any error-ﬁnding
method they chose and were not time-constrained. We observed
that participants followed seven diﬀerent strategies. We charac-
terized the strategies according to the frequency of use and their
success (which are not necessarily the same!). Consulting exter-
nal resources via a search engine was the most common strategy.
However, the most successful strategy was Expectation Conﬁrma-
tion, where there was a mismatch between what explanatory mark-
down cells claimed and what the code actually did. On average, our
participants found approximately 40% of the errors in the notebook
they analyzed.

2 BACKGROUND
Jupyter Notebooks are the “de-facto standard” for data scientists [16],
and much has been learned about how reproducible they are, the
quality of the code written in them, and the narratives that describe
the analyses within them [17, 19, 22, 23]. These studies agree that
notebook code is frequently low-quality and error-prone. Closely
related to our work is that from Yang et al. [28]. They report on
a tool to support bug detection in Kaggle notebooks, which they
characterized as ‘data wrangling code’. Like us, they show this style
of development is quite diﬀerent than pure source code approaches
and prone to errors, yet not well supported by tools. They intro-
duce WrangleDoc, a program synthesis technique to summarize
code in order to facilitate debugging. We did not examine such
summarization approaches in our study, and our Jupyter instance
contained no plugins.

While the work of Yang et al. highlights the potential problems
with data science code, there is still much we do not understand. In
particular, their study looked at speciﬁc tool support using a docu-
mentation approach. We focused instead on the strategies leading
to the discovery of notebook errors, the starting point of the de-
bugging process.

Challenges of using notebooks. While the literature on note-
book error detection is limited, other research has revealed the
challenges of using notebooks. Chattopadhyay et al. identiﬁed the
main pain points of using computational notebooks, including Jupyter
[6]. Of interest to this study is the Manage Code pain point which
mentions that without suﬃcient software engineering support, de-
bugging, writing code, managing dependencies, and testing relies
on ad-hoc workarounds. Speciﬁcally, they found that writing code
in notebooks eﬃciently requires knowledge of all the function names
and classes, plus the use of a second window to search for online
resources such as documentation [6]. They also observed a divide
in how their participants felt about debugging in notebooks. Some
participants were able to ﬁnd errors in a notebook quickly, but oth-
ers found that debugging was a horrible experience when they had
to rely on print() statements. In addition, they found that test-
ing in computational notebooks was diﬃcult as there is no stan-
dard method to test a notebook. Some study participants wrote test
cases in the same notebook, while others created a new notebook
for testing.

Debugging traditional programs. There is plenty of related re-
search on debugging conventional programs. We mention some

D. Robinson, N. Ernst, E. Larios Vargas, M.-A. Storey

closely related studies here. Murphy et al. present a qualitative
study of the debugging strategies employed by computer science
students [14]. They observed three distinct categories of strategy:
the good, the bad, and the quirky. The good strategies (or eﬀec-
tive strategies) included gaining domain knowledge, tracing, test-
ing, understanding the code, using resources, using tools, isolating
the problem, pattern matching, and considering alternatives. They
also identiﬁed that many students employed strategies that were
less eﬀective (the bad). These “bad” strategies were the same as
the eﬀective strategies, but employed less eﬀectively. Finally, the
quirky strategies were ones which surprised Murphy et al..

Hypothesis-driven debugging. Alaboudi and Latoza authored
two papers that relate to our study. The ﬁrst paper, titled “Using
Hypothesis as a Debugging Aid” [1], describes two studies. In the
ﬁrst study, they observed live-stream videos of developers’ pro-
gramming activities. Their second study was a controlled study
of 25 participants tasked with debugging three API misuse prob-
lems. Overall, they observed that developers found it challenging
to formulate a reasonable hypothesis about a potential error. In
the second paper, “An Exploratory Study of Debugging Episodes”,
Alaboudi and Latoza observed 15 live-streamed programming ses-
sions (in C, C#, JavaScript) [2]. They found that developers spent
48% of their programming sessions debugging. They also found
that no single activity dominated a debugging session, with devel-
opers spending varied amounts of time on diﬀerent activities. Ad-
ditionally, they observed signiﬁcant diﬀerences between long and
short debugging episodes. Short debugging episodes focused on
editing and testing code, while long debugging episodes involved
various activities, such as consulting external resources, and in-
specting program state in addition to testing and editing.

Student approaches to debugging. Like our study, Whalley et
al. examined students’ thoughts about their debugging process (in
non-notebook code). They examined whether reﬂecting on the de-
bugging process helps students perceive a need for change in their
approach, and if they perceive value in a structured, formal debug-
ging process [26]. Whalley et al. used semi-structured interviews to
answer their research questions. Their analysis uncovered themes
about code comprehension, bug location, information gathering
strategies, challenges locating bugs, emotions felt during the de-
bugging process, and the value students give to a formal debugging
process. When students were asked to reﬂect on their debugging
process, their comments referred to both high- and low-level activ-
ities. High-level activities included activities such as reading code,
search space reduction, and hypothesis forming. Most of the reﬂec-
tions shared about debugging were about low-level activities, such
as where to place print statements, code tracing, and examining
function parameters and return values. Many students perceived
debugging as ineﬃcient, likely due to the lack of a formal process
to follow. One-third of the participants described their debugging
process as ﬂawed, and they universally described their hypothesis-
forming method as imprecise, opting to guess and check instead.

Data science debugging. Debugging traditional programs is well
studied and the works discussed above are a very small subset of
the work available on debugging. In contrast to the debugging of
traditional programs, data science work can be quite diﬀerent [27].
For example, it is a common part of the workﬂow for scientists

Error Identification Strategies for Python Jupyter Notebooks

to re-run analyses (e.g., as part of exploratory data analysis). This
might happen when, for example, removing outliers or experiment-
ing with diﬀerent hyper-parameters. Thus, research has looked at
supporting such experimental workﬂows in order to manage ver-
sions of notebooks [10, 24], and support cleanup and refactoring
[9]. Related to that is work that uses tools to debug data ﬂows
in large (non-notebook) data analytics pipelines [18], or support
statistical transparency with multiverse analysis [7]. None of this
work focused on how errors were detected. However, Brown et al.
introduces four error types present in data analysis [4].

3 MATERIALS AND METHODS
To answer our research question, we observed the behaviour of 14
participants as they browsed and debugged existing Jupyter Note-
books that contained errors. The observations took place over Zoom,
and participants shared their screens. We recorded video and au-
dio of the meetings for a later qualitative analysis of the strategies
our participants used. The data collection was conducted from No-
vember 2020 to January 2021.

3.1 Participants
Table 1 summarizes participant demographics. A majority (8) of the
participants were from the domain of computer science or com-
puter science combined with either music, biochemistry, or life
sciences. The other (5) participants were each students in one of
chemistry, physics, civil engineering, software engineering, and
electrical/computer engineering. The remaining participant was a
professional who worked in the education domain. Nine partici-
pants had used computational notebooks for less than one year,
and the remaining ﬁve used them for between one and three years.
All participants stated that Jupyter Notebooks was their computa-
tional notebook of choice, with two also using Google Colab.

We recruited participants by contacting instructors of three 400/500-

level courses with data science themes, by posting on online com-
munities, and through personal contacts. Participation in our study
was voluntary, but we encouraged participation by providing a $25
Amazon gift card to the ﬁrst 12 respondents. Our study was ap-
proved by our institutional review board.

3.2 Study Materials
The notebooks used for our study were translated from R note-
books created as part of an in-progress study to investigate data
scientists’ debugging behavior [13]. The R notebooks were written
by statistics and data science education researchers [3] and covered
two diﬀerent topics (NBA Player of the Week and the 2011 Spain
Election). Each notebook had three versions: A, B, and C. Errors
were introduced into versions A and B of the original R notebooks
by two members of the R study [13] team, the C notebook had no
errors. Table 2 lists the number of errors per notebook.

We translated the R notebooks (including errors) to Python, and
the translations were veriﬁed by a third party, experienced in Python,
statistics, and data science. Additionally, the ﬁrst author of this pa-
per veriﬁed that the Python translations returned the same data,
visualizations, and values. The Python and R Notebooks are avail-
able at [3], along with a complete list of the errors.

1

2

3

1

2

3

4

nba = nba.assign(

.replace('cm','').str.replace('-[0-9]*','')),

Height = pd.to_numeric(nba['Height'].str ⌋
↩→
Weight = pd.to_numeric(nba['Weight'].str ⌋
↩→

.replace('kg','')))

Listing 1: A data error: assumes all data is in cm/kg.

my_test = ttest_ind(

x1 = nba[(nba['Position'] == 'PG')]['Height'],
x2 = nba[(nba['Position'] == 'SG')]['Height'],
alternative = 'smaller')

Listing 2: A statistical error: using a 1-sided t-test when a
2-sided t-test is the proper choice.

We retained the error classiﬁcation system from the R Study,
which identiﬁed embedded errors as: data, statistical, and pro-
gramming [3]. In the notebooks, error types were not mutually
exclusive and a given error could be a programming error in addi-
tion to a data or statistical error. These three diﬀerent types of er-
rors align with the diﬀerent categories of errors deﬁned by Brown
et al. [4]. We describe the three categories of errors below and pro-
vide examples using the NBA Player of the Week notebook(s).

Data errors occur when a notebook does not fully explore the
dataset, or when the format of the data is misunderstood. Listing 1
shows an example of a data error: the Height column is of type
string and is assumed to either contain the centimeter unit or a
string value representing a measurement. Similarly, it is assumed
that the measurements in the Weight column are all in kilograms,
with some measurements containing the kilogram units. In fact,
the Height column has units of either centimeters, such as 203cm,
or feet-inches, such as 6-11. The Weight column has measurements
which are in kilograms and contain the kilogram units, or are mea-
surements in pounds that contain no units. The above code cell
removes the centimeter unit by calling str.replace('cm','').
The inches measurement is also removed by calling str.replace ⌋
('-[0-9]*',''). The same is done with the Weight column, re-
moving the kilogram units via str.replace('kg',''). Thus, the
column is incorrectly cleaned as the above code cell performs no
unit conversions. This leaves the Height and Weight columns in
mismatched units without any unit identiﬁer.

Statistical errors occur either when an incorrectly chosen sta-
tistical test or visualization is used, or when a correctly chosen test
or visualization is wrongly interpreted by the user. In Listing 2, the
goal is to determine if a statistical diﬀerence between the average
height of point guards and shooting guards exists using a t-test.
The error is that the alternative parameter is set to smaller,
indicating a one-sided t-test. This parameter should be set to ‘two-
sided’ as the goal was to determine whether or not a statistical
diﬀerence exists, rather than which average was smaller.

Lastly, programming errors occur when a code cell does not
achieve the goal stated in the preceding markdown cell. The goal
of Listing 3 is to ﬁlter the nba dataframe so that it only contains
unique players. While this code cell does output a set of unique

D. Robinson, N. Ernst, E. Larios Vargas, M.-A. Storey

Table 1: Participant Demographics

Participant

Role

Domain

Notebook Experience (yrs)

P1
P2
P3
P4
P5
P6
P7
P8
P9
P10
P11
P12
P13
P14

Master’s
Master’s
Undergraduate
Undergraduate
Master’s
Undergraduate
Master’s
Undergraduate
Undergraduate
Educational Specialist
Undergraduate
Undergraduate
Doctoral
Undergraduate

Electrical & Computer Engineering
Chemistry
Computer Science & Biochemistry
Computer Science & Life Sciences
Computer Science
Physics
Civil Engineering
Software Engineering
Computer Science
Education
Computer Science & Music
Computer Science & Music
Computer Science
Computer Science & Music

< 1
1-3
< 1
1-3
< 1
1-3
< 1
< 1
1-3
< 1
< 1
< 1
1-3
< 1

1

2

3

4

nba.groupby('Player').agg(

Height=('Height', 'median'),
Weight=('Weight', 'median'),
Position=('Position', 'first'))

Listing 3: A programming error: using the original, not the
ﬁltered data-frame.

players, a copy is returned, which is not saved to the nba dataframe.
Through the remainder of this notebook, the original nba dataframe
is used, and thus the code has an error and does not achieve its goal.

3.3 Jupyter Notebook Study Design
Each participant was tasked with ﬁnding potential errors in one of
the four notebooks which contained errors (Versions A or B). Ver-
sion C was shown to them after their analysis if they wanted to see
an error-free version. We aimed to balance the number of partici-
pants analyzing each notebook (see Table 2). This task was open-
ended in that the participants were allowed to use any method they
liked to ﬁnd potential errors. The only speciﬁc instructions given
were for them to think aloud whenever possible and to notify the
researcher when they thought they had found an error.

We performed two rounds of pilots (with members of our re-
search group) to improve the study task and to conﬁrm our study
would provide suﬃcient observations on error ﬁnding strategies.
The feedback from the pilots helped us improve the study materi-
als. The supplementary materials contain the task description, in-
terview questions, and Jupyter Notebooks [3].

At the start of each study session, we described the task and
emphasized that our aim was not to test their skills. Participants
were then presented with a Jupyter Notebook and informed that
any of the notebook components might contain errors that they
should try to identify. We mentioned they could modify the note-
book, search the documentation, or use the internet for help.

Each study session consisted of two phases: an observational
phase and an interview phase. During the observational phase, par-
ticipants analyzed the notebook for errors while one researcher ob-
served their behaviours and took notes. Once the participant was
satisﬁed with their analysis of the notebook, we held the interview
phase of the study.

The interview began with unstructured questions, using notes
from our observations to guide our follow-up questions. Asking
these questions immediately after the participant had performed
their task was important as their strategies were still fresh in their
mind. These unstructured questions were asked to gain insights
into a speciﬁc approach and why it was used. Following the un-
structured part of the interview, additional questions were asked
about the participant’s domain of study, how long they had been
using Jupyter Notebooks, and the computational notebook they
used most often. The complete list of these additional questions is
available in a replication package [3].

3.4 Data Collection and Analysis
The Zoom video recordings of the studies were uploaded and we
analyzed the recordings directly using ATLAS.ti 85. We used an
open coding process to code all activities performed by our partic-
ipants. The ﬁrst author of this paper performed the initial coding.
After the initial coding cycle, discussion sessions were held with
the second and fourth authors, where the codes were further ana-
lyzed and compared with the ﬁndings from previous participants,
and reﬁned in an iterative manner.

Throughout our discussion sessions, we identiﬁed emergent higher-

level groups for the codes and merged some codes:

• Action: An action that a participant performed.
• Docs: A speciﬁc documentation website that a participant

visited.

• Online Resource: An online resource other than documen-

tation that a participant visited.

• Reasoning: A reason for performing an action or a reason

for why something was an error.

• Participant Attribute: To describe a participant.

5https://atlasti.com/product/what-is-atlas-ti/.

Error Identification Strategies for Python Jupyter Notebooks

Table 2: Number of Participants and Errors per Notebook
(D: Data Error S: Statistical Error P: Programming Error)

Notebook

# of Participants

Participants

# of Errors

nba_analysis_A
nba_analysis_B
elections_analysis_A
election_analysis_B

4
3
4
3

P1, P8, P10, P13
P2, P9, P12
P3, P5, P6, P14
P4, P7, P11

6
4
10
10

Distribution of
Errors

Size (# of Code
Cells)

D:3 S:3 P:2
D:0 S:2 P:3
D:2 S:7 P:5
D:2 S:9 P:3

10
12
12
12

Throughout our discussion sessions, we noticed many actions
were performed together. We called these connected sets of actions
strategies. The ﬁrst author analyzed the raw data again to identify
and code strategies from each group of actions.

Once we identiﬁed strategies, we analyzed videos again to de-
termine the success rate of each strategy. Whenever we observed
a participant analyzing an erroneous cell, we entered their chosen
strategy into a spreadsheet, along with the type of error they were
working on and whether that strategy was successful or not.

4 FINDINGS
Our research question asked what strategies data scientists use to
ﬁnd statistics, data/domain, and programming errors. Our analysis
reveals (a) actions and (b) strategies that our participants employ to
ﬁnd errors in Python Jupyter notebooks. Additionally, we present
(c) the relationship between strategies and error-ﬁnding success. Ta-
bles 3 and 5 show the entire list of actions and strategies identi-
ﬁed in our exploratory observational study. Finally, in Table 6, we
present how strategies relate to the diﬀerent error types. We de-
scribe each action, strategy, and their respective relationship with
an error type below.

4.1 Actions Taken
Our participants performed various actions while analyzing the
notebooks to ﬁnd errors. In this context, an action is an (atomic)
activity such as reading a markdown/code cell or examining a CSV
ﬁle. Table 3 lists the number of participants who performed each
action and the average amount of time all participants spent per ac-
tion. There were some actions that participants always used, such
as reading code and markdown cells, writing or editing code, and
using the search engine. Other actions often used included look-
ing at the documentation, checking code output, and inspecting
dataframes. Finally, a few actions were only occasionally used, such
as inspecting a CSV ﬁle or adding a comment. We describe these
actions in more detail below.

A1: Reading a code cell. This action refers to when participants
(P1-P14) read through a code cell to understand what it was doing.

A2: Reading markdown cells. This action refers to when a par-
ticipant (P1-P14) read through a markdown cell to gain context into
what the preceding code cell tried to accomplish.

When analyzing a notebook to ﬁnd errors, participants performed

actions A1 and A2 successively. In this scenario, P8 emphasized,
“[I read] the documentation ﬁrst then [I read] the code”. Additionally,
P12 highlighted the value of reading the markdown aloud to better
understand what was going on.

A3: Writing/Editing code. This action occurred when participants
(P1-P14) wrote new code in a code cell (either one they added or one
present in the notebook) or edited a code cell that was initially in
the notebook. We observed that participants edited code for several
diﬀerent reasons. For example, P14 stated that they edited code
cells to make them more readable. Other participants, such as P10,
edited the parameters of functions to view more of the data re-
turned by that function: for example, P10 edited calls to Pandas
Series.nlargest() function. Some participants also wrote new
code into the notebooks, which served various purposes. For in-
stance, P13 wrote code during their analysis of the nba_analysis_A
notebook to verify if two sets of rows in the nba dataframe were
the same. Both P6 and P11 wrote code to perform type checking
through the use of Python’s type() method.

A4: Using a search engine. All participants used a search en-
gine to access some online resource or documentation page. Typically
participants transitioned from the notebook to the search engine
and then to either an online resource or a documentation page.
Depending on whether or not the initial search result was helpful,
they would return to the notebook or select another result from
the search engine. The Search Engine action is highly associated
with both A5: Looking at documentation and A8: Looking at
an online resource. We deﬁne online resources as any website
other than a documentation page. Table 4 shows the most com-
monly accessed documentation websites and online resources.

A6: Checking code output. Commonly, participants (P1-P12, P14)
inspected the output of a code cell visually, either one initially present
in the notebook or one which the participant added.

A7: Inspecting dataframe. We observed that participants (P1-P4,
P6-P12, P14) used the Dataframe.head() method to visually in-
spect the dataframe, either to gain a preliminary understanding of
the data or to check if anything seemed out of place.

A9: Inspecting a graph. Participants (P3, P5-P14) performed this
action to visually inspect any graph present in the notebook.

A10: Inspecting CSV File. In a similar situation to A7, partici-
pants (P5, P6, P9, P10, P13) inspected the data in its raw state. For
instance, P13 pointed out that when using Jupyter Notebooks, they
do not use the CSV viewer native to Jupyter; instead, they use an
alternative application. Likewise, P9 indicated they use Notepad++
to view their CSV ﬁles. Finally, P10 highlighted that they inspect
the CSV ﬁle when they are unsure how to perform a task program-
matically. In this matter, P10 stated, “I’m just learning Python, so I
can’t...list these things, I actually refer to the CSV quite a bit”.

D. Robinson, N. Ernst, E. Larios Vargas, M.-A. Storey

Table 3: Actions Taken in Error Identiﬁcation.

Action

Action ID # Participants Average Time Spent (mm:ss)

Reading code cell
Reading markdown
Writing/Editing code
Using a search engine
Looking at documentation
Checks code output
Inspecting DataFrame
Looking at an online resource
Inspecting graph
Inspecting CSV ﬁle
Reading an error message
Adding a comment

A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
A11
A12

14
14
14
14
13
13
12
12
11
5
3
2

06:46
05:51
03:21
01:55
03:00
02:24
04:07
03:06
01:53
02:40
01:51
08:42

Table 4: Number of Visits. † indicates a Documentation page.
The remainder are Online Resources. Fourteen other Online
Resources were each visited between one and three times.

Resource

Number of Visits

Pandas †
Plotnine †
Statsmodels †
stackoverﬂow.com
geeksforgeeks.org
investopedia.com
Numpy †
Scipy.stats †
tutorialspoint.com
w3schools.com

58
28
21
14
6
6
4
4
4
4

A11: Reading an error message. This action occurred when par-
ticipants (P1, P3, P5, P8-P11) changed the notebook as initially the
notebooks did not return any error messages. In this scenario, P10
pointed out that when they see an error message, they “don’t have
a clue”.

A12: Adding a comment. This action occurred when participants
(P5, P11, P14) added a comment to a code cell either in the form of a
note or to comment out code.

While these actions capture the more atomic tasks our partici-
pants performed, we also observed that several actions were used
together to form strategies that helped participants ﬁnd or under-
stand the cause of errors. In the remainder of this section, we de-
scribe these strategies in more detail.

4.2 Error-Finding Strategies
Participants performed many of the preceding actions together to
serve a particular purpose. We call a collection of related actions a
strategy. We describe the strategies we found in detail. Table 5 gives
a brief description along with the number of participants who used
each strategy.

Search Engine-Driven Approach. The most common strategy
we observed was the search engine-driven approach, which every
participant used. All participants made several transitions from the

notebook to the search engine, then to an external resource, until
they found a helpful online resource or documentation page.

Participants outlined three diﬀerent reasons for using the search
engine and external resources. First, they used the search engine
as a ﬁrst step to gather a solution from an online reference. For
instance, P12 highlighted that they use Google quite often when
using a Jupyter Notebook, and without it, they would not know
what to do. Not knowing what to do without the search engine
hints at being dependent on it; it is unknown whether this is caused
by a lack of general programming knowledge or knowledge of a
speciﬁc API, such as Pandas.

Second, the search engine was also used as a conﬁrmatory aid;
this happened when participants had prior knowledge. However,
they sought supplementary expertise to conﬁrm or refresh their
intuition. For instance, P7 stated they often remember general con-
cepts but use the search engine to gather information about what
some speciﬁc terms mean to interpret them correctly, such as when
P7 gathered information about interpreting the results of an ordi-
nary least squares (OLS) regression.

Finally, participants also used the search engine to gather code
snippets as potential solutions. P8 emphasized that their particular
use of the search engine was to ﬁnd code snippets that could help
them ﬁx the errors they identiﬁed.

Assume and (Sometimes) Check. Participants would only cur-
sorily inspect a code cell, see what the code is doing, and return
to it only when they identiﬁed a potential problem in their theory
of the notebook’s execution. They then made an assumption about
where in the preceding cells that problem happened, and then ex-
amined that code in more detail than they did on their ﬁrst pass
over it. However, participants “sometimes” left some assumptions
unchecked. This may be due to the contrived nature of the study
(ﬁxing the bug was not part of the task). When participants did
check assumptions, they wrote new code in the notebook or exam-
ined the dataframe/CSV ﬁle.

Consider the error and thought processes of P8 while they use
the assume and (sometimes) check strategy to determine the error
described in Listing 1 (code cell 3 of the notebook NBA_Analysis_A
). P8 began analyzing the notebook using a once-over (see the next
strategy) and noticed in a later code cell that the given mean of the
height column was roughly 12. They then remarked that a “mean

Error Identification Strategies for Python Jupyter Notebooks

Table 5: Strategy Descriptions

Strategy

Description

# Participants Associated Actions

Search Engine-Driven Approach Using the search engine and external resources to gather useful infor-

Assume and (Sometimes) Check Making an assumption related to the notebook or to an API call and

mation.

Expectation Conﬁrmation

Once-Over

Re-implement to Check

Key Information

Start With What You Know

sometimes checking it.
The participant’s expectation, set up by an explanatory markdown cell,
of what a code cell does cannot be conﬁrmed upon seeing its output.
Brieﬂy browsing through the notebook in order to gain a preliminary
understanding of what it contains.
Re-implementing a code cell using a diﬀerent syntax in order to check
its validity.
Extracting need-to-know information from a markdown cell and plac-
ing it in a comment inside the related code cell.
Starting at a point in the notebook which is most familiar.

14

14

7

4

3

1

1

A4, A5, A8

A3, A7, A12

A1, A2

A1, A2, A6

A3, A6

A2, A10

A1, A2

height of 12 doesn’t seem to make a lot of sense” (since height in
cm should be (broadly) greater than 100cm and less than 225cm).
They then transitioned to read code cell 3 (Listing 1 line 2), which
cleaned and adjusted the height column. Rereading the code cell
led them to assume something must have gone wrong in that note-
book cell. They then inspected the original dataframe and made an-
other assumption: “Here the measurements are presumably in feet-
inches and over here we have them in cm”. This second assumption
is an example of assuming the purpose of a series of method calls.
A closer inspection of code cell 3 allowed them to identify the er-
ror as replacing inches with the empty string and not accurately
converting feet-inches to cm.

Expectation Conﬁrmation. Seven participants (P1, P3, P5, P7,
P10, P11, P13) indicated that a discrepancy between explanatory
text in a markdown cell and the subsequent code cell helped them
identify an error. P7 described the explanatory markdown as a
“guidance for what I should be looking for”, and that when a dif-
ference occurred between the markdown and the code cell, they
knew something was incorrect. Additionally, P5 used an analogy
to describe the discrepancy between the markdown and code, stat-
ing, “It’s basically like ‘Hey, we did this’ and then [I] look at the code
and it’s like ‘No, you didn’t.”’ Finally, P11 emphasized, “what I was
expecting is that we want a percentage and this is obviously not a
percentage”, outlining how the markdown sets their expectations.
When the code does not fulﬁll these expectations, they know some-
thing is wrong.

Once-Over. Four participants (P2, P6, P8, P14) used this strategy,
which involves looking through the notebook to gain a preliminary
understanding. This strategy consists of reading markdown and
code cells, running code cells and brieﬂy checking their output, and
generally inspecting the notebook’s initial state. A once-over gives
a basic understanding of what the notebook is doing without too
much detail. All four of the participants, when using the once-over
strategy, employed diﬀerent language to describe it. For example,
P2 stated they were getting “a lay of the land”.

Re-implement to Check. The re-implement to check strategy was
used by three participants (P1, P6, P11) and implies rewriting a
code cell using a diﬀerent syntax and then comparing the results of

1

nba[(nba['Position'] == 'PG') | (nba['Position'] ==
'SG')].groupby('Position').agg(Height=('Height',
↩→
'mean'))

↩→

Listing 4: Code snippet P1 wrongly thought was incorrect.

1

nba[(nba['Position'] == 'PG') | (nba['Position'] ==
'SG')].groupby('Position').agg('Height').mean()

↩→

Listing 5: P1’s re-implementation of Listing 4.

both to see if there are any diﬀerences. For example, P1 wrongly be-
lieved that Listing 4 was incorrect due to the .agg() syntax. They
continued to add a new cell and rewrite the code (Listing 5), only
to ﬁnd that they produced the same result.

P6 stated that they would have shown a correlation by plotting
rather than using an OLS regression, but they did not re-implement
this code cell as they were unfamiliar with the Plotnine package
used to generate the plots. While not precisely re-implementation,
P11 wrote pseudocode before looking at a code cell and after read-
ing its markdown explanation. They then compared this pseudocode
to the actual code, and if similar, P11 believed this code cell was cor-
rect and continued to a new cell. Additionally, participants com-
bined this pseudocode strategy with a re-implementation to fur-
ther validate a given code snippet.

Key Information. The Key Information strategy was used four
times by P5 and describes extracting only the information you need
from the markdown description of a code cell; P5 then placed this
information inside the code cell as a comment. Extraction of the
key information allowed P5 to get the information closer to the
code, and reduced the number of times they re-read a markdown
cell to remind themselves of what a code cell was doing. In addi-
tion, they highlighted how extracting the key information allowed
for easier comparison of the code and markdown, and eliminated
any extraneous information they did not need to know. Using the
Key Information strategy allowed P5 to more easily employ the Ex-
pectation Conﬁrmation strategy.

Table 6: Relating Strategies (from Table 5) and Error Type. A dash (-) indicates no use. The once-over strategy was not used for
any of the error types. There are a maximum of 21 programming errors, 12 statistical errors, and 7 data errors.

D. Robinson, N. Ernst, E. Larios Vargas, M.-A. Storey

Strategy

Error Type

Times Used

Errors Found

Percentage

Search Engine-Driven Approach

Assume and Check

Expectation Conﬁrmation

Re-implement to Check

Start With What You Know

Key Information

Programming
Statistical
Data

Programming
Statistical
Data

Programming
Statistical
Data

Programming
Statistical
Data

Programming
Statistical
Data

Programming
Statistical
Data

26
16
10

13
8
5

20
6
7

1
-
1

1
-
1

3
2
-

11
7
2

7
4
4

17
0
7

0
-
0

0
-
0

2
1
-

52.4%
53.9%
28.6%

33.3%
30.8%
57.1%

81.0%
0%
100%

0%
-
0%

0%
-%
0%

9.52%
7.69%
-%

Start With What You Know. P5 employed another strategy named
Start With What You Know, which involved analyzing parts of the
notebook they were familiar with ﬁrst. They mentioned that do-
ing so made them “feel more conﬁdent”, and that starting with the
topics they were more familiar with gave them a better chance to
ﬁnd errors. This conﬁdence then allowed them to ﬁnd errors in the
other sections of the notebook as they were better able to under-
stand the nature of the errors.

4.3 Strategy Success
We now describe how the strategies outlined in Section 4.2 were
used to ﬁnd the various types of errors present in each notebook.
As our study was exploratory, we do not make any claim that these
are the best strategies for ﬁnding a particular type of error (such
a claim would require future work). Recall that our study included
the analysis of three types of errors from [13]: programming errors,
statistical errors, and data/context errors (see Section 3.2). The er-
ror types are not mutually exclusive and a given error can belong
to more than one error type. While some strategies were less suc-
cessful, they are still worth examining. First, we cannot claim that
unsuccessful strategies might not be successful in diﬀerent con-
texts. Second, these strategies, if repeatedly used, might become
anti-patterns for debugging that are important to know about and
to avoid. Finally, strategy success can be user-dependent. Murphy
et al. [14] also found that the same strategy can be eﬀective or in-
eﬀective, depending on the way it is used.

Table 6 outlines the number of times our participants used each
strategy per error type, the number of errors found per strategy,
and the percentage of total errors found by each strategy. We re-
port on all seven strategies. The most successful strategies are Ex-
pectation Conﬁrmation and Search Engine-Driven Approach.
The Expectation Conﬁrmation strategy success is inﬂuenced by

the markdown present in our notebooks. The markdown descrip-
tion set expectations for our participants. When the participants
read the code following the descriptive text, they contrasted their
expectations of what the code was supposed to do with what the
code actually did. We note that in practice, Pimentel et al. found
that notebooks contain very little markdown [17].

Additionally, we note that the eﬃcacy of the Search Engine-
Driven Approach is associated with the popularity of using on-
line resources to guide users of Jupyter Notebooks [11], as pointed
out by participants P7, P8, and P12. Koenzen et al. similarly de-
termined that code reuse in Jupyter Notebooks most commonly
comes from searches on the web, most often from websites that
provide a tutorial, followed by API documentation [11].

5 DISCUSSION
We discuss the implications of our work to Jupyter notebook users,
notebook tool designers, and educators. We also provide insights
about the diﬀerences in debugging notebooks and non-notebook
code, and the threats to the validity of our work.

5.1 Implications
In general, the error-ﬁnding strategies we identiﬁed point to the
need for more tool support when developing Jupyter Notebooks
to bring them to the same level as support in more mature non-
notebook code tools. For example, the release of Jupyter Lab 3.0
introduced a visual debugger that can be used to step through code
or to check the value of a variable [21]. This need for more tool
support is suggested by other studies as well [6, 28].

We also uncovered two strategies that are not common in other
approaches and may be speciﬁc to notebooks: Re-implement to Check

Error Identification Strategies for Python Jupyter Notebooks

and Start With What You Know. We discuss the implications of
these two strategies for notebook stakeholders below.

Murphy et al. and Whalley et al., that are similar to those we have
identiﬁed [14, 26].

Re-implement to Check:

Tool designers could implement a tool which supplies the user
with code snippets that use a diﬀerent implementation so
they could compare if the results are the same.

Users, if unsure what a particular code cell does, could be advised
to re-implement the code to increase their understanding of
the code in question and make it easier to identify an error.
Educators, when teaching students how to perform a task, could
help students be aware that there may be more than one
correct implementation. This would mitigate the false as-
sumption that unfamiliar implementations (e.g., Pythonic
list comprehensions) are incorrect.

Start With What You Know:

Tool designers could provide complexity measures for code cells
so that the user can compare their own previous experience
with the complexity of the cell to gauge where to start.
Users could be advised to start by self-reﬂecting on their skills in
code understanding (e.g., data cleaning vs. statistical analy-
sis) and start the process of error identiﬁcation in cells by
leveraging that skill. This may make the process of error
ﬁnding easier as the user is more familiar with this approach
and can build conﬁdence in error ﬁnding.

Educators should understand what students are most familiar
with (statistical, code, data domains) and then help them
build knowledge in other areas. They could include a compo-
nent on Jupyter or other notebook-speciﬁc debugging skills,
such as the shift tab shortcut to access documentation.

5.2 Comparing Debugging Notebooks and
Debugging Non-Notebook Code

The development of non-notebook code diﬀers from the develop-
ment of computational notebooks. The type of problem managed
in a notebook involves more data wrangling, experimentation, and
analysis code. Following the study which inspired our research
[13], our study separated these into potential problems with sta-
tistics, programming, and data / domain knowledge. The notebook
environment also has a literate programming component that goes
beyond code comments, with markdown cells that can be used to
describe the purpose of the code.

Furthermore, non-notebook IDEs have robust tool support for
debugging, for example, setting breakpoints in IntelliJ. However,
in the traditional computational notebook interface (say Jupyter
Notebook), debugging is not speciﬁcally supported by the tool. Data
science tools are actively working to ﬁx this, for example, Jupyter
Lab’s debugger [21] and RStudio’s debugging interface. IDEs are
also now able to integrate notebook code into the IDE directly, such
as with Visual Studio Code.

Given these diﬀerences, we ask whether error identiﬁcation ap-
proaches for Jupyter Notebooks are also diﬀerent. This study iden-
tiﬁed several strategies participants used to identify errors in Jupyter
Notebooks. Other researchers have identiﬁed strategies for debug-
ging non-notebook code. Table 7 outlines strategies identiﬁed by

Table 7: Strategies Similar to Those We Identiﬁed

Strategies We Identiﬁed

Similar Strategies

Search Engine-Driven Approach Using Resources [14]
Assume and (Sometimes) Check

Expectation Conﬁrmation
Once-over

Re-implement to Check
Key Information

Start With What You Know

Information Gathering [26], Bug
Location [26]
Pattern Matching [14]
Gain Domain Knowledge [14], Un-
derstanding the Code [14], Static
Code Comprehension [26]
N/A
Understanding Code [14], Static
Code Comprehension [26]
N/A

The Search Engine-Driven Approach strategy is closely re-
lated to the Using Resources strategy identiﬁed by Murphy et al.
in [14]. Both strategies involve the use of documentation and tu-
torials. The diﬀerence between these two strategies is that we ob-
served our participants using the search engine as their gateway
to many resources. Murphy et al. make no mention of the search
engine.

Both the Information Gathering and Bug Location strate-
gies identiﬁed by Whalley et al. mention the use of speculation and
guessing about the locations and causes of bugs. Alaboudi and La-
Toza also report on using hypotheses as a debugging aid [1]. Our
Assume and (Sometimes) Check strategy is similar, based on
making an assumption and optionally checking that assumption.
In their description of the Pattern Matching strategy, Murphy
et al. state that their participants found bugs due to things not
“looking right”. In our notebook study, this was made more explicit
than the heuristics Murphy et al. describe. Our participants were
able to identify errors when a code cell seemed like it was not cor-
rect based on a description given in a markdown cell (the Expec-
tation Conﬁrmation strategy). The breakdown of an expectation
could be thought of as pattern matching as our participants were
attempting to match the pattern of what they were told the code
was trying to accomplish to what they could observe the code do-
ing. However, the presence of explicit documentation makes this
strategy quite successful (at least for our example notebooks).

The Once-over and Key Information strategies identiﬁed by
us are both similar to the Understanding Code and Static Code
Comprehension strategies identiﬁed by Murphy et al. and Whal-
ley et al., respectively. In addition, the Once-over strategy is sim-
ilar to the Gain Domain Knowledge strategy identiﬁed by Mur-
phy et al. Both of our strategies were used in order to gain un-
derstanding about the contents of the notebook, and involve com-
prehending both the code and the markdown, much like the Un-
derstanding Code and Static Code Comprehension strategies
are about comprehending code. The Once-over strategy was used
to gain domain knowledge in the sense that the four participants
who used this strategy did so to gain a brief understanding of the
domain the notebook covered.

D. Robinson, N. Ernst, E. Larios Vargas, M.-A. Storey

of our participants were students who used Jupyter Notebooks for
school assignments and not professionally. However, we designed
the tasks according to our participants’ skill levels, and the con-
text of tasks was fairly approachable (elections and sports) by any
participant independently of their academic background. Our par-
ticipants did not express unfamiliarity with the domain. However,
some participants expressed unfamiliarity with speciﬁc packages
imported into the notebook, namely Pandas and Plotnine. Another
threat to external validity is the inclusion of markdown cells in the
notebooks which may not reﬂect real-world notebooks [17].

Reliability. The open coding process was performed by one re-
searcher, the ﬁrst author of this paper. To reduce potential researcher
bias and subjectivity, we conducted several discussion sessions to
iteratively build a codebook. We conﬁrmed with the feedback of
an expert reviewer, the fourth author of this paper, to raise the re-
liability and maturity of our ﬁndings.

7 CONCLUSION
We conducted an observational study with fourteen participants,
mostly university students from varying technical backgrounds,
and observed the strategies these Jupyter Notebook users employed
to identify errors seeded in four sample notebooks. The most com-
monly used strategy we observed was using the search engine to
ﬁnd external help such as API documentation or websites that pro-
vide a tutorial. However, the most successful strategy was Expecta-
tion Conﬁrmation, when they discovered a mismatch between the
description and the code itself. We identiﬁed some implications
for practice, including the need for better debugging support in
notebooks, and showed that while there are similarities with non-
notebook code, debugging in notebooks leverages notebook-only
properties such as code cell independence and hidden state. Out of
the seven identiﬁed strategies, ﬁve had been previously identiﬁed
in the literature as debugging strategies for non-notebook code,
while two are novel to the notebook environment. Future work
could involve collaborating with members of the original RMark-
down study to compare the error ﬁnding strategies of data scien-
tists between the respective studies. We hope our insights will help
both notebook tool designers and educators improve how data sci-
entists discover errors more easily in their notebooks.

ACKNOWLEDGMENTS
We thank Kelly Bodwin and Ian Flores Siaca for sharing the initial
RMarkdown notebooks; Hunter Glanz for reviewing our Python
notebook translations; and Amelia McNamara, Amal Abel-Ghani,
Philipp Burckhardt, Allison Theobold and Greg Wilson for the study
idea. We acknowledge the support of the Natural Sciences and En-
gineering Research Council of Canada and Venture for Canada.

One key diﬀerence between notebooks and non-notebook code
is that code cells are capable of independent output, closer to a
Read-Eval-Print Loop session than debugging a complete source
ﬁle. This diﬀerence may be to blame for why the Re-implement
to Check and Start With What You Know strategies were not
observed in other literature related to debugging non-notebook
code. As the code in notebooks is often more granular and inde-
pendent, these strategies are more viable when used in a notebook
debugging context. This independence and high granularity allows
for easier isolation of changes and re-implementations as a single
unit of code in notebooks than in non-notebook code. The value of
these strategies, however, may be dependent on the user’s level of
experience. For example, the Re-implement to Check strategy
would more likely be adopted by users who know more than one
way to implement a given task. On the other hand, the Start With
What You Know strategy is more likely to be used by novice users
that may want to stay within their comfort zone for as long as pos-
sible.

We found that debugging non-notebook code diﬀers from de-
bugging computational notebooks in a few ways. One, the type of
development is diﬀerent: there are more data science-related tasks
such as data wrangling. Two, the development tools are at diﬀer-
ent levels of maturity when it comes to debugging support. Three,
while ﬁve out of seven of the strategies we observed are related
to non-notebook code debugging strategies identiﬁed in the litera-
ture, we found that two strategies were not found in non-notebook
code studies. We also saw diﬀerences in how Expectation Conﬁr-
mation and Assume and (Sometimes) Check are conducted in
practice, given the way a notebook isolates individual code cells.

6 THREATS TO VALIDITY
In the following, we address the validity of this study in the context
of qualitative research [8, 12].

Internal validity. We did not impose time-constraints on our par-
ticipants, and they were assured our study was not a test of their
skill. However, given the nature of the task, it is possible our par-
ticipants felt pressure to perform well. Due to this pressure, par-
ticipants may have overlooked errors in the Jupyter Notebooks.
However, during the interviews we conducted immediately after
the tasks, we did not detect that our participants felt any undue
stress due to the study.

Construct validity. Our study prompt and task description may
have inﬂuenced participants to perform actions which were not
part of their typical error identiﬁcation process in Jupyter Note-
books. For instance, modifying the notebook, searching documen-
tation, and using the internet for help may not have been natu-
ralistic behaviours. To mitigate this threat, we adopted multiple
strategies, such as two rounds of pilots, to ensure comprehensibil-
ity and raise the realism of the tasks. In addition, task descriptions
and scripts were reviewed and validated by a domain expert, and
the task was conﬁrmed to be within the recruited participants’ skill
level.

External validity. The primary threat to external validity is how
we recruited and selected participants. We used convenience sam-
pling methods to recruit participants from upper-level undergrad-
uate and graduate-level courses at the university. Therefore, most

Error Identification Strategies for Python Jupyter Notebooks

REFERENCES
[1] Abdulaziz Alaboudi and Thomas D. LaToza. 2020.

Using Hypotheses
as a Debugging Aid. In 2020 IEEE Symposium on Visual Languages and
Human-Centric Computing (VL/HCC).
IEEE, Dunedin, New Zealand, 1–9.
https://doi.org/10.1109/VL/HCC50065.2020.9127273

[2] Abdulaziz Alaboudi and Thomas D. LaToza. 2021. An Exploratory Study
arXiv:2105.02162

CoRR abs/2105.02162 (2021).

of Debugging Episodes.
https://arxiv.org/abs/2105.02162

[3] Kelly Nicole Bodwin, Ian Flores Siaca, and Derek Robinson. 2022. Materials for
"Looks okay to me": A study of best practice in data analysis code review. (April
2022). https://doi.org/10.5281/zenodo.6419727

[4] Andrew W Brown, Kathryn A Kaiser, and David B Allison. 2018. Issues with data
and analyses: Errors, underlying themes, and potential solutions. Proceedings of
the National Academy of Sciences 115, 11 (2018), 2563–2570.

[5] Alberto Cardoso, Joaquim Leitão, and César Teixeira. 2019. Using the Jupyter
Notebook as a Tool to Support the Teaching and Learning Processes in Engi-
neering Courses. In The Challenges of the Digital Transformation in Education,
Michael E. Auer and Thrasyvoulos Tsiatsos (Eds.). Springer International Pub-
lishing, Cham, 227–236.

[6] Souti Chattopadhyay, Ishita Prasad, Austin Z. Henley, Anita Sarma, and Titus
Barik. 2020. What’s Wrong with Computational Notebooks? Pain Points, Needs,
and Design Opportunities. In Proceedings of the 2020 CHI Conference on Human
Factors in Computing Systems. Association for Computing Machinery, New York,
NY, USA, 1–12. https://doi.org/10.1145/3313831.3376729

[7] Pierre Dragicevic, Yvonne Jansen, Abhraneel Sarma, Matthew Kay, and Fanny
Chevalier. 2019.
Increasing the Transparency of Research Papers with Ex-
plorable Multiverse Analyses. In Proceedings of the 2019 CHI Conference on Hu-
man Factors in Computing Systems. https://doi.org/10.1145/3290605.3300295
[8] Egon G Guba. 1981. Criteria for assessing the trustworthiness of naturalistic
inquiries. Educational Technology research and development 29, 2 (1981), 75–91.
[9] Andrew Head, Fred Hohman, Titus Barik, Steven M Drucker, and Robert DeLine.
2019. Managing messes in computational notebooks. In Proceedings of the 2019
CHI Conference on Human Factors in Computing Systems. 1–12.

[10] Mary Beth Kery, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A
Myers. 2018. The Story in the Notebook: Exploratory Data Science Using a Lit-
erate Programming Tool. In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems (CHI ’18). Association for Computing Machinery,
New York, NY, USA. https://doi.org/10.1145/3173574.3173748

[11] Andreas P. Koenzen, Neil A. Ernst, and Margaret-Anne D. Storey. 2020. Code
Duplication and Reuse in Jupyter Notebooks. In 2020 IEEE Symposium on Vi-
sual Languages and Human-Centric Computing (VL/HCC). IEEE, Dunedin, New
Zealand, 1–9. https://doi.org/10.1109/VL/HCC50065.2020.9127202

[12] Irene Korstjens and Albine Moser. 2018. Series: Practical guidance to qualitative
research. Part 4: Trustworthiness and publishing. European Journal of General
Practice 24, 1 (2018), 120–124.

[13] Amelia McNamara, Amal Abel-Ghani, Ian Siaca Flores, Kelly Bodwin, Allison
Theobold, Philipp Burkhardt, and Greg Wilson. 2022. Looks okay to me: A study
of best practice in data analysis code review. (2022). in preparation.

[14] Laurie Murphy, Gary Lewandowski, Renée McCauley, Beth Simon, Lynda
Thomas, and Carol Zander. 2008. Debugging: the good, the bad, and the quirky–
a qualitative analysis of novices’ strategies. ACM SIGCSE Bulletin 40, 1 (2008),
163–167.

[15] Peter Parente. 2014. Estimate of public Jupyter notebooks on GitHub.
https://github.com/parente/nbestimate (Accessed on 01/16/2022).

(2014).

[16] Jeﬀrey M Perkel. 2018.
tational notebook of
https://doi.org/10.1038/d41586-018-07196-1

Why Jupyter
Nature

choice.

is data scientists’ compu-
145–147.
563, 7732 (2018),

[17] João Felipe Pimentel, Leonardo Murta, Vanessa Braganholo, and Juliana
Freire. 2019.
A Large-Scale Study About Quality and Reproducibility
of Jupyter Notebooks. In 2019 IEEE/ACM 16th International Conference on
Mining Software Repositories (MSR). IEEE, Montreal, QC, Canada, 507–517.
https://doi.org/10.1109/MSR.2019.00077

[18] El Kindi Rezig, Ashrita Brahmaroutu, Nesime Tatbul, Mourad Ouzzani,
and Michael Stone-
Debugging large-scale data science pipelines using Dag-
Proceedings of the VLDB Endowment 13, 12 (Aug. 2020), 2993–2996.

Nan Tang, Timothy Mattson, Samuel Madden,
braker. 2020.
ger.
https://doi.org/10.14778/3415478.3415527

[19] Adam Rule, Aurélien Tabard, and James D. Hollan. 2018.

and Explanation in Computational Notebooks.
2018 CHI Conference on Human Factors
https://doi.org/10.1145/3173574.3173606

Exploration
In Proceedings of
the
in Computing Systems. 1–12.

[20] Adam A Smith. 2016. Teaching computer science to biologists and chemists, us-
ing jupyter notebooks: tutorial presentation. Journal of Computing Sciences in
Colleges 32, 1 (2016), 126–128. https://doi.org/10.5555/3007225.3007252

[21] Jeremy Tuloup. 2021. JupyterLab 3.0 is released!. The 3.0 release of JupyterLab
brings . . . . (2021). https://blog.jupyter.org/jupyterlab-3-0-is-out-4f58385e25bb
(Accessed on 07/28/2021).

[22] Jiawei Wang, Tzu-yang Kuo, Li Li, and Andreas Zeller. 2020. Restoring Repro-
ducibility of Jupyter Notebooks. Association for Computing Machinery, New
York, NY, USA, 288–289. https://doi.org/10.1145/3377812.3390803

[23] Jiawei Wang, Li Li, and Andreas Zeller. 2020. Better Code, Better Sharing: On the
Need of Analyzing Jupyter Notebooks. In Proceedings of the ACM/IEEE 42nd In-
ternational Conference on Software Engineering: New Ideas and Emerging Results
(ICSE-NIER ’20). Association for Computing Machinery, New York, NY, USA, 53–
56. https://doi.org/10.1145/3377816.3381724

[24] Nathaniel Weinman, Steven M. Drucker, Titus Barik, and Robert DeLine. 2021.
Fork It: Supporting Stateful Alternatives in Computational Notebooks. In Pro-
ceedings of the 2021 CHI Conference on Human Factors in Computing Systems.
https://doi.org/10.1145/3411764.3445527

[25] Charles J. Weiss. 2021.

A Creative Commons Textbook for Teach-
ing Scientiﬁc Computing to Chemistry Students with Python and
Jupyter Notebooks.
Journal of Chemical Education 98, 2 (2021), 489–494.
https://doi.org/10.1021/acs.jchemed.0c01071

[26] Jacqueline Whalley, Amber Settle, and Andrew Luxton-Reilly. 2021. Novice Re-
ﬂections on Debugging. Association for Computing Machinery, New York, NY,
USA, 73–79. https://doi.org/10.1145/3408877.3432374

[27] Kanit Wongsuphasawat, Yang Liu, and Jeﬀrey Heer. 2019. Goals, Process,
and Challenges of Exploratory Data Analysis: An Interview Study. CoRR
abs/1911.00568 (2019). arXiv:1911.00568 http://arxiv.org/abs/1911.00568
[28] Chenyang Yang, Shurui Zhou, Jin LC Guo, and Christian Kästner. 2021. Subtle
bugs everywhere: Generating documentation for data wrangling code. In Pro-
ceedings of the 36th IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, Vol. 11.

[29] Andreas Zeller. 2009. CHAPTER 1 - How Failures Come to Be.

In Why Pro-
grams Fail (Second Edition) (second edition ed.), Andreas Zeller (Ed.). Morgan
Kaufmann, Boston, 1–23. https://doi.org/10.1016/B978-0-12-374515-6.00001-0

