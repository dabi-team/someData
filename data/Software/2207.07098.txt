Large-Scale Direct Numerical
Simulations of Turbulence Using GPUs
and Modern Fortran

Journal Title
XX(X):1–13
©The Author(s) 2016
Reprints and permission:
sagepub.co.uk/journalsPermissions.nav
DOI: 10.1177/ToBeAssigned
www.sagepub.com/

SAGE

Martin Karp1, Daniele Massaro2, Niclas Jansson3, Alistair Hart4, Jacob Wahlgren1, Philipp
Schlatter2, and Stefano Markidis1

2
2
0
2

n
u
J

3
2

]
S
M

.
s
c
[

1
v
8
9
0
7
0
.
7
0
2
2
:
v
i
X
r
a

Abstract
We present our approach to making direct numerical simulations of turbulence with applications in sustainable shipping.
We use modern Fortran and the spectral element method to leverage and scale on supercomputers powered by the
Nvidia A100 and the recent AMD Instinct MI250X GPUs, while still providing support for user software developed in
Fortran. We demonstrate the efﬁciency of our approach by performing the world’s ﬁrst direct numerical simulation of the
ﬂow around a Flettner rotor at Re=30’000 and its interaction with a turbulent boundary layer. We present one of the ﬁrst
performance comparisons between the AMD Instinct MI250X and Nvidia A100 GPUs for scalable computational ﬂuid
dynamics. Our results show that one MI250X offers performance on par with two A100 GPUs and has a similar power
efﬁciency.

Keywords
GPU, Fortran, Spectral Element Method, Computational Fluid Dynamics, Direct Numerical Simulation, High
Performance Computing

Introduction

High-performance computing resources drive discoveries in
ﬁelds of crucial importance for society. A societal issue
of increasing importance is environmental sustainability
and reducing our energy consumption. Our dependence
on non-renewable energy has become an increasingly
pressing issue due to climate change and increasing global
temperatures (Pachauri et al. 2007).

Global shipping is responsible for more than 2% of all
carbon emissions and the largest cargo ships in the world
make up a signiﬁcant portion of these emissions (Smith
et al. 2015). As the number of tonnes of goods transported
by waterway is only expected to increase in the coming
years, reducing fuel consumption will have a signiﬁcant
impact on both costs, reliablity, and emissions (Smith
et al. 2015). Active research is concerned with several
unconventional designs being considered to make ships more
energy efﬁcient. One of these approaches is the Flettner rotor,
a rotating cylinder that can reduce the fuel consumption
in ships by more than 20% by using the Magnus force,
effectively working as a sail (Magnus 1853; Seddiek and
Ammar 2021). Rotors are now being tested in practice on
ships as illustrated in Figure 1. However, as far as numerical
simulations go we have so far been limited to less detailed
and accurate simulations based on the Reynolds-Averaged
Navier-Stokes (RANS) approach (De Marco et al. 2016).
Direct numerical simulations (DNS) that resolve all scales of
the ﬂow in both time and space have not been used because
of their high computational cost, both with regards to time
and power usage.

Modern computing clusters are now approaching a
size and power efﬁciency where DNS is possible within
time and number of nodes.
a reasonable amount of

Prepared using sagej.cls [Version: 2017/01/17 v1.20]

Especially, as Graphics Processing Units (GPUs) have
become commonplace and outmatch older computing
solutions with regards to power efﬁciency and performance
(9 out of the top 10 Green 500 computers are powered by
GPUs in November 2021 (Green500 2021). However, while
our computer systems are more performant and energy-
efﬁcient than ever, computer systems have also grown more
complex and heterogeneous.

The increased complexity of modern computer systems
has limited the usage of
trusted legacy software that
scales and performs exemplary on multi-core processors.
Therefore, several new frameworks that automate parts of the
porting process to heterogeneous hardware, or non-intrusive
pragma based approaches, have been introduced (Medina
et al. 2014; Keryell et al. 2015; Wienke et al. 2012).
However, these packages either require large code changes
if the legacy code is written in Fortran 77, or offers relatively
low performance. We take another approach, where we use
modern Fortran to accommodate various backends, but not
be dependent on any single overarching software package.
Instead we aim to provide the ﬂexibility to use the most

1Department of Computer Science, KTH Royal Institute of Technology,
Stockholm, Sweden
2Engineering Mechanics, KTH Royal Institute of Technology, Stockholm,
Sweden 3PDC Centre for High Performance Computing, KTH Royal
Institute of Technology, Stockholm, Sweden 4 Hewlett Packard Enterpise
(HPE), UK

Corresponding author:
Martin Karp, KTH Royal Institute of Technology, School of Electrical
Engineering and Computer Science, Lindstedsv ¨agen 5, 100 44
Stockholm, Sweden
Email: makarp@kth.se

 
 
 
 
 
 
2

Journal Title XX(X)

Neko

Neko is a spectral element framework targeting incompress-
ible ﬂows. With its roots and main inspiration in Nek5000, a
CFD solver widely acclaimed for its numerical methods and
scalability on multicore processors (Tufo and Fischer 1999),
Neko is a new code making use of object-oriented Fortran to
control memory allocation and allow multi-tier abstractions
of the solver stack. Neko has been used to accommodate
various hardware backends ranging from general-purpose
processors, accelerators, vector processors (Jansson 2021),
as well as (limited) FPGA support (Karp et al. 2021, 2022b).
In our work, we extend and optimize Neko to perform a
large-scale DNS of practical industrial relevance on GPUs.
The spectral element method used in Neko is a high-order,
matrix-free, ﬁnite element method which provides high order
convergence on unstructured meshes. Here we provide a
brief overview of Neko, the spectral element discretization,
time-stepping schemes, numerical solvers, and an overview
of the overarching software structure.

Discretization

We integrate the incompressible, non-dimensional, Navier-
Stokes equation in time

∇ · v = 0,

∂v
∂t

+ v · ∇v = −∇p +

1
Re

∇2v + F

(1)

(2)

where Re = U L
is a Reynolds number deﬁned by a
ν
characteristic velocity U , length scale L, and kinematic
viscosity ν, v is the instantaneous velocity ﬁeld, p the non-
dimensional pressure and F an external forcing/source term.
We solve the problem on a domain Ω with suitable boundary
conditions on ∂Ω. As this system of equations is coupled, we
use the so called PN − PN splitting scheme to separate the
pressure and velocity solves at each time step. This splitting
scheme was originally proposed by Karniadakis et al. (1991)
and has shown to have the same order of convergence as
the time stepping scheme that is used, with the error being
primarily isolated to the boundaries of the domain. The
PN − PN scheme discretizes the momentum equation in
time in the following fashion

γ0vn+1
∆t

=

ˆv
∆t

− ∇pn+1 +

1
Re

∇2vn+1

(3)

with

ˆv = ∆t

Je−1
(cid:88)

q=0

βqvn−q · ∇vn−q +

Ji−1
(cid:88)

q=0

αqvn−q + F.

(4)

We use an explicit scheme of order Je for the nonlinear terms
and an implicit scheme of order Ji for the linear terms. In our
case we use a standard backward differentiation scheme of
order three for the implicit terms and a extrapolation scheme
of order three for the explicit terms. To decouple the velocity
and pressure solves we take the divergence of (3) and use
incompressiblity, our explicit treatment of v and the identity

∇2vn+1 = ∇(∇ · vn+1) − ∇ × (∇ × vn+1)

(5)

to arrive at a pressure-Poisson equation for the pressure

− ∇2pn+1 = −

∇ · ˆv
∆t

+

1
Re

∇ · (∇ × ω)

(6)

Figure 1. An illustration of a rotor ship which uses Flettner
rotors (in blue) to reduce fuel consumption and carbon
emissions. We also highlight how the rotor and the wind coming
from the side of the ship generates a force directed forward.

suitable software for a given device. With the Fortran core,
we can also efﬁciently utilize older, veriﬁed, CPU code
without major changes. We base our solver on the numerical
methods and schemes from the spectral element method and
Nek5000 (Fischer et al. 2008) and leverage them in Neko
– a portable spectral element framework targeting several
computer architectures (Jansson et al. 2021).

Neko can both exploit the computing power of modern
accelerators and use older
tools for Nek5000. Using
the spectral element method, Neko provides high-order
convergence on unstructured meshes while having a low
communication cost and high performance. In this work,
we present our development and usage of Neko to leverage
two state-of-the-art GPUs, the Nvidia A100 and the very
recent AMD Instinct MI250X. In addition, we interface
with older toolboxes written for Fortran 77. We present
the efﬁciency of our solver by making the ﬁrst DNS of a
Flettner rotor in a turbulent boundary layer. We illustrate how
our developments lead to energy and time savings, both by
strong-scaling and reducing the run time of the simulation,
but also how a solver in modern Fortran can make the porting
process to new hardware easier.

Our contribution is the following:

• We illustrate the software and algorithmic considera-
tions in developing Neko, a highly scalable solver that
caters both to increasingly heterogeneous supercom-
puters and legacy user software.

• We present the ﬁrst direct numerical simulation of
a Flettner rotor and its interaction with a turbulent
boundary layer at Reynolds number of 30 000.

• We show the scaling, performance, and power
measurements of our simulations on both the new
AMD Instinct MI250X and Nvidia A100 GPUs,
showing a matching power efﬁciency and that one
MI250X performs similarly to two A100s.

The paper is organized as follows, in Section II. we present
Neko, its numerical methods and software design. In Section
III. we brieﬂy cover the physics of a Flettner rotor before we
describe in Section IV. our simulation setup and computer
platforms used for our experiments. In Section V. we present
the simulation results and compare the performance and
power efﬁciency of our computer platforms. In the two ﬁnal
sections we cover the related work and make our concluding
remarks.

Prepared using sagej.cls

WindForceKarp et al.

3

with ω being the vorticity computed explicitly

written as

ω =

Je−1
(cid:88)

q=0

βq(∇ × vn−q).

(∇v, ∇u) =

(7)

E
(cid:88)

e

(ve)T DT GeDue =

E
(cid:88)

(ve)T Aeue

(9)

e

√

With this approach, we have decoupled the pressure and
velocity solve and each time step can thus be computed
by ﬁrst solving the system for the pressure according to
(6) followed by using the resulting pressure ﬁeld pn+1
in equation (3) and obtaining the velocity at step n +
1. Crucial to this splitting approach is choosing suitable
boundary conditions for the pressure. It is found that the
temporal error of the splitting scheme is highest along the
boundary and directly depends on properly imposed pressure
boundary conditions. Using Neumann boundary conditions
it is possible to limit the temporal splitting error for the
pressure and divergence to O(∆tJe ) and thus limit the
splitting error for the velocity to O(∆tJe+1) as shown in
Orszag et al. (1986); Karniadakis et al. (1991). In addition, it
can be shown that the error in the divergence of the velocity
at step n + 1 decays exponentially from the boundary
proportionally to e−s/l, where s is a coordinate normal to
γ0ν∆t is the so-called numerical
the boundary and l =
boundary layer thickness. As l → 0 for Re → ∞ the error
decays rapidly from the boundary for highly turbulent
ﬂows (Karniadakis et al. 1991). Unique for this splitting
scheme, compared to the more conventional PN − PN −2
splitting for the spectral element method presented by Maday
and Patera (1989) is that the velocity and pressure solve can
be done with the same function space, and thus use the same
polynomial degree N . With our temporal discretization in
place, we apply our spectral element discretization in space.
The spectral element method is used because of its high-
order convergence and high accuracy with relatively few
grid points in space compared to e.g. ﬁnite volume methods,
while still accommodating unstructured meshes (Deville
et al. 2002). The linear system is evaluated in a matrix-
free fashion, using optimized tensor-operations, which
yields a high operational intensity, performance, and low
communication costs. These aspects make the spectral
element method well suited for DNS in complex geometries.
For the spectral element discretization we consider the
weak form of the Navier-Stokes equations and decompose
the domain into E non-overlapping hexahedral elements
Ω = (cid:83)E
e Ωe. On these elements we represent the solution
with polynomial basis functions based on the Legendre
polynomials of degree N interpolated on the Gauss-Lobatto-
Legendre (GLL) points ξi, i = 0, N . In 3D the number of
GLL points per element is (N + 1)3. A function on the
reference element can be represented according to

ue(ξ, η, γ) =

N
(cid:88)

i,j,k=0

ue
i,j,kli(ξ)lj(η)lk(γ)

(8)

where li are the polynomial basis functions and ξ, η, γ
are the location within the reference element. As we use
non-overlapping hexahedral elements, differentiation can be
evaluated efﬁciently through tensor-products Orszag (1979).
For instance, the weak form of the Laplace operator can

Prepared using sagej.cls

where we introduce the differentiation matrices D and the
geometric factors Ge. These two matrices are deﬁned as

Dii =

dli(ξ)
dξ

(cid:12)
(cid:12)
(cid:12)
(cid:12)ξi

,

(Ge

st)ijk = ρiρjρkJijk

3
(cid:88)

l=1

∂rs
∂xl

∂rt
∂xl

st = Ge

where J = ∂x
is the Jacobian for the mapping to the
∂r
reference element. We have that Ge
ts and thus we
have that Ge is symmetric and contains 6 non-trivial
values per Gauss-Lobatto-Legendre (GLL) point. In Neko,
the global system Ae is never formed as this would be
prohibitely expensive, but Ae is instead evaluated in a
matrix-free fashion (Deville et al. 2002). For this we
introduce the local representation uL = Qu where Q is
known as the scatter operator. The local representation uL
contains duplicate values along element boundaries, but
enables for efﬁcient tensor-operations. By ensuring that

ijk = ue(cid:48)
ue

i(cid:48)j(cid:48)k(cid:48)

if xe

ijk = xe(cid:48)

i(cid:48)j(cid:48)k(cid:48)

(10)

and using the local representation we can therefore evaluate
the Laplace operator as

E
(cid:88)

(ve)T Aeue = (Qv)T ALQu

(11)

e

with AL being the block diagonal matrix of Ae. The ﬁnal
discrete system that we solve for can thus be written as

QQT ALuL = AuL.

(12)

A similar approach is applied to the Helmholtz equation
for the velocity arising from the implicit treatment of the
viscous term. Using this discretization and time splitting we
have all the components to set up our ﬂow case. To solve
the systems for the velocity and pressure we use heavily
optimized numerical solvers.

Numerical Solvers

The pressure solve is the main source of stiffness in
incompressible ﬂow and because of this, we use a restarted
generalized minimal residual method (GMRES) (Saad and
Schultz 1986) combined with a preconditioner based on
an overlapping additive Schwarz method and a coarse
the
grid solve (Lottes and Fischer 2005).
In Neko,
coarse grid solve is done with 10 iterations of
the
preconditioned conjugate gradient (P-CG) method. This
type of preconditioner signiﬁcantly reduces the number
of iterations necessary for convergence at the expense of
increased work per iteration.

For the velocity solver, we utilize P-CG with a block-
Jacobi preconditioner. For both the velocity and pressure
solve we utilize a projection scheme where we project the
solution of the current time step on previous solution vectors
to decrease the start residual signiﬁcantly. This has been
observed to reduce the number of iterations for the Krylov

4

Journal Title XX(X)

solver signiﬁcantly (Fischer 1998). All of our solvers are
designed with locality across the memory hierarchy in mind,
to be able to as efﬁciently as possible use modern GPUs with
a signiﬁcant machine imbalance. Parts of this optimization
process and the theoretical background is described in Karp
et al. (2022a).

We illustrate how each time step is computed on a high
level
in Figure 2. Here we also illustrate the software
structure of Neko, which we will cover more in-depth in the
next section.

Software Design

The primary consideration in Neko is how to efﬁciently uti-
lize different computer backends, without re-implementing
the whole framework for each backend and while main-
taining the core of the solver in modern Fortran. We solve
this problem by considering the weak form of the equations
used in the spectral element method. The weak formulation
allows us to formulate equations as abstract problems which
enable us to keep the abstractions at the top level of the
software stack and reduce the amount of platform-dependent
kernels to a minimum. The multi-tier abstractions in Neko
are realized using abstract Fortran types, with deferred
implementations of required procedures. For example, to
allow for different formulations of a simulation’s governing
equations, Neko provides an abstract type ax_t, deﬁning
the abstract problem’s matrix-vector product. The abstract
type is shown in Figure 2 under Abstract types and requires
any derived, concrete type to provide an implementation of
the deferred procedure compute. This procedure would
return the action of multiplying the stiffness matrix of a
given equation with a vector. For the Poisson equation, the
compute subroutine corresponds to computing expression
(9).

In a typical object-oriented fashion, whenever a routine
needs a matrix-vector product, it is always expressed as a
call to compute on the abstract base type and never on
the actual concrete implementation. Abstract types are all
deﬁned at the top level in the solver stack during initialization
and represent large, compute-intensive kernels, thus reducing
overhead costs associated with the abstraction layer.

Furthermore,

this abstraction also accommodates the
possibility of providing tuned matrix-vector products
(ax_t) for speciﬁc hardware, only providing a particular
implementation of compute without having to modify
the entire solver stack. The ease of supporting various
hardware backends is the key feature behind the performance
portability of Neko.

However, a portable matrix-vector multiplication backend
is not enough to ensure performance portability. Therefore,
abstract types are used to describe a ﬂow solver’s common
parts. These abstract types describe all the parts necessary
to compute a time step and provides deferred procedures
for essential subroutines. These building blocks then make
up the entire solver used to calculate one time step. We
illustrate this structure in Figure 2 where a solver such
as fluid_pnpn_t implements the deferred subroutine
step. The solver is then ready to solve a particular
case. Each case (case_t) is deﬁned based on a mesh
(mesh_t), parameters (param_t),
together with the
abstract solver (solver_t),
later deﬁned as an actual

Prepared using sagej.cls

extended derived type, such as fluid_pnpn_t, based
on the simulation parameters. The abstract solvers contains
a set of deﬁned derived types necessary for a spectral
element simulation, such as a function space (space_t),
coefﬁcients (coef_t), and various ﬁelds (field_t).
Additionally, they contains further abstract types for deﬁning
matrix-vector products (ax_t) and gather-scatter QQT
kernels (gs_t). Each of these abstract types is associated
with an actual
implementation in an extended derived
type, allowing for hardware or programming model speciﬁc
implementations, all interchangeable at runtime. This way,
Neko can accommodate different backends with both native
and ofﬂoading type execution models without unnecessary
code duplication in the solver stack.

GPU Implementation Considerations

As Neko provides abstractions on several levels it is possible
to utilize different libraries to leverage accelerators. In our
work, we do not rely on vendor-speciﬁc solutions (e.g.
CUDA Fortran) or directives-based approaches;
instead,
we exploit the device abstraction layer to manage device
memory, data transfer, and kernel launches from Fortran.
Behind this interface, Neko calls the native accelerator
implementation written in, e.g. CUDA, HIP or OpenCL.
In our work we extend and optimize the CUDA and HIP
backends in Neko.

As we extend several features of the device layer in
Neko in this work, a few aspects of the code development
are important to point out regarding the GPU backend. In
particular, we observed a considerable difference in how
HIP and CUDA kernels map to their respective platform. As
several important kernels in Neko follow a similar tensor-
product structure to the kernels considered by ´Swirydowicz
et al. (2019), and perform close to the memory-bound
rooﬂine on Nvidia GPUs, the same kernels ported to HIP
needed to be changed to better exploit AMD GPUs. We
found that a 1D thread structure and manual calculation of
indices in a thread block enabled higher performance on
the AMD Instinct MI250X, rather than launching a 2D/3D
thread block as we did previously on Nvidia. It would also
appear that there is a difference in the order the threads
execute their operations on AMD, meaning that we at one
point needed to synchronize the thread-block in HIP, while
the same code performed deterministically on Nvidia GPUs.
These differences between CUDA/Nvidia and HIP/AMD
warrant further and a more systematic investigation as it is
not immediately evident why we observe such differences.

One contribution of the GPU backend in Neko is that
we limit data movement to and from the device as far as
possible. In practice, this means that only data needed for
communication over the network or to do I/O is sent back
to the host. As the bandwidth between host and device is
signiﬁcantly lower than that of the high-bandwidth memory
(HBM) on the GPUs, limiting data movement to the host
is essential to obtain high performance. However, this also
means that the HBM memory of the GPUs must ﬁt all the
data necessary to carry out the simulation. While it might be
beneﬁcial to utilize both CPU and GPUs in the future, the
GPU nodes evaluated in this work have signiﬁcantly more
powerful GPUs than the host CPU with regards to both ﬂop/s
and memory bandwidth (more than 10×). We found that the

Karp et al.

5

Figure 2. A chart illustrating the structure of the Neko framework. The Neko framework, with various abstract types and backends
make up different solvers which are used to solve a case. The case is deﬁned by user provided functions, as well as the input
parameters and mesh. In our case, the simulation is performed using the PN − PN method to perform a DNS of a Flettner rotor.

most effective way to leverage this computing power was
to use the host only as a coordinator and ofﬂoad the entire
calculation to the GPUs.

In addition to limiting the data movement to the host, we
also avoid data movement from the GPU global memory
as far as possible. We do this by merging kernels and by
utilizing shared memory and registers in compute heavy
kernels. For modern GPUs, the spectral element method is
in the memory-bound domain as discussed by Kolev et al.
(2021) and optimizing the code for temporal and spatial
locality is our main priority when designing kernels for the
GPU backend in Neko.

Using the device abstraction layer,

from the user
perspective, it is possible to write user-speciﬁed functions
without having to implement handwritten accelerator code
e.g. CUDA or HIP. This enables tools from Nek5000 to be
used on the device with relatively minor changes as we
introduce device kernels with the same functionality as the
original subroutines in Nek5000. Neko provides support for
different backends for the gather-scatter operation and Ax as
previously mentioned, but also for mathematical operators
such as the gradient and curl. With the device layer, we can
with only minor changes incorporate veriﬁed toolboxes and
functions necessary for our ﬂow case into Neko and run them
directly on the GPU without sending data back to the host.

Modern Fortran

As is clear from the description of Neko, modern Fortran
(e.g. Fortran 03, 08, 18) offers similar object-oriented
features to other programming languages such as C++ (Reid
2008). Most importantly, modern Fortran supports dynamic
memory allocation. Other features of Fortran are the native
support for the quad data-type, FP128, that all arrays are by
default not aliased, pure functions, and that modern Fortran

both supports older Fortran code and can interface with C-
code. As Fortran is tailored to high-performance numerical
computations it caters to the functionalities necessary to
perform high-ﬁdelity ﬂow simulations.

Flettner Rotor

the Magnus effect

A Flettner rotor, named after the German inventor Anton
Flettner, uses
to generate lift, a
force perpendicular to the incoming ﬂow. The physical
phenomenon has been known in the literature since the
nineteenth century and was introduced ﬁrst by Magnus
(1853). The Magnus effect consists of the generation of
sidewise force by a rotating object, e.g. a cylinder or sphere,
submerged in a ﬂow. The force results from the velocity
changes induced by the spinning motion. The rotation leads
to a pressure difference at the cylinder’s opposite sides,
generating a force pointing from the high to the low pressure
side. In a rotor ship, the cylinder stands vertically and
is mechanically driven to develop lift in the direction of
the ship, as illustrated in Figure 1. Over the last century,
various designs using this effect have been proposed for both
ships and airplanes, but with limited success (Seifert 2012).
Recently, Flettner rotors have gained renewed interest as a
potential alternative to sails to reduce the fuel consumption
of ships on our way towards net-zero carbon emissions.
This is already being tested in practice for a select number
of shipping routes. Because of Flettner rotors’ promise
in reducing carbon emissions, DNS of a Flettner rotor
and its interaction with a turbulent boundary layer are of
fundamental interest.

Experimental Setup

We describe here our precise problem description to
in
accurately capture the physics of a Flettner

rotor

Prepared using sagej.cls

6

Journal Title XX(X)

Figure 3. The spectral elements grid is shown. The domain sizes are reported in terms of cylinder diameter D and open channel
height h (γ = h/D=10). The blue arrows indicate the ﬂow direction.

a turbulent boundary layer. In addition, we detail
the
computational setup for our scaling runs on the GPUs. We
also describe our baseline CPU platform and provide some
further information regarding our ﬁnal production run that
was used to collect the simulation results.

Flow Conﬁguration

In marine applications, the Flettner rotor is usually placed
on the deck of the ship and hit by an incoming ﬂow, which
is reasonably modeled by a turbulent boundary layer with a
certain thickness, typically on the order of the rotor height.
In the current model, we simulate such a conﬁguration as an
open channel ﬂow where the Flettner rotor is located at the
origin of our domain. The reference system is oriented with
the y-axis along with the cylinder (vertical) and the x and z
being the streamwise and spanwise directions respectively.
The ﬂow is fully characterized by three non-dimensional
parameters:

• the Reynolds number based on the center-line velocity
ucl and the height h of the open channel: Recl =
uclh/ν, where ν is the kinematic ﬂuid viscosity

• the ratio between the height h and the cylinder

diameter γ = h/D

• the ratio between the center-line and the spinning

cylinder velocity α = usp/ucl.

We set those parameters as Recl = 30 000, γ = 10, and
α = 3, aiming to reproduce a realistic conﬁguration, within
the limits of DNS where all the scales are simulated and no
turbulence model is introduced.

The mesh is Cartesian with a block-structured conﬁgura-
tion, taking into account the resolution requirements in the
different regions and the ﬂow physics. It counts 930070 spec-
tral elements, which turns into n ≈ 0.48 billion unique grid
points since the polynomial order is N = 7 and we have 83
GLL points per element. We show the spectral element mesh
with domain dimensions, without GLL points, in Figure 3.

Prepared using sagej.cls

Close to the cylinder, high resolution is necessary to properly
resolve the smallest scales in the boundary layer and we show
a snapshot of the large number of GLL points in Figure 4.
For the simulation, a time step of ∆t∗ = 2 · 10−5 is used,
corresponding to a Courant–Friedrichs–Lewy (CFL) number
around 0.41. With regards to our iterative solvers, we use
a residual tolerance of 10−5 for the pressure and 10−8 for
the velocity. The choice of tolerances is case-dependent. We
consider different orders of magnitude as the pressure solver
is the most time-consuming. However, the relaxed tolerance
for the pressure is still small enough compared to other
error sources. The grid is designed to avoid any blockage
effects on the rotor, i.e. the domain sizes are adequately, and
conservatively, large: (−50D, 120D) in x, (−25D, 25D)
in z and (0, 10D) in y, where x, z, y are the streamwise,
spanwise and vertical directions respectively.

At

the inﬂow (x = −50D)

the Dirichlet boundary
condition (BC) prescribes a turbulent channel velocity proﬁle
with the power-law ux/ucl = (y/h)1/7. Unlike the parabolic
laminar proﬁle, it is ﬂatter in the central part of the channel
and drops rapidly at the walls. On the rotor, a Dirichlet-
like BC is used as well, prescribing the wall impermeability
and setting a rotational velocity uθ = usp = αucl with α =
3 and uρ = 0. Here, the BC is expressed in cylindrical
coordinates, being uθ and uρ the velocities in the azimuthal
and radial direction, respectively. To avoid incompatibility
conditions at the base of the cylinder the spinning velocity is
smoothed as a function of y

s(y = 0) = 0,
s(0 < y < δ) = usp/(1 + e1/(q−1)+1/q)
s(y ≥ δ) = usp

(13)

where q = y/δ and δ = 0.02h. The outﬂow at x = 120D
consists of natural boundary condition (−pI + ν∇u) · n =
0. We consider an inﬁnitely long cylinder in the y direction,
as a consequence symmetric Robin (or mixed) boundary
the top surface (u · n = 0
conditions are prescribed at

xzxyh25D25D120D50DKarp et al.

7

with (∇u · t) · n = 0). For the spanwise boundaries mixed
conditions allowing transportation are used, similar to the
open boundary, but prescribing no velocity changes in non-
normal directions.

To keep the front streamwise extent of the computational
domain as short as possible, reduce the computational
the laminar-
cost, and sustain the incoming turbulence,
turbulent
transition is initiated via the boundary layer
tripping introduced by Schlatter and ¨Orl¨u (2012). It consists
of a stochastic forcing term which is added in a small
elliptical region close to the wall. The region is centered
along a user-deﬁned line at x0 = −4.5 and it extends from
z = −1 to z = 1. The implementation follows Schlatter and
¨Orl¨u (2012), using a weak random volume force that acts
in the y direction. The term which enters the Navier-Stokes
equation is:

F2 = g(z, t) · exp

(cid:18) (x − x0)2
l2
x

−

(cid:19)

y2
l2
y

Figure 4. The ﬁgure shows the Gauss-Lobatto-Legendre points
around the cylinder. The cylinder rotates along the vertical y
axis, and the blue arrow indicates the direction of rotation.

(14)

where lx and ly are the spatial Gaussian attenuation of
the forcing region. The function g(z, t) consists of two
terms, corresponding to steady and unsteady perturbations,
with amplitudes Ts and Tu respectively. The form is the
following:

g(z, t) = Tsg(z)
+ Tu

(cid:2)(1 − b(t))hi(z) + b(t)hi+1(z)(cid:3)

(15)

with i = int(t/ts), p = t/ts − i, and b(t) = 3p2 − 2p3.
Third-order Lagrangian interpolants with time scale ts are
used for the temporal ﬂuctuations and the forcing has a
continuous ﬁrst order derivative in time. The functions
g(z) and h(z) are Fourier series of unit amplitude with
Nm random coefﬁcients. The noise is generated with a
uniform distribution over all frequencies lower than the
cutoff frequency, corresponding to 2π/ts. The values of the
parameters has to be properly chosen, here we use Ts = 0,
Tu = 0.3, ts = 0.14 and Nm = 40.

This tripping procedure has been used extensively
in Nek5000 (see Tanarro et al.
(2019); Atzori et al.
(2021)) and keeps the same original implementation of the
pseudo-spectral code SIMSON (Chevalier et al. 2007). We
implement the interpolation on the GPU using the Neko
device layer to avoid unnecessary data movement to and
from the device. This can be done in Neko without making
too many changes to the original implementation and we can
rely on a tripping procedure that is well-used and validated
in our simulation.

Computational Setup

We perform our experiments on two different GPU clusters,
Alvis at C3SE at Chalmers University of Technology in
Gothenburg (Sweden) and on an internal HPE Cray EX
system. We present the technical details of both computer
setups in Table 1. The largest difference between the two
setups is the Peak double precision (FP64) performance of
their respective GPUs, but only a factor of two difference
for the available memory bandwidth. As each MI250X
consists of two graphics compute dies (GCDs), each GCD
corresponding to one logical GPU, we will throughout the

Prepared using sagej.cls

discussion compare logical GPUs, comparing one GCD
to one A100. We note that the HPE Cray EX system is
an internal engineering test system at HPE. Although our
results are in line with expectations,
is possible that
the performance or scalability are affected by temporary
hardware settings and conﬁguration.

it

To provide a baseline, we also make CPU measurements
on Dardel, also a Cray HPE EX system at PDC at KTH
Royal Institute of Technology. Each node on Dardel is
equipped with 2 AMD EPYC 7742 CPUs with 256 GB
DDR4 memory. For compilation, we used the Cray Compiler
Environment (CCE) 11.0.4 and Cray MPICH 8.1.11. The
interconnect on Dardel is Slingshot 10.

Experimental Methodology

For our scaling runs we scale between 8 and 32 Alvis nodes
each with four A100 GPUs. We measure the performance
over 500 time steps and use the last 100 to collect the average
wall time per time step as well as the standard deviation. The
same procedure is used on the HPE cluster where we scale
from 4 − 16 nodes, each equipped with four MI250X GPUs.
Since they are dual chip GPUs, we use eight MPI ranks per
node. On Dardel we scale from 16 − 64 nodes, each with two
CPUs, using one MPI rank per core. As the ﬂow case is large,
we could not measure the performance at a fewer number of
nodes.

We also compare the power consumption of the two
different GPU clusters. For these results we rerun the
scaling runs, but also measure the power on the GPUs
during the runs. On the Alvis cluster, a report with
the average power usage for each GPU is automatically
generated after each run through the NVIDIA Datacenter
GPU Management interface. In addition to this automatic
system-provided report, we poll nvidia-smi during the run
to continually measure the power of the GPUs. With these
two measurements, we check that the average power usage
during the run is consistent between the two measurements.
As initialization does not properly represent the power draw

xzR8

Journal Title XX(X)

Setup

Alvis

HPE Cray EX

GPU
CPU
DRAM
MPI
Interconnect

Compiler
GPU Driver
CUDA/ROCM
Bandwidth
GPU Peak FP64
GPU TDP

4 AMD Instinct MI250X

4 Nvidia A100 SXM4
2 Intel Xeon Gold 6338 AMD EPYC 7A53
512GB DDR4
OpenMPI 4.0.5
Mellanox ConnectX-6
(2x200 Gb/s)
GCC 10.2
510.39.01
CUDA 11.1.1
1550 GB/s
9.746 TFLOPS
400W

512GB DDR4
Cray MPICH 8.1.14
HPE Slingshot Interconnect
200 GbE NICs(4x200 Gb/s)
CCE 13.0
5.11.32
ROCM 4.5.2
3277 GB/s
47.87 TFLOPS
560W

Table 1. Hardware and software details for our GPU platforms. Hardware listed is per node. Bandwidth and performance is per
GPU. FP64 from tensor cores omitted.

during a simulation we use the measurements from nvidia-
smi to compute the average power usage for the GPUs during
the run after initialization.

On the internal HPE Cray EX system, the power draw of
the node is measured using a similar method to that on the
Cray XC systems described by Hart et al. (2014). On the
HPE Cray EX system, separate user-readable counters are
provided on each node for the CPU, memory, and each of the
four accelerator sockets. By measuring the energy counters
and the timestamp at the start and end of the job script, a
mean power draw for each component can be computed and
averaged across the nodes used. We also poll the GPUs with
rocm-smi during the run to get an overview of the power
draw during the run. With these measurements, we verify
that the average power draw between the two measurements
during the course of the run is consistent for the GPUs.
We then use the measurements from rocm-smi to compute
the average power usage during the computation, excluding
initialization. We note that the differences in methodology
mean that some care should be taken when comparing time-
averaged power draws between the two GPU systems.

For our pilot production simulation, we use Alvis at C3SE
to generate over a thousand samples of the ﬁeld and produce
over 10TB of ﬂow ﬁeld data over the course of only 3 days
using between 64 and 128 Nvidia A100 GPUs.

Simulation Results

After running the simulation for around 15t∗ ﬂow time units,
where t∗ = tucl/h is the non-dimensional time, and using
around 10 000 GPU hours, we show the ﬂow ﬁeld in Figure
5. We can observe a signiﬁcant interaction between the rotor
and the turbulent boundary layer, with the wake deﬂected by
the spinning motion. The Flettner rotor generates a spanwise
force which could be used to signiﬁcantly reduce the amount
of required thrust of a ship.

A ﬁrst estimation has been performed by time-averaging
the surface integral of the pressure and viscous stress tensor.
The resulting aerodynamic force, Fa, is computed as

where S is the cylinder surface, p the pressure, n the vector
normal to the cylinder surface and τ the viscous stress τij =
µ∂ui/∂uj. According to the standard convention, the force
is normalized with 0.5ρuclhD. As a preliminary result, we
present the most relevant integral quantities, i.e. the drag
and lift coefﬁcients. They are the streamwise and spanwise
normalized force components respectively. The spanwise
lift coefﬁcient Cl = 7.464 shows an agreement with the
experimental data that was measured to be between 7 and
8 for the same α = 3 (Bordogna et al. 2019). The Reynolds
number in our case is smaller, but its inﬂuence on Cl seems
to be limited. Quite the opposite, the drag coefﬁcient Cd =
1.092 is strangely dependent on the Reynolds number. Both
standard deviations are small (order of magnitude ≈ 10−3),
showing no large temporal ﬂuctuations for these integral
estimations. This ﬁrst pilot investigation shows interesting
ﬂow features, as the wake of the cylinder interacts with the
boundary layer, encouraging us to continue the study by
observing the rotation speed inﬂuence or testing different
incoming velocity orientations. We aim to highlight new
ﬂow characteristics by means of statistical ﬂow analysis
the
and the introduction of modal decomposition, e.g.
proper orthogonal decomposition (POD) to extract the most
energetic coherent structures of the ﬂow. More in-depth
knowledge of the Flettner rotor dynamics turns out to be
extremely relevant in many engineering applications. While
our results in this work are preliminary, we have shown how
DNS is a valuable tool in understanding how we can leverage
Flettner rotors for more sustainable transportation and how
DNS can be used to understand ﬂow features not visible in
physical experiments.

Performance Results

For the performance results we compare the two GPU
platforms to a CPU baseline by strong scaling with our
Flettner rotor case. We also compare the power efﬁciency of
the Nvidia A100 and the AMD Instinct 250X GPUs.

Scaling Performance

Fa =

(cid:90)

S

−pn + τ n dA

(16)

In Figure 6 we present
the achieved strong scaling
performance for both the AMD Instinct MI250X, Nvidia

Prepared using sagej.cls

Karp et al.

9

Figure 5. Two zoomed-in visualizations of the ﬂow around our simulated Flettner rotor. We show the velocity magnitude where a
lower velocity is darker violet and a higher velocity becomes white then orange. At the top we show a cut at z = 0 and at the bottom
we show a plane at y = 0.1. The coordinate axes reﬂect the mesh as shown in Figure 3, but are zoomed in to better visualize the
turbulence close to the cylinder.

to a recent report by Kolev et al. (2022) where they observed
that one GCD of the MI250X performs closer to 79% of one
A100 for a single rod simulation with NekRS, a similar code
based on Nek5000 (Fischer et al. 2021). They also share
our impression during code development that kernels that
work well with CUDA/Nvidia do not necessarily map well
to HIP/AMD, but need to be modiﬁed to properly exploit
AMD GPUs. Comparing the two architectures in Tab. 1, and
considering that we expect to be memory-bound as has been
discussed in Karp et al. (2021); Kolev et al. (2021), we see
that the fraction between the bandwidth of two A100 vs one
MI250X GPU is 1.05, indicating that they should perform
similarly. As many solvers in the wider HPC community are
memory-bound, we anticipate that several other codes will
obtain similar performance numbers to ours on upcoming
GPU supercomputers powered by the AMD Instinct MI250X
relative to systems using the Nvidia A100. The relation
between the ﬂoating-point performance is signiﬁcantly larger
than the observed difference, considering we do not use
the tensor cores. The difference in peak FP64 performance
suggests that compute-bound applications in FP64 might see
larger performance improvements with the MI250X.

Figure 6. Performance in time per time step as well as shaded
areas for the standard deviation. The orange and blue lines
represent the GPU systems, while the green line represents the
baseline CPU system. We illustrate linear scaling with dotted
lines for each platform.

A100, and the AMD EPYC 7742 CPU baseline. First,
comparing the two GPUs, it is clear from the time per time
step that
two logical GPUs on the MI250X correspond to
two A100s with regards to performance. We observe that
the perfromance of one A100 and one GCD of the MI250X
matches very well in all measurements. The average time per
time step between the two architectures differs by less than
5%, comparing two A100 to one MI250X. This is in contrast

We observe a slightly higher standard deviation for the
GPUs, compared to the CPU, and this can primarily be
attributed to the projection scheme that is used in Neko. This
scheme resets every 20-time steps on the GPU, effectively
leading to a time variation with a period of 20 for the wall
time per time step. On the CPU the projection space is not
reset but a more complex scheme where the space is re-
orthogonalized is used instead, and we see a lower standard

Prepared using sagej.cls

3264128Number of CPUs or logical GPUs0.20.51[s / time step]Strong ScalingNvidia A100AMD Instinct MI250XAMD EPYC 774210

Journal Title XX(X)

Figure 7. Power usage in watts and the parallel energy efﬁciency of the GPUs, during a simulation. The shaded area shows the
standard deviation between GPUs used in the run for their average power usage. The parallel energy efﬁciency is computed
similarly to the parallel efﬁciency, but considers energy instead of run time.

deviation. Other than this difference, the numerical methods
used are identical on CPU and GPU.

Comparing the two GPUs to the CPU baseline we see
that for a large problem size such as this, the GPUs offer
signiﬁcantly higher performance than modern CPUs at lower
node counts. The CPU requires more nodes to achieve the
same performance as only a few GPU nodes. In particular,
128 CPUs, totaling 8192 cores performs similarly to only
64 Nvidia A100 GPUs or 32 AMD Instinct MI250X for
this speciﬁc problem. While the strong scaling of the CPU
is higher than the GPUs and running on a large number of
cores can offer even higher performance, the possibility of
using fewer nodes to get a similar performance is an alluring
aspect of the GPUs. For even larger problems with even
more elements, these results indicate that GPUs might be
the only way to obtain satisfactory performance for smaller
computing clusters with a limited number of nodes.

We compute the overall parallel efﬁciency for the AMD
Instinct MI250X and Nvidia A100 to 70% and 75% for 128
GCDs and 128 GPUs respectively, with around 3.75 million
grid points (7000 elements) per A100 or GCD. This parallel
efﬁciency is comparable and within one standard deviation.
Also, we observe a slightly better average performance for
the MI250X with fewer nodes than the A100, adding to the
slightly lower parallel efﬁciency. We anticipate that we will
have a satisfactory parallel efﬁciency until 3500 elements
per logical GPU. For the CPU on the other hand we see a
super linear parallel efﬁciency as the problem size per CPU
is very large, between 200 000 grid points (450 elements)
per core when using 32 CPUs. There is a signiﬁcant cache
effect as the number of points per core is decreased, leading
to its super linear scaling. This super linear behavior for the
strong scaling of spectral element codes has been observed
previously and is also described by Fischer et al. (2021);
Offermans et al. (2016). Overall, our parallel efﬁciency is
very high and shows how high-order, matrix-free methods
can be used for large-scale DNS, both with conventional
CPUs and when GPU acceleration is considered.

It is clear from these scaling results that our problem
size is large for the considered number of CPU cores, and
that the case requires at least 16k cores to run efﬁciently
on the CPUs, but could use even more, the scaling limit
on CPUs has previously been measured to around 50-100

elements per core (Offermans et al. 2016). Potentially, the
CPUs could overtake the GPUs for a large number of
cores with regards to performance. However, it is clear
that GPU acceleration is necessary to run a case such
as this on a fewer number of nodes, yielding a higher
performance when the problem size per node is large. For
more realistic ﬂow cases, such as simulating the entire rotor
ship illustrated in Figure 1, we would have a Re of several
million, and thus at least a factor 105 increase in number
of gridpoints, which for the foreseeable future is out of
reach. With upcoming exascale machines we anticipate that
GPU acceleration enables simulations at Re = 300 000,
requiring thousands of GPUs. To properly simulate the entire
ship conﬁguration, accurate wall-models will be essential to
decrease the computational cost as was recently discussed by
Bae and Koumoutsakos (2022).

Energy Efﬁciency

We compare the MI250X and A100 GPUs’ power
consumption in Figure 7. Here, we see that the MI250X and
the A100 perform on par with regard to power efﬁciency
for Neko. The average energy consumed per time step is
consistently within 10% between the two GPUs, with a
slight edge for the A100 for a smaller number of nodes and
the MI250X for a larger number of nodes. The standard
deviation for the average power between GPUs is small
(less than 5%) and the average power measured was highly
consistent across GPUs on different nodes during the runs.
As the AMD MI250X is expected to power several of the
largest upcoming supercomputers in the world, the impact
of its power efﬁciency should not be understated, both
concerning operating costs and sustainability impact. As
the A100 was previously unmatched with regards to power
efﬁciency for this type of application (Karp et al. 2021), our
results indicate that the AMD MI250X is among the most
energy-efﬁcient options for large-scale computational ﬂuid
dynamics (CFD) currently available. Further supporting the
power efﬁciency of the A100 and MI250X is that nine of
the ten most power efﬁcient supercomputers in the world
in November 2021 utilize Nvidia A100 GPUs (Green500
2021). We should note that we omit the energy usage of
the network, CPUs, and other peripherals in our power
measurements and isolate our focus on only the accelerators.

Prepared using sagej.cls

3264128Number of logical GPUs45678910121416[kW]GPU Power UsageNvidia A100AMD Instinct MI250X3264128Number of logical GPUs0.00.20.40.60.81.0Parallel energy efficiencyKarp et al.

11

The power measurement methodology also differs and this
adds uncertainty to our measurements. The overarching
computer system will have a signiﬁcant impact on the energy
efﬁciency of the computations, but for accelerator heavy
systems, the GPUs will draw a signiﬁcant amount of the
overall power.

In Figure 7 we also show the parallel energy efﬁciency,
similar to the usual parallel efﬁciency, but we consider
energy consumed per time step by the GPUs instead of run
time. We note that the energy usage per time step that we
observe is almost constant as we increase the number of
GPUs, unlike the parallel efﬁciency. For the two GPUs, the
parallel energy efﬁciency stays above 80% with the MI250X
being close to or above 90%. This indicates, that while GPUs
do not offer the same strong scaling characteristics as CPUs,
the absolute run time is competitive or better and the energy
needed for the GPUs during the simulation is not heavily
affected by scaling penalties. It might be more important
to consider energy usage per simulation, rather than run
time per simulation when comparing different architectures
and algorithms. This more correctly corresponds with the
operating costs of the computation and sustainability impact.
While we have used the power counters available to us
through rocm/nvidia-smi, we note that obtaining accurate
power usage is a challenge. Currently, direct power meter
measurements are not wide-spread and used in production
systems and it is an open question how we should compare
different components of heterogenous computer systems at
scale with regards to power.

Another aspect of our power measurements is that our
simulations are made in double precision. Using lower
precision arithmetic to reduce power consumption and
increase performance is gaining traction as more and
more computing units support lower precision arithmetic.
Incorporating other precision formats into Neko is an active
area of research, both to increase performance, but also
to decrease the energy consumption of large-scale DNS
even further. Going forward, Neko will be used to assess
the impact of numerical precision on direct numerical
simulations. In addition, including adaptive mesh reﬁnement
and data-compression techniques to reduce the load on the
I/O subsystem will let us use Neko for a wider range of cases
and more complex geometries.

Related Work

Our work targets high-ﬁdelity incompressible ﬂow simula-
tions and is based on a spectral element method. The spectral
element method was ﬁrst introduced for CFD by Patera
(1984); Maday and Patera (1989). The method has then been
developed and derived into several different frameworks
such as Nektar++ (Cantwell et al. 2015) and Nek5000 (Fis-
cher et al. 2008). For DNS, Nek5000 has been well used
because of its extreme scalability and accuracy but uses
static memory and Fortran 77, making it most suitable for
CPUs (Tufo and Fischer 1999), although acceleration using
OpenACC directives has been considered (Otero et al. 2019).
Neko shares its roots with Nek5000, but unlike Nek5000,
Neko makes use of object-oriented modern Fortran, dynamic
memory, and uses a new communication backend. Another
approach based on Nek5000, NekRS (Fischer et al. 2021),

Prepared using sagej.cls

also provides GPU support, but is rewritten in C++ and
uses OCCA extensively to generate code for different back-
ends (Medina et al. 2014). As our simulation makes use of
the openly available KTH Toolboxes originally developed
for Nek5000 (Peplinski 2022), we could with Neko’s device
layer make very limited changes to port the necessary parts
to Neko and GPUs.

Flettner rotors have recently been studied by conducting
massive experimental campaigns in a large wind tunnel (Bor-
dogna et al. 2019, 2020). Numerical simulations allow to
set up of a virtual wind tunnel, where the instantaneous
solution of the entire ﬂow ﬁeld is available, unlike wind
tunnel experiments where only some local probes or in-
plane particle images velocimetry (PIV) measurements are
available. Despite the growing interest in this rotor conﬁg-
uration, the literature is limited to RANS simulations see
e.g. De Marco et al. (2016). In these numerical studies, only
the mean ﬂow dynamics is solved and a turbulence model
needs to be introduced to close the problem. The presence
of a model itself compromises the level of ﬁdelity and
accuracy. We present here the ﬁrst DNS for the ﬂow around
a spinning cylinder immersed in a turbulent boundary layer,
no periodic boundary conditions are imposed. We do not rely
on any model and all the scales are spatially and temporally
resolved. While the lift generation mechanism has been well
known since the last century (Magnus 1853), several physical
aspects remain unclear, e.g. the inﬂuence of the Reynolds
number and wind orientation on the aerodynamic force of
the rotating cylinder, and are only possible to study through
high-ﬁdelity simulations.

Conclusion

We have extended and optimized Neko, a Fortran-based
solver with support for modern accelerators, ready for AMD
and Nvidia GPUs. We leveraged modern Fortran to use
previous tools developed for Fortran 77 and performed the
world’s ﬁrst direct numerical simulation of a Flettner rotor
in a turbulent boundary layer. We have presented some
initial integral quantities and shown that DNS can further
our understanding of the turbulent coherent structures and
how Flettner rotors can be used to increase sustainability in
shipping. We have also presented one of the ﬁrst performance
and energy comparisons between the new AMD Instinct
MI250X and Nvidia A100. Overall, we see that modern
GPUs enable us to perform relevant, large-scale DNS on
relatively few nodes within a time frame and energy budget
that was previously not possible.

Acknowledgements

We are grateful to HPE for providing access to their system for this
research. We thank C3SE for the opportunity to scale and perform
the majority of our simulation using their Alvis system at Chalmers
University of Technology in Gothenburg.

Declaration of conﬂicting interests

The Author(s) declare(s) that there is no conﬂict of interest.

Funding

The author(s) disclosed receipt of the following ﬁnancial support
for the research, authorship, and/or publication of this article:

12

Journal Title XX(X)

Financial support was provided by the Swedish e-Science Research
Centre Exascale Simulation Software Initiative (SESSI);
the
Swedish Research Council project grant “Efﬁcient Algorithms for
Exascale Computational Fluid Dynamics” [grant reference 2019-
04723]. Parts of the computations were enabled by resources
provided by the Swedish National Infrastructure for Computing
(SNIC), partially funded by the Swedish Research Council through
grant agreement no. 2018-05973, at C3SE and PDC Centre for High
Performance Computing.

References

Atzori M, Vinuesa R, Stroh A, Gatti D, Frohnapfel B and Schlatter
P (2021) Uniform blowing and suction applied to nonuniform
adverse-pressure-gradient wing boundary layers. Phys. Rev.
Fluids .

Bae HJ and Koumoutsakos P (2022) Scientiﬁc multi-agent
reinforcement learning for wall-models of turbulent ﬂows.
Nature Communications 13(1): 1–9.

Bordogna G, Muggiasca S, Giappino S, Belloli M, Keuning J
and Huijsmans R (2019) Experiments on a Flettner rotor at
critical and supercritical Reynolds numbers. Journal of Wind
Engineering & Industrial Aerodynamics 188: 193–204.

Bordogna G, Muggiasca S, Giappino S, Belloli M, Keuning J and
Huijsmans R (2020) The effects of the aerodynamic interaction
on the performance of two ﬂettner rotors. Journal of Wind
Engineering & Industrial Aerodynamics 196.

Cantwell C, Moxey D, Comerford A, Bolis A, Rocco G, Mengaldo
G, De Grazia D, Yakovlev S, Lombard JE, Ekelschot D,
Jordi B, Xu H, Mohamied Y, Eskilsson C, Nelson B, Vos P,
Biotto C, Kirby R and Sherwin S (2015) Nektar++: An open-
source spectral/hp element framework. Computer Physics
Communications 192: 205–219.

Chevalier M, Lundbladh A and Henningson D (2007) SIMSON–a
pseudo-spectral solver for incompressible boundary layer ﬂow.
Technical report.

De Marco A, Mancini S, Pensa C, Calise G and De Luca F (2016)
Flettner rotor concept for marine applications: A systematic
study. International Journal of Rotating Machinery .

Deville MO, Fischer PF and Mund E (2002) High-order methods
for incompressible ﬂuid ﬂow, volume 9. Cambridge university
press.

Fischer P, Kerkemeier S, Min M, Lan YH, Phillips M, Rathnayake
T, Merzari E, Tomboulides A, Karakus A, Chalmers N et al.
(2021) NekRS, a GPU-accelerated spectral element Navier-
Stokes solver. arXiv preprint arXiv:2104.05829 .

Fischer PF (1998) Projection techniques for iterative solution of
Ax= b with successive right-hand sides. Computer methods
in applied mechanics and engineering 163(1-4): 193–204.
Fischer PF, Lottes JW and Kerkemeier SG (2008) nek5000 Web

page. http://nek5000.mcs.anl.gov.

Green500 (2021) Green500 Web page.
top500.org/lists/green500/.

https://www.

Hart A, Richardson H, Doleschal J, Ilsche T, Bielert M and
Kappel M (2014) User-level power monitoring and application
performance on cray xc30 supercomputers. Proceedings of the
Cray User Group (CUG) 1.

Jansson N (2021) Spectral Element Simulations on the NEC SX-
Aurora TSUBASA. In: The International Conference on High
Performance Computing in Asia-Paciﬁc Region, HPC Asia

Prepared using sagej.cls

2021. New York, NY, USA: ACM, pp. 32–39.

Jansson N, Karp M, Podobas A, Markidis S and Schlatter P
(2021) Neko: A modern, portable, and scalable framework
for high-ﬁdelity computational ﬂuid dynamics. arXiv preprint
arXiv:2107.01243 .

Karniadakis GE,

Israeli M and Orszag SA (1991) High-
order splitting methods for the incompressible Navier-Stokes
equations. Journal of computational physics 97(2): 414–443.

Karp M, Podobas A, Jansson N, Kenter T, Plessl C, Schlatter
P and Markidis S (2021) High-performance spectral element
methods on ﬁeld-programmable gate arrays : Implementation,
evaluation, and future projection. In: 2021 IEEE International
Parallel and Distributed Processing Symposium (IPDPS). pp.
1077–1086. DOI:10.1109/IPDPS49936.2021.00116.

Karp M, Podobas A, Jansson N, Schlatter P and Markidis S (2022a)
Reducing communication in the conjugate gradient method:
In: Platform for
A case study on high-order ﬁnite elements.
Advanced Scientiﬁc Computing Conference (PASC ’22). DOI:
10.1145/3539781.3539785.

Karp M, Podobas A, Kenter T, Jansson N, Plessl C, Schlatter
P and Markidis S (2022b) A high-ﬁdelity ﬂow solver
for unstructured meshes on ﬁeld-programmable gate arrays:
In: International
Design, evaluation, and future challenges.
Conference on High Performance Computing in Asia-Paciﬁc
Region, HPCAsia2022. New York, NY, USA: Association for
Computing Machinery.
ISBN 9781450384988, p. 125–136.
DOI:10.1145/3492805.3492808.

Keryell R, Reyes R and Howes L (2015) Khronos sycl for opencl: a
tutorial. In: Proceedings of the 3rd International Workshop on
OpenCL. pp. 1–1.

Kolev T, Fischer P, Abdelfattah A, Beams N, Brown J, Camier JS,
Carson R, Chalmers N, Dobrev V, Dudouit Y et al. (2022)
Ecp milestone report high-order algorithmic developments and
optimizations for more robust exascale applications wbs 2.2.
6.06, milestone ceed-ms38 .

Kolev T, Fischer P, Min M, Dongarra J, Brown J, Dobrev
V, Warburton T, Tomov S, Shephard MS, Abdelfattah A
et al. (2021) Efﬁcient exascale discretizations: High-order
ﬁnite element methods. The International Journal of High
Performance Computing Applications 35(6): 527–552.

Lottes JW and Fischer PF (2005) Hybrid multigrid/schwarz
Journal of

algorithms for the spectral element method.
Scientiﬁc Computing 24(1): 45–78.

Maday Y and Patera AT (1989) Spectral element methods for
the incompressible Navier-Stokes equations. State-of-the-art
surveys on computational mechanics (A90-47176 21-64). New
York : 71–143.

Magnus G (1853) ¨Uber die Abweichung der Geschosse, und: ¨Uber
eine abfallende Erscheinung bei rotierenden K¨orpern. Annalen
der Physik 1: 1–29.

Medina DS, St-Cyr A and Warburton T (2014) Occa: A uniﬁed
arXiv preprint

approach to multi-threading languages.
arXiv:1403.0968 .

Offermans N, Marin O, Schanen M, Gong J, Fischer P, Schlatter
P, Obabko A, Peplinski A, Hutchinson M and Merzari E
(2016) On the strong scaling of the spectral element solver
nek5000 on petascale systems. In: Proceedings of the Exascale
Applications and Software Conference 2016. pp. 1–10.

Orszag SA (1979) Spectral methods for problems in complex
In: Numerical methods for partial differential

geometrics.

Karp et al.

13

equations. Elsevier, pp. 273–305.

Orszag SA, Israeli M and Deville MO (1986) Boundary conditions
for incompressible ﬂows. Journal of Scientiﬁc Computing 1(1):
75–111.

Otero E, Gong J, Min M, Fischer P, Schlatter P and Laure E (2019)
Openacc acceleration for the PN–PN-2 algorithm in nek5000.
Journal of Parallel and Distributed Computing 132: 69–78.
Pachauri RK, Reisinger A et al. (2007) IPCC fourth assessment

report. IPCC, Geneva 2007.

Patera AT (1984) A spectral element method for ﬂuid dynamics:
laminar ﬂow in a channel expansion. Journal of Computational
Physics 54(3): 468–488.

Peplinski A (2022) KTH Framework for Nek5000. https://

github.com/KTH-Nek5000/KTH_Framework.

Reid J (2008) The new features of Fortran 2008. In: ACM SIGPLAN
Fortran Forum, volume 27. ACM New York, NY, USA, pp. 8–
21.

Saad Y and Schultz MH (1986) GMRES: A generalized minimal
residual algorithm for solving nonsymmetric linear systems.
SIAM Journal on scientiﬁc and statistical computing 7(3): 856–
869.

Schlatter P and ¨Orl¨u R (2012) Turbulent boundary layers at
moderate reynolds numbers: Inﬂow length and tripping effects.
J. of Fluid Mech. 710: 5–34.

Seddiek IS and Ammar NR (2021) Harnessing wind energy
on merchant ships: case study Flettner rotors onboard bulk
Environmental Science and Pollution Research
carriers.
28(25): 32695–32707.

Seifert J (2012) A review of the Magnus effect in aeronautics.

Progress in Aerospace Sciences 55: 17–45.

Smith T, Traut M, Bows-Larkin A, Anderson K, McGlade C
and Wrobel P (2015) CO2 targets, trajectories and trends for
international shipping .

´Swirydowicz K, Chalmers N, Karakus A and Warburton T
(2019) Acceleration of tensor-product operations for high-
order ﬁnite element methods. The International Journal of
High Performance Computing Applications 33(4): 735–757.
Tanarro A, Vinuesa R and Schlatter P (2019) Effect of adverse
pressure gradients on turbulent wing boundary layers. J. of
Fluid Mech. 883.

Tufo HM and Fischer PF (1999) Terascale spectral element
algorithms and implementations. In: Proceedings of the 1999
ACM/IEEE Conference on Supercomputing. pp. 68–es.

Wienke S, Springer P, Terboven C et al. (2012) Openacc—ﬁrst
In: European

experiences with real-world applications.
Conference on Parallel Processing. Springer, pp. 859–870.

Prepared using sagej.cls

