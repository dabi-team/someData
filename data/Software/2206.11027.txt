Can you meaningfully consent in eight seconds? Identifying
Ethical Issues with Verbal Consent for Voice Assistants

William Seymour
william.1.seymour@kcl.ac.uk
King’s College London
London, UK

Mark Coté
mark.cote@kcl.ac.uk
King’s College London
London, UK

Jose Such
jose.such@kcl.ac.uk
King’s College London
London, UK

2
2
0
2

n
u
J

2
2

]

C
H
.
s
c
[

1
v
7
2
0
1
1
.
6
0
2
2
:
v
i
X
r
a

ABSTRACT
Determining how voice assistants should broker consent to share
data with third party software has proven to be a complex prob-
lem. Devices often require users to switch to companion smart-
phone apps in order to navigate permissions menus for their other-
wise hands-free voice assistant. More in line with smartphone app
stores, Alexa now oﬀers “voice-forward consent”, allowing users to
grant skills access to personal data mid-conversation using speech.
While more usable and convenient than opening a companion app,
asking for consent ‘on the ﬂy’ can undermine several concepts core
to the informed consent process. The intangible nature of voice
interfaces further blurs the boundary between parts of an interac-
tion controlled by third-party developers from the underlying plat-
forms. This provocation paper highlights key issues with current
verbal consent implementations, outlines directions for potential
solutions, and presents ﬁve open questions to the research com-
munity. In so doing, we hope to help shape the development of
usable and eﬀective verbal consent for voice assistants and similar
conversational user interfaces.

CCS CONCEPTS
• Human-centered computing → Ubiquitous and mobile com-
puting design and evaluation methods; Natural language in-
terfaces; • Security and privacy → Usability in security and
privacy.

KEYWORDS
Voice assistants; consent; data protection; data sharing.

ACM Reference Format:
William Seymour, Mark Coté, and Jose Such. 2022. Can you meaningfully
consent in eight seconds? Identifying Ethical Issues with Verbal Consent
for Voice Assistants. In 4th Conference on Conversational User Interfaces
(CUI 2022), July 26–28, 2022, Glasgow, United Kingdom. ACM, New York,
NY, USA, 4 pages. https://doi.org/10.1145/3543829.3544521

1 INTRODUCTION
A vital aspect of the software marketplaces available on today’s
smart devices, including voice assistants (VAs), is the way that shar-
ing of personal data with third parties is managed. Consent has
emerged as the primary means of managing this relationship with
skill (also called voice applications) developers and is intended to

CUI 2022, July 26–28, 2022, Glasgow, United Kingdom
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the author’s version of the work. It is posted here for your personal use. Not
for redistribution. The deﬁnitive Version of Record was published in 4th Conference on
Conversational User Interfaces (CUI 2022), July 26–28, 2022, Glasgow, United Kingdom,
https://doi.org/10.1145/3543829.3544521.

allow users to decide for themselves what they are willing to share
when using third party skills. In fact, users are known to have
rather diﬀerent data sharing preferences depending on the third
party skill [3]. The consent process is further enshrined in the
GDPR, and even when skills use other legal bases for data collec-
tion, consent remains a vital ethical component of the design and
operation of these platforms. The growing presence of VAs that are
able to listen to everything said around them in phones, speakers,
TVs, headphones, and other devices brings into scope wider con-
cerns from the Ubicomp community around informed consent in
the context of ubiquitous computing [10].

Unsurprisingly, designing mechanisms that broker consent for
data sharing between users of voice assistants and third-party skills
has proven to be a challenging problem from an HCI and UX per-
spective. Unlike apps which request access to device functionality
(e.g. location determined by GPS), skills generally request access
to personal data directly (e.g. the address where the device is lo-
cated).1 Currently, Alexa and Google Assistant direct users to com-
panion smartphone apps to give consent for data sharing. This mir-
rors the ﬂow of managing permissions on smartphone apps that
was once commonplace, where consent would be granted in the
app store at the threshold of installation. But directing users of a
supposedly hands-free device to manually interact with a smart-
phone leads to a poor user experience and negates one of the main
selling points of VAs. Last year, Amazon introduced “voice-forward
consent” (VFC) for Alexa, allowing developers to request consent
for data sharing mid-conversation using speech. On the face of
it this not only reduces friction in the user experience, but also
mirrors the shift seen in smartphone apps towards asking for per-
missions ‘just in time’ (a move recommended by some privacy
scholars [13] but whose implementation has been criticised by oth-
ers [14]).

But this current implementation of voice-based consent for VAs
has several key problems and neglects core principles of informed
consent. This provocation paper draws on literature from HCI and
Ubicomp, as well as key data protection regulations such as the Eu-
ropean General Data Protection Regulation (GDPR) [1] to map out
concerns with current verbal consent in voice assistants, explore
potential solutions, and lay out the open questions in this space for
future discussion. In so doing, we hope to foster discussion within
the CUI community about how VAs and similar systems might bet-
ter be able to navigate consent in the future.

1The current set of Alexa permissions are: Device Address, Customer Name, Customer
Email Address, Customer Phone Number, Lists Read/Write, Amazon Pay, Reminders,
Location Services, and Skill Personalisation.

 
 
 
 
 
 
CUI 2022, July 26–28, 2022, Glasgow, United Kingdom

William Seymour, Mark Coté, and Jose Such

User
Alexa

User
Alexa

Alexa, open Ride Hailer.
Welcome to Ride Hailer. Where would you like to
go?
The Space Needle.
Sure. I need access to your name, current location,
and mobile number so that I can ﬁnd a ride for
you.

User
Alexa

Alexa (OS) Do you give Ride Hailer permission to access your
name, current location, and mobile number? You
can say ‘I approve’ or ‘no’.
I approve.
Thank you. A ride to the Space Needle from your
current location will cost ﬁfteen dollars, and the
driver can pick you up in ten minutes. Do you
want me to book it?
Yes.
Great. Your driver will arrive in ten minutes.

User
Alexa

Table 1: Sample VFC Flow from
https://developer.amazon.com/en-US/docs/alexa/
custom-skills/use-voice-forward-consent.html

2 CAN YOU MEANINGFULLY CONSENT IN 8

SECONDS?

Analysis of the relevant literature and regulation suggests several
key issues with the voice forward consent process which we sum-
marise here. These are: (1) the introduction of time pressure to the
permissions process; (2) diﬃculty conveying the required amount
of information via speech; (3) separating the process of granting
consent from its revocation; and (4) the lack of distinction between
speech from third party skills and the Alexa Operating System
(OS).2 Each violates established principles of informed consent, al-
beit in diﬀerent ways.

2.1 Time Pressure
VFC’s placement within both the Alexa turn-taking model and the
interaction itself adds a previously unseen sense of time pressure
to the consent process. By default, Alexa times out and re-prompts
the user after eight seconds, conditioning users to provide a re-
sponse within this window to prevent follow-up by the device. This
limits the time for decision-making, especially if the justiﬁcation
for data sharing by the skill developer is vague or non-existent. At
the same time, the position of VFC within an interaction (as op-
posed to at the threshold of use) means that skills can increase
the likelihood of users consenting through loss aversion [4], as
refusal to grant consent will most likely result in the end of the
interaction. Given this ‘take it or leave it’ Hobson’s choice after
the investment of time and eﬀort, the default response from many
users is to grant consent to prevent having to repeat the interac-
tion with a diﬀerent skill, threatening the freely given nature of
informed consent [15]. This is further complicated by the use of
specially crafted consent dialogues on the web that are designed

2The documentation refers to it as the Alexa Skill, but this terminology quickly be-
comes confusing.

to steer users towards granting consent [9], with Amazon and skill
designers facing a similar conﬂict of interest for VFC.

2.2 Limited Information
The need to deliver short, concise messages in conversations causes
further problems, as UX concerns preclude platforms providing
information about the scope of access and associated data rights;
the lower bandwidth of speech compared to graphical user inter-
faces means that VFC needs to accomplish in a few sentences what
would normally occupy paragraphs of text. The sample VFC ﬂow
shown on the previous page highlights the brevity employed, men-
tioning only the name of the skill and permissions requested de-
spite taking around twenty seconds to deliver. Information about
the skill developer is omitted and it is left to the skill to accurately
describe (or not) what the information will be used for, which is
problematic given the known lack of traceability in skill privacy
policies [6, 7]. As with the existing graphical VA consent dialogues,
it is also not clear whether the skill is requesting access to data at
the level of an individual person (via an associated voice proﬁle),
or the Amazon account.

2.3 Breaking Interface Symmetry
The main permissions ﬂow, where users are directed to grant per-
missions in the VA’s companion app, also shows a list of all permis-
sions that could be requested by the skill and their status (granted/
not granted). This signposts to users that they can revoke consent
in the future by returning to the same location in the interface. The
addition of VFC moves the granting of consent to the voice inter-
face, meaning that users then give consent for data sharing without
seeing or being informed about how they can withdraw consent (or
even that they can withdraw consent at all), as there is no mech-
anism for revoking consent using speech. This becomes increas-
ingly important with the introduction of proactive modes of oper-
ation to the platform (such as ‘reminders’ and ‘routines’ that can
be triggered at certain times of the day) and early work on proac-
tive assistants [5] suggests that VAs of the future may increasingly
act of their own volition. These features normalise skills running
without user intervention but with the same set of permissions and
ability to access personal data, further removing the experience of
using a skill from the consent process.

2.4 Lack of Distinction Between System and

Skills

As a ﬁnal example, though delivery of voice-forward consent is
handled by the Alexa OS (see Table 1) there is no audible diﬀer-
ence between speech delivered by the Alexa OS and speech deliv-
ered by a third party skill. Users are already known to struggle
when distinguishing between Alexa and third-party skills [2, 11],
making it diﬃcult for users to accurately interpret and place their
trust in VFC dialogues; many will assume that Alexa controls the
entire process and oversees the entire consent ﬂow including rea-
sons for processing data, whereas in reality skills are encouraged
to account for their own usage of personal data before initiating
VFC. In an extreme case, it is possible for a skill to perfectly imi-
tate the language and response choices used in VFC without ever
delegating to Alexa and with no audible diﬀerence to the user. This

Identifying Ethical Issues with Verbal Consent for Voice Assistants

CUI 2022, July 26–28, 2022, Glasgow, United Kingdom

potential to frame consent decisions with information incorrectly
attributed to the VA vendor also carries with it the risk of being ex-
ploited by skill developers to increase consent grant rates. While
using diﬀerent voices to communicate speech from ﬁrst and third
parties is an intuitive solution, work on the computers are social ac-
tors paradigm suggests that people may then interpret each voice
as an independent assistant [12].

3 POTENTIAL SOLUTIONS
Using the same resources used to highlight the problems with VFC
we are exploring a variety of measures and mitigations that can be
used by skill and platform developers in their products. At a ba-
sic level this involves strengthening policies and mechanisms that
are already present, such as incorporating developer justiﬁcation
for using data into the skill certiﬁcation process and making their
disclosure a mandatory part of VFC ﬂows.

For developers, the key ﬁrst step is to motivate the need for ac-
cess to data in the precious few words that are available when com-
municating permission requests to users. While not suggested in
the Amazon developer documentation, utilising other media such
as response cards and external communications channels can help
to overcome the bare-bones nature of VA conversational interac-
tions.

For VA platforms, allowing users to easily distinguish between
speech from the operating system and speech from third party
skills is a crucial ﬁrst step that facilitates the formation of more
accurate mental models by users. Modiﬁcation of the usual con-
versational turn-taking model to facilitate engagement with con-
sent and removal of the timeout window could help lessen feel-
ings of being rushed. However, research on smartphone permis-
sions suggests that in-context requests increase permission grant
rates (especially where no or poor justiﬁcation is given) [8], oﬀer-
ing justiﬁcation for returning to an up-front permissions decision.
Pointers to additional resources, perhaps via the companion app,
would give users material they can return to after the interaction
is complete to foster understanding and reﬂection. Allowing users
to revoke consent via speech, and describing how to do so when
it is granted, would remove barriers to engagement by restoring
the symmetry of consent ﬂows. Where users do revoke consent,
platforms could recommend alternative skills that do not require
as many permissions.

Finally, the situation also presents opportunities for VAs to em-
brace better consent practices that will, we believe, also help peo-
ple feel more conﬁdent about using devices that are often perceived
as inherently unsettling. Examples of this include the potential to
check-in with users in the weeks and months following an initial
granting of consent, utilising the lightweight nature of VFC as a
way of embracing consent as a living, ongoing practice [10]. The
necessary conciseness of speech, while appearing on the surface to
be a hindrance actually interacts positively with recommendations
to provide “short, speciﬁc privacy notices” [13].

4 OPEN QUESTIONS
Having outlined the major issues facing voice-based consent along-
side potential solutions, this section describes the open questions

to the CUI community that we hope and believe that future re-
search can address.

(1) How to develop the user experience of VFC?

Usability remains a key concern in the aftermath of several
well-intentioned but ultimately poorly implemented EU pri-
vacy regulations3 and it is vital that voice-based consent
does not become another burden to which people become
habituated into dismissing without a second thought. This
could involve user studies evaluating diﬀerent implementa-
tions of the guidelines, co-design opportunities with diﬀer-
ent user groups, and policy recommendations to regulators.
We envisage that a combination of these methods will ul-
timately be required to put voice-based consent on a ﬁrm
footing for the future as VA technology develops and en-
compasses new aspects of daily life.

(2) What aspects of VFC are the most important and/or relevant?

There are many diﬀerent components to VFC, including many
that are not currently implemented. The interactional and
UX constraints imposed by the fundamental design of VAs
means that a key step in the research agenda is determining
which of these are relevant to VFC and the context in which
it is used (e.g. not all aspects of informed consent, dynamic
consent, or consent as a legal ground for data collection are
required for VAs in the home). Prioritising how these ele-
ments are woven into the consent ﬂow is also important, as
certain aspects of VFC are limited resources (e.g. the amount
of text that is delivered before a user makes a consent deci-
sion). Similarly, decisions need to be taken around which
aspects are featured more prominently and which are rele-
gated to secondary devices such as smartphones.
(3) How could VFC best utilise other interaction modalities?

Related to the above, how might consent practices go be-
yond their current compulsory use of companion apps to
best make use of connected screens, devices, and web apps
to provide optional extensions to the experience? While it
is true that the conversational components of VFC must
be carefully balanced to remain eﬀective and usable, the
ability to back this up with text, images, augmented real-
ity, and other experiences with devices commonly found in
the home should be explored.

(4) How could better VFC practice be integrated into platform

APIs and ﬂows?
Within the frameworks and architectures that currently ex-
ist, how could voice-based consent be better integrated? This
could include developer justiﬁcation for data collection, al-
lowing multiple devices to be used, or more granular con-
sent options as seen in dynamic consent. This type of work
can be tricky, as the internal structures and policies of plat-
forms is often hidden, but recent work has shown how large
scale investigations into VA platform black boxes can be suc-
cessful [16].

(5) What could change in terms of platform architecture and reg-

ulation?
Given the continual evolution of voice assistant norms and
underlying technology, as well as the gradually changing

3Such as for cookies, and more recently around the GDPR.

CUI 2022, July 26–28, 2022, Glasgow, United Kingdom

William Seymour, Mark Coté, and Jose Such

stance towards tech regulation, the next 5–10 years will un-
doubtedly herald changes to the market that will require
re-thinking the trajectory of voice-based consent. Anticipat-
ing these changes and designing consent mechanisms and
agendas that can be ﬂexible in the face of uncertainty should
be a high priority. This will often involve thinking and de-
signing outside of current architectures; what communities,
tools, and practices around VFC might emerge, for example,
if developers had much more freedom to shape the consent
experience (as opposed to having mechanisms and guidance
thrust upon them) and/or the regulatory environment that
was much more proactive (as opposed to cases being infre-
quent and mainly high proﬁle).

5 CONCLUSION
The introduction of voice-forward consent for Alexa represents
a great improvement in user experience but one that potentially
undermines the informed consent process. We set out a research
agenda for developing usable, eﬀective voice-based consent mech-
anisms. Our own early-stage work involves using literature from
HCI and Ubicomp, alongside extracts from the GDPR to highlight
speciﬁc problems with the current VFC implementation and po-
tential solutions. As this work progresses we will draw on expert
opinion to further motivate and reﬁne the ideas presented in this
position paper, ultimately creating guidelines for developers and
platforms that promote healthy and dynamic verbal consent prac-
tices in voice assistants and similar conversational interfaces.

ACKNOWLEDGMENTS
This work is funded by the Secure AI Assistants project via Grant
EP/T026723/1 from the UK Engineering and Physical Sciences Re-
search Council.

REFERENCES
[1] 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council
of 27 April 2016 on the protection of natural persons with regard to the pro-
cessing of personal data and on the free movement of such data, and repealing
Directive 95/46/EC (General Data Protection Regulation). OJ L 119 (2016), 1–8.
http://data.europa.eu/eli/reg/2016/679/oj

[2] Noura Abdi, Kopo M Ramokapane, and Jose Such. 2019. More than smart speak-
ers: security and privacy perceptions of smart home personal assistants. In Fif-
teenth Symposium on Usable Privacy and Security (SOUPS 2019). 451–466.

[3] Noura Abdi, Xiao Zhan, Kopo M Ramokapane, and Jose Such. 2021. Privacy
Norms for Smart Home Personal Assistants. In Proceedings of the 2021 CHI Con-
ference on Human Factors in Computing Systems. 1–14.

[4] Alessandro Acquisti, Idris Adjerid, Rebecca Balebako, Laura Brandimarte,
Lorrie Faith Cranor, Saranga Komanduri, Pedro Giovanni Leon, Norman
Sadeh, Florian Schaub, Manya Sleeper, Yang Wang, and Shomir Wilson.
2017. Nudges for Privacy and Security: Understanding and Assisting Users’
Choices Online. ACM Comput. Surv. 50, 3, Article 44 (aug 2017), 41 pages.
https://doi.org/10.1145/3054926

[5] Narae Cha, Auk Kim, Cheul Young Park, Soowon Kang, Mingyu Park, Jae-Gil
Lee, Sangsu Lee, and Uichin Lee. 2020. Hello There! Is Now a Good Time to Talk?
Opportune Moments for Proactive Interactions with Smart Speakers. Proc. ACM
Interact. Mob. Wearable Ubiquitous Technol. 4, 3, Article 74 (sep 2020), 28 pages.
https://doi.org/10.1145/3411810

[6] Jide Edu, Xavier Ferrer Aran, Jose Such, and Guillermo Suarez-Tangil. 2022. Mea-
suring Alexa Skill Privacy Practices across Three Years. In The Web Conference
(WWW). ACM.

[7] Jide Edu, Xavier Ferrer-Aran, Jose Such, and Guillermo Suarez-Tangi. 2021. Skill-
Vet: Automated Traceability Analysis of Amazon Alexa Skills. IEEE Transactions
on Dependable and Secure Computing (TDSC) (2021).

[8] Yusra Elbitar, Michael Schilling, Trung Tin Nguyen, Michael Backes, and Sven
Bugiel. 2021. Explanation Beats Context: The Eﬀect of Timing & Rationales
on Users’ Runtime Permission Decisions. In 30th USENIX Security Symposium
(USENIX Security 21). 785–802.

[9] Matthias Fassl, Lea Theresa Gröber, and Katharina Krombholz. 2021. Stop the
Consent Theater. Association for Computing Machinery, New York, NY, USA.
https://doi.org/10.1145/3411763.3451230
[10] Ewa Luger and Tom Rodden. 2013.

An Informed View on Consent
for UbiComp. In Proceedings of the 2013 ACM International Joint Confer-
ence on Pervasive and Ubiquitous Computing (Zurich, Switzerland) (UbiComp
’13). Association for Computing Machinery, New York, NY, USA, 529–538.
https://doi.org/10.1145/2493432.2493446

[11] David Major, Danny Yuxing Huang, Marshini Chetty, and Nick Feamster. 2021.
Alexa, Who Am I Speaking To?: Understanding Users’ Ability to Identify Third-
Party Apps on Amazon Alexa. ACM Transactions on Internet Technology (TOIT)
22, 1 (2021), 1–22.

[12] Cliﬀord Nass, Jonathan Steuer, and Ellen R. Tauber. 1994. Computers Are Social
Actors. In Proceedings of the SIGCHI Conference on Human Factors in Comput-
ing Systems (Boston, Massachusetts, USA) (CHI ’94). Association for Computing
Machinery, New York, NY, USA, 72–78. https://doi.org/10.1145/191666.191703
[13] Florian Schaub, Rebecca Balebako, and Lorrie Faith Cranor. 2017. Designing
eﬀective privacy notices and controls. IEEE Internet Computing 21, 3 (2017), 70–
77.

[14] Bingyu Shen, Lili Wei, Chengcheng Xiang, Yudong Wu, Mingyao Shen,
Yuanyuan Zhou, and Xinxin Jin. 2021. Can Systems Explain Permissions Better?
Understanding Users’ Misperceptions under Smartphone Runtime Permission
Model. In 30th USENIX Security Symposium (USENIX Security 21). 751–768.
[15] Yolande Strengers, Jathan Sadowski, Zhuying Li, Anna Shimshak, and Florian
’Floyd’ Mueller. 2021. What Can HCI Learn from Sexual Consent? A Feminist
Process of Embodied Consent for Interactions with Emerging Technologies. In
Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
(Yokohama, Japan) (CHI ’21). Association for Computing Machinery, New York,
NY, USA, Article 405, 13 pages. https://doi.org/10.1145/3411764.3445107

[16] Dawei Wang, Kai Chen, and Wei Wang. 2021.

Demystifying the Vet-
ting Process of Voice-Controlled Skills on Markets.
Proc. ACM Interact.
Mob. Wearable Ubiquitous Technol. 5, 3, Article 130 (sep 2021), 28 pages.
https://doi.org/10.1145/3478101

