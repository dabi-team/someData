2
2
0
2

y
a
M
8
1

]
L
C
.
s
c
[

1
v
0
4
7
8
0
.
5
0
2
2
:
v
i
X
r
a

A reproducible experimental survey on biomedical sentence
similarity: a string-based method sets the state of the art

Alicia Lara-Clares1*, Juan J. Lastra-D´ıaz1, Ana Garcia-Serrano1

1 NLP & IR Research Group, E.T.S.I. Inform´atica, Universidad Nacional de
Educaci´on a Distancia (UNED), Madrid (Spain)

* alara@lsi.uned.es

Abstract

This registered report introduces the largest, and for the first time, reproducible
experimental survey on biomedical sentence similarity with the following aims: (1) to
elucidate the state of the art of the problem; (2) to solve some reproducibility
problems preventing the evaluation of most of current methods; (3) to evaluate several
unexplored sentence similarity methods; (4) to evaluate for the first time an
unexplored benchmark, called Corpus-Transcriptional-Regulation (CTR); (5) to carry
out a study on the impact of the pre-processing stages and Named Entity Recognition
(NER) tools on the performance of the sentence similarity methods; and finally, (6) to
bridge the lack of software and data reproducibility resources for methods and
experiments in this line of research. Our reproducible experimental survey is based on
a single software platform, which is provided with a detailed reproducibility protocol
and dataset as supplementary material to allow the exact replication of all our
experiments and results. In addition, we introduce a new aggregated string-based
sentence similarity method, called LiBlock, together with eight variants of current
ontology-based methods, and a new pre-trained word embedding model trained on the
full-text articles in the PMC-BioC corpus. Our experiments show that our novel
string-based measure sets the new state of the art on the sentence similarity task in
the biomedical domain and significantly outperforms all the methods evaluated herein,
with the only exception of one ontology-based method. Likewise, our experiments
confirm that the pre-processing stages, and the choice of the NER tool for
ontology-based methods, have a very significant impact on the performance of the
sentence similarity methods. We also detail some drawbacks and limitations of current
methods, and warn on the need of refining the current benchmarks. Finally, a
noticeable finding is that our new string-based method significantly outperforms all
state-of-the-art Machine Learning (ML) models evaluated herein.

Introduction

Measuring semantic similarity between sentences is an important task in the fields of
Natural Language Processing (NLP), Information Retrieval (IR), and biomedical text
mining, among others. For instance, the estimation of the degree of semantic
similarity between sentences is used in text classification [1–3], question
answering [4, 5], evidence sentence retrieval to extract biological expression language
statements [6, 7], biomedical document labeling [8], biomedical event extraction [9],
named entity recognition [10], evidence-based medicine [11, 12], biomedical document

May 19, 2022

1/48

 
 
 
 
 
 
clustering [13], prediction of adverse drug reactions [14], entity linking [15], document
summarization [16, 17] and sentence-driven search of biomedical literature [18], among
other applications. In the question answering task, Sarrouti and El Alaomi [4] build a
ranking of plausible answers by computing the similarity scores between each
biomedical question and the candidate sentences extracted from a knowledge corpus.
Allot et al. [18] introduce a system to retrieve the most similar sentences in the BioC
biomedical corpus [19] called Litsense [18], which is based on the comparison of the
user query with all sentences in the aforementioned corpus. Likewise, the relevance of
the research in this area is endorsed by the proposal of recent conference series, such
as SemEval [20–25] and BioCreative/OHNLP [26], and works based on sentence
similarity measures, such as the work of Aliguliyev [16] in automatic document
summarization, which shows that the performance of these applications depends
significantly on the sentence similarity measures used.

The aim of any semantic similarity method is to estimate the degree of similarity

between two textual semantic units as perceived by a human being, such as words,
phrases, sentences, short texts, or documents. Unlike sentences from the language in
general use whose vocabulary and syntax is limited both in extension and complexity,
most sentences in the biomedical domain are comprised of a huge specialized
vocabulary made up of all sort of biological and clinical terms, in addition to an
uncountable list of acronyms, which are combined in complex lexical and syntactic
forms.

Nowadays, there exist several works in the literature that experimentally evaluate

multiple methods on biomedical sentence similarity. However, they are either
theoretical or have a limited scope and cannot be reproduced. For instance, Kalyan et
al. [27], Khattak et al. [28], and Alsentzer et al. [29] introduce theoretical surveys on
biomedical embeddings with a limited scope. On the other hand, the experimental
surveys introduced by Sogancioglu et al. [30], Blagec et al. [31], Peng et al. [32], and
Chen et al. [33] among other authors, cannot be reproduced because of the lack of
source code and data to replicate both methods and experiments, or the lack of a
detailed definition of their experimental setups. Likewise, there are other recent works
whose results need to be confirmed. For instance, Tawfik and Spruit [34]
experimentally evaluate a set of pre-trained language models, whilst Chen et al. [35]
propose a system to study the impact of a set of similarity measures on a Deep
Learning ensembled model, which is based on a Random Forest model [36].

The main aim of this work is to introduce a comprehensive and very detailed
reproducible experimental survey of methods on biomedical sentence similarity to
elucidate the state of the problem by implementing our previous registered report
protocol [37]. Our experiments are based on our software implementation and
evaluation of all methods analyzed herein into a common and new software platform
based on an extension of the Half-Edge Semantic Measures Library (HESML) [38, 39],
called HESML1 for Semantic Textual Similarity (HESML-STS). All our experiments
have been recorded into a Docker virtualization image that is provided as
supplementary material together with our software [40] and a detailed reproducibility
protocol [41] and dataset [42] to allow the easy replication of all our methods,
experiments, and results. This work is based on our previous experience developing
reproducible research in a series of publications in the area, such as the experimental
surveys on word similarity introduced in [43–46], whose reproducibility protocols and
datasets [47, 48] are detailed and independently confirmed in two companion
reproducible papers [38, 49], and a reproducible benchmark on semantic measures
libraries for the biomedical domain [39]. Finally, we refer the reader to our previous
work [37] for a very detailed review of the literature on sentence similarity measures,

1http://hesml.lsi.uned.es

May 19, 2022

2/48

which is omitted herein because of the lack of room and to avoid being redundant.

Main motivations and research questions

Our main motivation is the lack of a comprehensive and reproducible experimental
survey on biomedical sentence similarity that allows setting the state of the problem in
a sound and reproducible way, as detailed in our previous registered report
protocol [37]. Our main research questions are as follows:

RQ1 Which methods get the best results on biomedical sentence similarity?

RQ2 Is there a statistically significant difference between the best-performing

methods and the remaining ones?

RQ3 What is the impact of the biomedical Named Entity Recognition (NER) tools

on the performance of the methods on biomedical sentence similarity?

RQ4 What is the impact of the pre-processing stage on the performance of the

methods on biomedical sentence similarity?

RQ5 What are the main drawbacks and limitations of current methods on biomedical

sentence similarity?

A second motivation is implementing a set of unexplored methods based on
adaptations from other methods proposed for the general language domain. A third
motivation is the evaluation in the same software platform of the three known
benchmarks on biomedical sentence similarity reported in the literature as follows: the
Biomedical Semantic Similarity Estimation System (BIOSSES) [30] and Medical
Semantic Textual Similarity (MedSTS) [50] datasets, as well as the evaluation for the
first time of the Microbial Transcriptional Regulation (CTR) [51] dataset in a sentence
similarity task, despite it having been previously evaluated in other related tasks, such
as the curation of gene expressions from scientific publications [52]. A fourth
motivation is a study on the impact of the pre-processing stage and NER tools on the
performance of the sentence similarity methods, such as that done by Gerlach et
al. [53] for stop-words in topic modeling task. And finally, our fifth motivation is the
lack of reproducibility software and data resources on this task, which allow an easy
replication and confirmation of previous methods, experiments, and results in this line
of research, as well as encouraging the development and evaluation of new sentence
similarity methods.

Definition of the problem and contributions

The two main research problems tackled in this work are the design and
implementation of a large and reproducible experimental survey on sentence similarity
measures for the biomedical domain, and the evaluation of a set of unexplored
methods based on adaptations from previous methods used in the general language
domain. Our main contributions are as follows: (1) the largest, and for the first time,
reproducible experimental survey on biomedical sentence similarity; (2) the first
collection of self-contained and reproducible benchmarks on biomedical sentence
similarity; (3) the evaluation of a set of previously unexplored methods, such as a new
string-based sentence similarity method, based on Li et al. [54] and Block distance [55],
eight variants of the current ontology-based methods from the literature based on the
work of Sogancioglu et al. [30], and a new pre-trained Word Embedding (WE) model
based on FastText [56] and trained on the full-text of articles in the PMC-BioC
corpus [19]; (4) the evaluation for the first time of an unexplored benchmark, called

May 19, 2022

3/48

CTR [51]; (5) the study on the impact of the pre-processing stage and Named Entity
Recognition (NER) tools on the performance of the sentence similarity methods; (6)
the integration for the first time of most sentence similarity methods for the
biomedical domain into the same software library, called HESML-STS, which is
available both in Github 2 and in a reproducible dataset [42]; (7) a detailed
reproducibility protocol together with a collection of software tools and datasets
provided as supplementary material to allow the exact replication of all our
experiments and results; and finally, (8) an analysis of the drawbacks and limitations
of the current state-of-the-art methods.

The rest of the paper is structured as follows. First, we introduce a collection of
new sentence similarity methods evaluated herein for the first time. Next, we describe
a detailed experimental setup for our experiments on biomedical sentence similarity
and introduce our experimental results. Then, we discuss our results and answer the
research questions detailed above. Subsequently, we introduce our conclusions and
future work. Finally, we introduce three appendices with supplementary material as
follows. Appendix A introduces all statistical significance results of our experiments,
whilst Appendix B introduces all data tables reporting the performance of all methods
with all pre-processing configurations evaluated herein, and the Appendix C introduces
a reproducibility protocol detailing a set of step-by-step instructions to allow the exact
replication of all our experiments, which is published at protocols.io [41].

The new sentence similarity methods

This section introduces a new string-based sentence similarity method based on the
aggregation of the Li et al. [54] similarity and Block distance [55] measures, called
LiBlock, as well as eight new variants of the ontology-based methods proposed by
Sogancioglu et al. [30], and a new pre-trained word embedding model based on
FastText [56] and trained on the full-text of the articles in the PMC-BioC corpus [19].

The new LiBlock string-based method

Two key advantages of the family of string-based methods are as follows. Firstly, they
can be very efficiently computed because they do not require the use of external
knowledge or pre-trained models, and secondly, they obtain competitive results as
shown in table 8. However, the string-based methods do not capture the semantics of
the words in the sentence, which prevent them from recognizing semantic relationships
between words, such as synonymy and meronymy among others. On the other hand,
the family of ontology-based methods capture the semantic relationships between
words in a sentence pair and obtain state-of-the-art results in the sentence similarity
task for the biomedical domain, as shown in table 8. However, the effectiveness of
ontology-based methods depends on the lexical coverage of the ontologies and the
ability to recognize automatically the underlying concepts in sentences by using
Named Entity Recognition (NER) and Word Sense Desambiguation (WSD) tools,
whose coverage and performance could be limited in several application domains.
Precisely, the NER task is still an open problem [57] in the biomedical domain because
of the vast biomedical vocabulary and the complex lexical and syntactic forms found
in the biomedical literature. Otherwise, the methods based on pre-trained word
embedding models provide a broader lexical coverage than the ontology-based ones
and obtain better results. However, the methods based on word embeddings do not
significantly outperform all ontology-based measures in a word similarity task [46] in

2https://github.com/jjlastra/HESML/tree/HESML-STS_master_dev

May 19, 2022

4/48

addition to requiring large corpus for training, a complex training phase, and more
computational resources than the families of string-based and ontology-based methods.
To overcome the drawbacks and limitations of the string-based and ontology-based

methods detailed above, we propose here a new aggregated string-based measure
called LiBlock and denoted by simLiBk henceforth, which is based on the combination
of a similarity measure derived from the Block Distance [55] and an adaptation from
the ontology-based similarity measure introduced by Li et al. [54] that removes the use
of ontologies, such as WordNet [58] or Systematized Nomenclature of Medicine
Clinical Terms (SNOMED-CT) [59]. The LiBlock similarity measure obtains the best
results in combination with the cTAKES NER tool [60], which allows the detection of
synonyms of CUI concepts. Nevertheless, the LiBlock method obtains competitive
results regarding the state-of-the-art methods with no use, either implicitly or
explicitly, of an ontology, as detailed in table 12.

The simLiBk method detailed in equation (1) is defined by the linear aggregation

of an adaptation of the Li et al. [54] measure, called simLiAd (3), and a similarity
measure derived from the Block Distance measure [55], called simBk (2). Let be LΣ
the set of word sequences in a universal unseen alphabet Σ, the simLiBk function
returns a value between 0 and 1 which indicates the similarity score between two input
sentences, as defined in equation 1. The simBk function is based on the computation
of the word frequencies f r(wi, sj) for each input sentence s1 and s2 and their
concatenation s1 + s2, as detailed in equation (2). The auxiliary function f r(wi, sj)
returns the frequency of a word wi in the word sequence sj, whilst the function
f r(wi, s1 + s2) returns the number of occurrences of the word wi in the concatenation
of the two word sequences, denoted by s1 + s2. On the other hand, the simLiAd
function takes two word sets obtained by invoking the σ function (5) with the
sentences s1 and s2, and then it computes the cosine similarity of the two binary
semantic vectors corresponding to invoke the ϕ(S1) function (4) with the σ(s1) and
σ(s2) word sets. Finally, the simLiBk score is defined by either the linear combination
of simBk and simLiAd, as detailed in equation (1), or simBk if simLiAd is 0.

A walk-through example. Algorithm 1 details the step-by-step procedure to
compute the simLiBk function, whilst figure 1 shows the pipeline for calculating the
LiBlock similarity score defined in equation 1, as well as an example for illustrating an
end-to-end calculation of the simLiBk similarity score of two sentences.

Algorithm 1 LiBlock sentence similarity measure for two input pre-processed sen-
tences.
1: function simLiBlock(s1, s2)
2:

3:
4:

5:
6:

7:

8:
9:

S1 ← σ(s1)
S2 ← σ(s2)
D ← S1 ∪ S2
b1 ← ϕ(S1)
b2 ← ϕ(S2)
scoreLiAd ← simLiAd(b1, b2)
scoreBk ← simBk(s1, s2)
scoreLiBk ← simLiBk(scoreLiAd, scoreBk)
return scoreLiBk

10:
11: end function

⊲ being s1, s2 word sequences ∈ LΣ
⊲ word set sentence 1
⊲ word set sentence 2
⊲ construct the dictionary D
⊲ construct the semantic binary vector b1
⊲ construct the semantic binary vector b2
⊲ compute LiAdapted similarity
⊲ compute Block Distance similarity
⊲ compute LiBlock similarity

May 19, 2022

5/48

simLiBk : LΣ × LΣ → [0, 1] ⊂ R, LΣ = {word sequences in alphabet Σ}

(LiBlock similarity)

(1)

simBk(s1, s2),

if simLiAd(σ(s1), σ(s2)) = 0

simLiBk(s1, s2) =

(

1

2 simBk(s1, s2) + 1

2 simLiAd(σ(s1), σ(s2)),

otherwise

simBk : LΣ × LΣ → [0, 1] ⊂ R,
|D|

(Block distance)

(2)

simBk(s1, s2) = 1 −

i=1
P

|f r(wi, s1) − f r(wi, s2)|

f r(wi, s1 + s2)

|D|

i=1
P

, D = σ(s1) ∪ σ(s2) ∈ P(Σ)

simLiAd : P(D) × P(D) → [0, 1] ⊂ R,

(Li’s score adaptation)

(3)

simLiAd(S1, S2) =

ϕ(S1) · ϕ(S2)
||ϕ(S1)|| ∗ ||ϕ(S2)||

ϕ : P(D) → {0, 1}|D|,

(binary vector constructor)

(4)

ϕ(S) = (b1, b2, . . . , b|D|),

bi =

1, wi ∈ D
0, wi 6∈ D

(

σ : LΣ → P(Σ),

(word set generator)

(5)

σ(s) = {w ∈ Σ : ∃k ∈ [1, length(s)] such that sk = w}

The eight new variants of current ontology-based methods

The current family of ontology-based methods for biomedical sentence similarity
proposed by Sogancioglu et al. [30] is based on the ontology-based semantic similarity
between word and concepts within the sentences to be compared. Thus, this later
family of methods defines a framework in which we can design new variants by
exploring other word similarity measures. For this reason, we propose herein the
evaluation of a set of new ontology-based sentence similarity measures based on two
different unexplored notions as follows: (1) the evaluation of state-of-the-art word
similarity measures from the general domain [46] not evaluated in the biomedical
domain yet; and (2) the evaluation of several ontology-based word similarity measures
based on a recent and very efficient shortest-path algorithm, called Ancestors-based
Shortest-Path Length (AncSPL) [39], which is a fast approximation of the Dijkstra’s
algorithm [61] for taxonomies that is introduced with the first HESML version for the
biomedical domain [39].

Thus, we propose here the evaluation based on the combination of WBSM and
UBSM methods with the path-based word similarity methods as follows: WBSM-Rada
(M7); WBSM-cosJ&C (M9); WBSM-coswJ&C (M10); WBSM-Cai (M11);
UBSM-Rada (M12); UBSM-cosJ&C (M14); UBSM-coswJ&C (M15); and UBSM-Cai
(M16). The detailed information about this later method is shown in table 3.

May 19, 2022

6/48

Fig 1. This figure details the workflow for computing the new LiBlock measure and
an example illustrating a use case of the workflow following the steps defined in
algorithm 1.

s2

s1

σ

σ

S1

D = S1 ∪ S2

S2

ϕ

ϕ

simBk

b1

simLiAd

simLiBk

b2

simLiBkscore

Input : Raw s1 ← “Lung tumour formation in mice by oncogenic KRAS requires

formation Craf, but not Braf.”

Raw s2 ← “The oncogenic activity of mutant Kras appears dependent”

on functional Craf but not on Braf.”

step 1: s1 ← {c0280089, formation, mice, oncogenic, c1537502,

requires, formation, craf, c0812241}

s2 ← {oncogenic, activity, mutant, c1537502, appears,
dependent, functional, craf, c0812241}

step 2: S1 ← {c0280089, formation, mice, oncogenic, c1537502,

requires, craf, c0812241}

step 3: S2 ← {oncogenic, activity, mutant, c1537502, appears,

dependent, functional, craf, c0812241}

step 4: D ← {c0280089, formation, mice, oncogenic, c1537502, requires,

craf, c0812241, activity, mutant, appears, dependent, functional}

step 5: b1 ← {1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0}
step 6: b2 ← {0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1}
step 7: simLiAd ← 0.471
step 8: simBk ← 0.444
step 9: simLiBk ← 0.458

The new pre-trained word embedding model

Current sentence similarity methods based on the evaluation of pre-trained embedding
models are mostly trained using PubMed Central (PMC) Open Access dataset 3, or
Medical Information Mart for Intensive Care (MIMIC-III) clinical notes [62]. However,
as far as we know, there are no models in the literature trained on the full text of the

3https://www.ncbi.nlm.nih.gov/labs/pmc/

May 19, 2022

7/48

articles in the PMC-BioC corpus [19]. Therefore, we propose evaluating a new
FastText [56] word embedding model trained on the aforementioned BioC corpus.
FastText overcomes one significant limitation of other methods, such as word2vec [63]
and GloVe [64], which ignore the morphology of words by assigning a vector to each
word in the vocabulary. For a more detailed review of the family of word embedding
methods, we refer the authors to the recent reproducible survey by Lastra-D´ıaz et
al. [46]. The configuration parameters for training this model are detailed in table 4,
and all the necessary information and resources for evaluating it are available in our
reproducibility dataset [42], as detailed in table 6.

The reproducible experimental survey

This section introduces a detailed experimental setup to evaluate and compare all the
sentence similarity methods for the biomedical domain proposed in our primary
work [37], together with the new methods introduced herein. The main aims of our
experiments are as follows: (1) the evaluation of most of known methods for
biomedical sentence similarity onto the three biomedical datasets shown in table 1,
and implemented in the same software platform; (2) the evaluation of a set of new
sentence similarity methods adapted from their definitions for the general-language
domain; (3) the evaluation of a new sentence method called LiBlock introduced in this
work, eight variants of the current ontology-based methods from the literature based
on the work of Sogancioglu et al. [30], and a new word embedding model based on
FastText and trained on the full-text of articles in the PMC-BioC corpus [19]; (4) the
setting of the state of the art of the problem in a sound and reproducible way; (5) the
replication and independent confirmation of previously reported methods and results;
(6) a study on the impact of different pre-processing configurations on the
performance of the sentence similarity methods; (7) a study on the impact of different
Name Entity Recognition (NER) tools, such as MetaMap [65] and clinic Text Analysis
and Knowledge Extraction System (cTAKES) [60], onto the performance of the
sentence similarity methods; (8) the evaluation for the first time of the CTR [51]
dataset; (9) the identification of the main drawbacks and limitations of current
methods; and finally, (10) a detailed statistical significance analysis of the results.

Table 1. Benchmarks on biomedical sentence similarity evaluated in this work.

Dataset
BIOSSES [30]
MedSTS [50]
CTR [51]

#pairs Corresponding file (*.tsv) in HESML-STS distribution
100
1,068
170

BIOSSESNormalized.tsv
CTRNormalized averagedScore.tsv
MedStsFullNormalized.tsv

Selection of methods

The criteria for the selection of the sentence similarity methods evaluated herein is as
follows: (a) all the methods that have been evaluated in BIOSSES and MedSTS
datasets; (b) a selection of methods that have not been evaluated in the biomedical
domain yet; (c) a collection of new variants or adaptations of methods previously
proposed for the general or biomedical domain, which are evaluated for the first time
in this work, such as the WBSM-cosJ&C [30, 39, 44, 66],
WBSM-coswJ&C [30, 39, 44, 66], WBSM-Cai [30, 39, 67], UBSM-cosJ&C [30, 39, 44, 66],
UBSM-coswJ&C [30, 39, 44, 66], and UBSM-Cai [30, 39, 67] methods detailed in tables 3
and 4; and (d) a new string-based method based on Li et al. [54] introduced in this

May 19, 2022

8/48

work. For a more detailed description of the selection criteria of the methods, we refer
the reader to our registered report protocol [37].

Tables 2 and 3 detail the configuration of the string-based measures and

ontology-based measures that are evaluated herein, respectively. Both WBSM and
UBSM methods are evaluated in combination with the following word and concept
similarity measures: Rada et al. [68], Jiang&Conrath [69], and three state-of-the-art
unexplored measures, called cosJ&C [39, 44], coswJ&C [39, 44], and Cai et al. [39, 67].
The word similarity measure which reports the best results is used to evaluate the
COM method [30, 68]. Table 4 details the sentence similarity methods based on the
evaluation of pre-trained character, word, and Sentence Embedding (SE) models that
are evaluated in this work. Finally, table 5 details the pre-trained language models
that are evaluated in our experiments.

Table 2. Detailed setup for the string-based sentence similarity measures which are
evaluated in this work. All the string-based measures follow the implementation of
Sogancioglu et al. [30], who use the Simmetrics library [70]. LiBlock method proposed
herein is an adaptation from Li et al. [54] combined with a string-based measure, as
detailed in the previous section.

ID Method

Detailed setup of each method

M1 Qgram [71]

sim(a, b) = 2×|q−grams(a)∪q−grams(b)|
and b sets of q words, and with q = 3.

|q−grams(a)|+|q−grams(b)| , being a

M2

Jaccard [72, 73]

sim(a, b) = |a∪b|
|a∩b| , being a and b sets of words
of the first and second sentence respectively.

M3

Block distance
[55]

|D|
P
i=1

|f r(wi,s1)−f r(wi,s2)|

,

f r(wi,s1+s2)

sim(s1, s2) = 1 −

|D|
P
i=1
as detailed in equation 2.

M4

LiBlock
(this work)

LiBlock method (see eq. 1) annotated with
CUI concepts and using cTAKES combined
with the Block Distance [55] method using
its best pre-processing configuration.

M5

Levenshtein
distance [74]

M6

Overlap
coefficient [75]

Measures the minimal cost number of insertions,
deletions and replacements needed for
transforming the first into the second sentence.
Insert, delete and substitution cost set to 1.

sim(a, b) =
words of the first and second sentence respectively.

|Min(|a|,|b|)| , being a and b sets of

|a∩b|

Pre-processing methods evaluated in this work

The pre-processing stage aims to ensure a fair comparison of the methods that are
evaluated in a single end-to-end pipeline. To achieve this later goal, the pre-processing
stage normalizes and decomposes the sentences into a series of components that
evaluate the same sequence of words applied to all the methods simultaneously. The
selection criteria of the pre-processing components have been conditioned by the

May 19, 2022

9/48

Table 3. Detailed setup for the ontology-based sentence similarity measures evaluated
in this work. The evaluation of the methods using Rada [68], coswJ&C [44], and
Cai [67] word similarity measures use a reformulation of the original path-based
measures based on the new Ancestors-based Shortest-Path Length (AncSPL)
algorithm [39].

ID

Sentence similarity method Detailed setup of each method

M7 WBSM-Rada [30, 39, 68]

WBSM [30] combined with Rada [68]
measure using the AncSPL algorithm [39]

M8 WBSM-J&C [30, 66, 69]

WBSM [30] combined with J&C [69]
measure and Sanchez et al. [66] IC model

M9

WBSM-cosJ&C
[30, 39, 44] (this work)

WBSM [30] with cosJ&C [44]
measure and Sanchez et al. [66] IC model
using the AncSPL algorithm [39]

M10

WBSM-coswJ&C
[30, 39, 44, 66] (this work)

WBSM [30] with coswJ&C [44] measure
and Sanchez et al. [66] IC model
using the AncSPL algorithm [39]

M11 WBSM-Cai [30, 39, 67]

WBSM [30] combined with Cai et al. [67]
measure and Cai et al. [67] IC model
using the AncSPL algorithm [39]

M12 UBSM-Rada [30, 39, 68]

UBSM [30] with Rada et al. [68]
measure using the AncSPL algorithm [39]

M13 UBSM-J&C [30, 66, 69]

UBSM [30] combined with J&C [69]
measure and Sanchez et al. [66] IC model

M14

UBSM-cosJ&C
[30, 44, 66] (this work)

UBSM [30] with cosJ&C [44] measure
and Sanchez et al. [66] IC model
using the AncSPL algorithm [39]

M15

UBSM-coswJ&C
[30, 39, 44, 66] (this work)

UBSM [30] with coswJ&C [44] measure
and Sanchez et al. [66] IC model
using the AncSPL algorithm [39]

M16 UBSM-Cai [30, 39, 67]

M17 COM [30, 68]

UBSM [30] combined with Cai et al. [67]
measure and Cai et al. [67] IC model
using the AncSPL algorithm [39]

λ·WBSM-Rada + (1 − λ)·UBSM-Rada
with λ = 0.5

following constraints: (a) the pre-processing methods and tools used by state-of-the-art
methods; and (b) the availability of resources and software tools. Figure 2 details all
the possible combinations of pre-processing configurations that are evaluated in this
work. String, word and sentence embedding, and ontology-based methods, are
evaluated using all the available configurations except the WordPieceTokenizer [90],
which is specific to BERT-based methods. Thus, BERT-based methods are evaluated

May 19, 2022

10/48

Table 4. Detailed setup for the sentence similarity methods based on pre-trained
character, word (WE) and sentence (SE) embedding models evaluated herein.
Sentence similarity method Detailed setup of each method

ID

M18 Flair [76]

M19 Pyysalo et al. [77]

M20 BioConceptVec [78]

M21 BioConceptVec [78]

M22 Newman-Griffis et al. [79]

M23 Newman-Griffis et al. [79]

M24 Newman-Griffis et al. [79]
M25 BioConceptVecGloV e [78]

Contextual string embeddings
trained on PubMed
Skip-gram trained on PubMed + PMC
Skip-gram WE model trained on PubMed
using word2vec program
CBOW WE model trained on PubMed
using word2vec program
Skip-gram WE model trained on PubMed
using word2vec program
CBOW WE model trained on PubMed
using word2vec program
GloVe WE model trained on PubMed
GloVe We model trained on PubMed

M26 BioWordVecint [80]

M27 BioWordVecext [80]

FastText [56] WE model trained on
PubMed + MeSH
FastText [56] trained on PubMed + MeSH

M28 BioNLP2016win2 [81]

M29 BioNLP2016win30 [81]

FastText [56] WE model based on skip-gram
and trained on PubMed with training setup
detailed in [81, table 18]

FastText [56] WE model based on skip-gram
and trained on PubMed with training setup
detailed in [81, table 18]

M30 BioConceptVecf astT ext [78] FastText [56] WE model trained on PubMed

M31

Universal Sentence
Encoder (USE) [82]

M32 BioSentVec [33]

M33

FastText-Skipgram-BioC
(this work)

USE SE pre-trained model of Cer et al. [82]

sent2vec [83] SE model trained on PubMed
+ MIMIC-III

FastText [56] WE model based on Skip-gram
and trained on PMC-BioC corpus (05,09,2019)
with the following setup: vector dim. = 200,
learning rate = 0.05, sampling thres. = 1e-4,
and negative examples = 10

using different char filtering, lower casing normalization, and stop words removal
configurations. We use the Pearson and Spearman correlation metrics together with
their harmonic score values to determine the impact of the different pre-processing
configurations on the performance of the methods evaluated herein. However, we set
the best overall performing pre-processing configuration using the harmonic average
scores, as well as answering the remaining research questions.

Most methods receive as input the sequences of words making up the sentences to

be compared. The process of splitting sentences into words can be carried out by

May 19, 2022

11/48

Table 5. Detailed setup for the sentence similarity methods based on pre-trained
language models evaluated in this work.

ID

Sentence similarity method

Detailed setup of each method

M34

BioBERT Base 1.0 [84]
(+ PubMed)

BERT [85] trained on English Wikipedia
+ BooksCorpus + PubMed abstracts

M35

BioBERT Base 1.0 [84]
(+ PMC)

M36

BioBERT Base 1.0 [84]
(+ PubMed + PMC)

BERT [85] trained on English
Wikipedia +
BooksCorpus + PMC full-text articles

BERT [85] trained on English Wikipedia
+ BooksCorpus + PubMed
abstracts + PMC full-text articles

M37

BioBERT Base 1.1 [84]
(+ PubMed)

BERT [85] trained on English Wikipedia
+ BooksCorpus + PubMed abstracts

M38

BioBERT Large 1.1 [84]
(+ PubMed)

BERT [85] trained on English Wikipedia
+ BooksCorpus + PubMed abstracts

M39

NCBI-BlueBERT
Base [32] PubMed

M40

NCBI-BlueBERT
Large [32] PubMed

M41

M42

NCBI-BlueBERT
Base [32]
PubMed + MIMIC-III

NCBI-BlueBERT
Large [32]
PubMed + MIMIC-III

BERT [85] trained on PubMed abstracts

BERT [85] trained on PubMed abstracts

BERT [85] trained on PubMed abstracts
+ MIMIC-III

BERT [85] trained on PubMed abstracts
+ MIMIC-III

SciBERT [86]

M43
M44 ClinicalBERT [87]

BERT [85] trained on PubMed abstracts
BERT [85] trained on PubMed abstracts

M45

PubMedBERT [88]
(abstracts)

BERT [85] trained on PubMed abstracts

M46

PubMedBERT [88]
(abstracts + full text)

BERT [85] trained on PubMed abstracts
+ full text

M47

ouBioBERT-Base [89]
(Uncased)

BERT [85] trained on PubMed abstracts

M48 BioClinicalBERT [29]

BERT [85] trained on MIMIC-III

M49

BioDischargesummaryBERT
[29]

BERT [85] trained on MIMIC-III summaries

M50 DischargesummaryBERT [29] BERT [85] trained on MIMIC-III summaries

May 19, 2022

12/48

tokenizers, such as the well-known general domain Stanford CoreNLP tokenizer [91],
which is used by Blagec et al. [31], or the biomedical domain BioCNLPTokenizer [92].
On the other hand, the use of lexicons instead of tokenizers for sentence splitting
would be inefficient because of the vast general and biomedical vocabulary. Besides,
there would not be possible to provide a fair comparison of the methods because the
pre-trained language models have no identical vocabularies.

The tokenized words that conform the sentence, named tokens, are usually

pre-processed by removing special characters and lower-casing, and removing the stop
words. To analyze all the possible combinations of token pre-processing configurations
from the literature, we replicate for each method those pre-processing configurations
used by other authors, such as Blagec et al. [31] and Sogancioglu et al. [30], and we
also evaluate all the pre-processing configurations that have not been evaluated yet.
We also study the impact of the pre-processing configurations by not removing special
characters and stop words from the tokens, nor normalizing them using lower-casing.
Ontology-based sentence similarity methods estimate the similarity of a sentence
by exploiting the ’is-a’ relationships between the concepts in an ontology. Therefore,
the evaluation of any ontology-based method receives a set of concept-annotated pairs
of sentences. The aim of the biomedical NER tools is to recognize automatically
biomedical entities in pieces of raw text, such as diseases or drugs. We evaluate the
impact of the three more broadly-used biomedical NER tools on the performance of
the sentence similarity methods, as follows: (a) MetaMap [65], (b) cTAKES [60], and
(c) MetaMap Lite [93]. MetaMap tool [65] is used by UBSM and COM methods [30]
for recognizing Unified Medical Language System (UMLS) [94] concepts in the
sentences, which is the standard compendium of biomedical vocabularies. Likewise, we
use the default configuration of MetaMap restricted to the UMLS sources of
SNOMED-CT and MeSH implemented by HESML V1R5 [39, 95], which is defined by
the following features: (i) the use of all available semantic types; (ii) the MedPost
Part-of-speech tagger [96]; and (iii) the MetaMap Word-Sense Disambiguation (WSD)
module. We also evaluate cTAKES [60] because it has shown to be a robust and
reliable tool to recognize biomedical entities [97]. Encouraged by the high
computational cost of MetaMap in evaluating large text corpus, Demner-Fushman et
al. [93] introduce a lighter MetaMap version, called Metamap Lite, which provides a
real-time implementation of the basic MetaMap annotation capabilities without a
large degradation of its performance.

Due to the large number of possible combinations of each pre-processing dimension,

such as Named Entity Recognizers, tokenizers or char filtering methods, we have
evaluated the pre-processing combinations of each dimension by defining a fixed
pre-processing configuration for the rest of dimensions, except for the string-based
methods, whose performance is high enough to not cause a significant variation in the
running time of the experiments.

Detailed workflow of our experiments

Figure 3 shows the workflow for running the experiments implemented in this work.
Given an input dataset, such as BIOSSES [30], MedSTS [50], or CTR [51], the first
step is to pre-process all the sentences, as shown in figure 4. For each sentence pair
(s1, s2) in the dataset, the pre-processing stage is divided into four stages as follows:
(1.a) named entity recognition of UMLS [94] concepts, using different state-of-the-art
NER tools, such as MetaMap [65] or cTAKES [60]; (1.b) tokenization of the sentences,
using well-known tokenizers, such as the Stanford CoreNLP tokenizer [91],
BioCNLPTokenizer [92], or WordPieceTokenizer [90] for BERT-based methods; (1.c)
lower-case normalization; (1.d) character filtering, which allows the removal of
punctuation marks or special characters; and finally, (1.e) the removal of stop-words,

May 19, 2022

13/48

Fig 2. Detail of the pre-processing configurations that are evaluated in this work. (*)
WordPieceTokenizer [90] is used only for BERT-based methods.

Pre-procesing
configurations






-Named Entity Recognizers 


- None
- MetaMap [65]
- MetaMap Lite [93]
- cTAKES [60]



- WhiteSpace (it breaks text into terms using white spaces)
- StanfordCoreNLP [91]
- BioCNLPTokenizer [92]
- WordPieceTokenizer* [90]

-Tokenizers 


-Lower-case normalization

-Char Filtering 



-Stop words removal

- Yes
- No

(cid:26)

- None
- Default
- BIOSSES [30]
- Blagec2019 [31]

- None
- BIOSSES [30]
- NLTK2018 [31, 98]






following different approximations evaluated by other authors like Blagec et al. [31] or
Sogancioglu et al. [30]. Once each dataset is pre-processed in step 1 detailed in figure
3), the aim of step 2 is to calculate the similarity score between each pair of sentences
in the dataset to produce a raw output file containing all raw similarity scores, one
score per sentence pair. Finally, a R-language script is used in step 3 to process the
raw similarity files and produce the final human-readable tables reporting the Pearson
and Spearman correlation values shown in table 8, as well as the statistical
significance of the results and any other supplementary data table required by our
study on the impact of the pre-processing and NER tools reported in appendices A
and B respectively.

Finally, we also evaluate all the pre-processing combinations for each family of
methods to study the impact of the pre-processing methods on the performance of the
sentence similarity methods, with the only exception of the BERT-based methods.
The pre-processing configurations of the BERT-based methods are only evaluated in
combination with the WordPiece Tokenizer [90] because it is required by the current
BERT implementations.

Evaluation metrics

The evaluation metrics used to compare the performance of the methods analyzed are
the following: (1) the Pearson correlation, denoted by r in equation (6); (2) the
Spearman rank correlation, denoted by ρ in equation (7); (3) and the harmonic score,
denoted by h in equation (8). The Pearson correlation evaluates the linear correlation
between two random samples, whilst the Spearman rank correlation is rank-invariant
and evaluates the monotonic relationship between two random samples, and the
harmonic score allows comparing sentence similarity methods by using a single
weighted score based on their performance in Pearson and Spearman correlation.

May 19, 2022

14/48

Fig 3. Detailed workflow implemented by our experiments for pre-processing the
input sentences, calculating the raw similarity scores, and post-processing the results
obtained in the evaluation of the biomedical datasets. This workflow generates a
collection of raw and processed data files.

Start

Input
dataset

Raw similarity
scores ﬁle

(3)
Post-processing
& report generation
(R script)

For each pair of sentences in the dataset (S1,S2):

s1

s2

(1)
Preprocess
sentence

S1

S2

(2)
Calculate
similarity

Raw sentence
similarity score

Final result ﬁles
& HTML reports

End

Fig 4. Detailed sentence pre-processing workflow that are implemented in our
experiments. The pre-processing stage takes an input sentence and produces a
pre-processed sentence as output. (*) The named entity recognizer are only evaluated
in ontology-based methods.

Start

Raw
sentence

(1.a)
NER*

(1.b)
Tokenizer

End

Preprocessed
sentence

(1.e)
Stop words
removal

(1.d)
Char ﬁltering

(1.c)
Lower-case
normalization

n
i=1

Xi − X

Yi − Y

n
(cid:0)
Xi − X
P
i=1
n
i=1 d2
6
(cid:0)
P
i
n (n2 − 1)

,

P

2

(cid:1) (cid:0)

n
i=1

(cid:1)
Yi − Y

2

(cid:1)

q

(cid:1)
P
di = (xi − yi)

(cid:0)

r =

q
ρ = 1 −

h =

2rρ
r + ρ

(6)

(7)

(8)

Finally, we use the well-known t-Student test to carry-out a statistical significance
analysis of the results of the evaluation of the methods in the tree biomedical datasets
shown in table 1. In order to compare the overall performance of the semantic
measures that is evaluated in our experiments, we use the harmonic score average in
all datasets. The statistical significance of the results is evaluated using the p-values
resulting from the t-student test for the mean difference between the harmonic score
values reported by each pair of semantic measures in all datasets. The p-values are
computed using a one-sided t-student distribution on two paired random sample
vectors made up by the harmonic (h) score values obtained in the evaluation of the
three aforementioned datasets. Our null hypothesis, denoted by H0, is that the
difference in the average performance between each pair of compared sentence
similarity methods is 0, whilst the alternative hypothesis, denoted by H1, is that their

May 19, 2022

15/48

average performance is different. For a 5% level of significance, it means that if the
p-value is greater or equal than 0.05, we must accept the null hypothesis. Otherwise,
we can reject H0 with an error probability of less than the p-value. In this latter case,
we say that a first sentence similarity method obtains a statistically significantly
higher value than the second one or that the former one significantly outperforms the
second one.

Uniform size datasets for our statistical significance analysis. The scarcity
of the datasets and the notable size difference among datasets varying from 100 to
1,068 sentence pairs prevent both from studying the statistical significance of the
results with adequate sample size and carry-out a fair comparison of the results. For
this reason, we have divided the MedSTS dataset into 10 parts considered as
independent datasets to perform the study of the statistical significance of the results.
Thus, we have artificially obtained 12 datasets of 100 to 200 pairs of sentences. This
set of datasets allows us to obtain the p-values comparing the statistical significance
between the measure, but does not modify the processed results from table 8. All the
necessary resources for obtaining both the table 8 and the table containing all the
p-values reported in Appendix A are publicly available in the reproducibility dataset
and the companion Lab Protocol article under preparation, as detailed in table 6.

Statistical performance analysis of the best methods

In order to answer the RQ5 research question, we study how well the sentence
similarity methods are estimating the degree of semantic similarity between two
sentences by analyzing the deviation of their estimated values regarding the human
similarity scores. We want to analyze why the methods are doing well or bad on
specific sentence pairs to elucidate some explanation to this behaviour, as well as
identifying the main drawbacks and limitations of the current state-of-the-art methods.
To carry out this performance analysis, we analyze the statistics of the similarity error
function Esim of the methods defined in equation 9. We only use some sentences
extracted from the BIOSSES dataset for this analysis because this dataset has no
licensing restrictions on its use, which allows us to reproduce their sentences herein,
unlike MedSTS. On the other hand, we could have also used CTR because it has no
licensing restrictions; however, CTR has not been previously used in this sentence
similarity task.

Esim : LΣ × LΣ → [0, 1] ⊂ R

Esim(s1, s2) = sim(s1, s2) − humanSim(s1, s2)

(9)

Our methodology to conduct the performance analysis is detailed below:

1. Selection of the best-performing method from each family of methods.

2. Estimation of the Probability Density Function (PDF) of the Esim function for

the evaluation of the selected best-performing methods in each dataset by calling
the “density” function provided by the R statistical package.

3. Selection of the sentences based on their similarity error in the BIOSSES dataset:

3.1 the sentences with the lowest and highest absolute similarity error |Esim|

for each method are extracted.

3.2 each sentence selected in the step above is pre-processed using the best

pre-processing configuration for each method.

May 19, 2022

16/48

3.3 the resulting pre-processed sentences and the statistical information of the

similarity scores are analyzed in the Discussion section.

Software implementation

We have developed a new sentence measures library for the biomedical domain called
HESML-STS, which is based on HESML V1R5 [38, 39], as detailed in table 6. All our
experiments are generated by running the HESMLSTSclient and
HESMLSTSImpactpre-processingclient programs, which generates a raw output file in
comma-separated file format (*.csv) for each dataset detailed in table 1. The raw
output files contain the raw similarity values returned by each sentence similarity
method in the evaluation of the degree of similarity between sentences. The final
results for the Pearson and Spearman correlation, and the harmonic values detailed in
table 8 are automatically generated by running a R-language script file on the
collection of raw similarity files, which also generates all the tables reported in
appendices A and B provided as supplementary material. All tables are written both
in Latex and comma-separated file format (*.csv) formats. For a more detailed
description of the protocol for running our experiments, we refer the reader to the
protocol [41] detailed in appendix C.

We implemented a parser for loading pre-trained embedding models based on

FastText [56] and other word embedding models [77–81], which are efficiently
evaluated as sentence similarity measures in HESML by implementing the averaging
Simple Word EMbeddings (SWEM) approach introduced by Shen et al. [99]. On the
other hand, the software replication required to evaluate sentence embeddings and
BERT-based language models is extremely complex and out of the scope of this work.
For this reason, these models are evaluated using the original software artifacts used
to generate the aforementioned pre-trained models. Thus, we implemented a collection
of Python wrappers for evaluating the available models by using the provided software
artifacts as follows: (1) Sent2vec-based models [33] are evaluated using the Sent2vec
library [83]; (2) Flair models [76] are evaluated using the flairNLP framework [76]; and
USE models [82] are evaluated using the open source platform TensorFlow [100]. All
BERT-based pre-trained models are evaluated using the open source bert-as-a-service
library [101].

Reproducing our benchmarks

For the sake of reproducibility, we introduce a detailed reproducibility protocol at
protocols.io [41] that is based on a reproducibility dataset [42] containing all the
software and data necessary to allow the exact replication of all our experiments and
results. Our reproducibility protocol is mainly based on a Docker-based image 4 that
include a pre-installation of all the necessary software and the Java source code and
binary files of our benchmark program. Our source code files are tagged in Github
with a permanent tag named “SentenceSimilarityBenchmark” 5.

In addition, we plan to submit a Lab Protocol6 article under preparation [102],

which will provide a detailed description of the publicly available reproducibility
dataset [42] and a very detailed reproduciblility protocol [41] to allow the exact
replication of all our methods, experiments, and results. We also plan to submit a
research article under preparation [103] to introduce the new HESML-STS software
library integrated into the latest HESML V2R1 version, together with a set of
reproducible benchmarks on semantic measures libraries for biomedical sentence

4https://hub.docker.com/repository/docker/alicialara/hesml_v2r1
5https://github.com/jjlastra/HESML/releases/tag/Release_HESML_V1R5.0.2
6https://collections.plos.org/collection/lab-protocols

May 19, 2022

17/48

similarity. The new HESML V2R1 release will make publicly available soon, once we
have appropriately separated the configurations requiring software restricted by
third-party licenses, such as cTAKES and Metamap NER tools, from the rest of the
project. However, our reproducibility dataset allows the full and exact replication of
all our experiments by completing the licensing requirements of the UMLS databases
and the aforementioned NER tools for the National Library of Medicine (NLM) of the
United States 7 .

Table 6 details all the reproducibility resources provided as supplementary material

with this work. Our benchmarks are implemented using Java 8, Python 3 and R
programming languages, and thus, they can be reproduced in any Java-complaint or
Docker-complaint platforms, such as Windows, MacOS, or any Linux-based system.

Table 6. Supplementary material and reproducibility resources of this work.

Material

Description

Reproducibility dataset [42]

Reproducibility protocol [41]

Lab Protocol article [102]
(under preparation)

HESML-STS software library
(integrated into HESML V2R1)

HESML V2R1 software release
(under preparation)

All raw input and output data files, pre-trained
model files, and a long-term reproducibility image
based on Docker, which is publicly available in the
Spanish Dataverse Network 8

Raw step-by-step instructions to download the
required resources and reproduce the experiments
evaluated in this work

Data and methods article introducing a very detailed
description of our experiments, datasets, and
reproducibility protocol to allow the independent
replication of our experiments and results

Release of the new HESML-STS library. This library
is based on the previous HESML V1R5 version [38, 39]
published in Github 9 and the Spanish Dataverse
Network [42] under a CC By-NC-SA-4.0 license.

Release of the new HESML V2R1 version which
will be published soon. This new release will be
based on the previous HESML V1R5 version,
including the new HESML-STS software package
that has been developed for this work, after
managing all the licensing restrictions of
the NER tools.

HESML-STS software paper [103]
(under preparation)

Software article introducing our sentence similarity
library, called HESML-STS, together with some
benchmarks under preparation.

7https://www.nlm.nih.gov/databases/umls.html#license_request

May 19, 2022

18/48

Results obtained

Table 7 shows the selected pre-processing configuration of each method for obtaining
their best-performing results, whilst table 8 shows the results obtained in the
evaluation of all methods in the three biomedical datasets evaluated herein by using
their best pre-processing configurations. Table 9 shows the comparison of results for
the highest (best) and lowest (worst) average harmonic score values for the
best-performing method of each family shown in blue in table 8, which are defined by
the method obtaining the highest average harmonic score. Furthermore, table 10
shows the results obtained in our study on the impact of NER tools on the
performance of the sentence similarity methods in the evaluation of the MedSTS
dataset [50]. Table 11 shows the harmonic and average harmonic scores obtained in
the evaluation of the three biomedical datasets, as well as the resulting p-values
comparing the NER tools for each ontology-based method. Table 12 shows the results
obtained in the evaluation of the LiBlock method in the three biomedical datasets by
using its best pre-processing configuration, and annotating the sentences with all the
NER tools combinations. In addition, the aforementioned table details the resulting
p-values comparing the best-performing LiBlock-NER combination with the other
NER tools. Tables 13, 14, 15, and 16 show the raw input sentence pairs and their
corresponding pre-processed versions in which the best-performing methods obtain the
lowest and highest similarity error (Esim) in the BIOSSES dataset [30]. Table 17
detail the statistical information for the best-performing methods of each family in the
evaluation of the three biomedical datasets evaluated herein. Finally, figure 5 shows
the Probability Density Function (PDF) of the similarity error obtained by the
best-performing methods of each family in the evaluation of the BIOSSES, MedSTS,
and CTR datasets respectively.

On the other hand, appendix A shows the resulting p-values comparing all the
methods using their best pre-processing configuration as detailed in 8, which allows us
to study the statistical significance of the results, as detailed in the Discussion section.
In addition, appendix B shows the experimental results on the impact of
pre-processing configurations in all the methods evaluated herein, whose best
configuration has been used to determine the final scores for each method. Finally,
appendix C detail the protocol for reproducing all the experiments evaluated herein,
which is also published in protocols.io [41].

May 19, 2022

19/48

Table 7. Best-performing pre-processing configurations used to evaluate the methods compared in this work as reported in
table 8, which are derived from our cross-evaluation of each method with the pre-processing configurations shown in figure
2 (see Appendix B). (*) COM (M17) uses the best configuration of the WBSM-Rada (M7) and UBSM-Rada (M12)
methods for computing the similarity scores.

ID

Sentence similarity method

NER

Tokenizer

Lower-case

Qgram
Jaccard
Block distance
LiBlock (this work)
Levenshtein distance
Overlap coefficient

M1
M2
M3
M4
M5
M6
M7 WBSM-Rada
M8 WBSM-J&C
M9 WBSM-cosJ&C (this work)
M10 WBSM-coswJ&C (this work)
M11 WBSM-Cai
M12 UBSM-Rada
M13 UBSM-J&C
M14 UBSM-cosJ&C (this work)
M15 UBSM-coswJ&C (this work)
M16 UBSM-Cai
M17 COM (*)
M18
Flair
M19 Pyysalo et al.
M20 BioConceptVecword2vec sg
M21 BioConceptVecword2vec cbow
M22 Newman-Griffisword2vec sgns
M23 Newman-Griffisword2vec cbow
M24 Newman-Griffisglove
M25 BioConceptVecglove
M26 BioWordVecint
M27 BioWordVecext
M28 BioNLP2016win2
M29 BioNLP2016win30
M30 BioConceptVecf astT ext
M31 USE

M32

BioSentVec
(PubMed+MIMIC-III)
M33
FastText-SkGr-BioC (this work)
M34 BioBERT Base 1.0 (+ PubMed)
M35 BioBERT Base 1.0 (+ PMC)
M36 BioBERT Base 1.0 (PubMed+PMC)
M37 BioBERT Base 1.1 (+ PubMed)
M38 BioBERT Large 1.1 (+ PubMed)
M39 NCBI-BlueBERT Base PubMed
M40 NCBI-BlueBERT Large PubMed

M41

M42

NCBI-BlueBERT
Base PubMed + MIMIC-III
NCBI-BlueBERT
Large PubMed + MIMIC-III
SciBERT

M43
M44 ClinicalBERT
M45 PubMedBERT (abstracts)
M46 PubMedBERT (abstracts+full text)
ouBioBERT-Base, Uncased
M47
M48 BioClinicalBERT
M49 BioDischargesummaryBERT
M50 DischargesummaryBERT

None
None
None
cTakes
None
None
Exact matching
Exact matching
Exact matching
Exact matching
Exact matching
cTAKES
MetamapLite
MetamapLite
cTAKES
MetamapLite
-
None
None
None
None
None
None
None
None
None
None
None
None
None
None

None

None
None
None
None
None
None
None
None

None

None

None
None
None
None
None
None
None
None

WhiteSpace
WhiteSpace
WhiteSpace
CoreNLP
WhiteSpace
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
-
WhiteSpace
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP
CoreNLP

CoreNLP

CoreNLP
WordPiece
WordPiece
WordPiece
WordPiece
WordPiece
WordPiece
WordPiece

WordPiece

WordPiece

WordPiece
WordPiece
WordPiece
WordPiece
WordPiece
WordPiece
WordPiece
WordPiece

yes
yes
yes
yes
no
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
-
no
yes
yes
yes
yes
yes
yes
yes
yes
yes
no
no
yes
no

yes

yes
yes
yes
yes
no
no
yes
yes

yes

yes

yes
no
yes
yes
yes
yes
no
no

Char
filtering
BIOSSES
BIOSSES
BIOSSES
Default
None
Default
BIOSSES
BIOSSES
BIOSSES
BIOSSES
BIOSSES
BIOSSES
BIOSSES
BIOSSES
BIOSSES
BIOSSES
-
BIOSSES
Default
Default
Default
Default
Default
Default
Default
BIOSSES
BIOSSES
Default
Default
Default
Default

Stop words
removal
NLTK2018
NLTK2018
NLTK2018
NLTK2018
BIOSSES
NLTK2018
NLTK2018
None
None
NLTK2018
None
NLTK2018
NLTK2018
NLTK2018
NLTK2018
NLTK2018

None
BIOSSES
BIOSSES
BIOSSES
NLTK2018
NLTK2018
NLTK2018
BIOSSES
None
None
NLTK2018
NLTK2018
BIOSSES
None

BIOSSES

BIOSSES

None
None
None
None

Default
BIOSSES
BIOSSES
BIOSSES
Blagec2019 NLTK2018
Blagec2019 NLTK2018
Blagec2019
BIOSSES

None
None

BIOSSES

BIOSSES

BIOSSES

None

NLTK2018
BIOSSES
BIOSSES
Blagec2019
NLTK2018
Default
NLTK2018
Default
None
Default
Blagec2019
BIOSSES
Blagec2019 NLTK2018
Blagec2019 NLTK2018

May 19, 2022

20/48

Table 8. Pearson (r), Spearman (ρ), harmonic (h), and harmonic average (AVG) scores obtained by each sentence
similarity method evaluated herein in the three biomedical sentence similarity benchmarks arranged by families. All
reported values were obtained using the best pre-processing configurations detailed in table 7. The results in bold show the
best scores whilst results in blue color show the best average harmonic score for each family.

Sentence similarity methods
Qgram
Jaccard
Block distance
LiBlock (this work)
Levenshtein distance
Overlap coefficient

ID
M1
M2
M3
M4
M5
M6
M7 WBSM-Rada
M8 WBSM-J&C
M9 WBSM-cosJ&C (this work)
M10 WBSM-coswJ&C (this work)
M11 WBSM-Cai
M12 UBSM-Rada
M13 UBSM-J&C
M14 UBSM-cosJ&C (this work)
M15 UBSM-coswJ&C (this work)
M16 UBSM-Cai
M17 COM
M18
Flair
M19 Pyysalo et al. [77]
M20 BioConceptVecword2vec sg
M21 BioConceptVecword2vec cbow
M22 Newman-Griffisword2vec sgns
M23 Newman-Griffisword2vec cbow
M24 Newman-Griffisglove
M25 BioConceptVecglove
M26 BioWordVecint
M27 BioWordVecext
M28 BioNLP2016win2
M29 BioNLP2016win30
M30 BioConceptVecf astT ext
M31 USE
M32 BioSentVec
M33
FastText-SkGr-BioC (this work)
M34 BioBERT Base 1.0 (+ PubMed)
M35 BioBERT Base 1.0 (+ PMC)
M36 BioBERT Base 1.0(P ubM ed+P M C)
M37 BioBERT Base 1.1 (+ PubMed)
M38 BioBERT Large 1.1 (+ PubMed)
M39 NCBI-BlueBERT Base PubMed
M40 NCBI-BlueBERT Large PubMed

M41

M42

NCBI-BlueBERT Base
PubMed + MIMIC-III
NCBI-BlueBERT Large
PubMed + MIMIC-III
SciBERT

M43
M44 ClinicalBERT
M45 PubMedBERT (abstracts)

M46

PubMedBERT
(abstracts+full text)
ouBioBERT-Base, Uncased

M47
M48 BioClinicalBERT
M49 BioDischargesummaryBERT
M50 DischargesummaryBERT

BIOSSES [30]
ρ
0.773
0.815
0.818
0.828
0.536
0.795
0.791
0.549
0.549
0.566
0.542
0.809
0.573
0.648
0.769
0.579
0.809
0.625
0.706
0.743
0.655
0.763
0.686
0.678
0.585
0.806
0.725
0.693
0.751
0.262
0.669
0.767
0.777
0.567
0.663
0.609
0.647
0.546
0.668
0.712

h
0.763
0.798
0.808
0.824
0.533
0.788
0.782
0.514
0.514
0.568
0.497
0.800
0.550
0.631
0.749
0.562
0.801
0.626
0.709
0.742
0.662
0.767
0.681
0.674
0.565
0.818
0.738
0.695
0.748
0.135
0.668
0.782
0.795
0.568
0.664
0.612
0.657
0.551
0.675
0.700

r
0.752
0.782
0.798
0.820
0.529
0.782
0.772
0.483
0.483
0.571
0.458
0.792
0.529
0.615
0.730
0.545
0.793
0.628
0.713
0.742
0.670
0.771
0.675
0.671
0.547
0.831
0.752
0.697
0.745
0.091
0.666
0.797
0.814
0.569
0.664
0.616
0.668
0.557
0.682
0.688

MedSTSf ull [50]
ρ
0.674
0.680
0.683
0.710
0.634
0.564
0.709
0.614
0.614
0.651
0.601
0.700
0.621
0.638
0.625
0.628
0.708
-0.035
0.641
0.652
0.650
0.641
0.647
0.643
0.648
0.686
0.673
0.594
0.609
0.456
0.606
0.638
0.660
0.576
0.581
0.561
0.616
0.622
0.565
0.588

h
0.687
0.693
0.706
0.739
0.622
0.623
0.740
0.630
0.630
0.677
0.615
0.730
0.650
0.667
0.659
0.656
0.739
-0.020
0.693
0.698
0.695
0.697
0.693
0.688
0.682
0.724
0.712
0.642
0.657
0.435
0.640
0.695
0.706
0.616
0.624
0.601
0.661
0.657
0.617
0.611

r
0.701
0.706
0.731
0.769
0.610
0.696
0.774
0.647
0.647
0.705
0.629
0.763
0.683
0.699
0.697
0.686
0.773
-0.014
0.754
0.751
0.746
0.764
0.746
0.740
0.720
0.766
0.756
0.699
0.714
0.416
0.679
0.763
0.758
0.662
0.674
0.647
0.712
0.695
0.679
0.636

CTR [51]
ρ
0.766
0.797
0.801
0.808
0.536
0.793
0.765
0.516
0.516
0.590
0.459
0.794
0.585
0.646
0.673
0.576
0.783
0.719
0.803
0.800
0.714
0.835
0.768
0.729
0.694
0.735
0.729
0.759
0.810
0.264
0.684
0.821
0.760
0.642
0.647
0.663
0.663
0.650
0.719
0.674

h
0.764
0.777
0.799
0.800
0.516
0.787
0.775
0.526
0.526
0.613
0.475
0.785
0.602
0.676
0.693
0.607
0.786
0.684
0.773
0.768
0.685
0.817
0.731
0.731
0.657
0.746
0.732
0.724
0.774
0.212
0.674
0.806
0.760
0.629
0.623
0.650
0.653
0.612
0.693
0.640

r
0.763
0.759
0.797
0.793
0.498
0.781
0.785
0.536
0.536
0.637
0.492
0.776
0.620
0.709
0.713
0.642
0.789
0.652
0.744
0.738
0.659
0.799
0.697
0.732
0.624
0.757
0.736
0.691
0.742
0.178
0.663
0.791
0.761
0.616
0.601
0.638
0.643
0.579
0.668
0.609

AVG
h
0.738
0.756
0.771
0.788
0.557
0.733
0.766
0.557
0.557
0.619
0.529
0.772
0.601
0.658
0.700
0.608
0.776
0.430
0.725
0.736
0.681
0.760
0.701
0.698
0.635
0.763
0.727
0.687
0.727
0.261
0.660
0.761
0.754
0.604
0.637
0.621
0.657
0.607
0.662
0.650

0.537

0.536

0.536

0.733

0.624

0.674

0.548

0.553

0.550

0.587

0.560

0.578

0.569

0.675

0.628

0.651

0.487

0.504

0.496

0.572

0.653
0.415
0.502

0.616
0.483
0.524

0.634
0.447
0.513

0.727
0.652
0.626

0.643
0.566
0.531

0.683
0.606
0.575

0.604
0.470
0.479

0.682
0.500
0.645

0.641
0.485
0.550

0.652
0.512
0.546

0.659

0.651

0.655

0.712

0.590

0.645

0.596

0.675

0.633

0.644

0.687
0.416
0.376
0.395

0.729
0.447
0.397
0.465

0.707
0.431
0.387
0.427

0.707
0.646
0.637
0.655

0.583
0.562
0.565
0.567

0.639
0.601
0.599
0.608

0.670
0.472
0.385
0.376

0.694
0.478
0.465
0.407

0.682
0.475
0.421
0.391

0.676
0.502
0.469
0.475

May 19, 2022

21/48

Table 9. Comparison of results for the “best” and the “worst” pre-processing configurations for the best-performing
methods of each family in table 8. The last column shows the t-Student p-values comparing the best and worst
configurations.

ID

Methods

M4

LiBlock
(worst)

M4

LiBlock
(best)

M17

COM
(worst)

M17

COM
(best)

M26

BioWordVecint
(worst)

M26

BioWordVecint
(best)

M47

OuBioBert
(worst)

M47

OuBioBert
(best)

Pre-processing
configuration
TOK-Whitespace
LC-No
SW-NLTK2018
CF-None

TOK-CoreNLP
LC-Yes
SW-NLTK2018
CF-Default
- WBSM-Rada
- UBSM-Rada
(worst):
TOK-Whitespace
LC-Yes
SW-None
CF-None

- WBSM-Rada
- UBSM-Rada
(best):
TOK-CoreNLP
LC-Yes
SW-NLTK2018
CF-BIOSSES
TOK-Whitespace
LC-No
SW-None
CF-None
Pooling-Sum

TOK-CoreNLP
LC-Yes
SW-None
CF-BIOSSES
Pooling-Min
TOK- WordPiece
LC-Yes
SW-BIOSSES
CF-Default

TOK-WordPiece
LC-Yes
SW-None
CF-Default

BIOSSES

MedSTSf ull

CTR

AVG

r

ρ

h

r

ρ

h

r

ρ

h

h

p-val

0.779

0.793

0.786

0.736

0.676

0.704

0.765

0.717

0.741

0.744

0.820

0.828

0.824

0.769

0.710

0.739

0.793

0.808

0.800

0.788

0.610

0.635

0.622

0.681

0.648

0.664

0.656

0.662

0.659

0.648

0.793

0.809

0.801

0.773

0.708

0.739

0.789

0.783

0.786

0.776

0.436

0.497

0.465

0.532

0.619

0.572

0.529

0.674

0.593

0.543

0.831

0.809

0.820

0.764

0.682

0.721

0.761

0.736

0.748

0.763

0.608

0.627

0.617

0.730

0.622

0.672

0.669

0.696

0.682

0.657

0.687

0.729

0.707

0.707

0.583

0.639

0.670

0.694

0.682

0.676

0.000

0.000

0.000

0.000

May 19, 2022

22/48

Table 10. Pearson (r), Spearman (ρ) and harmonic (h) values obtained in our experiments from the evaluation of ontology
similarity methods detailed below in the MedSTSf ull [50] dataset for each NER tool.

Methods

r
ID
0.711
M12 UBSM-Rada
0.576
M13 UBSM-J&C
M14 UBSM-cosJ&C
0.637
M15 UBSM-coswJ&C 0.675
0.606
M16 UBSM-Cai
0.758
M17 COM

MetaMap
ρ
0.653
0.547
0.575
0.608
0.555
0.692

h
0.681
0.561
0.605
0.64
0.58
0.724

MetaMap Lite
ρ
0.689
0.621
0.638
0.659
0.628
0.706

r
0.753
0.683
0.699
0.722
0.686
0.770

h
0.720
0.65
0.667
0.689
0.656
0.737

cTAKES
ρ
0.7
0.549
0.581
0.625
0.552
0.708

h
0.73
0.588
0.617
0.659
0.591
0.739

r
0.764
0.634
0.659
0.697
0.635
0.773

Table 11. Harmonic score obtained by each combination of a sentence similarity method with a NER tool in the
evaluation of the three sentence similarity datasets. The p-values shown in this table are obtained by using the method for
building uniform size datasets detailed above. The last column shows the p-values corresponding to the t-Student test
comparing the performance of each combination with the best pair in each group.

ID

Method

NER tool

M12 UBSM-Rada

M13 UBSM-J&C

M14 UBSM-cosJ&C

cTAKES
MetamapLite
Metamap
MetamapLite
cTAKES
Metamap
MetamapLite
cTAKES
Metamap
cTAKES

M15 UBSM-coswJ&C MetamapLite

M16 UBSM-Cai

M17 COM

Metamap
MetamapLite
cTAKES
Metamap
cTAKES
MetamapLite
Metamap

BIOSSES
h
0.800
0.744
0.742
0.55
0.595
0.316
0.631
0.681
0.537
0.749
0.678
0.656
0.562
0.616
0.419
0.801
0.788
0.792

MedSTS
h
0.730
0.72
0.680
0.65
0.588
0.561
0.667
0.617
0.605
0.659
0.689
0.64
0.656
0.591
0.58
0.739
0.737
0.724

CTR
h
0.785
0.785
0.723
0.602
0.552
0.234
0.674
0.626
0.434
0.693
0.732
0.551
0.607
0.571
0.318
0.786
0.789
0.768

Avg
h
0.772
0.751
0.715
0.601
0.578
0.37
0.657
0.641
0.525
0.700
0.700
0.616
0.608
0.593
0.439
0.776
0.772
0.761

p-value

—
0.011
0.000
—
0.000
0.000
—
0.002
0.000
—
0.018
0.005
—
0.001
0.000
—
0.052
0.004

Table 12. Pearson (r) and Spearman (ρ) correlation values, harmonic score (h), and harmonic average (AVG) score
obtained by the LiBlock method in combination with each NER tool using the best pre-processing configuration detailed in
7. In addition, last column (p-val) report the p-values for the comparison of the LiBlock method with cTAKES and the
remaining NER combinations.

BIOSSES [30]

MedSTSf ull [50]

CTR [51]

AVG

ID

M4
M4
M4
M4

Sentence similarity
methods

LiBlock-cTAKES
LiBlock-noNER
LiBlock-MetamapLite
LiBlock-Metamap

r

ρ

h

r

ρ

h

r

ρ

h

h

0.820
0.814
0.799
0.807

0.828
0.823
0.819
0.826

0.824
0.819
0.809
0.816

0.769
0.770
0.763
0.753

0.710
0.709
0.705
0.690

0.739
0.738
0.733
0.720

0.793
0.795
0.794
0.792

0.808
0.805
0.808
0.807

0.800
0.800
0.801
0.799

0.788
0.786
0.781
0.779

p-val

-
0.14
0.015
0.003

May 19, 2022

23/48

Table 13. Raw and pre-processes sentence pairs obtaining the lowest and highest similarity error Esim together with their
corresponding Normalized human similarity score (Human) and normalized similarity value (Method) estimated by the
LiBlock (M4) method for the raw and pre-processed sentence pairs with the lowest (L) and highest (H) similarity error
Esim.

Esim Input sentence

s1: “Centrosomes increase both in
size and in microtubule-nucleating
capacity just before mitotic entry.”

L

H

s2: “Functional studies showed that, when
introduced into cell lines, miR-146a was
found to promote cell proliferation in cervical
cancer cells, which suggests that miR-146a works
as an oncogenic miRNA in these cancers.”
s1: “Consequently miRNAs have been
demonstrated to act either as
oncogenes (e.g., miR-155, miR-17−5p
and miR-21) or tumor suppressors (e.g.,
miR-34, miR-15a, miR-16−1 and let-7)”

s2: “Given the extensive involvement of
miRNA in physiology, dysregulation of
miRNA expression can be associated with
cancer pathobiology including oncogenesis],
proliferation, epithelial-mesenchymal
transition, metastasis, aberrations in
metabolism, and angiogenesis, among others”

Pre-processed sentence
analyzed by the method

s1: “C0242608 increase size C0026046
nucleating capacity mitotic entry”

s2: “functional studies showed introduced
C0007634 lines mir 146a found promote
C0007634 C0334094 C4048328 C0007634
suggests mir 146a works oncogenic
mirna C0006826”

s1: “consequently mirnas demonstrated
C0427611 either oncogenes e g mir 155 mir
17 5p mir 21 C0027651 suppressors
e g mir 34 mir 15a mir 16 1 let 7”

s2: “given extensive involvement mirna
physiology dysregulation mirna C0185117
associated C0006826 pathobiology including
oncogenesis C0334094 epithelial mesenchymal
transition metastasis aberrations C0025519
angiogenesis among others”

Human Method

0.0

0.0

0.7

0.0

May 19, 2022

24/48

Table 14. Raw and pre-processes sentence pairs obtaining the lowest and highest similarity error Esim together with their
corresponding Normalized human similarity score (Human) and normalized similarity value (Method) estimated by the
COM (M17) method for the raw and pre-processed sentence pairs with the lowest (L) and highest (H) similarity error Esim.
We show the raw and pre-processed sentence pairs evaluated by the WBSM and UBSM similarity methods that make up
the COM method. The UBSM method use the cTAKES NER tool.

Esim Input sentence

s1: “The in vivo data is still preliminary
and other potential roadblocks such as
drug resistance have not been examined.”

Low

s2: “The GEM model used in this study
retains wild-type Tp53, suggesting
that the tumors successfully treated
with bortezomib and fasudil might
not be as aggressive as those
in most NSCLC patients”

s1: “The oncogenic activity of mutant Kras
appears dependent on functional Craf,
but not on Braf”

High

s2: “Notably, c-Raf has recently been
found essential for development
of K-Ras-driven NSCLCs”

Pre-processed sentence
analyzed by the method
s1, WBSM-Rada: “vivo data still preliminary
potential roadblocks drug resistance examined”
s1, UBSM-Rada: “vivo data still preliminary potential
roadblocks C0013227 resistance examined”

s2, WBSM-Rada: “gem model used study retains
wild type tp53 suggesting tumors successfully treated
bortezomib fasudil might aggressive nsclc patients”
s2, UBSM-Rada: “gem model used study retains wild
type tp53 suggesting C0027651 successfully treated
C1176309 fasudil might aggressive C0007131 patients”
s1, WBSM-Rada: “oncogenic activity mutant kras
appears dependent functional craf braf”
s1, UBSM-Rada: “oncogenic C0026606 mutant
kras appears dependent functional craf braf”

s2, WBSM-Rada: “notably c raf recently found
essential development k ras driven nsclcs”
s2, UBSM-Rada: “notably c raf recently
found essential development k C0525678
driven nsclcs”

Human Method

0.0

0.0

0.75

0.0

Table 15. Raw and pre-processes sentence pairs obtaining the lowest and highest similarity error Esim together with their
corresponding Normalized human similarity score (Human) and normalized similarity value (Method) estimated by the
BioWordVecint (M26) method for the raw and pre-processed sentence pairs with the lowest (L) and highest (H) similarity
error Esim.

Esim Input sentence

s1: “The up-regulation of miR-146a
was also detected in cervical
cancer tissues.”

s2: “The expression of miR-146a
has been found to be up-regulated
in cervical cancer.”
s1: “This oxidative branch activity
is elevated in comparison to many
cancer cell lines, where the
oxidative branch is typically reduced
and accounts for ¡20% of the carbon
flow through PPP.”

Low

High

Pre-processed sentence
analyzed by the method
s1: “the up regulation of mir 146a
was also detected in cervical
cancer tissues”

s2: “the expression of mir 146a
has been found to be up regulated in
cervical cancer”
s1: “this oxidative branch activity
is elevated in comparison to many
cancer cell lines where the
oxidative branch is typically reduced
and accounts for ¡ 20 % of the
carbon flow through ppp”

Human Method

1.0

0.986

s2: “The Downward laboratory went
all the way from identifying
GATA2 as a novel synthetic lethal gene
to validating it using
Kras-driven GEM models.”

s2: “the downward laboratory went
all the way from identifying gata2
as a novel synthetic lethal gene
to validating it using kras driven
gem models”

0.0

0.912

May 19, 2022

25/48

Table 16. Raw and pre-processes sentence pairs obtaining the lowest and highest similarity error Esim together with their
corresponding Normalized human similarity score (Human) and normalized similarity value (Method) estimated by the
OuBioBert (M47) method for the raw and pre-processed sentence pairs with the lowest (L) and highest (H) similarity error
Esim.

Esim Input sentence

s1: “Expression of an activated form
of Ras proteins can induce senescence in
some primary fibroblasts.”

Low

s2: “The senescent state has been
observed to be inducible in certain
cultured cells in response to high
level expression of genes
activated such as the ras oncogene.”
s1: “The in vivo data is still preliminary
and other potential roadblocks such as drug
resistance have not been examined.”

High

s2: “The GEM model used in this study
retains wild-type Tp53, suggesting
that the tumors successfully treated with
bortezomib and fasudil might not be as
aggressive as those in most NSCLC patients”

Pre-processed sentence
analyzed by the method

Human Method

s1: “expression activated form ras proteins
induce senescence primary fibroblasts”

s2: “senescent state observed inducible
certain cultured cells response high level
expression genes activated ras oncogene”

0.9

0.908

s1: “vivo data still preliminary potential
road bl ocks drug resistance examined”

s2: “gem model used study retains wild
type tp53 suggesting tumors successfully
treated bortezomib fas udi l might
aggressive nsclc patients”

0.0

0.773

May 19, 2022

26/48

Fig 5. Probability Density Function (PDF) and mean value of the similarity error
(Esim) obtained by the best-performing methods in the evaluation of each dataset as
follows: (a) BIOSSES, (b) MedSTS, and (c) CTR.

Table 17. Comparison of the mean, minimum and maximum similarity scores of the
Normalized Human similarity scores (Human) and the estimated valued returned by
the best-performing methods of each family in the evaluation of the three biomedical
datasets.

BIOSSES dataset

ID Method

Human
LiBlock (this work)

-
M4
M17 COM [30]
M26 BioWordVecint [80]
M47 OuBioBert [89]

MedSTS dataset

ID Method

Human
LiBlock (this work)

-
M4
M17 COM [30]
M26 BioWordVecint [80]
M47 OuBioBert [89]

CTR dataset

ID Method

Human
LiBlock (this work)

-
M4
M17 COM [30]
M26 BioWordVecint [80]
M47 OuBioBert [89]

Mean
similarity
0.549
0.194
0.22
0.933
0.808

Minimum
similarity
0
0
0
0.858
0.582

Maximum
similarity
1
0.506
0.596
0.987
0.936

Mean
similarity
0.632
0.611
0.631
0.957
0.885

Minimum
similarity
0
0
0
0.832
0.437

Maximum
similarity
1
1
1
1
0.997

Mean
similarity
0.254
0.103
0.118
0.898
0.724

Minimum
similarity
0
0
0
0.752
0.472

Maximum
similarity
1
0.743
0.793
0.992
0.98

Discussion

Comparison of string-based methods

LiBlock (M4) obtains the highest average harmonic score among the family of
string-based methods and significantly outperforms all of them. This conclusion can be
drawn by looking at the average column in table 8 for this group of methods and
checking the p-values reported in table A.1, such as Block Distance (p-value=0.000),
Jaccard (p-value=0.000), QGram (p-value=0.000), Overlap Coefficient
(p-value=0.000), and Levenshtein (p-value=0.000).

LiBlock (M4) obtains the highest Pearson correlation value in the BIOSSES and

MedSTS datasets among the family of string-based methods, whilst Block Distance
(M3) obtains the highest Pearson correlation in the CTR dataset. This conclusion can
be drawn by looking the results for the first group of methods detailed in table 8.

May 19, 2022

27/48

LiBlock (M4) obtains the highest Spearman correlation value in all datasets among

the family of string-based methods. This conclusion can be drawn by looking at the
results for the first group of methods detailed in table 8.

LiBlock (M4) obtains the highest harmonic score in all datasets among the family
of string-based methods. This conclusion can be drawn by looking the results for the
first group of methods detailed in table 8.

Comparison of Ontology-based methods

COM (M17) obtains the highest average harmonic score among the family of
ontology-based methods significantly outperform all of them, with the only exception of
WBSM-Rada (M7). This conclusion can be drawn by looking at the average column in
table 8 for the second group of methods and checking the p-value shown in table A.1
for the comparison of COM (M17) with WBSM-Rada (M7) (p-value=0.088).

COM (M17) obtains the highest Pearson correlation value in the BIOSSES and
CTR datasets among the family of ontology-based methods, whilst the WBSM-Rada
(M7) methods obtain the highest Pearson correlation value in the MedSTS dataset.
This conclusion can be drawn by looking at the second group of methods in 8.

COM (M17) obtains the highest Spearman correlation values in the BIOSSES
dataset among the family of ontology-based methods, whilst WBSM-Rada (M7) and
UBSM-Rada (M12) do it in the MedSTS and CTR datasets, respectively. This
conclusion can be drawn by looking at the second group of methods in 8.

COM (M17) obtains the highest harmonic score in the BIOSSES and CTR datasets

among the family of ontology-based methods, whilst WBSM-Rada (M7) does it in the
MedSTS dataset. This conclusion can be drawn by looking at the second group of
methods detailed in table 8.

Comparison of embeddings methods

BioWordVecint (M26) obtains the highest average harmonic score in all datasets
among the family of embedding methods detailed in table 4, and significantly
outperforms all of them. This conclusion can be drawn by looking at the third group
of methods in table 8 and checking the p-values reported in table A.1, which compare
the harmonic score values obtained by the BioWordVecint (M26) method with the rest
of methods from the same family, such as FastText-SkGr-BioC (p-value=0.032),
BioWordVecext (p-value = 0.007), and BioSentVec (p-value=0.022) among others.
BioWordVecint (M26) obtains the highest Pearson correlation value in the

BIOSSES and MedSTS datasets among the family of embedding methods, whilst the
Newman
-Griffisword2vec sgns (M22) model does it in the CTR dataset. This conclusion can be
drawn by looking the results for third group of methods detailed in table 8.

BioWordVecint (M26) obtains the highest Spearman correlation in the BIOSSES

and MedSTS datasets among the family of embedding methods, whilst the
Newman-Griffisword2vec sgns (M22) model does it in the CTR dataset. This later
conclusion can be drawn by looking the results for the third group of measures
detailed in table 8.

BioWordVecint (M26) obtains the highest harmonic score in the BIOSSES and

MedSTS datasets among the family of embedding methods, whilst the
Newman-Griffisword2vec sgns (M22) model does it in the CTR dataset. This later
conclusion can be drawn by looking the results for the third group of measures
detailed in table 8.

May 19, 2022

28/48

Comparison of BERT-based methods

OuBioBERT (M47) obtains the highest average harmonic score among the family of
BERT-based methods. However, it does not significantly outperform all of them. This
conclusion can be drawn by looking at the last group of methods in table 8 and
checking the p-values reported in table A.1. Table A.1 shows that ouBioBERT obtains
p-values higher than 0.05 when it is compared with many BERT-based methods, such
as BioBERT Large 1.1 (p-value=0.224) and PubMedBERT (abstracts+full text)
(p-value=0.101) among others.

NCBI-BlueBERT Large PubMed (M40) obtains the highest Pearson correlation
value in the BIOSSES dataset among the family of BERT-based methods, whilst the
NCBI-BlueBERT Base PubMed + MIMIC-III (M41) and the ouBioBERT (M47)
models do it in the MedSTS and the CTR datasets, respectively. This later conclusion
can be drawn by looking at the last group of measures detailed in table 8.

ouBioBERT (M47) obtains the highest Spearman correlation value in the BIOSSES

dataset among the family of BERT-based methods, whilst SciBERT (M43) and
NCBI-BlueBERT Base PubMed (M39) do it in the MedSTS and CTR datasets,
respectively. This conclusions can be drawn by looking at the last group of measures
detailed in table 8.

ouBioBERT (M47) obtains the highest harmonic score in the BIOSSES dataset

among the family of BERT-based methods, whilst SciBERT (M43) and
NCBI-BlueBERT Base PubMed (M39) do it in the MedSTS and CTR datasets,
respectively. This conclusion can be drawn by looking at the last group of measures
detailed in table 8.

Comparison of all methods

LiBlock (M4) obtains the highest average harmonic score for all the methods evaluated
herein, and significantly outperforms all the methods based on embeddings and language
models. However, there is no a statistically significant difference in performance with
the ontology-based methods COM (M17) and WBSM-Rada (M7). This conclusion can
be drawn by looking at the average column in table 8 and checking the p-value
reported in table A.1, which compare the harmonic score obtained by the LiBlock
method with the COM (p-value=0.121) and WBSM-Rada (p-value=0.098) methods.

BioWordVecint (M26) obtain the highest Pearson correlation values in the

BIOSSES dataset among all methods evaluated herein, whilst WBSM-Rada (M7) and
Newman-Griffisword2vec sgns (M22) do it in the MedSTS and CTR datasets,
respectively. This conclusion can be drawn by looking at the bold values detailed in
table 8.

LiBlock (M4) obtains the highest Spearman correlation value in the BIOSSES and

MedSTS datasets among all methods evaluated herein, whilst
Newman-Griffisword2vec sgns (M22) does it in the CTR dataset. This conclusions can
be drawn by looking at the bold values detailed in table 8.

LiBlock (M4) obtains the highest harmonic score in the BIOSSES dataset among all

methods evaluated herein, whilst WBSM-Rada (M7) and Newman-Griffisword2vec sgns
(M22) do it in the MedSTS and CTR datasets, respectively. This conclusion can be
drawn by looking at the bold values detailed in table 8.

COM (M17) obtains the second highest average harmonic score among all methods

evaluated herein, and it is able to outperform significantly all methods with the only
exception of LiBlock (M4) and WBSM-Rada (M7). This conclusion can be drawn by
looking at the bold values detailed in table 8 and checking the p-value reported in
table A.1.

May 19, 2022

29/48

Non ML-based methods versus ML-based ones

The string-based methods LiBlock (M4) and Block Distance (M3) obtain a higher
average harmonic score than all the embedding-based methods in all datasets. Moreover,
the string-based method LiBlock (M4) significantly outperforms all the methods based
on embedding models. This conclusion can be drawn by looking at the average column
in table 8 and checking the p-values reported in table A.1, such as BioWordVecint
(p-value 0.003), FastText-SkGr-BioC (p-value 0.002), BioConceptVecglove (p-value
0.001), Flair (p-value 0.027), and the rest of embedding-based methods (p-value 0.000).

All string-based methods obtain a higher average harmonic score than all the

BERT-based methods considering all datasets, with the only exception of the
Levenshtein distance (M5). Moreover, most string-based methods significantly
outperforms all BERT-based methods, with the only exception of the Levenshtein
distance (M5). This conclusion can be drawn by looking at the average column in
table 8 and checking the p-values reported in table A.1.

The ontology-based methods COM (M17), WBSM-Rada (M7) and UBSM-Rada
(M12) obtain a higher average harmonic score than all the embedding-based methods
considering all datasets and significantly outperforms all of them. This conclusion can
be drawn by looking at the average column in table 8 and checking the p-values
reported in table A.1, which compare the harmonic scores obtained by COM (M17),
WBSM-Rada (M7) and UBSM-Rada (M12) with all the embedding-based methods.

The ontology-based methods UBSM-Rada (M12), WBSM-Rada (M7), COM (M17)

and UBSM-coswJ&C (M15) obtain a higher average harmonic score than all the
BERT-based methods. Moreover, the ontology-based methods UBSM-Rada (M12),
WBSM-Rada (M7), and COM (M17) significantly outperforms all the BERT-based
methods. This conclusion can be drawn by looking at the average column in table 8
and checking the p-values reported in table A.1.

All embedding methods obtain a higher average harmonic score than all

BERT-based methods, with the only exceptions of Flair (M18), BioConceptVecglove
(M25), BioConceptVecf astT ext (M30) and USE (M31). This conclusion can be drawn
by looking at the last column in table 8.

BioWordVecint (M26) obtains a higher average harmonic score than all the

BERT-based methods considering all datasets and significantly outperforms all of them.
This conclusion can be drawn by looking at the average column in table 8 and checking
the p-values reported in table A.1, which compare the harmonic scores obtained by
BioWordVecint (M26) with all the BERT-based methods, such as SciBERT (p-value
0.001), NCBI-BlueBERT Base PubMed + MIMIC-III (p-value 0.002), BioBERT Large
1.1 (p-value 0.001), and the rest of BERT-based methods (p-value 0.000).

Impact of the NER tools on the ontology-based methods

This section analyzes the impact of the NER tools on the performance of the sentence
similarity methods, and studies the overall impact of the NER configurations. Table
10 shows the results obtained on the performance of NER tools for the sentence
similarity methods evaluated in the MedSTS dataset [50], whilst table 11 shows the
harmonic and average harmonic scores, as well as the resulting p-values comparing the
harmonic score of the best-performing NER tool for each ontology-based method in
the three datasets with the harmonis scores obtained by the other two NER tools.

MetamapLite obtains the highest Pearson, Spearman, and harmonic scores for the

MedSTS dataset in combination with UBSM-J&C (M13), UBSM-cosJ&C (M14),
UBSM-coswJ&C (M15) and UBSM-Cai (M16), whilst cTAKES obtains the highest
Pearson, Spearman and harmonic scores for the MedSTS dataset in combination with
UBSM-Rada (M12) and COM (M17). This later conclusion can be drawn by looking

May 19, 2022

30/48

at the results shown in table 10.

cTAKES obtains the highest average harmonic score for the three datasets in
combination with UBSM-Rada (M12), UBSM-coswJ&C (M15) and COM (M17)
methods, whilst MetamapLite obtains the highest average harmonic score for the three
datasets in combination with UBSM-J&C (M13), UBSM-cosJ&C (M14) and
UBSM-Cai (M16). This conclusion can be drawn by looking at the harmonic scores of
the NER tools in table 11.

cTAKES combined with COM (M17) obtains the best-performing results of

ontology-based methods for the three datasets. This conclusion can be drawn by looking
at the average harmonic scores column shown in table 11.

cTAKES is the best-performing tool in combination with the UBSM-Rada (M12),

UBSM-coswJ&C (M15), and COM (M17) methods in the three datasets, and
significantly outperforms MetamapLite and Metamap or the two former methods.
However, there is no a statistically significant diference regarding the Metamap tools
when it is combined with the COM (M17) method. This conclusion can be drawn by
looking at the average harmonic scores and p-values shown in table 11.

MetamapLite is the best-performing tool in combination with the UBSM-J&C
(M13), UBSM-cosJ&C (M14), and UBSM-Cai (M16) methods in the three datasets,
and significantly outperforms cTAKES and Metamap. This conclusion can be drawn
by looking at the average harmonic scores and p-values shown in table 11.

The choice of the best NER tool for each method significantly impact their
performance in most cases. This conclusion follows from the conclusions above.

Answering RQ3. Our results show that the ontology-based methods obtain their
best performance in the task of biomedical sentence similarity when they use either
MetamapLite or cTAKES. Thus, Metamap should not be used in combination with
any of the ontology-based methods evaluated herein in this later task. Likewise, the
results and p-values reported table 11 show that there is a significant difference in the
performance of each ontology-based method according to the NER tool used in most
cases. The conclusions above confirm that the selection of the NER tool significantly
impacts the performance of the sentence similarity methods using it.

Impact of the NER tools on the new LiBlock measure

This section analyzes the impact of the NER tools on the new simLiBk similarity
measure. Table 12 shows the results obtained by the simLiBk measure in the three
biomedical datasets using its best pre-processing configuration, and annotating the
sentences with all the combinations of NER tools. In addition, the aforementioned
table details the resulting p-values comparing the best-performing LiBlock-NER
combination with the combinations based on the other two NER tools.

LiBlock-cTAKES obtains the highest average harmonic score for the three datasets
among the LiBlock-NER combinations. However, it does not significantly outperform
LiBlock with no use of a NER tool. This conclusion can be drawn by looking at the
average column in table 12 and checking the p-values in the last column. This
conclusion is especially relevant because it shows that there is no a statistically
significant difference between using a NER tool like cTAKES or not using it in the
case of the LiBlock measure. We conjecture that this later conclusion could be caused
by two reasons, firstly the incapability of LiBlock to capture semantic relationships
beyond the synonymy, and secondly the current limitations of cTakes to recognize all
mentions of biomedical entities.

LiBlock-cTAKES obtains the highest Pearson correlation value in the BIOSSES
dataset among all LiBlock-NER combinations, whilst LiBlock with no use of a NER

May 19, 2022

31/48

tool obtains the highest Pearson correlation value in the MedSTS and CTR datasets,
respectively. This conclusion can be drawn by looking the results detailed in table 12.
LiBlock-cTAKES obtains the highest Spearman correlation value in the BIOSSES
and MedSTS datasets among the LiBlock-NER combinations, whilst LiBlock-cTAKES
and LiBlock-MetamapLite obtain the highest Spearman correlation value in the CTR
dataset. This conclusion can be drawn by looking the results detailed in table 12.

LiBlock-cTAKES obtains the highest harmonic correlation value in the BIOSSES

and MedSTS datasets among the LiBlock-NER combinations, whilst
LiBlock-MetamapLite obtains the highest harmonic correlation value in the CTR
dataset. This conclusion can be drawn by looking the results detailed in table 12.

Impact of the remaining pre-processing stages

This section analyzes the impact of each pre-processing step on the performance of the
sentence similarity methods, except for the NER tools already analyzed in the previous
section. Finally, we study the overall impact of the pre-processing configurations.

Impact of tokenization

The family of string-based methods obtains its best-performing results either by splitting
the sentence from the white spaces between words or using the Stanford CoreNLP
tokenizer. This conclusion can be drawn by looking at the table 7, which summarizes
the pre-processing tables detailed in Appendix B.

The family of ontology-based methods obtains its best-performing results in

combination with the Stanford CoreNLP tokenizer. This conclusion can be drawn by
looking at the table 7.

The family of methods based on embeddings obtains its best-performing results in

combination with the Stanford CoreNLP tokenizer, with the only exception of Flair
(M18). This conclusion can be drawn by looking at the table 7.

None method based on strings, ontologies, or embeddings obtain its best-performing
results in combination with the BioCNLPTokenizer. This conclusion can be drawn by
looking at the table 7. Thus, the BioCNLPTokenizer should not be used in
combination with any method in the former families in the task of biomedical sentence
similarity. On the other hand, we recall that all BERT-based methods evaluated
herein can only be used in combination with the WordPiece Tokenizer [90] based on a
subword segmentation algorithm, because it is required by the current BERT
implementations.

All families of methods show a strong preference by a specific tokenizer, with the
only exception of the string-based one. This conclusion can be drawn from previous
conclusions that confirm the preference of the methods based on ontologies and
embeddings by the CoreNLP tokenizer, and the mandatory use of the WordPiece
tokenizer by the family of BERT-based methods.

Impact of character filtering

The family of string-based methods obtains its best-performing results by using either
the BIOSSES char-filtering method or the default method which removes the
punctuation marks and special symbols from the sentences, with the only exception of
the Levenshtein distance method (M5), which does not remove special characters. This
conclusion can be drawn by looking at the table 7, which summarizes the
pre-processing tables detailed in Appendix B.

May 19, 2022

32/48

All ontology-based methods obtain their best-performing results in combination with

the BIOSSES char-filtering method. This conclusion can be drawn by looking at the
table 7.

Most of embeddings methods obtain their best-performing results in combination
with the default char filtering method. However, Flair (M18), BioWordVec (M26,M27),
and BioSentVec (M32) obtain their best-performing results with the BIOSSES
char-filtering method. This conclusion can be drawn by looking at the table 7.

The BERT-based methods do not show a noticeable preference pattern by a specific

char filtering method, obtaining their best-performing results with the BIOSSES,
Blagec2019, or the default one. This conclusion can be drawn by looking at the table 7.

Impact of stop-words removal

All string-based methods obtain their best-performing results in combination with the
NLTK2018 stop-word list, with the only exception of the Levenshtein distance (M5).
This conclusion can be drawn by looking at the table 7, which summarizes the
pre-processing tables detailed in Appendix B.

All ontology-based methods obtain their best-performing results in combination with

the NLTK2018 stop-word list, with the only exception of WBSM-J&C (M8),
WBSM-cosJ&C (M9), which do not remove stop words. This conclusion can be drawn
by looking at the table 7.

The methods based on embeddings do not show a noticeable preference pattern by a

specific stop-word list, obtaining their best-performing results by using the stop-word
list of BIOSSES, NLTK2018, or none. This conclusion can be drawn by looking at the
table 7.

The methods based on language models do not show a noticeable preference pattern

by a specific stop-word list, obtaining their best-performing results by using the
stop-word list of BIOSSES, NLTK2018, or none. This conclusion can be drawn by
looking at the table 7.

The best-performing results for the methods based on strings or ontologies show a
noticeable preference by the use of the stop-words list NLTK2018. This conclusion can
be drawn by looking at the table 7.

Impact of lower-casing

Only 10 of the 50 methods evaluated in this work obtain their best performance by
avoiding converting words to lowercase at the sentence pre-processing stage. This
conclusion can be drawn by looking at the tables 7 and 8, and the pre-processing
tables detailed in Appendix B. Moreover, these ten aforementioned methods obtain a
low performance in our experiments, with the only exception of the BioNLP2016win30
(M29) pre-trained model, which obtains the third best Spearman correlation value in
the CTR dataset. Thus, our experiments confirm that the lower-casing normalization
of the sentences positively impacts the performance of the methods, and it should be
considered as default option in any biomedical sentence similarity task.

We conjecture that lower-casing improves the performance of the families of

string-based and ontology-based methods because it improves the exact comparison of
words. On the other hand, we also conjecture that the impact of lower-casing the
sentences on the families of methods based on embeddings and language models
strongly depends on the pre-processing methods used in their training.

May 19, 2022

33/48

Overall impact of the pre-processing

To study the overall impact of the pre-processing stage on the performance of the
sentence similarity methods, we selected the configuration reporting the highest (best)
and lowest (worst) average harmonic score values for each method, as shown in table 9.
These configurations were selected from a total of 1081 pre-processing configurations
reported in Appendix B.

The best-performing methods of each family show a statistically significant

difference in performance between their best and worst pre-processing configurations.
This conclusion can be drawn by looking at the average (AVG) and the p-values in
table 9.

Answering RQ4. Our results and the conclusions above show that the
pre-processing configurations significantly impact the performance of the sentence
similarity methods, and thus, it should be specifically defined for each method. All
families of methods show a strong preference by a specific tokenizer, with the only
exception of the string-based one. In addition, the BioCNLPTokenizer does not
contribute to the best-performing configuration of any method evaluated herein. The
family of string-based methods shows a preference pattern of using either the
BIOSSES or default char filtering method, whilst all ontology-based methods use the
BIOSSES char filtering method, and most embedding methods use the default char
filtering method. However, BERT-based methods do not show a noticeable preference
pattern by a specific char filtering method. On the other hand, the families of string
and ontology-based methods show a noticeable preference pattern by the use of the
NLTK2018 stop-words list, whilst the families of embeddings and BERT-based
methods do not show a noticeable pattern. Finally, the experiments confirm that the
lower-casing normalization of the sentences positively impacts the performance of the
methods, and it should be considered as default option in any biomedical sentence
similarity task.

The new state-of-the-art

We set the new state of the art to answer our RQ1 and RQ2 questions as follows.

LiBlock (M4) measure sets the new state of the art for the sentence similarity task

in the biomedical domain (see table 8), being the best overall performing method to
tackle this later task. Moreover, LiBlock significantly outperforms all the methods
based on embeddings and language models. However, LiBlock cannot significantly
outperform the COM (M17) and WBSM-Rada (M7) ontology-based methods (see
Appendix A.1). Thus, LiBlock is a convincing but non-definitive winner among the
biomedical sentence similarity methods evaluated herein.

COM (M17) method sets the new state of the art among the family of

ontology-based methods for biomedical sentence similarity, being the best-performing
method in this later task (see table 8). COM significantly outperforms all methods
based on embeddings and BERT-based language models, as well as all string-based
and ontology-based methods with the only exception of LiBlock (M4) and
WBSM-Rada (M7) (see Appendix A.1).

BioWordVecint (M26) sets the new state of the art among the family of methods

based on pre-trained embedding models, being the best-performing method in this
later task (see table 8), and significantly outperforming the remaining methods in the
same family (see Appendix A.1).

OuBioBERT (M47) sets the new state of the art in among the family of methods
based on pre-trained BERT models, being the best-performing method in this later

May 19, 2022

34/48

task (see table 8). However, OuBioBERT is unable to outperform significantly all
remaining methods from the same family (see Appendix A.1).

Finally, our results show that our new string-based method, called LiBlock (M4),
obtains the best overall performing results, despite it does not capture the semantic
information of the sentences. This is a very noticeable finding because it contradicts a
common belief on the potential outperformance of the ontology-based methods
integrating word and concept semantics over the non-semantics methods in this
similarity task. A second and very noticeable finding is that our non-semantics and
non-ML LiBlock method is able to outperform significantly state-of-the-art methods
based on large ML models trained with the most recent and advanced word
embeddings [46] and BERT language models [85] in an unsupervised context. This
later finding is very remarkable because LiBlock is easy of implementing, easy of
evaluating, very efficient (2635 sentence pairs per second with no use of a NER tool),
and it requires neither large text resources nor complex algorithms for its training and
evaluation, which is a very clear advantage in the biomedical sentence similarity task.

Answering RQ1 and RQ2. The string-based method LiBlock (M4) obtains the
highest average harmonic score in all datasets, and significantly outperforms the
remaining string-based methods, as well as all methods based on embeddings and
BERT language models, and all the ontology-based methods with the only exceptions
of COM (M17) and WBSM-Rada (M7). In addition, LiBlock obtains the highest
Spearman correlation values in the BIOSSES and MedSTS datasets, which contains
100 and 1068 sentence pairs respectively.

Main drawbacks and limitations of current methods

This section analyzes the behaviour of the best-performing methods in each family of
sentence similarity methods to answer our RQ5. The best-performing methods of each
family, according to the harmonic average value reported in table 8, are LiBlock (M4),
COM (M17), BioWordVecint (M26), and OuBioBERT (M47).

String and ontology-based methods underestimate in average the human similarity
value in the BIOSSES and CTR datasets, whilst their average similarity error is close
to 0 in the MedSTS dataset. This conclusion can be drawn by looking at the average
similarity error values and the mean error values shown in figure 5 together with the
mean values shown in table 17. LiBlock and COM obtain mean error values of -0.021
and -0.001 in MedSTS, as shown in figure 5.b. On the other hand, both methods
report a mean similarity score much lower than the mean of the Human normalized
score in the BIOSSES and CTR datasets and a mean similarity score close to the
Human normalized score in the MedSTS dataset, as shown in table 17.

The methods based on embeddings and language models overestimate in average the
human similarity value in the three datasets. This conclusion can be drawn by looking
at the average similarity error values and the mean error values shown in figure 5,
together with the mean similarity values shown in table 17. The two aforementioned
families of methods report a mean similarity score much higher than the mean of the
Human normalized score in the three datasets, as show in table 17.

String and ontology-based methods share a similar underestimation behavior, in
opposition to the overestimation behaviour shown by the methods based on embeddings
and language models, which is very noticeable in the three datasets. This conclusion
can be drawn by looking at the minimum and maximum similarity values columns in
table 17, and the plots of the probability error distribution function for the three
datasets in figure 5. For instance, despite the human similarity scores are in the range
of 0 to 1 n the BIOSSES dataset, as shown in table 17, the string and ontology-based

May 19, 2022

35/48

methods report similarity scores in the range of 0 to 0.596, whilst the methods based
on embeddings and language models report similarity scores in the range of 0.582 to
0.987.

String and ontology-based methods tend to obtain their best results in sentences
with a Human normalized score close to 0, whilst the methods based on embeddings and
language models obtain their best results in sentences with a Human normalized score
close to 1. This conclusion can be drawn by looking at the tables 13, 14, 15 and 16.
On the other hand, string and ontology-based methods tend to obtain their worst
results in sentences with a Human normalized score close to 1, whilst the methods
based on embeddings and language models obtain their worst results in sentences with
a Human normalized score close to 0.

None of the methods for semantic similarity of sentences in the biomedical domain
evaluated herein use an explicit syntactic analysis or syntax information to obtain the
similarity value. We conjecture that syntactic analysis would improve the performance
in some cases. For instance, the sentences s1 and s2 with highest Esim in table 13
shows an implicit relation between the concepts ”miRNA” and ”oncogenesis”, which
should increase the final semantic similarity score of the sentences. However, none of
the methods evaluated herein consider and reward these semantics relationships
because its recognition demands some form of syntactic analysis. On the one hand,
string and ontology-based methods consider the concepts in a sentence as bags of
words, whilst on the other hand the methods based on embeddings and language
models implicitly consider the structure of the sentences but not the relationships
between the parts of the sentences that are related.

Our results show that the family of string-based methods is rewarded by the high
frequency of overlapping words in the sentences of the current biomedical datasets,
whilst the former methods are not able to deal properly with sentences that are
semantically different but not exhibit a word overlapping pattern. The main advantages
of the string-based methods are as follows: (1) they are able to obtain high correlation
values without the need of using external resources for their training or evaluation; (2)
they are fast and efficient; and finally; (3) they require low computational resources.
However, string-based methods are unable to capture the semantics of the words in
the sentence, which prevent them from recognizing semantic relationships, such as
synonymy, meronymy and morphological variants. On the other hand, the use of NER
tools in combination with string-based methods is a good option to integrate at least
the capability of recognizing synonyms, as shown by LiBlocK-CTakes (M4).

Ontology-based methods strongly depends on the lexical coverage of the ontologies
and the ability to recognize automatically the underlying concepts in sentences. Our
results show that the ontology-based methods are able to properly estimate a
similarity score when it is evaluated in a dataset with either high word overlapping or
NER and WSD tools that find all possible entities to properly calculate the similarity
between sentences. The main advantages of ontology-based methods are that they are
fast and require low computational resources. However, the effectiveness of the
ontology-based methods depends on the lexical coverage of the ontologies and the
ability of the NER and WSD tools to recognize the underlying concepts in sentences,
whose coverage and performance could be limited in several application domains.
The LiBlock (M4) string-based method and the COM (M17) ontology-based

method use a NER tool in the pre-processing stage to recognize the biomedical entities
(UMLS CUI codes) present in the input sentences. The objective of annotating entities
in the semantic similarity task is the identification and disambiguation of biomedical
concepts to provide semantic information to sentences. LiBlock uses the NER tool to
normalize and disambiguate the underlying concepts in a sentence, unifying different
concepts with acronyms and synonyms in the same CUI code and creating an

May 19, 2022

36/48

overlapping between concepts, while ontologies also make use of the similarity of
concepts within ontologies.

The biomedical NER tools evaluated in this work are unable to identify and
disambiguate correctly many biomedical concepts due to the use of acronyms and
different morphological variations, among others. For example, the CUI concepts
“KRAS gene” (C1537502), “BRAF gene” (C0812241), and “RAF1 gene” (C0812215)
in the sentences s1 and s2 with highest Esim obtained by the COM (M17) method in
table 14, appear as “K-ras”, “Braf”, “c-Raf” and “Craf’. However, cTakes is unable of
recognizing these later morphological variants of the same biomedical concepts. A
second example is the word “act” in the sentence “Consequently miRNAs have been
demonstrated to act either as oncogenes [...]”, which is wrongly recognized as the
entity “Activated clotting time measurement” (C0427611), rather than as a verb in
the sentence s1 with highest Esim in table 13. And finally, a third example is the
acronym “NSCLC”, which denotes the concept “Non-Small Cell Lung Carcinoma
(C0007131), which is not recognized in the plural variant “NSCLCs” in the sentence s2
with highest Esim from table 14.

The methods based on pre-trained embeddings and language models provide a
broader lexical coverage than the ontology-based methods, and do not need the use of
NER or WSD tools to find intrinsic semantic relationships between the words in the
sentences. However, these later methods need large corpus for their training, as well as
a complex training phase and more computational resources than the methods from
the families of string-based and ontology-based. On the other hand, our experiments
show that those methods tend to estimate higher similarity values than those
estimated by a human being in the three datasets. In most cases, the aforementioned
method report similarity scores that tend to 1, which indicates that the semantics
obtained from the sentences is not sufficient to compute correctly a similarity score.
For instance, the sentences s1 and s2 with highest Esim from tables 15 and 16 shows
similarity values close to 1, where the sentences have neither word overlapping nor
similar concepts, and the human similarity score is 0 in both cases. On the other hand,
BERT-based methods are trained for downstream tasks, using a supervised approach,
and do not perform well in an unsupervised context.

Answering RQ5. String-based methods capture neither the word semantics within
the sentences nor the semantic relationships between words, such as synonymy and
meronymy, and their effectiveness mainly relies on the word overlapping frequency in
the sentences. However, the LiBlock method uses the NER tool to normalize and
disambiguate the underlying concepts in a sentence, but unfortunately, it does not
significantly outperform LiBlock with no use of a NER tool, which could be caused by
two reasons as follows. Firstly, the incapability of LiBlock to capture semantic
relationships beyond the synonymy, and secondly the current limitations of cTakes to
recognize all mentions of biomedical entities. On the other hand, ontology-based
methods use NER and WSD tools to recognize the underlying concepts in the
sentences, which are not able to correctly identify and disambiguate these concepts in
many cases. In addition, they require external resources to capture the semantic
information from the sentences, which limits their lexical coverage. Thus,
ontology-based methods require both high word overlapping and high recognition
coverage of named entities to properly estimate the similarity between sentences. On
the other hand, the methods based on pre-trained embeddings and language models
need large corpus for training, a complex training phase, and considerable
computational resources to calculate the similarity between sentences. Moreover, those
methods tend to obtain high similarity scores in most cases, which may penalize them
in a balanced dataset and in a real environment. Finally, BERT-based methods are

May 19, 2022

37/48

trained for downstream tasks, using a supervised approach, and do not perform well in
an unsupervised context.

Comparison of running times

Table 18 details the running time reported by the best-performing methods for each
family, as well as the sentences per second that computes each method by average for
the three datasets evaluated herein. The experiments were executed in a desktop
computer with an AMD Ryzen 7 5800x CPU (16 cores) with 64 Gb RAM and 2TB
Gb SSD disk. In all the cases, the running time also comprises the pre-processing time
for each method. The string-based method Block Distance (M3) obtain the lowest
running times because it does not need complex mechanisms or pre-trained models to
calculate the similarity between sentences. On the other hand, the BERT-based
methods obtain the worst results mainly due to its pre-processing stage, which uses
the WordPiece tokenization method.

Table 18. This table shows the running times in miliseconds (ms) and the average
sentences pairs per second (sent/sec) reported by the best-performing method of each
family of methods in the evaluation of the 1339 sentence pairs that conform the three
datasets. (*) The LiBlock method reports the running times in both NER and noNER
versions showing that the efficiency of the method with no NER tool is much higher,
despite the fact that there is no statistically significant difference in the results
between both pre-processing configurations.

Method
LiBlock-cTAKES
LiBlock-noNER (*)
Block distance

ID
M4
M4
M3
M12 UBSM-Rada
M17 COM
M27 BioWordVecint
M32 BioSentVec
ouBioBERT
M47
BioBERT Large 1.1
(+ PubMed)

M38

Running time (ms)
56605
508
308
32341
41558
1211
54706
575770

Sentence pairs / sec
23,66
2635,83
4347,4
41,40
32,22
1105,69
24,48
2,33

3312566

0,40

Inconsistent results in the calculation of the statistical
significance matrix.

Despite the artificial increase of datasets to calculate the statistical significance of the
results, we have identified an inconsistent result with respect to the comparison of the
p-values of the LiBlock (M4) and the WBSM-Rada (M7) and UBSM-Rada (M12)
methods. Table 8 shows that the UBSM-Rada method (M12) has a higher average
harmonic score compared to WBSM-Rada (M7). However, by building the artificial
datasets, the value of UBSM-Rada (M12) with respect to LiBlock (M4) shows a
significant difference, while WBSM-Rada (M7) with respect to LiBlock (M4) shows a
non-significant difference. We conjecture that this problem could be solved by
increasing the number of datasets created for this task, which would allow to increase
the sample size and obtain more consistent results.

May 19, 2022

38/48

Conclusions and future work

We have introduced the largest, detailed, and for the first time, reproducible
experimental survey on biomedical sentence similarity reported in the literature. Our
work also introduces a collection of self-contained and reproducible benchmarks on
biomedical sentence similarity based on the same software platform, called
HESML-STS, which has been especially developed for this work, being provided as
part of the new HESML V2R1 version that will be made publicly available soon. We
provide a detailed reproducibility protocol [41] and dataset [42] to allow the exact
replication of all our experiments, methods, and results. In addition, we introduce a
new aggregated string-based sentence similarity method called LiBlock, together with
eight variants of the ontology-based methods introduced by Sogancioglu et al. [30],
and a new pre-trained word embedding model based on FastText [56] and trained on
the full-text of the articles in the PMC-BioC corpus [19]. We also evaluate for the first
time the CTR [51] dataset in a benchmark on biomedical sentence similarity.

The string-based LiBlock (M4) measure sets the new state-of-the-art for the

sentence similarity task in the biomedical domain and significantly outperforms all the
methods evaluated herein, with the only exception of the COM (M17) and
WBSM-Rada (M7) ontology-based methods. However, our data analysis shows that at
least with the three datasets evaluated herein, there is no statistically significant
difference between the performance of the LiBlock (M4) method using the cTakes or
none NER tool. Thus, using the LiBlock method without any NER tool could be a
competitive and much more efficient solution for high-throughput applications.

Concerning the impact of the Named Entity Recognition (NER) tools, our results

confirm that the choice of the best NER tool for each method significantly impacts
their performance. MetamapLite [93] and cTAKES [60] set the best-performing
configurations for the family of ontology-based methods, whilst Metamap [65] sets the
best-performing option for none.

Our experiments confirm that the pre-processing stage has a very significant
impact on the performance of the sentence similarity methods evaluated herein,
despite this fact have neither been studied nor reported in the literature. Thus, the
selection of the proper configuration for each sentence similarity method should be
confirmed experimentally. However, our experiments suggest some default
configurations to make these decisions, such as the use of lower-casing normalization,
some specific char filtering methods, and some specific tokenizers with the only
exception of BioCNLPTokenizer. Finally, the families of string and ontology-based
methods show a noticeable preference pattern by the use of the NLTK2018 stop-words
list. For a detailed description of the best pre-processing configurations, we refer the
readers to our discussion.

String-based methods do not capture either the semantics of the words in the
sentence or the semantic relationships between words, and their effectiveness relies on
the word overlapping frequency in the sentences. Ontology-based methods Named
Entity Recognition (NER) and Word Sense Disambiguation (WSD) tools to recognize
the underlying concepts in the sentences and require external resources to capture the
semantic information from the sentences, which limits their lexical coverage. In
addition, they require either high word overlapping or high recognition coverage of
named entities in order to properly calculate the similarity between sentences. On the
other hand, the methods based on pre-trained embeddings and language models need
a large corpus for training, a complex training phase, and considerable computational
resources to calculate the similarity between sentences. Moreover, these methods tend
to obtain high similarity scores in most cases, which may penalize them in a balanced
dataset and in a real environment. Finally, BERT-based methods are trained for
downstream tasks, using a supervised approach, and do not perform well in an

May 19, 2022

39/48

unsupervised context.

Our experiments suggest that the current benchmarks do not cover all the

language features that characterize the biomedical domain, such as the frequent use of
acronyms and rhetorical expressions like synonymy, meronymy, etc. In addition,
current benchmarks have a very limited sample size that difficult the analysis of
results. We conjecture that LiBlock, COM, and UBSM-Rada perform well because
there is a noticeable overlap of terms that may benefit the former methods over the
others reported in the literature. Furthermore, Chen et al. [104] highlights the need to
improve and create new benchmarks from different perspectives, to reflect the
multifaceted notion of the similarity of sentences. Therefore, we found a strong need
for improving existing benchmarks for the task of semantic similarity of sentences in
the biomedical domain.

As forthcoming activities, we plan to publish our new software release HESML
V2R1 including the HESML-STS software package developed for this work. We also
plan to evaluate the new sentence similarity methods introduced herein in a
benchmark for the general language domain. In addition, we will study the evaluation
of the sentence similarity methods in an extrinsic task, such as semantic medical
indexing [105] or summarization [106]. We also consider the evaluation of further
pre-processing configurations, such as biomedical NER systems based on recent Deep
Learning techniques [10], or extending our experiments and research to the
multilingual scenario by integrating multilingual biomedical NER systems like
Cimind [107]. Finally, we plan to evaluate some recent biomedical concept embeddings
based on MeSH [108], which has not been evaluated in the sentence similarity task yet.

Acknowledgments

We are grateful to Gizem Sogancioglu and Kathrin Blagec for answering kindly our
questions to replicate their methods and experiments, Fernando Gonz´alez and Juan
Corrales for setting up our reproducibility dataset, and Hongfang Liu and Yanshan
Wang for providing us the MedSTS dataset. UMLS CUI codes, SNOMED-CT US
ontology and MeSH thesaurus were used in our experiments by courtesy of the
National Library of Medicine of the United States.

Appendix A. The statistical significance results

We provide a series of tables reporting the p-values for each pair of methods evaluated
in this work as supplementary material.

Appendix B. The pre-processing raw output files

We provide all the pre-processing raw output tables for the experiments evaluated
herein as supplementary material

Appendix C. A reproducibility protocol and dataset
on the biomedical sentence similarity

We provide the reproducibility protocol published at protocols.io [41] as supplementary
material to allow the exact replication of all our experiments, methods, and results.

May 19, 2022

40/48

References

1. Tafti AP, Behravesh E, Assefi M, LaRose E, Badger J, Mayer J, et al. bigNN:
An open-source big data toolkit focused on biomedical sentence classification.
In: 2017 IEEE International Conference on Big Data (Big Data); 2017. p.
3888–3896.

2. Kim S, Kim W, Comeau D, Wilbur WJ. Classifying gene sentences in

biomedical literature by combining high-precision gene identifiers. In: Proc. of
the 2012 Workshop on Biomedical Natural Language Processing; 2012. p.
185–192.

3. Chen Q, Panyam NC, Elangovan A, Davis M, Verspoor K. Document triage
and relation extraction for protein-protein interactions affected by mutations.
In: Proc. of the BioCreative VI Workshop. vol. 6; 2017. p. 52–51.

4. Sarrouti M, Ouatik El Alaoui S. A passage retrieval method based on

probabilistic information retrieval model and UMLS concepts in biomedical
question answering. J Biomedical Informatics. 2017;68:96–103.

5. Kosorus H, B¨ogl A, K¨ung J. Semantic Similarity between Queries in QA

System using a Domain-specific Taxonomy. In: ICEIS (1); 2012. p. 241–246.

6. Ravikumar KE, Rastegar-Mojarad M, Liu H. BELMiner: adapting a

rule-based relation extraction system to extract biological expression language
statements from bio-medical literature evidence sentences. Database.
2017;2017(1).

7. Rastegar-Mojarad M, Komandur Elayavilli R, Liu H. BELTracker: evidence

sentence retrieval for BEL statements. Database. 2016;2016.

8. Du J, Chen Q, Peng Y, Xiang Y, Tao C, Lu Z. ML-Net: multi-label

classification of biomedical texts with deep neural networks. J Am Med Inform
Assoc. 2019;26(11):1279–1285.

9. Liu H, Hunter L, Keˇselj V, Verspoor K. Approximate subgraph

matching-based literature mining for biomedical events and relations. PLoS
One. 2013;8(4):e60954.

10. Hahn U, Oleynik M. Medical Information Extraction in the Age of Deep

Learning. Yearb Med Inform. 2020;29(1):208–220.

11. Kim SN, Martinez D, Cavedon L, Yencken L. Automatic classification of

sentences to support Evidence Based Medicine. BMC Bioinformatics. 2011;12
Suppl 2:5.

12. Hassanzadeh H, Groza T, Nguyen A, Hunter J. A supervised approach to

quantifying sentence similarity: with application to evidence based medicine.
PLoS One. 2015;10(6):e0129392.

13. Boyack KW, Newman D, Duhon RJ, Klavans R, Patek M, Biberstine JR, et al.
Clustering more than two million biomedical publications: comparing the
accuracies of nine text-based similarity approaches. PLoS One.
2011;6(3):e18029.

14. Dey S, Luo H, Fokoue A, Hu J, Zhang P. Predicting adverse drug reactions
through interpretable deep learning framework. BMC Bioinformatics.
2018;19(Suppl 21):476.

May 19, 2022

41/48

15. Lamurias A, Ruas P, Couto FM. PPR-SSM: personalized PageRank and
semantic similarity measures for entity linking. BMC Bioinformatics.
2019;20(1):534.

16. Aliguliyev RM. A new sentence similarity measure and sentence based

extractive technique for automatic text summarization. Expert Syst Appl.
2009;36(4):7764–7772.

17. Shang Y, Li Y, Lin H, Yang Z. Enhancing biomedical text summarization

using semantic relation extraction. PLoS One. 2011;6(8):e23862.

18. Allot A, Chen Q, Kim S, Vera Alvarez R, Comeau DC, Wilbur WJ, et al.

LitSense: making sense of biomedical literature at sentence level. Nucleic Acids
Res. 2019;.

19. Comeau DC, Wei CH, Islamaj Do˘gan R, Lu Z. PMC text mining subset in

BioC: about three million full-text articles and growing. Bioinformatics. 2019;.

20. Agirre E, Cer D, Diab M, Gonzalez-Agirre A. Semeval-2012 task 6: A pilot on
semantic textual similarity. In: * SEM 2012: The First Joint Conference on
Lexical and Computational Semantics–Volume 1: Proc. of the main conference
and the shared task, and Volume 2: Proc. of the Sixth International Workshop
on Semantic Evaluation (SemEval 2012). ACL; 2012. p. 385–393.

21. Agirre E, Cer D, Diab M, Gonzalez-Agirre A, Guo W. * SEM 2013 shared

task: Semantic textual similarity. In: Second Joint Conference on Lexical and
Computational Semantics (* SEM), Volume 1: Proc. of the Main Conference
and the Shared Task: Semantic Textual Similarity. vol. 1. ACL; 2013. p. 32–43.

22. Agirre E, Banea C, Cardie C, Cer D, Diab M, Gonzalez-Agirre A, et al.

Semeval-2014 task 10: Multilingual semantic textual similarity. In: Proc. of
the 8th international workshop on semantic evaluation (SemEval 2014). ACL;
2014. p. 81–91.

23. Agirre E, Banea C, Cardie C, Cer D, Diab M, Gonzalez-Agirre A, et al.

Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on
interpretability. In: Proc. of the 9th international workshop on semantic
evaluation (SemEval 2015). ACL; 2015. p. 252–263.

24. Agirre E, Banea C, Cer D, Diab M, others. Semeval-2016 task 1: Semantic

textual similarity, monolingual and cross-lingual evaluation. 10th International
Workshop on Semantic Evaluation (SemEval-2016). 2016;.

25. Cer D, Diab M, Agirre E, Lopez-Gazpio I, Specia L. SemEval-2017 Task 1:

Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation.
In: Proceedings of the 11th International Workshop on Semantic Evaluation
(SemEval-2017). Vancouver, Canada: Association for Computational
Linguistics; 2017. p. 1–14.

26. Wang Y, Afzal N, Liu S, Rastegar-Mojarad M, Wang L, Shen F, et al.
Overview of the BioCreative/OHNLP Challenge 2018 Task 2: Clinical
Semantic Textual Similarity. Proc of the BioCreative/OHNLP Challenge.
2018;2018.

27. Kalyan KS, Sangeetha S. SECNLP: A survey of embeddings in clinical natural

language processing. J Biomed Inform. 2020;101:103323.

May 19, 2022

42/48

28. Khattak FK, Jeblee S, Pou-Prom C, Abdalla M, Meaney C, Rudzicz F. A

survey of word embeddings for clinical text. Journal of Biomedical Informatics:
X. 2019;4:100057.

29. Alsentzer E, Murphy J, Boag W, Weng WH, Jindi D, Naumann T, et al.

Publicly Available Clinical BERT Embeddings. In: Proc. of the 2nd Clinical
Natural Language Processing Workshop. Minneapolis, Minnesota, USA:
Association for Computational Linguistics; 2019. p. 72–78.

30. Sogancioglu G, ¨Ozt¨urk H, ¨Ozg¨ur A. BIOSSES: a semantic sentence similarity

estimation system for the biomedical domain. Bioinformatics.
2017;33(14):49–58.

31. Blagec K, Xu H, Agibetov A, Samwald M. Neural sentence embedding models

for semantic similarity estimation in the biomedical domain. BMC
Bioinformatics. 2019;20(1):178.

32. Peng Y, Yan S, Lu Z. Transfer Learning in Biomedical Natural Language
Processing: An Evaluation of BERT and ELMo on Ten Benchmarking
Datasets. In: Proc. of the 18th BioNLP Workshop and Shared Task. Florence,
Italy: Association for Computational Linguistics; 2019. p. 58–65.

33. Chen Q, Peng Y, Lu Z. BioSentVec: creating sentence embeddings for

biomedical texts. In: 2019 IEEE International Conference on Healthcare
Informatics (ICHI). IEEE; 2019. p. 1–5.

34. Tawfik NS, Spruit MR. Evaluating Sentence Representations for Biomedical

Text: Methods and Experimental Results. J Biomed Inform. 2020; p. 103396.

35. Chen Q, Du J, Kim S, Wilbur WJ, Lu Z. Deep learning with sentence

embeddings pre-trained on biomedical corpora improves the performance of
finding similar sentences in electronic medical records. BMC Medical
Informatics and Decision Making. 2020;20(1):73.
doi:https://doi.org/10.1186/s12911-020-1044-0.

36. Breiman L. Random Forests. Machine Learning. 2001;45(1):5–32.

37. Lara-Clares A, Lastra-D´ıaz JJ, Garcia-Serrano A. Protocol for a reproducible

experimental survey on biomedical sentence similarity. PLoS One.
2021;16(3):e0248663.

38. Lastra-D´ıaz JJ, Garc´ıa-Serrano A, Batet M, Fern´andez M, Chirigati F.

HESML: a scalable ontology-based semantic similarity measures library with a
set of reproducible experiments and a replication dataset. Information Systems.
2017;66:97–118.

39. Lastra-D´ıaz JJ, Lara-Clares A, Garcia-Serrano A. HESML: a real-time

semantic measures library for the biomedical domain with a reproducible
survey. BMC Bioinformatics. 2022;23(1):23.

40. Lara-Clares A, Lastra Diaz JJ, Garcia Serrano A. Reproducible experiments

on word and sentence similarity measures for the biomedical domain; 2022.
Available from: https://doi.org/10.21950/EPNXTR.

41. Lara-Clares A, Lastra-D´ıaz JJ, Garcia-Serrano A. A reproducibility

protocol and dataset on the biomedical sentence similarity; 2022. Available from:
https://www.protocols.io/view/a-reproducibility-protocol-and-dataset-on-the-biom-b5ckq2uw.

May 19, 2022

43/48

42. Lara-Clares A, Lastra Diaz JJ, Garcia Serrano A. Reproducible experiments

on word and sentence similarity measures for the biomedical domain; 2022.
Available from: https://doi.org/10.21950/EPNXTR.

43. Lastra-D´ıaz JJ, Garc´ıa-Serrano A. A new family of information content models

with an experimental survey on WordNet. Knowledge-Based Systems.
2015;89:509–526.

44. Lastra-D´ıaz JJ, Garc´ıa-Serrano A. A novel family of IC-based similarity
measures with a detailed experimental survey on WordNet. Engineering
Applications of Artificial Intelligence Journal. 2015;46:140–153.

45. Lastra-D´ıaz JJ, Garc´ıa-Serrano A. A refinement of the well-founded

Information Content models with a very detailed experimental survey on Word-
Net. ETSI Inform´atica. Universidad Nacional de Educaci´on a Distancia (UNED).
http://e-spacio.uned.es/fez/view/bibliuned:DptoLSI-ETSI-Informes-Jlastra-refinement;
2016. TR-2016-01.

46. Lastra-Diaz JJ, Goikoetxea J, Hadj Taieb MA, Garc´ıa-Serrano A, Ben Aouicha
M, Agirre E. A reproducible survey on word embeddings and ontology-based
methods for word similarity: Linear combinations outperform the state of the
art. Engineering Applications of Artificial Intelligence. 2019;85:645 – 665.

47. Lastra-D´ıaz JJ, Garc´ıa-Serrano A. WordNet-based word similarity

reproducible experiments based on HESML V1R1 and ReproZip; 2016.
Mendeley Data, v1. http://doi.org/10.17632/65pxgskhz9.1.

48. Lastra-D´ıaz JJ, Goikoetxea J, Hadj Taieb MA, Garc´ıa-Serrano A, Aouicha MB,
Agirre E. Reproducibility dataset for a large experimental survey on word
embeddings and ontology-based methods for word similarity. Data in Brief.
2019;26:104432.

49. Lastra-D´ıaz JJ, Goikoetxea J, Hadj Taieb M, Garc´ıa-Serrano A, Ben Aouicha

M, Agirre E, et al. A large reproducible benchmark of ontology-based methods
and word embeddings for word similarity. Information Systems.
2021;96:101636.

50. Wang Y, Afzal N, Fu S, Wang L, Shen F, Rastegar-Mojarad M, et al. MedSTS:
a resource for clinical semantic textual similarity. Language Resources and
Evaluation. 2018; p. 1–16.

51. Lithgow-Serrano O, Gama-Castro S, Ishida-Guti´errez C, Mej´ıa-Almonte C,
Tierrafr´ıa VH, Mart´ınez-Luna S, et al. Similarity corpus on microbial
transcriptional regulation. Journal of Biomedical Semantics. 2019;10(1):8.

52. Lithgow-Serrano O, Gama-Castro S, Ishida-Guti´errez C, Collado-Vides J.

L-Regulon: A novel soft-curation approach supported by a semantic enriched
reading for RegulonDB literature. bioRxiv.
2020;doi:10.1101/2020.04.26.062745.

53. Gerlach M, Shi H, Amaral LAN. A universal information theoretic approach to

the identification of stopwords. Nature Machine Intelligence.
2019;1(12):606–612.

54. Li Y, McLean D, Bandar ZA, James DO, Crockett K. Sentence Similarity

Based on Semantic Nets and Corpus Statistics. IEEE Trans Knowl Data Eng.
2006;18(8):1138–1150.

May 19, 2022

44/48

55. Krause EF. Taxicab Geometry: An Adventure in Non-Euclidean Geometry.

Online: Courier Corporation; 1986.

56. Bojanowski P, Grave E, Joulin A, Mikolov T. Enriching Word Vectors with
Subword Information. Transactions of the Association for Computational
Linguistics. 2017;5:135–146.

57. Song B, Li F, Liu Y, Zeng X. Deep learning methods for biomedical named
entity recognition: a survey and qualitative comparison. Brief Bioinform.
2021;22(6).

58. Miller GA. WordNet: A Lexical Database for English. ACM.

1995;38(11):39–41.

59. Donnelly K. SNOMED-CT: The advanced terminology and coding system for

eHealth. Books Google. 2006;121:279–290.

60. Savova GK, Masanz JJ, Ogren PV, Zheng J, Sohn S, Kipper-Schuler KC, et al.
Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES):
architecture, component evaluation and applications. J Am Med Inform Assoc.
2010;17(5):507–513.

61. Dijkstra EW. A note on two problems in connexion with graphs. Numerische

Mathematik. 1959;1(1):269–271.

62. Johnson AEW, Pollard TJ, Shen L, Lehman LWH, Feng M, Ghassemi M, et al.

MIMIC-III, a freely accessible critical care database. Sci Data. 2016;3:160035.

63. Mikolov T, Sutskever I, Chen K, Corrado GS, others. Distributed

representations of words and phrases and their compositionality. Adv Neural
Inf Process Syst. 2013;.

64. Pennington J, Socher R, Manning C. Glove: Global vectors for word

representation. In: Proc. of the 2014 conference on empirical methods in
natural language processing (EMNLP). ACL Web; 2014. p. 1532–1543.

65. Aronson AR, Lang FM. An overview of MetaMap: historical perspective and

recent advances. J Am Med Inform Assoc. 2010;17(3):229–236.

66. S´anchez D, Batet M, Isern D. Ontology-based information content
computation. Knowledge-Based Systems. 2011;24(2):297–303.

67. Cai Y, Zhang Q, Lu W, Che X. A hybrid approach for measuring semantic
similarity based on IC-weighted path distance in WordNet. Journal of
intelligent information systems. 2017; p. 1–25.

68. Rada R, Mili H, Bicknell E, Blettner M. Development and application of a

metric on semantic nets. IEEE Transactions on Systems, Man, and
Cybernetics. 1989;19(1):17–30.

69. Jiang JJ, Conrath DW. Semantic similarity based on corpus statistics and
lexical taxonomy. In: Proc. of International Conference Research on
Computational Linguistics (ROCLING X); 1997. p. 19–33.

70. Chapman S, Norton B, Ciravegna F. Armadillo: Integrating knowledge for the

semantic web. In: Proceedings of the Dagstuhl Seminar in Machine Learning
for the Semantic Web. Researchgate; 2005. p. 90.

May 19, 2022

45/48

71. Ukkonen E. Approximate string-matching with q-grams and maximal matches.

Theor Comput Sci. 1992;92(1):191–211.

72. Jaccard, P . Nouvelles recherches sur la distribution florale. Bull Soc Vaud sci

nat. 1908;44:223–270.

73. Manning CD, Manning CD, Sch¨utze H. Foundations of Statistical Natural

Language Processing. Online: MIT Press; 1999.

74. Levenshtein VI. Binary codes capable of correcting deletions, insertions, and
reversals. In: Soviet physics doklady. vol. 10. Springer; 1966. p. 707–710.

75. Lawlor LR. Overlap, Similarity, and Competition Coefficients. Ecology.

1980;61(2):245–251.

76. Akbik A, Blythe D, Vollgraf R. Contextual String Embeddings for Sequence

Labeling. In: Proc. of the 27th International Conference on Computational
Linguistics. Santa Fe, New Mexico, USA: Association for Computational
Linguistics; 2018. p. 1638–1649.

77. Pyysalo S, Ginter F, Moen H, Salakoski T, Ananiadou S. Distributional

semantics resources for biomedical text processing. Proc of LBM. 2013; p.
39–44.

78. Chen Q, Lee K, Yan S, Kim S, Wei CH, Lu Z. BioConceptVec: Creating and

evaluating literature-based biomedical concept embeddings on a large scale.
PLOS Computational Biology. 2020;16(4):1–18.
doi:10.1371/journal.pcbi.1007617.

79. Newman-Griffis D, Lai A, Fosler-Lussier E. Insights into Analogy Completion

from the Biomedical Domain. In: BioNLP 2017. Vancouver, Canada,:
Association for Computational Linguistics; 2017. p. 19–28.

80. Zhang Y, Chen Q, Yang Z, Lin H, Lu Z. BioWordVec, improving biomedical

word embeddings with subword information and MeSH. Sci Data. 2019;6(1):52.

81. Chiu B, Crichton G, Korhonen A, Pyysalo S. How to Train good Word
Embeddings for Biomedical NLP. In: Proc. of the 15th Workshop on
Biomedical Natural Language Processing. Berlin, Germany: Association for
Computational Linguistics; 2016. p. 166–174.

82. Cer D, Yang Y, Kong Sy, Hua N, Limtiaco N, St John R, et al. Universal
Sentence Encoder for English. In: Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing: System Demonstrations.
Brussels, Belgium: Association for Computational Linguistics; 2018. p.
169–174.

83. Pagliardini M, Gupta P, Jaggi M. Unsupervised Learning of Sentence

Embeddings Using Compositional n-Gram Features. In: Proc. of the 2018
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long
Papers). New Orleans, Louisiana: Association for Computational Linguistics;
2018. p. 528–540.

84. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained

biomedical language representation model for biomedical text mining.
Bioinformatics. 2019;36(4):1234–1240. doi:10.1093/bioinformatics/btz682.

May 19, 2022

46/48

85. Devlin J, Chang M, Lee K, Toutanova K. BERT: Pre-training of Deep

Bidirectional Transformers for Language Understanding. In: Burstein J, Doran
C, Solorio T, editors. Proc. of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT, (Long and Short Papers). Minneapolis, MN, USA:
Association for Computational Linguistics; 2019. p. 4171–4186. Available from:
https://doi.org/10.18653/v1/n19-1423.

86. Beltagy I, Lo K, Cohan A. SciBERT: A Pretrained Language Model for

Scientific Text. In: Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China:
Association for Computational Linguistics; 2019. p. 3615–3620.

87. Huang K, Altosaar J, Ranganath R. ClinicalBERT: Modeling Clinical Notes

and Predicting Hospital Readmission. arXiv e-prints. 2019; p.
arXiv:1904.05342.

88. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain-Specific
Language Model Pretraining for Biomedical Natural Language Processing.
arXiv e-prints. 2020; p. arXiv:2007.15779.

89. Wada S, Takeda T, Manabe S, Konishi S, Kamohara J, Matsumura Y. A

pre-training technique to localize medical BERT and to enhance biomedical
BERT. arXiv e-prints. 2020; p. arXiv:2005.07202.

90. Wu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W, et al. Google’s

Neural Machine Translation System: Bridging the Gap between Human and
Machine Translation. arXiv. 2016;.

91. Manning C, Surdeanu M, Bauer J, Finkel J, Bethard S, McClosky D. The
Stanford CoreNLP natural language processing toolkit. In: Proc. of 52nd
annual meeting of the association for computational linguistics: system
demonstrations. ACL; 2014. p. 55–60.

92. Comeau DC, Islamaj Do˘gan R, Ciccarese P, Cohen KB, Krallinger M, Leitner
F, et al. BioC: a minimalist approach to interoperability for biomedical text
processing. Database. 2013;2013:bat064.

93. Demner-Fushman D, Rogers WJ, Aronson AR. MetaMap Lite: an evaluation

of a new Java implementation of MetaMap. J Am Med Inform Assoc.
2017;24(4):841–844.

94. Bodenreider O. The Unified Medical Language System (UMLS): integrating
biomedical terminology. Nucleic Acids Res. 2004;32(Database issue):267–70.

95. Lastra-D´ıaz JJ, Lara-Clares A, Garcia-Serrano A. HESML V1R5 Java software
library of ontology-based semantic similarity measures and information content
models; 2020. e-cienciaDatos, v1. https://doi.org/10.21950/1RRAWJ.
Available from: https://doi.org/10.21950/1RRAWJ.

96. Smith L, Rindflesch T, Wilbur WJ. MedPost: a part-of-speech tagger for

bioMedical text. Bioinformatics. 2004;20(14):2320–2321.

97. Re´ategui R, Ratt´e S. Comparison of MetaMap and cTAKES for entity

extraction in clinical notes. BMC Med Inform Decis Mak. 2018;18(Suppl 3):74.

May 19, 2022

47/48

98. Bird S, Klein E, Loper E. Natural Language Processing with Python:

Analyzing Text with the Natural Language Toolkit. O’Reilly Media, Inc.; 2009.

99. Shen D, Wang G, Wang W, Min MR, Su Q, Zhang Y, et al. Baseline Needs

More Love: On Simple Word-Embedding-Based Models and Associated Pooling
Mechanisms. In: Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers). Melbourne, Australia:
Association for Computational Linguistics; 2018. p. 440–450.

100. Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al. Tensorflow: A
system for large-scale machine learning. In: 12th USENIX symposium on
operating systems design and implementation OSDI 16). usenix.org; 2016. p.
265–283.

101. Xiao H. bert-as-service; 2018.

https://github.com/hanxiao/bert-as-service.

102. Lara-Clares A, Lastra-D´ıaz JJ, Garcia-Serrano A. A reproducibility protocol

and dataset on the biomedical sentence similarity. To be submitted to PlosOne
journal as a Lab Protocol article. 2022;.

103. Lara-Clares A, Lastra-D´ıaz JJ, Garcia-Serrano A. HESML Java software
library of semantic similarity measures for the biomedical domain. To be
submitted. 2020;.

104. Chen Q, Rankine A, Peng Y, Aghaarabi E, Lu Z. Benchmarking Effectiveness
and Efficiency of Deep Learning Models for Semantic Textual Similarity in the
Clinical Domain: Validation Study. JMIR Medical Informatics.
2021;9(12):e27386.

105. Couto FM, Krallinger M. Proposal of the First International Workshop on

Semantic Indexing and Information Retrieval for Health from Heterogeneous
Content Types and Languages (SIIRH). In: Advances in Information Retrieval.
Springer International Publishing; 2020. p. 654–659.

106. Mishra R, Bian J, Fiszman M, Weir CR, Jonnalagadda S, Mostafa J, et al.

Text summarization in the biomedical domain: a systematic review of recent
research. J Biomed Inform. 2014;52:457–467.

107. Cabot C, Darmoni S, Soualmia LF. Cimind: A phonetic-based tool for

multilingual named entity recognition in biomedical texts. J Biomed Inform.
2019;94:103176.

108. Abdedda¨ım S, Vimard S, Soualmia LF. The MeSH-Gram Neural Network
Model: Extending Word Embedding Vectors with MeSH Concepts for
Semantic Similarity. In: Ohno-Machado L, S´eroussi B, editors. MEDINFO
2019: Health and Wellbeing e-Networks for All - Proceedings of the 17th
World Congress on Medical and Health Informatics. vol. 264 of Studies in
Health Technology and Informatics. IOS Press; 2019. p. 5–9.

May 19, 2022

48/48

