Preprint

Visual Notations in Container Orchestrations:

An Empirical Study with Docker Compose

Bruno Piedade · João Pedro Dias · Filipe F. Correia

2
2
0
2

l
u
J

9
1

]
E
S
.
s
c
[

1
v
7
6
1
9
0
.
7
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract

we empirically evaluated it in a controlled experiment

Context. Container orchestration tools supporting

infrastructure-as-code allow new forms of collaboration

between developers and operatives. Still, their text-

based nature permits naive mistakes and is more

diﬃcult to read as complexity increases. We can ﬁnd

few examples of low-code approaches for deﬁning the

orchestration of containers, and there seems to be

with novice developers.

Results. The results show a signiﬁcant reduction in

development time and error-proneness when deﬁning

Docker Compose ﬁles, supporting our hypothesis. The

participants also thought the prototype easier to use

and useful, and wanted to use it in the future.

a lack of empirical studies showing the beneﬁts and

1 Introduction

limitations of such approaches.

Goal & method. We hypothesize that a complete

visual notation for Docker-based orchestrations could

reduce the eﬀort, the error rate, and the development

time. Therefore, we developed a tool featuring such a

visual notation for Docker Compose conﬁgurations, and

Bruno Piedade

Faculty of Engineering, University of Porto.

E-mail: up201505668@fe.up.pt

João Pedro Dias

Faculty of Engineering, University of Porto. BUILT CoLAB.

E-mail: jpmdias@fe.up.pt

Filipe F. Correia

The concept of infrastructure-as-code (IaC) pertains

to the management of infrastructure (i.e., hardware,

software and network resources) through conﬁguration

ﬁles within a code-base [1]. Early tools to support this

practice focused on bare-metal infrastructure [2,3] but

the notion later expanded to the management and

provisioning of infrastructure resources on the cloud [1,

4]. Containers, and Docker in particular, rely on IaC to

allow developers to fully specify runtime environments [5,

6], in a much more lightweight way than virtual machines

allow it [7,8]. Despite the interest given by industry and

research to IaC topics in the last few years, there is a

Faculty of Engineering, University of Porto. INESC TEC.

consensus that the best practices for developing and

E-mail: ﬁlipe.correia@fe.up.pt

maintaining IaC are still weakly established [9].

 
 
 
 
 
 
2

Bruno Piedade et al.

Docker Compose is a tool for orchestrating multiple

than the usual text-based computer programming. We

containers using Docker.

It supports deﬁning an

ﬁnd such approaches useful for diﬀerent purposes and

orchestration through a YAML ﬁle, by which it

domains, such as in manufacturing, where it is often

conﬁgures the containers of the application, the

used for conﬁguring programmable logic controllers

corresponding images and how they are related to

(PLC) via ladder and sequential function charts [13,

each other, the volumes for data persistence, and

14]. In software engineering, visual notations like the

the networks for connecting the containers. The

Uniﬁed Modeling Language (UML) are reasonably

containers can then be run conventionally through the

well known and adopted [15]. More recent applications

command-line interface (CLI), resulting in the creation

of visual approaches exist for educational purposes,

or execution of the declared resources. The YAML

and in the area of Internet-of-Things (IoT) [16,17,

ﬁles used by Docker Compose follow a well-deﬁned

18]. There are also examples of such approaches in

format named Compose Speciﬁcation that claims to be

the operations ﬁeld, such as for managing cloud and

a “developer-focused standard for deﬁning cloud and

container resources—some of them focusing speciﬁcally

platform agnostic container-based applications” [10].

on Docker technologies [19].

The Compose Speciﬁcation resulted from unifying the

ﬁle formats of versions 2.x and 3.x of Docker Compose.

The speciﬁcation supports the deﬁnition of services,

networks, volumes, conﬁgs, and secrets.

We hypothesize that a complete visual approach

has the potential to be useful for a broad audience

of end-users ranging from ﬁrst-time developers who

beneﬁt from some support in understanding how the

The process of setting up orchestration ﬁles for

technology works to more experienced users who might

simple systems is reasonably straightforward, but the

take advantage of the visualization aspects to have a

textual nature of these ﬁles may become challenging

clearer overview of complex conﬁgurations.

as the complexity of the system increases, due to the

number or the heterogeneity of the containers. In

In the work reported in this article, we empirically

such cases, we expect that understanding container

evaluate a low-code approach for container orchestration.

dependencies becomes diﬃcult, as related deﬁnitions

We expand on the work presented at the LowCode 2020

begin to get further apart within the ﬁle. Also,

workshop [20], providing a detailed review of the most

advanced conﬁguration aspects, such as port mapping

relevant related works and a more detailed account of

and volume management, might be confusing for

the empirical study and of the discussion of the results.

inexperienced users. Additionally, developing such ﬁles

by trial-and-error seems to be common [11], and there

is some evidence that misconﬁgurations in IaC scripts

are a real concern [12].

Next, in this article, we start by presenting our

research goals and methodology (cf. Section 2) and

discuss relevant related works (cf. Section 3). Then

Section 4 describes our approach, and Sections 5 and 6

Low-code approaches to software development enable

the empirical study and respective results. We end with

the visual development of applications, allowing to

a discussion of the validation threats and some closing

create software through a graphical user interface rather

remarks (cf. Sections 7 and 8).

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

3

2 Research Goals and Methodology

RQ2 To what extent does a visual notation for

The purpose of this work is to explore the beneﬁts

of low-code for the development of docker compose

orchestration ﬁles. In particular, the hypothesis that

guides this work is that a complete visual notation for

developing orchestration ﬁles can improve the overall

developer experience and reduce the error proneness and

development time.

By a complete visual notation, we mean a way

to visually express all the elements of an orchestration

ﬁle that are supported by its text notation. This

includes elements such as containers, volumes, networks,

conﬁgs, and secrets, as well as diﬀerent relationships

and dependencies.

By orchestration ﬁles we mean the descriptions

of what containers make a given system, and how they

depend on each other and on infrastructure resources.

For the scope of this work we consider the Docker

Compose Speciﬁcation [10].

By developer experience, we mean the overall

ease-of-use and intuitiveness of the full experience,

considering the steps and actions needed to successfully

specify a container orchestration setup.

By error proneness and development time, we

respectively mean the number of errors and execution

attempts, and the time required to successfully specify

a container orchestration setup.

(Docker) orchestration ﬁles reduce the number of

errors?

We aim to understand if a visual notation is

truly useful in reducing error proneness while

orchestrating a Docker Compose ﬁle.

RQ3 What is the perception of developers towards

using a visual notation for the orchestration of

(Docker) orchestration ﬁles?

We aim to understand if a visual notation is

perceived by the developers as enjoyable and

useful, and if they show intention of using it

again in the future after being exposed to it.

To ﬁnd answers to these research questions we

start by surveying existing visual approaches

for managing and orchestrating container and

infrastructure resources (cf. Section 3), seeking to

ﬁnd works that come closest to providing insights to

our research questions.

Next, we develop a tool prototype that oﬀers a

low-code environment, with a complete visual notation

for developing orchestration ﬁles for Docker Compose

(cf. Section 4). We name this tool Docker Composer, and

we use it to empirically evaluate the beneﬁts of using

such a visual notation. More speciﬁcally, we conduct a

controlled experiment with novice software developers,

where we gather performance and perception-based

metrics, and compare the use of a visual notation with

Given this, we consider the research questions:

the conventional text-based one (cf. Section 5).

RQ1 To what extent does a visual notation for the

orchestration of (Docker) containers reduce the

3 Related Work

development time?

As we sought to propose and evaluate a visual approach

We aim to understand if a visual notation is truly

for developing container orchestration ﬁles, we evaluated

useful in reducing the time of development of a

related works employing visual tools for managing and

Docker Compose ﬁle.

orchestrating containers. These tools allow to deﬁne or

4

Bruno Piedade et al.

inspect Docker resources, locally or remotely, usually

as creating and deleting containers, and a few container

for development purposes. Such resources can include

monitoring utilities, including performance graphs.

containers, images, volumes, and networks. Many times,

they work as wrappers for Docker’s CLI commands,

from common functions such as container creation and

deletion to the orchestration of containers.

Albeit less thoroughly, we also review tools to

visually manage and orchestrate infrastructure, as they

address a closely related domain and could support

useful additional insights. Such infrastructure resources

can include physical (i.e., hardware and facilities),

virtual machines, and network resources.

We can instantiate a new project by creating a

new or loading an existing docker-compose.yml ﬁle.

We then visualize the overall containers scheme. The

containers and relationships are represented in a

graph-like diagram, and we can edit diﬀerent aspects

through form ﬁelds, such as environment variables,

volumes, and ports. Changes are reﬂected in an

underlying docker-compose.yml ﬁle, which can also

be seen in its text form within the editor, in a diﬀerent

tab. In the same way, changes to this text form will be

We started our analysis by using Google Scholar

reﬂected on the diagram.

and, upon realizing the low number of research

works addressing visual approaches for container and

infrastructure resources, extended our review to other

visual tools that are readily available to practitioners by

querying Google and GitHub. We based the analysis of

these tools primarily on their available documentation

and manually installed them when necessary.

3.1 Visual Tools for Managing and Orchestrating

Containers and Infrastructure

The next paragraphs brieﬂy describe the tools that

we have surveyed. We sought to identify capabilities

and limitations of these tools and, for those supporting

the management and orchestration of containers, we

assessed the extent to which they support a complete

Fig. 1 showcases the scheme perspective in the

project tab for a simple Compose orchestration ﬁle

and some additional UI elements. The top action bar

includes quick actions which trigger Docker Compose

commands such as docker-compose up for the start

button. In the scheme itself, the boxes represent

the containers and the dotted arrows represent the

depends_on relation between the containers. To add

a container to the scheme, the user can drag the

intended image from the palette, to the left, to the

scheme area, to the right. In the current version, it

is not possible to visually add dependencies between

containers (i.e., depends_on), requiring the user to

instead use the editor and add the dependency textually

in the docker-compose.yml ﬁle.

visual notation (cf. Section 2).

This tool stands as one of the closest to the one we

DockStation1 seems to be considerably adopted,

propose in this work, although a few limitations stand

with over 1.8K stars on GitHub as of January of 2022. It

out: (a) volumes are speciﬁed via a form and have

provides a native GUI for handling Docker containers in

no visual notation; (b) dependencies are represented

local and remote environments and is aimed primarily at

visually but they can only be deﬁned through the

development. It supports container management, such

text editor; (c) conﬁgs and secrets are not represented

1 DockStation, available at https://dockstation.io/

visually and can only be deﬁned through the text

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

5

Fig. 1: A simple docker-compose conﬁguration in DockStation.

editor; and (d) both visual and textual representations

volumes, networks, and closures 4. Each can be created

are available within the tool but one cannot view or

and conﬁgured via a form-based user interface. It is

interact with both simultaneously. The user has to

then possible to visually connect each container or

explicitly click on a save button in the editor for the

closure with a network or volume by click-and-dragging

changes to persist before switching.

the mouse pointer from the source to the target

Admiral2 seems to have a smaller user base

than DockStation, with over 252 stars on GitHub

as of January 20223. It oﬀers a web-based GUI for

container management and provisioning over a cluster

of

infrastructure. Unlike DockStation, this tool

is

mainly deployment and production-oriented. Besides

the provisioning of single containers,

it supports

orchestrating containers by the deﬁnition of templates.

These include four main components: containers,

2 Admiral, by VMware, available at https://github.com/

component. Each template can be directly provisioned

to a conﬁgured cluster or exported in one of two

formats—YAML Blueprints5 and Docker Compose

ﬁles—and it is also possible to import from these ﬁle

formats to visualize and edit the orchestration.

Fig. 2 displays a simple template containing 3

containers, 2 networks, and 1 volume. The user can

add a new component (container, network, volume, or

closure) by hovering over the empty box with the plus

icon and clicking on the desired element. Upon which

they are redirected to the corresponding form to edit

vmware/admiral

4 Closures are a notion supported by VMWare tools, and not

3 As of January of 2022 the project is marked as archived

an oﬃcial feature of the Docker Compose speciﬁcation.

on GitHub, suggesting that no future developments are to be

5 YAML Blueprints is a format speciﬁed by VMWare and used

expected.

by tools provided by the company, such as vRealize Automation.

6

Bruno Piedade et al.

Fig. 2: Sample of Admiral’s template visual orchestrator.

its properties. Each container has a set of network and

they are lost (with no warning) when exporting an

volume anchor points, located at the bottom, for the

orchestration to a docker-compose.yml ﬁle, possibly

total number of networks and volumes declared in the

giving a false sense of what developers may expect to

conﬁguration (3 in this instance). These allow the user

be actually building with the tool.

to attach the containers to their corresponding volumes

Docker Studio6 and its predecessor, Docker

and networks. Dependencies between containers, known

as links, are displayed and directly editable as properties

of a container, within its box. To edit more advanced

properties, the user must expand the container and

access its full edit form.

Four limitations stand out in Admiral: (a) like in

DockStation, conﬁgs and secrets cannot be visually

represented; (b) the dependencies between containers

(depends_on and links) can be speciﬁed via a drop-

down, but are not represented as lines connecting the

containers; (c) the user is not allowed to rearrange

any of visual elements, with the exception of network

connections; and (d) some elements supported by

Designer [21], do not seem to be in common use by

professionals. The tool is an Eclipse-based prototype,

oﬀering a native GUI. It employs a model-driven

approach to address deployment and maintenance in

production environments. Its user interface is shown

in Fig. 3. It features a palette on its right side that

allows conﬁguring diﬀerent container and infrastructure

elements. To the center, there is a design area that

provides a graphical representation of model. The tool

allows to visually establish the dependencies between

containers and represent their volumes and networks.

Docker Studio allows also to run or stop containers

in their execution environment. Green and red colors

Admiral are speciﬁc to VMWare tools (e.g., closures)

6 Docker Studio is available at http://occiware.github.io/

but not part of the Docker Compose speciﬁcation, and

content/user-guides/snapshot/connector-docker.html.

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

7

represent respectively resources that are in running or

in stopped states.

Limitations of

this

tool

include:

(a)

like in

DockStation and Admiral, conﬁgs and secrets cannot

be deﬁned visually; and (b) there is no way of accessing

Docker’s output, which can become a concern when

trying to troubleshoot any issue that might arise.

Fig. 4: Sample of CodeHerent’s user interface.

Visual Composer8 resembles CodeHerent but

focuses speciﬁcally on AWS EC2 CloudFormation

templates. It uses a web-based GUI, representing

infrastructure artifacts following a tree-like diagram and

oﬀering multiple types of connections, such as arrows

for dependencies and references between resources. The

user can add a node as a descendent of another and edit

its properties in a form-based interface. Furthermore, it

Fig. 3: Sample of Docker Designer’s user interface,

includes snippets of documentation directly accessible

adapted from Paraiso et al. [21].

CodeHerent7 is a web-based visual development

environment, leveraging a hybrid visual programming

language (VPL) for editing and visualizing Terraform

conﬁguration ﬁles. Unlike DockStation, Admiral, and

Docker Studio, it does not address container resources.

A sample of its user interface can be seen in Fig. 4.

Although this tool

initially adopted a box-based

representation in which the diﬀerent elements are

hierarchically organized in boxes, more recent releases

opt for a graph-based diagram to represent the distinct

elements and their relationships.

by hovering help icons for each element. Similar to

other hybrid visual approaches, it supports switching

between the visual composer and a built-in textual

editor of the corresponding YAML ﬁle. A sample of

Visual Composer’s GUI can be seen in Fig. 5.

8 Visual Composer, by CloudSoft, originally available at

https://cloudsoft.io/software/cfn-composer/ and on the

AWS marketplace at https://aws.amazon.com/marketplace/pp/

prodview-pqc3effdvhy3s, seems to have been discontinued as of

7 CodeHerent, available at https://codeherent.tech/home

January 2022.

8

Bruno Piedade et al.

bound to speciﬁc technologies. The experiments

conducted by Sandobalin et al. [23] with 67 Computer

Science students have empirically compared Argon with

a well-known IaC tool (Ansible) and showed promise

in the eﬀectiveness and perceived ease of use and

usefulness of Argon’s visual approach.

3.2 Discussion

We can classify the surveyed tools as simultaneously

form-based (i.e., using form ﬁelds or a spreadsheet-like

user interface) and hybrid (i.e., combining text and

visual systems), according to the scheme proposed by

Fig. 5: Sample of Visual Composer’s interface.

Argon is an infrastructure modeling tool supporting

Boshernitsan et al. [24]. The tools that come closer

diﬀerent IaC platforms [22]. Like Docker Studio, it is an

to supporting our goals are DockStation, Admiral and

Eclipse-based tool and oﬀers a native GUI. Argon allows

Docker Studio, but they have yet to fully explore the

selecting such resources from a palette window and

potential of a visual approach for the following reasons:

editing each resource’s properties through a form-based

interface. The resulting visually-created model of the

infrastructure can then be used to generate scripts for

diﬀerent IaC platforms (e.g., Ansible and Terraform),

unlike Codeherent and Visual Composer, which are

Fig. 6: Sample of Argon’s interface, adapted from

Sandobalin et al. [22].

– Incomplete visual notations. One of the issues

found is the lack of visual representations for some

of the elements supported in the Docker Compose

Speciﬁcation. In particular, Admiral and Docker

Studio appear to be the most complete, but they do

not support specifying Docker conﬁgs and secrets.

An incomplete visual notation encourages developers

to fall back into the text notation when something

cannot be understood from the visual notation. We

believe that the friction caused by this additional

context-switching may discourage using the visual

notation or reduce the beneﬁts to be gained from

using it.

– Limited visual editing. Only Docker Studio

allows editing all of the elements and properties

of Docker Compose that it supports (conﬁgs

and secrets, as stated before, are not supported

at all). DockStation allows to circumvent this

limitation by providing a textual editor for the

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

9

docker-compose.yml ﬁle within the application,

the elements that one can understand or express

but the resulting workﬂow does not provide a

through the text notation of Docker Compose ﬁles.

streamlined experience; a user must switch between

Unfortunately, these tools do not yet provide such

the textual and visual perspectives as needed.

support (cf. Section 3.2 and Table 1), so we have

– Sub-optimal

directness. Current

solutions

ultimately decided to develop a new tool, which we

present lower directness [25] than desired, as they

named Docker Composer.

require several steps to manipulate a Compose

Fig. 7 shows the high-level architecture of the

orchestration ﬁle visually. For instance, Admiral

tool. Within the host environment, the prototype

requires a few navigation steps to create or edit

(in the ﬁgure, represented as the Docker Composer

an artifact. The user must ﬁrst click on a button

App) generates Docker Compose YAML ﬁles and

that leads to a new page with form ﬁelds for input.

launches shell

instances where it executes Docker

After altering the deﬁnition, the user must conﬁrm

Compose via CLI commands. In turn, Docker Compose

their action for it to take eﬀect. This indirectness

communicates with the Docker Engine. Docker

is inconvenient, hindering the workﬂow, and hides

Composer also generates requests to the remote Docker

useful information that could otherwise be always

Hub’s public API, to receive information about the

visible.

images hosted on this service.

Table 1 (p. 10) summarises the surveyed tools,

illustrating their capabilities and limitations in visually

representing a container orchestration. This table also

contrasts the surveyed tools with Docker Composer, a

tool that we describe in the next section (Section 4).

Finally, it is worth noting that, as far as we know,

the only one of these tools that was used in the context

of an empirical study is Argon [23]. The positive results

of this study, albeit in the domain of infrastructure,

Fig. 7: Deployment diagram of the prototype.

encouraged us further to evaluate the beneﬁts and

possible limitations of using a low-code approach for

orchestrating containers.

4 The Docker Composer tool

Furthermore, Docker Composer can open and save

any docker-compose.yml ﬁle. Opening one such ﬁle

translates its contents to an object model, which then

supports the features related to visualization and user

We have considered using tools such as DockStation,

interaction. This object model is designed to express

Admiral or Docker Studio to empirically evaluate the

all the elements that we can ﬁnd in a Docker Compose

beneﬁts and limitations of a visual notation in the

orchestration ﬁle. Saving back to a ﬁle is the reverse

development of container orchestration ﬁles. However,

process of serializing this object model to a YAML ﬁle

we thought it essential that the tool handled all

following the Docker Compose Speciﬁcation.

Host<<instantiates>><<uses>><<process>>Docker Compose<< generates>><<compoent>>Docker ComposerApp<<artifact>>docker-composer-tmp.yml<<compoent>>Docker Hub API<<component>>Docker Engine10

Bruno Piedade et al.

Table 1: Comparative overview of visual tools for managing and orchestrating container and infrastructure resources.

The latter refer to virtual machines and other resources that can be used to host containers but are not themselves

containers or part of containers. A ﬁlled circle (

) is used for elements that can be diagrammatically represented

and edited (e.g., using boxes and arrows); a dot (•) for when editing the element is done through a form ﬁeld; and

(cid:32)

an empty circle (

) for when editing an element requires interacting directly with the fully textual form of the

Compose orchestration ﬁle.

(cid:35)

Container Speciﬁcation

Infrastructure

Services

V olu m es

N etw orks

C on ﬁgs

Secrets

(cid:32)

(cid:32)

(cid:32)

•

•

(cid:32)

•

(cid:32)

depends_on

(cid:35)

(cid:32)

links

(cid:35)
•

(cid:32)

(cid:32)

(cid:32)

(cid:32)

(cid:32)

DockStation

Admiral

Docker Studio

CodeHerent

Visual Composer

Argon

Docker Composer

(cid:32)

(cid:32)

(cid:32)

(cid:32)

(cid:32)

(cid:32)

(cid:32)

In Fig. 8, we present the prototype’s main view,

– Properties Editor. Useful to access and edit the

which features ﬁve distinct panels, namely:

various properties of the currently selected object

(artifact or connection) in the graph editor.

– Terminal view. Displays the output produced by

the services (containers) once created and started. It

– Toolbar. To the left, it includes a status indicator,

contains a General tab with the combined output of

which lights up diﬀerent colors according to the state

all services and additional logs (i.e., Docker Compose

of the running orchestration, and a set of buttons to

logs) and individual tabs for the output of services

start and stop the services (containers). To the right,

that comprise the orchestration.

it includes a few buttons for ﬁle management. The

settings menu allows to set the working directory

Fig. 9 shows the visual notation of a service node.

and adjust preferences when exporting ﬁles.

It includes a set of anchor points located on the right

– Image Palette. Allows searching for images hosted

edge. Each anchor point is used as the source point to

on Docker Hub and the addition of new services by

set connections between the service and some target

clicking and dragging the target image and dropping

artifact. This can be achieved by left-click dragging

it in the graph editor area.

from the source point to a compatible target artifact.

– Graph editor. This area displays an interactive

These connections are typed, meaning that only certain

visual map of the orchestration containing the

artifacts are expected as targets, and the tool only

various artifacts

that comprise it and their

allows this type of connection. To make the type of the

dependencies.

connection more explicit, the colors of the anchor points

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

11

Fig. 8: Layout of the prototype’s main view, showcasing the graph editor (drawing canvas), main control toolbar,

image palette (from Docker Hub) and the properties editor (corresponding to one of the services in the canvas).

match that of the allowed type of artifact, except for

Fig. 10 shows the remaining elements (volumes,

depends_on (yellow) and links (blue) anchors. These last

networks, conﬁgs and secrets). They are represented

two anchors are used to connect services; depends_on

by a similar notation, only diﬀering in color, size, and

establishes the order of container creation, while links

labels, depending on their type. All nodes allow to

allows containers to be reachable at an alias hostname.

input their key as exempliﬁed in the ﬁgure for secrets

and conﬁgs.

The tool provides static validations while editing an

orchestration. These include duplicate key detection

and invalid property value formats (e.g., for values

speciﬁed as time duration or memory size). The result

of the validations is conveyed to the users through

warning icons that appear near the artifacts’ visual

representation. It is possible to hover these icons with

the mouse pointer to visualize a full summary of the

Fig. 9: Visual representation of a service artifact node.

warnings. These inconsistencies are purely presented as

warnings and are not enforced as errors and ultimately

provide additional feedback to users. Fig. 11 shows

ToolbarPropertieseditorImage paletteGraph editorTerminal view12

Bruno Piedade et al.

Fig. 11: Example of the static validation notation. Both

services include the warning icon because they deﬁne

the same key (ser ).

Another form of validation is the mechanism used

to control the consistency of some property values.

In particular, when deﬁning port mappings, the user

cannot deﬁne host ports without ﬁrst setting a container

port. We achieve this by controlling whether inputs are

disabled or not. Additionally, the connections between

Fig. 10: Example orchestration in Docker Composer.

containers are typed, thus erroneous connections are not

The two services share the internal network, and the

allowed, such as trying to connect a Networks gate to a

mongodb service uses a volume for storing its data named

Service block).

mongo-data. Both services also share a secret (an SSH

To more clearly compare and demonstrate the

key) and the nodejs container uses a speciﬁc hostname

diﬀerences of representation between the conventional

conﬁg. The two forms to the bottom of the ﬁgure show

text-based approach and the designed visual approach,

the input ﬁelds for editing the secret and conﬁg as they

Fig. 12 shows a concrete example with a side-by-side

will appear on the right sidebar of Docker Composer

comparison between the textual representation (a) and

when the respective element is selected.

the equivalent visual representation (b). While it may

not be immediately clear, both representations convey

the same information. While the visual approach makes

the artifacts themselves and their connections more

an example of static validation. The warning in this

evident, some properties (e.g., stdin_open on the

example results from the use of the same key (ser ) for

client service) will be shown when hovering some of

both services.

the elements with a mouse pointer. The particular

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

13

orchestration that is presented follows a client-server

expressed in textual Docker Compose ﬁles, as we can

architecture comprised of three services: a web frontend

see by the overview given in Table 1 (p. 10).

service (client), a backend web service (server ), and

MongoDB database service (db). We also include two

5 Empirical Study

custom networks, named private and public, to isolate

the backend from the frontend as well as a named

volume, called mongo-data, for data persistence.

With this study we seek to evaluate the viability and

practical usefulness of a low-code environment in the

domain of container orchestration. This is particularly

relevant as empirical work in this ﬁeld is still fairly

Docker Composer,

and the

entire

low-code

limited [26]. Although the theoretical beneﬁts have been

environment that it provides, diﬀers from the tools

thoroughly evaluated in the past, there is still a severe

that we review in Section 3 in diﬀerent aspects, and

lack of studies to assess whether these truly translate

most notably in the support that it provides for the

to practical scenarios.

Docker Compose Speciﬁcation. The tools that are

The empirical study focuses on the evaluation of

most closely-related to Docker Composer (DockStation,

three activities in software engineering—analyzing,

Admiral and Docker Studio) do not oﬀer a visual

debugging, and implementing—in the context of Docker

notation that covers all the elements that can be

Compose conﬁgurations. A task was prepared for each

(a) Docker Compose.

(b) Docker Composer.

Fig. 12: Concrete example of a Docker compose ﬁle. (a) presents the default textual representation of a

docker-compose.yml ﬁle, and (b) a visual representation of the same ﬁle using Docker Composer.

14

Bruno Piedade et al.

activity, and we collected both performance-based and

directory for both groups and the prototype tool for the

perception-based metrics.

EG.

5.1 Participants

5.3 Task Deﬁnition

We selected a total of 16 students from the MSc

in Informatics and Computing Engineering at the

University of Porto, who volunteered to participate in

the experiment. This methodology makes our sample

a convenience sample [27]. All participants had prior

experience with Docker and Docker Compose due to

their academic path and were randomly distributed

between two groups, corresponding to the treatments:

control

(CG) and experimental

(EG). Both

groups were asked to solve the same set of tasks. The

participants of the CG had access to a text editor to

edit the orchestration ﬁle and to a command-line shell

to access the conventional toolchain. The participants

of the EG had access to the experimental prototype to

manage the orchestration as well as a command-line

shell to execute additional commands if required

(Docker related or not). In addition, both groups had

complete access to the oﬃcial Docker and Docker

Compose documentation as well as any other resources

on the internet.

5.2 Environment

The experimental sessions were conducted remotely. We

opted for a remote workstation, set up in advance with

the required software and materials, which was later

made available to the participants. These resources

included a browser (to access the experimental guidelines

and surveys), a text editor set up in the appropriate

directory, a command-line shell set up in the appropriate

As previously stated, the goal was to evaluate the

behavior of the tool for three basic activities: analyzing,

debugging, and implementing an orchestration ﬁle.

This eﬀort was translated into 4 tasks each featuring a

corresponding scenario. In Task 1 (T1) a functioning

Docker Compose conﬁguration was provided and the

goal was to analyze its structure and understand

the overall behavior.

In Task 2 (T2) a buggy

conﬁguration was provided and the goal was to debug

and ﬁx the faulty behavior. Task 3 (T3) focused on

implementing and was divided into T3.1—build a

simple conﬁguration from the ground up, involving two

containers, two environment variables, a custom network

and a volume (implementation)—and T3.2—modify

the conﬁguration to use secrets instead of environment

variables (increment).

To ensure a balance between scale, complexity,

realism, and expected time to completion in the tasks,

we conducted a brief study to characterize the typical

size of Docker Compose ﬁles. Namely, we tried to

Fig. 13: Distribution of 875 526 Docker Compose YAML

ﬁles on Github by size, in bytes, as of January 2022.

> 30003.1%1500..19992.6%1000..14994.5%500..99924.9%0..49962.2%Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

15

determine the typical number of containers in projects

– Tutorial. Before solving the actual tasks, the

using Docker Compose, using GitHub as a data source.

participants had to follow a simple tutorial

As depicted in Fig. 13, approximately 62% of the 875

reviewing some basics of Docker Compose. This

526 considered ﬁles had sizes up to 500 bytes, which we

was mostly targeted to the EG so that they had

equated to low complexity—typically containing one or

some prior hands-on experience with the prototype.

two containers and minimal additional conﬁgurations.

Nonetheless, to maintain consistency between both

We gathered this data using the oﬃcial code search

groups, participants in the CG also had to achieve

API for ﬁles named speciﬁcally docker-compose.yml

the same goal with the conventional toolchain.

and, thus, limited by search mechanism both in terms

– Experimental Tasks. Participants were instructed

of precision and number of results9 .

to solve a set of four orchestration-related tasks. To

5.4 Procedure

A full session took between 50 minutes to 2 hours per

participant. Each session was conducted individually

with the researcher overseeing and observing the full

procedure. Communication was done via remote voice

call. The participants were encouraged to think aloud

throughout the session so that the researcher could more

clearly understand and follow along with their rationale.

This strategy was also useful in identifying potentially

unforeseen issues with the experiment’s design.

maintain the total duration reasonable, time limits

were set for each task. Participants were asked to

advance to the next task whenever this time limit

was exceeded.

– Post-experiment Survey. Participants were asked

to ﬁll a survey to assess their experience and evaluate

the experience of working with the tools. The survey

in the EG diﬀered from the control since it included

an additional set of questions to speciﬁcally evaluate

the solution prototype.

Once the connection to the remote workstation

5.5 Research Variables

was established, the participant had access to the

instructions for the full procedure available in the

remote environment. We make these instructions

available as part of our replication package10 [28]. The

procedure was organized in the following steps:

We use both performance-based metrics and perception-

based metrics as dependent variables in our study.

The performance-based metrics consist of:

– Task Completion, which refers to the ratio

– Background Survey. This survey contained a

between the participants that successfully completed

set of questions to assess the current degree of

a task and the number of participants that tried to

experience with technologies which we had foreseen

complete it.

to potentially be confounding factors.

– Work Context Times refers to the times spent on

9 GitHub Code

Search,

https://docs.github.com/en/

diﬀerent work contexts, which we deﬁne later in this

search-github/searching-on-github/searching-code
10 A replication package to facilitate and encourage the

section.

independent replication of this experimental design is accessible

– Task Times refers

to the total

time spent

at https://doi.org/10.5281/zenodo.4001049.

completing each task.

16

Bruno Piedade et al.

– Execution Attempts refers to the the number of

5.6 Data Collection

times a participant tried to run the orchestration.

– Context Switches refers to the number of times

The results of the background questionnaire consist of

participants accessed each of the contexts.

answers of diﬀerent types, including items using 5-point

Likert scale, linear numeric scales, and multiple-choice

Some of these performance-based metrics lean on

questions.

the diﬀerent existing work contexts that participants

switched between when executing the tasks, and that

we deﬁne as:

Performance measurements for tasks were recorded

manually by the researcher. An application named Turns

Timer11 was used to register the time spent on individual

– Script. Time spent looking at the instructions and

activities, as well as the number of changes between

task description.

contexts. This was achieved by attributing a timer for

– Documentation. Time spent in the oﬃcial Docker

each context. The sum of all the timers was the total

and Docker Compose documentation and Docker

time spent on that task.

Hub.

Participants were asked to register the start and

– Composer. Time spent in the solution prototype,

end time for each task in the form as a redundancy

Docker Composer

(only

applicable

to

the

precaution in case some data was lost or incorrectly

experimental group)

recorded by the researcher. In addition, the number of

– Browser. Time spent on the browser when accessing

execution attempts was also registered by the researcher.

service’s UIs and other documentation resources

These performance metrics, namely, durations and

outside of those speciﬁed in the Documentation

execution attempts, addressed RQ1 and RQ2.

context.

– Editor. Time spent on the text editor to access and

edit the materials.

– Terminal. Time spent on the terminal, mostly

for executing Docker and Docker Compose CLI

commands.

Participants were also asked to save their solutions in

the workstation. This was done for subsequent review if

needed. The solutions considered the answers given and

the developed docker-compose.yaml ﬁles as requested

in the tasks.

RQ3 was addressed through the post-experiment

The perception-based metrics we use were ﬁrst

survey. This questionnaire mostly contained Likert-scale

introduced by Davis et al. [29,30] and consist of:

questions as well as a few open-ended questions. The

– Perceived Ease of Use (PEOU) refers to how

much eﬀort would be required to use the prototype.

– Perceived Usefulness (PU) refers to how well

the prototype satisﬁes the participant’s needs and

expectations.

– Intention to Use (ITU) refers to the degree that

former questions focused on the perception-based

metrics—PEOU, PU and ITO (cf. Section 5.5)— for

which we opted to follow a similar design to that

employed by Sandobalin et al. [23].

11 Turns Timer,

is an Android application available at

https://play.google.com/store/apps/details?id=com.

the participant wishes to use the tool in the future.

deakishin.yourturntimer

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

17

It is important to note that we measure PEOU

tests, and ρ as the probability of rejecting H0. We also

in both groups but measure PU and ITU exclusively

denote σ as the standard deviation and x as the mean.

in the experimental group. We adopt this approach

for PU and ITU because these metrics intrinsically

assume a subjective reference point. We believe that

participants in the CG would state their perception in

relative terms to the non-existence of Docker Compose,

and participants in the EG would most likely state it

in comparison to manipulating a docker-compose.yml

ﬁle directly. Participants could also share further

observations and considerations in the open-ended

questions. These were primarily useful in detecting

potentially overlooked issues with the experimental

procedure and even unforeseen validity threats.

5.7 Pilot Experiments

We conducted two pilot experiments to gather feedback

about the quality and consistency of the materials and of

the experimental procedure itself. The ﬁrst pilot allowed

us to realise that some tasks were too complex to ﬁt

within the time of the experiment. As a result, we have

redesigned and simpliﬁed them. The second pilot allowed

to reﬁne details in the materials, including typos and

small inconsistencies, as well as to streamline the data

collection process, in particular, the use of the Turns

6.1 Background and Tutorial

The background survey gathers information about

confounding factors to ensure that the groups are

balanced in experience and skills. Questions are deﬁned

as Likert items and numeric values and inquire if

the participants consider

themselves experienced

with (1) visual programming,

(2) orchestration

frameworks and tools, (3) Docker and (4) Docker

Compose—conﬁguration of volumes, networks, conﬁgs

and secrets.

We show a summary of the results for the Likert

and numeric scale questions in Table 2. Considering the

alternative hypothesis stating that the control group

is diﬀerent from the experimental group (CG (cid:54)= EG)

for each of the background questions, we found no

signiﬁcant diﬀerence in experience or skills between

the groups, except for BQ6. We discuss this diﬀerence

at the end of this section.

Timer application to register context times.

The participants were also asked to specify what

6 Results and Discussion

other orchestration frameworks, if any, had they used

in the past. Only Kubernetes came up in the answers,

with 2 participants of the CG and 3 of the EG reporting

The data collected was mainly quantitative, and we

to have used it. Considering an alternative hypothesis

have used it for hypothesis testing, employing the Mann-

that the control group is diﬀerent from the experimental

Whitney U (MW-U) [31,32] and McNemar [33] tests

group (CG (cid:54)= EG) for the number of participants that

against our variables of interest. The notation used

have used Kubernetes in the past, the results show no a

represents H0 as the null hypothesis and H1 as the

signiﬁcant diﬀerence of experience and skills between

alternative hypothesis, u for the U statistic of MW-U

the groups (cf. Table 3).

18

Bruno Piedade et al.

CG

EG

MW-U

show a signiﬁcant diﬀerence between the groups

x

σ

x

σ

H1

u

ρ

(cf. Table 4).

BQ1

2.88

0.398

3.13

0.398

(cid:54)= 29.0

0.372

BQ2

2.13

0.581

1.63

0.263

(cid:54)= 30.5

0.431

BQ3

4.00

0.189

4.13

0.350

(cid:54)= 25.5

0.214

BQ4

3.25

0.412

3.13

0.389

(cid:54)= 30.5

0.434

BQ5

3.25

0.412

2.75

0.458

(cid:54)= 25.0

0.223

BQ6

2.88

0.295

1.63

0.263

(cid:54)=

9.0

0.060

BQ7

4.38

0.822

4.88

0.515

(cid:54)= 26.0

0.262

BQ8

2.88

0.895

2.63

0.925

(cid:54)= 31.0

0.458

BQ9

3.50

1.052

3.75

0.675

(cid:54)= 25.5

0.244

I consider myself experienced with ...

BQ1. ... visual programming tools.

BQ2. ... with orchestration frameworks.

BQ3. ... with the Linux OS.

BQ4. ... with Docker.

CG

%

Volumes

37.5

Networks

37.5

EG

%

62.5

25.0

McNemar

H1

(cid:54)=

(cid:54)=

ρ

0.687

1.000

Table 4: Results of the McNemar test for conﬁgured

Docker Compose options.

To conclude the background analysis, taking into

account all of the data collected and corresponding

BQ5. ... with Docker Compose for development purposes.

analysis, we believe that we can argue with some level of

BQ6. ... with Docker Compose in production environments.

Until now, approximately in how many projects have you ...

BQ7. ... worked on which have used Docker Compose?

BQ8. ... created/updated a docker-compose.yml ﬁle?

BQ9. ... used docker-compose.yml ﬁles created by others?

Table 2: Summary of the answers to the Likert and

numeric scale questions in the background questionnaire.

CG

EG

MW-U

x

σ

x

σ

H1

u

ρ

OF 0.25

0.463

0.38

0.518

(cid:54)= 28

1.000

conﬁdence that the subjects were balanced across both

groups. Unfortunately, we cannot explain the answers to

BQ6, and perhaps they translate a statistical anomaly,

as they are not consistent with the answers pertaining

to the number of projects (BQ7, BQ8, and BQ9), nor

with the number of orchestration frameworks used (OF)

or the Docker Compose options that participants have

conﬁgured in the past.

To further ensure the groups were under equivalent

OF. Number of orchestration frameworks used

conditions, before starting the experimental tasks, they

Table 3: Summary of the number of previously used

tools speciﬁed in the background questionnaire.

have run a simple tutorial in the respective toolchain

that they were requested to use.

Another question inquired subjects about what

individual Docker Compose conﬁguration options

they had conﬁgured in the past. The same number of

During the task we measured the task completion, work

context times, task times, execution attempts and context

6.2 Experimental Tasks

participants in each group reported having conﬁgured

switches, as analysed next.

secrets and conﬁgs. To conﬁrm that there is not a

signiﬁcant diﬀerence in the use of volumes and networks,

6.2.1 Task Completion

we ran a McNemar test. Considering an alternative

We have considered the eﬀectiveness of task execution

hypothesis that the control group is diﬀerent from the

by looking at task completion. We deﬁne this metric as

experimental group (CG (cid:54)= EG), the results do not

the ratio between successfully completed tasks and the

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

19

total number of tasks. A task is successfully completed

directly comparable between the two groups—Script,

only if the subject ﬁnished within the allotted time limit

Docs (documentation) and Browser.

and the solution was correct.

Fig. 14 displays the distribution of completed tasks

by group. While all participants completed T1 and T3.2,

there is a clear diﬀerence in T2 and T3.1. While most of

the participants in the EG completed the experimental

tasks, only approximately half of the participants in

the CG were able to complete them. The participants

By looking at the data in Fig. 15, we can identify a

large discrepancy in the time spent on the Docs context

for reading documentation. This is further supported

by the discrepancy of the time spent on the Browser

context, which was also mostly dedicated to reading

other non-oﬃcial documentation resources. We ran a

Mann-Whitney U test for the independent contexts to

in both groups who were unable to complete the tasks

conﬁrm our intuition.

were so due to the imposed time constraints on solving

them. No case was registered in which the solution was

incorrect. We can conclude that fewer participants in

the CG ﬁnished the task T2 and T3.1. This, in turn,

impacts the metrics considered for the remainder of this

analysis since the registered times were capped up to

the moment when the time limit was exceeded. If the

time limit was not set, the diﬀerences might have been

Considering the alternative hypothesis that the time

spent in the Docs and Browser contexts is higher for the

CG, the results shown in Table 6 conﬁrm that, indeed,

the CG spent signiﬁcantly longer than the EG in

these contexts. We can also see that there is not a

signiﬁcant diﬀerence between the groups in the time

spent on the Script context for reading the script.

even sharper. However, this was a necessary sacriﬁce to

It is diﬃcult to draw any other useful information

keep the overall time reasonable and manageable.

from the remainder of the variables when considered

individually, as they are either exclusive to some group

(i.e., Composer for the EG) or partly replace the purpose

of one another across both groups. However, we can

consider the sum of time spent on editor and terminal

(E+T) in the CG to be roughly equivalent to the sum of

time spent on the textual editor (which mostly equates

to the time spent accessing other textual materials such

as conﬁguration ﬁles which were used in the tasks) and

on Docker Composer (E+C) in the EG. No participant

in the EG used the terminal to execute any other

Docker or Docker Compose CLI commands besides those

that were available in the prototype. We refer to this

Fig. 14: Distribution of total completed tasks per group.

6.2.2 Work Context Times

The times spent on each work context allow us to

composite context focused on the management of the

understand the behavior of the participants better.

orchestrations as Stack Management and show it as the

Table 5 and Fig. 15 overview the global times per

last line in Table 5. The time diﬀerence shown in this line

context. We shall ﬁrst look into those that are most

does not appear to be very high. Testing the hypothesis

# Completed02468T1T2T3.1T3.2CGEG20

Bruno Piedade et al.

(CG > EG using the MW-U test rendered u = 29 and

Table 7 summarizes the results obtained for the

ρ = 0.399. These results do indeed not show that the

times of each task along with the results of the MW-U

participants in the EG have spent signiﬁcantly less

signiﬁcance test performed to compare both. By

time managing the containers than those of the CG.

considering this data and the expected alternative

Therefore, it seems reasonable to conclude that the

hypothesis which states that the participants in the

biggest impact on the overall duration was the time

EG would ﬁnish tasks faster than those of the CG

spent consuming documentation. This diﬀerence is in

(i.e., CG > EG) for all tasks, the results demonstrate

line with the expectation that a visual programming

that EG did indeed ﬁnish task T2, T3.1 and

language promotes an exploratory approach in which

T3.2 signiﬁcantly faster than the CG. These tasks

the solution space is constrained by the options that

evaluated debugging,

implementing, and updating

are explicitly made available through the user interface,

activities. Particularly, it is interesting to note the

and users are able to converge to solutions by searching

signiﬁcant diﬀerence in T3.2. The scenario in this task

the options provided by our prototype.

required the participants to use a particular feature

6.2.3 Task Times

of Docker Compose—secrets—with which most did in

fact not have any prior experience. In practice, the

Analyzing the times per context provides detailed insight

workﬂow to use this feature in the prototype was very

into the participants’ behavior. We can, however, also

similar to that of other artifacts, such as volumes and

look at the time spent globally (i.e., the total sum of

networks. These results support that the prototype was

time spent on each activity) to assess the overall speed.

suﬃciently intuitive for participants to learn how to use

Fig. 16 displays the distribution of times by task for

this new feature, after having some experience with it,

each group. We can identify that the participants in the

simply by following a similar rationale and without the

EG generally have ﬁnished tasks T2, T3.1, and T3.2

need to consult additional documentation.

sooner than the participants in the CG. In contrast, for

The prototype successfully reduced the overall

task T1, both groups are more balanced.

time required to develop and debug orchestrations.

Table 5: Summary of the global time registered per activity for the sum of time taken in all tasks, with the mean

and standard deviation for each group. The Composer context does not contain data for the CG as this

context was not available for this group and was exclusive to the EG.

Context

(cid:80)

CG

x

σ

(cid:80)

EG

x

σ

Script

Composer

Docs

Browser

Editor

Terminal

2:06:04

15:46

06:10

1:29:40

11:13

03:52

-

-

-

3:50:08

28:46

10:29

1:51:36

13:57

06:25

0:18:59

02:22

02:15

0:41:44

05:13

04:02

0:11:36

01:27

01:53

2:52:17

21:32

03:57

0:04:51

00:36

00:29

1:53:03

14:08

02:38

0:02:10

00:16

00:31

Stack Management

3:34:01

26:45

07:16

3:54:59

29:22

10:47

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

21

Fig. 15: Distribution of the global times for each subject by context, by group. The Stack Management context

refers to the sum of time spent on the Editor and Terminal contexts for the CG and the sum of time spent on the

Editor and Composer contexts for the EG.

Table 6: Result of the Mann-Whitney U equality test

Table 7: Summary of the completion times for each task

for the sum of time spent on three contexts.

across groups.

Context H1

Script

Docs

Browser

>

>

>

u

17

ρ

0.065

2 <0.001

9

0.007

While there was no meaningful improvement for task

T1 (in which participants had the goal of analyzing

an orchestration), overall, the prototype managed to

reduce the duration of the remaining tasks. Some of

the questions in T1 required a deeper knowledge of

CG

EG

MW-U

Task

x

σ

x

σ

H1

u

ρ

T1

T2

0:13:05

0:05:51

0:12:12

0:05:19

> 31

0.480

0:22:59

0:04:55

0:14:41

0:05:59

> 11

0.014

T3.1

0:24:56

0:07:04

0:13:47

0:06:57

T3.2

0:09:35

0:04:26

0:04:01

0:01:56

>

>

3

6

0.001

0.002

exploring the features of the prototype, in search of

answers for the ﬁrst task.

concepts that were not immediately conveyed by the

6.2.4 Execution Attempts

prototype. Although the participants in EG already

had some hands-on experience with the prototype

In addition to the task times, the execution attempts

during the tutorial, we think that they spent some time

were also registered for each task, that is, the number of

22

Bruno Piedade et al.

Fig. 16: Distribution of times to completion for each

Fig. 17: Distribution of execution attempts for each

subject by task, by group.

subject by task, by group.

times a participant tried to run the orchestration (i.e.,

run the command docker-compose up).

task across both groups.

Table 8: Summary of the execution attempts for each

Fig. 17 displays the distribution of execution

attempts by task for each group. We can see that the

participants in the EG have generally performed fewer

execution attempts in tasks T2, T3.1, and T3.2 than

the participants in the CG. In contrast, for task T1,

both groups are more balanced, but it is important

to note that the results for T1 are not very revealing

as the execution of the orchestration was completely

optional for this task.

CG

EG

MW-U

Task

x

σ

x

σ

H1

u

ρ

T1

T2

0.38

0.518

0.50

0.535

7.00

3.928

5.63

2.560

T3.1

10.13

4.357

5.25

4.097

T3.2

3:50

1.690

1.75

0.463

>

>

>

>

28.0

0.500

21.5

0.134

11.5

0.014

13.0

0.016

These results are in line with the time diﬀerence

established above. Overall, the participants in the

EG were more eﬃcient and did not spend as much

time restarting the containers. This behavior was also

Table 8 displays the results for execution attempts.

expected as in practice, many execution attempts in the

Considering this data and the expected alternative

CG resulted from syntax errors. The prototype avoided

hypothesis which states that the EG would need fewer

most syntax errors simply due to the more strict

execution attempts than the CG (i.e., CG > EG) for

form inputs (with stronger validation) and subsequent

all tasks, the results demonstrate that EG did require

automatic code generation, free of errors. We believe

signiﬁcantly fewer execution attempts for T3.1

that this was the biggest factor contributing to the

and T3.2.

non-signiﬁcant diﬀerence in T2 since a partially working

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

23

orchestration was provided in this task, and it required

diﬀerence in context switching across the two groups if

few changes.

we were to condition the CG to use a built-in terminal.

6.2.5 Context Switches

In addition to the work context times, the context

switches were also recorded, that is, the number of

times the participant accessed each of the contexts. To

keep this metric uniform across participants, we consider

the context switches per minute (s/m) instead of to the

total count of context switches. This metric is useful in

evaluating participants’ degree of focus when using the

tool. We argue that a higher number of context switches

Fig. 18: Distribution of global context switches for each

translates into a less optimized experience since users

subject by group.

have to shift their attention more frequently.

We analyze the global context switches during the

full session, that is, the total sum of the switches between

Table 9: Results of the MW-U test for global context

all contexts for all tasks. Fig. 18 seems to suggest that

changes.

the participants in the EG performed fewer context

switches than those of the CG. To conﬁrm this intuition,

we performed a MW-U test (cf. Table 9). Considering

this data and the alternative hypothesis which states

that participants in the EG would execute fewer context

switches than those in the CG (i.e., CG > EG) overall,

the results show that the participants in the EG did, in

CG

EG

MW-U

s/m

x

σ

x

σ

Global

4.628

0.912

3.479

0.929

H1

<

u

10

ρ

0.010

6.3 Assessment Survey

fact, execute signiﬁcantly fewer context switches

This survey had the goal of gathering insights on the

than those in the CG. These results suggest that the

perception of participants regarding aspects of the

process was more streamlined for the EG, which is in

experiment itself and of the visual approach that we

line with the results of the task time analysis performed

aimed to evaluate. We start by asking how participants

previously.

perceived the environment (cf. Section 6.3.1) and

Notwithstanding, to interpret these results, we

the clarity of the instructions and task descriptions

must also consider that CG participants used external

(cf. Section 6.3.2), with the goal of detecting unforeseen

terminal windows to execute CLI commands, rather

confounding factors. The subsequent questions intend

than a built-in terminal within the text editor. It

to collect data about three perception-based metrics:

is reasonable to expect that we would see a smaller

perceived ease of use (PEOU, cf. Section 6.3.3),

24

Bruno Piedade et al.

perceived usefulness (PU, cf. Section 6.3.4), and

EG found the task descriptions more understandable

intention to use (ITU, cf. Section 6.3.5). The survey

than the CG. This discrepancy implies a relevant

provided to EG featured an additional set of questions

threat to validity, since it could entail that diﬀerences

to evaluate the perceived usefulness of

individual

in performance reﬂect an intrinsic diﬃculty by the

features and overall PU as well as ITU sentiment

CG in understanding the instructions. However, the

towards the prototype. The questions which focused on

balance between the two groups that we report in

PU were formulated to compare the usefulness of the

Section 6.1 makes us believe that that is unlikely.

prototype in relation to the participant’s perception of

Another explanation that we must consider is that

the conventional method and toolchain.

this diﬀerence in perception is the result of the more

6.3.1 Environment

Analyzing the data in Table 10 and considering an

alternative hypothesis that the perception of the CG

of environment factors diﬀers from the EG for all

environment-related questions, the results demonstrate

that there is not a signiﬁcant diﬀerence between

the groups. These results support the hypothesis that

the inﬂuence of environmental factors on performance

during tasks was balanced across both groups and

pronounced diﬃculties of the CG in executing the

task—that is, in moving from the problem space to

the solution space—and that this diﬃculty may have

inﬂuenced their judgment about the instructions.

The prototype used by the EG provided a more

streamlined and focused experience (as supported by

the lower context switching) which, we think, has helped

participants to concentrate on the provided instructions

and take a more linear approach to performing the

tasks.

therefore, did not have a meaningful impact on the

Table 11: Summary of the answers to the CLR Likert-

outcomes.

scale items of the assessment survey.

Table 10: Summary of the answers to the ENV Likert-

scale items of the assessment survey.

CG

EG

MW-U

x

σ

x

σ

H1

u

ρ

ENV1

4.13

1.356

3.75

1.282

ENV2

1.88

1.356

2.13

1.458

(cid:54)=

(cid:54)=

25.5

0.231

29

0.367

ENV1. It was easy working in the remote machine.

ENV2. The environment was distracting.

6.3.2 Clarity of the Instructions and Task Descriptions

CG

EG

MW-U

x

σ

x

σ

H1

u

ρ

CLR1

2.25

1.282

1.25

0.463

CLR2

3.00

0.926

1.50

0.756

(cid:54)=

(cid:54)=

17.0

0.080

6.5

0.005

CLR1. I found the procedure instructions complex and hard to follow

CLR2. I found the task descriptions complex and hard to follow.

6.3.3 Perceived Ease of Use

By analyzing the data in Table 12 and considering

the hypothesis that participants in the EG would

ﬁnd the prototype easier to use (i.e., CG > EG for

We found a signiﬁcant diﬀerence regarding how the

PEOU1 and PEOU2 and CG < EG for PEOU3) for

task descriptions were perceived across both groups, as

all equivalent PEOU questions, the results demonstrate

shown by Table 11, and the means suggest that the

that the EG did indeed ﬁnd that it was signiﬁcantly

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

25

easier to work with the prototype. Therefore we

Table 13 summarizes the obtained results and allows

can state with some conﬁdence that participants did

us to conclude that the feature considered most useful

ﬁnd the prototype easier to use than the conventional

was the visual map of artifacts (VM) while the least was

method. Additionally, the exclusive question PEOU4

the Docker Hub integration (DHI). These results match

also demonstrates that the participants in EG strongly

our expectations as the DHI feature was secondary

agreed that the tool is easy to learn.

and mostly added for ease-of-use and convenience. The

Table 12: Summary of the results of the answers to

since participants could copy and paste the image names

the Likert-scale questions related to perceived ease of

and tags from the provided script without the need to

use (PEOU) in the assessment survey. (*) PEOU4 was

locate them manually. In contrast, the VM feature was

designed tasks did not take full advantage of this feature

exclusive for the EG.

CG

EG

MW-U

x

σ

x

σ

PEOU1

2.64

1.188

1.00

0.000

PEOU2

2.63

0.916

1.13

0.354

PEOU3

3.63

0.744

5.00

0.000

H1

>

>

<

u

8.0

5.5

4.0

ρ

0.002

0.001

0.001

PEOU4*

n/a

n/a

4.88

0.354

n/a

n/a

n/a

PEOU1. Overall, I found the tool diﬃcult to use.

PEOU2. I found it diﬃcult to understand stacks with the tool.

PEOU3. I found it easy to deﬁne stacks with the tool.

PEOU4. Overall, I found the tool easy to learn.

6.3.4 Perceived Usefulness

This and the next section are about questions exclusive

to the survey provided to the EG, as they were

speciﬁcally about Docker Composer, and could only be

answered by the group that used it.

The survey provided to the EG contained a section

dedicated to evaluating the perceived usefulness of our

prototype when compared with a usual workﬂow without

Docker Composer. The questions focused on speciﬁc

features, namely, the visual map of artifacts (VM),

the direct result of the hypothesis of this dissertation

and corresponded to the most novel and premeditated

feature. Regardless, the response was positive for all

features.

Table 13: Summary of the results of the answers to the

Likert-scale questions related to perceived usefulness

of features in the assessment survey. n/a means not

applicable.

VM

x

σ

ULE

4.88

0.35

UQ

4.75

0.46

DLE 4.88

0.35

DQ

4.88

0.35

DHI

VF

UIC

x

4

4

4

4

σ

1.20

1.20

1.20

1.20

x

σ

4.50

0.76

4.75

0.71

n/a

n/a

n/a

n/a

x

n/a

n/a

σ

n/a

n/a

4.75

0.71

4.75

0.71

I ﬁnd the [VM|DHI|VF|UIC] ...

ULE. ... helpful to understand stacks with less eﬀort.

UQ. ... helpful to understand stacks more quickly.

DLE. ... helpful to deﬁne stacks with less eﬀort.

DQ. ... helpful to deﬁne stacks more quickly.

Docker Hub integration (DHI), visual feedback (VF),

The answers to questions about the overall perceived

and executing commands on the UI (UIC). They are used

usefulness are shown in Table 14. Taking into account

to assess the perceived usefulness with more granularity

this feedback and that about the usefulness of individual

and support us in better understanding the impact of

features (cf. Table 13), we can state that participants

each feature in the overall perception.

did indeed ﬁnd the tool useful.

26

Bruno Piedade et al.

Table 14: Summary of the results of the answers to the

the conventional method, generally useful, and were

Likert-scale questions related to the overall perceived

interested in using it in the future.

usefulness in the assessment survey.

x

σ

PU1

5.00

0.000

PU2

5.00

0.000

PU3

1.50

0.756

PU4

1.00

0.000

PU5

4.88

0.354

7 Validation Threats

We identify and discuss threats that might hinder the

soundness of the obtained results. We tried to mitigate

most threats throughout the experimental planning and

PU1. I believe this tool would reduce the eﬀort required to deﬁne

design. Nevertheless, we identify the following threats.

stacks.

PU2. Overall, I found the tool useful.

PU3. A stack visualized with the tool would be more diﬃcult to

understand.

PU4. Overall, I think this tool is ineﬀective for deﬁning stacks.

PU5. Overall, I think this tool improves the stack deﬁnition process.

6.3.5 Intention to Use

7.1 Internal Validity

Psychological bias. For the results to be unbiased, it

is important to ensure that participants are unaware

of what group they belong to. However, this is hard to

Taking into account the results displayed in Table 15,

achieve in practice. Participants may have suspected

we can state with some conﬁdence that participants do

they were part of the EG, since they were asked to use

indeed intend to use the tool in the future.

a tool that was not known to them. Nevertheless, all

Table 15: Summary of the results of the answers to the

by preparing the materials to omit any relevant

Likert-scale questions related to the overall perceived

information and avoiding any verbal exchange during

usefulness in the assessment survey.

the experiments themselves, which could allude to this

eﬀorts were made to mitigate this threat, particularly

x

σ

ITU1

4.50

0.535

ITU2

4.75

0.463

ITU3

5.00

0.000

ITU4

4.50

0.756

ITU5

4.75

0.463

ITU1. This tool would make it easier for practitioners to deﬁne

orchestrations.

fact.

Experience diﬀerences. It is crucial to ensure that

the results are independent of possible skills and

experience diﬀerences between groups. For this reason,

the background questionnaire was part of the process,

and the data analysis supports that both groups were

ITU2. Using this tool would make it easier to explain the stack.

balanced. Therefore, we believe we can discard this

ITU3. I would recommend this tool to work with Docker Compose.

ITU4. I would like to use this tool in the future.

threat.

ITU5. It would be easy for me to become skillful in using this tool.

Environment inﬂuences. Performing the sessions

remotely raised additional concerns in regards to

Overall, the results demonstrate that the response to

possible deviations due to uncontrolled external factors.

the prototype was overwhelmingly positive and generally

However, we believe that we were able to ensure

very consistent across participants. The participants

a consistent environment for all participants with

found the approach more straightforward to use than

this approach. In addition, the researcher’s constant

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

27

observation during the sessions was also useful

in

two pilot experiments, as described in Section 5.7. This

identifying any unforeseen anomalies. The results in the

allowed improving potentially dubious questions before

assessment questionnaire further support that there

running the experiment.

was not any signiﬁcant diﬀerence between groups. Thus,

Auto-layout ineﬃciencies. As described in Section 4,

we believe we can discard this threat.

the prototype implemented an automatic layout

7.2 External Validity

algorithm to position artifacts when loading an

orchestration from a YAML ﬁle. However,

the

layouts achieved were not as good as if manually

Limited sample size. The somewhat small sample

constructed. To mitigate possible deviations resulting

size limits the extent to which we can conﬁdently assert

from this limitation, the orchestrations provided to

the generalizability of our ﬁndings. To do so, replications

the experimental group were prepared in advance to

of these studies should try to use a larger sample size

a more readable format and were loaded using the

to mitigate this threat.

custom storage feature. In practice, this did not seem

Sample bias. All the participants were students

to inﬂuence the results as no participant suggested this

with similar backgrounds. While this helped to ensure

improvement, therefore we believe that we can discard

that there was not a signiﬁcant experience disparity

this threat.

between groups, the results may be biased towards

novice software developers. Research conducted by

8 Conclusions

Host et al. [34] concluded that graduate students can

be appropriate subjects, if properly trained, as they

represent the future generations of developers. Salman

et al. [35] also conclude that, independently of the

experience level, subjects perform similarly when they

apply a new approach for the ﬁrst time. Thus, we

believe the results are meaningful. Despite this, further

studies may obviously provide more insights, especially

Our ﬁndings support some of the beneﬁts that we

expected to have from using low-code in this domain

and,

in particular, support the hypothesis that a

visual approach for orchestration can indeed reduce

development time and error-proneness signiﬁcantly. We

delve into this hypothesis in more detail by answering

the three research questions that we ﬁrst introduce in

if they feature a more heterogeneous sample, with

Section 2.

more diverse backgrounds, to achieve results with more

RQ1 To what extent does a visual notation for the

conﬁdence and mitigate this threat.

orchestration of (Docker) containers reduce the

7.3 Construct Validity

development time?

Answer: The work context times show a positive

impact of Docker Composer, but the clearest

Clarity of the questions. There is always the

gains are speciﬁcally in the reduction of the time

chance that some participants have interpreted our

spent reading documentation (cf. Section 6.2.2).

questionnaire items in a diﬀerent way than the one we

Nevertheless, a clear

improvement can be

intended. To address this concern, we have ﬁrst run

observed in task times, and the most visible

28

Bruno Piedade et al.

beneﬁts appear speciﬁcally in tasks involving

end, we make our prototype and experimental package

the development or debugging of orchestrations

readily available and provide a roadmap along three

(cf. Section 6.2.3).

diﬀerent topics—the visual approach, the developed

RQ2 To what extent does a visual notation for

prototype, and the empirical evaluation.

(Docker) orchestration ﬁles reduce the number of

errors?

Answer: Both the analysis of the execution

attempts (cf. Section 6.2.4) and of the context

switches (cf. Section 6.2.5) suggests that the

experience was overall more streamlined when

using a visual notation. Docker Composer,

allowed to spend less time restarting containers

and to avoid syntax errors, which resonates

with the signiﬁcantly lower number of context

switches.

RQ3 What is the perception of developers towards

using a visual notation for the orchestration of

(Docker) orchestration ﬁles?

Answer: The ﬁndings related to perception-

8.1.1 Visual Approach

The designed visual approach is highly tied to the

underlying concepts of Docker Compose, but we

believe that similar visual approaches are applicable

to a broader context and,

in particular, to other

orchestration technologies. Therefore,

it would be

interesting to explore domain-speciﬁc visual notations

for other orchestration technologies (e.g., Kubernetes).

Furthermore, one can even consider the possibility of

a more generic and technology agnostic model-driven

approach, useful

for a wider set of use-cases. In

conjunction with the growing adoption of microservices

architectures, the positive results obtained in this work

provide strong motivation to promote research in this

based metrics were positive

overall

(cf.

ﬁeld.

Section 6.3). The participants

felt

that

Docker Composer was more comfortable to use,

8.1.2 Prototype

was generally useful, and they showed strong

intentions of using it in the future. These results

give us some conﬁdence that developers ﬁnd

the tool easy-to-use and intuitive, considering

the steps and actions needed to conﬁgure some

orchestrations successfully.

8.1 Future Work

While the results that we have obtained are promising,

We propose evolving the prototype to a production-ready

application by reﬁning and expanding existing features

and exploring other ideas that go beyond the immediate

objective of this work, which we believe may further

improve the orchestration process. Some of these ideas

stem from the conceptual stages of our implementation

but were ultimately not realized as they did not directly

contribute towards our goal, while others result from

how we foresee the prototype could evolve.

we do not consider the prototype production-ready.

– Textual editor. We propose the inclusion of a

Additional research would be useful to consolidate

textual editor, which would work in parallel with

further and conﬁrm our ﬁndings, primarily to address

the graphical editor, as the similar feature oﬀered

some of the validation threats described above. To this

by DockStation. However, unlike DockStation,

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

29

we propose to view both perspectives on the

– Exploring liveness. Liveness is a characteristic of

same window simultaneously. This would require

development environments that refers to its ability

real-time sync mechanisms

to maintain both

to provide information to the developer about what

views consistent on change in either one and

they are constructing [41]. Tanimoto established a

could be achieved through techniques for MDSE

scale that can be used to evaluate the level of liveness

bidirectional

transformation [36,37,38,39] and

of an environment [42].

ﬂexible modeling [40]. We consider this as one of the

We may use liveness as an indicator of how much

most signiﬁcant improvements since it could broaden

these tools can provide timely feedback to their

the potential target audience of the prototype, as

users and, therefore, how much they can discourage

many developers (especially the more experienced

switching between these and other applications.

ones) seem to prefer working with text ﬁles. This

While the previous two points above already

addition would mean that the solution would not

contribute towards a more live experience, we can

substitute the conventional method and instead

see the possibility of exploring this concept more

complement it with more information and options.

exhaustively to improve the process of deﬁning

– Automatic layout. As previously noted the

orchestrations.

automatic layout feature was sub-optimal and

requires further research to improve the display

arrangement of the diﬀerent visual nodes. The

8.1.3 Empirical Evaluation

approach should ideally optimize placement based

Replication of our study would help to consolidate and

on the type of artifact rather than considering all

increase the conﬁdence level of these results. With this

the elements on the same level (e.g., it may make

in mind, we have compiled a replication package as

sense to tend to represent volumes and networks on

described in Section 5.4.

the lower half of the layout).

– Visual

feedback. This is a broad subject,

in

Furthermore, we can see diﬀerent variants to our

controlled study that would complement the results

which we may consider minor changes, such as

presented in this article:

more detailed status indicators, to more substantial

improvements, such as optimizing feedback for other

– Running the same study with professionals would

technologies, like Docker Swarm and its multiple

show how much the results we obtained here with

containers per service.

beginners are really extendable to professional

– Static validation. While the prototype considers

software developers.

validations for some property ﬁelds, there is the

– Conditioning the CG to use a terminal built-into the

potential to further enrich this feature with even

text editor, instead of an external one, would provide

more. These include validations for ports taking into

a baseline that is closer to the environment currently

account the available host ports.

used by developers and a more accurate account

of the amount of context-switching in question (cf.

Section 6.2.5).

30

Bruno Piedade et al.

– Expanding our study design to consider DockStation,

2. S. Pandey, “Investigating community, reliability and usability

Admiral and Docker Studio would show the extent

of cfengine, chef and puppet,” Master’s thesis, University of

to which our ﬁndings hold independently of the

idiosyncrasies of speciﬁc implementations. Even

Oslo, 2012.

3. J. Humble and D. Farley, Continuous Delivery: Reliable

Software Releases through Build, Test, and Deployment

though these two tools don’t provide a complete

Automation. Pearson Education, 2010.

visual notation for Docker Compose ﬁles, they could

4. T. B. Sousa, A. Aguiar, H. S. Ferreira, and F. F. Correia,

still allow narrower studies to be done on the merits

of visual programming languages in this domain.

While controlled user studies are powerful

in

“Engineering software for the cloud: patterns and sequences,”

in Proceedings of the 11th Latin-American Conference on

Pattern Languages of Programming, pp. 1–8, 2016.

5. D. Merkel,

“Docker:

lightweight Linux containers

for

consistent development and deployment,” Linux Journal,

identifying isolated cause-eﬀect relations, they fail to

vol. 2014, no. 239, p. 2, 2014.

fully capture the intricacies of real-world scenarios.

6. C. Pahl, A. Brogi, J. Soldani, and P. Jamshidi, “Cloud

Case Conducting a case study with Docker Composer in

an industrial environment would be another invaluable

source of

insight into the actual behavior of the

approach in more realistic scenarios.

Container Technologies: a State-of-the-Art Review,” IEEE

Transactions on Cloud Computing, vol. 7161, no. c, pp. 1–14,

2017.

7. A. M. Joy,

“Performance

comparison between Linux

containers and virtual machines,” Conference Proceeding -

2015 International Conference on Advances in Computer

Engineering and Applications, ICACEA 2015, pp. 342–346,

Acknowledgments

2015.

8. T. B. Sousa, F. F. Correia, and H. S. Ferreira, “Patterns for

Thank you to David Reis, Jessica Diaz, and the

software orchestration on the cloud,” in Proceedings of the

participants and anonymous reviewers of the LowCode

2020 workshop [43] for discussing earlier versions of this

work with us. Also, we thank the anonymous reviewers

22nd Conference on Pattern Languages of Programs, PLoP

’15, (USA), The Hillside Group, 2015.

9. I. Kumara, M. Garriga, A. U. Romeu, D. Di Nucci,

F. Palomba, D. A. Tamburri, and W.-J. van den Heuvel, “The

of the SoSym journal who, through their comments

do’s and don’ts of infrastructure code: A systematic gray

and suggestions, have helped considerably to improve

literature review,” Information and Software Technology,

the article’s clarity. This work was partially funded by

the Integrated Masters in Informatics and Computing

vol. 137, p. 106593, 2021.

10. N. D.

loof, H. Yan, J. Cormack, E. Hripko, and

U. Souza, “Docker compose speciﬁcation,” 2021.

https:

Engineering of the Faculty of Engineering, University

//compose-spec.io/.

of Porto (FEUP).

References

11. D. Reis, B. Piedade, F. F. Correia, J. P. Dias, and A. Aguiar,

“Developing docker and docker-compose speciﬁcations: A

developers’ survey,” IEEE Access, 2021.

12. A. Rahman, N. Carolina, C. Parnin, N. Carolina, L. Williams,

and N. Carolina, “Gang of Eight : A Defect Taxonomy for

1. T. B. Sousa, H. S. Ferreira, and F. F. Correia, “Overview of

Infrastructure as Code Scripts.” Accepted submission for the

a Pattern Language for Engineering Software for the Cloud,”

International Conference on Software Engineering (ICSE)

in Proceedings of the 25th Conference on Pattern Languages

2020.

of Programs, PLoP ’18, (USA), pp. 1–9, The Hillside Group,

13. W. Bolton, “Ladder and functional block programming,”

2018.

Jurnal of Sports Science and Medicine, 2006.

Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose

31

14. D. Torres, J. P. Dias, A. Restivo, and H. S. Ferreira, “Real-

24. M. Boshernitsan and M. Downes, “Visual Programming

time feedback in node-red for iot development: an empirical

Languages: A Survey,” Computer Science Division (EECS),

study,” in 2020 IEEE/ACM 24th International Symposium

2004.

on Distributed Simulation and Real Time Applications (DS-

25. M. Burnett, “Visual programming,” Wiley Encyclopedia of

RT), pp. 1–8, IEEE, 2020.

Electrical and Electronics Engineering, vol. 32, no. 1-3,

15. M. Ozkaya and F. Erata,

“A survey on the practical

pp. 275–283, 1999.

use of uml for diﬀerent software architecture viewpoints,”

26. A. Rahman, R. Mahdavi-hezaveh, and L. Williams,

“A

Information and Software Technology, vol. 121, p. 106275,

systematic mapping study of infrastructure as code research,”

2020.

16. P. P. Ray, “A survey on visual programming languages in

internet of things,” Scientiﬁc Programming, vol. 2017, 2017.

Information and Software Technology, vol. 108, 12 2018.

27. S. Baltes and P. Ralph, “Sampling in Software Engineering

Research: A Critical Review and Guidelines.” unpublished,

17. J. P. Dias, J. P. Faria, and H. S. Ferreira, “A reactive

2020.

and model-based approach for developing internet-of-things

systems,”

in 2018 11th International Conference on the

Quality of Information and Communications Technology

(QUATIC), pp. 276–281, 2018.

28. B. Piedade, J. P. Dias, and F. Correia, “docker-composer:

Research replication package,” Aug. 2020.

29. F. D. Davis, “Perceived usefulness, perceived ease of use, and

user acceptance of information technology,” MIS quarterly,

18. F.

Ihirwe, D. Di Ruscio, S. Mazzini, P. Pierini, and

pp. 319–340, 1989.

A. Pierantonio, “Low-code engineering for internet of things:

A state of research,” in Proceedings of the 23rd ACM/IEEE

International Conference on Model Driven Engineering

Languages and Systems: Companion Proceedings, pp. 1–8,

2020.

19. R. McKendrick and S. Gallagher, Mastering Docker - Second

Edition. Packt Publishing, 2017.

20. B. Piedade, J. a. P. Dias, and F. F. Correia,

“An

empirical study on visual programming docker compose

conﬁgurations,”

in Proceedings of

the 23rd ACM/IEEE

International Conference on Model Driven Engineering

Languages and Systems: Companion Proceedings, MODELS

’20, (New York, NY, USA), Association for Computing

Machinery, 2020.

30. F. D. Davis, R. P. Bagozzi, and P. R. Warshaw, “User

acceptance of computer technology: A comparison of two

theoretical models,” Management science, vol. 35, no. 8,

pp. 982–1003, 1989.

31. H. B. Mann and D. R. Whitney, “On a test of whether one of

two random variables is stochastically larger than the other,”

The annals of mathematical statistics, pp. 50–60, 1947.

32. F. Wilcoxon, “Individual comparisons by ranking methods,”

in Breakthroughs in statistics, pp. 196–202, Springer, 1992.

33. Q. McNemar,

“Note on the

sampling error of

the

diﬀerence between correlated proportions or percentages,”

Psychometrika, vol. 12, no. 2, pp. 153–157, 1947.

34. M. Höst, B. Regnell, and C. Wohlin, “Using students as

subjects—a comparative study of students and professionals

in lead-time

impact assessment,” Empirical Software

21. F. Paraiso, S. Challita, Y. Al-Dhuraibi, and P. Merle,

Engineering, vol. 5, pp. 201–214, 11 2000.

“Model-driven management of docker containers,” IEEE

International Conference on Cloud Computing, CLOUD,

pp. 718–725, 2017.

35. I. Salman, A. T. Misirli, and N. Juristo, “Are students

representatives of professionals

in software engineering

experiments?,” in 2015 IEEE/ACM 37th IEEE International

22. J. Sandobalin, E. Insfran, and S. Abrahao, “ARGON: A Tool

Conference on Software Engineering, vol. 1, pp. 666–676,

for Modeling Cloud Resources,” Lecture Notes in Computer

IEEE, 2015.

Science (including subseries Lecture Notes in Artiﬁcial

36. P. Stevens, “Bidirectional model transformations in QVT:

Intelligence and Lecture Notes in Bioinformatics), vol. 10797

Semantic issues and open questions,” Lecture Notes in

LNCS, no. November, pp. 393–397, 2017.

Computer Science (including subseries Lecture Notes in

23. J. Sandobalin, E.

Insfran, and S. Abrahao,

“On the

Artiﬁcial Intelligence and Lecture Notes in Bioinformatics),

Eﬀectiveness of Tools to Support Infrastructure as Code

vol. 4735 LNCS, pp. 1–15, 2007.

: Model-Driven versus Code-Centric,” IEEE Access, vol. 8,

37. L. Angyal, L. Lengyel, and H. Charaf, “A synchronizing

2020.

technique for syntactic model-code round-trip engineering,”

32

Bruno Piedade et al.

Proceedings - Fifteenth IEEE International Conference and

Workshops on the Engineering of Computer-Based Systems,

ECBS 2008, pp. 463–472, 2008.

38. S. Hidaka, Z. Hu, K. Inaba, H. Kato, and K. Nakano,

“GRoundTram: An integrated framework for developing

well-behaved bidirectional model transformations,” 2011

26th IEEE/ACM International Conference on Automated

Software Engineering, ASE 2011, Proceedings, pp. 480–483,

2011.

39. B. Hoisl, Z. Hu, and S. Hidaka, “Towards Bidirectional

Higher-Order Transformation

for Model-Driven Co-

evolution,” Communications in Computer and Information

Science, p. 15, 2015.

40. F. F. Correia and A. Aguiar, “Patterns of ﬂexible modeling

tools,” in Proceedings of the 20th Conference on Pattern

Languages of Programs, pp. 1–17, 2013.

41. A. Aguiar, A. Restivo, F. F. Correia, H. S. Ferreira,

and J. a. P. Dias, “Live software development: Tightening

the feedback loops,”

in Proceedings of

the Conference

Companion of the 3rd International Conference on Art,

Science, and Engineering of Programming, Programming

’19, (New York, NY, USA), Association for Computing

Machinery, 2019.

42. S. Tanimoto,

“A perspective on the evolution of

live

programming,”

International Conference

on Software

Engineering, vol. 41, no. 10, pp. 31–34, 2013.

43. E. Guerra and L. Iovino, eds., MODELS ’20: ACM/IEEE

23rd International Conference on Model Driven Engineering

Languages and Systems, Virtual Event, Canada, 18-23

October, 2020, Companion Proceedings, ACM, 2020.

