2
2
0
2

y
a
M
0
2

]
h
p
-
t
n
a
u
q
[

1
v
1
9
0
0
1
.
5
0
2
2
:
v
i
X
r
a

TensorCircuit: a Quantum Software Framework for the NISQ Era

Shi-Xin Zhang,1 Jonathan Allcock,2 Zhou-Quan Wan,3, 1 Shuo Liu,3, 1 Jiace Sun,4 Hao
Yu,5 Xing-Han Yang,6, 1 Jiezhong Qiu,1 Zhaofeng Ye,1 Yu-Qin Chen,1 Chee-Kong Lee,7
Yi-Cong Zheng,1 Shao-Kai Jian,8 Hong Yao,3 Chang-Yu Hsieh,1, ∗ and Shengyu Zhang1, †

1Tencent Quantum Laboratory, Tencent, Shenzhen, Guangdong 518057, China
2Tencent Quantum Laboratory, Tencent, Hong Kong, China
3Institute for Advanced Study, Tsinghua University, Beijing 100084, China
4Division of Chemistry and Chemical Engineering,
California Institute of Technology, Pasadena, CA 91125, USA
5Department of Electrical and Computer Engineering,
McGill University, Quebec H3A 0E9 , Canada
6Shenzhen Middle School, Shenzhen, Guangdong 518025, China
7Tencent America, Palo Alto, California 94306, USA
8Department of Physics, Brandeis University, Waltham, Massachusetts 02453, USA
(Dated: May 23, 2022)

TensorCircuit is an open source quantum circuit simulator based on tensor network contraction,
designed for speed, ﬂexibility and code eﬃciency. Written purely in Python, and built on top of
industry-standard machine learning frameworks, TensorCircuit supports automatic diﬀerentiation,
just-in-time compilation, vectorized parallelism and hardware acceleration. These features allow
TensorCircuit to simulate larger and more complex quantum circuits than existing simulators, and
are especially suited to variational algorithms based on parameterized quantum circuits. TensorCir-
cuit enables orders of magnitude speedup for various quantum simulation tasks compared to other
common quantum software, and can simulate up to 600 qubits with moderate circuit depth and
low-dimensional connectivity. With its time and space eﬃciency, ﬂexible and extensible architec-
ture and compact, user-friendly API, TensorCircuit has been built to facilitate the design, simulation
and analysis of quantum algorithms in the Noisy Intermediate-Scale Quantum (NISQ) era.

CONTENTS

I. Introduction

A. Challenges in simulating quantum circuits
B. Machine learning libraries
C. The next phase of quantum software

II. TensorCircuit Overview
A. Design philosophy
B. Tensor network engine
C. Installing and contributing to TensorCircuit

III. Circuits and gates

A. Preliminaries
B. Basic circuits and outputs
C. Specifying the input state and composing circuits
D. Circuit transformation and visualization

IV. Gradients, optimization and variational algorithms

A. Optimization via ML backends

∗ kimhsieh@tencent.com
† shengyzhang@tencent.com

2
3
4
5

5
5
6
6

8
8
8
11
11

11
13

 
 
 
 
 
 
B. Optimization via SciPy

V. Density matrices and mixed state evolution

A. Density matrix simulation with tc.DMCircuit
B. Monte Carlo simulation with tc.Circuit
1. Externalizing the randomness

VI. Advanced features

A. Conditional measurements and post-selection

1. Conditional measurements
2. Post-selection

B. Pauli string expectation

1. Pauli structures and weights
2. Explicit loop with c.expectation_ps
3. Expectations of Hamiltonians via operator_expectation
4. vmap over Pauli structures

C. vmap and vectorized_value_and_grad

1. Batched input states
2. Batched circuits
3. Batched cost function evaluation
4. Batched Machine Learning
5. Batched VQE
6. Batched Monte Carlo noise simulation

D. QuOperator and QuVector

1. QuVector as the input state for the circuit
2. QuVector as the output state of the circuit
3. QuOperator as the operator to be measured
4. QuOperator as quantum gates

E. Custom contraction settings

1. Customized contraction path ﬁnder
2. Subtree reconﬁguration

F. Advanced automatic diﬀerentiation

VII. Integrated examples

A. Quantum machine learning
B. Moleculer VQE with TensorCircuit and OpenFermion
C. Demonstration of barren plateaus
D. Very large circuit simulation

VIII. Outlook and concluding remarks

13

14
14
15
16

17
17
17
17
18
18
19
19
20
21
22
23
24
24
25
25
27
28
28
29
29
29
30
31
31

35
35
36
37
38

39

I.

INTRODUCTION

The landscape of open-source and proprietary software for simulating quantum computers [1] has grown
in recent years. While features and functionality vary between packages, across the board users now have
many high-quality options available for constructing and simulating quantum circuits. However, to deepen
our understanding of quantum algorithm performance, researchers increasingly need to simulate larger and
more complex quantum circuits, and to optimize quantum circuits that may contain a large number of
tunable parameters. In spite of the promising state of existing quantum software, there remain a number of
challenges in running large-scale, complex simulations. Here we introduce TensorCircuit , a new open source
tensor network based quantum circuit simulator built to address these challenges. Written in Python, and
designed for speed, ﬂexibility and ease-of-use, TensorCircuit is built on top of a number of industry leading
libraries. Via the TensorFlow [2], JAX [3] and PyTorch [4] machine learning libraries, convenient access

2

is provided for automatic diﬀerentiation, just-in-time compilation, vectorized parallelism, and hardware
acceleration. Fast tensor network contraction is enabled by the state-of-the-art cotengra [5, 6] package,
which also gives users customizable control over the tensor network contraction process. These features
enable eﬃcient optimization of parameterized quantum circuits, allowing for more complex cases to be
modelled. In addition, the TensorCircuit syntax aims to allow complicated tasks to be implemented with a
minimal amount of code, saving time spent coding as well as in simulation.

A. Challenges in simulating quantum circuits

As fully fault-tolerant quantum computers capable of running large scale quantum algorithms may still
be many years away, considerable research eﬀort has been spent investigating the prospects for quantum
advantage in the nearer term. Some algorithms for Noisy Intermediate-Scale Quantum (NISQ) [7] quantum
computers aim to leverage classical computational power to supplement quantum computers, which may
have only a limited number of error-prone qubits under control.
In particular, hybrid quantum-classical
algorithms [8, 9] such as the Variational Quantum Eigensolver (VQE) [10] and the Quantum Approximate
Optimization Algorithm (QAOA) [11] are based around the concept of parameterized quantum circuits
(PQC). These circuits, which contain quantum gates with tunable parameters (for instance, single-qubit
gates with variable rotation angles), are embedded in a classical optimization loop. By optimizing the value
of the parameters in the circuit, one aims to drive the output state of the quantum circuit towards the
solution to a given problem.

In a prototypical VQE example, one wishes to ﬁnd the ground state energy E0 of a quantum system with
Hamiltonian H. The output of a PQC is a quantum state |ψ(θ)(cid:105), where θ is a vector of tunable parameters.
This trial state – known as an ansatz – forms a guess for the ground state wavefunction. By performing
appropriate measurements on |ψ(θ)(cid:105), one can estimate the expected energy (cid:104)H(cid:105)θ = (cid:104)ψ(θ)| H |ψ(θ)(cid:105). By
minimizing (cid:104)H(cid:105)θ with respect to the parameters θ, one obtains an upper bound estimate of the ground state
energy:

E0 ≤ min

θ

(cid:104)H(cid:105)θ.

(1)

There are a number of issues that aﬀect the eﬃcacy and eﬃciency for this approach. Firstly, for an
accurate estimate of E0, the ansatz |ψ(θ)(cid:105) should, for some values of θ, be a good approximation to the
true ground state. Whether this is so depends on the nature of the problem and the complexity of the
PQC from which the ansatz is constructed. On the one hand, simple ansätze – for instance, the "hardware
eﬃcient" ansatz of [12] – may be easier to implement on real or simulated quantum computers, but may
either not be suﬃciently accurate (for instance, due to lack of physically-relevant structure or short depth)
or else suﬀer from other issues, such as so-called barren plateaus in parameter space [13] or local minima in
the energy landscape [14]. On the other hand, more complex ansätze , such as the unitary coupled cluster
(UCC) [15] approach proposed for quantum chemistry problems, may require quantum circuit complexities
and depths beyond what can currently be implemented or simulated, and the associated circuits must then
be truncated or simpliﬁed. The ability to simulate larger and deeper quantum circuits would enable a
systematic investigation of larger classes of ansätze.

Secondly, for a given ansatz, the evaluation of (cid:104)H(cid:105)θ can be an involved process. For instance, consider the
case where H is an n-qubit Hamiltonian, which can be expressed as a weighted sum of tensor products of
Pauli operators, i.e.,

H =

K
(cid:88)

j=1

αjPj,

(2)

where αj are real coeﬃcients, each Pauli string Pj is of the form Pj = σi1 ⊗ σi2 ⊗ . . . σin
are single-
qubit Pauli operators or the identity. In the most straightforward approach, estimating (cid:104)H(cid:105)θ is performed by
ﬁrst estimating the expectation of each term (cid:104)Pj(cid:105)θ and then adding together these individual contributions.
If the Hamilonian consists of many Pauli terms, ways of speeding up the evaluation of Eq. (2) – for instance,

, and σij

3

by exploiting eﬃcient representations of H (e.g. as a sparse matrix or Matrix Product Operator (MPO) [16]),
or being able to compute multiple terms in parallel – can have a large impact on the computation time.

Thirdly, the optimization problem in Eq. (1) is, in general, non-convex. Thus, ﬁnding a global minimum
is in general computationally intractable. However, if gradients of potentially complicated expressions can
be eﬃciently evaluated, one can use gradient descent methods to ﬁnd a local minimum which may yield
a decent approximate solution. By initializing the optimization from a number of diﬀerent positions (i.e.
diﬀerent values of θ), multiple local minima can be obtained, improving the chances that one of these gives
a good solution. If these multiple solutions can be optimized in parallel, one may achieve large eﬃciency
gains.

B. Machine learning libraries

The challenges discussed in the previous section overlap to a large degree with problems faced in machine
learning, and especially deep learning [17]. Fortunately, the need to tackle machine learning problems of
increasing complexity and to deal with datasets of ever-larger size, has led to the development of impressive
software, and many frameworks are now available that combine powerful features with easy-to-use syntax.
In particular:

Fast gradients. At the heart of all advanced machine learning packages is the ability to perform automatic
diﬀerentiation (AD) [18, 19], of which the backpropagation algorithm used to train neural networks is a
special case. AD enables eﬃcient computation of the gradients of functions deﬁned in code, and is vital to
the optimization of many machine learning models.

JIT. Just-in-time compilation is a way of compiling certain parts of code during program execution. For
interpreted languages such as Python, "jitting" a function can lead to large performance gains, with the
time required to execute a jitted function often only a small fraction of the time needed if the function
were interpreted. While the ﬁrst time the function is called there may be an overhead cost required for
compilation (staging time), this cost can be negligible to the time saved if the function is subsequently called
many times.

Vectorization (VMAP). This feature allows a function to be evaluated on multiple points in parallel, with
signiﬁcant speedup compared with using a naïve for loop. In machine learning this allows one, for instance,
to perform computations on batches of data at the same time.

Hardware acceleration. For complex machine learning models, the ability to execute code on multiple
CPUs, GPUs and TPUs may be necessary for training to complete in a reasonable amount of time.

These powerful features are also beneﬁcial in simulating quantum computers, and are particularly suited to
variational quantum algorithms. Here, fast gradient evaluation via automatic diﬀerentiation gives simulators
an inherent advantage over real quantum computers, which must estimate gradients in a less direct way;
for instance by sampling the outputs for various input parameter choices and computing ﬁnite diﬀerences
[20, 21]. Vectorization can (among other things) be used to evaluate PQC circuits with multiple parameter
choices concurrently, or compute expectations of multiple Pauli strings simultaneously, and JIT and hardware
acceleration provide for further time savings.

In addition, there is increasing interest in solving machine learning problems via quantum computing, as
well as combining classical and quantum machine learning algorithms. Both of these can be facilitated by
better integration between classical ML frameworks and quantum circuit simulators, and great value can be
derived from a seamless integration of the two.

4

AD JIT VMAP GPU
TensorFlow (cid:88)(cid:88) (cid:88)(cid:88)(cid:88) (cid:88)(cid:88) (cid:88)(cid:88)(cid:88)
(cid:88)(cid:88)(cid:88) (cid:88)(cid:88)(cid:88) (cid:88)(cid:88)(cid:88) (cid:88)(cid:88)(cid:88)
(cid:88) (cid:88)(cid:88)(cid:88)

JAX

(cid:88)

PyTorch (cid:88)(cid:88)
NumPy

·

·

·

·

Table I. Backends supported by TensorCircuit . Check marks indicate the level of support for the corresponding
features. TensorFlow and JAX both oﬀer comprehensive support for AD, JIT, VMAP and hardware acceleration, and
are recommended for most tasks. If no backend is chosen, TensorCircuit defaults to using NumPy as a backend, which
does not support these advanced ML features. For more on the choice of ML backend, please see the documentation:
faq.html#which-ml-framework-backend-should-i-use .

C. The next phase of quantum software

While quantum software has progressed a great deal in the last few years – with packages such as Qiskit [22],
Cirq [23], HiQ [24], Q# [25] and qulacs [26] all oﬀering powerful functionality and features – there remain
signiﬁcant advantages to be gained from eﬃcient quantum simulation software supplemented with the power
and features of state-of-the-art machine learning. Recently, a new generation of quantum software has started
to emerge, with TensorFlow Quantum [27], Pennylane [28], Paddle Quantum [29] and MindQuantum [30] in
the Python ecosystem making inroads here, and providing these features to varying degrees. However, to
date, none of these fully combine all of the key features of the previous section with fast quantum circuit
simulation. TensorCircuit has been designed to ﬁll this gap, and give users a faster, more ﬂexible and more
convenient way to simulate quantum circuits.

II. TENSORCIRCUIT OVERVIEW

Our goal with TensorCircuit is to provide the ﬁrst eﬃcient, tensor network based quantum simulator that
is fully compatible with the key features of modern machine learning frameworks, especially the program-
ming paradigms of automatic diﬀerentiation, vectorized parallelism and just-in-time compilation. These
features are provided via a number of popular machine learning backends which, at the time of writing, are
TensorFlow, JAX and PyTorch (see Table. I).

Integration with these backends allows for general hybrid quantum-classical models, where the outputs
of a parameterized quantum circuit may be fed into a classical neural network or vice versa, and simulated
seamlessly (see Figure 1). This integration is also key for research related to quantum machine learning [31].
Currently, there are very limited options for tensor network based quantum simulators, with most popular
software making use of state vector simulators. State vector simulators are strongly limited by memory as
wavefunction amplitudes are stored in full, and can thus struggle to simulate circuits with larger numbers
of qubits. On the other hand, quantum circuits with large numbers of (possibly noisy) qubits but relatively
short circuit depths – such as those corresponding to NISQ devices, whose qubits have short coherence times
– fall into the applicable region of tensor network simulators.

A. Design philosophy

With seamless integration with modern machine learning paradigms supported by an eﬃcient tensor
network based simulation engine, the design of TensorCircuit has, from the start, been based on the following
principles.

Speed. TensorCircuit uses tensor network contraction to simulate quantum circuits, and is compatible
with state-of-the-art third party contraction engines. In contrast, the majority of current popular quantum
simulators are state-vector based. The tensor contraction framework allows TensorCircuit , in many cases,

5

to simulate quantum circuits with improved eﬃciency in terms of time and space compared with other
simulators. JIT, AD, VMAP and GPU support can all also provide signiﬁcant acceleration (often, of several
orders of magnitude) in many scenarios.

Flexibility. TensorCircuit is designed to be backend agnostic, making it easy to switch between any of the
machine learning backends with no change in syntax or functionality. Via diﬀerent ML backends, there is
ﬂexibility to simulate hybrid quantum-classical neural networks, run code on CPUs, GPUs and TPUs, and
switch between 32 bit and 64 bit precision data types.

Code eﬃciency. Modern machine learning frameworks such as TensorFlow, PyTorch and JAX have user-
friendly syntax, allowing for powerful tasks to be carried out with a minimal amount of code. With Tensor-
Circuit , we are similarly focused on a compact and easy-to-use API to boost readability and productivity.
Compared with other popular quantum simulation software, TensorCircuit can often perform similar tasks
with signiﬁcantly less code (see tc vs. tfq for VQE for an example). The backend agnostic syntax of Ten-
sorCircuit additionally makes it easy to switch between ML frameworks with a single line of setup code.

Community focused. TensorCircuit is open source software, and we care about readability, maintainability
and extensibility of the codebase. We invite all members of the quantum computing community to take part
in its continued development and use.

B. Tensor network engine

TensorCircuit simulates quantum circuits using the tensor network formalism, which has a long history in
computational physics and was more recently pioneered for quantum circuit simulation [32] . Indeed, the
graphical representations for quantum circuits and tensor networks are consistent, rendering a direct and
simple translation from quantum circuit simulation to tensor network contraction. In this picture, quantum
circuits are represented by a network of low-rank tensors corresponding to individual quantum gates, and
the computation of amplitudes, expectation values or other scalar quantities is performed by contracting the
edges of the network until only a single node remains. The order, or path, in which the edges are contracted
is important, and can have a large impact on the time and space required to contract the network [6, 33, 34].
See [35] for a good introduction to tensor networks from a physics perspective.

In the most general case, as for other types of quantum simulators, the time required to simulate quantum
circuits via tensor network contraction is exponential in the number of qubits. However, in special cases
– including many of practical relevance – tensor network contraction can oﬀer signiﬁcant advantages since
it avoids the memory bottleneck that plagues full state simulators and, to date, the largest scale quantum
computing simulations such as the simulation of random circuits used in quantum supremacy experiments [36,
37] have all been performed via this approach [38–43].

The tensor network data structure used in TensorCircuit is powered by the TensorNetwork [44] package.
In addition, TensorCircuit can utilize state-of-the-art external Python packages such as cotengra for selecting
eﬃcient contraction paths, and the contraction is then performed via einsum and matmul by the machine
learning backend selected by the user (see Figure 2).

Architecture. The tensor network engine underlying the simulation of quantum circuits is built on top of
various machine learning frameworks with an abstraction layer in between that uniﬁes diﬀerent backends. At
the application layer, TensorCircuit also includes various advanced quantum algorithms based on our latest
research [45–49]. The overall software architecture is shown in Figure 3.

C.

Installing and contributing to TensorCircuit

TensorCircuit is open-sourced under the Apache 2.0 license. The software is available on the Python

Package Index (PyPI) and can be installed using the command pip install tensorcircuit.

6

Figure 1. A general hybrid quantum-classical neural network, where a cost function C is summed over a batch of
input states {ρj} and is dependent on the parameters of a quantum circuit and classical neural network. By using a
classical optimizer, the parameters of both networks can be iteratively improved. Integration with classical machine
learning backends allows the entire end-to-end process to be seamlessly simulated in TensorCircuit , with vmap, jit
and automatic diﬀerentiation enabling eﬃciency gains throughout the optimization process.

Figure 2. Schematic of the quantum circuit components in Figure 1. In TensorCircuit , the gates that comprise the
quantum circuit are contained in a tc.Circuit object. Computation of circuit outputs is executed in two steps. First,
a tensor contraction path is determined by a contraction engine, e.g., cotengra, and the backends take care of the
actual contraction.

Figure 3. Software architecture of TensorCircuit . The abstraction layers are shown on the left while representative
APIs are shown on the right. At the bottom, diﬀerent machine learning frameworks such as TensorFlow, JAX
and PyTorch are uniﬁed via a set of backend agnostic APIs. These backends, together with the tensor network
engine, enable eﬃcient quantum circuit simulation and implementation of quantum-classical hybrid algorithms and
applications.

7

ML Frameworkstc.set_backendtc.set_dtypeTensorNetworkCotengratc.backend.jit tc.backend.grad Tensor Network Enginetc.set_contractor tc.QuOperator Quantum Circuittc.Circuit tc.DMCircuit Quantum Algorithmstc.applications tc.templates Applications + Research Unified BackendCircuit ClassThe development of TensorCircuit is open-sourced and centered on GitHub: TensorCircuit Repository . We

welcome all members of the quantum community to contribute, whether it is

• Answering questions on the discussion page or issues page.

• Raising issues such as bug reports or feature requests on the issues page.

• Improving the documentation (docstrings/tutorials) by pull requests.

• Contributing to the codebase by pull requests.

For more details, please refer to the contribution guide: contribution.html .

III. CIRCUITS AND GATES

Jupyter notebook:

3-circuits-gates.ipynb

In TensorCircuit , a quantum circuit on n qubits – which supports both noiseless and noisy simulations
via Monte Carlo trajectory methods – is created by the tc.Circuit(n) API. Here we show how to create basic
circuits, apply gates to them, and compute various outputs.

In the remainder of this document, we assume that we have both TensorCircuit and NumPy imported as

A. Preliminaries

1 import tensorcircuit as tc
2 import numpy as np

Furthermore we assume that a TensorCircuit backend has been set, e.g.

1 K = tc . set_backend ( " tensorflow " )

and the symbol K that appears in code snippets (e.g. K.real()) refers to that backend. Other options for the
set_backend method are “jax”, “pytorch” and "numpy" (the default backend).

In TensorCircuit , qubits are numbered from 0, with multiqubit registers numbered with the zeroth qubit
on the left, e.g. |0(cid:105)q0 |1(cid:105)q1 |0(cid:105)q2
. Unless needed, we will omit subscripts and use compact notation e.g. |010(cid:105)
to denote multiqubit states. X, Y, Z denote the standard single qubit Pauli operators, with subscripts e.g.
X3 to clarify which qubit is acted on. Expectation values of operators with respect to a state |ψ(cid:105) such as
(cid:104)ψ| Z ⊗ I ⊗ X |ψ(cid:105) will be denoted in shorthand as (cid:104)Z0X2(cid:105). Unless stated, expectation values are always with
respect to the output state of a given quantum circuit. If that circuit is parameterized by a set of angles θ,
then the parameter-dependent expectation value may be denoted by (cid:104)·(cid:105)θ.

In TensorCircuit the default runtime datatype is complex64, but if higher precision is required this can be

set as follows:

1 tc . set_dtype ( " complex128 " )

B. Basic circuits and outputs

Consider the following two-qubit quantum circuit consisting of a Hadamard gate on qubit q0, a CNOT on
qubits q0 and q1, and a single qubit rotation RX (0.2) of qubit q1 by angle 0.2 about the X axis (see Figure 4).
Qubits are numbered from 0 with q0 on the top row. This circuit can be implemented in TensorCircuit as

1 c = tc . Circuit (2)
2 c . h (0)
3 c . cnot (0 , 1)
4 c . rx (1 , theta =0.2)

8

H

|0(cid:105)

|0(cid:105)

Rx(0.2)

Figure 4. A two-qubit circuit consisting of Hadamard, CNOT and single-qubit RX rotation.

From this, various outputs can be computed.

Basic outputs. The full wavefunction can be obtained via

1 c . state ()

which will output an array [α00, α01, α10, α11] corresponding to the amplitudes of the |00(cid:105) , |01(cid:105) , |10(cid:105) , |11(cid:105)
basis states. The full wavefunction can also be used to generate the reduced density matrix of a subset of
the qubits, e.g.

1 # reduced density matrix for qubit 1
2 s = c . state ()
3 tc . quantum . r e d u c e d _ d e n s i t y _ m a t r i x (s , cut =[0])

# cut : list of qubit indices to trace out

Amplitudes of individual basis vectors are computed by passing the corresponding bit-string value to the
amplitude function. For example, the amplitude of the |10(cid:105) basis vector is computed by

1 c . amplitude ( " 10 " )

The unitary matrix corresponding to the entire quantum circuit can also be output:

1 c . matrix ()

Measurements and samples. Random samples corresponding to Z-measurements , i.e. in the {|0(cid:105) , |1(cid:105)}
basis, on all qubits can be generated using

1 c . sample ()

which will output a (bitstring, probability) tuple, comprising a binary string corresponding to the mea-
surement outcomes of a Z measurement on all the qubits and the associated probability of obtaining that
outcome. Z measurements on a subset of qubits can be performed with the measure command

1 # return ( outcome , probability ) of measuring qubit 0 in Z basis
2 print ( c . measure (0 , with_prob = True ) )

For measurement of multiple qubits, simply provide a list of indices to measure, e.g.
circuit, measurement of qubits 1, 3 can be done via

if c were a 4-qubit

1 c . measure (1 , 3 , with_prob = True )

Note that measurement gates do not need to be explicitly added to the circuit in order to compute these
outcomes and the measure and sample commands do not collapse the circuit output state. Measurement gates
can be added and used, for instance, when gates must be applied conditioned on mid-circuit measurement
outcomes (see Section VI A).

Expectation values. Expectation values such as (cid:104)X0(cid:105), (cid:104)X1 + Z1(cid:105) or (cid:104)Z0Z1(cid:105) can be computed via the
expectation method of a circuit object, where the operator is deﬁned via Gate object or simply an array.

1 c . expectation ([ tc . gates . x () , [0]])
2 c . expectation ([ tc . gates . x () + tc . gates . z () , [1]])
3 c . expectation ([ tc . gates . z () , [0]] , [ tc . gates . z () , [1]])

# <X0 >
# < X1 + Z1 >
# < Z0 Z1 >

9

and expectations of user-deﬁned operators can also be computed by supplying the corresponding array of
matrix elements. For instance, the operator 2X + 3Z can be expressed as a matrix as

(cid:32)

(cid:33)

3 2
2 −3

and implemented (assuming the observable is measured on qubit 0) as

1 c . expectation ([ np . array ([[3 , 2] , [2 , -3]]) , [0]])

Expectations of Pauli strings. While expectations of products of Pauli operators, e.g. (cid:104)Z0X1(cid:105) can be
computed using c.expectation as above, TensorCircuit provides another way of computing such expressions
which may be more convenient for longer Pauli strings:

1 c . expectation_ps ( x =[1] , z =[0])

and longer Pauli strings can similarly be computed by providing lists of indices corresponding to the qubits
that the X, Y, Z operators act on. For example, for an n = 5 qubit circuit, the expectation value (cid:104)Z0X1Z2Y4(cid:105)
is computed as

1 c . expectation_ps ( x =[1] , y =[4] , z =[0 , 2])

Standard quantum gates. Beyond the CNOT, Hadamard and RX gates we have encountered so far,
TensorCircuit provides support for a wide variety of commonly encountered quantum gates. The full list of
gates can be found by querying

1 tc . Circuit . sgates # non - parameterized gates
2 tc . Circuit . vgates # parameterized gates

The matrix corresponding to a given gate, e.g. the Hadamard h gate, can be accessed in the following way

1 tc . gates . m atrix_for_gate ( tc . gates . h () )

Arbitrary unitaries. User-deﬁned unitary gates may be implemented by specifying their matrix elements
(cid:33)

(cid:32)

– which can also directly be added by calling c.s() –

as an array. As an example, the unitary S =

can be implemented as

1 0
0 i

1 c . unitary (0 , unitary = np . array ([[1 ,0] ,[0 ,1 j ]]) , name = ’S ’)

where the optional name argument speciﬁes how this gate is displayed when the circuit is output to LATEX.

Exponential gates. Gates of the form eiθG where matrix G satisﬁes G2 = I admit a fast implementation
via the exp1 command, e.g., the two qubit gate eiθZ⊗Z acting on qubits 0 and 1

1 c . exp1 (0 , 1 , theta =0.2 , unitary = tc . gates . _zz_matrix )

where tc.gates._zz_matrix creates a numpy array corresponding to the the matrix








Z ⊗ Z =

1 0
0 0
0 −1 0 0
0 0 −1 0
0 1
0 0








.

General exponential gates, where G2 (cid:54)= I can be implemented via the exp command:

1 c . exp (0 , theta =0.2 , unitary = np . array ([[2 , 0] ,[0 , 1]]) )

10

Non-unitary gates. TensorCircuit also supports the application of non-unitary gates by supplying a non-
unitary matrix as the argument to c.unitary, e.g.

1 c . unitary (0 , unitary = np . array ([[1 ,2] ,[2 ,3]]) , name = ’ non_unitary ’)

Note that non-unitary gates will lead to an output state that is no longer normalized, since normalization

is often unnecessary and takes additional computational time.

C. Specifying the input state and composing circuits

By default, quantum circuits are applied to the initial all-zero product state. Arbitrary initial states can
be set by passing an array containing the input state amplitudes to the optional inputs argument of tc.Circuit.
For example, the maximally entangled state |00(cid:105)+|11(cid:105)

can be input as follows:

√

2

1 c1 = tc . Circuit (2 , inputs = np . array ([1 , 0 , 0 , 1] / np . sqrt (2) ) )

Input states in Matrix Product State (MPS) form can also be input via the optional mps_inputs argument
of tc.Circuit. See Section VI D for details.

Circuits that act on the same number of qubits can be composed together via the c.append() or c.prepend()

commands. With c1 deﬁned as above, we can create a new circuit c2 and then compose them together:

1 c2 = tc . Circuit (2)
2 c2 . cnot (0 , 1)
3
4 c3 = c1 . append ( c2 )

This leads to a circuit C3 which is equivalent to ﬁrst applying C1 and then C2.

D. Circuit transformation and visualization

tc.Circuit objects can be converted to and from Qiskit QuantumCircuit objects. Export to Qiskit is done

by

1 c . to_qiskit ()

and the resulting QuantumCircuit object can then be compiled and run on compatible physical quantum
processors and simulators. Conversely, importing a QuantumCircuit object from Qiskit is done via

1 c = tc . Circuit . from_qiskit ( QuantumCircuit )

There are two ways to visualize quantum circuits generated in TensorCircuit . The ﬁrst is to use

1 print ( c . tex () )

which outputs the code for drawing the associated quantum circuit using the LATEXquantikz package [50].
The second method uses the draw function:

1 c . draw ()

which is a shortcut for

1 qc = c . to_qiskit ()
2 qc . draw ()

Underlying circuit transformation and visualization utilities is the quantum intermediate representation

(IR) for TensorCircuit Circuit objects, which can be obtained by

1 c . to_qir ()

IV. GRADIENTS, OPTIMIZATION AND VARIATIONAL ALGORITHMS

11

Jupyter notebook:

4-gradient-optimization.ipynb

TensorCircuit is designed to make optimization of parameterized quantum gates easy, fast and convenient.
Consider a variational circuit acting on n qubits, and consisting of k layers, where each layer comprises
parameterized eiθX⊗X gates between neighboring qubits followed by a sequence of single qubit parameterized
Z and X rotations:

|0(cid:105)

|0(cid:105)

|0(cid:105)

Rz(θ)

Rx(θ)

Rz(θ)

Rx(θ)

eiθXX

eiθXX

Rz(θ)

Rx(θ)

Rz(θ)

Rx(θ)

eiθXX

eiθXX

Rz(θ)

Rx(θ)

Rz(θ)

Rx(θ)

Figure 5. A parameterized, layered quantum circuit. Each gate is dependent on a separate parameter, here all
schematically represented as θ.

We now show how to implement such circuits in TensorCircuit , and how to use one of the machine learning
backends to compute cost functions and gradients easily and eﬃciently. The circuit for general n, k and set
of parameters can be deﬁned as follows:

1 def qcircuit (n , k , params ) :
c = tc . Circuit ( n )
2
for j in range ( k ) :

3

4

5

6

7

8

9

10

11

for i in range ( n - 1) :

c . exp1 (

i , i + 1 , theta = params [ j * (3 * n - 1) + i ] , unitary = tc . gates . _xx_matrix

)

for i in range ( n ) :

c . rz (i , theta = params [ j * (3 * n - 1) + n - 1 + i ])
c . rx (i , theta = params [ j * (3 * n - 1) + 2 * n - 1 + i ])

return c

As an example, we take n = 3, k = 2, set TensorFlow as our backend, and deﬁne an energy cost function to
be minimized

E = (cid:104)X0X1(cid:105)θ + (cid:104)X1X2(cid:105)θ.

1 n = 3
2 k = 2
3
4 K = tc . set_backend ( " tensorflow " )
5

6
7 def energy ( params ) :
8

c = qcircuit (n , k , params )
e = c . expectation_ps ( x =[0 , 1]) + c . expectation_ps ( x =[1 , 2])
return K . real ( e )

9

10

K.grad and K.value_and_grad. Using the ML backend support for automatic diﬀerentiation, we can now
quickly compute both the energy and the gradient of the energy (with respect to the parameters):

1 ene rgy_val_ grad = K . value_and_grad ( energy )

This creates a function which, given a set of parameters as input, returns both the energy and the gradient
of the energy. If only the gradient is desired, then this can be computed by K.grad(energy). While we could
run the above code directly on a set of parameters, if multiple evaluations of the energy will be performed,
signiﬁcant time savings can be had by using a just-in-time compiled version of the function:

12

1 e n e r g y _ v al _ g r a d _ j i t = K . jit ( energy_val_grad )

With K.jit, the initial evaluation of the energy and gradient may take longer, but subsequent evaluations
will be noticeably faster than non-jitted code. We recommend always using jit as long as the function is
‘tensor-in, tensor-out’, and we have worked hard to make all aspects of the circuit simulator compatible with
JIT.

A. Optimization via ML backends

With the energy function and gradients available, optimization of the parameters is straightforward. Below

is an example of how to do this via stochastic gradient descent:

1 learning_rate = 2e -2
2 opt = K . optimizer ( tf . keras . optimizers . SGD ( learning_rate ) )
3

4
5 def grad_descent ( params , i ) :
6

val , grad = e n e r g y _ v a l _ g r a d _j i t ( params )
params = opt . update ( grad , params )
if i % 10 == 0:

print ( f " i ={ i } , energy ={ val } " )

return params

7

8

9

10

11

12
13 params = K . implicit_randn ( k * (3 * n - 1) )
14 for i in range (200) :
15

params = grad_descent ( params , i )

While this example was done with the TensorFlow backend, switching to JAX can be done easily. All that
is required is to redeﬁne the optimizer opt using the JAX optimization library optax [51]:

1 import optax
2 opt = tc . backend . optimizer ( optax . sgd ( learning_rate )

Then, choose JAX as the backend via

1 K = tc . set_backend ( " jax " )

and perform the gradient descent exactly as above. Note that if no backend is set explicitly, TensorCircuit
defaults to using NumPy as the backend, which does not allow for automatic diﬀerentiation.

B. Optimization via SciPy

An alternative to using the machine learning backends for the optimization is to use SciPy. This can be
done via the scipy_interface API call, and allows for gradient based (e.g. BFGS) and non-gradient based
(e.g. COBYLA) optimizers to be used, which are not available via the ML backends.

1 import scipy . optimize as optimize
2
3 f_scipy = tc . interfaces . scipy_interface ( energy , shape =[ k * (3 * n - 1) ] , jit = True )
4 params = K . implicit_randn ( k * (3 * n - 1) )
5 r = optimize . minimize ( f_scipy , params , method = "L - BFGS - B " , jac = True )

The ﬁrst line above speciﬁes the shape of the parameters to be supplied to the function to be minimized,
which here is the energy function. The jit=True argument automatically takes care of jitting the energy
function. Gradient-free optimization can similarly be performed eﬃciently by supplying the gradient=False
argument to scipy_interface:

1 f_scipy = tc . interfaces . scipy_interface (
2
3 )

energy , shape =[ k * (3 * n - 1) ] , jit = True , gradient = False

13

4 params = K . implicit_randn ( k * (3 * n - 1) )
5 r = optimize . minimize ( f_scipy , params , method = " COBYLA " )

V. DENSITY MATRICES AND MIXED STATE EVOLUTION

Jupyter notebook:

5-density-matrix.ipynb

TensorCircuit provides two methods for simulating noisy, mixed state quantum evolution. Full density
matrix simulation of n qubits is provided by using tc.DMCircuit(n), and then adding quantum operations
– both unitary gates as well as general quantum operations speciﬁed by Kraus operators – to the circuit.
Relative to pure state simulation of n qubits via tc.Circuit, full density matrix simulation is twice as memory
intensive, and thus the maximum system size simulatable will be half of what can be simulated in the pure
state case. A less memory intensive option is to use the standard tc.Circuit(n) object and stochastically
simulate open system evolution via Monte Carlo methods.

A. Density matrix simulation with tc.DMCircuit

We illustrate this method below, by considering a simple circuit on a single qubit, which takes as input

the mixed state corresponding to a probabilistic mixture of the |0(cid:105) state and the maximally mixed state

This state is then passed through a circuit which applies an X gate, followed by a quantum operation
corresponding to an amplitude damping channel Eγ with parameter γ. This has Kraus operators

ρ(α) = α|0(cid:105)(cid:104)0| + (1 − α)I/2.

K0 =

(cid:32)
1
0

(cid:33)

√

0
1 − γ

, K1 =

(cid:32)
0
0

(cid:33)

√

γ
0

This circuit thus induces the evolution

ρ(α) X−→ Xρ(α)X

Eγ−→

1
(cid:88)

i=0

KiXρ(α)XK †
i

To simulate this in TensorCircuit , we ﬁrst create a tc.DMCircuit (density matrix circuit) object and set the
input state using the dminputs optional argument (note that if a pure state input is provided to tc.DMCircuit,
this should be done via the inputs optional argument rather than dminputs). ρ(α) has the matrix form

(cid:32) 1+α
2

ρ(α) =

(cid:33)

,

1−α
2

and thus (taking α = 0.6) we initialize the density matrix circuit as follows

1 def rho ( alpha ) :
2

return np . array ([[(1 + alpha ) / 2 , 0] , [0 , (1 - alpha ) / 2]])

3

4
5 input_state = rho (0.6)
6 dmc = tc . DMCircuit (1 , dminputs = input_state )

Adding the X gate (and other unitary gates) is done in the same way as for pure state circuits:

1 dmc . x (0)

14

To implement a general quantum operation such as the amplitude damping channel, we use general_kraus,
supplied with the corresponding list of Kraus operators.

1 def amp_damp_kraus ( gamma ) :
2

K0 = np . array ([[1 , 0] , [0 , np . sqrt (1 - gamma ) ]])
K1 = np . array ([[0 , np . sqrt ( gamma ) ] , [0 , 0]])
return K0 , K1

3

4

5

6
7 K0 , K1 = amp_damp_kraus (0.3)
8 dmc . general_kraus ([ K0 , K1 ] , 0)

# apply channel with Kraus operators [ K0 , K1 ] to qubit 0

and the full density matrix output can be returned via

1 dmc . state ()

In this example we input the Kraus operators for the amplitude damping channel manually, in order to
illustrate the general approach to implementing quantum channels with the tc.DMCircuit method. In fact,
TensorCircuit includes built-in methods for returning the Kraus operators for a number of common channels,
including the amplitude damping, depolarizing, phase damping and reset channels. For example, the Kraus
operators for the phase damping channel with parameter γ

K0 =

(cid:32)
1
0

(cid:33)

√

0
1 − γ

, K1 =

(cid:32)
0
0

(cid:33)

0
√
γ

can be returned by calling

1 gamma = 0.3
2 K0 , K1 = tc . channels . p h a s e d a m p i n g c ha n n e l ( gamma )

and the phase damping channel added to a circuit via

1 dmc . general_kraus ([ K0 , K1 ] , 0)

The above operation can be further simpliﬁed using one API call:

1 dmc . phasedamping (0 , gamma =0.3)

B. Monte Carlo simulation with tc.Circuit

Monte Carlo methods can be used to sample noisy quantum evolution using tc.Circuit instead of
tc.DMCircuit, where the mixed state is eﬀectively simulated by an ensemble of pure states. As for den-
sity matrix simulation, quantum channels E can be added to a circuit object by providing a list of their
associated Kraus operators {Ki}. The API is the same as for the full density matrix simulation:

1 input_state = np . array ([1 , 1] / np . sqrt (2) )
2 c = tc . Circuit (1 , inputs = input_state )
3 c . general_kraus ( tc . channels . p h a s ed a m p i n g c h a n n e l (0.5) , 0)
4 c . state ()

In this framework though, the output of a channel acting on |ψ(cid:105) , i.e.

E (|ψ(cid:105) (cid:104)ψ|) =

Ki|ψ(cid:105)(cid:104)ψ|K †
i

(cid:88)

i

is viewed as an ensemble of states

√

Ki|ψ(cid:105)

that each occurs with probability pi = (cid:104)ψ| K †

(cid:104)ψ|K†

i Ki|ψ(cid:105)

the code above stochastically produces the output of a single qubit initialized in state |ψ(cid:105) = |0(cid:105)+|1(cid:105)
passed through a phase damping channel with parameter γ = 0.5.

√

2

i Ki |ψ(cid:105). Thus,
being

15

The Monte Carlo simulation of channels where the Kraus operators are all unitary matrices (up to a
constant factor) can be more eﬃciently handled by using unitary_kraus instead of general_kraus. For instance,
the depolarizing channel with Kraus operators parameterized by px, py, pz:
(cid:33)

(cid:33)

(cid:33)

(cid:32)

(cid:32)

(cid:32)

(cid:33)

(cid:32)

, K1 = px

, K2 = py

, K3 = pz

0 −i
i 0

1 0
0 −1

0 1
1 0

K0 = (1 − px − py − pz)

can be implemented by

1 0
0 1

1 px , py , pz = 0.1 , 0.2 , 0.3
2 c . unitary_kraus ( tc . channels . d e p o la r i z i n g c h a n n e l ( px , py , pz ) , 0)

where, in the second line, tc.channel.depolarizingchannel(px, py, pz) returns the required Kraus operators.

1. Externalizing the randomness

The general_kraus and unitary_kraus examples above both handle randomness generation from inside the
respective methods. That is, when the list [K0, K1, . . . , Km−1] of Kraus operators is supplied to general_kraus
or unitary_kraus, the method partitions the interval [0, 1] into m contiguous intervals [0, 1] = I0 ∪I1 ∪. . . Im−1
where the length of Ii is proportional to the relative probability of obtaining the outcome i. Then a
uniformly random variable x in [0, 1] is drawn from within the method, and outcome i is selected based
on which interval x lies in.
In TensorCircuit , we have full backend agnostic infrastructure for random
number generation and management. However, the interplay between jit, random numbers and back-
end switching is often subtle if we rely on the random number generation inside these methods. See
advance.html#randoms-jit-backend-agnostic-and-their-interplay for details.

In some situations, it may be preferable to ﬁrst generate the random variable from outside the method,
and then pass the value generated into general_kraus or unitary_kraus. This can be done via the optional
status argument:

1 px , py , pz = 0.1 , 0.2 , 0.3
2 x = K . impli cit_randn ()
3 c . unitary_kraus ( tc . channels . d e p o la r i z i n g c h a n n e l ( px , py , pz ) , 0 , status = x )

This is useful, for instance, when one wishes to use vmap to batch compute multiple runs of a Monte Carlo
simulation. This is illustrated in the example below, where vmap is used to compute 10 runs of the simulation
in parallel:

1 def f ( x ) :
2

3

4

5

6

c = tc . Circuit (1)
c . h (0)
c . unitary_kraus ( tc . channels . d e p o la r i z i n g c h a n n e l (0.1 , 0.2 , 0.3) , 0 , status = x )
return c . state ()

7
8 f_vmap = K . vmap (f , ve c to r iz e d_ ar g nu m s =0)
9 X = K . impli cit_randn (10)
10 f_vmap ( X )

Conceptually, the line

1 f_vmap = K . vmap (f , ve c to r iz e d_ ar g nu m s =0)

creates a function which acts as

fvmap




 =






x0
x1
...











f (x0)
f (x1)
...

and the argument vectorized_argnums=0 indicates that is the zeroth argument (in this case the only argu-
ment) of f that we wish to batch compute in parallel.

16

VI. ADVANCED FEATURES

A. Conditional measurements and post-selection

Jupyter notebook:

6-1-conditional-measurements-post-selection.ipynb

TensorCircuit allows for two kinds of operations to be performed that are related to measurement out-
comes. These are (i) conditional measurements, the outcomes of which can be used to control downstream
conditional quantum gates, and (ii) post-selection, which allows the user to select the post-measurement
state corresponding to a particular measurement outcome.

1. Conditional measurements

The cond_measure command is used to simulate the process of performing a Z measurement on a qubit,
generating a measurement outcome with probability given by the Born rule, and collapsing the wavefunction
in accordance with the measured outcome. The classical measurement outcome obtained can then act as a
control for a subsequent quantum operation via the conditional_gate API and can be used, for instance, to
implement the canonical teleportation circuit.

1 # quantum teleportation of state | psi > = a |0 > + sqrt (1 - a ^2) |1 >
2 a = 0.3
3 input_state = np . kron ( np . array ([ a , np . sqrt (1 - a ** 2) ]) , np . array ([1 , 0 , 0 , 0]) )
4
5 c = tc . Circuit (3 , inputs = input_state )
6 c . h (2)
7 c . cnot (2 , 1)
8 c . cnot (0 , 1)
9 c . h (0)
10
11 # mid - circuit measurements
12 z = c . cond_measure (0)
13 x = c . cond_measure (1)
14
15 # if x = 0 apply I , if x = 1 apply X ( to qubit 2)
16 c . c ond i tio n al_gate (x , [ tc . gates . i () , tc . gates . x () ] , 2)
17
18 # if z = 0 apply I , if z = 1 apply Z ( to qubit 2)
19 c . c ond i tio n al_gate (z , [ tc . gates . i () , tc . gates . z () ] , 2)

|ψ(cid:105)

|0(cid:105)

|0(cid:105)

H

H

X

Z

Figure 6. Teleportation circuit implemented with c.cond_measure and c.conditional_gate.

2. Post-selection

Post-selection is enabled in TensorCircuit via the post_select method. This allows the user to select the
post-Z-measurement state of a qubit via the keep argument. Unlike cond_measure, the state returned by
post_select is not normalized. As an example, consider

17

1 c = tc . Circuit (2 , inputs = np . array ([1 , 0 , 0 , 1] / np . sqrt (2) ) )
2 c . post_select (0 , keep =1)
3 c . state ()

# measure qubit 0 , post - select on outcome 1

which initializes a 2-qubit maximally entangled state |ψ(cid:105) = |00(cid:105)+|11(cid:105)
in the Z-basis, and the unnormalized state |11(cid:105) /
post-selected.

. The ﬁrst qubit (q0) is then measured
2 corresponding to the measurement outcome 1 being

√

√

2

This post-selection scheme with unnormalized states is fast and can, for instance, be used to explore
various quantum algorithms and nontrivial quantum physics such as measurement-induced entanglement
phase transitions [52–55].

B. Pauli string expectation

Jupyter notebook:

6-2-pauli-string-expectation.ipynb

Minimizing the expectation values of sums of Pauli strings is a common task in quantum algorithms.
For instance, in the VQE ground state preparation of an n-site transverse-ﬁeld Ising model (TFIM) with
Hamiltonian

H =

n−2
(cid:88)

i=0

JiXiXi+1 −

n−1
(cid:88)

i=0

hiZi,

where the Ji, hi are model parameters, one wishes to minimize

E(θ) = (cid:104)H(cid:105)θ :=

n−2
(cid:88)

i=0

Ji(cid:104)XiXi+1(cid:105)θ −

n−1
(cid:88)

i=0

hi(cid:104)Zi(cid:105)θ

(3)

with respect to the circuit parameters θ. TensorCircuit provides a number of ways to compute expressions
of this form, useful in diﬀerent scenarios. At a high level these are

• Looping over terms, each computed by c.expectation_ps.

• Supplying a dense matrix, sparse matrix or MPO representation of the Hamiltonian to the opera-

tor_expectation function.

• Using vmap to compute each of the terms in a vectorized parallel manner, where each term is input as

a structure vector.

Underlying these methods are a variety of ways of representing strings of Pauli operators and converting
between these representations. Before going through the above methods in more detail, let us introduce the
Pauli structure vector representation utilized in TensorCircuit .

1. Pauli structures and weights

A string of Pauli operators acting on n qubits can be represented as a length-n vector v ∈ {0, 1, 2, 3}n,
where the value of vi = j corresponds to σj
, i.e. Pauli operator σj acting on qubit i (with σ0 = I, σ1 =
i
X, σ2 = Y, σ3 = Z). For example, in this notation, if n = 3 the term X1X2 corresponds to v = [0, 1, 1].
We refer to such a vector representation of a Pauli string as a structure, and a list of structures, one for
each Pauli string term in the Hamiltonian, is used as the input to compute sums of expectation values in a
number of ways.

18

1 # Pauli structures for Transverse Field Ising Model
2 structures = []
3 for i in range ( n - 1) :
4

s = [0 for _ in range ( n ) ]
s [ i ] = 1
s [ i + 1] = 1
structures . append ( s )

7
8 for i in range ( n ) :
9

s = [0 for _ in range ( n ) ]
s [ i ] = 3
structures . append ( s )

5

6

10

11

If each structure has an associated weight, e.g. the term XiXi+1 has weight Ji in Hamiltonian (3), then we
deﬁne a corresponding tensor of weights

1 # Weights , taking J_i = 1.0 , all h_i = -1.0
2 J_vec = [1.0 for _ in range ( n - 1) ]
3 h_vec = [ -1.0 for _ in range ( n ) ]
4 weights = tc . array_to_tensor ( np . array ( J_vec + h_vec ) )

2. Explicit loop with c.expectation_ps

As introduced in Section III B, given a TensorCircuit quantum circuit c, a single Pauli string expectation
can be computed by supplying a list of indices to c.expectation_ps. The sum can then be computed by using
a straightforward loop:

1 def tfim_energy (c , J_vec , h_vec )
2

e = 0.0
n = c . _nqubits
for i in range ( n ) :

e += h_vec [ i ] * c . expectation_ ps ( z =[ i ])

for i in range (n -1) :

e += J_vec [ i ] * c . expectation_ ps ( x =[ i , i +1])

return K . real ( e )

3

4

5

6

7

8

3. Expectations of Hamiltonians via operator_expectation

Given a TensorCircuit quantum circuit c and an operator op representing the Hamiltonian, the expectation
value of the energy can also be computed via the operator_expectation API from the TensorCircuit templates
library

1 e =

tc . templates . measurements . o p e r a t o r _ e x p e c t a t i o n (c , op )

The operator op itself can be expressed in one of three forms: (i) dense matrix, (ii) sparse matrix, and (iii)
Matrix Product Operator (MPO).

Dense Matrix Input. As a simple example, take the Hamiltonian

This has dense matrix representation

X0X1 − Z0 − Z1.

−2








1

1

1



1





2

and the expectation value of the Hamiltonian (with respect to circuit c) can be computed by

19

1 op = tc . a rray_to_tensor ([[ -2 , 0 , 0 , 1] , [0 , 0 , 1 , 0] , [0 , 1 , 0 , 0] , [1 , 0 , 0 , 0]])
2 e = tc . templates . measurements . o p e r a t o r _ e x p e c t a t i o n (c , op )

The matrix elements above were input by hand. TensorCircuit also provides a way of generating the matrix
elements from the associated Pauli structures and weights, e.g.

1 structure = [[1 , 1] , [0 , 3] , [3 , 0]]
2 weights = [1.0 , -1.0 , -1.0]
3 H_dense = tc . quantum . P a u l i S t r i n g S u m 2 D e n s e ( structure , weights )

Sparse Matrix Input. Signiﬁcant computational advantage in terms of space and time can be obtained if
the Hamiltonian is sparse, in which case a sparse representation of the operator is preferable. This can be
implemented in a backend agnostic way by converting from a list of Pauli structures in a two-stage process.
First we convert to a sparse numpy matrix in COO (COOrdinate) format. For example, with the structures
and weights deﬁned in Section VI B 1, we call

1 H_sparse_ numpy = tc . quantum . P a u l i S t r i n g S u m 2 C O O _ n u m p y ( structures , weights )

Then we can convert to a sparse tensor compatible with the selected backend K

1 H_sparse = K . c oo_ spa rse _ma tri x (
2

np . transpose ( np . stack ([ H_sparse_numpy . row , H_sparse_numpy . col ]) ) ,
H_sparse_numpy . data ,
shape =(2 ** n , 2 ** n ) ,

3

4
5 )

and then call operator_expectation on this sparse tensor

1 e = tc . templates . measurements . o p e r a t o r _ e x p e c t a t i o n (c , H_sparse )

MPO Input. The TFIM Hamiltonian, as a short-ranged spin Hamiltonian, admits an eﬃcient Matrix
Product Operator representation. Again this is a two-stage process using TensorCircuit . We ﬁrst convert
the Hamiltonian into an MPO representation via the TensorNetwork [44] or Quimb package [56]:

1 # generate the corresponding MPO by converting the MPO in tensornetwork package
2
3 Jx = np . array ([1.0 for _ in range ( n - 1) ])
4 Bz = np . array ([1.0 for _ in range ( n ) ])
5 # Note the convention for the sign of Bz
6 ham iltonia n_mpo = tn . m a t ri x p r o d u c t s t a t e s . mpo . FiniteTFI ( Jx , Bz , dtype = np . complex64 )

# strength of xx interaction ( OBC )

# strength of transverse field

and then convert the MPO into a QuOperator object compatible with TensorCircuit .

1 ham iltonia n_mpo = tc . quantum . tn2qop ( hamiltonian_mpo )

# QuOperator in TensorCircuit

The expectation value of the energy can then be computed via operator_expectation

1 e = tc . templates . measurements . o p e r a t o r _ e x p e c t a t i o n (c , hamiltonian_mpo )

4.

vmap over Pauli structures

Given a state |s(cid:105), the expectation value (cid:104)s| P |s(cid:105) of a Pauli string P with a given structure can be computed

as a function of tensor inputs as

1 # assume the following are defined
2 # state : 2** n vector of coefficients corresponding to input state
3 # structure : Pauli structure ( i . e . list of integers {0 ,1 ,2 ,3}** n )
4
5 structure = tc . array_to_tensor ( structure )
6 state = tc . array_to_tensor ( state )
7

8

20

CPU
time (s)
65.7
explicit loop
0.68
vmap over Pauli structures
dense matrix representation
0.26
sparse matrix representation 0.008

GPU
119
0.0018
0.0015
0.0014

Table II. Performance benchmarks for Pauli string sum evaluation methods, with Hamiltonian corresponding to an
H2O molecule. 12 qubits are used for the binary encoding of STO-3G orbitals. The qubit Hamiltonian contains
1390 Pauli string terms in total. Data was obtained using the JAX backend. GPU simulations were performed
on the Nvidia T4 GPU, while CPU simulations used Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz. The MPO
representation is not applicable in this case, since the required bond dimension is too large.

9 def e ( state , structure ) :
10

c = tc . Circuit (n , inputs = state )
return tc . templates . measurements . p a r a m e t e r i z e d _ m e a s u r e m e n t s (

11

12

13

c , structure , onehot = True

)

where the parameterized_measurements function is used to compute the expectation of the output of a circuit.
Then, if the Hamiltonian is represented by a list of Pauli structures [v1, . . . , vk], vmap can be used to compute
the expectation with respect to the circuit c of each term in parallel:

1 e_vmap = K . vmap (e , ve c to r iz e d_ ar g nu m s =1)

Similar to the example in Section V B 1, vmapping creates a function which acts as





evmap


s,












 =

← v1 →
...
← vk →











e(s, v1)
...
e(s, vk)

i.e., outputs a vector of expectation values corresponding to terms in the Hamiltonian, and

1

v ec t or i z e d _a r gn u ms =1

indicates that it is the ﬁrst argument v of the e(s,v) function which should be computed in parallel, while the
zeroth argument (i.e. s) is ﬁxed. With structures and weights as deﬁned in Section VI B 1, the expectation
value of the Hamiltonian with respect to the circuit c can then be computed as

1 s = c . state ()
2 e_terms = e_vmap (s , structures )
3 h a m i l t o n i a n _ e x p e c t a t i o n = K . sum ( e_terms * K . real ( weights ) )

We benchmark these diﬀerent approaches to Pauli string sum evaluation for Hamiltonians correspond-
ing to an H2O molecule and the TFIM spin model, with results in Tables II and III, respectively. See
examples/vqeh2o_benchmark.py and examples/vqetﬁm_benchmark.py for more details. In the H2O case,
we observe an acceleration of 85,000 times for VQE evaluation using the sparse matrix representation com-
pared to a naïve loop as used in most other quantum software.

C.

vmap and vectorized_value_and_grad

Jupyter notebook:

6-3-vmap.ipynb

As we have seen in Sections V B 1 and VI B 4, vmap allows for batches of function evaluations to be
performed simultaneously in parallel. If batch evaluation of gradients as well as function values is required,
In the simplest case, consider a function f (x, y)
then this can be done via vectorized_value_and_grad.

21

CPU
time (s)
1.73
explicit loop
10.68
vmap over Pauli structures
sparse matrix representation
0.61
MPO matrix representation 0.0007

GPU
0.11
0.20
0.0086
0.0039

Table III. Performance benchmark for diﬀerent Pauli string sum evaluation on a 20-qubit TFIM Hamiltonian with
open boundary conditions. The qubit Hamiltonian contains 39 Pauli string terms. Data was obtained using the JAX
backend. GPU simulations used the Nvidia T4 GPU, while CPU simulations used Intel(R) Xeon(R) Platinum 8255C
CPU @ 2.50GHz. The dense matrix representation is not applicable for systems of 20 qubits due to excessive memory
requirements.

where x ∈ Rp, y ∈ Rq are both vectors, and one wishes to evaluate both f (x, y) and (cid:80)
(cid:80)
x

x ∇yf (x, y) =
over a batch x1, x2, . . . , xk of inputs x. This is achieved by creating a new,

(cid:16) ∂f (x,y1)
∂y1

(cid:17)(cid:62)

∂yq

, . . . , ∂f (x,yq)
vectorized value-and-gradient function






fvvg






← x1 →
...
← xk →










 , y


 =







f (x1, y)
...
f (xk, y)


 , (cid:80)k


i=1 ∇yf (xi, y)






which takes as zeroth argument the batched inputs expressed as a k × p tensor, and as ﬁrst argument the
variables we wish to diﬀerentiate with respect to. The outputs are a vector of function values evaluated at
all points (xi, y), and the gradient averaged over all those points. A toy example is implemented as follows:

1 def f (x , y ) :
2

return x [0] * x [1] * y [0] ** 2

3

4
5 f_vvg = K . v e c t o r i z e d _ v a l u e _ a n d _ g r a d (f , argnums =1 , v ec t or i ze d_ a rg n um s =0)
6 X = tc . a rra y_to_tensor ([[1 , 2] , [2 , 3] , [0 , -1]])
7 y = tc . a rra y_to_tensor ([2])
8 f_vvg ( x_tensor , y )

argnums indicates which argument we wish to take derivatives with respect to, and vectorized_argnums
indicates which argument corresponds to the batched input. It is even possible to set the values of argnums
and vectorized_argnums to be the same, i.e., we batch compute over diﬀerent initial values of the parameters
we wish to optimize over. This can be useful, for example, in batched VQE computations (see Section VI C 5).

Consider a quantum circuit U (w) on n qubits, parameterized by weights w = [w1, . . . , wk], and with

state-dependent loss function f (ψ, w), e.g.

1. Batched input states

L =

(cid:88)

ψ

f (ψ, w) =

(cid:88)

ψ

(cid:104)ψ| U †(w)Z2U (w) |ψ(cid:105)

Then, both the value f (ψ, w) and the gradient with respect to the weights ∇wf (ψ, w) can be evaluated for
a batch of k input states |ψ1(cid:105) , . . . , |ψk(cid:105) simultaneously.

1 f_vvg = K . v e c t o r i z e d _ v a l u e _ a n d _ g r a d (f , argnums =1 , v ec t or i ze d_ a rg n um s =0)
2 f_vvg ( psi_matrix , weights )

The vectorized value and grad workﬂow for batched input states can be visualized in Figure 7.

22

Figure 7. Schematic of applying vectorized_value_and_grad on quantum simulation tasks with multiple input states.
In the ﬁgure, the function f’ at the bottome is transformed from the original function f deﬁned at the top via f’
= K.vectorized_value_and_grad(f). The wavefunction input is vectorized while diﬀerentiation is with respect to the
weights input.

2. Batched circuits

Consider a family of parameterized quantum circuits U1(w), . . . , Uk(w) acting on the same number of
qubits, where each circuit Ui(w) can be expressed as a diﬀerent parametrization of a single parent circuit,
i.e.

Uj(w) = U (w, xj)

where xj is a vector of parameters. A loss function f is deﬁned on these circuits, e.g.
(cid:104)0| U †
cuits simultaneously, e.g. by deﬁning

f (w, xj) =
j X1Uj |0(cid:105) and its gradient with respect to the weights w can also be batch evaluated across all cir-



fvvg


w,






← x1 →
...
← xk →













 =







f (w, x1)
...
f (w, xk)


 , (cid:80)k


i=1 ∇wf (w, x1)






In this case, the batched inputs correspond to the ﬁrst argument, and the variables to diﬀerentiate with
respect to correspond to the zeroth argument:

1 f_vvg = K . v e c t o r i z e d _ v a l u e _ a n d _ g r a d (f , argnums =0 , v ec t or i ze d_ a rg n um s =1)
2 f_vvg ( weights , x_matrix )

23

input  wavefunction,weightsQuantum Circuitf' = vvag(f, vectorized_argnums=0, argnums=1)input  wavefunctions,weights,3. Batched cost function evaluation

As discussed in Section VI B 4, vmap can be used to accelerate the computation of expectations of sums
of Pauli strings by making use of the Pauli structure vectors representation. The same can be done for
gradients of such expectation values using vectorized_value_and_grad. Consider a circuit on n = 3 qubits,
with k = 4 Pauli terms and cost function

f (w) = (cid:104)Z0X2(cid:105) + (cid:104)X1Y2(cid:105) + (cid:104)X0X1Z2(cid:105) + (cid:104)Z1(cid:105)

where (cid:104)Z0X2(cid:105) := (cid:104)0| U †(w)Z0X2U (w) |0(cid:105) for some parameterized circuit U (w), and similarly for the other
terms. Suppose we have a parameterized circuit deﬁned:

1 def param_circuit ( params ) :
c = tc . Circuit ( n )
2
# circuit details omitted
return c

3

4

then, we can express the expectation of a Pauli term with structure v as:

1 def f (w , v ) :
2

c = param_circuit ( w )
return tc . templates . measurements . p a r a m e t e r i z e d _ m e a s u r e m e n t s (c , v , onehot = True )

3

To batch compute the value and gradient of each term in the cost function, we create a tensor of the Pauli
structures:

1 structures = tc . array_to_tensor ([[3 ,0 ,1] , # < Z0 X2 >
[0 ,1 ,2] , # < X1 Y2 >
2
[1 ,1 ,3] , # < X0 X1 Z2 >
[0 ,3 ,0]]) # <Z1 >

3

4

which we then pass in to the vectorized_value_and_grad version of f

1 f_vvg = K . v e c t o r i z e d _ v a l u e _ a n d _ g r a d (f , argnums =0 , v ec t or i ze d_ a rg n um s =1)
2 f_vvg ( params , structures )

Schematically, this returns



fvvg


w,






← v1 →
...
← vk →









 =











f (w, v1),
...
f (w, vk)


 , (cid:80)k


i=1 ∇wf (w, v1)






with each vector vi corresponding to a diﬀerent Pauli term.

4. Batched Machine Learning

In machine learning problems, one often wishes to perform batch computation over (data, label) pairs. This
can be done by supplying a tuple of indices to the vectorized_argnums argument of vectorized_value_and_grad.
Consider the following toy problem where data vectors x ∈ [0, 1]2 have labels y ∈ {0, 1} and one wishes to
train the weights of a parameterized quantum circuit based on a training set of (x, y) pairs. The x vectors
are encoded in the angles of an initial set of parameterized gates, with the remaining weights w to be trained
to minimize the cost function below:

1 def f (x , y , w ) :
2

c = tc . Circuit (2)

3

4

5

6

7

# encode x in qubit rotations
c . rx (0 , theta = x [0])
c . rx (1 , theta = x [1])

24

8

9

10

11

12

13

14

# # parameterized circuit to determine label
c . rx (0 , theta = w [0])
c . cnot (0 , 1)
c . rx (1 , theta = w [1])

yp = c . expectation_ps ( z =[1])
return K . real ( e - y [0]) ** 2 , yp

Using vectorized_value_and_grad now requires the vectorized_argnums argument to be a tuple corresponding
to the x, y argument indices:

1 f_vvg = K . v e c t o r i z e d _ v a l u e _ a n d _ g r a d (
2
3 )

f , argnums =2 , v ec t or i ze d _a rg n um s =(0 , 1) , has_aux = True

In the f (x, y, w) cost function, x, y are the zeroth and ﬁrst arguments (which we wish to batch compute over),
while w is the second argument, which we wish to take derivatives with respect to. The has_aux argument, if
set to True, indicates that the function returns a tuple with only the ﬁrst element diﬀerentiated. In our case,
the ﬁrst output is the loss function to be minimized, and the second auxiliary output is the predicted label
yp, which is also helpful to keep for calculating other metrics such as AUC or ROC. The batch computation
can then be performed as follows

1 X = tc . a rra y_to_tensor ([[3 , 2] , [1 , -4] , [0 , 1]])
2 Y = tc . a rra y_to_tensor ([[0] , [1] , [1]])
3 w = tc . a rra y_to_tensor ([0.1 , 0.3])
4 f_vvg (X , Y , w )

Consider a cost function deﬁned by a simple parameterized quantum ciruit, e.g.,

5. Batched VQE

1 def f ( w ) :
2

c = tc . Circuit (2)
c . rx (0 , theta = w [0])
c . cnot (0 , 1)
c . rx (1 , theta = w [1])
e = c . expectation_ps ( z =[0 , 1])
return K . real ( e )

3

4

5

6

7

The function value and gradient (with respect to the weights w[0], w[1]) can be batch computed for multiple
weights simultaneously as follows:

1 f_vvag = K . v e c t o r i z e d _ v a l u e _ a n d _ g r a d (f , argnums =0 , ve c to r iz e d_ a rg n um s =0)
2
3 W = tc . a rra y_to_tensor ([[0.1 , 0.2] , [0.3 , 0.4]])
4 f_vvag ( W )

While the above is a toy problem for illustrative purposes, combining this method with jit can be a powerful
approach to ﬁnding ground state energies via VQE, starting from multiple initial points in parameter space
and ﬁnding the best local minimum reached by gradient descent. The batched VQE workﬂow, which admits
independent optimization loops running simultaneously, is sketched in Figure 8.

6. Batched Monte Carlo noise simulation

As introduced in Section V B 1, by using external random number arrays, we can simulate quantum noise
via multiple Monte Carlo trajectories computed in a vectorized parallel fashion. The following example
shows how to vmap quantum noise in TensorCircuit .

25

Figure 8. Applying vectorized_value_and_grad for VQE with batched circuit weights. This enables multiple opti-
mization loop evaluations to be performed at the same time. In the ﬁgure, the function f’ at the bottom is transformed
from the original function f deﬁned at the top via f’ = K.vectorized_value_and_grad(f). The weights input is both
diﬀerentiated and vectorized at the same time.

1 nwires = 6
2

3
4 def f ( weights , status ) :
5

c = tc . Circuit ( nwires )
# omit the details of constructing a PQC with ‘ weights ‘ parameters
for i in range ( nwires ) :

c . depolarizing (i , px =0.2 , py =0.2 , pz =0.2 , status = status [ i ])
# quantum noise controlled by external random number ‘ status ‘ argument

loss = c . expectation_ps ( x =[ nwires // 2])
loss = tc . backend . real ( loss )
return loss

6

7

8

9

10

11

12

13

14
15 # get the circuit gradient while vmapping the depolarizing noise
16 f_vg = tc . backend . jit ( tc . backend . vvag (f , argnums =0 , v ec t or i ze d _a r gn u ms =1) )
17
18 # random number with batch dimension
19 status = tc . backend . implicit_randu ( shape =[ batch , nwires ])
20 f_vg ( weights , status )

26

weightsQuantum Circuitf' = vvag(f, vectorized_argnums=0, argnums=0)batched weights,D. QuOperator and QuVector

Jupyter notebook:

6-4-quoperator.ipynb

tc.quantum.QuOperator, tc.quantum.QuVector and tc.quantum.QuAdjointVector are data classes which be-
have like matrices and vectors (columns or rows) when interacting with other ingredients, while their inner
structures correspond to tensor networks for eﬃciency and compactness.

Typical tensor network structures for a QuOperator/QuVector correspond to Matrix Product Operators

(MPO) / Matrix Product States (MPS). The former represents a matrix as:

Mi1,i2,...in; j1,j2,...jn =

Tk

ik,jk ,

(cid:89)

k

i.e., a product of d×d matrices T ik,jk
a vector as:

k

, where d is known as the bond dimension. Similarly, an MPS represents

Vi1,...in =

T ik
k ,

(cid:89)

k

where the T ik
are, again, d × d matrices. MPS and MPO often occur in computational quantum physics
k
contexts, as they give compact representations for certain types of quantum states and operators. For an
introductory review on MPS/MPO in quantum physics, please refer to [16].

QuOperator/QuVector objects can represent any MPO/MPS, but they can additionally express more ﬂex-
Indeed, any tensor network with two sets of dangling edges of the same
ible tensor network structures.
dimension (i.e., for each k, the set {T ik,jk
of matrices has ik and jk running over the same index
set) can be treated as a QuOperator. A general QuVector is even more ﬂexible, in that the dangling edge
dimensions can be chosen freely; thus, arbitrary tensor products of vectors can be represented.

}ik,jk

k

In this section, we will illustrate the eﬃciency and compactness of such data structures, and show how
they can be integrated seamlessly into quantum circuit simulation tasks. First, consider the following code
and tensor diagram (Figure 9) as an introduction to these data class abstractions.

1 n1 = tc . gates . Gate ( np . ones ([2 , 2 , 2]) )
2 n2 = tc . gates . Gate ( np . ones ([2 , 2 , 2]) )
3 n3 = tc . gates . Gate ( np . ones ([2 , 2]) )
4 n1 [2] ^ n2 [2]
5 n2 [1] ^ n3 [0]
6
7 matrix = tc . quantum . QuOperator ( out_edges =[ n1 [0] , n2 [0]] , in_edges =[ n1 [1] , n3 [1]])
8
9 n4 = tc . gates . Gate ( np . ones ([2]) )
10 n5 = tc . gates . Gate ( np . ones ([2]) )
11
12 vector = tc . quantum . QuVector ([ n4 [0] , n5 [0]])
13
14 nvector = matrix @ vector
15
16 assert type ( nvector ) == tc . quantum . QuVector
17 nvector . eval_matrix ()
18 # array ([[16.] , [16.] , [16.] , [16.]])

# matrix - vector multiplication

Note that the convention in deﬁning a QuOperator is to ﬁrst state out_edges (left index or row index of the
matrix) and then state in_edges (right index or column index of the matrix). Also note that tc.gates.Gate is
just a wrapper for the Node object in the TensorNetwork package.

As seen above, QuOperator/QuVector objects support matrix-matrix or matrix-vector multiplication via

the @ operator. Other common matrix/vector operations are also supported:

1 matrix . adjoint ()
2 5 * vector
3 vector | vector
4 matrix . partial_trace ([0])

# adjoint i . e . , conjugate transpose
# scalar multiplicatoin
# tensor product
# partial trace ( of subsystem 0)

27

Matrix elements of these objects can be extracted via .eval() or .eval_matrix(). The former keeps the shape
information of the tensor network while the latter gives the matrix representation (i.e, as a rank 2 tensor).

Figure 9. Tensor network schematic demonstrating the usage of QuOperator and QuVector.

1. QuVector as the input state for the circuit

Since a QuVector behaves like a regular vector, albeit with a more compact representation, it can be used
as the input state to a quantum circuit instead of a state represented as a regular array. The beneﬁt of
doing so is memory eﬃciency. For an n-qubit circuit, regular vector inputs require 2n complex values to be
stored in memory. On the other hand, for an MPS with bond dimension d represented by a QuVector, only
O(nd2) complex elements in total need to be stored. Such compact MPS representations can be obtained,
for instance, by DMRG [57] calculations, and a DMRG ground state to quantum machine learning model
pipeline can thus be built in TensorCircuit with the help of this feature.

The following example shows how we input the |111(cid:105) state, encoded as an MPS, to a quantum circuit.

Note how mps_inputs argument is used when constructing the circuit.

1 n = 3
2 nodes = [ tc . gates . Gate ( np . array ([0.0 , 1.0]) ) for _ in range ( n ) ]
3 mps = tc . quantum . QuVector ([ nd [0] for nd in nodes ])
4 c = tc . Circuit (n , mps_inputs = mps )
5 c . x (0)
6 c . expectation_ps ( z =[0])

# 1

2. QuVector as the output state of the circuit

For a given input state, a tc.Circuit object can itself be treated as a tensor network with one set of dangling
edges corresponding to the output state, and thus the whole circuit object can be regarded as a QuVector,
obtained via c.quvector(). We can then further manipulate the circuit using QuOperator objects.

28

n1n2n3n1[0]n1[1]n3[1]n3[0]n2[1]n2[0]n2[2]n1[2]matrix@n4n5vectorn4[0]n5[0]nvectoreval_matrix3.

QuOperator as the operator to be measured

As shown in Section VI B, Hamiltonians can also be represented by a QuOperator. This can be a pow-
erful and eﬃcient approach for computing expectation values for some lattice model Hamiltonians like the
Heisenberg model or transverse ﬁeld Ising model (TFIM), as the MPO form of the Hamiltonian for such
short-ranged spin models has a very low bond dimension (e.g., d = 3 for TFIM). For comparison, for an
n-qubit TFIM Hamiltonian, the dense matrix representation stores O(22n) complex elements, the sparse
matrix representation stores O(n2n) complex elements, while the MPO or QuOperator representation stores
only 18n complex elements, which scales linearly with the system size.

Here we show a toy example of measuring the expectation value of an operator represented as a QuOperator.
The quantity of interest here is (cid:104)Z0Z1(cid:105), where the expectation is with respect to the output of a simple
circuit which consists of an X gate on one of the qubits. Instead of using the c.expectation() API, we use
mpo_expectation.

1 z0 , z1 = tc . gates . z () , tc . gates . z ()
2 mpo = tc . quantum . QuOperator ([ z0 [0] , z1 [0]] , [ z0 [1] , z1 [1]])
3 c = tc . Circuit (2)
4 c . X (0)
5 tc . templates . measurements . mpo_expecta tion (c , mpo )

# -1

4. QuOperator as quantum gates

Since quantum gates correspond to unitary matrices, an MPO representation for these matrices may allow
for signiﬁcant space eﬃciencies in some scenarios. A typical example is the multi-controlled gate, which
admits a very compact MPO representation that can be characterized by a QuOperator in TensorCircuit .
For an n-qubit gate with n − 1 control qubits, the matrix representation for this gate has 22n elements while
the MPO representation can be reduced to bond dimension d = 2, leading to only 16n elements in memory.

TensorCircuit has a built-in way to generate these eﬃcient multi-controlled gates:

1 # CCNOT gate
2 c = tc . Circuit (3)
3 c . multicontrol (0 , 2 , 1 , ctrl =[1 , 0] , unitary = tc . gates . x () ) # if q0 =1 and q2 =0 , apply X to q1

The 0,2,1 arguments refer to the qubits the gate is applied on (the ordering matters, with the ﬁnal index
referring to the target qubit), the unitary argument deﬁnes the operation that is applied if all controls are
activated and the ctrl argument refers to whether the control is activated when the corresponding control
qubit is in the 0 state or 1 state. General MPO gates expressed as a QuOperator can also be applied
via the c.mpo(*index, mpo=) API, in the same way that general unitary matrices can be applied via the
c.unitary(*index, unitary=) API (see Section III B).

E. Custom contraction settings

Jupyter notebook:

6-5-custom-contraction.ipynb

By default, TensorCircuit uses a greedy tensor contraction path ﬁnder provided by the opt_einsum pack-
age. While this is typically satisfactory for moderately sized quantum circuits, for circuits with 16 qubits or
more, we recommend using customized contraction path ﬁnders provided by the user or third-party packages.
A simple quantum circuit can be constructed using the following code as a testbed for diﬀerent contraction

methods:

1 def testbed () :
2

n = 40
d = 6
param = K . ones ([2 * d , n ])

3

4

29

5

6

7

8

9

c = tc . Circuit ( n )
c = tc . templates . blocks . example_block (c , param , nlayers =d , is_split = True )
# the two qubit gate is split and truncated via SVD decomposition
return c . expectation_ps ( z =[ n // 2] , reuse = False )
# by reuse = False , we compute the expectation as a single tensor network instead of first

computing the wavefunction

By using tc.templates.blocks.example_block, a circuit with d layers of exp(iθZZ) gates and Rx gates is
created. When is_split is True, each two-qubit gate will not be treated as an individual tensor but will
be split into two connected tensors via singular value decomposition (SVD), which further simpliﬁes the
tensor network structure of the corresponding circuit. The task is to calculate the expectation value of the
Z operator on the middle (i.e, n/2-th) qubit.

The API for contraction setup is tc.set_contractor.

In our example, 2n × d tensors need to be con-
tracted since single-qubit gates can be absorbed into two-qubit gates when preprocessing is set to True in
set_contractor. We have some built-in contraction path ﬁnder options such as "greedy", "branch", and
"optimal" from opt-einsum [58], though only the default "greedy" option is suitable for circuit simulation
tasks as other options require time exponential in the number of qubits. The contraction_info option in this
setup API, if set True, will print the contraction path information after the path searching. Metrics that
measure the quality of a contraction path include

• FLOPs: the total number of computational operations required for all matrix multiplications involved
when contracting the tensor network via the given path. This metric characterizes the total simulation
time.

• WRITE: the total size (the number of elements) of all tensors – including intermediate tensors –

computed during the contraction.

• SIZE: the size of the largest intermediate tensor stored in memory.

Since simulations in TensorCircuit are AD-enabled, where all intermediate results need to be cached and
traced, the more relevant spatial cost metric is write instead of size.

1. Customized contraction path ﬁnder

For large quantum circuits, the performance of the default "greedy" contraction path ﬁnder may not be
satisfactory. If this is the case, a custom contraction path ﬁnder can be used to enhance the performance
of contraction by ﬁnding better paths in terms of ﬂops (time) and writes (space). Here we use the path
ﬁnder provided by the third-party cotengra package , a python library for contracting tensor networks or
computing einsum expressions. The way to use the cotengra path ﬁnder in TensorCircuit is as follows:

1 import cotengra as ctg
2
3 opt = ctg . R e u s a b l e H y p e r O p t i m i z e r (
4

methods =[ " greedy " , " kahypar " ] ,
parallel = True ,
minimize = " write " ,
max_time =120 ,
max_repeats =1024 ,
progbar = True ,

5

6

7

8

9
10 )
11 tc . set_contractor ( " custom " , optimizer = opt , preprocessing = True , contraction_info = True )
12 testbed ()

A number of parameters are used to conﬁgure a contraction path ﬁnder opt in cotengra: method decides the
strategy this path ﬁnder will be based on. minimize decides the score function you want to minimize during
the path ﬁnding, and can be set as "write", "ﬂops", "size" or a combination of these. A time limit and a
limit on the number of trial contraction trees can also be set using max_time and max_repeats respectively.
For more details, refer to the cotengra documentation. You can also design your own contraction path ﬁnder
as long as you provide an opt function compatible with the interface of the opt_einsum optimizer.

30

2. Subtree reconﬁguration

Given a contraction path, e.g. given by a "greedy" search, its performance can be further enhanced by
conducting a so-called subtree reconﬁguration. This process repeatedly optimizes subtrees of the whole con-
traction tree, and in practice often results in a better contraction path. This can be done in TensorCircuit as
follows:

1 opt = ctg . R e u s a b l e H y p e r O p t i m i z e r (
2

minimize = " combo " ,
max_repeats =1024 ,
max_time =120 ,
progbar = True ,

3

4

5
6 )
7

11

12

13

14

15

8
9 def opt_reconf ( inputs , output , size , ** kws ) :
10

tree = opt . search ( inputs , output , size )
tree_r = tree . s u b t r e e _ r e c o n f i g u r e _ f o r e s t (

progbar = True , num_trees =10 , num_restarts =20 , s u b t r e e _ w e i g h t _ wh a t =( " size " ,)

)
return tree_r . get_path ()

16
17 tc . set_contractor (
" custom " ,
18
optimizer = opt_reconf ,
co ntr a ction_info = True ,
preprocessing = True ,

19

20

21
22 )
23

24
25 testbed ()

Notice that subtree_reconﬁgure_forest is used after ﬁnding a contraction tree.
In this function, you can
set the number of trees in the random forest whose contraction paths will be updated, and also the metric
to be optimized in the subtrees. In the above example, a user customized function opt_reconf is fed into
contractor setup as a legal contraction path ﬁnder.

As mentioned earlier, there are three metrics to measure the quality of a contraction path. By setting
diﬀerent score functions (changing the minimize parameter), the resulting contraction path given by these
contractors will exhibit diﬀerent properties. Table IV summarizes the contraction performance of diﬀerent
contraction strategies for our example case (n = 40, d = 6). As we can see, the cotengra optimizer and
subtree reconﬁguration can greatly improve the quality of the contraction path and improve the eﬃciency
of quantum circuit simulation. For example, we gain more than a factor of two improvement in simulation
time and simulation space compared to the default contractor, and the degree of improvement can increase
for larger system sizes.

F. Advanced automatic diﬀerentiation

Jupyter notebook: 6-6-advanced-automatic-diﬀerentiation.ipynb

TensorCircuit provides backend-agnostic wrappers to a number of advanced AD features, useful in a variety
of quantum circuit simulation scenarios. In the remainder of this section we will illustrate these using the
following circuit example:

1 n = 6
2 nlayers = 3
3

4
5 def ansatz ( thetas ) :

31

reconﬁguration log10[FLOPs] log2[SIZE] log2[WRITE]

subtree("ﬂops")
subtree("size")

contractor
default
cotengra("ﬂops")
cotengra("ﬂops")
cotengra("ﬂops")
cotengra("write")
cotengra("write")
cotengra("write")
cotengra("combo")
cotengra("combo") subtree("ﬂops")
cotengra("combo") subtree("size")

subtree("ﬂops")
subtree("size")

7.373
7.080
7.006
7.006
7.442
7.000
7.017
7.480
7.003
7.011

12
13
12
12
14
12
12
14
12
12

20.171
20.493
20.069
20.075
19.061
19.988
19.958
19.061
20.000
19.885

Table IV. Performance of diﬀerent contractor settings which include the default opt_einsum contractor and cotengra
contractors with and without subtree reconﬁguration. Parameters in the brackets indicate the score function used dur-
ing the path searching and reconﬁguration. For cotengra contractors, we set max_repeats=1024,max_time=120 and
method=["greedy","kahypar"]. "combo" means the score function is a combination of "ﬂops" and "write" (ﬂops+64×
write by default). For subtree reconﬁguration, we set num_trees =20, num_restarts=20. Results shown are from one
run, and performance may vary from run to run since these algorithms are intrinsically random.

c = tc . Circuit ( n )
for j in range ( nlayers ) :

for i in range ( n ) :

c . rx (i , theta = thetas [ j ])

for i in range ( n - 1) :

c . cnot (i , i + 1)

return c

6

7

8

9

10

11

12

13

14
15 def psi ( thetas ) :
16

c = ansatz ( thetas )
return c . state ()

17

Jacobian (jacfwd and jacrev). Given an n-input, m-output function f the n × m Jacobian matrix is given
by

Jf :=

∂f
∂x

=







∂f1
∂x1

...

∂fm
∂x1







∂f1
∂xn

. . .
. . .
. . . ∂fm
∂xn

By the chain rule, the Jacobian matrix of a composition of functions is the product of the Jacobians of the
composed functions (evaluated at appropriate points). e.g. if h : Rn → Rp, g : Rp → Rq, f : Rq → Rm and
y : Rn → Rm with y(x) = f (g(h(x))) then

∂y
∂x

·

=

∂f (b)
∂b

∂h(x)
∂x
= Jf (b) · Jg(a) · Jh(x)

∂g(a)
∂a

·

where a = h(x), b = g(a) and · denotes matrix multiplication. Forward mode AD and reverse mode AD
(‘backpropagation’) are two approaches to computing a composite Jacobian, and diﬀer in the order in
which the products are computed. Forward mode AD computes the above product from right to left, i.e.
Jy = Jf (b) · (Jg(a) · Jh(x)) at a cost of pqn + qnm multiplications. Taking f to be the output state ψ(θ) of
the above circuit, this is computed as

1 thetas = K . implicit_randn ([ nlayers ])

32

2 ja c_ fwd _ fun ction = K . jacfw ( psi )
3 jac_fw = j a c_fwd_function ( thetas )

Reverse mode AD computes the product from left to right, i.e. Jy = (Jf (b)·) Jg(a) · Jh(x) at a cost of
mpq + pnm multiplications:

1 ja c_ rev _ fun ction = K . jacrev ( psi )
2 jac_rev = j ac_rev_function ( thetas )

The relative eﬃciency of these methods depends on the input and output dimensions of the functions involved.
For instance, when p = q forward mode AD is advantageous if n (cid:28) m (i.e., the input dimension is much
smaller than the output dimension, corresponding to a ‘tall’ Jacobian) and vice versa for reverse mode AD.

Jacobian-vector product (jvp). Computing the product of the Jacobian with a vector v (i.e.
the
directional derivative) can be a useful primitive as it utilizes forward mode AD and is suitable when the
output dimension is much larger than the input. For instance, setting v = ei (i.e. the vector with a 1 in the
i-th coordinate and zeroes elsewhere) gives the vector of partial derivatives

Jf ei =

(cid:16) ∂f1
∂xi

, . . . , ∂fm
∂xi

(cid:17)(cid:62)

Taking v = (1.0, 0, 0), the value of ψ and the Jacobian-vector product ∂ψ
∂θ0
point θ = (0.1, 0.2, 0.3)) as follows:

can be evaluated (e.g., at the

1 state , p a r t i a l _ p s i _ p a r t i a l _ t h e t a 0 = K . jvp (
2

psi ,
tc . arra y_to_tensor ([0.1 , 0.2 , 0.3]) ,
tc . arra y_to_tensor ([1.0 , 0 , 0] , dtype = " float32 " ) ,

3

4
5 )

Quantum Fisher Information (qng). The Quantum Fisher Information (QFI) is an important concept
in quantum information, and can be utilized in so-called quantum natural gradient descent optimization [59]
as well as variational quantum dynamics [60, 61].

There are several variants of QFI-like quantities, all of which depend on the evaluation of terms of the
form (cid:104)∂iψ|∂jψ(cid:105) − (cid:104)∂iψ|ψ(cid:105)(cid:104)ψ|∂jψ(cid:105). Such quantities are easily obtained with advanced AD frameworks, by
ﬁrst computing the Jacobian for the output state and then vmapping the inner product over Jacobian rows.
The detailed eﬃcient implementation can be found at the codebase. Here we directly call the corresponding
API to obtain the quantum natural gradient.

1 from tensorcircuit . experimental import qng
2
3 # function to get qfi given circuit parameters
4 qfi_fun = K . jit ( qng ( psi ) )
5
6 # suppose the vanilla circuit gradient is ‘ grad ‘
7 # then we can obtain quantum natural gradient as
8 ngrad = tc . backend . solve ( qfi_fun ( thetas ) , grad , assume_a = " sym " )

Hessian (hessian). The Hessian Hij = ∂(cid:104)H(cid:105)θ
∂θi∂θj
(taking H = Z0 for simplicity)

of a parameterized quantum circuit can be computed as

1 def h ( thetas ) :
2

c = ansatz ( thetas )
return c . expectation_ps ( z =[0])

3

4
5 # hess is the Hessian function which takes thetas as input
6 hess = K . hessian ( h )

which can then also be jitted to make multiple evaluations more eﬃcient:

1 hess_jit = K . jit ( hess )

33

Information on the Hessian matrix may be useful in investigating loss landscapes, or for second order opti-
mization routines.

(cid:104)ψ| H |∂ψ(cid:105). In variational quantum dynamics problems (see, e.g. [60]) one often wishes to compute quantities
of the form

(cid:104)ψ(θ)| H

∂ |ψ(θ)(cid:105)
∂θi

.

This can be done via the stop_gradient API, which prevents certain parameters from being diﬀerentiated.
Again, taking H = Z0, we can deﬁne an appropriate function for which only the parameters corresponding
to |ψ(θ)(cid:105) will be diﬀerentiated:

1 z0 = tc . quantum . P a u l i S t r i n g S u m 2 D e n s e ([[3 , 0 , 0 , 0 , 0 , 0]])
2

3
4 def h ( thetas ) :
5

w = psi ( thetas )
w_left = K . conj ( w )
w_right = K . stop_gradient ( w )
w_left = K . reshape ([1 , -1])
w_right = K . reshape ([ -1 , 1])
e = w_left @ z0 @ w_right
return K . real ( e ) [0 , 0]

6

7

8

9

10

11

Then, gradients can be computed as usual:

1 ps i _h _p a r ti al_ ps i = K . grad ( h )

With the advanced automatic diﬀerentiation infrastructure, we can obtain quantum circuit gradient re-
lated quantities, such as those listed above, much more quickly than via traditional quantum software that
utilizes parameter shifts to evaluate gradients. In Figure 10, we show the acceleration of QFI and Hessian
computations compared to Qiskit. The benchmark code is detailed in examples/gradient_benchmark.py .
From the data, we see that for even moderate-sized quantum circuits, TensorCircuit can achieve a speedup
over Qiskit of nearly a million times.

Figure 10. Acceleration factor for TensorCircuit over Qiskit for the evaluation of QFI and Hessian information. For
4 and 6 qubit systems, we use PQC comprising two blocks of CNOT+Rx+Rz+Rx gates, while for the larger systems
we use four such blocks. The simulation runs on AMD EPYC 7K62 CPU 2.60GHz, and TensorCircuit results use
the JAX backend. As the Qiskit running times are long (e.g., the 12-qubit QFI calculation takes more than 20,000s
∼ 5.5 hours) the acceleration factors presented above are based on only a single Qiskit evaluation. In contrast, the
TensorCircuit running times are low enough (0.026s for 12-qubit QFI) that we average over multiple runs.

34

VII.

INTEGRATED EXAMPLES

A. Quantum machine learning

Jupyter notebook:

tutorials/mnist_qml.ipynb

Background. Quantum and hybrid quantum-classical neural networks are popular approaches to NISQ
era quantum computing, and both can be easily modelled and tested in TensorCircuit . In the highlighted
features below, we illustrate how to build a hybrid machine learning pipeline in TensorCircuit , and in
the benchmarking section we compare TensorCircuit with other quantum software for performing batched
supervised learning on the MNIST dataset using a parameterized quantum circuit.

Highlighted features. Seamless integration of quantum and classical neural networks can be obtained by
wrapping the TensorCircuit tc.Circuit object (with weights as input and expectation value as output) in a
QuantumLayer, which is a subclass of the Keras Layer. The following code snippet shows how such a wrapper
is implemented and used:

1 def qml_ys (x , weights , nlayers ) :
2

n = 9
weights = tc . backend . cast ( weights , " complex128 " )
x = tc . backend . cast (x , " complex128 " )
c = tc . Circuit ( n )
for i in range ( n ) :

c . rx (i , theta = x [ i ])

for j in range ( nlayers ) :

for i in range ( n - 1) :

c . cnot (i , i + 1)

for i in range ( n ) :

c . rx (i , theta = weights [2 * j , i ])
c . ry (i , theta = weights [2 * j + 1 , i ])

ypreds = []
for i in range ( n ) :

ypred = c . expectation ([ tc . gates . z () , (i ,) ])
ypred = tc . backend . real ( ypred )
ypred = ( tc . backend . real ( ypred ) + 1) / 2.0
ypreds . append ( ypred )

# return <z_i > as an n dimensional vector
return tc . backend . stack ( ypreds )

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23
24 # wrap the quantum function in a Keras layer
25 ql = tc . keras . QuantumLayer ( partial ( qml_ys , nlayers = nlayers ) , [(2 * nlayers , 9) ])
26 # build the hybrid Keras model with quantum and classical parts
27 model = tf . keras . Sequential ([ ql , tf . keras . layers . Dense (1 , activation = " sigmoid " ) ])
28
29 # train as a normal Keras model
30 model . compile (
31

32

loss = tf . keras . losses . B in a ry C ro s se nt r op y () ,
optimizer = tf . keras . optimizers . Adam (0.01) ,
metrics =[ tf . keras . metrics . BinaryAccuracy () ] ,

33
34 )
35 model . fit ( x_train , y_train , batch_size =32 , epochs =100)

Benchmarking. We benchmark TensorCircuit against other software for binary classiﬁcation (‘3’ vs. ‘6’) of
the MNIST dataset, using quantum machine learning with batched inputs. For software with only parameter-
shift gradient support, each PQC must be evaluated O(np) times, where np is the number of circuit pa-
rameters. This is much slower than AD-enabled simulators, where only one evaluation of the PQC suﬃces
to obtain all circuit weight gradients. Therefore, we only compare TensorCircuit with other AD enabled

35

batch size
Pennylane (GPU)

32
0.042∗
TensorFlow Quantum 0.058
TensorCircuit (CPU) 0.0070
TensorCircuit (GPU) 0.0035

128
0.0089
0.24
0.021
0.0039

512
0.020
0.49
0.085
0.0054

Table V. Performance benchmarks (running time in seconds) for value and gradient evaluation of a PQC (n = 10
qubits, circuit depth p = 3) with classiﬁcation square distance error as the objective function and batched dataset
input. TensorCircuit results use the JAX backend. GPU simulations use the Nvidia V100 32G GPU while CPU
simulations use Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz. Note that TensorFlow Quantum only runs
circuit simulations on CPU. *The Pennlyane running time on GPU is indeed higher for smaller batch size.

software such as TensorFlow Quantum and Pennylane (With Pennylane, we utilize its JAX backend simula-
tor, and use jit and vmap tricks to increase performance). The TensorCircuit results use the default greedy
contraction path ﬁnder, and further improvements are possible with customized contraction path ﬁnders.
See benchmarks for full benchmark details and Table V for results.

B. Moleculer VQE with TensorCircuit and OpenFermion

Jupyter notebook:

tutorials/vqe_h2o.ipynb

Background. Quantum computing is envisioned to be a powerful tool for quantum simulation and quantum
In the highlighted features below we show how to interface TensorCircuit with
chemistry tasks [62, 63].
OpenFermion [64] to compute the ground state energy of an H2O molecule, while in the benchmarking
section we compare the performance of TensorCircuit with other software for the VQE energy computation
of a transverse ﬁeld Ising model.

Highlighted features. OpenFermion is an open-source Python package that provides an eﬃcient interface
between quantum chemistry and quantum computing. In particular, it provides a convenient method for
generating molecular Hamiltonians and converting them into qubit Hamiltonians compatible with simulating
in quantum circuits. To make use of this, we provide the tc.templates.chems.get_ps API, which provides an
interface between TensorCircuit and OpenFermion, and converts the OpenFermion qubit Hamiltonian object
into the Pauli structures and weights tensors used by TensorCircuit (see Section VI B 1). The relevant code
snippet used to generate the Hamiltonian representation in TensorCircuit via OpenFermion is shown below.

1 from openfermion . chem import MolecularData , g e o m e t r y _ f r o m _ p u b c h e m
2 from openfermion . transforms import get_fermion_operator , jordan_wigner
3 from o pen f ermionpyscf import run_pyscf
4
5 multiplicity = 1
6 basis = " sto -3 g "
7 # 14 spin orbitals for H2O
8 geometry = g e o m e t r y _ f r o m _ p u b c h e m ( " h2o " )
9 molecule = MolecularData ( geometry , basis , multiplicity )
10 # obtain H2O molecule object
11 molecule = run_pyscf ( molecule , run_mp2 = True , run_cisd = True , run_ccsd = True , run_fci = True )
12 print ( molecule . fci_energy , molecule . ccsd_energy , molecule . hf_energy )
13
14 mh = molecule . g e t _ m o l e c u l a r _ h a m i l t o n i a n ()
15 # get fermionic Hamiltonian
16 fh = g e t _ f e r m i o n _ o p e r a t o r ( mh )
17 # get qubit Hamiltonian via Jordan - Wigner transformation
18 jw = jordan_wigner ( fh )
19 # converting to Pauli structures in tc

36

PQC
Pennylane (GPU)
TensorFlow Quantum
TensorCircuit (CPU)
TensorCircuit (GPU)

n = 10, d = 3 n = 16, d = 16 n = 22, d = 11
0.68
0.026
0.078
0.023

0.067
0.005
0.00077
0.0026

OOM
0.68
4.70
0.19

Table VI. Performance benchmarks (running time in seconds) for value and gradient evaluations of n-qubit, d-layer
parameterized quantum circuits with a TFIM Hamiltonian objective function. TensorCircuit results use the JAX
backend. GPU simulations use the Nvidia V100 32G GPU while CPU simulations use Intel(R) Xeon(R) Platinum
8255C CPU @ 2.50GHz. OOM indicates that the GPU memory is insuﬃcient to run the corresponding benchmark
code. Note that TensorFlow Quantum only currently supports quantum circuit simulations on CPU.

20 structures , weights = tc . templates . chems . get_ps ( jw , 14)
21 # build sparse numpy matrix representation for the Hamiltonian
22 ma = tc . quantum . P a u l i S t r i n g S u m 2 C O O _ n u m p y ( strutcures , weights )

Benchmarking. We provide some benchmark data for TFIM VQE evaluation here. Speciﬁcally, we com-
pute the TFIM energy expectation value and corresponding circuit gradients (with respect to the circuit
parameters). With TensorCircuit , we use the (potentially less eﬃcient) explicit for loop to perform the Pauli
string summation (see Section VI B 2) to obtain a fair comparison, with our benchmarking focus on the eﬃ-
ciency of evaluating the PQC and its gradients. See benchmarks for benchmark setups and details. Results
are summarized in Table VI. From the benchmarks on QML and VQE tasks in the two examples, we can
see that TensorCircuit indeed brings substantial speedup in quantum circuit simulation. The acceleration is
more impressive on GPU, especially when the circuit size or the batch dimension is large.

C. Demonstration of barren plateaus

Jupyter notebook:

tutorials/barren_plateaus.ipynb

Background. The so-called barren plateaus phenomenon refers to gradients of random circuits vanishing
exponentially quickly as the number of qubits increases. To demonstrate this numerically requires computing
the circuit gradient variance over diﬀerent circuit structures (i.e., choice of random gates in the circuit) and
circuit weights. Gradients can be obtained via automatic diﬀerentiation, while the diﬀerent circuit weights
can be vectorized and jitted to boost performance.
In addition, as in this example, the diﬀerent circuit
structures can also be vectorized and jitted to obtain further speed up.

Highlighted features. This example showcases how jit and vmap can be applied to diﬀerent circuit archi-
tectures. The ability to vmap circuit structures was introduced in Section VI C 2. Here, we give more details
on how to encode diﬀerent circuit structures via a tensor parameter as input. The core part of the circuit
construction is as follows.

1 Rx = tc . gates . rx
2 Ry = tc . gates . ry
3 Rz = tc . gates . rz
4
5 # params is a tensor for the circuit weights with shape [ n_qubits , n_layers ]
6 # seeds is a tensor for the circuit structure with shape [ n_qubits , n_layers ]
7
8 c = tc . Circuit ( n_qubits )
9 for l in range ( n_layers ) :
10

11

for i in range ( n_qubits ) :
c . unitary_kraus (

37

TensorFlow Quantum TensorCircuit (CPU) TensorCircuit (GPU)
0.12

0.011

6.24

time (s)

Table VII. Performance benchmark for gradient variance evaluation over diﬀerent random circuit architectures and
circuit weights (10 qubits, 10 layers, 100 diﬀerent circuits). TensorCircuit results use the JAX backend. GPU
simulations use the Nvidia V100 32G GPU while CPU simulations use Intel(R) Xeon(R) Gold 6133 CPU @ 2.50GHz.
Note that TensorFlow Quantum only supports CPU for quantum circuit simulation.

12

13

14

15

16

17

18

[ Rx ( params [i , l ]) , Ry ( params [i , l ]) , Rz ( params [i , l ]) ] ,
i ,
prob =[1 / 3 , 1 / 3 , 1 / 3] ,
status = seeds [i , l ] ,

)

for i in range ( n_qubits - 1) :

c . cz (i , i + 1)

The seeds tensor controls the circuit architecture via the unitary_kraus API. This API tells the circuit to
stochastically attach one gate from Rx, Ry, Rz with probability 1/3 each. The status argument externalizes
the random number generation (see Section V B 1). Namely, when status is less than 1/3, the ﬁrst gate is
applied, when status is in the range [1/3, 2/3], the second gate in the list is applied and so on. Therefore,
by generating a random number array seeds = K.implicit_randu(size=[n_qubits, n_layers]), we can generate
diﬀerent random circuit s. The simulation over diﬀerent architectures can be vectorized and jitted by
generating seeds with an extra batch dimension corresponding to the number of diﬀerent architectures you
wish to compute in parallel.

Benchmarking. We benchmark the gradient variance computation over diﬀerent random circuit weights
and diﬀerent circuit structures using both TensorFlow Quantum and TensorCircuit , (see details in
examples/bp_benchmark.py ). The computational times for 100 random circuit constructions, each a 10-
qubit, 10-layer circuit, are shown in Table VII. With 10 × 10 circuit weights and 100 diﬀerent circuit,
the combination of vmap and jit provides TensorCircuit with a more than ﬁve hundred times speedup over
TensorFlow Quantum for this task.

D. Very large circuit simulation

Python script:

examples/vqe_extra_mpo.py

Background. As previously mentioned, tensor network quantum simulators do not face the same memory
bottlenecks that limit full state simulators, and can thus simulate larger numbers of qubits as long as circuit
connectivity and depth are reasonably low. In the highlighted features and benchmarking sections below, we
consider a 1D TFIM VQE workﬂow on 600 qubits with seven layers of two-qubit gates, arranged in a ladder
layout, that estimates energy with more than 99% accuracy compared to ground truth.

Highlighted features. The combination of the MPO formalism with the cotengra path ﬁnder allows us
to simulate circuits with very large qubit counts. Speciﬁcally, we utilize (i) the advanced cotengra path
ﬁnder equipped with subtree reconﬁguration post-processing, and with a setup similar to Section VI E 2; (ii)
the space eﬃcient MPO representation to evaluate the quantum expectation of the TFIM Hamiltonian (see
Section VI B 3). For the circuit construction, the split conﬁguration was used to decompose the parameterized
ZZ gates, using SVD to reduce the bond dimension to 2, which further simpliﬁes the tensor network structure
to be contracted (see Figure 11). Such a two-qubit gate decomposition is implemented as follows.

1 split_conf = {
2

" m a x _ s i n g u la r _ v a l u e s " : 2 ,
" fixed_choice " : 1 ,

3

38

number of qubits log2[WRITE] time for one step energy accuracy reached

200
400
600

27.3
29.3
31.5

5.7
11.8
18.2

99.6%
99.5%
99.4%

Table VIII. Performance benchmarks (running time in seconds) for a large scale VQE task with diﬀerent qubit
counts. TensorCircuit results use the TensorFlow backend and run on Nvidia A100 40G GPU. Note that the VQE
optimization hyperparameters were not tuned, so the energy accuracy obtained only represents a lower bound on the
performance of the current method. Time for one step includes computing both the energy expectation value and
the circuit gradients.

4 }
5 # set the SVD decomposition option in circuit level
6 c = tc . Circuit (n , split = split_conf )
7 # or set the SVD decomposition option in gate level
8 c . exp1 (i , i + 1 , theta = param , unitary = tc . gates . _xx_matrix , split = split_conf )

The ground energy for reference is obtained via two-site DMRG using the Quimb package.

Figure 11. SVD decomposition of a two-qubit gate as a tensor network. This operation can be used to reduce the
complexity of the underlying network. While the bond dimension d after decomposition for general two-qubit gates
is 4, for certain types of two-qubit gates it can be lower. For example, in our case, where two-qubit parameterized
gates are of the form exp (iθXX), d = 2.

Benchmarking. The parameterized circuit we utilized has n qubits and 7 layers, and contains 21n single-
qubit gates and 7n two-qubit gates. Results for n = 200, 400, 600 are summarized Table VIII, with times
per computational step corresponding to the evaluation of both the energy expectation value and the circuit
gradients.

VIII. OUTLOOK AND CONCLUDING REMARKS

We have introduced TensorCircuit , an open-source Python package, designed to meet the requirements of
larger and more complex quantum computing simulations. TensorCircuit is built on top of, and incorporates,
all the main engineering paradigms from modern machine learning libraries, and its ﬂexible and customizable
tensor network engine enables high-performance circuit computation.

Outlook. We will continue the development of TensorCircuit , towards delivering a more eﬃcient and
elegant, full-featured and ML-compatible quantum software package. At the top of our priority list are:

1. Better tensor network contraction path ﬁnders:

integrate more advanced algorithms and machine

learning techniques for optimal contraction path searching.

2. Pulse level optimization and quantum control: enable end-to-end diﬀerentiable pulse level optimization

and optimal quantum control schedules [65, 66].

39

22222222d3. Distributed quantum circuit simulation: enable tensor network parallel slicing and distributed compu-

tation on multiple hosts.

4. Approximate circuit simulation based on MPS: introduce TEBD-like algorithms [67, 68] to approxi-

mately simulate quantum circuits with large size and depth.

5. More quantum-aware or manifold-aware optimizers:

include optimizers such as SPSA [69], roto-

solve [70], and Riemannian optimizers [71].

6. Quantum applications: develop application-level libraries for quantum computing for ﬁnance, materials,

energy, biology, drug discovery, climate prediction and more.

With the continued rapid progress in quantum computing theory and hardware, our hope is that Ten-
sorCircuit , a quantum simulator platform designed for the NISQ era, will play an important role in the
academic and commercial progress of this exciting ﬁeld.

Acknowledgements: The authors would like to thank our teammates at the Tencent Quantum Laboratory
for supporting this project, and Tencent Cloud for providing computing resources. Shi-Xin Zhang would
like to thank Rong-Jun Feng, Sai-Nan Huai, Dan-Yu Li, Zi-Xiang Li, Lei Wang, Hao Xie, Shuai Yin and
Hao-Kai Zhang for their helpful discussions.

BIBLIOGRAPHY

[1] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information: 10th Anniversary Edition,

10th ed. (Cambridge University Press, USA, 2011).

[2] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al.,
Tensorﬂow: A system for large-scale machine learning, in 12th USENIX symposium on operating systems design
and implementation (OSDI 16) (2016) pp. 265–283.

[3] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas,
S. Wanderman-Milne, and Q. Zhang, JAX: composable transformations of Python+NumPy programs (2018).
[4] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai,
and S. Chintala, in Advances in Neural Information Processing Systems.

[5] J. Gray, cotengra, https://github.com/jcmgray/cotengra (2020).
[6] J. Gray and S. Kourtis, Hyper-optimized tensor network contraction, Quantum 5, 410 (2021).
[7] J. Preskill, Quantum computing in the nisq era and beyond, Quantum 2, 79 (2018).
[8] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea, A. Anand, M. Degroote, H. Heimonen, J. S.
Kottmann, T. Menke, W.-K. Mok, S. Sim, L.-C. Kwek, and A. Aspuru-Guzik, Noisy intermediate-scale quantum
algorithms, Reviews of Modern Physics 94, 015004 (2022).

[9] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii, J. R. McClean, K. Mitarai, X. Yuan,

L. Cincio, and P. J. Coles, Variational quantum algorithms, Nature Reviews Physics 3, 625 (2021).

[10] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou, P. J. Love, A. Aspuru-Guzik, and J. L. O’brien,

A variational eigenvalue solver on a photonic quantum processor, Nature communications 5, 1 (2014).

[11] E. Farhi, J. Goldstone, and S. Gutmann, A quantum approximate optimization algorithm, arXiv preprint

arXiv:1411.4028 (2014).

[12] A. Kandala, A. Mezzacapo, K. Temme, M. Takita, M. Brink, J. M. Chow, and J. M. Gambetta, Hardware-eﬃcient

variational quantum eigensolver for small molecules and quantum magnets, Nature 549, 242 (2017).

[13] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven, Barren plateaus in quantum neural

network training landscapes, Nature communications 9, 1 (2018).

[14] E. R. Anschuetz, Critical points in quantum generative models, arXiv:2109.06957 (2021).
[15] M.-H. Yung, J. Casanova, A. Mezzacapo, J. Mcclean, L. Lamata, A. Aspuru-Guzik, and E. Solano, From tran-

sistor to trapped-ion computers for quantum chemistry, Scientiﬁc reports 4, 1 (2014).

[16] U. Schollwöck, The density-matrix renormalization group in the age of matrix product states, Annals of Physics

326, 96 (2011).

[17] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature 521, 436 (2015).
[18] M. Bartholomew-Biggs, S. Brown, B. Christianson, and L. Dixon, Automatic diﬀerentiation of algorithms, J.

Comput. Appl. Math. 124, 171 (2000).

40

[19] A. Güneş Baydin, B. A. Pearlmutter, A. Andreyevich Radul, J. Mark Siskind, A. G. Baydin, B. A. Pearlmutter,
A. A. Radul, and J. M. Siskind, Automatic diﬀerentiation in machine learning: A survey, J. Mach. Learn. Res.
18, 1 (2018).

[20] J. Li, X. Yang, X. Peng, and C.-P. Sun, Hybrid quantum-classical approach to quantum optimal control, Phys.

Rev. Lett. 118, 150503 (2017).

[21] M. Schuld, V. Bergholm, C. Gogolin, J. Izaac, and N. Killoran, Evaluating analytic gradients on quantum

hardware, Phys. Rev. A 99, 032331 (2019).

[22] M. S. A. et al., Qiskit: An open-source framework for quantum computing (2021).
[23] C. Developers, Cirq (2021).
[24] H. H. team, Huawei HiQ: A high-performance quantum computing simulator and programming framework,

http://hiq.huaweicloud.com.

[25] K. Svore, A. Geller, M. Troyer, J. Azariah, C. Granade, B. Heim, V. Kliuchnikov, M. Mykhailova, A. Paz, and
M. Roetteler, Q# enabling scalable quantum computing and development with a high-level dsl, in Proceedings
of the real world domain speciﬁc languages workshop 2018 (2018) pp. 1–10.

[26] Y. Suzuki, Y. Kawase, Y. Masumura, Y. Hiraga, M. Nakadai, J. Chen, K. M. Nakanishi, K. Mitarai, R. Imai,
S. Tamiya, T. Yamamoto, T. Yan, T. Kawakubo, Y. O. Nakagawa, Y. Ibe, Y. Zhang, H. Yamashita, H. Yoshimura,
A. Hayashi, and K. Fujii, Qulacs: a fast and versatile quantum circuit simulator for research purpose, Quantum
5, 559 (2021).

[27] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H. Yoo, S. V. Isakov, P. Massey, R. Halavati, M. Y.
Niu, A. Zlokapa, et al., Tensorﬂow quantum: A software framework for quantum machine learning, arXiv preprint
arXiv:2003.02989 (2020).

[28] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, M. S. Alam, S. Ahmed, J. M. Arrazola, C. Blank, A. Del-
gado, S. Jahangiri, et al., Pennylane: Automatic diﬀerentiation of hybrid quantum-classical computations, arXiv
preprint arXiv:1811.04968 (2018).

[29] Paddle Quantum, https://github.com/PaddlePaddle/Quantum (2020).
[30] M. Developer, Mindquantum, version 0.5.0, https://gitee.com/mindspore/mindquantum (2021).
[31] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd, Quantum machine learning, Nature

549, 195 (2017).

[32] I. L. Markov and Y. Shi, Simulating quantum computation by contracting tensor networks, SIAM Journal on

Computing 38, 963 (2008).

[33] J. Brennan, M. Allalen, D. Brayford, K. Hanley, L. Iapichino, L. J. O’Riordan, M. Doyle, and N. Moran, Tensor

network circuit simulation at exascale, arXiv:2110.09894 (2021).

[34] E. A. Meirom, H. Maron, S. Mannor, and G. Chechik, Optimizing tensor network contraction using reinforcement

learning, arXiv:2204.09052 (2022).

[35] R. Orús, A practical introduction to tensor networks: Matrix product states and projected entangled pair states,

Annals of physics 349, 117 (2014).

[36] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin, R. Barends, R. Biswas, S. Boixo, F. G. S. L. Brandao,
D. A. Buell, B. Burkett, Y. Chen, Z. Chen, B. Chiaro, R. Collins, W. Courtney, A. Dunsworth, E. Farhi, B. Foxen,
A. Fowler, C. Gidney, M. Giustina, R. Graﬀ, K. Guerin, S. Habegger, M. P. Harrigan, M. J. Hartmann, A. Ho,
M. Hoﬀmann, T. Huang, T. S. Humble, S. V. Isakov, E. Jeﬀrey, Z. Jiang, D. Kafri, K. Kechedzhi, J. Kelly,
P. V. Klimov, S. Knysh, A. Korotkov, F. Kostritsa, D. Landhuis, M. Lindmark, E. Lucero, D. Lyakh, S. Mandrà,
J. R. McClean, M. McEwen, A. Megrant, X. Mi, K. Michielsen, M. Mohseni, J. Mutus, O. Naaman, M. Neeley,
C. Neill, M. Y. Niu, E. Ostby, A. Petukhov, J. C. Platt, C. Quintana, E. G. Rieﬀel, P. Roushan, N. C. Rubin,
D. Sank, K. J. Satzinger, V. Smelyanskiy, K. J. Sung, M. D. Trevithick, A. Vainsencher, B. Villalonga, T. White,
Z. J. Yao, P. Yeh, A. Zalcman, H. Neven, and J. M. Martinis, Quantum supremacy using a programmable
superconducting processor, Nature 574, 505 (2019).

[37] Y. Wu, W.-S. Bao, S. Cao, F. Chen, M.-C. Chen, X. Chen, T.-H. Chung, H. Deng, Y. Du, D. Fan, M. Gong,
C. Guo, C. Guo, S. Guo, L. Han, L. Hong, H.-L. Huang, Y.-H. Huo, L. Li, N. Li, S. Li, Y. Li, F. Liang, C. Lin,
J. Lin, H. Qian, D. Qiao, H. Rong, H. Su, L. Sun, L. Wang, S. Wang, D. Wu, Y. Xu, K. Yan, W. Yang,
Y. Yang, Y. Ye, J. Yin, C. Ying, J. Yu, C. Zha, C. Zhang, H. Zhang, K. Zhang, Y. Zhang, H. Zhao, Y. Zhao,
L. Zhou, Q. Zhu, C.-Y. Lu, C.-Z. Peng, X. Zhu, and J.-W. Pan, Strong quantum computational advantage using
a superconducting quantum processor, Phys. Rev. Lett. 127, 180501 (2021).

[38] C. Guo, Y. Liu, M. Xiong, S. Xue, X. Fu, A. Huang, X. Qiang, P. Xu, J. Liu, S. Zheng, H.-L. Huang, M. Deng,
D. Poletti, W.-S. Bao, and J. Wu, General-purpose quantum circuit simulator with projected entangled-pair
states and the quantum supremacy frontier, Phys. Rev. Lett. 123, 190501 (2019).

[39] F. Pan and P. Zhang, Simulation of quantum circuits using the big-batch tensor network method, Phys. Rev.

Lett. 128, 030501 (2022).

[40] Y. A. Liu, X. L. Liu, F. N. Li, H. Fu, Y. Yang, J. Song, P. Zhao, Z. Wang, D. Peng, H. Chen, C. Guo, H. Huang,
W. Wu, and D. Chen, Closing the "quantum supremacy" gap, in Proceedings of the International Conference for

41

High Performance Computing, Networking, Storage and Analysis (ACM, 2021).

[41] C. Huang, F. Zhang, M. Newman, J. Cai, X. Gao, Z. Tian, J. Wu, H. Xu, H. Yu, B. Yuan, M. Szegedy, Y. Shi,

and J. Chen, Classical simulation of quantum supremacy circuits (2020).

[42] X. Liu, C. Guo, Y. Liu, Y. Yang, J. Song, J. Gao, Z. Wang, W. Wu, D. Peng, P. Zhao, F. Li, H.-L. Huang,
H. Fu, and D. Chen, Redeﬁning the quantum supremacy baseline with a new generation sunway supercomputer,
arXiv:21111.01066 (2021).

[43] F. Pan, K. Chen, and P. Zhang, Solving the sampling problem of the sycamore quantum supremacy circuits,

arXiv:2111.03011 (2021).

[44] C. Roberts, A. Milsted, M. Ganahl, A. Zalcman, B. Fontaine, Y. Zou, J. Hidary, G. Vidal, and S. Leichenauer,

Tensornetwork: A library for physics and machine learning, arXiv:1905.01330 (2019).

[45] S.-X. Zhang, C.-Y. Hsieh, S. Zhang, and H. Yao, Diﬀerentiable quantum architecture search, arXiv:2010.08561

(2020).

[46] S.-X. Zhang, C.-Y. Hsieh, S. Zhang, and H. Yao, Neural predictor based quantum architecture search, Machine

Learning: Science and Technology 2, 045027 (2021).

[47] S.-X. Zhang, Z.-Q. Wan, C.-K. Lee, C.-Y. Hsieh, S. Zhang, and H. Yao, Variational Quantum-Neural Hybrid

Eigensolver, Physical Review Letters 128, 120502 (2022).

[48] S.-X. Zhang, Z.-Q. Wan, C.-Y. Hsieh, H. Yao, and S. Zhang, Variational quantum-neural hybrid error mitigation,

arXiv:2112.10380 (2021).

[49] S. Liu, S.-X. Zhang, C.-Y. Hsieh, S. Zhang, and H. Yao, Probing many-body localization by excited-state vqe,

arXiv:2111.13719 (2021).

[50] A. Kay, Tutorial on the quantikz package, arXiv preprint arXiv:1809.03842 (2018).
[51] M. Hessel, D. Budden, F. Viola, M. Rosca, E. Sezener, and T. Hennigan, Optax: composable gradient transfor-

mation and optimisation, in jax! (2020).

[52] Y. Li, X. Chen, and M. P. A. Fisher, Quantum zeno eﬀect and the many-body entanglement transition, Phys.

Rev. B 98, 205136 (2018).

[53] A. Chan, R. M. Nandkishore, M. Pretko, and G. Smith, Unitary-projective entanglement dynamics, Phys. Rev.

B 99, 224307 (2019).

[54] B. Skinner, J. Ruhman, and A. Nahum, Measurement-induced phase transitions in the dynamics of entanglement,

Phys. Rev. X 9, 031009 (2019).

[55] Y. Li, X. Chen, and M. P. A. Fisher, Measurement-driven entanglement transition in hybrid quantum circuits,

Phys. Rev. B 100, 134306 (2019).

[56] J. Gray, quimb: A python package for quantum information and many-body calculations, Journal of Open Source

Software 3, 819 (2018).

[57] S. R. White, Density matrix formulation for quantum renormalization groups, Phys. Rev. Lett. 69, 2863 (1992).
[58] D. G. a. Smith and J. Gray, opt_einsum - a python package for optimizing contraction order for einsum-like

expressions, Journal of Open Source Software 3, 753 (2018).

[59] J. Stokes, J. Izaac, N. Killoran, and G. Carleo, Quantum natural gradient, Quantum 4, 269 (2020).
[60] X. Yuan, S. Endo, Q. Zhao, Y. Li, and S. C. Benjamin, Theory of variational quantum simulation, Quantum 3,

191 (2019).

[61] S. Endo, J. Sun, Y. Li, S. C. Benjamin, and X. Yuan, Variational quantum simulation of general processes, Phys.

Rev. Lett. 125, 010501 (2020).

[62] S. McArdle, S. Endo, A. Aspuru-Guzik, S. C. Benjamin, and X. Yuan, Quantum computational chemistry, Rev.

Mod. Phys. 92, 015003 (2020).

[63] Y. Cao, J. Romero, J. P. Olson, M. Degroote, P. D. Johnson, M. Kieferová, I. D. Kivlichan, T. Menke, B. Per-
opadre, N. P. D. Sawaya, S. Sim, L. Veis, and A. Aspuru-Guzik, Quantum Chemistry in the Age of Quantum
Computing, Chemical Reviews 119, 10856 (2019).

[64] J. R. McClean, K. J. Sung, I. D. Kivlichan, Y. Cao, C. Dai, E. S. Fried, C. Gidney, B. Gimby, P. Gokhale,
T. Häner, T. Hardikar, V. Havlíček, O. Higgott, C. Huang, J. Izaac, Z. Jiang, X. Liu, S. McArdle, M. Neeley,
T. O’Brien, B. O’Gorman, I. Ozﬁdan, M. D. Radin, J. Romero, N. Rubin, N. P. D. Sawaya, K. Setia, S. Sim,
D. S. Steiger, M. Steudtner, Q. Sun, W. Sun, D. Wang, F. Zhang, and R. Babbush, Openfermion: The electronic
structure package for quantum computers, arXiv:1710.07629 (2017).

[65] N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-Herbrüggen, and S. J. Glaser, Optimal control of coupled spin
dynamics: design of nmr pulse sequences by gradient ascent algorithms, Journal of Magnetic Resonance 172,
296 (2005).

[66] X. Ni, H.-H. Zhao, L. Wang, F. Wu, and J. Chen, Integrating quantum processor device and control optimization

in a gradient-based framework, arXiv:2112.12509 (2021).

[67] G. Vidal, Eﬃcient classical simulation of slightly entangled quantum computations, Phys. Rev. Lett. 91, 147902

(2003).

[68] Y. Zhou, E. M. Stoudenmire, and X. Waintal, What limits the simulation of quantum computers?, Phys. Rev.

42

X 10, 041038 (2020).

[69] J. Spall, Adaptive stochastic approximation by the simultaneous perturbation method, IEEE Transactions on

Automatic Control 45, 1839 (2000).

[70] M. Ostaszewski, E. Grant, and M. Benedetti, Structure optimization for parameterized quantum circuits, Quan-

tum 5, 391 (2021).

[71] I. A. Luchnikov, A. Ryzhov, S. N. Filippov, and H. Ouerdane, QGOpt: Riemannian optimization for quantum

technologies, SciPost Phys. 10, 79 (2021).

43

