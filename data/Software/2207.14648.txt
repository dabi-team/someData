Using Graph Neural Networks for Program Termination

Yoav Alon
yoav.alon@bristol.ac.uk
University of Bristol
Bristol, UK

Cristina David
cristina.david@bristol.ac.uk
University of Bristol
Bristol, UK

ABSTRACT
Termination analyses investigate the termination behavior of pro-
grams, intending to detect nontermination, which is known to cause
a variety of program bugs (e.g. hanging programs, denial-of-service
vulnerabilities). Beyond formal approaches, various attempts have
been made to estimate the termination behavior of programs us-
ing neural networks. However, the majority of these approaches
continue to rely on formal methods to provide strong soundness
guarantees and consequently suffer from similar limitations. In
this paper, we move away from formal methods and embrace the
stochastic nature of machine learning models. Instead of aiming for
rigorous guarantees that can be interpreted by solvers, our objective
is to provide an estimation of a program’s termination behavior
and of the likely reason for nontermination (when applicable) that
a programmer can use for debugging purposes. Compared to pre-
vious approaches using neural networks for program termination,
we also take advantage of the graph representation of programs by
employing Graph Neural Networks. To further assist programmers
in understanding and debugging nontermination bugs, we adapt the
notions of attention and semantic segmentation, previously used for
other application domains, to programs. Overall, we designed and
implemented classifiers for program termination based on Graph
Convolutional Networks and Graph Attention Networks, as well as
a semantic segmentation Graph Neural Network that localizes AST
nodes likely to cause nontermination. We also illustrated how the
information provided by semantic segmentation can be combined
with program slicing to further aid debugging.

CCS CONCEPTS
• Computing methodologies → Artificial intelligence; Knowl-
edge representation and reasoning; Neural networks.

KEYWORDS
Graph Neural Networks, Graph Attention Networks, Program Ter-
mination

2
2
0
2

l
u
J

8
2

]
E
S
.
s
c
[

1
v
8
4
6
4
1
.
7
0
2
2
:
v
i
X
r
a

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE 2022, 14 - 18 November, 2022, Singapore
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/1122445.1122456

ACM Reference Format:
Yoav Alon and Cristina David. 2022. Using Graph Neural Networks for
Program Termination. In Proceedings of The 30th ACM Joint European Soft-
ware Engineering Conference and Symposium on the Foundations of Soft-
ware Engineering (ESEC/FSE 2022). ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Termination analysis describes a classical decision problem in com-
putability theory where program termination has to be determined.
It is critical for many applications such as software testing, where
nonterminating programs will lead to infinite executions. As proved
by Turing in 1936, a general algorithm that solves the termina-
tion problem for all possible program-input pairs doesn’t exist
[60]. While there are a large number of works on termination
analysis, the majority of them employ formal symbolic reason-
ing [15, 22, 23, 25, 33, 36]. In recent years, various attempts have
been made to estimate termination behavior using neural networks.
For instance, Giacobbe et al.[29] introduced an approach where
neural networks are trained as ranking functions (i.e. monotone
maps from the program’s state space to well-ordered sets). A similar
idea is employed in [3], where Abate et al. use a neural network to
fit ranking supermartingale (RMS) over execution traces. Given that
program analysis tasks such as termination analysis are generally
expected to provide formal guarantees, these works use satisfia-
bility modulo theories (SMT) solvers to show the validity of their
results. While promising, they still face limitations specific to for-
mal symbolic methods. Namely, programs need to be translated to
a symbolic representation to generate the verification conditions
that are then passed to the solver. Additionally, these verification
conditions may be expressed in undecidable logical fragments or
may require extra program invariants for the proof to succeed.

In this paper, we move away from formal methods and lean into
the stochastic nature of machine learning models. Instead of looking
for rigorous formal guarantees that can be interpreted by solvers,
our objective is to provide an estimation of a program’s termination be-
havior, as well as localizing the likely cause of nontermination (when
applicable) that a programmer can use for debugging purposes. Our
work also serves as a study of the applicability of machine learning
techniques previously used for other classes of applications to pro-
gram analysis. In particular, as explained next, we use Graph Neural
Networks (GNNs) [66] and Graph Attention Networks (GANs) [61].
Instead of looking at execution traces like the aforementioned
works, we are interested in using the source code with the assump-
tion that it contains patterns that can assist in understanding its
functionality. Notably, program analysis techniques generally work
on source code, and specifically on graph representations of pro-
grams. To emulate this for machine learning, we make use of GNNs,

 
 
 
 
 
 
ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Alon and David

which are a class of neural networks optimized to perform vari-
ous analyses on graph-structured data. GNNs are gaining a lot of
interest as they are being used to analyze graph-based systems
denoting social networks [65], physical systems [53], knowledge
graphs [34], point-cloud classification [71] etc. Additionally, GNNs
have recently been applied to program analysis tasks such as vari-
able misuse detection and type inference [7], and self-supervised
bug detection and repair [9].

Inspired by [7, 9], we use GNNs to estimate program termination.
Our baseline program termination classifier is based on the Graph
Convolutional Networks (GCN) [40].

On its own, estimating a program’s termination behavior doesn’t
provide a lot of practical help a programmer interested in under-
standing and debugging a nontermination bug. Rather, we would
like to provide additional information such as the code location
corresponding to the likely root cause of the failure (in our case
nontermination). This objective is similar to that of fault localiza-
tion, which takes as input a set of failing and passing test cases,
and produces a ranked list of potential causes of failure [41].

As opposed to fault localization techniques, we are interested
in investigating using the mechanisms of attention and semantic
segmentation from machine learning. To the best of our knowledge,
we are the first ones to use attention and segmentation in the
context of programs.

Attention is a technique that mimics cognitive attention. Intu-
itively, it enhances some parts of the input data while diminishing
others with the expectation that the network is focusing on a small,
but important part of the data. In our context, we use attention to
get an intuition about the instructions relevant for the estimation
of the termination behavior. This allows us to visualize those parts
of the program that the neural network focuses on to estimate
its termination behavior. To integrate attention in our work, we
build another program termination classifier inspired by the Graph
Attention Network (GAT) architecture described in [61]. Given
the varied influence that different instructions in a program have
on its termination behavior, we also expect the attention mecha-
nism to improve the results of classification when compared to the
GCN-based baseline.

To localize the likely cause of the nontermination behavior, we
use semantic segmentation. Usually used in image recognition,
the goal of semantic image segmentation is to label each pixel of
an image with a particular class, allowing one to identify objects
belonging to that class (e.g. given an image, one can identify a
cat that appears in it). In our work, we use the same principle for
programs to identify those statements that cause nontermination
vs those that don’t.

To further aid debugging, we also show how to use the informa-
tion provided by semantic segmentation to carve out a nontermi-
nating slice from the original program (i.e. a smaller subprogram
exhibiting the same nontermination behavior as the original pro-
gram). Intuitively, such a smaller program is easier to understand
and debug.

Our experimental evaluation for multiple datasets, both custom
and based on benchmarks from software verification competitions,
confirms a high ability to generalize learned models to unknown
programs.

The main contributions of this research are as follows:

• We designed and implemented a GCN-based architecture for

the binary classification of program termination.

• We designed and implemented a GAT-based architecture that
improves the termination classification using a self-attention
mechanism and allows visualization of the nodes relevant
when estimating termination.

• We designed and implemented a semantic segmentation GAT
that localizes nodes causing nontermination. In particular,
in this work, we try to localize the outermost infinitely loop-
ing constructs. We illustrate how the information provided
by semantic segmentation can be combined with program
slicing to further aid debugging.

• We devised datasets for both classification and segmentation

of program termination.

2 PRELIMINARIES ON GRAPH NEURAL

NETWORKS

GNNs are an effort to apply deep learning to non-euclidean data
represented as graphs. These networks have recently gained a lot of
interest as they are being used to analyze graph-based systems [34,
53, 65, 71]. A comprehensive description of existing approaches for
GNNs and their applications can be found in [66, 72, 73].

The basic idea behind most GNN architectures is graph convo-
lution or message passing, which is adapted from Convolutional
Neural Networks (CNN). Each vertex (node) in the graph has a set
of attributes, which we refer to as a feature vector or an embed-
ding. Then, a graph convolution estimates the features of a graph
node in the next layer as a function of the neighbors’ features. By
stacking GNN layers together, a node can eventually incorporate
information from other nodes further away. For instance, after 𝑛
layers, a node has information about the nodes 𝑛 steps away from
it.

Given a graph 𝐺 = (𝑉 , 𝐸), where 𝑉 denotes the set of vertices
and 𝐸 represents the set of edges, message passing works as follows.
For each node 𝑖 ∈ 𝑉 and its embedding ℎ (𝑙)
at time step 𝑙, the
𝑖
embeddings ℎ (𝑙)
of its neighbours 𝑗 ∈ 𝑁 (𝑖) are aggregated and the
𝑗
current node’s embedding is updated to ℎ (𝑙+1)
function 𝐴 and update function 𝑈 :

using aggregation

𝑖

ℎ (𝑙+1)
𝑖

= 𝑈 (𝑙) (ℎ𝑙

𝑖, 𝐴(𝑙) ({ℎ (𝑙)

𝑗

, ∀𝑗 ∈ 𝑁 (𝑖)}))

(1)

We use 𝑁 (𝑖) to describe the set of direct neighbors of 𝑖. Each
GNN architecture varies the implementation of the update 𝑈 and
aggregation 𝐴 functions used for message passing. In this paper,
we make use of two GNN architectures: Graph Convolutional Net-
works (GCNs) and Graph Attention Networks (GATs), which we
will discuss in subsequent sections.

3 DESCRIPTION OF OUR TECHNIQUE
In this section, we provide details about our technique. At a high
level, we first convert programs to feature graphs and then feed
them into a GNN. We will describe each step, including the structure
of the models for termination classification and semantic segmen-
tation of the nodes responsible for nontermination.

Using Graph Neural Networks for Program Termination

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

(a)

(c)

(b)

(d)

Figure 1: (a) AST (for clarity, we point out the corresponding lines of code). (b) Visualization of attention for binary classifica-
tion as edge color from blue for low attention to red for high attention. The edges with high attention are those connecting
the while nodes to their loop guards. Among these, the highest attention is given to the nonterminating outer loop. (c) Re-
sult of semantic segmentation, where the node corresponding to the outermost infinite loop is coloured in red. (d) Semantic
segmentation visualized together with the attention extracted from the segmentation network. Best viewed in color.

3.1 Generation of feature graphs
There are many graph representations of a program. In this work,
we start from the Abstract Syntax Tree (AST), which is a homoge-
nous, undirected graph, where each node denotes a construct occur-
ring in the program. We picked ASTs as the starting point because
they are simple to understand and construct while containing all the
necessary information for investigating a program’s termination.
As future work, we may consider different graph representations
of programs.

We start by generating the AST of each program in our datasets.
Then, all ASTs are converted into feature graphs by converting each
node to a local feature vector in local representation or ’one-hot’-
encoding. For this purpose, all the nodes in the ASTs generated for

each dataset are gathered in a dictionary. The dictionary holds each
encoding as key with the instruction as value.

Running example. For illustration, we refer to our running ex-
ample in Figure 3, which contains three loops. Out of them, the
first outer loop is nonterminating for any initial value of 𝑏 less than
2 and of 𝑎 greater than 𝑏. Note that for machine integers (i.e. bit-
vectors), the inner loop does terminate even when starting from a 𝑐
greater than 𝑑 because 𝑑 will eventually underflow, thus triggering
a wrap-around behavior.

The running example is first converted to its AST as shown in
Figure 1(a). Then, the AST is converted to the initial feature graph
(before any convolutions). For reasons of space, we don’t show the
whole feature graph, but rather just the subgraph corresponding

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Alon and David

uniform and the bias 𝑏 with zero. The experiments of [40] with
other aggregation operators rather than the one used here, such
as Multi-Layer-Perceptron aggregator [70] and Graph Attention
Networks [61], achieved similar results.

For the architecture in [40], the graph convolution layers gener-
ate node-wise features. In our architecture, given that termination
classification is a graph-level task, we choose to mean all the nodes
once graph convolution is done and pass the resulting mean feature
vector through three fully connected layers. Then, we apply a soft-
max function on the resulting two-dimensional vector to achieve a
binary estimation. The resulting vector is then optimized by com-
puting the cross-entropy loss with the ground truth.

Running example. For our running example in Figure 3, our clas-
sifier correctly concludes that the program is nonterminating. As
aforementioned, we operate in a supervised training setting, mean-
ing that our classifier has already been trained on the required
dataset. We will give more details on the training and testing in
Section 4.

3.3 Binary classification of termination based

on GAT

√

𝑐𝑖 𝑗 =

1
√
|𝑁 (𝑖) |

If we revisit the update rule of GCNs given by Equation 2, it uses
the coefficient 1
. Intuitively, this coefficient
|𝑁 ( 𝑗) |
suggests the importance of node 𝑗’s features for node 𝑖 and it is
heavily dependent on the structure of the graph (i.e. each node’s
neighbours). The main idea of GATs [61] is to compute this coeffi-
cient implicitly rather than explicitly as GCNs do by considering it
to be a learnable attention mechanism. In the rest of the section,
we refer to this coefficient as the attention score.

The main observation that led us to use GATs is the fact that not
all instructions in a program are equally important when investi-
gating its termination behavior. For instance, loops tend to be more
important than straight-line code, and should therefore be given
increased attention. Thus, our task naturally lends itself to using
the attention mechanism. Moreover, our intention for this work is
not only to design a program termination classifier but also to gain
insights into its decisions. Using GATs helps us in this direction
as it allows us to visualize those nodes that influence the decision
related to termination.

Next, we explain next how the aggregation and update of node
features ℎ (𝑙)
for iteration 𝑙 is derived. Initially, each node
𝑖
feature is passed through a simple linear layer by multiplying the
feature with a learnable weight matrix 𝑊 (𝑙) :

to ℎ (𝑙+1)
𝑖

(3)

𝑖 = 𝑊 (𝑙)ℎ (𝑙)
𝑧 (𝑙)
𝑖
To obtain the pair-wise importance 𝑒 (𝑙)
ℎ (𝑙)
𝑖
𝑧 (𝑙)
𝑖
fed into another linear layer with weight matrix𝑊 (𝑙)
that learns the
2
attention scale 𝑎 (𝑙) . Additionally, a leakyReLU activation function
[68] is applied to introduce non-linearity:

𝑗 we concatenate the previously computed embeddings
). Then, the concatenated embeddings are

𝑖 𝑗 of two neighbor features

and ℎ (𝑙)
and 𝑧 (𝑙)
𝑗

to (𝑧 (𝑙)
𝑖

|𝑧 (𝑙)
𝑗

𝑎 (𝑙) = LeakyReLU(𝑊 (𝑙) (𝑧 (𝑙)
𝑖

|𝑧 (𝑙)
𝑗

))

(4)

Figure 2: Conversion of an AST to a local feature graph (us-
ing ’one-hot’-encoding).

1
2
3
4
5
6
7
8
9

d e f main ( a , b , c , d )

:

while a > b :

a = 2
while c > d :

d −= 2

while a > 0 :

a −= 2

Figure 3: Running example

to the instructions while c>d: d-=2 in Figure 2. The dictionary
maps each node in the AST to a one-hot encoding. For instance,
the while node is mapped to 0 · · · 00010000. Similarly, both nodes
corresponding to variable d have the same encoding, 0 · · · 00000010.
As we use the same dictionary for a whole dataset, identical nodes
in the dataset will have the same encoding.

3.2 Binary classification of termination based

on GCN

For our first attempt at a binary classifier for program termination,
we make use of GCN. In particular, we take inspiration from the
architecture in [40] in a supervised training setting. Currently, we
do not train on recursive programs, where the nontermination could
be caused by infinite recursion. We plan to do this as future work.
For now, our datasets contain programs with potentially infinite
loops.

Our termination classification is a graph level task, where we
predict an attribute for the entire graph (in our case the termination
behavior of the corresponding program) by aggregating all node
features.

As explained in Section 2, GNNs use message passing to aggre-
gate the information about a node’s neighbors to update the node’s
value. According to [40], we use:

ℎ (𝑙+1)
𝑖

= 𝜎 (𝑏 (𝑙) +

∑︁

ℎ (𝑙)
𝑗 𝑊 (𝑙) )

(2)

1
𝑐𝑖 𝑗

𝑗 ∈𝑁 (𝑖)
where 𝑐𝑖 𝑗 = √︁|𝑁 (𝑖)|√︁|𝑁 ( 𝑗)|, 𝑁 (𝑖) describes the set of direct

neighbours of 𝑖, and 𝜎 is the activation function.

Best results are achieved using a rectified linear unit (ReLu)
activation function. The weights 𝑊 (𝑙) are initialized using Glorot

Using Graph Neural Networks for Program Termination

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

to the three loops is not reliable enough to differentiate between
the three loops and determine the cause for nontermination.

3.4 Semantic segmentation of nodes causing

nontermination

While GATs provide some insight into the decision made by the
classifier (by allowing us to visualize those AST nodes influencing
the decision), we want to find the likely cause of nontermination. In
particular, in this work, we try to localize the outermost infinitely
looping constructs. Note that in the case where we have several
nested loops and the outer one is infinitely looping, all of the inner
ones will also be visited infinitely often. In such a situation, we
identify the outer loop as the likely cause of nontermination, i.e. the
outermost infinitely visited loop. As aforementioned, for now, we do
not train on recursive programs, where the nontermination could
be caused by infinite recursion. Thus, in our setting nontermination
can only be caused by infinite loops.

Identifying the outermost infinitely visited loop is not possible
with the information that we gained up until this point. Although
in Figure 1(b), the edge between the nonterminating outer loop and
its guard gets the highest attention, the edges connecting the other
two loops and their guards also have high attention scores. Thus,
it is hard to differentiate between the different loops based on the
attention score alone.

To achieve our goal of identifying the outermost infinitely visited
loop, we make use of semantic segmentation. Semantic segmenta-
tion has been primarily used in image recognition to label each pixel
of an image with a particular class, allowing one to identify objects
belonging to that class. Sometimes in the literature, the semantic
segmentation of graphs is also named ’node-wise classification’ or
’node-classification’.

In this work, we attempt to extend the same principle that was
applied in image recognition to programs by using semantic seg-
mentation to identify those statements that cause nontermination.
If we consider that program statements can belong to two classes,
namely those that cause nontermination and those that don’t, we
can envision that segmentation may help identify the former.

The graph network must convert feature vectors to an estimation.
For this purpose, it is trained on the loss between ground truth and
prediction on a node level. This can be accomplished with most
segmentation loss functions such as a simple cross-entropy loss for
binary segmentation with ground truth 𝑦 and prediction ˆ𝑦:

𝐿(𝑦, ˆ𝑦) = −(𝑦 log( ˆ𝑦) + (1 − 𝑦) log(1 − ˆ𝑦))

(8)

Although the above loss enables the setup of an initial training
session, it is not ideal to eradicate hard negatives. To improve
and continue training beyond binary cross-entropy convergence,
we use focal loss [39, 46], an extension of cross-entropy, which
down-weights simple samples and gives additional weight to hard
negatives:

𝐹 𝐿(𝑝𝑡 ) = −𝛼𝑡 (1 − 𝑝𝑡 )𝛾𝑙𝑜𝑔(𝑝𝑡 )
(9)
with 𝛾, 𝛼 ∈ [0, 1] and the modulation fact (1 − 𝑝𝑡 )𝛾 , where 𝑝𝑡 = 𝑝
for 𝑦 = 1 and 𝑝𝑡 = 1 − 𝑝 otherwise.

Following the same message passing method explained in Sec-
tion 3.2, the new node features hold the segmentation prediction.

Figure 4: GAT-based architecture for the binary classifica-
tion of program termination.

After computation of 𝑎, corresponding to the activation scale, the
pair-wise importance 𝑒 (𝑙)

𝑖 𝑗 of two neighbor features is:

𝑒 (𝑙)
𝑖 𝑗 = LeakyReLU( (cid:174)𝑎 (𝑙)𝑇

(𝑧 (𝑙)
𝑖

|𝑧 (𝑙)
𝑗

))

(5)

As noted by [61], the resulting attention scale can be considered
as edge data. In our problem setting, this gives us insight into the
importance of two connected syntax tree nodes with respect to the
program’s termination.

To normalize the attention scores for all incoming edges we a

apply a softmax layer:

𝛼 (𝑙)
𝑖 𝑗 =

exp(𝑒 (𝑙)
𝑖 𝑗 )
𝑘 ∈𝑁 (𝑖) exp(𝑒 (𝑙)
(cid:205)
𝑖𝑘 )

(6)

Then, the neighbor embeddings are aggregated and scaled by the
final attention scores 𝛼 (𝑙)
𝑖 𝑗

:

ℎ (𝑙+1)
𝑖

= 𝜎 (cid:169)
(cid:173)
(cid:171)

∑︁

𝑗 ∈𝑁 (𝑖)

𝑖 𝑗 𝑧 (𝑙)
𝛼 (𝑙)
𝑗 (cid:170)
(cid:174)
(cid:172)

(7)

The architecture of our GAT-based termination classifier is shown
in Figure 4 and includes multiple connected GAT layers with ReLu
activation. Graph convolutions extract features to an intermediate
representation, denoted by the last GAT layer with ReLu activa-
tion. Conversely to regular graph attention networks, we devise a
prediction on the program’s termination by extracting the mean
of node features and using dense layers with a final softmax layer.
The attention information is extracted from the last GAT layer and
can be associated with the AST edges.

Running example. Similar to the GCN, the GAT-based classifier
identifies that the running example is nonterminating. Additionally,
in Figure 1(b) we also obtain a visualization of edge-wise attention
from blue for low attention to red for high attention. Notably, the
edges with high attention are those connecting the while nodes to
their loop guards. The extraction of attention gives us an intuition
about which nodes have a high influence on the final prediction
provided by the classifier. Although for this example, the edge con-
necting the nonterminating outer loop with its loop guard gets the
highest attention, the difference between the attention scores linked

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Alon and David

1
2
3

d e f main ( a , b )

:

while a > b :

a = 2

Figure 6: Nonterminating slice of the running example

aims to extract parts of a program according to a particular slicing
criterion (e.g. it can extract instructions responsible for the write
accesses to a particular variable). In our scenario, given a nontermi-
nating loop returned by segmentation, we can use several slicing
criteria in order to isolate the faulty loop, while preserving the
nontermination behavior highlighted by segmentation. One option,
which we illustrate below on the running example in Figure 3, is
a criterion aiming to preserve the reachability of a control flow
point (in particular, we are interested in the control flow point at
the entrance to the nonterminating loop). Among other tools, such
a slicing capability is provided by the software analysis platform
Frama-C [10]. Other slicing criteria can be used, e.g. preserving the
values of the variables used by the guard of the infinite loop. Slicing
has been previously used in the context of debugging nontermina-
tion, e.g. failure-slices in [51].

Going back to our running example in Figure 3, once segmen-
tation has identified the first outer loop as the likely culprit for
nontermination, we slice the program such that we preserve the
reachability of the control flow point at the entrance to the body
of the nonterminating outer loop. This gives us the program in
Figure 6. This sliced program preserves the nontermination be-
havior of the original program (induced by the loop identified by
segmentation) while cutting down the syntactic structure, which
corresponds to reducing its state space.

Intuitively, cutting down the state space of the program makes
it more amenable to other existing debugging techniques such as
fuzzing. For illustration, we have asked libFuzzer [1] (a library for
coverage-guided fuzz testing) to search for an input that triggers
execution of the sliced program longer than 5s. LibFuzzer was able
to find the input 𝑎 = 165017090 and 𝑏 = -183891446 within 6s. This
input triggers the nontermination behavior and can assist in further
debugging. Conversely, when asking libFuzzer to find such input
for the original in Figure 3, it required 12s. We expect the difference
between the two running times to increase for larger programs.

Overall, the debugging workflow that we envision follows the
following steps: (1) our classifier estimates that a program is non-
terminating; (2) our technique based on GNNs and semantic seg-
mentation finds the likely root cause of nontermination; (3) slicing
cuts down the syntactic structure of the program and implicitly its
state space, while aiming to maintain the faulty nontermination
behavior; (4) fuzzing finds faulty inputs for the sliced program.

If a program contains several nontermination bugs, the debug-
ging workflow above may need to be repeated until all of the bugs
are eliminated.

4 EXPERIMENTS
4.1 System
For our experiments, we used the deep learning framework PyTorch
[52] with the GNN package DGL (Deep Graph Library) and various

Figure 5: Proposed GAT-based architecture for the segmen-
tation of nodes causing nontermination.

Our architecture for semantic segmentation is visualized in Fig-
ure 5. Graph attention convolution generates a semantic segmen-
tation resulting in a node-wise estimation of likelihood to cause
nontermination. By the attention mechanism, edge-wise scores
denote the weight of features based on relational patterns.

Running example. The result of semantic segmentation for our
running example is highlighted in Figure 1 (c), where the red-
colored node denotes the outermost infinite loop. Figure 1 (d) shows
the result of segmentation with attention.

3.5 Using the result of segmentation for

debugging

One issue with the result of segmentation is that it only highlights
the node corresponding to the head of the outermost nontermi-
nating loop, rather than all the statements contributing to nonter-
mination. This is obvious for our running example in Figure 1 (c),
where segmentation only identifies the node corresponding to the
nonterminating while loop.

This was a conscious decision on our part, meant to simplify
the construction of the required training datasets. While there
are a huge number of programs available online, it’s not at all
straightforward to use them for training machine learning-based
techniques for program analysis. The main reason for this is that
such programs are not labeled with the result of any analysis, and
manually labeling them is difficult and error-prone. In particular, for
the termination classification task, we must know which programs
are terminating and which are not. Even more problematic, for
semantic segmentation, the annotation should, in principle, identify
all the instructions contributing to nontermination. For instance,
for our running example, it should identify lines 3 and 4. In general,
it can be very difficult to annotate such datasets.

In this work, to simplify the annotation for segmentation, we
only label the node corresponding to the head of the outermost
infinite loop as the reason for nontermination. More details on the
way we generate our datasets are given in Section 4.

While identifying the outermost infinite loop is already useful,
debugging can be further aided by finding the rest of the instruc-
tions that contribute to nontermination. This can be achieved with
slicing [63]. In general, slicing is a program analysis technique that

Using Graph Neural Networks for Program Termination

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

packages for preprocessing and visualization. The experiments
were performed using an RTX 2080 8GB GPU on a local machine.
The architecture of the GNNs used, depending on the particular
experiment consists of between 4 and 6 graph layers. While this
is typical for the effective training of GNNs, it could be extended
using residual blocks [44].

4.2 Binary classification of program

termination

We start by discussing our experimental evaluation for the binary
classification of program termination. Given a program, our classi-
fier extracts a termination estimation, i.e. whether the program is
terminating or nonterminating. Concerning the latter, a program is
considered nonterminating if there exists an input for which the
program’s execution is infinite.

4.2.1 Classification datasets. Our experiments on classification
and semantic segmentation of the cause for nontermination re-
quire different datasets. This is because classification data requires
program-level annotations (a program is labeled as terminating or
nonterminating), whereas segmentation requires node-wise annota-
tions (each node in the AST is either a cause for nontermination or
not). In this section, we discuss the datasets used for classification,
whereas more details on those used for segmentation can be found
in Section 4.3.1.

For classification, we use a total of four datasets, out of which
two are based on existing benchmarks and two are generated by
us. Concerning the datasets containing existing benchmarks, the
first one, DS-SV-COMP, contains C programs from the termination
category of the SV-COMP 2022 competition [2, 12]. In particular,
this dataset contains all programs from the following subcategories:
termination-crafted, termination-crafted-lit, termination-numeric,
and termination-restricted-15. In total, there are 55 nonterminating
programs and 194 terminating. For a total of 249 programs, we
count a total of 386 loops with a maximum of five nested loops for
the program NO_04.c. The second dataset of existing benchmarks,
DS-TERM-COMP, contains 150 C programs from the Termination
Competition [31] with a total of 452 loops and a maximum of 2
nested loops per program. These benchmarks are selected such that
there is no overlap with those in the DS-SV-COMP dataset.

The benchmarks in both DS-SV-COMP and DS-TERM-COMP
expect inputs meaning that their termination behavior depends on
non-deterministic values. These benchmarks come already labeled
as terminating or nonterminating, where the nontermination label
indicates that there exists an input for which the program’s execu-
tion doesn’t terminate. The pre-processing of the datasets includes
the generation of the AST representation for each program and the
conversion to a feature graph using DGL.

For efficiency reasons, we consider batches of 30 graphs using the
𝑑𝑔𝑙 .𝑏𝑎𝑡𝑐ℎ. The two datasets have an associated dictionary featuring
all the distinct AST nodes from all the programs. Intuitively, this
dictionary gives us the vocabulary used by the benchmarks. For
more details on the generation of feature graphs, see Section 3.1.
As previously discussed in Section 3.5, one of the main challenges
of our work is the fact that, while there are many available programs,
only a few are already labeled as terminating or nonterminating (as
we could see above, even existing software verification competitions

only consider a relatively small number of benchmarks). However,
machine learning techniques generally require large training data,
which is not available in our setting. Due to this, we chose to also
generate our additional custom datasets. For the first custom dataset,
DS1, we used the dictionary generated for DS-SV-COMP and DS-
TERM-COMP, meaning that the vocabulary of DS1 is the same as
that of DS-SV-COMP and DS-TERM-COMP. This is important as
it allows us to train on DS1 and only use DS-SV-COMP and DS-
TERM-COMP for testing. DS1 contains 950 C programs. The second
custom dataset, DS2, contains 950 generated Python programs, out
of which 800 are used for training and 150 for testing.

To label the benchmarks in DS1 and DS2, we fuzz test for nonter-
mination by generating a large number of inputs for each program.
If the execution time reaches a predefined timeout for at least one
of the inputs, then we label the program as nonterminating. The
dataset is balanced by providing an equal number of terminating
and nonterminating samples.

Apart from DS-SV-COMP and DS-TERM-COMP, each dataset is
split into training and test samples by the general rule of approxi-
mately 80/20 depending on the dataset size. The exact split for each
set is specified in Table 1.

4.2.2 Training. As explained in Section 3, we deploy two different
neural networks, one based on GCN [40] and another one based
on GAT [61]. In our experiments, we found that a number of 4
layers was sufficient for both the GCN and GAT architectures,
respectively. Training is performed using an Adam-optimizer with
an initial learning rate of 0.0001. We use a regular cross-entropy loss
function and record various metrics such as the Receiver Operating
Characteristic (ROC) curve and the Precision-Recall during training
both for validation and testing.

For a significant evaluation, we perform a total of ten training

sessions until convergence of the test metrics.

4.2.3 Results and evaluation. To judge the performance of our
classifier, we use the Precision-Recall (PR) and Receiver Operating
Characteristic (ROC) and their respective Area Under the Curve
(AUC) and Average Precision (AP). AUC takes values from 0 to 1,
where value 0 indicates a perfectly inaccurate test and 1 reflects a
perfectly accurate test.

PR is used as an indication for the tradeoff between precision
and recall for different thresholds. Consequently, high recall and
high precision reflect in a high area under the curve. High precision
is based on a low rate of false positives and a high recall is based
on a low rate of false negatives.

The ROC curve plots the true positive rate versus the false posi-
tive rate for each threshold. The closer to the top-left corner a ROC
curve is, the better, and the diagonal line corresponds to random
guessing. A high area under the curve indicates a better classifica-
tion performance.

The results of the experimental evaluation of our classifier are
summarized in Table 2, where we record the AUC for positive
(terminating) instances, the AUC for negative (nonterminating)
instances, and the mean Average Precision (mAP), which is calcu-
lated as the average of the AP for terminating and nonterminating
instances.

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Alon and David

Dataset

Origin

Language

Experiment

Training samples Test samples

DS-SV-COMP
SV-Comp
DS-TERM-COMP TermComp

C
C

DS1
DS2

DS-Seg-Py1

DS-Seg-C

custom
custom

custom

custom

C
Python

classification
classification

classification
classification

Python

semantic segmentation

C

semantic segmentation

Table 1: Datasets used in this work.

-
-

800
800

180

180

249
150

150
150

50

50

The first and arguably most important observation is that, with
values of over 0.82 for all mAPs for both PR and ROC, we can con-
clude that classification results are significant and that we achieve
a high ability to generalize for unknown data.

The second remark refers to the comparison of GCN and GAT.
As observed in Table 2, the PR and ROC mAP numbers are generally
higher for GAT than GCN, suggesting that the application of graph
attention improves the result of the binary classification (we use
bold font for the higher mAP numbers). This indicates that the
applied self-attention mechanism does assign a higher weight to
patterns responsible for deciding the termination behavior.

The third comparison we are interested in is between the classi-
fication of terminating and nonterminating programs. We notice
that the PR AUC negative tends to be higher than AUC positive,
meaning that the classifier performs better at identifying nontermi-
nating programs. We conjecture that nonterminating patterns are
easier to identify based on relational probabilistic patterns that are
likely to cause nontermination.

In Figure 7 we visualize the ROC curves per class and layer type.
The better performance for nontermination classes is reflected in
higher red curves while the better performance of GAT layers is
represented by higher continuous lines compared to doted CGN
curves.

4.3 Semantic segmentation of nodes causing

nontermination

In this section, we describe our experiments for identifying the
outermost nonterminating node using semantic segmentation.

Segmentation datasets. Several reasons prevent us from reusing

4.3.1
the same benchmarks from DS-SV-COMP and DS-TERM-COMP for
semantic segmentation. Firstly, they only contain a program-level
label denoting whether the program terminates. For semantic seg-
mentation, we require node-level annotations that indicate whether
the corresponding AST node is likely to cause nontermination or
not. Secondly, it is often very difficult to determine the cause for
nontermination in existing benchmarks, as it isn’t labeled.

For this reason, we generate two datasets that consist only of
nonterminating samples (as we would only attempt to find the
cause for nontermination for those programs that were deemed
nonterminating by our classifier). As explained in Section 3.5, we
restrict our annotations to only identify the outermost nontermi-
nating loop in the program. Note that if there are several distinct,
outermost, nonterminating loops, only one of them will be labeled

Figure 7: ROC curves per class (class 0 corresponds to non-
terminating programs and class 1 to terminating ones) and
experimental configuration.

as causing nontermination. The reason for this is the fact that we
use fuzz testing to determine the culpable loop, and we stop the
fuzzing once we found the first nonterminating scenario. We use
label 0 to indicate that the corresponding AST node has no rela-
tion to nontermination, and label 1 to indicate a high likelihood of
causing nontermination.

The two datasets are described in the last two rows of Table 1.
The first, DS-SegPy1 contains Python programs, whereas the sec-
ond, DS-Seg-C, contains C programs. Each dataset is composed of
180 programs for training and 50 for testing. The program sample
generator for custom data is probabilistic and generates programs
that contain from two to five loops that can be nested.

4.3.2 Training. To improve and continue training beyond binary
cross-entropy convergence, we deployed a focal loss [39, 46], an
extension of cross-entropy that down-weights simple samples and
gives additional weight to hard negatives.

To achieve optimal results, the experiments required modifying
the learning rate, optimizer, network structure, and the number of
features compared to those used for classification. The final config-
uration features an Adam optimizer with a learning rate of 0.001. In
our experiments, we found that a number of 4 layers was sufficient

Using Graph Neural Networks for Program Termination

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

for both the GCN and GAT architectures, respectively. While the
GCN and GAT architectures used for semantic segmentation are
respectively similar to those used for classification, they do not
contain a node mean function and linear layers. The last GCN/-
GAT layer projects to a one-dimensional feature indicating the
confidence that a node is causing nontermination.

Similar to the experiments for classification, we perform a total

of ten training sessions until convergence of the test metrics.

4.3.3 Results and evaluation. The metrics used for evaluation are
Jaccard loss based on intersection over union (IoU), the related Dice-
Coefficient, and node-wise accuracy. A comprehensive overview of
these metrics is provided by [38]. Similar to image segmentation
where the pixel-wise accuracy is determined, here the node-wise
distinction of true-positives, true-negatives, false-positives, and
false-negatives for the node prediction compared to the annotated
node ground truth is used.

We calculate the mean metrics for an average of 10 models that
were trained to a convergence of validation scores for each experi-
mental configuration. In the evaluation, we provide mean values
with standard deviation.

From the results for semantic segmentation (detailed in Table 3)
we can infer several observations. With mean values of more than
0.84 for Dice and IoU and a node accuracy of 0.81 in all experimental
validation sets, the ability to generalize for unknown programs is
high. The low standard deviation indicates a robust result for the
state of convergence of validation metrics.

For dataset Seg-Py1 the mean Dice-Coefficient is higher for the
GAT architecture by 3.3 percent than for GCN. Similarly, node ac-
curacy is better by 4.7 percent with only a decrease of 0.8 percent
for the IoU. This indicates an improvement in segmentation per-
formance for GAT compared to GCN. Similarly, for C programs
using dataset Seg-C, all metrics improved significantly with a 5
percent better performance for Dice-Coefficient and node accuracy
and an increase of 12 percent for IoU. From the comparative better
performance of GAT-layers for node-wise segmentation, we infer
that the use of self-attention mechanisms enables the network to
weigh more relevant node transitions and therefore achieve better
performance. Higher attention is assigned to edges with a high
likelihood of causing nontermination.

4.4 Comparison to recurrent algorithms
In Section 1, we discussed the suitability of GNNs for analyzing
programs given the inherent graph structure of programs. The
objective of the current section is to justify this statement by evalu-
ating whether GNNs are better at estimating program termination
than recurrent algorithms. Note that the latter are based on a se-
quential program representation.

For this purpose, we create an additional set of experiments
for the binary classification of program termination. To make the
comparison fair, the experimental configuration ensures that the
number of learnable parameters of all recurrent algorithms is higher
than the learnable parameters of the GNNs they were compared
against. We use the PyTorch implementations for RNNs [57], Long
short-term memory (LSTM) [37] and Gated recurrent unit’s (GRU)
[19].

Figure 8: Binary classification for termination in recur-
rent and graph neural networks. The graph neural network
based approaches perform significantly better than classical
recurrent approaches.

For this experiment, we only use the DS2 dataset containing
Python programs. Conversely to the previous experiments, for
the recurrent algorithms, the programs are not converted into an
AST but are directly encoded into a one-hot encoding based on a
dictionary of unique instructions.

For a significant evaluation, we perform a total of ten training

sessions until convergence of the test metrics.

4.4.1 Results. Similar to the previous classification experiments
we record the Precision-Recall (PR) and Receiver Operating Char-
acteristic (ROC) and their respective AUC.

We summarize our experimental results in Table 4. Notably,
graph-based methods outperform recurrent approaches by more
than 5 percent for the best recurrent ROC-AUC and PR-AUC met-
rics. Intuitively, we infer that the graph representation of programs
is a better fit for termination estimation than the sequential repre-
sentation. Furthermore, the understanding of the context within
the recurrent networks is limited by the model capacity and by the
natural constraints that don’t allow for large sequences of data to
be processed.

Additional insight into the comparison of the classification re-
sults can be gained by examining Figure 8, where it can be observed
that the GNN and GAT-based architectures perform better than the
recurrent approaches.

5 RELATED WORK
5.1 Termination analysis
Termination analysis has been studied for a long time. While the
majority of the approaches make use of symbolic methods such as
loop summarisation [16, 74], program synthesis [25, 27], quantifier
elimination [42], abstract interpretation [24], there were recent
attempts to employ machine learning techniques.

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Alon and David

Precision-Recall

ROC

Approach

Dataset

mAP AUC negative AUC positive mAP AUC neg. AUC pos.

GCN

GAT

DS-SV-COMP
DS-TERM-COMP
DS1
DS2

DS-SV-COMP
DS-TERM-COMP
DS1
DS2

0.83
0.87
0.92
0.87

0.9
0.91
0.93
0.90

0.94
0.92
0.94
0.91

0.96
0.93
0.91
0.94

0.76
0.84
0.90
0.88

0.86
0.89
0.74
0.88

0.87
0.90
0.93
0.91

0.93
0.92
0.93
0.92

0.88
0.89
0.92
0.91

0.93
0.91
0.94
0.93

0.88
0.89
0.92
0.91

0.93
0.91
0.93
0.93

Table 2: Experimental results for the binary classification of program termination using different network architectures and
datasets. ROC and Precision-Recall are recorded after a test accuracy convergence.

Dice-Coefficient

Jaccard Index (IoU) Node Accuracy

Approach Dataset

Language

value

GCN

GAT

Seg-Py1
Seg-C

Seg-Py1
Seg-C

Python
C

Python
C

0.843
0.896

0.876
0.947

𝜎

0.021
0.031

0.020
0.012

value

0.856
0.812

0.848
0.943

𝜎

0.029
0.029

0.024
0.024

value

0.810
0.892

0.857
0.944

𝜎

0.026
0.034

0.023
0.013

Table 3: Experimental results for the semantic segmentation of termination using different network architectures and datasets.

Approach

Network

ROC-AUC PR-AUC

recurrent

RNN [57]
LSTM [37]
GRU [19]

graph based GCN (custom)
GAT (custom)

0.78
0.88
0.88

0.94
0.96

0.77
0.87
0.88

0.94
0.96

Table 4: Experimental comparison of binary classification
using RNNs vs. GNNs.

For instance, Neural Termination Analysis [29], trains neural net-
works as ranking functions, i.e. monotone maps from the program’s
state space to well-ordered sets.

This method uses sampled executions to learn ranking functions,
which are then verified with an SMT solver. Several limitations
exist for this approach, e.g. loops with a limited number of itera-
tions do not provide enough data to learn a ranking function, the
verification of the ranking function may require additional loop
invariants to succeed. Moreover, this work has only been evaluated
on terminating programs with a maximum of two nested loops.
Calude and Dumitrescu proposed a probabilistic algorithm for the
Halting problem based on running times, where they define a class
of computable probability distributions on the set of halting pro-
grams [14]. In [4], Abate et al. present the first machine learning
approach to the termination analysis of probabilistic programs,
where they use a neural network to fit a ranking supermartingales
over execution traces and then verify it over the source code with
an SMT solver.

As opposed to these existing works, we do not attempt to pro-
vide strong guarantees about the termination decision. Instead, our
objective is to provide an estimation of a program’s termination
behavior, as well as localizing the likely cause of nontermination
(when applicable) that a programmer can use for debugging pur-
poses. For this purpose, we employ the attention mechanism to
identify those nodes relevant for the termination estimation. More-
over, for programs classified as nonterminating, we use semantic
segmentation to distinguish the outermost loop causing the infinite
execution.

5.2 Graph Neural Networks
The original approach to GNNs as presented by Kipf and Welling
[40] used the sum of normalized neighbor embeddings as aggrega-
tion in a self-loop. With a Multi-Layer-Perceptron as an aggregator,
Zaheer et al. [70] presented an approach that propagates states
through a trainable MLP. With the development of advanced at-
tention networks, the approach of Velickovic et al. [61] focused on
attention weights that allow to prioritize the influence of features
based on self-learned attention. For heterogeneous graphs with
additional edge features, Relational Graph Convolution Networks
were introduced by Schlichtkrull et. al. [56] to enable link prediction
and entity classification, allowing the recovery of missing entity
attributes for high-dimensional knowledge graphs.

6 CONCLUSIONS
We proposed a technique for estimating the termination behavior
of programs using a GNN. We also devised a GAT architecture that
uses a self-attention mechanism to allow the visualization of nodes
relevant for the termination decision. Finally, for nonterminating

Using Graph Neural Networks for Program Termination

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

programs, we constructed a GAT for the semantic segmentation of
those nodes likely responsible for nontermination.

Experimental evaluation of our algorithm confirms a superior
performance in termination estimation with a high ability to gen-
eralize for unknown programs.

REFERENCES
[1] [n. d.]. https://llvm.org/docs/LibFuzzer.html
[2] 2022. Competition on Software Verification. https://sv-comp.sosy-lab.org/2022/
[3] Alessandro Abate, Mirco Giacobbe, and Diptarko Roy. 2021. Learning Probabilistic

Termination Proofs. 3–26. https://doi.org/10.1007/978-3-030-81688-9_1

[4] Alessandro Abate, Mirco Giacobbe, and Diptarko Roy. 2021. Learning Probabilistic
Termination Proofs. In Computer Aided Verification - 33rd International Conference,
CAV 2021, Virtual Event, July 20-23, 2021, Proceedings, Part II (Lecture Notes in
Computer Science, Vol. 12760), Alexandra Silva and K. Rustan M. Leino (Eds.).
Springer, 3–26. https://doi.org/10.1007/978-3-030-81688-9_1

[5] Christophe Alias, Alain Darte, Paul Feautrier, and Laure Gonnord. 2010. Multi-
dimensional rankings, program termination, and complexity bounds of flowchart
programs. In International Static Analysis Symposium. Springer, 117–133.
[6] C. Alias, A. Darte, P. Feautrier, and L. Gonnord. 2013. Rank: A Tool to Check
Program Termination and Computational Complexity. In 2013 IEEE Sixth Inter-
national Conference on Software Testing, Verification and Validation Workshops.
238–238. https://doi.org/10.1109/ICSTW.2013.75

[7] Miltiadis Allamanis. [n. d.]. Graph Neural Networks on Program Analysis.

([n. d.]).

[8] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2017. Learn-
CoRR abs/1711.00740 (2017).

ing to Represent Programs with Graphs.
arXiv:1711.00740 http://arxiv.org/abs/1711.00740

[9] Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. 2021.
CoRR abs/2105.12787 (2021).

Self-Supervised Bug Detection and Repair.
arXiv:2105.12787 https://arxiv.org/abs/2105.12787

[10] Patrick Baudin, François Bobot, David Bühler, Loïc Correnson, Florent Kirchner,
Nikolai Kosmatov, André Maroneze, Valentin Perrelle, Virgile Prevosto, Julien
Signoles, and Nicky Williams. 2021. The dogged pursuit of bug-free C programs:
the Frama-C software analysis platform. Commun. ACM 64, 8 (2021), 56–68.
https://doi.org/10.1145/3470569

[11] Amir M. Ben-Amram and Samir Genaim. 2012. On the Linear Ranking Problem
for Integer Linear-Constraint Loops. CoRR abs/1208.4041 (2012). arXiv:1208.4041
http://arxiv.org/abs/1208.4041

[12] Dirk Beyer. 2020. Advances in Automatic Software Verification: SV-COMP 2020.
In Tools and Algorithms for the Construction and Analysis of Systems, Armin Biere
and David Parker (Eds.). Springer International Publishing, Cham, 347–367.
[13] David Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow. 2020. Learning
to execute programs with instruction pointer attention graph neural networks.
arXiv preprint arXiv:2010.12621 (2020).

[14] Cristian Calude and Monica Dumitrescu. 2017. A probabilistic anytime algorithm
for the halting problem. Computability 7 (05 2017), 1–13. https://doi.org/10.3233/
COM-170073

[15] Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Petr Novotný, and Dorde
Zikelic. 2021. Proving non-termination by program reversal. In PLDI ’21: 42nd
ACM SIGPLAN International Conference on Programming Language Design and
Implementation, Virtual Event, Canada, June 20-25, 20211, Stephen N. Freund and
Eran Yahav (Eds.). ACM, 1033–1048. https://doi.org/10.1145/3453483.3454093

[16] Hong-Yi Chen, Cristina David, Daniel Kroening, Peter Schrammel, and Björn
Wachter. 2018. Bit-Precise Procedure-Modular Termination Analysis. ACM Trans.
Program. Lang. Syst. 40, 1 (2018), 1:1–1:38. https://doi.org/10.1145/3121136
[17] Wei Chen, Tie-Yan Liu, Yanyan Lan, Zhiming Ma, and Hang Li. 2009. Ranking

Measures and Loss Functions in Learning to Rank. 315–323.

[18] Yu-Fang Chen, Matthias Heizmann, Ondřej Lengál, Yong Li, Ming-Hsien Tsai,
Andrea Turrini, and Lijun Zhang. 2018. Advanced automata-based algorithms
for program termination checking. In Proceedings of the 39th ACM SIGPLAN
Conference on Programming Language Design and Implementation. 135–150.
[19] Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.
CoRR abs/1412.3555 (2014). arXiv:1412.3555 http://arxiv.org/abs/1412.3555
[20] Michael A Colón and Henny B Sipma. 2002. Practical methods for proving
program termination. In International Conference on Computer Aided Verification.
Springer, 442–454.

[21] Byron Cook. 2009. Priciples of program termination. Engineering Methods and

Tools for Software Safety and Security 22, 161 (2009), 125.

[22] Byron Cook, Andreas Podelski, and Andrey Rybalchenko. 2006. Termination
proofs for systems code. In Proceedings of the ACM SIGPLAN 2006 Conference on
Programming Language Design and Implementation, Ottawa, Ontario, Canada,
June 11-14, 2006, Michael I. Schwartzbach and Thomas Ball (Eds.). ACM, 415–426.
https://doi.org/10.1145/1133981.1134029

[23] Byron Cook, Abigail See, and Florian Zuleger. 2013. Ramsey vs. Lexicographic
Termination Proving. In Tools and Algorithms for the Construction and Analysis of
Systems - 19th International Conference, TACAS 2013, Held as Part of the European
Joint Conferences on Theory and Practice of Software, ETAPS 2013, Rome, Italy,
March 16-24, 2013. Proceedings (Lecture Notes in Computer Science, Vol. 7795), Nir
Piterman and Scott A. Smolka (Eds.). Springer, 47–61. https://doi.org/10.1007/978-
3-642-36742-7_4

[24] Patrick Cousot and Radhia Cousot. 2012. An abstract interpretation framework
for termination. In Proceedings of the 39th ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, POPL 2012, Philadelphia, Pennsylvania,
USA, January 22-28, 2012, John Field and Michael Hicks (Eds.). ACM, 245–258.
https://doi.org/10.1145/2103656.2103687

[25] Cristina David, Daniel Kroening, and Matt Lewis. 2015. Unrestricted Termi-
nation and Non-termination Arguments for Bit-Vector Programs. In Program-
ming Languages and Systems - 24th European Symposium on Programming,
ESOP 2015, Held as Part of the European Joint Conferences on Theory and Prac-
tice of Software, ETAPS 2015, London, UK, April 11-18, 2015. Proceedings (Lec-
ture Notes in Computer Science, Vol. 9032), Jan Vitek (Ed.). Springer, 183–204.
https://doi.org/10.1007/978-3-662-46669-8_8

[26] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and
Xavier Bresson. 2020. Benchmarking graph neural networks. arXiv preprint
arXiv:2003.00982 (2020).

[27] Grigory Fedyukovich, Yueling Zhang, and Aarti Gupta. 2018. Syntax-Guided
Termination Analysis. In Computer Aided Verification - 30th International Con-
ference, CAV 2018, Held as Part of the Federated Logic Conference, FloC 2018, Ox-
ford, UK, July 14-17, 2018, Proceedings, Part I (Lecture Notes in Computer Science,
Vol. 10981), Hana Chockler and Georg Weissenbacher (Eds.). Springer, 124–143.
https://doi.org/10.1007/978-3-319-96145-3_7

[28] Mirco Giacobbe, Mohammadhosein Hasanbeig, Daniel Kroening, and Hjalmar
Wijk. 2021. Shielding Atari Games with Bounded Prescience. CoRR abs/2101.08153
(2021). arXiv:2101.08153 https://arxiv.org/abs/2101.08153

[29] Mirco Giacobbe, Daniel Kroening, and Julian Parsert. 2021. Neural Termination

Analysis. arXiv:2102.03824 [cs.LG]

[30] Jürgen Giesl, Cornelius Aschermann, Marc Brockschmidt, Fabian Emmes, Florian
Frohn, Carsten Fuhs, Jera Hensel, Carsten Otto, Martin Plücker, Peter Schneider-
Kamp, et al. 2017. Analyzing program termination and complexity automatically
with AProVE. Journal of Automated Reasoning 58, 1 (2017), 3–31.

[31] Jürgen Giesl, Fred Mesnard, Albert Rubio, René Thiemann, and Johannes Wald-
mann. 2015. Termination Competition (termCOMP 2015). https://doi.org/10.
1007/978-3-319-21401-6_6

[32] Martin Grohe. 2021. The Logic of Graph Neural Networks. CoRR abs/2104.14624

(2021). arXiv:2104.14624 https://arxiv.org/abs/2104.14624

[33] Ashutosh Gupta, Thomas A. Henzinger, Rupak Majumdar, Andrey Rybalchenko,
and Ru-Gang Xu. 2008. Proving non-termination. In Proceedings of the 35th ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL
2008, San Francisco, California, USA, January 7-12, 2008, George C. Necula and
Philip Wadler (Eds.). ACM, 147–158. https://doi.org/10.1145/1328438.1328459

[34] Takuo Hamaguchi, Hidekazu Oiwa, Masashi Shimbo, and Yuji Matsumoto. 2017.
Knowledge transfer for out-of-knowledge-base entities: A graph neural network
approach. arXiv preprint arXiv:1706.05674 (2017).

[35] Matthias Heizmann, Jochen Hoenicke, and Andreas Podelski. 2013. Software
Model Checking for People Who Love Automata. In Computer Aided Verification -
25th International Conference, CAV 2013, Saint Petersburg, Russia, July 13-19, 2013.
Proceedings (Lecture Notes in Computer Science, Vol. 8044), Natasha Sharygina and
Helmut Veith (Eds.). Springer, 36–52. https://doi.org/10.1007/978-3-642-39799-
8_2

[36] Matthias Heizmann, Jochen Hoenicke, and Andreas Podelski. 2014. Termination
Analysis by Learning Terminating Programs. https://doi.org/10.1007/978-3-319-
08867-9_53

[37] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-term Memory. Neural

computation 9 (12 1997), 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735

[38] Mohammad Hossin and Sulaiman M.N. 2015. A Review on Evaluation Metrics for
Data Classification Evaluations. International Journal of Data Mining & Knowledge
Management Process 5 (03 2015), 01–11. https://doi.org/10.5121/ijdkp.2015.5201
[39] Shruti Jadon. 2020. A survey of loss functions for semantic segmentation. 2020
IEEE Conference on Computational Intelligence in Bioinformatics and Compu-
tational Biology (CIBCB) (Oct 2020). https://doi.org/10.1109/cibcb48159.2020.
9277638

[40] Thomas N. Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. CoRR abs/1609.02907 (2016). arXiv:1609.02907
http://arxiv.org/abs/1609.02907

[41] Tien-Duy B. Le, David Lo, Claire Le Goues, and Lars Grunske. 2016. A learning-
to-rank based fault localization approach using likely invariants. In Proceedings
of the 25th International Symposium on Software Testing and Analysis, ISSTA 2016,
Saarbrücken, Germany, July 18-20, 2016, Andreas Zeller and Abhik Roychoudhury
(Eds.). ACM, 177–188. https://doi.org/10.1145/2931037.2931049

[42] Ton Chanh Le, Lei Xu, Lin Chen, and Weidong Shi. 2018. Proving Conditional
Termination for Smart Contracts. In Proceedings of the 2nd ACM Workshop on

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Alon and David

[66] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
Philip S. Yu. 2019. A Comprehensive Survey on Graph Neural Networks. CoRR
abs/1901.00596 (2019). arXiv:1901.00596 http://arxiv.org/abs/1901.00596
[67] Hongwei Xi. 2002. Dependent types for program termination verification. Higher-

Order and Symbolic Computation 15, 1 (2002), 91–131.

[68] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. 2015. Empirical Evaluation of
Rectified Activations in Convolutional Network. CoRR abs/1505.00853 (2015).
arXiv:1505.00853 http://arxiv.org/abs/1505.00853

[69] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful

are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).

[70] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabás Póczos, Ruslan
Salakhutdinov, and Alexander J. Smola. 2017. Deep Sets. CoRR abs/1703.06114
(2017). arXiv:1703.06114 http://arxiv.org/abs/1703.06114

[71] Yingxue Zhang and Michael G. Rabbat. 2018. A Graph-CNN for 3D Point Cloud
Classification. CoRR abs/1812.01711 (2018). arXiv:1812.01711 http://arxiv.org/
abs/1812.01711

[72] Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2018. Deep Learning on Graphs: A
Survey. CoRR abs/1812.04202 (2018). arXiv:1812.04202 http://arxiv.org/abs/1812.
04202

[73] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:
A review of methods and applications. AI Open 1 (2020), 57–81. https://doi.org/
10.1016/j.aiopen.2021.01.001

[74] Shaowei Zhu and Zachary Kincaid. 2021. Termination analysis without the
tears. In PLDI ’21: 42nd ACM SIGPLAN International Conference on Programming
Language Design and Implementation, Virtual Event, Canada, June 20-25, 2021,
Stephen N. Freund and Eran Yahav (Eds.). ACM, 1296–1311. https://doi.org/10.
1145/3453483.3454110

Blockchains, Cryptocurrencies, and Contracts, BCC@AsiaCCS 2018, Incheon, Re-
public of Korea, June 4, 2018, Satya V. Lokam, Sushmita Ruj, and Kouichi Sakurai
(Eds.). ACM, 57–59. https://doi.org/10.1145/3205230.3205239

[43] Chin Soon Lee, Neil D Jones, and Amir M Ben-Amram. 2001. The size-change
principle for program termination. In Proceedings of the 28th ACM SIGPLAN-
SIGACT symposium on Principles of programming languages. 81–92.

[44] Guohao Li, Matthias Müller, Guocheng Qian, Itzel C. Delgadillo, Abdulellah
Abualshour, Ali K. Thabet, and Bernard Ghanem. 2019. DeepGCNs: Making
GCNs Go as Deep as CNNs. CoRR abs/1910.06849 (2019). arXiv:1910.06849
http://arxiv.org/abs/1910.06849

[45] Jun Li, Bodong Zhao, and Chao Zhang. 2018. Fuzzing: a survey. Cybersecurity 1

(12 2018). https://doi.org/10.1186/s42400-018-0002-y

[46] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017.
Focal loss for dense object detection. In Proceedings of the IEEE international
conference on computer vision. 2980–2988.

[47] Meng Liu, Hongyang Gao, and Shuiwang Ji. 2020. Towards deeper graph neural
networks. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. 338–348.

[48] Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, and Arnt-Børre Salberg. 2020.
SCG-Net: Self-Constructing Graph Neural Networks for Semantic Segmentation.
CoRR abs/2009.01599 (2020). arXiv:2009.01599 https://arxiv.org/abs/2009.01599
[49] Valentin J. M. Manès, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel
Egele, Edward J. Schwartz, and Maverick Woo. 2018. Fuzzing: Art, Science, and
Engineering. CoRR abs/1812.00140 (2018). arXiv:1812.00140 http://arxiv.org/abs/
1812.00140

[50] Aravind Nair, Avijit Roy, and Karl Meinke. 2020.

funcGNN: A Graph Neu-
ral Network Approach to Program Similarity. CoRR abs/2007.13239 (2020).
arXiv:2007.13239 https://arxiv.org/abs/2007.13239

[51] Ulrich Neumerkel and Frédéric Mesnard. 1999. Localizing and Explaining
Reasons for Non-terminating Logic Programs with Failure-Slices. In Princi-
ples and Practice of Declarative Programming, International Conference PPDP’99,
Paris, France, September 29 - October 1, 1999, Proceedings (Lecture Notes in Com-
puter Science, Vol. 1702), Gopalan Nadathur (Ed.). Springer, 328–342.
https:
//doi.org/10.1007/10704567_20

[52] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic differentiation in PyTorch. In NIPS-W.

[53] Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel,
Martin Riedmiller, Raia Hadsell, and Peter Battaglia. 2018. Graph networks as
learnable physics engines for inference and control. In International Conference
on Machine Learning. PMLR, 4470–4479.

[54] N. Sasirekha, A. Edwin Robert, and M. Hemalatha. 2011. Program slicing
techniques and its applications. CoRR abs/1108.1352 (2011). arXiv:1108.1352
http://arxiv.org/abs/1108.1352

[55] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2008. Computational capabilities of graph neural networks. IEEE
Transactions on Neural Networks 20, 1 (2008), 81–102.

[56] M. Schlichtkrull, Thomas Kipf, P. Bloem, Rianne van den Berg, Ivan Titov, and M.
Welling. 2018. Modeling Relational Data with Graph Convolutional Networks.
In ESWC.

[57] Alex Sherstinsky. 2018. Fundamentals of Recurrent Neural Network (RNN)
and Long Short-Term Memory (LSTM) Network. CoRR abs/1808.03314 (2018).
arXiv:1808.03314 http://arxiv.org/abs/1808.03314

[58] Weijing Shi and Ragunathan Rajkumar. 2020. Point-GNN: Graph Neural Net-
work for 3D Object Detection in a Point Cloud. CoRR abs/2003.01251 (2020).
arXiv:2003.01251 https://arxiv.org/abs/2003.01251

[59] X. Si, H. Dai, Mukund Raghothaman, M. Naik, and Le Song. 2018. Learning Loop

Invariants for Program Verification. In NeurIPS.

[60] Alan Turing. 1936. On Computable Numbers, with an Application to the N
Tscheidungsproblem. Proceedings of the London Mathematical Society 42, 1 (1936),
230–265. https://doi.org/10.2307/2268810

[61] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Graph Attention Networks.

Pietro Liò, and Yoshua Bengio. 2018.
arXiv:1710.10903 [stat.ML]

[62] Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. 2020. LambdaNet: Prob-
abilistic Type Inference using Graph Neural Networks. CoRR abs/2005.02161
(2020). arXiv:2005.02161 https://arxiv.org/abs/2005.02161

[63] Mark Weiser. 1982. Programmers Use Slices When Debugging. Commun. ACM

25, 7 (jul 1982), 446–452. https://doi.org/10.1145/358557.358577

[64] Mark Weiser. 1984. Program Slicing. IEEE Transactions on Software Engineering

SE-10, 4 (1984), 352–357. https://doi.org/10.1109/TSE.1984.5010248

[65] Yongji Wu, Defu Lian, Yiheng Xu, Le Wu, and Enhong Chen. 2020. Graph Con-
volutional Networks with Markov Random Field Reasoning for Social Spammer
Detection. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI
2020, The Thirty-Second Innovative Applications of Artificial Intelligence Confer-
ence, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press,
1054–1061. https://aaai.org/ojs/index.php/AAAI/article/view/5455

