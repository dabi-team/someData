2
2
0
2

r
a

M
7
2

]
E
M

.
t
a
t
s
[

1
v
4
0
3
4
1
.
3
0
2
2
:
v
i
X
r
a

An Extended Simpliﬁed Laplace strategy for Approximate Bayesian
inference of Latent Gaussian Models using R-INLA

Cristian Chiuchiolo (cristian.chiuchiolo@kaust.edu.sa )
AND
Janet van Niekerk (janet.vanniekerk@kaust.edu.sa )
AND
H˚avard Rue (haavard.rue@kaust.edu.sa )
CEMSE Division
King Abdullah University of Science and Technology
Kingdom of Saudi Arabia

March 29, 2022

Abstract

Various computational challenges arise when applying Bayesian inference approaches to complex hi-
erarchical models. Sampling-based inference methods, such as Markov Chain Monte Carlo strategies,
are renowned for providing accurate results but with high computational costs and slow or questionable
convergence. On the contrary, approximate methods like the Integrated Nested Laplace Approximation
(INLA) construct a deterministic approximation to the univariate posteriors through nested Laplace Ap-
proximations. This method enables fast inference performance in Latent Gaussian Models, which encode
a large class of hierarchical models. R-INLA software mainly consists of three strategies to compute all
the required posterior approximations depending on the accuracy requirements. The Simpliﬁed Laplace
approximation (SLA) is the most attractive because of its speed performance since it is based on a
Taylor expansion up to order three of a full Laplace Approximation. Here we enhance the methodology
by simplifying the computations necessary for the skewness and modal conﬁguration. Then we propose
an expansion up to order four and use the Extended Skew Normal distribution as a new parametric ﬁt.
The resulting approximations to the marginal posterior densities are more accurate than those calculated
with the SLA, with essentially no additional cost.

1 Introduction

Hierarchical models appear to be challenging within a Bayesian inference framework due to their highly
correlated structure and/or high dimensionality. Sampling-based methods such as Markov Chain Monte
Carlo (MCMC) may require non-negligible computational demand when applied to these models. Ap-
proximate methods such as Laplace approximations aim to circumvent the high computational demand of
sampling-based methods by approximating marginal posterior distributions [22, 24, 23]. A simple Gaussian
approximation (Laplace Method) for the unknown joint density parameters can be quite crude and restricts
the marginal posterior densities to be symmetric. Integrated Nested Laplace Approximations (INLA) as in-
troduced by [20] are based on a series of Laplace approximations for the regression parameters, for example,
resulting in an unknown parametric form of the marginal posterior density functions. Nonetheless, INLA
performs full Bayesian inference in a fraction of the time of sampling-based methods.

A Simpliﬁed Laplace strategy (SLA) was proposed as a simpler alternative to the original INLA, in the sense
that Skew Normal densities approximate the marginal posterior density functions [1, 2, 6, 7, 26, 17, 30].
This approach implies a skew marginal posterior density, which improves the Gaussian marginals from the
Laplace method. The Skew Normal approximations capture the true marginal posterior densities quite ac-
curately, while the computational cost for this approach is much less than the full INLA. This Skew Normal

1

 
 
 
 
 
 
family-based approach embodies the optimal strategy to approximate skewed marginal and joint posterior
densities [9].

We propose a new approximation for the marginals of unknown latent parameters of a latent Gaussian
model based on the INLA framework. We use an extended Skew Normal approximation to the marginal
posterior densities using the Extended Skew Normal distribution [3], with the hope of capturing the skew-
ness and kurtosis more accurately, than the Skew Normal approximations. We term this approximation an
Extended Simpliﬁed Laplace Approximation (ESLA), as a direct extension of the Simpliﬁed Laplace strategy
employed by the R-INLA R package. This extension is a ﬁrst attempt to move beyond the Skew Normal
density approximation, while maintaining computational eﬃciency.

In Section 2 we discuss the Latent Gaussian Model formulation by emphasizing the role of Gaussian as-
sumptions onto the latent ﬁeld, which appear to be a natural choice when a deterministic approach such as
INLA is used. Section 3 introduces one possible extension of the Skew Normal distribution, the Extended
Skew Normal, as a candidate for approximating the marginals when marginal skewness is non-negligible.
Section 4 presents a transparent and wieldy way to localize the mode when ﬁtting Skew Normal distribu-
tions, whereafter we present the details of the extended Skew Normal approximation. Section 5 shows some
general skewed examples where the new strategy is applied and compared to the other approaches. Section
6 contains a brief discussion on the proposed methodology and its performance as well as possibilities for
further extensions in this regard.

2 Latent Gaussian models (LGM) and Integrated Nested Laplace

Approximations (INLA)

2.1 LGM

Latent Gaussian Models (LGMs) are appealing in Bayesian computational inference when using INLA for
two main intrinsic assumptions: the log-likelihood contribution is log concave in terms of its linear predictor
and the latent ﬁeld is Gaussian distributed, a priori. These assumptions ensure the posterior distribution of
the model to be Gaussian-like and therefore easily handled by INLA. Log concavity on the log-likelihood is
a strong beneﬁcial assumption as it enforces the distribution on the observed data to be close to a Gaussian
distribution when conditional independence to each latent term is assumed.
Such assumptions make clear that it is less fruitful to assume a statistical structure that goes far beyond
a Gaussian distribution when dealing with Latent Gaussian Models. As an example, we consider a simple
latent structure with no hyperparameters. By model assumptions we have the latent ﬁeld x ∼ N(0, Q) with
a precision matrix Q having marginal variances equal to 1, and likelihood contribution y|x ∼ (cid:81)
i π(yi|xi)
with n data observations.
We assume |π(yi|xi)| < ˜Ci where each likelihood density is a function of xi bounded by a constant ˜Ci which
is unique for each observation. The posterior distribution of the corresponding latent model is

(1)
˜Ci. Since this Gaussian bound exists for the latent density, we can question if a similar bound

π(x|y) ∝ π(x)π(y|x) ≤ π(x) ˜C

where ˜C = (cid:81)
i
is preserved for each latent marginal. We will show that

π(x|y) ≤ ˜Cπ(x) ⇒ π(xi|y) ≤ ˜Cπ(xi)

(2)

where π(xi) is the respective ith Gaussian marginal density from its multivariate counterpart π(x).
The above statement provides a legitimate justiﬁcation to using Gaussian assumptions onto the latent ﬁeld
of a Latent Gaussian Model structure. This marginal implication can be shown in few steps.
We deﬁne functions gi(xi) = log(π(yi|xi)) and write the latent joint conditional density as

π(x|y) = G exp

(cid:16)

−

1
2

xT Qx +

(cid:17)

gi(xi)

n
(cid:88)

i=1

(3)

2

where G is the normalization constant. Each posterior latent marginal xi is obtained by integrating out all
the other latent components x−i

π(xi|y) =

(cid:90)

x−i

π(x|y) dx−i

= exp(gi(xi))

(cid:90)

x−i

G exp

(cid:16)

−

1
2

xT Qx

(cid:17)

exp

(cid:16)(cid:88)

(cid:17)

gj(xj)

dx−i

j(cid:54)=i

Since each gi(xi) is bounded per our initial assumptions, then

π(xi|y) ≤ ˜Ci

(cid:90)

x−i

(cid:16)

G exp

−

1
2

xT Qx

(cid:17) (cid:89)

j(cid:54)=i

˜Cj dx−i

≤ ˜C exp

(cid:16)

−

1
2

i (Q−1)−1
x2
ii

(cid:17)

(4)

(5)

which corresponds to (2). The notation (Q−1)ii refers to the ith marginal variance term Σii derived from the
covariance matrix Σ = Q−1. The result (5) shows that the Gaussian distribution represents a natural bound
for each marginal up to a constant. This emphasizes that distributions with a Gaussian-like behavior are
the most natural choice for approximating posterior marginals. Their tails must follow a Gaussian behavior
while the main bulk of the distribution can show diﬀerences from a Gaussian density because of location and
skewness.
The Gaussian and Simpliﬁed Laplace strategies represent an appropriate embodiment of this Gaussian
feature since their application provides accurate marginal posterior approximations in most of the cases
by exploiting Gaussian-like distributions. In Section 3 we show that Skew Normal family distributions are
natural candidates as their tail behavior approximately resembles the one from a Gaussian distribution.
Appendix A also discusses if this argument still holds when considering more heavy-tailed assumptions such
as the Student-t distribution.

2.2 INLA

The Integrated Nested Laplace Approximation (INLA) methodology consists of computing discrete approx-
imations to univariate posteriors of the unknown parameters of a Latent Gaussian Model (LGM). Amongst
others, the Stochastic Partial Diﬀerential Equation (SPDE) approach employed by the INLA methodology
in the geostatistics ﬁeld [11] has heavily impacted the applied sciences community. New insights and exten-
sions about the interpolation algorithms applied to the hyperparameter posterior marginals to improve speed
while retaining accuracy is presented by [14, 27]. Enhanced model features [13] and GLMMs corrections [9],
a measurement error model [15], introduction of a new prior methodology [28], criticisms and Bayesian model
diagnostics [8], a book about spatial and spatiotemporal models [5] with more advanced examples in [10], are
all contributions to the INLA methodology and applications. Three main reviews about new advancements
can be read at [21, 4, 12]. Recent new applications on joint models using the PARDISO library ([25]) was
proposed by [29, 16].
Clearly, the INLA methodology provides a new path for Bayeisan inference that is eﬃcient, accurate and
can be applied to many statistical applications. Here we brieﬂy explain the INLA methodology.

Assuming n-dimensional data y with likelihood (cid:81)
i π(yi|xi, θ), an unobserved latent ﬁeld vector x with prior
π(x|θ), and a hyperparameter set θ with prior π(θ), the unknown parameters (latent and hyperparameters)
of a Latent Gaussian Model has joint posterior density

π(x, θ|y) ∝ π(θ)π(x|θ)

π(yi|xi, θ)

(cid:89)

i

(6)

whose implicit hierarchical structure is summarised into

3

y|x, θ ∼

n
(cid:89)

i=1

π(yi|xi, θ)

x|θ ∼ N (0, Q−1(θ))

θ ∼ π(θ)

(7)

The likelihood contribution to the model entirely comes from each π(yi|xi, θ) where each observation yi only
correspond to one single latent term xi. Each observation yi has a corresponding linear predictor term ηi
additive for all unknown model parameters: ﬁxed coeﬃcients or random terms related to cluster eﬀects,
non-linear functions, temporal or spatial speciﬁcation.
Both these parameters and the linear predictor vector η belong to the latent ﬁeld x which is assumed to be
Gaussian distributed with a sparse precision matrix Q. This latent Gaussian assumption is well speciﬁed for
both small and large dimensions by the concept of Gaussian Markov Random Fields (GMRFs, [18]) and is
fundamental for Latent Gaussian Models.
GMRFs allow modeling the dependency structure of the latent components of the model simultaneously,
providing the ground for fast computations due to the precision sparsity structure. By encoding the linear
predictor into the latent ﬁeld, INLA can compute all the possible posteriors of the model without much
computational eﬀort in most cases.
The hyperparameter set θ contains all the hyperparameters of the Latent Gaussian Model, and its dimen-
sion can lead to more costly computations if the dimension of θ is too high. We can deal with most of the
cases routinely when |θ| < 20. The hyperpriors π(θ) are not bounded to be Gaussian, and many diﬀerent
distributions can be used.

INLA is a deterministic algorithm that computes accurate approximations for the univariate posterior
marginals of the unknown parameters of a Latent Gaussian Model.
From the joint posterior density in (6) we derive its marginal densities as follows

π(xi|y) =

(cid:90)

θ

π(xi|θ, y)π(θ|y) dθ,

i = 1, . . . , N

π(θj|y) =

(cid:90)

θ−j

π(θ|y) dθ−j,

j = 1, . . . , p

(8)

(9)

with N being the overall dimension of the latent ﬁeld x and p being the dimension of the hyperparameter
θ. The approximations of the marginals in (9) result from numerically integrating out the hyperparameter
uncertainty θ to get

˜π(xi|y) ≈

K
(cid:88)

k=1

˜π(xi|y, θk)˜π(θk|y)∆k

˜π(θ|y) ∝

π(x∗, θ|y)
˜πG(x∗|θ, y)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)x∗=µ(θ)

(10)

with K being the total number of points used in the numerical integration process (see [20, 12]). ˜πG(x∗|θ, y)
is the Gaussian approximation obtained by matching the mode and curvature at the mode of the full joint
density π(x|θ, y) found after an iterative process.
The whole methodology can be summarised as follows

• Explore the approximation log ˜π(θ|y) through a grid exploration scheme in a p-dimensional space. Lo-
cate the mode and compute a set of conﬁguration points θk, k = 1 . . . , K within the region oh high
probability mass

• Evaluate log ˜π(θ1|y), . . . , log ˜π(θK|y) and use these results to compute both ˜π(xi|y, θk) and ˜π(xi|y)

4

INLA computes ﬁrst the hyperparameter posterior marginals in (10) by using Laplace Approximations on
the entire ratio and then evaluates it at the denominator mean µ(θ). A second Laplace Approximation is
then applied to the full conditional posterior densities π(xi|y, θk) by using the pre-computed points θk as
follows

˜π(xi|y, θk) ≈

π(x∗, θk|y)

˜πG(x∗

−i|xi, θk, y)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)x∗

−i=µ−i(θ)

(11)

−i|xi, θk, y) being the Gaussian Approximation with modal conﬁguration µ−i(θ).

with ˜πG(x∗
Depending on the nature of these densities, there are three approximation strategies that can be applied
to (11) (in order of increasing accuracy and computational cost): Gaussian Approximation, Simpliﬁed Laplace
Approximation and Laplace Approximation.
The Gaussian approximation is preferable when the Gaussian assumptions hold for the model likelihood and
has the best speed performance. However, this strategy may have inaccuracies in location and skewness
adjustments when the likelihood contribution deviates signiﬁcantly from a Gaussian behavior (as also stated
in [19]).
On the other side, the Laplace approximation is more computationally intensive but ensures more accuracy
since it exploits a more on-point Gaussian approximation for each latent term at the denominator in (11).
Then the ratio is simpliﬁed through a series of selected points based on selected criteria that save compu-
tations (see [20] for details), and the marginal approximated density result for each latent term is given
by the product of a Gaussian kernel and a cubic spline. The spline itself interpolates selected points of the
marginal latent variable to the log density diﬀerence between the resulting Laplace approximation and respec-
tive Gaussian approximation. Another strategy available is the Simpliﬁed Laplace approach, which applies
a third-order Taylor expansion of the Laplace approximation, therefore providing a more computational-
friendly result at some negligible accuracy in most cases. This strategy is the INLA default choice and
exploits Skew Normal densities to get the full conditional approximations π(xi|y, θk) in (11). Most of the
present work goes through the details of the Simpliﬁed Laplace approximations while proposing a way to
extend its capabilities through another distribution of the Skew Normal class.

3 The Extended Skew Normal Distribution and its properties

INLA uses the Skew Normal family when the Gaussian assumptions are not accurate enough. These skewed
distributions tend to be good approximations of the marginal posteriors of a Latent Gaussian Model.
Observed skewness is retrieved through the third moment of a Skew Normal distribution. In more extreme
settings, the marginal skewness from the full conditional densities π(xi|θ, y) can beneﬁt from a more complex
structure with additional free parameters.
The Extended Skew Normal distribution (see [1] for other alternative distributions and more insights on
the Skew Normal family) belongs to the Skew Normal family and can model skewness using two parameters
instead of one.
First, we introduce some basic deﬁnitions and properties of this extended version of the Skew Normal family.
We deﬁne T ∼ ESN(ξ, ω, α, τ ) to be an Extended Skew Normal random variable whose probability density
function is

f (t; ξ, ω, α, τ ) =

1
ωΦ(τ )

φ

(cid:16) t − ξ
ω

(cid:17)

(cid:16)

τ

Φ

(cid:112)

α2 + 1 + α

(cid:17)

t − ξ
ω

(12)

with location parameter ξ, scale ω, skewness parameter α and hidden mean parameter τ (or truncation
parameter as mentioned in [6, 3]) while φ(·), Φ(·) are respectively the probability and cumulative density
function of a standard Gaussian.
For τ = 0 the equation in (12) reduces to a Skew Normal distribution with the same parameter notation.
The cumulant generating function of T is given by

K(u) = log M (u) = ξu +

1
2

ω2u2 + C0(τ + δωu) − C0(τ )

(13)

5

where M (u) = E[euT ] is the moment generating function with parameterization δ =
log 2Φ(z). From K(u) we get the ﬁrst four moments

α√
1+α2 and C0(z) =

E(T ) = ξ + C1(τ )ωδ
Var(T ) = ω2[1 + C2(τ )δ2]

γ1(T ) =

γ2(T ) =

C3(τ )δ3
(1 + C2(τ )δ2)3/2
C4(τ )δ4
(1 + C2(τ )δ2)2

(14)

with γ1, γ2 being the standardized skewness and kurtosis. The C(·) functions are deﬁned with respect to τ
by [3] as

with the ﬁrst ﬁve derivatives being

Cr(τ ) =

∂r
∂τ r log 2Φ(τ )

C1(τ ) =

φ(τ )
Φ(τ )

C2(τ ) = −[C1(τ )]2 − τ C1(τ )
C3(τ ) = −τ C2(τ ) − 2C1(τ )C2(τ ) − C1(τ )
C4(τ ) = −τ C3(τ ) − 2C2(τ ) − 2C2
C5(τ ) = −3C3(τ ) − τ C4(τ ) − 6C2(τ )C3(τ ) − 2C1(τ )C4(τ )

2 (τ ) − 2C1(τ )C3(τ )

(15)

(16)

Using (16), we can retrieve the constants associated to the moments of a Skew Normal random variable
(cid:113) 2
π

when τ = 0, since C1(0) =
The behavior of these C functions is shown in Figure 1 where we observe the following:

and C4(0) = − 24

π , C2(0) = − 2

π , C3(0) =

π2 + 8
π .

(4−π)
π

(cid:113) 2

• C1(τ ) has a linear behavior for negative values and quickly decays to zero as τ approaches zero towards

the positive range side

• C2(τ ) assumes values in the range (−1, 0) and follows a logistic like shape

• C3(τ ) assumes values in the range (0, 0.3) and resembles a probability density function

• C4(τ ) assumes values in the range (−0.2, 0.1) and quickly decays to zero as τ < −1 and τ > 4

In particular, the function C3 approximately satisﬁes all the required properties of a probability density
function, the range is positive and the respective integral is close to one.
Numerical integration shows that the integral is 0.9991876 with absolute error less than 8.1e-05 for values
of τ within the range [-35, 35]. This is helpful to simplify an implementation of the Extended Skew Normal
distribution as there is no additional gain in considering large values of τ . We return to this issue in Section
4.
Both the additional hidden mean parameter and the C function patterns make the Extended Skew Nor-
mal distribution appealing for better modeling skewed posterior behaviors when properly encoded in the
Simpliﬁed Laplace strategy.
A closed expression for the parameterization δ is obtained from(14) as follows

(cid:115)

δ = sign(γ1)

|γ1|2/3
[C3(τ )]2/3 − C2(τ )|γ1|2/3

(17)

Similar to existing solutions for the Skew Normal distribution, we may use the moments to construct a
proper mapping for the Extended Skew Normal. We substitute equation (17) into the kurtosis one in (14)
and achieve a solution for τ . Then we see the following:

6

Figure 1: Plotting C functions of an Extended Skew Normal distribution up to order four with respect to
the τ parameter with range values [−10, 10].

• the fourth equation with respect to τ does not have a closed form solution

• the kurtosis is unbounded as its range is [0, ∞) and this can lead to numerical issues or unreasonable

outcomes

As we do not control kurtosis results within a ﬁnite range, a mapping between parameters and moments of
the Extended Skew Normal density moments is not feasible.
In Section 4 we show that it is way easier and more eﬃcient to follow a similar scheme adopted for the
Simpliﬁed Laplace strategy where we ﬁt Skew Normal distributions by matching higher-order derivatives
evaluated at the mode of the target distribution. The Extended Skew Normal distribution is no exception to
this methodology since we only need an additional higher-order derivative to get solutions for the parameter
τ . This extended Skew Normal version can also be used to model skewness within the Latent Gaussian
Model paradigm as it satisﬁes the Gaussian pattern discussed in Section 2.

3.1 Tail behaviour of the Skew Normal family

Gaussian-like assumptions lead to accurate approximations of the posterior marginals of a Latent Gaussian
Model in INLA. Apart from the Gaussian distribution, the Skew Normal family appears to be another
natural choice for modeling these marginals. The Simpliﬁed Laplace strategy is built upon Skew Normal
distributions, granting fast and accurate results when Gaussian assumptions are too limiting. Although
the bulk of the distribution around the mode diﬀers from a Gaussian due to its asymmetrical nature, we
demonstrate here that both the Skew Normal distribution and its extended version satisfy the Gaussian-like
posterior marginal representation discussed in Section 2. Consider the log densities of a standard Skew
Normal and Extended Skew Normal distribution

7

log fSN(x; α) = log(2) + log(φ(x)) + log(Φ(αx))
1
2

= log(2) + log(φ(x)) + log

(cid:16) 1
2

+

erf

(cid:17)(cid:17)

(cid:16) αx
√
2

log fESN(x; α, τ ) = − log(Φ(τ )) + log(φ(x)) + log(Φ(αx + τ

(cid:112)

= − log(Φ(τ )) + log(φ(x)) + log

(cid:16) 1
2

+

1
2

erf

1 + α2
2

(cid:17)(cid:17)

(18)

1 + α2))
√
√

(cid:16) αx + τ

(cid:82) x
with erf(x) = 2√
0 exp(−z2) dz being the error function. We see that Gaussian distributions bound both
π
densities since Φ(p) ≤ 1. However, tail behavior is another important aspect of a distribution as it provides
information of extreme observations. Considering Skew Normal family distributions as natural candidates
for our deterministic marginal approximations, we need to ensure that even their tails follow a Gaussian
behavior. This can be accomplished by computing series expansions of both log densities in (18) for the
limiting cases x → ±∞. An asymptotic expansion of the log Gaussian density φ(x) is straightforward
and consists of one squared term. Skew Normal family densities add more complexity because of the Φ(·)
function term. Asymptotic expansion results for both Skew Normal family tails are provided below, where
ν = τ

1 + α2. The results for the right tail are

√

log fSN(x; α)|x→+∞ ≈ −

log fESN(x; α, τ )|x→+∞ ≈ −

1
2

1
2

while for the left tail we have

x2 + exp

(cid:16)

−

x2 + exp(−

α2x2
2
α2x2
2

(cid:17)(cid:16)

−

1
2

√

2
√

αx

(cid:17)

π

+ . . .
√

(cid:16)

− ναx)

−

1
2

2 exp(− 1
√
παx

2 ν2)

(cid:17)

+ . . .

(19)

log fSN(x; α)|x→−∞ ≈ −

log fESN(x; α, τ )|x→−∞ ≈ −

1
2

1
2

x2(1 + α2) + log

(cid:16)

−

1
√

(cid:17)

+ . . .

(α2 + 1)x2 + νx + log

−

αx
(cid:16)

√

2π
1
2

2 exp(− 1
2 ν2)
π
αx

√

(cid:17)

+ . . .

(20)

The expanded results in (19) show a sequence of higher-order terms that quickly approach zero as x → +∞.
As expected, the right tail of both Skew Normal and Extended Skew Normal density gets more and more
similar to the desired Gaussian one. Corresponding left tail results (20) for x → −∞ show a similar Gaussian
pattern but with a slower decay. Here we recognize a log Gaussian density contribution with additional
logarithmic terms coming from the expanded cumulative density Φ(αx).
As discussed in Section 2, these Skew Normal family densities appear to be a natural, reasonable choice to
approximate Latent Gaussian posterior marginals as accurately as possible.

4 The Simpliﬁed Laplace strategy using the Extended Skew Nor-

mal distribution

The Simpliﬁed Laplace strategy described in [20] is one of the most attractive choices to get posterior
approximations of a Latent Gaussian Model structure as it essentially ensures fast computations with a
negligible cost in accuracy for most of the cases.
This strategy applies a third-order Taylor expansion to the target posterior approximations. Then ﬁts Skew
Normal distributions by matching the expansion terms with the high order derivatives of its log-likelihood
evaluated at the mode.
This section reviews the methodology behind this strategy, adding a new way to compute the required Skew
Normal moments, which avoids further approximation and optimization steps for evaluating the mode (see
also [30]). We propose to extend this whole approach by considering a fourth-order Taylor expansion and ﬁt

8

an Extended Skew Normal distribution which uses an additional hidden mean parameter τ .
In this setting, we need to make sure this extended distribution ensures both robustness of the results and
fast computational performances.

4.1 Third order Taylor expansion

The computational advantages of the Simpliﬁed Laplace strategy rely on accurate parametric density ap-
proximations instead of computing the more costly Laplace ones, which are based on a non-parametric
representation of the posterior marginals.
The strategy consists of ﬁtting a Skew Normal distribution to a third-order Taylor expanded density of the
form

log(π(z)) = K −

1
2

z2 + ˜µz +

1
3!

˜γ1z3 + . . .

(21)

where K is a constant, (˜µ, ˜γ1) are terms derived from the third order Taylor expansion of the Laplace
Approximation.
The resulting density in (21) is N (˜µ, 1) up to second order while the third term ˜γ1 provides information of
the third order derivative evaluated at the mode.
Consider R ∼ SN(ξ, ω, α) with unknown location ξ, scale ω and skewness parameter α. Then we can deﬁne a
system of three equations to compute the respective parameter triplet ( ˜ξ, ˜ω, ˜α) to approximate the expanded
density in (21). By matching the ﬁrst two non central moments and the third derivative of the Skew Normal
at the mode z∗, the resulting system is

E(R) = ˜µ

Var(R) = 1
∂3
(cid:12)
(cid:12)
∂r3 log π(r; ξ, ω, α)
(cid:12)r=z∗

= ˜γ1

(22)

However the mode z∗ is not analytically available. Following Appendix B of [20], we can expand log π(r; ξ, ω, α)
at its location point r = ξ to compute an approximation to the mode as

r∗ =

(cid:17)

(cid:16) α
ω

√

2π + 2ξ(cid:0) α
ω
(cid:1)2
π + 2(cid:0) α

ω

(cid:1)

(23)

We evaluate the third derivative of the log Skew Normal density at the approximated mode (23). This
expression is then expanded at α
ω around α = 0 to allow for an exact analytical result and fast computations.
We can avoid these steps and compute a more accurate modal conﬁguration for a Skew Normal random
variable using interpolation between skewness and third log derivative values. Figure 2 shows that the
interpolation curve of the two quantities is smooth and can oﬀer more precise results.
This interpolation avoids using an approximation for the mode. We use the interpolant to obtain the skewness
and then compute the Skew Normal parameters directly from the corresponding mapping.
In most cases, we do not detect signiﬁcant improvements but the new approach still makes the Simpliﬁed
Laplace approximations slightly more accurate when non-negligible skewness is involved. Additionally, it
simpliﬁes the default INLA methodology avoiding computations for solving the system of equations.
The third equation in the system (22) then becomes

˜γ1 = C3(0)

(cid:16) α
ω

(cid:17)3

(24)

where the right side is exactly the resulting polynomial expansion of ∂3
the C function formulation derived from the Extended Skew Normal distribution.
Using equation (24), we can directly solve the system (22) since α is a function of the sole scale parameter
ω with C3(0) being a constant (≈ 0.218).

∂r3 log π(r; ξ, ω, α)

with C3(·) being

(cid:12)
(cid:12)
(cid:12)r=r∗

9

Figure 2: The curve describes the exact relation of skewness and log third derivative evaluated at the exact
mode of a standard Skew Normal random variable for many possible values of skewness in the range (-1,1).
The modes are computed by numerical optimization for maximum accuracy purposes.

4.2 Fourth order Taylor expansion

The Simpliﬁed Laplace methodology can be further extended by considering a fourth-order term in the
expansion (21). In this framework, the Extended Skew Normal distribution described in Section 3 represents
a natural match since it extends the Skew Normal distribution by introducing a fourth parameter in its
analytical representation.
The corresponding log density of (12) can be written in a C function formulation as

log f (t; ξ, ω, α, τ ) = log

(cid:104) 1
ω

φ

(cid:16) t − ξ
ω

(cid:17)(cid:105)

(cid:16)

τ

+ C0

(cid:112)

1 + α2 + α

(cid:17)

t − ξ
ω

− C0(τ )

(25)

If τ = 0 the extended log density in (25) degenerates into a Skew Normal one. Moreover, the role of the
hidden mean parameter becomes irrelevant when α = 0 as the density reverts back to a Gaussian distribution
with mean ξ and variance ω2.
According to [26] and [3], τ aﬀects both skewness and kurtosis of the distribution when α is not zero. It also
determines the asymmetry of the density since it tends to 0 as τ → ±∞.
The log derivatives up to order four are the following

log f (t; ξ, ω, α, τ ) = −

∂
∂t
∂2
∂t2 log f (t; ξ, ω, α, τ ) = −
∂3
∂t3 log f (t; ξ, ω, α, τ ) = C3
∂4
∂t4 log f (t; ξ, ω, α, τ ) = C4

t − ξ
ω2 + C1
(cid:16)
1
ω2 + C2
(cid:16)
(cid:112)

τ

(cid:112)

(cid:16)

τ

1 + α2 +

α
ω

(t − ξ)

(cid:112)

1 + α2 +

(t − ξ)

(cid:17) α
ω
(cid:17)2
(cid:17)(cid:16) α
ω

τ

1 + α2 +

(t − ξ)

(cid:112)

(cid:16)

τ

1 + α2 +

(t − ξ)

α
ω
α
ω

α
ω
(cid:17)(cid:16) α
ω
(cid:17)(cid:16) α
ω

(cid:17)3

(cid:17)4

(26)

We do not have an analytical solution for the mode due to the intractable structure of the ﬁrst log derivative
in (26).
According to the Simpliﬁed Laplace methodology, we ﬁrst expand the third log derivative at t = ξ getting
the new approximated mode

10

t∗ =

(cid:16) α
ω

√

(cid:17) C1(τ

1 + α2) − C2(τ

√

√

1 − C2(τ

1 + α2)

1 + α2)ξ
(cid:16) α
ω

(cid:17)2

(cid:17)

(cid:16) α
ω

(27)

which reverts back to (23) as τ = 0. We chose not to use the interpolant function of Figure 2 for the
Extended distribution since there are now two free parameters. Another existing numerical approximation
for the mode is provided in [3] by using the centralized moments of Skew Normal family densities. The ﬁnal
step sees the expansion of the third and fourth log derivatives of the Extended Skew Normal distribution at
the mode (27) with respect to α
ω around α = 0. Then we obtain two new polynomial approximations for
these high order log derivatives

∂3
(cid:12)
(cid:12)
∂t3 log f (t; ξ, ω, α, τ )
(cid:12)t=t∗
∂4
(cid:12)
(cid:12)
∂t4 log f (t; ξ, ω, α, τ )
(cid:12)t=t∗

≈ C3(τ )

≈ C4(τ )

(cid:16) α
ω
(cid:16) α
ω

(cid:17)3

(cid:17)4

(28)

that are available as functions of the scale parameter ω, the skewness parameter α and the hidden mean
parameter τ .
The new system consists of four equations and is obtained by matching the ﬁrst two moments of the Extended
Skew Normal random variable and its higher-order expanded log derivatives in (28) as follows

ξ + ωδC1(τ ) = ˜µ
ω2(1 + C2(τ )δ2) = 1

C3(τ )

C4(τ )

(cid:16) α
ω
(cid:16) α
ω

(cid:17)3

(cid:17)4

= ˜γ1

= ˜γ2

(29)

with (˜γ1, ˜γ2) being the third and fourth log derivatives evaluated at the mode derived from the target
approximated posterior in (21).
Lastly, we compute the solutions of the Extended Skew Normal parameters by solving the system (29). No
straightforward analytical solution is available and we must rely on interpolation to the parameter τ .

4.3 Interpolating the hidden mean solutions

The Extended Skew Normal distribution can be used to ﬁt a univariate target posterior approximation
through a polynomial expansion up to order four.
The Simpliﬁed Laplace methodology describes how to get accurate results from a system of equations that
involves matching moments and high order log derivatives of the new extended distribution. We observe
that the last two equations in (29) lead to the following relation

˜γ2
[˜γ1]4/3

=

C4(τ )
[C3(τ )]4/3

(30)

which is cumbersome to solve in terms of τ values. Nevertheless, Figure 3 shows there exists quite a smooth
behaviour amongst the τ solutions for the C function ratio (30).
Instead of relying on costly non-linear solvers, we construct an interpolant function between τ and its
derivative ratio within a reasonable range of solutions.
The interpolant function ensures accurate and fast solutions for reasonable boundaries of τ . We can also
notice that the derivative ratio is positively bounded from above as follows

− ∞ <

C4(τ )
[C3(τ )]4/3

< 2.4 with − ∞ < τ < ∞

(31)

11

Figure 3: Relationship between the truncation parameter τ and the C function derivative ratio
obtained from 30.

C4(τ )
[C3(τ )]4/3

which matters as an Extended Skew Normal distribution converges to a Gaussian distribution when τ → ±∞.
1√
More precisely [6] shows that the limiting Gaussian cases are N (ξ, ω2) for τ → ∞ and N (−α|τ |,
1−δ2 ) for
τ → −∞.
Since C3(τ ) approximately resembles a probability density function with respect to the parameter τ , we
can consider a criterion to decide whether a resulting value of τ is reasonable or not according to the log
derivative outcomes (˜γ1, ˜γ2).
As discussed in Section 3, we establish that a |τ | > 10 value is already far extreme and can lead to unlikely
or unstable results. As the respective parameter probability space coverage given by (cid:82) 10
−10 C3(τ ) dτ ≈ 0.99 is
high, this rule of thumb ensures to keep most of the solutions.
Additionally, a low value of ˜γ1 results in an unreasonable ratio outcome of 31 for the corresponding inter-
polant. When ˜γ1 approaches zero, the Extended Skew Normal density bends to a Gaussian one and the new
approach gets unstable.
To account for these unreasonable scenarios we simply return to the original Simpliﬁed Laplace approach
using Skew Normal distribution, if this happens.
Overall the interpolant for the hidden mean parameter τ works well and does not add computational costs.

By exploiting interpolation to solve the ratio in (30), we can obtain solutions for the system derived from
using an Extended Skew Normal distribution.
Assuming ˜γ1 is not zero, we write a∗ = C3(˜τ ) where ˜τ is the result obtained by the interpolant. We write
the skewness parameter as ˜α = ˜ωb∗ with b∗ = ( γ3

a∗ )(1/3) and get

12

(cid:115)

˜ω =

−d∗ + (cid:112)(d∗)2 + 4c∗σ2
2c∗

(32)

where c∗ = (b∗)2(1 + C2(˜τ )) and d∗ = 1 − (b∗)2. If ˜τ approaches 0 then we revert to a Skew Normal system
of equations. Here we know that the location ˜ξ is given by

where ˜δ = ˜α√
system.

1+ ˜α2 . The last expression (33) gives the ﬁnal location solution for the Extended Skew Normal

˜ξ = ˜µ − ˜ω˜δC1(˜τ )

(33)

5 Applications

Skew Normal family provides a class of parametric distributions that well approximate posterior marginals
for Latent Gaussian Models. As discussed in Section 4, we can use Skew Normal and Extended Skew Normal
distributions to get deterministic approximations for these posteriors using INLA and its Simpliﬁed Laplace
strategy. Since we are interested in comparing outcomes from diﬀerent strategies from INLA and MCMC in
more extreme cases, we set a series of simulations that trigger high marginal skewness. We expect to observe
accuracy diﬀerences between the two parametric choices in this framework. We simulate data from Binomial
and Poisson likelihoods with diﬀerent sample sizes and one single covariate with Gaussian prior to keep things
simple. We then proceed with a Bayesian inference analysis onto these GLM models by using the following
strategies: the standard Simpliﬁed Laplace Approximation (SLA) with Skew Normal distributions, the
extended Simpliﬁed Laplace Approximation (ESLA) using Extended Skew Normal distributions strategies,
the full Laplace Approximation (LA) in INLA and the MCMC samples from JAGS.

5.1 Comparing INLA and MCMC strategies

The simulations for both Binomial and Poisson likelihoods are done with varying sample size dimensions n
from one observation up to 100. This setting results in non-negligible marginal skewness for the respective
marginal posteriors. The posterior marginals tend to be less extreme when the sample size increases as they
will converge to a Gaussian limit.
All resulting posterior marginals obtained from the diﬀerent strategies are reported in the plots below.
Comparison results from the Binomial model can be observed in Figure 4, 5, 6 and 7 while the Poisson ones
are shown in Figure 8, 9, 10 and 11. For low sample size n, we observe that the ESLA strategy provides
more accurate results around the mode. The full Laplace (LA) and MCMC methods report the true results
and do not diﬀer in practice. ESLA posterior results appear closer to LA and MCMC than SLA strategy,
where the mode is far oﬀ the expected location.
For larger sample size n, we tend to observe similar results for all strategies, with ESLA being slightly more
accurate. A summary of the posterior modal conﬁgurations for diﬀerent sample sizes is given on Tables 1
and 3, while interquartile ranges (IQR) are reported in Table 2 and 4.
These simulations underline that ESLA strategy is preferable in more extreme settings where the skewness
is high. The extended methodology also preserves robustness as it is forced to revert back to a standard
Simpliﬁed strategy in non-extreme cases.

6 Discussion

Latent Gaussian Models provide an appealing hierarchical model structure for Bayesian inference as the a
priori Gaussian assumption binds the posterior marginals. We discussed that densities with Gaussian tails
can be a natural choice for approximating these marginals. Under these assumptions, the INLA methodology
works well for this class of models by constructing fast and accurate deterministic approximations. Among
the diﬀerent available approximation options, the Simpliﬁed Laplace strategy is indeed one of the most
advantageous for its speed and accuracy trade-oﬀ. This strategy relies on Skew Normal approximations of
a third-order Taylor series expansion of the Laplace approximations, the latter of which are known to be

13

Figure 4: Comparative results between SLA (black line), LA (red line), ESLA (blue line) and MCMC (green
line) strategies with n = 1 observations and a Bernoulli likelihood. Extreme negative skewness setting with
minimum sample size. Since LA and MCMC strategies embody the posterior truth, we can observe that the
SLA approach shows way less accuracy around the mode than its extended version denoted by ESLA. Tail
behavior is similar for both SLA and ESLA and still appears to be slightly inaccurate in the left direction.

Figure 5: Comparative results between SLA (black line), LA (red line), ESLA (blue line) and MCMC (green
line) strategies with n = 10 observations and a Bernoulli likelihood. Extreme positive skewness setting with
small sample size. Since LA and MCMC strategies embody the posterior truth, we can see that the SLA
approach shows way less accuracy around the mode than its extended version ESLA. Tail behavior is similar
for both SLA and ESLA and still appears to be moderately inaccurate in the right direction.

14

Figure 6: Comparative results between SLA (black line), LA (red line), ESLA (blue line) and MCMC (green
line) strategies with n = 50 observations and a Bernoulli likelihood. Extreme positive skewness setting with
moderate sample size. All employed strategies for this application show similar results except for the SLA
methodology, which appears to be more inaccurate around the mode. Still, both SLA and ESLA suﬀer minor
deviations in the right tail compared to LA and MCMC truth.

Figure 7: Comparative results between SLA (black line), LA (red line), ESLA (blue line) and MCMC
(green line) strategies with n = 100 observations and a Bernoulli likelihood. High positive skewness setting
with enough large sample size. All employed strategies for this application show similar results with minor
deviations around the mode given by the SLA methodology. Large sample sizes tend to provide more stable
expected results no matter the approximation strategy we use. Still, ESLA strategy is much closer to the
true posterior results than SLA.

15

Figure 8: Comparative results between SLA (black line), LA (red line), ESLA (blue line) and MCMC (green
line) strategies with n = 1 observations and a Poisson likelihood. Extreme negative skewness setting with
minimum sample size. Since LA and MCMC strategies embody the posterior truth, we can observe that
the SLA approach shows way less accuracy around the mode than its extended version denoted by ESLA.
Unlike the Binomial case, tail behaviors for both SLA and ESLA closely match with no evident diﬀerences.

Figure 9: Comparative results between SLA (black line), LA (red line), ESLA (blue line) and MCMC (green
line) strategies with n = 5 observations and a Poisson likelihood. Extreme positive skewness setting with
small sample size. Since LA and MCMC strategies embody the posterior truth, we can see that the SLA
approach shows way less accuracy around the mode than its extended version ESLA. Unlike the Binomial
case, tail behaviors for both SLA and ESLA closely match with no evident diﬀerences.

16

Figure 10: Comparative results between SLA (black line), LA (red line), ESLA (blue line) and MCMC
(green line) strategies with n = 10 observations and a Poisson likelihood. High negative skewness setting
with small sample size. All employed strategies for this application show similar results except for the SLA
methodology, which appears to be more inaccurate around the mode. Still, both SLA and ESLA suﬀer minor
deviations in the left tail compared to LA and MCMC truth.

Figure 11: Comparative results between SLA (black line), LA (red line), ESLA (blue line) and MCMC
(green line) strategies with n = 50 observations and a Bernoulli likelihood. Moderate negative skewness
setting with enough large sample size. All employed strategies for this application closely converge to the
same posterior result with no evident diﬀerence. Large sample sizes tend to provide more stable expected
results no matter the approximation strategy we use.

17

Table 1: Binomial simulations for increasing sample sizes up to n = 100 and posterior mode evaluations
using SLA, ESLA, LA and MCMC strategies. For low sample sizes, the modes derived from ESLA strategy
are closer to the true ones from LA and MCMC approaches than the default SLA strategy. As the sample
size n increases, we notice a decreasing pattern for the positive skewness sequence (apart from n = 1), with
the mode values converging to the same result for all strategies. Overall, ESLA provides more coherent
results to LA and MCMC, conﬁdently representing the truth.

n

Skew Mode(SLA) Mode(ESLA) Mode(LA) Mode(MCMC)

1
2
5
10
20
50
100

-0.578
0.644
0.627
0.495
0.451
0.306
0.218

-8.979
0.346
1.17
1.207
0.639
0.908
0.85

-14.528
0.783
1.764
1.39
0.722
0.934
0.862

-16.581
1.084
1.844
1.459
0.784
0.964
0.881

-17.249
0.995
1.914
1.363
0.764
0.94
0.876

Table 2: Binomial simulations for increasing sample sizes up to n = 100 and posterior interquartile range
(IQR) evaluations using SLA, ESLA, LA and MCMC strategies. The IQRs from both SLA and ESLA
strategies get closer and closer to the truth provided by LA and MCMC posterior results as soon as the
sample size increases. Although the diﬀerence is less relevant than the one from the respective mode in Table
1, ESLA grants more accurate results towards the truth than its simpler version SLA.

n

Skew IQR(SLA)

IQR(ESLA)

IQR(LA)

IQR(MCMC)

1
2
5
10
20
50
100

-0.578
0.644
0.627
0.495
0.451
0.306
0.218

25.865
2.046
2.316
1.189
0.755
0.468
0.365

26.909
2.138
2.4
1.232
0.78
0.477
0.37

28.73
2.838
2.751
1.313
0.813
0.483
0.372

28.949
3.012
2.80
1.31
0.816
0.483
0.372

Table 3: Poisson simulations for increasing sample sizes up to n = 100 and posterior mode evaluations using
SLA, ESLA, LA and MCMC strategies. For low sample sizes, the modes derived from ESLA strategy are
closer to the true ones from LA and MCMC approaches than the default SLA strategy. As the sample size
n increases, we notice a decreasing pattern for the negative skewness sequence (apart from n = 2), with the
mode values converging to the same result for all strategies. Overall, ESLA provides more coherent results
to LA and MCMC, conﬁdently representing the truth.

n

Skew Mode(SLA) Mode(ESLA) Mode(LA) Mode(MCMC)

1
2
5
10
20
50
100

-0.446
0.496
-0.322
-0.311
-0.223
-0.179
-0.113

0.972
-2.696
1.882
0.796
0.992
1.109
1.033

0.905
-2.195
1.85
0.78
0.983
1.106
1.032

0.87
-2.06
1.822
0.767
0.973
1.103
1.03

0.886
-1.87
1.814
0.763
0.969
1.108
1.026

18

highly accurate but computationally demanding. Skew Normal densities satisfy the Gaussian tail argument
for modeling the latent posterior marginals while allowing non-negligible skewness.
However, this parametric assumption can pose a limit in more extreme cases, and we questioned if a more
appropriate solution can be formulated. We chose another natural parametric distribution that still belongs
to the Skew Normal family and ensured the Gaussian bounds are preserved: the Extended Skew Normal
distribution. As reported in [3], this distribution is one of the Skew Normal extensions that has an additional
parameter that aﬀects all the moments, but in particular, the skewness for our purposes. Like the Simpliﬁed
Laplace strategy, we formulated a system of equations by matching higher order derivatives of the Extended
Skew Normal distribution evaluated at the mode, with the respective ones obtained from the expanded
Laplace approximations. By interpolating some of the fourth parameter τ solutions, we eﬃciently calculate all
four parameters necessary to ﬁt an Extended Skew Normal approximation to the expansion. This alternative
parametric approximation extends the capabilities of the Simpliﬁed Laplace strategy oﬀering more accurate
skew marginals, especially in more extreme settings. This work contributes an additional accurate and
computational eﬃcient approximation within the INLA framework, based on the Extended Skew Normal
distribution and innovative solutions to calculate the necessary parameters. We believe that this contribution
enables more accurate but still eﬃcient Bayesian inference of complex models in the statistical community
as well as the scientiﬁc community at large.

Appendix A: A special case: t-student as a normal mixture

The Gaussian distribution provides bounds for the posterior marginals of a generic Bayesian inference up to a
constant (see Section 2). This is even more clear when the observed data y1, . . . , yn are Gaussian distributed
since the constants follow the same pattern. There are other cases that may show a non normal behaviour
but they can still be cast into a Latent Gaussian paradigm. As an example, the t-student distribution
is a statistical representation that allows for normal mixture structure but one can also consider logistic
and Laplace distributions as well. As reported in Chapter 4 in [18], t-student assumptions can be encoded
through a scale mixture of normals by having x|λ ∼ N (0, λ−1Q−1) with x being a latent ﬁeld component
and λ a diagonal matrix of auxiliary variables. Introducing such auxiliary variables into the hierarchical
representation of the latent model eases the overall structure when non normal assumptions are involved.
Combining auxiliary variables and t-student information lead to the so called hierarchical t-formulation. In
particular, we underline the case where we assume the latent ﬁeld x to be t-student distributed with normal
data y. As we employ auxiliary variables to get a normal scale structure, its hierarchical t-representation
would be as follows

Table 4: Poisson simulations for increasing sample sizes up to n = 100 and posterior interquartile range
(IQR) evaluations using SLA, ESLA, LA and MCMC strategies. The IQRs from both SLA and ESLA
strategies get closer and closer to the truth provided by LA and MCMC posterior results as soon as the
sample size increases. Although the diﬀerence is less relevant than the one from the respective mode in Table
3, ESLA grants more accurate results towards the truth than its simpler version SLA.

n

Skew IQR(SLA)

IQR(ESLA)

IQR(LA)

IQR(MCMC)

1
2
5
10
20
50
100

-0.446
0.496
-0.322
-0.311
-0.223
-0.179
-0.113

0.598
3.588
0.492
0.251
0.224
0.092
0.082

0.618
3.717
0.5
0.256
0.227
0.093
0.083

0.64
3.936
0.5
0.256
0.227
0.093
0.083

0.644
3.92
0.5
0.257
0.227
0.093
0.083

19

y|x ∼

n
(cid:89)

i=1

π(yi|xi)

x ∼ N(0, λ−1Q−1)

diag{λ} ∼

n
(cid:89)

i=1

(cid:17)

G

(cid:16) ai
2

,

ai
2

(34)

where π(y|x) is Gaussian while the mixing parameters λ1, . . . , λn are Gamma distributed. We assume our
likelihood to be bounded by constants ˜K1, . . . , ˜Kn, and end up with the following joint posterior relation

where ˜K = (cid:81)
i

˜Ki is an overall constant. Then the full conditional is bounded as

π(x, λ|y) ∝ π(y|x)π(x|λ)π(λ) ≤ π(x|λ)π(λ) ˜K

π(x|λ, y) ≤ ˜Kπ(x|λ)

Similarly to the derivation in Section 2, we obtain a bound for the corresponding marginals

(cid:16)
π(xi|λi, y) ≤ ˜Ki exp

−

λix2
i
2

(Q−1)−1
ii

(cid:17)

,

(35)

(36)

(37)

that are again bounded by a Gaussian distribution. The inequalities (36) and (37) show that we have control
on all possible full conditional densities of the model as they are bounded by Gaussian densities. The same
does not apply to the marginals π(xi|y) since they would still be bounded by t-student distributions. Non-
normal assumptions on latent ﬁeld or likelihood add complexity in approximating posterior marginals from
these hierarchical structure.
The mixture representation of marginal posterior densities (8) entirely depends on full conditionals as we
integrate out all the hyperparameters. Both the parametric and non parametric strategies of the methodology
will still provide accurate results when the latent ﬁeld is not normal.

References

[1] Azzalini, A. and Capitanio, A. (1999). Statistical applications of the multivariate skew normal distribu-

tion. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):579–602.

[2] Azzalini, A. and Capitanio, A. (2003). Distributions generated by perturbation of symmetry with em-
phasis on a multivariate skew t -distribution. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 65(2):367–389.

[3] Azzalini, A. and Capitanio, A. (2018). The skew-normal and related families. Cambridge Cambridge

University Press.

[4] Bakka, H., Rue, H., Fuglstad, G. A., Riebler, A., Bolin, D., Illian, J., Krainski, E., Simpson, D., and Lind-
gren, F. (2018). Spatial modelling with R-INLA: A review. WIREs Computational Statistics, 10:e1443(6).
(Invited extended review).

[5] Blangiardo, M., Cameletti, M., Baio, G., and Rue, H. (2013). Spatial and spatio-temporal models with

R-INLA. Spatial and Spatio-Temporal Epidemiology, 3(December):39–55.

[6] Canale, A. (2011). Statistical aspects of the scalar extended skew-normal distribution. Metron, LXIX:279–

295.

[7] Canale, A. (2015). A note on regions of given probability of the extended skew-normal distribution.

Communications in Statistics - Theory and Methods, 44(12):2507–2516.

20

[8] Ferkingstad, E., Held, L., and Rue, H. (2017). Fast and accurate Bayesian model criticism and conﬂict

diagnostics using R-INLA. Stat, 6(1):331–344.

[9] Ferkingstad, E. and Rue, H. (2015). Improving the INLA approach for approximate Bayesian inference

for latent Gaussian models. Electronic Journal of Statistics, 9:2706–2731.

[10] Krainski, E. T., G´omez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilio, D., Simpson, D., Lindgren,
F., and Rue, H. (2018). Advanced Spatial Modeling with Stochastic Partial Diﬀerential Equations using R
and INLA. CRC press. Github version www.r-inla.org/spde-book.

[11] Lindgren, F., Rue, H., and Lindstr¨om, J. (2011). An explicit link between Gaussian ﬁelds and Gaussian

Markov random ﬁelds: The SPDE approach (with discussion). 73(4):423–498.

[12] Martino, S. and Riebler, A. (2019). Integrated nested laplace approximations (inla). arXiv preprint

arXiv:1907.01248.

[13] Martins, T. G. and Rue, H. (2014). Extending INLA to a class of near-Gaussian latent models. 41(4):893–

912.

[14] Martins, T. G., Simpson, D., Lindgren, F., and Rue, H. (2013). Bayesian computing with INLA: New

features. 67:68–83.

[15] Muﬀ, S., Riebler, A., Rue, H., Saner, P., and Held, L. (2015). Bayesian analysis of measurement error

models using integrated nested Laplace approximations. 64(2):231–252.

[16] Niekerk, J. v., Bakka, H., and Rue, H. (2021). Competing risks joint models using r-inla. Statistical

Modelling, 21(1-2):56–71.

[17] Paulino P´erez-Rodriguez, Jos´e A, V. n. o. (2017). Bayesian estimation for the centered parameterization

of the skew-normal distribution. Revista Colombiana de Estad ˜Astica, 40:123 – 140.

[18] Rue, H. and Held, L. (2005). Gaussian Markov Random Fields: Theory and Applications, volume 104

of Monographs on Statistics and Applied Probability. Chapman & Hall, London.

[19] Rue, H. and Martino, S. (2007). Approximate bayesian inference for hierarchical gaussian markov

random ﬁelds. Journal of Statistical Planning and Inference, 137:3177–3192.

[20] Rue, H., Martino, S., and Chopin, N. (2009). Approximate Bayesian inference for latent Gaussian

models using integrated nested Laplace approximations (with discussion). 71(2):319–392.

[21] Rue, H., Riebler, A., Sørbye, S. H., Illian, J. B., Simpson, D. P., and Lindgren, F. K. (2017). Bayesian
computing with INLA: A review. Annual Reviews of Statistics and Its Applications, 4(March):395–421.

[22] Ruli, E., Sartori, N., and Ventura, L. (2014). Marginal Posterior Simulation via Higher-order Tail Area

Approximations. Bayesian Analysis, 9(1):129 – 146.

[23] Ruli, E., Sartori, N., and Ventura, L. (2016). Improved laplace approximation for marginal likelihoods.

Electronic Journal of Statistics, 10:3986–4009.

[24] Ruli, E. and Ventura, L. (2016). Higher-order bayesian approximations for pseudo-posterior distribu-

tions. Communications in Statistics - Simulation and Computation, 45(8):2863–2873.

[25] Schenk, O. and G¨artner, K. (2004). Solving unsymmetric sparse systems of linear equations with pardiso.

Future Generation Computer Systems, 20(3):475 – 487. Selected numerical algorithms.

[26] Seijas-Macias, A., Oliveira, A., and Oliveira, T. (2017). The presence of distortions in the extended

skew: normal distribution. In Proceedings 2nd ISI Regional Statistics Conference. ISI-RSC.

[27] Simpson, D., Lindgren, F., and Rue, H. (2011). Fast approximate inference with inla: the past, the

present and the future. arXiv preprint arXiv:1105.2982.

21

[28] Simpson, D., Rue, H., Riebler, A., Martins, T. G., and Sørbye, S. H. (2017). Penalising model component

complexity: A principled, practical approach to constructing priors. Statistical science, 32(1):1–28.

[29] Van Niekerk, J., Bakka, H., Rue, H., and Schenk, O. (2019). New frontiers in bayesian modeling using

the inla package in r. arXiv preprint arXiv:1907.10426.

[30] Wood, S. N. (2019). Simpliﬁed integrated nested Laplace approximation. Biometrika, 107(1):223–230.

22

