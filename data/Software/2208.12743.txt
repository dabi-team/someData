2
2
0
2

g
u
A
6
2

]
E
S
.
s
c
[

1
v
3
4
7
2
1
.
8
0
2
2
:
v
i
X
r
a

White-box Fuzzing RPC-based APIs with EvoMaster:
An Industrial Case Study

Man Zhang1, Andrea Arcuri1, Yonggang Li2, Yang Liu2, Kaiming Xue2
1Kristiania University College, Norway
2 Meituan, China

Abstract

Remote Procedure Call (RPC) is a communication protocol to support client-server interactions
among services over a network. RPC is widely applied in industry for building large-scale distributed
systems, such as Microservices. Modern RPC frameworks include for example Thrift, gRPC,
SOFARPC and Dubbo. Testing such systems using RPC communications is very challenging, due
to the complexity of distributed systems and various RPC frameworks the system could employ.
To the best of our knowledge, there does not exist any tool or solution that could enable automated
testing of modern RPC-based services. To fill this gap, in this paper we propose the first approach
in the literature, together with an open-source tool, for fuzzing modern RPC-based APIs. The
approach is in the context of white-box testing with search-based techniques. In this approach,
a RPC schema is defined to formulate the API specification that could document all necessary
info to make a RPC call and possible responses (e.g., throwing exception, failure). The schema of
the RPC-based services can be automatically extracted from the source code with our approach.
This allows to test the services developed with different RPC frameworks. Then with the extracted
schema, we employ search to produce tests by maximizing white-box heuristics and newly defined
heuristics specific to RPC domain. We built our approach as an extension to an open-source fuzzer
(i.e., EvoMaster) and applied the MIO algorithm for test generation. To assess our novel approach,
we conducted an empirical study with two artificial and four industrial web services selected by
our industrial partner. The tool has been integrated into a real industrial pipeline, and could be
applied to real industrial development process for fuzzing RPC-based APIs. To further demonstrate
its effectiveness and application in industrial settings, we also report results of employing our tool
for fuzzing another 30 industrial APIs autonomously conducted by our industrial partner in their
testing processes. Results show that our novel approach is capable of enabling automated test case
generation for industrial RPC-based APIs (i.e., two artificial and 34 industrial). We also compared
with a simple grey-box technique and existing manually written tests. Our white-box solution
achieves significant improvements on code coverage. Regarding fault detection, by conducting
a careful review with our industrial partner of the tests generated by our novel approach in the
selected four industrial APIs, a total of 41 real faults were identified, which have now been fixed.

Keywords: Mircoservices, RPC, fuzzing, test generation, SBST, gRPC, Thrift

1

Introduction

It is a common practice in industry to develop large enterprise systems with microservice architec-
tures [50, 68, 58]. For example, Meituan is a large e-commerce enterprise with more than 630 millions
customers in China, with microservice systems like Meituan Select comprising more than 1000 different
web services. Testing this kind system is very complex, due to their distributed nature and access to
external services such as databases. There is a dire need in industry for automation for this kind of
systems.

Although in the recent years there has been an interest in the research community on fuzzing
REST web services (e.g., with tools like Restler [27], RestTestGen [57], Restest [49], RestCT [59],
bBOXRT [43], and Schemathesis [40]), to the best of our knowledge there is no work in the literature
on the testing of modern Remote Procedure Call (RPC) web services. None of existing fuzzers for
RESTful APIs can be directly applied on fuzzing RPC systems, as the API schemas and communication
protocols are different.

1

 
 
 
 
 
 
As part of an industry-driven collaboration [38, 14, 36, 37, 35], when we first tried to use our
EvoMaster fuzzer [13] on the web services developed at Meituan, we could not apply it directly [64].
We had to manually write REST APIs as wrappers for the RPC systems (which use Apache Thrift).
Not only it is time consuming, but also the generated tests are more difficult to use for debugging any
found fault. Two web services were used as case study. Such study (with interviews and questionnaires
among the developers at Meituan) pointed out to few research challenges, including the need for a native
support for RPC systems for web service fuzzers. Such support not only requires not trivial engineering
effort (our extension to EvoMaster required more than 10 000 lines of code, not including test cases),
but also there are few research challenges that need to be addressed to best handle RPC-based APIs
(as we will discuss in more details later in the paper).

In this paper, we provide a novel approach1 to automatically fuzz RPC-based APIs, built on top of

EvoMaster. Main contributions of the paper include:

1. the first approach in the literature for fuzzing RPC-based APIs;

2. an open-source tool support (i.e., a fuzzer);

3. an empirical study carried out in industrial settings that involves in total 34 industrial RPC-based

APIs comprising 647,998 lines of codes (computed with JaCoCo) for business logic;

4. an in-depth analysis on 4 selected industrial APIs with our industrial partner; and

5. identifying lessons learned and research challenges that must be addressed before better results

can be obtained.

The paper is organized as follows. Section 2 provides background information on RPC-based APIs
and EvoMaster. Section 3 analyzes related work. The details of our novel approach are presented
in Section 4. Our empirical study is discussed in Section 5, followed by lessons learned in Section 6.
Threats to validity are discussed in Section 7. Finally, we conclude the paper in Section 8.

2 Background

2.1 Remote Procedure Call (RPC)

Remote Procedure Call (RPC) enables to call methods in other processes, possibly on a different
machine, communicating over a network. This is a common practice in distributed systems, particularly
in microservice architectures [50]. There are different frameworks to develop RPC-based APIs, like for
example Apache Thrift [10](originally from Facebook), Apache Dubbo [1](originally from Alibaba),
gRPC [4] (from Google) and SOFARPC [7] (from Alibaba). All these popular frameworks were
developed to address the scale of large distributed systems. Compared to other types of web services
(e.g., RESTful APIs), RPC-based APIs aim at optimizing performance at the cost of stronger coupling
between client and server applications (there is no silver bullet). For example, given a schema for the
API (e.g., a .thrift file for Thrift or a .proto file for gRPC), a compiler is used to create a server
application (which then can be extended with the business logic of the API) and a client library. A
process that wants to communicate with the server API must include this client library, and use these
client-stubs to remotely call the API in the server process. For example (rpc1 in Figure 1), a client
process (in the service A) would have a reference to a class-stub B, and, every time that stubB.bcd()
is called, then the client-library will make a network call to execute B.bcd() on the server API.

The actual communications between the client and the server depend on the framework imple-
mentation, e.g., typically HTTP/2 using Protobuf for gRPC, but it can be configured to use other
protocols. Both Thrift and gRPC support the generation of client/server code in different languages
(e.g., Java, C# and JavaScript), whereas SOFARPC and Dubbo support only Java.

Figures 2 and 3 represent examples of a schema specified with different frameworks (i.e., Thrift and
gRPC), and snippets of code of client-stub classes and server classes (e.g., interface/abstract classes)

1EvoMaster is open-source, and it is available at www.evomaster.org. A replication package for this study is

available at https://github.com/anonymous-authorxyz/fuzzing-rpc

2

Figure 1: RPC-based APIs in Microservices

to implement/extend that are automatically generated by the framework based on the schema. As
shown in Figure 2a for Thrift and Figure 3a for gRPC, the two schemas for NcsService have the same
function, i.e., bessj that evaluates Bessel function by taking one integer and one double numbers
as inputs then returning a Data Transfer Objects (DTO) which comprises same fields. Based on the
schemas, the compiler could automatically generate source code of client libraries, such as Client class
at line 13 in Figure 2b and NcsServiceBlockingStub class at line 22 in Figure 3b. Then, with such
client libraries, the functions of RPC-based API could be accessed. For instance, snippet code shown
in Figure 2c represents an example of a test for the NcsService implemented with Thrift framework.
Lines 4 and 6 in Figure 2c represent how to instantiate a client to access the service, such as a URL
with http: // localhost: 8080/ ncs and an accepted protocol to perform communications between
client and service as TBinaryProtocol. Line 12 is to make a network call to bessj function with the
instantiated client then receive a response, and lines 14 and 15 show assertions on the response.

Besides the client-stub classes, the compiler also generate the source code which users could extend
for implementing business logic of the services. In this example, with Java language, Thrift outputs
Interface class which comprises a list of methods to implement, and each method corresponds to a
RPC function (see lines 7--9 in Figure 2b). For gRPC, it is similar that the gRPC compiler outputs
abstract class which comprises a list of methods to extend (see lines 13--18 in Figure 3b). As
the examples, such methods in the classes define specifications about how to access the services. In
addition, a schema specification (e.g., .thrift file) might not be always available for a RPC-based
API, and the service could be initially defined with programming language, such as SOFARPC2 and
Dubbo3 with Java interface classes. To fuzz RPC-based APIs, extracting specifications to access
the API is a prerequisite. The various RPC frameworks allow abstraction classes (i.e., Interface and
abstract class in Java) to define RPC-based APIs (we refer the classes which define RPC-based
APIs as RPCInterfaces in later sections). Thus, if we could enable an extraction of the specification
based on such RPCInterfaces, it would generalize the application of the approach for fuzzing the APIs
with different RPC frameworks (as we propose in this paper).

Moreover, in the context of microservice architectures, as shown in Figure 1, the microservices
comprise of a set of connected RPC-based APIs, and the API could have multiple stub-classes of
other direct interacted APIs (e.g., B has stub-classes of the services C and D). Processing a request
from the user typically involves multiple APIs. With different inputs in the request, it could result
in various sequences of RPC calls with different APIs. For instance, assume that a user sends a
request that results in a function call to abc of the service A. In order to provide a response to the

2https://www.sofastack.tech/en/projects/sofa-rpc/getting-started-with-sofa-boot/
3https://github.com/apache/dubbo

3

user, it needs to involve multiple APIs (e.g., B, C and D) that could trigger various sequences of RPC
communications, e.g., rpc1 → rpc2 → rpc3 or rpc1 → rpc4 → rpc5 as shown in Figure 1. Note that
this example illustrates communications among RPC-based services for processing one request. To test
one RPC-based API (e.g., B is the System Under Test (SUT)), C and D could be considered as external
services of B, and the test for the API B would consist of a sequence of network calls to the SUT as
the example shown in Figure 2c.

2.2 EvoMaster

EvoMaster is an open-source tool for fuzzing enterprise applications with search-based techniques,
in the context of both white-box and black-box testing [13, 17, 25]. To enable white-box testing,
the tool is composed of two components, i.e., driver and core, as shown in Figure 4. The driver is
responsible for collecting Search-Based Software Testing (SBST) heuristics with code instrumentation
(currently targeting JVM [15] and JavaScript [65]) and controlling the SUT (i.e., start/stop/reset).
It is implemented as a library, so that it is easy to be applied by the SUT using for example Maven
and Gradle. The core encompasses the main functionality of generating test cases with search-based
techniques, e.g., various search algorithms and fitness function.

To generate more effective white-box tests for enterprise APIs, EvoMaster is equipped with
a set of novel techniques. For instance, boolean flag is a common problem in handling white-box
testing with search, i.e., no gradients for search to solve a constraint which is either true or false.
To enable fitness gradients for such problems in the source code, EvoMaster is integrated with
testability transformations [22, 23]. This enables branch distance computations for such flag problems by
transforming source code (e.g., via replacement methods) with code instrumentation. The replacement
methods also track inputs (referred as taint analysis) for providing additional information to the
search for solving this kind of problem. In addition, enterprise APIs typically interact with databases.
Database with various data would represent various states of the SUT. To test the APIs with various
states, SQL-handling [19, 20] was developed in EvoMaster that can extract SQL queries and calculate
heuristics for the queries at runtime. Then, with these heuristics, EvoMaster can directly generate
data into the database (with SQL commands such as INSERT). REST is one of popular architectural
styles for building web services. To better support it, EvoMaster employs a set of techniques
designed in particular for the REST domain, e.g., OpenAPI parser, smart sampling, test reformulation
for REST APIs [16], resource- and dependency- based strategies [67, 62].

EvoMaster also employs a set of techniques designed in particular for the REST domain [16, 67, 62].
Furthermore, EvoMaster is enhanced with adaptive hypermutation [61]. It is as an advanced search
mutator for handling long and structured chromosomes, like the tests for REST APIs which comprise a
set of INSERT commands and a sequence of HTTP requests with query parameters and body payloads
(e.g., JSON objects).

To serve as a SBST fuzzer, EvoMaster includes the implementation of different search algorithms,
i.e., MOSA [52], WTS [34, 53] and Random. MIO [11, 15] is a search algorithm that was designed
specifically for system test generation in the context of white-box testing. MIO has been empirically
studied by comparing with the other existing work (e.g., WTS, MOSA and Random), using artificial
and real case studies. Results showed that MIO achieved the overall best performance [11, 15] in this
domain.

3 Related Work

To the best of our knowledge, there does not exist any technique for fuzzing modern RPC-based APIs
(e.g., using frameworks like Apache Thrift, gRPC and SOFARPC). In addition, EvoMaster seems
the only open-source tool which supports white-box testing for Web APIs, and it gives the overall
best results in recent empirical studies comparing existing fuzzers for REST APIs [42, 63]. However,
currently EvoMaster only supports fuzzing RESTful APIs [16] and GraphQL APIs [30].

In the literature, there has been work on the fuzzing of other kinds of web services. The oldest
approaches deal with black-box fuzzing of SOAP [31] web services, like for example [55, 51, 60, 28,
54, 47, 39, 45, 29, 44]. SOAP is a type of RPC protocol. However, SOAP’s reliance on XML format

4

i 3 2 r e s u l t A s I n t ,

1 :
2 : double r e s u l t A s D o u b l e

1 namespace j a v a o r g . t h r i f t . n c s
2
3 s t r u c t Dto {
4
5
6 }
7
8 s e r v i c e N c s S e r v i c e {
9

10 Dto b e s s j ( 1 : i 3 2 n , 2 : double x )
11
12 }

. . .

(a) Snippet of an example of RPC schema (i.e., ncs.thrift) specified with Thrift

1 package o r g . t h r i f t . n c s . c l i e n t ;
2
3 @SuppressWarnings ( { " c a s t " ,
4 @javax . a n n o t a t i o n . Generated ( v a l u e = " A u t o g e n e r a t e d by T h r i f t Compiler

" unchecked " ,

" r a w t y p e s " ,

" s e r i a l " ,

" unused " } )

( 0 . 1 5 . 0 ) " , d a t e = "

2021−10−21 " )

public Dto b e s s j ( i n t n , double x ) throws o r g . apache . t h r i f t . TException ;
. . .

public s t a t i c c l a s s C l i e n t extends o r g . apache . t h r i f t . T S e r v i c e C l i e n t implements I f a c e {

public Dto b e s s j ( i n t n , double x ) throws o r g . apache . t h r i f t . TException
{

}

public i n t e r f a c e I f a c e {

5 public c l a s s N c s S e r v i c e {
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22 }

s e n d _ b e s s j ( n , x ) ;
return r e c v _ b e s s j ( ) ;

}
. . .

}

(b) Snippet of an example of an interface generated with Thrift framework

t r a n s p o r t = new THttp Clie nt ( " h t t p : / / l o c a l h o s t : 8 0 8 0 / n c s " ) ;

TTransport
T P r o t o c o l p r o t o c o l = new T B i n a r y P r o t o c o l ( t r a n s p o r t ) ;
N c s S e r v i c e . C l i e n t

c l i e n t = new N c s S e r v i c e . C l i e n t ( p r o t o c o l ) ;

1 @Test ( t i m e o u t = 6 0 0 0 0 )
2 public void t e s t ( ) throws E x c e p t i o n {
3
4
5
6
7
8
9
10
11
12
13
14
15
16 }

i n t a r g 0 = 5 7 7 ;
double a r g 1 = 0 . 2 0 4 9 1 3 5 4 5 7 5 8 5 6 1 5 8 ;
res_1 = c l i e n t . b e s s j ( arg0 , a r g 1 ) ;

}
a s s e r t E q u a l s ( 0 ,
a s s e r t T r u e ( numbersMatch ( 0 . 0 ,

o r g . t h r i f t . n c s . c l i e n t . Dto res_1 = n u l l ;
{

res_1 . r e s u l t A s I n t ) ;

res_1 . r e s u l t A s D o u b l e ) ) ;

(c) An example of a Junit test for NcsService implemented with Thrift (see Figure 2b )
* note that complete versions of the schema and its implementation with Thrift framework can be found at
https://github.com/EMResearch/EMB/tree/master/jdk_8_maven/cs/rpc/thrift/artificial/thrift-ncs

Figure 2: An example of RPC schema, its automatically generated source code with Thrift framework
and a test for the RPC-based service

5

r p c b e s s j ( B e s s j R e q u e s t )
. . .

1 s y n t a x = " p r o t o 3 " ;
2
3 o p t i o n j a v a _ m u l t i p l e _ f i l e s = true ;
4 o p t i o n java_package = " o r g . g r p c . n c s . g e n e r a t e d " ;
5
6 s e r v i c e N c s S e r v i c e {
7
8
9
10 }
11
12 message B e s s j R e q u e s t {
13
14
15 }
16
17 message DtoResponse {
18
19
20 }

i n t 3 2 r e s u l t A s I n t = 1 ;
double r e s u l t A s D o u b l e = 2 ;

i n t 3 2 n = 1 ;
double x = 2 ;

r e t u r n s

( DtoResponse ) {}

(a) Snippet of an example of RPC schema (i.e., ncs.proto) specified with gRPC

1 package o r g . g r p c . n c s . g e n e r a t e d ;
2
3 import s t a t i c i o . g r p c . M e t h o d D e s c r i p t o r . generateFullMethodName ;
4
5 @javax . a n n o t a t i o n . Generated (
6 v a l u e = " by gRPC p r o t o c o m p i l e r
7 comments = " S o u r c e : n c s . p r o t o " )
8 @io . g r p c . s t u b . a n n o t a t i o n s . GrpcGenerated
9 public f i n a l c l a s s N c s S e r v i c e G r p c {

( v e r s i o n 1 . 4 1 . 0 ) " ,

10
11
12
13
14
15
16
17

18
19
20
21
22

23
24

25
26
27
28
29
30
31 }

public s t a t i c f i n a l S t r i n g SERVICE_NAME = " N c s S e r v i c e " ;

public s t a t i c abstract c l a s s N c s S e r v i c e I m p l B a s e implements i o . g r p c . B i n d a b l e S e r v i c e {

public void b e s s j ( o r g . g r p c . n c s . g e n e r a t e d . B e s s j R e q u e s t
i o . g r p c . s t u b . StreamObserver<o r g . g r p c . n c s . g e n e r a t e d . DtoResponse> r e s p o n s e O b s e r v e r ) {

r e q u e s t ,

i o . g r p c . s t u b . S e r v e r C a l l s . asyncUnimplementedUnaryCall ( g e t B e s s j M e t h o d ( ) ,

r e s p o n s e O b s e r v e r )

;
}
. . .

}

public s t a t i c f i n a l c l a s s N c s S e r v i c e B l o c k i n g S t u b extends i o . g r p c . s t u b . A b s t r a c t B l o c k i n g S t u b <

N c s S e r v i c e B l o c k i n g S t u b > {

public o r g . g r p c . n c s . g e n e r a t e d . DtoResponse b e s s j ( o r g . g r p c . n c s . g e n e r a t e d . B e s s j R e q u e s t
r e q u e s t ) {

return i o . g r p c . s t u b . C l i e n t C a l l s . b l o c k i n g U n a r y C a l l (
g e t C h a n n e l ( ) , g e t B e s s j M e t h o d ( ) , g e t C a l l O p t i o n s ( ) ,

r e q u e s t ) ;

}
. . .

}
. . .

(b) Snippet of an example of an interface generated with gRPC framework

* note that complete versions of the schema and its implementation with gRPC framework can be found at
https://github.com/EMResearch/EMB/tree/master/jdk_8_maven/cs/rpc/grpc/artificial/grpc-ncs

Figure 3: An example of RPC schema and its automatically generated source code with gRPC
framework

6

Figure 4: Architecture of EvoMaster

for schema definitions and message encoding has lead this protocol to lose most of its market share
in industry (i.e., apart from maintaining legacy systems, it is not used so much any more for new
projects).

In recent years, there has been a large interest in the research community in testing RESTful
APIs [32], which are arguably the most common type of web services. Several tools for fuzzing
RESTful APIs have been developed in the research community, like for example (in alphabetic order)
bBOXRT [43], EvoMaster [12], RESTest [48], RestCT [59], RESTler [27] and RestTest-
Gen [57]. Another recently introduced type of web services is GraphQL [3], which is getting momentum
in industry. However, there is only little work in academia on the automated testing of this kind of
web services [56, 41, 30].

The automated testing of different kinds of web services (e.g., modern-RPC, SOAP, REST and
GraphQL), share some common challenges (e.g., how to define white-box heuristics on the source
code of the SUT, and how to deal with databases and interactions with external services). However,
there are as well specific research challenges for each type of web service, as we will show later in the
paper. A fuzzer for SOAP or REST APIs would not be directly applicable to a RPC web service, and
vice-versa.

Given a client library for a RPC-based API, a unit test generator could be used directly on it, like
for example the popular EvoSuite [33] for Java classes. This might work if the SUT and the client
library are run in the same JVM. However, all the issues when dealing with system testing of web
services would still be there, e.g., how to deal with databases and what to use as test oracle. Also,
likely such unit testing tools would need some modifications (e.g., to collect coverage from all the
classes and not just the RPC-client one). Therefore, how a unit test generator could be adapted and
fare in such a system testing scenario is an open research question.

4 Fuzzing RPC-based APIs

Our novel approach is built on top of EvoMaster, which we extended. Figure 5 represents an overview
of our novel approach. In order to fuzz RPC-based APIs, we purpose RPC Schema specification
(Section 4.1) which formulates necessary info to allow the execution of RPC function calls and the
analysis of execution result. In addition, with the specification, as shown in the figure, the approach is
composed of six steps distributed between the driver and core of EvoMaster, plus initial settings
manually provided by the user, for enabling automated fuzzing of RPC-based APIs with search
techniques. We briefly summarize these steps, where their details will be provided in the rest of this
section.

To employ EvoMaster, a SUTdriver is required to be specified for implementing how to
start/stop/reset the SUT (recall Section 2.2). In the context of RPC-based APIs testing, in the
SUTdriver, we further need the user to specify (1) RPCInterfaces: what interfaces are defining the API
in the SUT with their class names and (2) RPC clients: the corresponding client instances used to make
RPC calls during test generation (Step 0). Then, with the specified interface info, RPC Schema Parser
will extract and identify the API schema based on proposed RPC schema specification, in order to
access the RPC functions (Step 1). At the core side, the extracted schemas will be further reformulated
(Step 2) to be as components (i.e., RPC Actions and Genes) of the search for producing tests (Step 3).
In our approach, a generated test is evaluated by its execution on the SUT (Step 4) performed on the
driver side. Then, responses, SBST heuristics (e.g., code coverage with code instrumentation) and

7

Figure 5: Overview of the approach built with EvoMaster

identified potential faults resulted in the execution will be returned to the Fitness Function (Step 5)
for calculating the fitness value of the executed test. Producing and evaluating tests are performed
iteratively (i.e., Steps 3-5), within a given search budget. At the end of the search, a set of the best (in
terms of code coverage and fault detection) tests for the RPC-based SUT will be outputted (Step 6)
with a given format (e.g., JUnit 5).

4.1 RPC Schema Specification

Nowadays, there exist various RPC frameworks for building modern RPC-based APIs, e.g., Thrift [10],
gRPC [4], Dubbo [1] and SOFARPC [7]. As discussed in Section 2.1, most of the techniques would result
in RPCInterfaces (e.g., implemented as interface or abstract class) in their API implementations
representing how the services can be accessed, together with a client-stub to make the actual RPC
calls. Considering all the possible types of communication protocols supported by the different
RPC frameworks, calling a RPC API directly from a fuzzer would be a major technical endeavor.
Furthermore, it would require to support the different schema languages for each framework, like for
example .thrift (see Figure 2a) and .proto (see Figure 3a) formats, and the schema file might not
be available, such as the APIs with SOFARPC and Dubbo.

In order to enable automating testing of RPC-based APIs in a more generic way, we propose a
schema specification specific to RPC domain that formulates main concepts for facilitating invocations
of RPC function calls (Section 4.1.1) and result analysis (Section 4.1.2). Such a specification could be
extracted based on RPCInterfaces which are developed with various RPC frameworks, then employ
client-libraries of the APIs to make the RPC calls. Figure 6 represents Data Transfer Objects (DTO)s
defined in our RPC schema specification with UML class diagram.

4.1.1 RPCInterface

To extract info for enabling invocations of RPC function calls, there exist five main concepts to define
RPCInterfaces (denoted as classes with white background in Figure 6):

• RPCInterfaceSchemaDto: it represents the RPCInterface, such as the Interface with Thrift
(see Figure 2b) and abstract class with gRPC (see Figure 3b). A RPCInterfaceSchemaDto
comprises one or more RPCActionDto (see 1..* functions), a set of functions for authentication

8

handling (see * authFunctions) and a set of specifications of data types (see * types). For instance,
NcsService.Iface interface has a bessj function and employs Dto data structure (as shown in
Figure 2b). Note that a RPC-based API might exist multiple interfaces as industrial APIs which
we studied in this paper.

• RPCActionDto: it capture info to make a RPC function call, i.e., input parameters if exist (see *
requestParams) and additional authentication setup (see 0..1 authSetup). Each RPCActionDto
also has interfaceId, clientInfo and actionName properties to identify the RPC function to call.
In addition, we identify a property isAuthorized representing whether the RPCActionDto is
restricted with authentications in its implementation.

• ParamDto: it is used to describe values of input parameters and return. A ParamDto links to
an explicit datatype (see type) and might be composed of a set of ParamDtos for representing
complex data types, such as object, collection and map (see * innerContent). The ParamDto
might be specified with a default value (see 0..1 default), e.g., a field in a DTO can be assigned
with a default value. In addition, we define stringValue to assign a value for the input parameter
or represent the actual value of the return. Note that stringValue is applicable only if there is no
any internal elements. To construct constraints of the input parameters if exist, we define a set
of properties in ParamDto as:

– isNullable represents whether the parameter is nullable to make the call.

– isMutable indicates whether the parameter is mutable. A value of the property is derived
based on whether the parameter is assigned with a fixed value. For instance, a parameter
must be true if it is specified with @AssertTrue, thus the parameter is considered as
immutable.

– minSize and maxSize represents boundaries in size if specified. The constraint could be

applicable to data types, i.e., collection, map, array and char sequence (e.g., string).

– minValue and minInclusive are used to represent a minimum value, and a value of the

parameter must be higher than or equal to the minimum.

– maxValue and maxInclusive are used to represent a maximum value, and a value of the

parameter must be lower than or equal to the maximum.

– precision and scale capture constraints for numeric values regarding its precision and scale

(e.g., number of digits in their decimal part).

– pattern represents a regular expression that a string value must match.

Such captured constraints could contribute to test data generation for fuzzing Web APIs, by
sampling values within the boundaries of these constraints. Values of all of the constraint
properties could be derived automatically based on the PRCInterface, which is explained in the
RPC schema extraction (see Section 4.2).

• TypeDto and RPCSupportedDataType identify the datatype info of the ParamDto. A list of data
types we support is defined as an enumeration RPCSupportedDataType which covers the most
commonly used data types, i.e., array, byte buffer, date, enumeration, list, map, set, string, integer,
boolean,double, float,long, character, byte, short, big integer, big decimal, and any customized
DTO object, for enabling the fuzzing of RPC-based APIs. In TypeDto, it can be specified with
an example (see 0..1 example) for representing a generic type of collection, array and map. Note
that this list of supported data types is not meant to be complete for all RPC frameworks. But,
if needed, it can be extended.

4.1.2 RPC Execution Result Analysis

By using client to invoke RPC function call, a result received at the client side could be a return
value as defined or an exception thrown from the API. To enable the result analysis, we proposed
five main concepts denoted as classes with gray background in Figure 6, i.e., ActionResponseDto,

9

Figure 6: Data Transfer Objects defined in our RPC schema specification

RPCExceptionInfoDto, RPCExceptionCategory, RPCExceptionType and CustomizedCallResultCode.
ActionResponseDto is a DTO which captures all info returned from a RPC function call, i.e., throw an
exception (see 0..1 exceptionInfoDto) or return a value as specified (see 0..1 response).

Regarding exception, handling the exception info for RPC functions is crucial for testing purposes,
e.g., to be able to use automated oracles to identify faults in the SUT. To analyze an exception, in
our proposed schema, we define RPCExceptionInfoDto which captures exceptionName, exception-
Message, type and exceptionDto, which is an optional DTO representing possible additional info
for customized exceptions (e.g., the exceptions declared with the keyword throws in Java). In ad-
dition, when invoking RPC function calls with clients which could be proxy clients, an exception
caught at the client side might be wrapped, such as UndeclaredThrowableException4 in Java. To
get the exact exception info, we further extract and analyze the actual exception (e.g., with cause
of UndeclaredThrowableException) as RPCExceptionInfoDto. We also perform further exception
analysis on UndeclaredThrowableException, as it was needed for our industrial case study, and the
property isCasueOfUndeclaredThrowableException represents whether such a wrapped exception is
thrown from the SUT. Note that the actual exception analysis could be extended in future when
needed.

Beside exceptionName and exceptionMessage, to better identify exceptions in the context of
RPC-based APIs, based on domain knowledge, we classify exceptions into four categories as RPCExcep-
tionCategory: APPLICATION (e.g., internal server errors), TRANSPORT (e.g., connection timeouts),
USER (e.g., sending invalid data), and UNCLASSIFIED. Different RPC frameworks can define their own
exceptions for handling various situations for RPC (e.g., type of TApplicationException [9] defined
in TException for Thrift, status [8] defined in StatusException and StatusRuntimeException for
gRPC). To cover such knowledge captured in various RPC frameworks, we define RPCExceptionType,
and each of the type should belong to a category in RPCExceptionCategory. The RPCException-
Type now provides a full support for analyzing exceptions in the Thrift framework, which covers
the complete 24 exception types from TApplicationException (refer to APPLICATION category),
TProtocolException (refer to USER category), and TTransportException (refer to TRANSPORT
category). In addition, we define two generic exception types, i.e., CUSTOMIZED_EXCEPTION
representing a declared exception (e.g., throws keyword in Java), UNEXPECTED_EXCEPTION
representing an exception which is not declared in the function and does not belong to any other iden-
tified types (e.g., RuntimeException in Java). The generic exception types link to UNCLASSIFIED
category that covers the cases whereby the exception type is unspecified or its identification is not
supported yet for linking it to a specific RPC exception (like Thrift).

With HTTP, a result for a request could be identified based on status code in its response, e.g.,

4https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/InvocationHandler.html

10

(cid:0)✁✂✄☎✆✝✞✟✠✡✝☛✡☞✝✌✠✍✆✎✏✑✒✓✔✕✖✗✓✘✙✚✛✒✔✏✑✜✗✢✏✓✑✒✘✑✕✣✚✛✒✔✏✑✜(cid:0)✁✂✤✡✆✥✎☎✍✆✎✏✑✒✓✔✕✖✗✓✘✙✚✛✒✔✏✑✜✗✢✏✓✑✒✘✑✕✣✚✛✒✔✏✑✜✖✗✒✏✣✑✦✖✧✓✚✛✒✔✏✑✜✏★✩✪✒✫✣✔✏✬✓✙✚✭✣✣✢✓✖✑✁✠✞✠✌✍✆✎✑✖✧✓✚✛✒✔✏✑✜✏★✦✪✢✢✖✭✢✓✚✭✣✣✢✓✖✑✧✏✑✛✏✬✓✚✮✣✑✜✧✖✯✛✏✬✓✚✮✣✑✜✧✏✑✰✖✢✪✓✚✛✒✔✏✑✜✧✖✯✰✖✢✪✓✚✛✒✔✏✑✜✱✔✓✗✏★✏✣✑✚✘✑✒★✗✖✢✓✚✘✑✒✧✏✑✘✑✗✢✪★✏✲✓✚✭✣✣✢✓✖✑✧✖✯✘✑✗✢✪★✏✲✓✚✭✣✣✢✓✖✑✱✖✒✒✓✔✑✚✛✒✔✏✑✜✏★✳✪✒✖✭✢✓✚✭✣✣✢✓✖✑★✒✔✏✑✜✰✖✢✪✓✚✛✒✔✏✑✜✴✵✶✝✍✆✎✕✪✢✢✷✸✱✓✦✖✧✓✚✛✒✔✏✑✜✕✪✢✢✷✸✱✓✦✖✧✓✹✏✒✫✺✓✑✓✔✏✗✷✸✱✓✚✛✒✔✏✑✜✒✸✱✓✚✻✼✽✛✪✱✱✣✔✒✓✙✾✖✒✖✷✸✱✓✕✏✯✓✙✘✒✓✧★✚✿❀❁✛✒✔✏✑✜❂❃❃❄❅❆❇❈❉❊❋❇●❄❍❆❉■❏❆❇❈❉❊❋❇●❄❉❑▲▼●❄◆▼❖❆▼●❉P❍◆❍◗●❘❃❃❂❍❆❉■❙▼❉❆▲❄❊❇❇▼◆❚❋❇❉▼❇❉❯❯✝☎❱✌✝✞✠✆✥✎☎❲❲(cid:0)✁✂☛❱✶✶✎✞✆✝❳✍✠✆✠✴✵✶✝✼❨✘✦✷✼❨❩❬✷❭✼❨✛❪❫✻✷✼❨✮❫✦✺✼❨❴✮❫✩✷✼❨✾❫❵❩✮❭✼❨❩❫❫✮❭✩✦✼❨✽❪✩✻✘✦✷❩❬✷❭✛❪❫✻✷✮❫✦✺❴✮❫✩✷✾❫❵❩✮❭❩❫❫✮❭✩✦✽❪✩✻✛✷✻✘✦✺❭✦❵✳✩✻✻✩❬✮✘✛✷✛❭✷✳✩✼❵✷✘✮❨✾✩✷❭✼✩✘✻❩❬✷❭❩❵❴❴❭✻✽❵✛✷❫✳❨❫❩❛❭✽✷✽❵✛✷❫✳❨✽❬✽✮❭❨❫❩❛❭✽✷❩✘✺✘✦✷❭✺❭✻❩✘✺✾❭✽✘✳✩✮❯❯✝☎❱✌✝✞✠✆✥✎☎❲❲(cid:0)✁✂❜❝✡✝✶✆✥✎☎✴✵✶✝✽❵✛✷❫✳✘❞❭✾❨❭❡✽❭✼✷✘❫✦❵✦❭❡✼❭✽✷❭✾❨❭❡✽❭✼✷✘❫✦✩✼✼❨❩✩✾❨✛❭❢❵❭✦✽❭❨✘✾✩✼✼❨✘✦✷❭✻✦✩✮❨❭✻✻❫✻✩✼✼❨✘✦✰✩✮✘✾❨✳❭✛✛✩✺❭❨✷❬✼❭✩✼✼❨✘✦✰✩✮✘✾❨✼✻❫✷❫✽❫✮✩✼✼❨✘✦✰✩✮✘✾❨✷✻✩✦✛❴❫✻✳✩✼✼❨✳✘✛✛✘✦✺❨✻❭✛❵✮✷✩✼✼❨✼✻❫✷❫✽❫✮❨❭✻✻❫✻✩✼✼❨❵✦❣✦❫✹✦✩✼✼❨❵✦❣✦❫✹✦❨✳❭✷❪❫✾✩✼✼❨❵✦✛❵✼✼❫✻✷❭✾❨✽✮✘❭✦✷❨✷❬✼❭✩✼✼❨✹✻❫✦✺❨✳❭✷❪❫✾❨✦✩✳❭✷✻✩✦✛❨✩✮✻❭✩✾❬❨❫✼❭✦✷✻✩✦✛❨✽❫✻✻❵✼✷❭✾❨✾✩✷✩✷✻✩✦✛❨❭✦✾❨❫❴❨❴✘✮❭✷✻✩✦✛❨✦❫✷❨❫✼❭✦✷✻✩✦✛❨✷✘✳❭✾❨❫❵✷✷✻✩✦✛❨❵✦❣✦❫✹✦✼✻❫✷❫❨❩✩✾❨✰❭✻✛✘❫✦✼✻❫✷❫❨✾❭✼✷❪❨✮✘✳✘✷✼✻❫✷❫❨✘✦✰✩✮✘✾❨✾✩✷✩✼✻❫✷❫❨✦❭✺✩✷✘✰❭❨✛✘❞❭✼✻❫✷❫❨✦❫✷❨✘✳✼✮❭✳❭✦✷❭✾✼✻❫✷❫❨✛✘❞❭❨✮✘✳✘✷✼✻❫✷❫❨❵✦❣✦❫✹✦❯❯✝☎❱✌✝✞✠✆✥✎☎❲❲(cid:0)✁✂❜❝✡✝✶✆✥✎☎✂✠✆✝❤✎✞✵✩✼✼✮✘✽✩✷✘❫✦✷✻✩✦✛✼❫✻✷❵✛❭✻❵✦✽✮✩✛✛✘❴✘❭✾(cid:0)✁✂❜❝✡✝✶✆✥✎☎✄☎✟✎✍✆✎✓✯✗✓✱✒✏✣✑✦✖✧✓✚✛✒✔✏✑✜✓✯✗✓✱✒✏✣✑✳✓★★✖✜✓✚✛✒✔✏✑✜✒✸✱✓✚✻✼✽❭✯✗✓✱✒✏✣✑✷✸✱✓✏★✽✖✪★✓❫✕❵✑✙✓✗✢✖✔✓✙✷✫✔✣✐✖✭✢✓✚✭✣✣✢✓✖✑❉❑▲▼❘❃❃❂▼❥❍◗▲❦▼❘❃❃❂◆▼●▲❋❇●▼✤✡✆✥✎☎(cid:0)✝❧✶✎☎❧✝✍✆✎♠★✣✑✻✓★✱✣✑★✓✚✛✒✔✏✑✜✗✪★✒✣✧✏✬✓✙✽✖✢✢✻✓★✪✢✒✽✣✙✓✚✽✪★✒✣✧✏✬✓✙✽✖✢✢✻✓★✪✢✒✽✣✙✓❯❯✝☎❱✌✝✞✠✆✥✎☎❲❲✂❱❧✆✎✌✥♥✝❳✂✠♦♦(cid:0)✝❧❱♦✆✂✎❳✝✛❵✽✽❭✛✛✛❭✻✰✘✽❭❨❭✻✻❫✻❫✷❪❭✻❨❭✻✻❫✻❘❃❃❂▼❥❈▼▲❉❊❋❇♣❉❋❈❍❉▼q❋◆❑❘❃❃❂r▼❅❍❆❦❉❘❃❃❂▼❥❈▼▲❉❊❋❇s❇❅❋♣❉❋2xx indicates a success, 4xx indicates a client error and 5xx indicates a server error. Such a standard
is useful in developing automated testing approaches, e.g., reward requests with 500 status code (for
finding potential faults in the SUT) and 2xx status code (for covering a successful request). However, in
the context of RPC, there does not exist such standard, and a result (e.g., success or failure) of the call
cannot be directly determinate based on the return value if there is no exception thrown. Therefore, we
propose CustomizedCallResultCode which defines three categories (i.e., SUCCESS, SERVICE_ERROR
and OTHER_ERROR) to better identify a return value of a RPC function call. Identifying the return
value could vary from SUTs to SUTs, and from companies to companies. So, we expose an interface to
allow a customization of the identification (see Section 4.2).

Thus, with our RPC result analysis specification as shown in Figure 6, each result by a RPC
function call would be constructed as an instance of ActionResponseDto. If there is an exception
thrown, RPCExceptionInfoDto could be instantiated to describe info of exception in detail, such as
exception class, message, type and category. If a value is returned as defined, the value could be
represented as a JSON object (if could) and an instance of ParamDto, and the result could be further
identified with CustomizedCallResultCode.

4.2 RPC Schema Extraction and Execution Support

As a white-box fuzzer, beside source code of SUT, a SUTdriver is the only input which EvoMaster
needs a user to specify, then the Sutdriver is employed at the driver side for, e.g., starting/stopping/re-
setting the SUT. In the context of RPC-based API fuzzing, we further need the user to provide info of
RPCInterfaces and corresponding client instances for extracting the API schema and accessing the SUT.
As shown in Figure 5, in the driver, with the provided SUTdriver (Step 0), we developed a RPC Schema
Parser, by directly extracting the interface definitions (which do represent the API schema) from the
source code using reflection technique, such as Java Reflection5. Thus, with any RPC framework,
if the available RPC functions are defined as an interface/abstract class (which is usually the
case), our approach could be applicable. The extracted information is further formulated as a generic
RPC Schema (see Section 4.1), i.e., a RPCInterface will be formulated as a RPCInterfaceSchemaDto
which contains specifications to invoke RPC function calls (i.e., RPCActionDto (Step 1 → Step 2)). In
addition, we developed RPC Test Client which allows to make a RPC function call against the SUT
with RPCActionDto, then return ActionResponseDto (Step 5 ↔ Step 4) using specified RPC client
instances. The driver is implemented as a service using REST API, and the two components (i.e.,
RPC Schema Parser and RPC Test Client) are exposed as two HTTP endpoints, i.e., /infoSUT for
extracting RPC API schema and /newAction for executing RPC function calls. Thus, with a provided
SUTdriver, our driver employed with proposed RPC Schema would allow a unique interface of our
tool to support invocations of RPC functions and result analysis. This is an essential prerequisite for
fuzzing RPC-based API.

Note that instead of enabling RPC function execution at the driver side, an alternative approach
would have been to include the two components and RPC API client-library directly into core process,
which might be more efficient (as calls from the core do not need to go through the driver with HTTP
requests). But that would introduce a lot of usability issues to configure it up (e.g., how to dynamically
load a library at runtime, and how to deal with different JVM versions and different programming
languages). When introducing a novel approach, it is important to take into account how complex
it is to set it up by practitioners. For this, industry collaborations, where actual engineers use these
techniques on their systems (as we do for this paper), are paramount.

4.2.1 SUT driver

Figure 7 represents an example of a SUTdriver for manipulating the SUT and specifying info of a
RPC-based API. For instance, a startSut method at lines 9--24 represent how to start a RPC-based
SUT which is implemented with the Thrift framework and SpringBoot. The method also instantiates
needed clients to access the SUT after it starts, see lines 18--20. To provide info specific to RPC problem,
lines 29--31 specify the RPCInterface (i.e., NcsService.Iface see Figure 2b) and corresponding client

5https://www.oracle.com/technical-resources/articles/java/javareflection.html

11

instance. Note that the info is specified with a map since an API might have multiple RPCInterfaces
as we observed in our industrial case studies.

In addition, each framework or each company might define their own rules to represent results.
For instance, we found that, in our industrial case study, in most cases a failed function call would not
result in any exception thrown to avoid propagation of exceptions in the distributed system, since the
services are connected with each other. Thus, inside the response, our industrial partner has its own
customized specification to reflect results of RPC function calls that are linked to their business logic.
Without a thrown exception, a response representing an error might be falsely identified as a success
if no further info is provided. To address this concrete issue in industrial APIs, in our approach, we
provide an extensible method (i.e., getCustomizedValueInRequests at line 35) to enable customized
categorization of responses with the three levels as CustomizedCallResultCode defined in our RPC
schema (Section 4.1). By extending the method, the user could directly link their own rules into
our testing context. Note that such setup can be easily reused by multiple SUTs if they use same
customized specification (as it was for all web services developed by our industrial partner).

As an enterprise system, authentication is typically required to be handled. However, there are
many different ways to implement an authentication system in a RPC API, as it is usually not
supported natively (at least not in Thrift). For this paper, we are mainly supporting the authentication
systems used by our industrial partner. Authentication tokens need to be sent as a field in payloads of
the messages (similarly as HTTP authentication headers in RESTful APIs). An authentication token
can be either static (i.e., pre-fixed) or dynamic. This latter requires to get the token from an endpoint
(e.g., a login RPC endpoint where valid username/password info must be provided), and then add it
to all following RPC calls. In our implementation, we support both approaches, which needs to be
configured in the driver, i.e., by extending the method getInfoForAuthentication at line 38 and
getCustomizedValueInRequests at line 41 as shown in Figure 7. To serve a more fine-tuned setup
for authentication, we enable options to specify (1) if either the authentication is applied for all API
functions; or (2) specific only to some functions in that SUT, that could be filtered by names or
by special annotations applied on these functions. More detail about how to configure the option
could be found in two DTOs, i.e., JsonAuthRPCEndpointDto and CustomizedRequestValueDto, in
our implementation1.

4.2.2 RPC Schema Parser

Regarding the extraction of RPC interface definitions, currently, we target JVM RPC-based APIs
using Java Reflection. As examples shown in Figures 2 and 3, a client-stub RPCInterface is composed
of a set of available RPC functions to be extracted. Each operation in the interface depicts a RPC
function to be called in this service. Then, with reflection, for each interface, we identify all such
public methods, and then further extract info on their input parameters, return type and declared
exception types.

Regarding datatype, as currently targeting JVM projects, we have supported the most com-
monly used data types, i.e., Array, ByteBuffer, Date, Enum, List, Map, Set, String, Integer, int,
Boolean, bool, Double, double, Float, float, Long, long Character, char Byte, byte, Short,
short, BigInteger, BigDecimal, and any customized DTO object. For the handling of generics, we
support their instantiation for any of these common data types. Note that all of the datatype could be
mapped to an item defined in RPCSupportedDataType.

Regarding the parameter, besides its datatype, we also need to extract info, such as accessibility
and constraints if they exist. Extracting accessibility is needed for the parameter typed with DTO,
then its fields might be publicly accessible or not, i.e., declared as public or not in Java. If the filed is
not publicly accessible, there is a need to further extract its existing getter and setter that would be
used in assertion generation (with getter) and parameter construction (with setter) in our context.
Note that the accessibility info for each parameter is maintained inside the RPC Test Client that does
not expose in DTO, since the user does not need to care about how to construct the data instance for
the parameter and assertion generation. More detail about how the info is constructed can be found in
the class AccessibleSchema1.

Regarding the constraints, a parameter might be specified with constraints in its implementation.

12

} ) ;

try {

c l i e n t ;

t r a n s p o r t ;

"−−s e r v e r . p o r t=0"

public S t r i n g s t a r t S u t ( ) {

} catch ( T T r a n s p o r t E x c e p t i o n e ) {}

S t r i n g u r l = " h t t p : / / l o c a l h o s t : "+g e t S u t P o r t ( )+" / n c s " ;

c t x = S p r i n g A p p l i c a t i o n . run ( N c s A p p l i c a t i o n . c l a s s , new S t r i n g [ ] {

t r a n s p o r t = new THttpClie nt ( u r l ) ;
p r o t o c o l = new T B i n a r y P r o t o c o l ( t r a n s p o r t ) ;
c l i e n t = new N c s S e r v i c e . C l i e n t ( p r o t o c o l ) ;

private C o n f i g u r a b l e A p p l i c a t i o n C o n t e x t c t x ;
private N c s S e r v i c e . C l i e n t
private TTransport
private T P r o t o c o l p r o t o c o l ;

1 public c l a s s EmbeddedEvoMasterController extends EmbeddedSutController {
2
3
4
5
6
7
8 @Override
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26 @Override
27
28
29
30
31
32
33
34 @Override
35
36
37 @Override
38
39
40 @Override
41
42
43 }

return new RPCProblem (new HashMap<S t r i n g , Object >() {{
put ( N c s S e r v i c e . I f a c e . c l a s s . getName ( ) , c l i e n t ) ;

public C u s t o m i z e d C a l l R e s u l t C o d e c a t e g o r i z e B a s e d O n R e s p o n s e ( O b j e c t

public P r o b l e m I n f o g e t P r o b l e m I n f o ( ) {

return u r l ;

} } ) ;

}

}

public L i s t <A u t h e n t i c a t i o n D t o > g e t I n f o F o r A u t h e n t i c a t i o n ( ) { return n u l l ; }

public L i s t <CustomizedRequestValueDto> g e t C u s t o m i z e d V a l u e I n R e q u e s t s ( ) { return n u l l ; }

r e s p o n s e ) { return n u l l ; }

Figure 7: Snippet code of a driver for NcsService (see Figure 2)

13

For example, an integer representing the day of the month could be constrained between the values 1
and 31. To make RPC function calls that do not fail due to input validation, we need to handle such
constraints when generating input data for the call. Therefore, for each parameter, with the proposed
schema, we define possible constraints as properties of ParamDto (see Section 4.1.1 and Figure 6).
With the extraction, we further identify the properties based on the data types. For instance, all
parameters are defined with a property named isNullable representing whether a parameter object can
be null (the value of this property for all primitive types is always false). Parameters with numeric data
types are defined with min and max properties. For parameters representing collections (e.g., maps
and lists) and string types, properties for constraining their size/length are defined, i.e., minSize and
maxSize. For strings, we define pattern for supporting a constraint specified with regular expressions.
If a string has to represent a numeric value, we use minValue and maxValue for supporting a possible
range constraint for it.

To identify constraints defined in the interface definitions (typically with annotations), we enable
constraint extraction on javax.validation.contraints [6] which is the standard library for defining
built-in constraints for Java objects. We support 16 commonly used constraints, i.e., AssertFalse,
AssertTrue, DecimalMax, DecimalMin, Digits, Max, Min, Negative, NegativeOrZero, NotBlank,
NotEmpty, NotNull, Pattern, Positive, PositiveOrZero, and Size. Besides standard javax anno-
tations, constraints could be defined in other ways as well. For instance, in Thrift, whether a field is
required is represented by a requirementType property of the FieldMetaData class. Thus, in order
to deal with constraints in the Thrift framework, we further extract and analyze the metaDataMap
object in the interface for obtaining such constraints.

In addition, since there is no general standard to restrict such interface implementations (as long
as it compiles), the method and the data type might use Java Generics (as we found in our industrial
case study). Therefore, we further handle such generic types when processing RPC function extraction,
e.g., analyze getParameterizedType for each parameter.

With the RPC Schema Parser, it is capable of formulating each RPCInterface as RPCInter-

faceSchemaDto shown in Figure 6.

4.2.3 RPC Test Client

This component mainly enables invocation of RPC function call with RPCActionDto, analysis of
the response or exception after the invocation, then outputting ActionResponseDto. With the
RPCActionDto, we could know what interface the action belongs to and what parameters are needed
to construct, then the invocation is made based on the provided RPC client instance. Result analysis is
performed based on concepts we discussed in Section 4.1.2. For instance, now we support extract name
and message info from all exceptions which inherit from java.lang.Exception. Its explicit type could
be identified if it belongs to the Thrift framework, i.e., org.apache.thrift.TException can be found
in the client library, the class of the thrown exception inherits from TException, then extract its super
classes to recognize the exception category (e.g., APPLICATION ) under RPCExceptionCategory and
its type property to identify a type (e.g., APP_INTERNAL_ERROR) under RPCExceptionType.
If the exception is not from Thrift framework, its explicit class would be extracted, then set it with
CUSTOMIZED_EXCEPTION and UNEXPECTED_EXCEPTION based on whether the exception
is a part of throw clause declared in the RPC function. Note that the result analysis needs to be
extended if one wants to support the other RPC frameworks, such as gRPC. However, exceptions in
the context of RPC domain have been formulated in our schema. The additional work would be only
technical details which we need to cope with, e.g., add additional types if they are not covered yet,
then extract the specific info to identify the type.

More technical details on this implementation (e.g., how the parameters could be constructed for
each data type, how to automatically recognize input parameters with customized info, and how to
extract data and type from a Java object) can be found in our open source repository1.

14

4.3 Test Reformulation

With the outputted RPCSchemaDto, we need to reformulate it as search problem for allowing to
solve it with search algorithms. In our context, a test could be reformulated as an individual which is
composed of a sequence of RPC function calls. Each function call is formulated as RPCCallAction,
which comprises input parameters (if any), optional authentication info, and a response (if declared).
Regarding the input parameters, we could re-use existing Genes defined in EvoMaster for supporting
REST API testing,

• Straightforward mapping: ArrayGene for Array, Set and List; BooleanGene for Boolean and
bool; DoubleGene for Double and double; LongGene for Long and long; FloatGene for Float
and float; EnumGene for Enum; DateGene, DateTimeGene and TimeGene for DateTime;

• MapGene for Map. Note that the original version of MapGene only supports key with string type.
However, other types such as enum and integer are quite common in RPC-based APIs. Therefore,
we further extended MapGene for enabling key to be specified with IntegerGene, StringGene,
LongGene and EnumGene.

• IntegerGene for Integer, int, Short, short, Byte and byte (various types here are distinguished
by min value and max value, e.g., max value is configured as 127 for Byte by default if it is not
constrained);

• StringGene for Character, char, String and ByteBuffer (various types are distinguished by min
and max length, e.g., max length is configured as 1 for char by default if it is not constrained);

• RegexGene for a pattern specified in String parameter;

• ObjectGene for representing customized class object;

• CycleObjectGene for a field in the customized class object that leads to a cycle;

• Optional is for handling any parameter whose isNullable property is true.

In addition, we also purpose new genes, such as BigDecimalGene and BigIntegerGene for BigDecimal
and BigInteger respectively. In the original implementation of Genes in EvoMaster, constraints
for all types are not fully supported. Therefore, to fully support testing the RPC APIs in our case
study, we extended genes by enabling all constraints we defined in RPC schema, such as handling
precision and scale for numeric genes, and min and max size constraints for ArrayGene, MapGene and
StringGene.

To test a RPC-based API, the input parameters could be either automatically generated or manually
configured by the user (e.g., unlike header in HTTP request, in RPC function call, authentication info
could be specified as parts of input DTOs). The former one would be handled by search techniques in
our approach. The way to enable authentication as part of the input parameters can be identified as
the latter option, i.e., manually defined inputs. To allow further combinational handling with both
automatic and manual solutions, we decided to extend the test reformulation with a new gene, i.e.,
SeededGene, for handling manual inputs in a more generic way. A SeededGene, representing a gene
which has a set of candidates, is constructed with: (1) gene is the original genotype of the parameter
that could be mutated with the search; (2) seeded is an EnumGene with the same type as gene
presenting enumerated candidates; and (3) employSeeded is a boolean to indicate whether the original
gene or the seeded gene is used for the phenotype of SeededGene. Besides handling authentication info,
this kind of the gene also allows further seeding with existing data (if any) that would be in particular
useful in solving industrial problems.

With ActionResponseDto, considering how a RPC call is handled by the SUT and if there is any
exception, we classified it into seven kinds of execution results which would contribute to define search
heuristics for optimizing generated tests:

• (ER1) internal error: an exception which represents an internal error is thrown, e.g., TApplicatinException

with INTERNAL_ERROR type in Thrift.

15

• (ER2) user error: if an exception was thrown that can be traced to a failed input validation,

based on Thrift’s protocol errors.

• (ER3) transport error: an exception which represents transport errors is thrown.

• (ER4) other exception: other exception (e.g., other types of TApplication except internal error)

is thrown.

• (ER5) declared exception: an exception declared in the function is thrown.

• (ER6) unexpected exception: any other exception which is not declared in the function is thrown.

• (ER7) handled: a value is returned as declared without any exception thrown. If users specify
their result categorization, this label is further refined as one of success, service error and other
error.

4.4 Test Generation with Search

Our test reformulation enables its use in various search algorithms for supporting RPC-based API
fuzzing. In this work, we use MIO because it is the default in EvoMaster, as it achieved the overall
best results in an empirical study conducted by comparing it with various other algorithms [11] on the
fuzzing of RESTful APIs (recall Section 2.2). A comparison of different search algorithms for fuzzing
RPC-based APIs is not in the scope of this paper.

4.4.1 Search Operators

MIO is an evolutionary algorithm inspired by (1+1) EA that uses two search operators, for sampling
and mutation, respectively. We employ the same strategies as EvoMaster for RESTful API testing.
The sampling is implemented to produce a valid test by selecting a sequence of one or more available
actions at random. Values of Genes in these tests are initialized at random, within the constraints if
any (e.g., a ArrayGene will have n randomly generated elements based on its min and max length).
Authentication info, if any, is enabled with a given probability, i.e., 95%, which is the default one used
in EvoMaster. In addition, at the beginning of the sampling, we also prepare a set of adhoc tests
which cover all available RPC function calls and all authentication combinations, i.e., each test has an
action configured with and without authentication. In other words, the structure of the first k tests
are not sampled at random, where k = a × n, with n being the number of functions in the RPC API
and a being the different authentication settings.

Regarding the mutation operator, actions in a test can be added or removed for manipulating
the structure of the test, given a certain probability. To mutate values of Genes inside the tests, we
employ the default value mutation in EvoMaster, which has been integrated with taint analysis [23]
and adaptive hypermutation [61].

If the SUT interacts with a SQL database, genes to represent INSERTION operations will be

automatically added to the tests, in the same way as done in EvoMaster for RESTful APIs [21].

4.4.2 Test Execution

With our RPC handling support in the driver, we enable tests to be executed during the search. Then,
with the JVM instrumentation provided by EvoMaster, various SBST heuristics (e.g., code coverage,
branch distance and SQL queries heuristics) can be returned after the test is executed (see JVM
Instrumentation → Fitness Function in Figure 5), additionally to the RPC function call execution
results (i.e., ActionResponseDto). Regarding the authentication handling, dynamic tokens acquired
via a login endpoint can be regarded as an additional action which is needed to be invoked before the
other RPC functions can be called. This has been enabled automatically in our implementation.

16

Table 1: RPC testing targets with heuristics

# Execution Results

1 ER1: internal error
2 ER2: user error
3 ER4: other exception
4 ER5-6: unexpected/declared
5 ER7: handled

success
6
7
server error
8 other error

9
a null response is returned
10 a non-null response is returned

11 an empty collection is returned
12 a non-empty collection is returned

Handled

Error

isFault

0.5
0.1
0.5
0.5
1
Success
1
0.5
0.1
NotNull
0.5
1
NotEmpty
0.5
1

1
0.1
1
1
0.5
Fail
0.5
1
0.1
Null
1
0.5
Empty
1
0.5

Yes
-
-
Yes
-
isFault
-
Yes
-

-
-

-
-

4.4.3 Fitness Function

In the context of testing RPC-based APIs, besides using SBST heuristics at code coverage level, we
propose additional testing targets (with their heuristics) on the responses of the RPC calls for guiding
the test case generation, as shown in Table 1. Note that, with MIO, each testing target has a fitness
value between 0.0 and 1.0, where a higher value is better. A value with 1.0 means that the target is
covered, and any value more than 0.0 but less than 1.0 indicates that a testing target is reached but
not covered.

For each RPC function, we create two testing targets Handled and Error, representing that the
call is handled or in error, respectively, by the SUT. Based on the execution results we reformulated in
Section 4.3, we set a fitness value of Handled and Error testing targets as #1-#5 in Table 1, after the
call is executed. For instance, if the execution result is identified as handled, fitness values are set as
1.0 for Handled and 0.5 for Error (0.5 here represents the target is reached but not covered, which is
heuristically better than not calling the method at all). If any unexpected or declared exception is
thrown, the fitness values are set as 0.5 for Handled and 1.0 for Error. Since the exception type for
the unexpected/declared exceptions is unclear, the execution would be further rewarded with a testing
target for potential fault finding. If the exception type could be further identified, the fitness values
of Handled and Error would be handled as #1-#3. Note that, for these three types of categorized
exceptions, only internal error is rewarded for potential fault finding. Considering that the protocol
error typically refers to user errors, compared with other exceptions, it would be less important, then
it is set with lower fitness values (i.e., 0.1) for both Handled and Error. As transport error (ER3) is
usually due to issues in the testing environment (e.g., timeouts), then we do not reward such exception
with any fitness values.

In addition, if the handled results could be further categorized by user in terms of their business logic,
we propose two additional testing targets Success and Fail, representing whether the request succeeds
or fails to be performed on the SUT. Heuristics for handling the two targets regarding execution results
are defined in #6-#8. The strategy to decide the fitness values is similar with Handled and Error (e.g.,
server error is rewarded with potential finding and other error is recognized as less important) that
aims at covering both Success and Fail of RPC function actions in terms of business logic. Moreover,
to maximize response coverage, we also propose another four testing targets by considering whether
any null or non-null value is ever returned (i.e., #9-#10), and whether any empty or non-empty value
is ever returned for collection datatypes (i.e., #11-#12). Note that, although some of these fitness
values do not provide much gradient for the search (e.g., only two values like 0.5 and 1), they are still
useful. Test cases for reached but not covered targets (e.g., 0.5) are kept in the archive of MIO, and

17

will be still be sampled and mutated throughout the search.

4.5 Test Writer

In the same context of API testing, we could re-use parts of EvoMaster test writer to generate
the SUT test scaffolding. For example, we use the same initClass for setting up the necessary
testing environment (e.g., start SUT), tearDown for performing a cleanup after all tests are executed
(e.g., shutdown SUT), and initTest for resetting the state of the SUT for making test execution
independent with each other. To enable a more efficient test execution, we extended initTest with
our smart database clean procedure, by considering the union of all accessed tables, and their linked
tables, for all tests that are generated.

Regarding handling of action execution and assertion generation, with EvoMaster, tests are
generated with RestAssured to make HTTP calls toward the tested REST API. This is not applicable
in the context of RPC testing. Then, to support RPC-based API testing, we develop a Test Writer
that could handle instantiation of input parameters, RPC function call invocation (based on the RPC
client library), and assertions on response objects with JUnit. An example of generated tests can be
found at this link6.

In our industrial case study, we found that some responses contain info such as time-stamps and
random tokens, and they could change over time. In order to avoid test failing due to such flakiness,
we defined some general keywords (e.g., date, token, time) to highlight those cases. If any keyword
appear in either datatype, field name, or value with string type, the assertion would be commented
out to avoid the test become flaky. We comment them out instead of removing them completely since
it would be still interesting, for the users, to show what the response was originally.

In addition, there might exist quite large responses in some API endpoints, especially when dealing
with collections of data. For example, in one SUT in our case study, a response contained 470 elements,
and each element further contains data with list type, and 7579 assertions were generated for this
response. As such a large number of assertions would reduce the readability of the tests, we then
developed a strategy to randomly select only n (e.g., n = 2) elements from the returned collections to
generate assertions on in the tests. Due to space limitation, more details of the writer can be found in
our open-source repository1.

5 Empirical Study

5.1 Research Questions

In this paper, we conduct an empirical study to answer the following research questions:

RQ1: How does our white-box fuzzing perform compared with a baseline grey-box technique?

RQ2: How does our novel approach perform in terms of code coverage?

RQ3: Does our novel approach find real faults in industrial settings?

5.2 Experiment Setup

To evaluate our approach (denoted as RPC-EVO), we carried out an empirical study with two artificial
and four industrial RPC-based APIs selected by our industrial partner. The industrial case studies are
from a large-scale e-commerce platform (comprising hundreds of web services referred as microservices)
developed by Meituan. Descriptive statistics of the case studies are summarized in Table 2. thrift-ncs
and thrift-scs are re-implemented by us with Thrift, based on existing artificial RESTful APIs that
have been used to assess the effectiveness of solving numeric and string problems [16, 23, 17, 61].
CS1 -CS4 are from our industrial partner. Each one of them is a part of large microservice architecture,
where each API interacts with other services and a database. #Serv. (in Table 2) shows the amount

6https://github.com/anonymous-authorxyz/fuzzing-rpc/blob/main/example/src/em/EM_RPC_1_Test_others.

java

18

Table 2: Descriptive statistics of case studies

#Inte. #Func. #Ser.(U,D) #Class #LoCf (#LoCj) #Table

thrift-ncs
thrift-scs
CS1
CS2
CS3
CS4

Total

1
1
3
5
8
8

6
11
24
20
51
55

0
0
7 ( 6, 1)
11 ( 7, 4)
18 (14, 4)
36 (27, 9)

7
12
101
144
339
868

506 (
695 (

254)
260)
12559 ( 4019)
18987 ( 1821)
45987 (18800)
116340 (20760)

26

167

72 (54, 18)

1471

195072 (45914)

0
0
6
17
156
50

229

#Inte. represents the number of RPCInterfaces, #Func. represents the number of available RPC functions, #Ser.
represents the number of direct interacted external services (divided between #U of upstream and #D downstream
services), #Class is the number of Java class files, #LoCf is the number of lines of code in Files (#LoCj is the number
of lines of code reported by JaCoCo), and #Table is the number of SQL tables.

of external services that a SUT directly interacts with (see an example in Figure 1), where #U is the
amount of its upstream services which the SUT depends on, and #D is the amount of its downstream
services which call the SUT. The lines of code (LoC) numbers include everything, like comments, empty
lines and import statements. The actual lines with code (which results in LINENUMBER instructions in
the compiled .class files) are calculated with the coverage tool JaCoCo (i.e., #LoCj).

Ideally, experiments should be carried on real industrial systems. However, we employed also two
artificial case studies (which we open-sourced) to make at least parts of our experiments replicable1,
as of course we cannot share the code of the industrial systems. In addition, to further demonstrate
its adoption and performance in industrial settings, we also report preliminary results on 30 further
industrial APIs. Note that the testing of those 30 APIs was autonomously performed by our industrial
partner (e.g., prepare EvoMaster drivers, without any researcher involved), as part of an internal
evaluation to see whether/how to integrate EvoMaster in their CI pipelines.

For the choice of baselines for comparisons, regarding other tools in the literature, to the best of our
knowledge, there does not exist any other automated testing solution for RPC-based APIs that could
be applied as a baseline in this study. Therefore, we adapted our approach to be used by the Random
Search Algorithm in EvoMaster, which can be regarded as a grey-box technique (testing targets
such as code coverage are still employed to evaluate tests to produce a final test suite as output at the
end of the search). This random search serves as a baseline to evaluate our approach in the context of
white-box testing. To be comparable, the same search budget (i.e., 100 000 RPC function calls) are
applied for all settings with these techniques. In addition, to further evaluate the performance of our
generated tests, we also compare them with existing tests in the industrial case studies.

Regarding the applied evaluation metrics, effectiveness of our approach is assessed in terms of
target coverage, code coverage and fault detection. Note that the target coverage is an aggregated
metric in EvoMaster that considers all code coverage metrics (e.g., for classes and branches) and
fault finding that is only used for comparing our approach with the grey-box technique.

Considering the stochastic nature of the search algorithms, all experiments on each of the 6 main
APIs were repeated 30 times, by following common guidelines in the literature [18]. thrift-ncs and
thrift-scs were executed on an HP Z6 G4 Workstation with Intel(R) Xeon(R) Gold 6240R CPU
@2.40GHz 2.39GHz processor, 192G RAM, and 64-bit Windows 10. The four industrial APIs were
executed on the actual hardware pipelines of our industrial partner. With these pipelines, all external
services of the SUTs are up and running. In an industrial testing environment, databases can be
pre-loaded with lots of data (e.g., replicas of the production database), for covering their specific
business logic. The amount of such data can be quite large, e.g., 256 024 data entries in CS3 . In an
automated testing process, it is difficult to maintain such large data cost-effectively (e.g., clean and
re-insert them back after each test execution) for ensuring that each test is executed with the same
state of the SUT. Therefore, we decided to use empty databases to conduct our experiments with the
industrial APIs. The preliminary results on the other 30 APIs are based only on 1 run, each one where
EvoMaster was run for 10 hours.

19

Table 3: Pair comparisons between our approach (RPC-EVO) and Random with #Targets and %Lines
on all SUTs.

SUT

Metrics

thrift-ncs #Targets

%Linese

thrift-scs #Targets

CS1

CS2

CS3

CS4

%Linese
#Targets
%Linese
#Targets
%Linese
#Targets
%Linese
#Targets
%Linese

RPC-EVO Random

1773.1

376.3

549.4

p-value

Relative

ˆA12
1.00 ≤0.001 +44.07%
60.0% 1.00 ≤0.001 +47.00%
1.00 ≤0.001 +19.85%
59.6% 1.00 ≤0.001 +20.59%
0.97 ≤0.001 +10.16%
23.1% 0.99 ≤0.001 +11.15%
0.96 ≤0.001 +10.81%
24.7% 0.94 ≤0.001 +10.18%
0.96 ≤0.001 +9.50%
14.0% 0.99 ≤0.001 +9.78%
0.80 ≤0.001 +3.02%
6.3% 0.91 ≤0.001 +4.46%

3099.1

4067.2

6046.8

542.2
88.2%
658.4
71.9%

1953.3

25.6%

3434.1

27.2%

4453.4

15.4%

6229.7

6.5%

5.3 RQ1: Comparison with Grey-box Technique

To answer RQ1, we applied our approach and the random search strategy on all of the six case studies
with the same search budget, i.e., 100 000 RPC calls. The computation cost of two settings with at least
30 repetitions are 30.64 hours for the two artificial APIs, and 129.12 days for the four industrial APIs
(maximum 18.27h and minimum 9.41h per run on the industrial APIs). Note that the 30 repetitions
are only applied in this empirical study for evaluating the approach. When used by practitioners on
their systems, the approach can be run just once.

Table 3 reports the results of target and line coverage, with comparison results on the two metrics
using statistical analysis. Given these results, our approach demonstrates significantly better results
than the baseline technique, with a low p-value (i.e., ≤ 0.001) and a high effect size (i.e., ˆA12 > 0.80)
on all of the six case studies with the two metrics.

In addition, Figure 8 plots the average covered targets over time (i.e., at every 5% of the used
budget) for two techniques on each case study. Based on these line plots, our approach clearly
outperforms Random by a large margin throughout the whole process of the search, and the results
are consistent on all of the case studies. This further demonstrates the effectiveness of our white-box
techniques in both artificial and industrial settings.

RQ1: Based on the target and line coverage results, our approach significantly outperforms random
search on all of the six case studies. The relative improvements are up to 47% on the artificial case
studies and 11.15% on the industrial case studies.

5.4 RQ2: Results of Code Coverage

5.4.1 Artificial APIs

Based on coverage (i.e., %Linese) reported in Table 1, on the two artificial case studies representing
numeric and string testing problems, our approach achieves high line coverage (i.e., 88.2% on thrift-scs
and 71.9% on thrift-scs) when using 100k calls as budget. This high code coverage could demonstrate
that RPC-EVO effectively enables the white-box fuzzing for RPC-based APIs, i.e., based on the
white-box heuristics, effectively optimize the inputs of extracted/reformulated RPC function calls.

RQ2.1: Our approach achieves high line coverage on the two artificial case studies, demonstrating its
effectiveness in enabling white-box fuzzing of RPC APIs.

5.4.2

Industrial APIs

Regarding the four industrial case studies, as a fully automated solution, our approach achieved useful
(for our industrial partner) coverage on CS1 and CS2 (more than 25%), but limited coverage on CS3

20

(a) thrift-ncs

(b) thrift-scs

(c) CS1

(d) CS2

(e) CS3

(f) CS4

Figure 8: At every 5% of the used budget (x-axis), average covered targets (y-axis) achieved by
RPC-EVO and Random.

21

4004505005102030405060708090100MIORAND5405605806006206406605102030405060708090100MIORAND1700175018001850190019505102030405060708090100MIORAND28003000320034005102030405060708090100MIORAND3900400041004200430044005102030405060708090100MIORAND580059006000610062005102030405060708090100MIORANDTable 4: Numbers of generated test cases in the worst run (RPC-EVOw) and the best run (RPC-EVOb)
of our approach out of the 30 repetitions, compared to the number of existing tests in the industrial
APIs.

SUT RPC-EVOw RPC-EVOb Existing (Manual, Replay)

CS1
CS2
CS3
CS4

80
89
186
236

75
71
156
232

12 ( 0, 12)
5 ( 5, 0)
27 ( 1, 26)
46 (12, 34)

and CS4 (especially CS4 ). The results are also related to the complexity of these SUTs (as shown in
Table 2), given the same limited search budget. For example, based on LoC values, CS3 and CS4
are much larger (and likely more difficult to fully cover) than CS1 and CS2 , where CS4 has more
than 2.5 times LoC than CS3 . In addition, based on the line plots in Figure 8, the slope of the lines in
CS1 -CS4 is greater than thrift-ncs and thrift-scs, especially for CS3 and CS4 . This indicates that
more targets would likely be covered if more budget is used, i.e., if the fuzzers were run for longer, like
24 or 48 hours.

Comparison with existing tests. To study the performance on code coverage in industrial
settings, we compare our generated tests with existing tests. The analysis was conducted with three
groups of tests:

• W : a test suite generated by RPC-EVO that achieves the worst result out of the 30 repetitions;

• B: a test suite generated by RPC-EVO that achieves the best result;

• E: a set of existing tests.

The code coverage was collected by executing the generated tests and existing tests on the SUTs with
Intellij [5].

In Table 4, we report the numbers of tests in these groups for each of CS1 --CS4 . Regarding
the existing tests in the industrial setting of our partner, there exist two types of tests. One type is
manually written automated test (e.g., JUnit), and the other is with a replay of manual testing (using a
custom testing tool). The manual testing would be driven from the user-side, as the example shown in
Figure 1. For instance, a tester could perform a real business scenario as a user by directly interacting
with an app on their mobile phones. Then, with the requests from the user, it would result in various
RPC communications among the services. Those communications are recorded (such as what calls are
invoked) along with the states of the connected external services. With an industrial testing tool, such
records are performed as a replay (such as re-execute the calls) on the SUTs for conducting manual
regression testing. In order to collect code coverage for the record for the comparisons in this paper,
we converted the records as JUnit tests by extracting the calls and their inputs (but a setup of the
states of external services with the replay tool cannot be transferred into the JUnit tests).

Results of line coverage reported with Intellij achieved by the three groups for CS1 --CS4 are
reported in Table 5. Note that the line coverage might be slightly different with the results in Table 3
which are reported with EvoMaster bytecode instrumentation. However, the comparison is always
performed with results obtained from the same coverage runner.

In this table, we also provide a union of code coverage achieved by RPC-EVO and existing tests
(see Total%), and a code coverage achieved by the existing tests but not RPC-EVO (see Uncovered%).
Then we observed that

• First, by checking Total% with existing tests E in industrial APIs, on all SUTs, RPC-EVO (i.e.,
both the worst and the best test suites) can attribute to additional code coverage compared with
the existing ones;

• By comparing RPC-EVO with E, on all of the four industrial case studies, the code coverage by
RPC-EVO (i.e., the worst and the best) are clearly greater than the existing ones, except CS4
which achieves the equivalent results (i.e., 8.31%);

22

Table 5: Results of line coverage achieved by the worst run and the best run of our approach (denoted
as W and B respectively) and existing tests (denoted as E). ‘‘Total’’ represents the union coverage
achieved by RPC-EVO and existing tests, and ‘‘Uncovered’’ represents the coverage achieved by the
existing tests but not by our approach.

SUT

CS1
CS2
CS3
CS4

RPC-EVO (%) Ex.(%)
[W , B]

Total (%)
E [W ∪ E, B ∪ E]

Uncovered(%)
[E \ W , E \ B]

[22.50, 26.81]
[25.46, 26.02]
[13.37, 15.89]
[ 8.31, 9.15]

14.29
16.28
5.29
8.31

11.04

[22.95, 27.85]
[25.46, 26.40]
[13.46, 15.92]
[10.22, 10.90]

[0.45, 1.04]
[0.00, 0.37]
[0.09, 0.03]
[1.91, 1.75]

[18.02, 20.27]

[0.61, 0.80]

Avg.

[17.41, 19.47]

• Regarding Uncovered, the percentage is minor (i.e., the max is 1.91%). This indicates that
RPC-EVO is capable of covering most of the code achieved by the existing tests, i.e., above
77.02% (=1 − 1.91/8.31), up to 100% of the code covered by E;

• One interesting observation here is that the selected worst run perform better in covering lines
achieved by existing tests on CS1 and CS2 than the best run. This might further reveal various
promising regions of search space in industrial problems.

Based on the observation, we could conclude

RQ2.2: Compared with existing tests, RPC-EVO is capable of contributing additional code coverage
and demonstrates clear better results. In addition, RPC-EVO could cover above 77.02% line
coverage achieved by the existing tests.

In-depth analysis of the coverage reports. To further study why higher coverage was not
achieved, we performed a manual analysis on the code coverage reports generated by the best test suite
(i.e., B) and the source code of these APIs. For CS4 , we found that 10 RPC functions out of the 55
functions are not accessible with the given client library. By checking with our industrial partner, they
think these problems are due to some issues in their testing environment (e.g., which uses a service
discovery mechanism and load balancers) that were found as well in executing existing tests. These
problems are currently under investigation. This might be a reason for the least line coverage achieved
by our approach on CS4 .

Based on the coverage reports on the four APIs, we found that our approach achieved limited
code coverage on the code which is related to database handling and communications with external
services. Regarding the database handling code, most of it is automatically generated with an in-house
framework for facilitating various manipulations on the database, e.g., to perform a query with various
conditions. As discussed with our industrial partner, usually, not all of the generated manipulation
code would be used in implementing their business logic. However, they are still generating and keep
such code in case of future use. Therefore, a lot of this code is infeasible to be covered with any system
test.

Regarding the code related to the communications with the external services, all these services
were up and running in the test environment of our industrial partner. Since these external services are
not mocked nor their code is instrumented with our SBST heuristics, i.e., not being part of our testing
process, then we mostly fail to get different responses with our automatically generated inputs that
maximize the code coverage in the SUT (e.g., all the code used to read and act upon the responses
given by these external services). How to deal with external services is a major research challenge
which applies to all kinds of web services.

Another main issue that we found is related to input validation. In our approach, we have handled
all the constraints specified as parts of interface definitions, but there also exist further restricted
checks on the inputs. The inputs could be restricted in the internal business logic of the SUT, e.g., an
input parameter x could be validated with an external service regarding whether it exists: then, if it
exists, it could further query another service for the related data with x. With our current heuristics,
such valid inputs could be rarely generated. Moreover, the needed inputs are often complex. For

23

Table 6: Results of line coverage of 30 industrial APIs achieved by RPC-EVO with 1 run using 10
hours budget.

#

#LoCj %Linesj #

#LoCj %Linesj #

#LoCj %Linesj

40150
16863
8126
55860
33713
10224
29356
20549
35114
20640

#01
#02
#03
#04
#05
#06
#07
#08
#09
#10
#LoCj
%Linesj

8.02 #11
22.10 #12
17.34 #13
8.13 #14
6.32 #15
32.28 #16
6.80 #17
10.10 #18
14.13 #19
22.40 #20

8.60
36.54 #21
15.50
10.62 #22
9.64
13.95 #23
7.94
28.77 #24
10.23
32.91 #25
32.87
12.86 #26
6.59
22.25 #27
16.22
21.86 #28
7.51
14.41 #29
14.34
21.83 #30
Sum: 602084; Avg: 20069 ; Max: 55860; Min: 762
Avg: 16.44; Max: 36.54; Min: 6.32
30∼40%: 4 SUTs; 20∼30%: 6 SUTs; 10∼20%: 11 SUTs; 5∼10%: 9 SUTs
# is an index of industrial SUTs; #LoC is the number of lines of code reported by JaCoCo; %Lines is the line coverage
achieved by our approach

762
5315
1701
4699
4458
2029
29430
13431
17062
28087

30642
23474
29806
25191
25093
31133
29037
1807
16180
12152

instance, we noticed that, in a generated test, there are 2024 lines for instancing a single input DTO.
As we checked, the length for all lists in that instance is less than 5. Then, we further checked the
implementation of the DTO, which it contains 25 fields, and the fields could be other DTOs or lists of
DTOs. Such very large DTO would lead to additional difficulty to generate valid inputs, e.g., if any
element (e.g., in a collection) violates any constraint, then the whole DTO would be considered as
invalid, and fail the input validation.

RQ2.3: Our approach achieves useful coverage (26.81% and 26.02%) on two out of the four
industrial case studies, and limited coverage (15.89% and 9.15%) on the other two larger industrial
case studies. Based on a manual analysis on code coverage and the source code, we found that the
main issues are related to the communications with external services and to generate inputs for
complex DTO with various constraints.

Additional analysis with code coverage collected by our industrial partner. In Table 6,
we report the preliminary results of line coverage in 30 industrial APIs achieved by RPC-EVO with 1
run using 10 hours search budget. Note that all these 30 industrial APIs plus CS1 --CS4 are parts of
one single microservice architecture, with hundreds of web APIs. The 10-hour budget was decided by
our industrial partner by considering their application context and time cost per run in this experiment.
Based on the results, on 30 industrial APIs with 602k lines of code (#LoCj) in total and 20k lines
of code on average, RPC-EVO achieves 30%∼40% line coverage on 4 SUTs, 20%∼30% line coverage
on 6 SUTs, 10%∼20% line coverage on 11 SUTs, and 5%∼10% line coverage on 9 SUTs.

RQ2.4: RPC-EVO has been successfully applied in white-box fuzzing 30 industrial RPC-based APIs
in practice by our industrial partner. With 10-hour search budget, results show that our approach is
capable of achieving on average 16.44% (up to 36.54%) line coverage.

5.5 RQ3: Results of Fault Detection

To assess the fault detection capabilities of our novel approach, we performed a detailed analysis on
the identified faults with our industrial partner, as researchers and industrial practitioners might have
different views on the severity and importance of the found faults. The manual analysis is based on the
test suites that achieved the best code coverage (out of the 30 runs) for each of the four industrial APIs
we analyzed in details. We applied such selection due to the time constraints of manually checking
all the generated test suites in all the 30 repetitions. With this selection, the amount of tests to be
reviewed are 534 as RPC-EVOW shown in Table 4. With these tests, faults are identified based on
(1) any exception thrown in the calls; (2) service error represented by assertions on the responses;
(3) failed tests when executing them on the SUT (mainly due to flaky assertions); and (4) whether
responses are expected based on the given inputs. The review was firstly conducted by the first author,

24

Table 7: Results of the potential distinct faults automatically reported by our approach with 30 runs
for each industrial SUT. We report as well the number of real faults manually identified and confirmed
with the industrial partner.

SUT
CS1
CS2
CS3
CS4
Total

Real Faults

Potential Faults
Avg.[Min, Max] L1 L2 L3 Total
22
17
19
3
30
1
55
3
126
24

40.2 [40, 41]
21.8 [21, 26]
51.6 [51, 55]
91.8 [74, 111]

0
1
0
16
17

5
15
29
36
85

L1 : faults that will be fixed; L2 : faults are needed to be fixed but less important; L3 : faults that are tolerable, and likely
no need to fix

then an employee of our industrial partner (a QA Manager who has 8 years of testing experience in
industry) performed the same kind of analysis on these tests. At the end, a meeting was held to discuss
and confirm the final results reported in Table 7.

As shown in Table 7, in total 126 real unique faults were found with the selected test suites on the
four industrial APIs. The faults could be further classified into three levels, i.e., L1, L2 and L3, based
on willingness of our industrial partner to fix these faults. L1 represents the number of faults that are
serious enough that should be fixed. These faults are related to mistakes in the code implementation,
errors in handling databases, errors in transaction processing and potential risky errors in returning a
misleading response. At the time of writing this paper, the identified faults have all been confirmed
and fixed. L2 is the number of faults that should be fixed but are less critical. Most of the faults
at L2 are related to the implementation of input validation and external service response handling
when exceptions are thrown. In their context, it is better to properly handle exceptions within the
SUT, as such thrown exceptions might lead to further problems in the services that depend on the
tested application. L3 is the number of minor faults that are tolerable, and most likely our industrial
partner will not fix them. These faults are mainly due to input validation throwing exceptions such as
NullPointerException, IndexOutOfBoundsException and java.text.ParseException. However,
if the exceptions are caught and handled within the SUT, they consider that such faults are tolerable.
In Table 7, we also report the number of potential faults automatically reported by our approach.
As expected, the number of real faults we manually identified is less than the potential ones. This is
mainly due to (1) problems in the test environment (e.g., some external services might not had been up
and running when the experiments were carried out); (2) data preparation in databases (e.g., an empty
database might lead to some problems that would never happen in production); (3) communications
over the network (e.g., connection timeouts); (4) client problems (e.g., some remote functions fail for
some configuration issues when we ran the experiments for CS4 ). However, with the generated tests,
our employed automated oracles could identify most of the real faults, except the errors related to
returning a misleading/unexpected response (as this requires the users to manually check the content
of these responses, as no formal specification is available).

RQ3: With an in-depth analysis of the generated tests with our industrial partner, we confirm that
our approach was capable of finding 41 actual real faults that have now been fixed.

6 Lessons Learned

Automated testing requires a reset of the SUT, however, it is challenging to reset the
state of a real industrial API. To enable the generated tests to be used for regression testing, and
to properly evaluate the fitness of each test case in isolation, it is needed to execute every test with the
same state of SUT (i.e., test case executions should be independent from each other). Thus, it requires
to perform a state reset of the SUT before a test is executed on it, e.g., clear all data in the database
or reset databases to a specific state. With open-source case studies, it is trivial, e.g., clean data in
database. For instance, EvoMaster provides a utility DbClearner for facilitating the cleaning of data

25

for various types of SQL databases, e.g., Postgres and MySQL. Such a clean on the database does work
fine for small-scale applications. However, in large-scale industrial settings, cleaning all data in the
database is quite expensive, even when the database is empty. For instance, in one of the industrial
APIs used in this paper, it takes 5.3s to clean an empty database, and it takes more time if there exist
data. Thus, within 1 hour as search budget, a fuzzer can execute at most 680 RPC function calls. This
would significantly limit the fuzzer in terms of cost-effectiveness. To better enable our approach in
industrial settings, by taking advantage of existing SQL handling in EvoMaster, we developed an
automated smart clean on the database, by considering only what tables are actually modified during
the search. With the smart clean, after a test is executed, data only in the accessed tables and linked
tables (e.g., with foreign key) will be removed. In addition, we also allow SQL commands/scripts to
initialize data into the database (e.g., for username/password authentication info). If a table which
has initial data is cleaned, a post action will be performed to add the initial data for the table again.
With such smart database clean, we could effectively reduce time spent by more than 90%, e.g., from
5.3s to 285ms. This is because there can be tens/hundreds of tables in an industrial API, but only
few of them are actually accessed during the executing of a single test. However, how to reset the
state of the databases with a large amount of existing data still needs to be addressed. Besides the
database, the states of direct connected external services also need to be reset. Currently, fuzzing by
our approach is performed on the industrial test environment where all services are up and running.
With such an environment, the states of external services might be varied over time (e.g., failed tests
as discussed in Section 5.5). Mocking technique could be a potential solution to address this, e.g., set
up specific states of the external services before test execution. However, mocking RPC-based services
in microservices is also challenging, e.g., due to network communications and environment setup in
industrial settings. It could be considered as important future work.

Real industrial APIs have more complex inputs and apply more restrict constraints
in input validations with considerations of various aspects. By checking code coverage and
fault detection, we found that most of codes and faults are related to the parts of implementation
for input validation. One reason could be due to the complexity of the input with cycle objects and
collections in DTO. For instance, we found that a DTO is initialized with more than 2k lines, and
generating a valid input for such a huge DTO would not be trivial. In enterprise applications, often
there exist several constraints on the inputs when processing their business logic. This can lead to
major challenges for automated testing approaches to generate such inputs. The input validation is
performed at two levels, i.e., in the schema and business logic. The schema level would perform simple
checks (e.g., null, format and range) and checks on constraints related to multiple fields in inputs.
Although we have supported the handling of all these constraints defined with javax annotations, it is
clear that it is not enough in industrial settings. Because not all constraints are fully specified in the
interface definitions, e.g., with javax.validation.constraints, the validation could be implemented
as a utility or with libraries, e.g., com.google.common.base.Preconditions, directly in the code of
the business logic. To address this, further white-box handling is required to provide more effective
gradient to cover the code.

Regarding the validation in terms of business logic, it could perform a check with database and
external services. Data preparation in database and mocking external services would be
vital in testing of industrial RPC-based APIs, not only for input validation. For databases,
currently our approach employs the SQL handling in EvoMaster [21] for facilitating data preparation
in the database. However, as identified in this study, there might exist some limitations in handling
industrial settings cost-effectively, e.g., currently EvoMaster lacks support for composite primary
keys. This does limit the performance on code coverage. For instance, we found that a query action
with no input parameters is always failing with an exception thrown.
In this case, we could do
nothing by manipulating the input parameters. Then, with a manual check on the source code, we
found that the query is required to have data in the database, but the data fails to be generated due
to some unsupported SQL features. In addition, with only SQL query heuristics, it might not be
cost-effective to build meaningful links between RPC function calls and inserted data into the database.
Smart strategies would be required here to handle industrial RPC-based APIs, such as the enhanced
SQL handling strategies for REST APIs [62]. For external services, if we could mock such external

26

services, then the problem might be solved by directly manipulating their responses. Automating such
manipulation as parts of the search would be another important challenge.

Another possibility to improve code coverage would be to develop advanced search operators for
the RPC domain. For instance, we found that, in the generated tests, function calls in a test may not
be related to each other for testing a meaningful scenario. In order to better generate tests with related
function calls, we could have strategies to sample function calls by considering dependency among
functions (e.g., [66]) in the context of RPC testing, e.g., based on which SQL tables they do access.

An industrial RPC-based API is often a part of large-scale microservices that closely
interacts with multiple APIs. Such interaction would result in a huge search space. To
test a single API or an API in a small-scale microservice system, testing targets (such as lines of code)
could be feasible to reach with an empty database (with/without a small amount of data initialized by
SQL script) by manipulating input parameters and data into the database (e.g., INSERT). However,
testing an industrial API in microservices is not like this case. As the example shown in Figure 1, the
states of other services and databases often have a strong impact in processing business
logic that would result in code coverage. Therefore, all such possible states are considered as
a part of search. In this paper, we provide descriptive statistics for 34 industrial APIs with #LoCj
(in total more than 600k). All of the APIs are parts of one microservice architecture, and there exist
hundreds of other APIs which were not used in these experiments. To cope with such a huge complexity
of the state, an empty database (as we employed) might limit performance. In addition, as discussed
with our industrial partner, they think that it is important to involve their real historical data
(collected in production) in the automated testing. Likely it would improve the chances to
cover more of their business scenarios in the generated tests. Furthermore, such tests would be more
valuable for them, e.g., they would consider that all faults identified by these tests would have higher
priority to be addressed. However, such data is complex and possibly huge, and how to effectively and
efficiently utilize this data with search would be another research challenge that we will address in the
future.

Enabling fuzzers on CI would promote their adoption in industrial settings. Our
approach is now integrated into one of the industrial development pipelines (same as for the experiments
we ran in this paper), as a trial to check its applicability into the daily testing activities of our industrial
partner. Since all services are developed with the same framework, by studying one of the EvoMaster
driver configurations for our approach, our industrial partner has implemented an automated solution
to automatically generate such drivers for their services to be tested (e.g., identify all available
interfaces and instantiate corresponding clients). For instance, the drivers of the 30 industrial APIs
in Table 6 were automatically generated with this automated solution. Regarding the application
context, as discussed with our industrial partner, our approach is planned to be employed on the
services for generating white-box system tests when the implementation for a requirement of the
services is considered as done, as a kind of extra check before putting these new features in production.
In addition, the generated tests would be kept for further usage in (1) regression testing of the services
and (2) industrial test environment validation as scheduled tasks (e.g., to see whether all services on
the pipeline are up and running correctly before QA engineers start manual test sessions).

Flakiness and readability are required to be considered in test generation in industrial
APIs. As we found in the industrial APIs, responses could contain info such as time-stamps and
random tokens, and they could change over time. In order to avoid test failing due to such flakiness,
we defined some strategies with general keywords (e.g., date, token, time) observed in the industrial
APIs to comment out assertions with such sources. How to systematically identify possible sources
of flakiness existing in the industrial APIs (e.g., time-stamps, results of SQL queries) and properly
handle it during the automation and in the test generation would be another important problem that
researchers should address. During the process of reviewing the generated tests with industrial partner,
we found that test readability requires to be improved. This is mainly due to very large blocks of
code for input instantiations and large numbers of tests in the test suites. As identified in the review,
our industrial partner found that the tests which lead to exception thrown are more interesting for
them. Therefore, to improve test readability, we now provide a simple strategy to split such tests into
different files (the implementation is straight-forwarded, but it is quite useful for our partner). Further

27

possible improvements could be achieved by better organizing the code for large input instantiations,
and sorting/splitting tests based on various considerations, e.g., fault classification [46].

7 Threats to Validity

Conclusion validity. Our study is in the context of SBST, and our experiments were conducted by
following common guidelines in the literature to assess randomized techniques [18]. For instance, with
a consideration for the stochastic nature of the employed search algorithms, we collected results for all
settings with at least 30 repetitions. The results were interpreted with statistical analysis, such as
Mann-Whitney-Wilcoxon U-tests (p-value) and Vargha-Delaney effect sizes ( ˆA12 for pair comparisons.
Regarding fault detection capability, a number of real faults was identified, and those were reviewed
together with our industrial partner.

Construct validity. To avoid bias in the results among different settings and techniques, all results
to be compared were executed the on same physical environment, e.g., experiments on artificial case
studies were deployed on a local machine, and experiments on industrial case studies were deployed on
the pipeline of our industrial partners.

Internal validity, Our implementation was tested with various units tests and end-2-end tests, but
we cannot guarantee no fault in our implementation. However, our tool and artificial case studies are
open-source. This enables further verification on our implementation and replication of our experiments
on the artificial case studies by other researchers. Note that, due to the confidential info of the
employed industrial case studies, detailed results of these industrial APIs cannot be made publicly
available.

External validity. In this study, our approach was assessed with artificial case studies using Thrift
and 34 industrial case studies (from one company) using their own RPC framework which is initially
built based on Thrift. There might exist a threat to generalize our results to other RPC frameworks or
other companies.

8 Conclusion

RPC is widely applied in industry for developing large-scale distributed systems, such as microservices.
However, automated testing of such systems is very challenging. To the best of our knowledge, there
does not exist any tool or solution in the research literature that could enable automated testing of
such systems. Therefore, having such a solution with tool support could bring significant benefits to
industrial practice.

In this paper, we propose the first approach for automatically white-box fuzzing RPC-based APIs,
using search-based techniques. To access the RPC-based APIs, the approach is developed by extracting
available RPC functions with RPCInterfaces from the source code. This can enable its adoption to
most RPC frameworks in the context of white-box testing. To enable search techniques (e.g., MIO) in
RPC domain, we reformulate the problem and propose additional handling and heuristics specialized
for RPC.

The approach is implemented as an open-source tool built on top of our EvoMaster [2] fuzzer. A
detailed empirical study of our novel approach was conducted with two artificial and four industrial
APIs, plus a preliminary (e.g., no fault analysis) study on a further 30 APIs. In total, more than a
million lines of business code (including comments/empty-lines which are not considered by JaCoCo,
but excluding third-party libraries) were used in this study. When third-party libraries are considered
as well (e.g., for carrying out taint analysis [24]), several millions of lines of code were analyzed and
executed in these experiments.

Results demonstrate the successful applicability of our novel approach in industrial settings. Our
tool extension presented in this paper is already in daily use in the Continuous Integration systems of
Meituan, a large e-commerce enterprise with hundreds of millions of customers. In addition, to evaluate
the effectiveness of our approach in the context of white-box search-based testing, we compared
our approach with a grey-box technique. The results show that our approach achieves significant
improvements on code coverage. To further evaluate the capability of fault detection, we carried out

28

an in-depth manual review with one employee of our industrial partner on the tests generated by our
novel approach. A total of 41 real faults were identified that have now been fixed.

Considering how widely used RPC frameworks such as Apache Thrift, Apache Dubbo, gRPC and
SOFARPC have been in industry in the last decade, it can be surprising to see how such important
software engineering topic has been practically ignored by the research community so far. One possible
explanation is the lack of easy access to case studies for researchers, as this kind of systems are used to
build enterprise applications. Therefore, these systems are seldom available on open-source repositories,
or online on the internet (i.e., general access web services are usually developed as REST APIs). To
be able to empirical evaluate our novel techniques, industry collaborations (e.g., with Meituan) were a
strong requirement.

Although our tool extension is already of use for practitioners in industry, more needs to be done
to achieve better results. Future work will focus on improving white-box heuristics to increase the
achieved code coverage, and how to handle and analyze the interactions with external web services.
Our tool extension of EvoMaster is freely available online on GitHub [2] and Zenodo (e.g.,
EvoMaster version 1.5.0 [26]), and the replication package for this study can be found at the
following link1.

Acknowledgment

This project has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme (grant agreement No 864972).

References

[1] [n.d.]. Dubbo. https://dubbo.apache.org/en/.

[2] [n.d.]. EvoMaster. https://github.com/EMResearch/EvoMaster.

[3] [n.d.]. GraphQL Foundation. https://graphql.org/foundation/.

[4] [n.d.]. gRPC. https://grpc.io/.

[5] [n.d.]. Intellij IDEA Code coverage. https://www.jetbrains.com/help/idea/code-coverage.html.

[6] [n.d.].

javax.validation.constraints.

https://javaee.github.io/javaee-

spec/javadocs/javax/validation/constraints/package-summary.html.

[7] [n.d.]. SOFARPC. https://www.sofastack.tech/en/.

[8] [n.d.]. Status code in gRPC. https://grpc.github.io/grpc/core/md_doc_statuscodes.html.

[9] [n.d.]. TApplicationException in Thrift.

https://javadoc.io/doc/org.apache.thrift/

libthrift/latest/org/apache/thrift/TApplicationException.html.

[10] [n.d.]. thrift. https://thrift.apache.org/.

[11] Andrea Arcuri. 2017. Many Independent Objective (MIO) Algorithm for Test Suite Generation.

In International Symposium on Search Based Software Engineering (SSBSE). 3--17.

[12] Andrea Arcuri. 2017. RESTful API Automated Test Case Generation. In IEEE International

Conference on Software Quality, Reliability and Security (QRS). IEEE, 9--20.

[13] Andrea Arcuri. 2018. EvoMaster: Evolutionary Multi-context Automated System Test Generation.
In IEEE International Conference on Software Testing, Verification and Validation (ICST).
IEEE.

29

[14] Andrea Arcuri. 2018. An experience report on applying software testing academic results in
industry: we need usable automated test generation. Empirical Software Engineering 23, 4 (2018),
1959--1981.

[15] Andrea Arcuri. 2018. Test suite generation with the Many Independent Objective (MIO) algorithm.

Information and Software Technology 104 (2018), 195--206.

[16] Andrea Arcuri. 2019. RESTful API Automated Test Case Generation with EvoMaster. ACM

Transactions on Software Engineering and Methodology (TOSEM) 28, 1 (2019), 3.

[17] Andrea Arcuri. 2020. Automated Black-and White-Box Testing of RESTful APIs With EvoMaster.

IEEE Software 38, 3 (2020), 72--78.

[18] A. Arcuri and L. Briand. 2014. A Hitchhiker’s Guide to Statistical Tests for Assessing Randomized
Algorithms in Software Engineering. Software Testing, Verification and Reliability (STVR) 24, 3
(2014), 219--250.

[19] Andrea Arcuri and Juan P. Galeotti. 2019. SQL Data Generation to Enhance Search-Based System
Testing. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO ’19).
Association for Computing Machinery, New York, NY, USA, 1390–1398. https://doi.org/10.
1145/3321707.3321732

[20] Andrea Arcuri and Juan P. Galeotti. 2020. Handling SQL Databases in Automated System Test
Generation. ACM Transactions on Software Engineering and Methodology (TOSEM) 29, 4 (2020),
1--31.

[21] Andrea Arcuri and Juan P Galeotti. 2020. Handling SQL databases in automated system test
generation. ACM Transactions on Software Engineering and Methodology (TOSEM) 29, 4 (2020),
1--31.

[22] Andrea Arcuri and Juan P Galeotti. 2020. Testability transformations for existing APIs. In 2020
IEEE 13th International Conference on Software Testing, Validation and Verification (ICST).
IEEE, 153--163.

[23] Andrea Arcuri and Juan P Galeotti. 2021. Enhancing Search-based Testing with Testability
Transformations for Existing APIs. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 1 (2021), 1--34.

[24] Andrea Arcuri and Juan P Galeotti. 2021. Enhancing Search-based Testing with Testability
Transformations for Existing APIs. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 1 (2021), 1--34.

[25] Andrea Arcuri, Juan Pablo Galeotti, Bogdan Marculescu, and Man Zhang. 2021. EvoMaster: A
Search-Based System Test Generation Tool. Journal of Open Source Software 6, 57 (2021), 2153.

[26] Andrea Arcuri, ZhangMan, asmab89, Bogdan, Amid Gol, Juan Pablo Galeotti, Seran, Al-
berto Martín López, Agustina Aldasoro, Annibale Panichella, and Kyle Niemeyer. 2022. EMRe-
search/EvoMaster:. https://doi.org/10.5281/zenodo.6651631

[27] Vaggelis Atlidakis, Patrice Godefroid, and Marina Polishchuk. 2019. RESTler: Stateful REST
API Fuzzing. In ACM/IEEE International Conference on Software Engineering (ICSE) (ICSE).
IEEE, 748–758.

[28] Xiaoying Bai, Wenli Dong, Wei-Tek Tsai, and Yinong Chen. 2005. WSDL-based automatic test
case generation for web services testing. In Service-Oriented System Engineering, 2005. SOSE
2005. IEEE International Workshop. IEEE, 207--212.

[29] Cesare Bartolini, Antonia Bertolino, Eda Marchetti, and Andrea Polini. 2009. WS-TAXI: A
WSDL-based testing tool for web services. In Software Testing Verification and Validation, 2009.
ICST’09. International Conference on. IEEE, 326--335.

30

[30] Asma Belhadi, Man Zhang, and Andrea Arcuri. 2022. Evolutionary-based Automated Testing for

GraphQL APIs. In Genetic and Evolutionary Computation Conference (GECCO).

[31] Francisco Curbera, Matthew Duftler, Rania Khalaf, William Nagy, Nirmal Mukhi, and Sanjiva
Weerawarana. 2002. Unraveling the Web services web: an introduction to SOAP, WSDL, and
UDDI. IEEE Internet computing 6, 2 (2002), 86--93.

[32] Roy Thomas Fielding. 2000. Architectural styles and the design of network-based software

architectures. Ph.D. Dissertation. University of California, Irvine.

[33] Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: automatic test suite generation for object-
oriented software. In ACM Symposium on the Foundations of Software Engineering (FSE).
416--419.

[34] Gordon Fraser and Andrea Arcuri. 2013. Whole Test Suite Generation. IEEE Transactions on

Software Engineering 39, 2 (2013), 276--291.

[35] Vahid Garousi, Matt M Eskandar, and Kadir Herkiloğlu. 2016. Industry--academia collaborations
in software testing: experience and success stories from Canada and Turkey. Software Quality
Journal (2016), 1--53.

[36] Vahid Garousi and Michael Felderer. 2017. Worlds apart: a comparison of industry and academic

focus areas in software testing. IEEE Software 34, 5 (2017), 38--45.

[37] Vahid Garousi, Michael Felderer, Marco Kuhrmann, and Kadir Herkiloğlu. 2017. What industry
wants from academia in software testing?: Hearing practitioners’ opinions. In Proceedings of the
21st International Conference on Evaluation and Assessment in Software Engineering. ACM,
65--69.

[38] Vahid Garousi, Dietmar Pfahl, João M Fernandes, Michael Felderer, Mika V Mäntylä, David
Shepherd, Andrea Arcuri, Ahmet Coşkunçay, and Bedir Tekinerdogan. 2019. Characterizing
industry-academia collaborations in software engineering: evidence from 101 projects. Empirical
Software Engineering 24, 4 (2019), 2540--2602.

[39] Samer Hanna and Malcolm Munro. 2008. Fault-based web services testing. In Information
Technology: New Generations, 2008. ITNG 2008. Fifth International Conference on. IEEE,
471--476.

[40] Zac Hatfield-Dodds and Dmitry Dygalo. 2022. Deriving Semantics-Aware Fuzzers from Web
API Schemas. In 2022 IEEE/ACM 44th International Conference on Software Engineering:
Companion Proceedings (ICSE-Companion). IEEE, 345--346.

[41] Stefan Karlsson, Adnan Čaušević, and Daniel Sundmark. 2020. Automatic Property-based Testing

of GraphQL APIs. arXiv preprint arXiv:2012.07380 (2020).

[42] Myeongsoo Kim, Qi Xin, Saurabh Sinha, and Alessandro Orso. 2022. Automated Test Generation
for REST APIs: No Time to Rest Yet. https://doi.org/10.48550/ARXIV.2204.08348

[43] Nuno Laranjeiro, João Agnelo, and Jorge Bernardino. 2021. A black box tool for robustness

testing of REST services. IEEE Access 9 (2021), 24738--24754.

[44] Yin Li, Zhi-an Sun, and Jian-Yong Fang. 2016. Generating an Automated Test Suite by Variable
Strength Combinatorial Testing for Web Services. CIT. Journal of Computing and Information
Technology 24, 3 (2016), 271--282.

[45] Chunyan Ma, Chenglie Du, Tao Zhang, Fei Hu, and Xiaobin Cai. 2008. WSDL-based automated
test data generation for web service. In Computer Science and Software Engineering, 2008
International Conference on, Vol. 2. IEEE, 731--737.

31

[46] Bogdan Marculescu, Man Zhang, and Andrea Arcuri. 2022. On the Faults Found in REST APIs
by Automated Test Generation. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 3 (2022), 1--43.

[47] Evan Martin, Suranjana Basu, and Tao Xie. 2006. Automated robustness testing of web services.
In Proceedings of the 4th International Workshop on SOA And Web Services Best Practices
(SOAWS 2006).

[48] Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cortés. 2020. RESTest: Black-Box
Constraint-Based Testing of RESTful Web APIs. In International Conference on Service-Oriented
Computing.

[49] Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cortés. 2021. RESTest: Automated
Black-Box Testing of RESTful Web APIs. In ACM Int. Symposium on Software Testing and
Analysis (ISSTA). ACM.

[50] Sam Newman. 2015. Building Microservices. " O’Reilly Media, Inc.".

[51] Jeff Offutt and Wuzhi Xu. 2004. Generating test cases for web services using data perturbation.

ACM SIGSOFT Software Engineering Notes 29, 5 (2004), 1--10.

[52] Annibale Panichella, Fitsum Kifetew, and Paolo Tonella. 2018. Automated Test Case Generation
IEEE

as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets.
Transactions on Software Engineering (TSE) 44, 2 (2018), 122--158.

[53] José Miguel Rojas, Mattia Vivanti, Andrea Arcuri, and Gordon Fraser. 2017. A detailed investiga-
tion of the effectiveness of whole test suite generation. Empirical Software Engineering (EMSE)
22, 2 (2017), 852--893.

[54] Harry M Sneed and Shihong Huang. 2006. WSDLTest-a tool for testing web services. In Web Site

Evolution, 2006. WSE’06. Eighth IEEE International Symposium on. IEEE, 14--21.

[55] Wei-Tek Tsai, Ray Paul, Weiwei Song, and Zhibin Cao. 2002. Coyote: An xml-based framework
for web services testing. In High Assurance Systems Engineering, 2002. Proceedings. 7th IEEE
International Symposium on. IEEE, 173--174.

[56] Daniela Meneses Vargas, Alison Fernandez Blanco, Andreina Cota Vidaurre, Juan Pablo Sandoval
Alcocer, Milton Mamani Torres, Alexandre Bergel, and Stéphane Ducasse. 2018. Deviation testing:
A test case generation technique for GraphQL APIs. In 11th International Workshop on Smalltalk
Technologies (IWST). 1--9.

[57] Emanuele Viglianisi, Michael Dallago, and Mariano Ceccato. 2020. RESTTESTGEN: Automated
Black-Box Testing of RESTful APIs. In IEEE International Conference on Software Testing,
Verification and Validation (ICST). IEEE.

[58] Muhammad Waseem, Peng Liang, Mojtaba Shahin, Amleto Di Salle, and Gastón Márquez. 2021.
Design, monitoring, and testing of microservices systems: The practitioners’ perspective. Journal
of Systems and Software 182 (2021), 111061.

[59] Huayao Wu, Lixin Xu, Xintao Niu, and Changhai Nie. 2022. Combinatorial Testing of RESTful

APIs. In ACM/IEEE International Conference on Software Engineering (ICSE).

[60] Wuzhi Xu, Jeff Offutt, and Juan Luo. 2005. Testing web services by XML perturbation. In
Software Reliability Engineering, 2005. ISSRE 2005. 16th IEEE International Symposium on.
IEEE, 10--pp.

[61] Man Zhang and Andrea Arcuri. 2021. Adaptive Hypermutation for Search-Based System Test
Generation: A Study on REST APIs with EvoMaster. ACM Transactions on Software Engineering
and Methodology (TOSEM) 31, 1 (2021).

32

[62] Man Zhang and Andrea Arcuri. 2021. Enhancing Resource-Based Test Case Generation for
RESTful APIs with SQL Handling. In International Symposium on Search Based Software
Engineering. Springer, 103--117.

[63] Man Zhang and Andrea Arcuri. 2022. Open Problems in Fuzzing RESTful APIs: A Comparison

of Tools. arXiv preprint arXiv:2205.05325 (2022).

[64] Man Zhang, Andrea Arcuri, Yonggang Li, Kaiming Xue, Zhao Wang, Jian Huo, and Weiwei
Huang. 2022. Fuzzing Microservices In Industry: Experience of Applying EvoMaster at Meituan.
https://doi.org/10.48550/ARXIV.2208.03988

[65] Man Zhang, Asma Belhadi, and Andrea Arcuri. 2022. JavaScript Instrumentation for Search-Based
Software Testing: A Study with RESTful APIs. In IEEE International Conference on Software
Testing, Verification and Validation (ICST). IEEE.

[66] Man Zhang, Bogdan Marculescu, and Andrea Arcuri. 2019. Resource-based test case generation for
RESTful web services. In Proceedings of the Genetic and Evolutionary Computation Conference.
1426--1434.

[67] Man Zhang, Bogdan Marculescu, and Andrea Arcuri. 2021. Resource and dependency based test
case generation for RESTful Web services. Empirical Software Engineering 26, 4 (2021), 1--61.

[68] Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Wenhai Li, and Dan Ding. 2018. Fault analysis
and debugging of microservice systems: Industrial survey, benchmark system, and empirical study.
IEEE Transactions on Software Engineering (TSE) (2018).

33

