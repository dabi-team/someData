2
2
0
2

n
u
J

3
2

]

G
C
.
s
c
[

1
v
7
8
8
1
1
.
6
0
2
2
:
v
i
X
r
a

VRKITCHEN2.0-INDOORKIT: A TUTORIAL FOR AUGMENTED
INDOOR SCENE BUILDING IN OMNIVERSE

A PREPRINT

Yizhou Zhao∗
Department of Statistics
University of California, Los Angeles
yizhouzhao@g.ucla.edu

Steven Gong∗
Department of Computer Science
University of California, Los Angeles
nikepupu@g.ucla.edu

Xiaofeng Gao
Department of Statistics
University of California, Los Angeles
schaffergao@gmail.com

Wensi Ai
Department of Computer Science
University of California, Los Angeles
va0817@g.ucla.edu

Song-Chun Zhu
Department of Statistics
University of California, Los Angeles
sczhu@stat.ucla.edu

June 24, 2022

ABSTRACT

With the recent progress of simulations by 3D modeling software and game engines, many researchers
have focused on Embodied AI tasks in the virtual environment. However, the research community
lacks a platform that can easily serve both indoor scene synthesis and model benchmarking with vari-
ous algorithms. Meanwhile, computer graphics-related tasks need a toolkit for implementing advanced
synthesizing techniques. To facilitate the study of indoor scene building methods and their potential
robotics applications, we introduce INDOORKIT: a built-in toolkit for NVIDIA OMNIVERSE that
provides ﬂexible pipelines for indoor scene building, scene randomizing, and animation controls. Be-
sides, combining Python coding in the animation software INDOORKIT assists researchers in creating
real-time training and controlling for avatars and robotics. The source code for this toolkit is available
at https://github.com/realvcla/VRKitchen2.0-Tutorial, and the tutorial along with the
toolkit is available at https://vrkitchen20-tutorial.readthedocs.io/en/latest/.

Keywords Simulation environment · Embodied AI · Animation · Indoor scene synthesis

1

Introduction

Simulation engines are tools used by designers to code and plan out a simulation environment quickly and easily without
building one from the ground up. As the development of those simulation engines, researchers and game designers are
deploying the recent advances in the ﬁeld of artiﬁcial intelligence (AI) to train autonomous and intelligent agents which
grow from experimental laboratories into executable products [1].

However, even though learning-based algorithms have gradually increased their inﬂuence on training agents in virtual
environments, there is still a lack of the toolkit that connects the simulation environment and the state-of-the-art
developments in the AI community, including innovative tasks, comprehensive datasets, and powerful algorithms [2].

*These authors contributed equally to this work

 
 
 
 
 
 
arXiv Template

A PREPRINT

Figure 1: Toolkit overview. We present this tutorial for our new toolkit INDOORKIT. (1) INDOORKIT supports a wide
range of indoor scene datasets synthesized or designed manually, and it allows users to set up their custom dataset. (2)
We provide the data processing modules to store scenes that may be from various types of formats into the USD format,
making them transferable for other 3D software of game engines. (3) INDOORKIT can be connected with machine
learning models for downstream tasks. (Tasks including but not limited to character animation, physical simulation, and
robotics.

Recently, with the recent release of NVIDIA OMNIVERSE1, a scalable development platform for simulation and design
collaboration, researchers can deploy recent advances in AI in OMNIVERSE due to its indispensable features:

• Python has taken a lead in determining the programming language for AI or neural networks, and OMNIVERSE

exposes much of its functionality through Python bindings;

• In OMNIVERSE both physics simulation and the neural network policy training reside on GPU and communi-

cate directly [3];

• The universal scene description (USD) format in OMNIVERSE contains many details in 3D computer graphics
scene elements and is supported by a wide range of 3D modeling software (e.g. Blender and Autodesk Maya)
and game engines (e.g. Unreal Engine and Unity).

We present INDOORKIT: a toolkit built in NVIDIA OMNIVERSE that provides ﬂexible pipelines for scene building,
character animation, and robotic controls. The innovative features of our INDOORKIT include but not limited to:

• Photo-realistic scene rendering by utilizing a wide range of popular 3D assets (e.g. 3D-Front [4], iThor [5],

SAPIEN [6], AKB-48 [7], e.t.c.);

• Real-time character and robot control by leveraging 3D animatable assets(e.g. AMASS [8], Adobe Mixamo [9];

• Comprehensive and ﬂexible pipeline for data labeling, model training testing.

Besides, INDOORKIT is an early work related to OMNIVERSE and we hope that this work paves the way for the creation
of inﬂuential representation learning in the future. We also provide extensive documentation including a massive
amount of demos and tutorials to encourage related research.

2 Library overview

INDOORKIT is developed by the Center for Vision, Cognition, Learning, and Autonomy at the University of California,
Los Angeles. It enables easy data loading and experiment sharing for synthesizing scenes and animation with the
Python API in OMNIVERSE. This section brieﬂy describes several features of our INDOORKIT.

Working with 3D scene assets

We integrate multiple indoor scene datasets in INDOORKIT. For datasets with different parameterizations of the body,
we include documents for meta-data descriptions and visualization tools to illustrate the characteristics of each dataset.
Datasets covered include, but not limited to, 3D-Front [4], scenes of AI2Thor [5], and other indoor scene building
pipelines [8]. Besides, to facilitate a contribution to the community, INDOORKIT also provides detailed instructions for
users to upload and pre-process their custom datasets.

1https://developer.nvidia.com/nvidia-omniverse

2

arXiv Template

A PREPRINT

Why we choose Omniverse

There are several reasons for us to choose the newly platform: OMNIVERSE.

First, for its powerful simulation support: rigid body, soft body, articulated body, and ﬂuid are the main types of
simulation that are supported in OMNIVERSE.

• Rigid body: This type simulates the physics in a static environment. Users can set your mass and gravity for

each object and assign forces to objects such as friction or buoyancy.

• Soft body: based on the concept of Soft Body Dynamics (SBD), the soft body is controlled by mathematical
equations which describe how physical objects behave in the real world when they collide with other objects
or themselves. The problem with traditional computer graphics is that the physics simulation can only be done
by hand and it takes a lot of time to get an accurate result. With OMNIVERSE: soft body this task can be done
automatically.

• Articulation body: working with OMNIVERSE, INDOORKIT provides the tool that enables us to build physics
articulations such as robotic arms, kinematic chains, and avatars that are hierarchically organized. It also helps
us get realistic physics behaviors in the context of simulation for industrial applications.

• Fluid: OMNIVERSE allows you to simulate the behavior of liquids and gases. It’s based on the Navier-Stokes
equations, which describe how ﬂuids ﬂow in an environment. INDOORKIT connects the set up of ﬂuid with
indoor scene assets, which provides better ﬂexibility of custom tasks.

Second, for its Python scripting environment, it is easy to bring open-source and third-party Python libraries into
OMNIVERSE to help the research.

• Python is a general-purpose programming language that can be used for many different tasks. It has a simple
syntax and readable code, and it’s easy to learn. Python is also very popular in the industry, because of its high
performance, robustness, and versatility.

• Deep learning can be applied to many different ﬁelds from computer vision, speech recognition, natural lan-
guage processing (NLP), translation, robotics, and much more. Deep learning with Python using Pytorch [10]
and Tensorﬂow [11] libraries can be easily supported in OMNIVERSE.

Third, OMNIVERSE has the powerful rendering ability with the ray tracing technology.

• Ray tracing is a system that improves the lighting in the simulation. It cab be used in everything from
reﬂections to shadows both in the environment and scene elements including atmospheric effects, surfaces
reﬂections and even diffused lighting.

• Besides, users can render the scenes with the universal scene description (USD) format. This is a highly
compressed format that allows to store the entire scene information in a single ﬁle. It is also very simple to
use and understand. The basic idea behind USD is that it stores all of the information about how the scene is
rendered, including models, textures, lights, cameras, and animation.

3 Potential use case

Another essential goal of INDOORKIT is to apply the state-of-the-art embodied AI research while synthesizing
photo-realistic and physical-realistic rendering. Here, we list a few common use cases of our library. The full demo and
tutorial for them can be found at https://github.com/realvcla/VRKitchen2.0-Tutorial

Indoor scene labeling

Indoor scene labeling is the process of identifying and labelling indoor scenes. The goal of this project is to improve the
quality, quantity and consistency of indoor scene labels in an effort to facilitate more efﬁcient research use. The ﬁeld of
the EAI still lacks of meaningful and near-realistic indoor scenes. We apply OMNIVERSE platform to perform labeling
steps for indoor scenes. The sampled data piece contains the information of the game, and allows users to perform
downstream tasks. INDOORKIT also offers a clean interface for labeling scene and robot information.

3

arXiv Template

A PREPRINT

Figure 2: We also provide different randomization strategies for indoor scenes. (1) Scene/Background random-
ization: the same simulation task can share different backgrounds. (2) Material randomization: scene items can be
rendered as randomized materials. (3) Decoration randomization: colors and materials of the wall and ﬂoor can vary.
(4) Light randomization: simulation tasks can be performed under different lighting conditions.

Indoor character animation

With the recent progress in 3D character animation creation, the popularity of animation generation, as well as its
application, grows. Meanwhile, with the improvement of virtual environment simulation capability, researchers have
started to study agent behavior in the context of Embodied AI. However, the production process of adapting the
generated animation in a photo-realistic and physics-reliable environment is laborious. The key idea is to make a
balance between the original animation clip, physics scene, and social interaction meaning through reinforcement
learning on the kinematics.

Robotics oriented simulation

Robotics is a ﬁeld of study that involves the design, construction and operation of robots. The goal of robotics in
OMNIVERSE is to build machines capable of performing tasks that are difﬁcult or impossible for humans to perform.
Robotics can be used in many different ﬁelds including medicine, manufacturing, research and education.

4 Development and maintenance

The project is developed by the Center for Vision, Cognition, Learning, and Autonomy at University of California, Los
Angeles. INDOORKIT is developed publicly through Github with an issue tracker to report bugs and ask questions.
Documentation consists of tutorials, examples, and API documentation. The third-party packages include Pytorch [12]
for the deep learning framework, and Jupyter notebook [13]. For the demo and tutorial, please visit https://
vrkitchen20-tutorial.readthedocs.io/en/latest/..

5 Conclusion

We presented INDOORKIT, an Python library to help researchers to easily develop their indoor scene synthesizing
methods and apply the scene to EAI tasks.

References

[1] Masahiko Onosato and Kazuaki Iwata. Development of a virtual manufacturing system by integrating product

models and factory models. CIRP annals, 42(1):475–478, 1993.

[2] Haechan Park and Nakhoon Baek. Developing an open-source lightweight game engine with dnn support.

Electronics, 9(9):1421, 2020.

[3] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller,
Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation
for robot learning. arXiv preprint arXiv:2108.10470, 2021.

4

arXiv Template

A PREPRINT

[4] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia,
Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 10933–10942, 2021.

[5] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke
Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint
arXiv:1712.05474, 2017.

[6] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu
Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 11097–11107, 2020.

[7] Liu Liu, Wenqiang Xu, Haoyuan Fu, Sucheng Qian, Qiaojun Yu, Yang Han, and Cewu Lu. Akb-48: A real-world
articulated object knowledge base. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 14809–14818, 2022.

[8] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive
of motion capture as surface shapes. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 5442–5451, 2019.

[9] Adobe. Mixamo. 2020.
[10] Eli Stevens, Luca Antiga, and Thomas Viehmann. Deep learning with PyTorch. Manning Publications, 2020.
[11] Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton,
Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorﬂow distributions. arXiv preprint arXiv:1711.10604, 2017.
[12] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems, 32:8026–8037, 2019.

[13] Bernadette M Randles, Irene V Pasquetto, Milena S Golshan, and Christine L Borgman. Using the jupyter
notebook as a tool for open science: An empirical study. In 2017 ACM/IEEE Joint Conference on Digital Libraries
(JCDL), pages 1–2. IEEE, 2017.

5

