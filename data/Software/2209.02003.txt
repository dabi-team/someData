Rosetta: a container-centric science platform for
resource-intensive, interactive data analysis.

Stefano Alberto Russoa,c,∗, Sara Bertoccoa, Claudio Ghellerb, Giuliano Taﬀonia

aINAF (Italian National Institute for Astrophysics) – Astronomical Observatory of Trieste, Italy
bINAF (Italian National Institute for Astrophysics) – Institute of Radioastronomy, Bologna, Italy
cDepartment of Mathematics and Geosciences, University of Trieste, Italy

2
2
0
2

p
e
S
5

]

M

I
.
h
p
-
o
r
t
s
a
[

1
v
3
0
0
2
0
.
9
0
2
2
:
v
i
X
r
a

Abstract

Rosetta is a science platform for resource-intensive, interactive data analysis which runs user tasks as software containers. It is
built on top of a novel architecture based on framing user tasks as microservices - independent and self-contained units - which
allows to fully support custom and user-deﬁned software packages, libraries and environments. These include complete remote
desktop and GUI applications, besides common analysis environments as the Jupyter Notebooks. Rosetta relies on Open Container
Initiative containers, which allow for safe, eﬀective and reproducible code execution; can use a number of container engines and
runtimes; and seamlessly supports several workload management systems, thus enabling containerized workloads on a wide range
of computing resources. Although developed in the astronomy and astrophysics space, Rosetta can virtually support any science
and technology domain where resource-intensive, interactive data analysis is required.

Keywords: Science platforms, Data analysis, Reproducibility, Software containers, Big Data, HPC

1. Introduction

Data volumes are rapidly increasing in several research
ﬁelds, as in bioinformatics, particle physics, earth sciences, and
more. Next generation sequencing technologies, new particle
detectors, recent advances in remote sensing techniques and
higher resolutions in general, on both the instrumental and the
simulation side, are constantly setting new challenges for data
storage, processing and analysis.

Astrophysics is no diﬀerent, and the upcoming generation of
surveys and scientiﬁc instruments as the Square Kilometer Ar-
ray (SKA) [1], the Cherenkov Telescope Array (CTA) [2], the
Extremely Large Telescope (ELT) [3], the James Webb Space
telescope [4], the Euclid satellite [5] and the eROSITA All-Sky
Survey [6] will pile up on this trend, bringing the data vol-
umes in the exabyte-scale. Moreover, numerical simulations,
a theoretical counterpart capable of reproducing the formation
and evolution of the cosmic structures of the Universe, must
reach both larger volumes and higher resolutions to cope with
the large amount of data produced by current and upcoming
surveys. State of the art cosmological N-body hydrodynamic
codes (as OpenGADGET, GADGET4 [7] and RAMSES [8])
can generate up to 20 petabytes of data out of a single simu-
lation run, which are required to be further post-processed and
compared with observational data [9, 10, 11, 12].

The size and complexity of these new experiments (both ob-
servational and numerical) require therefore considerable stor-
age and computing resources for their data to be processed and

∗Corresponding author
Email address: stefano.russo@inaf.it (Stefano Alberto Russo)

analyzed, and possibly to adopt new approaches and architec-
tures. High Performance Computing (HPC) systems including
Graphical Processing Units (GPUs) and Field Programmable
Gate Arrays (FPGAs), together with the so called “bring com-
puting close to the data” paradigm are thus becoming key play-
ers in obtaining new scientiﬁc results [13], not only by reducing
the time-to-solution, but also by becoming the sole approach
capable of processing datasets of the expected size and com-
plexity.

In particular, even the last steps of the data analysis pro-
cesses, which could be usually performed on researchers’ work-
stations and laptops, are getting too resource-intensive and pro-
gressively required to be oﬄoaded to such systems as well.

Although capable of satisfying the necessary computing and
storage requirements, these systems are usually hosted in re-
mote computing centers and managed with queue systems, in
order to dynamically share their resources across diﬀerent users
and to optimize the workload and the throughput. This can
strongly complicate the user interaction, requiring remote con-
nections for shell and graphical access (as SSH and X pro-
tocol forwarding), careful data transfer and management, and
scheduler-based access to computing resources which strongly
limits interactive access to the system. Bringing along the soft-
ware required for the analysis can be even more challenging,
and without proper setup (in particular with respect to its de-
pendencies) it can not only fail to start or even compile, but
also severe reproducibility issues can arise [14].

To address these challenges, we see an increasing eﬀort in
developing the so called science platforms [15, 16, 17, 18]. A
science platform (SP) is an environment designed to oﬀer users
a smoother experience when interacting with remote computing

Preprint submitted to Astronomy and Computing

September 7, 2022

 
 
 
 
 
 
and storage resources, in order to mitigate some of the issues
outlined above.

In science and, more speciﬁcally, in astronomy, a number of

SPs have been designed and developed over the past years.

CERN SWAN [19] represents CERN’s eﬀort to build towards
the science platform paradigm. SWAN is a service for interac-
tive, web-based data analysis which makes Jupyter Notebooks
widely available on CERN computing infrastructure together
with a Dropbox-like solution for data management. However,
as of today, this solution does not provide support for appli-
cations other than the Jupyter Notebooks and a built-in shell
terminal, does not allow using custom or graphical software en-
vironments and requires heavy system-level integration in order
to be used on top of existent computing resources.

ESA Datalabs [20] is a science platform speciﬁc to astron-
omy and astrophysics. Similarly to CERN SWAN, it allows
users to work on ESA’s computing infrastructure using inter-
active computing environments as Jupyter Lab and Octave (or
to choose from pre-packaged applications as TOPCAT). Data-
labs is mainly focused on enabling users to gain direct access
to ESA’s datasets, it does not support using custom software
environments, and it is not an open source project.

The Large Synoptic Survey Telescope (LSST) developed a
similar science platform [21], based on a set of integrated web
applications and services through which the scientiﬁc commu-
nity will be able to “access, visualize, subset and analyze LSST
data”. The platform vision document does not mention applica-
tions other than the Jupyter Notebooks, nor support for custom
or graphical software environments, and refers to its own com-
puting architecture.

There are also a number of initiatives entirely focus on sup-
porting Jupyter Notebooks on cloud and HPC infrastructures
(such as [22], [23], [24] and [25]), which might fall in our SP
deﬁnition to some extent, and in particular in Astronomy and
Astrophysics it is worth to mention SciServer [18], Jovial [26]
and CADC Arcade [27].

Lastly, it has to be noted that the private sector is moving
fast with respect to resource-intensive and interactive data anal-
ysis, mainly driven by the recent advances in artiﬁcial intelli-
gence and machine learning. In this context, we want to cite
Google Colab [28] and Kaggle Notebooks [29], which are built
around heavily customised versions of the Jupyter Notebooks,
and Azure Machine Learning [30], which provides a nearly
full-optional SP speciﬁcally targeted at machine learning work-
ﬂows.

While on one hand all of the above mentioned SPs do make
it easier to access and use remote computing resources, on the
other, since they are mainly focused on web-based and inte-
grated analysis environments built on top Jupyter Notebooks or
similar software, they also introduce two main drawbacks:

1. users are restricted in using pre-deﬁned software pack-
ages, libraries and environments, which besides constrain-
ing their work can also lead to reproducibility issues, and
2. graphical software environments as remote desktops and
GUI applications are supported only to a limited extent, if
not completely unsupported.

Moreover, the deployment options for most of the SPs devel-
oped today rely on technologies originating from the IT indus-
try (e.g. Kubernetes) and require deep integration at system-
level, which is often hard to achieve in the framework of HPC
clusters and data-intensive system. This is not only because of
technological factors and legacy aspects, but also because of a
generalized pushback for exogenous technologies from some
parts of the HPC community [31, 32, 33, 34].

In this paper we present a science platform which aims at
overcoming these limitations: Rosetta. Built on top of a novel
architecture based on framing user tasks as microservices - in-
dependent and self-contained units - Rosetta allows to fully sup-
port custom software packages, libraries and environments, in-
cluding remote desktops and GUI applications, besides stan-
dard web-based analysis environments as the Jupyter Note-
books. Its user tasks are implemented as software containers
[35], which allow for safe, eﬀective and reproducible code ex-
ecution [36], and that in turn allows users to add and use their
own software containers on the platform.

Rosetta is also designed with real-world deployment scenar-
ios in mind, and thus to easily integrate with existing com-
puting and storage resources including HPC clusters and data-
intensive systems, even when they do not natively support con-
tainerization.

Although astronomy remains its mainstay (Rosetta has been
developed in the framework of the EU funded project ES-
CAPE1), Rosetta can virtually support any science and tech-
nology domain.

This paper is organized as follows. In Sections 2, 3 and 4, we
discuss the architecture of the Rosetta platform, its implementa-
tion and the security aspects. This is followed, in Section 5, by
an overview of the platform from a user prospective. Next, we
present the deployment and usage scenario in a real production
environment and a few use cases we are supporting (Section 6),
leaving the last section to conclusions and future work.

2. Architecture

Rosetta’s architecture is entirely designed to provide simpli-
ﬁed access to remote, dynamically allocated computing and
storage resources without restricting users to a set of pre-
deﬁned software packages, libraries and environments. It un-
folds in two main components: the platform architecture and
the task orchestration architecture.

The platform architecture follows a standard approach where
a set of services implement the various functionalities, and it
is schematized in Figure 1. These comprise a web application
service for the main application logic and the web-based UI,
a database service for storing internal data and a proxy service
for securing the connections. The web application service func-
tionalities can be further grouped in modules which are respon-
sible for managing the software containers, interacting with the

1ESCAPE aims to address the open science challenges shared by SKA,
CTA, KM3Net, EST, ELT, HL-LHC, FAIR as well as other pan-European re-
search infrastructures as CERN, ESO, JIVE in astronomy and particle physics.

2

computing and storage resources, orchestrating the user tasks,
handling the user authentication and so on.

In particular:

• Software functionalities allow to track the software con-
tainers available on the platform, their settings and con-
tainer registries2;

• Computing functionalities allow to interact with both stan-
dalone and clustered computing resources, hosted either
on premises (e.g. via Openstack) or on cloud systems (e.g.
on Amazon AWS);

• Storage functionalities allow browsing and operating on
local and shared ﬁle system (as Ext4, NFS, BeeGFS);

• Task functionalities allow submitting and stopping tasks as
well as viewing their logs, by interacting with the comput-
ing resources workload management systems (WMSs) as
Slurm and Kubernetes and/or their container engines (e.g.
Docker, Singularity, Podman);

• Account functionalities provide user account and proﬁle
management features including user registration, login and
logout, supporting both local and external authentication
(e.g. OpenID Connect, Shibbolet).

Figure 1: Rosetta main architecture. The ﬁrst level of abstrac-
tion consist in the proxy, database and web application services.
The web application service is further break down into its main
components (software, computing, storage, tasks and account)
together with their real world counterparts, some examples of
which are given in the right part of the ﬁgure.

Rosetta’s task orchestration architecture follows instead a
novel, microservice-oriented architecture [37] based on soft-
ware containers. Microservices [38] are independent, self-
contained and self-consistent units that perform a given task,
which can range from a simple functionality (e.g. serving a ﬁle
to download) to complex computer programs (e.g. classifying
images using a neural network). They are interacted with using

2A container registry is a place where container images are stored, which
can be public or private, and deployed both on premises or in the Cloud. Many
container registries can co-exisist at the same time.

3

a speciﬁc interface, usually a REST API over HTTP, which is
exposed on a given port. Microservices ﬁt naturally in the con-
tainerisation approach, where each microservice runs in its own
container, isolated from the underlying operating system, net-
work, and storage layers. User tasks in Rosetta are thus always
executed as software containers, and treated as microservices.
Rosetta can therefore stay agnostic with respect to the task in-
terface, some examples of which include a Jupyter Notebook
server, a web-based remote desktop or a virtual network com-
puting (VNC) server, but also a secure shell (SSH) server with
X protocol forwarding is a perfectly viable choice.

One of the main features of this approach, where user tasks
are completely decoupled from the platform, is to make it pos-
sible for the users to add their own software containers. There
is indeed no diﬀerence between “platform” and “user” contain-
ers, as long as they behave as a microservice. Rosetta users can
thus upload their own software containers on a container reg-
istry, add them in the platform by setting up a few parameters
(as the container image and the interface port), and then use
them for their tasks.

In order to make use of this architecture for user tasks or-
chestration, Rosetta needs to be able to submit to the computing
resources a container for execution, and to know how to reach
it (i.e. on which IP address). These functionalities are standard
and built-in in most modern container orchestrators (e.g Kuber-
netes), however as mentioned in the introduction Rosetta has
been designed to also support computing resources not natively
supporting containerized workloads (e.g. HPC clusters and
data-intensive systems). On these computing resources, also
depending on the WMS and container engine used, some key
features might not be available, as full container-host ﬁlesys-
tem isolation, network virtualization and TCP/IP traﬃc routing
between the containers. To work around these missing features,
Rosetta relies on an agent, which is a small software component
in charge of helping to manage the task container life cycle. Its
main features comprises setting up the environment for the con-
tainer execution, managing dynamic port allocation, reporting
the host IP address to the platform, and running the container
itself. The agent internal logic is described more in detail in
section 3.4.

When a container is started, its interface has to be made ac-
cessible by the user. This is achieved ﬁrst by making the inter-
face port reachable on the internal network between the com-
puting resource and Rosetta, and then by exposing it to the out-
side world through Rosetta itself, thus making it accessible by
the user. The ﬁrst step can make use of simple TCP/IP tun-
nels as well as more sophisticated techniques usually available
in modern container orchestrators and WMSs, while the second
one can be accomplished either by directly exposing the task
interface as-is or by relaying on a proxy service, which also
allows to enforce access control and connection encryption.

Once tasks are executed and their interfaces made accessible,
no further operations are required, and the users can be looped
in. A diagram of this ﬂow is presented with two examples:
the ﬁrst using a WMS supporting containerized workloads with
direct connection to the task interface (Figure 2), the second
using the agent to run the task container and relaying on the

proxy for connecting to the task interface (Figure 3).

grouping introduced in section 2: Software, Computing, Stor-
age, Tasks and Account.

Figure 2: Rosetta user task orchestration using the computing
resource’s WMS and a direct connection to the task interface
through a TCP/IP tunnel.

Figure 3: Rosetta user task orchestration using the agent and
the proxy service on top of a TCP/IP tunnel for connecting to
the task interface.

3. Implementation

Rosetta is entirely built using open-source technologies, in
particular Python and the Django web framework, and re-
leased as an open source project3. Other technologies include
HTML and JavaScript for the UI, Postgres for the database4
and Apache for the proxy. The platform services (not to be
confused with the user tasks software containers) are container-
ised using the Docker engine and using Docker Compose as
the default orchestrator5. Besides the web application, database
and proxy services, Rosetta includes an optional container reg-
istry service, which can be used to store software containers lo-
cally, and a test Slurm cluster service for testing and debugging.
Rosetta deployment tools provide a set of management scripts
to build, bootstrap and operate the platform and a logging sys-
tem capable of handling both user-generated and system errors,
exceptions and stack traces.

The web application functionalities are handled with a com-
bination of Django object–relational mapping (ORM) models
and standard Python functions and classes. The ORM schema,
which represents how the ORM models are actually stored in
the database, is summarized in Figure 4. In the following sub-
sections we will describe their implementation according to the

3https://www.ict.inaf.it/gitlab/exact/Rosetta
4The database service can be replaced by any other database supported by

Django.

5Other orchestrators can be supported as well, e.g. Kubernetes.

4

Figure 4: The Rosetta Django ORM schema, showing the var-
ious models and their relationships. Some minor and less rel-
evant models as the user proﬁle, the login tokens and the key
pairs have been excluded for the sake of simplicity.

3.1. Software

Software lives in Rosetta only as software containers. Soft-
ware containers are represented using a Django ORM model
which acts as a twin of the “real” container, providing metadata
about the container itself. Rosetta relies on Open Container
Initiative (OCI) containers, which must be stored on an OCI-
compliant container registry.

The Container ORM model has

a name and a
description ﬁelds to represent the container on Rosetta, and
a series of attributes to identify its image: the registry (to set
on which container registry it is hosted), the image_name (to
locate it on the registry) and the image_tag (to set a speciﬁc
container version).

The image_arch, image_os and image_digest attributes
provide instead more ﬁne-grained control in order to uniquely
identify the image, and should be used in production environ-
ments. A container image is indeed uniquely identiﬁed on an
OCI registry only if using, besides its name, either a triplet of
tag, architecture and OS or an image hash digest (usually gen-
erated with SHA-256). This is because on OCI registries, mul-
tiple images can be assigned to the same tag, in order to enable
multi-OS and multi-architecture support. Moreover, it has also
to be noted that while a tag can be re-assigned, a digest is an
immutable identiﬁer and ensures reproducibility.

Containers can be registered in Rosetta as platform contain-
ers or user containers. A platform container is not associated
with a speciﬁc user and thus available for all of them, while a
user container belongs to and is accessible by a specif user only,
according to its user attribute. Containers can also be shared
within (and made accessible only to) a speciﬁc group.

An interface_port attribute lets Rosetta know on
which port the container will expose its interface, and the
interface_protocol sets the corresponding protocol (e.g.
HTTP, SSH, VNC etc.). The interface_transport (de-
faulted to TCP/IP) can be used to cover non-standard scenarios
(e.g. if using UDP).

Since as explained in Section 2, the container interfaces are
made accessible to the outside world, they need to be secured.
For this to happen, Rosetta allows to setup a one-time pass-
word or token at task creation-time to be used for accessing
the task interface afterwards. Task interfaces can get password-
protected in two ways: by implementing a password-based au-
thentication at task-level, or by delegating it to the proxy ser-
vice. In the ﬁrst case, the container must be built to support
this feature and must be registered on the platform with the ex-
tra supports_interface_auth attribute set to True. Rosetta
can then forward the password or token to the container via an
environment variable.
Instead, if the container makes use of
an HTTP-based interface, it can delegate its access control to
the proxy service, and just expose a plain, unprotected interface
over HTTP. In this case, Rosetta will setup the proxy service in
order enforce user authentication when accessing the task inter-
face, and encrypt it using SSL. Delegating the task authentica-
tion to the proxy service is the default method for HTTP-based
interfaces, since it is far more secure than leaving the authenti-
cation to be implemented at task-level, as it will be discussed in
Section 4.

In order to support container engines missing port mapping
capabilities, Rosetta provides a mechanism to let containers re-
ceive instructions on which port to start their interface on. As
already mentioned in Section 2, while most industry-standard
container engines can autonomously manage TCP port map-
ping between containers and their host to avoid conﬂicts with
ports already allocated (either by another service, by another
container or by another instance of the same container), some
of them cannot (e.g. Singularity).
In this case, the Rosetta
agent can provide a speciﬁc port to the container where to
make its interface to listen on, which is chosen between the
free ephemeral ports of the host and passed to the container
via an environment variable. To let Rosetta (and the agent)
know that a given container supports this mechanism, its extra
attribute supports_custom_interface_port must be set to
True (and the interface_port attribute is then discarded).

Rosetta comes with a number of a base containers for GUI
applications, generic remote desktops and Jupyter Notebooks
which can be easily extended to suit several needs:

• JupyterNotebook, the oﬃcial Jupyter Notebook container

extended to support custom interface ports;

• GUIApplication, a container built to run a single GUI ap-

plication with no desktop environment;

• MinimalDesktop,

a desktop environment based on
Fluxbox where more than one application can be run in
parallel;

• BasicDesktop, a desktop environment based on Xfce for

tasks requiring common desktop features as a ﬁle manager
and a terminal.

The GUIApplication and Desktop containers make use of
KasmVNC, a web-based VNC client built on top of modiﬁed
versions of TigerVNC and NoVNC which provides seamless
clipboard sharing between the remote application or desktop
and the user’s local desktop environment, as well as support-
ing dynamic resolution changes in order to always ﬁt the web
browser window, that are essential features in the everyday use.

3.2. Computing

Computing resources are divided in two main types: stan-
dalone and clusters. The ﬁrst ones may or may not have a WMS
in place, while the second ones always do. If a computing re-
source has no WMS, the task execution is synchronous, other-
wise the execution is asynchronous and the tasks are queued.

The Django ORM model class used to represent computing
resources is named Computing, and it includes a type, a name
and a description ﬁelds for identifying a speciﬁc comput-
ing resource within Rosetta. A set of attributes describe how
to access it and to submit user tasks: the access_mode spec-
iﬁes how the computing resource is accessed (i.e. over SSH,
using a command line interface (CLI), or a set of APIs); the
auth_mode speciﬁes how the platform gets authorized on the
computing resource; the wms speciﬁes the WMS in place (or
if there is none) and the container_engine speciﬁes which
container engines (and runtimes) are available. With respect
to the container_engine, if the WMS natively supports con-
tainerized workloads and there is no need of running tasks using
a speciﬁc container engine or runtime, then it can be just set to
the value “internal”.

Some example combinations of these attributes are reported
in Table 1, where each row corresponds to a physical computing
resource. The ﬁrst row represents a classic HPC cluster using
Slurm as WMS and Singularity as container engine, and requir-
ing an accredited cluster user to submit tasks over SSH using
the Slurm command line interface. The second row represents
the same cluster but supporting, besides Singularty, also the
Docker engine with both runC and Kata runtimes, in order to al-
low Rosetta (or its users) to chose the best one for a given task.
The third row represents yet the same cluster but accessed over
Slurm REST APIs using JSON web tokens (JWT) for authen-
tication. The fourth and ﬁfth rows represent instead standalone
computing resources, using the Docker container engine, and
accessed using SSH as a standard user for the fourth and the
Docker REST APIs with a platform certiﬁcate for the ﬁfth. The
sixth, seventh and eight rows all use computing resources man-
aged with Kubernetes, and in the eight row the container run-
times available within Kubernetes are explicitly stated. The last
row is instead an example using Fargate, an hosted container
execution service from Amazon Web Services (AWS) built on
top of their Elastic Container Service (ECS), and accessed us-
ing its proprietary APIs.

When deploying Rosetta on a commercial cloud infrastruc-
ture as AWS or Google Cloud Platform (GCP), there are two

5

options. The ﬁrst one is to treat such infrastructures as trans-
parent, and simply use standard (i.e. not proprietary) access
modes as SSH, Slurm, or Kubernetes. In this case there is no
diﬀerence between using Rosetta with computing resources de-
ployed on premises or on such commercial cloud systems. The
second option is to instead integrate at a deeper level, using
AWS or GCP proprietary APIs and/or clients to automatically
start new virtual machines upon request, or to use some of their
native scheduling systems, as the last example of table 1.

The implementation work to support all of the combina-
tions of access and authentication modes, container engines and
WMSs is still ongoing, as we privileged SSH and Slurm since
they ﬁt well in the application scenarios we encountered so far.
However, we wanted to lie down a general framework in order
to easily expand the platform in future.

The Computing model describes the computing resource ar-
chitectures as well, and in particular the arch attribute de-
ﬁnes the native architecture (e.g.
the
supported_archs attribute lists extra supported architectures
(e.g. 386 on amd64 architectures) and the emulated_archs
attribute lists the architectures that can be emulated.

amd64, arm64/v8),

Computing resources can be also assigned to a speciﬁc group
of users, using the group attribute which, if set, restricts ac-
cess to the group members only, and the conf attribute can
be used to store some computing resource-speciﬁc conﬁgura-
tions (e.g.
the host of the computing resource). Lastly, the
Computing ORM model implements an additional manager
property which provides common functionalities for accessing
and operating on the real computing resource, as submitting
and stopping tasks, viewing their logs, and executing generic
commands. This property is implemented as a Python function
which upon invocation instantiates and returns an object sub-
classing the ComputingManager class, based on the computing
resource type, access_mode, auth_mode and wms attributes.
Computing resources which are accessed using SSH can be
accessed both as a standard user (using its account on the com-
puting resource) or using a superuser (e.g. a “platform” user),
depending on the deployment requirements.

In order to access using a standard user on the computing
resource, Rosetta generates a dedicated private/public key pair,
the public key of which is required to be added on the com-
puting resource account by the user. To instead access using a
“platform” superuser (and thus using the same user for orches-
trating all of the user tasks), a dedicated account and key pairs
are required to be setup both on the computing resource and
within Rosetta.

Accessing computing resources using SSH requires no in-
tegration with the existent infrastructure at all, provided that
standard SSH is available and a container engine is installed.
For this reason, it perfectly ﬁts our requirement of operating on
HPC clusters and data-intensive systems where more complex
integrations are hard to achieve.

3.3. Storage

attributes, which include a name, a type, an auth_mode and
an access_mode. If a storage is attached to a computing re-
source, then the computing attribute can be set. In this case, if
the storage and the computing resource share the same access
mode, the access_through_computing option can be ticked
so that Rosetta can just use the computing resource one. The
group attribute, if set, speciﬁes the set of users authorized to ac-
cess the storage. The base_path attribute sets the internal path
the $USER,
to the storage, and supports using two variables:
which is substituted with the Rosetta internal user name, and
the $SSH_USER, which is substituted with the SSH username (if
the access method is based on SSH). The bind_path sets in-
stead where the storage is made accessible within the software
containers. If a data storage is attached to a computing resource
and its bind_path is set, it will be then made accessible from
all of the containers running on that computing resource, under
the location speciﬁed by the bind_path.

For example, a storage mounted on the /data mount point of
an SSH-based computing resource (and represented in Rosetta
using generic_posix as type and SSH+CLI as access method)
could have a base_path set to /data/users/$USER and a
bind_path set to /storages/user_data, in order to sepa-
rate data belonging to diﬀerent users at orchestration-level.

At the moment only POSIX ﬁle systems are supported, which
must be mounted on the various computing resources and that
are in turn exposed inside the containers using the standard
binding mechanism oﬀered by most container engines. Any
ﬁlesystem that can be mounted as such (e.g using FUSE) is
therefore automatically supported, as CephFS or Amazon S3.

We envision adding support for other storage types in future
releases, as for example object storages, but in this case access-
ing the storage APIs is up to the application running inside the
container, and Rosetta can only act as a ﬁle manager. How to
provide access in a standardized way to non-POSIX ﬁle systems
within containers is indeed still an open theme.

Storage functionalities also include a set of APIs to provide
support for the ﬁle manager embedded in the Rosetta web-
based UI, which is built on top of the Rich File Manager 6 open
source project. These APIs implement common functionalities
(as get, put, dir, rename etc.) to perform ﬁle management oper-
ations, the internal logic of which depends on the storage type,
making it easy to expand them in the future.

3.4. Tasks

Tasks are represented using an ORM model and a set of states
(queued, running or stopped). Tasks running on computing re-
sources without a WMS are directly created in the running state,
while when a WMS is in place they are created in the queued
state and set as running only when they get executed. States
are stored in the state attribute of the Task model, which also
includes a name and the links with the software container and
the computing resource executing the task, plus its options (the
container, computing and computing_options attributes,
respectively). A set of other attributes as the interface_ip,

Storage functionalities provide a way of deﬁning, mounting
and browsing data storages. A Storage is deﬁned by a set of

6https://github.com/psolom/RichFilemanager

6

Computing resource #1
Computing resource #2
Computing resource #3
Computing resource #4
Computing resource #5
Computing resource #6
Computing resource #7
Computing resource #8
Computing resource #9

access_mode
SSH+CLI
SSH+CLI
API
SSH+CLI
API
CLI
SSH+CLI
API
API

wms
Slurm
Slurm
Slurm
none
none

auth_mode
user keys
user keys
JWT
user keys
platform cert.
platform cert. Kubernetes
platform keys Kubernetes
platform cert. Kubernetes
platform cert.

Fargate

container_engines
Singularity
Docker[runC,Kata],Singularity
Docker,Singularity
Docker
Docker
internal
internal
internal[runC,Kata]
internal

Table 1: Examples of various combinations of computing resource attributes. In order to schedule containerized workloads on a
given computing resource, Rosetta needs to know how to access it (access_mode), how to get authorized (auth_mode), if and
what WMS to use (wms), and which container engines are available (container_engines), possibly with their runtimes.

interface_port, tcp_tunnel_port and auth_token let
Rosetta know how to instantiate the connection to the task (i.e.
for setting up the tunnel and/or conﬁguring the proxy service).
Once a task starts on a computing resource, its IP address and
port are saved in the corresponding Task ﬁelds, and the task is
If the task was queued, an email is sent
marked as running.
to the user with a link to the task, which is particularly useful
to let users immediately know when their tasks are ready, thus
preventing to waste computing time on shared systems. Task
functionalities also include opening the TCP/IP tunnel to the
task interface port and/or conﬁguring the HTTP proxy service
in order to provide access to the task interface.

One of the main components of the task management func-
tionalities is the agent, which as introduced in Section 2 allows
to seamlessly support both WMSs not natively supporting con-
tainerized workloads and container engines missing some key
features.
In other words, it makes all of the computing re-
sources behave in the same way from a Rosetta prospective.
The agent is implemented as a Python script which is served
by the Rosetta web application and that can run both as a supe-
ruser and as a standard, unprivileged user. When it is required,
Rosetta delivers a bootstrap script on the computing resource
which pulls and executes the agent code. As soon as it gets
executed, the agent calls back the Rosetta web application and
communicates the IP address of its host. If the agent landed on
a computing resource using a container engine missing the dy-
namic port mapping feature, then it also searches for an avail-
able ephemeral TCP/IP port and communicates it to the web
application as well. Lastly, the agent sets up the environment
for the user task container, and starts it.

3.5. Account

Account and proﬁle functionalities provide support for both
local and external authentication services (e.g. Open ID con-
nect). The accounts linking between local and external identi-
ties is based on the user email address, which is the standard
approach in this kind of services.

Local and external authentication can co-exist at the same
time, provided that if a user originally singed up using an ex-
ternal authentication service it will be then always required to
log-in using that service. If allowing to register as local users

or to entirely rely on external authentication is up to the admin-
istrators, and can be conﬁgured in the web application service.
Rosetta provides both user-based and group-based authoriza-
tion, so that computing and storage resources, as well as soft-
ware containers, can be made available to speciﬁc users or sub-
sets of users only.

The user proﬁle also supports some user-based conﬁgura-
tion parameters for accessing the computing resources (e.g. the
computing resource username if using an SSH-based access
mode with user keys). Other minor functionalities, as password
recovery, login tokens and time zone settings are provided as
well.

4. Security

Security of computing systems and web applications is a
wide chapter and an extensive discussion on the topic is be-
yond the scope of this article, however we wanted to mention
the main issues and how we took them into account.

The ﬁrst layer of security in Rosetta consists in using soft-
ware containers for the user tasks. The base executable unit in
Rosetta is indeed the container itself, meaning that users has no
control outside of their containers at all: once a container is sent
for execution and Rosetta handles all the orchestration, the user
is dropped inside it and cannot escape.

For this reason, even if a container gets compromised, all the
other ones as well as the underlying host system does not get af-
fected. However, this statement is true in the measure of which
the container engine can guarantee isolation and prevent privi-
lege escalation. The Docker engine has an intrinsic issue with
this respect, as it makes use of a daemon running with superuser
privileges. Podman, a nearly drop-in replacement for Docker,
runs instead in user-space and prevents this kind of issues by
design, as well as Singularity. Other container engines as gVi-
sor and Kata push security even further, providing respectively
kernel and hardware virtualization.

Moreover, when Rosetta is integrated on computing re-
sources using SSH-based access, the administrators can opt for
revoking direct SSH user access on them, leaving Rosetta - and
its containerized tasks - the only access point, thus greatly im-
proving overall security.

7

With respect to potential malicious software, the ﬁrst line of
defense usually takes place in the container registry. Docker
Hub, for example, has a built-in security scanning system, and
there are a number of free and open source scanners that can be
used for on-premise container registries as Klar/Clair7.

Scanning for malicious software can also be done when ex-
ecuting task containers8, but not all container engines support
this feature. Allowing only containers coming from registries
which run security scanning, or to implement these checks
along the building pipeline could be the best approach to protect
against malicious software in container images [39].

For what concerns software packages that can be installed at
runtime inside the containers, Rosetta does not do any check-
ing as it would be technically very hard if not even impossible.
This is a common issue when giving users the freedom to down-
load and execute code, including on commercial platforms as
Google Colab and Kaggle. Even restricting user permissions
would not prevent such issue, given that these packages can be
always just downloaded and executed from a diﬀerent location
(e.g. a temporary folder). Having users to download and exe-
cute malicious software by mistake is therefore something very
hard to prevent, and that has no simple mitigation approach un-
less relying on classic antivirus software which should run in-
side the containers.

As introduced in Section 2, since Rosetta user task interfaces
are made accessible to the outside world, they are required to
be secured, both in term of access control and connection en-
cryption. With this respect, it is necessary to make a distinction
between HTTP-based and generic task interfaces. HTTP-based
task interfaces can rely on the authentication and SSL encryp-
tion provided by the proxy service, and can therefore just use a
plain HTTP protocol. Generic task interfaces (e.g. a VNC or X
server) are instead required to be secured at task-level, and it is
responsibility of the task container to enforce it. As explained
in subsection 3.1, access control setup is in this case achieved
by forwarding to the task a one-time password set by the user
at task creation-time, which is then to be used by the container
interface to authenticate the user. Encryption has to be setup at
task-level too, and can be provided in ﬁrst instance using self-
signed certiﬁcates, or implementing more complex solutions as
dynamic certiﬁcates provisioning.

An important detail in the task security context is that Rosetta
makes a strong distinction between standard and power users,
through a status switch in their proﬁle. By default, only the
latter can setup custom software containers using generic task
interface protocols other than the HTTP, since handling secu-
rity at task level (which is always required in this case) is error-
prone and must be treated carefully. Standard users can there-
fore add and use custom software containers for their tasks on
the platform only if using an HTTP-based interface, which is in
turn forced to be secured by the proxy service.

For what concerns the tunnel from the web application ser-
vice to the tasks, this is protocol-agnostic (above the TCP/IP

7https://github.com/optiopay/klar
8https://docs.docker.com/engine/scan/

8

transport layer) and is either accomplished by a direct connec-
tion on a private and dedicated network (e.g.
if using Kuber-
nets) or using an SSH-based TCP/IP tunnel using users’ pub-
lic/private keys, as explained in Section 2, and thus assumed
safe.

In terms of web security, we considered potential security
risks originating from cross-site request forgery (CSRF), cross-
origin resource sharing (CORS), cross-site scripting (XSS), and
similar attacks. The same origin policy (SOP) of modern web
browsers is already a strong mitigation for these attacks, and
all the platform web pages and APIs (with a few exceptions for
internal functionalities) uses Django’s built-in CSRF token pro-
tection mechanism. However, the SOP policy has limitations
[40, 41], in particular in our scenario where users can run cus-
tom (and possibly malicious) JavaScript code from within the
platform, either using the Jupyter Notebooks or by other means
(e.g. by setting up a task serving a web page).

We therefore focused on isolating user tasks from the rest
of the platform even on the web browser side. Using the
same domain for both the platform and the user tasks (e.g.
https://rosetta.platform/tasks/1 is indeed deﬁnitely
not a viable solution as it does not allow to enforce the SOP
policy at all. Also using dedicated subdomains (e.g. https:
//task1.rosetta.platform) has several issues, in particu-
lar involving the use of cookies [42, 43, 44].

The secure-by-design, safe solution is to serve user tasks
from a separate domain (e.g. rosetta-tasks.platform).
Then, each task can have its own subdomain (as https://
task1.rosetta-tasks.platform) and stay separated form
the main platform domain. However, handling and secur-
ing subdomains like this requires wildcard DNS services and
SSL certiﬁcates, which for many institutional domains are not
available [45], including ours. For this reason, in Rosetta we
opted for an intermediate solution: we serve user tasks from
a separate domain (e.g. rosetta-tasks.platform) assign-
ing each of them to a diﬀerent port, under the same SSL cer-
tiﬁcate.
In this way, the URL to reach the task number 1 at
https://rosetta-tasks.platform:7001 can be secured
by the same SSL certiﬁcate covering the URL for task num-
ber 2 at https://rosetta-tasks.platform:7002, but are
treated as diﬀerent origins by web browsers. SSL certiﬁcates
are indeed port-agnostic, while the SOP (which basically in-
volves the triplet protocol, host and port for deﬁning the ori-
gin) it is not, thus enabling web browsers to enforce it between
the task 1 and 2, and in general securing all of the users tasks
against each others. While this approach might lead to some
issues with institutional ﬁrewalls blocking external port access
beyond the standard 80 and 443 ports, we found it to be the
right compromise in our environment. Moreover, switching to
serving each task from its own subdomain is just a matter of a
quick change in the Rosetta proxy service conﬁguration.

5. User Experience

From a user prospective, Rosetta presents itself as a web
application with a web-based user interface (UI) that is shown
upon user login in Figure 5. The UI, following the architecture

presented in section 2, is organised in ﬁve main areas:
the
Software section, where to browse for the software containers
available on the platform or to add custom ones; the Computing
section, where to view the available computing resources;
the Storage section, which provides a ﬁle manager for the
various data storages; the Tasks dashboard, where to manage
and interact with the user tasks, including connecting with
them and viewing their logs; and the Account pages, where to
conﬁgure or modify user credentials and access keys.

Figure 6: Software containers list. For each software entry, a
brief description is provided, together with the container image
name and a menu from which to select a speciﬁc version. The
”play” button will start a new task with the given software.

Figure 5: The Rosetta science platform main page and menu.

To run a typical analysis task, the user ﬁrst accesses the Soft-
ware section (Figure 6) in order to choose (or add) the desired
software container. If adding a new software container, the user
has to set its registry, image name, tag, the container interface
port and protocol, plus some optional advanced attributes (Fig-
ure 7). The new container will then be listed together with the
other ones so that the can be chosen for execution.

Once the software container is chosen, the user hits the
“play” button to create a new task. The platform will then
ask the user on which computing resource to run the task, and
to set a task name. A one-time password token is also gen-
erated, which is usually automatically handled by Rosetta and
not required to be entered manually when connecting to the task
(Figure 8). For some computing resources, extra options as the
queue or partition name, CPU cores and memory requirements
can be set as well. The task is then created and submitted.

As soon as the task is starting up on the computing resource,
a “connect” button in the task dashboard becomes active. At
this point, the user can connect to the task with just one click:
Rosetta will automatically handle all the tunneling required to
reach the task on the computing resource where it is running,
and drop the user inside it (Figures 9 and 10)

Users can transfer ﬁles to and from the data storages (and
thus the tasks) using the built-in ﬁle manager (Figure 11),
which is an eﬀective solution for light datasets, analysis
scripts, plots and results. Larger data sets are instead supposed
to be already located on a storage, either because the data
repository is located on the storage itself (in a logic of bringing
the computing close to the data) or because they have been
previously staged using an external procedures.

Figure 7: Adding a new software container. Besides a name
and a brief description, the key ﬁelds are the container registry,
image and tag, plus the port and protocol of its interface.

6. Deployment and use cases

Rosetta is deployed in production at the computing center of
INAF - Osservatorio Astronomico di Trieste [46], using an ex-
tranal, aggregated authentication system named RAP [47] and
serving a set of diﬀerent users with diﬀerent software require-
ments.

To support our user community, we oﬀer a pre-deﬁned port-
folio of containerized applications that span from generic data
analysis and exploration tools (as iPython, R and Julia) to
speciﬁc Astronomy and Astrophysics codes. These include
common astronomical data reduction software and pipelines as
IRAF, CASA, DS9, Astropy, but also Cosmological simulation
visualization and analysis tools, and project-speciﬁc applica-
tions and codes. All of them are listed in the Software section
of Rosetta and are accessible from the users’ web browsers by
running a task instance.

9

Figure 8: Last step of new task creation, after selecting the soft-
ware container and a computing resource. The interface asks to
enter a task name and possibly other task parameters as the re-
quired number of CPUs, memory, or queue name.

Figure 11: The Rosetta built-in ﬁle manager, which allows for
browsing data storages and to upload or download data ﬁles.
While not suitable for large datasets, it is an eﬀective tool for
lighter ones as well as analysis scripts, plots and results.

In the following we discuss more in detail four diﬀerent
use cases among the various projects we support: the LOFAR
pipelines, the SKA data challenges, the Astrocook quasar spec-
tral analysis software, and the HPC FPGA bitstream design.

6.1. The LOFAR pipelines

The software collection for the LOFAR community consists
in a set of tools and pipelines used to process LOFAR data, as
the Prefactor and DDFacet data reduction codes [48], for which
we created a set of software containers.

A typical run of the LOFAR data processing pipelines holds
for several days, and requires signiﬁcant computing resources
(in terms of RAM, CPUs and Storage) to process terabytes of
data (∼ 15TB). Several checks are necessary during a pipeline
run to verify the status of the data processing and the conver-
gence of the results.

In this context, we are using Rosetta to run the pipelines
within a software container that provides both the pipelines
themselves and visual tools to check the status of the process-
ing phase. Rosetta tasks run on an HPC cluster managed using
the Slurm WMS, which allocates a set of resources in terms of
RAM and CPUs as requested by the scientists in the task cre-
ation phase. These tasks compete with other standard Slurm
jobs running on the cluster, thus ensuring an optimized alloca-
tion of the available resources among all users.

Scientist running the pipelines in this mode are not required
to interact with the Slurm WMS or to manually deploy any soft-
ware on the cluster, instead they can just rely on Rosetta and
update the containers with new software if necessary.

The container source codes are available online as part of the
LOFAR Italian collaboration 9 and once built are registered to
an INAF private container registry in order to account for both
public and private codes as required by the diﬀerent LOFAR
Key Projects collaborations.

9https://www.ict.inaf.it/gitlab/lofarit/containers

10

Figure 9: A Rosetta user task running a GUI application from
the CASA suite, in a remote desktop environment. The remote
desktop server is web-based, and supports dynamic resolution
changes and seamless clipboards sharing with the client, allow-
ing for a smooth user experience.

Figure 10: A Rosetta user tasks running a Jupyter Notebook,
displaying a plot using Numpy and Matplotlib. The authenti-
cation for the Notebook server is handled by the Rosetta proxy
service, which also secures the connection over SSL.

6.2. The SKA data challenges

INAF participated in the SKA Data Challenges10 as infras-
tructure provider. The purpose of these challenges is to allow
the scientiﬁc community to get familiar with the data that SKA
will produce, and to optimise their analyses for extracting sci-
entiﬁc results from them.

The participants in the second SKA Data Challenge analysed
a simulated dataset of 1 TB in size, in order to ﬁnd and char-
acterise the neutral hydrogen content of galaxies across a sky
area of 20 square degrees. To process and visualize such a large
dataset, it was necessary to use at least 512 GB of RAM, and
INAF oﬀered a computing infrastructure where such resources
were available.

We used Rosetta to provide simpliﬁed access to this com-
puting infrastructure (an HPC cluster managed using the Slurm
WMS) and, as for the LOFAR pipelines use case, we provided
a software container that provided all of the tools and applica-
tions necessary to complete the challenge (as CASA, CARTA,
WSClean, Astropy and Soﬁa) in a desktop environment.

Most notably, users were able to ask for speciﬁc computing
resource requirements when starting their analysis tasks (512
GB of RAM, in this case), and the cluster parallel ﬁle system
used to store the dataset provided high I/O performance (> 4
GB/s) and plenty of disk space, so that users could focus on the
scientiﬁc aspects of the challenge and not worry about orches-
tration and performance issues.

6.3. The Astrocook quasar spectral analysis software

Astrocook[49] is a quasar spectral analysis software built
with the aim of providing many built-in recipes to process
a spectrum. While this software is not necessarily resource-
intensive in general, it can require quite relevant computing
power in order to apply the various recipes.

Astroccok comes as a GUI application with some common
and less common Python dependencies which are sometimes
hard to install (as Astropy, StatsModels and wxPython) and it
is a great example about how to use Rosetta in order to provide
one-click access to a GUI application which might require some
extra computing power.

Figure 12 shows Astrocook running in a Rosetta task on a
mid-sized, standalone computing resource, and accessed using
the web-based remote desktop interface.

6.4. The HPC FPGA bitstream design

Field Programmable Gate Arrays (FPGAs) can be used as
accelerators in the context of physics simulations and scientiﬁc
computing and they have been adopted as a low-energy accel-
eration devices for exascale testbeds.

One of these testbeds is ExaNeSt’s (European Exascale Sys-
tem Interconnect and Storage) prototype [50], a liquid-cooled
cluster composed by proprietary Quad-FPGA daughterboard
computing nodes, interconnected with a custom network and
equipped with a BeeGFS parallel ﬁlesystem.

10https://sdc2.astronomers.skatelescope.org/sdc2-challenge

11

Figure 12: The Astrocook quasar spectral analysis software
running in a Rosetta task on a mid-sized computing resource.
The ”Spectrum” and ”Spectrum detail” windows are the main
components of Astrocook, while the ”Sessions” and ”System
table” windows recap the analysis steps and parameters.

To use this cluster it is necessary to re-engineer codes and
algorithms [51, 52, 53]: the substantial programming eﬀorts re-
quired to program FPGAs using the standard approach based
on Hardware Description Languages (HDLs), together with its
subsequent weak code portability have long been the main chal-
lenges in using FPGA-enabled HPC clusters as the ExaNeSt’s
prototype.

However, thanks to the High Level Synthesis (HLS) ap-
proach, FPGAs can be programmed using high level languages,
thus highly reducing the programming eﬀort and and greatly
improving portability. HLS tools use high level input languages
as C, C++, OpenCL and SystemC which, after a process in-
volving intermediate analysis, logic synthesis and algorithmic
optimization, are translated into FPGA-compatible code as the
so called “bitstream” ﬁles.

This last step in particular requires a considerable amount of
resources: 128GB of RAM, extensive multi-threading support
and 100 GB of hard disk space are the requirements for creat-
ing the bitstream ﬁles for the above mentioned FPGA-enabled
HPC cluster. Moreover, from a user prospective, the design of
an FPGA bitstream requires the interactive use of several GUI
applications (as nearly all the HLS tools) and to let the software
work for several hours.

Rosetta was adopted as the primary tool for programming
INAF’s FPGA cluster prototype, and suited very well the
use case. Thanks to enabling access to persistent, web-based
remote desktops with the required computing and storage
resources, users were indeed capable of using HLS tools
from their standard computing equipment, and to let them
work for as many hours as needed, even if disconnecting and
reconnecting the day after.

7. Discussion

8. Conclusions and future work

In designing and implementing Rosetta we faced two main
challenges: supporting custom software packages, libraries and
environments, and integrating with computing resources not na-
tively supporting containerized workloads.

We addressed the ﬁrst challenge by developing a novel ar-
chitecture based on framing user tasks as microservices. This
allowed Rosetta to fully support custom software packages, li-
braries and environments (including GUI applications and re-
mote desktops) and together with software containers allowed
to ensure safe, consistent and reproducible code execution
across diﬀerent computing resources.

With respect to the second challenge, it has ﬁrst to be noted
that HPC clusters and data-intensive systems still rely on Linux
users for a number of reasons, including accounting purposes
and local permission management. This means that most of the
containerisation solutions born in the IT industry, which assume
to operate as a superuser, are in general not suitable. For this
reason, the Singularity container engine was built to operate
exclusively at user-level, and quickly become the standard in
the HPC space.

However, Singularity is not designed to provide full isola-
tion between the host system and the containers, and by default
directories as the home folder, /tmp, /proc, /sys, and /dev
are all shared with the host, environment variables are exported
as they are set on host, the PID namespace is not created from
scratch, and the network and sockets are as well shared with the
host. Also, the temporary ﬁle system provided by Singularity
in order to make the container ﬁle system writable (which is re-
quired for some software) is a relatively weak solution, since it
is stored in memory and often with a default size of 16MB, thus
very easy to ﬁll up.

We therefore had to address all these issues before being
able to use Singularity as a container engine from Rosetta.
In particular, we used a combination of command line ﬂags
(-cleanenv, -containall, -pid) and ad-hoc runtime sand-
boxing for the key directories which require write access (as
the user home), orchestrated by the agent. This step was key
for the success of our approach and proved to remove nearly all
the issues related to running Singularity containers on diﬀerent
computing systems.

Similarly, we had to work around a series of features lacking
in WMSs not natively supporting containerized workloads (as
Slurm), including container life cycle management itself, net-
work virtualization and TCP/IP traﬃc routing between the con-
tainers, all solved using the agent as explained in the previous
sections.

Once we were able to ensure a standardised behaviour of
container engines and WMSs, we were able to make task ex-
ecution uniform across diﬀerent kinds of computing resources,
providing the very same user experience. In this sense, Rosetta
can be considered as an umbrella for a variety of computing re-
sources, and can act as a sort of bridge in the transition towards
software containers.

We presented Rosetta, a science platform for resource-
intensive, interactive data analysis which runs user tasks as soft-
ware containers. Its main characteristic lies in providing simpli-
ﬁed access to remote computing and storage resources without
restricting users to a set of pre-deﬁned software packages, li-
braries and environments.

To achieve this goal, we developed a novel architecture based
on framing user tasks as microservices - independent and self-
contained units - which we implemented as software contain-
ers. This approach allowed us to fully support custom soft-
ware packages, libraries and environments, including remote
desktops and GUI applications besides standard web-based so-
lutions as the Jupyter Notebooks. Moreover, adopting software
containers allowed for safe, eﬀective and reproducible code ex-
ecution, and enabled us to let our users to add and use their own
software containers on the platform.

We also took real-world deployment scenarios in mind, and
designed Rosetta to easily integrate with existent computing re-
sources, even where they lacked native support for container-
ized workloads. This proved to be particularly helpful for inte-
grating with HPC clusters and data-intensive systems.

We successfully tested Rosetta for a number of use cases, in-
cluding the LOFAR data reduction pipelines at INAF comput-
ing centers in the context of the ESCAPE project which funded
this work, the SKA data challenges, and other minor use cases
of our user community.

The beneﬁts of seamlessly oﬄoading data analysis tasks to a
sort of “virtual workstation”, hosted on a computing system ca-
pable of providing CPUs, RAM and storage resources as per re-
quests were immediately clear, removing constrains and speed-
ing up the various activities.

Although astronomy and astrophysics remains its mainstay,
Rosetta can virtually support any science and technology do-
main requiring resource-intensive, interactive data analysis, and
it is currently being tested and evaluated in other institutions.

Future work include adding support for distributed work-
loads (e.g. MPI, Ray) and for computing resources with mixed
architectures, developing a command line interface, integrating
with data staging solutions and continuing the implementation
eﬀorts for integrating with current and new WMSs (e.g. Torque,
Openshift, Rancher, Nomad, and more).

9. Acknowledgements

This work was supported by the European Science Cluster
of Astronomy and Particle Physics ESFRI Research Infrastruc-
tures project, funded by the European Union’s Horizon 2020 re-
search and innovation programme under Grant Agreement no.
824064. We also acknowledge the computing center of INAF-
Osservatorio Astronomico di Trieste, [46, 54], for the availabil-
ity of computing resources and support.

12

References

[1] P. E. Dewdney, P. J. Hall, R. T. Schilizzi, T. J. L. W. Lazio, The Square
Kilometre Array, IEEE Proceedings 97 (2009) 1482–1496. doi:10.
1109/JPROC.2009.2021005.

[2] B. Acharya, M. Actis, T. Aghajani, G. Agnetta, J. Aguilar, F. Aharonian,
M. Ajello, A. Akhperjanian, M. Alcubierre, J. Aleksi´c, et al., Introducing
the CTA concept, Astroparticle Physics 43 (2013) 3–18. doi:10.1016/
j.astropartphys.2013.01.007.

[3] T. de Zeeuw, R. Tamai, J. Liske, Constructing the E-ELT, The Messenger

158 (2014) 3–6.

[4] J. P. Gardner, J. C. Mather, M. Clampin, R. Doyon, M. A. Greenhouse,
H. B. Hammel, J. B. Hutchings, P. Jakobsen, S. J. Lilly, K. S. Long, et al.,
The james webb space telescope, Space Science Reviews 123 (2006)
485–606. doi:10.1007/s11214-006-8315-7.

[5] R. Laureijs, P. Gondoin, L. Duvet, G. S. Criado, J. Hoar, J. Amiaux, J.-L.
Augu`eres, R. Cole, M. Cropper, A. Ealet, et al., Euclid: ESA’s mission
to map the geometry of the dark universe, in: Space Telescopes and In-
strumentation 2012: Optical, Infrared, and Millimeter Wave, Vol. 8442,
2012, p. 84420T. doi:10.1117/12.926496.

[6] A. Merloni, P. Predehl, W. Becker, H. B¨ohringer, T. Boller, H. Brun-
ner, M. Brusa, K. Dennerl, M. Freyberg, P. Friedrich, A. Georgakakis,
F. Haberl, G. Hasinger, N. Meidinger, J. Mohr, K. Nandra, A. Rau, T. H.
Reiprich, J. Robrade, M. Salvato, A. Santangelo, M. Sasaki, A. Schwope,
J. Wilms, the German eROSITA Consortium, erosita science book: Map-
ping the structure of the energetic universe (2012). arXiv:1209.3114.
[7] V. Springel, R. Pakmor, O. Zier, M. Reinecke, Simulating cosmic
structure formation with the GADGET-4 code, arXiv e-prints (2020)
arXiv:2010.03567arXiv:2010.03567.

[8] A. Bleuler, R. Teyssier, Towards a more realistic sink particle algorithm
for the RAMSES CODE, MNRAS 445 (4) (2014) 4015–4036. arXiv:
1409.6528, doi:10.1093/mnras/stu2005.

[9] V. Springel, R. Pakmor, A. Pillepich, R. Weinberger, D. Nelson, L. Hern-
quist, M. Vogelsberger, S. Genel, P. Torrey, F. Marinacci, J. Naiman, First
results from the IllustrisTNG simulations: matter and galaxy clustering
475 (1) (2018) 676–698. arXiv:1707.03397, doi:10.1093/mnras/
stx3304.

[10] A. Ragagnin, K. Dolag, V. Biﬃ, M. Cadolle Bel, N. J. Hammer,
A. Krukau, M. Petkova, D. Steinborn, A web portal for hydrodynamical,
cosmological simulations, Astronomy and Computing 20 (2017) 52–67.
arXiv:1612.06380, doi:10.1016/j.ascom.2017.05.001.
[11] G. Taﬀoni, G. Murante, L. Tornatore, M. Katevenis, N. Chrysos,
M. Marazakis, Shall Numerical Astrophysics Step Into the Era of Ex-
ascale Computing?, in: M. Molinaro, K. Shortridge, F. Pasian (Eds.),
Astronomical Data Analysis Software and Systems XXVI, Vol. 521 of
Astronomical Society of the Paciﬁc Conference Series, 2019, p. 567.
arXiv:1904.11720.

[12] S. Habib, A. Pope, H. Finkel, N. Frontiere, K. Heitmann, D. Daniel,
P. Fasel, V. Morozov, G. Zagaris, T. Peterka, V. Vishwanath, Z. Luki´c,
S. Sehrish, W.-k. Liao, HACC: Simulating sky surveys on state-of-the-
art supercomputing architectures, New Astronomy 42 (2016) 49–65.
arXiv:1410.2805, doi:10.1016/j.newast.2015.06.003.

[13] M. Asch, T. Moore, R. Badia, M. Beck, P. Beckman, T. Bidot, F. Bodin,
F. Cappello, A. Choudhary, B. de Supinski, et al., Big data and extreme-
scale computing: Pathways to convergence-toward a shaping strategy
for a future software and data ecosystem for scientiﬁc inquiry, The In-
ternational Journal of High Performance Computing Applications 32 (4)
(2018) 435–479.

[14] J. Bhandari Neupane, R. P. Neupane, Y. Luo, W. Y. Yoshida, R. Sun, P. G.
Williams, Characterization of leptazolines a–d, polar oxazolines from the
cyanobacterium leptolyngbya sp., reveals a glitch with the “willoughby–
hoye” scripts for calculating nmr chemical shifts, Organic letters 21 (20)
(2019) 8449–8453.

[15] G. Taﬀoni, G. Lemson, M. Molinaro, A. Schaaﬀ, D. Morris,
Z. Meyer–Zhao, Science Platforms: Towards Data Science, in: R. Pizzo,
E. R. Deul, J. D. Mol, J. de Plaa, H. Verkouter (Eds.), Astronomical So-
ciety of the Paciﬁc Conference Series, Vol. 527 of Astronomical Society
of the Paciﬁc Conference Series, 2020, p. 777.

[16] V. Desai, M. Allen, C. Arviset, B. Berriman, R.-R. Chary, D. Cook,
A. Faisst, G. Dubois-Felsmann, S. Groom, L. Guy, G. Helou, D. Imel,
S. Juneau, M. Lacy, G. Lemson, B. Major, J. Mazzarella, T. Mcglynn,
I. Momcheva, E. Murphy, K. Olsen, J. Peek, A. Pope, D. Shupe, A. Smale,

A. Smith, N. Stickley, H. Teplitz, A. Thakar, X. Wu, A Science Platform
Network to Facilitate Astrophysics in the 2020s, in: Bulletin of the Amer-
ican Astronomical Society, Vol. 51, 2019, p. 146.

[17] C. Cui, Y. Tao, C. Li, D. Fan, J. Xiao, B. He, S. Li, C. Yu, L. Mi,
Y. Xu, J. Han, S. Yang, Y. Zhao, Y. Xue, J. Hao, L. Liu, X. Chen,
J. Chen, H. Zhang, Towards an astronomical science platform: Expe-
riences and lessons learned from Chinese Virtual Observatory, Astron-
omy and Computing 32 (2020) 100392. arXiv:2005.10501, doi:
10.1016/j.ascom.2020.100392.

[18] M. Taghizadeh-Popp, J. W. Kim, G. Lemson, D. Medvedev, M. J. Rad-
dick, A. S. Szalay, A. R. Thakar, J. Booker, C. Chhetri, L. Dobos,
M. Rippin, SciServer: A science platform for astronomy and beyond,
Astronomy and Computing 33 (2020) 100412. arXiv:2001.08619,
doi:10.1016/j.ascom.2020.100412.

[19] D. Piparo, E. Tejedor, P. Mato, L. Mascetti, J. Moscicki, M. Lamanna,
SWAN: A service for interactive analysis in the Cloud, Future Generation
Computer Systems 78 (2018) 1071–1078.

[20] Esa datalabs: Towards a collaborative e-science platform for esa, in: J.-E.
Ruiz, F. Pierfederici (Eds.), ADASS XXX, Vol. TBD of ASP Conf. Ser.,
ASP, San Francisco, 2021, p. 999 TBD.

[21] M. Juri´c, D. Ciardi, G. Dubois-Felsmann, LSST science platform vision

document, LSE-319, LSST (2017).

[22] J. W. Nicklas, D. Johnson, S. Oottikkal, E. Franz, B. McMichael,
A. Chalker, D. E. Hudak, Supporting distributed, interactive jupyter and
rstudio in a scheduled hpc environment with spark using open ondemand,
in: Proceedings of the Practice and Experience on Advanced Research
Computing, PEARC ’18, Association for Computing Machinery, New
York, NY, USA, 2018, pp. 1–8. doi:10.1145/3219104.3219149.
[23] K. M. Mendez, L. Pritchard, S. N. Reinke, D. I. Broadhurst, Toward col-
laborative open data science in metabolomics using jupyter notebooks and
cloud computing, Metabolomics 15 (10) (2019) 1–16.

[24] M. Milligan, Jupyter as Common Technology Platform for Interactive

HPC Services, arXiv e-prints (Jul. 2018). arXiv:1807.09929.

[25] A. M. Castronova, P. Doan, M. Seul, A general approach for enabling
cloud-based hydrologic modeling using jupyter notebooks, HydroShare
(2018).

[26] M. Araya, M. Osorio, M. D´ıaz, C. Ponce, M. Villanueva, C. Valenzuela,
M. Solar, JOVIAL: Notebook-based astronomical data analysis in the
Cloud, Astronomy and computing 25 (2018) 110–117.

[27] B. Major, J. Kavelaars, S. Fabbro, D. Durand, H. Jeeves, Arcade: An In-
teractive Science Platform in CANFAR, in: P. J. Teuben, M. W. Pound,
B. A. Thomas, E. M. Warner (Eds.), Astronomical Data Analysis Soft-
ware and Systems XXVII, Vol. 523 of Astronomical Society of the Paciﬁc
Conference Series, 2019, p. 277.

[28] E. Bisong, Google colaboratory, in: Building Machine Learning and Deep
Learning Models on Google Cloud Platform, Springer, 2019, pp. 59–64.
[29] Kaggle, How to use kaggle - notebooks, https://web.archive.
org/web/20211112085454/https://www.kaggle.com/docs/
notebooks, accessed: 2021-11-24 (2018).

[30] D. Chappell, Introducing azure machine learning, A guide for technical

professionals, sponsored by Microsoft corporation (2015).

[31] Department of Energy High Performance Computing Act of 1989: hear-
ing before the Subcommittee on Energy Research and Development of
the Committee on Energy and Natural Resources on s. 1976 to provide
for continued United States leadership in High Performance Computing,
Vol. 4, U.S. Government printing oﬃce, 1990, pp. 197–198.

[32] B. Gorda, HPC in the Cloud?

yes,

no and in between,

https://web.archive.org/web/20210326114937/https:
//www.arm.com/blogs/blueprint/hpc-cloud, accessed: 2021-3-
26.

[33] CERFACS COOP-Algo Team,

in

scientiﬁc

Python
org/web/20210218084422/https://cerfacs.fr/coop/
fortran-vs-python, accessed: 2021-2-18.

The
computing,

counter-intuitive

of
https://web.archive.

rise

[34] J. Dursi, HPC is dying, and MPI

is killing it, https://web.

archive.org/web/20211029041512/https://www.dursi.ca/
post/hpc-is-dying-and-mpi-is-killing-it, accessed:
10-29.

2021-

[35] SUSE, Technology

deﬁnitions

- Containers,

https://web.

archive.org/web/20220112132030/https://www.suse.com/
suse-defines/definition/containers/, accessed: 2022-1-12.

13

[54] G. Taﬀoni, U. Becciani, B. Garilli, G. Maggio, F. Pasian, G. Umana,
R. Smareglia, F. Vitello, CHIPP: INAF Pilot Project for HTC, HPC and
HPDA, in: R. Pizzo, E. R. Deul, J. D. Mol, J. de Plaa, H. Verkouter
(Eds.), Astronomical Society of the Paciﬁc Conference Series, Vol. 527
of Astronomical Society of the Paciﬁc Conference Series, 2020, p. 307.

[36] C. Boettiger, An introduction to Docker for reproducible research, ACM

SIGOPS Operating Systems Review 49 (1) (2015) 71–79.

[37] S. A. Russo, et al., A microservice-oriented science platform architecture,
in: J.-E. Ruiz, F. Pierfederici (Eds.), ADASS XXX, Vol. TBD of ASP
Conf. Ser., ASP, San Francisco, 2021, p. 999 TBD.

[38] S. Newman, Building microservices, ” O’Reilly Media, Inc.”, 2015.
[39] K. Brady, S. Moon, T. Nguyen, J. Coﬀman, Docker container security in
cloud computing, in: 2020 10th Annual Computing and Communication
Workshop and Conference (CCWC), IEEE, 2020, pp. 0975–0980.
[40] J. Schwenk, M. Niemietz, C. Mainka, Same-origin policy: Evaluation in
modern browsers, in: 26th {USENIX} Security Symposium ({USENIX}
Security 17), 2017, pp. 713–727.

[41] J. Chen, J. Jiang, H. Duan, T. Wan, S. Chen, V. Paxson, M. Yang, We still
don’t have secure cross-domain requests: an empirical study of {CORS},
in: 27th {USENIX} Security Symposium ({USENIX} Security 18), 2018,
pp. 1079–1093.

[42] M. Zalewski, The tangled Web: A guide to securing modern web appli-

cations, No Starch Press, 2012.

[43] M. Zalewski, Browser security handbook, https://web.archive.

org/web/20211113072530/https://code.google.com/
archive/p/browsersec/wikis/Main.wiki,
13 (2009).

accessed:

2021-11-

[44] M. Squarcina, M. Tempesta, L. Veronese, S. Calzavara, M. Maﬀei, Can i
take your subdomain? exploring same-site attacks in the modern web, in:
30th {USENIX} Security Symposium ({USENIX} Security 21), 2021, pp.
2917–2934.

[45] JupyterHub, Security overview, https://web.archive.org/web/

20211127104158/https://jupyterhub.readthedocs.io/en/1.
4.2/reference/websecurity.html, accessed: 2021-11-27 (2016).

[46] S. Bertocco, D. Goz, L. Tornatore, A. Ragagnin, G. Maggio, F. Gasparo,
C. Vuerli, G. Taﬀoni, M. Molinaro, INAF Trieste Astronomical Observa-
tory Information Technology Framework, in: R. Pizzo, E. R. Deul, J. D.
Mol, J. de Plaa, H. Verkouter (Eds.), Astronomical Society of the Paciﬁc
Conference Series, Vol. 527 of Astronomical Society of the Paciﬁc Con-
ference Series, 2020, p. 303.

[47] F. Tinarelli, S. Zorba, C. Knapic, G. Jerse, The authentication and autho-
rization inaf experience, Astronomical Data Analysis Software and Sys-
tems XXVII 522 (2020) 727.

[48] C. Tasse, B. Hugo, M. Mirmont, O. Smirnov, M. Atemkeng, L. Bester,
M. Hardcastle, R. Lakhoo, S. Perkins, T. Shimwell, Faceting for
direction-dependent spectral deconvolution, Astronomy & Astrophysics
611 (2018) A87.

[49] G. Cupani, V. D’Odorico, S. Cristiani, S. A. Russo, G. Calderone, G. Taf-
foni, Astrocook: your starred chef for spectral analysis, in: Software and
Cyberinfrastructure for Astronomy VI, Vol. 11452, International Society
for Optics and Photonics, 2020, p. 114521U.

[50] M. Katevenis, R. Ammendola, A. Biagioni, P. Cretaro, O. Frezza, F. Lo
Cicero, A. Lonardo, M. Martinelli, P. S. Paolucci, E. Pastorelli, F. Sim-
ula, P. Vicini, G. Taﬀoni, J. A. Pascual, J. Navaridas, M. Luj ˜A¡n,
J. Goodacre, B. Lietzow, A. Mouzakitis, N. Chrysos, M. Marazakis,
P. Gorlani, S. Cozzini, G. P. Brandino, P. Koutsourakis, J. van Ruth,
Y. Zhang, M. Kersten, Next generation of exascale-class systems: Ex-
anest project and the status of its interconnect and storage develop-
ment, Microprocessors and Microsystems 61 (2018) 58–71. doi:https:
//doi.org/10.1016/j.micpro.2018.05.009.

[51] G. Taﬀoni, L. Tornatore, D. Goz, A. Ragagnin, S. Bertocco, I. Coretti,
M. Marazakis, F. Chaix, M. Plumidis, M. Katevenis, R. Panchieri,
G. Perna, Towards exascale: Measuring the energy footprint of astro-
in: 2019 15th International Conference on
physics hpc simulations,
eScience (eScience), 2019, pp. 403–412. doi:10.1109/eScience.
2019.00052.

[52] G. Taﬀoni, S. Bertocco, I. Coretti, D. Goz, A. Ragagnin, L. Tornatore,
Low power high performance computing on arm system-on-chip in as-
trophysics, in: K. Arai, R. Bhatia, S. Kapoor (Eds.), Proceedings of the
Future Technologies Conference (FTC) 2019, Springer International Pub-
lishing, Cham, 2020, pp. 427–446.

[53] D. Goz, G. Ieronymakis, V. Papaefstathiou, N. Dimou, S. Bertocco,
F. Simula, A. Ragagnin, L. Tornatore, I. Coretti, G. Taﬀoni, Perfor-
mance and energy footprint assessment of fpgas and gpus on hpc sys-
tems using astrophysics application, Computation 8 (2) (2020). doi:
10.3390/computation8020034.

14

