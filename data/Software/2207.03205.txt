DUAL STREAM COMPUTER-GENERATED IMAGE DETECTION NETWORK BASED ON
CHANNEL JOINT AND SOFTPOOL

School of Computer Science and Engineering, Sun Yat-sen University, GuangZhou 510006, China;

Ziyi Xi,Hao Lin,Weiqi Luo*

2
2
0
2

l
u
J

7

]

V
C
.
s
c
[

1
v
5
0
2
3
0
.
7
0
2
2
:
v
i
X
r
a

ABSTRACT

With the development of computer graphics technology, the
images synthesized by computer software become more and
more closer to the photographs. While computer graphics
technology brings us a grand visual feast in the ﬁeld of games
and movies, it may also be utilized by someone with bad in-
tentions to guide public opinions and cause political crisis or
social unrest. Therefore, how to distinguish the computer-
generated graphics (CG) from the photographs (PG) has be-
come an important topic in the ﬁeld of digital image forensics.
This paper proposes a dual stream convolutional neural net-
work framework based on channel joint and softpool. The
proposed network architecture includes a residual module for
extracting image noise information and a joint channel infor-
mation extraction module for capturing the shallow semantic
information of image. In addition, we also design a residual
structure to enhance feature extraction and reduce the loss of
information in residual ﬂow. The joint channel information
extraction module can obtain the shallow semantic informa-
tion of the input image which can be used as the information
supplement block of the residual module. The whole net-
work uses SoftPool to reduce the information loss of down-
sampling for image. Finally, we fuse the two ﬂows to get the
classiﬁcation results. Experiments on SPL2018 and DsTok
show that the proposed method outperforms existing meth-
ods, especially on the DsTok dataset. For example, the per-
formance of our model surpasses the state-of-the-art Quan [1]
by a large margin of 3%. The source code is accessible on
github. 1

Index Terms— Digital image forensics, Convolutional
neural network, Natural images, Computer generated images,
CG detection

1. INTRODUCTION

CG is the abbreviation of computer-generated graphics,
which refers to the virtual but visually resonable images
generated by computer software. PG is the abbreviation of
Photographs, which means the real images taken by cam-
eras. In recent years, CG technology has been widely used
In this process, a large
in the ﬁelds of games and movies.

1https://github.com/zoie-ui/CG-Detection

(a) CG images

(b) PG images

Fig. 1. The visual comparison diagram of CG and PG, image
examples from SPL2018.

number of image processing tools were born, such as Vray,
corona, Enscape and lumion. So people without professional
knowledge can generate CG easily. As shown in Fig.1, it is
difﬁcult to distinguish between CG and PG by naked eyes.
In addition, some studies have shown that although people’s
recognition rate will increase after receiving targeted training,
they still can’t effectively distinguish CG and PG, especially
when they have doubts about a picture, they tend to identify
it as a natural image [2]. This has raised people’s concerns
about safety, because these advanced CG technologies may
be used by criminals to create realistic CG to confuse the
public. Therefore, the study of CG detection technology has
important academic signiﬁcance and practical application
value.

In this paper, we propose a novel dual stream convolu-
tional neural network for CG detection. The proposed net-
work is composed of the residual extraction module which
learns the noise of image and the joint channel information
extraction module which extracts the shallow semantic infor-
mation of image. The main contributions of our work can be
summarized as follows:

 
 
 
 
 
 
• we propose a dual stream framework based on channel
joint and softpool to solve the CG detection problem.
The network consists of two main modules, namely,
the residual extraction module and the channel joint
feature extraction module. By fusing the features ex-
tracted from the two modules, we can achieve the best
detection performance on SPL2018 and DsTok.

• In the residual extraction module, we adopt a special
residual structure, which can effectively enhance the
learning of residual information.

• we ﬁrst introduce the SoftPool into the CG detection
ﬁeld, which reduces the information loss caused by
downsampling.

2. RELATED WORKS

Existing methods for CG detection can be generally divided
into two categories——hand-crafted feature-based methods
and deep learning-based methods.
For traditional hand-
crafted methods, It usually depends on statistics or internal
characteristics difference of CG and PG, and demands peo-
ple to design an efﬁcient algorithm to extract features and
make decisions between them. A simple strategy is to ﬁnd
a category sensitive scalar feature and select an appropriate
classiﬁcation threshold. The deep learning-based methods
usually directly utilize deep neural network to autonomously
learn complex features to complete classiﬁcation.

Rahmouni et al. [8] ﬁrst used convolutional neural net-
work to learn a group of ﬁlters for image preprocessing and
trained a multi-layer perception to complete the classiﬁcation
tasks. Quan et al. [9] convinced that the detection accuracy of
the model is directly affected by the image sampling mode.
Therefore, they used the maximum poisson disk sampling to
complete the data enhancement, and then trained a CNN with
seven layers, ﬁnally produced the prediction result through
the simple majority voting principle. Yao et al. [10] proposed
using high pass ﬁlters to remove the low-frequency compo-
nent of the image, in other words, focusing on observing the
sensor noise and residual introduced by the digital camera.
Therefore, they designed three high-pass ﬁlters according to
prior knowledge. The cropped image is ﬁrst ﬁltered by high
pass ﬁlters, and then is transmitted to CNN for further learn-
ing. Quan et al. [1] proposed an attention network based CNN
with 10 layers, which integrated RGB and ﬁltered RGB im-
ages, with a total of 6 channels as the input of network. Zhang
et al. [14] proposed a module composed of stacking convo-
lution layers to preprocess R, G and B channel, then concat
them by channel and transmit them to the ﬁve layers convolu-
tion neural network for further learning to get the results. He
et al. [15] took the six-channel image obtained by the channel
fusion of the Gaussian ﬁlter preprocessed image and the orig-
inal image as the input, and then sent it to a four layers dual

stream network with different scales only in the ﬁrst convolu-
tion layer, and ﬁnally fused the two streams through a simple
attention mechanism.

Rezende et al. [11] based on transfer learning, send an
RGB image after gray processing to the ﬁne-tuning resnet50
[19] which had been pre-trained on ImageNet, achieved a
good precision. It is worth mentioning that their time for de-
tecting an image is 1.02s, which has a great signiﬁcance for
practical application. Nguyen et al. [12] found that the se-
mantic information of the image will gradually lose with the
increase of the depth of the neural network. This will make
the features tend to be homogeneous, so they took the out-
puts of the ﬁrst three layers as the extracted image features
and transmitted them to the pre-constructed feature conver-
sion module, and then trained the classiﬁer to obtain the de-
tection results. Yao et al. [16] also extracted the output fea-
ture maps of the ﬁrst three layers of vgg19 [20] as an input
of the three stream network separately. The innovation is that
the convolution block attention module is introduced before
stream fusion to enhance the feature representation ability.
He et al. [13] proposed a method that CNN combined with
recurrent neural network (RNN) to detect CG.

3. PROPOSED METHOD

In this part, we will describe our network in detail. We pro-
pose a dual stream network to detect CG, which composed
of residual stream extracting the residual information of im-
age and joint channel stream extracting shallow semantic in-
formation. As shown in Figure 2, for the residual extraction
module, we utilize the SRM to extract residual information
of RGB color space through various channels, and then con-
cat the feature maps by channel and further reﬁned them by
feature extraction network. Finally, we can obtain the 128
dimensional residual feature maps. In the joint channel infor-
mation extraction module, we utilize the convolutional neural
network to reﬁne the original RGB image to gain the 128 di-
mensional shallow semantic feature maps. Besides, the whole
network adopts SoftPool for downsampling, which is con-
ducive to reduce the information loss in the process of net-
work training. Finally, we merge the classiﬁcation results of
dual stream mentioned above to get the ﬁnal output. Next we
will introduce several module used in this network in detail.

3.1. Preprocessing

Inspired by Goljan M et al. [3] and Quan et al. [1], we uti-
lize 30 SRM to ﬁlter the R, G and B channel respectively to
extract the residual features, and then merge them by channel
to obtain the 90 residual feature maps. It can strengthen the
characterization of the relationship between local pixels in the
same channel and get more complex statistical characteristics
by concatnation, and also proﬁts to enlarge the difference be-
tween PG and CG images.

Fig. 2. The framework of the proposed network.

3.2. Feature extraction

The network structure of the feature extraction module con-
sists of ﬁve convolution layers, where each convolution layer
is followed by a batch normalization, a ReLU activation func-
tion and a SoftPool. The purpose of the module is to fully
learn the residual characteristics after fusion. There are three
consecutive residual structures in the middle three layers.

The residual structure is mainly composed of two branches,
as shown in Figure 3, where the previous layer input ﬁrst
passes through a convolution layer of size 3 × 3 with step
1, and then activated by a ReLU function, ﬁnally undergoes
sampling in the SoftPool layer. There is a convolution layer
of size 3 × 3 with step 2 in the branch. This, we can extract
the residual information with the same scale. Finally, the
results of two branches are fused by adding.

It is worth noting that we adopt SoftPool instead of Max-
Pool to down sample image. The basic functions of pool-
ing layer include: reducing the amount of calculation, reduc-
ing model redundancy, preventing model over ﬁtting and etc.
Softpool [17] is a variant structure of pooling layer, it can
enhance feature representation and retain the basic attributes
of input. Speciﬁcally, SoftPool reduces the information loss
caused by pooling while maintaining the basic functions of
the pooling layer. Its calculation process is shown in Figure
4. For a region R of size 2 × 2, we ﬁrst calculate the Soft-
Max value wi(i = 1, 2, 3, 4) of each pixel in the region, then
multiply the SoftMax value and the original pixel value by el-
ements, and accumulate the four values to obtain the pooled
result (cid:101)α. The speciﬁc formula as follows:

wi =

(cid:80)

eαi
j∈R eαj

˜α = (cid:80)

j∈R wj × αj

where αi represents the pixel value of the i-th pixel point, wi
represents the weight corresponding to the i-th pixel.

3.3. Joint channel information extraction module

We adopt the original RGB image as the input of this mod-
ule.
It is also composed of ﬁve convolution layers. Simi-
larly, each convolution layer of size 3 × 3 is followed by a
batch normalization, a relu activation function and a SoftPool
layer. Finally, we can also obtain the 128 dimensional shal-
low semantic information feature maps with the same size as
the output of the residual extraction module. Unlike natural
images, which are limited by time, place and environment,
CG contain many scenes that do not exist in reality. There-
fore, the semantic content of image also contains important
information for classiﬁcation. After preprocessing, the origi-
nal image content has been basically omitted. Therefore, the
joint channel information extraction module can be the infor-
mation supplement block of the residual module.

4. EXPERIMENTAL RESULT

4.1. Datasets

We conduct experiments on the SPL2018 and DsTok dataset.
SPL2018 dataset was constructed by He et al. [13], which

4.2. Experimental Settings

Like most existing methods, we utilize accuracy (Acc) as
our evaluation metrics, and its calculation formula can be
expressed as follows:

Acc =

T P + T N
P + N

× 100%

where P denotes the number of positive samples, in this pa-
per represents the total number of natural images, N refers
to the number of negative samples. TP and TN respectively
refers to the number of positive samples and negative samples
correctly classiﬁed.

Other experimental settings as follows: we use NVIDIA’s
Titan GPU to train our model in PyTorch deep learning frame-
work. In the process of model training, we choose the cross
entropy loss as our loss function, and exploit the SGD opti-
mizer to optimize the model, where the size of mini-batch is
64. The initial learning rate is set as 1e-3, and is reductioned
to 0.5 times of the original every 20 epochs. The weight decay
rate is set as 1e-3. The total of training epochs is 120.

4.3. Comparisons With Other State-of-the-art Methods

We mainly compare with six existing advanced detection
methods, and evaluated our model on DsTok and SPL2018
respectively. In order to make the experimental results more
convincing, we randomly divided each dataset for three times
and conducted fair test on each division. Finally, we adopt the
average test result of three divisions as our ﬁnal assessment
value.

The experimental results are shown in Table 1. From the
table we can see that our method has reached the best detec-
tion performance on both of the two datasets. Especially on
DsTok, our model has been improved by 3% than Quan [1].
On SPL2018, compared with the current advanced CG de-
tection models such as Quan [1] and Yao [16], the proposed
method surpasses by 1.1% and 0.4% respectively. It is worth
noting that Quan’s method has a similar preprocessing pro-
cedure with ours, but they exploit MaxPool to down sample
image in the whole network, which will cause the loss of in-
formation. In addition, Quan’s method also ignores the re-
lationship between residual information and image shallow
semantic information.

4.4. Ablation Study

In order to verify the rationality of our network structure, this
section we mainly conducted the following ablation exper-
iments: 1) Ablation of dual stream framework; 2) Perfor-
mance analysis of different combinations of SRM residual ﬁl-
ter cores; 3) The validity analysis of residual structure and the
inﬂuence of its placement position on classiﬁcation; 4) The
impact of different pooling combinations on network perfor-
mance.

Fig. 3. The residual structure.

Fig. 4. The processing of SoftPool for a 2x2 region.

contained 6800 PG and 6800 CG. CG are collected from more
than 50 rendering software, PG are taken by different types
of camera under various environmental conditions, including
indoor and outdoor. The range of image resolution is from
266 × 199 to 2048 × 3200, which is highly heterogeneous
and difﬁcult to detect. We divide it into training set, valida-
tion set and test set according to the ratio of 10:3:4. DsTok
dataset [18] contains 4850 PG and 4850 CG, and the image
resolution are from 609 × 603 to 3507 × 2737. All CG and
PG images in DsTok are collected from the Internet and have
strong heterogeneity. We also divide it into three sets in the
ratio of 3:1:1. In order to unify the input standard, we sam-
pled the center region of each image with size 224 × 224 as
the input of our model.

Method

Quan [9]
Yao [10]
He [13]
Zhang [14]
Quan [1]
Yao [16]
Ours

DsTok

85.3%
88.8%
83.2%
93.4%
93.9%
92.1%
96.9%

SPL2018

89.4%
89.8%
88.0%
92.8%
92.8%
93.5%
93.9%

Table 1. Comparisons with other methods.

4.4.1. Ablation of dual stream framework

The proposed network is a dual stream framework including
residual extraction ﬂow and joint channel information extrac-
tion ﬂow. In order to explore the rationality of dual stream
framework, we remove one of them(i.e. residual ﬂow or joint
channel ﬂow) and compare them with the method in this pa-
per. The experimental results are shown in Table 2. From Ta-
ble 2, we can see that on SPL2018, the result of the proposed
method is 93.9%, which increased 1% and 2.9% respectively
than only exploit residual ﬂow or only exploit joint channel
ﬂow. On DsTok dataset, the accuracy of single residual ﬂow
and single joint channel ﬂow are 96.6% and 84.5% respec-
tively. The detection result of the proposed network is 96.9%,
which is improved by 0.3% and 12.4% . It can be seen that
the residual feature is an important feature to distinguish CG
and PG images. It is also conﬁrmed that the dual streams will
get better results than any single ﬂow of them.

Model

SPL2018

DsTok

Only residual stream
Only joint channel stream
Ours

92.9%
91.0%
93.9%

96.6%
84.5%
96.9%

Table 2. Comparative results of single stream and the pro-
posed dual stream.

Filter Set 1storder 2storder 3storder

3x3

5x5

Ours

Acc

94.6%

95.1%

95.7% 95.3% 95.5% 96.9%

Table 3. Comparative studies for different SRM ﬁlter combi-
nations.

4.4.2. Different combinations of SRM residual ﬁlter cores

In this paper, we exploit 30 residual ﬁltering cores to ﬁlter
each channel. This section we will explore the impact of us-
ing the combinations of different ﬁltering cores on the ﬁnal
performance of the model. According to the division of ﬁlter
cores in [3], we conduct ﬁve groups of ablation experiments,

including ﬁrst-order ﬁlter cores (8), second-order ﬁlter cores
(4), third-order ﬁlter cores (8), 3×3 ﬁlter cores (17, including
12 ﬁlled ﬁrst-order and second-order ﬁlter cores , 4 edge 3×3
ﬁlter cores and square 3 × 3 ﬁlter core) and 5 × 5 ﬁlter cores
(13, including 8 ﬁlled third-order ﬁlter cores, 4 edge 5 × 5
ﬁlter cores and 1 square 5 × 5 ﬁlter core). Besides, except for
the preprocessing part of the residual extraction module, the
rest part of the network remains unchanged. In addition, the
experiments are only conducted on the DsTok dataset. The
experimental results are shown in Table3, where we can ob-
serve that using 30 residual ﬁlter cores (Ours) can achieve the
best performance for CG detection, and the Acc can reach
96.9% on DsTok.

4.4.3. residual structure

In the residual extraction module, we also designed a residual
structure. In order to verify the effectiveness of this structure,
we designed ablation experiments as follows:

• VA: Neither residual ﬂow nor joint channel ﬂow exploit

residual structure in the middle three layers.

• VB: Residual ﬂow does not use residual structure, and
joint channel ﬂow exploits the structure in the middle
three layers.

• VC: Both residual ﬂow and joint channel ﬂow exploit

this structure in the middle three layers.

Our method is that the residual ﬂow uses the residual structure
in the middle three layers, and the joint channel ﬂow does not
use the residual structure. Other layers remain unchanged,
and the experimental results see Table 4. Here we can see
that the use of residual structure in the middle three layers of
residual ﬂow can greatly increase the detection performance
on DsTok, which proves that the proposed residual structure
can indeed enhance the feature learning of residual ﬂow and
improve the classiﬁcation accuracy. One possible explanation
is that the similar network structure makes the learning char-
acteristics of the two ﬂows tend to be homogeneous. There-
fore, it is determined that the residual structure is ultimately
used in the residual ﬂow and not in the joint channel ﬂow.

Next, we will continue to discuss the impact of the loca-
tion setting of the residual structure on the residual ﬂow. We
have set up three groups of ablation experiments for residual
ﬂow as follows.

(1) Ours (3 layers): The ﬁrst and the last layer of the net-
work are ordinary convolution layer, and the layer 2-4
are residual structure.

(2) 4 layers: Layer 2-5 are residual structure, and layer 1 is

ordinary convolution layer;

(3) 5 layers: The whole layers utilize the residual structure.

The experimental results are shown in Table 5. From the ta-
ble, we can see that on SPL2018, the result of exploiting 4
layers is the best, with an accuracy of 93.1%, which has a
weak advantage over using 3 layers and 5 layers. However,
on DsTok, using 3 layer outperforms the other methods and
has fewer network parameters. Therefore, we choose exploit-
ing the residual structure in the middle three layers of residual
ﬂow.

Structure

SPL2018

DsTok

VA
VB
VC
Ours

94.1%
93.8%
93.5%
93.9%

94.3%
94.4%
96.4%
96.9%

Table 4. Comparative studies for different residual structure.

Layer

SPL2018

DsTok

4 layers
5 layers
Ours(3 layers)

93.1%
93.0%
92.9%

95.3%
95.7%
96.6%

Table 5. Comparative results for using different residual
structures in the proposed model.

pooling combination

SPL2018

M1
M2
M3
Ours

93.8%
93.5%
93.7%
93.9%

DsTok

96.1%
95.7%
96.3%
96.9%

Table 6. Comparative results for using different pooling com-
binations in the proposed model.

4.4.4. pooling combinations

In this section, we will explore the effectiveness of using var-
ious combinations of pooling. We have set up four groups of
experiments:

• M1:Both residual ﬂow and joint channel ﬂow use the

MaxPool.

• M2:Residual ﬂow utilizes the SoftPool, and joint chan-

nel ﬂow utilizes the MaxPool.

• M3:Residual ﬂow utilizes the MaxPool, and joint chan-

nel ﬂow utilizes the SoftPool.

• Ours: Both residual ﬂow and joint channel ﬂow use the

SoftPool.

The experimental results are shown in Table 6, where we can
see that if we utilize the other combinations of pooling, the
detection accuracy decreased in varying degrees on both of
two datasets. This indicates that SoftPool proﬁts to CG detec-
tion.

5. SUMMARY

In this paper, we propose a dual stream convolution neural
network for CG detection, which includes a residual extrac-
tion ﬂow for learning the noise of image and a joint channel
information extraction ﬂow for learning shallow semantic in-
formation. In addition, we also designed an effective residual
structure, which can reduce the loss of information caused by
the pooling layer and it also proﬁts to improve the detection
ability of the model. We have evaluated several related works
on two mainstream CG detection datasets, namely DsTok and
SPL2018. A large number of comparative experiments show
that the proposed method has achieved the best detection per-
formance at present. In addition, we also designed a series
of ablation experiments to verify the rationality of the pro-
posed network structure. Although the model in this paper
has achieved the current optimal detection effect on the cur-
rent two mainstream datasets, there is still room for further
improving. Next we will resort to explore the difference of
CG and PG in spatial domain and frequency domain. At the
same time, we plan to introduce attention and other mecha-
nisms, and organically integrate these features into the exist-
ing network framework to enrich the extracted features and
further improve the detection performance of the model.

6. REFERENCES

[1] W. Quan, K. Wang, D.-M. Yan, X. Zhang, and D. Pel-
lerin, “Learn with diversity and from harder samples:
Improving the generalization of cnn-based detection of
computer-generated images,” Forensic Science Interna-
tional: Digital Investigation, vol. 35, p. 301023, 2020.

[2] O. Holmes, M. S. Banks, and H. Farid, “Assessing and
improving the identiﬁcation of computer-generated por-
traits,” ACM Transactions on Applied Perception (TAP),
vol. 13, no. 2, pp. 1–12, 2016.

[3] M. Goljan, J. Fridrich, and R. Cogranne, “Rich model
for steganalysis of color images,” in 2014 IEEE Interna-
tional Workshop on Information Forensics and Security
(WIFS).

IEEE, 2014, pp. 185–190.

[4] W. Chen, Y. Q. Shi, and G. Xuan, “Identifying computer
graphics using hsv color model and statistical moments
of characteristic functions,” in 2007 ieee international
conference on multimedia and expo.
IEEE, 2007, pp.
1123–1126.

[15] P. He, H. Li, H. Wang, and R. Zhang, “Detection of com-
puter graphics using attention-based dual-branch convo-
lutional neural network from fused color components,”
Sensors, vol. 20, no. 17, p. 4743, 2020.

[16] Y. Yao, Z. Zhang, X. Ni, Z. Shen, L. Chen, and D. Xu,
“Cgnet: Detecting computer-generated images based on
transfer learning with attention module,” Signal Pro-
cessing: Image Communication, vol. 105, p. 116692,
2022.

[17] A. Stergiou, R. Poppe, and G. Kalliatakis, “Reﬁning ac-
tivation downsampling with softpool,” in Proceedings of
the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 10 357–10 366.

[18] E. Tokuda, H. Pedrini, and A. Rocha, “Computer gen-
erated images vs. digital photographs: A synergetic fea-
ture and classiﬁer combination approach,” Journal of Vi-
sual Communication and Image Representation, vol. 24,
no. 8, pp. 1276–1292, 2013.

[19] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual
learning for image recognition,” in Proceedings of the
IEEE conference on computer vision and pattern recog-
nition, 2016, pp. 770–778.

[20] K. Simonyan and A. Zisserman, “Very deep convo-
lutional networks for large-scale image recognition,”
arXiv preprint arXiv:1409.1556, 2014.

[5] A. E. Dirik, H. T. Sencar, and N. Memon, “Source cam-
era identiﬁcation based on sensor dust characteristics,”
in 2007 IEEE Workshop on Signal Processing Applica-
tions for Public Security and Forensics.
IEEE, 2007,
pp. 1–6.

[6] T.-T. Ng, S.-F. Chang, J. Hsu, L. Xie, and M.-P. Tsui,
“Physics-motivated features for distinguishing photo-
graphic images and computer graphics,” in Proceedings
of the 13th annual ACM international conference on
Multimedia, 2005, pp. 239–248.

[7] F. Peng, J. Liu, and M. Long, “Identiﬁcation of natu-
ral images and computer generated graphics based on
hybrid features,” in Emerging Digital Forensics Appli-
cations for Crime Detection, Prevention, and Security.
IGI Global, 2013, pp. 18–34.

[8] N. Rahmouni, V. Nozick, J. Yamagishi, and I. Echizen,
“Distinguishing computer graphics from natural im-
ages using convolution neural networks,” in 2017
IEEE Workshop on Information Forensics and Security
(WIFS).

IEEE, 2017, pp. 1–6.

[9] W. Quan, K. Wang, D.-M. Yan, and X. Zhang, “Distin-
guishing between natural and computer-generated im-
ages using convolutional neural networks,” IEEE Trans-
actions on Information Forensics and Security, vol. 13,
no. 11, pp. 2772–2787, 2018.

[10] Y. Yao, W. Hu, W. Zhang, T. Wu, and Y.-Q. Shi, “Distin-
guishing computer-generated graphics from natural im-
ages based on sensor pattern noise and deep learning,”
Sensors, vol. 18, no. 4, p. 1296, 2018.

[11] E. R. De Rezende, G. C. Ruppert, and T. Carvalho, “De-
tecting computer generated images with deep convolu-
tional neural networks,” in 2017 30th SIBGRAPI Con-
ference on Graphics, Patterns and Images (SIBGRAPI).
IEEE, 2017, pp. 71–78.

[12] H. H. Nguyen, T. N.-D. Tieu, H.-Q. Nguyen-Son,
V. Nozick, J. Yamagishi, and I. Echizen, “Modular con-
volutional neural network for discriminating between
computer-generated images and photographic images,”
in Proceedings of the 13th international conference on
availability, reliability and security, 2018, pp. 1–10.

[13] P. He, X. Jiang, T. Sun, and H. Li, “Computer graph-
ics identiﬁcation combining convolutional and recur-
rent neural networks,” IEEE Signal Processing Letters,
vol. 25, no. 9, pp. 1369–1373, 2018.

[14] R.-S. Zhang, W.-Z. Quan, L.-B. Fan, L.-M. Hu, and
D.-M. Yan, “Distinguishing computer-generated images
from natural images using channel and pixel correla-
tion,” Journal of Computer Science and Technology,
vol. 35, no. 3, pp. 592–602, 2020.

