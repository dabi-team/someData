2
2
0
2

r
a

M
8
2

]
E
S
.
s
c
[

1
v
6
8
6
4
1
.
3
0
2
2
:
v
i
X
r
a

REPTILE: A Proactive Real-Time Deep
Reinforcement Learning Self-adaptive
Framework

Flavio Corradini1[0000−1111−2222−3333], Michele Loreti1[1111−2222−3333−4444],
Marco Piangerelli1[2222−−3333−4444−5555], and Giacomo Rocchetti1

University of Camerino, Camerino MC 62031, Italy
{flavio.corradini, michele.loreti, marco.piangerelli}@unicam.it
giacomo.rocchetti@studenti.unicam.it

Abstract. In this work a general framework is proposed to support the
development of software systems that are able to adapt their behaviour
according to the operating environment changes. The proposed approach,
named REPTILE, works in a complete proactive manner and relies on
Deep Reinforcement Learning-based agents to react to events, referred
as novelties, that can aﬀect the expected behaviour of the system. In our
framework, two types of novelties are taken into account: those related to
the context/environment and those related to the physical architecture
itself. The framework, predicting those novelties before their occurrence,
extracts time-changing models of the environment and uses a suitable
Markov Decision Process to deal with the real-time setting. Moreover,
the architecture of our RL agent evolves based on the possible actions
that can be taken.

Keywords: Self-Adaptivity · Deep Reinforcement Learning · Proactiv-
ity

1

Intoduction

The last decades witnessed a signiﬁcant evolution in software designing enact-
ment as the process of managing, developing and maintaining software is be-
coming more and more complex. The always increasing expansion of Internet
together with the advent of a plethora of smart devices, posed new challenges
to engineers and developers that have to design and maintain software infras-
tructure that must be able to adapt their behaviour to new requirements and
updates in the system architecture often unknown at design time. These can
have a great impact on the Quality of Service (QoS) oﬀered by the system, thus
impacting the ﬁnal user experience.

In this context, Kephart and Chess stated that “the only option remaining is
autonomic computing systems that can manage themselves given high-level objec-
tives from administrators” [10]. Such systems are commonly called Self-Adaptive
Systems (SASs) and they have been extensively researched and developed in the

 
 
 
 
 
 
2

F. Corradini et al.

last twenty years. The main feature of SASs is to be able to operate in an en-
vironment that is changing in an unpredictable way without any, or limited,
external control. For this reason, a big eﬀort has made to design methodologies
and techniques supporting SASs adaptation.

Several approaches have been proposed to design and develop SASs [13]. In
particular we can mention here the model-based, the architecture-based, the con-
trol theory-based, the formal modeling and veriﬁcation-approach and the Learning-
based approaches. The model-based approach uses model-driven engineering, ﬁll-
ing incomplete information at design time with the usage of run-time models
(MUSIC framework [9]). The architecture-based approach use the concept of
the high level structure of software for diﬀerent activities. One of the most
well-known frameworks that implements this approach is the Rainbow frame-
work, which provides an architecture layer whose components deﬁne adaptation
plans [8]. In the control theory-based approach the system continuously takes
measurements (from sensors) and performs adjustments to keep the measured
variable in a deﬁned range in a process called feedback loop [18]. Together with
the implementation of SASs, there is the need to prove the correctness of such
systems using formal modeling and veriﬁcation approaches. A common method
is the formal reference model for self-adaptation (FORMS), which uses the Z
notation in order to deﬁne the whole system, from its interaction with the en-
vironment to the adaptation [27]. The latter approach, the learning-based one,
given the increasing popularity of the ﬁeld of Machine Learning (ML), mainly
in the last decade, aﬀected a lot the research of Self-Adaptive Systems using ML
algorithms for a variety of purposes: from the use of genetic algorithms for the
run-time adaptation of mobile applications in [20], or the use of Reinforcement
Learning (RL) for an architecture-based adaptation [11], to the application of
Deep Learning (DL) for SASs in network security [19,15].

Moreover, challenges such as the analysis of streams of data in real-time and
the extraction of environmental models in order to aid a proactive adaptability
for unforeseen changes have to be adequately taken into account. To this extent,
we can mention a framework for self adaptive systems proposed in [7]. The
authors tackle the problem of unforeseen changes in the environment by learning
the impact of adaptation decisions on the system’s goal applying a reactive, i.e.
post-hoc or after-the-fact, model. A more reﬁned framework is the one proposed
in [12]. It is based on the MAPE-K framework, extended with a simpliﬁed RL
component.

Finally, the IoTArchML framework refers to a proactive SAS applied to iot
systems [17]. In particular, it is designed for the control of system’s QoS param-
eters, using a proactive approach to anticipate eventual changes before a QoS
deviation event.

In this work we propose a general framework for software SASs dealing with
real-time streams of data. Our framework works in a complete proactive manner.
It uses of Deep Reinforcement Learning (DRL) algorithms, in order to adapt
itself to eventual novelties, i.e. new events that can be managed in order to work
properly. In our framework, as already pointed out in [12], we identify two types

Title Suppressed Due to Excessive Length

3

of novelties: those relating to the context/environment and those relating to
the physical architecture itself. The framework, predicting those novelties before
their occurrence, extract time-changing models of the environment and uses a
modiﬁed Markov Decision Process to deal with the real-time setting as shown
in [23]. Moreover the architecture of our RL agent evolves based on the possible
actions that can be taken.

The paper is structured as follows: in section 2 the self-adaptability frame-
work is described, section 3 presents the explanation of the adaptability mecha-
nisms, in section 4 we show the results obtained by applying the framework to
a real case study and, ﬁnally, in section 5 we discuss the results.

2 REPTILE Architecture

In this Section we provide a general overview of our framework, REPTILE.
It is based on the well known IBM’s MAPE-K architecture enriched with a
Reinforcement Deep Learning module. The whole architecture of REPTILE is
depicted in Figure 1. It consists of six modules: the ﬁve from the standard MAPE-
K architecture (Monitoring, Analyzing, Planning, Executing, and Knowledge)
plus a Reinforcement Deep Learning one. Below, the role of these modules is
outlined together with their use in a simple running scenario.
Monitoring (yellow). It continuously collects data from the environment and
performs an extraction of useful features of the system, based on the domain
application. It then sends data to the Knowledge component.

Example 1 (Running Case Study (1/6)). To better illustrate the technicalities of
our framework we present here a running case study. We applied our framework
to a Heating, Ventilation and Air Conditioning (HVAC) system. The reason be-
hind the choice of this particular application domain is its simplicity. A simulator
for the environment was implemented in Python: it computes the next internal
temperature of a single room based on the actions performed by the system. The
simulator is made up by the following three devices:

– Heater: it produces power (Watts) in order to heat the room.
– Cooler: it produces power (Watts) in order to cool the room.
– Window: managed by a binary signal (open or closed).

The gained/lost temperature inside a room is obtained by calculating the
heat loss between the room and the outside, through walls and windows, and
converting the power produced by the heater as a heat gain and the one produced
by the cooler as heat loss.

Both the description of the room and the available devices are customizable.
A device is any instrument that can modify the internal temperature with its
action. Table 1 shows the simulations parameters. External temperatures data
have been pre-processes from two existing datasets [1,2].

4

F. Corradini et al.

Fig. 1: Overview of REPTILE.

Parameter
Room Width
Room Height
Room Depth
Wall R-Value
Window R-Value
Air Changes per Hour
N. Heaters
N. Coolers
N. Windows
Heater Power (W)
Cooler Power (W)

Value
5
3
3
3.6
0.176
1.7
1
1
1
0, 200, 400
0, 400

Table 1: Room parameters used in simulations

Title Suppressed Due to Excessive Length

5

Knowledge (yellow and blue). It receives data from the Monitoring module
and it builds two types of models of the environment: time-varying models, MT V
t, ¯α .
and global models, MG
ξ . The former is used for monitoring the model in the
short-period, while the latter represents the comprehensive knowledge of the
environment. In the previous notation t represents a generic time step, ¯α the
vector containing the parameters of the model. Superscript T V indicates that we
are modeling the time-varying, while G stands for Global, where ξ = 1, 2, 3..., k
is the index identifying each model . Moreover, this module contains some pre-
trained agents, MA
ξ , where ξ = 1, 2, 3..., n is the index identifying each model
and the superscript A stands for agent.

Example 2 (Running Case Study (2/6)). In our running case study, the knowl-
t, ¯α , MG
edge consists in a set of models ( MT V
t, ¯α is represented
by using an Auto-Regressive, Moving Average and Integrated (ARIMA)
model [3].

ξ ). Each MT V

ξ , MA

Such models are described formally by the following equation:

y(cid:48)
t = c + (cid:15)t +

p
(cid:88)

i=1

ϕiy(cid:48)

t−i +

q
(cid:88)

i=1

θi(cid:15)t−i

(1)

where θi and φi are the parameters of the model, p and q are the orders,
respectively of the auto-regression and the moving average contributions; The
orders of an ARIMA model are deﬁned as follows:

– p: the number of lag observations included in the model, also called the lag

order.

– q: the size of the moving average window, also called the order of moving

average.

iid∼ N (0, σ2) and c = µ + ϕ0 with µ the expected value of yt and y(cid:48)

t = yt − yt−1.
(cid:15)t
To obtain the model we used a number of data points d = 30. Each time a
new data point arrives, we compute a new model discarding the oldest point
end incorporating the new one. The MG
ξ models can be obtained using more
sophisticated techniques. In this work, for simplicity, we already had such models.
Finally, MA
ξ is the model obtained using some DRL-based model.

Analyzing (blue). It inspects the data stored in the Knowledge, that is a model
of the collected data, and uses it to forecast possible evolution. At each step this
module is able to detect potential unseen behaviours, i.e. novelties.

We can distinguish two kinds of novelties: Contextual novelty and Archi-
tectural novelty [12]. The former indicates a novelties are mainly inferred by
comparing the predictions done at the previous steps with the one at the cur-
rent step. For instance, the module can detect that an observable, namely the
temperature, is going to run out of the expected range in the next ﬁfteen minutes,
since the predicted value increased in the last observations. The latter refer to
a change in the interaction between the system and the environment due to a

6

F. Corradini et al.

new capability that is now available. For instance, a new device is installed that
can aﬀect the observations.

Actually, this module is also able to detect novelties in a reactive manner, i.e.
if the observable violates a certain ”soft” constraint for a given amount of time.
For instance, whenever temperature is in the Intermediate zone (see Section 2.1)
for t previous time steps.

Example 3 (Running Case Study (3/6)). In this case study, we identify novelties
in the following way:

– a new device has been inserted into the system (architectural novelty).
– the internal temperature does not stay in the interval [18 °C, 22 °C] (con-

textual novelty).

First of all, REPTILE looks for new devices or changes in the devices them-
selves, i.e. e new heater with diﬀerent powers of use. If such a component is
detected, the system begins the adaptation procedure in order to adapt the
structure of the learning module to manage new actions. If no new device is
found the system continues to behave as usual. Moreover, REPTILE uses also
the predictions of the next three points for deciding about the onset of the adapt-
ability process: if the third point (the farthest in time) is not in the comfort zone
then the adaptability process is triggered, otherwise everything remains the same
(proactive process). In the meantime, the T = 12 previous time steps are moni-
tored in order to establish if system has been keeping violating constraints since
12 time steps: in that case the adaptability process is triggered, too (reactive
process). The executing module, i.e, the DRL-based agent performs an action
every 15 minutes while the ARIMA model is derived every 5 minutes meaning
that the system had 3 time steps to try to correct its behavior.

Learning (blue and grey). It is the crucial component of the framework and
it is activated when a novelty is foreseen. This module is responsible for the
choice of the optimal adaptive policy. It uses the Knowledge and a DRL-based
agent (the same used by the Planning module) for the adaptation process. Once
a novelty is detected, the Learning component is invoked, otherwise the Plan
component will decide the next action based on the current DRL-based agent.
Generally speaking, DRL is about maximizing a reward function, i.e. a cost
function, in order to make the DRL-based agent to learn how to behave in an
environment (refer to Subsection 2.1 for the reward function used in this work).

Example 4 (Running Case Study (4/6)). We used the Deep Q-Learning algo-
rithm for implementing the Learning module: in particular we implemented a
Duelling Deep Q-network [26]. The Dueling Deep Q-network splits the single
stream of fully connected layers in two separate streams, one for the value func-
tion, V , and one for the advantage function, A, deﬁned as:

Aπ(s, a) = Qπ(s, a) − V π(s)

(2)

In our implementation of the Dueling Deep-Q network we used the real time
markov decision process (RTMDP) framework [23], see Section 2.1 for more

Title Suppressed Due to Excessive Length

7

details. Accordingly, states and actions evolve simultaneously. The state vec-
tor is then denoted as a state-action pair xt = (st, at). The actions in input
to the Duelling Deep Q-network are encoded using a one-hot encoding of the
combinations of the basic actions of the three devices. For example, referring
to Table 4, there are 12 actions (3 for the Heater, 2 for the Cooler and 2 for
the Window). Using a possible one-hot encoding, Table 2 shows all the pos-
sible actions: If the vector at = (1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) meaning Heater =

at

Heater Cooler Window
0
0
200
200
0
0
200
200
0
0
200
200

CLOSE 1,0,0,0,0,0,0,0,0,0,0,0
OPEN 0,1,0,0,0,0,0,0,0,0,0,0
CLOSE 0,0,1,0,0,0,0,0,0,0,0,0
OPEN 0,0,0,1,0,0,0,0,0,0,0,0
CLOSE 0,0,0,0,1,0,0,0,0,0,0,0
OPEN 0,0,0,0,0,1,0,0,0,0,0,0
CLOSE 0,0,0,0,0,0,1,0,0,0,0,0
OPEN 0,0,0,0,0,0,0,1,0,0,0,0
CLOSE 0,0,0,0,0,0,0,0,1,0,0,0
OPEN 0,0,0,0,0,0,0,0,0,1,0,0
CLOSE 0,0,0,0,0,0,0,0,0,0,1,0
OPEN 0,0,0,0,0,0,0,0,0,0,0,1

0
0
0
0
200
200
200
200
400
400
400
400

Table 2: One-Hot encodind of the actions at

0 W, Cooler = 0 W, Window = CLOSE, and st = (Text = 17, Tin = 19),
then xt = (17, 19, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0). As shown in Figure 2, our Duelling
Deep-Q Networks is made by 3 fully connected hidden layers each of which con-
tains 256 neurons. The Input layer contains 14 neurons and the Output ones
consist of 12 neurons.

Planning (green). This component simply uses the DRL-based agent to choose
the next action to perform.

Example 5 (Running Case Study (5/6)). Given in input xt = (17, 19, 1, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0), then the Deep Q-Network chooses the next action to perform
as at+1 = (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0) (Heater = 0 W, Cooler = 200 W, Window
= OPEN).

Executing (red). Just as a usual MAPE-K, this component is in charge of
executing the action planned by the Planning component.

Example 6 (Running Case Study (6/6)). The Executor will closed the window
and switch oﬀ the heater

8

F. Corradini et al.

Fig. 2: The Dueling Deep Q-Network used in this work.

2.1 Real Time Reinforcement Learning Agent

The classical idea of a RL-based agent is the following: the agent receives the
state from the environment, it selects an action according to a transition distri-
bution and it performs that action, changing the environment. This scenario is
modeled by using Markov Decision Process (MDP) [25]. The fundamental prob-
lem with this conception is that the environment is considered paused during
the action selection and that is why MDP is referred to Turn-based Markov De-
cision Process (TBMDP) in [23]. While it is suited for problems such as board
games, it is certainly not suited for real-time applicationsIn order to derive the
MA
ξ , where ξ = 1, 2, 3..., n is the index assign to each model and the superscript
A indicates that we are speaking about a model of the Agent, we implemented
a Dueling Deep Q-network (Figure 2), using the RTRL paradigm. In RL, the
goal is maximizing the reward function to let the agent learn how to behave in a
given environment. It has an experience replay and a target network, performing
a soft update, to optimize the training phase.

The exploration-exploitation trade-oﬀ is given by (cid:15): the chosen action at
time step t, will be a greedy action (exploit) with probability (1-(cid:15)) or may be a
random action (explore) with probability of (cid:15). Then it decrease the parameter (cid:15)
by an (cid:15)-decay value. The soft update is regulated by the parameter τ . The list
of parameters used in the study are shown in Table 3.

To enable the agent to calculate the reward of its actions and to manage the
training process, the Open AI Gym toolkit has been used [5]. The advantage of
using this tool is that it can be used by every type of algorithm and it does not
need any knowledge about the RL agent interacting with it. In fact, an Open
AI environment only needs the action chosen by the agent in order to return

Title Suppressed Due to Excessive Length

9

Parameter
Memory Size
Batch Size
(cid:15)-decay
τ
Hidden Layers Neurons
Training Episodes

Value
1000000
256
0.0002
0.005
256
600

Table 3: Reinforcement Learning Agent parameters.

the next state and the reward for that action. Every step of the environment is
considered to be scheduled at 15 minutes intervals.

Reward Function A reward function balancing energy consumption and
thermal comfort was chosen. The two aspects are conﬂicting: in order to have
an ideal thermal comfort, consumption of energy is needed, while saving energy
could result in thermal discomfort. In this work, we modiﬁed the reward function
deﬁned in [4]. Let us deﬁne the setpoint as the desired comfort temperature set
by the user and let us deﬁne two parameters (cid:15), δ > 0 such that (cid:15) < δ.

– Comfort Zone: area in the interval [setpoint − (cid:15), setpoint + (cid:15)]
– Intermediate Zone: two areas enclosed in the intervals (setpoint−δ, setpoint−

(cid:15)] and (setpoint + (cid:15), setpoint + δ].

– Danger Zone: the two remaining intervals (−∞, setpoint − δ), (setpoint +

δ, +∞).

The reward function is deﬁned as follows:





0,
−β · energy − ρ · (setpoint − Tin)2,

if setpoint − (cid:15) ≤ Tin ≤ setpoint + (cid:15)
if setpoint − δ ≤ Tin < setpoint − (cid:15)
∨ setpoint + (cid:15) < Tin ≤ setpoint + δ

−β · energy − ρ · (|setpoint − Tin|)3, otherwise

(3)
where Tin is the internal temperature, β and ρ are parameters that permit to
balance the trade-oﬀ between the energy consumption and the thermal comfort.
Table 4 shows the values of these parameters used in the simulations.

3 Self-Adaptive strategy

To managing the two diﬀerent kinds of novelties, we implement three diﬀer-
ent self-adaptive strategies: the Model Switching Adaptation, the Architectural
Adaptation and the DQN-Knowledge-based Adaptation. The latter method al-
ways starts only if the ﬁrst two fail.

10

F. Corradini et al.

Parameter Value

Setpoint
(cid:15)
δ
β
ρ

20
2
4
0.05
1

Table 4: Reward function parameters used.

Model Switching Adaptation REPTILE applies this strategy when a context
novelty is detected. All the available pre-trained models MA
k are
evaluated and the best one, in terms of some evaluation parameters, is selected.
Our selecting criterion is based on returning the greatest reward using the current
data.

2 ,...,MA

1 , MA

Architectural Adaptation This strategy of adaptation is based on the idea
that the DRL-agent can learn to perform new actions if it is aware of having
instruments allowing them: equipped with new devices there is a change in the
number of potential actions the agent can perform. The solution we adopted is
pretty straightforward: we modify the structure of our Dueling Deep-Q network,
adding a number of input neurons and output neuron that allow us to encode
the new actions, too. Let us consider a model MA
i that has been trained using N
actions, if a new device is available we enrich the Dueling Deep-Q network with
a number K of neurons such that the other N + M actions can be taken into
account. Actually the re-modulation of the Dueling Deep-Q network architecture
does not imply automatically the re-training of the model MA
i : if the agent can
perform well there in no need to use the new device. That is the reason why the
connections to and from the new neurons are set to 0: in this way, the trained
models can be used in a context of N + M actions, but maintaining the original
behavior.

DQN-Knowledge-based Adaptation This strategy answer the question:
what if the other two adaptation methods fail? If a novelty that cannot be
managed using already available models, is detected, REPTILE should use the
data stored in the Knowledge for training a new DRL-based agent both if there
are new devices and if not.

3.1 Self-adaptation process

The complete self-adaptation process, shown in Figure 3, works in the following
way:

1. The system continuously monitors the environment and updates the current

Knowledge.

Title Suppressed Due to Excessive Length

11

2. The Analyzing module checks for a possible novelty: if an architectural nov-
elty is detected, all current models are updated (neurons are added) and
the system goes back to the monitor phase (step 1); on the other hand, if
it is not an architectural novelty, the system selects the best model for the
current environment, which is the model allowing the RTRL agent to gain
the most cumulative reward; if no novelty is detected, the system goes back
to the monitor phase.

3. Once the best model has been selected, the system waits n steps in order to
observe the behavior of the agent using the selected model. The quantity N
can vary based on the application domain of the SAS.

4. After N steps, the system checks if the model fulﬁlls all system’s goals even
in the presence of the novelty: if so, the system has successfully adapted to
the novelty and the monitor process starts again (step 1), otherwise an alarm
is sent to the maintenance: this is done because it means that the system
can not fulﬁll its goals with its current models, communicating that it has
to begin the training phase.

5. Once the alarm has been ﬁred, the system keeps selecting the best model at
its disposal in order to maintain a safe state while a new model is trained us-
ing the new environment’s data stored in the Knowledge to generate episodes
for the RTRL agent.

6. Once the training process is terminated, the system selects the new model

and observes the new behavior (step 3).

Fig. 3: Model Switching Adaptation Diagram.

12

F. Corradini et al.

4 Results

In this paper we applied REPTILE to a HVAC system. Such a system was
implemented in Python and it is able to compute the internal temperature of
a room by simulating the actions of three devices: a heater, a cooler and a
window. The parameters used by the simulator are reported in Table 1. Since
REPTILE is based on DRL-basd agent we interfaced our simulator with the the
Open AI environment that invokes the temperature simulator to calculate the
Agents’s next state. REPTILE extends the MAPE-K framework with a Learning
module, based on DRL, and equips the Knowledge module with a proactive
capability based on the possibility of creating on-the-ﬂy time-varying models of
the environment and foreseen their next states.

Fig. 4: Winter Agent. On the left, the monitoring of the temperature and the
actions taken by the agent for maintaining the temperature in the comfort zone;
on the right, the mazimization of the reward function is shown

2 ,...,MA

The Learning module comes into play whenever a novelty is detected and,
using the knowledge of the environment (or simulate the knowledge from the
collected data), it is able to trigger an adaptation strategy: the model switching
one, the system architectural one or the re-training one. The proactive Knowl-
edge module is able to derive a model of the environment by using the ARIMA
formalism and, at the same time, contains some pre-trained models of DRL-based
1 , MA
agents, MA
k , and some global models of the changing environment,
1 ,...,MG
MG
j , in order to avoid to use some speciﬁc techniques to derive such
models. In this work, only the real data about external temperatures, Text, were
used. They consist of time series whose values were collected every hour [1,2]. We
pre-processed those data by replicating each value 3 times in order to simulate
a sampling frequency of 15 minutes. We also have a model MG and a model
MA. We also have no available new devices. We show the results in two diﬀer-
ent operative conditions: summer and winter. All the results described below are
obtained using a laptop with the Mint Linux OS, 8Gb RAM, a dual-core Intel
i7@2.7GHz CPU and a graphic card Nvidia GeForce GTX 940MX. Initially, we

Title Suppressed Due to Excessive Length

13

started by having an agent that is trained to behave in winter conditions. We
used 600 episodes, with a decreasing (cid:15)-greedy exploration-exploitation trade-oﬀ
starting from 1 and an experience reply soft optimization. The learning proce-
dure took about 1 hour to be completed.

The behavior of such an agent is depicted in the left panel of Figure 4: the
upper block shows the internal temperature (blue line) remaining inside the
comfort zone, 18°C - 22°C (red horizontal lines), and the external temperature
(orange line).

(a) No adaptation

(b) Adaptation

Fig. 5: Test of Novelty Detection. The Agent can not go back in the Comfort
Zone, an alarm is sent and a new model is trained.

Moreover, the three blocks below display the actions performed by all the
tree devices. In particular only the heater keep on increasing and decreasing its
power while the cooler and the window do not act.

In order to test the Model Switching Adaptation and the DQN-Knowledge-
based Adaptation, we let the agent behaves in a summer scenario where external
temperatures range from 20 to 40 °C. This is a trick for being sure the agent
stumbles across a novelty. Figure 5a shows an example of triggering an adap-
tation process. Horizontal red lines represent our comfort zone while the blue
line is the indoor temperature. After 15 time steps (red circle) the Analyzing
modules detects, in a reactive manner, a violation of our “soft” constraint: not
to be in the Intermediate zone for an hour. Now the system starts the adaptation
process ﬁnding no new devices (actually the search for new devices is performed
at each time step) and only one pre-trained model available: MA. Such a model
has been prepared in a way that the system can choose it (its reward is higher)
but is not successful in adapting. After 3 time steps (remember that the agent
perform an action every ﬁfteen minutes), at time step 18, the indoor temperature
is again out of range; moreover, the third predicted point (the rightmost yellow
point) is in the Danger zone. At this point, given the impossibility of adapting

14

F. Corradini et al.

with the ﬁrst two strategies, the system chooses to re-train a new DRL-based
model using the available MG notifying and alarm to the system administrator.
The new agent is trained with MG, containing summer environmental infor-
mation, for the same 600 episodes as the winter agent. The inset in Figure 5b
shows the maximization of the reward function. The training phase took 1 hour
to be completed in this case, too. Once the new agent has been trained, the
system switches form the current model to the new one and it is able to bring
the internal temperature back to the Comfort Zone, as depicted in Figure 5b.

The above result underlines the ability of the system to adapt its behavior
to new operative conditions. In this case it is worth noticing that the knowledge
regarding the new environment (summer scenario) was already in the knowledge
module but REPTILE potentially allows to implement any technique to acquire
such a knowledge. Finally, in order to test the Architectural Adaptation a new
heater with diﬀerent power values (50 W, 200 W, 250W ,400W) was plugged
into the system.

Fig. 6: Adaptation after a new device was plugged into the system. On the left
the dynamics of the monitoring of the system with the corresponding actions
taken by the DRL-based agent; on the right the maximization of the reward
function during the re-training process.

Once the system detected the new device, it immediately modiﬁed the ar-
chitecture of the Duelling Deep-Q network in order to be able to manage two
more actions and then it ran the adaptive procedure: in this the system was
running in summer, it was not able to manage the winter external condition so
it started the adaptive procedure. The procedure took again about one hour and
produce a behavior like the one shown in Figure 6. Comparing Figure 6 (left)
with Figure 4 it is evident the diﬀerent actions the agent is performing in order
to keep the temperature in the comfort zone. It is also evident that the agent
continues to open and close the window and synchronously raises and lowers the
heating. The meaning of this behavior could be that the agent, despite man-
aging to keep the temperature in the comfort zone, would need more episodes
to optimize its behavior. The previous hypothesis can be conﬁrmed if we look

Title Suppressed Due to Excessive Length

15

at Figure 6 (right): even though the reward function is maximizing, there are
many large amplitude ﬂuctuations. It is worth noticing that for showing the
eﬀectiveness of Architectural Adaptation the system was induced to apply the
DQN-Knowledge-based Adaptation, too.

5 Conclusions

In this paper, we have presented REPTILE, a framework for the design of proac-
tive, real-time and self-adaptive systems based on Deep Reinforcement Learning
(DRL). The proposed framework extends IBM’s MAPE-K loop with a DRL
module that is used to see in advance potential dangerous and unwanted situa-
tions, named novelties, that may aﬀect the expected behaviour of the considered
system. Two types of novelties are taken into account in our framework: those
related to the context/environment and those regarding the physical architecture
itself [12]. In the ﬁrst case, a set of models, that are either built via a pre-existing
knowledge or via the one acquired on the ﬂy, are used to identify the best coun-
termeasures/actions to execute in order to guarantee the satisfaction of some
given requirements. If no model is available, or when an architectural novelty
is detected, the DRL module is updated to learn new behavioral policies and,
consequently, to derive new models that will support the adaptation in the new
conﬁguration. When both these strategies fail, a DQN-Knowledge-based Adap-
tation is executed to re-train the DRL module.

The proposed framework can be applied to both simulated and real/physical
systems. However, in the latter case, the appropriate software interfaces should
be developed to let REPTILE controls physical devices. To show an application
of REPTILE, in the paper, a Heating, Ventilation and Air Conditioning sys-
tem (HVAC) has been used. The performed experiments conﬁrm the capability
of our framework to guide the adaptation procedure. We can observe that in
the considered example, both the Model Switching Adaptation and the DQN-
Knowledge-based Adaptation seem working well, due to the global model MG
ussed for training the new DRL module.

As far as we know, this is the ﬁrst proposal taking into account both the
proactivity and the use of real-time deep reinforcement learning paradigm, ap-
plied to a Deep Q-Network, for managing data streams.

Even if the Architectural Adaptation showed good performances, we believe
that further studies are needed to evaluate the possibility of exploiting the trans-
fer learning paradigm to avoid the completely re-training of the Deep-Q Net-
work [14].

Finally, one of the key point in REPTILE is MG

ξ that is the model used
whenever we need to re-train the DRL-Agent. The better is MG
ξ , the more
accurate is MA
ξ . These models can be derived directly from data collected from
existing systems by means of diﬀerent data analysis techniques [16,24,21,6,22].
In the case of the MG
ξ models considered in this paper, existing time series have
been used. Nevertheless, further studies are needed to identify the algorithms

16

F. Corradini et al.

that are able to derive a model from a (possibly small) set of data, or fast enough
to deal with a vary large amount of data.

References

1. Beach weather stations - automated sensors (2020), https://catalog.data.gov/

dataset/beach-weather-stations-automated-sensors

2. Beniaguev, D.: Historical hourly weather data 2012-2017 (2017), https://www.

kaggle.com/selfishgene/historical-hourly-weather-data

3. Box, G.E., Jenkins, G.M., Reinsel, G.C., Ljung, G.M.: Time series analysis: fore-

casting and control. John Wiley & Sons (2015)

4. Brandi, S., Piscitelli, M.S., Martellacci, M., Capozzoli, A.: Deep reinforcement
learning to optimise indoor temperature control and heating energy consumption
in buildings. Energy and Buildings 224, 110225 (2020)

5. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,

Zaremba, W.: Openai gym. arXiv preprint arXiv:1606.01540 (2016)

6. De Simone, A., Piangerelli, M.: A bayesian approach for monitoring epidemics in
presence of undetected cases. Chaos, Solitons & Fractals 140, 110167 (2020)
7. Elkhodary, A., Esfahani, N., Malek, S.: Fusion: a framework for engineering self-
tuning self-adaptive software systems. In: Proceedings of the eighteenth ACM SIG-
SOFT international symposium on Foundations of software engineering. pp. 7–16
(2010)

8. Garlan, D., Cheng, S.W., Huang, A.C., Schmerl, B., Steenkiste, P.: Rainbow:
Architecture-based self-adaptation with reusable infrastructure. Computer 37(10),
46–54 (2004)

9. Hallsteinsen, S., Geihs, K., Paspallis, N., Eliassen, F., Horn, G., Lorenzo, J.,
Mamelli, A., Papadopoulos, G.A.: A development framework and methodology
for self-adapting applications in ubiquitous computing environments. Journal of
Systems and Software 85(12), 2840–2859 (2012)

10. Kephart, J.O., Chess, D.M.: The vision of autonomic computing. Computer 36(1),

41–50 (2003)

11. Kim, D., Park, S.: Reinforcement learning-based dynamic adaptation planning
method for architecture-based self-managed software. In: 2009 ICSE Workshop on
Software Engineering for Adaptive and Self-Managing Systems. pp. 76–85. IEEE
(2009)

12. Kl¨os, V., G¨othel, T., Glesner, S.: Adaptive knowledge bases in self-adaptive system
design. In: 2015 41st Euromicro Conference on Software Engineering and Advanced
Applications. pp. 472–478. IEEE (2015)

13. Krupitzer, C., Roth, F.M., VanSyckel, S., Schiele, G., Becker, C.: A survey on
engineering approaches for self-adaptive systems. Pervasive and Mobile Computing
17, 184–206 (2015)

14. Kwiatkowski, R., Lipson, H.: Task-agnostic self-modeling machines. Science

Robotics 4(26), eaau9354 (2019)

15. Maim´o, L.F., G´omez, ´A.L.P., Clemente, F.J.G., P´erez, M.G., P´erez, G.M.: A self-
adaptive deep learning-based system for anomaly detection in 5g networks. IEEE
Access 6, 7700–7712 (2018)

16. Merelli, E., Piangerelli, M.: Rnn-based model for self-adaptive systems (2014)
17. Muccini, H., Vaidhyanathan, K.: A machine learning-driven approach for proactive
decision making in adaptive architectures. In: 2019 IEEE International Conference
on Software Architecture Companion (ICSA-C). pp. 242–245. IEEE (2019)

Title Suppressed Due to Excessive Length

17

18. M¨uller, H.A., Pezz`e, M., Shaw, M.: Visibility of control in adaptive systems. In:

ULSSIS@ ICSE. pp. 23–26 (2008)

19. Papamartzivanos, D., M´armol, F.G., Kambourakis, G.: Introducing deep learning
self-adaptive misuse network intrusion detection systems. IEEE Access 7, 13546–
13560 (2019)

20. Pascual, G.G., Pinto, M., Fuentes, L.: Run-time adaptation of mobile applications
using genetic algorithms. In: 2013 8th International Symposium on Software En-
gineering for Adaptive and Self-Managing Systems (SEAMS). pp. 73–82. IEEE
(2013)

21. Piangerelli, M., Maestri, S., Merelli, E.: Visualising 2-simplex formation in
metabolic reactions. Journal of Molecular Graphics and Modelling 97, 107576
(2020)

22. Quadrini, M., Cavallin, M., Daberdaku, S., Ferrari, C.: Prosps: Protein sites pre-
diction based on sequence fragments. In: International Conference on Machine
Learning, Optimization, and Data Science. pp. 568–580. Springer (2021)

23. Ramstedt, S., Pal, C.: Real-time reinforcement learning.

In: Wallach, H.,
Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., Garnett, R.
(eds.) Advances in Neural Information Processing Systems. vol. 32. Curran
Associates,
Inc. (2019), https://proceedings.neurips.cc/paper/2019/file/
54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf

24. Rucco, M.: Survey of topdrim applications of topological data analysis
25. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press

(2018)

26. Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., Freitas, N.: Dueling
network architectures for deep reinforcement learning. In: International conference
on machine learning. pp. 1995–2003. PMLR (2016)

27. Weyns, D., Malek, S., Andersson, J.: Forms: a formal reference model for self-
adaptation. In: Proceedings of the 7th international conference on Autonomic com-
puting. pp. 205–214 (2010)

