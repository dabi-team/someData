Received ;

Revised ;

Accepted

DOI: xxx/xxxx

ARTICLE TYPE

iExam: A Novel Online Exam Monitoring and Analysis System
Based on Face Detection and Recognition

Xu Yang1 | Daoyuan Wu*1 | Xiao Yi1 | Jimmy H. M. Lee2 | Tan Lee3

1Department of Information Engineering,
The Chinese University of Hong Kong,
Hong Kong SAR, China
2Department of Computer Science and
Engineering, The Chinese University of
Hong Kong, Hong Kong SAR, China
3Department of Electronic Engineering, The
Chinese University of Hong Kong, Hong
Kong SAR, China

Correspondence
* Corresponding author. Email:
dywu@ie.cuhk.edu.hk

Present Address
The Chinese University of Hong Kong,
Shatin, NT, Hong Kong SAR, China

2
2
0
2

n
u
J

7
2

]

V
C
.
s
c
[

1
v
6
5
3
3
1
.
6
0
2
2
:
v
i
X
r
a

Summary

Online exams via video conference software like Zoom have been adopted in many

schools due to COVID-19. While it is convenient, it is challenging for teachers to
supervise online exams from simultaneously displayed student Zoom windows. In

this paper, we propose iExam, an intelligent online exam monitoring and analysis

system that can not only use face detection to assist invigilators in real-time student

identiﬁcation, but also be able to detect common abnormal behaviors (including face

disappearing, rotating faces, and replacing with a diﬀerent person during the exams)

via a face recognition-based post-exam video analysis. To build such a novel sys-
tem in its ﬁrst kind, we overcome three challenges. First, we discover a lightweight
approach to capturing exam video streams and analyzing them in real time. Second,

we utilize the left-corner names that are displayed on each student’s Zoom window
and propose an improved OCR (optical character recognition) technique to automat-
ically gather the ground truth for the student faces with dynamic positions. Third, we
perform several experimental comparisons and optimizations to eﬃciently shorten
the training and testing time required on teachers’ PC. Our evaluation shows that

iExam achieves high accuracy, 90.4% for real-time face detection and 98.4% for post-

exam face recognition, while maintaining acceptable runtime performance. We have

made iExam’s source code available at https://github.com/VPRLab/iExam.

KEYWORDS:
Intelligent System, Computer Vision, Face Detection, Face Recognition

1

INTRODUCTION

With the usage of remote conference software, more and more online teaching modes and platforms have become famous around
the world. In the context of COVID-19, varieties of instant meeting software play an irreplaceable role in bridging teachers and
students to maintain normal courses as usual. As an essential component of students’ performance evaluation, the remote exam-
ination is gradually becoming a dominant trend after online teaching and distance learning in numerous universities. However,
unlike conventional classroom courses, it is hard to guarantee integrity and security of online exams due to the possibility of
fraud.

Although proctoring diﬃculty is a common problem that examiners have paid more attention to, there are few manners or
tools to detect and impede it. Taking online exams on Zoom as an example, ID-checking, requesting exam-takers to turn on
webcams, staring at the monitoring screen and recording the whole procedure are the common approaches of examiners. The

 
 
 
 
 
 
2

Xu Yang ET. AL.

current approach usually used in Zoom requires students to log in with designated campus accounts and authenticate student
identities with ID cards or other certiﬁcated document by webcam before exam start. It is challenging to focus on each individual
all the time, which will substantially increase the labor costs for invigilation. In fact, a massive amount of proctors almost have
no time to play the previous recording videos back and carefully check the details. In other words, this neglectful behavior
will inevitably lead to someone taking a risk to plagiarize in a short time gap, such as facing another direction to ask for help
via textbooks or even employing surrogate exam-takers to participate in online exams. Therefore, this type of problem can be
generally deﬁned as follows: Given one student ID and name mapping ﬁle and one test screen recording, detect the number of
face disappearances and localize the time of abnormal behavior in the video.

In order to oﬀer a remedy to the remote examination platform on Zoom, face recognition and veriﬁcation is needed for the
whole session. Therefore, we try to build a novel supervising system called iExam to identify the suspicious behavior when faces
disappear. If someone’s face is not capable of being recognized in a default time slot setting, then the system will note down
the corresponding period. Through this study, we address the diﬃculty of online examination supervision and management on
Zoom and truly achieve the goal of automatically monitoring the process by analyzing the recording video. At the same time,
iExam will separately generate the text and chart feedback for integrating detection results.

2

BACKGROUND

In this section, we will demonstrate the speciﬁc usage setting in one popular online conference software, Zoom, and mainstream
aﬃrmed methods by the public in the study of face classiﬁcation.

FIGURE 1 Face Recognition for Invigilation.

2.1 Online Exams in the “Zoom” Setting
As a powerful software, Zoom gives a solution to real-time recording view of online examinations and displays 25 (5 rows *
5 columns) participants per screen in default when enables gallery view mode. It can also support 49 (7 rows * 7 columns)
participants per screen if setting changes. For the reason that one recording screen at most can monitor up to 25 people, it is
necessary to employ extra assistants to keep recording for the other pages when participants are more than one screen limit. And
each recording video is capable of storing all the examinees’ facial features in “mp4” format video ﬁle, which will be stored in
local and will be the input of iExam. Besides, Zoom oﬀers a personal label for each participant to mark self-identity as above

Xu Yang ET. AL.

3

Fig. 1 . As an online conference platform, it satisﬁes the demand of monitoring the exam-takers surroundings environment. But
it is also lack of some algorithms to judge whether the students’ faces have already disappeared and give some alerts.

2.2 Mainstream Face Classiﬁcation Approaches
Many former researchers have already put forwards multiple face detection algorithms based on feature or image. The devel-
opment of the feature-based approach includes gray-scale, color and edge analysis. Under image-based detection algorithms
category, linear subspace methods such as Principal Component Analysis (PCA) 1 and Linear Discriminant Analysis (LDA) 2,3
also bring hope to the face recognition. But in this paper, we will exploit another branch in image-based detection algorithms
category called neural networks to localize the faces. The deep convolutional neural networks is capable of downloading from
the Internet which can be straightly called in our real-time face detection and also contribute to the dataset construction in post-
analysis. Additionally, it is pretrained by a lot of facial features which can signiﬁcantly decrease the detection error. At the same
time, we also ﬁnd the DNN method not only have the best detection amount but also the fast speed by comparison with python
current popular face detection methods.

3

SYSTEM OVERVIEW

FIGURE 2 A high-level workﬂow of iExam and its two phases of the analysis.

The purpose of this task is to automatically detect face and mark the face location in the real-time exam and we also aim
to summarize each person’s exam status speciﬁcally and ﬁnd concrete disappear period to avoid cheating by face recognition
and veriﬁcation in post-exam analysis. Thus we need two stages of operation, auxiliary proctoring tools during the exam and
analysis of the disappearance of each exam-taker after the exam.

DesktopInput videoand rosterDetectprocessDetect face and OCRThread#1Thread#2Multi-threadBuilddatasetandpreprocesstrainingProcessCNNmodelRecognize processVerify faceReport and GraphTest videoThread#1Thread#2CaptureZoomwindowDetect faceface locationRealTimeMonitoringPostExamAnalysis4

Xu Yang ET. AL.

In real-time component, we supply a lightweight website platform for capturing Zoom window to show the original gallery
mode view and the processed image. The handled image is to determine whether a student face is detected or not, which can
enable teachers quickly locate if a student is facing the screen to answer questions instead of facing the other side to ask for help.
In post-analysis component, we would like to utilize deep learning to recognize each student’s face and seek for the disappear
period. The main steps include dataset construction, training reasonable model, testing exam video and delivering a summary
report. In brief, We use a short input video to cut the face and compose a data set frame by frame by multi-threads. Then
choose appropriate model to learn each person’s facial features. Eventually, the name of the student corresponding to the face
is predicted based on the pretrained model and students who do not appear in the speciﬁed time period are ﬂagged. All above
workﬂow can be found in Fig. 2 .

4

LIGHTWEIGHT REAL-TIME VIDEO MONITORING BASED ON FACE DETECTION

In order to satisfy the remote low-cost, low-latency execution and reduce the need for conﬁguration environment, we also develop
the online version to locate the face quickly. The ﬁrst challenge is how to capture the Zoom window to the online webpage.
Because Zoom has not oﬀered an existing API to directly synchronize the screen view. The second challenge is which detection
method should be exploited. The third challenge is whether we need to deploy the detection method to the server sides. For
example, the client accesses this website and captures window images and then the browser needs to send pictures to the server
to locate the faces and ﬁnally send them back.

For task one, Mozilla oﬀers a mature Web API that can suggest to the user to choose and allow permissions to capture the
content of the display or its parts(such as a window) as a MediaStream. Our website can obtain any window content and then
render the same picture that users choose. For task two, we provide a relatively conservative method, haar-cascade detection,
and a relatively modern way, deep neural network detection. These two methods are both based on the OpenCVjs, which is a
JavaScript version for OpenCV functions used on the web platform. Of course, these two methods are also deployed conﬁguration
ﬁles as a basis to compare the facial feature with trained model ﬁle. For task three, the trained model has always been embedded in
the browser and not need to send private face information to the public outside network, which can perfectly solve the information
leakage issues that we have been concerned about. So this analysis is crucial to be executed on the client-side.

When using this lightweight real-time video monitoring approach, proctors can use the given website link to open the online
proctoring platform. Then proctors click the plus symbol and choose the Zoom window to start the window mirror. Now you can
see two views vertically arranged throughout the page. After selecting window, our website will download the model classiﬁer
from the Internet for detection. The upper frame on the page is the original Zoom view, and the below frame is the processed
image. Here are the intermediate process screenshot and the pictures after editing in Fig. 3 and Fig. 4 .

FIGURE 3 Choose “Zoom” in Browser.

FIGURE 4 Processed Image Display.

Xu Yang ET. AL.

5

5

COMPREHENSIVE POST-EXAM VIDEO ANALYSIS BASED ON FACE RECOGNITION

The main target of iExam’s post-exam analysis component is to automatically detect the disappearing faces and predict which
exam-taker did not appear in a certain time period in given test video. The obtained data will be used to generate reports with
corresponding histograms on cross-platform (Windows, macOS). This application is a new try for widely-used deep learning to
supply the cheating post-analysis.

Regarding these requirements, the whole system consists of three procedures: capturing faces, training classiﬁer, monitoring
suspicious behaviors and obtaining feedback. Initially, the proctor inputs a ﬁve-minute video as the source of training dataset.
Due to the storage of local personal computer RAM limitation, all captured face parts have to be saved as image ﬁle format
in local disk and build an exclusive dataset. Next, this dataset will be eﬀectively classiﬁed by adaptive ResNet which is a kind
of convolutional neural network (CNN). This convolutional neural network will extract and study the facial features to train an
adaptive classiﬁer for all exam-takers. Furthermore, the loaded checkpoint will be used to compare with diﬀerent individuals’
faces from the complete exam recording video (test video). This model will predict the student’s name and display the marked
label in the processing window.

To tackle this task, two major challenges emerge from the above procedures:

• How to get the corresponding label for each faces for CNN training.

The label and predicted name will be checked matching or not. If yes, it veriﬁes that the corresponding candidate is facing
the screen to answer the questions without head-turning cheating and the candidate has not asked a proxy to take the exam.

• How to design the neural network to train the classiﬁer more swiftly and obtain the good performance

Based on the above demand, we design the structure of iExam as the following graph Fig. 2 . Additionally, face detection

and recognition process will utilize multi-threads to speed up signiﬁcantly.

Automatic Ground Truth Collection

5.1
The basic idea for obtaining face images is to save trimmed images with corresponding labels from each frame in a training video
when iExam has already detected faces. As we all know, a sequence of images can compose a video, which means that halting
at a speciﬁc time of this sequence can obtain one image called one frame. Afterward, we load the DNN network and pretrained
Caﬀe model which is based on the Single Shot-Multibox Detector (SSD) and uses ResNet-10 architecture as its backbone. To
improve the detection accuracy, we also resize the image shape as 300*300 to ﬁt the model input. Then we can retrieve the
concrete face position (x-coordinate, y-coordinate, width and height) and save it. In order to save the cropped image, the key
point is to rename the image ﬁle that will determine which person it belongs to and which subfolder should be placed in.

It is obvious that the video screen is composed of ﬁve-by-ﬁve cells and each cell stands for one exam-taker. Unluckily, the
unﬁxed position before the exam inevitably aﬀects the order setting of the training tags, which is led by the Zoom gallery view
setting principle: newcomers are always given priority to be placed in the lower right corner. At the same time, other original
students behind the leaving person will move one by one to the left and above. As a result, we cannot manually mark the cell
position with one ﬁxed label and assume that one student at this location has not changed all the time.

Moreover, the bottom left corner of each cell labels everyone’s name, which will follow this cell when its location changes.
This observation inspires us, and we decide to exploit related optical character recognition (OCR) ways to attain the ground
truth for classiﬁcation. However, you can see that character is easier to distinguish each letter by human eyes but not computer
from the following grayscaled image Fig. 5 .

FIGURE 5 Detected by PyTesseract.

FIGURE 6 Thresholding Process Comparison.

Although the frame image only has one color channel, the surrounding background illumination is still close to the illumination
of character like the shown sample “DU”, which will inﬂuence text extraction. To improve contrast between the gray level

6

Xu Yang ET. AL.

for characters and surrounding pixels, thresholding is a simple and eﬀective preprocessing tool for telling objects from the
background 4.

According to the feature of OCR-engine, the black background and white text or opposite case can be easier to recognize the
original letters. Through ﬁne tuning threshold value, if background gray level is lower than the threshold, then the color value
will change to 0 and it will become the darkest in the sight. If background gray level is greater than the threshold, the color
value will keep. The following Fig. 6 is a comparison of before and after thresholding, which the noise around “DU” can be
removed. Additionally, the scale of characters will also inﬂuence the OCR results because too small cropped pictures are diﬃcult
to identify the inside text. We also zoom in the cropped image to make text bigger for diﬀerent sizes. After the above processing,
computers can recognize most names from every cropped image, which can be regarded as a subfolder name for each face.

After constructing the dataset, we are supposed to preprocess the dataset and discard some unqualiﬁed subfolders. In the
beginning, we directly delete the subfolder whose number of images is too small such as 100. The reason why these folders have
a small quantity of images is that PyTesseract cannot guarantee to extract the same letters for the same person name in diﬀerent
frames. Thus, we straightly regard these subfolders as noise and the small number of images cannot have a valuable impact on
the latter training. Sequentially, we try to compare the name of subfolders in the dataset and combine similar subfolders. The
comparison method is checking whether the student name in the roster is a substring of the OCR result or not. Next, we also help
the name not in roster folder to ﬁnd a similar string in the student list. After combination, we delete the subfolders whose name
is not in the roster. When the video demo is too short, there exists another problem that the number of ground truth will be too
small. At that time, iExam will delete the whole dataset and remind user to upload a longer video and perform the previous step.

Eﬃcient Face Training and Recognition

5.2
To capture facial features, iExam will rely on the oﬀ-the-shelf deep learning-based face detection (opencv dnn module), which is
a detector by comparing with the pretrained Caﬀe ﬁle. The reason for selecting DNN not the traditional haar-cascade classiﬁer is
the better face detection performance, such as face detection counts and the time cost. Here is the comparison of the mainstream
detection methods and the time cost for processing one ﬁve-minute video in Fig. 7 .

FIGURE 7 Choosing a face recognition algorithm.

With the development of convolutional neural networks, a large number of excellent networks emerged such as AlexNet,

ResNet, GoogleNet and so on.

The coherent nature of the video leads to the fact that the captured facial features are all quite similar, which means the neural
network cannot extract the diversity of each student’s face. We perform some degree of data augmentation like random ﬂip,

mtcnnhaardnndlib020000400006000080000100000120000127834116626129359101339detectionamountmtcnnhaardnndlib050010001500200025003000seconds31679601961683timecostMainstreamDetectionApproachComparisonXu Yang ET. AL.

7

rotation and crop transforms to prevent overﬁtting. We utilize cross-entropy loss function to compute the training loss and check
the training process is convergent or not.

FIGURE 8 Training Loss for diﬀerent Models

By comparison of loss decrease with training epoches in Fig. 8 , we can clearly see all networks will converge at 10 epoches
and there is no signiﬁcant improvement in loss decrease, which means we can stop training earlier to avoid spending too much
time.

Considering the eﬃciency of test, we will temporarily store the predicted results for each position in one second, which saves
a lot of time compared to making predictions for each frame. This solution is also served in OCR procedure because we do not
need to know all labels in each frame. And this suppose is in line with real scenarios. In order to prevent surrogate manner, we
also set a threshold for the probability distribution result, which utilizes softmax function to normalize the output. If the result is
upper than threshold, then we can treat this student as successful recognition. Confronted with the problem of face disappearing
or turning head to another side, we ﬁrstly detect whether iExam can capture faces correctly. Then the number of successful
recognition is more than 10 times in a default time period such as 30 seconds. Otherwise, the system will note down the speciﬁc
time period in the report.

Additionally, reasonably deploying multi-threads is also saving too much time for constructing dataset. If we continue to
capture face in the next frame when this frame is completely checked, then our computer will always process one image in the
same time, which is a serious waste of cpu usage.

6

EVALUATION

The Performance and Accuracy of Face Detection Algorithms in Real-time Exam Monitoring
6.1
In this section, we evaluate the performance and accuracy of iExam’s real-time component when leveraging two mainstream
face detection algorithms for online exam monitoring.

Experimental Setup. We collected real-world online exam data with a two-hour video, as illustrated earlier in Fig. 3 and 4
(see Section 4). To test this exam video, we setup two versions of iExam’s real-time iExam components — one uses traditional
haar-cascade face detector while the other implements the modern DNN face detector. Except using diﬀerent face detection
algorithms, there is no other diﬀerence in the rest of component code and running environment. These two versions of real-time
monitoring are available in https://vprlab.github.io/iexam/ (for the default DNN version) and https://vprlab.github.io/iexam/haar/
(for the haar-cascade version).

135791113151719epoches0.000.050.100.150.200.250.300.350.40loss valueTraining lossvariablealexNetgoogleNetresNet18resNet50resNet152squeezeNetmobileNetdenseNet1218

Xu Yang ET. AL.

Detection Performance. We measure the runtime performance of face detection according to the face detection times of the
same students. As shown in Fig. 9 , haar-cascade is able to detect around two times more faces under the same frequency of
video frames. This indicates that DNN has a slower detection rate than the haar-cascade method, which is because DNN requires
more time for the inputs to go through the neural network layers. Although DNN takes two times longer to process the image, it
can detect the location of the face with higher accuracy. This is because DNN is a learning method, not feature-based, and will
lead to more accuracy than haar-cascade method when someone covers the eyes with sunglasses or tilts the head to a side.

Detection Accuracy. We manually sampled from a two-hour video and intercepted 50 frames of view for simple random
sampling. By using the same face, we can ﬁnd that DNN gets around 90% accuracy in Fig. 10 , which is much higher than haar-
cascade method and is capable of adapting twisted head and certain angles of face rotation scenes. Besides, we also observe that
haar-cascade cannot detect a face when the face is too close to the webcam or generates wrong face location, which is the main
reason that we discard this quick detection approach.

FIGURE 9 Runtime performance of face detection.

FIGURE 10 Accuracy comparison of face detection.

The Performance and Accuracy of Face Recognition Models in Post-exam Video Analysis
6.2
In this section, we evaluate the performance and accuracy of iExam’s post-exam component when leveraging eight DNN face
recognition models for post-exam data analysis.

Experimental Setup. First, we need to distribute the training, validation and test dataset to evaluate and select the right model
for our task. We shuﬄe all the ﬁle index and choose the ﬁrst 70% as training sampler the next 20% as validation sampler and
the rest as test sampler. In order to avoid bias, we also try another sub-dataset of whole in a 90-minute exam video to test the
reality of our model. Here are the distribution of the training and validation dataset Fig. 11 . In terms of graphics card, we use
a GTX1650Ti with 4GB of memory to train our model.

Training Performance. Fig. 12 shows the time cost and the test accuracy of diﬀerent models. We can ﬁnd googleNet costs

the least time than any other networks, with only 244 minutes for 10 epoches.

Recognition Accuracy. On the other hand, resNet50 has the highest test accuracy at 98.41%. To balance the training eﬃciency

and model prediction result, we ﬁnally choose resNet50 as our task network.

A Case Study of iExam’s Post-exam Analysis Report

6.3
In Fig. 12 , we choose alexNet, googleNet, resNet50, resNet152, squeezeNet, mobileNet, denseNet121 to train the model and
compare the time cost with recognition accuracy. We can ﬁnd that googleNet has the fastest training time which is around four

dnnhaar-cascade010002000300040005000600070008000Amountsstu1 face detect timesstu2 face detect timesdnnhaar-cascade0.00.20.40.60.8Accuracy91.0%81.8%89.8%79.6%stu1 accuracystu2 accuracyXu Yang ET. AL.

9

FIGURE 11 The distribution of training and validation datasets.

FIGURE 12 Time Cost and Accuracy

hours, followed by the alexNet and resNet18. ResNet50, denseNet121 and mobileNet took one hour and a half or two hours longer
than previous three methods. The time cost of the rest two methods has increased signiﬁcantly. In terms of recognition accuracy,
we can obviously observe that resNet50 has a surprisingly high degree than others. Although the accuracy of resNet152 is also
above 90%, it also takes half of the time for training the model compared to resNet50. Except for the two methods mentioned, the
accuracy of the rest approaches are all around 70% to 80%. Because the results of our experiments rely heavily on the accuracy
of the identiﬁcation to determine which student is suspected of cheating at a given time. Therefore, we ﬁnally chose resNet50
as our training tool.

According to the generated report, we use bar charts to reﬂect the results of the number of students whose faces were

recognized and predicted for each student in Fig. 13 .

Because the whole test video picture can be divided into 5*5 cells, we use one success prediction label in one second (one
second is equal to 30 frames), which means we can speed up the entire testing process considerably and predict the same cell in
next second. In Fig. 14 , we can visually note in which speciﬁc time period the student did not appear to facilitate the teacher to

BailinHEBingHUBowenFANChenghaoLYUHanweiCHENLiZHANGLiujiaDUPakKwanCHANQijieCHENRouwenGERuiGUORunzeWANGRuochenXIESiqinLISiruiLITONGHiuYanTszKuiCHOWYanWUYimingZOUYuMingCHANYuanTIANYuchuanWANGZiwenLUZiyaoZHANGstu_name010002000300040005000image numberTrain SetBailinHEBingHUBowenFANChenghaoLYUHanweiCHENLiZHANGLiujiaDUPakKwanCHANQijieCHENRouwenGERuiGUORunzeWANGRuochenXIESiqinLISiruiLITONGHiuYanTszKuiCHOWYanWUYimingZOUYuMingCHANYuanTIANYuchuanWANGZiwenLUZiyaoZHANGstu_name0200400600800100012001400image numberVal SetBailinHEBingHUBowenFANChenghaoLYUHanweiCHENLiZHANGLiujiaDUPakKwanCHANQijieCHENRouwenGERuiGUORunzeWANGRuochenXIESiqinLISiruiLITONGHiuYanTszKuiCHOWYanWUYimingZOUYuMingCHANYuanTIANYuchuanWANGZiwenLUZiyaoZHANGalexNetgoogleNetresNet18resNet50resNet152squeezeNetmobileNetdenseNet1210100200300400500600Time(minutes)293244303405650555469445Times for all modelsalexNetgoogleNetresNet18resNet50resNet152squeezeNetmobileNetdenseNet1210.00.20.40.60.81.0Accuracy0.77260.83820.88280.98410.93020.69070.78330.8617Accuracy for all models10

Xu Yang ET. AL.

FIGURE 13 Recognition Frequency.

look back at the speciﬁc proctored video as a criterion for review. Furthermore, we also make a plot based on the time axis that
each person is absent consecutively from the feedback in Fig. 15 . The more consecutive absent time slot, the higher possibility
of suspicious behaviors someone commits.

FIGURE 14 Present Frequency.

FIGURE 15 Consecutive Disappear Summary.

7

RELATED WORK

In the early stage, holistic methods and feature-based methods have been fully applied in face image classiﬁcation. Eigenface
method exploits Principal Component Analysis (PCA) to train a face classiﬁer by ﬁnding the eigenvectors of given faces and
saving these weights, which can recognize a face via projecting to subspace and comparing these weights with the known
eigenface features. Another normal method, Fisherface, extracts face features independently not as a whole like Eigenface and
reduce the face space dimension by Linear Discriminant Analysis (LDA) which focuses more on maximizing the separability
among classes. The third traditional one, Local Binary Patterns Histograms (LBPH), processes partial pixels in a 3*3 cell,
encodes the newly binary sequence to decrease higher dimension and create a merged histogram for all regions. Although

02505007501000125015001750Recognition TimesBailinHEBingHUBowenFANChenghaoLYUHanweiCHENLiZHANGLiujiaDUPakKwanCHANQijieCHENQingboLIRouwenGERuiGUORunzeWANGRuochenXIESiqinLISiruiLITONGHiuYanTszKuiCHOWYanWUYimingZOUYuMingCHANYuanTIANYuchuanWANGZiwenLUZiyaoZHANGstudent nameconclusion of recognition timesBailinHEBingHUBowenFANChenghaoLYUHanweiCHENLiZHANGLiujiaDUPakKwanCHANQijieCHENQingboLIRouwenGERuiGUORunzeWANGRuochenXIESiqinLISiruiLITONGHiuYanTszKuiCHOWYanWUYimingZOUYuMingCHANYuanTIANYuchuanWANGZiwenLUZiyaoZHANG050100150200250300Time(seconds)recognition distributionBingHUBowenFANQijieCHENYuMingCHANSiqinLIRunzeWANGYanWULiZHANGHanweiCHENRouwenGEZiwenLUChenghaoLYUQingboLIZiyaoZHANGRuochenXIEYuanTIANBailinHETszKuiCHOWYimingZOUYuchuanWANGLiujiaDUPakKwanCHANRuiGUOSiruiLITONGHiuYanname050100150200250300Time(seconds)1s-300s13s-144s187s-300s55s-162s145s-210s25s-84s145s-204s193s-240s115s-144s67s-96s127s-156s145s-168s7s-30s217s-240s133s-144s211s-222s271s-276s133s-138s229s-234s67s-72s0s-0s0s-0s0s-0s0s-0s0s-0sConsecutive disappear time for testing videoXu Yang ET. AL.

11

Eigenface, Fisherface and LBPH can mostly obtain face features, they will also lessen the accuracy in illumination and store
plenty of invalid face information. Besides, it can not be denied that the former two methods will discard some potential face
features because PCA and LDA are not sensitive to the nonlinear correlation. Therefore, we turn our attention to deep learning
methods.

Haar-like Features. Viola and Jones 5 proposed an algorithm integrated by Haar-like features and Adaboost to rapidly ﬁnd the
faces. Haar-like features are composed of white rectangular regions and black rectangular regions in one rectangular. These black
and white regions represent the luminance diﬀerence of the image part. For example, some facial features can be simply described
by rectangular features such as eyes are darker than cheeks. Thus, a large number of diﬀerent black and white rectangular will
show diﬀerent description patterns in one picture 6. Additionally, it is particularly suitable for detection of objects with good
biological symmetry such as human faces, smiles, eyes and mouths because of its symmetric property.

Traditional Face Recognizer. OpenCV provides three methods for face recognition: Eigenface, Fisherface and Local Binary
Patterns Histograms (LBPH). These three methods all implement the recognition by comparing detected face with the training
set of the known face. Eigenface is based on Principal Component Analysis (PCA) 1 while Fisherface is based on Linear Dis-
criminant Analysis (LDA) 2,3. The core idea of PCA is to retain the main information as much as possible of the original data and
reduce the redundant information to facilitate subsequent recognition after projection. While the core idea of LDA is to maxi-
mize the diﬀerence between classes and minimize the diﬀerence within classes, which is to ensure that diﬀerent face images of
the same person can be gathered together after projection but face images of diﬀerent people are separated by a large distance.
Both Eigenface and Fisherface regard the dataset as a whole, but LBPH analyses each image independently and focuses on the
local pattern which will obtain better performance and save much more training time than the two before methods. But all three
methods cannot pay attention to non-linear relationship among diﬀerent classes and their labels. So we need to use a modern
way to learn this correlation, that is DNN.

Deep neural network (DNN). Convolutional neural network (CNN) 7 is a deep learning framework which has been suc-
cessfully proved to be a well-known neural network in various ﬁelds such as image classiﬁcation and recognition 8, speech
recognition, language translation, etc. The main reason for the success of CNN is that the CNN can “understand” the image
characteristics better than above traditional ways. In other words, CNN is more suitable to construct the connections of the data
structure because image can be obviously regarded as plentiful connected data representation. In its network, it always con-
sists of three types of layer: convolutional layer, pooling layer and fully connected layer. The convolutional and pooling layer
are used for facial extraction. By input image data (matrix), convolutional layer learns image features and preserves the spatial
relationship between adjacent pixels. It applies kernel to the input image (matrices). A speciﬁc translation stride will translate
each of the convolutional ﬁlters. After that layer, in most ConvNets, activation function and this layer is used to introduce a non-
linearity in the system. Pooling layer is responsible for progressively decrease the spatial size of the representation, which is
aim to reduce the amount of parameters and computation in the network. Finally, fully connected layer is feed forward artiﬁcial
neural network, which is learning a function in that space.

OCR and PyTesseract. Optical character recognition (OCR) algorithms will change graphics ﬁle into text ﬁle depending
on localize, detect or recognize characters on an image by computing the pixel position. It always contains six steps: image
acquisition, preprocessing, segmentation, feature extraction, training a neural network and post-processing. Hence, a good OCR
tool should meet the following conditions. It can use scanner such as webcam to obtain the image, delete the area without text,
optimize the image noise and make the raw data usable by computers. Additionally, it is able to divide the text into meaningful
segments to ease for people understanding. PyTesseract is an optical character recognition python library and indeed a powerful
tool provided by Google’s Tesseract-OCR Engine, which can easily extract the characters from a scanned document or a photo.

8

CONCLUSION

In this paper, we proposed iExam, an intelligent online exam monitoring and analysis system to assist invigilators in real-
time student identiﬁcation and to perform a face recognition-based post-exam video analysis. Our evaluation shows that iExam
achieved high accuracy and maintained acceptable runtime performance.

12

ACKNOWLEDGMENTS

Xu Yang ET. AL.

This research/project is partially supported by Courseware Development Grant (ref. no. 4170827) and Funding Scheme for
Virtual Teaching and Learning (ref. no. 4170890) from The Chinese University of Hong Kong.

References

1. Ramadhani Anissa Lintang, Musa Purnawarman, Wibowo Eri Prasetyo. Human face recognition application using pca and

eigenface approach. In: :1–5IEEE; 2017.

2. Chellappa Rama, Wilson Charles L, Sirohey Saad. Human and machine recognition of faces: A survey. Proceedings of the

IEEE. 1995;83(5):705–741.

3. Chan Lih-Heng, Salleh Sh-Hussain, Ting Chee-Ming, Ariﬀ AK. Face identiﬁcation and veriﬁcation using PCA and LDA.

In: ; 2008.

4. Sezgin Mehmet, Sankur Bülent. Survey over image thresholding techniques and quantitative performance evaluation. Journal

of Electronic imaging. 2004;13(1):146–165.

5. Viola Paul, Jones Michael. Rapid object detection using a boosted cascade of simple features. In: ; 2001.

6. Chen Jun-Horng, Tang I-Lin, Chang Chun-Hsuan. Enhancing the detection rate of inclined faces. In: :143-146; 2015.

7. LeCun Yann, Bottou Léon, Bengio Yoshua, Haﬀner Patrick. Gradient-based learning applied to document recognition.

Proceedings of the IEEE. 1998;86(11):2278–2324.

8. Krizhevsky Alex, Sutskever Ilya, Hinton Geoﬀrey E. Imagenet classiﬁcation with deep convolutional neural networks.

Advances in neural information processing systems. 2012;25:1097–1105.

