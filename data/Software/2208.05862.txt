2
2
0
2

g
u
A
1
1

]

C
D
.
s
c
[

1
v
2
6
8
5
0
.
8
0
2
2
:
v
i
X
r
a

Network Emulation in Large-Scale Virtual Edge
Testbeds: A Note of Caution and the Way Forward

Soeren Becker∗, Tobias Pfandzelter†, Nils Japke†, David Bermbach†, Odej Kao∗
∗Technische Universit¨at Berlin
Distributed and Operating Systems Research Group
{soeren.becker,odej.kao}@tu-berlin.de
†Technische Universit¨at Berlin & Einstein Center Digital Future
Mobile Cloud Computing Research Group
{tp,nj,db}@mcc.tu-berlin.de

Abstract—The growing research and industry interest in the
Internet of Things and the edge computing paradigm has
increased the need for cost-efﬁcient virtual testbeds for large-scale
distributed applications. Researchers, students, and practitioners
need to test and evaluate the interplay of hundreds or thousands
of real software components and services connected with a
realistic edge network without access to physical infrastructure.
While advances in virtualization technologies have enabled
parts of this, network emulation as a crucial part in the develop-
ment of edge testbeds is lagging behind: As we show in this paper,
NetEm, the current state-of-the-art network emulation tooling
included in the Linux kernel,
imposes prohibitive scalability
limits. We quantify these limits, investigate possible causes, and
present a way forward for network emulation in large-scale
virtual edge testbeds based on eBPFs.
Index Terms—edge computing,

things, virtual

internet of

testbeds, network emulation

I. INTRODUCTION

As edge computing and the Internet of Things are becoming
more important in both research and industry, the demand for
environments to test new algorithms, platforms, and software
systems is increasing [1], [2]. Between simulation, which can
efﬁciently analyze an abstracted subset of an edge or IoT
system [3]–[5], and physical infrastructure, which is accurate
yet expensive to implement and cumbersome to maintain [6],
[7], virtual testbeds running real software on virtualized infras-
tructure have emerged. These testbeds enable a wide audience
of researchers, students, and practitioners to evaluate software
systems on large-scale distributed infrastructures by combining
existing tooling for virtualization and emulation [8]–[10].

A key factor in building these testbeds for large-scale
infrastructure is the scalability of the underlying tools itself.
Yet, while process isolation and machine virtualization have
received signiﬁcant interest from industry with their broad
adoption in ﬁelds such as cloud computing [11], [12], the
network emulators required to reﬂect the communication char-
acteristics of edge and IoT are lagging behind.

The Linux trafﬁc control network emulator (NetEm) [13]
was originally introduced in kernel version 2.6.7 and has been

Partially funded by the Deutsche Forschungsgemeinschaft (DFG, German

Research Foundation) – 415899119 and 414984028.

continuously extended since then. Because of its wide avail-
ability, maturity, and high accuracy [10], [14]–[16], NetEm
is a popular choice for network emulation in edge and IoT
testbeds, e.g., [8], [9], [17]–[24]. We ﬁnd, however, that NetEm
has only limited applicability for large-scale testbeds as it
struggles to support networks of hundreds or even thousands
of nodes efﬁciently. In this paper, we show these limits to
enable future research into large-scale virtual edge and IoT
testbeds and outline a possible way forward.

The rest of this paper is structured as follows: We introduce
NetEm and related concepts as well as alternative approaches
in Section II. In a scalability analysis using the context of
virtual edge testbeds, we analyze the performance of NetEm
at scale, exposing bottlenecks in conﬁguration, latency over-
heads, and impact on network throughput (Section III). We
use novel Linux kernel features such as extended Berkeley
Packet Filters (eBPF) and the EDT model to propose an alter-
native approach to network emulation for large scale IoT and
edge testbeds and present promising preliminary performance
results (Section IV). Finally, we conclude in Section V.

II. BACKGROUND & RELATED WORK
The Linux trafﬁc control subsystem (tc) provides mecha-
nisms to control the transmission of packets in the Linux ker-
nel [13]. The main component of the trafﬁc control subsystem
are queuing disciplines (qdisc) that queue outgoing packets
for a network device. The default queuing discipline is a
FIFO queue, but other strategies are available to support more
ﬁne-grained control over trafﬁc shaping, e.g., by prioritizing
packets that match a certain description. This matching is
performed by ﬁlters used to classify packets into queuing
disciplines based on a classiﬁer.

The NetEm network emulator (tc-netem) is an extension
to the trafﬁc control subsystem introduced to aid in the devel-
opment of network protocols [25], succeeding NIST Net [26]
and Dummynet [27]. At its core, NetEm is itself a queuing
discipline, but provides further capabilities to emulate packet
delay, loss, duplication, and reordering. Several empirical eval-
uations have shown NetEm to be more accurate and to have
better performance than competing network emulators [10],
[14]–[16], yet have not considered scalability of the emulator.

 
 
 
 
 
 
Recent advances include extended Berkeley Packet Filters
(eBPF) [28] and eXpress Data Path (XDP) [29]. An evolution
of the original Berkeley Packet Filters (BPF), eBPF programs
run in a kernel virtual machine, are statically checked when
loaded into the kernel, and are limited in size and complexity.
Applications can extend the kernel at runtime safely by at-
taching an eBPF program to one of the exposed hooks. As the
lowest level of the Linux networking stack, XDP is one such
hook and supports processing packets by eBPF even before
memory is allocated by the operating system [30].

Based on XDP, Hemminger has presented XNetEm [31]
as an update to the original NetEm. XNetEm runs an eBPF
program as a kernel plugin that hooks into XDP processing
on a network device and emulates packet loss, corruption, and
marking. XDP offers high performance for packet processing
yet is limited in functionality and cannot be used to inject
emulated network delays, which are also required for packet
reordering. As an alternative, the author suggests using Linux
Trafﬁc Control hooks that could expose the required function-
ality, an approach we take in our system (Section IV).

Kumar et al. [32] describe Bandwidth Enforcer (BwE),
a solution to bandwidth allocation at Google. After ﬁnding
that existing approaches with policies enforced on network
the authors
routers cannot support TCP trafﬁc efﬁciently,
develop BwE to enforce service-speciﬁc limits on hosts. In
their work they leverage a global knowledge of the network
topology in order to offer a hierachical bandwidth allocation
for competing services across clusters of nodes connected in
a WAN environment.

Saeed et al. [33] show that this approach of host-based traf-
ﬁc shaping at scale consumes considerable CPU and memory
resources on the hosts and present Carousel as an alternative.
In Carousel, a timestamp for each packet is computed before
it is enqueued in a time-based queue and dequeued when the
timestamp is met. By deploying a single queue per CPU core
and enabling multicore lock-free coordination, Carousel is able
to signiﬁcantly increase the resource efﬁciency of host-based
trafﬁc shaping.

Although similar to our approach, BwE and Carousel aim to
improve the network performance and utilization in wide area
networks and data center communications whereas we target
network emulation.

III. SCALABILITY ANALYSIS
Our performance analysis of NetEm concerns the overhead
introduced by network emulation in the context of a virtual
edge testbed. In this section, we describe this scenario in more
detail (Section III-A), show bottlenecks encountered during
NetEm conﬁguration (Section III-B), and consider latency
(Section III-C) and bandwidth overheads (Section III-D). We
make our software artifacts available as open-source1.

A. Motivation and Scenario

Our experiments are motivated by our work on Celes-
tial [18], [19] which emulates low earth orbit (LEO) edge

1https://github.com/srnbckr/ebpf-network-emulation

Fig. 1. As the percentage of conﬁgured links grows during the NetEm conﬁg-
uration, the time per conﬁguration change increases linearly. At N = 1024,
conﬁguring one link takes up to 47.4ms.

infrastructure such as SpaceX’ Starlink constellation. In Ce-
lestial, individual satellites are emulated through a Firecracker
microVM [11] each and link characteristics are shaped through
NetEm. Due to the sheer number of satellites in a constellation
(which is usually in the 5,000 to 10,000 range according
to current plans), the scalability of NetEm is a signiﬁcant
factor. Therefore, we derive the scenario characteristics from
the Celestial use case and build on the Celestial prototype for
experiments, leading to the following scenario:

We assume a fully meshed topology of N processes, each
with their own network address. We assume all processes
running on a single host. With emulated connections between
these processes that let us make individual changes to the net-
work link characteristics, we require emulation of N ×(N −1)
network links. This assumes one link per direction, e.g., a link
attached to process A emulating network characteristics to a
process B and a further link for the opposite direction attached
to process B. While this does not reﬂect all network topologies
encountered in IoT or edge computing deployments, it serves
as a straightforward baseline example and allows us to show
scalability in relation to the factor N .

Per process, we create a tap network device. All ap-
plications used for performance measurements are run in
Firecracker microVMs attached to one of the network devices.
We conﬁgure a root hierarchical token bucket (HTB) queuing
discipline for each network device. For each outgoing link
from this network device, we add a subordinate HTB queuing
discipline using an ascending index. We then set the desired
NetEm emulation properties on this queuing discipline and
attach a ﬁlter that matches by destination IP address. Our
experiments are executed on a single Google Cloud Platform
n2-standard-32 machine with 32 vCPUs and 128 GB
memory running Ubuntu 20.04 LTS in the eu-west-3
region.

B. Conﬁguration Bottlenecks

Before examining the network performance impact of
NetEm at scale, we consider the overhead of actually con-

020406080100%Links01020304050TimeperLink[ms]N1282565121024Fig. 2. Experiment results show that network delay increases as more ﬁlters
and queuing disciplines are attached to the device. The latency overhead of
NetEm is only approximately 0.1µs per ﬁlter, yet this accumulates to 5.7ms
with more existing ﬁlters. Using a ﬁlter that matches our target address at
the 30,000th location reveals that ﬁlters are checked in sequence and a match
limits that overhead.

Fig. 3. Throughput tests show a 4.6Gbit/s baseline without any ﬁlters yet up
to 7.3Gbit/s using 5,000 ﬁlters. After this peak, throughput decreases steadily
to only 0.4Gbit/s at 65,000. Again, matching ﬁlter 30,000 with our test packets
limits the throughput overhead for experiments with additional ﬁlters.

ﬁguring link emulation. We observe considerable bottlenecks
that inhibit large-scale emulated testbeds.

We consider N processes and N × (N − 1) emulated links.
In our experiment, conﬁguring link emulation for the ﬁrst link
takes only 50µs. At the order of N = 1000, this overhead
would accumulate to 50s if links were conﬁgured sequentially.
Yet as Figure 1 shows, the overhead of conﬁguring one link
grows with the number of links that are already conﬁgured.
As a result, the time to create links with N = 1024 grows to
19,422.7ms, or three hours and 24 minutes.

We identify two possible causes for this behavior: First,
new queuing disciplines are attached to the root discipline in a
tree, which requires traversing existing handles when attaching
a new one. Second,
the kernel might check for duplicate
or otherwise conﬂicting queuing discipline entries, requiring
reading all existing entries.

In a further test, we try to parallelize the ﬁlter creation
by conﬁguring each network device with a separate process.
In theory, these links have independent queuing disciplines,
and we expect a signiﬁcant speed-up in link conﬁguration.
Unfortunately this did not change our results, possibly because
a global lock for Trafﬁc Control prevents concurrent changes
to the network subsystem.

We decided to not further investigate and remedy the causes
of the effects we observe in setting up large numbers of links
for NetEm as our performance measurements do not justify
using NetEm at this scale. Further, we note that our alternative
approach (Section IV) does not exhibit any such conﬁguration
bottlenecks.

C. Latency

We ﬁrst consider the network latency overhead caused
by NetEm. Using the setup described in Section III-A, we
run ping in one microVM and probe network delay to the
underlying host. We consider a growing number of links
conﬁgured for this node, yet set emulated network delay to

0ms and disable bandwidth throttling in HTB in order to have
results reﬂect only additional overhead caused by processing
in NetEm. We use a maximum of 65,000 links as we are
limited by the 216 address space of Trafﬁc Control child
handles. To account for the impact of a ﬁlter matching our
ICMP packets, processing it, and returning before additional
ﬁlters are checked, we test two scenarios: First, none of the
ﬁlters match the target IP address, requiring NetEm to check
all existing ﬁlters. Second, we add a ﬁlter matching the host’s
address at the 30,000th index to see whether this earlier entry
in the ﬁlter sequence leads to lower overheads when NetEm
does not check remaining ﬁlters.

We show the results of our delay measurement in Figure 2.
Since the connection between VM and host machine is vir-
tual, there should be minimal networking overhead during
performance measurements, and we measure an average of
0.3ms round trip time as a baseline. With increasing numbers
of ﬁlters attached to our network device, however, latency
increases linearly to up to 6ms when processing 65,000 ﬁlters.
As expected, a ﬁlter matching our test packet means that no
further ﬁlters are tested by NetEm. In our tests, increasing
the number of ﬁlters beyond 30,000 does not further increase
latency of around 3.3ms when a ﬁlter matches at this index.

D. Bandwidth

Our evaluation of the bandwidth overhead of NetEm follows
the same methodology as described in Section III-C, yet we
use iperf3 [34] to perform TCP bandwidth measurements.
Again we measure once without any ﬁlter matching our test
packets and once with the ﬁlter at index 30,000 aligning with
the host’s address.

The results of our tests with NetEm are shown in Figure 3.
As a baseline, we see a throughput of 4.6GBit/s without
the use of any ﬁlters. Surprisingly, we observe a small drop
in throughput to 3.9Gbit/s when attaching 2,000 ﬁlters yet
see it increase to 7.3Gbit/s with 5,000 ﬁlters. While more
throughput might be agreeable in general, unexpected effects

0100002000030000400005000060000#Filters123456MeanLatency[ms]MatchedIndexNoMatch30,0000100002000030000400005000060000#Filters246Throughput[Gbit/s]MatchedIndexNoMatch30,000of running NetEm at scale are undesirable in research testbeds.
In our case, we assume that mainly the interplay of nested
virtualization with Firecracker, tap devices and iperf3
induces this behaviour, since it was not reproducable on a bare-
metal node. After this peak, throughput decreases steadily to
only 0.4Gbit/s at 65,000 attached ﬁlters. As in our latency
measurements, we see how a ﬁlter matching our packets
impacts those results, with an increase in ﬁlters beyond 30,000
not further reducing throughput in these tests.

IV. SOLUTION

In order to tackle the aforementioned scalability issues,
we propose a network emulation approach that leverages a
Trafﬁc Control classifier and eBPF program to adjust
the departure timestamp of outgoing packets and subsequently
forwards them to a single fair queuing (FQ) queuing disci-
pline. A similar method was outlined in Carousel [33], yet
only implemented in Software NICs since necessary features
were not yet available in the kernel. With the recent switch of
the Linux networking stack to the early departure time (EDT)
model, each egress packet has a departure timestamp which in
turn can be adjusted with eBPF programs [35]. Consequently,
this enables the use of a timing wheel [36] packet scheduler
to the network interface based on
that releases a packet
its departure timestamp instead of, e.g., bfifo or pfifo
queues. In recent works [33], [35] this model was used for
rate limiting and pacing in, e.g., data centers. We also consider
it useful to efﬁciently emulate network characteristics in large
scale virtual edge testbeds. Therefore, we propose an approach
as depicted in Figure 4 using the following components:

• An eBPF map shared between the kernel and user space
that contains network link parameters such as bandwidth
limitations or latency to different destination IP addresses.
• A testbed agent running in user space which adjusts the
map with parameters used to emulate the link settings to
different nodes based on the destination IPs.

In addition, we attach an eBPF program to the locally
generated trafﬁc utilizing a Trafﬁc Control egress hook
which conducts the following steps for each outgoing packet:
Initially, a map lookup with the destination IP of the packet is
executed to check the shared eBPF map for potential network
emulation parameters. In case no parameters exist for the
destination, the packet is instantly forwarded to the network
device,
therefore only imposing a negligible overhead on
trafﬁc which should not be affected by any network emulation.
When parameters are found, the program computes a new
timestamp for the given packet.

Here we propose a two-fold approach: For a deﬁned
bandwidth limitation we apply an algorithm as outlined by
Fomichev et al. [35], that introduces rate-limiting by adjusting
the inter-packet gaps, while utilizing the last timestamp of a
packet obtained from a second eBPF map. As described by
Algorithm 1, if a previous timestamp exists, the inter-packet
gap is increased by a delay depending on the packet size
and given rate. Finally, the map is updated with the newly
computed timestamp for the next packet. We further extend

this method by storing the latest packet timestamps for several
IP addresses in order to enable varying network emulation
settings based on the destination.

Algorithm 1 Set the departure timestamp for egress packets

throttle(packet, rate)

1: Input: packet
2: ip ← destIP (packet)
3: rate, latency ← mapLookup(mapemulation, ip)
4: if rate then
5:
6: end if
7: if latency then
8:
9: end if
10: return packet
11: procedure THROTTLE(packet, rate)
12:

injectDelay(packet, latency)

tlast ← mapLookup(maptstamp, ip)
gap ← packetlen ∗ 109/rate
if tlast then

13:
14:
15:
16:
17:
18:

tnext ← tlast + gap

else

tnext ← now

end if
mapUpdate(maptstamp, ip, tnext)
updateTimestamp(packet, tnext)

19:
20:
21: end procedure
22: procedure INJECTDELAY(packet, rate)
23:
24:
25: end procedure

tnext ← packettstamp + latency
updateTimestamp(packet, tnext)

Moreover, in case an additional latency to the destination
was deﬁned in the eBPF map, the timestamp of the packet is
further increased by that delay. Finally, the packet is enqueued
in the timing wheel scheduler which subsequently releases the
packet to the network interface when the departure timestamp
is met.

Compared to NetEm, where in the worst case it is required
to create a filter as well as HTB and NetEm queuing
disciplines for each link that should be emulated, our ap-
proach relies on a single Trafﬁc Control classifier, FQ
queuing discipline, and shared eBPF map which signiﬁcantly
simpliﬁes and accelerates the setup and adjustment process:
In our experiments, ﬁlling the eBPF map with 65,000 IP and
network emulation parameter pairs takes around 170ms and
the values can be updated with a simple user space program.
This is especially important considering the scenario depicted
in Section III-A, which requires a vast amount of network
characteristics to be set for different links. Additionally, IP
pairs can be used as map keys, which allows to employ a
single eBPF map describing all possible links.

We implement our approach as a proof-of-concept prototype
that is able to limit bandwidth and inject latencies following
the method outline above. While we omit more sophisticated
network emulation settings such as delay distribution, packet

Fig. 4. Our proposed solution relies on a tc egress hook that forwards outgoing packets to an eBPF program which adjusts the departure time of packets.
The eBPF code obtains network emulation parameters used to compute the new timestamp from an eBPF map that is conﬁgured via a testbed agent and a
timing wheel scheduler is employed to release packets to the network interface when the departure time is met.

loss or corruption in this prototype, these could easily be
implemented by adapting methods from XNetEm [31].

Preliminary evaluation: We conduct a preliminary evalu-
ation of our prototype using the same experimental setup as de-
scribed in Section III-A. Here, we employ the aforementioned
NetEm method for up to 65,000 ﬁlters on a single network
interface and compare the overhead to our eBPF approach
and a map ﬁlled with the same amount of entries, repeating
the experiments from Section III-C and Section III-D.

Figure 5 shows the increase in network latency overhead for
ascending amounts of non-matching ﬁlter and map entries.
Whereas NetEm increases the latency to up to 6ms, our
prototype only poses a constant and negligible overhead of
around 0.1ms on the latency. This is mainly due to the fact
that packets can be checked in O(1) using our eBPF map
whereas NetEm checks each ﬁlter individually in O(N ). In
our prototype, non-matching packets are then directly released
to the network interface.

The same trend can be identiﬁed in Figure 6, which depicts
the bandwidth overhead for non-matching ﬁlters: In case of
our prototype, the bandwidth throughput is around 4.53GBit/s
and therefore close to the optimum, regardless of the amount
of map entries. As discussed in Section III-D, we assume
that nested virtualization with Firecracker microVMs results
in the increase of throughput at around 5,000 ﬁlters for NetEm
and additionally tested the two methods on a AMD EPYC
7282 bare-metal machine with 32 CPU cores. Although the
experiment was again conducted in two microVMs, the results
depicted in Figure 7 show a baseline throughput of around
is not exceeded anymore at 5,000 ﬁlters,
19.5GBit/s that
therefore supporting this assumption and motivating further
research in scenarios deviating from the use case described in
Section III-A, that we mainly evaluated in this paper.

Fig. 5. Overhead of increasing queuing disciplines and ﬁlters for NetEm
as well as map entries for the eBPF approach on the latency, without any
matches. For eBPF the overhead stays relatively constant.

Finally, we also compared the accuracy of the latency
injection and bandwidth limitation for both network emulation
methods: Figure 8 shows the latency for an artiﬁcial delay
of 20ms, monitored for one minute using ping and added
as a single ﬁlter respectively map entry. As a baseline we
monitored the latency without any emulation and increased
the values by 20ms. As can be seen, our prototype is able to
produce an emulation equivalent to NetEm.

Moreover, we also deployed two Firecracker microVMs,
each connected to a tap device as described in Section III-A,
limited the bandwidth between the nodes to 100Mbit/s and
added a latency of 5ms with a single ﬁlter and map entry.
We tested the available throughput between the nodes with
iperf3, running in the microVMs. As can be seen in
Figure 6, with both methods the throughput approaches the
theoretical maximum and the eBPF emulation even slightly

Packet(emulation) eBPF Mapdest IPRateDelayx.x.x.x1 MB/s10 msy.y.y.y5 MB/s30 msPacketPacket...User spaceKernelDynamically adjust  link settingsTestbed agenteBPF Code   Network InterfaceEgress traffictcegress hookGet link settingsApplicationsTiming WheelClsact qdiscRelease packetFQ qdisc(tstamp)eBPF map0100002000030000400005000060000#Filters/MapEntries123456MeanLatency[ms]MethodNetEmeBPFFig. 6. Overhead of increasing queuing disciplines and ﬁlters for NetEm
as well as map entries for the eBPF approach on the bandwidth, without
any matches. Non-matching entries have a negligible impact on the overall
bandwidth in our prototype and it is not affected by an unexpected peak like
NetEm.

Fig. 7. Bandwidth throughput overhead on a bare-metal machine for increas-
ing queuing disciplines and ﬁlters or map entries for the NetEm respectively
eBPF approach. Compared to nested virtualization, the baseline throughput is
not exceeded anymore in case of NetEm, although the drop at around 2,000
ﬁlters remains. The overhead of the eBPF approach is still negligible.

Fig. 8. Ping results for an emulated delay of 20ms using the NetEm
method and our approach. Baseline refers to the measured latency without
any emulation, increased by 20ms.

Fig. 9. iperf3 results for a bandwidth limitation of 100Mbit/s and emulated
delay of 5ms. We show the mean throughput in the last 10 seconds for a
duration of 10 minutes.

outperforms NetEm in terms of rate limit utilization in our
experiment.

V. CONCLUSION & FUTURE WORK

In this paper, we discussed the caveats of using NetEm for
building large-scale virtual testbeds for the edge and IoT. In a
scalability analysis, we showed that NetEm causes consider-
able performance overheads during setup and execution which
make it unsuitable for larger emulated networks.

Using novel Linux kernel features such as eBPFs and the
EDT model, we are able to remedy some of these scalability
issues without relying on complex dependencies. Our evalua-
tion of this approach shows negligible latency and bandwidth
overheads even with as many as 65,000 ﬁlters attached to a
network device thanks to packet matching in constant time. We
are already integrating this solution into a large-scale virtual
testbed and hope that it can beneﬁt the research community
in building the next generation of IoT and edge computing
testbed tools.

REFERENCES

[1] D. Bermbach, F. Pallas, D. Garc´ıa P´erez, P. Plebani, M. Anderson,
R. Kat, and S. Tai, “A research perspective on fog computing,” in
Proceedings of
the 2nd Workshop on IoT Systems Provisioning &
Management for Context-Aware Smart Cities (ISYCC 2017), Nov. 2017,
pp. 198–210.

[2] T. Pfandzelter, J. Hasenburg, and D. Bermbach, “From zero to fog: Efﬁ-
cient engineering of fog-based internet of things applications,” Software:
Practice and Experience, vol. 51, no. 8, pp. 1798–1821, 2021.

[3] X. Zeng, S. K. Garg, P. Strazdins, P. P. Jayaraman, D. Georgakopoulos,
and R. Ranjan, “Iotsim: A simulator for analysing iot applications,”
Journal of Systems Architecture, vol. 72, pp. 93–107, 2017.

[4] J. Hasenburg, S. Werner, and D. Bermbach, “Supporting the evaluation
of fog-based IoT applications during the design phase,” in Proceedings
of the 5th Workshop on Middleware and Applications for the Internet
of Things (M4IoT 2018), Dec. 2018, pp. 1–6.

[5] ——, “FogExplorer,” in Proceedings of the 19th ACM/IFIP International
Middleware Conference, Demos, and Posters (Middleware 2018), Dec.
2018, pp. 1–2.

[6] B. C. S¸ enel, M. Mouchet, J. Cappos, O. Fourmaux, T. Friedman, and
R. Mcgeer, “Edgenet: a multi-tenant and multi-provider edge cloud,”
in Proceedings of the 4th International Workshop on Edge Systems,
Analytics and Networking (EdgeSys 2021), Apr. 2021, pp. 49–54.

0100002000030000400005000060000#Filters/MapEntries246Throughput[Gbit/s]MethodNetEmeBPF0100002000030000400005000060000#Filters/MapEntries05101520Throughput[Gbit/s]MethodNetEmeBPF0102030405060Time[s]20.3020.3520.4020.4520.5020.5520.60Latency[ms]MethodBaselineNetEmeBPF0100200300400500600Time[s]96979899100Throughput[Mbit/s]MethodNetEmeBPF[7] S. Wang, Q. Li, M. Xu, X. Ma, A. Zhou, and Q. Sun, “Tiansuan
constellation: An open research platform,” in Proceedings of the 2021
IEEE International Conference on Edge Computing (EDGE 2021), Dec.
2021, pp. 94–101.

[8] J. Hasenburg, M. Grambow, E. Gr¨unewald, S. Huk, and D. Bermbach,
“MockFog: Emulating fog computing infrastructure in the cloud,” in
Proceedings of the First IEEE International Conference on Fog Com-
puting 2019 (ICFC 2019), Jun. 2019, pp. 144–152.

[9] J. Hasenburg, M. Grambow, and D. Bermbach, “MockFog 2.0: Auto-
mated execution of fog application experiments in the cloud,” IEEE
Transactions on Cloud Computing, pp. 1–1, 2021.

[10] Y. Li, R. Bartos, and C. Liang, “Are containers coupled with netem a re-
liable tool for performance study of network protocols?” in Proceedings
of IEEE SoutheastCon 2019, Apr. 2019, pp. 1–7.

[11] A. Agache, M. Brooker, A. Iordache, A. Liguori, R. Neugebauer,
P. Piwonka, and D.-M. Popa, “Firecracker: Lightweight virtualization for
serverless applications,” in Proceedings of the 17th USENIX symposium
on networked systems design and implementation (NSDI ’20), 2020, pp.
419–434.

[12] D. Ernst, D. Bermbach, and S. Tai, “Understanding the container
ecosystem: A taxonomy of building blocks for container lifecycle and
cluster management,” in Proceedings of the 2nd International Workshop
on Container Technologies and Container Clouds (WoC 2016), 2016.

[13] M. A. Brown, “Trafﬁc control howto,” 2006.
[14] A. Jurgelionis, J.-P. Laulajainen, M. Hirvonen, and A. I. Wang, “An
empirical study of netem network emulation functionalities,” in Pro-
ceedings of 20th international conference on computer communications
and networks (ICCCN 2011), Aug. 2011, pp. 1–6.

[15] L. Nussbaum and O. Richard, “A comparative study of network link
emulators,” in Proceedings of the 12th Communications and Networking
Simulation Symposium (CNS 2009), Mar. 2009.

[16] R. L¨ubke, P. B¨uschel, D. Schuster, and A. Schill, “Measuring accuracy
and performance of network emulators,” in Proceedings of the 2014
IEEE International Black Sea Conference on Communications and
Networking (BlackSeaCom), May 2014, pp. 63–65.

[17] S. Becker, F. Schmidt, and O. Kao, “Edgepier: P2p-based container im-
age distribution in edge computing environments,” in Proceedings of the
40th IEEE International Performance, Computing, and Communications
Conference (IPCCC 2021), Oct. 2021, pp. 1–8.

[18] T. Pfandzelter and D. Bermbach, “Celestial: Virtual software system
the 23rd ACM/IFIP

testbeds for the leo edge,” in Proceedings of
International Middleware Conference (Middleware 2022), Nov. 2022.

[19] ——, “CELESTIAL: Virtual software system testbeds for the leo edge,”
TU Berlin & ECDF, Mobile Cloud Computing Research Group, Tech.
Rep., Apr. 2022.

[20] S. Herrnleben, R. Ailabouni, J. Grohmann, T. Prantl, C. Krupitzer, and
S. Kounev, “An iot network emulator for analyzing the inﬂuence of
varying network quality,” in Proceedings of the 12th EAI International
Conference on Simulation Tools and Techniques (SIMUtools 2020), Aug.
2020, pp. 580–599.

[21] A. G. Schmidt, V. Venugopalan, M. Paolieri, and M. French, “Constella-
tions in the cloud: Virtualizing remote sensing systems,” in Proceedings
of the 2019 IEEE International Geoscience and Remote Sensing Sym-
posium (IGARSS 2019), Aug. 2019, pp. 5351–5354.

[22] H. Ahmed, S. Pierre, and A. Quintero, “A ﬂexible testbed architecture
for vanet,” Vehicular Communications, vol. 9, pp. 115–126, 2017.
[23] Q. Xu and J. Zhang, “pifogbed: A fog computing testbed based on rasp-
berry pi,” in Proceedings of the 38th IEEE International Performance
Computing and Communications Conference (IPCCC 2019), 2019, pp.
1–8.

[24] I. Behnke, L. Thamsen, and O. Kao, “H´ector: A framework for testing
iot applications across heterogeneous edge and cloud testbeds,” in
Proceedings of the 12th IEEE/ACM International Conference on Utility
and Cloud Computing Companion, Dec. 2019, pp. 15–20.

[25] S. Hemminger et al., “Network emulation with netem,” no. Apr., 2005.
[26] M. Carson and D. Santay, “Nist net: A linux-based network emulation
tool,” SIGCOMM Computer Communication Review, vol. 33, no. 3, pp.
111–126, 2003.

[27] L. Rizzo, “Dummynet: a simple approach to the evaluation of network
protocols,” ACM SIGCOMM Computer Communication Review, vol. 27,
no. 1, pp. 31–41, 1997.

[28] J. Schulist, D. Borkmann, and A. Starovoitov, “Linux socket ﬁltering aka
berkeley packet ﬁlter (bpf),” https://www.kernel.org/doc/Documentation/
networking/ﬁlter.txt, 2019, accessed: 2022-7-7.

[29] T. Høiland-Jørgensen, J. D. Brouer, D. Borkmann, J. Fastabend, T. Her-
bert, D. Ahern, and D. Miller, “The express data path: Fast pro-
grammable packet processing in the operating system kernel,” in Pro-
ceedings of the 14th International Conference on Emerging Networking
EXperiments and Technologies (CoNEXT ’18), Dec. 2018, pp. 54–66.

[30] M. A. Vieira, M. S. Castanho, R. D. Pac´ıﬁco, E. R. Santos, E. P. C.
J´unior, and L. F. Vieira, “Fast packet processing with ebpf and xdp:
Concepts, code, challenges, and applications,” ACM Computing Surveys
(CSUR), vol. 53, no. 1, pp. 1–36, 2020.

[31] S. Hemminger, “Xnetem network emulation using xdp,” in Proceedings
of the 2017 Technical Conference on Linux Networking (NetDev 2.2),
Nov. 2017.

[32] A. Kumar, S. Jain, U. Naik, A. Raghuraman, N. Kasinadhuni, E. C.
Zermeno, C. S. Gunn, J. Ai, B. Carlin, M. Amarandei-Stavila, M. Robin,
A. Siganporia, S. Stuart, and A. Vahdat, “Bwe: Flexible, hierarchical
bandwidth allocation for wan distributed computing,” in Proceedings
of
the 2015 ACM Conference on Special Interest Group on Data
Communication (SIGCOMM ’15), Aug. 2015, pp. 1–14.

[33] A. Saeed, N. Dukkipati, V. Valancius, V. The Lam, C. Contavalli,
and A. Vahdat, “Carousel: Scalable trafﬁc shaping at end hosts,” in
Proceedings of the Conference of the ACM Special Interest Group on
Data Communication (SIGCOMM ’17), Aug. 2017, pp. 404–417.
[34] J. Dugan, S. Elliott, B. A. Mah, J. Poskanzer, and K. Prabhu, “iperf3,”

https://iperf.fr/, 2022, accessed: 2022-7-13.

[35] S. Fomichev, E. Dumazet, W. de Bruijn, V. Dumitrescu, and B. Sommer-
feld, “Replacing htb with edt and bpf,” https://legacy.netdevconf.info/
0x14/pub/papers/55/0x14-paper55-talk-paper.pdf, 2018, accessed: 2022-
7-11.

[36] G. Varghese and T. Lauck, “Hashed and hierarchical timing wheels:
data structures for the efﬁcient implementation of a timer facility,”
in Proceedings of the 11th ACM Symposium on Operating Systems
Principles (SOSP ’87), Nov. 1987, pp. 25–38.

