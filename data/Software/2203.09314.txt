2
2
0
2

r
a

M
7
1

]
S
M

.
s
c
[

1
v
4
1
3
9
0
.
3
0
2
2
:
v
i
X
r
a

The Sparse Grids Matlab kit - a Matlab implementation of sparse
grids for high-dimensional function approximation and uncertainty
quantiﬁcation

Chiara Piazzola1 and Lorenzo Tamellini2

1,2Istituto di Matematica Applicata e Tecnologie Informatiche “E. Magenes”, Consiglio
Nazionale delle Ricerche, Via Ferrata, 5/A 27100, Pavia, Italy
chiara.piazzola@imati.cnr.it,tamellini@imati.cnr.it

Abstract

The Sparse Grids Matlab Kit is a collection of Matlab functions for high-dimensional interpolation and
quadrature, based on the combination technique form of sparse grids. It is lightweight, high-level and easy
to use, good for quick prototyping and teaching. It is somehow geared towards Uncertainty Quantiﬁcation
(UQ), but it is ﬂexible enough for other purposes. The goal of this paper is to give an overview of the
implementation structure of the Sparse Grids Matlab Kit and to showcase its potentialities, guided by some
illustrative tests and a ﬁnal comprehensive example on forward and inverse UQ analysis.

1 Introduction

The aim of this manuscript is to showcase the Sparse Grids Matlab Kit as a tool for approximation of high-
dimensional functions and Uncertainty Quantiﬁcation (UQ). The Sparse Grids Matlab Kit is freely available
at https://sites.google.com/view/sparse-grids-kit under the BSD2 license: the ﬁrst version was
released in 2014 (14-4 “Ritchie”), and the current version was released in 2018 (18-10 “Esperanza”); this
manuscript is being written in occasion of the new release (22-2 “California”). It is written in Matlab, and
its compatibility with Octave has been tested.
From a mathematical point of view, the package implements the combination technique form of sparse grids:
it is a high-level package, with syntax quite close to the mathematical description of sparse grids, which
makes it (hopefully) easy to use, and therefore suitable for quick prototyping and didactic purposes; during
the discussion, we will however point out a couple of functionalities (interface with the Matlab Parallel
Toolbox and the so-called grid recycling and function recycling) that make the Sparse Grids Matlab Kit
reasonably usable for moderate-sized problems (say up to N ≈ 20 dimensions), although it has been used
also for problems with hundreds of random variables, such as in [28] and [9]; we also mention the book [23],
that uses the Sparse Grids Matlab Kit for UQ purposes.
The Sparse Grids Matlab Kit is somehow geared towards UQ, although it is general enough to be used for
manipulation of high-dimensional functions in other frameworks. As such, it belongs to the same niche of
a number of other UQ software; we provide a (knowingly incomplete) list in Table 1. The software in the
Table closest to the Sparse Grids Matlab Kit (in terms of language, functionalities and usability) is probably
Spinterp, which is however no longer maintained and does not implement any UQ function.
The rest of this paper is organized as follows. Section 2 introduces the minimal mathematical background
necessary to understand the entities implemented in the Sparse Grids Matlab Kit. Then, Section 3 covers

1

 
 
 
 
 
 
Name
Dakota
PyApprox
MUQ
UQLab
ChaosPy
SG++
Spinterp
UQTk
Tasmanian C++, Python, MATLAB, Fortran 90/95

Language
C++
Python
C++, Python
Matlab
Python
Python, MATLAB, Java, C++
Matlab
C++, Python

Reference Webpage
[1]
–
[29]
[22]
[12, 11]
[30]
[19, 20]
[8, 7]
[36, 38, 37]

https://dakota.sandia.gov
https://pypi.org/project/pyapprox
https://mituq.bitbucket.io
https://uqlab.com
https://chaospy.readthedocs.io
https://sgpp.sparsegrids.org/
http://calgo.acm.org/847.zip1
https://sandia.gov/uqtoolkit
https://tasmanian.ornl.gov

Table 1: Comparison of high-dimensional approximation / UQ-related software.

how sparse grids are generated in the Sparse Grids Matlab Kit, how they are stored in memory (data struc-
ture) and what options are available for their generation. Note in particular that the Sparse Grids Matlab
Kit provides two mechanisms to generate sparse grids: a-priori and adaptive a-posteriori. Section 4 discusses
the main functionalities made available by the Sparse Grids Matlab Kit, from the “basic ones” (quadrature,
interpolation, evaluation, plotting) to more “advanced manipulation tools” (computing derivatives, conver-
sion to polynomial chaos expansions). Finally, Section 5 illustrates the usage of the Sparse Grids Matlab Kit
over a simple (yet quite paradigmatic) problem of forward and inverse UQ for partial diﬀerential equations
(PDEs). This ﬁnal example should hopefully convey that it is quite straightforward to use the Sparse Grids
Matlab Kit to perform a wide array of possible common tasks in UQ, with a relatively small number of lines
of code. All the code snippets discussed in the paper, as well as the complete code for the ﬁnal PDE example
are available on the Sparse Grids Matlab Kit webpage.
Note that this manuscript is not intended as a documentation of the Sparse Grids Matlab Kit, but rather
just a “showcase” about the underlying structure of the software, its features and an example of application.
As such, some functionalities are not discussed, and not all options of the main commands are reported here.
To this end, we refer the interested reader to the extensive documentation of the package, including the help
of the functions, and the multiple thorough tutorials provided.

2 A short overview on the basics of sparse grids

The aim of this section is to lay the mathematical foundations of sparse grids, which will guide the software
implementation. We consider the two problems of a) approximating and b) computing integrals of a function
f : RN → RV given some samples of f whose location we are free to choose. More speciﬁcally, we assume
that f depends on N random variables y = (y1, . . . , yN ) ∈ Γ, with Γ = Γ1 × . . . × ΓN ⊂ RN being the
set of all possible values of y. We denote by ρn : Γn → R+ the probability density function (pdf) of
each variable yn, n = 1, . . . , N and assume independence of y1, . . . , yN , such that the joint pdf of y is
ρ(y) = (cid:81)N
The ﬁrst building block of sparse grids is a set of collocation knots for each variable yn. We denote the
number of knots to be used in each direction by Kn ∈ N+, and introduce a discretization level in ∈ N+ and
a so-called “level-to-knots function”

n=1 ρn(yn), ∀y ∈ Γ.

m : N+ → N+ such that m(in) = Kn.

Then, we denote by Tn,in the set of Kn discretization knots along yn, i.e.,

Tn,in =

(cid:110)

y(jn)
n,m(in) : jn = 1, . . . , m(in)

(cid:111)

for n = 1, . . . , N.

(1)

(2)

1Last oﬃcially released version, to the best of knowledge of the authors of this manuscript. A later version can be found at

https://people.sc.fsu.edu/~jburkardt/m_src/spinterp/spinterp.html

2

Such sequence is usually chosen according to the probability distribution of the random variables ρn for
eﬃciency reasons. Furthermore, sequences of nested knots, i.e. Tn,in ⊂ Tn,jn with jn ≥ in, are beneﬁcial.
These aspects will be further discussed in Sect. 3.1.
The second building block of sparse grids are N -dimensional tensor grids, that are obtained by taking
the Cartesian product of the N univariate sets of knots just introduced. For this purpose we collect the
discretization levels in in a multi-index i = [i1, . . . , iN ] ∈ NN
+ and denote the corresponding tensor grid by
Ti = (cid:78)N
n=1 Tn,in, and its number of knots by Mi = (cid:81)N
n=1 m(in). Using standard multi-index notation, we
can then write

Ti =

(cid:110)

y(j)
m(i)

(cid:111)

j≤m(i)

,

with

(cid:104)

y(j)
m(i) =

1,m(i1), . . . , y(jN )
y(j1)

N,m(iN )

(cid:105)

and j ∈ NN
+ ,

where m(i) = [m(i1), m(i2), . . . , m(iN )] and j ≤ m(i) means that jn ≤ m(in) for every n = 1, . . . , N .
A tensor grid approximation of f (y) (cf. task a) above) is based on global Lagrangian interpolants collocated
at these grid knots and can be written in the following form

Ui(y) :=

(cid:88)

(cid:16)

f

y(j)
m(i)

(cid:17)

L(j)

m(i)(y),

j≤m(i)

(3)

(cid:110)

(cid:111)

L(j)

where

m(i)(y)
Lagrange polynomials, i.e.

j≤m(i)

are N -variate Lagrange basis polynomials, deﬁned as tensor products of univariate

L(j)

m(i)(y) =

N
(cid:89)

n=1

l(jn)
n,m(in)(yn) with

l(jn)
n,m(in)(yn) =

m(in)
(cid:89)

k=1,k(cid:54)=jn

yn − y(k)
n,m(in) − y(jn)
y(k)

n,m(in)

n,m(in)

.

Similarly, the tensor grid quadrature of f (y), i.e. the approximation of its integral (cf. task b) above), can
be computed by taking the integral of the Lagrangian interpolant in Eq. (3):

Qi :=

(cid:90)

Γ

Ui(y)ρ(y) dy =

(cid:88)

(cid:17)

(cid:16)
y(j)
m(i)

f

j≤m(i)

(cid:32) N
(cid:89)

(cid:90)

n=1

Γn

l(jn)
n,m(in)(yn)ρ(yn) dyn

(cid:33)

(cid:88)

=

(cid:16)

f

y(j)
m(i)

j≤m(i)

(cid:32) N
(cid:89)

(cid:17)

i=1

(cid:33)

ω(jn)
n,m(in)

=

(cid:88)

(cid:16)

f

y(j)
m(i)

(cid:17)

ω(j)
m(i),

j≤m(i)

(4)

m(i) are their multivariate counterparts.

n,m(in) are the standard quadrature weights obtained by computing the integrals of the associated

where ω(jn)
univariate Lagrange polynomials, and ω(j)
Naturally, the approximations Ui and Qi are more and more accurate the higher the number of collocation
knots in each direction, and therefore one would ideally choose all the components of i to be large, say
i = i(cid:63) with i(cid:63)
n (cid:29) 1, ∀n = 1, . . . , N . The cost of these approximations could be however too large even for
N moderately large, due to fact that they would require (cid:81)N
n) evaluations of f , i.e., their cost grows
exponentially fast in N .
To circumvent this issue, the sparse grid method replaces Ui(cid:63) with a linear combination of multiple coarser
Ui, and similarly for Qi(cid:63) (from now on we use the generic symbol Fi to denote both Ui and Qi). To this
aim we introduce the so-called “detail operators” (univariate and multivariate). They are deﬁned as follows,
with the understanding that Fi(y) = 0 when at least one component of i is zero. Thus, we denote by en the
n-th canonical multi-index, i.e. (en)k = 1 if n = k and 0 otherwise, and deﬁne

n=1 m(i(cid:63)

Univariate detail: ∆n[Fi] = Fi − Fi−ei with 1 ≤ n ≤ N ;

Multivariate detail: ∆[Fi] =

N
(cid:79)

n=1

∆n[Fi]

(5)

3

where taking tensor products of univariate details amounts to composing their actions, i.e.

∆[Fi] =

N
(cid:79)

n=1

∆n[Fi] = ∆1 [ · · · [∆N [Fi] ] ] .

By replacing the univariate details with their deﬁnitions, we can then see that this implies that the mul-
tivariate operators can be evaluated by evaluating certain full-tensor approximations Fi, and then taking
linear combinations:

∆[Fi] = ∆1 [ · · · [∆N [Fi] ] ] =

(cid:88)

(−1)(cid:107)j(cid:107)1 Fi−j.

j∈{0,1}N

Observe that by introducing these detail operators a hierarchical decomposition of Fi can be obtained;
indeed, the following telescopic identity holds true:

Fi =

(cid:88)

j≤i

∆[Fj].

(6)

As an example, consider the case N = 2. Recalling that by deﬁnition F[j1,j2] = 0 when either j1 = 0 or
j2 = 0, it can be seen that

(cid:88)

[j1,j2]≤[2,2]

∆[F[j1,j2]] =∆[F[1,1]] + ∆[F[1,2]] + ∆[F[2,1]] + ∆[F[2,2]]

=F[1,1] + (F[1,2] − F[1,1]) + (F[2,1] − F[1,1]) + (F[2,2] − F[2,1] − F[1,2] + F[1,1])
=F[2,2].

The crucial observation that allows to get to sparse grids is that, under suitable regularity assumptions for
f (y), not all of the details in the hierarchical decomposition in Eq. (6) contribute equally to the approxima-
tion, i.e., some of them can be discarded and the resulting formula will retain good approximation properties
at a fraction of the computational cost: roughly, the multi-indices to be discarded are those corresponding
to “high-order” details, i.e., those for which (cid:107)j(cid:107)1 is suﬃciently large, see e.g. [5]. For instance, following the
example above and considering (cid:107)j(cid:107)1 ≤ 3 we obtain

F[2,2] ≈

(cid:88)

j1+j2≤3

∆[F[j1,j2]] = ∆[F[1,1]] + ∆[F[1,2]] + ∆[F[2,1]] = −F[1,1] + F[1,2] + F[2,1].

In general, upon collecting the multi-indices to be retained in the sum in a multi-index set I ⊂ NN
sparse grids approximation of f and of its integral can ﬁnally be written as (see e.g. [40]):

+ the

f (y) ≈ UI(y) =

(cid:88)

i∈I

∆[Ui(y)] =

(cid:88)

i∈I

ci Ui ,

ci :=

(cid:88)

(−1)(cid:107)j(cid:107)1

j∈{0,1}N
i+j∈I

(cid:90)

Γ

f (y)ρ(y) dy ≈ QI(y) =

(cid:88)

i∈I

∆[Qi(y)] =

(cid:88)

i∈I

ciQi,

and the sparse grid is deﬁned as

TI =

(cid:91)

Ti.

i∈I
ci(cid:54)=0

(7)

(8)

(9)

The right-most equalities in Eqs. (7) and (8) are known are “combination technique” form of the sparse grids
approximation and quadrature, and are valid only if I is chosen as downward closed, i.e.

∀k ∈ I, k − en ∈ I for every n = 1, . . . , N such that kn > 1.

(10)

We illustrate this property in Fig. 1. An example for a better understanding of the combination technique
formula is given in Example 1.

4

Figure 1: Downward closedness of a multi-index set. The set of the multi-indices marked in blue is downward
closed. Instead, the multi-index [3, 2] (in red) violates the rule in Eq. (10): the multi-index [2, 2] = [3, 2] − e1
is not contained in the multi-index set and hence the set of blue and red multi-indices is not downward
closed.

(a) T[1,1]

(b) T[1,2]

(c) T[3,1]

(d) TI

Figure 2: Tensor grids (ﬁrst three plots) and the sparse grid (fourth plot) of Gauss–Legendre knots on
Γ = [0, 1]2 corresponding to the blue multi-index set of Fig. 1.

Example 1 (Combination technique formula) Let us consider the downward closed multi-index set re-
ported in Fig. 1, i.e. I = {[1, 1], [1, 2], [2, 1], [3, 1]}, and exemplify the combination technique form of the
sparse grid approximation and quadrature in Eqs. (7) and (8), respectively. We use again the generic symbol
Fi to denote both Ui and Qi and obtain

FI(y) = c[1,1]F[1,1](y) + c[1,2]F[1,2](y) + c[2,1]F[2,1](y) + c[3,1]F[3,1](y)

with

c[1,1] = (−1)(cid:107)[0,0](cid:107)1 + (−1)(cid:107)[1,0](cid:107)1 + (−1)(cid:107)[0,1](cid:107)1 = −1,
c[1,2] = (−1)(cid:107)[0,0](cid:107)1 = +1,
c[2,1] = (−1)(cid:107)[0,0](cid:107)1 + (−1)(cid:107)[1,0](cid:107)1 = 0,
c[3,1] = (−1)(cid:107)[0,0](cid:107)1 = +1.

Since c[2,1] = 0, only three Lagrangian interpolant/quadrature operators explicitly appear in the combination
technique formulas (7) and (8), and only the corresponding three tensor grids contribute to the sparse grid
(cf. Eq. (9)), i.e.

FI(y) = −F[1,1](y) + F[1,2](y) + F[3,1](y).

We refer to Fig. 2 for a visualization of such tensor grids and of the corresponding sparse grid, assuming
that the univariate collocation knots Tn,in in Eq. (2) are Gauss–Legendre knots, and that the level-to-knots
function in Eq. (1) is m(i) = i (more details on these choices follow below).

5

3 The Sparse Grids Matlab Kit

Equations (7), (8), (9) are the formulas that constitute the backbone of the implementation of the Sparse
Grids Matlab Kit. The choice of implementing the combination technique instead of the multivariate detail
operators (5) allows to keep the data structure to a minimum, and guarantees ease of use and high-level,
“close-to-the-math” coding. We illustrate the Sparse Grids Matlab Kit in two steps:
in this section we
discuss the creation of the sparse grid data structure, whereas in the next section we describe the main
operations that can be done (interpolation, quadrature, etc).

The basic creation of a sparse grid is as easy as the following code snippet (Listing 1), which generates the
sparse grid discussed in Example 1. We report it and then spend the rest of the section dissecting these lines
and discussing the alternative options available for the same operations.

% Gauss-Legendre knots on [0,1]

1 knots = @(n) knots uniform(n,0,1);
2 lev2knots = @lev2knots lin;
3 I = [1 1;
1 2;
2 1;
3 1];

4

5

6
7 S = smolyak grid multiidx set(I,knots,lev2knots);
8 Sr = reduce sparse grids(S);
9 plot sparse grid(S,[],'ok','LineWidth',3,'MarkerFaceColor','k','MarkerSize',12)

Listing 1: Basic creation of a sparse grid.

The ﬁrst six lines declare the “ingredients” of the sparse grid, while the actual construction of the grid is
delegated to lines 7 and 8. The command plot sparse grids at line 9 generates the plot of the sparse grid
reported in Fig. 2d. Most of the lines have a one-to-one correspondence with the mathematical description
of sparse grids presented in the previous section:

• knots are the univariate set of collocation knots Tn,in in Eq. (2);
• lev2knots is the level-to-knots function m in Eq. (1);

• I is the multi-index set I in Eqs. (7), (8), (9);

• S contains the sparse grid understood as collection of tensor grids Ti, i ∈ I, ci (cid:54)= 0;

• Sr contains the sparse grid understood as the “uniqued” union of tensor grids, i.e., without repetitions

of knots in common (see Eq. (9)).

Next, we present and discuss the alternatives available in the Sparse Grids Matlab Kit for each of the
operations in the listing above: assignment of knots is discussed in Sect. 3.1, assignment of lev2knots is
discussed in Sect. 3.2, construction of the multi-index set I is discussed in Sect. 3.3, and ﬁnally construction
of S and Sr, and the details of the respective data structure are discussed in Sect. 3.4.

3.1 Knots

We begin by addressing the diﬀerent types of collocation knots that are implemented in the Sparse Grids
Matlab Kit, i.e., the sets Tn,in in Eq. (2), which are the possible options to replace the assignment in line 1 of
Listing 1, knots = @(n) knots uniform(n,0,1). As already hinted, the knots should be chosen accord-
ingly to the pdf of the corresponding random variables yn; in Table 2 we report the random variables that
are considered in the Sparse Grids Matlab Kit, along with the expression of the respective pdf, emphasizing
the parameters on which the pdfs depend.2

2It is possible to choose diﬀerent types of collocation knots to each random variable yn, see package documentation.

6

gamma

beta

uniform

normal

uniform

normal

domain
Γ = [a, b]

parameters
a, b ∈ R

Γ = R

µ, σ ∈ R

exponential Γ = [0, ∞)

λ ∈ R, λ > 0
Γ = [0, ∞) α, β ∈ R, α > −1, β > 0
a, b, α, β ∈ R, α, β > −1

Γ = [a, b]

e− (y−µ)2

2σ2

pdf
ρ(y) = 1
b−a
ρ(y) = 1
√

σ

2π
ρ(y) = λe−λy
ρ(y) = βα+1

ρ(y) =

Γ(α+1) xαe−βy
Γ(α+1)Γ(β+1)(b−a)α+β+1 (y − a)α(b − y)β

Γ(α+β+2)

Table 2: Random variables and corresponding pdfs.

knots
Gauss–Legendre
Clenshaw–Curtis
Leja (standard, symmetric, P-disk)
Equispaced
Midpoint
Gauss–Hermite
Genz–Keister
weighted Leja (standard, symmetric)

exponential Gauss–Laguerre

gamma

beta

weighted Leja
Gauss–generalized Laguerre
weighted Leja
Gauss–Jacobi
weighted Leja (standard, symmetric)

nestedness
function
No
knots uniform
Yes
knots CC
Yes
knots leja
Yes
knots trap
Yes
knots midpoint
No
knots normal
Yes
knots GK
Yes
knots normal leja
No
knots exponential
knots exponential leja Yes
No
knots gamma
Yes
knots gamma leja
No
knots beta
Yes
knots beta leja

Table 3: Diﬀerent type of collocation knots, corresponding function of the Sparse Grids Matlab Kit and a
ﬂag on the nestedness property.

The Sparse Grids Matlab Kit provides two families of points that can be used to construct collocation knots
for all the random variables listed in Table 2 (Gauss-type knots and Leja-type knots), and some additional
choices that are available only for certain random variables: speciﬁcally, Clenshaw–Curtis, equispaced knots,
and midpoint knots for uniform random variables, and Genz–Keister knots for normal random variables.
We give details on all these families of knots in the next paragraphs, but we mention already that one big
distinction is that some of these families of knots are nested whereas others are not, and one should in
general favor knots of the former class, as stated already in Section 2. More speciﬁcally, Gauss-type knots
are not nested, whereas all the other choices are nested. The full list of knots available in the Sparse Grids
Matlab Kit and the corresponding commands to generate them are reported in Table 3, together with a ﬂag
to indicate whether a speciﬁc choice is nested or not.

Gauss-type knots Given a pdf, Gauss-type collocation knots are deﬁned as the zeros of the corresponding
orthogonal polynomials:

• Legendre polynomials for uniform random variables;

• Hermite polynomials for normal (gaussian) random variables;

• Laguerre polynomials for exponential random variables;

• generalized Laguerre polynomials for gamma random variables;

• Jacobi polynomials for beta random variables.

7

Gauss–Legendre
uniform

(a)
knots
variables on [0, 1]

for

Gauss–Hermite
standard
for

(b)
knots
normal variables

(c)
Gauss–Laguerre
knots for exponential
variables with λ = 1

(d) Gauss–generalized
Laguerre
for
gamma variables with
α = 2 and β = 1

knots

Gauss–Jacobi
(e)
knots
for beta vari-
ables on [0, 1] with
α = β = 0.5

Figure 3: Gauss-type knots with the corresponding pdfs.

The weights can be formally computed by taking the integral of the corresponding Lagrange polynomials.
However, in practice, knots and weights are more conveniently derived from the eigenvalue decomposition of
a suitable matrix, see e.g. [14] for further details. Note that Gauss-type quadrature rules employing k knots
have degree of exactness 2k − 1. In Fig. 3 we display the ﬁrst 8 Gauss-type knots for the diﬀerent random
variables listed above.

Leja knots Leja knots were ﬁrst introduced for unweighted interpolation on univariate and bivariate
domains, see e.g. [21], and are therefore a suitable choice when yn are uniform random variables on the
interval [a, b]. They are built recursively as:

t(1) = b, t(2) = a, t(3) =

a + b
2

, and t(j) = argmaxt∈[a b]

j−1
(cid:89)

|t − t(k)|,

k=1

(11)

and the corresponding quadrature weights are obtained by evaluating the integrals of the Lagrangian poly-
nomials in Eq. (4) by Gauss-type quadrature rules with appropriate degree of exactness. Observe that by
construction Leja knots are nested.
A limitation of Leja knots is that they are not symmetric with respect to the mid-point a+b
2 of the interval.
However, sequences of symmetric knots can be constructed generating only the even elements of the sequence
with the standard formula in Eq. (11) and then symmetrizing them to obtain the odd elements, i.e.

t(1) = b, t(2) = a, t(3) =

a + b
2

,

t(2j) = argmaxt∈[a b]

2j−1
(cid:89)

|t − t(k)|,

t(2j+1) =

a + b
2

−

k=1

(cid:18)

t(2j) −

(cid:19)

.

a + b
2

(12)

Another class of Leja-type knots, called P-disk knots, can be obtained projecting on the real axis the Leja
knots generated on the complex unit ball. P-disk knots are then naturally deﬁned on [−1, 1], and they
amount to:

t(j) = cos φj ,

where






φ1 = 0, φ2 = π, φ3 = π
2 ,
φ2j+2 = φj+2
2 ,
φ2j+3 = φ2j+2 + π.

(13)

Of course, a linear transformation needs to be applied for random variables on the general interval [a, b]. For
more details on this family of knots we refer to [26]. The ﬁrst 9 knots of the three classes just described are
plotted in Fig. 4a.

8

(a)

(b)

(c)

(d)

Figure 4: a)-c): Collocation knots for uniform random variables on [0, 1]: a) Leja knots; b) nested Clenshaw–
Curtis (CC) knots; c) comparison of the diﬀerent types of knots. d): Collocation knots for standard normal
random variables.

Weighted Leja knots
It is possible to extend the construction of Leja knots to the case when yn are
random variables with a probability distribution other than uniform, see [25]. These knots are the so-called
weighted Leja knots and can be computed again recursively, by suitably introducing a weight in Eq. (11),
i.e., solving

t(j) = argmaxt∈Γ

(cid:112)ρ(t)

j−1
(cid:89)

k=1

(t − t(k)),

where ρ is the pdf of the random variable and Γ its domain. As for standard (non-weighted) Leja sequences,
weighted sequences can be symmetrized, extending analogously Eq. (12). In the Sparse Grids Matlab Kit,
symmetric weighted Leja knots are available for normal and beta random variables.

Clenshaw–Curtis knots Besides the variants of Leja knots, the so-called Clenshaw–Curtis knots are
another nested alternative to Gauss-type knots in case of uniform random variables. They are computed on
[−1, 1] as

t(j)
K = cos

(cid:19)

(cid:18) (j − 1)π
K − 1

,

1 ≤ j ≤ K,

and then linearly transformed to any generic interval [a, b]; the corresponding weights can be eﬃciently
computed by Fast Fourier Transform, see [39]. Quadrature formulas based on this family of knots result to
be as accurate as the Gauss-type quadrature. Moreover, two sets of Clenshaw-Curtis knots with K1 and
K2 knots are nested if (K2 − 1)/(K1 − 1) = 2l for some integer l. Such sequences are obtained e.g. for
K = 1, 3, 5, 9, 17, see Fig. 4b.

Equispaced and midpoint knots These two sequences of knots are provided for uniform random vari-
ables only. They provide classical low-order quadrature formulas [33] and can therefore be useful for quadra-
ture of functions with low regularity. In particular:

• Equispaced knots in [a, b] are provided by the function knots trap that implements the trapezoidal

quadrature rule with knots t(j)

K and weights ω(j)

K






t(j)
K = a + h(j − 1)
K = ω(K)
ω(1)
ω(j)
K = h

K = h/2

1 ≤ j ≤ K

2 ≤ j ≤ K − 1

with h =

1
K − 1

.

Similarly to Clenshaw–Curtis knots, two sets of equispaced knots with K1 and K2 knots are nested if
(K2 − 1)/(K1 − 1) = 2l for some integer l.

9

linear
2-step
doubling
tripling
Genz–Keister m(1) = 1, m(2) = 3, m(3) = 9, m(4) = 19, m(5) = 35

deﬁnition
m(in) = in
m(in) = 2(in − 1) + 1
m(1) = 1, m(in) = 2in−1 + 1
m(in) = 3in−1

function
lev2knots lin
lev2knots 2step
lev2knots doubling
lev2knots tripling
lev2knots GK

Table 4: Diﬀerent type of level-to-knots functions and corresponding function of the Sparse Grids Matlab
Kit.

• Midpoint knots are provided by the function knots midpoint, that implements the midpoint quadra-

ture rule:






t(j)
K = a +
ω(j)
K = h

h
2

+ h(j − 1)

1 ≤ j ≤ K

1 ≤ j ≤ K

with h =

1
K

.

Two sets of midpoints knots with K1 and K2 knots are nested if K2/K1 = 3l for some integer l.

Observe that these two rules depart a bit from the general framework of the package, in that they do not
derive quadrature weights as integral of global Lagrange polynomials, but rather as integrals of piecewise
Lagrange polynomials (linear and constant, respectively). However, we remark that the package does not
provide piecewise interpolation, but only global interpolation. Therefore, if one were to use these knots to
build a sparse grid interpolant of a function (see Section 4.1 for details), Runge phenomena can be expected,
leading to poor approximations. These knots are thus to be intended essentially as quadrature knots only.
To summarize, in Fig. 4c we give a comparison of the types of collocation knots available for uniform random
variables.

Genz–Keister knots The sequence of Genz–Keister knots is obtained by modifying the sequence of
Gauss–Hermite knots to enforce nestedness, see [15, 10]. The knots and the corresponding weights are
tabulated and available for K = 1, 3, 9, 19, 35. In Fig. 4d we display the ﬁrst 9 Genz–Keister knots, together
with Gauss-type and weighted Leja knots for normal random variables.

3.2 Level-to-knots functions

The above list of knots can be used in conjunction with diﬀerent types of level-to-knots functions (see Eq.
(1)) that associate the discretization levels collected in the multi-index i to the number of collocation knots
to be used; of course, some choices are smarter than others, such as those that guarantee nestedness. For
example, in the case of Clenshaw–Curtis knots, any number of knots K can be used, but a speciﬁc relation
must hold between the cardinality of the sequences of knots such that they are nested (see discussion above).
In the case of symmetric Leja knots instead, it is natural to use a level-to-knots function that adds two knots
(or, more generally, an even number of knots) at each level, to make sure that sequences that are constructed
by symmetrization are actually used as such.

In Table 4 we list the level-to-knots functions that are implemented in the Sparse Grids Matlab Kit, which are
the possible options to replace the assignment in line 2 of Listing 1, lev2knots = @lev2knots doubling.
The function linear can be used for Gauss-type and Leja knots: however, as already pointed out, in case of
symmetric Leja knots it is convenient to resort to the functions of type 2-step or doubling. The function of
type doubling is essential for the generation of sequences of nested Clenshaw–Curtis and equispaced knots;
the function tripling must be used to obtain nested midpoint knots, and the tabulated function Genz–Keister
is speciﬁc for the case of Genz–Keister knots. Table 5 gives an overview on the possible combination of knots
and level-to-knots functions that generate sequences of nested collocation knots.

10

uniform

knots
Leja - standard
Leja - symmetric
Leja - P-disk
Clenshaw–Curtis
equispaced
midpoint
weighted Leja - standard
weighted Leja - symmetric
Genz–Keister
exponential weighted Leja
weighted Leja
gamma
weighted Leja - standard
beta
weighted Leja - symmetric

normal

level-to-knots functions for nestedness
linear, 2-step, doubling
2-step, doubling
linear, 2-step, doubling
doubling
doubling
tripling
linear, 2-step, doubling
2-step, doubling
Genz–Keister
linear, 2-step, doubling
linear, 2-step, doubling
linear, 2-step, doubling
2-step, doubling

Table 5: Families of knots with the corresponding level-to-knots functions to ensure nestedness.

3.3 Multi-index sets

As discussed in Sect. 2, the quality of the sparse grid approximations (7) and (8) depends on the choice of the
multi-index set I. Roughly speaking, we can think that the larger the multi-index set, the more terms the
approximation includes and the better the approximation, but also the larger the number of knots where f
has to be evaluated and hence the cost: therefore, some trade-oﬀ between accuracy and cost must be found.
One approach to this end is to design the set based on some knowledge on the problem at hand, such as the
smoothness of the function to be approximated, see e.g. [5, 27, 35, 2]. An alternative “a-posteriori”/adaptive
approach will be discussed later on.
Multi-index sets are stored in the Sparse Grids Matlab Kit as rows of matrices, in lexicographic order. Small
multi-index sets can be typed in “manually”3, such as in lines 3-6 of Listing 1, but of course dedicated
functions are available. In particular multiidx box set(jj), where jj is a multi-index (i.e., a row vector),
generates the hyper-rectangular (a.k.a. “box”) set I = {i ∈ NN
+ : i ≤ j}. Furthermore, all multi-index sets
that can be written as I = {i ∈ NN
+ : r(i) ≤ w} for some w ∈ N+ can be created invoking the function
multiidx gen as follows

1 N = 2;
2 rule = @(ii) sum(ii-1);
3 w = 4;
4 base = 1;
5 I = multiidx gen(N,rule,w,base);

Listing 2: Creation of a multi-index set.

where rule is a @-function such that rule(ii) evaluates r(i), and the integer value w deﬁnes the size of the
multi-index set, and hence controls the quality of the associated sparse grid approximation. In the following,
we refer to r as rule of the approximation and to w as level of the approximation. The implementation of
multiidx gen is recursive over n, n = 1, . . . , N . This is a ﬂexible solution that can accommodate for a
wide range of rules r (although admittedly not the most eﬃcient speed-wise): the only requests are that r
is non-decreasing in each argument, and that r(j) > w ⇒ r([j, 1]) > w, ∀j ∈ Nn
A list of multi-index sets I commonly found in literature and the implementation of the corresponding rule
in Matlab is available in Table 6. The vectors g ∈ RN
+ appearing in the deﬁnitions in Table 6 4 are the so-
called anisotropy weights, whose purpose is to tune the shape of the multi-index set in such a way that more

+, n < N .

3Matlab provides commands to check whether the rows of a matrix are sorted, such as issortedrows
4The peculiar writing g(1:length(ii)) is due to fact that multiidx gen works recursively on the dimensions n = 1

to N , and therefore we need to be able to call the @-function also when the length of g and ii do not match.

11

deﬁnition
Imax(w) = (cid:8)i ∈ NN
i ∈ NN
Isum(w) =
(cid:110)

(cid:110)

Iprod(w) =

i ∈ NN

+ : maxn gn(in − 1) ≤ w(cid:9)
(cid:111)
+ : (cid:80)N
n=1 gn(in − 1) ≤ w
+ : (cid:81)N
n=1 ign
n ≤ w

(cid:111)

function
rule = @(ii) max(g(1:length(ii)).*(ii-1))
rule = @(ii) sum(g(1:length(ii)).*(ii-1))

rule = @(ii) prod((ii).ˆg(1:length(ii)))

Table 6: Multi-index sets and corresponding deﬁnitions in Matlab.

collocation knots are placed along the directions of Γ deemed to be more important; in particular, the larger
the weight, the more penalized the corresponding direction, i.e. the lower the number of collocation knots
placed in that direction. Whenever g1 = . . . = gN , the resulting grid is named isotropic, and anisotropic
otherwise. Finally, we clarify the role of the variable base=1 in Listing 2: its purpose is to specify that the
smallest entry of each multi-index is 1 (another option would be base=0; multi-index sets like these will be
needed later on, in Section 4.4). We further illustrate the use of multiidx gen with the help of the following
example.

Example 2 (Multi-index sets) The two-dimensional isotropic multi-index set Isum(4) can be generated
as in Listing 2. For the case of anisotropic sets we have to further specify the vector of weights g:

1 N = 2;
2 g = [1,2];
3 rule = @(ii) sum(g(1:length(ii)).*(ii-1));
4 w = 4;
5 base = 1;
6 I aniso = multiidx gen(N,rule,w,base);
7 plot multiidx set(I aniso,'sk','LineWidth',2,'MarkerSize',22,'MarkerFaceColor','b')

Fig. 5 (top-row) shows the isotropic and anisotropic version of the index sets (the listing above also reports
how to use the function plot multiidx set to plot a multi-index set), together with the corresponding sparse
grids, generated using symmetric Leja knots and level-to-knots function of type 2-step. The code to generate
such grids is almost identical to the one in Listing 1 and will be discussed in details later on.
The following rows of the ﬁgure show multi-index sets and grids for the other choices of sets introduced
in Table 6, i.e., Imax (mid-row of the ﬁgure) and Iprod (bottom-row of the ﬁgure), again in their isotropic
and anisotropic versions (left and right part of the ﬁgure, respectively). The code to generate these multi-
index sets is identical to the listing above (other than changing Line 3 with the appropriate deﬁnition from
Table 6), therefore we do not report it. Note that Imax sets can be equivalently generated with the command
multiidx box set. Finally, we also mention that for faster generation of Isum, the function fast TD set
is available - the reason for this naming will be clearer later on.

Typing rules in Table 6 might be inconvenient, especially if anisotropic rules are needed. To this end, the
Sparse Grids Matlab Kit provides the convenience function define functions for rule that takes as input
a string (“the name of the rule”) and returns the @-function and the level-to-knots function for a number of
common sparse grid constructions.

1 [lev2knots,rule] = define functions for rule(<string-name>)

The names of these “pre-sets” and the corresponding rule, and level-to-knots functions are reported in
Table 7. The names of the ﬁrst three pre-sets ('TP','TD','HC') are borrowed from the literature on
polynomial approximation spaces, and refer to the Tensor Product, Total Degree, and Hyberbolic Cross
spaces, respectively.
Indeed, each sparse grid approximation is a polynomial approximation, and it can
be shown that the polynomial approximations generated by these pre-sets belong to the above-mentioned
polynomial spaces [3]. The fourth pre-set 'SM' can be used to deﬁne one of the most used type of sparse
grid, the so-called Smolyak-type grid, which is obtained combining Isum with the level-to-knots function of
type doubling (the complete example will be given in Listing 3 later on).

12

(a) Isotropic Isum(4) (left) and corresponding grid (right)

(b) Anisotropic Isum(4) (left) and corresponding grid
(right)

(c) Isotropic Imax(4) (left) and corresponding grid (right)

(d) Anisotropic Imax(4) (left) and corresponding grid
(right)

(e) Isotropic Iprod(4) (left) and corresponding grid (right)

(f) Anisotropic Iprod(4) (left) and corresponding grid
(right)

Figure 5: Isotropic and anisotropic multi-index sets and corresponding sparse grids of symmetric Leja knots
on Γ = [0, 1]2.

3.4 Sparse grid generation and data structure

We are ﬁnally ready to discuss lines 7 and 8 of Listing 1, that are responsible of generating the sparse grid.
The approach reported in the snippet can be called a-priori, since it requires to specify the multi-index set I
before sampling the function f . In this section we detail this approach ﬁrst, and then discuss also the “dual”
approach, i.e. the adaptive algorithm to generate sparse grids, in which the multi-index set I is computed
simultaneously to the sampling of the function f (Sect. 3.4.2).

Note that regardless of the way in which sparse grids are generated, they are eventually stored with the same
data structure, that we describe below, while discussing the a-priori approach.

13

name
'TP'
'TD'
'HC'
'SM'

rule
Imax
Isum
Iprod
Isum

level-to-knots functions
linear
linear
linear
doubling

Table 7: Pre-sets available in define function for rule. Rules are deﬁned in Table 6, level-to-knots
functions in Table 4.

3.4.1 A-priori sparse grids

Listing 1 shows a ﬁrst way to generate a sparse grid with the multi-index set determined a-priori, i.e., by
deﬁning a multi-index set ﬁrst (in any of the ways described in Section 3.3: directly typing in the set, or
using the functions multiidx box set, multiidx gen, or define functions for rule), and then using the
function smolyak grid multiidx set. Another possibility is to call the command smolyak grid, that takes
care of generating both the multi-index set given the rule and the associated sparse grid, as shown in the snip-
pet below. Before going further, note that the commands smolyak grid and smolyak grid multiidx set
are used to generate all sorts of sparse grids and not just the strictly speaking Smolyak-type ('SM') sparse
grids, i.e., those corresponding to line 4 of the pre-sets in Table 7. The “misleading” name of the functions
comes from early-stage implementations of the software and is maintained for back-compatibility with earlier
versions of the Sparse Grids Matlab Kit.

1 N = 2;
2 w = 3;
3 knots = @(n) knots CC(n,0,1);
4 lev2knots = @lev2knots doubling;
5 rule = @(ii) sum(ii-1);
6 S = smolyak grid(N,w,knots,lev2knots,rule);

Listing 3: Basic creation of a sparse grid given a multi-index set deﬁned by a rule.

The output of smolyak grid (and of smolyak grid multiidx set) is a sparse grid in the so-called extended
format, that stores separately the information of each tensor grid composing the sparse grid (more precisely,
of those grids whose coeﬃcient in the combination technique formula is non-zero, see Eqs. (7) and (8)). The
data structure chosen to this end is a structure array, where each structure identiﬁes one of the tensor grids:

1 >> S
2 S =

3
4 1x7 struct array with fields:

5
6 knots
7 weights
8 size
9 knots per dim
10 m
11 coeff
12 idx

Each tensor grid contains the following ﬁelds:

• idx: the multi-index i ∈ I corresponding to the current tensor grid;

• knots: matrix collecting the knots Ti, each knot being a column vector;

• weights: vector of the quadrature weights ω(j)

m(i) corresponding to the knots;

• size: size of the tensor grid, i.e. the number of knots Mi = (cid:81)N

n=1 m(in);

14

(a) TIsum(3)

(b) T[1,3] with c[1,3] = −1

(c) T[1,4] with c[1,3] = 1

(d) T[2,2] with c[1,3] = −1

(e) T[2,3] with c[1,3] = 1

(f) T[3,1] with c[1,3] = −1

(g) T[3,2] with c[1,3] = 1

(h) T[4,1] with c[1,3] = 1

Figure 6: The sparse grid TIsum(3) of Clenshaw–Curtis knots and the seven tensor grids that generates it.

• knots-per-dim: cell array with N components, each component collecting in an array the set of

one-dimensional knots Tn,in used to build the tensor grid;

• m: vector collecting the number of knots used in each of the N directions m(i) = [m(i1), m(i2), . . . , m(iN )];

• coeff: the coeﬃcients ci of the sparse grid in the combination technique formulas (7) and (8).

The structure array above has seven components, since seven tensor grids are used in the construction of
the sparse grid. We report below the ﬁrst component of the structure array:

1 >> S(1)
2 ans =

3
4 struct with fields:

5
6 knots: [2x5 double]
7 weights: [-0.0333 -0.2667 -0.4000 -0.2667 -0.0333]
8 size: 5
9 knots per dim: {[0.5000]
10 m: [1 5]
11 coeff: -1
12 idx: [1 3]

[1 0.8536 0.5000 0.1464 0]}

We plot in Fig. 6a the sparse grid generated in Listing 3, and in Fig. 6b–h the seven tensor grids that
explicitely contribute to it. We can easily observe that some knots appear in multiple tensor grids, and
therefore the sparse grids structure S contains redundant information. This is a general fact, happening not
only when nested knots are used, but also (to a smaller extent) in the case of non-nested knots. Hence,
it is convenient to generate a structure containing only the non-repeated knots and a list of corresponding
weights (we will detail in a moment how these weights are determined). This can be done calling the function
Sr = reduce sparse grid(S), that outputs a unique structure containing the following ﬁelds:

15

• knots: matrix collecting the list of non-repeated knots, i.e., the set TI in Eq. (9);

• weights: vector of quadrature weights corresponding to the knots above;

• size: size of the sparse grid, i.e. the number of non-repeated knots;

• m: index array that maps each knot of Sr.knots to their original position in [S.knots] (if they have

been retained as unique representative of several repeated knots);

• n: index array that maps each knot of [S.knots] to Sr.knots.

In practice, the list of unique knots is created by detecting (up to a certain tolerance, tunable by the user)
the identical knots and deleting the possible repetitions. The quadrature weights of the repeated knots are
obtained taking the linear combination of the quadrature weights of each instance of the repeated knot with
the combination coeﬃcient weights in Eq. (7). For instance, the sparse grid S above consists of 67 knots

1 >> size([S.knots])
2 ans =

3
4 2

67

most of them repeated in multiple tensor grids (cf. Figure 6b–h). The corresponding reduced sparse grid
results in 29 non-repeated knots:

1 >> Sr = reduce sparse grid(S)
2 Sr =

3
4 struct with fields:

5
6 knots: [2x29 double]
7 m: [29x1 double]
8 weights: [1x29 double]
9 n: [67x1 double]
10 size: 29

Both extended and reduced formats are useful for working with sparse grids and should always be stored
in memory. This implies a certain redundancy in memory storage, but speeds up some computations and
simpliﬁes considerably the operations from the point of view of the user.

Finally, for quick generation of a sparse grid (both extended and reduced formats), the Sparse Grids Matlab
Kit provides also the convenience function smolyak grid quick preset, which generates Smolyak-type grids
of level w with Clenshaw–Curtis points in the default interval [−1, 1]N , and only takes as inputs N and w

1 >> [S,Sr]=smolyak grid quick preset(N,w);

We close this subsection by discussing the computational cost required to generate a sparse grid by means
of the following example.

Example 3 (Computational cost) Let us consider Smolyak-type ('SM') sparse grids for N = 2, . . . , 10,
generated for levels w = 3 and w = 5 with Clenshaw–Curtis knots. In Fig. 7a we display the sparse grid size
and the computational time for generating these two sequences of sparse grids in reduced format (as obtained
by calling the functions smolyak grid and reduce sparse grid in sequence). Fig. 7b shows in particular
the percentage of such computational time taken by the function reduce sparse grid. Conversely, in Fig. 7c
we display the sparse grid size and computational time, and in Fig. 7d the time percentage of the reduction
step, obtained when ﬁxing the dimension to N = 3 or N = 5 and increasing the level of approximation
w = 2, . . . , 10. Both the computational time and the sparse grid size can be seen to grow faster with respect
to w than N . Moreover, the time taken by the reduction step is mildly impacting on the total time when
keeping w to small values and increasing N (panel b), whereas steadily increasing with w (panel d). This

16

(a) Sparse grid size and total computa-
tional time for increasing N (logarithmic
scale in the vertical axis)

(b) Percentage of computational time
taken by the reduction step for increas-
ing N

(c) Sparse grid size and computational
time for increasing w (logarithmic scale in
the vertical axis)

(d) Percentage of computational time
taken by the reduction step for increas-
ing w

Figure 7: Computational cost and size of sparse grids for diﬀerent values of N and w.

phenomenon is partially due to the chosen level-to-knots function (doubling type here) and using another type
of level-to-knots function can be expected to result in a lower percentage of time being spent on reduction.

The tests are carried out in Matlab 2019b on a standard Laptop with processor Intel(R) Core(TM) i7-8665U
CPU 2.10/4.80 GHz and 16 GB RAM.

3.4.2 Adaptive sparse grids

The Sparse Grids Matlab Kit provides also the possibility to construct adaptive sparse grids, i.e. sparse
grids where the multi-index set, and hence the approximations in Eqs. (7) and (8), are constructed in
an iterative way, relying on some heuristic criteria based on the values of the function currently being
interpolated/integrated. The function provided to this end is called adapt sparse grids and a minimal
working example of its use is provided in Listing 4.

1 f = @(y) exp(sum(y)); % @-function, takes as input a column vector for y and returns a scalar

% or column-vector

2
3 N = 2;
4 knots = @(n) knots CC(n,0,1);
5 lev2knots = @lev2knots doubling;
6 controls = struct('nested',true); % each field of this struct specifies an optional argument

7
8 Ad = adapt sparse grids(f,N,knots,lev2knots,[],controls);

% to control the algorithm

Listing 4: Basic creation of an adaptive sparse grid.

17

The adaptive algorithm implemented in the Sparse Grids Matlab Kit is described in details in [28] and
extends the original one in [16]; we brieﬂy describe it here to the extent needed to illustrate the options
available in the current implementation. Roughly speaking, the algorithm starts with the trivial multi-index
set I = {[1, 1, . . . , 1]} and iteratively adds to I the multi-index i with the largest heuristic proﬁt indicator
choosing from a set of candidates, called reduced margin of I and deﬁned as follows:

R(I) = {i ∈ NN

+ s.t. i (cid:54)∈ I and i − en ∈ I ∀ n ∈ {1, . . . , N } s.t. in > 1}.

Note that the condition i ∈ R(I) is requested to guarantee that I ∪ {i} is downward closed, cf. Eq. (10).
The role of the proﬁt indicator is to balance error reduction and additional computational costs brought in
by each multi-index (where the cost is measured as number of new evaluations needed to add a multi-index
to I). In other words, it quantiﬁes the fact that ideally we would like to add to the sparse grid multi-indices
that carry a large reduction in interpolation/quadrature error for a minimal extra cost. Several alternatives
in the following, we describe those that have been
for proﬁt deﬁnitions have been proposed in literature:
implemented in the function adapt sparse grids.
To this end, let us introduce the following two error indicators that express the improvement in the sparse
grid approximation obtained by enlarging an arbitrary set I by a multi-index i ∈ R(I):

E Q
i = |QI∪{i} − QI|
E U ,ξ
i = max
y∈H

(cid:0)|UI∪{i}(y) − UI(y)|ξ(y)(cid:1) ,

(14)

(15)

where ξ(y) is either 1 (for y distributed as uniform random variables) or ξ(y) = ρ(y) (for the other supported
random variables) and the set H ∈ Γ is a suitable set of “testing points”: when nested points are considered,
we take H = TI∪i \ TI, i.e., the set of collocation points brought in the sparse grid by the addition of i
(this choice can be motivated by the interpolatory property of the sparse grids for nested points, see [28] for
details); conversely, when non-nested points are considered we take H = Ti, i.e., the entire tensor grid just
added (see again [28] for details).
Furthermore, we introduce a work indicator deﬁned as follows, to estimate the number of new evaluations of
f required to build the sparse grid based on I ∪ {i} given that the sparse grid based on I is already available:

Wi =

Wi =

N
(cid:89)

n=1

N
(cid:89)

n=1

(m(in) − m(in − 1))

for nested points

m(in)

for non-nested points.

(16)

(17)

Note that while in the case of nested points the number of new points is computed exactly, in the case of
non-nested points the formula above is just a worst-case upper bound, i.e., the case in which none of the
points required by Ti is already in TI.
Proﬁts are then computed dividing the error contribution of each multi-index i (either Eq. (14) or (15)) by the
appropriate weight deﬁnition (either Eq. (16) or (17), depending on the nestedness of the points). Note that
computing an error indicator entails evaluating the function f at each new collocation point, and is therefore
a potentially expensive operation. This also justiﬁes why the algorithm is typically referred to as “a-posteriori
adaptive”, since the choice on which multi-index should be added to I is taken after having evaluated the
function. Table 8 summarizes the discussion above and lists of the proﬁts implemented in the Sparse Grids
Matlab Kit. The proﬁt to be used can be speciﬁed as further ﬁeld of the structure array controls (line
6 of Listing 4), such as e.g. controls = struct(..,'prof','Linf'). Note that some proﬁt indicators
actually do not consider the work contribution and therefore are not proﬁts, strictly speaking. The weight ξ
in Eq. (15) can be provided as a function handle as ﬁeld of controls = struct(..,'pdf',@(y) ..). The
formula for Wi and the testing set H are automatically selected based on the ﬁeld 'nested' of the structure
array, which can take value <true/false> and is the only mandatory ﬁeld to be provided (check Listing 4,

18

deltaint
deltaint/new points
Linf
Linf/new points (default)
weighted Linf
weighted Linf/new points

deﬁnition
E Q
E Q/W
E U ,1
E U ,1/W
E U ,ρ
E U ,ρ/W

Table 8: Proﬁts and corresponding names implemented in the Sparse Grids Matlab Kit.

name
'nested'
'prof'
'pdf'
'max pts'
'prof tol'
'var buffer size'

default values
none (must be always set)
Linf/new points
none (must be set if required by proﬁt)
1000
10−14
0 (i.e., no buﬀer)

Table 9: Main control options of the implementation of adapt sparse grids and their default value.

line 6). To the best of the authors’ knowledge, the Sparse Grids Matlab Kit is one of the few packages that
provides the possibility of using adaptive grids with non-nested points.
The adaptive algorithm terminates when one or more suitable stopping criterion is met. Common choices are
checking that the computational work or the proﬁts of the multi-indices in the reduced margin are respectively
above or below a certain tolerance. These termination criteria can be controlled by setting further ﬁelds in
the controls structure array: controls = struct(..,'max pts',<value>, 'prof tol',<value>).
Another important feature of the implementation of the adaptive algorithm in the Sparse Grids Matlab
Kit is the so-called buﬀering of dimensions, see [28]. When the function f to be sampled depends on a
large number of random variables, N (cid:29) 1, the adaptive algorithm might be computationally intensive,
since the size of the reduced margin R(I) grows quickly with N . Thus, if we know that the random
variables are “sorted according to their importance” (this might be e.g. the case when f is the solution of
a PDE with random inputs coming from a Karhunen–Lo`eve expansion:
in this case the random variables
are multiplied by coeﬃcients that decrease, sometimes rather quickly, hence their impact on the solution is
“less and less important”, see e.g. [6], [34]) it makes sense to start the algorithm by exploring only an initial
subset of random variables (the most relevant ones) and then gradually adding more and more variables
to the approximation, in such a way that at each iteration the algorithm is “seeing” a certain number
Nbuﬀ of “non-activated” random variables (“buﬀered random variables”). The size of Nbuﬀ can be set as
controls = struct(..,'var buffer size',<value>). We summarize the options discussed so far in
Table 9. We refer readers to the help of adapt sparse grids for a full list of options.
The output of the function adapt sparse grids is also a structure array, which contains in separate ﬁelds
both the extended and the reduced version of the grid, as well as additional information such as the values
of the function f evaluated at the collocation points, the current index set I, the reduced margin R(I)
and others; we refer again readers to the help of adapt sparse grids for a full list of outputs. The sparse
grid being output is actually the one built over I ∪ R(I), since the evaluations of f required to add the
multi-indices in R(I) to the sparse grids are available anyways.

1 >> Ad
2 Ad =

3
4 struct with fields:

5
6 N: 2
7 S: [1x7 struct]

19

8 Sr: [1x1 struct]
9 f on Sr: [1x257 double] % values of f at the sparse grid knots
10 nb pts: 257 % nb. of knots in Sr
11 nested: 1
12 nb pts visited: 257 % number of points considered during the construction.

13

14

17

18

19

% For nested knots, this is equal to nb pts; for non-nested points,
% this will be larger than nb pts, because some points enter and then exit
% the grid when the corresponding idx exits from the combination technique.

15
16 num evals: 257 % actual nb of evaluations of f required to build the sparse grid.

% This is not necessarily equal to nb pts visited because for speed reasons
% some function evaluations might be taken more than once (e.g. when
% if evaluating f is faster than checking whether the point has been
% already evaluated)

20
21 intf: 2.9525 % expected value of f
22 private: [1x1 struct] % contains more detailed info on the status of the adaptive algorithm

3.4.3 Grid recycling

We close this section pointing out a useful feature shared by all the described functions to build sparse
grids (smolyak grid, smolyak grid multiidx set, and adapt sparse grids). For eﬃciency reasons, all
these functions can take as optional input another grid already available in memory and recycle as many
computations as possible from there, in terms e.g. of generating the multi-index set, computing the coeﬃcients
of the combination technique, and generating the tensor grids needed for the the extended version of their
storage. The mechanisms implemented to this end are a bit diﬀerent depending whether one is working with
a-priori or adaptive sparse grids:

1. the commands smolyak grid and smolyak grid multiidx set to generate a-priori grids take as input

an extended version of a previous grid:

2

1 I = [1 1;
1 2;
2 1;
3 1];

3

4
5 S = smolyak grid multiidx set(I,knots,lev2knots);
6 I new = [I;

7

8

1 3;
2 2;
4 1];

9
10 I new = sortrows(I new); % multi-indices must be sorted in lexicographic order
11 S new = smolyak grid multiidx set(I new,knots,lev2knots,S);

2. adapt sparse grids takes as input the result of a previous computation, to resume the computation

where the previous one stopped:

1 % stop the adaptive algorithm when profits < 1e-5
2 controls = struct('nested',true,'prof tol',1e-5);
3 Ad = adapt sparse grid(f,N,knots,lev2knots,[],controls);
4 % run the algorithm up to default tolerance
5 controls next = struct('nested',true); % default tolerance 1e-14
6 Next = adapt sparse grid(f,N,knots,lev2knots,Ad,controls next);

3. When the new grid diﬀers from the previous one by just one multi-index, the code can be further

optimized, and an ad-hoc function in this case is available:

2

1 I in = [1 1;
1 2;
2 1;
3 1];

3

4
5 S in = smolyak grid multiidx set(I in,knots,lev2knots);

20

6 coeff in = combination technique(I in);
7 new idx = [4 1];
8 S new = smolyak grid add multiidx(new idx,S in,I in,coeff in,knots,lev2knots);

4 Main functions

After having discussed how to generate a sparse grid, the aim of this section is to present the main func-
tionalities of the Sparse Grids Matlab Kit. We ﬁrst discuss basic tasks, i.e. evaluation, quadrature and
interpolation of the function f , as well as plotting of the sparse grid approximation and some advanced
features that can be optionally activated to save computational time (parallelization, evaluation recycling).
We then move to tools to manipulate the sparse grid approximation: computation of gradients and Hessians,
and computation of Sobol indices by conversion to Polynomial Chaos Expansion. Most of these functions
work for vector-valued f : Γ ⊂ RN → RV , in which case the function is applied component-wise to f .

4.1 Basic operations: function evaluation, quadrature, interpolation, and plot-

ting

Once a sparse grid is built, the basic operations of interest are: evaluating a function f for each point of the
grid, f (yj) ∀yj ∈ TI, approximating the integral (expected value) (cid:82)
Γ f (y)ρ(y)dy, evaluating and plotting
the sparse grid approximation UI(y) for y ∈ Γ; cf. Eqs. (7), (8), and (9). The corresponding functions of
the Sparse Grids Matlab Kit are:

• evaluate on sparse grid, that evaluates the function f at the sparse grid points;

• quadrature on sparse grid, that computes integrals of f by quadrature formulas collocated at the

sparse grid points, cf. Eq. (8);

• interpolate on sparse grid, that evaluates the sparse grids approximation as deﬁned in Eq. (7);

• plot sparse grids interpolant, that plots such sparse grids approximation (diﬀerent visualization

formats are used for N = 2, N = 3, N > 3).

We illustrate them with the help of the following snippets. First, the values of f at the sparse grid points
are obtained as follows:

1 f = @(y) exp(sum(y));
2 % S as in Listing 3
3 Sr = reduce sparse grid(S);
4 f on Sr = evaluate on sparse grid(f,Sr);

We plot the result in Fig. 8a. Once the values of f at the sparse grid points are available we can compute
the approximated value of the integral of f , as well as construct and evaluate the sparse grid approximation
deﬁned in Eq. (7). The snippet below shows how to compute approximated integrals of f and of f 2:

1 q = quadrature on sparse grid(f on Sr ,Sr);
2 q2= quadrature on sparse grid(f on Sr.ˆ2,Sr);

It is also possible to condense the calls of evaluate on sparse grids and quadrature on sparse grids
in a single call (note that the ﬁrst argument this time is f, @-function):

1 [q,f on Sr] = quadrature on sparse grid(f,Sr);

21

(a) Values of f at the sparse grid points

(b) Sparse grid approximation of f evaluated
at the points of a uniform cartesian grid(the
values are marked in red)

Figure 8: Evaluations of f (y) = exp(y1 + y2) at the sparse grid knots and corresponding sparse grid approx-
imation.

Finally, the following piece of code shows how to evaluate the sparse grid approximation at a uniform cartesian
grid of 15 × 15 points. The resulting surface is reported in Fig. 8b. Note that for this operation both the
extended and reduced versions of the sparse grid are needed. This is because in pratice the implementation
works tensor grid by tensor grid, creating the Lagrange interpolants of f on each of them and then ﬁnally
summing up everything.

1 % building the cartesian grid of 15x15 equispaced points
2 y = linspace(0,1,15);
3 [Y1,Y2] = meshgrid(y,y);
4 % rearrange the points such that each point is a column
5 eval points = [Y1(:)';Y2(:)']; % matrix of dim. 2x15
6 % S, Sr and f on Sr as above
7 f vals = interpolate on sparse grid(S,Sr,f on Sr ,eval points);
8 surf(Y1,Y2,reshape(f vals,15,15))

The snippet above is actually very close to the implementation for the case N = 2 of the last “basic” function
mentioned at the beginning of this section, i.e. plot sparse grids interpolant, that plots the sparse grids
approximation. For N = 3, this function would instead plot a number of contourf (i.e. ﬂat surfaces colored
according to the value of the interpolant), stacked over the same axis; ﬁnally, when N > 3 a number of bi-
dimensional cuts of the domain Γ are considered, and for each of them the surface (i.e., the plot for N = 2)
will be generated, keeping all other yn constant to their mid-value. Examples for cases N = 3 and N = 4 are
reported in the listing below, and the corresponding plots are shown in Fig. 9 and 10, respectively. We refer
the readers to the help of plot sparse grids interpolant for the full description of the optional controls
to generate plots like these. Of course, the Sparse Grids Matlab Kit contains also functions to plot the
points of a sparse grid: plot sparse grid (that we have already used multiple times in previous snippets)
and plot3 sparse grid (see snippet below). The former can be used to plot two-dimensional sparse grids,
or two-dimensional projections of a higher-dimensional sparse grid; the latter can be used instead to plot
three-dimensional sparse grids, or three-dimensional projections.

1 N = 3;
2 f = @(x) exp(sum(x));
3 % S as in Listing 3 with N = 3, Sr its reduced version
4 plot3 sparse grid(S)
5 f on Sr = evaluate on sparse grid(f,Sr);
6 domain = [0 0 0; 1 1 1]; % domain where to plot
7 plot sparse grids interpolant(S,Sr,domain,f on Sr ,'nb contourfs',5,'nb countourf lines',10);

8
9 N = 4;

22

(a) Sparse grid

(b) Sparse grid approximation

Figure 9: Sparse grid and corresponding approximation of f (y) = exp(y1 + y2 + y3).

(a) Cut for (y1, y2)

(b) Cut for (y3, y4)

(c) Cut for (y1, y4)

Figure 10: Sparse grid approximation of f (y) = exp(y1 + 1

2 y2 + 3

2 y3 + 2y4).

10 f = @(x) exp(x(1,:)+0.5*x(2,:)+1.5*x(3,:)+2*x(4,:))
11 % S as in Listing 3 with N=4, Sr its reduced version
12 f on Sr = evaluate on sparse grid(f,Sr);
13 domain = [0 0 0 0; 1 1 1 1]; % domain where to plot
14 plot sparse grids interpolant(S,Sr,domain,f on Sr ,'two dim cuts',[1 2 3 4 1 4])
15 % cut for (y1,y2),(y3,y4),(y1,y4)

4.2 Parallelization, evaluation recycling

The functions evaluate on sparse grids and quadrature on sparse grids are in practice wrappers for
a) looping through the points of a sparse grid and calling the evaluation of f and b) computing the sum of
the values of f , each weighted by the corresponding quadrature weight. These wrappers are also convenient
since they implement a couple of advanced features that can be used to improve the computational eﬃciency.
First, both these functions also act as wrappers to calls of the Matlab parallel toolbox construct parfor, to
speed-up the evaluations of f by running the evaluations on multiple cores; this functionality can be activated
by setting certain optional arguments, for which we refer to the help of the functions. Second, along the lines
of grid recycling discussed in Sect. 3.4.3, when multiple grids have to be built and the function f evaluated
at the corresponding knots, signiﬁcant computational savings can be achieved by recycling the evaluations
of f that have been already performed. This can be obtained by passing to evaluate on sparse grids and
quadrature on sparse grids the previous grids and evaluations as additional inputs, as shown in the next
listing where we generate a sequence of grids with increasing w in a for loop.

23

(a) Number of evaluations without recycling
(solid lines) and with recycling (dashed lines)
(logarithmic scale on the vertical axis)

(b) Percentage of saved evaluation in case of
recycling with respect to the number of eval-
uations without recycling

Figure 11: Assessment of the recycling functionality: test for diﬀerent values of w.

1 N = 2;
2 f = @(y) exp(sum(y));
3 knots = @(n) knots CC(n,0,1);
4 lev2knots = @lev2knots doubling;
5 rule = @(ii) sum(ii-1);
6 % consider a sequence of grids with increasing level
7 w vec = 1:6;
8 S old = []; % initialize the grid
9 Sr old = []; % initialize the reduced grid
10 evals old = []; % initialize the set of evaluations
11 for w = w vec

12

13

14

15

16

17

18

19

20

21

22

23

24

% recycle tensor grids from S old
S = smolyak grid(N,w,knots,lev2knots,rule,S old);
Sr = reduce sparse grid(S);
% recycle evaluations on S old
f evals = evaluate on sparse grid(f,S,Sr,evals old,S old,Sr old);
% recycling with quadrature would be obtained in the same way
% [f evals,q] = quadrature on sparse grid(f,S,Sr,evals old,S old,Sr old)
..
% do something with f evals
..
% then update the containers
S old = S;
Sr old = Sr;
evals old = f evals;

25
26 end

In Fig. 11a we compare the number of function evaluations required by the listing above (also for higher
dimensional sparse grids with N = 4, 6), with and without recycling from the results for the previous value
of w. Moreover, in Fig. 11b we display the percentage of evaluations saved when making use of the recycling
functionality, and indeed observe that such saving is considerable. However, note that this amount depends
on the type of multi-index set, the level-to-knots function and the type of knots used.

4.3 Gradients and Hessian

Computing gradients and Hessians of the sparse grids approximation of f can be useful in applications, such
as the inverse UQ problem discussed in Sect. 5.2, optimization or local sensitivity analysis studies. To this
end, the Sparse Grids Matlab Kit provides the functions derive sparse grid and hessian sparse grid,

24

name
Legendre
Hermite
Laguerre
generalized Laguerre
probabilistic Jacobi5
Chebyshev (1st kind) Chebyshev

orthonormal pdf
uniform
normal
exponential
gamma
beta

function
lege eval multidim
herm eval multidim
lagu eval multidim
generalized lagu eval multidim
jacobi prob eval multidim
cheb eval multidim

Table 10: Orthonormal polynomials, corresponding pdfs, and function name.

that respectively compute gradients and Hessians of the sparse grid approximation of a scalar-valued func-
tion f : RN → R by centered ﬁnite diﬀerences schemes. More speciﬁcally, derive sparse grid returns
the gradient of f evaluated at multiple points as a matrix, each column being the gradient in one point;
hessian sparse grid conversely computes the Hessian matrix at a single point. The step-size h of the
computation along each direction yk is defaulted to (b − a)/105, where a, b are the boundaries of Γ along
yk (or of the subset of Γk where derivatives are needed, in case of unbounded domains), but the user can
specify alternative values if needed.

4.4 Polynomial Chaos Expansion and Sobol indices computation

The sparse grid approximation of a function f is based on Lagrange interpolation polynomials, and hence
is a nodal approximation. Sometimes it is interesting to work with modal approximations instead, like the
generalized Polynomial Chaos Expansion (gPCE), i.e., an expansion of f over multi-variate ρ-orthonormal
polynomials:

(cid:88)

f ≈

cpPp(y),

p∈Λ

where Λ ⊂ NN is a multi-index set (note that here multi-indices can have entries with value zero), and
Pp = (cid:81)N
n=1 Ppn (yn) are products of N univariate ρn-orthonormal polynomials of degree pn. An example
of a situation when having the gPCE expansion of f is helpful is the computation of the Sobol indices for
global sensitivity analysis of f : an eﬃcient way to compute such Sobol indices is indeed to perform some
algebraic manipulations on the coeﬃcients of the gPCE, cp, see [13].
The Sparse Grids Matlab Kit actually provides functions to evaluate a number of diﬀerent multi-variate
orthonormal polynomials Pp: the list of available polynomials with the corresponding orthonormal pdf and
the associated function names are listed in Table 10.

Thus, a gPCE in Sparse Grids Matlab Kit is completely determined upon prescribing a multi-index set Λ
and the vector containing the corresponding expansion coeﬃcients:

• the multi-index set Λ can be constructed with the same procedures detailed in Sect. 3.3: the only
diﬀerence is that now the entries of each multi-index indicate a polynomial degree, therefore entries
with value zero are allowed. This can be obtained e.g. by means of the option base=0 in the function
multiidx gen.

1 N = 2;
2 rule = @(ii) sum(ii);
3 w = 3;
4 base = 0;
5 Lambda = multiidx gen(N,rule,w,base);

5These polynomials are slightly diﬀerent from the classical Jacobi polynomials: the former are orthonormal with respect to
the beta pdf, cf. Table 2, whereas the latter are orthonormal with respect to the weight function w(y) = (y − a)β (b − y)α –
note the switch in the role of the parameters α, β.

25

(a) Sparse grid multi-index set I

(b) PCE multi-index set Λ

Figure 12: Sparse grid multi-index set and corresponding PCE multi-index set.

• The coeﬃcients cp can be computed in several ways, e.g. by quadrature [41], least squares ﬁtting [4],
or compressed sensing approaches [17]. We detail below the default strategy provided by the Sparse
Grids Matlab Kit.

Once the multi-index set is deﬁned and gPCE coeﬃcients are available, the gPCE can be evaluated in a
straightforward way as follows:

1 PCE coeffs = [..]; % a vector of coefficients obtained e.g. by least square or quadrature
2 eval points = rand(N,10); % points where to evaluate the gPCE of f
3 PCE vals = zeros(1,length(eval points));
4 for i = 1:length(PCE coeffs)

PCE vals = PCE vals+PCE coeffs(i)*lege eval multidim(eval points,Lambda(i,:),0,1);
% 0,1 are shape param and indicate that the Legendre polynomials are defined over [0,1]ˆN

5

6
7 end

Coming to the computation of the gPCE coeﬃcients, the Sparse Grids Matlab Kit provides a function to
compute them given the evaluation of f over a sparse grid: such function is called convert to modal, and
supports transformation to all polynomials listed in Table 10. The algorithm that performs the conversion
is based on a change of basis from the Lagrange polynomials to a ρ-orthogonal basis, which is performed by
solving a Vandermonde-like linear system for each tensor grid in the sparse grid, see [13]. We remark that the
resulting gPCE approximation of f is actually identical to the original sparse grid approximation: it is the
same polynomial approximation of f , just expressed over a diﬀerent basis. The function convert to modal
returns a vector the gPCE coeﬃcients PCE coeffs and a matrix Lambda containing the multi-index set of
the gPCE:

1 f = @(y) exp(sum(y));
2 w = 3;
3 base = 1;
4 knots = @(n) knots CC(n,0,1);
5 lev2knots = @lev2knots doubling;
6 rule = @(ii) sum(ii-1);
7 I = multiidx gen(N,rule,w,base);
8 S = smolyak grid multiidx set(I,knots,lev2knots);
9 Sr = reduce sparse grid(S);
10 f on Sr = evaluate on sparse grid(f,Sr);
11 domain = [0 0; 1 1];
12 [PCE coeffs,Lambda] = convert to modal(S,Sr,f on Sr ,domain,'legendre');

Note that the multi-index set of the PCE, Λ, is completely determined by the choices of the level-to-knots
function, m, and of the multi-index set of the sparse grid, I, from which the conversion procedure begins,
see [13]; in Fig. 12 we plot both multi-index sets from the snippet above.

26

Finally, as already mentioned, a reason to perform the conversion to gPCE is to compute the indices of
the Sobol decomposition of f :
to this end, the Sparse Grids Matlab Kit provides a wrapper function
compute sobol indices from sparse grid which calls convert to modal and performs some algebraic
manipulations of the gPCE coeﬃcients of f to compute the Sobol indices of f (see [13]); we give an exam-
ple of its usage in Sect. 5.1. Another reason to perform the conversion to gPCE is to inspect the spectral
content of the sparse grid approximation, to verify how much the nodal representation is storing “redundant
information”, see e.g. [9].

5 Working example

The aim of this section is to give a comprehensive overview on the functionalities of the Sparse Grids Matlab
Kit for forward and inverse UQ problems. To this end we consider the following PDE:

(cid:40)

−∂x(a(x, y)∂xu(x, y)) = 1,
u(0, ·) = u(1, ·) = 0

for x ∈ (0, 1)

with diﬀusion coeﬃcient a(x, y) : [0, 1] × Γ → R, Γ = [−

√

√

3,

3]N of the following form

a(x, y) = µ +

N
(cid:88)

n=1

σn ynI[γn−1,γn](x),

where µ = 1, γn = n
with zero mean and unit variance.

N for 0 ≤ n ≤ N and yn are independent uniform random variables yn ∼ U(−

(18)

(19)

√

√

3,

3)

5.1 Forward UQ analysis

We focus ﬁrst on the forward UQ analysis, i.e. the study of the propagation of the randomness in the diﬀusion
coeﬃcient a(x, y) to a functional of the solution of Eq. (18), often called quantity of interest, that we denote
in the following by I(y).
In this example we consider as quantity of interest the spatial integral of the
solution u, i.e.

(cid:90) 1

I(y) =

u(x, y) dx,

(20)

and aim to estimate its expected value E[I] = (cid:82)
pdf.

Γ I(y)ρ(y) dy, its variance Var[I] = E[I 2] − E[I]2, and its

In the following we choose N = 2 (which allows plotting the sparse grid approximation as a surf plot), and
set the coeﬃcients σ1 and σ2 in Eq. (19) to 0.5 and 0.1, respectively.

0

1 mu = 1; %
2 sigma = [0.5, 0.1]; % sigma 1, sigma 2 in Eq. (19)
3 PDE rhs = @(x) ones(size(x)); % PDE right-hand side, cf. Eq. (18)

mean of the diffusion coefficient, cf. Eq. (19)

Both the estimation of the moments and of the pdf of I require evaluations of the quantity of interest at
the sparse grids points. It is then convenient to deﬁne a wrapper for the call to the PDE solver (we use a
piece-wise linear ﬁnite element solver using 200 elements, but of course any other solver can be used) and
the estimation of I.

1 N el = 200; % number of FEM elements
2 xx = linspace(0,1,N el+1); % discretization of the physical domain
3 I = @(y) QoI(xx,y,mu,sigma,PDE rhs);

27

To perform the UQ analysis we choose to work with Smolyak-type grids of Clenshaw–Curtis knots, since the
random variables yn are uniform. As the level of approximation w determines the accuracy of the sparse
grid approximation and quadrature, we test w = 2, . . . , 8 and compare the corresponding estimates of the
expected value with respect to a very accurate result obtained with w = 10 (see Fig. 13a).

1 N = 2;
2 knots = @(n) knots CC(n,-sqrt(3),sqrt(3));
3 lev2knots = @lev2knots doubling;
4 rule = @(ii) sum(ii-1);
5 % initializations for recycling
6 S old = []; Sr old = []; evals old = [];
7 for w = 2:8 % testing different values of w

S = smolyak grid(N,w,knots,lev2knots,rule,S old);
Sr = reduce sparse grid(S);
[exp I,evals] = quadrature on sparse grid(I,S,Sr,evals old,S old,Sr old);
S old = S; Sr old = Sr;

evals old = evals;

8

9

10

11
12 end

The plot shows that w = 4 gives already very accurate results in terms of expected value, and we expect
errors in the other quantities (variance, pdf) to be roughly of the same size, due to the “simple” structure
of the problem at hand. Hence, for the following numerical results we consider w = 4. The expected value
is estimated to

1 >> exp I
2 exp I =

3
4 0.0935

For the computation of the variance of I we proceed as follows

1 I on Sr = evaluate on sparse grid(I,Sr);
2 int I2 = quadrature on sparse grid(I on Sr.ˆ2,Sr); % computing E[Iˆ2]
3 var I = int I2-exp Iˆ2;

and obtain

1 >> var I
2 var I =

3
4 0.0010

Further, the sparse grid approximation of I (cf. Eq. (7)) is plotted in Fig. 13b.

1 domain = [-sqrt(3),-sqrt(3);sqrt(3),sqrt(3)];
2 plot sparse grids interpolant(S,Sr,domain,I on Sr) % we recall that this command embeds the
3 % call to interpolate on sparse grid that actually constructs the sparse grid approximation

Then, the estimate of the pdf of I can be done sampling such sparse grid approximation: we evaluate the
sparse grid approximation at M = 5000 uniformly distributed samples of Γ and estimate the pdf by means
of an histogram or a kernel density estimator. The results are reported in Fig. 13c.

1 M = 5000;
2 y samples = -sqrt(3)+2*sqrt(3)*rand(N,M); % traslate the uniform samples on [0,1]ˆ2
3
4 I vals rand = interpolate on sparse grid(S,Sr,I on Sr ,y samples);
5 H = histogram(I vals rand,'Normalization','pdf');
6 [pdf vals,pdf pts] = ksdensity(I vals rand);
7 plot(pdf pts,pdf vals,'LineWidth',2)

% generated by rand to [-sqrt(3),sqrt(3)]ˆ2

Finally, we check the principal and total Sobol indices of I. The principal Sobol indices are two values
between 0 and 1 which quantify the variability of I due to y1 and y2 alone, respectively. The total Sobol

28

(a) Error in the estimate of E[I]
for increasing values of w

(b) Sparse grid surface for w = 4

(c) pdf estimation by histogram and
kernel density estimation (curve in
red)

Figure 13: Forward UQ. Sparse grid approximation of I.

indices quantify instead the variability of I due to each random variable considering both its individual
contributions as well as its contributions in combination with any other variable, and are again numbers
between 0 and 1 (note that, on the contrary, their sum can be larger than 1). In other words, the ﬁrst total
Sobol index quantiﬁes the variability of I due to y1 alone and combined with y2, and vice versa for the
second one.

1 domain = [-sqrt(3) -sqrt(3);sqrt(3) sqrt(3)];
2 [Sob idx, Tot Sob idx]=compute sobol indices from sparse grid(S,Sr,I on Sr ,domain,'legendre');

They result to be

1 >> Sob idx
2 Sob idx =

3

4

5

0.9709
0.0244

6
7 >> Tot Sob idx
8 Tot Sob idx =

9

10

11

0.9756
0.0291

and clearly show that the ﬁrst random variable is more inﬂuential on I than the second one. This could
also be inferred from the plot of the sparse grid approximation in Fig. 13b, where we can observe that the
variability of I with respect to y1 is larger than with respect to y2.

5.2 Inverse UQ analysis

In this section we address inverse UQ analysis, i.e. “adjusting” the pdf of the stochastic input y given a set
of noisy measurements (data) of the solution of Eq. (18), or of functionals thereof. Such pdf is usually called
data-informed or posterior pdf, ρpost, and can be derived using the Bayes theorem, which can be informally
stated as

pdf(y given data) = pdf(data given y) × pdf(y) ×

,

(21)

1
pdf(data)

where “pdf(y given data)” is the posterior pdf ρpost that we aim at computing, “pdf(data given y)” is
the so-called likelihood function, which is denoted in the following by L(y), and “pdf(y)” is the pdf of the
variables based only on a-priori information on the parameter, denoted by ρprior. The “pdf(data)” can be

29

simply considered to be the normalization constant such that the posterior pdf is actually a pdf (i.e., its
integral is equal to 1). Thus, we can write in more formal terms that

ρpost(y) ∝ L(y)ρprior(y).

(22)

Then, let us consider as data a set of values (cid:101)uk, k = 1, . . . , K of the solution of the PDE in Eq. (18) at
diﬀerent measurement points x1, x2, . . . , xK, for an unknown value of the inputs y∗. Moreover, we assume
that these observations are corrupted by additive random errors ε1, . . . , εK (noise), which we assume for
simplicity to be independent, identically distributed normal random variables with variance σ2

ε , i.e.

(cid:40)

(cid:101)uk = u(xk, y∗) + εk,
εk ∼ N (0, σ2

ε ).

k = 1, . . . , K

(23)

We also introduce the misﬁts between such data and the values obtained by the PDE model in Eq. (18) for
any value of y, i.e.

Mk(y) := (cid:101)uk − u(xk, y),

k = 1, . . . , K.

The next step for the computation of the posterior pdf of y is to write a computable expression for the
likelihood function L(y) in Eq. (22). Due to the assumptions on the measurement errors εk, we can write

L(y) =

K
(cid:89)

k=1

1
(cid:112)2πσ2

ε

( (cid:101)uk −u(xk ,y))2
2σ2
ε

−

e

=

K
(cid:89)

k=1

1
(cid:112)2πσ2

ε

M2

k (y)
2σ2
ε

.

−

e

Finally, we make the quite strong (but often reasonable) assumption that the posterior pdf ρpost can be
well approximated by a normal distribution. This assumption is reasonable in presence of numerous data
with small measurement noise, and when the observation functional (in this case the evaluation of u at
xk) is adequately sensitive to the input parameters, such that the actual posterior is unimodal and well
peaked. In case any of these requirements is not valid, a more appropriate approach would be to apply e.g.
Markov-Chain Monte Carlo methods, which are agnostic with respect to the shape of ρpost, see e.g.
[32]
and references therein. Under normality assumption of the posterior, we only have to choose its mean and
covariance matrix, see again e.g. [32]:

• The mean can be taken as the mode of the posterior pdf (i.e., its maximum), which we denote as yMAP
(Maximum A-Posteriori); note that yMAP can be interpreted as a reasonable approximation of the true
value of y that generated the data, i.e., y∗ ≈ yMAP, cf. Eq. (23).
From a computational point of view, it is convenient to compute yMAP by taking the negative logarithm
of Eq. (22) and computing the minimum of such quantity. Moreover, since we assumed a uniform prior
pdf for y (see Eq. (19)), the operation just described ﬁnally amounts to computing the minimum of
the so-called negative log-likelihood function (NLL):

NLL(y) := − log (L(y)) =

K
(cid:88)

k=1
yMAP := argminy∈Γ NLL(y).

M2

k(y)
2σ2
ε

+ K log σ2

ε + K log

√

2π,

(24)

(25)

Note that this minimization is actually independent of σ2
least-squares approach for calibrating y, i.e. to the minimization of the sum of the squared errors:

ε , and it is in practice equivalent to the classical

LS(y) =

K
(cid:88)

k=1

M2

k(y),

yMAP = argminy∈Γ LS(y).

30

(26)

(27)

An approximation of σ2
sample variance estimator:

ε can actually be recovered after having determined yMAP, by using the standard

σ2
ε ≈

1
K

K
(cid:88)

k=1

M2

k(yMAP).

We remark that, in case the measurement errors are not assumed independent, additive normal random
variables with constant variance, the expression of the NLL is diﬀerent from the one in Eq. (24) and
its minimization does would not result in the classic least-squares method.

• The covariance matrix Σpost can be taken as the inverse of the Hessian of the NLL at yMAP. A

convenient approximation for Σpost is

Σpost ≈ σ2

ε (JyMTJyM)−1,

where JyM is the Jacobian matrix of M1, . . . , MK with respect to y, i.e. (JyM)k,(cid:96) = ∂Mk
∂y(cid:96)

.

For the numerical tests reported in this section we ﬁx σ1 = σ2 = 0.5 in Eq. (19). We consider a FEM
discretization with K + 2 = 82 knots, and generate a set of K synthetic data (one for each internal knot of
the mesh), i.e., we use Eq. (23) with xk = k 1

K+1 , k = 1, . . . , K, y∗ = [0.9, −1.1] and σε = 0.01.

1 K = 80;
2 x k = linspace(0,1,K+2);
3 sigma = [0.5, 0.5];
4 y star = [0.9,-1.1];
5 sigma eps = 0.01;
6 u star = PDE solver(x k,y star,mu,sigma,PDE rhs);
7 u star = u star(2:end-1); % select the values at the internal knots of the mesh
8 u tilde = u star + sigma eps*randn(K,1);

The data thus generated are plotted in Fig. 14a. The negative log-likelihood function is deﬁned as

1 u = @(y) PDE solver(x k,y,mu,sigma,PDE rhs);
2 misfits = @(y) [0;u tilde;0] - u(y); % misfits are computed also at the boundary to keep

3
4 NLL = @(y) sum(misfit(y).ˆ2/(2*sigma epsˆ2))+(K-2)*log(sigma epsˆ2)+(K-2)*0.5*log(2*pi);

% the code compact, but their contribution is zero

A plot of NLL a neighborhood of y∗ is given in Fig. 14b-c. Its isolines are approximately ellipses, which
indicates that the normal approximation of ρpost is appropriate in this case. Note that in realistic scenarios,
where we deal with real data instead of synthetic ones, at this point one would not be able to plot the NLL,
since σε is in general unknown. The shape of the isolines of the NLL however does not depend on σε, and a
plot of the least squares functional of Eq. (26) would convey the same information.
The minimization in Eqs. (25) or (27) can be computationally expensive, especially when sampling the values
u(·, y) calls a complex model (this is actually not the case for our current example, but we illustrate this
step nonetheless for the sake of generality). Hence, the exact quantity of interest u can be replaced in Eq.
(24) by its sparse grid approximation U, which is in general cheaper to evaluate. We consider here the sparse
grid built in Sect. 5.1 (third snippet) corresponding to w = 5 and proceed as follows

1 u on Sr = evaluate on sparse grid(u,Sr); % u on Sr has dimension (K-2) x Sr.size:

2
3 u approx = @(y) interpolate on sparse grid(S,Sr,u on Sr ,y);
4 % note that interpolate on sparse grids supports vector-valued functions,
5 % so in one call we get the values of u at all nodes x i
6 LS = @(y) sum(misfits(y).ˆ2);

% the ith row contains u(x i,Sr.knots)

For the minimization we resort to the Matlab function fminsearch

1 % initial guess of the minimization: center of Gamma
2 y start = [0; 0];
3 y MAP = fminsearch(LS,y start);
4 sigma eps approx = sqrt(mean(misfits(y MAP).ˆ2));

31

(a) Data and “true” values

(b) Contour of the NLL in a neigh-
borhood of y∗

(c) NLL in a neighborhood of y∗

Figure 14: Inverse UQ.

which returns

1 >> y MAP
2 y MAP =

3

4

5

0.8787
-1.0581

6
7 >> sigma eps approx
8 sigma eps approx =

9

10

0.0084

which are reasonably good approximations of y∗ and σ2
ε , as expected. Now, for the computation of an
approximation of the covariance matrix we need the Jacobian matrix of the misﬁts. Each row of the Jacobian
matrix can also be approximated by sparse grids techniques, replacing full model evaluations by sparse grid
approximations in a ﬁnite diﬀerences scheme. This is performed by the function derive sparse grid as
shown in the following

1 Jac at MAP = zeros(K-2,N);
2 domain = [-sqrt(3), -sqrt(3); sqrt(3), sqrt(3)];
3 for i = 1:(K-2)

Jac at MAP(i,:) = derive sparse grid(S,Sr,u on Sr(i,:),domain,y MAP)';

4
5 end
6 Sigma post = sigma eps approxˆ2*inv(Jac at MAP'*Jac at MAP);

which returns

1 >> Sigma post
2
3 Sigma post =

4

5

6

0.0036
-0.0013

-0.0013

0.0009

The resulting normal approximation of the posterior distribution of y is plotted in Fig. 15.

Finally, having the posterior distribution of y at hand, we can perform the forward UQ analysis of the
quantity of interest I deﬁned in Eq. (20) based on such pdf. The procedure is analoguous to what done in
Sect. 5.1, but the sparse grid generation needs to be further discussed due to the fact that this time y1 and
y2 are no longer independent. A possible solution is to introduce the transformation

y = yMAP + H Tz

32

(28)

(a) Contour

(b) Surface

Figure 15: Inverse UQ. Gaussian approximation of the posterior distribution of y.

(a) Prior-based pdf

(b) Posterior-based pdf

Figure 16: Inverse UQ. Prior and data-informed pdf of I. Note the diﬀerence in the support of the two pdfs.

with H being the Choleski factor of Σpost (i.e. H TH = Σpost) and z ∼ N (0, I), which allows to move from
independent variables z1, z2 to y1, y2 dependent random variables. Hence, we ﬁrst construct a sparse grid
according to the distribution of z, for example

1 N = 2;
2 w = 4;
3 knots = @(n) knots normal(n,0,1);
4 rule = @(ii) sum(ii-1);
5 lev2knots = @lev2knots lin;
6 S = smolyak grid(N,w,knots,lev2knots,rule);
7 Sr = reduce sparse grid(S);

and then apply the change of variables in Eq. (28) to the sparse grid knots before calling the PDE solver:

1 H = chol(Sigma post);
2 I = @(z) QoI(xx,H'*z+y MAP,mu,sigma,PDE rhs); % moving from z to y
3 I on Sr = evaluate on sparse grid(I,Sr);

Similarly, for the estimation of the distribution of I we have to draw samples according to the distribution
of z. This time we do not need to apply the change of variables in Eq. (28), since we are not calling the
solver, but using its evaluations just obtained, which already took into account the change of variables:

1 M = 5000;

33

2 z samples = randn(N,M);
3 I vals randn = interpolate on sparse grid(S,Sr,I on Sr ,z samples);
4 [pdf vals,pdf pts] = ksdensity(I vals randn);

In Fig. 16 we compare the posterior-based pdf and prior-based of I. We can immediately observe that the
variability of I is strongly reduced in the posterior-based case.

6 Conclusions

In this manuscript we have showcased the use of the Sparse Grids Matlab Kit for approximation of high-
dimensional functions. The package implements the combination technique form of the sparse grids, it is
written in Matlab, and its compatibility has been tested with Octave. It is high-level, and hopefully easy
it was indeed conceived with more emphasis on usability (which makes it easy to use for quick
to use:
prototyping and didactic purposes) than on speed of execution. However, it features some functionalities
that makes it reasonably usable for moderate-sized problems (say up to N ≈ 20), although it has been used
also for problems with hundreds of random variables, such as in [28] and [9]. Among such functionalities, the
interface with the Matlab Parallel Toolbox to speed up the evaluation of the function at hand on multiple
grid points, and recycling of computations to build sequences of grids and to evaluate the function over such
sequences.

In particular, we have covered the two ways to create sparse grids supported by the Sparse Grids Matlab
Kit (a-priori and adaptive a-posteriori ), and detailed the available alternatives concerning the “building
blocks” of sparse grids, i.e., univariate knots, level-to-knots functions, functions to build multi-index sets
(for the a-priori grids) and proﬁt deﬁnitions (for the adaptive a-posteriori grids). We then have discussed the
main functionalities made available by the Sparse Grids Matlab Kit (quadrature, interpolation, evaluation,
plotting, computing derivatives, conversion to PCE) and ﬁnally showcased the usage of the Sparse Grids
Matlab Kit over a simple (yet illustrative) problem of forward/inverse UQ for PDEs, showing how a handful
of high-level lines of Sparse Grids Matlab Kit code can be enough to perform several common tasks in UQ.

Of course, the Sparse Grids Matlab Kit is being continuously expanded with new functionalities. The next
ones in the working pipeline are adding support for computation of Morris indices for sensitivity analysis
[24], and a module for multi-ﬁdelity UQ analysis based on the Multi-Index Stochastic Collocation algorithm
discussed e.g.
in [18, 31]. A Python version of the code will also be released in the near future, and we
encourage interested users to check periodically the website, or to get in touch with us for more information
on this point.

Ackowledgment

Lorenzo Tamellini and Chiara Piazzola have been supported by the PRIN 2017 project 201752HKH8 “Nu-
merical Analysis for Full and Reduced Order Methods for the eﬃcient and accurate solution of complex
systems governed by Partial Diﬀerential Equations (NA-FROM-PDEs)”.

The authors gratefully acknowledge several persons who contributed to the development of the package either
by providing implementation for some functions or by using the software and reporting success cases, bugs and
missing features. In particular: Fabio Nobile (early version of the code and continued support throughout
the development of the project), Alessandra Sordi and Maria Luisa Viticchi`e (early contributions to the
code), Eva Vidliˇckov´a and Michele Pisaroni (Python implementation), Francesco Tesei and Diane Guignard
(adaptive sparse grids), Giovanni Porta (Sobol indices and conversion to PCE), Bj¨orn Sprungk (adaptive
sparse grids and weighted Leja knots), Francesca Bonizzoni (compatibility with Octave).

34

References

[1] B. Adams, W. Bohnhoﬀ, K. Dalbey, M. Ebeida, J. Eddy, M. Eldred, R. Hooper, P. Hough, K. Hu,
J. Jakeman, M. Khalil, K. Maupin, J. Monschke, E. Ridgway, A. Rushdi, D. Seidl, J. Stephens, L. Swiler,
and J. Winokur. Dakota, a multilevel parallel object-oriented framework for design optimization, pa-
rameter estimation, uncertainty quantiﬁcation, and sensitivity analysis: Version 6.15 user’s manual.
Technical report, Sandia National Laboratiories, November 2021.

[2] K. I. Babenko. Approximation by trigonometric polynomials in a certain class of periodic functions of

several variables. Soviet Math. Dokl., 1:672–675, 1960.

[3] J. B¨ack, F. Nobile, L. Tamellini, and R. Tempone. Stochastic spectral Galerkin and collocation methods
for PDEs with random coeﬃcients: a numerical comparison. In Spectral and High Order Methods for
Partial Diﬀerential Equations, volume 76 of Lecture Notes in Computational Science and Engineering,
pages 43–62. Springer, 2011.

[4] B. Blatman, G. Sudret. Adaptive sparse polynomial chaos expansion based on least angle regression.

Journal of Computational Physics, 230(6):2345 – 2367, 2011.

[5] H. Bungartz and M. Griebel. Sparse grids. Acta Numer., 13:147–269, 2004.

[6] A. Chkifa, A. Cohen, and C. Schwab. High-dimensional adaptive sparse polynomial interpolation and
applications to parametric PDEs. Foundations of Computational Mathematics, 14(4):601–633, 2014.

[7] B. Debusschere, K. Sargsyan, C. Safta, and K. Chowdhary. Uncertainty Quantiﬁcation Toolkit (UQTk).
In R. Ghanem, D. Higdon, and H. Owhadi, editors, Handbook of Uncertainty Quantiﬁcation, pages
1807–1827. Springer International Publishing, Cham, 2017.

[8] B. J. Debusschere, H. N. Najm, P. P. P´ebay, O. M. Knio, R. G. Ghanem, and O. P. Le Maˆıtre. Numerical
Challenges in the Use of Polynomial Chaos Representations for Stochastic Processes. SIAM Journal on
Scientiﬁc Computing, 26(2):698–719, 2004.

[9] O. G. Ernst, B. Sprungk, and L. Tamellini. Convergence of Sparse Collocation for Functions of Countably
Many Gaussian Random Variables (with Application to Lognormal Elliptic Diﬀusion Problems). SIAM
Journal on Numerical Analysis, 56(2):877–905, 2018.

[10] O. G. Ernst, B. Sprungk, and L. Tamellini. On expansions and nodes for sparse grid collocation of
lognormal elliptic pdes. Arxiv e-prints, (1906.01252), 2019. To appear on the Springer book “Sparse
grids and application - Munich 2018” proceedings.

[11] J. Feinberg, V. G. Eck, and H. P. Langtangen. Multivariate polynomial chaos expansions with dependent

variables. SIAM Journal on Scientiﬁc Computing, 40:199––223, 2018.

[12] J. Feinberg and H. P. Langtangen. Chaospy: an open source tool for designing methods of uncertainty

quantiﬁcation. Journal of Computational Science, 11:46–57, 2015.

[13] L. Formaggia, A. Guadagnini, I. Imperiali, V. Lever, G. Porta, M. Riva, A. Scotti, and L. Tamellini.
Global sensitivity analysis through polynomial chaos expansion of a basin-scale geochemical compaction
model. Computational Geosciences, 17(1):25–42, 2013.

[14] W. Gautschi. Orthogonal Polynomials: Computation and Approximation. Oxford University Press,

Oxford, 2004.

[15] A. Genz and B. D. Keister. Fully symmetric interpolatory rules for multiple integrals over inﬁnite

regions with Gaussian weight. J. Comput. Appl. Math., 71(2):299–309, 1996.

35

[16] T. Gerstner and M. Griebel. Dimension-adaptive tensor-product quadrature. Computing, 71(1):65–87,

2003.

[17] J. Hampton and A. Doostan. Compressive sampling of polynomial chaos expansions: Convergence

analysis and sampling strategies. Journal of Computational Physics, 280:363 – 386, 2015.

[18] J. D. Jakeman, M. S. Eldred, G. Geraci, and A. Gorodetsky. Adaptive multi-index collocation for
International Journal for Numerical Methods in

uncertainty quantiﬁcation and sensitivity analysis.
Engineering, 2019.

[19] A. Klimke. Uncertainty modeling using fuzzy arithmetic and sparse grids. PhD thesis, Universit¨at

Stuttgart, Shaker Verlag, Aachen, 2006.

[20] A. Klimke and B. Wohlmuth. Algorithm 847: Spinterp: Piecewise multilinear hierarchical sparse grid

interpolation in matlab. ACM Trans. Math. Softw., 31(4):561–579, dec 2005.

[21] S. D. Marchi. On Leja sequences: some results and applications. Appl. Math. Comput., 152:621–647,

2004.

[22] S. Marelli and B. Sudret. UQLab: A Framework for Uncertainty Quantiﬁcation in Matlab. In M. Beer,
S.-K. Au, and J. W. Hall, editors, Vulnerability, Uncertainty, and Risk, pages 2554–2563. American
Society of Civil Engineers, 2014.

[23] J. Mart´ınez-Frutos and F. Periago. Optimal Control of PDEs under Uncertainty: An introduction with

application to optimal shape design of structures. Springer International Publishing, 2018.

[24] M. D. Morris. Factorial sampling plans for preliminary computational experiments. Technometrics,

33(2):161–174, 1991.

[25] A. Narayan and J. D. Jakeman. Adaptive Leja Sparse Grid Constructions for Stochastic Collocation
and High-Dimensional Approximation. SIAM Journal on Scientiﬁc Computing, 36(6):A2952–A2983,
2014.

[26] F. Nobile, L. Tamellini, and R. Tempone. Comparison of Clenshaw–Curtis and Leja Quasi-Optimal
Sparse Grids for the Approximation of Random PDEs. In R. M. Kirby, M. Berzins, and J. S. Hesthaven,
editors, Spectral and High Order Methods for Partial Diﬀerential Equations - ICOSAHOM ’14, volume
106 of Lecture Notes in Computational Science and Engineering, pages 475–482. Springer International
Publishing, 2015.

[27] F. Nobile, L. Tamellini, and R. Tempone. Convergence of quasi-optimal sparse-grid approximation
of Hilbert-space-valued functions: application to random elliptic PDEs. Numerische Mathematik,
134(2):343–388, 2016.

[28] F. Nobile, L. Tamellini, F. Tesei, and R. Tempone. An adaptive sparse grid algorithm for elliptic
PDEs with lognormal diﬀusion coeﬃcient.
In J. Garcke and D. Pﬂ¨uger, editors, Sparse Grids and
Applications – Stuttgart 2014, volume 109 of Lecture Notes in Computational Science and Engineering,
pages 191–220. Springer International Publishing Switzerland, 2016.

[29] M. Parno, A. Davis, L. Seelinger, and Y. Marzouk. Mit uncertainty quantiﬁcation (MUQ) library, 2014.

[30] D. Pﬂ¨uger. Spatially Adaptive Sparse Grids for High-Dimensional Problems. Verlag Dr. Hut, 2010.

[31] C. Piazzola, L. Tamellini, R. Pellegrini, R. Broglia, A. Serani, and M. Diez. Comparing Multi-Index
Stochastic Collocation and Multi-Fidelity Stochastic Radial Basis Functions for Forward Uncertainty
Quantiﬁcation of Ship Resistance. Engineering with Computers, 2022.

36

[32] C. Piazzola, L. Tamellini, and R. Tempone. A note on tools for prediction under uncertainty and
identiﬁability of SIR-like dynamical systems for epidemiology. Mathematical Biosciences, 332:108514,
2021.

[33] A. Quarteroni, R. Sacco, and F. Saleri. Numerical mathematics, volume 37 of Texts in Applied Mathe-

matics. Springer-Verlag, Berlin, second edition, 2007.

[34] C. Schillings and C. Schwab. Sparse, adaptive Smolyak quadratures for Bayesian inverse problems.

Inverse Problems, 29(6), 2013.

[35] J. Shen and L.-L. Wang. Sparse spectral approximations of high-dimensional problems based on hyper-

bolic cross. SIAM J. Numer. Anal., 48(3):1087–1109, 2010.

[36] M. Stoyanov. User manual: Tasmanian sparse grids. Technical Report ORNL/TM-2015/596, Oak Ridge

National Laboratory, One Bethel Valley Road, Oak Ridge, TN, 2015.

[37] M. Stoyanov. Adaptive sparse grid construction in a context of local anisotropy and multiple hierarchical

parents. In Sparse Grids and Applications-Miami 2016, pages 175–199. Springer, 2018.

[38] M. K. Stoyanov and C. G. Webster. A dynamically adaptive sparse grids method for quasi-optimal
interpolation of multidimensional functions. Computers & Mathematics with Applications, 71(11):2449–
2465, 2016.

[39] L. N. Trefethen. Is Gauss quadrature better than Clenshaw-Curtis? SIAM Rev., 50(1):67–87, 2008.

[40] G. Wasilkowski and H. Wozniakowski. Explicit cost bounds of algorithms for multivariate tensor product

problems. Journal of Complexity, 11(1):1–56, 1995.

[41] D. Xiu. Eﬃcient collocational approach for parametric uncertainty analysis. Communications in Com-

putational Physics, 2(2):293–309, 2007.

37

