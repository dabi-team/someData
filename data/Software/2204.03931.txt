1

2
2
0
2

r
p
A
8

]
E
S
.
s
c
[

1
v
1
3
9
3
0
.
4
0
2
2
:
v
i
X
r
a

HINNPerf: Hierarchical Interaction Neural Network for
Performance Prediction of Configurable Systems

JIEZHU CHENG, Sun Yat-sen University, China
CUIYUN GAO, Harbin Institute of Technolgy, Shenzhen, China
ZIBIN ZHENGâˆ—, Sun Yat-sen University, China

Modern software systems are usually highly configurable, providing users with customized functionality
through various configuration options. Understanding how system performance varies with different option
combinations is important to determine optimal configurations that meet specific requirements. Due to
the complex interactions among multiple options and the high cost of performance measurement under
a huge configuration space, it is challenging to study how different configurations influence the system
performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural
network for performance prediction of configurable systems. HINNPerf employs the embedding method and
hierarchic network blocks to model the complicated interplay between configuration options, which improves
the prediction accuracy of the method. Besides, we devise a hierarchical regularization strategy to enhance the
model robustness. Empirical results on 10 real-world configurable systems show that our method statistically
significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction
accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture
provides some insights about the interaction complexity and the significance of configuration options, which
might help users and developers better understand how the configurable system works and efficiently identify
significant options affecting the performance.

CCS Concepts: â€¢ Software and its engineering â†’ Software performance.
Additional Key Words and Phrases: Software performance prediction, highly configurable systems, deep neural
network, machine learning

ACM Reference Format:
Jiezhu Cheng, Cuiyun Gao, and Zibin Zheng. 2022. HINNPerf: Hierarchical Interaction Neural Network for
Performance Prediction of Configurable Systems. ACM Trans. Softw. Eng. Methodol. 1, 1, Article 1 (January 2022),
30 pages. https://doi.org/10.1145/3528100

1 INTRODUCTION
Modern configurable software systems offer customized services to users through a set of configu-
ration options. By specifying a combination of these options (i.e., a configuration), one can tailor
the systemâ€™s behavior to meet specific functional requirements. Meanwhile, the non-functional
properties of the system may be significantly influenced by different configuration selections.

âˆ—Corresponding author

Authorsâ€™ addresses: Jiezhu Cheng, Sun Yat-sen University, No. 132, East of Outer Ring Road, Guangzhou Higher Education
Mega Center, Panyu District, Guangzhou, Guangdong Province, China, 510006, chengjzh@mail2.sysu.edu.cn; Cuiyun Gao,
Harbin Institute of Technolgy, Shenzhen, Xili University Town, Nanshan District, Shenzhen, Guangdong Province, China,
518071, gaocuiyun@hit.edu.cn; Zibin Zheng, Sun Yat-sen University, No. 132, East of Outer Ring Road, Guangzhou Higher
Education Mega Center, Panyu District, Guangzhou, Guangdong Province, China, 510006, zhzibin@mail.sysu.edu.cn.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â© 2022 Association for Computing Machinery.
1049-331X/2022/1-ART1 $15.00
https://doi.org/10.1145/3528100

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

 
 
 
 
 
 
1:2

J. Cheng et al.

Performance (such as response time and throughput) is one of the most important non-functional
properties as it directly affects user experience and cost [12, 13]. It is necessary for both users
and developers to efficiently identify the performance-optimal configurations when deploying
and testing the software system. However, it is infeasible to exhaustively measure the system
performance under all possible configurations, since the combinatorial explosion of configuration
options results in an exponential number of configurations [12, 13, 42].

Recently, researchers have devised various machine learning methods to predict system per-
formance under any certain configuration [11â€“14, 42â€“44, 52]. In this way, a performance model is
constructed to ease understanding, debugging, and optimization of highly configurable software
systems. For example, the performance model can help a user find the best performing configura-
tion under specific functional constraints. And a developer may compare the performance model
with his own mental model to check whether the software behaves as expected, and improve the
software based on the comparison [42]. Compared to the huge time cost of measuring the system
performance by executing a complex benchmark, a performance model can predict the performance
under a certain configuration within a few seconds, which significanlty reduces software testing
cost.

To build a performance model, the scalar performance values of a set of configurations are re-
quired to be first measured. The configurations and their corresponding performance measurements
compose a sample for training a model which later can be used for predicting the performance
values of new configurations. Most importantly, prediction accuracy determines whether a perfor-
mance model can really help users and developers improve software quality, rather than causing
unnecessary misleading. This is why most of prior work have focused on improving the accuracy
of performance models [11â€“14, 42â€“44, 52]. Nevertheless, learning a performance model with high
accuracy is very challenging because of the following aspects:

(1) Complex interactions among features: Configurable systems typically have multiple
binary and/or numeric configuration options (also called features). The interactions among
different features can be non-linear, multi-way and hierarchical [43, 45]. Failing to learn the
interactions that contribute substantially to software performance causes significant drop on
prediction accuracy.

(2) Small sample: Typically, measuring the performance of a configuration requires executing
a realistic workload on the whole system, which is costly and time-consuming [33]. Hence,
only a limited set of configurations can be measured as a training sample in practice [12]. A
small sample could cause the performance model to easily overfit (i.e., the model performs
well on the training data but badly during testing).

Existing methods tackle the above challenges in varied ways. For interaction modeling, SPLCon-
queror [42â€“44] learns the influences of individual configuration options (features) and their interplay
on performance by multiple linear regression. However, the method only models the interactions
with linear combinations of low-order functions (e.g., linear, quadratic and logarithmic), limiting its
flexibility in recognizing complex non-linear interactions. Further, CART/DECART [11, 12] employs
classification and regression trees to build a more flexible non-linear performance model. Fourier
learning algorithms [14, 52] transform the regression problem into its Fourier form and predict the
performance by estimating the Fourier coefficients. The main disadvantage of CART/DECART and
Fourier methods is that they can only learn the interactions among binary configuration options.
DeepPerf [13] introduces a deep feedforward neural network (FNN) to overcome the shortcomings
of previous methods. Nonetheless, a deep FNN can only model the feature interactions implicitly
through multiple hidden layers, which probably limits its flexibility and accuracy. For example,
some hierarchical configuration interactions prone to affecting the system performance are hard to

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:3

be engaged to DeepPerf, since the interactions between different layers of FNN is opaque [3]. On the
other hand, to mitigate the limitation of the small sample, both SPLConqueror [42] and DECART [12]
incorporate several sampling heuristics to select a set of representative configurations for training.
However, it takes extra time and effort to determine the appropriate sampling strategy for each
system since there is no universal optimal sampling heuristics [10]. Instead, PerLasso [14] and
DeepPerf [13] focus on restricting the performance model to be sparse with ğ¿1 regularization. This
strategy takes the risk of making the model highly sensitive to the regularization hyperparameter,
that is, small changes in hyperparameter may cause large fluctuations in model accuracy. In this
case, the regularization hyperparameter must be carefully tuned in a large search space, which
increases model training cost.

To better address the aforementioned challenges, we propose a novel Hierarchical Interaction
Neural Network for Performance Prediction and name it HINNPerf. Similar to DeepPerf [13], our
method can model all types of complex interactions (i.e., binary, numeric, and binary-numeric
interactions) in various configurable systems. Differently, inspired by previous work [42, 43, 45]
revealing that interactions among configuration options are hierarchical, we decompose the deep
neural network architecture into multiple blocks connected hierarchically, where lower (higher)
blocks learn the influences of lower-order (higher-order) interactions on system performance.
Within each block, similarly to embedding methods [4, 19, 51] widely used in deep learning world,
we employ an FNN to embed the feature interactions into a vector. All the blocks link to each
other by the vector concatenation. Also, each block predicts a partial performance value and the
final prediction is the sum of all partial predictions. In this way, it is much easier for HINNPerf to
learn the influences of individual features and their interactions on performance via a deeper and
more expressive architecture1. In addition, based on the sparsity prior of the software performance
functions [13, 42, 43], we engage the ğ¿1 regularization [48] for the first FNN layer of each block to
address the challenge of small sample. We empirically find that such hierarchical regularization
architecture is more robust to the regularization hyperparameter and reduces hyperparameter
tuning cost. As for practicality, on the one hand, the hierarchical structure of HINNPerf enables it to
provide some insights on configuration coupling of the system, which can help users and developers
understand which kind of interactions (lower- or higher-order) has the major influence on the
system performance. On the other hand, combined with the Integrated Gradients [47] method,
HINNPerf recognizes the significant configuration options in different interaction orders and reveals
some potential interaction patterns for the system, presenting more actionable information for
users and developers. Finally, it can be demonstrated that the DeepPerf [13] model is simply a
special case of our HINNPerf architecture. From this perspective, our method provides a general
deep learning framework for performance prediction of configurable systems.

We evaluate our method on 10 real-world configurable software systems including compilers, web
servers, video encoders, etc. The experimental results show that HINNPerf outperforms the state-
of-the-art methods on most systems. Remarkably, when compared to the advanced deep learning
method DeepPerf [13], HINNPerf achieves statistically significant improvements on binary-numeric
systems (i.e., the systems with both binary and numeric configuration options), demonstrating its
strength in capturing complex feature interactions.
In summary, our contributions are of four-folds:
â€¢ We introduce the embedding method and propose a novel deep neural network architecture to
model the hierarchical interactions among configuration options and predict the performance
of configurable software systems. To our knowledge, although the embedding method has

1Here we refer the expressive power of a method to the capability of learning various functions [39]. More expressive
methods can learn more complex functions.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:4

J. Cheng et al.

achieved great success in various deep learning tasks [4, 19, 51, 56], few literatures have
employed it to model the feature interactions of configurable systems. And we aim to bridge
the gap in this work.

â€¢ We devise a hierarchical regularization strategy to help the ğ¿1 regularization address the
small sample challenge more efficiently, with the advantages of improving model robustness
to the regularization hyperparameter and reducing hyperparameter tuning cost.

â€¢ We conduct extensive experiments on 10 real-world configurable software systems with
various sample sizes and show the advantages of our method against most state-of-the-art
baseline approaches, demonstrating new benchmark on the public datasets. Our experimental
data and source code are publicly available at the Google Drive2.

â€¢ In addition to accuracy improvement, our method provides some insights on the complexity
of configuration coupling and identifies the significant configuration options in different
interaction orders by automatic learning, which could help users and developers better
understand how the configurable system works.

2 BACKGROUND AND MOTIVATION

2.1 Problem Formulation
Table 1 illustrates a performance example of a configurable software system. The system has 14
configuration options with 11 binary options and 3 numeric options. Each row of the table repre-
sents one configuration and its performance measurement that can be response time, throughput,
workload and so on. Evidently, different configurations usually lead to different performance values.
Our main task is to predict the performance value of a new configuration that is not measured
(e.g., the value ? in the last row of Table 1), based on the observations of previously measured
configurations.

Formally, we aim to learn a function that maps a configuration with ğ‘› options o = [ğ‘œ1, ğ‘œ2, ..., ğ‘œğ‘›]âŠ¤

to its performance value:

(1)
ğ‘ = ğ‘“ (o) = ğ‘“ (ğ‘œ1, ğ‘œ2, ..., ğ‘œğ‘›),
where ğ‘“ : X â†’ R is the performance function and X is the Cartesian product of the domains of all
the configuration options. Naively, we can derive a precise performance function by exhaustively
measuring the performance of every valid configuration of the software system, which is infeasible
in practice because of the exponentially growing configuration space [12, 13, 42, 49]. Therefore, the
objective is to design a performance model for precisely approximating the performance function
using only a small sample.

Table 1. Performance values of different configurations

Configuration Options
ğ‘œ13
ğ‘œ2
1
1
5
1
2
1
.
.
5
1
6
1
3
1

ğ‘œ12
64
64
256
.
4096
4096
256

ğ‘œ3
1
0
0
.
0
0
0

...
...
...
...
...
...
...
...

Performance
ğ‘
101.754
567.883
219.512
.
399.705
643.311
?

ğ‘œ14
0
6
3
.
4
6
3

ğ‘œ1
0
0
1
.
0
0
0

2https://drive.google.com/drive/folders/1qxYzd5Om0HE1rK0syYQsTPhTQEBjghLh?usp=sharing

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:5

2.2 Hierarchical Interactions
A large number of configuration options with different types (binary and numeric) can generate
complex interactions that affect configurable system performance in different ways. In theory, any
combination of the configuration options may cause a distinct interaction pattern [43]. Fortunately,
researchers have found that relevant interactions form a hierarchical relationship [42, 43, 45]. That
is, higher-order interactions usually build on lower-order interactions. As is pointed out by [42],
three-way interactions (i.e., interactions among three options) build on corresponding two-way
interactions among the same set of options. To improve the accuracy of performance prediction,
it is necessary to model the influence of diverse interaction patterns on performance through a
hierarchical and expressive method.

2.3 Deep Feedforward Neural Network
Deep neural networks are one of the most expressive models owing to their excellent strength in
capturing non-linear data relationships [9]. A feedforward neural network (FNN) is a network which
connects the input and the output through multiple stacked layers (hidden layers) of computational
units (neurons) [9, 13]. According to the universal approximation theorem [18, 26, 29], an FNN with
at least one hidden layer can approximate any continuous function from one finite dimensional
space to the other at any level of accuracy, as long as enough neurons and a suitable activation
function (e.g., sigmoid, ReLU) are provided [13]. Hence, it is feasible to approximate the performance
prediction function of a configurable system with an FNN. However, the number of neurons needed
to approximate a real-world function might be infinitely large and it is impossible to train such an
FNN in finite time. Instead, researchers have tuned to train deep FNNs under an upper bound of
the approximation error [38, 50]. Theoretically, a deeper FNN can better enrich the "levels" of data
features and is more expressive to approximate functions with higher level of accuracy [6, 15, 27].
Nevertheless, we propose that a deep FNN is probably not a sufficiently good architecture for the
performance prediction problem, mainly due to following reasons:

â€¢ The interaction modeling process of a deep FNN is opaque. Under a general FNN architecture,
features of lower hidden layers serve as inputs to higher hidden layers and the final output is
the non-linear combinations of the features in the last hidden layer. In this way, it is easy
for a deep FNN to learn a performance function of high-level data features but ignore the
hierarchical interactions between lower- and higher-order features. Such a limitation might
not be well suited for the hierarchical interaction nature of configurable software systems
and reduce the prediction accuracy.

â€¢ The FNN architecture is a complete black box model such that it can only predict the scalar
performance value without providing insights about how feature interactions of different
orders influence the system performance.

To overcome above limitations of FNN, deep learning researchers have extended general FNNs
into hierarchical network architectures, achieving significant improvements in image processing [15,
28], natural language processing [4, 24], time series prediction [36], etc. Similarly in this paper,
we devise a novel network architecture consisting of multiple blocks connected hierarchically,
where different network blocks learn the influence of different interactions on system performance.
Our method well fits the hierarchical interaction nature of configurable systems and thus achieves
higher accuracy than the DeepPerf [13] method. Besides, by measuring the contributions of different
blocks to the system performance and computing the significance score of each configuration
option in different interaction orders, our approach provides some additional insights into the
coupling complexity and interaction patterns of the system, as we will discuss in later sections.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:6

J. Cheng et al.

2.4 Interaction Embedding
The main task of the embedding method is to translate an object from a high-dimensional abstract
space (e.g., a word, an image, a video, etc) into a vector representation of a low-dimensional space,
while maintaining some basic properties such as semantics and temporal relationships. Various
embedding methods have been proposed to improve a variety of deep learning tasks such as natural
language processing [4, 31, 32], recommendation systems [2, 51], and video encoding[19].

As we described in Section 1, the interaction space of configurable systems could be extremely
high-dimensional due to the exponential configuration space and complex interaction patterns.
Hence, it is reasonable to embed the interactions into low-dimensional vectors and model their
influence on the performance. However, there is little work about introducing embedding methods
into performance prediction of configurable systems and we hope to make a step forward. Following
the popular neural network language embedding method [4], we employ multiple FNNs to perform
interaction embedding of different orders and empirically find it beneficial for prediction accuracy
improvement.

2.5 Sparsity Regularization
In general, training a deep neural network with high prediction accuracy requires large-scale
datasets, which is not applicable for the performance prediction task domain. A good way to mitigate
the limited dataset issue is to incorporate prior knowledge into the neural network to guide its
learning process [13]. One important prior knowledge is that only a small number of configuration
options and their interactions have a significant impact on system performance [13, 42, 43], implying
sparsity on the parameters of the performance model (i.e., making parameters of many insignificant
options and interactions equal to zero).

The ğ¿1 regularization [48] is one of the most widely-used techniques for sparsity enforcement.
By adding a penalty term constructed from the sum of the absolute values of model parameters
to the loss function, ğ¿1 regularization encourages the magnitude of parameters to be small and
even zero [35]. However, the model accuracy might be highly sensitive to the regularization
hyperparameter and it usually takes a lot of effort and time to search for optimal value on a
large hyperparameter space [13, 14]. To reduce training cost, it is necessary to design an effective
mechanism to improve model robustness and reduce the hyperparameter space.

3 APPROACH
We propose HINNPerf, a hierarchical interaction neural network to predict performance values of
configurable systems. Based on the discussion in Section 2, our network architecture design relies
on three key principles. First, for better interaction modeling, the architecture should incorporate
the embedding method and the hierarchical interaction nature of configurable systems. Second,
considering the small sample challenge, the architecture should be sparse but efficient for hyperpa-
rameter tuning. Third, the architecture should provide some insights for users and developers on
how feature interactions of different orders affect the system performance. We now describe in
detail how these principles converge to our method.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:7

3.1 Theoretical Model
We construct the theoretical model by decomposing the performance function ğ‘“ of Equation (1)
into the sum of ğ‘š subfunctions:

ğ‘“ (o) =

ğ‘š
âˆ‘ï¸

ğ‘—=1

ğ‘“ğ‘— (xğ‘— ) ,

(cid:40)

xğ‘— =

o
xğ‘—âˆ’1 âŠ• ğ¼ ğ‘—âˆ’1(xğ‘—âˆ’1)

ğ‘— = 1
2 â‰¤ ğ‘— â‰¤ ğ‘š

,

(2)

where o âˆˆ X âŠ‚ Rğ‘› is the configuration with ğ‘› options and âŠ• denotes the vector concatenation
operation. The model defined by Equation (2) contains two function families:

â€¢ {ğ¼ ğ‘— : Rğ‘—ğ‘› â†’ Rğ‘› }1â‰¤ ğ‘— â‰¤ğ‘š, the interaction embedding functions. Each ğ¼ ğ‘— learns the interactions
among different features of input xğ‘— and embeds the results into a new vector. Here we set the
embedding dimension to ğ‘›, the same as the number of options in configuration o. That is, each
ğ¼ ğ‘— embeds the interaction information into a vector of Rğ‘› space. With this condition, we can
infer that xğ‘— âˆˆ Rğ‘—ğ‘› from the definition of Equation (2). Note that the embedding dimension
could be designed flexibly according to the demands of the real-world environmental settings.
: Rğ‘—ğ‘› â†’ R}1â‰¤ ğ‘— â‰¤ğ‘š, the performance subfunctions. Each ğ‘“ğ‘— maps the input features
xğ‘— âˆˆ Rğ‘—ğ‘› to a partial performance value, which is summed with other partial values into the
actual performance value ğ‘“ (o).

â€¢ {ğ‘“ğ‘—

The design rationale of our theoretical model is exactly the hierarchical interaction rule described
in Section 2.2 and the interaction embedding described in Section 2.4. In this case, the higher-order
interaction xğ‘— is built by the concatenation of lower-order interactions xğ‘—âˆ’1 and ğ¼ ğ‘—âˆ’1(xğ‘—âˆ’1), which
facilitates the performance representation of different interaction orders. For example, subfunction
ğ‘“1 might only represent the partial performance value under the influences of original configurable
options and the two-way interactions among them. And the embedding function ğ¼1 might embed
these two-way interactions into the vector ğ¼1(o). By concatenating the original configuration o
with its interaction embedding vector ğ¼1(o) as the input for ğ‘“2, it is easier for ğ‘“2 to represent the
partial performance value under the influences of three-way interactions. Similarly, higher-order
subfunctions ğ‘“ğ‘— ( ğ‘— â‰¥ 3) correspond to performance influences of higher-order interactions. Note
that the interaction patterns of real-world configurable software may be much more complex than
the simple example above, but our theoretical model is flexible to handle different situations.

3.2 Network Architecture
According to our theoretical model, we design a hierarchical interaction network architecture to
approximate all the performance subfunctions and interaction embedding functions in Equation (2).
Figure 1 (right) presents an overview of the proposed HINNPerf architecture. Similar in spirit to
the N-BEATS model [36] and other hierarchical network architectures [24, 28], our model consists
of multiple hierarchically connected building blocks. Figure 1 (left) shows the ğ‘—-th block of the
HINNPerf architecture. The ğ‘—-th block accepts its input xğ‘— and employs a multi-layer feedforward
neural network (FNN) to approximate subfunction ğ‘“ğ‘— and embedding function ğ¼ ğ‘— . The input and the
hidden layers of FNN are shared by the two approximation functions Ë†ğ‘“ğ‘— and Ë†ğ¼ ğ‘— while different output
layers are utilized to produce partial performance prediction Ë†ğ‘“ğ‘— (xğ‘— ) and interaction embedding
vector Ë†ğ¼ ğ‘— (xğ‘— ), respectively.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:8

J. Cheng et al.

Fig. 1. An overview of the proposed HINNPerf architecture.

Suppose the FNN of the ğ‘—-th block has ğ‘™ ğ‘— hidden layers with each layer containing ğ‘‘ ğ‘— neurons,

the computation of the ğ‘—-th block is described by the following equations:

hğ‘—,1 = ReLU(WâŠ¤
hğ‘—,2 = ReLU(WâŠ¤

ğ‘—,1xğ‘— + bğ‘—,1),
ğ‘—,2hğ‘—,1 + bğ‘—,2),

...... ,

hğ‘—,ğ‘™ ğ‘— = ReLU(WâŠ¤

ğ‘—,ğ‘™ ğ‘— hğ‘—,ğ‘™ ğ‘— âˆ’1 + bğ‘—,ğ‘™ ğ‘— ),

(3)

Ë†ğ‘“ğ‘— (xğ‘— ) = WâŠ¤
Ë†ğ¼ ğ‘— (xğ‘— ) = ReLU(WâŠ¤

ğ‘—,ğ‘“ hğ‘—,ğ‘™ ğ‘— + ğ‘ ğ‘—,ğ‘“ ,

ğ‘—,ğ¼ hğ‘—,ğ‘™ ğ‘— + bğ‘—,ğ¼ ),

where:

â€¢ Wğ‘—,ğ‘˜, bğ‘—,ğ‘˜ (1 â‰¤ ğ‘˜ â‰¤ ğ‘™ ğ‘— ) are weights and biases of the ğ‘˜-th hidden layer.
â€¢ ReLU is the activation function for non-linear learning in deep FNN [8, 34].
â€¢ hğ‘—,ğ‘˜ âˆˆ Rğ‘‘ ğ‘— (1 â‰¤ ğ‘˜ â‰¤ ğ‘™ ğ‘— ) is the output vector of the ğ‘˜-th hidden layer.
â€¢ Wğ‘—,ğ‘“ âˆˆ Rğ‘‘ ğ‘— , ğ‘ ğ‘—,ğ‘“ âˆˆ R are weights and bias used for predicting the partial performance
since the performance

value Ë†ğ‘“ğ‘— (xğ‘— ), respectively. Note that Ë†ğ‘“ğ‘— (xğ‘— ) is the linear output of hğ‘—,ğ‘™ ğ‘—
prediction is a regression problem [13].

â€¢ Wğ‘—,ğ¼ âˆˆ Rğ‘‘ ğ‘— Ã—ğ‘›, bğ‘—,ğ¼ âˆˆ Rğ‘› are weights and biases that project the output vector of the last

hidden layer to the interaction embedding space, respectively.

3.3 Hierarchical Regularization
As discussed in Section 1 and Section 2.5, training a deep neural network on the small sample
of a configurable system is difficult because of the high risk of overfitting. One way to solve the
problem is to introduce additional restriction to the network parameters by employing a suitable
regularization technique. Instructively, researchers have found that for configurable software
systems, although there are exponential number of interactions among configuration options, a
very large portion of potential interactions has no influence on system performance [13, 42, 43].

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

...x1x2xjnxjâˆˆRjnFNNinthej-thblock...L1regularizedhj,1...hj,2...hj,ljË†fj(xj)partialprediction...interactionembeddingË†Ij(xj)âˆˆRnFNN-1Ë†I1Ë†f1oâˆˆRnâŠ•FNN-2Ë†I2Ë†f2x2âˆˆR2nâŠ•......FNN-jË†IjË†fjxjâˆˆRjnâŠ•......FNN-mË†ImË†fmxmâˆˆRmn1stblock2ndblockj-thblockm-thblock+Ë†f(o)HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:9

This observation inspires us that parameters corresponding to unimportant interactions should
be eliminated in the performance model, that is, parameters of the performance model could be
quite sparse. Under the sparsity condition, we decide to apply the ğ¿1 regularization technique to
our method because of its strength on sparsity enforcement.

Denote ğœ½ as the parameters of a performance model, X as the input data, Y as the output data
and ğ½ (ğœ½, X, Y) as the loss function of the model. Generally, The ğ¿1 regularization changes the loss
function from ğ½ (ğœ½, X, Y) to:

(4)

ğ½reg(ğœ½, X, Y) = ğ½ (ğœ½, X, Y) + ğœ†||ğœ½ ||1,
where || Â· ||1 denotes the ğ¿1 norm and ğœ† is the regularization hyperparameter. For a deep FNN, ğœ½
includes weights and biases of all the hidden layers in the network. However, as discussed in [13],
it is infeasible to apply regularization to all the hidden layers in deep FNN due to the diversity and
high sensitivity of the regularization hyperparameters in different hidden layers. Hence, [13] only
applies ğ¿1 regularization to the first hidden layer of their DeepPerf model, which achieves a great
improvement on performance prediction. The disadvantage of such strategy is that the sparsity of
the whole network is controlled only by the regularization of the first hidden layer. In this case, the
behavior of the model heavily depends on the regularization hyperparameter of the first hidden
layer and even small changes of ğœ† may cause large fluctuations in prediction accuracy. Therefore,
one should carefully search for the optimal hyperparameter value on a large numerical space in
order to achieve desired results, which increases the cost of hyperparameter tuning.

Taking the advantage of the hierarchical network architecture of our HINNPerf model, we suggest
to apply ğ¿1 regularization to the first layer of the FNN in each block with one global regularization
hyperparameter, as shown in Figure 1 (left). The loss function of our method is changed to:

ğ½reg (ğœ½, X, Y) = ğ½ (ğœ½, X, Y) + ğœ†

ğ‘š
âˆ‘ï¸

ğ‘—=1

(||Wğ‘—,1||1 + ||bğ‘—,1||1).

(5)

Such hierarchical regularization technique allows us to shrink the parameters of unnecessary
interactions in different orders and thus is more flexible and effective. In our experiments, we show
that our method is more robust to the regularization hyperparameter and our hyperparameter
search space is reduced by 6 times compared to the DeepPerf model [13].

3.4 Complexity Analysis
We now analyze the complexity of our HINNPerf model through the scale of network parameters.
For model construction, we set the number of hidden layers and the number of hidden neurons
to be the same in all blocks, that is, we set ğ‘™1 = ğ‘™2 = Â· Â· Â· = ğ‘™ğ‘š = ğ‘™ and ğ‘‘1 = ğ‘‘2 = Â· Â· Â· = ğ‘‘ğ‘š = ğ‘‘.
According to Equation (3), the ğ‘—-th block of HINNPerf has ( ğ‘—ğ‘› + 1)ğ‘‘ + (ğ‘™ âˆ’ 1)(ğ‘‘ + 1)ğ‘‘ + (ğ‘‘ + 1)(ğ‘› + 1)
parameters, where:

â€¢ the first term ( ğ‘—ğ‘› + 1)ğ‘‘ comes from the weights Wğ‘—,1 âˆˆ Rğ‘—ğ‘›Ã—ğ‘‘ and biases bğ‘—,1 âˆˆ Rğ‘‘ of the first

hidden layer;

â€¢ the second term (ğ‘™ âˆ’ 1)(ğ‘‘ + 1)ğ‘‘ corresponds to the weights Wğ‘—,ğ‘˜ âˆˆ Rğ‘‘Ã—ğ‘‘ and biases bğ‘—,ğ‘˜ âˆˆ Rğ‘‘

(2 â‰¤ ğ‘˜ â‰¤ ğ‘™) of the last ğ‘™ âˆ’ 1 hidden layers;

â€¢ the third term (ğ‘‘ +1)(ğ‘›+1) is produced by the weights and biases of the interaction embedding

(Wğ‘—,ğ¼ âˆˆ Rğ‘‘Ã—ğ‘›, bğ‘—,ğ¼ âˆˆ Rğ‘›) and the partial performance output (Wğ‘—,ğ‘“ âˆˆ Rğ‘‘, ğ‘ ğ‘—,ğ‘“ âˆˆ R).

Hence, the scale of network parameters in the ğ‘—-th block is O ( ğ‘—ğ‘›ğ‘‘ + ğ‘™ğ‘‘ 2), implying total O (ğ‘š2ğ‘›ğ‘‘ +
ğ‘šğ‘™ğ‘‘ 2) complexity of the HINNPerf model. On the other hand, suppose that the DeepPerf [13]
model has ğ‘™ â€² hidden layers and the same ğ‘‘ neurons in each hidden layer as HINNPerf, the total
complexity of DeepPerf is O (ğ‘›ğ‘‘ + ğ‘™ â€²ğ‘‘ 2). Note that ğ‘™ < ğ‘™ â€² since the number of hidden layers in

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:10

J. Cheng et al.

each block of HINNPerf is typically smaller than the total number of hidden layers in DeepPerf,
and we roughly have ğ‘šğ‘™ â‰ˆ ğ‘™ â€². Accordingly, HINNPerf mainly increases O (ğ‘š2ğ‘›ğ‘‘) complexity
compared to DeepPerf. However, based on the previous works [42, 43, 45] revealing that there are
at most five-way interactions in configurable systems, we restrict ğ‘š â‰¤ 5 in our model. Therefore,
the increase in model complexity caused by the parameter ğ‘š could be regarded as constant (i.e.,
O (ğ‘š2ğ‘›ğ‘‘) â‰ˆ O (ğ‘€ğ‘›ğ‘‘) where ğ‘€ is a constant).

Another question is that how many configuration options can be handled by our model? Is there
an upperbound of the option number ğ‘› for HINNPerf ? We state that according to the the universal
approximation theorem [18, 26, 29] of deep neural networks, there is no theoretical upperbound
of ğ‘› under our HINNPerf architecture. Theoretically, HINNPerf can predict the performance of
complex systems with arbitrary number of configuration options. The complete theoretical proof
is beyond the scope of this paper, and here we intuitively justify the statement using the main
theorem from [29]:

Theorem 1 (main theorem). Under certain assumptions, it holds that for any function ğ‘“ : Rğ‘› â†’ R
with given approximation error ğœ–, there exists a positive integer ğ‘§, and a fully connected and feed-
forward deep neural network of ğ‘™ = âŒˆlog2 ğ‘§âŒ‰ hidden layers, with ğ‘› inputs and a single output and with
ReLU activation such that:

ğ‘§ â‰¤

ğ¶ğ‘›
ğœ–2

,

(6)

where the constant ğ¶ depends on certain assumptions, but not on ğ‘›.

All the assumptions and the complete proof of the above theorem can be found in [29]. Note that
as ğ‘› â†’ âˆ or ğœ– â†’ 0, we have ğ‘§ < âˆ and thus ğ‘™ < âˆ, implying that for any value of ğ‘›, there always
exists a deep FNN that can approximate the function ğ‘“ with proper error ğœ–. When it comes to our
HINNPerf architecture, the ğ‘—-th block employs a deep FNN with ğ‘™ ğ‘— hidden layers to approximate the
performance subfunction ğ‘“ğ‘— : Rğ‘—ğ‘› â†’ R under an error bound ğœ– â‰¤
. Therefore, we conclude
that in theory HINNPerf can approximate the performance function of configurable systems having
arbitrary ğ‘› configuration options without upperbound. And we show that the approximation error
of HINNPerf is smaller than other baselines in the experiment part of this paper.

âˆšï¸ƒ ğ¶ ğ‘—ğ‘›
2ğ‘™ ğ‘—

3.5 Model Advantages
3.5.1 Beyond the Accuracy. Although the goal of our method is to improve the prediction accuracy
of the performance model, the proposed hierarchical architecture allows us to extract some valuable
information about the configurable system. First of all, by measuring the contributions of different
partial performance functions ğ‘“ğ‘— (xğ‘— ) to the whole performance function ğ‘“ (o) in Equation (2), we
can obtain some insights about the influences of different interactions on the system performance.
If the partial function of lower-order interactions (e.g., ğ‘“1(o)) has the major contribution, it implies
that the interactions among different configuration options are relatively simple and each option
tends to affect the system performance independently (i.e., the configuration options are designed
to be relatively disentangled). On the contrary, if the partial function of higher-order interactions
(e.g., ğ‘“ğ‘š (xğ‘š)) has the major contribution, the feature interactions of the software system might
be very complex and more careful configuration decisions should be considered when deploying
the configurable system. Second, by computing the significance score of each configuration option
in different blocks, we can identify which options have the greatest impact on different partial
performance functions ğ‘“ğ‘— (xğ‘— ). Recognizing significant options in different interaction orders (i.e.,
different blocks) could help users and developers efficiently pinpoint the key performance factors
and reveal some potential interaction patterns of the system. The above two advantages make our
model more helpful to users and developers than other deep learning methods such as DeepPerf [13].

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:11

We will show these insights of different real-world subject systems in the experimental part of this
paper.

3.5.2 Generality. If we only use one block and empty interaction embedding function (i.e., ğ‘š = 1
and ğ¼1(Â·) = âˆ…), our HINNPerf architecture reduces to the DeepPerf model [13]. Therefore, DeepPerf
can be viewed as a special case of our method. From this point of view, HINNPerf shows a general
deep learning framework for performance prediction of configurable systems. And we believe that
more advanced deep learning performance models can be derived from this framework.

3.6 Training Details
To train our proposed model, we utilize the following technical practice of machine learning:

â€¢ Dataset separation: As in most machine learning scenarios, we split the performance dataset
into training, validation, and testing sets. We use the training set to optimize the model
parameters, the validation set to search for the hyperparameters and the testing set to
evaluate the model accuracy.

â€¢ Data normalization: To speed up network training process and facilitate hyperparameter
tuning, we normalize the input and the output of the training data. There are two kinds of
normalization techniques: maximization and Gaussian normalization. The former normalizes
the data in [0, 1] and the latter normalizes the data to a Gaussian distribution. We use the
validation dataset to choose the better normalization technique. During testing, we normalize
the testing dataset by exactly the same parameters of normalization during training, and
then denormalize the model predicted output for accuracy evaluation.

â€¢ Loss function: The loss function of our approach is the mean square error between the real
performance values and the predicted output, which is the common regression loss function
in machine learning.

â€¢ Optimization: We utilize the Adam [23] algorithm to train our neural network. During the
training process, the batch size is equal to the whole sample size of the training dataset since
the size of the training data is small for performance prediction problem.

â€¢ Hyperparameter setting: We conduct a grid search over all hyperparameters of our method.
We fix the initial learning rate to be 0.001 and employ a learning rate schedule that drops the
rate by a factor of 10âˆ’3 after every training epoch. We vary the number of blocks in {2, 3, 4, 5}
(i.e., the hyperparameter ğ‘š in Equation (2)) since the interactions among different options are
at most five-way [42, 43, 45]. Also, we choose the number of hidden layers in each block from
{2, 3, 4} and fix the number of neurons in each hidden layer to be 128. The ğ¿1 regularization
hyperparameter ğœ† is chosen from {0.001, 0.01, 0.1, 1, 10}. The optimal hyperparameter setting
is the setting that achieves the smallest validation error.

Finally, we implement our proposed method HINNPerf using Python 3.6 and Tensorflow 1.13.0 [1]
under a machine with one NVIDIA GeForce GTX 1080 GPU and one Intel(R) Core(TM) i7-6800K
CPU 3.40GHz.

4 EVALUATION
The core of this section is to evaluate whether our method can better model the complex interactions
in real-world configurable systems and accurately predict the performance with a small training
sample (i.e., the two challenges mentioned in Section 1). Specifically, we aim at answering the
following research questions (RQ):

â€¢ RQ1: How accurate is our approach in predicting performance of configurable software

systems?

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:12

J. Cheng et al.

â€¢ RQ2: Are the proposed hierarchical interaction embedding architecture and the hierarchical

ğ¿1 regularization strategy necessary for improving prediction accuracy?

â€¢ RQ3: How do the ğ¿1 regularization hyperparameter ğœ† and the interaction order ğ‘š affect the

efficacy of HINNPerf ?

â€¢ RQ4: In addition to predicting performance values, can HINNPerf provide some insights
on configuration coupling and help users and developers find out significant configuration
options in the performance model?

â€¢ RQ5: What is the time cost of training and testing the HINNPerf model for performance

prediction?

We next conduct different experiments to answer the RQs above, comprehensively demonstrating

the effectiveness, robustness and practicality of our approach.

4.1 Evaluation Metric
As described in Section 3.6, we use the training and validation datasets to generate a performance
model for each system, and then use this model to predict the performance values of configurations
on the testing dataset. Similar to [12â€“14, 42], we utilize the mean relative error (MRE) to evaluate
the modelâ€™s prediction accuracy:

ğ‘€ğ‘…ğ¸ =

1

|T |

âˆ‘ï¸

oâˆˆT

|ğ‘“ (o) âˆ’ Ë†ğ‘“ (o)|
ğ‘“ (o)

Ã— 100,

(7)

where T denotes the testing dataset, ğ‘“ (o) is the actual performance value of configuration o, and
Ë†ğ‘“ (o) is the predicted performance value of configuration o.

4.2 Subject Systems

Table 2. Overview of the subject systems

|C|
1152
180
432

Performance Value
Encoding time
Response time
Compression time

Domain
Video Encoder
Database System
File Archive Utility
Video Encoder
Code Optimizer

System
x264
BDB-J
LRZIP
VP9
POLLY
Dune MGS Multi-Grid Solver
HIPAğ‘ğ‘
Image Processing
Stencil-Grid Solver
HSMGP
Garbage Collector
JavaGC
Compiler
SaC

Variance
2.64 Ã— 104
1.76 Ã— 107
1.35 Ã— 1012
5.12 Ã— 102
4.16 Ã— 101
1.35 Ã— 107
2.41 Ã— 102
6.78 Ã— 105
5.84 Ã— 106
3.25 Ã— 102
|B|: number of binary options; |N |: number of numeric options; |C|: total number of valid configurations
measured for each system; Range & Variance: the range and the variance of performance values in each
dataset.

216000 Encoding time
60000
2304
13485
3456
166975 Collection time
62523 Compilation time

Range
[244, 822]
[2960, 16531]
[37470, 5811280]
[41, 100]
[4, 32]
[4422, 58092]
[21, 122]
[100, 4397]
[370, 80004]
[1, 492]

Runtime
Solving time
Solving time
Solving time

|N |
0
0
0
0
0
3
2
3
23
7

|B|
16
26
19
42
40
8
31
11
12
53

In our experiments, we consider 10 real-world configurable software systems from different
domains, including video encoders, database systems, multi-grid solvers, image processing frame-
works, compilers, etc. Five of these systems only have binary configuration options and the other
five systems have both binary and numeric configuration options. Different systems have different
sizes (from 45 thousands to more than 300 thousands lines of code) and are written in different
programming languages (Java, C, and C++). Table 2 provides an overview of the subject systems.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:13

According to previous work [13, 22, 42, 43], the performance datasets of the ten subject systems
are collected in the following way:

â€¢ x264 is a video encoder for the H.264 compression format. Relevant configuration options
included the number of reference frames, enabling or disabling the default entropy encoder,
and the number of frames for ratecontrol and lookahead. We have measured the time to
encode the Sintel trailer (734 MB) on an Intel Core Q6600 with 4 GB RAM (Ubuntu 14.04).
â€¢ BDB-J is the Java version of the Berkeley Database. we use Oracleâ€™s standard benchmark to
measure the performance of BDB-J. The workload produced by the benchmarks is a typical
sequence of database operations.

â€¢ LRZIP is a file compression tool. We consider configuration options that define, for instance,
the compression level and the use of encryption. We used the uiq28 generator to generate a
file (632 MB), and we measured the time for compressing this file with version 0.600 on a
machine with AMD Athlon64 Dual Core, 2 GB RAM (Debian GNU/Linux 6).

â€¢ VPXENC (VP9) is a video encoder that uses the VP9 video coding format. It offers different
configuration options, such as adjusting the quality, the bitrate of the coded video, and the
number of threads to use. We measured the encoding time of 2 seconds from the Big Buck
Bunny trailer on an Intel Xeon E5-2690 and 64 GB RAM (Ubuntu 16.04).

â€¢ POLLY is a loop optimizer that rests on top of LLVM. POLLY provides various configuration
options that define, for example, whether code should be parallelized or the choice of the tile
size. We used POLLY version 3.9, LLVM version 4.0.0, and Clang version 4.0.0. As benchmark,
we used the gemm program from polybench and measured its runtime on an Intel Xeon
E5-2690 and 64 GB RAM (Ubuntu 16.04).

â€¢ Dune MGS is a geometric multi-grid solver based on the Dune framework. The framework
provides algorithms for smoothing and solving Poisson equations on structured grids. Binary
options include several smoother and solver algorithms. Numeric options include different
grid sizes and pre- and post-smoothing steps. We measured the time to solve Poissonâ€™s
equation on a Dell OptiPlex-9020 with an Intel i5-4570 Quad Code and 32 GB RAM (Ubuntu
13.4).

â€¢ HIPAğ‘ğ‘ is an image processing acceleration framework, which generates efficient low-level
code from a high-level specification. Binary options are, among others, the kind of memory
to be used (e.g., texture vs. local). The number of pixels calculated per thread is an example
of a numeric option. We measured the time needed for solving a test set of partial differential
equations on an nVidia Tesla K20 card with 5GB RAM and 2496 cores (Ubuntu 14.04).

â€¢ HSMGP is a highly scalable multi-grid solver for large-scale data sets. Binary options include
in-place conjugate gradient and in-place algebraic multi-grid solvers. Numeric options include
the number of smoothing steps and the number of nodes used for computing the solution. As
a benchmark, we performed a multi-grid iteration of solving Poissonâ€™s equation. We executed
the benchmark runs on JuQueen, a Blue Gene/Q system, located at the Julich Supercomputing
Center, Germany.

â€¢ JavaGC is the Java garbage collector (version 7) with several options for adaptive garbage-
collection boundary and size policies. For measurement, we executed the DaCapo benchmark
suite on a computing cluster consisting of 16 nodes each equipped with an Intel Xeon E5-2690
Ivy Bridge having 10 cores and 64 GB RAM (Ubuntu 14.04).

â€¢ SaC is a variant of C for high-performance computing based on stateless arrays. The SaC
compiler implements a large number of high-level and low-level optimizations to tune
high-level programs for efficient parallel executions. The compiler is highly configurable,
allowing users to select various optimizations and to customize the optimization effort (e.g.,

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:14

J. Cheng et al.

optimization cycles and loop-unrolling threshold). As benchmark, we compile and execute an
n-body simulation shipped with the compiler, measuring the execution time of the simulation
at different optimization levels. We executed all benchmarks on an 8 core Intel i7-2720QM
machine with 8 GB RAM (Ubuntu 12.04).

4.3 RQ1: Performance Prediction Comparison
4.3.1 Baselines. As introduced in Section 1, many learning methods have been proposed to predict
performance values of configurable systems with binary and/or numeric options, including SPLCon-
queror [42, 43], Fourier methods [14, 52], DECART [12], and DeepPerf [13]. Besides, traditional
machine learning methods such as Random Forests [17] (RF) can also be used for the performance
prediction task. For configurable systems with only binary options, DECART and DeepPerf are re-
cent advanced methods and can achieve higher prediction accuracy than others [12, 13]. Therefore,
to save experiment time, we only compare the proposed method with three baselines DECART,
DeepPerf and RF on binary systems. For configurable systems with both binary and numeric
options, we compare the effectiveness of our HINNPerf model with that of SPLConqueror, DeepPerf
and RF, since the DECART model cannot deal with numeric options.

Setup. Following the experiment setup in [13], we randomly select a certain number of
4.3.2
configurations and their corresponding performance values to construct the training and validation
datasets (sample3), and use the remaining configuration measurements as the testing dataset.
For each subject system, we test the model accuracy under different sample sizes. The sample
sizes of each binary system are ğ‘›, 2ğ‘›, 4ğ‘›, 6ğ‘›, where ğ‘› is the number of binary options of each
system (shown in the column |B| of Table 2); for each binary-numeric system, we choose the same
sample sizes that SPLConqueror suggested [42]. Same as in [13], for DECART, DeepPerf, RF and our
HINNPerf methods, we repeat the random sampling, training and testing process 30 times, and
then report the mean and the 95% confidence interval of each methodâ€™s MRE obtained after 30
experiments. For SPLConqueror, however, we cannot choose a random sample since it combines
different sampling heuristics for binary options (e.g., option-wise (OW), pair-wise (PW), etc), and
several experimental design methodologies (e.g., Plackett-Burman (PBD), Random Design (RD), etc)
for numeric options to predict system performance. Each combination of sampling heuristics and
non-random experimental designs yields a unique sample, implying that repeating 30 experiments
for SPLConqueror produces exactly the same results and thus the variance of the 30 experimental
results is zero. Hence, we only report the MRE of SPLConqueror on the testing dataset since the 95%
confidence interval is also zero when the variance is zero [13]. Moreover, we check whether there is
a statistically significant difference between the results of the best and the second best methods by
using the Wilcoxon rank sum test [7], a non-parametric test that compares different distributions.
When the ğ‘-value is smaller than 0.05, we consider that there is a significant difference and report
the significantly better method.

To replicate the results of SPLConqueror, DECART and DeepPerf, we utilize the code published on
their supplement websites and run their models with the best hyperparameter settings [12, 13, 42].
For the RF method, we use the function RandomForestRegressor in scikit-learn package [37] to
perform model training and prediction. To select the best hyperparameters for RF, we construct the
grid search by varying four hyperparameters in RandomForestRegressor: 10 values of n_estimators
ranging from 10 to 1000, 10 values of min_samples_leaf ranging from 0.05 to 0.5, 10 values of
max_leaf_nodes ranging from 2 to 100 and the max_features are auto, sqrt or log2. With this
setting, the hyperparameter space contains 3000 different combinations of hyperparameter values.
Hence, we believe it is sufficient enough to find the optimal setting for RF.
3We use 67% of sample for training and 33% for validation.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:15

Table 3. Performance prediction results of ten subject systems

RF

VP9

x264

BDB-J

LRZIP

POLLY

DeepPerf

DECART

HINNPerf

Dune MGS

SPLConqueror

Subject
System

â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
20.1
22.1
11
8.3
14.2
13.8
13.9
13.9
4.5
2.8
2.2
1.7
21.9
28.2
24.6
18.8
21.1
20.3
16
30.7

3.87
1.30
0.47
0.25
4.67
0.16
0.09
0.13
92.61
20.36
2.98
2.07
0.13
0.23
0.14
0.09
3.79
1.11
0.62
0.52
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“

17.71
9.31
4.26
2.05
10.04
2.23
1.72
1.57
198.79
56.73
19.06
11.22
3.15
2.23
1.21
0.86
17.98
5.84
4.20
3.03
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“

Sample
Size (%)
ğ‘› (1.39%)
2ğ‘› (2.78%)
4ğ‘› (5.56%)
6ğ‘› (8.34%)
ğ‘› (14.44%)
2ğ‘› (28.88%)
4ğ‘› (57.76%)
6ğ‘› (86.64%)
ğ‘› (4.40%)
2ğ‘› (8.80%)
4ğ‘› (17.60%)
6ğ‘› (26.40%)
ğ‘› (0.02%)
2ğ‘› (0.04%)
4ğ‘› (0.08%)
6ğ‘› (0.12%)
ğ‘› (0.07%)
2ğ‘› (0.14%)
4ğ‘› (0.28%)
6ğ‘› (0.42%)
49 (2.13%)
78 (3.39%)
384 (16.67%)
600 (26.04%)
261 (1.94%)
528 (3.92%)
736 (5.46%)
1281 (9.50%)
77 (2.23%)
173 (5.01%)
384 (11.11%)
480 (13.89%)
855 (0.51%)
2571 (1.54%)
3032 (1.82%)
5312 (3.18%)
2060 (3.29%)
2295 (3.67%)
2499 (4.00%)
3261 (5.22%)

â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
â€“
OW RD
PW RD
PW PBD(49,7)
PW PBD(125,5)
OW RD
OW PBD(125,5)
OW PBD(49,7)
PW RD
OW RD
PW RD
OW PBD(49,7)
OW PBD(125,5)
OW PBD(125,5)
PW PBD(49,7)
PW RD
PW PBD(125,5)
OW RD
OW PBD(125,5)
OW PBD(49,7)
PW RD

Sampling Heuristic Mean Mean Margin Mean Margin Mean Margin Mean Margin
10.43
3.61
1.49
0.92
7.25
2.07
1.67
1.49
173.11
67.23
19.76
12.75
3.12
2.38
1.83
1.06
15.61
5.36
4.82
3.60
15.73
13.67
7.20
6.44
9.39
6.38
5.06
3.75
6.76
3.60
2.53
2.24
21.83
16.48
12.76
11.03
15.83
19.25
16.73
15.64

2.28
0.54
0.38
0.15
4.21
0.32
0.12
0.13
50.32
27.04
2.61
2.54
0.12
0.13
0.14
0.15
2.68
0.85
0.68
0.41
0.90
0.82
0.18
0.20
0.37
0.44
0.35
0.26
0.87
0.2
0.13
0.11
7.07
6.59
0.56
0.45
1.25
6.03
1.13
1.18
The best result is in bold and the second best result is underlined.
Mean: mean of the MREs seen in 30 experiments. Margin: margin of the 95% confidence interval of the MREs
in 30 experiments.
Better Method is chosen between the best method and the second best method using Wilcoxon test on
MREs of 30 experiments with 5% significance level.
The percentages (%) after each sample size show the ratio of the training sample size to the total number of
data samples measured for each system.

Better
Method
HINNPerf
HINNPerf
HINNPerf
HINNPerf
Same
Same
Same
Same
Same
Same
Same
Same
Same
HINNPerf
HINNPerf
HINNPerf
Same
Same
Same
DECART
HINNPerf
HINNPerf
HINNPerf
HINNPerf
HINNPerf
HINNPerf
HINNPerf
HINNPerf
Same
SPLConqueror
HINNPerf
Same
HINNPerf
HINNPerf
HINNPerf
HINNPerf
HINNPerf
HINNPerf
HINNPerf
HINNPerf

18.19
12.39
8.47
7.35
16.64
9.17
5.04
3.85
229.59
151.58
96.44
69.94
14.54
15.54
14.19
13.38
24.38
18.04
14.51
13.88
17.73
18.19
8.29
7.46
14.49
11.38
9.87
7.54
34.48
22.31
14.78
12.56
50.59
33.40
31.29
23.90
62.72
59.37
58.07
51.39

9.68
3.00
0.98
0.42
4.94
2.06
1.53
1.42
214.52
48.44
16.53
10.05
3.09
1.85
0.73
0.44
16.33
6.20
4.14
3.50
13.43
11.93
6.74
5.86
7.24
4.55
3.59
2.81
5.59
3.02
1.98
1.75
15.99
10.02
9.74
7.12
13.50
12.94
12.37
11.48

1.40
0.31
0.16
0.06
2.27
0.15
0.07
0.14
84.82
8.82
2.90
2.03
0.21
0.15
0.08
0.02
2.52
0.89
0.48
0.22
0.84
0.67
0.19
0.13
0.36
0.20
0.12
0.06
0.75
0.18
0.15
0.08
1.17
0.44
0.54
0.24
0.94
0.79
0.70
0.73

2.24
0.99
0.46
0.32
2.35
1.23
0.54
0.39
57.47
32.53
19.86
17.01
2.86
1.36
0.42
0.42
1.34
1.62
1.82
1.14
0.64
0.47
0.11
0.09
0.27
0.19
0.14
0.10
3.17
1.16
0.37
0.68
2.27
0.71
0.66
0.47
3.02
1.61
1.90
2.02

HSMGP

HIPAğ‘ğ‘

JavaGC

SaC

4.3.3 Results. Table 3 summarizes the performance prediction results of each approach on the 10
subject systems with small and large sample sizes, from which we obtain the following observations:

â€¢ Our method clearly outperforms other approaches on most of the subject systems, showing
the superiority of hierarchical interaction network design for performance prediction. Specif-
ically, HINNPerf achieves best prediction accuracy on 7 out of 10 systems, reducing the MRE
by an average of 22.67% compared to the second best results of other baselines. Remarkably,
HINNPerf achieves the best 54.35% improvement on the x264 system with 6ğ‘› sample size.
Among the 7 winning systems, the Wilcoxon test shows that HINNPerf significantly out-
performs the second best method on 5 of them. For the other 3 systems LRZIP, POLLY, and
HSMGP, HINNPerf and other methods perform quite similarly without significant difference

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:16

J. Cheng et al.

between them. On the other hand, we observe that the sample size of ğ‘› in systems with
only binary options seems too small to train each performance model for achieving good
accuracy. As we can see from Table 3, the prediction MREs of all models under sample size ğ‘›
are more than twice higher than the MREs under sample size 2ğ‘› on the x264, BDB-J, LRZIP,
and POLLY systems, respectively. Whatâ€™s worse, the MREs of all models under the sample
size ğ‘› of the LRZIP system are more than 100%, implying that all performance models cannot
work when trained in this case. Therefore, the prediction results under sample size ğ‘› may
not really reveal the efficacy of each performance model.

â€¢ We conduct the Wilcoxon test on the prediction results of our method and the most related
deep learning method DeepPerf [13]. It shows that HINNPerf significantly outperforms
DeepPerf over all of the 5 systems with both binary and numeric configuration options (i.e.,
Dune MGS, HIPAğ‘ğ‘ , HSMGP, JavaGC, and SaC). Since binary-numeric interactions are usually
more complicated than binary-binary interactions [13, 42], the result of the Wilcoxon test
demonstrates that our method can better model the influences of complex interactions on
system performance.

â€¢ Since the major contribution of the performance model is to accurately predict the perfor-
mance when only a small subset of data samples is avaliable for training, it is important to
evaluate the accuracy of each performance model under a small sample size. From Table 3,
except for the BDB-J system, the smallest sample size used for training is less than 5% of
the total number of data samples measured for each system. Using these small data subsets,
our HINNPerf model achieves a prediction accuracy of more than 83% (i.e., MRE less than
17%) on 9 out of 10 systems, with accuracy of 5 systems (i.e., x264, BDB-J, VP9, HIPAğ‘ğ‘ , and
HSMGP) more than 90%. Besides, our method outperforms other baselines on 7 out of 10
systems under the smallest sample size. Therefore, we conclude that our proposed HINNPerf
model can still attain a good accuracy when trained with only a small subset of data samples.
Furthermore, to obtain the same level of accuracy, HINNPerf needs fewer data than other
methods. For example, with the system JavaGC, HINNPerf only needs 2571 sample to achieve
a prediction MRE of 10.02% (i.e., accuracy of 89.98%) whilst DeepPerf needs 5312 (more than
2 times) sample to obtain the similar prediction accuracy. These observations demonstrate
the effectiveness of our approach in small sample predictions.

â€¢ We observe that there are some differences in the prediction MREs of different systems.
For example, the prediction MRE of all performance models on the LRZIP system is much
higher than other subject systems, probably due to the large scales and variance of the
performance value of the LRZIP system (see Table 2). It seems that the scales and variance of
the performance metrics may be an issue affecting the prediction accuracy of the performance
model, and the data with larger variance is more difficult to predict. This observation inspires
us that, in practice, it is better to properly control the data variance when measuring the
performance of a configurable system to train the performance model. Since the goal of
this paper is to design a novel performance model that outperforms other state-of-the-art
models, we demonstrate the effectiveness of our model only using the public benchmark
datasets [13, 22, 42, 43] without changing the scales and variance of the datasets. Instead, we
show that our HINNPerf model achieves better accuracy on most systems with performance
metrics having different scales and variance.

In summary, compared to existing methods, our proposed architecture HINNPerf provides a
more effective and accurate deep learning performance model to address the interaction modeling
and the small sample challenges mentioned in Section 1.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:17

4.4 RQ2: Effectiveness of Architecture Design
Our method consists of two key components, the hierarchical interaction embedding (Equation (2))
and the hierarchical ğ¿1 regularization, as shown in Figure 1. In this experiment, we aim to evaluate
whether these two components are necessary to improve performance prediction accuracy, or they
can be substituted by other techniques. To show the effectiveness of each component, we compare
HINNPerf with following design alternatives:

â€¢ MB-FNN : We remove the hierarchical interaction embedding from the HINNPerf architecture.
In this way, the neural network reduces to a multi-block FNN with ğ¿1 regularization on the
first layer of each block.

â€¢ L2-HINN : We replace the ğ¿1 regularization of HINNPerf with ğ¿2 regularization.
â€¢ Plain-HINN : This architecture is simply the HINNPerf model without regularization.
â€¢ Dropout-HINN : We replace the ğ¿1 regularization of HINNPerf with the dropout technique [46].
We evaluate each architecture on the 10 subject systems and compare the prediction results with
the HINNPerf model.

Setup. For all the alternative models, we also use the same grid search strategy to tune the
4.4.1
hyperparameters. All the hyperparameter settings are the same as those for HINNPerf. Except that,
for Dropout-HINN, the search range for the dropout rate is {0.1, 0.25, 0.5, 0.75, 0.9} since the dropout
rate needs to be â‰¤ 1.

4.4.2 Results. Table 4 shows the prediction MREs of HINNPerf and 4 alternative methods. Overall,
HINNPerf achieves the best accuracy on most subject systems. Comparing the results of HINNPerf
and MB-FNN, we find that removing the interaction embedding from the architecture causes a
large accuracy drop overall (e.g., on average, 58% accuracy drop on VP9 and 68% accuracy drop
on POLLY), which demonstrates that the hierarchical interaction embedding plays an important
role in improving the prediction accuracy of our performance model. Hence, we believe that it is
worth introducing the embedding method to further improve deep learning performance models,
confirming our discussion in Section 2.4.

On the other hand, removing the regularization technique from HINNPerf (i.e., the Plain-HINN
model) causes the most significant accuracy drops, since a deep neural network is prone to overfit on
training data. The L2-HINN and Dropout-HINN methods perform much better than Plain-HINN but
are still less effective than HINNPerf. These observations show that the hierarchical ğ¿1 regularization
architecture may be the most important component for our approach.

In conclusion, both the hierarchical interaction embedding and the hierarchical ğ¿1 regularization
techniques are necessary for prediction accuracy improvement. The full HINNPerf architecture is
the most accurate and effective peformance model for configurable systems when compared to
other alternatives.

4.5 RQ3: Hyperparameter Test
4.5.1 Robustness of Regularization. In the context of performance prediction with small sample,
the ğ¿1 regularization hyperparameter ğœ† of Equation (5) is crucial for the model accuracy. Usually,
it takes effort and time to search for the optimal hyperparameter value. DeepPerf conducts grid
search with 30 points logarithmically spaced in the range [0.01, 1000] to find the best value for
the ğ¿1 regularization hyperparameter [13]. However, for the regularization hyperparameter of
our proposed approach, we only use grid search with 5 points in {0.001, 0.01, 0.1, 1, 10} to obtain
better accuracy than DeepPerf, reducing the search space by 6 times. We believe that the reduction
of hyperparameter space benefits from the improvement of the model robustness. In this experi-
ment, we demonstrate our hypothesis by testing the sensitivity of our model to the regularization

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:18

J. Cheng et al.

Table 4. Comparison between HINNPerf and other alternatives

Subject
System

x264

BDB-J

LRZIP

VP9

POLLY

Dune MGS

HIPAğ‘ğ‘

HSMGP

JavaGC

SaC

Sample
Size
2ğ‘›
4ğ‘›
6ğ‘›
2ğ‘›
4ğ‘›
6ğ‘›
2ğ‘›
4ğ‘›
6ğ‘›
2ğ‘›
4ğ‘›
6ğ‘›
2ğ‘›
4ğ‘›
6ğ‘›
49
78
384
600
261
528
736
1281
77
173
384
480
855
2571
3032
5312
2060
2295
2499
3261

MB-FNN

HINNPerf

Plain-HINN

Dropout-HINN
L2-HINN
Mean Margin Mean Margin Mean Margin Mean Margin Mean Margin
3.00
0.98
0.42
2.06
1.53
1.42
48.44
16.53
10.05
1.85
0.73
0.44
6.20
4.14
3.50
13.43
11.93
6.74
5.86
7.24
4.55
3.59
2.81
5.59
3.02
1.98
1.75
15.99
10.02
9.74
7.12
13.50
12.94
12.37
11.48

4.55
1.90
1.02
2.26
1.61
1.68
83.85
26.31
10.32
2.40
1.32
0.90
14.43
5.66
3.80
13.09
11.83
6.83
6.08
7.53
4.60
3.66
2.82
6.58
3.11
1.84
1.78
20.79
13.27
12.24
10.04
14.34
14.51
13.01
12.54

3.09
1.07
0.54
2.40
1.53
1.30
58.21
22.01
12.07
2.21
1.22
0.83
14.61
6.31
4.15
12.86
12.03
7.19
6.36
7.28
4.67
3.63
2.86
6.28
3.11
2.33
1.79
16.05
11.17
10.44
8.66
15.59
14.12
14.10
14.13

7.61
4.47
3.42
4.27
2.77
3.09
77.08
42.10
18.95
4.17
2.88
2.35
20.51
11.72
7.86
13.28
12.45
7.56
6.65
9.19
5.71
4.49
3.15
7.15
4.95
2.29
2.09
20.41
14.10
13.92
10.80
15.20
13.86
12.91
11.68

0.48
0.17
0.09
0.23
0.09
0.18
19.82
6.38
2.11
0.15
0.06
0.06
1.49
0.66
0.27
0.82
0.63
0.18
0.16
0.29
0.19
0.08
0.05
0.66
0.23
0.09
0.13
1.40
0.57
0.50
0.38
0.75
0.97
0.65
0.59

9.09
4.49
3.18
5.70
2.56
1.86
82.97
47.95
28.03
6.13
3.63
3.08
22.71
14.96
10.55
13.72
12.82
7.18
6.14
10.32
7.30
6.04
4.07
7.05
3.23
1.87
1.58
27.59
18.61
17.47
14.85
22.49
21.92
20.57
18.95

0.54
0.34
0.16
0.48
0.15
0.25
17.50
10.55
4.73
0.23
0.12
0.07
1.73
1.40
1.16
0.72
0.53
0.18
0.10
0.39
0.27
0.16
0.10
1.46
1.86
0.15
0.14
0.55
0.33
0.64
0.24
0.92
0.84
0.59
0.61

0.98
0.21
0.16
0.84
0.25
0.21
13.48
12.55
9.83
0.47
0.12
0.11
2.32
2.07
0.78
1.22
1.09
0.24
0.18
0.94
0.19
0.22
0.20
1.39
0.15
0.08
0.10
1.02
0.37
0.39
0.18
0.56
0.61
0.42
0.43

0.28
0.16
0.08
0.30
0.08
0.16
15.91
6.29
4.75
0.24
0.10
0.08
3.13
1.66
0.38
2.23
0.97
0.34
0.39
0.37
0.31
0.16
0.07
1.52
0.40
0.33
0.10
0.95
0.65
0.79
0.25
1.22
1.06
1.20
0.97

0.31
0.16
0.06
0.15
0.07
0.14
8.82
2.90
2.03
0.15
0.08
0.02
0.89
0.48
0.22
0.84
0.67
0.19
0.13
0.36
0.20
0.12
0.06
0.75
0.18
0.15
0.08
1.17
0.44
0.54
0.24
0.94
0.79
0.70
0.73

hyperparameter ğœ† and compare the results with DeepPerf. Figure 2 shows the prediction MREs
of the two methods under different hyperparameter values logarithmically spaced in the range
[0.001, 100]. We present the results on 9 subject systems except the HSMGP system, since our
method seems to make insignificant improvement on the HSMGP system from Table 3. We can
observe that HINNPerf is much less sensitive to the changes of regularization hyperparameter than
DeepPerf on most systems, demonstrating better robustness of our model. Hence, we conclude
that the hierarchical ğ¿1 regularization strategy makes HINNPerf more robust to the regularization
hyperparameter and thus we can search for the optimal hyperparameter value on a much smaller
space.

Impact of Interaction Order. In our experiment, HINNPerf chooses the interaction order ğ‘š
4.5.2
(i.e., the number of blocks) from {2, 3, 4, 5} and learns the best order value for each system from
the data. We set the upper limit of ğ‘š to 5 degrees rigorously based on the findings of previous
work [42, 43, 45] that the interaction order of configurable systems is at most five. Moreover, it is
necessary to investigate how different values of ğ‘š affect the efficacy of HINNPerf on each system.
Hence, we fix the hyperparameter ğ‘š to a default value of 1, 2, 3, 4, 5, 6 respectively and train the
HINNPerf to predict the performance of the ten subject systems. Here we additionally evaluate
ğ‘š = 6 to see how the HINNPerf behaves when ğ‘š > 5. Figure 3 presents the prediction errors of
HINNPerf under different interaction orders. It can be observed that each system has its own best
interaction order and increasing or decreasing the optimal value of ğ‘š will reduce the performance
prediction accuracy. We thus conclude that learning the interaction order ğ‘š from the data of each

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:19

Fig. 2. Results of hyperparameter sensitivity tests. Within each subfigure, the x-axis shows values of the
regularization hyperparameter ğœ† while the y-axis shows the prediction MREs. And the std means the standard
deviation of all points on the same polyline.

Fig. 3. Prediction results of each system under different interaction orders. Within each subfigure, the x-axis
shows values of the interaction order ğ‘š while the y-axis shows the prediction MREs.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:20

J. Cheng et al.

system is better than setting a default value manually. Besides, we find that setting ğ‘š = 6 does not
cause significant differences4 with ğ‘š â‰¤ 5, showing that it is possible for the HINNPerf to learn the
information of interaction order higher than 5. However, to keep the consistency with previous
work [42, 43, 45], we only consider ğ‘š â‰¤ 5 in this paper and leave the exploration with ğ‘š > 5 to
future work.

4.6 RQ4: Model Practicality
4.6.1 Coupling Complexity. We leverage the hierarchical architecture of HINNPerf to investigate
whether the performance of the configurable systems is mainly affected by lower- or higher-order
interactions, so as to provide additional insights into the complexity of coupling among configu-
ration options. As discussed in Section 3.5.1, we first train the HINNPerf model with the training
dataset of a configurable system, and then predict the values of each partial performance function
ğ‘“ğ‘— (Â·) and the final output ğ‘“ (Â·) of Equation (2) on the testing dataset. We measure the contribution of
each partial function ğ‘“ğ‘— (Â·) to the whole performance function ğ‘“ (Â·) through computing the average
percentage:

1

âˆ‘ï¸

ğ‘ƒğ‘“ğ‘— =

ğ‘“ğ‘— (xğ‘— )
ğ‘“ (o)
where T denotes the testing dataset, ğ‘“ (o) is the performance value of configuration o, ğ‘“ğ‘— (xğ‘— ) is the
ğ‘—-th partial performance value of configuration o, ğ‘š is the number of subfunctions/blocks learned
by HINNPerf. As we describe in Section 3.1, different subfunctions ğ‘“ğ‘— (Â·) learn the influences of
different interaction orders. Therefore, the average percentage ğ‘ƒğ‘“ğ‘—
allows us to measure how much
of the ğ‘—-th order interaction affects the system performance.

, 1 â‰¤ ğ‘— â‰¤ ğ‘š,

|T |

oâˆˆT

(8)

Figure 4 shows the contributions of different partial functions on the 10 subject systems, from

which we obtain the following important observations:

â€¢ Most configurable software systems have up to 3 partial performance functions with the
second and the third subfunctions ğ‘“2(Â·), ğ‘“3(Â·) having the most significant influence on the
system performance. Since subfunctions ğ‘“2(Â·) and ğ‘“3(Â·) of our method aims to learn partial
performance values of higher-order interactions, this observation implies that multi-way
hierarchical interactions are common in configurable software systems and the most common
interaction patterns might be two-way and three-way. These findings are consistent with
previous work [42] and demonstrate the feasibility of employing a hierarchical interaction
architecture such as HINNPerf to model the interactions among configuration options.
â€¢ For the system VP9 where the lower-order subfunction ğ‘“1(Â·) has the major influence on the
system performance, HINNPerf tells us that the configuration options of this system are
relatively decoupled and have simple interactions. Accordingly, when using systems such as
VP9, users can make moderately casual decisions on configuration selections without paying
much attention to feature interactions. And developers can optimize each configuration
option relatively separately to improve software performance.

â€¢ For the LRZIP system, however, the lower-order subfunction ğ‘“1(Â·) has a significant positive
influence on the system performance while higher-order subfunctions ğ‘“2(Â·) and ğ‘“3(Â·) have
negative influences, implying more complex relationships among configuration options. This
may warn developers and users to carefully consider the feature interactions when deploying
and using the system. Also, the JavaGC system should have similar concerns.

The observations above demonstrate that our HINNPerf model provides some insights about
which kind of interactions have the most siginificant influence on the system performance. And

4We have conducted the Wilcoxon rank sum test [7] between the results of ğ‘š = 5 and ğ‘š = 6, and found that the ğ‘-value is
bigger than 0.05.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:21

Fig. 4. Contributions of different partial functions to the system performance. Note that different systems
might have different numbers of partial functions (e.g., x264 has 3 partial functions while BDB-J has 4 partial
functions) since the number of partial functions ğ‘š is a hyperparameter learned by HINNPerf.

(a) ğ‘š = 3

(b) ğ‘š = 4

(c) ğ‘š = 5

(d) ğ‘š = 6

Fig. 5. Contributions of different partial functions to the system performance under ğ‘š = 3, 4, 5, 6, respectively.

we believe that these insights can help users and developers better understand the complexity of
system coupling and make more rational decisions when deploying and using configurable systems.
Note that HINNPerf automatically learns these insights from data without any manual intervention,
which ensures the objectivity and the efficiency of our model.

While employing the HINNPerf to automatically select the best value of ğ‘š for each system seems
promising, we also interested in how HINNPerf reveals the coupling complexity when a default
value of ğ‘š is used. Besides, for the HSMGP system in Figure 4, it could be worth testing for the six

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:22

J. Cheng et al.

levels of interactions (i.e., ğ‘š = 6). Hence, we fix the hyperparameter ğ‘š to a default value of 3, 4, 5, 6
respectively and train the HINNPerf to learn the contribution of each partial function. Figure 5
shows the results under different values of ğ‘š, most of which are consistent with Figure 4. For
example, Figure 4 shows that for the x264, POLLY, Dune MGS, HIPAğ‘ğ‘ , JavaGC and SaC systems, the
third-order interaction (i.e., ğ‘ƒğ‘“3) has the most significant influence on the system performance. This
is also true in Figure 5, no matter ğ‘š = 3, 4, 5 or 6. And Figure 5d shows that the contribution of the
6-th interaction order has insignificant contribution to the performance of all systems, including the
HSMGP. Therefore, we believe that HINNPerf can still capture the importance of each interaction
order when a (proper) default value of ğ‘š is used.

4.6.2 Option Significance. Based on previous insights about the percentage importance of different
interaction orders, we further explore the significance of every configuration option in each
interaction level. Recognizing the option significance in different interaction orders could help us
pinpoint the options that have great impact on the performance and also provide some insights
about the interaction patterns. Here we employ the Integrated Gradients [47] method to assign a
significance score to each configuration option:

Score(ğ‘œğ‘– ) = (ğ‘œğ‘– âˆ’ ğ‘œ â€²
ğ‘– )

âˆ« 1

ğ›¼=0

ğœ•ğ‘“ (oâ€² + ğ›¼ (o âˆ’ oâ€²))
ğœ•ğ‘œğ‘–

dğ›¼, 1 â‰¤ ğ‘– â‰¤ ğ‘›,

(9)

where ğ‘œğ‘– is the ğ‘–-th option of the configuration o, oâ€² âˆˆ Rğ‘› is the baseline zero vector and ğ‘“ (Â·) denotes
the performance function. We compute the significance score of each option ğ‘œğ‘– in different blocks
of HINNPerf and report the option significance of three subject systems x264, SaC and HIPAğ‘ğ‘ in
Figure 6 for examples. Comparing the option significance in different configurable systems, we can
further extract some valuable information for both users and developers:

â€¢ For most configurable software systems, only a small number of configurations and their
interactions have significant impact on the performance. Figure 6 shows that there are
only 3/16, 4/60 and 4/33 significant configuration options in the x264, SaC and HIPAğ‘ğ‘
systems, respectively. This finding is also consistent with all the previous work [13, 42,
43]. The main contribution of our work here is that we recognize the specific significant
options through automatic learning without any manual efforts. For example, we can identify
that the performance of the x264 system is mainly affected by three significant options
no-mixed-refs, ref-1, and ref-9 from Figure 6a.

â€¢ Combining with insights about coupling complexity in the previous section, we can quickly
pinpoint configuration options that might have the greatest impact on the system performance.
For instance, Figure 4 shows that the third-order interactions (ğ‘ƒğ‘“3 ) contribute the most to the
SaC system performance, which corresponds to Block 3 in Figure 6b. Accordingly, we could
roughly infer that the most significant options dcr and maxwlur in Block 3 has the greatest
influence on the performance of SaC, so as to remind users and developers to pay special
attention to these two options when using the system.

â€¢ The option significance in different interaction orders also reveals some potential interaction
patterns of the configurable system. For the HIPAğ‘ğ‘ system in Figure 6c, the LocalMemory
and the pixelPerThread options have the greatest importance in the first-order interaction
(Block 1). After entering into the second-order interaction (Block 2), the contribution of
the CUDA option increases. Finally in the third-order interaction (Block 3), the CUDA, Ldg,
and LocalMemory options maintain positive significance while the pixelPerThread option
changes into negative significance. This observation inspires us that the HIPAğ‘ğ‘ system might

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:23

(a) Option Significance of x264

(b) Option Significance of SaC

(c) Option Significance of HIPAğ‘ğ‘

Fig. 6. The significance score of each configuration option in the three subject systems x264, SaC and HIPAğ‘ğ‘ .
Block ğ‘— denotes the ğ‘—-th interaction order of the system, which corresponds to the subfunction ğ‘“ğ‘— (Â·).

have interaction patterns in the following form:

ğ¼ğ‘ (LocalMemory, pixelPerThread) + ğ¼ğ‘ (CUDA, LocalMemory, pixelPerThread)
+ ğ¼ğ‘Ÿ (CUDA, Ldg, LocalMemory, âˆ’pixelPerThread),

(10)

where ğ¼ğ‘, ğ¼ğ‘, ğ¼ğ‘Ÿ could be arbitrary interaction functions. Such an interaction form is intuitive
since the usage and communication of the local memory between CPU and GPU (CUDA) can
significantly affect the system performance, while the number of pixels calculated per thread
is also highly related to the local memory capacity and the GPU computing power.

In summary, by inspecting the option significance in different interaction orders learned by
HINNPerf, one can efficiently locate the small subset of configuration options and their interactions
that have the greatest impact on system performance, instead of wasting a huge amount of time
searching hard through a large number of configurations. Besides, HINNPerf can provide some
insights on potential interaction patterns of the system through the hierarchical interaction archi-
tecture. We believe these advantages are more actionable for users and developers to deploy and
test the configurable software systems.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:24

J. Cheng et al.

Fig. 7. Time cost of the two deep learning methods on the 10 subject systems

4.7 RQ5: Time Cost
Although our method attains higher prediction accuracy and is more robust to the regularization
hyperparameter than other approaches, it is also a more complex architecture with quantities
of trainable parameters. Therefore, to evaluate the practicality and feasibility of our approach, it
is necessary to measure the time consumed by the training and the testing process of HINNPerf.
Since we implement the two deep learning methods DeepPerf and HINNPerf under CPU and GPU
environment while the other methods SPLConqueror, DECART and RF under the environment
without GPU, we only report the time cost of DeepPerf and HINNPerf on 10 subject systems with
maximum sample size. As shown in Figure 7 (left), for all the systems with various configurable
options, it takes HINNPerf 14 - 16 minutes to do model training and hyperparameter searching,
while DeepPerf spends 5 - 13 minutes. Even though HINNPerf takes longer time to train the
performance model than DeepPerf, the time cost of HINNPerf is still acceptable. As for testing
process, Figure 7 (right) shows that both methods perform similarly with time cost less than 0.2
seconds, which is consistent with our analysis in Section 3.4 that HINNPerf only increases constant
complexity than DeepPerf.

In addition to model complexity, the main reason why the training of our method is more time-
consuming than DeepPerf is that DeepPerf employs a more efficient hyperparameter search strategy.
It first searches the optimal number of hidden layers for a non-regularized FNN. After applying
the same number of hidden layers as the non-regularized FNN on the DeepPerf architecture, it
then searches for the best regularization hyperparameter value. Such a separation hyperparameter
search strategy can significantly reduce the hyperparameter space, but it is not suitable for all deep
neural network architectures (i.e., not universally applicable). Whatâ€™s worse, this strategy risks
hurting model accuracy because the optimal hyperparameters of the non-regularized FNN may not
be optimal for DeepPerf. Instead, we directly search all optimal hyperparameters for the HINNPerf
architecture, slightly increasing the training time but guaranteeing high prediction accuracy. Besides,
compared to the search strategy with accuracy drawdown risk, it is more reasonable to develop a
robust network structure to reduce the hyperparameter space.

4.8 Discussions

Strengths and Limitations. The main strength of our HINNPerf method is that it can model
4.8.1
complicated interactions among the configuration options and attain higher prediction accuracy on
various configurable systems than other state-of-the-art approaches. As shown in our experiments,
for most of the software systems with binary and numeric options, compared to other baselines,
HINNPerf achieves higher prediction accuracy with less sample. For software systems with binary
options, HINNPerf can achieve better or comparable results compared to other methods.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:25

HINNPerf â€™s second strength is that it provides additional insights about the contributions of
different interactions to the system performance and significant configuration options in different
interation orders, which could reveal some potential interaction patterns of the configurable
system and help users and developers efficiently make rational decisions on configurations. We
emphasize that unlike the SPLConqueror method [42], HINNPerf learns these insights from data
automatically without any human efforts to design the interaction patterns. Another automated
approach DeepPerf [13] does not show any additional insights in addition to performance prediction.
Hence, compared to other performance models, our method is more efficient and helpful for users
and developers.

Finally, the third strength of HINNPerf is that it is less sensitive to the regularization hyperpa-
rameter and more robust than another deep learning method DeepPerf [13]. Therefore, our method
effectively reduces the hyperparameter tuning cost and improves the training efficiency.

The major limitation of HINNPerf is that it takes longer time to train than other baseline methods.
However, considering the benefits of accuracy improvement and interaction insights brought by
HINNPerf, it is worth taking slightly more time to train a more useful performance model. Also, we
will optimize the network structure and explore more efficient deep learning methods in the future
work.

4.8.2 Difference with DeepPerf. Although our method is designed based on the feedforward neural
network (FNN) as DeepPerf, HINNPerf is significantly different from DeepPerf in the following
aspects. First, HINNPerf is a deep neural network model specially designed for performance
prediction of configurable systems, which considers the unique characteristics of hierarchical
interaction patterns in configurable systems. However, DeepPerf is a general FNN model that
has been widely used in deep learning field without specific design for the configurable systems.
From this point of view, HINNPerf makes an essential contribution to the field of deep learning-
based performance prediction than DeepPerf. Second, the hierarchical network architecture design
allows HINNPerf to recognize important options in different interaction orders and reveal some
potential interaction patterns of configurable systems, which could help users and developers better
understand how configurable systems work and efficiently identify significant options affecting
the performance. This makes HINNPerf more interpretable and practical than DeepPerf which
cannot provide any information about the internal interaction structure in configurable systems.
We believe that the advantages guarantee the novelty and contribution of our method.

4.8.3 Effectiveness of Neural Networks. Although deep neural networks generally requires large
data for training, the superiority of DeepPerf and HINNPerf over traditional machine learning
(ML) methods such as SPLConqueror, DECART and RF shows that deep neural networks are also
effective in the performance prediction domain with small data. The main reason is probably that
the excellent non-linear learning ability enables neural networks to better capture the complex
interactions between different configuration options. Although traditional ML methods are more
intuitive to process small data, the poor learning ability prevents them from learning the complicated
interactions and achieving higher accuracy in the performance prediction task domain. Instead,
as demonstrated in Section 4.6 (RQ4), the non-linear activation mechanism in the neural network
allows it to extract richer information from the data to compensate for the shortcomings of the
small sample. Therefore, it is necessary to employ deep neural networks to better learn the complex
hierarchical interaction patterns in configurable systems and attain higher accuracy for performance
prediction, which has been demonstrated by the experiment results in Section 4.3 (RQ1).

4.8.4 Threats to Validity. For internal validity, to minimize the measurement bias of random
sampling, we select samples of different sizes from each subject system to train all the methods, and

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:26

J. Cheng et al.

then evaluate the prediction accuracy of each method on a test dataset. The test dataset excludes all
the configurations of the training and validation datasets so as to evaluate the real predictive ability
of each model. Besides, we repeat the random sampling, training and testing process 30 times and
report the averages and the 95% confidence intervals of the mean relative error for analysis. In this
way, we believe that we control the measurement bias sufficiently and the experimental results
reported in this paper are solid.

To increase external validity, we evaluate all the approaches on public datasets of 10 configurable
systems. These systems are from different domains, with varying number of configuration options
and different implementation languages. Although there is no guarantee that our method works
for all other configurable systems, we are confident that the extensive experiments in this paper
control this threat sufficiently.

The workload used to measure the configurable systems is also an important factor which can
influence system performance but ignored by our work. The related studies [11â€“14, 22, 42, 43, 52]
have not considered the workload factor either, which may be attributed to the great difficulty in
quantifying the factor. According to previous work [22, 42, 43], all the datasets of the ten subject
systems used in our experiments are collected under different workload settings, probably leading
to mix workload in the data. However, in this work, we focus on designing a novel model for
more accurate performance prediction. Thus, for fair comparison with the previous work, we
directly adopt the widely-used public datasets released by [22, 42, 43]. With same datasets for
comparison, the mix workload would not be an issue in our study, and the influence of workload
on the performance prediction task is avoided. The experiment results in Table 3 demonstrates that
our method can achieve the state-of-the-art accuracy in different workload settings. In future work,
we will explore the influence of the workload on performance modeling.

5 RELATED WORK
Modern software systems are becoming increasingly complex and large-scale. Since poor perfor-
mance can often be the cause of software project failure, how to manage system performance
concerns along the software lifecycle has attracted great attention in the research and software in-
dustry communities [20, 53â€“55]. For large-scale configurable systems, understanding the influences
of the configuration options and their interactions on system performance plays an important role
in software testing and maintenance phases [14].

Recently, software researchers have conducted a large amount of work on performance prediction
of highly configurable systems. In Section 1, we have discussed pros and cons of most state-of-the-
art approaches, including SPLConqueror [42â€“44], CART/DECART [11, 12], Fourier methods [14, 52],
and DeepPerf [13]. Throughout the history of performance models, the evolution of prediction
methods tends to sacrifice the interpretability of the model to improve the prediction accuracy.
SPLConqueror [42â€“44] is an early linear regression model that achieves low accuracy but can
explicitly show the influences of configuration options and their interactions through regression
coefficients. Later, CART/DECART [11, 12] employs decision trees to improve the prediction accuracy
for binary configurable systems, but can only qualitatively measures the feature importance through
the tree structure. Recently, DeepPerf [13] utilizes deep feedforward neural networks to greatly
improve the performance prediction accuracy, but with the model interpretability completely lost.
On the other hand, the major contribution of Fourier learning [52] is to derive a sample size that
guarantees a theoretical boundary of the prediction accuracy. However, it attains no significant
improvement in model interpretability and accuracy compared to SPLConqueror and CART/DECART,
which is why we do not compare our model with Fourier methods in this paper.

Although our proposed HINNPerf approach mainly focuses on achieving higher accuracy with
less sample for performance prediction of configurable systems, we also make some progress in

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:27

model interpretability by introducing the hierarchical interaction network architecture. In this
way, users and developers can better understand how different interactions influence the system
performance and make rational configuration decisions to maximize software efficacy. Besides,
there is also some work on increasing explainability of the performance model by measuring
the uncertainty in performance estimations with Bayesian method [5]. We will also consider the
uncertainty measurement of our method in the future work.

Another related research topic is to select an optimal sample of configurations. Sayyad et
al. [40, 41] propose evolutionary methods to select optimal features under multiple objectives. Later,
Christopher et al. [16] extend the multi-objective search-based optimization with constraint solver
and improve Software Product Line (SPL) feature selection. Other researchers focus on selecting
proper configuration option under certain coverage criterion [21, 25, 30]. Recently, Christian et
al. [22] propose a distance-based sampling strategy to better cover different kinds of interactions
among configuration options in the sample set. Since we aim to propose a novel learning method
for performance prediction in this paper, our method only employs random sampling strategy to
build a performance model. In fact, we can combine our approach with different sampling work to
further improve the model accuracy.

6 CONCLUSION
In this paper, we propose HINNPerf, a novel hierarchical interaction neural network architecture for
performance prediction of highly configurable systems. Our method decomposes the whole deep
neural network into multiple hierarchical blocks and employs the embedding method to model the
complex interactions among configuration options. Besides, we devise a hierarchical regularization
strategy to ensure the model accuracy when trained on a small sample. The experimental results on
public datasets show that HINNPerf can better learn the complicated interactions in configurable
systems and achieve better performance prediction accuracy with less data, when compared to
other state-of-the-art methods. Furthermore, our approach provides additional insights for users
and developers and is more robust than the advanced deep learning method DeepPerf [13].

For future research, the proposed model can be further improved by incorporating effective
sampling heuristics [22, 42]. We will also explore the uncertainty measurement methods [5] to
increase the interpretability of our approach.

ACKNOWLEDGMENTS
The research is supported by the Key-Area Research and Development Program of Guangdong
Province (2020B010165003), the National Natural Science Foundation of China under project
(62032025, 62002084), Stable support plan for colleges and universities in Shenzhen under project
(GXWD20201230155427003-20200730101839009). The corresponding author is Zibin Zheng.

REFERENCES
[1] MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,
Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,
Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016.
TensorFlow: A System for Large-Scale Machine Learning. In Proceedings of the 12th USENIX Conference on Operating
Systems Design and Implementation (Savannah, GA, USA) (OSDIâ€™16). USENIX Association, USA, 265â€“283.

[2] Oren Barkan and Noam Koenigstein. 2016. Item2Vec: Neural Item Embedding for Collaborative Filtering. In 2016 IEEE
26th International Workshop on Machine Learning for Signal Processing (MLSP). 1â€“6. https://doi.org/10.1109/mlsp.2016.
7738886

[3] Yoshua Bengio. 2009. Learning Deep Architectures for AI. Found. Trends Mach. Learn. 2, 1 (jan 2009), 1â€“127. https:

//doi.org/10.1561/2200000006

[4] Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Language Model.

J. Mach. Learn. Res. 3, null (mar 2003), 1137â€“1155.

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:28

J. Cheng et al.

[5] Johannes Dorn, Sven Apel, and Norbert Siegmund. 2020. Mastering Uncertainty in Performance Estimations of
Configurable Software Systems. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software
Engineering (Virtual Event, Australia) (ASE â€™20). Association for Computing Machinery, New York, NY, USA, 684â€“696.
https://doi.org/10.1145/3324884.3416620

[6] Ronen Eldan and Ohad Shamir. 2016. The Power of Depth for Feedforward Neural Networks. In 29th Annual Conference
on Learning Theory (Proceedings of Machine Learning Research, Vol. 49). PMLR, Columbia University, New York, New
York, USA, 907â€“940. http://proceedings.mlr.press/v49/eldan16.html

[7] Jean Dickinson Gibbons and Subhabrata Chakraborti. 2011. Nonparametric Statistical Inference. Springer Berlin

Heidelberg, Berlin, Heidelberg, 977â€“979. https://doi.org/10.1007/978-3-642-04898-2_420

[8] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep Sparse Rectifier Neural Networks. In Proceedings of the
Fourteenth International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research,
Vol. 15). JMLR Workshop and Conference Proceedings, Fort Lauderdale, FL, USA, 315â€“323. http://proceedings.mlr.
press/v15/glorot11a.html

[9] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.

org.

[10] Alexander Grebhahn, Norbert Siegmund, and Sven Apel. 2019. Predicting Performance of Software Configurations:

There is no Silver Bullet. CoRR abs/1911.12643 (2019). arXiv:1911.12643 http://arxiv.org/abs/1911.12643

[11] Jianmei Guo, Krzysztof Czarnecki, Sven Apely, Norbert Siegmundy, and Andrzej Wasowski. 2013. Variability-Aware
Performance Prediction: A Statistical Learning Approach. In Proceedings of the 28th IEEE/ACM International Conference
on Automated Software Engineering (Silicon Valley, CA, USA) (ASEâ€™13). IEEE Press, 301â€“311. https://doi.org/10.1109/
ASE.2013.6693089

[12] Jianmei Guo, Dingyu Yang, Norbert Siegmund, Sven Apel, Atrisha Sarkar, Pavel Valov, Krzysztof Czarnecki, Andrzej
Wasowski, and Huiqun Yu. 2018. Data-Efficient Performance Learning for Configurable Systems. Empirical Softw.
Engg. 23, 3 (June 2018), 1826â€“1867. https://doi.org/10.1007/s10664-017-9573-6

[13] Huong Ha and Hongyu Zhang. 2019. DeepPerf: Performance Prediction for Configurable Software with Deep Sparse
Neural Network. In Proceedings of the 41st International Conference on Software Engineering (Montreal, Quebec, Canada)
(ICSE â€™19). IEEE Press, 1095â€“1106. https://doi.org/10.1109/ICSE.2019.00113

[14] Huong Ha and Hongyu Zhang. 2019. Performance-Influence Model for Highly Configurable Software with Fourier
Learning and Lasso Regression. In 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME
2019, Cleveland, OH, USA, September 29 - October 4, 2019. IEEE, 470â€“480. https://doi.org/10.1109/ICSME.2019.00080

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 770â€“778. https://doi.org/10.1109/CVPR.2016.90
[16] Christopher Henard, Mike Papadakis, Mark Harman, and Yves Le Traon. 2015. Combining Multi-Objective Search and
Constraint Solving for Configuring Large Software Product Lines. In 2015 IEEE/ACM 37th IEEE International Conference
on Software Engineering, Vol. 1. 517â€“528. https://doi.org/10.1109/ICSE.2015.69

[17] Tin Kam Ho. 1995. Random decision forests. In Proceedings of 3rd International Conference on Document Analysis and

Recognition, Vol. 1. 278â€“282 vol.1. https://doi.org/10.1109/ICDAR.1995.598994

[18] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer Feedforward Networks Are Universal

Approximators. Neural Netw. 2, 5 (jul 1989), 359â€“366. https://doi.org/10.1016/0893-6080(89)90020-8

[19] Sheng-hung Hu, Yikang Li, and Baoxin Li. 2016. Video2vec: Learning semantic spatio-temporal embeddings for video
representation. In 23rd International Conference on Pattern Recognition, ICPR 2016, CancÃºn, Mexico, December 4-8, 2016.
IEEE, 811â€“816. https://doi.org/10.1109/ICPR.2016.7899735

[20] Gang Huang, Hong Mei, and Fuqing Yang. 2006. Runtime recovery and manipulation of software architecture of
component-based systems. Autom. Softw. Eng. 13, 2 (2006), 257â€“281. https://doi.org/10.1007/s10515-006-7738-4
[21] Martin Fagereng Johansen, Ã˜ystein Haugen, and Franck Fleurey. 2012. An Algorithm for Generating T-Wise Covering
Arrays from Large Feature Models. In Proceedings of the 16th International Software Product Line Conference - Volume 1
(Salvador, Brazil) (SPLC â€™12). Association for Computing Machinery, New York, NY, USA, 46â€“55. https://doi.org/10.
1145/2362536.2362547

[22] Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, Jianmei Guo, and Sven Apel. 2019. Distance-Based
Sampling of Software Configuration Spaces. In Proceedings of the 41st International Conference on Software Engineering
(Montreal, Quebec, Canada) (ICSE â€™19). IEEE Press, 1084â€“1094. https://doi.org/10.1109/ICSE.2019.00112

[23] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference
on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio
and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980

[24] Kamran Kowsari, Donald E. Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi, Matthew S. Gerber, and Laura E.
Barnes. 2017. HDLTex: Hierarchical Deep Learning for Text Classification. In 2017 16th IEEE International Conference
on Machine Learning and Applications (ICMLA). 364â€“371. https://doi.org/10.1109/ICMLA.2017.0-134

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems

1:29

[25] Yu Lei, Raghu Kacker, D. Richard Kuhn, Vadim Okun, and James Lawrence. 2008.

IPOG-IPOG-D: Efficient Test
Generation for Multi-Way Combinatorial Testing. Softw. Test. Verif. Reliab. 18, 3 (sep 2008), 125â€“148. https://doi.org/
10.1002/stvr.381

[26] Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. 1993. Multilayer feedforward networks with
a nonpolynomial activation function can approximate any function. Neural Netw. 6, 6 (1993), 861â€“867. https:
//doi.org/10.1016/S0893-6080(05)80131-5

[27] Shiyu Liang and R. Srikant. 2016. Why Deep Neural Networks for Function Approximation? CoRR abs/1610.04161

(2016). arXiv:1610.04161 http://arxiv.org/abs/1610.04161

[28] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan Yuille, and Li Fei-Fei. 2019. Auto-
DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation. In 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 82â€“92. https://doi.org/10.1109/CVPR.2019.00017

[29] Yulong Lu and Jianfeng Lu. 2020. A Universal Approximation Theorem of Deep Neural Networks for Expressing

Probability Distributions. CoRR abs/2004.08867 (2020). arXiv:2004.08867 https://arxiv.org/abs/2004.08867

[30] Dusica Marijan, Arnaud Gotlieb, Sagar Sen, and Aymeric Hervieu. 2013. Practical Pairwise Testing for Software
Product Lines. In Proceedings of the 17th International Software Product Line Conference (Tokyo, Japan) (SPLC â€™13).
Association for Computing Machinery, New York, NY, USA, 227â€“235. https://doi.org/10.1145/2491627.2491646
[31] TomÃ¡s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in

Vector Space. http://arxiv.org/abs/1301.3781

[32] Tomas Mikolov, Jiri Kopecky, Lukas Burget, Ondrej Glembek, and Jan ?Cernocky. 2009. Neural Network Based Language
Models for Highly Inflective Languages. In Proceedings of the 2009 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP â€™09). IEEE Computer Society, USA, 4725â€“4728. https://doi.org/10.1109/ICASSP.2009.4960686
[33] Stefan MÃ¼hlbauer, Sven Apel, and Norbert Siegmund. 2019. Accurate Modeling of Performance Histories for Evolving
Software Systems. In Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
(San Diego, California) (ASE â€™19). IEEE Press, 640â€“652. https://doi.org/10.1109/ASE.2019.00065

[34] Vinod Nair and Geoffrey E. Hinton. 2010. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings
of the 27th International Conference on Machine Learning (Haifa, Israel) (ICMLâ€™10). Omnipress, Madison, WI, USA,
807â€“814.

[35] Andrew Y. Ng. 2004. Feature Selection, L1 vs. L2 Regularization, and Rotational Invariance. In Proceedings of the
Twenty-First International Conference on Machine Learning (Banff, Alberta, Canada) (ICML â€™04). ACM, New York, NY,
USA, 78. https://doi.org/10.1145/1015330.1015435

[36] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2019. N-BEATS: Neural basis expansion
analysis for interpretable time series forecasting. CoRR abs/1905.10437 (2019). arXiv:1905.10437 http://arxiv.org/abs/
1905.10437

[37] Fabian Pedregosa, GaÃ«l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and Ã‰douard Duchesnay. 2011. Scikit-Learn: Machine Learning in Python. J. Mach.
Learn. Res. 12, null (nov 2011), 2825â€“2830.

[38] Tomaso A. Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. 2017. Why and When
Can Deep - but Not Shallow - Networks Avoid the Curse of Dimensionality: a Review. Int. J. Autom. Comput. 14, 5
(2017), 503â€“519. https://doi.org/10.1007/s11633-017-1054-2

[39] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. 2017. On the Expressive Power
of Deep Neural Networks. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of
Machine Learning Research, Vol. 70). PMLR, International Convention Centre, Sydney, Australia, 2847â€“2854. http:
//proceedings.mlr.press/v70/raghu17a.html

[40] Abdel Salam Sayyad, Joseph Ingram, Tim Menzies, and Hany Ammar. 2013. Scalable Product Line Configuration: A
Straw to Break the Camelâ€™s Back. In Proceedings of the 28th IEEE/ACM International Conference on Automated Software
Engineering (Silicon Valley, CA, USA) (ASEâ€™13). IEEE Press, 465â€“474. https://doi.org/10.1109/ASE.2013.6693104
[41] Abdel Salam Sayyad, Tim Menzies, and Hany Ammar. 2013. On the value of user preferences in search-based software
engineering: A case study in software product lines. In 2013 35th International Conference on Software Engineering
(ICSE). 492â€“501. https://doi.org/10.1109/ICSE.2013.6606595

[42] Norbert Siegmund, Alexander Grebhahn, Sven Apel, and Christian KÃ¤stner. 2015. Performance-Influence Models for
Highly Configurable Systems. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering
(Bergamo, Italy) (ESEC/FSE 2015). ACM, New York, NY, USA, 284â€“294. https://doi.org/10.1145/2786805.2786845
[43] Norbert Siegmund, Sergiy S. Kolesnikov, Christian KÃ¤stner, Sven Apel, Don Batory, Marko RosenmÃ¼ller, and
Gunter Saake. 2012. Predicting Performance via Automated Feature-Interaction Detection. In Proceedings of
the 34th International Conference on Software Engineering (Zurich, Switzerland) (ICSE â€™12). IEEE Press, 167â€“177.
https://doi.org/10.1109/ICSE.2012.6227196

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:30

J. Cheng et al.

[44] Norbert Siegmund, Marko Rosenmuller, Christian Kastner, Paolo G. Giarrusso, Sven Apel, and Sergiy S. Kolesnikov.
2011. Scalable Prediction of Non-Functional Properties in Software Product Lines. In Proceedings of the 2011 15th
International Software Product Line Conference (SPLC â€™11). IEEE Computer Society, USA, 160â€“169. https://doi.org/10.
1109/SPLC.2011.20

[45] Norbert Siegmund, Alexander von Rhein, and Sven Apel. 2013. Family-Based Performance Measurement. In Proceedings
of the 12th International Conference on Generative Programming: Concepts & Experiences (Indianapolis, Indiana, USA)
(GPCE â€™13). ACM, New York, NY, USA, 95â€“104. https://doi.org/10.1145/2517208.2517209

[46] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout:
A Simple Way to Prevent Neural Networks from Overfitting. J. Mach. Learn. Res. 15, 56 (jan 2014), 1929â€“1958.
http://jmlr.org/papers/v15/srivastava14a.html

[47] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings
of the 34th International Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia) (ICMLâ€™17). JMLR.org,
3319â€“3328.

[48] Robert Tibshirani. 1996. Regression Shrinkage and Selection via the Lasso. J. R. Stat. Soc. Ser. B Methodol. 58, 1 (1996),

267â€“288. https://doi.org/10.1111/j.2517-6161.1996.tb02080.x

[49] Tianyin Xu, Long Jin, Xuepeng Fan, Yuanyuan Zhou, Shankar Pasupathy, and Rukma Talwadker. 2015. Hey, You Have
given Me Too Many Knobs!: Understanding and Dealing with over-Designed Configuration in System Software. In
Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering (Bergamo, Italy) (ESEC/FSE 2015).
ACM, New York, NY, USA, 307â€“319. https://doi.org/10.1145/2786805.2786852

[50] Dmitry Yarotsky. 2017. Error bounds for approximations with deep ReLU networks. Neural Netw. 94 (2017), 103â€“114.

https://doi.org/10.1016/j.neunet.2017.07.002

[51] Wen Zhang, Yuhang Du, Taketoshi Yoshida, and Ye Yang. 2019. DeepRec: A deep neural network approach to
recommendation with item embedding and weighted loss function. Inf. Sci. 470 (2019), 121â€“140. https://doi.org/10.
1016/j.ins.2018.08.039

[52] Yi Zhang, Jianmei Guo, Eric Blais, and Krzysztof Czarnecki. 2015. Performance Prediction of Configurable Software
Systems by Fourier Learning. In Proceedings of the 30th IEEE/ACM International Conference on Automated Software
Engineering (Lincoln, Nebraska) (ASE â€™15). IEEE Press, 365â€“373. https://doi.org/10.1109/ASE.2015.15

[53] Yilei Zhang, Zibin Zheng, and Michael R. Lyu. 2014. An Online Performance Prediction Framework for Service-Oriented
Systems. IEEE Trans. Syst. Man Cybern.: Syst. 44, 9 (2014), 1169â€“1181. https://doi.org/10.1109/TSMC.2013.2297401
[54] Peilin Zheng, Zibin Zheng, Xiapu Luo, Xiangping Chen, and Xuanzhe Liu. 2018. A Detailed and Real-Time Performance
Monitoring Framework for Blockchain Systems. In Proceedings of the 40th International Conference on Software
Engineering: Software Engineering in Practice (Gothenburg, Sweden) (ICSE-SEIP â€™18). ACM, New York, NY, USA, 134â€“143.
https://doi.org/10.1145/3183519.3183546

[55] Zibin Zheng, Xinmiao Wu, Yilei Zhang, Michael R. Lyu, and Jianmin Wang. 2013. QoS Ranking Prediction for Cloud

Services. IEEE Trans. Parallel Distrib. Syst. 24, 6 (2013), 1213â€“1222. https://doi.org/10.1109/TPDS.2012.285

[56] Andrey Zhmoginov and Mark Sandler. 2016. Inverting face embeddings with convolutional neural networks. CoRR

abs/1606.04189 (2016). arXiv:1606.04189 http://arxiv.org/abs/1606.04189

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2022.

