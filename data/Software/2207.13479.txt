2
2
0
2

l
u
J

7
2

]

V
C
.
s
c
[

1
v
9
7
4
3
1
.
7
0
2
2
:
v
i
X
r
a

AutoTransition: Learning to Recommend Video
Transition Eﬀects

Yaojie Shen1,2,3∗, Libo Zhang1,2∗, Kai Xu3∗, Xiaojie Jin3∗†

1 Institute of Software, Chinese Academy of Sciences
2 University of Chinese Academy of Sciences
3 ByteDance Inc.

Abstract. Video transition eﬀects are widely used in video editing to
connect shots for creating cohesive and visually appealing videos. How-
ever, it is challenging for non-professionals to choose best transitions due
to the lack of cinematographic knowledge and design skills. In this paper,
we present the premier work on performing automatic video transitions
recommendation (VTR): given a sequence of raw video shots and com-
panion audio, recommend video transitions for each pair of neighboring
shots. To solve this task, we collect a large-scale video transition dataset
using publicly available video templates on editing softwares. Then we
formulate VTR as a multi-modal retrieval problem from vision/audio to
video transitions and propose a novel multi-modal matching framework
which consists of two parts. First we learn the embedding of video tran-
sitions through a video transition classiﬁcation task. Then we propose a
model to learn the matching correspondence from vision/audio inputs to
video transitions. Speciﬁcally, the proposed model employs a multi-modal
transformer to fuse vision and audio information, as well as capture the
context cues in sequential transition outputs. Through both quantitative
and qualitative experiments, we clearly demonstrate the eﬀectiveness of
our method. Notably, in the comprehensive user study, our method re-
ceives comparable scores compared with professional editors while im-
proving the video editing eﬃciency by 300×. We hope our work serves
to inspire other researchers to work on this new task. The dataset and
codes are public at https://github.com/acherstyx/AutoTransition.

Keywords: video transition eﬀects recommendation, multi-modal re-
trieval, video editing

1

Introduction

With the advance of multimedia technology and network infrastructures, video
is ubiquitous, occurring in numerous everyday activities such as education, en-
tertainment, surveillance, etc. There is a massive amount of needs for people to
edit videos and share with others. However, video editing is challenging for non-
professionals since it is not only laborious but also needs a lot of cinematography

* Equal contribution.
† Project lead and corresponding author <jinxiaojie@bytedance.com>.

 
 
 
 
 
 
2

Y. Shen et al.

and design knowledge. Some editing tools like Adobe Premier and Apple Final
Cut Pro are developed to assist video editing, however their main target users
are professionals while novices may ﬁnd it diﬃcult to learn. Moreover, they still
lack the ability of automatic video editing, i.e., users have to manipulate videos
on their own. Recently, popular video editing tools like InShot Video Editor and
CapCut provide the function of creating videos in one-click. Nevertheless, since
they only utilize simple strategies or ﬁxed video templates and ignore the content
of input vision/audio, the quality of generated video is unsatisfactory.

The video transition eﬀects play an important role in video editing to join
shots for creating smooth and cohesive videos. In this paper, we introduce a
new task of automatic video transition recommendation (VTR) and provide a
systematic solution. Speciﬁcally, VTR is deﬁned as: given a sequence of raw video
shots and the companion audio (which can either be original sound or overwritten
music), recommend a sequence of video transitions for each neighboring shots.
Diﬀerent from conventional classiﬁcation problems which choose only one most
probable category as the output, VTR aims to provide a ranking of candidate
transition categories so that users can choose freely in practical usage.

When working on this task, we encounter following challenges. First, there is
no publicly available video transition dataset for training and evaluation. It takes
enormous eﬀorts to manually collect and annotate a large-scale video dataset.
Meanwhile, due to the large complexity and diversity of video editing, the evalu-
ation of video quality is subjective and vary from person to person. Thus during
creating the dataset, it is crucial to design proper criteria for selecting video
samples that are appealing to most. Besides dataset, solving VTR is also chal-
lenging. A good video transitions recommender should ensure top transitions
match well with both the dynamics and contents of videos and the rhythm and
theme of audio (or music). Moreover, transitions recommended at multiple video
connections should be harmonious so that the ﬁnal video is visually smooth and
uniﬁed. Being the ﬁrst eﬀort in addressing VTR, we need to take all of above
factors into consideration for delivering the optimal solution.

We start with building a video transition dataset from those video editing
templates that are publicly available on video editing tools. We design compre-
hensive rules via trials to select high-quality video templates followed by pre-
processing for reﬁnement. Afterwards, we extract video shots and corresponding
transitions (used as ground-truth in training), creating the ﬁrst large-scale video
transition dataset. More details are introduced in Sec. 4.

To ﬁgure out the best way for modeling VTR, we conduct extensive ex-
periments to compare the classiﬁcation-based and retrieval-based solutions and
ﬁnally demonstrate the latter performs better. In the classiﬁcation-based solu-
tion, the model takes neighboring video shots as inputs and output the prediction
of transition categories. The cross-entropy loss between predictions and ground-
truth transitions is used. In the retrieval-based solution, we ﬁrst pre-train a
transition classiﬁcation network to learn the embedding of transitions. Then we
propose a multi-modal transformer to learn the fused vision/audio features in
sequential video shots. A triplet margin loss is devised to minimize the distance

AutoTransition: Learning to Recommend Video Transition Eﬀects

3

between fused input features and pre-trained transition embedding. Similarly, in
testing, according to the distance calculated between input features and tran-
sition embedding, the transition categories with smaller distances are in higher
ranking position.

To conclude, our contributions are threefold:

1. We introduce a new task of automatically recommending video transition
eﬀects given video and audio inputs. We collect the ﬁrst large-scale video
transition dataset to facilitate future research on this task.

2. We formulate the transition recommendation task as a multi-modal retrieval
problem and propose a framework for learning the correspondence between in-
put vision/audio and video transitions in feature space. The proposed frame-
work is capable of fully utilizing the multi-modal input information for gen-
erating sequential transition outputs.

3. Through both quantitative and qualitative evaluation, we demonstrate that
the proposed method can successfully learn the matching from vision/audio
to transitions and generate reasonable recommendation results. Moreover,
a user study conducted to evaluate the quality of generated videos further
demonstrates the eﬀectiveness of our method.

2 Related work

Video editing. Automatic video editing is challenging due to following rea-
sons. First, the model has to fully understand the spatial-temporal context and
multi-modal information in videos to obtain semantically coherent results. Sec-
ond, video editing requires lots of professional knowledge to endow videos with
creativity and particular aesthetic taste. Third, the evaluation of the quality
of the generated videos may be subjective. Recently, there are some progresses
towards performing automatic video editing, each focusing on diﬀerent aspects.
Frey et al. [1] proposes an automatic approach to transfer the editing styles of
an edited video to the new raw shots. Wang et al. [2] builds a tool for creating
video montage based on the text description. Koorathota et al. [3] proposes a
method to perform video editing according to a short text query by utilizing
contextual and multi-modal information. Liao et al. [4] introduces a method for
music-driven video montage. Several methods focus on solving video ordering
and shot selection [1, 2, 5]. Distinguished from all tasks above, VTR is still un-
explored although its equal importance in creating high-quality videos in video
editing. In this work, we take the ﬁrst step to close the research gap.

Video transitions. Video transition is a widely used post-production tech-
nique for achieving smooth transitions between neighboring shots via special
image/video transforms. There are various kinds of transitions including straight
cuts, fades, and 3D animations among many others. To professionals, each type
of transition is with a dedicated meaning to convey speciﬁc emotions, feelings
or scene information to viewers, thus should be used meticulously. Moreover,
when multiple video transitions are used, they should work in a harmonious way

4

Y. Shen et al.

to ensure the visual uniﬁcation of the ﬁnal video. Due to above reasons, it is
diﬃcult for non-professionals to apply video transitions in their edit. Our work
can substantially assist these people by automatically recommending reasonable
video transitions on the ﬂy.
Visual-semantic embedding. Many recent works on video retrieval are based
on the alignment of visual-semantic embedding [6, 7]. Embedding techniques are
employed to measure the similarity of diﬀerent modalities in cross-modal video
retrieval tasks, where features from diﬀerent modals are mapped into a shared
embedding space for better alignment [8, 9]. Miech et al. [9] utilizes millions of
video text pairs to learn the text-video embedding. Escorcia et al. [10] aligns
the embedding of text and moments in the videos. They share the same idea of
jointly aligning representations from two diﬀerent modals. Triplet loss is initially
proposed for learning the distance metric [11]. It is used for learning multi-
modal embedding through deep neural networks in recent works [12, 13]. By
optimizing the distance directly with a soft margin, triplet loss is suitable for
ranking tasks [14]. Our method also employs triplet loss to learn the distance
between representations. A multi-modal transformer is used therein to learn
the features of vision/audio inputs which aim to match with the pre-trained
video transition embedding. Through extensive experiments, we demonstrate
the eﬀectiveness of our methods.
Multi-modal transformer and sequence modeling. Our task is closely
related to recent progress in modeling the spatial-temporal and multi-modal in-
formation in vision, speech and text. Transformer [15] is widely used in these
tasks to encode cross-modal and spatial-temporal information [16–19]. It employs
an attention mechanism to represent multi-modal information in a common la-
tent space. In other sequential problems, Lin et al. [20] uses a modality-speciﬁc
classiﬁer and a diﬀerentiable tokenization scheme to fuse multi-modal informa-
tion via transformer. Gabeur et al. [17] introduces a video encoder architecture
with multi-modal transformer for video retrieval. We also use modal-speciﬁc net-
works to extract embeddings from vision and audio inputs. Speciﬁcally, we use
SlowFast [21] and Harmonic CNN [22] to extract video and audio features re-
spectively. Diﬀerent with the vanilla transformer architecture which adopts an
encoder-decoder architecture [15], Lei et al. [23] uses uniﬁed encoder-decoder
transformer model for sequential modeling. In our method, we exploit a multi-
modal transformer for learning the multi-modal representations as well as cap-
turing the context information in sequential transitions.

3 Task Deﬁnition

We aim to solve the task of video transitions eﬀect recommendation (VTR), the
goal of which is to recommend appropriate transitions between neighboring video
shots. As shown in Fig. 1a, we take a sequence of raw video shots {v1, v2, . . . , vn}
as inputs. To simplify the task, we assume that the order of the videos is already
determined, all the videos are already cut and scaled to the target range, and
the background audio (either original sound and/or overwritten music) is already

AutoTransition: Learning to Recommend Video Transition Eﬀects

5

Fig. 1: The deﬁnition of the task. We take a sequence of raw video shots, for ex-
ample {v1, v2, v3}, and companion audio as the inputs, the task is to recommend
categories of the transitions {t1,2, t2,3} used in the edited video.

speciﬁed. Then for a pair of video shots vk and vk+1, a transition eﬀect is added
to join them. We denote the video clip added with transition eﬀect as tk,k+1.
Since video transitions generally mix neighboring video shots, we separate the
ﬁnal video after adding transitions into two parts, one is the clips added by
transitions (i.e. tk,k+1), the other is uncontaminated video shots (i.e. v(cid:48)
k and
k+1). Then the output video after adding transitions can be denoted as v(cid:48) =
v(cid:48)
{v(cid:48)
1, t1,2, v(cid:48)
n}. The dataset we collect contains both the start and
end timestamps of each transition so that we can obtain the exact positions of
video shots and the categories of transitions. Note that in the collected dataset,
we can only get access to the output video v(cid:48) since the original videos v are not
publicly available.

2, . . . , tn−1,n, v(cid:48)

In our method, we use the video clip tk,k+1 and the label of corresponding
transition eﬀect ck,k+1 to train the transition classiﬁcation network, for learning
the embedding of transitions based on their visual representation. When training
the transition recommendation model, we remove tk,k+1 from the input to be
consistent with the inference setting where input videos are without any tran-
sition. The uncontaminated video shots v(cid:48)
k+1 are used to represent the
original video shots, i.e. vk and vk+1 respectively. This strategy is reasonable
since the duration of transition eﬀect is short, v(cid:48)
k+1 can serve as good
approximations to their original counterparts.

k and v(cid:48)

k and v(cid:48)

4 Video Transition Dataset

4.1 Raw Data Collection

With the development of video editing tools and platforms, a large amount of
well-designed video templates are publicly available. Produced by professionals,
video templates deﬁne ﬁxed combinations of essential editing elements, including
the number of video shots, the length of video, music, transitions, animations,
and camera movement, etc. By simply replacing materials in templates with self
videos, even novice users can create edited videos easily. Each video template
also comes with an example video made by the designer using this template

(a) Before editing(b) EditedRaw video shotsVideo transitionv1v2v3v1'v2'v3't1,2t2,3Background Audio6

Y. Shen et al.

(a) The label distribution on the small
dataset at the ﬁrst stage of data collec-
tion.

(b) The label distribution on the ﬁnal
dataset after ﬁltering.

Fig. 2: Label distribution of top-30 categories in the dataset.

Table 1: The statistical information of the transition dataset.

Dataset
Number of videos
Number of transitions
Transitions per video
Average video length 15.83 secs 15.79 secs

train set
29,998
118,984
3.966

test set
5,000
19,869
3.973

and his/her original videos. In our experiments, we collect video templates from
these online video platforms and get the annotations related to transition eﬀects,
including each transition’s category and corresponding start/end time. Though
video templates may contain other special visual eﬀects like animations and 3D
movements, we only consider video transition eﬀects in this paper.

4.2 Data Filtering

We perform data ﬁltering to improve the quality and diversity of the dataset.
At ﬁrst, we limit the maximum duration of the videos to 60 seconds, and the
templates are ﬁltered depending on the number of user likes and usages. We
ignore those videos without any transition. We gather a small dataset through
manual crawling and examine the overall distribution of collected samples. To
guarantee that there are enough training samples for each category, we only select
top-30 categories for training and testing according to the amount of samples.
By statistical results, we observe that there are many duplicated transitions in a
single video, and diﬀerent types of transitions are distributed in a severe long-tail
manner. The label distribution of the small dataset is shown in Fig. 2a.

We believe that the duplication and long-tailed distribution are harmful to
the diversity of recommendation. To solve this issue, we use two additional rules
to select samples: each video should contain more than two diﬀerent types of
transitions, and the usage times of the same transition should be no more than
six. Following these rules, we acquire the ﬁnal dataset which contains 34998
videos (train-29998 and test-5000) in total and 138.8K valid transitions between

mixpull inpull outwhite flashblack fadefloodlightturn pagecircle 1cubeopenswitchwindmillleftblanchrightpanecircle 2clock wipememory 1downsqueezeblurheart 1dissolve 1gradient wipememory 2blindssuperimposethen and nowkaleidoscopeCategories0.00.2Percentagepull inmixpull outcircle 1openwindmillcubeswitchleftpanecircle 2rightblack fadeturn pageclock wipeblindsheart 1squeezefloodlightdownkaleidoscopememory 1white flashmemory 2blurgradient wipesuperimposedissolve 1star 1blanchCategories0.00.1PercentageAutoTransition: Learning to Recommend Video Transition Eﬀects

7

Fig. 3: Extracting transition embedding. A transition classiﬁcation network is
built to learn the transition embedding.

neighboring video shots. Table 1 shows more statistical results of the dataset.
The label distribution is shown in Fig. 2b.

5 Video Transition Recommendation

5.1 Pre-training Transition Embedding

We formulate the video transition eﬀects recommendation as a multi-modal re-
trieval problem. Since retrieving from vision/audio to video transitions requires
the model to learn correspondence between vision/audio inputs and video tran-
sitions, the ﬁrst problem we need to solve is thus how to learn a strong repre-
sentation for each transition. We notice that some video transitions have similar
visual eﬀects like “pull in” and “pull out”. It is natural that we expect the
learned embedding can also reﬂect these connections. To achieve this goal, we
employ a video classiﬁcation network to learn transition embedding based on
their visual appearance. As shown in Fig. 3, we take the transition clips t as in-
put and use the video backbone to extract visual representations. After passing
through linear transform and normalization, we obtain a unit vector for each
transition. Then we apply another linear transform and use the cross-entropy
loss to optimize the classiﬁcation objective. As expected, the embedding of the
transitions are separably distributed in latent space and similar transitions stay
close to each other. The visualization result of learned embedding through t-SNE
is illustrated in Fig. 5.

5.2 Multi-modal Transformer

As shown in Fig. 4, we propose a multi-modal transformer to extract representa-
tions from the raw video shots and audio. For recommending a transition tk,k+1
between video shots vk and vk+1, we take both the video frames and audio waves
from the end period of video shot vk and the start period of video shot vk+1 as
input. Speciﬁcally, n video frames are sampled uniformly from each video shot.
After obtaining video frames and corresponding audio waves, we extract their
features by feeding into visual and audio backbone respectively. Note that these
backbones can be conveniently replaced by other common video or audio models.

Transition EmbeddingCross-EntropyLinearClassifierVideoTransitionBackboneFCNormalize(bs, n, c, h, w)(bs, n, dim_embedding)8

Y. Shen et al.

Fig. 4: Transition recommendation model for retrieving matching transitions
based on vision/audio inputs. First, we use modality-speciﬁc networks to extract
visual and audio features. After that, a multi-modal transformer encoder fuses
the tokens from diﬀerent modalities. Finally, a triplet loss is used to optimize
the network end-to-end.

In our experiments, we use the SlowFast [21] and the Harmonic CNN [22] as video
and audio backbone respectively.

In order to make full use of multi-modal information of vision/audio, we com-
bine the visual and audio features as the inputs for the multi-modal transformer.
By doing so, the model not only learns the matching relationship from inputs to
transitions but also captures the context cues among sequential transitions.

Before being fed into the transformer, visual tokens and audio tokens are
projected to the same dimension by independent linear transformations. Then
learnable positional embedding and modal embedding are element-wisely added
to these tokens. We share the positional embedding for vision/audio tokens at the
same time point. Modal embedding is applied to inform the model which modal-
ity the token belongs to. After above processing, tokens from all video shots are
input into transformer as a whole. In this way, the transformer is encouraged to
learn the contextual relationship in sequential transitions. As demonstrated by
experimental results, such a context-aware training method contributes to gener-
ate harmonious sequential outputs. The self-attention mechanism in transformer
encoder can model complex mutual relationship among input tokens. From all
output tokens, we concatenate those four tokens which corresponds to each tran-
sition point (for each modality of vision and audio, there are two tokens before
and after the transition point) along the feature dimension and get the ﬁnal
representation by a projection to the same dimension of transition embedding.

BackboneBackboneBackboneBackbonePositionembeddingModalembedding.........Transformer encoderTriplet loss......Concat & Projection Concat & Projection .....................Similarity matrixProjectionProjectionProjectionProjection...VisualAudioPre-trained transition embeddingAutoTransition: Learning to Recommend Video Transition Eﬀects

9

5.3 Transition Recommendation

We denote Etrans = {etrans

As introduced in Sec. 5.2, the multi-modal video embedding is extracted by the
multi-modal transformer, which is used as the query to retrieve the transitions by
matching the pre-trained transition embedding mentioned in Section 5.1. Dur-
ing retrieval, we apply learnable linear transformations to both the pre-trained
transition embedding and the video embedding for achieving better alignment.
} as the set of pre-trained transition
embedding where N is the number of categories and N = 30 in our experiments.
Evideo = {evideo
} is denoted as the multi-modal input embedding in
a batch. V is the number of all transition points in a batch. For each sample
evideo
, we calculate its matching score with every transition embedding through
v
a similarity metric Φ(evideo
). In our implementation, Φ adopts the form
of dot-production due to its simplicity, i.e.

, . . . , etrans

, . . . , evideo

, etrans
k

N

V

v

1

1

Φ(evideo
v

, etrans
k

) = (cid:10)evideo

v

, etrans
k

(cid:11) .

(1)

Eq. (1) is used to calculate the ranking loss in the following training steps.
Training. We expect that the model can learn to rank transitions by their
distances with inputs in embedding space. To achieve so, we utilize the triplet
margin loss to optimize the similarity between the transition embedding and
multi-modal video embedding. For the embedding of each training sample evideo
with ground truth label c, we deﬁne our training objective with triplet loss as

L(evideo) =

1
N − 1

(cid:88)

T (evideo, etrans

c

, etrans
k

)

k(cid:54)=c,k∈{1,...,N }

where T calculates the triplet margin loss for each triplet (evideo, etrans

c

, etrans
k

):

T (a, p, n) = max(Φ(a, p) − Φ(a, n) + M, 0).

M is the soft margin, a, p and, n are anchor, positive sample, and negative
sample, respectively. Φ follows the deﬁnition of Eq. (1). In our settings, we take
video embedding evideo as the anchor, the transition embedding with category c
as the positive sample, others transitions as negative samples. The ﬁnal objective
is the average over all samples, i.e.

L(Evideo) =

1
V

(cid:88)

L(evideo
v

).

v∈{1,...,V }

(2)

By optimizing Eq. (2), the model encourages the similarity between the multi-
modal video embedding and its ground-truth transition embedding higher than
the similarity between non-matching pairs with a margin of M .
Evaluation. In evaluation, we follow Eq. (1) to measure the matching degree
between multi-modal video embedding and candidate transition embedding. For
two neighboring video shots, we sort the similarities of candidate transitions in
descending order and select the top one as ﬁnal result.

10

Y. Shen et al.

6 Experiments

6.1 Implementation Details

Model details. We employ SlowFast 8×8 [21] as the backbone to extract visual
features. The same backbone is also used as the transition embedding network.
By default, we freeze SlowFast from stage 1 to stage 3 during our experiments
to save memory. We train all models on one machine with 8 NVIDIA V100
GPUs, except the experiment without freezing the SlowFast backbone, in which
we use two machines and 16 GPUs in total. For audio modal, we use Harmonic
CNN [22] to extract the local audio features around the transition. Then we
linearly project the feature of two modals to the same dimension of dmodel and
take them as the input tokens of the multi-modal transformer. The multi-modal
transformer consists of two transformer layers with dmodel = 2048 and nhead = 8.
Before matching, we apply linear projections to both the video embedding and
pre-trained transition embedding in order to map them into a joint space. Such
a projection is experimentally proved to be beneﬁcial as shown in Table 3.

Data preprocessing and training. When training the transition embedding
network, we uniformly sample 16 frames with the image size of 224×224 from
the transition duration as the input. The batch size is set to 256. The training
process is 30 epochs in total, and start with a warm up for 5 epoch to raise the
learning rate from 1e-6 to the initial learning rate 1e-3, then decay by a factor of
0.1 every 10 epochs. We use the model parameters of the last epoch to generate
the transition embedding.

When training the transition recommendation model, we uniformly sample
16 frames with the image size of 224×224 as the visual inputs. The local au-
dio features are extracted using the pre-trained Harmonic CNN before training.
Given a time point, the Harmonic CNN generate a feature vector with a dimen-
sion of 100 based on the audio in around one second. For the sequential inputs,
we set the maximum sequence length to 8. Redundant transitions beyond the
maximum length are dropped while zero tensors are padded if the length is less
than 8. We use Adam optimizer in all the experiments and set the soft margin
M = 0.3 in triplet margin loss. The initial learning rate is set to 1e-5, then decay
by a factor of 0.1 every 10 epochs. The total training epoch is 30 epochs.

Metrics. For testing the ﬁne-grained performance of our model, we evaluate
the individual recommendation results in testing. The commonly used Recall@K
metric (k ∈ {1, 5}) and Mean Rank are employed as evaluation metrics. Since
in our dataset, there is only one ground-truth for each transition, Recall@K
indicates the likelihood of hitting the target in top K retrieval results. Mean
rank is the averaged rank of all ground-truth transitions, whose math formula is

MR =

1
|S|

(cid:88)

i∈S

Rank(i)

where S represents the set of all transitions in test.

AutoTransition: Learning to Recommend Video Transition Eﬀects

11

Fig. 5: t-SNE visualization of the transition embedding on the train set after pre-
training. We use cosine similarity as the distance metrics when running t-SNE.
We drop the outliers and randomly pick 10K samples from the t-SNE output
for visualization. We can see that transitions with similar visual eﬀects, such as
“left” (8), “right” (11), and “down” (19) are close to each other in the embedding
space.

6.2 Extracting Transition Embedding

A transition classiﬁcation network is trained using annotated transition cate-
gory in the dataset as the transition embedding network. After training, we
use the pre-trained network to extract the embeddings of all transitions in the
training set. Normalization is then applied for the embedding as shown in Fig.
3 to convert the embedding into a unit vector. We drop the outliers utilizing
the three-sigma rule by assuming the embeddings follow a normal distribution.
The embeddings after dropping are visualized in Fig. 5. The remaining embed-
dings for each category are averaged to generate the ﬁnal transition embedding.
We then demonstrate the eﬀectiveness of the transition embedding by the ex-
periments shown in Table 3. Notably, the transition embedding network is ad-
vantageous in extending the model to support new transition categories since
retraining the recommendation model is circumvented.

6.3 Ablation Studies and Comparisons

We start by showing the advantages of leveraging contextual and multi-modal
information in inputs. Then we verify the eﬀectiveness of the pre-trained tran-
sition embedding by comparing it with random initialization. We also compare
with the classiﬁcation method to demonstrate the superiority of retrieval meth-
ods on this task. Due to space limit, more details of results are referred to the
supplementary material.
Context and multi-modal. In this experiment, we study the impact of con-
textual and multi-modal inputs on the recommendation performance, and the

402002040x402002040y012345678910111213141516171819202122232425262728290:  pull in 1:  mix 2:  pull out 3:  circle 1 4:  open 5:  windmill 6:  cube 7:  switch8:  left 9:  pane 10: circle 2 11: right 12: black fade 13: turn page 14: clock wipe 15: blinds16: heart 1 17: squeeze 18: floodlight 19: down 20: kaleidoscope 21: memory 1 22: white flash 23: memory 2 24: blur 25: gradient wipe 26: superimpose 27: dissolve 1 28: star 129: blanch12

Y. Shen et al.

Table 2: The impact of contextual and multi-modal information to the recom-
mendation results.

Sequential (Context)

Modal
Visual Audio

Recall@1 Recall@5 Mean Rank

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

66.25%
24.12%
56.61%
19.39%
25.40%
66.33%
28.06% 66.85%

5.758
7.012
5.665
5.480

Table 3: Advantages of pre-training transition embedding from the visual eﬀects.
We demonstrate its eﬀectiveness by comparing it with a random initialized em-
bedding.

Transition Embedding
Random initialization
Pre-trained transition embedding
Pre-trained transition embedding

Projection Recall@1 Recall@5 Mean Rank

66.3%
25.67%
26.24%
66.03%
28.06% 66.85%

5.646
5.623
5.480

(cid:88)

result is shown in Table 2. From the ﬁrst and third rows of Table 2, we can see
that sequential inputs introduce the context information to the model, which is
helpful for modeling the temporal relations between the transitions. From the
second and third rows of Table 2, visual modal inputs perform much better than
audio as input, which indicates that visual content is more related to the tran-
sition eﬀects than audio. The results in the last three rows demonstrate that
the multi-modal inputs can improve the accuracy of recommendations than the
single modal inputs.

The eﬀectiveness of pre-trained transition embedding. In this experi-
ment, we study the eﬀectiveness of our proposed pre-trained transition embed-
ding, and the results are shown in Table 3. All the embedding is frozen during
training. In the random initialization setting, we use a normalized random em-
bedding as the replacement of the pre-trained transition embeddings. It is ob-
served that the performance of using random embedding is worse compared to
our pre-trained embedding. From the results in second and third rows in Table 3,
the importance of the linear projection can be demonstrated. We conjecture the
linear projection helps learning better mapping between the pre-trained transi-
tion embedding and multi-modal input embedding in a shared space.

Comparing with the classiﬁcation method. In this ablation study, we re-
move the transition embedding, replace triplet margin loss with cross-entropy,
and train the recommendation model utilizing the transition category label. The
comparison result is shown in Table 4. The classiﬁcation model performs worse
compared with the retrieval model. The reason is that the learned transition
embeddings in retrieval model contain richer visual properties of the transitions
compared with semantically meaningless one-hot vectors used in the classiﬁca-

AutoTransition: Learning to Recommend Video Transition Eﬀects

13

Table 4: Comparing with the classiﬁcation method.

Methods
Classiﬁcation
61.82%
Matching with pre-trained transition embedding 28.06% 66.85%

Recall@1 Recall@5 Mean Rank
22.27%

6.099
5.480

tion model. In addition, the cross-entropy loss may impose excessive punishment
for negative categories due to using one-hot ground-truth, thus neglecting the
fact that there are similar transitions as the ground-truth transition and they
can also be used as favorable alternatives.

6.4 User Study

Since the transition recommendation is subjective, the viewer’s feeling is essen-
tial to the evaluation. Therefore, we conduct a user study to further verify its
eﬀectiveness. Speciﬁcally, we collect raw video shots from online copyright free
video sources, e.g. videvo.net4, covering various topics such as travel, life, enter-
tainment, sports, nature, and animals. For each set of video shots, we ﬁx their
orders and assign an appropriate background music, leaving only the transitions
between neighboring video shots to be added. After selecting video transitions,
we use the tool of CapCut to connect the raw video shots by transitions, pro-
ducing the ﬁnal videos. We compare among following three methods of selecting
video transitions.

1. Weighted random pick. At each transition point, select the transition
category by a random sampling from a multinomial distribution. The prob-
ability of each category is its frequency in our collected video transition
dataset.

2. Professional video editor. We ask a professional editor who has 6 years
of video editing to select transitions. He is free to take as long as he wants
to select the best transitions depending on his understanding of the given
video/audio.

3. Our method. The top-1 video transition predicted by our method at each

transition point is used as the best selection.

We collect 20 groups of video results in total for user study. Each group
contains three videos edited using above three methods respectively. We invite
overall 15 non-expert volunteers to participate in the evaluation. They are asked
to choose a favorite video from each group (Q1) and rate each video on a scale
of 1 to 5 (1 = poor, 5 = excellent, Q2), taking into account the general visual
quality of videos and the matching degree between transitions and video/audio.
Table 5 and Fig. 6 show the statistics of the results. As shown in Table 5,
the videos generated using random pick receive the lowest score. Our method
is pretty close to the professional editing in terms of average score, but being

4 https://www.videvo.net/

14

Y. Shen et al.

Table 5: The statistical results of user study. The inference time is reported as
the time cost of our method.
Method
Weighted random pick
Professional video editor
Our method

Avg. score Avg. time (per video)

-
7.5 min
1.5 secs

2.96
3.80
3.76

Fig. 6: The voting results for three methods in Q1.

much more eﬃcient by drastically reducing the average processing time of each
video from 7.5 minutes to only 1.5 seconds (a 300× speedup). Interestingly,
in Fig. 6 which shows the voting results of Q1, one can see that videos from
our methods are slightly more appealing than that from the professional editor.
Above experimental results clearly demonstrate the advantages of our method
in producing high-quality video transitions recommendations.

7 Conclusions and Future Works

The recent development of online video tools and platforms creates a high de-
mand for a user-friendly video editing experience, which asks for a computational
method or artiﬁcial intelligent model to lower the barrier, improve eﬃciency and
ensure quality for doing video editing. Therefore, we propose a new task of
video transition recommendation (VTR) to automatically recommend transi-
tions based on any visual and audio inputs. We start with building a large-scale
transition dataset. Then we formulate VTR as a multi-modal retrieval problem
and propose a ﬂexible framework for addressing the task. Through extensive
qualitative and quantitative evaluations, we clearly demonstrate the eﬀectiveness
of our method. We hope this work can inspire more researchers to work on VTR
and bring creativity and convenience to both professionals and non-professional
users. Future works include but are not limited to extending the framework to
support more video editing eﬀects like video animation, 3D movements, etc., de-
veloping more eﬃcient models for mobile deployment and integrating with other
video editing techniques to create more comprehensive video editing systems.

8 Acknowledgement

Yaojie Shen did this work when interning at ByteDance Inc.

Our methodProfessional video editorWeightedrandompick43.3%44.3%12.4%AutoTransition: Learning to Recommend Video Transition Eﬀects

15

References

1. Frey, N., Chi, P., Yang, W., Essa, I.: Automatic non-linear video editing transfer.

arXiv preprint arXiv:2105.06988 (2021) 3

2. Wang, M., Yang, G.W., Hu, S.M., Yau, S.T., Shamir, A.: Write-a-video: computa-
tional video montage from themed text. ACM Trans. Graph. 38(6) (2019) 177–1
3

3. Koorathota, S., Adelman, P., Cotton, K., Sajda, P.: Editing like humans: A con-
textual, multimodal framework for automated video editing. In: 2021 IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),
IEEE Computer Society (2021) 1701–1709 3

4. Liao, Z., Yu, Y., Gong, B., Cheng, L.: Audeosynth: music-driven video montage.

ACM Transactions on Graphics (TOG) 34(4) (2015) 1–10 3

5. Pardo, A., Caba, F., Alc´azar, J.L., Thabet, A.K., Ghanem, B.: Learning to cut by
watching movies. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. (2021) 6858–6868 3

6. Hendricks, L.A., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.: Lo-
calizing moments in video with natural language.
In: 2017 IEEE International
Conference on Computer Vision (ICCV), IEEE Computer Society (2017) 5804–
5813 4

7. Akbari, H., Yuan, L., Qian, R., Chuang, W.H., Chang, S.F., Cui, Y., Gong, B.:
Vatt: Transformers for multimodal self-supervised learning from raw video, audio
and text. In: Advances in Neural Information Processing Systems. (2021) 4

8. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image
IEEE Transactions on Pattern Analysis and Machine Intelligence

descriptions.
39(4) (2017) 664–676 4

9. Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev,

I., Sivic, J.:
Howto100m: Learning a text-video embedding by watching hundred million nar-
In: 2019 IEEE/CVF International Conference on Computer
rated video clips.
Vision (ICCV), IEEE 2630–2640 4

10. Escorcia, V., Soldan, M., Sivic, J., Ghanem, B., Russell, B.: Temporal local-
ization of moments in video collections with natural language. arXiv preprint
arXiv:1907.12763 (2019) 4

11. Schultz, M., Joachims, T.: Learning a distance metric from relative comparisons.

Advances in neural information processing systems 16 (2003) 4

12. Faghri, F., Fleet, D.J., Kiros, J.R., Fidler, S.: Vse++: Improving visual-semantic
embeddings with hard negatives. arXiv preprint arXiv:1707.05612 (2017) 4
13. Schroﬀ, F., Kalenichenko, D., Philbin, J.: Facenet: A uniﬁed embedding for face
recognition and clustering. In: 2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), IEEE Computer Society (2015) 815–823 4

14. Hoﬀer, E., Ailon, N.: Deep metric learning using triplet network. In: International

workshop on similarity-based pattern recognition, Springer (2015) 84–92 4

15. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
(cid:32)L., Polosukhin, I.: Attention is all you need. Advances in neural information
processing systems 30 (2017) 4

16. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇci´c, M., Schmid, C.: Vivit: A

video vision transformer. arXiv preprint arXiv:2103.15691 (2021) 4

17. Gabeur, V., Sun, C., Alahari, K., Schmid, C.: Multi-modal transformer for video
retrieval. In: European Conference on Computer Vision, Springer (2020) 214–229
4

16

Y. Shen et al.

18. Lin, T., Wang, Y., Liu, X., Qiu, X.: A survey of transformers. arXiv preprint

arXiv:2106.04554 (2021) 4

19. Hendricks, L.A., Mellor, J., Schneider, R., Alayrac, J.B., Nematzadeh, A.: Decou-
pling the role of data, attention, and losses in multimodal transformers. Transac-
tions of the Association for Computational Linguistics 9 (2021) 570–585 4

20. Lin, X., Bertasius, G., Wang, J., Chang, S.F., Parikh, D., Torresani, L.: Vx2text:
End-to-end learning of video-based text generation from multimodal
inputs.
In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), IEEE Computer Society (2021) 7001–7011 4

21. Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recogni-
tion. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV),
IEEE Computer Society (2019) 6201–6210 4, 8, 10

22. Won, M., Chun, S., Nieto, O., Serrc, X.: Data-driven harmonic ﬁlters for audio
representation learning. In: ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), IEEE (2020) 536–540 4, 8, 10
23. Lei, J., Wang, L., Shen, Y., Yu, D., Berg, T.L., Bansal, M.: Mart: Memory-
augmented recurrent transformer for coherent video paragraph captioning. arXiv
preprint arXiv:2005.05402 (2020) 4

AutoTransition: Learning to Recommend Video Transition Eﬀects

17

A Supplementary Material

In this supplementary material, we provide following ablation studies and anal-
ysis. (1) More detailed analysis of the collected dataset. (2) Ablation on freezing
part of SlowFast and varying the model size. (3) Analysis of recommendation re-
sults. We show that they are reasonable and plausible by following common video
editing guidelines. (4) Experimental results of adding “direct cut” in transitions
recommendation. (5) Visualization of demo videos rendered with 30 transition
types.

A.1 Analysis of Dataset

We conduct experiments to analyze more relations between transitions and the
properties of visual/audio inputs. Speciﬁcally, we use two classiﬁers of video
style/audio mood trained on in-house datasets to get labels of video shots and
audio respectively. Then we get the statistical results of the frequencies of transi-
tions w.r.t. each category, followed by a column-wise normalization. As visualized
in Fig. A.1, one can get some hints of how transitions match with visual/audio
semantics. To give a few examples, “ﬂoodlight” and “black fade” appear more
with the visual style of “fresh” and “nostalgic” respectively. Both “star” and
“heart” tend to come with the audio mood of “sweet” and “romantic”. Note
these results comply with general preferences in video editing, which evidences
the feasibility of learning meaningful correspondence from inputs to transitions
using the dataset.

A.2 Model Architecture Ablation

Impact of freezing SlowFast backbone. When training the recommendation
model, we freeze stage one to stage three of the SlowFast network by default.
To verify its impact, we do some experiments with diﬀerent freezing proportions
of SlowFast network in the setting with only visual as inputs. From the results
in Table A.1, we can see that freezing all parameters severely damages the per-
formance of the model than freezing the parameters from stage 1-3. While not
freezing any parameters only improves performance by a small amount than
freezing the parameters from stage 1-3. So in order to balance the performance
and eﬃciency of the algorithm in training, we choose to freeze stage 1-3 of the
SlowFast network by default.
Variations on the model architecture. In this experiment, we study the
inﬂuence of the diﬀerent model architecture and the embedding dimension. In
Table A.2 (a), the inﬂuence of diﬀerent dimension in the embedding matching
space is investigated. Reducing the dimension of matching embedding does harm
to the performance. From Table A.2 (b) and (c), we observe that a smaller feature
dimension or number of the transformer layers in the transformer network also
hurts performance. At the same time, a larger feature dimension or number of
the transformer layers has adverse eﬀects on performance, indicating that large
model size may cause severe over-ﬁtting.

18

Y. Shen et al.

Fig. A.1: The relationships between video styles, audio mood and transition
types.

Table A.1: The impact of freezing the SlowFast backbone in visual-only setting.
By default, we freeze stage 1-3 to facilitate testing.

SlowFast Freezing
Freeze all parameters
Freeze stage 1-3
No freezing

Recall@1 Recall@5 Mean Rank
26.53%
22.39%
25.40%
66.33%
25.97% 66.95%

6.097
5.665
5.579

Table A.2: Ablation study on the model size. Where N is the number of trans-
former encoder layers, dmodel is the dimension of transformer layers, and dmatching
is the dimension of the common matching space.

base

(a)

(b)

(c)

N dmodel dmatching Recall@1 Recall@5 Mean Rank
2
2
2
2
2
1
4

28.06% 66.85%
67.09%
26.59%
67.07%
26.40%
66.71%
25.52%
66.55%
26.93%
66.45%
25.77%
66.47%
27.26%

5.480
5.493
5.499
5.598
5.541
5.623
5.528

2048
1024
512
2048
2048
2048
2048

2048
2048
2048
1024
4096
2048
2048

switchopencubeturn pagesqueezestar 1circle 2circle 1kaleidoscopeblindspanewindmillheart 1clock wipememory 2memory 1rightpull outmixwhite flashpull insuperimposeleftdownblanchgradient wipeblurfloodlightblack fadedissolve 1Transition typessceniclovelysweetsexyethnicdynamicarchaisticwarmfreshartisticnostalgiccalmfashionabletechnologicalfancyVideo styleswitchopencubeturn pagesqueezestar 1circle 2circle 1kaleidoscopeblindspanewindmillheart 1clock wipememory 2memory 1rightpull outmixwhite flashpull insuperimposeleftdownblanchgradient wipeblurfloodlightblack fadedissolve 1Transition typesangrychillcutedynamicexcitedhappyromanticsorrowtenseweirdsweetinspirationalAudio mood0.00.10.20.30.000.050.100.150.200.25AutoTransition: Learning to Recommend Video Transition Eﬀects

19

Fig. A.2: The video transition recommendations generated by our method. In
each row, the leftmost and the rightmost images represents the video shots before
and after transitions respectively. The frames in middle represent the video clip
in transition. For better visualization, please refer to videos (a)-(c) under folder
“video/ﬁg-A-2”.

A.3 Results Analysis

In this section, we show through concrete examples that our method indeed
learns the general guidelines of using transitions in video editing. Therefore, the
videos generated by our method comply with common cinematography knowl-
edge and aesthetic principles.

As shown in Fig. A.2, our method can capture the visual changes in neighbor-
ing video shots and thus recommend suitable video transitions. In Fig. A.2 (a),
since the neighboring video shots have similar scenes, our method ranks gentle
transitions higher, such as “mix”, “black fade”, “dissolve” and “blur”, in order
to keep the coherence of video narration. In Fig. A.2 (b) where neighboring video
shots have large scene changes and brightness changes, our method recommends
more dynamic transitions like “ﬂoodlight”. In Fig. A.2 (c) where a long video
shot switches to a shorter one, our method recommends the “pull in” to ensure
natural transition .

In Fig. A.3 (a), we show an example that our method is able to adapt to
the characteristics of music. Since the background music used in this example is
with soft tune, our method prefers to recommend gentle transitions. Otherwise
using abrupt transitions may break the visual-auditory harmonious. In contrast,
since the weighted random pick method does not consider visual/audio contents,

"Mix" "Floodlight" (a)(b)"Pull in" (c)20

Y. Shen et al.

Fig. A.3: Transitions recommended for video shots with soft background music
(see videos (a)-(b) under folder “video/ﬁg-A-3”). (a) and (b) corresponds to the
results of our method and weighted random pick respectively.

Fig. A.4: A comparison of the transitions selected by three diﬀerent methods
(see videos (a)-(c) under folder “video/ﬁg-A-4”). Transitions in (a), (b) and
(c) are selected by weighted random pick, our method and professional editor
respectively.

"Mix" "Blur"  "Mix" (a)(b)"Floodlight" "Memory 1" "White flash" (a)(b)(c)"Pull in" "Circle 1" "Turn page" "Mix" "Mix" "Floodlight" "Mix" "Left" "Pull in" AutoTransition: Learning to Recommend Video Transition Eﬀects

21

Fig. A.5: t-SNE visualization of the pre-trained transition embedding (with “di-
rect cut”, the index of “direct cut” is 30).

its results are less reasonable (e.g. using too much “ﬂashing”) as shown in Fig.
A.3 (b).

We show another comparing example among three methods in Fig. A.4. Com-
pared with weighted random pick (Fig. A.4 (a)), our method (Fig. A.4 (b))
recommends better results which are reﬂected by the consistency in sequen-
tial transitions predictions, as well as the nice matching with visual and audio
contents. While the professional editor (Fig. A.4 (c)) may be advantageous in
capturing the details in videos like the speciﬁc movement pattern of basketball
players to select dynamic transitions, we note our method is much faster in terms
of eﬃciency. This comparison result also motivates us to use more ﬁne-grained
video embedding to further improve our method, which we leave for future work.

A.4 “Direct Cut” and More User Study

We note that “direct cut” is also widely used in video editing by directly connect-
ing shots without using any eﬀects. We add “direct cut” and re-train/re-evaluate
our model to verify the extendibility of our proposed framework. We draw sim-
ilar conclusions from this new experiment as in the main paper and verify both
quantitatively and qualitatively that our method can handle this case. Fig. A.5
shows the learned embeddings of transitions including “direct cut”. The seman-
tic relationships are still preserved. As shown in Table A.3, the performance of
model with “direct cut” is on par with that without “direct cut”, showing the
model can successfully learn the matching from input to transitions.

To further verify the eﬀectiveness of our method, we conduct a more com-
prehensive user study following the practice introduced in Section 6.4 in the
main paper. In this experiment, we hire 10 professional and 10 amateur editors,

402002040x402002040y300123456789101112131415161718192021222324252627282922

Y. Shen et al.

Table A.3: Ablative experiments on direct cut.

Modal
Visual+Audio
Visual+Audio

with “direct cut” R@1 ↑ R@5 ↑ Mean Rank ↓

(cid:88)

28.06% 66.85%
30.57% 67.98%

5.480
5.347

Fig. A.6: User study results with more editors, “direct cut” is allowed in all
comparing methods.

and direct cut is allowed to use in all comparing methods. As show in Fig. A.6,
in terms of both voting result and average score, our method surpasses ama-
teur editors and achieves comparable results with professional editors, therefore
demonstrating the eﬀectiveness of our method.

A.5 Example videos of all transition eﬀects

To help readers understand video transitions more straightforwardly, we pro-
vide example videos created using video transitions and a ﬁxed pair of video
shots. Overall 30 demo videos are included in the zipped ﬁle, under the folder
“video/transitions”.

Our method34.2%Weighted   random pick11.6%Professionaleditor    34.2%Amateur editor20.0%Voting resultsOurmethodWeightedrandom pickProfessionaleditorAmateureditorMethods024Average score3.742.923.763.49Scores