Methods to Simplify Object Tracking in Video Data

Orban, C. M.,1, 2, ∗ Zimmerman, S.,3, 4 Kulp, J. T.,1 Boughton, J.,5 Perrico, Z.,6 Rapp, B.,6 and Teeling-Smith, R.6
1Physics Department, The Ohio State University, 191 W Woodruff Ave, Columbus, OH 43210
2Physics Department, The Ohio State University at Marion, 1461 Mount Vernon Ave, Marion, OH 43302
3Department of Mathematics, The Ohio State University, 231 West 18th Avenue, Columbus, OH 43210
4Department of Mathematics, The Ohio State University at Marion, 1461 Mount Vernon Ave, Marion, OH 43302
5Great Oaks Career Campuses, 110 Great Oaks Drive, Cincinnati, OH, 45241
6University of Mount Union, Alliance, OH, 44601

2
2
0
2

t
c
O
4
1

]
h
p
-
d
e
.
s
c
i
s
y
h
p
[

3
v
9
0
9
7
0
.
6
0
2
2
:
v
i
X
r
a

I.

INTRODUCTION

Recent years have seen an explosion of interest in analyz-
ing the motion of objects in video data as a way for students
to connect the concepts of physics to something tangible like
a video recording of an experiment [1, 2]. Inexpensive de-
vices can now record videos at 120 or 240 frames per second.
And among instructors there is an appetite for substantially
re-thinking the objectives of introductory labs (e.g. [3–5]).

Generally, the goal of a student activity involving analysis
of video data is to obtain the (x, y) position of a particular
object in as many frames of the video as possible. Once ob-
tained, this data can be used to infer velocities, acceleration
and any number of other quantities like momentum or energy.
A variety of software exists for students to look at individ-
ual frames and click on the object to infer the (x, y) position
(e.g. [2, 6–9]). For longer videos or videos recorded at a high
frame rate this task can become tedious. Some, but not all, of
these tools include a capability to automatically identify the
position of the object in the frame [2, 6, 7]. But it is not un-
usual, especially when inexperienced users are recording the
video and conﬁguring the program, for these algorithms to
struggle to “lock on" to the moving object. In this paper, we
both include some general advice to help object tracking al-
gorithms locate an object and we provide our own algorithms
that are simpler and potentially more effective than the so-
phisticated image processing algorithms that are currently be-
ing used. Our algorithms are built into a free and open source
program called the STEMcoding Object Tracker (available
at http://go.osu.edu/objecttracker) which works through the
browser and is compatible with a variety of operating systems
including Chromebooks. An interesting “feature" of this tool
is that it does not automatically calculate the velocity and ac-
celeration of the object being tracked. Instead students must
build their data analysis skills to extract these quantities from
the raw (x, y) versus time data.

II. THE CHALLENGE OF OBJECT TRACKING

Fig. 1 shows a screenshot from a video tutorial on object
tracking from the Let’s Code Physics YouTube channel by

∗ orban@physics.osu.edu

Dr. Brian Lane [10]. Dr. Lane is using a program called
Tracker Video to analyze a video where he is walking across
the sidewalk [2]. In the analyzed video, Dr. Lane has a red
piece of paper taped to his right shoulder. This piece of paper
is the object being tracked.

According to the Tracker Video help guide[11], “Auto-
tracker works by creating one or more template images of
a feature of interest and then searching each frame for the
best match to that template." In the video tutorial, Dr. Lane
explains that it is important to deﬁne the template image of
the object not at the center of the paper but rather at one of
the corners so that the object tracking algorithm searches for
a splotch of red next to some gray, which is the color of his
shirt. With this hint, the object tracking works well and the
plot of x versus time shows that Dr. Lane is walking at an
approximately constant velocity as expected.

Although this is just a brief moment in the tutorial video it
underscores the difﬁculty of what automatic object tracking
is attempting to do. The background of Fig. 1, for example,
has many different features that the program potentially needs
to scan through as it searches for the the red piece of paper.
Further complicating matters, the paper itself will look dif-
ferent in each frame due to changes in lighting and shadows.
In our own experimentation with object tracking using a dif-
ferent program – speciﬁcally the Vernier Video Physics app –
we found it to be surprisingly difﬁcult to get the program to
“lock on" to a moving object.

We provide some general thoughts on object tracking and
then we describe two different algorithms that can be used
to track an object moving in front of a solid colored back-
ground. We demonstrate this method on a difﬁcult object
tracking problem – that of a roll of tape that is spinning as
it moves through the air. We implement these algorithms in
a program called the STEMcoding Object Tracker which is
freely available at http://go.osu.edu/objecttracker.

III. SIMPLIFYING THE TASK

In this paper we advocate for three methods to simplify the

challenge of automatically tracking objects:

1. Use a solid colored background, like a black poster

board, a green screen, or a white classroom wall.

2. Use an object that is bright red, green, or blue.

 
 
 
 
 
 
FIG. 1. A screenshot from the program Tracker Video where a red piece of paper on the shoulder of a walking person is being tracked across
the screen. The "Autotracker" window on the left shows the template image being searched for in each frame and the matching image found
in a given frame.

3. Use an object tracking algorithm that searches for spe-
ciﬁc colors instead of searching for a template image.

In the present work we show two examples of videos
recorded with a solid colored background – a basketball being
tossed in front of a black curtain and a blue roll of tape mov-
ing in front of a white background. In our tests we found that
object trackers could easily ﬁnd the objects in each frame.

The reason we advocate for a black or white background
with a bright red, green or blue object in the foreground is that
video data is stored in an array of pixels, and each pixel stores
three values – one for red, one for green and one for blue.
For historical reasons1, each color has a value between 0 and
255. So pure red would be (255, 0, 0), pure green would be
(0, 255, 0) and pure blue would be (0, 0, 255). In this scheme,
pure white is (255, 255, 255) and pure black is (0, 0, 0).

From an information perspective the largest contrast that
one can achieve would be a bright red, green or blue object in
front of a dark background. One can also use a solid colored
blue, green, red or white background so long as the object
is a different color. In our own tests we found that using a
green screen background with a basketball in the foreground
is another good combination. This is because green screens
(which are also called “chromakey") use a shade of green that
is close to (0, 255, 0).

1 An 8-bit unsigned integer can take any integer value between 0 and 255

Even if using a template image instead of a reference color
for the object, the contrast between the object color and
the background color helps the object tracking algorithm sift
through all the regions of the image to ﬁnd the best match.

IV. SIMPLIFYING THE ALGORITHM

The STEMcoding Object Tracker includes two different
object tracking algorithms both of which focus on the color
of the object as compared to that of the background. The user
is prompted to click the object at three different locations in a
frame, at which time the program stores three separate values
for each color variables r, g, and b corresponding (ideally)
to the object color. The user is then instructed to click the
background at three different locations in the same frame and
three more values for each of r, g, and b corresponding to the
background color are collected. The values of r, g, and b are
each averaged individually for the object and the background.
One can instead manually deﬁne the average color variables
for the background and the object if desired.

In our program are two different ways of taking these av-
erage colors and searching through the pixels to locate the
object: (1) The Color Distance Method, and (2) Brightness
Agnostic Mode.

A. The Color Distance Method

This vector can be normalized by its magnitude to create a
unit vector ˆc

As physics instructors, it is convenient to think about the

rgb colors like a three element vector

(cid:126)c = r ˆi + g ˆj + b ˆk

(1)

where r, g and b are integers between 0 and 255. Each pixel
has some color (cid:126)c which can be compared to the average color
of the object ((cid:126)cobj) and the background ((cid:126)cbg). Here, robj,
gobj, and bobj are the average color values of the object as
discussed above, and rbg, gbg, and bbg are the averages for
the background.

A simple way to compare the vectors is by taking the norm
of the difference between them – in other words calculate the
color distance, for example,

|(cid:126)c − (cid:126)cobj| =

(cid:113)

(r − robj)2 + (g − gobj)2 + (b − bobj)2 (2)

and

(cid:113)

|(cid:126)c − (cid:126)cbg| =

(r − rbg)2 + (g − gbg)2 + (b − bbg)2.

(3)

The “color distance method” ﬂags a particular pixel as object
if |(cid:126)c − (cid:126)cobj| is smaller than |(cid:126)c − (cid:126)cbg|.

While straightforward, the color distance method can fail if
the object is dimly lit or not particularly bright colored. Gray-
ish pixels on a solid black background can have rgb values
similar to (100, 100, 100) which may be closer to the object
color than to a pure black background of (0, 0, 0). In general,
gray pixels will have r ∼ g ∼ b so any amount of gray in the
background can potentially be confused for the object.

B. Brightness Agnostic Method

The problem with the color distance method is essentially
that the brightness of the pixel affects whether the pixel is
considered object or part of the background, especially if one
is using a dark or bright white background. One approach
to avoid this would involve normalizing the color vectors by
their magnitude (in other words dividing each color channel
by the brightness). This operation converts the color vectors
to unit vectors (i.e. vectors of magnitude 1). Such color vec-
tors are independent of the brightness of the pixel. However,
following this scheme would be problematic for pure black
pixels (0, 0, 0) because the normalization would involve di-
viding by zero.

Instead we propose a scheme where a constant 255/2 is
subtracted from each component of the color vectors so that
the color channels take non-integer values between −255/2
and +255/2 as follows. Writing

(cid:126)c0 = (255/2)ˆi − (255/2)ˆj − (255/2)ˆk,

(4)

we have the following:

(cid:126)c − (cid:126)c0 = (r − 255/2)ˆi + (g − 255/2)ˆj + (b − 255/2)ˆk. (5)

ˆc =

(cid:126)c − (cid:126)c0
|(cid:126)c − (cid:126)c0|

.

(6)

By deﬁning ˆc in this way we can be sure that |ˆc| evaluates
to 1, including for black (0, 0, 0) pixels. Before computing ˆc
for every pixel in the frame, the algorithm will compute ˆcobj
from the object colors and ˆcbg from the background colors as
follows

ˆcobj =

(cid:126)cobj − (cid:126)c0
|(cid:126)cobj − (cid:126)c0|

ˆcbg =

(cid:126)cbg − (cid:126)c0
|(cid:126)cbg − (cid:126)c0|

.

(7)

The ˆc from each pixel can then be compared to the object and
the background using a dot product.

ˆc · ˆcobj = |ˆc||ˆcobj| cos θobj = cos θobj

ˆc · ˆcbg = |ˆc||ˆcbg| cos θbg = cos θbg.

(8)

(9)

If the pixel color and object color are exactly the same, then
ˆc and ˆcobj are the same vector, and so cos θobj = 1. If instead
the pixel color is very different than the object color, then
cos θobj should be close to −1. Similarly, if the pixel color
and the background color are exactly the same, cos θbg = 1,
and if they are very different, cos θbg should be close to −1.
Our algorithm ﬂags pixels as being contained in the object
if cos θobj > cos θbg. This inequality implies that the angle
between the pixel’s normalized color vector (ˆc) and the ob-
ject’s normalized color vector (ˆcobj) is smaller than the the
angle between ˆc and ˆcbg.

Fig. 2 shows the result of using this algorithm with a bas-
ketball moving in front of a black curtain. Although there is
a thin line of gray in the frame that the color distance method
often ﬂags as part of the object, this mistake is avoided in
brightness agnostic mode because the brightness of a pixel
does not impact whether it is included as part of the object of
the background. Note that, if we digitally remove the thin line
of gray, the color distance method can track the basketball as
well as the brightness agnostic method can.

C. Comparing to Tracker Video

As mentioned earlier, the object ﬁnding method used in
Tracker Video [2] uses an image of the object rather than an
rgb color. Of course the image of the object contains the rgb
colors so certainly Tracker Video is using colors in the frame
as part of its algorithm to ﬁnd the object. Therefore in prac-
tice the method used in Tracker Video is similar to what we
propose here. An essential difference is that, once the object
is located in Tracker Video, the program uses the image of
the object in the most recent frame as the new "template im-
age" for the following frame. If at some frame the program
fails to locate the object correctly, it may be difﬁcult for it

FIG. 2. A basketball moving in front of a black curtain is being tracked using the two different modes of the STEMcoding Object Tracker.
Pixels identiﬁed as object are highlighted in white. The center of all object pixels is shown with a purple dot and the movement of that center
is shown with a green line. The green box deﬁnes the region of interest. Using color distance mode (left panel), a line of gray pixels in the
background is incorrectly ﬂagged as object. Using brightness agnostic mode (right panel), only the orange-red pixels of the basketball are
identiﬁed as object.

to ﬁnd it again in later frames. However, in the methods we
propose, the ability of the program to locate the object in one
frame does not affect its ability to locate the object later on as
the only data used are the colors of the object and the back-
ground, and these colors are kept constant for the duration of
the video.

V. TRACKING PIXELS ON A ROTATING HOLLOW
OBJECT

A potential advantage of tracking colors is that the object
does not need to be spherical like a ball or ﬂat and facing the
camera like the red paper in the example highlighted in Fig. 1.
Fig. 3 shows an example where a roll of blue-colored painters
tape has been tossed straight upwards while it rotates about its
axis.

Both the color distance and brightness agnostic methods
can easily ﬁnd the blue pixels on a white-ish background and,
in Fig. 3, each of these blue pixels are covered with a pure
white pixel to indicate that the program has identiﬁed these
pixels as part of the object. Because the object is circular, the
center of these pixels happens to be the center of mass of the
object. As is well known, the center of mass of the object
should behave like a simple projectile.

A plot of the inferred vertical velocity versus time is shown
in Fig. 4. The linear nature of the vy versus time plot con-
ﬁrms the essential usefulness of these color-oriented methods
for tracking objects. The best ﬁt implies a gravitational con-
stant of −10.1 m/s2 which is reasonably close to reality. The
ﬁt excludes the points at the beginning and the end because
the tape is moving into the region where the pixels are being
tracked which biases the center pixel from the center of mass

of the object.

We have not necessarily proven that “image template"
methods will generally fail on this example but we hope that
the basic ideas outlined in this paper can be used to im-
prove object tracking methods and help make automatic ob-
ject tracking more widespread and robust. For the wider need
to make physics labs more focused on developing laboratory
skills (e.g. [3–5]), setting up and recording a direct measure-
ment video can be an interesting and engaging way to get
students to think about experimental design. Likewise, im-
proved video analysis tools can help connect lab instruction
to “computational thinking" and simple physical simulations
as argued in Orban & Teeling-Smith [12].

VI. DRAWBACKS / PRACTICALITIES

Because the methods described here make no assumption
about where the object is located, each pixel in each frame
within a user deﬁned rectangular region of interest needs to be
scanned and calculations performed on the color data, which,
for high resolution videos, is a time- and memory-consuming
task. However this can be done in quasi-real time on typ-
ical computers (e.g. Chromebooks) if the video resolution
is coarsened. We developed an online video converter at
http://go.osu.edu/convert2gif where students can upload mp4
or mov format videos and download a coarse resolution gif
ﬁle.

The STEMcoding Object Tracker works best on the
Chrome and Edge browsers. Other browsers have difﬁculty
freeing up the RAM after scanning through a frame. The
STEMcoding Object Tracker has been tested and used suc-
cessfully on Windows, MacOS, iPad, Chromebooks, and

FIG. 3. A still frame from a blue roll of painters tape being tossed in the air. Each blue-colored pixel is identiﬁed as object and a pure white
pixel is drawn over that pixel. The center of all of these object pixels is shown with a purple dot which is a useful way to follow the motion
of the center of mass. The green box shows the region of interest.

VII. CONCLUSIONS

Automated object tracking in direct measurement videos is
a difﬁcult problem from a computational point of view. The
problem can be made simpler by using a solid colored back-
ground and a brightly colored object in the video recording.
Computationally, it is also much simpler to track colors than
it is to track a “template image" and we outline two methods
for identifying pixels that are part of the object from the pixel
color alone. We demonstrate the usefulness of these methods
on the tricky problem of a hollow, blue rotating object as it
moves through the air. We implement these algorithms into
a program called the STEMcoding Object Tracker which is
freely available to use at http://go.osu.edu/objecttracker.

FIG. 4. The inferred vertical velocity versus time of the center of
mass of the blue painters tape shown in Fig. 3. Also shown is a
best ﬁt from a linear regression on the data which gives a slope of
−10.1 m/s2.

Linux.

00.10.20.30.40.50.6−3−2−10123Time (s)vy (m/s)Best fit:vy = (−10.1 m/s2)t + 2.95 m/s  Object Tracking DataBest FitACKNOWLEDGMENTS

We acknowledge support from the 2021 AIP Meggers
Award, the Big Idea Seed Grant from the OSU College of

Education and Human Ecology, and the Sit Lux grant pro-
gram at the University of Mount Union. JB was supported
by the Department of Defense High Performance Computing
intern program. Many thanks to Brian Lane for useful con-
versations.

[1] Direct measurement video is now pivot interactives. https://
www.pivotinteractives.com/direct-measurement-video, 2019.
Innovative uses of video

[2] Douglas Brown and Anne J. Cox.

analysis. The Physics Teacher, 47(3):145–150, 2009.

[3] Eugenia Etkina. Millikan award lecture: Students of physics
– listeners, observers, or collaborative participants in physics
scientiﬁc pratices? American Journal of Physics, 83(8):669–
679, 2015.

[4] Katherine Ansell and Mats Selen.

Student attitudes in a
new hybrid design-based introductory physics laboratory.
In
Physics Education Research Conference 2016, PER Confer-
ence, pages 36–39, Sacramento, CA, July 20-21 2016.

[5] N. G. Holmes and C. E. Wieman. Introductory physics labs:
We can do better. Physics Today, 71(1):38–45, January 2018.

[6] Vernier. https://www.vernier.com/product/video-physics-for-

ios/, 2010.
[7] Roland Wejner.

id717653395, 2020.

https://apps.apple.com/de/app/newtondv/

[8] New York Hall of Science.

https://apps.apple.com/us/app/

playground-physics/id947124790, 2015.

[9] Luca Demian. https://jst.lucademian.com/, 2018.
[10] Brian Lane.

Importing a video and getting data (tracker for

beginners 1). https://youtu.be/pPgfBfqvQ2k, 2019.

[11] Tracker help. https://physlets.org/tracker/help/frameset.html.
[12] C. M. Orban and R. M. Teeling-Smith. Computational thinking
in introductory physics. The Physics Teacher, 58(4):247–251,
2020.

