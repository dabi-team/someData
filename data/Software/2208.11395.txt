2
2
0
2

g
u
A
4
2

]
S
M

.
s
c
[

1
v
5
9
3
1
1
.
8
0
2
2
:
v
i
X
r
a

Distributed Objective Function Evaluation for
Optimization of Radiation Therapy Treatment
Plans

Felix Liu1,2, M˚ans I. Andersson1, Albin Fredriksson2, and Stefano Markidis1

1 KTH Royal Institute of Technology
2 RaySearch Laboratories

Abstract. The modern workﬂow for radiation therapy treatment plan-
ning involves mathematical optimization to determine optimal treatment
machine parameters for each patient case. The optimization problems
can be computationally expensive, requiring iterative optimization algo-
rithms to solve. In this work, we investigate a method for distributing
the calculation of objective functions and gradients for radiation ther-
apy optimization problems across computational nodes. We test our ap-
proach on the TROTS dataset— which consists of optimization problems
from real clinical patient cases—using the IPOPT optimization solver in
a leader/follower type approach for parallelization. We show that our
approach can utilize multiple computational nodes eﬃciently, with a
speedup of approximately 2-3.5 times compared to the serial version.

Keywords: Optimization · Radiation Therapy · Distributed Computing

1

Introduction

Radiation therapy is one of the most common forms of cancer treatment today.
Before a patient undergoes treatment, the control parameters for the treatment
machine, such as the dose rate and the shape of the aperture through which to
irradiate the patient, must be determined. This process is known as treatment
planning. Ultimately, the goal of the treatment is to deliver suﬃcient dose to the
tumor to kill cancerous cells, while sparing surrounding healthy tissue.

In modern radiation treatment planning, mathematical optimization is used to
determine parameters for the treatment machine (often a linear- or particle ac-
celerator). The planning process typically begins after a CT scan of the patient
has been imported into a treatment planning system (TPS), a software prod-
uct designed for radiation treatment planning. Contours of important structures
such as risk organs and the tumor—the regions-of-interest (ROIs)—in the pa-
tient are drawn on the CT. Optimization functions in the form of objectives
and constraints corresponding to desirable qualities of the dose delivered to the
ROIs are deﬁned, yielding a mathematical optimization problem. The optimiza-
tion problem is then solved numerically, and, if needed, the process is repeated

 
 
 
 
 
 
2

F. Liu et al.

in a trial-and-error fashion where the optimization functions and possible im-
portance weights are changed until a high-quality plan is achieved.

The treatment planning process is time-consuming, in part because the opti-
mization problem is computationally demanding. Eﬃcient algorithms for solving
the problem is crucial, both for eﬃciency at the clinics and for the quality of
the resulting treatment plans. Due to the high computational demand, utilizing
HPC resources such as accelerators or distributed computing clusters is an im-
portant step.

In this work, we propose a method for distributing the computation of objective
function, constraints and gradients—in many cases an important computational
bottleneck—across multiple computational nodes. We show the eﬀectiveness of
our approach by utilizing the IPOPT solver [8], a general software library for
nonlinear optimization, together with our method for distributing optimization
function evaluations across nodes, on optimization problems from radiation ther-
apy. We study problems from the TROTS dataset, an open dataset with data
for optimization problems from real patients treated for cancers in the head-
and-neck region. We provide an implementation to calculate function values and
gradients using input data from TROTS in a distributed fashion and show that
our approach can produce solutions of high quality while being able to eﬀectively
utilize distributed computing resources, with approximately a 2-3x speedup com-
pared to the single node version.

2 Background and Related Work

We consider treatment planning for high-energy photons using the treatment
technique volumetric modulated arc therapy (VMAT) [7], where the gantry head
of the treatment machine is continuously rotated around the patient during the
treatment. The optimization variables in our problems are beamlet weights, which
are intensity values for the delivered beams in the plane in front of the gantry.
From the beamlet weights, the actual treatment machine settings required to
deliver such an intensity proﬁle at each beam angle can be determined and a
deliverable plan can be created.

2.1 TROTS Dataset

The TROTS dataset [1] consists of data for patients with cancers in the head-
and-neck region, liver or prostate. The dataset contains objective functions and
constraints for the dose in each ROI of the patient, which form the nonlinear
optimization problem to be solved. Furthermore, dose inﬂuence matrices are
provided for each ROI separately. The dose inﬂuence matrix gives the relation
between the optimization variables, beamlet weights, and the resulting dose in
the ROI.

Distributed Optimization for Radiation Therapy

3

We note that having separate dose matrices for each ROI is not ubiquitous
in radiation therapy optimization, it is also possible to provide a single dose
matrix covering the entire relevant part of the patient volume and to extract the
doses for each ROI from the total dose. Since there is only a single dose matrix
in this case, distributing the computation becomes more complicated, in which
case it may be more natural to look to GPU accelerators instead, see for instance
the work in [6]. For our experiments, we use the same optimization functions as

Comment

i=1 e−α(di(x)− ˆd) where α is a scalar

g is min/max

Name
LTCP:
min/max dose:
Mean dose
Generalized mean: (cid:0) 1
Quadratic:

(cid:80)n

f (x)
1
n
g(d(x))
(cid:80)n
1
n

i=1 di(x)
i=1 di(x)a(cid:1) 1
(cid:80)n
2 xT Ax + bT x + c

n

1

a

a is a parameter
a,b are constant vectors

Table 1: The diﬀerent types of optimization functions used in the TROTS dataset
problems we have used. In the following f (x) denotes an optimization function,
di(x) is the dose in voxel i of a given ROI, and ˆd denotes a desired dose level.

provided in TROTS, with the exception of the min and max dose constraints,
which we substitute with quadratic penalties of the form:

f (x) =

1
n

n
(cid:88)

i=1

(g(di(x) − ˆd, 0))2

where g again is either min or max.

The dependence between the dose d(x) for a given ROI and the optimization
variables x is linear. To calculate the dose for a given ROI and value of x, we
simply multiply x by the dose inﬂuence matrix A for that ROI: d(x) = Ax. The
TROTS dataset provides one dose inﬂuence matrix for each ROI, which is stored
as a sparse matrix.

The optimization problem for a TROTS case is then speciﬁed using a weighted
sum of the objectives, shown in Table 1 on the diﬀerent ROIs, together with the
constraints on ROIs in the following form:

min
x

s.t.

n
(cid:88)

wifi(x)

i=1
gi(x) ≤ 0
x ≥ 0

Here, fi(x) are the objectives for the diﬀerent ROIs, with wi being their corre-
sponding weight and gi(x) are the constraints on doses in the ROIs.

4

F. Liu et al.

2.2 Dose Inﬂuence Matrices

Since all the optimization functions used in the optimization problems are func-
tions of dose in the diﬀerent regions of interest in the patient, dose calculation is
an important computational kernel. The dose calculation in our case is a sparse
matrix-vector product, d = Ax, with the dose inﬂuence matrices A being pro-
vided by the TROTS dataset. Note also that in the case of TROTS, the dose
inﬂuence matrices are given for each ROI separately, instead of for the patient
volume as a whole.

The size of the dose inﬂuence matrix varies quite signiﬁcantly between the ROIs,
meaning that the computational time needed to compute the optimization func-
tion for the diﬀerent ROIs varies signiﬁcantly. A histogram of the number of
non-zero values in the diﬀerent dose inﬂuence matrices (there are approximately
40 in total) is shown in Fig. 1a. We see that the number of non-zeros varies
signiﬁcantly, with some matrices having approximately 5000 non-zero elements,
while others have closer to 10 million.

2.3 Radiatiation Therapy Plan Quality

The ultimate goal when solving optimization problems in radiation therapy
treatment planning is to create treatment plans of high quality. To this end,
the mathematical optimization problem can be seen as a proxy, providing a way
to produce plans with desirable dose characteristics. Considering this, the qual-
ity of the resulting plan should not only be evaluated based on how accurately
the optimization problem was solved, but also using other dose-based metrics
which may reﬂect plan quality more accurately.

One common method for evaluating and comparing treatment plans is using
dose-volume histograms (DVH) [3]. DVHs are deﬁned using the so called volume-
at-dose metric. For a given ROI with n voxels (with identical sizes), discretized
dose d ∈ Rn and dose level ˆd, the volume-at-dose V ˆd(d) is deﬁned as:

V ˆd(d) =

(cid:80)n

i=1

1(di ≥ ˆd)
n

,

where 1 is the indicator function. Informally, the volume-at-dose gives the pro-
portion of the ROI volume receiving a dose of at least ˆd. A DVH for a given ROI
is a plot showing V ˆd as a function of diﬀerent dose levels ˆd, and can be used to
visually compare dose distributions from diﬀerent plans.

3 Methodology

3.1 Serial Version and Data Preprocessing

While the TROTS dataset provides the data required to specify the optimization
problems for each of its patient cases, it does not provide the code to compute

Distributed Optimization for Radiation Therapy

5

(a) Histogram of the distribution of the num-
ber of non-zeros for the dose inﬂuence matri-
ces in the Head-and-Neck problem 01 from the
TROTS dataset.

(b) Greedy distribution of the dose inﬂu-
ence matrices for objective functions.

(c) Greedy distribution of the dose inﬂu-
ence matrices for constraints.

Fig. 1: Distribution of the dose inﬂuence matrices using the greedy algorithms
on six ranks (one rank reserved for IPOPT). The histogram of the distribution
of non-zeros of the dose matrices is shown on top. The bottom ﬁgure shows the
distribution of dose matrices produced by our greedy heuristic with MPI ranks
on the x-axis and corresponding colored bars being the sizes of the dose matrices
assigned to that rank. The total size of all dose matrices for each ranks is shown
above each bar.

the objective functions, constraints or gradients, or code to interface the data to
optimization libraries. To be able to interface the problem to the IPOPT solver,
we have developed a C++-library (which is available on Github3) to enable
the use of general optimization libraries on the dataset. The library provides
a TROTSProblem class, which represents a single TROTS optimization prob-

3 https://github.com/felliu/Optimization-Benchmarks

103104105106107Number of non-zeros024681012Number of matricesHistogram of Number of Non-Zeros for Dose Matrices12345Rank02468Number of non-zeros9142738771995777344576709662709453612345Rank0.000.250.500.751.001.251.501.75Number of non-zeros18686403170752101707727118679547170785116

F. Liu et al.

lem and provides member functions to compute objective functions, constraints
and gradients. Finally, to interface the TROTS problem to the IPOPT opti-
mization library, we simply use IPOPT’s C++-interface. Note also that our
library provides functions to compute function values and ﬁrst-derivatives only,
meaning that the optimization solver used needs to be able to run using only
ﬁrst-derviatives. This does not exclude the use of second-order methods which
incorporate Hessian information in the optimization however, since the Hessian
can be approximated using quasi-Newton methods [2], which are supported in
IPOPT.

IPOPT supports the use of multiple diﬀerent linear solvers to solve the lin-
ear systems arising internally from its optimization algorithm. In general, one
can expect that the overall performance of the optimization solver depends on
the choice of linear solver. Initially, we tried IPOPT using the linear solvers
MUMPS, and MKL Pardiso (a part of Intel MKL), since those packages are
freely available. We used IPOPT’s internal timers to compare the two linear
solvers, and the results are summarized in Table 2. The optimization was run
for a total of 3000 iterations. As seen in Table 2, we get signiﬁcantly better

Function Evaluation (s) PD System Solution (s) Total Time (s)

Setup
891.163
IPOPT w. MUMPS
279.399
IPOPT w. MKL Pardiso
Table 2: Timings comparing IPOPT using MUMPS and MKL Pardiso. All times
are wall-clock times and measured in seconds.

598.884
58.951

248.607
212.223

performance when using the MKL Pardiso linear solver, compared to MUMPS.
Thus we use the MKL Pardiso solver for the remainder of this work.

When using IPOPT with MKL Pardiso, we see that the computational time
becomes dominated by the function evaluations, taking approximately 76% of
the total wall clock time in optimization. This part is thus a natural candidate
for parallelization to further improve performance.

3.2 Parallelization

As mentioned in the previous section, function evaluation is a signiﬁcant compu-
tational bottleneck in the optimization. This part thus becomes a natural target
for parallelization, which can be achieved by distributing terms of the objective
function and constraints between computational nodes. Indeed, that is the ap-
proach we use in this work.

We use MPI to distribute the computation, where MPI rank 0 (the leader) holds
the IPOPT instance (which does not natively support MPI), and the remaining

Distributed Optimization for Radiation Therapy

7

ranks compute objective function and constraints in parallel when requested by
rank 0. The parallelization works such that each MPI process is assigned a set
of terms of the objective function and constraints for which it is responsible for
computing values. When rank 0 requires new function values, it broadcasts the
current values of the optimization variables to all other processes, which then
computes the function and gradient values for which it is responsible, before MPI
collectives are used to aggregate the result to rank 0, which can then proceed
with the next iteration in the optimization algorithm. A conceptual overview of
the parallelization method is shown in Fig. 2.

Listing 1.1 shows the code for the function handling the dispatching of func-
tion evaluations to the diﬀerent MPI ranks which is the key in enabling the use
of a distributed method for computing function values with an MPI-unaware
optimization solver. At startup, when initialization is ﬁnished, all ranks, except
rank 0, call the compute vals mpi function and wait at the MPI Barrier. When
the optimization solver requires new function and constraint values to continue,
it calls compute vals mpi, thus releasing the barrier and allowing all ranks to
compute the required values in parallel. Considering the uneven distribution in

Fig. 2: Illustration of the parallelization method used. Rank 0 holds the opti-
mization solver and the global values for objective functions, constraints and
gradients. The diﬀerent terms and values are spread across the ranks, which
compute local values that are aggregated back to Rank 0 using MPI collectives.

sizes of the dose matrices, as seen in Fig. 1a, a way to balance the workload
between processes is required. When calculating objectives and constraints, the
most computationally expensive part is the sparse matrix-vector product for the

Rank 0IPOPTsolverRank 1Dose MatricesRank 2Dose MatricesRank 3Dose MatricesRank 4Dose MatricesObj. func.GradientConstraintsJacobianRank Local Func. ValuesMPI_Reduce /MPI_Gather /MPI_GathervGlobal Func. ValuesObj. func.GradientConstraintsJacobian8

F. Liu et al.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34

double compute vals mpi(bool calc obj, const double∗ x, double∗ cons vals,

bool calc grad, double∗ grad,
LocalData& local data,
std::optional<ConsDistributionData> distrib data,
bool done) {

while (true) {

//"Task-pool", wait here until rank 0 is requesting function values to be computed
MPI Barrier(MPI COMM WORLD);

//Check if rank 0 is signalling that the optimization is done
int done flag = static_cast<int>(done);
MPI Bcast(&done flag, 1, MPI INT, 0, MPI COMM WORLD);
if (done flag)
return 0.0;

int rank;
MPI Comm rank(MPI COMM WORLD, &rank);
double obj val = 0.0;

//Are we calculating objectives or constraints?
int calc obj flag = static_cast<int>(calc obj);
MPI Bcast(&calc obj, 1, MPI INT, 0, MPI COMM WORLD);
if (calc obj) {

obj val = compute obj vals mpi(x, calc grad, grad, local data);

} else {

compute cons vals mpi(x, cons vals, calc grad,grad, local data, distrib data);

}

//Rank 0 returns to the optimization solver
//to continue to the next iteration / step
if (rank == 0)

return obj val;

}

}

Listing 1.1: ”Task-pool”, function handling dispatching of function evaluations
to diﬀerent ranks. All ranks but 0 wait in the barrier at line 7 until rank 0
requests new function values to be computed.

dose. Thus, we use the number of non-zeros in the dose inﬂuence matrices for
each term to balance the workload between processors.

The problem of distributing the terms of the objectives and constraints as evenly
as possible is an instance of the multi-way number partitioning problem [5], where
one seeks a partitioning of a multiset of integers into k partitions, such that
the discrepancy between the sizes of the partitions is minimal. This problem is
NP-complete, making heuristic algorithms attractive choices. A simple greedy
heuristic is to sort the numbers in descending order, then, in order, assign the
numbers to the partition with the smallest sum at that point. On average, one
would expect this to give a partitioning with a discrepancy on the order of the
smallest number [5]. Considering the diﬀerence in size between the smallest and
largest dose inﬂuence matrices (again, see Fig. 1a), we expect the greedy heuris-
tic to work well enough in our case. Thus, our method of distributing terms of
the objectives and constraints is as follows:

1. Sort the terms in descending order based on number of non-zeros in the

corresponding dose matrix

2. Go through the terms in order, assigning each term to the processor with

the smallest total number of non-zeros.

Distributed Optimization for Radiation Therapy

9

An example of a resulting distribution of matrices for the case with six total
MPI ranks (recalling that one rank is reserved for the optimization solver) is
shown in Fig. 1.

3.3 Experimental Setup

The performance experiments in this study are carried out on following systems:

– Dardel is an HPE Cray EX supercomputer at PDC in Stockholm, Sweden.
We use the main partition on Dardel where each node has two AMD EPYC
7742 CPUs.

– Kebnekaise is a supercomputer at HPC2N in Ume˚a, Sweden. Again we use
the Compute partition of the cluster where each node has a single Intel Xeon
E5-2690v4 CPU, 128 GB of RAM.

We compile our codes using GCC 11.2.0 on both systems. We refer to the Github
repository of the code for the dependencies required to build our library. For
MPI, we use Cray MPICH 8.1.11 on Dardel and OpenMPI 4.1.1 on Kebnekaise.

4 Results

4.1 Performance and Parallel Scaling

(a) Parallel scaling on Dardel, the serial
case uses half a node, since we only want
to use one CPU socket to get a proper
serial reference.

(b) Parallel scaling on Kebnekaise. The
2 node case is omitted, since there is no
parallelism in that case due to the opti-
mization solver occupying one full node.

Fig. 3: Scaling tests on Dardel and Kebnekaise

We begin by assessing the performance and parallel scaling of our code on two
supercomputing clusters. Fig. 3 shows the total run time of the optimizer de-
pending on the number of nodes. In all cases, the optimization was run for 3000

serial2345678Nodes050100150200250Time (s)Optimization Time / Nodes on DardelTotal timeFunction eval timeAmdahl's Law12345678910Nodes050100150200250Time (s)Optimization Time / Nodes on KebnekaiseTotal timeFunction eval timeAmdahl's Law10

F. Liu et al.

iterations, and repeated ﬁve times with the average times and standard devia-
tions (vertical red lines) shown. All times were measured using IPOPT’s internal
timers, using the print timing statistics option. The upper data points (tri-
angles) show the total execution time of the optimization. The lower data points
(squares) show the portion of time in function evaluations only. The solid red
line shows the theoretically possible time as predicted by Amdahl’s law, where
the serial portion is the time spent in IPOPT.

4.2 Plan Quality

Fig. 4: DVH comparison between our plan, computed using IPOPT and 3 nodes
on Dardel, and a reference plan from the TROTS dataset. The DVH curves from
our plan are shown in solid lines and the reference plan is the dashed line. This
plot was created using a slightly modiﬁed version of Matlab scripts provided by
the TROTS authors4.

To verify that our approach produces treatment plans of high quality, we com-
pare DVH curves (see Section 2.3) from our plans with the reference solution
provided from TROTS. Fig. 4 shows DVH curves from our parallel implemen-
tation (solid lines) compared to a reference plan provided by TROTS (dashed
lines) on the ﬁrst VMAT Head-and-Neck case. While a complete discourse on
evaluating plans based on DVH curves is out of scope for this paper, we can see
that the DVH curves between our solution and the reference are quite similar. A
general guideline for treatment plans is that the planning target volume (PTV),
which encompasses the tumor, should receive a suﬃciently high uniform dose,
while organs at risk should receive as little dose as possible.

4 https://github.com/SebastiaanBreedveld/TROTS

0 5 101520253035404550Dose (Gy)0  10 20 30 40 50 60 70 80 90 100Volume (%)PTV 0-46 GyParotid (R)Parotid (L)SMG (R)SMG (L)Oral CavitySpinal CordBrainstemLarynxMCSMCMMCIMCPOesophagusCochlea (R)Cochlea (L)PatientPTV Shell 0 mmPTV Shell 5 mmPTV Shell 15 mmPTV Shell 30 mmPTV Shell 40 mmExternal Ring 20 mmMPI IPOPT PlanTROTS Reference SolutionDistributed Optimization for Radiation Therapy

11

4.3 Performance Analysis and Execution Tracing

Fig. 5: Tracing from a number of iterations of the optimization. Rank 0 handles
the optimization solver (green is time spent in IPOPT) while the other ranks
do the function evaluations where cyan is sparse linear algebra operations (to
compute the dose), red is time waiting for the optimization solver, and the brown
is computing function values from the dose. In the second panel we zoom in and
show three evaluations. In the third panel the diﬀerent stages of the objective
function evaluation are clearly distinguishable.

To further understand the performance of our code, we traced our applications
using Score-P [4]. We traced for a total of 100 iterations using 6 MPI ranks on
the Dardel system, and the trace for a few iterations can be seen in Fig. 5, which
shows the tracing for a few iterations but zoomed in at diﬀerent scales. The red
bars show the idle time in MPI Barrier for each rank, the green bars in rank
0 is the time rank 0 spends in the IPOPT optimization solver. The cyan bars
in the other ranks is the time computing the sparse matrix-vector products to
evaluate function values, and the brown bars come from time spent in function
evaluations outside the matrix-vector products.

From the tracing in Fig. 5 we see that our load balancing scheme works reason-
ably well, with the amount of time spent in the diﬀerent ranks when computing
function values being similar. There is some imbalance, especially when looking
at rank 5, which appears to be caused by the function value evaluation from
the dose. On closer investigation the function type causing the long evaluation
times is the generalized mean, for which we use the C++ standard library func-
tion std::pow to compute the powers, which may be quite expensive. From the
tracing we can also see the limitation in scaling imposed by the serial IPOPT
solver, where the other MPI ranks are idle.

Fig. 6 shows the accumulated time spent in diﬀerent functions for the MPI

12

F. Liu et al.

Fig. 6: The accumulated exclusive time per function. The optimization solver is
found on rank 0 and, green represents compute. The color scheme is the same
as in Fig. 5

ranks, with each row representing one MPI rank. The length of each bar shows
the proportion of time spent in the corresponding function call. We see that the
load balancing between the ranks is decent, but with some room for improve-
ment.

5 Discussion and Conclusion

We have developed a parallel code for solving optimization problems from the
TROTS dataset for radiation therapy treatment planning, capable of utilizing
multi-node computational clusters for evaluating objective functions, constraints
and their gradients. We show that our code can produce treatment plans of high
quality while utilizing high-performance computing clusters eﬀectively. Our ap-
proach distributes the function evaluations at each iteration across computa-
tional nodes, while using the state of the art single-node optimization solver
IPOPT to compute the next iteration. We show that our approach can improve
solution times by a factor of around 3.5 when compared to the serial time on
a traditional supercomputer. While the possible parallel scaling is limited by
the serial portion coming from the optimization solver, computational eﬃciency
is often crucial for real clinics, and improvements in optimization times and
time-to-solution are valued highly.

References

1. Breedveld, S., Heijmen, B.: Data for trots–the radiotherapy optimisation test set.

Data in brief 12, 143–149 (2017)

2. Dennis, Jr, J.E., Mor´e, J.J.: Quasi-newton methods, motivation and theory. SIAM

review 19(1), 46–89 (1977)

3. Drzymala, R., Mohan, R., Brewster, L., Chu, J., Goitein, M., Harms, W., Urie, M.:
Dose-volume histograms. International Journal of Radiation Oncology* Biology*
Physics 21(1), 71–78 (1991)

4. Kn¨upfer, A., R¨ossel, C., Biersdorﬀ, S., Diethelm, K., Eschweiler, D., Geimer, M.,
Gerndt, M., Lorenz, D., Malony, A., Nagel, W.E., et al.: Score-p: A joint performance
measurement run-time infrastructure for periscope, scalasca, tau, and vampir. In:
Tools for High Performance Computing 2011, pp. 79–91. Springer (2012)

5. Korf, R.E.: Multi-way number partitioning. In: Twenty-First International Joint

Conference on Artiﬁcial Intelligence (2009)

Distributed Optimization for Radiation Therapy

13

6. Liu, F., Jansson, N., Podobas, A., Fredriksson, A., Markidis, S.: Accelerating radia-
tion therapy dose calculation with nvidia gpus. In: 2021 IEEE International Parallel
and Distributed Processing Symposium Workshops (IPDPSW). pp. 449–458. IEEE
(2021)

7. Otto, K.: Volumetric modulated arc therapy: Imrt in a single gantry arc. Medical

physics 35(1), 310–317 (2008)

8. W¨achter, A., Biegler, L.T.: On the implementation of an interior-point ﬁlter line-
search algorithm for large-scale nonlinear programming. Mathematical program-
ming 106(1), 25–57 (2006)

