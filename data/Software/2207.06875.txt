Open Tracing Tools: Overview and Critical Comparison

Andrea Janesa, Xiaozhou Lib, Valentina Lenarduzzic

aFree University of Bozen-Bolzano, Italy
bTampere University, Finland
cUniversity of Oulu, Finland

2
2
0
2

l
u
J

4
1

]
E
S
.
s
c
[

1
v
5
7
8
6
0
.
7
0
2
2
:
v
i
X
r
a

Abstract

Background. Coping with the rapid growing complexity in contemporary software architecture, tracing has become an in-
creasingly critical practice and been adopted widely by software engineers. By adopting tracing tools, practitioners are able to
monitor, debug, and optimize distributed software architectures easily. However, with excessive number of valid candidates,
researchers and practitioners have a hard time ﬁnding and selecting the suitable tracing tools by systematically considering
their features and advantages.
Objective. To such a purpose, this paper aims to provide an overview of the popular tracing tools on the market via a critical
comparison.
Method. Herein, we ﬁrst identiﬁed 11 tools in an objective, systematic, and reproducible manner adopting the Systematic Mul-
tivocal Literature Review protocol. Then, we characterized each tool looking at the 1) measured features, 2) popularity both in
peer-reviewed literature and online media, and 3) beneﬁts and issues.
Results. As a result, this paper presents a systematic comparison amongst the selected tracing tools in terms of their features,
popularity, beneﬁts and issues.
Conclusion. Such a result mainly shows that each tracing tool provides a unique combination of features with also different
pros and cons. The contribution of this paper is to provide the practitioners better understanding of the tracing tools facilitat-
ing their adoption.

Keywords: Open Tracing Tool, Telemetry, Multivocal Literature Review

1. Introduction

In software engineering, the outcome of the engineering pro-
cess is invisible [1, 2]. As a consequence, it is difﬁcult to un-
derstand progress and to reason about the produced output
[3]. This is particularly complicated when developing sys-
tems that consist of many components. Today’s trend of de-
veloping systems interacting with components deployed in
the cloud or based on microservice architectures only exac-
erbates this problem.

One way to cope with invisibility is through measurement.
Measurement enhances observability as it provides the data
needed to understand the internal states of systems and its
components [4]. Measurement is deﬁned as the process of
assigning numbers or symbols to the attributes of real-world
entities to describe them using clearly deﬁned rules [5, 2].

Medicine distinguishes between the measuring instru-
ments that need to look inside a patient and those that do not.
For example, blood pressure can be measured directly via an
arterial catheter (called invasive) or by placing a stethoscope
on an artery, pumping up a cuff placed around the arm, and

Email addresses: andrea.janes@unibz.it (Andrea Janes),

xiaozhou.li@tuni.fi (Xiaozhou Li), valentina.lenarduzzi@oulu.fi
(Valentina Lenarduzzi)

reading blood pressure on a special meter called a sphygmo-
manometer (such an approach is called non-invasive). Fol-
lowing the same terminology, measurement of software can
be invasive or non-invasive: we can distinguish methods that
require to modify the source code of the measured system
(e.g., logging relevant events) or methods that consider the
observed system a black box and measure how it interacts
with the environment.

The two terms often used in software measurement are
tracing and telemetry. Tracing, as the word trace, means “a
mark or line left by something that has passed” [6], is often
used by developers to log what has happened and to under-
stand if software is working as expected or not. While trac-
ing is also measurement, the term emphasizes that relevant
events are logged—together with the time of occurrence—
during the execution of software. In contrast, counting the
lines of code of a class is not tracing, it is only measurement.
The second frequent term is telemetry: the word is com-
posed by the Greek adjective tele (remote) and the word me-
tron (measure) and means to measure something and trans-
mitting the results to a distant station [7]. Such an approach
is often needed when a large quantity of data is collected and
it cannot be processed in situ; or, if data is collected from sev-
eral sources (as in a distributed system) and it is necessary to
collect the data in one place to obtain the complete picture of

Preprint submitted to Journal of Systems and Software

July 15, 2022

 
 
 
 
 
 
what is happening in the system. Another term often used in
a distributed context is distributed tracing, where a trace rep-
resents the “whole journey of a request as it moves through
all of the services of a distributed system” [8] and the term
span describes the part of the trace belonging to one service:
a span represents a logical unit of work in completing a user
request within one service; combining all spans describes one
request across all services. Distributed tracing, implicitly, in-
cludes telemetry, since the data collected from various points
needs to be transmitted for processing to another device.

Tracing is used in a variety of cases, e.g., to locate the cause
why a system does not meet performance requirements or
where failures occur. It is part of the toolkit used by software
engineers to monitor, debug, and optimize distributed soft-
ware architectures, such as microservices or serverless func-
tions. Researchers and practitioners need to be aware of the
tools currently in use and what features they possess. Tool
vendors and (potential) tool producers need to understand
how popular and adopted their tools are, as well as for through
features they distinguish themselves from the competitors.

For this purpose, this paper aims to obtain an overview
of tracing tools and to perform a critical comparison among
them, focusing on those that are used among researchers and
practitioners and are available on the market using an Open
Source license. To achieve this objective, we ﬁrst identiﬁed
the tools in an objective, systematic, and reproducible man-
ner adopting a Systematic Multivocal Literature Review ap-
proach [9]. Then, we characterized each tool looking at what
the tracing tool is able to measure, the popularity (both in
peer-reviewed literature and online media), and the reported
advantages/disadvantages.

The remainder of this paper is structured as follows. Sec-
tion 2 presents the process we followed to identify the Open
Tracing Tools studied in this paper, while Sect. 3 describes the
empirical study we conducted. Section 4 describes the ob-
tained results, while Sect. 5 discusses them. Section 6 high-
lights the threats to validity of this work. Section 7 describes
the related work, while Sect. 8 draws the conclusion and fu-
ture works.

2. Systematic Open Tracing Tools Selection

To identify a list of Open Tracing Tools in an objective, sys-
tematic, and reproducible manner, we adopted the approach
of a Systematic Multivocal Literature Review (MLR) [9]. The
MLR process [9] includes both peer reviewed as well as gray
literature and the different perspectives between practition-
ers and academic researchers are taken account in the results.
A MLR emphasizes the inclusion of gray literature in the data
collection process for topics with a strong interest by practi-
tioners. MLR classiﬁes contributions as academic literature
in case of peer-reviewed papers and as gray literature other
types of content like blog posts, white-papers, pod casts, etc.
The process is divided into different phases with the main
steps we followed depicted in Figure 1. We started with the
deﬁnition of the overall goal of the study and the formulation
of research questions. The goal and the research questions

determine the selection of the data sources and the search
terms. The literature review is executed searching the liter-
ature and performing snowballing [10]. After reviewing the
initial set of documents, applying the inclusion/exclusion cri-
teria, and evaluating the quality and credibility of sources, we
obtained 41 documents. Based on a deﬁned data extraction
scheme, we extracted data useful to answer the deﬁned re-
search questions, and—through data synthesis and interpre-
tation—we obtained the answers to the research questions.

Figure 1: Overview of the followed MLR process (adapted from Fig. 7 in [9]

Formulated as a GQM measurement goal [11], the objec-
tive of this paper can be described as follows: “Analyze the
current literature about Open tracing tools for the purpose
of characterization, with respect to its distinctive features, its
popularity, and beneﬁts and issues, from the point of view
of a software developer in the context of a software develop-
ment organization.”

Following the guidelines to formulate questions along the
GQM (goal, question, metric) paradigm [12], we devise the
below research questions operationalizing “utility” into four

2

Deﬁnitive setof documents(41)Selection of datasources and searchtermsDeﬁnition of the MLRgoalDocumentresearchquestionsDeﬁnition of researchquestionsSearch of theliterature andsnowballingGrayliteraturePeer reviewedliteratureGooglesearchInitial set ofdocuments(173)Reading theretrieved literatureApplication ofinclusion/exclusioncriteriaData extractionData synthesisData interpretationgoalresultssynthetizedresultsAnswers to the research questionsDataSequenceData ﬂowLegendActivitydimensions: ability to measure, popularity, beneﬁts, issues.
Consequently, we formulated the following research questions:

RQ1: Which distinctive features do the tools possess?
RQ2: How popular are the identiﬁed tools?
RQ3: Which beneﬁts do the identiﬁed tools claim to achieve?
RQ4: Which issues do the identiﬁed tools introduce?

To obtain a high recall and include as many papers as possi-
ble, we used the following broad search string to retrieve lit-
erature about open tracing tools:

(opentracing OR “open tracing”) AND tool*

We used the asterisk character (*) to capture possible term va-
riations such as plurals and verb conjugations. The search
terms were applied to all the ﬁelds (title, abstract, keywords,
body, and references), so as to include as many works as pos-
sible. We now continue reporting about the next steps in the
review.

Peer-reviewed literature search. We considered the pa-
pers indexed by several bibliographic sources, namely: ACM
digital Library1, IEEEXplore Digital Library2, Science Direct3,
Scopus4, Google Scholar5, CiteseerX6, Inspec7, and Springer
link8. The search was conducted in May 2022, and all raw
data are presented in the replication package (Section 3.4).

Gray literature search. We adopted the same search terms
for retrieving gray literature from online sources as we did
for peer-reviewed ones. We performed the search using four
search engines: Google Search9, Twitter10, Reddit11 and Me-
dium12. The search results consisted in books, blog posts, fo-
rums, websites, videos, white-paper, frameworks, and pod-
casts. This search was performed in April 2021.

Snowballing. Snowballing refers to using the reference
list of a paper or the citations to the paper to identify addi-
tional papers [10]. We applied backward-snowballing to the
academic literature to identify relevant papers from the refer-
ences of the selected sources. Moreover, we applied backward-
snowballing for the gray literature following outgoing links of
each selected source.

Application of inclusion and exclusion criteria. Based
on guidelines for Systematic Literature Reviews [13], we de-
ﬁned inclusion and exclusion criteria. We included only Open
tools, excluding tools that could not be downloaded or in-
stalled, tools with no documentation on how to install or de-
ploy them, as well as tools without a web site.

1https://dl.acm.org
2https://ieeexplore.ieee.org
3https://www.sciencedirect.com
4https://www.scopus.com
5https://scholar.google.com
6https://citeseer.ist.psu.edu
7https://iee.org/Publish/INSPEC/
8https://link.springer.com/
9https://www.google.com/
10https://twitter.com/
11https://www.reddit.com/
12https://medium.com

The term Open Source Software is often deﬁned as “code
that is designed to be publicly accessible—anyone can see,
modify, and distribute the code as they see ﬁt [14].” In this
study, we consider a tool as open already if its source code
is publicly accessible; we did not verify if the license allows
programmers to modify and distribute the source code.

Evaluation of the quality and credibility of sources. Dif-
ferently than peer-reviewed literature, gray literature does not
go through a formal review process, and therefore its quality
is less controlled. To evaluate the credibility and quality of the
selected gray literature sources and to decide whether to in-
clude a gray literature source or not, we extended and applied
the quality criteria proposed by [9] considering the authority
of the producer, the applied methodology, objectivity, date,
novelty, and impact.

Two authors assessed each source using the aforemen-
tioned criteria, with a binary or three-point Likert scale, de-
pending in the criteria itself. In case of disagreement, we dis-
cussed the evaluation with the third author that helped to
provide the ﬁnal assessment.

Table 1 lists the outcome of the systematic tool selection,
i.e., the tools that we identiﬁed through the above described
process.

Table 1: Identiﬁed open tracing tools

Web site
https://www.appdynamics.com
https://www.datadoghq.com
https://www.elastic.co/apm
https://www.inspectit.rocks

https://www.instana.com

https://www.jaegertracing.io
https://lightstep.com
https://skywalking.apache.org

https://www.stagemonitor.org
https://tanzu.vmware.com/observability

https://zipkin.io

Tool name
AppDynamics
Datadog
ElasticAPM
inspectIT Ocelot
(Ocelot)
IBM Observability by
Instana (Instana)
Jaeger
LightStep
Apache SkyWalking
(SkyWalking)
StageMonitor
Wavefront
(Wavefront)
Zipkin

3. Tool analysis

The following steps—based on the research goal and derived
questions described in Sect. 1—describe the study context,
the data extraction and analysis, and describe its veriﬁability
and replicability following the approach suggested by [15].

As a ﬁrst step, we conducted a preliminary analysis aimed
at characterizing each tool with respect to their distinctive
features (RQ1) and the popularity (RQ2) in terms of adop-
tion both in academia and among the developers commu-
nities. Once characterized each tool, we proceeded with a
ﬁner-grained investigation considering beneﬁts (RQ3) and is-
sues (RQ4). Though many tools provide similar functionali-
ties, each one also provides unique features compared to the
others. For practitioners, it is critical to understand the unique

3

advantages and drawbacks of each tool so that they can adopt
the ones best suiting their needs.

3.1. Study context

We considered the eleven open tracing tools retrieved ac-
cording the process described in Sect. 2; Table 1 shows the
selected tools with their relative web site.

3.2. Data extraction

In this section, for each research question, we describe
the collected data, i.e., the data we extracted from the retrieved
search results.

Distinctive features of each tool (RQ1). We extracted char-
acteristics about the identiﬁed tools and grouped them into
the following categories:

1. General information: the links to the source code repos-
itory, the chosen licenses and the adopted program-
ming languages;

2. Deployment: the components contained in each tool,
also in comparison to the suggested architecture de-
ﬁned by the Open Application Performance Manage-
ment (OpenAPM) initiative [16] and their supposed de-
ployment;

3. Usage: the suggested steps to use the tools, i.e., to setup

data collection and to use the collected data;

4. Data: the actual data that can be collected with each

tool, also compared to what the OpenTelemetry [17] stan-
dard suggests: traces, metrics, and logs;

5. Interoperability: aspects that are important to guaran-
tee a high degree of operability: the availability of an
API, support for OpenTelemetry [17], and if self-hosting
is possible.

Tool popularity (RQ2). We evaluated the popularity in
terms of how much the tools are mentioned in public online
sources. The following sources were investigated:

• Peer-reviewed literature: Using the same sources as in
Section 2, we investigated the popularity of each tool by
applying the following search string on all ﬁelds includ-
ing title, abstract, body, and references: (“tool Name”
OR “tool url”) AND (*opentracing* OR “open tracing”).
In the case of tools with different names, we considered
all variants in the “OR” term. Two authors indepen-
dently evaluated the relevance of each publication re-
ported by Google Scholar and Scopus, so as to exclude
papers not written in English, false positives, or from
different domains. In case of disagreement, a third au-
thor provided his/her opinion

• Online media: Using the same sources as in Section 2,
we collected eventual posts, tags, users, groups or web-
sites pertaining to the tools. In particular, we searched
the tools’ own communities, eventual groups present

4

on LinkedIn and Google groups, as well as the number
of appearances in commonly used communities and
discussion forums such as: StackOverﬂow, Reddit, DZo-
ne, and Medium.

Beneﬁts and issues (RQ3 and RQ4) To get the different
opinions, especially on the advantages and problems of each
tool, we extracted the corresponding content of the discus-
sion threads from popular technology forums, including Stack-
Overﬂow, Medium and DZone. StackOverﬂow is the largest
forum of technology-related questions and answers (Q&As)
for developers and tech-enthusiasts. Compared to StackOver-
ﬂow, DZone is also one of the world’s largest online commu-
nities for developers, but focuses more on new tech-trends,
e.g., DevOps, AI and big data, Microservices, etc. Further-
more, similar to DZone, Medium is also a well-known tech-
nology forum that provides tutorials and reviewing articles.
Comparatively, articles on Medium are more common-reader-
friendly and written in a non-technical style. These three plat-
forms are the largest tech communities that can be consid-
ered covering a representative school of opinions described
in different styles.

Due to a different availability of APIs and different crawl-
ing policies of these three platforms, we applied different data
crawling strategies for each platform accordingly:

• StackOverﬂow. We applied API-based content crawl-
ing using the StackExchange API to retrieve the ques-
tions and answers regarding each selected tracing tool.
Speciﬁcally, we used the advanced search API13 to ex-
tract all the questions that contain the name of the tool
in either the title or the body of the question, together
with the according answers. Please note, due to the
daily query limitation of the API, the pagesize parame-
ter was set to the maximum (i.e., 100 results shown per
query) to minimize the crawling time.

• Medium: We adopted a manual crawling approach to
respect Medium’s policy of not allowing web crawling
with tools like BeautifulSoup14. First, we searched the
name of each tool and obtained all the articles about
the tool. For each article, we manually copy/pasted the
content into an individual text ﬁle named with the tool
name and a sequence number.

• DZone: We adopted a hybrid crawling approach comb-
ing manual search and the use of BeautifulSoup. Dif-
ferent from Medium, Dzone allows crawling with Beau-
tifulSoup within each individual article but not within
the list of search results. Hence, we conducted a hy-
brid crawling strategy by manually collecting all article
URLs for each tool and then automatically crawled the
article content for each URL using BeautifulSoup.

13https://api.stackexchange.com/docs/advanced-search
14https://www.crummy.com/software/BeautifulSoup/

topic for each tool. Therefore, the outcome shall pro-
vide the detailed reﬂection on the percentage of posi-
tiveness and negativeness for each topic for each tool.

3.3. Data Analysis

In this Section, we report the data analysis protocol adopted
to answer the research questions.

Distinctive features (RQ1). We inspected the documen-
tation and the website of each tool and mapped the different
features. Then, we created a Table that reports which features
is available for each tool.

Tool popularity (RQ2). We use the statistics from both
scientiﬁc sources and media sources to interpret and com-
pare the popularity of the selected tools.

Beneﬁts and issues (RQ3 and RQ4). Figure 2 depicts the
approach overview of how to elicit the beneﬁts and the issues
for the users regarding open tracing tools adoption based on
the analysis of social media articles. The process is composed
by ﬁve main steps:

• Step 1: Preprocessing: This step pre-processes the raw
text data and prepares them for further analysis. First,
we divide texts from the dataset into sentence level in-
stances since each text can contain multiple topics and
various sentiments. Second, we build the bigram and
trigram models, which means we identify the common
phrases (e.g., New York instead of new and york). Sub-
sequently, for each sentence, a series of text processing
activities are required, including transforming text into
lower cases, removing non-alpha-numeric symbols, screen-
ing stop-words, and eliminating extra white spaces, and
lemmatization.

• Step 2: Filtering: This step is to ﬁlter out the non infor-
mative sentences with trained text classiﬁer. By doing
so, we shall identify the sentences that contain useful
information and screen out those not relevant. Aim-
ing to answer RQ3 and RQ4, the informative sentences
shall contain the explicit description on the beneﬁts or
issues regarding the tracing tools. For example, “In fact,
with automated instrumentation as part of AppDynam-
ics, metric data is produced consistently and compre-
hensively across all teams.” is informative by describ-
ing the beneﬁt of using ; “I checked the source code.” is
non-informative and should be ﬁltered out.

• Step 3: Topic Modeling: Herein, we detect the main top-
ics of the informative sentences identiﬁed from the pre-
vious step. Using topic modeling techniques, we shall
identify the aspects about which the articles are dis-
cussing.

• Step 4: Topic Mapping: In this step, using the topic model
built with the informative texts, we can map each piece
of text, which is about one particular tool, to one or
multiple topic. By doing so, we shall know regarding
each tool which topics are discussed in the social me-
dia and how frequent for each topic.

• Step 5: Opinion Mining: Finally, using opinion mining
techniques on the texts, we can also know the collec-
tive sentiment from the social media concerning each

Figure 2: Approach to detect beneﬁts and issues from analyzing social media
texts (RQ3 and RQ4)

Based on the approach described above, the beneﬁts of
each tool can be obtained analyzing the detected topics in
which the tool is discussed positively, based on the collected
opinions, which shall answer RQ3. Similarly, the issues of
each tool can be summarized by the according topics in which
the tool is mentioned negatively, which answers RQ4.

3.4. Veriﬁability and replicability
To allow our study to be replicated, we have published the
complete raw data in the replication package.15

15https://figshare.com/s/be9e300777faaa8f8b2e

5

Step 1:PreprocessingRaw textsretrieved inSect. 3.2Step 1:PreprocessingStep 2:FilteringStep 3:Topic modellingStep 4:Topic mappingStep 5:Opinion miningDataSequenceDataﬂowLegendActivityDatabasecontentTopics foreach toolOpinionsabout eachtoolToolTopicToolTopicPositiveNegative• Agents are responsible for collecting data from a partic-
ular context, e.g., an application, the operating system,
a mobile app, a database, or a web site. They usually
run as part of applications or as an independent pro-
cess and forward the data to collection components.

• Libraries are used in source code to send data to an
agent or directly to the collection component. In some
scenarios, agents are able to modify the application au-
tomatically so that it sends data to an agent without
necessary source code changes (e.g., instrumenting Java
byte code).

• Storage: After the data is collected, it needs to be stored.
To improve performance, this can occur through a trans-
port component that can fulﬁll routing or caching tasks.
Collectors receive data from agents or other data sources
and persist it to Storage components, e.g., a time-series
database.

• Finally, data processing components elaborate incom-
ing data according to the analysis goals and prepare it
for being used; the OpenAPM initiative distinguishes
visualizations (e.g., in form of charts), dashboarding,
and alerting.

Figure 3 depicts a generalized deployment scenario of the
various components using the terminology of the OpenAPM
initiative. The architecture depicted in Figure 3 also corre-
sponds to the suggested architecture by the OpenTelemetry
project [17] (see below).

4. Results

In this Section, we report the obtained results applying

the steps described in the previous sections.

4.1. Distinctive features (RQ1)

To answer RQ1, we studied each tool to extract their dis-

tinctive features. The results are as follows.

General information. Table 2, for each tool summarizes
the licenses we found in the repositories of the application,
the reported programming languages, and the reference to
the repository. We use the license identiﬁers deﬁned by the
SPDX workgroup [18]. Please note that not all tools are en-
tirely open: AppDynamics, Datadog, Instana, LightStep, and
Wavefront have proprietary parts (e.g., the backend) but re-
lease agents or libraries (see below) using an Open Source li-
cense. The reported programming languages for each tool are
only taken from the repositories with an Open Source license.

Table 2: Features about the tool itself (RQ1)

Tool

License

AppDynamics Proprietary, Apache-2.0,

Datadog

ElasticAPM

Ocelot

Instana

Jaeger

LightStep

SkyWalking

GPL-3.0, MIT [19]
Proprietary, Apache-2.0,
BSD-3-Clause, GPL-2.0,
MIT, MPL-2.0 [20]
Apache-2.0,
BSD-2-Clause,
BSD-3-Clause,
Elastic-2.0, MIT [21]
Apache-2.0 [22]

Proprietary, Apache-2.0,
GPL-2.0, MIT [23]
Apache-2.0 [24]

Proprietary, Apache-2.0,
BSD-2-Clause,
BSD-3-Clause,
CC-BY-SA-4.0, MIT [25]
Apache-2.0 [26]

StageMonitor Apache-2.0 [27]

Wavefront

Zipkin

Proprietary, Apache-2.0,
MIT, MPL-2.0 [28]
Apache-2.0 [29]

Repository

Programming
language
Java, Shell, Python,
JavaScript, Go [19]
Go, Python, Ruby,
JavaScript, Java [20]

Go, Shell, Groovy,
Makeﬁle, HCL, Python,
Other [21]

Java, JavaScript,
Other [22]
Shell, JavaScript, Go,
Java, Python [23]
Go, Shell, Makeﬁle,
Python, Jsonnet,
Dockerﬁle [24]
Go, JavaScript, Python,
Java, HCL [25]

Java, Shell, Python,
FreeMarker, ANTLR,
Batchﬁle, Other [26]
Java, HTML, JavaScript,
Other [27]
Java, Go, Python,
JavaScript, Shell [28]
Java, JavaScript,
TypeScript, Shell,
Dockerﬁle, Smarty [29]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

Deployment. To better understand how each tool is sup-
posed to be used in a tracing scenario, we studied the docu-
mentation of each tool to extract the suggested deployment
conﬁguration.

However, before looking at how the various tools are de-
ployed, it is useful to deﬁne the typical components of a trac-
ing tool. We use the terminology deﬁned by the Open Appli-
cation Performance Management (OpenAPM) initiative [16]
(see Figure 3):

6

Figure 3: APM components according to the OpenAPM initiative [16] and
their typical communication data ﬂow.

User interfaceContextAgentCollectorStorageVisualizationTransportDataprocessingDataDataﬂowLegendComponentGroupingLibraryDashboardingAlertingData collectionAfter studying the documentation of every tool, we noted
that the deployment scenarios of all tools except Ocelot and
StageMonitor can be traced back to the one in Figure 3; these
tools contain various components that allow to collect, pro-
cess, and visualize tracing data. The tools Ocelot and Stage-
Monitor are agents, i.e., they rely on other collectors (e.g., Pro-
metheus16) to further process the collected data. Also ac-
cording to the Gartner Group, AppDynamics, Datadog, Elas-
ticAPM, Instana, and Wavefront are Application performance
monitoring and observability platforms [30].

Table 3 lists each retrieved tool, its type, the identiﬁed com-
ponents using the terminology suggested by the OpenAPM
initiative, and the source (next to the name of the tool), where
we obtained this information. When a tool uses a different
term for a component, we mention this below the table. Please
note that this table contains the components that are explic-
itly mentioned in the documentation. The absence of a com-
ponent, e.g. an explicit transport component, does not mean
that such a component does not exist in the platform but rather
that this component might be contained in another compo-
nent, e.g. the agent. Table 3 also shows that the used termi-
nology is not standardized and that different producers and
teams call components in different ways.

Usage. Regarding the usage of the tracing tools, we stud-
ied their installation and setup requirements, as described in
the documentation.

All tools (except Ocelot and StageMonitor, which are
Agents) are based on a similar setup, based on the suggested
measurement architecture that tracing tools are based on
(see Figure 3). Therefore, using tracing tools always involves
the following steps:

1. Backend installation: If the tool is installed on premise
(as e.g., ElasticAPM [42], SkyWalking [43], or Zipkin [44]):
installing the tool following the documentation;

2. Backend setup: Preparing the backend to receive data:
this step might involve creating an account for the or-
ganization (also called tenant in AppDynamics [45]), se-
lecting a data collection site to respect privacy regula-
tions (as for, e.g., Datadog [46]) and setting up a project.
In the case of ElasticAPM, which uses a combination of
tools within the backend, these tools have to be conﬁg-
ured and connected with each other.

3. Agent setup and application instrumentation: All tools
require the installation of agents and their conﬁgura-
tion [45, 46, 47, 48, 42, 49, 43, 50, 51]. All tools offer
a variety of agents that are able to either a) automati-
cally instrument an application or b) allow developers
to manually instrument it. Automatic instrumentation
means that the target application is modiﬁed in such a
way that it logs and transmits the required data to the
agent without manual work, manual instrumentation
means that the developer has to modify the code man-
ually using the provided library to send what is needed

Table 3: Identiﬁed architectural components

t
r
o
p
s
n
a
r
T

g
n
i
s
s
e
c
o
r
p
a
t
a
D

e
g
a
r
o
t
S

r
o
t
c
e
l
l
o
U
C
(cid:53)2 (cid:53)2 (cid:53)2 (cid:53)2
(cid:53)3 (cid:53)3 (cid:53)3 (cid:53)
(cid:53)
(cid:53)5 (cid:53)

I

t
n
e
g
A

m
r
o
f
t
a
l
P
(cid:53)
(cid:53)
(cid:53)

s
e
i
r
t
a
n
r
e
b
g
i
A
L
(cid:53)1 (cid:53)
(cid:53)
(cid:53)
(cid:53)4 (cid:53)
(cid:53)
(cid:53)6 (cid:53)
(cid:53)7 (cid:53)
(cid:53)7 (cid:53)8
(cid:53)10

(cid:53)

(cid:53)

(cid:53)

(cid:53)
(cid:53)

(cid:53)
(cid:53)
(cid:53)
(cid:53)

Tool
AppDynamics [31]
Datadog [32]
ElasticAPM [33]
Ocelot [34]
Instana [35]
Jaeger [36]
LightStep [37]
SkyWalking [38]
StageMonitor [39]
Wavefront [40]
Zipkin [41]
1 called SDK
2 called Controller
3 called Backend
4 called Tracer API
5 called APM Integration
6 depending on the technology to monitor, the documentation calls the
data collection component library, sensor, tracing SDK, or collector

(cid:53)
(cid:53)3 (cid:53)
(cid:53)
(cid:53)
(cid:53)
(cid:53)9 (cid:53)9 (cid:53)9 (cid:53)9
(cid:53)12 (cid:53)
(cid:53)11 (cid:53)
(cid:53)13
(cid:53)

(cid:53)
(cid:53)1 (cid:53)
(cid:53)

(cid:53)17 (cid:53)17 (cid:53)17

(cid:53)14 (cid:53)15 (cid:53)

(cid:53)
(cid:53)

(cid:53)16

(cid:53)

(cid:53)

(cid:53)

7 relies on the APIs and SDKs provided by the OpenTelemetry [17] project.
8 called Microsatellites
9 called Engine
10 called Probes
11 called Receiver cluster
12 called Aggregator cluster
13 called Widget, only in web applications
14 called Proxy
15 called Service
16 called Reporter
17 called Server

to the agent. The agents have to be conﬁgured that they
send the data to the backend, linking the data to a par-
ticular project. Ocelot and StageMonitor are agents and
require the conﬁguration of a backend, e.g., InﬂuxDB17.

4. Data processing: once the data collection is in place,
the various tools (see Figure 3) allow three different
types of data processing: a) exploratory data analysis
querying the collected data or visualizing it in charts b)
pre-deﬁning frequently needed queries and charts and
storing and presenting them in form of dashboards c)
pre-deﬁning queries and deﬁning thresholds to obtain
alerts if certain conditions are met.

Data. As mentioned in the introduction, distributed tracing
aims to track requests as they ﬂow through the services of a
distributed system. Therefore, foremost, distributed tracing
tools collect data about textittraces, i.e., how a request tra-
verses different services. In addition, tracing tools often also
collect [17] metrics and logs: metrics are measurements that
describe the state of the observed system, e.g., the memory
utilization at timestamp 2022-07-03T18:53:55Z of micro-
service 1. Logs are messages that developers emit with their

16https://prometheus.io

17https://www.influxdata.com

7

code to inform about important events, e.g., that the event
ItemDeleted was initiated by user 7 and occurred with the
timestamp 2022-07-03T18:53:55Z.

Table 4 reports which of the three aspects—tracing, met-
rics, and logs—are collected by the analyzed tools. All tools
collect traces, which is obvious as we are looking at tracing
tools, and all tools except Zipkin allow the additional collec-
tion of metrics and unstructured log data. These additional
data is linked to the component in which the current trace
was recorded and can be helpful when observing a trace. Next
to each cross we provide the point in the documentation de-
scribing the presence of a particular data collection capabil-
ity.

Table 4: Metrics (RQ1)

s
e
c
a
r
T

s
c
i
r
t
e
M

s
g
o
L

Tool
AppDynamics (cid:53) [52] (cid:53) [53] (cid:53) [54]
(cid:53) [55] (cid:53) [56] (cid:53) [57]
Datadog
(cid:53) [58] (cid:53) [59] (cid:53) [60]
ElasticAPM
(cid:53) [61] (cid:53) [62] (cid:53) [63]
Ocelot
(cid:53) [64] (cid:53) [65] (cid:53) [66]
Instana
(cid:53) [67] (cid:53) [68] (cid:53) [69]
Jaeger
(cid:53) [70] (cid:53) [71] (cid:53) [72]
LightStep
(cid:53) [73] (cid:53) [74] (cid:53) [75]
SkyWalking
(cid:53) [27] (cid:53) [76] (cid:53) [77]
StageMonitor
(cid:53) [78] (cid:53) [79] (cid:53) [80]
Wavefront
(cid:53) [29]
Zipkin

Interoperability. To evaluate interoperability, we looked
at three aspects: the presence of a documented API, the sup-
port for OpenTelemetry [17], and if it is possible to self-host
the tool, i.e., to install everything locally.

OpenTelemetry is a “vendor-neutral open-source Observ-
ability framework for instrumenting, generating, collecting,
and exporting telemetry data such as traces, metrics, logs [17].”
We found that it is supported by all tools except StageMonitor.
The results are reported in Table 5. The gray crosses in-
dicate that self-hosting is implicitly possible because the en-
tire tool is provided with an OpenSource license. Please note,
that it might be still possible that a local installation might be
complex, but technically, it is possible.

From the point of view of interoperability, also the data
provided in Table 2 can be of relevance: this table describes
the used licenses of the tool and the used programming lan-
guages.

4.2. Tool popularity (RQ2)

Tool popularity based on peer-reviewed literature. For
each tool, we searched the Peer-reviewed publications that
mentioned it. We found that only three Open Tracing Tools
have been cited by more than 10 papers: Zipkin 29 times,
Jaeger 18 times, and LightStep 10 times. The other nine con-
sidered tools have been cited less than 10 times.

Tool popularity based on Online Media. Shown in the
Figure 4, the search results from three different technology-
based social media platforms, i.e., StackOverﬂow, Medium
and DZone on each tool reﬂect their varied popularity.

Table 5: Features about Interoperability (RQ1)

y
r
t
e
m
e
l
e
T
n
e
p
O

t
r
o
p
p
u
s

g
n
i
t
s
o
h

-
f
l
e
S

API

(cid:53) [84] (cid:53) [85]
(cid:53) [86] (cid:53) [87] (cid:53)
(cid:53) [88] (cid:53)

Tool
AppDynamics (cid:53) [81] (cid:53) [82] (cid:53) [83]
Datadog
ElasticAPM
Ocelot
Instana
Jaeger
LightStep
SkyWalking
StageMonitor
Wavefront
Zipkin
1 OpenTelemetry data can be exported to Zipkin [101].

(cid:53) [89] (cid:53) [90] (cid:53) [91]
(cid:53) [92] (cid:53) [93] (cid:53)
(cid:53) [94] (cid:53) [95]
(cid:53) [96] (cid:53) [97] (cid:53)
(cid:53)

(cid:53) [98] (cid:53) [99]
(cid:53) [100] (cid:53)1

(cid:53)

First of all, Datadog, among the 11 tools, is the most pop-
ular considering the volume of posts in StackOverﬂow (both
questions and answers) and articles in Medium. However, re-
garding the articles on DZone, AppDynamics is the most pop-
ular, which has nearly twice the publicity as the second and
third runner, Zipkin and Datadog. Meanwhile, considering
any of the three platforms, AppDynamics, Datadog, Jaeger and
Zipkin are the most popular, though the rankings vary from
one platform to another. The popularity difference between
these four tools and the others is obvious, as the social media
content volume of them is at least 1.5 times of the ﬁfth run-
ner, ElasticAPM. Except for the ﬁve relevantly popular tools,
the other six are limitedly exposed in the social media. Re-
garding StackOverﬂow, none of these six tools have more than
20 questions or answers in total with Ocelot and LightStep no
more than ﬁve. In DZone, Instana and LightStep have more
coverage than the other four, but still cannot compete with
the top ﬁve. Surprisingly, ElasticAPM is not often mentioned
in Medium (four articles) compared to the other popular tools,
even to the overall less popular Wavefront (28 articles).

4.3. Beneﬁt and issues (RQ3 and RQ4)

Following the approach described in Section 3, here we

describe the steps conducted to answer RQ3 and RQ4.

Step 1: Preprocessing. We pre-processed the crawled tex-
tual data by retaining only the natural language sentences.
Herein we eliminated unnecessary content, such as, the source
code, URLs, publishing date and author info, etc. For Medium
articles, we started eliminating the heading of the article that
includes publishing date and author info by splitting the string
at the common last character of the part “min read” and se-
lecting the later part. Subsequently, we used the sentence to-
kenizer from the Natural Language Toolkit (NLTK)18 to obtain
the list of sentences from each article. As the tokenizer does
not identify the source code or URLs, we eliminated them by

18http://www.nltk.org/

8

3 000 with an incremental step of 20. The test data ratio is
set as default (0.25). Figure 5 shows that with the given train-
ing data, NB performs better than EMNB with the accuracy
can reach as high as 0.76. Thus, we adopted the NB classi-
ﬁer for ﬁltering the informative sentences. Using the classi-
ﬁer trained by the 3 000 training data, we obtained 47 439 in-
formative sentences.

Figure 4: Social media content distribution (updated on April 27, 2021).

selecting only the sentences ending with a period, an excla-
mation mark, or a question mark.

Based on the social media data crawled from StackOver-
ﬂow (1 539 questions and 1 369 answers), Medium (326 arti-
cles) and Dzone (959 posts), we obtained 111 443 sentences
(including source code slices) using the NLTK sentence tok-
enizer. In addition, we screened out the source code slices
by identifying the sentence-level texts ending with a full stop,
question mark or exclamation mark and get 92 979 sentences.
Step 2: Filtering. Herein, we identiﬁed the informative
sentences using a Naïve Bayes (NB) classiﬁer and the Expec-
tation Maximization for Naïve Bayes (EMNB) classiﬁer [102].
The selection shall be based on the accuracy comparison of
these two classiﬁers with the obtained dataset. First, we man-
ually labelled a sufﬁcient number of training data includ-
ing 50% informative sentences and 50% half non-informative
ones. The selection criteria of informative sentences is that
the sentence must explicitly present: 1) the beneﬁts/features
of the tools or 2) the issues of the tools. With increasing num-
ber of training and testing data, the two classiﬁers shall be re-
spectively trained and compared with the F1-score and using
a 5-fold cross validation.

In this study, from the 92 979 sentences obtained pre-
viously, we manually selected 3 000 training data, includ-
ing 1 500 informative sentences and 1 500 non-informative
ones. To evaluate the performance of informativeness ﬁlter-
ing, with a series of experiments, we compared the results of
the NB algorithm and the EMNB algorithm with 3 000 train-
ing data. We inspected the accuracy comparison of the two
classiﬁer with different amount of data starting from 200 to

Figure 5: Testing Informative Text Classiﬁer Accuracy

Step 3: Topic Modeling. To detect the topic of a set of
text using an LDA topic modeling approach [103], a num-
ber of preprocessing steps are required, which include: re-
move punctuation, remove extra space, restore word to its
root form (lemmatization), remove stopwords, and build the
bigram and trigram models.

Furthermore, to ﬁnd the best topic number for each re-
view subset, we conducted a series of experiments for each
set testing with the topic numbers ranging from 2 to 40. We
used the topic coherence representing the quality of the topic
models. Topic coherence measures the degree of semantic
similarity between high scoring words in the topic. A high
coherence score for a topic model indicates the detected top-
ics are more interpretable. Thus, by ﬁnding the highest topic
coherence score, we can decide the most ﬁtting topic num-
ber. Herein, we use c_v coherence measure, which is based
on a sliding window, one-set segmentation of the top words
and an indirect conﬁrmation measure that uses normalized
pointwise mutual information (NPMI) and the cosine simi-
larity [104]. Note that we pick the model that has the highest
c_v value before ﬂattening out or a major drop, in order to
prevent the model from over-ﬁtting.

Shown in Figure 6, we built topic models using LDA with
the number of topics from 2 to 40 for text data. A clear turn-
ing point from the local highest value is at 9 when the model
starts over-ﬁtting when topic number exceeds 15. Thus, the
topic number was determined as 9.

Therefore, with the LDA topic model, we detected the 9
topics as follows: based on the allocated keywords in proba-
bility order, together with the overall term frequency as ref-
erence, two domain experts synthesized their interpretation
of the topics. The extracted topics are: Setup, Instrumenta-

9

1985479931728251197361198452831112543871034225139419651212851339170704461043841121710%20%40%60%80%100%AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKinStackOverflow questionsStackOverflow answersMedium articlesDzone articlesTraining Data500100015002000250030000.550.600.650.700.75AccuracyNaïve BayesExpectation Maximization for Naïve Bayesture are the following-up most frequent topics when for In-
stana it is Inspection/Logging and Usefulness. For Tanzu, the
frequency of the other topics are mostly similar when Inspec-
tion/Logging is the least frequent topic. Furthermore, for Ocelot,
SkyWalking and StageMonitor, number of identiﬁed informa-
tive sentences are very limited, although Deployment/Scala-
bility is the most popular topic for them as well.

Figure 6: Topic Number Detection

tion, Performance, Deployment/Scalability, Management, Ar-
chitecture, Adjusting, Usefulness, and Inspection/Logging. The
list of topics and the according lists of indicator keywords are
shown in Table 6.

Table 6: Topic Interpretation with Indicator Keywords

Topic
Setup

Instrumentation

Performance

Deployment/
Scalability
Management

Architecture

Adjusting

Usefulness

Inspection/
Logging

Indicator keywords
support, version, dependency, instrument, apm,
install, module, setting, etc.
container, conﬁgure, api, instrumentation, code,
conﬁg, change, track, etc.
metric, show, memory, time, cpu, dashboard, status,
graph, attribute, etc.
span, run, monitor, deploy, docker, resource,
pipeline, infrastructure, etc.
service, client, event, network, access, internal port,
communication, etc.
cluster, microservice, base, architecture, several,
export, collection, component, etc.
conﬁguration, option, check, problem, issue, alert,
ensure, parameter, etc.
cloud, provider, help, cost, decision, customize,
pricing, company, insight, business, etc.
trace, error, log, issue, response, cause, metric,
exception, ﬁx, etc.

Step 4: Topic Mapping. With the obtained LDA topic mo-
del, we then mapped each of the informative sentence to one
of the topics to which it was most likely related. Shown in
Figure 7, the numbers of topic-related sentences from the ar-
ticles on each tool are summarized.

Compared with Figure 4, the number of informative sen-
tences for each tool correlated to that of the article crawled
proportionally. Speciﬁcally, amongst the tools which are cov-
ered by more articles, i.e., AppDynamics, Datadog, Zipkin,
Jaeger and ElasticAPM, Deployment/Scalability is the most fre-
quent topic. For ElasticAPM, Zipkin and Jaeger, Inspection/Log-
ging is the second most frequent topic; for AppDynamics and
Datadog, it is the Usefulness of the tools that is talked about
often. Among the tools with fewer relevant texts, i.e., Light-
Step, Instana and Tanzu, Deployment/Scalability remains the
most popular topic. For LightStep, Usefulness and Architec-

10

Figure 7: Topic frequency for each tool

Step 5: Opinion Mining. Using VADER method [105], we
can assess the sentiment of each informative sentence and
furthermore the overall sentiment of each tool in terms of
each topic. Herein, we take into account percentage of posi-
tive, neutral and negative sentences without considering the
according sentiment strength. The percentage sentences in
different sentiments for each tool on each topic is shown in
Figure 8. To determine the beneﬁts and issues in terms of
each extracted aspect, i.e., topic, we compared each set of
sentiment percentage to the average sentiment percentage of
all sentences.

Shown in Table 7, the percentage of the different senti-
ment for each tool was used as the reference. Therefore, we
determined each topic for each tool being either a beneﬁt, an
issue or neutral opinion according to the following criteria.

• If the percentage of positive sentences is higher than
average and the percentage of negative ones lower than
average, the topic is considered as a beneﬁt for the tool.

• If the percentage of positive sentences is lower than av-

Topics510152530350.260.300.320.340.38Coherence20400.280.340.40115716582791413356913921994757112211942731613048315013412269085810792942799405781910105604345931251277334551285356623229015789931081323131127218722913312321341118023981436992501351391103131212213271214945714518412064221231441433161804632772441366001516117948543204127018827148513210%20%40%60%80%100%AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKinSetupInstrumentationPerformanceDeployment/ScalabilityManagementArchitectureAdjustingUsefulnessTable 7: Topic Sentiment Average Percentage for Each Tool

Tool
AppDynamics
Datadog
ElasticAPM
Ocelot
Instana
Jaeger
LightStep
SkyWalking
StageMonitor
Wavefront
Zipkin

Positive Neutral Negative
16.4%
35.7%
13.7%
40.7%
14.8%
39.4%
21.4%
33.5%
15.6%
37.7%
12.7%
46.3%
14.7%
38.4%
16.9%
44.4%
14.4%
53.3%
11.4%
45.2%
13.1%
45.4%

47.9%
45.6%
45.8%
45.1%
46.7%
41.0%
46.9%
38.6%
32.2%
43.4%
41.4%

erage and the percentage of negative ones higher than
average, the topic is considered as an issue for the tool.

• For any other circumstances, the topic is considered

disputed.

Thus, according to the criteria, the according beneﬁts and
issues for each tool in terms of the topics can be summarized
as Table 8, which answers RQ3 and RQ4.

Table 8: The Beneﬁts and Issues for Each Tool regarding Topics

(a) Setup

(b) Instrumentation

(c) Performance

g
o
d
a
t
a
D

M
P
A
c
i
t
s
a
l
E

s
c
i
m
a
n
y
D
p
p
A
– – – – – – – + – – –
+
+ +
–

r
o
t
i
n
o
M
e
g
a
t
S

g
n
i
k
l
a
W
y
k
S

p
e
t
S
t
h
g
i
L

a
n
a
t
s
n
I

+ –

t
o
l
e
c
O

r
e
g
e
a
J

u
z
n
a
T

n
i
k
p
i
Z

+

–

–

– +

–

+ + – – +

–

– –

Criteria
Inspection/Logging
Usefulness
Adjusting
Architecture
Management
Deployment/Scalability + + +
Performance
Instrumentation
Setup
+Beneﬁt
–Issue

– – – – – – –
+
+
+ + + + +
+

+ +

+

+

(d) Deployment/Scalability

(e) Management

(f) Architecture

– –
–
+

4.3.1. RQ3. What beneﬁts are achieved by adopting Open Trac-

ing Tools?

Based on the synthetic analysis on the collected social me-
dia data described above, the main beneﬁt aspects that can
be achieved by adopting each open tracing tool are summa-
rized in Table 8. Speciﬁcally, among all the 11 open tracing
tools, SkyWalking is the only tool providing beneﬁts in Inspec-
tion/Logging. Meanwhile, AppDynamics, Datadog, Instana,
LightStep and Zipkin are the tools that are beneﬁcial in terms
of Usefulness. Tanzu is the only tool proving Architecture re-
lated beneﬁts. On the other hand, the majority of the tools
provide Deployment/Scalability beneﬁts and also Setup bene-
ﬁts. Only four tools, AppDynamics, Ocelot, Instana, and Light-
Step, are beneﬁcial in terms of Instrumentation. However,
none of the 11 tools are considered helpful and providing com-
monly acknowledged beneﬁts in Adjusting, Management, and
Performance.

11

(g) Inspection/Logging

(h) Adjusting

(i) Usefulness

Figure 8: Topic Sentiment for Each Tool

504351646145534333424839483929264940385651451191071377191177AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%49454169473952232535413842473141503669505950131312012111282569AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%424038413932453510373240464230445339506050521915203017151715301316AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%504847494844453447414337424224404540475051481311112712101519389AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%3840363939374135223333484854464953474467615514131015129122211612AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%384040254241443904439454850633649456280485017131113221111020911AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%35313526303031432926313845404940534250435149272425263117277292420AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%484339504640534050403633434425384631502543501915172516131610251714AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%565354445549523225464432363044363837447542481312161291311240128AppDynamicsDatadogElasticAPMOcelotInstanaJaegerLightStepSkyWalkingStagemonitorWavefrontZipKin0%50%100%4.3.2. RQ4. Do Open Tracing Tools introduce any issues?

Similarly, the issues introduced by each of the open trac-

ing tools are also shown in Table 8. Except for SkyWalking men-
tioned previously, all other tools introduce issues in terms of
Inspection/Logging, regardless of the amount of textual con-
tent obtained. Comparatively, SkyWalking is the only tool that
raises issue in Usefulness. Datadog, ElasticAPM, Jaeger and
Zipkin are the tools that introduce issues in Adjusting. Fur-
thermore, AppDynamics, Instana, and StageMonitor also in-
troduce issues in Architecture when SkyWalking is also the
only tool introducing Management related issues. The Ma-
jority of the tools are commonly considered issue-prone in
terms of Performance. LightStep and SkyWalking are the tools
introducing issues in Deployment/Scalability when only Stage-
Monitor introduces issue in Instrumentation. Setup has not
been commonly considered an issue for all the tools.

To be noted, such an NLP-based approach of detecting
the beneﬁts and issues of open tracing tools may fall shorts
due to the potential data abundance. For some particular
tools, e.g., Ocelot, SkyWalking and StageMonitor, the related
text data is not sufﬁcient compared to that of the other tools,
which shall likely result in the lack of reliability in the related
conclusion. On the other hand, the selected data sources,
such as, Dzone and Medium, aim to introduce and promote
emerging or prevailing technologies rather than to criticize
them. As a result, the number of texts with negative senti-
ment is far lower than positive or neutral ones. It is recom-
mended to include more data sources that covers the opin-
ions from the forum end users who provide more unbiased
comments and feedback. Furthermore, due to the limitation
of LDA topic modeling on short texts, the performance of the
approach can be further enhanced by adopting other tech-
niques, such as the Biterm topic model [106], which shall be
included in the future work.

5. Discussion

The analysis of the results revealed interesting insights that
let us distill a number of lessons and/or implications both for
researchers and practitioners.

How to select a tool? Based on the comparative results,
we cannot conclude that any of these tracing tool candidates
is clearly better than the others. According to the results ob-
tained in our study, and speciﬁcally for RQ1, different open
tracing tools provide different features that suit users and or-
ganizations with different preferences. For example, users
who value data retention will more likely choose Elastic APM
or AppDynamics; those who prefer open source solutions with-
out payment will likely select StageMonitor; those using Docker
would prefer LightStep and Zipkin.

We provide comparison across the 11 selected open trac-
ing tools in details regarding their metrics, features, usage, in-
teroperability and so on. Such tips shall largely ease the ef-
fort of the teams in selecting a proper tool according to their
needs.

Which tools are best for what? The outcome of opin-
ion mining from the gray literature (RQ3 and RQ4) shows the

tools’ quality reﬂected by the practitioners in the nine key as-
pects. Therein, the results show that all selected tools per-
form well in some aspects but relatively inadequate in some
others. It is depending on the teams’ main interests and their
criticality criteria that a selected tool can help them the most
with its provided beneﬁts. To be noted, due to the limited
number of mined texts, for some tools, e.g., StageMonitor and
SkyWalking, the reﬂected beneﬁts or issues might contain
bias. Extracting data from other sources, such as, forum
posts, blogs, and tweets, shall enrich the opinion pool; how-
ever, the data crawling for such sources is time-consuming
and possibly violating the privacy protocols.

Which tools are popular? We observed that the com-
munities behind the analyzed tools differ signiﬁcantly.
In
particular, Zipkin and Jaeger are the most cited tools in
peer-reviewed publications, while AppDynamics, Datadog,
Jaeger and Zipkin are the most discussed tools in the selected
online media (RQ2). Moreover, the community behind Data-
dog is the most responsive in term of questions answered in
the online media channels investing in supporting the TD
community by creating posts and tags. The other tools do not
seem to have an active online community supporting them.
Despite the clear gap between the tools in terms of popularity,
it certainly does not reﬂect their quality or usefulness to the
customers. The difference in popularity may result from mar-
keting strategies, investment in promotion, personal prefer-
ences, or simply the psychology of conformity.

What is still missing? It is difﬁcult to identify the beneﬁts
and issues in details regarding each identiﬁed aspect using
the topic modeling and sentiment analysis method. Surveys
and interviews of the practitioners or domain expects shall
facilitate the further investigation. On the other hand, it is
also worthwhile to investigate the in-depth reasons for the
high popularity of some tools, e.g., Datadog and AppDynam-
ics. Furthermore, from the perspective of software evolution,
it is also interesting to investigate the changes of these tools
over time in terms of the provided features, user perceived
beneﬁts and issues, and their popularity.

6. Threats to Validity

Our paper might suffer from threats related to the inac-
curacy of the data extraction, a possible incomplete set of
results due to limitation of the search terms, bibliographic
sources and gray literature search engine, and possible sub-
jectivity related to the deﬁnition and the application of the
exclusion/inclusion criteria. In the this section, we discuss
these threats and the strategies we adopted to mitigate them,
based on the standard checklist for validity threats proposed
in [15].

Construct validity. Construct validities are concerned
with issues that to what extent the object of study truly rep-
resents theory behind the study [15]. The RQs and the clas-
siﬁcation schema adopted might might suffer of this threat.
To limit this threat, the authors reviewed independently and
then discussed collaboratively RQs and the related classiﬁca-
tion schema.

12

Internal Validity. The source selection approach adopted
in this work is described in Section 2.
In order enable the
replicability of our work, we carefully identiﬁed and reported
bibliographic sources adopted to identify the peer-review lit-
erature, search engines, adopted for the gray literature, search
strings as well as inclusion and exclusion criteria. Possible
issues in the selection process are related to the selection of
search terms that could have lead to a non complete set of re-
sults. To overcome to mitigate this risk, we applied a broad
search string. This was possible because of the novelty of
the topic. To overcome the limitation of the search engines,
we queried the academic literature from eight bibliographic
sources, while we included the gray literature from Google,
Medium Search, Twitter Search and Reddit Search. Addition-
ally, we applied a snowballing process to include all the pos-
sible sources. The application of inclusion and exclusion can
be affected by researchers’ opinion and experience. To miti-
gate this threat, all the sources were evaluated by at least two
authors independently.

Conclusion validity. Conclusion validity is related to the
reliability of the conclusions drawn from the results [15].
To ensure the reliability of our treatments, the terminology
adopted in the schema has been reviewed by the authors to
avoid ambiguities. All primary sources were reviewed by at
least two authors to mitigate bias in data extraction and each
disagreement was resolved by consensus, involving the third
author.

External Validity. External validity is related to the gen-
eralizability of the results of our multivocal literature review.
In our study we map the literature on Open Tracing Tools,
considering both the academic and the gray literature. How-
ever, we cannot claim to have screened all the possible liter-
ature, since some documents might have not been properly
indexed, or possibly copyrighted or, even not freely available.

7. Related Work

Among the literature, we identiﬁed only one study that—
as a primary research goal—compares tools to understand
their suitability in different contexts: Li et al. [107] conduct
an industrial survey regarding the different adoption strate-
gies of distributed tracing tools. Covering ten different tools
and ten different companies, the study ﬁnds that the compa-
nies’ tracing and analysis pipelines are similar and that com-
panies choose different tools based on different concerns and
focuses caused by their company size.

More often, researchers compare tracing tools in their
state-of-the-art section to point out a research gap and then
propose their own approach. For example, Bento et al. [108]
propose using tracing data to extract service metrics, depen-
dency graphs and work-ﬂows with the objective of detecting
anomalous services and operation patterns. Therein, the au-
thors reﬂect on advantages and disadvantages of the tools
Jaeger and Zipkin and point out their lack of automated anal-
ysis and processing functionality. Along the same lines, Song
et al. [109] propose ASTracer, a tracing tool for the Apache

Hadoop19 distributed ﬁle system. The authors point out the
shortcomings of tracing tools like Zipkin, Jaeger and Htrace20,
e.g., not considering the execution of different call trees or not
being able to adapt its sampling rate during execution.

Some researchers discuss the use of a varying sampling
rate when collecting data so that a tool can collect more data
when a problem arises and fewer data otherwise. For exam-
ple, Berg et al. [110] propose Snicket, a distributed tracing
system, in which database-style queries are used to express
the analysis the developer wants to perform on the trace data.
This query is then used to generate microservice extensions
that intercept the needed data. The authors compare trac-
ing tools such as Dapper [111], Jaeger, and Canopy [112], and
point out that they may miss important unusual trace in-
formation with a uniform, up-front decided sampling rate.
They also mention LightStep, which prioritizes latest unusual
traces with dynamic tracing and sampling. Also Las-Casas et
al. [113] propose Sifter, a general-purpose distributed tracing
framework that is able to adapt the sampling rate in case of
anomalous and outlier executions. Sifter integrates with X-
Trace [114], Jaeger and Zipkin to obtain tracing samples.

Another frequent type of paper is when researchers use
tracing tools to obtain traces, which are then used in their
research. For example, Gorige et al.
[115] propose a pri-
vacy risk detection framework based on distributed tracing,
to identify privacy and security risks in microservices. They
use Jaeger to collect the required data. Iurman et al. [116]
propose a uniﬁed solution combining in-band telemetry (an
approach to collect data about the network state without af-
fecting network performance) and Application Performance
Management.
In their solution, Jaeger and other tracing
tools can be used to collect application level information.
Avritzer et al. [117] propose PPTAM, a set of tools for perfor-
mance testing and performance-based application monitor-
ing. They also extend their approach in [118] to detect perfor-
mance anti-patterns.

In summary, within the identiﬁed literature, we observe
that tracing tools are mentioned to a) develop innovative
tracing approaches or b) to enhance existing tools, e.g., al-
lowing an adaptive sampling rate. Moreover, often c) re-
searchers use the collected tracing data to obtain other re-
search goals, e.g., identifying anti-patterns. Rarely, in fact we
identiﬁed only one, a general-purpose comparison of various
tools available on the market is the research goal. Such in-
formation can be found often in gray literature, such as blogs
or online articles, e.g., [119], where Barker compares Zipkin,
Jaeger, and Appdash21. Therefore, to obtain an overview over
the available tools, a study has to 1) systematically collect the
tools discussed in the literature and 2) consider both white
and gray literature. This research gap is addressed in this pa-
per.

19https://hadoop.apache.org
20http://htrace.org
21https://github.com/sourcegraph/appdash

13

8. Conclusion

In this work, we compared eleven Open Tracing Tools iden-
tiﬁed adopting the Systematic Multivocal Literature Review
process (MLR). For each tool, we investigated the measured
features, the popularity both in peer-reviewed literature and
online media, and beneﬁts and issues.

The achieved results provided interesting insights among
the eleven tools investigated in this study. The deepest com-
parison we conducted did not allows us to clearly identify a
“silver bullet” tool for any usage. Each tool has different im-
plications under different conditions.

Moreover, it is difﬁcult to identify the beneﬁts and issues

in details regarding each identiﬁed aspect using the topic mod-
eling and sentiment analysis method.

References

[1] Brooks, No Silver Bullet Essence and Accidents of Software Engineer-

ing, Computer 20 (1987) 10–19.

[2] N. E. Fenton, S. L. Pﬂeeger, Software Metrics: A Rigorous and Practical

Approach, PWS Publishing Co., Usa, 2nd edition, 1998.

[3] B. C. Society, R. A. of Engineering (Great Britain), The Challenges of
Complex IT Projects: The Report of a Working Group from the Royal
Academy of Engineering and the British Computer Society, Royal
Academy of Engineering, 2004.

[4] R. Kalman, On the general theory of control systems, IFAC Proceed-
ings Volumes 1 (1960) 491–502. 1st International IFAC Congress on Au-
tomatic and Remote Control, Moscow, USSR, 1960.

[5] L. Finkelstein, M. Leaning, A review of the fundamental concepts of

measurement, Measurement 2 (1984) 25–34.

[6] Merriam-Webster.com Dictionary, trace, https://www.merriam-w

ebster.com/dictionary/trace, accessed on July 7, 2022.

[7] Merriam-Webster.com Dictionary, Telemeter, https://www.merriam
-webster.com/dictionary/telemeter, accessed on July 7, 2022.
[8] I. Mägi, Distributed tracing for dummies, https://plumbr.io/bl
og/monitoring/distributed-tracing-for-dummies, 2020. Ac-
cessed on July 7, 2022.

[9] V. Garousi, M. Felderer, M. V. Mäntylä, Guidelines for including grey
literature and conducting multivocal literature reviews in software en-
gineering, Information and Software Technology 106 (2019) 101–121.
[10] C. Wohlin, Guidelines for Snowballing in Systematic Literature Stud-
in: Proceedings of
ies and a Replication in Software Engineering,
the 18th International Conference on Evaluation and Assessment in
Software Engineering, Ease ’14, Association for Computing Machin-
ery, New York, NY, USA, 2014.

[11] V. R. Basili, A. Trendowicz, M. Kowalczyk, J. Heidrich, C. Seaman,
J. Münch, D. Rombach, Aligning Organizations Through Measure-
ment: The GQM+Strategies Approach, The Fraunhofer IESE Series on
Software and Systems Engineering, Springer International Publishing,
2014.

[12] V. R. Basili, G. Caldiera, H. D. Rombach, The Goal Question Metric Ap-

proach, John Wiley & Sons, 1994.

[13] B. Kitchenham, S. Charters, Guidelines for performing Systematic Lit-
erature Reviews in Software Engineering, Technical Report Ebse 2007-
001, Keele University and Durham University Joint Report, 2007.
[14] RedHat, What is open source?, https://www.redhat.com/en/topi
cs/open-source/what-is-open-source, 2019. Accessed on July 7,
2022.

[15] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, Experimen-

tation in Software Engineering., 2012.

[16] Novatec Consulting, OpenAPM, https://openapm.io, accessed on

July 7, 2022.

[17] The OpenTelemetry Authors, OpenTelemetry, https://openteleme

try.io/docs/, accessed on July 7, 2022.

[18] SPDX Workgroup, SPDX License List, https://spdx.org/licens

es/, 2022. Accessed on July 7, 2022.

14

[19] Appdynamics, Github repository of , https://github.com/Appdyn

amics, accessed on July 7, 2022.

[20] DataDog, Github repository of DataDog, https://github.com/Data

Dog, accessed on July 7, 2022.

[21] Elastic APM-Server contributors, Github repository of the Elas-
tic APM-Server, https://github.com/elastic/apm-server, ac-
cessed on July 7, 2022.

[22] Novatec Consulting, Github repository of inspectIT Ocelot, https://
github.com/inspectIT/inspectit-ocelot, accessed on July 7,
2022.

[23] Instana, Github repository of Instana, https://github.com/instan

a, accessed on July 7, 2022.

[24] Jaeger contributors, Github repository of Jaeger, https://github.c

om/jaegertracing/jaeger, accessed on July 7, 2022.

[25] Lightstep, Github repository of Lightstep, https://github.com/li

ghtstep, accessed on July 7, 2022.

[26] Apache SkyWalking contributors, Github repository of Apache Sky-
Walking, https://github.com/apache/skywalking, accessed on
July 7, 2022.

[27] Stagemonitor contributors, Github repository of Stagemonitor, http
s://github.com/stagemonitor/stagemonitor, accessed on July
7, 2022.

[28] Wavefront, Github repository of Wavefront, https://github.com/wa

vefrontHQ, accessed on July 7, 2022.

[29] Zipkin contributors, Github repository of Zipkin, https://github.c

[30] F. D.

Silva,

om/openzipkin/zipkin, accessed on July 7, 2022.
J.
Byrne,
Application

Gartner Magic
Quadrant
Monitoring,
https://www.gartner.com/en/documents/4000354, 2021. Accessed
on July 7, 2022.

Performance

Chessman,

for

P.

[31] AppDynamics, Architecture, https://docs.appdynamics.com/disp

lay/PRO14S/Architecture, accessed on July 7, 2022.

[32] Datadog, Set up Datadog APM, https://docs.datadoghq.com/trac

ing/setup_overview, accessed on July 7, 2022.

[33] Elastic, Components and documentation, https://www.elast
ic.co/guide/en/apm/guide/current/apm-components.html,
accessed on July 7, 2022.

[34] Novatec Consulting, Hello World, https://inspectit.github.io/

inspectit-ocelot/docs/doc1, accessed on July 7, 2022.

[35] Instana, Setting up and managing Instana, https://www.ibm.com/
docs/en/obi/current?topic=setting-up-managing-instana,
accessed on July 7, 2022.

[36] Jaeger contributors, Architecture, https://www.jaegertracing.io/

docs/1.36/architecture/, accessed on July 7, 2022.

[37] Lightstep, Lightstep Architecture Explained, https://lightstep.c
accessed

om/blog/lightstep-xpm-architecture-explained,
on July 7, 2022.

[38] Apache Skywalking contributors, Overview, https://skywalking.a
pache.org/docs/main/v9.0.0/en/concepts-and-designs/over
view, accessed on July 7, 2022.

[39] Stagemonitor contributors, Installation, https://github.com/stag
emonitor/stagemonitor/wiki/Installation, accessed on July 7,
2022.

[40] VMware, Enterprise Grade on Day 1, https://tanzu.vmware.com/

content/vmware-tanzu-observability-features/enterpri
se-grade-on-day-1, accessed on July 7, 2022.

[41] Zipkin contributors, Architecture, https://zipkin.io/pages/arch

itecture.html, accessed on July 7, 2022.

[42] Elastic, Quick start, https://www.elastic.co/guide/en/apm/guid
e/current/apm-quick-start.html, accessed on July 7, 2022.

[43] Apache

Skywalking

contributors,

SkyWalking

9.x

showcase,

https://skywalking.apache.org/docs/skywalking-showc
ase/latest/readme/, accessed on July 7, 2022.

[44] Zipkin contributors, Quickstart, https://zipkin.io/pages/quic

kstart.html, accessed on July 7, 2022.

[45] AppDynamics, Getting Started, https://docs.appdynamics.com/
appd/22.x/latest/en/appdynamics-essentials/getting-sta
rted, accessed on July 7, 2022.

[46] Datadog, Getting Started, https://docs.datadoghq.com/gettin

g_started/, accessed on July 7, 2022.

[47] IBM, Getting started with Instana, https://www.ibm.com/docs/en/

instana-observability/current?topic=references-getti
ng-started-instana, accessed on July 7, 2022.

[48] Lightstep, Get started, https://docs.lightstep.com/get-start

ed, accessed on July 7, 2022.

[49] Jaeger contributors, Getting Started, https://www.jaegertracing.i

o/docs/1.36/getting-started/, accessed on July 7, 2022.

[75] Apache Skywalking contributors, Log Collection and Analysis,

https://skywalking.apache.org/docs/main/latest/en/setu
p/backend/log-analyzer/, accessed on July 7, 2022.

[76] Stagemonitor contributors, Track your own metrics, https://gith
ub.com/stagemonitor/stagemonitor/wiki/Track-your-own-m
etrics, accessed on July 7, 2022.

[50] Wavefront, Getting Started Pages, https://docs.wavefront.com/la

[77] Stagemonitor contributors, Logging Dashboard, https://github

bel_getting%20started.html, accessed on July 7, 2022.

[51] Zipkin contributors, Tracers and Instrumentation, https://zipkin
.io/pages/tracers_instrumentation.html, accessed on July 7,
2022.

[52] AppDynamics, What Is Distributed Tracing?, https://www.appdyna
mics.com/topics/distributed-tracing, accessed on July 7, 2022.
[53] AppDynamics, Extensions and Custom Metrics, https://docs.app
dynamics.com/appd/21.x/21.7/en/infrastructure-visibilit
y/machine-agent/extensions-and-custom-metrics, accessed
on July 7, 2022.

.com/stagemonitor/stagemonitor/wiki/Logging-Dashboard,
accessed on July 7, 2022.

[78] VMware, Microservices Observability With Distributed Tracing,
https://tanzu.vmware.com/content/vmware-tanzu-observabi
lity-features/microservices-observability-with-distr
ibuted-tracing, accessed on July 7, 2022.

[79] VMware, App Metrics for VMware Tanzu, https://docs.vmware.c

om/en/App-Metrics-for-VMware-Tanzu/2.1/app-metrics/GU
ID-index.html, accessed on July 7, 2022.

[80] Wavefront, Log Data Integration, https://docs.wavefront.com/lo

[54] AppDynamics, Log Analytics, https://www.appdynamics.com/prod

g.html, accessed on July 7, 2022.

uct/how-it-works/application-analytics/log-analytics,
accessed on July 7, 2022.

[55] Datadog, Distributed Tracing Overview, https://www.datadoghq.c
om/knowledge-center/distributed-tracing/, accessed on July
7, 2022.

[56] Datadog, Metrics, https://docs.datadoghq.com/metrics/, ac-

cessed on July 7, 2022.

[57] Datadog, Visualize Key Log Metrics, https://www.datadoghq.com/

dg/logs/log-metrics/, accessed on July 7, 2022.

[58] Elastic, Distributed tracing, https://www.elastic.co/guide/en/
apm/guide/current/apm-distributed-tracing.html, accessed
on July 7, 2022.

[59] Elastic, Metrics, https://www.elastic.co/guide/en/apm/guide/
current/data-model-metrics.html, accessed on July 7, 2022.
[60] Elastic, How to easily correlate logs and APM traces for better observ-
ability, https://www.elastic.co/blog/how-to-easily-correla
te-logs-apm-traces-for-better-observability-elastic-s
tack, accessed on July 7, 2022.

[61] Novatec Consulting, Tracing, https://inspectit.github.io/insp

ectit-ocelot/docs/tracing/tracing, accessed on July 7, 2022.

[62] Novatec Consulting, Metrics Recorders, https://inspectit.githu
b.io/inspectit-ocelot/docs/metrics/metric-recorders, ac-
cessed on July 7, 2022.

[63] Novatec Consulting, Log Correlation, https://inspectit.githu
b.io/inspectit-ocelot/docs/tracing/log-correlation, ac-
cessed on July 7, 2022.

[81] AppDynamics, AppDynamics APIs, https://docs.appdynamics.c
om/appd/21.x/21.7/en/extend-appdynamics/appdynamics-api
s, accessed on July 7, 2022.

[82] AppDynamics, AppDynamics for OpenTelemetry, https://www.app
dynamics.com/product/opentelemetry, accessed on July 7, 2022.
Started with AppDynamics On-Premise,

[83] AppDynamics, Get

https://docs.appdynamics.com/display/PRO39/Get+Star
ted+with+AppDynamics+On-Premise, accessed on July 7, 2022.
[84] Datadog, API Reference, https://docs.datadoghq.com/api/late

st, accessed on July 7, 2022.

[85] Datadog, OpenTelemetry and OpenTracing, https://docs.datad
oghq.com/tracing/setup_overview/open_standards/, accessed
on July 7, 2022.

[86] Elastic, API, https://www.elastic.co/guide/en/apm/guide/cu

rrent/api.html, accessed on July 7, 2022.

[87] Elastic, OpenTelemetry integration, https://www.elastic.co/gu
ide/en/apm/guide/current/open-telemetry.html, accessed on
July 7, 2022.

[88] Novatec Consulting, Breaking changes in 2.0.0, https://inspecti
t.github.io/inspectit-ocelot/docs/breaking-changes/Brea
king%20Changes#breaking-changes-in-200, accessed on July 7,
2022.

[89] Instana, Agent REST API, https://instana.github.io/openapi/

#section/Agent-REST-API, accessed on July 7, 2022.

[90] IBM, OpenTelemetry, https://www.ibm.com/docs/en/obi/curren

t?topic=apis-opentelemetry, accessed on July 7, 2022.

[64] Instana, Distributed Tracing

for Microservice Applications,

[91] IBM, Self-hosted Instana backend on Docker

(on-premises),

https://www.instana.com/automatic-distributed-traci
ng-and-analysis/, accessed on July 7, 2022.

[65] Instana, Application Metrics, https://instana.github.io/openap

https://www.ibm.com/docs/en/obi/current?topic=instan
a-self-hosted-backend-docker-premises, accessed on July 7,
2022.

i/#tag/Application-Metrics, accessed on July 7, 2022.

[92] Jaeger contributors, APIs, https://www.jaegertracing.io/docs/

[66] Instana, LogDNA, https://www.instana.com/supported-technol

1.36/apis/, accessed on July 7, 2022.

ogies/logdna/, accessed on July 7, 2022.

[67] Jaeger contributors, Introduction, https://www.jaegertracing.i

o/docs/1.36/, accessed on July 7, 2022.

[93] Jaeger contributors, Deprecating Jaeger clients, https://www.jaege
rtracing.io/docs/1.35/client-libraries/#deprecating-jae
ger-clients, accessed on July 7, 2022.

[68] Jaeger contributors, Introduction, https://www.jaegertracing.i

[94] Lightstep, Overview, https://api-docs.lightstep.com/refere

o/docs/1.36/, accessed on July 7, 2022.

nce/overview, accessed on July 7, 2022.

[69] Jaeger contributors, Troubleshooting, https://www.jaegertracing
.io/docs/1.36/troubleshooting/, accessed on July 7, 2022.
[70] Lightstep, Distributed tracing: A Complete Guide, https://lights

tep.com/distributed-tracing, accessed on July 7, 2022.

[71] Lightstep, Get started with Lightstep Observability, https://docs.l
ightstep.com/docs/welcome-to-lightstep, accessed on July 7,
2022.

[72] Lightstep, Find correlated areas of latency and errors, https://do
cs.lightstep.com/docs/find-correlated-areas-of-latency,
accessed on July 7, 2022.

[73] Apache Skywalking contributors, Overview , https://skywalking
.apache.org/docs/main/latest/en/concepts-and-designs/ov
erview/, accessed on July 7, 2022.

[95] Lightstep, OpenTelemetry, https://lightstep.com/developers/

opentelemetry, accessed on July 7, 2022.

[96] Apache

Skywalking

contributors,
https://skywalking.apache.org/docs/skywalking-banya
ndb/latest/api-reference, accessed on July 7, 2022.

Protocol Documentation,

Skywalking

[97] Apache

contributors,
https://skywalking.apache.org/docs/main/latest/en/se
tup/backend/opentelemetry-receiver, accessed on July 7, 2022.
[98] Wavefront, Wavefront REST API, https://docs.wavefront.com/wa

OpenTelemetry

receiver,

vefront_api.html, accessed on July 7, 2022.

[99] Wavefront, Send OpenTelemetry Data, https://docs.wavefront.c
om/opentelemetry_tracing.html, accessed on July 7, 2022.
[100] Zipkin contributors, Server extensions and choices, https://zipkin

[74] Apache Skywalking contributors, Meter receiver, https://skywalki

.io/pages/extensions_choices.html, accessed on July 7, 2022.

ng.apache.org/docs/main/latest/en/setup/backend/backen
d-meter, accessed on July 7, 2022.

[101] Zipkin contributors, Opentelemetry Zipkin Exporter, https://open
-telemetry.github.io/opentelemetry-python/exporter/zipk

15

in/zipkin.html, accessed on July 7, 2022.

[102] K. Nigam, A. K. McCallum, S. Thrun, T. Mitchell, Text classiﬁcation
from labeled and unlabeled documents using EM, Machine learning
39 (2000) 103–134.

[103] D. M. Blei, A. Y. Ng, M. I. Jordan, Latent dirichlet allocation, the Journal

of machine Learning research 3 (2003) 993–1022.

[104] S. Syed, M. Spruit, Full-text or abstract? Examining topic coherence
scores using latent dirichlet allocation,
in: 2017 IEEE International
conference on data science and advanced analytics (DSAA), Ieee, pp.
165–174.

[105] C. Gilbert, E. Hutto, Vader: A parsimonious rule-based model for sen-
timent analysis of social media text, in: Eighth International Confer-
ence on Weblogs and Social Media (ICWSM-14), volume 81, p. 82.

[106] X. Cheng, X. Yan, Y. Lan, J. Guo, Btm: Topic modeling over short
IEEE Transactions on Knowledge and Data Engineering 26

texts,
(2014) 2928–2941.

[107] B. Li, X. Peng, Q. Xiang, H. Wang, T. Xie, J. Sun, X. Liu, Enjoy your ob-
servability: an industrial survey of microservice tracing and analysis,
Empirical Software Engineering 27 (2022) 1–28.

[108] A. Bento, J. Correia, R. Filipe, F. Araujo, J. Cardoso, Automated analysis
of distributed tracing: Challenges and research directions, Journal of
Grid Computing 19 (2021) 1–15.

[109] Y. Song, Y. Li, S. Wu, H. Yang, W. Li, ASTracer: An Efﬁcient Tracing Tool
for HDFS with Adaptive Sampling, in: IFIP International Conference
on Network and Parallel Computing, Springer, pp. 107–119.

[110] J. Berg, F. Ruffy, K. Nguyen, N. Yang, T. Kim, A. Sivaraman, R. Netravali,
S. Narayana, Snicket: Query-Driven Distributed Tracing, in: Proceed-
ings of the Twentieth ACM Workshop on Hot Topics in Networks, pp.
206–212.

[111] B. H. Sigelman, L. A. Barroso, M. Burrows, P. Stephenson, M. Plakal,
D. Beaver, S. Jaspan, C. Shanbhag, Dapper, a Large-Scale Distributed
Systems Tracing Infrastructure, Technical Report, Google, Inc., 2010.

[112] J. Kaldor, J. Mace, M. Bejda, E. Gao, W. Kuropatwa, J. O’Neill, K. W. Ong,
B. Schaller, P. Shan, B. Viscomi, V. Venkataraman, K. Veeraraghavan,
Y. J. Song, Canopy: An End-to-End Performance Tracing And Analysis
System, in: Proceedings of the 26th Symposium on Operating Systems
Principles, Sosp ’17, Association for Computing Machinery, New York,
NY, USA, 2017, p. 34–50.

[113] P. Las-Casas, G. Papakerashvili, V. Anand, J. Mace, Sifter: Scalable sam-
pling for distributed traces, without feature engineering, in: Proceed-
ings of the ACM Symposium on Cloud Computing, pp. 312–324.
[114] R. Fonseca, G. Porter, R. H. Katz, S. Shenker, X-Trace: A Pervasive Net-
work Tracing Framework, in: 4th USENIX Symposium on Networked
Systems Design & Implementation (NSDI 07), USENIX Association,
Cambridge, MA, 2007.

[115] D. Gorige, E. Al-Masri, S. Kanzhelev, H. Fattah, Privacy-Risk Detec-
tion in Microservices Composition Using Distributed Tracing, in: 2020
IEEE Eurasia Conference on IOT, Communication and Engineering
(ECICE), Ieee, pp. 250–253.

[116] J. Iurman, F. Brockners, B. Donnet, Towards cross-layer telemetry, in:
Proceedings of the Applied Networking Research Workshop, pp. 15–21.
[117] A. Avritzer, M. Camilli, A. Janes, B. Russo, J. Jahic, A. Hoorn, R. Britto,
C. Trubiani, PPTAMλ: What, Where, and How of Cross-domain Scala-
bility Assessment, pp. 62–69.

[118] A. Avritzer, R. Britto, C. Trubiani, B. Russo, A. Janes, M. Camilli,
A. Van Hoorn, R. Heinrich, M. Rapp, J. Henß, A Multivariate Char-
acterization and Detection of Software Performance Antipatterns, pp.
61–72.

[119] D. Barker, 3 open source distributed tracing tools, https://open
source.com/article/18/9/distributed-tracing-tools, 2018.
Accessed on July 7, 2022.

8.1. Acknowledgment

The research presented in this article has been partially
funded by the “Software Rejuvenation” project funded by Ulla
Tuominen Foundation.

16

