2
2
0
2

n
u
J

0
1

]

C
D
.
s
c
[

2
v
8
8
2
1
0
.
6
0
2
2
:
v
i
X
r
a

Decentralized Training of Foundation Models in
Heterogeneous Environments

Binhang Yuan†∗, Yongjun He†∗, Jared Quincy Davis‡, Tianyi Zhang‡, Tri Dao‡,
Beidi Chen‡, Percy Liang‡, Christopher Re‡, Ce Zhang†
†ETH Z¨urich, Switzerland ‡Stanford University, USA
{binhang.yuan, yongjun.he, ce.zhang}@inf.ethz.ch
{tz58, jaredq, beidic, trid, pliang, chrismre}@stanford.edu

Abstract

Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens
of thousands of GPUs running continuously for months. These models are typically trained in special-
ized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that
support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and
difﬁcult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and
lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized set-
ting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art
schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous
data center setting. In this paper, we present the ﬁrst study of training large foundation models with model
parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a
scheduling algorithm that allocates different computational “tasklets” in the training of foundation models
to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal
cost model and further propose an efﬁcient evolutionary algorithm to ﬁnd the optimal allocation strategy.
We conduct extensive experiments that represent different scenarios for learning over geo-distributed de-
vices simulated using real-world network measurements. In the most extreme case, across 8 different cities
spanning 3 continents, our approach is 4.8× faster than prior state-of-the-art training systems (Megatron).

Code Availability: https://github.com/DS3Lab/DT-FM

1 Introduction

Recent years have witnessed the rapid development of deep learning models, particularly foundation mod-
els (FMs) [1] such as GPT-3 [2] and PaLM [3]. Along with these rapid advancements, however, comes
computational challenges in training these models: the training of these FMs can be very expensive — a
single GPT3-175B training run takes 3.6K Petaﬂops-days [2]— this amounts to $4M on today’s AWS on
demand instances, even assuming 50% device utilization (V100 GPUs peak at 125 TeraFLOPS)! Even the
smaller scale language models, e.g., GPT3-XL (1.3 billion parameters), on which this paper evaluates, re-
quire 64 Tesla V100 GPUs to run for one week, costing $32K on AWS. As a result, speeding up training
and decreasing the cost of FMs have been active research areas. Due to their vast number of model pa-
rameters, state-of-the-art systems (e.g., Megatron[4], Deepspeed[5], Fairscale[6]) leverage multiple forms
of parallelism [4, 7, 8, 9, 10, 11]. However, their design is only tailored to fast, homogeneous data center
networks.

* Equal contribution.

1

 
 
 
 
 
 
Figure 1: Given 1(cid:13) a set of computation tasklets involved in training foundation models (corresponding to
different micro-batches and layers), and 2(cid:13) a heterogeneous network between devices, the goal is to ﬁnd the
optimal 3(cid:13) allocation of tasklets to devices.

On the other hand, decentralization is a natural and promising direction. Jon Peddie Research reports that
the PC and AIB GPU market shipped 101 million units in Q4 2021 alone [12]. Furthermore, many of these
GPUs are underutilized. Leveraging this fact, volunteer computing projects such as Folding@Home [13] have
sourced upwards of 40K Nvidia and AMD GPUs continuously [14]. Moreover, The incremental electricity and
HVAC costs of running a V100 GPU for a volunteer are 50–100× lower than the spot prices for an equivalent
device on AWS [15]. If we could make use of these devices in a decentralized open-volunteering paradigm
for foundation model training, this would be a revolutionary alternative to the expensive solutions offered
by data centers.

This vision inspired many recent efforts in decentralized learning, including both those that are theoreti-
cal and algorithmic [16, 17, 18], as well as recent prototypes such as Learning@Home [19] and DeDLOC [20].
However, efforts to-date in decentralized training either focus solely on data parallelism [16, 17, 18, 20],
which alone is insufﬁcient for FMs whose parameters exceed the capacity of a single device, or orient around
alternative architectures, e.g., mixture of experts [19]. These alternative architectures provide promising
directions for decentralized learning, however, they are currently only trained and evaluated on smaller
datasets and at a smaller computational scale (e.g., MNIST and WikiText-2 in [19]) than their state-of-the-
In this paper, we focus on a standard GPT-style architecture, without
art counterparts, e.g., GLaM [21].
considering any changes that might alter the model architecture or the convergence behaviour during train-
ing.

To fulﬁll the potential of decentralization for the training of FMs, we need to be able to (1) take advantage
of computational devices connected via heterogeneous networks with limited bandwidth and signiﬁcant
latency, and (2) support forms of parallelism beyond pure data parallelism.
In this paper, we tackle one
fundamental aspect of this goal — how can we assign different computational “tasklets”, corresponding to a
macro-batch and a subset of layers, to a collection of geo-distributed devices connected via heterogeneous,
slow networks? This is not an easy task — even for fast and homogeneous data center networks, such
assignments are still an open ongoing research challenge [22, 23, 24, 25, 26]. For the heterogeneous setting,
it becomes even more challenging as the size of the search space increases dramatically. In the homogeneous
setting, the homogeneity of the edges in the communication graph reduces the search space into many
equivalent classes representing allocation strategies with the same communication costs, enabling efﬁcient
polynomial runtime algorithms [23, 24, 22, 25, 26]; however, in the heterogeneous setting, one has to
consider potentially exponentially many more distinct allocation strategies — as we will see later, because of
the heterogeneity of the communication matrix, even the sub-problem of ﬁnding the best pipeline parallelism
strategy equates to a hard open loop travelling salesman problem [27].

In this paper, we focus on this challenging scheduling problem of decentralized training of FMs over slow,

2

OregonFrankfurtOhioLondonVirginiaTokyoSeoulIrelandMacro Batch 1Macro Batch 2Stages“Tasklets”0.4Gbps, 235ms0.4Gbps, 223ms0.5Gbps, 96ms1.2Gbps, 76ms0.4Gbps, 136ms1.1Gbps, 49ms0.8Gbps, 67ms1.1Gbps, 24ms1.1Gbps, 34ms❶❷tasklet allocation❸heterogeneousbandwidth & latencyheterogeneous networks, and make the following contributions:

• We study the problem of allocating distributed training jobs over a group of decentralized GPU devices

connected via a slow heterogeneous network. More speciﬁcally:

– To capture the complex communication cost for training FMs, we propose a natural, but novel,
formulation involving decomposing the cost model into two levels: the ﬁrst level is a balanced
graph partitioning problem corresponding to the communication cost of data parallelism, whereas
the second level is a joint graph matching and traveling salesman problem corresponding to the
communication cost of pipeline parallelism.

– We propose a novel scheduling algorithm to search for the optimal allocation strategy given our
cost model. Developing a direct solution to this optimization problem is hard; thus, we propose
an efﬁcient evolutionary algorithm based on a collection of novel heuristics, going beyond the
traditional heuristics used in standard graph partitioning methods [28].

• We carefully designed and implemented a collection of system optimizations to hide communication

within the computation to further reduce the impact of slow connections.

• We conduct extensive experiments that represent different scenarios of collaborative decentralized
learning, simulated by using network measurements from different geographical regions of AWS. In
the worldwide setting with 64 GPUs across 8 regions (Oregon, Virginia, Ohio, Tokyo, Seoul, Lon-
don, Frankfurt, Ireland), we show that our system is 3.8-4.8× faster, in end-to-end runtime, than the
state-of-the-art system, Megatron, for training GPT3-XL, without any difference in what is computed or
convergence dynamics.
In addition, we also provide careful ablation studies to show the individual
effectiveness of the scheduler and system optimizations.

• We shed light on the potential of decentralized learning — our prototype in the global heterogeneous
setting is only 1.7-3.5× slower than Megatron in data centers even though its network can be 100×
slower. We hope this paper can inspire future explorations of decentralized learning for FMs, over
geo-distributed servers, desktops, laptops, or even mobile devices.

Limitations and Moving Forward.

In this paper, we tackle one foundational aspect of decentralized
learning but leave as future work many problems that are important for a practical system. We assume that
communication between devices is relatively stable for a reasonable amount of time and that all devices are
always online without failure or eviction. Note that we also do not train a full system to full convergence,
instead running partial training to conﬁrm intermediate result equivalence across regimes. Scheduling over
a dynamic, heterogeneous environment and providing fault tolerance, potentially with checkpointing, while
training to convergence are directions for future exploration.

2 Decentralized Training of Foundation Model: Problem Formulation

We ﬁrst introduce concepts, technical terms, and the procedure of decentralized training. Then we formally
deﬁne the scheduling problem this paper tackles.

Decentralized setting. We assume a group of devices (GPUs) participating in collaborative training of
a foundation model. Each pair of devices has a connection with potentially different delay and bandwidth.
These devices can be geo-distributed, as illustrated in Figure 1, with vastly different pairwise communication
bandwidth and latency. In decentralized training, all layers of a model are split into multiple stages, where
each device handles a consecutive sequence of layers, e.g., several transformer blocks [29].
In addition,
since the input for foundation model pre-training is huge, e.g., a few millions of tokens, it is also split into
multiple macro-batches that can be handled in parallel.

3

Problem deﬁnition. We deﬁne tasklets as a collection of
computational tasks in foundation model training — Tasklet
ti,j is the forward and backward computation for a stage j with
a macro-batch i of training data in a training iteration. We
aim to design an effective scheduler to assign each tasklet to a
particular device so that the training throughput is maximized
in decentralized training.

Parallelism. The above setting (shown in Figure 2) in-
volves two forms of parallelism, pipeline and data. In pipeline
parallelism, the compute in multiple stages is parallelized —
each device handles activation or gradient computation for dif-
ferent macro-batches in parallel and the results can be com-
municated or passed to subsequent stages. Data parallelism
means that devices compute the gradient for different macro-
batches independently, but need to synchronize these gradients
through communication. In a decentralized environment, the
training procedure is communication-bounded. The scheduling
problem is to accelerate the communication procedure by allocating tasklets that require high communica-
tion volumes between them to devices with faster connections.

Figure 2: An example pipeline and data
parallelism with four devices.

Formalization of the scheduling problem. Formally, our scheduling problem is as follows.
• Let D = {d1 . . . dN } be a set of N devices; A ∈ RN ×N

be the communication matrix
+
between these devices describing the delay and bandwidth respectively, where the delay and bandwidth
between device d and d(cid:48) is αd,d(cid:48) and βd,d(cid:48).

and B ∈ RN ×N

+

• Given the communication matrix A and B, we construct a communication graph G (Figure 3(a)) —
each device corresponds to a node in G and each edge between d and d(cid:48) is labeled with the average
latency and bandwidth between d and d(cid:48): ((αd,d(cid:48) + αd(cid:48),d)/2, (βd,d(cid:48) + βd(cid:48),d)/2). Even though A and B
are asymmetric (i.e., upload and download speed might be different), the communication graph G is
symmetric because in our workloads all communications between two devices happen to involve the
same amount of upload and download.

• The number of stages that a macro-batch needs to go through is DPP (noted as pipeline parallel degree);
the number of batch partition that needs to run model gradient synchronization is DDP (noted as data
parallel degree); we have DDP × DPP = N , i.e., the total number of devices.

• cPP (resp. cDP) represent the number of bytes of activations for a macro-batch (resp. parameters/gradi-

ents for a stage) communicated in pipeline parallelism (resp. data parallelism).

• We denote a training tasklet as ti,j, where i ∈ {1, ..., DDP} and j ∈ {1, ..., DPP}, each of which corre-

sponds to one speciﬁc macro-batch i and pipeline stage j.

• An assignment strategy σ ∈ DDDP×DPP assigns, for each tasklet ti,j, a device σi,j ∈ D, which means that
device σi,j runs the training tasklet ti,j. An valid assignment needs to be unique, i.e., ∀(i, j) (cid:54)= (i(cid:48), j(cid:48)):
σi,j (cid:54)= σi(cid:48),j(cid:48). We use Σ to denote the set of all valid assignments.

• An optimal assignment strategy is an assignment σ that minimizes the communication cost

σ∗ = arg min
σ∈Σ

COMM-COST (σ)

Challenges and Goals. Our goal is to ﬁnd the optimal assignment strategy, which involves two chal-
lenges: (1) How to effectively model the communication cost COMM-COST(σ) for a given assignment σ
under a heterogeneous network environment? and (2) How to effectively search for the optimal assignment
strategy that minimizes such a cost? We tackle these two questions in Section 3.

4

Transformer BlockTransformer BlockTransformer BlockTransformer BlockTransformer BlockTransformer BlockForward ActivationBackward GradientGradientMacro-batch 1Macro-batch 2Device 1Device 2Device 3Device 4Figure 3: (a) Communication graph G; and (b, c, d, e) an illustration of the cost model given G.

3 Scheduling in Heterogeneous Environments

Scheduling in the heterogeneous setting is a challenging task, as the size of the search space increases
dramatically compared to that of the homogeneous case. In the homogeneous data-center case, the network
delay can be usually ignored (e.g., A = 0) and the bandwidth B are assumed to be formed by just a
few constants — e.g., the communication bandwidths between different machines on the same rack are
assumed to be same [23, 24, 22, 22, 26]. This signiﬁcantly constrains the search space — one can ignore
the inﬂuence of communication given uniform connections [23, 24, 22], or organize the device with a
hierarchical structure [22, 26], making the scheduling problem solvable in polynomial time in terms of the
number of machines.

In contrast, in the fully heterogeneous scenario the communication matrix A and B consists of distinct
values, which can make the search space grows exponentially. In this section, we describe our scheduler that
searches for an optimal strategy in the complex search space.

3.1 Overview of the scheduler

We carefully design a bi-level scheduling algorithm based on extended balanced graph partition problem
(see Figure 3), and solve this problem by an evolutionary algorithm with a carefully designed local search
strategy. Given an assignment strategy σ = {σi,j} for all tasklets {ti,j}, we ﬁrst model its communication cost.
During the training of FMs, the communication costs come from two different sources: (1) Data parallel:
All devices that are assigned with the tasklets dealing with the same stage j (handling the same layers)
of different macro-batches need to communicate within themselves to exchange gradients of these layers.
For layer j, we call these devices its data parallel group: Cj = {σi,j | ∀i ∈ [DDP]}. We can implement the
communication using different primitives, e.g., AllReduce [30], ScatterGatter [31], or other decentralized
average protocols [16]. (2) Pipeline parallel: All devices that are assigned with the tasklets dealing with
the same macro-batch i of different stages need to form a pipeline, communicating within themselves to
exchange activations and backward gradients. For macro-batch i, these devices are Pi = {σi,j | ∀j ∈ [DPP]}.
Because these devices need to form a linear pipeline, any permutation over Pi corresponds one strategy of
how these machines can conduct pipeline parallelism within them.

Scheduling Problem. The goal of our scheduler is to minimize both costs. One design decision that we
made is to decompose this complex optimization problem into two levels. At the ﬁrst level, we consider the
best way of forming Cj’s, incurring data parallel communication costs within them. At the second level, we
consider the cost of pipeline parallelism given an layout from the ﬁrst level:

min
C1...CDPP
s.t.

COMM-COST (C1...CDPP ) := DATAP-COST(C1...CDPP ) + PIPELINEP-COST(C1...CDPP )

|C1| = .... = |CDPP | = DDP, ∀j, j(cid:48) : Cj ∩ Cj(cid:48) = ∅, C1 ∪ ... ∪ CDPP = D

(1)

where computing PIPELINEP-COST(C1...CDPP ) involves ﬁnding the optimal pipeline structure.

5

Deviced1…(a) Communication Topology Graph Gover Ndevices(b) Each partitionCideals with one stage,runningdata parallel within each partition(c) Coarsened graph  𝑮"decoding the cost of pipeline parallel(d) perfect matching corresponds to how devices in Cianddevices in Cjcommunicate in a pipeline.(e) Open-loop-traveling-salesman provides a pipeline structureDeviced2(1)(2)C2C1C3In Section 3.2 and Section 3.3, we provide details on COMM-COST (C1...CDPP ). Notice that this modiﬁed
objective makes our problem different from the textbook graph partition problem; thus, we need a carefully
designed evolutionary algorithm for ﬁnding such a solution introduced in Section 3.4.

3.2 Modelling data parallel communication cost

Given the communication graph G forming data parallel groups C1...CDPP corresponds to a partition of G—
In Figure 3(b), different colors correspond to devices in the same Cj. The data parallel cost within Cj only
relies on all communication channels (edges in the communication graph) connecting devices in Cj. If we
assume a colocated sharded parameter server [31] implementation for communicating within each Cj, and
recall that cDP represents the total amount of data (in bytes) that needs to be exchanged during gradient
aggregation — each device in Cj needs to manage cDP/DDP bytes of parameter shard. Once the gradient
is ready, each device needs to send each of its local shards to the corresponding device; next, each device
can aggregate the gradients it receives from all other devices in Cj; and ﬁnally, each device will send the
aggregated gradient shard to all other devices. Therefore, we can model the data parallel cost for Cj as
follows:

DATAP-COST(Cj) = max
d∈Cj

(cid:88)

d(cid:48)∈Cj −{d}

(cid:18)

2 ·

αd,d(cid:48) +

(cid:19)

cdp
DDPβd,d(cid:48)

.

(2)

Here, the total cost is bounded by the slowest device (maxd∈Cj ), which needs to exchange data with all other
machines ((cid:80)
d(cid:48)∈Cj −{d}). Because the communication of these different data parallel groups C1...CDPP can
be conducted in parallel and we are only bounded by the slowest data parallel group. This allows us to
model the total communication cost for data parallelism as:

DATAP-COST(C1...CDPP ) = max
j∈[DPP]

DATAP-COST(Cj)

3.3 Modeling pipeline parallel communication cost

Given C1...CDPP , to model the communication cost of pipeline parallelism, we need to consider two factors:
(1) each permutation π of {C1...CDPP } corresponds to a speciﬁc pipeline strategy — devices in Cπj and
devices in Cπj+1 communicates to exchange activations (during forward pass) and gradients on activations
(during backward pass); and (2) devices in Cπj and devices in Cπj+1 need to be “matched” — only devices
that are dealing with the same macro-batch needs to communicate. This makes modeling the cost of pipeline
parallel communication more complex.

To model the cost of pipeline parallel communication, we ﬁrst consider the best possible way that devices
in Cj and Cj(cid:48) can be matched. We do this by creating a coarsened communication graph (Figure 3(c)). A
coarsened communication graph (cid:98)GC1...CDPP
is a fully connected graph, and each partition Cj in the original
communication graph G corresponds to a node in (cid:98)GC1...CDPP

.

In the coarsened graph (cid:98)G, the weight on an edge between Cj and Cj(cid:48) corresponds to the following — if
Cj and Cj(cid:48) need to communicate in a pipeline, what is the communicate cost of the optimal matching strategy
between devices in Cj and devices in Cj(cid:48)? Recall that cPP represents the amount of data between two devices
for pipeline parallel communication, we can model this cost by

(cid:18)

min
M

max
(d,d(cid:48))∈M

2

αd,d(cid:48) +

(cid:19)

cPP
βd,d(cid:48)

(3)

where M is a perfect matching between Cj and Cj(cid:48) — (d, d(cid:48)) ∈ M means that device d ∈ Cj will communi-
cate with device d(cid:48) ∈ Cj(cid:48) (i.e., they deal with the same macro-batch). Computing this value is similar to the
classical minimal sum weight perfect matching problem (MinSumWPM) in bipartite graphs [32], with the
only difference being that we compute the max instead of the sum. As we will show in the supplementary
material, similar to MinSumWPM, Eq 3 can also be solved in PTIME.

6

The coarsened communication graph captures the pipeline parallel communication cost between two
groups of devices, assuming they become neighbors in the pipeline. Given this, we need to ﬁnd an optimal
permutation of C1...CDPP , corresponds to the structure of the pipeline. This becomes the open-loop traveling
salesman problem [27] over this condensed graph (Figure 3(e)). Formally, we have the following deﬁnition
of the pipeline parallel cost:

PIPELINEP-COST (C1...CDPP ) = OPENLOOPTSP

(cid:16)

(cid:98)GC1...CDPP

(cid:17)

(4)

where (cid:98)GC1...CDPP

is the coarsened graph deﬁned above.

3.4 Searching via hybrid generic algorithm

The scheduling problem solves the optimization problem in Eq 1, which corresponds to a balanced graph par-
tition problem with a complex objective corresponding to the communication cost. Balanced graph partition
problem is a challenging NP-hard problem [33]. Over the years, researchers have been tackling this problem
via different ways [34, 35, 36]. We follow the line of research that uses hybrid genetic algorithm [37, 28]
since it provides us the ﬂexibility in dealing with complex objective.

Hybrid Genetic Algorithm. A hybrid genetic algorithm for balanced graph partition usually follows a
structure as as follows. The input is a set of candidate balanced graph partitions which serves as the initial
population. The algorithm generates the next generation as follows. It ﬁrst generates a new “offspring” o
given two randomly selected “parents” p1 and p2. One popular way is to randomly swap some nodes between
these two parents (we follow [28]). Given this offspring o, we then conduct local search starting at o to ﬁnd
a new balanced partitioning strategy o∗ that leads to better cost. We then add o∗ to the population and
remove the worst partition candidate in the population if o∗ has a better cost. As suggested by [37], the
combination of heuristic-based local search algorithms and generic algorithm can accelerate convergence by
striking the balance between local and global optimum.

Existing Local Search Strategy. The key in designing this algorithm is to come up with a good local
search strategy. For traditional graph partitioning task, one popular choice is to use the Kernighan-Lin
Algorithm [38]. Which, at each iteration, tries to ﬁnd a pair of nodes: d in partition Cj and d(cid:48) in partition
Cj(cid:48), to swap. To ﬁnd such a pair to swap, it uses the following “gain” function:

GAINKL((d, Cj) ↔ (d(cid:48), Cj(cid:48))) =

(cid:88)

d(cid:48)(cid:48)∈Cj(cid:48)

wd,d(cid:48)(cid:48) −

(cid:88)

wd,d(cid:48)(cid:48) +

d(cid:48)(cid:48)∈Cj −{d}

(cid:88)

d(cid:48)(cid:48)∈Cj

wd(cid:48),d(cid:48)(cid:48) −

(cid:88)

wd(cid:48),d(cid:48)(cid:48) − 2wd,d(cid:48)

d(cid:48)(cid:48)∈Cj(cid:48) −{d(cid:48)}

where wi,j corresponds to the weight between node i and j in the graph. However, directly applying this
local search strategy, as we will also show in the experiment (Section 5) does not work well. Greedily
following GAINKL does not decrease the communication cost of foundation model training. Therefore, we
have to design a new local search strategy tailored to our cost model.

Improving Local Search Strategy. Our local search strategy is inspired by two observations:

1. Removing the device d1 with a fast connection (say with d2) within partition Cj will not tend to change

the data parallel cost within Cj, since it is only bounded by the slowest connections.

2. Once d1 is moved to Cj(cid:48), highly likely the pipeline parallel matching between Cj and Cj(cid:48) will consist of

the link d1 ↔ d2, since it is a fast connection.
Therefore, in our local search strategy we only consider the fastest connection within Cj: d1 ↔ d2 and
2, d2 ↔ d(cid:48)
1,

2 and generate four swap candidates: d1 ↔ d(cid:48)

1, d1 ↔ d(cid:48)

the fastest connection within Cj(cid:48): d(cid:48)
d2 ↔ d(cid:48)

2. We use the following gain function (take d1 ↔ d(cid:48)

1 ↔ d(cid:48)

1 as an example):

GAIN((d, Cj) ↔ (d(cid:48), Cj(cid:48))) =

1
|Cj(cid:48)|

(cid:88)

d(cid:48)(cid:48)∈Cj(cid:48)

wd1,d(cid:48)(cid:48) − wd1,d2 +

1
|Cj|

(cid:88)

d(cid:48)(cid:48)∈Cj

7

wd(cid:48)

1,d(cid:48)(cid:48) − wd(cid:48)

1,d(cid:48)
2

(cid:80)

1
|Cj(cid:48) |

d(cid:48)(cid:48)∈Cj(cid:48) wd1,d(cid:48)(cid:48) measures the expected pipeline parallel cost of connecting d1 with other devices
where
in Cj(cid:48) before the swap, and wd1,d2 is the cost of connecting d1 with other devices in Cj(cid:48) after the swap,
assuming this fast link d1 ↔ d2 will now be used for pipeline parallelism.

Just like how Kernighan-Lin Algorithm [38] can be extended to a circular version [28] to swap multiple
nodes beyond a pair, we can also extend our method into a circular one, following procedure as circular KL
with our new gain function.

3.5 Other System Optimizations

We also have some system optimizations to further improve the performance. The most important optimiza-
tion involves pipelining of communications and computations. We divide each stage in the pipeline into
three slots: a receiving slot, a computation slot, and a sending slot. The receiving slot of stage j needs to
build connections to receive activations from the stage j − 1 in forward propagation and to receive gradients
of activations from stage j + 1. The computation slot handles the computation in forward and backward
propagation. Symmetric to the receiving slot, the sending slot of stage j needs to build connections to send
activations to stage j + 1 in the forward propagation and send gradients of activations to stage j − 1 in the
backward propagations. These three slots are assigned to three CUDA streams so that they will be further
pipelined efﬁciently; as a result, communication will overlap with computation. In the decentralized scenario
(communication bound), computation can be fully hidden inside the communication time.

4 Evaluation

We demonstrate that our system can speed up foundation model training in decentralized setting. Specif-
ically, (1) We show that our system is 4.8× faster, in end-to-end runtime, than the state-of-the-art system
Megatron training GPT3-XL in world-wide geo-distributed setting. Surprisingly, it is only 1.7 − 2.3× slower
than Megatron in data centers. This indicates that we can bridge the gap between decentralized and data
center training (up to 100× slower networks) through scheduling and system optimization; (2) We demon-
strate the necessity of our scheduler through an ablation study. We show that with the scheduler, our system
is 2.7× faster in world-wide geo-distributed setting.

Experimental Setup To simulate the decentralized setting, we use 8 different AWS regions (Oregon, Vir-
ginia, Ohio, Tokyo, Seoul, London, Frankfurt, and Ireland) and measure the latency and bandwidth between
these regions (we consider the bandwidth that we can realistically obtain using NCCL and UDP hole punching
between these regions). Given these measurements, we use 64 Tesla V100 GPUs and control their pairwise
communication latency and bandwidth for ﬁve different cases:

Case 1. Data center on demand. This is a standard setting that a user can obtain to train foundation mod-
els. we use 8 AWS p3.16xlarge nodes (each with 8 V100 GPUs); the intra-node connection has a bandwidth
of 100 Gbps, and the inter-node connection has a bandwidth of 25 Gbps. We do not manually control latency
and bandwidth in this case.

Case 2. Data center spot instances. Spot GPUs are cheaper in a data center, but can be located on different
types of machine. In this case, we rent 4 AWS p3.8xlarge nodes (each with 4 V100) and 32 p3.2xlarge
nodes (each with 1 V100); the intra- p3.8xlarge node connection has a bandwidth of 100 Gbps, and the
inter-node connection has a bandwidth of 10 Gbps. We do not manually control latency and bandwidth in
this case.

Case 3. Multiple Data Centers. We consider two organizations, one in Ohio and another in Virginia, each
organization contributes 32 V100 GPUs; within each organization, the bandwidth is 10 Gbps, and connec-
tions cross different campuses have a delay of 10 ms and bandwidth of 1.12 Gbps.

Case 4. Regional geo-distributed. We consider individual GPUs cross four different regions in US (Califor-
nia, Ohio, Oregon, and Virginia) ; within each region, the delay is 5 ms and bandwidth is 2 Gbps; cross
different regions, the delay is 10∼70ms and the bandwidth is 1.0∼1.3 Gbps.

8

Figure 4: End to end compassion of our system with Megatron in 5 different scenarios. Column (a) and
(b) visualize the delay and bandwidth of 5 scenario respectively; Column (c) illustrate the comparison of
Megatron and our system w and w/o scheduler.

9

Case 1Data Centeron DemandCase 2Data CenterSpot InstancesCase 3Multiple Data CentersCase 5World-wide DistributedCase 4Regional Distributed(a) Network Latency (ms)(c) PFLOPS(b) Network Bandwidth (Gbps)M0M1M2M3M4M5M6M74GPU Machines1GPU MachinesOhio MachinesVirginia MachinesVirginiaOregonCAOhioOregonVirginiaOhioTokyoSeoulLondonFrankfurtIrelandM0M1M2M3M4M5M6M74GPU Machines1GPU MachinesOhio MachinesVirginia MachinesVirginiaOregonCAOhioOregonOhioTokyoSeoulLondonFrankfurtIrelandVirginia3.8xHigher FLOPS (Faster)More Layers / Larger BatchOursOursOursOurs2.3xOurs1.6x1.4x4.8x2.5x1.8x1.7x1.4x1.2xFigure 5: Comparison of Different Local Search Strategies.

Case 5. World-wide geo-distributed. We consider individual GPUs cross eight different regions world-wide
(Oregon, Virginia, Ohio, Tokyo, Seoul, London, Frankfurt, and Ireland); within each region, the delay is 5 ms
and bandwidth is 2 Gbps; cross different regions, the delay is 10∼250ms and the bandwidth is 0.3∼1.3Gbps.

Metrics and Model Architecture. Since we do not introduce any optimizations that might change the
computation or convergence, we can compare all methods by its throughput, we can compare all systems
by the total number of ﬂoating point operations per second (PFLOPS), which is inverse proportional to the
runtime of each iteration (which we show in Appendix). We use the standard GPT3-XL architecture [2],
while also benchmarked different number of layers {24, 32, 40}, and batch sizes {1024, 2048, 4096}. Tuning
of Megatron. When comparing with Megatron, we did a careful grid search of different parallelism settings
in Megatron and report the optimal results in each case—in Case 1, the optimal setting includes tensor model
parallelism; all other cares the optimal settings are based on pipeline and data parallelism. We discuss more
details in Appendix.

4.1 Results

End-to-end Comparison. Figure 4(c) shows the end-to-end comparison between Megatron and our system
in terms of averaged PFLOPS achieved across different settings and different batch sizes and number of
layers. From the world-wide geo-distributed cases, we observe that our system achieves an 4.8× speedup,
as compared with Megatron. While in all other cases, our system can be 1.2 − 2.5× faster. If we compare
our system in Case 5 (world-wide geo-distributed) and Megatron in Case 1 (data center on demand), it is
exciting to see that the performance slowdown caused by decentralization is only 1.7 − 3.5×! This illustrates
the great potential of decentralized training for foundation models. Additionally, Figure 4(c) illustrates
another interesting behavior pattern. As increasing the batch size does not increases the communication cost
of data parallelism and increasing # layers per device does not increases the communication cost of pipeline
parallelism, with a larger batch size and a deeper model, the gap between centralized Megatron and our
decentralized system is even smaller.

Effectiveness of Scheduler. To evaluate the effectiveness of the scheduler, we disable it and use a random
assignment in all cases and the results are also illustrated in Figure 4(c). We see that with our scheduler

10

# Iterations(a) Case 1Estimated Cost (s)# Iterations(b) Case 2# Iterations(c) Case 3# Iterations(d) Case 4# Iterations(e) Case 5Estimated Cost (s)provides up to 2.7× speeds up. To evaluate our local search strategy, we also compare our scheduler with
a scheduler that uses the standard Kernighan-Lin algorithm for local search, illustrated in Figure 5 (a -
e). We see that, while both outperform random, our carefully designed local search strategy signiﬁcantly
outperforms Kernighan-Lin.

5 Related Work

Foundation models. Foundation models[1] refer to models that are trained on large-scale data and can be
adapted (e.g., ﬁne-tuned) to a wide range of downstream tasks. Current examples include BERT [39],
GPT-3 [2], and CLIP[40]. Foundation models are usually trained in a data center, where the connec-
tion between GPUs is fast and homogeneous. ML infrastructures such as Megatron [4], ZeRO [10, 11],
Fairscale [6], Ray [41, 42], Alpa [8], etc. have been proposed to distribute the training of these foundation
models in a data center. Speciﬁc optimizations have also been considered to accelerate such training, for
example, TeraPipe [9] includes an additional token level parallelism; Wavelet [43] introduces a Tick-Tock
scheduling schema to improve the hardware efﬁciency. The most widely used toolkits are Megatron [4] and
ZeRO [10], where Megatron uses AllReduce to synchronize activations in tensor models parallelism; ZeRO
adopts ScatterGather to dispatch sharded parameters for layer-wise data parallelism. However, such col-
lective communication paradigms would cause serious performance problems with slow and heterogeneous
connections (see Appendix for detailed discussions).

Decentralized optimization. Decentralized training is ﬁrst proposed within the scope of data parallelism,
where each worker only synchronizes gradients with its neighbors (instead of all workers) to remove the
latency bottleneck [17, 44, 16, 45, 46, 47]. Recently, [48] has also modiﬁed the implementation of data
parallelism to support training in an open collaborative environment. Varuna [49] is released by Microsoft to
support the training of GPT models in spot instances from a cloud service provider, which has the potential to
be extended to the open collective scenario, but there is limited consideration with respect to the challenges
of heterogeneous connections.

Volunteer computing. Distributing computationally intensive tasks over an open collaborative environment
has been advocated for a few decades since the development of BOINC [50]; for example, the folding@home
project [13] has been running simulations about protein dynamics on volunteers’ personal computers for
more than 20 years. Recently, the learning@home project[19] starts to consider training of mixture-of-
expert transformers in such a volunteer computing setting.

6 Conclusion

In this paper, we probe the opportunity to train foundation models via a decentralized training regime with
devices connected over a heterogeneous network. We propose an effective scheduling algorithm to assign
tasklets from the foundation model pre-train computation. Empirical studies suggest that, in the worldwide
geo-distributed scenario, our proposed scheduling algorithm enables a 4.8× speed-up compared to prior
state-of-the-art training systems. We believe that the decentralization and democratization of the training of
foundation models can shift the balance of power positively, but also necessitate new governance structures
to help ensure the responsible development and deployment of foundation models.

References

[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258, 2021.

11

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learn-
ers. Advances in neural information processing systems, 33:1877–1901, 2020.

[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language
modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[4] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv
preprint arXiv:1909.08053, 2019.

[5] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations
enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506, 2020.

[6] Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth Goyal, Myle Ott, Ben-
jamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam Sheiffer, et al. Fairscale: A general purpose
modular pytorch library for high performance and large scale training, 2021.

[7] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim
Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable paralleliza-
tion for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.

[8] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang,
Yuanzhong Xu, Danyang Zhuo, Joseph E Gonzalez, et al. Alpa: Automating inter-and intra-operator
parallelism for distributed deep learning. arXiv preprint arXiv:2201.12023, 2022.

[9] Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica.
Terapipe: Token-level pipeline parallelism for training large-scale language models. In International
Conference on Machine Learning, pages 6543–6552. PMLR, 2021.

[10] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations
In SC20: International Conference for High Performance

toward training trillion parameter models.
Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020.

[11] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia
Zhang, Dong Li, and Yuxiong He. {ZeRO-Ofﬂoad}: Democratizing {Billion-Scale} model training. In
2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 551–564, 2021.

[12] Q4’21 sees a nominal rise in gpu and pc shipments quarter-to-quarter. https://www.jonpeddie.com/

press-releases/q421-sees-a-nominal-rise-in-gpu-and-pc-shipments-quarter-to-quarter.

[13] Michael Shirts and Vijay S Pande. Screen savers of the world unite! Science, 290(5498):1903–1904,

2000.

[14] OS Statistics. https://stats.foldingathome.org/os, 2022. [Online; accessed 15-May-2022].

[15] Gpu

economics

cost

analysis.

https://venturebeat.com/2018/02/25/

the-real-cost-of-mining-ethereum/.

[16] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algo-
rithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient
descent. Advances in Neural Information Processing Systems, 30, 2017.

[17] Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and
gossip algorithms with compressed communication. In International Conference on Machine Learning,
pages 3478–3487. PMLR, 2019.

12

[18] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A uniﬁed theory
of decentralized sgd with changing topology and local updates. In International Conference on Machine
Learning, pages 5381–5393. PMLR, 2020.

[19] Max Ryabinin and Anton Gusev. Towards crowdsourced training of large neural networks using decen-

tralized mixture-of-experts. Advances in Neural Information Processing Systems, 33:3659–3672, 2020.

[20] Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton Sinitsin, Dmitry Popov,
Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del Moral, et al. Distributed
deep learning in open collaborations. Advances in Neural Information Processing Systems, 34:7879–
7897, 2021.

[21] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou,
Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy Meier-Hellstern, Toju
Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efﬁcient
scaling of language models with mixture-of-experts. CoRR, abs/2112.06905, 2021.

[22] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R
Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: generalized pipeline parallelism for dnn
training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1–15, 2019.

[23] Jakub M Tarnawski, Amar Phanishayee, Nikhil Devanur, Divya Mahajan, and Fanny Nina Paravecino.
Efﬁcient algorithms for device placement of dnn graph operators. Advances in Neural Information
Processing Systems, 33:15451–15463, 2020.

[24] Jakub M Tarnawski, Deepak Narayanan, and Amar Phanishayee. Piper: Multidimensional planner for

dnn parallelization. Advances in Neural Information Processing Systems, 34, 2021.

[25] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long,
Jun Yang, Lixue Xia, et al. Dapple: A pipelined data parallel approach for training large models. In
Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,
pages 431–445, 2021.

[26] Jay H Park, Gyeongchan Yun, M Yi Chang, Nguyen T Nguyen, Seungmin Lee, Jaesik Choi, Sam H Noh,
and Young-ri Choi. {HetPipe}: Enabling large {DNN} training on (whimpy) heterogeneous {GPU}
In 2020 USENIX
clusters through integration of pipelined model parallelism and data parallelism.
Annual Technical Conference (USENIX ATC 20), pages 307–321, 2020.

[27] Christos H Papadimitriou. The euclidean travelling salesman problem is np-complete. Theoretical

computer science, 4(3):237–244, 1977.

[28] So-Jin Kang and Byung-Ro Moon. A hybrid genetic algorithm for multiway graph partitioning.

In
Proceedings of the 2nd Annual Conference on Genetic and Evolutionary Computation, pages 159–166.
Citeseer, 2000.

[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
30, 2017.

[30] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in tensorﬂow.

arXiv preprint arXiv:1802.05799, 2018.

[31] Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, and Chuanxiong Guo. A uniﬁed architecture
In 14th USENIX

for accelerating distributed {DNN} training in heterogeneous {GPU/CPU} clusters.
Symposium on Operating Systems Design and Implementation (OSDI 20), pages 463–479, 2020.

13

[32] Michel X Goemans. Lecture notes on bipartite matching. Massachusetts Institute of Technology, 2009.

[33] Michael R Garey and David S Johnson. Computers and intractability, volume 174. freeman San Fran-

cisco, 1979.

[34] Konstantin Andreev and Harald Racke. Balanced graph partitioning. Theory of Computing Systems,

39(6):929–939, 2006.

[35] Peter Sanders and Christian Schulz. Think locally, act globally: Highly balanced graph partitioning. In

International Symposium on Experimental Algorithms, pages 164–175. Springer, 2013.

[36] Aydın Buluc¸, Henning Meyerhenke, Ilya Safro, Peter Sanders, and Christian Schulz. Recent advances

in graph partitioning. Algorithm engineering, pages 117–158, 2016.

[37] Tarek A El-Mihoub, Adrian A Hopgood, Lars Nolle, and Alan Battersby. Hybrid genetic algorithms: A

review. Eng. Lett., 13(2):124–137, 2006.

[38] Thang Nguyen Bui and Byung Ro Moon. Genetic algorithm and graph partitioning. IEEE Transactions

on computers, 45(7):841–855, 1996.

[39] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-

tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision.
In International Conference on Machine Learning, pages 8748–8763.
PMLR, 2021.

[41] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih
Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed framework for emerg-
ing {AI} applications.
In 13th USENIX Symposium on Operating Systems Design and Implementation
(OSDI 18), pages 561–577, 2018.

[42] Hao Zhang, Zhuohan Li, Lianmin Zheng, and Ion Stoica. Simple and automatic distributed machine
In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data

learning on ray.
Mining, pages 4094–4095, 2021.

[43] Guanhua Wang, Kehan Wang, Kenan Jiang, Xiangjun Li, and Ion Stoica. Wavelet: Efﬁcient dnn training

with tick-tock scheduling. Proceedings of Machine Learning and Systems, 3:696–710, 2021.

[44] Youjie Li, Mingchao Yu, Songze Li, Salman Avestimehr, Nam Sung Kim, and Alexander Schwing. Pipe-
sgd: a decentralized pipelined sgd framework for distributed deep net training. In Proceedings of the
32nd International Conference on Neural Information Processing Systems, pages 8056–8067, 2018.

[45] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient

descent. In International Conference on Machine Learning, pages 3043–3052. PMLR, 2018.

[46] Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, and Ji Liu. Communication compression for decen-
tralized training. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pages 7663–7673, 2018.

[47] Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training over decentral-

ized data. In International Conference on Machine Learning, pages 4848–4856. PMLR, 2018.

[48] G Tse Edwin, Dana M Klug, and Matthew H Todd. Open science approaches to covid-19. F1000Research,

9, 2020.

14

[49] Sanjith Athlur, Nitika Saran, Muthian Sivathanu, Ramachandran Ramjee, and Nipun Kwatra. Varuna:
scalable, low-cost training of massive deep learning models. In Proceedings of the Seventeenth European
Conference on Computer Systems, pages 472–487, 2022.

[50] David P Anderson. Boinc: A system for public-resource computing and storage.

In Fifth IEEE/ACM

international workshop on grid computing, pages 4–10. IEEE, 2004.

[51] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and
megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint
arXiv:2201.11990, 2022.

[52] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee,
Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efﬁcient training of giant neural networks using
pipeline parallelism. Advances in neural information processing systems, 32, 2019.

[53] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff
Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: experiences on accelerating data
parallel training. Proceedings of the VLDB Endowment, 13(12):3005–3018, 2020.

[54] Ernie Chan, Marcel Heimlich, Avi Purkayastha, and Robert Van De Geijn. Collective communication:
theory, practice, and experience. Concurrency and Computation: Practice and Experience, 19(13):1749–
1783, 2007.

[55] Nccl. https://developer.nvidia.com/nccl.

[56] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efﬁcient
In International Conference on Machine Learning, pages 7937–7947.

pipeline-parallel dnn training.
PMLR, 2021.

A Anatomy of the Current ML Systems for Foundation Model Training

Training foundation models [1] is a challenging task due to the enormous scale of such models — even the
most powerful GPU cannot hold a complete copy of parameters for such models [51]. Thus, one cannot train
such a model without distribution or using vanilla data parallelism.

Two popular approaches have been proposed to distribute the training of such foundation models in a

data center:
• Megatron [4] distributes training by combining its proposed tensor model parallelism with pipeline par-
allelism [52, 22] and data parallelism [53]. The tensor model parallelism partitions individual layers
across a group of workers and must run one AllReduce for the output activations of each layer in forward
propagation and one AllReduce for the corresponding gradients in backward propagation for each layer.
• ZeRO [10] can be viewed as an effective optimization for data parallelism. The most effective mode
is called ZeRO stage-3 from the Deepspeed implementation [5], and the equivalent implementation is
In this mode, the parameter is
known as Fully Sharded Data Parallelism (FSDP) from Fairscale [6]).
sharded among all workers — in forward propagation, each worker conducts one AllGather to collect
the parameters demanded for the current layer and discard the parameter after the forward computation;
in backward propagation, each worker uses one AllGather to collect the parameter again and run one
ReduceScatter to synchronize the gradients of this layer after the backward computation.
Both Megatron and ZeRO take a heavy usage of collective communication paradigms [54], which leads

to two fundamental problems when it is deployed with heterogeneous and slow connections:

15

• Demanding high bandwidth connections. Both Megatron and ZeRO require high bandwidth connections
for collective communications, since the compute cores are idled during communication slots. As long as
communication takes an increasing share (due to lower bandwidth) of the execution time, the hardware
efﬁciency drops dramatically. In fact, tensor model parallelism is recommended only within a single DGX
server equipped with high-bandwidth NVLinks [51].

• Sensitive to straggler. The design and implementation of state-of-the-art collective communication libraries,
e.g., NCCL [55], assume highly homogeneous connections within a data center, thus there is not sufﬁcient
robustness to handle the straggler among workers due to the heterogeneity of the open collective run-
time. Furthermore, the layer-wise usage of collective communications in both Megatron and ZeRO has
intensiﬁed this problem.
To bridge the performance gap between the data center and the decentralized open environment, we

need to rethink the communication paradigms in different parallel strategies.
• Pipeline parallelism is communication efﬁcient. Pipeline parallelism [52, 22, 56] partitions the model into
multiple stages and a batch into multiple mini-batches, where once a worker ﬁnished the forward com-
putation of a micro-batch, this worker will send the activations to the worker running the next stages;
on the other hand, a worker needs to send the gradients of the activation back to the last stage in the
backward propagation. Notice that pipeline parallelism utilizes point-to-point communications instead of
collective paradigms. As long as one can put an increasing amount of computation inside a stage, the
ratio of communication cost will also drop, leading to more efﬁcient utilization of compute cores.1 On
the other hand, pipeline parallelism has its own limitation — one can only partition a model to a limited
number of stages, which cannot scale out to lots of GPUs. We need to combine pipeline parallelism with
data parallelism to scale out the training.

• Scheduling is essential. The point-to-point communication pattern in pipeline parallelism provides good
opportunities to assign the training procedure on the decentralized environment that utilizes fast links
and avoids slow links by a carefully designed scheduler, as presented in Section 3.

B Additional Details of Experimental Evaluation

We enumerate some additional details about our experiments.

B.1 Multiple Execution of the Benchmark

We repeated all the benchmarks of 5 different scenarios listed in Section 4 three times. For our system with
scheduler, since the scheduled layout is the same, we simply issued three independent executions in each
scenarios; For our system without scheduler, we used three different random seeds (2022, 2023, 2024) to
generate three layouts, and issued one execution for each layout in each scenario. The number in Figure
4 is based on an average of three different runs for each scenario — to avoid visual confusion, we did not
plot the error bar within this line plot. We also repeated the scheduling algorithms three times with random
seeds (0, 1, 2) to generate scheduled layouts and reported the average estimated cost (seconds) in Figure 5.
In Figure 6, we plot runtime of each iteration as a bar chart with error bars. Notice that the variance of all
executions in each setting is within 5%.

B.2 Tuning of Megatron

We carefully tuned Megatron to perform a fair comparison with our system. As we mentioned in Section A.
Megatron has three free degrees of parallel strategies: tensor model parallelism, pipeline parallelism, and data

1Notice this is not always true since the device memory is limited. However, one can ofﬂoad [11] (e.g., activations and parameters)
to host memory to perform training on larger models with limited GPU device memory. Furthermore, the ofﬂoading through PCI-e is
much faster compared to the decentralized connections, although it is slower than NVLink between GPUs in a data center.

16

parallelism, we note the degrees of these parallel strategies as DTP, DPP, and DDP respectively. We conduct a
complete grid search of the combinations of these hyper-parameters in the space of:

{(DTP, DPP, DDP) |DTP, DPP, DDP ∈ {1, 2, 4, 8} and DTP × DPP × DDP = 64} .

And we reported the optimal setting as the results for Megatron. Interestingly, only in Case 1 (data center
on demand), the optimal setting includes tensor model parallelism (i.e., DTP (cid:54)= 1), where DTP = 2, DPP =
4, DDP = 8; in all other scenarios, the optimal setting is DTP = 1, DPP = 8, DDP = 8. This illustrates that tensor
model parallelism is not suitable in slow and heterogeneous settings, consistent with our analysis in Section
A. Since Megatron does not include a similar scheduler as its own components, we use the same random
layouts as what we test for our system without scheduler.

B.3 Additional Experimental Results

In Figure 6, we plot runtime of each iteration for each scenario, this is a supplement to Figure 4.

17

Figure 6: End to end compassion of our system with Megatron in terms of runtime of each iteration in
5 different scenarios. Column (a) and (b) visualize the delay and bandwidth of 5 scenario respectively;
Column (c) illustrate the comparison of Megatron and our system w and w/o scheduler.

18

Case 1Data Centeron DemandCase 2Data CenterSpot InstancesCase 3Multiple Data CentersCase 5World-wide DistributedCase 4Regional Distributed(a) Network Latency (ms)(c) RuntimeperIteration(s)(b) Network Bandwidth (Gbps)M0M1M2M3M4M5M6M74GPU Machines1GPU MachinesOhio MachinesVirginia MachinesVirginiaOregonCAOhioOregonVirginiaOhioTokyoSeoulLondonFrankfurtIrelandM0M1M2M3M4M5M6M74GPU Machines1GPU MachinesOhio MachinesVirginia MachinesVirginiaOregonCAOhioOregonOhioTokyoSeoulLondonFrankfurtIrelandVirginiaLongerruntime(Slower)More Layers / Larger Batch