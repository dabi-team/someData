2
2
0
2

g
u
A
6
2

]

G
L
.
s
c
[

2
v
6
2
3
3
0
.
6
0
2
2
:
v
i
X
r
a

Compilation and Optimizations for Eﬃcient
Machine Learning on Embedded Systems

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Abstract Deep Neural Networks (DNNs) have achieved great success in a variety of
machine learning (ML) applications, delivering high-quality inferencing solutions
in computer vision, natural language processing, and virtual reality, etc. However,
DNN-based ML applications also bring much increased computational and storage
requirements, which are particularly challenging for embedded systems with limited
compute/storage resources, tight power budgets, and small form factors. Challenges
also come from the diverse application-speciﬁc requirements, including real-time re-
sponses, high-throughput performance, and reliable inference accuracy. To address
these challenges, we introduce a series of eﬀective design methodologies, including
eﬃcient ML model designs, customized hardware accelerator designs, and hard-
ware/software co-design strategies to enable eﬃcient ML applications on embedded
systems.

Keywords: Deep Neural Networks, machine learning, embedded systems, eﬃcient
ML model, hardware accelerator, compilation, optimization, hardware/software co-
design

X. Zhang · Y. Li · D. Chen
Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Champaign, IL,
USA, e-mail: xiaofan3@illinois.edu;leeyh@illinois.edu;dchen@illinois.edu

Y. Chen
Advanced Digital Sciences Center, Singapore e-mail: yao.chen@adsc-create.edu.sg

C. Hao
Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA e-mail:
callie.hao@gatech.edu

S. Huang
Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA
e-mail: sitaoh@uci.edu

1

 
 
 
 
 
 
2

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

1 Introduction

The recent development of Deep Neural Networks (DNNs) has made machine learn-
ing based smart solutions more relevant and accessible to the general public. We
have seen that some DNN technologies have been integrated into our daily applica-
tions to provide high-quality inference services, such as image recognition, natural
language processing, self-driving cars, and augmented and virtual reality [1, 2, 3, 4],
which have made our lives more convenient and our work more eﬃcient. A signiﬁ-
cant number of these machine learning applications leverage edge devices and need
to be deployed onto resource-constrained embedded systems, such as cell phones,
cameras, and unmanned aerial vehicles (UAVs). They require not only higher infer-
ence accuracy to achieve intelligent responses but also aggressive inference speed,
throughput, and energy eﬃciency to meet real-life demands.

As DNNs become more complicated, developing and serving the DNN-enabled
applications requires more compute and memory resources, longer latency, and
greater energy consumption. For example, the computation demands for DNN train-
ing have risen by over 300,000 times between AlexNet [5], the champion model of
the 2012 ImageNet competition, and the AlphaGo Zero [6], the AI player proposed
in 2017 for the board game Go with superhuman skills [7]. By checking the image
recognition models, there is a 16 times increase in model complexity from AlexNet
with 85% top-5 accuracy to ResNet-152 [2] with 95% top-5 accuracy.

Such exponentially increasing compute and memory demands have created chal-
lenges and diﬃculties for DNN deployment on hardware, especially when targeting
edge embedded devices with strictly limited compute and memory resources and
tight power budgets [8, 9]. Although cloud computing can alleviate the burden of
edge computing by taking over computationally intensive tasks, it is not always
feasible when dealing with various real-life scenarios. Primary reasons for sticking
to edge embedded devices come from the unique requirements of the edge appli-
cations, which typically require real-time decision-making and reduced reliance on
network communication and accessibility. They typically can not tolerate the extra
latency caused by network data transfer due to the real-time response requirements.
In addition, private information, such as personal and sensitive data, should not be
uploaded to the cloud without permission. It means that the edge devices are required
to deliver not only high inference accuracy from DNNs, but also aggressive inference
speed, throughput, and energy eﬃciency to meet various real-life demands. In sum-
mary, the challenges of deploying machine learning workloads on edge embedded
devices mainly come from three aspects: 1) DNN models are getting complicated
and may fail to run eﬃciently, especially when targeting the low-power edge de-
vices with scarce compute and memory resources; 2) Mapping DNN onto existing
hardware or building domain-speciﬁc hardware is tedious and time-consuming; 3)
Additional challenges come from ineﬃcient optimization strategies that focus only
on hardware or software optimizations alone but lack software/hardware co-design
or cross-system stack design methods that can potentially deliver better overall so-
lutions.

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

3

Despite the aforementioned challenges, there has been continuous progress in
recent studies to explore various optimization strategies for edge machine learning
solutions. In this chapter, we present comprehensive design methodologies to face
and overcome the challenges and enable eﬃcient DNN applications on embedded
systems. These methods include eﬃcient DNN model designs in Sec. 3, accelerator
design and workload mapping technologies in Sec. 4, and cross-stack optimization
strategies in Sec. 5.

2 Background and Related Works

Existing solutions to enable eﬃcient DNN on embedded systems attempt to address
challenges from the DNN model to the entire hardware-software system. These dif-
ferent methods cover diﬀerent development cycles and have diﬀerent characteristics,
as shown in Table 1. In this section, we present existing work on the diﬀerent design
methods in terms of their diﬀerent properties.

Table 1 Design methodologies and their attributes.

Methods

Eﬃcient DNN model design

Eﬃcient accelerator design and DNN mapping

Eﬃcient DNN/accelerator co-design

Attributes
Design methods to create DNN models with less
parameters, less memory demands and less
computational complexity
Solutions to build domain speciﬁc
hardware/software accelerators with optimized
task scheduling
Optimization strategies that integrate both the
hardware design process and DNN algorithm
design process

2.1 Eﬃcient DNN designs

A DNN includes multiple intermediate layers between the input and output lay-
ers, and each intermediate layer consists of artiﬁcial neurons for transforming the
input information (e.g., input feature maps) following the predeﬁned network con-
nection. In general, a DNN contains millions of parameters and requires billions
of operations during inference. To successfully deploy DNNs onto hardware with
desired performance, developers focus on network compression to reduce network
complexities and lower the compute and memory demands. Recent research has
demonstrated the possibility of using quantized data to represent original ﬂoating-
point parameters, such as using 8-bit quantization or even binary and ternary data
representation [10, 11, 12, 13, 14, 15]. These solutions are intended to replace the

4

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

hardware-intensive ﬂoating-point multiplications by logical operations so that DNNs
can be more eﬃcient on hardware platforms.

Another method to compress DNN is network pruning, which aims to reduce
the redundancy of DNN structures [16, 17, 18]. According to the published pruning
strategies, the less essential connections between DNN layers are discarded, and
network retraining is then performed to regain accuracy. Signiﬁcant reductions can
be achieved on the classic DNNs, such as AlexNet [5] and VGG-16 [19]. Since the
major beneﬁt of network compression comes from the fully-connected (FC) layers,
to continuously have eﬀective pruning results for latter DNNs (e.g., GoogleNet [20]
and ResNet [2]) with fewer FC layers, more sophisticated algorithms are required
to achieve eﬀective network pruning, such as using evolutionary algorithms [21],
alternating direction method of multipliers [22], and iterative pruning [23].

As most of the computations happen inside the convolutional (Conv) layers, previ-
ous works also attempt to reduce the computational complexity by using depth-wise
separable Conv layers [24]. The depth-wise separable structure can eﬀectively re-
duce the number of operations and provide more compact DNN designs for resource-
constrained hardware. To further improve the DNN deployment on hardware, layer
fusion is proposed in [25] to minimize data movements between on-chip and oﬀ-chip
memory.

2.2 Eﬃcient accelerator designs and DNN mapping methods

Building domain-speciﬁc hardware accelerators is another popular approach for eﬃ-
cient DNN deployment. These accelerators attempt to take advantage of customized
or specialized hardware and software designs, such as adopting acceleration libraries
on CPUs [26], exploring kernel optimization on GPUs [27], and building customized
accelerators on FPGAs [28, 29, 30] and ASICs [31, 32, 10] to improve the speed
and eﬃciency of DNN inference and training processes. Among these accelerator
designs, FPGA- and ASIC-based designs can be fully customized to implement the
neural network functionality with improved latency, throughput, and energy con-
sumption compared to CPU- and GPU-based designs.

Still, developing customized accelerators present signiﬁcant challenges, such as
the tedious hardware design process, the intricate hardware veriﬁcation problems,
and the time-consuming design space exploration during DNN deployment. To
alleviate these challenges, recent investigations have started focusing on techniques
including high-level synthesis [33, 34, 35] and end-to-end design frameworks for fast
DNN accelerator design and eﬃcient workload deployment [30, 36, 37, 38]. They
support high abstraction inputs, such as Python-based DNN descriptions used by
popular machine learning frameworks (e.g., Caﬀe [39], TensorFlow [40], PyTorch
[41]), so DNNs can be directly imported without manual code conversions and be
parsed and then mapped onto hardware. These frameworks, such as DNNBuilder
[30] and HybridDNN [37] also integrate design space exploration (DSE) engines

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

5

to perform eﬀective and systematical explorations and deliver highly optimized
accelerators to meet the user-speciﬁc requirements.

2.3 Eﬃcient co-design optimization

Recent research also focuses on cross-stack co-design optimizations to enable suc-
cessful DNN deployment on embedded systems [42]. Instead of independently opti-
mizing hardware and software components, researchers proposed algorithm/acceler-
ator co-design and co-search to solve the edge AI challenges: DNNs are designed to
satisfy accuracy demands and must be aware of the hardware constraints with rational
network conﬁgurations. At the same time, the accelerators need to provide extensive
support for diﬀerent DNN components without introducing too many restrictions on
network design and guarantee performance to meet the speciﬁcations. The authors
in [43] proposed the concept of DNN/accelerator co-design for the ﬁrst time, which
aims to consider software and hardware metrics simultaneously to “automatically
generate both DNN models and their corresponding implementations as pairs". This
concept is then demonstrated by winning the competitive System Design Contest for
low power object detection in the 56th IEEE/ACM Design Automation Conference
(DAC-SDC) [44].

Many follow-up works continued investigating the co-design opportunities be-
tween diﬀerent AI algorithms and hardware devices [45, 46, 47, 48, 49, 50, 51, 52].
These co-design approaches have been studied with remarkable achievements by
combining multiple optimization techniques across both hardware and software.
For example, while neural architecture search (NAS) has been largely successful
in designing high-quality DNN models [53, 54], hardware-aware NAS is drawing
increasing attention, which aims at delivering high-accuracy models with hardware
eﬃciency as well (e.g., FBNet [55] and MNasNet [56]). Other machine-learning
algorithm/hardware co-design works include FNAS [57], NAIS [48], EDD [58],
and NASAIC [59]. Driven by the success of such a co-design strategy, other types
of co-design methods are also proposed recently, including software/compiler co-
design [60, 61, 62], compiler/hardware co-design [63, 64, 65], etc.

3 Eﬃcient Machine Learning Model Designs

Machine learning applications require not only high inference accuracy but also
aggressive inference speed, throughput, and energy eﬃciency to meet real-life de-
mands. They rely on hardware-eﬃcient DNN designs, especially when targeting edge
scenarios with limited hardware resources. In this section, we introduce ELB-NN
[12] and VecQ [14] to deliver hardware-eﬃcient DNNs for embedded systems.

6

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

3.1 The ELB-NN

ELB-NN (Extremely Low Bit-width Neural Network) is proposed to enhance energy
eﬃciency when running image classiﬁcation on an embedded FPGA. It is one of
the ﬁrst hybrid low-bit-width designs that supports arbitrary DNN quantization.
This subsection presents the hybrid quantization feature of the ELB-NN and its
corresponding hardware accelerator design on embedded systems.

3.1.1 Hybrid quantization scheme

Hybrid quantization means that diﬀerent quantization schemes are involved for the
network’s parameters and activations. The quantization scheme can go all the way
down to binary. To better adapt the hybrid quantization, we ﬁrst investigate their
impacts on the network inference accuracy. We follow Eq. 1 to calculate the binary
weights. Here ˜𝑤 represents the full precision weights after back propagation, while
𝐸 (| ˜𝑤|) represents the mean of all the full-precision weights as a scaling factor. For the
ternary training, the 𝑤𝑡 (representing ternary parameters) can be calculated following
Eq. 2. Here we set the threshold 𝑤𝑡 ℎ𝑟 𝑒𝑠 = 0.7𝐸 (| ˜𝑤|) and calculate the scaling factor
𝐸 as suggested in [66]. We also apply relatively high precision using 8- or 4-bit
ﬁxed-point representation. We then use AlexNet [5] to perform quantitative analysis
when applying hybrid quantization.

𝑤𝑏 = 𝑠𝑖𝑔𝑛(| ˜𝑤|) × E(| ˜𝑤|)
(cid:40)𝑠𝑖𝑔𝑛( ˜𝑤)×E |𝑤𝑡 | > 𝑤𝑡 ℎ𝑟 𝑒𝑠
|𝑤𝑡 | ≤ 𝑤𝑡 ℎ𝑟 𝑒𝑠
0

𝑤𝑡 =

(1)

(2)

Fig. 1 Network representation when using hybrid quantization [12]

In this analysis with AlexNet, we focus on the impact of 1) the quantized pa-
rameters of the convolutional (CONV) and fully-connected (FC) layer, and 2) the
quantized activations. We use mid-CONV to denote all the CONV layers except the
ﬁrst CONV layer and mid-FC to denote all the FC layers except the last FC layer.
The naming rule of the proposed hybrid precision network can be referred to Fig. 1.
In Table 2, the 8-bit design (Alexnet-8-8888) only reduces the accuracy by 1.3%
compared to the original ﬂoat32 version. The accuracy is still promising after using
ternary (Alexnet-8-8228) and binary (Alexnet-8-8218) parameters for mid-CONV

Alexnet-4-8218network typeActivation bitwidthfirst-CONV weights bitwidthmid-CONV weights bitwidthmid-FC weights  bitwidthLast-FC weights bitwidthCompilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

7

Table 2 Inference accuracy with hybrid quantization using ImageNet dataset [12].

Network precision
Alexnet with ﬂoat32
Alexnet-8-8888
Alexnet-8-8228
Alexnet-8-8218
Alexnet-8-8118
Alexnet-4-8218
Alexnet-2-8218
Alexnet-4-8218 (w/o g.)
Alexnet-4-8218 (ext.)

Accuracy (Top-1)
55.9%[67]
54.6%
53.3%
52.6%
51.1%
49.3%
46.1%
53.2%
54.5%

and mid-FC layer. It means that the network is relatively robust to the precision
of parameters. On the contrary, the precision of activations signiﬁcantly impacts
classiﬁcation accuracy. Compared to the Alexnet-8-8218, we observe 3.3% and 6.5%
accuracy drop when activations move to 4 bits (Alexnet-4-8218) and 2 bits (Alexnet-
2-8218). To further investigate, we disable the group function, which was originally
proposed to handle the limited GPU memory issue. As a result, we capture an 80%
computation increase and a 3.9% accuracy improvement in Alexnet-4-8218 (w/o g.).
We further increase the channel number for the ﬁve CONV layers in Alexnet-4-8218
(ext.) and achieve 1.3% accuracy gain by aﬀording extra 61% computation compared
to Alexnet-4-8218 (w/o g.). By increasing the model complexity, we can bring back
the accuracy. These observations are insightful for hybrid DNN quantization as
parameters can be quantized more aggressively than activations.

3.1.2 Hardware accelerator for ELB-NN

To handle ELB-NN, we propose a parameterized Computation Engine (CE) in [12]
with ﬂexible support of low bit-width quantization and conﬁgurable parallelism
during execution. As shown in Fig. 2, it contains a four-input-parallel CE as an ex-
ample, where four inputs can be processed simultaneously (including binary/ternary
operations and accumulation) and then followed by batch normalization (BN) and
activation function. The precision of the accumulator is adjustable, which is intended
to allow more ﬂexible quantization designs and maintain the output accuracy. For a
larger number of inputs, an adder tree will be used before the accumulator for timing
enclosure.

To demonstrate the hardware eﬃciency of ELB-NN, we adopt the accelerator
with the proposed CE and accelerate diﬀerent quantized versions of the AlexNet and
VGG16 using an embedded platform called ZC706 (with an ARM CPU and a Xilinx
Kintex-7 FPGA). Results are shown in Table 3. ELB-NN can achieve throughput
performance up to 10.3 TOPS, which outperforms previous designs in [68, 69, 70].

8

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 2 Computation engine (CE) with binary and ternary logic operations [12].

Table 3 ELB-NN performance evaluated on an embedded platform (Xilinx ZC706) [12].

Network

LUT

Utilization
FF

BRAM

DSP

Alexnet-8-8888
Alexnet-8-8218
Alexnet-4-8218

86262(39%) 51387(12%) 303(56%) 808(90%)
103505(47%) 90125(21%) 498(91%) 550(61%)
105673(48%) 94149(22%) 463(85%) 880(98%)
Alexnet-4-8218 (w/o g.) 127393(58%) 105328(24%) 435(80%) 839(93%)
Alexnet-4-8218 (ext.) 124317(57%) 101558(23%) 481(88%) 783(87%)
112992(52%) 99396(23%) 509(93%) 298(33%)
137973(63%) 113262(26%) 499(92%) 651(72%)

VGG16-4-8218
VGG16-2-8118

Batch
size
2
5
8
7
7

Bandwidth
(GBytes/s)
10.8
3.35
3.35
4.30
3.4

Complexity
(GOP)
1.45
1.45
1.45
2.61
4.22

2
3

5.85
6.67

31.0
31.0

imges/s

340
856.1
1369.6
1198.5
599.2

110.7
332.2

Perf.
(TOPS)
0.493
1.24
1.99
2.59
2.53

3.43
10.3

3.2 The VecQ

Vectorized Quantization (VecQ) is a training quantization framework that is based
on a novel quantization loss measurement metric called vector loss. It is proposed to
provide ﬂexible bitwidth support with minimal quantization loss to achieve higher
model accuracy. In this subsection, we present the detailed deﬁnition of vector loss
and the VecQ quantization framework.

3.2.1 Quantization with Vector Loss

We use the square of the Euclidean distance of the data before and after quantiza-
tion to represent quantization loss. It is also called Square 2-norm or L2 distance.
Minimizing the L2 loss during the model training is proved to be adequate in pro-
viding higher model accuracy [71, 72, 73, 67, 74]. However, due to the non-convex
characteristic of optimization for L2 loss under a constrained bitwidth requirement,
the quantization easily falls into sub-optimal solution space. In addition, adopting
the L2 distance collects the loss of each quantized data individually and neglects the
distribution and correlations among these data points in a kernel or a layer.

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

9

Focusing on the limitations above, in VecQ, we ﬁrst ﬂatten and reshape the
weight set Wf (𝑙) for a layer of the DNN and reshape them as a vector 𝒘𝒇 (𝑙) with
the dimension of the size of the elements. For example, it will be a 𝑁 × 𝑀 × 𝐾 2-
dimensional vector for a CNN layer with 𝑁 input channel, 𝑀 output channel and 𝐾
size of ﬁlter kernel. The quantized weight vector is denoted as 𝒘𝒒 (𝑙).

Fig. 3 The attributes of a vector and the quantization angle of 𝑤 𝑓 and 𝑤𝑞.

There are two attributes of a vector, as shown in Fig. 3 orientation and modulus. We
deﬁne a quantization angle representing the intersection angle between the original
weight vector and the quantized vector. So, the vector distance between the weight
vector before and after quantization is determined by the quantization angle and the
vector’s modulus. We then deﬁne a vector loss, denoted as 𝐽𝑣, and compose it with
the orientation loss 𝐽𝑜 and the modulus loss 𝐽𝑚.

𝐽𝑣 = 𝐽 (𝒘 𝑓 , 𝒘𝑞) = 𝐽𝑜 (𝒘 𝑓 , 𝒘𝑞) + 𝐽𝑚(𝒘 𝑓 , 𝒘𝑞)

(3)

Where 𝐽𝑜 and 𝐽𝑚 are computed as:

𝐽𝑜 = 1 − cos 𝜃, (cos 𝜃 =

𝛼𝒘𝑞
|𝛼𝒘𝑞 |

𝒘 𝑓
|𝒘 𝑓 |

)

= 1 − 𝒆𝒗 𝒆𝒘 𝒇

𝑑
∑︁

= 1 −

(𝑒𝑣𝑖 𝑒𝑤 𝑓 𝑖 )

𝑖=1
𝐽𝑚 = ||𝒘 𝑓 − 𝛼𝒘𝑞 ||2
2

(4)

here, the 𝒆𝒗 and 𝒆𝒘 𝒇 represent the unit vector for 𝒗 and 𝒘𝒇 . 𝒘𝒇 is a weight vector of
a layer of a DNN containing 𝑑 weights.

With these approaches, the orientation loss 𝐽𝑜 indicates the optimized quantization
angle and the modulus loss 𝐽𝑚 indicates the optimized scale at this angle. Therefore,
our quantization takes two stages to minimize the two losses independently, which
are deﬁned as steering stage and driving stage as shown in Fig. 4. In the steering
stage, we adjust the orientation of the weight vector to minimize the orientation loss.
Then, we ﬁx the orientation and only scale the modulus of the vector at the driving
stage to minimize the modulus loss.

10

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 4 The overall ﬂow of quantization process, including both steering and driving stage [14].

3.2.2 Framework integration

Fig. 5 Integrated quantization process in DNN training [14].

The VecQ quantization is integrated into the DNN training ﬂow for both the
weight data and the activation data. As shown in Fig. 5. For weight data, taking a
layer 𝑙 as an example, during the forward propagation, the weights 𝑤 𝑓 (𝑙) represented
in ﬂoating-point is quantized into 𝑤𝑞 (𝑙), then use the quantized weights to compute
the output of this layer. To simplify the computing process, the weight is treated as
normally distributed and an interval 𝜆 is used regarding the given bitwidth constraint.

𝜃𝛼Find the optimal orientation vectorFind the optimal scaling factorConvlotion Full connection SoftmaxConvlotion Full connection Convlotion Full connection ConvVecQClipClipλ k Bitsλ Quantized weightsUniformly quantizing weights to discrete valuesInputOutput𝜔𝑓(cid:4666)𝑙(cid:4667) 𝜔𝑞(cid:4666)𝑙(cid:4667) Weights distributionActivation()VecQ𝑔(cid:4666)𝑙(cid:4667) Layer 𝑙 forward process 𝐴(cid:4666)𝑙(cid:3398)1(cid:4667) 𝐴(cid:4666)𝑙(cid:4667) Convlotion Full connection SoftmaxConvlotion Full connection Convlotion Full connection ConvVecQClipClipλ k Bitsλ Quantized weightsUniformly quantizing weights to discrete valuesInputOutput𝜔𝑓(cid:4666)𝑙(cid:4667) 𝜔𝑞(cid:4666)𝑙(cid:4667) Weights distributionActivation()VecQ𝑔(cid:4666)𝑙(cid:4667) Layer 𝑙 forward process 𝐴(cid:4666)𝑙(cid:3398)1(cid:4667) 𝐴(cid:4666)𝑙(cid:4667) Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

11

During the backward propagation, the gradient is calculated with 𝑤𝑞 (𝑙) instead of
𝑤 𝑓 (𝑙) and propagated. In the ﬁnal update process, the gradient 𝑔(𝑙) of 𝑤𝑞 (𝑙) is
updated to 𝑤 𝑓 (𝑙) [67].

For the activation output of a layer, during the training, we compute a distribution
parameter of the activation outputs 𝑝(𝑡) and update it with Exponential Moving
Average. During the inference, the distribution parameter is employed as a linear
factor to the activation function [75]. The 𝐴(𝑙) is the activation output of layer 𝑙,
and 𝐴𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛(·) is the non-linear activation function following the convolution or
fully-connected layers, such as Sigmoid, Tanh, ReLU.

We evaluate VecQ on image classiﬁcation task with the popular models and
compare the results to the state-of-the-art quantization solutions with the same
DNN model and bitwidth conﬁgurations. The state-of-the-art quantization solutions
include BWN [11], TWN [66], TTQ [76], TSQ [77], INQ [78] and ENN [79]. Note
here, not all of these quantization solutions provide bitwidth support from 1 to 8 bits.
As shown in Fig. 6, our VecQ quantization outperforms most of the solutions with the
same bitwidth conﬁgurations, and VecQ provides a wider range of bitwidth coverage
as well. It only loses the advantage when comparing to the solutions speciﬁcally
designed for binary weights.

Fig. 6 Comparison with state-of-the-art solutions.

4 Eﬃcient accelerator design and workload mapping

As discussed before, there exists an ever-widening barrier between fast DNN model
design in software and slow hardware accelerator implementation. To bridge the
hardware-software gap, in this section, we introduce DNNBuilder [30] and PyLog
[38] to provide eﬃcient solutions for automatically generating high-performance
hardware accelerators for DNN workload deployments.

2468Bitwidth6261605958575655AlexNetTop1Accuracy(%)2468Bitwidth2468Bitwidth72.570.067.565.062.560.057.555.0MobileNetV2Top1Accuracy(%)70686664626058ResNet18Top1Accuracy(%)BWNTWNTTQTSQINQENNVecQBWNBPWNTWNTTQINQENNVecQDCHAQQATTQTVecQ12

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

4.1 DNNBuilder

DNNBuilder is an end-to-end automation framework that can transform DNN de-
signs from popular deep learning frameworks to highly optimized hardware deploy-
ment on customized accelerators implemented on FPGAs. Users are no longer re-
quired to design and optimize accelerators manually but can enjoy the auto-generated
hardware accelerators for desired AI workloads. DNNBuilder introduces two ma-
jor architecture innovations: the ﬁne-grained layer-based pipeline architecture and
the column-based cache scheme, which achieve 7.7× and 43× reduction of latency
and on-chip memory usage, respectively. This subsection presents the novel designs
introduced by DNNBuilder and showcases its promising edge AI performance.

4.1.1 An end-to-end automation ﬂow

Fig. 7 The end-to-end design ﬂow introduced by DNNBuilder [30].

DNNBuilder produces customized DNN accelerators in three steps as Design,
Generation, and Execution (Fig. 7). During the Design step, a DNN is designed and
trained using deep learning frameworks, which in general employ CPUs and GPUs.
After training, network deﬁnition ﬁles and trained parameters are passed to the next
step. To ensure design freedom speciﬁed by users, the proposed ﬂow supports hybrid
quantization schemes, where diﬀerent quantization schemes can be applied to the
parameters and activations of diﬀerent network layers, to explore tradeoﬀs among
inference accuracy, resource utilization, performance, etc. One important feature of
this step is the feedback function that provides hardware metrics estimation. If the
current DNN runs slower or consumes more resources than expected, users could
update their network designs, such as adjusting quantization schemes or modifying
network layers to meet performance and resource requirements. This function also
makes the hardware-software co-design possible.

In the Generation step, network parsing is launched to decompose the input
models. Diﬀerent network layers, e.g., CONV, Pooling, and FC layers, are decom-

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

13

posed and then mapped to our pre-built RTL IPs, which are the basic building blocks
of the generated accelerator. The computational intensive nested loops are captured
by parameterized compute engines. Then, automated optimization works for ex-
ploring the hardware design space and provides conﬁguration guidelines so that the
generated accelerator can achieve maximum performance. Following these guide-
lines, network construction is responsible for building DNN implementations with
the pre-built RTL IPs, dataﬂow controller, and memory instances, which are highly
conﬁgurable to ensure the adaptability and scalability for various DNNs. After that,
code generation generates accelerator related ﬁles for FPGA-based instances.

In the Execution step, the DNN accelerator is instantiated in FPGA with uniﬁed
interfaces, including a FIFO-like data input/output interface and a weight access
interface connecting the oﬀ-chip memory controller. In this ﬁnal step, the DNN
accelerator is ready for eventual deployment.

4.1.2 Architecture novelties

Fig. 8 Latency comparison between the proposed ﬁne-grained (left) and conventional (right)
pipeline when handling the same object detection DNN model with a ZC706 embedded FPGA
[30].

We propose a ﬁne-grained layer-based pipeline to deliver high throughput perfor-
mance and promising real-time response. Each major neural network layer, such as
CONV or FC layer, in the targeted DNN model, is handled by one pipeline stage,
as major layers dominate computation and memory consumption. The rest of the
layers, such as batch normalization (BN), scale, and activation layers, are aggregated
to their neighboring major layers so that we reduce the number of pipeline stages
for lower latency. In addition, DNNBuilder enables pipeline stage overlapping to
overcome the long initial latency, which is frequently encountered by conventional
pipelines. We demonstrate the proposed ﬁne-grained layer-based pipeline by accel-
erating an object detection DNN model called YOLO [80] and show the results in
Fig. 8. DNNBuilder can eﬀectively hide the data transmission delay and generate
outputs even when the ﬁrst input frame is still loading. It helps achieve a 7.7× smaller
startup latency (9.92ms) compared to the conventional pipeline design (411.99ms).

14

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 9 The proposed column-based cache scheme [30].

The other novel design is the column-based cache scheme, which reduces on-chip
memory utilization during DNN inference and supports high-deﬁnition image input
for resource-constrained embedded systems. By following the pipeline architecture,
intermediate results between pipeline stages are stored on-chip to guarantee seamless
pipeline operations. However, feature maps can be enormous when inputs become
large in real life and become impossible to be held on-chip entirely. The column-
based cache scheme is designed to address this problem as it only keeps a subset of
the input feature map on chip. Fig. 9 shows an example when DNNBuilder processes
a convolution layer (with kernel size=3 and stride=1). Since slices 1∼3 contribute to
the ﬁrst sliding window operation (from top to bottom), we name the ﬁrst three slices
as column 1. Similarly, column 2 represents the amount of data for the second sliding
window operation, so that slices 2∼4 constitute column 2. DNNBuilder caches at
least two columns before starting computing, which allows the kernel to perform
the second vertical sliding window operation immediately after ﬁnishing the ﬁrst
one. Delay caused by data shortage will not happen by caching one more column.
Meanwhile, slice 5 will start buﬀering to form the next column (with slices 3∼5)
after releasing the room taken by slice 1. By serving the same objection detection AI
model (YOLO with high-deﬁnition inputs), the proposed column-based cache can
signiﬁcantly reduce 43× on-chip memory usage compared to the accelerator without
this technology [30].

4.1.3 State-of-the-art performance

We demonstrate our design by accelerating popular AI workloads on an embedded
platform (ZC706). As shown in Table 4, our DNNBuilder generated design reaches

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

15

the best performance (524 and 262 GOPS in Fix8 and Fix16 quantization schemes)
and power eﬃciency (72.8 GOPS/Watt in Fix8 and 36.4 GOPS/Watt in Fix16).
We also extend our comparison to the embedded GPU (TX2) in Table 5. The
DNNBuilder-generated design can deliver higher eﬃciency than the TX2-based
solution even without using batch processing (batch size = 1), and it can achieve up
to 47.2 image/Second/Watt.

Table 4 Comparison with existing embedded FPGA-based DNN accelerators [30].

Reference
FPGA chip
Frequency
Network
Precision
DSPs (used/total)
DSP Eﬃciency
Performance (GOPS)
Power Eﬃciency (GOPS/W)

[29]

DNNBuilder

[81]
Zynq XC7Z045 Zynq XC7Z045 Zynq XC7Z045
100 MHz
VGG
Fix16
824/900
69.6%
230
24.4

200MHz
VGG
Fix16 (Fix8)
680/900
96.2%
262 (524)
36.4 (72.8)

150 MHz
VGG
Fix16
780/900
44.0%
137
14.2

Table 5 Alexnet inference comparison on embedded GPU and FPGA platforms [30].

Platform

Precision Batch

DNNBuilder (ZC706) Fix16, Fix8 1, 2

GPU-TX2[82]

Float16

2

Throughput
(img./S)
170, 340
250

Power
(W)
7.2
10.7

Eﬃciency
(img./S/W)
23.6, 47.2
23.3

4.2 PyLog: A Python-based FPGA Programming Flow

The fast-growing complexity of new applications and new use scenarios poses seri-
ous challenges for computing systems. Embedded hardware accelerator systems have
demonstrated great ﬂexibility, performance, and eﬃciency in many diﬀerent appli-
cations and scenarios. However, as system complexity and application complexity
grow rapidly, programming and optimizing embedded accelerator systems require
great manual eﬀorts and consume a lot of time. Compiling and optimizing a gen-
eral application speciﬁed in high-level programs like Python are becoming common
tasks in creating embedded accelerator designs. High-level Synthesis (HLS) trans-
forms design inputs written in high-level languages (e.g., C++, OpenCL, Python) to
hardware descriptions in RTL (Register-Transfer Level) languages such as Verilog.
HLS oﬀers up to 10× code reduction and 1, 000× simulation time reduction over
manual RTL design solutions. HLS has been intensively studied in the past three

16

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

decades [33, 34, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97], and there
are popular commercial HLS tools used by many designers [98, 99].

4.2.1 PyLog Flow Overview

PyLog [38] is a Python-based high-level programming ﬂow for FPGAs. It allows
users to create FPGA accelerators with Python code using PyLog high-level opera-
tors and Python syntax. PyLog presents a uniﬁed programming model for host and
accelerator logic with consistent Python-based syntax and semantics. This seam-
less host-accelerator programming model enables agile system design, convenient
functional simulation, and ﬂexible design space exploration.

Fig. 10 shows the overall PyLog at high level. PyLog ﬂow allows users to create
eﬃcient FPGA accelerators and program host system with Python. The input to the
PyLog ﬂow is Python code, where the FPGA kernel function is decorated with the
@pylog decorator. The PyLog ﬂow contains an accelerator synthesis ﬂow and a
runtime ﬂow.

Fig. 10 The PyLog Flow and Example System Architecture [38]

In the accelerator synthesis ﬂow, the @pylog decorator calls the PyLog compiler
to compile the kernel function into optimized high-level synthesis (HLS) C code,
which is then compiled into eﬃcient FPGA IPs with HLS ﬂow, and integrated
into a complete accelerator system by PyLog system generator. Beside the PyLog
kernel function, the rest of the PyLog program is interpreted by the standard Python
interpreter running on the host CPU side, which supports all Python libraries and

@pylogdefaccel(d, w):...# maind = np.array(...)w = np.array(...)accel(d, w)acceleratorhostPyLogProgramPyLogCompilerPyLogRuntimeSystem GeneratorPYNQHLS C CodeCPUMain DDR MemoryHostOn-Board DDR MemoryFPGA BoardPCIe-AXI BridgeApplication LogicApplication LogicBRAMBRAMBRAMDSPDDR CtrlFPGA ChipPCIeFPGA BitstreamPyLogFlowXRTCompilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

17

Table 6 PyLog Supported Language Features [38]

Category
PyLog high-level operators map, dot, user-deﬁned ops

Operators

NumPy operators

Python features

argmax, argmin, max, min, matmul,
convolve, sort
list, functions, calls, lambda,
for, while, if...else..., slice,
subscript, attribute, bin_op,
unary_op, return

language features. This part of PyLog program naturally becomes the host program of
whole accelerator system. After the accelerator system is generated by the synthesis
ﬂow, the system can be deployed at the target FPGA platform using the generated
FPGA bitstream conﬁguration ﬁle, and runs with support from the PyLog runtime
ﬂow. During runtime, PyLog runtime can prepare input data, launch accelerator,
and collect results according to the host code. Host CPU and the FPGA accelerator
interactions are handled automatically by the PyLog runtime and the underlying
Xilinx PYNQ library [100].

4.2.2 PyLog Features

PyLog has several unique features that help users to create FPGA accelerators more
eﬃciently.

(i) High-Level Operators.

In addition to commonly used Python standard language features, PyLog also
supports several built in high-level operators and NumPy operators that allow
users to express computation patterns at high level and enable better compiler
optimizations. Table 6 summarizes the language features supported in PyLog,
including PyLog high-level operators, NumPy operators, and standard Python
features. Listing 1 demonstrates a few example usages of PyLog map and dot
operators.

1 # Vector add
2 out = map( lambda x, y: x + y, vec_a , vec_b)
3
4 # 1D convolution
5 out = map( lambda x:w0*x[ -1]+ w1*x[0]+ w2*x[1], vec)
6
7 # Inner product
8 out_vec [i] = dot( matrix [i ,:], in_vec )
9
10 # Square matrix multiplication
11 out = map( lambda x,y: dot(x[0,:],y[: ,0]) , ma , mb)

Listing 1 PyLog map and dot examples. [38]

These operators not only simplify programming for users, they also pass more in-
formation on computation to the compiler (e.g., computation patterns, data ﬂow

18

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 11 Diﬀerent implementations generated from the same PyLog code. [38]

information, data/task parallelism, etc.), compared to programming in C/C++,
and thus allows compilers to perform more optimizations and choose the optimal
code generation.
Fig. 11 shows an example of generating multiple hardware implementations
from a PyLog map operation. The compiler generates HLS C implementations
in diﬀerent styles, which corresponds to diﬀerent hardware structures, e.g. shift
registers, systolic arrays, etc. Depending on the context and constraints, the
optimal implementation will be chosen.

(ii) Type Inference and Type Checking.

Python is a dynamically typed languages and there is no explicit type declaration
in the Python code. PyLog has a builtin type inference and type checking engine
that can infer the type and shape information of code objects in the PyLog kernel
functions. This type inference engine is critical in PyLog since same operators
may have completely diﬀerent meanings when applied to operands with diﬀerent
types or shapes. With this type inference engine, PyLog users do not need to
provide explicit type annotations or hints in PyLog program.

(iii) Compiler Optimizations.

PyLog provides a set of compiler optimizations that improve the design quality
of generated accelerators. PyLog uses its own PyLog intermediate representa-
tion (PLIR) as the internal representation of the input code. PyLog code analysis
and transformation passes work on PLIR to perform a sequence of optimizations
including high-level operator lowering, loop transformation, HLS pragma inser-
tion, etc. The internal PLIR is capable of expressing diﬀerent design options
and can therefore form a design space that not only covers low-level design
parameter tuning, but also high-level design pattern selection, which has not
been explored in previous tools.

for (i...) {for (j...) {for (k...) {...} } }y= map(lambdaa, b: dot(a[0,:],b[:,0]), mat_a, mat_b)for (k...) {#pragma HLS unrollfor (i...) {for (j...) {} } }...for (i...) {for (ii...) {...} }.........PEPEPEPEPEPEPEPEPEPEPEPEPEPEPyLoghigh-level operationsHLS C ImplementationsHardware ImplementationsCompilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

19

Fig. 12 Length of HLS C code and PyLog code. [38]

4.2.3 PyLog Evaluation Results

We evaluate the performance of PyLog in terms of expressiveness and accelerator
performance, using real-world applications.

(i) Expressiveness. We evaluated the expresiveness of PyLog by comparing the
number of lines of code to implement a benchmark using PyLog and HLS C.
For the benchmarks evaluated, on average PyLog only needs around 30% of the
code length of HLS C. This indicates that PyLog provides good expressiveness
compared with HLS C and allows users to describe their computation with fewer
lines of code.

(ii) Accelerator Performance. We evaluated the performance of the accelerators
generated by PyLog using real-world benchmarks. Evaluation was done on Ama-
zon EC2 F1 f1.2xlarge instance. The evaluated benchmarks are from diﬀerent
domains and have various computation patterns. They are representative FPGA
workloads, including linear algebra, data analytics, stencil, sparse operations,
etc. Amazon EC2 F1 f1.2xlarge instance is a cloud computing platform that
has an 8-core Intel Xeon E5-2686 v4 CPU and a Xilinx Virtex UltraScale+
XCVU9P FPGA. Table 7 shows the evaluation results. The table lists the FPGA
resource utilization as well as the accelerator execution time. We compared the
PyLog accelerator execution time against the optimized CPU time as well as
the execution time of accelerators generated from [101]. On average, PyLog
accelerators achieve around 3.17× and 1.24× speedup over CPU baseline and
manually optimized accelerators [38].

5 Eﬃcient Optimizations

With a great number of eﬃcient optimization techniques, in this section, we introduce
three key optimization techniques: hardware-aware NAS, FPGA/DNN co-design, a
specialized approach for FPGA/DNN co-design [45], and a uniﬁed diﬀerentiable
co-design approach, across diﬀerent platforms [58].

00.20.40.60.81GEMMKNNK-meansJacobi2DSeidelGaussianSpMVLines of Code(normalized to HLS C)HLS CPyLog20

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Table 7 Accelerator Performance Evaluation on AWS F1 Instance

Benchmark LUT

FF BRAM DSP 𝑓 (MHz) 𝑃(W) 𝑇CPU 𝑇HCL 𝑇PyLog

KNN 109276 74889
K-means 10829 17604
Jacobi [102] 93925 111144
Seidel [102] 47304 57854
Gaussian [102] 56580 75846
GEMM 12868 63759
SpMV 8294 12787
7647
4096

Histogram [103]

𝑇CPU
𝑇PyLog

𝑇HCL
𝑇PyLog
0 256.40 37.222 0.48 0.45 0.26 1.85 1.73
425
3
7 273.97 37.429 38.16 4.24 4.45 8.58 0.95
96 704 269.03 37.327 11.31 8.25 5.19 2.18 1.59
30 304 269.03 37.341 21.37 8.22 5.16 4.14 1.59
48 688 147.15 37.783 23.63 7.34 5.19 4.55 1.41
655 1024 250.00 39.657 60.34 8.13 13.05 4.62 0.62
25
13

21 273.97 37.225 0.29
0 273.97 37.327 5.85

0.24 1.21
2.07 2.83

-
-

-
-

Geometric Mean 3.17 1.24
𝑇CPU: Execution time on CPU; 𝑇HCL: Execution time on HeteroCL[101] generated accelerator;
𝑇PyLog: Execution time on PyLog generated accelerator; All time values are in milliseconds
(ms); ‘-’ means the implementation is not publicly available.

5.1 Overview of Hardware-aware Neural Architecture Search (NAS)

Table 8 A brief overview of Neural Architecture Search components and example algorithms.

Search space

Architecture search space

Hardware search space

Search algorithm

Sampling-based

Supernet-based

Network evaluation

NASNet [54], DARTS [104], FB-Net [55],
ProxylessNAS [105], FlexiBERT [106]...
Quantization, sparsiﬁcation, tiling parameters,
number of PEs, other HW speciﬁc parameters...
Reinforcement learning, evolutionary algorithm,
Bayesian optimization, random search...
DARTS, Random sampling [107], SNAS [108],
EDD [58], ProxylessNAS, OFA [109]...
Early stopping, NAO [110], NASWOT [111],
Synﬂow [112], GenNAS [113]...

Neural Architecture Search (NAS) refers to the automated process of neural
architectural design [114]. It has been largely successful in producing many state-
of-the-art networks. Typically, a NAS process requires three distinct components as
shown in Table 8:

1. Search space. A search space includes all possible network architectures that
follow a predeﬁned template. For example, the networks can be sequential
layer-wise architecture [115, 116], cell-based architecture [54], and hierarchical
architecture [56]. Also, hardware parameters should be considered into the
search space for HW-awared NAS.

2. Search algorithm. The fundamental component of NAS is the search algorithm.
Given the prohibitively large search space, the search algorithm can greatly
inﬂuence the eﬃciency of the search and the eﬀectiveness of the ﬁnal network

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

21

architecture. Generally, search algorithms can be classiﬁed into two categories:
supernet-based search [104, 55] and sampling-based search [115, 115, 110, 113].
3. Network evaluation. Network evaluation is the key for eﬃcient NAS, since fast
evaluation is required to estimate the quality of individual networks to guide the
search algorithm to choose top-performing architectures from the search space.
Network evaluation can be prohibitively expensive due to network training,
so that various approximation approaches have been proposed to expedite the
evaluation such as few-shot and one-shot training [117, 118] and using proxy
tasks [112].

5.2 HW-aware NAS Formulation

In recent years, driven by the need of deploying power-hungry DNNs into resource-
constrained devices, hardware-aware NAS (HW-NAS) has emerged as one of the
most promising techniques [119]. There is a great amount of hardware-aware work,
each of which often adopts a speciﬁc hardware device (CPU, GPU, embedded/mobile
device) and requires a diﬀerent hardware-cost metric (e.g., prioritizes latency or
energy). For example, FBNet [55] develops a diﬀerentiable neural architecture search
(DNAS) framework and discovers state-of-the-art DNNs balancing both accuracy
and hardware eﬃciency, by incorporating a loss consisting of both the cross-entropy
loss that leads to better accuracy and the latency loss that penalizes the network’s
latency on a target device. To provide more integrated co-optimization solutions,
EDD [58] fuses the design space of DNN architecture and hardware accelerator
and formulates the DNN and hardware design as a co-search problem. EDD aims
to discover the most suitable combination of DNN and hardware within the co-
search space and maximize software and hardware metrics given the targeted edge
AI application. Once for All (OFA) [109] is the ﬁrst work that proposes an elastic
training scheme for supernet. By training the supernet, high-accuracy architectures
is directly searched by selecting from the OFA network without additional training.
One of the classic search method for HW-NAS is to ﬁrst deﬁne a template based

search space, and then incorporate hardware performance into the loss function:

L = L𝑇 + L𝐻 𝑊

or

L = L𝑇 · L𝐻 𝑊

(5)

where L𝑇 is the task-speciﬁc loss of NAS, such as cross-entropy loss for classiﬁcation
tasks or Mean squared error (MSE) loss for regression tasks. L𝐻 𝑊 is the hardware
performance loss, such as measured or estimated execution latency of the network
architectures on the target device.

22

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 13 FPGA/DNN co-design framework [45].

5.3 FPGA/DNN Co-design

Hao and Chen ﬁrst proposed the concept of accelerator and DNN co-design in an
invited paper titled “Deep Neural Network Model and FPGA Accelerator Co-design:
Opportunities and Challenges” [43], where they advocated “automatically generate
both DNN models and their corresponding implementations as pairs”. Later, based on
the proposed co-design method, we implemented the ﬁrst simultaneous FPGA/DNN
co-design framework [45]. It has two major components, as shown in Fig. 13: (1)
a hardware-oriented bottom-up DNN model design, named Auto-DNN, which is
an eﬃcient search engine to explore DNN candidates under hardware resource and
performance constraints; (2) a DNN-driven top-down FPGA accelerator design,
named Auto-HLS, which is a fast board-level design generator to automatically map
DNNs onto FPGAs.

5.3.1 The key to co-design: Bundle

The key to achieve co-design, i.e., to execute Auto-DNN and Auto-HLS simultane-
ously, is to propose basic building blocks that can be used to construct both DNNs
and their accelerators at the same time. We call such building blocks Bundles,
the common building block of both DNN architectures as well as their hardware
implementation, as shown in Fig. 14. The beneﬁts of Bundles are two-fold. First,
a DNN can be constructed by replicating a bundle for a certain number of layers
with pooling layers inserted, which is a common and eﬀective way to construct
high-quality DNNs, such as the residual block in ResNet [2], the Inception block in
GoogLeNet [20]; meanwhile, many NAS approaches follow such cell-based strat-
egy [54, 56, 104]. Second, an accelerator can be constructed by building a hardware

Callie Hao | Sharc‐lab @ Georgia Institute of Technology29DNN/FPGA Co‐Design FlowTarget ML task; FPGA device (resources); performance targets (QoS)Auto‐HLS:FPGA Accelerator GeneratorStep 1.Basic building block modelingStep 2.Building block selectionStep 3.DNN search and updateAuto‐DNN: Co‐Search EngineSoftware:DNN ModelHardware:FPGA AcceleratorCompilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

23

Fig. 14 The key to co-design: Bundle – the common basic building block to both DNN design and
accelerator design [45].

module for the certain bundle and reusing it for diﬀerent DNN layers, given that the
DNN is built by replicating the bundle; this can signiﬁcantly reduce the resource
usage of the accelerator by resource sharing and shorten the hardware development
cycle. As an example, a Bundle can be a set of DNN layers including: one 3 × 3
convolution, one batch normalization, one activation, one 1 × 1 convolution, and one
activation. Meanwhile, the hardware accelerator will need one instance for the 3 × 3
convolution, one instance for the 1 × 1 convolution, and so on.

5.3.2 Progressively reducing search space

It is non-trivial to select an optimal Bundle given the large design space and the
prohibitively long DNN training time. Therefore, it it essential to narrow down the
search space as early as possible. Our approach is in a three-step progressive way,
by ﬁltering out unfavourable bundles at early stage and conducting detailed search
at later stage using promising ones. The three steps are as follows.
Step 1. Analytical models for performance and resource estimation for bundles and
DNNs. Denoting a Bundle as 𝑏𝑢𝑛𝑑𝑖, the resource of 𝑏𝑢𝑛𝑑𝑖 is computed as:

𝑅𝑒𝑠𝑟

𝑏𝑢𝑛𝑑𝑖 =

𝑅𝑒𝑠𝑟

𝑗 + Γ𝑟
𝑖

∑︁

𝑝 𝑗

(6)

𝑗 is the resource usage of instance 𝑝 𝑗 of resource type 𝑟 ( including
𝑖 represents other resource overhead such as LUTs

where 𝑅𝑒𝑠𝑟
DSP, LUTs, FF and BRAM). Γ𝑟
consumed by control logic and multiplexers.
The latency of a Bundle is estimated as:

SoftwareHardwareDNNFPGAConv3x3Conv1x1DW‐Conv3x3ActivationConv3x3Conv1x1DW‐Conv3x3ActivationInputOutput…2ndBasic Block1stBasic Block3rdBasic BlockConvolution1x1Convolution3x3PoolingReluDepth‐wise Convolution 3x3Implements24

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

𝐿𝑎𝑡𝑏𝑢𝑛𝑑𝑖 = 𝛼𝑖 ·

𝐶𝑜𝑚 𝑝 𝑗 +

𝛽𝑖 · Θ(𝐷𝑎𝑡𝑎𝑖)
𝑏𝑤

∑︁

𝑝 𝑗

(7)

where 𝐶𝑜𝑚 𝑝 𝑗 is the computation latency of instance 𝑝 𝑗 , and Θ(𝐷𝑎𝑡𝑎𝑖) is the data
amount processed by 𝑏𝑢𝑛𝑑𝑖. 𝑏𝑤 represents the oﬀ-chip memory bandwidth. Denote
the latency of one execution of 𝑝 𝑗 as 𝑙𝑎𝑡 𝑗 , and the total number of reuses of 𝑝 𝑗 as
𝑟𝑒𝑢𝑠𝑒 𝑗 , the computation latency 𝐶𝑜𝑚 𝑝 𝑗 is estimated as:

𝐶𝑜𝑚 𝑝 𝑗 =

∑︁

𝑟𝑒𝑢𝑠𝑒 𝑗 · 𝑙𝑎𝑡 𝑗

(8)

1≤ 𝑗 ≤𝑛
𝑟𝑒𝑢𝑠𝑒 𝑗 can be computed by the input/output dimensions of the data processed by the
IP and the data dimensions of 𝑝 𝑗 ’s interface. The parameter 𝛼𝑖 in Eq. 7 describes
how much computation is overlapped because of IP pipelining, and 𝛽𝑖 describes
how much data transfer is overlapped during computations. 𝛼𝑖, 𝛽𝑖 and Γ𝑖 will be
determined for each 𝑏𝑢𝑛𝑑𝑖 using Auto-HLS sampling.

The overall DNN latency based on 𝐿𝑎𝑡𝑏𝑢𝑛𝑑𝑖 in Eq. 7 is estimated as:

𝐿𝑎𝑡𝐷 𝑁 𝑁 =

𝑁
∑︁

𝑖=1

𝐿𝑎𝑡𝑏𝑢𝑛𝑑 + 𝜙 · 𝐿𝑎𝑡𝐷 𝑀

(9)

where 𝑁 is the the number of Bundle repetitions of the DNN, and 𝜙·𝐿𝑎𝑡𝐷 𝑀 represents
the inter-bundle data movement latency. For overall DNN resource utilization, we
have:

𝑅𝑒𝑠𝐷 𝑁 𝑁 = 𝑅𝑒𝑠𝑏𝑢𝑛𝑑𝑖 + 𝛾 · 𝑅𝑒𝑠𝑐𝑡𝑙
(10)
where 𝑅𝑒𝑠𝑏𝑢𝑛𝑑𝑖 is the resource of 𝑏𝑢𝑛𝑑𝑖, and 𝑅𝑒𝑠𝑐𝑡𝑙 is additional control logic
overhead, e.g., ﬁnite state machine and multiplexers. 𝜙, 𝛾, 𝐿𝑎𝑡𝐷 𝑀 and 𝑅𝑒𝑠𝑐𝑡𝑙 will
be decided and calibrated through actual hardware synthesis and implementation.
Step 2. Bundle evaluation and selection. In this step, we evaluate the latency, re-
source, and accuracy metrics for each Bundle, as deﬁned in Step 1. Since we cannot
evaluate the accuracy for a single Bundle, we replicate a Bundle for 𝑛 times to build
a DNN and train it for a small number of epochs (20 in the experiment). We plot
Pareto curves for the Bundles to examine the tradeoﬀ between DNN accuracy and
resource utilization, and the Bundles on the Pareto curve will be selected for detailed
search in the next step.
Step 3. DNN construction using Bundles and training. After selecting top-𝑁 promis-
ing Bundle candidates, We search DNN models under resource and latency con-
straints. For each Bundle, 𝐾 initial DNNs are generated and are progressively updated
by adjusting the number of channels, pooling layer positions, etc., until the latency
target is met. Then, we perturb the initial DNNs by changing three variables: the
number of Bundle replications, down-sampling conﬁgurations between bundles, and
channel expansion conﬁguration. We adopted Stochastic Coordinate Descent (SCD)
algorithm for perturbation, while other heuristic or evolutionary algorithms can be
applied as well. The goal of the search algorithm is to ﬁnd the DNN architecture
which meets the performance constraints with highest accuracy.

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

25

Table 9 Performance Comparisons (FPGA and GPU competition data are obtained from [120])

Model
DNN1

IoU

Latency (ms)

Ours

DNN3

DNN2

68.6% 80.0 (100 MHz)
57.4 (150 MHz)
61.2% 62.6 (100 MHz)
44.1 (150 MHz)
59.3% 47.8 (100 MHz)
33.7 (150 MHz)
62.4% 84.6 (150 MHz)
1st in FPGA
49.2% 38.5 (150 MHz)
2nd in FPGA
57.3% 136.1 (150 MHz)
3rd in FPGA
1st in GPU
69.8% 40.7 (854 MHz)
2nd in GPU Tiny-Yolo 69.1% 39.5 (854 MHz)
3rd in GPU Tiny-Yolo 68.5% 42.3 (854 MHz)

SSD
–
–
Yolo

Power
Energy
FPS
2.2W 8.80 KJ
12.5
2.5W 7.18 KJ
17.4
2.2W 7.50 KJ
16.0
2.4W 5.51 KJ
22.7
2.2W 5.74 KJ
20.9
2.4W 4.04 KJ
29.7
4.2W 17.56 KJ
11.96
2.5W 4.81 KJ
25.97
7.35
2.6W 17.69 KJ
24.55 12.6W 25.66 KJ
25.3
13.3W 26.28 KJ
23.64 10.3W 21.79 KJ

Eﬃciency
0.18 J/pic
0.14 J/pic
0.15 J/pic
0.11 J/pic
0.11 J/pic
0.08 J/pic
0.35 J/pic
0.10 J/pic
0.35 J/pic
0.51 J/pic
0.53 J/pic
0.44 J/pic

5.3.3 Evaluation results

To evaluate the eﬀectiveness of the co-design framework, we apply it on a low-power
object detection competition [120], and compare to the top-3 winners for both FPGA
and GPU categories. The results are shown in Table 9. We make comparisons in: (1)
the Intersection over Union (IoU); (2) the latency for processing one frame and the
overall frame per second (FPS); (3) the board power; (4) the energy consumption
for all testing data; and (5) the energy eﬃciency per frame (J/pic). The results are
collected from the board-level implementations on Pynq-Z1. The latency refers to
the execution time for a single frame in millisecond, while FPS is measured using
total run-time for the 50K images including image loading, preprocessing, and DNN
inference.

Compared to the 1st-place winner of the FPGA category, we achieve 6.2% higher
IoU, 40% lower power, and 2.5× better energy eﬃciency, which we attribute to
the eﬀectiveness of an automated co-search instead of manual designs. Compared to
GPU-based designs, our DNN1 model is more accurate than the 3rd-place design and
only 1.2% lower IoU than the 1st-place GPU design. Regarding the energy eﬃciency,
ours is 3.6× better than the 1st-place GPU design with 40% longer latency despite a
nearly 6× slower clock frequency.

5.4 EDD: Eﬃcient Diﬀerential DNN Architecture Search

On top of the FPGA/DNN co-design introduced in Sec. 5.3, we further develop
co-design to a more generalized and uniﬁed approach, i.e., fully simultaneous neural
architecture and implementation co-search, targeting arbitrary hardware platforms.
Neural architecture and implementation co-search (NAIS) [48] is the ﬁrst work that
stylized design methodology targeting both FPGAs and GPUs, while EDD [58] is
a fully simultaneous, eﬃcient diﬀerentiable DNN architecture and implementation
co-search methodology. The overall architecture of EDD is presented in Fig. 15.

26

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 15 The overall architecture of EDD [58].

5.4.1 Fused co-design space

The key technology is to fuse the design space of DNN architecture search and
hardware implementation search. We collectively denote the variables used in DNN
search and implementation search as 𝐴 and 𝐼, respectively, and the fused space of
co-search is { 𝐴, 𝐼}. To carry out both DNN architecture and hardware accelerator
co-search in the fused DNN/accelerator space as described in E. 5, we minimize the
following loss function:

𝑚𝑖𝑛 : L = 𝐴𝑐𝑐𝑙𝑜𝑠𝑠 ( 𝐴, 𝐼) · 𝑃𝑒𝑟 𝑓𝑙𝑜𝑠𝑠 (𝐼) + 𝛽 · 𝐶 𝑅𝐸 𝑆 (𝐼 )−𝑅𝐸 𝑆𝑢𝑏

(11)

In the above equation, 𝐴𝑐𝑐𝑙𝑜𝑠𝑠 is the DNN accuracy loss; 𝑃𝑒𝑟 𝑓𝑙𝑜𝑠𝑠 is the hardware
performance loss such as end-to-end inference latency, throughput, energy, DNN
model complexity, etc.; multiple performance metrics can be optimized simultane-
ously by deﬁning a single weighted loss. 𝑅𝐸 𝑆 is the resource utilization and 𝑅𝐸 𝑆𝑢𝑏
is resource upper bound. Apparently, 𝐴𝑐𝑐𝑙𝑜𝑠𝑠 is a function of 𝐴 and 𝐼; 𝑃𝑒𝑟 𝑓𝑙𝑜𝑠𝑠 and
𝑅𝐸 𝑆 are functions of 𝐼. Resource upper-bound 𝑅𝐸 𝑆𝑢𝑏 is expressed in an exponent
term to introduce large penalty when being violated. Worth noting, in the existing
hardware-aware NAS approaches, only 𝐴 is searched while 𝐼 is ﬁxed during NAS.
In our proposed co-search formulation, 𝐼 is variable, and 𝐴 and 𝐼 are fused as one
design space {𝐴, 𝐼}.

𝑜𝑝(cid:3036)(cid:2869)𝑜𝑝(cid:3036)(cid:2870)𝑜𝑝(cid:3036)(cid:3040)…𝐼𝑛𝑝𝑢𝑡(cid:3036)𝑂𝑢𝑡𝑝𝑢𝑡(cid:3036)𝜃(cid:3036),(cid:2869)𝜃(cid:3036),(cid:2870)𝜃(cid:3036),(cid:3040)𝑑𝑤‐𝑘(cid:3400)𝑘𝐶𝑜𝑛𝑣‐1(cid:3400)1𝐶𝑜𝑛𝑣‐1(cid:3400)1Channel expandsby 𝑐ℎ(cid:3036)(cid:3040)Channel shrinksby 𝑐ℎ(cid:3036)(cid:3040)…𝑏𝑙𝑜𝑐𝑘(cid:3036)DNNOne candidate operation 𝑜𝑝(cid:3036)(cid:3040)Candidate operations of 𝑏𝑙𝑜𝑐𝑘(cid:3036)𝜃(cid:3036),(cid:3040)∈𝜣: sampling parameters of operation 𝑜𝑝(cid:3036)(cid:3040)𝑞(cid:2869)𝑞(cid:2870)𝑞(cid:2901)inputoutput…𝜙(cid:3036),(cid:3040),(cid:3044)∈𝜱: Sampling parameters of 𝑞‐𝑏𝑖𝑡quantization𝐼𝑛𝑝𝑢𝑡(cid:3036)𝑂𝑢𝑡𝑝𝑢𝑡(cid:3036)Neural Architecture Search (NAS)Implemen‐tationSearch…𝜙(cid:3036),(cid:3040),(cid:2869)𝜙(cid:3036),(cid:3040),(cid:2870)𝜙(cid:3036),(cid:3040),(cid:3018)Other implementation variables in𝐼(cid:3036)(cid:3040)•FPGA: parallel factors, loop tiling factors, etc.•GPU: batch size, etc.‐bit‐bit‐bit𝑨…Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

27

NAS design space. In the search space, each DNN is composed of 𝑁 basic building
blocks in a single-path fashion without branches [121]. Inside each block, there are
𝑀 candidate operations. We adopt the most commonly used DNN blocks in NAS
approaches, called MBConv [56], which is composed of sequential layers of 𝑐𝑜𝑛𝑣-
1×1, 𝑑𝑤𝑐𝑜𝑛𝑣-𝑘 ×𝑘 and 𝑐𝑜𝑛𝑣-1×1, where 𝑘 is the kernel size. Between 𝑐𝑜𝑛𝑣-1×1 and
𝑑𝑤𝑐𝑜𝑛𝑣-𝑘 ×𝑘, the number of channels expands/shrinks by a ratio of 𝑐ℎ𝑚
𝑖 for operation
𝑜 𝑝𝑚
𝑖 . The output of each block is calculated based on the outputs of its 𝑀 candidate
operations. Speciﬁcally, we adopt the Gumbel-Softmax function in [55], where each
operation 𝑜 𝑝𝑖
𝑚 will be sampled from a sampling parameter 𝜃𝑖,𝑚 following Gumbel-
Softmax distribution, which converts the discrete non-diﬀerentiable sampling to
continuous diﬀerentiable sampling. The sampled operations form a complete DNN,
which can be evaluated for accuracy and implementation performance.
Implementation search space We let each candidate operation 𝑜 𝑝𝑚
𝑖 has its own
implementation variables, forming an implementation search space 𝐼 𝑚
𝑖 . The primary
implementation variable is quantization 𝑞, i.e., data precision, since it has a large im-
pact on DNN accuracy, implementation performance and hardware resource. Rather
than using a train-and-quantize approach, the quantization shall be searched together
with DNN structure to provide implementation performance feedback. Besides quan-
tization, other implementation variables can also be integrated into the framework,
such as accelerator parallelism, loop tiling factors, batch size, etc.

5.4.2 Diﬀerentiable performance and resource formulation

The key technology is how to formulate the loss function diﬀerentiable with respect
to the search space 𝐴 and 𝐼. Since NAS search space 𝐴 is discrete, diﬀerentiable
formulation requires continuous relaxation. DARTS [104] is the ﬁrst work that uses
softmax for relaxation, while FBNet uses Gumbel-softmax [122] by sampling from
the discrete space. Such relaxation has been demonstrated to be GPU-hours eﬃ-
cient with appealing model accuracy [104, 55, 113]. Motivated by FBNet, a similar
technique using Gumbel-softmax can be applied to diﬀerentiable implementation 𝐼
to convert the discrete implementation search space into continuous. Therefore, by
descending the loss function on validation set, { 𝐴, 𝐼} can be learned simultaneously.

5.4.3 State-of-the-art results

We demonstrate the results on a subset of ImageNet dataset randomly sampled from
100 classes and target three hardware architectures, each with a searched DNN
model, called EDD-Net: (1) low-latency oriented GPU (EDD-Net-1); (2) folded
FPGA architecture (EDD-Net-2), where a single processing element (PE) will be
reused by all layers; (3) pipelined FPGA architecture (EDD-Net-3), where each layer
has its own PE, and all PEs work simultaneously. Each model is produced through
EDD within a 12-hour search on a P100 GPU.

28

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Table 10 Comparisons with existing NAS solutions [58].

Test Error (%) GPU Latency FPGA Latency
Titan RTX ZCU102 [123]
Top-1 Top-5

Baseline Models
GoogleNet
MobileNet-V2 [124]
ShuﬄeNet-V2 [125]
ResNet18

30.22 10.47
28.1
30.6
30.2

9.7
11.7
10.9

Hardware-aware NAS Models

MNasNet-A1 [56]
FBNet-C [55]
Proxyless-cpu [105]

24.8
24.9
24.7
Proxyless-Mobile [105] 25.4
24.9
25.3
25.4

Proxyless-gpu [105]
EDD-Net-1
EDD-Net-2

7.5
7.6
7.6
7.8
7.5
7.7
7.9

27.75 ms
17.87 ms
21.91 ms
9.71 ms

17.94 ms
22.54 ms
21.34 ms
21.23 ms
15.72 ms
11.17 ms
13.00 ms

13.25 ms
10.85 ms
NA
10.15ms

8.78 ms
12.21 ms
10.81 ms
10.78 ms
10.79 ms
11.15 ms
7.96 ms

Table 11 EDD-Net-1 accuracy and latency on 1080 Ti [58].

32-bit Floating 16-bit Floating 8-bit Integer

Test Error
Latency

25.5%
2.83 ms

25.3%
2.29 ms

26.4%
1.74 ms

Table 12 Comparison of EDD-Net-3 with DNNBuilder [30]

Top-1 Error (%) Top-5 Error (%) Throughput (ZC706)

VGG16
EDD-Net-3

29.5
25.6

10.0
7.7

27.7 fps
40.2 fps

For GPU-targeted EDD-Net-1, the results are as shown in Table 10, where the
GPU latency is tested on Titan RTX. It shows that EDD-Net-1 reaches similar or
better accuracy comparing with the state-of-the-art DNN models and other NAS
approaches while achieving the shortest inference latency. Table 11 shows the accu-
racy and latency tradeoﬀ of diﬀerent precisions of EDD-Net-1 on Nvidia 1080 Ti
GPU. For FPGA-targeted EDD-Net-2, the latency values are collected by running
DNN models with CHaiDNN accelerators on ZCU102 FPGA as shown in Table
10. It shows that EDD-Net-2 delivers the shortest latency on FPGA among all the
DNNs. FPGA-targeted EDD-Net-3 is searched targeting a pipelined FPGA accel-
erator. As shown in Table 12, EDD-Net-3 achieves higher throughput with a much
higher accuracy comparing with the state-of-the-art.

6 Conclusion

Emerging DNN-based AI applications are challenging for embedded systems as these
applications come with high computation and memory demands as well as diverse
application-speciﬁc requirements, such as real-time responses, high-throughput per-
formance, and reliable inference accuracy. This chapter introduced a series of eﬀec-
tive design methods to overcome these challenges to enable embedded AI solutions.
These methods can be categorized into eﬃcient machine learning algorithms, ac-

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

29

celerator and compiler designs, and various co-design and optimization strategies.
We ﬁrst proposed ELB-NN and VecQ to strengthen the AI model’s hardware ef-
ﬁciency by enabling extremely low bit-width quantization during model training
and inference. Then, we proposed DNNBuilder and PyLog for customized hardware
accelerator design and DNN workload mapping to such accelerators. At last, we in-
troduced eﬃcient co-design strategies, including FPGA/DNN co-design and EDD,
when deploying AI workloads on embedded systems.

We believe embedded AI solutions will involve more eﬀective and comprehen-
sive design methods in the future, covering AI algorithms, customized accelerators,
and co-design and co-optimization strategies between algorithm and hardware. For
example, our eﬃcient AI algorithm designs, such as ELB-NN and VecQ, can adopt
more advanced quantization schemes to minimize network compression loss. Future
works will consider more diverse network architecture and layer-wise data distri-
bution features. To facilitate a smoother accelerator design process, we will extend
DNNBuilder and PyLog to create frameworks and tools for hardware design, synthe-
sis, and workload compilation. Major directions include 1) heterogeneous computing
support, which intends to enable system-level design and optimization for heteroge-
neous AI systems, and 2) dynamic computational graph scheduling, which enables
the generation of runtime adaptive accelerators for future AI applications. Our future
works will also cover more advanced software/hardware co-design for emerging AI
models running on heterogeneous systems, which contains a much larger design
space and is thus more challenging. For example, multi-modal multi-task (MMMT)
models [126] and customized hardware designs [127] working for autonomous driv-
ing have demonstrated the importance of heterogeneity in AI model and hardware
designs. The co-design and co-optimization methods must be developed for such
heterogeneous scenarios.

Acknowledgments The works presented in this book chapter are mainly supported by the IBM-
Illinois Center for Cognitive Computing System Research (C3SR) – a research collaboration as
part of IBM AI Horizons Network, Semiconductor Research Corporation (SRC), the Campus for
Research Excellence and Technological Enterprise (CREATE) program in Singapore, AMD-Xilinx
Center of Excellence, and a Google PhD Fellowship to Xiaofan Zhang. The authors also want to
thank Chao Zhu, Cheng Gong, Chengyue Wang, Hyunmin Jeong, Jinjun Xiong, Junsong Wang,
Kun Wu, Kyle Rupnow, Tao Li, Qiuwen Lou, Wen-mei Hwu, Xinheng Liu, Ye Lu, and Yonghua
Lin for their valuable contributions.

References

1. Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521(7553):436–

444, 2015.

2. Kaiming He, Xiangyu Zhang, Shaoqing Ren, et al. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition
(CVPR), 2016.

3. Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. Attention is all you need. In Advances in

neural information processing systems, 2017.

30

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

4. Stephen Lombardi, Jason Saragih, Tomas Simon, et al. Deep appearance models for face

rendering. ACM Transactions on Graphics, 37(4):1–13, 2018.

5. Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. Advances in neural information processing systems, 2012.
6. David Silver, Julian Schrittwieser, Karen Simonyan, et al. Mastering the game of go without

human knowledge. Nature, 550(7676):354–359, 2017.

7. OpenAI. AI and compute. 2018.
8. Shengkui Zhao, Saima Ahmed, Yun Liang, et al. A real-time 3d sound localization system
with miniature microphone array for virtual reality. In Proceedings of the IEEE Conference
on Industrial Electronics and Applications (ICIEA), 2012.

9. Deming Chen, Jason Cong, Swathi Gurumani, et al. Platform choices and design demands for
iot platforms: cost, power, and performance tradeoﬀs. IET Cyber-Physical Systems: Theory
& Applications, 1(1):70–77, 2016.

10. Norman P Jouppi, Cliﬀ Young, Nishant Patil, et al.

In-datacenter performance analysis
of a tensor processing unit. In Proceedings of the International Symposium on Computer
Architecture (ISCA), 2017.

11. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, et al. Xnor-net: Imagenet classiﬁca-
tion using binary convolutional neural networks. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 525–542, 2016.

12. Junsong Wang, Qiuwen Lou, Xiaofan Zhang, et al. Design ﬂow of accelerating hybrid ex-
tremely low bit-width neural network in embedded FPGA. In Proceedings of the International
Conference on Field Programmable Logic and Applications (FPL), pages 163–1636, 2018.
13. Dibakar Gope, Ganesh Dasika, and Matthew Mattina. Ternary hybrid neural-tree networks for
highly constrained iot applications. In Proceedings of the Conference on Machine Learning
and Systems (MLSys), 2019.

14. Cheng Gong, Yao Chen, Ye Lu, et al. Vecq: Minimal loss dnn model compression with

vectorized weight quantization. IEEE Transactions on Computers (TC), 2020.

15. Yao Chen, Kai Zhang, Cheng Gong, et al. T-DLA: An open-source deep learning accelera-
torfor ternarized DNN models on embedded FPGA. In Proceedings of the IEEE Computer
Society Annual Symposium on VLSI (ISVLSI), 2019.

16. Song Han, Jeﬀ Pool, John Tran, et al. Learning both weights and connections for eﬃcient

neural network. In Advances in neural information processing systems, 2015.

17. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural
In Proceedings of the

networks with pruning, trained quantization and huﬀman coding.
Proceedings of the International Conference on Learning Representations (ICLR), 2016.
18. Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ﬁlter level pruning method for deep
neural network compression. In Proceedings of the International Conference on Computer
Vision (ICCV), 2017.

19. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556, 2014.

20. Christian Szegedy, Wei Liu, Yangqing Jia, et al. Going deeper with convolutions. In Pro-
ceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2015.
21. Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. Nest: A neural network synthesis tool based
on a grow-and-prune paradigm. IEEE Transactions on Computers (TC), 68(10):1487–1497,
2019.

22. Ao Ren, Tianyun Zhang, Shaokai Ye, et al. ADMM-NN: An algorithm-hardware co-design
In Proceedings of
framework of dnns using alternating direction methods of multipliers.
the International Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS), 2019.

23. Xiaohan Ding, Guiguang Ding, Jungong Han, et al. Auto-balanced ﬁlter pruning for eﬃ-
cient convolutional neural networks. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2018.

24. Andrew G Howard, Menglong Zhu, Bo Chen, et al. Mobilenets: Eﬃcient convolutional neural

networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

31

In

25. Manoj Alwani, Han Chen, Michael Ferdman, et al. Fused-layer CNN accelerators.
Proceedings of the International Symposium on Microarchitecture (MICRO), 2016.

26. Bryan Brown. Intel® math kernel library for deep learning networks. 2018.
27. Dustin Franklin. NVIDIA Jetson AGX Xavier delivers 32 tera ops for new era of AI in

robotics. NVIDIA Accelerated Computing| Parallel For all, 2018.

28. Chen Zhang, Peng Li, Guangyu Sun, et al. Optimizing FPGA-based accelerator design for
In Proceedings of the International Symposium on

deep convolutional neural networks.
Field-Programmable Gate Arrays (FPGA), 2015.

29. Jiantao Qiu, Jie Wang, Song Yao, et al. Going deeper with embedded FPGA platform for
In Proceedings of the International Symposium on Field-

convolutional neural network.
Programmable Gate Arrays (FPGA), 2016.

30. Xiaofan Zhang, Junsong Wang, Chao Zhu, et al. DNNBuilder: An automated tool for building
high-performance DNN hardware accelerators for FPGAs. In Proceedings of the International
Conference on Computer-Aided Design (ICCAD), pages 1–8. IEEE, 2018.

31. Yu-Hsin Chen, Tushar Krishna, Joel S Emer, et al. Eyeriss: An energy-eﬃcient reconﬁgurable
accelerator for deep convolutional neural networks. In Proceedings of the International Solid-
State Circuits Conference (ISSCC), 2016.

32. Song Han, Xingyu Liu, Huizi Mao, et al. EIE: Eﬃcient inference engine on compressed deep
neural network. In Proceedings of the International Symposium on Computer Architecture
(ISCA), 2016.

33. Alexandros Papakonstantinou, Karthik Gururaj, John A. Stratton, et al. FCUDA: Enabling
eﬃcient compilation of CUDA kernels onto FPGAs. In Proceedings of the Symposium on
Application Speciﬁc Processors, 2009.

34. Kyle Rupnow, Yun Liang, Yinan Li, et al. A study of high-level synthesis: Promises and

challenges. In Proc. of the International Conference on ASIC, 2011.

35. Xinheng Liu, Yao Chen, Tan Nguyen, et al. High level synthesis of complex applications: An
H. 264 video decoder. In Proceedings of the International Symposium on Field-Programmable
Gate Arrays (FPGA), 2016.

36. Chen Zhang, Guangyu Sun, Zhenman Fang, et al. Caﬀeine: Toward uniformed representation
and acceleration for deep convolutional neural networks. IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems (TCAD), 38(11):2072–2085, 2018.

37. Hanchen Ye, Xiaofan Zhang, Zhize Huang, et al. HybridDNN: A framework for high-
In Proceedings of the

performance hybrid DNN accelerator design and implementation.
Design Automation Conference (DAC), pages 1–6. IEEE, 2020.

38. Sitao Huang, Kun Wu, Hyunmin Jeong, et al. PyLog: An algorithm-centric Python-based
FPGA programming and synthesis ﬂow. IEEE Transactions on Computers (TC), 70(12):2015–
2028, 2021.

39. Yangqing Jia, Evan Shelhamer, Jeﬀ Donahue, et al. Caﬀe: Convolutional architecture for
fast feature embedding. In Proceedings of the ACM international conference on Multimedia,
2014.

40. Martín Abadi, Paul Barham, Jianmin Chen, et al. Tensorﬂow: A system for large-scale
machine learning. In Proceedings of the USENIX symposium on operating systems design
and implementation (OSDI), 2016.

41. Adam Paszke, Sam Gross, Francisco Massa, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems,
2019.

42. Cong Hao, Jordan Dotzel, Jinjun Xiong, et al. Enabling design methodologies and future
trends for edge ai: specialization and codesign. IEEE Design & Test, 38(4):7–26, 2021.
43. Cong Hao and Deming Chen. Deep neural network model and fpga accelerator co-design:
Opportunities and challenges. In 2018 14th IEEE International Conference on Solid-State
and Integrated Circuit Technology (ICSICT), pages 1–4. IEEE, 2018.

44. Xiaofan Zhang, Haoming Lu, Cong Hao, et al. Skynet: a hardware-eﬃcient method for object
detection and tracking on embedded systems. In Proceedings of the Conference on Machine
Learning and Systems (MLSys), pages 216–229, 2020.

32

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

45. Cong Hao, Xiaofan Zhang, Yuhong Li, et al. FPGA/DNN co-design: An eﬃcient design
In Proceedings of the Design Automation

methodology for IoT intelligence on the edge.
Conference (DAC), pages 1–6. IEEE, 2019.

46. Yifan Yang, Qĳing Huang, Bichen Wu, et al. Synetgy: Algorithm-hardware co-design for
convnet accelerators on embedded fpgas. In Proceedings of the International Symposium on
Field-Programmable Gate Arrays (FPGA), 2019.

47. Kaiyuan Guo, Shulin Zeng, Jincheng Yu, et al. A survey of fpga-based neural network infer-
ence accelerators. ACM Transactions on Reconﬁgurable Technology and Systems (TRETS),
12(1):1–26, 2019.

48. Cong Hao, Yao Chen, Xinheng Liu, et al. Nais: Neural architecture and implementation
search and its applications in autonomous driving. arXiv preprint arXiv:1911.07446, 2019.
49. Weiwen Jiang, Lei Yang, Edwin H-M Sha, Qingfeng Zhuge, Shouzhen Gu, Sakyasingha Das-
gupta, Yiyu Shi, and Jingtong Hu. Hardware/software co-exploration of neural architectures.
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD),
2020.

50. Junsong Wang, Xiaofan Zhang, Yubo Li, et al. Exploring HW/SW co-optimizations for ac-
celerating large-scale texture identiﬁcation on distributed GPUs. In Proc. of the International
Conference on Parallel Processing (ICPP), pages 1–10, 2021.

51. Xiaofan Zhang, Yuan Ma, Jinjun Xiong, et al. Exploring HW/SW co-design for video analysis
on CPU-FPGA heterogeneous systems. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems (TCAD), 2021.

52. Yonggan Fu, Yongan Zhang, Chaojian Li, et al. A3C-S: Automated agent accelerator co-search
In Proceedings of the Design Automation

towards eﬃcient deep reinforcement learning.
Conference (DAC), 2021.

53. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.

The Journal of Machine Learning Research, 20(1):1997–2017, 2019.

54. Barret Zoph, Vĳay Vasudevan, Jonathon Shlens, et al. Learning transferable architectures for
scalable image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition (CVPR), pages 8697–8710, 2018.

55. Bichen Wu, Xiaoliang Dai, Peizhao Zhang, et al. Fbnet: Hardware-aware eﬃcient convnet
design via diﬀerentiable neural architecture search. In Proceedings of the IEEE conference
on computer vision and pattern recognition (CVPR), pages 10734–10742, 2019.

56. Mingxing Tan, Bo Chen, Ruoming Pang, et al. Mnasnet: Platform-aware neural architecture
search for mobile. In Proceedings of the IEEE conference on computer vision and pattern
recognition (CVPR), pages 2820–2828, 2019.

57. Weiwen Jiang, Xinyi Zhang, Edwin H-M Sha, et al. Accuracy vs. eﬃciency: Achieving both
through fpga-implementation aware neural architecture search. In Proceedings of the Design
Automation Conference (DAC), pages 1–6, 2019.

58. Yuhong Li, Cong Hao, Xiaofan Zhang, et al. EDD: Eﬃcient diﬀerentiable DNN architec-
ture and implementation co-search for embedded AI solutions. Proceedings of the Design
Automation Conference (DAC), 2020.

59. Lei Yang, Zheyu Yan, Meng Li, et al. Co-exploration of neural architectures and heterogeneous
asic accelerator designs targeting multiple tasks. Proceedings of the Design Automation
Conference (DAC), 2020.

60. Xiaolong Ma, Fu-Ming Guo, Wei Niu, et al. Pconv: The missing but desirable sparsity in dnn
weight pruning for real-time execution on mobile devices. In AAAI, pages 5117–5124, 2020.
61. Wei Niu, Xiaolong Ma, Sheng Lin, et al. Patdnn: Achieving real-time dnn execution on mobile
devices with pattern-based weight pruning. In Proceedings of the International Conference
on Architectural Support for Programming Languages and Operating Systems (ASPLOS),
pages 907–922, 2020.

62. Ji Lin, Wei-Ming Chen, John Cohn, et al. MCUNet: Tiny deep learning on IoT devices. In

Advances in neural information processing systems, 2020.

63. PULP - An Open Parallel Ultra-Low-Power Processing-Platform. http://iis-projects.

ee.ethz.ch/index.php/PULP.

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

33

64. Angelo Garofalo, Giuseppe Tagliavini, Francesco Conti, et al. Xpulpnn: accelerating quan-
In Proceedings of the
tized neural networks on risc-v processors through isa extensions.
Design, Automation & Test in Euro. Conf. & Exhibition (DATE), pages 186–191. IEEE,
2020.

65. Angelo Garofalo, Manuele Rusci, Francesco Conti, et al. Pulp-nn: accelerating quantized
neural networks on parallel ultra-low-power risc-v processors. Philosophical Transactions of
the Royal Society A, 378(2164):20190155, 2020.

66. Fengfu Li, Bo Zhang, and Bin Liu.

Ternary weight networks.

arXiv preprint

arXiv:1605.04711, 2016.

67. Shuchang Zhou, Yuxin Wu, Zekun Ni, et al. Dorefa-net: Training low bitwidth convolutional
neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.
68. Ritchie Zhao, Weinan Song, Wentao Zhang, et al. Accelerating binarized convolutional
neural networks with software-programmable FPGAs. In Proceedings of the International
Symposium on Field-Programmable Gate Arrays (FPGA), pages 15–24, 2017.

69. Yaman Umuroglu, Nicholas J Fraser, Giulio Gambardella, et al. Finn: A framework for fast,
scalable binarized neural network inference. In Proceedings of the International Symposium
on Field-Programmable Gate Arrays (FPGA). ACM, 2017.

70. Eriko Nurvitadhi, David Sheﬃeld, Jaewoong Sim, et al. Accelerating binarized neural net-
works: Comparison of FPGA, CPU, GPU, and ASIC. International Conference on Field-
Programmable Technology (FPT), 2016.

71. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Bina-
rized neural networks. In Advances in neural information processing systems, pages 4107–
4115, 2016.

72. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep
neural networks with binary weights during propagations. In Advances in neural information
processing systems, pages 3123–3131, 2015.

73. Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, et al. Neural networks with few mul-
tiplications. In Proceedings of the Proceedings of the International Conference on Learning
Representations (ICLR), 2016.

74. Canran Jin, Heming Sun, and Shinji Kimura. Sparse ternary connect: Convolutional neural
networks using ternarized weights with enhanced sparsity. In Proceedings of the Asia and
South Paciﬁc Design Automation Conference (ASP-DAC), pages 190–195, 2018.

75. Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training
In Proceedings of the International Conference on

by reducing internal covariate shift.
Machine Learning (ICML), pages 448–456, 2015.

76. Chenzhuo Zhu, Song Han, Huizi Mao, et al. Trained ternary quantization. In Proceedings of
the Proceedings of the International Conference on Learning Representations (ICLR), 2017.
77. Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. Two-
step quantization for low-bit neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition (CVPR), pages 4376–4384, 2018.

78. Aojun Zhou, Anbang Yao, Yiwen Guo, et al. Incremental network quantization: Towards

lossless cnns with low-precision weights. 2017.

79. Cong Leng, Zesheng Dou, Hao Li, et al. Extremely low bit neural network: Squeeze the last
bit out with admm. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI),
2018.

80. Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the

IEEE conference on computer vision and pattern recognition (CVPR), 2017.

81. Qingcheng Xiao, Yun Liang, Liqiang Lu, et al. Exploring heterogeneous algorithms for
accelerating deep convolutional neural networks on FPGAs. In Proceedings of the Design
Automation Conference (DAC), 2017.

82. Dustin Franklin. NVIDIA Jetson TX2 delivers twice the intelligence to the edge. NVIDIA

Accelerated Computing| Parallel For all, 2017.

83. Alexandros Papakonstantinou, Yun Liang, John A Stratton, et al. Multilevel granularity
parallelism synthesis on FPGAs. In Proceedings of the International Symposium on Field-
Programmable Custom Computing Machines (FCCM), pages 178–185. IEEE, 2011.

34

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

84. Swathi T. Gurumani, Jacob Tolar, Yao Chen, et al. Integrated CUDA-to-FPGA synthesis with
Network-on-Chip. In Proceedings of the International Symposium on Field-Programmable
Custom Computing Machines (FCCM), pages 21–24, 2014.

85. Yao Chen, Swathi T. Gurumani, Yun Liang, et al. FCUDA-NoC: A scalable and eﬃcient
network-on-chip implementation for the CUDA-to-FPGA ﬂow. IEEE Transactions on Very
Large Scale Integration (VLSI) Systems, 24(6):2220–2233, 2016.

86. Tan Nguyen, Swathi Gurumani, Kyle Rupnow, et al. FCUDA-SoC: Platform integration for
ﬁeld-programmable soc with the CUDA-to-FPGA compiler. In Proceedings of the Interna-
tional Symposium on Field-Programmable Gate Arrays (FPGA), page 5–14, 2016.

87. Ying Chen, Tan Nguyen, Yao Chen, et al. FCUDA-HB: Hierarchical and scalable bus
architecture generation on FPGAs with the FCUDA ﬂow. IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems (TCAD), 35(12):2032–2045, 2016.

88. Jason Cong, Hui Huang, and Wei Jiang. A generalized control-ﬂow-aware pattern recognition
algorithm for behavioral synthesis. In Proceedings of the Design, Automation & Test in Euro.
Conf. & Exhibition (DATE), pages 1255–1260, 2010.

89. Jason Cong, Bin Liu, Stephen Neuendorﬀer, et al. High-level synthesis for FPGAs: From
IEEE Transactions on Computer-Aided Design of Integrated

prototyping to deployment.
Circuits and Systems (TCAD), 30(4):473–491, 2011.

90. Zhiru Zhang, Yiping Fan, Wei Jiang, et al. AutoPilot: A Platform-Based ESL Synthesis

System, pages 99–112. Springer Netherlands, 2008.

91. Jason Cong, Yiping Fan, Guoling Han, et al. Platform-based behavior-level and system-level

synthesis. In 2006 IEEE International SOC Conference, pages 199–202, 2006.

92. Andrew Canis, Jongsok Choi, Mark Aldham, et al. LegUp: High-level synthesis for FPGA-
In Proceedings of the International Symposium on

based processor/accelerator systems.
Field-Programmable Gate Arrays (FPGA), page 33–36, 2011.

93. Andrew Canis, Jongsok Choi, Mark Aldham, et al. LegUp: An open-source high-level syn-
thesis tool for FPGA-based processor/accelerator systems. ACM Transactions on Embedded
Computing Systems, 13(2), sep 2013.

94. Hanchen Ye, Cong Hao, Jianyi Cheng, et al. Scalehls: A new scalable high-level synthesis
framework on multi-level intermediate representation. In Proceedings of the International
Symposium on High Performance Computer Architecture (HPCA), pages 741–755, 2022.
95. Deming Chen, Jason Cong, Yiping Fan, et al. LOPASS: A low-power architectural synthesis
system for FPGAs with interconnect estimation and optimization. IEEE Transactions on Very
Large Scale Integration (VLSI) Systems, 18(4):564–577, 2009.

96. Deming Chen, Jason Cong, and Junjuan Xu. Optimal module and voltage assignment for
In Proceedings of the Asia and South Paciﬁc Design Automation Conference

low-power.
(ASP-DAC), volume 2, pages 850–855, 2005.

97. Deming Chen, Jason Cong, and Junjuan Xu. Optimal simultaneous module and multivoltage
assignment for low power. ACM Transactions on Design Automation of Electronic Systems,
11(2):362–386, 2006.

98. Vitis HLS.

https://www.xilinx.com/support/documentation-navigation/

design-hubs/dh0090-vitis-hls-hub.html.

99. Siemens High-Level Synthesis & Veriﬁcation. https://eda.sw.siemens.com/en-US/

ic/ic-design/high-level-synthesis-and-verification-platform/.

100. PYNQ. http://www.pynq.io/.
101. Yi-Hsiang Lai, Yuze Chi, Yuwei Hu, et al. HeteroCL: A multi-paradigm programming infras-
tructure for software-deﬁned reconﬁgurable computing. In Proceedings of the International
Symposium on Field-Programmable Gate Arrays (FPGA), pages 242–251, 2019.

102. Scott Grauer-Gray, Lifan Xu, Robert Searles, et al. Auto-tuning a high-level language targeted

to gpu codes. In 2012 Innovative Parallel Computing (InPar), pages 1–10, 2012.

103. Ryan Kastner, Janarbek Matai, and Stephen Neuendorﬀer. Parallel programming for fpgas,

2018.

104. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Diﬀerentiable architecture search.

arXiv preprint arXiv:1806.09055, 2018.

Compilation and Optimizations for Eﬃcient Machine Learning on Embedded Systems

35

105. Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on

target task and hardware. arXiv preprint arXiv:1812.00332, 2018.

106. Shikhar Tuli, Bhishma Dedhia, Shreshth Tuli, et al. Flexibert: Are current transformer

architectures too homogeneous and rigid? arXiv preprint arXiv:2205.11656, 2022.

107. Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture

search. In Uncertainty in artiﬁcial intelligence, pages 367–377. PMLR, 2020.

108. Sirui Xie, Hehui Zheng, Chunxiao Liu, et al. Snas: stochastic neural architecture search.

arXiv preprint arXiv:1812.09926, 2018.

109. Han Cai, Chuang Gan, Tianzhe Wang, et al. Once-for-all: Train one network and specialize

it for eﬃcient deployment. arXiv preprint arXiv:1908.09791, 2019.

110. Renqian Luo, Fei Tian, Tao Qin, et al. Neural architecture optimization. Advances in neural

information processing systems, 31, 2018.

111. Joe Mellor, Jack Turner, Amos Storkey, et al. Neural architecture search without training. In
Proceedings of the International Conference on Machine Learning (ICML), pages 7588–7598.
PMLR, 2021.

112. Mohamed S Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, et al. Zero-cost proxies for

lightweight nas. arXiv preprint arXiv:2101.08134, 2021.

113. Yuhong Li, Cong Hao, Pan Li, et al. Generic neural architecture search via regression.

Advances in Neural Information Processing Systems, 34, 2021.

114. George Kyriakides and Konstantinos Margaritis. An introduction to neural architecture search

for convolutional networks. arXiv preprint arXiv:2005.11074, 2020.

115. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv

preprint arXiv:1611.01578, 2016.

116. Bowen Baker, Otkrist Gupta, Nikhil Naik, et al. Designing neural network architectures using

reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.

117. Yiyang Zhao, Linnan Wang, Yuandong Tian, et al. Few-shot neural architecture search. In
Proceedings of the International Conference on Machine Learning (ICML), pages 12707–
12718. PMLR, 2021.

118. Hongyang Li, David Eigen, Samuel Dodge, et al. Finding task-relevant features for few-shot
learning by category traversal. In Proceedings of the IEEE conference on computer vision
and pattern recognition (CVPR), pages 1–10, 2019.

119. Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, et al. A comprehensive
survey on hardware-aware neural architecture search. arXiv preprint arXiv:2101.09336,
2021.

120. Xiaowei Xu, Xinyi Zhang, Bei Yu, et al. Dac-sdc low power object detection challenge for uav
applications. IEEE transactions on pattern analysis and machine intelligence, 43(2):392–403,
2019.

121. Dimitrios Stamoulis, Ruizhou Ding, Di Wang, et al. Single-path nas: Device-aware eﬃcient

convnet design. arXiv preprint arXiv:1905.04159, 2019.

122. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.

arXiv preprint arXiv:1611.01144, 2016.

123. CHaiDNN. https://github.com/Xilinx/CHaiDNN.
124. Mark Sandler, Andrew Howard, Menglong Zhu, et al. Mobilenetv2: Inverted residuals and
linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern
recognition (CVPR), pages 4510–4520, 2018.

125. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, et al. Shuﬄenet v2: Practical guidelines for
eﬃcient cnn architecture design. In Proceedings of the European Conference on Computer
Vision (ECCV), pages 116–131, 2018.

126. Cong Hao and Deming Chen. Software/hardware co-design for multi-modal multi-task
learning in autonomous systems. In Proceedings of the International Conference on Artiﬁcial
Intelligence Circuits and Systems (AICAS), pages 1–5. IEEE, 2021.

127. Emil Talpes, Debjit Das Sarma, Ganesh Venkataramanan, et al. Compute solution for tesla’s

full self-driving computer. IEEE Micro, 40(2):25–35, 2020.

