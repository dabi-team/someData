2
2
0
2

g
u
A
6
2

]

G
L
.
s
c
[

2
v
6
2
3
3
0
.
6
0
2
2
:
v
i
X
r
a

Compilation and Optimizations for Eï¬ƒcient
Machine Learning on Embedded Systems

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Abstract Deep Neural Networks (DNNs) have achieved great success in a variety of
machine learning (ML) applications, delivering high-quality inferencing solutions
in computer vision, natural language processing, and virtual reality, etc. However,
DNN-based ML applications also bring much increased computational and storage
requirements, which are particularly challenging for embedded systems with limited
compute/storage resources, tight power budgets, and small form factors. Challenges
also come from the diverse application-speciï¬c requirements, including real-time re-
sponses, high-throughput performance, and reliable inference accuracy. To address
these challenges, we introduce a series of eï¬€ective design methodologies, including
eï¬ƒcient ML model designs, customized hardware accelerator designs, and hard-
ware/software co-design strategies to enable eï¬ƒcient ML applications on embedded
systems.

Keywords: Deep Neural Networks, machine learning, embedded systems, eï¬ƒcient
ML model, hardware accelerator, compilation, optimization, hardware/software co-
design

X. Zhang Â· Y. Li Â· D. Chen
Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Champaign, IL,
USA, e-mail: xiaofan3@illinois.edu;leeyh@illinois.edu;dchen@illinois.edu

Y. Chen
Advanced Digital Sciences Center, Singapore e-mail: yao.chen@adsc-create.edu.sg

C. Hao
Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA e-mail:
callie.hao@gatech.edu

S. Huang
Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA
e-mail: sitaoh@uci.edu

1

 
 
 
 
 
 
2

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

1 Introduction

The recent development of Deep Neural Networks (DNNs) has made machine learn-
ing based smart solutions more relevant and accessible to the general public. We
have seen that some DNN technologies have been integrated into our daily applica-
tions to provide high-quality inference services, such as image recognition, natural
language processing, self-driving cars, and augmented and virtual reality [1, 2, 3, 4],
which have made our lives more convenient and our work more eï¬ƒcient. A signiï¬-
cant number of these machine learning applications leverage edge devices and need
to be deployed onto resource-constrained embedded systems, such as cell phones,
cameras, and unmanned aerial vehicles (UAVs). They require not only higher infer-
ence accuracy to achieve intelligent responses but also aggressive inference speed,
throughput, and energy eï¬ƒciency to meet real-life demands.

As DNNs become more complicated, developing and serving the DNN-enabled
applications requires more compute and memory resources, longer latency, and
greater energy consumption. For example, the computation demands for DNN train-
ing have risen by over 300,000 times between AlexNet [5], the champion model of
the 2012 ImageNet competition, and the AlphaGo Zero [6], the AI player proposed
in 2017 for the board game Go with superhuman skills [7]. By checking the image
recognition models, there is a 16 times increase in model complexity from AlexNet
with 85% top-5 accuracy to ResNet-152 [2] with 95% top-5 accuracy.

Such exponentially increasing compute and memory demands have created chal-
lenges and diï¬ƒculties for DNN deployment on hardware, especially when targeting
edge embedded devices with strictly limited compute and memory resources and
tight power budgets [8, 9]. Although cloud computing can alleviate the burden of
edge computing by taking over computationally intensive tasks, it is not always
feasible when dealing with various real-life scenarios. Primary reasons for sticking
to edge embedded devices come from the unique requirements of the edge appli-
cations, which typically require real-time decision-making and reduced reliance on
network communication and accessibility. They typically can not tolerate the extra
latency caused by network data transfer due to the real-time response requirements.
In addition, private information, such as personal and sensitive data, should not be
uploaded to the cloud without permission. It means that the edge devices are required
to deliver not only high inference accuracy from DNNs, but also aggressive inference
speed, throughput, and energy eï¬ƒciency to meet various real-life demands. In sum-
mary, the challenges of deploying machine learning workloads on edge embedded
devices mainly come from three aspects: 1) DNN models are getting complicated
and may fail to run eï¬ƒciently, especially when targeting the low-power edge de-
vices with scarce compute and memory resources; 2) Mapping DNN onto existing
hardware or building domain-speciï¬c hardware is tedious and time-consuming; 3)
Additional challenges come from ineï¬ƒcient optimization strategies that focus only
on hardware or software optimizations alone but lack software/hardware co-design
or cross-system stack design methods that can potentially deliver better overall so-
lutions.

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

3

Despite the aforementioned challenges, there has been continuous progress in
recent studies to explore various optimization strategies for edge machine learning
solutions. In this chapter, we present comprehensive design methodologies to face
and overcome the challenges and enable eï¬ƒcient DNN applications on embedded
systems. These methods include eï¬ƒcient DNN model designs in Sec. 3, accelerator
design and workload mapping technologies in Sec. 4, and cross-stack optimization
strategies in Sec. 5.

2 Background and Related Works

Existing solutions to enable eï¬ƒcient DNN on embedded systems attempt to address
challenges from the DNN model to the entire hardware-software system. These dif-
ferent methods cover diï¬€erent development cycles and have diï¬€erent characteristics,
as shown in Table 1. In this section, we present existing work on the diï¬€erent design
methods in terms of their diï¬€erent properties.

Table 1 Design methodologies and their attributes.

Methods

Eï¬ƒcient DNN model design

Eï¬ƒcient accelerator design and DNN mapping

Eï¬ƒcient DNN/accelerator co-design

Attributes
Design methods to create DNN models with less
parameters, less memory demands and less
computational complexity
Solutions to build domain speciï¬c
hardware/software accelerators with optimized
task scheduling
Optimization strategies that integrate both the
hardware design process and DNN algorithm
design process

2.1 Eï¬ƒcient DNN designs

A DNN includes multiple intermediate layers between the input and output lay-
ers, and each intermediate layer consists of artiï¬cial neurons for transforming the
input information (e.g., input feature maps) following the predeï¬ned network con-
nection. In general, a DNN contains millions of parameters and requires billions
of operations during inference. To successfully deploy DNNs onto hardware with
desired performance, developers focus on network compression to reduce network
complexities and lower the compute and memory demands. Recent research has
demonstrated the possibility of using quantized data to represent original ï¬‚oating-
point parameters, such as using 8-bit quantization or even binary and ternary data
representation [10, 11, 12, 13, 14, 15]. These solutions are intended to replace the

4

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

hardware-intensive ï¬‚oating-point multiplications by logical operations so that DNNs
can be more eï¬ƒcient on hardware platforms.

Another method to compress DNN is network pruning, which aims to reduce
the redundancy of DNN structures [16, 17, 18]. According to the published pruning
strategies, the less essential connections between DNN layers are discarded, and
network retraining is then performed to regain accuracy. Signiï¬cant reductions can
be achieved on the classic DNNs, such as AlexNet [5] and VGG-16 [19]. Since the
major beneï¬t of network compression comes from the fully-connected (FC) layers,
to continuously have eï¬€ective pruning results for latter DNNs (e.g., GoogleNet [20]
and ResNet [2]) with fewer FC layers, more sophisticated algorithms are required
to achieve eï¬€ective network pruning, such as using evolutionary algorithms [21],
alternating direction method of multipliers [22], and iterative pruning [23].

As most of the computations happen inside the convolutional (Conv) layers, previ-
ous works also attempt to reduce the computational complexity by using depth-wise
separable Conv layers [24]. The depth-wise separable structure can eï¬€ectively re-
duce the number of operations and provide more compact DNN designs for resource-
constrained hardware. To further improve the DNN deployment on hardware, layer
fusion is proposed in [25] to minimize data movements between on-chip and oï¬€-chip
memory.

2.2 Eï¬ƒcient accelerator designs and DNN mapping methods

Building domain-speciï¬c hardware accelerators is another popular approach for eï¬ƒ-
cient DNN deployment. These accelerators attempt to take advantage of customized
or specialized hardware and software designs, such as adopting acceleration libraries
on CPUs [26], exploring kernel optimization on GPUs [27], and building customized
accelerators on FPGAs [28, 29, 30] and ASICs [31, 32, 10] to improve the speed
and eï¬ƒciency of DNN inference and training processes. Among these accelerator
designs, FPGA- and ASIC-based designs can be fully customized to implement the
neural network functionality with improved latency, throughput, and energy con-
sumption compared to CPU- and GPU-based designs.

Still, developing customized accelerators present signiï¬cant challenges, such as
the tedious hardware design process, the intricate hardware veriï¬cation problems,
and the time-consuming design space exploration during DNN deployment. To
alleviate these challenges, recent investigations have started focusing on techniques
including high-level synthesis [33, 34, 35] and end-to-end design frameworks for fast
DNN accelerator design and eï¬ƒcient workload deployment [30, 36, 37, 38]. They
support high abstraction inputs, such as Python-based DNN descriptions used by
popular machine learning frameworks (e.g., Caï¬€e [39], TensorFlow [40], PyTorch
[41]), so DNNs can be directly imported without manual code conversions and be
parsed and then mapped onto hardware. These frameworks, such as DNNBuilder
[30] and HybridDNN [37] also integrate design space exploration (DSE) engines

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

5

to perform eï¬€ective and systematical explorations and deliver highly optimized
accelerators to meet the user-speciï¬c requirements.

2.3 Eï¬ƒcient co-design optimization

Recent research also focuses on cross-stack co-design optimizations to enable suc-
cessful DNN deployment on embedded systems [42]. Instead of independently opti-
mizing hardware and software components, researchers proposed algorithm/acceler-
ator co-design and co-search to solve the edge AI challenges: DNNs are designed to
satisfy accuracy demands and must be aware of the hardware constraints with rational
network conï¬gurations. At the same time, the accelerators need to provide extensive
support for diï¬€erent DNN components without introducing too many restrictions on
network design and guarantee performance to meet the speciï¬cations. The authors
in [43] proposed the concept of DNN/accelerator co-design for the ï¬rst time, which
aims to consider software and hardware metrics simultaneously to â€œautomatically
generate both DNN models and their corresponding implementations as pairs". This
concept is then demonstrated by winning the competitive System Design Contest for
low power object detection in the 56th IEEE/ACM Design Automation Conference
(DAC-SDC) [44].

Many follow-up works continued investigating the co-design opportunities be-
tween diï¬€erent AI algorithms and hardware devices [45, 46, 47, 48, 49, 50, 51, 52].
These co-design approaches have been studied with remarkable achievements by
combining multiple optimization techniques across both hardware and software.
For example, while neural architecture search (NAS) has been largely successful
in designing high-quality DNN models [53, 54], hardware-aware NAS is drawing
increasing attention, which aims at delivering high-accuracy models with hardware
eï¬ƒciency as well (e.g., FBNet [55] and MNasNet [56]). Other machine-learning
algorithm/hardware co-design works include FNAS [57], NAIS [48], EDD [58],
and NASAIC [59]. Driven by the success of such a co-design strategy, other types
of co-design methods are also proposed recently, including software/compiler co-
design [60, 61, 62], compiler/hardware co-design [63, 64, 65], etc.

3 Eï¬ƒcient Machine Learning Model Designs

Machine learning applications require not only high inference accuracy but also
aggressive inference speed, throughput, and energy eï¬ƒciency to meet real-life de-
mands. They rely on hardware-eï¬ƒcient DNN designs, especially when targeting edge
scenarios with limited hardware resources. In this section, we introduce ELB-NN
[12] and VecQ [14] to deliver hardware-eï¬ƒcient DNNs for embedded systems.

6

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

3.1 The ELB-NN

ELB-NN (Extremely Low Bit-width Neural Network) is proposed to enhance energy
eï¬ƒciency when running image classiï¬cation on an embedded FPGA. It is one of
the ï¬rst hybrid low-bit-width designs that supports arbitrary DNN quantization.
This subsection presents the hybrid quantization feature of the ELB-NN and its
corresponding hardware accelerator design on embedded systems.

3.1.1 Hybrid quantization scheme

Hybrid quantization means that diï¬€erent quantization schemes are involved for the
networkâ€™s parameters and activations. The quantization scheme can go all the way
down to binary. To better adapt the hybrid quantization, we ï¬rst investigate their
impacts on the network inference accuracy. We follow Eq. 1 to calculate the binary
weights. Here Ëœğ‘¤ represents the full precision weights after back propagation, while
ğ¸ (| Ëœğ‘¤|) represents the mean of all the full-precision weights as a scaling factor. For the
ternary training, the ğ‘¤ğ‘¡ (representing ternary parameters) can be calculated following
Eq. 2. Here we set the threshold ğ‘¤ğ‘¡ â„ğ‘Ÿ ğ‘’ğ‘  = 0.7ğ¸ (| Ëœğ‘¤|) and calculate the scaling factor
ğ¸ as suggested in [66]. We also apply relatively high precision using 8- or 4-bit
ï¬xed-point representation. We then use AlexNet [5] to perform quantitative analysis
when applying hybrid quantization.

ğ‘¤ğ‘ = ğ‘ ğ‘–ğ‘”ğ‘›(| Ëœğ‘¤|) Ã— E(| Ëœğ‘¤|)
(cid:40)ğ‘ ğ‘–ğ‘”ğ‘›( Ëœğ‘¤)Ã—E |ğ‘¤ğ‘¡ | > ğ‘¤ğ‘¡ â„ğ‘Ÿ ğ‘’ğ‘ 
|ğ‘¤ğ‘¡ | â‰¤ ğ‘¤ğ‘¡ â„ğ‘Ÿ ğ‘’ğ‘ 
0

ğ‘¤ğ‘¡ =

(1)

(2)

Fig. 1 Network representation when using hybrid quantization [12]

In this analysis with AlexNet, we focus on the impact of 1) the quantized pa-
rameters of the convolutional (CONV) and fully-connected (FC) layer, and 2) the
quantized activations. We use mid-CONV to denote all the CONV layers except the
ï¬rst CONV layer and mid-FC to denote all the FC layers except the last FC layer.
The naming rule of the proposed hybrid precision network can be referred to Fig. 1.
In Table 2, the 8-bit design (Alexnet-8-8888) only reduces the accuracy by 1.3%
compared to the original ï¬‚oat32 version. The accuracy is still promising after using
ternary (Alexnet-8-8228) and binary (Alexnet-8-8218) parameters for mid-CONV

Alexnet-4-8218network typeActivation bitwidthfirst-CONV weights bitwidthmid-CONV weights bitwidthmid-FC weights  bitwidthLast-FC weights bitwidthCompilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

7

Table 2 Inference accuracy with hybrid quantization using ImageNet dataset [12].

Network precision
Alexnet with ï¬‚oat32
Alexnet-8-8888
Alexnet-8-8228
Alexnet-8-8218
Alexnet-8-8118
Alexnet-4-8218
Alexnet-2-8218
Alexnet-4-8218 (w/o g.)
Alexnet-4-8218 (ext.)

Accuracy (Top-1)
55.9%[67]
54.6%
53.3%
52.6%
51.1%
49.3%
46.1%
53.2%
54.5%

and mid-FC layer. It means that the network is relatively robust to the precision
of parameters. On the contrary, the precision of activations signiï¬cantly impacts
classiï¬cation accuracy. Compared to the Alexnet-8-8218, we observe 3.3% and 6.5%
accuracy drop when activations move to 4 bits (Alexnet-4-8218) and 2 bits (Alexnet-
2-8218). To further investigate, we disable the group function, which was originally
proposed to handle the limited GPU memory issue. As a result, we capture an 80%
computation increase and a 3.9% accuracy improvement in Alexnet-4-8218 (w/o g.).
We further increase the channel number for the ï¬ve CONV layers in Alexnet-4-8218
(ext.) and achieve 1.3% accuracy gain by aï¬€ording extra 61% computation compared
to Alexnet-4-8218 (w/o g.). By increasing the model complexity, we can bring back
the accuracy. These observations are insightful for hybrid DNN quantization as
parameters can be quantized more aggressively than activations.

3.1.2 Hardware accelerator for ELB-NN

To handle ELB-NN, we propose a parameterized Computation Engine (CE) in [12]
with ï¬‚exible support of low bit-width quantization and conï¬gurable parallelism
during execution. As shown in Fig. 2, it contains a four-input-parallel CE as an ex-
ample, where four inputs can be processed simultaneously (including binary/ternary
operations and accumulation) and then followed by batch normalization (BN) and
activation function. The precision of the accumulator is adjustable, which is intended
to allow more ï¬‚exible quantization designs and maintain the output accuracy. For a
larger number of inputs, an adder tree will be used before the accumulator for timing
enclosure.

To demonstrate the hardware eï¬ƒciency of ELB-NN, we adopt the accelerator
with the proposed CE and accelerate diï¬€erent quantized versions of the AlexNet and
VGG16 using an embedded platform called ZC706 (with an ARM CPU and a Xilinx
Kintex-7 FPGA). Results are shown in Table 3. ELB-NN can achieve throughput
performance up to 10.3 TOPS, which outperforms previous designs in [68, 69, 70].

8

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 2 Computation engine (CE) with binary and ternary logic operations [12].

Table 3 ELB-NN performance evaluated on an embedded platform (Xilinx ZC706) [12].

Network

LUT

Utilization
FF

BRAM

DSP

Alexnet-8-8888
Alexnet-8-8218
Alexnet-4-8218

86262(39%) 51387(12%) 303(56%) 808(90%)
103505(47%) 90125(21%) 498(91%) 550(61%)
105673(48%) 94149(22%) 463(85%) 880(98%)
Alexnet-4-8218 (w/o g.) 127393(58%) 105328(24%) 435(80%) 839(93%)
Alexnet-4-8218 (ext.) 124317(57%) 101558(23%) 481(88%) 783(87%)
112992(52%) 99396(23%) 509(93%) 298(33%)
137973(63%) 113262(26%) 499(92%) 651(72%)

VGG16-4-8218
VGG16-2-8118

Batch
size
2
5
8
7
7

Bandwidth
(GBytes/s)
10.8
3.35
3.35
4.30
3.4

Complexity
(GOP)
1.45
1.45
1.45
2.61
4.22

2
3

5.85
6.67

31.0
31.0

imges/s

340
856.1
1369.6
1198.5
599.2

110.7
332.2

Perf.
(TOPS)
0.493
1.24
1.99
2.59
2.53

3.43
10.3

3.2 The VecQ

Vectorized Quantization (VecQ) is a training quantization framework that is based
on a novel quantization loss measurement metric called vector loss. It is proposed to
provide ï¬‚exible bitwidth support with minimal quantization loss to achieve higher
model accuracy. In this subsection, we present the detailed deï¬nition of vector loss
and the VecQ quantization framework.

3.2.1 Quantization with Vector Loss

We use the square of the Euclidean distance of the data before and after quantiza-
tion to represent quantization loss. It is also called Square 2-norm or L2 distance.
Minimizing the L2 loss during the model training is proved to be adequate in pro-
viding higher model accuracy [71, 72, 73, 67, 74]. However, due to the non-convex
characteristic of optimization for L2 loss under a constrained bitwidth requirement,
the quantization easily falls into sub-optimal solution space. In addition, adopting
the L2 distance collects the loss of each quantized data individually and neglects the
distribution and correlations among these data points in a kernel or a layer.

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

9

Focusing on the limitations above, in VecQ, we ï¬rst ï¬‚atten and reshape the
weight set Wf (ğ‘™) for a layer of the DNN and reshape them as a vector ğ’˜ğ’‡ (ğ‘™) with
the dimension of the size of the elements. For example, it will be a ğ‘ Ã— ğ‘€ Ã— ğ¾ 2-
dimensional vector for a CNN layer with ğ‘ input channel, ğ‘€ output channel and ğ¾
size of ï¬lter kernel. The quantized weight vector is denoted as ğ’˜ğ’’ (ğ‘™).

Fig. 3 The attributes of a vector and the quantization angle of ğ‘¤ ğ‘“ and ğ‘¤ğ‘.

There are two attributes of a vector, as shown in Fig. 3 orientation and modulus. We
deï¬ne a quantization angle representing the intersection angle between the original
weight vector and the quantized vector. So, the vector distance between the weight
vector before and after quantization is determined by the quantization angle and the
vectorâ€™s modulus. We then deï¬ne a vector loss, denoted as ğ½ğ‘£, and compose it with
the orientation loss ğ½ğ‘œ and the modulus loss ğ½ğ‘š.

ğ½ğ‘£ = ğ½ (ğ’˜ ğ‘“ , ğ’˜ğ‘) = ğ½ğ‘œ (ğ’˜ ğ‘“ , ğ’˜ğ‘) + ğ½ğ‘š(ğ’˜ ğ‘“ , ğ’˜ğ‘)

(3)

Where ğ½ğ‘œ and ğ½ğ‘š are computed as:

ğ½ğ‘œ = 1 âˆ’ cos ğœƒ, (cos ğœƒ =

ğ›¼ğ’˜ğ‘
|ğ›¼ğ’˜ğ‘ |

ğ’˜ ğ‘“
|ğ’˜ ğ‘“ |

)

= 1 âˆ’ ğ’†ğ’— ğ’†ğ’˜ ğ’‡

ğ‘‘
âˆ‘ï¸

= 1 âˆ’

(ğ‘’ğ‘£ğ‘– ğ‘’ğ‘¤ ğ‘“ ğ‘– )

ğ‘–=1
ğ½ğ‘š = ||ğ’˜ ğ‘“ âˆ’ ğ›¼ğ’˜ğ‘ ||2
2

(4)

here, the ğ’†ğ’— and ğ’†ğ’˜ ğ’‡ represent the unit vector for ğ’— and ğ’˜ğ’‡ . ğ’˜ğ’‡ is a weight vector of
a layer of a DNN containing ğ‘‘ weights.

With these approaches, the orientation loss ğ½ğ‘œ indicates the optimized quantization
angle and the modulus loss ğ½ğ‘š indicates the optimized scale at this angle. Therefore,
our quantization takes two stages to minimize the two losses independently, which
are deï¬ned as steering stage and driving stage as shown in Fig. 4. In the steering
stage, we adjust the orientation of the weight vector to minimize the orientation loss.
Then, we ï¬x the orientation and only scale the modulus of the vector at the driving
stage to minimize the modulus loss.

10

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 4 The overall ï¬‚ow of quantization process, including both steering and driving stage [14].

3.2.2 Framework integration

Fig. 5 Integrated quantization process in DNN training [14].

The VecQ quantization is integrated into the DNN training ï¬‚ow for both the
weight data and the activation data. As shown in Fig. 5. For weight data, taking a
layer ğ‘™ as an example, during the forward propagation, the weights ğ‘¤ ğ‘“ (ğ‘™) represented
in ï¬‚oating-point is quantized into ğ‘¤ğ‘ (ğ‘™), then use the quantized weights to compute
the output of this layer. To simplify the computing process, the weight is treated as
normally distributed and an interval ğœ† is used regarding the given bitwidth constraint.

ğœƒğ›¼Find the optimal orientation vectorFind the optimal scaling factorConvlotion Full connection SoftmaxConvlotion Full connection Convlotion Full connection ConvVecQClipClipÎ» k BitsÎ» Quantized weightsUniformly quantizing weights to discrete valuesInputOutputğœ”ğ‘“(cid:4666)ğ‘™(cid:4667) ğœ”ğ‘(cid:4666)ğ‘™(cid:4667) Weights distributionActivation()VecQğ‘”(cid:4666)ğ‘™(cid:4667) Layer ğ‘™ forward process ğ´(cid:4666)ğ‘™(cid:3398)1(cid:4667) ğ´(cid:4666)ğ‘™(cid:4667) Convlotion Full connection SoftmaxConvlotion Full connection Convlotion Full connection ConvVecQClipClipÎ» k BitsÎ» Quantized weightsUniformly quantizing weights to discrete valuesInputOutputğœ”ğ‘“(cid:4666)ğ‘™(cid:4667) ğœ”ğ‘(cid:4666)ğ‘™(cid:4667) Weights distributionActivation()VecQğ‘”(cid:4666)ğ‘™(cid:4667) Layer ğ‘™ forward process ğ´(cid:4666)ğ‘™(cid:3398)1(cid:4667) ğ´(cid:4666)ğ‘™(cid:4667) Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

11

During the backward propagation, the gradient is calculated with ğ‘¤ğ‘ (ğ‘™) instead of
ğ‘¤ ğ‘“ (ğ‘™) and propagated. In the ï¬nal update process, the gradient ğ‘”(ğ‘™) of ğ‘¤ğ‘ (ğ‘™) is
updated to ğ‘¤ ğ‘“ (ğ‘™) [67].

For the activation output of a layer, during the training, we compute a distribution
parameter of the activation outputs ğ‘(ğ‘¡) and update it with Exponential Moving
Average. During the inference, the distribution parameter is employed as a linear
factor to the activation function [75]. The ğ´(ğ‘™) is the activation output of layer ğ‘™,
and ğ´ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(Â·) is the non-linear activation function following the convolution or
fully-connected layers, such as Sigmoid, Tanh, ReLU.

We evaluate VecQ on image classiï¬cation task with the popular models and
compare the results to the state-of-the-art quantization solutions with the same
DNN model and bitwidth conï¬gurations. The state-of-the-art quantization solutions
include BWN [11], TWN [66], TTQ [76], TSQ [77], INQ [78] and ENN [79]. Note
here, not all of these quantization solutions provide bitwidth support from 1 to 8 bits.
As shown in Fig. 6, our VecQ quantization outperforms most of the solutions with the
same bitwidth conï¬gurations, and VecQ provides a wider range of bitwidth coverage
as well. It only loses the advantage when comparing to the solutions speciï¬cally
designed for binary weights.

Fig. 6 Comparison with state-of-the-art solutions.

4 Eï¬ƒcient accelerator design and workload mapping

As discussed before, there exists an ever-widening barrier between fast DNN model
design in software and slow hardware accelerator implementation. To bridge the
hardware-software gap, in this section, we introduce DNNBuilder [30] and PyLog
[38] to provide eï¬ƒcient solutions for automatically generating high-performance
hardware accelerators for DNN workload deployments.

2468Bitwidth6261605958575655AlexNetTop1Accuracy(%)2468Bitwidth2468Bitwidth72.570.067.565.062.560.057.555.0MobileNetV2Top1Accuracy(%)70686664626058ResNet18Top1Accuracy(%)BWNTWNTTQTSQINQENNVecQBWNBPWNTWNTTQINQENNVecQDCHAQQATTQTVecQ12

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

4.1 DNNBuilder

DNNBuilder is an end-to-end automation framework that can transform DNN de-
signs from popular deep learning frameworks to highly optimized hardware deploy-
ment on customized accelerators implemented on FPGAs. Users are no longer re-
quired to design and optimize accelerators manually but can enjoy the auto-generated
hardware accelerators for desired AI workloads. DNNBuilder introduces two ma-
jor architecture innovations: the ï¬ne-grained layer-based pipeline architecture and
the column-based cache scheme, which achieve 7.7Ã— and 43Ã— reduction of latency
and on-chip memory usage, respectively. This subsection presents the novel designs
introduced by DNNBuilder and showcases its promising edge AI performance.

4.1.1 An end-to-end automation ï¬‚ow

Fig. 7 The end-to-end design ï¬‚ow introduced by DNNBuilder [30].

DNNBuilder produces customized DNN accelerators in three steps as Design,
Generation, and Execution (Fig. 7). During the Design step, a DNN is designed and
trained using deep learning frameworks, which in general employ CPUs and GPUs.
After training, network deï¬nition ï¬les and trained parameters are passed to the next
step. To ensure design freedom speciï¬ed by users, the proposed ï¬‚ow supports hybrid
quantization schemes, where diï¬€erent quantization schemes can be applied to the
parameters and activations of diï¬€erent network layers, to explore tradeoï¬€s among
inference accuracy, resource utilization, performance, etc. One important feature of
this step is the feedback function that provides hardware metrics estimation. If the
current DNN runs slower or consumes more resources than expected, users could
update their network designs, such as adjusting quantization schemes or modifying
network layers to meet performance and resource requirements. This function also
makes the hardware-software co-design possible.

In the Generation step, network parsing is launched to decompose the input
models. Diï¬€erent network layers, e.g., CONV, Pooling, and FC layers, are decom-

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

13

posed and then mapped to our pre-built RTL IPs, which are the basic building blocks
of the generated accelerator. The computational intensive nested loops are captured
by parameterized compute engines. Then, automated optimization works for ex-
ploring the hardware design space and provides conï¬guration guidelines so that the
generated accelerator can achieve maximum performance. Following these guide-
lines, network construction is responsible for building DNN implementations with
the pre-built RTL IPs, dataï¬‚ow controller, and memory instances, which are highly
conï¬gurable to ensure the adaptability and scalability for various DNNs. After that,
code generation generates accelerator related ï¬les for FPGA-based instances.

In the Execution step, the DNN accelerator is instantiated in FPGA with uniï¬ed
interfaces, including a FIFO-like data input/output interface and a weight access
interface connecting the oï¬€-chip memory controller. In this ï¬nal step, the DNN
accelerator is ready for eventual deployment.

4.1.2 Architecture novelties

Fig. 8 Latency comparison between the proposed ï¬ne-grained (left) and conventional (right)
pipeline when handling the same object detection DNN model with a ZC706 embedded FPGA
[30].

We propose a ï¬ne-grained layer-based pipeline to deliver high throughput perfor-
mance and promising real-time response. Each major neural network layer, such as
CONV or FC layer, in the targeted DNN model, is handled by one pipeline stage,
as major layers dominate computation and memory consumption. The rest of the
layers, such as batch normalization (BN), scale, and activation layers, are aggregated
to their neighboring major layers so that we reduce the number of pipeline stages
for lower latency. In addition, DNNBuilder enables pipeline stage overlapping to
overcome the long initial latency, which is frequently encountered by conventional
pipelines. We demonstrate the proposed ï¬ne-grained layer-based pipeline by accel-
erating an object detection DNN model called YOLO [80] and show the results in
Fig. 8. DNNBuilder can eï¬€ectively hide the data transmission delay and generate
outputs even when the ï¬rst input frame is still loading. It helps achieve a 7.7Ã— smaller
startup latency (9.92ms) compared to the conventional pipeline design (411.99ms).

14

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 9 The proposed column-based cache scheme [30].

The other novel design is the column-based cache scheme, which reduces on-chip
memory utilization during DNN inference and supports high-deï¬nition image input
for resource-constrained embedded systems. By following the pipeline architecture,
intermediate results between pipeline stages are stored on-chip to guarantee seamless
pipeline operations. However, feature maps can be enormous when inputs become
large in real life and become impossible to be held on-chip entirely. The column-
based cache scheme is designed to address this problem as it only keeps a subset of
the input feature map on chip. Fig. 9 shows an example when DNNBuilder processes
a convolution layer (with kernel size=3 and stride=1). Since slices 1âˆ¼3 contribute to
the ï¬rst sliding window operation (from top to bottom), we name the ï¬rst three slices
as column 1. Similarly, column 2 represents the amount of data for the second sliding
window operation, so that slices 2âˆ¼4 constitute column 2. DNNBuilder caches at
least two columns before starting computing, which allows the kernel to perform
the second vertical sliding window operation immediately after ï¬nishing the ï¬rst
one. Delay caused by data shortage will not happen by caching one more column.
Meanwhile, slice 5 will start buï¬€ering to form the next column (with slices 3âˆ¼5)
after releasing the room taken by slice 1. By serving the same objection detection AI
model (YOLO with high-deï¬nition inputs), the proposed column-based cache can
signiï¬cantly reduce 43Ã— on-chip memory usage compared to the accelerator without
this technology [30].

4.1.3 State-of-the-art performance

We demonstrate our design by accelerating popular AI workloads on an embedded
platform (ZC706). As shown in Table 4, our DNNBuilder generated design reaches

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

15

the best performance (524 and 262 GOPS in Fix8 and Fix16 quantization schemes)
and power eï¬ƒciency (72.8 GOPS/Watt in Fix8 and 36.4 GOPS/Watt in Fix16).
We also extend our comparison to the embedded GPU (TX2) in Table 5. The
DNNBuilder-generated design can deliver higher eï¬ƒciency than the TX2-based
solution even without using batch processing (batch size = 1), and it can achieve up
to 47.2 image/Second/Watt.

Table 4 Comparison with existing embedded FPGA-based DNN accelerators [30].

Reference
FPGA chip
Frequency
Network
Precision
DSPs (used/total)
DSP Eï¬ƒciency
Performance (GOPS)
Power Eï¬ƒciency (GOPS/W)

[29]

DNNBuilder

[81]
Zynq XC7Z045 Zynq XC7Z045 Zynq XC7Z045
100 MHz
VGG
Fix16
824/900
69.6%
230
24.4

200MHz
VGG
Fix16 (Fix8)
680/900
96.2%
262 (524)
36.4 (72.8)

150 MHz
VGG
Fix16
780/900
44.0%
137
14.2

Table 5 Alexnet inference comparison on embedded GPU and FPGA platforms [30].

Platform

Precision Batch

DNNBuilder (ZC706) Fix16, Fix8 1, 2

GPU-TX2[82]

Float16

2

Throughput
(img./S)
170, 340
250

Power
(W)
7.2
10.7

Eï¬ƒciency
(img./S/W)
23.6, 47.2
23.3

4.2 PyLog: A Python-based FPGA Programming Flow

The fast-growing complexity of new applications and new use scenarios poses seri-
ous challenges for computing systems. Embedded hardware accelerator systems have
demonstrated great ï¬‚exibility, performance, and eï¬ƒciency in many diï¬€erent appli-
cations and scenarios. However, as system complexity and application complexity
grow rapidly, programming and optimizing embedded accelerator systems require
great manual eï¬€orts and consume a lot of time. Compiling and optimizing a gen-
eral application speciï¬ed in high-level programs like Python are becoming common
tasks in creating embedded accelerator designs. High-level Synthesis (HLS) trans-
forms design inputs written in high-level languages (e.g., C++, OpenCL, Python) to
hardware descriptions in RTL (Register-Transfer Level) languages such as Verilog.
HLS oï¬€ers up to 10Ã— code reduction and 1, 000Ã— simulation time reduction over
manual RTL design solutions. HLS has been intensively studied in the past three

16

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

decades [33, 34, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97], and there
are popular commercial HLS tools used by many designers [98, 99].

4.2.1 PyLog Flow Overview

PyLog [38] is a Python-based high-level programming ï¬‚ow for FPGAs. It allows
users to create FPGA accelerators with Python code using PyLog high-level opera-
tors and Python syntax. PyLog presents a uniï¬ed programming model for host and
accelerator logic with consistent Python-based syntax and semantics. This seam-
less host-accelerator programming model enables agile system design, convenient
functional simulation, and ï¬‚exible design space exploration.

Fig. 10 shows the overall PyLog at high level. PyLog ï¬‚ow allows users to create
eï¬ƒcient FPGA accelerators and program host system with Python. The input to the
PyLog ï¬‚ow is Python code, where the FPGA kernel function is decorated with the
@pylog decorator. The PyLog ï¬‚ow contains an accelerator synthesis ï¬‚ow and a
runtime ï¬‚ow.

Fig. 10 The PyLog Flow and Example System Architecture [38]

In the accelerator synthesis ï¬‚ow, the @pylog decorator calls the PyLog compiler
to compile the kernel function into optimized high-level synthesis (HLS) C code,
which is then compiled into eï¬ƒcient FPGA IPs with HLS ï¬‚ow, and integrated
into a complete accelerator system by PyLog system generator. Beside the PyLog
kernel function, the rest of the PyLog program is interpreted by the standard Python
interpreter running on the host CPU side, which supports all Python libraries and

@pylogdefaccel(d, w):...# maind = np.array(...)w = np.array(...)accel(d, w)acceleratorhostPyLogProgramPyLogCompilerPyLogRuntimeSystem GeneratorPYNQHLS C CodeCPUMain DDR MemoryHostOn-Board DDR MemoryFPGA BoardPCIe-AXI BridgeApplication LogicApplication LogicBRAMBRAMBRAMDSPDDR CtrlFPGA ChipPCIeFPGA BitstreamPyLogFlowXRTCompilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

17

Table 6 PyLog Supported Language Features [38]

Category
PyLog high-level operators map, dot, user-deï¬ned ops

Operators

NumPy operators

Python features

argmax, argmin, max, min, matmul,
convolve, sort
list, functions, calls, lambda,
for, while, if...else..., slice,
subscript, attribute, bin_op,
unary_op, return

language features. This part of PyLog program naturally becomes the host program of
whole accelerator system. After the accelerator system is generated by the synthesis
ï¬‚ow, the system can be deployed at the target FPGA platform using the generated
FPGA bitstream conï¬guration ï¬le, and runs with support from the PyLog runtime
ï¬‚ow. During runtime, PyLog runtime can prepare input data, launch accelerator,
and collect results according to the host code. Host CPU and the FPGA accelerator
interactions are handled automatically by the PyLog runtime and the underlying
Xilinx PYNQ library [100].

4.2.2 PyLog Features

PyLog has several unique features that help users to create FPGA accelerators more
eï¬ƒciently.

(i) High-Level Operators.

In addition to commonly used Python standard language features, PyLog also
supports several built in high-level operators and NumPy operators that allow
users to express computation patterns at high level and enable better compiler
optimizations. Table 6 summarizes the language features supported in PyLog,
including PyLog high-level operators, NumPy operators, and standard Python
features. Listing 1 demonstrates a few example usages of PyLog map and dot
operators.

1 # Vector add
2 out = map( lambda x, y: x + y, vec_a , vec_b)
3
4 # 1D convolution
5 out = map( lambda x:w0*x[ -1]+ w1*x[0]+ w2*x[1], vec)
6
7 # Inner product
8 out_vec [i] = dot( matrix [i ,:], in_vec )
9
10 # Square matrix multiplication
11 out = map( lambda x,y: dot(x[0,:],y[: ,0]) , ma , mb)

Listing 1 PyLog map and dot examples. [38]

These operators not only simplify programming for users, they also pass more in-
formation on computation to the compiler (e.g., computation patterns, data ï¬‚ow

18

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 11 Diï¬€erent implementations generated from the same PyLog code. [38]

information, data/task parallelism, etc.), compared to programming in C/C++,
and thus allows compilers to perform more optimizations and choose the optimal
code generation.
Fig. 11 shows an example of generating multiple hardware implementations
from a PyLog map operation. The compiler generates HLS C implementations
in diï¬€erent styles, which corresponds to diï¬€erent hardware structures, e.g. shift
registers, systolic arrays, etc. Depending on the context and constraints, the
optimal implementation will be chosen.

(ii) Type Inference and Type Checking.

Python is a dynamically typed languages and there is no explicit type declaration
in the Python code. PyLog has a builtin type inference and type checking engine
that can infer the type and shape information of code objects in the PyLog kernel
functions. This type inference engine is critical in PyLog since same operators
may have completely diï¬€erent meanings when applied to operands with diï¬€erent
types or shapes. With this type inference engine, PyLog users do not need to
provide explicit type annotations or hints in PyLog program.

(iii) Compiler Optimizations.

PyLog provides a set of compiler optimizations that improve the design quality
of generated accelerators. PyLog uses its own PyLog intermediate representa-
tion (PLIR) as the internal representation of the input code. PyLog code analysis
and transformation passes work on PLIR to perform a sequence of optimizations
including high-level operator lowering, loop transformation, HLS pragma inser-
tion, etc. The internal PLIR is capable of expressing diï¬€erent design options
and can therefore form a design space that not only covers low-level design
parameter tuning, but also high-level design pattern selection, which has not
been explored in previous tools.

for (i...) {for (j...) {for (k...) {...} } }y= map(lambdaa, b: dot(a[0,:],b[:,0]), mat_a, mat_b)for (k...) {#pragma HLS unrollfor (i...) {for (j...) {} } }...for (i...) {for (ii...) {...} }.........PEPEPEPEPEPEPEPEPEPEPEPEPEPEPyLoghigh-level operationsHLS C ImplementationsHardware ImplementationsCompilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

19

Fig. 12 Length of HLS C code and PyLog code. [38]

4.2.3 PyLog Evaluation Results

We evaluate the performance of PyLog in terms of expressiveness and accelerator
performance, using real-world applications.

(i) Expressiveness. We evaluated the expresiveness of PyLog by comparing the
number of lines of code to implement a benchmark using PyLog and HLS C.
For the benchmarks evaluated, on average PyLog only needs around 30% of the
code length of HLS C. This indicates that PyLog provides good expressiveness
compared with HLS C and allows users to describe their computation with fewer
lines of code.

(ii) Accelerator Performance. We evaluated the performance of the accelerators
generated by PyLog using real-world benchmarks. Evaluation was done on Ama-
zon EC2 F1 f1.2xlarge instance. The evaluated benchmarks are from diï¬€erent
domains and have various computation patterns. They are representative FPGA
workloads, including linear algebra, data analytics, stencil, sparse operations,
etc. Amazon EC2 F1 f1.2xlarge instance is a cloud computing platform that
has an 8-core Intel Xeon E5-2686 v4 CPU and a Xilinx Virtex UltraScale+
XCVU9P FPGA. Table 7 shows the evaluation results. The table lists the FPGA
resource utilization as well as the accelerator execution time. We compared the
PyLog accelerator execution time against the optimized CPU time as well as
the execution time of accelerators generated from [101]. On average, PyLog
accelerators achieve around 3.17Ã— and 1.24Ã— speedup over CPU baseline and
manually optimized accelerators [38].

5 Eï¬ƒcient Optimizations

With a great number of eï¬ƒcient optimization techniques, in this section, we introduce
three key optimization techniques: hardware-aware NAS, FPGA/DNN co-design, a
specialized approach for FPGA/DNN co-design [45], and a uniï¬ed diï¬€erentiable
co-design approach, across diï¬€erent platforms [58].

00.20.40.60.81GEMMKNNK-meansJacobi2DSeidelGaussianSpMVLines of Code(normalized to HLS C)HLS CPyLog20

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Table 7 Accelerator Performance Evaluation on AWS F1 Instance

Benchmark LUT

FF BRAM DSP ğ‘“ (MHz) ğ‘ƒ(W) ğ‘‡CPU ğ‘‡HCL ğ‘‡PyLog

KNN 109276 74889
K-means 10829 17604
Jacobi [102] 93925 111144
Seidel [102] 47304 57854
Gaussian [102] 56580 75846
GEMM 12868 63759
SpMV 8294 12787
7647
4096

Histogram [103]

ğ‘‡CPU
ğ‘‡PyLog

ğ‘‡HCL
ğ‘‡PyLog
0 256.40 37.222 0.48 0.45 0.26 1.85 1.73
425
3
7 273.97 37.429 38.16 4.24 4.45 8.58 0.95
96 704 269.03 37.327 11.31 8.25 5.19 2.18 1.59
30 304 269.03 37.341 21.37 8.22 5.16 4.14 1.59
48 688 147.15 37.783 23.63 7.34 5.19 4.55 1.41
655 1024 250.00 39.657 60.34 8.13 13.05 4.62 0.62
25
13

21 273.97 37.225 0.29
0 273.97 37.327 5.85

0.24 1.21
2.07 2.83

-
-

-
-

Geometric Mean 3.17 1.24
ğ‘‡CPU: Execution time on CPU; ğ‘‡HCL: Execution time on HeteroCL[101] generated accelerator;
ğ‘‡PyLog: Execution time on PyLog generated accelerator; All time values are in milliseconds
(ms); â€˜-â€™ means the implementation is not publicly available.

5.1 Overview of Hardware-aware Neural Architecture Search (NAS)

Table 8 A brief overview of Neural Architecture Search components and example algorithms.

Search space

Architecture search space

Hardware search space

Search algorithm

Sampling-based

Supernet-based

Network evaluation

NASNet [54], DARTS [104], FB-Net [55],
ProxylessNAS [105], FlexiBERT [106]...
Quantization, sparsiï¬cation, tiling parameters,
number of PEs, other HW speciï¬c parameters...
Reinforcement learning, evolutionary algorithm,
Bayesian optimization, random search...
DARTS, Random sampling [107], SNAS [108],
EDD [58], ProxylessNAS, OFA [109]...
Early stopping, NAO [110], NASWOT [111],
Synï¬‚ow [112], GenNAS [113]...

Neural Architecture Search (NAS) refers to the automated process of neural
architectural design [114]. It has been largely successful in producing many state-
of-the-art networks. Typically, a NAS process requires three distinct components as
shown in Table 8:

1. Search space. A search space includes all possible network architectures that
follow a predeï¬ned template. For example, the networks can be sequential
layer-wise architecture [115, 116], cell-based architecture [54], and hierarchical
architecture [56]. Also, hardware parameters should be considered into the
search space for HW-awared NAS.

2. Search algorithm. The fundamental component of NAS is the search algorithm.
Given the prohibitively large search space, the search algorithm can greatly
inï¬‚uence the eï¬ƒciency of the search and the eï¬€ectiveness of the ï¬nal network

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

21

architecture. Generally, search algorithms can be classiï¬ed into two categories:
supernet-based search [104, 55] and sampling-based search [115, 115, 110, 113].
3. Network evaluation. Network evaluation is the key for eï¬ƒcient NAS, since fast
evaluation is required to estimate the quality of individual networks to guide the
search algorithm to choose top-performing architectures from the search space.
Network evaluation can be prohibitively expensive due to network training,
so that various approximation approaches have been proposed to expedite the
evaluation such as few-shot and one-shot training [117, 118] and using proxy
tasks [112].

5.2 HW-aware NAS Formulation

In recent years, driven by the need of deploying power-hungry DNNs into resource-
constrained devices, hardware-aware NAS (HW-NAS) has emerged as one of the
most promising techniques [119]. There is a great amount of hardware-aware work,
each of which often adopts a speciï¬c hardware device (CPU, GPU, embedded/mobile
device) and requires a diï¬€erent hardware-cost metric (e.g., prioritizes latency or
energy). For example, FBNet [55] develops a diï¬€erentiable neural architecture search
(DNAS) framework and discovers state-of-the-art DNNs balancing both accuracy
and hardware eï¬ƒciency, by incorporating a loss consisting of both the cross-entropy
loss that leads to better accuracy and the latency loss that penalizes the networkâ€™s
latency on a target device. To provide more integrated co-optimization solutions,
EDD [58] fuses the design space of DNN architecture and hardware accelerator
and formulates the DNN and hardware design as a co-search problem. EDD aims
to discover the most suitable combination of DNN and hardware within the co-
search space and maximize software and hardware metrics given the targeted edge
AI application. Once for All (OFA) [109] is the ï¬rst work that proposes an elastic
training scheme for supernet. By training the supernet, high-accuracy architectures
is directly searched by selecting from the OFA network without additional training.
One of the classic search method for HW-NAS is to ï¬rst deï¬ne a template based

search space, and then incorporate hardware performance into the loss function:

L = Lğ‘‡ + Lğ» ğ‘Š

or

L = Lğ‘‡ Â· Lğ» ğ‘Š

(5)

where Lğ‘‡ is the task-speciï¬c loss of NAS, such as cross-entropy loss for classiï¬cation
tasks or Mean squared error (MSE) loss for regression tasks. Lğ» ğ‘Š is the hardware
performance loss, such as measured or estimated execution latency of the network
architectures on the target device.

22

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 13 FPGA/DNN co-design framework [45].

5.3 FPGA/DNN Co-design

Hao and Chen ï¬rst proposed the concept of accelerator and DNN co-design in an
invited paper titled â€œDeep Neural Network Model and FPGA Accelerator Co-design:
Opportunities and Challengesâ€ [43], where they advocated â€œautomatically generate
both DNN models and their corresponding implementations as pairsâ€. Later, based on
the proposed co-design method, we implemented the ï¬rst simultaneous FPGA/DNN
co-design framework [45]. It has two major components, as shown in Fig. 13: (1)
a hardware-oriented bottom-up DNN model design, named Auto-DNN, which is
an eï¬ƒcient search engine to explore DNN candidates under hardware resource and
performance constraints; (2) a DNN-driven top-down FPGA accelerator design,
named Auto-HLS, which is a fast board-level design generator to automatically map
DNNs onto FPGAs.

5.3.1 The key to co-design: Bundle

The key to achieve co-design, i.e., to execute Auto-DNN and Auto-HLS simultane-
ously, is to propose basic building blocks that can be used to construct both DNNs
and their accelerators at the same time. We call such building blocks Bundles,
the common building block of both DNN architectures as well as their hardware
implementation, as shown in Fig. 14. The beneï¬ts of Bundles are two-fold. First,
a DNN can be constructed by replicating a bundle for a certain number of layers
with pooling layers inserted, which is a common and eï¬€ective way to construct
high-quality DNNs, such as the residual block in ResNet [2], the Inception block in
GoogLeNet [20]; meanwhile, many NAS approaches follow such cell-based strat-
egy [54, 56, 104]. Second, an accelerator can be constructed by building a hardware

CallieÂ HaoÂ |Â Sharcâ€labÂ @Â GeorgiaÂ InstituteÂ ofÂ Technology29DNN/FPGAÂ Coâ€DesignÂ FlowTargetÂ MLÂ task;Â FPGAÂ deviceÂ (resources);Â performanceÂ targetsÂ (QoS)Autoâ€HLS:FPGAÂ AcceleratorÂ GeneratorStepÂ 1.BasicÂ buildingÂ blockÂ modelingStepÂ 2.BuildingÂ blockÂ selectionStepÂ 3.DNNÂ searchÂ andÂ updateAutoâ€DNN:Â Coâ€SearchÂ EngineSoftware:DNNÂ ModelHardware:FPGAÂ AcceleratorCompilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

23

Fig. 14 The key to co-design: Bundle â€“ the common basic building block to both DNN design and
accelerator design [45].

module for the certain bundle and reusing it for diï¬€erent DNN layers, given that the
DNN is built by replicating the bundle; this can signiï¬cantly reduce the resource
usage of the accelerator by resource sharing and shorten the hardware development
cycle. As an example, a Bundle can be a set of DNN layers including: one 3 Ã— 3
convolution, one batch normalization, one activation, one 1 Ã— 1 convolution, and one
activation. Meanwhile, the hardware accelerator will need one instance for the 3 Ã— 3
convolution, one instance for the 1 Ã— 1 convolution, and so on.

5.3.2 Progressively reducing search space

It is non-trivial to select an optimal Bundle given the large design space and the
prohibitively long DNN training time. Therefore, it it essential to narrow down the
search space as early as possible. Our approach is in a three-step progressive way,
by ï¬ltering out unfavourable bundles at early stage and conducting detailed search
at later stage using promising ones. The three steps are as follows.
Step 1. Analytical models for performance and resource estimation for bundles and
DNNs. Denoting a Bundle as ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘–, the resource of ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘– is computed as:

ğ‘…ğ‘’ğ‘ ğ‘Ÿ

ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘– =

ğ‘…ğ‘’ğ‘ ğ‘Ÿ

ğ‘— + Î“ğ‘Ÿ
ğ‘–

âˆ‘ï¸

ğ‘ ğ‘—

(6)

ğ‘— is the resource usage of instance ğ‘ ğ‘— of resource type ğ‘Ÿ ( including
ğ‘– represents other resource overhead such as LUTs

where ğ‘…ğ‘’ğ‘ ğ‘Ÿ
DSP, LUTs, FF and BRAM). Î“ğ‘Ÿ
consumed by control logic and multiplexers.
The latency of a Bundle is estimated as:

SoftwareHardwareDNNFPGAConv3x3Conv1x1DWâ€Conv3x3ActivationConv3x3Conv1x1DWâ€Conv3x3ActivationInputOutputâ€¦2ndBasicÂ Block1stBasicÂ Block3rdBasicÂ BlockConvolution1x1Convolution3x3PoolingReluDepthâ€wiseÂ ConvolutionÂ 3x3Implements24

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

ğ¿ğ‘ğ‘¡ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘– = ğ›¼ğ‘– Â·

ğ¶ğ‘œğ‘š ğ‘ ğ‘— +

ğ›½ğ‘– Â· Î˜(ğ·ğ‘ğ‘¡ğ‘ğ‘–)
ğ‘ğ‘¤

âˆ‘ï¸

ğ‘ ğ‘—

(7)

where ğ¶ğ‘œğ‘š ğ‘ ğ‘— is the computation latency of instance ğ‘ ğ‘— , and Î˜(ğ·ğ‘ğ‘¡ğ‘ğ‘–) is the data
amount processed by ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘–. ğ‘ğ‘¤ represents the oï¬€-chip memory bandwidth. Denote
the latency of one execution of ğ‘ ğ‘— as ğ‘™ğ‘ğ‘¡ ğ‘— , and the total number of reuses of ğ‘ ğ‘— as
ğ‘Ÿğ‘’ğ‘¢ğ‘ ğ‘’ ğ‘— , the computation latency ğ¶ğ‘œğ‘š ğ‘ ğ‘— is estimated as:

ğ¶ğ‘œğ‘š ğ‘ ğ‘— =

âˆ‘ï¸

ğ‘Ÿğ‘’ğ‘¢ğ‘ ğ‘’ ğ‘— Â· ğ‘™ğ‘ğ‘¡ ğ‘—

(8)

1â‰¤ ğ‘— â‰¤ğ‘›
ğ‘Ÿğ‘’ğ‘¢ğ‘ ğ‘’ ğ‘— can be computed by the input/output dimensions of the data processed by the
IP and the data dimensions of ğ‘ ğ‘— â€™s interface. The parameter ğ›¼ğ‘– in Eq. 7 describes
how much computation is overlapped because of IP pipelining, and ğ›½ğ‘– describes
how much data transfer is overlapped during computations. ğ›¼ğ‘–, ğ›½ğ‘– and Î“ğ‘– will be
determined for each ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘– using Auto-HLS sampling.

The overall DNN latency based on ğ¿ğ‘ğ‘¡ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘– in Eq. 7 is estimated as:

ğ¿ğ‘ğ‘¡ğ· ğ‘ ğ‘ =

ğ‘
âˆ‘ï¸

ğ‘–=1

ğ¿ğ‘ğ‘¡ğ‘ğ‘¢ğ‘›ğ‘‘ + ğœ™ Â· ğ¿ğ‘ğ‘¡ğ· ğ‘€

(9)

where ğ‘ is the the number of Bundle repetitions of the DNN, and ğœ™Â·ğ¿ğ‘ğ‘¡ğ· ğ‘€ represents
the inter-bundle data movement latency. For overall DNN resource utilization, we
have:

ğ‘…ğ‘’ğ‘ ğ· ğ‘ ğ‘ = ğ‘…ğ‘’ğ‘ ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘– + ğ›¾ Â· ğ‘…ğ‘’ğ‘ ğ‘ğ‘¡ğ‘™
(10)
where ğ‘…ğ‘’ğ‘ ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘– is the resource of ğ‘ğ‘¢ğ‘›ğ‘‘ğ‘–, and ğ‘…ğ‘’ğ‘ ğ‘ğ‘¡ğ‘™ is additional control logic
overhead, e.g., ï¬nite state machine and multiplexers. ğœ™, ğ›¾, ğ¿ğ‘ğ‘¡ğ· ğ‘€ and ğ‘…ğ‘’ğ‘ ğ‘ğ‘¡ğ‘™ will
be decided and calibrated through actual hardware synthesis and implementation.
Step 2. Bundle evaluation and selection. In this step, we evaluate the latency, re-
source, and accuracy metrics for each Bundle, as deï¬ned in Step 1. Since we cannot
evaluate the accuracy for a single Bundle, we replicate a Bundle for ğ‘› times to build
a DNN and train it for a small number of epochs (20 in the experiment). We plot
Pareto curves for the Bundles to examine the tradeoï¬€ between DNN accuracy and
resource utilization, and the Bundles on the Pareto curve will be selected for detailed
search in the next step.
Step 3. DNN construction using Bundles and training. After selecting top-ğ‘ promis-
ing Bundle candidates, We search DNN models under resource and latency con-
straints. For each Bundle, ğ¾ initial DNNs are generated and are progressively updated
by adjusting the number of channels, pooling layer positions, etc., until the latency
target is met. Then, we perturb the initial DNNs by changing three variables: the
number of Bundle replications, down-sampling conï¬gurations between bundles, and
channel expansion conï¬guration. We adopted Stochastic Coordinate Descent (SCD)
algorithm for perturbation, while other heuristic or evolutionary algorithms can be
applied as well. The goal of the search algorithm is to ï¬nd the DNN architecture
which meets the performance constraints with highest accuracy.

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

25

Table 9 Performance Comparisons (FPGA and GPU competition data are obtained from [120])

Model
DNN1

IoU

Latency (ms)

Ours

DNN3

DNN2

68.6% 80.0 (100 MHz)
57.4 (150 MHz)
61.2% 62.6 (100 MHz)
44.1 (150 MHz)
59.3% 47.8 (100 MHz)
33.7 (150 MHz)
62.4% 84.6 (150 MHz)
1st in FPGA
49.2% 38.5 (150 MHz)
2nd in FPGA
57.3% 136.1 (150 MHz)
3rd in FPGA
1st in GPU
69.8% 40.7 (854 MHz)
2nd in GPU Tiny-Yolo 69.1% 39.5 (854 MHz)
3rd in GPU Tiny-Yolo 68.5% 42.3 (854 MHz)

SSD
â€“
â€“
Yolo

Power
Energy
FPS
2.2W 8.80 KJ
12.5
2.5W 7.18 KJ
17.4
2.2W 7.50 KJ
16.0
2.4W 5.51 KJ
22.7
2.2W 5.74 KJ
20.9
2.4W 4.04 KJ
29.7
4.2W 17.56 KJ
11.96
2.5W 4.81 KJ
25.97
7.35
2.6W 17.69 KJ
24.55 12.6W 25.66 KJ
25.3
13.3W 26.28 KJ
23.64 10.3W 21.79 KJ

Eï¬ƒciency
0.18 J/pic
0.14 J/pic
0.15 J/pic
0.11 J/pic
0.11 J/pic
0.08 J/pic
0.35 J/pic
0.10 J/pic
0.35 J/pic
0.51 J/pic
0.53 J/pic
0.44 J/pic

5.3.3 Evaluation results

To evaluate the eï¬€ectiveness of the co-design framework, we apply it on a low-power
object detection competition [120], and compare to the top-3 winners for both FPGA
and GPU categories. The results are shown in Table 9. We make comparisons in: (1)
the Intersection over Union (IoU); (2) the latency for processing one frame and the
overall frame per second (FPS); (3) the board power; (4) the energy consumption
for all testing data; and (5) the energy eï¬ƒciency per frame (J/pic). The results are
collected from the board-level implementations on Pynq-Z1. The latency refers to
the execution time for a single frame in millisecond, while FPS is measured using
total run-time for the 50K images including image loading, preprocessing, and DNN
inference.

Compared to the 1st-place winner of the FPGA category, we achieve 6.2% higher
IoU, 40% lower power, and 2.5Ã— better energy eï¬ƒciency, which we attribute to
the eï¬€ectiveness of an automated co-search instead of manual designs. Compared to
GPU-based designs, our DNN1 model is more accurate than the 3rd-place design and
only 1.2% lower IoU than the 1st-place GPU design. Regarding the energy eï¬ƒciency,
ours is 3.6Ã— better than the 1st-place GPU design with 40% longer latency despite a
nearly 6Ã— slower clock frequency.

5.4 EDD: Eï¬ƒcient Diï¬€erential DNN Architecture Search

On top of the FPGA/DNN co-design introduced in Sec. 5.3, we further develop
co-design to a more generalized and uniï¬ed approach, i.e., fully simultaneous neural
architecture and implementation co-search, targeting arbitrary hardware platforms.
Neural architecture and implementation co-search (NAIS) [48] is the ï¬rst work that
stylized design methodology targeting both FPGAs and GPUs, while EDD [58] is
a fully simultaneous, eï¬ƒcient diï¬€erentiable DNN architecture and implementation
co-search methodology. The overall architecture of EDD is presented in Fig. 15.

26

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Fig. 15 The overall architecture of EDD [58].

5.4.1 Fused co-design space

The key technology is to fuse the design space of DNN architecture search and
hardware implementation search. We collectively denote the variables used in DNN
search and implementation search as ğ´ and ğ¼, respectively, and the fused space of
co-search is { ğ´, ğ¼}. To carry out both DNN architecture and hardware accelerator
co-search in the fused DNN/accelerator space as described in E. 5, we minimize the
following loss function:

ğ‘šğ‘–ğ‘› : L = ğ´ğ‘ğ‘ğ‘™ğ‘œğ‘ ğ‘  ( ğ´, ğ¼) Â· ğ‘ƒğ‘’ğ‘Ÿ ğ‘“ğ‘™ğ‘œğ‘ ğ‘  (ğ¼) + ğ›½ Â· ğ¶ ğ‘…ğ¸ ğ‘† (ğ¼ )âˆ’ğ‘…ğ¸ ğ‘†ğ‘¢ğ‘

(11)

In the above equation, ğ´ğ‘ğ‘ğ‘™ğ‘œğ‘ ğ‘  is the DNN accuracy loss; ğ‘ƒğ‘’ğ‘Ÿ ğ‘“ğ‘™ğ‘œğ‘ ğ‘  is the hardware
performance loss such as end-to-end inference latency, throughput, energy, DNN
model complexity, etc.; multiple performance metrics can be optimized simultane-
ously by deï¬ning a single weighted loss. ğ‘…ğ¸ ğ‘† is the resource utilization and ğ‘…ğ¸ ğ‘†ğ‘¢ğ‘
is resource upper bound. Apparently, ğ´ğ‘ğ‘ğ‘™ğ‘œğ‘ ğ‘  is a function of ğ´ and ğ¼; ğ‘ƒğ‘’ğ‘Ÿ ğ‘“ğ‘™ğ‘œğ‘ ğ‘  and
ğ‘…ğ¸ ğ‘† are functions of ğ¼. Resource upper-bound ğ‘…ğ¸ ğ‘†ğ‘¢ğ‘ is expressed in an exponent
term to introduce large penalty when being violated. Worth noting, in the existing
hardware-aware NAS approaches, only ğ´ is searched while ğ¼ is ï¬xed during NAS.
In our proposed co-search formulation, ğ¼ is variable, and ğ´ and ğ¼ are fused as one
design space {ğ´, ğ¼}.

ğ‘œğ‘(cid:3036)(cid:2869)ğ‘œğ‘(cid:3036)(cid:2870)ğ‘œğ‘(cid:3036)(cid:3040)â€¦ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡(cid:3036)ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡(cid:3036)ğœƒ(cid:3036),(cid:2869)ğœƒ(cid:3036),(cid:2870)ğœƒ(cid:3036),(cid:3040)ğ‘‘ğ‘¤â€ğ‘˜(cid:3400)ğ‘˜ğ¶ğ‘œğ‘›ğ‘£â€1(cid:3400)1ğ¶ğ‘œğ‘›ğ‘£â€1(cid:3400)1ChannelÂ expandsbyÂ ğ‘â„(cid:3036)(cid:3040)ChannelÂ shrinksbyÂ ğ‘â„(cid:3036)(cid:3040)â€¦ğ‘ğ‘™ğ‘œğ‘ğ‘˜(cid:3036)DNNOneÂ candidateÂ operationÂ ğ‘œğ‘(cid:3036)(cid:3040)CandidateÂ operationsÂ ofÂ ğ‘ğ‘™ğ‘œğ‘ğ‘˜(cid:3036)ğœƒ(cid:3036),(cid:3040)âˆˆğœ£: samplingÂ parametersÂ ofÂ operationÂ ğ‘œğ‘(cid:3036)(cid:3040)ğ‘(cid:2869)ğ‘(cid:2870)ğ‘(cid:2901)inputoutputâ€¦ğœ™(cid:3036),(cid:3040),(cid:3044)âˆˆğœ±:Â SamplingÂ parametersÂ ofÂ ğ‘â€ğ‘ğ‘–ğ‘¡quantizationğ¼ğ‘›ğ‘ğ‘¢ğ‘¡(cid:3036)ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡(cid:3036)NeuralÂ ArchitectureÂ SearchÂ (NAS)Implemenâ€tationSearchâ€¦ğœ™(cid:3036),(cid:3040),(cid:2869)ğœ™(cid:3036),(cid:3040),(cid:2870)ğœ™(cid:3036),(cid:3040),(cid:3018)OtherÂ implementationÂ variablesÂ inğ¼(cid:3036)(cid:3040)â€¢FPGA:Â parallelÂ factors,Â loopÂ tilingÂ factors,Â etc.â€¢GPU:Â batchÂ size,Â etc.â€bitâ€bitâ€bitğ‘¨â€¦Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

27

NAS design space. In the search space, each DNN is composed of ğ‘ basic building
blocks in a single-path fashion without branches [121]. Inside each block, there are
ğ‘€ candidate operations. We adopt the most commonly used DNN blocks in NAS
approaches, called MBConv [56], which is composed of sequential layers of ğ‘ğ‘œğ‘›ğ‘£-
1Ã—1, ğ‘‘ğ‘¤ğ‘ğ‘œğ‘›ğ‘£-ğ‘˜ Ã—ğ‘˜ and ğ‘ğ‘œğ‘›ğ‘£-1Ã—1, where ğ‘˜ is the kernel size. Between ğ‘ğ‘œğ‘›ğ‘£-1Ã—1 and
ğ‘‘ğ‘¤ğ‘ğ‘œğ‘›ğ‘£-ğ‘˜ Ã—ğ‘˜, the number of channels expands/shrinks by a ratio of ğ‘â„ğ‘š
ğ‘– for operation
ğ‘œ ğ‘ğ‘š
ğ‘– . The output of each block is calculated based on the outputs of its ğ‘€ candidate
operations. Speciï¬cally, we adopt the Gumbel-Softmax function in [55], where each
operation ğ‘œ ğ‘ğ‘–
ğ‘š will be sampled from a sampling parameter ğœƒğ‘–,ğ‘š following Gumbel-
Softmax distribution, which converts the discrete non-diï¬€erentiable sampling to
continuous diï¬€erentiable sampling. The sampled operations form a complete DNN,
which can be evaluated for accuracy and implementation performance.
Implementation search space We let each candidate operation ğ‘œ ğ‘ğ‘š
ğ‘– has its own
implementation variables, forming an implementation search space ğ¼ ğ‘š
ğ‘– . The primary
implementation variable is quantization ğ‘, i.e., data precision, since it has a large im-
pact on DNN accuracy, implementation performance and hardware resource. Rather
than using a train-and-quantize approach, the quantization shall be searched together
with DNN structure to provide implementation performance feedback. Besides quan-
tization, other implementation variables can also be integrated into the framework,
such as accelerator parallelism, loop tiling factors, batch size, etc.

5.4.2 Diï¬€erentiable performance and resource formulation

The key technology is how to formulate the loss function diï¬€erentiable with respect
to the search space ğ´ and ğ¼. Since NAS search space ğ´ is discrete, diï¬€erentiable
formulation requires continuous relaxation. DARTS [104] is the ï¬rst work that uses
softmax for relaxation, while FBNet uses Gumbel-softmax [122] by sampling from
the discrete space. Such relaxation has been demonstrated to be GPU-hours eï¬ƒ-
cient with appealing model accuracy [104, 55, 113]. Motivated by FBNet, a similar
technique using Gumbel-softmax can be applied to diï¬€erentiable implementation ğ¼
to convert the discrete implementation search space into continuous. Therefore, by
descending the loss function on validation set, { ğ´, ğ¼} can be learned simultaneously.

5.4.3 State-of-the-art results

We demonstrate the results on a subset of ImageNet dataset randomly sampled from
100 classes and target three hardware architectures, each with a searched DNN
model, called EDD-Net: (1) low-latency oriented GPU (EDD-Net-1); (2) folded
FPGA architecture (EDD-Net-2), where a single processing element (PE) will be
reused by all layers; (3) pipelined FPGA architecture (EDD-Net-3), where each layer
has its own PE, and all PEs work simultaneously. Each model is produced through
EDD within a 12-hour search on a P100 GPU.

28

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

Table 10 Comparisons with existing NAS solutions [58].

Test Error (%) GPU Latency FPGA Latency
Titan RTX ZCU102 [123]
Top-1 Top-5

Baseline Models
GoogleNet
MobileNet-V2 [124]
Shuï¬„eNet-V2 [125]
ResNet18

30.22 10.47
28.1
30.6
30.2

9.7
11.7
10.9

Hardware-aware NAS Models

MNasNet-A1 [56]
FBNet-C [55]
Proxyless-cpu [105]

24.8
24.9
24.7
Proxyless-Mobile [105] 25.4
24.9
25.3
25.4

Proxyless-gpu [105]
EDD-Net-1
EDD-Net-2

7.5
7.6
7.6
7.8
7.5
7.7
7.9

27.75 ms
17.87 ms
21.91 ms
9.71 ms

17.94 ms
22.54 ms
21.34 ms
21.23 ms
15.72 ms
11.17 ms
13.00 ms

13.25 ms
10.85 ms
NA
10.15ms

8.78 ms
12.21 ms
10.81 ms
10.78 ms
10.79 ms
11.15 ms
7.96 ms

Table 11 EDD-Net-1 accuracy and latency on 1080 Ti [58].

32-bit Floating 16-bit Floating 8-bit Integer

Test Error
Latency

25.5%
2.83 ms

25.3%
2.29 ms

26.4%
1.74 ms

Table 12 Comparison of EDD-Net-3 with DNNBuilder [30]

Top-1 Error (%) Top-5 Error (%) Throughput (ZC706)

VGG16
EDD-Net-3

29.5
25.6

10.0
7.7

27.7 fps
40.2 fps

For GPU-targeted EDD-Net-1, the results are as shown in Table 10, where the
GPU latency is tested on Titan RTX. It shows that EDD-Net-1 reaches similar or
better accuracy comparing with the state-of-the-art DNN models and other NAS
approaches while achieving the shortest inference latency. Table 11 shows the accu-
racy and latency tradeoï¬€ of diï¬€erent precisions of EDD-Net-1 on Nvidia 1080 Ti
GPU. For FPGA-targeted EDD-Net-2, the latency values are collected by running
DNN models with CHaiDNN accelerators on ZCU102 FPGA as shown in Table
10. It shows that EDD-Net-2 delivers the shortest latency on FPGA among all the
DNNs. FPGA-targeted EDD-Net-3 is searched targeting a pipelined FPGA accel-
erator. As shown in Table 12, EDD-Net-3 achieves higher throughput with a much
higher accuracy comparing with the state-of-the-art.

6 Conclusion

Emerging DNN-based AI applications are challenging for embedded systems as these
applications come with high computation and memory demands as well as diverse
application-speciï¬c requirements, such as real-time responses, high-throughput per-
formance, and reliable inference accuracy. This chapter introduced a series of eï¬€ec-
tive design methods to overcome these challenges to enable embedded AI solutions.
These methods can be categorized into eï¬ƒcient machine learning algorithms, ac-

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

29

celerator and compiler designs, and various co-design and optimization strategies.
We ï¬rst proposed ELB-NN and VecQ to strengthen the AI modelâ€™s hardware ef-
ï¬ciency by enabling extremely low bit-width quantization during model training
and inference. Then, we proposed DNNBuilder and PyLog for customized hardware
accelerator design and DNN workload mapping to such accelerators. At last, we in-
troduced eï¬ƒcient co-design strategies, including FPGA/DNN co-design and EDD,
when deploying AI workloads on embedded systems.

We believe embedded AI solutions will involve more eï¬€ective and comprehen-
sive design methods in the future, covering AI algorithms, customized accelerators,
and co-design and co-optimization strategies between algorithm and hardware. For
example, our eï¬ƒcient AI algorithm designs, such as ELB-NN and VecQ, can adopt
more advanced quantization schemes to minimize network compression loss. Future
works will consider more diverse network architecture and layer-wise data distri-
bution features. To facilitate a smoother accelerator design process, we will extend
DNNBuilder and PyLog to create frameworks and tools for hardware design, synthe-
sis, and workload compilation. Major directions include 1) heterogeneous computing
support, which intends to enable system-level design and optimization for heteroge-
neous AI systems, and 2) dynamic computational graph scheduling, which enables
the generation of runtime adaptive accelerators for future AI applications. Our future
works will also cover more advanced software/hardware co-design for emerging AI
models running on heterogeneous systems, which contains a much larger design
space and is thus more challenging. For example, multi-modal multi-task (MMMT)
models [126] and customized hardware designs [127] working for autonomous driv-
ing have demonstrated the importance of heterogeneity in AI model and hardware
designs. The co-design and co-optimization methods must be developed for such
heterogeneous scenarios.

Acknowledgments The works presented in this book chapter are mainly supported by the IBM-
Illinois Center for Cognitive Computing System Research (C3SR) â€“ a research collaboration as
part of IBM AI Horizons Network, Semiconductor Research Corporation (SRC), the Campus for
Research Excellence and Technological Enterprise (CREATE) program in Singapore, AMD-Xilinx
Center of Excellence, and a Google PhD Fellowship to Xiaofan Zhang. The authors also want to
thank Chao Zhu, Cheng Gong, Chengyue Wang, Hyunmin Jeong, Jinjun Xiong, Junsong Wang,
Kun Wu, Kyle Rupnow, Tao Li, Qiuwen Lou, Wen-mei Hwu, Xinheng Liu, Ye Lu, and Yonghua
Lin for their valuable contributions.

References

1. Yann LeCun, Yoshua Bengio, and Geoï¬€rey Hinton. Deep learning. Nature, 521(7553):436â€“

444, 2015.

2. Kaiming He, Xiangyu Zhang, Shaoqing Ren, et al. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition
(CVPR), 2016.

3. Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. Attention is all you need. In Advances in

neural information processing systems, 2017.

30

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

4. Stephen Lombardi, Jason Saragih, Tomas Simon, et al. Deep appearance models for face

rendering. ACM Transactions on Graphics, 37(4):1â€“13, 2018.

5. Alex Krizhevsky, Ilya Sutskever, and Geoï¬€rey E Hinton. Imagenet classiï¬cation with deep
convolutional neural networks. Advances in neural information processing systems, 2012.
6. David Silver, Julian Schrittwieser, Karen Simonyan, et al. Mastering the game of go without

human knowledge. Nature, 550(7676):354â€“359, 2017.

7. OpenAI. AI and compute. 2018.
8. Shengkui Zhao, Saima Ahmed, Yun Liang, et al. A real-time 3d sound localization system
with miniature microphone array for virtual reality. In Proceedings of the IEEE Conference
on Industrial Electronics and Applications (ICIEA), 2012.

9. Deming Chen, Jason Cong, Swathi Gurumani, et al. Platform choices and design demands for
iot platforms: cost, power, and performance tradeoï¬€s. IET Cyber-Physical Systems: Theory
& Applications, 1(1):70â€“77, 2016.

10. Norman P Jouppi, Cliï¬€ Young, Nishant Patil, et al.

In-datacenter performance analysis
of a tensor processing unit. In Proceedings of the International Symposium on Computer
Architecture (ISCA), 2017.

11. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, et al. Xnor-net: Imagenet classiï¬ca-
tion using binary convolutional neural networks. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 525â€“542, 2016.

12. Junsong Wang, Qiuwen Lou, Xiaofan Zhang, et al. Design ï¬‚ow of accelerating hybrid ex-
tremely low bit-width neural network in embedded FPGA. In Proceedings of the International
Conference on Field Programmable Logic and Applications (FPL), pages 163â€“1636, 2018.
13. Dibakar Gope, Ganesh Dasika, and Matthew Mattina. Ternary hybrid neural-tree networks for
highly constrained iot applications. In Proceedings of the Conference on Machine Learning
and Systems (MLSys), 2019.

14. Cheng Gong, Yao Chen, Ye Lu, et al. Vecq: Minimal loss dnn model compression with

vectorized weight quantization. IEEE Transactions on Computers (TC), 2020.

15. Yao Chen, Kai Zhang, Cheng Gong, et al. T-DLA: An open-source deep learning accelera-
torfor ternarized DNN models on embedded FPGA. In Proceedings of the IEEE Computer
Society Annual Symposium on VLSI (ISVLSI), 2019.

16. Song Han, Jeï¬€ Pool, John Tran, et al. Learning both weights and connections for eï¬ƒcient

neural network. In Advances in neural information processing systems, 2015.

17. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural
In Proceedings of the

networks with pruning, trained quantization and huï¬€man coding.
Proceedings of the International Conference on Learning Representations (ICLR), 2016.
18. Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ï¬lter level pruning method for deep
neural network compression. In Proceedings of the International Conference on Computer
Vision (ICCV), 2017.

19. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556, 2014.

20. Christian Szegedy, Wei Liu, Yangqing Jia, et al. Going deeper with convolutions. In Pro-
ceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2015.
21. Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. Nest: A neural network synthesis tool based
on a grow-and-prune paradigm. IEEE Transactions on Computers (TC), 68(10):1487â€“1497,
2019.

22. Ao Ren, Tianyun Zhang, Shaokai Ye, et al. ADMM-NN: An algorithm-hardware co-design
In Proceedings of
framework of dnns using alternating direction methods of multipliers.
the International Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS), 2019.

23. Xiaohan Ding, Guiguang Ding, Jungong Han, et al. Auto-balanced ï¬lter pruning for eï¬ƒ-
cient convolutional neural networks. In Proceedings of the AAAI Conference on Artiï¬cial
Intelligence (AAAI), 2018.

24. Andrew G Howard, Menglong Zhu, Bo Chen, et al. Mobilenets: Eï¬ƒcient convolutional neural

networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

31

In

25. Manoj Alwani, Han Chen, Michael Ferdman, et al. Fused-layer CNN accelerators.
Proceedings of the International Symposium on Microarchitecture (MICRO), 2016.

26. Bryan Brown. IntelÂ® math kernel library for deep learning networks. 2018.
27. Dustin Franklin. NVIDIA Jetson AGX Xavier delivers 32 tera ops for new era of AI in

robotics. NVIDIA Accelerated Computing| Parallel For all, 2018.

28. Chen Zhang, Peng Li, Guangyu Sun, et al. Optimizing FPGA-based accelerator design for
In Proceedings of the International Symposium on

deep convolutional neural networks.
Field-Programmable Gate Arrays (FPGA), 2015.

29. Jiantao Qiu, Jie Wang, Song Yao, et al. Going deeper with embedded FPGA platform for
In Proceedings of the International Symposium on Field-

convolutional neural network.
Programmable Gate Arrays (FPGA), 2016.

30. Xiaofan Zhang, Junsong Wang, Chao Zhu, et al. DNNBuilder: An automated tool for building
high-performance DNN hardware accelerators for FPGAs. In Proceedings of the International
Conference on Computer-Aided Design (ICCAD), pages 1â€“8. IEEE, 2018.

31. Yu-Hsin Chen, Tushar Krishna, Joel S Emer, et al. Eyeriss: An energy-eï¬ƒcient reconï¬gurable
accelerator for deep convolutional neural networks. In Proceedings of the International Solid-
State Circuits Conference (ISSCC), 2016.

32. Song Han, Xingyu Liu, Huizi Mao, et al. EIE: Eï¬ƒcient inference engine on compressed deep
neural network. In Proceedings of the International Symposium on Computer Architecture
(ISCA), 2016.

33. Alexandros Papakonstantinou, Karthik Gururaj, John A. Stratton, et al. FCUDA: Enabling
eï¬ƒcient compilation of CUDA kernels onto FPGAs. In Proceedings of the Symposium on
Application Speciï¬c Processors, 2009.

34. Kyle Rupnow, Yun Liang, Yinan Li, et al. A study of high-level synthesis: Promises and

challenges. In Proc. of the International Conference on ASIC, 2011.

35. Xinheng Liu, Yao Chen, Tan Nguyen, et al. High level synthesis of complex applications: An
H. 264 video decoder. In Proceedings of the International Symposium on Field-Programmable
Gate Arrays (FPGA), 2016.

36. Chen Zhang, Guangyu Sun, Zhenman Fang, et al. Caï¬€eine: Toward uniformed representation
and acceleration for deep convolutional neural networks. IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems (TCAD), 38(11):2072â€“2085, 2018.

37. Hanchen Ye, Xiaofan Zhang, Zhize Huang, et al. HybridDNN: A framework for high-
In Proceedings of the

performance hybrid DNN accelerator design and implementation.
Design Automation Conference (DAC), pages 1â€“6. IEEE, 2020.

38. Sitao Huang, Kun Wu, Hyunmin Jeong, et al. PyLog: An algorithm-centric Python-based
FPGA programming and synthesis ï¬‚ow. IEEE Transactions on Computers (TC), 70(12):2015â€“
2028, 2021.

39. Yangqing Jia, Evan Shelhamer, Jeï¬€ Donahue, et al. Caï¬€e: Convolutional architecture for
fast feature embedding. In Proceedings of the ACM international conference on Multimedia,
2014.

40. MartÃ­n Abadi, Paul Barham, Jianmin Chen, et al. Tensorï¬‚ow: A system for large-scale
machine learning. In Proceedings of the USENIX symposium on operating systems design
and implementation (OSDI), 2016.

41. Adam Paszke, Sam Gross, Francisco Massa, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems,
2019.

42. Cong Hao, Jordan Dotzel, Jinjun Xiong, et al. Enabling design methodologies and future
trends for edge ai: specialization and codesign. IEEE Design & Test, 38(4):7â€“26, 2021.
43. Cong Hao and Deming Chen. Deep neural network model and fpga accelerator co-design:
Opportunities and challenges. In 2018 14th IEEE International Conference on Solid-State
and Integrated Circuit Technology (ICSICT), pages 1â€“4. IEEE, 2018.

44. Xiaofan Zhang, Haoming Lu, Cong Hao, et al. Skynet: a hardware-eï¬ƒcient method for object
detection and tracking on embedded systems. In Proceedings of the Conference on Machine
Learning and Systems (MLSys), pages 216â€“229, 2020.

32

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

45. Cong Hao, Xiaofan Zhang, Yuhong Li, et al. FPGA/DNN co-design: An eï¬ƒcient design
In Proceedings of the Design Automation

methodology for IoT intelligence on the edge.
Conference (DAC), pages 1â€“6. IEEE, 2019.

46. Yifan Yang, QÄ³ing Huang, Bichen Wu, et al. Synetgy: Algorithm-hardware co-design for
convnet accelerators on embedded fpgas. In Proceedings of the International Symposium on
Field-Programmable Gate Arrays (FPGA), 2019.

47. Kaiyuan Guo, Shulin Zeng, Jincheng Yu, et al. A survey of fpga-based neural network infer-
ence accelerators. ACM Transactions on Reconï¬gurable Technology and Systems (TRETS),
12(1):1â€“26, 2019.

48. Cong Hao, Yao Chen, Xinheng Liu, et al. Nais: Neural architecture and implementation
search and its applications in autonomous driving. arXiv preprint arXiv:1911.07446, 2019.
49. Weiwen Jiang, Lei Yang, Edwin H-M Sha, Qingfeng Zhuge, Shouzhen Gu, Sakyasingha Das-
gupta, Yiyu Shi, and Jingtong Hu. Hardware/software co-exploration of neural architectures.
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD),
2020.

50. Junsong Wang, Xiaofan Zhang, Yubo Li, et al. Exploring HW/SW co-optimizations for ac-
celerating large-scale texture identiï¬cation on distributed GPUs. In Proc. of the International
Conference on Parallel Processing (ICPP), pages 1â€“10, 2021.

51. Xiaofan Zhang, Yuan Ma, Jinjun Xiong, et al. Exploring HW/SW co-design for video analysis
on CPU-FPGA heterogeneous systems. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems (TCAD), 2021.

52. Yonggan Fu, Yongan Zhang, Chaojian Li, et al. A3C-S: Automated agent accelerator co-search
In Proceedings of the Design Automation

towards eï¬ƒcient deep reinforcement learning.
Conference (DAC), 2021.

53. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.

The Journal of Machine Learning Research, 20(1):1997â€“2017, 2019.

54. Barret Zoph, VÄ³ay Vasudevan, Jonathon Shlens, et al. Learning transferable architectures for
scalable image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition (CVPR), pages 8697â€“8710, 2018.

55. Bichen Wu, Xiaoliang Dai, Peizhao Zhang, et al. Fbnet: Hardware-aware eï¬ƒcient convnet
design via diï¬€erentiable neural architecture search. In Proceedings of the IEEE conference
on computer vision and pattern recognition (CVPR), pages 10734â€“10742, 2019.

56. Mingxing Tan, Bo Chen, Ruoming Pang, et al. Mnasnet: Platform-aware neural architecture
search for mobile. In Proceedings of the IEEE conference on computer vision and pattern
recognition (CVPR), pages 2820â€“2828, 2019.

57. Weiwen Jiang, Xinyi Zhang, Edwin H-M Sha, et al. Accuracy vs. eï¬ƒciency: Achieving both
through fpga-implementation aware neural architecture search. In Proceedings of the Design
Automation Conference (DAC), pages 1â€“6, 2019.

58. Yuhong Li, Cong Hao, Xiaofan Zhang, et al. EDD: Eï¬ƒcient diï¬€erentiable DNN architec-
ture and implementation co-search for embedded AI solutions. Proceedings of the Design
Automation Conference (DAC), 2020.

59. Lei Yang, Zheyu Yan, Meng Li, et al. Co-exploration of neural architectures and heterogeneous
asic accelerator designs targeting multiple tasks. Proceedings of the Design Automation
Conference (DAC), 2020.

60. Xiaolong Ma, Fu-Ming Guo, Wei Niu, et al. Pconv: The missing but desirable sparsity in dnn
weight pruning for real-time execution on mobile devices. In AAAI, pages 5117â€“5124, 2020.
61. Wei Niu, Xiaolong Ma, Sheng Lin, et al. Patdnn: Achieving real-time dnn execution on mobile
devices with pattern-based weight pruning. In Proceedings of the International Conference
on Architectural Support for Programming Languages and Operating Systems (ASPLOS),
pages 907â€“922, 2020.

62. Ji Lin, Wei-Ming Chen, John Cohn, et al. MCUNet: Tiny deep learning on IoT devices. In

Advances in neural information processing systems, 2020.

63. PULP - An Open Parallel Ultra-Low-Power Processing-Platform. http://iis-projects.

ee.ethz.ch/index.php/PULP.

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

33

64. Angelo Garofalo, Giuseppe Tagliavini, Francesco Conti, et al. Xpulpnn: accelerating quan-
In Proceedings of the
tized neural networks on risc-v processors through isa extensions.
Design, Automation & Test in Euro. Conf. & Exhibition (DATE), pages 186â€“191. IEEE,
2020.

65. Angelo Garofalo, Manuele Rusci, Francesco Conti, et al. Pulp-nn: accelerating quantized
neural networks on parallel ultra-low-power risc-v processors. Philosophical Transactions of
the Royal Society A, 378(2164):20190155, 2020.

66. Fengfu Li, Bo Zhang, and Bin Liu.

Ternary weight networks.

arXiv preprint

arXiv:1605.04711, 2016.

67. Shuchang Zhou, Yuxin Wu, Zekun Ni, et al. Dorefa-net: Training low bitwidth convolutional
neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.
68. Ritchie Zhao, Weinan Song, Wentao Zhang, et al. Accelerating binarized convolutional
neural networks with software-programmable FPGAs. In Proceedings of the International
Symposium on Field-Programmable Gate Arrays (FPGA), pages 15â€“24, 2017.

69. Yaman Umuroglu, Nicholas J Fraser, Giulio Gambardella, et al. Finn: A framework for fast,
scalable binarized neural network inference. In Proceedings of the International Symposium
on Field-Programmable Gate Arrays (FPGA). ACM, 2017.

70. Eriko Nurvitadhi, David Sheï¬ƒeld, Jaewoong Sim, et al. Accelerating binarized neural net-
works: Comparison of FPGA, CPU, GPU, and ASIC. International Conference on Field-
Programmable Technology (FPT), 2016.

71. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Bina-
rized neural networks. In Advances in neural information processing systems, pages 4107â€“
4115, 2016.

72. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep
neural networks with binary weights during propagations. In Advances in neural information
processing systems, pages 3123â€“3131, 2015.

73. Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, et al. Neural networks with few mul-
tiplications. In Proceedings of the Proceedings of the International Conference on Learning
Representations (ICLR), 2016.

74. Canran Jin, Heming Sun, and Shinji Kimura. Sparse ternary connect: Convolutional neural
networks using ternarized weights with enhanced sparsity. In Proceedings of the Asia and
South Paciï¬c Design Automation Conference (ASP-DAC), pages 190â€“195, 2018.

75. Sergey Ioï¬€e and Christian Szegedy. Batch normalization: Accelerating deep network training
In Proceedings of the International Conference on

by reducing internal covariate shift.
Machine Learning (ICML), pages 448â€“456, 2015.

76. Chenzhuo Zhu, Song Han, Huizi Mao, et al. Trained ternary quantization. In Proceedings of
the Proceedings of the International Conference on Learning Representations (ICLR), 2017.
77. Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. Two-
step quantization for low-bit neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition (CVPR), pages 4376â€“4384, 2018.

78. Aojun Zhou, Anbang Yao, Yiwen Guo, et al. Incremental network quantization: Towards

lossless cnns with low-precision weights. 2017.

79. Cong Leng, Zesheng Dou, Hao Li, et al. Extremely low bit neural network: Squeeze the last
bit out with admm. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence (AAAI),
2018.

80. Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the

IEEE conference on computer vision and pattern recognition (CVPR), 2017.

81. Qingcheng Xiao, Yun Liang, Liqiang Lu, et al. Exploring heterogeneous algorithms for
accelerating deep convolutional neural networks on FPGAs. In Proceedings of the Design
Automation Conference (DAC), 2017.

82. Dustin Franklin. NVIDIA Jetson TX2 delivers twice the intelligence to the edge. NVIDIA

Accelerated Computing| Parallel For all, 2017.

83. Alexandros Papakonstantinou, Yun Liang, John A Stratton, et al. Multilevel granularity
parallelism synthesis on FPGAs. In Proceedings of the International Symposium on Field-
Programmable Custom Computing Machines (FCCM), pages 178â€“185. IEEE, 2011.

34

Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, Deming Chen

84. Swathi T. Gurumani, Jacob Tolar, Yao Chen, et al. Integrated CUDA-to-FPGA synthesis with
Network-on-Chip. In Proceedings of the International Symposium on Field-Programmable
Custom Computing Machines (FCCM), pages 21â€“24, 2014.

85. Yao Chen, Swathi T. Gurumani, Yun Liang, et al. FCUDA-NoC: A scalable and eï¬ƒcient
network-on-chip implementation for the CUDA-to-FPGA ï¬‚ow. IEEE Transactions on Very
Large Scale Integration (VLSI) Systems, 24(6):2220â€“2233, 2016.

86. Tan Nguyen, Swathi Gurumani, Kyle Rupnow, et al. FCUDA-SoC: Platform integration for
ï¬eld-programmable soc with the CUDA-to-FPGA compiler. In Proceedings of the Interna-
tional Symposium on Field-Programmable Gate Arrays (FPGA), page 5â€“14, 2016.

87. Ying Chen, Tan Nguyen, Yao Chen, et al. FCUDA-HB: Hierarchical and scalable bus
architecture generation on FPGAs with the FCUDA ï¬‚ow. IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems (TCAD), 35(12):2032â€“2045, 2016.

88. Jason Cong, Hui Huang, and Wei Jiang. A generalized control-ï¬‚ow-aware pattern recognition
algorithm for behavioral synthesis. In Proceedings of the Design, Automation & Test in Euro.
Conf. & Exhibition (DATE), pages 1255â€“1260, 2010.

89. Jason Cong, Bin Liu, Stephen Neuendorï¬€er, et al. High-level synthesis for FPGAs: From
IEEE Transactions on Computer-Aided Design of Integrated

prototyping to deployment.
Circuits and Systems (TCAD), 30(4):473â€“491, 2011.

90. Zhiru Zhang, Yiping Fan, Wei Jiang, et al. AutoPilot: A Platform-Based ESL Synthesis

System, pages 99â€“112. Springer Netherlands, 2008.

91. Jason Cong, Yiping Fan, Guoling Han, et al. Platform-based behavior-level and system-level

synthesis. In 2006 IEEE International SOC Conference, pages 199â€“202, 2006.

92. Andrew Canis, Jongsok Choi, Mark Aldham, et al. LegUp: High-level synthesis for FPGA-
In Proceedings of the International Symposium on

based processor/accelerator systems.
Field-Programmable Gate Arrays (FPGA), page 33â€“36, 2011.

93. Andrew Canis, Jongsok Choi, Mark Aldham, et al. LegUp: An open-source high-level syn-
thesis tool for FPGA-based processor/accelerator systems. ACM Transactions on Embedded
Computing Systems, 13(2), sep 2013.

94. Hanchen Ye, Cong Hao, Jianyi Cheng, et al. Scalehls: A new scalable high-level synthesis
framework on multi-level intermediate representation. In Proceedings of the International
Symposium on High Performance Computer Architecture (HPCA), pages 741â€“755, 2022.
95. Deming Chen, Jason Cong, Yiping Fan, et al. LOPASS: A low-power architectural synthesis
system for FPGAs with interconnect estimation and optimization. IEEE Transactions on Very
Large Scale Integration (VLSI) Systems, 18(4):564â€“577, 2009.

96. Deming Chen, Jason Cong, and Junjuan Xu. Optimal module and voltage assignment for
In Proceedings of the Asia and South Paciï¬c Design Automation Conference

low-power.
(ASP-DAC), volume 2, pages 850â€“855, 2005.

97. Deming Chen, Jason Cong, and Junjuan Xu. Optimal simultaneous module and multivoltage
assignment for low power. ACM Transactions on Design Automation of Electronic Systems,
11(2):362â€“386, 2006.

98. Vitis HLS.

https://www.xilinx.com/support/documentation-navigation/

design-hubs/dh0090-vitis-hls-hub.html.

99. Siemens High-Level Synthesis & Veriï¬cation. https://eda.sw.siemens.com/en-US/

ic/ic-design/high-level-synthesis-and-verification-platform/.

100. PYNQ. http://www.pynq.io/.
101. Yi-Hsiang Lai, Yuze Chi, Yuwei Hu, et al. HeteroCL: A multi-paradigm programming infras-
tructure for software-deï¬ned reconï¬gurable computing. In Proceedings of the International
Symposium on Field-Programmable Gate Arrays (FPGA), pages 242â€“251, 2019.

102. Scott Grauer-Gray, Lifan Xu, Robert Searles, et al. Auto-tuning a high-level language targeted

to gpu codes. In 2012 Innovative Parallel Computing (InPar), pages 1â€“10, 2012.

103. Ryan Kastner, Janarbek Matai, and Stephen Neuendorï¬€er. Parallel programming for fpgas,

2018.

104. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Diï¬€erentiable architecture search.

arXiv preprint arXiv:1806.09055, 2018.

Compilation and Optimizations for Eï¬ƒcient Machine Learning on Embedded Systems

35

105. Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on

target task and hardware. arXiv preprint arXiv:1812.00332, 2018.

106. Shikhar Tuli, Bhishma Dedhia, Shreshth Tuli, et al. Flexibert: Are current transformer

architectures too homogeneous and rigid? arXiv preprint arXiv:2205.11656, 2022.

107. Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture

search. In Uncertainty in artiï¬cial intelligence, pages 367â€“377. PMLR, 2020.

108. Sirui Xie, Hehui Zheng, Chunxiao Liu, et al. Snas: stochastic neural architecture search.

arXiv preprint arXiv:1812.09926, 2018.

109. Han Cai, Chuang Gan, Tianzhe Wang, et al. Once-for-all: Train one network and specialize

it for eï¬ƒcient deployment. arXiv preprint arXiv:1908.09791, 2019.

110. Renqian Luo, Fei Tian, Tao Qin, et al. Neural architecture optimization. Advances in neural

information processing systems, 31, 2018.

111. Joe Mellor, Jack Turner, Amos Storkey, et al. Neural architecture search without training. In
Proceedings of the International Conference on Machine Learning (ICML), pages 7588â€“7598.
PMLR, 2021.

112. Mohamed S Abdelfattah, Abhinav Mehrotra, Åukasz Dudziak, et al. Zero-cost proxies for

lightweight nas. arXiv preprint arXiv:2101.08134, 2021.

113. Yuhong Li, Cong Hao, Pan Li, et al. Generic neural architecture search via regression.

Advances in Neural Information Processing Systems, 34, 2021.

114. George Kyriakides and Konstantinos Margaritis. An introduction to neural architecture search

for convolutional networks. arXiv preprint arXiv:2005.11074, 2020.

115. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv

preprint arXiv:1611.01578, 2016.

116. Bowen Baker, Otkrist Gupta, Nikhil Naik, et al. Designing neural network architectures using

reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.

117. Yiyang Zhao, Linnan Wang, Yuandong Tian, et al. Few-shot neural architecture search. In
Proceedings of the International Conference on Machine Learning (ICML), pages 12707â€“
12718. PMLR, 2021.

118. Hongyang Li, David Eigen, Samuel Dodge, et al. Finding task-relevant features for few-shot
learning by category traversal. In Proceedings of the IEEE conference on computer vision
and pattern recognition (CVPR), pages 1â€“10, 2019.

119. Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, et al. A comprehensive
survey on hardware-aware neural architecture search. arXiv preprint arXiv:2101.09336,
2021.

120. Xiaowei Xu, Xinyi Zhang, Bei Yu, et al. Dac-sdc low power object detection challenge for uav
applications. IEEE transactions on pattern analysis and machine intelligence, 43(2):392â€“403,
2019.

121. Dimitrios Stamoulis, Ruizhou Ding, Di Wang, et al. Single-path nas: Device-aware eï¬ƒcient

convnet design. arXiv preprint arXiv:1905.04159, 2019.

122. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.

arXiv preprint arXiv:1611.01144, 2016.

123. CHaiDNN. https://github.com/Xilinx/CHaiDNN.
124. Mark Sandler, Andrew Howard, Menglong Zhu, et al. Mobilenetv2: Inverted residuals and
linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern
recognition (CVPR), pages 4510â€“4520, 2018.

125. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, et al. Shuï¬„enet v2: Practical guidelines for
eï¬ƒcient cnn architecture design. In Proceedings of the European Conference on Computer
Vision (ECCV), pages 116â€“131, 2018.

126. Cong Hao and Deming Chen. Software/hardware co-design for multi-modal multi-task
learning in autonomous systems. In Proceedings of the International Conference on Artiï¬cial
Intelligence Circuits and Systems (AICAS), pages 1â€“5. IEEE, 2021.

127. Emil Talpes, Debjit Das Sarma, Ganesh Venkataramanan, et al. Compute solution for teslaâ€™s

full self-driving computer. IEEE Micro, 40(2):25â€“35, 2020.

