Submitted to the Proceedings of the US Community Study
on the Future of Particle Physics (Snowmass 2021)

Snowmass21 Accelerator Modeling Community White Paper

by the Beam and Accelerator Modeling Interest Group (BAMIG)∗

Authors (alphabetical): S. Biedron13, L. Brouwer1, D.L. Bruhwiler7, N. M. Cook7, A. L.
Edelen6, D. Filippetto1, C.-K. Huang9, A. Huebl1, T. Katsouleas15, N. Kuklev4, R. Lehe1, S.
Lund12, C. Messe1, W. Mori10, C.-K. Ng6, D. Perez9, P. Piot4,5, J. Qiang1, R. Roussel6, D.
Sagan2, A. Sahai11, A. Scheinker9, E. Stern14, M. Th´evenet16, F. Tsung10, J.-L. Vay1, D.
Winklehner8, and H. Zhang3

1Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA
2Cornell University, Ithaca, NY 14853, USA
3Thomas Jeﬀerson National Accelerator Facility, Newport News, VA 23606, USA
4Argonne National Laboratory, Lemont, IL 60439, USA
5Northern Illinois University, DeKalb, IL 60115, USA
6SLAC National Accelerator Laboratory, Menlo Park, CA 94025, USA
7RadiaSoft LLC, Boulder, CO 80301, USA
8Massachusetts Institute of Technology, Cambridge, MA, 02139, USA
9Los Alamos National Laboratory, Los Alamos, NM 87545, USA
10University of California at Los Angeles, Los Angeles, CA 90095, USA
11University of Colorado Denver, Denver, CO 80204, USA
12Michigan State University, East Lansing, MI 48824, USA
13University of New Mexico, Albuquerque, NM 87106, USA
14Fermi National Accelerator Laboratory, Batavia, IL 60563, USA
15University of Connecticut, Storrs, CT 06269, USA
16Deutsches Elektronen-Synchrotron DESY, 22607 Hamburg, Germany

September 26, 2022

2
2
0
2

p
e
S
2
2

]
h
p
-
c
c
a
.
s
c
i
s
y
h
p
[

4
v
5
3
3
8
0
.
3
0
2
2
:
v
i
X
r
a

∗AccBeamModelSnowmass21@lbl.gov

1

 
 
 
 
 
 
Contents

Executive Summary

1 Introduction

2 Previous Reports and Recommendations

3

8

9

3 Modeling needs

11
3.1 RF-based acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.2 Plasma-based wakeﬁeld acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.3 Structure-based wakeﬁeld acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.4 PetaVolts per meter plasmonics and Plasmonic acceleration . . . . . . . . . . . . . . 14
3.5 Materials modeling for accelerator design . . . . . . . . . . . . . . . . . . . . . . . . 15
3.6 Structured plasmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.7 Superconducting magnets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

4 To the next frontier: ultraprecise, ultrafast virtual twins of particle accelerators 19
Interdisciplinary simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
4.1
. . . . . . . . . . . . . . . . . . . . . . . . . 20
4.2 End-to-end Virtual Accelerators (EVA)
4.3 Virtual twins of particle accelerators . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

5 Cutting-edge and emerging computing opportunities

21
5.1 Advanced algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
5.2 Artiﬁcial intelligence, machine learning, and diﬀerentiable simulations
. . . . . . . . 22
5.3 Quantum computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.4 Storage ring quantum computers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

6 Computational needs

28
6.1 Hardware: CPU/GPU time, memory, archive . . . . . . . . . . . . . . . . . . . . . . 28
6.2 Software performance, portability and scalability . . . . . . . . . . . . . . . . . . . . 29
6.3 Scalable I/O and in-situ analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

7 Sustainability, reliability, user support, training

31
7.1 Code robustness, validation & veriﬁcation, benchmarking, reproducibility . . . . . . 32
7.2 Usability, user support and maintenance . . . . . . . . . . . . . . . . . . . . . . . . . 34
7.3 Training and education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

8 Toward community ecosystems & data repositories

36
. . . . . . . . . . . . . . . . . . . . . . . . . 36
8.1 Loose integration: Integrated workﬂows
8.2 Tighter integration: Integrated frameworks
. . . . . . . . . . . . . . . . . . . . . . . 38
8.3 Data repositories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
8.4 Centers & consortia, collaborations with industry . . . . . . . . . . . . . . . . . . . . 41

9 Outlook

42

2

Executive summary

Computer modeling is essential to beam and accelerator physics research, as well as to the design,
commissioning and operation of particle accelerators. Somewhat surprisingly, despite accelerator
physics being a ﬁeld with extreme levels of coordination and long-range planning for the research,
design, construction, and operation of its largest accelerator complexes, e.g., at CERN or at Fermilab,
the development of beam and accelerator physics codes has often been largely uncoordinated. This
comes at a great cost, is not desirable and may not be tenable.

Accelerator simulation is a large, complex topic, and much time and eﬀort has been spent in
developing a large collection of simulation software that cover an expanding range of intertwined
physics topics. The complexity of the overall endeavor has risen sharply in the last decade with the
impetus to adapt the algoritms and codes to rapidly changing computing hardware and software
environments, and the additional task of having to reimagine the algorithms and recast them for
quantum computing. This is compounded by the need to infuse a rapidly evolving set of AI/ML
technologies, which represent tremendous opportunities but can also be very disruptive.

After a summary of relevant comments and recommendations from various reports over the
last ten years, this community paper examines the modeling needs in accelerator physics, from the
modeling of single beams and individual accelerator elements, to the realization of virtual twins
that replicate all the complexity to model a particle accelerator complex as accurately as possible.
A discussion follows on cutting-edge and emerging computing opportunities, such as advanced
algorithms, AI/ML and quantum computing, computational needs in hardware, software performance,
portability and scalability, and needs for scalable I/O and in-situ analysis. Considerations of
reliability, long-term sustainability, user support and training are covered next, followed by an
overview of the beneﬁts of ecosystems with integrated workﬂows based on standardized input and
output, and with integrated frameworks and data repositories developed as a community. The last
section highlights how the community can work more collaboratively and eﬃciently through the
development of consortia and centers, and via collaboration with industry.

The following high-level recommendations are provided to synthesize the summary of recommen-

dations for the topics discussed in the paper and listed afterward:

1. Develop a comprehensive portfolio of particle accelerator and beam physics modeling tools in
support of achieving Accelerator and Beam Physics Thrust Grand Challenges on intensity,
quality, control, and prediction.

2. Develop software infrastructure to enable end-to-end virtual accelerator modeling and corre-

sponding virtual twins of particle accelerators.

3. Develop advanced algorithms and methods including AI/ML modalities and quantum comput-

ing technologies.

4. Develop eﬃcient and scalable software frameworks and associated tools to eﬀectively leverage

next generation high-performance and high-throughput computing hardware.

5. Develop sustainable and reliable code maintenance practices, community benchmarking
capabilities, and training opportunities to foster the cooperative application of accelerator
software.

6. Foster an open community that spans academia, national labs and industry to (a) develop
software ecosystems, libraries, frameworks and standards, (b) curate data repositories, and (c)
establish dedicated centers and distributed consortia with open governance models.

3

Summary of recommendations (extended version)

The Beam and Accelerator Modeling Interest Group (BAMIG) proposes the following recommenda-
tions to the Snowmass21 conveners:

1. Recommendation on Modeling needs: Support the development of a comprehensive
portfolio of particle accelerator and beam physics modeling tools for all types of particle
accelerators (e.g., RF-based, plasma-based, structured-based wakeﬁeld, plasmonic), accelerator
components (e.g., materials, superconducting magnets, structured plasmas), and which target
the Accelerator and Beam Physics Thrust Grand Challenges on intensity, quality, control, and
prediction.

(a) Subrecommendation on RF-based acceleration: Support the development of mod-
eling tools and methods that target the Accelerator and Beam Physics Thrust Grand
Challenges (intensity, quality, control, and prediction), which will require modeling of
collective eﬀects with improved ﬁdelity on long time scales, improving computational speed
to allow for statistical ensembles and design optimization, and improved integration with
realistic magnet and RF modeling.

(b) Subrecommendation on plasma-based wakeﬁeld acceleration: Support the de-
velopment of modeling tools and methods that will enable start-to-end simulations that
predict the full 6-D (+ spin) evolution of beams in a PBA-based linear collider, from their
creation to their ﬁnal focusing at the interaction point, and include all the intermediate
phases of acceleration, transport, manipulations, collisions, etc.

(c) Subrecommendation on structure-based wakeﬁeld acceleration: Support the
development of eﬃcient and accurate algorithms capable of modeling the beam interaction
with its wakeﬁeld over long interaction lengths in structures with arbitrary geometries
and constitutive parameters.

(d) Subrecommendation on PetaVolts per meter plasmonics: Support the develop-
ment of a new quantum-kinetic approach to model large-amplitude plasmons sustained
by oscillations of ultra-dense conduction-band free electron Fermi gas. Modeling the
dynamics of ionic lattice and the energy band structure under the inﬂuence of PetaVolts
per meter plasmonic ﬁelds of relativistic, nonlinear plasmons is critical to understand the
eﬀect on materials.

(e) Subrecommendation on materials modeling for accelerator design: Support the
development of automated scale-bridging methods that can autonomously parameterize
higher-scale models from large numbers of lower-scale calculations, so as to enable
predictive materials studies over a broad space of materials and conditions.

(f) Subrecommendation on structured plasmas: Support the development and integra-
tion of ﬂuid and kinetic codes to meet the modeling demands for a new class of structured
plasma devices coupling macroscopic plasma properties with strict requirements on kinetic
interactions.

(g) Subrecommendation on superconducting magnets: Support the development of
novel mixed ﬁnite element formulations and algorithms, and their implementation in
open source software tailored for superconducting magnet design.

2. Recommendation on the next frontier: ultraprecise, ultrafast virtual twins of
particle accelerators: Support the development of accelerator modeling software that

4

orchestrate interdisciplinary set of tools with standardized data representations to enable
end-to-end virtual accelerator modeling and virtual twins of particle accelerators, which
combine ﬁrst-principle models together with machine learning-based surrogate models, for
tunability from maximum precision for accurate and realistic accelerator design to maximum
speed for online particle accelerator tuning.

(a) Subrecommendation on interdisciplinary simulations: Support interdisciplinary
simulations especially eﬀorts to establish standards that would ease the sharing of infor-
mation and data across codes as well as standards for interfacing codes.

(b) Subrecommendation on end-to-end Virtual Accelerators (EVA): Support the
development of software that are capable of end-to-end virtual accelerator (EVA) modeling
(Grand Challenge of Accelerator and Beam Physics) that incorporate all components
(including both conventional and AAC sections) and all pertinent physical eﬀects.

(c) Subrecommendation on virtual twins of particle accelerators: Support the de-
velopment of virtual twins of particle accelerators, which combine high-performance
computing, ﬁrst-principle models together with machine learning-based surrogate models,
with tunability from maximum precision for accurate and realistic accelerator design to
maximum speed for online particle accelerator tuning.

3. Recommendation on cutting-edge and emerging computing opportunities: Sup-
port the research and development on cutting-edge and emerging computing opportunities,
including advanced algorithms, AI/ML methods, quantum computing algorithms for beam
and accelerator physics, as well as on the development of storage ring quantum computers.

(a) Subrecommendation on Advanced algorithms: Support research on algorithms,
from reﬁning the understanding of the properties and bottlenecks of existing algorithms to
the elaboration of novel algorithms that exhibit better properties, remove the bottlenecks,
and improve the speed and accuracy of accelerator modeling.

(b) Subrecommendation on Artiﬁcial intelligence, machine learning, and diﬀer-
entiable simulations: Support the development of ML modeling techniques and their
integration into accelerator simulation and control systems, with an emphasis on fast-
executing (up to real-time) and diﬀerentiable models, continual learning and adaptive
ML for time-varying systems and distribution shifts, uncertainty quantiﬁcation to assess
conﬁdence of model predictions, and physics-informed methods to enable broader model
generalization to new conditions and reduced reliance on large training data sets.

(c) Subrecommendation on Quantum computing: Support quantum computing al-
gorithm and code development for accelerator modeling, feasibility study on quantum
computing implementation in accelerator modeling, and quantum computing education in
accelerator community.

4. Recommendation on computational needs: Support the development of increasingly
powerful and specialized High-Performance Computing (HPC) and High-Throughput Comput-
ing (HTC) capabilities for accelerator modeling, and maintenance of software to run eﬃciently
on these hardware (e.g., port of codes to GPUs) with eﬃcient and scalable I/O, post-processing
and in situ data analysis solutions, which will be needed to support ab initio modeling at
increasing ﬁdelity, training of surrogate models and AI/ML guided designs.

(a) Subrecommendation on hardware: CPU/GPU time, memory, archive: Sup-
port the development of increasingly powerful and specialized High-Performance Computing

5

(HPC) and High-Throughput Computing (HTC) capabilities for accelerator modeling,
which will be needed to support ab initio modeling at increasing ﬁdelity, interactive and
parallel data analysis, training of surrogate models and AI/ML guided designs.

(b) Subrecommendation on software performance, portability and scalability:
Foster the development and maintenance of codes that run eﬃciently on the latest
hardware (e.g., add support for GPUs/FPGAs) by using maintainable single-source,
portable solutions, and that are scalable on leadership-scale supercomputers with multiple
levels of parallelization and support for eﬀective dynamic load balancing.

(c) Subrecommendation on scalable I/O and in-situ analysis: Support the develop-
ment and maintenance of eﬃcient and scalable I/O, post-processing, in situ data analysis,
and data sharing solutions in particle accelerator codes. Coordinate on scientiﬁc data
documentation, standardization, development of interactive and reproducible analysis
workﬂows and foster data reuse.

5. Recommendation on sustainability, reliability, user support, training: Provide suf-
ﬁcient resources for code maintenance, automated testing, benchmarking, documentation and
code reviews. Convene a community eﬀort to identify topics and teaching teams to deliver
academic classes designed to foster sharing and cooperation, to be taught at the U.S. Particle
accelerator school.

(a) Subrecommendation on code robustness, validation & veriﬁcation, bench-
marking, reproducibility: Establish and maintain open review, automated testing,
validation and benchmark procedures for modeling tools and ensure reproducibility by
tracking all changes in a documented and openly accessible manner.

(b) Subrecommendation on usability, user support and maintenance: Consider
maintenance of code reviews for improvements, documentation, installation and testing to
be central to the mission of usable scientiﬁc software: establish open feedback and support
channels, perform regular releases with change logs, use permissive open source licensing
whenever possible and cover all scientiﬁc functionality with automated tests.

(c) Subrecommendation on training and education: Convene a community eﬀort to
identify topics and teaching teams to deliver academic classes designed to foster sharing
and cooperation. The classes should be taught in the US Particle Accelerator School with
course materials and linked tutorials/extensions regularly maintained and publicly posted.

6. Recommendation on community ecosystems & data repositories: Organize the
beam and accelerator modeling tools and community through the development of (a) ecosystems
of codes, libraries and frameworks that are interoperable via open community data standards,
(b) open access data repositories for reuse and community surrogate model training, (c)
dedicated Centers and distributed consortia with open community governance models and
dedicated personnel to engage in cross-organization and -industry development, standardization,
application and evaluation of accelerator and beam modeling software and data.

(a) Subrecommendation on loose integration: Integrated workﬂows: Foster the
adoption and continued development of open community (meta-)data standards and their
implementation in modeling tools and data acquisition for seamless integration of the
community accelerator modeling tools into multiphysics workﬂows.

(b) Subrecommendation on tighter integration: Integrated frameworks: Establish
open, contributable, modular libraries and integrated frameworks for central computing,

6

modeling and analysis tasks that foster the sharing of common functionalities between
applications, using open licenses and best practices/policies.

(c) Subrecommendation on data repositories: Establish open access data repositories
and foster publishing of modeled and measured accelerator & beam data to allow re-use
(e.g. beam transport to applications), model training (e.g. AI/ML), preservation, recasting
and reinterpretation.

(d) Subrecommendation on centers & consortia, collaborations with industry:
Organize the beam and accelerator modeling community through the development of
dedicated Centers and distributed consortia. Dedicate resources to adopt open community
governance models and dedicate personnel to engage in cross-organization and -industry
cooperation.

7

1

Introduction

For particle accelerators—among the most complex and largest high-tech devices ever built—
computational tools are indispensable. Computer simulations are critical to the design, commission-
ing, operation, and upgrading of accelerator facilities which cost many millions to billions of dollars.
It is thus widely recognized that the importance of accelerators to society and the high cost of new
accelerator facilities demand that the most advanced and sophisticated high-performance computing
(HPC) tools be brought to bear on modeling activities in accelerator science and technology [1–6].[]
Accelerator simulation is a large, complex topic, and much time and eﬀort has been spent in
developing a large collection of simulation software, many developed by a single accelerator physicist
and some by interdisciplinary collaborations of computational accelerator physicists, computer
scientists, applied mathematicians and software engineers. Some address a single physics topic while
others involve multiphysics, interdisciplinary frameworks, or workﬂows. Some software has grown
very large. Some include very sophisticated algorithms that are pushing the state-of-the-art in
accelerator modeling, and sometimes in applications outside of accelerator physics (e.g., astrophysics).
While some accelerator modeling research or design can be done on a laptop or workstation, others
need the full power of the largest supercomputers. For a signiﬁcant fraction of these activities,
approximations, idealizations or others means to reduce the computational needs are necessary to
ﬁt the simulations within the available computer memory and runtime, eventually compromising
accuracy and ﬁdelity. In addition to pursue its longstanding tradition in research and development
of novel algorithms, the accelerator modeling community is investigating emerging opportunities
based on machine learning and quantum computing.

Despite accelerator physics being a ﬁeld with extreme levels of coordination and long-range
planning for the research, design, construction, and operation of its largest accelerator complexes,
e.g., at CERN or at Fermilab, the development of beam and accelerator physics codes has often
been largely uncoordinated. This comes at a great cost, is not desirable and may not be tenable.
Due to developers retiring or moving on to other projects, numerous simulation programs have
been completely abandoned or are seldom used. This has resulted in a collection of codes that are
not interoperable, use diﬀerent I/O formats and quite often duplicate some physics functionalities
using the exact same underlying algorithms. Frequently there is a huge impediment to maintaining
these programs due to poorly-written code and lack of documentation. Additionally, many of the
programs that are available tend to be “rigid”. That is, it is generally diﬃcult to modify a program
to simulate something it is not designed to simulate a priori. Adding a new type of lattice element
that a particle can be tracked through is one such example [5]. Abandoned simulation programs
represent a huge cost [7], not only in terms of time and money spent in developing a program,
but also in terms of researchers leveraging existing technology. Indeed, a researcher who wants to
simulate something that existing programs are unable to, will, due to time and monetary constraints,
generally not be able to fully develop a comprehensive simulation program from scratch as compared
to what could have been done if existing software could be leveraged. As simulation programs
become more complex due to the ever-increasing demands placed upon machine performance, the
situation will become worse if not addressed.

After a summary of relevant comments and recommendations from various reports over the last
ten years, this paper examines the modeling needs in accelerator physics, from the modeling of
single beams and individual accelerator elements, to the realization of virtual twins that replicate
all the complexity to model a particle accelerator complex as accurately as possible. We then
discuss cutting-edge and emerging computing opportunities, such as advanced algorithms, AI/ML
and quantum computing, computational needs in hardware, software performance, portability and
scalability, and needs for scalable I/O and in-situ analysis. Considerations of reliability, long-term

8

sustainability, user support and training are considered next, before discussing the beneﬁts of
ecosystems with integrated workﬂows based on standardized input and output, and with integrated
frameworks and data repositories developed as a community. Last, we highlight how the community
can work more collaboratively and eﬃciently through the development of consortia and centers, and
via collaboration with industry. A recommendation is proposed at the beginning of each section
and subsection.

2 Previous Reports and Recommendations

This section summarizes relevant comments and recommendations from various reports over the
last ten years.

2012 Oﬃce of HEP Accelerator R&D Task Force report

Software should help researchers optimize operating regimes and reduce the overall risk that underlies all
modern accelerator design. Much of the current software has not taken advantage of the many computer
improvements that have been developed in the last few decades. Such progress includes vastly increased
processor speed, exploding memory capabilities, disk storage growth and cloud computing. There is not
enough overall commercial demand for such high performance accelerator design software to have
conﬁdence this problem will be solved without US government intervention.
...
Accelerators across the board also need advanced simulation studies, and long-term support for code
development and maintenance is therefore needed.

— Oﬃce of HEP Accelerator R&D Task Force report, 2012

2014 Particle Physics Project Prioritization Panel (P5) report

The present practice is to handle much of the computing within individual projects. Rapidly evolving
computer architectures and increasing data volumes require eﬀective crosscutting solutions that are being
developed in other science disciplines and in industry. Mechanisms are needed for the continued
maintenance and development of major software frameworks and tools for particle physics and long-term
data and software preservation, as well as investments to exploit next-generation hardware and computing
models. Close collaboration of national laboratories and universities across the research areas will be
needed to take advantage of industrial developments and to avoid duplication.
Recommendation 29: Strengthen the global cooperation among laboratories and universities to
address computing and scientiﬁc software needs, and provide eﬃcient training in next-generation
hardware and data-science software relevant to particle physics. Investigate models for the development
and maintenance of major software within and across research areas, including long-term data and
software preservation.
...
Computing in particle physics continues to evolve based on needs and opportunities. For example, the use
of high-performance computing, combined with new algorithms, is advancing full 3-D simulations at
realistic beam intensities of nearly all types of accelerators. This will enable “virtual prototyping” of
accelerator components on a larger scale than is currently possible.

— Report of the Particle Physics Project Prioritization Panel (P5), 2014

9

2015 HEPAP report

Recommendation 3. Support a collaborative framework among laboratories and universities that
assures suﬃcient support in beam simulations and in beam instrumentation to address beam and particle
stability including strong space charge forces.
...
With Scenario B funding, an ambitious computational accelerator science program could be initiated to
develop new algorithms, techniques, and generic simulation code with the goal of end-to-end simulations
of complex accelerators that will guide the design, and improve the operations, of future accelerators of
all types. Advancing the capabilities of accelerator simulation codes to capitalize on the drive toward
exascale computing would have large beneﬁts in improving accelerator design and performance. New
computational algorithms coupled with the latest computer architectures are likely to reduce execution
times for many classes of simulation code by several orders of magnitude, thereby making practical
end-to-end simulations of complex accelerator systems. Such capabilities will enable cost-eﬀective
optimization of wakeﬁeld accelerators, as well as near-real-time simulations of large operational machines
such as megawatt proton accelerators or a very high-energy proton-proton collider. In the near term,
advanced simulation tools will maximize the productivity of R&D for all future accelerators.
...
One area in which there has been some recent movement towards a nationally uniﬁed eﬀort is in
accelerator-related computation. Eﬀort in the area has been boosted by funding from the SciDAC
(Scientiﬁc Discovery through Advanced Computing) program jointly funded by ASCR (DOE Oﬃce of
Advanced Scientiﬁc Computational Research) and HEP. One of the outgrowths of this eﬀort is the
CAMPA (Consortium for Advanced Modeling of Particle Accelerators) initiative from LBNL, SLAC, and
Fermilab to establish a national program in advanced modeling of accelerators. There are, however, still
many isolated simulation eﬀorts within the program.
...
It is likely that there will be signiﬁcant developments in accelerating technologies, both conventional
(NCRF, SRF) and advanced (DWFA, PWFA, LWFA) technologies, in the coming decades. ... Accelerator
physics and simulation support in these areas are crucial for making progress.
...
Computer simulations play an indispensable role in all accelerator areas. Currently, there are many
simulation programs used for accelerator physics. There is, however, very little coordination and
cooperation among the developers of these codes. Moreover there is very little eﬀort currently being made
to make these codes generally available to the accelerator community and to support the users of these
codes. The CAMPA framework is an exception, and such activities should be encouraged.

The direction of development in computer technologies makes it mandatory that the accelerator
simulation codes (as well as all other HEP-related codes) adapt to modern computer architectures. High
performance computers are another resource that HEP has not yet suﬃciently exploited. The eﬀort to
coordinate such advanced computational activities for HEP is taking place within the Forum for
Computing Excellence (FCE). Accelerator simulation eﬀort in the direction of advanced computing
should also be an integral part of the FCE, as are the other areas of HEP computation. An overall goal of
this coordinated eﬀort is to maintain and update mainline accelerator computer codes to take ad-vantage
of the most modern computer architectures.

Advances in simulations, as well as in computational capabilities, raise the exciting possibility of making a
coherent set of comprehensive numerical tools available to enable virtual prototyping of accelerator
components as well as virtual end-to-end accelerator modeling of beam dynamics. It should be possible to
construct real-time simulations to support accelerator operations and experiments, allowing more rapid
and detailed progress to be made in under-standing accelerator performance.

Simulation eﬀorts are vital for new accelerator development and supporting experimental accelerator
R&D studies. Such coherent eﬀorts could be tailored after the successful LARP model that identiﬁed
mutual study goals for assuring success of a given project (HL-LHC in the case of LARP) and supported
collaboration among various university and laboratory partners.

— HEPAP report, 2015

10

3 Modeling needs

Recommendation: Support the development of a comprehensive portfolio of particle
accelerator and beam physics modeling tools for all types of particle accelerators (e.g., RF-
based, plasma-based, structured-based wakeﬁeld, plasmonic), accelerator components (e.g.,
materials, superconducting magnets, structured plasmas), and which target the Accelerator
and Beam Physics Thrust Grand Challenges on intensity, quality, control, and prediction.

3.1 RF-based acceleration

contributed by J. Qiang, C. Mitchell, C-K. Ng

Recommendation: Support the development of modeling tools and methods that target
the Accelerator and Beam Physics Thrust Grand Challenges (intensity, quality, control, and
prediction), which will require modeling of collective eﬀects with improved ﬁdelity on long
time scales, improving computational speed to allow for statistical ensembles and design
optimization, and improved integration with realistic magnet and RF modeling.

In an RF accelerator, a train of charged particles is accelerated to very high energy (TeVs) by
RF cavities, and conﬁned by magnetic elements for high energy physics applications. For such
a high intensity charged particle beam, the collective eﬀects from charged particle interactions
among themselves and from the other beams play an important role in limiting the beam quality
and accelerator performance. These collective eﬀects include space-charge, intrabeam scattering,
coherent synchrotron radiation, short-range and long-range wakeﬁelds, beam-beam, electron cloud
and electron cooling eﬀects.

A number of computational methods have been developed to model these collective eﬀects in an
RF accelerator. The particle-in-cell method has been used to simulate space-charge, beam-beam, and
electron cloud eﬀects on massive parallel computers [8–13]. The modeling of coherent synchrotron
radiation eﬀects was reviewed in a recent publication [14], where the recent advance has led to
multi-dimensional and self-consistent simulation capability for the ﬁrst time. A Monte-Carlo method
has been used to simulate the intrabeam scattering eﬀect [15]. A Langevin method was developed
to self-consistently simulate the intrabeam scattering eﬀect by solving the Fokker-Planck equations
with the Landau/Rosenbluth potential [16]. The fast multipole method was developed to simulate
both the space-charge and the electron cooling eﬀects [17, 18].

Even though signiﬁcant progress has been made in modeling RF accelerators, self-consistent
modeling of the above collective eﬀects remains challenging, especially for understanding dynamics
on a long time scale. This includes long-term simulation of space-charge and beam-beam eﬀects, self-
consistent ﬁrst-principles modeling of intrabeam scattering and electron cooling, electron cloud and
coherent synchrotron radiation eﬀects. For example, one key goal involves the accurate prediction of
halo formation and low-level beam loss at high intensity [19]. Fast advanced computational methods
are needed to improve both the speed and the accuracy of modeling these eﬀects. The computing
time required must be suﬃciently short to enable the large ensembles of runs needed for design
optimization. Parallel programming paradigms that can make use of the latest computer hardware
such as multi-node GPUs are needed to further improve the speed of the simulation. Studies
are needed to understand numerical artifacts such as numerical noise associated with long-term
simulations. A ﬁnal area lies in improved magnet modeling and improved integration of RF and
magnet models with existing tracking tools. Methods such as surface methods that can include
realistic external ﬁeld eﬀects are needed for improving the ﬁdelity of RF accelerator designs.

11

The manufacture of accelerator structures and components is a major cost of an RF accelerator.
Virtual prototyping of accelerator components has been a key process in the design and optimization
of accelerators. Therefore, the ability to virtually prototype with HPC tools to create designs that
work ”out of the box” will substantially reduce the R&D and operational costs of these accelerator
components by eliminating the delicate, labor-intensive modiﬁcations employed in previous practices.
Virtual prototyping requires multi-physics modeling capabilities for determining RF parameters
such as shunt impedance, wakeﬁeld eﬀects and higher-order-mode (HOM) damping, temperature
distribution and thermal heat load, as well as mechanical stress and shape deformation [20].

To provide the required luminosity for a linear collider, it is critical to reduce emittance dilution
in the main linac from machine tolerances such as cavity misalignments and imperfections [21]. For
example, in a superconducting linac, misalignments can arise from individual cavity misalignments
and changes in the properties of coupled HOMs in a cryomodule with misaligned and deformed
cavities. The RF parameters and ﬁelds of the HOMs can be evaluated for random distributions of
cavity oﬀsets (in a cryomodule) and cavity deformations along the full linac. The ﬁelds are then
used for beam emittance dilution evaluation. A statistical analysis [22] using the constraints form
realistic fabrication and component placement tolerances will be facilitated by the HPC capabilities
of advanced simulation codes.

3.2 Plasma-based wakeﬁeld acceleration

contributed by J.-L. Vay, W. Mori, F. Tsung

Recommendation: Support the development of modeling tools and methods that will
enable start-to-end simulations that predict the full 6-D (+ spin) evolution of beams in a
PBA-based linear collider, from their creation to their ﬁnal focusing at the interaction point,
and include all the intermediate phases of acceleration, transport, manipulations, collisions,
etc.

This section summarizes the modeling needs for plasma-based accelerators (PBAs). A more

detailed description is given in [6], Section 2.1.

Several aspects of plasma-based accelerators (PBA) are particularly challenging to model. These
include detailed kinetic modeling of the wake excitation; trapping of background plasma electrons
in the wakeﬁelds; ion motion for a nonlinear beam loading scenario; the need to model the processes
in three dimensions; the propagation of ultra-low emittance and energy spread beams in the plasma
over meter distances (many time steps), the long (ps to ns) evolution of the perturbed plasma;
the disparity in time and space scales of wake and driver evolution; and the large number of 3D
simulations required to study the tolerances to nonideal eﬀects.

The most widely used numerical tool to study plasma accelerators is the particle-in-cell (PIC)
algorithm [23]. However, ab initio simulations of particle accelerators with PIC codes are limited by
the number of time steps that are needed to resolve the beam propagation through the accelerator
and by the number of grid cells that are sometimes needed to cover the wide disparity of spatial
scales. Several methods to reduce the computational cost, while maintaining physical ﬁdelity, have
been developed. Reduced dimensionality: When the beam and structure are nearly azimuthally
symmetric, a two-dimensional (r-z) representation with a truncated series of azimuthal modes may
be used [24–27]. Quasistatic approximation: For PBA, one of the most successful methods to speed
up simulations is the quasistatic approximation [28–33], which relies on the separation of time scales
to make approximations and decouple the (long time scale) driver evolution and the (short time
scale) plasma wave excitation, providing orders of magnitude speedup over standard PIC. Boosted
frame: An alternative to performing a quasi-static approximation to handle the disparity of scales

12

is to shrink the range of scales by using the Lorentz-boosted frame method [34]. With this method,
the ultrahigh relativistic velocity of the driver is taken advantage of by using a frame of reference
that is moving close to the speed of light, and in which the range of space and time scales of the key
physics parameters is reduced by orders of magnitude, lowering the number of time steps—thus
speeding up the simulations—by the same factor.

While enormous progress has been made in the algorithms and codes to model PBAs, the modeling
of a chain of tens to thousands of PBA stages for a multi-TeV collider is extremely challenging,
needing further developments [3, 6]. It is also essential to integrate physics models beyond single-
stage plasma physics in the current numerical tools, including ionization and recombination, coupling
with conventional beamlines, production of secondary particles, spin polarization, QED physics at
the interaction point, collisions, and in some cases plasma hydrodynamics for long-term plasma/gas
evolution and accurate plasma/gas proﬁles. Modeling PBA is currently challenging because many
beam-loading scenarios are still being considered. These choices may diﬀer between accelerating
electrons and positrons, and the best numerical choices for each scenario are not the same.

3.3 Structure-based wakeﬁeld acceleration

contributed by P. Piot

Recommendation: Support the development of eﬃcient and accurate algorithms capable
of modeling the beam interaction with its wakeﬁeld over long interaction lengths in structures
with arbitrary geometries and constitutive parameters.

Structure-based wakeﬁeld acceleration (SWFA) relies on high-charge “drive” bunches [O(10–100
nC)] passing through slow-wave structures (SWSs) to excite electromagnetic wakeﬁelds. SWFAs
can be conﬁgured in either two-beam acceleration (TBA) or collinear wakeﬁeld acceleration (CWA).
The produced wakeﬁelds can be directly used to accelerate a delayed “main” bunch (CWA) or be
out-coupled and guided to an optimized accelerating structure that accelerates the main bunch
(TBA) in a parallel beamline. CWA oﬀers a simpler conﬁguration where both the drive and main
bunches are transported along the same beamline; the TBA scheme decouples the drive and bunch
beam dynamics at the expense of increased complexity (e.g., two parallel beamlines are required).
The beam dynamics associated with the simultaneous transport of the accelerating main bunch and
decelerating drive bunch is one of the major challenges in CWA.

Signiﬁcant achievements in TBA research include accelerating gradients in excess of ∼ 300 MV/m,
wakeﬁeld-based power generation of ∼ 0.5 GW, [35] and demonstration of staged acceleration [36].
Similarly, recent progress along the CWA includes the beam-based demonstration of 300-MV/m
gradient in a THz structure [37], the generation of record transformer ratio of > 5 via improved
longitudinal beam shaping [38, 39], and signiﬁcant advances in the theoretical understanding of
beam stability [40].

These developments have been possible due to consistent progress in software capable of modelling
the beam-wave interactions in complex accelerating and power-extracting and transmission structures
(PETSs). Modeling of the SWFA includes radiofrequency (RF) design of complex structures where
the RF properties (e.g., R/Q, operating frequency, scattering parameters) are optimized using
a frequency-domain solver for eigenmode or scattering parameters (e.g., such as omega3p and
s3p [41]). Likewise, time-domain simulations are performed to understand the coupling (wakeﬁeld
generation and acceleration) of an electron bunch with the mode supported by the structures. These
investigations are generally performed with computer programs such as t3p [41] and warpx [42].
In unison with the roadmap elaborated in Ref. [43] the demonstration of SWFA as a mature

13

technology for a linear collider will require development to (i) perform integrated experiments in
dedicated test facilities, (ii) explore high-eﬃciency schemes, and (iii) push accelerating gradient.
Such developments can only be guided with high-ﬁdelity simulations. Developing integrated
experiments demands software capable of simulating the long-term beam dynamics within meter-
scale structures. The target for high-eﬃciency acceleration relies on a ﬁne control of the phase-space
distribution of the drive and possibly main beams [44]. Understanding the precise evolution of these
phase-space distributions over long interaction distances is critical to understanding the onset of
the beam break-up (BBU) instability. Simulations over long integration time are also needed for
TBA where the generation of GW-level power involves the deceleration of a train of bunches (with
100’s ps separation) in a structure. Finally, pushing the accelerating gradient requires optimization
of complex electromagnetic structures operated in a transient mode. Implementing boundaries
with complex geometries frequency-dependent electromagnetic and material properties is critical
to the development of exotic structures and understanding of their dynamical response to intense
ﬁelds [37]; see also Section 3.5.

3.4 PetaVolts per meter plasmonics and Plasmonic acceleration

contributed by Aakash Sahai along with T. Katsouleas, D. Filippetto
and other contributors of the PV/m plasmonics Snowmass paper

Recommendation: Support the development of a new quantum-kinetic approach to
model large-amplitude plasmons sustained by oscillations of ultra-dense conduction-band free
electron Fermi gas. Modeling the dynamics of ionic lattice and the energy band structure
under the inﬂuence of PetaVolts per meter plasmonic ﬁelds of relativistic, nonlinear plasmons
is critical to understand the eﬀect on materials.

PetaVolts per meter (PV/m) plasmonics eﬀort [45–48] focuses on access to extreme electromag-
netic ﬁelds using plasmonics. Plasmonics is based upon the unique properties of condensed matter
that stem from the structure of the ionic lattice which inevitably gives rise to electronic energy
bands. The electrons that inherently occupy the conduction band in conductive materials are free
to move across the entire lattice in response to external excitation. Plasmonic modes are collective
oscillations of the Fermi gas that is constituted by the conduction band electrons [49].

Below we brieﬂy summarize the PV/m initiative and its modeling needs. Further details are
available in the above referred work as well as the independent PV/m Plasmonic Snowmass paper
[50].

The scope of PV/m plasmonics is limited to mechanisms where there exists a strongly correlated
ionic lattice and corresponding electronic energy bands over relevant timescales. Under these
conditions, quantum mechanical eﬀects dominate. Speciﬁcally, our eﬀort focuses on acceleration
and focusing gradients as high as PetaVolts per meter [45–48].

PV/m plasmonics initiative [45–48] introduces a novel class of non-perturbative plasmonic modes
that are strongly electrostatic. These novel modes are excited by intense ultra-short charged particle
beams. Optical photons cannot directly excite these strongly electrostatic modes as their energy is
signiﬁcantly smaller than metallic density plasmons. Moreover, high amplitude optical plasmons
are technologically infeasible due to the large pre-pulse or pedestal energy in high-intensity optical
pulses which ablate and disrupt the ionic lattice. This turns any sample into solid-state plasma
which are out of the scope of the PV/m plasmonic eﬀort.

The electrostatic nature of plasmons makes it possible to access unprecedented PetaVolts per
meter electromagnetic ﬁelds. Strong excitation of the Fermi electron gas drives the oscillations to

14

the wavebreaking limit. Because metals have an equilibrium free electron density n0, as high as
1024cm−3, the coherence or “wavebreaking” limit [51] of EM ﬁelds is,

Eplasmon = 0.1

(cid:112)

n0(1024cm−3)PVm−1.

A comprehensive approach to modeling of relativistic and highly nonlinear plasmonic modes in
conductive materials is required to appropriately incorporate the multi-scale and multi-physics nature
of the underlying processes. Being strongly electrostatic, nonlinear plasmonic modes signiﬁcantly
diﬀer from the conventional optical plasmons and do not lend themselves to being modeled using
purely electromagnetic codes. Optical plasmons are conventionally modeled using the Finite-
Diﬀerence-Time-Domain (FDTD) method where the perturbative electron oscillations are simply
approximated using constitutive parameters. The FDTD approach with constitutive parameters
cannot be used when the collective oscillations become non-perturbative.

Our work [45, 46, 48] has adopted the kinetic approach along with Particle-In-Cell (PIC)
computational modeling of collective oscillations of the free electrons Fermi gas to account for
the nonlinear characteristics of strongly electrostatic plasmons. It is noted that PIC methodology
already utilizes the FDTD solver for calculation of electromagnetic ﬁelds but utilizes charge and
current densities based upon particle tracking. This approach utilizes the collisionless nature of
relativistic oscillations of the Fermi gas and does not assume any constitutive parameters as part of
the initial conditions. However, initialization and self-consistent evolution of the electron density
implicitly accounts for most of the constitutive parameters. Moreover, as relativistic oscillations of
the free electron Fermi gas have been experimentally observed to go beyond the Ohm’s law, evolution
of these oscillations still remains unaccounted for by collisionless modeling approach. Speciﬁcally,
conductivity, which is a critical constitutive parameter based upon electron-ion collisions, is not
properly understood in the relativistic plasmonic regime. Therefore, heating processes resulting
from relativistic plasmonic modes are not fully incorporated in the kinetic modeling approach.

In consideration of this immense promise of PV/m plasmonics using nanomaterials, we call for
the support of the modeling community to engage with the our eﬀort and help understand several
new challenges that are not part of existing modeling tools.

Below we outline a few of the key challenges identiﬁed through our ongoing eﬀorts and bring

out the unique requirements our modeling eﬀort:

• Modeling the eﬀect of relativistic energy gain of free electron Fermi gas in the the ionic lattice

potential.

• Understanding the eﬀects of the energy density of plasmonic modes on the energy-band

structure.

• Accounting for self-ﬁelds within the ultra-dense electron beams where particle-to-particle

interaction may play a relevant role.

• Incorporating eﬀects from atomistic modeling within the kinetic approach.
• Devising an approach to handle collisions to determine the longer term eﬀects of electrostatic

plasmonic ﬁelds and account eﬀects related to conductivity.

3.5 Materials modeling for accelerator design

contributed by Danny Perez

Recommendation: Support the development of automated scale-bridging methods that
can autonomously parameterize higher-scale models from large numbers of lower-scale cal-
culations, so as to enable predictive materials studies over a broad space of materials and
conditions.

15

The quest for ever-higher acceleration gradients leads to stringent demands on the ability of
materials to maintain high performance in the harsh environments typical of high-gradient accelerator
cavities. Of special interest is the understanding of the origin and properties of RF breakdown
precursors, with the goal of eventually using this understanding to develop breakdown-mitigation
strategies. These simulations are typically conducted at multiple scales, ranging from ﬁrst-principle
quantum calculations using methods such as density functional theory (DFT). Such calculations
allow one to accurately estimate elastic, thermodynamic, and electronic properties of diﬀerent alloys.
However, the steep scaling of these methods (cubic in the number of electrons) strongly limits
possible simulation sizes and times. In order to obtain nanostructural information, such as defect
nucleation, propagation, and reactions, classical molecular dynamics (MD) simulations are instead
employed. These simulations are typically extremely scalable (O(Natoms or O(Natoms log Natoms ).
A key challenge with moving from DFT to MD is that electrons are typically integrated out, which
implies that electronic physics (e.g., the coupling of electrons with the electric and magnetic ﬁelds
in the cavity) has to be re-introduced ”by hand”, which often requires ad hoc assumptions and
is typically extremely time-consuming. While MD simulations are orders of magnitude cheaper
than their DFT counterparts, simulations times (< µs) and sizes (∼ 109 atoms) remain extremely
limited compared to engineering scales. This requires the introduction of yet larger-scale models
that operate at the continuum level, including for example crystal plasticity or continuum heat
and mass transport codes. While these continuum codes are often extremely scalable, accurately
parameterizing the models with materials-speciﬁc physics is also extremely tedious and requires a
signiﬁcant level of understanding of the relevant factors, which is often not unambiguously available.
We note that the systematic and automated parameterization of continuum models from lower-scale
simulations is often not possible, except for “simple” properties such as elastic constants.

The Materials Genome Initiative has pioneered the use of large numbers of quantum simulations
(typically at the DFT level) to explore the space of possible material in search of solutions with
optimized performance characteristics. These approaches have proved very successful at exploring the
space of functional materials (such as materials for batteries), and extensions to structural materials
are also possible when suitable ﬁgures of merit (i.e., computable from computationally-aﬀordable
DFT calculations) can be deﬁned. First applications to accelerator materials have recently been
carried out for dilute binary Cu alloys, which identiﬁed new candidates for breakdown-tolerant
materials, as well as rationalized the observed performance of certain alloys like Cu/Ag. Extending
this exploration to more complex multi-component materials will prevent exhaustive enumeration and
is instead likely to require more sophisticated machine-learning-guided active-learning approaches.
Similarly, including complex microstructural eﬀects in the ﬁgure of merit (e.g., to also take into
account the microstructure of the materials) is likely to require a combination of state-of-the-art
computational methods with data-driven ML algorithms.

Further, the development of robust and automated workﬂows, such as those described in
section 8.1 ,for the estimation and upscaling of materials properties to inform higher-scale models
from lower-scale simulations would be extremely valuable, as it would enable the development
of predictive materials models in a practical amount of human eﬀort, compensating with a more
aggressive use of high-performance computing resources.

3.6 Structured plasmas

contributed by Nathan Cook

16

Recommendation: Support the development and integration of ﬂuid and kinetic codes to
meet the modeling demands for a new class of structured plasma devices coupling macroscopic
plasma properties with strict requirements on kinetic interactions.

This section summarizes the modeling needs for structured plasmas. A more detailed description

is given in [3], Section 2.4.

Structured plasmas, systems for which plasma density, temperature, composition, and ionization
state are tailored to enhance an interaction between a beam or laser with that plasma system,
are increasingly important for future accelerators. Controllable plasma channels are essential to
increasing peak energy and quality across all types of PBAs; they permit the guiding of intense
lasers over long distances for laser-driven schemes [52–55], control of plasma density over meter-scale
distances as required for beam-driven schemes [56–58], and the realization of plasma columns for
novel positron acceleration schemes [59, 60].Structured plasmas have also found application as
ﬂexible focusing elements. Discharge capillaries can produce orders-of-magnitude larger magnetic
ﬁeld gradients than traditional electromagnets [61], subsequently enabling the compact staging of
plasma accelerators [62]. Alternative approaches include passive plasma lenses, consisting of a narrow
plasma jet outﬂow generated by laser pre-ionization [63, 64], which can provide comparable focusing
at high densities. Discharge plasmas have been employed as tunable dechirpers to remove correlated
energy spreads from GeV-scale electron beams [65]. Finally, a class of proposed non-destructive
diagnostics with high spatiotemporal resolution will rely on controllable plasma densities in concert
with electron and laser beams [66].

Although speciﬁc needs vary with device type, scale, and application, structured plasmas impose
unique modeling requirements as compared with simulations of other accelerator systems. Capillary
discharge dynamics require the characterization of a discharge current and corresponding plasma
transport properties, including electrical resistivity and thermal conductivity. Magnetohydrodynamic
(MHD) codes are well suited to capturing the basic physics of these systems, while maintaining larger
timesteps and reduced resolution requirements from kinetic approaches. Signiﬁcant progress has
been made in demonstrating their agreement with 1D analytical models and experimental results for
waveguides and active plasma lenses [67, 68]. The coupling of such MHD codes with laser envelope
models [69] have contributed to record-breaking LPA acceleration of electrons to 8 GeV in 20 cm
capillary [70, 71]. Similarly, active plasma lens studies have beneﬁted from the application of MHD
to reproduce species-dependent nonlinearities in current ﬂow and magnetic ﬁeld [72–74]. MHD
codes have also been used to support a new class if structured plasmas predicted on the use of an
extended laser focus, known as hydrodynamic optical-ﬁeld-ionized channels (OFI or HOFI) [75, 76];
Initial investigations into these systems have been supported by 1D hydrodynamic simulations [75,
77]. Despite these advances, MHD simulations face challenges in accurately capturing dynamics
at early timescales, where low ionization rates and large temperature gradients impose constraints
on fundamental assumptions of local thermal equilibrium (LTE), and may introduce numerical
aberrations resulting from poor convergence of equation of state methods and inaccurate transport
models [78]. Overcoming these limitations can require prohibitive increases in simulation runtime.
More work is needed to improve the speed, reliability, and accessibility of MHD codes for modeling
these systems.

Pre-ionized plasma sources, such as those used for PWFA stages and passive lenses, present
additional modeling challenges. Neutral gas dynamics necessitate hydrodynamic simulation on
µs-timescales to generate proper initial conditions, while the ionization laser requires propagation
of an intense electromagnetic ﬁelds on fs-timescales. The resulting plasma does not constitute
a local thermal equilibrium, therefore the LTE dynamics implemented by most MHD codes is
insuﬃcient to capture the ionization and heating dynamics. Moreover, vacuum-plasma interfaces are

17

of interest for matching incident drive beams [79], and multi-species plasmas have been employed for
high-brightness injection scheme [80]. In these cases, kinetic codes, for example PIC techniques, may
be coupled with hydrodynamic codes to obtain reasonable approximations, or alternatively, ﬁrst
principles models may be employed [63, 64, 81–83]. Similar challenges are faced in understanding
the longterm evolution of these systems following laser or beam interactions. More work is needed
to improve the interfacing between ﬂuid and kinetic modeling tools to address the multi-physics
nature of these systems across disparate spatiotemporal regimes.

3.7 Superconducting magnets

contributed by Lucas Brouwer and Christian Messe

Recommendation: Support the development of novel mixed ﬁnite element formulations
and algorithms, and their implementation in open source software tailored for superconducting
magnet design.

Advanced modeling tools are currently utilized across the full range of US Magnet Development
Program (US-MDP) research activities [84], enabling the design of improved conductors, magnets,
and diagnostics. A diverse and challenging set of new modeling tools are required to continue
this eﬀort and ultimately improve design time, cost, and performance of future superconducting
accelerator magnets. The new developments include: (1) simulation of conductor and cable, (2)
advanced modeling of interfaces and other potential sources of training in stress-managed designs,
(3) modeling of LTS/HTS hybrid magnets, and (4) radiation environment thermal eﬀects on
magnets [85].

The material laws for superconducting materials are highly non-linear, which implies that a lot of
iterations must be made per simulated timestep to achieve a numerically suﬃcient convergence. To
reduce the high computational cost of these material laws cause, a novel ﬁnite element formulation
called h-φ is being discussed within the HTS modeling community [86, 87]. By using the scalar
magnetic potential φ rather than the vector potential a that is traditionally used to model the
magnetic ﬁeld in air and vacuum domains. Since the computational eﬀort is roughly proportional
to the number of unknowns to the power of two, this means that reducing the magnetic potential
form a vector to a scalar can reduce the computational eﬀort up to a factor of nine.

One challenge in suﬃciently implementing this novel formulation into a ﬁnite element code lies
in the hybrid use of node and edge-based elements, the way boundary conditions are imposed and
how eﬃcient implementations must be designed. Another challenge lies in eﬃciently solving the
system matrices which are non-symmetric, non-positive-deﬁnite and ill-conditioned.

Regarding the h-φ formulation, some of the most pressuring questions discussed in the HTS

modeling community are:

• imposing current boundary conditions in a user-friendly fashion[88, 89]
• discretization of tape structures (thin shells) in 2D and 3D space [90]
• quench prediction (maxwell-thermal coupling)
• delamination (maxwell-thermal-mechanical coupling)

Most of the published formulations have been implemented on top of commercial toolboxes such
as COMSOL and are not yet available to the general public. Moreover, it is in the very nature of
closed source codes to limit the access the developer has to the data structure and the knowledge
of underlying algorithms. The desire of having full control over the data structure motivated us
to develop a custom codebase that is tailored to the needs of HTS modeling. Having deﬁned the

18

project goals below, we decided to develop a new ﬁnite element framework from scratch to achieve
them:

• Support h-a and h-φ formulations for 2D and 3D, as well as thin shells, both with ﬁrst and

higher order elements.

• Support multiphysics, speciﬁcally thermal and mechanical coupling, as well as current sharing.
• Have a text-based user interface that is tailored to the needs of HTS magnet and cable

modeling.

• Use popular open-source data formats, such as HDF5, GMSH, and Exodus II (ParaView)
• Link against modern sparse linear algebra solvers such as STRUMPACK, PETSc, PARDISO

and MUMPS.

• Run in parallel using the MPI standard.
• Be readable, extendable and maintainable.

4 To the next frontier: ultraprecise, ultrafast virtual twins of par-

ticle accelerators

contributed by Jean-Luc Vay, Axel Huebl, R´emi Lehe, Chengkun
Huang, Nikita Kuklev, Ji Qiang, David Sagan

Recommendation: Support the development of accelerator modeling software that
orchestrate interdisciplinary set of tools with standardized data representations to enable end-
to-end virtual accelerator modeling and virtual twins of particle accelerators, which combine
ﬁrst-principle models together with machine learning-based surrogate models, for tunability
from maximum precision for accurate and realistic accelerator design to maximum speed for
online particle accelerator tuning.

The development of future particle accelerators requires predictive modeling tools that will range
from (i) very detailed, full physics and dimensionality, ﬁrst-principle kinetic simulation tools that
are needed for detailed runs for physics studies (typically based on Monte-Carlo and Particle-In-Cell
methods) [91], to (ii) ultrafast simulation tools that are needed for ensemble runs for design studies
(using a combination of, e.g., reduced physics, low dimensionality, low resolution, artiﬁcial intelligence
(AI)/machine learning (ML) surrogates [92, 93]).

4.1 Interdisciplinary simulations

Recommendation: Support interdisciplinary simulations especially eﬀorts to establish
standards that would ease the sharing of information and data across codes as well as standards
for interfacing codes.

Interdisciplinary simulations are important in a number of areas. In vacuo particle tracking
coupled with particle/matter interactions is an example of a growing need. One application is in
simulating the radiation induced by ”dark current” electrons in accelerating cavities. This radiation
may cause damage to cavities which leads to shortened lifetimes of the devices and a radiation
safety hazard for the surrounding environment. Dark current induced problems have been observed
at many facilities such as the CEBAF[94], LCLS-II[95], ANL, etc. Suﬃcient shielding is required
to properly contain the radiation which in turn requires a good understanding and prediction of
radiation levels through simulations. Another example is the modeling of positron production in a

19

target from the impact of high-energy electron beams accelerated through a linac injector. These
simulations require accurate calculations using electromagnetic RF codes for accelerator structures
and beam dynamics codes for particle transport in a beamline to characterize the beam proﬁle
before it hits the accelerator enclosure or the target.

Particle/matter simulation codes exist. Examples include Geant4[96], FLUKA[97], and MARS[98]
which have traditionally been used for detector simulation in HEP experiments. However, since
these codes and accelerator simulation codes have all been developed without common standards,
interfacing them is a laborious task. A seamless simulation requires the proper transfer of ﬁeld and
particle data from accelerator to radiation codes. Communication in a standardized format such
as openPMD[99], which has been adopted in some accelerator codes, would help ensure eﬃcient
and error-free ﬁeld and particle data transfers[100]. Another issue with an integrated simulation is
in matching of the geometry of the vacuum chamber surface. The surface geometry in accelerator
simulations is generally poorly deﬁned if at all. The most comprehensive simulations deﬁne the
surface using a ﬁnite element mesh generated from a CAD model (e.g., both WARP [8] and OPAL
[13, 101] have some limited capacity for this). In contrast, radiation codes generally employ a faceted
representation of the CAD model boundary. A converter for mapping ﬁnite element curved surfaces
to faceted divisions on an interface boundary is required to accurately determine the location of a
particle crossing from one computational domain to another. Much time and eﬀort would be saved
if the surface geometry descriptions were standardized so that a single converter module could be
used in multiple codes.

Increasingly, accelerator simulation tools are also incorporating more micro-physics models to
better describe the complex interplay of the various physics phenomena. One particular example
being the emission modeling of a high-brightness electron photocathode gun. A photocathode
gun provides a high-brightness electron source for the downstream accelerator beamline where
the beam brightness can only be degraded, not improved. Thus, it is essential to understand the
cathode emission characteristics and the method to control the beam quality in the gun environment
through validated simulations. While Monte Carlo photo-electron emission simulations have been
widely employed in studying photocathode performance for dedicated experiments, its potential in
integrated simulations has only been explored recently [102]. For such purposes, a tight integration
of the micro-physics models into existing gun simulations can be achieved via the best practices and
standardization as discussed below.

4.2 End-to-end Virtual Accelerators (EVA)

Recommendation: Support the development of software that are capable of end-to-end
virtual accelerator (EVA) modeling (Grand Challenge of Accelerator and Beam Physics) that
incorporate all components (including both conventional and AAC sections) and all pertinent
physical eﬀects.

The realization of software that are capable of end-to-end virtual accelerator (EVA) modeling

has been identiﬁed as a Grand Challenge of Accelerator and Beam Physics [103].

Thorough modeling of a full particle accelerator in individual parts (injector, magnets, beam
dynamics, etc.) is already an important aspect of particle accelerator development; one, without
which no project nowadays can proceed to the building stage. For example, start-to-end simulations
have been used in x-ray light source accelerator designs [104–106]. However, simulating the system
in many small units poses two signiﬁcant challenges: (i) The various codes used for the individual
parts are often from diﬀerent areas, lack the state-of-the-art high-ﬁdelity models for extremely bright
and intense beams, written in diﬀerent languages (C/C++, Fortran, Python, etc.), and use diﬀerent

20

standards for data I/O (particles, ﬁelds, etc.), slowing down the design and simulation process;
(ii) Without multiphysics couplings, subtle, but important eﬀects (collective eﬀects, halo, coherent
synchrotron radiation, etc.) might not be considered in the design or without suﬃcient accuracy,
ultimately leading to issues during commissioning, the need to modify the machine (shimming,
additional steering, shielding, etc.), and longer downtimes due to added radiation. Furthermore,
opportunities for more eﬃcient working points that can be achieved only from system optimization
of the entire accelerator may be missed entirely.

Thus, there is a clear need for full physics 6-D computer simulations of the entire accelerator
system that incorporate all components (including both conventional and AAC sections), all
pertinent physical eﬀects, and that execute fast and reliably. Such EVAs need to be validated
with experiments at both the component and system levels. Furthermore, these EVAs should be
able to leverage modern computing infrastructure like HPC clusters and GPU computing, and
fully integrate AI/ML tools to maximize eﬃciency for practical applications. The development of
such tools requires continuous advances in fundamental beam theory and applied mathematics,
improvements in mathematical formulations and algorithms, and their optimized implementation
on the latest computer architectures.

4.3 Virtual twins of particle accelerators

Recommendation: Support the development of virtual twins of particle accelerators,
which combine high-performance computing, ﬁrst-principle models together with machine
learning-based surrogate models, with tunability from maximum precision for accurate and
realistic accelerator design to maximum speed for online particle accelerator tuning.

Ultimately, the availability of multiphysics, end-to-end simulation capabilities will lead to the
realization of virtual twins of particle accelerators. These will enable the design and optimization of
future accelerators at scale on supercomputers, with unprecedented levels of accuracy and speed.
This capability will dramatically increase the breadth of parameter space that can be explored,
thereby enabling the design of particle accelerators that have not been possible before. Ultrafast
and ultraprecise tunability will let users simulate as needed: from maximum precision for accurate
and realistic accelerator design to maximum speed for online particle accelerator tuning. Modern
ML optimization frameworks can be used to take advantage of this tunability via multi-ﬁdelity
algorithms, allowing for faster scalable design reﬁnement. Automated uncertainty quantiﬁcation can
be used to provide hints as to where the computational budget should be applied for best overall
simulation quality.

5 Cutting-edge and emerging computing opportunities

Recommendation: Support the research and development on cutting-edge and emerging
computing opportunities, including advanced algorithms, AI/ML methods, quantum computing
algorithms for beam and accelerator physics, as well as on the development of storage ring
quantum computers.

Just as it is essential to continually invest in fundamental studies of accelerator technologies
and beam science, it is also essential to do the same for the software and algorithms that enable
such studies using computers. This includes the continuation of the study and development of
novel algorithms and optimization strategies, as well as leveraging AI/ML and exploring algorithm
that will take advantage of upcoming quantum computers. This also includes the development of

21

modeling tools to study and design quantum computers based on storage rings.

5.1 Advanced algorithms

contributed by Jean-Luc Vay, Ji Qiang

Recommendation: Support research on algorithms, from reﬁning the understanding of
the properties and bottlenecks of existing algorithms to the elaboration of novel algorithms
that exhibit better properties, remove the bottlenecks, and improve the speed and accuracy of
accelerator modeling.

Computational beam and accelerator physics has spurred the development of many algorithmic
advances [33, 34, 107–164] that have boosted the computational capabilities tremendously, in some
instances by orders of magnitude at a time. Yet, the modeling of particles accelerators remains
very demanding, and even extremely challenging in the case of design of future plasma-based or
structure-based particle wakeﬁeld HEP colliders. It is thus essential to pursue the research on
algorithms, from reﬁning the understanding of the properties and bottlenecks of existing algorithms
to the elaboration of novel algorithms that exhibit better properties, remove the bottlenecks, and
improve the speed and accuracy of accelerator modeling. The discovery of new and better algorithms
for beam and accelerator modeling is a fundamental topic that is poised to provide big boosts to
the other fundamental topics that are the discovery of new and better accelerator technologies and
concepts, and the discoveries of new laws of natures with colliders that they enable.

This also includes the adoption of algorithms that have proven to be very eﬀective into other
ﬁelds, such as adaptive mesh reﬁnement [120], which enables to zoom in on regions that need higher
resolution (e.g., sharp edge of high-intensity beam) while zooming out to simulate larger regions
(e.g., beam with pipe to include image charges and halo eﬀects). This is discussed in more detail in
[6].

5.2 Artiﬁcial intelligence, machine learning, and diﬀerentiable simulations

contributed by Auralee Edelen, Remi Lehe, Nikita Kuklev, Alexander
Scheinker, Ryan Roussel

Recommendation: Support the development of ML modeling techniques and their
integration into accelerator simulation and control systems, with an emphasis on fast-executing
(up to real-time) and diﬀerentiable models, continual learning and adaptive ML for time-
varying systems and distribution shifts, uncertainty quantiﬁcation to assess conﬁdence of
model predictions, and physics-informed methods to enable broader model generalization to
new conditions and reduced reliance on large training data sets.

Machine learning (ML) and artiﬁcial intelligence (AI) have revolutionized many computational
tasks in recent years, from protein folding prediction to fusion energy control. Speeding up
computationally-intensive accelerator simulations and aiding optimization of particle accelerators are
two key areas where ML & AI have also been making substantial contributions. Many ML/AI tools
are now technically mature, production-ready, and can be used as part of standard computational
toolkits for particle accelerators. Notable examples so far include fast-to-execute accelerator models
that can be used online as “digital twins”, as well as in design and online optimization using
Bayesian optimization and other methods, whereby a model that is learned on-the-ﬂy is used to
guide sampling of the parameter space. These methods, and tools supporting their use, are beginning
to be integrated into dedicated simulation workﬂows and into control systems for use online that

22

share common standards for interoperability. For example, the open source package LUME [165]
has been used to deploy online ML and simulation models for a variety of systems, and xopt [166]
has been used to generate datasets (both oﬄine and online) for training ML models.

So far, most studies in AI/ML for accelerators have focused on proof-of-principle tests of new
methods under narrow operating conditions, rather than integration into dedicated use online
where they would be tested under a wide variety of conditions and face additional challenges in
robustness. Looking forward, there is great potential to now expand upon the fundamental methods
algorithmically, combine the strengths of diﬀerent methods in novel ways (for example, combining
neural network models or diﬀerentiable simulations with uncertainty quantiﬁcation and Bayesian
optimization), and address the challenges of dedicated deployment that often do not arise in R&D
testing of each new algorithm (e.g. robust uncertainty quantiﬁcation and adaptation to account for
changing conditions over time or exploration of new regions of parameter space, extension to very
high dimensional systems with hundreds of freely-tunable accelerator settings, etc.). We highlight
some current and future directions below.

Fast-executing, adaptive accelerator models Physics-based accelerator models can very
accurately predict expected beam phase space evolution. These models can be calibrated oﬄine by
hand or online based on feedback from diagnostics, such as using energy spread spectra to tune a
model to predict the beam’s longitudinal phase space at FACET [167]. The major limitation of
physics-based simulation models is that they are computationally expensive and therefore cannot
give real-time (e.g. at rates of 120 Hz to MHz) predictions in the control room. For physics-based
simulations that include the major nonlinear collective beam eﬀects, prediction latencies can be
on the order of tens-of-minutes to hours depending on the simulation ﬁdelity. In contrast, trained
ML models do have the potential to provide real-time predictions, in some cases providing orders
of magnitude improvements to execution speed [168]. Many studies in ML-based modeling of
accelerator systems have relied on supervised learning on bulk inputs/outputs and fairly standard
neural network architectures, such as multi-layer perceptrons [168–172] and convolutional neural
network (CNN) based encoder-decoder architectures [173–176], as well as Gaussian process models
[177, 178]. Common inputs and outputs include accelerator settings such as magnet currents,
cavity gradients, and initial conditions such as laser parameters; common outputs include scalar
beam parameters such as the emittance and bunch length, and 2D projections of the beam phase
space. When trained on large simulation datasets of input-output variables, these models have
been demonstrated to have as much as a million times speedup in execution speed with reasonable
prediction accuracy [168]. Training on measured data (either from scratch or by adapting a model
that is pre-trained on simulations) also enables diﬀerences between the as-built machine and the
simulation to be represented.

Looking forward, there is a need to further develop methods for making ML-based accelerator
models reliable under shifting input-output data distributions. For example, this can include both
time-varying changes in inputs from an operational accelerator and deliberate changes in operating
conditions or simulation inputs that go beyond the statistical distribution of the training data.
Methods to ﬂag when model performance is degraded, as well as methods to adapt models to new
conditions are needed. Having accurate uncertainty estimates in addition to model predictions is
critical for applications in accelerators, especially in cases where decisions about control actions,
analysis, or design are made based on model predictions. Uncertainty quantiﬁcation methods, such
as the use of Bayesian neural networks[179] and ensembling [180], have been used in accelerators
to highlight cases where model uncertainty is high. However, additional work is needed to ensure
robustness of uncertainty estimates under changing conditions. In general, further work incorporating

23

state-of-the-art methods from continual online learning [181] and transfer learning [182] could be
used to maintain model accuracy over time and aid re-use of models in diﬀerent settings. This
could also help reduce reliance on large training data sets when transferring models between speciﬁc
accelerators (for example, between injectors with similar designs). Operation in tandem with
local feedback algorithms can also help retain model accuracy over time as conditions change.
Sample-eﬃciency is a key consideration for accelerator problems in which the data collection is
very time-consuming or detailed beam measurements would interrupt operations. By incorporating
physics constraints within ML frameworks, such as developing beam dynamics models based on
Taylor maps, it is possible to greatly reduce the amount of data needed for re-training models [183],
as discussed in more detail below. Further work is also needed in developing advanced methods
for cases where online re-training is not feasible or practical. Some avenues toward addressing this
challenge without re-training include the use of adaptive feedback / local optimization to make the
overall ML system more robust to unknown and time-varying system changes and inputs. Some
work towards the challenge of ML without re-training is utilizing adaptive feedback within the
ML framework by adjusting ML model inputs or internal parameters with gradient-free adaptive
feedback algorithms [176, 184]. In the context of accelerator tuning, it can also be accomplished by
combining ML system models for control with local adaptive feedback or optimization algorithms
[185].

Physics-informed and physics-guided ML modeling Recently there has been interest in
incorporating physics information more directly into ML models. This is expected to improve sample
eﬃciency and accurate generalization to unseen regions of parameter space. Such “physics-informed”
modeling spans a wide variety of techniques that use physics information more or less directly
(see [186] for a review). In the most broad sense of “physics informed”, training on very broad,
high-ﬁdelity simulation data sets introduces observational biases that allowing ML models to learn
functions, vector ﬁelds and operators that reﬂect the physical structure of the data [186]. Numerous
examples exist in the literature where training on broad accelerator simulation data sets enables
enough of the underlying physics to be represented in a suﬃciently generalizable form that the learned
models can reliably interpolate to unseen conﬁgurations or provide qualitative estimates of beam
behavior even outside of the training distribution [168, 172], for both scalar predictions and beam
phase space projections. A recent example highlights that this can also be extended to unobserved
components of the beam parameter space [176]. In that work, a convolutional neural network
(CNN)-based encoder-decoder trained and then utilized in an unsupervised adaptive approach to
predict the 15 unique 2D projections of a beam’s 6D phase space for an unknown and time-varying
input beam distribution. The approach in [176] is to adaptively tune the low-dimensional latent
space of the encoder-decoder to match a measured 2D (longitudinal phase space) projection of an
unknown 6D phase space distribution and in doing so accurately predict the other unmeasured 2D
projections of the 6D distribution.

More direct methods of making “physics-informed” models include adding physics-based con-
straints into the learning process [186, 187], or structuring the learning algorithm in a way that
mathematically resembles the physics problem (e.g., replacing one component of a Hamiltonian
update with a neural network [188]). In accelerator physics this avenue has been less explored
but holds signiﬁcant potential. In one accelerator example, the Hessian from a physics-based or
learned model is used to inject expected correlations into a Gaussian process kernel [189], which
results in more sample-eﬃcient learning and optimization. Another accelerator example uses a
diﬀerentiable formulation of beam dynamics in a ring, implemented in the tensorﬂow machine
learning library, to enable learning of free parameters using traditional ML workﬂows [183]. For

24

more eﬃcient predictions, Galerkin-based or symplectic reduced order modeling (ROM) may present
new opportunities for nonlinear beam dynamics study beyond the theoretical formulation.

Diﬀerentiable simulators While there is a long history of using automatic diﬀerentiation in
accelerator physics for higher-order calculations in rings [190], accelerator physics simulation codes
do not readily support arbitrary computation of gradients. This necessitates the use of numerical
estimation of gradients when using gradient-based optimization methods. Having simulation codes
that inherently support automatic diﬀerentiation would enable gradient-based optimization to be
more readily used with accelerator physics simulations. Tying together simulations with gradient-
based learning algorithms also has signiﬁcant promise; for example, this could aid gradient-based
learning of free parameters, as was done for accelerators with a diﬀerentiable lie map formulation
in [183] and for a diﬀerential hysteresis model in [191]. This is similar to examples using adaptive
feedback that approximates gradient descent to optimize free parameters in physics models [167].
Gradient-based learning of external ML algorithms (such as learning a control policy in reinforcement
learning) can also be aided by a diﬀerentiable forward model, as was done for example in [171]
but with a diﬀerentiable learned ML model rather than a diﬀerentiable simulator. Tying together
standard simulation tools with ML components (e.g., using a learned model for one machine section
or one computation component) would beneﬁt substantially from having end-to-end diﬀerentiability.
Expanding this concept to detailed diﬀerentiable simulations of non-linear collective eﬀects (e.g., with
particle-in-cell codes) could enable new capabilities in optimizing particle accelerator simulations and
using them in conjunction with AI/ML. One example of integrating optimization with a diﬀerentiable
physics model is given in [191], where an analytic model of hysteresis is made diﬀerentiable and
used in conjunction with measured data to eﬃciently learn an accurate model of a magnet response
that includes hysteresis; this is then used for hysteresis-aware Bayesian optimization that can more
precisely optimize the resultant beam.

ML/AI frameworks have highly-optimized implementations of automatic-diﬀerentiation and
other low level computational tools for running eﬃciently on CPU/GPU. Major libraries (PyTorch,
TensorFlow, JAX, Julia) provide low level hooks for these routines. Some frameworks can be used
to provide gradients for existing codes written in C or Fortran. This presents two promising avenues
toward making diﬀerentiable physics simulators: (1) re-writing or translating existing low-level
accelerator physics codes into formats compatible with automatic diﬀerentiation and (2) wrapping
sections of existing code with external software to provide derivatives. This also opens a path toward
using eﬃcient AI/ML learning algorithms and frameworks directly with physics simulations.

Model-based optimization The development and use of advanced algorithms for accelerator
design and optimization aims to eﬃciently solve challenging optimization problems in high dimen-
sional parametric spaces. The so called “curse of dimensionality”, where optimization diﬃculty
scales exponentially with the number of free parameters, limits the number of optimization problems
that can be done, especially when running high-ﬁdelity accelerator simulations is computationally
expensive. To make a problem solvable with a limited computational budget, either assumptions
are made to simplify the simulation itself or the free parameter space must be heavily restricted.
However, if novel techniques that substantially improve the eﬃciency of simulated optimization
can be implemented, previously unsolvable optimization problems in accelerator physics can be
realistically tackled.

Traditional design optimization in accelerators has relied on computationally intensive algorithms
such as genetic algorithms (e.g. NSGA-II [192]) and particle swarm optimization, which require
many simulation samples to converge and do not leverage any learned representations of the system.

25

ML-based optimization methods that attempt to learn a model on-the-ﬂy during optimization or use
an existing model can improve sample eﬃciency and convergence speed by enabling more judicious
choices of input variables to examine.

In cases where gradient information is not available, known as “Black Box” problems, model based
optimization (such as Bayesian optimization) techniques can be used to maximize optimization
eﬃciency. These algorithms have already been shown to perform well in certain experimental
[177, 189] and simulated [193, 194] contexts. However, there remains substantial room for further
innovation and improvement in sample eﬃciency, scaling, and tailoring to speciﬁc applications. For
example, multi-ﬁdelity Bayesian optimization [195], where evaluations of the optimization objective
at a lower ﬁdelity are used as a proxy for high ﬁdelity simulations, is naturally well-suited to
simulated optimization of accelerator based problems where multiple ﬁdelities are straightforward
to interpret. These algorithms could be used to help balance computational resources and required
ﬁdelity automatically during search reﬁnement. Furthermore, if models used for these algorithms also
provide an associated conﬁdence metric in their predictions, they can be used in information-based
optimization to either speed up optimization in high dimensional parameter spaces based on model
conﬁdence regions [196] or improve mutual information gain during nested optimization procedures
[197].

Model based algorithms also enable the use of transfer learning based-techniques, where previ-
ously created models of similar systems are used to inform algorithms applied to novel optimization
problems and aid sample-eﬃciency. For example, using information about correlated input pa-
rameters from a physics simulation to inform the internal model in Bayesian optimization of an
operating accelerator [177, 189] can also be considered a form of kernel-based transfer learning.
Physics-informed models or diﬀerentiable simulators also have great potential for use in sample-
eﬃcient, model-based optimization across diﬀerent systems. They can be used in conjunction with
gradient-based optimization to learn system model parameters on-the-ﬂy with limited data and
exploit this information in control. For example, in [191], the parameters of a general diﬀerentiable
hysteresis model for magnets is learned from beam measurements and is used in conjunction with
Bayesian optimization for hysteresis-aware ﬁne-tuning.

5.3 Quantum computing

contributed by He Zhang, Ji Qiang

Recommendation: Support quantum computing algorithm and code development for
accelerator modeling, feasibility study on quantum computing implementation in accelerator
modeling, and quantum computing education in accelerator community.

The start-to-end simulation of an accelerator using real beams with billions or more particles
remains challenging and expensive even with state-of-the-art exascale machines. The development
of quantum computers, which can potentially provide an exponential improvement in eﬃciency
for some classes of simulations, may bring new opportunities to enhance the particle accelerator
community’s simulation abilities.

The technique for building quantum computer has entered the Noisy Intermediate Scale Quantum
(NISQ) era [198]. Current state-of-the-art quantum computers have above 50 qubits and quantum
supremacy has been demonstrated on some speciﬁc problems [199, 200]. Quantum computing is
currently available to the public through cloud services provided by some commercial companies
such as IBM [201], D-Wave [202], Amazon [203] and Microsoft [204]. Meanwhile, studies on quantum
algorithms have also been booming in the past few years. One hot topic is quantum machine
learning. Machine learning can be used to optimize the performance of accelerators in design,

26

commissioning and operation. Some time-consuming simulations can also be replaced by well-
trained learning systems. Quantum computing can speed up many learning algorithms, including
SVM [205, 206], Bayesian networks [207], convolutional neural networks [208], Bayesian deep learning
[209], reinforcement learning [210, 211], etc.

Another topic that is relevant for accelerator modeling is solving linear systems [212–216] as
well as ordinary diﬀerential equations (ODEs) and partial diﬀerential equations (PDEs) [217–221].
Solving Poisson’s equation and the Vlasov equations — which are often used in the simulation of
collective eﬀects such as space charge and beam-beam interactions — with quantum computing is
being explored [222–224]. From the perspective of programming, quantum computer simulators,
as code developing-, debugging-, and testing platforms, are available for almost all mainstream
programming languages, e.g. C/C++, Python, Java, Matlab, etc. There also exist some languages
speciﬁcally designed for quantum computing [225]. All these provide the community with the
fundamental blocks to build simulation tools for beam and accelerator physics.

To make a problem suitable for quantum simulations, it has to be described by unitary operators
so that the system, which may be classical, is mathematically equivalent to a quantum system.
Additionally, in quantum computing, all variables are stored in quantum states and reading out
the results accurately can be time intensive, which should be carefully managed. Many problems
in accelerator modeling and beam dynamic simulations meet the above criteria. The motion of a
charged particle, even under collective eﬀects, can often be described as Hamiltonians, the evolution
of which can be carried out by unitary operators. Recently, a quantum Schrodinger approach
was used to solve the Poisson-Vlasov equations to simulate space-charge eﬀects in high intensity
beams [226]. Although particles are used to model beams, in many cases we care more about the
macroscopic property of the beam, e.g., emittance, momentum spread, bunch size, luminosity, etc.,
rather than the individual particle and reading out the results of all the particles can be avoided.
However, most previous work on quantum algorithms focused on the realization of an algorithm
with quantum circuits, but applying the algorithm to solve a practical scientiﬁc problem is seldom
discussed [227]. Clearly there is a gap between the development of the algorithms and their
implementation. To implement the quantum computing technique in accelerator modeling, we need
to keep abreast with the latest developments and get involved in algorithm design and program
development. We expect, at least in the near future, a quantum computer will not replace a
classical computer but will probably work together with one. It is thus probably better for now
to focus on how to make new quantum simulation tools that collaborate with the existing pool of
accelerator modeling programs. Ideally a protocol will be invented through which the quantum
packages could be called by the classical programs. Also needed are innovative analyzing tools
to process the simulation results before the reading-out. To achieve these tasks, the accelerator
physics problems that will beneﬁt from quantum computing need to be identiﬁed and for each of
them the proper mathematical model will need to be established, which may be diﬀerent from the
conventional classical one. For this, contributions from experts in both accelerator physics and
quantum computing are required.

5.4 Storage ring quantum computers

contributed by Sandra Biedron, Jean-Luc Vay

Recommendation: Support research and development of storage ring quantum comput-

ers.

In addition to beneﬁt from quantum computing in the future, storage rings can be used as
quantum computing devices themselves [228], with the advantages, over Paul [229, 230] or Penning

27

traps [231], of longer coherence time, the ability to store a signiﬁcantly higher number of ions and
the possibility of storing multiple ion chains [228]. A storage ring quantum computer (SRQC)
could potentially store thousands of ions to form an Ion Coulomb Crystal in a closed trajectory.
For SRQC, we anticipate unique challenges associated with the unprecedented large number of
ions in the ICC. For example, on timing the laser pulses to eﬃciently cool the ICC beam to its
equilibrium energy conﬁguration, from which ion manipulations can be initiated. The solutions to
the equilibrium positions of the ICC can be solved numerically for an arbitrary N, but they quickly
become lengthy and impractical as N becomes large, from hours to days. A solution that employs
AI/ML techniques was described recently that brings the prediction to a few seconds [232].

SRQC presents a particularly interesting case, where the design of the storage ring can beneﬁt
from high-performance computing and AI/ML, to produce quantum computing devices that will
open unprecedented capabilities in computing with application to many ﬁelds, including beam and
accelerator physics and designs themselves.

6 Computational needs

Recommendation: Support the development of increasingly powerful and specialized
High-Performance Computing (HPC) and High-Throughput Computing (HTC) capabilities
for accelerator modeling, and maintenance of software to run eﬃciently on these hardware
(e.g., port of codes to GPUs) with eﬃcient and scalable I/O, post-processing and in situ data
analysis solutions, which will be needed to support ab initio modeling at increasing ﬁdelity,
training of surrogate models and AI/ML guided designs.

6.1 Hardware: CPU/GPU time, memory, archive

contributed by Jean-Luc Vay, Axel Huebl

Recommendation: Support the development of increasingly powerful and specialized
High-Performance Computing (HPC) and High-Throughput Computing (HTC) capabilities
for accelerator modeling, which will be needed to support ab initio modeling at increasing
ﬁdelity, interactive and parallel data analysis, training of surrogate models and AI/ML guided
designs.

Many activities in accelerator modeling research and design both push the envelope in High-
Performance Computing (HPC) as well as High-Throughput Computing (HTC). Typical HPC
problems are spearheaded by plasma-based accelerator modeling, where simulations are limited by
the available computational resources, whether in spatial grid resolutions, number of macroparticles,
number of time steps, even if using the most powerful supercomputers available at the time of
writing [233]. Emerging techniques like machine-guided optimization of ensemble studies and AI/ML
workﬂows are often HTC workﬂows, where many parallel runs need to be coordinated. Likewise,
parallel data analysis tasks are often interactive HTC workﬂows that furthermore can require
interactivity and elasticity of the allocated compute resources (e.g., Jupyter Notebooks). Over
time as computational resources available at leadership-scale computing facilities and individual
institutions grow, some prior HPC workﬂows become HTC workﬂows as exploration phases evolve
into design phases.

Some accelerator software have been or are being ported to GPUs and are being ready to run
eﬃciently on upcoming Exascale supercomputers [234, 235]. It is expected that alongside GPUs,

28

additional specialized compute hardware such as Field Programmable Gate Arrays (FPGA), reduced
instruction set (RISC) multicore-CPUs, tensor-processing units (TPUs) and specialized neural
engines will be part of near-term computing architectures.

While the mentioned improvements on GPUs will increase the ﬁdelity and accuracy of the
simulations, the bar of ﬁdelity and accuracy to aim at is being raised at the same time. The needs
in computational hardware are expected to increase in the foreseeable future, whether to simulate
up to every real particle, including in the halo, of beams that can be composite of many bunches, to
account with high ﬁdelity of collective eﬀects in complex geometries or to simulate beams for very
long times in conventional, plasma-based or structure-based wakeﬁeld colliders. While there is an
increasing amount of eﬀorts aimed at speeding up simulations with surrogate models, the training
of these models will rely on simulations, in addition to experimental data. Producing the large
volume of data from simulations will require tremendous needs in computational power, memory
and archiving capabilities.

6.2 Software performance, portability and scalability

contributed by Axel Huebl, David Sagan

Recommendation: Foster the development and maintenance of codes that run eﬃ-
ciently on the latest hardware (e.g., add support for GPUs/FPGAs) by using maintainable
single-source, portable solutions, and that are scalable on leadership-scale supercomputers
with multiple levels of parallelization and support for eﬀective dynamic load balancing.

6.2.1 Performance

Performance increase for modeling software is generally desirable, even outside of large-scale HPC
runs, and improves overall scientiﬁc productivity when studying particle accelerators at all scales.
For instance, most optimization and machine-learning workﬂows are GPU-accelerated as well to
increase computational throughput for studies, while ultraprecise numerical schemes can often use
GPU-accelerated linear algebra and FFT routines; thus GPU acceleration also beneﬁts small model
sizes.

Developing software for multiple GPU-platforms in parallel to existing CPU architectures is an
undertaking that requires the redesign of many existing algorithms and code-bases. Programming
models and languages evolve rapidly and the need to port algorithms to signiﬁcantly more ﬁne-
grained parallelism is a good incentive to rewrite and adopt modern software practices and popular
programming languages.

6.2.2 Portability

Portability of implementations is important as many scientists work with three major platforms
(Linux, macOS, Windows), although the former two (Unix-like) variants are certainly dominating
with HPC developers and machines. An additional challenge arises with the diversity in GPU and
other compute hardware, which ideally should be addressed with performance-portable software
design. In such a design, an algorithm is ideally implemented only in a single, abstract form and
then specialized/optimized for the speciﬁc hardware needs at compilation-time.

Performance portability relies on using and contributing to standard, industry-supported pro-
gramming languages to allow such a single-source approach. Performance portability layers (e.g.,
Kokkos [236], Alpaka [237], Raja [238]) are currently implemented predominantly as modern C++

29

libraries. The Julia language natively supports coroutines, multi-threading, distributed and GPU
computing. Other languages lacking signiﬁcantly in release time, compiler variety and robustness.
It is to be expected that performance portability architecture will evolve into standardized program-
ming language components, such as ISO C++, in coming years. HEP modeling needs can provide
important input to their standardization by openly sharing critical community use cases and codes
and interacting directly with computer scientists for development.

Common numerical and data management libraries (e.g., AMReX [239], CoPA [240]) build
on such performance portability layers and only as the next steps, domain-speciﬁc libraries and
applications are implemented to beneﬁt from all aforementioned developments.

6.2.3 Parallel scalability

Parallel scalability on leadership-scale machines continues to be only achievable with additional par-
allelization over multiple coupled compute nodes (each powered with CPUs or GPUs). Methods that
(a) split the computational domain into local sub-problems, and (b) ideally overlap communications
(usually of data in guard cell regions) between collaborating nodes with computation within each
node, are essential for such capability simulation runs. Furthermore, at large-scale, load balancing
of particle-based methods as well as reﬁnement of ﬁdelity needs (e.g., adaptive mesh-reﬁnement) of
the simulation domain become important to make optimal use of available resources.

Looking at the rapidly evolving HPC landscape, many themes in advancing accelerator and
beam modeling software are common, e.g., Poisson solvers, robust particle pushers, need to model
hybrid particle accelerators combining conventional and advanced accelerator elements, etc. It is
thus evident that collaborative eﬀorts between conventional and advanced accelerator development
can beneﬁt the community, relying for instance on similar or compatible low-level parallelization
layers, but also coordination when implementing and sharing domain-speciﬁc numerics can ensure
that scalable solution can be readily extended for research needs.

6.3 Scalable I/O and in-situ analysis

contributed by Axel Huebl, Jean-Luc Vay

Recommendation: Support the development and maintenance of eﬃcient and scalable
I/O, post-processing, in situ data analysis, and data sharing solutions in particle acceler-
ator codes. Coordinate on scientiﬁc data documentation, standardization, development of
interactive and reproducible analysis workﬂows and foster data reuse.

While modern programming models, software design and advanced algorithms ensure that
simulations can execute eﬃciently, one further challenge emerges to scientiﬁc productivity on HPC
hardware: When comparing the computational peak performance evolution to the parallel system
storage bandwidth of HPC machines as in Table 1, the gap between potential simulation ﬁdelity
that can be computed and data that can be stored for analysis widens with every new machine
generation.

Following this trend, modeling applications that run at a system’s capability and follow the
traditional workﬂow of simulation + post-processing will spend more of their time in large-scale
data input and output, which eventually can dominate the time-to-solution. Driven by this trend,
several steps can be taken by simulation developers.

As a ﬁrst step, adopting scalable, parallel I/O libraries with adequate data layouts [241] can
extent the applicability to truly make optimal use of parallel ﬁlesystems [242]. Implementing these

30

System Specs

Titan

Summit

Frontier

Peak Performance
Storage Bandwidth
Ratio

27 PFLOP 200 PFLOP >1.5 EFLOP
2-4x Summit
150-300:1

2.5 TB/s
80:1

1 TB/s
27:1

Evolution of

Table 1:
the Oak Ridge Leadership Com-
puting Facility with respect to peak performance and storage bandwidth, according to
https://www.olcf.ornl.gov/frontier/.

leadership-scale machines at

libraries in both data producing as well as data consuming codes opens another possibility to
standardize on and share the community solutions, as demonstrated in the openPMD project [99].
As a second step and long-term solution, data analysis and processing need to be transitioned to
in situ-processing, running online with the modeling simulation and avoiding traditional, ﬁle-based
long-term storage for raw data. This approach essentially moves the design of data analysis to the
setup and design phase of a simulation that, similar to an experimental setup, needs to plan the
locations, acceptance and dynamic ranges of “virtual diagnostics” before even starting a high-ﬁdelity
modeling run. Virtual diagnostics can reduce data sizes and thus data throughput needs by orders
of magnitudes, e.g., by calculating beam momenta, phase space histograms, spectra and even 3D
videos as a simulation runs instead of storing realistically trillions of particles in regular intervals
to disk for later analysis. While in-situ data processing and analysis are not new, and have even
been mainstream in some accelerator codes or frameworks (e.g., Warp [126], PIConGPU [243]),
the discrepancy between peak performance and storage bandwidth reported in Table 1 triggers the
development of community libraries that enable a more systematic description of new diagnostics in
an in-situ approach, ensuring high eﬃciency and scalability.

A challenge arises in the rapid setup of in situ processing pipelines, since typical approaches of
implementing a virtual detector in the simulation code itself can be more complicated compared to
scripted (serial) analysis workﬂows on ﬁles. Innovative software design can potentially alleviate this
challenge, with approaches such as streaming data with close-to-ﬁle-like analysis syntax [244] or
embedding of interpreters into running simulations to execute scripts on in-memory data. Continued
research in this area and close collaboration with computer science and engineering teams will be
essential to ensure that ﬂexible, science-case speciﬁc analysis can be designed by accelerator and
beam physicists.

7 Sustainability, reliability, user support, training

contributed by Axel Huebl, R´emi Lehe, David Sagan, Christopher
Mayes, Jean-Luc Vay, Kiersten Ruisard, Steven Lund

Recommendation: Provide suﬃcient resources for code maintenance, automated testing,
benchmarking, documentation and code reviews. Convene a community eﬀort to identify topics
and teaching teams to deliver academic classes designed to foster sharing and cooperation, to
be taught at the U.S. Particle accelerator school.

An important aspect of the development of scientiﬁc modeling tools is to ensure that the
corresponding software is robust, easy to use and can be extended for new science cases. This
is made more diﬃcult by the fact that these tools are oftentimes constantly evolving, with new
capabilities being continuously added to meet ever changing computational needs. Here we mention
a few important practices [245] that facilitate this process.

31

Accelerator simulation is a large, complex topic, and much time and eﬀort has been spent in
developing simulation software. Nevertheless, in the ﬁeld of accelerator physics, simulation code
development has often been a haphazard aﬀair. Due to developers retiring or moving on to other
projects, numerous simulation programs have been completely abandoned or are seldom used.

As simulation programs become more complex due to the ever-increasing demands placed upon
machine performance, the situation will become worse if not addressed. Such demands include
the accelerator and beam physics Grand Challenges that have been identiﬁed recently by the
community [91, 103]

• Increasing beam intensities by orders of magnitude.
• Increasing the beam phase-space density by orders of magnitude, towards the quantum

degeneracy limit.

• Complete and highly accurate start-to-end “virtual particle accelerators” simulations.
• Fast and accurate multi-objective optimization methods to speed up the design process.

Accelerator and beam modeling software development should allow for extensive testing of
new functionality while preserving demonstrated capabilities on previously validated scenarios[245].
Performance and interoperability must be constantly improved to increase understanding (multi-
physics problems) and optimization (machine learning). One must also ensure a transfer of knowledge
over generations of scientists in form of formal education and easy accessibility to the tools.

Addressing these Grand Challenges will require a community eﬀort to coordinate and modernize
the current set of modeling tools, with capabilities that extend far beyond what the current software
can do, including interdisciplinary simulations and advanced models for virtual prototyping of
complete accelerator systems.

7.1 Code robustness, validation & veriﬁcation, benchmarking, reproducibility

Recommendation: Establish and maintain open review, automated testing, validation
and benchmark procedures for modeling tools and ensure reproducibility by tracking all changes
in a documented and openly accessible manner.

Figure 1: There are a number of aspects that make software sustainable. Broadly, they can be
grouped into the “intrinsic” characteristics of the software itself and the “extrinsic” environment in
which the software is developed and used.

There are several aspects that must be addressed to enable the development of the quality
software that will be needed for the machines of tomorrow. One facet can be put under the rubric
of “software sustainability” which can be deﬁned as [246]:

32

Readability / UnderstandabilityExtensibility / EvolvabilityPortability / InstallabilityScalabilitySpeedLearnabilityDocumentationModularity / Reusability  Correctness / TestabilityAccessibility / Openly Available?--------Usefulness---Licensing-FundingCommunity-Version-Control---Actively Maintained--------IntrinsicExtrinsic“the capacity of the software to endure. In other words, sustainability means that
the software will continue to be available in the future, on new platforms, meeting new
needs.”

There are many aspects to software sustainability, as illustrated in Fig 1. Broadly, these
aspects can be grouped into the “intrinsic” characteristics of the software itself and the “extrinsic”
environment in which the software is developed and used. Software sustainability has been studied
academically and there is even a Software Sustainability Institute [247], which promotes “the
advancement of software in research by cultivating better, more sustainable, research software to
enable world-class research”.

As mentioned above, many software packages developed for simulating accelerators, while
showing excellent results for their speciﬁc application and their era, are not “sustainable” in the
long run. However, software sustainability is extremely important given the limited resources that
the accelerator community has for code development in conjunction with the even more limited
resources for maintaining codes. To meet future needs, it is imperative that there is a community
wide eﬀort to promote sustainable practices.

Because modeling tools are being continuously improved and extended by developers, there is
always a risk that a given change to the source code may introduce bugs and produce erroneous
results in some speciﬁc cases. It is therefore paramount that the developers track the changes to
the source code (with a version control tool such as git), and that they simultaneously maintain a
comprehensive suite of tests that the code should always successfully pass. In the case of scientiﬁc
modeling codes, these tests can consist of a number of physical setups for which there are well-known
It can thus be checked that the results of the code conform to these
theoretical predictions.
predictions. In order for these tests to be most eﬀective, they need to be run automatically whenever
the source code is modiﬁed – a process known as continuous integration.

In addition to validating against theoretical predictions, it is also critical to benchmark against
experimental observations. The relevance of code predictions to accelerator performance and output
may be limited by a number of factors, such as:

• results from experiments that are not fully characterized (e.g., magnet misalignments)
• initial beam distribution is unknown or incompletely known (e.g., beam phase-space projections
from experiments may not allow full reconstruction of beam 6-D phase-space accurately)
• diagnostics or simulations with insuﬃcient range or resolution to fully characterize relevant
parameters (e.g., phase space diagnostics that are sensitive to halo / beam loss levels [19])

• missing or incompletely described physics in simulations

Proper benchmarking of codes against theory and experiments is time-consuming and requires
dedicated resources.
In general, there are few resources dedicated to code benchmarking at
experimental facilities and in simulation teams. As the complexity of codes and the computers that
they run on increase, it is essential to ensure enough resources for proper benchmarkings, including
dedicated beam time and test stands for well-characterized experimental results [19].

In addition, the development of large-scale scientiﬁc tools nowadays involve a whole community
of developers and users rather than a single, isolated developer. Therefore, the robustness and
quality of the software is generally greatly improved by the adoption of collaborative development
platforms. These online platforms allow the developers to easily and eﬃciently review each other’s
changes to the source code, before these changes are incorporated in the mainline version of the code.
They also provide a central venue for users to raise issues and questions, and generally communicate
their needs with the developers.

33

Finally, another aspect of a code’s robustness is its interface with the user. As the code evolves,
the interface with the user often needs to be modiﬁed so as to enable functionalities or generalize
existing ones. It is not uncommon that these changes break some of the users’ scientiﬁc workﬂows.
Thus, it is important for the developers to regularly publish releases of the source code, where these
changes are clearly documented. The diﬀerent releases of a given software can be made citable
and available through an online archival service, so that users can roll back to a previous release if
needed and scrutinize changes in a reproducible manner.

7.2 Usability, user support and maintenance

Recommendation: Consider maintenance of code reviews for improvements, docu-
mentation, installation and testing to be central to the mission of usable scientiﬁc software:
establish open feedback and support channels, perform regular releases with change logs, use
permissive open source licensing whenever possible and cover all scientiﬁc functionality with
automated tests.

7.2.1 Usability

In order to maximize the impact of a given software tool on its scientiﬁc community, it is also key
that this tool be user-friendly. For example, a common obstacle to the adoption of some scientiﬁc
tools is their complex installation procedure, especially if many dependencies are involved. The
developers can drastically simplify the installation procedure of a given software tool by making it
available through modern package managers [248–250]. These package managers can automatically
download and compile the source code as well as its set of dependencies. They can also ensure
that compatible versions of the dependencies are being used. Importantly, some of these package
managers can allow users to have the exact same installation workﬂow on large-scale HPC resources
as on their desktop workstation.

Another key aspect of the user experience is the code documentation. For scientiﬁc modeling
tools, the documentation usually includes sections on how to install and use the code, as well as a
description of algorithm being used along with relevant references. Since, again, scientiﬁc tools are
often continuously evolving, it can be beneﬁcial that the documentation be generated automatically,
whenever the source code is modiﬁed or whenever a new release is published. Just as the modeling
tools themselves, documentation should be easily accessible for the whole community and easy to
improve from developers and users alike.

7.2.2 User support

In order for scientiﬁc codes to continuously improve and meet the needs of the community, it is
important to have a communication channel for users to provide feedback [245]. This feedback may
include potential bug reports, questions on installation issues, and inquiries on typical usage. Again,
many tools exist, including issue trackers (e.g., GitHub issues [251]), chat rooms (e.g., Gitter [252]
or Slack [253]), mailing list, forums, stack-overﬂow, etc. These diﬀerent tools have diﬀerent purposes
and target diﬀerent segments of the user pool. The aim is thus certainly not for a developer to
embrace all of them simultaneously, but rather to select one or two channels that are best suited
for their particular code. These communication channels should also be clearly identiﬁed in the
documentation.

It can also be helpful for these communications channels to be openly accessible and easily
searchable, so as to avoid that multiple users report the same issues. In this regard, some of the

34

conversations in chat rooms may sometimes have to be transferred to an issue tracker, or summarized
in the documentation.

Open user support channels establish signiﬁcant synergies not only across related accelerator
modeling projects, but with the computational physics community as a whole. For instance,
documented computational and numerical issues, testing approaches, chosen conventions and
implemented automation frameworks add signiﬁcantly to the eﬀectivity of the community, in terms
of development speed, issue triage, installation, solution/work-around propagation, communication
with high-performance computing centers and adoption of best-practices, to name a few.

7.2.3 Maintenance

It is now widely accepted, in the scientiﬁc community,
Version control and software releases -
that any signiﬁcant software development project should be managed with a version control system
(e.g., git [254]). However, a less common practice is that of regularly publishing new releases of the
code.

Code releases are important because scientiﬁc codes are constantly evolving and improving, and,
as part of this process, the code interface with the user may change, bugs may be ﬁxed, and new
bugs may occasionally be introduced. It is therefore not uncommon for a new version of the code to
suddenly break a workﬂow that a user previously relied on. Well-documented releases (e.g., with a
CHANGELOG document) help the user understand how the code has evolved with each release, and
how to update their workﬂow to adapt to these changes. Archiving each release (e.g., with tools
such as Zenodo [255]) may also allow the user to roll back to a previous version of the code, if a new
version is altogether incompatible with a prior workﬂow. Also here, readily available services can
speed up the generation of CHANGELOG documents and publication with package managers through
so-called automated deployments.

Access and licensing - over the last decade, it emerged that scrutinizable implementations
and low-barrier contributions to accelerator modeling software are highly desirable for scientiﬁc
productivity. Consequently, a signiﬁcant part of the community considers open source development
[256–258] and permissive open source licensing [259, 260] the preferred way for software access,
contribution, teaching, collaboration and reuse [245, 256, 261, 262]. For a few modules where such a
model is not possible, e.g., due to export control constrains, other community sharing methods such
as limited access through a memorandum of understanding (MoU) exist.

Automated testing -
In order to minimize the risk of new bugs, it is important to verify that
the code still works as expected, whenever the source code is modiﬁed. In the case of scientiﬁc
codes, this can be done for instance by comparing the results of the code against known solutions,
for a number of analytically-tractable problems. This process has long been done by hand, but
experience shows that it is time-consuming and may in practice be often omitted by the developers.
Instead, these tests can be done automatically and systematically, whenever the source code
is modiﬁed. Automated tests usually also results in faster code development, since the diﬀerent
contributors to the code can be rapidly assured that their changes do not break previous functional-
ities. A number of tools that are free to open source software allow to easily setup those automated
tests, including GitHub Actions [263], Azure Pipelines [264] and Travis-CI [265]. Automated test
on exotic hardware or HPC platforms is intricate, but addressed for instance by the E4S [266] eﬀort
within the Exascale Computing Project.

35

7.3 Training and education

Recommendation: Convene a community eﬀort to identify topics and teaching teams
to deliver academic classes designed to foster sharing and cooperation. The classes should
be taught in the US Particle Accelerator School with course materials and linked tutori-
als/extensions regularly maintained and publicly posted.

Training and education are key to the long term viability of positions put forth in this white paper.
University classes cover the core of essential programming skills, numerical algorithms, AI/ML, and
some software development and maintenance tools. Traditionally workers in the ﬁeld rely heavily on
online documentation and training materials as well as help from colleagues. Much beneﬁt is derived
from the larger plasma and EM modeling communities. Accelerator science and engineering also
relies on specialized training provided by the US Particle Accelerator School (USPAS). The USPAS
convenes two, intensive-format sessions per year to provide graduate-level training in topics that are
not practical to teach regularly at universities. Recommendations for enhancing the USPAS are
covered in a separate Education, Outreach, and Diversity white paper [267]. Speciﬁc to accelerator
modeling, USPAS oﬀerings have been primarily centered on code demonstrations and applications
in various accelerator dynamics, design, and applications classes. Recent classes have covered
optimization and ML, python/matlab applications, and levels of self-consistent modeling in beams
and plasmas. It would beneﬁt the community if more oﬀerings were regularly available in the
USPAS. The USPAS classes are also an excellent opportunity to foster community cooperation via
team teaching eﬀorts and to help ingrain productive collaborative work approaches in the younger
generation. Focused topical collaborations centered on classes needed could also be used to generate
needed tutorial materials as well as disseminate and archive useful code tools while demonstrating
the advantages of eﬀective use in classes. Tutorials and archives of materials from the courses would
improve modeling eﬀorts and encourage increased productivity via sharing.

8 Toward community ecosystems & data repositories

contributed by Jean-Luc Vay, Axel Huebl, R´emi Lehe,
Warren B. Mori, Frank Tsung, David Bruhwiler, Ji Qiang, David
Sagan

Recommendation: Organize the beam and accelerator modeling tools and community
through the development of (a) ecosystems of codes, libraries and frameworks that are inter-
operable via open community data standards, (b) open access data repositories for reuse and
community surrogate model training, (c) dedicated Centers and distributed consortia with
open community governance models and dedicated personnel to engage in cross-organization
and -industry development, standardization, application and evaluation of accelerator and
beam modeling software and data.

8.1 Loose integration: Integrated workﬂows

Recommendation: Foster the adoption and continued development of open community
(meta-)data standards and their implementation in modeling tools and data acquisition for
seamless integration of the community accelerator modeling tools into multiphysics workﬂows.

Integrated workﬂows are often needed to answer complex contemporary research questions,
such as the start-to-end description of a hybrid accelerator with conventional and plasma elements.

36

Figure 2: Longitudinal electric ﬁeld (in V/m) in a laser-driven plasma acceleration stage at two
times (top: t ≈ 300fs, bottom: t ≈ 600fs) along the laser propagation from 2-D PIC simulations
with: (left) Warp; (right) Osiris. Plots are based on rendering from the openPMD-viewer.

Furthermore, all optimization, AI/ML training as well as exploratory studies beneﬁt from establishing
and maintenance of workﬂows. These workﬂows might (a) link several codes together to solve a
larger problem or solve a problem with more physical processes, (b) benchmark and validate the
various codes against each other and known solutions, and (c) establish reproducibility for new
scientiﬁc results.

8.1.1 I/O standardization

At the moment, many simulation codes are developed independently with little coordination between
various groups. In order to establish workﬂows that span multiple applications, a typical approach
so far is to rely on ﬁle-based data exchange and code-speciﬁc high-level wrapping of input options.
Along these lines, the Consortium for Advanced Modeling of Particle Accelerators (CAMPA [268])
supports (among other activities) the development of standardized input and output formats through
the openPMD [99] and PICMI [269] projects.

The Open Standard for Particle-Mesh Data (openPMD) is a meta-data standard for research data,
adding scientiﬁc self-description on top of modern, portable, scalable ﬁle-formats such as ADIOS [270]
and HDF5 [271]. openPMD achieves this by collaboratively deﬁning an open standard document
that evolves in versions, similar to software. Community members then implement openPMD data
handling in their simulation codes and analysis types either directly against supported ﬁle formats
or use an open source library layer, openPMD-api [272].

The standard input format for Particle-In-Cell codes (PICMI) strives to unify the usage of
accelerator modeling codes by standardization on an API for simulation design. Conceptually,
PICMI deﬁnes a common, explicit PIC modeling interface (API) for a wide range of numerical and
initialization options. A PICMI script can be run with multiple codes, given that both implement
the requested features, and simpliﬁes comparability of physics cases tremendously. Code “backend”
choices can be picked based on performance or numerical needs, without breaking context for the
user and relearning the simulation design for each speciﬁc application.

As part of the CAMPA eﬀorts to foster compatibility and coordination between particle accel-
erator modeling applications, a code-validation workﬂow between the Warp [126] and Osiris [273]
code has been carried out. In the study on the two codes in Figure 2, both independent code bases
modeled the same physical setup of an LWFA (with a laser driving a wake in a plasma column)
and used standardized openPMD output and the same, community-developed post-processing tool

37

(openPMD-viewer) to analyze the data. A one-to-one comparison to machine precision between the
two codes was beyond the scope of the study because: (a) while using the same physical parameters
to initialize the simulations with each code, some secondary parameters were left unspeciﬁed such
as, e.g., the exact phase of the laser oscillations, (b) some numerical aspects are diﬀerent such as,
e.g., the method of initialization of the laser in the boosted frame. Nonetheless, it is rewarding to
observe on Figure 2 that the two codes predict very similar wake evolutions and dephasing of the
laser as it propagates through the plasma column.

Another series of comparisons is underway that also incorporates a common Python input script
using PICMI, and including more codes, in the workﬂow, to validate the workﬂow concept for
further adoption and development.

Another example is the integrated electromagnetics and beam dynamics simulation on high
performance computers using the ACE3P and the IMPACT codes [274]. In this case, the design
of the RF cavity using the ACE3P and the beam dynamics simulation using the IMPACT were
integrated into an optimization workﬂow for ﬁnal beam quality optimization.

8.2 Tighter integration: Integrated frameworks

Recommendation: Establish open, contributable, modular libraries and integrated
frameworks for central computing, modeling and analysis tasks that foster the sharing of
common functionalities between applications, using open licenses and best practices/policies.

Integrated developments of frameworks could implement more tightly coupled workﬂows on a
library level instead of relying on high-level application coupling. As presented at the beginning of
the section in the concept of an modular software ecosystem in [6], modular, cross-cutting software
designs can provide reusability, eﬃcient development investments, and compatibility. Built-in
features of lower-level software such as GPU-support, multi-node parallelism, translation to AI/ML
frameworks and multi-physics support can, if evolved together, be inherited by a wide community. In
particular, common or compatible in-memory data structures have not been attempted in accelerator
and beam modeling so far, but would provide signiﬁcant performance and software maintenance
beneﬁts for tightly-coupled, co-developed physics modules for integrated modeling frameworks that
address grand challenges.

Several community examples exist to demonstrate modular development. For instance, the
codes WarpX [233], HiPACE++ [275] and PIConGPU [243] as well as several analysis frameworks
share the development of the I/O library openPMD-api [272], which provides access to high-
performant, portable low-level I/O libraries and a standardized metadata schema [99]. WarpX,
FBPIC implement PICMI assisted by a shared, central interface library, with implementation
underway in other codes (e.g., OSIRIS). WarpX, HiPACE++ [276] and related codes share the data
structures from AMReX [239] and parts of their code bases.

8.2.1 Community development

The adoption of a more coordinated and collaborative approach is also driven by the need to
transition a large body of software from CPUs to GPUs, which is more disruptive than most past
transitions, including the one from serial to parallel codes. Furthermore, this latest transition
is anticipated to be one of many transitions to come to adapt to the rapidly evolving computer
hardware.

More realisms, better numerics and advanced algorithms are continuously being added to many
accelerator codes, and code developers have developed best practices for adding new modules while
maintaining the performance needed to run on state-of-the-art HPC centers around the world. Thus,

38

instead of porting solely to GPUs, accelerator modeling teams already teamed up with computer
scientists and embrace libraries that abstract computer hardware speciﬁc details for performance
and sustainability. Establishing more community standards, funding reliable software dependencies
and reusable physics modules are the logical steps to speed up development of code bases further.
An eﬃcient mean for guiding community development is the collaborative setting of and adherence
to common policies. Rather than starting from scratch, one can adopt an existing libraries and codes
and build a beam and accelerator modeling ecosystem on it. As a blueprint, one could build on
community development concepts of the Interoperable Design of Extreme-scale Application Software
(IDEAS) project [277] and its Extreme-scale Scientiﬁc Software Development Kit (xSDK) [278].

The main goal of the IDEAS project is to “help move scientiﬁc software development toward an
approach of building new applications as a composition of reusable, robust, and scalable software
components and libraries, using the best available practices and tools.”[277], while the xSDK project
is being developed to “provide a coordinated infrastructure for independent mathematical libraries
to support the productive and eﬃcient development of high-quality applications” [278]:

Rapid, eﬃcient production of high-quality, sustainable extreme-scale scientiﬁc applications is
best accomplished using a rich ecosystem of state-of-the art reusable libraries, tools, lightweight
frameworks, and deﬁned software methodologies, developed by a community of scientists who
are striving to identify, adapt, and adopt best practices in software engineering. The vision
of the xSDK is to provide infrastructure for and interoperability of a collection of related and
complementary software elements—developed by diverse, independent teams throughout the
high-performance computing (HPC) community—that provide the building blocks, tools, models,
processes, and related artifacts for rapid and eﬃcient development of high-quality applications.

Although xSDK targets “extreme-scale scientiﬁc applications” of relevance to exascale super-
computing, its derived community policies apply well to all scales of computing, from laptops to
clusters or Cloud computing. Hence, the paradigm and tools are readily applicable to the full set of
modeling needs of the particle accelerator community.

Individual packages in such an ecosystem are composable libraries, frameworks, and domain
components developed by individual groups in the community. Each package publishes technical
design documents, documentation (reference, tutorials, how-to guides), API deﬁnitions, and a
concrete implementation [245]. In order to solve a speciﬁc physics case, a typical application uses
functionalities from several packages, often in an innovative way.

There are many advantages to adopting policies and tools such as the IDEAS/xSDK:

• Community policies have been established over years by teams of specialists in scientiﬁc
software development and computing, and they include best practices in software development.
• For reusable components, the policies require the use of permissive open source licenses
(“Non-critical optional dependencies can use any OSI-approved license.”) that allow reuse
of source and binary code, including for commercial and proprietary applications, fostering
collaborations across laboratories, academia and industrial partners.

• A wide-ranging set of open source, interoperable tools from a variety of backgrounds (including
numerical solvers, e.g., Hypre, multi-parameter optimizer, e.g., LibEnsemble, and mesh-
reﬁnement frameworks, e.g., AMReX) can be combined and used as foundation of accelerator
and beam physics toolkits and codes.

• A wide community of developers that can help improve software capability and sustainability

across projects.

• Partial overlap in functionality is not problematic and in fact improves the diversity of the

whole.

39

• Time of development and maintenance of domain-speciﬁc software is greatly reduced, as the
domain scientists can concentrate on the domain-speciﬁc functionalities while lower-level,
cross-cutting, numerical packages are maintained by dedicated specialists.

• Portability across platforms (CPUs, GPUs, etc.) of low-level libraries ensures portability of
the software that is built upon them. Building new tools on these low-level libraries greatly
reduces the overall burden on the community for porting codes to new platforms.

• The domain-speciﬁc packages that are built, following established policies are portable across
platforms and interoperable, and can be further combined to form larger toolkits and codes.
• Since packages are reused across software and duplication is minimized, bugs and ineﬃciencies

are spotted earlier.

• Many codes, libraries and packages exist in the community that can be reused as building
blocks, and progressively modernized and blended in a module that is portable and runs
eﬃciently on modern CPUs and GPUs. Hence, not all functionalities have to be rewritten
from scratch at once, oﬀering a progressive path from the current status to an ecosystem of
accelerator modeling tools.

It is to be emphasized that the goal is not to propose that only one code be developed to
study, e.g., RF accelerators or plasma-based accelerators. On the contrary, the goal is to provide a
coordinated infrastructure that enables inclusive and collaborative development of modeling tools for
the community, and for these tools, to follow modern practices for scientiﬁc software: be portable,
eﬃcient, robust and leading to consistently accurate and reproducible results.

8.3 Data repositories

Recommendation: Establish open access data repositories and foster publishing of
modeled and measured accelerator & beam data to allow re-use (e.g. beam transport to
applications), model training (e.g. AI/ML), preservation, recasting and reinterpretation.

In current community practice, the reporting and archival of many modeling results are often
limited to prose and ﬁgures in papers. As accelerator systems and support for experimental
campaigns become more complex, it is desirable to establish reuse of modeling results, especially if
simulations themselves made use of signiﬁcant HPC resources.

Similar to needs in HEP detector data [279], it is desirable to systematically categorize and share
accelerator data, such as beam distributions, source information, ﬁeld maps, among others. The
beneﬁts of such archives are manifold: sharing data enables preservation, recasting, reinterpretation
[279] and meta-analysis of published results; archives establish training sets for AI/ML workﬂows,
calibration and benchmark cases for new models and theories, beam transport from one model
(and code) into another, design of hybrid particle accelerators combining conventional and plasma
elements/sources, accurate input for modeling at interaction points, etc. The standardization of
data and meta-data (see previous section) for machine-readability, ﬁndability, accessibility and reuse
would maximize the productivity when sharing data over such repositories.

Following Open Science practices [280, 281], such data archives and repositories should be
established in an open access manner [282], similar to existing data repositories in particle physics
experiments [283, 284]. Lastly, open science compatible data policies need to be advanced on
the national, laboratory and university level. For instance, as of today data sharing in the U.S.
Department of Energy laboratories is bound to ﬁrst assert (often individually) copyright for all
but raw data, which can produce signiﬁcant workﬂow overheads for scientists and entry burdens to
contribute to community data repositories. The policies for universities vary with individual states.

40

Clear advise also needs to be established for corner-cases such as publishing interactive notebooks
and code snippets, which traditionally fall under software copyright and publishing policies, with
data sets in a single and time-eﬀective authorization and license.

8.4 Centers & consortia, collaborations with industry

Recommendation: Organize the beam and accelerator modeling community through
the development of dedicated Centers and distributed consortia. Dedicate resources to adopt
open community governance models and dedicate personnel to engage in cross-organization
and -industry cooperation.

8.4.1 Centers & consortia

It quickly becomes clear that, in order to achieve what is described and proposed in the earlier
sections, a coherent and consolidated eﬀort is needed. This is best achieved in the form of dedicated
Centers for Accelerator and Beam Physics Modeling [285]. Other areas of computer science have
already embraced this. New colleges for computing are established at universities to consolidate
the dispersed computing eﬀorts of the various departments (e.g., MIT’s Schwarzman College of
Computing [286]), and new centers for Quantum Computing [287, 288] have been built. Exascale
computing has been embraced through the Exascale Computing Project [289]. The US Department
of Energy (DOE) has founded SciDAC [290] to accelerate progress in scientiﬁc computing across
the diﬀerent programs supported by DOE: Advanced Scientiﬁc Computing Research, Basic Energy
Sciences, Biological and Environmental Research, Fusion Energy Sciences, High-Energy Physics,
and Nuclear Physics.

The respective communities have beneﬁted strongly from these new centers and the partnerships

across disciplines.

Accelerator and Beam Physics Modeling would no doubt beneﬁt similarly. The centers can be at
a given location or distributed geographically and among institutions across laboratories, academia
and industrial partners. They would bring together domain scientists (computational accelerator
and beam physicists), applied mathematicians, computer scientists and software engineers with
collaborations across the full landscape of accelerator modeling. In addition, some of the computer
science centers mentioned above are already supporting accelerator modeling eﬀorts on which the
Centers for Accelerator and Beam Physics Modeling could build.

Depending on the overall size, the centers could enable part or all of the following:

• Community development and maintenance of codes using industry-standard quality processes

by dedicated, specialized teams [245].

• Collect libraries for ﬁeld solvers, particle trackers, and other modules.
• Provide a modular community ecosystem for multiphysics particle accelerator modeling and

design [291].

• Standardize input scripts, output data, lattice description and start-to-end workﬂows [100].
• Provide compatibility layers to use the same libraries and modules in a number of programming

languages.

• Development and maintenance of End-to-end Virtual Accelerators (EVA) [91].
• User support, high-quality and detailed documentations, online tutorials, and training.
• Easy-to-use, standardized, user interfaces for preparation and analysis of simulations.
• Automated tools for ensemble simulations for optimization with builtin AI/ML support.

41

• Suite of test problems with well-characterized solutions for benchmarking, quality assurance

and regression testing.

• Development, analysis and eﬃcient implementation of novel algorithms and numerical methods
(e.g., high-order solvers, symplectic multiparticle tracking, Fast Multipole Methods, adaptive
mesh reﬁnement).

• Providing a space to meet (physically or virtually) for the integration of developments
from contributors into larger codes, such as PhD projects from external groups, organizing
development hackathons, knowledge-transfer, and onboarding.

• Developing and organizing workshops for developers and users of codes alike. Inviting national

and international speakers/developers (travel/hosting funds).

• Interacting with existing schools, by developing and maintaining state-of-the-art educational

resources (e.g., tutorials, lectures) on codes.

• Exploration of novel use of machine learning for accelerator modeling, and, further in the

future, of quantum computing [92].

Multiple Centers can be organized through a Consortium (e.g., CAMPA [268]). Except for
special restrictions such as export control, it would be desirable for the software developed by the
Center to be open source, enabling crosschecking, testing and contribution by the community at
large, beyond the participants to the Center(s) [256].

8.4.2 Collaborations with industry

Eﬀective long-term collaboration between national laboratories, academia and industry will lead
to important beneﬁts for the entire HEP community. Labs and universities will have access to
better software with lower lifecycle costs [292]. Companies will be strengthened by knowledge
transfer from labs and universities. Computational scientists will be better able to concentrate on
core competencies, without spending time on user interface design, ease of use, cloud computing,
etc. Society will reap the beneﬁts of better science, more innovation, and stronger businesses.
State-of-the-art simulation codes will become readily available to students. Training time and
associated costs will be reduced, as new team members will become productive more quickly. This
will contribute to equity, diversity and inclusion (EDI), as barriers to entry are removed for scientists
in developing countries and for those at US institutions with less federal funding and no direct
access to code developers.

In order to facilitate the necessary collaborations and to provide the entire community with
conﬁdence that the software will be widely available and adequately supported over decades, an open
source license is required for the industry software and can be very helpful for the entire software
ecosystem [256]. This imposes an open source business model on the corresponding businesses, at
least with regard to this speciﬁc activity. The software design objectives must include seamless
integration with legacy codes, low barrier to entry for new users, easily moving between GUI and
command-line modes, cataloging of provenance to aid reproducibility, and simpliﬁed collaboration
through multi-modal sharing.

9 Outlook

Computer simulations will continue to be essential to particle accelerator research, design and
operation. Its relative importance is even expected to grow, thanks to improvements in algorithms,
computer hardware, and new opportunities in machine learning and quantum computing. These will
enable accelerator modeling capabilities that include more physics that is integrated self-consistently

42

to model accelerators with ever increasing ﬁdelity and accuracy, toward the ultimate realization of
the grand challenge of virtual twins of particle accelerators. A more collaborative and coordinated
approach that enables the development of community ecosystems, adopting best practices in software
developments and maintenance, is needed to meet the challenge in a realistic budget envelope and
timeframe.

References

[1] Accelerating Discovery: A Strategic Plan for Accelerator R&D in the U.S., Report of the High
Energy Physics Advisory Panel (HEPAP) Accelerator Research and Development Subpanel,
2015. https : / / science . osti . gov/ - /media / hep / hepap / pdf / Reports / Accelerator RD
Subpanel Report.pdf.

[2] Advanced Accelerator Concepts Research Roadmap Workshop Report. 2016. url: https:
/ / science . energy . gov / ∼ / media / hep / pdf / accelerator - rd - stewardship / Advanced
Accelerator Development Strategy Report.pdf.

[3] ALEGRO. “Towards an Advanced Linear International Collider”. In: (Jan. 2019). url:

http://arxiv.org/abs/1901.10370.

[4] S. Nagaitsev et al. “Accelerator and Beam Physics Research Goals and Opportunities”. In:

(Jan. 2021). url: https://arxiv.org/abs/2101.04107.

[5] D. Sagan et al. “Simulations of future particle accelerators: issues and mitigations”. In: Journal
of Instrumentation 16.10 (Oct. 2021), T10002. doi: 10.1088/1748-0221/16/10/t10002. url:
https://doi.org/10.1088/1748-0221/16/10/t10002.

[6] J.-L. Vay et al. “Modeling of advanced accelerator concepts”. In: Journal of Instrumentation
16.10 (Oct. 2021), T10003. doi: 10.1088/1748-0221/16/10/t10003. url: https://doi.org/
10.1088/1748-0221/16/10/t10003.

[7] Carole Goble. “Better Software, Better Research”. In: IEEE Internet Computing 18.5 (2014),

pp. 4–8. doi: 10.1109/MIC.2014.88.

[8] A. Fiedman et al. “3D particle simulation of beams using the warp code”. In: Part. Accel.

37-38 (1992), pp. 131–139.

[9] Ji Qiang et al. “An Object-Oriented Parallel Particle-in-Cell Code for Beam Dynamics
Simulation in Linear Accelerators”. In: Journal of Computational Physics 163.2 (2000),
pp. 434–451. issn: 0021-9991. doi: https://doi.org/10.1006/jcph.2000.6570.

[10] J. Amundson et al. “Synergia: An accelerator modeling tool with 3-D space charge”. In:
Journal of Computational Physics 211.1 (2006), pp. 229–248. issn: 0021-9991. doi: https:
//doi.org/10.1016/j.jcp.2005.05.024.

[11] Ji Qiang et al. “Three-dimensional quasistatic model for high brightness beam dynamics
simulation”. In: Physical Review Special Topics - Accelerators and Beams 9.4 (Apr. 2006),
p. 044204. issn: 1098-4402. doi: 10.1103/PhysRevSTAB.9.044204. url: https://link.aps.
org/doi/10.1103/PhysRevSTAB.9.044204.

[12] M.A. Furman et al. “Self-consistent 3D modeling of electron cloud dynamics and beam re-
sponse”. In: Proceedings of the IEEE Particle Accelerator Conference. 2007. isbn: 1424409179.
doi: 10.1109/PAC.2007.4441093.

43

[13] Andreas Adelmann et al. “OPAL a Versatile Tool for Charged Particle Accelerator Simula-
tions”. In: arXiv:1905.06654 [physics] (May 2019). arXiv: 1905.06654. url: http://arxiv.
org/abs/1905.06654 (visited on 05/17/2019).

[14] C. Mayes. “Computational approaches to Coherent Synchrotron Radiation in two and three

dimensions”. In: Journal of Instrumentation 16 (2021), P10010.

[15] P. Yu, Y. Wang, and W. Huang. “Lattice and beam dynamics for the pulse mode of the
laser-electron storage ring for a Compton x-ray source”. In: Phys. Rev. ST Accel. Beams 12
(2009), p. 061301.

[16] J. Qiang, R. D. Ryne, and S. Habib. “Self-consistent Langevin simulation of Coulomb
collisions in charged-particle beams”. In: Proceedings of the IEEE/ACM SC2000 Conference.
Dallas Texas, USA, Nov. 4–10, 2000.

[17] He Zhang and Martin Berz. “The fast multipole method in the diﬀerential algebra frame-
work”. In: Nuclear Instruments and Methods in Physics Research Section A: Accelerators,
Spectrometers, Detectors and Associated Equipment 645.1 (2011). The Eighth International
Conference on Charged Particle Optics, pp. 338–344. issn: 0168-9002. doi: https://doi.
org/10.1016/j.nima.2011.01.053.

[18] A. Al Marzouk et al. “Eﬃcient algorithm for high ﬁdelity collisional charged particle beam

dynamics”. In: Phys. Rev. ST Accel. Beams 24 (2021), p. 074601.

[19] Kiersten Ruisard, Alexander Aleksandrov, and Sarah Cousineau. “Loss prediction through
modeling of high dynamic range beam distributions”. In: Snowmass21 LOI (2020). url:
https://www.snowmass21.org/docs/files/summaries/AF/SNOWMASS21-AF1 AF2 Kiersten
Ruisard-145.pdf.

[20] L. Xiao et al. “Advances in Multiphysics Modeling for Parallel Finite-Element Code Suite
ACE3P”. In: IEEE J. Multiscale and Multiphysics Comp. Tech. 4 (2019), pp. 298–306. doi:
10.1109/JMMCT.2019.2954946.

[21] V. Akcelik et al. “SRF Cavity Imperfection Studies Using Advanced Shape Uncertainty
Quantiﬁcation Tools”. In: Proceedings of LINAC08. Victoria, BC, Canada, Sept. 29–Oct. 3,
2008.

[22] A. Lunin et al. “Statistical analysis of the eigenmode spectrum in the SRF cavities with
mechanical imperfections”. In: Proceedings of 13th Int. Computational Accelerator Physics
Conf. Key West, FL, USA, Sept. 20–24, 2018.

[23] C K Birdsall and A B Langdon. Plasma Physics Via Computer Simulation. Adam-Hilger,

1991, Xxvi+479 Pp. isbn: 0 07 005371 5.

[24] B B Godfrey and MISSION RESEARCH CORP ALBUQUERQUE NM. The IPROP Three-
Dimensional Beam Propagation Code. Defense Technical Information Center, 1985. url:
https://books.google.com/books?id=hos OAAACAAJ.

[25] A F Lifschitz et al. “Particle-in-Cell modelling of laser plasma interaction using Fourier
decomposition”. In: Journal of Computational Physics 228.5 (2009), pp. 1803–1814. issn:
0021-9991. doi: http://dx.doi.org/10.1016/j.jcp.2008.11.017. url: http://www.
sciencedirect.com/science/article/pii/S0021999108005950.

[26] C. Benedetti et al. “Eﬃcient modeling of laser-plasma accelerators with INF&RNO”. In:

AIP conference proceedings 1299.1 (2010), pp. 250–255.

44

[27] A. Davidson et al. “Implementation of a hybrid particle code with a PIC description in
r–z and a gridless description in φ into OSIRIS”. In: Journal of Computational Physics 281
(2015), p. 1063.

[28] P Sprangle, E Esarey, and A Ting. “Nonlinear-Theory Of Intense Laser-Plasma Interactions”.

In: Physical Review Letters 64.17 (Apr. 1990), pp. 2011–2014. issn: 0031-9007.

[29] P. Mora and T. M. Antonsen. “Kinetic modeling of intense, short laser pulses propagating in

tenuous plasmas”. In: Physics of Plasmas (1997), p. 217.

[30] C. K. Huang et al. “QUICKPIC: A highly eﬃcient particle-in-cell code for modeling wakeﬁeld

acceleration in plasmas”. In: Journal of Computational Physics 217 (2006), p. 658.

[31] W. An ad V. K. Decyk, W. B. Mori, and T. M. Atonsen. “An improved iteration loop for
thre three dimensionnal quasi-static particle-in-cell algorithm: QuickPIC”. In: Jour. Comp.
Phys. 250 (2013), pp. 165–177.

[32] T Mehrling et al. “HiPACE: a quasi-static particle-in-cell code”. In: Plasma Physics and
Controlled Fusion 56.8 (July 2014), p. 084012. doi: 10.1088/0741-3335/56/8/084012. url:
https://doi.org/10.1088/0741-3335/56/8/084012.

[33] F. Li et al. “A quasi-static particle-in-cell algorithm based on an azimuthal Fourier decompo-
sition for highly eﬃcient simulations of plasma-based acceleration: QPAD”. In: Computer
Physics Communications 261 (2021), p. 107784.

[34] J.-L. Vay. “Noninvariance Of Space- And Time-Scale Ranges Under A Lorentz Transformation
And The Implications For The Study Of Relativistic Interactions”. In: Physical Review Letters
98.13 (2007), pp. 1–4. issn: 0031-9007.

[35] J. Picard and al. Generation of 1 GW of 11.7 GHz Power using a Metamaterial-based
Power Extractor for Structure-based Wakeﬁeld Acceleration. Presented at the APS April
Meeting 2021, Volume 66, Number 5. 2021. url: https://meetings.aps.org/Meeting/
APR21/Session/T08.1.

[36] C. Jing et al. “Electron acceleration through two successive electron beam driven wakeﬁeld
acceleration stages”. In: Nuclear Instruments and Methods in Physics Research Section A:
Accelerators, Spectrometers, Detectors and Associated Equipment 898 (2018), pp. 72–76.
issn: 0168-9002. doi: https : / / doi . org / 10 . 1016 / j . nima . 2018 . 05 . 007. url: https :
//www.sciencedirect.com/science/article/pii/S0168900218305928.

[37] B. D. O’Shea et al. “Observation of acceleration and deceleration in gigaelectron-volt-per-
metre gradient dielectric wakeﬁeld accelerators”. In: Nature Communications 7.1 (Sept. 2016),
p. 12763. issn: 2041-1723. doi: 10.1038/ncomms12763. url: https://doi.org/10.1038/
ncomms12763.

[38] Q. Gao et al. “Observation of High Transformer Ratio of Shaped Bunch Generated by
an Emittance-Exchange Beam Line”. In: Phys. Rev. Lett. 120 (11 Mar. 2018), p. 114801.
doi: 10 . 1103 / PhysRevLett . 120 . 114801. url: https : / / link . aps . org / doi / 10 . 1103 /
PhysRevLett.120.114801.

[39] R. Roussel et al. “Single-Shot Characterization of High Transformer Ratio Wakeﬁelds
in Nonlinear Plasma Acceleration”. In: Phys. Rev. Lett. 124 (4 Jan. 2020), p. 044802.
doi: 10 . 1103 / PhysRevLett . 124 . 044802. url: https : / / link . aps . org / doi / 10 . 1103 /
PhysRevLett.124.044802.

45

[40] S. S. Baturin and A. Zholents. “Stability condition for the drive bunch in a collinear
wakeﬁeld accelerator”. In: Phys. Rev. Accel. Beams 21 (3 Mar. 2018), p. 031301. doi:
10 . 1103 / PhysRevAccelBeams . 21 . 031301. url: https : / / link . aps . org / doi / 10 . 1103 /
PhysRevAccelBeams.21.031301.

[41] ACE3P. url: https://portal.slac.stanford.edu/sites/ard public/acd/Pages/Default.

aspx.

[42] WarpX. url: https://github.com/ECP-WarpX/WarpX.

[43] Advanced Accelerator Development Strategy Report: DOE Advanced Accelerator Concepts
Research Roadmap Workshop. English. Tech. rep. Sponsor Org.: USDOE Oﬃce of Science
(SC), High Energy Physics (HEP). United States, 2016. doi: 10.2172/1358081. url: https:
//doi.org/10.2172/1358081.

[44] S. Nagaitsev et al. Accelerator and Beam Physics Research Goals and Opportunities. 2021.

arXiv: 2101.04107 [physics.acc-ph].

[45] A. A. Sahai. “Nanomaterials Based Nanoplasmonic Accelerators and Light-Sources Driven by
Particle- Beams”. In: IEEE Access 9 (2021), pp. 54831–54839. doi: 10.1109/ACCESS.2021.
3070798. url: https://ieeexplore.ieee.org/document/9395099.

[46] A. A. Sahai. “Emergence of TeraVolts per meter plasmonics using relativistic surface plas-
monic modes”. In: Proc. Vol. 11797, Plasmonics: Design, Materials, Fabrication, Charac-
terization, and Applications XIX. Vol. 117972A. SPIE, 2021. doi: 10.1117/12.2596637.
url: https://www.spiedigitallibrary.org/conference- proceedings- of- spie/11797/
2596637/Emergence-of-TeraVolts-per-meter-plasmonics-using-relativistic-surface-
plasmonic/10.1117/12.2596637.short?SSO=1.

[47] A. A. Sahai. anostructure nanoplasmonic accelerator, high-energy photon source, and re-
lated methods. 2021. url: https://patentimages.storage.googleapis.com/55/0a/6c/
7b757efb9e9547/WO2021216424A1.pdf.

[48] A. A. Sahai. “Plasmonic nano-focusing of particle beams by surface crunch-in plasmons
excited in tapered tubes”. In: Proc. SPIE 11999, Ultrafast Phenomena and Nanophoton-
ics XXVI. Vol. 1199903. SPIE, 2022. doi: 10 . 1117 / 12 . 2605722. url: https : / / www .
spiedigitallibrary.org/conference-proceedings-of-spie/11999/2605722/Plasmonic-
nano-focusing-of-particle-beams-by-surface-crunch-in/10.1117/12.2605722.short.

[49] D. Bohm and D. Pines. “A Collective Description of-Electron Interactions: III. Coulomb
Interactions in a Degenerate Electron Gas”. In: Phys. Rev. 92 (1953), p. 609. doi: 10.1109/
ACCESS.2021.3070798. url: https://ieeexplore.ieee.org/document/9395099.

[50] A. Sahai and collaborators. PetaVolts per meter Plasmonics: Snowmass21 White Paper. 2022.

url: https://arxiv.org/abs/2203.11623.

[51] J. M. Dawson. “Nonlinear Electron Oscillations in a Cold Plasma”. In: Phys. Rev. (1959)
113 (1959), p. 383. doi: 10.1109/ACCESS.2021.3070798. url: https://ieeexplore.ieee.
org/document/9395099.

[52] C. G. R. Geddes et al. “High-quality electron beams from a laser wakeﬁeld accelerator using
plasma-channel guiding”. In: Nature 431.7008 (Sept. 30, 2004), pp. 538–541. issn: 0028-0836.
url: http://dx.doi.org/10.1038/nature02900.

[53] W. P. Leemans et al. “GeV electron beams from a centimetre-scale accelerator”. In: Nat Phys
2.10 (Oct. 2006), pp. 696–699. issn: 1745-2473. url: http://dx.doi.org/10.1038/nphys418.

46

[54] T. P. A. Ibbotson et al. “Laser-wakeﬁeld acceleration of electron beams in a low density
plasma channel”. In: Phys. Rev. ST Accel. Beams 13 (3 Mar. 2010), p. 031301. doi: 10.
1103/PhysRevSTAB.13.031301. url: http://link.aps.org/doi/10.1103/PhysRevSTAB.13.
031301.

[55] W. P. Leemans et al. “Multi-GeV Electron Beams from Capillary-Discharge-Guided Sub-
petawatt Laser Pulses in the Self-Trapping Regime”. In: Phys. Rev. Lett. 113 (24 Dec. 2014),
p. 245002. doi: 10.1103/PhysRevLett.113.245002. url: https://link.aps.org/doi/10.
1103/PhysRevLett.113.245002.

[56] B. E. Blue et al. “Plasma-Wakeﬁeld Acceleration of an Intense Positron Beam”. In: Phys.
Rev. Lett. 90 (21 May 2003), p. 214801. doi: 10 . 1103 / PhysRevLett . 90 . 214801. url:
https://link.aps.org/doi/10.1103/PhysRevLett.90.214801.

[57]

Ian Blumenfeld et al. “Energy doubling of 42 GeV electrons in a metre-scale plasma wakeﬁeld
accelerator”. In: Nature 445 (Feb. 15, 2007), 741 EP -. url: http://dx.doi.org/10.1038/
nature05538.

[58] M. Litos et al. “High-eﬃciency acceleration of an electron beam in a plasma wakeﬁeld
accelerator”. In: Nature 515 (Nov. 5, 2014), 92 EP -. url: http://dx.doi.org/10.1038/
nature13882.

[59] Spencer Gessner et al. “Demonstration of a positron beam-driven hollow channel plasma
wakeﬁeld accelerator”. In: Nature Communications 7 (June 2, 2016), 11785 EP -. url:
http://dx.doi.org/10.1038/ncomms11785.

[60] S. Diederichs et al. “Positron transport and acceleration in beam-driven plasma wakeﬁeld
accelerators using plasma columns”. In: Phys. Rev. Accel. Beams 22 (8 Aug. 2019), p. 081301.
doi: 10.1103/PhysRevAccelBeams.22.081301. url: https://link.aps.org/doi/10.1103/
PhysRevAccelBeams.22.081301.

[61] J. van Tilborg et al. “Active Plasma Lensing for Relativistic Laser-Plasma-Accelerated Elec-
tron Beams”. In: Phys. Rev. Lett. 115 (18 Oct. 2015), p. 184802. doi: 10.1103/PhysRevLett.
115.184802. url: http://link.aps.org/doi/10.1103/PhysRevLett.115.184802.

[62] S. Steinke et al. “Multistage coupling of independent laser-plasma accelerators”. In: Nature
530.7589 (Feb. 11, 2016), pp. 190–193. issn: 0028-0836. url: http://dx.doi.org/10.1038/
nature16525.

[63] R. Lehe et al. “Laser-plasma lens for laser-wakeﬁeld accelerators”. In: Phys. Rev. ST Accel.
Beams 17 (12 Dec. 2014), p. 121301. doi: 10.1103/PhysRevSTAB.17.121301. url: https:
//link.aps.org/doi/10.1103/PhysRevSTAB.17.121301.

[64] C. E. Doss et al. “Laser-ionized, beam-driven, underdense, passive thin plasma lens”. In: Phys.
Rev. Accel. Beams 22 (11 Nov. 2019), p. 111001. doi: 10.1103/PhysRevAccelBeams.22.111001.
url: https://link.aps.org/doi/10.1103/PhysRevAccelBeams.22.111001.

[65] R. D’Arcy et al. “Tunable Plasma-Based Energy Dechirper”. In: Phys. Rev. Lett. 122 (3 Jan.
2019), p. 034801. doi: 10.1103/PhysRevLett.122.034801. url: https://link.aps.org/doi/
10.1103/PhysRevLett.122.034801.

[66] Paul Scherkl et al. Plasma-photonic spatiotemporal synchronization of relativistic electron

and laser beams. 2019. arXiv: 1908.09263 [physics.plasm-ph].

[67] G. A. Bagdasarov et al. “Laser beam coupling with capillary discharge plasma for laser
wakeﬁeld acceleration applications”. In: Physics of Plasmas 24.8 (Aug. 7, 2017), p. 083109.
issn: 1070-664X. doi: 10.1063/1.4997606. url: https://doi.org/10.1063/1.4997606.

47

[68] C. V. Pieronek et al. “Laser-heated capillary discharge waveguides as tunable structures for
laser-plasma acceleration”. In: Physics of Plasmas 27.9 (2020), p. 093101. doi: 10.1063/5.
0014961. eprint: https://doi.org/10.1063/5.0014961. url: https://doi.org/10.1063/5.
0014961.

[69] C Benedetti et al. “An accurate and eﬃcient laser-envelope solver for the modeling of laser-
plasma accelerators”. In: Plasma Physics and Controlled Fusion 60.1 (Oct. 2017), p. 014002.
doi: 10.1088/1361-6587/aa8977. url: https://doi.org/10.1088/1361-6587/aa8977.

[70] G. A. Bagdasarov et al. “Creation of an axially uniform plasma channel in a laser-assisted
capillary discharge”. In: Physics of Plasmas 28.5 (2021), p. 053104. doi: 10.1063/5.0046428.
eprint: https://doi.org/10.1063/5.0046428. url: https://doi.org/10.1063/5.0046428.

[71] A. J. Gonsalves et al. “Petawatt Laser Guiding and Electron Beam Acceleration to 8 GeV
in a Laser-Heated Capillary Discharge Waveguide”. In: Phys. Rev. Lett. 122 (8 Feb. 2019),
p. 084801. doi: 10.1103/PhysRevLett.122.084801. url: https://link.aps.org/doi/10.
1103/PhysRevLett.122.084801.

[72] J. van Tilborg et al. “Nonuniform discharge currents in active plasma lenses”. In: Phys. Rev.
Accel. Beams 20 (3 Mar. 2017), p. 032803. doi: 10.1103/PhysRevAccelBeams.20.032803.
url: https://link.aps.org/doi/10.1103/PhysRevAccelBeams.20.032803.

[73] C. A. Lindstrøm et al. “Emittance Preservation in an Aberration-Free Active Plasma Lens”.
In: Phys. Rev. Lett. 121 (19 Nov. 2018), p. 194801. doi: 10.1103/PhysRevLett.121.194801.
url: https://link.aps.org/doi/10.1103/PhysRevLett.121.194801.

[74] N M Cook et al. “Modeling of capillary discharge plasmas for wakeﬁeld acceleration and
beam transport”. In: Journal of Physics: Conference Series 1596 (Sept. 2020), p. 012063. doi:
10.1088/1742-6596/1596/1/012063. url: https://doi.org/10.1088/1742-6596/1596/1/
012063.

[75] R. J. Shalloo et al. “Hydrodynamic optical-ﬁeld-ionized plasma channels”. In: Phys. Rev. E
97 (5 May 2018), p. 053203. doi: 10.1103/PhysRevE.97.053203. url: https://link.aps.
org/doi/10.1103/PhysRevE.97.053203.

[76] B. Miao et al. “Optical Guiding in Meter-Scale Plasma Waveguides”. In: Phys. Rev. Lett.
125 (7 Aug. 2020), p. 074801. doi: 10.1103/PhysRevLett.125.074801. url: https://link.
aps.org/doi/10.1103/PhysRevLett.125.074801.

[77] A. Picksley et al. “Guiding of high-intensity laser pulses in 100-mm-long hydrodynamic optical-
ﬁeld-ionized plasma channels”. In: Phys. Rev. Accel. Beams 23 (8 Aug. 2020), p. 081303.
doi: 10.1103/PhysRevAccelBeams.23.081303. url: https://link.aps.org/doi/10.1103/
PhysRevAccelBeams.23.081303.

[78] A. Diaw et al. “Impact of electron transport models on capillary discharge plasmas”. In:
Physics of Plasmas 29.6 (2022), p. 063101. doi: 10.1063/5.0091809. eprint: https://doi.
org/10.1063/5.0091809. url: https://doi.org/10.1063/5.0091809.

[79] X. L. Xu et al. “Physics of Phase Space Matching for Staging Plasma and Traditional
Accelerator Components Using Longitudinally Tailored Plasma Proﬁles”. In: Phys. Rev.
Lett. 116 (12 Mar. 2016), p. 124801. doi: 10.1103/PhysRevLett.116.124801. url: https:
//link.aps.org/doi/10.1103/PhysRevLett.116.124801.

[80] A. Deng et al. “Generation and acceleration of electron bunches from a plasma photocathode”.
In: Nature Physics 15.11 (2019), pp. 1156–1160. issn: 1745-2481. doi: 10.1038/s41567-019-
0610-9. url: https://doi.org/10.1038/s41567-019-0610-9.

48

[81] Y. Xi et al. “Hybrid modeling of relativistic underdense plasma photocathode injectors”. In:
Phys. Rev. ST Accel. Beams 16 (3 Mar. 2013), p. 031303. doi: 10.1103/PhysRevSTAB.16.
031303. url: https://link.aps.org/doi/10.1103/PhysRevSTAB.16.031303.

[82] G. G. Manahan et al. “Advanced schemes for underdense plasma photocathode wakeﬁeld
accelerators: pathways towards ultrahigh brightness electron beams”. In: Philosophical
Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences
377.2151 (Aug. 12, 2019), p. 20180182. doi: 10.1098/rsta.2018.0182. url: https://doi.
org/10.1098/rsta.2018.0182.

[83] S. Gessner and the AWAKE Collaboration. Evolution of a plasma column measured through
modulation of a high-energy proton beam. 2020. arXiv: 2006.09991 [physics.acc-ph].
[84] S. Gourlay et al. “The US Magnet Development Program Plan”. In: (2016). url: https:

//osti.gov/scitech/servlets/purl/1306334.

[85] D. Arbelaez et al. “Numerical Modeling for Superconducting Accelerator Magnets”. In:
Snowmass21 LOI (2020). url: https : / / www . snowmass21 . org / docs / files / summaries /
CompF/SNOWMASS21-CompF2 CompF0-AF7 AF0-027.pdf.

[86] Julien Dular, Christophe Geuzaine, and Benot Vanderheyden. “Finite-Element formulations
for systems with high-temperature superconductors”. In: IEEE Transactions on Applied
Superconductivity 30.3 (2019), pp. 1–13. issn: 1051-8223. doi: 10.1109/tasc.2019.2935429.
url: https://doi.org/10.1109/tasc.2019.2935429.

[87] Julien Dular et al. “On the stability of mixed ﬁnite-element formulations for high-temperature
superconductors”. In: IEEE Transactions on Applied Superconductivity 31.6 (2021), pp. 1–
12. issn: 1051-8223. doi: 10.1109/tasc.2021.3098724. eprint: 2106.00313. url: https:
//doi.org/10.1109/tasc.2021.3098724.

[88] Alexandre Arsenault, Frdric Sirois, and Francesco Grilli. “Implementation of the h-φ formula-
tion in COMSOL Multiphysics for simulating the magnetization of bulk superconductors and
comparison with the h-formulation”. In: IEEE Transactions on Applied Superconductivity
31.2 (2021), pp. 1–11. issn: 1051-8223. doi: 10.1109/tasc.2020.3033998. eprint: 2006.13784.
url: https://doi.org/10.1109/tasc.2020.3033998.

[89] Alexandre Arsenault, Bruno de Sousa Alves, and Frdric Sirois. “COMSOL Implementation
of the h-φ-formulation with thin cuts for modeling superconductors with transport currents”.
In: IEEE Transactions on Applied Superconductivity 31.6 (2021), pp. 1–9. issn: 1051-8223.
doi: 10.1109/tasc.2021.3097245. url: https://doi.org/10.1109/tasc.2021.3097245.

[90] Bruno de Sousa Alves et al. “Thin-shell approach for modeling superconducting tapes in the

h-φ ﬁnite-element formulation”. In: arXiv (2021). h-phi thin. eprint: 2108.08828.

[91] J.-L. Vay et al. “End-to-End Virtual Accelerators (EVA)”. In: Snowmass21 LOI (2020). url:
https://www.snowmass21.org/docs/files/summaries/CompF/SNOWMASS21-CompF2 CompF0-
AF1 AF0 Vay-067.pdf.

[92] R. Lehe et al. “Machine learning and surrogates models for simulation-based optimization of
accelerator design”. In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/docs/
files/summaries/CompF/SNOWMASS21-CompF2 CompF3-AF1 AF6 Lehe-075.pdf.

[93] D. Winklehner and A. Adelmann. “Application of Machine Learning to Particle Accelerator
Simulations”. In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/docs/files/
summaries/CompF/SNOWMASS21-CompF3 CompF0-AF1 AF0 Winklehner-108.pdf.

49

[94] C. Hovater et al. “Operation of the CEBAF 100 MV Cryomodules”. In: Proc. of Linear Accel-
erator Conference (LINAC’16), East Lansing, MI, USA, 25-30 September 2016 (East Lansing,
MI, USA). Linear Accelerator Conference 28. Geneva, Switzerland: JACoW, May 2017, pp. 65–
67. isbn: 978-3-95450-169-4. doi: https://doi.org/10.18429/JACoW- LINAC2016- MOOP11.
url: http://jacow.org/linac2016/papers/moop11.pdf.

[95] J. R. Lewandowski et al. “RF Gun Dark Current Suppression with a Transverse Deﬂecting
Cavity at LCLS”. In: Proc. 37th Int. Free Electron Laser Conf. (FEL’15) (Daejeon, Ko-
rea). https://doi.org/10.18429/JACoW-FEL2015-WEP001. JACoW Publishing, Aug. 2015,
pp. 583–586. doi: doi:10.18429/JACoW-FEL2015-WEP001.

[96] J. Allison et al. “Recent developments in Geant4”. In: Nuclear Instruments and Methods in
Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment
835 (2016), pp. 186–225.

[97] Alfredo Ferrari et al. “FLUKA: A multi-particle transport code (Program version 2005)”. In:

(Oct. 2005). doi: 10.2172/877507.

[98] Nikolai V. Mokhov and Catherine C. James. “The MARS Code System User’s Guide Version

15(2016)”. In: (Feb. 2017). doi: 10.2172/1462233.

[99] Axel Huebl et al. “openPMD: A meta data standard for particle and mesh based data”. In:
(2015). doi: 10.5281/zenodo.591699. url: https://doi.org/10.5281/zenodo.591699.

[100] A. Huebl et al. “Develop/integrate data standards and start-to-end workﬂows”. In: Snow-
mass21 LOI (2020). url: https://www.snowmass21.org/docs/files/summaries/CompF/
SNOWMASS21-CompF2 CompF7-AF1 AF0 Huebl-079.pdf.

[101] Daniel Winklehner et al. “Realistic simulations of a cyclotron spiral inﬂector within a particle-
in-cell framework”. In: Physical Review Accelerators and Beams 20.12 (Dec. 2017), p. 124201.
doi: 10.1103/PhysRevAccelBeams.20.124201. url: https://link.aps.org/doi/10.1103/
PhysRevAccelBeams.20.124201 (visited on 10/15/2018).

[102] C.-K. Huang et al. “Physics-based high-ﬁdelity modeling of high brightness beam injectors”.
In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/docs/files/summaries/
AF/SNOWMASS21-AF7 AF1-CompF2 CompF0 Huang-183.pdf.

[103] S. Nagaitsev et al. “Accelerator and Beam Physics: Grand Challenges and Research Oppor-
tunities”. In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/docs/files/
summaries/AF/SNOWMASS21-AF1 AF7 S Nagaitsev-056.pdf.

[104] J Qiang et al. “Start-to-end simulation of x-ray radiation of a next generation light source
using the real number of electrons”. In: Physical Review Special Topics-Accelerators and
Beams 17.3 (2014), p. 030701.

[105] Ji Qiang. “Start-to-End Beam Dynamics Optimization of X-Ray FEL Light Source Ac-
celerators”. In: Proceedings of North American PAC2016. Chicago, IL, USA, Oct. 2016,
pp. 838–842.

[106] Ji Qiang. “X-ray FEL linear accelerator design via start-to-end global optimization”. In:
Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers,
Detectors and Associated Equipment (2022), p. 166294.

[107] J.-L. Vay and C. Deutsch. “A three-dimensional electromagnetic particle-in-cell code to
simulate heavy ion beam propagation in the reaction chamber”. In: Fusion Engineering and
Design 32-33.1-4 (1996). issn: 09203796. doi: 10.1016/S0920-3796(96)00502-9.

50

[108] J.L. Vay and C. Deutsch. “Charge compensated ion beam propagation in a reactor sized
chamber”. In: Physics of Plasmas 5.4 (1998). issn: 1070664X. doi: 10.1063/1.872648.

[109] Ji Qiang and Salman Habib. “Second-order stochastic leapfrog algorithm for multiplicative
noise Brownian motion”. In: Physical Review E 62.5 (Nov. 2000), p. 7430. issn: 1063651X.
doi: 10.1103/PhysRevE.62.7430. url: https://journals.aps.org/pre/abstract/10.1103/
PhysRevE.62.7430.

[110] J.-L. Vay. “An Extended Fdtd Scheme For The Wave Equation: Application To Multiscale
Electromagnetic Simulation”. In: Journal of Computational Physics 167.1 (Feb. 2001), pp. 72–
98. issn: 0021-9991.

[111] J. Qiang and R. D. Ryne. “Parallel 3D Poisson Solver for a Charged Beam in a Conducting

Pipe”. In: Comp. Phys. Comm. 138 (2001), p. 18.

[112] Ji Qiang, Miguel A. Furman, and Robert D. Ryne. “Strong-strong beam-beam simulation using
a Green function approach”. In: Physical Review Special Topics - Accelerators and Beams 5.10
(Oct. 2002), pp. 60–66. issn: 10984402. doi: 10.1103/PHYSREVSTAB.5.104402/FIGURES/9/
MEDIUM. url: https://journals.aps.org/prab/abstract/10.1103/PhysRevSTAB.5.104402.

[113] J.-L. Vay. “Asymmetric Perfectly Matched Layer For The Absorption Of Waves”. In: Journal
of Computational Physics 183.2 (Dec. 2002), pp. 367–399. issn: 0021-9991. doi: 10.1006/
Jcph.2002.7175.

[114] J.-L. Vay et al. “Mesh reﬁnement for particle-in-cell plasma simulations: Applications to and
beneﬁts for heavy ion fusion”. In: Laser and Particle Beams 20.4 (2002). issn: 02630346.
doi: 10.1017/S0263034602204139.

[115] Ji Qiang and Robert L. Gluckstern. “Three-dimensional Poisson solver for a charged beam
with large aspect ratio in a conducting pipe”. In: Computer Physics Communications 160.2
(July 2004), pp. 120–128. issn: 0010-4655. doi: 10.1016/J.CPC.2004.03.002.

[116] J.-L. Vay, J.-C. Adam, and A Heron. “Asymmetric Pml For The Absorption Of Waves.
Application To Mesh Reﬁnement In Electromagnetic Particle-In-Cell Plasma Simulations”.
In: Computer Physics Communications 164.1-3 (Dec. 2004), pp. 171–177. issn: 0010-4655.
doi: 10.1016/J.Cpc.2004.06.026.

[117] J.-L. Vay et al. “Application Of Adaptive Mesh Reﬁnement To Particle-In-Cell Simulations
Of Plasmas And Beams”. In: Physics Of Plasmas 11.5 (May 2004), pp. 2928–2934. issn:
1070-664X. doi: 10.1063/1.1689669.

[118] J. Qiang, D. Todd, and D. Leitner. “A 3D model for ion beam formation and transport

simulation”. In: Comp. Phys. Comm. 175 (2006), p. 416.

[119] V. K. Decyk. “UPIC: A framework for massively parallel particle-in-cell codes”. In: Computer

Physics Communications 177 (2007), p. 95.

[120] J L Vay. “Simulation Of Beams Or Plasmas Crossing At Relativistic Velocity”. In: Physics

Of Plasmas 15.5 (May 2008), p. 56701. doi: 10.1063/1.2837054.

[121] J.-L. Vay et al. “Application Of The Reduction Of Scale Range In A Lorentz Boosted
Frame To The Numerical Simulation Of Particle Acceleration Devices”. In: Proc. Particle
Accelerator Conference. Vancouver, Canada, 2009. url: https://accelconf.web.cern.ch/
PAC2009/papers/tu1pbi04.pdf.

51

[122] R H Cohen et al. “An Implicit “Drift-Lorentz{”} Mover For Plasma And Beam Simulations”.
In: Nuclear Instruments & Methods In Physics Research Section A-Accelerators Spectrometers
Detectors And Associated Equipment 606.1-2 (July 2009), pp. 53–55. issn: 0168-9002. doi:
10.1016/J.Nima.2009.03.083.

[123] J L Vay et al. “Numerical Methods For Instability Mitigation In The Modeling Of Laser
Wakeﬁeld Accelerators In A Lorentz-Boosted Frame”. In: Journal of Computational Physics
230.15 (July 2011), pp. 5908–5929. doi: 10.1016/J.Jcp.2011.04.003.

[124] J -L. Vay et al. “Modeling Of 10 Gev-1 Tev Laser-Plasma Accelerators Using Lorentz
Boosted Simulations”. In: Physics Of Plasmas 18.12 (Dec. 2011). issn: 1070-664X. doi:
10.1063/1.3663841.

[125] Jl Vay et al. “Eﬀects Of Hyperbolic Rotation In Minkowski Space On The Modeling Of
Plasma Accelerators In A Lorentz Boosted Frame”. In: Physics Of Plasmas 18.3 (Mar. 2011),
p. 30701. doi: 10.1063/1.3559483.

[126] J.-L. Vay et al. “Novel methods in the particle-in-cell accelerator code-framework warp”. In:
Computational Science and Discovery 5.1 (2012), 014019 (20 pp.) issn: 1749-4680.

[127] Brendan B Godfrey and Jean-Luc Vay. “Numerical stability of relativistic beam multidimen-
sional {PIC} simulations employing the Esirkepov algorithm”. In: Journal of Computational
Physics 248.0 (2013), pp. 33–46. issn: 0021-9991. doi: http : / / dx . doi . org / 10 . 1016 /
j . jcp . 2013 . 04 . 006. url: http : / / www . sciencedirect . com / science / article / pii /
S0021999113002556.

[128] Jean Luc Vay, Irving Haber, and Brendan B. Godfrey. “A domain decomposition method
for pseudo-spectral electromagnetic simulations of plasmas”. In: Journal of Computational
Physics 243 (2013), pp. 260–268.

[129] Brendan B Godfrey, Jean-Luc Vay, and Irving Haber. “Numerical stability analysis of the
pseudo-spectral analytical time-domain {PIC} algorithm”. In: Journal of Computational
Physics 258.0 (2014), pp. 689–704. issn: 0021-9991. doi: http://dx.doi.org/10.1016/
j . jcp . 2013 . 10 . 053. url: http : / / www . sciencedirect . com / science / article / pii /
S0021999113007298.

[130] Brendan B Godfrey and Jean-Luc Vay. “Suppressing the numerical Cherenkov instability
in {FDTD} {PIC} codes”. In: Journal of Computational Physics 267.0 (2014), pp. 1–6.
issn: 0021-9991. doi: http : / / dx . doi . org / 10 . 1016 / j . jcp . 2014 . 02 . 022. url: http :
//www.sciencedirect.com/science/article/pii/S0021999114001429.

[131] Brendan B. Godfrey, Jean Luc Vay, and Irving Haber. “Numerical stability improvements
for the pseudospectral EM PIC algorithm”. In: IEEE Transactions on Plasma Science 42.5
(2014), pp. 1339–1344.

[132] Brendan B. Godfrey and Jean Luc Vay. “Improved numerical Cherenkov instability suppres-
sion in the generalized PSTD PIC algorithm”. In: Computer Physics Communications 196
(2015), pp. 221–225.

[133] Remi Lehe et al. “Elimination of numerical Cherenkov instability in ﬂowing-plasma particle-
in-cell simulations by using Galilean coordinates”. In: Physical Review E 94.5 (Nov. 2016),
p. 053305. issn: 2470-0045. doi: 10.1103/PhysRevE.94.053305. url: https://link.aps.
org/doi/10.1103/PhysRevE.94.053305.

52

[134] M. Kirchen et al. “Stable discrete representation of relativistically drifting plasmas”. In:
Physics of Plasmas 23.10 (Oct. 2016), p. 100704. issn: 1070-664X. doi: 10.1063/1.4964770.
url: http://aip.scitation.org/doi/10.1063/1.4964770.

[135] H. Vincenti and J.-L. Vay. “Detailed analysis of the eﬀects of stencil spatial variations with
arbitrary high-order ﬁnite-diﬀerence Maxwell solver”. In: Computer Physics Communications
200 (Mar. 2016), pp. 147–167. issn: 00104655. doi: 10.1016/j.cpc.2015.11.009. url: https:
//apps.webofknowledge.com/full record.do?product=UA&search mode=GeneralSearch&
qid=1&SID=1CanLFIHrQ5v8O7cxqV&page=1&doc=2.

[136] J. Qiang. “Eﬃcient Three-Dimensional Poisson Solvers in Open Rectangular Conducting

Pipe”. In: Comp. Phys. Comm. 203 (2016), p. 122.

[137] G. Blaclard et al. “Pseudospectral Maxwell solvers for an accurate modeling of Doppler
harmonic generation on plasma mirrors with particle-in-cell codes”. In: Physical Review
E 96.3 (Sept. 2017), p. 033305. issn: 2470-0045. doi: 10.1103/PhysRevE.96.033305. url:
https://link.aps.org/doi/10.1103/PhysRevE.96.033305.

[138] S. Jalas et al. “Accurate modeling of plasma acceleration with arbitrary order pseudo-spectral
particle-in-cell methods”. In: Physics of Plasmas 24.3 (Mar. 2017), p. 033115. issn: 1070-664X.
doi: 10.1063/1.4978569. url: http://aip.scitation.org/doi/10.1063/1.4978569.

[139] Ji Qiang. “Symplectic multiparticle tracking model for self-consistent space-charge simulation”.
In: Physical Review Accelerators and Beams 20.1 (Jan. 2017), p. 014203. issn: 24699888. doi:
10.1103/PhysRevAccelBeams.20.014203.

[140] J. Qiang. “A Fast Numerical Integrator for Relativistic Charged Particle Tracking”. In:

Instruments & Methods in Physics Research A 885 (2017), p. 55.

[141] J.-L. Vay et al. “Warp-X: A new exascale computing platform for beam–plasma simu-
lations”. In: Nuclear Instruments and Methods in Physics Research Section A: Accelera-
tors, Spectrometers, Detectors and Associated Equipment (Jan. 2018). issn: 0168-9002. doi:
10.1016/J.NIMA.2018.01.035. url: https://www.sciencedirect.com/science/article/
pii/S0168900218300524.

[142] J. L. Vay et al. “Toward plasma wakeﬁeld simulations at exascale”. In: 2018 IEEE Advanced
Accelerator Concepts Workshop, ACC 2018 - Proceedings. Institute of Electrical and Elec-
tronics Engineers Inc., Mar. 2019. isbn: 9781538677216. doi: 10.1109/AAC.2018.8659392.

[143] H. Vincenti and J.-L. Vay. “Ultrahigh-order Maxwell solver with extreme scalability for
electromagnetic PIC simulations of plasmas”. In: Computer Physics Communications (2018).
issn: 00104655. doi: 10.1016/j.cpc.2018.03.018.

[144] J. Qiang. “Symplectic particle-in-cell model for space-charge beam dynamics simulation”. In:

Phys. Rev. ST Accel. Beams 21 (2018), p. 054201.

[145] Haithem Kallala, Jean Luc Vay, and Henri Vincenti. “A generalized massively parallel
ultra-high order FFT-based Maxwell solver”. In: Computer Physics Communications 244
(Nov. 2019), pp. 25–34. issn: 00104655. doi: 10.1016/j.cpc.2019.07.009.

[146] Olga Shapoval, Jean-Luc Vay, and Henri Vincenti. “Two-step perfectly matched layer for
arbitrary-order pseudo-spectral analytical time-domain methods”. In: Computer Physics
Communications 235 (Feb. 2019), pp. 102–110. issn: 0010-4655. doi: 10.1016/J.CPC.2018.
09.015. url: https://www.sciencedirect.com/science/article/pii/S0010465518303291?
via%3Dihub.

53

[147] J. Qiang. “Fast 3D Poisson solvers in elliptical conducting pipe for space-charge simulation”.

In: Phys. Rev. ST Accel. Beams 22 (2019), p. 104601.

[148] Manuel Kirchen et al. “Scalable spectral solver in Galilean coordinates for eliminating the
numerical Cherenkov instability in particle-in-cell simulations of streaming plasmas”. In:
Physical Review E 102.1 (July 2020), p. 13202. issn: 24700053. doi: 10.1103/PhysRevE.102.
013202. url: https://journals.aps.org/pre/abstract/10.1103/PhysRevE.102.013202.

[149] Olga Shapoval et al. “Overcoming timestep limitations in boosted-frame particle-in-cell
simulations of plasma-based acceleration”. In: Physical Review E 104.5 (Nov. 2021), p. 055311.
issn: 2470-0045. doi: 10 . 1103 / PHYSREVE . 104 . 055311 / FIGURES / 6 / MEDIUM. url: https :
//journals.aps.org/pre/abstract/10.1103/PhysRevE.104.055311.

[150] Edoardo Zoni et al. A Hybrid Nodal-Staggered Pseudo-Spectral Electromagnetic Particle-In-

Cell Method with Finite-Order Centering https://arxiv.org/abs/2106.12919v1. 2021.

[151] Remi Lehe et al. Absorption of charged particles in Perfectly-Matched-Layers by optimal

damping of the deposited current https://arxiv.org/abs/2201.09084. 2022.

[152] V. K. Decyk annd C. D. Norton. “UCLA Parallel PIC Framework”. In: Computer Physics

Communications 164 (2004), p. 80.

[153] Samuel F Martins et al. “Numerical Simulations Of Laser Wakeﬁeld Accelerators In Optimal
Lorentz Frames”. In: Computer Physics Communications 181.5 (May 2010), pp. 869–875.
issn: 0010-4655. doi: 10.1016/J.Cpc.2009.12.023.

[154] V. K. Decyk and T. V. Singh. “Adaptable Particle-in-Cell algorithms for graphical processing

units”. In: Computer Physics Communications 182 (2011), p. 641.

[155] Xinlu Xu et al. “Numerical instability due to relativistic plasma drift in EM-PIC simulations”.
In: Computer Physics Communications 184.11 (2013), pp. 2503–2514. issn: 0010-4655. doi:
http://dx.doi.org/10.1016/j.cpc.2013.07.003. url: http://www.sciencedirect.com/
science/article/pii/S0010465513002312.

[156] V. K. Decyk and T. V. Singh. “Particle-in-Cell algorithms for emerging computer architec-

tures”. In: Computer Physics Communications 185 (2014), p. 708.

[157] Peicheng Yu et al. “Elimination of the numerical Cerenkov instability for spectral EM-PIC
codes”. In: Computer Physics Communications 192 (July 2015), pp. 32–47. issn: 00104655.
doi: 10.1016/j.cpc.2015.02.018. url: https://apps.webofknowledge.com/full record.
do?product=UA&search mode=GeneralSearch&qid=2&SID=1CanLFIHrQ5v8O7cxqV&page=1&
doc=3.

[158] Peicheng Yu et al. “Mitigation of numerical Cerenkov radiation and instability using a
hybrid ﬁnite diﬀerence-FFT Maxwell solver and a local charge conserving current deposit”.
In: Computer Physics Communications 197 (Dec. 2015), pp. 144–152. issn: 00104655. doi:
10.1016/j.cpc.2015.08.026. url: https://apps.webofknowledge.com/full record.do?
product=UA&search mode=GeneralSearch&qid=2&SID=1CanLFIHrQ5v8O7cxqV&page=1&doc=2.

[159] P. Yu et al. “Enabling Lorentz boosted frame particle-in-cell simulations of laser wakeﬁeld
acceleration in quasi-3D geometry”. In: Journal of Computational Physics 316 (2016), p. 747.

[160] Fei Li et al. “Controlling the numerical Cerenkov instability in PIC simulations using a
customized ﬁnite diﬀerence Maxwell solver and a local FFT based current correction”.
In: Computer Physics Communications 214 (May 2017), pp. 6–17. issn: 0010-4655. doi:
10.1016/J.CPC.2017.01.001.

54

[161] F. Li et al. “A new ﬁeld solver for modeling of relativistic particle-laser interactions using

the particle-in-cell algorithmhttps://arxiv.org/abs/2004.03754”. In: (2020).

[162] F. Li et al. “Accurately simulating nine-dimensional phase space of relativistic particles in

strong ﬁelds”. In: Jour. Comp. Phys. 438 (2021), p. 110367.

[163] X. L. Xu et al. “On nnumerical errors to the ﬁelds surrounding a relativistically moving

particle in PIC codes”. In: Jour. Comp. Phys. 413 (2020), p. 109451.

[164] M. Vranic et al. “Classical radiation reaction in particle-in-cell simulations”. In: Comp. Phys.

Comm. 204 (2016), pp. 141–151.

[165] C. E. Mayes et al. “Lightsource uniﬁed modeling environment (LUME), a start-to-end

simulation ecosystem”. In: Proc. of IPAC. 2021, THPAB217.

[166] Christopher Mayes. xopt: Simulation optimization, based on DEAP. https://github.com/ChristopherMayes/xopt.

[167] Alexander Scheinker and Spencer Gessner. “Adaptive method for electron bunch proﬁle pre-
diction”. In: Physical Review Special Topics-Accelerators and Beams 18.10 (2015), p. 102801.
doi: https://doi.org/10.1103/PhysRevSTAB.18.102801.

[168] Auralee Edelen et al. “Machine learning for orders of magnitude speedup in multiobjective
optimization of particle accelerator systems”. In: Phys. Rev. Accel. Beams 23 (4 Apr. 2020),
p. 044601. doi: 10.1103/PhysRevAccelBeams.23.044601. url: https://link.aps.org/doi/
10.1103/PhysRevAccelBeams.23.044601.

[169] A. L. Edelen et al. “Neural Networks for Modeling and Control of Particle Accelerators”. In:
IEEE Transactions on Nuclear Science 63.2 (2016), pp. 878–897. doi: 10.1109/TNS.2016.
2543203.

[170] C. Emma et al. “Machine learning-based longitudinal phase space prediction of parti-
cle accelerators”. In: Phys. Rev. Accel. Beams 21 (11 Nov. 2018), p. 112802. doi: 10 .
1103 / PhysRevAccelBeams . 21 . 112802. url: https : / / link . aps . org / doi / 10 . 1103 /
PhysRevAccelBeams.21.112802.

[171] A. Edelen et al. “Using Neural Network Control Policies for Rapid Switching Between Beam
Parameters in a free electron laser”. In: NeurIPS 2017. https://dl4physicalsciences.
github.io/files/nips dlps 2017 16.pdf. Long Beach, CA, 2017.

[172] A. Edelen et al. “Machine Learning Models for Optimization and Control of X-ray Free
Electron Lasers”. In: NeurIPS 2019. https : / / ml4physicalsciences . github . io / 2019 /
files/NeurIPS ML4PS 2019 90.pdf. Vancouver, Canada, 2019.

[173] AL Edelen et al. “First steps toward incorporating image based diagnostics into parti-
cle accelerator control systems using convolutional neural networks”. In: arXiv preprint
arXiv:1612.05662 (2016).

[174] Lipi Gupta et al. “Improving surrogate model accuracy for the LCLS-II injector frontend
using convolutional neural networks and transfer learning”. In: Machine Learning: Science
and Technology 2.4 (Oct. 2021), p. 045025. doi: 10.1088/2632-2153/ac27ff. url: https:
//doi.org/10.1088/2632-2153/ac27ff.

[175] J. Zhu et al. “High-Fidelity Prediction of Megapixel Longitudinal Phase-Space Images of
Electron Beams Using Encoder-Decoder Neural Networks”. In: Phys. Rev. Applied 16 (2
Aug. 2021), p. 024005. doi: 10.1103/PhysRevApplied.16.024005. url: https://link.aps.
org/doi/10.1103/PhysRevApplied.16.024005.

55

[176] Alexander Scheinker. “Adaptive machine learning for time-varying systems: low dimensional
latent space tuning”. In: Journal of Instrumentation 16.10 (2021), P10008. url: https:
//doi.org/10.1088/1748-0221/16/10/P10008.

[177] Joseph Duris et al. “Bayesian optimization of a free-electron laser”. In: Physical review letters

124.12 (2020), p. 124801.

[178] J. ¨Ogren, C. Gohil, and D. Schulte. “Surrogate modeling of the CLIC ﬁnal-focus system
using artiﬁcial neural networks”. In: Journal of Instrumentation 16.05 (May 2021), P05012.
doi: 10.1088/1748-0221/16/05/p05012. url: https://doi.org/10.1088/1748-0221/16/
05/p05012.

[179] Aashwin Ananda Mishra et al. “Uncertainty quantiﬁcation for deep learning in particle
accelerator applications”. In: Physical Review Accelerators and Beams 24.11 (2021), p. 114601.

[180] Owen Convery et al. “Uncertainty quantiﬁcation for virtual diagnostic of particle accelerators”.
In: Phys. Rev. Accel. Beams 24 (7 July 2021), p. 074602. doi: 10.1103/PhysRevAccelBeams.
24.074602. url: https://link.aps.org/doi/10.1103/PhysRevAccelBeams.24.074602.

[181] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. “Deep online learning via meta-learning:

Continual adaptation for model-based RL”. In: arXiv preprint arXiv:1812.07671 (2018).

[182] Sinno Jialin Pan and Qiang Yang. “A survey on transfer learning”. In: IEEE Transactions

on knowledge and data engineering 22.10 (2009), pp. 1345–1359.

[183] Andrei Ivanov and Ilya Agapov. “Physics-based deep neural networks for beam dynamics
in charged particle accelerators”. In: Physical Review Accelerators and Beams 23.7 (July
2020), p. 074601. issn: 2469-9888. doi: 10 . 1103 / PhysRevAccelBeams . 23 . 074601. url:
https://doi.org/10.1103/PhysRevAccelBeams.23.074601%20https://link.aps.org/doi/
10.1103/PhysRevAccelBeams.23.074601.

[184] Alexander Scheinker et al. “An adaptive approach to machine learning for compact particle
accelerators”. In: Scientiﬁc reports 11.1 (2021), pp. 1–11. url: https://doi.org/10.1038/
s41598-021-98785-0.

[185] Alexander Scheinker et al. “Demonstration of model-independent control of the longitudinal
phase space of electron beams in the linac-coherent light source with femtosecond resolution”.
In: Physical review letters 121.4 (2018), p. 044801. url: https : / / doi . org / 10 . 1103 /
PhysRevLett.121.044801.

[186] George Em Karniadakis et al. “Physics-informed machine learning”. In: Nature Reviews

Physics 3.6 (2021), pp. 422–440.

[187] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. “Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear
partial diﬀerential equations”. In: Journal of Computational physics 378 (2019), pp. 686–707.

[188] Alvaro Sanchez-Gonzalez et al. “Hamiltonian Graph Networks with ODE Integrators”. In:

(Sept. 2019). arXiv: 1909.12790 [cs.LG].

[189] Adi Hanuka et al. “Physics model-informed Gaussian process for online optimization of
particle accelerators”. In: Physical Review Accelerators and Beams 24.7 (2021), p. 072802.

[190] M. Berz. “Diﬀerential Algebraic Description of Beam Dynamics to Very High Orders”. In:

Part. Accel. 24 (1989), pp. 109–124.

[191] R Roussel et al. “Diﬀerentiable Preisach Modeling for Characterization and Optimization of
Accelerator Systems with Hysteresis”. In: arXiv preprint arXiv:2202.07747 (2022).

56

[192] K. Deb et al. “A fast and elitist multiobjective genetic algorithm: NSGA-II”. In: IEEE
Transactions on Evolutionary Computation 6.2 (2002), pp. 182–197. doi: 10.1109/4235.
996017.

[193] Ryan Roussel, Adi Hanuka, and Auralee Edelen. “Multiobjective Bayesian optimization
for online accelerator tuning”. In: Phys. Rev. Accel. Beams 24 (6 June 2021), p. 062801.
doi: 10.1103/PhysRevAccelBeams.24.062801. url: https://link.aps.org/doi/10.1103/
PhysRevAccelBeams.24.062801.

[194] Ryan Roussel et al. “Turn-key constrained parameter space exploration for particle ac-
celerators using Bayesian active learning.” In: Nature Communications 12 (1 2021). url:
https://doi.org/10.1038/s41467-021-25757-3.

[195] Jian Wu et al. “Practical Multi-ﬁdelity Bayesian Optimization for Hyperparameter Tuning”.

In: Uncertainty Quantiﬁcation in Artiﬁcial Intelligence. 2019.

[196] David Eriksson et al. “Scalable Global Optimization via Local Bayesian Optimization”.
In: Advances in Neural Information Processing Systems. Ed. by H. Wallach et al. Vol. 32.
Curran Associates, Inc., 2019. url: https://proceedings.neurips.cc/paper/2019/file/
6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf.

[197] Willie Neiswanger, Ke Alexander Wang, and Stefano Ermon. “Bayesian Algorithm Execution:
Estimating Computable Properties of Black-box Functions Using Mutual Information”. In:
Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila
and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18–24 Jul
2021, pp. 8005–8015. url: https://proceedings.mlr.press/v139/neiswanger21a.html.

[198] John Preskill. “Quantum computing in the NISQ era and beyond”. In: Quantum 2 (2018),

p. 79.

[199] Frank Arute et al. “Quantum supremacy using a programmable superconducting processor”.

In: Nature 574.7779 (2019), pp. 505–510.

[200] Han-Sen Zhong et al. “Quantum computational advantage using photons”. In: Science

370.6523 (2020), pp. 1460–1463.

[201]

IBM Quantum Computing. https://www.ibm.com/quantum- computing/. 2020 (accessed
March 08, 2022).

[202] Leap2. https://www.dwavesys.com/. 2020.

[203] Amazon Quantum Solutions Lab. https://aws.amazon.com/quantum-solutions-lab/. 2020.

[204] Azure Quantum. https://azure.microsoft.com/en-us/services/quantum/. 2020.

[205] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. “Quantum support vector machine

for big data classiﬁcation”. In: Physical review letters 113.13 (2014), p. 130503.

[206] Rupak Chatterjee and Ting Yu. “Generalized coherent states, reproducing kernels, and

quantum support vector machines”. In: arXiv preprint arXiv:1612.03713 (2016).

[207] Guang Hao Low, Theodore J Yoder, and Isaac L Chuang. “Quantum inference on Bayesian

networks”. In: Physical Review A 89.6 (2014), p. 062315.

[208]

Iris Cong, Soonwon Choi, and Mikhail D Lukin. “Quantum convolutional neural networks”.
In: Nature Physics 15.12 (2019), pp. 1273–1278.

[209] Zhikuan Zhao et al. “Bayesian deep learning on a quantum computer”. In: Quantum Machine

Intelligence 1.1 (2019), pp. 41–51.

57

[210] Vedran Dunjko, Jacob M Taylor, and Hans J Briegel. “Quantum-enhanced machine learning”.

In: Physical review letters 117.13 (2016), p. 130501.

[211] Vedran Dunjko, Jacob M Taylor, and Hans J Briegel. “Advances in quantum reinforcement
learning”. In: 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC).
IEEE. 2017, pp. 282–287.

[212] Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. “Quantum algorithm for linear systems

of equations”. In: Physical review letters 103.15 (2009), p. 150502.

[213] B David Clader, Bryan C Jacobs, and Chad R Sprouse. “Preconditioned quantum linear

system algorithm”. In: Physical review letters 110.25 (2013), p. 250504.

[214] Andrew M Childs, Robin Kothari, and Rolando D Somma. “Quantum algorithm for systems
of linear equations with exponentially improved dependence on precision”. In: SIAM Journal
on Computing 46.6 (2017), pp. 1920–1950.

[215] Carlos Bravo-Prieto et al. “Variational quantum linear solver: A hybrid algorithm for linear

systems”. In: arXiv preprint arXiv:1909.05820 (2019).

[216] Yonghae Lee, Jaewoo Joo, and Soojoon Lee. “Hybrid quantum linear equation algorithm and
its experimental test on ibm quantum experience”. In: Scientiﬁc reports 9.1 (2019), p. 4778.

[217] Sarah K Leyton and Tobias J Osborne. “A quantum algorithm to solve nonlinear diﬀerential

equations”. In: arXiv preprint arXiv:0812.4423 (2008).

[218] Dominic W Berry. “High-order quantum algorithm for solving linear diﬀerential equations”.

In: Journal of Physics A: Mathematical and Theoretical 47.10 (2014), p. 105301.

[219] Juan Miguel Arrazola et al. “Quantum algorithm for nonhomogeneous linear partial diﬀeren-

tial equations”. In: Physical Review A 100.3 (2019), p. 032306.

[220] Andrew M Childs, Jin-Peng Liu, and Aaron Ostrander. “High-precision quantum algorithms

for partial diﬀerential equations”. In: arXiv preprint arXiv:2002.07868 (2020).

[221] Tao Xin et al. “Quantum algorithm for solving linear diﬀerential equations: Theory and

experiment”. In: Physical Review A 101.3 (2020), p. 032307.

[222] Yudong Cao et al. “Quantum algorithm and circuit design solving the Poisson equation”. In:

New Journal of Physics 15.1 (2013), p. 013021.

[223] Shengbin Wang et al. “Quantum Fast Poisson Solver: the algorithm and modular circuit

design”. In: arXiv preprint arXiv:1910.09756 (2019).

[224] Alexander Engel, Graeme Smith, and Scott E Parker. “Quantum algorithm for the Vlasov

equation”. In: Physical Review A 100.6 (2019), p. 062315.

[225] Q# and the Quantum Development Kit. https://www.microsoft.com/en- us/quantum/

development-kit. 2020 (accessed March 08, 2022).

[226] J. Qiang. “Simulation of space-charge eﬀects using a quantum Schrodinger approach”. In:

Phys. Rev. ST Accel. Beams 25 (2022), p. 034602.

[227]

Ilya Y Dodin and Edward A Startsev. “On applications of quantum computing to plasma
simulations”. In: arXiv preprint arXiv:2005.14369 (2020).

[228] K. A. Brown and T. Roser. “Towards storage rings as quantum computers”. In: Physical
Review Accelerators and Beams 23.5 (May 2020), p. 054701. issn: 24699888. doi: 10.1103/
PHYSREVACCELBEAMS.23.054701/FIGURES/4/MEDIUM. url: https://journals.aps.org/prab/
abstract/10.1103/PhysRevAccelBeams.23.054701.

58

[229] Wolfgang Paul. “Electromagnetic traps for charged and neutral particles”. In: Reviews of
Modern Physics 62.3 (July 1990), p. 531. issn: 00346861. doi: 10.1103/RevModPhys.62.531.
url: https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.62.531.

[230] D F V James. Applied Physics B Lasers and Optics Quantum dynamics of cold trapped ions

with application to quantum computation. Tech. rep. 1998, pp. 181–190.

[231] Lowell S. Brown and Gerald Gabrielse. “Geonium theory: Physics of a single electron or ion
in a Penning trap”. In: Reviews of Modern Physics 58.1 (Jan. 1986), p. 233. issn: 00346861.
doi: 10.1103/RevModPhys.58.233. url: https://journals.aps.org/rmp/abstract/10.
1103/RevModPhys.58.233.

[232] Bohong Huang et al. “Artiﬁcial Intelligence-Assisted Design and Virtual Diagnostic for the
Initial Condition of a Storage-Ring-Based Quantum Information System”. In: IEEE Access
10 (2022), pp. 14350–14358. issn: 21693536. doi: 10.1109/ACCESS.2022.3147727.

[233] J. L. Vay et al. “Modeling of a chain of three plasma accelerator stages with the WarpX
electromagnetic PIC code on GPUs”. In: Physics of Plasmas 28.2 (Feb. 2021), p. 23105. issn:
10897674. doi: 10.1063/5.0028512. url: https://doi.org/10.1063/5.0028512.

[234] A. Myers et al. “Porting WarpX to GPU-accelerated platforms”. In: Parallel Computing 108

(Dec. 2021), p. 102833. issn: 0167-8191. doi: 10.1016/J.PARCO.2021.102833.

[235] Synergia. url: http://web.fnal.gov/sites/synergia.
[236] Kokkos · GitHub. url: https://github.com/kokkos.

[237] Erik Zenker et al. “Alpaka - An Abstraction Library for Parallel Kernel Acceleration”. In:
Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium,
IPDPS 2016 (Feb. 2016), pp. 631–640. doi: 10.1109/ipdpsw.2016.50. url: https://arxiv.
org/abs/1602.08477v1.

[238] GitHub - LLNL/RAJA: RAJA Performance Portability Layer (C++). url: https://github.

com/LLNL/RAJA.

[239] AMReX. url: https://amrex-codes.github.io/.
[240] The Copa Team. Copa. url: https://github.com/ECP-copa.

[241] Lipeng Wan et al. “Improving I/O Performance for Exascale Applications through Online
Data Layout Reorganization”. In: accepted in IEEE Transactions on Parallel and Distributed
Systems (2021). url: https://arxiv.org/abs/2107.07108.

[242] Axel Huebl et al. “On the Scalability of Data Reduction Techniques in Current and Upcoming
HPC Systems from an Application Perspective”. In: High Performance Computing. Ed. by
Julian M. Kunkel et al. Cham: Springer International Publishing, 2017, pp. 15–29. isbn:
978-3-319-67630-2.

[243] M. Bussmann et al. “Radiative Signatures of the Relativistic Kelvin-Helmholtz Instability”.
In: Proceedings of the International Conference on High Performance Computing, Networking,
Storage and Analysis. SC ’13. Denver, Colorado: ACM, 2013, 5:1–5:12. isbn: 978-1-4503-2378-9.
doi: 10.1145/2503210.2504564. url: http://doi.acm.org/10.1145/2503210.2504564.

[244] Franz Poeschel et al. “Transitioning from ﬁle-based HPC workﬂows to streaming data

pipelines with openPMD and ADIOS2”. In: submitted (2021).

[245] R. Lehe et al. “Embracing modern software tools and user-friendly practices, when distributing
scientiﬁc codes”. In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/docs/
files/summaries/CompF/SNOWMASS21-CompF2 CompF0 Lehe-076.pdf.

59

[246] Daniel S. Katz. Scientiﬁc Software Challenges and Community Responses. Talk given at RTI
International on 7 December 2015, discussing 12 scientiﬁc software challenges and how the
scientiﬁc software community is responding to them. 2015. url: https://www.slideshare.
net/danielskatz/scientific-software-challenges-and-community-responses.

[247] Software Sustainability Institute. The University of Edinburgh. url: https://www.software.

ac.uk/.

[248] Spack. https://spack.io/.

[249] Conda. https://docs.conda.io/en/latest/.

[250] The Phython package Installer. https://pip.pypa.io/en/stable/.

[251] GitHub Issues. https://guides.github.com/features/issues/.

[252] Gitter. https://gitter.im/.

[253] Slack. https://slack.com/.

[254] Git. https://git-scm.com/.

[255] Zenodo. https://zenodo.org/.

[256] A. Huebl et al. “Aspiration for Open Science in Accelerator & Beam Physics Modeling ”.
In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/docs/files/summaries/
CompF/SNOWMASS21-CompF2 CompF7-AF1 AF0 Huebl-081.pdf.

[257] Matthew Feickert et al. “Long Term Reproducibility and Sustainability of Scientiﬁc Software”.
In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/docs/files/summaries/
CompF/SNOWMASS21-CompF7 CompF0 Matthew Feickert-107.pdf.

[258] Daniel S. Katz et al. Software Sustainability & High Energy Physics. Oct. 2020. doi: 10.

5281/zenodo.4095837. url: https://arxiv.org/abs/2010.05102.

[259] Open Source Initiative. The Open Source Deﬁnition. 2004. url: https://opensource.org/

docs/osd.

[260] Free Software Foundation. The Free Software Deﬁnition. 1990. url: https://www.gnu.org/

philosophy/free-sw.en.html.

[261] R. Schmitz, T. Eichlersmith, and A. Huebl. “Barriers to Entry in Physics Computing”.
In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/docs/files/summaries/
CompF/SNOWMASS21-CompF0 CompF0 Ryan Schmitz-104.pdf.

[262] Xiaoli Chen et al. “Open is not enough”. In: Nature Physics 15.2 (Nov. 2018), pp. 113–119.

doi: 10.1038/s41567-018-0342-2.

[263] GitHub actions. https://github.com/features/actions.

[264] Azure pipelines. https://azure.microsoft.com/en-us/services/devops/pipelines/.

[265] Travis CI. https://travis-ci.org/.

[266] E4S. https://e4s-project.github.io/.

[267] M. Bai et al. Snowmass AF1 White Paper: Strategies in Education, Outreach, and Inclusion

to Enhance the US Workforce in Accelerator Science and Engineering. 2022.

[268] CAMPA: Consortium for Advanced Modeling of Particle Accelerators. url: http://campa.

lbl.gov.

[269] PICMI. url: https://github.com/picmi-standard.

60

[270] William F. Godoy et al. “ADIOS 2: The Adaptable Input Output System. A framework for
high-performance data management”. In: SoftwareX 12 (2020), p. 100561. issn: 2352-7110.
doi: https://doi.org/10.1016/j.softx.2020.100561. url: https://www.sciencedirect.
com/science/article/pii/S2352711019302560.

[271] The HDF Group. Hierarchical data format version 5. url: http://www.hdfgroup.org/HDF5.

[272] Axel Huebl et al. “openPMD-api: C++ & Python API for Scientiﬁc I/O with openPMD”.

In: (2018). doi: 10.14278/rodare.27. url: https://github.com/openPMD/openPMD-api.

[273] R. A. Fonseca. “No Title”. In: Lec. Notes In Comp. Sci. 2329 (2002), p. 342.

[274] D.A. Bizzozero et al. “Multi-objective optimization with an integrated electromagnetics and
beam dynamics workﬂow”. In: Nucl. Instrum. Methods Phys. Res. A 1020 (2021), p. 165844.

[275] Severin Diederichs et al. HiPACE++: a portable, 3D quasi-static Particle-in-Cell code. 2022.

eprint: arXiv:2109.10277. url: https://arxiv.org/abs/2109.10277.

[276] Maxence Th´evenet et al. HiPACE++: Highly eﬃcient Plasma Accelerator Emulation, qua-

sistatic particle-in-cell code. 2021. url: https://github.com/Hi-PACE/hipace.
IDEAS: Interoperable Design of Extreme-scale Application Software. url: https://ideas-
productivity.org.
xSDK: Extreme-scale Scientiﬁc Software Development Kit. url: http://xsdk.info.

[277]

[278]

[279] Bierlich, Christian and Buckley, Andy Cranmer, Kyle and others. “Data and Analysis
Preservation, Recasting, and Reinterpretation”. In: Snowmass21 Whitepaper to TF07 and
CompF7 (2022).

[280] United Nations Educational, Scientiﬁc and Cultural Organization (UNESCO). Open Science
Movement. url: http : / / www . unesco . org / new / en / communication - and - information /
portals-and-platforms/goap/open-science-movement/.

[281] FOSTER Plus (European Union funded open project in Horizon 2020 and beyond). Open
Science Deﬁnition. url: https : / / www . fosteropenscience . eu / foster - taxonomy / open -
science-definition.

[282] United Nations Educational, Scientiﬁc and Cultural Organization (UNESCO). What is Open

Access? url: https://en.unesco.org/open-access/what-open-access.

[283] CERN Data Portal. url: http://opendata.cern.ch.
[284] FAIR Principles. url: https://www.go-fair.org/fair-principles/.

[285] Jean-Luc Vay et al. “Center(s) for Accelerator and Beam Physics Modeling”. In: Snow-
mass21 LOI (2020). url: https://www.snowmass21.org/docs/files/summaries/CompF/
SNOWMASS21-CompF2 CompF0-AF1 AF0 Vay-069.pdf.

[286] MIT Schwarzman College of Computing. MIT Schwarzman College of Computing. url:

https://computing.mit.edu/ (visited on 06/23/2021).
[287] QuICS. url: https://quics.umd.edu/ (visited on 06/29/2021).
[288] Chris Fisher. IBM — Quantum Computing. url: https://www.ibm.com/quantum-computing/

/ (visited on 06/29/2021).

[289] Exascale Computing Project. Exascale Computing Project. url: https://www.exascaleproject.

org/ (visited on 06/23/2021).

[290] Scientiﬁc Discovery through Advanced Computing. url: https://www.scidac.gov/ (visited

on 06/23/2021).

61

[291] J.-L. Vay et al. “A modular community ecosystem for multiphysics particle accelerator
modeling and design”. In: Snowmass21 LOI (2020). url: https://www.snowmass21.org/
docs/files/summaries/CompF/SNOWMASS21-CompF2 CompF0-AF1 AF0 Vay-070.pdf.

[292] David Bruhwiler et al. “Collaboration between industry and the HEP community”. In:
Snowmass21 LOI (2020). url: https : / / www . snowmass21 . org / docs / files / summaries /
CommF/SNOWMASS21-CommF1 CommF0-AF0 AF1 Bruhwiler-066.pdf.

62

