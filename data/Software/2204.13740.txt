2
2
0
2

r
p
A
8
2

]

C
D
.
s
c
[

1
v
0
4
7
3
1
.
4
0
2
2
:
v
i
X
r
a

Black-Scholes Option Pricing on Intel CPUs and
GPUs: Implementation on SYCL and
Optimization Techniques

Elena Panova1, Valentin Volokitin1,2, Anton Gorshkov1, and Iosif Meyerov1,2,3

1 oneAPI Center of Excellence, Lobachevsky University, Nizhni Novgorod 603950,
Russia
2 Mathematical Center, Lobachevsky University, Nizhni Novgorod 603950, Russia
3 meerov@vmk.unn.ru

Abstract. The Black-Scholes option pricing problem is one of the widely
used ﬁnancial benchmarks. We explore the possibility of developing a
high-performance portable code using the SYCL (Data Parallel C++)
programming language. We start from a C++ code parallelized with
OpenMP and show optimization techniques that are beneﬁcial on mod-
ern Intel Xeon CPUs. Then, we port the code to SYCL and consider im-
portant optimization aspects on CPUs and GPUs (device-friendly mem-
ory access patterns, relevant data management, employing vector data
types). We show that the developed SYCL code is only 10% inferior to
the optimized C++ code when running on CPUs while achieving rea-
sonable performance on Intel GPUs. We hope that our experience of
developing and optimizing the code on SYCL can be useful to other re-
searchers who plan to port their high-performance C++ codes to SYCL
to get all the beneﬁts of single-source programming.

Keywords: high-performance computing, Black-Scholes formula, het-
erogeneous computing, parallel computing, oneAPI, DPC++, SYCL,
single-source programming, performance optimization

1

Introduction

The ﬁnancial industry solves many computationally intensive problems requiring
high-performance computing. Over time some ﬁnancial problems have become
traditional workload benchmarks. The Black-Scholes option pricing problem [1]
is one of the most widespread among them due to several reasons. Firstly, it
is one of the basic elements of ﬁnancial market analysis and, therefore, has a
great practical interest. Secondly, ease of understanding and implementation,
combined with high computational requirements, make the Black-Scholes model
popular for learning the basics of optimization and performance testing on dif-
ferent architectures, including various accelerators such as GPU, Xeon Phi, and
others [2,3,4,5,6]. In this paper, we consider the Black-Scholes formula for a fair
price of a European call option and employ several optimization techniques to

 
 
 
 
 
 
2

Elena Panova et al.

improve performance on CPUs and GPUs. Our main motivation is to under-
stand is it possible to achieve reasonable performance on Intel CPUs and GPUs
using a new oneAPI programming model.

oneAPI [7] is a new technology of single-source heterogeneous programming,
introduced by Intel in 2020. Just like OpenCL [8], OpenACC [9], Kokkos [10], Al-
paka [11] and many others, it is a software framework that allows implementing a
single code for various hardware. With a growing diversity of architectures and
their wide distribution, including among supercomputer systems, such frame-
works are of increasing interest for developing high-performance applications in
various problem areas. oneAPI provides a large set of tools for eﬃcient develop-
ment and performance analysis and optimization on CPUs, GPUs, FPGAs, and
other devices. oneAPI includes the Data Parallel C++ (DPC++) programming
language [12] based on the SYCL model [13,14]. DPC++ and SYCL are almost
the same languages, so both designations will be used in the same sense in the
rest of the paper.

The opportunity to write a single portable code that can be compiled and
run on a variety of computing devices of various architectures and vendors is
an important advantage of DPC++. This feature could allow us to develop a
single portable code instead of a family of codes optimized for each computer
architecture. However, whether performance portability is possible to achieve is
still the open question.

In this paper, we address this important question by calculating the fair
prices of a set of European call options. Compared to our previous paper [2]
we analyze which optimization techniques to the C++ code parallelized with
OpenMP improve performance on modern CPUs. We consider this implemen-
tation as a baseline for CPUs. After that, we show how to port the code to
DPC++/SYCL and achieve reasonable performance not only on Intel CPUs
but also on Intel GPUs. We highlight which optimization techniques are bene-
ﬁcial for this memory-bound application, and hope that our experience will be
useful to other developers.

The paper is organized as follows. Section 2 provides the Black-Scholes for-
mula and the corresponding algorithm. Test infrastructure and some launch
parameters used in computational experiments are described in Section 3. The
OpenMP implementation on C++ and its step-by-step optimization are pre-
sented in Section 4. In Section 5 we propose the DPC++ implementation of
the Black-Scholes formula and optimize it on Intel CPUs and GPUs. Section 6
summarizes the paper.

2 Black-Scholes Option Pricing

We consider the European share call option, which is the simplest variant of an
option and describes the following contract between two parties. The ﬁrst party
promises to sell the second one a share at a ﬁxed strike price K at some point in
time T after the contract conclusion, and the second party pays the ﬁrst party
an amount C for this right. After the maturity of T , the second party can decide

Black-Scholes Option Pricing on Intel CPUs and GPUs

3

to buy the share or not, it depends on its current price ST : if ST − K − C > 0
then it is proﬁtable. The question is how to deﬁne the fair option price C, which
is a balance of gains and losses for each party. It can be determined by the
Black-Scholes formula for European call option price:

C = S0F (d1) − Ke−rT F (d2)

d1 = ln

d2 = ln

S0
K

+

(cid:16)

S0
K

+

(cid:17)

T

(cid:16)

r + σ2
2
√

T
σ
(cid:17)
r − σ2
2
√

σ

T

T

,

(1)

(2)

(3)

where S0 > 0 is a starting price of a share, r is an interest rate (describes how
many times the price of bonds in the ﬁnancial market will increase), σ is volatility
(a coeﬃcient of risk assessment), K is a strike price, T is maturity, C is an option
price, and F (x) is a cumulative normal distribution function. We do not go into
detail about key assumptions and ideas that lead to the Black-Scholes formula,
this is fully described in [1,2]. In what follows, we will assume that r and σ are
ﬁxed, while S0, K and T depend on the current state of the ﬁnancial market.
The main intention is to calculate the optimal option price C based on the given
input parameters.

In practice, prices are calculated for a huge amount of diﬀerent options for
many speciﬁc market conditions. The performance of ﬁnancial calculations sig-
niﬁcantly inﬂuences the decision-making speed, so reducing computational time
for a large set of options is very important. Below we present the Black-Scholes
formula implementation on C++ and DPC++ programming languages and op-
timize it step by step for CPUs and GPUs. We demonstrate that although the
formula looks very simple to implement, it is a good testing ground for employ-
ing and analyzing a wide range of ideas related to performance optimization,
especially when using devices of diﬀerent architectures.

3 Test Infrastructure

Experiments were performed at the following infrastructures:

– A node of the Endeavour supercomputer with 2x Intel Xeon Platinum 8260L
(Cascade Lake, 24 cores each, 2400 MHz), 192 GB RAM, RedHat 4.8.5, Intel
C++ Compiler Classic, and Intel DPC++ Compiler from the Intel OneAPI
Toolkit 2022.1 (experiments on CPUs).

– A node of the Intel DevCloud platform with Intel Iris Xe MAX (96 EU,
1650 MHz, 4 GB RAM, 68 GB/sec of memory bandwidth), Intel Core i9
10920X, Ubuntu 18.04 LTS, Intel DPC++ Compiler from the Intel OneAPI
Toolkit 2022.1 (experiments on GPUs).

According to peak performance and memory bandwidth, the designated GPU
loses to the CPU, so we expect a worse performance on the GPU compared to

4

Elena Panova et al.

the CPU. It is conﬁrmed in practice. However, optimizations for such GPUs are
of interest due to the prospect of new server GPUs being released by Intel.

The whole set of options is processed in 5 batches of 240,000,000 elements
(each batch takes ∼3.8 GB). The minimum time for all batches is chosen as the
resulting execution time. It is relevant for DPC++ experiments, as this way the
DPC++ kernel compilation time, which increases the processing time of the ﬁrst
batch, is excluded from our measurements.

4 C++ Implementation and Optimization

4.1 Baseline

Even though the formulas 1-3 look quite simple, everything is not as elementary
as it seems. First, we need to decide on the choice of a relevant data type and
memory structure. The nature of the problem implies that it does not require
high computational accuracy, so we can use single precision (’ﬂoat’ type) or even
half precision (the last one is the subject of further research). It is worth noting
that mixing ’double’ and ’ﬂoat’ types is a typical mistake of many programmers:
usage of double precision literals and inappropriate mathematical functions can
signiﬁcantly increase execution time. In its turn, careful handling of data types
has only a positive eﬀect on performance.

Employing a cache-friendly memory access pattern is one of the key perfor-
mance optimization techniques. Applied to the Black-Scholes formula, we have
two options to organize data: Array of Structures (AoS) and Structure of Arrays
(SoA). In the AoS pattern case, all data related to the ﬁrst option is placed into
memory continued by data from the second option, and so on. In the SoA case,
the data is packed in multiple arrays, and each array is responsible for storing
a single parameter (initial price of a share, strike price, maturity, option price).
More details about the advantages and disadvantages of each memory pattern
for the option pricing problem are presented in [2]. Here we only want to point
out that while SoA has given quite a signiﬁcant speedup over AoS for the Black-
Scholes code several years ago [2,4], now the diﬀerence has been disappeared.
The AoS pattern works even a little faster than SoA on hardware described in
Section 3. However, we need to develop code that is generally eﬃcient across
diﬀerent architectures. Although the gap between SoA and AoS on the modern
infrastructure does not exceed 1%, on older infrastructures SoA gives a signif-
icant increase (up to 3 times) due to the better use of vector instructions [2].
Therefore, we still use the SoA pattern.

The next issue that needs to be addressed is the choice of an appropriate
mathematical library. Note that the cumulative normal distribution function
(F (x) in Eq. 1-3) is of particular interest. For example, it is implemented as
cdfnorm() function in the Intel Math Library of Intel C++ Compiler Classic. A
more ﬂexible and cross-platform solution is to use mathematical functions from
the C++ language standard, which does not contain the normal distribution
function but implements an error function and a similar complementary error

Black-Scholes Option Pricing on Intel CPUs and GPUs

5

function. In future computations we oﬀer to use the next formula (Eq. 4). Note
that it can work even faster than cdfnorm() in some implementations due to
simpler compute approximation.

F (x) = 0.5 + 0.5 erf

x
√

T

σ

(4)

Considering all of the above, the baseline implementation of the Black-Scholes
formula is presented in Listing 1. Its execution time on the CPU presented in
Section 3 was 3.062 sec (Table 1).

// volatility (0.2 -> 20%)
// interest rate (0.05 -> 5%)

const float sig = 0.2 f ;
const float r = 0.05 f ;
// the following constants are used
// to initialize pT , pS0 and pK arrays
const float T = 3.0 f ;
const float S0 = 100.0 f ;
const float K = 100.0 f ;

// maturity (3 -> 3 years )
// initial stock price
// strike price

void GetOptionPrices ( float * pT , float * pK , float * pS0 ,

float * pC , int N ) {
for ( int i = 0; i < N ; i ++)
{

float d1 = ( std :: log ( pS0 [ i ]/ pK [ i ]) + ( r + 0.5 f * sig * sig ) *
pT [ i ]) / ( sig * std :: sqrt ( pT [ i ]) ) ;
float d2 = ( std :: log ( pS0 [ i ]/ pK [ i ]) + ( r - 0.5 f * sig * sig ) *
pT [ i ]) / ( sig * std :: sqrt ( pT [ i ]) ) ;
float erf1 = 0.5 f + 0.5 f * std :: erf ( d1 / std :: sqrt (2.0 f ) ) ;
float erf2 = 0.5 f + 0.5 f * std :: erf ( d2 / std :: sqrt (2.0 f ) ) ;
pC [ i ] = pS0 [ i ]* erf1 - pK [ i ] *

std :: exp (( -1.0 f ) * r * pT [ i ]) * erf2 ;

}

}

Listing 1. Baseline C++ implementation of the Black-Scholes formula.

4.2 Loop Vectorization

Vectorization is one of the most eﬀective optimization techniques. As to the
Black-Scholes code, the absence of data dependencies between loop iterations
and a suitable memory pattern are expected to provide high performance of
vector calculations, which is limited by the vector register length (∼8x for 256-
bit registers and ∼16x for 512-bit registers). Modern optimizing compilers are
eﬀective enough when it comes to vectorization; the compiler report gives de-
tailed information on whether vector calculations are involved or not and why,
as well as what speedup is expected from vectorization. As a rule, for successful
vectorization, a programmer needs to specify the absence of data dependencies
explicitly because it is often impossible for the compiler to prove that compu-
tations are independent even if it is true. Moreover, we have noticed that for

6

Elena Panova et al.

the baseline Black-Scholes code (Section 4.1) and the test infrastructure we used
(Section 3) the compiler has generated two versions of the code at once, one
vectorized and one not. In this case, the decision on the possibility of using vec-
torized code is deferred until the program is run and much more information
is available. Often the vectorized version is chosen at runtime. Thus, we just
had to deﬁne a set of AVX512 instructions (-xHost, -qopt-zmm-usage=high
compiler keys) instead of default SSE instructions to better utilize CPU vector
units. As a result, execution time has been reduced to 1.155 sec (∼2.5x speedup).

4.3 Precision Reduction

We said above that the speciﬁcity of this particular ﬁnancial problem allows us to
involve inaccurate but fast calculations of mathematical functions. Accordingly,
we used the next options of the Intel compiler:

– ﬁmf-precision=low tells the compiler to use mathematical function imple-
mentations with only 11 accurate bits in mantissa instead of 24 bits available
in single precision.

– ﬁmf-domain-exclusion=31 allows the compiler to exclude special values
in mathematical functions, such as inﬁnity, NaN, or extreme values of func-
tion arguments.

This technique has given a 2x speedup compared to the previous version
(0.527 sec, see Table 1). Need to mark, we got a diﬀerent and less accurate
result (20.927 instead of a reference 20.924 for the start parameters presented in
Listing 1), but the diﬀerence does not occur until the 5th decimal digit, which is
acceptable for ﬁnancial models. However, the precision controls should be used
with care and properly analyzed for the accuracy of the result.

4.4 Parallelism

In order to use the maximum potential of a CPU, it is expedient to distribute
the computational load between its physical cores. We used the OpenMP model;
in the case of the Black-Scholes code, it was enough to add omp parallel for
pragma before the main loop and specify -qopenmp compiler option. The pro-
cessor on which the experiments were carried out has 48 physical cores (2 CPUs
with 24 cores each) and supports hyper-threading technology. The thread aﬃn-
ity mask is set to ’compact’. The maximal speedup of the parallel code was
achieved on 48 threads, hyper-threading did not impact the performance. The
best execution time was 0.053 sec, that is 10 times faster (Table 1). The reason
for which we do not observe better speedup is limited memory bandwidth, and
we can increase performance through more eﬃcient memory management (see
Section 4.5 for more details).

Black-Scholes Option Pricing on Intel CPUs and GPUs

7

4.5 NUMA-Friendly Memory Allocation

The heterogeneous memory structure is a characteristic of many modern CPUs.
Non-Uniform Memory Access (NUMA) implies that a processor can access its
memory faster than the memory of another processor on the same cluster node.
We use a two-socket system, and in the worst case, we can get 50% remote (and
ineﬃcient) access because all memory is allocated on only one processor. To avoid
this, it is enough to allocate memory in a NUMA-friendly manner. According to
the ﬁrst-touch policy [15], we should care about the appreciate initialization of a
current option batch in a parallel loop before the main calculations, as shown in
Listing 2. This trick gave the 2x speedup, the execution time has become ﬁnal
0.022 sec (Table 1).

# pragma omp parallel for simd
for ( int i = 0; i < N ; i ++) {

pT [ i ] = T ;
pS0 [ i ] = S0 ;
pK [ i ] = K ;
pC [ i ] = 0;

}

Listing 2. NUMA-friendly initialization.

Nevertheless, we have not reached the maximal speedup in parallel mode
(20x instead of maximal 48x). The Rooﬂine model [16] shown in Figure 1 in-
dicates that performance is limited by L3 cache bandwidth. In this case, the
most common technique is to increase data locality, for example, by using cache
blocking techniques, but it does not apply to the Black-Scholes code. Likely, we
have reached the execution time limit.

All optimization stages of the C++ Black-Scholes code is presented in Ta-
ble 1. We can see that the total speedup through all optimizations and paral-
lelization compared to the baseline serial code is ∼140x.

Table 1. Execution time of each C++ optimization stage.

Version
Baseline
Loop Vectorization
Precision Reduction
Parallelism
NUMA-Friendly Memory Allocation

Time, sec
3.062
1.155
0.527
0.053
0.022

5 Porting to DPC++

5.1 Baseline and Optimizations on CPUs

The eﬀective implementation of any algorithm on various types of accelerators
implies its decomposition to a suﬃciently large number of small subtasks that can

8

Elena Panova et al.

Fig. 1. The Rooﬂine model of the optimized Black-Scholes C++ code on the CPU.
The diagram is plotted by Intel Advisor from OneAPI Toolkit 2022.1. The red point
denotes the main Black-Scholes computational loop.

be performed in parallel. In the baseline Black-Scholes formula implementation
on DPC++, we choose a single option price calculation as a work item and
run all of them in parallel on a device (see Listing 3). We use SYCL built-in
mathematical functions.

handler . parallel_for ( sycl :: range <1 >( N ) , [=]( sycl :: id <1 > i ) {
float d1 = ( sycl :: log ( pS0 [ i ] / pK [ i ]) + ( r + sig2 *0.5 f ) *
pT [ i ]) / ( sig * std :: sqrt ( pT [ i ]) ) ;
float d2 = ( sycl :: log ( pS0 [ i ] / pK [ i ]) + ( r - sig2 *0.5 f ) *
pT [ i ]) / ( sig * std :: sqrt ( pT [ i ]) ) ;

float erf1 = 0.5 f + 0.5 f * sycl :: erf ( d1 * invsqrt2 ) ;
float erf2 = 0.5 f + 0.5 f * sycl :: erf ( d2 * invsqrt2 ) ;
pC [ i ] = pS0 [ i ] * erf1 - pK [ i ] * sycl :: exp (( -1.0 f ) * r *

pT [ i ]) * erf2 ;

}) ;

Listing 3. Baseline DPC++ kernel implementation of the Black-Scholes formula and
its parallel launch.

When writing high-performance code for graphics accelerators, we should
consider speciﬁcs of their memory management. Discrete GPUs require a smart
data transfer policy between host and device. DPC++ provides diﬀerent ways
to do it, explicit or implicit. In the baseline Black-Scholes code we consider a
convenient Buﬀers&Accessors strategy [12] (Listing 4). More details about the
various memory management strategies are presented in Section 5.3.

void GetOptionPrices ( float * pT , float * pK , float * pS0 ,

float * pC , int N ) {
// declaration of buffers

147104070100400700100040000.010.040.070.10.40.714710SP Vector Add Peak: 1316.47 GFLOPSSP Vector FMA Peak: 2632.69 GFLOPSDRAM Bandwidth: 227.14 GB/secL2 Bandwidth: 2735.52 GB/secDP Vector FMAPeak: 1316.1 GFLOPSDP Vector AddPeak: 658.31 GFLOPSScalar Add Peak: 85.83 GFLOPSL1 Bandwidth: 5501.74 GB/secL3 Bandwidth:412.3 GB/secGFLOPSFLOP/Byte(Arithmetic Intensity)Black-Scholes Option Pricing on Intel CPUs and GPUs

9

sycl :: buffer < float , 1 > pTbuf ( pT , sycl :: range <1 >( N ) ) ;
sycl :: buffer < float , 1 > pKbuf ( pK , sycl :: range <1 >( N ) ) ;
sycl :: buffer < float , 1 > pS0buf ( pS0 , sycl :: range <1 >( N ) ) ;
sycl :: buffer < float , 1 > pCbuf ( pC , sycl :: range <1 >( N ) ) ;

queue . submit ([&]( auto & handler ) {

// declaration of accessors
sycl :: accessor pT ( pTbuf , handler , sycl :: read_only ) ;
sycl :: accessor pK ( pKbuf , handler , sycl :: read_only ) ;
sycl :: accessor pS0 ( pS0buf , handler , sycl :: read_only ) ;
sycl :: accessor pC ( pCbuf , handler , sycl :: write_only ) ;
// here the BS computational kernel should be

}) . wait () ;

// data copying from device to host
sycl :: host_accessor pCacc ( pCbuf ) ;
// here we need to process the result

}

Listing 4. Buﬀers&Accessors data management.

As to DPC++ on CPUs, the concept of SYCL let us not think about vec-
torization and parallelism. However, we need to take into account non-uniform
memory access to avoid problems described in Section 4.5, otherwise, we get a
time similar to that presented in Section 4.4 (Table 2, Baseline). We can allo-
cate memory in a NUMA-friendly manner by, for example, initializing parameter
arrays in the separate DPC++ kernel (Listing 5).

handler . parallel_for ( sycl :: range <1 >( N ) , [=]( sycl :: id <1 > i ) {

pT [ i ] = T ;
pS0 [ i ] = S0 ;
pK [ i ] = K ;
pC [ i ] = 0;

}) ;

Listing 5. Initialization in DPC++ kernel.

Nevertheless, when we just added this piece of code, the execution time was
reduced to 0.035 sec only instead of the expected 0.022 sec observed in the
case of optimized C++ code (Section 4.5). The issue is related to the TBB
library which is used by DPC++ in this scenario. TBB threads distribute work
dynamically without regard to NUMA. As a result, we have some percentage
of remote memory access even after the appreciate initialization. To avoid this,
the DPCPP CPU PLACES environment variable with the numa domains
value should be set. In this case, dynamic scheduling of work within the TBB-
based runtime occurs within each processor separately, which removes access to
remote memory. After that, we got ﬁnal 0.024 sec, which is only 10% slower
than the optimized C++ implementation (Table 2). Taking into account that
DPC++ code can be run on CPUs and GPUs, we consider 10% of performance
to be a reasonable price to pay for portability.

10

Elena Panova et al.

Table 2. Execution time of each DPC++ optimization stage on CPUs.

Version
Baseline
Initialization in a separate kernel
NUMA-Friendly (DPCPP CPU PLACES)
Optimized C++ code

Time, sec
0.057
0.035
0.024
0.022

5.2 Experiments on GPUs

The great advantage of the DPC++ model is that a DPC++ code can be run
on many devices: CPUs, GPUs, and others. However, it does not mean that
the same code will work optimally on all the devices, and we still need to take
into account the device speciﬁcs when developing code. But it is worth noting
that the GPU-optimized code often shows high performance on CPUs as well,
so it makes sense to develop and optimize for GPUs ﬁrst. Some common GPU-
speciﬁc optimizations are presented in Section 5.4, but all of them have not given
any positive result for the benchmark at stake. The reason for it is the DRAM
memory bandwidth limitations of the GPU (Figure 2).

Fig. 2. The Rooﬂine model of the Black-Scholes DPC++ code on the GPU. The dia-
gram is plotted by Intel Advisor from OneAPI Toolkit 2022.1. The red point denotes
the main computational kernel.

The DPC++ baseline shows 0.062 sec of the kernel execution time, and this
is the best result we got. It is worth saying that this is a fairly good result, cor-
responding to the characteristics of the device. Taking into account the charac-
teristics of the memory bandwidth of this GPU and the fact that the benchmark
is memory-bound, we expected the kernel execution time to be at least 0.056 sec
(3.8 GB of batch size divided by 68 GB/sec of memory bandwidth).

407010040070010001471040SLM Bandwidth: 1089.48 GB/secL3 Bandwidth: 528.53 GB/secSP Vector FMA Peak: 2298.58 GFLOPSDRAM Bandwidth: 63.03 GB/secGTI Bandwidth: 198 GB/secSP Vector Add Peak: 1174.01 GFLOPSGFLOPSFLOP/Byte(Arithmetic Intensity)Black-Scholes Option Pricing on Intel CPUs and GPUs

11

The initialization kernel takes similar to the main kernel time – 0.061 sec.
However, we have noticed that the overhead of memory transfer and other op-
erations is highly large – more than 2 sec (Table 3). In the next section, we
demonstrate how to reduce these additional costs.

5.3 Memory Management

As the experiments in Section 5.2 show, it takes a lot of time to synchronize
and copy data from host to GPU and back. In this section, we attempt to
reduce overhead by using diﬀerent memory management strategies provided by
DPC++. The code presented in Section 5.1 is based on the Buﬀers&Accessors
model, whereby we create a buﬀer object for each parameter array and use
special accessor objects to read and write buﬀers. Based on buﬀers and accessors,
a decision to copy data from host to device and back is made independently of
a programmer. However, we can inﬂuence this by specifying at what point in
time we need the data we are interested in on a particular device by creating
a buﬀer or accessor object in the appropriate place in a DPC++ program. For
example, in Listing 5.1, the shared buﬀers for both the initialization and the
option pricing kernels, as well as the absence of appropriate accessors between
them, ensure that data is not transferred from device to host between these
kernels.

It is worth paying attention that given that we process options in batches,
we can allocate memory for only one batch and create the buﬀer objects once
when the program starts. This technique had a good eﬀect – the overhead costs
were reduced by 3 times (Table 3).

An alternative approach to using buﬀers and accessors for data transfers relies
on Uniﬁed Shared Memory (USM), which is introduced in DPC++ [12]. USM
provides explicit and implicit ways to copy data from host to device and vice
versa. The explicit way assumes direct APIs to move data between device and
host. The implicit approach is based on shared memory support implemented
in software and/or hardware, so users need just to allocate a shared memory
region that will be accessible across all the devices automatically. In contrast
to accessors, a user has to manage all the data dependencies manually while
using USM, but at the same time, shared memory is much easier to adopt for
”highly C++” codes. It is worth mentioning that the performance of USM data
transfers is strongly dependent on its particular implementation and may diﬀer
for various devices. We review both implicit and explicit approaches in relation
to the Black-Scholes formula and the infrastructure under consideration.

Following the implicit USM model, we create data pointers using the
malloc shared() function and then simply access them inside a kernel, with
any necessary data movement done automatically (Listing 6). In our case, this
strategy slightly reduces data transfer time, but apparently requires additional
synchronizations between the host and device when calling malloc shared(),
which still does not matter if memory is allocated at once and common for all
batches (Table 3).

12

Elena Panova et al.

void GetOptionPrices ( float * pT , float * pK , float * pS0 ,

float * pC , int N ) {
// shared memory allocation
float * pT = sycl :: malloc_shared < float >( N , queue ) ;
float * pS0 = sycl :: malloc_shared < float >( N , queue ) ;
float * pK = sycl :: malloc_shared < float >( N , queue ) ;
float * pC = sycl :: malloc_shared < float >( N , queue ) ;

// here we need to run the initialization kernel
// and the option price calculation kernel
// and process the result pC

// freeing memory
sycl :: free ( pT ) ; sycl :: free ( pS0 ) ;
sycl :: free ( pK ) ; sycl :: free ( pC ) ;

}

Listing 6. Implicit USM data movement.

It is worth noting that the resulting performance of implicit data movement
in USM strongly depends on its internal architecture-speciﬁc implementation.
Perhaps on newer hardware and software, the malloc shared approach will pro-
vide more beneﬁts. As for the current infrastructure, we suspect that some data
arrays are copied from host to device and back again a few extra times. There-
fore, we decided to allocate memory directly on the device and explicitly transfer
only the result array pC to the host under the explicit USM approach. The cor-
responding implementation shown in Listing 7 gave the best result (0.37 sec
overhead, Table 3).

void GetOptionPrices ( float * pT , float * pK , float * pS0 ,

float * pC , int N ) {
// memory allocation on host to keep result
float * pCHost = new float [ N ];
// memory allocation on device
float * pT = sycl :: malloc_device < float >( N , queue ) ;
float * pS0 = sycl :: malloc_device < float >( N , queue ) ;
float * pK = sycl :: malloc_device < float >( N , queue ) ;
float * pC = sycl :: malloc_device < float >( N , queue ) ;

// here we need to run the initialization kernel
// and the option price calculation kernel

// data copying from device to host
queue . memcpy ( pCHost , pC , N * sizeof ( float ) ) . wait () ;
// here we need to process the result pCHost
// and free all allocated memory

}

Listing 7. Explicit USM data movement.

Black-Scholes Option Pricing on Intel CPUs and GPUs

13

Table 3. Diﬀerent DPC++ memory management models: kernel execution time and
overhead time on the GPU. ’One batch’ means that we allocate memory for each
option batch and include its time into measurements; ’multiple batches’ means that
we allocate memory that is shared for all batches at once, and do not include this into
measurements. The table shows the time of one batch processing.

Version

Buﬀers&Accessors, one batch
Buﬀers&Accessors, multiple batches
USM, implicit, one batch
USM, implicit, multiple batches
USM, explicit, one batch
USM, explicit, multiple batches

Main
kernel,
sec

Initiali-
zation
kernel, sec

0.062

0.061

sec

Over- Total,
head,
sec
2.367
0.748
2.987
0.658
1.158
0.369

2.551
0.932
3.110
0.781
1.281
0.492

5.4 Several Common Optimization Tricks

In this subsection, we look at important optimization techniques that can provide
signiﬁcant performance gains on GPUs. Note that in our particular case, they
did not work for various reasons. In several cases, the compiler and runtime did a
great job without our help. In other cases, the nature of the task, where memory
access operations are a signiﬁcant part of the work compared to calculations, did
not allow for achieving additional speedup. In the meantime, it seemed important
to us to demonstrate these techniques for performance optimization, as they may
be useful to researchers and developers in other applications.

In the baseline DPC++ implementation, we start N work items for every op-
tion. The initial one-dimensional range is divided into work-groups automatically
by the DPC++ compiler and runtime. However, a programmer can vary a range,
work-group or sub-group size, and here the potential for optimization lies. The
trick presented in Listing 8 contributes to the solution of possible optimization
problems such as underutilization of GPU cores, the overhead of maintaining a
large number of threads, and suboptimal work-group size. The example demon-
strates the variation in the total number of threads and the group size. It is worth
paying attention that in the internal kernel loop it is necessary to walk through
the iterations with a step equal to the number of threads to avoid memory bank
conﬂicts. As to the Black-Scholes code, we checked diﬀerent combinations of
thread count and work-group size. Experiments on GPUs have shown that the
optimal values are the maximal thread count and group size, that is, the default
conﬁguration used by the DPC++ compiler and runtime is the most beneﬁcial.

handler . parallel_for ( sycl :: nd_range <1 >( THREAD_COUNT ,

GROUP_SIZE ) , [=]( sycl :: nd_item <1 > item ) {
for ( int i = item . get_global_id (0) ; i < N ; i += THREAD_COUNT ) {

// the i - th element processing

}}) ;

Listing 8. An example of varying the number of threads and group size.

14

Elena Panova et al.

Another advantageous technique is to use vector data types [12]. In this way,
we explicitly state that some elements are arranged in a row in memory. It can
positively aﬀect the speed of loading and computing on GPUs as well as on
CPUs. We have tried to use sycl::ﬂoat4 data type but, in our case, it has not
impacted the performance.

It should also be said that we tried using the diﬀerent memory patterns in
an attempt to optimize it for GPUs. The AoS pattern did not improve execution
time compared to SoA, but we have noticed that it is proﬁtable to store read-only
and write-only data separately to reduce the load on the performance-limiting
DRAM.

6 Conclusion

In this paper, we present a high-performance implementation of the Black-
Scholes option pricing formula in the C++ and SYCL (DPC++) programming
languages. We optimize it step by step and demonstrate how various optimization
techniques improve performance for commonly applied Intel CPUs and GPUs. In
particular, on CPUs, we employ and analyze vectorization, precision reduction,
single-node parallelism, and optimal use of the NUMA architecture. For GPUs,
we discuss device-friendly memory access patterns, relevant data management,
and employing vector data types. Experiments showed that on CPUs, the opti-
mized DPC++ implementation was only 10% slower than the highly optimized
OpenMP implementation, while it was possible to run it also on GPUs and ob-
serve the expected performance based on the device computational capabilities.
We demonstrated that the performance was limited by memory bandwidth both
on CPU and GPU.

We hope that our results will be useful to other researchers who are planning
to port their codes from C++ to SYCL (DPC++). In this regard, we discussed
not only those optimization techniques that improved performance but also those
that did not speed up our application. We believe that these techniques can
work in other codes. They also can help on upcoming devices with signiﬁcantly
larger memory bandwidth. In general, the emergence and development of SYCL
ideas within the oneAPI model looks like a promising direction in modern High
Performance Computing.

7 Acknowledgements

The work is supported by the oneAPI Center of Excellence program and by the
Ministry of Science and Higher Education of the Russian Federation, project no.
0729-2020-0055.

References

1. Black, F., Scholes, M.: The pricing of options and corporate liabilities. In: World
Scientiﬁc Reference on Contingent Claims Analysis in Corporate Finance: Volume
1: Foundations of CCA and Equity Valuation, pp. 3–21. World Scientiﬁc (2019)

Black-Scholes Option Pricing on Intel CPUs and GPUs

15

2. Meyerov, I., Sysoyev, A., Astaﬁev, N., Burylov, I.: Performance optimization of
In: High Performance Parallelism Pearls: Multicore and

black-scholes pricing.
Many-core Programming Approaches, pp. 319–340. Springer, Cham (2014)

3. Grauer-Gray, S., Killian, W., Searles, R., Cavazos, J.: Accelerating ﬁnancial ap-
plications on the gpu. In: Proceedings of the 6th Workshop on General Purpose
Processor Using Graphics Processing Units, pp. 127–136 (2013)

4. Smelyanskiy, M., Sewall, J., Kalamkar, D.D., Satish, N., Dubey, P., Astaﬁev, N.,
Burylov, I., Nikolaev, A., Maidanov, S., Li, S., et al.: Analysis and optimization of
ﬁnancial analytics benchmark on modern multi-and many-core ia-based architec-
tures. In: 2012 SC Companion: High Performance Computing, Networking Storage
and Analysis, pp. 1154–1162. IEEE (2012)

5. Podlozhnyuk, V.: Black-scholes option pricing (2007)
6. Pharr, M., Fernando, R.: GPU Gems 2: Programming techniques for high-
performance graphics and general-purpose computation (gpu gems). Addison-
Wesley Professional (2005)

7. oneAPI: A New Era of Heterogeneous Computing.

https://www.intel.com/

content/www/us/en/developer/tools/oneapi/overview.html

8. OpenCL: open standard for parallel programming of heterogeneous systems. https:

//www.khronos.org/opencl/

9. OpenACC. https://www.openacc.org/

10. Edwards, H.C., Trott, C.R.: Kokkos: Enabling performance portability across
manycore architectures. In: 2013 Extreme Scaling Workshop (xsw 2013), pp. 18–24.
IEEE (2013)

11. Zenker, E., Worpitz, B., Widera, R., Huebl, A., Juckeland, G., Kn¨upfer, A., Nagel,
W.E., Bussmann, M.: Alpaka–an abstraction library for parallel kernel accelera-
tion. In: 2016 IEEE International Parallel and Distributed Processing Symposium
Workshops (IPDPSW), pp. 631–640. IEEE (2016)

12. Reinders, J., Ashbaugh, B., Brodman, J., Kinsner, M., Pennycook, J., Tian, X.:
Data parallel C++: mastering DPC++ for programming of heterogeneous systems
using C++ and SYCL. Springer Nature (2021)

13. Reyes, R., Lom¨uller, V.: Sycl: Single-source c++ accelerator programming.
Parallel Computing: On the Road to Exascale, pp. 673–682. IOS Press (2016)

In:

14. SYCL. https://www.khronos.org/sycl/
15. Hager, G., Wellein, G.: Introduction to high performance computing for scientists

and engineers. CRC Press (2010)

16. Marques, D., Duarte, H., Ilic, A., Sousa, L., Belenov, R., Thierry, P., Matveev, Z.A.:
Performance analysis with cache-aware rooﬂine model in intel advisor. In: 2017
International Conference on High Performance Computing & Simulation (HPCS),
pp. 898–907. IEEE (2017)

