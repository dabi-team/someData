2
2
0
2

g
u
A
8
1

]

V
C
.
s
c
[

1
v
6
3
8
8
0
.
8
0
2
2
:
v
i
X
r
a

A Multi-modal Registration and Visualization
Software Tool for Artworks using CraquelureNet

Aline Sindel, Andreas Maier, and Vincent Christlein

Pattern Recognition Lab, FAU Erlangen-N¨urnberg, Germany
aline.sindel@fau.de

Abstract. For art investigations of paintings, multiple imaging tech-
nologies, such as visual light photography, infrared reﬂectography, ultra-
violet ﬂuorescence photography, and x-radiography are often used. For
a pixel-wise comparison, the multi-modal images have to be registered.
We present a registration and visualization software tool, that embeds
a convolutional neural network to extract cross-modal features of the
crack structures in historical paintings for automatic registration. The
graphical user interface processes the user’s input to conﬁgure the regis-
tration parameters and to interactively adapt the image views with the
registered pair and image overlays, such as by individual or synchronized
zoom or movements of the views. In the evaluation, we qualitatively and
quantitatively show the eﬀectiveness of our software tool in terms of reg-
istration performance and short inference time on multi-modal paintings
and its transferability by applying our method to historical prints.

Keywords: Multi-modal registration · Visualization · Convolutional neu-
ral networks.

1

Introduction

In art investigations often multiple imaging systems, such as visual light photog-
raphy (VIS), infrared reﬂectography (IRR), ultraviolet ﬂuorescence photography
(UV), and x-radiography (XR), are utilized. For instance, IR is used to reveal un-
derdrawings, UV to visualize overpaintings and restorations, and XR to highlight
white lead. Since the multi-modal images are acquired using diﬀerent imaging
systems, we have to take into account diﬀerent image resolutions and varying
viewpoints of the devices. Thus, for a direct comparison on pixel level, image
registration is crucial to align the multi-modal images.

Image registration methods for art imaging can mainly be split into intensity-,
control point- and feature-based approaches. The intensity-based method of
Cappellini et al . [4] uses mutual information to iteratively register multispec-
tral images. The control point- and feature-based methods compute a geometric
transform based on detected correspondences in both images. Murashov [12]
detects local grayscale maxima as control points in VIS-XR image pairs and ap-
plies coherent point drift [13]. Conover et al . [6] uses Wavelet transform, phase
correlation, and disparity ﬁltering for control point detection in sub-images of

© Pattern Recognition for Cultural Heritage (PatReCH) 2022 Workshop at the 26th International
Conference on Pattern Recognition (ICPR) 2022, scheduled for August 21, 2022.

 
 
 
 
 
 
2

A. Sindel et al.

Fig. 1: Our multi-modal registration and visualization tool CraquelureNetApp.
Users can select the images and registration options in the GUI. The registra-
tion itself is performed fully automatically using the CraquelureNet, a CNN for
multi-modal keypoint detection and description, which was ported from PyTorch
to C++. Due to the high-resolution images of paintings, the CraquelureNet is
applied patch-wise. The registration results are depicted inside the GUI and
the users can interact with the visualizations, e.g. joint zooming of the views.
Image sources: Workshop Lucas Cranach the Elder, Katharina of Bora (De-
tail), visual light and UV-ﬂuorescence photographs, captured by Ulrike H¨ugle,
Stiftung Deutsches Historisches Museum, Berlin, Cranach Digital Archive, KKL-
No IV.M21b, all rights reserved

VIS, IR, and XR. The feature-based method of Zacharopoulos et al . [19] uses
SIFT [11] to align multispectral images of artworks. CraquelureNet [17] uses a
convolutional neural network (CNN) to detect keypoints and descriptors based
on branching points in the crack structure (craquelure) of the paint, as it is vis-
ible by all imaging systems, in contrast to the depicted image content that can
be very diﬀerent in the multi-modal images.

More and more museums or art projects provide the ability to inspect their
artworks in interactive website viewers. Speciﬁcally for multi-modal images web-
site viewer have been designed that allow a synchronized scrolling and zooming of
the image views [1,10] or a curtain viewer [1] that allows to interactively inspect
multiple images in a single view. For these projects, the speciﬁc multi-modal
images of artworks were pre-registered oﬄine.

For the daily work, it would be practical for art technologists and art histori-
ans to have a tool, with which they can easily perform the registration themselves
and also can interactively inspect the registered images. In the ﬁeld of medical
2D-2D or 3D-3D registration, there are open source tools such as MITK [18]
that provides a graphical user interface (GUI) application for iterative rigid and
aﬃne registration and also a developer software framework. Since these software
tools are very complex, domain knowledge is required to adapt the algorithms
for multi-modal registration of paintings. The ImageOverlayApp [16] is a small
and easy to handle GUI application that allows the direct comparison of two reg-
istered artworks using image superimposition and blending techniques. However,
here as well as in the online viewers, image registration is not provided.

Registration and Visualization Software Tool for Artworks

3

In this paper, we propose a software tool for automatic multi-modal image
registration and visualization of artworks. We designed a GUI to receive the
user’s registration settings and to provide an interactive display to show the
registration results, such as superimposition and blending of the registered im-
age pair, and both, individual and synchronized zooming and movement of the
image views. As registration method, we integrate the keypoint detection and
description network CraquelureNet into our application by porting it to C++. A
quantitative evaluation and qualitative examples show the eﬀective application
of our software tool on our multi-modal paintings dataset and its transferability
to our historical prints dataset.

2 Methods

In this section, the software tool for multi-modal registration and visualization
is described.

2.1 Overview of the Registration and Visualization Tool

In Fig. 1, the main building blocks of the CraquelureNetApp are shown, con-
sisting of the GUI, the preprocessing such as data loading and patch extraction,
the actual registration method, which is split into the keypoint detection and
description network CraquelureNet that was ported from PyTorch to LibTorch
and into descriptor matching, homography estimation, and image warping, and
the computation of visualizations of the registration results and its interactions
with the user.

2.2 Registration Method

CraquelureNet [17] is composed of a ResNet backbone and a keypoint detection
and a keypoint description head. The network is trained on small 32 × 32 × 3
sized image patches using a multi-task loss for both heads. The keypoint de-
tection head is optimized using binary cross-entropy loss to classify the small
patches into “craquelure” and “background”, i.e. whether the center of the patch
(a) contains a branching or sharp bend of a crack structure (“craquelure”) or
(b) contains it only in the periphery or not at all (“background”). The key-
point description head is trained using bidirectional quadruplet loss [17] to learn
cross-modal descriptors. Using online hard negative mining, for a positive key-
point pair the hardest non-matching descriptors are selected in both directions
within the batch. Then, the positive distances (matching keypoint pairs) are
minimized while the negative distances (hardest non-matching keypoint pairs)
are maximized.

For inference, larger image input sizes can be fed to the fully convolutional
network. Due to the architectural design, the prediction of the keypoint detec-
tion head and description head are of lower resolution than the input image and
hence are upscaled by a factor of 4 using bicubic interpolation for the keypoint

4

A. Sindel et al.

Fig. 2: The graphical user interface (GUI) of our CraquelureNetApp. The diﬀer-
ent elements of the GUI are marked in orange: the menu, the toolbar items for
registration and visualization and the views.

heatmap and bilinear interpolation for the descriptors based on the extracted
keypoint positions. Keypoints are extracted using non-maximum suppression
and by selecting all keypoints with a conﬁdence score higher than τkp based on
the keypoint heatmap. Since the images can be very large, CraquelureNet is ap-
plied patch-based and the keypoints of all patches are merged and reduced to the
Nmax keypoints with the highest conﬁdence score. Then, mutual nearest neighbor
descriptor matching is applied and Random Sample Consensus (RANSAC) [9]
is used for homography estimation [17].

CraquelureNet [17] is completely implemented in Python using PyTorch and
OpenCV. To transfer the trained PyTorch model to C++, we made use of the
TorchScript conversion functionality and restructured the source code accord-
ingly using a combination of TorchScript modules and Tracing. We have directly
reimplemented the other parts of the registration pipeline in C++ using the same
OpenCV functions as in Python.

For homography estimation, we provide the option to choose between diﬀer-
ent robust estimators of the OpenCV library: RANSAC [9] which is already used
in [17,15] is the default option. RANSAC is an iterative method to estimate a
model, here homography, for data points that contain outliers. In each iteration,
a model is estimated on a random subset of the data and is scored using all data
points, i.e. the number of inliers for that model is computed. In the end, the
best model is selected based on the largest inlier count and can optionally be
reﬁned. Universal RANSAC (USAC) [14] includes diﬀerent RANSAC variants
into a single framework, e.g. diﬀerent sampling strategies, or extend the veriﬁca-
tion step by also checking for degeneracy. We use the USAC default method of
OpenCV 4.5.5 that applies iterative local optimization to the so far best mod-
els [5]. Further, we enable two RANSAC variants which are also included in the
OpenCV USAC framework: Graph-Cut RANSAC (GC-RANSAC) [2] uses the

Registration and Visualization Software Tool for Artworks

5

Fig. 3: The menu and dialogs to conﬁgure registration and save options.

graph-cut algorithm for the local optimization and MAGSAC++ [3] applies a
scoring function that does not require a threshold and a novel marginalization
method.

2.3 Graphical User Interface

The GUI is implemented in C++ as a Qt5 desktop application. In Fig. 2 the
main GUI elements are marked in orange. The user can interact with the GUI
using the menu or the toolbar that provides items speciﬁed for the registration
and visualization task or can also directly interact with the views. The size of
the views depends on the size of the application window.

User interaction prior to registration: To perform registration, two images
have to be selected. They can either be chosen via the menu (Fig. 3a) using a
ﬁle opener or per drag and drop of the image ﬁle to the image view. The selected
images are visualized in the two image views in the top row. Per click on the
conﬁgure button in the menu (Fig. 3b) or on the registration toolbar (“gear”
icon), a user dialog (Fig. 3c) is opened to choose the registration options, such as
patch size, number of maximum keypoints Nmax, image input size, method for
homography estimation (RANSAC, USAC, GC-RANSAC, or MAGSAC++),
whether to run on the GPU (using CUDA) or on the CPU and whether to
visualize keypoint matches. Custom settings of patch size, input image size, and
maximum number of keypoints are also possible when the predeﬁned selections
are not suﬃcient. The settings are saved in a conﬁguration ﬁle to remember the
user’s choice. It is always possible to restore the default settings. The registration
is started by clicking the run button in the menu (Fig. 3b) or in the toolbar
(“triangle” icon).

Visualizations and user interaction: We include two diﬀerent superimposi-
tion techniques to visually compare the registration results, a false-color image
overlay (red-cyan) of the registered pair and a blended image overlay, similarly
to [16]. After registration, the views in the GUI are updated with the transformed
moving image (view 2) and the image fusions in the bottom row (view 3 and 4).
The red-cyan overlay (view 4), stacks the reference image (view 1) into the red

6

A. Sindel et al.

Fig. 4: VIS-IRR registration of paintings using CraquelureNetApp. In (a) the
keypoint matches between the reference image (VIS) and moving image (IRR)
are visualized as yellow lines. In (b) the registered image pair and the blended
overlay and the false color overlay are depicted as complete images and in (c)
they are synchronously zoomed in. Image sources: Meister des Marienlebens,
Tempelgang Mari¨a, visual light photograph and infrared reﬂectogram, German-
isches Nationalmuseum, Nuremberg, on loan from Wittelsbacher Ausgleichs-
fonds/Bayerische Staatsgem¨aldesammlungen, Gm 19, all rights reserved

and the transformed moving image (view 2) into the green and blue channel.
The blended image (view 3) is initially shown with alpha value 0.5. By moving
the slider in the visualization toolbar, the user can interactively blend between
both registered images. For the optional visualization of the keypoint matches, a
separate window is opened that shows both original images with superimposed
keypoints as small blue circles and matches as yellow lines using OpenCV.

Additionally to the image overlays, we have implemented a synchronization
feature of the views that enriches the comparison. It can be activated by the
“connect views” icon in the visualization toolbar. All interactions with one view
are propagated to the other views. Using the mouse wheel the view is zoomed
in or out with the focus at the current mouse position. Using the arrow keys the
image view can be shifted in all directions. By activating the “hand mouse drag”
item in the toolbar, the view can be shifted around arbitrarily. By pressing the
“maximize” icon in the toolbar, the complete image is ﬁtted into the view area
preserving aspect ratio. This can also be useful after asynchronous movement of
single views to reset them to a common basis.

To save the registration results to disk, the user can click the “save” button
in the menu (Fig. 3a) to choose the ﬁles to save in a dialog (Fig. 3d) or the “save
all” button to save them all.

3 Applications and Evaluation

In this section, we apply the CraquelureNetApp to multi-modal images of paint-
ings and to images of diﬀerent prints of the same motif. We use the test param-

Registration and Visualization Software Tool for Artworks

7

Fig. 5: VIS-UV registration of paintings using CraquelureNetApp. In (a) the
keypoint matches of VIS and UV are densely concentrated at the craquelure
in the facial region. (b) and (c) show the registered pair and image fusions of
the complete images and synchronized details. Image sources: Workshop Lucas
Cranach the Elder or Circle, Martin Luther, visual light photograph, captured
by Gunnar Heydenreich and UV-ﬂuorescence photograph, captured by Wibke
Ottweiler, Lutherhaus Wittenberg, Cranach Digital Archive, KKL-No I.6M3, all
rights reserved

eters as deﬁned in [17]: GPU, patch size of 1024, Nmax = 8000, τkp = 0, resize
to same width, and RANSAC with a reprojection error threshold τreproj = 5.

For the quantitative evaluation, we measure the registration performance
based on the success rate of successfully registered images. The success rate is
computed by calculating the percentage of image pairs for which the error dis-
tance of manual labeled control points of the registered pair using the predicted
homography is less or equal to an error threshold (cid:15). As metric for the error dis-
tance, we use the mean Euclidean error (ME) and maximum Euclidean error
(MAE) [15].

As comparison methods, we use the conventional keypoint and feature de-
scriptor SIFT [11] and the pretrained models of the two deep learning methods,
SuperPoint [7] and D2-Net [8], which we apply patch-based to both the paintings
and prints. SuperPoint is a CNN with a keypoint detection head and a keypoint
description head and is trained in a self-supervised manner by using homographic
warpings. D2-Net simultaneously learns keypoint detection and description with
one feature extraction CNN, where keypoints and descriptors are extracted from
the same set of feature maps. For all methods, we use the same test settings as
for our method (patch size, Nmax, RANSAC).

3.1 Multi-modal Registration of Historical Paintings

The pretrained CraquelureNet [17], which we embedded into our registration
tool, was trained using small patches extracted from multi-modal images of 16th
century portraits by the workshop of Lucas Cranach the Elder and large German

8

A. Sindel et al.

Fig. 6: VIS-XR registration of paintings using CraquelureNetApp. In (a) the
keypoint matches of VIS and XR are mostly concentrated at the craquelure in
the lighter image area such as the face, hands, and green background. (b) and
(c) show the qualitative registrations results as overall view and as synchro-
nized zoomed details. Image sources: Lucas Cranach the Elder, Martin Luther
as “Junker J¨org”, visual light photograph Klassik Stiftung Weimar, Museum, X-
radiograph HfBK Dresden (Mohrmann, Riße), Cranach Digital Archive, KKL-No
II.M2, all rights reserved

panel paintings from the 15th to 16th century. We use the images of the test
split from these multi-modal datasets (13 pairs per domain: VIS-IRR, VIS-UV,
VIS-XR) and the corresponding manually labeled control point pairs (40 point
pairs per image pair) [17] to test the CraquelureNetApp.

The qualitative results of three examples (VIS-IRR, VIS-UV and VIS-XR)
are shown in Figs. 4 to 6. For each example, the keypoint matches, the complete
images (Fit to view option in the GUI), and a zoomed detail view in the syn-
chronization mode are depicted showing good visual registration performance
for all three multi-modal pairs.

Secondly, we compare in Table 1 the success rates of ME and MAE of the
registration for the C++ and PyTorch implementation of CraquelureNet us-
ing RANSAC for homography estimation or alternatively using USAC, GC-
RANSAC, or MAGSAC++. The C++ implementation using RANSAC achieves
comparable results as the PyTorch implementation (both τreproj = 5): For VIS-
IRR and VIS-UV, all images are registered successfully using ME with an error
threshold (cid:15) = 5 (PyTorch) and (cid:15) = 7 (C++). For VIS-XR, 12 (C++) or 11
(PyTorch) out of 13 images were successfully registered using ME ((cid:15) = 7). The
small deviations between the two models are due to slightly diﬀerent implementa-
tions e.g. of non-maximum suppression that were necessary for the conversion to
C++. Using the more recent USAC, GC-RANSAC, or MAGSAC++ is slightly
less robust for VIS-IRR and VIS-UV, since not more than 12 out of 13 image
pairs can be registered using ME ((cid:15) = 7) as one image pair fails completely.
VIS-XR registration is the most diﬃcult part of the three domain pairs due to

Registration and Visualization Software Tool for Artworks

9

Table 1: Quantitative evaluation for multi-modal registration of paintings for
the VIS-IRR, VIS-UV, and VIS-XR test datasets (each 13 image pairs) using
CraquelureNet with diﬀerent robust homography estimation methods. Registra-
tion results are evaluated with the success rates (SR) of mean Euclidean error
(ME) and maximum Euclidean error (MAE) of the control points for diﬀerent
error thresholds (cid:15). Best results are highlighted in bold.

Multi-modal CraquelureNet
Dataset

Model

Homography
Method

SR of ME [%] ↑
(cid:15) = 5

(cid:15) = 3

(cid:15) = 7

SR of MAE [%] ↑
(cid:15) = 8

(cid:15) = 10

(cid:15) = 6

VIS-IRR

VIS-UV

VIS-XR

84.6
PyTorch (Python) RANSAC
92.3
RANSAC
LibTorch (C++)
USAC
LibTorch (C++)
92.3
GC-RANSAC 92.3
LibTorch (C++)
MAGSAC++ 92.3
LibTorch (C++)

PyTorch (Python) RANSAC
92.3
RANSAC
LibTorch (C++)
84.6
92.3
USAC
LibTorch (C++)
GC-RANSAC 92.3
LibTorch (C++)
84.6
MAGSAC++
LibTorch (C++)

69.2
PyTorch (Python) RANSAC
53.8
RANSAC
LibTorch (C++)
76.9
USAC
LibTorch (C++)
GC-RANSAC
LibTorch (C++)
76.9
MAGSAC++ 84.6
LibTorch (C++)

100.0
92.3
92.3
92.3
92.3

100.0
92.3
92.3
92.3
92.3

84.6
84.6
84.6
84.6
84.6

100.0
100.0
92.3
92.3
92.3

100.0
100.0
92.3
92.3
92.3

84.6
92.3
84.6
84.6
84.6

38.5
38.5
46.2
46.2
46.2

46.2
53.8
53.8
53.8
46.2

23.1
30.8
23.1
23.1
15.4

69.2
69.2
69.2
69.2
69.2

53.8
53.8
69.2
69.2
61.5

38.5
46.2
38.5
38.5
53.8

84.6
84.6
84.6
84.6
84.6

61.5
53.8
69.2
69.2
69.2

61.5
61.5
76.9
76.9
76.9

the visually highly diﬀerent appearance of the VIS and XR images. Here, we can
observe similar performance of ME, with a slightly higher percentage of USAC,
GC-RANSAC, and especially MAGSAC++ for (cid:15) = 3, but for (cid:15) = 7 those are on
par with the PyTorch model and are slightly inferior to the C++ model using
RANSAC. Regarding the success rates of MAE, we observe a bit higher values
for USAC, GC-RANSAC, and MAGSAC++ than for both RANSAC models for
VIS-UV and VIS-XR ((cid:15) = {8, 10}), while for VIS-IRR they are the same.

In Fig. 7, the multi-modal registration performance of our C++ Craque-
lureNet (RANSAC) is measured in comparison to SIFT [11], SuperPoint [7],
and D2-Net [8]. CraquelureNet achieves the highest success rates of ME and
MAE for all multi-modal pairs. D2-Net is relatively close to CraquelureNet for
VIS-IRR with the same SR of ME but with a lower SR of MAE. For VIS-IRR
and VIS-UV, all learning-based methods are clearly better than SIFT. For the
challenging VIS-XR domain, the advantage of our cross-modal keypoint detector
and descriptor is most distinct, since for (cid:15) = 7, CraquelureNet still achieves high
success rates of 92.3 % for ME and 61.5 % for MAE, whereas for (cid:15) = 7, D2-Net
only successfully registers 61.5 % for ME and 15.3 % for MAE, SuperPoint only
23 % for ME and none for MAE, and lastly, SIFT does not register any VIS-
XR image pair successfully. D2-Net was developed to ﬁnd correspondences in
diﬃcult image conditions, such as day-to-night or depiction changes [8], hence
it also is able to detect to some extend matching keypoint pairs in VIS-XR im-
ages. SuperPoint’s focus is more on images with challenging viewpoints [7,8],
thus the pretrained model results in a lower VIS-XR registration performance.
In our prior work [17], we ﬁne-tuned SuperPoint using image patches extracted

10

A. Sindel et al.

SIFT

D2-Net

SuperPoint

CraquelureNet (C++)

1
3
.
2
9

1
3
.
2
9

2
9
.
6
7

0
0
.
0
0
1

0
0
.
0
0
1

2
6
.
4
8

8
0
.
3
2

8
0
.
3
2

100
80
60
40
20
0

0
0
.
0
0
1

1
3
.
2
9

2
6
.
4
8

6
4
.
8
3

2
6
.
4
8

2
9
.
6
7

4
5
.
1
6

8
0
.
3
2

100
80
60
40
20
0

100
80
60
40
20
0

5
8
.
3
5

8
0
.
3
2

9
6
.
7

0
0
.
0

1
3
.
2
9

4
5
.
1
6

8
0
.
3
2

0
0
.
0

(cid:15) = 3

(cid:15) = 7

(cid:15) = 3

(cid:15) = 7

(cid:15) = 3

(cid:15) = 7

(a) VIS-IRR

(b) VIS-UV

(c) VIS-XR

2
6
.
4
8

2
9
.
6
7

2
9
.
6
7

6
4
.
8
3

8
3
.
5
1

9
6
.
7

9
6
.
7

8
0
.
3
2

100
80
60
40
20
0

100
80
60
40
20
0

5
8
.
3
5

6
4
.
8
3

7
7
.
0
3

8
3
.
5
1

5
8
.
3
5

5
8
.
3
5

5
1
.
6
4

8
3
.
5
1

100
80
60
40
20
0

4
5
.
1
6

7
7
.
0
3

0
0
.
0

0
0
.
0

0
0
.
0

8
3
.
5
1

0
0
.
0

0
0
.
0

(cid:15) = 6

(cid:15) = 10

(cid:15) = 6

(cid:15) = 10

(cid:15) = 6

(cid:15) = 10

]

%

[

E
M

f
o
R
S

]

%

[

E
A
M

f
o
R
S

(d) VIS-IRR

(e) VIS-UV

(f) VIS-XR

Fig. 7: Quantitative evaluation for our multi-modal paintings dataset using SR
of ME with (cid:15) = 3, 7 and SR of MAE with (cid:15) = 6, 10.

from our manually aligned multi-modal paintings dataset, which did not result
in an overall improvement. On the other hand, CraquelureNet is robust for the
registration of all multi-modal pairs and does not require an intensive training
procedure, as it is trained in eﬃcient time only using very small image patches.
The execution time of CraquelureNet (C++ and PyTorch) for the VIS-IRR
registration (including loading of network and images, patch extraction of size
1024×1024×3 pixels, network inference, homography estimation using RANSAC
and image warping) was about 11 s per image pair on the GPU and about 2 min
per image pair on the CPU. We used an Intel Xeon W-2125 CPU 4.00 GHz
with 64 GB RAM and one NVIDIA Titan XP GPU for the measurements. The
comparable execution times of the C++ and PyTorch implementation of Craque-
lureNet (the restructured one for a fair comparison) is not surprising, as PyTorch
and OpenCV are wrappers around C++ functions. The relatively fast inference
time for the registration of high-resolution images makes CraquelureNet suitable
to be integrated into the registration GUI and to be used by art technologists
and art historians for their daily work.

3.2 Registration of Historical Prints

To evaluate the registration performance for prints, we have created a test
dataset of in total 52 images of historical prints with manually labeled con-
trol point pairs. The dataset is composed of 13 diﬀerent motifs of 16th century
prints with each four exemplars that may show wear or production-related dif-
ferences in the print. For each motif a reference image was selected and the other

Registration and Visualization Software Tool for Artworks

11

Fig. 8: VIS-VIS registration of prints using CraquelureNetApp. CraquelureNet
detects a high number of good matches for the two prints in (a), although it
did not see images of prints during training. (b) and (c) show the qualitative
registration results as overall and detail views. Image sources: Monogramist IB
(Georg Pencz), Philipp Melanchthon with beret and cloak, (left) Germanisches
Nationalmuseum, Nuremberg, K 21148, captured by Thomas Klinke and (right)
Klassik Stiftung Weimar, Museen, Cranach Digital Archive, DE KSW Gr-2008-
1858, all rights reserved

three copies will be registered to the reference, resulting in 39 registration pairs.
For each image pair, 10 control point pairs were manually annotated.

We test the CraquelureNet, that was solely trained on paintings, for the 16th
century prints. One qualitative example is shown in Fig. 8. CraquelureNet also
ﬁnds a high number of good matches in this engraving pair, due to the multitude
of tiny lines and branchings in the print as CraquelureNet is mainly focusing on
these branching points.

For the quantitative evaluation, we compute the success rates of ME and
MAE for the registration of the print dataset using our CraquelureNet C++
model in comparison to using SIFT [11], SuperPoint [7], and D2-Net [8]. For
the registration, the images are scaled to a ﬁxed height of 2000 pixel, as then
the structures in the prints have a suitable size for the feature detectors. In
the GUI of CraquelureNetApp this option is “resize to custom height”. The
results of the comparison are depicted in Fig. 9. The CNN-based methods obtain
clearly superior results to SIFT for both metrics. Overall, CraquelureNet and
SuperPoint show the best results, as both achieve a success rate of ME of 100 %
at error threshold (cid:15) = 4, where they are closely followed by D2-Net at (cid:15) = 5
and all three methods achieve a success rate of MAE close to 100 % at (cid:15) = 10.
For smaller error thresholds, CraquelureNet is slightly superior for MAE and
SuperPoint for ME. As none of the methods was ﬁne-tuned for the print dataset,
this experiment shows the successful possibility of applying the models to this
new dataset.

12

A. Sindel et al.

SIFT

100

80

60

40

20

0

]

%

[

E
M

f
o
R
S

D2-Net

SuperPoint

CraquelureNet (C++)

]

%

[

E
A
M

f
o
R
S

100

80

60

40

20

0

5

6

7

8

9

10

Error threshold

(b)

1

2

3

4

5

6

Error threshold

(a)

Fig. 9: Quantitative comparison of success rates for registration of prints (39
image pairs). For all methods RANSAC with τreproj = 5 was used. None of
the methods was ﬁne-tuned for the print dataset. In (a) the success rate of
mean Euclidean error (ME) for the error thresholds (cid:15) = {1, 2, ..., 6} and in (b)
the success rate of maximum Euclidean error (MAE) for the error thresholds
(cid:15) = {5, 6, ..., 10} is plotted.

4 Conclusion

We presented an interactive registration and visualization tool for multi-modal
paintings and also applied it to historical prints. The registration is performed
fully automatically using CraquelureNet. The user can choose the registration
settings and can interact with the visualizations of the registration results. In the
future, we could extend the application by including trained models of Craque-
lureNet on other datasets, such as the RetinaCraquelureNet [15] for multi-modal
retinal registration. A further possible extension would be to add a batch pro-
cessing functionality to the GUI to register a folder of image pairs.

Acknowledgements Thanks to Daniel Hess, Oliver Mack, Daniel G¨orres, Wib-
ke Ottweiler, Germanisches Nationalmuseum (GNM), and Gunnar Heydenreich,
Cranach Digital Archive (CDA), and Thomas Klinke, TH K¨oln, and Amalie
H¨ansch, FAU Erlangen-N¨urnberg for providing image data, and to Leibniz Soci-
ety for funding the research project “Critical Catalogue of Luther portraits (1519
- 1530)” with grant agreement No. SAW-2018-GNM-3-KKLB, to the European
Union’s Horizon 2020 research and innovation programme within the Odeuropa
project under grant agreement No. 101004469 for funding this publication, and
to NVIDIA for their GPU hardware donation.

Registration and Visualization Software Tool for Artworks

13

References

1. The Bosch Research and Conservation Project. http://boschproject.org, ac-

cessed: 2022-05-25

2. Bar´ath, D., Matas, J.: Graph-Cut RANSAC. 2018 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) pp. 6733–6741 (2018).
https://doi.org/10.1109/CVPR.2018.00704

3. Bar´ath, D., Noskova, J., Ivashechkin, M., Matas, J.: MAGSAC++, a Fast,
Reliable and Accurate Robust Estimator. 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) pp. 1301–1309 (2020).
https://doi.org/10.1109/CVPR42600.2020.00138

4. Cappellini, V., Del Mastio, A., De Rosa, A., Piva, A., Pelagotti, A., El Ya-
mani, H.: An automatic registration algorithm for cultural heritage images.
IEEE International Conference on Image Processing 2005 2, II–566 (2005).
https://doi.org/10.1109/ICIP.2005.1530118

5. Chum, O., Matas, J., Kittler, J.: Locally Optimized RANSAC. Pattern Recognition

pp. 236–243 (2003). https://doi.org/10.1007/978-3-540-45243-0 31

6. Conover, D.M., Delaney, J.K., Loew, M.H.: Automatic registration and mosaicking
of technical images of Old Master paintings. Applied Physics A 19, 1567–1575
(2015)

7. DeTone, D., Malisiewicz, T., Rabinovich, A.: SuperPoint: Self-Supervised In-
terest Point Detection and Description. 2018 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) Workshops pp. 224–236 (2018).
https://doi.org/10.1109/CVPRW.2018.00060

8. Dusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic, J., Torii, A., Sattler,
T.: D2-Net: A Trainable CNN for Joint Description and Detection of Local Fea-
tures. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) pp. 8084–8093 (2019). https://doi.org/10.1109/CVPR.2019.00828

9. Fischler, M.A., Bolles, R.C.: Random Sample Consensus: A Paradigm for Model
Fitting with Applications to Image Analysis and Automated Cartography. Com-
mun. ACM 24(6), 381–395 (1981). https://doi.org/10.1145/358669.358692

10. Fransen, B., Temmermans, F., Currie, C.: Imaging techniques and methodologies
for acquisition, processing and distribution of multimodal image data from the
oeuvre of Jan van Eyck. Optics, Photonics and Digital Technologies for Imaging
Applications VI 11353, 68 – 81 (2020). https://doi.org/10.1117/12.2556260

11. Lowe, D.G.: Distinctive

from Scale-Invariant Key-
Features
International Journal of Computer Vision 60(2), 91–110 (2004).

Image

points.
https://doi.org/10.1023/B:VISI.0000029664.99615.94

12. Murashov, D.: A Procedure For Automated Registration Of Fine Art Images In
Visible And X-Ray Spectral Bands. Proc. of the International Conference on Com-
puter Vision Theory and Applications (VISAPP-2011) pp. 162–167 (2011)

13. Myronenko, A., Song, X.: Point set registration: Coherent point drift. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence 32(12), 2262–2275 (2010).
https://doi.org/10.1109/TPAMI.2010.46

14. Raguram, R., Chum, O., Pollefeys, M., Matas, J., Frahm, J.M.: USAC:
A Universal Framework for Random Sample Consensus.
IEEE Transac-
tions on Pattern Analysis and Machine Intelligence 35(8), 2022–2038 (2013).
https://doi.org/10.1109/TPAMI.2012.257

15. Sindel, A., Hohberger, B., Dehcordi, S.F., Mardin, C., L¨ammer, R., Maier, A.,
Christlein, V.: A Keypoint Detection and Description Network Based on the Vessel

14

A. Sindel et al.

Structure for Multi-Modal Retinal Image Registration. Bildverarbeitung f¨ur die
Medizin 2022 p. 57–62 (2022). https://doi.org/10.1007/978-3-658-36932-3 12
16. Sindel, A., Maier, A., Christlein, V.: A Visualization Tool for Image Fusion of Art-
works. 25th International Conference on Cultural Heritage and New Technologies
(2020)

17. Sindel, A., Maier, A., Christlein, V.: CraquelureNet: Matching The Crack
Structure In Historical Paintings For Multi-Modal Image Registration. IEEE
International Conference on Image Processing 2021 pp. 994–998 (2021).
https://doi.org/10.1109/ICIP42928.2021.9506071

18. Stein, D., Fritzsche, K., Nolden, M., Meinzer, H., Wolf, I.: The extensible open-
source rigid and aﬃne image registration module of the Medical Imaging Interac-
tion Toolkit (MITK). Computer Methods and Programs in Biomedicine 100(1),
79–86 (2010). https://doi.org/10.1016/j.cmpb.2010.02.008

19. Zacharopoulos, A., Hatzigiannakis, K., Karamaoynas, P., Papadakis, V.M., Andri-
anakis, M., Melessanaki, K., Zabulis, X.: A method for the registration of spectral
images of paintings and its evaluation. Journal of Cultural Heritage 29, 10 – 18
(2018). https://doi.org/10.1016/j.culher.2017.07.004

