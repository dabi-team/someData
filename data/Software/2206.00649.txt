2
2
0
2

y
a
M
1
3

]

C
N
.
o
i
b
-
q
[

1
v
9
4
6
0
0
.
6
0
2
2
:
v
i
X
r
a

Differentiable programming for functional
connectomics

Rastko Ciric
Department of Bioengineering
Stanford University
Stanford, CA 94041
rastko@stanford.edu

Armin W. Thomas
Stanford Data Science
Stanford University
Stanford, CA 94041

Oscar Esteban
Department of Radiology
Université de Lausanne
Lausanne, Switzerland

Russell A. Poldrack
Department of Psychology
Stanford University
Stanford, CA 94041

Abstract

Mapping the functional connectome has the potential to uncover key insights into
brain organisation. However, existing workﬂows for functional connectomics
are limited in their adaptability to new data, and principled workﬂow design is
a challenging combinatorial problem. We introduce a new analytic paradigm
and software toolbox that implements common operations used in functional
connectomics as fully differentiable processing blocks. Under this paradigm,
workﬂow conﬁgurations exist as reparameterisations of a differentiable functional
that interpolates them. The differentiable program that we envision occupies
a niche midway between traditional pipelines and end-to-end neural networks,
combining the glass-box tractability and domain knowledge of the former with the
amenability to optimisation of the latter. In this preliminary work, we provide a
proof of concept for differentiable connectomics, demonstrating the capacity of our
processing blocks both to recapitulate canonical knowledge in neuroscience and to
make new discoveries in an unsupervised setting. Our differentiable modules are
competitive with state-of-the-art methods in problem domains including functional
parcellation, denoising, and covariance modelling. Taken together, our results and
software demonstrate the promise of differentiable programming for functional
connectomics.

1

Introduction

Many scientiﬁc disciplines depend on complex analytic workﬂows to extract salient information
from data. In the life sciences, and in large-scale brain mapping in particular, the development
and reﬁnement of these workﬂows has grown to become an informatic subdiscipline in its own
right. Although the introduction of better tools is a necessary vector of progress, informatic practice
also comes at the cost of increased analytic ﬂexibility, or “researcher degrees of freedom” [1], [2].
In other words, the development of new instruments presents researchers with the combinatorial
challenge of selecting, from among the growing set of available informatic tools, an analytic workﬂow
conditioned on their dataset and scientiﬁc question. This challenge is signiﬁcant for two reasons. First,
many reported results are sensitive to particular pipeline conﬁgurations and fail to replicate under
alternative conﬁgurations [1], [2]. Second, publication bias obscures the often iterative process of tool
development, and a skewed scientiﬁc incentive structure motivates informaticians to reﬁne algorithms
until they outperform state-of-the-art (SOTA) methods on benchmark datasets. Anecdotally, this

Preprint. Under review.

 
 
 
 
 
 
Figure 1: Problem setting and aspirational overview. Details in text.

is often accompanied by either improper or incomplete implementation of the SOTA baselines, or
insufﬁcient consideration of benchmark dataset properties that might lead to inﬂated performance.

The problem of designing a data transformation workﬂow in a principled way is thus of interest to
many scientiﬁc disciplines, particularly areas such as medical imaging and brain mapping where
ground truths are largely unknown or inaccessible. Differentiable programming promises one potential
resolution to this problem. A differentiable program can instantiate each block of a workﬂow as a
neural network module that subsumes (a subset of) existing analytic options under reparameterisation.
In other words, this differentiable program is a functional that interpolates over existing workﬂow
conﬁgurations, relaxing the combinatorial problem of principled workﬂow design into one that can
be optimised locally using gradient methods.

Functional connectomics is the enterprise of creating, reﬁning, and explaining whole-brain maps
of synchrony and statistical dependence in order to develop an understanding of how neuronal
populations communicate with one another [3]. The standard functional connectivity workﬂow
(Figure 1, Bottom) begins with a preprocessed blood oxygenation level-dependent (BOLD) time
series—a proxy for underlying neural activity [4]. A parcellation block ﬁrst reduces the dimension
of this input, and the estimated parcel-wise time series are then denoised. Next, some measure
of connectivity among the denoised time series is estimated; this connectivity matrix might be
projected from the positive semideﬁnite cone[5] before it is passed to a ﬁnal model that is ﬁt to some
research objective. Differentiable programming enables gradients to propagate back from the model
to reconﬁgure the parameters of the workﬂow itself.

Functional connectomics is an attractive test bed for prototyping differentiable programs because
nearly all atoms of the workﬂow described above are either immediately differentiable or have
differentiable relaxations. To demonstrate this, we implemented 18 different functional connectivity
workﬂow conﬁgurations using different combinations of parcellations, denoising schemes, and ﬁlters.
We then developed an open source PyTorch-based [6] software library, hypercoil1, to facilitate
differentiable programming for functional connectomics. We used this library to parameterise neural
network modules that replicated exactly each of the standard pipelines, as evidenced by perfect
correspondence between the connectomes they output (Figure 1, Top right). We also conducted
a series of experiments, detailed below, as a proof of concept for the differentiable programming
paradigm in functional connectomics. All experiments in the present manuscript were implemented
using functionality freely available in hypercoil.

1https://hypercoil.github.io/

2

Table 1: The four loss terms of the differentiable temporal-spatial clustering (dTSC) objective. Details
online at https://hypercoil.github.io/loss.html.

Spatial

Temporal

Within

A ◦ (cid:13)

Compactness
(cid:13)
(cid:13)C − AC
(cid:13)cols
A1

(cid:104)

Second Moment

A ◦ (cid:0)Ti − To

A1

(cid:1)2(cid:105) 1
A1

Dispersion
(cid:16) AiC

Ai1 − AjC

Aj1

− (cid:80)

i,j d

(cid:17)

Determinant
− log det corr(To)

Between

2 Related work

Parameterising the entire neuroimaging software stack as a neural network was ﬁrst proposed by
Vilamala et al. [7], who designed a module that could learn an optimal spatial smoothing kernel for
fMRI data. The current work was directly inspired by recent comprehensive combinatorial evaluation
of functional connectivity workﬂows in the prediction setting [8], [9]. Churchill et al. [10] introduced
an algorithm for adaptive neuroimaging workﬂow optimisation using a cross-validation framework.
Dafﬂon et al. [11] developed a learning method for efﬁciently estimating an approximate map of the
space of workﬂow conﬁgurations and of their performance on a prediction objective.

Differentiable programming is not the only promising approach for resolving the problem of princi-
pled workﬂow design. A complementary paradigm, multiverse analysis [2], is based on evidence
that, although results gleaned from individual workﬂow conﬁgurations are often brittle and workﬂow-
speciﬁc, meta-analytic aggregation across the “multiverse” of workﬂow conﬁgurations can separate
workﬂow-sensitive results from those that are robust and reproducible [2]. Differentiable program-
ming enables a complementary kind of aggregation: ensembling across a “multi-barreled” workﬂow
whose parallel program blocks can learn complementary information in service of a research objective
(Figure 1, Top).

3 Experiments

Functional parcellation We begin with a differentiable implementation of the ﬁrst workﬂow block,
brain parcellation. The delineation of functional subunits of the brain is among the longest standing
problems in neuroscience. Practically, brain parcellation is also a critical dimension-reducing step
that reduces the computational requirements of all downstream blocks. For BOLD fMRI data,
the objective of the parcellation problem is to learn a mapping from the vertex-wise time series
Ti ∈ Rv×t to the parcel-wise time series To ∈ Rp×t with minimal loss of information; in practice,
this mapping is usually operationalised as either a linear parcellation matrix A ∈ Rp×v or a linear
projection [12], [13]. Here we take the former approach due to its simplicity, although our method is
easily extended to the latter. Each entry Aij in the parcellation matrix encodes the model’s estimate of
the probability that vertex j is assigned to parcel i. We initialise each column of A by ﬁrst sampling
from a Dirichlet distribution and then log-transforming the Dirichlet samples. During each forward
pass, the parcellation logits are projected to the probability simplex using a softmax mapping.

To learn the parcellation matrix A, we combine four terms into a differentiable temporal-spatial
clustering (dTSC) objective (Figure 2, Top left; Table 1). A compactness loss penalises the distance
from each vertex in a parcel to the parcel’s centre of mass using vertex coordinates C, thereby
promoting spatially compact parcels. A complementary dispersion objective promotes spatial
separation of different parcels’ centres of mass. All distances are computed as spherical geodesics
after projecting data from each cortical hemisphere onto a spherical mesh [14], [15].

To minimise information loss and learn functionally uniform parcels, we wish to assign vertices with
similar signals to the same parcel and vertices with different signals to different parcels. Accordingly,
in addition to the two spatial terms, temporal loss terms promote parcel homogeneity. A second
moment objective quadratically penalises parcels for including vertices whose time series differ from
the parcel’s time series. Finally, we promote temporally independent parcels by placing a penalty on

3

(Top) The
Figure 2: dTSC / SWAPR parcellation.
model recovers reference functional boundaries [16] with success relative to a null model. (Middle)
Quantitative assessment of parcellation homogeneity (higher is better). (Bottom) Consistency and
difference across ﬁve parcellation resolutions in four representative regions.

(Top left) Schematic of the dTSC model.

the negative log-determinant of the correlation matrix among parcel time series. Our approach is
readily modiﬁed to produce null parcellations: ablating the temporal loss terms leaves a model that
learns entirely from spatial relationships, without using any information from the time series data.

We supplement the dTSC objective with three simple regularisations. First, to obtain parcels of ap-
proximately equal size, we impose an L2 parcel equilibrium penalty. Second, we enforce approximate
symmetry across cortical hemispheres by tethering each parcel’s centre to an analogue in the opposite
hemisphere using another L2 penalty. Third, to obtain deterministic parcels, we penalise the entropy
of each vertex’s parcel assignment distribution. Because a strong entropy penalty can lead the model
to ﬁxate irreversibly on its maximum assignment, we begin with a small multiplier for the entropy
term and progressively cascade it upward over the course of training. The cascading entropy loss
induces parcel probabilities to converge toward a deterministic assignment, at the cost of worsening
parcel homogeneity as reﬂected by a growing second-moment term (Figure 3a, Top), with boundary
regions slowest to converge (Figure 3a, Bottom, entropy multipliers denoted in black boxes).

Because the boundaries of brain subsystems vary substantially between individuals [17], [18], we
use weight averaging to smooth the objective for our parcellation. In particular, we modify the
stochastic weight averaging (SWA; [19]) algorithm for better compatibility with entropy penalties.
Our modiﬁcation, SWA with Parameter Revolution (SWAPR), yields parcels that qualitatively better
match the boundaries of reference group-averaged brain subsystems ([16]; Figure 2, Top).

4

(a) Parcel assignment uncertainty is modulated by a
progressive penalty on the distribution entropy. The
paired vertical lines denote training cycles during which
the entropy multiplier is cascaded upward.

(b) The learned parcellation proceeds from dis-
tributed functional modes to deterministic areal
parcels. Here, we show four exemplar parcels
from the right cortical hemisphere of a 666-region
parcellation at the end of each of four stages of
the entropy cascade.

Figure 3: Learning a parcellation using the dTSC / SWAPR model with a cascading entropy loss.

We used the dTSC objective with SWAPR to train 5 different brain parcellation models corresponding
to 5 spatial scales (300, 400, 666, 800, 1000 cortical parcels) using 2457 BOLD images from the
Human Connectome Project (HCP) dataset [20]. In comparison with a spatial null model, the
dTSC / SWAPR parcellations better reﬂected established contours of the brain’s canonical functional
subsystems (Figure 2, Top) across all spatial resolutions, with differences particularly evident at
coarser resolutions (e.g., 300 parcels). On average, the deterministic parcels learned by our model
also achieved better signal homogeneity than comparably sized parcels from the null model and
performed competitively with state-of-the-art methods (gwMRF: gradient-weighted Markov Random
Field [21]; MMP: multimodal parcellation [22]; Grad. BM: gradient boundary mapping [23]) in
held-out data (Figure 2, Middle).

Across spatial resolutions, we observed consistency and difference in parcel structure (Figure 2,
Bottom). Although some areal boundaries (notably of the occipital visual cortex and medial V1)
were reproduced with high ﬁdelity across resolutions, we qualitatively found greater consistency in
the orientation of local axes of signal similarity and difference, as reﬂected in the eccentricities and
orientations of parcel contours. This pattern suggests a limitation of areal parcellation schemes such
as the one we introduce; parcellations based on connectopic gradients (e.g., [24]) offer a potential
direction for future improvement.

Another interesting insight can be derived from the cascading entropy penalty used to train the
parcellation: at each stage of the cascade (labelled with the entropy multiplier in Figure 3b), the
spatial extent of parcels decreases, and their maximum assignment probability increases toward
unity. Qualitatively, this is reﬂected in a shift from spatially overlapping, probabilistic modes of brain
activity (e.g., [13]) toward deterministic areal assignments. In particular, the shown areal parcels 351
and 356 begin with near complete overlap and substantially differentiate only in the ﬁnal stages of
the cascade. By evaluating parcel differentiation over the course of the cascade, the progression of
training can potentially be used to investigate hierarchically nested modes of brain function [25]. As
complementary information might be offered by overlapping probabilistic modes and circumscribed
parcels, the stopping entropy also becomes a hyperparameter of the differentiable program, which
can be ensembled over.

Our approach differs from most previous work in that it is designed to ﬁt entirely in GPU memory and
(with some reﬁnement) integrate into a uniﬁed differentiable workﬂow. Principally, we accomplish
this (i) by using only global information about the absolute spatial position of each vertex, while
many competing methods maintain full adjacency graphs of neighbours, often performing expensive
label propagation updates at every step, and (ii) by using only losses that operate directly on the v × t
vertex-wise BOLD time series, bypassing the need to compute the complete v × v connectivity matrix,
which is computationally slow and prohibitively expensive to store all at once in GPU memory. Our

5

Figure 4: A shallow denoising model (RFNN) trained on the QC-FC loss outperforms SOTA methods
in held-out data.

(a) The RFNN model learned to remove a single
nuisance regressor from BOLD images so as to
reduce motion-related variance.

(b) QC-FC benchmark results (lower is better).

Model Abs. Med. Corr. N. Sig. Edges

Null
GSR
36P
RFNN

0.16640
0.09036
0.07487
0.07185

50765
14558
11099
6439

(c) Top a priori confounds that loaded onto the RFNN
model were global signal (GS)-related.

deterministic parcels are accordingly not as circumscribed as those found by methods that do incor-
porate local information (e.g., [21]–[23]), although this is easily remedied using (nondifferentiable)
automatic postprocessing options such as connected component identiﬁcation. However, we ﬁnd that
(even subject to these constraints) we can achieve parcel homogeneities that are competitive with
SOTA in held-out data (Figure 2, middle row).

Artefact removal Next, we implement a differentiable module for denoising BOLD time series.
The movement of subjects during a scan introduces artefactual ﬂuctuations into the BOLD signal [26],
which typically inﬂate estimates of functional connectivity. To make matters worse, motion artefact is
correlated with measures of scientiﬁc interest, such as subject age [27]. In the functional connectivity
workﬂow, denoising is typically implemented by residualising the BOLD time series with respect to
a confound model that is thought to approximately explain structured sources of artefact. We adopt
this framework here.

In particular, let ΣXX be the covariance matrix of BOLD time series, ΣY Y be the covariance matrix
of confound time series, and ΣXY be the matrix of covariances between BOLD and confound time
series. We operationalise the residual functional connectivity as the conditional covariance
ΣX|Y = ΣXX − ΣXY Σ−1

Y Y Σ

(cid:124)
XY

This is equivalent to the Schur complement of the confound covariance, and also to the covariance
of BOLD data that have been residualised with respect to a linear least-squares ﬁt of the confounds.
Ceteris paribus, a more parsimonious confound model—one with fewer time series—is thought
to be better because it removes fewer degrees of freedom from the BOLD data [28]. Thus, in this
experiment, our objective is to remove as much motion-related variance as possible from the BOLD
data using only a single learned confound time series.

We pursue this objective by parameterising a simple, shallow neural network model (RFNN) to
learn a single linear combination of 57 confound time series selected a priori for their demonstrated
efﬁcacy in denoising [29]. These include direct estimates of subject movement; mean signals from
high-noise tissue compartments (white matter—WM, cerebrospinal ﬂuid—CSF); the overall global
signal across the entire brain (GS); and localised signals obtained using singular value decompositions
of WM and CSF (CompCor; [30]). We parameterise the RFNN to also learn 5 response functions;
the RFNN is permitted to select any linear combination of the 57 a priori confound time series and
their 285 = 5 × 57 response function convolutions.

To train the RFNN to optimally remove motion artefact, we introduce the QC-FC loss function, a
differentiable implementation of the standard QC-FC benchmark for residual motion artefact [31].

6

Figure 5: Results of covariance experiments. Left, Methods for augmenting covariance data. Centre,
Clustering on the time-by-time covariance matrix partitions the brain into unimodal and higher-order
subnetworks with distinct temporal reconﬁguration proﬁles. Right, Clustering on the parcel-by parcel
covariance matrix partitions each time series into connectivity states.

The QC-FC loss computes the correlation between a gross estimate of subject motion (QC) and each
edge of the functional connectivity matrix (FC) across the batch dimension. Thus, QC-FC loss goes
to zero when subject movement and functional connectivity are uncorrelated.

In a held-out sample of 351 BOLD images from the HCP dataset (Figure 4), the RFNN model
outperformed both the top-performing a priori one-confound model (GSR, global signal regression)
and a 36-confound model (36P, a superset of GSR) that has previously been demonstrated to give
SOTA performance [29]. Both the number of connections signiﬁcantly related to motion (Sig. Edges;
p < 0.01, uncorrected) and the median of the size of motion effects (QC-FC Distr.) were reduced
relative to all evaluated a priori models. We caution that this superior performance reﬂects a limitation
of the benchmarks: the functional connectome denoised using the RFNN did not feature the zero-
centred correlations that are a hallmark of successful GSR-based denoising [32]. It is possible that
this reﬂects the speciﬁcity of the learned model for removing motion-related variance—by contrast,
GSR-based models also effectively remove physiological artefacts related to respiratory and cardiac
processes, which typically inﬂate global estimates of connectivity [33]. With this and additional
beneﬁts of GSR in consideration [34], we strongly recommend continued use of GSR in functional
connectivity workﬂows; future work will address possible integration of RFNN-like models with
GSR. Notably, when we computed the extent to which a priori confounds captured variance in the
RFNN confound, the top three loaded a priori confounds were all GSR-related: the global signal
(GS), the square of the global signal (GS2), and the temporal derivative of the square ( ∂GS2
) (Figure
∂t
4 Top, distribution across images).

(cid:0)T − ¯T(cid:1) Θ (cid:0)T − ¯T(cid:1)(cid:124)

Covariance clustering We ﬁnally turn our attention to covariance estimation. The functional
connectome is typically estimated using a derivative of covariance, such as Pearson correlation,
among all pairs of parcel time series. The usual deﬁnition of empirical covariance among a set of time
series T can be generalised via parameterisation by the (typically diagonal) weight matrix Θ: CΘ =
1
. This parameterised form ﬁnds an application in data augmentation:
n−1
setting the weights θii along the diagonal to random nonnegative integers whose sum is the total
number of observations is equivalent to a resample. Relaxing this constraint to require only a
nonnegative support and a mean of 1, satisﬁed inter alia by noise sampled from an appropriately
chosen gamma or truncated normal distribution, leads to a simple method for augmenting covariance
datasets that complements random windowing of the input time series (Figure 5, Left).

In this proof of concept, however, our focus is on the problem setting of clustering. Speciﬁcally, how
can we cluster observations such that the covariance matrices estimated from different clusters are,
for some measure of separation, maximally different? This nonconvex maximisation problem has
two immediate applications in functional connectomics: subnetwork detection and state detection. To
further elaborate, given the time series matrix T ∈ Rp×t, we can obtain two covariance matrices, the
p × p connectome matrix of inter-parcel correlations and the t × t matrix of correlations among brain
activity proﬁles across time (e.g., [35]). For the p × p connectome, observations correspond to time
points, and clustering them to produce maximally distinct connectomes can be interpreted as brain
state detection. For the t × t matrix, observations correspond to different brain parcels; clustering
them is a form of subnetwork detection.

7

For each problem setting, we train a clustering model to learn Θ ∈ Rc×p or Rc×t, where c is the
number of clusters, here selected a priori. We instantiate a clustering loss to maximise the total L2
dispersion among the covariance matrices corresponding to different detected clusters. As in the
parcellation problem, we penalise the entropy to promote deterministic assignment of each parcel or
time point to a single subnetwork or state. For the state detection problem, we also encourage the
model to learn persistent states by imposing an L2 smoothness penalty on the backward difference of
Θ. Because state detection (and numerous other neuroimaging applications, such as subject-speciﬁc
parcellation) requires learning a unique time course for each data instance, the hypercoil package
also includes extensions of PyTorch optimisers that accept ephemeral, instance-speciﬁc parameter
groups for optimisation and optionally clear them from memory after they have been updated.

The results of the clustering experiment in subsamples of the HCP dataset are shown in Figure 5.
Clustering t × t covariances into c = 2 subnetworks divides the brain along a unimodal (blue) to
higher-order (red) axis (Figure 5, Centre); this clustering is replicable across data splits. We show the
t × t covariance matrices for 2 example subjects, illustrating the distinct dynamic proﬁles of the two
subnetworks. Clustering p × p covariances into c = 3 states yields three whole-brain connectivity
states that are each distinct both from one another and from the time-averaged connectome (Figure 5,
Right). The learned assignment of time frames to the three states is plotted for an example subject.
In contrast with the most commonly used state detection methods (e.g. [36]), the method we apply
does not require estimating the covariance over a sliding window; it instead directly learns to assign
time frames to states. Compared with a classical approach that uses k-means clustering [36], our
differentiable method learns superior separation of each pair of detected brain states in our test case.

Community detection As a second proof of concept for connectome estimation, we combine the
parameterised covariance function described in the previous section with community detection. The
objective of community detection is to learn a partition of graph vertices (here, brain parcels) into
communities whose members preferentially connect to one another (e.g., [37]). In connectomics, this
corresponds to identiﬁcation of modular subsystems of brain function. By combining community
detection with the parameterised covariance described previously, we can learn dynamic time courses
that track the modularisation and demodularisation of these functional subsystems.

Early algorithms for community detection were neither convex nor differentiable, although more
recently neural networks have been developed to learn community partitions [38]. Here, we opt for a
more direct approach, because functional connectome matrices typically have fewer vertices than the
large-scale graphs for which deep methods were developed. Our approach also differs from most
algorithms for dynamic community detection (e.g., [39], [40]) in that existing algorithms typically
aim to characterise the temporal evolution of community boundaries, whereas the objective of our
approach is to delineate epochs during which communities coalesce and dissipate.

We train our model using a differentiable relaxation of the Girvan-Newman modularity objective
[41], which indexes the ability of a proposed community structure to explain the arrangement of
edges in a real graph relative to a null model. Formally, let A ∈ Rv×v be the graph adjacency matrix
and C ∈ Rv×c be a proposed assignment of v vertices to c communities. The modularity matrix
B = A − γP expresses the extent to which edges in the observed graph deviate from expectation
under the null model P, subject to a resolution hyperparameter γ. Here, we use γ = 5 and the
original Girvan-Newman null model, PGN = A11(cid:124)A
1(cid:124)A1 , which can be interpreted as the expected
weight of connections between each pair of vertices if all existing edges are cut and then randomly
rewired. The modularity objective is then LQ = 1(cid:124)(H ◦ B)1, where ◦ denotes the Hadamard product
and H = CC(cid:124)
is the community coafﬁliation matrix. Our goal is to learn a C that maximises
the modularity objective. If we constrain all entries in C to {0, 1}, this relaxation converges to the
original deﬁnition of modularity [41]. To maintain differentiability, we instead learn the logits of C,
which we pass through a softmax, permitting community assignments to vary continuously in the
probability simplex.

The modularity objective is applied to the time-averaged connectome and combined with time-
selective modularity objectives applied separately for each community. Speciﬁcally, for each commu-
nity Ci, we let ACi be the covariance parameterised by a learnable time course ΘCi of the community’s
(cid:124)
modularisation, and HCi = CiC
i . As before, we regularise ΘCi using a smoothness/persistence
loss.

8

Figure 6: A community detection model learns to track the modularisation and demodularisation of
brain subsystems.

Training this unsupervised model on 30 parcellated time series from the Midnight Scan Club (MSC)
dataset [18], we identify a set of dynamic communities that accord well with previously characterised
brain subnetworks. At the top of Figure 6, we show the learned spatial distributions Ci of each
community and an example time course ΘCi of the community’s modularisation (grey: present,
black: absent) in a single BOLD image. Relative to a null model obtained by randomly shifting
modularisation time courses, the learned model consistently obtained better modularity (Figure 6,
Bottom left). We also ﬁnd that communities tend to coalesce and dissipate in tandem (correlation
matrix and time course at the right of Figure 6), consistent with previous work that characterised
transient “excursions” of connectivity using a sliding-window approach [42].

4 Discussion

Our experimental results demonstrate the promises of a fully differentiable software stack in the
domain of functional connectomics. All experiments were implemented using the PyTorch-based
hypercoil software library that we introduce and freely release. This manuscript additionally
functions as an open invitation to members of the community with an interest in applying differentiable
programming to brain mapping to contribute to further research and development of hypercoil.

We must remark that a substantial prior body of work has established limited utility for deep neural
networks in the setting of brain mapping (e.g., [8], [43]–[45], but see also [46]). Importantly, this
work has largely focused on one mode of scientiﬁc understanding—prediction. We believe, based
on the proof of concept presented here, that the greatest promise of the system we introduce lies not
in prediction but in developing other modes of understanding about neuroscience and the design of
scientiﬁc workﬂows.

There additionally exists a central philosophical objection against the differentiable programming
paradigm. The argument goes as follows: differentiable programming does not in fact resolve the
problem of workﬂow design; instead, it is merely a re-delegation of the workﬂow design burden from
selection of analytic options to speciﬁcation of workﬂow hyperparameters. Different hyperparameter
conﬁgurations (e.g., the balances of loss and regularisation multipliers) are inherently going to
produce different “optimal” workﬂows. Although it can be argued that the hyperparameter selection
process sometimes offers researchers more transparent control over workﬂow design, because a
numerical objective is more intuitively related to a research objective, this is often not the case.
Although responses to this argument (e.g., mapping and understanding the most relevant subsets of
the hyperparameter space; strides in automating the process of hyperparameter optimisation, e.g.
[47]) are under active development in the machine learning world, this does not fully address doubts.
Nevertheless, even if a differentiable program does not completely address the problem of principled

9

workﬂow optimisation, it has the potential to provide both an engine for new discoveries and an
informative complement to multiverse analysis.

References

[1]

J. Carp, “On the Plurality of (Methodological) Worlds: Estimating the Analytic Flexibility of
fMRI Experiments,” Frontiers in Neuroscience, vol. 6, 2012. DOI: 10.3389/fnins.2012.
00149.

[2] R. Botvinik-Nezer, F. Holzmeister, C. F. Camerer, et al., “Variability in the analysis of a single
neuroimaging dataset by many teams,” Nature, vol. 582, no. 7810, pp. 84–88, Jun. 2020.
[3] S. M. Smith, D. Vidaurre, C. F. Beckmann, et al., “Functional connectomics from resting-
state fMRI,” Trends in Cognitive Sciences, vol. 17, no. 12, pp. 666–682, Dec. 2013. DOI:
10.1016/j.tics.2013.09.016.

[4] N. K. Logothetis, J. Pauls, M. Augath, T. Trinath, and A. Oeltermann, “Neurophysiological

investigation of the basis of the fMRI signal,” Nature, vol. 412, p. 8, 2001.

[5] B. Ng, M. Dressler, G. Varoquaux, J. B. Poline, M. Greicius, and B. Thirion, “Transport on
Riemannian Manifold for Functional Connectivity-Based Classiﬁcation,” in Medical Image
Computing and Computer-Assisted Intervention – MICCAI 2014, P. Golland, N. Hata, C.
Barillot, J. Hornegger, and R. Howe, Eds., vol. 8674, Series Title: Lecture Notes in Computer
Science, Cham: Springer International Publishing, 2014, pp. 405–412. DOI: 10.1007/978-3-
319-10470-6_51.

[6] A. Paszke, S. Gross, F. Massa, et al., “Pytorch: An imperative style, high-performance deep
learning library,” in Advances in Neural Information Processing Systems 32, H. Wallach, H.
Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett, Eds., Curran Associates,
Inc., 2019, pp. 8024–8035.

[7] A. Vilamala, K. H. Madsen, and L. K. Hansen, Towards end-to-end optimisation of functional
image analysis pipelines, Number: arXiv:1610.04079 arXiv:1610.04079 [cs, q-bio, stat], Oct.
2016.

[8] U. Pervaiz, D. Vidaurre, M. W. Woolrich, and S. M. Smith, “Optimising network mod-
elling methods for fMRI,” NeuroImage, vol. 211, p. 116 604, May 2020. DOI: 10.1016/j.
neuroimage.2020.116604.

[9] K. Dadi, M. Rahim, A. Abraham, et al., “Benchmarking functional connectome-based pre-
dictive models for resting-state fMRI,” NeuroImage, vol. 192, pp. 115–134, May 2019. DOI:
10.1016/j.neuroimage.2019.02.062.

[10] N. W. Churchill, P. Raamana, R. Spring, and S. C. Strother, “Optimizing fMRI preprocessing
pipelines for block-design tasks as a function of age,” NeuroImage, vol. 154, pp. 240–254, Jul.
2017. DOI: 10.1016/j.neuroimage.2017.02.028.
J. Dafﬂon, P. F. Da Costa, F. Váša, et al., “Neuroimaging: Into the Multiverse,” Neuroscience,
preprint, Oct. 2020. DOI: 10.1101/2020.10.29.359778.

[11]

[12] C. Beckmann, C. Mackay, N. Filippini, and S. Smith, “Group comparison of resting-state
FMRI data using multi-subject ICA and dual regression,” NeuroImage, vol. 47, S148, Jul.
2009. DOI: 10.1016/S1053-8119(09)71511-3.

[13] K. Dadi, G. Varoquaux, A. Machlouzarides-Shalit, et al., “Fine-grain atlases of functional
modes for fMRI analysis,” NeuroImage, vol. 221, p. 117 126, Nov. 2020. DOI: 10.1016/j.
neuroimage.2020.117126.

[14] B. Fischl, M. I. Sereno, and A. M. Dale, “Cortical Surface-Based Analysis II: Inﬂation,
Flattening, and a Surface-Based Coordinate System,” NeuroImage, vol. 9, no. 2, pp. 195–207,
Feb. 1999. DOI: 10.1006/nimg.1998.0396.

[15] E. C. Robinson, S. Jbabdi, M. F. Glasser, et al., “MSM: A new ﬂexible framework for
Multimodal Surface Matching,” NeuroImage, vol. 100, pp. 414–426, Oct. 2014. DOI: 10.
1016/j.neuroimage.2014.05.069.

[16] B. T. T. Yeo, F. M. Krienen, J. Sepulcre, et al., “The organization of the human cerebral cortex
estimated by intrinsic functional connectivity,” Journal of Neurophysiology, vol. 106, no. 3,
pp. 1125–1165, Sep. 2011. DOI: 10.1152/jn.00338.2011.

[17] T. O. Laumann, E. M. Gordon, B. Adeyemo, et al., “Functional System and Areal Organization
of a Highly Sampled Individual Human Brain,” Neuron, vol. 87, no. 3, pp. 657–670, Aug.
2015. DOI: 10.1016/j.neuron.2015.06.037.

10

[18] E. M. Gordon, T. O. Laumann, A. W. Gilmore, et al., “Precision Functional Mapping of
Individual Human Brains,” Neuron, vol. 95, no. 4, 791–807.e7, Aug. 2017. DOI: 10.1016/j.
neuron.2017.07.011.

[19] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson, Averaging Weights Leads
to Wider Optima and Better Generalization, Number: arXiv:1803.05407 arXiv:1803.05407
[cs, stat], Feb. 2019.

[20] D. C. Van Essen, S. M. Smith, D. M. Barch, T. E. Behrens, E. Yacoub, and K. Ugurbil, “The
WU-Minn Human Connectome Project: An overview,” NeuroImage, vol. 80, pp. 62–79, Oct.
2013. DOI: 10.1016/j.neuroimage.2013.05.041.

[21] A. Schaefer, R. Kong, E. M. Gordon, et al., “Local-Global Parcellation of the Human Cerebral
Cortex from Intrinsic Functional Connectivity MRI,” Cerebral Cortex, vol. 28, no. 9, pp. 3095–
3114, Sep. 2018. DOI: 10.1093/cercor/bhx179.

[22] M. F. Glasser, T. S. Coalson, E. C. Robinson, et al., “A multi-modal parcellation of human
cerebral cortex,” Nature, vol. 536, no. 7615, pp. 171–178, Aug. 2016. DOI: 10 . 1038 /
nature18933.

[23] E. M. Gordon, T. O. Laumann, B. Adeyemo, J. F. Huckins, W. M. Kelley, and S. E. Petersen,
“Generation and Evaluation of a Cortical Area Parcellation from Resting-State Correlations,”
Cerebral Cortex, vol. 26, no. 1, pp. 288–303, Jan. 2016. DOI: 10.1093/cercor/bhu239.

[24] Y. Tian, D. S. Margulies, M. Breakspear, and A. Zalesky, “Topographic organization of
the human subcortex unveiled with functional connectivity gradients,” Nature Neuroscience,
vol. 23, no. 11, pp. 1421–1432, Nov. 2020. DOI: 10.1038/s41593-020-00711-6.
[25] A. R. Pines, B. Larsen, Z. Cui, et al., “Dissociable multi-scale patterns of development in
personalized brain networks,” Nature Communications, vol. 13, no. 1, p. 2647, Dec. 2022. DOI:
10.1038/s41467-022-30244-4.
J. D. Power, K. A. Barnes, A. Z. Snyder, B. L. Schlaggar, and S. E. Petersen, “Spurious but
systematic correlations in functional connectivity MRI networks arise from subject motion,”
NeuroImage, vol. 59, no. 3, pp. 2142–2154, Feb. 2012. DOI: 10.1016/j.neuroimage.2011.
10.018.

[26]

[27] T. D. Satterthwaite, D. H. Wolf, J. Loughead, et al., “Impact of in-scanner head motion on
multiple measures of functional connectivity: Relevance for studies of neurodevelopment in
youth,” NeuroImage, vol. 60, no. 1, pp. 623–632, Mar. 2012. DOI: 10.1016/j.neuroimage.
2011.12.063.

[28] R. H. Pruim, M. Mennes, J. K. Buitelaar, and C. F. Beckmann, “Evaluation of ICA-AROMA
and alternative strategies for motion artifact removal in resting state fMRI,” NeuroImage,
vol. 112, pp. 278–287, May 2015. DOI: 10.1016/j.neuroimage.2015.02.063.

[29] R. Ciric, D. H. Wolf, J. D. Power, et al., “Benchmarking of participant-level confound regres-
sion strategies for the control of motion artifact in studies of functional connectivity,” Neu-
roImage, vol. 154, pp. 174–187, Jul. 2017. DOI: 10.1016/j.neuroimage.2017.03.020.

[30] Y. Behzadi, K. Restom, J. Liau, and T. T. Liu, “A component based noise correction method
(CompCor) for BOLD and perfusion based fMRI,” NeuroImage, vol. 37, no. 1, pp. 90–101,
Aug. 2007. DOI: 10.1016/j.neuroimage.2007.04.042.
J. D. Power, A. Mitra, T. O. Laumann, A. Z. Snyder, B. L. Schlaggar, and S. E. Petersen,
“Methods to detect, characterize, and remove motion artifact in resting state fMRI,” NeuroImage,
vol. 84, pp. 320–341, Jan. 2014. DOI: 10.1016/j.neuroimage.2013.08.048.

[31]

[33]

[32] K. Murphy, R. M. Birn, D. A. Handwerker, T. B. Jones, and P. A. Bandettini, “The impact of
global signal regression on resting state correlations: Are anti-correlated networks introduced?”
NeuroImage, vol. 44, no. 3, pp. 893–905, Feb. 2009. DOI: 10.1016/j.neuroimage.2008.
09.036.
J. D. Power, M. Plitt, T. O. Laumann, and A. Martin, “Sources and implications of whole-brain
fMRI signals in humans,” NeuroImage, vol. 146, pp. 609–625, Feb. 2017. DOI: 10.1016/j.
neuroimage.2016.09.038.
J. Li, R. Kong, R. Liégeois, et al., “Global signal regression strengthens association between
resting-state functional connectivity and behavior,” NeuroImage, vol. 196, pp. 126–141, Aug.
2019. DOI: 10.1016/j.neuroimage.2019.04.016.
J. D. Medaglia, T. D. Satterthwaite, A. Kelkar, et al., “Brain state expression and transitions
are related to complex executive cognition in normative neurodevelopment,” NeuroImage,
vol. 166, pp. 293–306, Feb. 2018. DOI: 10.1016/j.neuroimage.2017.10.048.

[34]

[35]

11

[36] E. A. Allen, E. Damaraju, S. M. Plis, E. B. Erhardt, T. Eichele, and V. D. Calhoun, “Tracking
Whole-Brain Connectivity Dynamics in the Resting State,” Cerebral Cortex, vol. 24, no. 3,
pp. 663–676, Mar. 2014. DOI: 10.1093/cercor/bhs352.

[37] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, “Fast unfolding of communities
in large networks,” Journal of Statistical Mechanics: Theory and Experiment, vol. 2008, no. 10,
P10008, Oct. 2008. DOI: 10.1088/1742-5468/2008/10/P10008.

[38] X. Su, S. Xue, F. Liu, et al., “A Comprehensive Survey on Community Detection with Deep
Learning,” IEEE Transactions on Neural Networks and Learning Systems, pp. 1–21, 2022,
arXiv:2105.12584 [cs]. DOI: 10.1109/TNNLS.2021.3137396.

[39] W. H. Thompson, J. Wright, J. M. Shine, and R. A. Poldrack, “The identiﬁcation of temporal
communities through trajectory clustering correlates with single-trial behavioural ﬂuctuations
in neuroimaging data,” Neuroscience, preprint, Apr. 2019. DOI: 10.1101/617027.

[40] L.-E. Martinet, M. A. Kramer, W. Viles, et al., “Robust dynamic community detection with
applications to human brain functional networks,” Nature Communications, vol. 11, no. 1,
p. 2785, Dec. 2020. DOI: 10.1038/s41467-020-16285-7.

[41] M. E. J. Newman and M. Girvan, “Finding and evaluating community structure in networks,”
Physical Review E, vol. 69, no. 2, p. 026 113, Feb. 2004, arXiv:cond-mat/0308217. DOI:
10.1103/PhysRevE.69.026113.

[42] R. F. Betzel, M. Fukushima, Y. He, X.-N. Zuo, and O. Sporns, “Dynamic ﬂuctuations coincide
with periods of high and low modularity in resting-state functional brain networks,” NeuroIm-
age, vol. 127, pp. 287–297, Feb. 2016. DOI: 10.1016/j.neuroimage.2015.12.001.
[43] T. He, R. Kong, A. J. Holmes, et al., “Deep neural networks and kernel regression achieve
comparable accuracies for functional connectivity prediction of behavior and demographics,”
NeuroImage, vol. 206, p. 116 276, Feb. 2020. DOI: 10.1016/j.neuroimage.2019.116276.
[44] M.-A. Schulz, B. T. T. Yeo, J. T. Vogelstein, et al., “Different scaling of linear models
and deep learning in UKBiobank brain images versus machine-learning datasets,” Nature
Communications, vol. 11, no. 1, p. 4238, Dec. 2020. DOI: 10.1038/s41467-020-18037-z.
[45] R. M. Thomas, S. Gallo, L. Cerliani, P. Zhutovsky, A. El-Gazzar, and G. van Wingen, “Classi-
fying Autism Spectrum Disorder Using the Temporal Statistics of Resting-State Functional
MRI Data With 3D Convolutional Neural Networks,” Frontiers in Psychiatry, vol. 11, p. 440,
May 2020. DOI: 10.3389/fpsyt.2020.00440.

[46] A. Abrol, Z. Fu, M. Salman, et al., “Deep learning encodes robust discriminative neuroimaging
representations to outperform standard machine learning,” Nature Communications, vol. 12,
no. 1, p. 353, Dec. 2021. DOI: 10.1038/s41467-020-20655-6.

[47] X. He, K. Zhao, and X. Chu, “AutoML: A survey of the state-of-the-art,” Knowledge-Based
Systems, vol. 212, p. 106 622, Jan. 2021. DOI: 10.1016/j.knosys.2020.106622.

12

Differentiable programming for functional
connectomics
Supplemental matter

A Parcellation

Dataset, preprocessing, and computational resources The openly available Human Connectome
Project (HCP) dataset comprises scans from 1200 participants performing a number of in-scanner
tasks. For each participant, the study acquisition protocols included 4 separate resting-state (task-free)
scans, each acquired over the course of approximately 15 minutes of scanning using an accelerated
multiband sequence with a sampling rate of 0.72 seconds [1], [2]. In reality, the protocols were
incomplete for many participants.

To obtain our training and evaluation datasets, we ﬁrst performed a random 12-way split on 1098 HCP
subjects with resting-state fMRI data. We then selected the ﬁrst 7 of these splits, comprising 2457
resting-state BOLD time series, to train our model. To create an evaluation set, we selected another
split, from which a single 69-time series shard was randomly selected for homogeneity evaluation.

Data were preprocessed according to the minimal preprocessing pipeline of the Human Connectome
Project [3], and were acquired following a standardised protocol in accordance with the host institu-
tion’s IRB [1]. To better respect the topology of the cortex and to reduce the effects of inter-subject
variability, we used time series projected onto a spherical surface [4] and aligned using the MSMAll
algorithm [5]. Before each time series was passed to the model, we performed several additional
preprocessing steps. First, we projected each time series to the orthogonal complement of a subspace
deﬁned as the span of (i) a quadratic polynomial (to mitigate scanner drift artefact) and (ii) the average
global signal across all voxels (following previous parcellation work, and in accordance with the
overall demonstrated beneﬁts of this approach; [6], [7]). Next, we normalised each time series to
a mean of 0 and a variance of 1. Finally, to reduce the memory footprint, we selected a random
500-sample window from each time series as input to the model.

The model was trained on a computer running Ubuntu Linux, using only a single commercial-grade
(RTX 2080 Ti) GPU with 11 GB of RAM. Memory usage was reduced through serialisation of
expensive loss computations (details in Loss section below). Each of the 5 parcellations required
approximately 18-24 hours of GPU time to train. Data were stored as tar shards for compatibility
with the webdataset format [8]. Due to the large size of each BOLD time series, the most signiﬁcant
bottleneck during training was reading from the disk. Unfortunately, we found that data loaders were
not well equipped to expedite this process for the large samples used, although it is likely that this
was due to insufﬁcient optimisation in sharding and worker deployment. We used a ﬁxed batch size
of 3 time series for training. To reduce disk I/O operations, we trained the model for 5 steps with
each batch before sampling the next batch. The model was trained for a total 6000 steps using the
Adam optimiser [9].

Loss function As detailed in the main text, the loss function we use comprises four clustering
terms (spatial compactness of parcels, spatial dispersion among parcels, second moment of parcel
time series, and negative log determinant of the correlation among parcel time series), as well as
three regularisation terms (parcel equilibrium, vertex-wise distribution entropy, and inter-hemispheric
spatial tether). We discuss here several details of the loss function implementation.

The broadcasting operations involved in the second moment term require computation of a large
tensor for each hemisphere’s BOLD time series, of dimension v × p × t, where v is the number of
vertices (order of 30000 per hemisphere), p is the number of parcels (150-500 per hemisphere), and
t is the number of time frames (500 after random windowing). A tensor of this dimension was not
able to ﬁt into our GPU memory, so we performed a data ﬂow serialisation to reduce the memory
footprint of the second moment loss function. Speciﬁcally, we sliced the computation along the time
axis, processing 3-10 time points per slice, looping over slices, and renormalising the slice-wise loss

13

to adjust for slice size. The computational graph was immediately destroyed for each slice after
computing the gradient. This enabled us to perform the necessary computation, and we veriﬁed
equality of the accumulated gradients. In principle, this slicing approach should also enable us to work
with the full v × v connectivity matrix (order of 60000 × 60000 for cortical-only or 90000 × 90000
for all coordinate time series) in loss computations used in future iterations of this method.

Through experimentation, we also made one further change to the second moment loss term: we
removed the normalisation 1
A1 , the use of which we found could yield extremely several large
parcels that did not correspond with any well-known functional systems of the brain. With this
change, the second moment term we use essentially reduces to a weighted mean squared error term,
where each “target” is a parcel time series, each “prediction” is a vertex time series, and the weight
assigned to each target-prediction pair is the assignment probability of the corresponding vertex to
the corresponding parcel.

The negative log determinant term has the potential to go to inﬁnity, for instance if parcel-wise time
series are not independent. While the purpose of this term is to promote parcel independence, an
exploding loss can irreversibly cause parameter values to go out of bounds when it is propagated
back. To minimise the risk of this occurrence while preserving the efﬁcacy of the determinant term,
we added a small reconditioning term to the correlation matrix. For each entry along the diagonal
of the correlation matrix, we randomly sampled i.i.d. noise from Uniform(0, 0.001) and added this
noise to the diagonal. Although this reconditioning added further stochasticity to the determinant
term, we observed an overall downward trend of the determinant loss during training.

Details
are
function
https://hypercoil.github.io/loss.html.

deﬁnitions

loss

of

available

on

the

documentation

hub

at

SWAPR algorithm SWAPR is a simple modiﬁcation of Stochastic Weight Averaging (SWA; [10])
that adds revolution of parameters between the averaged model and the data-facing model. The
rationale for this modiﬁcation follows: when entropy is sufﬁciently penalised, as it is in the ﬁnal stages
of the entropy cascade that we use, a data-facing model will eventually converge to a deterministic
solution that receives weak gradients and negligible parameter updates. Given sufﬁcient training steps,
any model that averages over this data-facing model will converge to the same deterministic solution,
thereby obviating it altogether. To allow for some useful weight averaging in this high-negentropy
setting (while also allowing for an averaged model that is eventually approximately deterministic),
we revolve parameters from the averaged model into the data-facing model at the end of each cascade
stage. Practically, the average parameters from the previous stage of the cascade become the new
data-facing parameters at the current stage, and a new averaged model is initialised (although it is
also possible to decrease the weight of the running average instead of initialising a completely new
averaged model). We use a large, constant learning rate of 0.05 during SWA steps.

Multiplier schedules and hyperparameters To train the parcellation model, we scheduled
changes in the relative balance of loss multiplier hyperparameters over the course of training. We
highlight the most critical aspect of this schedule, the entropy cascade, in the main text: we peri-
odically increase the penalty on each vertex’s parcel assignment distribution entropy to transition
the parcellation solution from distributed and overlapping functional modes to compact and circum-
scribed deterministic parcels. Here, we discuss several additional scheduling decisions; the exact
values of all multiplier hyperparameters and schedules are included in the associated code.

First, the loss terms that favour spatial and temporal separation of parcels—the dispersion and
determinant terms—are initialised with large multipliers of 10 and .005 respectively in order to
achieve rapid differentiation of parcels at the beginning of training. We anneal these multipliers to 0.5
and 0.0001 respectively before the 400th training step. Second, approximately following a practice
established in previous parcellation efforts (e.g., [6]), we progressively increase the compactness
multiplier to collapse the spatial extent of parcels; we ﬁnd that this increase works synergistically with
the entropy cascade. Third, to reduce the chances of immediate ﬁxation and convergence at the start
of each entropy cascade step, we temporarily pump the multipliers for two terms that tend to compete
with the entropy, the parcel equilibrium and second moment. These terms decay exponentially back
to a baseline over the course of the cascade stage. Future work will more comprehensively evaluate
the roles of these schedules in order to streamline the parcellation training regime.

14

Figure S1: Parcellation evaluation. Left, distributions of parcel sizes for different evaluated parcella-
tions. Background colour indicates maximum scoring parcellation for that size according to log ﬁt.
Right, Absolute homogeneity evaluation without adjustment for null.

Evaluation The quality of the learned parcellation was evaluated using a measure of parcel homo-
geneity, operationalised as follows. For fairer comparison with reference parcellations, which are
deﬁned deterministically, we ﬁrst took the arg max over vertex probability assignments to create a
deterministic parcellation at each of the 5 evaluated parcel scales. (In practice, the ﬁnal stages of
the entropy cascade already yielded maximum assignment probabilities close to unity for nearly all
vertices.) For each parcel at each scale, we then computed the pairwise Pearson correlation among all
of that parcel’s assigned vertex-wise time series for each BOLD image in the held-out test set. For
each BOLD image, the parcel homogeneity was operationalised as the mean of these correlations. We
then deﬁned the overall homogeneity of that parcel as its mean homogeneity across all images in the
test set. We note that the homogeneity scores we obtained here were low in relation to many previous
reports; this might be attributable in part to the lack of spatial smoothing in our data. Additionally,
of the reference parcellations, only the MMP parcellation [11] was deﬁned using the HCP dataset;
because of differences between datasets, acquisition protocols, and coordinate spaces in which the
different parcellations were deﬁned, comparison results should be interpreted with caution. As an
additional note of caution, it is possible that the “unseen” data used for evaluation were not held out
for the MMP parcellation. Furthermore, it should be noted that the reference gwMRF parcellation
that we evaluated, like the dTSC/SWAPR parcellation, is in fact a set of parcellations deﬁned across
different spatial scales (all multiples of 100 parcels between 100 and 1000, inclusive).

Parcel homogeneity is related to parcel size—it is more likely for a smaller parcel to be more
homogeneous. Accordingly, after computing homogeneity scores for each parcel, we also computed
the best logarithmic least-squares ﬁt of parcel size to parcel homogeneity. This ﬁt was computed for
all evaluated parcellations, including the null model. To generate the Relative Homogeneity plots
in the main text, we subtracted the null model’s best ﬁt from all other ﬁts and homogeneity scores.
As a reference, we also plot the analogous ﬁgures without this adjustment here (Figure S1, Right).
Parcels with a size of under 20 vertices were excluded from evaluation. We also include KDE plots to
facilitate visualisation of size distributions for different parcellations (Figure S1, Left). The parcel size
distribution for our method reveals a tighter distribution than for other methods, likely due to a fairly
stringent parcel equilibrium regularisation. In future work, we will explore the impact of relaxing this
regularisation, particularly in consideration of the desirability for more variable parcel resolutions
across the brain in certain applications. More ﬂexible alternatives to a parcel equilibrium penalty,
such as a unilateral L2 loss imposed on parcels whose total weight is less than some minimum, could
also be explored.

B Denoising

Dataset, preprocessing, and computational resources We again use the minimally preprocessed
Human Connectome Project dataset detailed in the parcellation section above; however, we make a
few adjustments. First, we use BOLD time series whose dimension has already been reduced via a
mapping to parcels. We use the popular 400-region gwMRF parcellation introduced by Schaefer et al.
[6], which is based on a Markov random ﬁeld approach and recapitulates previously characterised
boundaries of functional subsystems. Second, we use a different data split, training on 4 of the 12
random splits, selecting another 4 for validation, and setting the last 4 aside for evaluation. Due to
computational limitations (because all data must ﬁt into memory simultaneously for evaluation), we
use a 351-image subset of the evaluation split for the main results. In the supplement, we report
benchmark results using each of the 12 splits (Figure S2).

15

Additionally, because the HCP dataset includes only gross motion estimates and ICA decompositions,
we ran additional preprocessing steps to obtain a more complete complement of confound time series.
For this additional preprocessing, we used stereotaxically embedded BOLD data volumes. First, we
computed the mean time series across all brain voxels (the global signal, GS). We then computed
the mean time series across voxels in binary masks indicating membership in white matter (WM)
and cerebrospinal ﬂuid (CSF) compartments; these masks were included with the HCP dataset. We
also computed singular value decompositions of WM and CSF voxels, yielding orthogonal sets of
principal component time series (aCompCor; [12]); we included the ﬁrst 10 from each compartment
in the input to the RFNN model. Finally, we computed a standardised version of the DVARS, the
standard deviation across all voxels in temporal difference images [13]. Following reports that head
motion estimates can be contaminated by respiratory artefact [14], we applied a notch ﬁlter to the
gross estimates of head motion included with the HCP dataset. We then used these ﬁltered estimates
to compute the framewise displacement (FD) that we used as our main QC metric: the sum of the
absolute values of the 6 ﬁltered motion estimates [15]. We then computed an expansion of the 6 gross
head motion estimates (translation and rotation along the x-, y-, and z- axes) and 3 compartment
mean signals (GS, WM, and CSF); this expansion consisted of temporal derivatives obtained via
backward differences, quadratic terms, and squares of derivatives. This expansion resulted in 36
terms, which together comprised the 36P model we used that has previously been demonstrated to
give excellent performance among a priori confound models [16]–[18].

In order to obtain relatively stable QC-FC correlations, the model was trained with a large batch size
of 100 time series. After parcellation, data were further preprocessed as follows. First, BOLD data
and a priori confounds were normalised such that each time series had a mean of 0 and a variance
of 1. Next, training and validation (but not evaluation) data were randomly windowed to select 500
contiguous time frames.

Because of incomplete acquisitions, data might be missing for some images. To address this, we
created a missing data mask for each batch of time series. We then synthesised data to impute
the missing time frames while minimally impacting actual time frames in downstream analyses.
We used a hybrid approach for time series imputation: any missing epochs comprising 3 or fewer
frames were imputed using a weighted average of the closest frames, while longer missing epochs
were imputed using a periodographic approach. To elaborate, we created a basis of sine and cosine
functions, selected the frames of each basis function that corresponded to seen epochs in the data,
and ﬁt each basis function to the data. We then used the estimated coefﬁcients from all basis function
ﬁts to reconstruct unseen data as a linear combination of basis functions. This approach is inspired
by a previous method introduced by Power et al. [19], which in turn draws inspiration from the
Lomb-Scargle periodogram [20]. (We did not apply censoring to denoise the data for this experiment
because, for this proof of concept, we were interested in optimising denoising model performance
without censoring.) After this imputation step, time series were ﬁltered to retain frequencies between
0.01 and 0.1 Hz using a brick-wall ideal ﬁlter applied multiplicatively in the frequency domain. The
selected low-frequency band is relatively artefact-free [16] and corresponds reasonably with the
frequency of the haemodynamic BOLD response. Filter weights were frozen and not conﬁgured to
be learnable. To prevent reintroduction of previously orthogonalised signals during the denoising
step [21], identical imputation and ﬁlter transformations were applied to BOLD and confound time
series. We again used several hours of compute time on a single RTX 2080 Ti GPU on a machine
running Ubuntu Linux to train the RFNN model.

RFNN architecture, denoising, and loss The model we used for denoising is a minimal, shallow
neural network with a single hidden convolutional layer. The hidden layer takes one channel of
dimension c × t—the c = 57 a priori confound time series—as its input. It has a total f = 5
learnable ﬁlters, each of dimension 1 × 9. With appropriate padding and a stride of 1, the output of the
convolutional layer is thus a set of f c × t time series, which are the input time series convolved with
each of the f “response functions”. These are passed through a thresholding leaky ReLU nonlinearity
(subtracting a bias term, applying the nonlinearity, and adding back the bias term) before they are
concatenated together with the input confound time series. The result is a (f + 1)c × t model matrix.
The output layer of the RFNN linearly maps the (f + 1)c confound time series to a single model
confound.

To obtain a denoised connectome, we compute the covariance of the parcellated BOLD time series
conditioned on this confound as deﬁned in the main text. Weights of any imputed observations were

16

Figure S2: Denoising evaluation across all 12 data splits, each comprising around 350 images. The
results shown in the main text are from the ﬁrst test split. If the QC-FC benchmarks are taken at face
value, the single-confound RFNN model outperforms other single-confound models with substantial
consistency and performs competitively with the SOTA 36-confound model.

Model Dist. Dep.

-0.083395
Null
-0.121429
GSR
-0.138829
36P
RFNN -0.117902
Table S1: Distance dependence of QC-FC correlations (lower absolute value is better).

set to 0, so denoised connectomes were based only on actual, seen data. We then normalise the
conditional covariance to a Pearson correlation before computing the loss function. This symmetric
correlation matrix is our estimate of the functional connectome. As described in the main text, the
loss function (QC-FC) is a “second-order” correlation computed across the batch dimension between
the correlation that represents each connectome edge and a measure of artefact, in this case the mean
ﬁltered framewise displacement across all time frames. Because an edge with an inverse relationship
to motion is just as undesirable as one with a direct relationship, the loss is operationalised as the
mean of the absolute value of QC-FC correlations. We train the RFNN model for 200 epochs using
stochastic gradient descent (SGD), sampling 10 batches randomly per epoch. We also create a null
model by randomly initialising a RFNN model without training it.

Evaluation We evaluate model performance using standard benchmarks derived from edge-wise
QC-FC correlations [19]. Benchmarks are computed on a held-out dataset of 351 BOLD time series.
Benchmarks include the number of edges for which a signiﬁcant relationship with motion is detected
across subjects and the median of the absolute value of all QC-FC correlations.

Although it is not a focus of the current proof of concept, QC-FC correlations also tend to be
distance-dependent: motion artefactually inﬂates estimates of short-range connections more than it
does estimates of long-range connections. Furthermore, methods that remove the global signal are
especially effective at removing widespread artefact that acts over longer ranges, so this distance-
dependence is apparently exacerbated after GSR (e.g., [16], [19]). We report the “third-order”
correlation between QC-FC correlations and inter-node separation in Supplemental Table S1.

There is arguably some circularity in using QC-FC correlations as the outcome measure after directly
training our model to minimise the mean of absolute QC-FC correlations. We leave to future work
any investigation of the limitations of QC-FC measures and a more complete characterisation of the
reasons that the RFNN was able to ostensibly outperform or compete with SOTA methods on these
benchmarks without concomitant zero-centring of connectome edges. In the interim, we reiterate
our tentative recommendation from the main text against use of this method until a more thorough
investigation is conducted.

C Covariance

Time-by-time covariance (subnetwork detection) We again used the HCP dataset for the time-
by-time covariance clustering experiment. Of our 12 HCP dataset splits, we selected 4 for training

17

and 4 for evaluation. Vertex-wise BOLD data were mapped onto 400 parcel-wise time series using
the 400-parcel dTSC/SWAPR parcellation that we trained in our ﬁrst experiment. Because of the
hemispheric tether regularisation that we used when learning the parcellation, we were able to impose
a soft inter-hemispheric symmetry constraint on the subnetwork detection problem.

We used a batch size of 20 and selected a random window of 800 time frames from each parcellated
time series when sampling a batch. Further preprocessing steps included imputation, ﬁltering, and
denoising, each implemented as described in Denoising above. Denoising was performed using
a 36-confound model with demonstrated efﬁcacy at removing structured artefact from fMRI data
[16]–[18]. The model was trained for 600 epochs, each of which consisted of 15 training steps, using
SGD. Training required several hours on a single RTX 2080 Ti GPU on a machine running Ubuntu
Linux.

The objective of the clustering is to learn a c × p covariance parameter Θ: an assignment of p parcels
to c clusters, which we interpret as subnetworks of the brain. For this proof of concept, we selected as
our learning objective the minimal nontrivial clustering into c = 2 subnetworks. During each forward
pass through the covariance layer, a softmax mapped the 2-dimensional cluster assignment of each
parcel onto the set of Bernoulli distributions. Next, we computed the two time-by-time covariances
parameterised by the matrices Θ obtained by embedding the elements of each row of Θ along the
main diagonal. These time-by-time covariances can be interpreted as dynamic proﬁles of the two
detected subnetworks. Finally, the covariances were normalised to time-by-time Pearson correlations.

To perform the clustering, we used a loss function consisting of 5 terms. First, a dispersion term, equal
to the negative L2 distance between the vectorised upper triangles of time-by-time matrices, promoted
separation between dynamic proﬁles. To favour structured dynamics with larger correlations, we also
imposed a symmetric L2 term that penalised distance of each entry in the correlation matrix from
either -1 or 1. Entropy and equilibrium terms were used as in the parcellation experiment to promote
eventual deterministic assignment to each of 2 approximately equal-sized subnetworks. As in the
parcellation experiment, we started with a weak entropy term that we increased at the 500th step.
The ﬁnal term was a Jensen-Shannon divergence penalty placed on the distance between a parcel’s
subnetwork assignment distribution, and the assignment distribution of that parcel’s analogue in the
opposite hemisphere. This penalty resulted in a relatively symmetric subnetwork assignment.

Because of the substantial autocorrelation between temporally proximal BOLD frames, which is
further inﬂated by the temporal ﬁlter that we apply, each time-by-time correlation matrix typically
features large values near its main diagonal. To downweight the importance of these autocorrelations
in the clustering, we took the Hadamard product of each time-by-time correlation matrix with a
Toeplitz-structured exponential discounting matrix. Speciﬁcally, before it was passed to the loss
function, each entry of the correlation matrix was scaled by the factor 1 − e−λt, where t denotes its
offset from the main diagonal and λ = 0.1 is a discount hyperparameter. To smooth training and
improve clustering repeatability, we also used stochastic weight averaging [10]. We tuned model and
training hyperparameters by repeating the analysis on our training set until we found hyperparameters
that yielded a repeatable solution.

We then assessed the out-of-sample replicability of the learned subnetwork structure by training the
model on a held-out evaluation set and assessing the convergence between the results across samples.
Although there were subtle differences, the subnetwork assignments detected in the two samples were
approximately the same after alignment (mean JS divergence over 400 parcels: 0.0052; 3 parcels’
maximum assignments differed per hemisphere). In the main text, we show maps of each parcel’s
maximum subnetwork assignment, together with examples of dynamic proﬁles for two subjects from
the training set. The dynamic proﬁles are windowed time-by-time correlation matrices parameterised
by subnetwork assignments.

Here, we also run a further evaluation of the subnetwork detection (Figure S3, Top). We used the
subnetworks detected in the training set as a reference, and we created 200 null subnetwork models by
randomly assigning each hemisphere’s parcels to subnetworks while preserving the number of parcels
per subnetwork and inter-hemispheric symmetry. We selected 300 subjects from the evaluation set and
computed for each subject the L2 distance between the dynamic proﬁles of the learned subnetworks,
as well as the L2 distance between the dynamic proﬁles of each null model’s subnetworks. Figure
S3 compares the separation between dynamic proﬁles for the learned subnetworks (red) against null
distributions (black) for each subject. In all cases, we ﬁnd that the learned subnetwork assignments
yield a superior separation to all null models.

18

Figure S3: Top, Compared with random symmetric subnetwork assignments, the subnetworks
detected by clustering consistently achieved greater separation of time-by-time dynamic covariance
matrices. Bottom, The detected states were relatively consistent across both subsamples used for the
initialisation and entirely unseen data.

Regional covariance (state detection) For the state detection experiment, we used small subsam-
ples of HCP data. Each of the 12 data splits we created was further divided randomly into 5 shards,
each consisting of approximately 50-80 resting-state BOLD time series. The results in the main text
are for one shard with 59 time series; we found this to be sufﬁcient for stable state detection. We
show these results together with 9 replicates in Figure S3.

Preprocessing followed a standard functional connectivity pipeline. The ﬁrst stage was parcellation
using the 400-parcel gwMRF atlas [6], which we selected because its parcels are ordered to follow the
brain’s large-scale community structure and would thus be conducive to visualisation of differences
between states. The 400 parcel-wise time series were then processed through imputation, ﬁltering,
and denoising as described in the Denoising section. We again denoised the time series using the
proven 36-confound model, which has demonstrated efﬁcacy in removing structured artefact due to
motion and respiration from BOLD time series [16]–[18]. The state detector model was trained for
50 epochs. Training required several minutes on a 12-core ﬁrst-generation Threadripper CPU on a
machine running Ubuntu Linux.

We used k-means clustering to initialise k = 3 learnable state templates. For this proof of concept,
we select k = 3 because it is the smallest number of clusters that does not produce only simple
high-connectivity and low-connectivity states. The templates are initialised by ﬁrst computing sliding-
window correlations (as in [22]) with a window size of 50 and a sliding step size of 25 for each of 1180
total images that constitute 4 splits of the HCP dataset. All windows are submitted as observations to
the k-means clustering algorithm. We use the L2 distance to cluster in this proof of concept because
it is compatible with various common implementations of k-means; however, a cosine distance
might better capture differences in connectivity conﬁgurations that are not simply due to connection
magnitude. Each of the k-means states that we detected using k-means clustering approximately
corresponded with a single state that we later detected using our differentiable approach. The k-means
initialisations additionally formed a baseline for model evaluation.

The learnable parameters of the clustering model include the templates thus initialised and instance-
speciﬁc c × t covariance parameters Θ, where we chose c = k = 3. Each of the c parameter vectors
of length t can be interpreted as the time course of a state; we interpret a value close to 0 as the
absence of the state, and a value close to 1 as the presence of the state at some time point. We require
an instance-speciﬁc parameterisation for each state because the presence or absence of a state is
likely to occur at different times for each subject under the unconstrained conditions of the resting
state. The use of instance-speciﬁc parameterisations is not handled natively by optimisers included

19

with many existing deep learning software libraries. This is likely in part because of the ambiguity
over how to handle buffers associated with instance-speciﬁc parameters (such as parameter-speciﬁc
moments). For this proof of concept, we take the most direct approach of maintaining a separate
buffer for each instance. We thus implemented new optimiser classes to handle parameter groups that
exist ephemerally in memory. (This applies to the instance-speciﬁc parameters that we use here, but
could also be extended to parameters speciﬁc to subgroups of the dataset–for instance, subjects with
several runs.)

To train the state detection model, we used an instance-speciﬁc extension of SGD with a loss function
consisting of 5 terms. First, an L2 penalty was applied to the backwards temporal difference of
the instance-speciﬁc state time courses Θ in order to promote state persistence and increase the
evidence required for the model to predict a transition between states. Second, an equilibrium loss
was imposed to ensure that all states were represented in each instance. Third, a dispersion penalty
was used to promote separation of each instance’s detected states. The two remaining loss terms used
the learnable templates to align detected states across instances. One term, another dispersion penalty,
was used to promote separation of templates, and the ﬁnal term was a penalty on the L2 distance
between each instance’s three detected states and the three templates. This ﬁnal term encouraged
the learnable templates to function as population-level representations of the states detected across
instances. We did not use an entropy penalty for this clustering problem.

The main text ﬁgure shows the learned parameters Θ and the learned templates, and compares the L2
distance between the states detected by the k-means initialisation and the averages (across instances)
of analogous states detected by the differentiable approach. We also repeated the state detection
experiment in 4 additional subsamples of the HCP dataset, each comprising between 50 and 80
instances, which were used in computing the k-means initialisation, and 5 similar subsamples that
were not used in computing the initialisation. We qualitatively observed consistent detection of three
analogous states across all subsamples (Figure S3, Bottom).

D Community detection

Dataset and preprocessing We use the Midnight Scan Club dataset (MSC) for the community
detection analysis. The MSC dataset includes a total 10 subjects each densely scanned over 10
sessions, both at rest and performing a number of directed cognitive tasks [23]. MSC data are openly
available and were acquired in accordance with the guidelines of the host institution’s IRB. In this
proof of concept, we limit our analysis to resting-state data. We use the ﬁrst three scans from each
subject for the community detection analysis.

Data were parcellated using the 400-parcel gwMRF parcellation [6], ﬁltered, and denoised using
a 36-confound model with demonstrated efﬁcacy at removing structured artefact associated with
motion and respiration from BOLD data [16]–[18]. The denoised, parcellated BOLD time series
were inputs to the community detection model.

The learnable parameters of the model were (i) the p × c community afﬁliation matrix C of p
parcels to c communities and (ii) the instance-speciﬁc c × t time courses Θ of each community. The
community afﬁliation of each parcel was mapped through a softmax function that ensured it was a
valid probability distribution, and each community time course was mapped through a sigmoid that
constrained it to (0, 1). Each community time course could thus be interpreted as the extent to which
the corresponding community was modularised during each time frame of the scan. Each of these
community time courses thereafter parameterised a separate covariance matrix.

Model details We thus computed c = 10 parameterised 400 × 400 covariance matrices ACi and
1 unparameterised (standard or “time-averaged”) 400 × 400 covariance matrix A among the 400
parcel-wise BOLD time series. (All covariance matrices were normalised to Pearson correlations for
the purpose of further analysis.) Next, we used the community afﬁliation C to estimate the relaxed
Girvan-Newman modularity [24] on the unparameterised covariance matrix A as described in the
main text. We then considered each column of C separately. Each column Ci of C corresponds to a
(cid:124)
single community Ci, and the entries of the rank-one outer product CiC
i indicate the extent to which
each edge of the connectome graph connects two vertices in Ci. We used each of these low-rank outer
products as coafﬁliation matrices HCi to compute the relaxed Girvan-Newman modularity on the
corresponding parameterised covariance ACi.

20

We thus sought to optimise the c + 1 modularities by jointly learning the afﬁliation of each parcel
to a community and the extent to which the community was modularised during each time frame.
To achieve this objective, we trained the model using a loss function with four main terms. Two
multipliers controlled the balance between static communities parameterised only by C and dynamic
communities parameterised by C and Θ. The remaining two multipliers regularised the learned
parameters: one modulated an L2 penalty on the minimum distance between each entry in Θ and
either 0 or 1, thereby steering the model to make a binary decision for each time point as to whether a
community was modularised or not, and the other modulated a backward difference L2 penalty on Θ
that promoted persistence of modularisation and demodularisation states for each community. The
model was trained for 500 epochs using the Adam optimiser [9]. Training required under 1 hour on a
12-core, 24-thread CPU.

Evaluation To evaluate the validity of the learned community time courses Θ, we created, for each
instance, a distribution of 500 null community time courses by randomly shifting that instance’s
learned time courses. Time frames that the shift displaced over the end of each time course were
wrapped back to its start. We then computed, for each of the null covariances (cid:101)ACi parameterised
by a randomly shifted time course, the corresponding modularity. We created the main text plot of
learned and null modularities for each community by subtracting each of the 500 null modularities
computed for each instance from the learned modularity for that instance. We generally found that the
learned modularity outperformed random-shift nulls. (We remark that small random shifts, and even
random shifts of zero, are likely to occur because 500 random shifts are selected for each subject,
and each time course is 814 frames long. Due to factors including the persistence imposed on each
community’s modularisation state and the substantial autocorrelation of ﬁltered BOLD time series,
these small random shifts will likely have modularity very close to that of the learned model.)

References

[1] D. Van Essen, K. Ugurbil, E. Auerbach, et al., “The Human Connectome Project: A data
acquisition perspective,” NeuroImage, vol. 62, no. 4, pp. 2222–2231, Oct. 2012. DOI: 10.
1016/j.neuroimage.2012.02.018.

[2] D. C. Van Essen, S. M. Smith, D. M. Barch, T. E. Behrens, E. Yacoub, and K. Ugurbil, “The
WU-Minn Human Connectome Project: An overview,” NeuroImage, vol. 80, pp. 62–79, Oct.
2013. DOI: 10.1016/j.neuroimage.2013.05.041.

[3] M. F. Glasser, S. N. Sotiropoulos, J. A. Wilson, et al., “The minimal preprocessing pipelines
for the Human Connectome Project,” NeuroImage, vol. 80, pp. 105–124, Oct. 2013. DOI:
10.1016/j.neuroimage.2013.04.127.

[4] B. Fischl, M. I. Sereno, and A. M. Dale, “Cortical Surface-Based Analysis II: Inﬂation,
Flattening, and a Surface-Based Coordinate System,” NeuroImage, vol. 9, no. 2, pp. 195–207,
Feb. 1999. DOI: 10.1006/nimg.1998.0396.

[5] E. C. Robinson, S. Jbabdi, M. F. Glasser, et al., “MSM: A new ﬂexible framework for
Multimodal Surface Matching,” NeuroImage, vol. 100, pp. 414–426, Oct. 2014. DOI: 10.
1016/j.neuroimage.2014.05.069.

[6] A. Schaefer, R. Kong, E. M. Gordon, et al., “Local-Global Parcellation of the Human Cerebral
Cortex from Intrinsic Functional Connectivity MRI,” Cerebral Cortex, vol. 28, no. 9, pp. 3095–
3114, Sep. 2018. DOI: 10.1093/cercor/bhx179.
J. Li, R. Kong, R. Liégeois, et al., “Global signal regression strengthens association between
resting-state functional connectivity and behavior,” NeuroImage, vol. 196, pp. 126–141, Aug.
2019. DOI: 10.1016/j.neuroimage.2019.04.016.

[7]

[8] A. Aizman, G. Maltby, and T. Breuel, “High Performance I/O For Large Scale Deep Learning,”
in 2019 IEEE International Conference on Big Data (Big Data), Los Angeles, CA, USA:
IEEE, Dec. 2019, pp. 5965–5967. DOI: 10.1109/BigData47090.2019.9005703.

[9] D. P. Kingma and J. Ba, Adam: A Method for Stochastic Optimization, Number:

arXiv:1412.6980 arXiv:1412.6980 [cs], Jan. 2017.

[10] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson, Averaging Weights Leads
to Wider Optima and Better Generalization, Number: arXiv:1803.05407 arXiv:1803.05407
[cs, stat], Feb. 2019.

21

[11] M. F. Glasser, T. S. Coalson, E. C. Robinson, et al., “A multi-modal parcellation of human
cerebral cortex,” Nature, vol. 536, no. 7615, pp. 171–178, Aug. 2016. DOI: 10 . 1038 /
nature18933.

[12] Y. Behzadi, K. Restom, J. Liau, and T. T. Liu, “A component based noise correction method
(CompCor) for BOLD and perfusion based fMRI,” NeuroImage, vol. 37, no. 1, pp. 90–101,
Aug. 2007. DOI: 10.1016/j.neuroimage.2007.04.042.

[13] S. Afyouni and T. E. Nichols, “Insight and inference for DVARS,” NeuroImage, vol. 172,

pp. 291–312, May 2018. DOI: 10.1016/j.neuroimage.2017.12.098.

[14] D. A. Fair, O. Miranda-Dominguez, A. Z. Snyder, et al., “Correction of respiratory artifacts in
MRI head motion estimates,” NeuroImage, vol. 208, p. 116 400, Mar. 2020. DOI: 10.1016/j.
neuroimage.2019.116400.
J. D. Power, K. A. Barnes, A. Z. Snyder, B. L. Schlaggar, and S. E. Petersen, “Spurious but
systematic correlations in functional connectivity MRI networks arise from subject motion,”
NeuroImage, vol. 59, no. 3, pp. 2142–2154, Feb. 2012. DOI: 10.1016/j.neuroimage.2011.
10.018.

[15]

[16] T. D. Satterthwaite, M. A. Elliott, R. T. Gerraty, et al., “An improved framework for confound
regression and ﬁltering for control of motion artifact in the preprocessing of resting-state
functional connectivity data,” NeuroImage, vol. 64, pp. 240–256, Jan. 2013. DOI: 10.1016/j.
neuroimage.2012.08.052.

[17] R. Ciric, D. H. Wolf, J. D. Power, et al., “Benchmarking of participant-level confound regres-
sion strategies for the control of motion artifact in studies of functional connectivity,” Neu-
roImage, vol. 154, pp. 174–187, Jul. 2017. DOI: 10.1016/j.neuroimage.2017.03.020.

[18] L. Parkes, B. Fulcher, M. Yücel, and A. Fornito, “An evaluation of the efﬁcacy, reliability,
and sensitivity of motion correction strategies for resting-state functional MRI,” NeuroImage,
vol. 171, pp. 415–436, May 2018. DOI: 10.1016/j.neuroimage.2017.12.073.
J. D. Power, A. Mitra, T. O. Laumann, A. Z. Snyder, B. L. Schlaggar, and S. E. Petersen,
“Methods to detect, characterize, and remove motion artifact in resting state fMRI,” NeuroImage,
vol. 84, pp. 320–341, Jan. 2014. DOI: 10.1016/j.neuroimage.2013.08.048.

[19]

[20] N. R. Lomb, “Least-squares frequency analysis of unequally spaced data,” Astrophysics and
Space Science, vol. 39, no. 2, pp. 447–462, Feb. 1976. DOI: 10.1007/BF00648343.
[21] M. N. Hallquist, K. Hwang, and B. Luna, “The nuisance of nuisance regression: Spectral
misspeciﬁcation in a common approach to resting-state fMRI preprocessing reintroduces noise
and obscures functional connectivity,” NeuroImage, vol. 82, pp. 208–225, Nov. 2013. DOI:
10.1016/j.neuroimage.2013.05.116.

[22] E. A. Allen, E. Damaraju, S. M. Plis, E. B. Erhardt, T. Eichele, and V. D. Calhoun, “Tracking
Whole-Brain Connectivity Dynamics in the Resting State,” Cerebral Cortex, vol. 24, no. 3,
pp. 663–676, Mar. 2014. DOI: 10.1093/cercor/bhs352.

[23] E. M. Gordon, T. O. Laumann, A. W. Gilmore, et al., “Precision Functional Mapping of
Individual Human Brains,” Neuron, vol. 95, no. 4, 791–807.e7, Aug. 2017. DOI: 10.1016/j.
neuron.2017.07.011.

[24] M. E. J. Newman and M. Girvan, “Finding and evaluating community structure in networks,”
Physical Review E, vol. 69, no. 2, p. 026 113, Feb. 2004, arXiv:cond-mat/0308217. DOI:
10.1103/PhysRevE.69.026113.

22

