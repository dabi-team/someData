parGeMSLR: A Parallel Multilevel Schur Complement Low-Rank Preconditioning and
Solution Package for General Sparse Matrices

Tianshi Xua,∗, Vassilis Kalantzisb, Ruipeng Lic, Yuanzhe Xid, Geoﬀrey Dillone, Yousef Saada

aDepartment of Computer Science and Engineering, University of Minnesota, Minneapolis, MN 55455
bThomas J. Watson Research Center, IBM Research, Yorktown Heights, NY 10598
cCenter for Applied Scientiﬁc Computing, Lawrence Livermore National Laboratory, P.O. Box 808, L-561, Livermore, CA 94551
dDepartment of Mathematics, Emory University, Atlanta, GA 30322
eDepartment of Mathematics, University of South Carolina, Columbia, SC 29208

2
2
0
2

y
a
M
4

]
S
M

.
s
c
[

1
v
4
2
2
3
0
.
5
0
2
2
:
v
i
X
r
a

Abstract
This paper discusses parGeMSLR, a C++/MPI software library for the solution of sparse systems of linear algebraic equations
via preconditioned Krylov subspace methods in distributed-memory computing environments. The preconditioner implemented in
parGeMSLR is based on algebraic domain decomposition and partitions the symmetrized adjacency graph recursively into several
non-overlapping partitions via a p-way vertex separator, where p is an integer multiple of the total number of MPI processes. From
a numerical perspective, parGeMSLR builds a Schur complement approximate inverse preconditioner as the sum between the matrix
inverse of the interface coupling matrix and a low-rank correction term. To reduce the cost associated with the computation of
the approximate inverse matrices, parGeMSLR exploits a multilevel partitioning of the algebraic domain. The parGeMSLR library is
implemented on top of the Message Passing Interface and can solve both real and complex linear systems. Furthermore, parGeMSLR
can take advantage of hybrid computing environments with in-node access to one or more Graphics Processing Units. Finally, the
parallel eﬃciency (weak and strong scaling) of parGeMSLR is demonstrated on a few model problems arising from discretizations
of 3D Partial Diﬀerential Equations.

Keywords: Schur complement, low-rank correction, distributed-memory preconditioner, sparse non-Hermitian linear systems,
Graphics Processing Units

1. Introduction

This paper discusses a distributed-memory library for the
iterative solution of systems of linear algebraic equations of the
form

Ax = b,
(1)
where the matrix A ∈ Cn×n is large, sparse, and (non-)Hermitian.
Problems of this form typically originate from the discretization
of a Partial Diﬀerential Equation in 2D or 3D domains.

Iterative methods solve (1) by a preconditioned Krylov sub-
space iterative methods [1, 2], e.g., preconditioned Conjugate
Gradient [1], if A is Hermitian and positive-deﬁnite, or GM-
RES [3] if A is non-Hermitian. The role of the preconditioner
is to cluster the eigenvalues in an eﬀort to accelerate the con-
vergence of Krylov subspace method. For example, an eﬃcient
right preconditioner M transforms (1) into the preconditioned
system AM−1(Mx) = b, where M−1 can be applied inexpen-
sively. An additional requirement is that the setup and applica-
tion of the operator M−1 should be easily parallelizable.

∗The work of the ﬁrst and the last author was supported by the National
Science Foundation (NSF) grant DMS-1912048. The work of the fourth au-
thor was supported by the NSF grant OAC-2003720. This work was per-
formed under the auspices of the U.S. Department of Energy by Lawrence
Livermore National Laboratory under Contract DE-AC52-07NA27344 (LLNL-
JRNL-830724)

Similarly to Krylov subspace methods, algebraic multigrid
(AMG) methods are another widely-used class of iterative solvers
[4]. AMG uses the ideas of interpolation and restriction to
build multilevel preconditioners that eliminate the smooth er-
ror components. AMG is provably optimal for Poisson-like
problems on regular meshes where the number of iterations to
achieve convergence almost stays constant as the problem size
increases. This property leads to appealing weak scaling re-
sults of AMG in distributed-memory computing environments
[5, 6, 7]. However, AMG can fail when applied either to in-
deﬁnite problems or irregular meshes. It is worth mentioning
that AMG can also be used as a preconditioner in the context of
Krylov subspace methods.

For general sparse linear systems, a well-known class of
general-purpose preconditioners is that of Incomplete LU (ILU)
factorization preconditioners [8, 9, 1]. Here, the matrix A is ap-
proximately factored as A ≈ LU where L is lower triangular
and U is upper triangular, and the preconditioner is deﬁned as
M = LU. Applying M−1 then consists of two triangular sub-
stitutions. ILU preconditioners can be applied to a greater se-
lection of problems than AMG, including indeﬁnite problems
such as discretized Helmholtz equations [10, 11], and their ro-
bustness can be improved by modiﬁed/shifted ILU strategies
[12, 13, 14]. On the other hand, the scalability of ILU precon-
ditioned Krylov subspace methods is typically inferior com-

Preprint submitted to Elsevier

May 9, 2022

 
 
 
 
 
 
pared to AMG. In particular, even for Poisson-like problems,
the number of iterations to achieve convergence by ILU pre-
conditioned Krylov subspace methods increases with respect to
the matrix size. Moreover, the sequential nature of triangular
substitutions limit the parallel eﬃciency of ILU precondition-
ers implemented on distributed-memory systems, and recent ef-
forts have been focusing on improving their scalability, e.g., see
[15, 16, 17].

The parallel eﬃciency of ILU preconditioners can be en-
hanced by domain decomposition (DD), where the original prob-
lem is decomposed into several subdomains which correspond
to diﬀerent blocks of rows of the coeﬃcient matrix A. The sim-
plest DD-based ILU approach is the block-Jacobi ILU precon-
ditioner, where a local ILU is performed on each local subma-
trix. Since this method ignores all of the oﬀ-diagonal matri-
ces corresponding to inter-domain couplings, its convergence
rate tends to become slower as the number of subdomains in-
creases, and several strategies have been proposed to handle
the inter-domain couplings in order to improve the convergence
rate. Restricted Additive Schwarz (RAS) methods expand the
local matrix by a certain level to gain a faster convergence rate
at the cost of losing some memory scalability [18]. Global fac-
torization ILU methods factorize local rows corresponding to
interior unknowns ﬁrst, after which a global factorization of the
couplings matrix is applied based on some graph algorithms
[19, 20]. These methods use partial ILU techniques with drop-
ping [21, 22], incomplete triangular solve [23], and low-rank
approximation [24] to form the Schur complement system and
can be generalized into multilevel ILU approaches [21, 22, 24].
When the Finite Element method is used and the elements are
known, two-level DD methods including BDDC [25] and FETI-
DP [26, 27], as well as the GenEO preconditioner [28] are also
have been shown to be eﬀective approaches. We note that an
additional strategy is to combine approximate direct factoriza-
tion techniques with low-rank representation of matrix blocks
[29], PasTix [30], and DDLR [31]. When the matrix A is SPD,
it is possible to reduce the size of the Schur complement matrix
without introducing any ﬁll-in, e.g., see SpaND [32].

Other preconditioning strategies that can be implemented
on distributed-memory environments include the (factorized)
sparse approximate inverse preconditioners [33, 34, 35, 36, 37],
polynomial preconditioners [38], and rank-structured precondi-
tioners [39, 40, 41, 42]; see also [43] for a distributed-memory
hierarchical solver. Some of the these techniques can be further
compounded with AMG, as “smoothers”, or ILU-based precon-
ditioners. For example, a combination of SLR [44] and polyno-
mial preconditioning is discussed in [38].

1.1. Contributions of this paper

This paper discusses the implementation of a distributed-
memory library, termed1 parGeMSLR, for the iterative solution
of sparse systems of linear algebraic equations in large-scale
distributed-memory computing environments. parGeMSLR2 is

1The abbreviation of the library is derived by the complete name “parallel

Generalized multilevel Schur complement Low-Rank preconditioner”

2The source code can be found in https://github.com/Hitenze/pargemslr

2

written in C++, and communication among diﬀerent processor
groups is achieved by means of the Message Passing Interface
standard (MPI). The parGeMSLR library is based on the Gen-
eralized Multilevel Schur complement Low-Rank (GeMSLR)
algorithm described in [24]. GeMSLR applies a multilevel par-
titioning of the algebraic domain, and the variables associated
with each level are divided into either interior or interface vari-
ables. The multilevel structure is built by applying a p-way
graph partitioner to partition the induced subgraph associated
with the interface variables of the preceding level. Once the
multilevel partitioning is completed, GeMSLR creates a sepa-
rate Schur complement approximate inverse at each level. Each
approximate inverse is the sum of two terms, with the ﬁrst term
being an approximate inverse of the interface coupling matrix,
and the second term being a low-rank correction which aims
at bridging the gap between the ﬁrst term and the actual Schur
complement matrix inverse associated with that level. Below,
we summarize the main features of the parGeMSLR library:

1. Scalability. parGeMSLR extends the capabilities of low-
rank-based preconditioners, such as GeMSLR, by recur-
sively partitioning the algebraic domain into levels which
have the same number of partitions. In turn, this leads to
enhanced scalability when running on distributed-memory
environments.

2. Robustness and complex arithmetic. In contrast to ILU
preconditioners, the numerical method implemented in
parGeMSLR is less sensitive to indeﬁniteness and can be
updated on-the-ﬂy without discarding previous computa-
tional eﬀorts. Additionally, parGeMSLR supports com-
plex arithmetic and thus can be utilized to solve com-
plex linear systems such as those originating from the
discretization of Helmholtz equations.

3. Hybrid hardware acceleration. GPU acceleration is
supported in several iterative solver libraries aiming to
speed-up the application of preconditioners such as AMG
or ILU, e.g., hypre [45], PARALUTION [46], ViennaCL
[47], HIFLOW [48], PETSc [49], and Trilinos [50]. A
number of direct solver libraries including STRUMPACK
[51, 52, 53] and SuperLU DIST [54] also provide GPU
support. Similarly, parGeMSLR can exploit one or more
GPUs by oﬄoading any computation for which the user
provides a CUDA interface.

This paper is organized as follows. Section 2 discusses low-
rank correction preconditioners and provides an algorithmic de-
scription of parGeMSLR. Section 3 provides details on the mul-
tilevel reordering used by parGeMSLR. Section 4 presents in-
depth discussion and details related to the implementation and
parallel performance aspects of parGeMSLR. Section 5 demon-
strates the performance of parGeMSLR on distributed-memory
environments. Finally, our concluding remarks are presented in
Section 6.

2. Schur complement approximate inverse preconditioners

via low-rank corrections

This section discussed the main idea behind (multilevel)
Schur complement preconditioners enhanced by low-rank cor-
rections, e.g., see [55, 44, 24, 56].

2.1. The Schur complement viewpoint

Let the linear system Ax = b be permuted as

A0x = PT AP(PT x) = PT b,

(2)

where P is an n × n permutation matrix such that


A0 =

(cid:34)

B F
E C

(cid:35)

=

B(2)

. . .

B(1)



E(1) E(2)

· · · E(p)

F(1)
F(2)
...
B(p) F(p)
C





,

and the matrices B(i), F(i), and E(i) are of size di × di, di × s, and
s × di, respectively. The matrix C is of size s × s, and the matrix
d j + s j = n. Such matrix permu-
partitioning satisﬁes d + s =

j=p(cid:80)
j=1

tations can be computed by partitioning the adjacency graph of
the matrix |A| + (cid:12)(cid:12)(cid:12)AT (cid:12)(cid:12)(cid:12) into p ∈ N non-overlapping partitions and
reordering the unknowns/equations such that the variables asso-
ciated with the d interior nodes across all partitions are ordered
before the variables associated with the s interface nodes.

Following the above notation, the linear system in (2) can

be written in a block form

(cid:34)

B F
E C

(cid:35)
(cid:35) (cid:34)
u
v

=

(cid:35)

(cid:34)

f
g

,

(3)

where u, f ∈ Rd and v, g ∈ Rs. Once the solution in (3) is
computed, the solution x of the original, non-permuted system
of linear algebraic equations Ax = b can be obtained by the in-
verse permutation x = P

. Throughout the rest of this section

(cid:35)
(cid:34)
u
v

we focus on the solution of the system in (3).

Following a block-LDU factorization of the matrix A0, the

permuted linear system in (2) can be written as
(cid:35) (cid:34)

(cid:35) (cid:34)

(cid:34)

(cid:35)

B

I B−1F

I
EB−1

I

S

I

(cid:35) (cid:34)
u
v

=

(cid:35)

(cid:34)

f
g

,

(4)

where S = C − EB−1F denotes the Schur complement matrix.
The solution of (3) is then equal to

(cid:34)

=

(cid:34)

(cid:35)
u
v

I −B−1F

(cid:35) (cid:34)

B−1

I

(cid:35) (cid:34)

I
−EB−1

S −1

(cid:35) (cid:34)

(cid:35)

f
g

,

I

which requires: a) the solution of two linear systems with the
block-diagonal matrix B, and b) the solution of one linear sys-
tem with the the Schur complement matrix S . Note that since
the matrix B is block-diagonal, the associated linear systems are
decoupled into p independent systems of linear algebraic equa-
tions. Assuming a distributed-memory computing environment

3

with p separate processor groups, each system of linear alge-
braic equations can be solved in parallel by means of applying
a direct solver locally in each separate process.

In several real-world applications, e.g., those involving the
discretization of PDEs on three-dimensional domains, solving
the systems of linear algebraic equations with matrices B and S
through a direct solver is generally impractical, primarily due
to the large computational and memory cost associated with
forming and factorizing the Schur complement matrix. An al-
ternative then is to solve the linear systems with matrices B and
S inexactly. For example, the solution of linear systems with
matrix B can be computed approximately by replacing its ex-
act LU factorization with an incomplete threshold LU (ILUT)
[8]. Likewise, the exact Schur complement can be sparsiﬁed
by discarding entries below a certain threshold value or located
outside a pre-determined pattern [22, 57] The approximate fac-
torizations of the matrices B and S can be combined to form an
approximate LDU factorization of (4) which can be then used
as a preconditioner in a Krylov subspace iterative solver such
as GMRES.

2.2. Schur complements and low-rank corrections

One of the main drawbacks associated with incomplete fac-
torizations is that they can not be easily updated if one needs a
more accurate preconditioner unless the iterative ParILUT [15,
16] works for the problem and is used. Moreover, their robust-
ness can be limited when the matrix A is indeﬁnite. For such
scenarios, it has been advocated to add a low-rank correction
term to enhance the eﬃciency of the Schur complement precon-
ditioner, without discarding the previously computed incom-
plete factorizations. The low-rank enhancement implemented
in parGeMSLR follows the GeMSLR multilevel preconditioner
[24], a non-Hermitian extension of [44, 56]. Other approaches
based on low-rank corrections can be found in [55, 58].

The GeMSLR preconditioner expresses the Schur comple-

ment matrix as

S = (I − EB−1FC−1)C = (I − G)C,

(5)

where G = EB−1FC−1. Consider now the complex Schur de-
composition G = WRW H, where the s × s matrix W is unitary
and the s × s matrix R is upper-triangular such that its diagonal
entries contain the eigenvalues of matrix G. Plugging the latter
in (5) results to

S = (I − WRW H)C = W(I − R)W HC,

from which we can write the inverse of the Schur complement
matrix as (Sherman-Morrison-Woodbury formula):

S −1 = C−1 + C−1W[(I − R)−1 − I]W H.

(6)

Following (6), a system of linear equations with the Schur com-
plement matrix requires the solution of a system of linear equa-
tions with matrix C, as well as matrix-vector multiplications
and triangular matrix inversions with matrices W/W H and (I −
R)−1, respectively. The product of matrices W[(I − R)−1 − I]W H

is a Schur decomposition by itself, with corresponding eigen-
values γi/(1 − γi), i = 1, . . . , s, where γi denotes the i-th eigen-
value of the matrix G. Therefore, as long as the eigenvalues
of the latter matrix are not located close to one, the matrix
C(S −1 − C−1) = W[(I − R)−1 − I]W H can be approximated by
a low-rank matrix, i.e., S −1 is approximately equal to C−1 plus
some low-rank correction.

The expression in (6) can be transformed into a practical
preconditioner if the matrix W[(I − R)−1 − I]W H is replaced by
a rank-k approximation, where k ∈ N is generally a user-given
parameter. More speciﬁcally, let Wk denote the s × k matrix
which holds the leading k Schur vectors of matrix G, and let Rk
denote the k × k leading principal submatrix of matrix R. Then,
the GeMSLR approximate inverse preconditioner is equal to

M−1 = C−1 + C−1Wk[(I − Rk)−1 − I]W H

k ≈ S −1.

(7)

2.3. Computations with an incomplete factorization of B

For large-scale problems, computing an exact factorization
of the block-diagonal matrix B can be quite expensive. Instead,
what is typically available is an ILUT factorization LU ≈ B.
Therefore, instead of computing a rank-k Schur decomposition
of matrix G, in practice we approximate a truncated Schur de-
composition of the matrix (cid:98)G = E(U−1L−1)FC−1. Let then

(cid:98)G(cid:98)Vm = (cid:98)Vm (cid:98)Hm + βm(cid:98)vm+1eH
m,

denote an m-length Arnoldi relation obtained with matrix (cid:98)G,
where [(cid:98)Vm,(cid:98)vm+1]H[(cid:98)Vm,(cid:98)vm+1] = I, and (cid:98)Hm is upper-Hessenberg.
Moreover, let (cid:98)Hm = QT QH denote the complex Schur decom-
position of matrix (cid:98)Hm. The low-rank correction term used in
k , where Tk ∈ Rk×k
GeMSLR is of the form (cid:98)Wk[(I − (cid:98)Rk)−1 − I] (cid:98)W H
denotes the k × k leading principal submatrix of matrix T , and
(cid:98)Wk = (cid:98)VmQk, where Qk ∈ Rs×k denotes the matrix holding the k
leading Schur vectors of matrix (cid:98)Hm.

the unknowns/equations such that the variables associated with
the interior nodes across all partitions are ordered before the
variables associated with the interface nodes of the adjacency
graph. The matrix Cl−1 is then permuted in-place through the
sl−1 × sl−1 permutation matrix Pl−1, where sl−1 denotes the size
of the matrix Cl−1.

The solution of a system of linear algebraic equations with
as the right-

matrix Al as the coeﬃcient matrix and
hand side, can be computed as

gT
l

f T
l

(cid:105)T

(cid:104)

(cid:34)

=

(cid:35)

(cid:34)
ul
vl

I −B−1
l Fl
I

(cid:35) (cid:34)

B−1
l

(cid:35) (cid:34)

I
−ElB−1
l

S −1
l

(cid:35)

,

(cid:35) (cid:34)

fl
gl

I

where S l = Cl − ElB−1
l Fl denotes the sl × sl Schur complement
matrix associated with the l-th level, where sl ∈ N denotes the
size of the matrix Cl. Instead of computing the exact LU factor-
izations of matrices Bl and S l, the preconditioner implemented
l ≈ (LlUl)−1, where
in the parGeMSLR library substitutes B−1
LlUl denotes an ILUT factorization of matrix Bl, and
l Wl,k[(I − Rl,k)−1 − I]W H
l,k,

l ≈ C−1
S −1

+ C−1

(9)

l

where (cid:98)Wl,k denotes the matrix which holds the approximate
leading k Schur vectors of the matrix (cid:98)Gl = ET
l FlC−1
,
and (cid:98)Rl,k denotes the approximation of the k × k leading principal
submatrix of the matrix (cid:98)Rl that satisﬁs the Schur decomposition
(cid:98)Gl = (cid:98)Wl(cid:98)Rl (cid:98)W H
l . Algorithm 1 summarizes the above discussion
(“setup phase”) in the form of an algorithm. Notice that the re-
cursion stops at level lev − 1, and an ILUT of the matrix Clev−1
is computed explicitly.

l L−1

l U−1

l

Algorithm 1 Parallel GeMSLR Setup
1: procedure pGeMSLRSetup(A, lev)
2:
3:
4:
5:

Generate lev-level structure by Algorithm 3.
for l from 0 to lev − 1 do

Compute ILU factorization LlUl ≈ Bl.
Compute matrices (cid:98)Wl,k and (cid:98)Rl,k.
If l = lev − 1, compute an ILUT factorization

2.4. Multilevel extensions

6:

For large-scale, high-dimensional problems, the application
of the matrix C−1 by means of an LU factorization of matrix
C can still be expensive; especially when the value of p is too
large, leading to large vertex separators. The idea suggested
in [56, 31], and employed by GeMSLR, is to take advantage
of the purely algebraic formulation developed in the previous
section and apply C−1 inexactly by using the Schur complement
low-rank preconditioner described in the previous section. In
fact, this approach can be repeated more than once, leading to a
multilevel preconditioner.

More speciﬁcally, let lev ∈ N denote the number of levels,

and deﬁne the sequence of matrices

(cid:35)

(cid:34)

Bl Fl
El Cl

, C−1 = A,

Al = Pl−1Cl−1Pl−1 =

l = 0, 1, . . . , lev−1,
(8)
where the matrix Bl is block-diagonal with p on-diagonal ma-
trix blocks. The 2 × 2 block matrix partition of each matrix Al
is obtained by partitioning the adjacency graph of the matrix
|Cl−1| + |CT
l−1| into p non-overlapping partitions and reordering

Llev−1Ulev−1 ≈ Clev−1; exit.

end for

7:
8: end procedure

Algorithm 2 outlines the procedure associated with the ap-
plication of the GeMSLR preconditioner (“solve phase”). At
each level, the preconditioning step consists of a forward and
backward substitution with the ILUT triangular factors of Bl,
followed by the application of the rank-k correction term. When
l = lev − 1, there is no low-rank correction term applied, since
this is the last level. Moreover, when l = 0 (root level), it is
possible to enhance the GeMSLR preconditioner by applying
a few steps of right preconditioned GMRES. Note though that
these iterations are performed with the inexact Schur comple-
ment (cid:98)S l = Cl − El(U−1L−1)Fl.

3. Multilevel reordering

This section outlines the multilevel reordering approach im-
plemented in the parGeMSLR library. For simplicity, we focus

4

Algorithm 2 Standard Parallel GeMSLR Solve
1: procedure pGeMSLRSolve(b, l)

= Pl−1b.

(cid:35)

(cid:34)
b1
b2
l b1.

Apply reordering
l L−1

Solve z1 = U−1
Compute z2 = b2 − Elz1.
if l = 0 then

Solve (cid:98)S ly2 = z2 by right preconditioned GMRES.

else

Compute u2 = (cid:98)Wl,k[(I − (cid:98)Rl,k)−1 − I] (cid:98)W H
l,kz2
Call y2 = pGeMSLRSolve(u2 + z2, l + 1).

end if
Compute y1 = z1 − U−1
l L−1
l Fly2.
(cid:35)
(cid:34)
y1
Apply reordering q = Pl−1
y2
return x

.

13:
14: end procedure

2:

3:

4:
5:
6:
7:
8:
9:

10:
11:

12:

on symmetric reorderings obtained by applying a p-way vertex
separator to the adjacency graph associated with the matrices
|Cl−1| + |CT
l−1|, l = 0, . . . , lev − 1, C−1 = A, [59, 60, 61, 62]. In
particular, given a graph G = (V, E), a p-way vertex separator
computes a separator S ⊂ V and p non-overlapping (disjoint)
sets V1, . . . , Vp ⊂ V such that V1 ∪ . . . ∪ Vp ∪ S = V and there
are no edges connecting the sets Vi and V j when i (cid:44) j.

Apply p-way partitioning to the graph associated

Algorithm 3 Parallel GeMSLR Reordering
1: procedure pGeMSLRReordering(A, lev)
2:
3:
4:

Set C−1 ≡ A.
for l from 0 to lev − 1 do

with the matrix |Cl−1| + |CT

l−1|.

5:

Set Al = Pl−1Cl−1Pl−1 =

end for
6:
return
7:
8: end procedure

(cid:34)

(cid:35)
.

Bl Fl
El Cl

3.1. Hierarchical Interface Decomposition

The GeMSLR preconditioner relies on a Hierarchical In-
terface Decomposition (HID) [63] to reduce the setup cost of
the ILU and low-rank correction parts associated with the setup
phase of the preconditioner. The main idea behind HID is to
partition the adjacency graph of |A| + |AT | into 2lev partitions
via nested dissection with a recursion depth of lev. The vertex
separators at level l are disjoint with each other since they are
divided by vertex separators from higher levels. When ordered
by levels, the global permutation of matrix A will have a block-
diagonal structure with 2lev−l blocks at level 0 ≤ l ≤ lev − 1, i.e.,
the number of diagonal blocks at each level reduce by a factor
of two.

3.2. Multilevel partitioning through p-way vertex separators

In contrast to low-rank correction preconditioners such as
MSLR and GeMSLR [56, 24], the main goal of parGeMSLR
is to sustain good parallel eﬃciency, and thus HID is not ap-
propriate.3 Instead, the default approach in parGeMSLR is to
partition the adjacency graph by a multi-level partitioner where
each level consists of p partitions and a vertex separator. The
latter choice results to a ﬁxed number of p partitions at each
level, and thus load balancing is generally much better than that
obtained using HID.

A high-level description can be found in Algorithm 3. At
the root level (l = 0), the graph associated with the matrix
|A| + |AT |, is partitioned into p subdomains with a p-way ver-
tex separator, resulting to p non-overlapping connected com-
ponents and their associated vertex separator. The multilevel
partitioner then proceeds to the next level, l = 1, and applies
the p-way vertex partitioner to the induced subgraph associated
with the vertex separator at level l = 0. This leads to a second
set of p non-overlapping connected components and a new, al-
beit smaller vertex separator. The p-way vertex partitioner is
then applied again to the induced subgraph associated with the
vertex separator obtained at level l = 1, etc. The procedure con-
tinues until either level lev − 1 is reached, or the vertex separator
at the current level l has so few vertices that it can not be further
partitioned into p non-overlapping partitions.

An illustration of a three-level, four-way partitioner applied
to a three-dimensional algebraic domain (a unit cube) is shown
in Figure 1. The leftmost subﬁgure shows the p = 4 sepa-
rate partitions obtained by the application of the four-way ver-
tex partitioner as well as the vertex separator itself (shown in
white color) at level l = 0. This vertex separator, which con-
sists of four two-dimensional faces, forms the algebraic object
to be partitioned at level l = 1, and the partitioning is shown in
the middle subﬁgure, where this time the vertex separator is a
one-dimensional object. Finally, at level l = 2, the most recent
vertex separator is further partitioned into four independent par-
titions, leading to a new vertex separator which consists of only
three vertices; see the rightmost subﬁgure.

In addition to the above illustration, Figure 2 plots the spar-
sity pattern of a Finite Diﬀerence discretization of the Laplace
operator on a three-dimensional domain, after reordering its
rows and columns according to a p-way, multilevel reordering
with lev = 4 and p = 4 (left). A zoom-in of the submatrix
associated with the permutation of the vertex separators is also
shown (right). Note that in this particular example, the last level
has already too few variables to be partitioned any further. In
addition to the global, multilevel permutation, each matrix Bl
can be further permuted locally by a reordering scheme such as
reverse Cuthill-Mckee (RCM) algorithm or approximate mini-
mal degree algorithm (AMD) [64, 65] to reduce the ﬁll-ins.

4. Implementation details of parGeMSLR

The parGeMSLR library consists of three main modules: a)
a distributed-memory reordering scheme, b) a Krylov subspace

3Nonetheless, HID is oﬀered in parGeMSLR.

5

Figure 1: Left: a three-dimensional domain partitioned into p = 4 subdomains. The vertex separator consists of four faces, with each face located between
neighboring subdomains. Center: partitioning of the root-level separator into p = 4 subdomains. Right: partitioning of the vertex separator at the second level.

Figure 2: Left: global permutation of matrix A following a multilevel partitioning with lev = 4 and p = 4. Right: zoom-in at the submatrix associated with the
permutation of the vertex separators (right-bottom submatrix of the left subﬁgure).

iterative accelerator, and c) the setup and application of the
GeMSLR preconditioner. The ﬁrst module was described in
greater detail in Section 3, and is implemented through a distributed-
memory partitioner such as ParMETIS. Additional point-to-point
communication between neighboring partitions, as well as a
single All-to-All message are required (to ﬁnd the new neigh-
bors of each partition post-partitioning). Next, we focus on
the implementation of the other two modules in a distributed-
memory environment where diﬀerent processor groups commu-
nicate via MPI.

4.1. Distributed-memory operations in Krylov accelerators

Standard, non-preconditioned Krylov iterative methods are
built on top of simple linear algebraic operations such as matrix-
vector multiplication, vector scaling and additions, and DOT
products. Iterative solvers such as GMRES or FGMRES also
require the solution of small-scale ordinary linear-least squares
problems which are typically solved redundantly in each MPI
process.

Assuming that the data associated with the system of lin-
ear algebraic equations we wish to solve is already distributed
across the diﬀerent MPI process via 1D row distribution, AXPY
operations can be executed locally and involve no communica-
tion overhead. On the other hand, sparse matrix-vector mul-
tiplications and DOT products involve either point-to-point or
collective communication. In particular, assume np ∈ N MPI
processes. A DOT product then requires a collective operation,
i.e., MPI Allreduce, to sum the np local DOT products. The
cost of this operation is roughly O(log(np)α), where α ∈ R
denotes the maximum latency between two MPI process. On
the other hand, a matrix-vector multiplication with the coeﬃ-
cient matrix of the linear system requires point-to-point com-
munication, where the local matrix-vector product in each MPI
process consists of operations using local data, as well as data
associated with MPI processes which are assigned to neighbor-
ing subdomains, e.g., see [66] for additional details and recent
advances.

6

0100020003000400001000200030004000020040060080002004006008004.2. Preconditioner setup and application

The main module of parGeMSLR is the setup of the GeM-
SLR preconditioner, followed by the application of the latter at
each iteration of the Krylov subspace iterative solver of choice.
Following a multilevel partition into lev levels (see Section 3),
the setup phase of the GeMSLR preconditioner associated with
each level l = 0, 1, . . . , lev − 1, is further divided into two sep-
arate submodules: a) computation of an ILUT factorization
Bl ≈ LlUl, and b) computation of an approximate rank-k Schur
decomposition of the matrix (cid:98)Gl = ET

l U−1

l L−1

l FlC−1

Let us consider each one of the above two tasks separately.
Recall that the data matrix at each level 0 ≤ l ≤ lev − 1 has the
following pattern

.

l

Al = PlCl−1Pl =

(cid:34)

Bl Fl
El Cl

(cid:35)

=





B(1)
l

B(2)
l

E(1)
l

E(2)
l

. . .

B(p)
l
· · · E(p)

l





F(1)
l
F(2)
l
...
F(p)
l
Cl

.

Now, without loss of generality, assume that each partition is as-
signed to a separate MPI process. Figure 3 (left) plots a graph-
ical illustration of the data layout of matrix Al obtained by a
permutation using p = 4, across four diﬀerent MPI processes.
Data associated with separate MPI processes are presented with
a diﬀerent color. Notice that the right-bottom submatrix denotes
the matrix Cl representing the coupling between variables of the
vertex separator at level l. Computing an ILUT factorization of
the matrix Bl decouples into p independent ILUT subproblems
l ≈ L( j)
B( j)
j = 1, . . . , p, and thus no communication over-
head is enabled. On the other hand, the computation of the low-
rank correction term requires the application of several steps of
the Arnoldi iteration, and requires communication overhead.

l U( j)

,

l

More speciﬁcally, the Arnoldi iteration requires communi-
cation among the various MPI processes to compute matrix-
vector multiplications with the iteration matrix (cid:98)Gl, as well as to
maintain orthogonality of the Krylov basis. When the latter is
achieved by means of standard Gram-Schmidt, Arnoldi requires
one MPI Allreduce operation at each iteration. Similarly, the
matrix-vector multiplication between (cid:98)Gl and a vector z is equal
to

(cid:104)

E(1)
l

· · · E(p)

l

l U(1)
L(1)

l

. . .

(cid:105)





l U(p)
L(p)

l







F(1)
l
...
F(p)
l

C−1

l z.




−1 

The computation of the product C−1
l z requires access to the in-
complete ILUT factorizations and rank-k correction terms asso-
ciated with all levels l < (cid:98)l ≤ lev − 1. Therefore, the rank-k cor-
rection terms are built in a bottom-up fashion, from l = lev −1 to
l = 0, so that level l has immediate access to the data associated
with all levels(cid:98)l > l. Once the matrix-vector multiplication C−1
l z
is computed, the matrix-vector multiplication with matrix Fl
is computed with trivial parallelism among the MPI processes,
and the same holds for the linear system solutions with matri-
ces L( j)
j = 1, . . . , p. Finally, the matrix-vector multipli-
l
cation with matrix El requires an MPI Allreduce operation.

, U( j)
l

,

Note though that if we were to replace vertex separators with
edge separators (this option is included in parGeMSLR) then the
latter multiplication would also be communication-free.

Finally, applying the preconditioner requires embarrassingly
parallel triangular substitutions with the ILUT factorizations of
the block-diagonal matrices Bl as well as dense matrix-vector
l,k, and (I − (cid:98)Rl,k)−1. A
multiplications with matrices (cid:98)Wl,k, (cid:98)W H
matrix-vector multiplication with the matrix (cid:98)Wl,k requires no
communication among the MPI processes, while a matrix-vector
multiplication with the matrix (cid:98)W H
l,k requires an MPI Allreduce
operation at level l. Finally, the matrix-vector multiplication
with the k × k matrix (I − (cid:98)Rl,k)−1 is performed redundantly in
each MPI process since k is typically pretty small.

4.2.1. Communication overhead analysis

In this section we focus on the communication overhead
associated with setting up and applying the preconditioner im-
plemented in parGeMSLR. For simplicity, we assume that the
number of MPI processes np is equal to the number of partitions
p at each level. The main parameters of the preconditioner are
the number of levels lev and the value of rank k.

l+2, W H

l+1, W H

l+1,k, C−1

Let us ﬁrst consider the application of m Arnoldi iterations
to compute the matrices (cid:98)Wl,k and (I − (cid:98)Rl,k)−1 for some 0 ≤ l ≤
lev − 1. As was discussed in the previous section, computing
matrix-vector products with the matrix (cid:98)Gl requires communi-
cation only during the application of the matrices El and C−1
.
l
In turn, the latter requires computations with the distributed ma-
trices C−1
l+2,k, and so on, until we reach level
lev − 1 where an ILUT of the matrix Clev−1 is computed explic-
itly. Thus, a matrix-vector multiplication with the matrix (cid:98)Gl
requires lev−(l+1) (low-rank correction term) and lev−l (C−1
re-
l
cursion) MPI Allreduce operations. In summary, an m-length
Arnoldi cycle with standard Gram-Schmidt orthonormalization
requires (2lev − 2l + 1)m MPI Allreduce operations, where we
also accounted for the two MPI Allreduce operations stem-
ming by Gram-Schmidt and vector normalization at each iter-
ation. This communication overhead is inversely proportional
to the level index l. Accounting for all lev − 1 levels, the to-
tal communication overhead associated with the setup phase of
the preconditioner amounts is bounded by δ(k) (cid:80)l=lev−1
(2lev −
2l + 1)m MPI Allreduce operations, where δ(k) ∈ N denotes
the maximum number of cycles performed by Arnoldi at any
level. In parGeMSLR, the default cycle length is m = 2k itera-
tions. Finally, after the set up phase, one full application of the
preconditioner implemented in the parGeMSLR library requires
2(lev − l) + 1 MPI Allreduce operations.

l=0

The analysis presented in this section demonstrates that the
communication overhead associated with the construction of
the GeMSLR preconditioner is directly proportional to an in-
crease in the value of lev. On the other hand, increasing the
value of lev can reduce the computational complexity associ-
ated with setting up the GeMSLR preconditioner in lower lev-
els. Nonetheless, the value of lev can not be too large, espe-
cially when the value of p is large, since the size of the vertex
separator reduces dramatically between successive levels (as is
demonstrated in Figure 1).

7

Figure 3: Left: layout of the matrix correction term across four MPI processes (same for any level 0 ≤ l ≤ lev − 1). Right: layout of a rank-k correction term across
four MPI processes (same for any level 0 ≤ l < lev − 1).

4.3. Applying C−1

lev−1

Due to partitioning with a multilevel vertex separator, the
matrix Clev−1 forms a separate partition which is replicated among
all MPI processes. Therefore, the simplest approach to apply
C−1
lev−1 is to do so approximately, through computing an ILUT
redundantly in each MPI process. However, for large problems,
this approach can quickly become impractical, even if a shared-
memory variant of ILUT is considered [15]. On the other hand,
applying a distributed-memory approach that requires commu-
nication among the MPI processes can lead to high communica-
tion overhead since the application of C−1
lev−1 is the most common
operation during the setup phase of the preconditioner.

parGeMSLR includes several4 options to apply an approxi-
mation of C−1
lev−1. The default option considered throughout our
experiments is to apply C−1
lev−1 approximately through a block-
Jacobi approach where Clev−1 is ﬁrst permuted by reverse RCM
and then replaced by its on-diagonal block submatrices while
the rest of the entries are discarded. Generally speaking, drop-
lev−1 has minor eﬀects since Clev−1 is al-
ping these entries of C−1
ready close to being block-diagonal for modest values of lev
(e.g., three or four) as was already demonstrated in Figure 1.
By default, the number of retained on-diagonal blocks of ma-
trix Clev−1 is set equal to p. The approximate application of
C−1
lev−1 is then trivially parallel among the MPI processes, and
each one of the retained on-diagonal blocks is applied through
ILUT.

5. Numerical Experiments

In this section we demonstrate the parallel performance of
parGeMSLR. We run our experiments on the Quartz cluster
of Lawrence Livermore National Laboratory. Each node of
Quartz has 128 GB memory and consists of 2 Intel Xeon E5-
2695 CPUs with 36 cores in total. We use MVAPICH2 2.2.3, to
compile parGeMSLR is compiled with MVAPICH2 2.2.3, fol-
lowing rank-to-core binding. By default, all of the experiments

4See section 2.1 in https://github.com/Hitenze/pargemslr/blob/

main/ParGeMSLR/DOCS/Documentation.pdf

presented below are executed in double-precision.5 On top of
distributed-memory parallelism, parGeMSLR can take advan-
tage of shared memory parallelism using either OpenMP or
CUDA. The current version of parGeMSLR uses LAPACK for
sequential matrix decompositions and ParMETIS for distributed
graph partitioning [59]. A detailed documentation of parGeMSLR
can be found in the “DOCS” directory of https://github.
com/Hitenze/pargemslr. This documentation provides de-
tailed information on how to compile and run parGeMSLR, and
includes a detailed description of all command-line parameters
as well as visualization of the source code hierarchy. Several
test drivers, and a sample input ﬁle, are also included.

Throughout the rest of this section, we choose Flexible GM-
RES (FGMRES) with a ﬁxed restart size of ﬁfty as the outer
iterative solver. The motivation for using FGMRES instead of
GMRES is that the application of the preconditioner is subject
to variations due to the application of the inner solver in step 9
of Algorithm 2. The stopping tolerance for the relative residual
norm in FGMRES is set equal to 1.0e − 6. Unless mentioned
otherwise, the solution of the linear system Ax = b will be equal
to the vector of all ones with an initial approximation equal to
zero. The low-rank correction term at each level consists of
approximate Schur vectors such that the corresponding approx-
imate eigenvalues are accurate to two digits of accuracy, and
the restart cycle of thick-restart Arnoldi is equal to 2k.

Our distributed-memory experiments focus on the parallel
eﬃciency of parGeMSLR both when the problem size remains
ﬁxed and np increases (strong scaling) and the problem size in-
creases at the same rate with np. In the case of weak scaling,
the parallel eﬃciency is equal to T1
, where T1 and Tnp denote
Tnp
the wall-clock time achieved by the sequential and distributed-
memory version (using np MPI processes) of parGeMSLR, re-
spectively. Likewise, in the case of strong scaling, the paral-
lel eﬃciency is equal to T1
. In addition, we also compare
npTnp
parGeMSLR against: a) the BoomerAMG parallel implemen-
tation of the algebraic multigrid method in hypre, and b) the
two-level SchurILU approach in [22]. The latter preconditioner
uses partial ILU to form an approximation of the Schur com-

5We note though that parGeMSLR supports both real and complex arith-

metic, as well as both single and double precision.

8

Distribution of matrix Alacross four MPI processesMPIprocess#1MPIprocess#2MPIprocess#3MPIprocess#4Vertex separatorDistribution of low-rank correction terms across four MPI processesReplicatedonallMPIprocessesMPIprocess#1MPIprocess#2MPIprocess#3MPIprocess#4plement matrix. The preconditioning step is then performed by
applying GMRES with block-Jacobi preconditioning to solve
the linear system associated with the sparsiﬁed Schur comple-
ment. The block-Jacobi preconditioner is applied through one
step of ILUT, and our implementation of SchurILU is based on
the parallel ILU(T) in hypre.

Throughout the rest of this section, we adopt the following

notation:

• np ∈ N: total number of MPI processes.
• ﬁll ∈ R: ratio between the number of non-zero entries of

the preconditioner and that of matrix A.

• p-t ∈ R: preconditioner setup time. This includes the
time required to compute the ILUT factorizations and
low-rank correction terms in parGeMSLR.

• i-t ∈ R: iteration time of FGMRES.
• its ∈ N: total number of FGMRES iterations.
• k ∈ N: number of low-rank correction terms at each

level.

• F: ﬂag signaling that FGMRES failed to converge within

1000 iterations.

5.1. A Model Problem

This section considers a Finite Diﬀerence discretization of

the model problem

−∆u − cu = f

in Ω,

u = 0 on ∂Ω.

We consider a 7-pt stencil and set Ω = (0, 1)3.

(10)

5.1.1. Weak scaling

Our ﬁrst set of experiments studies the weak scaling eﬃ-
ciency of parGeMSLR. Since varying the values of lev and k lead
to diﬀerent convergence rates, we ﬁrst consider the case where
the number of FGMRES iterations is set equal to thirty, regard-
less of whether convergence was achieved or not. The problem
size on each MPI process is ﬁxed to 503, while the number of
subdomains at each level is set equal to 8 × np. Moreover, the
number of levels is varied as lev ∈ {2, 3} while the rank of the
low-rank correction terms is varied as k ∈ {0, 100, 200}.

Figure 4 plots the weak scaling eﬃciency of parGeMSLR on
up to np = 1, 024 MPI processes. The achieved eﬃciency is
similar for both options of lev with a slightly higher eﬃciency
observed for the case lev = 3. As expected, the highest eﬃ-
ciency achieved during the preconditioner setup phase was for
the case k = 0, since there is no communication overhead stem-
ming from the low-rank correction terms. Nonetheless, even
in this case there is some loss in eﬃciency due to load imbal-
ancing introduced by the ILUT factorizations at diﬀerent levels.
Regardless of the value of k, the eﬃciency of parGeMSLR drops
the most when the number of MPI processes is small, regardless
of the value of lev. This reduction is owed to the relatively large
increase on the size of the local Schur complement versus when
a larger number of MPI processes is utilized. Note though,
although not reported in our experiments, that the weak scal-
ing eﬃciency is typically much higher when each MPI process

Figure 4: Weak scaling of parGeMSLR for the Poisson problem when the num-
ber of iterations performed by FGMRES is ﬁxed to thirty, and the number of
levels is set to lev = 2 and lev = 3. The number of unknowns on each MPI
process is 125, 000, for a maximum problem size n = 800 × 400 × 400.

handles exactly one subdomain. Finally, the eﬃciency of the
reordering phase is rather limited, since the wall-clock time re-
quires to partition the graph associated with the matrix |A|+|AT |
and permute the distributed matrix A increases as the problem
size grows.

Figure 5 plots the weak scalability of parGeMSLR and two-
level SchurILU, where this time we allow enough iterations in
FGMRES until convergence. As previously, we use eight sub-
domains per MPI process, but this time we ﬁx lev = 3 and k =
10. In summary, parGeMSLR is both faster and more scalable
than SchurILU during the solve phase. Moreover, parGeMSLR
also converges much faster than SchurILU, and the number of
total FGMRES iterations increases only marginally with the
problem size. On the other hand, the weak scaling of the pre-
conditioner setup phase of parGeMSLR is impacted negatively
as the problem size increases due to the need to perform more
Arnoldi iterations to compute the low-rank correction terms.

5.1.2. Strong scaling

We now present strong scaling results obtained by solving
(10) with parGeMSLR on a regular mesh of ﬁxed size as the

9

1248163264128256512102450100NumberofMPIprocessesEﬃciency(%)Eﬃciencyofthereorderingphase1248163264128256512102450100NumberofMPIprocessesEﬃciency(%)Eﬃciencyofthesetupphase1248163264128256512102450100NumberofMPIprocessesEﬃciency(%)Eﬃciencyofthesolvephaseperiterationlev=2,k=0lev=2,k=100lev=2,k=200lev=3,k=0lev=3,k=100lev=3,k=200Figure 5: Weak scaling of parGeMSLR and SchurILU on Poisson problems. The number of unknowns on each MPI process is 125, 000, for a maximum problem
size n = 800 × 400 × 400.

numbers of MPI processes varies. More speciﬁcally, the size
of the problem is ﬁxed to n = 3203 while the number of MPI
processes varies up to np = 1, 024. The values of lev and k are
varied as previously.

in terms of ﬂoating-point arithmetic operations as lev decreases,
thus although scalability deteriorates as lev increases, the actual
wall-clock time might actually decrease if the number of MPI
processes used is small.

5.2. General Problems

This section discusses the performance of parGeMSLR on a

variety of problems in engineering.

5.2.1. Unstructured Poisson problem on a crooked pipe

We consider the numerical solution of (10) where f = 1 and
c = 0 on a 3D crooked pipe mesh. The problem is discretized by
second-order Finite Elements using the MFEM library [67, 68]
with local uniform and parallel mesh reﬁnement. The initial
approximation of the solution is set equal to zero. We visual-
ize the (inhomogeneous) mesh using the package GLVis [69] in
Figure 7. Our experiments consider diﬀerent reﬁnement levels

Figure 6: Strong scaling results for Poisson problems of size n = 3203. The
number of subdomains is set equal to 2048 in all levels.

Figure 6 plots the strong scaling of parGeMSLR. In con-
trast to the weak scaling case, setting lev = 2 leads to higher
eﬃciency during both the setup and application phases of the
preconditioner. The reason for this behavior is twofold. First,
increasing the value of lev generally deteriorates the eﬀective-
ness of the preconditioner unless k is large and the threshold
used in the local ILUT factorizations is small. Second, de-
creasing the value of lev enhances strong scalability since it
leads to smaller communication overheads (i.e., recall the dis-
cussion in Section 4). As a general remark, we note that the
setup phase of parGeMSLR generally becomes more expensive

Figure 7: Left: Poisson problem on a crooked pipe mesh. Right: zoom-in of
the center part of the mesh.

to generate problems of diﬀerent sizes. Moreover, the max-
imum number of inner iterations in step 9 of Algorithm 2 is
varied between three and ﬁve. We compare parGeMSLR against
BoomerAMG with Hybrid Modiﬁed Independent Set (HMIS)
coarsening, where we consider both Gauss-Seidel and l1 Ja-
cobi smoother [70], and report the corresponding results in Ta-
ble 1. parGeMSLR is able to outperform Schur ILU, especially
for larger problems. Moreover, the iteration time of parGeMSLR
is similar to that of BoomerAMG with Gauss-Seidel smoother,
but much lower than that of BoomerAMG with l1 Jacobi smoother.

10

1248163264128256512102410−210−1100101102NumberofMPIprocessesTime(s)050100150IterationNumber1248163264128256512102410−210−1100101102NumberofMPIprocessesTime(s)050100150IterationNumberpargemslr-reordering+setuppargemslr-solvepargemslr-totalpargemslr-itsschurilu-reordering+setupschurilu-solveschurilu-totalschurilu-its326412825651210246080100NumberofMPIprocessesEﬃciency(%)Eﬃciencyofthesetupphase326412825651210246080100NumberofMPIprocessesEﬃciency(%)Eﬃciencyofthepreconditioningaloneperiterationlev=2,k=0lev=2,k=50lev=2,k=100lev=3,k=0lev=3,k=50lev=3,k=100Table 1: Solving (10) on a crooked pipe mesh.

Table 2: Comparison between two-level ILU and the GeMSLR for 3D Linear
elasticity problem. µ = 1 and λ = 10, Poisson ratio is 5

11 ≈ 0.455.

prec
Boomer
AMG
GS

Boomer
AMG
Jacobi

Schur
ILU

par
GeMSLR

i-t

p-t

size

np k ﬁll

its
126,805 16 - 1.71 0.17 0.69 106
5.7 198
966,609 32 - 1.79 0.79
7,544,257 64 - 1.81 3.36 45.12 250
126,805 16 - 1.71 0.18 1.29 226
966,609 32 - 1.79 0.8 10.95 431
7,544,257 64 - 1.81 3.39 72.1 568
126,805 16 - 1.53 0.22 0.51 65
966,609 32 - 1.86 1.2 12.46 383
7,544,257 64 - 1.94 5.51
F
126,805 16 10 1.05 0.54 0.46 25
966,609 32 10 1.18 3.59 4.70 53
7,544,257 64 10 1.32 11.76 48.35 128

-

prec

Schur-
ILU

par
GeMSLR

i-t

ﬁll

np k
its
p-t
size
4 - 2.62 0.03 0.06 49
2,475
15,795 8 - 3.78 0.32 0.60 238
111,843 16 - 7.81 4.80 19.05 751
F
839,619 64 - 11.82 19.67
4 20 1.94 0.12 0.01 18
2,475
15,795 8 40 3.58 0.92 0.04 23
111,843 16 40 7.86 10.06 0.64 41
839,619 64 80 10.05 63.25 3.13 65

-

Table 3: Comparison between two-level ILU and the GeMSLR for 3D Linear
elasticity problem. µ = 1 and λ = 80, Poisson ratio is 40

81 ≈ 0.494.

5.2.2. Linear elasticity equation

In the section we consider the solution of the following lin-

ear elasticity equation:

µ∆u + (λ + µ)∇(∇ · u) = f

in Ω,

(11)

where Ω is a 3D cantilever beam as shown in Figure 8. The left

prec

Schur-
ILU

par
GeMSLR

i-t

p-t

np k
ﬁll
its
size
2.21 0.03 0.26 336
-
4
2,475
4.03 0.35 1.48 549
15,795 8
-
F
111,843 16 -
8.94 6.45
F
839,619 64 - 14.75 32.17
4 20 1.91 0.15 0.01 41
2,475
15,795 8 40 3.58 1.09 0.15 75
111,843 16 80 6.48 16.16 1.49 93
839,619 64 120 10.31 133.2 6.15 128

-
-

where we use the Perfectly Matched Layer (PML) boundary
condition [71] and set the number of points per wavelength
equal to eight. We used random initial guesses.

Our ﬁrst set of experiments focuses on the performance of
parGeMSLR where the number of low-rank terms is varied as
k = {10, 20, . . . , 100}, and the number of levels is set equal to
lev = 3. The size of the Helmholtz problem is set equal to
n = 503. The maximum ﬁll-in attributed to the low-rank cor-
rection term was roughly equal to three. Figure 9 plots the par-
allel wall-clock time as a function of the number of low-rank
terms k while the number of MPI processes is ﬁxed equal to
sixteen. Overall, larger values of k lead to lower total and iter-
ation times up to the point where the time increase associated
with constructing the parGeMSLR preconditioner outweighs the
gains from improving the convergence rate during the iterative
solution by FGMRES. Next, we consider the same problem but

Figure 9: Total and iteration wall-clock times of the 3-level parallel GeMSLR
to solve the Helmholtz equation of size n = 503 using 16 MPI processes.

Figure 8: Linear elasticity problem on a 3D beam.

end of the beam is ﬁxed, while a constant force (represented
by f ) pulls down the beam from the right end. Herein, u is the
displacement, while λ and µ are the material’s Lam˙e constants.
The initial approximation is again set equal to zero in order to
satisfy the boundary condition.

Tables 2 and 3 show a comparison between parGeMSLR and
SchurILU for diﬀerent uniform mesh reﬁnements obtained us-
ing ﬁrst-order Finite Element. For each mesh, the problem be-
comes more ill-conditioned as the ratio λ
µ grows larger. For
this reason, we ﬁx µ = 1 and vary λ = 10 and λ = 80. Note
that standard AMG converge slowly for this problem since it is
almost singular. Concisely, parGeMSLR leads to considerable
wall-clock time savings compared to SchurILU, even when the
latter is allowed a higher level of ﬁll-in.

5.2.3. Helmholtz equation

In this section we consider the complex version of parGeMSLR

and apply it to solve the Helmholtz problem

− (∆ + ω2)u = f

in Ω = [0, 1]3,

(12)

11

102030405060708090100101102RanksTime(s)TimewithdiﬀerentrankstotaltimesolvetimeTable 4: parGeMSLR with/without complex shifts for. The problem size is equal
to n = (4ω/π)3.

i-t

r-t

its ﬁll

with shift
p-t

without shift
ω np k ﬁll
its
time
5π
9
1 0 3.40 0.04 0.02 0.05 9 3.80 0.12
7.5π 1 0 3.81 0.17 0.10 0.40 20 4.76 6.47 241
10π 2 5 3.52 0.43 0.41 1.03 36 4.11 15.48 449
12.5π 4 5 3.79 0.70 0.58 1.50 42 4.79
F
15π 8 10 4.16 1.25 1.20 2.33 55 4.63
F
20π 16 10 4.40 1.51 1.29 3.51 57 4.77
F
40π 64 20 5.49 4.87 7.84 14.43 92 5.73
F

-
-
-
-

this time we add a complex shift equal to 0.05i ∗ (cid:80)
i |Aii|/nA dur-
ing the the ILU factorization of the on-diagonal blocks. The
same idea was already considered in [72, 24, 14] but this time
we apply it in the context of distributed-memory computing and
make it available in parGeMSLR. Similarly to the previous ref-
erences, adding a shift helps creating a more stable ILU for
indeﬁnite problems, i.e., see Table 4.

5.3. GPU acceleration of the solution phase

The parGeMSLR library can also take advantage of special-
ized hardware such as GPUs to speed-up numerical kernels.
The current release of parGeMSLR does not support GPU com-
puting during the setup phase of the GeMSLR preconditioner,
but allows the use of GPUs during the application of the GeM-
SLR preconditioner, i.e., triangular substitutions and dense, rect-
angular matrix-vector multiplications. Nonetheless, accelerat-
ing the solution phase might still lead to signiﬁcant reductions
in the overall wall-clock time, e.g., when we need to solve for
multiple right-hand sides.

To demonstrate these beneﬁts, we consider a n = 1283 dis-
cretization of the model problem (10) and focus on the speedup
achieved during the solution phase if GPUs are enabled. We set
the number of levels equal to lev = 2 and lev = 3, and vary the
low-rank correction terms as k ∈ {0, 100, 200, 300, 400, 500}.
At each level, we apply a 4-way partition and assign each parti-
tion to a separate MPI process binded to a V100 NVIDIA GPU.
Figure 10 plots the speedups achieved by the hybrid CPU+GPU
version of parGeMSLR during its solve phase. As expected, the
peak speedup is obtained for the case k = 500, since the cost to
apply the low-rank correction term increases linearly with the
value of k.

6. Concluding remarks and future work

In this paper we presented parGeMSLR, a C++ parallel soft-
ware library for the iterative solution of general sparse systems
distributed among several processor groups communicating via
MPI. environments [24]. parGeMSLR is based on the GeM-
SLR preconditioner and can be applied to both real and com-
plex systems of linear algebraic equations. The performance
of parGeMSLR on distributed-memory computing environments
was demonstrated on both model and real-world problems, ver-
ifying the eﬃciency of the library as a general-purpose solver.

12

Figure 10: Speedup of the solution phase
of parGeMSLR if GPU acceleration is enabled when
lev = {2, 3}, and k ∈ {0, 100, 200, 300, 400, 500}. The problem
size is equal to n = 1283.

As future work we plan to replace standard Arnoldi by ei-
ther its block variant or randomized subspace iteration. This
should improve performance by reducing latency during the
preconditioner setup phase. Moreover, the cost of the setup
phase can be amortized over the solution of linear systems with
multiple right-hand sides, e.g., see [73, 74, 75, 76], and we plan
to apply parGeMSLR to this type of problems. In this context,
we also plan to apply parGeMSLR to the solution of sparse lin-
ear systems appearing in eigenvalue solvers based on rational
ﬁltering [77, 78], and domain decomposition [79, 80].

References

[1] Y. Saad, Iterative Methods for Sparse Linear Systems, Other Titles in
Applied Mathematics, Society for Industrial and Applied Mathematics,
2003. doi:10.1137/1.9780898718003.

[2] H. A. Van der Vorst, Iterative Krylov methods for large linear systems,

no. 13, Cambridge University Press, 2003.

[3] Y. Saad, M. H. Schultz, GMRES: A Generalized Minimal Residual Al-
gorithm for Solving Nonsymmetric Linear Systems, SIAM Journal on
Scientiﬁc and Statistical Computing 7 (3) (1986) 856–869, publisher: So-
ciety for Industrial and Applied Mathematics. doi:10.1137/0907058.
[4] J. W. Ruge, K. St{\”u}ben, Algebraic Multigrid, in: Multigrid Methods,
Frontiers in Applied Mathematics, Society for Industrial and Applied
Mathematics, 1987, pp. 73–130. doi:10.1137/1.9781611971057.
ch4.

[5] V. E. Henson, U. M. Yang, BoomerAMG: A parallel algebraic multigrid
solver and preconditioner, Applied Numerical Mathematics 41 (1) (2002)
155–177. doi:10.1016/S0168-9274(01)00115-5.

[6] A. J. Cleary, R. D. Falgout, V. E. Henson, J. E. Jones, T. A. Manteuﬀel,
S. F. McCormick, G. N. Miranda, J. W. Ruge, Robustness and scalabil-
ity of algebraic multigrid, SIAM Journal on Scientiﬁc Computing 21 (5)
(2000) 1886–1908.

[7] N. Bell, S. Dalton, L. N. Olson, Exposing ﬁne-grained parallelism in al-
gebraic multigrid methods, SIAM Journal on Scientiﬁc Computing 34 (4)
(2012) C123–C152.

[8] Y. Saad, ILUT: A dual threshold incomplete LU factorization, Numerical
Linear Algebra with Applications 1 (4) (1994) 387–402. doi:10.1002/
nla.1680010405.

[9] E. Chow, Y. Saad, Experimental study of ilu preconditioners for indeﬁnite
matrices, Journal of computational and applied mathematics 86 (2) (1997)
387–414.

[10] O. G. Ernst, M. J. Gander, Why it is Diﬃcult to Solve Helmholtz Prob-
I. G. Graham, T. Y. Hou,
lems with Classical Iterative Methods, in:
O. Lakkis, R. Scheichl (Eds.), Numerical Analysis of Multiscale Prob-
lems, Vol. 83, Springer Berlin Heidelberg, Berlin, Heidelberg, 2012, pp.
325–363. doi:10.1007/978-3-642-22061-6_10.

01002003004005002.833.2RankSpeedupSpeedupwithdiﬀerentrankslev=2lev=3[11] X. Liu, Y. Xi, Y. Saad, M. V. de Hoop, Solving the three-dimensional
high-frequency helmholtz equation using contour integration and polyno-
mial preconditioning, SIAM Journal on Matrix Analysis and Applications
41 (1) (2020) 58–82.

[12] M. Magolu monga Made, R. Beauwens, G. Warz´ee, Preconditioning of
discrete Helmholtz operators perturbed by a diagonal complex matrix,
Communications in Numerical Methods in Engineering 16 (11) (2000)
801–817.
doi:https://doi.org/10.1002/1099-0887(200011)
16:11<801::AID-CNM377>3.0.CO;2-M.

[13] Y. A. Erlangga, C. Vuik, C. W. Oosterlee, Comparison of multigrid and
incomplete LU shifted-Laplace preconditioners for the inhomogeneous
Helmholtz equation, Applied Numerical Mathematics 56 (5) (2006) 648–
666. doi:10.1016/j.apnum.2005.04.039.

[14] D. Osei-Kuﬀuor, Y. Saad, Preconditioning Helmholtz linear systems, Ap-
plied Numerical Mathematics 60 (4) (2010) 420–431. doi:10.1016/j.
apnum.2009.09.003.

[15] H. Anzt, E. Chow, J. Dongarra, Parilut—a new parallel threshold ilu fac-
torization, SIAM Journal on Scientiﬁc Computing 40 (4) (2018) C503–
C519.

[16] H. Anzt, T. Ribizel, G. Flegar, E. Chow, J. Dongarra, Parilut-a paral-
lel threshold ilu for gpus, in: 2019 IEEE International Parallel and Dis-
tributed Processing Symposium (IPDPS), IEEE, 2019, pp. 231–241.
[17] E. Chow, A. Patel, Fine-Grained Parallel Incomplete LU Factorization,
SIAM Journal on Scientiﬁc Computing 37 (2) (2015) C169–C193. doi:
10.1137/140968896.

[18] X.-C. Cai, M. Sarkis, A Restricted Additive Schwarz Preconditioner for
General Sparse Linear Systems, SIAM Journal on Scientiﬁc Computing
21 (2) (1999) 792–797. doi:10.1137/S106482759732678X.

[19] D. Hysom, A. Pothen, Eﬃcient parallel computation of ILU(k) precon-
ditioners, in: Proceedings of the 1999 ACM/IEEE conference on Super-
computing, SC ’99, Association for Computing Machinery, New York,
NY, USA, 1999, pp. 29–es. doi:10.1145/331532.331561.

[20] G. Karypis, V. Kumar, Parallel Threshold-based ILU Factorization, in:
Supercomputing, ACM/IEEE 1997 Conference, 1997, pp. 28–28. doi:
10.1145/509593.509621.

[21] Y. Saad, J. Zhang, BILUTM: A Domain-Based Multilevel Block ILUT
Preconditioner for General Sparse Matrices, SIAM Journal on Ma-
trix Analysis and Applications 21 (1) (1999) 279–299, publisher:
Society for Industrial and Applied Mathematics.
doi:10.1137/
S0895479898341268.

[22] Z. Li, Y. Saad, M. Sosonkina, pARMS: a parallel version of the algebraic
recursive multilevel solver, Numerical Linear Algebra with Applications
10 (5-6) (2003) 485–509. doi:10.1002/nla.325.

[23] I. C. L. NIEVINSKI, M. SOUZA, P. GOLDFELD, D. A. AUGUSTO,
J. R. P. RODRIGUES, L. M. CARVALHO, Parallel Implementation of
a Two-level Algebraic ILU(k)-based Domain Decomposition Precondi-
tioner, TEMA (S ˜A£o Carlos) 19 (2018) 59–77, publisher: scielo. doi:
10.5540/tema.2018.019.01.0059.

[24] G. Dillon, V. Kalantzis, Y. Xi, Y. Saad, A Hierarchical Low Rank Schur
Complement Preconditioner for Indeﬁnite Linear Systems, SIAM Journal
on Scientiﬁc Computing 40 (4) (2018) A2234–A2252. doi:10.1137/
17M1143320.

[25] J. Mandel, C. R. Dohrmann, Convergence of a balancing domain decom-
position by constraints and energy minimization, Numerical linear alge-
bra with applications 10 (7) (2003) 639–659.

[26] C. Farhat, M. Lesoinne, P. LeTallec, K. Pierson, D. Rixen, Feti-dp: a
dual–primal uniﬁed feti method—part i: A faster alternative to the two-
level feti method, International journal for numerical methods in engi-
neering 50 (7) (2001) 1523–1544.

[27] A. Heinlein, A. Klawonn, M. Lanser, J. Weber, Combining machine
learning and adaptive coarse spaces—a hybrid approach for robust feti-
dp methods in three dimensions, SIAM Journal on Scientiﬁc Computing
43 (5) (2021) S816–S838.

[28] N. Spillane, V. Dolean, P. Hauret, F. Nataf, C. Pechstein, R. Scheichl, Ab-
stract robust coarse spaces for systems of pdes via generalized eigenprob-
lems in the overlaps, Numerische Mathematik 126 (4) (2014) 741–770.

[29] P. R. Amestoy, I. S. Duﬀ, J.-Y. L’Excellent, J. Koster, Mumps: a general
purpose distributed memory sparse solver, in: International Workshop on
Applied Parallel Computing, Springer, 2000, pp. 121–130.

[30] P. H´enon, P. Ramet, J. Roman, Pastix: a high-performance parallel direct
solver for sparse symmetric positive deﬁnite systems, Parallel Computing

28 (2) (2002) 301–321.

[31] R. Li, Y. Saad, Low-Rank Correction Methods for Algebraic Domain De-
composition Preconditioners, SIAM Journal on Matrix Analysis and Ap-
plications 38 (3) (2017) 807–828. doi:10.1137/16M110486X.

[32] E. G. Boman, L. Cambier, C. Chen, E. Darve, S. Rajamanickam, R. S.
Tuminaro, A preconditioner based on sparsiﬁed nested dissection and
low-rank approximation, in: XXI Householder Symposium on Numeri-
cal Linear Algebra, 2020, p. 128.

[33] M. Benzi, M. Tuma, A Sparse Approximate Inverse Preconditioner for
Nonsymmetric Linear Systems, SIAM Journal on Scientiﬁc Computing
19 (3) (1998) 968–994. doi:10.1137/S1064827595294691.

[34] E. Chow, Y. Saad, Approximate Inverse Preconditioners via Sparse-
Sparse Iterations, SIAM Journal on Scientiﬁc Computing 19 (3) (1998)
995–1023. doi:10.1137/S1064827594270415.

[35] C. Janna, M. Ferronato, G. Gambolati, A Block FSAI-ILU Parallel Pre-
conditioner for Symmetric Positive Deﬁnite Linear Systems, SIAM Jour-
nal on Scientiﬁc Computing 32 (5) (2010) 2468–2484, publisher: Society
for Industrial and Applied Mathematics. doi:10.1137/090779760.
[36] H. Anzt, T. K. Huckle, J. Br¨ackle, J. Dongarra, Incomplete sparse approx-
imate inverses for parallel preconditioning, Parallel Computing 71 (2018)
1–22.

[37] M. J. Grote, T. Huckle, Parallel preconditioning with sparse approximate
inverses, SIAM Journal on Scientiﬁc Computing 18 (3) (1997) 838–853.
[38] X. Ye, Y. Xi, Y. Saad, Preconditioning via gmres in polynomial space

(2019).

[39] D. Cai, E. Chow, L. Erlandson, Y. Saad, Y. Xi, SMASH: Structured matrix
approximation by separation and hierarchy, Numerical Linear Algebra
with Applications 25 (6) (2018) e2204. doi:https://doi.org/10.
1002/nla.2204.

[40] W. Hackbusch, A Sparse Matrix Arithmetic Based on $\Cal H$-Matrices.
Part I: Introduction to ${\Cal H}$-Matrices, Computing 62 (2) (1999) 89–
108. doi:10.1007/s006070050015.

[41] W. Hackbusch, B. N. Khoromskij, A Sparse $\Cal H$-Matrix Arithmetic.
Part II: Application to Multi-Dimensional Problems, Computing 64 (1)
(2000) 21–47. doi:10.1007/PL00021408.

[42] Y. Xi, J. Xia, S. Cauley, V. Balakrishnan, Superfast and Stable Struc-
tured Solvers for Toeplitz Least Squares via Randomized Sampling,
SIAM Journal on Matrix Analysis and Applications 35 (1) (2014) 44–
72, publisher: Society for Industrial and Applied Mathematics. doi:
10.1137/120895755.

[43] C. Chen, H. Pouransari, S. Rajamanickam, E. G. Boman, E. Darve, A
distributed-memory hierarchical solver for general sparse linear systems,
Parallel Computing 74 (2018) 49–64.

[44] R. Li, Y. Xi, Y. Saad, Schur complement-based domain decomposition
preconditioners with low-rank corrections, Numerical Linear Algebra
with Applications 23 (4) (2016) 706–729. doi:10.1002/nla.2051.

[45] R. D. Falgout, U. M. Yang, hypre: A Library of High Performance Pre-
conditioners, in: P. M. A. Sloot, A. G. Hoekstra, C. J. K. Tan, J. J.
Dongarra (Eds.), Computational Science ICCS 2002, Lecture Notes in
Computer Science, Springer, Berlin, Heidelberg, 2002, pp. 632–641.
doi:10.1007/3-540-47789-6_66.

[46] P. Labs, Paralution v1.1.0, http://www.paralution.com/ (2016).
[47] K. Rupp, P. Tillet, F. Rudolf, J. Weinbub, A. Morhammer, T. Grasser,
A. Jungel, S. Selberherr, Viennacl—linear algebra library for multi-and
many-core architectures, SIAM Journal on Scientiﬁc Computing 38 (5)
(2016) S412–S439.

[48] S. Gawlok, P. Gerstner, S. Haupt, V. Heuveline, J. Kratzke, P. L¨osel,
K. Mang, M. Schmidtobreick, N. Schoch, N. Schween, J. Schwegler,
C. Song, M. Wlotzka, Hiﬂow3 – technical report on release 2.0, Preprint
Series of the Engineering Mathematics and Computing Lab (EMCL)
0 (06) (2017). doi:10.11588/emclpp.2017.06.42879.

[49] S. Balay, K. Buschelman, W. D. Gropp, D. Kaushik, M. G. Knepley,
L. C. McInnes, B. F. Smith, H. Zhang, Petsc, See http://www. mcs. anl.
gov/petsc (2001).

[50] T. Trilinos Project Team, The Trilinos Project Website.
[51] P. Ghysels, S. L. Xiaoye, C. Gorman, F.-H. Rouet, A robust parallel pre-
conditioner for indeﬁnite systems using hierarchical matrices and ran-
domized sampling, in: 2017 IEEE International Parallel and Distributed
Processing Symposium (IPDPS), IEEE, 2017, pp. 897–906.

[52] P. Ghysels, X. S. Li, F.-H. Rouet, S. Williams, A. Napov, An eﬃcient
multicore implementation of a novel hss-structured multifrontal solver us-

13

ing randomized sampling, SIAM Journal on Scientiﬁc Computing 38 (5)
(2016) S358–S384.

uncertainty quantiﬁcation by solving linear systems with multiple right-
hand sides, Numerical Algorithms 62 (4) (2013) 637–653.

[76] V. Kalantzis, A. C. I. Malossi, C. Bekas, A. Curioni, E. Gallopoulos,
Y. Saad, A scalable iterative dense linear system solver for multiple right-
hand sides in data analytics, Parallel Computing 74 (2018) 136–153.
[77] V. Kalantzis, Y. Xi, L. Horesh, Fast randomized non-hermitian eigen-
solvers based on rational ﬁltering and matrix partitioning, SIAM Jour-
nal on Scientiﬁc Computing 43 (5) (2021) S791–S815. arXiv:https:
//doi.org/10.1137/20M1349217, doi:10.1137/20M1349217.
[78] Y. Xi, Y. Saad, Computing Partial Spectra with Least-Squares Rational
Filters, SIAM Journal on Scientiﬁc Computing 38 (5) (2016) A3020–
A3045. doi:10.1137/16M1061965.

[79] V. Kalantzis, Y. Xi, Y. Saad, Beyond automated multilevel substructuring:
Domain decomposition with rational ﬁltering, SIAM Journal on Scientiﬁc
Computing 40 (4) (2018) C477–C502. arXiv:https://doi.org/10.
1137/17M1154527, doi:10.1137/17M1154527.

[80] V. Kalantzis, A domain decomposition rayleigh–ritz algorithm for sym-
metric generalized eigenvalue problems, SIAM Journal on Scientiﬁc
Computing 42 (6) (2020) C410–C435.

[53] F.-H. Rouet, X. S. Li, P. Ghysels, A. Napov, A distributed-memory pack-
age for dense hierarchically semi-separable matrix computations using
randomization, ACM Transactions on Mathematical Software (TOMS)
42 (4) (2016) 1–35.

[54] X. S. Li, J. W. Demmel, SuperLU DIST: A scalable distributed-memory
sparse direct solver for unsymmetric linear systems, ACM Trans. Mathe-
matical Software 29 (2) (2003) 110–140.

[55] L. Grigori, F. Nataf, S. Yousef, Robust algebraic Schur complement pre-
conditioners based on low rank corrections, Research Report RR-8557,
INRIA (Jul. 2014).

[56] Y. Xi, R. Li, Y. Saad, An Algebraic Multilevel Preconditioner with Low-
Rank Corrections for Sparse Symmetric Matrices, SIAM Journal on Ma-
trix Analysis and Applications 37 (1) (2016) 235–259. doi:10.1137/
15M1021830.

[57] S. Rajamanickam, E. G. Boman, M. A. Heroux, Shylu: A hybrid-hybrid
solver for multicore platforms, in: 2012 IEEE 26th International Parallel
and Distributed Processing Symposium, IEEE, 2012, pp. 631–643.
[58] H. A. Daas, T. Rees, J. Scott, Two-level nystr\” om–schur precondi-
tioner for sparse symmetric positive deﬁnite matrices, arXiv preprint
arXiv:2101.12164 (2021).

[59] G. Karypis, V. Kumar, A Fast and High Quality Multilevel Scheme for
Partitioning Irregular Graphs, SIAM Journal on Scientiﬁc Computing
20 (1) (1998) 359–392, publisher: Society for Industrial and Applied
Mathematics. doi:10.1137/S1064827595287997.

[60] U. V. Catalyurek, C. Aykanat, Hypergraph-partitioning-based decom-
position for parallel sparse-matrix vector multiplication, IEEE Transac-
tions on Parallel and Distributed Systems 10 (7) (1999) 673–693. doi:
10.1109/71.780863.

[61] B. Hendrickson, R. Leland, The Chaco User’s Guide Version 2, Sandia

National Laboratories, Albuquerque NM (1994).

[62] F. Pellegrini, Scotch and libScotch 5.1 User’s Guide, INRIA Bordeaux

Sud-Ouest, IPB & LaBRI, UMR CNRS 5800 (2010).

[63] P. H´enon, Y. Saad, A parallel multistage ilu factorization based on a hi-
erarchical graph decomposition, SIAM Journal on Scientiﬁc Computing
28 (6) (2006) 2266–2293.

[64] P. R. Amestoy, T. A. Davis, I. S. Duﬀ, An Approximate Minimum De-
gree Ordering Algorithm, SIAM Journal on Matrix Analysis and Appli-
cations 17 (4) (1996) 886–905, publisher: Society for Industrial and Ap-
plied Mathematics. doi:10.1137/S0895479894278952.

[65] A. George, J. W. Liu, Computer Solution of Large Sparse Positive Deﬁ-

nite, Prentice Hall Professional Technical Reference, 1981.

[66] A. Bienz, W. D. Gropp, L. N. Olson, Node aware sparse matrix–vector
multiplication, Journal of Parallel and Distributed Computing 130 (2019)
166–178.

[67] R. Anderson, J. Andrej, A. Barker, J. Bramwell, J.-S. Camier, J. C. V.
Dobrev, Y. Dudouit, A. Fisher, T. Kolev, W. Pazner, M. Stowell, V. To-
mov, I. Akkerman, J. Dahm, D. Medina, S. Zampini, MFEM: A modu-
lar ﬁnite element library, Computers & Mathematics with Applications
(2020). doi:10.1016/j.camwa.2020.06.009.

[68] MFEM: Modular ﬁnite element methods [Software], mfem.org. doi:

10.11578/dc.20171025.1248.

[69] GLVis: Opengl ﬁnite element visualization tool, glvis.org. doi:10.

11578/dc.20171025.1249.

[70] A. H. Baker, R. D. Falgout, T. V. Kolev, U. M. Yang, Multigrid Smoothers
for Ultraparallel Computing, SIAM Journal on Scientiﬁc Computing
33 (5) (2011) 2864–2887, publisher: Society for Industrial and Applied
Mathematics. doi:10.1137/100798806.

[71] X. Liu, Y. Xi, Y. Saad, M. V. de Hoop, Solving the 3d high-frequency
helmholtz equation using contour integration and polynomial precondi-
tioning, arXiv preprint arXiv:1811.12378 (2018).

[72] Y. A. Erlangga, C. Vuik, C. W. Oosterlee, On a class of preconditioners for
solving the helmholtz equation, Applied Numerical Mathematics 50 (3-4)
(2004) 409–425.

[73] V. Simoncini, E. Gallopoulos, An iterative method for nonsymmetric sys-
tems with multiple right-hand sides, SIAM Journal on Scientiﬁc Comput-
ing 16 (4) (1995) 917–933.

[74] A. Hussam, L. GRIGORI, P. H´enon, P. RICOUX, Enlarged gmres for

solving linear systems with one or multiple right-hand sides.

[75] V. Kalantzis, C. Bekas, A. Curioni, E. Gallopoulos, Accelerating data

14

