2
2
0
2

p
e
S
7

]
E
S
.
s
c
[

1
v
1
1
3
3
0
.
9
0
2
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

1

SZZ in the Time of Pull Requests

Fernando Petrulio, David Ackermann, Enrico Fregnan, G ¨ul Calikli, Marco Castelluccio, Sylvestre Ledru,
Calixte Denizet, Emma Humphries, and Alberto Bacchelli

Abstract—In the multi-commit development model, programmers complete tasks (e.g., implementing a feature) by organizing their
work in several commits and packaging them into a commit-set. Analyzing data from developers using this model can be useful to
tackle challenging developers’ needs, such as knowing which features introduce a bug as well as assessing the risk of integrating
certain features in a release. However, to do so one ﬁrst needs to identify ﬁx-inducing commit-sets. For such an identiﬁcation, the SZZ
algorithm is the most natural candidate, but its performance has not been evaluated in the multi-commit context yet.
In this study, we conduct an in-depth investigation on the reliability and performance of SZZ in the multi-commit model. To obtain a
reliable ground truth, we consider an already existing SZZ dataset and adapt it to the multi-commit context. Moreover, we devise a
second dataset that is more extensive and directly created by developers as well as Quality Assurance (QA) engineers of Mozilla.
Based on these datasets, we (1) test the performance of B-SZZ and its non-language-speciﬁc SZZ variations in the context of the
multi-commit model, (2) investigate the reasons behind their speciﬁc behavior, and (3) analyze the impact of non-relevant commits in a
commit-set and automatically detect them before using SZZ.

Index Terms—SZZ, Bug-Inducing Commits, Empirical Research, Pull Request, Dataset, Commit Set

(cid:70)

1 INTRODUCTION

M ANY software projects adopt the multi-commit devel-

opment model [1, 2, 3]. In this model, developers
complete their tasks (e.g., ﬁxing a bug or implementing a
new feature) by organizing their work into several commits
packaged into a commit-set. Among the various instances
of the multi-commit model, the pull-based development
model proposed by GitHub is the most popular [1].

The availability of data accumulated by projects using
the multi-commit model opens up new opportunities for
research to understand and support developers’ needs. By
using a multi-commit model developers can bundle all the
commits they work on when implementing a feature or bug
ﬁx in a single commit-set. These commit-sets can be used
by researchers to have access to feature level information,
which, in turn, can be used to conduct feature-level defect
prediction, features’ risk assessment, as well as empirical
studies on software quality involving features.

Feature-level information is considered important by
practitioners. For example, a recent study [4] found that de-
velopers are interested in knowing which feature introduced
a defect (rather than which commit, component, or even
method as previously thought [5]). Furthermore, release
engineers need to determine how risky each feature is [6]
when deciding which features to integrate into a release [7].
To use software evolution data (such as the multi-
commit one) for most applications related to software qual-
ity, one needs to know which changes introduced a defect [8].

•

F. Petrulio, D. Ackermann, E. Fregnan, and A. Bacchelli are with ZEST in
the Department of Informatics at the University of Zurich, Switzerland.
E-mail: fpetrulio@iﬁ.uzh.ch

• G. Calikli is with the School of Computing Science, University of Glasgow,

UK.

• M. Castelluccio, S. Ledru, C. Denizet, and E. Humpries are with Mozilla

Corporation.

Manuscript received . . . ; revised . . .

In 2005, ´Sliwerski, Zimmermann, and Zeller devised an al-
gorithmic approach (known as SZZ) to detect the commit(s)
responsible for introducing a speciﬁc defect in a software
system [9]. Thanks to SZZ, researchers could conduct im-
pactful studies on software quality [8].

SZZ is the natural candidate for detecting which commit-
sets introduce defects in the multi-commit model. However,
SZZ and its variations [10, 11, 12, 13] work at the commit
level1 and have been evaluated accordingly. Therefore, we
do not know whether SZZ can be used reliably in the multi-
commit context and, for example, whether SZZ variations
retain their beneﬁts and if one should adapt the input and
output of SZZ.

In this paper, we present an in-depth empirical evalu-
ation of SZZ in the multi-commit development model. We
investigated how SZZ and its variations perform in this
context and how they can be adapted.

As the ﬁrst step in our study, we focused on obtaining
a solid benchmark on which to evaluate SZZ in the multi-
commit context. We tackled this from two angles. On the
one hand, we considered the most recent and comprehen-
sive benchmark created to evaluate SZZ variations at the
commit level [14]. We adapted this benchmark for the multi-
commit context. On the other hand, we teamed up with
Mozilla to obtain a novel, developer-created dataset speciﬁc
to the multi-commit model. In this case, we designed and
deployed a new dedicated data ﬁeld for the issue tracker
tool used by Mozilla: Bugzilla. Through this new ﬁeld prac-
titioners can specify the commit-set whose implementation
induced the bug at hand. Developers, Quality Assurance
(QA) engineers, users, and automated tools (overseen by
developers) have been using this extension for more than
22 months. This effort resulted in the creation of our second
dataset, which comprises manually validated links between

1. Commit-level: the algorithm’s input is a single ﬁxing commit and

its output is a set of one or more candidate ﬁx-inducing commits.

 
 
 
 
 
 
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

2

5,348 commit-sets, for a total of 24,089 commits.

In the second step of our study, we evaluated the per-
formance of SZZ and its variations in the multi-commit
context. The more extensive benchmark (i.e., the Mozilla
one) contains different languages and technologies, so we
focused on non-language-speciﬁc SZZ variations. To gain
deeper knowledge on the factors that improve or reduce the
performance of the algorithms, we also manually analyzed
262 cases split among the unique ﬁndings and mistakes of
each SZZ variation.

Finally, since SZZ gets commits as input and not all
the commits in a commit-set may help SZZ link the right
ﬁx-inducing commit-sets, we investigated the impact of re-
moving the non-relevant input for SZZ. We also explored
multiple machine learning models to automatically remove
non-relevant input and evaluated their impact on SZZ’s
results.

2 BACKGROUND AND RELATED WORK
We provide background on SZZ, its variations, and the
multi-commit development model, particularly its pull-
based form. We also describe empirical studies that used SZZ
and its variations, as well as empirical studies on the multi-
commit model. We conclude motivating why it is important
and timely to evaluate ways to identifying bug-inducing
commit-sets, in addition to single bug-inducing commits.

Fig. 1: SZZ at (a) commit level and (b) commit-set level (i.e.,
in the multi-commit development model)

2.1 B-SZZ: The Original SZZ Algorithm

The original version of SZZ [9], usually referred to as B-SZZ
[14, 15], consists of two consecutive stages: (1) identifying
bug-ﬁxing changes and (2) identifying ﬁx-inducing changes.
The ﬁrst stage is concerned with linking bug reports
to the commits ﬁxing them. The difﬁculty of this step is
dictated by the speciﬁc tools and development process used
in the software systems under analysis. Nowadays, the ﬁrst
stage can be completed reliably and with minor effort. Most
contemporary software projects use issue tracking systems
(e.g., Bugzilla, GitHub) that maintain a link between each
issue and the commits (or commit-sets) solving it.

bug-ﬁxing commit (as identiﬁed in stage 1) and the history
of commits. By using the annotate command of CVS,
for each line in the bug ﬁxing commit (depicted as c6 in
Figure 1-A), the algorithm ﬁnds the most recent commit
(e.g., c4 in Figure 1-A) that modiﬁed the line before the ﬁx.
Since a commit can contain more than one line and each line
might have been modiﬁed in different past commits, several
commits can be marked as ﬁx-inducing. In the example
in Figure 1-A, the output of B-SZZ is the set of commits
{c1, c3, c4}. Accurately completing the second stage is an
open research challenge and is the focus of the work on SZZ
variations as well as our study.

2.2 SZZ Variations

In the following, we focus on the SZZ variations we con-
sider in our investigation. Our empirical evaluation takes
inspiration from the study by Rosa et al. [14], which is the
most recent and comprehensive study assessing B-SZZ and
its variations at commit level. Therefore, for our assessment,
we started by considering all the variations analyzed in
the study [14]. Then, we excluded those that work only
∗ [13] and OpenSZZ [16]) or on only
on Java ﬁles (RA-SZZ
Java/Javascript/C ﬁles (MA-SZZ [15]). In fact, in the Mozilla
dataset we created for our study, Java ﬁles are only 0.34%
of all ﬁles, and Javascript/C ones are less than 30%. We also
had to exclude SZZ Unleashed [17] because it could not scale
up to the size of the source code repository of Mozilla.
AG-SZZ. B-SZZ uses the annotate command of CVS to
track down modiﬁed/deleted lines. However, annotate
cannot detect changes in methods’ names. Therefore, when-
ever the name of a method containing a buggy line is
changed, B-SZZ cannot do the mapping and ﬁx-inducing
changes are undetected (thus increasing false negatives).
AG-SZZ solves this issue by using annotation graphs [10]. AG-
SZZ also reduces false positives by ignoring format changes
and changes to comments and blank lines.

L-SZZ & R-SZZ. The AG-SZZ algorithm may return multiple
ﬁx-inducing commits for each bug-ﬁxing commit. L-SZZ
and R-SZZ employ heuristics to ﬁlter the output that AG-
SZZ produces to return only one ﬁx-inducing commit for
each bug-ﬁxing commit. L-SZZ uses a heuristic to return the
change with the highest number of added/deleted/changed
lines, whereas R-SZZ returns the most recent commit among
all the candidates.

PYDRILLER. PYDRILLER [18], a tool for mining software
repositories, contains an enhanced implementation of AG-
SZZ. PYDRILLER uses git-hyper-blame to bypass meta
changes that do not change software behavior (e.g., refac-
torings) to decrease false positives. Such changes need to
be manually speciﬁed in a dedicated ﬁle containing all
revisions (commit hashes) to exclude from SZZ computation.

2.3 Applications of SZZ in Empirical SE

Most of the applications of B-SZZ and SZZ variations in
empirical SE research focus on software quality aspects.

The second stage’s goal is to determine which commit(s)
introduced a bug that was ﬁxed by the ﬁxing commit at
hand. In this stage, the B-SZZ algorithm receives as input a

Defect prediction. In the study where Herbold et al.
[19]
analyzed the issues with B-SZZ, the authors reported that
as of August 2019, slightly more than 50% (8 out of 15)

b. SZZ at commit-set (CS) level a. SZZ at commit level changehistoryC1C2C3C4C5C6changehistoryCS3CS1CS2C1C2C3C4C5C6C10C8C9C7bug-ﬁxingﬁx-inducingwrong candidateCxcommitIEEE TRANSACTIONS ON SOFTWARE ENGINEERING

3

of existing public datasets used for defect prediction re-
search were labeled by using B-SZZ. Various studies in the
literature prepared their own datasets (i.e., ground truth)
by using B-SZZ or its variations to learn and evaluate the
defect prediction models [20, 21, 22, 23, 24, 25, 26, 27].
However, the accuracy of the defect prediction models relies
on the accuracy of the SZZ approach used for labeling the
dataset [28]. Fan et al. [28] empirically investigated the effect
of mislabeled data on defect prediction performance. The
authors reported that mislabeled changes by AG-SZZ lead to
a statistically signiﬁcant reduction in prediction accuracy; in
contrast, data mislabeled by B-SZZ and MA-SZZ do not cause
a considerable reduction in prediction models’ accuracy.
Analysis of software quality related factors. Researchers
used the datasets labeled by B-SZZ and SZZ variations to
investigate how software quality relates to several factors
empirically [27, 29, 30, 31, 32, 33]. For instance, Eyolfson et al.
[30] studied the correlation between a commit’s bugginess
and its time-based properties (e.g., commit’s frequency, time
of the day, day of the week) in three open-source datasets
where they labeled ﬁx-inducing commits by B-SZZ. Bernardi
et al. [31] used AG-SZZ to label four open-source datasets so
that they could analyze how a code change’s fault prone-
ness relates to the communication between developers who
commit the code change. et al. Izquierdo-Cort´azar et al. [32]
and Tufano et al. [33] investigated the correlation between
developers’ experience and defect proneness in the datasets
they labeled by B-SZZ.

2.4 The Multi-Commit Development Model

Multi-commit development models are code delivery sys-
tems that allow a developer to bundle changes organized
across multiple commits. This practice is used to wrap fea-
tures or ﬁxes that beneﬁt from being separated in different
milestones.

This feature had a particular resonance in GitHub, with
the Pull-Based development model. In this model, contribu-
tors clone a software project’s repository to their local repos-
itory by using the fork function and make their changes
in the source code independently of each other. When the
code change is ready to be submitted, a contributor creates
a pull request and the following iterative process begins:
A core team member of the project (integrator) reviews
the changes. If the changes are satisfactory, the integrator
merges the pull request (i.e., pulls the contributors’ changes
to the main branch); if the changes are unsatisfactory, the
integrator may request additional changes or reject the pull
request.

There are various options to merge pull requests. By
using the standard merge option, integrators merge a pull
request into the main branch by retaining all the commits
from the contributor’s local branch (i.e., preserving the his-
tory). Similar to merge, rebase option integrates changes
from one branch to another. However, rebase moves a
feature branch (e.g., contributor’s local branch) to another
branch (e.g., master) without preserving the feature branch.
The squash option combines all the commits in a pull
request into a single commit and merges this single commit
to the main branch. As a result, a series of commits in a
contributor’s local branch corresponds to a single commit
in the software project’s master branch.

The choice of using these merging options depends on
the policies and/or the model of multi-commit development
used by a project. For example, in the approach used at
Mozilla, the whole commit-set completing a development
task is merged in a rebase fashion and, sometimes, a
commit-set may be merged only partially (e.g., merge com-
mit A and B ﬁrst, then C after a while).

GitHub is the most popular provider of hosting services
for git projects. However, depending on the underlying
Distributed Version Control System (DVCS), a project can
be stored in a different remote location and supported
with different tools to integrate commitments. For example,
Mozilla has its custom source-code management tool that
allows developers to choose the DVCS they prefer and apply
all changes to the remote repository, which is a Mercurial
repository: git users commonly create branches, while mer-
curial users rely on mercurial bookmarks.

2.5 The Multi-Commit Model in Empirical SE

In 2014, the study by Kalliamvakou et al. [34] revealed that
the majority of projects (65%) used the multi-commit model,
(i.e., pull requests, PRs). In 2016, the study by Gousios et al.
[35] emphasized an increasing trend in the usage of PRs,
reporting that over 135,000 repositories on GitHub received
more than 600,000 PRs, and over 45% of collaborative
GitHub projects were pull-based.

[22] and Wen et al.

The raw data for software quality studies often comes
from the repositories (e.g., GitHub) that support the multi-
commit development model [20, 21, 22, 23, 24]. For instance,
Kim et al.
[23] labeled the datasets
that contain PRs by using B-SZZ to learn defect prediction
models. However, in an empirical study conducted on 398
releases of 38 Apache projects, Herbold et al. [19] showed
that the existence of commit-sets results in the introduction
of false positives on the labeled datasets (i.e., wrongly iden-
tifying commits as bug-ﬁxing commits). The reason for these
false positives was that the SZZ linking marked commits as
bug-ﬁxing in the wrong pull-request.

2.6 Detecting Bugs at the Commit-Set Level

Detecting defective commit-sets is a crucial activity for dif-
ferent stakeholders. In this respect, Adams and McIntosh [7]
highlights how Release Managers would beneﬁt from tools
able to detect defective commit-sets and prevent them from
being integrated into upcoming releases. The presence of
undiscovered bugs in a release may generate a cascade effect
that contaminates branches created on top of the defective
release or that import defective modiﬁcations. Besides the
slowdown of code production in such branches, to locate
a bug in a CI environment, each commit of the release
needs to be tested, further increasing the workload on the
bug resolution [36]. In the work by Castelluccio et al. [37],
Mozilla Release Managers state that based on the content,
multiple commits (or patches) can be uplifted together and,
for this reason, they are wrapped under the same commit-
set (or release). Uplifting a commit-set means integrating all
the commits in the production environment. Since Release
Managers mostly rely on developers’ suggestions to review
a commit-set, skipping the stabilization phase increases the

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

4

probability to introduce a bug, causing a regression in the
codebase.

Developers are interested in defect detection at coarser
granularity too. For instance, the scope of the defect may
not be limited to the modiﬁed lines as assumed by the
defect predictors: ensuring quality standards in such cases
requires an update of different connected ﬁles, as reported
by Dunsmore et al. [38]. Also, analyzing a commit-set may
give more insights into the root location of a speciﬁc bug
[39]. Even more recently, Wan et al.
[4] interviewed prac-
titioners to understand their expectations about the future
evolution of defect prediction. On the topic of granularity,
both experienced and beginner developers agree that defect
prediction should extend the scope to commit-sets: despite
the indication of bug location may be less precise, targeting
commit-sets gives the reviewers a better overview of the
code quality. In this way, the bug-ﬁxing procedure increases
the evolvability of code components.

3 CREATING THE ROSA’S BENCHMARK

In 2021, Rosa et al. [14] presented a novel approach to
generate benchmarks to evaluate SZZ at the commit level.
Their approach relies on the fact that developers sometimes
explicitly document—in the commit message—which com-
mit introduced the defect they ﬁx with the current commit
(e.g., “THRIFT-4513: ﬁx bug in comparator introduced by
e58f75d” [14]). Starting from this bases, the methodology
by Rosa et al. [14] applies information retrieval techniques
to ﬁlter commits’ messages looking for an unequivocal link
between a ﬁx-inducing commit and a bug-ﬁxing one.

Based on this novel approach and a further manual vali-
dation they conducted, Rosa et al. [14] created and released
a novel dataset gathering data from publicly available
repositories on GitHub. Their dataset comprises 1,930 links
between ﬁx-inducing and bug-ﬁxing commits pertaining to
eight popular programming languages (C, C++, C#, Java,
JavaScript, Ruby, PHP, and Python).

3.1 Adapting the original dataset by Rosa et al. [14] for
evaluating SZZ in the multi-commit development model

Being validated and approved by the software engineering
research community for evaluating SZZ in the single-commit
context, we consider the dataset by Rosa et al. [14] as a valu-
able ground to evaluate SZZ in the multi-commit context
as well. Therefore, we adapted the dataset by Rosa et al.
[14] for this context. This adaptation is possible because the
original dataset gathers data from GitHub projects, therefore
commits can belong to pull requests (i.e., commit-sets). Lo-
cating to which pull request a bug-ﬁxing commit belongs as
well as to which pull request the linked ﬁx-inducing commit
belongs makes it possible to deﬁne a bug-ﬁxing commit-set
and a ﬁx-inducing commit-set, respectively.

Figure 2 shows this adaptation process. The top-half of
the ﬁgure (i.e., Figure 2a) shows the history of a software
system with an example bug-to-ﬁx link, as it is available
in the original dataset by Rosa et al. [14]. In particular, c6
is a bug-ﬁxing commit and is linked to c3–its ﬁx-inducing
commit. Both commits can be linked to pull requests (c6

Fig. 2: Adaptation of the Rosa’s Benchmark from (a) com-
mit level to (b) commit-set level (i.e., in the multi-commit
development model)

to CS3 and c3 to CS2). Then, CS3 is labeled as a bug-
ﬁxing commit-set and CS2 is its corresponding ﬁx-inducing
commit-set.

In practice, to implement this adaptation, we build a
script that leverages GraphQL API [40] to query commits’
information from GitHub for each bug-to-ﬁx link in the
original dataset by Rosa et al. [14]. Speciﬁcally, for each link
in the dataset we:

• extract the ﬁx-inducing commit;
• use GraphQL API to check whether this ﬁx-inducing
commit belongs to a commit-set (e.g., this happens for
c3 in Figure 2);

• in case a commit-set is found (e.g., CS2 in Figure 2), add
all the commits in the commit-set (i.e., c3 and c4) to the
list of ﬁx-inducing commits for that bug-to-ﬁx link.

• repeat the same for the bug-ﬁxing commit.
When adapting the original dataset by Rosa et al. [14] for

our goal, we encountered the following problems:
Missing repositories. Some commits in the dataset belong
to repositories that are no longer available in GitHub (i.e.,
they became private or were deleted).

Missing pull requests. Most of the links include commits
that cannot be linked to a PR. This case is also depicted in
Figure 2 in the case of c5: If c5 was either a ﬁx-inducing or
bug-ﬁxing commit, the entire link could not be considered
because of the missing PRs. We had to discard these cases.
Forks. GitHub produces incorrect mapping whenever the
bug and ﬁx are merged from a fork. A fork is a copy of
a repository mostly used to perform custom changes to a
project. In GitHub, changes from a fork can be imported
in the main project through PRs. If the PR is applied in
a fork and then imported in the original one, the PR id
will change creating a duplicated reference to the same
PR. This happens because the PR id is a progressive
counter that in a fork is reset to 0. During the merge,
GitHub assigns new PR ids to integrated PRs to keep the
consistency with the main project id counter. Therefore,
querying the PR id of a speciﬁc commit may return
multiple references when a commit exists in a PR created
in a fork and also integrated in the main project. This

a. Original dataset by Rosa et al. (commit level)changehistoryC1C2C3C4C5C6b. Dataset by Rosa et al. adapted for the multi-commit modelCS3changehistoryC1C2C5C6C0C7CS2C3C4CS1bug-ﬁxingﬁx-inducingCxcommitCSxcommit-setoriginal bug-to-ﬁx linkinferred bug-to-ﬁx linkIEEE TRANSACTIONS ON SOFTWARE ENGINEERING

5

may lead to a confusing mapping with unrelated PRs or
commits. Moreover, a given PR id can correspond to both
a PR created in a fork and a different PR integrated in
the original project. As a result, while inspecting a fork,
GraphQL will return all commits (including unrelated
commits) that belong to all PRs with the same reference
id, if this PR is also in the original project.

Due to the aforementioned issues, we had to discard
100 links due to missing repositories, 1,680 links because
the ﬁx-inducing or the bug-ﬁxing commit was not part of
a pull request, and ﬁve links due to forks from the original
dataset by Rosa et al. [14]. The resulting adapted benchmark
contains 145 links bug-to-ﬁx commit-sets. Henceforth, we
refer to this adapted dataset as Rosa’s Benchmark. The 145
links have all 1:1 cardinality (i.e., one ﬁx-inducing commit-
set and one bug-ﬁxing commit-set) and comprise a total of
2,142 commits, of which 1,315 in ﬁx-inducing commit-sets
and 827 in bug-ﬁxing ones.

3.2 Limitations of Rosa’s Benchmark

Despite the high quality bug-to-ﬁx links provided by Rosa
et al. [14], Rosa’s Benchmark has the following limitations:

Benchmark Size. The adapted dataset, due to the issues
encountered in the processing phase (Section 3.1), only
consists of 145 bug-to-ﬁx links. This size poses limitations
to the generalizability of the results.

Ghost Commits and Extrinsic Bugs. Different studies
[41, 42] highlight the presence of bugs that cannot be
retrieved by SZZ. Rezk et al. [41] refers to ghost commits
whenever a ﬁx-inducing or a bug-ﬁxing commit cannot
be retrieved due to their modiﬁcation type. In fact, a ﬁx
containing only new lines cannot be back-traced by Version
Control System (VCS) log functions (e.g., git blame) and a
bug produced only by line deletions does not leave traces in
future commits. Also, some modiﬁcations may be operated
outside the VCS domain (e.g., platform updates) yet produce
a bug that requires a ﬁx in the source code. For this reason,
SZZ produces false positive results on any processed commit
that ﬁxes such bugs. These cases are deﬁned by Rodr´ıguez-
P´erez et al. [42] as extrinsic bugs.

Both ghost commits and extrinsic bugs have a relevant
impact in the real-case scenarios: Rezk et al. [41] found out
that, on average, the 15.78% of all modiﬁcations are ghost
commits, and Rodr´ıguez-P´erez et al. [42] found that 15% of
bugs are extrinsic.

When analyzing the original dataset by Rosa et al. [14],
we unexpectedly found none of these cases (even at the
commit level) represented. That is, no linked commits were
either a ghost commit or an extrinsic bugs. This may limit the
representativeness of SZZ evaluations using this dataset.

Candidate ﬁx-inducing commits outside commit-sets.
Even in the adapted dataset, SZZ could output candidate
commits that do not belong to any commit-set. Figure 1
(b) shows such an example. SZZ could mark commit c7 as
ﬁx-inducing candidate, but c7 is not part of any commit-
set. To make the evaluation of SZZ results possible in these
cases, we consider any candidate ﬁx-inducing commit not
belonging to a commit-set as belonging to a virtual commit-
set composed of only the candidate commit itself.

4 CREATING THE MOZILLA’S BENCHMARK

Many datasets have been proposed to test SZZ, and even
more have been built with SZZ to train and study defect
prediction models. However, none of them have been di-
rectly built by the code owners. Also, SZZ has never been
studied at coarser granularity: despite a bug-ﬁxing commit
gathered by researchers belongs to a commit-set, only this
speciﬁc commit is taken into account to retrieve the bug.
This could be related to multiple factors: NLP techniques
to detect keywords in bug-ﬁxing commits may exclude
other ﬁx-related commits, or researchers’ understanding of
the content of commits may be limited due to the lack of
experience with the project and its technologies.

In this regard, to properly investigate SZZ and address
limitations of Rosa’s Benchmark presented in Section 3.2, it
is essential to have a reliable benchmark, which is possibly
large-scale, based on real-world data, and relies on develop-
ers’ work rather than researchers’ approximations [14].

In this section, we describe how we created such a
benchmark at Mozilla. Henceforth, we refer to this dataset
as the Mozilla’s Benchmark.

4.1 Data Collection and Preparation

The process that led to data in our benchmark consists of
the following steps:
1) Modiﬁcation of Bugzilla:

In April 2019, we cre-
ated a dedicated ﬁeld ‘regressions’ and its mirror
‘regressed by’ for each issue entry in Bugzilla. For
each bug b, in the ﬁeld ‘regressed by,’ there is the
list of issues whose ﬁxing commits introduced the bug b,
while the ﬁeld ‘regressions’ contains the list of bugs
that the ﬁxing commits of bug b itself introduced. For in-
stance, as shown in Figure 3, once developers detect that
ﬁxing bug 1618202 introduced bug 1622113, they
speciﬁed ‘bug 1618202’ in the ‘regressed by’ ﬁeld
of bug 1622113. Also, ‘bug 1622113’ is speciﬁed in
the ‘regression’ ﬁeld of bug 1618202.

2) Data completion and collection in Bugzilla: Since the
introduction of the new ﬁelds in April 2019, ﬁlling
in the ‘regressions’ and ‘regressed by’ ﬁelds in
issue reports has become a norm for Mozilla developers,
users, and QA engineers whenever relevant and possi-
ble. The ﬁelds are validated by the developers who are
assigned to either the regressing or the regressor issue.
In addition, developers also ﬁlled in these ﬁelds for a
subset of older issues dating backward to 2007. At the
date of the creation of the benchmark used in our study,
a total of 9,110 issues were linked by 608 practitioners
and veriﬁed by the developers assigned to the issues.

3) Linking the commit-sets: Each issue is ﬁxed through a
commit-set. Once the links among the ﬁx-inducing and
bug-ﬁxing issues were established (through the previous
step), we mapped the commit-sets solving these issues
to one another to create the ﬁnal dataset. To do so, we
extended a tool at Mozilla, BugBug [43], to gather the
relevant data from the issue tracker and code repository.
We created a script to combine the information contained
in the Bugzilla reports (regressed and regressed by ﬂags)
with commit information retrieved through the bug id.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

6

This way, we created the set of linked ﬁx-inducing
and bug-ﬁxing commit-sets that also includes commit-
related data.

Fig. 3: An example showing how developers ﬁll
in
‘regression’ and ‘regressed by’ ﬁelds in Bugzilla:
(a) bug 1622113 and (b) bug
Bugzilla reports for
1618202.

4.2 Mozilla’s Benchmark’s Descriptives

Applying the procedure described in the previous subsec-
tion, we obtained a dataset of 5,348 links between ﬁx-
inducing and bug-ﬁxing commit-sets, comprising 24,089 dif-
ferent commits and 9,110 commit-sets. The relation between
bug-ﬁxing and ﬁx-inducing commit-sets is 1 : N : For each
bug there can be only one bug-ﬁxing commit-set, but the
bug could be induced in several commit-sets. This situation
occurs in 121 cases. The cases where a bug is addressed by
multiple bug-ﬁxing commit-sets are extremely rare (only 9
cases), so we discarded them to avoid any incorrect map-
ping. A total of 1,586 pairs have a 1 : 1 commit cardinality
ratio. In the dataset, 7,043 commits are bug-ﬁxing, 16,159
commits are ﬁx-inducing, and 884 commits are in a chain of
both bug-ﬁxing and ﬁx-inducing commit-sets. Even though
we collected a snapshot to generate the dataset used in the
current study, the ‘regressions’ and ‘regressed by’
ﬁelds we devised and deployed are still in use at Mozilla
and the dataset is continuously growing with new data. To
the best of our knowledge, the dataset we contribute with
this paper is currently the largest publicly available dataset
with bug-ﬁxing and ﬁx-inducing links among commits and
commit-sets.

Our benchmark is based on commit-sets whose code
belongs to the Mozilla’s codebase. This codebase represents
a heterogeneous system employing a variety of program-
ming languages and application contexts, ranging from web
development to statistical analysis. The heterogeneity of
Mozilla’s codebase contributes to (1) increasing the variety
in the nature of the cases on which we apply SZZ and
(2) reducing the bias introduced by focusing on a speciﬁc
programming language or domain.

TABLE 1: Languages involved in the Mozilla codebase

Language

Files

Blanks

Comments

LOC

JavaScript
C++
HTML
C/C++ Header
Rust
C
JSON
Python
XML
Assembly
INI
XHTML
Java
Other

72,870
11,772
90,776
16,564
8,365
3,998
2,245
6,746
2,813
561
12,582
3,562
854
11,893

1,199,781
801,098
463,590
519,357
246,505
321,980
883
222,750
7,005
35,477
73,130
23,033
24,503
745,785

1,753,236
669,043
105,185
956,346
442,208
502,674
0
260,302
2,973
35,924
175
8,097
62,588
178,693

5,540,827
4,476,606
4,118,159
2,475,718
2,384,387
2,158,164
1,190,423
872,281
453,026
294,756
231,725
189,678
156,493
1,227,722

Total

251,601

4,174,520

4,977,444

25,769,965

5 RESEARCH QUESTIONS

We set to empirically evaluate how SZZ performs when
applied to commit-sets. Therefore, we ask:

RQ1: What is the performance achieved by SZZ and its

main variations at the commit-set level?

The change of granularity leads to a change in the input
space of SZZ: Each bug-ﬁxing commit-set can be composed
of more than one commit, as opposed to at the commit
level where only one input commit can be used. This may
have both positive and negative effects: Increasing the algo-
rithm’s input could increase the chances to ﬁnd ﬁx-inducing
candidates, yet it could also lead to more false positives.

We hypothesize that by removing the irrelevant and
noisy commits from a bug-ﬁxing commit-set, the overall
results of SZZ would signiﬁcantly improve. In our second
research question, we set out to investigate this hypothesis
and, if conﬁrmed, study whether and to what extent an au-
tomated approach can automatically recognize non-useful
commits from a bug-ﬁxing commit-set. Finally, we use this
approach to re-compute the new results for an improved
SZZ at commit-set level. Therefore, we ask:

RQ2: To what extent can SZZ’s results be improved by
retaining only the useful input commits from a bug-
ﬁxing commit-set?

6 RQ1: SZZ PERFORMANCE FOR COMMIT-SETS

In this section, we evaluate the performance of SZZ when
applied to the multi-commit context.

Identification of fix- inducing commit- setAdding fix- inducing reference id to Regressed by fieldAdding bug- fix reference id to Regressions fieldabIEEE TRANSACTIONS ON SOFTWARE ENGINEERING

7

6.1 Methodology

consider

We evaluate the performance of SZZ at commit-set level
from three complementary perspectives.
- Evaluation perspective 1: We

the Mozilla’s
Benchmark. Given a bug-ﬁxing commit-set, we run SZZ
on each commit it includes. Then, for each ﬁx-inducing
candidate commit found, we consider as output all the
commit-sets that include at least one of these candidate
commits. For example, in Figure 1, we consider both CS1
and CS2 as the output, because they each contain at least
one commit that was linked by SZZ from the commits in
CS3.

the

consider

- Evaluation perspective 2: We

in
Mozilla’s Benchmark in which both the ﬁx and the bug
consists of a single commit. This selection corresponds to
recasting the evaluation to match a commit level one and
enables a more direct comparison with, for example, the
work by Rosa et al. [14]. Mozilla’s Benchmark contains
1,586 such cases.

cases

- Evaluation perspective 3: We consider Rosa’s Bench-
mark. In this subset of the original dataset by Rosa et al.
[14] that we created (see Section 3), both the ﬁx-inducing
and bug-ﬁxing commits are embedded in commit-sets.
This enables a meaningful comparison at the commit-set
level. Rosa’s Benchmark contains 145 such cases.

Table 2 provides information on how the different per-

spectives are related to the datasets we consider.

Evaluation metrics. To evaluate the performance of SZZ, we
adopt measures of recall, precision, and F1 score as used in
information retrieval [44]:

recall =

|correct ∩ identif ied|
|correct|

precision =

|correct ∩ identif ied|
|identif ied|

F 1 = 2 ·

recall · precision
recall + precision

In the formulas above, identiﬁed represents the set of
candidate commit-sets retrieved by SZZ, and correct rep-
resents the set of ﬁx-inducing commit-sets established by
developers in the Mozilla’s Benchmark or by Rosa et al. [14].
Furthermore, we compute the Jaccard distance. This
measure represents the similarity between two sets as the
proportion of shared elements among all elements in both
sets. Given a speciﬁc commit-set (CS), we consider as
sets for the Jaccard distance (1) the ﬁx-inducing commits
retrieved by SZZ (F iCSZZ) and (2) the ﬁx-inducing commits
from the ground truth (F iCgt).

JD(CSi) = 1 −

|F iCSZZ(CSi) ∩ F iCgt(CSi)|
|F iCSZZ(CSi) ∪ F iCgt(CSi)|

JD(variation) =

1
n

n
(cid:88)

i=1

JD(CSi)

A Jaccard distance’s value closer to 1 means a higher
dispersion between the two considered sets and closer to 0
is almost no dispersion.

SZZ variations. We test several variations of SZZ focusing
on non-language-speciﬁc ones: B-SZZ, AG-SZZ, R-SZZ, and
L-SZZ. We use the implementation of these algorithms as
offered by Rosa et al. [14]. Moreover, we include in our
evaluation the improved version of B-SZZ provided by PY-
DRILLER [18], which—differently from the original B-SZZ—
allows users to specify a set of commits to exclude. The
developers from Mozilla created a list of massive-refactoring
commits (e.g., Mozilla commit 7558c2), which we ﬁltered
out using git-hyper-blame function when applying SZZ.
This makes PyDriller’s variation of B-SZZ similar to RA-SZZ.

6.2 Results

In the following, we present the results of our evaluations
by perspective.

Evaluation perspective 1 - Table 3a shows the results of
this perspective. R-SZZ is the variation reaching the high-
est F1 measure, matching the ranking reported by Rosa
et al. [14] at the commit level. In terms of recall, B-SZZ and
PYDRILLER outperform the other variations. Most likely
because there is no ﬁltering of the considered commits,
which, as a trade-off, lowers the precision signiﬁcantly.
The last column of Table 3a shows the results in terms of
average Jaccard distance between the ground truth and
SZZ variations. The Jaccard distance is always higher than
0.8, highlighting a remarkable dispersion of the results.
Evaluation perspective 2 - This perspective’s results (Ta-
ble 3b) show an increase in precision with negligible
decreases in recall. For commit-sets of one commit only,
B-SZZ and PYDRILLER represent the best choice.
In terms of average Jaccard distance (last column of Ta-
ble 3b), we observe that the average dispersion of results
is high but generally lower than in commit-set case.
This triangulates the improvements in terms of precision
providing further evidence that having only one input
commit for SZZ signiﬁcantly reduces the dispersion from
the expected result set, when compared to having more
than one input commit.

Evaluation perspective 3 - Table 3c reports the results for
perspective 3. Similarly to the case of perspective 1, R-
SZZ outperforms every other model, and B-SZZ as well
as PYDRILLER have low precision due to more input
commits. Yet, the performance of SZZ, when applied to
this subset of Rosa’s Benchmark, is better compared to
when SZZ is applied to the Mozilla’s Benchmark. This
result is conﬁrmed by the average Jaccard distance, which
is the lowest among the three perspectives.

Overall, in all three perspectives, the performance of SZZ
(for all the variations we tested) at commit-set level is 20 or
more percentage points lower in both precision and recall
than that reported by Rosa et al. [14] at the commit level.
For example, in our dataset B-SZZ reaches 0.49 and 0.19 for
recall and precision, respectively, while it reaches 0.69 and
0.39 in the study by Rosa et al. [14].

Since we use a different dataset than Rosa et al. [14], we
cannot rule out that the reason for this lower performance

2. https://hg.mozilla.org/mozilla-central/rev/

7558c8821a074b6f7c1e7d9314976e6b66176e5c

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

8

TABLE 2: Characteristics of the datasets considered in the different perspectives

Perspective

Dataset

Characteristics

Perspective 1 Mozilla’s

Perspective 2

Perspective 3

Benchmark
Subset
of
Mozilla’s
Benchmark
Rosa’s
Benchmark

Created by Mozilla developers
(Section 4)
A single commit on both ﬁx-
inducing and bug-ﬁxing com-
mit sets
Subset of the dataset by Pas-
carella et al. [21] where both ﬁx-
inducing and bug-ﬁxing com-
mits are in a commit set

Number of links

total

1:1

5,348

5,227

1:N

121

1,586

1,586

145

145

0

0

Number of commits

total

in ﬁx-inducing set

in bug-ﬁxing set

24,086

2,869

2,142

7,927

1,586

827

17,043

1,358

1,315

TABLE 3: SZZ’s commit-set level performance

(a) Perspective 1 (Mozilla’s Benchmark), N = 5, 348

Variation

Identiﬁed

Correct

Rec.

Prec.

F1

avg JD

B-SZZ
AG-SZZ
L-SZZ
R-SZZ
PyDr.

13,323
7,964
4,975
5,008
13,420

2,582
1,997
1,527
1,981
2,548

0.49
0.38
0.29
0.38
0.48

0.19
0.25
0.31
0.40
0.19

0.28
0.30
0.30
0.39
0.27

0.81
0.82
0.84
0.80
0.81

(b) Perspective 2 (subset of the Mozilla’s Benchmark with
a single commit on both ﬁx-inducing and bug-ﬁxing
commit-sets), N = 1, 586

Variation

Identiﬁed

Correct

Rec.

Prec.

F1

avg JD

B-SZZ
AG-SZZ
L-SZZ
R-SZZ
PyDr.

1,310
1,236
1,245
1,239
1,291

763
620
470
626
754

0.47
0.38
0.29
0.39
0.47

0.58
0.50
0.37
0.50
0.58

0.52
0.43
0.33
0.44
0.52

0.65
0.66
0.70
0.61
0.65

(c) Perspective 3 (Rosa’s Benchmark), N = 145

Variation

Identiﬁed

Correct

Rec.

Prec.

F1

avg JD

B-SZZ
AG-SZZ
L-SZZ
R-SZZ
PyDr.

861
499
256
247
676

76
71
52
68
55

0.51
0.47
0.36
0.47
0.37

0.09
0.12
0.21
0.35
0.13

0.15
0.22
0.26
0.35
0.13

0.36
0.38
0.41
0.40
0.37

is due to speciﬁc characteristics of the Mozilla’s Bench-
mark, rather than the change in granularity from commit
to commit-set level. Indeed, the results for Perspective 2
(which simulates a commit level scenario, thus makes it
more comparable to the study by Rosa et al. [14]) in which
even the best performing variation does not achieve the
results reported by Rosa et al. [14], may indicate this as
a likely reason. At the same time, the results presented
in Perspective 3 (i.e., evaluated on Rosa’s Benchmark) are
also lower: It could be that commit-set level poses speciﬁc
challenges hindering the effectiveness of SZZ.

Finding 1: The performance of SZZ and its variations is
20 percentage points lower than the results previously
reported at the commit level.

6.3 Further Analysis of the Results

To understand whether and how speciﬁc conditions inﬂu-
ence SZZ, we further analyzed cases in which variations

behaved similarly or very differently. In particular, we ana-
lyzed the cases of commit-sets (A) missed by all variations
(i.e., false negatives), and (B) missed or retrieved by only
one SZZ variation. Our aim is to investigate whether these
commit-sets share particular features that can be used in the
future to improve SZZ selection criteria. In this respect, we
focused on results obtained by SZZ on Mozilla’s Benchmark
as we consider this ground truth more reliable because it is
entirely built by Mozilla’s developers.

(A) Commit-sets missed by all variations. Figure 4 shows
an overview of the agreement among pairs of SZZ variations,
in terms of true positives (i.e., the agreement rate on correct
results - Figure 4a), false positives (i.e., the agreement rate
on incorrectly identiﬁed commit-sets - Figure 4b), and false
negatives (i.e., the agreement rate on missed commit-sets
- Figure 4c). In Figure 4a and Figure 4b, the agreement
is never lower than 56%. This is expected, since all the
algorithms heavily derive from the B-SZZ root. In Figure 4c
(the agreement rate for missed commit-sets), the agreement
is as high as 90%.

In total, we have 1,174 links in our dataset that cannot
be retrieved by SZZ. Those links represent the 45% of the
entirely mismatched cases and the 22% of the Commit-Set
based dataset.

One of the authors manually inspected randomly se-
lected 50 cases of false negatives. For each of them, he
examined the nature of the changes in both the ﬁx-inducing
and bug-ﬁxing commit-sets (which lines and ﬁles are modi-
ﬁed, what kind of modiﬁcations have been performed) and
debugged SZZ execution to detect the root cause of the error.
In this way, we have been able to spot the following three
main reasons:

– New lines of code cause mismatches - Whenever a bug
is ﬁxed by only introducing new lines of code, the
algorithm cannot retrieve the ﬁx-inducing commit. We
found 21 of these cases in our Commit-Set based
dataset. Moreover, 10 of them also do not share any
ﬁle with the ﬁx-inducing commit-set. This limitation is
known at commit-level as the ghost commit effect [15, 41].
– A bug was introduced in a different ﬁle - The main reason
why SZZ cannot reach a ﬁx-inducing commit is that
the bug-ﬁxing commit-set modiﬁes different ﬁles from
the ﬁx-inducing commit-set. This situation is present
in Mozilla’s Benchmark in 1,164 cases (43% of the
entirely mismatched cases). In 367 of them, the ﬁles
in the bug-ﬁxing commit-set and the ones in the ﬁx-
inducing commit-set do not even share the same direc-

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

9

(a) True Positives

(b) False Positives

(c) False Negatives

Fig. 4: Results overlapping among variations by SZZ implementation pairs

tory. Such cases still belong to the deﬁnition of Ghost
Commits [15, 41].

was created, yet there is no self-evident reason we could
discover.

– No Commit in ﬁx-inducing commit-set - In 45 cases we
found that there is no ﬁx-inducing commit-set associ-
ated to the bug. This condition usually happens when
some updates to the environment are applied but such
modiﬁcations are not operated by code (e.g., a library
update). In this case, SZZ cannot be applied since it
produces only false positive results. However, the al-
gorithm cannot prevent such occurrences. This kind
of bugs are known in literature as Extrinsic Bugs [42].
As an example, the bug 15253733 has its root in the
previous ﬁx of the bug 15690914. In this case, the
problem was created by a local machine with an inner
problem not related with Mozilla’s code. To solve that,
developers needed to reinstall the Operating System
(OS) on that machine, generating a compatibility prob-
lem with the Mozilla’s codebase. To solve the bug, the
developers added a few instructions to enable the new
OS to instrument the code. So, for this reason, there is
a bug-ﬁxing commit-set (to address the compatibility
issue) but the ﬁx-inducing problem (the new installed
OS) is not reﬂected on the VCS.

The Case of the Rosa’s Benchmark. To understand how
prominent the aforementioned three conditions are in other
cases, we checked the percentage of untraceable links (i.e.,
Ghost Commits and Extrinsic Bugs) in the original dataset
by Rosa et al. [14] at both commit and commit-set level.
Unexpectedly, none of the ground truth links in the dataset
(and, as a consequence, in Rosa’s Benchmark) is affected by any of
the three aforementioned conditions: the bug is always placed
in at least a commit and the ﬁx always includes at least
a modiﬁcation/deletion in a ﬁle that is shared by the ﬁx-
inducing and bug-ﬁxing commits.

This characteristic of the dataset by Rosa et al. [14]
(which is reﬂected also on Rosa’s Benchmark) is surprising
because past research (as well as the data in the Mozilla’s
Benchmark) has provided evidence that the three conditions
exist in the majority of situation. The lack of these cases
in the dataset by Rosa et al. [14] may be due to reasons
connected to the methodology with which the benchmark

3. https://bugzilla.mozilla.org/show bug.cgi?id=1525373
4. https://bugzilla.mozilla.org/show bug.cgi?id=1569091

TABLE 4: SZZ’s commit-set level performance on Mozilla’s
Benchmark when excluding non-linkable cases (N = 4, 174)

Variation

Identiﬁed

Correct

Rec.

Prec.

F1

avg JD

B-SZZ
AG-SZZ
L-SZZ
R-SZZ
Pydriller

10,730
6,342
4,028
4,050
10,814

2,573
1,986
1,515
1,970
2,515

0.63
0.48
0.37
0.48
0.61

0.24
0.31
0.38
0.49
0.23

0.35
0.38
0.37
0.48
0.34

0.54
0.56
0.62
0.49
0.54

To quantify the effect of excluding Ghost Commits and
Extrinsic Bugs, we discarded cases that cannot be linked in
our Commit-Set based dataset and re-run the experiments.
Table 4 reports the results.

As expected, we observe a strong increase of SZZ perfor-
mance in each of its variations. In most of the cases, SZZ also
performs better in Mozilla Commit-Set based dataset than
in the Rosa’s Benchmark. After removing these unlinkable
cases, the ranking of the variations is stable: R-SZZ outper-
form every other model in terms of precision and F1 score,
but B-SZZ and PYDRILLER reach higher recall.

Finding 2: More than 20% of the links in Mozilla’s
Benchmark cannot be retrieved by current implementa-
tions of SZZ. By excluding these cases, the recall values
of SZZ at commit-set level improve substantially, getting
closer to those reported for the commit level.

(B) Commit-sets retrieved/missed by only one variation.
To gain further insights into the conditions under which
SZZ works/fails, we focus on the commit-sets that were
correctly identiﬁed (115 cases) or missed (547) by only one
SZZ variation.

For each SZZ variation, we randomly extracted a sta-
tistically signiﬁcant sample (for a total of 262 records) and
manually analyzed each case, exploring the git history and
deﬁning why a given variation behavior differs from the
others. Two authors performed this analysis independently:
they only agreed on which sources to consider (bug reports
and source code) during the analysis. Then, each of the au-
thors described the problem case in a few words, specifying
the reason behind the unique miss or ﬁnding related to the

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

10

SZZ variation and its heuristic. Their initial results reached
an inter-rater agreement of 82.82%. Then, the two authors
discussed the cases of disagreement until they reached an
agreement.

In the following, we report the ﬁndings by variation.

B-SZZ: The true positive ﬁndings unique to B-SZZ are often
identiﬁed through the lines with code comments. Com-
ment lines help B-SZZ succeed where other variations fail
because the latter discard such lines a-priori. Also, some
bugs reside in large refactoring commits, but only the
basic version of SZZ can detect this type of bug.
At the same time, refactoring commits represent the pri-
mary source of error for B-SZZ [12].

AG-SZZ: This variation relies on an annotation graph to
connect modiﬁed lines with functions or methods that
wrap them, providing a better mapping of the code and
excluding cosmetic changes. However, in our experimen-
tal setup, it does not provide any particular beneﬁt in
identifying ﬁx-inducing commits: we are not able to de-
tect special cases where AG-SZZ outperforms any other
variation. Also, we found that AG-SZZ incorrectly labeled
the correct ﬁx-inducing changes as refactoring. This in-
correct labeling leads the algorithm to retrieve as ﬁx-
inducing changes commits older than the correct ones (in
particular, when multiple ﬁles or commits are involved).
Furthermore, the criteria to detect refactoring commits
are sometimes too stringent, inducing AG-SZZ to mark
speciﬁc commits as irrelevant.
A typical example of AG-SZZ failure is represented by
the bug 16687555: here the bug-ﬁxing commit-set is
composed by only one commit, a7aef336, and the
ﬁx-inducing commit-set is represented by the commit
4413dfb7. The ﬁx-inducing and bug-ﬁxing commits
share the ﬁle defaultBrowserNotification.js but,
since the modiﬁcation contains over 150 modiﬁed lines, it
is considered a refactoring. For this reason, AG-SZZ traced
the ﬁle to an earlier point, wrongly landing on the commit
d261b6a8 (i.e., the ﬁrst commit where the ﬁle appeared).
the set of ﬁx-
inducing candidates only keeping the largest (L-SZZ) and
the most recent (R-SZZ) commits. We found that only in
seven cases, the two variations perform better than the
other variations in detecting ﬁx-inducing commits. On the
contrary, they have the highest amount of unique errors
(e.g., L-SZZ has 377 unique errors). These mistakes are
caused by the selection criteria of L-SZZ and R-SZZ. In the
case of R-SZZ, the algorithm often stopped before reaching
the correct commit marking more recent commits as bug-
inducing. For instance, this often happened when the
correct ﬁx-inducing commit was followed by a refactoring
commit. L-SZZ, instead, marks as ﬁx-inducing the commit
that contains the highest number of changes. However,
this criterion often led to mistakes: e.g., L-SZZ selected a
refactoring commit (where a large number of lines of code

L-SZZ & R-SZZ: These variations ﬁlter

5. https://bugzilla.mozilla.org/show bug.cgi?id=1668755
6. https://github.com/ambroff/gecko/commit/

a7aef3341915becacfc6a0edc961461b147da896

7. https://github.com/ambroff/gecko/commit/

4413dfbe1a9b9d4d78104cfb8d1aedfeb087c875

8. https://github.com/ambroff/gecko/commit/

d261b6a4f26055b719dc23302ca4033ce3ed3f8d

have been changed) as opposed to the correct ﬁx-inducing
commit (where only few lines have been modiﬁed).

PYDRILLER: This variation can be considered as an ex-
tended version of B-SZZ since it provides the possibility
to specify a list of commits to skip during the execu-
tion. In fact, most of the ﬁx-inducing commits correctly
identiﬁed only by PYDRILLER are located in changes that
other SZZ variations marked as refactoring. For instance,
PYDRILLER outperformed the other variations when iden-
tifying errors concerning modiﬁcations in the parameters
of a method or related to code that undergoes multiple
cosmetic changes. In its execution, PYDRILLER skips all
the commits marked as refactoring by Mozilla developers
(see Section 6). However, Mozilla’s developers listed only
a portion of commits that could be considered as refactor-
ing. For this reason, PYDRILLER is not able to skip some
big changes (unrelated to the bug), thus providing incor-
rect results. Moreover, the implementation of PYDRILLER
(version 1.15) we considered cannot correctly identify
C/C++ directives (e.g., #ifndef or #ifdef) marking
them as comments.

Finding 3: Filtering SZZ output increases the precision
of the algorithm, sometimes discarding useful commits.
Such ﬁltering heuristics can be leveraged in respect of the
trade-off between precision and recall.

7 RQ2: SELECTION OF INPUT COMMITS

When applied at the commit-set level, SZZ and its variations
achieve low precision values (e.g., see Table 3a). The un-
derlying reason might be that some commits in a bug-ﬁxing
commit-set are irrelevant to the bug ﬁx and create additional
noise for SZZ. This hypothesis is corroborated by the results
of evaluation Perspective 2 (i.e., the analysis of 1-commit
commit-sets, shown in Table 3b): In the cases in which only
one commit is available as input for SZZ, the precision of the
algorithm is signiﬁcantly higher, with only minor losses in
terms of recall.

In this research question, we ﬁrst set out to challenge
our hypothesis by evaluating whether and to what extent
removing all irrelevant/noisy commits from the input of
SZZ improves its performance at commit-set level.

Based on results supporting our hypothesis, we inves-
tigate to what extent an automated approach can classify
commits in a bug-ﬁxing commit-set as either useful or not
for SZZ linking. Last, we evaluate the ﬁnal effects of using
such an approach on the results of SZZ.

7.1 The Impact of Irrelevant Commits on SZZ

To test our hypothesis, we use the Rosa’s Benchmark con-
sidering the cases in which both ﬁx-inducing and bug-
ﬁxing commits are within commit-sets (i.e., cases used for
evaluation Perspective 3 in RQ1). The use of this benchmark
helped us to get an overview from many different projects.
We remove all the commits in each bug-ﬁxing commit-
set that lead to SZZ linking wrong ﬁx-inducing commit-sets.
Then, we evaluate the performance of SZZ and compare to
those achieved without this ﬁltering (i.e., Table 3c) to verify
our hypothesis.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

11

7.1.1 Data Labeling

Whether a commit in a bug-ﬁxing commit-set is a good
linker depends on the SZZ variation. For example, a commit
could lead to a wrong commit when using B-SZZ, yet to
the correct ﬁx-inducing commit when used as input for
PYDRILLER. Therefore, we apply two labeling methods and
evaluate their effect:

• Single-variation labeling: We label commits based on a
speciﬁc SZZ variation. For example, when evaluating B-
SZZ, we consider as good all the commits that provide
good links for B-SZZ, while we consider as bad the
commits that do not help B-SZZ (even if they help other
variations).

• All-variations labeling: We consider as good links only
bug-ﬁxing commits that lead to correct links for all SZZ
variations, and we consider as bad links only bug-ﬁxing
commits that provide a bad link for all SZZ variations.
We exclude commits that lead to good/bad links for
only a subset of variations.
These two labeling options also inﬂuence the dataset we
consider: in the Single-variation labeling, we can consider
all commits in the computation, while in the All-variations
labeling, we rely only on overlapping results, thus reducing
the available amount of bug-ﬁxing commits.

7.1.2 Results

Table 5 reports the results of SZZ considering only commits
in bug-ﬁxing commit-sets that provide a correct link to a ﬁx-
inducing commit-set adopting the evaluation perspective 3.
Those results represent the highest performance achievable
by each SZZ variation, maximizing the precision of the
algorithm. We consider them as an upper bound to our
machine learning pipeline. R-SZZ is the best performing
model, followed by L-SZZ. The key difference between these
variations and the rest is the precision: they both reach 1.00,
meaning they perfectly identify the ﬁx-inducing commit-set
without any false positives. For the same reason, B-SZZ com-
pletely lacks in precision: the broader range of ﬁx-inducing
candidates implies a higher number of correct links (and
thus a higher recall) and incorrect ﬁx-inducing candidates.
By ﬁltering the input commits of SZZ, we will increase the
precision of algorithms like B-SZZ and PYDRILLER, reducing
the amount of identiﬁed commit-sets and trying to keep
the recall as high as possible. Also, there is no way to
improve the recall of such algorithms without modifying
their internal behavior. Reducing the number of bug-ﬁxing
commits to consider in a commit-set may also reduce the
number of correct links retrieved, thus reducing the recall of
the algorithm. We aim to ﬁnd the right trade-off to increase
the precision more than the amount of lost recall.

Finding 4: Filtering SZZ input commits increases the al-
gorithm’s precision, but it is impossible to improve the
recall without designing a new SZZ variation.

7.2 Automated Classiﬁcation of Input Commits for SZZ

thus justifying the investigation on how to select and keep
only the relevant commits in a commit-set. Based on this
ﬁnding, we present our investigation on the creation of an
automated approach to classify commits into good/bad for
SZZ linking.

7.2.1 The Methodology in a Nutshell
The automated approach needs to tackle a binary classiﬁca-
tion problem: Given a commit in a bug-ﬁxing commit-set,
it determines whether it is a good linker commit that can be
used by SZZ to ﬁnd the correct ﬁx-inducing commit-set.

Employing sophisticated techniques (e.g., based on deep
learning) to accomplish this task goes beyond the scope of
the current work. Our aim is two-fold: (1) Verifying whether
it is feasible to create an automatic classiﬁcation approach
that provides reasonable results and (2) deﬁning an initial
baseline against which future methods can be tested. There-
fore, we employ supervised machine learning, which bases
its decision on a set of features we deﬁne for which the
weights and interaction are automatically computed from
the training set.

We take advantage of the Mozilla’s Benchmark to train
and test several machine learning algorithms with the fea-
tures we deﬁne. Then, we run a cross-dataset evaluation: We
train our models on the Mozilla’s Benchmark and evaluate
their results on the data from Rosa’s Benchmark. This cross-
dataset evaluation allows us to better establish the gener-
alizability of the results, because we apply the classiﬁer to
an entirely unseen dataset, extracted from different projects
with diverse development practices.

7.2.2 Dataset Creation
As the purpose of the machine learning approach is to
classify bug-ﬁxing commits into good or bad links for SZZ,
we need to create a dataset at the commit level granularity.
To this aim, we gathered all commits from all bug-ﬁxing
commit-sets in the Mozilla’s Benchmark. We got 7,048 data
points from the 5,348 commit-sets in the Mozilla’s Bench-
mark.
Classiﬁcation Features. Since we set to classify commits (as
either useful for linking or not), we have the opportunity
to compute features from both the given commit and the
commit-set that includes the commit. Table 6 lists all the
features we considered, their granularity, and our rationale
for their inclusion.
Cross-dataset Evaluation. To perform the aforementioned
cross-dataset evaluation, we also collected 827 data points
from the Rosa’s Benchmark.

7.2.3 Training and Evaluation
We train and evaluate the following classiﬁcation models:
Support Vector Machine (SVM), Decision Trees (DT), Ran-
dom Forest (RF), Logistic Regression (LR), Gradient Boost
(GB), and Naive Bayes (NB). We selected these models be-
cause they are the most widely-used supervised algorithms
in software engineering [46] and make different assump-
tions about the underlying data and the interactions among
the features.

The results in Table 5 support our hypothesis: SZZ performs
better when the input is reduced to only the relevant com-
mits. Moreover, the additional improvement is signiﬁcant,

Since our dataset is imbalanced, we combined those
models with different sampling methods: Random Over-
sampling, Random Undersampling, and SMOTE [47]. To

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

12

TABLE 5: SZZ performance upper-bound at the commit-set level considering perspective 3 for Single-variation labeling,
N = 145

Algorithm Identiﬁed
B-SZZ
259
AG-SZZ
144
L-SZZ
52
R-SZZ
68
Pydriller
159
Overlap
35

Correct Discarded Commit-sets

76
71
52
68
55
35

70/145
74/145
93/145
77/145
91/145
110/145

Prec.
0.29
0.49
1.00
1.00
0.34
1.00

Rec.
0.51
0.48
0.36
0.47
0.37
0.24

F1
0.37
0.48
0.53
0.64
0.35
0.39

TABLE 6: Feature extracted in the bug-ﬁxing commits of the dataset

Feature Name
Addition

Granularity Description
Commit

Number of added LOC

Deletion

Commit

Number of deleted LOC

Files

Commit

Number of modiﬁed ﬁles

CS Addition

Commit-set

Percentage of added LOC
relative to the commit-set

CS Deletion

Commit-set

Percentage of deleted LOC
relative to the commit-set

CS Files

Commit-set

Percentage of touched ﬁles
relative to the commit-set

Order

Commit-set

Normalized time position
in the commit-set

CS Shared Files

Commit-set

If any ﬁle modiﬁed is
touched somewhere else

Rationale
Lines added in a commit are parts of the input
of SZZ. The more lines we have, the wider the
search space and thus the chances to retrieve
a ﬁx-inducing commit. Also, SZZ variations that
rely on a heuristic to spot refactoring may ﬁnd it
beneﬁcial to have a certain range of added lines.
Lines deleted in a commit are parts of the input
of SZZ. Deleted lines can also be backtraced by
SZZ, and so the more deleted lines we have, the
wider the research space.
The more ﬁles are modiﬁed, the more ﬁles SZZ
will explore. This feature represents the branch-
ing of SZZ across a project.
As for added lines but in proportion with the
entire commit-set. A higher percentage of added
lines may indicate that most of the changes
are condensed in a single commit that could
highlight a good linker.
As for deleted lines, but in proportion with the
entire commit-set. If a commit contains most
of the deletion, it means the code has been
discarded for some reason, possibly a bug too.
That may indicate that at a huge percentage of
deleted lines corresponds to a good linker.
As for ﬁles, but in proportion with the entire
commit-set. If a commit contains multiple mod-
iﬁcations on different ﬁles, it may indicate the
presence of a unlocalized bug. Therefore, we
believe that the more ﬁles are involved, the more
the commit is prone to contain a bug. Also, the
commit with the highest percentage of modiﬁed
lines has a broader branching scope that the
others commit.
Like for R-SZZ, the order of commits may iden-
tify a relevant ﬁx. By this heuristic, we can
assume that the core ﬁx is usually performed
ﬁrst and so the ﬁrst commit of a commit-set
is most probably the closest to the ﬁx-inducing
commit-set.
Extends the concept of bug locality to commit-
set level [45]. If a ﬁle is modiﬁed multiple times
in the same commit-set, it may indicate a certain
impact/relevance of such ﬁle in the bug-ﬁxing
commit-set.

type
Int

Int

Int

Float

Float

Float

Float

Bool

ensure the validity of our ﬁnal results, we applied these
sampling methods only to the training set. Also, we used
cost-sensitive learning [48] whenever possible, applying
proportional weights based on labels’ class ratio. To obtain
a comprehensive and more reliable set of results, we com-
bined all models with different cross-validation techniques:
shufﬂing, k-fold, and repeated k-fold validation with differ-
ent numbers of splits/iterations. We scaled the features by
unit variance to favor the convergence of SVM classiﬁers.
Then, we evaluated precision, recall, F1 score, confusion
matrix, and AUC-ROC for each model.

We also studied feature correlation with Spearman’s
Index to spot (and eventually discard) highly correlated
features. We also investigated feature importance to de-
termine the features with the strongest impact on these
models’ prediction performance. Table 7 reports the best
combination of classiﬁer, sampling method, and validation
technique for each SZZ variation.

To corroborate our ﬁndings, we applied these models
using cross-dataset evaluation: we used trained models to
predict good linkers from Rosa’s Benchmark bug-ﬁxing
commits in the version containing commit-set information.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

13

7.2.4 Results
Dataset Composition. The dataset in which we perform
All-variations labeling consists of 6,178 bug-ﬁxing commits:
4,789 are labeled as bad-links while 1,389 are labeled as
good-links. Instead, the Single-variation labeling dataset
contains 7,048 bug-ﬁxing commits but the labeling ratio
changes based on the SZZ variation we are considering.

We found no strong correlation between features, thus
no further reﬁnement is necessary. We also conducted
Principal Component Analysis (PCA) [49] that revealed
a contribution of almost all
features to the variance:
we cover the 95% of dataset variance with ﬁve features
(‘Deletion’,
‘CS Deletion’,
and ‘CS Shared Files’).

‘CS Addition’,

‘Files’,

Machine Learning Models. We tested several machine
models that better address the problem of binary classiﬁca-
tion. Considering that the dataset is imbalanced, we rely on
the F1 score to select the best models for each SZZ varia-
tion: the false positive rate for highly imbalanced datasets
decreased due to a large number of true negatives, and this
undermines the reliability of both AUC and accuracy scores
[50].

Table 7 shows that all selected models perform similarly.
In all variation-aware models, the optimum one is Gradient
Boost which reaches 0.66 of F1 score and 0.69 in accuracy
on B-SZZ. Also, in PYDRILLER we obtain relatively good
performance, conﬁrming the existing similarity between
those two SZZ variations. In the overlap approach, the best
model is Logistic Regression. Although the F1 score in the
overlap approach is in line with variation-aware models,
we can detect higher accuracy and AUC scores. Table 8 lists
all contributions obtained by each feature in each model.
The percentage of deleted lines is a good predictor for good
linkers.

Finding 5: SZZ Machine Learning Models are a viable
option to ﬁlter commit-set to detect the best bug-ﬁx linker
commits. The most relevant feature is the percentage of
deleted lines in such commits with respect to the total
deleted lines in the commit-set.

7.3 Evaluating SZZ with Automatically Filtered Com-
mits

As a last step in our investigation, we use the best perform-
ing models (as trained and tested on Mozilla’s Benchmark)
to predict good bug linkers in the unseen Rosa’s Benchmark.
Then we ﬁltered SZZ results to keep only algorithm itera-
tions on such commits, discarding all iterations performed
on bad bug linkers. We applied both Single-variation based
and All-variations based models, as reported in Table 11 and
Table 12 respectively. As expected, the SZZ performances
on the ground-truth at the commit-set level increase in
precision while the recall is slightly reduced. However, if
we add the Machine Learning ﬁlter on top of SZZ ﬁrst stage
(Section 2.1), the output obtained by the algorithm will be
more reliable in terms of correctness. In fact, by excluding
bad linker commits from the ground truth, the links between
ﬁx-inducing and bug-ﬁxing commit-sets contain fewer false
positives, and the relation between relevant and retrieved
commit-sets is preferable. R-SZZ is still the most reliable

solution for ﬁx-inducing commit-set retrieval, with a minor
increase of F1 score from 0.39 in the normal case to 0.41
for the Single-variation based solution and 0.40 in the All-
variations based solution. R-SZZ performances are even
better if we exclude bad-linkers also from the evaluation
(with F1 score = 0.58 for All-variations based solution) but,
despite the higher scores, most of the issues have been
completely excluded by SZZ execution. This is because all
ﬁx-inducing commits of such commit-sets are labeled as
bad linkers. In Single-variation based models, the number
of excluded commit-sets varies from 58 to 72, depending
on the SZZ variation, while we discard 77 commit-sets on
the All-variations based models. This is expected since the
overlap solution is trained on stricter labeling constraints.
However, although the machine learning models halve the
number of the overall issue (145), the number of relevant
commit-sets retrieved by SZZ is quite similar. In the Single-
variation based conﬁguration, we retrieve on average 19 rel-
evant ﬁx-inducing commit-sets less than the normal case. In
comparison, on average, we miss 24.8 relevant ﬁx-inducing
commit-sets less in the All-variations based conﬁguration.
This means that most discarded issues would produce in-
correct results on SZZ anyway.

Finding 6: Despite being imperfect, applying automated
ﬁltering of commits within commit-sets leads to an over-
all improvement in SZZ at the commit-set level.

8 DISCUSSION

A Commit-Set SZZ is needed. The multi-commit devel-
opment model, in general, and commit-sets, in particular,
have become a widely adopted practice in software de-
velopment [35]. Our results show that SZZ seems not to
be ready for this more complex context. For this reason,
further research should reﬁne SZZ for the multi-commit
model. Our study revealed that, when applied to commit-
sets, R-SZZ outperforms SZZ variations, similarly to the
commit-level case. However, B-SZZ and PYDRILLER outclass
all other variations in terms of recall. This means that the
noise introduced by the coarser granularity may represent
an obstacle, but at the same time, the extension of the
search space represents a beneﬁt in terms of the ﬁx-inducing
commits retrieved. Reducing the noise by performing a
commit selection in the bug-ﬁxing commit-set is a viable
option. As proposed in this work, a machine learning model
can represent a powerful tool in this perspective, but further
investigation is needed to discover new features that can
increase ML model performances.

Integrating dynamic project
information in SZZ. Our
manual analysis conﬁrmed existing problems of SZZ: the
impossibility for the algorithm to establish a relationship
between ﬁx-inducing and bug-ﬁxing commits when they
do not have ﬁles in common [41, 42]. To resolve such
cases, we envision integrating SZZ with information on the
dynamic ﬂow of the project, as only the code’s static analysis
currently performed by SZZ is not sufﬁcient in this context.
However, this kind of information might prove challenging
to extract, especially for non open-source repositories. To

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

14

TABLE 7: Performance of the best machine learning models as computed on Mozilla’s Benchmark

Algorithm
Overlap
B-SZZ
AG-SZZ
L-SZZ
R-SZZ
PYDRILLER

Model
LR
GB
GB
GB
GB
GB

Sampling
SMOTE
Random Oversampling
Random Oversampling
Random Oversampling
Random Oversampling
Random Undersampling N-Fold

Cross Validation
N-Fold
N-Time N-Fold
Shufﬂing
Shufﬂing
N-Time N-fold

Splits
5
5
5
5
5
10

Prec.
0.44
0.54
0.44
0.35
0.43
0.53

Rec.
0.79
0.82
0.84
0.81
0.81
0.83

F1
0.56
0.66
0.57
0.49
0.56
0.65

AUC Acc.
0.72
0.63
0.69
0.57
0.66
0.51
0.64
0.47
0.64
0.54
0.67
0.68

TABLE 8: Feature importance for the best models as computed on the Mozilla’s Benchmark.

Algorithm
Overlap(*)
B-SZZ
AG-SZZ
L-SZZ
R-SZZ
PYDRILLER

Addition Deletion

0.05
0.03
0.03
0.07
0.04
0.04

0.02
0.04
0.04
0.10
0.03
0.07

Files
-1.05
0.01
0.08
0.08
0.05
0.02

CS Addition
-0.07
0.04
0.03
0.05
0.05
0.04

CS Deletion
1.35
0.73
0.68
0.59
0.71
0.72

CS Files Order
0.00
0.10
0.07
0.04
0.05
0.05

0.10
0.02
0.02
0.03
0.03
0.03

CS Shared Files
0.33
0.01
0.02
0.01
0.01
0.00

(*): since feature importance cannot be calculated for all models, we added the given contribution of weighted coefﬁcients in Logistic Regression.

TABLE 9: Results of the cross-dataset evaluation with Single-variation labeling, N = 827

Algorithm
B-SZZ
AG-SZZ
L-SZZ
R-SZZ
PYDRILLER

TP
48
48
36
43
47

TN FP
22
701
24
715
33
726
27
708
21
703

FN Rec.
0.46
56
0.54
40
0.52
32
0.46
49
0.45
56

Prec.
0.68
0.66
0.52
0.61
0.69

F1
0.55
0.60
0.52
0.53
0.54

TABLE 10: Results of the cross-dataset evaluation with All-variations labeling, N = 827

Algorithm
B-SZZ
AG-SZZ
L-SZZ
R-SZZ
PYDRILLER

TP
44
43
32
41
45

TN FP
21
702
22
717
33
726
24
711
20
704

FN Rec.
0.42
60
0.48
45
0.47
36
0.44
51
0.43
58

Prec.
0.67
0.66
0.49
0.63
0.69

F1
0.52
0.56
0.48
0.52
0.53

solve this issue, we propose to integrate this information
directly into Bugzilla, where SZZ could easily access it.

Benchmark. The results of our evaluation of SZZ and
its variations, considering all Commit-Sets and the Commit-
Sets with only one commit, present signiﬁcant differences
from what was reported in the study by Rosa et al.
[14]
(as shown in Section 6). These differences might have been
caused by the different datasets used for the evaluation.
Project-speciﬁc aspects (e.g., the programming languages
used) or practices might introduce bias in the performance
of SZZ when applied to a speciﬁc dataset. For this reason, we
believe future studies should focus on evaluating SZZ (and
its variations) using multiple datasets to mitigate possible
biases. To this aim, we made our dataset openly available to
be used in future evaluations of SZZ.

9 THREATS TO VALIDITY

Construct Validity. As mentioned in section 4, we deployed
our dedicated data ﬁeld in April 2019. Based on the infor-
mation retrieved from Bugzilla, developers improved the
description of most bugs from 2007 to date. However, the
link between bug and ﬁx is not always explicitly available:
especially when bugs are trivial to ﬁx, the bug discussion

tends to be short or nonexistent. Although original develop-
ers cared about their bugs, it is still possible that recalling in-
formation about the speciﬁc bug could be challenging. This
could have led to missing or false links in our benchmark.

The development process at Mozilla matches the use
of rebase strategy to merge a commit-set into the main
branch. For this reason, all commits belonging to the same
commit-set appear in the history of a single, unique branch.
Actually, we are not able to gather any evidence on how
SZZ performs in combination with merge or squash op-
erations. However, this condition favors the application of
SZZ, considering its limitation due to the use of annotation
functions. Having a linear and uniﬁed environment is the
only opportunity to keep the commit history consistent
without losing the possibility to run SZZ along the entire
code evolution of Mozilla’s projects.

Internal Validity. Carrying out a manual analysis usually
introduces a subjectivity in the output. Also in our case, we
cannot assure the full correctness of our ﬁndings in Section
6. However, the fact that two authors reached the 82.82% of
inter-rater agreement before any confrontation highlights a
certain evidence of conditions affecting SZZ performances.
Also, the rigorous process of inspection of the git-history,
supported by the comparison with other variations’ results,
increases our conﬁdence in the ﬁndings.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

15

TABLE 11: Performance of SZZ at the commit-set level considering Rosa’s Benchmark on perspective 3 (cases with both
ﬁx-inducing & bug-ﬁxing commits within commit-sets) for Single-variation labeling, N = 145

Algorithm Identiﬁed

Correct Discarded Commit-sets

B-SZZ
AG-SZZ
L-SZZ
R-SZZ
Pydriller

113
99
65
65
104

53
53
37
44
52

58/145
64/145
72/145
62/145
64/145

Ground Truth Scores Without Bad Linkers
F1
Prec.
0.49
0.44
0.52
0.51
0.47
0.57
0.56
0.68
0.51
0.47

Prec.
0.44
0.51
0.57
0.68
0.47

Rec.
0.34
0.34
0.25
0.30
0.33

Rec.
0.54
0.53
0.40
0.48
0.54

F1
0.38
0.40
0.35
0.41
0.39

TABLE 12: Performance of SZZ at the commit-set level considering Rosa’s Benchmark on perspective 3 (cases with both
ﬁx-inducing & bug-ﬁxing commits within commit-sets) for All-variations labeling, N = 145

Algorithm Identiﬁed

Correct Discarded Commit-sets

B-SZZ
AG-SZZ
L-SZZ
R-SZZ
Pydriller

101
86
60
60
98

45
44
33
42
46

77/145
77/145
77/145
77/145
77/145

Ground Truth Scores Without Bad Linkers
F1
Prec.
0.48
0.45
0.51
0.51
0.47
0.55
0.58
0.70
0.50
0.47

Prec.
0.45
0.51
0.57
0.70
0.47

Rec.
0.52
0.51
0.40
0.49
0.53

Rec.
0.30
0.30
0.22
0.28
0.31

F1
0.36
0.38
0.32
0.40
0.37

As mentioned in Section 4.2, our dataset belongs to a
complete new generation. Considering the different gran-
ularity, a comparison with previous dataset may result
unfeasible. In this respect, we did our best to compare
our work with Rosa’s one by ﬂattening the problem from
both the granularities: the commit level and the commit-
set level. Also, we voluntarily excluded ghost commits and
extrinsic bugs from our dataset to fairly represent the effect
of numerous ﬁltering stages adopted by Rosa et al.

[14].

External Validity. Mozilla is a heterogeneous case of study
under many aspects: it involves multiple technologies and
programming languages, and is aimed at different contexts
like security, web development, machine learning, and data
analysis. However, Mozilla is an open source project with
high standards in development practices and code quality.
For this reason, it is not representative of all development
contexts. To mitigate the effect this bias may have on the
results, we also considered Rosa’s Benchmark, which gath-
ers data from a large set of open-source software systems
hosted on GitHub.

10 CONCLUSIONS

We evaluated the performance of SZZ and its variations
(i.e., SZZ algorithms) in a multi-commit environment. We
extended the work by Rosa et al. [14] to analyze the prob-
lem at the commit-set granularity. We also designed and
deployed a new dedicated data ﬁeld in the Mozilla Bugzilla,
which 608 Mozilla developers and QA engineers used to
link 5,348 bug-ﬁxing issues to ﬁx-inducing issues across 22
months (and these numbers are still increasing).

As a result of conducting quantitative and qualitative
analyses to evaluate SZZ algorithms’ performance, we found
that R-SZZ achieves the best performance, whereas SZZ
variations proposed for commit-level also apply in a multi-
commit environment. Moreover, machine learning models
can effectively increase the precision of SZZ at coarser granu-
larity and help exclude bug-ﬁxing commits that would lead
SZZ to an incorrect result.

Overall, the main contributions of this paper include:

• A publicly available dataset of 5,348 links between ﬁx-
inducing and bug-ﬁxing commit-sets (totaling 24,089
commits), whose creation involved professional devel-
opers from Mozilla;

• An empirical evaluation of SZZ and its variations at
commit-set level using both Rosa’s and our dataset,
showing that R-SZZ is the most reliable solution beyond
the change of granularity;

• Empirical data, based on a manual investigation of 262
commit-sets, on the unique ﬁndings/mistakes of each
considered SZZ variation that highlights their shortcom-
ings in this context;

• Empirical evidence of the impact of irrelevant commits
in commit-sets on the results of SZZ at the commit-set
level.

• The creation and evaluation of a set of machine learning
models to automatically detect commits that are good
linkers for SZZ in a commit-set, as well as an evaluation
of their application on the ﬁnal results of SZZ.

REFERENCES

[1] G. Gousios, M. Pinzger, and A. v. Deursen, “An ex-
ploratory study of the pull-based software develop-
ment model,” in Proceedings of the 36th International
Conference on Software Engineering, 2014, pp. 345–355.

[2] E. T. Barr, C. Bird, P. C. Rigby, A. Hindle, D. M. German,
and P. Devanbu, “Cohesive and isolated development
with branches,” in International Conference on Fundamen-
tal Approaches to Software Engineering.
Springer, 2012,
pp. 316–331.

[3] G. Gousios, A. Zaidman, M.-A. Storey, and A. v.
Deursen, “Work practices and challenges in pull-based
development: The integrator’s perspective,” in 2015
IEEE/ACM 37th IEEE International Conference on Soft-
ware Engineering, vol. 1, 2015, pp. 358–368.

[4] Z. Wan, X. Xia, A. E. Hassan, D. Lo, J. Yin, and X. Yang,
“Perceptions, expectations, and challenges in defect
prediction,” IEEE Transactions on Software Engineering,
vol. 46, no. 11, pp. 1241–1266, 2018.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

16

[5] Y. Kamei and E. Shihab, “Defect prediction: Accom-
plishments and future challenges,” in 2016 IEEE 23rd
international conference on software analysis, evolution, and
reengineering (SANER), vol. 5.

IEEE, 2016, pp. 33–45.

[6] O. Saliu and G. Ruhe, “Supporting software release
planning decisions for evolving systems,” in 29th An-
nual IEEE/NASA Software Engineering Workshop.
IEEE,
2005, pp. 14–26.

[7] B. Adams and S. McIntosh, “Modern release engineer-
ing in a nutshell–why researchers should care,” in 2016
IEEE 23rd international conference on software analysis,
evolution, and reengineering (SANER), vol. 5. IEEE, 2016,
pp. 78–90.

[8] G. Rodr´ıguez-P´erez, G. Robles, and J. M. Gonz´alez-
Barahona, “Reproducibility and credibility in empirical
software engineering: A case study based on a system-
atic literature review of the use of the szz algorithm,”
Information and Software Technology, vol. 99, pp. 164–176,
2018.
J. ´Sliwerski, T. Zimmermann, and A. Zeller, “When do
changes induce ﬁxes?” ACM sigsoft software engineering
notes, vol. 30, no. 4, pp. 1–5, 2005.

[9]

[10] S. Kim, T. Zimmermann, K. Pan, E. James Jr et al., “Au-
tomatic identiﬁcation of bug-introducing changes,” in
21st IEEE/ACM international conference on automated soft-
ware engineering (ASE’06).

IEEE, 2006, pp. 81–90.

[11] S. Davies, M. Roper, and M. Wood, “Comparing text-
based and dependence-based approaches for determin-
ing the origins of bugs,” Journal of Software: Evolution
and Process, vol. 26, no. 1, pp. 107–139, 2014.

[12] E. C. Neto, D. A. da Costa, and U. Kulesza, “The impact
of refactoring changes on the szz algorithm: An empir-
ical study,” in 2018 IEEE 25th International Conference on
Software Analysis, Evolution and Reengineering (SANER).
IEEE, 2018, pp. 380–390.

[13] E. C. Neto, D. A. d. Costa, and U. Kulesza, “Revis-
iting and improving szz implementations,” in 2019
ACM/IEEE International Symposium on Empirical Soft-
ware Engineering and Measurement (ESEM), 2019, pp. 1–
12.

[14] G. Rosa, L. Pascarella, S. Scalabrino, R. Tufano,
G. Bavota, M. Lanza, and R. Oliveto, “Evaluating SZZ
implementations through a developer-informed ora-
cle,” in 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE).
IEEE, 2021, pp. 436–447.
[15] D. A. Da Costa, S. McIntosh, W. Shang, U. Kulesza,
R. Coelho, and A. E. Hassan, “A framework for eval-
uating the results of the szz approach for identifying
bug-introducing changes,” IEEE Transactions on Soft-
ware Engineering, vol. 43, no. 7, pp. 641–657, 2016.
[16] V. Lenarduzzi, F. Palomba, D. Taibi, and D. A. Tam-
burri, “Openszz: A free, open-source, web-accessible
implementation of the szz algorithm,” ser. ICPC ’20,
2020, pp. 446–450.

[17] M. Borg, O. Svensson, K. Berg, and D. Hansson,
“Szz unleashed: an open implementation of the szz
algorithm-featuring example usage in a study of just-
in-time bug prediction for the jenkins project,” in Pro-
ceedings of the 3rd ACM SIGSOFT International Workshop
on Machine Learning Techniques for Software Quality Eval-
uation, 2019, pp. 7–12.

[18] D. Spadini, M. Aniche, and A. Bacchelli, “PyDriller:
Python framework for mining software repositories,”
in Proceedings of the 2018 26th ACM Joint Meeting
and
on European Software Engineering Conference
Symposium on the Foundations of Software Engineering
- ESEC/FSE 2018. New York, New York, USA:
ACM Press, 2018, pp. 908–911. [Online]. Available:
http://dl.acm.org/citation.cfm?doid=3236024.3264598
[19] S. Herbold, A. Trautsch, F. Trautsch, and B. Ledel,
“Issues with szz: An empirical assessment of the state
of practice of defect prediction data collection,” arXiv
preprint arXiv:1911.08938, 2019.

[20] Z. T ´oth, P. Gyimesi, and R. Ferenc, “A public bug
database of github projects and its application in bug
prediction,” in Computational Science and Its Applica-
tions – ICCSA 2016, O. Gervasi, B. Murgante, S. Misra,
A. M. A. Rocha, C. M. Torre, D. Taniar, B. O. Apduhan,
E. Stankova, and S. Wang, Eds.
Cham: Springer
International Publishing, 2016, pp. 625–638.

[21] L. Pascarella, F. Palomba, and A. Bacchelli, “Fine-
grained just-in-time defect prediction,” Journal of Sys-
tems and Software, vol. 150, pp. 22–36, 2019.

[22] S. Kim, E. J. Whitehead, and Y. Zhang, “Classifying
software changes: Clean or buggy?” IEEE Transactions
on Software Engineering, vol. 34, no. 2, pp. 181–196, 2008.
[23] M. Wen, R. Wu, and S.-C. Cheung, “Locus: Locating
bugs from software changes,” in 2016 31st IEEE/ACM
International Conference on Automated Software Engineer-
ing (ASE), 2016, pp. 262–273.

[24] M. Yan, X. Xia, Y. Fan, A. E. Hassan, D. Lo, and S. Li,
“Just-in-time defect identiﬁcation and localization: A
two-phase framework,” IEEE Transactions on Software
Engineering, pp. 1–1, 2020.
[25] S. Kim, T. Zimmermann, E.

J. Whitehead Jr, and
A. Zeller, “Predicting faults from cached history,” in
29th International Conference on Software Engineering
(ICSE’07).

IEEE, 2007, pp. 489–498.
[26] F. Rahman, D. Posnett, A. Hindle, E. Barr, and P. De-
vanbu, “Bugcache for inspections: Hit or miss?” in
Proceedings of the 19th ACM SIGSOFT Symposium and
the 13th European Conference on Foundations of Software
Engineering, ser. ESEC/FSE ’11, 2011, pp. 322–331.
[27] B. Chen and Z. M. J. Jiang, “Extracting and studying the
logging-code-issue-introducing changes in java-based
large-scale open source software systems,” Empirical
Software Engineering, vol. 24, no. 4, pp. 2285–2322, 2019.
[28] Y. Fan, X. Xia, D. A. da Costa, D. Lo, A. E. Hassan, and
S. Li, “The impact of changes mislabeled by szz on just-
in-time defect prediction,” IEEE Transactions on Software
Engineering, no. 01, pp. 1–1, jul 2019.

[29] H. Aman, S. Amasaki, T. Yokogawa, and M. Kawahara,
“Empirical study of fault introduction focusing on the
similarity among local variable names.” in QuASoQ@
APSEC, 2019, pp. 3–11.

[30] J. Eyolfson, L. Tan, and P. Lam, “Correlations between
bugginess and time-based commit characteristics,” Em-
pirical Software Engineering, vol. 19, no. 4, pp. 1009–1039,
2014.

[31] M. L. Bernardi, G. Canfora, G. A. Di Lucca, M. Di Penta,
and D. Distante, “The relation between developers’
communication and ﬁx-inducing changes: An empiri-

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

17

cal study,” Journal of Systems and Software, vol. 140, pp.
111–125, 2018.

[32] D. Izquierdo-Cort´azar, G. Robles, and J. M. Gonz´alez-
Barahona, “”do more experienced developers intro-
duce fewer bugs?”,” in ”Open Source Systems: Long-Term
Sustainability”, I. ”Hammouda, B. Lundell, T. Mikko-
nen, and W. Scacchi, Eds.
”Berlin, Heidelberg”:
”Springer Berlin Heidelberg”, ”2012”, pp. ”268–273”.

[33] M. Tufano, G. Bavota, D. Poshyvanyk, M. Di Penta,
R. Oliveto, and A. De Lucia, “An empirical study on
developer-related factors characterizing ﬁx-inducing
commits,” Journal of Software: Evolution and Process,
vol. 29, no. 1, p. e1797, 2017.

[34] E. Kalliamvakou, G. Gousios, K. Blincoe, L. Singer,
D. M. German, and D. Damian, “The promises and
perils of mining github,” in Proceedings of the 11th
working conference on mining software repositories, 2014,
pp. 92–101.

[35] G. Gousios, M.-A. Storey, and A. Bacchelli, “Work
practices and challenges in pull-based development:
the contributor’s perspective,” in 2016 IEEE/ACM 38th
International Conference on Software Engineering (ICSE).
IEEE, 2016, pp. 285–296.

[36] F. Zampetti, G. Bavota, G. Canfora, and M. Di Penta,
“A study on the interplay between pull request review
and continuous integration builds,” in 2019 IEEE 26th
International Conference on Software Analysis, Evolution
and Reengineering (SANER).

IEEE, 2019, pp. 38–48.

[37] M. Castelluccio, L. An, and F. Khomh, “An empirical
study of patch uplift in rapid release development
pipelines,” Empirical Software Engineering, vol. 24, no. 5,
pp. 3008–3044, 2019.

[38] A. Dunsmore, M. Roper, and M. Wood, “Practical code
inspection techniques for object-oriented systems: an
experimental comparison,” IEEE software, vol. 20, no. 4,
pp. 21–29, 2003.

[39] A. Ram, A. A. Sawant, M. Castelluccio, and A. Bac-
chelli, “What makes a code change easier to review: an
empirical investigation on code change reviewability,”
in Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, 2018, pp. 201–
212.

[40] GitHub. Github graphql api.

[Online]. Available:

https://docs.github.com/en/graphql

[41] C. Rezk, Y. Kamei, and S. Mcintosh, “The ghost commit
problem when identifying ﬁx-inducing changes: An
empirical study of apache projects,” IEEE Transactions
on Software Engineering, 2021.

[42] G. Rodr´ıguez-P´erez, G. Robles, A. Serebrenik, A. Zaid-
man, D. M. Germ´an, and J. M. Gonzalez-Barahona,
“How bugs are born: a model to identify how bugs are
introduced in software components,” Empirical Software
Engineering, vol. 25, no. 2, pp. 1294–1340, 2020.

[43] Mozilla. Bugbug open source ml project. [Online].

Available: https://zenodo.org/record/4911346

[44] C. Manning, P. Raghavan, and H. Sch ¨utze, “Introduc-
tion to information retrieval,” Natural Language Engi-
neering, vol. 16, no. 1, pp. 100–103, 2010.

[45] F. Rahman, D. Posnett, A. Hindle, E. Barr, and P. De-
vanbu, “Bugcache for inspections: hit or miss?” in

Proceedings of the 19th ACM SIGSOFT symposium and
the 13th European conference on Foundations of software
engineering, 2011, pp. 322–331.

[46] D. Zhang and J. J. Tsai, “Machine learning and software
engineering,” Software Quality Journal, vol. 11, no. 2, pp.
87–119, 2003.

[47] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer, “Smote: synthetic minority over-sampling
technique,” Journal of artiﬁcial
research,
vol. 16, pp. 321–357, 2002.

intelligence

[48] C. X. Ling and V. S. Sheng, “Cost-sensitive learning and
the class imbalance problem,” Encyclopedia of machine
learning, vol. 2011, pp. 231–235, 2008.

[49] H. Abdi and L. J. Williams, “Principal component
analysis,” Wiley interdisciplinary reviews: computational
statistics, vol. 2, no. 4, pp. 433–459, 2010.

[50] T. Saito and M. Rehmsmeier, “The precision-recall plot
is more informative than the roc plot when evaluating
binary classiﬁers on imbalanced datasets,” PloS one,
vol. 10, no. 3, p. e0118432, 2015.

Fernando Petrulio is an Italian PhD Student in
Software Engineering at the University of Zurich
and a member of ZEST since May 2019.

He obtained both Bachelor’s degree in Com-
puter Science (December 2016) and Master’s
degree in Computer Science with a Specializa-
tion in Data Science (February 2019) at Univer-
sit `a degli Studi di Salerno. He spent a period
in UZH with the International Mobility Program,
during which he produced my Thesis Work un-
der the supervision of Prof. Bacchelli. His main

interests are in defect prediction and empirical data analysis.

David Ackermann is a Software Engineer that
received his M.Sc. in Data Science and B.Sc.
in Business Informatics from the University of
Zurich. His interests are centered on making
fuzz testing more effective by combining it with
different machine learning algorithms and lever-
aging the human-in-the-loop.

Enrico Fregnan is a Ph.D. student in the Zurich
Empirical Software engineering Team (ZEST) at
the University of Zurich. He received his bache-
lor’s degree at Politecnico di Milano, Italy and his
master’s degree at Delft University of Technol-
ogy, The Netherlands. His research focuses on
investigating how to support developers during
code review.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

18

G ¨ul Calikli received Ph.D. and master’s degrees
in Computer Engineering and bachelor’s degree
in Mechanical Engineering from Bo ˘gazici Univer-
sity in Istanbul, Turkey. She is a lecturer (assis-
tant professor) in Software Engineering with the
School of Computing Science at the University
of Glasgow, United Kingdom.

Alberto Bacchelli received the bachelor’s and
master’s degrees in computer science from the
University of Bologna, Italy, and the PhD degree
in software engineering from the Universit `a della
Svizzera Italiana, Switzerland. He is an asso-
ciate professor of Empirical Software Engineer-
ing with the Department of Informatics in the
Faculty of Business, Economics and Informatics
at the University of Zurich, Switzerland.

Marco Castelluccio is an engineering manager
at Mozilla. He received his Bachelor, his Master
and his Ph.D. of Computer Science Engineering
at University of Napoli Federico II. His research
interests include software engineering, software
reliability, software maintenance and evolution,
software analytics, and artiﬁcial intelligence.

Sylvestre Ledru is a director of engineering at
Mozilla and a volunteer on Debian, LLVM and
Rust, with more than 20 years of experience in
the software development industry. He is focus-
ing his efforts on low level systems, software de-
velopment processes, code quality and security.

Calixte Denizet is a senior software engineer at
Mozilla and a former math teacher. He received
his Master of Mathematics at the University of
Caen in 1997.

Emma Humpries works as a developer expe-
rience engineer at Bandcamp. They received
their Master of Science in Economics from the
University of Wisconsin, Madison.

