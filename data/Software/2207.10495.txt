A Forgotten Danger in DNN Supervision Testing:
Generating and Detecting True Ambiguity

Michael Weiss
Universit`a della Svizzera italiana
Lugano, Switzerland
0000-0002-8944-389X

Andr´e Garc´ıa G´omez
Universit`a della Svizzera italiana
Lugano, Switzerland
0000-0003-3102-6986

Paolo Tonella
Universit`a della Svizzera italiana
Lugano, Switzerland
0000-0003-3088-0339

2
2
0
2

l
u
J

1
2

]
E
S
.
s
c
[

1
v
5
9
4
0
1
.
7
0
2
2
:
v
i
X
r
a

i.e.,

Abstract—Deep Neural Networks (DNNs) are becoming a
crucial component of modern software systems, but they are
from the
prone to fail under conditions that are different
ones observed during training (out-of-distribution inputs) or
on inputs that are truly ambiguous,
inputs that admit
multiple classes with nonzero probability in their ground truth
labels. Recent work proposed DNN supervisors to detect high-
uncertainty inputs before their possible misclassiﬁcation leads
to any harm. To test and compare the capabilities of DNN
supervisors, researchers proposed test generation techniques, to
focus the testing effort on high-uncertainty inputs that should
be recognized as anomalous by supervisors. However, existing
test generators can only produce out-of-distribution inputs. No
existing model- and supervisor independent technique supports
the generation of truly ambiguous test inputs.

In this paper, we propose a novel way to generate ambiguous
inputs to test DNN supervisors and used it
to empirically
compare several existing supervisor techniques. In particular,
we propose AMBIGUESS to generate ambiguous samples for
image classiﬁcation problems. AMBIGUESS is based on gradient-
guided sampling in the latent space of a regularized adversarial
autoencoder. Moreover, we conducted what is – to the best of
our knowledge – the most extensive comparative study of DNN
supervisors, considering their capabilities to detect 4 distinct
types of high-uncertainty inputs, including truly ambiguous ones.

I. INTRODUCTION

language processing,

Recently, more and more software systems are Deep Learn-
ing based Software Systems (DLS), i.e., they contain at least
one Deep Neural Network (DNN), as a consequence of the
impressive performance that DNNs achieve in complex tasks,
such as image, speech or natural
in
addition to the availability of affordable, but highly performant
hardware (i.e., GPUs) where DNNs can be executed. DNN
algorithms can identify, extract and interpret relevant features
in a training data set, learning to make predictions about an
unknown function of the inputs at system runtime. Given the
complexity of the tasks for which DNNs are used, predictions
are typically made under uncertainty, where we distinguish
between epistemic uncertainty, i.e., model uncertainty which
may be removed by better training of the model, possibly
on better training data, and aleatoric uncertainty, which is
model-independent uncertainty, inherent in the prediction task
(e.g., the prediction of a non-deterministic event). The former
uncertainty is due to out-of-distribution (OOD) inputs, i.e.,
inputs that are inadequately represented in the training set.

The latter may be due to ambiguity – a major issue often
ignored during DNN testing, as recently recognized by Google
AI Scientists: ”many evaluation datasets contain items that
(...) miss the natural ambiguity of real-world context” [1].

The existence of uncertainty led to the development of DNN
Supervisors (in short, supervisors), which aim to recognize
inputs for which the DL component is likely to make incorrect
predictions, allowing the DLS to take appropriate counter-
measures to prevent harmful system misbehavior [2]–[8]. For
instance, the supervisor of a self-driving car might safely
disengage the auto-pilot when detecting a high uncertainty
driving scene [2], [9]. Other examples of application domains
where supervision is crucial include medical diagnosis [10],
[11] and natural hazard risk assessment [12].

While most recent literature on uncertainty driven DNN
testing is focused on out of distribution detection [2]–[5], [13]–
[17], studies considering true ambiguity are lacking, which
poses a big practical risk: We cannot expect that supervisors
which perform well in detecting epistemic uncertainty are
guaranteed to perform well at detecting aleatoric uncertainty.
Actually, recent literature suggests the opposite [18]. The lack
of studies considering true ambiguity is related to – if not
caused by – the unavailability of ambiguous test data for
common case studies: While to create ODD data, such as
corrupted and adversarial inputs, a variety of precompiled
dataset and generation techniques are publicly available [19]–
[21], and invalid or mislabelled data is trivial to create in
most cases, we are not aware of any approach targeting the
generation of true ambiguity in a way that is sufﬁcient for
reliable and fair supervisor assessment. In this paper we aim
to close this gap by making the following contributions:
Approach We propose AMBIGUESS, a novel approach to
generate diverse, labelled, ambiguous images for image
classiﬁcation tasks. Our approach is classiﬁer indepen-
dent, i.e., it aims to create data which is ambiguous
to a hypothetical, perfectly well trained oracle (e.g., a
human domain expert), and which does not just appear
ambiguous to a speciﬁc, suboptimally trained DNN.
Datasets Using AMBIGUESS, we generated and released two
ready-to-use ambiguous datasets for common bench-
marks in deep learning testing: MNIST [22], a collection
of grayscale handwritten digits, and Fashion-MNIST [23],
a more challenging classiﬁcation task, consisting of

1

 
 
 
 
 
 
grayscale fashion images.

Supervisor Testing Equipped with our datasets, we measured
the capability of 16 supervisors at detecting different
types of high-uncertainty inputs,
including ambiguous
ones. Our results indicate that there is complementarity
in the supervisors’ capability to detect either ambiguity
or corrupted inputs.

II. BACKGROUND

Ambiguous Inputs: In many real-world applications, the
data observed at prediction time might not be sufﬁcient
to make a certain prediction, even assuming a hypothetical
optimal oracle such as a domain expert with exhaustive
knowledge: If some information required to make a correct
prediction is missing, such missing information can be seen
as a random inﬂuence, thus introducing aleatoric uncertainty
in the prediction process.

Formally, in a given classiﬁcation problem, i.e., a machine
learning (ML) problem where the output is the class c the input
x is predicted to belong to, let P (c | x) denote the ground truth
probability that x belongs to c, where observation x ∈ O and
O denotes the observable space, i.e., the set of all possibly
observable inputs. We deﬁne true ambiguity as follows:

Deﬁnition 1 (True Ambiguity in Classiﬁcation): A data point
x ∈ O is truly ambiguous if and only if P (c | x) > 0 for more
than once class c.

Inputs to a classiﬁcation problem are considered truly am-
biguous if and only if in the (usually unknown) ground truth,
such input is truly part of an overlap between two or more
classes. We emphasize true ambiguity to indicate ambiguity
intrinsic to the data and independent from any model and its
classiﬁcation conﬁdence/accuracy. In this way we distinguish
ours from other papers which also use the term ambiguous
with different meaning, such as low conﬁdence inputs, mis-
labelled inputs, where a label in the training/test set is not
consistent with the ground truth [24], or invalid inputs, where
no true label exists for a given input1. In simple domains,
where humans may have no epistemic uncertainty (i.e., they
know the matter perfectly), true ambiguity is equivalent to
human ambiguity. In the remainder of this paper we focus
only on true ambiguity and if not otherwise mentioned we
use the term ambiguity as a synonym for true ambiguity.

Out-of-Distribution (OOD) Inputs: A prediction-time
input
is denoted OOD if it was insufﬁciently represented
at training time, which caused the DNN not to generalize
well on such types of inputs. This is the primary cause of
epistemic uncertainty. OOD test data is used extensively to
measure supervisor performance in academic studies, e.g. by
modifying nominal data in a model-independent, realistic and
label-preserving way (corrupted data) [2], [14], [19], [20]
or by minimally modifying nominal data to fool a speciﬁc,
given model (adversarial data). In practice, both OOD and

1It can be noticed that the term invalidity is context dependent. Dola et.
al. [17] consider an input invalid if it is out-of-distribution w.r.t. the training
data, while still being an input which clearly belongs to one class, whereas
other works consider as invalid input any relevant edge case [19], [20].

(a) 4/9

(b) 8/9

(c) 4/8

(d) 4/8

(e) 3/8

(f) 4/9

(g) 6/9

(h) 1/7

(i) 5/9

(j) 4/9

(k) 4/9

(l) 6/8

(m) 0/5/9 (n) 4/9

(o) 0/8

(p) 3/5

(q) 0/9

(r) 0/6

Fig. 1: The 18 most ambiguous images, manually selected
from the 300 (3%) samples with the highest predictive entropy
in the MNIST test set [22]. Only a few of them are clearly
ambiguous, showing that ambiguous data are scarce in existing
datasets.

true ambiguity are important problems when building DLS
supervisors [25].

Decision Frontier: Much recent literature works on the
characterization of the decision frontier of a given model, i.e.,
its boundary of predictions between two classes in the input
space [26]–[29]. It is imporant to note that the decision frontier
is not equivalent to the sets of ambiguous inputs: The decision
frontier is model speciﬁc, while ambiguity depends only on
the problem deﬁnition and is thus independent of the model.
I.e., the fact that an input is at a speciﬁc model’s frontier,
does not guarantee that it is indeed ambiguous (it may also be
unambiguous, i.e., belong to a speciﬁc class with probability
1, or invalid, i.e., have 0 probability to belong to any class).
The decision frontier may thus be considered the “model’s
ambiguity”, while true ambiguity implies that an input
is
perceived as ambiguous by a hypothetical, perfectly well
trained domain expert (hence matching “human ambiguity”
in many classiﬁcation tasks).

III. RELATED WORK

The research works that are most related to our approach
deal with automated test generation for DNNs [2], [14], [19]–
[21], [30]. In these works, some reasons for uncertainty, such
as ambiguity, are not considered. Hence, automatically gener-
ated tests do not allow meaningful evaluations under ambiguity
of DNN supervisors, as well as of the DNN behavior, in
the absence of supervisors. We illustrate this in Figure 1:
Using an off-the-shelve MNIST [22] classiﬁer, we calculated
the predictive entropy to identify the 300 samples with the
presumably highest aleatoric uncertainty in the MNIST test set.
Out of these 300 images, we manually selected the ones we
considered potentially ambiguous, and show them in Figure 1.
Clearly, some of them are ambiguous, showing that ambiguity
exists and is present in the MNIST test set, but the scarcity
of truly ambiguous inputs indicates that supervisors cannot be
conﬁdently tested for their capability of handling ambiguity
using this test set.

2

input

In the DNN test

input generators (TIG) literature [2],
[14], [19], [20], [30], [31], with just one notable preprint as
an exception [18], we are not aware of any paper aiming
to generate true ambiguity directly, while most TIG aim
for other objectives. Some works [19], [20], [32] propose
to corrupt nominal
in predeﬁned, natural and label-
preserving ways to generate OOD test data. DeepTest [30]
applies corruptions to road images, e.g., by adding rain, while
aiming to generate data that maximizes neuron coverage. Also
targeting road images, DeepRoad [14] is a framework using
Generative Adversarial Networks (GAN) to change conditions
(such as the presence of snow) on nominal
images. The
Udacity Simulator, used by Stocco et. al., [2], allows to
dynamically add corruptions, such as rain or snow, when
testing self-driving cars. Similar to DeepTest, TensorFuzz [33]
and DeepHunter [34] generate data with the objective to
increase test coverage. Again, aiming to generate diverse and
unseen inputs, these approaches will mostly generate OOD
inputs and only occasionally – if at all – truly ambiguous
data.

A fundamentally different objective is taken in adversarial
input generation [35], where nominal data is not changed in
a natural, but in a malicious way. Based on the tested model,
nominal input data is slightly changed to cause misclassiﬁ-
cations. Literature and open source tools provide access to
a wide range of different speciﬁc adversarial attacks [21].
While very popular, neither input corruptions nor adversarial
attacks generate ambiguous data. As they rely on the ground
truth label of the modiﬁed input to remain unchanged, true
ambiguity, associated with an ambiguous ground truth label,
would imply unsuccessful test data generation.

Another popular type of test data generators aims to create
inputs along the decision boundary: DeepJanus [29] uses
a model based approach, while SINVAD [27] and MANI-
FOLD [28] use the generative power of variational autoen-
coders (VAE) [36]. Note that we cannot expect inputs along
the decision boundary to be always truly ambiguous – they
may just as well be OOD, invalid or in rare cases even low-
uncertainty inputs. In addition, these approaches are by design
model speciﬁc, making them unsuitable to generate a generally
applicable, model-independent, ambiguous dataset.

Thus, out of all the approaches discussed above, none aims
to generate a truly ambiguous dataset. A notable exception
is a recent, yet unpublished, preprint by Mukhoti et. al.,
[18]. In their work, to evaluate the uncertainty quantiﬁcation
approach they propose, they needed an ambiguous MNIST
dataset. To that extent, they used a VAE to generate a vast
amount of data (which also contains invalid, OOD and un-
ambiguous data) which they then ﬁlter and stratify based
on two mis-classiﬁcation prediction (MP) techniques, aiming
to end up with a dataset consisting of ambiguous images.
We argue that, while certainly valuable in the scope of their
paper, the so-created dataset is not sufﬁcient as a standard
benchmark for DNN supervisors, as the approach itself relies
on a supervision techniques (MP), hence being circular if
the created
used for DNN supervisor assessment. In fact,

3

ambiguity may be particularly hard (or easy) to be detected
by supervisors using different (resp. similar) MP techniques.
We anyway compared their approach to ours empirically and
found that it is less successful in generating truly ambiguous
test data than ours.

IV. USES OF AMBIGUOUS TEST SETS

In this paper we focus on the usage of ambiguous test data
for the assessment of DNN supervisors, but ambiguous data
have also other uses, including the assessment of test input
prioritizers.

A. Assessment of DNN supervisors

We cannot assume that results on DNN supervisors’ capabil-
ities obtained on nominal and OOD data generalize to ambigu-
ous data. Recent studies [5], [37] have shown that there is no
clear performance dominance amongst uncertainty quantiﬁers
used as DNN supervisors, but such studies overlook the threats
possibly associated with the presence of ambiguity. Warnings
on such threats in medical machine learning based systems
were raised already in 2000 [38], with ambiguity in a cancer
detection dataset mentioned as a speciﬁc example. The authors
proposed to equip the system with an ambiguity-speciﬁc
supervisor, to “detect and re-classify as ambiguous” [38] such
threatening data. To test such supervisors, such as the one
proposed by Mukhoti et. al. [18], model and MP independent
and diverse ambiguous data is needed.

B. Assessment of DNN input prioritizers

test

Test input prioritizers, possibly based on MP, aim to prior-
itize test cases (inputs) in order to allow developers to detect
mis-behaviours (i.e., mis-classiﬁcations) as early as possible.
Hence, they should be able to recognize ambiguous inputs.
Correspondingly,
input prioritizers should be assessed
also on ambiguous inputs. On the contrary, when the goal
is active learning, an ambiguous input should be given the
least priority or excluded at all, as the aleatoric uncertainty
causing its mis-classiﬁcation cannot by deﬁnition be avoided
using more training data. Thus, recognition of ambiguous test
data is clearly of high importance when developing a test input
prioritizer, be that to make sure that the ambiguous samples are
given a high priority (during testing) or a low priority (during
active learning).

C. Other Uses of Ambiguous Data

Ambiguous data is potentially useful in at least two other
DLS testing applications: First, it allows to assess an iden-
tiﬁed decision frontier. Much recent literature works on the
characterization of the decision boundary of a given model,
i.e., its frontier of predictions between two classes in the input
space [26]–[29]. While not every point at a models frontier is
ambiguous, the inverse should be true: Any truly ambiguous
input must be placed - by a sufﬁciently well trained classiﬁer
- at the models decision frontier. Thus, our generated datasets
may be used as an oracle to evaluate a model’s decision
frontier, when we expect the model to exhibit low conﬁdence
if presented with ambiguous input.

Fig. 2: Autoencoder (blue) and its extension to a Regularized
Adversarial Autoencoder (green)

Fig. 3: Image Sampling in the Latent Space

Second,

informing the developers of a DLS about

the
root causes of uncertainties and mispredictions would greatly
facilitate further improvement of the DLS, especially because
DNNs are known for their low explainability [39], which
makes debugging particularly challenging when dealing with
them. To develop any DNN debugging technique that sup-
ports uncertainty disentanglement or uncertainty reasoning,
the availability of ambiguous data in the test set is a strict
prerequisite, because ambiguity (aleatoric uncertainty) is an
important root cause of mis-behaviours.

V. GENERATING AMBIGUOUS TEST DATA

We designed AMBIGUESS, a TIG targeting ambiguous data
for image classiﬁcation, based on the following design goals
(DG):
DG1 (labelled ambiguity): The generated data should be
truly ambiguous and have correspondingly probabilistic labels,
i.e., each generated data is associated with a probability
distribution over the set of labels. Probabilistic labels are the
most expressive description of true ambiguity and a single
or multi-class label can be trivially derived from probabilistic
labels.
DG2 (model independence): To allow universal applicability
of the generated dataset, our TIG should not depend on any
speciﬁc DNN under test.
DG3 (MP independence): The created dataset should allow
fair comparison between different supervisors. Since supervi-
sors are often based on MPs (e.g., uncertainty or conﬁdence
quantiﬁers), our TIG should not use any MP as part of the data
generation process, to avoid circularity, which might give some
supervisor an unfair advantage or disadvantage over another
one.
DG4 (diversity): The approach should be able to generate a
high number of diverse images.

A. Interpolation in Autoencoders

Autoencoders (AEs) are a powerful tool, used in a range
of TIG [18], [27], [28], [31]. AEs follow an encoder-decoder
architecture as shown in the blue part of Figure 2: An encoder
E compresses an input into a smaller latent space (LS), and the
decoder D then attempts to reconstruct x from the LS. The

4

reconstruction loss, i.e., the difference between input x and
reconstruction ˆx is used as the loss to be minimized during
training of the AE.

On a trained AE, sampling arbitrary points in the latent
space, and using the decoder to construct a corresponding
image, allows for cheap image generation. This is shown in
Figure 3, where the shown images are not part of the training
data, being reconstructions based on randomly sampled points
in the latent space. In the following Section, we leverage the
generative capability of AEs, by proposing an architecture that
can target ambiguous samples speciﬁcally and can label the
generated data probabilistically (DG1).

B. AMBIGUESS

Our TIG AMBIGUESS consists of three components: (1)
The Regularized LS Generation component, which trains a
speciﬁcally designed AE to have a LS that facilitates the
generation of truly ambiguous samples. (2) The Automatic
Labelling component, which leverages the AE architecture to
support probabilistic labelling of any images produced by the
AE’s decoder. (3) The Heterogenous Sampling component,
which chooses samples in the LS in a way that leads to high
diversity of the generated images.

1) Regularized Latent Space Generation:

Interpolation
from one class to another in the latent space, i.e., the gradual
perturbation of the reconstruction by moving from one cluster
of latent space points to the another one, may produce am-
biguous samples between those two classes (satisfying both
DG2 and DG3). An example of such an interpolation is shown
in Figure 4. Clearly, we want the two clusters to be far from
each other, providing a wide range for sampling in between
them, and no other cluster should be in proximity, as it would
otherwise inﬂuence the interpolation. However,
these two
conditions are usually not met by traditional autoencoders used
in other TIG approaches. For example, Figure 3 shows the LS
of a standard variational autoencoder (a popular architecture
in TIG). Here, interpolating between classes 0 and 7 would,
amongst others, cross the cluster representing class 4, and thus
samples taken from the interpolation line would clearly not be
ambiguous between 0 and 7, but would be reconstructed as a
4 (or any of the other clusters lying between them). We solve

these requirements by using 2-class Regularized Adversarial
AEs:

them, i.e., around (0, 0). This makes reconstructions around
(0, 0) potentially highly ambiguous.

2

2-class AE: Instead of training one AE on all classes,
we train multiple AEs, each one with the training data of
just two classes. This has a range of advantages: First and
foremost, it prevents interferences with third classes. Then,
as the corresponding reduced (2-class) datasets have naturally
a lower variability (feature density), 2-class autoencoders are
expected to require fewer parameters and show faster con-
vergence during training. Further, the fact that the number of
(cid:1) grows exponentially in the number
combinations of classes (cid:0)c
of classes c is of only limited practical relevance: In very
large, real-world datasets, ambiguity is much more prevalent
between some combinations of classes than between others, so
not all pairwise combinations are equally interesting for the
test generation task. For example, let us consider a self driving
car component which classiﬁes vehicles on the road. While an
image of a vehicle where one cannot say for sure weather it is
a pick-up or a SUV (hence having true ambiguity) is clearly
a realistic case, an image which is truly ambiguous between
a SUV and a bicycle is hard to imagine. This phenomenon
is well known in the literature, as it leads to heteroscedastic
aleatoric uncertainty [40], i.e., aleatoric uncertainty which is
more prevalent amongst some classes than amongst others. In
such a case, using AMBIGUESS, one would only construct
the 2-class AEs for selected combinations where ambiguity is
realistic.

Regularized Adversarial AE (rAAE): To guide the train-
ing process towards creating two disjoint clusters represent-
ing the two classes, with an adequate amount of space be-
tween them, we use a Regularized Adversarial Autoencoder
(rAAE) [41]. The architecture of an rAAE is shown in Figure 2:
Encoder E, Decoder D and the LS are those of a standard
AE. In addition, similar to other adversarial models [42], a
discriminator Disc is trained to distinguish labelled, encoded
images z from samples drawn from a predeﬁned distribution
p(z|y). Speciﬁcally, we deﬁne p(z|y) as a multi-modal (2
classes) multi-variate (number of dimensions in latent space)
gaussian distribution, consisting of p(z|c1) and p(z|c2) for
classes c1 and c2, respectively. Then, training a rAAE consists
of three training steps, which are executed on every training
epoch: First, similar to a plain AE, E and D are trained
to reduce the reconstruction loss. Second, Disc is trained to
discriminate encoded images from samples drawn from p(z|y),
and third, E is trained to fool Disc, i.e., E is trained with
the objective that the training set projected onto the latent
space matches the distribution p(z|y). This last property can be
leveraged for ambiguous test generation: Given two classes c1
and c2, to clear up space between them in the latent space we
can choose a p(z|y) such that p(z|c1) > (cid:15) on LS points disjoint
from the LS points where p(z|c2) > (cid:15), for some small (cid:15). For
example, assume a two-dimensional latent space: Choosing
p(z|c1) = N ([−3, 0], [1, 1]) and p(z|c2) = N ([3, 0], [1, 1])
will, after successful training, lead to a latent space where
points representing c1 are clustered around (−3, 0) and points
representing c2 around (3, 0), with few if any points between

2) Probabilistic Labelling of Images: The Disc of a 2-
class rAAE can be used to automatically label the images
generated by its decoder: Given a latent space sample z∗ on
a 2-class rAAE for classes c1 and c2, Disc(z∗, c1) approx-
imates p(z∗|c1). Assuming p(c1) = p(c2) = 0.5, we have
p(z∗|c1) = p(c1|z∗). Hence, Disc(z∗, c1) approximates the
likelihood that z∗ belongs to class c1. The same holds for
Disc(z∗, c2). Normalizing these two values s.t. they add up
to 1 thus provides a probability distribution over the classes
(thus realizing DG1).

3) Selecting Diverse Samples in the LS: Diversity in a
generated dataset (see DG4) is in general hard to achieve
when generating a dataset by sampling the LS, as the distance
between two points in the LS does not directly translate to
a corresponding difference between the generated images.
While in some parts of the LS, which we denote as high
density parts, moving a point slightly in the LS space can
lead to clearly visible changes in the decoder’s output, in
low density parts, large junks of the LS lead to very similar
reconstructed images. We solve this problem by proposing
a novel way to measure the density in the LS, and sample
more points from high density regions than from low density
regions. First, we divide the relevant part of the LS (i.e., the
region lying between the distributions p(z|c1) and p(z|c2))
into a predeﬁned (large) number of small, equally sized (multi-
dimensional) grid cells, and identify a point at the center
of every grid cell, denoted as grid cell anchor. Grid cells
for which the anchor leads to insufﬁcient ambiguity, i.e., the
difference between the two class probabilities according to the
probabilistic labelling process is higher than some threshold
δmax are ignored. For the remaining anchors, we then calculate
the decoder gradient ∆s for any grid cell s at it’s anchor
point. To calculate the derivatives ∂ ˆx/∂zi (for every latent
space dimension i), we measure the difference between two
generated images, ∂ ˆx, using Euclidean distance. The norm of
the decoder gradients ||∆s|| can thus be used as a measurement
of the density at the corresponding anchor point (and, by
extension, grid cell s). Hence, to achieve diverse sampling,
we select a grid cell with a selection probability proportional
to ||∆s|| and than we choose a point within the selected grid
cell uniformly at random.

C. Pre-Generated Ambiguous Datasets

We built and released two ready-to-use ambiguous datasets
for mnist [22], the most common dataset used in software
testing literature [43], where images of handwritten numbers
between 0 and 9 are to be classiﬁed, and its more challenging
drop-in replacement Fashion mnist (fmnist) [23], consisting of
images of 10 different types of fashion items.

AMBIGUESS conﬁguration: For each pair of classes,
we trained 20 rAAEs to exploit the non-determinism of the
training process to generate even more diversiﬁed outputs.
To make sure we only use rAAEs where the distribution in
the LS is as expected, we check if the discriminator cannot

5

Top-1
Acc

Top-2
Acc

Top-Pair
Acc

Entropy

Case S.

Test Set

fmnist

mnist

ambiguous
nominal
ambiguous
nominal

0.51
0.88
0.53
0.97

0.94
0.96
0.98
0.99

0.87
nan
0.95
nan

1.33
0.35
1.22
0.15

TABLE I: Prediction Performance using nominal or ambigu-
ous test data.

than for nominal samples, as choosing the correct (i.e., higher
probability) class, even using an optimal model, is affected by
chance.
Top-2 Accuracy Percentage of inputs for which the true
label is among the two classes with the highest predicted
probability. For samples which are truly ambiguous between
two classes, we expect a well-trained model to achieve much
better performance than on Top-1 accuracy (ideally, 100%).
Top-Pair Accuracy Novel metric for data known to be
ambiguous between two classes, measured as the percentage
of inputs for which the two most likely predicted classes equal
the two true classes between which the input is ambiguous. By
deﬁnition Top-Pair accuracy is lower than or equal to Top-2
accuracy. It is an even stronger measure to show that the model
is uncertain between exactly the two classes for which the true
probabilistic label of the input shows nonzero probability.
Entropy Average entropy in the Softmax prediction arrays.
Used as a metric to measure aleatoric uncertainty (and thus
ambiguity) in related work [18].

We focus our evaluations on models trained using a mixed-
ambiguous dataset consisting of both nominal and ambiguous
data. This aims to make sure our ambiguous test sets are not
OOD, and that thus the observed uncertainty primarily comes
from the ambiguity in the data. For completeness, we also run
the evaluation on a model trained using only nominal data.
With this model we expect even lower values of regular (top-1)
accuracy on ambiguous data, as these are out-of-distribution,
not just ambiguous.

B. Quantitative Results

The results of our experiments for the model trained on
a mixed-ambiguous training set are shown in Table I. The
results for the models trained on a clean dataset can be found
in the replication package; they are in line with those in
Table I. We noticed that the use of the mixed-ambiguous
training sets reduces the model accuracy on nominal data
only by a negligible amount: On mnist, the corresponding
accuracy is 96.98% (97.42% using a clean training set) and
88.43% on fmnist (88.37% using a clean training set). Thus,
our ambiguous training datasets can be added to the nominal
ones without hesitation.

Results indicate that our datasets are indeed suitable to
induce ambiguity into the prediction process, as the generated
data is perceived as ambiguous by the DNN: Top-1 accuracies
for both case studies is around 50%, but they increase almost

Fig. 4: Interpolation between two classes in the latent space
of a 2-class Regularized Adversarial Autoencoder.

distinguish LS samples obtained from input images w.r.t. LS
samples drawn from p(z|y): the accuracy on this task should
be between 0.4 and 0.6. At the same time, we check if the
discriminator’s accuracy in assigning a higher probability to
the correct label of nominal samples is above 0.9. Otherwise
it is discarded. Combined, we used the resulting rAAEs to
draw 20,000 training and 10,000 test samples for both mnist
and fmnist, using δmax = .25 for test data and δmax = 0.4
for training data. We ignored generated samples where the
difference between the two label’s probabilities was above
δmax. We chose different δmax, (the loose upper threshold
of difference in the two class probabilities) for train and test
set as our test set should be clearly and highly ambiguous,
e.g. to allow studies that speciﬁcally target ambiguity (hence
a low δmax). In turn, the training set should more continuously
integrate with the nominal data, hence we allow for less
ambiguous data.

VI. EVALUATION OF GENERATED DATA

The goal of this experimental evaluation is to assess both
quantitatively and qualitatively whether AMBIGUESS can in-
deed generate truly ambiguous data. We evaluate the ambigu-
ity in our generated datasets ﬁrst using a quantitative analysis
where we analyze the outputs of a standard, well-trained
classiﬁer and second by visually inspecting and critically
discussing samples created using AMBIGUESS.

A. Quantitative Evaluation of AMBIGUESS

We performed our experiments using four different DNN
architectures as supervised models: A simple convolutional
DNN [44], a similar but fully connected DNN, a model
consisting of Resnet-50 [45] feature extraction and three fully
connected layers for classiﬁcation and lastly a Densenet-
architecture [46]. Results are averaged over the four archi-
tectures, individual results are reported in the reproduction
package.

We compare the predictions made for our ambiguous dataset
to the predictions made on nominal, non-ambiguous data,
using the following metrics:
Top-1 / Regular Accuracy Percentage of correctly classiﬁed
inputs. We expect this to be considerably lower for ambiguous

6

to the levels of the nominal test set when considering Top-
2 accuracies. Even Top-Pair accuracy, with values of 95.37%
and 86.71% (on mnist and fmnist, respectively) are very high,
showing that for the vast majority of test inputs, the two classes
considered most likely by the well-trained DNN are exactly
the classes between which we aimed to create ambiguity.
Consistently, entropy is substantially higher for ambiguous
data than for nominal data.

Finally, we compared our ambigous mnist dataset against
AmbiguousMNIST by Mukhoti et al. [18], the only publicly
available dataset aiming to provide ambiguous data. Results2
are clearly in favour of our dataset, which has a lower Top-
1 accuracy (53.31% vs. 72.50%), indicating that our dataset
is harder (more ambiguous) and has a higher Top-2 accuracy
(97.99% vs. 90.93%) showing that our dataset contains more
samples whose predicted class is amongst the 2 most likely
labels. Top-Pair accuracy cannot be computed for Ambigu-
ousMNIST, as 37% of its claimed “ambiguous” inputs have
non-ambiguous labels. Most strikingly, the average softmax
entropy for AmbiguousMNIST is 0.88 (ours: 1.22), even
though AmbiguousMNIST is created by actively selecting
inputs with a high softmax entropy.

C. Qualitative Discussion of AMBIGUESS

Some test samples generated using AMBIGUESS, for both
mnist and fmnist, are shown in Figure 5. They have been
chosen to highlight different strengths and weaknesses that
emerged during our qualitative manual review of 300 randomly
selected images in our generated test sets per case study.

mnist: AMBIGUESS (see Fig. 5a-e) is in general capable
of combining features of different classes, where possible: 5a
and 5c can both be seen as an 8, but the 8-shape was combined
with a 3-shape or 2-shape, respectively. For the combination
between 0 and 7, shown in 5b, only the upper (horizontal)
part of the 7 was combined with the 0-shape, such that both
a 7 and a 0 are clearly visible, making the class of the image
ambiguous. Fig. 5d shows an edge case of an almost invalid
image: Knowing that the image is supposed to be ambiguous
between 1 and 4, one can identify both numbers. However,
neither of them is clearly visible and the image may appear
invalid to some humans. Overall, we considered only few
samples generated by AMBIGUESS for mnist as bad, i.e., as
clearly unambiguous or invalid. An example of them is shown
in Fig. 5e. By most humans, this image would be recognized
as an unambiguous 0. In fact, there’s a barely visible, tilted
line within the 0 which apparently was sufﬁcient to trick the
rAAEs discriminator into also assigning a high probability to
digit 1.

fmnist: Realistic true ambiguity is not possible between
most classes of fmnist. Hence, we assessed how well AM-
BIGUESS performs at creating data that would trigger an
ambiguous classiﬁcation by humans, even though such data
might be impossible to experience in the real world. Examples
are given in Fig. 5f-j. In most cases (e.g. Fig. 5f-h), the

2Available in reproduction package.

(a) good
(3/8)

(b) good
(0/7)

(c) good
(2/8)

(d) ok
(1/4)

(e) bad
(0/1)

(f) good
(shirt/
sneaker)

(g) good
(sandal/
shirt)

(h) good
(top/
trouser)

(i) good
(sneaker/
bag)

(j) bad
(sneaker/
ankle boot)

Fig. 5: Selected good and bad outputs of AMBIGUESS,
chosen to demonstrate strengths and weaknesses.

interpolations created by AMBIGUESS show an overlay of two
items of the two considered classes, with features combined
only where possible. We can also observe that some non-
common features are removed, giving more weight to common
features. For instance, in Fig. 5i, the tip of the shoe, and the
lower angles of the bag are barely noticeable, such that the
image has indeed high similarity with both shoes and bags.
As a negative example we observe that, in some cases, it
appears that the overlay between the two considered classes is
dominated by one one of them (such as Fig. 5j, which would
be seen as non-ambiguous ankle boot by most humans).

Summary (Evaluation of AMBIGUESS-datasets)

AMBIGUESS successfully generated highly ambiguous
data sets, with high prediction entropy, top-1 accuracy
close to 50% and top-2 accuracy close to 100%, out-
performing the ambiguous dataset previously produced
by Mokhoti et al. [18].

VII. TESTING OF SUPERVISORS
We assess the capability of 16 supervisors3 to discriminate
nominal from high-uncertainty inputs for mnist and fmnist,
each on 4 distinct test sets representing different root causes
of mis-classiﬁcations, among which our ambiguous test set.

A. Experimental Setup

We performed our experiments using four different DNN
architectures (explained in subsection VI-A) as supervised
models. Our training sets consist of both nominal and am-
biguous data, to ensure that the ambiguous test data used later
for testing is in-distribution. We then measure the capability
of different supervisors to discriminate different types of high-
uncertainty inputs from nominal data. We measure this using
the area under the receiver operating characteristic curve
(AUC-ROC), a standard, threshold-independent metric.

3We are inclusive in our notion of supervisor: we consider also prioritization
techniques that recognize unexpected inputs, as it is straightforward to adopt
them to supervise a model.

7

We assess the supervisors using the following test sets:
Invalid test sets, where we use mnist images as inputs to
models trained for fashion-mnist and vice-versa, corrupted
test sets available from related work (mnist-c [19] and fmnist-
c [32]), adversarial data, created using 4 different attacks [35],
[47]–[49] and lastly the ambiguous test sets generated by AM-
BIGUESS. Adversarial test sets were not used with ensembles,
as an ensemble does not rely on the (single) model targeted
by the considered adversarial test generation techniques.

To account for random inﬂuences during training, such
as initial model weights, we ran the experiments for each
DNN architecture 5 times. Results reported are the means of
the observed results. Standard deviations are available in the
replication package and indicate that 5 repetitions are enough
to ensure sufﬁcient stability of the results.

B. Tested Supervisors

Due to limited available space, our description of the tested
supervisors is brief and we refer to the corresponding papers
for a detailed presentation. Our terminology, implementation
and conﬁguration of the ﬁrst
three supervisors described
below, i.e., Softmax, MC-Dropout and Ensembles, are based
on the material released with a recent empirical study [5].
Plain Softmax Based solely on the softmax output array of
a DNN prediction, these approaches provide very fast and
easy to compute supervision: Max. Softmax, highest softmax
value as conﬁdence [50], Prediction-Conﬁdence Score (PCS),
the difference between the two highest softmax values [37],
DeepGini,
the complement of the softmax vector squared
norm [51], and ﬁnally the entropy of the values in the predicted
softmax probabilities [52].
Monte-Carlo Dropout (MC-Dropout) [53], [54] Enabling the
randomness of dropout layers at prediction time, and sampling
multiple randomized samples allows the inference of an output
distribution, hence of an uncertainty quantiﬁcation. We use
the quantiﬁers Variation Ratio (VR), Mutual Information (MI),
Predictive Entropy (PI), or simply the highest value of the
mean of the predicted softmax likelihoods (Mean-Softmax,
MS).
Ensembles [55] Similar to MC-Dropout, uncertainty is in-
ferred from samples, but randomness is induced by training
multiple models (under random inﬂuences such as initial
weights) and collecting predictions from all of them. Here, we
use the quantiﬁers MI, PI and MS, on an Ensemble consisting
of 20 models.
Dissector [56] On a trained model, for each layer, a submodel
(more speciﬁcally, a perceptron) is trained, predicting the label
directly from the activations of the given layer. From these
outputs, the support value for each of the submodels for the
prediction made by the ﬁnal layer is calculated, and the overall
prediction validity value is calculated as a weighted average
of the per-layer support values.
Autoencoders AEs can be used as OOD detectors: If the
reconstruction error of a well-trained AE for a given input
is high,
to be sufﬁciently represented in
the training data. Stocco et. al., [2] proposed to use such

is likely not

it

OOD detection technique as DNN supervisor. Based on their
ﬁndings, we use a variational autoencoder [36].
Surprise Adequacy This approach detects inputs that are
surprising, i.e., for which the observed DNN activation pat-
tern is OOD w.r.t. the ones observed on the training data.
We consider three techniques to quantify surprise adequacy:
LSA [15], where surprise is calculated based on a kernel-
density estimator ﬁtted on the training activations of the
predicted class, MDSA [57], where surprise is calculated
based on the Mahalanobis distance between the tested input’s
activations and the training activations of the predicted class,
and DSA [15] which is calculated as the ratio between two
Euclidean distances: the distance between the tested input and
the closest training set activation in the predicted class, and the
distance between the latter activation and the closest training
set activation from another class. As DSA is computationally
intensive, growing linearly in the number of training samples,
we follow a recent proposal to consider only 30% of the
training data [58].

Our comparison includes most of the popular supervisors
used in recent software engineering literature. Some of the
excluded techniques do not provide a single, continuous un-
certainty score and no AUC-ROC can thus be calculated for
them [18], [59], [60], or they are not applicable to the image
classiﬁcation domain [8]. With its 16 tested supervisors, two
case studies and four different data-centric root causes of DNN
faults, our study is – to the best of our knowledge – by far
the most extensive of its kind.

C. Results

Results are presented in Table II.

Ambiguous Data: We can observe that

the predicted
softmax likelihoods capture aleatoric uncertainty pretty well.
Thus, not only do Max. Softmax, DeepGini, PCS, Soft-
max Entropy perform well at discriminating ambiguous from
nominal data, but also supervisors that rely on the softmax
predictions indirectly, such as Dissector, or the MS, MI and
PE quantiﬁers on samples collected using MC-Dropout or
DeepEnsembles. DSA, LSA, MDSA and Autoencoders are
not capable of detecting ambiguity, and barely any of their
AUC-ROCs exceeds the 0.5 value expected from a random
classiﬁer on a balanced dataset. MDSA, LSA and DSA show
particularly low values, which conﬁrms that they do only one
job – detecting OOD, not ambiguous data – but they do it well
(in our experimental design, ambiguous data is in-distribution
by construction, while adversarial, corrupted and invalid data
is OOD).

Adversarial Data: The surprise adequacy based super-
visors and the autoencoder reliably detected the unknown
patterns in the input, discriminating adversarial from nominal
data. Softmax-based supervisors showed good results on mnist,
but less so on fmnist. Cleary, the adversarial sample detection
capabilities of Softmax-based supervisors depend critically on
the choice of adversarial data: With minimal perturbations,
just strong enough to trigger a misclassiﬁcation, softmax-
based metrics can easily detect them, as the maximum of

8

ambiguous

adversarial

corrupted

invalid

ambiguous

adversarial

corrupted

invalid

mnist

fmnist

Max. Softmax
MC-Dropout (VR)
MC-Dropout (MS)
Deep Ensemble (MS)
Dissector
DSA
LSA
MDSA
Autoencoder

0.96
0.79
0.96
0.97
0.95
0.48
0.17
0.31
0.62

0.79
0.69
0.79
n.a.
0.79
0.93
0.78
0.94
0.95

0.78
0.65
0.80
0.84
0.76
0.87
0.73
0.87
0.84

0.79
0.72
0.80
0.85
0.79
0.98
0.77
0.98
1.00

0.91
0.76
0.91
0.90
0.88
0.31
0.16
0.32
0.53

0.61
0.62
0.61
n.a.
0.68
0.85
0.75
0.86
0.80

0.71
0.66
0.73
0.75
0.72
0.85
0.74
0.83
0.77

0.73
0.72
0.77
0.64
0.75
0.90
0.86
0.95
0.49

Seven supervisors (rows) are omitted to save space: PCS, DeepGini and Softmax Entropy perform similarly to Max-Softmax. For MC-Dropout and Ensembles,
quantiﬁers MI and PE perform similarly to MS. The full results table can be found in the reproduction package.

TABLE II: Supervisors’ performance at discriminating nominal from high-uncertainty inputs (AUC-ROC),

averaged over 20 runs (5 runs for 4 different model architectures each).

the predicted softmax likelihood is artiﬁcially reduced by the
adversarial technique being used. However, one could apply
stronger attacks, increasing the predicted likelihood of the
wrong class close to 100%, which would make Softmax-based
supervisors ineffective. Speciﬁc attacks against the other su-
pervisors, i.e., the OOD detection based approaches (surprise
adequacies and autoencoders) and Dissector, might also be
possible in theory, but they are clearly much harder.

Corrupted Data: Most approaches perform comparably
well, with the exception of DSA on fmnist, which shows
superior performance, with an average AUC-ROC value more
than .1 higher than most other supervisors. DNNs are known to
sometimes map OOD data points close to feature representa-
tions of in-distribution points (known as feature collapse) [61],
thus leading to softmax output distributions similar to the
ones of in-distribution images. This impacts negatively the
OOD detection capability of Softmax-based supervisors (such
as Max. Softmax, MC-Dropout, Ensembles or Dissector),
especially in cases with a feature-rich training set, such as
fmnist.

Invalid Data: The discussion of the supervisors’ detec-
tion of invalid data is most interesting when comparing the
two case studies. For mnist (a low-feature dataset), fmnist (a
high-feature dataset) was used as invalid dataset, and vice-
versa. It appears that the ﬁrst case is much easier to detect for
most supervisors: With the exception of LSA, all approaches
reliably discriminated invalid from nominal data with mean
AUC-ROCs close to 1.0. For fmnist, however, where invalid
inputs are characterized by lower feature complexity than the
training set, the problem becomes apparently much harder.
Similarly to corrupted data, most likely due to feature collapse,
the performance of supervisors relying on softmax likelihoods
suffers dramatically. Instead, surprise adequacies (DSA, LSA
and MDSA) showed exceptional performance, almost perfectly
discriminating the mnist-outliers from the nominal fmnist
samples. For what concerns autoencoders, reconstructions of
images with higher feature complexity than the training set
(i.e., fmnist reconstruction by an mnist-AE) consistently lead
to high reconstruction errors and thus provides a very reliable
outlier detection, with a mean AUC-ROC of 1.0. However,
an autoencoder trained on a high-feature training set would

also learn to reconstruct low-feature inputs accurately. Hence,
discrimination between mnist and fmnist using an AE trained
on fmnist led to a performance similar to the one of a random
classiﬁer.

Discussion: Related literature suggests that no single
supervisor performs well under all conditions [5], [14], [59],
and some works even suggest that certain supervisors are
not capable to detect anything but aleatoric uncertainty (e.g.
Softmax Entropy [18] or MC-Dropout [62]). Our evaluation,
to the best of our knowledge, is the ﬁrst one which compares
supervisors on four different uncertainty-inducing test sets. We
found that softmax-based approaches (including MC-Dropout,
Ensembles and Dissector) are effective on all four types of test
sets, i.e., their detection capabilities reliably exceed the perfor-
mance expected from a random classiﬁer. They do have their
primary strength in the detection of ambiguous data, where the
other, OOD focused techniques are naturally ineffective, but
they are actually an inferior choice when targeting epistemic
uncertainty. To detect corrupted inputs, DSA exhibited the best
performance, but due to its high computational complexity it
may not be suitable to all domains. The much faster MDSA
may offer a good trade-off between detection capability and
runtime complexity. Regarding invalid inputs, on low-feature
problems, where invalid samples are expected to be more com-
plex than nominal inputs, AEs provide a fast approach with
the additional advantage that it does not rely on the supervised
model directly, but only on its training set, which may facilitate
maintenance and continuous development. For problems where
the nominal inputs are rich of diverse features, an AE is not a
valid option. However, our results again suggest MDSA as a
reliable and fast alternative supervisor. For what regards inputs
created by adversarial attacks, softmax-based approaches are
easily deceived, being hence of limited practical utility. On the
other hand, OOD detectors, such as surprise adequacy metrics
and AEs, or Dissector can provide a more reliable detection
performance against standard adversarial attacks. Of course,
these supervisors are not immune from particularly malicious
attackers that target them speciﬁcally. Here, the reader can
refer to the wide range of research discussing defenses against
adversarial attacks (survey provided by Akhtar et. al. [63]).

9

Stability of Results: We found that our results are barely
sensitive to random inﬂuences due to training: Out of 488
reported mean AUC-ROCs (4 architectures, 8 test sets, 16
MPs, averaged over 5 runs) most of them showed a negligible
standard deviation: The average observed standard deviation
was 0.015, the highest one was 0.124, only 114 were larger
than 0.02, only 29 were larger than 0.05, all of which corre-
spond to results with low mean AUC-ROC (<0.9). The latter
differences do not inﬂuence the overall observed tendencies.
Summary (Comparison of Misclassiﬁcation Detec-
tors)

We assessed 16 supervisors on their capability to
discriminate between nominal inputs and inputs which
are ambiguous, adversarial, corrupted or invalid. For
every category, we identiﬁed supervisors which per-
form particularly well, but we also found that to target
all types of high-uncertainty inputs developers of DLS
will have to rely on multiple, diverse supervisors.

VIII. THREATS TO VALIDITY

External validity: we conducted our study on misclas-
siﬁcation predition considering two standard case studies,
MNIST and Fashion-MNIST. While our observations may
not generalize to more challenging, high uncertainty datasets,
the choice of two simple datasets with easily understandable
features, allowed us to achieve a clear and sharp separation
of the reasons for failures, which may not be the case when
dealing with more complex datasets. On the other hand,
we recognize the importance of replicating and extending
this study considering additional datasets. To support such
replications we provide all our experimental material as open
source/data.

Internal validity: The supervisors being compared include
hyper-parameters that require some tuning. Whenever possible,
we reused the original values and followed the guidelines
proposed by the authors of the considered approaches. We also
conducted a few preliminary experiments to validate and ﬁne
tune such hyper-parameters. However, the conﬁgurations used
in our experiment could be suboptimal for some supervisor.

Conclusion validity: We repeated our experiments 5 times
to mitigate the non determinism associated with the DNN
training process. While this might look like a low number
of repetitions, we checked the standard deviation across such
repetitions and found that it was negligible or small in all
cases. To amount for the inﬂuence of the DNN architecture,
we performed our experiments on 4 completely different DNN
architectures, obtaining overall consistent ﬁndings.

IX. CONCLUSION

This paper brings two major advances to the ﬁeld of DNN
supervision testing: First, we proposed AMBIGUESS, a novel
technique to create labeled ambiguous images in a way that
is independent of the tested model and of its supervisor, and
we generated pre-compiled ambiguous datasets for two of the

most popular case studies in DNN testing research, MNIST
and Fashion-MNIST.

Using four different metrics, we were able to verify the
validity and ambiguity of our datasets, and we further in-
vestigated how AMBIGUESS achieves ambiguity based on a
qualitative analysis. On the four considered quantitative indi-
cators, AMBIGUESS clearly outperformed AmbiguousMNIST,
the only similar-purposed dataset in the literature.

We assessed the capabilities of 16 DNN supervisors at dis-
criminating nominal from ambiguous, adversarial, corrupted
this is
and invalid inputs. To the best of our knowledge,
not only the largest empirical case study comparing DNN
supervisors in the literature, it is also the ﬁrst one to do so
by speciﬁcally targeting four distinct and clearly separable
data-centric root causes of DNN faults. Our results show
that softmax-based approaches (including MC-Dropout and
Ensembles) work very well at detecting ambiguity, but have
clear disadvantages when it comes to adversarial, corrupted,
and invalid inputs. OOD detection techniques, such as surprise
adequacy or autoencoder-based supervisors, often provide a
better detection performance with the targeted types of high-
uncertainty inputs. However, these approaches are incapable
of detecting in-distribution ambiguous inputs.

DNN developers can use the ambiguous datasets created
by AMBIGUESS to assess novel DNN supervisors on their
capability to detect aleatoric uncertainty. They can also use
our tool to evaluate test prioritization approaches on their
capability to prioritize ambiguous inputs (depending on the
developers’ objectives, high priority is desired to identify
inputs that are likely to be misclassiﬁed during testing; low
priority is desired to exclude inputs with unclear ground truth
from the training set).

i.e.,

to look at

As future work, we plan to investigate the concept of true
ambiguity for regression problems,
inputs
to regression problems with high variability in their ground
truth. This is relevant in domains, such as self-driving cars
and robotics, where the DNN output is a continuous signal
for an actuator. This problem is particularly appealing as all
the approaches in our study that worked well at detecting
ambiguity are based on softmax and thus are not applicable
to regression problems.

X. DATA AVAILABILITY

Upon acceptance of this paper, we will release the generated
datasets using a range of common formats and through dif-
ferent platforms (such as Huggingface-datasets [64], Github
and Zenodo). We will license our artifacts as permissively
as the underlying datasets allow: Our code will be MIT,
our ambigous-mnist dataset will be CC-BY-SA and our
ambiguous-fashion-mnist dataset will be CC-BY licensed.

While this paper is under review, the data will be made

available upon request.

REFERENCES

[1] L.
ing

Aroyo
unknown

and

P.
unknowns

Paritoshs.
in

(2021,

machine

Feb.)
learning.

Uncover-
Google

10

Research.
uncovering-unknown-unknowns-in-machine.html

[Online]. Available:

https://ai.googleblog.com/2021/02/

[2] A. Stocco, M. Weiss, M. Calzana, and P. Tonella, “Misbehaviour
prediction for autonomous driving systems,” in Proceedings of 42nd
International Conference on Software Engineering. ACM, 2020, p. 12
pages.

[3] J. Henriksson, C. Berger, M. Borg, L. Tornberg, C. Englund, S. R.
Sathyamoorthy, and S. Ursing, “Towards structured evaluation of deep
neural network supervisors,” in 2019 IEEE International Conference On
Artiﬁcial Intelligence Testing (AITest).

IEEE, apr 2019.

[4] J. Henriksson, C. Berger, M. Borg, L. Tornberg, S. R. Sathyamoorthy,
and C. Englund, “Performance analysis of out-of-distribution detection
on various trained neural networks,” in 2019 45th Euromicro Conference
on Software Engineering and Advanced Applications (SEAA).
IEEE,
2019, pp. 113–120.

[5] M. Weiss and P. Tonella, “Fail-safe execution of deep learning based
systems through uncertainty monitoring,” in 2021 IEEE 14th Inter-
national Conference on Software Testing, Validation and Veriﬁcation
(ICST). IEEE, 2021.

[6] F. O. Catak, T. Yue, and S. Ali, “Prediction surface uncertainty quan-
tiﬁcation in object detection models for autonomous driving,” 2021.
[7] F. Hell, G. Hinz, F. Liu, S. Goyal, K. Pei, T. Lytvynenko, A. Knoll, and
C. Yiqiang, “Monitoring perception reliability in autonomous driving:
Distributional shift detection for estimating the impact of input data on
prediction accuracy,” in Computer Science in Cars Symposium, 2021,
pp. 1–9.

[8] M. Hussain, N. Ali, and J.-E. Hong, “Deepguard: a framework for
safeguarding autonomous driving systems from inconsistent behaviour,”
Automated Software Engineering, vol. 29, no. 1, pp. 1–32, 2022.
[9] P. Wintersberger, F. Janotta, J. Peintner, A. L¨ocken, and A. Riener,
“Evaluating feedback requirements for trust calibration in automated
vehicles,” it-Information Technology, vol. 63, no. 2, pp. 111–122, 2021.
[10] M. S. Davidson, C. Andradi-Brown, S. Yahiya, J. Chmielewski, A. J.
O’Donnell, P. Gurung, M. D. Jeninga, P. Prommana, D. W. Andrew,
M. Petter et al., “Automated detection and staging of malaria parasites
from cytological smears using convolutional neural networks,” Biologi-
cal imaging, vol. 1, 2021.

[11] J. M. Brown and G. Leontidis, “Deep learning for computer-aided
in Neural

diagnosis in ophthalmology: a review,” State of
Networks and their Applications, pp. 219–237, 2021.

the Art

[12] S. Bjarnadottir, Y. Li, and M. G. Stewart, “Climate adaptation for
housing in hurricane regions,” in Climate Adaptation Engineering.
Elsevier, 2019, pp. 271–299.

[13] D. Berend, X. Xie, L. Ma, L. Zhou, Y. Liu, C. Xu, and J. Zhao, “Cats are
not ﬁsh: Deep learning testing calls for out-of-distribution awareness,” in
The 35th IEEE/ACM International Conference on Automated Software
Engineering.
New York, NY, USA: Association for Computing
Machinery, 2020.

[14] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, “Deeproad:
Gan-based metamorphic testing and input validation framework for
autonomous driving systems,” in Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, ser. ASE
2018. New York, NY, USA: ACM, 2018, pp. 132–142. [Online].
Available: http://doi.acm.org/10.1145/3238147.3238187

[15] J. Kim, R. Feldt, and S. Yoo, “Guiding deep learning system testing

using surprise adequacy,” 2018.

[16] S. Kim and S. Yoo, “Multimodal surprise adequacy analysis of inputs
for natural language processing dnn models,” in 2021 2021 IEEE/ACM
International Conference on Automation of Software Test (AST) (AST).
Los Alamitos, CA, USA: IEEE Computer Society, may 2021, pp.
80–89. [Online]. Available: https://doi.ieeecomputersociety.org/10.1109/
AST52587.2021.00017

[17] S. Dola, M. B. Dwyer, and M. L. Soffa, “Distribution-aware testing of

neural networks using generative models,” pp. 226–237, 2021.

[18] J. Mukhoti, A. Kirsch, J. van Amersfoort, P. H. S. Torr, and Y. Gal,
“Deterministic neural networks with appropriate inductive biases capture
epistemic and aleatoric uncertainty,” 2021.

[19] N. Mu and J. Gilmer, “Mnist-c: A robustness benchmark for computer

vision,” CoRR, 2019.

[20] D. Hendrycks and T. Dietterich, “Benchmarking neural network robust-
ness to common corruptions and perturbations,” International Confer-
ence on Learning Representations, 2018.

Reliable Machine Learning in the Wild Workshop, 34th International
Conference on Machine Learning, 2017. [Online]. Available: http:
//arxiv.org/abs/1707.04131

[22] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[23] H. Xiao, K. Rasul, and R. Vollgraf. (2017) Fashion-mnist: a novel image

dataset for benchmarking machine learning algorithms.

[24] D. Seca, “A review on oracle issues in machine learning,” arXiv preprint

arXiv:2105.01407, 2021.

[25] N. Humbatova, G. Jahangirova, G. Bavota, V. Riccio, A. Stocco,
and P. Tonella, “Taxonomy of real faults in deep learning systems,”
in Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering, 2020, pp. 1110–1121.

[26] H. Karimi, T. Derr, and J. Tang, “Characterizing the decision boundary

of deep neural networks,” 2019.

[27] S. Kang, R. Feldt, and S. Yoo, “Sinvad: Search-based image space nav-
igation for dnn image classiﬁer test input generation,” in Proceedings of
the IEEE/ACM 42nd International Conference on Software Engineering
Workshops, 2020, pp. 521–528.

[28] T. Byun and S. Rayadurgam, “Manifold for machine learning assurance,”
in Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering: New Ideas and Emerging Results, 2020, pp. 97–
100.

[29] V. Riccio and P. Tonella, “Model-based exploration of the frontier of
behaviours for deep learning system testing,” in Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, 2020, pp. 876–
888.

[30] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing
of deep-neural-network-driven autonomous cars,” in Proceedings of the
40th international conference on software engineering, 2018, pp. 303–
314.

[31] I. Dunn, H. Pouget, D. Kroening, and T. Melham, “Exposing previously
undetectable faults in deep neural networks,” in Proceedings of the
30th ACM SIGSOFT International Symposium on Software Testing and
Analysis, 2021, pp. 56–66.

[32] M. Weiss and P. Tonella, “Simple techniques work surprisingly well
for neural network test prioritization and active learning (replicability
study),” in Proceedings of
the 31st ACM SIGSOFT International
Symposium on Software Testing and Analysis, ser. ISSTA 2022. New
York, NY, USA: Association for Computing Machinery, 2022, p.
139–150. [Online]. Available: https://arxiv.org/abs/2205.00664

[33] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao,
B. Li, J. Yin, and S. See, “Deephunter: a coverage-guided fuzz testing
framework for deep neural networks,” in Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
2019, pp. 146–157.

[34] A. Odena, C. Olsson, D. Andersen, and I. Goodfellow, “TensorFuzz:
Debugging neural networks with coverage-guided fuzzing,”
in
Proceedings of the 36th International Conference on Machine Learning,
ser. Proceedings of Machine Learning Research, K. Chaudhuri and
Long Beach, California, USA:
R. Salakhutdinov, Eds., vol. 97.
PMLR, 09–15 Jun 2019, pp. 4901–4911.
[Online]. Available:
http://proceedings.mlr.press/v97/odena19a.html

[35] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.

[36] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv

preprint arXiv:1312.6114, 2013.

[37] X. Zhang, X. Xie, L. Ma, X. Du, Q. Hu, Y. Liu, J. Zhao, and M. Sun,
“Towards characterizing adversarial defects of deep learning software
from the lens of uncertainty,” in Proceedings of 42nd International
Conference on Software Engineering. ACM, 2020.

[38] T. P. Trappenberg and A. D. Back, “A classiﬁcation scheme for ap-
plications with ambiguous data,” in Proceedings of the IEEE-INNS-
ENNS International Joint Conference on Neural Networks. IJCNN 2000.
Neural Computing: New Challenges and Perspectives for the New
Millennium, vol. 6.
IEEE, 2000, pp. 296–301.

[39] W. Samek, T. Wiegand, and K.-R. M¨uller, “Explainable artiﬁcial in-
telligence: Understanding, visualizing and interpreting deep learning
models,” arXiv preprint arXiv:1708.08296, 2017.

[21] J. Rauber, W. Brendel, and M. Bethge, “Foolbox: A python toolbox
to benchmark the robustness of machine learning models,” in

[40] M. S. Ayhan and P. Berens, “Test-time data augmentation for estima-
tion of heteroscedastic aleatoric uncertainty in deep neural networks,”

11

[63] N. Akhtar, A. Mian, N. Kardan, and M. Shah, “Advances in adversarial
attacks and defenses in computer vision: A survey,” IEEE Access, vol. 9,
pp. 155 161–155 196, 2021.

[64] Q. Lhoest, A. Villanova del Moral, Y. Jernite, A. Thakur, P. von Platen,
S. Patil, J. Chaumond, M. Drame, J. Plu, L. Tunstall, J. Davison,
M. ˇSaˇsko, G. Chhablani, B. Malik, S. Brandeis, T. Le Scao, V. Sanh,
C. Xu, N. Patry, A. McMillan-Major, P. Schmid, S. Gugger, C. Delangue,
T. Matussi`ere, L. Debut, S. Bekman, P. Cistac, T. Goehringer, V. Mustar,
F. Lagunas, A. Rush, and T. Wolf, “Datasets: A community library for
natural language processing,” in Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing: System
Demonstrations. Online and Punta Cana, Dominican Republic:
Association for Computational Linguistics, Nov. 2021, pp. 175–184.
[Online]. Available: https://aclanthology.org/2021.emnlp-demo.21

Presented at ”Medical Imaging with Deep Learning 2018”, Amsterdam,
Jul. 2018, available on OpenReview.

[41] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, “Adver-

sarial autoencoders,” arXiv preprint arXiv:1511.05644, 2015.

[42] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”
Advances in neural information processing systems, vol. 27, 2014.
[43] V. Riccio, G. Jahangirova, A. Stocco, N. Humbatova, M. Weiss, and
P. Tonella, “Testing machine learning based systems: a systematic
mapping,” Empirical Software Engineering, 2020.

[44] F. Chollet, “Keras documentation: Simple mnist convnet,” Apr 2020.
[Online]. Available: https://keras.io/examples/vision/mnist convnet/
[45] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[46] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2017, pp. 4700–4708.
[47] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” arXiv preprint
arXiv:1706.06083, 2017.

[48] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples
in the physical world,” in Artiﬁcial intelligence safety and security.
Chapman and Hall/CRC, 2018, pp. 99–112.

[49] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in Proceedings of
the IEEE conference on computer vision and pattern recognition, 2016,
pp. 2574–2582.

[50] D. Hendrycks and K. Gimpel, “A baseline for detecting misclassiﬁed

and out-of-distribution examples in neural networks,” 2016.

[51] Y. Feng, Q. Shi, X. Gao, J. Wan, C. Fang, and Z. Chen, “Deepgini:
prioritizing massive tests to enhance the robustness of deep neural
networks,” in Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis, 2020, pp. 177–188.
[52] M. Weiss and P. Tonella, “Uncertainty-wizard: Fast and user-friendly
neural network uncertainty quantiﬁcation,” in 2021 14th IEEE Confer-
ence on Software Testing, Veriﬁcation and Validation (ICST), 2021, pp.
436–441.

[53] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation:
Representing model uncertainty in deep learning,” in Proceedings of the
33rd International Conference on International Conference on Machine
Learning - Volume 48, ser. ICML’16. JMLR.org, 2016, pp. 1050–1059.
[Online]. Available: http://dl.acm.org/citation.cfm?id=3045390.3045502
[54] Y. Gal, “Uncertainty in deep learning,” Ph.D. dissertation, University of

Cambridge, 2016.

[55] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable
predictive uncertainty estimation using deep ensembles,” in Advances in
neural information processing systems, 2017, pp. 6402–6413.

[56] H. Wang, J. Xu, C. Xu, X. Ma, and J. Lu, “Dissector: Input validation for
deep learning applications by crossing-layer dissection,” in Proceedings
of 42nd International Conference on Software Engineering.
ACM,
2020.

[57] J. Kim, J. Ju, R. Feldt, and S. Yoo, “Reducing dnn labelling cost using
surprise adequacy: An industrial case study for autonomous driving,”
in Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering, 2020, pp. 1466–1476.

[58] M. Weiss, R. Chakraborty, and P. Tonella, “A review and reﬁnement of
surprise adequacy,” in 2021 IEEE/ACM Third International Workshop on
Deep Learning for Testing and Testing for Deep Learning (DeepTest).
IEEE, 2021, pp. 17–24.

[59] F. O. Catak, T. Yue, and S. Ali, “Uncertainty-aware prediction validator
in deep learning models for cyber-physical system data,” ACM Trans-
actions on Software Engineering and Methodology, 2021.

[60] J. Postels, H. Blum, C. Cadena, R. Siegwart, L. Van Gool, and
F. Tombari, “Quantifying aleatoric and epistemic uncertainty using
density estimation in latent space,” arXiv preprint arXiv:2012.03082,
2020.

[61] J. van Amersfoort, L. Smith, A. Jesson, O. Key, and Y. Gal, “On feature
collapse and deep kernel learning for single forward pass uncertainty,”
arXiv preprint arXiv:2102.11409, 2021.

[62] I. Osband, “Risk versus uncertainty in deep learning: Bayes, bootstrap
and the dangers of dropout,” in NIPS workshop on bayesian deep
learning, vol. 192, 2016.

12

