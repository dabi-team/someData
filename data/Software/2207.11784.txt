2
2
0
2

t
c
O
6

]
E
S
.
s
c
[

2
v
4
8
7
1
1
.
7
0
2
2
:
v
i
X
r
a

CARGO: AI-Guided Dependency Analysis for Migrating
Monolithic Applications to Microservices Architecture

Vikram Nitin‚àó
vikram.nitin@columbia.edu
Columbia University
New York, USA

Baishakhi Ray
rayb@cs.columbia.edu
Columbia University
New York, USA

Shubhi Asthana
sasthan@us.ibm.com
IBM Research
California, USA

Rahul Krishna‚Ä†
rkrsn@ibm.com
IBM Research
New York, USA

ABSTRACT
Microservices Architecture (MSA) has become a de-facto standard
for designing cloud-native enterprise applications due to its efficient
infrastructure setup, service availability, elastic scalability, depend-
ability, and better security. Existing (monolithic) systems must be
decomposed into microservices to harness these characteristics.
Since manual decomposition of large scale applications can be labo-
rious and error-prone, AI-based systems to decompose applications
are gaining popularity. However, the usefulness of these approaches
is limited by the expressiveness of the program representation and
their inability to model the application‚Äôs dependency on critical ex-
ternal resources such as databases. Consequently, partitioning rec-
ommendations offered by current tools result in architectures that
result in (a) distributed monoliths, and/or (b) force the use of (often
criticized) distributed transactions. This work attempts to over-
come these challenges by introducing CARGO (short for Context-
sensitive lAbel pRopaGatiOn)‚Äîa novel un-/semi-supervised parti-
tion refinement technique that uses a context- and flow-sensitive
system dependency graph of the monolithic application to refine
and thereby enrich the partitioning quality of the current state-
of-the-art algorithms. CARGO was used to augment four state-
of-the-art microservice partitioning techniques (comprised of 1
industrial tool and 3 open-source projects). These were applied on
five Java EE applications (comprised of 1 proprietary and 4 open
source projects). Experiments show that CARGO is capable of im-
proving the partition quality of all four partitioning techniques.
Further, CARGO substantially reduces distributed transactions, and
a real-world performance evaluation of a benchmark application
(deployed under varying loads) shows that CARGO also lowers the
overall the latency of the deployed microservice application by 11%
and increases throughput by 120% on average.

‚àóThis work was done when the author was an intern at IBM Research.
‚Ä†Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9475-8/22/10. . . $15.00
https://doi.org/10.1145/3551349.3556960

CCS CONCEPTS
‚Ä¢ Computer systems organization ‚Üí Cloud computing.

KEYWORDS
Microservice Architecture, Community Detection, Database Trans-
action.

ACM Reference Format:
Vikram Nitin, Shubhi Asthana, Baishakhi Ray, and Rahul Krishna. 2022.
CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Ap-
plications to Microservices Architecture. In 37th IEEE/ACM International
Conference on Automated Software Engineering (ASE ‚Äô22), October 10‚Äì14,
2022, Rochester, MI, USA. ACM, New York, NY, USA, 12 pages. https://doi.
org/10.1145/3551349.3556960

1 INTRODUCTION
Businesses are gradually migrating their existing applications to the
cloud as their enterprise applications outgrow their monolithic ar-
chitectures. This allows them to leverage better scalability and relia-
bility, faster development, easier maintenance and deployment, and
better fault isolation, among others [51]. This migration requires a
decomposition of monolithic applications into loosely connected
compositions of specialized microservices [17, 42, 43]. However,
modernizing an enterprise application with entrenched technology
stacks is a challenging task. Manual approaches to modernization
consume a significant amount of engineering time and resources [2]
and a complete rebuild is rarely feasible. Therefore, big enterprises
such as IBM [29], Amazon [3], and GitHub [2] frequently advocate
for partitioning applications by identifying functional boundaries
in the code that may be extracted as microservices. This has led to
a rapid growth of research in using program analysis to automat-
ically discover these functional boundaries and partitions within
the application [5, 8, 11, 15, 19, 22, 32, 34, 38].
Existing work and limitations. A typical microservice extrac-
tion approach starts by converting a monolithic application into a
call-graph or control-flow graph representation. Then, an off-the-
shelf graph clustering algorithm is used to find loosely connected
functional boundaries and make partitioning recommendations.
The performance of these graph clustering algorithms is directly
influenced by the expressiveness and completeness of the graphs
they use. Unfortunately, the program graphs used by current tech-
niques are ill-suited to adequately model enterprise applications.
The key limitations of current approaches include:

 
 
 
 
 
 
ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

Nitin, V., et al.

‚Ä¢ Missing database interactions: Existing AI-guided microservice
partitioning methods miss the interconnections between applica-
tions and databases. Transactional databases, which are stored and
maintained centrally, are typically used by monolithic enterprise
applications to execute and persist key business logic. If these inter-
actions are not taken into account, the partitioning guidelines could
force the adoption of distributed transactions (such as 2 Phase com-
mits). This represents a considerable effort for developer who must
then ensure that state changes are coordinated across several ser-
vices. Further, distributed transactions pose other challenges such
as: a) lack of consistency in the data; b) probable safety violations;
and c) loss of ACID isolation guarantees [23, 31, 44].
‚Ä¢ Imprecise program analysis: Popular approaches such as CoGCN
[12] and microservice extractor 1 use context-insensitive static-
analysis which tends to be incomplete and imprecise. Further, these
techniques use off-the-shelf static analysis tools such as SOOT [33]
and WALA 2 which are known to ignore crucial features used by
enterprise java applications such as dependency injection [18], and
centralized object store [27], among others. This often results in a
sparse coverage of several critical sections of the application [6]. To
overcome this challenge, certain approaches construct a dynamic
call graph by instrumenting the program to amass light-weight
traces as the application‚Äôs many use-cases are exercised [25, 29].
However, for dynamic analysis to be useful, it must be run with
sufficient inputs to expose as many different program behaviors
as possible, and this can be hard to satisfy. Further, building call-
graphs with dynamic tracing has additional drawbacks‚Äîit can:
(a) be intrusive, (b) lead to considerable performance degradation,
and (c) produce an incomplete representation of the application.
Our approach. To address the above challenges, we propose a
microservice partitioning and refinement tool that statically an-
alyzes JEE applications to build a comprehensive, highly-precise,
context-sensitive system dependency graph (SDG). Our SDG is a
rich graphical abstraction of a Java EE application with various cru-
cial relationships that exist in the application, namely: call-return
edges, data-flow edges, heap-dependency edges, and database trans-
action edges. Our work also proposes a novel community detection
algorithm called Context-sensitive lAbel pRopaGatiOn (abbrevi-
ated as CARGO, serving as an acronym for our general tool) that
isolates ‚Äúsnapshots‚Äù of the system dependency graph under vari-
ous contexts to discover and/or to refine existing partitions into
highly-cohesive and loosely-coupled compositions of the program
which can be implemented as microservices.
Results summary. We evaluate CARGO on 5 monolithic appli-
cations (4 open source and 1 proprietary) a suite of architectural
metrics. Further, using CARGO‚Äôs partitions, we modernize a bench-
mark monolithic application (daytrader) to evaluate its real-world
performance in terms of latency and throughput. Our key findings
are listed below:
‚Ä¢ CARGO is able to almost eliminate distributed database transac-
tions, both when applied in conjunction with a baseline approach
and when applied stand-alone.

‚Ä¢ We use CARGO to refine the partitions produced by a baseline
approach, and find that the resulting microservice application has
11% less latency and 120% more throughput than the baseline.
‚Ä¢ When we compare CARGO to baseline partitioning approaches
on 4 architectural metrics, CARGO is better than the baselines on 3
metrics and inferior on 1 metric.
Contributions. CARGO makes the following contributions to im-
prove the current state of the art:
(1) CARGO builds a highly precise statically derived system depen-
dency graph (SDG) with full context and flow sensitivity (detailed
in ¬ß3). The nodes of the SDG represent the methods and database ta-
bles in the application. The various inter-procedural edges express
different types of program dependencies between one method and
another, and transactional edges highlight relationships between
methods and database tables. Together, the SDG constructed by
CARGO offers a fine-granular view of an application that includes
all its internal and external (i.e., database) relationships.
(2) CARGO introduces a novel partitioning algorithm based on
label propagation (aka. LPA) [54] called context-sensitive label prop-
agation. Context-sensitive label propagation (detailed in ¬ß3) extracts
‚Äúcontext-snapshots‚Äù of the SDG, i.e. abstractions of the possible dy-
namic states of the program [40], and applies label propagation
across all contexts to identify the best functional boundaries in
code.
(3) CARGO offers multiple modes of usage: (i) as a stand-alone tool,
wherein the partitions are discovered in an unsupervised manner;
or (ii) as an add-on to existing tools, wherein the existing partitions
are refined to improve their overall quality.
(4) CARGO and the associated artifacts will soon be made publicly
available on Github as part of the Konveyor Tackle DGI project. 3

2 BACKGROUND AND MOTIVATION
A recent report shows that only 20% of the enterprise workloads are
in the cloud, and they were predominately written for native cloud
architectures. This leaves 80% of applications as monoliths deployed
on-premises, waiting to be refactored and modernized for the cloud.
In a monolithic architecture the entire application is bundled into a
single package in a monolithic architecture, which includes source
code, libraries, configurations, and any other dependencies required
for it to execute. Monoliths aren‚Äôt intrinsically bad, although they
do have several drawbacks: (a) Deployment artifacts are often huge,
sluggish to start, and consume a substantial amount of resources;
(b) changing one component of the program necessitates rebuild-
ing and redeploying the entire application, which slows developer
productivity‚Äîa challenge that triggered GitHub to migrate away
from monoliths to a microservice architecture4.

In comparison to monoliths, microservices split an application
into discrete units that have clearly defined service boundaries. This
offers a number of benefits such as: continuous delivery of large and
complex applications, small and easily maintainable applications,
low resource-intensiveness, independently deployable and scalable
applications, easy fault isolation, and autonomy in development.

1https://aws.amazon.com/microservice-extractor/
2https://github.com/wala/WALA

3https://github.com/konveyor/tackle-data-gravity-insights
4https://www.infoq.com/articles/github-monolith-microservices/

CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Applications to Microservices Architecture

ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

The migration from monolith to a microservice architecture is
no mean feat. Even a small, seemingly trivial, upgrade to a mono-
lithic software can be time consuming and resource intensive. For
example, a rails upgrade to a 10 year old system at GitHub took
a year and a half to complete [2]. Therefore, the best strategy to
migrate monolithic applications to microservices is to do so incre-
mentally by identifying boundaries in the application such that the
functionalities encompassed by them are highly cohesive, yet as
loosely coupled as possible to other functionalities in the code.

The study of identifying these boundaries has seen a lot of in-
terest lately with prominent industry tools such as Mono2Micro5
and several others in academia [5, 8, 11, 15, 19, 22, 29, 32, 34, 38].
However, when confronted with enterprise applications, each of
these approaches faces some difficulties. We will look at a common
motivating scenario below and the rest of this paper attempts to
address this problem.

2.1 Motivation
In this section, we use a real-world example to highlight the chal-
lenges that may arise when attempting to partitioning a monolith.
We use a state-of-the-art microservice partitioning algorithm to
partition a monolithic version of a popular JEE trading applica-
tion called daytrader6. Fig. 1 shows an example with three classes
from daytrader: QuoteDatabean, TradeDirect, and TradeConfig.
Fig. 1a shows the most common partitioning recommendation of
these three classes where, partition A contains QuoteDatabean
and partition B contains TradeDirect and TradeConfig.

At first glance, this may seem like a reasonable partitioning
strategy (especially when inspecting the call-edges between the
classes in Fig. 1a), but a closer look (as seen in Fig. 1b) paints a very
different picture. Several challenges will arise if a developer tried
to partition according to the recommendations made by current
approaches, as in Fig. 1a. Some of these are described below:
Challenge #1: Services are tightly coupled leading to distributed
monolithic behavior. Coupling refers to the degree of separabil-
ity between two classes. There exists a tight coupling between
the QuoteDatabean and TradeDirect, although not apparent in
Fig. 1a, due to heap-dependencies induced by SQLHandler and
TradeObject. These objects carry the payload of information ex-
change between the two classes that is crucial to execute the busi-
ness functionality. If partitioned according to Fig. 1a, one would
have to rely on serialization/de-serialization protocols as well as
tightly choreographed object state management to ensure the busi-
ness logic is executed accurately. This makes the partition behave
more like a monolith that is deployed as microservices (aka. a dis-
tributed monolith) rather than a true microservice application.
Challenge #2: Services engage in transactions to the same data-
base table. If partitioned according to Fig. 1a, we would be forced
to reconcile a distributed transaction between QuoteDatabean,
TradeDirect, and the database table QuoteDB (as is seen in Fig. 1b).
This would require a developer to implement 2 Phase commits
(or 2PCs) to coordinate the state change across QuoteDatabean,
TradeDirect. The use of 2PCs can be very problematic due a num-
ber of reasons including but not limited to (1) Loss of atomicity

(a) A typical partitioning recommendation for two important
classes in Daytrader offered by FoSCI and CoGCN.

(b) Recommended partitioning considering the the high coupling
between QuoteDataBean and TradeDirect.

Figure 1: An example from Daytrader that illustrates a dis-
tributed monolithic behavior. The two classes QuoteDatabean and

TradeDirect lie in different partitions, yet they are tightly coupled
by their: a) shared references to TradeObject and SQLConnector; and
b) part-taking in a distributed transaction with QuoteDB .

and/or isolation guarantees, (2) Non-trivial coordination of dis-
tributed locks, and (3) Very high latency. Therefore they must be
avoided if possible lest one is forced to rely on expensive data
centers and meticulously synchronized atomic clocks 7.

We note that these challenges are very common, and appear often
in enterprise scale systems. As a consequence, the recommendations
from certain microservice partitioning tools are often rendered
useless due to the potential ramifications of implementing their
recommendations. This inspired the construction of CARGO, where
we attempt to refine the recommendations of existing approaches
to reduce incidents of distributed monoliths and/or distributed
transactions. The partitions in Fig. 1b were derived from CARGO.
Here, QuoteDatabean and TradeDirect have been grouped in the
same partition, essentially overcoming the above challenges.

3 METHODOLOGY
This section describes CARGO and how it identifies (and/or refines)
microservice partitions in monolithic enterprise Java applications
to obtain loose coupling between services and high cohesion within
the services. Fig. 2 illustrates an end-to-end workflow of the pro-
posed approach. In the first stage (¬ß3.1), we build a context and flow
sensitive system dependence graph (SDG) of an enterprise applica-
tion which has methods and DB tables as nodes with various edges

5https://www.ibm.com/cloud/blog/announcements/ibm-mono2micro
6https://github.com/WASdev/sample.daytrader7

7https://www.youtube.com/watch?v=iKQhPwbzzxU

QuoteDatabeanTradeConÔ¨ÅgABTradeDirectQuoteDatabeanTradeConÔ¨ÅgABTradeDirectCall EdgeHeap EdgeTransaction EdgeTradeObjectSQLHandlerQuoteDBASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

Nitin, V., et al.

Figure 2: Overview of CARGO.

between them such as call-return edges, data-dependency edges,
heap-dependency edges, and transactional edges. In the second
stage (¬ß3.2), we extract sub-graphs of the SDG under each context
and across all the transactional scopes. We refer to these as con-
textual snapshots. In the final stage (¬ß3.2.3), we apply a novel label
propagation algorithm across each contextual snapshot to group
functionally similar components of the program together.

3.1 Building a System Dependency Graph
In this section, we describe the construction of a System Depen-
dence Graph (SDG) [16, 24], which is an instrumental component
of CARGO. The SDG captures the semantics of the whole program
and models different program contexts explicitly. This makes it par-
ticularly well-suited to model large enterprise applications, since
context-sensitive analyses offer a precise way to distinguish dif-
ferent invocations of the application without need for expensive
dynamic tracing. The rest of this section first describes context-
sensitivity followed by the various dependency (edge) types in the
SDG (¬ß3.1.2).

3.1.1 Context sensitivity. Context-sensitivity dictates that every
relationship in the SDG is qualified by a context. This ensures that
different calls/information flows across a method are annotated
with a unique identifier. This enables us to distinguish between
different paths through the same section of the code, thereby en-
hancing the precision and richness of our SDG.

Consider the sample program in Fig. 3: here, in lines 2 and 5, we
allocate two objects a1 and a2 of type A, and for each of these objects,
we call the class method A.foo(. . .) which in turn calls B.bar(. . .)
(on line 12) and C.id(. . .) (on line 19). A context-insensitive view of
this program as seen in 1 , fails to distinguish between two distinct
calls to B.bar(. . .). Whereas, a context-sensitive analysis as seen
in 2 distinguishes between the call arising from a1.foo(. . .) and
a2.foo(. . .) as separate edges.

Contexts may be thought of as abstractions of the possible dy-
namic executions of a program, i.e., a method and its associated

Figure 3: Difference between a context-sensitive and a con-
text insensitive call graph. Context-insensitive analysis can-
not differentiate between two calls to the same method,
context-sensitive analysis uses object allocation sites (high-
lighted in red) to distinguish between calls.

variables under two different contexts represent non-overlapping
program states that may be reached in one or more dynamic runs
of the program. The main flavors of context sensitivity are call-site-
sensitivity [47, 48], object-sensitivity [40], and type-sensitivity [50].
Of these, object-sensitivity has been shown to be remarkably precise
for analyzing object-oriented programs like Java [7, 50]. Contexts
are qualified using the object allocation site the current method
was invoked on, e.g., in Fig. 3 contexts are qualified at A/1, A/2,
B/1, and C/1. Thus, the contexts of two invocations may differ even
if their call-site was the same but the object that the method was
called on was different. For example, in 2 , B.bar(. . .) appears in
two contexts, even though it has the same call-site (on line 11).

MonolithDB1DB2DB1DB2PDG DB1DB2Extract PDGExtract SnapshotsTransactionsSnaphotTransactionsSnaphotContext SnaphotsContext 1Context 2Context NStage 1: Extract Context Sensitive Program dependency graph (PDG)Stage 3: Propogate labels across nodes with DB transactions and through each context snaphot iteratively Microservice BoundariesStage 2: Generate context-/transactional-snapshots from the PDGSeed Lables*LPALPALPALPA*optionalContext 1Context 2Context NInitial LPA to eleminate distributed transactions Object()); Object());1void  main  (Object    []    args){2Aa1=newA(); // A/13Objectv1 = a1. foo     (newObjectv2 = a2. foo     (new45Aa2=new A(); // A/267}89classA {10object foo  (Objectv){11Bb=new B(); // B/112return b. bar (v);13}14}1516class B {17Object bar (Objectv){18Cc=new C(); // C/119return c. id (v);20}21}2223class C {24object id  (Objectv){     return      v; }25}main()A.foo()B.bar()C.id()main()A.foo()B.bar()A.foo()B.bar()C.id()Context Insensitive[‚àÖ,A/1][A/1,B/1][‚àÖ,A/2][A/2,B/1][B/1,C/1][‚àÖ,‚àÖ]Context-Sensitive (2ObjH)21CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Applications to Microservices Architecture

ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

In this paper, we use 2-object-sensitive-analysis with a context-
sensitive heap (2objH) for all our analysis as this has been consid-
ered the gold-standard for scalable yet precise analysis [7, 35, 36].
Although even more precise analyses are available, and can be de-
veloped, they seldom scale when applied to enterprise JAVA appli-
cations [7] 8. A crucial property of the object-sensitivity analysis is
the inherent flow-sensitivity imparted by the contexts. As illustrated
in Fig. 3, each object allocation site is annotated with a numeric
identifier (e.g., A/1, A/2). These identifiers indicate the flow of the
program, with A/1 and all its subsequent contexts appearing be-
fore A/2 and all the contexts that follow. This facilitates accurate
reasoning about potential control-/data-flows in the program.

We note that context sensitivity is not limited to just call-graphs;
it may be used for any static analysis such as data-flow analysis,
alias analysis, heap-dependency analysis, and more.

3.1.2 Dependency (edge) types. An SDG represents the possible
information flows in the form of a graph with nodes (which are
classes in our case) and edges representing both the payload of the
information as well as the nature of the information flow between
methods, i.e., in the form of a call-return relationship, a data-flow
relationship, and/or a heap-carried relationship. Further, our for-
mulation of the SDG also highlights the information flow between
classes and database tables (which are additional nodes in the SDG)
in the form of transaction reads and writes.

Fig. 4 illustrates a simple example highlighting the various de-
pendencies that are extracted from the application. Specifically,
Fig. 4a highlights the inter-procedural dependencies between in-
structions in different classes, where each dependency is annotated
with a 2objH context information. Fig. 4b, illustrates a class-level
abstraction of the SDG in Fig. 4a. The dependency types are:
(1) Call-Return dependencies: These are the cornerstone of inter-
procedural static analysis. Each edge represents a possible method
invocation from an instruction in the source method to a target
method. Since Java is an object-oriented language, each source and
target method has a fully qualified reference object associated with
it. This enables us to associate an object-sensitive context with both
the caller and the callee. In Fig. 4, the call-return edges are denoted
with ‚Üê‚Üí. We note that each call return edge is associated with a
2ObjH context, e.g., in Fig. 4 (a) there are two call-graph edges from
class Main.main() to the Obj.getVal() with different contexts
[ùúô, Obj/1] and [ùúô, Obj/2].
(2) Data Dependencies: These edges indicate a static abstraction of
the data flows through any two pairs of instructions in the program.
Data flow edges (denoted by ‚àí‚Üí in Fig. 4a) can arise in a number of
ways: (a) arguments passed through a call-graph edge (e.g., in the
call from Main.main to DBHandler.write(x,y)); or (b) variable
reads/writes that are propagated across classes (e.g., in the field
writes o1.val = x) in DBHandler.read(). It is possible to extract
many of these dependencies with taint tracking.
(3) Heap-carried Dependencies: These edges indicate if two state-
ments access the same object on the heap. In order to detect heap-
carried dependencies, we use points-to analysis in order to de-
termine which objects each statement may access. Heap-carried

8A detailed discussion of context-sensitivity is beyond the scope of this work; we
therefore direct interested readers to some influential work in this area [7, 40, 49].

(a) An example program

(b) A class-level SDG abstraction of the program in Fig. 4a.

Figure 4: An example program and it‚Äôs corresponding SDG.
While the program is analyzed at an instruction level gran-
ularity, the SDG is constructed at a class level abstraction.
Edges in the SDG are implicitly qualified with 2objH context-
sensitivity. Note: for the sake of simplicity, this example does
not show all the inter-procedural dependencies.

edges (denoted by ‚àí‚Üí in Fig. 4a) use context-sensitivity to cap-
ture the heap-objects with a 1-heap-sensitivity (indicated by the
‚ÄòH‚Äô in 2objH). This heap-sensitivity allows us to precisely discover
which instances of the heap-objects are being used. A good exam-
ple of this can be seen in Fig. 4a where we first write to o1.val=x
in DBHandler.read() and then read it back in x = o1.getVal().
This is a typical heap carried dependency that arises due to a shared
object [Obj/1]. Other heap-carried edges can be differentiated with
their corresponding heap-context (e.g., [Obj/2] in Fig. 4a).
(4) Transactions Dependencies: These edges identify sections of the
code that take part in database transactions. Database transactions
are a critical and an irreplaceable component of Java EE applications.
It is possible to discover transactions using static analyzers like
WALA/SOOT/DOOP. To do this, we leverage JEE features such as
Java Persistence API (JPA) and JAVA Transaction API (JTA). These

class Obj {  int val;  int getVal() {return val;}}class DBHandler {  write(int x, y) {    success = SQL.write("DBTable", x, y);    return success;  }  read(Obj o1, Obj o2) {    int x, y;    success = SQL.read("DBTable", x, y);    o1.val = x;    o2.val = y;    return success;  }}class Main{  void main(String... args) {    Obj o1 = new Obj();    Obj o2 = new Obj();    success = DBHandler1.read(o1, o2);    int x = o1.getVal();    int y = o2.getVal();    success = DBHandler2.write(x, y);    exit(success)  }}[‚àÖ,Obj/2][‚àÖ,Obj/1][‚àÖ,DBHandler/1][‚àÖ,DBHandler/2][‚àÖ,DBHandler/2][Main,DBHandler/1][Main,DBHandler/1][Obj/2][Obj/1]DBTableTransaction DependencyCall-Return DependencyHeap-Carried DependencyDataflow DependencyObjMainDBHandlerASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

Nitin, V., et al.

Figure 5: A sample workflow depicting the extraction of context-snapshot from the SDG. From left to right, the sample program
from Fig. 3; the system dependence graph 1 and context transition graph 2 ; SDG sub-graphs under various contexts 3 .

APIs allow users to annotate a method with @Transactional to
indicate that the method is part of a transaction. We process these
methods their instructions to identify the destination database of
the transaction.

We note that, as per the norm [24], we gather the dependen-
cies at an instruction level, but abstract away the intra-partition
dependencies to minimize the volume of the graph. Since JEE based
monoliths are often partitioned at a class-level, we conjecture that
intra-procedural dependencies play a minimal role in enabling that
process when compared to inter-procedural dependencies.

3.2 Partitioning with CARGO
In this section, we describe Context sensitive Label Propagation (or
CARGO)‚Äîa novel community detection algorithm that builds upon
the principles of Label Propagation Algorithm (LPA) [54] to effec-
tively leverage the rich context-sensitive dependencies in the SDG
to identify and demarcate functional boundaries in the code. We
first describe off-the-shelf label propagation (¬ß3.2.1). Then we dis-
cuss motivation for, and construction of, context and transactional
snapshots from the SDG (¬ß3.2.2). Finally we describe CARGO and
how label propagation across each snapshot can be used generate
highly-cohesive and functionally isolated partitions (¬ß3.2.3).

3.2.1 Label propagation. LPA is a semi-supervised algorithm for
identifying communities of densely connected regions in a network
that works based just on network structure, without a need for
predefined objective functions or a priori knowledge about com-
munities [54]. Our intuition is based on the idea that if sections of
the code are cohesive, then they tend to be densely connected and
therefore the labels from LPA will propagate quickly through them
and a single label will come to dominate that section of the code.
Whereas, for sections of the code that are only loosely coupled,
labels will fail to propagate to these regions. This essentially iso-
lates loosely coupled regions of the code while coalescing cohesive
sections of the code. Consequently, as the Label Propagation con-
verges, each highly cohesive group of nodes (classes) will acquire a

consensus on a single label and each loosely coupled regions of the
code will have their own unique labels.

3.2.2 Context and Transactional Snapshots. Using off-the-shelf la-
bel propagation on the entire SDG is inaccurate and may limit us
from leveraging the precision and completeness of the SDG. This
is because the SDG‚Äìin it‚Äôs entirety‚Äìrepresents a superposition of
all the possible contextual states of the program. At any given
point, the program may only exist under only one of these contexts
(and not all of them simultaneously). Consider the sample program
shown in Fig. 5 and its system dependency graph 1 . Here, the
SDG consists of two versions of the same methods A.foo() and
B.bar() under different contexts. At a given instance, these meth-
ods can exist under one context, but not both. Therefore, to process
the SDG accurately, we need extract sub-graphs of the SDG that
represent all the individual program states under various contexts.
We denote these sub-graphs with the term ‚Äúcontext snapshots‚Äù.

To extract the context snapshots, we first construct a context
transition graph to capture all possible context transitions in the
program. This can be derived from the SDG by analyzing all pairs
of vertices connected by an edge and recording their contexts. For
example, in Fig. 5, we analyze the SDG 1 to obtain the contexts of
each pair of vertices to build the context transition graph seen in
2 . Next, for each context (say ùëêùë°ùë•ùëñ ) in the context transition graph,
we extract all the nodes from the SDG belonging to the current
context ùëêùë°ùë•ùëñ as well as the nodes belonging to the preceding context
(ùëêùë°ùë•ùëñ‚àí1) and the succeeding context (ùëêùë°ùë•ùëñ+1). The key intuition here
is that, for any given context, we must be able to know three things:
(a) what is the current state (i.e., ùëêùë°ùë•ùëñ )?, (b) how did we get to the
current state (i.e., the previous contexts ùëêùë°ùë•ùëñ‚àí1)?, and (c) which
program state can we transition to (i.e., the next contexts ùëêùë°ùë•ùëñ+1)?
Together, these nodes represent a ‚Äúsnapshot‚Äù of the program‚Äôs state.
This is repeated for all contexts to get a list of ‚Äúcontext-snapshots‚Äù.
In a spirit similar to that of context-snapshots, we also generate
a transactional snapshot which is a sub-graph of the SDG that in-
cludes vertices that are databases and those classes which engage

 Object()); Object());1void  main  (Object    []    args){2Aa1=newA(); // A/13Objectv1 = a1. foo     (newObjectv2 = a2. foo     (new45Aa2=new A(); // A/267}89classA {10object foo  (Objectv){11Bb=new B(); // B/112return b. bar (v);13}14}1516class B {17Object bar (Objectv){18Cc=new C(); // C/119return c. id (v);20}21}2223class C {24object id  (Objectv){     return      v; }25}main()A.foo()B.bar()A.foo()B.bar()C.id()[‚àÖ,A/1][A/1,B/1][‚àÖ,A/2][A/2,B/1][B/1,C/1][‚àÖ,‚àÖ][‚àÖ,A/1][A/1,B/1][‚àÖ,A/2][A/2,B/1][B/1,C/1][‚àÖ,‚àÖ]Context Transition Graph2Context Transition Graph Traversal* and context snapshot isolation     3System Dependence Graph1[‚àÖ,‚àÖ][‚àÖ,A/1][‚àÖ,A/2][A/1,B/1][A/2,B/1][B/1,C/1]main()A.foo()A.foo()[‚àÖ,‚àÖ]main()A.foo()B.bar()[‚àÖ,A/1]B.bar()B.bar()C.id()[B/1,C/1]Context 1Context 2Context N*Any ordered/un-ordered graph traversal     CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Applications to Microservices Architecture

ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

in transactions with this database. It produces a perspective of the
application that is centered explicitly around databases. Our key in-
tuition for including this view is to encourage our label propagation
to explicitly collect classes that share transactional dependencies
with one another. This should, in theory, reduce incidences of dis-
tributed transactions.

for a fictious airline company which allows users to book and
manage flights; JPetStore [30]: a Java web application for a pet store
where users shop for pets online; (5) one proprietary app which
we call Proprietary1. They contain 109, 33, 66, 37 and 82 classes
respectively. We note that of the above benchmark applications,
Daytrader uses 6 SQL database tables for its business logic.

3.2.3 Context Sensitive Label Propagation (CARGO). Context Sen-
sitivity Label Propagation (CARGO) is a novel variant of LPA
that attempts to leverage the SDG (particularly, the context- and
transactional-snapshots) to identify functionally similar and loosely
coupled components of the application. It is comprised of 3 steps,
namely (1) Initialization with seed labels; (2) Initial LPA with the
transactional snapshot; and (3) Iterative LPA over context snapshots.
Each of these are described in detail below:
‚ó¶ Step 1: Initialization with seed labels. Label propagation, and by
extension CARGO, are required to be seeded with initial partition-
ing labels. In the presence of an existing partitioning assignment
via the use of an existing algorithm (or a user-preferred partitioning
assignment), we may use that to seed CARGO. In the absence of
these seeds, we assign each class its own partition ID. In this sense,
CARGO can be fully unsupervised, or it may be semi-supervised.
‚ó¶ Step 2: Initial LPA with the transactional snapshot. In the first
stage, we apply LPA over the transactional snapshot of the SDG.
This identifies classes that engage in distributed transactions and
propagates seed labels between them. If more than one class shares
transaction edges with the same DB table, then LPA attempts to
assign the same label to them. For two or more classes that do
not write to the same table, their labels will remain unchanged.
While LPA doesn‚Äôt guarantee that distributed transactions will
be eliminated, this initial seeding reduces incidents of distributed
transactions in the subsequent stages as other class nodes adapt to
the seeds from this stage.
‚ó¶ Step 3: Iterative LPA over context-snapshots. In this stage, we
iteratively apply label propagation on each context snapshot of
the SDG. The underlying intuition is as follows‚Äîin a significant
number of snapshots, if a group of classes seem to be connected
to one another (through call, data, or heap-dependency), they are
likely cohesive and must thus belong together. In contrast, if classes
are seldom connected across all context snapshots, they are weakly
tied and must exist in different partitions. As we apply LPA to each
context snapshot, classes that are cohesive, frequently co-occurring,
and related will quickly share the same label. For classes that do not
frequently co-occur, a separate label will be used. This means that
CARGO can isolate weakly connected portions of the code while
combining cohesive portions of the code in a single partition.

4 EVALUATION
4.1 Benchmark Applications
We use 5 commonly used Java applications as our benchmarks to
perform evaluation. These include: (1) Daytrader [52]: A Java EE7
application built around the paradigm of an online stock trading
system. The application allows users to login, view their portfolio,
and buy or sell stock shares; (2) Plants [53]: a simple Java EE 6
application which uses CDI managed beans, Java Server Faces (JSF),
and Java Server Pages (JSP); (3) AcmeAir [4]: a Java web application

4.2 Implementation
We use Doop [10] and datalog to implement all our static analysis as
described in ¬ß3. We also leverage JackEE [6], a framework that per-
forms scalable and precise analysis of Java Enterprise applications.
Our datalog rules will be made available as part of our published
code. We then use networkx [20] to parse the generated edges and
build an SDG that represents the program. We run CARGO in two
modes:
(1) Native mode: This is a purely unsupervised version of the
algorithm, where we are provided only with a maximum number
of partitions ùêæ. We take a random ordering of the nodes of the
SDG, and initialize the ùëñth node with the label (ùëñ % ùêæ). We then
run CARGO as described in ¬ß3 using these initial labels to arrive at
partition assignments for each node.
(2) Refinement mode: This is a semi-supervised version of the
algorithm, where we are provided with an initial set of seed labels
obtained from another partitioning approach. These labels are as-
signed to the corresponding nodes of the SDG, and the rest of the
SDG nodes are initialized with "-1", denoting no label. We then
run CARGO to propagate these initial labels. When the algorithm
finishes, if nodes are still labeled "-1", these are unreachable classes.
Note: In the rest of the paper, each algorithm Algorithm refined
with CARGO is denoted by Algorithm++. The name CARGO refers
to the algorithm run in native mode, unless specified otherwise.

4.3 Metrics
4.3.1 Database Transactional Purity. To minimize distributed trans-
actions, we would like each database object to be accessed by classes
from one partition only. To design a metric that quantifies this, we
use entropy [46] which measures the homogeneity of a set of sam-
ples. A low value of entropy indicates a high purity of the samples
and vice-versa.

For each database table, find the list of partitions from which
it is accessed. Let the entropy of this list be ‚Ñé; then transactional
purity for this database table is defined as 1 ‚àí ‚Ñé. If a database table
is accessed only by classes from a single partition, the transactional
purity for that table will be 1.0. The transactional purity of the
whole program is the average of the transactional purity across all
database tables. Higher transactional purity is indicative of fewer
distributed transactions, which is a desirable property for a parti-
tioning scheme.

4.3.2 Run-time Performance Metrics. These measures evaluate the
run-time behavior of the program.
‚Ä¢ Latency measures how long, in milliseconds, it takes to perform
a user action. We calculate average latency of all the requests made
by all the users.
‚Ä¢ Throughput measures how many requests are completed per
second, averaged over the entire run time of the program.

ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

Nitin, V., et al.

4.3.3 Architectural Metrics. Using our PDG and dynamic call graphs,
we define four metrics to measure the effectiveness of partitions:
‚Ä¢ Coupling [37] measures the degree of interaction between dif-
ferent partitions of the microservice. Coupling for each partition is
the number of edges in the SDG going from or to the partition. We
calculate the sum of coupling across all partitions, and divide this
by the total number of edges in the SDG.
‚Ä¢ Cohesion [37] measures the degree of interconnectedness be-
tween classes in a partition. For each partition, it is computed
as (internal edges)/(internal edges + external edges). We
calculate cohesion for each partition separately and compute the
mean across all partitions.
‚Ä¢ Inter-call Percentage (ICP) [29] measures the percentage of
the overall call volume occurring between two partitions. ICPùëñ,ùëó =
ùëêùëñ,ùëó /(cid:205)ùëÄ
ùëñ‚â†ùëó ùëêùëñ,ùëó , where ùëêùëñ,ùëó represents the number of calls between
partition ùëñ and partition ùëó. Lower the ICP, better the partitioning.
‚Ä¢ Business Context Purity (BCP) [28] measures the average en-
tropy of business use cases per partition. Each class is associated
with one use case, so each partition has a list of use cases. To main-
tain semantic and functional coherence of the partitions, we would
like each partition to not handle too many different use cases. A list
consisting of many different kinds of use cases would have high
entropy and high BCP, so lower BCP means better partitions.

5 EXPERIMENTAL RESULTS
We evaluate our approach through the following questions :
‚Ä¢ RQ1: Mitigating distributed Transactions. How effective is
our approach in minimizing distributed transactions?
‚Ä¢ RQ2: Run-time performance. How do the latency and through-
put of a refined microservice application compare to those of the
original application?
‚Ä¢ RQ3: Performance on architectural metrics. How does our
approach perform on architectural metrics?

5.1 RQ1: Distributed Database Transactions
Mitigating distributed transactions is crucial for a microservice
application. In this section we evaluate how effective our approach
is in minimizing distributed transactions.
Evaluation: We run CARGO to refine the output of other partition-
ing algorithms. We also run CARGO stand-alone. Since Daytrader
is the only dataset that uses external databases, we run all parti-
tioning algorithms only on Daytrader. We run each algorithm with
ùêæ = [3, 5, 7, 9, 11, 13] and 5 random seeds, and measure the average
transactional purity across all seeds and values of ùêæ.
Discussion: The transactional purity is shown in Fig. 6. CARGO
increases the transactional purity of all baseline approaches (i.e.,
approaches suffixed with ++ perform better than the ones without
the suffix). When used in native mode (CARGO), and when used to
refine the outputs of FoSCI (FoSCI++) and MEM (MEM++), CARGO
achieves transactional purity of 1.0. This indicates that there is no
database table that is accessed by classes from multiple partitions.

Summary: For each of the 4 baselines, the refined partitions
have higher transactional purity. Further, FoSCI++, MEM++ and
CARGO have transactional purity of 1.0, implying that there can
be no distributed transactions with these partitioning schemes.

Figure 6: Transaction purity for CARGO and other ap-
proaches. The transactional purity of the refined partitions
is higher than that of the original partitions for all ap-
proaches. FoSCI++, MEM++ and CARGO achieve transac-
tional purity of 1.0, indicating that no database objects is
accessed by classes belonging to different partitions.

5.2 RQ2 : Run-time Performance
In this section, we evaluate the performance of our partitioning
algorithm in a real world setting. When we implement the recom-
mended partitioning scheme as a microservice application, we want
the application to have low latency and high throughput.
Evaluation: The Daytrader application has the largest number of
classes among our benchmark applications, so we choose it for these
performance evaluations. Daytrader is also the only benchmark to
use external databases.

We use Mono2Micro [1] to construct a microservice version of
the monolithic Daytrader application, and repeat this process for
Mono2Micro++. Both these apps are deployed by using individ-
ual Docker containers for each partition, and orchestrated using
docker-compose.

We use Apache JMeter to measure latency and throughput un-
der varying loads. We simulate various use cases consisting of
sequences of user actions. We test the application with multiple
users executing concurrent requests every second. The number of
users is varied from 211 ‚âà 2ùêæ to 220 ‚âà 1ùëÄ in exponents of 2.

Discussion: The latency and throughput are plotted as a func-
tion of the number of users, for different use cases. The results are
shown in Figure 7. We observe that:
‚Ä¢ When the number of users is sufficiently high, the Mono2Micro++
partitioning scheme achieves lower latency and higher throughput
than the original Mono2Micro partitioning scheme. As the load
on the application increases, the coupling between microservices
becomes more of an impediment to performance. We hypothesize
that Mono2Micro++ performs better than Mono2Micro under high
loads because it has less inter-partition coupling.
‚Ä¢ This holds true across all the use cases that we have considered.
‚Ä¢ We compute the average percentage improvement in latency
and throughput across all use cases, and find that Mono2Micro++
represents a 11% improvement in latency and a 120% improvement
throughput, over Mono2Micro.

CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Applications to Microservices Architecture

ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

Figure 7: Latency and Throughput for Mono2Micro versus Mono2Micro++, for different use cases in Daytrader. We simulate
each application with the number of users varying from 211 ‚âà 2ùêæ to 220 ‚âà 1ùëÄ in exponents of 2. Mono2Micro++ achieves lower
latency and higher throughput than Mono2Micro, indicating better performance.

Summary: When we use our approach to re-partition a mi-
croservice application, the resulting application has 11% lower
latency and 120% higher throughput on average across use cases
than the original application.

5.3 RQ3: Performance on Architectural Metrics
RQ1 measured how effectively CARGO minimizes distributed trans-
actions, and RQ2 evaluated the real-world performance of CARGO.
In this section, we evaluate CARGO on architectural metrics that
use PDG and call-graph information to measure the quality of par-
titions. We would like to see loose coupling between partitions, and
high cohesion within classes of the same partition.
Evaluation: We use four baseline partitioning approaches -
Mono2Micro (M2M) [29], CoGCN [12], FoSCI [25] and MEM [39].
We run these approaches on our benchmark set of five monolithic
applications. Each of these approaches allows one to specify a
maximum number of partitions, ùêæ. We run each algorithm with ùêæ =
[3, 5, 7, 9, 11, 13], and use CARGO to refine the partitions produced
for each value of ùêæ. Additionally, we run CARGO in native mode
with the same values of ùêæ. Both when refining existing partitions
and while running CARGO stand-alone, we run the algorithm five
times with different random seeds. For each metric, we report the
average across all seeds and values of ùêæ.
Discussion: The results are shown in Table 1. The best results are
highlighted in blue We make the following observations:
‚Ä¢ Cohesion and Coupling: When used to refine the partitions pro-
duced by another algorithm, CARGO always improves the Cohesion
and Coupling. Further, when used stand-alone, CARGO achieves
the best Cohesion and Coupling across all algorithms.
‚Ä¢ ICP: When used to refine the partitions produced by another
algorithm, the refined partitions are noticeably better than the

original partitions in terms of ICP. When used in native mode,
CARGO performs significantly better than all the other approaches
in 3 out of 5 datasets.
‚Ä¢ BCP: BCP measures the semantic coherence of each partition
in terms of business use cases that it handles. When CARGO is
used stand-alone on Daytrader, it outperforms all other approaches
on this metric. However when used to refine existing partitions,
it performs poorly. The definition of BCP depends heavily on the
quality of the generated business use cases for each class, which
is information that CARGO does not have access to. This is one
possible explanation for its poor performance on this metric.

Finally, we note that CARGO when used stand-alone with ran-
domly initialized labels achieves state-of-the-art performance on
all four metrics on Daytrader. This shows that even in the absence
of domain knowledge, CARGO can effectively learn the structure
of the program and form partitions.

Summary: In 3 out of the 4 metrics, the refined partitions
are better than the original partitions for a majority of the ap-
proaches. Further, CARGO used in native mode achieves state-
of-the-art performance on all 4 metrics on the Daytrader bench-
mark.

6 RELATED WORK
Software Modularization: Microservice decomposition is a spe-
cific instance of the Software Modularization problem, which has
seen a long line of work. In general, software modularization ap-
proaches construct a Module Dependence Graph (MDG), and per-
form various analyses on this graph. Doval et al. [14], Mitchell
and Mancoridis [41] and Harman et al. [21] define a modularity
metric based on intra-cluster and inter-cluster connections and op-
timize this metric using genetic algorithms or hill climbing. Sarkar

50100LatencyQuotes50100Home50100Portfolio50100Account50100Buy0200Logout255075UpdateProfile102030RegisterPage2K1MUsers2040Throughput2K1MUsers10202K1MUsers10202K1MUsers510152K1MUsers2462K1MUsers2.55.02K1MUsers242K1MUsers123Mono2MicroMono2Micro++ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

Nitin, V., et al.

Table 1: Performance of different approaches on four architectural metrics computed on 5 applications. We evaluate
Mono2Micro (M2M), CoGCN, FoSCI and MEM, along with their refined partitions denoted with a "++". We also evaluate CARGO
stand-alone. The symbol ‚ñ≥ next to a metric indicates that higher is better, while ‚ñΩ indicates that lower is better. For each ap-
proach and refined approach, we shade the better of the two blue . We count the number of wins, ties and losses for the refined
approaches versus the original approaches across all the datasets, and shade the corresponding cell green if wins outnumber
the losses. The best performing approach in each row is bold. If CARGO is the best performing approach, then we shade its
cell orange

Coupling ‚ñΩ

Cohesion ‚ñ≥

+
+
M
2
M

N
C
G
o
C

M
2
M

+
+
N
C
G
o
C

+
+
I
C
S
o
F

M
E
M

+
+
M
E
M

O
G
R
A
C

I
C
S
o
F

+
+
M
2
M

N
C
G
o
C

M
2
M

+
+
N
C
G
o
C

+
+
I
C
S
o
F

+
+
M
E
M

O
G
R
A
C

M
E
M

I
C
S
o
F

Plants 0.31 0.04 0.40 0.29 0.75 0.31 0.29 0.11 0.05

Daytrader 0.78 0.02 0.25 0.02 0.68 0.02 0.12 0.01 0.01 0.30 0.62 0.37 0.61 0.23 0.54 0.26 0.46 0.71
0.32 0.61 0.39 0.46 0.24 0.40 0.39 0.44 0.6
Acmeair 0.58 0.04 0.52 0.13 0.71 0.22 0.51 0.15 0.03 0.27 0.48 0.21 0.32 0.16 0.51 0.37 0.55 0.96
Jpetstore 0.77 0.03 0.78 0.06 0.79 0.03 0.77 0.05 0.03 0.27 0.29 0.20 0.24 0.18 0.36 0.30 0.44 0.94
0.57 0.78 0.69 0.73 0.37 0.57 0.48 0.67 0.75
5 / 0 / 0

Proprietary1 0.42 0.03 0.22 0.04 0.48 0.02 0.27 0.03 0.04
5 / 0 / 0
Win/Tie/Loss

5 / 0 / 0

5 / 0 / 0

5 / 0 / 0

5 / 0 / 0

5 / 0 / 0

5 / 0 / 0

BCP ‚ñΩ

ICP ‚ñΩ

+
+
M
2
M

N
C
G
o
C

M
2
M

+
+
N
C
G
o
C

+
+
I
C
S
o
F

I
C
S
o
F

M
E
M

+
+
M
E
M

O
G
R
A
C

+
+
M
2
M

N
C
G
o
C

M
2
M

+
+
N
C
G
o
C

+
+
I
C
S
o
F

I
C
S
o
F

+
+
M
E
M

O
G
R
A
C

M
E
M

Daytrader 2.31 2.57 1.40 1.90 2.55 2.26 2.23 2.32 1.31 0.60 0.05 0.16 0.05 0.48 0.07 0.04 0.07 0.00
0.54 0.11 0.65 0.11 0.70 0.12 0.28 0.11 0.06
Plants 1.68 2.20 1.96 1.54 2.37 1.71 1.93 1.53 1.79
0.91 0.35 0.67 0.63 0.85 0.63 0.41 0.72 0.32
Acmeair 1.29 1.48 1.18 1.12 1.75 1.61 1.94 1.95 1.75
Jpetstore 2.25 2.35 1.71 1.77 2.26 2.45 2.54 2.74 2.87 0.13 0.41 0.33 0.35 0.24 0.40 0.26 0.42 0.30
Proprietary1 1.23 1.53 1.27 1.42 1.49 1.48 1.48 1.95 1.55 0.02 0.35 0.17 0.25 0.37 0.32 0.32 0.32 0.37
Win/Tie/Loss 0 / 0 / 5

4 / 0 / 1

3 / 0 / 2

1 / 1 / 3

3 / 0 / 2

1 / 0 / 4

4 / 0 / 1

2 / 0 / 3

et al. [45] define information-theory based metrics to evaluate the
quality of software modularization. Bavota et al. [9] quantify the
cohesion between classes using the volume of information flowing
between functions, and the semantic similarity of the identifiers
and comments used.
Decomposition into Microservices: There have been several re-
cent papers that attempt to decompose monolithic applications into
microservices [5, 8, 11, 15, 19, 22, 32, 34, 38]. Escobar et al. [15]
present a rule-based algorithm to partition JEE applications that
separates classes according to the data that they manipulate. [38]
define 3 coupling criteria between classes based on an analysis
of commit patterns and semantic similarity. They then construct
a graph where the weight of an edge connecting two classes is
proportional to the strength of the coupling between them, and
use clustering algorithms on this graph. Gysel et al. [19] propose a

similar algorithm, with 16 coupling criteria instead of 3. There are
also some approaches that use dynamic traces to cluster classes into
microservices. Jin et al. [26] identify functional atoms, or groups
of classes that frequently occur together in a trace, and define four
optimization objectives to group these functional atoms. Desai et al.
[13] construct a co-occurrence graph between classes that occur in
the same execution trace, and use a Graph Neural Network (GNN)
to obtain node embeddings and cluster nodes on this graph. [29]
measure the number of direct and indirect calls between classes in
a dynamic trace, and apply hierarchical clustering on the similarity
graph thus obtained.

CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Applications to Microservices Architecture

ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

7 THREATS TO VALIDITY
As with any evaluation study, biases can affect the final results.
Therefore, any conclusions of this work must be considered with
the following issues in mind:

‚Ä¢ Evaluation Bias: In RQ1 and RQ3, we compare the performance
of CARGO using a set of architectural metrics (defined in ¬ß4.3). The
results and the conclusions of these RQs are scoped by the efficacy
of the metrics to evaluate the architecture. Since these metrics are
statically computed, there may be slightly different conclusions
with other measurements. This is to be explored in future research.
‚Ä¢ Construct Validity: At various places in this paper, we made
engineering decisions about program graph granularity, traversal
orders, and so on. While these decisions were made using advice
from the literature, we acknowledge that other constructs might
lead to other conclusions. To enable further exploration of our de-
sign decisions and in the interest of open science, we make available
our replication package.

‚Ä¢ External Validity: For this study, we have selected a diverse set
of subject systems, including a proprietary application. We also use
a number of microservice partitioning algorithms for comparison.
There is a possibility that our outcomes may vary for a different set
of applications. Therefore, one has to be careful when generalizing
our findings to other applications or algorithms. Even though we
tried to run our experiment on a variety of applications, we do not
claim that our results generalize beyond the specific case studies
explored here. That said, to enable reproducibility, we have shared
our scripts and the gathered performance data.

8 CONCLUSION
Microservices Architecture (MSA) has evolved into a de-facto stan-
dard for developing cloud-native enterprise applications during
the last decade. A number of automated approaches are being de-
veloped to recommend functional partitions in monolithic code
that can be extracted away and modernized as microservices. How-
ever, many of these approaches produce recommendations that are
not actionable as they result in (a) distributed monoliths, and/or
(b) force the use of distributed transactions. This work attempts
to overcome these challenges by introducing CARGO-‚Äîa novel
un-/semi-supervised partition refinement technique that was used
to refine and thereby enrich the partitioning quality of the cur-
rent state-of-the-art algorithms. Our findings over several Java EE
projects suggest that CARGO can improve the partition quality
of the existing approach and steer the recommendations towards
generating better microservice recommendations.

REFERENCES
[1] [n.d.]. Transform monolithic Java applications into microservices with the
power of AI. https://developer.ibm.com/tutorials/transform-monolithic-java-
applications-into-microservices-with-the-power-of-ai/. Accessed: 2022-05-05.
[2] [n.d.]. Upgrading GitHub from Rails 3.2 to 5.2. https://github.blog/2018-09-28-

upgrading-github-from-rails-3-2-to-5-2/. Accessed: 2022-05-06.

[3] [n.d.]. What Led Amazon to its Own Microservices Architecture. https://www.
infoq.com/articles/github-monolith-microservices/. Accessed: 2022-05-06.

[4] acmeair. 2019. acmeair. https://github.com/acmeair/acmeair
[5] Mohsen Ahmadvand and Amjad Ibrahim. 2016. Requirements reconciliation for
scalable and secure microservice (de) composition. In 2016 IEEE 24th International
Requirements Engineering Conference Workshops (REW). IEEE, 68‚Äì73.

[6] Anastasios Antoniadis, Nikos Filippakis, Paddy Krishnan, Raghavendra Ramesh,
Nicholas Allen, and Yannis Smaragdakis. 2020. Static analysis of Java enterprise
applications: frameworks and caches, the elephants in the room. In Proceedings

of the 41st ACM SIGPLAN Conference on Programming Language Design and
Implementation. 794‚Äì807.

[7] Anastasios Antoniadis, Nikos Filippakis, Paddy Krishnan, Raghavendra Ramesh,
Nicholas Allen, and Yannis Smaragdakis. 2020. Static Analysis of Java Enterprise
Applications: Frameworks and Caches, the Elephants in the Room. In Proceedings
of the 41st ACM SIGPLAN Conference on Programming Language Design and
Implementation (London, UK) (PLDI 2020). Association for Computing Machinery,
New York, NY, USA, 794‚Äì807. https://doi.org/10.1145/3385412.3386026

[8] Luciano Baresi, Martin Garriga, and Alan De Renzis. 2017. Microservices identi-
fication through interface analysis. In European Conference on Service-Oriented
and Cloud Computing. Springer, 19‚Äì33.

[9] Gabriele Bavota, Andrea De Lucia, Andrian Marcus, and Rocco Oliveto. 2013.
Using structural and semantic measures to improve software modularization.
Empirical Software Engineering 18, 5 (2013), 901‚Äì932.

[10] Martin Bravenboer and Yannis Smaragdakis. 2009. Strictly declarative specifica-
tion of sophisticated points-to analyses. In Proceedings of the 24th ACM SIGPLAN
conference on Object oriented programming systems languages and applications.
243‚Äì262.

[11] Rui Chen, Shanshan Li, and Zheng Li. 2017. From monolith to microservices: A
dataflow-driven approach. In 2017 24th Asia-Pacific Software Engineering Confer-
ence (APSEC). IEEE, 466‚Äì475.

[12] Utkarsh Desai, Sambaran Bandyopadhyay, and Srikanth Tamilselvam. 2021.
Graph Neural Network to Dilute Outliers for Refactoring Monolith Applica-
tion. In Proceedings of Association for the Advancement of Artificial Intelligence.
AAAI, virtual, 72‚Äì80.

[13] Utkarsh Desai, Sambaran Bandyopadhyay, and Srikanth Tamilselvam. 2021.
Graph neural network to dilute outliers for refactoring monolith application. In
Proceedings of 35th AAAI Conference on Artificial Intelligence (AAAI‚Äô21).
[14] Diego Doval, Spiros Mancoridis, and Brian S Mitchell. 1999. Automatic clustering
of software systems using a genetic algorithm. In STEP‚Äô99. Proceedings Ninth
International Workshop Software Technology and Engineering Practice. IEEE, 73‚Äì
81.

[15] Daniel Escobar, Diana C√°rdenas, Rolando Amarillo, Eddie Castro, Kelly Garc√©s,
Carlos Parra, and Rubby Casallas. 2016. Towards the understanding and evo-
lution of monolithic applications as microservices. In 2016 XLII Latin American
computing conference (CLEI). IEEE, 1‚Äì11.

[16] Jeanne Ferrante, Karl J Ottenstein, and Joe D Warren. 1987. The program de-
pendence graph and its use in optimization. ACM Transactions on Programming
Languages and Systems (TOPLAS) 9, 3 (1987), 319‚Äì349.

[17] Martin Fowler. 2004. Strangler Fig Application. https://martinfowler.com/bliki/

StranglerFigApplication.html

[18] Martin Fowler. 2006. Inversion of Control Containers and Dependency Injection

pattern. http://www. martinfowler. com/articles/injection. html (2006).

[19] Michael Gysel, Lukas K√∂lbener, Wolfgang Giersche, and Olaf Zimmermann. 2016.
Service cutter: A systematic approach to service decomposition. In European
Conference on Service-Oriented and Cloud Computing. Springer, 185‚Äì200.
[20] Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. 2008. Exploring Network
Structure, Dynamics, and Function using NetworkX. In Proceedings of the 7th
Python in Science Conference, Ga√´l Varoquaux, Travis Vaught, and Jarrod Millman
(Eds.). Pasadena, CA USA, 11 ‚Äì 15.

[21] Mark Harman, Robert M Hierons, and Mark Proctor. 2002. A New Representation
And Crossover Operator For Search-based Optimization Of Software Modular-
ization.. In GECCO, Vol. 2. 1351‚Äì1358.

[22] Sara Hassan, Nour Ali, and Rami Bahsoon. 2017. Microservice ambients: An
architectural meta-modelling approach for microservice granularity. In 2017 IEEE
International Conference on Software Architecture (ICSA). IEEE, 1‚Äì10.

[23] Gregor Hohpe and Bobby Woolf. 2002. Enterprise integration patterns. In 9th

conference on pattern language of programs. 1‚Äì9.

[24] Susan Horwitz, Thomas Reps, and David Binkley. 1990. Interprocedural slicing
using dependence graphs. ACM Transactions on Programming Languages and
Systems (TOPLAS) 12, 1 (1990), 26‚Äì60.

[25] W. Jin, T. Liu, Y. Cai, R. Kazman, R. Mo, and Q. Zheng. 2019. Service Candi-
date Identification from Monolithic Systems based on Execution Traces. IEEE
Transactions on Software Engineering 47, 5 (Apr 2019), 1‚Äì21.

[26] Wuxia Jin, Ting Liu, Qinghua Zheng, Di Cui, and Yuanfang Cai. 2018.
Functionality-oriented microservice extraction based on execution trace cluster-
ing. In 2018 IEEE International Conference on Web Services (ICWS). IEEE, 211‚Äì218.
[27] Rod Johnson. 2004. Expert one-on-one J2EE design and development. John Wiley

& Sons.

[28] Anup K. Kalia, Chen Lin, Jin Xiao, Saurabh Sinha, John Rofrano, Maja Vukovic,
and Debasish Banerjee. 2020. Mono2Micro: An AI-based Toolchain for Evolving
Monolithic Enterprise Applications to a Microservice Architecture. In Proceedings
of ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering. ACM, Sacramento, 1606‚Äì1610.

[29] Anup K Kalia, Jin Xiao, Rahul Krishna, Saurabh Sinha, Maja Vukovic, and Deba-
sish Banerjee. 2021. Mono2Micro: a practical and effective tool for decomposing
monolithic Java applications to microservices. In Proceedings of the 29th ACM
Joint Meeting on European Software Engineering Conference and Symposium on

ASE ‚Äô22, October 10‚Äì14, 2022, Rochester, MI, USA

Nitin, V., et al.

the Foundations of Software Engineering. 1214‚Äì1224.

[30] KimJongSung. 2013. jPetStore. https://github.com/KimJongSung/jPetStore
[31] Martin Kleppmann. 2017. Designing data-intensive applications: The big ideas
behind reliable, scalable, and maintainable systems. " O‚ÄôReilly Media, Inc.".
[32] Sander Klock, Jan Martijn EM Van Der Werf, Jan Pieter Guelen, and Slinger
Jansen. 2017. Workload-based clustering of coherent feature sets in microservice
architectures. In 2017 IEEE International Conference on Software Architecture
(ICSA). IEEE, 11‚Äì20.

[33] Patrick Lam, Eric Bodden, Ondrej Lhot√°k, and Laurie Hendren. 2011. The Soot
framework for Java program analysis: a retrospective. In Cetus Users and Compiler
Infastructure Workshop (CETUS 2011), Vol. 15.

[34] Alessandra Levcovitz, Ricardo Terra, and Marco Tulio Valente. 2016. Towards
a technique for extracting microservices from monolithic enterprise systems.
arXiv preprint arXiv:1605.03175 (2016).

[35] Yue Li, Tian Tan, Anders M√∏ller, and Yannis Smaragdakis. 2018. Precision-guided
context sensitivity for pointer analysis. Proceedings of the ACM on Programming
Languages 2, OOPSLA (2018), 1‚Äì29.

[36] Yue Li, Tian Tan, Anders M√∏ller, and Yannis Smaragdakis. 2018. Scalability-
first pointer analysis with self-tuning context-sensitivity. In Proceedings of the
2018 26th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 129‚Äì140.

[37] Robert C Martin, James Newkirk, and Robert S Koss. 2003. Agile software de-
velopment: principles, patterns, and practices. Vol. 2. Prentice Hall Upper Saddle
River, NJ.

[38] Genc Mazlami, J√ºrgen Cito, and Philipp Leitner. 2017. Extraction of microservices
from monolithic software architectures. In 2017 IEEE International Conference on
Web Services (ICWS). IEEE, 524‚Äì531.

[39] Genc Mazlami, J√ºrgen Cito, and Philipp Leitner. 2017. Extraction of Microser-
vices from Monolithic Software Architectures. In Proceedings of International
Conference on Web Services. IEEE, Honolulu, 524‚Äì531.

[40] Ana Milanova, Atanas Rountev, and Barbara G Ryder. 2005. Parameterized
object sensitivity for points-to analysis for Java. ACM Transactions on Software
Engineering and Methodology (TOSEM) 14, 1 (2005), 1‚Äì41.

[41] Brian S Mitchell and Spiros Mancoridis. 2006. On the automatic modularization of
software systems using the bunch tool. IEEE Transactions on Software Engineering
32, 3 (2006), 193‚Äì208.

[42] Sam Newman. 2021. Building microservices. " O‚ÄôReilly Media, Inc.".
[43] Chris Richardson. 2018. Microservices patterns: with examples in Java. Simon and

Schuster.

[44] Bernd Ruecker. 2021. Practical Process Automation. " O‚ÄôReilly Media, Inc.".
[45] Santonu Sarkar, Girish Maskeri Rama, and Avinash C Kak. 2006. API-based and
information-theoretic metrics for measuring the quality of software modulariza-
tion. IEEE Transactions on Software Engineering 33, 1 (2006), 14‚Äì32.

[46] Claude Elwood Shannon. 2001. A mathematical theory of communication. ACM
SIGMOBILE mobile computing and communications review 5, 1 (2001), 3‚Äì55.
[47] Micha Sharir, Amir Pnueli, et al. 1978. Two approaches to interprocedural data flow
analysis. New York University. Courant Institute of Mathematical Sciences . . . .
[48] Olin Grigsby Shivers. 1991. Control-flow analysis of higher-order languages or

taming lambda. Carnegie Mellon University.

[49] Yannis Smaragdakis, Martin Bravenboer, and Ondrej Lhot√°k. 2011. Pick your
contexts well: understanding object-sensitivity. In Proceedings of the 38th annual
ACM SIGPLAN-SIGACT symposium on Principles of programming languages. 17‚Äì
30.

[50] Yannis Smaragdakis, George Kastrinis, and George Balatsouras. 2014. Introspec-
tive analysis: context-sensitivity, across the board. In Proceedings of the 35th
ACM SIGPLAN Conference on Programming Language Design and Implementation.
485‚Äì495.

[51] Johannes Th√∂nes. 2015. Microservices. IEEE software 32, 1 (2015), 116‚Äì116.
[52] WASdev. 2015.
daytrader7

https://github.com/WASdev/sample.

sample.daytrader7.

[53] WASdev. 2019. sample.mono-to-ms.pbw-monolith. https://github.com/WASdev/

sample.mono-to-ms.pbw-monolith

[54] Zhu Xiaojin and Ghahramani Zoubin. 2002. Learning from labeled and unlabeled
data with label propagation. Tech. Rep., Technical Report CMU-CALD-02‚Äì107,
Carnegie Mellon University (2002).

