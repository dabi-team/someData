2
2
0
2

r
p
A
5
1

]
L
C
.
s
c
[

1
v
3
6
3
7
0
.
4
0
2
2
:
v
i
X
r
a

Is Surprisal in Issue Trackers Actionable?

James Caddy
University of Adelaide
Adelaide, Australia
james.caddy@adelaide.edu.au

Markus Wagner
University of Adelaide
Adelaide, Australia
markus.wagner@adelaide.edu.au

Christoph Treude
University of Melbourne
Melbourne, Australia
christoph.treude@unimelb.edu.au

Earl T. Barr
University College London
London, United Kingdom
e.barr@ucl.ac.uk

Miltiadis Allamanis
Microsoft Research
Cambridge, United Kingdom
miltos@allamanis.com

ABSTRACT

Background. From information theory, surprisal is a measure-
ment of how unexpected an event is. Statistical language models
provide a probabilistic approximation of natural languages, and
because surprisal is constructed with the probability of an event
occuring, it is therefore possible to determine the surprisal associ-
ated with English sentences. The issues and pull requests of soft-
ware repository issue trackers give insight into the development
process and likely contain the surprising events of this process.

Objective. Prior works have identiﬁed that unusual events in
software repositories are of interest to developers, and use sim-
ple code metrics-based methods for detecting them. In this study
we will propose a new method for unusual event detection in soft-
ware repositories using surprisal. With the ability to ﬁnd surpris-
ing issues and pull requests, we intend to further analyse them to
determine if they actually hold importance in a repository, or if
they pose a signiﬁcant challenge to address. If it is possible to ﬁnd
bad surprises early, or before they cause additional troubles, it is
plausible that eﬀort, cost and time will be saved as a result.

Method. After extracting the issues and pull requests from 5000
of the most popular software repositories on GitHub, we will train
a language model to represent these issues. We will measure their
perceived importance in the repository, measure their resolution
diﬃculty using several analogues, measure the surprisal of each,
and ﬁnally generate inferential statistics to describe any correla-
tions.

CCS CONCEPTS
• Information systems → Language models; Data mining; •
Computing methodologies → Anomaly detection; Maximum like-
lihood modeling.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
MSR ’22, May 23–24, 2022, Pittsburgh, Pennsylvania, United States
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $XX.XX
https://doi.org/XXXXXXX.XXXXXXX

KEYWORDS
self-information, n-gram, GitHub issues

ACM Reference Format:
James Caddy, Markus Wagner, Christoph Treude, Earl T. Barr, and Miltiadis
Allamanis. 2022. Is Surprisal in Issue Trackers Actionable?. In Proceedings of
19th International Conference on Mining Software Repositories (Registered Re-
ports) (MSR ’22). ACM, New York, NY, USA, 8 pages. https://doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION
Surprisal is a measure in information theory that can quantify how
unexpected and thus how informative a word 𝑤𝑡 is given the words
that precede it (𝑤1, ..., 𝑤𝑡−1). A higher word surprisal value indi-
cates that the current word is less expected given the context. In
mathematical terms, surprisal is deﬁned as the negative logarithm
of the word’s conditional probability of occurrence [18].

In this work, we propose to apply the surprisal measure to soft-
ware engineering artefacts, motivated by many researchers argu-
ing that software developers need to be aware of unusual or sur-
prising events in their repositories, e.g., when summarizing project
activity [19], notifying developers about unusual commits [7, 9],
and for the identiﬁcation of malicious content [26]. The basic intu-
ition is that catching bad surprises early will save eﬀort, cost, and
time, since bugs cost signiﬁcantly more to ﬁx during implementa-
tion or testing than in earlier phases [17], and by extension, bugs
cost more the longer they exist in a product after being reported
and before being addressed.

Following recent work on applying natural language techniques
to software engineering data [14], in this work, we investigate
whether the information-theoretic measure of surprisal is action-
able when applied to software repositories. In this study we anal-
yse issues and pull requests separately, but as a matter of conven-
tion, we will refer to these simply as issues going forward. In our
method, the diﬀerences are negligible, but separately they may pro-
duce unique analyses.

We investigate two scenarios involving surprisal: its eﬀect on
resolution diﬃculty and the perceived importance of surprising is-
sues. Prior work [23] found that conformance to project-speciﬁc
language norms reduces issue resolution time. Assuming that sur-
prising issues do not conform to such norms, we investigate whether
they are more diﬃcult to resolve. Other work [22] has analysed
what kind of issues are reopened, and speciﬁc issue metrics such as
number of comments are shown to correlate. We conceptualise dif-
ﬁculty in terms of (1) reopened rate, (2) amount of discussion, and

 
 
 
 
 
 
MSR ’22, May 23–24, 2022, Pittsburgh, Pennsylvania, United States

Caddy, et al.

(3) resolution time. Further, prior work established that developers
want to be aware of unexpected issues in their repositories [19],
and that high importance issues are more likely to be included in
release notes [12]. In this work, we investigate whether surprising
issues are treated with higher importance. We conceptualise impor-
tance in terms of (1) mention in release notes, (2) ﬁrst issues to be
worked on after a break, (3) issues attracting GitHub reactions [15],
and (4) priority labels being assigned to issues.

1.1 Motivating Examples
Take for example, a software product sustainment team that re-
ceives a steady stream of issue reports which they must triage.
Each of these issues have a cost increasing at a given rate, which
itself might be accelerating. This cost may be in safety, budget,
and/or reputation for example. It is in the interest of stakeholders
that cost is minimised across the lifetime of the product.

In order to predict the cost of the new issues coming in, someone
with experience needs to spend time comparing against previous
experience, or applying metrics based methods. Then time needs
to be spent addressing the issue for the cost to be eliminated.

If surprisal can be used to tell which issues are challenging or
notable without human input, it is possible that eﬃciency can be
gained in the resolution process. The triage process is better in-
formed with notable issues that might need to be prioritised sooner.
The cost to address an issue, weighed against its cost to the project,
is better informed by the challenge that the issue presents. Chal-
lenging issues can have more people assigned to resolve it. Infor-
mation about notable issues can be distributed to other developers
so they know what mistakes to avoid in the future.

Another example is of a new developer just joining a project.
In order to familiarise themselves with the history and most im-
portant changes or developments that have occurred, they would
usually have to rely on release notes or a version history. In the
case where the project either lacks release notes, or in the case
where the release notes contain even the most minor changes, this
can be overwhelming and of limited value to the new developer. A
tool based on surprisal could extract the most important or notable
changes from a mature project for this developer, or even tell some-
one who has been away from a project for a period what notable
things have happened since they left.

2 BACKGROUND
In Claude Shannon’s seminal work on information theory he de-
scribes radio signals and their ability to communicate information.
In doing so, he makes the ﬁrst formal description of information
entropy or information uncertainty.

2.1 Uncertainty of an Event
Take two events, represented by the symbols ‘A’ and ‘B’. If we
know that these events are equally likely to occur, we could say
that we are equally uncertain which the next event will be. The
information that we gain from observing an event can be repre-
sented by 𝐼 (𝑥) = − log2 𝑃 (𝑥), where 𝑃 (𝑥) is the probability of
𝑥 occurring. In this example, we can see that for either outcome,
𝑃 (𝐴) = 𝑃 (𝐵) = 0.5 and so 𝐼 (𝐴) = 𝐼 (𝐵) = 1. We gain exactly 1 bit

of information, as there are two possible outcomes and either is as
likely to occur.

If we take another example, and modify the probabilities of the
outcomes such that 𝑃 (𝐴) = 0.0 and 𝑃 (𝐵) = 1.0, we can see that
if we observe B, we gain exactly 0 bits of information from 𝐼 (𝐵) =
− log2 1 = 0. This is intuitive, since we already knew that the event
would be B, there was no uncertainty.

Now we take an example where an event is extremely unlikely
to happen. When 𝑃 (𝐴) = 0.999 and 𝑃 (𝐵) = 0.001, if we observe
the event A, we gain 𝐼 (𝐴) = − log2 0.999 ≈ 0.001 bits of informa-
tion. Since it was likely to happen, we do not receive much infor-
mation, but still a little. It is then surprising when we observe the
event B, since it is unlikely to happen. The information we gain
from observing B, 𝐼 (𝐵) = −𝑙𝑜𝑔20.001 ≈ 9.966, is extreme in com-
parison.

2.2 Statistical Language Models
Shannon describes an approximation of the English language by
utilising n-gram statistical language models. Initially he describes
a crude unigram model that selects the next word in a sentence
based on their relative frequencies in the English language. This
has an obvious ﬂaw; unigram models assume that each choice of
word is independent from the last. Syntax and grammar demand
more careful choice of words than pure randomness alone, so to
account for this, greater order n-gram models are used.

Bigram models select the next word based on the previous word,
and trigrams select based on the previous two. These better capture
the structure of English, and while higher order models are possi-
ble, these tend to overﬁt the training data. The language model also
becomes exponentially sparser as the order increases, which will
in turn require exponentially more training data or risk severely
underﬁtting the training data.

With a model that statistically represents the English language
in n-grams, we can now determine how likely a word 𝑤𝑡 is, given
the words that precede it (𝑤1, ..., 𝑤𝑡−1).

2.3 Probability Distributions
To determine how closely an issue’s description represents the whole
corpus of descriptions, in an eﬀort to see how surprising or not it
is, we can use cross entropy. To understand cross entropy, it is im-
portant to discuss the underlying concept of entropy. Entropy can
be quantiﬁed as the average number of symbols needed to repre-
sent an event from a distribution of events. Take for example, a
distribution of events where:

𝑃 (𝐴) = 0.5,
𝑃 (𝐵) = 0.25,
𝑃 (𝐶) = 0.25

This distribution is biased, or skewed in favour of observing the
event ‘A’. We can describe these events with their relative frequen-
cies in a Shannon–Fano coding [5], where:

𝐴 = {0},
𝐵 = {01},
𝐶 = {10}

Is Surprisal in Issue Trackers Actionable?

MSR ’22, May 23–24, 2022, Pittsburgh, Pennsylvania, United States

In this example, we can see that half of the time, we only require 1
bit of information to represent any event. The other half of the time,
we require 2 bits. We can therefore say that the average number
of bits required to represent an event from this distribution (its
entropy) is 1.5. Just as above, where an event that occurs often can
be represented with less bits, distributions that are more biased,
that yield one particular event more often, present less entropy.
Shannon formalises this into the following equation, using 𝑋 as
the support for random events 𝑥:

𝐻 (𝑃) = − Õ
𝑥 ∈ 𝑋

𝑃 (𝑥) × log(𝑃 (𝑥))

(1)

Cross entropy describes how many symbols on average are re-
quired to represent an event from one distribution in a coding opti-
mised for another, if both have the same support. In our setting, it
measures how many symbols are required to represent an issue’s
description, based on the distribution of words observed (𝑃𝑜 ) in the
issue, in the true distribution (𝑃tt ) of words in all issues.

𝐻 (𝑃𝑜, 𝑃tt ) = − Õ
𝑥 ∈ 𝑋

𝑃𝑜 (𝑥) × log(𝑃tt (𝑥))

(2)

When 𝑃𝑜 = 𝑃tt , Equation (2) is equivalent to Equation (1). Oth-
erwise, when the observed distribution (the issue’s use of words)
diﬀers from the distribution of the training set (the set of all issues),
cross entropy increases. Higher values of cross entropy therefore
imply an issue is more surprising.

There are potentially other surprisal metrics that provide diﬀer-
ent evaluations. More simple measures for instance, might take the
minimum, maximum, or average surprisal of all words in an issue.
Cross entropy is a very well-realised metric of model accuracy in
the literature [11], and it is for this reason that we are using it.

3 RESEARCH QUESTIONS
A number of research questions and hypotheses have been made
to guide the investigation.

RQ1: How well does information theory’s surprisal, as
measured by a statistical language model, align
with perceived surprisal?

With RQ1, we hope to ﬁnd how statistical language models com-
pare to the human perception of surprisal, and also which factors
of the language model inﬂuence its ability to measure the surprisal
of an issue.

RQ2: Are surprising issues correlated with resolution dif-

ﬁculty?

In RQ2, we deﬁne “resolution diﬃculty” as a combination of the
following factors. Diﬃcult to resolve issues are: reopened more of-
ten; attract more discussion prior to resolution; and take longer to
be resolved, when compared to issues with little or no resolution
diﬃculty. This is perhaps an incomplete list, but will serve as the
basis for the report.

RQ2 can be formalised into the following hypotheses:
H2.1: Surprising issues are more likely to be reopened.

H02.1: There is no signiﬁcant diﬀerence in how often surprising
issues are reopened, compared to unsurprising issues.

H2.2: Surprising issues attract more discussion. (Number of peo-

ple involved)

H02.2: There is no signiﬁcant diﬀerence in how much discussion

surprising issues draw, compared to unsurprising issues.

H2.3: Surprising issues attract more discussion. (Number of inter-

actions)

H02.3: There is no signiﬁcant diﬀerence in how much discussion
surprising issues draw, compared to an unsurprising issue.

H2.4: Surprising issues take longer to resolve.
H02.4: There is no signiﬁcant diﬀerence in time to resolve surpris-

ing issues, compared to unsurprising issues.

H2.5: Surprising issues are diﬃcult, and diﬃcult issues are best
represented as some combination of reopen rate, amount of
discussion, and time to resolve.

H02.5: Surprising issues are diﬃcult, but diﬃculty is best repre-
sented as only one factor of reopen rate, amount of discus-
sion, or time to resolve.

RQ3: Are surprising pull requests correlated with resolu-

tion diﬃculty?

As in RQ2, RQ3 deﬁnes diﬃculty in the same way but for pull
requests rather than issues. The purpose of RQ3 is to determine
if the more structured and formal contents of pull requests are
more suitable than issues for establishing a relationship between
surprisal and diﬃculty. Pull requests oﬀer a suitable alternative be-
cause they are intended to directly address a need that would typ-
ically be expressed in an issue. Additionally, merged pull requests
are reviewed and thus are unlikely to be duplicated, describe what
they are resolving, and what they address is far less likely to be
misreported as a defect if it is not.

The formal hypotheses for RQ3 are formulated in a manner iden-
tical to the hypotheses of RQ2, since pull requests and issues are
functionally identical in this context. In the interest of brevity, the
full text of these hypotheses H2.1 through H2.5 has been omitted.

RQ4: Are surprising issues correlated with perceived im-

portance?

In RQ4, we deﬁne “perceived importance” as a combination of
the following factors. Important issues are: mentioned in release
notes; addressed soon after periods of breaks; attract more GitHub
reactions; and have high-priority labels added. Once again, this is
perhaps an incomplete list, but will serve as the basis for the report.

RQ4 can be formalised into the following hypotheses:

H4.1: Surprising issues are more likely to be mentioned in release

notes.

H04.1: There is no signiﬁcant diﬀerence in how often surprising
issues are mentioned in release notes, compared to unsur-
prising issues.

H4.2: Surprising issues are addressed with priority over unsurpris-

ing issues after a hiatus.

MSR ’22, May 23–24, 2022, Pittsburgh, Pennsylvania, United States

Caddy, et al.

H04.2: There is no signiﬁcant diﬀerence in the time addressing post-

hiatus surprising issues, compared to unsurprising issues.

H4.3: Surprising issues attract more GitHub reactions.
H04.3: There is no signiﬁcant diﬀerence in how many reactions a
surprising issue receives, compared to unsurprising issues.

H4.4: Surprising issues are more often labelled as high-priority

issues.

H04.4: There is no signiﬁcant diﬀerence in what priority surprising
issues are labelled, compared to unsurprising issues.

In H3.2, we deﬁne a hiatus as the top 25% longest times between
issue resolutions per contributor for a particular repository. Prior-
ity in this case is the order in which issues are worked on after a
hiatus.

RQ5: Are surprising pull requests correlated with per-

ceived importance?

As before with RQ2 and RQ3, RQ4 focuses on issues, and RQ5
will focus on their pull request counterparts. The full text of hy-
potheses H4.1 through H4.4 has been omitted for brevity.

4 VARIABLES
A summary of the variables involved for all the research questions
can be found in Table 1.
Predictor Variable:

• Surprisal. How surprising the content of the issue is to the
language model. This is calculated as the cross entropy of
the issue text and the corpus of issues.

Response Variables:

• Reopenings. How many times the issue has been reopened.
An issue can be labelled as closed and then reopened for nu-
merous reasons, just as a pull request can be merged and
then reopened. This usually indicates a regression of func-
tionality, reoccuring bug, or unsuccessful ﬁx [8], all indi-
cations that the issue has additional complexity associated
with it.

• Participants. How many individual participants have inter-
acted with the issue. Every event that takes place on an issue
has an actor associated with it. This actor represents some-
body interacting with the issue. If a particular issue involves
multiple assignees for example, it may be a sign that addi-
tional expertise is needed to resolve it. It could also mean
that it aﬀects a lot of people but is not necessarily more dif-
ﬁcult. Kavaler et. al. [23] show that there is a signiﬁcant in-
crease in issue resolution time with an increased number of
unique participants.

• Interactions. How many interactions have been made with
the issue, including comments, mentions, taggings, assign-
ments and state changes. A full list of events is available
in the GitHub Issue API documentation [3]. Some of these
interactions are considered a normal part of the resolution
process, although we expect to see more interactions if the
issue reveals hidden complexity over time. Kavaler et. al. [23]

also show there is an increase in issue resolution time cor-
responding to the number of comments made.

• Open State Duration. How long the issue has been unre-
solved; from ﬁrst submission to last closure, or if it has not
been closed, the time of analysis. While some issues may
not be diﬃcult but especially time consuming, we expect
to see longer resolution times for issues that are diﬃcult to
diagnose or replicate.

• Mentions in Release Notes. How many times the issue
has been mentioned within release notes on GitHub. Some
repositories make no use of GitHub’s Releases feature, so
only the repositories that do, and that mention issues at
all in them will be considered. Highly important issues are
more likely to be included in release notes [12].

• Order of Address. After lengthy breaks of development,
whether independently taken or due to holidays, it is likely
that the most pressing of issues in the backlog are chosen for
immediate resolution. Commits after extended breaks have
been described as interesting, in a previous paper [19]. Each
contributor has a time between addressing issues, so taking
the top 25% of these breaks, and then ordering the issues
that they worked on afterwards gives us an indication of
what importance that author places on each issue.

• Reactions. How many reactions have been made on the
issue. Reactions give a quick way for users to interact with
an issue. For example, users can express joy that a particular
issue is closed, or frustration if it disrupts them personally,
through reactions. This can be seen as a community rating
of importance, rather than that of the maintainers [15].
• Labelling. Many repositories use the GitHub issue labelling
system to triage incoming bug reports and feature requests.
Issues are sorted by maintainers into priorities and labelled
as such, from low priority to high priority [1], §3.B. These
labels can be seen as the maintainer’s rating of importance.

5 DATA SETS
In this section we present the data sources that we will use, and
how we will use them.

5.1 Sources
For this study, open-source software stored on GitHub serves as
our primary and sole source for software issues. Unfortunately,
GitHub imposes a restrictive limit to how many interactions with
its API a user can make. Typically this is 5000 calls per hour [2], and
so we take the top 5000 most ‘starred’ repositories as our data set.
Stars represent a user liking a repository, and therefore indicate
popular repositories, more likely to have high numbers of issues
due to increased testing and feature requests — a result of more
users [13].

This supposes that GitHub is used in the same way by the main-
tainers of those 5000 repositories, which is not the case. Many of
these repositories do not make use of the Issues feature; many of
these repositories are not software development related and pos-
sess little to no code; and other repositories are simply a mirror of
a repository developed and hosted elsewhere [29]. Additionally, we

Is Surprisal in Issue Trackers Actionable?

MSR ’22, May 23–24, 2022, Pittsburgh, Pennsylvania, United States

Variable

Hypotheses

Description

Measure Operationalisation

Table 1: Variables

Surprisal

Predictor for
all hypotheses

How surprising an issue is to the statistical
language model.

Ratio

Reopenings

Response for
H1.1, H2.1

Participants Response for

H1.2, H2.2

Interactions Response for

Open State
Duration

Mentions

Order of
Address
Reactions

Labelling

H1.3, H2.3
Response for
H1.4, H2.4

Response for
H3.1, H4.1
Response for
H3.2, H4.2
Response for
H3.3, H4.3
Response for
H3.4, H4.4

Ratio

After an issue has been labelled as closed
or resolved, it can be reopened either due
to a ﬁx being unsuccessful, a regression, or
reoccurring bug.
Number of unique individuals involved
with the issue. Each event (as described by
the GitHub API [3]) associated with an is-
sue has an actor that initiates it, whether
that event is a comment, or a state update.
Number of events associated with the issue. Ratio

Ratio

Length of time between the issue’s creation,
and it being resolved for the ﬁnal time.

Ratio

Number of mentions within a repository’s
release notes.
Issue order after top 25% of contributor’s
inter-issue resolution times.
Number of reactions on a particular issue.

Ratio

Interval

Ratio

Assigned priority or importance label of a
particular issue.

Interval

Cross entropy of issue, obtained with
probability from SLM trained on cor-
pus of all issues.
GitHub Issue API’s “reopened” event.

GitHub Issue API’s event “actor” for
each event associated with an issue.

issue numbers through

Count of events, as described by the
GitHub Issue API [3].
Diﬀerence between GitHub Issue API’s
“created_at” value for the issue and ﬁ-
nal “closed” or “merged” event.
Scrape for
GitHub’s Releases API.
Issues assigned to a contributor are re-
trieved through the GitHub Issues API.
Count of reactions for an issue, from
GitHub’s Reactions API.
Labels are extracted via the GitHub
Issues API,
and then normalised
(per repository) to a 3-degree scale,
‘low-importance’, ’regular-importance’,
‘high-importance’.

wish to limit the scope to English language repositories. To ﬁnd the
repositories that ﬁt these conditions, we plan to use G-Repo [16] as
a means to ﬁlter out non-software repositories, repositories mak-
ing little use of the Issues feature (less than 1000 issues), and non-
English repositories.

5.2 Language Model Transfer
The statistical language model (SLM) used to determine the sur-
prisal of an issue, is intended to represent a probabilistic model of
what the content of an issue looks like. Software issues are typi-
cally written in such a way that requires domain-speciﬁc knowl-
edge of terminologies and jargon, and are in most cases very spe-
ciﬁc to the project they reference. As a result, a more speciﬁc SLM,
trained on a software-based corpus would see some improvement
in accurately modelling the software language used, compared to
a more general pre-trained model [4], §5.3. For this reason, it was
decided that a bespoke language model will be trained for the task.

5.3 Pre-processing
Issues are composed of a title and description. Both of these ele-
ments have the possibility of individually containing pertinent in-
formation, and in some examples do not describe the information
the other holds. It is for this reason that during the pre-processing
stage, we will prepend the issue description with the issue title. The
SLM will then be trained on these title-description combinations.
Before training a model on the issue text, we will clean the data

with the following pre-processing steps:

(1) HTML Void Elements [27] are translated into special tokens,

e.g., <br> becomes [BR].
(Also code blocks, see details following.)

(2) Other HTML elements are replaced with their contents, e.g.,

a list element becomes a simple string of its content.

(3) Text undergoes normalisation of its Unicode forms. Canoni-
cal Composition (NFC) is used in accordance with the Char-
acter Model standard proposed by W3C [10].

(4) Punctuation and symbols are removed on word boundaries,

and when isolated.

(5) Stop words are removed, and remaining words are stemmed.

(See details following.)

MSR ’22, May 23–24, 2022, Pittsburgh, Pennsylvania, United States

Caddy, et al.

As singular code tokens — variable names and the like — may
provide important contextual information across multiple issues,
they will be preserved in the training data. Code blocks on the
other hand risk introducing too many globally-unique tokens into
the model without introducing useful and actionable information
to the description. The reason is that the syntax of code is entirely
disparate with that of English for example, and so code blocks
will be replaced with a special token [CODE] where they are de-
marcated with the <pre> and <code> HTML tags (as is typical on
GitHub).

It is possible that the quality of the language model could be im-
proved by further transforming the text in a ﬁnal step. In many nat-
ural language processing applications, stop word removal is con-
ducted to remove low-information tokens, thereby increasing the
average entropy. In other applications, lemmatisation or stemming
is used to reduce the occurrence of high-information tokens. Con-
cepts are learned better if you can reduce “ﬁshing” and “ﬁshlike” to
“ﬁsh”. However we cannot know for certain what using either will
achieve when modelling surprisal, and a preliminary investigation
will be conducted to see if it indeed increases the accuracy of the
model. The choice of algorithms is still to be determined, and will
be revisited in the ﬁnal report.

5.3.1 Priority Labelling. Some GitHub repositories use the labelling
system to assign priority grades or importance to issues in their
triage process. These labels are user text input and can represent
these grades in a number of diﬀerent ways. For example, one repos-
itory may use the labels ‘Low Priority’, and ‘High Priority’, where
another may use the labels ‘P1’ through ‘P5’.

In order to process these priorities, it is necessary for manual
classiﬁcation to normalise these diﬀerent ranges. For the purpose
of this experiment, priority is distributed among the three degrees
‘low-importance’, ‘regular-importance’, and ‘high-importance’. Tie-
breaking is settled by ruling in favour of a higher importance, using
the previous examples for example; ‘low-importance’ would take
‘P1’ and ‘Low Priority’; ‘regular-importance’ would take ‘P2’ and
‘P3’; and ‘high-importance’ would take ‘P4’, ‘P5’, and ‘High Prior-
ity’.

To ensure that these have been correctly classiﬁed, we propose
giving two researchers a list of 400 randomly selected labels from
the population of approximately 9000 diﬀerent labels and asking
them to classify them using the method above, or classify them
as ‘unrelated’ to priority or importance. The agreement between
the researchers is then calculated using Cohen’s kappa, and in the
event that a consensus value of 0.7 is reached, we consider the
task suﬃciently unambiguous, and a single researcher can then be
trusted to accurately represent the rest of the ∼ 9000 labels. If not,
a more involved process needing agreement by more researchers
is required to classify the labels.

6 EXECUTION PLAN
In this section we present the method we plan to use for the study.
It is important that all the decisions made during the experiment
will be recorded, along with intermediate outputs. For example:
a record will be made listing the 5000 most starred repositories
on GitHub at the time of the experiment; after repositories are re-
moved when failing to meet selection criteria, those will also be

made into a record; and decisions such as how the total issue count
was calculated will be made into a record too. All the code used will
be made available in a GitHub repository for the ﬁnal report.

6.1 Method
To test all hypotheses the following method is proposed:

(1) Using the GitHub API, query the 5000 most starred reposi-

tories on GitHub.

(2) Query the number of issues. If any repository has less than

1000 issues, remove it from the pool.

(3) Extract all GitHub issues from each repository.
(4) Train SLM on entire issue description corpus.
(5) For each repository:

(a) Determine if it uses priority/importance labels, and nor-
malise those labels into three degrees of importance. Ties
broken in favour of being more important.

(b) For each issue:

(i) Determine and record surprisal of issue.
(ii) Record how many re-openings have occurred.
(iii) Record how many unique participants have interacted

with the issue.

(iv) Record how many interactions have been made with

the issue.

(v) Calculate and record open duration for the issue.
(vi) Record number of reactions to the issue.
(vii) Record normalised importance label, if any.
(c) Parse all release notes for the repository, incrementing
the mentions of a particular issue when it appears.

(d) For each contributor:

(i) Determine top 25% of inter-issue resolution times.
(ii) Split all issues assigned to the contributor into bands
following a break of at least the 25th percentile’s length.

(iii) Order the issues by oldest assigned time.
(iv) Record the ordinal position of each issue in its inter-

break band.

(6) Split issues from GitHub into pull requests and simple is-

sues.

(7) Generate descriptive statistics for each repository according

to analysis plan.

(8) Generate inferential statistics across entire issue data set ac-

cording to analysis plan.

6.2 Statistical Language Modelling
N-gram models saw some popularity during the 2000s and 2010s,
but gradually saw declining popularity due to their perceived con-
textual fragility [28](2000), and competitive neural language mod-
els being developed [21](2012). Despite this, n-gram models still
perform admirably in Natural Language Processing applications.
For our SLM, we use a trigram model using word tokens (shingles).
We chose this due to its simplicity, familiarity, popularity, and rea-
sonably eﬀective results [6]. More modern statistical approaches
typically use a n-gram model utilising a backoﬀ strategy to deal
with unknown tokens.

We train the SLM using the entire corpus of issues. This gives
us the advantage of never coming across an unknown token, as all
the tokens that we generate surprisal values for are contained in

Is Surprisal in Issue Trackers Actionable?

MSR ’22, May 23–24, 2022, Pittsburgh, Pennsylvania, United States

Start

Query GitHub API

Pre-processing

Calculate Diﬃculties

Train SLM

Calculate Importance

Select Test Issue

Generate Statistics

End

Figure 1: Method Flowchart

the training set. Any unknown, in this case ‘unique’, tokens would
otherwise be inﬁnitely surprising, which might not accurately rep-
resent how common it is to appear in say a larger sample size of
repositories and issues.

The loss of generality realised by the lack of testing set is in-
signiﬁcant when compared to the small sample size of 5000. For
this study, we are more concerned with the formulation of sur-
prisal, and the two correlations applying to the repositories we
have selected. Transferability can be further investigated after this
proof of concept, if successful. Conversely, because singular repos-
itories have a small sample size of issues, the speciﬁcity gained by
training only on a single repository introduces massive sparsity
in the model. This is another reason we include all issues into the
training set.

6.3 Model Smoothing
Despite the lack of unknown tokens, we still apply a smoothing
algorithm to obtain a more uniform, less sparse, and more repre-
sentative model of the larger corpus of issues outside of the repos-
itories that we have selected. The modiﬁed Kneser-Ney algorithm
described by Chen and Goodman [6], §3, was chosen due to its
excellent smoothing performance.

6.4 Model Improvement
While trigrams were chosen for the reasons previously mentioned,
it is worth understanding how n-gram order aﬀects the language
model’s calculation of surprisal. It is also worth understanding how
the use of a testing set from the training data aﬀects this calculation
of surprisal.

These two factors contribute to the SLM and reveal important
information regarding RQ1. In order to measure the aﬀects of these
factors, we propose the following additional experiment which com-
pares the SLM performance against a manual classiﬁcation:

(1) A repository of relatively few issues is chosen as a model.
(2) Take a representative sample of issues from the repository.
(3) Two researchers are asked to rate the surprisal in each issue,

after reading all issues thoroughly.
(a) Issues are rated on a Likert scale.
(b) A value of 1 means that the description of an issue is not

surprising.

(c) A value of 5 means that the issue contains almost com-

pletely unique information.

(d) Surprisal judgements should be based on factors such as

topic, formatting, length, and word usage.

(4) Choose the SLM training set:

(a) Entire issue descriptions corpus.
(b) Entire issue descriptions corpus, with the chosen reposi-

tory absent.

(c) Entire issue descriptions corpus, with each issue removed

individually.

(5) Choose n-gram order, repeating from 1-grams to 10-grams.
(6) Measure and record surprisal of each issue (the same indi-
vidually removed from the training set if that type of train-
ing set is chosen).

(7) Generate agreement statistics between each model and the

manual classiﬁcation.

7 ANALYSIS PLAN
This is a correlational study of independent random variables, the
following is a design of the quantitative analysis that will be under-
taken. All calculations will be performed using a statistics package.

Model Analysis. In Section 6.4 we propose a method with which
the surprisal of a repository-representative sample of issues is judged
manually by two researchers. The surprisal of the issues is then
measured using the surprisal calculated by the SLM. We propose
using Cohen’s kappa to measure the agreement between the two
researchers, and report how contentious this task is. We also pro-
pose using a Kendall rank correlation to measure the agreement
between the SLM- and manually-computed surprisal values. In the
event that the agreement is statistically signiﬁcant, it can be said
that SLMs can calculate surprisal to a similar degree as a human
participant can, and are suitable for the following analyses. A quali-
tative analysis of the issues that have disagreement in the surprisal
ratings is conducted. Both the case of inter-researcher disagree-
ment and researcher-SLM disagreement will be analysed in an ef-
fort to understand what caused this disagreement.

We expect that a language model using the entire corpus of issue
descriptions and trigrams suﬃciently agrees with the researcher’s
judgements. However, we will run the deeper analysis using dif-
ferent training sets and n-gram orders to conﬁrm those choices
are indeed correct. A combination of each choice will maximise
the agreement with the manual computation, and this combina-
tion will be used in the analyses going forward. In the event that
even the combination that maximises the agreement is still not sta-
tistically signiﬁcant, a more thorough inspection of the factors in-
ﬂuencing the SLM is required.

Descriptive Statistics. We will present descriptive statistics of the
predictor and response variables in a summary of the complete

MSR ’22, May 23–24, 2022, Pittsburgh, Pennsylvania, United States

Caddy, et al.

[16] Simone Romano, Maria Caulo, Matteo Buompastore, Leonardo Guerra, Anas
Mounsif, Michele Telesca, Maria Teresa Baldassarre, and Giuseppe Scan-
niello. 2021.
In
2021 IEEE International Conference on Software Analysis, Evolution and
Reengineering (SANER).
IEEE, Los Alamitos, CA, United States, 551–555.
https://doi.org/10.1109/SANER50967.2021.00064

to Support MSR Studies on GitHub.

G-Repo: a Tool

[17] Maurice Dawson, Darrell Norman Burrell, Emad Rahim, and Stephen Brewster.
2010. Integrating software assurance into the software development life cycle
(SDLC). Journal of Information Systems Technology and Planning 3, 6 (2010), 49–
53.

[18] Kristijan Armeni, Roel M Willems, and Stefan L Frank. 2017. Probabilistic lan-
guage models in cognitive neuroscience: Promises and pitfalls. Neuroscience &
Biobehavioral Reviews 83 (2017), 579–588.

[19] Christoph Treude, Fernando Figueira Filho, and Uirá Kulesza. 2015. Summariz-
ing and measuring development activity. In Proceedings of the Joint Meeting on
Foundations of Software Engineering. ACM, New York, NY, United States, 625–
636.

[20] Michael H Kutner, Christopher J Nachtsheim, John Neter, et al. 2004. Applied lin-
ear regression models. Vol. 4. McGraw-Hill/Irwin, New York, NY, United States.
[21] Tomáš Mikolov. 2012. Statistical Language Models Based on Neural Networks.
Ph.D. thesis. Brno University of Technology, Faculty of Information Technology.
https://www.ﬁt.vut.cz/study/phd-thesis/283/

[22] Abdillah Mohamed, Li Zhang, Jing Jiang, and Ahmed Ktob. 2018. Predicting
which pull requests will get reopened in GitHub. In 2018 25th Asia-Paciﬁc Soft-
ware Engineering Conference (APSEC). IEEE, Los Alamitos, CA, United States,
375–385.

[23] David Kavaler, Sasha Sirovica, Vincent Hellendoorn, Raul Aranovich, and
Vladimir Filkov. 2017. Perceived language complexity in GitHub issue discus-
sions and their eﬀect on issue resolution. In 2017 32nd IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, Los Alamitos, CA,
United States, 72–83.

[24] Samuel Sanford Shapiro and Martin B Wilk. 1965. An analysis of variance test

for normality (complete samples). Biometrika 52, 3/4 (1965), 591–611.

[25] Charles Spearman. 1987. The proof and measurement of association between
two things. The American Journal of Psychology 100, 3/4 (1987), 441–471.
[26] Danielle Gonzalez, Thomas Zimmermann, Patrice Godefroid, and Max Schäfer.
2021. Anomalicious: Automated Detection of Anomalous and Potentially Ma-
licious Commits on GitHub. In Proceedings of the International Conference on
Software Engineering: Software Engineering in Practice. IEEE, Los Alamitos, CA,
United States, 258–267.

[27] HTML Living Standard. 2022.

HTML Living Standard: Void Elements.

[28] R. Rosenfeld. 2000.

https://html.spec.whatwg.org/#void-elements
Two decades of
Proc.

where do we go from here?
https://doi.org/10.1109/5.880083

statistical
language modeling:
IEEE 88, 8 (2000), 1270–1278.

[29] Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel M Ger-
man, and Daniela Damian. 2014. The promises and perils of mining github. In
Proceedings of the 11th Working Conference on Mining Software Repositories (MSR).
ACM, New York, NY, United States, 92–101.

data set. Sample size, mean, standard deviation, maximum, and
minimum values for each variable will be included

Inferential Statistics. Before presenting a correlation and regres-
sion analysis using a linear regression, we graph the relationship
between the surprisal and each response variable for the issues in
a repository. We will then test each hypothesis separately with its
corresponding variable, and look for statistical signiﬁcance.

A Shapiro-Wilk test is used to determine if the data shows nor-
mality [24]. If the data is normally distributed, we can choose to
use the additional descriptive power of a Pearson correlation. If
not, a Spearman correlation can be used instead [25], §1.2.a).

In order to test H1.5 and H2.5, we will perform a multiple linear
regression, this time including one statistically signiﬁcant measure
of diﬃculty into the null model. If the F-test is not statistically
signiﬁcant, we can accept the alternate hypothesis.

This test shows if a combination of our chosen diﬃculty factors
is better in representing diﬃculty as a whole, in comparison to the
measure added to the null model on its own. When performing this
second regression, we expect to see multicollinearity, a high Vari-
ance Inﬂation Factor for these measures [20], §10.5, as our belief is
that they represent the same variable: diﬃculty.

Interpretation of Results. We will present our interpretation of
our ﬁndings, a discussion on any assumptions discovered, limita-
tions, threats to validity, and future research.

REFERENCES
[1] Xiaoyuan Xie, Yuhui Su, Songqiang Chen, Lin Chen, Jifeng Xuan, and Baowen
Xu. 2021. MULA: A Just-In-Time Multi-labeling System for Issue Reports. IEEE
Transactions on Reliability 1, 1 (2021), 1–14.

[2] GitHub Docs. 2022. Rate limit. https://docs.github.com/en/rest/reference/rate-limit
[3] GitHub

types.

Docs.

event

2022.

Issue

https://docs.github.com/en/developers/webhooks-and-events/events/issue-event-types

[4] Jun Wang, Xiaofang Zhang, and Lin Chen. 2021. How well do pre-trained contex-
tual language representations recommend labels for GitHub issues? Knowledge-
Based Systems 232 (2021), 107476.

[5] Robert M Fano. 1949. The transmission of information. Massachusetts Institute of
Technology, Research Laboratory of Electronics, Cambridge, MA, United States.
[6] Stanley F Chen and Joshua Goodman. 1999. An empirical study of smoothing
techniques for language modeling. Computer Speech & Language 13, 4 (1999),
359–394.

[7] Larissa Leite, Christoph Treude, and Fernando Figueira Filho. 2015. UEDash-
board: Awareness of unusual events in commit histories. In Proceedings of the
Joint Meeting on Foundations of Software Engineering. ACM, New York, NY,
United States, 978–981.

[8] Jing Jiang, Abdillah Mohamed, and Li Zhang. 2019. What are the characteristics
of reopened pull requests? a case study on open source projects in github. IEEE
Access 7 (2019), 102751–102761.

[9] Raman Goyal, Gabriel Ferreira, Christian Kästner, and James Herbsleb. 2018.
Identifying unusual commits on GitHub. Journal of Software: Evolution and Pro-
cess 30, 1 (2018), e1893.

[10] W3C Working Group. 2021. Character Model for the World Wide Web: String
Matching. https://www.w3.org/TR/charmod-norm/#normalizationChoice
[11] Robert K. Niven. 2007. Combinatorial Information Theory: I. Philosophical Basis
of Cross-Entropy and Entropy. arXiv:cond-mat/0512017 [cond-mat.stat-mech]
[12] Surafel Lemma Abebe, Nasir Ali, and Ahmed E Hassan. 2016. An empirical
study of software release notes. Empirical Software Engineering 21, 3 (2016),
1107–1142.

[13] Hudson Borges, Andre Hora, and Marco Tulio Valente. 2016. Understanding
the factors that impact the popularity of GitHub repositories. In Proceedings of
the International Conference on Software Maintenance and Evolution. IEEE, Los
Alamitos, CA, United States, 334–344.

[14] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.
2016. On the naturalness of software. Commun. ACM 59, 5 (2016), 122–131.
[15] Hudson Borges, Rodrigo Brito, and Marco Tulio Valente. 2019. Beyond Textual
Issues: Understanding the Usage and Impact of GitHub Reactions. In Proceedings
of the Brazilian Symposium on Software Engineering. ACM, New York, NY, United
States, 397–406.

