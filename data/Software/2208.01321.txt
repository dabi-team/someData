Mimicking Production Behavior with Generated
Mocks

1

Deepika Tiwari, Martin Monperrus, and Benoit Baudry

KTH Royal Institute of Technology, Sweden

(cid:70)

2
2
0
2

g
u
A
2

]
E
S
.
s
c
[

1
v
1
2
3
1
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Mocking in the context of automated software tests allows
testing program units in isolation. Designing realistic interactions be-
tween a unit and its environment, and understanding the expected
impact of these interactions on the behavior of the unit, are two key
challenges that software testers face when developing tests with mocks.
In this paper, we propose to monitor an application in production to gen-
erate tests that mimic realistic execution scenarios through mocks. Our
approach operates in three phases. First, we instrument a set of target
methods for which we want to generate tests, as well as the methods
that they invoke, which we refer to mockable method calls. Second, in
production, we collect data about the context in which target methods
are invoked, as well as the parameters and the returned value for each
mockable method call. Third, ofﬂine, we analyze the production data
to generate test cases with realistic inputs and mock interactions. The
approach is automated and implemented in an open-source tool called
RICK. We evaluate our approach with three real-world, open-source Java
applications. RICK monitors the invocation of 128 methods in production
across the three applications and captures their behavior. Next, RICK
analyzes the production observations in order to generate test cases
that include rich initial states and test inputs, mocks and stubs that
recreate actual interactions between the method and its environment,
as well as mock-based oracles. All the test cases are executable, and
52.4% of them successfully mimic the complete execution context of the
target methods observed in production. We interview 5 developers from
the industry who conﬁrm the relevance of using production observations
to design mocks and stubs.

Index Terms—Mocks, production monitoring, mock-based oracles, test
generation

1 INTRODUCTION

S OFTWARE testing is an expensive, yet indispensable,

activity. It is done to verify that the system as a whole,
as well as the individual modules that compose it, behave as
expected. The latter is achieved through unit testing. When
a unit interacts with others, or with external components,
such as the ﬁle system, database, or the network, it becomes
challenging to test it in isolation. As a solution to this
problem, developers rely on a technique called mocking [1].
Mocking allows a unit to be tested on its own, by replacing
dependent objects that are irrelevant to its functionality,
with “fake“ implementations. There are several advantages
of mocking, such as faster test executions, fewer side-effects,
and quicker fault localization [2].

Despite their advantages, using mocks within tests re-
quires considerable engineering effort. While designing the

tests for one component, developers must ﬁrst decide which
interactions may be replaced with mocks [3]. They must
also determine how these mocks behave when triggered
with a certain input, i.e., how they are stubbed [4]. In
order to address these challenges, several techniques have
been developed to automatically generate mocks. For ex-
ample, mocks have been generated through search-based
algorithms by including contextual information, such as the
state of the environment, in the search space for input data
generation [5], [6]. Dynamic symbolic execution has been
used to deﬁne the behaviour of mocks through generated
mock classes [7]. System test executions can be monitored
to derive unit tests that use mocks [8]. This preliminary
research has validated the concept of mock generation.

The fundamental premise of mocking is to replace a real
object with a fake one that mimics it [9], [10]. This implies
that, for it to be useful, the behavior of the mock should
resemble, as closely as possible, that of a real object [11].
This is where the state of art falls short, the generated mocks
are not guaranteed to perform realistic behavior.

Our key insight is to derive realistic behavior from real
behavior, i.e. to generate mocks from production usage. This
builds upon the fact that 1) the behavior of an application in
production is the reference one [12], and 2) this production
behavior can be used for test generation [13], [14]. Given a
set of methods of interest for test generation, our idea is to
monitor the invocation of these methods in production. As
a result of this monitoring, we capture realistic execution
contexts and use it to generate tests for each target method,
where external objects are replaced with mocks, and stubbed
based on production observations.

In this paper, we propose RICK, a tool for mimicking
production behavior with generated mocks. RICK monitors
an application executing in production with the goal of
generating valuable tests. The intention of the generated
tests is to verify the behavior of the methods, where the
reference behavior captured in the oracle is the one from
production. The interactions of this method with other exter-
nal objects are mocked. Within each generated test, the data
captured from production is expressed as rich serialized test
inputs. Each generated test has mock-based oracles, which
verify distinct aspects of the invocation of the target method
and its interactions with mock methods, such as the object
returned from the target method, the parameters with which

 
 
 
 
 
 
the mock methods are called, as well as the frequency and
sequence of these mock method calls. The oracles ensure
that the generated tests fail if the production behavior is not
reproduced ofﬂine.

We evaluate the capabilities of RICK with three open-
source Java applications: a map-based routing application
called GraphHopper, a feature-rich graph analysis and vi-
sualization tool called Gephi, and the utility library PDFBox
to work with PDF documents. We target 212 methods across
the three applications, which get invoked as the applications
execute in typical production scenarios. RICK generates 294
tests for 128 of these methods. Within each generated test,
RICK recreates execution states that mimic production ones
with objects that range from 37 bytes to 39 megabytes. When
executed, 154 of the 294 (52.4%) tests successfully recreate
the complete production execution context. These results
indicate that RICK is capable of monitoring applications in
production, capturing realistic behavior for target methods,
and transforming it into tests that mimic the behavior of the
methods, while isolating it from its interactions with exter-
nal objects. To assess the quality of the RICK-generated tests,
we interview 5 software developers from different sectors of
the IT industry. All of them ﬁnd the collection of production
values to be relevant to generate realistic mocks. Moreover,
they appreciate the structure and the understandability of
the tests generated by RICK.

To sum up, the key contributions of this paper are:
• The concept of the automated generation of mocks,
stubs, and oracles using data collected from production.
• A comprehensive methodology for generating tests that
mimic complex production interactions through mocks,
by capturing receiving objects, parameters, returned
values, and method invocations, for a method under
test, while an application executes.

• An evaluation of the approach on 3 widely-used, large,
open-source Java applications, demonstrating the feasi-
bility of generating mocks on real world software.

• A publicly available tool called RICK implementing the
approach for Java, and a replication package for future
research on this novel topic1.
The rest of the paper is organized as follows. section 2
presents the background on mocking and mock objects. We
describe how RICK generates tests and mocks in section 3.
Next, section 4 discusses the methodology we follow for
our experiments, applying RICK to real-world Java projects,
followed by the results of these experiments in section 5. sec-
tion 6 presents closely related work, and section 7 concludes
the paper.

2 BACKGROUND

This section summarizes the key concepts of mock objects,
and how they are used in practice. We also discuss the
challenges of using mocks within tests.

2.1 Mock Objects

Software comprises of individual modules or units. These
units interact with each other, as well as with external

2

libraries, for example, to send emails, transfer data over
the network, or perform database operations. This facili-
tates modular development, as different teams can work
in parallel on implementing distinct functionalities of the
system. The modules are then composed together, in order
to achieve use cases. Yet, a disadvantage of this coupling is
that testing each unit in isolation from others is not straight-
forward. Mocking was proposed as a solution to this problem
[1]. It is a mechanism to replace real objects with skeletal
implementations that mimic them [2]. Mocking allows indi-
vidual functionalities to be tested independently. The pro-
cess of unit testing with mocks is faster and more focused
[3]. Since the test intention is to verify the behaviour of
one individual unit, mocking can facilitate fault localization.
Interactions that are difﬁcult to set up within a test can be
replaced with mocks [15]. Furthermore, a test can be made
more reliable by using mocks to replace external, potentially
unreliable or non-deterministic, components [5], [6]. Mock
objects typically behave in speciﬁc, pre-determined ways
through a process called stubbing [9]. For example a method
called getAnswer invoked on a mock object can be stubbed
to return a value of 42, without the actual invocation of
getAnswer. Stubbing can be very useful for inducing be-
haviour that may be hard to produce locally within the test,
such as error- or corner-cases. Mocking can also be used
for verifying object protocols [16]. For example, consider a
method called subscribeToNewsletter, which should call
another method sendWelcomeEmail on an object of type
EmailService. Developers can mock the EmailService
object within the test for subscribeToNewsletter, to ver-
ify that the method sendWelcomeEmail is indeed invoked
on EmailService exactly once, with a parameter of type
UserID. This interaction is therefore veriﬁable without side-
effects, i.e., without an actual email being sent.

In short, the three key concepts of testing with mocks are
Mocking, Stubbing, and Verifying. Real objects can be replaced
with fake implementations called mocks. These mocks can
be stubbed to deﬁne tailor-made behaviors, i.e., produce a
certain output for a given input. Moreover, the interactions
made with the mocks can be veriﬁed, such as the number
of times they were triggered with a given input, and in a
speciﬁc sequence.

2.2 The Practice of Testing with Mocks

Mocks can be implemented in several ways. For example,
developers may manually write classes that are intended
as replacements for real implementations [17]. However,
a more common way of deﬁning and using mocks is
through the use of mocking libraries, which are available
for most programming languages. Mockito2 is one of the
most popular mocking frameworks for Java, both in the
industry and in software engineering research [18], [19]. It
can be integrated with testing frameworks such as JUnit and
TestNG, allowing developers to write tests that use mocks.
the method
purchaseTickets presented in Listing 1. This method
handles the purchase of tickets, including the interaction
with the payment gateway, PaymentService.
is
deﬁned in the ReservationCentre class, and takes two

consider

example

Let us

the

of

It

1. https://github.com/castor-software/rick-experiments

2. https://site.mockito.org/

1 public class ReservationCentre {
2
3
4

...
// Target method
public void purchaseTickets(int quantity, PaymentService

paymentService) {

...
double amount = basePrice * quantity;
...
// Mockable method call #1
if (paymentService.checkActiveConnections() > 0) {

...
// Mockable method call #2
boolean isPaymentSuccessful =

paymentService.processPayment(amount);

...

}
...
return ...;

}
...

5
6
7
8
9
10
11
12

13
14
15
16
17
18
19 }

Listing 1: Target method purchaseTickets has mockable
method calls on the PaymentService object

1 @Test
2 public void testTicketPurchasing() {
3

ReservationCentre resCentre = new ReservationCentre();

// Mock external types
PaymentService mockPayService = mock(PaymentService.class);

// Stub behavior
when(mockPayService.checkActiveConnections()).thenReturn(1);
when(mockPayService.processPayment(42.24)).thenReturn(true);

resCentre.purchaseTickets(2, mockPayService);

// Verify invocations on mocks
verify(mockPayService, times(1)).checkActiveConnections();
verify(mockPayService, times(1)).processPayment(anyDouble());

5
6

8
9
10

12

14
15
16
17 }

Listing 2: A test for the purchaseTickets method which
mocks PaymentService

parameters. The ﬁrst parameter is an integer value for the
quantity of tickets, and the second parameter is the object
paymentService of the external
type PaymentService.
Two methods are called on the paymentService object:
checkActiveConnections on line 9, and processPayment
on line 12 which accepts a parameter of type double.
We illustrate the use of mocks, stubs, and veriﬁcation
through the unit test for purchaseTickets presented in
Listing 2. The intention of this test, testTicketPurchasing,
is to verify the behavior of purchaseTickets, while
mocking its interactions with PaymentService. First, the
receiving object resCentre of type ReservationCentre
is set up (line 3). Next, PaymentService is mocked,
(line 6). Lines 9
through the mockPayService object
and 10 stub the two methods called on this mock:
checkActiveConnections is stubbed to return a value
of 1, and processPayment is stubbed to return true
when invoked with the double value 42.24. Finally, on
line 12, purchaseTickets is called with the quantity
of 2, and the mocked parameter mockPayService. The
statements on lines 15 and 16 verify that this invocation
of
checkActiveConnections
exactly once, and processPayment exactly once, with
a double value (speciﬁed using anyDouble()). Thus,
this test veriﬁes the behavior of
the target method,
purchaseTickets, isolating it from the interactions with
a real PaymentService object. Moreover, method calls on

purchaseTickets

calls

3

PaymentService are stubbed so that purchaseTickets
gets executed as it normally would, without the side-effect
of an actual payment being made. This allows for more
focus on the method under test, purchaseTickets.

2.3 The Challenges of Mocking

Despite the beneﬁts of mocking that we highlight,
it
is not trivial to incorporate mocks in practice. Deciding
what to mock, and how the mocks should behave,
is
hard [10]. For example, developers would ﬁrst identify
that PaymentService may be mocked within the test for
purchaseTickets in Listing 2. Next, they must also manu-
ally deﬁne concrete values for the parameters and returned
values for stubbing the calls made on this mock, in order
trigger a speciﬁc path through purchaseTickets. Addition-
ally, they would have to determine which interactions made
on this mock are veriﬁable. It is also challenging to decide
between conventional object-based testing and mock-based
testing. A study by Spadini et al. [18] found that mocks are
most likely to be introduced at the time a test class is ﬁrst
written. It is unlikely that mocks are added to a test class
when the code evolves, or even that the mocks implemented
in the ﬁrst version of a test class are later updated. This
implies that, even for larger, well-tested projects, developers
may be hesitant to start using mocks, despite their many
advantages. To address the challenges of manually imple-
menting mocks, several studies propose methodologies for
their automated generation, such as through search-based
algorithms [20] or symbolic execution [21]. However, none
of these studies use data from production executions to do
so. In this work, we propose to monitor an application in
production, with the goal of generating tests with mocks.
These tests use mocks to 1) isolate a target method from
external units, and 2) verify distinct aspects of the behavior
with oracles speciﬁc to mocks.

3 MOCK GENERATION WITH RICK
We introduce RICK, a novel tool for automatically generating
tests with mock objects, using data collected during the
execution of an application. In production, RICK collects
realistic data for the receiving object state and parameters
of the method under test, as well as the parameters and
values returned by methods called on external objects. In
subsection 3.1, we present an overview of the RICK pipeline.
This is followed in subsection 3.2 by a discussion of the
kinds of oracles produced by RICK. Next, subsection 3.3
motivates the design decisions of RICK and highlights its key
features. Finally, subsection 3.4 presents technical details of
its implementation.

3.1 Overview

RICK operates in three phases. We illustrate them in Figure 1.
In the ﬁrst phase 1 , RICK identiﬁes test generation targets
within an application. These targets are called methods
under test, and they have mockable method calls. We deﬁne
them as follows:

Methods under Test (MUTs): The target methods for test
generation by RICK are called methods under test (MUTs).
A method is considered as being an MUT, if it invokes

4

Fig. 1: The RICK test generation pipeline: ofﬂine, identify methods under test and mockable method calls; in production,
monitor state and arguments for methods under test and mockable method calls; ofﬂine again, generate tests with mocks
for the methods under test

methods on objects of other types. The identiﬁcation of
MUTs forms the basis of the test generation effort, since
the intention of each test generated by RICK is to verify the
behavior of one MUT after isolating it from such external
interactions.

Mockable method calls: We deﬁne a mockable method
call as a method call nested within an MUT, that is made on
a ﬁeld or a parameter object whose type is different from the
declaring type of the MUT. When RICK generates a test for
the MUT, a mockable method call becomes a mock method
call, i.e., the external object is replaced with a mock object,
and the mockable method call occurs on this mock object.

Figure 1 illustrates the identiﬁcation of an MUT (high-
lighted in yellow), together with its mockable method calls
(shown here in green circles). In subsection 3.3 we detail
how a nested method call qualiﬁes as being mockable. As an
example, consider the class ClassUnderTest presented in
Listing 3. RICK identiﬁes the method methodUnderTestOne
(line 5) as an MUT. Moreover,
to
mockableMethodOne on line 8 within methodUnderTestOne
is identiﬁed as a mockable method call because it
is
made on the ﬁeld extField of ClassUnderTest (line
2), which is of an external type. Similarly, the method
methodUnderTestTwo is also considered as an MUT by
RICK, because it has two mockable methods called within
it. The ﬁrst is mockableMethodTwo called 42 times inside a
loop (line 20), and the second one is mockableMethodThree
(line 22). Both of these methods are called on extParam,
which is an external parameter of methodUnderTestTwo.

the nested call

The second phase 2 of RICK occurs when the appli-
cation is deployed and running. During this phase, RICK
monitors the invocation of the MUTs identiﬁed in the
previous phase, and collects data corresponding to these
invocations. Our core motivation for this monitoring is that
the behavior of an application in production reﬂects real
interactions by end-users. Moreover, it may also represent
usage scenarios that are not tested by developer-written
tests [12]. Therefore, tests generated using production data
as inputs are independent of the existing test suite of an
application, yet they can augment it [14]. RICK collects data

for an invocation in production in order to recreate the
same invocation within a generated test. This data includes
the parameters and returned values for each MUT and its
corresponding mockable method calls, as well as the object
on which the MUT is invoked, which we refer to as the
receiving object. Figure 1 depicts the second phase, where the
monitor deﬁned within RICK is attached to both an MUT as
well as its mockable method calls, in order to collect data
about their invocations. subsection 3.3 presents more details
on this monitoring.

Finally, in the third phase 3 , RICK uses the data col-
lected in the second phase as inputs to generate tests with
mocks, as illustrated in Figure 1. These tests are designed
to recreate the invocation of the MUTs, and verify their
behavior as was observed in production, while simulating
the interactions of the MUTs with external objects using
mocks and stubs. They can serve as regression tests, and
can also potentially lead to faster fault localization because
they isolate the invocation of the MUT from the mockable
method calls. For example, Listing 4 presents one test gen-
erated for the MUT methodUnderTestOne. This test veriﬁes
the observed behavior of methodUnderTestOne through the
assertion on line 17, while mocking the ExternalTypeOne
object
the mockable method
mockableMethodOne becomes the mock method, and is
stubbed on line 11 using its parameter and returned value
captured from production. We present more information on
how RICK processes production data to generate tests in
subsection 3.3. The generated tests are the ﬁnal output of
the RICK pipeline. Each generated test falls under one of
three categories, determined by the kind of oracle within it.
We discuss these categories in subsection 3.2.

(line 8). Within the test,

3.2 Mock-based Oracles

The oracle in a unit test speciﬁes a behavior that is expected
as a consequence of running the MUT with a speciﬁc test
input [4]. In the context of the tests generated by RICK,
the oracles in the generated test verify the behavior of the
MUT, while isolating it from method calls to external objects
made within the MUT, i.e., mockable method calls. This

nestedmethodcallmonitormethodunder test(MUT)mockable method call Phase 1: IdentificationPhase 2: Monitoringnestedmethodcallmethodmockable method call mockable method call mockable method call MUT  @Test   void testForMUT() {     ...         mock-based oracle     } mock  method call Phase 3: Generationmock  method call 1231 class ClassUnderTest {
2
3

ExternalTypeOne extField;
...

int methodUnderTestOne(int param) {

...
// mockable method call on field
int x = param + extField.mockableMethodOne(booleanVal);
// nested, non-mockable method call
int y = x + methodFour();
...
return ...;

}

int methodUnderTestTwo(double param,

ExternalTypeTwo extParam) {

...
// mockable method calls on parameter
for (int i = 0; i < 42; i++) {

listOfInts.add(extParam.mockableMethodTwo(floatVals[i]));

}
int z = extParam.mockableMethodThree(intVal);
...
return ...;

}

void methodThree() { ... }

int methodFour() {

// nested, non-mockable method call
methodThree();
...
return ...;

}

5
6
7
8
9
10
11
12
13

15
16
17
18
19
20
21
22
23
24
25

27

29
30
31
32
33
34
35 }

Listing 3: The class, ClassUnderTest, has four methods.
RICK identiﬁes two of these methods, methodUnderTestOne
and methodUnderTestTwo, as MUTs as they have mockable
method calls.

facilitates the decoupling of the MUT from its environment,
and allows the focus of the testing to be on the MUT itself.
Moreover, the behaviour being veriﬁed in the generated
tests, both for the MUT and the mockable method calls,
is sourced from production. This means that through these
generated tests, developers can verify how the system be-
haves for actual users.

There is no systematized knowledge on oracles for tests
with mocks. To overcome this, we now deﬁne three cate-
gories of oracles, all considered by RICK.

Output Oracle, OO: The ﬁrst category of tests generated
by RICK have an output oracle. This oracle veriﬁes that the
output produced by the invocation of the MUT within the
test is equal to the one that was observed in production.
A failure in a test with an output oracle can indicate a
regression in the MUT, which may be caused by its inter-
action with the mockable method call. Listing 4 presents an
example of a generated test with an output oracle. The MUT
is methodUnderTestOne, and the mockable method call is
mockableMethodOne. This test corresponds to one invoca-
tion of methodUnderTestOne observed by RICK in produc-
tion. The test recreates the receiving object, productionObj,
as it was observed in production, by deserializing it (line
5). Next, it mocks the ﬁeld extField and injects it into
productionObj (line 8). This is followed in line 11 by
stubbing mockableMethodOne, to return a value of 27 when
invoked with the parameter true, in accordance with the
observed production behaviour of mockableMethodOne. On
line 14, methodUnderTestOne is invoked on productionObj
with the production parameter 17. Finally, the output or-

5

1 @Test
2 public void testMethodUnderTestOne_OO() {
3 // Arrange
4
5

// Create test fixture from serialized production data
ClassUnderTest productionObj = deserialize(new File(

"receiving1.xml"));

7
8

10
11

// Inject the mock
ExternalTypeOne mockExternalTypeOne =

injectMockField_extField_InClassUnderTest();

// Stub the behavior
when(mockExternalTypeOne.mockableMethodOne(true)).thenReturn(27);

13 // Act
14

int actual = productionObj.methodUnderTestOne(17);

16 // Assert
17
18 }

assertEquals(42, actual);

Listing 4: A RICK test with an Output Oracle, OO, for the
MUT methodUnderTestOne

acle is the assertion on line 17, which veriﬁes that the
output from this invocation of methodUnderTestOne, with
the stubbed call to mockableMethodOne, is 42, which is the
value observed for this invocation in production.

Parameter Oracle, PO: The second category of gen-
erated tests have an oracle to verify that the mockable
method calls occur with speciﬁc parameter(s), the same
as production, within the invocation of the MUT. A test
with a parameter oracle may fail due to regressions in
the MUT which cause an expected mockable method call
to be made with unexpected parameters. An example
of this oracle is presented in Listing 5. This test recre-
ates the receiving object productionObj, for the MUT
methodUnderTestTwo (line 5). Next, it prepares a mock
object for ExternalTypeTwo called mockExternalTypeTwo
(line 8), and stubs the 42 invocations of the mockable
method call, mockableMethodTwo. For brevity, we include
only two of these stubs on lines 11 and 12. The single in-
vocation of mockableMethodThree (line 14) is also stubbed,
with the parameter and returned value observed in produc-
tion. This is followed by a call to methodUnderTestTwo on
productionObj (line 17), passing it mockExternalTypeTwo
as parameter. Finally, the statements on lines 20 to 23
are unique to this category of tests, and serve as the pa-
rameter oracle. The statements on lines 21 and 22 ver-
ify that mockableMethodTwo is called at least once on
mockExternalTypeTwo with the concrete production pa-
rameter 4.2F, as well as with 9.8F. We omit the veriﬁca-
tion of the other 40 invocations of mockExternalTypeTwo
from this code snippet. Next, the parameter oracle ver-
iﬁes on line 23 that mockableMethodThree is called at
least once with the parameter 15, within this invocation of
methodUnderTestTwo.

Call Oracle, CO: Oracles in the generated tests for
the third category verify the sequence and frequency of
mockable method calls within the invocation of the MUT.
Any deviation from the expected sequence and frequency
of mockable method calls within an MUT will cause a test
with a call oracle to fail. This can be helpful for developers
when localizing a regression related to object protocols
within the MUT. An example of this oracle is presented
in Listing 6. This test ﬁrst recreates the receiving object,
productionObj, for the MUT methodUnderTestTwo (line

1 @Test
2 public void testMethodUnderTestTwo_PO() {
3 // Arrange
4
5

// Create test fixture from serialized production data
ClassUnderTest productionObj = deserialize(new File(

"receiving2.xml"));

7
8

10
11
12
13
14

// Create the mock
ExternalTypeTwo mockExternalTypeTwo = mock(

ExternalTypeTwo.class);

// Stub the behavior
when(mockExternalTypeTwo.mockableMethodTwo(4.2F)).thenReturn(89);
when(mockExternalTypeTwo.mockableMethodTwo(9.8F)).thenReturn(92);
...
when(mockExternalTypeTwo.mockableMethodThree(15)).thenReturn(48);

16 // Act
17

productionObj.methodUnderTestTwo(6.2, mockExternalTypeTwo);

19 // Assert
20

verify(mockExternalTypeTwo, atLeastOnce())

.mockableMethodTwo(4.2F);

verify(mockExternalTypeTwo, atLeastOnce())

.mockableMethodTwo(9.8F);

...
verify(mockExternalTypeTwo, atLeastOnce())

.mockableMethodThree(15);

21

22
23

24 }

Listing 5: A RICK test with a Parameter Oracle, PO, for the
MUT methodUnderTestTwo

1 @Test
2 public void testMethodUnderTestTwo_CO() {
3 // Arrange
4
5

// Create test fixture from serialized production data
ClassUnderTest productionObj = deserialize(new File(

"receiving2.xml"));

7
8

10
11
12
13
14

// Create the mock
ExternalTypeTwo mockExternalTypeTwo = mock(

ExternalTypeTwo.class);

// Stub the behavior
when(mockExternalTypeTwo.mockableMethodTwo(4.2F)).thenReturn(89);
when(mockExternalTypeTwo.mockableMethodTwo(9.8F)).thenReturn(92);
...
when(mockExternalTypeTwo.mockableMethodThree(15)).thenReturn(48);

16 // Act
17

productionObj.methodUnderTestTwo(6.2, mockExternalTypeTwo);

19 // Assert
20
21

InOrder orderVerifier = inOrder(mockExternalTypeTwo);
orderVerifier.verify(mockExternalTypeTwo, times(42))

.mockableMethodTwo(anyFloat());

22

orderVerifier.verify(mockExternalTypeTwo, times(1))

.mockableMethodTwo(anyInt());

24 }

Listing 6: A RICK test with a Call Oracle, CO, for the MUT
methodUnderTestTwo

5), stubs the mockable method calls to mockableMethodTwo
and mockableMethodThree (lines 11 to 14), and invokes
methodUnderTestTwo with the mocked parameter (line 17).
Next, the call oracle in this test veriﬁes the number of times
the mockable method calls occur within this invocation of
methodUnderTestTwo, as well as the order in which these
calls occur. This is achieved with the order veriﬁer deﬁned
on line 20. The statements on lines 21 and 22 verify that
mockableMethodTwo is invoked exactly 42 times with a ﬂoat
parameter, and that these invocations are followed by one
call to mockableMethodThree with an integer parameter, as
was observed in production.

6

3.3 Key Phases

As outlined in subsection 3.1, RICK operates in three phases.
We now discuss these three phases in more detail.

3.3.1 Identiﬁcation of Test Generation Targets
It is not possible to generate test cases with mocks for all
methods with nested method calls. For example, a static
method invoked within another method is typically not
mocked [3]. Also, it is not feasible to replace an object
created within the body of a method, and subsequently
mock the interactions made with it. Therefore, RICK deﬁnes
a set of rules to determine the methods that can be valid
targets for the generation of test cases and mocks. Develop-
ers can provide an initial set of methods of interest for test
generation, which RICK can check. Otherwise, by default,
RICK can analyze the whole application to determine the set
of valid candidates.
3.3.1.1

Identifying MUTs: First, RICK ﬁnds methods
that are part of the API of the application, i.e., methods that
are public, non-abstract, non-deprecated, and non-empty
[22], [23]. These criteria have also been used previously to
generate differential unit tests for open-source Java projects
[14]. Of these methods, RICK selects as MUTs the ones that
invoke other methods on objects of external types.

3.3.1.2

Identifying Mockable Method Calls: Second,
RICK identiﬁes the nested method calls within each of the
selected MUTs. An MUT may have several nested method
calls, not all of which are suitable for mocking. For it to
be mocked, a nested method call must be invoked on a
parameter or a ﬁeld, such that a mock can be injected to
substitute it in the generated test. Also, the declaring type
of this parameter or ﬁeld should be different from the type
of the MUT, in order to represent an interaction of the MUT
with an external type. If a nested method call fulﬁlls all
these criteria, RICK identiﬁes it as a mockable method call. The
generated test then has the clear test intention of verifying
the behavior of the MUT, while stubbing the call to the
nested method.

We illustrate the rules for target selection with the help
of the excerpt of the class ClassUnderTest in Listing 3. This
excerpt includes a ﬁeld as well as four methods deﬁned in
ClassUnderTest. The ﬁrst method, methodUnderTestOne
(lines 5 to 13), accepts an integer parameter, and returns
an integer value. The body of methodUnderTestOne in-
cludes a call to the method mockableMethodOne(boolean),
on the ﬁeld extField (line 8). This ﬁeld is declared
as being of type ExternalTypeOne in ClassUnderTest
(line 2). There is another call on line 10 to a method
deﬁned in ClassUnderTest called methodFour. The sec-
ond method, methodUnderTestTwo (lines 15 to 25), re-
turns an integer value, and accepts two parameters. The
ﬁrst parameter is a double, and the second parame-
ter called extParam is of type ExternalTypeTwo. Within
the loop on lines 19 to 21, methodUnderTestTwo calls
the method mockableMethodTwo(float) on the parameter
extParam (line 20). There is another call on extParam to
mockableMethodThree(int) (line 22). The third method
deﬁned in ClassUnderTest is a private method called
methodThree (line 27). It does not call any other method.
Finally, the fourth method in this excerpt is methodFour
(lines 29 to 34), which has a call to methodThree (line 31).

7

We systematically consider special cases. First, a mock-
able method may be invoked from different MUTs. Also,
an MUT may itself be a mockable method for another
MUT. Moreover, an MUT may be invoked without its cor-
responding mockable method call(s), if the latter is invoked
within a branch, for example. It is therefore important to
ensure that the data collected for a mockable method call
is associated with a speciﬁc invocation of an MUT. RICK
implements this association by assigning a unique identiﬁer
to each MUT invocation, and the same identiﬁer to each
mockable method call within it. This information is required
for the generation of all of the three kinds of oracles, i.e., the
output oracle, the parameter oracle, as well as the call oracle.
Second, one invocation of an MUT may have multiple
mockable method calls, which may or may not have the
same signature. Furthermore, these invocations occur in a
speciﬁc order within the MUT. In order to account for this,
RICK collects the timestamps for each mockable method
call. This is done to synthesize statements corresponding
to the call oracle, which verify the sequence and frequency
of mockable method calls in the generated tests.

3.3.3 Generation of Tests with Mocks
Once RICK has collected data about invocations of MUTs
and corresponding mockable method calls, the ﬁnal phase
can begin, triggered by the developer. RICK connects an
MUT invocation with mockable method calls by utilizing
the unique identiﬁers assigned to each invocation observed
in production. It then generates code to produce the three
categories of tests for each invocation, as detailed in sub-
section 3.2. The ﬁnal output from the test generation phase
is a set of test classes, which include tests from the three
categories, for each invocation of an MUT that was observed
by RICK in production.

RICK generates tests by bringing together all the data
it has observed, collected, and linked to the respective
invocation of an MUT and its mockable method calls. Within
each test generated by RICK for an MUT:

• The serialized receiving object and parameter(s) for the
MUT are deserialized to recreate their production state.
For example, the receiving object of the respective MUT
is recreated from its serialized XML state on line 5 of
Listing 4, Listing 5, and Listing 6.

• External objects, on which mockable method calls oc-
cur, are substituted with mock objects. For example, a
mock object substitutes the external ﬁeld extField on
line 8 of Listing 4. A mock object is prepared for the
parameter ExternalTypeTwo on line 8 of Listing 5 and
Listing 6.

• Mockable method calls become mock method calls:
stubbed, with production parameter(s)
they are
and returned value. The mock method call within
methodUnderTestOne is stubbed on line 11 of Listing 4.
The mock method calls within methodUnderTestTwo
are stubbed from lines 11 to 14 in Listing 5 and List-
ing 6.

• The oracle veriﬁes a unique aspect about the invocation
of the MUT and its interactions with the external ob-
ject(s): the OO on line 17 of Listing 4 veriﬁes the output
of methodUnderTestOne, the PO on lines 20 to 23 of
Listing 5 verify that the mock method calls occur with

the invocation of

Fig. 2: Monitoring method invocations in production:
RICK observes
the target method,
methodUnderTestTwo, and captures the receiving object
and parameters for this invocation, as well as the object
returned from it. RICK also observes the method calls to
mockableMethodTwo and mockableMethodThree, collecting
their parameters and returned values.

As a consequence of the aforementioned criteria, RICK
identiﬁes methodUnderTestOne and methodUnderTestTwo
calls,
the
as MUTs. Moreover,
nested method
and
mockableMethodOne,
and
mockableMethodTwo
mockableMethodThree,
in these MUTs respectively, are
recognized as mockable method calls by RICK. However,
the call to methodFour within methodUnderTestOne is not
mockable within methodUnderTestOne, and methodThree
and methodFour are not MUTs since they do not fulﬁll
these criteria.

We want to ensure that tests are generated for non-
trivial MUTs and mockable method calls. To achieve this,
RICK selects MUTs and mockable methods that have at least
a certain number of lines of code (LOC). A set of MUTs,
and the corresponding set of mockable method calls is the
outcome of the ﬁrst phase of RICK.

3.3.2 Monitoring Test Generation Targets

Once it ﬁnds a set of MUTs and their corresponding mock-
able method calls, RICK instruments them in order to mon-
itor their execution as the application runs in production.
The goal of this instrumentation is to collect data as the
application executes. RICK collects data about each invoca-
tion of an MUT: the receiving object, which is the object on
which it is invoked, the parameters passed to the invocation,
as well as the object returned from the invocation. At the
same time, RICK collects data about the mockable method
calls within this MUT. This includes the parameters and
the returned value for each mockable method call. The
data collected from this monitoring is serialized and saved
to disk. For example, the sequence diagram in Figure 2
illustrates the monitoring of the MUT methodUnderTestTwo
in ClassUnderTest presented in Listing 3. For one invo-
cation of methodUnderTestTwo, RICK collects its receiving
object, parameters, and returned value, as well as the pa-
rameters and returned values corresponding to the invoca-
tions of mockable method calls to mockableMethodTwo and
mockableMethodThree within methodUnderTestTwo.

productionObj: ClassUnderTestproductionObj2:ExternalTypeTwomockableMethodTwo(  )R...P2V1V2V3mockableMethodTwo(  )P3mockableMethodThree(  )P4V4RPVReceiving objectParametersreturned ValueObjects collected:methodUnderTestTwo(  )P1the same parameters as they did in production, and
the CO on lines 21 and 22 of Listing 6 verify that the
mock method calls happen in the same sequence and
the same frequency as they did in production.

3.4 Implementation

RICK is implemented in Java. MUTs and their corresponding
mockable method calls are identiﬁed through static analysis
with Spoon [24]. Once identiﬁed, they are instrumented
and monitored in production with Glowroot, an open-
source Application Production Monitoring agent3. Data col-
lection from production is handled through serialization by
XStream4. RICK relies on the code generation capabilities of
Spoon5 to produce JUnit tests6. These tests deﬁne and use
Mockito7 mocks. RICK uses some capabilities provided by
the PANKTI framework [14].

4 EXPERIMENTAL METHODOLOGY

This section introduces our experimental methodology. We
ﬁrst describe the open-source projects we use as case studies
to evaluate mock generation with RICK. Then, we describe
the production conditions that we use to collect data for test
generation. Next, we present the research questions we aim
to answer through this evaluation, and deﬁne the protocol
that we use to do so.

4.1 Case Studies

As detailed in section 3, RICK uses data collected from an
application in production, in order to generate tests with
mock-based oracles. We therefore target applications that we
can build, deploy, and for which we can deﬁne a production-
grade usage scenario. We manually search for three notable,
open-source Java projects that satisfy these criteria. Table 1
summarizes the details of the projects we use as case studies
to evaluate the capabilities of RICK. For each case study, we
provide the exact version number as well as the SHA of
the commit used for our experiments. This information will
facilitate further replication. We also provide the number of
lines of code and the number of commits as indicators of the
importance of the project, as well as the number of stars, as
an indicator of its visibility. The last row in Table 1 indicates
the number of candidate MUTs for each case study, which
is the set of MUTs with mockable method calls identiﬁed by
RICK.

Our ﬁrst case study is the web-based routing appli-
cation based on OpenStreetMap called GraphHopper8. It
allows users to ﬁnd the route between locations on a map,
considering diverse means of transport and other routing
information such as elevation. We use version 5.3 of Graph-
Hopper, with 89K lines of code (LOC) and 5, 844 commits.
The GraphHopper project repository on GitHub has 3.7K
stars.

3. https://glowroot.org/
4. https://x-stream.github.io/
5. https://spoon.gforge.inria.fr/
6. https://junit.org/
7. https://site.mockito.org/
8. https://www.graphhopper.com/open-source/

TABLE 1: Case studies for the evaluation of RICK

8

METRIC

VERSION

SHA

STARS

COMMITS

LOC

CANDIDATE_MUTS

GraphHopper

Gephi

PDFBox

5.3

0.9.6

2.0.24

af5ac0b

ea3b28f

8876e8e

3.7K

5.8K

89K

356

4.9K

6.5K

35K

115

1.7K

8.7K

165K

319

The second case study is Gephi, an application to work
with graph data9. With 4.9K stargazers on GitHub, Gephi is
very popular, and has been adopted by both the industry
and by researchers [25]. It allows users to import graph
ﬁles, manipulate them, and export them in different ﬁle
formats. We use version 0.9.6 of Gephi, which includes
6, 548 commits, and 126K LOC. For our evaluation, we
exclude the GUI modules, as the generation of GUI tests has
its own challenges [26] that are outside the scope of RICK.
The 8 modules we consider are implemented in 35K LOC.

The last case study is PDFBox, a PDF manipula-
tion command-line tool developed and maintained by the
Apache Software Foundation10. Some of the functionalities
offered by PDFBox include the extraction of text and images
from PDF documents, conversion between text ﬁles and
PDF documents, encryption and decryption, and splitting
and merging of PDF documents. As highlighted in Table 1,
we use version 2.0.24 of PDFBox, which has 165K LOC,
8, 797 commits, and has been starred by 1.7K GitHub users.
To generate tests with RICK, the ﬁrst step consists in
identifying candidate MUTs, which RICK will instrument
so their invocations can be monitored as the project exe-
cutes in production. According to the criteria introduced in
subsection 3.3, RICK identiﬁes and instruments a total of
790 CANDIDATE_MUTs across the three applications: 356
in GraphHopper, 115 in Gephi, and 319 in PDFBox. These
methods have interactions with objects of external types,
where these objects are either the parameters of the MUT, or
a ﬁeld deﬁned within the declaring type of the MUT.

4.2 Production Usage

Once the candidate MUTs for an application are identiﬁed,
the instrumented application is deployed and run under a
certain workload. At runtime, RICK collects data, which it
will use for test and mock generation. Table 2 summarizes
the key characteristics of the workloads that represent typ-
ical usage scenarios for the three applications. Rows 2 and
3 capture the scope for test generation that we consider for
our experiments. The number of candidate MUTs actually
invoked in production is indicated by MUT_INVOKED,
while MOCKABLE_INVOKED represents the number of
distinct mockable methods invoked within these invoked
MUTs. We also report the number of times the MUTs

9. https://gephi.org/
10. https://pdfbox.apache.org/

TABLE 2: Characteristics of the workloads for the case
studies in production

METRIC

GraphHopper

Gephi

PDFBox

MUT_

INVOKED

MOCKABLE_

INVOKED

MUT_

INVOCATIONS

MOCKABLE_

INVOCATIONS

72

81

68

63

72

55

73,025

21,548

7,429,800

246,822

202,630

5,144,790

Fig. 3: Snapshot of GraphHopper in production. We query
for the route between 4 locations in Sweden, as RICK moni-
tors method invocations.

and their mockable methods are invoked in the rows
MUT_INVOCATIONS and MOCKABLE_INVOCATIONS,
respectively. The numbers of invocations of MUTs and
mockable methods demonstrate the relevance and compre-
hensiveness of the production scenarios we design. They
represent the extent to which we exercise the three ap-
plications in production, and reﬂect actual usage of their
functionalities.

GraphHopper: To experiment with GraphHopper, we de-
ploy its server and use its website to search for the car and
bike route between four points in Sweden, speciﬁcally, from
the residence of each author to their common workplace
in Stockholm11. Figure 3 is a snapshot of this experiment.
Recall from subsection 4.1 that RICK monitors the invocation
of 356 CANDIDATE_MUTs as GraphHopper executes. As
presented in Table 2, 72 of the candidate MUTs are invoked
(MUT_INVOKED), which become test generation targets for
RICK. Within these MUTs, 81 distinct mockable methods
are also called. With this production scenario, the 72 MUTs
are invoked a total of 73, 025 times, while the 81 mockable
methods are invoked 246, 822 times within the MUTs.

Gephi: As production usage for Gephi, we deploy the
application and import a graph data ﬁle. This ﬁle has details
about the top 999 Java artifacts published on Maven Central,

11. https://bit.ly/3LG2zSQ

9

Fig. 4: We use Gephi to import a data ﬁle with details
on 999 Java artifacts on Maven Central. We interact with
the features of Gephi to manipulate the resulting graph.
RICK monitors method invocations corresponding to these
interactions in production.

as well as the dependencies between them. We retrieve
this data ﬁle from previous work [27], [28]. We use Gephi
to produce a graph from this data, and to manipulate its
layout, as illustrated in Figure 4. Finally, we export the re-
sulting graph in PDF, PNG, and SVG formats, before exiting
the application. These interactions with Gephi lead to the
invocation of 68 of the 115 candidate MUTs, and 63 distinct
mockable methods called by these MUTs. Moreover, these
MUTs are invoked 21, 548 times, while there are 202, 630
mockable method calls.

PDFBox: We use the command-line utilities provided by
PDFBox to perform 10 typical PDF manipulation operations
on 5 PDF documents. These documents are sourced from
[29], and the operations performed on them include text and
image extraction, conversion into a text ﬁle, and vice-versa,
etc. This methodology has been adopted from previous
work [14]. Of the 319 candidate MUTs, this workload leads
to the invocation of 72 MUTs and 55 different mockable
methods. The MUTs are called over 7 million times, and
the mockable methods are called over 5 million times. The
magnitude of these invocation counts is due to the process-
ing of a myriad of media content from the real-world PDF
documents we select.

4.3 Research Questions

For each application described in subsection 4.1 and exer-
cised in production per the workload speciﬁed in subsec-
tion 4.2, RICK generates tests with mocks using the captured
production data. Through these experiments, we aim to
answer the following research questions.

• RQ1 [methods under test]: To what extent can RICK

generate tests for MUTs invoked in production?

• RQ2 [production-based mocks]: How rich is the pro-
duction context reﬂected in the tests, mocks, and oracles
generated by RICK?

• RQ3 [mimicking production]: To what extent can the
execution of generated tests and mocks mimic realistic
production behavior?

• RQ4 [quality]: What is the opinion of developers about

the tests generated by RICK?
Each of the research questions presented above high-
lights a unique aspect of the capabilities of RICK, with

respect to automated generation of tests with mocks, using
data sourced from production executions.

4.4 Protocols for Evaluation

The MUTs instrumented by RICK are invoked thousands of
times (cf Table 2). For experimental purposes and to allow
for a thorough, qualitative analysis of the results, we collect
data about the ﬁrst invocation of the MUT in production
and use it to generate tests with RICK. We analyze these
generated tests according to the following protocols in order
to answer the research questions presented in subsection 4.3.
Protocol for RQ1: This ﬁrst research question aims at
characterizing the target MUTs for which RICK transforms
observations made in production into concrete test cases.
We describe the MUTs by reporting their number of lines
of code (LOC) as well as the number of parameters. We also
report the number of tests generated by RICK for these target
MUTs. This includes the tests with the three kind of mock-
based oracles, OO for MUTs that return primitive values,
PO, and CO, for one invocation of the MUT observed in
production.

Protocol for RQ2: With RQ2, we analyse the ability of
RICK to capture rich production contexts and turn them into
test inputs and oracles that verify distinct aspects of their
behavior. We answer RQ2 by dissecting the data captured
by RICK as the three applications execute, as well as the tests
generated by RICK using this data. First, to characterize the
receiving object and parameters captured for the MUTs from
production, we discuss the size of the serialized production
state on disk. Second, we analyse the three kinds of oracles
in the generated tests, speciﬁcally the assertion statement
in the OO tests, and the number of veriﬁcation statements
in PO and CO tests. Furthermore, we report the number of
external objects (ﬁelds and/or parameters) mocked within
the tests, the stubs produced by RICK based on production
observations, as well as the mock method calls.

Protocol for RQ3: With RQ3 we highlight the feasibility
and complexity of automatically generating tests and mocks
that successfully execute in order to mimic actual produc-
tion behavior, in an isolated manner. In order to answer
RQ3, we execute the generated tests, and we analyse the
outcome. There are three possible outcomes of the execution
of a generated test. First, a test is successfully executed if
the oracles pass, implying that the test mimics the behavior
of both the MUT and the mock method calls(s) observed
by RICK in production. Second, a generated oracle may
fail, meaning that the test and its mocks do not replicate
the production observations. For example, objects recreated
within the generated test through deserialization may not
be identical to those observed in production [14]. Third,
during the execution of the test, a runtime exception may
happen before the oracles are evaluated, suggesting that the
interactions between the MUT and the mocked object are
too complex to be handled in an automated way.

Protocol for RQ4: RQ4 is a qualitative assessment of
the tests generated by RICK. It serves as a proxy for the
readiness of the RICK tests to be integrated into the test
suite of projects. To assess the quality of the generated
tests, we conduct a developer survey, presenting a set of
6 tests generated by RICK for GraphHopper, to 5 software

10

testers from industry. Developer surveys have previously
been conducted to assess mocking practices [3], [11]. The
key novelty of our survey consists in assessing mocks that
have been automatically generated.

We conduct each survey online, and follow this system-
atic structure: introduce mocking, the RICK test generation
pipeline, and the GraphHopper case study; next, we give
the participant access to a fork of GraphHopper on GitHub,
with the RICK tests added, inviting the participant to clone
this repository, or browse through it online; ﬁnally, we ask
them questions about the generated tests. The goal of this
survey is to gauge the opinion of developers about the
quality of the 6 tests with respect to three criteria: mocking
effectiveness, structure, and understandability. Our replica-
tion package12 includes all the details about this survey.

5 EXPERIMENTAL RESULTS
This section presents the results from our evaluation of RICK
with GraphHopper, Gephi, and PDFBox. In subsection 5.1,
subsection 5.2 and subsection 5.3, we answer RQ1, RQ2 and
RQ3 based on the metrics summarized in Table 3, Table 4,
and Table 5. In subsection 5.4 we answer RQ4 based on the
surveys conducted with testers from industry.

5.1 Results for RQ1 [Methods Under Test]

As presented in the ﬁrst four columns of Table 3, Table 4, and
Table 5, RICK generates tests for 23 MUTs in GraphHopper,
57 in Gephi, and 48 in PDFBox. In total, RICK generates tests
for 128 MUTs which have at least one mockable method call.
The median number of LOCs in these 128 target methods is
18, while the largest method is MUT#26 in PDFBox with
328 lines of code. The median number of parameters for
the MUTs is 1, while several MUTs (such as MUT#38 and
MUT#39 in Gephi) take as many as 6 parameters. In general,
RICK handles a wide variety of MUTs in the case studies,
with successful identiﬁcation and instrumentation of these
methods, detailed monitoring in production, as well as the
generation of tests that compile and run.

These results validate that mock generation from pro-
duction can indeed be fully automated, and is robust with
respect to the complexity of real world methods. RICK han-
dles the diversity of methods, data types, and interactions
observed in real software and production usage scenarios.

In Table 2, we see that the workloads trigger the execu-
tion of 72 MUTs in GraphHopper, 68 in Gephi, and 72 in
PDFBox. A subset of them are actually used as targets for
test generation: 23, 57 and 48 MUTs in the respective case
studies. This happens due to two reasons.

First, when statically identifying targets for test genera-
tion, RICK ﬁnds MUTs with mockable method calls. How-
ever, as highlighted in subsubsection 3.3.2, MUTs may be
invoked, without their corresponding mockable methods
being called. For example, a mockable method call may
happen only within a certain path through the MUT, which
is not observed in production. In this case, a test with mocks
is not generated. Second, the receiving objects for some
MUTs are sometimes too large and complex to be captured
through serialization, which is the case for some MUTs

12. https://github.com/castor-software/rick-experiments

TABLE 3: Experimental results for GraphHopper

11

RQ1: Method under test

RQ2: Production-based mocks

RQ3: Mimicking production

MUT_ID #LOC

#PARAMS

#TESTS

CAPTURED_

#MOCK_

#MOCK_

#STUBS

#OO_

#PO_

#CO_

#SUCCESSFULLY_

#INCOMPLETELY_

#UNHANDLED_

OBJ_SIZE

OBJECTS

METHODS

STMNTS

STMNTS

STMNTS

MIMIC

MIMIC

MUT_BEHAVIOR

MUT # 1

MUT # 2

MUT # 3

MUT # 4

MUT # 5

MUT # 6

MUT # 7

MUT # 8

MUT # 9

MUT # 10

MUT # 11

MUT # 12

MUT # 13

MUT # 14

MUT # 15

MUT # 16

MUT # 17

MUT # 18

MUT # 19

MUT # 20

MUT # 21

MUT # 22

MUT # 23

7

19

8

5

15

13

7

15

25

15

26

5

5

25

5

58

18

5

74

8

57

37

4

1

1

1

1

1

0

1

2

0

0

0

1

1

0

1

1

1

1

1

0

0

2

0

2

3

2

2

3

3

2

3

3

3

3

3

3

3

3

3

3

3

2

2

3

3

2

41 B

47 B

53 B

284 B

477 B

982 B

1.1 KB

1.5 KB

2.7 KB

2.7 KB

3 KB

3.1 KB

3.1 KB

56.7 KB

156.8 KB

156.8 KB

156.9 KB

383 KB

590 KB

729.2 KB

2.2 MB

9.8 MB

9.8 MB

1

1

1

1

1

1

0

1

2

2

2

1

1

2

1

1

1

1

2

1

3

1

2

1

3

2

1

1

3

1

2

5

3

2

1

1

4

1

1

1

1

2

2

3

3

1

TOTAL: 23

MEDIAN:

MEDIAN:

62

15

1

MEDIAN:

3.1 KB

31

45

0

6

0

1

0

2

1

4

9

11

11

1

1

5

1

1

1

1

0

20

2

3

0

81

0

1

0

0

1

1

1

1

1

1

1

1

1

1

1

1

1

1

0

0

1

1

0

16

1

6

2

1

1

3

1

4

10

7

11

1

1

5

1

1

1

1

2

20

3

4

1

88

1

9

3

1

1

3

1

2

8

6

2

1

1

5

1

1

1

1

2

20

3

4

1

78

0

0

2

2

3

3

2

3

0

0

0

3

3

0

3

3

3

3

2

0

0

2

0

0

3

0

0

0

0

0

0

3

3

3

0

0

3

0

0

0

0

0

2

0

1

2

37

20

2

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

3

0

0

5

invoked within GraphHopper. We have observed serialized
object snapshots beyond tens of megabytes, which reaches
the scalability limits of the state of the art serialization
techniques, even with one of the best Java serialization
libraries.

The difference between the number of invoked MUTs
and the number of MUTs for which RICK generates tests
is most signiﬁcant for GraphHopper. We notice that many
receiving objects for GraphHopper do not get successfully
serialized owing to their large size. For example, the receiv-
ing object for an invoked MUT was as large as 454 MB,
before even being serialized as XML. To counter this, we
allocated more heap space, increasing it up to 9 GB, while
deploying the server, yet were unsuccessful in serializing it.
Our research on using serialization for automated mocking
clearly touches the frontier of serialization for handling
arbitrarily large and complex data from production.

The total number of tests generated by RICK for the 128
target MUTs is 294. Recall from subsection 4.2 that we only
monitor and generate tests for a single invocation – the ﬁrst
one – of each of the target MUTs. Note also that, as signiﬁed
by the column #TESTS in Table 3, Table 4, and Table 5,
RICK generates either 2 or 3 tests for each target MUT. The
number of tests generated for an MUT depends on its return
type. For MUTs that return a non-void, primitive value, such
as MUT#2 in GraphHopper, RICK generates an OO test to
assert on the output of the MUT invocation, in addition
to a PO and CO test. Across the three case studies, RICK
generates OO tests for 38 MUTs, and PO and CO tests for
all 128 MUTs.

Answer to RQ1

RICK captures the production behavior for a set of 128 out
of the 212 MUTs invoked in production. RICK transforms
the data collected from these production invocations into
294 concrete tests with different oracles. The key result
of RQ1 is that RICK handles a large variety of real world
methods in an end to end manner, from data collection in
production to the generation of tests with mocks.

5.2 Results for RQ2 [Production-based Mocks]

RICK generates mocks from real data observed in produc-
tion. While RQ1 has demonstrated feasibility, RQ2 explores
how the complexity of production is reﬂected in test cases.
Columns 5 through 11 in Table 3, Table 4, and Table 5
present the results for RQ2. Each row highlights the data
that RICK collects for one MUT and its mock method
call(s) from production. The column CAPTURED_OBJ_SIZE
presents the size on disk (in B / KB / MB) of the serialized
object states for the receiving object and, if present, the pa-
rameter objects of the MUT. We also present the number of
external objects mocked within the test (#MOCK_OBJECTS),
the number of methods called on these mock objects
(#MOCK_METHODS), the number of stubs (#STUBS) de-
ﬁned for these mock methods, and the number of statements
corresponding to the OO, PO, and CO oracles in the gener-
ated tests (signiﬁed through #OO_STMNTS, #PO_STMNTS,
and #CO_STMNTS, respectively).

We ﬁrst discuss the serialized state of the receiving object
on which the MUT is invoked, as well as the parameters
with which this invocation is made. This data characterizes
the complexity of the objects that RICK captures for the
128 MUT, as they are invoked during the execution of
the applications. Recall that within the generated test, the

12

TABLE 4: Experimental results for Gephi

RQ1: Method under test

RQ2: Production-based mocks

RQ3: Mimicking production

MUT_ID #LOC

#PARAMS

#TESTS

CAPTURED_

#MOCK_

#MOCK_

#STUBS

#OO_

#PO_

#CO_

#SUCCESSFULLY_

#INCOMPLETELY_

#UNHANDLED_

OBJ_SIZE

OBJECTS

METHODS

STMNTS

STMNTS

STMNTS

MIMIC

MIMIC

MUT_BEHAVIOR

MUT # 1

MUT # 2

MUT # 3

MUT # 4

MUT # 5

MUT # 6

MUT # 7

MUT # 8

MUT # 9

MUT # 10

MUT # 11

MUT # 12

MUT # 13

MUT # 14

MUT # 15

MUT # 16

MUT # 17

MUT # 18

MUT # 19

MUT # 20

MUT # 21

MUT # 22

MUT # 23

MUT # 24

MUT # 25

MUT # 26

MUT # 27

MUT # 28

MUT # 29

MUT # 30

MUT # 31

MUT # 32

MUT # 33

MUT # 34

MUT # 35

MUT # 36

MUT # 37

MUT # 38

MUT # 39

MUT # 40

MUT # 41

MUT # 42

MUT # 43

16

14

9

6

16

31

12

12

11

8

18

13

14

12

17

24

43

18

7

29

11

40

50

23

13

4

23

53

21

85

85

25

10

90

15

28

40

18

16

5

62

15

26

MUT # 44

185

MUT # 45

11

MUT # 46

123

MUT # 47

MUT # 48

MUT # 49

MUT # 50

59

45

20

18

MUT # 51

126

MUT # 52

MUT # 53

MUT # 54

MUT # 55

MUT # 56

MUT # 57

24

24

15

11

7

32

1

1

3

1

1

3

2

2

2

1

3

1

1

1

1

1

3

1

1

1

1

3

2

3

1

1

3

1

2

4

1

1

1

0

3

2

2

6

6

2

6

6

0

0

0

0

5

6

0

2

0

0

0

2

2

3

3

2

2

2

3

3

3

2

2

3

2

2

2

2

2

2

2

3

2

2

2

3

2

2

2

2

3

2

3

2

2

2

2

3

3

2

2

2

2

3

2

2

2

2

2

2

2

2

2

2

3

2

2

2

2

2

2

2

48 B

48 B

56 B

56 B

56 B

56 B

118 B

134 B

143 B

186 B

206 B

237 B

238 B

255 B

291 B

299 B

307 B

340 B

344 B

425 B

472 B

1.7 KB

2.2 KB

4.4 KB

58.2 KB

58.2 KB

59.2 KB

60.1 KB

79 KB

100 KB

112.5 KB

112.5 KB

149.5 KB

177.4 KB

196.3 KB

276 KB

300 KB

370 KB

600 KB

3.6 MB

3.6 MB

3.8 MB

3.9 MB

3.9 MB

3.9 MB

3.9 MB

4 MB

4.1 MB

4.2 MB

4.5 MB

10.8 MB

10.8 MB

10.8 MB

11.2 MB

11.6 MB

11.6 MB

11.6 MB

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

3

1

1

3

1

1

1

1

1

4

1

1

1

1

1

1

1

1

1

2

1

1

1

1

2

1

2

1

1

1

1

1

1

1

1

1

2

1

1

1

1

1

4

2

2

1

4

2

1

1

1

1

1

1

1

1

2

3

2

1

3

1

1

4

1

1

4

1

1

1

3

1

1

1

1

1

8

1

1

1

1

3

1

2

1

1

1

1

1

1

2

1

2

2

1

1

1

1

0

2

0

2

0

4

1

1

1

1

1

1

1

1

1

2

1

1

1

3

1

1

10

1

1

5

1

1

1

0

1

1

1

1

1

8

1

6

6

0

1

3

3

0

0

6

1

1

1

4

1

3

0

0

0

1

1

1

0

0

1

0

0

0

0

0

0

0

1

0

0

0

1

0

0

0

0

1

0

1

0

0

0

0

1

1

0

0

0

0

1

0

0

0

0

0

0

0

0

0

0

1

0

0

0

0

0

0

0

2

1

1

1

1

1

4

2

2

1

4

2

1

1

1

1

1

1

1

1

2

3

2

1

3

1

1

4

1

1

4

1

1

1

3

1

1

1

1

1

8

1

1

1

1

3

1

2

1

1

1

1

1

1

2

1

3

2

1

1

1

1

1

4

2

2

1

20

2

1

1

1

1

1

1

1

1

2

3

2

1

3

1

1

27

1

1

6

1

1

1

3

1

1

1

1

1

8

1

1

1

1

3

1

2

1

1

1

1

1

1

2

1

3

0

0

2

0

3

3

2

1

3

0

1

0

0

0

0

0

0

0

2

0

3

0

0

2

0

3

2

0

2

0

0

2

2

0

2

0

0

0

0

0

0

0

0

0

2

0

0

0

2

0

0

0

0

2

1

2

0

0

0

0

0

0

0

0

1

0

0

1

0

0

0

0

0

0

0

0

0

0

2

2

0

2

0

0

3

0

0

0

0

1

3

0

0

0

0

0

0

0

0

2

0

0

2

0

0

0

0

2

2

2

0

1

0

0

2

2

0

3

0

0

0

0

0

2

0

2

2

2

2

2

3

2

0

2

0

0

0

0

0

0

0

0

0

2

2

0

0

0

0

2

2

2

3

2

2

2

0

2

0

0

2

2

0

3

0

0

0

0

0

0

2

TOTAL: 57

MEDIAN:

MEDIAN:

18

1

126

MEDIAN:

60.1 KB

67

93

103

12

94

135

44

26

56

TABLE 5: Experimental results for PDFBox

13

RQ1: Method under test

RQ2: Production-based mocks

RQ3: Mimicking production

MUT_ID #LOC

#PARAMS

#TESTS

CAPTURED_

MOCK_

MOCK_

#STUBS

#OO_

#PO_

#CO_

#SUCCESSFULLY_

#INCOMPLETELY_

#UNHANDLED_

OBJ_SIZE

OBJECTS

METHODS

STMNTS

STMNTS

STMNTS

MIMIC

MIMIC

MUT_BEHAVIOR

MUT # 1

MUT # 2

MUT # 3

MUT # 4

MUT # 5

84

6

11

5

5

MUT # 6

101

MUT # 7

MUT # 8

MUT # 9

MUT # 10

MUT # 11

MUT # 12

MUT # 13

MUT # 14

MUT # 15

MUT # 16

MUT # 17

MUT # 18

MUT # 19

MUT # 20

MUT # 21

MUT # 22

MUT # 23

MUT # 24

MUT # 25

28

34

42

33

16

10

19

19

18

18

8

9

8

18

5

19

39

85

40

MUT # 26

328

MUT # 27

MUT # 28

MUT # 29

MUT # 30

MUT # 31

MUT # 32

MUT # 33

MUT # 34

MUT # 35

MUT # 36

MUT # 37

MUT # 38

MUT # 39

MUT # 40

MUT # 41

MUT # 42

MUT # 43

MUT # 44

MUT # 45

MUT # 46

MUT # 47

11

29

79

19

31

62

23

27

40

20

12

12

11

23

11

5

8

14

17

36

12

MUT # 48

117

5

1

1

1

1

3

1

1

2

1

1

0

3

0

0

2

0

1

0

1

1

0

1

1

1

2

1

2

4

1

1

4

1

2

1

1

1

1

1

1

0

1

4

0

2

2

0

1

2

2

2

2

2

2

2

2

3

2

2

3

3

2

3

2

2

3

3

2

2

3

2

3

2

2

3

3

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

37 B

352 B

3.3 KB

4.3 KB

4.7 KB

6.5 KB

11.7 KB

267 KB

1.6 MB

2 MB

2.2 MB

2.3 MB

2.3 MB

2.4 MB

3 MB

3 MB

3.2 MB

3.6 MB

3.8 MB

3.8 MB

4.4 MB

4.6 MB

4.6 MB

5.3 MB

5.3 MB

5.4 MB

5.6 MB

5.8 MB

6.9 MB

6.4 MB

6.6 MB

6.8 MB

7.2 MB

7.4 MB

7.6 MB

8.3 MB

9.5 MB

9.5 MB

9.7 MB

10.1 MB

11.1 MB

21.4 MB

30.8 MB

31 MB

32 MB

32 MB

34 MB

39 MB

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

3

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

3

1

1

1

2

1

1

2

1

1

1

1

1

1

1

1

1

2

2

1

1

1

1

1

1

2

1

2

1

1

1

1

2

2

1

1

1

1

1

1

1

1

1

1

1

1

1

1

5

2

2

1

5

2

0

2

0

0

0

1

0

1

1

0

1

2

2

1

1

1

1

1

1

2

1

2

0

1

1

1

2

3

0

0

0

1

0

0

1

1

0

0

0

0

0

0

4

0

0

0

0

2

0

0

0

0

0

0

0

0

1

0

0

1

1

0

1

0

0

1

1

0

0

1

0

1

0

0

1

1

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

2

1

1

1

1

5

3

1

1

1

2

2

1

1

1

1

1

1

2

1

2

1

1

1

1

2

2

1

1

1

1

1

1

1

1

1

1

1

1

1

1

7

2

3

1

5

2

TOTAL: 48

MEDIAN:

MEDIAN:

19

1

106

MEDIAN:

5.3 MB

53

66

38

10

75

1

4

1

1

1

1

1

2

1

1

1

2

2

1

1

1

1

1

1

2

1

2

1

1

1

1

2

4

1

1

1

1

1

1

1

1

1

1

1

0

1

1

13

2

3

1

5

3

80

2

0

0

2

2

0

2

2

3

2

0

3

3

2

3

2

2

3

3

2

2

3

2

0

0

0

3

2

2

2

2

2

2

0

0

0

2

2

2

2

1

2

0

0

0

0

2

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

3

0

0

0

1

0

0

0

0

0

0

0

2

0

0

0

0

1

0

2

0

2

0

0

0

0

2

2

0

0

2

0

0

0

0

2

0

0

0

0

0

0

0

0

0

0

0

0

0

2

2

0

0

0

0

0

0

0

2

2

0

0

0

0

0

0

0

0

2

0

2

0

2

73

11

22

receiving object and the parameters are deserialized from
these serialized forms in order to recreate their production
state.

For example, consider MUT#32 in PDFBox (Table 5).
RICK captures the production state of the receiving object
on which MUT#32 gets invoked, and the 4 parameters for
this invocation. The size of these captured objects amounts
to a total of 6.8 MB. These objects serve as a snapshot
for reproducing the production state within the 2 tests
generated by RICK for MUT#32. For our experiments with
the three applications, the maximum size of the captured
objects is 39 MB for MUT#48 in PDFBox. The median size of
these captured objects is 486.5 bytes of serialized data.

The main feature of RICK is to monitor and collect

data about mockable method calls that occur within the
MUT, with the receiving objects and passed parameters.
Speciﬁcally, as detailed in subsection 3.3, RICK mocks the
parameters and ﬁelds of external types, on which mockable
method calls occur in production. For example, there are 2
mock objects within the 2 tests generated for MUT#19 in
GraphHopper (Table 3) for two ﬁelds in the declaring type
of MUT#19. Moreover, 2 mock method calls are made within
the tests, one on each of the 2 mock objects. In comparison,
there is 1 mock object within each of the 3 tests generated
for MUT#21 in Gephi (Table 4). This mock object replaces
the parameter of MUT#21, on which 2 mockable method
calls are observed by RICK in production. In total, RICK uses
151 mock objects as parameter or ﬁeld across the generated

1 @Test
2 @DisplayName("moveNode with parameter oracle,
3 mocking Node.x(), Node.y(), Node.setX(float), Node.setY(float)")
4 public void testMoveNode_PO() throws Exception {
5
6
7
8
9
10
11

// Arrange
StepDisplacement receivingObject = deserialize("receiving.xml");
Object[] paramObjects = deserialize("params.xml");
ForceVector paramObject2 = (ForceVector) paramObjects[1];
Node mockNode = Mockito.mock(Node.class);
when(mockNode.x()).thenReturn(-423.78378F);
when(mockNode.y()).thenReturn(107.523186F);

// Act
receivingObject.moveNode(mockNode, paramObject2);

// Assert
verify(mockNode, atLeastOnce()).x();
verify(mockNode, atLeastOnce()).y();
verify(mockNode, atLeastOnce()).setX(-403.92587F);
verify(mockNode, atLeastOnce()).setY(105.14341F);

13
14

16
17
18
19
20
21 }

Listing 7: The PO test generated by RICK for MUT#7 in
Gephi, which is a method called moveNode. The test mocks
a parameter on which four mocked methods are invoked.
The behavior of two mocked methods is stubbed within the
test.

tests. 204 mock methods are invoked on these mock objects,
reﬂecting the production interactions with external objects
within the MUTs. These generated mock objects recreate
actual interactions of the MUT with its environment. The
data serialized from production provide developers with
realistic test data.

Within the generated tests, the behavior of the mock
objects is deﬁned through method stubs. A stub provides the
canned response that should be returned from a non-void
method called on a mock object, given a set of parameters,
i.e., it deﬁnes the behavior of a mock method within the test
for an MUT. RICK sources the parameters and the primitive
returned value from production observations, and repre-
sents them through stubs within the generated tests. For ex-
ample, RICK generates 2 tests for MUT#7 in Gephi (Table 4).
This MUT is the method moveNode(Node,ForceVector).
Within each of the generated tests, 1 of the parameters of
moveNode is mocked, and 4 methods are invoked on this
mock object. We present the PO test generated for moveNode
in Listing 7. On line 9 the Node object is mocked. Within
the invocation of moveNode in production, RICK recorded
the invocation of the methods x() and y() on the Node
object, including their returned values. Consequently, RICK
expresses their behavior through the 2 stubs in the gener-
ated test (lines 10 and 11) using this production data. The in-
vocations of the two other mockable methods setX(float)
and setY(float) are not stubbed because they are void
methods. In total, RICK generates a total of 222 stubs to
mimic the production behavior of mockable method calls
that occur within MUTs. These generated stubs guide the
behavior of the MUT within the generated test, per the
observations made for it in production.

We observe from the column #OO_STMNTS in Table 3,
Table 4, and Table 5, that the number of OO statements
for any MUT is either 0 or 1. This is because the oracle
in OO tests is expressed as a single assertion statement
for the output of an MUT that returns a primitive value.
However, the number of PO and CO statements within
each test varies depending on the observations made for the

14

corresponding MUT in production. For instance, three tests
are generated for MUT#8 in GraphHopper (Table 3). In each
of the three tests, there is 1 mock object and 2 different mock
methods are invoked on this mock object. The behavior of
the 2 mock methods is deﬁned through the 4 generated
stubs. The OO test has an assertion to verify the output of
MUT#8. The PO test has 4 veriﬁcation statements to verify
the parameters with which the mock methods are called.
The CO test has 2 veriﬁcation statements which correspond
to the observations made by RICK about the sequence and
frequency of these mock method calls within the invocation
of MUT#8 in production.

In total, across the 294 tests generated for the 128 MUTs,
RICK generates 38 assertion statements (one in each OO),
257 statements that verify the parameters with which mock
methods are called, and 293 statements to verify the se-
quence and frequency of mock method calls. Furthermore,
RICK uses the parameters passed to, and the value returned
from the mockable method calls observed in production, to
generate a total of 222 stubs across the 294 tests.

Answer to RQ2

RICK captures a wide range of production data for test
generation. We have demonstrated with static insights that
it can handle well the two dimensions of mock-based test-
ing: the complexity of mock objects and the complexity of
mockable methods that are stubbed. Finally, the analysis of
the generated tests shows that RICK can generate various
types of oracles that verify different aspects of the MUT
interacting with its environment.

5.3 Results for RQ3 [Mimicking Production]

The results for RQ3 are presented in the last three columns
in Table 3, Table 4, and Table 5. Since all the tests gen-
erated by RICK for the 128 MUTs are self-contained and
compile correctly, we run each of them and report the
status of the execution. In each row, the column #SUCCESS-
FULLY_MIMIC highlights the number of tests that com-
pletely recreate the observed production context with mocks
and oracle(s) that pass, while #INCOMPLETELY_MIMIC
signiﬁes the number of tests for which at least one oracle
fails. The last column, #UNHANDLED_MUT_BEHAVIOR,
represents those test executions where we observe a runtime
exception thrown by the MUT. We now discuss the implica-
tions of each of these scenarios, and why they occur.

An OO test successfully mimics the production context
if the MUT returns the same output as it did in production,
when invoked with mocks replacing the external objects,
and stubs for the behavior of the mock method calls. If
the test does not completely mimic the production behavior
of the MUT, the invocation of the MUT returns a different
output, which is unequal to the one returned by the MUT
in production. Consequently, the assertion statement within
the OO test fails.

For a PO test to be successful, all the veriﬁcation state-
ments must pass, indicating that mock methods are called
by the MUT within the generated test with the same argu-
ments as the ones observed in production. In comparison,
a failure in any of the veriﬁcation statements implies that

a mock method call occurs with different parameters than
the ones observed for its invocation in production. This
implies that the test does not faithfully recreate the observed
interactions of the MUT and the mock method.

A passing CO test veriﬁes that the mock method calls oc-
cur in the same order and the same number of times within
the MUT, as they were in production. On the contrary, if the
test does not completely mimic the order and/or frequency
of mock method calls, a veriﬁcation statement will fail.

The proportion of tests that successfully mimic produc-
tion behavior differs across case studies (column #SUC-
CESSFULLY_MIMIC). Of the tests generated for GraphHop-
per, 59.7% are successful. In Gephi, the successful tests are
35% of the total generated tests, while in PDFBox 68.9%
tests are successful. Overall, the 154 successful tests account
for 52.4% of all the tests generated. This is arguably a high
ratio given the multi-stage pipeline of RICK, where each
stage can fail in some conditions. In 52.4% of cases, all stages
of RICK succeed: All the captured objects are correctly serial-
ized and deserialized before the MUT is invoked, recreating
an appropriate and realistic execution state. Also, the mock
methods are successfully stubbed: they mimic production
behavior without impacting the behavior of the MUT.

In the column #INCOMPLETELY_MIMIC, we observe
that 57 generated tests, which account for 19.4% of the total,
have a failing oracle, implying that they do not completely
mimic production behavior. This includes 32.2% of the
GraphHopper tests, 20.6% Gephi tests, and 10.4% of the
tests generated for PDFBox. These failures can occur due to
the following reasons.

Unfaithful recreation of production state: The captured ob-
jects may be inaccurately deserialized within the generated
test, implying that production states are not completely
recreated. Deserialization of complex objects captured from
production is a known problem and a key challenge for
recreating real production conditions in generated tests [14].
A test may also fail because a production resource, such as
a ﬁle, is not available during test execution, resulting in an
exception. Moreover, since mocks are skeletal objects that
substitute a concrete object within the test, they can induce
a change in the path taken through the MUT, which renders
the oracle unsuccessful. For example, the tests generated for
MUT#2 in GraphHopper (Table 3) fail due to failing OO,
PO, and CO oracles. The tests mock a list object, and MUT#2
tries to invoke a loop over the size of this mock list. Since the
loop is not exercised, a different path is traversed through
the MUT than the one observed in production.

Type-based stubbing: We observe that some tests fail be-
cause of the granularity of stubbing. RICK stubs mockable
methods called on the type of an object, but not based
on speciﬁc instances of the object. A failure can occur
if an MUT calls the same mockable method on multiple
parameters or ﬁelds of the same type. For instance, we
present an excerpt of MUT#24 in PDFBox in Listing 8.
This method codeToGID(int) (line 6), has three calls to the
same mockable method, getGlyphId(int) (lines 9, 11, and
13). These calls are made on three different ﬁelds of type
CmapSubtable called cmapWinUnicode, cmapWinSymbol,
and cmapMacRoman deﬁned in the PDTrueTypeFont class
(lines 2 to 4). RICK records the mockable method call in
production, and mocks the three ﬁelds. However, the in-

15

1 class PDTrueTypeFont {
2
3
4

CmapSubtable cmapWinUnicode;
CmapSubtable cmapWinSymbol;
CmapSubtable cmapMacRoman;

public int codeToGID(int code) {

...
if (...) {

gid = cmapWinUnicode.getGlyphId(...);

} else if (...) {

gid = cmapMacRoman.getGlyphId(...);

} else {

gid = cmapWinSymbol.getGlyphId(...);

}
...
return gid;

}

6
7
8
9
10
11
12
13
14
15
16
17
18 }

Listing 8: The same mockable method, getGlyphId, is called
multiple times within MUT#24 of PDFBox, codeToGID. Each
call is made on a different ﬁeld of the same type.

formation on which of these mock ﬁelds actually calls the
mock method is not available. One solution would be to do
object-based stubbing, but we are not aware of any work on
this and consider this sophistication as future work.

of

We

the

cases

now discuss

#UNHAN-
DLED_MUT_BEHAVIOR. The execution of 83 of
the
294 generated tests (28.2%) causes exceptions to be thrown
by the MUT. For example, the MUT may have multiple non-
mockable interactions with the mock object, before the mock
method is invoked on it. Any of these other interactions
can behave unexpectedly. This results in exceptions to be
raised before the oracle is even evaluated within the test.
We observe these cases in all three applications: 8.1% in
GraphHopper, 44.5% in Gephi, 20.7% in PDFBox. For
example, a null pointer exception is thrown from MUT#1
and an illegal argument exception is raised within MUT#21
in GraphHopper. MUT#1 calls other methods on the mock
parameter, and MUT#21 calls a method which accesses the
mock ﬁeld, before the mockable method is called. Handling
such cases is an important direction for future work on
automated mock generation.

Answer to RQ3

RICK succeeds in generating 294 tests, of which 154
(52.4%) fully mimic production observations with fully
passing oracles. For 19.4% of the test cases, at least one
oracle statement fails, showing that the oracles can indeed
differentiate between successfully mimicked and incom-
pletely mimicked contexts. At runtime, the majority of the
tests generated by RICK do completely mimic production
behavior in the sense that the state asserted by the oracle
is equal to the one observed in production. The cases
where the generated tests fail at runtime reveal promising
research directions for sophisticated production monitor-
ing tools, such as effective deserialization and efﬁcient
resource snapshotting.

5.4 Results for RQ4 [Quality]

Per the protocol described in subsection 4.4, we have in-
terviewed 5 developers between June and July, 2022, with
the goal of assessing their opinion on the tests generated
by RICK. Table 6 presents the details of the participants

TABLE 6: Proﬁles of the developers who participated in the survey to assess the quality of tests generated by RICK

16

PART

EXP

SECTOR

WRITES TESTS

USES MOCKS

PROGRAMMING ENV

P1

P2

P3

P4

P5

6

30

7

10

14

Consultancy

“everyday, testing is [my] life"

sometimes

JUnit + Mockito

Consultancy

often

sometimes

JUnit + Mockito

Telecom

TDD practitioner

sometimes

Python (pytest)

Product (GraphHopper)

“all the time"

sometimes

JUnit + Mockito

Game development

sometimes

rarely, “want to mock more“

C, C#, .NET

(PART) of the survey. The ﬁve developers work in different
sectors of the IT industry, and have between 6 and 30 years
of experience with software development (denoted by the
column EXP). Notably, participant P4 is a core contributor
to GraphHopper, with the highest number of commits to its
GitHub repository in the last 5 years. From Table 6, we see
that all participants write tests sometimes or everyday, and
most of them also deﬁne and use mocks. The developers
also work with diverse programming environments (PRO-
GRAMMING ENV). P1, P2, and P4 work with Java testing
and mocking frameworks, speciﬁcally JUnit and Mockito.
P3 is a Python developer, while P5, who works in a game
development company, works mostly with C, C#, and .NET
framework.

We begin each meeting by introducing the concepts and
terminology of mock-based testing, the RICK pipeline, and
our experiments with GraphHopper. In order to demon-
strate the features of the tests generated by RICK, we select
a total of 6 generated tests, one for each of the three
mock-based oracles for 2 MUTs deﬁned in GraphHopper.
We manually select these MUTs based on their complexity
(measured in LOC), as well as the number and target of the
mockable method calls within them. The two selected MUTs
have 58 and 13 LOC, respectively (MUT#18 and MUT#14 in
Table 3). The ﬁrst MUT has a mockable method call on an
external parameter object. The second MUT has mockable
method calls on a ﬁeld. We introduce these MUTs to the
participant, also presenting their source code. We invite
them to clone a fork of GraphHopper which includes the
generated tests. During the meeting, we browse through the
generated tests with them via screen sharing. Finally, we ask
the participant three sets of questions about the generated
tests while documenting their responses. These questions
relate to mocking effectiveness,
i.e., how mocks are used
within the tests, as well as the structure and understandability
of the generated tests.

Mocking effectiveness: The ﬁrst set of questions relates to
how the mocks are used in the generated tests. Per their
answers, all ﬁve participants agree that the generated tests
for the 2 MUTs represent realistic behavior of GraphHopper
in production, which would be useful for developers. P2
observed that this can “save the time spent on deciding the
combination of inputs and ﬁnding corner cases, especially for
methods with branches." P5 added that, according to them,
collecting data from production is “where RICK shines most,
since it abstracts away [for developers] the tricky exercise of
deciding test inputs and internal states." Furthermore, we
had detailed discussions with the participants about the

veriﬁcation statements in PO and CO tests. P1, P3, and P5
noted that they contribute differently to the veriﬁcation of
the behavior of the MUT. P5 remarked that while some
veriﬁcation statements may be redundant, they “can be
manually customized by developers" when generated tests are
presented to them. However, P3, who works primarily with
Python, a dynamically-typed language, commented that for
them “the veriﬁcation of the frequency of the mock method calls
in the CO tests is useful, but not the anyInt() or anyString()
wildcards to match argument types." Additionally, P3 and P4
also discussed the stability of these tests with respect to code
refactoring, with P4 saying that “the tests might break in case
a developer refactors legacy code. But because they are so detailed,
a regression can also be ﬁgured out at a very low level." P5
highlighted an interesting aspect about the human element
in software development by sharing that “while RICK ﬁts
exactly an actual problem in the industry, a potential disadvantage
is that it can spoil developers who may become incentivized to
design without testability in mind. The tests can be automatically
produced later when the application is production-ready."

Structure: Next, we assess the opinion of developers
about the structure of the generated tests. All 5 develop-
ers appreciated the “Arrange-Act-Assert" pattern that is
systematically followed in the generated tests. They note
that this pattern makes the structure of the tests clear. P2,
who has experimented with other test generation tools,
noted that clear structure and intention is important to
improve the adoption of automated tools. All the devel-
opers mentioned that they typically use the pattern when
writing tests, including P5 who added that the structure
was “spot on." Moreover, P1, P2, and P4 were appreciative
of the description generated by RICK for each test, using the
@DisplayName JUnit annotation, with P2 noting that “it is
rare and useful."

Understandability: Finally, we question each participant
about the understandability of the generated tests. P1 and
P2 mentioned that the comments demarcating each phase
in the generated tests contribute to their intuitiveness and
make their intention clearer, with P5 exclaiming that they
make the tests “super easy to visualize." Additionally, P2,
P4, and P5 noted that the comments are useful, especially
since they help understand what is happening within tests
generated automatically. However, P4 observed that while
the comments “do not hurt", they would not add them
while writing the test manually. P3 was also of the opinion
that the comments may be removed without impacting the
understandability of the tests.

The perception of developers of automatically generated

17

tests and mocks, using data collected from production, is
valuable qualitative feedback about the relevance of RICK.
The 5 experienced developers and testers conﬁrm that the
data collected in production is realistic and useful to gen-
erate tests and mocks. They also appreciate the systematic
structure of the test cases, as well as the explicit intention
documented in comments.

This qualitative study suggests further work in the area
of automated mock generation. For example, P3 expressed
interest in analyzing the inﬂuence of the architecture of the
system under test on the stability of the mocks. P4 observed
that it would be useful to evaluate the overall testability of
an application in terms of how many MUTs have mockable
method calls. We also discussed with P5 about adding
more context and business-awareness to test names and
comments to further help developers troubleshoot test exe-
cutions. We identify these as excellent directions for further
work, with much potential for impact on the industry.

their use in the replacement of calls to the ﬁle system [36].
Spadini et al. discuss the criteria developers consider when
deciding what to mock [3], as well as how these mocks
evolve [18]. MockSniffer by Zhu et al. [37] uses machine
learning models to recommend mocks. Mockingbird by
Lockwood et al. [38] uses mocks to isolate the code under
dynamic analysis from its dependencies. The use of mocks
for Test Driven Development [39], modeling [40], and as an
educational tool for object-oriented design [41], [42] have
also been investigated. These works have been inspirational
for us in many respects. Moreover, RICK is designed to
generate tests that use Mockito, which is the most popular
mocking framework [19], and is itself a subject of study
[43]–[46]. However, none of the related work uses data
collected from production to automatically generate mocks
and mock-based oracles, nor do previous works discuss the
characteristics of mocks generated automatically. These are
the two key contributions of our work.

Answer to RQ4

Five experienced developers conﬁrm that the concept of
data collection in production is relevant for the gener-
ation of tests with mocks. They all appreciate the sys-
tematic “Arrange-Act-Assert" template for the test which
contributes to the overall good understandability of the
generated tests.

5.5 Threats to validity

One threat to the internal validity of RICK comes from the
technical operations that it relies on. For example, instru-
mentation of some methods may fail, or (de)serialization
of large and complex objects may be unsuccessful due to
low level bugs. To mitigate this threat, we found RICK on
a suite of state-of-the-art tools in the respective domains
of static analysis, instrumentation, serialization, and code
generation.

A source of threat to the external validity of our ﬁndings
is the applications we consider for the evaluation of RICK. To
mitigate this threat, we make sure that the three applications
are 1) complex, and 2) from very different application do-
mains. Also, we ensure that our production usage scenarios
are typical and representative of their operation.

6 RELATED WORK

This section presents the literature on mock objects, as well
as their automated generation. We also discuss studies about
the use of information collected from production for the
generation of tests.

6.1 Studies on Mocking

Since mocks were ﬁrst proposed [1], they have been widely
studied [2], [30]. Their use has been analyzed for major
platforms, such as Java [18], Python [31], C [32], Scala
[33], and Android [34], [35]. These studies highlight the
prevalence and practices of deﬁning and using mocks. Some
empirical studies analyze more speciﬁc aspects about the
usage of mocks, such as their deﬁnition through developer-
written mock classes [17], or mocking frameworks [19], or

6.2 Mock Generation

Several studies propose approaches to automatically gen-
erate mocks. For example, search-based test generation can
be extended to include mock objects [20], to mock calls to
the ﬁle system [5] and the network [6]. Symbolic execution
may also be used to generate mocks [7], [21], [47], to mock
the ﬁle system [48], or a database for use in tests [49].
Honﬁ and Micskei [50] generate mocks to replace external
interactions with calls to a parameterized sandbox. This
sandbox receives inputs from the white-box test generator,
Pex [22]. Moles by Halleux and Tillmann [51] also works
with Pex to isolate the unit under test from interactions with
dependencies by delegating them to alternative implemen-
tations. Salva and Blot [52] propose a model-based approach
for mock generation. Bhagya et al. [53] use machine learning
models to mock HTTP services using network trafﬁc data.
GenUTest [54] generates JUnit tests and mock aspects by
capturing the interactions that occur between objects during
the execution of medium-sized Java programs.

To the best of our knowledge, RICK is the only tool that
generates mock-based oracles to verify distinct aspects of
production executions.

The deﬁnition and behavior of mocks can also be ex-
tracted from other artifacts. For instance, the design contract
of the type being mocked can be used to deﬁne the behav-
ior of a mock [55]. Samimi et al. [56] propose declarative
mocking, an approach that uses constraint solving with
executable speciﬁcations of the mock method calls. Solms
and Marshall [57] extract the behavior of mock objects
from interfaces that specify their contract. Wang et al. [58],
[59] propose an approach to refactor out developer-written
subclasses that are used for the purpose of mocking, and
replace them with Mockito mocks. The executions of tests
in the existing test suite can also be leveraged to generate
mocks. This approach has been used by Saff et al. [8] through
system test executions, and Fazzini et al. [34] to generate
mocks for mobile applications. Bragg et al. [60] use the
test suite of Sketch programs to generate mocks in order
to modularize program synthesis. Mocks have also been
generated in the context of cloud computing [61], such
as for the emulation of infrastructure by MockFog [15].

Jacinto et al. propose a mock-testing mode for drag-and-
drop application development platforms [62]. Contrary to
these approaches, RICK monitors applications in production
in order to generate mocks. Consequently, the generated
tests reﬂect the behaviour of an application with respect to
actual user interactions.

6.3 Production-based Oracles

Monitoring an executing application with the goal of gener-
ating tests is an effective means of bridging the gap between
the developers’ understanding of their system, and how
it is actually exercised by users [12]. To this end, several
studies propose tools that capture runtime information.
Thummalapenta et al. [63] use execution traces for the
generation of parameterized unit tests. Wang and Orso [13]
capture the sequence of method executions in the ﬁeld,
and apply symbolic execution to generate tests for untested
behavior. Jaygarl et al. [64] capture objects from program
executions, which can then be used as inputs by other
tools for the generation of method sequences. Tiwari et al.
[14] generate tests for inadequately tested methods using
production object states. Incoming production requests have
also been utilized to produce tests for databases [65] and
web applications [66], [67]. RICK leverages this methodology
with the novel and speciﬁc goal of generating tests with
mock-based oracles that verify the interactions between a
method and objects of external types, as they occur in
production.

7 CONCLUSION

In this paper, we present RICK, a novel tool that generates
tests with mocks using data captured from the observation
of applications executing in production. RICK instruments
a set of methods under tests, monitors their invocations
in production, and captures data about the methods and
the mockable method calls. Finally, RICK generates tests
using the captured data. The mock-based oracles verify
distinct aspects of the interactions of the method under
test with the external object, such as the output (OO), the
parameters (PO), and the sequence of these invocations
(CO). Our evaluation with three open-source applications
demonstrates that RICK never gives up: It monitors and
transforms observed production behavior into concrete tests
(RQ1). The data collected from production is expressed
within these generated tests as complex receiving objects
and parameters for the methods, as well stubs and mock-
based oracles (RQ2). When executed, 52.4% of the generated
tests successfully mimic the observed production behavior.
This means that they recreate the execution context for the
method under test, the stubbed behavior is appropriate, and
the oracle veriﬁes that the method under test behaved the
same way as it did in production (RQ3). Furthermore, our
qualitative survey with professional software developers
revealed that the data and oracle extracted from production
by RICK are relevant, and that the systematic structure of
RICK tests is understandable (RQ4).

Overall, we are the ﬁrst to demonstrate the feasibility
of creating tests with mocks directly from production, in
other terms to capture production behavior in isolated tests.

18

Since the generated tests reﬂect the actual behavior of an
application in terms of concrete inputs and oracles, they are
valuable for developers to augment manually crafted inputs
with ones that are relevant in production.

Our ﬁndings open up several opportunities for more
research. It would be useful to handle more kinds of in-
teractions of a method under test with its environment,
such as all method calls made on an external object within
the method under test, in order to achieve further isolation
within the generated tests. The feedback we received from
developers indicates that in the future, RICK should take the
speciﬁcities of the application under test into account for
tailored test and mock generation.

REFERENCES

[1] T. Mackinnon, S. Freeman, and P. Craig, “Endo-testing: unit testing
with mock objects,” Extreme programming examined, pp. 287–301,
2000.

[2] D. Thomas and A. Hunt, “Mock objects,” IEEE Software, vol. 19,

no. 3, pp. 22–24, 2002.

[3] D. Spadini, M. Aniche, M. Bruntink, and A. Bacchelli, “To mock
or not to mock? an empirical study on mocking practices,” in
2017 IEEE/ACM 14th International Conference on Mining Software
Repositories (MSR).
IEEE, 2017, pp. 402–412.

[4] E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo, “The
oracle problem in software testing: A survey,” IEEE transactions on
software engineering, vol. 41, no. 5, pp. 507–525, 2014.

[5] A. Arcuri, G. Fraser, and J. P. Galeotti, “Automated unit test gener-
ation for classes with environment dependencies,” in Proceedings
of the 29th ACM/IEEE international conference on Automated software
engineering, 2014, pp. 79–90.

[6] ——, “Generating tcp/udp network data for automated unit test
generation,” in Proceedings of the 2015 10th Joint Meeting on Founda-
tions of Software Engineering, 2015, pp. 155–165.

[7] M. Islam and C. Csallner, “Dsc+ mock: A test case+ mock class
generator in support of coding against interfaces,” in Proceedings
of the Eighth International Workshop on Dynamic Analysis, 2010, pp.
26–31.

[8] D. Saff, S. Artzi, J. H. Perkins, and M. D. Ernst, “Automatic test fac-
toring for java,” in Proceedings of the 20th IEEE/ACM international
Conference on Automated software engineering, 2005, pp. 114–123.
[9] M. Fowler, “Mocks aren’t stubs,” https://martinfowler.com/

articles/mocksArentStubs.html, date accessed 2022-06-01.

[10] M. Christakis, P. Emmisberger, P. Godefroid, and P. Müller, “A
general framework for dynamic stub injection,” in 2017 IEEE/ACM
39th International Conference on Software Engineering (ICSE).
IEEE,
2017, pp. 586–596.

[11] E. Daka and G. Fraser, “A survey on unit testing practices and
problems,” in 2014 IEEE 25th International Symposium on Software
Reliability Engineering.

IEEE, 2014, pp. 201–211.

[12] Q. Wang, Y. Brun, and A. Orso, “Behavioral execution comparison:
Are tests representative of ﬁeld behavior?” in 2017 IEEE Inter-
national Conference on Software Testing, Veriﬁcation and Validation
(ICST).

IEEE, 2017, pp. 321–332.

[13] Q. Wang and A. Orso, “Improving testing by mimicking user
behavior,” in 2020 IEEE International Conference on Software Main-
tenance and Evolution (ICSME), 2020, pp. 488–498.

[14] D. Tiwari, L. Zhang, M. Monperrus, and B. Baudry, “Production
monitoring to improve test suites,” IEEE Transactions on Reliability,
pp. 1–17, 2021.

[15] J. Hasenburg, M. Grambow, and D. Bermbach, “Mockfog 2.0: Au-
tomated execution of fog application experiments in the cloud,”
IEEE Transactions on Cloud Computing, pp. 1–1, 2021.

[16] N. E. Beckman, D. Kim, and J. Aldrich, “An empirical study of
object protocols in the wild,” in European Conference on Object-
Oriented Programming. Springer, 2011, pp. 2–26.

[17] G. Pereira and A. Hora, “Assessing mock classes: An empirical
study,” in 2020 IEEE International Conference on Software Mainte-
nance and Evolution (ICSME), 2020, pp. 453–463.

[18] D. Spadini, M. Aniche, M. Bruntink, and A. Bacchelli, “Mock
objects for testing java systems,” Empirical Software Engineering,
vol. 24, no. 3, pp. 1461–1498, 2019.

[19] S. Mostafa and X. Wang, “An empirical study on the usage of
mocking frameworks in software testing,” in 2014 14th Interna-
tional Conference on Quality Software, 2014, pp. 127–132.

[20] A. Arcuri, G. Fraser, and R. Just, “Private API access and func-
tional mocking in automated unit test generation,” in 2017 IEEE
International Conference on Software Testing, Veriﬁcation and Valida-
tion, ICST, 2017, pp. 126–137.

[21] N. Tillmann and W. Schulte, “Mock-object generation with be-
havior,” in 21st IEEE/ACM International Conference on Automated
Software Engineering (ASE’06).

IEEE, 2006, pp. 365–368.

[22] N. Tillmann and J. d. Halleux, “Pex–white box test generation for.
Springer, 2008,

net,” in International conference on tests and proofs.
pp. 134–153.

[23] S. Thummalapenta, T. Xie, N. Tillmann, J. De Halleux, and
W. Schulte, “Mseqgen: Object-oriented unit-test generation via
mining source code,” in Proceedings of the 7th joint meeting of the
European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering, 2009, pp. 193–
202.

[24] R. Pawlak, M. Monperrus, N. Petitprez, C. Noguera, and L. Sein-
turier, “Spoon: A library for implementing analyses and trans-
formations of java source code,” Software: Practice and Experience,
vol. 46, no. 9, pp. 1155–1179, 2016.

[25] M. Bastian, S. Heymann, and M. Jacomy, “Gephi: an open source
software for exploring and manipulating networks,” in Proceedings
of the international AAAI conference on web and social media, vol. 3,
no. 1, 2009, pp. 361–362.

[26] A. M. Memon and M. B. Cohen, “Automated testing of gui
applications: models, tools, and controlling ﬂakiness,” in 2013 35th
International Conference on Software Engineering (ICSE).
IEEE, 2013,
pp. 1479–1480.

[27] A. Benelallam, N. Harrand, C. Soto-Valero, B. Baudry, and
O. Barais, “The maven dependency graph: A temporal graph-
based representation of maven central,” in Proceedings of the 16th
International Conference on Mining Software Repositories, ser. MSR’19.
IEEE Press, 2019, p. 344–348.

[28] C. Soto-Valero, A. Benelallam, N. Harrand, O. Barais, and
B. Baudry, “The emergence of software diversity in maven cen-
tral,” in Proceedings of the 16th International Conference on Mining
Software Repositories, ser. MSR’19.

IEEE Press, 2019, p. 333–343.

[29] S. Garﬁnkel, P. Farrell, V. Roussev, and G. Dinolt, “Bringing science
to digital forensics with standardized forensic corpora,” digital
investigation, vol. 6, pp. S2–S11, 2009.

[30] S. Freeman, T. Mackinnon, N. Pryce, and J. Walnes, “Mock roles,
not objects,” in Companion to the 19th annual ACM SIGPLAN
conference on Object-oriented programming systems, languages, and
applications, 2004, pp. 236–246.

[31] F. Trautsch and J. Grabowski, “Are there any unit tests? an em-
pirical study on unit testing in open source python projects,” in
2017 IEEE International Conference on Software Testing, Veriﬁcation
and Validation (ICST), 2017, pp. 207–218.

[32] S. Mudduluru, “Investigation of test-driven development based
on mock objects for non-oo languages,” Master’s thesis, Linköping
University, Department of Computer and Information Science,
The Institute of Technology, 2012.

[33] K. Laufer, J. O’Sullivan, and G. K. Thiruvathukal, “Tests as main-
tainable assets via auto-generated spies: A case study involving
the scala collections library’s iterator trait,” in Proceedings of the
Tenth ACM SIGPLAN Symposium on Scala, 2019, pp. 17–21.

[34] M. Fazzini, A. Gorla, and A. Orso, “A framework for automated
test mocking of mobile apps,” in 2020 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE).
IEEE, 2020,
pp. 1204–1208.

[35] M. Fazzini, C. Choi, J. M. Copia, G. Lee, Y. Kakehi, A. Gorla,
and A. Orso, “Use of test doubles in android testing: An in-depth
investigation,” in 2022 IEEE/ACM 44th International Conference on
Software Engineering (ICSE), 2022, pp. 2266–2278.

[36] M. R. Marri, T. Xie, N. Tillmann, J. De Halleux, and W. Schulte, “An
empirical study of testing ﬁle-system-dependent software with
mock objects,” in 2009 ICSE Workshop on Automation of Software
Test.

IEEE, 2009, pp. 149–153.

[37] H. Zhu, L. Wei, M. Wen, Y. Liu, S.-C. Cheung, Q. Sheng, and
C. Zhou, “Mocksniffer: Characterizing and recommending mock-
ing decisions for unit tests,” in Proceedings of the 35th IEEE/ACM
International Conference on Automated Software Engineering, ser. ASE
’20. New York, NY, USA: Association for Computing Machinery,
2020, p. 436–447.

19

[38] D. Lockwood, B. Holland, and S. Kothari, “Mockingbird: a frame-
work for enabling targeted dynamic analysis of java programs,”
in 2019 IEEE/ACM 41st International Conference on Software Engi-
neering: Companion Proceedings (ICSE-Companion).
IEEE, 2019, pp.
39–42.

[39] T. Kim, C. Park, and C. Wu, “Mock object models for test driven
development,” in Fourth International Conference on Software Engi-
neering Research, Management and Applications (SERA’06).
IEEE,
2006, pp. 221–228.

[40] J. Stoel, T. van der Storm, and J. Vinju, “Modeling with mocking,”
in 2021 14th IEEE Conference on Software Testing, Veriﬁcation and
Validation (ICST).

IEEE, 2021, pp. 59–70.
[41] J. Nandigam, V. N. Gudivada, A. Hamou-Lhadj, and Y. Tao,
“Interface-based object-oriented design with mock objects,” in
2009 Sixth International Conference on Information Technology: New
Generations.

IEEE, 2009, pp. 713–718.

[42] J. Nandigam, Y. Tao, V. N. Gudivada, and A. Hamou-Lhadj,
“Using mock object frameworks to teach object-oriented design
principles,” The Journal of Computing Sciences in Colleges, vol. 26,
no. 1, pp. 40–48, 2010.

[43] G. Gay, “Challenges in using search-based test generation to iden-
tify real faults in mockito,” in International Symposium on Search
Based Software Engineering. Springer, 2016, pp. 231–237.

[44] A. J. Turner, D. R. White, and J. H. Drake, “Multi-objective
regression test suite minimisation for mockito,” in International
Symposium on Search Based Software Engineering.
Springer, 2016,
pp. 244–249.

[45] S. Wang, M. Wen, X. Mao, and D. Yang, “Attention please:
Consider mockito when evaluating newly proposed automated
program repair techniques,” in Proceedings of the Evaluation and
Assessment on Software Engineering, 2019, pp. 260–266.

[46] D. J. Kim, N. Tsantalis, T.-H. P. Chen, and J. Yang, “Studying
test annotation maintenance in the wild,” in 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE).
IEEE, 2021,
pp. 62–73.

[47] N. Alshahwan, Y. Jia, K. Lakhotia, G. Fraser, D. Shuler, and
P. Tonella, “AUTOMOCK: Automated Synthesis of a Mock En-
vironment for Test Case Generation,” in Practical Software Testing
: Tool Automation and Human Factors, ser. Dagstuhl Seminar Pro-
ceedings (DagSemProc), vol. 10111, 2010, pp. 1–4.

[48] S. Kong, N. Tillmann, and J. de Halleux, “Automated testing
of environment-dependent programs - a case study of modeling
the ﬁle system for pex,” in 2009 Sixth International Conference on
Information Technology: New Generations, 2009, pp. 758–762.

[49] K. Taneja, Y. Zhang, and T. Xie, “Moda: Automated test generation
for database applications via mock objects,” in Proceedings of the
IEEE/ACM international conference on Automated software engineer-
ing, 2010, pp. 289–292.

[50] D. Honﬁ and Z. Micskei, “Automated isolation for white-box
test generation,” Information and Software Technology, vol. 125, p.
106319, 2020.

[51] J. d. Halleux and N. Tillmann, “Moles: tool-assisted environment
isolation with closures,” in International Conference on Modelling
Techniques and Tools for Computer Performance Evaluation. Springer,
2010, pp. 253–270.

[52] S. Salva and E. Blot, “Using model learning for the generation
of mock components,” in IFIP International Conference on Testing
Software and Systems. Springer, 2020, pp. 3–19.

[53] T. Bhagya, J. Dietrich, and H. Guesgen, “Generating mock skele-
tons for lightweight web-service testing,” in 2019 26th Asia-Paciﬁc
Software Engineering Conference (APSEC).
IEEE, 2019, pp. 181–188.
[54] B. Pasternak, S. Tyszberowicz, and A. Yehudai, “Genutest: a unit
test and mock aspect generation tool,” International journal on
software tools for technology transfer, vol. 11, no. 4, pp. 273–290, 2009.
[55] S. J. Galler, A. Maller, and F. Wotawa, “Automatically extracting
mock object behavior from design by contract™ speciﬁcation
for test data generation,” in Proceedings of the 5th Workshop on
Automation of Software Test, 2010, pp. 43–50.

[56] H. Samimi, R. Hicks, A. Fogel, and T. Millstein, “Declarative
mocking,” in Proceedings of the 2013 International Symposium on
Software Testing and Analysis, 2013, pp. 246–256.

[57] F. Solms and L. Marshall, “Contract-based mocking for services-
oriented development,” in Proceedings of the Annual Conference of
the South African Institute of Computer Scientists and Information
Technologists, 2016, pp. 1–8.

[58] X. Wang, L. Xiao, T. Yu, A. Woepse, and S. Wong, “An automatic
refactoring framework for replacing test-production inheritance
by mocking mechanism,” in Proceedings of the 29th ACM Joint

20

Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, 2021, pp. 540–552.
[59] ——, “Jmocker: Refactoring test-production inheritance by mock-
ito,” in 2022 IEEE/ACM 44th International Conference on Software
Engineering: Companion Proceedings (ICSE-Companion).
IEEE, 2022,
pp. 125–129.

[60] N. F. Bragg, J. S. Foster, C. Roux, and A. Solar-Lezama, “Program
sketching by automatically generating mocks from tests,” in Inter-
national Conference on Computer Aided Veriﬁcation. Springer, 2021,
pp. 808–831.

[61] L. Zhang, X. Ma, J. Lu, T. Xie, N. Tillmann, and P. De Halleux,
“Environmental modeling for automated cloud application test-
ing,” IEEE software, vol. 29, no. 2, pp. 30–35, 2011.

[62] A. Jacinto, M. Lourenço, and C. Ferreira, “Test mocks for low-
code applications built with outsystems,” in Proceedings of the 23rd
ACM/IEEE International Conference on Model Driven Engineering
Languages and Systems: Companion Proceedings, 2020, pp. 1–5.

[63] S. Thummalapenta, J. d. Halleux, N. Tillmann, and S. Wadsworth,
“Dygen: Automatic generation of high-coverage tests via mining
gigabytes of dynamic traces,” in International Conference on Tests
and Proofs. Springer, 2010, pp. 77–93.

[64] H. Jaygarl, S. Kim, T. Xie, and C. K. Chang, “Ocat: object capture-
based automated testing,” in Proceedings of the 19th international
symposium on Software testing and analysis, 2010, pp. 159–170.
[65] J. Yan, Q. Jin, S. Jain, S. D. Viglas, and A. Lee, “Snowtrail: Testing
with production queries on a cloud database,” in Proceedings of the
Workshop on Testing Database Systems, 2018, pp. 1–6.

[66] A. Marchetto, P. Tonella, and F. Ricca, “State-based testing of ajax
web applications,” in 2008 1st International Conference on Software
Testing, Veriﬁcation, and Validation.

IEEE, 2008, pp. 121–130.

[67] L. Zetterlund, D. Tiwari, M. Monperrus, and B. Baudry, “Har-
vesting production graphql queries to detect schema faults,” in
2022 IEEE Conference on Software Testing, Veriﬁcation and Validation
(ICST).

IEEE, 2022, pp. 365–376.

