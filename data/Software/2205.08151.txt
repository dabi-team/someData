2
2
0
2

y
a
M
9
1

]

O
R
.
s
c
[

2
v
1
5
1
8
0
.
5
0
2
2
:
v
i
X
r
a

Cluster on Wheels

Yuanyuan Yang, Delin Feng and S¨oren Schwertfeger

Published with:

IEEE International Conference for Advancement in Technology
(ICONAT) 2022

Citation:

Yuanyuan Yang, Delin Feng and S¨oren Schwertfeger, ”Cluster on Wheels”, IEEE International Conference for
Advancement in Technology (ICONAT) 2022: IEEE Press, 2022.

DOI: 10.1109/ICONAT53423.2022.9725992

This is a publication from the Mobile Autonomous Robotic Systems Lab (MARS Lab), School of Information Science
and Technology (SIST) of ShanghaiTech University. For this and other publications from the MARS Lab please visit:
https://robotics.shanghaitech.edu.cn/publications

© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses,
in any current or future media, including reprinting/republishing this material for advertising or promotional purposes,
creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of
this work in other works.

STAR CENTERShanghaiTech Automation and Robotics Center  • 上海科技大学自动化与机器人中心MARS LAB 
 
 
 
 
 
Cluster on Wheels

Yuanyuan Yang1, Delin Feng1 and S¨oren Schwertfeger1

Abstract— This paper presents a very compact 16-node
cluster that is the core of a future robot for collecting and
storing massive amounts of sensor data for research on Simul-
taneous Localization and Mapping (SLAM). To the best of our
knowledge, this is the ﬁrst time that such a cluster is used in
robotics. We ﬁrst present the requirements and different options
for computing of such a robot and then show the hardware
and software of our solution in detail. The cluster consists of
16 nodes of AMD Ryzen 7 5700U CPUs with a total of 128
cores. As a system that is to be used on a Clearpath Husky
robot, it is very small in size, can be operated from battery
power and has all required power and networking components
integrated. Stress tests on the completed cluster show that it
performs well.

Index Terms- Robotics, Computing, Cluster, Sensor

Data

I. INTRODUCTION

Localization and Mapping, often used together in a Si-
multaneous Localization and Mapping (SLAM) system, are
essential components for many mobile autonomous robotic
systems [1]. For the development and testing of SLAM
algorithms it is essential to use datasets. Public datasets
enable researchers to objectively compare their approaches.
Recent published datasets include [2] and [3], while [4]
includes a survey of various robotic datasets for SLAM.

SLAM can be done using different sensors, for example
2D and 3D LiDARs, RGB and RGB-D depth cameras,
event cameras, ultrasound or radar. Often sensor-fusion is
used with two or more sensors, like stereo vision, non-
overlapping sensor setups or the inclusion of additional data
from wheel odometry, Inertial Measurement Units (IMUs),
GPS or WiFi localization. This leads to a fragmentation of
the benchmarking, since datasets typically provide only a
subset of said sensor conﬁgurations and researchers are thus
often forced to use their own datasets for benchmarking,
since public datasets do not fulﬁll the data requirements of
their SLAM system.

Some of the aforementioned sensors can provide massive
amounts of data. This is especially true for RGB, RGB-D
and event cameras, which often could provide more than
600MBps (USB 3.0 speed). For example the cameras used
in [4] (GS3-U3-51S5C-C) can provide 2,448 x 2,048 pixel
at up to 75Hz, which corresponds to 1.05GBps of 8-bit
RGB data after debayering. Consequently, datasets typically
reduce resolution or frame rate to avoid huge datasets. But
is conceivable that future robotic systems may utilize
it
specialized data processing processors that can handle such

1All authors are with the School of

Information Science and
Technology, ShanghaiTech University, China. [yangyy2, fengdl,
soerensch]@shanghaitech.edu.cn

Fig. 1. The cluster (the silver and black box in the center of the robot)
mounted on an early mockup of the ShanghaiTech Mapping Robot II.

huge amounts of data - if it turns out to be advantageous for
SLAM or another application (e.g. computer vision).

It

is thus desirable to collect datasets with as high
properties as possible (e.g. frame rate and resolution) and
with as many different sensors in different conﬁgurations
simultaneously, to enable the development and comparison
of present and future SLAM algorithms. Building robots
to collect such datasets is a challenging task. In [4] we
presented a system that used 9 cameras (stereo: front, left,
right, up; monocular: back), two Velodyne HDL-32E 3D
LiDARs and an IMU. The robot used a single Intel Core
i7-6770k CPU and additional USB 3.0 cards to connect the
cameras, but was only able to collect the full resolution
camera data at 10 Hz due to CPU limitations.

We are now in the process of building the ShanghaiTech
Mapping Robot II, which should feature many more sensors
(e.g. 11 cameras: additionally stereo back and monocular
down, all at 60 frames per second; three event cameras;
stereo infrared cameras; an omni-directional camera; 5 RGB-
D cameras; 4 high-resolution 3D LiDARs, as well as sonar &
radar sensors). Fig. 1 shows a mockup of that robot with just
a few sensors and our cluster at its center. The computational
needs for this robot are massive: there should be enough
high-bandwidth I/O ports (USB 3.0 and Gigabit Ethernet);
enough CPU processing power to compress images to high-
quality JPEG images and enough bandwidth and size for

ﬁle storage on SSDs. We did not opt to utilize specialized
hardware for video capture and encoding like 1, because we
prefer to keep the system general and capable of collecting
data from all kind of current and future sensors.

Computation for robotics often includes special hardware
like FPGAs [5] or GPUs [6], but there is scarce literature on
the topic of on-board high-performance CPU computing for
robotics. In [7] a theoretical foundation on using a cluster
for robotics is presented and experiments are conducted
with a toy experiment consisting of four Raspberry PI 3
boards. The authors further explore this topic in [8] where
ubiquitous supercomputing for robotics is explored,
that
includes distributed computing over several robots and the
cloud. Computation ofﬂoading [9] and cloud computing for
robotic are hot topics [10], but not applicable for the high
I/O demand scenario presented here.

In [11] another small system of 5 ODROID-x2 nodes for

computation on a robotic blimp is presented.

Multi-processor approaches to speciﬁc robotic systems
have been proposed early on, for example 1992 on the
problem of inverse dynamics [12], but such papers are often
theoretic/ algorithm oriented, don’t present concrete systems
and usually assume a uniﬁed memory and not a cluster
system.

Willow Garage’s PR2 robot had two high-performance
computers (2 × Intel i7 Xeon quad-core processors, 24 GB
RAM each), of which the second is network booting from the
ﬁrst 2 - in so far one could speak of a very small cluster. The
Robot Operating System (ROS) [13] developed by Willow
Garage natively supports multi-node messaging, a fact that is
heavily exploited in the software architecture of this project.
An article on ROS 2 and Kubernetes Basics 3 explores how
to distribute ROS 2 compute tasks across multiple machines
with Kubernetes, in the example using three nodes. Actually,
we are considering following this software design using ROS
2 and Kubernetes in the future.

Multi-node computing is more prevalent in autonomous
driving, presumably because of the availability of more
power and space. E.g. [14] report on a multi-node system and
NVIDIA is supporting distributed multi-node autonomous
vehicle AI training with the NVIDIA DGX Robotic Drive
systems 4.

This paper describes the requirements, options and ﬁnal
solution for the computation needs of the ShanghaiTech
Mapping Robot II. We believe that our approach of using a
cluster for robotics with more than just a few nodes has never
been used or reported in literature. Section II presents the
computing requirements for the mapping robot and different
options. Section III describes the hardware of the 16-node

1http://www.magewell.com/products/

eco-capture-hdmi-4k-m2 or
https://buy.advantech.com/Boards-Cards/
Video-Cards/Indus_Video_Sulotion.products.htm
2https://www.clearpathrobotics.com/assets/

downloads/pr2/pr2_manual_r321.pdf

cluster that we designed for our wheeled data collection
robot while Section IV presents its software stack. Section
V concludes this paper.

II. COMPUTING REQUIREMENTS AND OPTIONS

The requirements for the computing solution for the
ShanghaiTech Mapping Robot II are dictated by the robot
and the planned sensor suite, but should also leave room for
future upgrades and changes. Physically the system has to ﬁt
in the payload bay of the Clearpath Husky robot, with a width
and length of 380 mm and 290 mm, respectively. The height
should be moderate, because several 360 degree ﬁeld-of-view
sensors (omni-camera, 3D LiDAR) have to be placed above
the computing solution. The power consumption should not
be too high for battery powered operation, not much higher
than 1kW. The main computation requirements are:

1) I/O: receive data at full USB 3.0 bandwidth of 20
devices (11 RGB cameras, 1 omni camera, 3 event
cameras, 5 RGB-D cameras) and several other high-
bandwidth Gigabit Ethernet devices (4 3D LiDARs, 2
infrared cameras)

2) CPU power: ability to debayer and JPEG compress
(quality 90) at least 11 streams of 5MP @ 60Hz frame
rate, while handling the I/O of the other data.

3) Storage: have enough bandwidth and size to store the
data (e.g. 9 full bandwidth USB 3.0 devices (9 x 600
MBps) + compressed camera (11 x 150 MBps) + other
devices (6 x 100MBps) ≈ 7.6 GBps ≈ 27 TB per hour)
Given the available battery, mapping runs can be limited
to one hour, requiring about 30TB of disk space at full specs.
While a size of 30 TB for a robot dataset may seem daunting,
it can be stored in an Amazon S3 Glacier instance for about
120 USD per month. A download with a Gigabit Internet
connection can be achieved in less than 3 days, incurring a
cost of about 80 USD from Amazon.

A. Computing Options

There were several options under consideration to fulﬁll
the above computation requirements: A) a single high-power
CPU node with multiple USB 3.0 I/O cards and a big RAID
of SSD disks; B) four workstation PCs, each with USB 3.0
I/O cards and a RAID of SSD disks; C) a cluster of mini PC
boards, utilizing their on-board USB controllers, each with a
single SSD. For 32TB of storage Option A) would utilize a
RAID of 8 4TB SSDs, Option B) would use two 4TB SSDs
per node while Option C) can use one 2TB SSD per node.
Table I shows some speciﬁcations of the different computing
nodes and some of the theoretical cluster speciﬁcations.
It should be noted that the TDP (thermal design power)
listed is just for the CPU, while other components may
also draw signiﬁcant power (e.g. board, main memory, SSD,
fans), potentially more than doubling the TDP value for the
consumption of the whole system.

In order to compare the computation performance of
the CPUs we utilized public results from the well-known
cross-platform benchmark Geekbench 5. Benchmarking is
a complex problem and all kinds of factors (e.g. cooling,

3https://ubuntu.com/blog/exploring-ros-2-with-kubernetes
4https://developer.nvidia.com/blog/

validating-distributed-multi-node-av-ai-training-with-dgx-systems-on-openshift-with-dxc-robotic-drive/

TABLE I
COMPUTER OPTIONS COMPARISON

Motherboard

CPU

CPU TDP

Cores / Threads
One-Node option ( 1 × )

Size

I/O

ASROCK TRX40 Creator

AMD Ryzen
Threadripper 3970X

280W

32 / 64

305 × 244 mm

ASROCK Z590 Taichi

Intel Core i9-11900

65W
(260W)

8 / 16
( 32 / 64 )

300 × 240 mm

Four-Node Options ( 4 × )

ZOTAC ZBOX MI574

Intel Core i7-9700

65W
(260W)

8 / 16
( 32 / 64 )

185 × 185 mm

ASUS Mini PN51
(used in this project)

AMD Ryzen7 5700U

15W
(240W)

8 / 16
( 128 / 256 )

115 × 115 mm

16-Node Options ( 16 × )

ASRock 4X4-4800U

AMD Ryzen 7 4800U

15-25W
(240-400W)

8 / 16
( 128 / 256 )

4 × 4
(104 × 102 mm)

ASRock 4X4-V2000M

AMD Ryzen
Embedded V2718

10-25W
(160-400W)

8 / 16
( 128 / 256 )

4 × 4
(104 × 102 mm)

NUC11 PAHi7

Intel Core i7-1165G7

12-28W
(192-448W)

4 / 8
( 64 / 128 )

117×112 mm

GIGABYTE/BSI7-1165G7

Intel Core i7-1165G7

12-28W
(192-448W)

4 / 8
( 64 / 128 )

196 × 140 mm

4 USB 3.2 Gen2
8 USB 3.2 Gen1
2 Gigabit+ Ethernet
3 M.2
8 SATA3
3 x PCIe 4-port USB 3.0 cards
1 x PCIe 4-port Ethernet card

2 (8) Thunderbolt 4 USB
6 (24) USB 3.2
2 (4)a Gigabit+ Ethernet
3 (12) M.2
8 (32) SATA3
1 (4) 2-port (8) PCIe Ethernet card
1 (4) USB 3.1 Type-C
6 (24) USB3.0
2 (4)a Gigabit+ Ethernet
2 (8) M.2
1 (4) SATA33
1 (4) 2-port (8) PCIe Ethernet card

2 (32) USB 3.2 Gen2 Type C
3 (48) USB 3.1 Gen1
1 (0)a Gigabit Ethernet
1 (16) M.2
1 (16) SATA3
1 (16) 1-port (16) M.2 Ethernet card
4 (64) USB 3.2 Gen2
4 (64) USB 2.0
2 (16)a Gigabit Ethernet
1 (16) M.2
1 (16) SATA3
2 (32) USB 3.2 Gen2
4 (64) USB 2.0
2 (16)a Gigabit Ethernet
1 (16) M.2
1 (16) SATA3
2 (32) Thunderbolt 4
3 (48) USB 3.1
1 (0)a Gigabit Ethernet
1 (16) M.2 2280
2 (32) SATA3
1 (16) Thunderbolt 4
6 (96) USB 3.2
2 (16)a Gigabit Ethernet
2 (32) M.2
1 (16) SATA3

aIn a cluster setup one Ethernet port per node is used by the cluster and thus not available for connecting sensors.

conﬁguration, operating system) play a role here, but nev-
ertheless the results published on their website allow for
a rough comparison of the processor speeds. The software
measures single-threaded and multi-threaded performance
with multiple algorithms and analyzes and scores the op-
erating performance of the processor in an all-round way.
The benchmark results are shown in Table II.

B. Computing Options Tests & Discussion

In the following the advantages and disadvantages of the
different options are discussed and our ﬁnal selection is
justiﬁed.

1) AMD Ryzen Threadripper 3970X: Using a single, very
powerful computer was our initial approach, as it allows

for a fairly quick and easy integration into the robot (even
though water-cooling may be advised) and also eases the
use of the robot, having to operate only one computer. An
initial test in our lab with two 4-port USB 3.1 PCIe 4-lane
cards (StarTech PEXUSB3S44V) relieved that using more
than 8 GS3-U3-51S5C-C cameras results in frame drops and
disconnected cameras, using the capture tool provided by the
camera manufacturer or ROS, regardless of the combinations
of USB ports used from the motherboard or the PCIe cards.
This is of course unacceptable for use in our robot. This
option would also severely limit a possible future extension
of the requirements, as the I/O and storage system are maxed
out in this setup already.

TABLE II
GEEKBENCH 5 SCORES

CPU
AMD Ryzen Threadripper 3970X
Intel Core i9-11900
Intel Core i7-9700
AMD Ryzen 7 5700Ua
AMD Ryzen 7 4800U
AMD Ryzen Embedded V2718
Intel Core i7-1165G7

aUsed in this project.

Single-Core Multi-Core (Cluster)

1,264
1,549
1,198
1,044
1,029
1,165
1,398

24,183
6,539 (26,156)
6,397 (25,588)
5,526 (88,416)
5,905 (94,408)
6,856 (109,696)
4,573 (73,168)

2) Four-Node Options: Our lab already has experience
with such a setup, as we operate a big box with four such
boards next to each other to easily add computation and
sensors to cars for research on smart cars and autonomous
driving. For the mapping robot four nodes could be mounted
vertically, on top of each other. So the size and also the power
requirements outlined earlier are met. The I/O requirements
can also be met (24 USB3.2 ports are available over the 4
systems) when adding a PCIe card for additional Ethernet
ports.

A considerable advantage of these boards (as well as the
Threadripper above) is that they can be easily extended using
the PCIe slots, for example with dedicated cards for some
specialized sensors or with FPGAs or GPUs to enhance
computation or encoding performance.

A problem for this option is the low overall CPU per-
formance. As mentioned in the requirements, the data from
11 cameras should be debayered and JPEG compressed @
5MP @ 60 Hz. Besides handling all the other I/O, each node
would have to process three such streams. We do not have
such CPUs in our lab, but in the boxes mentioned about we
utilize 6-core i7 processors. These are able to handle two
such streams with the standard ROS drivers - with almost
full CPU load. We are working on writing optimized storage
nodes, that, for example, also should speed things up by
utilizing libjpeg-turbo5, but having three such streams on the
8-core Intel CPUs is pushing the boundaries, especially since
the four PCs also have to handle all the other I/O. The 16-
Node option offers more performance and also offers CPU
and I/O performance for future extensions.

The ZOTAC ZBOX Mi574 is smaller than the ASROCK
Z590 motherboard, but has a slower CPU and offers fewer
extension ports.

3) 16-Node Options: The requirements state that 11 RGB
cameras need to be compressed, and additional 9 full-
bandwidth USB 3.0 devices plus 6 high-bandwidth Ethernet
sensors need to be handled. With 16 nodes we can assign
1 (or 0) RGB cameras with JPEG compression to each
node, plus either another USB 3.0 device or high-bandwidth
Ethernet sensor. Following this setup already leaves capacity
for 6 more devices, while our tests showed that the node

5This library claims to be 2-6 times faster in JPEG encoding, compared
to libjpeg: https://libjpeg-turbo.org/; While ROS is utilizing
libjpeg-turbo through OpenCV, its actual turbo (SIMD) is deactivated at the
time of the writing of this article: https://github.com/opencv/
opencv/issues/12115.

we selected can actually comfortably handle an additional
full-bandwidth device (with or without JPEG compression).
The benchmark also shows that the 16-Node Options have
far superior overall multi-core performance. This allows for
alternative use cases, for example for high-performance com-
putations for live SLAM from a number of high-bandwidth
sensors. This would permit for mapping runs that do not
store all raw data on the disks (even though, depending on
the actual CPU load, battery changes every 1 or 2 hours will
be required).

For all options it is quite difﬁcult to get the data off the
cluster. While a download of assumed 30TB over the cluster
network will take about 2.8 days, attaching an HDD (which
are available in sizes of up to 18TB as of 2021) to each of
the 16 nodes will reduce the time to about 130 minutes.

4) ASRock 4×4: ASRock 4×4-4800U is a motherboard
released in 2020. It is equipped with a Ryzen 7 4800U
processor. The base frequency is 1.8GHz, the acceleration
frequency is 4.2GHz. One of its advantages is that it has 2
wired network ports. Additionally, using a sample board we
purchased, we found that it is possible to turn the CPU fan by
180 degree. The plan was to do this for all boards, such that
the power connection and ethernet ports are all inside, while
still blowing the hot air outside. This would have allowed
a cleaner power-connection setup compared to the solution
presented below.

The ASRock 4×4 motherboard series also has a board
named ASRock 4×4-V2000M which is very similar to 4×4-
4800U, except for the processor. 4×4-V2000M is an indus-
trial grade product, its production cycle (7-15 years) is longer
than 4×4-4800U (1-2 years).

Both these CPUs are faster (and more expensive) than
the AMD Ryzen 7 5700U that we ultimately chose for
this project, such that, together with the other advantages
mentioned above, these were our clear favorite options. But
our vendors were unable to provide the 16 units needed for
the cluster, such that we had to give up on choosing them.
5) NUC11 PAHi7: NUC11 PAHi7 was released in 2021.
It is equipped with an Intel Core i7-1165G7 processor. The
CPU frequency is 2.8 GHz, and the highest turbo frequency
reaches 4.7 GHz. It has two Thunderbolt interfaces, but only
one Ethernet port. The CPUs multi-thread performance is
signiﬁcantly lower than the Ryzen 7 boards (manly due to
having only half the number of cores).

Another mini computer is also equipped with the Intel
Core i7-1165G7 processor: GIGABYTE/BSI7-1165G7. It has
two M.2 interfaces for SSD and one Thunderbolt interfaces.
But the size is quite large compared with the other models.
6) ASUS Mini PN51: The ASUS Mini PN51 is a mini
computer released in 2021. It is equipped with a Ryzen 7
5700U processor with a 7nm manufacturing process. The
base and turbo frequencies are 1.8GHz and 4.3GHz, and the
CPU power consumption is only 15W. A disadvantage of this
board is, that it only has one Ethernet port, so we replaced
the WiFi M2 card (which we do not need) with an Ethernet
card. This works but makes the space inside the cluster more
crowded.

Fig. 2.
boards each.

The 16 nodes of the cluster, organized in four columns of four

Overall we selected this as the best available option: 16
Ryzen 7 5700U CPUs, with 32GB RAM each (512GB total),
a fast 2TB M.2 SSD (Samsung 970EVO Plus 2TB (each
1,700 MBps sustained sequential write performance6, almost
enough to store 3 full-bandwidth USB 3.0 data streams
(3×600MBps)) and the M.2 Ethernet card.

For this option we actually had to purchase the complete
mini PCs, from which we then extracted the board and
upgraded its RAM and SSD. The cost was about 750
USD per PC and 700 USD for the upgrades, such that the
total price of the cluster, including the costs for the other
components (DC/DC etc.), is about 25,000 USD.

III. CLUSTER HARDWARE

The 16 nodes of our cluster are depicted in Figure 2. They
are organized in four columns of four nodes each. The back-
side of the board with the power connector, the LAN port,
two USB 3.1 ports, one USB-C 3.2 port, an HDMI connector
as well as the fan are pointing to the outside, while one USB
3.1 and one USB-C 3.2 port are located inside the cluster
housing. These could be made available via extension cables,
if needed.

This setup is not sufﬁcient to run the cluster. Additionally
needed is a stable and protective housing, that also provides
proper cooling. Furthermore is a power distribution system
needed as well as networking. Similar to the system de-
scribed in [4], the mapping robot will also require hardware
sensor synchronization. We will again utilize an Asus Tinker
Board for this, as is provides real-time signaling as well as
sufﬁcient CPU speed to run the needed ROS nodes. This
board will also installed in the housing (on the top plate)
and can be seen as node 17, even though it is running its
own operating system and not using netboot.

6https://www.tomshardware.com/reviews/

samsung-970-evo-plus-ssd,5608.html

Fig. 3.
The CAD design of the cluster housing with the air-ﬂow plan.
Outside air is sucked into the housing by two times four 6-cm fans on the
top and warm air is pushed out by each node and a total of four 6cm fans
at the bottom, for cooling the DC/ DC converters and the network switch.

1) Mechanical and Airﬂow Design: Figure 3 shows the
airﬂow design for the cluster. Eight 6cm fans are blowing
outside air into the housing, while each node has a CPU fan
blowing air outside. Additionally, four 6cm fans blow air out
of the bottom compartment of the housing for cooling the
DC/DC converters and the 24-port network switch.

The housing itself is made from 7mm thick black acrylic
glass (PMMA: Polymethyl Methacrylate) that is laser-cut
from the CAD design. Six 20×20mm aluminum proﬁle bars
connect the walls with the bottom, middle and top plate. The
front covers are CNC’ed from 1mm aluminum sheets. Fig.
4 shows the housing design of the cluster.

2) Power Distribution: The top plate has two pairs of
Anderson Powerpole connectors to connect batteries or
AC/ DC power supplies. Each connector goes to a Diode
(85HF10) - this is to prevent batteries charging each other.
The idea behind the two power connectors is, that the user
can temporarily connect a second battery while changing the
main robot battery, thus avoiding a shutdown of the cluster.
Cabling is done with 15mm2 copper cabling. Both inputs
are connected to the circuit breaker (C25) which may trip
for currents higher than 125 A. The power monitor attached
to the top plate uses a big FL shunt that is connected to
the circuit. Two wires then connect the top plate with the
bottom compartment. Fig. 4 shows the wiring in the bottom
compartment.

The ground is distributed to all consumers via one big 24-
port distribution block. The battery power is directly fed to
the network switch and to the ﬁve DC/ DC converters. For
powering the nodes four DC/ DC converters (input: 22V -
40V; output: 19V 20A max) are used, each powering four
nodes. A 10,000 uF capacitor is added to each of the four
output circuits (without it nodes on the same DC/ DC crash
if one node is plugged in). An additional DC/ DC with 12V
output is powering the housing fans.

3) Middle Compartment: The 16 nodes are mounted in
four columns on a plate with a big hole (Fig. 2). Each column
is held by four rods with M4 threading and metal spacers
(3cm at the bottom, 4cm between nodes). Plastic washers
protect the boards from the spacers (which are a little too
thick and may touch components on the board). The external
RJ-45 Ethernet sockets are mounted on four thin aluminum
sheets that are screwed in the middle plate and connected to
a rod at the top.

The original mini PCs used a small PCB for two indication
LEDs (power and HDD usage) and the button, connected
with a short cable. We purchased 55cm cables with the ap-
propriate connectors on both ends (SH1mm 6P) and mounted
these small PCBs on the top board. We organized the four
cables for each column of nodes with cable mesh.

4) Assembly: The assembly of the cluster starts with the
mechanical construction of the housing (Fig. 4) and the in-
stallation of the electrical system of the bottom compartment.
In parallel the middle compartment with the 16 nodes can
be build (Fig. 2). The electrical components of the top plate
can also be installed on it in parallel.

Then the middle compartment is placed in the cluster
housing. The power connectors of each node are fed through
the hole of the middle compartment, as are the two power
cables that go to the top plate. The 16 network cables
that come from the nodes,
the six cables from the top
plate and an additional cable that will be connected to the
synchronization node are all also fed through the hole to the
bottom compartment and then connected to the industrial
24-port Gigabit Ethernet switch (OAM-600-65-4GX24GT).
In order for the connection of the cable to the switch to be
possible, the network cables have to be long enough to reach
a little outside of the housing. A problem occurred after that:
we used Cat6 network cables. These are quite stiff, so the
switch could not be moved completely into the housing after
connecting all 23 cables. We thus opted to have it stick out
of the bottom compartment a bit, since this doesn’t effect the
operation of the cluster.

The two power cables from the bottom need to be con-
nected to the top plate. The design of the holder for the 16
small PCBs with the power buttons allows us to ﬁrst connect
all cables to the PCBs and then mount the holder to the top
plate. The ﬁnal cluster is depicted in Fig. 5.

The CAD drawings, a part list as well as a video showing

some of the assembly process are available7.

Table III shows a summary of the hardware speciﬁcations

of the cluster.

IV. CLUSTER SOFTWARE STACK

For the cluster we chose to use Ubuntu 20.04 LTS as
operating system with ROS Noetic Ninjemys as the robot
operating system. Maintaining 16 of these installations is a
hassle, so we opted to use network booting. For that we
dedicate node 1 as the server and the 15 other nodes as

7https://robotics.shanghaitech.edu.cn/cluster_on_

wheels

Fig. 4.
The cluster housing. The bottom compartment is for power and
networking, where we can see the four 19V DC/ DC converters powering
the nodes (in the back in two columns of two devices), one 12V DC/ DC
for the fans (front) and the 24-port switch that will also go to the bottom.
The 16 power plugs for the nodes are connected to one of the four white
power distributors and the big central common ground distributor. Two high-
ampere cables (and a 12V cable) go to the top to be connected to the top
plate to receive the input battery power.

Fig. 5. The complete cluster. The top plate has several functions: For the
power there are two pairs of power pole connectors, a circuit breaker and
a display for the voltage, current, power and power used as well as power
buttons and LEDs for all 16 nodes. Furthermore, there are six Ethernet RJ-
45 sockets connected to the network switch. For the synchronization node
that is to be added later there are already two USB sockets, a HDMI socket
and a power button with LED as well as a 10cm × 10cm plate/ hole that
will provide the connectors for the synchronization signals.

Fig. 6. The software stack of the Cluster for the ShanghaiTech Mapping Robot II.

TABLE III

CLUSTER SPECIFICATIONS

Item
Dimensions Housing (w x d x h)
Weight
DC Power In
Power Consumption “Just Fans”/ Idle
Power Consumption Full Load (Stress)
Node CPUs
Cores/ Threads
RAM
Storage
Storage write speed
Cost

Value
288 x 270 x 400 mm
15kg
22 - 40 V
50/ 130 W
750 W
16 × AMD Ryzen 7 5700U
128 / 256
512GB
32TB
1.7GBps x 16 ≈ 27GBps
approx. 25,000 USD

initial kernel includes a NFS client, so we use NFS to provide
all other operating system ﬁles.

B. NFS

The network ﬁle system (NFS) is used to share a ﬁle
system over the network. On the server debootstrap was
used to create a basic Ubuntu 20.04 ﬁle system in the folder
/clusternfs. This system is shared via NFS with the nodes,
who mount it as their root ﬁle system. It is shared as ”read
only” to avoid various complications from multiple nodes
writing to the same ﬁle system. Instead some crucial folders
(/tmp, /var/log, /var/tmp) in the nodes are mounted as a
temporary ﬁle system (tmpfs), for which the writes are saved
in memory and are lost upon reboot.

clients. Fig. 6 gives an overview of the software stack of our
cluster. In the following software stack is described in detail.

C. Client Setup

A. Boot on LAN

Boot on LAN is a technique that lets a computer receive
all ﬁles needed to start the operating system over the network
instead of a local disk. This must be supported by the BIOS
and the network card and properly conﬁgured in the BIOS.
So we ﬁrst must ensure that all client nodes have been set up
to support the preboot execution environment (PXE), which
is usually set on basic input output system (BIOS). The
network hardware MAC addresses of all client nodes should
be obtained.

After installing Ubuntu 20.04 on the server (node 1) a

number of services must be installed and conﬁgured.

To build the cluster, it is necessary to use the dynamic
host conﬁguration protocol (DHCP) to assign IP addresses
to nodes based on the MAC address that we previously noted.
Upon booting another node of the cluster, its BIOS will
activate the network interface and receive its IP address via
DHCP. It will then load the kernel of the cluster OS via
TFTP. The trivial ﬁle transfer protocol (TFTP) is used for
simple ﬁle transfers between a client and a server. After
conﬁguration of TFTP some necessary ﬁles need to be
prepared on the server for booting the client: kernel image, an
initrd to go with it, a PXELINUX image, and a conﬁguration
ﬁle to go with the PXELINUX image.

The initial kernel is transferred via TFTP, but the rest of
the operating system needs a more sophisticated method. The

All clients mount their SSD as the folder /disk. This is
where, in the end, the collected sensor data will be stored.
The /disk folder is shared via NFS by all the nodes and
mounted by the server. This is mainly to be able to easily
check the collected sensor data from the server.

New software can be installed for the clients by logging
into the server and there changing into the cluster ﬁle
system on /clusternfs via the command chroot. Through this
ROS and the node management tools described below are
installed.

D. Nodes Management

To comfortably operate the robot with its many sensors
and ROS nodes, which are distributed over 16 cluster nodes,
a graphical user interface (GUI) to start and stop the ROS
nodes is needed. For that we extended a node management
tool called app manager to work with multiple servers. It
has two parts: app client and app manager. The app client
realizes an GUI which makes it easy to control and manage
the ROS nodes of client nodes. The app manager realizes
start ROS nodes of client from master through LAN.

E. Cluster Monitoring

In order to ensure that the system runs stably, a good way
to monitor the many nodes is needed. The monitoring tool
called Ganglia can monitor a large number of nodes and
only takes up very little CPU resources. It is mainly used to
monitor system performance, such as: CPU, memory, hard

……Node management tool: app managerNode management tool: app clientServer: Node1Boot: DHCP, TFTP, NFS configurationUbuntu 20.04ROSCluster Monitor tool: GangliaGmetadGmondGanglia-webROSBase image: Ubuntu 20.04 Cluster Monitor tool: GangliaGmondNode2Node3Node16Boot from ServerM.2 SSD:DataSensors:CamerasLidarsIMUGPS……rosbagUSB Ethernet ……Node management tool: app managerNode management tool: app clientServer: Node1Boot: DHCP, TFTP, NFS configurationUbuntu 20.04ROSCluster Monitor tool: GangliaGmetadGmondGanglia-webROSBase image: Ubuntu 20.04 Cluster Monitor tool: GangliaGmondNode2Node3Node16Boot from ServerM.2 SSD:DataSensors:CamerasLidarsIMUGPS……rosbagUSB Ethernet when they receive AC power, since the server node 1 must
be up and running ﬁrst.

We may also opt to use more ﬂimsy Ethernet cables in the
cluster, in order to be able to fully insert the network switch
to the housing. It may be possible to use unshielded cables,
since the distances are very short.

The next steps for us are the integration of the sensor
synchronization node into the top plate and the actual con-
struction of the ShanghaiTech Mapping Robot II.

REFERENCES

[1] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
I. Reid, and J. J. Leonard, “Past, present, and future of simultaneous
localization and mapping: Toward the robust-perception age,” IEEE
Transactions on robotics, vol. 32, no. 6, pp. 1309–1332, 2016.
[2] X. Shi, D. Li, P. Zhao, Q. Tian, Y. Tian, Q. Long, C. Zhu, J. Song,
F. Qiao, L. Song et al., “Are we ready for service robots? the
openloris-scene datasets for lifelong slam,” in 2020 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2020, pp.
3139–3145.

[3] W. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu, A. Kapoor,
and S. Scherer, “Tartanair: A dataset to push the limits of visual slam,”
in 2020 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS).

IEEE, 2020, pp. 4909–4916.
[4] H. Chen, Z. Yang, X. Zhao, G. Weng, H. Wan, J. Luo, X. Ye, Z. Zhao,
Z. He, Y. Shen et al., “Advanced mapping robot and high-resolution
dataset,” Robotics and Autonomous Systems, vol. 131, p. 103559, 2020.
[5] Z. Wan, B. Yu, T. Y. Li, J. Tang, Y. Zhu, Y. Wang, A. Raychowdhury,
and S. Liu, “A survey of fpga-based robotic computing,” IEEE Circuits
and Systems Magazine, vol. 21, no. 2, pp. 48–74, 2021.

[6] D. Qiu, S. May, and A. N¨uchter, “Gpu-accelerated nearest neighbor
search for 3d registration,” in International conference on computer
vision systems. Springer, 2009, pp. 194–203.

[7] L. Camargo-Forero, P. Royo, and X. Prats, “Towards high performance
robotic computing,” Robotics and Autonomous Systems, vol. 107, pp.
167–181, 2018.

[8] ——, “The archade: ubiquitous supercomputing for robotics. part i:
philosophy,” Robotics and Autonomous Systems, vol. 114, pp. 187–
198, 2019.

[9] K. Kumar, J. Liu, Y.-H. Lu, and B. Bhargava, “A survey of computation
ofﬂoading for mobile systems,” Mobile networks and Applications,
vol. 18, no. 1, pp. 129–140, 2013.

[10] B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, “A survey of research
on cloud robotics and automation,” IEEE Transactions on automation
science and engineering, vol. 12, no. 2, pp. 398–409, 2015.

[11] C. G. Ribeiro, M. S. Dutra, A. Rabelo, F. Oliveira, A. Barbosa,
L. Frinhani, D. Porto, and R. Milanez, “A robotic ﬂying crane
controlled by an embedded computer cluster,” in 2015 12th Latin
American Robotics Symposium and 2015 3rd Brazilian Symposium
on Robotics (LARS-SBR).

IEEE, 2015, pp. 91–96.

[12] A. Fijany, Parallel Computation Systems for Robotics: Algorithms and

Architectures. World Scientiﬁc, 1992, vol. 2.

[13] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, A. Y. Ng et al., “Ros: an open-source robot operating
system,” in ICRA workshop on open source software, vol. 3, no. 3.2.
Kobe, Japan, 2009, p. 5.

[14] J. Wei, J. M. Snider, J. Kim, J. M. Dolan, R. Rajkumar, and B. Litk-
ouhi, “Towards a viable autonomous driving research platform,” in
2013 IEEE Intelligent Vehicles Symposium (IV).
IEEE, 2013, pp.
763–770.

Fig. 7. One hour CPU load graph from the ganglia web interface from
the two day stress test of the system.

disk utilization, I/O load, network trafﬁc, etc. It is easy to see
the working status of each node through the diagrams. The
core of Ganglia includes gmond, gmetad and a web front
end: Ganglia-web. Gmond is run on the agent side, mainly
used to collect the performance status of each node. Gmetad
is run on the server side and collects and stores the original
data from gmond. Ganglia-web is a web service, which reads
the data stored by gmetad for web display.

F. Experiments

To stress test the cluster system we utilized the Ubuntu
program stress, which tests a computer by producing a high
CPU, disk and memory load - similar to what we except
when recording sensor data with the system. The power
values from Table III come from this test.

We also attempted a Linpack run of the cluster. One test
ran for two days - Fig. 7 shows a the CPU load graph
of Ganglia from that
test. We utilized OpenMPI, AMD
Optimizing CPU Libraries (AOCL) with a BLAS (Basic
Linear Algebra Subprograms) component called BLIS and
Linpack 2.3 for this. Unfortunately, due to our inexperience
with Linpack, we were not able to ﬁnd a conﬁguration/ setup
that resulted in meaningful results for the processing speed
of the system.

V. CONCLUSIONS

In order to collect robotic datasets for SLAM research
we are building a massive data collection robot. This robots
needs a very big amount of I/O and processing power to
collect, compress and store all the data in real time. After
analyzing the requirements for the computing system of the
robot and comparing several options, the actual solution, a
16-node cluster system, is presented. The hardware setup,
including housing, DC power, networking, etc. is described
and the software stack with booting from LAN and the app
manager is presented. The working cluster was stress tested
and found to be fully working.

Several improvements are planned on the cluster: Power
on LAN is currently not functional - due to problems in the
BIOS of the nodes. Hopefully this will be ﬁxed with future
BIOS updates. So currently nodes need to be started by hand
using the buttons, as it is not an option to power on the nodes

