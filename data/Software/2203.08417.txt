On the Use of Fine-grained Vulnerable Code Statements for
Software Vulnerability Assessment Models

Triet Huynh Minh Le
CREST - The Centre for Research on Engineering
Software Technologies, The University of Adelaide
Adelaide, Australia
triet.h.le@adelaide.edu.au

ABSTRACT
Many studies have developed Machine Learning (ML) approaches to
detect Software Vulnerabilities (SVs) in functions and fine-grained
code statements that cause such SVs. However, there is little work
on leveraging such detection outputs for data-driven SV assessment
to give information about exploitability, impact, and severity of
SVs. The information is important to understand SVs and priori-
tize their fixing. Using large-scale data from 1,782 functions of 429
SVs in 200 real-world projects, we investigate ML models for au-
tomating function-level SV assessment tasks, i.e., predicting seven
Common Vulnerability Scoring System (CVSS) metrics. We partic-
ularly study the value and use of vulnerable statements as inputs
for developing the assessment models because SVs in functions
are originated in these statements. We show that vulnerable state-
ments are 5.8 times smaller in size, yet exhibit 7.5-114.5% stronger
assessment performance (Matthews Correlation Coefficient (MCC))
than non-vulnerable statements. Incorporating context of vulner-
able statements further increases the performance by up to 8.9%
(0.64 MCC and 0.75 F1-Score). Overall, we provide the initial yet
promising ML-based baselines for function-level SV assessment,
paving the way for further research in this direction.

CCS CONCEPTS
• Security and privacy → Software security engineering.

KEYWORDS
Security Vulnerability, Vulnerability Assessment, Machine Learn-
ing, Mining Software Repositories

ACM Reference Format:
Triet Huynh Minh Le and M. Ali Babar. 2022. On the Use of Fine-grained
Vulnerable Code Statements for Software Vulnerability Assessment Models.
In Proceedings of MSR ’22: Proceedings of the 19th International Conference on
Mining Software Repositories (MSR 2022). ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

2
2
0
2

r
a

M
6
1

]
E
S
.
s
c
[

1
v
7
1
4
8
0
.
3
0
2
2
:
v
i
X
r
a

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

M. Ali Babar
CREST - The Centre for Research on Engineering
Software Technologies, The University of Adelaide
Adelaide, Australia
Cyber Security Cooperative Research Centre, Australia
ali.babar@adelaide.edu.au

1 INTRODUCTION
Software Vulnerabilities (SVs) can make software systems suscep-
tible to catastrophic cyber-attacks. Thus, addressing such SVs is a
critical task for developers. However, fixing all SVs at the same time
is not always practical due to limited resources and time [35]. A
common practice in this situation is to prioritize fixing SVs posing
imminent and serious threats to a system of interest. Such prioritiza-
tion usually requires inputs from SV assessment [40], and Common
Vulnerability Scoring System (CVSS) [22] is one of the most popular
SV assessment frameworks. CVSS characterizes SVs using various
metrics including their exploitability, impact, and severity. The met-
rics can then be used to select critical SVs to fix early. For example,
a severe SV that is easily exploitable and has critical impacts on a
system would likely have a high fixing priority.

Previous studies (e.g., [27, 40, 44, 73, 88]) have mostly used SV
reports to develop data-driven models for assigning the CVSS met-
rics to SVs. Among sources of SV reports, National Vulnerability
Database (NVD) [61] has been most commonly used for building
SV assessment models [40]. The popularity of NVD is mainly be-
cause it has SV-specific information (e.g., CVSS metrics) and less
noise in SV descriptions than other Issue Tracking Systems (ITSs)
like Bugzilla [23]. The discrepancy is because NVD reports are vet-
ted by security experts, while ITS reports may be contributed by
users/developers with limited security knowledge [13]. However,
NVD reports are mostly released long after SVs have been fixed.
Our analysis revealed that less than 3% of the SV reports with the
CVSS metrics on NVD had been published before SVs were fixed;
on average, these reports appeared 146 days later than the fixes.
Note that our findings accord with the previous studies [45, 62].
This delay renders the CVSS metrics required for SV assessment
unavailable at fixing time, limiting the adoption of report-level SV
assessment for understanding SVs and prioritizing their fixes.

Instead of using SV reports, an alternative and more straight-
forward way is to directly take (vulnerable) code as input to enable
SV assessment prior to fixing. Once a code function is confirmed
vulnerable, SV assessment models can assign it the CVSS metrics be-
fore the vulnerable code gets fixed, even when its report is not (yet)
available. Note that it is non-trivial to use static application security
testing tools to automatically create bug/SV reports from vulnerable
functions for current SV assessment techniques as these tools often
have too many false positives [2, 32]. To develop function-level as-
sessment models, it is important to obtain input information about
SVs in functions detected by manual debugging or automatic means
like data-driven approaches (e.g., [51, 89, 93]). Notably, recent stud-
ies (e.g., [47, 59]) have shown that an SV in a function usually stems

 
 
 
 
 
 
MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Triet Huynh Minh Le and M. Ali Babar

from a very small number of code statements/lines, namely vulner-
able statements. Intuitively, these vulnerable statements potentially
provide highly relevant information (e.g., causes) for SV assessment
models. Nevertheless, a large number of other (non-vulnerable)
lines in functions, though do not directly contribute to SVs, can still
be useful for SV assessment, e.g., indicating the impacts of an SV on
nearby code. It still remains largely unknown about function-level
SV assessment models as well as the extent to which vulnerable and
non-vulnerable statements are useful as inputs for these models.

We conduct a large-scale study to fill this research gap. We inves-
tigate the usefulness of integrating fine-grained vulnerable state-
ments and different types of code context (relevant/surrounding
code) into learning-based SV assessment models. The assessment
models employ various feature extraction methods and Machine
Learning (ML) classifiers to predict the seven CVSS metrics (Ac-
cess Vector, Access Complexity, Authentication, Confidentiality,
Integrity, Availability, and Severity) for SVs in code functions.

Using 1,782 functions from 429 SVs of 200 real-world projects,
we evaluate the use of vulnerable statements and other lines in func-
tions for developing SV assessment models. Despite being up to 5.8
times smaller in size (lines of code), vulnerable statements are more
effective for function-level SV assessment, i.e., 7.4-114.5% higher
Matthews Correlation Coefficient (MCC) and 5.5-43.6% stronger F1-
Score, than non-vulnerable lines. Moreover, vulnerable statements
with context perform better than vulnerable lines alone. Particularly,
using vulnerable and all the other lines in each function achieves
the best performance of 0.64 MCC (8.9% better) and 0.75 F1-Score
(8.5% better) compared to using only vulnerable statements. We
obtain such improvements when combining vulnerable statements
and context as a single input based on their code order, as well
as when treating them as two separate inputs. Having two inputs
explicitly provides models with the location of vulnerable state-
ments and context for the assessment tasks, while single input does
not. Surprisingly, we do not obtain any significant improvement of
the double-input models over the single-input counterparts. These
results show that function-level SV assessment models can still
be effective even without knowing exactly which statements are
vulnerable. Overall, our findings can inform the practice of building
function-level SV assessment models.

Our key contributions are summarized as follows:

• To the best of our knowledge, we are the first to leverage data-
driven models for automating function-level SV assessment
tasks that enable SV prioritization/planning prior to fixing.
• We study the value of using fine-grained vulnerable state-
ments in functions for building SV assessment models.
• We empirically show the necessity and potential techniques
of incorporating context of vulnerable statements to improve
the assessment performance.

• We release our datasets and models for future research [5].
Paper structure. Section 2 gives a background on function-level
SV assessment. Section 3 introduces and motivates the three RQs.
Section 4 describes the methods used for answering these RQs.
Section 5 presents our empirical results. Section 6 discusses the
findings and threats to validity. Section 7 mentions the related work.
Section 8 concludes the study and suggests future directions.

1 protected String getExecutionPreamble ()
2 {
3
4
5
6
7
8 -
9 +
10
11
12 }

if ( getWorkingDirectoryAsString () == null )
{ return null ;}
String dir = getWorkingDirectoryAsString();
StringBuilder sb = new StringBuilder();
sb.append("cd");
sb . append ( unifyQuotes ( dir ));
sb . append ( quoteOneItem ( dir , false ));
sb.append("&&");
return sb.toString();

Figure 1: A vulnerable function extracted from the fixing
commit b38a1b3 of an SV (CVE-2017-1000487) in the Plexus-
utils project. Notes: Line 8 is vulnerable. Deleted and added
lines are highlighted in red and green, respectively. Blue-
colored code lines affect or are affected by line 8 directly.

2 BACKGROUND AND MOTIVATION
2.1 SV Assessment with CVSS
Software Vulnerability (SV) assessment is an important step in the
SV lifecycle, which determines various characteristics of detected
SVs [71]. Such characteristics support developers to understand
the nature of SVs, which can inform prioritization and remediation
strategies. For example, if an SV can severely damage the confiden-
tiality of a system, e.g., allowing attackers to access/steal sensitive
information, this SV should have a high fixing priority. A fixing
protocol to ensure confidentiality can then be followed, e.g., check-
ing/enforcing privileges to access the affected component/data.

Common Vulnerability Scoring System (CVSS) [22] is one of the
most commonly used frameworks by both researchers and practi-
tioners to perform SV assessment. There are two main versions of
CVSS, namely versions 2 and 3, in which version 3 only came into
effect in 2015. CVSS version 2 is still widely used as many SVs prior
to 2015 can yet pose threats to contemporary systems. For instance,
the SV with CVE-2004-0113 first found in 2004 was exploited in
2018 [24]. Hence, we adopt the assessment metrics of CVSS version
2 as the outputs for the SV assessment models in this study.

CVSS version 2 provides metrics to quantify the three main as-
pects of SVs, namely exploitability, impact, and severity. We focus
on the base metrics because the temporal metrics (e.g., exploit avail-
ability in the wild) and environmental metrics (e.g., potential impact
outside of a system) are unlikely obtainable from project artifacts
(e.g., SV code/reports) alone. Specifically, the base Exploitability
metrics examine the technique (Access Vector) and complexity to
initiate an exploit (Access Complexity) as well as the authentication
requirement (Authentication). The base Impact metrics of CVSS
focus on the system Confidentiality, Integrity, and Availability. The
Exploitation and Impact metrics are used to compute the Severity
of SVs. Severity approximates the criticality of an SV. Nevertheless,
relying solely on Severity may be insufficient because an SV with
medium severity may still have high impacts as it is considerably
complex to be exploited. It is important to assign a high fixing
priority to such an SV as an affected system would face tremendous
risks in case of a successful cyber-attack. Therefore, in this study,
we consider all the base metrics of CVSS version 2, i.e., Access Vec-
tor, Access Complexity, Authentication, Confidentiality, Integrity,
Availability, and Severity, for developing SV assessment models.

2

On the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Figure 2: Methodology used to answer the research questions. Note: The vulnerable function is the one described in Fig. 1.

2.2 SV Assessment in Code Functions
There have been a growing number of studies to detect vulnerable
statements in code functions (e.g., [47, 48, 59]). Fine-grained detec-
tion assumes that not all statements in a function are vulnerable.
We confirmed this assumption; i.e., only 14.7% of the lines in our
curated functions were vulnerable (see section 4.1). However, it is
non-trivial to manually annotate a sufficiently large dataset of vul-
nerable functions and statements for training SV prediction models.
Instead, many studies (e.g., [46, 47, 84]) have automatically obtained
vulnerable statements from modified lines in Vulnerability Fixing
Commits (VFCs) as these lines are presumably removed to fix SVs.
The functions containing such identified statements are considered
vulnerable. Note that VFCs are used as they can be relatively easy to
retrieve from various sources like National Vulnerability Database
(NVD) [61]. An exemplary function and its vulnerable statement
are in Fig. 1. Line 8 “sb.append(unifyQuotes(dir));” is the vul-
nerable statement; this line was replaced with a non-vulnerable
counterpart “sb.append(quoteOneItem(dir, false));” in the
VFC. The replacement was made to properly sanitize the input
(dir), preventing OS command injection.

Despite active research in SV detection, there is little work on
utilizing the output of such detection for SV assessment. Previous
studies (e.g., [40, 44, 73, 74, 88]) have mostly leveraged SV reports,
mainly on NVD, to develop SV assessment models that alleviate
the need for manually defining complex rules for assessing ever-
increasing SVs. However, these SV reports usually appear long after
SV fixing time. For example, the SV fix in Fig. 1 was done 1,533
days before the date it was reported on NVD. In fact, such a delay,
i.e., disclosing SVs after they are fixed, is a recommended practice
so that attackers cannot exploit unpatched SVs to compromise
systems [91]. One may argue that internal bug/SV reports in Issue
Tracking Systems (ITS) such as JIRA [4] or Bugzilla [23] can be
released before SV fixing and have severity levels. However, ITS
severity levels are often for all bug types, not only SVs. These ITSs
also do not readily provide exploitability and impact metrics like
CVSS for SVs, limiting assessment information required for fixing
prioritization. Moreover, SVs are mostly rooted in source code; thus,
it is natural to perform code-based SV assessment. We propose
predicting seven base CVSS metrics after SVs are detected in code
functions to enable thorough and prior-fixing SV assessment. We
do not perform SV assessment for individual lines as for a given

function, like Li et al. [49], we observed that there can be more than
one vulnerable line and nearly all these lines are strongly related
and contribute to the same SV (having the same CVSS metrics).

Vulnerable statements represent the core parts of SVs, but we
posit that other (non-vulnerable) parts of a function may also be
usable for SV assessment. Specifically, non-vulnerable statements
in a vulnerable function are either directly or indirectly related
to the current SV. We use program slicing [85] to define directly
SV-related statements as the lines affect or are affected by the vari-
ables in vulnerable statements. For example, the blue lines in Fig. 1
are directly related to the SV as they define, change, or use the sb
and dir variables in vulnerable line 8. These SV-related statements
can reveal the context/usage of affected variables for analyzing SV
exploitability, impact, and severity. For instance, lines 5-6 denote
that dir is a directory and sb is a string (StringBuilder object),
respectively; line 7 then indicates that a directory change is per-
formed, i.e., the cd command. This sequence of statements suggests
that sb contains a command changing directory. Line 11 returns
the vulnerable command, probably affecting other components. Be-
sides, indirectly SV-related statements, e.g., the black lines in Fig. 1,
are remaining lines in a function excluding vulnerable and directly
SV-related statements. These indirectly SV-related lines may still
provide information about SVs. For example, lines 3-4 in Fig. 1 imply
that there is only a null checking for directory without imposing
any privilege requirement to perform the command, potentially
reducing the complexity of exploiting the SV. It remains unclear to
what extent different types of statements are useful for SV assess-
ment tasks. Therefore, this study aims to unveil the contributions
of these statement types to function-level SV assessment models.

3 RESEARCH QUESTIONS
To demystify the predictive performance of SV assessment mod-
els using vulnerable and other statements in code functions, we
investigate the following three Research Questions (RQs).

RQ1: Are vulnerable code statements more useful than
non-vulnerable counterparts for SV assessment models? Since
vulnerable and non-vulnerable statements are both potentially use-
ful for SV assessment (see section 2.2), RQ1 compares them for
building function-level SV assessment models. RQ1 tests the hy-
pothesis that vulnerable statements directly causing SVs would
provide an advantage in SV assessment performance. The findings

3

Vulnerability-Fixing Commitsprotected String getExecutionPreamble(){if (getWorkingDirectoryAsString() == null){return null;}String dir = getWorkingDirectoryAsString();StringBuilder sb = new StringBuilder();sb.append("cd");-sb.append(unifyQuotes(dir));+ sb.append(quoteOneItem(dir, false));sb.append("&&");return sb.toString();}-sb.append(unifyQuotes(dir));protected String getExecutionPreamble(){if (getWorkingDirectoryAsString() == null){return null;}String dir = getWorkingDirectoryAsString();StringBuilder sb = new StringBuilder();sb.append("cd");sb.append("&&");return sb.toString();}•Program slicing context•Surrounding context•Function contextRQ2: Vulnerable + Context (singleinput) for SVARQ3: Vulnerable + Context(twoinputs) for SVARQ1: Vulnerable vs. Non-vulnerable statements for SVAVulnerable statement(s)Non-vulnerable statement(s)Code changes in functionContextExtractionRepeated 10 timesSV Assessment (SVA)Model BuildingCode FeatureGenerationModel Training/Evaluation123Notes on the models in RQ2 and RQ3:•Singleinput: implicitlocation of SVs•Twoinputs:  explicitlocation of SVsMSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Triet Huynh Minh Le and M. Ali Babar

of RQ1 would also inform the practice of leveraging recent advances
in fine-grained SV detection for function-level SV assessment.

RQ2: To what extent do different types of context of vul-
nerable statements contribute to SV assessment performance?
RQ2 studies the impact of using directly and indirectly SV-related
statements as context for vulnerable statements, as discussed in
section 2.2, on the performance of SV assessment in functions. We
compare the performance of models using different types of con-
text lines (see section 4.2) that have been commonly used in the
literature. RQ2 findings would unveil what types of context (if
any) would be beneficial to use alongside vulnerable statements for
developing function-level SV assessment models.

RQ3: Does separating vulnerable statements and context
to provide explicit location of SVs improve assessment per-
formance? For SV assessment, RQ2 combines vulnerable state-
ments and their context as a single input following their order in
functions, while RQ3 treats these two types of statements as two
separate inputs. Separate inputs explicitly specify which statements
are vulnerable in each function for assessment models. RQ3 results
would give insights into the usefulness of the exact location of
vulnerable statements for function-level SV assessment models.

4 RESEARCH METHODOLOGY
This section presents the experimental setup we used to perform
a large-scale study on function-level SV assessment to support
prioritization of SVs before fixing. We used a computing cluster
with 16 CPU cores and 16GB of RAM to conduct all the experiments.
Workflow overview. Fig. 2 presents the workflow we followed
to develop function-level SV assessment models based on vari-
ous types of code inputs. The workflow has three main steps:
(i) Collection of vulnerable and non-vulnerable statements from
Vulnerability-Fixing Commits (VFCs) (section 4.1), (ii) Context ex-
traction of vulnerable statements (section 4.2), and (iii) Model build-
ing for SV assessment (sections 4.3, 4.4, and 4.5). We start with VFCs
containing code changes used to fix SVs. As discussed in section 2.2,
we consider the deleted (–) lines in each function of VFCs as vul-
nerable statements, while the remaining lines are non-vulnerable
statements. Details of the extracted VFCs and statements are given
in section 4.1. Both vulnerable and non-vulnerable statements are
used by the Context Extraction module (see section 4.2) to obtain
the three types of context with respect to vulnerable statements
that potentially provide additional information for SV assessment.
The extracted statements along with their context enter the Model
Building module. The first step in this module is to extract fixed-
length feature vectors from code inputs/statements (see section 4.3).
Subsequently, such feature vectors are used to train different data-
driven models (see section 4.4) to support automated SV assessment,
i.e., predicting the seven CVSS metrics: Access Vector, Access Com-
plexity, Authentication, Confidentiality, Integrity, Availability, and
Severity. The model training and evaluation are repeated 10 times
to increase the stability of results (see section 4.5).
RQ-wise method. The methods to collect data, extract features
as well as develop and evaluate models in Fig. 2 were utilized for
answering all the Research Questions (RQs) in section 3. RQ1 de-
veloped and compared two types of SV assessment models, namely

models using only vulnerable statements and those using only non-
vulnerable statements. In RQ2, for each of the program slicing,
surrounding, and function context types, we created a single fea-
ture vector by combining the current context and corresponding
vulnerable statements, based on their appearance order in the orig-
inal functions, for model building and performance comparison.
In RQ3, for each context type in RQ2, we extracted two separate
feature vectors, one from vulnerable statements and another one
from the context, and then fed these vectors into SV assessment
models. We compared the two-input approach in RQ3 with the
single-input counterpart in RQ2.

4.1 Data Collection
To develop SV assessment models, we need a large dataset of vul-
nerable functions and statements curated from VFCs, as discussed
in section 2.2. This section describes the collection of such dataset.
VFC identification. We first scraped VFCs from three popular
sources in the literature: NVD [61], GitHub Advisory Database,1
and VulasDB [64], a manually curated VFC dataset. The VFCs had
dates ranging from July 2000 to September 2021. We only selected
VFCs that had the CVSS version 2 metrics as we needed these
metrics for SV assessment. Following the recommendation of [55],
we removed any VFCs that had more than 100 files and 10,000
lines of code as these VFCs are likely tangled commits, i.e., not
only fixing SVs. We also discarded VFCs that were not written
in the Java programming language as Java has been commonly
used in both practice2 and the literature (e.g., [1, 30, 43, 55]). After
the filtering process, we obtained 900 VFCs to extract vulnerable
functions/statements for building SV assessment models.
Extraction of vulnerable functions and statements. For each
VFC, we obtained all the affected files (i.e., containing changed lines),
excluding test files because we focused on production code. We
followed a common practice [46, 47, 84] to consider all the functions
in each affected file as vulnerable functions and the deleted lines
in these functions as vulnerable statements. We removed functions
having only added changes and non-functional/cosmetic changes
such as removing/changing inline/multi-line comments, spaces, or
empty lines. For the former, added lines only exist in fixed code,
making it hard to pinpoint the exact vulnerable statements or root
causes leading to such code additions [66]. For the latter, cosmetic
changes likely do not contribute to SV fixes [55]. We also did not
use a function if its entire body was deleted because such a case did
not have any non-vulnerable statements for building SV assessment
models in our RQs (see section 3). After the filtering steps, we re-
trieved 1,782 vulnerable functions and 5,179 vulnerable statements
of 429 SVs in 200 Java projects. We also obtained the seven CVSS
metrics from NVD for each vulnerable function (see Fig. 3).
Manual validation of vulnerable functions. We randomly se-
lected 317 functions, i.e., with 95% confidence level and 5% error [10],
from our dataset. The first author and a PhD student with three-
year experience in Software Engineering and Cybersecurity in-
dependently validated the functions. The manual validation was
considerably labor-intensive, taking 120 man-hours. We achieved
a substantial agreement with a Cohen’s kappa score [80] of 0.72.

1https://github.com/advisories
2https://bit.ly/stack-overflow-survey-2021

4

On the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

directed edges that capture data or control dependencies among
nodes. A data dependency exists between two statements when one
statement affects/changes the value of a variable used in another
statement. For example, “int b = a + 1;” is data-dependent on
“int a = 1;” as the variable a defined in the second statement
is used in the first statement. A control dependency occurs when
a statement determines whether/how often another statement is
executed. For instance, in “if (b == 2) func(c);”, “func(c)”
only runs if “b == 2”, and thus is control-dependent on the former.
Based on data and control dependencies, backward and forward
slices are extracted. Backward slices directly change or control the
execution of statements affecting the values of variables in vulnera-
ble statements; whereas, forward slices are data/control-dependent
on vulnerable statements [14]. In a PDG, backward slices are nodes
that can go to vulnerable nodes through one or more directed edges.
In Fig. 1, the dir variable is defined in line 5 and then used in vul-
nerable line 8, so line 5 is a backward slice. Forward slices are the
nodes that can be reached from vulnerable nodes by following one
or more directed edges in a PDG. In Fig. 1, line 11 is data-dependent
on vulnerable line 8 as it uses the value of sb; thus, line 11 is a
forward slice. The program slicing context of vulnerable statements
in a function is a combination of all backward and forward slices.
Surrounding context. Another way to define context is to take a
fixed number of lines (𝑛) before and after a vulnerable statement,
which is referred to as surrounding context. These surrounding lines
may contain relevant information, e.g., values/usage of variables in
vulnerable statements. This context is also based on an observation
that developers usually first look at nearby code of vulnerable state-
ments to understand how to fix SVs [77]. We discarded surrounding
lines that were just code comments or blank lines as these probably
do not contribute to the functionality of a function [55]. We also
limited surrounding lines to be within-function.
Function context. Contrary to program slicing and surrounding
context that may not use all lines in a function, function context
uses all function statements, excluding vulnerable ones. This scope
has been commonly used for SV detection models [51, 93] because
vulnerable statements are unavailable at detection time. This scope
contains all the lines of program slicing/surrounding context and
other presumably indirectly related lines to vulnerable statements.
Accordingly, the performance of using indirectly SV-related lines
together with directly SV-relevant lines for SV assessment can
be examined. Note that for a given function, combining function
context with vulnerable statements as a single code block (RQ2 in
section 3) is equivalent to using the whole function, which would
result in the same input to SV assessment models regardless of
which statements are vulnerable. This input combination allows
us to evaluate the usefulness of the exact location of vulnerable
statements for function-level SV assessment models.

4.3 Code Feature Generation
Raw code from vulnerable statements and their context are con-
verted into fixed-length feature vectors to be consumable by learning-
based SV assessment models. This step describes five techniques
we used to extract features from code inputs.
Bag-of-Tokens. Bag-of-Tokens is based on Bag-of-Words, a popular
feature extraction technique in Natural Language Processing (NLP).

5

Figure 3: Class distributions of the seven CVSS metrics.

Disagreements were resolved through discussion. We found that
9% of the selected functions were not vulnerable, mainly due to
tangled fixes/VFCs. The functions in these VFCs fixed a non-SV
related issue, e.g., the nameContainsForbiddenSequence function
in the commit cefbb94 of the Eclipse Mojarra project. The modifier
of this function in the ResourceManager.java class was changed
from private to package. This change allowed the reuse of the
function in other classes like ClasspathResourceHelper.java for
sanitizing inputs to prevent a path traversal SV (CVE-2020-6950).
We assert that it is challenging to detect all of these cases without
manual validation. However, such validation is extremely expensive
to scale up to thousands of functions like the dataset we curated.

4.2 Vulnerable Code Context Extraction
This section describes the Context Extraction module that takes vul-
nerable and non-vulnerable statements as inputs and then outputs
program slicing, surrounding, or function context. These context
types have been previously used for bug/SV-related tasks [49, 51,
77]. However, there is little known about their use and value for
function-level SV assessment tasks, which are studied in this work.
Program slicing context. Program slicing captures relevant state-
ments to a point in a program (line of code) to support software
debugging [85]. This concept has been utilized for SV identifica-
tion [49, 90]. However, using this context for SV assessment is
fundamentally different. For SV detection, the location of vulnera-
ble statements is unknown, so program slicing context is usually
extracted for all statements including non-vulnerable ones in a
function of pre-defined types (e.g., array manipulations, arithmetic
operations, and function calls) [49]. In contrast, for SV assessment,
vulnerable statements are known; thus, program slicing context is
only obtained for these statements. With a focus on function-level
SV assessment, we considered intra-procedural program slicing, i.e.,
finding relevant lines within the boundary of a function of interest.
Following the common practice in the literature [49, 67], we
used Program Dependence Graph (PDG) [20] extracted from source
code to obtain program slices for vulnerable statements in each
function. A PDG contains nodes that represent code statements and

4.795.32.123.474.521.878.24.168.727.24.161.234.75.846.647.625.567.96.6AccessVectorAccessComplexityAuthenti-cationConfiden-tialityIntegrityAvailabilitySeverity0102030405060708090100NetworkLocalLowMediumHighSingleNoneNonePartialCompleteNonePartialCompleteNonePartialCompleteLowMediumHighPercentage (%)MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Triet Huynh Minh Le and M. Ali Babar

This technique has been commonly investigated for developing SV
assessment models based on textual SV descriptions/reports [25, 83,
86]. We extended this technique to code-based SV assessment by
counting the frequency of code tokens. We also applied code-aware
tokenization to preserve code syntax and semantics. For instance,
var++ was tokenized into var and ++, explicitly informing a model
about incrementing the variable var by one using the operator (++).
Bag-of-Subtokens. Bag-of-Subtokens extends Bag-of-Tokens by
splitting extracted code tokens into sequences of characters (sub-
tokens). These characters help a model learn less frequent tokens
better. For instance, an infrequent variable like ThisIsAVeryLongVar
is decomposed into multiple sub-tokens; one of which is Var, telling
a model that this is a potential variable. We extracted sub-tokens of
lengths ranging from two to six. Such values have been previously
adopted for SV assessment [44, 57]. We did not use one-letter char-
acters as they were too noisy, while using more than six characters
would significantly increase feature size and computational cost.
Word2vec. Unlike Bag-of-Tokens and Bag-of-Subtokens that do
not consider token context, Word2vec [56] extracts features of a
token based on its surrounding counterparts. The contextual in-
formation from surrounding tokens helps produce similar feature
vectors in an embedding space for tokens with (nearly) identical
functionality/usage (e.g., average and mean variables). Word2vec
generates vectors for individual tokens, so we averaged the vec-
tors of all input tokens to represent a code snippet. This averaging
method has been demonstrated to be effective [69]. Table 1 lists dif-
ferent values for the window and vector sizes of Word2vec used for
tuning the performance of learning-based SV assessment models.
fastText. fastText [7] enhances Word2vec by representing each
token with an aggregated feature vector of its constituent sub-
tokens. Technically, fastText combines the strengths of semantic
representation of Word2vec and subtoken-augmented features of
Bag-of-Subtokens. fastText has been shown to build competitive yet
compact report-level SV assessment models [44]. Like Word2vec,
the feature vector of a code snippet was averaged from the vectors of
all the input tokens. The length of sub-tokens also ranged from two
to six, resembling that of Bag-of-Subtokens. Other hyperparameters
of fastText for optimization are listed in Table 1.
CodeBERT. CodeBERT [19] is an adaptation of BERT [15], the
current state-of-the-art feature representation technique in NLP,
to source code modeling. CodeBERT is a pre-trained model using
both natural language and programming language data to produce
contextual embedding for code tokens. The same code token can
have different CodeBERT embedding vectors depending on other
tokens in an input; whereas, word2vec/fastText produces a single
vector for every token regardless of its context. In addition, the
source code tokenizer of CodeBERT is built upon Byte-Pair En-
coding (BPE) [68]. This tokenizer smartly retains sub-tokens that
frequently appear in a training corpus rather than keeping all of
them as in Bag-of-Subtokens and fastText, balancing between per-
formance and cost. CodeBERT also preserves a special token, [CLS],
to represent an entire code input. We leveraged the vector of this
[CLS] token to extract the features for each code snippet.

We trained all the feature models from scratch, except CodeBERT
as it is a pre-trained model. We used CodeBERT’s pre-trained vocab-
ulary and embeddings as commonly done in the literature [92]. To
build the vocabulary for the other feature extraction methods, we

Table 1: Hyperparameter tuning for SV assessment models.

Step
Feature
extraction

CVSS
metrics
prediction

Regularization coefficient:
0.01, 0.1, 1, 10, 100

Hyperparameters
Vector size: 150, 300, 500
Window size: 3, 4, 5

Model
Word2vec [69]
fastText [7]
Logistic Regression
(LR) [81]
Support Vector
Machine (SVM) [12]
K-Nearest
Neighbors
(KNN) [3]
Random Forest (RF) [29] No. of estimators: 100, 200,
Extreme Gradient
Boosting (XGB) [8]
Light Gradient
Boosting Machine
(LGBM) [34]

300, 400, 500
Max depth: 3, 5, 7, 9,
unlimited
Max. no. of leaf nodes: 100,
200, 300, unlimited (RF)

No. of neighbors: 5, 11, 31, 51
Weight: uniform, distance
Distance norm: 1, 2

considered tokens appearing at least in two samples in a training
dataset to avoid vocabulary explosion due to too rare tokens. The
exact vocabulary depended on the dataset used in each of the 10
training/evaluation rounds, as described in section 4.5. Note that
some code snippets, e.g., vulnerable lines extracted from (partial)
code changes, were not compilable (i.e., did not contain complete
code syntax); thus, we did not use Abstract Syntax Tree (AST) based
code representation like Code2vec [1] in this study as such rep-
resentation may not be robust for these cases [30, 31]. It is worth
noting that Bag-of-Tokens, Bag-of-Subtokens, Word2vec, fastText,
and CodeBERT can still work with these cases as these methods
operate directly on code tokens.

4.4 Data-driven SV Assessment Models
Features generated from code inputs enter ML models for predicting
the CVSS metrics. The predictions of the CVSS metrics are classifica-
tion problems (see Fig. 3). We used six well-known Machine Learn-
ing (ML) models for classifying the classes of each CVSS metric:
Logistic Regression (LR) [81], Support Vector Machine (SVM) [12],
K-Nearest Neighbors (KNN) [3], Random Forest (RF) [29], eXtreme
Gradient Boosting (XGB) [8], and Light Gradient Boosting Machine
(LGBM) [34]. LR, SVM, and KNN are single models, while RF, XGB,
and LGBM are ensemble models that leverage multiple single coun-
terparts to reduce overfitting. These classifiers have been used for
SV assessment based on SV reports [44, 73]. We also considered
different hyperparameters for tuning the performance of the classi-
fiers, as given in Table 1. These hyperparameters have been adapted
from the prior studies using similar classifiers [42, 44, 73]. Here,
we mainly focus on ML techniques, and thus using Deep Learning
models [39] for the tasks is out of the scope of this study.

4.5 Model Evaluation
Evaluation technique. To develop function-level SV assessment
models and evaluate their performance, we used 10 rounds of train-
ing, validation, and testing. We randomly shuffled the dataset of
vulnerable functions in section 4.1 and then split it into 10 partitions
of roughly equal size.3 In round 𝑖, for a model, we used fold 𝑖 + 1 for
validation, fold 𝑖 + 2 for testing, and all of the remaining folds for

3With 1,782 samples in total, folds 1-9 had 178 samples and fold 10 had 180 samples.

6

On the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

training. When 𝑖 +1 or 𝑖 +2 was larger than 10, its value was wrapped
around. For example, if 𝑖 = 10, then (𝑖 + 1) mod 10 = 11 mod 10 = 1
and (𝑖 + 2) mod 10 = 12 mod 10 = 2. A grid search of the hyperpa-
rameters in Table 1 was performed using the validation sets to select
optimal models. The performance of such optimal models on the test
sets was reported. It is important to note that our evaluation strat-
egy improves upon 10-fold cross-validation and random splitting
data into a single training/validation/test set, the two most com-
monly used evaluation techniques in (fine-grained) SV detection
and report-level SV assessment studies [28, 40, 47, 48, 59, 72]. Our
evaluation technique has separate test sets, which cross-validation
does not, to objectively measure the performance of tuned/optimal
models on unseen data. Using multiple (10) validation/test sets also
increases the stability of results compared to a single set [65]. More-
over, we aim to provide baseline performance for function-level
SV assessment in this study, so we did not apply any techniques
like class rebalancing or feature selection/reduction to augment
the data/features. Such augmentation can be explored in future
work. We also did not compare SV assessment using functions with
that using reports as their use cases are different; function-level SV
assessment is needed when SV reports are unavailable/unusable.
It may be fruitful to compare/combine these two artifacts for SV
assessment in the future.
Evaluation measures. We used F1-Score4 and Matthews Correla-
tion Coefficient (MCC) measures to quantify how well developed
models perform SV assessment tasks. F1-Score values are from
0 to 1, and MCC has values from –1 to 1; 1 is the best value for
both measures. These measures have been commonly used for SV
assessment (e.g., [38, 44, 73]). We used MCC as the main measure
for selecting optimal models because MCC takes into account all
classes, i.e., all cells in a confusion matrix, during evaluation [53].
Statistical analysis. To confirm the significance of results, we
employed one-sided Wilcoxon signed rank test [87] and its respec-
tive effect size (𝑟 = 𝑍 /
𝑁 , where 𝑍 is the 𝑍 -score statistic of the
test and 𝑁 is the total count of samples) [78].5 We used Wilcoxon
signed-rank test because it is a non-parametric test that can com-
pare two-paired groups of data, and we considered a test significant
if its confidence level was more than 99% (𝑝-value < 0.01). We did
not use the popular Cohen’s D [11] and Cliff’s 𝛿 [54] effect sizes as
they are not suitable for comparing paired data [84].

√

5 RESULTS
5.1 RQ1: Are vulnerable code statements more

useful than non-vulnerable counterparts
for SV assessment models?

Based on the extraction process in section 4.1, we collected 1,782
vulnerable functions containing 5,179 vulnerable and 57,633 non-
vulnerable statements. The proportions of these two types of state-
ments are given in the first and second boxplots, respectively, in
Fig. 4. On average, 14.7% of the lines in the selected functions were
vulnerable, 5.8 times smaller than that of non-vulnerable lines. Inter-
estingly, we also observed that 55% of the functions contained only
a single vulnerable statement. These values show that vulnerable
statements constitute a very small proportion of functions.

4The macro version of F1-Score was used for multi-class classification.
5𝑟 ≤ 0.1: negligible, 0.1 < 𝑟 ≤ 0.3: small, 0.3 < 𝑟 ≤ 0.5: medium, 𝑟 > 0.5: large [21]

Figure 4: Proportions of different types of lines in a function.
Notes: Min proportion of Vuln-only lines is non-zero (0.03%);
thus, max of Non-vuln line proportion is < 100%. Cosmetic
lines were excluded from the computation of ratios.

Despite the small size (no. of lines), vulnerable statements
outperformed non-vulnerable statements for the seven assess-
ment tasks (see Table 2). We considered two variants of non-
vulnerable statements for comparison. The first variant, Non-vuln
(random), randomly selected the same number of lines as vulnerable
statements from non-vulnerable statements in each function. The
second variant, Non-vuln (all) aka. Non-vuln (All - Vuln) in Fig. 4,
considered all non-vulnerable statements. Compared to same-sized
non-vulnerable statements (Non-vuln (random)), Vuln-only (using
vulnerable statements solely) produced 116.9%, 126.6%, 98.7%, 90.7%,
147.9%, 111.2%, 116.7% higher MCC for Access Vector, Access Com-
plexity, Authentication, Confidentiality, Integrity, Availability, and
Severity tasks, respectively. On average, Vuln-only was 114.5% and
43.6% better than Non-vuln (random) in MCC and F1-Score, re-
spectively. We obtained similar results of Non-vuln (random) when
repeating the experiment with differently randomized lines. When
using all non-vulnerable statements (Non-vuln (all)), the assessment
performance increased significantly, yet was still lower than that
of vulnerable statements. Average MCC and F1-Score of Vuln-only
were 7.4% and 5.5% higher than Non-vuln (all), respectively. The im-
provements of Vuln-only over the two variants of non-vulnerable
statements were statistically significant across features/classifiers
with 𝑝-values < 0.01 (𝑝-value𝑁 𝑜𝑛−𝑣𝑢𝑙𝑛 (𝑟𝑎𝑛𝑑𝑜𝑚) = 1.7 × 10−36 and
𝑝-value𝑁 𝑜𝑛−𝑣𝑢𝑙𝑛 (𝑎𝑙𝑙) = 7.2 × 10−11) and non-negligible effect sizes
(𝑟𝑁 𝑜𝑛−𝑣𝑢𝑙𝑛 (𝑟𝑎𝑛𝑑𝑜𝑚) = 0.62 and 𝑟𝑁 𝑜𝑛−𝑣𝑢𝑙𝑛 (𝑎𝑙𝑙) = 0.32). The low per-
formance of Non-vuln (random) implies that SV assessment models
likely perform worse if vulnerable statements are incorrectly iden-
tified. Moreover, the decent performance of Non-vuln (all) shows
that some non-vulnerable statements are potentially helpful for SV
assessment, which are studied in detail in RQ2.

5.2 RQ2: To what extent do different types of

context of vulnerable statements
contribute to SV assessment performance?

In RQ2, we compared the performance of models using Program
Slicing (PS), surrounding and function context. Notably, we removed
365 cases for which we could not extract PS context from the dataset

7

020406080100Proportionoflinesinafunction(%)All-Vuln-PS(IndirectlySV-related)Surroundingcontext(n=6)ProgramSlicing(PS)context(DirectlySV-related)Non-vuln(All-Vuln)Vuln-onlyTypeoflinesMSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Triet Huynh Minh Le and M. Ali Babar

Figure 5: Differences in testing SV assessment performance (F1-Score and MCC) between models using different types of
lines/context and those using only vulnerable statements. Note: The differences were multiplied by 100 to improve readability.

Table 2: Testing performance for SV assessment tasks of vul-
nerable vs. non-vulnerable statements. Note: Bold and grey-
shaded values are best row-wise performance.

CVSS metric

Evaluation
metric

Vuln-only

Access Vector

Access
Complexity

Authentication

Confidentiality

Integrity

Availability

Severity

Average

F1-Score
MCC
F1-Score
MCC
F1-Score
MCC
F1-Score
MCC
F1-Score
MCC
F1-Score
MCC
F1-Score
MCC

F1-Score
MCC

0.820
0.681
0.622
0.510
0.791
0.630
0.645
0.574
0.650
0.585
0.647
0.583
0.695
0.583

0.695
0.592

Input type
Non-vuln
(random)
0.650
0.314
0.458
0.225
0.602
0.317
0.411
0.301
0.384
0.236
0.417
0.276
0.414
0.269

0.484
0.276

Non-vuln
(all)
0.786
0.605
0.592
0.467
0.765
0.614
0.625
0.561
0.616
0.534
0.624
0.551
0.610
0.523

0.659
0.551

curated in section 4.1. Apparently, these cases were the same as
using only vulnerable statements, which would make comparisons
biased, especially against Vuln-only itself. The vulnerable state-
ments in these cases mainly did not contain any variable, e.g., using
an unsafe function without any parameter.6 In this new dataset,
PS and surrounding context approximately constituted 46%, on
average, of lines in vulnerable functions (see Fig. 4). We used six
lines before and after vulnerable statements (n = 6) as surrounding
context because this value resulted in the closest average context
size to that of PS context. It is impossible to have the exactly same
size because PS context is dynamically derived from vulnerable
statements, while surrounding context is predefined. The roughly
similar size helps test whether directly SV-related lines in PS con-
text would be better than pre-defined surrounding lines for SV
assessment. The training and evaluation processes on the dataset
in RQ2 were the same as in RQ1.7

6https://bit.ly/3pg36mp
7The RQ1 findings still hold when using the new dataset in RQ2, yet with a slight
(≈2%) decrease in absolute model performance.

8

3.30.64.6-3.2-2.4-3.66.41.75.6-3.8-7.4-10.67.85.06.4-8.5-8.8-12.48.37.610.5-3.1-8.2-6.26.03.15.2-1.8-2.7-5.64.12.24.7-5.7-2.2-17.00.65.04.4-0.9-7.2-12.15.23.65.9-3.8-5.5-9.6IntegrityAvailabilitySeverityAverageAccess VectorAccess ComplexityAuthenticationConfidentiality-20-10010-20-10010-20-10010-20-10010All - PS - VulnSurrounding contextPS contextFunction context + VulnSurrounding context + VulnProgram Slicing (PS) context + VulnAll - PS - VulnSurrounding contextPS contextFunction context + VulnSurrounding context + VulnProgram Slicing (PS) context + VulnPerformance (F1-Score) difference with respect to Vuln-only (using vulnerable statements only)Type of lines in a vulnerable function1.9-0.44.6-6.1-5.5-5.63.9-1.85.9-4.6-11.3-12.72.12.73.2-4.9-4.4-11.84.83.66.5-4.8-5.7-15.07.04.15.4-3.8-5.2-19.54.22.25.9-3.0-1.9-20.43.42.04.7-4.8-6.0-16.93.91.85.2-4.6-5.7-14.6IntegrityAvailabilitySeverityAverageAccess VectorAccess ComplexityAuthenticationConfidentiality-20-10010-20-10010-20-10010-20-10010All - PS - VulnSurrounding contextPS contextFunction context + VulnSurrounding context + VulnProgram Slicing (PS) context + VulnAll - PS - VulnSurrounding contextPS contextFunction context + VulnSurrounding context + VulnProgram Slicing (PS) context + VulnPerformance (MCC) difference with respect to Vuln-only (using vulnerable statements only)Type of lines in a vulnerable functionOn the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Overall, adding context to vulnerable statements led to bet-
ter SV assessment performance than using vulnerable state-
ments only (see Fig. 5). Among the three, function context was
the best, followed by PS and then surrounding context. In terms
of the main measure MCC, function context working together with
vulnerable statements beat Vuln-only by 6.4%, 6.5%, 9%, 8.2%, 11%,
11.4%, 9.7% for Access Vector, Access Complexity, Authentication,
Confidentiality, Integrity, Availability, and Severity tasks, respec-
tively. The higher F1-Score values when incorporating function
context to vulnerable statements are also evident in Fig. 5. On
average, combining function context and vulnerable statements
attained 0.64 MCC and 0.75 F1-Score, surpassing using vulnerable
lines solely by 8.9% in MCC and 8.5% in F1-Score. Although PS con-
text + Vuln performed slightly worse than function context + Vuln,
MCC and F1-Score of PS context + Vuln were still 6.7% and 7.5%
ahead of Vuln-only, respectively. The improvements of function and
PS context + Vuln over Vuln-only were significant across features/-
classifiers, i.e., 𝑝-values of 1.2 × 10−17 and 2.1 × 10−13 and medium
effect sizes of 0.42 and 0.36, respectively. Compared to function/PS
context + Vuln, surrounding context + Vuln outperformed Vuln-
only by smaller margins, i.e., 3% for MCC and 5.2% for F1-Score
(𝑝-value = 3.7 × 10−8 < 0.01 with a small effect size (𝑟 = 0.27)).
These findings show the usefulness of directly SV-related (PS) lines
for SV assessment models, while six lines surrounding vulnerable
statements seemingly contain less related information for the SV
assessment tasks. Further investigation revealed that only 49% of
lines in PS context overlapped with those in surrounding context
(n = 6). Note that the performance of surrounding context tended
to approach that of function context as the surrounding context
size increased. Using the dataset in RQ1, we also obtained the same
patterns, i.e., function context + Vuln > surrounding context + Vuln
> Vuln-only. This result shows that function context is generally
better than the other context types, indicating the plausibility of
building effective SV assessment models using only the output of
function-level SV detection (i.e., requiring no knowledge about
which statements are vulnerable in each function).

Although the three context types were useful for SV assess-
ment when combined with vulnerable statements, using these
context types alone significantly reduced the performance. As
shown from RQ1, using only function context (i.e., Non-vuln (all) in
Table 2) was 6.9% inferior in MCC and 5.2% lower in F1-Score than
Vuln-only. Using the new dataset in RQ2, we obtained similar reduc-
tions in MCC and F1-Score values. Fig. 5 also indicates that using
only PS and surrounding context decreased MCC and F1-Score of
all the tasks. Particularly, using PS context alone reduced MCC and
F1-Score by 7.8% and 5.5%, respectively; whereas, such reductions in
values for using only surrounding context were 9.8% and 8%. These
performance drops were confirmed significant with 𝑝-values < 0.01
and non-negligible effect sizes. Overall, the performance rankings
of the context types with and without vulnerable statements were
the same, i.e., function > PS > surrounding context. We also ob-
served that all context types were better (increasing 20.1-28.2% in
MCC and 6.9-17.5% in F1-Score) than non-directly SV-related lines
(i.e., All - PS - Vuln in Fig. 5). These findings highlight the need
for using context together with vulnerable statements rather than
using each of them alone for function-level SV assessment tasks.

Table 3: Differences in testing performance for SV assess-
ment tasks between double-input models (RQ3) and single-
input models (RQ1/2). Notes: The differences were multi-
plied by 100 to increase readability. Green and red colors de-
note value increase and decrease, respectively. Darker color
shows a higher magnitude of increase/decrease. Average per-
formance values (multiplied by 100) of double-input models
for the three context (ctx) types are in parentheses.

CVSS metric

Evaluation
metric

Access
Vector
Access
Complexity
Authentica-
tion
Confident-
iality

Integrity

Availability

Severity

Average

F1-Score
MCC
F1-Score
MCC
F1-Score
MCC
F1-Score
MCC
F1-Score
MCC
F1-Score
MCC
F1-Score
MCC

F1-Score
MCC

PS
ctx + Vuln
1.3
2.6
-0.5
1.5
0.5
1.0
-0.2
-0.7
0.2
-0.07
-0.5
0.4
0.7
0.2

0.2 (74.7)
0.7 (63.1)

Input type (double)
Surrounding
ctx + Vuln
1.2
2.7
0.6
0.5
0.7
1.7
0.01
-0.5
0.7
1.1
-0.2
0.4
0.9
0.02

0.5 0.5 (73.4)
0.8 (61.1)

Function
ctx + Vuln
1.2
2.4
-0.7
2.4
0.3
0.7
-0.9
-1.5
0.2
0.5
-1.0
0.1
0.7
0.2

-0.03 (75.2)
0.5 (64.1)

5.3 RQ3: Does separating vulnerable statements

and context to provide explicit location of
SVs improve assessment performance?
RQ3 evaluated the approach of separating vulnerable statements
from their context as two inputs for building SV assessment models.
Theoretically, this double-input method tells a model the exact vul-
nerable and context parts in input code, helping the model capture
the information from relevant parts for SV assessment tasks more
easily. To separate features, feature vectors are generated for each
of the two inputs and then concatenated to form a single vector
of twice the size of the vector used in RQ2. RQ3 used the same
dataset from RQ2 (i.e., excluding cases without PS context) and the
respective model evaluation procedure to objectively compare PS
context with the other context types.

Overall, double-input models improved the performance for
all types of context compared to single-input ones, but the im-
provements were not substantial (≈1%). Table 3 clearly indicated
the improvement trend; i.e., a majority of the cells have green color.
We noticed that the rankings of double-input models using differ-
ent context types still remained the same as in RQ2, i.e., function >
PS > surrounding context. Specifically, double-input models raised
the MCC values of single-input models using PS, surrounding, and
function context by 1.1%, 1.4%, and 0.8%, respectively. In terms of
F1-Score of double-input models, PS and surrounding context had
0.26% and 0.75% increase, while function context suffered from a
0.04% decline. We found the absolute performance differences be-
tween double-input and single-input models for the seven tasks
were actually small and not statistically significant with negligible
effect sizes (𝑟𝑃𝑆 𝑐𝑡𝑥+𝑉 𝑢𝑙𝑛 = 0.059, 𝑟𝑆𝑢𝑟𝑟𝑜𝑢𝑛𝑑𝑖𝑛𝑔 𝑐𝑡𝑥+𝑉 𝑢𝑙𝑛 = 0.092,
and 𝑟𝐹𝑢𝑛𝑐𝑡𝑖𝑜𝑛 𝑐𝑡𝑥+𝑉 𝑢𝑙𝑛 = 0.021). We observed similar changes/pat-
terns of function/surrounding context when using the full dataset
in RQ1. The findings suggest that models using function context +

9

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Triet Huynh Minh Le and M. Ali Babar

Figure 6: Average performance (MCC) of six classifiers and
five features for SV assessment in functions. Notes: BoT and
BoST are Bag-of-Tokens and Bag-of-Subtokens, respectively.

Vuln as a single-input in RQ2 still perform competitively. This result
strengthens the conclusion in RQ2 that SV assessment models ben-
efit from vulnerable statements along with (in-)directly SV-related
lines in functions, yet not necessarily where these lines are located.

6 DISCUSSION
6.1 Function-level SV Assessment: Baseline

Models and Beyond

From RQ1-RQ3, we have shown that vulnerable statements and
their context are useful for SV assessment tasks. In this section, we
discuss the performance of various features and classifiers used to
develop SV assessment models on the function level. We also ex-
plore the patterns of false positives of the models used in this work.
Through these discussions, we aim to provide recommendations
on building strong baseline models and inspire future data-driven
advances in function-level SV assessment.
Practice of building baselines. Among the investigated fea-
tures and classifiers, a combination of LGBM classifier and
Bag-of-Subtokens features produced the best overall perfor-
mance for the seven SV assessment tasks (see Fig. 6). In addition,
LGBM outperformed the other classifiers, and Bag-of-Subtokens
was better than the other features. However, we did not find a
single set of hyperparameters that was consistently better than
the others, emphasizing the need for hyperparameter tuning for
function-level SV assessment tasks, as generally recommended in
the literature [76, 79]. Regarding the classifiers, the ensemble ones
(LGBM, RF, and XGB) were significantly better than the single
counterparts (SVM, LR, and KNN) when averaging across all fea-
ture types, aligning with the previous findings for SV assessment
using SV reports [44, 73]. Regarding the features, the ones aug-
mented by sub-tokens (Bag-of-Subtokens, fastText, and CodeBERT)
had stronger performance, on average, than the respective feature
types using only word-based representation (Bag-of-Tokens and
Word2vec). This observation suggests that SV assessment models
benefit from sub-tokens, probably because rare code tokens are
more likely to be captured by these features. This result is similar to
Le et al. [44]’s finding for report-level SV assessment models. All of
the above comparisons were confirmed statistically significant with
𝑝-values < 0.01 and non-negligible effect sizes; similar patterns were
also obtained for F1-Score. Surprisingly, the latest feature model,
CodeBERT, did not show superior performance in this scenario,
likely because the model was originally pre-trained on multiple
languages, not only Java (the main language used in this study).
Fine-tuning the weights of CodeBERT using SV data in a specific

programming language is worthy of exploration for potentially
improving the performance of this feature model. Overall, using
the aforementioned baseline features and classifiers, we observed
a common pattern in the performance ranking of the seven CVSS
metrics across different input types, i.e., Access Vector > Authen-
tication > Severity > Confidentiality – Integrity – Availability >
Access Complexity. We speculate that the metric-wise class distri-
bution (see Fig. 3) can be a potential explanation for this ranking.
Specifically, Access Vector and Authentication are binary classifica-
tions, which have less uncertainty than the other tasks. In addition,
Confidentiality, Integrity, and Availability are all impact metrics
with roughly similar distributions, resulting in similar performance
as well. Access Complexity suffers the most from imbalanced data
among the tasks, and thus this task has the worst performance.
False-positive patterns. We manually analyzed the incorrect pre-
dictions by the optimal models using the best-performing (function)
context from RQ2/RQ3. From these cases, we found two key pat-
terns of false positives. The first pattern concerned SVs affecting
implicit code in function calls. For example, a feature requiring
authentication, i.e., the synchronous mode, was run before user’s
password was checked in a function (doFilter), leading to a po-
tential access-control related SV.8 The execution of such mode was
done by another function, processSync, but its implementation
along with the affected components inside was not visible to the
affected function. Such invisibility hinders a model’s ability to fully
assess SV impacts. A straightforward solution is to employ inter-
procedural analysis [50], but scalability is a potential issue as SVs
can affect multiple functions and even the ones outside of a project
(i.e., functions in third-party libraries). Future work can leverage
Question and Answer websites to retrieve and analyze SV-related
information of third-party libraries [41]. The second type of false
positives involved vulnerable variables with obscure context.
For instance, a function used a potentially vulnerable variable con-
taining malicious inputs from users, but the affected function alone
did not contain sufficient information/context about the origin
of the variable.9 Without such variable context, a model would
struggle to assess the exploitability of an SV; i.e., through which
components attackers can penetrate into a system and whether
any authentication is required during the penetration. Future work
can explore taint analysis [36] to supplement function-level SV
assessment models with features about variable origin/flow.

6.2 Threats to Validity
The first threat concerns the curation of vulnerable functions and
statements for building SV assessment models. We considered the
recommendations in the literature to remove noise in the data (e.g.,
abnormally large and cosmetic changes). We also performed manual
validation to double-check the validity of our data.

Another threat is about the robustness of our own implementa-
tion of the program slicing extraction. To mitigate the threat, we
carefully followed the algorithms and descriptions given in the
previous work [14] to extract the intra-procedural backward and
forward slices for a particular code line.

8https://bit.ly/32vyggM (CVE-2018-1000134)
9https://bit.ly/3plU7QP (CVE-2012-0391)

10

0.5270.4010.4650.4380.4550.4580.3980.3780.3720.3630.5020.4740.4280.3580.4150.4990.4270.2150.2840.2080.4800.4510.2560.3480.2560.2730.2600.3290.2630.314KNNLRSVMXGBRFLGBMWord2vecCodeBERTfastTextBoTBoSTFeature typeClassifier type0.20.30.40.5MCCOn the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Other threats are related to the selection and optimality of base-
line models. We assert that it is nearly impossible to consider all
types of available features and models due to limited resources.
Hence, we focused on the common techniques and their respective
hyperparameters previously used for relevant tasks, e.g., report-
level SV assessment. We are also the first to tackle function-level
SV assessment using data-driven models, and thus our imperfect
baselines can still stimulate the development of more advanced and
better-performing techniques in the future.

Regarding the reliability of our study, we confirmed the key find-
ings with 𝑝-values < 0.01 using non-parametric Wilcoxon signed
rank tests and non-negligible effect sizes. Regarding the general-
izability of our results, we only performed our study in the Java
programming language, yet we mitigated this threat by using 200
real-world projects of diverse domains and scales. The data and
models were also released to support reuse and extension to new
languages/applications [5].

7 RELATED WORK
7.1 Code Granularities of SV Detection
SV detection has long attracted attention from researchers and
there have been many proposed data-driven solutions to automate
this task [26]. Neuhaus et al. [58] were among the first to tackle
SV detection in code components/files. This seminal work has in-
spired many follow-up studies on component/file-level SV detection
(e.g., [9, 70, 75]). Over time, function-level SV detection tasks have
become more popular [6, 52, 60, 82, 93] as functions are usually
much smaller than files, significantly reducing inspection effort for
developers. For example, the number of code lines in the methods
in our dataset was only 35, on average, nearly 10 times smaller than
that (301) of files. Recently, researchers have begun to predict exact
vulnerable statements/lines in functions (e.g., [16, 47, 48, 59]). This
emerging research is based on an important observation that only
a small number of lines in vulnerable functions contain root causes
of SVs. Instead of detecting SVs as in these studies, we focus on SV
assessment tasks after SVs are detected. Specifically, utilizing the
outputs (vulnerable functions/statements) from these SV detection
studies, we perform function-level SV assessment to support SV
understanding/prioritization before fixing.

7.2 Data-driven SV Assessment
SV assessment has been an integral step for addressing SVs. CVSS
has been shown to provide one of the most reliable metrics for
SV assessment [33]. There has been a large and growing body of
research work on automating SV assessment tasks [40], especially
predicting the CVSS metrics for ever-increasing SVs. Most of the
current studies (e.g., [17, 18, 44, 73, 74, 88]) have utilized SV de-
scriptions available in bug/SV reports/databases, mostly NVD, to
predict the CVSS metrics. However, according to our analysis in
section 2.2, NVD reports of SVs are usually released long (up to
1k days) after SVs have been fixed, rendering report-level SV as-
sessment potentially untimely for SV fixing. Unlike the current
studies, we propose shifting the SV assessment tasks to the code
function level, which can help developers assess functions right
after they are found vulnerable and before fixing. Note that we
assess all types of SVs in source code, not only the ones present in

11

dependencies [37, 63]. Overall, our study informs the practice of
developing strong baselines for function-level SV assessment tasks
by combining vulnerable statements and their context.

8 CONCLUSIONS AND FUTURE WORK
We motivated the need for function-level SV assessment to provide
essential information for developers before fixing SVs. Through
large-scale experiments, we studied the use of data-driven models
for automatically assigning the seven CVSS assessment metrics to
SVs in functions. We demonstrated that strong baselines for these
tasks benefited not only from fine-grained vulnerable statements,
but also the context of these statements. Specifically, using vulner-
able statements with all the other lines in functions produced the
best performance of 0.64 MCC and 0.75 F1-Score. These promising
results show that function-level SV assessment tasks deserve more
attention and contribution from the community, especially tech-
niques that can strongly capture the relations between vulnerable
statements and other code lines/components.

ACKNOWLEDGMENTS
This work was supported with supercomputing resources provided
by the Phoenix HPC service at the University of Adelaide. We
would like to express our gratitude to David Hin for his technical
support at the early stage of this work. We also sincerely thank the
members from the Centre for Research on Engineering Software
Technologies (CREST), Roland Croft, Huaming Chen, Mubin Ul
Haque, and Faheem Ullah, as well as the anonymous reviewers for
the insightful and constructive comments to improve the paper.

REFERENCES
[1] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1–29.

[2] Bushra Aloraini, Meiyappan Nagappan, Daniel M German, Shinpei Hayashi,
and Yoshiki Higo. 2019. An empirical study of security warnings from static
application security testing tools. Journal of Systems and Software 158 (2019),
110427.

[3] Naomi S Altman. 1992. An introduction to kernel and nearest-neighbor nonpara-

metric regression. The American Statistician 46, 3 (1992), 175–185.

[4] Atlassian. [n. d.]. JIRA Issue Tracking System. Retrieved March, 2022 from

https://www.atlassian.com/software/jira
[5] Authors. [n. d.]. Reproduction package.

Retrieved March, 2022 from https:

//github.com/lhmtriet/Function-level-Vulnerability-Assessment

[6] Zeki Bilgin, Mehmet Akif Ersoy, Elif Ustundag Soykan, Emrah Tomur, Pinar
Çomak, and Leyli Karaçay. 2020. Vulnerability prediction from source code using
machine learning. IEEE Access 8 (2020), 150672–150684.

[7] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.
Enriching word vectors with subword information. Transactions of the Association
for Computational Linguistics 5 (2017), 135–146.

[8] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. 785–794.

[9] Istehad Chowdhury and Mohammad Zulkernine. 2011. Using complexity, cou-
pling, and cohesion metrics as early indicators of vulnerabilities. Journal of
Systems Architecture 57, 3 (2011), 294–313.

[10] William G Cochran. 2007. Sampling techniques. John Wiley & Sons.
[11] Jacob Cohen. 2013. Statistical power analysis for the behavioral sciences. Academic

press.

[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine

learning 20, 3 (1995), 273–297.

[13] Roland Croft, Ali Babar, and Li Li. 2022. An investigation into inconsistency of
software vulnerability severity across data sources. In 2022 29th IEEE International
Conference on Software Analysis, Evolution and Reengineering (SANER).

[14] Stanislav Dashevskyi, Achim D Brucker, and Fabio Massacci. 2018. A screening
test for disclosed vulnerabilities in foss components. IEEE Transactions on Software
Engineering 45, 10 (2018), 945–966.

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Triet Huynh Minh Le and M. Ali Babar

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).

[16] Yangruibo Ding, Sahil Suneja, Yunhui Zheng, Jim Laredo, Alessandro Morari, Gail
Kaiser, and Baishakhi Ray. 2022. VELVET: a noVel Ensemble Learning approach
to automatically locate VulnErable sTatements. In 2022 29th IEEE International
Conference on Software Analysis, Evolution and Reengineering (SANER).

[17] Xuanyu Duan, Mengmeng Ge, Triet Huynh Minh Le, Faheem Ullah, Shang Gao,
Xuequan Lu, and M Ali Babar. 2021. Automated security assessment for the
internet of things. In 2021 IEEE 26th Pacific Rim International Symposium on
Dependable Computing (PRDC). IEEE, 47–56.

[18] Clément Elbaz, Louis Rilling, and Christine Morin. 2020. Fighting N-day vulnera-
bilities with automated CVSS vector prediction at disclosure. In Proceedings of
the 15th International Conference on Availability, Reliability and Security. 1–10.

[19] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained
model for programming and natural languages. arXiv preprint arXiv:2002.08155
(2020).

[20] Jeanne Ferrante, Karl J Ottenstein, and Joe D Warren. 1987. The program de-
pendence graph and its use in optimization. ACM Transactions on Programming
Languages and Systems (TOPLAS) 9, 3 (1987), 319–349.

[21] Andy Field. 2013. Discovering statistics using IBM SPSS statistics. sage.
[22] FIRST. [n. d.]. Common Vulnerability Scoring System. Retrieved March, 2022

from https://www.first.org/cvss/

[23] Mozilla Foundation. [n. d.]. Bugzilla Issue Tracking System. Retrieved March,

2022 from https://www.bugzilla.org/

[24] Recorded Future. [n. d.]. Exploiting old vulnerabilities. Retrieved March, 2022

from https://www.recordedfuture.com/exploiting-old-vulnerabilities/

[25] Marian Gawron, Feng Cheng, and Christoph Meinel. 2017. Automatic vulnera-
bility classification using machine learning. In International Conference on Risks
and Security of Internet and Systems. Springer, 3–17.

[26] Seyed Mohammad Ghaffarian and Hamid Reza Shahriari. 2017. Software vulnera-
bility analysis and discovery using machine-learning and data-mining techniques:
A survey. ACM Computing Surveys (CSUR) 50, 4 (2017), 1–36.

[27] Zhuobing Han, Xiaohong Li, Zhenchang Xing, Hongtao Liu, and Zhiyong Feng.
2017. Learning to predict severity of software vulnerability using only vulnera-
bility description. In 2017 IEEE International Conference on Software Maintenance
and Evolution (ICSME). IEEE, 125–136.

[28] Hazim Hanif, Mohd Hairul Nizam Md Nasir, Mohd Faizal Ab Razak, Ahmad Fir-
daus, and Nor Badrul Anuar. 2021. The rise of software vulnerability: Taxonomy
of software vulnerabilities detection and machine learning approaches. Journal
of Network and Computer Applications (2021), 103009.

[29] Tin Kam Ho. 1995. Random decision forests. In 3rd International Conference on

Document Analysis and Recognition, Vol. 1. IEEE, 278–282.

[30] Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu
Ubayashi. 2019. DeepJIT: an end-to-end deep learning framework for just-in-
time defect prediction. In 2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR). IEEE, 34–45.

[31] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: Dis-
tributed representations of code changes. In Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering. 518–529.

[32] Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.
2013. Why don’t software developers use static analysis tools to find bugs?. In
2013 35th International Conference on Software Engineering (ICSE). IEEE, 672–681.
[33] Pontus Johnson, Robert Lagerström, Mathias Ekstedt, and Ulrik Franke. 2016.
Can the common vulnerability scoring system be trusted? a bayesian analysis.
IEEE Transactions on Dependable and Secure Computing 15, 6 (2016), 1002–1015.
[34] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting
decision tree. Advances in Neural Information Processing Systems 30 (2017), 3146–
3154.

[35] Saad Khan and Simon Parkinson. 2018. Review into state of the art of vulnerability
assessment using artificial intelligence. In Guide to Vulnerability Analysis for
Computer Networks and Systems. Springer, 3–32.

[36] Junhyoung Kim, TaeGuen Kim, and Eul Gyu Im. 2014. Survey of dynamic taint
analysis. In 2014 4th IEEE International Conference on Network Infrastructure and
Digital Content. IEEE, 269–272.

[37] Kyriakos Kritikos, Kostas Magoutis, Manos Papoutsakis, and Sotiris Ioannidis.
2019. A survey on vulnerability assessment tools and databases for cloud-based
web applications. Array 3 (2019), 100011.

[38] Patrick Kwaku Kudjo, Jinfu Chen, Minmin Zhou, Solomon Mensah, and Rub-
ing Huang. 2019. Improving the accuracy of vulnerability report classification
using term frequency-inverse gravity moment. In 2019 IEEE 19th International
Conference on Software Quality, Reliability and Security (QRS). IEEE, 248–259.

[39] Triet Huynh Minh Le, Hao Chen, and Muhammad Ali Babar. 2020. Deep learning
for source code modeling and generation: Models, applications, and challenges.
ACM Computing Surveys (CSUR) 53, 3 (2020), 1–38.

[40] Triet Huynh Minh Le, Huaming Chen, and M Ali Babar. 2021. A Survey on
Data-driven Software Vulnerability Assessment and Prioritization. arXiv preprint

12

arXiv:2107.08364 (2021).

[41] Triet Huynh Minh Le, Roland Croft, David Hin, and Muhammad Ali Babar. 2021.
A large-scale study of security vulnerability support on developer Q&A websites.
In Evaluation and Assessment in Software Engineering. 109–118.

[42] Triet Huynh Minh Le, David Hin, Roland Croft, and M Ali Babar. 2020. PUMiner:
Mining security posts from developer question and answer websites with PU
learning. In The 17th International Conference on Mining Software Repositories.
350–361.

[43] Triet Huynh Minh Le, David Hin, Roland Croft, and M Ali Babar. 2021. Deep-
cva: Automated commit-level vulnerability assessment with deep multi-task
learning. In 2021 36th IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, 717–729.

[44] Triet Huynh Minh Le, Bushra Sabir, and Muhammad Ali Babar. 2019. Automated
software vulnerability assessment with concept drift. In 2019 IEEE/ACM 16th
International Conference on Mining Software Repositories (MSR). 371–382.
[45] Frank Li and Vern Paxson. 2017. A large-scale empirical study of security patches.
In 2017 ACM SIGSAC Conference on Computer and Communications Security.
2201–2215.

[46] Yi Li, Shaohua Wang, and Tien N Nguyen. 2020. DLfix: Context-based code
transformation learning for automated program repair. In The ACM/IEEE 42nd
International Conference on Software Engineering. 602–614.

[47] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. Vulnerability detection with
fine-grained interpretations. In Proceedings of the 29th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering. 292–303.

[48] Zhen Li, Deqing Zou, Shouhuai Xu, Zhaoxuan Chen, Yawei Zhu, and Hai Jin.
2021. Vuldeelocator: a deep learning-based fine-grained vulnerability detector.
IEEE Transactions on Dependable and Secure Computing (2021).

[49] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and Zhaoxuan Chen. 2021.
SySeVR: A framework for using deep learning to detect software vulnerabilities.
IEEE Transactions on Dependable and Secure Computing (2021).

[50] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun
Deng, and Yuyi Zhong. 2018. Vuldeepecker: A deep learning-based system for
vulnerability detection. arXiv preprint arXiv:1801.01681 (2018).

[51] Guanjun Lin, Wei Xiao, Jun Zhang, and Yang Xiang. 2020. Deep learning-based
vulnerable function detection: A benchmark. In International Conference on Infor-
mation and Communications Security. 219–232.

[52] Guanjun Lin, Jun Zhang, Wei Luo, Lei Pan, Yang Xiang, Olivier De Vel, and Paul
Montague. 2018. Cross-project transfer representation learning for vulnerable
function discovery. IEEE Transactions on Industrial Informatics 14, 7 (2018), 3289–
3297.

[53] Amalia Luque, Alejandro Carrasco, Alejandro Martín, and Ana de las Heras. 2019.
The impact of class imbalance in classification performance metrics based on the
binary confusion matrix. Pattern Recognition 91 (2019), 216–231.

[54] Guillermo Macbeth, Eugenia Razumiejczyk, and Rubén Daniel Ledesma. 2011.
Cliff’s Delta Calculator: A non-parametric effect size program for two groups of
observations. Universitas Psychologica 10, 2 (2011), 545–555.

[55] Shane McIntosh and Yasutaka Kamei. 2017. Are fix-inducing changes a mov-
IEEE
ing target? a longitudinal case study of just-in-time defect prediction.
Transactions on Software Engineering 44, 5 (2017), 412–428.

[56] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
Distributed representations of words and phrases and their compositionality.
arXiv preprint arXiv:1310.4546 (2013).

[57] Shunta Nakagawa, Tatsuya Nagai, Hideaki Kanehara, Keisuke Furumoto, Makoto
Takita, Yoshiaki Shiraishi, Takeshi Takahashi, Masami Mohri, Yasuhiro Takano,
and Masakatu Morii. 2019. Character-level convolutional neural network for
predicting severity of software vulnerability from vulnerability description. IEICE
Transactions on Information and Systems 102, 9 (2019), 1679–1682.

[58] Stephan Neuhaus, Thomas Zimmermann, Christian Holler, and Andreas Zeller.
2007. Predicting vulnerable software components. In The 14th ACM Conference
on Computer and Communications Security. 529–540.

[59] Van Nguyen, Trung Le, Olivier De Vel, Paul Montague, John Grundy, and Dinh
Phung. 2021. Information-theoretic source code vulnerability highlighting. In
2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 1–8.
[60] Van Nguyen, Trung Le, Tue Le, Khanh Nguyen, Olivier DeVel, Paul Montague,
Lizhen Qu, and Dinh Phung. 2019. Deep domain adaptation for vulnerable code
function identification. In 2019 International Joint Conference on Neural Networks
(IJCNN). IEEE, 1–8.

[61] NIST. [n. d.]. National Vulnerability Database. Retrieved March, 2022 from

https://nvd.nist.gov/

[62] Valentina Piantadosi, Simone Scalabrino, and Rocco Oliveto. 2019. Fixing of
security vulnerabilities in open source projects: A case study of apache http
server and apache tomcat. In 2019 12th IEEE Conference on Software Testing,
Validation and Verification (ICST). IEEE, 68–78.

[63] Serena Elisa Ponta, Henrik Plate, and Antonino Sabetta. 2018. Beyond metadata:
Code-centric and usage-based analysis of known vulnerabilities in open-source
software. In 2018 IEEE International Conference on Software Maintenance and

On the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Evolution (ICSME). IEEE, 449–460.

[64] Serena Elisa Ponta, Henrik Plate, Antonino Sabetta, Michele Bezzi, and Cédric
Dangremont. 2019. A manually-curated dataset of fixes to vulnerabilities of
open-source software. In 2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR). IEEE, 383–387.

[65] Sebastian Raschka. 2018. Model evaluation, model selection, and algorithm

selection in machine learning. arXiv preprint arXiv:1811.12808 (2018).

[66] Christophe Rezk, Yasutaka Kamei, and Shane Mcintosh. 2021. The ghost commit
problem when identifying fix-inducing changes: An empirical study of Apache
projects. IEEE Transactions on Software Engineering (2021).

[67] Solmaz Salimi, Maryam Ebrahimzadeh, and Mehdi Kharrazi. 2020. Improving
real-world vulnerability characterization with vulnerable slices. In The 16th ACM
International Conference on Predictive Models and Data Analytics in Software
Engineering. 11–20.

[68] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine
translation of rare words with subword units. arXiv preprint arXiv:1508.07909
(2015).

[69] Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang
Su, Yizhe Zhang, Chunyuan Li, Ricardo Henao, and Lawrence Carin. 2018. Base-
line needs more love: On simple word-embedding-based models and associated
pooling mechanisms. arXiv preprint arXiv:1805.09843 (2018).

[70] Yonghee Shin and Laurie Williams. 2013. Can traditional fault prediction models
be used for vulnerability prediction? Empirical Software Engineering 18, 1 (2013),
25–59.

[71] Vincent Smyth. 2017. Software vulnerability management: how intelligence

helps reduce the risk. Network Security 2017, 3 (2017), 10–12.

[79] Christoph Treude and Markus Wagner. 2019. Predicting good configurations for
GitHub and stack overflow topic models. In 2019 IEEE/ACM 16th International
Conference on Mining Software Repositories (MSR). IEEE, 84–95.

[80] Anthony J Viera, Joanne M Garrett, et al. 2005. Understanding interobserver

agreement: the kappa statistic. Fam med 37, 5 (2005), 360–363.

[81] Strother H Walker and David B Duncan. 1967. Estimation of the probability of
an event as a function of several independent variables. Biometrika 54, 1-2 (1967),
167–179.

[82] Huanting Wang, Guixin Ye, Zhanyong Tang, Shin Hwei Tan, Songfang Huang,
Dingyi Fang, Yansong Feng, Lizhong Bian, and Zheng Wang. 2020. Combin-
ing graph-based learning with automated data collection for code vulnerability
IEEE Transactions on Information Forensics and Security 16 (2020),
detection.
1943–1958.

[83] Peichao Wang, Yun Zhou, Baodan Sun, and Weiming Zhang. 2019. Intelligent
prediction of vulnerability severity level based on text mining and XGBboost. In
2019 Eleventh International Conference on Advanced Computational Intelligence
(ICACI). IEEE, 72–77.

[84] Supatsara Wattanakriengkrai, Patanamon Thongtanunam, Chakkrit Tan-
tithamthavorn, Hideaki Hata, and Kenichi Matsumoto. 2020. Predicting defective
lines using a model-agnostic technique. IEEE Transactions on Software Engineering
(2020).

[85] Mark Weiser. 1984. Program slicing. IEEE Transactions on software engineering 4

(1984), 352–357.

[86] Tao Wen, Yuqing Zhang, Ying Dong, and Gang Yang. 2015. A novel automatic
severity vulnerability assessment framework. Journal of Communications 10, 5
(2015), 320–329.

[72] Tim Sonnekalb, Thomas S Heinze, and Patrick Mäder. 2022. Deep security analysis

[87] Frank Wilcoxon. 1992. Individual comparisons by ranking methods. In Break-

of program code. Empirical Software Engineering 27, 1 (2022), 1–39.

[73] Georgios Spanos and Lefteris Angelis. 2018. A multi-target approach to estimate
software vulnerability characteristics and severity scores. Journal of Systems and
Software 146 (2018), 152–166.

[74] Georgios Spanos, Lefteris Angelis, and Dimitrios Toloudis. 2017. Assessment of
vulnerability severity using text mining. In The 21st Pan-Hellenic Conference on
Informatics. 1–6.

[75] Yaming Tang, Fei Zhao, Yibiao Yang, Hongmin Lu, Yuming Zhou, and Baowen
Xu. 2015. Predicting vulnerable components via text mining or software metrics?
An effort-aware perspective. In 2015 IEEE International Conference on Software
Quality, Reliability and Security. IEEE, 27–36.

[76] Chakkrit Tantithamthavorn, Ahmed E Hassan, and Kenichi Matsumoto. 2018.
The impact of class rebalancing techniques on the performance and interpretation
of defect prediction models. IEEE Transactions on Software Engineering 46, 11
(2018), 1200–1219.

[77] Haoye Tian, Kui Liu, Abdoul Kader Kaboré, Anil Koyuncu, Li Li, Jacques Klein,
and Tegawendé F Bissyandé. 2020. Evaluating representation learning of code
changes for predicting patch correctness in program repair. In 2020 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE). IEEE, 981–992.
[78] Maciej Tomczak and Ewa Tomczak. 2014. The need to report effect size estimates
revisited. An overview of some recommended measures of effect size. Trends in
Sport Sciences 1, 21 (2014), 19–25.

throughs in Statistics. Springer, 196–202.

[88] Yasuhiro Yamamoto, Daisuke Miyamoto, and Masaya Nakayama. 2015. Text-
mining approach for estimating vulnerability score. In 2015 4th International
Workshop on Building Analysis Datasets and Gathering Experience Returns for
Security (BADGERS). IEEE, 67–73.

[89] Wei Zheng, Jialiang Gao, Xiaoxue Wu, Fengyu Liu, Yuxing Xun, Guoliang Liu,
and Xiang Chen. 2020. The impact factors on the performance of machine
learning-based vulnerability detection: A comparative study. Journal of Systems
and Software 168 (2020), 110659.

[90] Weining Zheng, Yuan Jiang, and Xiaohong Su. 2021. VulSPG: Vulnerability
detection based on slice property graph representation learning. arXiv preprint
arXiv:2109.02527 (2021).

[91] Jiayuan Zhou, Michael Pacheco, Zhiyuan Wan, Xin Xia, David Lo, Yuan Wang,
and Ahmed E Hassan. 2021. Finding a needle in a haystack: Automated mining
of silent vulnerability fixes. In 2021 36th IEEE/ACM International Conference on
Automated Software Engineering (ASE).

[92] Xin Zhou, DongGyun Han, and David Lo. 2021. Assessing Generalizability of
CodeBERT. In 2021 IEEE International Conference on Software Maintenance and
Evolution (ICSME). IEEE, 425–436.

[93] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019.
Devign: Effective vulnerability identification by learning comprehensive program
semantics via graph neural networks. arXiv preprint arXiv:1909.03496 (2019).

13

