2
2
0
2

y
a
M
0
2

]

R
A
.
s
c
[

1
v
2
4
0
0
1
.
5
0
2
2
:
v
i
X
r
a

1

ALPINE: Analog In-Memory Acceleration with
Tight Processor Integration for Deep Learning

Joshua Klein1, Irem Boybat2, Yasir Qureshi1, Martino Dazzi2, Alexandre Levisse1,

Giovanni Ansaloni1, Marina Zapater3, Abu Sebastian2, David Atienza1

1Embedded Systems Laboratory (ESL),

´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Lausanne, Switzerland
2IBM Research Europe, Ruschlikon, Switzerland,
3HEIG-VD, Yverdon-les-Bains, Switzerland

to digital

Abstract—Analog in-memory computing (AIMC) cores offers
signiﬁcant performance and energy beneﬁts for neural network
inference with respect
logic (e.g., CPUs). AIMCs
accelerate matrix-vector multiplications, which dominate these
applications’ run-time. However, AIMC-centric platforms lack
the ﬂexibility of general-purpose systems, as they often have hard-
coded data ﬂows and can only support a limited set of processing
functions. With the goal of bridging this gap in ﬂexibility,
we present a novel system architecture that tightly integrates
analog in-memory computing accelerators into multi-core CPUs
in general-purpose systems. We developed a powerful gem5-based
full system-level simulation framework into the gem5-X simu-
lator, ALPINE, which enables an in-depth characterization of
the proposed architecture. ALPINE allows the simulation of the
entire computer architecture stack from major hardware compo-
nents to their interactions with the Linux OS. Within ALPINE,
we have deﬁned a custom ISA extension and a software library
to facilitate the deployment of inference models. We showcase
and analyze a variety of mappings of different neural network
types, and demonstrate up to 20.5x/20.8x performance/energy
gains with respect to a SIMD-enabled ARM CPU implementation
for convolutional neural networks, multi-layer perceptrons, and
recurrent neural networks.

I. INTRODUCTION

Deep neural networks (DNNs) have revolutionized the state-
of-the-art in a wide range of AI applications ranging from
computer vision to natural language and speech processing.
DNNs are composed of multiple consecutive layers, and their
ability to address tasks often increases with the number and
size of layers. Today, modern DNNs are composed of hundreds
of layers and millions of weights, requiring massive amounts
of computation and memory [1], [2]. In the quest for achieving
higher accuracy and throughput, the AI domain suffers from
constant changes in the type and nature of the DNNs.

One of the main contributing factors to the expansion of
DNNs has been the introduction of more powerful CPUs and
GPUs. However, as DNNs become deeper and more complex,
their large memory footprint results in on-chip and off-chip
memory bottlenecks. Indeed, the extreme data-intensive nature
of DNNs mandates the design of computers that are efﬁcient
in storing, accessing and processing large amounts of data.
To improve efﬁciency and increase throughput, low-precision
number formats for weights and/or inputs can be adopted
(e.g., from 64/32-bit ﬂoating point to 8/4-bit integer). The
most extensive examples are the Google Tensor Processing
Units (TPUs) and the tensor cores of new NVIDIA GPUs [3],

[4]. Yet, despite these techniques, the time and energy for
accessing the data still dominate over the data processing,
even in the more aggressive technology nodes [3]. Data has to
be brought from the memory to the processing unit (CPU or
accelerators such as GPUs), and the result should be written
back to the memory after processing.

The distinction between processing and memory can be
blurred with custom designed memory, which look beyond
conventional von Neumann architectures. With analog in-
memory computing (AIMC) certain computations directly take
place where the data is located, exploiting device physics
and circuit laws [5]. In addition to signiﬁcantly reducing the
data movement, AIMC core enables the execution of millions
of operations (such as multiply-and-accumulate, MACs) in
parallel, greatly outperforming GPUs and other accelerators.
One approach to exploit in-memory computing for DNNs
is to design stand-alone accelerators where multiple AIMC
tiles and associated digital logic blocks are interconnected by
a suitable communication fabric [6], [7]. In such a multi-tile
accelerator, weights associated with different neural network
layers can be mapped to different AIMC tiles and data can be
propagated via tile-to-tile communication. Yet, these designs
fall short on supporting (1) a variety of neural networks
types, (2) changes in inter-layer connectivity and data-ﬂow, (3)
different processing and activation functions, and (4) multiple
number formats (e.g. quantization levels), all of which require
a high degree of ﬂexibility, difﬁcult to implement solely in
hardware. Combining AIMC with general purpose processors
can be key for powerful and cost-effective AI platforms,
completely avoiding the need for designing generations of
dedicated accelerators to cope with the rapid advances in DNN
workloads and models.

One way to address the limitations of standalone AIMC-
based accelerators is to add local CPUs [8], [9], [10], [11].
The CPU-AIMC interplay nonetheless often is the run-time
bottleneck in these systems, as AIMC tiles at competitive
technology nodes typically operate on the order of hundreds
of nanoseconds [12], [13]. Any communication overhead with
the CPU translates into unutilized AIMC tile compute cycles
and hence a performance loss. Therefore, it is essential to
work towards a tight integration to fully realize the potential
of AIMC-based acceleration, including support for a software
ecosystem so that systems can be easily conﬁgured to imple-
ment a wide range of neural networks.

 
 
 
 
 
 
In turn, the realization of this vision requires, as a nec-
essary precondition, the availability of ﬂexible, accurate and
hardware-validated simulation frameworks. Such frameworks
are crucial to perform the fast exploration of different AIMC
integration options from a system-wide viewpoint. They must
be capable of targeting entire and complex applications, eval-
uating the cost/performance of their accelerated and non-
accelerated parts (as well as their interplay),
taking into
account computation and communication.

Towards this end, we herein illustrate a detailed system
architecture exploration with a novel full system simulation
framework, named ALPINE (or “Analog In-Memory Accel-
eration with Tight Processor Integration for Deep Learning”).
ALPINE instances feature AIMC crossbars as dedicated com-
ponents, realised as tightly coupled accelerators, which allow
the storage and processing of megabytes of data in constant
time complexity. We show that such integration can be realized
without hampering the ﬂexibility of CPUs. Moreover, existing
hardware and software stacks can be leveraged by only adding
lightweight custom extensions to the instruction set in order
to govern the AIMC tile. Our contributions are as follows:

• We propose a new system simulation framework, extend-
ing industry-standard gem5 (with gem5-X extensions),
that allows the modeling of systems with AIMC tiles.
• Using this simulation framework, we model tightly in-
tegrated AIMC tiles, governed by custom instructions
extending the ARMv8 64-bit instruction set architecture
(ISA) with custom instructions.

• We introduce a custom software library, AIMClib, to show
how AIMC models can be leveraged from the software
programmer perspective and streamline the software de-
velopment process.

• We showcase the mapping of different artiﬁcial neu-
ral network (NN) types (MLP, LSTM, CNN) onto the
proposed architecture in both single-core and multi-core
cases. We obtain up to 20.5x performance and up to
20.8x energy beneﬁts (in CNNs) with respect to the multi-
threaded CPU + SIMD (ARM NEON) implementation.
• Using the aforementioned NNs as case studies, we
analyze the prevalence of matrix-vector multiplication
(MVM) operations as a computational hot-spot in DL
workloads, and quantify the application-wide beneﬁts
achievable by acceleration via tightly-coupled AIMC
tiles, including up to 12.8x/12.5x speedup and energy
improvement in MLPs and 9.4x/9.3x speedup and energy
improvement in LSTMs.

II. RELATED WORK

A. Full system simulations

Custom hardware extensions and accelerators can be mod-
eled and simulated at various abstraction levels. Hardware
description language (HDL) and register-transfer level (RTL)
simulation frameworks offer the most detailed view of hard-
ware being designed, in addition to a direct path to synthesis
and eventually fabrication. Yet, the development effort, synthe-
sis/compilation time, and simulation time costs are very high,

2

even with higher levels of hardware abstraction made possible
by high level synthesis [14].

While the overhead costs of novel hardware development
may still be worth the effort for uniquely low-level pieces of
hardware (e.g. new functional units), AIMC tiles and similar
accelerator technologies are made to work with user-space
applications (neural network inference) running on top of an
operating system and virtual memory sub-system. The time
for simulating full computer hardware and software stacks in
conventional HDL-level simulators goes up dramatically and
is therefore not scalable when the operating system needs to
boot, load user-space programs, and then keep track of OS
and low-level processes such as services and interrupts [14].
Due to this complexity, an abstract approach is more suit-
able for investigating system architectures in a more ﬂexible
manner. In broad terms, full system-level simulators simulate
crucial components of a computer (CPU cores, functional
units, buses, caches, memory, and more) as black boxes in
software with tunable latencies and attributes. As a result,
simulations can be run signiﬁcantly faster and with higher
degree of ﬂexibility than that of HDL-level simulators. The
premiere full system-level simulator in academia and industry
is gem5, which is an open-source full system-level simulator
that supports multiple ISAs (x86, ARM) and hardware models
upon installation, in addition to being regularly updated with
support from academia and industry[15].

Because critical hardware components are simulated in soft-
ware, full system-level simulators like gem5 must be validated
with respect to real hardware in order to produce performance
results representative of that one would attain using a HDL-
level or RTL simulator. For all of our applications and ex-
periments, we use gem5-X, an extended open-source version
of the gem5 simulator. gem5-X is shown to operate with less
than 5% error on performance statistics in comparison to a
real ARM Juno platform (developed by ARM in 2015) [15].
All of our experiments simulate the full computer architec-
ture stack, including user-space programs running on top of
Linux 4.3 and an Ubuntu LTS 16.04 disk image. The simulated
full system includes validated hardware models for the CPUs
and memory hierarchy.

B. Simulations of AIMC-based systems

Research efforts investigating AIMC for inference have, for
the most part, focused on achieving iso-accuracy with digital
systems. Prior work includes iso-accuracy studies for convo-
lutional neural networks (CNNS) [16], [17], [18], recurrent
neural networks (RNNs) [19], [17] and transformers [20].
However, very few papers have discussed how to model the
potential beneﬁts of AIMC at system level or discuss the
integration of AIMC for system acceleration.

Most of the works in this space are illustrated as part
of architectural and compilation frameworks studies, and are
therefore tightly linked to a hardware/software ecosystem.
Among them, Kourtis et al. [21] introduce a software stack
to automate the mapping of CNNs (described in high-level
language) onto multi-tile AIMC accelerators. As part of their
contribution, the authors showcase a cycle-accurate simulator.

3

Fig. 1. Analog in-memory acceleration for neural network inference. (a) Phase-change memory (PCM) device with true analog storage capability (32
representative levels shown on real hardware experimentally). (b) Matrix-vector multiplication with PCM devices. (c) Analog in-memory computing (AIMC)
tile with PCM array, data converters and digital control unit. (d) Weights of MLPs, RNNs and CNNs represented with AIMC tiles.

Similarly, Ankit et al. [9] describes an ISA and compiler
dedicated to programming and utilizing a multi-tile AIMC
accelerator, and an associated architectural simulator (named
PUMASim) to evaluate the energy and performance of com-
piled applications. The scope of both these approaches is
limited to the modelling of the AIMC accelerator alone,
neglecting other system components.

Ambrosi et al. [22] propose, as part of their ONNX com-
pilation framework, a set of simulators at three abstraction
levels: performance (low-level hardware simulation of single
AIMC instructions), functional (component-level simulation
of AIMC tiles and associated memories) and system. Our
research contribution is most closely related to the latter, which
is based on the QEMU emulator and aims at investigating
the execution performance of entire applications. However,
the system simulator in [22] does not take into account the
interaction between applications and operating systems, nor
does it consider the interplay between AIMC tiles and the rest
of the system, including CPUs and the memory hierarchy.

Most closely related to our approach is the paper of Vieira
et al., which details a full-system evaluation strategy of
AIMC acceleration. As in our case, the authors also base
their approach on AIMC-dedicated extensions to the gem5
environment [23]. Nonetheless, their approach is limited to
modelling the simple case of binary CNNs, and their per-
kernel mapping strategy does not scale to the larger and more
general applications we tackle in this paper. Moreover, as
opposed to our ISA extension enabling multi-core CPU sys-
tems with multiple AIMC tiles, their extension only supports
a single-core CPU and a single AIMC tile.

Finally, Ottavi et al. [24] proposes and synthesizes a het-
erogeneous architecture where an AIMC tile sits within a
cluster of RISC-V processors. They perform a design-space
exploration using the bottleneck of MobileNetV2 CNN where
point-wise and depth-wise convolutions are placed within the
AIMC tile. However, their synthesis approach hinders quick
exploration of alternative architectures and model runs.

III. BACKGROUND ON ANALOG IN-MEMORY
ACCELERATION

A. Analog in-memory computing paradigm

AIMC offers signiﬁcant advantages in terms of energy
and performance owing to two key properties. First,
the
computation takes place in the memory and therefore, the
expensive data movement can be avoided (addressing the
memory read bottleneck). Secondly, the computation can be
done in a massively parallel and analog manner by exploiting
the physical attributes and state dynamics of memory de-
vices. SRAM-based AIMC approaches are attractive owing to
SRAM’s technological maturity and scalability to aggressive
CMOS nodes [25], [26]. One drawback with this approach
is that only a single bit can be stored in an SRAM cell. An
alternative is to adopt AIMC based on non-volatile memory
technologies, including 2D [27] and 3D Flash [28], phase-
change memory (PCM) [13], and resistive random-access
memory (RRAM) [12]. These technologies offer analog data
storage capability, i.e. the ability to achieve a continuum of
resistance/conductance values (Fig. 1(a)). Their non-volatile
nature makes them particularly attractive for low-power em-
bedded applications as non-volatile memory-based AIMC tiles
consume negligible static power.

In this paper, we focus on PCM for AIMC, which is
arguably the most mature technology among the class of
resistance-based or memristive memory devices (Fig. 1(a)).
PCM devices have the potential to scale to nanoscale di-
mensions and can be integrated in the back-end of a CMOS
chip [13]. PCM-based implementations hence offer high per-
formance densities (TOp/s/mm2), where a pair of PCM devices
can represent signed multi-bit weights [16].

MVM operations, which form the bulk of computation for
DNN models, can be implemented in a PCM crossbar by rep-
resenting the elements of a M xN matrix as the conductance
values of memory devices, as shown in Fig. 1(b). Each element
of an input vector is translated into the duration of a voltage
pulse with ﬁxed amplitude V . The voltage pulses are applied
simultaneously to M word lines and each memory device
contributes to the current ﬂowing through one of the N bit
lines, with an amount directly proportional to its conductance

Control unitOutput memoryInput memoryDigital-to-analog converters8-bit digital input8-bit digital output...........................FC layerFC layerMLP Dense layerRNNAIMC tileAnalog-to-digital converters............................................................................................................Crystalline phaseAmorphous phase010203040500,00,20,40,60,81,0Cumulative distributionG [μS]Phase-change memoryt1tintG1G2Analog MVMt2tintVV..........................................................................................................................................CNN LSTM cell(a)(b)(c)(d)G (Ohm’s law). The total current integrated on each of the
bit lines over a certain period of time tint is indicative of
the result of the dot product between the M -element vector
and a column of the M xN matrix (Kirchoff’s current law).
Hence, the multiplication of an M xN matrix with an N -
element vector can be performed in a constant amount of time,
(in the range of 10s to 100s of nanoseconds [13]).

B. Analog in-memory compute tiles

An AIMC tile contains digital-to-analog and analog-to-
digital converters (DACs and ADCs) with dedicated registers
and a local controller, alongside the memory crossbar array
of unit cells, as presented in [13]. DACs convert the signed
digital input into a voltage pulse; the pulse amplitude is applied
either as V or −V according to the sign and the duration of
the pulse is proportional to input magnitude. The dot product
over the bit line is converted to a signed digital output via
ADCs. Each signed weight is represented with a pair of PCM
devices; therefore, a differential bit line current is received
by the ADC. The local controller orchestrates the data ﬂow
from the data bus into the DAC registers and out of the ADC
registers to the data bus. It also activates the MVM operation
when input data has arrived into the DAC registers.

In our design, we have a dedicated DAC and ADC for each
word line and bit line, respectively. The resolution of DACs
and ADCs are signed 8-bits. The input signal is scaled and
quantized in digital prior to its transfer to the AIMC tile. This
input scaling factor can be arbitrarily selected, preferably ﬁxed
to avoid dynamic scaling. Similarly, the ADC quantizes the
output of the MVM operation.

C. Computation precision with analog in-memory computing

In addition to the quantization introduced by DACs/ADCs,
the weights stored in the memory crossbar also have a low pre-
cision. Yet, the nature of the precision loss for the weights with
analog computing is substantially different from the weight
quantization of a digital implementation. The programming
and reading of analog weight value are prone to various non-
idealities, including noise, temporal conductance ﬂuctuations
and temperature-induced variations [29], [30].

The scalar multiplication of an analog input with PCM-
based weights is shown to be comparable to an implementation
with 4-bit ﬁxed-point inputs and weights [31], and even to
an implementation with 8-bit ﬁxed-point inputs and weights
with suitable innovations in device design [32]. To counter
the reduced weight precision, one can employ noise during
training, so that the model is more robust when performing
inference on AIMC tiles [16]. An alternative approach is to
encode weights using multiple PCM devices [19]. Despite
the reduced precision weights, AIMC implementations were
shown to address the inference of MLPs [29], [30], CNNs [16],
[29], [30], RNNs [19], [29], [30], and transformers [20] with
high accuracies.

IV. ARCHITECTING ALPINE SYSTEMS
In this section, we give an overview of the potential system
architectures that can be supported with the ALPINE frame-
work, discuss the input and output interfaces used to access

4

Fig. 2. A high-level overview of two AIMC-enabled system architectures.
(a) shows the classic loosely-coupled architecture where the CPU accesses
an off-chip cluster of AIMC tiles via the I/O bus. (b) conﬁgures each CPU
core to have direct access to its private AIMC tile in the proposed novel
tightly-coupled architecture.

Fig. 3. A high-level overview of how a tightly-coupled AIMC tile could
interact with an ARM Cortex-A55 pipeline. A dedicated line from an AIMC
control pipeline can send signals and data directly to and from the AIMC
tile’s control unit, akin to a co-processor.

and interact with AIMC tiles, and then ﬁnally present how to
interface AIMC tiles from a software designer perspective.

A. AIMC integration strategies

Loosely-coupled AIMC tile-enabled systems: A high-
level overview of a loosely-coupled AIMC-enabled system,
similar to those seen in [8], [10], can be seen in Fig. 2 (a).
Here, the AIMC tiles are peripheral devices that communicate
with the CPU cores via the I/O bus. CPU cores use load
and store instructions to read and write data to a particular
memory-mapped addresses. The AIMC tiles are typically
organized as a multi-tile accelerator with a control unit that
parses the incoming data so that one or multiple AIMC tiles
within the accelerator can be accessed.

CPU Core ClusterI/O BusSoC Peripheral DevicesAIMC Cluster...CPU Core ClusterI/O Bus(b)(a)SoC Peripheral DevicesPrivateAIMC TileInstructionFetchInstructionDecodeALU PipelineALU PipelineLoad/Store PipelineInstruction IssueInput MemoryOutput MemoryCtrlWritebackARM Cortex-A55 PipelineAIMC TileAIMC CTRLInstruction QueueL1 Cache(a) High-level Data-ﬂow with ISA Extension

(b) Proposed CM Instruction Deﬁnitions

Op
CM QUEUE
CM DEQUEUE
CM PROCESS
CM INITIALIZE

OpCode Rm R/W Ra Rn Rd
Ra Rn Rd
1
0x108
X Rn Rd
0
0x108
X
0
0x008
Rd
X
Ra Rn Rd
0
0x208

Rm
Rm
X
Rm

Fig. 4. (a) Visualization of the AIMC tile data-ﬂow when using the ALPINE
ISA extension. (b) The instruction deﬁnitions.

Tightly-integrated AIMC-enabled systems: To overcome
the communication overhead over the I/O bus, we propose a
novel tightly-coupled conﬁguration, as seen in Fig. 2 (b). Here,
the CPU uses an ISA extension to access private AIMC tiles
that are unique to each of the CPU cores, without requiring
the traversal of the memory hierarchy (Fig. 3).

B. Interfacing Tightly-integrated AIMC tiles

In this section, we present the ISA extension for the tight
AIMC integration. We implement the instructions using the
previously unused opcodes in the ARMv8 architecture, as
listed in Fig. 4 (b). Prior to inference,
the AIMC tile is
programmed through the CM INITIALIZE instruction, which
writes 8-bit weight values to the indices of the AIMC crossbar.
Active during the inference (our region of interest) are the
other three instructions, which are utilized as follows.

The program packs 8-bit

inputs into a 32-bit argument
register. CM QUEUE is then called to place the packed inputs
into the input memory of the AIMC tile. Additional argument
registers are used to specify the number of valid inputs packed
in the aforementioned argument register, as well as the input
memory index. Once all of the inputs are queued into the input
memory, CM PROCESS is called to operate the AIMC tile by
converting the values from input memory into analog voltages
via DACs, performing the MVM operation with the stationary
weights, and storing the results in the output memory after
digitizing them with ADCs. Finally, CM DEQUEUE is called
to retrieve packed 8-bit outputs from the AIMC output memory
and place them in a destination register. The argument registers
specify the number of packed outputs to retrieve and the index.
A visualization of this process is in Fig. 4 (a).

C. Interfacing tightly-integrated AIMC-Enabled systems

5

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

#include "aimclib.hh"

int main(int argc, char * argv[]) {

// Mapping weights to the crossbar with
// x, y offset of 0, 0 using AIMClib.
int8_t ** weights = ...
mapMatrix(0, 0, M, N, weights);

for (int i = 0; i < N_INFERENCES; i++) {
// Queue input array into the AIMC
// tile input memory using AIMClib.
queueVector(sizeof(input) /

sizeof(input[0]), input[i]);

// Perform MVM using AIMClib.
aimcProcess();

// Dequeue output memory contents into
// output array using AIMClib.
dequeueVector(N, output[i]);

}

return 0;

}

Fig. 5. A sample C++ code for a single fully-connected feed-forward layer
programmed onto AIMC tiles with AIMClib. At each inference step, the input
is loaded and queued into the input memory. This is followed by the MVM
via the aimcProcess method. Finally, the contents of the AIMC core output
memory are dequeued straight into an output matrix data structure.

AIMC-accelerated applications. This library is coded in C
and can be used with C or C++ applications. In addition to
containing the intrinsics in convenient in-lined wrapper meth-
asm), it includes numerous
ods (that use the C++ built-in
functions and templates, such as the ability to queue/dequeue
whole array or vector data structures of AIMC tile input/output
memories, tiling matrices at offsets in the crossbar (so that
to
multiple matrices of varying sizes can be placed next
each other), type-casting for tile inputs and outputs between
int8 t and higher precision types (e.g.
fp32), performing
activation functions and other digital processing operations
on tile outputs, and a checker program that simulates tightly-
coupled AIMC tiles in guest software so that programs that
utilize AIMClib can be debugged on the host machine before
engaging the real or simulated hardware.

A sample of C++ pseudo-code using AIMClib is presented
in Fig. 5. In addition to its own methods, we have also
used AIMClib in conjunction with the Eigen C++ library
(version 3.8). Eigen optimizes data structure space utilization
and access as well as incorporates SIMD vector operations in
our test applications [33].

V. SYSTEM SIMULATION

In this section, we discuss the technical details of the gem5-
X integration of AIMC tiles within the ALPINE framework.

A. Modeling AIMC Tiles in gem5-X

AIMC tiles are implemented in gem5-X using two classes:
one class acts as a wrapper with metadata for setting up AIMC
tiles, and the second class contains the AIMC tile itself.

The ISA extensions provide low-level support for tight
AIMC integration, however we also developed a higher-level
software library, AIMClib, to facilitate the development of

The wrapper class is designed to encompass both the loose
and tight coupling of AIMC tiles. For implementing the
loose coupling, the wrapper is deﬁned as a gem5 Peripheral

1. CM_QUEUE3. CM_DEQUEUE2. CM_PROCESS1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

# AIMC wrapper class declaration.
class AnalogComputationalMemory(

BasicPioDevice):
type = ’AnalogInMemoryComputing’
cxx_header = "dev/arm/AIMC_Wrapper.hh"

class RealView(Platform):

type = ’RealView’

# Instantiation of the AIMC wrapper.
aimc = AnalogComputationalMemory()

# Connecting the AIMC tile on the bus.
self.aimc.pio = bus.master
...

Fig. 6. gem5-X Conﬁguration Script code example for placing the AIMC
wrapper class on the ARM SoC (RealView) platform. The instantiation is
done in a similar manner for the loose-coupled and tight-coupled integration.

Input/Output (PIO) device where it
is accessible by load
and store instructions at a speciﬁc memory-mapped address.
Alternatively, for the tight coupling case, the wrapper can be
accessed by dedicated instruction.

The AIMC tile class contains the actual crossbar and pe-
ripheral simulated modeling components. We base our AIMC
tile implementation on [13]. Each generated AIMC tile has an
input memory array, the memory crossbar, and output memory
array. The component dimensions are parameterizable in the
wrapper class conﬁguration.

B. AIMC-Enabled Systems in gem5-X

After the wrapper object and AIMC tile objects are deﬁned,
the next step is to instantiate them on the system and deﬁne
the proper system interfaces (e.g., functional units and bus
controller). This is realized in gem5-X systems through gem5-
X’s conﬁguration scripts, of which an example is found in Fig.
6. By itself, this allows for the system-level implementation
of classic loosely-coupled AIMC-enabled systems.To simulate
the tightly-coupled AIMC-enabled architectures, we extend the
accelerator modeling in [23] such that the custom ARMv8 ISA
extension can also interface peripheral I/O (PIO) devices like
our wrapper object. For this, we add connections between the
ISA extension and PIO device via the system object (e.g.,
the simulated system that is instantiated on gem5-X’s launch).
The latency of the custom instructions is parameterizable,
providing modeling ﬂexibility on the AIMC tile. The return
value of these instructions (the result held in the destination
register) can also be data sent from the AIMC tile to the CPU.
In our implementation for the initial exploration of tightly-
coupled AIMC-enabled architectures, we generate one AIMC
tile for each CPU core. Note that this is a design choice and the
ALPINE framework supports alternative system deﬁnitions,
including instantiating multiple AIMC tiles per CPU core, a
hard-coded number of AIMC tiles, or others.

VI. EXPERIMENTAL SETUP

6

TABLE I
EXPERIMENTAL SETUP

(A) gem5-X FS Mode System Conﬁgurations

System
CPU Core Model
Number of CPU Cores
ISA
CPU Core Frequencies
Supply voltage VDD
L1 Data/Instruction Cache Size
LLC Cache Sizes
Memory Model

Low-Power

High-Power

Minor (In-Order) CPU
8
ARMv8 (AArch64)

0.8GHz
0.75 V
32kB
512kB

2.3GHz
1.3 V
64kB
1MB

8GB DDR4 @ 2400MHz

(B) System Energy and Power Figures

System
Idle Core Energy (pJ/Cycle)
WFM Core Energy (pJ/Cycle)
Active Core Energy (pJ/Cycle)
Mem Controller + IO Power (W)
LLC Leakage (mW/256kB)
LLC Read Energy (pJ/Byte)
LLC Write Energy (pJ/Byte)
DRAM Energy (pJ/Access)

Low-power
10.72
46.04
60.92
3.03
271.62
1.81
1.63

120.0

High-power
126.03
638.99
845.39
5.82
874.08
5.60
5.02

(C) AIMC tile performance and energy ﬁgures

AIMC tile crossbar size
AIMC tile latency
AIMC tile input/output data throughput
Input/output memory SRAM capacity
Supply voltage VDD (analog)
Supply voltage VDD (digital)
256x256 AIMC tile MVM energy efﬁciency

M rows, N columns
100 ns
4 GB/s
M B/N B
0.8 V, 1.2 V
0.8 V
12.8 TOp/s/W *

* The AIMC tile MVM energy is re-calculated for varying tile sizes,
considering the crossbar array size as well as data converters.

rations to represent different use-cases, namely the high-power
system such as those in higher-end devices and the high-
performance computing domain and the low-power system,
tailored for the embedded domain and internet-of-things edge
devices. We would like to note that a substantial body of
work focuses on AIMC tiles in high-power context; yet, the
low stand-by power AIMC tile compels an exploration of its
integration in low-power contexts as well [24], [34]. We use
the MinorCPU model in our explorations, which is a 4-stage
pipelined CPU with data forwarding and branch prediction.

The power models are shown in Table I-(B). Our core and
cache power model is based on a 28 nm bulk system with an
ARM Cortex A53 core [15], while our DRAM power model is
based on [35]. The core and cache power model is comprised
of active and WFM (wait for memory) CPU core energy per
cycle, as well as energy/power for the last-level cache (LLC).
We calculate the total energy of the system using the
gem5-X statistics. The generated statistics include total CPU
cycles, simulated time, and cache/memory accesses for each
experiment run. The full system energy is then the sum of the
energies for the core, cache, and DRAM components.

A. Target Systems and Power Model

B. AIMC Setup and Modeling

The system speciﬁcations of our gem5-X simulation are
listed in Table I-(A). We deﬁne two different system conﬁgu-

Table I-(C) reports the performance and energy metrics
of the AIMC tile estimated from hardware measurements

7

and chip designs in 14 nm technology node [13], [36]. For
compatibility with the core and cache model in 28 nm node,
we upscale the AIMC tile power estimates with a scaling factor
of 5.3x for the high-power system and 2x for the low-power
system. These factors are calculated following the classical
scaling theory under constant frequency with the formulation
αβ2, where α denotes the dimensional scaling and β is the
voltage scaling factor [37].

Note that it is not a straightforward exercise to provide
a simple power scaling factor for a mixed-signal design,
such as our AIMC tile. One reason for this is the avail-
ability of different technology types or processes (e.g., high-
performance, low-power, planar, FinFET) with speciﬁcations
changing across foundries [38]. Secondly, digital and ana-
log circuits follow different power scaling trends with the
technology node [39]. Given that analog circuits scale less
aggressively in comparison to its digital counterpart, our
scaling represents a rather conservative estimate in this respect.
We assume that the AIMC tile performance remains constant
between the two technology nodes.

VII. EXPLORATION ONE: MULTI-LAYER PERCEPTRONS

A. The multi-layer perceptron architecture and cases

In this ﬁrst case study, we focus on a two-layer MLP neural
network (1024, 1024) with ReLU activation functions (Fig. 7
(a)). While this acts as a simple example, it is worth noting
that MLP-based implementations are becoming more prevalent
through recommender systems [40] and vision [41].

We create four different analog MLP implementations
where vary the size and number of AIMC tiles, as shown
in Fig. 7 (b). More speciﬁcally, Cases 1 and 2 are single-CPU
core architectures which use one large AIMC tile. Case 3 is
a dual-CPU core architecture with one fully-connected layer
assigned to each CPU core. Each AIMC tile is also smaller
in capacity relative to the previous cases. Finally, Case 4 is a
quad-CPU core architecture with one fully-connected layer’s
computation being split between two CPU cores. The two CPU
cores of the ﬁrst layer sync their outputs via mutexes before
letting the second layer start its processing. Additionally, we
compare these implementations with a conventional CPU-only
and SIMD-enabled architecture.

C. Exploration Studies Overview

To showcase ALPINE’s abilities and explore the beneﬁts of
tightly-coupling AIMC tiles in systems, we built and optimized
a wide variety of neural network models using the Eigen
C++ library and AIMClib. For multi-core experiments, we use
libpthread to pipeline layers across cores, and implement ping-
pong buffering to prevent input/output blocking.

With these tools in hand, we then perform three neural
network explorations. First, we consider a Multi-Layer Per-
ceptron (MLP) to gauge the performance and energy beneﬁts
of a neural network that almost solely relies on matrix-
vector multiplication (MVM) operations. Then we consider
an alternative neural network architecture, the Long Short-
Term Memory (LSTM),
that has increased computational
requirements outside the MVM. Finally, we look at the fully-
pipelined implementation of a Convolutional Neural Network
(CNN) and explore how the proposed system behaves in
the presence of large number of MVMs in conjunction with
very intense memory access patterns. We further breakdown
these applications into multiple cases with different AIMC
tile and CPU core mappings. All of these AIMC tile-enabled
neural networks and their implementations are compared
against a digital-only, SIMD-enabled, reference application.
Furthermore, for more equitable performance comparisons, we
use similar precision across all applications (int8 t with fp32
accumulation where ﬂoating point operations apply, such as
in sigmoid and softmax operations).

In general, inference-to-inference we notice a deviation in
performance results and system metrics of less than 4%. Thus,
to save on simulation time, we only perform 10 inferences for
each of the cases in the MLP and LSTM neural networks. We
further reduce the number of inferences in the CNNs to 3 due
to the larger network requiring more simulation time.

B. Single-Core Results and Analysis

Aggregate results for all of our MLP experiments are shown
in Fig. 8. We focus on the run-time, energy footprint and
memory intensity (quantiﬁed as last-level cache misses per in-
struction, or LLCMPI). The latter provides important insights
onto the data movement overhead, which is a signiﬁcant driver
for AIMC-based acceleration.

In all implementations, AIMC tiles provide signiﬁcant bene-
ﬁts in terms of latency and energy in comparison to the CPU-
only runs. Looking at the single-CPU implementations, we
observe that Case 1 out-performs Case 2 by a slight margin
in terms of latency and energy. While a similar amount of
queuing and dequeuing takes place in both implementations,
the CM PROCESS instruction needs to be called twice as
much in order to perform the same number of inferences
in Case 2. Yet, this does not translate into a 2x run-time
and energy-overhead, because the MVMs account for only
a small fraction of the total run time, as shown in Fig. 9.
This shows that it is critical to provide a sufﬁciently large
input queue/dequeue bandwidth to maximize the beneﬁts from
AIMC acceleration, as also discussed in [11].

C. Multi-Core Results and Analysis

The latency and energy for the multi-core Cases 3 and 4
are displayed in Fig. 8, as well as their time distribution in 9.
What is immediately apparent with respect to the single-CPU
core implementations is that adding multiple CPU cores does
not equate to more performance gains. In fact, the performance
and energy of the system worsens with increasing number of
CPU cores: the single CPU-core Case 1 has approximately
20% and 30% better run time over the dual CPU-core Case 3
and quad CPU-core Case 4, respectively.

This slowdown is mainly attributed to the communication
overhead of sending inputs and activations across CPU cores.
As the number of CPU cores increase, we observe that the

8

Fig. 7.
mappings of the full-connected layers to a variety of AIMC-based conﬁgurations.

(a) Multi-layer perceptron architecture with two dense (fully-connected) layers (1024, 1024) and ReLU activation. (b) Cases 1 through 4 shows the

Fig. 8. Aggregate results for multi-layer perceptron experiments. From left to right, each column contains total time, memory intensity, and energy results
for the High-Power system (top row) and Low-Power system (bottom row) conﬁgurations. ”ANA” refers to analog AIMC-enabled application mappings with
implementation numbers corresponding to those in Fig. 7, while ”DIG” refers to a digital reference or CPU-only implementation. Results are also grouped
by the number of CPU cores utilized (1, 2, or 4).

in the quad-CPU core implementation Case 4 where the input
from memory as well as the intermediate activations must be
sent to two different CPU cores. The synchronization overhead
associated with mutexes of both layers aggravate this as well.
Interestingly, the memory intensity remains almost constant
across all implementations (Cases 1 through 4), suggesting
that memory access is no longer a signiﬁcant bottleneck.
Instead, we conclude that for an application whose run-time
is dominated by input load and analog queue, the overhead
of sharing data is no longer negligible and should be treated
as the primary bottleneck to gains in run time and energy.

Finally, across all cases,

the low-power system exhibits
lower performance gains in comparison to the high-power
system. This is primarily due to the smaller L1 cache size of
the low-power system conﬁguration. A smaller cache requires
more requests (and therefore endures more delay) to the L2
cache and memory, which is reﬂected in the slightly higher
memory intensity metric relative to the high-power system.

D. MLP Computational Complexity

Fig. 9. The run-time analysis of MLP cases. Run-time percentage of each
sub-ROI division in MLP cases averaged across high-power and low-power
systems. The average reference is obtained across single-, dual-, and quad-
core digital cases. Non-MVM Digital Operations refers to the combined run
times of the input load, digital activation, and output writeback. The standard
deviation of the timing distribution is less than 1.7%, 1.2%, 2.3%, and 1.5%
for the average reference, Case 1, Case 3, and Case 4, respectively

run-time associated with input load and analog queue make
up a larger portion of the overall run-time (Fig. 9). More
speciﬁcally, in the dual-CPU core implementation Case 3, the
overhead of communicating the output from the ﬁrst layer to
the input of the second layer signiﬁcantly increases the total
run time of the application. The overhead is then compounded

In this section, we present a computational complexity
analysis for the CPU-only and AIMC-based implementations.
We will assume that the limited cache size and cache trashing,
as well as the SIMD operations, do not impact the computa-
tional complexity. For the CPU-only run, each fully-connected
layer’s MVM operation has a quadratic complexity (O(n2)),

x1x2xnInput∑∑∑Dense Layer 1ReLUWijReLUReLU∑∑∑ReLUReLUReLUDense Layer 2y1y2ynOutputWkl............Case 1: 2k x 2k AIMCXYCase 2: 1k x 2k AIMCCase 3: 2 1k x 1k AIMCsCase 4: 4 1k x 512 AIMCsXYXYXY(a) MLP Network Architecture(b)0.000.010.020.03Time (s)Single-CoreDual-CoreQuad-Core12.8x5.1x3.1xHigh-Power0.000.020.040.06LLCM / InstructionSingle-CoreDual-CoreQuad-Core6.5x4.0x5.1xReferenceAIMC-Enabled0.000.040.080.12Energy (J)Single-CoreDual-CoreQuad-Core12.5x5.4x3.5xDIGANACase 1ANA2DIGANACase 3DIGANACase 40.000.020.040.06Time (s)8.3x3.5x2.2xLow-PowerDIGANACase 1ANA2DIGANACase 3DIGANACase 4Cases0.000.020.040.06LLCM / Instruction4.6x3.5x4.3xDIGANACase 1ANA2DIGANACase 3DIGANACase 40.000.010.030.04Energy (J)8.4x4.0x2.7x4.0%96.0%Average Reference15.2%39.2%0.7%29.2%15.7%ANA Case 1Input LoadAnalog QueueMVM OperationDigital Act/DequeueOutput WritebackNon-MVM Digital Operations20.5%42.2%1.3%24.8%11.2%ANA Case 331.9%49.4%1.4%10.0%7.3%ANA Case 4while the corresponding activation function (ReLU), as well as
loading initial inputs (input load) from memory and storing
outputs (output writeback), has a linear complexity (O(n)).
Given that the MLP experiments run for Ninf inferences,
the total complexity of the MLP run can be formulated as
Ninf ∗ (2O(n2) + 4O(n)) ≈ O(Ninf n2).

With the introduction of the AIMC tiles however,

the
complexity of MVM operations reduces to constant
time
(O(1)), assuming that the entirety of the weights of the fully-
connected layer can ﬁt in the AIMC tile. Therefore the total
computational intensity reduces to Ninf ∗ (2O(1) + 6O(n)) ≈
O(Ninf n) after including complexities for analog queueing,
shifting the dominating run-time factor to the linear operations
(queuing inputs/dequeuing outputs; Fig. 9).

E. MLP Memory Requirements

In this section, we analyze the memory footprint for the
CPU-only and AIMC-based implementations. In the CPU-only
implementation, the weights of the fully-connected layers must
be loaded from the main memory into L1/L2 caches at every
inference. Yet, this is not true in the analog implementations.
Let us deﬁne the working set as the required amount of data
memory per inference. In the CPU-only implementation, this
includes the weights of the fully connected layers (2W ), the
inputs loaded from memory (x), the intermediary activations
(l1), and the ﬁnal outputs stored to memory (y). For our imple-
mentation with 8-bit weights, inputs, activations and outputs,
the working set size is 2W +x+l1 +y = 2∗n2 +3n ≈ 2.1MB
for n = 1024. For all of our experimental conﬁgurations, this
working set size exceeds that of both the private L1 caches and
the shared L2 cache, meaning elements of the fully-connected
layers and the input/output must be thrashed (swapped in and
out of the caches, as well as potentially main memory), leading
to both worse memory performance and worse total run time.
In contrast, the AIMC-enabled MLP keeps all of the weights
of the fully-connected layers stationary inside the AIMC tiles.
After the one-time cost for programming the weights in our
MLP, the weights are never reprogrammed, and therefore, can
effectively be removed from the working set. In this case, the
working set size can be formulated as x + l1 + y = 3n ≈ 3kB,
which ﬁts comfortably in L1 private caches for both of our test
system conﬁgurations. This leads to lower memory intensity,
less cache thrashing, and thus improved overall performance.
The reduction in computation and throughput requirements
resulting from the introduction of the AIMC tiles in the single-
CPU core cases 1 and 2 is related to the ones obtained
with multi-threading (cases 3 and 4), with the caveat that the
computational complexity goes down by the number of hard-
ware threads and space complexity is distributed across the
CPU cores. However, even though the space complexities are
reduced, the impact of the linear computational complexities
of input load and analog queue are increased by the emerging
core-to-core communications bottleneck in the multi-CPU core
applications (cases 3 and 4).

We therefore reiterate that when AIMC tiles are introduced
to neural networks with very small digital operation require-
ments (such as only ReLU activation functions), that core-
to-core communications overhead should be minimized by

9

TABLE II
LSTM EXPERIMENT SETUP

(A) LSTM Neural Network Parameters

Input (x)
50
50
50

Hidden Layer (nh)
256
512
750

Output (y)
50
50
50

Total Parameters
377.3k
1.28M
2.6M

(B) LSTM AIMC Tile Dimensions

nh
256
512
750

Case 1
612 x 1074
1124 x 2098
1600 x 3050

Case 2
356 x 1074
612 x 2098
850 x 3050

Case 3
356 x 1024
612 x 2048
850 x 3000

Case 4
356 x 256
612 x 512
850 x 750

using fewer CPU cores and AIMC tiles possible. Furthermore,
greater performance gains can be achieved by localizing
digital logic requirements such that more linear operations are
performed by the same CPU.

VIII. EXPLORATION TWO: LONG SHORT-TERM MEMORY

A. LSTM Architecture

In our second exploration, we look at recurrent neural
networks (RNNs) in the form of a Long-short term memory
LSTM targeting character recognition using the Penn Treebank
(PTB) data set [42]. The LSTM has one cell (hidden) layer
and one fully-connected layer, as presented in Fig. 10 (a).
In comparison to the MLP, the LSTM features more compu-
tationally heavy digital operations (sigmoid, tanh, softmax).
Moreover, the data ﬂow bears differences owing to the recur-
rent connection of the LSTM cell. Figure 10 (b) shows the
different simulated cases. Cases 1 and 2 are single-CPU core
cases that use larger AIMC cores. Case 3 is a dual-CPU core
case with the cell layer assigned to the ﬁrst CPU core, and the
dense layer assigned to the second CPU core. Finally, Case 4
is a quin-CPU core case with the cell layer’s computation split
across the ﬁrst four CPU cores, and the dense layer assigned
to the last CPU core. Here, the four CPU cores associated
with the LSTM cell sync their outputs via mutexes before the
second layer starts its MVM operation.

In this exploration, we focus on LSTM instances sharing
the same architecture but with different
layer sizes. The
dimensions of the layers in these networks, as well as the
corresponding AIMC tile sizes, are listed in Table II. To
reduce CPU core-to-core communication as much as possible
in case 4, the LSTM cell layer is mapped to AIMC tiles such
that instead of the gates being distributed to different AIMC
tiles, they are sliced so that element-wise operations can be
performed by reading four consecutive columns [36].

We would like to note that the largest LSTM architecture
(nh = 750) is shown to experimentally yield high accuracy
when implemented on real PCM prototype hardware chip [30].

B. Single Core Results and Analysis

Aggregate results for all of our LSTM experiments are
shown in Fig. 11, including multi-core results, results for

10

Fig. 10. (a) The neural network diagram for the LSTM modeled in our AIMC-enabled test programs. It is a 2-layer LSTM with one LSTM hidden cell layer
and one dense (fully-connected) layer with various, more compute-intensive, activation functions. Note that the AIMC sizes are variable depending on the
size of the hidden cell layer and that all activation functions are performed in the CPU cores. (b) Cases 1 through 4 shows the mappings of the layers to a
variety of AIMC-based conﬁgurations.

Fig. 11. Aggregate results for all LSTM experiments. From left to right, each column contains total time, memory intensity, and energy results for the
high-power system (top row) and low-power system (bottom row) conﬁgurations. ”DIG” refers to digital reference applications while ”ANA” refers to a
speciﬁc analog, AIMC-enabled application case, which correspond to those in Figure 10. Results are grouped by the number of CPU cores utilized, and from
left to right, each grouped column refers to a different nh parameter which affects the total size of the network. The darker bars refer to a larger nh.

both system conﬁgurations, and factor improvements with the
largest of the networks (nh = 750). When nh is 256, we
observe 1.0-1.5x factor improvements across all metrics and
system conﬁgurations in the AIMC tile-enabled cases over the
digital case due to the very small working set. However, when
nh increases to 512 and then 750, the run time and energy
of the digital application increases up to 9.4x/9.3x with the
working set size increase of 7x. This is in comparison to the
AIMC tile-enabled applications, which sees an average run
time and energy increase of 1.4x, suggesting a sub-quadratic
increase in run-time complexity with higher space complexity.

C. Multi-Core Results and Analysis

Similar to the single-CPU core cases, the multi-CPU core
cases also have signiﬁcant performance gains as nh grows
larger, in comparison to the digital implementation (Fig. 11).
Additionally and unlike with the MLP, going from single-
CPU core to multi-CPU core with the LSTM in the AIMC
tile-enabled implementations does result in a speedup of 10%
(cases 1 vs. 4), due to the LSTM cell’s parallelized linear
operations (but not more due to inter-layer communication).

D. LSTM Complexity

Relative to the ﬁrst case study with the MLP, we see
smaller maximum time and energy gains (9.4x/9.3x versus
12.8x/12.5x); this is expected as a greater proportion of the
LSTM total run-time is dedicated to digital operations that do
not see a reduced computational complexity with the introduc-
tion of the AIMC tile. However, these digital operations do see
mild performance improvements as a result of lower memory
intensity and therefore less cache thrashing (due to space freed
from lack of weights loaded in AIMC-enabled LSTMs).

In the digital reference application, the computation of the
LSTM cell outputs involve four MVM operations of quadratic
complexity (4O((nh) ∗ (x + nh)) ≈ 4O(n2)) and nine opera-
tions of linear complexity (sigmoid, hyperbolic tangent, array
multiplication, array addition; 9O(nh) ≈ 9O(n)). In addition
to the LSTM cell’s added complexity, the softmax operation is
used as the fully-connected layer’s activation function which
doubles in complexity when compared to ReLU (2O(y) ≈
2O(n)). This is in addition to loading inputs and storing

Input∑∑∑SoftmaxSoftmaxSoftmaxDense Layery1y2ynOutput......Case 1Y(a) LSTM Network ArchitectureWfWiWaWoHt-1Ct-1Xtσσσtanh×××+CtHtLSTM Cell LayertanhHt-1XYHt-1XCase 2Case 3Case 4YHt-1XYHt-1X(b)0.000.010.030.04Time (s)Single-CoreDual-CoreQuin-Core9.4x5.0x2.5xHigh-Power0.000.020.030.05LLCM / InstructionSingle-CoreDual-CoreQuin-Core8.1x9.9x9.6xReference (nh=256)AIMC-Enabled (nh=256)Reference (nh=512)AIMC-Enabled (nh=512)Reference (nh=750)AIMC-Enabled (nh=750)0.000.050.100.15Energy (J)Single-CoreDual-CoreQuin-Core9.3x5.0x2.9xDIGANACase 1ANA2DIGANACase 3DIGANACase 40.000.030.050.08Time (s)6.1x3.4x1.9xLow-PowerDIGANACase 1ANA2DIGANACase 3DIGANACase 4Cases0.000.020.030.05LLCM / Instruction5.7x7.0x6.2xDIGANACase 1ANA2DIGANACase 3DIGANACase 40.000.020.030.05Energy (J)6.4x3.7x2.4x(a) CNN Architecture

(b) CNN Parameters

CNN-F

64 11x11 kernels
stride 4, pad 0
x2 pool, LRN
256 5x5 kernels
stride 1, pad 1
x2 pool, LRN
256 3x3 kernels
stride 1, pad 1
256 3x3 kernels
stride 1, pad 1
x2 pool

CNN-M
224x224x3
96 7x7 kernels
stride 2, pad 0
x2 pool, LRN
256 5x5 kernels
stride 1, pad 1
x2 pool, LRN
512 3x3 kernels
stride 1, pad 1
512 3x3 kernels
stride 1, pad 1
x2 pool
4096 dropout
1000 dropout

CNN-S

96 7x7 kernels
stride 2, pad 0
x3 pool, LRN
256 5x5 kernels
stride 1, pad 1
x2 pool
512 3x3 kernels
stride 1, pad 1
512 3x3 kernels
stride 1, pad 1
x3 pool

1.7M

5.6M

5.5M

Layer
Input
conv1

conv2

conv3+4

conv5

dense1+2
dense3
AIMC
params

Fig. 12.
(a) The architecture of the CNNs presented in [43] and their
mapping onto the ALPINE systems. The blue boxes with the AIMC tiles
represent the convolutional layers. The dense layers are not mapped to AIMC
tiles. (b) shows the dimensions and parameters of each CNN. The CNN
has 5 convolutional layers (3 with Max Pooling), 3 dense layers, and ReLU
activation functions for all layers except the last layer, which uses Softmax.

results (O(x) + O(y) ≈ 2O(n)). Thus the total computational
complexity of the ROI of our LSTM application for Ninf
inferences is Ninf ∗ (5O(n2) + 13O(n) ≈ O(Ninf n2). Even
though the computational complexity of the LSTM is similar
to the MLP, it is actually more than 3x more intensive than the
MLP in terms of linear digital operations and contains three
more MVM operations per inference.

With the introduction of the AIMC tiles in the single- and
dual-CPU core cases however, the application queues the con-
catenated input [ht−1, x] into the AIMC tile’s input memory
for the LSTM cell, and then perform all four MVM operations
with one CM PROCESS instruction call by tiling the LSTM
cell weights next to each other (in cases 1 and 2). Then when
the application fetches the result, it is actually fetching the
concatenated MVM results of the forget, input, activation,
and output gates (MVM results using Wf , Wi, Wa, and Wo,
respectively) before the activation functions are performed.
Thus the computational complexity of the LSTM cell layer
reduces the 4O(n2) factor to O(1) while adding the queu-
ing and dequeuing complexities of O([ht−1, x]) + O(nh) ≈
2O(n). Subsequently, the total computational complexity can
be reduced to Ninf ∗ (2O(1) + 15O(n)) ≈ O(Ninf n), or
linear complexity. The reduced complexity explains why as
nh increases there is not a substantial increase in the run time
of the AIMC-enabled LSTM application, as opposed to non-
accelerated (digital reference) mappings.

11

enabled and reference applications reduces by the number of
hardware threads used. Unlike the MLP however, there is not
a slowdown as a result of taking our AIMC-tiled application
multicore, due to the increased number of digital operations
in the LSTM cell being more adequately split across cores.

E. LSTM Working Set Analysis

Just like with the MLP application however, the LSTMs
also make signiﬁcant gains in memory intensity by using
stationary weights, and thus reducing the size of the working
set. If we calculate the approximate working set of the LSTM
application, it is comprised of the input (x + h0), the LSTM
cell layer’s weights (4 ∗ (nh ∗ (nh + x)), the fully-connected
layer’s weights (nh ∗ y), the intermediary result in-between
the layers (nh), and the output (y). The total size complexity
of the LSTM application per inference using 8-bit types is
(x + nh) + 4(n2
h + nhx) + nh + nhy + y. Using the numbers
from Table II, the size of the working set for nh = 256, 512,
750 is 378kB, 1.28MB, and 2.59MB, respectively. Even in
the smallest variant of the LSTM application, the working set
cannot entirely ﬁt in the private caches of the CPU core(s).
When nh is 512 or 750, in both low-power and high-power
system conﬁgurations, the CPU core(s) must go out to L2
cache and main memory to contain the working set.

When we utilize AIMC tiles in our LSTM application,
the weights are removed from the working set because they
are never needed by the CPU core(s) during the ROI, thus
reducing our total size complexity to (x + nh) + nh + y,
which is 0.66kB, 1.17kB, and 1.65kB for nh = 256, 512, and
750, respectively. For all values of nh tested, the working
set can ﬁt entirely in L1 private caches for both low-power
and high-power system conﬁgurations, hence the increasing
performance improvements and lower memory intensity with
greater values of nh.

Therefore, we conclude that while AIMC tiles greatly
speedup neural networks where MVMs are the dominating
operation, careful attention must be paid to both the size of
the neural network and the proportion of other non-optimized
digital operations. This case study of the LSTM shows that
when linear operations are more dominant, that realized gains
in performance are lower with the AIMC tiles (as compared
with the ﬁrst exploration study), and thus these other linear
operations which are not optimized with the inclusion of the
AIMC tiles become the new bottleneck. Furthermore, when
the neural network is small enough to efﬁciently leverage
a CPU core’s L1 private cache and a small portion of L2
cache (as when nh is 256), the overhead of queuing inputs
and dequeuing outputs to and from the AIMC tile ends up
having a similar run-time (with only minor performance gains)
to the reference application, which otherwise invalidates or
diminishes the potential beneﬁt of introducing AIMC tiles.

IX. EXPLORATION THREE: CONVOLUTION NEURAL
NETWORKS

A. CNN Architecture

Similar to the MLP, when the layers are pipelined together
in the multicore cases the complexity of both the AIMC-

For our last exploration study, we explore the beneﬁts of
introducing AIMC tiles for CNNs in an 8-CPU core MPSoC.

ReLUThread 0ReLUThread 1ReLUThread 2ReLUThread 3ReLUThread 4DenseLayer(4096)ReLUThread 5DenseLayer(4096)ReLUThread 6DenseLayer(1000)SoftmaxThread 7MaxPoolMaxPoolMaxPoolYX12

Fig. 13. Aggregate results for CNN experiments. From left to right, each column contains total time, memory intensity, and energy results for the High-Power
system (top row) and Low-Power system (bottom row) conﬁgurations. ANA refers to analog AIMC-enabled applications with CNN names corresponding to
those in Table 12, while DIG refers to a digital reference, non-AIMC-enabled, implementation. The CNNs F, M, and S represent fast, medium, and slow
variations on the same CNN architecture, respectively.

uneven CPU core utilization. To this end, Figure 14 shows both
the CPU idle percentage and the instructions per cycle (IPC)
count for each individual CPU core in our high-power CNN-S
application. CNN-M and CNN-F in both low-power and high-
power system conﬁgurations exhibit very similar trends. The
utilization of the ﬁrst convolutional layer is similar in both
CPU-only and AIMC tile-enabled benchmarks due to input
load from memory. For convolutional layers 2 and 3, AIMC
tiles provides signiﬁcant beneﬁts with idle cycles decreasing
up to 4x. Likewise, IPC increases relative to the CPU-only
benchmark by up to 3x. Convolutional layers 4 and 5 exhibit
more idle cycles in the AIMC-based implementation in com-
parison to prior layers. This is partly attributed to reduced size
of the feature maps, owing to the stride and pooling operations
of the previous layers. The CPU cores responsible for the fully-
connected layers spent the most time idling as fully-connected
layers are utilized once during inference.

Owing to the ﬁne-grained activation pipelining, the amount
of data to be communicated between layers are signiﬁcantly
reduced. Yet, the total inference run-time is more than the
MLP and LSTM cases owing to the multiple passes over the
convolutional kernels.

As a result, while the AIMC tiles offer signiﬁcant speedups
for CNNs as well, further exploration is needed to optimize
the data ﬂow for shifting bottlenecks in layers. This includes
replicating the initial layer convolutional kernels to balance
the CNN pipeline owing to varying feature map sizes [45],
including local SRAM in the AIMC tile for avoiding queueing
the same input volume of the feature maps multiple times [45]
and minimizing core-to-core communication overheads. We
would like to note that all these explorations can be carried
out on the ALPINE framework.

X. CONCLUSION

In this work, we presented and explored the performance
beneﬁts of a novel architecture that utilizes tightly-coupled
AIMC tiles. We implemented ALPINE, a gem5-X extension
for modeling AIMC tiles in the gem5 full system simulation
framework. With ALPINE, we extended the ARMv8 ISA
with custom instructions that interface AIMC tiles directly

Fig. 14. CPU utilization for CNN-S in the high-power system, expressed as
the percentage of idle CPU cycles (top) and instructions/cycle (IPC) (bottom).

Contrary to the previous studies, where each layer weights are
used only once per inference, convolution operations require
multiple passes on weights per inference via shifting kernels
over the feature maps. To perform the convolution operations
in AIMC tile-enabled applications, we ﬂatten the kernels into
columns and store these in the columns of the AIMC tile, as
described in [44], [16]. The feature maps are also ﬂattened
and queued to the AIMC tile.

We explore three CNN variants, labeled CNN-F(ast), CNN-
M(edium), and CNN-S(low) [43]. Figure 12 (a) shows the
proposed CNN implementation and data ﬂow, while Figure
12 (b) reports the CNN architecture parameters. Fine-grained
pipelining is applied for the data-ﬂow; the convolutions are
performed whenever the corresponding input volume of the
feature map is available. Contrary to the previous exploration
studies, we utilize the AIMC tiles only for convolutional lay-
ers. The feed-forward layers are processed in the CPU; these
layers are executed only once as opposed to the convolutional
layers and therefore do not constitute a bottleneck.

B. Results and Analysis

We present our results in Fig. 13. The largest performance
increase with respect
to the CPU-only implementation is
recorded for the largest CNN variant ”S”. This conﬁguration
exhibits the maximum speedup of 20.5x, a memory intensity
improvement of 3.7x, and an energy improvement of 20.8x
for the high-power system.

While in the prior case studies the core utilization across
layers does not vary widely across experiments, the CNN
benchmark is used to examine the AIMC acceleration with

0102030Time (s)7.3x12.4x20.5xHigh-Power0.000.010.02LLCM / Instruction2.0x3.0x3.7xReferenceAIMC-Enabled0.036.773.3110.0Energy (J)7.4x12.4x20.8xDIGFANAFDIGMANAMDIGSANAS0204060Time (s)8.8x11.2x17.6xLow-PowerDIGFANAFDIGMANAMDIGSANASCases0.000.010.02LLCM / Instruction2.2x3.0x3.8xDIGFANAFDIGMANAMDIGSANAS0.018.336.755.0Energy (J)8.9x11.5x18.4x0.025.050.075.0Idle Cycle %Convolutional LayersDense LayersHigh-Power CNN-S ReferenceHigh-Power CNN-S Analog12345678CPU Core Number0.00.20.40.6IPCfrom their execution in the CPU. For ease of programming
AIMC tiles and using them in user-space applications, we
implemented a dedicated software library (AIMCLib) for
interfacing AIMC tiles. Using ALPINE and AIMClib, we then
implemented and tested three different case studies across two
system conﬁgurations, namely, single and multi-core MLPs,
single and multi-core LSTMs, and ﬁnally multi-core CNNs.
Through these case studies we observed how computational
and size complexity is reduced by leveraging the AIMC tiles
in neural networks, ultimately demonstrating up to 20.5x/20.8x
performance/energy gains with respect to a SIMD-enabled
fully-digital reference implementation.

ACKNOWLEDGMENTS

We thank Geethan Karunaratne, Pier Andrea Francese and
Riduan Khaddam-Aljameh for technical discussions. This
work has been supported by the EC H2020 WiPLASH (GA
No. 863337) project and the ERC Consolidator Grant COM-
PUSAPIEN (GA No. 725657) projects.

REFERENCES

[1] K. He et al., “Deep residual learning for image recognition,” in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[2] A. Vaswani et al., “Attention is all you need,” in Advances in Neural

Information Processing Systems, vol. 30, 2017.

[3] N. P. Jouppi et al., “Ten Lessons From Three Generations Shaped
Google’s TPUv4i : Industrial Product,” in 2021 ACM/IEEE 48th Annual
International Symposium on Computer Architecture (ISCA), 2021, pp.
1–14.

[4] J. Choquette and W. Gandhi, “NVIDIA A100 GPU: Performance
amp; Innovation for GPU Computing,” in 2020 IEEE Hot Chips 32
Symposium (HCS), 2020, pp. 1–43.

[5] A. Sebastian et al., “Memory devices and applications for in-memory

computing,” Nature Nanotechnology, 2020.

[6] A. Shaﬁee et al., “ISAAC: A convolutional neural network accelerator
with in-situ analog arithmetic in crossbars,” in Proc. ACM SIGARCH
Computer Architecture News, 2016.

[7] M. Dazzi et al., “Efﬁcient pipelined execution of cnns based on
in-memory computing and graph homomorphism veriﬁcation,” IEEE
Transactions on Computers, vol. 70, no. 6, pp. 922–935, 2021.

[8] B. Feinberg et al., “Enabling scientiﬁc computing on memristive ac-
celerators,” in Proc. ACM/IEEE International Symposium on Computer
Architecture Symposium on Computer Architecture (ISCA), 2018.
[9] A. Ankit et al., “PUMA: A programmable ultra-efﬁcient memristor-
based accelerator for machine learning inference,” in Proc. Intl. Conf.
on Architectural Support for Programming Languages and Operating
Systems, 2019.

[10] H. Jia et al., “A programmable heterogeneous microprocessor based
on bit-scalable in-memory computing,” IEEE Journal of Solid-State
Circuits, vol. 55, no. 9, pp. 2609–2621, 2020.

[11] G. Ottavi et al., “End-to-end 100-tops/w inference with analog in-
memory computing: Are we there yet?” in 2021 IEEE 3rd International
Conference on Artiﬁcial Intelligence Circuits and Systems (AICAS),
2021, pp. 1–4.

[12] C.-X. Xue et al., “15.4 A 22nm 2Mb ReRAM compute-in-memory
macro with 121-28TOPS/W for multibit MAC computing for tiny
AI edge devices,” in 2020 IEEE International Solid- State Circuits
Conference - (ISSCC), 2020, pp. 244–246.

[13] R. Khaddam-Aljameh et al., “HERMES Core – A 14nm CMOS and
PCM-based In-Memory Compute Core using an array of 300ps/LSB
Linearized CCO-based ADCs and local digital processing,” in 2021
Symposium on VLSI Circuits, 2021, pp. 1–2.

[14] D. D. Gajski et al., High—Level Synthesis: Introduction to Chip and

System Design. Springer Science & Business Media, 2012.

[15] Y. M. Qureshi et al., “Gem5-x: A gem5-based system level simulation
framework to optimize many-core platforms,” in Spring Simulation Conf.
(SpringSim), 2019.

13

[16] V. Joshi et al., “Accurate deep neural network inference using computa-
tional phase-change memory,” Nature Communications, vol. 11, p. 2473,
2020.

[17] S. Kariyappa et al., “Noise-Resilient DNN: Tolerating Noise in PCM-
Based AI Accelerators via Noise-Aware Training,” IEEE Transactions
on Electron Devices, vol. 68, no. 9, pp. 4356–4362, 2021.

[18] M. R. Z. Fahimi et al., “Mitigating imperfections in mixed-signal

neuromorphic circuits,” 2021.

[19] H. Tsai et al., “Inference of long-short

term memory networks at
software-equivalent accuracy using 2.5 M analog phase change memory
devices,” in Symposium on VLSI Technology, 2019.

[20] K. Spoon et al., “Toward software-equivalent accuracy on transformer-
based deep neural networks with analog memory devices,” Frontiers in
Computational Neuroscience, vol. 15, p. 53, 2021. [Online]. Available:
https://www.frontiersin.org/article/10.3389/fncom.2021.675741

[21] K. Kourtis et al., “Compiling neural networks for a computational

memory accelerator,” arXiv preprint arXiv:2003.04293, 2020.

[22] J. Ambrosi et al., “Hardware-software co-design for an analog-digital
accelerator for machine learning,” in 2018 IEEE International Confer-
ence on Rebooting Computing (ICRC).

IEEE, 2018, pp. 1–13.

[23] J. Vieira et al., “A product engine for energy-efﬁcient execution of
binary neural networks using resistive memories,” in Proc. IFIP/IEEE
Intl. Conf. on Very Large Scale Integration (VLSI-SoC), 2019.

[24] G. Ottavi et al., “End-to-end 100-TOPS/W inference with analog in-
memory computing: Are we there yet?” in 2021 IEEE 3rd International
Conference on Artiﬁcial Intelligence Circuits and Systems (AICAS).
IEEE, 2021, pp. 1–4.

[25] A. Biswas and A. P. Chandrakasan, “CONV-SRAM: An Energy-Efﬁcient
SRAM With In-Memory Dot-Product Computation for Low-Power
Convolutional Neural Networks,” IEEE Journal of Solid-State Circuits,
vol. 54, no. 1, pp. 217–230, 2019.

[26] H. Jia et al., “15.1 a programmable neural-network inference accelerator
based on scalable in-memory computing,” in 2021 IEEE International
Solid- State Circuits Conference (ISSCC), vol. 64, 2021, pp. 236–238.
[27] F. Merrikh-Bayat et al., “High-performance mixed-signal neurocomput-
ing with nanoscale ﬂoating-gate memory cell arrays,” IEEE Transactions
on Neural Networks and Learning Systems, vol. 29, no. 10, pp. 4782–
4790, 2018.

[28] M. Kim et al., “An Embedded NAND Flash-Based Compute-In-Memory
Array Demonstrated in a Standard Logic Process,” IEEE Journal of
Solid-State Circuits, pp. 1–1, 2021.

[29] S. R. Nandakumar et al., “Precision of synaptic weights programmed
in phase-change memory devices for deep learning inference,” in 2020
IEEE International Electron Devices Meeting (IEDM), 2020, pp. 29.4.1–
29.4.4.

[30] I. Boybat, B. Kersting et al., “Temperature sensitivity of analog in-
memory computing using phase-change memory,” in 2021 IEEE Inter-
national Electron Devices Meeting (IEDM), 2021.

[31] M. Le Gallo et al., “Mixed-precision in-memory computing,” Nature

Electronics, vol. 1, pp. 246–253, 2018.

[32] I. Giannopoulos et al., “8-bit precision in-memory multiplication with
projected phase-change memory,” in Proc. IEEE Int. Electron Devices
Meeting (IEDM), 2018.

[33] G. Guennebaud, B. Jacob et al., “Eigen v3,” http://eigen.tuxfamily.org,

2010.

[34] C. Zhou et al., “AnalogNets: ML-HW Co-Design of Noise-robust
TinyML Models and Always-On Analog Compute-in-Memory Accel-
erator,” arXiv preprint arXiv:2111.06503, 2021.

[35] S. Lee et al., “Leveraging power-performance relationship of energy-

efﬁcient modern DRAM devices,” IEEE Access, 2018.

[36] S. Nandakumar et al., “Mixed-precision deep learning based on compu-

tational memory,” Frontiers in Neuroscience, 2020.

[37] G. G. Shahidi, “Chip Power Scaling in Recent CMOS Technology

Nodes,” IEEE Access, vol. 7, pp. 851–856, 2019.

[38] A. Stillmaker and B. Baas, “Scaling equations for the accurate prediction
of CMOS device performance from 180 nm to 7 nm,” Integration,
vol. 58, pp. 74–81, 2017.

[39] P. R. Kinget, “Ultra-low voltage analog integrated circuits for nanoscale
cmos,” in 2007 IEEE Bipolar/BiCMOS Circuits and Technology Meet-
ing, 2007, pp. 144–148.

[40] M. Naumov et al., “Deep learning recommendation model for personal-
ization and recommendation systems,” arXiv preprint arXiv:1906.00091,
2019.

[41] I. Tolstikhin et al., “MLP-Mixer: An all-MLP architecture for vision,”

arXiv preprint arXiv:2105.01601, 2021.

[42] M. P. Marcus et al., “Building a large annotated corpus of English: The

Penn Treebank,” Comput. Linguist., 1993.

[43] K. Chatﬁeld et al., “Return of the devil in the details: Delving deep into

convolutional nets,” arXiv preprint arXiv:1405.3531, 2014.

[44] C. Yakopcic, M. Z. Alom, and T. M. Taha, “Memristor crossbar deep
network implementation based on a convolutional neural network,”
in 2016 International Joint Conference on Neural Networks (IJCNN).
IEEE, 2016, pp. 963–970.

[45] M. Dazzi et al., “Accelerating inference of convolutional neural networks
using in-memory computing,” Frontiers in Computational Neuroscience,
vol. 15, p. 63, 2021.

AUTHOR BIOGRAPHIES

Joshua Klein is a 3rd-year doctoral student at the
Embedded Systems Laboratory of the ´Ecole Poly-
technique F´ed´erale de Lausanne (EPFL), Switzer-
land. He received his B.Sc.
in Computer Engi-
neering in 2017 magna cum laude and his M.Sc.
in Electrical and Computer Engineering in 2019
from Boston University, USA. His research interests
include system-level modeling and simulation, RISC
architectures, and novel accelerators for machine
learning applications.

Irem Boybat is a Research Staff Member at IBM
Research – Zurich. She received her B.Sc. degree
in Electronics Engineering from Sabanci University,
Turkey (2013), and M.Sc. and Ph.D. degrees in
Electrical Engineering from ´Ecole Polytechnique
F´ed´erale de Lausanne (EPFL), Switzerland (2015
and 2020, respectively). Her research interests in-
clude in-memory computing for AI systems, neuro-
morphic computing, and emerging resistive memory.
She has co-authored over 40 scientiﬁc papers in jour-
nals and conferences, received three best conference
presentation/paper/poster awards and holds 5 granted patents. She was a co-
recipient of the 2018 IBM Pat Goldberg Memorial Best Paper Award, 2018
IBM Research Division Award on neuromorphic computing using phase-
change memory devices, and 2020 EPFL PhD Thesis Distinction in Electrical
Engineering.

Yasir Mahmood Qureshi received his Ph.D.
in
Electrical Engineering from EPFL, Switzerland, in
2021. He is currently working as a Staff Design
Engineer at Inﬁneon Technologies, Ireland. His re-
search interests include energy-efﬁcient computing,
heterogeneous compute and hybrid memory archi-
tectures, compute sub-systems for automotive micro-
controllers, and safety-critical and secure compute
architectures.

Martino Dazzi received his B.Sc. and M.Sc. (cum
laude) degrees in Electronic Engineering from the
University of Udine, Italy, in 2015 and 2017, re-
spectively. Between 2017 and 2018, he held a Re-
search Assistant position at Alma Mater Studiorum-
Universita’ di Bologna, Italy, focusing on mixed-
signal cicuit design. Between 2018 and 2021, he
held a research position at IBM Research Zurich,
Switzerland. In 2021 he received his Ph.D. degree
in Electrical Engineering from ETH Zurich, Switzer-
land. In 2021, he co-founded Axelera AI, where he
currently holds a position as Algorithm and Quantization Researcher. His
main research interest is in machine learning, speciﬁcally in energy-efﬁcient,
reduced-precision design of neural networks.

14

Alexandre Levisse received his Ph.D. degree in
Electrical Engineering from CEA-LETI, France, and
in 2017.
from Aix-Marseille University, France,
From 2018 to 2021, he was a post-doctoral re-
searcher in the Embedded Systems Laboratory at
the Swiss Federal Institute of Technology Lausanne
(EPFL). From 2021, he works as a scientist
in
EPFL. His research interests include circuits and
architectures for emerging memory and transistor
technologies as well as in-memory computing and
accelerators.

Giovanni Ansaloni is a researcher at the Embedded
Systems Laboratory of EPFL (Lausanne, CH). He
previously worked as a Post-Doc at the University
of Lugano (USI, CH) between 2015 and 2020, and at
EPFL between 2011 and 2015. He received a Ph.D.
degree in Informatics from USI in 2011. His research
efforts focus on domain-speciﬁc and ultra-low-power
architectures and algorithms for edge computing sys-
tems, including hardware and software optimization
techniques.

Marina Zapater is an Associate Professor in the
REDS Institute at the School of Engineering and
Management of Vaud (HEIG-VD) of the University
of Applied Sciences Western Switzerland (HES-SO)
since 2020. She was post-doctoral Research Asso-
ciate in the Embedded System Laboratory (ESL) at
the Swiss Federal Institute of Technology Lausanne
(EPFL), Switzerland, from 2016 to 2020. She re-
ceived her Ph.D. degree in Electronic Engineering
from Universidad Polit´ecnica de Madrid, Spain, in
2015. Her research interests include thermal, power,
and performance design and optimization of complex heterogeneous ar-
chitectures, from embedded AI-enabled edge devices to high-performance
computing processors; and energy efﬁciency in servers and data centers. In
these ﬁelds, she has co-authored more than 50 papers in top-notch conferences
and journals. She is an IEEE and CEDA member.

Abu Sebastian is a Distinguished Research Staff
Member at IBM Research – Zurich. He received a
B. E. (Hons.) degree in Electrical and Electronics
Engineering from BITS Pilani, India, in 1998 and
M.S. and Ph.D. degrees in Electrical Engineering
(minor in Mathematics) from Iowa State University
in 1999 and 2004, respectively. He manages the
research effort on in-memory computing at IBM
Research Zurich. He is the author/co-author of over
200 publications in peer-reviewed journals/confer-
ence proceedings and holds over 70 US patents. In
2015 he was awarded the European Research Council (ERC) consolidator
grant and in 2020, he was awarded an ERC Proof-of-concept grant. He is
an IBM Master Inventor since 2016. In 2019 he received the Ovshinsky
Lectureship Award for his contributions to ”Phase-change materials for
cognitive computing”. He has served on the technical program committees
of several conferences including IEDM, AICAS and E\PCOS.

David Atienza is a full professor of electrical and
computer engineering, and head of the Embedded
Systems Laboratory (ESL) at EPFL, Switzerland.
He received his Ph.D.
in computer science and
engineering from UCM, Spain, and IMEC, Belgium,
in 2005. His research interests include system-level
design methodologies for multi-processor system-
on-chip (MPSoC) servers and edge AI architectures.
He has co-authored more than 350 papers, one book,
and 12 patents. Dr. Atienza has received, among
other recognitions, the ICCAD 10-Year Retrospec-
tive Most Inﬂuential Paper Award in 2020, the Most Inﬂuential DAC Under-40
Innovators Award in 2018, and an ERC Consolidator Grant in 2016. He is an
IEEE Fellow and an ACM Distinguished Member.

