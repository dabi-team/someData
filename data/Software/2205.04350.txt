A NOVEL AUGMENTED REALITY ULTRASOUND FRAMEWORK
USING AN RGB-D CAMERA AND A 3D-PRINTED MARKER ∗

2
2
0
2

y
a
M
9

]

V

I
.
s
s
e
e
[

1
v
0
5
3
4
0
.
5
0
2
2
:
v
i
X
r
a

Y. Zhou, G. Lelu, B. Labbé, G. Pasquier, P. Le Gargasson, A. Murienne and L. Launay
Institute of Research and Technology b<>com
Cesson-Sévigné, France
yitian.zhou@b-com.com

ABSTRACT

Purpose Ability to locate and track ultrasound images in the 3D operating space is of great beneﬁt
for multiple clinical applications. This is often accomplished by tracking the probe using a precise
but expensive optical or electromagnetic tracking system. Our goal is to develop a simple and low
cost augmented reality echography framework using a standard RGB-D Camera.

Methods A prototype system consisting of an Occipital Structure Core RGB-D camera, a
speciﬁcally-designed 3D marker, and a fast point cloud registration algorithm FaVoR was developed
and evaluated on an Ultrasonix ultrasound system. The probe was calibrated on a 3D-printed N-wire
phantom using the software PLUS toolkit. The proposed calibration method is simpliﬁed, requiring
no additional markers or sensors attached to the phantom. Also, a visualization software based on
OpenGL was developed for the augmented reality application.

Results The calibrated probe was used to augment a real-world video in a simulated needle insertion
scenario. The ultrasound images were rendered on the video, and visually-coherent results were
observed. We evaluated the end-to-end accuracy of our AR US framework on localizing a cube of 5
cm size. From our two experiments, the target pose localization error ranges from 5.6 to 5.9 mm and
from −3.9◦ to 4.2◦.

Conclusion We believe that with the potential democratization of RGB-D cameras integrated in
mobile devices and AR glasses in the future, our prototype solution may facilitate the use of 3D
freehand ultrasound in clinical routine. Future work should include a more rigorous and thorough
evaluation, by comparing the calibration accuracy with those obtained by commercial tracking
solutions in both simulated and real medical scenarios.

Keywords Ultrasound · Augmented reality · Probe calibration · RGB-D camera · 3D printing · Optical tracking system

1

Introduction

Ultrasound (US) is a safe, portable and relatively cheap medical imaging modality. US machines produce real-time
imaging of anatomical structures, providing essential information for many clinical practices such as disease diagnosis
and intervention guidance. Despite recent developments in 3D transducer technology, 2D US is still the gold standard
in clinical routine for its high image quality and low cost.

In 2D US, images are acquired using a handheld probe, resulting in a series of 2D slices. Ability to locate and track
those slices in the 3D operating space is of great beneﬁt for multiple clinical applications, as it allows to register
them to other signals or devices tracked in the same space. Examples include assisted needle biopsy, pre-operative or
per-operative image fusion, and volumetric US reconstruction from a series of 2D slices. This is often accomplished by

∗This work beneﬁted from the European Unions Horizon 2020 research and innovation program under grant agreement n856950
(5G-TOURS project). Also, it beneﬁted from State aid managed by the National Research Agency (FR) under the future investment
program bearing the reference ANR-17-RHUS-0005 (FollowKnee project).

 
 
 
 
 
 
tracking the probe using an optical or electromagnetic (EM) tracking system. Given that the tracking system can only
record the pose of the probe with respect to a world coordinate system, and not the image plane with respect to the
world, an US Probe Calibration (UPC) procedure should be performed beforehand to obtain the spatial transform which
maps coordinates in the US image plane to those of the probe [1].

A variety of calibration methods have been proposed in the literature [2, 3, 4]. The reader is referred to [5] for a
comprehensive review. Most of them operate by scanning a 3D phantom with known geometrical properties that may
consist of points, wires or planes [2, 3]. Among them, one popular option is to use the N-wire phantom where metal
wires are designed to form “N” shapes [6]. Recently, image-based auto-calibration was introduced in [4]. Local image
features are extracted from the patient’s US data. Registration techniques are then used to establish spatial alignment
between slices acquired from different probe positions, from which is further estimated the calibration matrix. The
obtained accuracy was comparable with the state-of-the-art phantom-based methods [4]. However, the authors claim
that successful auto-calibrations require that the anatomy of interest contains distinctive, localizable image structure
from which local features could be extracted [4], which remains one main limitation of the method.

An important component of the UPC and the associated Augmented Reality (AR) application is the underlying tracking
system. Classical systems consist of central control units and optical/EM sensors [7]. Many commercial solutions exist,
for example the NDI Polaris/Aurora and the CIVCO’s Ultra-Pro. Those systems ensure a high tracking precision. For
example, the NDI Aurora has a localization precision of ∼ 1.4 mm [8]. The UPC is performed priorly, by attaching
sensors to the phantom, to the probe and to the stylus respectively [7]. The stylus is an auxiliary tool used for ﬁnding the
spatial relationship between the phantom and its attached sensor [7]. Such solutions ensure usually a high calibration
precision (∼ 1.1 mm as reported in [9]). However, a costly, complex, and somewhat cumbersome system including a
myriad of tools and accessories, as well as well-deﬁned procedures, is required, which might hamper their utilization in
clinical routine.

Recently, several groups [10, 11] attempted to develop simpler and cheaper US tracking systems using standard
RGB(-D) cameras. Both proposed inside-out based tracking methods [11]. The camera is ﬁxed upon the probe and
used actually as a sensor. Its pose is estimated from contextual information contained in the recorded video streams,
using standard computer-vision techniques like the SLAM [11]. The authors stated that such inside-out designs aimed
to reduce the line-of-sight effect [11]. Nonetheless, those methods usually require textured scenes and the presence of
multiple ﬁducial markers in the Operating Room (OR) [11]. In addition, RGB(-D) cameras of signiﬁcant weight, as well
as the associated power supply cable, need to be mounted on the probe. All those elements could have a non-negligible
impact on the routine clinical environment.

Several groups have gone a step further by proposing 3D freehand US reconstructions without any external tracking
device [12][13]. Deep learning techniques and inertial measurement units are combined to directly estimate the motion
between successive frames from US data. Promising results are reported in [12][13]. However, in [12], the authors
claim that no back-and-forth probe motion can be managed. In fact, the probe is supposed to move in a ﬁxed direction,
for instance, from proximal to distal, which could be troublesome in some clinical situations.

In this paper, we propose a novel 2D UPC and the associated navigated US framework, combining an RGB-D camera, a
3D-printed marker and a previously-developed fast model-based 3D point cloud registration algorithm FaVoR [14]. Our
method works in a more conventional outside-in manner: real-time tracking of the marker by registering its virtual
model (CAD mesh) with the 3D point cloud captured by the depth camera. Unlike the inside-out-based solutions
proposed in [10, 11] where the camera’s real-time pose is estimated from RGB(-D) information, the tracking via FaVoR
is only based on depth information [14]. This is aimed at obtaining a stable solution despite the constraints related to
the OR, such as the strong OR light which can sometimes saturate the RGB sensors.

The main novelties are twofold: 1) FaVoR [14], an RGB-D camera, and a speciﬁcally-designed 3D-printed marker
dedicated for depth-based tracking are merged into an uniﬁed framework for AR US. Depth cameras have been
democratizing rapidly since several years, with the advantage of being much more affordable than commercial
optical/EM tracking systems. We believe that this could signiﬁcantly reduce the cost of navigated US in the future,
therefore facilitating its clinical use; and 2) The probe calibration workﬂow is simpliﬁed, thanks to FaVoR which
directly registers the virtual model of the phantom with the 3D representation of the world captured by the depth camera,
therefore obviating the need to attach additional markers to the phantom, unlike most existing solutions [7].

1.1 Ultrasound probe calibration

As aforementioned, before deploying a navigated US system, the US image needs to be calibrated with respect to the
probe marker. As shown in Fig. 1, our UPC system combines an RGB-D camera, an N-wire phantom, an US machine
and a mobile workstation used for data processing. The marker is speciﬁcally designed to be empty inside so as to

2

let pass through the cable. This speciﬁc design aims to avoid the occultation of the marker by the cable during the
manipulation by clinicians.

Figure 1: The proposed ultrasound probe calibration setting which consists of an Ultrasonix US machine, an Occipital
Structure Core RGB-D camera, a water tank containing the N-wire phantom, a mobile workstation and two US stream
conversion boxes. The 3D-printed marker is rigidly ﬁxed upon the probe. Zoomed views over the phantom and the
probe are displayed in red and green boxes respectively.

1.1.1 Preparations

The RGB-D camera is installed on a tripod as shown in Fig. 1. We chose the Occipital Structure Core, due mainly to its
high depth quality and framerate (∼ 60 Frame Per Second (FPS)). Its position and orientation are adjusted manually
to be between 40 and 80 cm to the phantom, corresponding to the very short range of the camera. The impact of this
distance on the calibration accuracy will be studied later in Sect. 2.

Ultrasound image streaming The Ultrasonix machine outputs the US images in DVI format. The goal is to feed the
US stream to our mobile workstation via an USB port. A stream conversion system consisting of two AJA boxes was
therefore designed. As illustrated in Fig. 2, a ﬁrst ROI-DVI box is used to crop the original US images and output the
stream in SDI format, which is further converted into USB format by the second SDI-USB converter.

Figure 2: Streaming of ultrasound images via an ROI-DVI box and a SDI-USB converter of the AJA company.

N-wire phantom The N-wire phantom “fCal-2.1” was used [7]. Additionally, a base was designed for ﬁxing the
N-wire phantom inside the water tank, since our method assumes that the phantom is not moving during the calibration.
The phantom and the base are both 3D-printed. Nine metal wires of 1 mm diameter were installed so as to form the “N”
shape at three different depth levels.

3

3D marker dedicated for depth cameras Fig. 1 provides a zoomed view of the marker. The patent-pending marker
results from a sophisticated combination of small cubes of size 1 cm, forming an object of roughly 4cm span. It was
3D-printed using plastic materials. This is aimed to obtain a sufﬁciently-textured while tiny and light object which can
be easily localized by RGB-D cameras. In addition, a ﬁxation system was 3D-printed in order to ﬁrmly ﬁx the marker
on the probe. In future applications, similar markers can also be attached to other devices, as the total weight of the
marker and the ﬁxation system is only about 30g.

1.1.2 Calibration workﬂow

Our calibration method is based on the open-source software PLUS toolkit [7]. Following the ofﬁcial tutorials [7], a
new tracking device FaVoR [14] was deﬁned and integrated into the “fCal” application [7]. Both the RGB-D camera
and the US image stream are connected to the mobile workstation via USB 3.0 cables, and are managed by the “fCal”
application.

Figure 3: The workﬂow of the proposed US probe calibration method. FaVoR [14] ﬁrst localizes the phantom, and then
tracks the probe marker. The poses are fed to PLUS [7] for calibration.

The calibration framework comprises three key modules: phantom localization, probe tracking and PLUS calibration.
Unlike most calibration procedures where additional markers/sensors need to be attached to the phantom for its
localization [7], we make full use of the generic nature of FaVoR and localize the N-wire phantom without any use of
markers. For this, FaVoR takes the virtual model of the N-wire phantom as input and directly register it with the 3D
point cloud captured by the depth camera. This generates the pose of the phantom in the camera’s coordinate system.
This pose is then frozen, leading to a static phantom pose (one simpliﬁcation that we make is that the phantom is not
moving during the calibration). Then, FaVoR is reparametrized with the marker’s CAD model to estimate the pose of
the probe marker in real time using the same RGB-D camera.

The probe is placed over the phantom for imaging the installed ﬁducial wires, producing US images with nine visible
white spots. Those points are automatically segmented using functionnalities provided within PLUS [7]. Then, the three
middle-wire points are matched to the groundtruth positions in the phantom [7]. Since both the phantom and the probe
marker are localized by the same camera, spatial positions in the phantom space are easily mapped to 3D coordinates in
the marker’s space. This generates correspondences between the segmented image pixels and the spatial locations in the
marker’s space. From such correspondences, PLUS computes the calibration matrix using least-square ﬁtting [7].

The whole workﬂow is illustrated in Fig. 3. The water tank is ﬁrst half ﬁlled, exposing the phantom outside in the
air. This is aimed to free the phantom localization from the optical refraction effect. Once localized, the pose of the
phantom is frozen and stays constant during the calibration. More water is then added to the tank, covering entirely the
phantom so that it can be visualized under US. FaVoR is then reconﬁgured to track the marker. Its pose, combined with
that of the phantom and the streamed US images are then fed to PLUS “fCal” for calibration matrix estimation [7]. The
technical details of the individual components are provided below.

4

Phantom localization FaVoR is parametrized with the virtual model (CAD mesh) of the fCal 2.1 phantom [7]. The
goal is to register the virtual model with the 3D representation of the real world captured by the RGB-D camera. FaVoR
requires a manual initialization. The RGB-D camera is adjusted manually to align the ROI center with a predeﬁned
zone of the phantom (Step 1 in Fig. 3). The localization is then triggered, generating real-time poses of the phantom
(Step 2 in Fig. 3). Since we assume that neither the phantom nor the camera are allowed to move during the calibration,
which is one constraint of our method, the pose of the phantom has constant values. As a result, the pose is frozen (Step
3 in Fig. 3) so as to free FaVoR for the following tracking of the probe marker.

Probe tracking Similar to the localization of the phantom, FaVoR is reconﬁgured to register the virtual model of the
marker with the 3D point cloud captured by the depth camera this time. The position of the probe is adjusted manually
so as to align the ROI with a predeﬁned zone on the marker (Step 5 in Fig. 3). Once initialized, FaVoR tracks the marker
at ∼ 40 FPS on the mobile workstation Intel(R) Core(TM) i7-10850H of 2.70GHz (Step 6 in Fig. 3).

PLUS calibration After having properly positioned the probe over the phantom, the ﬁducial wires are visualized as
white bright spots on US. They are segmented automatically using tools of PLUS which are based on mathematical
morphology and thresholding. An example of the segmentation is shown in Fig. 3 (Step 7). The segmented spots are
then matched to the nine wires by PLUS [7]. The pixel coordinates of the three middle-wire points in the US images
are matched to 3D coordinates in the marker’s coordinate system using the poses of the marker and the phantom [7].
PLUS then estimates the calibration matrix from these image-marker correspondences [7]. The calibrated US image
and the phantom, as well as the installed wires, are visualized together under PLUS “fCal” (Step 8 in Fig. 3).

1.2 Navigated US and augmented real-world video

The calibrated probe and the RGB-D camera-based tracking system are deployed to augment a real-world scenario with
US image information. A visualization software based on OpenGL has been developed for rendering an US image
within a standard RGB image. It takes as input the marker’s pose estimated by FaVoR, the calibration matrix, and
the intrinsic parameters of the color sensor of the RGB-D camera, and renders both the marker and the US image in
real-time.

2 Result

2.1 Evaluation within PLUS

2.1.1 Tuning the camera-to-phantom distance

In our case, the distance between the RGB-D camera and the top of the phantom is a key variable that could impact the
calibration accuracy, and therefore the precision of the AR US application. As a result, we performed the calibration at
ﬁve different distances ranging from 40 to 80 cm. At each distance, ﬁve calibrations were repeated. The magnitudes of
the calibration errors on N-wire phantom were listed in Tab. 1. We observe that the Occipital Structure Core camera
achieved its best accuracy at 50 cm, corresponding to an average calibration error of 2.6 mm. The calibration errors
were computed by PLUS on additional US images acquired on the N-wire phantom [7]. The wires were segmented
by PLUS and reprojected to the marker’s space using the calibration matrix. Those points are then transformed to the
phantom’s space since both the phantom and the marker are localized by the same camera. Distances between those
points and the groundtruth were computed and then averaged. At 40 cm, bigger errors were obtained. This is because
that while scanning the probe, the distance between the marker and the camera could be below 30 cm, which is beyond
the operating range of the depth camera.

Table 1: Evolution of the calibration error over the camera-phantom distance.

Distance (cm)

40

50

60

70

80

Calib. Err. (mm)

30.1 ± 20.0

2.6 ± 0.4

3.4 ± 0.5

3.3 ± 0.7

4.0 ± 0.6

2.1.2 Visualizing the calibration result on N-wire phantom

Fig. 4 shows an example of the calibrated image under PLUS “fCal” at the optimal camera-to-phantom distance of 50
cm. The groundtruth wires are shown as red lines, and we can observe that they are aligned with the white spots in the
US images (metal wires are visualized in US as higher intensity pixels).

5

Figure 4: US image calibrated with the N-wire phantom visualized in “fCal” of PLUS [7] (red lines show the groundtruth
wires)

2.2 Simulation of needle insertion on a BluePrint phantom

Figure 5: Augmented ultrasound simulating needle insertion in real-world video (the tracked marker is rendered in blue
and the calibrated US image is displayed on the RGB image)

Fig. 5 shows the deployment of the calibrated probe for augmenting a real-world video on a BluePrint phantom. A
needle insertion procedure was simulated. The US images are rendered using a proper color map which highlights pixels
with higher intensities in bright yellow. As we can observe in Fig. 5, the part of the needle which is originally invisible
in the RGB image, since it is inside the phantom, is displayed in the US image on the video. A spatial continuity is
observed between the needle part which is outside the phantom (highlighted for better visualization as the color sensor
of the Occipital Structure Core camera has a rather limited spatial resolution) and the inside part revealed by US.

6

2.3 Evaluation of the localization accuracy of a cube using our AR US framework

We aim to further evaluate the end-to-end accuracy of our AR US framework. A cube of size 5 cm was used. Similar to
the method used in [15], the cube was placed at a precise position in the middle of a planar board with markers attached
to it, as shown in Fig. 6a. The board deﬁnes a world Coordinate System (CS) with its origin at the center. All the marker
corners’ positions are known. From the RGB images, part of the markers can be localized using OpenCV [15], as is
displayed in Fig. 6b. The localized 2D pixels are further unprojected to the 3D CS of the camera, using the depth map
and the intrinsic parameters of the color sensor. This generates 3D-to-3D correspondences between the camera and
world CS. The pose of the cube is then computed using the technique introduced in [16]. In this way, we obtain the
groundtruth position of the cube, as displayed in Fig. 6b.

(a) A cube is ﬁxed inside a water tank which is carefully placed
at a predeﬁned position with respect to the ArUco markers

(b) The groundtruth pose of the cube in the camera space is esti-
mated using RGB-D camera-based pose estimation techniques

Figure 6: Generation of the groundtruth of the cube’s pose in the RGB-D camera’s space using ArUco markers.

Figure 7: Scan the cube edges using the proposed AR US system. The segmented US pixels are further unprojected to
3D points in the RGB-D camera’s space, through the calibration matrix and probe marker’s poses.

Now that the groundtruth is available, we are able to evaluate the accuracy of our AR US system quantitatively. For this,
as illustrated in Fig. 7, ﬁve edges of the cube are scanned, which is enough to reconstruct the cube. Pixels belonging
to the edge are segmented and transformed to the probe marker’s CS via the calibration matrix. Since the marker is
tracked, those points are further mapped to the camera’s CS where the groundtruth is available (Fig. 6). Fig. 8a and 8b
show the groundtruth cube edges and the corresponding localization points generated using the method introduced in
Fig. 7. Iterative Closest Point (ICP) was then used to compute the rigid transformation between the groundtruth edges
and the localized points. The transformation was applied to the groundtruth cube, yielding the reconstructed cube as
displayed in Fig 8a and 8b. The ICP registration has a residue of 5.4 mm average distance. An additional experiment
was performed, and the obtained accuracies from the two experiments are reported in Tab. 2. Note that here the level of
accuracy corresponds to the whole AR US method, which combines the probe calibration error, the probe tracking error

7

(a) localized cube edge points
(view point I)

(b) localized edge points (view
point II)

(c) cube ﬁtted from the
localized edge points
(view point I)

(d) cube ﬁtted from the
localized edge points
(view point II)

Figure 8: Evaluation of the cube localization accuracy obtained by our AR US method. In (a) and (b), solid lines show
groundtruth cube edges and the localization results are displayed as small spheres of the same color. In (c) and (d),
groundtruth cube is displayed in black color, and the ﬁtted one in yellow. The distance between the two cube centers is
5.9 mm, and the three Euler angles offsets are 0.8◦, −3.9◦ and 4.2◦.

via FaVoR, potential manual US image segmentation errors and ﬁnally geometric distortions in US images. This could
partially explain why the error magnitude is higher than the tracking error reported in [14].

Table 2: The level of accuracy obtained by our augmented reality ultrasound framework for localizing a cube of 5 cm
size.

ICP registration residue
(mean dist. in mm)

Cube center offset
(mm)

Euler angles offset
(degree)

Experiment n◦ 1
Experiment n◦ 2

5.4
4.5

5.9
5.6

{0.8, -3.9, 4.2}
{1.8, 1.4, -2.5}

3 Conclusion

A new simple and low cost AR US framework including essentially an RGB-D Camera, a 3D-printed marker, and a
fast point-cloud registration algorithm FaVoR was developed and evaluated on an Ultrasonix US system. The probe
calibration was performed using the N-wire phantom and the software PLUS [7]. Preliminary results using the Occipital
Structure Core showed a mean ﬁducial-based calibration error of 2.6 mm generated within the PLUS application,
corresponding to the camera to phantom distance of around 50 cm. The calibrated probe was then used to augment a
real-world video in a simulated needle insertion scenario. Visually-coherent results were observed by showing a spatial
continuity between the part of the needle outside the phantom visible in RGB images and the remaining part revealed
by US. Furthermore, we evaluated the the end-to-end accuracy of our AR US framework on localizing a cube of 5 cm
size. From our two experiments, the target pose localization error ranges from 5.6 to 5.9 mm and from −3.9◦ to 4.2◦.

One limitation of our method is that during the calibration, neither the camera nor the water tank upon which the N-wire
phantom is ﬁxed are allowed to move. Future work should include a more comprehensive validation of the proposed
method, through quantitative evaluations in both simulated and real medical scenarios. The planar board could be
3D-printed together with the evaluation object in order to minimize all potential errors in groundtruth generation due
to misalignment between the object and the board. More complex 3D shapes, for instance anatomical organ models,
should also be used for the evaluation. Also, both the probe tracking and the calibration should be further optimized by
using a marker mesh of a higher spatial resolution (currently downsampled to 3 mm for higher FPS ∼ 50) in order to
improve the accuracy. A thorough comparison with existing AR US solutions relying on commercial tracking system
in terms of both accuracy and computational time will also be beneﬁcial. Besides, currently we used the Occipital
Structure Core, a comprehensive comparison of different commercial RGB-D cameras, regarding both the depth and
color sensors, will be useful. In future extensions, the system shall also support the use of multiple 3D markers in order
to track more than one device. We believe that with the potential future democratization of RGB-D cameras integrated
in both mobile devices and AR glasses, our prototype solution may facilitate the use of 3D freehand ultrasound in
clinical practices.

8

References

[1] Chen E, Peters TM, Ma B (2016) Guided ultrasound calibration: where, how, and how many calibration ﬁducials.
International Journal of Computer Assisted Radiology and Surgery 11(6):889-898. https://doi.org/10.1007/s11548-
016-1390-7

[2] Wen T, Wang C, Zhang Y (2020) A Novel Ultrasound Probe Spatial Calibration Method Using a Combined

Phantom and Stylus. Ultrasound in Medicine & Biology. https://doi.org/10.1016/j.ultrasmedbio.2020.03.018

[3] García-Cano E, Cosío FA, Robles FT, Fanti Z, Belleﬂeur C, Joncas J, Labelle H, Duong L (2020)
A freehand ultrasound framework for spine assessment in 3D: a preliminary study. In 42nd Annual In-
ternational Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) 2096-2100.
https://doi.org/10.1109/EMBC44109.2020.9176689

[4] Toews M, Wells WM (2017) Phantomless auto-calibration and online calibration assessment

for
imaging 37(1):262-272.

a tracked freehand 2-D ultrasound probe.
https://doi.org/10.1109/TMI.2017.2750978

IEEE transactions on medical

[5] Mercier L, Langø T, Lindseth F, Collins DL (2005) A review of calibration techniques for freehand 3-D ultrasound

systems. Ultrasound in medicine & biology, 31(4), 449-471

[6] Chen TK, Abolmaesumi P, Thurston AD, Ellis RE (2006) Automated 3D freehand ultrasound calibration with real-
time accuracy control. International Conference on Medical Image Computing and Computer-Assisted Intervention
pp. 899-906

[7] Lasso A, Heffter T, Rankin A, Pinter C, Ungi T, Fichtinger G (2014) PLUS: open-source toolkit for
ultrasound-guided intervention systems. IEEE transactions on biomedical engineering 61(10):2527-2537.
https://doi.org/10.1109/TBME.2014.2322864

[8] https://www.ndigital.com/products/3d-guidance/3d-guidance-transmitters/
[9] Lange T, Kraft S, Eulenstein S, Lamecker H, Schlag PM (2011) Automatic calibration of 3D ultrasound probes.

Bildverarbeitung für die Medizin 2011 (pp. 169-173)

[10] Baba MM, Mohamed OA, Awwad F, Daoud MI (2016) A low-cost camera-based transducer tracking system
for freehand three-dimensional ultrasound. The 14th IEEE International New Circuits and Systems Conference
(NEWCAS) 1-4. https://doi.org/10.1109/NEWCAS.2016.7604825

[11] Busam B, Ruhkamp P, Virga S (2018) Markerless inside-out tracking for interventional applications, arXiv preprint

1804.01708

[12] Prevost R, Salehi M, Jagoda S, Kumar N, Sprung J, Ladikos A, Bauer R, Zettinig O, Wein W (2018)
3D freehand ultrasound without external tracking using deep learning. Medical image analysis 48:187-202.
https://doi.org/10.1016/j.media.2018.06.003

[13] Guo H, Xu S, Wood B, Yan P (2020) Sensorless freehand 3D ultrasound reconstruction via deep contextual
learning. International Conference on Medical Image Computing and Computer-Assisted Intervention pp. 463-472
[14] Murienne A, Labbé B, Launay L (2020) Model-based 3D Tracking for Augmented Orthopedic Surgery. EPiC

Series in Health Sciences 4:197-200. https://doi.org/10.29007/v3nc

[15] Hinterstoisser S, Lepetit V, Ilic S, Holzer S, Bradski G, Konolige K, Navab N (2012) Model based training,
detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. Asian conference on computer
vision pp. 548-562

[16] Gonzalez M, Kacete A, Murienne A, Marchand E (2021) L6DNet: Light 6 DoF Network for Robust and Precise

Object Pose Estimation with Small Datasets. IEEE Robotics and Automation Letters.

9

