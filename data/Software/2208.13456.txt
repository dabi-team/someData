2
2
0
2

g
u
A
9
2

]
E
S
.
s
c
[

1
v
6
5
4
3
1
.
8
0
2
2
:
v
i
X
r
a

Common Problems and Effects of Feedback on Fun
When Programming Ozobots in Primary School

Luisa Greifenstein
University of Passau
Passau, Germany

Isabella GraÃŸl
University of Passau
Passau, Germany

Ute Heuer
University of Passau
Passau, Germany

Gordon Fraser
University of Passau
Passau, Germany

ABSTRACT
Computational thinking is increasingly introduced at primary school
level, usually with some form of programming activity. In particular,
educational robots provide an opportunity for engaging students
with programming through hands-on experiences. However, pri-
mary school teachers might not be adequately prepared for teaching
computer science related topics, and giving feedback to students
can often be challenging: Besides the content of the feedback (e.g.,
what problems have to be handled), the way the feedback is given is
also important, as it can lead to negative emotional effects. To sup-
port teachers with the way of giving feedback on common problems
when teaching programming with robotics, we conducted a study
consisting of seven workshops with three third and four fourth
grade primary school classes. Within seven different activities, the
116 primary school children first programmed the Ozobot Evo robot
in the pen-and-paper mode and then on a digital device. Through-
out these activities we collected data on the problems the students
encountered, the feedback given, and the fun they experienced. Our
analysis reveals eight categories of problems, which we summarise
in this paper together with corresponding possible feedback. We
observed that problems that are urgent or can harm the studentsâ€™
self-efficacy have a negative impact on how enjoyable an activity
is perceived. While direct instruction significantly decreased the
experienced fun, hints had a positive effect. Generally, we found
programming the Ozobot Evo to be encouraging for both girls and
boys. To support teachers, we discuss ideas for giving encourag-
ing feedback on common problems of Ozobot Evo programming
activities and how our findings transfer to other robots.

CCS CONCEPTS
â€¢ Social and professional topics â†’ Software engineering ed-
ucation; K-12 education; â€¢ Software and its engineering â†’
Visual languages.

KEYWORDS
Block-based programming, Feedback, Fun, Interest, Motivation,
Ozobot Evo, Physical Programming, Primary Education, Robotics

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9853-4/22/10. . . $15.00
https://doi.org/10.1145/3556787.3556860

ACM Reference Format:
Luisa Greifenstein, Isabella GraÃŸl, Ute Heuer, and Gordon Fraser. 2022.
Common Problems and Effects of Feedback on Fun When Programming
Ozobots in Primary School. In Proceedings of the 17th Workshop in Primary
and Secondary Computing Education (WiPSCE â€™22), October 31-November 2,
2022, Morschach, Switzerland. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3556787.3556860

1 INTRODUCTION
Computer science related topics are increasingly taught in primary
schools around the world [24], and programming is commonly used
as a vehicle to promote different aspects of computational think-
ing [36]. This, however, challenges primary school teachers [19],
who have to cover many subjects without specialising in all of them,
such that they often lack subject knowledge [57]. This problem is
further exacerbated by the ongoing debate about the appropriate
approach for teaching programming in primary schools: Program-
ming concepts can be taught unplugged, using computers, or with
physical approaches using programmable robots and microcon-
trollers. Even within the specific domain of programming with
robotics, new educational robots emerge regularly [12, 61].

The lack of appropriate subject knowledge particularly affects
the ability to provide feedback [19] and to assist when learners face
challenges such as debugging their programs [42, 68]. Especially
for giving corrective and meaningful feedback, content knowledge
as well as pedagogical content knowledge are needed: Elaborated
feedback includes, e.g., explanations of concepts or hints on how
to proceed [43], for which a prerequisite is to understand the root
cause of the problem experienced by the learner; in other words,
a teacher needs to be able to debug the problem on the fly. Conse-
quently, one aim of this paper is to shed light on what are common
problems encountered in primary school programming activities, in
order to adequately prepare teachers to provide feedback on them.
While corrective feedback on problems is crucial in the learning
process, it however might interfere with the experienced fun: Fun is
strongly related to intrinsic motivation [35] and intrinsic motivation
can be decreased by corrective feedback [67]. This is problematic,
as (1) intrinsic motivation is needed to develop individual interest
and to pursue a learning goal independent of extrinsic rewards [56,
67], and (2) motivation and interest are considered some of the
most important aims of programming in primary schools [19] in
general. Moreover, fun activities can lead to situational interest
that represents the earlier phases of interest development [52]. As
a consequence, motivating approaches such as game-based and
active learning [3, 58] are commonly used in the primary school
classroom [21]. Since giving students corrective feedback might
affect the fun the students experience, the second aim of this paper is

 
 
 
 
 
 
WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

Greifenstein et al.

to understand whether the different problems and different tutoring
types of feedback have an effect on fun.

As introducing children to programming early can reduce or
even avoid prejudices and fears [61, 64], it is a common strategy
to encourage girls in particular to become enthusiastic about pro-
gramming [1, 61]. This is expected to increase their self-efficacy
and STEM-related confidence [2, 9] while overcoming their anxiety
of the perceived male dominated computer science and associated
negative stereotypes [1, 33]. In order to support these efforts, it is
important to understand how the problems girls experience affect
them. Therefore, a third aim of this paper is to investigate whether
any gender-specific differences can be observed with respect to
problems and fun experienced.

In this paper, we evaluate the combination of corrective feedback
on common problems and encouraging students via Ozobot Evo
programming activities:

RQ 1: Where do primary school children need support with Ozobot
Evo programming activities?

RQ 2: Do primary school children that receive feedback on a spe-
cific problem or via a specific tutoring component have less fun?

RQ 3: Does gender have an effect on problems or fun?

To answer these research questions, we conducted a study con-
sisting of seven workshops with 41 third and 75 fourth grade stu-
dents. They were tasked to perform seven programming activities
with the Ozobot Evo robot. The students noted their experienced
fun and we noted the studentsâ€™ problems and our feedback. To
support teachers with giving corrective feedback in an encouraging
learning climate, we summarise the studentsâ€™ problems and our
ideas for appropriate and efficient feedback.

Our results indicate that feedback should, whenever possible,
include the participation of the students, for example by giving
hints without revealing the solution. Moreover, feedback on urgent
problems that need to be solved quickly to complete the task, or
problems that might be attributed to personal abilities (and thus
might decrease self-efficacy) tend to be more discouraging than
feedback on other problems. This is why we suggest that especially
task constraints or difficult concepts should be clearly addressed in
front of the class or adequately prepared in additional material.

2 RELATED WORK
The context of our study is primary school education, where the
introduction of programming concepts is frequently conducted
using programmable robots such as the Ozobot Evo.

2.1 Educational Use of Robots
The educational use of microcontrollers and robots in general has
been shown to enhance learning and engage students in STEM
fields [4, 5, 15, 31, 48]. Regarding primary school teachers, the re-
sults are rather mixed [32]: They are significantly less enthusiastic
about using educational robots than secondary school teachers [50].
This might be attributed to their low confidence and lacking knowl-
edge observed at the beginning of workshops on robots: both have
been observed to increase throughout the course of such work-
shop [28, 29].

2.2 Teacher Training on Robotics
Generally, teacher training is the most common strategy of ex-
perienced teachers to master teacher-related challenges [19]. For
researchers and in particular teacher trainers, the question on how
to design a workshop for teachers emerges. Experienced teachers
prefer teacher training that promotes both content knowledge and
pedagogical content knowledge (PCK) [19]. Regarding the PCK,
supporting individual students is considered a main challenge that
is further exacerbated by the lack of time, diagnostic skills of teach-
ers, open-ended nature of programming and ratio of students to
teachers [41, 57, 68]. Teachers should therefore be supported with
dealing with common problems by providing appropriate teacher
training or ideas for auxiliary material for the students.

2.3 Robots and Feedback
There are many studies on the effects of different robots in the pri-
mary school classroom such as LEGO WeDo [39], the mBot [47], or
Bee-Bot [10]. Recent studies focused, e.g., on feedback regarding the
productive collaboration between students [70] or a gender-neutral
design of the robot [61]. However, only few studies focused on
the problems experienced while introducing robots, e.g., with the
Arduino programming syntax, the malfunction of digital pins [73]
or the controllability of the Thymio II robot [54]. To the best of our
knowledge, there is no prior work explicitly focusing on how to
support teachers with giving feedback on the programming activi-
ties, neither for Ozobot robots, nor for others. There are, however,
several experience reports [6, 14, 17, 30, 38, 46, 62, 63, 69]: While
these reports mention some flaws, they nevertheless clearly recom-
mend the use of Ozobots because of its positive effects. This is why
we aim at bridging these two aspects by measuring the experienced
fun, but also investigating the effects of common problems and
their associated feedback.

2.4 Effective Feedback and Fun
For feedback to be effective, it has to fulfil certain criteria such as be-
ing timely and actionable [66]. The latter implies that giving praise
or grades does not suffice to support students in improving [67].
Instead, elaborated feedback should be given, e.g., in terms of hints,
explanations or guiding questions [43]. Generally, corrective feed-
back might at the same time hamper learning as it can impair
motivational aspects [67]. This is crucial, as less motivated learners
process corrective feedback worse [16]. To improve the processing
of corrective feedback, one common assumption is that the students
should enjoy the activities. Motivation and fun are correlated as
â€œintrinsic motivation is defined as the doing of an activity for its
inherent satisfactions [...]â€ [56] and â€œwhen intrinsically motivated
a person is moved to act for the fun or challenge entailed [...]â€ [56].
Fun activities have also been shown to be an effective intrinsic
motivator in programming education [34, 54]. Positive emotions
during learning activities raise situational interest, which in turn
can lead to individual interest in the long term [52] as â€œindividual
interest is characterized by positive emotions, such as enjoyment,
personal meaningfulness, knowledge, and continued engagement
over timeâ€ [53]. To strike a balance between corrective feedback and
fun, thus to enable learning but at the same time reaching the goal
of getting children interested in programming [19], feedback should

Programming Ozobots in Primary School

WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

to the colour of the line. The colour codes consist of two or three
small coloured segments that are inserted into a line and offer more
possibilities: There are predefined colour codes for controlling the
speed, direction, finish, special moves, timers and counters1.

3.1.2 Programming with Digital Devices. The Ozobot Evo can also
be programmed via the app â€œOzobot Evoâ€ or a web interface2. Both
options include the block-based programming language â€œOzoBlocklyâ€.
The blocks are separated into categories and levels and can be drag-
and-dropped to implement a program (see Fig. 1b). While we mainly
used level 1 which contains few and only graphical blocks with
numbers, in the levels 2â€“5 many textual blocks and further cate-
gories such as â€œFunctionsâ€ and â€œListâ€ are available. The programs
of the app are transferred to the Ozobot Evo via Bluetooth.

3.2 Ozobot Evo Activities
The workshop is designed for a duration of 90 minutes, or roughly
two lessons. During this time, the students perform seven Ozobot
Evo activities in the procedure suggested by KÃ¶rber et al. [30] and
summarised in Fig. 2. These activities are based on teaching material
available online and consist of six pen-and-paper mode activities3
and one digital mode activity4. Whenever a student or group of
students completes an activity early, they can spend some time
drawing their own lines for the robot to follow.

3.3 Learning Objectives
The workshops of 90 minutes each aimed at the students...
â€¢ getting to know sensors and actuators of a robot,
â€¢ using abstraction to match desired behaviour of the robot

and available colour codes,

â€¢ observing the robot while executing a path in a specific order

and loops in circles,

â€¢ debugging a given sequence in a block-based environment,
â€¢ collaborating with each other to develop a solution,
â€¢ and moreover, enjoying their first programming experience.
The children first elaborated on each activity in pairs and to
consolidate the learning objectives, we discussed the childrenâ€™s
observations in class (see Fig. 2).

4 RESEARCH METHOD
Using the Ozobot Evo workshop, we aim to empirically answer our
research questions specified at the end of Section 1.

4.1 Participants
We conducted seven workshops with a total of 116 children aged
eight to ten years; 41 students were in the third grade, and 75 in the
fourth grade. In total, there were 55 girls, and 61 boys. Whenever
possible, we split them into groups of two (mostly the seatmates).
This resulted in 19 all-female, 23 all-male, 12 mixed groups (consist-
ing of one girl and one boy), and 8 students worked individually.

1https://play.ozobot.com/print/guides/ozobot-ozocodes-reference.pdf, last accessed
June 3, 2022
2https://ozobot.com/create/ozoblockly, last accessed June 3, 2022
3https://storage.googleapis.com/ozobot-lesson-library/3-5-basic-training-color-
codes/3-5-Basic-Training-Color-Codes-full-version.pdf, last accessed June 3, 2022
4https://storage.googleapis.com/ozobot-lesson-library/ozoblockly-training-k-
1/ozoblockly-training-k-1.pdf, last accessed June 3, 2022

(a) Ozobot Evoâ€™s path of activity 3.

(b) OzoBlockly code for activity 7.

Figure 1: The two programming modes.

also be given in an encouraging way. While there are profound
findings on the encouraging effects of positive feedback [16, 23], in
this paper, we look at ways how to give corrective feedback in an
encouraging way.

2.5 Gender in Robotics
When it comes to increasing the ratio of women in STEM disci-
plines, it is relevant to understand how children learn to program
with robots and thus address potential gender-specific behaviours
in the most effective way. In the field of robot programming, boys
have been reported to perform better than girls in more advanced
tasks such as repeat-loops with KIWIE robotics kit, while no dif-
ference was found in basic tasks [60, 61]. This may be a result of
boys having more exposure to conventional â€œmaleâ€ toys, and thus
are more interested in robotics and take more risks in program-
ming than girls [61, 64]. In addition, girls and boys have different
learning approaches, methods and expectations in programming;
for example, girls prefer discussing and debating in groups more
than boys [45, 55, 64]. In this context, pair programming can help
to increase enthusiasm and learning outcomes in programming in
primary school [26]. In learning groups, it has been shown that stu-
dentsâ€™ grades are slightly better in same-sex groups than in mixed
groups [71] and all-female groups are more engaged with tasks
and follow instructions better [40, 61, 72]. Overall, quick positive
feedback in tasks may result in a higher engagement of girls in
programming activities [64].

3 THE OZOBOT PROGRAMMING

WORKSHOP

In order to enable our investigations, we designed a workshop
that introduces primary school children to programming using the
popular Ozobot Evo robots.

3.1 The Ozobot Evo Robot
We decided to use the Ozobot Evo robot (see Fig. 1a) especially
because it can be programmed in two ways. In both modes, the
Ozobot Evoâ€™s actuators and sensors enable sounds, flashing, and
moving in various ways such as zigzagging or U-turns.

3.1.1 The Pen-and-paper Mode. In the pen-and-paper mode, the
Ozobot Evo can be controlled via colours and colour codes (see
Fig. 1a). It follows coloured lines and its lights change according

WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

Greifenstein et al.

0: Introduction

1+2: Line-Following

3+4: Commands

 Collecting prior knowledge on robots
gg Exploring equipment of Ozobot Evo
 Collecting findings

gg Inserting black (worksheet 1) and coloured
lines (worksheet 2)
 Reflecting on the colour sensor

gg Inserting given colour codes
(worksheet 3 and 4)
 Reflecting on the symmetry

5: Randomness

6: Controlling

7: OzoBlockly

gg Tracking the arrival colour (worksheet 5/left)
 Collecting the data
gg Inserting a code from the table (worksheet 5/right)

gg Inserting suit-
able colour
codes
(worksheet 6)

 Executing sequences using printed commands
 Demonstrating the OzoBlockly app
gg Debugging an erroneous program (worksheet 7)

gg = working in pairs;  = class discussion

Figure 2: Overview of the Programming Workshop Schedule described in Section 3.2.

Figure 3: Columns of the sheet used to take notes.

4.2 Data Collection
In every workshop, three to four supervisors were available to
answer studentsâ€™ questions. In total, there were two male and six
female supervisors, including three researchers and five undergrad-
uates in the field of computer science and teaching studies. None
of them knew the children before which may result in less individ-
ualised but therefore also more neutral feedback as if the feedback
was given by the teacher.

For each activity (Fig. 2), the children first received an expla-
nation and then performed it. When they encountered a problem
they raised a hand, and one of us then gave feedback to this group
until they overcame their difficulties. Immediately after each con-
versation we took a note of it on the sheet shown in Fig. 3. After
each activity (Fig. 2), each child filled in the â€œFun Toolkitâ€, which
measures fun with children efficiently, as it requires only little writ-
ing and reading [49]. We implemented the Fun Toolkit using the
â€œSmileyometerâ€ (Fig. 4a) and the â€œAgain Again Tableâ€ (Fig. 4b).

4.3 Data Analysis
In order to answer the three main research questions, we consider
the two sources of data: The supervisor notes on problems and feed-
back, and the student perception measured with the Fun Toolkit.

Studentsâ€™ Problems and Supervisorsâ€™ Feedback. The notes on
4.3.1
the studentsâ€™ problems and supervisorsâ€™ feedback provided us with
qualitative data on which we applied thematic analysis [8]. We col-
lected themes, then counted them and in a final step again related
them to the original data and our research questions. To ensure
inter-rater reliability (ğ¾ = 0.65), two authors independently an-
notated all qualitative data. To categorise the problems that the
students faced (RQ1), we used the third column of the notes (Fig. 3)
where the supervisor noted the studentsâ€™ statements and the ac-
tual issue. To answer RQ2 we additionally categorised the notes on
the supervisorsâ€™ feedback (fourth column of Fig. 3) extending the
tutoring strategies by Narciss [43].

4.3.2 Correlation of Supervisorsâ€™ Feedback and Studentsâ€™ Fun. The
â€œAgain Again Tableâ€ and the â€œSmileyometerâ€ (Fig. 4) both contain

scales that have to be filled for each activity summarised in Fig. 2.
This provides us with two values between 0 and 4, respectively 0
and 2, per child and activity. We compared the means to find corre-
lations between being given specific feedback and the experienced
fun. Moreover, the values of the used point-biserial correlation co-
efficient ğ‘Ÿğ‘ğ‘ can range from -1 to 1â€”indicating the most negative
(-1) to no (0) to the most positive correlation (1).

4.3.3 Gender Differences. Besides the overall classification of prob-
lems, we further aim to determine if the group constellations have
an influence on the occurrence of these problems (RQ3). After the
thematic analysis, we therefore used a Kruskal-Wallis H test to mea-
sure significant differences between the three group constellations
(female only, male only, mixed). We calculated ğœ‚2 to measure the
effect size. If ğœ‚2 > 0.14 there are large, ğœ‚2 > 0.06 there are medium
and ğœ‚2 > 0.01 there are small effects. We also measured statistical
differences between gender and between the group constellations
using a Wilcoxon Rank Sum test with ğ›¼ = 0.05 and calculated ğ‘Ÿ to
measure the effect size. If ğ‘Ÿ > 0.5 there are large effects, ğ‘Ÿ > 0.3
there are medium and ğ‘Ÿ > 0.1 there are small effects.

4.4 Limitations
The limitations mostly result from the workshop design, which is
one specific workshop which lasts only 90 minutes. However, even
short lessons can increase interest in programming [51]. Teaching
programming with Ozobot robots for a longer time might reveal
further recurring problems as well as problems to be solved only
once. Furthermore, our workshop focused on the pen-and-paper-
mode (activities 1 to 6) but in subsequent lessons, more block-based
programming activities could follow activity 7. This might shift the
focus of the problems from robot-specific content to algorithmic
control structures. Besides the problems, it might also be interesting
to see how the fun develops over time and if (not) using the pen-
and-paper mode in introductory lessons affects the experienced fun
of further lessons.

5 RESULTS
Using the data collected throughout the seven workshops, we em-
pirically answer our three main research questions.

Programming Ozobots in Primary School

WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

â€¢ T5: Other refers to feedback not matching any of the above

categories.

â€¢ T6: Guiding questions can be used to help students arrive

at a solution on their own.

The categorisation of problems resulted in eight distinct cate-
gories (P1â€“P8), which are discussed in detail in the following in the
order of their frequency starting with the most frequent, together
with the forms of feedback used. The problems are discussed in the
context of the associated activities that are summarised in Fig. 2.

5.1.1 P1: Understanding of the Activity. The most frequent prob-
lems encountered by primary school children deal with understand-
ing the activity. For all activities, some students of the workshops
had difficulties understanding the task constraints (P1, Table 1). It
often was sufficient to give an explanation, hints or guiding ques-
tions to the students as feedback (T1, T4 and T6, Table 2). In case
of major comprehension problems, direct instruction was needed
(T3, Table 2).

5.1.2 P2: Wrong Colour Code. Wrong colour codes occur not only
because of misinterpreted directions or U-turns (P5 and P6) but also
by accidentally copying codes incorrectlyâ€”especially for activity 6
where five different codes had to be inserted (P2, Table 1). To enable
the students to proceed, a new worksheetâ€”especially if there are
many mistakesâ€”or a sticker can be given (T2, Table 2).

5.1.3 P3: Ozobot Evo Specific Aspects. Another common problem
deals with the characteristics of the Ozobot Evo such as its sensors
and actuators. As can be seen in Table 2 (T1), giving explanations
on these concepts is the most frequently chosen kind of feedback.
Direct instruction on how to deal with the Ozobot Evoâ€™s character-
istics was also necessary sometimes (T3, Table 2). In activities 5 and
6, the children had to choose colour codes on their own for the first
time (P3, Table 1). This led to some groups being confused by one
colour code described as â€œjumpingâ€. Another frequent issue when
dealing with colour codes relates to drawing a custom line that was
also experienced in another workshop [18]: It is important to draw
a long enough single-coloured line before and after every code
to ensure that the Ozobot Evoâ€™s sensors recognise the respective
colour code.

5.1.4 P4: Reading Directions. Students often recognised the issue
of the reading directions on their own stating â€œIn what order do we
have to draw the colours?â€ or â€œWe should have started with greenâ€.
The support, however, differs: In the first case, the teacher should
explain the concept and in the latter, the teacher just has to hand
out a sticker to cover the wrong code (T1 and T2, Table 2). While
the codes could mostly be drawn in the same order as specified, in
activity 6, the students had to put themselves into the perspective
of the robot: If the robot arrives at the code from the right side, the
code has to be drawn the other way round which made activity 6
complex regarding spatial thinking (P4, Table 1).

5.1.5 P5: U-turns. Also for activity 6, U-turns were introducedâ€”
which seem to be difficult to understand, also because there are
two colour codes for U-turns (P5, Table 1). In most interactions
regarding U-turns, explanations were given, accompanied by white
stickers to correct errors where necessary (T1 and T2, Table 2). This

(a) The Smileyometer.

(b) The Again Again table.

Figure 4: The Fun Toolkit used to measure experienced fun.

Table 1: Problems encountered by the students regarding the
individual activities.

P1

P2

P3

P4

P5

P6

P7

P8

Activity 1
Activity 2
Activity 3
Activity 4
Activity 5
Activity 6
Activity 7

4
6
4
1
6
17
9

0
0
2
4
10
26
0

5
7
1
0
8
8
2

0
0
1
0
4
21
0

0
0
0
0
2
25
0

0
0
0
0
3
10
3

0
0
0
0
0
0
15

0
0
2
0
2
0
6

Table 2: Tutoring feedback type given by the supervisors or-
dered by frequency and related to the problems experienced
by the primary school children.

P1

P2

P3

P4

P5

P6

P7

P8

T1: Explanations
T2: Stickers
T3: Direct Instruction
T4: Hints
T5: Other
T6: Guiding questions

21
0
10
10
6
9

7
38
0
2
0
1

14
2
8
3
4
1

15
11
1
7
1
0

11
8
1
7
3
5

12
5
1
1
0
2

2
0
10
2
1
0

2
0
2
0
6
0

5.1 RQ 1: Problems and Support
To answer RQ1 we consider the supervisorsâ€™ notes (Fig. 3). Table 1
and Table 2 summarise the problems encountered by the primary
school children, and relate them to the activities respectively the
corresponding tutoring feedback type given. The categorisation of
the feedback resulted in six different categories that are inspired
by Narcissâ€™ tutoring feedback strategies [43] and listed by their
frequency:

â€¢ T1: Explanations describe general clarifications on the task

or the robot.

â€¢ T2: Stickers refer to problems where a student error re-
quired white stickers to hide the mistake and let the students
re-do the task.

â€¢ T3: Direct Instruction refers to information on specific
steps to achieve a certain task, for example when using the
app.

â€¢ T4: Hints describe suggestions that indicate how to com-

plete the task.

WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

Greifenstein et al.

Figure 5: Values of the Again Again table.

Figure 6: Values of the Smileyometer.

enables students to cover the wrong code and draw a new colour
code.

5.1.6 P6: Direction. Turning or moving into a specific direction was
an issue in activities 5 to 7 (P6, Table 1). However, students saying,
e.g., â€œItâ€™s not going to the leftâ€ could require different explanations
depending on the misconceptions of the students (T1, Table 2).
While some groups simply confused left and right, others did not
mind that the Ozobot Evo turns randomly at crossings if there
is no corresponding colour code. These issues were solved with
explanations of the directions or the randomness. When codes were
already drawn, stickers also have a supporting effect (T2, Table 2).

5.1.7 P7: App. During activity 7, we demonstrated the relevant
features of the OzoBlockly app (P7, Table 1). However, some groups
needed further direct instruction on deleting blocks and running the
program, or information on programming sequences (T3, Table 2).

5.1.8 P8: Other Aspects. There are further technical and organisa-
tional aspects such as the Ozobot Evo not driving anymore which
is not due to a conceptual understanding of the students, but for
other reasons such as the Ozobot Evo not being turned on or an
empty battery (which happens frequently in the digital mode [38]).
This comparatively trivial problem can best be clarified quickly by
explanations or by replacing the Ozobot Evo (T1 and T5, Table 2).

Summary (RQ1) The eight common problems can be gener-
alised to different degrees: For other learning robots, it might also
be important to ensure a general understanding of the activity
and the app and have explanations for directional movements.
When programming the Ozobot Evo, many common problems
can be solved by providing white stickers and spare robots.

5.2 RQ 2: Feedback and Fun
To answer RQ2 we consider the data of the Again Again table and
the Smileyometer. Fig. 5 and Fig. 6 show the distribution regarding
the activities that are summarised in Fig. 2.

5.2.1 Measured Fun. Generally, the values of fun are very high
with a mean of 1.9 for the Again Again table and 3.72 for the Smiley-
ometer. These values might be considered when looking at Table 3.
Generally, most students wanted to perform the respective activ-
ity again and considered it â€œbrilliantâ€ or â€œreally goodâ€ (see Fig. 6).
The activities 1 and 3 are rated best in the Again Again table with
1.95 on average and activities 5 and 7 are rated worst with 1.85 on
average. In the Smileyometer, activity 4 is rated best with 3.81 on

Table 3: Number of problems and associated fun ratings.
AT = Again Again Table, S = Smileyometer
Coefficients associated with statistically significant ğ‘-values are bold.

Problem

#Total

AT

P1: Task understanding
P2: Wrong colour code
P3: Ozobot Evo specific
P4: Reading direction
P5: U-turns
P6: Direction
P7: App
P8: Other

89
58
62
61
48
50
42
7

1.85
1.81
1.88
1.84
1.97
1.82
1.86
2

ğ‘Ÿğ‘ğ‘

0.03
0.08
-0.01
0.04
-0.1
0.06
0.02
-0.05

S

3.31
3.59
3.83
3.71
3.8
3.69
3.32
3.86

ğ‘Ÿğ‘ğ‘

0.18
0
-0.12
-0.06
-0.1
-0.05
0.11
-0.04

average and activity 7 is rated worst with 3.6. Even though there
is no significant difference, the individual bad ratings of activity 7
could be caused by different factors: Activity 7 was the last activity
of each workshop, which is why there often was insufficient time
to transition properly from the pen-and-paper mode to the digital
mode and the digital mode itself. This might have overwhelmed
some children, and it might be interesting to research good ways of
transitioning and how this might affect the experienced fun when
programming digitally. Another explanation might be that some
students might not have had sufficient time to draw their own path
in the pen-and-paper mode (because they did not complete other
activities early enough to do this additional task). Nevertheless the
ratings for activity 7 are still good (see Fig. 5 and Fig. 6) and the
slight differences might just derive from the pen-and-paper mode
being especially engaging.

5.2.2 Relation between Problems and Fun. Table 3 lists the means
of the fun ratings associated with having received feedback on
specific problems, and the correlation between fun and number of
problems of a type. As is common the case with survey questions
for this target age group, the ratings are generally very positive
(see Fig. 5 and Fig. 6). Table 3 shows that having experienced a
problem and thus having received feedback can however lead to
worse fun ratings (indicated by ğ‘Ÿğ‘ğ‘ values above 0) but also to better
fun ratings (indicated by ğ‘Ÿğ‘ğ‘ values below 0). We therefore take a
closer qualitative look at the individual means and relate them to
the groupsâ€™ problems.

Receiving feedback on rather technical and organisational issues
(â€œP8: Otherâ€) did not affect the experienced fun negatively (Again
Again table: ğ‘ = 0.345, ğ‘Ÿğ‘ğ‘ = âˆ’0.05 ; Smileyometer: ğ‘ = 0.376,

0%0%0%0%2%1%1%95%95%93%91%89%86%86%5%5%7%9%9%13%13%Task7Task6Task5Task4Task3Task2Task110050050100no (0)maybe (1)yes (2)0%0%0%0%0%4%5%97%97%97%96%92%92%89%3%3%3%4%8%4%6%Task7Task6Task5Task4Task3Task2Task110050050100awful (0)not very good (1)good (2)really good (3)brilliant (4)Programming Ozobots in Primary School

WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

Table 4: Number of tutoring type and associated fun ratings.
AT = Again Again Table, S = Smileyometer
Coefficients associated with statistically significant ğ‘-values are bold.

Table 5: Means of fun for each activity for girls and boys.
AT = Again Again Table, S = Smileyometer
m = male students, f = female students

Tutoring type

#Total

AT

T1: Explanations
T2: Stickers
T3: Direct instruction
T4: Hints
T5: Other
T6: Guiding questions

167
97
74
63
50
31

1.86
1.83
1.84
1.94
1.94
1.9

ğ‘Ÿğ‘ğ‘

0.05
0.07
0.05
-0.07
-0.07
-0.02

S

3.58
3.63
3.37
3.9
3.65
3.37

ğ‘Ÿğ‘ğ‘

0.01
-0.03
0.13
-0.17
-0.03
0.08

ğ‘Ÿğ‘ğ‘ = âˆ’0.04). One explanation might be that, e.g., an empty bat-
tery does not harm self-efficacy as these problems are out of the
studentsâ€™ control and rather easy to solve. For all other problems,
students might attribute the problem, and with that the corrective
feedback, to their own abilities. This might lead to a reduced in-
trinsic motivation as the students might not feel competent and
autonomous [56].

Within the problems that might be attributed to the studentsâ€™
abilities, the urgency of the problem might explain most differences
regarding how much the activities were liked. Many problems are
less urgent such as Ozobot Evo specific problems (P3), or U-turns
(P5), as they are rather concerned with one specific code or concept
and the students might meanwhile perform another part of the ac-
tivity. However, if the task is not understood (P1) or the app cannot
be used (P7), the students are not able to proceed. When having
experienced one of these problems, the students rated the like-
ability of the respective activity significantly worse (P1: ğ‘ = .001,
ğ‘Ÿğ‘ğ‘ = 0.18; P7: ğ‘ = 0.022, ğ‘Ÿğ‘ğ‘ = 0.11). As supporting students is a
time-consuming and common challenge in the computer science
classroom [41, 68], students might have to wait for supervisors or
teachers. It seems that this does not strongly affect the studentsâ€™
intention to perform the activity again but it comparatively often
negatively affects the rating if the students liked it (Table 3). This
might be because having to wait for corrective feedback to pro-
ceed might be frustrating [66] and thus reduce the likeability of the
activity. These observations match the general result that having
received feedback led to significantly worse ratings with the Smiley-
ometer than not having received feedback (ğ‘ = 0.029, ğ‘Ÿğ‘ğ‘ = 0.11).
Our data of the Again Again table (Table 3) however indicate that
fun and learning do not have to be mutually exclusive: Besides like-
ability (measured with the Smileyometer), the endurability of the
engagement (measured with the Again Again table) is another char-
acteristic of fun activities [49]. Our results show that the students
want to have further time to apply and practice the respective activ-
ity again which might indicate an interest in learning more about
it. This matches findings that having received corrective feedback
helps to acquire further skills and knowledge [67] and thus enables
learning. As the workshop aims at several learning objectives (see
Section 3), giving corrective feedback is crucial. However, the way
of how to give corrective feedback should be considered.

5.2.3 Relation between Tutoring Component and Fun. Table 4 lists
the means of the fun ratings associated with having received feed-
back via a specific tutoring component, and the correlation between
fun and how much of the feedback type was received. It shows that

A1

A2

A3

A4

A5

A6

A7

AT

S

AT

S

AT

S

AT

S

AT

S

AT

S

AT

S

m 1.93
1.96
f

3.77
3.78

1.9
1.93

3.75
3.8

1.93
1.96

3.7
3.75

1.9
1.96

3.74
3.89

1.93
1.76

3.77
3.62

1.85
1.91

3.6
3.79

1.86
1.83

3.64
3.55

Table 6: P-values, effect sizes and means of encountered
problems regarding group constellations.
m2 = all-male groups, f2 = all-female groups, fm = mixed groups

P1

P2

P3

P4

P5

P6

P7

P8

P1 to P8

ğ‘
0.154
ğœ‚2
0.03
0.39
m2
f2
0.68
fm 0.33

0.888
-0.03
0.52
0.47
0.5

0.447
-0.01
0.26
0.42
0.5

0.093
0.05
0.26
0.58
0.5

0.806
-0.03
0.39
0.37
0.5

0.029
0.1
0.17
0.32
0.58

0.651
-0.02
0.22
0.26
0.33

0.341
0
0.13
0.05
0

0.312
0.01
3.3
4.37
3.83

having received tutoring does not necessarily lead to worse fun
ratings. We again take a closer qualitative look at the individual
means and relate them to the tutoring component.

The mean ratings of the Again Again table are not substantially
affected by tutoring components (Table 4). For the Smileyome-
ter, there are statistically significant differences: When students
received direct instruction, the activity is liked significantly less
(ğ‘ = 0.01, ğ‘Ÿğ‘ğ‘ = 0.13). This matches results that direct instruction
is less motivating than, e.g., more cooperative or project-based
learning [11, 22]. However, direct instruction is proven to be very
effective, which is why modern and less strict forms such as Explicit
Direct Instruction in Programming suggested by Hermans and Smit
might be an appropriate solution [25].

Hints, on the contrary, affected the Smileyometer ratings in a pos-
itive way (ğ‘ = .001, ğ‘Ÿğ‘ğ‘ = âˆ’0.17). Giving hints involves the children
to think and thus gives them a partial autonomy in their learning
which might increase their intrinsic motivation [56]. As is known
from automated hints during programming, hints are especially
effective when they include a self-explanation prompt [37].

Summary (RQ2) Our results indicate that corrective feedback
can lead to worse fun ratings if the feedback is urgently needed
and might harm self-efficacy. Tutoring components such as direct
instruction can lead to worse, and hints to better fun ratings.
While the intention to perform an activity again tends to be only
slightly reduced, the likeability of an activity can be significantly
reduced by having received corrective feedback.

5.3 RQ 3: Effects of Gender
Since introducing programming at primary school level is one of the
strategies of addressing the issue of underrepresented groups, we
are specifically interested in understanding whether the problems
encountered differ based on gender-related group constellations.
Table 6 shows whether the group constellation had an effect on the
number of a specific problem.

WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

Greifenstein et al.

There is only one statistically significant difference: Mixed groups
needed feedback significantly more often on the problem of mov-
ing or turning into a specific direction (P6) (ğ‘ = 0.029, ğœ‚2 = 0.1).
Regarding the overall number of experienced problems, there is no
statistically significant difference between all-female, all-male and
mixed groups (ğ‘ = 0.312, ğœ‚2 = 0.01). Thus, the workshop tends to
be equally challenging for all group constellations. Other studies
found gender differences for more complex tasks, but basic tasks
were also performed similarly well [60, 61]. Our experiences match
these results as the Ozobot Evo programming activities deal with
introductory tasks and topics such as sequences.

Considering fun, on average over all activities girls and boys
liked programming with Ozobot Evo robots similarly with the
Smileyometer mean of 3.74 for female and 3.71 for male students
(Table 5). Only for activity 5, boys rated it significantly better with
the Again Again table than girls (ğ‘ = 0.019, ğ‘Ÿ = 0.19). This activ-
ity includes more letting the Ozobot Evo drive and less actively
drawing something (see Fig. 2) which might cause gender-specific
differences. For the group constellations there are no significant
differences but very similar results for all-female groups (Again
Again table: 1.9, Smileyometer: 3.75), all-male groups (Again Again
table: 1.9, Smileyometer: 3.69) and mixed groups (Again Again table:
1.88, Smileyometer: 3.72). The workshop tends to be very enjoyable
for both girls and boys. That matches former results where the
Ozobot robot raised the interest of young female learners [18].

Summary (RQ3) Programming the Ozobot Evo is suitable for
encouraging girls as the course was evaluated positively regard-
less of gender. Our results indicate that mixed groups might need
more feedback on directional movements.

6 DISCUSSION
We found several common problems (RQ 1) that can lead to worse
fun ratings when the feedback on them is urgently needed or when
the feedback is given via direct instruction (RQ 2). As the fun ratings
are generally very high, this course fulfils its goal of fostering
childrenâ€™s initial interest in programming, both for girls and boys
(RQ 3). Different categories of motivation appear to play a role while
children are acting, learning and achieving goals. One category
deals with explaining childrenâ€™s engagement and investment. Since
enjoyment is a well established reason in this context (see Ecclesâ€™
Expectancy-Value Theory) [65], we investigate the development of
fun during our intervention. Further research is needed to examine
the relations between positive emotions and learning when giving
corrective feedback to students working with educational robots.

6.1 Addressing Common Problems in Advance
In RQ 1, we explored common problems so that teachers can build
knowledge on them and prepare auxiliary material in advance to
counteract the challenge of supporting students [19, 57, 68]. Some
problems could be addressed in class and we discuss if this applies
to other programming activities, too.

To reduce the requests for feedback on the understanding of
the task, a presentation or worksheet with task constraints might
support students to remember and understand it. Parental support
can also redirect to the task, e.g., when programming with the

KIBO robotics kit [51], and could be realised in the form of a vis-
itorâ€™s day [19], while guiding questions on the task were rather
overwhelming when programming Thymio robots [13].

Introductory tasks often deal with robotic movements, e.g., for
Lego Mindstorms, the Bee-Bot or the Thymio robot [7, 27, 54]. To
understand the path of the robot, the students can be encouraged
to put themselves into the view of the robot, walking the path
step by step or rotating the map with the path. Moreover, the start
and end point could be emphasised verbally and/or highlighted in
colour for the students to remember. For robots that include â€œjumpsâ€
in their features, such as the Ozobot Evo, one can make clear in
advanceâ€”if the robots do not really jump in the airâ€”that they do
not have elastic springs. The actuators are often discussed as an
initial activity of a robotic workshop [30, 54].

While the colour codes are pre-programmed for the Ozobot Evo,
tasks for other robots also often deal with coloured lines, e.g., line-
following or reacting to colours [27, 54, 59]. To provide an initial
hands-on experience, some colour codes could be programmed by
the educator for other robots, too. This would allow sequences or
loops to be easily visualised by the robotâ€™s path without immediately
opening the black box. Moreover, the difference between static code
(= path with colour codes) and running code dynamically (= robot
moving along the path) could be addressed explicitly as it can be
shown in a comprehensible way. After that, for the transitioning
from a non-digital to the digital programming mode, one could
combine both modes by an animation: The colour codes could be
replaced by commands and put in the form of a script.

6.2 Giving Encouraging Feedback
Renninger [52] proposed a phase model of interest development,
that starts with phases describing situational interest. According
to this model, positive feelings are helpful to maintain situational
interest. This matches the aim of promoting interest when pro-
gramming in the primary school classroom [19]. While the Ozobot
Evo and the workshop setting generally had positive effects on the
childrenâ€™s fun, we still found differences depending on the specific
problem and tutoring component given to the children in RQ 2.

To raise situational interest, feedback should ensure that the
children are granted to solve their problem autonomously to some
extent. We found that hints should be preferred over direct in-
struction. As the basic human needs of autonomy, relatedness, and
competence are crucial for building intrinsic motivation [56], we
focussed on hints that empowered children to solve the problem
without direct instruction of the teacher. In our setting this is done
in a co-constructive process enabling children to experience success
when solving problems. Crucial for this process are hints that are
reasonably balanced between being too unspecific and revealing the
solution directly. Timing is another critical factor that needs to be
considered: Feedback should on the one hand be given timely [66],
but on the other hand slightly delayed so that the students are not
relying solely on feedback [13]. Finally we suggest that all informa-
tion which cannot at all be figured out independently and/or is not
crucial for the specific learning process should be given in front of
the whole class rather than being part of an individual feedback.

Programming Ozobots in Primary School

WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

6.3 Preparing Material for Individual Students
In RQ 3, we found that gender has no major effect on problems or
fun. Still, other factors of heterogeneity might cause different kind
of problems such as some groups needing support in the earlier ac-
tivities while most groups didnâ€™t (see Table 1). To reach all students,
experienced teachers differentiate their learning material [19] and
we discuss how this applies to robot programming activities.

Not all specific problems should be addressed in detail in front
of the class as students should first have the chance of solving a
problem on their own. To support struggling students, auxiliary ma-
terial can be given to them. This might save time during lessons, but
preparing it of course takes time beforehand. When programming
a robot requires using an app, general features such as connecting
robots or deleting code should be explained. Additionally, explana-
tions of further potential issues should be outsourced. One idea is
to use edited screen captures addressing subproblems.

When struggling to find the correct (colour) code, hints or guid-
ing questions could be printed out. There are somewhat ambiguous
results regarding these tutoring feedback types: While hints when
programming might be useful in an educational context [20, 44],
effects of guiding questions might depend on their meta-cognitive
complexity and the learnersâ€™ self-direction [13]. When wrong (colour)
codes have been used, the code has to be tested and debugged it-
eratively. For the Ozobot Evo robots, stickers are a common way
to handle colour code mistakes [17] (Table 2). This refers to, e.g.,
deleting a block in a block-based programming environment. Gen-
erally, it might be useful to have a dedicated desk where auxiliary
material, e.g., stickers or explanatory sheets, are provided.

7 CONCLUSIONS
In order to explore what are the problems that primary school
children encounter while working with Ozobot Evo robots, which
corrective feedback can be used to address these problems, and to
what extent this influences the fun, we conducted seven workshops
with 41 third and 75 fourth graders with the Ozobot Evo robot. We
observed that most of the problems were caused by task constraints
and wrong (colour) codes. We found that the corrective feedback on
the problems partially influenced the funâ€”depending on the type
of the problem and the tutoring component: We therefore suggest
to give prepared hints rather than using direct instruction.

Overall, the course was evaluated very positively by the students,
regardless of gender. We conclude that an introductory program-
ming course with Ozobot Evo robots can be equally enjoyable for
girls and boys. As a next step, we plan to evaluate auxiliary mate-
rial for common problems when programming the Ozobot Evo and
to study the effects of self-selected feedback on the studentsâ€™ self-
efficacy, fun and learning when building and programming another
robot. Moreover, we suggest to offer more advanced subsequent
coursesâ€”such as parent-child coursesâ€”in the future in order to
promote a more well-developed individual interest.

ACKNOWLEDGMENTS
This work is supported by the Federal Ministry of Education and Re-
search through project â€œprimary::programmingâ€ (01JA2021) as part
of the â€œQualitÃ¤tsoffensive Lehrerbildungâ€, a joint initiative of the
Federal Government and the LÃ¤nder. The authors are responsible
for the content of this publication.

REFERENCES
[1] Efthimia Aivaloglou and Felienne Hermans. 2019. How Is Programming Taught
in Code Clubs? Exploring the Experiences and Gender Perceptions of Code
Club Teachers. In Proceedings of the 19th Koli Calling International Conference
on Computing Education Research (Koli Calling â€™19). Association for Computing
Machinery, Koli, Finland, 1â€“10.

[2] K. Albusays, P. Bjorn, L. Dabbish, D. Ford, E. Murphy-Hill, A. Serebrenik, and
M.-A. Storey. 2021. The Diversity Crisis in Software Development. IEEE Software
38, 2 (March 2021), 19â€“25.

[3] Karen Anewalt. 2008. Making CS0 fun: an active learning approach using toys,
games and Alice. Journal of Computing Sciences in Colleges 23, 3 (2008), 98â€“105.
[4] Saira Anwar, Nicholas Alexander Bascou, Muhsin Menekse, and Asefeh Kardgar.
2019. A systematic review of studies on educational robotics. Journal of Pre-
College Engineering Education Research (J-PEER) 9, 2 (2019), 2.

[5] Fabiane Barreto Vavassori Benitti. 2012. Exploring the educational potential of
robotics in schools: A systematic review. Computers & Education 58, 3 (2012),
978â€“988.

[6] Martina Benvenuti, Sara Giovagnoli, and Elvis Mazzoni. 2019. Using educational
robot to enhance the potential of creative thinking in children.. In PSYCHOBIT.
[7] Idoia Beraza, Alfredo Pina, and Barbara Demo. 2010. Soft & Hard ideas to improve
interaction with robots for Kids & Teachers. Proceedings of SIMPAR (2010), 549â€“
557.

[8] Manfred Max Bergman. 2010. Hermeneutic content analysis: Textual and audio-
visual analyses within a mixed methods framework. SAGE Handbook of Mixed
Methods in Social and Behavioral Research. Thousand Oaks, SAGE (2010), 379â€“396.
[9] Amiangshu Bosu and Kazi Zakia Sultana. 2019. Diversity and Inclusion in
Open Source Software (OSS) Projects: Where Do We Stand?. In 2019 ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement
(ESEM). 1â€“11.

[10] Loredana Cacco and Michele Moro. 2014. When a Bee meets a Sunflower. In
Proceedings of 4th International Workshop Teaching Robotics Teaching with Robotics
and 5th International Conference on Robotics in Education, Padova, Italy. 68â€“75.

[11] Colette Carrabba and Aarek Farmer. 2018. The impact of project-based learn-
ing and direct instruction on the motivation and engagement of middle school
students. Language Teaching and Educational Research 1, 2 (2018), 163â€“174.
[12] Dave Catlin, Martin Kandlhofer, Stephanie Holmquist, Andrew Paul Csizmadia,
Julian Angel-Fernandez, and J Cabibihan. 2018. Edurobot taxonomy and Papertâ€™s
paradigm. Constructionism (2018), 151â€“159.

[13] Morgane Chevalier, Christian Giang, Laila El-Hamamsy, Evgeniia Bonnet, Vaios
Papaspyros, Jean-Philippe Pellet, Catherine Audrin, Margarida Romero, Bernard
Baumberger, and Francesco Mondada. 2022. The role of feedback and guidance
as intervention methods to foster computational thinking in educational robotics
learning activities for primary school. Computers & Education 180 (2022), 104431.
[14] Pao-Nan Chou. 2018. Little engineers: Young childrenâ€™s learning patterns in an
educational robotics project. In 2018 World Engineering Education Forum-Global
Engineering Deans Council (WEEF-GEDC). IEEE, 1â€“5.

[15] Miguel Ã Conde, Francisco J RodrÃ­guez-Sedano, Camino FernÃ¡ndez-Llamas, JosÃ©
GonÃ§alves, JosÃ© Lima, and Francisco J GarcÃ­a-PeÃ±alvo. 2021. Fostering STEAM
through challenge-based learning, robotics, and physical devices: A systematic
mapping literature review. Computer Applications in Engineering Education 29, 1
(2021), 46â€“65.

[16] Samantha DePasque and Elizabeth Tricomi. 2015. Effects of intrinsic motivation
on feedback processing during learning. NeuroImage 119 (2015), 175â€“186.
[17] Rostislav Fojtik. 2017. The Ozobot and education of programming. New Trends

and Issues Proceedings on Humanities and Social Sciences 4, 5 (2017).

[18] Jean H French and Hailey Crouse. 2018. Using early intervention to increase
female interest in computing sciences. Journal of Computing Sciences in Colleges
34, 2 (2018), 133â€“140.

[19] Luisa Greifenstein, Isabella GraÃŸl, and Gordon Fraser. 2021. Challenging but Full
of Opportunities: Teachersâ€™ Perspectives on Programming in Primary Schools.
In 21st Koli Calling International Conference on Computing Education Research.
1â€“10.

[20] Luisa Greifenstein, Florian ObermÃ¼ller, Ewald Wasmeier, Ute Heuer, and Gordon
Fraser. 2021. Effects of Hints on Debugging Scratch Programs: An Empirical
Study with Primary School Teachers in Training. In The 16th Workshop in Primary
and Secondary Computing Education. 1â€“10.

[21] Thomas Hainey, Thomas M Connolly, Elizabeth A Boyle, Amanda Wilson, and
Aisya Razak. 2016. A systematic literature review of games-based learning
empirical evidence in primary education. Computers & Education 102 (2016),
202â€“223.

[22] Martin HÃ¤nze and Roland Berger. 2007. Cooperative learning, motivational
effects, and student characteristics: An experimental study comparing cooperative
learning and direct instruction in 12th grade physics classes. Learning and
instruction 17, 1 (2007), 29â€“41.

[23] John Hattie. 2008. Visible learning: A synthesis of over 800 meta-analyses relating

to achievement. routledge.

WiPSCE â€™22, October 31-November 2, 2022, Morschach, Switzerland

Greifenstein et al.

[24] Fredrik Heintz, Linda Mannila, and Tommy FÃ¤rnqvist. 2016. A review of models
for introducing computational thinking, computer science and computing in
K-12 education. In FIE â€™16. 1â€“9.

[25] Felienne Hermans and Marileen Smit. 2018. Explicit Direct Instruction in Program-
ming Education. In Proceedings of the 29th Annual Conference of the Psychology
of Programming Interest Group (PPIG 2018). 86â€“93.

[26] Olivera Iskrenovic-Momcilovic. 2019. Pair Programming with Scratch. Educ Inf

Technol 24, 5 (Sept. 2019), 2943â€“2952.

[27] Karen H Jin, Kathleen Haynie, and Gavin Kearns. 2016. Teaching elementary
students programming in a physical computing classroom. In Proceedings of the
17th annual conference on information technology education. 85â€“90.

[28] Jennifer S Kay and Janet G Moss. 2012. Using robots to teach programming to
K-12 teachers. In 2012 Frontiers in Education Conference Proceedings. IEEE, 1â€“6.
[29] ChanMin Kim, Dongho Kim, Jiangmei Yuan, Roger B Hill, Prashant Doshi, and
Chi N Thai. 2015. Robotics to promote elementary education pre-service teachersâ€™
STEM engagement, learning, and teaching. Computers & Education 91 (2015),
14â€“31.

[30] Nina KÃ¶rber, Lisa Bailey, Luisa Greifenstein, Gordon Fraser, Barbara Sabitzer,
and Marina Rottenhofer. 2021. An Experience of Introducing Primary School
Children to Programming using Ozobots (Practical Report). In The 16th Workshop
in Primary and Secondary Computing Education. 1â€“6.

[31] Svetlana KubilinskienË™e, Inga Å½ilinskienË™e, Valentina DagienË™e, and Vytenis Sinke-
viÄius. 2017. Applying robotics in school education: A systematic review. Baltic
journal of modern computing 5, 1 (2017), 50â€“69.

[32] A Lathifah, CW Budiyanto, and RA Yuana. 2019. The contribution of robotics ed-
ucation in primary schools: Teaching and learning. In AIP Conference Proceedings,
Vol. 2194. AIP Publishing LLC, 020053.

[33] Alex Lishinski, Aman Yadav, Jon Good, and Richard Enbody. 2016. Learning to
Program: Gender Differences and Interactive Effects of Studentsâ€™ Motivation,
Goals, and Self-Efficacy on Performance. In Proceedings of the 2016 ACM Con-
ference on International Computing Education Research. ACM, Melbourne VIC
Australia, 211â€“220.

[34] Ju Long. 2007. Just For Fun: using programming games in software programming
training and education. Journal of Information Technology Education: Research 6,
1 (2007), 279â€“290.

[35] Thomas W Malone and Mark R Lepper. 2021. Making learning fun: A taxonomy of
intrinsic motivations for learning. In Aptitude, learning, and instruction. Routledge,
223â€“254.

[36] Linda Mannila, Valentina Dagiene, Barbara Demo, Natasa Grgurina, Claudio
Mirolo, Lennart Rolandsson, and Amber Settle. 2014. Computational thinking in
K-9 education. In ITICSE â€™14. 1â€“29.

[37] Samiha Marwan, Joseph Jay Williams, and Thomas Price. 2019. An evaluation of
the impact of automated programming hints on performance and learning. In
Proceedings of the 2019 ACM Conference on International Computing Education
Research. 61â€“70.

[38] KarolÃ­na MayerovÃ¡, Zuzana KubincovÃ¡, and Michaela VeselovskÃ¡. 2019. Creating
Activities for After School Robotic Workshop with Ozobot Evo. In 2019 18th
International Conference on Information Technology Based Higher Education and
Training (ITHET). IEEE, 1â€“5.

[39] KarolÃ­na MayerovÃ© and Michaela VeselovskÃ¡. 2017. How to teach with LEGO

WeDo at primary school. In Robotics in education. Springer, 55â€“62.

[40] Charlie McDowell, Linda Werner, Heather E. Bullock, and Julian Fernald. 2006.
Pair Programming Improves Student Retention, Confidence, and Program Quality.
Commun. ACM 49, 8 (Aug. 2006), 90â€“95.

[41] Tilman Michaeli and Ralf Romeike. 2019. Current status and perspectives of
debugging in the k12 classroom: A qualitative study. In 2019 ieee global engineering
education conference (educon). IEEE, 1030â€“1038.

[42] Tilman Michaeli and Ralf Romeike. 2019.

Improving debugging skills in the
classroom: The effects of teaching a systematic debugging process. In Proceedings
of the 14th workshop in primary and secondary computing education. 1â€“7.
[43] Susanne Narciss. 2013. Designing and evaluating tutoring feedback strategies

for digital learning. Digital Education Review 23 (2013), 7â€“26.

[44] Florian ObermÃ¼ller, Ute Heuer, and Gordon Fraser. 2021. Guiding next-step hint
generation using automated tests. In Proceedings of the 26th ACM Conference on
Innovation and Technology in Computer Science Education V. 1. 220â€“226.

[45] Sofia Papavlasopoulou, Kshitij Sharma, and Michail N. Giannakos. 2020. Cod-
ing Activities for Children: Coupling Eye-Tracking with Qualitative Data to
Investigate Gender Differences. Computers in Human Behavior 105 (April 2020),
105939.

[46] K Picka, M Dosedla, and L Stuchlikova. 2020. Robotic didactic aid Ozobot in Czech
schools. In 2020 18th International Conference on Emerging eLearning Technologies
and Applications (ICETA). IEEE, 525â€“533.

[47] Jelena Pisarov and Gyula Mester. 2019. Programming the mbot robot in school. In
Proceedings of the International Conference and Workshop Mechatronics in Practice
and Education, MechEdu. 45â€“48.

[48] Mareen Przybylla and Ralf Romeike. 2018. Impact of physical computing on
learner motivation. In Proceedings of the 18th Koli Calling International Conference

on Computing Education Research. 1â€“10.

[49] Janet C Read. 2008. Validating the Fun Toolkit: an instrument for measuring
childrenâ€™s opinions of technology. Cognition, Technology & Work 10, 2 (2008),
119â€“128.

[50] Natalia Reich-Stiebert and Friederike Eyssel. 2016. Robots in the classroom: What
teachers think about teaching and learning with education robots. In International
conference on social robotics. Springer, 671â€“680.

[51] Emily Relkin, Madhu Govind, Jaclyn Tsiang, and Marina Bers. 2020. How parents
support childrenâ€™s informal learning experiences with robots. Journal of Research
in STEM Education 6, 1 (2020), 39â€“51.

[52] K Ann Renninger. 2009. Interest and identity development in instruction: An

inductive model. Educational psychologist 44, 2 (2009), 105â€“118.

[53] K Ann Renninger and Suzanne E Hidi. 2019. The Cambridge handbook of motiva-

tion and learning. Cambridge University Press.

[54] Fanny Riedo, Morgane Chevalier, StÃ©phane Magnenat, and Francesco Mondada.
2013. Thymio II, a robot that grows wiser with children. In 2013 IEEE workshop
on advanced robotics and its social impacts. IEEE, 187â€“193.

[55] Miguel Angel Rubio, Rocio Romero-Zaliz, Carolina MaÃ±oso, and Angel P. de
Madrid. 2015. Closing the Gender Gap in an Introductory Programming Course.
Computers & Education 82 (March 2015), 409â€“420.

[56] Richard M Ryan and Edward L Deci. 2000. Intrinsic and extrinsic motivations:
Classic definitions and new directions. Contemporary educational psychology 25,
1 (2000), 54â€“67.

[57] Sue Sentance and Andrew Csizmadia. 2017. Computing in the curriculum: Chal-
lenges and strategies from a teacherâ€™s perspective. Education and Information
Technologies 22, 2 (2017), 469â€“495.

[58] Philipp Straubinger and Gordon Fraser. 2022. Gamekins: Gamifying Software

Testing in Jenkins. arXiv preprint arXiv:2202.06562 (2022).

[59] Juing-Huei Su, Chyi-Shyong Lee, Hsin-Hsiung Huang, Sheng-Hsiung Chuang,
and Chih-Yuan Lin. 2010. An intelligent line-following robot project for introduc-
tory robot courses. World Transactions on Engineering and Technology Education
8, 4 (2010), 455â€“461.

[60] Amanda Sullivan and Marina Umaschi Bers. 2013. Gender Differences in Kinder-
gartenersâ€™ Robotics and Programming Achievement. Int J Technol Des Educ 23, 3
(Aug. 2013), 691â€“702.

[61] Amanda Sullivan and Marina Umashi Bers. 2016. Girls, Boys, and Bots: Gender
Differences in Young Childrenâ€™s Performance on Robotics and Programming
Tasks. JITE:IIP 15 (2016), 145â€“165.

[62] Karin Tengler. 2020. Klein, kreativ, Ozobot: FÃ¶rderung von KreativitÃ¤t und
informatischem Denken durch spielerisches Programmieren. R&E-SOURCE
(2020).

[63] Diane van der Linde, Nicole van der Aar, and Joke Voogt. 2018. Best of The
Netherlands: How children use computational thinking skills when they solve a
problem using the Ozobot. In EdMedia+ Innovate Learning. Association for the
Advancement of Computing in Education (AACE), 2151â€“2157.

[64] Tina Vrieler, Aletta NylÃ©n, and Ã…sa Cajander. 2020. Computer Science Club
for Girls and Boys â€“ a Survey Study on Gender Differences. Computer Science
Education (Oct. 2020), 1â€“31.

[65] Allan Wigfield, Jacquelynne S Eccles, Ulrich Schiefele, Robert W Roeser, and
Pamela Davis-Kean. 2006. Development of achievement motivation. John Wiley &
Sons, Inc.

[66] Grant Wiggins. 2012. Seven keys to effective feedback. Feedback 70, 1 (2012),

10â€“16.

[67] Benedikt Wisniewski, Klaus Zierer, and John Hattie. 2020. The power of feedback
revisited: a meta-analysis of educational feedback research. Frontiers in Psychology
10 (2020), 3087.

[68] Aman Yadav, Sarah Gretter, Susanne Hambrusch, and Phil Sands. 2016. Expanding
computer science education in schools: understanding teacher experiences and
challenges. Computer Science Education 26, 4 (2016), 235â€“254.

[69] Martin Å½Ã¡Äek and Pavel Smolka. 2019. Development of Computational thinking:
Student motivation using Ozobot. In Proceedings of the 2019 3rd International
Conference on Education and E-Learning. 36â€“40.

[70] Zarifa Zakaria, Jessica Vandenberg, Jennifer Tsan, Danielle Cadieux Boulden,
Collin F. Lynch, Kristy Elizabeth Boyer, and Eric N. Wiebe. 2022. Two-Computer
Pair Programming: Exploring a Feedback Intervention to Improve Collaborative
Talk in Elementary Students. Computer Science Education 32, 1 (2022), 3â€“29.

[71] Amir Zeid and Rehab El-Bahey. 2011.

Impact of Introducing Single-Gender
Classrooms in Higher Education on Student Achievement Levels: A Case Study
in Software Engineering Courses in the GCC Region. In 2011 Frontiers in Education
Conference (FIE). T2Hâ€“1â€“T2Hâ€“6.

[72] Zehui Zhan, Patrick Fong, Hu Mei, and Ting Liang. 2015. Effects of Gender
Grouping on Studentsâ€™ Group Performance, Individual Achievements and At-
titudes in Computer-Supported Collaborative Learning. Computers in Human
Behavior 48 (July 2015), 587â€“596.

[73] Baichang Zhong and Tingting Li. 2020. Can Pair Learning Improve Studentsâ€™
Troubleshooting Performance in Robotics Education? Journal of Educational
Computing Research 58, 1 (2020), 220â€“248.

