2
2
0
2

g
u
A
2
1

]

A
N
.
h
t
a
m

[

1
v
4
9
1
6
0
.
8
0
2
2
:
v
i
X
r
a

Parallel QR Factorization of Block Low-Rank Matrices

M. Ridwan Apriansyah1 and Rio Yokota2

1School of Computing, Tokyo Institute of Technology
ridwan@rio.gsic.titech.ac.jp
2Global Scientiﬁc Information and Computing Center, Tokyo Institute of Technology
rioyokota@gsic.titech.ac.jp

Abstract

We present two new algorithms for Householder QR factorization of Block Low-Rank (BLR) matrices:
one that performs block-column-wise QR, and another that is based on tiled QR. We show how the
block-column-wise algorithm exploits BLR structure to achieve arithmetic complexity of O(mn), while the
tiled BLR-QR exhibits O(mn1.5) complexity. However, the tiled BLR-QR has ﬁner task granularity that
allows parallel task-based execution on shared memory systems. We compare the block-column-wise BLR-
QR using fork-join parallelism with tiled BLR-QR using task-based parallelism. We also compare these
two implementations of Householder BLR-QR with a block-column-wise Modiﬁed Gram-Schmidt (MGS)
BLR-QR using fork-join parallelism, and a state-of-the-art vendor-optimized dense Householder QR in
Intel MKL. For a matrix of size 131k × 65k, all BLR methods are more than an order of magnitude faster
than the dense QR in MKL. Our methods are also robust to ill-conditioning and produce better orthogonal
factors than the existing MGS-based method. On a CPU with 64 cores, our parallel tiled Householder
and block-column-wise Householder algorithms show a speedup of 50 and 37 times, respectively.

Keywords: Block low-rank matrix, QR factorization, householder reﬂections, task-based execution

1 Introduction

QR factorization plays a central role in solving scientiﬁc and engineering problems. It factorizes a matrix A ∈ Rm×n
(m ≥ n) into

(1)
A = QR,
where Q ∈ Rm×m is orthogonal (QQT = QT Q = I) and R ∈ Rm×n is upper triangular. QR factorization is well-known
as a stable direct method to solve least squares problems [40]. It is also used as a stable solver for linear systems,
possibly as an alternative to LU decomposition without pivoting [19], and has been used in eﬃcient algorithms for
computing polar decomposition [31] and spectral decomposition [32].

A wide range of problems in computational science requires factorizing dense matrices. Since traditional factorization
methods require O(n3), they have become the bottleneck for large-scale computation. Therefore, many techniques
have been proposed to perform eﬃcient factorization by exploiting the underlying structure of the matrices. A notable
example is the low-rank structure arising from the discretization of integral equations, where the resulting full-rank
matrices have been shown to possess many rank-deﬁcient oﬀ-diagonal blocks [21]. This observation provides the basis
of hierarchical low-rank representations [13, 8, 6] that greatly reduce storage requirement and allow factorization in
linear-polylogarithmic time. A similar low-rank structure has also been exploited in solving Toeplitz least squares
problems [41] and sparse least squares problems [18, 17].

Block Low-Rank (BLR) matrices [3] exploit a similar low-rank property, but produce a ﬂat 2D blocked structure
unlike the aforementioned hierarchical representations. This results in arithmetic complexity of O(n2) for LU and QR
factorization [4, 24]. Even though the hierarchical representations achieve lower complexity than BLR, the simplicity
and ﬂexibility of the BLR format make it easy to use in the context of a general-purpose, algebraic solver [5]. Its
simple, non-hierarchical structure is also eﬃcient on parallel computers [27, 1, 15, 37]. Moreover, it has been shown
that matrix-vector multiplication based on BLR-matrices is signiﬁcantly faster than Hierarchical matrices for a large
number of processes [25]. For these reasons, BLR-matrices may be a better choice, at least for some problem classes
and sizes [5].

In recent years, BLR factorization has generated considerable research interest. BLR-LU direct solvers have been
shown to perform better than full-rank solvers in a sequential environment [5]. Cholesky direct solvers using the BLR

1

 
 
 
 
 
 
format have been utilized for weather modeling [2] and PASTIX supernodal solver [33] on multicore architectures.
BLR direct solvers have also been used in large-scale computation on distributed memory systems for seismic and
electromagnetic [38] and geospatial statistics problems [1, 14].

However, QR factorization of BLR-matrices is not a well-studied problem. Ida et al. have combined the Gram
Schmidt orthogonalization with BLR-matrix arithmetic to perform QR factorization on distributed memory systems
[24]. Even so, the method still relies on the traditional fork-join approach that has relatively large synchronization
overhead. It may also suﬀer from numerical instability as it inherits the property of Gram-Schmidt iteration. In this
article, we present two new algorithms for the QR factorization of BLR-matrices: one that performs block-column-wise
QR based on the blocked Householder method [19], and another one that is based on the tiled QR [20]. Using
the numerically stable Householder triangularization and BLR-matrix arithmetic, the block-column-wise algorithm
achieves a theoretical complexity of O(mn), while the tiled BLR-QR exhibits O(mn1.5) complexity. Nonetheless,
the tiled BLR-QR has ﬁner granularity that allows for parallel task-based execution on shared memory systems.
This leads to an out-of-order execution with very loose synchronization compared to the fork-join model. Numerical
experiments show that our algorithms are more than an order of magnitude faster than the vendor-optimized dense
Householder QR in Intel MKL. Moreover, our algorithms are robust to ill-conditioning and achieve higher parallel
speedups compared to the existing Gram-Schmidt-based algorithm.

The rest of this article is organized as follows. In Section 2, we ﬁrst summarize well-known methods to perform
blocked and tiled QR decomposition of dense matrices. In Section 3 we brieﬂy introduce BLR-matrices and elaborate
on diﬀerent methods to perform QR decomposition on them. We begin with an overview of the existing method
that relies on modiﬁed Gram-Schmidt iteration. Then we explain our new algorithms: the ﬁrst one that performs
block-column-wise Householder QR; followed by one that is based on the tiled Householder QR. We then present the
parallelization of our algorithms in Section 4, using both traditional fork-join and modern task-based execution models
on shared-memory systems. Section 5 presents the results of various numerical experiments to show the performance
and accuracy of our algorithms using several examples. Section 6 concludes this article.

2 Block Dense QR

In this section, we summarize the standard blocked and tiled methods for QR decomposition of dense matrices.
Although they have similar O(mn2) arithmetic complexity as the unblocked version, the blocked and tiled methods
are known to be more eﬃcient on modern supercomputers since they are rich in Level-3 BLAS operations that provide
high performance on memory hierarchy systems [11].

To facilitate this, we assume that the matrix A ∈ Rm×n is subdivided into p × q square blocks of size b × b, where
b is the chosen block size, p = m/b, and q = n/b. Extension to rectangular block is possible with extra permutation
steps.

2.1 Blocked Modiﬁed Gram-Schmidt Dense QR

The modiﬁed Gram Schmidt (MGS) orthogonalization is a well-known method for computing QR factorization. The
blocked version of this method is proposed in [26]. Consider a matrix A ∈ Rm×n = [A1 A2] such that A1 and A2 are
the block-columns of A. Then we can rewrite Equation 1 as

[A1 A2] = [Q1 Q2]

(cid:20) R1,1 R1,2
R2,2

0

(cid:21)

.

(2)

Thus A can be orthogonalized by the following steps:

1. Orthogonalize block-column A1 = Q1R1,1.
2. Compute R1,2 = QT
3. Update A2 ← A2 − Q1R1,2.

1 A2.

4. Orthogonalize block-column A2 = Q2R2,2.

These steps can be extended for arbitrary number of block columns, as shown in Algorithm 1. This is typically the
method of choice when A is well-conditioned. However, when A is ill-conditioned, this method suﬀers from numerical
instability due to rounding errors inherent in ﬂoating-point arithmetic on computers [40].

2.2 Blocked Householder Dense QR

Householder triangularization is the principal method of QR factorization for its numerical stability. The blocked
version of this method is proposed in [19], where the basic idea is to reorganize the computation by applying the
Householder transformations in a cluster of columns at a time, i.e. triangularizing one block-column at a time.

2

Algorithm 1: Blocked Modiﬁed Gram-Schmidt (MGS) QR factorization

Input: A with p × q blocks
Output: Q with p × q blocks and R with q × q blocks such that A = QR

1 for j = 1 to q do
2

[Qj, Rj,j] = QR(Aj)
for k = j + 1 to q do

Rj,k = QT
Ak ← Ak − QjRj,k

j Ak

3

4

5

end

6
7 end

Before we proceed to the blocked version, let us recall the standard non-blocked method. Householder QR produces
a factorization as in Equation 1 by performing Householder triangularization on A. However, unlike the Gram Schmidt
QR, it does not directly produce the orthogonal factor Q. Instead, it performs in-place factorization such that in the
end A is replaced with R in its upper triangular part and Householder vectors in its lower triangular part. These
Householder vectors are then used to construct Q when needed. This extra step of construction from Householder
vectors is rich in matrix-vector multiplication and cannot fully utilize the parallelism in modern computers. For this
reason, the compact WY representation [36] has been proposed, which accumulates Householder reﬂectors such that

Q = I − Y T Y T ,

(3)

where Y ∈ Rm×n is a unit lower trapezoidal matrix containing Householder vectors, and T ∈ Rn×n is upper triangular.
The generation of Y and T requires extra steps of O(n3). Using this scheme, Y can still be stored in the lower
triangular part of A, but additional storage for T is needed. Throughout this article, we assume that Householder QR
factorization on dense matrices uses this representation to store Q.

For the sake of simplicity, consider the matrix A ∈ Rm×n partitioned into a 3 × 2 block matrix





A =

A1,1 A1,2
A2,1 A2,2
A3,1 A3,2



 ,

where Ai,j ∈ Rb×b. If we rewrite Equation 1 as





A1,1 A1,2
A2,1 A2,2
A3,1 A3,2





 = Q



R1,1 R1,2
R2,2
0

0
0



 ,

A can be triangularized as follows:


1. Triangularize block-column



2. Update





R1,2
A2,2
A3,2


 ← ˆQT

1





3. Triangularize block-column

A1,1
A2,1
A3,1


.

A1,2
A2,2
A3,2
(cid:20) A2,2
A3,2


 = ˆQ1







.

R1,1
0
0

(cid:21)

= ˆQ2

(cid:20) R2,2
0

(cid:21)

.

The orthogonal factor Q can then be constructed as

Q = ˆQ1

(cid:20) I
0

(cid:21)

.

0
ˆQ2

(4)

(5)

(6)

Algorithm 2 shows the generalization of blocked Householder QR based on step (1)-(3) above for an arbitrary number
of blocks. Note that the Householder triangularization ultimately amounts to multiplication by QT from the left. Thus,
when needed, multiplication with Q can be done by performing similar steps in the correct order and transposition.
Algorithm 3 shows the method to multiply a matrix C by Q from the left, which has roughly the same cost as
Algorithm 2.

3

Algorithm 2: Blocked Householder QR factorization

Input: A with p × q blocks
Output: Y, R with p × q blocks and T with 1 × q blocks such that R is upper triangular and Y, T contain

intermediate orthogonal factors





1 for k = 1 to q do
Ak,k
Ak+1,k
...
Ap,k











QR

2















= ˆQk








Rk,k
0
...
0








, such that ˆQk = I −








Yk,k
Yk+1,k
...
Yp,k








Tk








T







Yk,k
Yk+1,k
...
Yp,k

3

4

for j = k + 1 to q do

Update








Rk,j
Ak+1,j
...
Ap,j








← ˆQT
k








Ak,j
Ak+1,j
...
Ap,j








end

5
6 end

Algorithm 3: Left multiplication by Q from blocked Householder algorithm

Input: Y with p × q blocks, T with 1 × q blocks, C with p × q blocks
Output: C ← QC

1 for k = q downto 1 do
2

for j = q downto k do



3

Update






end

4
5 end

Ck,j
Ck+1,j
...
Cp,j



←






I −














Yk,k
Yk+1,k
...
Yp,k








Tk








Yk,k
Yk+1,k
...
Yp,k








T 





















Ck,j
Ck+1,j
...
Cp,j

2.3 Tiled Householder Dense QR

Gunter and Van De Geijn decomposed the operations of blocked Householder QR further to obtain the tiled Householder
method [20]. The method takes its root in the updating factorization technique [19, 39]. From Equation 5, the upper
triangularization of A can also be done by:

1. Upper triangularize block A1,1 = ˆQ1,1R1,1.
2. Compute R1,2 = ˆQT

1,1A1,2.
(cid:20) R1,1
A2,1

3. Upper triangularize

4. Update

(cid:21)

(cid:20) R1,2
A2,2

← ˆQT
2,1

(cid:21)
.

(cid:20) R1,2
A2,2
(cid:21)

(cid:48)

(cid:21)

= ˆQ2,1

(cid:21)

(cid:48)

(cid:20) R1,1
0

, which zeroes A2,1 and updates R1,1.

5. Upper triangularize

(cid:20) R1,1
A3,1

= ˆQ3,1

(cid:20) R1,1
0

(cid:48)(cid:48)

(cid:21)

, which zeroes A3,1 and updates R1,1

(cid:48).

6. Update

(cid:21)

(cid:20) R1,2
A3,2

← ˆQT
3,1

(cid:20) R1,2
A3,2

(cid:21)

.

7. Upper triangularize block A2,2 = ˆQ2,2R2,2.
(cid:20) R2,2
0

8. Upper triangularize

(cid:20) R2,2
A3,2

= ˆQ3,2

(cid:21)

(cid:48)

(cid:21)

, which zeroes A3,2 and updates R2,2.

Algorithm 4 shows the generalization of step (1)-(8) for an arbitrary number of blocks. Analogous to the blocked
Householder method, the construction of Q can also be done by performing similar steps in the correct order and

4

transposition. Algorithm 5 shows the method to left-multiply by Q, which also has roughly the same cost as Algorithm
4.

It is important to see that the tiled Householder QR is similar to the blocked Householder QR, except that the
upper triangularization of a block column is decomposed into multiple, smaller operations involving at most two
blocks at a time. This improves granularity and data locality of the operations. However, this leads to a larger storage
requirements as we need to store more ”T” matrices from the intermediate QR factorizations. On the contrary, the
same approach to decompose block-column operation is quite diﬃcult to apply to the blocked MGS method since
orthogonalization needs all information of the column.

Algorithm 4: Tiled Householder QR factorization

Input: A with p × q blocks
Output: Y, T, R with p × q blocks such that R is upper triangular and Y, T contain intermediate orthogonal

factors

1 for k = 1 to q do
2

QR (Ak,k) = ˆQk,kRk,k, such that ˆQk,k = I − Yk,kTk,kY T
k,k
for j = k + 1 to q do

Rk,j = ˆQT

k,kAk,j

end
for i = k + 1 to p do
(cid:21)(cid:19)
(cid:18)(cid:20) Rk,k
Ai,k
for j = k + 1 to q do

QR

= ˆQi,k

(cid:21)

(cid:20) Rk,j
Ai,j

← ˆQT
i,k

(cid:21)

(cid:20) Rk,j
Ai,j

(cid:21)

(cid:20) R(cid:48)
k,k
0

, such that ˆQi,k = I −

(cid:20)

I
Yi,k

(cid:21)

(cid:20)

Ti,k

I
Yi,k

(cid:21)T

10

end

end

11
12 end

Algorithm 5: Left multiplication by Q from tiled Householder algorithm

Input: C, Y, T with p × q blocks
Output: C ← QC

1 for k = q downto 1 do
2

for i = p downto k + 1 do
for j = q downto k do

Update

(cid:20) Ck,j
Ci,j

(cid:21)

←

end

end
for j = q downto k do

Update Ck,j ← (I − Yk,kTk,kY T

k,k)Ck,j

end

(cid:32)(cid:20)

(cid:21)

(cid:20)

Ti,k

I
Yi,k

I
Yi,k

(cid:21)T (cid:33) (cid:20) Ck,j
Ci,j

(cid:21)

3

4

5

6

7

8

9

3

4

5

6

7

8

9

10 end

3 Block Low-Rank QR

3.1 Block Low-Rank Matrices

Block low-rank matrices exploit the low-rank property by performing ﬂat 2D blocking and storing rank-deﬁcient
(admissible) blocks in low-rank representation. We brieﬂy introduce them in this section. For a detailed explanation,
we refer the reader to [3].

Given a dense matrix A ∈ Rm×n, a block size b is chosen to subdivide the matrix into p × q (p = m/b, q = n/b)

5

square blocks such that

˜A =










˜A1,1

˜A2,1
...
˜Ap,1

˜A1,2

· · ·

· · ·
· · ·

· · ·

· · ·

· · ·
· · ·










,

˜A1,q
...
...
˜Ap,q

where

(cid:40)

˜Ai,j =

Ai,j
Ui,jV T
i,j

(Ai,j is not admissible)
(Ai,j is admissible)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 ≤ i ≤ p, 1 ≤ j ≤ q

(7)

i,j is the low-rank compressed form of Ai,j so that Ui,j, Vi,j ∈ Rb×ri,j . In practice, the compression rank is
and Ui,jV T
chosen adaptively for each block. Given a prescribed error tolerance (cid:15) > 0, we choose ri,j = r(cid:15) to be the smallest
integer that satisﬁes (cid:107) ˜Ai,j − Ai,j(cid:107)F ≤ (cid:15)(cid:107)Ai,j(cid:107)F , where (cid:107) · (cid:107)F denotes the Frobenius norm. This implies the overall
BLR compression error

(cid:107) ˜A − A(cid:107)F ≤ (cid:15)(cid:107)A(cid:107)F
is also bounded by (cid:15) (see [23]). Let us denote r as the maximum rank of the admissible blocks. In many problem
classes, r can be shown to be much smaller than n [9, 7]. Throughout this article we assume that r is a small constant,
i.e. r = O(1).

(8)

The admissibility condition determines whether a block is admissible for low-rank compression. There are mainly
two types of admissibility conditions. One is the weak admissibility where all oﬀ-diagonal blocks are assumed to be
admissible. Another one is the strong admissibility where inadmissible oﬀ-diagonal blocks exist. Under the strong
admissibility condition, one typically use the admissibility constant η > 0 to determine admissible blocks based on the
underlying geometric information of the matrix (see [4] for details). Here we assume that the larger the η, the more
inadmissible oﬀ-diagonal blocks exist. When geometric information is not available, one approach is to attempt to
compress each block and revert the ones with large rank (e.g. larger than b/2) back to dense form. This requires extra
operations which is usually negligible in case of BLR compression.

The choice of admissibility condition depends on the problem. When all oﬀ-diagonal blocks of the matrix have
small rank, e.g. in 2D Poisson problems, weak admissibility condition is usually suﬃcient. However when the matrix
has full-rank oﬀ-diagonal blocks, e.g. in 3D Helmholtz problems, strong admissibility is commonly preferred. Figure 1
shows the examples of BLR-matrices with diﬀerent admissibility conditions. Grey-ﬁlled squares represent inadmissible
(dense) blocks and the other represent admissible (low-rank) blocks.

Figure 1: Example of block low-rank matrices: weakly admissible (left); strongly admissible (right)

The storage and computational costs involving BLR-matrices depend on the chosen block size and admissibility
condition. Unlike hierarchical representations where the (minimum) block size only contributes to lower order terms
of the cost, block size in BLR has a signiﬁcant eﬀect due to the ﬂat blocking scheme. It has been shown in [4] that
n) leads to O(n1.5) storage for a square BLR-matrix and O(n2) cost for
the optimal choice of block size b = O(
BLR factorization. Moreover, assuming that the cost to compress an admissible block is O(b2r), which for example is
achievable using Rank Revealing QR [19] or randomized SVD [22], constructing a BLR-matrix also requires O(n2).
Here we assume that each admissible block is compressed using Rank Revealing QR factorization. This produces the
approximation ˜Ai,j = Ui,jV T

i,j such that Ui,j has orthonormal columns.

√

Operations on BLR-matrices are similar to that of blocked dense matrices, with the addition of basic operations

involving dense and low-rank blocks. Let us deﬁne four operations:

6

OP1: Low-Rank + Dense = Dense
OP2: Low-Rank + Low-Rank = Low-Rank
OP3: Low-Rank × Dense = Low-Rank
OP4: Low-Rank × Low-Rank = Low-Rank

OP1 requires converting the low-rank block to dense form followed by dense blocks addition, which amounts to a total
cost of O(b2r). In OP2, two low-rank blocks can be summed by simply agglomerating their components. However, this
would make the rank grow very quickly. Therefore, the rounded addition method in Algorithm 6 [8, p. 16] is employed
to eﬃciently re-compress the resulting matrix back to rank r(cid:15). Here we assume that recompression is performed every
time two low-rank matrices are added, leading to a cost of O(br2) per operation. For OP3, the operation boils down to
multiplication between the dense block and V T part of the low-rank block from the right (or U part if it’s from the
left), which costs O(b2r). Lastly, for OP4, the resulting low-rank block C = A × B is formed by

UA
(cid:124)(cid:123)(cid:122)(cid:125)
UC

A UBV T
V T
,
B
(cid:125)
(cid:123)(cid:122)
(cid:124)
V T
C

which costs O(br2).

Algorithm 6: Rounded Low-Rank Addition

A , B = UBV T
Input: A = UAV T
Output: A + B ≈ C = UC V T

B , UA, VA ∈ Rb×rA , UB, VB ∈ Rb×rB
C , UC , VC ∈ Rb×r(cid:15)

(cid:3), V = (cid:2) VA VB

1 U = (cid:2) UA UB
2 [QU , RU ] = QR(U ), [QV , RV ] = QR(V )
3 [u, σ, v] = SV D(RU RT
4 UC ← ﬁrst r(cid:15) columns of QU u
5 VC ← ﬁrst r(cid:15) columns of QV vσ

V )

(cid:3)

3.2 Blocked Modiﬁed Gram-Schmidt BLR-QR

Ida et al.
[24] have combined the blocked MGS method with BLR-matrix arithmetic to formulate a BLR-QR
decomposition algorithm. Given a weakly admissible BLR-matrix ˜A, the algorithm produces two BLR-matrices ˜Q and
˜R under two assumptions:
• ˜Q and ˜R are BLR-matrices with the same structure as ˜A
• The oﬀ-diagonal blocks ˜Qi,j and ˜Ri,j are approximated using the same error threshold as the corresponding ˜Ai,j.
Under these conditions, the matrices ˜Q and ˜R serve as the approximate QR decomposition of ˜A, such that ˜A ≈ ˜Q ˜R.
The algorithm proceeds in the same way as Algorithm 1 with some operations tailored for BLR-matrices. Line 2 of
Algorithm 1, which corresponds to orthogonalization of a block column, is performed based on the method presented
in [10]. The matrix multiplications in line 4 and 5 are performed using BLR-matrix arithmetic.

The QR factorization of a block column is described as follows. First, let us write the block column ˜Aj as

˜Aj =








=















˜A1,j
˜A2,j
...
˜Ap,j

˜AU
1,j
˜AU
2,j

˜AU
p,j

˜AV
1,j
˜AV
2,j
...
˜AV
p,j








,

where

˜AU

i,j =

(cid:40)

Ib
Ui,j

( ˜Ai,j is not admissible)
( ˜Ai,j is admissible)

, ˜AV

i,j =

(cid:40) ˜Ai,j
V T
i,j

( ˜Ai,j is not admissible)
( ˜Ai,j is admissible)

(9)

(10)

for i = 1, 2, . . . , p. Note that we have just written each ˜Ai,j in left-orthogonal form. Now since each ˜AV
block, we can write the matrix

i,j is a dense








(11)

˜AV
1,j
˜AV
2,j
...
˜AV
p,j

Bj =








7

and perform the dense MGS QR factorization Bj = ˜QB
j
the subdivision of Bj in Equation 11. Since we know that each ˜AU
orthogonal factor as

˜RB

j . Then we partition the matrix ˜QB

j back according to
i,j has orthonormal columns, we can obtain the



˜AU
1,j

˜Qj =








0
...
0

0

˜AU
2,j
. . .
· · ·

· · ·
. . .
. . .
0

0

0

0
˜AU
p,j



˜QB

j =















˜AU
1,j
˜AU
2,j

˜AU
p,j

˜QB
1,j
˜QB
2,j
...
˜QB
p,j








,

(12)

and upper triangular ˜Rj,j = ˜RB
Assuming a block size b = O(

√

j such that ˜Aj = ˜Qj ˜Rj,j, that is the QR factorization of the block column ˜Aj.

n), this algorithm has an arithmetic complexity of O(mn), which is faster than the

O(mn2) dense MGS algorithm. We refer the reader to [24] for a detailed explanation.

3.3 Blocked Householder BLR-QR

Our ﬁrst algorithm follows the blocked Householder dense QR in Section 2.2 to perform orthogonal triangularization
of BLR-matrices. However, a BLR-matrix may contain admissible (low-rank) blocks that need to be handled in a
diﬀerent way than inadmissible (dense) blocks. Thus, the operations need to be extended to handle these low-rank
blocks. Let ˜A be a BLR-matrix as deﬁned in Equation 7. Our algorithm uses the steps from Algorithm 2 to produce
the approximate QR factorization ˜A ≈ ˜Q ˜R such that:
• ˜Q and ˜R are BLR-matrices with the same structure as ˜A.
• The admissible blocks ˜Qi,j and ˜Ri,j are approximated using the same error threshold as the corresponding admissible

block ˜Ai,j.

3.3.1 Triangularization of block column

The ﬁrst operation that we need to redeﬁne is the QR factorization of a block-column (line 2 of Algorithm 2). We adopt
a similar approach to the one presented in [29]. Let us write the k-th block column that needs to be triangularized as








˜Ak,k
˜Ak+1,k
...
˜Ap,k








=








˜AU
k,k

k,k

˜AU

k+1,k

k+1,k

˜AV
˜AV
...
˜AV

˜AU
p,k

p,k








(13)

where ˜AU
i,k
Householder triangularization to factorize the matrix

˜AV

i,k for i = k, k + 1, . . . , p are the left-orthogonal forms of ˜Ai,k as deﬁned in Equation 10. Next we perform








˜AV
˜AV

k,k

k+1,k

...
˜AV

p,k








= ˜QV
k








˜Rk,k
0
...
0








,

where

˜QV

k = I −








Yk,k
Yk+1,k
...
Yp,k








Tk








T








Yk,k
Yk+1,k
...
Yp,k

(14)

(15)

such that Tk ∈ Rb×b and Yi,k has the same dimension as ˜AV
columns, we set ˜Yi,k = ˜AU

i,kYi,k to obtain the orthogonal factor

i,k for i = k, k + 1, . . . , p. Since each ˜AU

i,k has orthonormal

ˆQk = I −








˜Yk,k
˜Yk+1,k
...
˜Yp,k








Tk








˜Yk,k
˜Yk+1,k
...
˜Yp,k

T








.

(16)

It is important to note that ˜Yi,k is a low-rank block if the corresponding ˜Ai,k is low-rank, otherwise it is dense.

8

To see why this works, let us deﬁne W = diag( ˜AU

W = diag(I, ˜AU

k+1,k, . . . , ˜AU

p,k). Multiplying ˆQT

k+1,k, . . . , ˜AU
k to the block-column in Equation 13 yields

k,k, ˜AU

p,k). Because diagonal blocks are dense,

ˆQT
k








˜Ak,k
˜Ak+1,k
...
˜Ap,k








=
















I − W

= W

(cid:17)T

(cid:16) ˜QV

k

Yk,k
Yk+1,k
...
Yp,k
˜AV
˜AV








k,k

k+1,k

...
˜AV

p,k








T T
k








Yk,k
Yk+1,k
...
Yp,k

T







W T








= W








˜Rk,k
0
...
0








=























W

˜Rk,k
0
...
0








k,k

k+1,k

˜AV
˜AV

...
˜AV


p,k

.






(17)

The cost for this operation is dominated by the QR factorization in Equation 14. Each ˜AV

Thus, we have upper triangularized the k-th block-column and at the same time obtained the QR factorization of it.
i,k has a small number
of rows r ((cid:28) b) if ˜Ai,k is low-rank. Under weak admissibility condition, this leads to dense QR factorization of a
(b + (p − k)r) × b matrix that costs O(b3 + pb2r). The cost is similar under strong admissibility condition as long as
the number of dense blocks in a block-column is bounded by a constant.

3.3.2 Apply block column reﬂector
This operation corresponds to line 4 of Algorithm 2 where we multiply the resulting ˆQk with a block-column. Let us
write the operation as

ˆQT
k








˜Ak,j
˜Ak+1,j
...
˜Ap,j








=

=















˜Ak,j
˜Ak+1,j
...
˜Ap,j
˜Ak,j
˜Ak+1,j
...
˜Ap,j






























−








˜Yk,k
˜Yk+1,k
...
˜Yp,k

T T
k








˜Yk,k
˜Yk+1,k
...
˜Yp,k



T 


























˜Ak,j
˜Ak+1,j
...
˜Ap,j








−








˜Yk,k
˜Yk+1,k
...
˜Yp,k

(cid:104)

(cid:105) (cid:104) ˜Y T

k,k

T T
k

˜Ak,j + ˜Y T

k+1,k

˜Ak+1,j + · · · + ˜Y T
p,k

˜Ap,j

(cid:105)

.

(18)

The operation shown in Equation 18 consists of multiplication between dense and low-rank blocks, and accumulation
by low-rank addition. Under the weak admissibility condition, this operation costs O(b2r + pbr2). Under strong
admissibility condition some block-columns require multiplication between dense blocks, leading to a cost of O(b3 +
b2r + pbr2).

3.3.3 Algorithm and Cost Estimate
With the extended operations in place, we can perform blocked Householder QR to a BLR-matrix ˜A. Figure 2 shows
the operations in Algorithm 2 performed on a BLR-matrix with 3 × 3 blocks under weak admissibility condition.

In the following, we estimate the arithmetic complexity of our ﬁrst algorithm. Under the strong admissibility
condition, the structure of the BLR-matrix largely depends on the problem, requiring problem-speciﬁc analysis that is
not the scope of this article. Thus, for simplicity let us assume the weak admissibility condition.

Table 1 shows the cost breakdown of one k-iteration in Algorithm 2. We get the total operation count by taking

the sum for k = 1, 2, . . . , q:

Tblocked(m, n, b) =

q
(cid:88)

b3 + pb2r + (q − k)(b2r + pbr2)

k=1
(cid:18)
1
2

=

2nb2 + 2mnr + n2r +

mn2r2

b2 − nbr −

(cid:19)

.

mnr2
b

√

Setting b = O(

n) yields the arithmetic complexity of O(mn). In terms of storage, this algorithm uses the existing
space of ˜A plus extra space to store T1, T2, . . . , Tq matrices, each of size b × b. This extra storage is not larger than
the space for a BLR-matrix, meaning that this algorithm requires O(m

n) storage.

√

9

Figure 2: Graphical representation of operations in Algorithm 2 on a BLR-matrix with p = q = 3. Thick borders show
the tiles that are being read and green ﬁlls shows the tiles that are being written at each step

Table 1: Operations inside one k-iteration (1 ≤ k ≤ q) of Algorithm 2 and their costs

Operation

Complexity Number of calls

Triangularization of block-column
Apply block-column reﬂector

b3 + pb2r
b2r + pbr2

1
q − k

3.4 Tiled Householder BLR-QR

Our second algorithm is based on the tiled Householder dense QR explained in Section 2.3. It proceeds in the same
way as Algorithm 4 to produce an approximate QR factorization ˜A ≈ ˜Q ˜R under the same assumption as our ﬁrst
algorithm. In the following, we explain how we extend the operations to handle low-rank blocks and estimate the
overall cost of the algorithm.

3.4.1 Householder QR factorization of diagonal block

This operation corresponds to line 2 of Algorithm 4 where we perform Householder triangularization of a diagonal
block

˜Ak,k = ˆQk,k ˜Rk,k.

Since diagonal blocks are always dense, there is no need to handle low-rank blocks. This operation costs O(b3).

3.4.2 Apply block reﬂector

This operation corresponds to line 4 of Algorithm 4 where we multiply an oﬀ-diagonal block with orthogonal reﬂector,
that is:

˜Rk,j = ˆQT

k,k

˜Ak,j.

The oﬀ-diagonal block of a BLR-matrix can be dense or low-rank. If the particular block is low-rank, this operation
amounts to a multiplication of dense and low-rank block that costs O(b2r).

10

k=1 TriangularizeT11Y11R11T1Y21Y31k=1, j=2 Apply TransformationT11Y11R11T1R12Y21Y31k=1, j=3 Apply TransformationT11Y11R11T1R12R13R13k=2 TriangularizeT11Y11R11T1R12Y21Y31Y32R22Y22T11T2R13k=2, j=3 Apply TransformationT11Y11R11T1R12Y21Y31Y32R22Y22T11T2R23k=3 TriangularizeY11R11Y21R13R12Y31Y32T11R23R22Y22R33Y33T2T11T1T33.4.3 Update QR factorization
This operation corresponds to line 7 of Algorithm 4 where we zero the oﬀ-diagonal block below ˜Ak,k using the QR
factorization

QR

(cid:18)(cid:20) ˜Rk,k
˜Ai,k

(cid:21)(cid:19)

= ˆQi,k

(cid:20) ˜R(cid:48)
k,k
0

(cid:21)

.

The upper triangular ˜Rk,k comes from the QR factorization of a diagonal block, so it is always a dense block. However
˜Ai,k can be a dense or low-rank block. In the case of a dense block, we simply concatenate the blocks and perform
Householder dense QR on them. But if ˜Ai,k is low-rank, we ﬁrst perform dense QR factorization of

where

(cid:21)

(cid:20) ˜Rk,k
˜AV
i,k

= ˆQV
i,k

(cid:20) ˜R(cid:48)
k,k
0

(cid:21)

,

˜QV

i,k = I −

(cid:20)

I
Yi,k

(cid:21)

(cid:20)

Ti,k

(cid:21)T

.

I
Yi,k

Because Ui,k has orthonormal columns, we set ˜Yi,k = Ui,kYi,k to obtain the orthogonal factor

ˆQi,k = I −

(cid:20)

I
˜Yi,k

(cid:21)

(cid:20)

Ti,k

(cid:21)T

.

I
˜Yi,k

(19)

(20)

(21)

Note that this is a specialization of the operation explained in Section 3.3.1 where the block column is composed of
a dense upper triangular block on top of an oﬀ-diagonal block. This operation requires O(b3) for both dense and
low-rank ˜Ai,k due to the cost for generating Ti,k.

3.4.4 Apply trapezoidal block reﬂector

This operation corresponds to line 9 of Algorithm 4 where we multiply the orthogonal reﬂector from Equation 21 to
the corresponding blocks, that is

(cid:21)

(cid:20) ˜Rk,j
˜Ai,j

← ˆQT
i,k

(cid:21)

(cid:20) ˜Rk,j
˜Ai,j

=

=

(cid:20) ˜Rk,j
˜Ai,j
(cid:20) ˜Rk,j
˜Ai,j

(cid:21)

(cid:21)

−

−

(cid:32)(cid:20)

(cid:21)

I
˜Yi,k

T T
i,k

(cid:20)

I
˜Yi,k

(cid:21) (cid:104)

T T
i,k

(cid:21)T (cid:20) ˜Rk,j
˜Ai,j

(cid:20)

I
˜Yi,k
(cid:105) (cid:104) ˜Rk,j + ˜Y T

i,k

˜Ai,j

(cid:105)

.

(cid:21)(cid:33)

The cost of this operation depends on the blocks ˜Yi,k, ˜Rk,j, and ˜Ai,j. If ˜Rk,j is low-rank and at least one of { ˜Yi,k, ˜Ai,j}
is low-rank, the cost is O(b2r); Otherwise it is O(b3).

3.4.5 Algorithm and Cost Estimate
Once we have the extended operations deﬁned, we can perform tiled Householder QR on a BLR-matrix ˜A. Figure 3
shows the operations that happen inside one outer iteration of Algorithm 4 on a BLR-matrix with 3 × 3 blocks under
weak admissibility condition.

In the following, we estimate the arithmetic complexity of our second algorithm. Let us also assume the weak
admissibility condition for the sake of simplicity. Table 2 shows the cost breakdown of one k-iteration of Algorithm 4.
Summing up for k = 1, 2, . . . , q gets us the total operation count:

Ttiled(m, n, b) =

q
(cid:88)

(p − k + 1)b3 + (p − k + 1)(q − k)b2r

k=1
(cid:18)
1
6

=

6mnb + 3nb2 +

3mn2r
b

− 3n2b −

(cid:19)

− 3mnr

5n3r
b

√

For b = O(

n), this algorithm has an arithmetic complexity of O(mn1.5), which is slower than the blocked
Householder variant. It also produces more T matrices, which in total amounts to storing a lower trapezoidal m × n
matrix. This leads to a storage requirement that grows similarly to that of dense Householder factorization (O(mn)).
However, this algorithm has ﬁner granularity that makes it more eﬃcient for parallel computation, which will be
shown in later section.

11

Figure 3: Graphical representation of one iteration of the outer loop in Algorithm 4 on a BLR-matrix with p = q = 3.
Thick borders show the tiles that are being read and green ﬁlls shows the tiles that are being written at each step

Table 2: Operations inside one k-iteration (1 ≤ k ≤ q) of Algorithm 4 and their costs

Operation

Complexity Number of calls

QR factorization of diagonal block
Apply block reﬂector
Update QR factorization
Apply trapezoidal block reﬂector

b3
b2r
b3
b2r

1
q − k
p − k
(p − k)(q − k)

4 Multithreaded Block Low-Rank QR

In order to fully utilize modern multi-core architectures, we present the parallel algorithms for the BLR-QR factorization
on shared memory systems. We ﬁrst recall the fork-join parallelization of the blocked MGS-based method [24]. Then
we use a similar fork-join approach to parallelize our proposed algorithms. Lastly, we show the task-based parallel
version of the tiled Householder BLR-QR, which is the main advantage of this variant.

Note that it is also possible to use task-based execution for the blocked MGS and Householder-based methods.
The approach described in [30] consists of representing the algorithm as a Directed Acyclic Graph (DAG) where nodes
represent tasks (either block-column factorization or update) and edges represent dependencies among them. They
refer this as dynamic lookahead technique. However, results show that their technique is still exposed to scalability
problems due to the relatively coarse granularity of the tasks. Therefore we focus on using task-based execution on
the ﬁnely grained tiled Householder-based algorithm.

4.1 Parallel Blocked MGS BLR-QR

Algorithm 1 can be executed in parallel using the fork-join model [16], which has been presented in [24]. The
block-column QR (line 2) can utilize a multithreaded BLAS/LAPACK kernel. The computation of Rj,k for diﬀerent k
(line 4) can be performed in parallel. After that, the updates to Ak can be performed in parallel too. These updates
are ﬁrst decomposed into independent block-by-block multiplications and computed simultaneously. Algorithm 7
shows the fork-join parallel version of Algorithm 1.

12

k=1GEQRTT11Y11R11T11k=1, j=2LARFBT11Y11R11T11R12R13k=1, j=3LARFBT11Y11R11T11R12Y21R13k=1, i=2, j=2TPMQRTT11Y11R11T11R12T21R12Y21R13k=1, i=2TPQRTT11Y11R11T11T21Y21R13k=1, i=2, j=3TPMQRTT11Y11R11T11R12T21Y21Y31R13k=1, i=3, j=2TPMQRTT11Y11R11T11R12T21T31R12Y21Y31R13k=1, i=3TPQRTT11Y11R11T11T21T31Y21Y31R13k=1, i=3, j=3TPMQRTT11Y11R11T11R12T21T31Algorithm 7: Fork-join blocked MGS BLR-QR factorization

Input: ˜A with p × q blocks
Output: ˜Q with p × q blocks and ˜R with q × q blocks such that ˜A ≈ ˜Q ˜R

1 for j = 1 to q do
2

[ ˜Qj, ˜Rj,j] = QR( ˜Aj) // multithreaded
for k = j + 1 to q do in parallel

3

4

5

6

7

˜Rj,k = ˜QT
j

˜Ak

end
forall ˜Ai,k where k > j and 1 ≤ i ≤ p do in parallel

˜Ai,k ← ˜Ai,k − ˜Qi,k ˜Rj,k

end

8
9 end

4.2 Parallel Blocked Householder BLR-QR

The fork-join approach can also be used to parallelize Algorithm 2. Let us look at the dependency among the
operations. Operations in line 4 for diﬀerent j update diﬀerent block-columns and only have a common dependency to
the computation of ˆQk (line 2). Thus they can be computed simultaneously as soon as the computation of ˆQk is done.
The computation of ˆQk itself (line 2) can be done by utilizing a multithreaded BLAS/LAPACK kernel. Algorithm
8 shows the parallel version where the j-loop (line 3) branches oﬀ to become a parallel region. Similarly, the left
multiplication by ˜Q in Algorithm 3 can also be executed in parallel using the same approach.

Algorithm 8: Fork-join blocked Householder BLR-QR factorization

Input: ˜A with p × q blocks
Output: ˜Y , ˜R with p × q blocks and T with 1 × q blocks such that R is upper triangular and ˜Y , T contain

intermediate orthogonal factors





1 for k = 1 to q do
˜Ak,k
˜Ak+1,k
...
˜Ap,k











QR

2















= ˆQk








˜Rk,k
0
...
0








, such that ˆQk = I −








˜Yk,k
˜Yk+1,k
...
˜Yp,k








Tk








˜Yk,k
˜Yk+1,k
...
˜Yp,k

T







// multithreaded

3

4





for j = k + 1 to q do in parallel
˜Rk,j
˜Ak+1,j
...
˜Ap,j

← ˆQT
k

Update


















˜Ak,j
˜Ak+1,j
...
˜Ap,j








end

5
6 end

4.3 Parallel Tiled Householder BLR-QR

Looking at the operations in Algorithm 4, the fork-join approach is also applicable to obtain a parallel algorithm.
The computations of Rk,j in line 4 for diﬀerent j only have a common dependency to the computation of ˆQk,k in
line 2, and thus can be performed in parallel. A similar dependency can be seen among the update operations in
line 9. Algorithm 9 shows the fork-join parallel version of Algorithm 4 where the two j-loops (line 3 and 8) branch
oﬀ to become parallel regions. Lastly, the computation of ˆQk,k (line 2) and ˆQi,k (line 7) can be performed using a
multithreaded BLAS/LAPACK kernel.

As we have mentioned before, the tiled Householder QR has ﬁner granularity compared to the other blocked
algorithms, which could be leveraged in a parallel environment. The idea of exploiting ﬁner granularity of tiled
Householder QR to obtain a highly parallel algorithm has been introduced in [11] in the context of optimizing dense
factorization. Since we are using the same tiled algorithm but adapted to the BLR format, we follow similar steps to
reach an eﬃcient parallelization scheme of our BLR-QR algorithm.

13

3

4

5

6

7

8

9

Algorithm 9: Fork-join tiled Householder BLR-QR factorization

Input: ˜A with p × q blocks
Output: ˜Y , T, ˜R with p × q blocks such that ˜R is upper triangular and ˜Y , T contain intermediate orthogonal

factors

1 for k = 1 to q do
2

QR ( ˜Ak,k) = ˆQk,k ˜Rk,k, such that ˆQk,k = I − ˜Yk,kTk,k ˜Y T
for j = k + 1 to q do in parallel
˜Ak,j

˜Rk,j = ˆQT

k,k

k,k // multithreaded

end
for i = k + 1 to p do
(cid:18)(cid:20) ˜Rk,k
(cid:21)(cid:19)
˜Ai,k

QR

= ˆQi,k

(cid:21)

(cid:20) ˜R(cid:48)
k,k
0

, such that ˆQi,k = I −

(cid:20)

I
˜Yi,k

(cid:21)

(cid:20)

Ti,k

I
˜Yi,k

(cid:21)T

// multithreaded

for j = k + 1 to q do in parallel

(cid:21)

(cid:20) ˜Rk,j
˜Ai,j

← ˆQT
i,k

(cid:21)

(cid:20) ˜Rk,j
˜Ai,j

10

end

end

11
12 end

Let us again look at the operations in Algorithm 4, but this time without limiting ourselves to one k iteration.
It turns out that there are direct dependencies between operations across consecutive iterations of k. This chain of
dependencies is best described by a DAG, where a node represents an operation and an edge represents a dependency
between two operations. Figure 4 shows the dependency graph when Algorithm 4 is executed on a BLR-matrix with
p = q = 3. Note that the construction of ˜Q uses the same set of operations so we can obtain a similar dependency
graph from it.

Figure 4: Dependency graph of Algorithm 4 on a BLR-matrix with p = q = 3

It can be seen from Figure 4 that the DAG also has a recursive structure. For any p1 ≥ p2, the DAG for BLR-matrix
with p2 × p2 blocks is a subgraph of the DAG for BLR-matrix with p1 × p1 blocks. This property allows for reusing
the existing DAG to accelerate the construction of a larger graph.

Once we obtain the DAG, we can use it as a guide in executing the tasks. A task can be started as soon as all
of its dependencies are fulﬁlled. Once a task T is ﬁnished, the scheduler fulﬁlls the dependency of tasks that are
dependent on T . As a result, the threads only need to check the task pool and execute tasks that are ”ready” to be
executed. All threads repeat this cycle until the task pool is empty and the algorithm is ﬁnished. This results in an
out-of-order execution with very loose synchronization required between the threads compared to the fork-join model.
A closer look into the graph in Figure 4 reveals that certain kinds of task have more outgoing edges than the

14

DiagonalQRApplyBlockReﬂectorApplyBlockReﬂectorUpdateQRUpdateQRApply TrapezoidalReﬂectorApply TrapezoidalReﬂectorApply TrapezoidalReﬂectorApply TrapezoidalReﬂectorDiagonalQRApplyBlockReﬂectorUpdateQRApply TrapezoidalReﬂectorDiagonalQRk=1k=2k=3others. This oﬀers a chance for improvement because executing a task with a large number of outgoing edges will
fulﬁll more dependencies, thus bringing more tasks into the “ready” state. Therefore, a priority value can be assigned
to each task, such that a task with more outgoing edges has a higher priority to be executed ﬁrst. In our algorithm, it
is clear that the factorization of diagonal blocks has the highest priority. The second one is updating QR factorization,
followed by the application of trapezoidal and block reﬂectors.

5 Numerical Results

In this section, we demonstrate the performance and accuracy of our algorithms using several example matrices on
a shared-memory system. The BLR-QR algorithms were implemented in C++ where ﬂoating point calculations
were performed in double precision. Fork-join and task-based parallelism were performed using OpenMP. BLAS and
LAPACK routines from Intel MKL were used for the inner kernels involving dense matrices, where single-threaded
kernels were used inside OpenMP parallel regions and multi-threaded kernels were used outside of parallel regions.
We modiﬁed LAPACK’s DGEQP3 routine to obtain a truncated rank revealing QR factorization based on relative error
threshold. Experiments were conducted on a system described in Table 3.

Table 3: Details of system used for experiments

Dual AMD EPYC™ 7502

Clock speed
# cores
Peak performance
Memory
Compiler suite
BLAS & LAPACK library
Multithreading
DGEMM performance

2.5 GHz
2 x 32 = 64
2560 GFlop/s
500 GB
GCC 8.4
Intel MKL 2020.1.217
OpenMP 4.5
1861 GFlop/s

The following algorithms have been compared:
• Dense Householder: Householder QR factorization subroutine of Intel MKL (DGEQRF).
• Blocked MGS: Blocked modiﬁed Gram-Schmidt-based QR factorization of weakly admissible BLR-matrices

explained in Section 3.2.

• Blocked Householder: Blocked Householder-based QR factorization of BLR-matrices explained in Section

3.3.

• Tiled Householder: Tiled Householder-based QR factorization of BLR-matrices explained in Section 3.4.

We evaluate the accuracy of BLR-QR methods using two metrics: one is residual that measures the quality of the
approximate factorization; the other one is orthogonality that measures the quality of the orthogonal factor ˜Q. Both
metrics are respectively given by

where n denotes the matrix size and

√

n is the Frobenius norm of order n identity matrix.

Res =

(cid:107) ˜Q ˜R − A(cid:107)F
(cid:107)A(cid:107)F

, Orth =

(cid:107) ˜QT ˜Q − I(cid:107)F
√
n

,

5.1 Performance on Random BLR Matrices

First, we test our methods using randomly generated BLR matrices such that each diagonal block is a random dense
matrix and each oﬀ-diagonal block is a rank-k matrix obtained from the outer product of two b × k random matrices,
where b is the chosen BLR block size. We assume weakly admissible BLR compression with error tolerance (cid:15) = 10−10.
We ﬁrst show the operation and memory complexities of our algorithms using BLR matrices of varying sizes. We
generate m × n (m = 2n) random BLR matrices with block size b = 2
n and rank k = 1 oﬀ-diagonal blocks. Table
4 shows that our BLR methods produce approximate factorization accurately to the level of the prescribed error
tolerance. Figure 5 shows the ﬂoating-point operations (ﬂops) count and factorization time using a single-core of the
machine, and Figure 6 shows the corresponding memory consumption.

√

15

Table 4: Accuracy on random BLR matrices ((cid:15) = 10−10)

m

n

Blocked Householder

Res

Orth

Tiled Householder
Orth
Res

2,048
8,192
32,768
131,072

1,024
4,096
16,384
65,536

4.9 · 10−15
1.9 · 10−14
2.8 · 10−14
2.2 · 10−13

3.7 · 10−15
8.0 · 10−15
1.7 · 10−14
3.7 · 10−14

6.5 · 10−14
1.6 · 10−13
9.8 · 10−14
3.9 · 10−13

4.1 · 10−13
1.7 · 10−12
1.1 · 10−12
5.3 · 10−14

Figure 5: Flops count (left) and factorization time (right) using a single-core

Figure 6: Memory consumption during executions using a single-core

Figure 5 clearly shows that as the matrix size becomes larger, the ﬂops count of both BLR-QR algorithms grow in
accordance with our estimate in Section 3. The right part of the ﬁgure shows that our BLR methods outperform
Dense QR on large matrices. On the largest matrix (n = 65, 536), the BLR methods are more than an order of
magnitude faster. However, for the smallest matrix (n = 1, 024), the BLR-QR methods, which operate on a set of
small matrix blocks, suﬀer from the suboptimal performance of BLAS libraries on small data sizes. The beneﬁt of
using BLR factorization starts to appear when the matrix is large enough.

Figure 6 shows that compression using BLR-matrix, followed by performing QR factorization on the compressed
form leads to orders of magnitude smaller memory consumption compared to performing direct factorization on the
dense matrix. The memory consumption of both blocked Householder and MGS-based methods grow as O(m
n),
which corresponds to the storage requirement of a rectangular BLR-matrix. Our blocked Householder-based method

√

16

210211212213214215216217Matrix size (n)100102104106Operations (GFlops)210211212213214215216217Matrix size (n)101100101102103104105Factorization time(s)Blocked HouseholderO(mn)Tiled HouseholderO(mn1.5)Dense HouseholderO(mn2)210211212213214215216217Matrix size (n)103102101100101102Memory usage (GB)O(mn)O(mn)Blocked MGSBlocked HouseholderTiled HouseholderDense Householderconsumes slightly less memory compared to the existing blocked MGS-based method because it reuses the lower
triangular part of ˜R (for Y matrices) plus a block diagonal matrix (for T matrices) to implicitly store the orthogonal
factor ˜Q, whereas the MGS-based method explicitly forms two BLR-matrices ˜Q and ˜R during the factorization.
However, the tiled Householder-based method consumes more memory than the other BLR methods since it needs
more additional space to store the T matrices coming from the update QR factorization steps. This leads to a
memory consumption that grows similarly to the dense QR. As a remedy, an inner blocking technique [11, 34] could
be employed to reduce additional storage for the T matrices.

We now demonstrate the parallel scalability of our methods. We use two random m × n (m = 2n) BLR-matrices
with n = 16, 384 and n = 32, 768, block size b = 256, and rank k = 16 oﬀ-diagonal blocks. We compare our parallel
algorithms with the existing parallel blocked MGS-based algorithm. Figure 7, 8, and 9 show the factorization time,
speedup, and ﬂops rate using diﬀerent number of threads, respectively.

Figure 7: Factorization time using diﬀerent number of threads: n=16,384 (left); n=32,768 (right)

Figure 8: Speedup using diﬀerent number of threads: n=16,384 (left); n=32,768 (right)

Figure 7 shows that all BLR methods scale nicely using up to 64 cores of the machine. For the matrix of size
n = 16, 384, the task-based tiled Householder method outperforms the blocked Householder when using 64 cores.
However, on the larger matrix (n = 32, 768), using 64 cores of our machine is not suﬃcient for this to happen. But we
can expect that when the number of threads increases, the tiled method would eventually outperform the blocked
method. This shows that the ﬁner granularity of the tiled Householder method that allows for eﬃcient dynamic
task-based execution is able to overcome the induced extra operations once we have a large number of computing
cores.

17

20212223242526Number of threads102103Factorization time(s)20212223242526Number of threads103104Fork-join Blocked MGSFork-join Blocked HouseholderFork-join Tiled HouseholderTask-based Tiled Householder20212223242526Number of threads20212223242526Speedup20212223242526Number of threads20212223242526Fork-join Blocked MGSFork-join Blocked HouseholderFork-join Tiled HouseholderTask-based Tiled HouseholderFigure 8 shows that the fork-join blocked MGS, tiled Householder, and blocked Householder-based methods showed
a speedup of up to 26, 26, and 37 times, respectively. The fork-join blocked Householder-based methods showed
higher speedups compared to the MGS-based. Even though both of these methods perform similar block-column-wise
QR, the blocked MGS has a bottleneck of computing the ˜Rj,k (line 3-5 of Algorithm 7) [24]. Furthermore, the
blocked Householder outperforms tiled Householder when using fork-join execution model. On the other hand, the
task-based tiled Householder achieved up to 50 times speedup, thanks to the DAG-based execution that fully utilizes
the dependency between operations in the tiled Householder QR, allowing it to scale almost perfectly as the number
of threads increases.

Although the scalability is promising, the actual performance of the BLR-QR algorithms is still far from the peak
performance of the machine, as shown in Figure 9. There are two reasons behind this. First, BLR-QR methods have
to deal with low-rank block operations, which involve manipulating a collection of small matrices instead of one large
matrix, making it more memory-bound [35, 42]. Second is the suboptimal performance of BLAS routines on small data
sizes. Therefore we cannot expect these algorithms to reach the same ﬂops rate as the traditional dense algorithms.

Figure 9: Flops rate using diﬀerent number of threads: n=16,384 (left); n=32,768 (right)

Figure 10: Execution traces of parallel tiled Householder BLR-QR on 8kx8k matrix: fork-join (top, 20.8s); task-based
(bottom, 12.9s). BLR-QR executions start at around 7.5 seconds of the timeline

18

20212223242526Number of threads050100150200250300GFlops/s20212223242526Number of threads050100150200250300Fork-join Blocked MGSFork-join Blocked HouseholderFork-join Tiled HouseholderTask-based Tiled HouseholderSamples of a parallel execution trace are shown in Figure 10 where the grey part corresponds to computation and
the white part corresponds to synchronization and overhead. The fork-join model has many synchronizations involving
all threads, which means threads that completed their task ﬁrst need to wait for the others before proceeding with the
execution. However, the task-based execution showed very loose synchronization because once a thread ﬁnishes a task,
it can take another ”ready” task from the task pool and begin another execution without waiting for other threads.
This leads to an out-of-order execution that eliminates unnecessary synchronizations and signiﬁcantly reduces the idle
time of threads. Furthermore, one can also expect the tiled method to perform better in distributed memory systems
due to its ﬁne granularity that would lead to smaller communication overhead compared to the blocked method. Note
that due to the limitation of OpenMP, in our implementation, the dependency graph is constructed on the ﬂy by the
master thread, which corresponds to the large overhead in the beginning. This is not very eﬃcient when using a small
number of threads since the master thread spends more than 70% of its time generating tasks. This overhead however
is not signiﬁcant when using a large number of threads.

5.2 Accuracy on Ill-Conditioned Matrices

In this example, we demonstrate the numerical stability of our methods in factorizing ill-conditioned matrices. We use
matrices arising from Boundary-Element-Method discretization of Single-Layer Potential (SLP) operator on the unit
circle, generated using H2Lib [12]. The resulting square matrices are ill-conditioned and have oﬀ-diagonal blocks with
small ranks, hence we assume weakly admissible BLR compression. We set the block size b = 2
n and error tolerance
(cid:15) = 10−9.

√

We compare the accuracy of our methods with the existing blocked MGS-based method. Table 5 shows that as the
condition number increases, our Householder methods are robust to this increase and produce numerical orthogonality
on the level of the prescribed tolerance. On the other side, the orthogonality produced by the MGS method clearly
deteriorates as the condition number increases.

Table 5: Accuracy on ill-conditioned matrices ((cid:15) = 10−9)

n

κF (A)

Block Max
Rank
Size

Blocked Householder

Res

Orth

Tiled Householder
Orth
Res

Blocked MGS
Res

Orth

1,024
4,096
16,384
32,768

2.8 · 105
4.6 · 106
7.4 · 107
2.9 · 108

64
128
256
512

11
12
12
13

6.8 · 10−10
1.0 · 10−9
2.1 · 10−9
2.3 · 10−9

6.9 · 10−11
1.2 · 10−10
6.2 · 10−11
6.0 · 10−11

6.1 · 10−10
9.6 · 10−10
1.8 · 10−9
2.0 · 10−9

5.2 · 10−11
4.5 · 10−11
6.7 · 10−11
5.3 · 10−11

5.1 · 10−10
8.6 · 10−10
1.8 · 10−9
2.0 · 10−9

1.9 · 10−8
1.0 · 10−7
1.5 · 10−6
6.9 · 10−6

5.3 Performance on Spatial Statistics Problems

In this example, we use square matrices arising from the Spatial Statistics problem with exponential kernel on uniform
3D grids, generated using STARS-H [28]. The resulting matrices have many oﬀ-diagonal blocks with relatively high
ranks, thus a strongly admissible compression is preferred.

We ﬁrst demonstrate the parallel scalability of our methods for the matrix of order n = 16, 384 compressed with
block size b = 256, tolerance (cid:15) = 10−6, and admissibility constant η = 0.3. Figure 11 shows that using 64 cores the
fork-join tiled and blocked householder methods achieve a speedup of 13 and 17 times, respectively. On the other
hand, the task-based tiled Householder method achieves 47 times speedup, allowing it to become faster than the
blocked method.

Next, we evaluate the performance and accuracy of our methods using matrices of varying sizes and admissibility
constant. We compare our fork-join blocked and task-based tiled Householder with the parallel dense QR of Intel MKL
using 64 cores in terms of factorization time and memory consumption. Table 6 shows that our BLR-QR methods are
able to produce residual and orthogonality to the level of the prescribed error tolerance. It also shows that we can
ﬁne-tune the admissibility constant (introduce more/less oﬀ-diagonal inadmissible block, i.e. decrease/increase the
maximum rank of admissible oﬀ-diagonal blocks) to reach the optimal factorization time and memory consumption.
On the matrix of order 16k, the BLR methods lose to the dense QR. However, for the larger matrix of order 65k, the
BLR methods are faster and achieve up to 80% less memory usage compared to the dense QR.

19

Figure 11: Parallel scalability on spatial statistics problem: factorization time (left); speedup (right)

Table 6: Accuracy on 3D Spatial Statistics problems ((cid:15) = 10−6)

Dense QR

n

Mem Time Block
Size
(s)
(MB)

16,384

2,048

8.26

65,536

32,768

310

256
256
256

512
512
512

BLR
η

0.2
0.3
0.4

0.2
0.3
0.4

Blocked Householder

Tiled Householder

Max Mem Time
(MB)
Rank

(s)

Res

Orth

117
86
58

175
94
51

847
954
1,284

6,698
10,236
14,684

32.5
23
16.4

257.7
185.6
230.9

1.1 · 10−9
8.0 · 10−10
2.1 · 10−10

3.7 · 10−9
1.5 · 10−9
5.1 · 10−10

1.3 · 10−11
1.1 · 10−11
3.8 · 10−12

1.5 · 10−10
8.8 · 10−11
4.3 · 10−11

Mem Time
(MB)

(s)

1,857
1,964
2,293

22,957
26,491
30,937

16.9
13.7
10.8

215.9
218.8
286.9

Res

Orth

1.5 · 10−9
1.4 · 10−9
3.8 · 10−10

6.3 · 10−9
2.8 · 10−9
9.9 · 10−10

1.1 · 10−9
1.0 · 10−9
2.2 · 10−10

4.0 · 10−9
1.8 · 10−9
6.3 · 10−10

5.4 Performance on Inverse Poisson Problems

In this example, we use sparse least squares matrices arising from the Inverse Poisson problem deﬁned on uniform 2D
grids, generated using the MATLAB code of spaQR [18]. We use strongly admissible BLR compression with block size
n and tolerance (cid:15) = 10−10. Since the geometry information is not available, we attempt to compress every
b = 2
oﬀ-diagonal block and revert back to dense the blocks whose rank is larger than b/2.

√

Figure 12 shows the parallel scalability of our BLR methods for the matrix of size 74, 112 × 36, 864. Using 64
cores, the fork-join tiled, fork-join blocked, and task-based tiled Householder methods achieve a speedup of 2, 3, and
14 times, respectively. These relatively lower speedups come from the fact that although the dimension is quite large,
the resulting BLR matrix is dominated by zero blocks that make the actual computation load smaller.

We then compare our fork-join blocked and task-based tiled Householder with the parallel dense QR of Intel MKL
using all 64 cores of the machine. Table 7 shows that the BLR methods are faster than Dense QR while producing
residual and orthogonality to the level of the prescribed error tolerance. Moreover, the tiled method is faster than the
blocked method in all three sparse matrices that we use.

Table 7: Accuracy on 2D Inverse Poisson problems ((cid:15) = 10−10)

Dense QR

Blocked Householder

Tiled Householder

m

n

Mem Time Mem Time
(MB)

(MB)

(s)

(s)

Res

Orth

Mem Time
(MB)

(s)

Res

Orth

74,112
100,800
131,584

36,864
50,176
65,536

20,844
38,587
65,792

200
479
1569

3,727
6,117
9,399

106
190
354

1.4 · 10−11
1.5 · 10−11
1.6 · 10−11

1.3 · 10−11
1.4 · 10−11
1.6 · 10−11

20,017
36,125
60,351

46
82
161

8.4 · 10−12
8.4 · 10−12
8.6 · 10−12

5.7 · 10−12
5.4 · 10−12
5.4 · 10−12

20

20212223242526Number of threads102Factorization time (s)20212223242526Number of threads20212223242526SpeedupFork-join Blocked HouseholderFork-join Tiled HouseholderTask-based Tiled HouseholderFigure 12: Parallel scalability on sparse least squares problem: factorization time (left); speedup (right)

6 Conclusion

We have presented two new algorithms for Householder QR factorization of Block Low-Rank matrices. One that
performs block-column-wise QR based on the blocked Householder method, and another one that performs ﬁne-grained,
block-wise QR based on the tiled Householder method. We have shown that both algorithms exploit BLR structure
to achieve arithmetic complexity of O(mn) and O(mn1.5), respectively. We have compared our algorithms with an
existing BLR-QR method that is based on the blocked Modiﬁed Gram Schmidt iteration. We also compared them to
a state-of-the-art vendor-optimized dense Householder QR of Intel MKL. Numerical experiments showed that all BLR
methods are more than an order of magnitude faster than the dense QR of MKL. The BLR methods are also more
eﬃcient in terms of memory consumption, possibly saving hundreds of gigabytes of memory for huge matrices.

We also have demonstrated the parallelization of our algorithms using both traditional fork-join and modern
task-based execution models. We compared our parallel algorithms with an existing fork-join blocked MGS-based
parallel algorithm. Results showed that our task-based tiled Householder algorithm outperforms the fork-join methods,
thanks to the dynamic task-based execution that allows for out-of-order execution with very loose synchronization
between the threads. We have shown that in a shared-memory parallel environment, the beneﬁt that comes from a
ﬁnely grained algorithm is able to overcome the extra operations that it introduces.

Numerical experiments also showed that our methods can be used in various computational science problems to
produce approximate QR factorization with controllable accuracy. This shows that BLR matrices can provide a good
approximation for the resulting orthogonal and upper triangular factors. Both Householder and MGS-based BLR
methods produced approximate QR factorization of similar residual. However in terms of orthogonality, our method is
robust to ill-conditioning, whereas the existing MGS-based method suﬀers from numerical instability.

Acknowledgements

This work was supported by JSPS KAKENHI Grant Number JP20K20624 and JP21H03447. This work is supported
by ”Joint Usage/Research Center for Interdisciplinary Large-scale Information Infrastructures” and ”High Performance
Computing Infrastructure” in Japan (Project ID: jh210024-NAHI). This work is conducted as research activities of
AIST - Tokyo Tech Real World Big-Data Computation Open Innovation Laboratory (RWBC-OIL).

References

[1] Kadir Akbudak, Hatem Ltaief, Aleksandr Mikhalev, Ali Charara, Aniello Esposito, and David Keyes. Exploiting
data sparsity for large-scale matrix computations. In Marco Aldinucci, Luca Padovani, and Massimo Torquati,
editors, Euro-Par 2018: Parallel Processing, pages 721–734, Cham, 2018. Springer International Publishing.

[2] Kadir Akbudak, Hatem Ltaief, Aleksandr Mikhalev, and David Keyes. Tile low rank cholesky factorization for
climate/weather modeling applications on manycore architectures. In Julian M. Kunkel, Rio Yokota, Pavan
Balaji, and David Keyes, editors, High Performance Computing, pages 22–40, Cham, 2017. Springer International
Publishing.

21

20212223242526Number of threads102Factorization time (s)20212223242526Number of threads20212223242526SpeedupFork-join Blocked HouseholderFork-join Tiled HouseholderTask-based Tiled Householder[3] Patrick. Amestoy, Cleve. Ashcraft, Olivier. Boiteau, Alfredo. Buttari, Jean-Yves. L’Excellent, and Cl´ement.
Weisbecker. Improving multifrontal methods by means of block low-rank representations. SIAM Journal on
Scientiﬁc Computing, 37(3):A1451–A1474, 2015.

[4] Patrick Amestoy, Alfredo Buttari, Jean-Yves L’Excellent, and Theo Mary. On the complexity of the block

low-rank multifrontal factorization. SIAM Journal on Scientiﬁc Computing, 39(4):A1710–A1740, 2017.

[5] Patrick R. Amestoy, Alfredo Buttari, Jean-Yves L’Excellent, and Theo Mary. Performance and scalability of the
block low-rank multifrontal factorization on multicore architectures. ACM Trans. Math. Softw., 45(1), February
2019.

[6] AmirHossein Aminfar, Sivaram Ambikasaran, and Eric Darve. A fast block low-rank dense solver with applications

to ﬁnite-element matrices. Journal of Computational Physics, 304:170–188, Jan 2016.

[7] Mario Bebendorf. Eﬃcient inversion of the galerkin matrix of general second-order elliptic operators with

nonsmooth coeﬃcients. Mathematics of Computation, 74(251):1179–1199, 2005.

[8] Mario Bebendorf. Hierarchical Matrices: A Means to Eﬃciently Solve Elliptic Boundary Value Problems. Lecture

Notes in Computational Science and Engineering. Springer Berlin Heidelberg, 2008.

[9] Mario Bebendorf and Wolfgang Hackbusch. Existence of h-matrix approximants to the inverse fe-matrix of elliptic

operators with L∞-coeﬃcients. Numer. Math., 95(1):1–28, jul 2003.

[10] Peter Benner and Thomas Mach. On the QR decomposition of H-matrices. Computing, 88(3):111–129, 2010.

[11] Alfredo Buttari, Julien Langou, Jakub Kurzak, and Jack Dongarra. A class of parallel tiled linear algebra

algorithms for multicore architectures. Parallel Comput., 35(1):38–53, jan 2009.

[12] Steﬀen B¨orm, Nadine Albrecht, Christina B¨orst, Sven Christophersen, Jonas Lorenzen, Dirk Boysen, Knut

Reimer, and Jessica G¨ordes. H2lib 3.0, 2016. Retrieved from http://www.h2lib.org.

[13] Steﬀen B¨orm, Lars Grasedyck, and Wolfgang Hackbusch. Introduction to hierarchical matrices with applications.

Engineering Analysis with Boundary Elements, 27(5):405–422, 2003. Large scale problems using BEM.

[14] Qinglei Cao, Yu Pei, Kadir Akbudak, Aleksandr Mikhalev, George Bosilca, Hatem Ltaief, David Keyes, and Jack
Dongarra. Extreme-scale task-based cholesky factorization toward climate and weather prediction applications.
In Proceedings of the Platform for Advanced Scientiﬁc Computing Conference, PASC ’20, New York, NY, USA,
2020. Association for Computing Machinery.

[15] Ali Charara, David Keyes, and Hatem Ltaief. Tile low-rank gemm using batched operations on gpus. In Marco
Aldinucci, Luca Padovani, and Massimo Torquati, editors, Euro-Par 2018: Parallel Processing, pages 811–825,
Cham, 2018. Springer International Publishing.

[16] Melvin E. Conway. A multiprocessor system design. In Proceedings of the November 12-14, 1963, Fall Joint
Computer Conference, AFIPS ’63 (Fall), page 139–146, New York, NY, USA, 1963. Association for Computing
Machinery.

[17] Abeynaya Gnanasekaran and Eric Darve. A fast sparse qr factorization for solving linear least squares problems
in graphics. In ACM SIGGRAPH 2021 Talks, SIGGRAPH ’21, New York, NY, USA, 2021. Association for
Computing Machinery.

[18] Abeynaya Gnanasekaran and Eric Darve. Hierarchical orthogonal factorization: Sparse least squares problems. J.

Sci. Comput., 91(2), may 2022.

[19] Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.). Johns Hopkins University Press,

Baltimore, MD, USA, 1996.

[20] Brian C. Gunter and Robert A. Van De Geijn. Parallel out-of-core computation and updating of the qr

factorization. ACM Trans. Math. Softw., 31(1):60–78, March 2005.

[21] Wolfgang Hackbusch. A sparse matrix arithmetic based on h-matrices. part i: Introduction to h-matrices.

Computing, 62:89–108, 04 1999.

[22] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for

constructing approximate matrix decompositions. SIAM Rev., 53(2):217–288, may 2011.

[23] Akihiro Ida, Takeshi Iwashita, Makiko Ohtani, and Kazuro Hirahara. Improvement of hierarchical matrices with
adaptive cross approximation for large-scale simulation. Journal of Information Processing, 23(3):366–372, 2015.

[24] Akihiro Ida, Hiroshi Nakashima, Tasuku Hiraishi, Ichitaro Yamazaki, Rio Yokota, and Takeshi Iwashita. Qr
factorization of block low-rank matrices with weak admissibility condition. Journal of Information Processing,
27:831–839, 2019.

22

[25] Akihiro Ida, Hiroshi Nakashima, and Masatoshi Kawai. Parallel hierarchical matrices with block low-rank
representation on distributed memory computer systems. In Proceedings of the International Conference on High
Performance Computing in Asia-Paciﬁc Region, HPC Asia 2018, pages 232–240, New York, NY, USA, 2018.
ACM.

[26] W. Jalby and B. Philippe. Stability analysis and improvement of the block gram-schmidt algorithm. SIAM J.

Sci. Stat. Comput., 12(5):1058–1073, September 1991.

[27] Claude-Pierre Jeannerod, Th´eo Mary, Cl´ement Pernet, and Daniel S Roche. Improving the Complexity of Block
Low-Rank Factorizations with Fast Matrix Arithmetic. SIAM Journal on Matrix Analysis and Applications,
40(4):1478–1496, November 2019.

[28] Extreme Computing Research Center King Abdullah University of Science and Technology. Stars-h, 2020.

Retrieved from https://ecrc.github.io/stars-h.

[29] Daniel Kressner and Ana Susnjara. Fast qr decomposition of hodlr matrices, 2018.

[30] Jakub Kurzak and Jack Dongarra. Implementing linear algebra routines on multi-core processors with pipelining
and a look ahead. In Applied Parallel Computing. State of the Art in Scientiﬁc Computing, pages 147–156, Berlin,
Heidelberg, 2007. Springer Berlin Heidelberg.

[31] Yuji Nakatsukasa, Zhaojun Bai, and Francois Gygi. Optimizing halley’s iteration for computing the matrix polar

decomposition. SIAM J. Matrix Analysis Applications, 31:2700–2720, 12 2010.

[32] Yuji Nakatsukasa and Nicholas J. Higham. Stable and eﬃcient spectral divide and conquer algorithms for the
symmetric eigenvalue decomposition and the svd. SIAM Journal on Scientiﬁc Computing, 35(3):A1325–A1349,
2013.

[33] G. Pichon, E. Darve, M. Faverge, P. Ramet, and J. Roman. Sparse supernodal solver using block low-rank
compression. In 2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),
pages 1138–1147, 2017.

[34] Gregorio Quintana-Ort´ı, Enrique S. Quintana-Ort´ı, Robert A. Van De Geijn, Field G. Van Zee, and Ernie Chan.
Programming matrix algorithms-by-blocks for thread-level parallelism. ACM Trans. Math. Softw., 36(3), jul 2009.

[35] Fran¸cois-Henry Rouet, Xiaoye S. Li, Pieter Ghysels, and Artem Napov. A distributed-memory package for dense
hierarchically semi-separable matrix computations using randomization. ACM Trans. Math. Softw., 42(4), jun
2016.

[36] Robert Schreiber and Charles VanLoan. A storage-eﬃcient wy representation for products of householder

transformations. SIAM Journal on Scientiﬁc and Statistical Computing, 10, 02 1989.

[37] M. Sergent, D. Goudin, S. Thibault, and O. Aumage. Controlling the memory subscription of distributed
applications with a task-based runtime system. In 2016 IEEE International Parallel and Distributed Processing
Symposium Workshops (IPDPSW), pages 318–327, 2016.

[38] Daniil V. Shantsev, Piyoosh Jaysaval, S´ebastien de la Kethulle de Ryhove, Patrick R. Amestoy, Alfredo Buttari,
Jean-Yves L’Excellent, and Theo Mary. Large-scale 3-D EM modelling with a Block Low-Rank multifrontal
direct solver. Geophysical Journal International, 209(3):1558–1571, 03 2017.

[39] G. W. Stewart. Matrix Algorithms: Volume 1, Basic Decompositions. Society for Industrial Mathematics, 1998.

[40] L.N. Trefethen and D. Bau. Numerical Linear Algebra. Other Titles in Applied Mathematics. Society for

Industrial and Applied Mathematics, 1997.

[41] Yuanzhe Xi, Jianlin Xia, Stephen Cauley, and Venkataramanan Balakrishnan. Superfast and stable structured
solvers for toeplitz least squares via randomized sampling. SIAM Journal on Matrix Analysis and Applications,
35, 01 2014.

[42] Chenhan D. Yu, Severin Reiz, and George Biros. Distributed o(n) linear solver for dense symmetric hierarchical
In 2019 IEEE 13th International Symposium on Embedded Multicore/Many-core

semi-separable matrices.
Systems-on-Chip (MCSoC), pages 1–8, 2019.

23

