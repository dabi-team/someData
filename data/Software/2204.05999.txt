2
2
0
2

r
p
A
7
1

]
E
S
.
s
c
[

2
v
9
9
9
5
0
.
4
0
2
2
:
v
i
X
r
a

InCoder: A Generative Model
for Code Inﬁlling and Synthesis

∗∗♡†♢

Daniel Fried

Sida Wang

♡

Armen Aghajanyan
Freda Shi

♣

∗♡

△

♣

Jessy Lin
Ruiqi Zhong

♣

Eric Wallace
♡

Wen-tau Yih

Luke Zettlemoyer

♡† Mike Lewis
University of Washington†

♡

♡

Facebook AI Research
♣

UC Berkeley

TTI-Chicago
dfried@andrew.cmu.edu, {armenag,mikelewis}@fb.com

Carnegie Mellon

△

♢

Abstract

Code is seldom written in a single left-to-right pass and is instead repeatedly edited
and reﬁned. We introduce INCODER, a uniﬁed generative model that can perform
program synthesis (via left-to-right generation) as well as editing (via inﬁlling).
InCoder is trained to generate code ﬁles from a large corpus of permissively
licensed code, where regions of code have been randomly masked and moved
to the end of each ﬁle, allowing code inﬁlling with bidirectional context. Our
model is the ﬁrst large generative code model that is able to inﬁll arbitrary regions
of code, which we evaluate in a zero-shot setting on challenging tasks such as
type inference, comment generation, and variable re-naming. We ﬁnd that the
ability to condition on bidirectional context substantially improves performance
on these tasks, while still performing comparably on standard program synthesis
benchmarks in comparison to left-to-right only models pretrained at similar scale.
The INCODER models and code are publicly released.2

1

Introduction

Large language models trained on vast repositories of code have demonstrated remarkable progress in
neural program synthesis and related tasks [18, 10, 68, 46, 21]. However, such models generate code
left-to-right, which makes them less directly applicable to many ubiquitous code editing tasks, such
as ﬁxing bugs, adding comments, or re-naming variables. We introduce INCODER, a uniﬁed model
for program synthesis and editing. Like prior work, INCODER is trained to maximize the likelihood
of a corpus of code. However, we adopt a causal masking objective [2], allowing INCODER to inﬁll
blocks of code conditioned on arbitrary left and right contexts.

More speciﬁcally, we learn to inﬁll by randomly replacing spans of code with a sentinel token and
moving them to the end of the sequence (Figure 1, top). The model is trained to predict all tokens in
the complete sequence in this permuted ordering. During inference, we can edit code by replacing
spans with sentinel tokens, prompting the model with the new sequence, and having it generate new
tokens to replace the masked spans (Figure 1, bottom). Because the model can also trivially generate
without sentinel tokens, the result is a uniﬁed approach for both program synthesis (via left-to-right
generation) and editing (via inﬁlling).

We evaluate performance on a range of zero-shot code inﬁlling tasks, both new and from existing work,
including challenging use cases such as type prediction, variable re-naming, comment generation,

∗

Equal contribution
2https://sites.google.com/view/incoder-code-models

 
 
 
 
 
 
Figure 1: At training time (top), our causal masking objective samples one or more spans of code in
training documents (in the upper left ﬁgure, a single span) and moves these spans to the end of the
document, with their original location denoted by special mask sentinel tokens. An autoregressive
language model is trained to produce these entire masked documents, allowing it to learn to generate
insertion text conditioned on bidirectional context. At inference time (bottom), we can perform a
variety of code editing and inﬁlling tasks in a zero-shot fashion by inserting mask tokens at desired
locations and allowing the model to generate code to insert there. All examples shown are real outputs
from our INCODER-6.7B model, with the regions inserted by the model highlighted in orange.

and completing missing lines of code. Zero-shot inﬁlling with bidirectional context substantially
outperforms approaches based on left-to-right-only models, and on several tasks obtains performance
INCODER also achieves strong
comparable to state-of-the-art models ﬁne-tuned on the tasks.
performance on existing left-to-right program synthesis tasks, where it matches existing code language
models trained at similar scale despite its more general training objective.

To enable further work on code inﬁlling and generation, we publicly release our models at
https://sites.google.com/view/incoder-code-models.

2

Inﬁlling and Synthesis via Causal Masking

Neural models for code generation have either utilized a left-to-right (causal) autoregressive language
modeling objective [16, 18] or, as BERT does, a masked language modeling objective [22, 24]. Both
approaches have strengths and weaknesses. Causal models only condition on context to the left of the
generated tokens, thus preventing inﬁlling, but they can autoregressively generate entire documents.

2

defwithas (filename: ) -> Dict[, ]:
         (filename, )  f:count_wordsstrstrintopen"""Count the number of occurrences of each word in the file."""
'r'        word_counts  {}
         line  f:
             word  line.split():
                 word=forinforinif                         word_counts:
                    word_counts[word]                  :
                    word_counts[word]       word_countsin+=else=return1
1
Type Inferencedefwithas (filename: ) -> Dict[, ]:
         (filename, )  f:count_wordsstrstrintopen"""Count the number of occurrences of each word in the file."""
'r'        word_count  {}
         line  f:
             word  line.split():
                 word=forinforinif                         word_count:
                    word_count[word]                  :
                    word_count[word]       word_countin+=else=return1
1
Variable Name Prediction                         word_counts:
                    word_counts[word]                  :
                    word_counts[word]       word_countsin+=else=return1
1
Docstring Generation        word_counts  {}
         line  f:
             word  line.split():
                 word=forinforinifdefwithas (filename: ) -> Dict[, ]:
         (filename, )  f:count_wordsstrstrintopen"""
    Counts the number of occurrences of each word in the given file.

    :param filename: The name of the file to count.
    :return: A dictionary mapping words to the number of occurrences.
    """
'r'fromimportdef=withasforinreturn collections  Counter

 (file_name):
        words  []
     (file_name)  :
         line  :
            words.append(line.strip())
     Counter(words)word_count"""Count the number of occurrences of each word in the file."""
openfilefileMulti-Region InfillingZero-shot Inferencedefwithas (filename: ) -> Dict[, ]:
         (filename, )  f:count_wordsstrstrintopen"""Count the number of occurrences of each word in the file."""
'r'        word_counts  {}
         line  f:
             word  line.split():
                 word =forinforinif<EOM>           word_counts:
                    word_counts[word]                  :
                    word_counts[word]       word_counts     <MASK:0>in+=else=return1
1
Masked Documentdefwithas (filename: ) -> Dict[, ]:
         (filename, )  f:count_wordsstrstrintopen"""Count the number of occurrences of each word in the file."""
'r'        word_counts  {}
         line  f:
             word  line.split():
                 word=forinforinif                         word_counts:
                    word_counts[word]                  :
                    word_counts[word]       word_countsin+=else=return1
1
Original DocumentTraining<MASK:0><MASK:0>On the other hand, masked language models can condition on both the left and right contexts to inﬁll
a masked region, however, their training objective is typically limited to generating only about 15%
of a document. In this paper, we adopt the recently proposed causal masking objective [2], which
aims to combine the strengths of both causal and masked language models.

2.1 Training

At training time, the causal masking procedure samples a number of spans of contiguous tokens
in each document to mask (Figure 1, top left). We sample the number of spans from a Poisson
distribution with a mean of one, truncated to the support [1, 256], so that there are typically a small
number of spans (with a single span around 50% of the time), but the distribution has a long tail (up
to 256 spans). The length of each span is sampled uniformly from the length of the document and the
set of sampled spans is rejected and resampled if any spans overlap.

Once spans are sampled, each span k is replaced with a special mask sentinel token, <Mask:k>. The
sequence of tokens in the span is then moved to the end of the document (Figure 1, top right), with
the mask sentinel token prepended and a special end-of-mask token <EOM> token appended. In other
words, when a mask token appears for the ﬁrst time in the left-to-right ordering, it marks the location
the span was removed from; when it appears for the second time, it marks the start of the moved
span text. More formally, assume we have a document D with N tokens, and we have sampled one
span Span = Di∶j. Let Left be the left context D0∶i and Right be the right context Dj∶N . Then, we
maximize the log probability of the masked document:

log P ([Left; <Mask:0>; Right; <Mask:0>; Span; <EOM>])
(1)
where ; denotes sequence concatenation. If more than one span were sampled, each would be
similarly appended at the end of the document in order. As in standard left-to-right generative
language modeling, we compute the probability of the sequence auto-regressively and train the model
using cross-entropy loss on all tokens except the mask sentinel tokens <Mask:k>, so that the model
does not generate these tokens during inference.

2.2

Inference

During inference, the model can either be used for left-to-right generation in the standard way (by
sampling autoregressively from the model, without using any special tokens), or it can insert code at
arbitrary locations in an existing document by inserting a <Mask:k> tokens at the desired location(s)
and continuing generation at the end of the document. Assuming for simplicity of notation that we
want to insert text at only a single location, we generate a span to insert between the location’s Left
and Right context sequences by sampling tokens autoregressively from the distribution

P (⋅ ∣ [Left; <Mask:0>; Right; <Mask:0>])

(2)

until either an <EOM> token is generated or a task-dependent stopping criterion is achieved.3

More generally, when inserting at multiple locations, we condition on the document with
multiple mask sentinel tokens inserted and a ﬁnal mask token appended. For example, to
insert at two locations we use [A; <Mask:0>; C; <Mask:1>; E; <Mask:2>]) and inﬁll the masks
in order, appending the appropriate <Mask:k> sentinel tokens to signal the start of generation
for the next span, i.e., the completed document for two insertion locations is represented by
[A; <Mask:0>; C; <Mask:1>; E; <Mask:2>; <Mask:0>; B; <EOM>; <Mask:1>; D; <EOM>],
where regions B and D have been inﬁlled.

When applied to code, this allows us to perform tasks that beneﬁt from the bidirectional context in
a zero-shot fashion, as shown in Figure 1, bottom. For example, we can perform Python docstring
generation conditioned on both the left context (function signature) and right context (function
implementation). We can also inﬁll multiple interdependent regions, e.g., generate import statements
required by a function that the model is generating.

3In practice, we sample from the distribution P (⋅ ∣ [Left; <Mask:0>; Right; <Mask:1>; <Mask:0>]),
where we insert an artiﬁcial <Mask:1> token. Not inserting <Mask:1> gives an implicit size hint to the model
that the <Mask:0> token should be expanded to ﬁll the rest of the 2048 token context window. Instead, inserting
a <Mask:1> token indicates to the model that some amount of the document is omitted after the right context.
We found that including this substantially improved the ability of the model to predict <EOM> appropriately when
generating an inﬁll for <Mask:0>. See [2] for more.

3

3 Training Data

We train our models on a corpus of (1) public code with permissive, non-copyleft, open-source
licenses and (2) StackOverﬂow questions, answers, and comments. Our primary focus in this paper
is on the Python language, but we also include code ﬁles from 28 total languages and all available
StackOverﬂow content. After the ﬁltering and deduplication steps outlined below, our corpus contains
a total of 159 GB of code, 52 GB of it in Python, and a total of 57 GB of content from StackOverﬂow.
See Figure 5 in the Appendix for the total data size by language.

3.1 Code

Sources. We obtained code ﬁles and repository metadata from GitHub and GitLab through the sites’
public APIs over a period ending on December 9th, 2021. We obtained approximately 670,000 public
non-fork repositories which GitHub/GitLab detected as containing primarily Python, JavaScript,
or Jupyter Notebook ﬁles, and with either an MIT, Apache 2.0, BSD-2, or BSD-3 clause license.
We included all code from a list of 28 languages (determined by ﬁle extension) contained in these
repositories.4 Since Python ﬁles can also be contained in non-majority-Python repositories, we also
included all other Python and Jupyter ﬁles obtainable through the GitHub archive on BigQuery that
we did not already obtain from GitHub directly.5 We preprocess Jupyter notebooks by including
all text and code (with Markdown formatting removed from text cells), with cells demarcated by
XML-style tags (see Section 3.3).

Deduplication. Recent work has shown that deduplicating training data can improve model per-
formance and reduce the risk of memorizing training data [5, 43, 38]. Our deduplication scheme
removes code ﬁles using exact match on the sequence of alphanumeric tokens in the ﬁle.6 This
removed approximately 75% of the corpus by ﬁle size (reducing from 1 TB to 250 GB) as there
are numerous duplicated repositories, library dependencies included as source ﬁles, and common
boilerplate code ﬁles (e.g., for Python web frameworks). We also use regular expressions to detect
email addresses in the code ﬁles and replace them with a dummy address,7 to reduce the risks of the
model memorizing real email addresses or hallucinating fake ones.

Decontamination. To ensure that our code generation models can be evaluated on several current
code generation benchmarks, we perform data decontamination: removing overlap between our
training data and the evaluation sets of these benchmarks. We remove any repositories contained in
the validation and test sets of CodeSearchNet [34], as these are used to construct validation and test
sets for several of the tasks in CodeXGLUE [45].8

Filtering. Our ﬁltering is similar to past work on generative models of code [18, 46, 68]: we
remove ﬁles that contain any line longer than 3000 tokens or an average line length greater than 100
tokens, have less than 40% of their characters being alphanumeric or underscores, or appear to be
automatically generated, which we determine using substring match on a small number of phrases

4We include source ﬁles from C, C++, CSS, C#, Common Lisp, Dart, Forth, Go, HTML, Haskell, Java,
JavaScript, Julia, Jupyter, Kotlin, Lua, Matlab, PHP, Perl, Python, R, Ruby, Rust, SQL, Scala, Shell, Swift, and
TypeScript, although the great majority of ﬁles are Python and JavaScript. See Figure 5.

5We use https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-anal

yze-all-the-open-source-code. We only include repositories with one of the above permissive licenses.

6We implement match checking using a Bloom ﬁlter [15] whose keys are: the ﬁle extension, number of
tokens in the ﬁle, and an MD5 hash [53] of the sequence of tokens, which is highly accurate at identifying ﬁles
with exactly matching token sequences.

7We replace detected email addresses with removed@example.com.
8We also search for occurrences of code from the HumanEval [18] dataset using substring match, but we
did not ﬁnd any matches in our training set. The solutions to the problems in this dataset, at the time that we
obtained our ﬁles from GitHub, were only contained in JSON ﬁles. We additionally removed repositories in the
evaluation sets of the JuICe tasks [1], although we did not evaluate our model on these tasks in this present work.

4

produced by automatic code and documentation generation systems.9 Our decontamination and
ﬁltering steps together remove roughly 10% of Python ﬁles.

3.2 StackOverﬂow

The second component of our corpus consists of questions, answers, and comments from StackOver-
ﬂow. The Pile [26], which was used to train recent generative code models that we compare to in
Section 6, also contains these questions and answers but does not contain comments. We include
all questions that have at least one answer, up to ten answers with a non-negative score (sorted
by score) per question, and up to ﬁve comments per question/answer. Qualitatively, we ﬁnd that
comments, together with the inﬁlling ability of the model, allow our model to have some capability
to do interactive code editing guided by language (see Figure 11 in the Appendix).

3.3 Metadata

We include some metadata on the code ﬁles and StackOverﬂow questions/answers directly in our
training data to allow attribute-conditioned generation [40, 75] and attribute prediction. For code ﬁle
data, our attributes are the code ﬁlename, the ﬁle extension (as a proxy for language), the ﬁle source
(GitHub or GitLab), and, for GitHub repositories, the number of stars binned into six buckets.10 To
allow this metadata to be optional when performing left-to-right prompting of the model, we insert
each attribute it the beginning of its document with a probability of 50% (allowing the model to
learn metadata conditioning); otherwise, we insert it at the end of its document (allowing metadata
prediction). See Figure 6a and Figure 6b in the appendix for examples. For StackOverﬂow, our
metadata attributes are the question tags for the topic (e.g., python,django) and the number of votes
for each question and answer, binned in the same way as repository stars. We insert comments directly
after the questions or answers they were written for. See Figure 6c in the appendix for examples.

3.4 Tokenization

To increase the amount of context that our code model can condition on, the length of documents
that the model can generate, and the efﬁciency of training and inference, we train a byte-level BPE
tokenizer [56, 51]. We allow tokens to extend across whitespace (excluding newline characters)
so that common code idioms (e.g., import numpy as np) are represented as single tokens in the
vocabulary. This substantially improves the tokenizer’s efﬁciency—reducing the total number of
tokens required to encode our training corpus by 45% relative to the byte-level BPE tokenizer and
vocabulary of GPT-2.

4 Training

Our primary model is INCODER-6.7B, a 6.7B Transformer [62] language model. We use the same
architecture as the dense 6.7B models described in [8]; the Fairseq architecture description can
be found in Table 8 in the appendix.
INCODER-6.7B was trained on 248 V100 GPUs for 24
days. We perform one epoch on the training data, using each training document exactly once. Our
implementation utilized the causal masking implementation [2] available in Fairseq [47], with the
underlying library being PyTorch [49]. Our per-GPU batch size was 8, with a maximum token
sequence length of 2048. We clip all gradient norms to 1.0 and used the Adam optimizer with
β1 = 0.9, β2 = 0.98 [41]. For our learning rate scheduler, we use the built-in polynomial decay
learning rate scheduler available in [49] with 1500 warmup updates. Fairscale was used for improving
memory efﬁciency through fully sharding model states [11].

We compare the validation perplexity of the 6B parameter model and a smaller 1.3B parameter model
(see Section 6.2 for details on the training of this 1.3B model) in Figure 2, showing comparable
scaling laws to those reported by Aghajanyan et al. [2]. Our models have also not yet saturated and

9Our ﬁlters target ﬁles automatically generated by the Protocol Buffer Compiler, Django, Epydoc, Pdoc,

Sphinx, MkDocs, or MathJax.

10We size bins using an inverse binary logarithmic scale, so that bucket 0 corresponds to the repositories with
star counts up to the 50th percentile, bucket 1 corresponds to the 50th to 75th percentiles, and bucket 5 to those
in the 97th percentile and above.

5

Figure 2: Loss curves show that perplexity is still
improving after one epoch and that perplexity
improves substantially with a larger model size.
This suggests that increasing epochs, data size, or
model size would improve performance.

Figure 3: Performance of INCODER-6.7B on
the HumanEval left-to-right synthesis benchmark
generally increases over the course of pre-training.
We plot a line of best ﬁt along with a 95% conﬁ-
dence interval via bootstrap resampling.

would beneﬁt from further training; we report the performance of the 6.7B model on the HumanEval
Python function synthesis benchmark [18] (see Section 6.1 for a description of this benchmark) and
see a consistent increase in performance over the course of training (Figure 3).

5

Inﬁlling Experiments

Our primary evaluation is performing zero-shot inﬁlling for a diverse set of programming tasks:
ﬁlling in multiple lines of code, predicting function return types, generating docstrings, renaming
variables, and inserting missing code tokens in a cloze task. We formulate each task as ﬁlling in one
or more masked-out regions of code.

To evaluate how INCODER beneﬁts from bidirectional context when generating inﬁlls, we compare
three different inference methods: the causal masking inference procedure described in Section 2,
a standard left-to-right generation approach (left-to-right single), and a left-to-right generation and
reranking approach (left-to-right reranking). Since our model is also able to generate left-to-right,
we can compare all three inference methods using the same INCODER-6.7B model and thus avoid
any confounding effects due to a change in the model. For all three inference methods, we obtain
generations from the model using Fairseq’s implementation [47] of top-p (nucleus) sampling [33]
with p = 0.95 and a temperature parameter tuned for each task and inference method using the task’s
development data. For all generation experiments, we preﬁx prompts with meta-data indicating the
code generated should be Python (i.e., <| file ext=.py |>; see Section 3.3).

Left-to-right single. This baseline does not use the context to the right of the masked location at
all. It generates a single completion for the location by conditioning on the left context and sampling
tokens autoregressively from the model P (⋅ ∣ Left) until a task-speciﬁc stop condition is reached
(e.g., for comment generation, a comment-ending delimiter is produced; for multi-line inﬁlling, a
speciﬁed number of lines is generated).

Left-to-right reranking. This baseline uses only the left context to propose candidates to inﬁll
the blank, but uses both the left and right contexts to choose among these candidates. Concretely,
we ﬁrst generate K possible completions for the blank region, Span1 . . . SpanK following the same
procedure as left-to-right single, using K = 10 unless otherwise speciﬁed. We then evaluate each
candidate by substituting it into the blank and scoring the completed document. We use either total
log probability of the completed document log P ([Left; Spank; Right]) or, following [18], log
probability averaged across the number of tokens in the completed document. We select between
these two scoring methods for each task using performance on the task’s development data.

6

0.00.20.40.60.81.0Fraction of Training Data Seen45678Validation PerplexityInCoder-6BInCoder-1B0.20.40.60.81.0Fraction of Training Data Seen81012141618HumanEval Pass@1Method

Pass Rate Exact Match

Method

Pass Rate Exact Match

L-R single
L-R reranking
CM inﬁlling

48.2
54.9
69.0

38.7
44.1
56.3

L-R single
L-R reranking
CM inﬁlling

24.9
28.2
38.6

15.8
17.6
20.6

(a) Single-line inﬁlling.

(b) Multi-line inﬁlling.

Table 1: On our single- and multi-line code inﬁlling benchmarks that we construct from HumanEval,
our causal-masked (CM) approach obtains substantial improvements over left-to-right single candidate
and left-to-right reranking baselines in both function test pass rate and exact match.

5.1

Inﬁlling Lines of Code (HumanEval)

We create an inﬁlling benchmark for complete lines of code from the HumanEval dataset [18]. This
dataset provides comment descriptions of functions paired with a canonical implementation of each
function and several input–output pairs that the function should pass. HumanEval was introduced as
a benchmark for the synthesis of entire Python functions; we will evaluate our models on this original
synthesis setting in Section 6.1. We use this dataset because it affords functional testing of completed
code (as opposed to relying solely on an evaluation of the code surface form), which is particularly
important when inﬁlling longer regions that have more potential ways to be completed correctly. We
construct two different inﬁlling tasks from the HumanEval dataset, for single-lines and multiple-lines:

Single-line inﬁlling.
In this task, we mask out each non-blank line of code in the canonical function
implementation in turn (creating N examples for a function with N non-blank lines). The task is to
generate a single-line completion for the blank conditioned on the natural language description of the
function and the code lines before and after the blank. We evaluate using (1) pass rate: the rate at
which the completed function passes all of the function’s input–output pairs (i.e., analogous to the
pass@1 metric from Chen et al. [18]) and (2) exact match: percentage of times that the completed
lines exactly match the masked lines in the canonical implementation. Performance is averaged
across all examples generated for all programs in the dataset.11

Multi-line inﬁlling. This task is constructed in the same way as single-line inﬁlling above but
allows each masked region to contain multiple lines of code, creating N × (N + 1)/2 examples for a
function with N non-blank lines. We again evaluate completions using pass rate and exact match,
averaged across all inﬁlling examples.

Inference details. To choose when to end the inﬁll produced by our inference methods, we truncate
the candidates generated by the left-to-right (L-R) baselines to the actual number of lines in the
blanked-out region. For our causal-masked (CM) inﬁlling method, we end the inﬁll when the model
generates the <EOM> token. For the L-R single and CM inﬁlling methods, we use a temperature of 0.2.
For the L-R rerank method, we use a temperature of 0.8 to sample K = 10 candidates and rescore
with the total log probability of the completed function.

Results. Table 1 shows the results for the single-line (left) and multi-line settings (right). In both
settings, CM inﬁlling improves substantially over the L-R single baseline and the L-R reranking
baseline. Note that these results are computed by averaging over all examples, which includes masked
regions at all positions in functions (including the beginning, when no left context is available, and
end, when no right context is available). Figure 4 shows a ﬁner-grained comparison, where we group
examples by the fraction of lines in the canonical function which are contained in the context to the

11We also report the performance of OpenAI’s recently-released code-davinci-002 code generation system
(https://openai.com/blog/gpt-3-edit-insert/), which supports inﬁlling, in Table 10, although since no
information about this system’s training procedure or data is currently public, this only serves to help gauge the
difﬁculty of our new task.

7

Figure 4: Inﬁlling pass rate by the fraction of the function’s lines which are provided to the right of
the region that must be inﬁlled, for single-line inﬁlling (left) and multi-line inﬁlling (right). Shaded
regions give 95% conﬁdence intervals, estimated using bootstrap resampling. Our causal-masked
(CM) inﬁlling method, blue, consistently outperforms both of the left-to-right (L-R) baselines, with
larger gains as more right-sided context becomes available (the right side of both graphs).

right of the inﬁll. The CM inﬁlling method sees larger improvements over the L-R baselines as more
right-sided context becomes available (i.e., when the blanked region occurs earlier in the function).12

5.2 Docstring generation (CodeXGLUE)

We next evaluate documentation string (docstring) generation, where models must generate a natural
language docstring that summarizes a Python code snippet. Right context may be particularly useful
for docstring generation, as conditioning on the function body can allow models to generate more
informative descriptions. Prior neural code generation models are ﬁne-tuned on supervised docstring-
code pairs to perform this task (e.g., [18, 45, 4]), we evaluate our model zero-shot, with no explicit
documentation string supervision.

We use the CodeXGLUE code-to-text docstring generation task [45], which is constructed from
CodeSearchNet [34], consisting of docstring-code pairs scraped from publicly available GitHub
repositories. The L-R single candidate baseline is prompted with the function signature in the left
context preceding the docstring. The CM inﬁlling and L-R reranking methods also observe the right
context, consisting of the function body.

We compare models following the original automatic evaluation setup for the task. In Table 2, we
report smoothed 4-gram BLEU scores for all models, using the reference docstrings provided in

Method

Ours: L-R single
Ours: L-R reranking
Ours: Causal-masked inﬁlling

RoBERTa (Finetuned)
CodeBERT (Finetuned)
PLBART (Finetuned)
CodeT5 (Finetuned)

BLEU

16.05
17.14
18.27

18.14
19.06
19.30
20.36

Table 2: CodeXGLUE Python Docstring generation BLEU scores. Our model is evaluated in a
zero-shot setting, with no ﬁne-tuning for docstring generation, but it approaches the performance of
pretrained code models that are ﬁne-tuned on the task’s 250K examples (bottom block).

12For multi-line inﬁlling, performance increases with increasing amounts of right context, as having a large
right context implies that fewer lines are available to be removed when constructing inﬁll examples, so the
resulting examples are often easier to inﬁll.

8

0.00.20.40.60.8Fraction of Lines in Right Context0.10.20.30.40.50.60.70.8Pass RateSingle-Line InfillingCM InfillingL-R SingleL-R Reranking0.00.20.40.60.8Fraction of Lines in Right Context0.10.20.30.40.50.60.7Pass RateMulti-Line InfillingCM InfillingL-R SingleL-R RerankingMethod

Python

JavaScript Ruby Go

Left-to-right single
Left-to-right reranking
CM inﬁll-token
CM inﬁll-region

CodeBERT

76.9
87.9
81.8
86.2

82.2

77.6
90.1
73.9
91.2

86.4

65.8
76.3
81.6
78.9

86.8

70.4
92.8
95.4
94.7

90.8

Java

74.1
91.7
77.6
89.8

90.5

PHP

77.1
90.4
87.0
91.4

88.2

Table 3: Accuracy on the CodeXGLUE max/min cloze task. We compare four different inference
methods. Left-to-right single: scoring with left-to-right ordering using only the left context and the
completion (containing max or min); Left-to-right reranking: scoring with left-to-right ordering using
the left context, completion, and right context; CM inﬁll-token: causal masking scoring, using only
a single token (containing max or min) as the inﬁll, CM inﬁll-region: causal masking scoring that
additionally contains 10 tokens from the right side context.

the dataset. These references have been preprocessed to strip extraneous content (e.g., argument
deﬁnitions) from the original scraped docstrings. We use greedy generation for the CM inﬁlling and
L-R single candidate generation methods and sample K = 10 candidates at temperature 0.8 with
average log probability scoring for the L-R reranking method (selected by tuning on the validation
set of the task). For all inference methods, we terminate generation if the model generates a newline.
We also include the performance of the supervised baseline as reported in the CodeXGLUE paper.
This baseline is an encoder-decoder model with a CodeBERT encoder ﬁne-tuned on 250k training
examples from the dataset. Our zero-shot performance approaches the performance of the ﬁne-tuned
CodeBERT model.

5.3 Code Cloze (CodeXGLUE)

CodeXGLUE cloze is created from CodeSearchNet to evaluate CodeBERT and consists of a short
natural language description followed by code in several programming languages. We evaluate on
the max/min subtask, where the model has to decide if the given mask should be ﬁlled with either
max or min. Since there are only two options in this task, we can closely compare the causal-masked
inﬁlling and left-to-right setups by scoring both options and selecting the sequence with the highest
likelihood.

Table 3 contains the main results. Using the causal-masked inﬁll format with a single token (containing
min/max) as the masked region (CM inﬁll-token) performs better than using just the left context, but
not as well as scoring the entire sequence left to right. Masking a larger region (CM inﬁll-region),
containing the left preﬁx and 10 right-side tokens in the masked region, performs comparably to
scoring the whole sequence. Inﬁll region length and tokenization can affect the performance, see A.2
for more details and more comparisons.

Note that comparing the scores of the sequences, which differ in their inﬁlls, with the left-to-right
setup is more computationally expensive than with the CM inﬁlling setup, as the Transformer
intermediate activations can be cached and shared across identical sequence preﬁxes, and in the CM
inﬁll setup all sequence differences occur at the ends.

5.4 Return Type Prediction

Predicting return type hints for Python functions is a challenging structured generation task (see
Figure 1, “type inference”). We evaluate on two datasets: one we construct from CodeXGLUE and
the dataset from TypeWriter OSS [50].

CodeXGLUE. We develop a benchmark for return type prediction using the same Python
CodeXGLUE dataset used in the code-to-text (docstring generation) task. We run an abstract
syntax tree (AST) processor and transformer on all functions in the development and test sets of this
dataset to (1) identify functions with a PEP 48413 return type hint annotation that is not None and (2)

13https://peps.python.org/pep-0484/

9

Method

Accuracy

Left-to-right single
Left-to-right reranking
Causal-masked inﬁlling

12.0
12.4
58.1

(a) Results on the test set of the
benchmark that we construct from
CodeXGLUE.

Method

Precision Recall

F1

Ours: Left-to-right single
Ours: Left-to-right reranking
Ours: Causal-masked inﬁlling

TypeWriter (Supervised)

30.8
33.3
59.2

54.9

30.8
33.3
59.2

43.2

30.8
33.3
59.2

48.3

(b) Results on a subset of the TypeWriter’s OSS dataset [50].
We include examples from which we were able to obtain
source ﬁles, successfully extract functions and types, that
have non-None return type hints, and that were not included
in our model’s training data.

Table 4: Results for predicting Python function return type hints on two datasets. We see substantial
improvements from causal masked inﬁlling over baseline methods using left-to-right inference.

remove all other type hints (e.g., for function arguments and variable declarations) from the function.
This leaves 232 functions in the development and 469 functions in the test set.

The task is to condition on the function signature and body and predict the type hint. We compare the
type hints predicted by our various methods to the annotated type hint in the original function, using
exact match accuracy on the normalized type hint14
To compare our three generation methods, we stop generation when a : is generated, which ends
the type hint and signals the start of the function body. We tune inference hyperparameters on
the development set, and we use a temperature of 0.2 for left-to-right-single, 0.8 for left-to-right
reranking, and greedy generation for causal masked inﬁlling. Results on the test set are given
in Table 4a. Conditioning on the right context (i.e., the function body) gives some beneﬁt in the
left-to-right reranking setting, but gives a substantial improvement via our causal masked inﬁlling.

TypeWriter OSS. Some recent work has developed supervised machine learning approaches
for predicting type annotations for dynamically-typed languages including Python [69, 6, 50] and
TypeScript [31, 66, 36]. We compare our zero-shot model to one such approach for Python, Type-
Writer [50], which combines a neural architecture for type hint prediction with a search-based
incremental type validation procedure.

To compare to the supervised TypeWriter approach, we obtain its predictions on the open-source
software (OSS) dataset used in that work [50], consisting of Python functions from GitHub. Unfortu-
nately, we could not evaluate on their full evaluation set since much of it was included in our model’s
training data. We ﬁlter to instances that were not included in our training data, for which we were
able to obtain ﬁles and extract functions and types from via AST parsing, and which have non-None
return type hints. This leaves 2,092 examples (about 12% of their evaluation set). We otherwise
emulate their exact setup, which allows our model to condition on ﬁle imports, the function body,
and the function signature to predict return type hints. We use the same inference hyperparameters as
we did for CodeXGLUE type hint prediction.

We present our results in two tables: Table 4b which represents metrics across non-None types,
and Table 11 in the Appendix, which includes None types as well (following [50]).15 We again see
substantial beneﬁts from causal masked inﬁlling’s ability to condition on the function body when
generating return types, and we ﬁnd that our zero-shot model outperforms the supervised TypeWriter
model on this dataset.

14We normalize each type hint by ﬁrst parsing the type to an AST and then un-parsing the AST to a surface
form string, then compute exact match on these surface forms. We note that this metric is somewhat noisy, given
that human-annotated type hints can be inaccurate, and that exact match does not reason about type uniﬁcation or
equivalence (e.g., there is no partial credit given for predicting Optional[str] rather than Union[None, str]).
15Our model makes a prediction for every example, so it has identical precision, recall, and F1 scores.

10

Method

Accuracy

Left-to-right single
Left-to-right reranking
Causal-masked inﬁlling

18.4
23.5
30.6

Table 5: Results on the variable renaming benchmark that we construct from CodeXGLUE. Our
model beneﬁts from using the right-sided context in selecting (L-R reranking and CM inﬁlling) and
proposing (CM inﬁlling) variable names.

5.5 Variable Name Prediction

Variable name prediction is a less-constrained code generation task that requires modeling bidirec-
tional context. We again use the test set from the CodexGlue code-to-text task (docstring generation)
and run an AST transform to isolate and either mask all the occurrences of the variable name (inﬁlling)
or take the left-most context from the ﬁrst variable name (left-to-right mode). In the inﬁlling setting,
given that we generate the number of masks equivalent to the number of times a variable is seen, we
select the most common prediction as our singular prediction. Furthermore, we only evaluate the set
of variable names containing four or more characters. For our re-ranking, we consider a candidate
set of 25 variables. We present our results in Table 5. We again see substantial beneﬁts from using
both left and right context: left-to-right reranking and causal-masked inﬁlling both outperform the
left-to-right single baseline (which uses only the left context). Causal-masked inﬁlling substantially
on the left-to-right reranking method, demonstrating the value of conditioning on the right context
when proposing candidate completions.

6 Model Comparisons

In this section, we compare our model to other generative code models. All published generative
models of code, to our knowledge, have been left-to-right only.16 We compare existing models on
two left-to-right Python program synthesis benchmarks in Section 6.1, ﬁnding that our model obtains
similar performance to models of similar size trained on similar amounts of data. We also perform
model ablations and variations of our loss, training data, and model size in Section 6.2. For example
model outputs (both for inﬁlling and left-to-right generation), see Section A.4.

6.1 Comparison to Left-to-Right Generative Models on Code Synthesis

We compare to past published work on generative code models on the HumanEval [18] and MBPP [10]
benchmarks, which require models to condition on natural language descriptions (docstrings) to
produce Python programs (typically a single function), and evaluates overall functional accuracy
(pass rate) across examples using several test cases for each program.

We evaluate our INCODER-6.7B model in zero-shot evaluation on both of these benchmarks. For
HumanEval, we follow past work by prompting with function signatures and docstring descriptions,
sample 200 candidate program completions, and compute pass@1, pass@10, and pass@100 using
the unbiased sampling estimator of Chen et al. [18]. For MBPP, which does not include function
signatures, we prompt only with the docstring description and compute pass@1 [21] using a single
candidate.17 We use top-p sampling with p = 0.95, with a temperature of 0.2 for pass@1 and 0.8 for
pass@10 and pass@100.

We compare our INCODER-6.7B model to models from past work (which have all been left-to-right
only) in Table 6, giving the model size and training data summary statistics as reported (or estimated,

16OpenAI has released API access to a system, code-davinci-002, that can perform single-region inﬁlling,
but no details have yet been made public about this system, its training data, or any postprocessing done on its
outputs. We compare to results obtained from their API on an inﬁlling task in Table 10 in the Appendix.

17While this setting is not directly comparable to the three-shot setting where the models of Austin et al. [10]
and Chowdhery et al. [21] performed best, we found that our model did not beneﬁt from additional examples in
the prompt, which we attribute to much smaller size of our model (6.7B, versus 137B or 540B parameters) and
the sensitivity of in-context learning to model scale.

11

Model

Released
CodeParrot [61]
PolyCoder [68]
GPT-J [63, 18]
INCODER-6.7B
GPT-NeoX [14]
CodeGen-Multi [46]
CodeGen-Mono [46]
CodeGen-Mono [46]

Unreleased
LaMDA [10, 60, 21]
AlphaCode [44]
Codex-12B [18]
PaLM-Coder [21]

Size
(B)

Python
Code (GB)

Other
Code (GB)

Other
(GB)

Code
License

Inﬁll?

HE
@1

HE
@10

HE
@100

MBPP
@1

1.5
2.7
6
6.7
20
6.1
6.1
16.1

137
1.1
12
540

50
16
6
52
6
62
279
279

None
54
180
∼20

None
238
90
107
90
375
375
375

None
660
None
∼200

None
None
730
57
730
1200
1200
1200

—
—
—
Permissive
—
—
—
—

???
None
>570
∼4000

—
—
—
Permissive

(cid:51)

4.0
5.6
11.6
15.2
15.4
18.2
26.1
29.3

8.7
9.8
15.7
27.8
25.6
28.7
42.3
49.9

14.0 —
28.2
17.1
28.8
46.8
36.0 —

17.9
17.7
27.7
47.0
41.2
44.9
65.8
75.0

47.3
45.3
72.3
88.4

—
—
—
19.4
—
—
—
—

14.8
—
—
47.0

Table 6: A comparison of our INCODER-6.7B model to published code generation systems using
pass rates @ K candidates sampled on the HumanEval and MBPP benchmarks. All models are
decoder-only transformer models. A “Permissive” code license indicates models trained on only
open-source repositories with non-copyleft licenses. The GPT-J, GPT-NeoX, and CodeGen models
are pre-trained on The Pile [26], which contains a portion of GitHub code without any license ﬁltering,
including 6 GB of Python. Although the LaMDA model does not train on code repositories, its
training corpus includes ∼18 B tokens of code from web documents. The total ﬁle size of the LaMDA
corpus was not reported, but it contains 2.8 T tokens total. We estimate the corpus size for PaLM
using the reported size of the code data and the token counts per section of the corpus.

in cases when a paper only reports token counts, as tokenizer efﬁciencies vary) in these papers.While
differences in details of the Transformer model architectures, datasets, and training procedures across
papers and experimental setups make a rigorous comparison impossible, we note that our model
achieves roughly comparable performance on the HumanEval metrics to CodeGen-Multi [46], which
is also a ∼6B parameter model trained on roughly the same amount of Python code, as well as
AlphaCode’s 1.1B decoder-only model [44] which also uses a similar amount of Python training data.

6.2 Ablation Experiments

For a more rigorous comparison on the effect of training a model with causal masking, and for an
analysis of the effects of model size and the training data, we train several variations of our model.
We compare model pass@1 scores on the HumanEval [18] and MBPP [10] left-to-right synthesis
benchmarks, with results in Table 7.

Objective. Comparing 1.3B parameter models trained on the same training data with the causal
masked (CM) objective (row 2) and the standard left-to-right language modeling (LM) objective (row
3), we see that the causal-masked model obtains slightly higher performance on the HumanEval and
MBPP tasks in pass@1 score. This provides further evidence that causal masking training does not
hurt the model’s ability to perform standard left-to-right generation.

Model size. With data held constant, increasing model size consistently improves performance
(comparing the 6.7B and 1.3B CM models in rows 1 and 2, as well as the 1.3B and 2.3B LM models
in rows 3 and 6).

Effects of data. We compare models trained on our entire dataset of multiple code languages and
StackOverﬂow (multi lang + SO, described in Section 3.1) to data ablations that train only on Python
code ﬁles and StackOverﬂow (Python + SO) and only Python code ﬁles (Python). We ﬁnd that
training on multiple languages gives a slight reduction in performance on these Python evaluations.
However, comparing rows 4 and 5, we see that including StackOverﬂow data in training substantially
improves performance on both HumanEval and MBPP. This suggests that future work on generative
code models for language-guided synthesis tasks should consider using StackOverﬂow or other
corpora that mix natural language and code as training data.

12

#

1)
2)
3)
4)
5)
6)

Size
(B)

6.7
1.3
1.3
1.3
1.3
2.3

Obj.

Training
Data

Data
Size

Train
Tokens

Train
Compute

HumanEval
Pass@1

MBPP
Pass@1

CM multi lang + SO 204 GB
CM multi lang + SO 204 GB
LM multi lang + SO 204 GB
104 GB
LM
LM
49 GB
LM multi lang + SO 204 GB

Python + SO
Python

52 B
52 B
52 B
25 B
11 B
52 B

3.0 Z
0.6 Z
0.6 Z
0.3 Z
0.1 Z
1.1 Z

15
8
6
9
5
9

19.4
10.9
8.9
9.8
6.1
12.7

Table 7: Ablation results, comparing model performance on the Python portion of a validation set
held out from our training corpora as well as the HumanEval and MBPP benchmarks. We compare
models by size (in billions of parameters), objective (causal masked, CM, versus standard left-to-right
language modeling, LM), training data, and total amount of compute in training (in zettaﬂops, 1×1021
ﬂoating point operations).

7 Qualitative Examples

We show a variety of qualitative examples from our model in Section A.4 in both the inﬁlling
and left-to-right generation modes: docstring generation, metadata conditioning, class attribute
inference from class usage, comment-conditioned code editing, interactive code editing via nautral
language descriptions, StackOverﬂow title and tag generation, and zero-shot bidirectional translation
of technical jargon between Chinese and English.

8 Related Work

Language Models for Code There has been a ﬂurry of recent work on training large-scale neural
language models on source code. Existing models differ in their architectural design and training
objectives, e.g., decoder-only language models [10, 18, 35, 68, 46], encoder-only masked language
models [24, 37], and encoder-decoder models [4, 44, 54, 64]. Decoder-only language models have
recently grown in popularity as they can perform zero-shot program synthesis by generating in a
left-to-right fashion. On the other hand, InCoder is a decoder-only causally-masked language model
that can inﬁll arbitrary spans of text. This allows the model to perform zero-shot program synthesis
and a myriad of other code inﬁlling tasks.

Inﬁlling Models Many real-world applications require inﬁlling sequences using left and right
context, e.g., editing sentences [57], restoring ancient text [9], and ﬁxing bugs in source code.
Unfortunately, standard left-to-right language models cannot directly inﬁll text, and popular masked
language models are mainly trained to inﬁll very short spans [17, 22, 52, 54]. Recent work addresses
this by changing model architectures, inference procedures, and training objectives [2, 59, 67, 3].
Most related to our approach is the work of Donahue et al. [23] and CM3 [2], who train left-to-right
language models to ﬁll in masked token segments of varying lengths; and the work of Alon et
al. [7], who train an inﬁlling-capable, AST-structured generative model of code on a smaller scale.
In addition, concurrent to our work, OpenAI released an API18 that enables code and text inﬁlling,
although their models are not open-sourced and the technical details are not public.

Machine Learning for Code Assistance There is an extensive literature on using machine learning
models to aid human programmers. This includes methods to infer variable types [50, 66], generate
unit tests [25], repair programs [30, 70, 20, 71], and verify program correctness [55]. Our InCoder
model can inﬁll arbitrary spans of code, which allows it to naturally complete many of the above
tasks in one uniﬁed zero-shot approach.

Machine Learning for Program Synthesis Program synthesis approaches directly generate pro-
grams from a speciﬁcation of functionality [29]. Such models work by taking e.g., input-output exam-
ples [12, 28, 19, 13], partial implementations [58], or natural language descriptions [74, 73, 72, 42, 18]
of the desired program as input. Our InCoder model differs from this past work as it can both

18https://openai.com/blog/gpt-3-edit-insert/

13

synthesize and inﬁll arbitrary spans of code, conditioning on natural language as well as partial
implementations.

9 Conclusion

We demonstrated that using a causal masking objective when training a generative model of code
enables strong zero-shot performance on many challenging and practical code inﬁlling and editing
tasks. The model’s additional inﬁlling capability does not appear to harm its ability to do standard left-
to-right generation: ablation and comparison experiments show that our causal-masked models have
comparable performance to similarly-resourced models on standard left-to-right language-to-code
synthesis benchmarks.

Looking forward, we expect our model performance to continue to increase with more parameters,
data, and training steps [39, 32]. Moreover, ﬁne-tuning would allow our models to be better able to
condition on natural language instructions and other indications of human intent [76, 65, 48]. Finally,
our model lays a foundation for future work on supervised inﬁlling & editing via model ﬁne-tuning,
as well as performing iterative decoding, where the model can be used to reﬁne its own output [27].

Acknowledgments

We thank the Code Clippy team for open sourcing their data acquisition and deduplication code,19
which we used as a starting point for our corpus collection. We are also grateful to Srini Iyer,
Marcin Kardas, Stephen Roller, Susan Zhang, Sam Shleifer, Evan Pu, Uri Alon, Frank Xu, and
Noah Goodman for helpful suggestions and discussions, and to Lucile Saulnier, Leandro von Werra,
Nicolas Patry, Suraj Patil, Omar Sanseviero, and others at HuggingFace for help with the model
release.

References

[1] Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. JuICe: A large scale distantly supervised
dataset for open domain context-based code generation. In Proceedings of EMNLP, 2019.

[2] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,
Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A
causal masked multimodal model of the Internet. arXiv preprint arXiv:2201.07520, 2022.

[3] Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and
Luke Zettlemoyer. HTLM: Hyper-text pre-training and prompting of language models. In ICLR,
2022.

[4] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-

training for program understanding and generation. In NAACL, 2021.

[5] Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of

code. In SPLASH, 2019.

[6] Miltiadis Allamanis, Earl T Barr, Soline Ducousso, and Zheng Gao. Typilus: Neural type hints.

In PLDI, 2020.

[7] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In

ICML, pages 245–256. PMLR, 2020.

[8] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Vic-
toria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efﬁcient large scale language
modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.

[9] Yannis M. Assael, Thea Sommerschield, and J. Prag. Restoring ancient text using deep learning:

a case study on Greek epigraphy. In EMNLP, 2019.

19https://github.com/CodedotAl/gpt-code-clippy

14

[10] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program
synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

[11] Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth Goyal, Myle
Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam Sheiffer, Anjali Sridhar, and
Min Xu. FairScale: A general purpose modular PyTorch library for high performance and large
scale training. https://github.com/facebookresearch/fairscale, 2021.

[12] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.

DeepCoder: Learning to write programs. In ICLR, 2017.

[13] Rohan Bavishi, Caroline Lemieux, Roy Fox, Koushik Sen, and Ion Stoica. AutoPandas:

neural-backed generators for program synthesis. PACMPL, 2019.

[14] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,
Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-
NeoX-20B: An open-source autoregressive language model. 2022.

[15] Burton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communications

of the ACM, 1970.

[16] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. In NeurIPS, 2020.

[17] William Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and Jakob Uszkoreit. KERMIT:

Generative insertion-based modeling for sequences. arXiv preprint arXiv:1906.01604, 2019.

[18] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[19] Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and
Denny Zhou. SpreadsheetCoder: Formula prediction from semi-structured context. In ICML,
2021.

[20] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and
Monperrus Martin. SequenceR: Sequence-to-sequence learning for end-to-end program repair.
IEEE Transactions on Software Engineering, 2021.

[21] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of

deep bidirectional transformers for language understanding. In NAACL, 2019.

[23] Chris Donahue, Mina Lee, and Percy Liang. Enabling language models to ﬁll in the blanks. In

ACL, 2020.

[24] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. CodeBERT: A pre-trained model for programming and
natural languages. In EMNLP Findings, 2020.

[25] Gordon Fraser and Andrea Arcuri. EvoSuite: Automatic test suite generation for object-oriented

software. In ACM SIGSOFT, 2011.

[26] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse
text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

[27] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-Predict: Parallel

decoding of conditional masked language models. In EMNLP, 2019.

15

[28] Sumit Gulwani. Automating string processing in spreadsheets using input-output examples.

ACM SIGPLAN Notices, 2011.

[29] Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. Program synthesis. In Foundations

and Trends in Programming Languages, 2017.

[30] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. DeepFix: Fixing common C

language errors by deep learning. In AAAI, 2017.

[31] Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis. Deep learning

type inference. In ESEC/FSE 2018, 2018.

[32] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,
Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive
generative modeling. arXiv preprint arXiv:2010.14701, 2020.

[33] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural

text degeneration. In ICLR, 2020.

[34] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
CodeSearchNet challenge: Evaluating the state of semantic code search. arXiv preprint
arXiv:1909.09436, 2019.

[35] Maliheh Izadi, Roberta Gismondi, and Georgios Gousios. CodeFill: Multi-token code comple-

tion by jointly learning from structure and naming sequences. In ICSE, 2022.

[36] Kevin Jesse, Premkumar T. Devanbu, and Touﬁque Ahmed. Learning type annotation: Is big

data enough? In ESEC/FSE, 2021.

[37] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating

contextual embedding of source code. In ICML, 2020.

[38] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy

risks in language models. arXiv preprint arXiv:2202.06539, 2022.

[39] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020.

[40] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.
CTRL: A conditional transformer language model for controllable generation. arXiv preprint
arXiv:1909.05858, 2019.

[41] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,

2015.

[42] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and

Percy S Liang. SPoC: Search-based pseudocode to code. In NeurIPS, 2019.

[43] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris
Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models
better. In ACL, 2022.

[44] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code
generation with AlphaCode. arXiv preprint arXiv:2203.07814, 2022.

[45] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. CodeXGlue: A machine learning
benchmark dataset for code understanding and generation. In NeurIPS, 2021.

[46] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
and Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint
arXiv:2203.13474, 2022.

16

[47] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. Fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint
arXiv:1904.01038, 2019.

[48] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis
Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with
human feedback. arXiv preprint arXiv:2203.02155, 2022.

[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative
style, high-performance deep learning library. In NeurIPS, 2019.

[50] Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. TypeWriter: Neural type

prediction with search-based validation. In ACM SIGSOFT, 2020.

[51] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language

models are unsupervised multitask learners. Technical report, OpenAI, 2019.

[52] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. In JMLR, 2020.

[53] Ronald Rivest. RFC1321: The MD5 message-digest algorithm, 1992.

[54] Baptiste Roziere, Marie-Anne Lachaux, Marc Szafraniec, and Guillaume Lample. DOBF: A

deobfuscation pre-training objective for programming languages. In NeurIPS, 2021.

[55] Gabriel Ryan, Justin Wong, Jianan Yao, Ronghui Gu, and Suman Sekhar Jana. CLN2INV:

Learning loop invariants with continuous logic networks. In ICLR, 2020.

[56] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words

with subword units. In ACL, Berlin, Germany, 2016.

[57] Yong-Siang Shih, Wei-Cheng Chang, and Yiming Yang. XL-Editor: Post-editing sentences

with XLNet. arXiv preprint arXiv:1910.10479, 2019.

[58] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia, and Vijay Saraswat.

Combinatorial sketching for ﬁnite programs. In ASPLOS, 2006.

[59] Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible

sequence generation via insertion operations. In ICML, 2019.

[60] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for
dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[61] Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Natural Language Processing with

Transformers. O’Reilly Media, Inc., 2022.

[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,

Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.

[63] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language

Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.

[64] Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH Hoi. CodeT5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code understanding and generation. In EMNLP, 2021.

[65] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR,
2022.

17

[66] Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. LambdaNet: Probabilistic type inference

using graph neural networks. In ICLR, 2020.

[67] Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena Hwang, and Yejin Choi.
Reﬂective decoding: Beyond unidirectional generation with off-the-shelf language models. In
ACL, 2021.

[68] Frank F Xu, Uri Alon, Graham Neubig, and Vincent J Hellendoorn. A systematic evaluation of

large language models of code. arXiv preprint arXiv:2202.13169, 2022.

[69] Zhaogui Xu, Xiangyu Zhang, Lin Chen, Kexin Pei, and Baowen Xu. Python probabilistic type

inference with natural language support. In SIGSOFT, 2016.

[70] Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from

diagnostic feedback. In ICML. PMLR, 2020.

[71] Michihiro Yasunaga and Percy Liang. Break-it-ﬁx-it: Unsupervised learning for program repair.

In ICML, 2021.

[72] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning
to mine aligned code and natural language pairs from stack overﬂow. In ACM MSR, 2018.

[73] Tao Yu, Rui Zhang, Kai-Chou Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma,
Irene Z Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider:
A large-scale human-labeled dataset for complex and cross-domain semantic parsing and
text-to-SQL task. In EMNLP, 2018.

[74] John M Zelle and Raymond J Mooney. Learning to parse database queries using inductive logic

programming. In AAAI, 1996.

[75] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner,

and Yejin Choi. Defending against neural fake news. In NeurIPS, 2019.

[76] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot

learning by meta-tuning on dataset and prompt collections. In EMNLP, 2021.

18

A Appendix

A.1 Corpus statistics and model hyperparameters

Figure 5: Code corpus composition (after deduplication and ﬁltering) by total ﬁle size for the most
common languages, as determined by ﬁle extension.

Parameter

INCODER-1.3B

INCODER-6.7B

–decoder-embed-dim
–decoder-output-dim
–decoder-input-dim
–decoder-ffn-embed-dim
–decoder-layers
–decoder-normalize-before
–decoder-attention-heads
–share-decoder-input-output-embed
–decoder-learned-pos

2048
2048
2048
8192
24
True
32
True
False

4096
4096
4096
16384
32
True
32
True
False

Table 8: Fairseq architecture hyperparameters for our INCODER models.

A.2 Cloze and single token inﬁll details

As shown in Table 9, breaking tokenization (-break) on inﬁll decreases the performance using all
scoring methods. For example, whereas Math.max( was a single token in the full sequence, the
sequence is broken into Math., max, and ( for inﬁlling. Inﬁlling with the original tokenization
increases the performance slightly, but does not match full left-right scoring. We suspect this is
because the model was not trained on inﬁlling single tokens, unlike CodeBERT. A way to ﬁx this
is to include a larger region on the left and a few more tokens on the right. This will only slightly
increase the scoring complexity. To show that our model uses the right context, we compare it with
scoring the left-only model. More precisely, the sequences being scored are

Left-to-right single:
Left-to-right reranking:
Inﬁll-token:
Left-region:
Inﬁll-region:

[Left; Token]
[Left; Token; Right]
[Left; <Mask:0>; Right; <Mask:1>; <Mask:0>; Token; <EOSS>]
[Left; Token; Right[:10]]
[ <Mask:0>; Right[10:]; <Mask:1>; <Mask:0>; Left; Token; Right[:10]; <EOSS>]

19

pythonjavascripthtmlcc++javajupyter notebooktypescriptgocssphpplsqlc#rubysqlshellobjective-cperlscalarust01020304050Total File Size (GB)Python

Javascript Ruby Go

Left-break
Left-token
Left-region

Left-right-break
Left-right

Inﬁll-break
Inﬁll-token
Inﬁll-region

CodeBERT
Codex

∗

72.4
76.9
84.2

77.9
87.9

79.1
81.8
86.2

82.2
93.6

72.1
77.6
88.6

79.4
90.1

83.1
73.9
91.2

86.4
93.4

68.4
65.8
73.7

63.2
76.3

84.2
81.6
78.9

86.8
94.7

71.7
70.4
85.5

89.5
92.8

90.1
95.4
94.7

90.8
99.3

Java

74.1
74.1
87.6

82.0
91.7

84.0
77.6
89.8

90.5
95.0

PHP

76.9
77.1
87.0

85.3
90.4

85.3
87.0
91.4

88.2
94.3

Table 9: Accuracy on CodeXGLUE cloze max/min. Left: scoring using only the left context, Left-
right: score the whole program, Inﬁll: score the inﬁlling sequence, -region: include left context
∗
and 10 tokens from the right. -break: break tokenization on the inﬁlled token. Codex
: version
code-davinci-001 of OpenAI’s Codex model, as accessed through their API. Information on the
training data for this model is unclear, and it may contain portions of CodeSearchNet (which contains
this task’s evaluation set).

Model

Inference

Pass Rate Exact Match

INCODER-6.7B
INCODER-6.7B
INCODER-6.7B

Left-to-right single
Left-to-right reranking
Inﬁlling

code-davinci-002 Left-to-right single
code-davinci-002 Left-to-right reranking
code-davinci-002

Inﬁlling

48.2
54.9
69.0

63.7
71.8
87.4

38.7
44.1
56.3

48.4
52.0
69.6

Table 10: We evaluate OpenAI’s proprietary code-davinci-002 system, accessed through their API,
on our single-line HumanEval inﬁlling task. Although no information is currently public about this
system, its training data or procedure, how it performs inﬁlls, or whether any postprocessing is done
on model outputs, we report its performance to help gauge the difﬁculty of our new task. For both
code-davinci-002 and our INCODER-6.7B model, conditioning on right-sided context improves
performance, with the most substantial improvements from inﬁlling.

Method

Ours: Left-to-right single
Ours: Left-to-right rerank
Ours: Inﬁll

Ours: Left-to-right single + Return checks
Ours: Left-to-right rerank + Return checks
Ours: Inﬁll + Return checks

TypeWriter (Supervised)

Precision Recall

F1

20.0
24.2
46.8

63.2
64.3
76.7

20.0
24.2
46.8

63.2
64.3
76.7

69.9

74.1

20.0
24.2
46.8

63.2
64.3
76.7

78.8

Table 11: Return type hint prediction results on the 25% subset of TypeWriter’s OSS dataset where
were able to obtain source ﬁles, extract functions and types from, and that were not contained in
our model’s training set. Given an overrepresentation of functions with None in this dataset, and the
static analysis capabilities of TypeWriter, we also give results using a simple post-processing step
that predicts None if the function does not have any non-trivial return statements.

20

A.3 Metadata Examples

(a) Metadata for code includes the ﬁle exten-
sion, source (github or gitlab), ﬁlename, and
binned number of stars for GitHub repositories (in
logarithmically-sized bins numbered 0 to 5, see
Section 3.3).

(b) In addition to the standard metadata used for
all other code ﬁles, Jupyter Notebook metadata
includes the kernel type (in this instance, Python)
as well as the type of the cells in the notebook
(either code or text).

(c) Metadata attributes for StackOverﬂow include question tags and
discretized scores of questions and answers.

Figure 6: Examples of metadata attributes included in the training data to allow attribute-conditioned
generation and attribute prediction. To allow both generation and prediction, attributes are randomly
included either at the beginning of the document or at the end (with probability 0.5 each). Attributes
occur in random order to allow arbitrary orderings at inference time.

21

<|==|>
fromimportdefwithas=forinforinifin+=else=return<|/==>  ext.py sourcegithub  typing  Dict

 (filename: ) -> Dict[, ]:
         (filename, )  f:
        word_counts  {}
         line  f:
             word  line.split():
                 word  word_counts:
                    word_counts[word]                  :
                    word_counts[word]       word_counts
  filenamestring_utils.py dstarsfilefilecount_wordsstrstrintopen1
1
4"""Count the number of occurrences of each word in the file."""
'r'<|=|>
<>
-</>
<>
%</>
<>
importasimportasfromimport</>
  ext.ipynb:python textThis notebook demonstrates using scikitlearn to perform .
textcellmatplotlib inline
cellcell numpy  np
 matplotlib.pyplot  plt
 sklearn.decomposition  cellfilePCAPCA
...<|=|>
-with<>
fromimport</>
 q tagspython,django Django  How to display a form  user Is what I have so far:
code django.shortcuts  render
codeCan you input

...
help'm trying to create a simple web application that can
allow a user to enter a username and password.
Here'?
<|/ q dscore=3 |>
<| c |>
Why aren't you using the django authentication library?
<|/ c |>
<| a dscore=4 |>
You need to add your form to the template:
<code>
...
</code>
<| a tags=python,django |>A.4 Example Model Outputs

(a) Reference docstring: Returns a snowflake.connection object.
Model docstring: Establishes a connection to the Snowflake cluster.

(b) Reference docstring: Format text with color or other effects into ANSI escaped string.
Model docstring: Prints a string with ANSI color codes.

Figure 7: Example docstring generations for the CodeXGLUE code-to-text dataset. Captions for each
example give the reference human-written docstring and the output from our INCODER-6.7B model
with causal-masked inﬁlling. The model generates docstrings zero-shot by inserting text between the
""" comment delimiters.

Figure 8: Meta-data conditioning on ﬁle extensions for Python (left) and Shell (right) allows com-
pleting the same text comment as either a Python script or a pipelined bash command, respectively.
Regions highlighted in orange are left-to-right generations from our INCODER-6.7B model.

22

def==**return (self):
        conn_config  ._get_conn_params()
    conn  snowflake.connector.connect(conn_config)
     conn
get_conn""" """
selfdef*return       forinifandelse (text, colors):
         .format(.join([(color) \
 color  colors, , context=text])) \
          colors  text
sprint""" """
"[m[m"";"\33{}{content}\33{}strRESETIS_ANSI_TERMINAL<|=|>
importimportdef==forinif=withasforin+=if==  ext.py  os
 sys

 ():
  cwd  os.getcwd()
  
  words    
   filename  os.listdir(cwd):
     filename.endswith():
      fname  os.path.join(cwd, filename)
       (fname)  infile:
         line  infile:
          words  (line.split())
  
  (words)

   :
  main()
file# count the words in all files in the current directory
main0
openlenprint__name__".in"'__main__'<|=|>
----  ext.sh find .  f name   wc w {} \file# count the words in all files in the current directory
typeexec"*.txt"; | sort -nr | head -n 20
Figure 9: Given the beginning of a class deﬁnition and right-sided context of the class being used,
the model is able to infer plausible attribute names for the class (e.g., “Eren” is likely to be a name,
18 is age, “Male” is the gender.) The region highlighted in orange is an inﬁll generation from our
INCODER-6.7B model.

Figure 10: By inserting a comment (# use a Huber loss) in the code and replacing the line after with
a mask token, a user can prompt the model to in-ﬁll a region of code with a contextually-appropriate
replacement. Lines in orange are in-ﬁll generations from our INCODER-6.7B model.

23

classdef==== :
    
     (self,name,age,gender):
        .namename
        .ageage
        .gendergender
        
p  Person(, , )Person__init__selfselfself18'Eren'"Male"data, target  data.to(device), target.to(device)
optimizer.zero_grad()
output  model(data)
loss  F.nll_loss(output, target)
loss.backward()

data, target  data.to(device), target.to(device)
optimizer.zero_grad()
output  model(data)
loss = F.nll_loss(output, target)
loss.backward()=====data, target  data.to(device), target.to(device)
optimizer.zero_grad()
output  model(data)
loss  F.nll_loss(output, target)
loss.backward()

data, target  data.to(device), target.to(device)
optimizer.zero_grad()
output  model(data)
criterion  nn.SmoothL1Loss()
loss  criterion(output, target)
loss.backward()=======# use a Huber loss
Original CodeInsert comment and regenenerateFigure 11: Pretraining on StackOverﬂow, and our model’s inﬁlling capability, allows it to perform
zero-shot interactive reﬁnement of a function. In the ﬁrst example, the model has generated the
orange region conditioned on the user description. In the second example, the user has added a
comment specifying a reﬁnement to the function, and selected the text for the model to replace. In
the third example, the orange region has been inﬁlled by the model.

24

I need to write a Python function called `snake_to_camel`. Here's a description:
"Write a function to convert a snake case string to a camel case string."
<| q tags=python |>
<| a |>
You can use str.title:
<code>
>>> 'my_snake_case'.title()
'MySnakeCase'
</code>
</ a tags=python dscore=3 |>
I need to write a Python function called `snake_to_camel`. Here's a description:
"Write a function to convert a snake case string to a camel case string."
<| q tags=python |>
<| a |>
You can use str.title:
<code>
>>> 'my_snake_case'.title()
'MySnakeCase'
</code>
</ a tags=python dscore=3 |>
<| c |>
Close, but you need to replace the underscores with an empty string.
<|/ c |>
<| c |>
Thanks, edited!
<|/ c |>
I need to write a Python function called `snake_to_camel`. Here's a description:
"Write a function to convert a snake case string to a camel case string."
<| q tags=python |>
<| a |>
<code>
def snake_to_camel(name):
	return ''.join(x.title() for x in name.split('_'))
</code>
</ a tags=python dscore=3 |>
<| c |>
Close, but you need to replace the underscores with an empty string.
<|/ c |>
<| c |>
Thanks, edited!
<|/ c |>
Generate answer conditioned on question2. Add comment3. Regenerate infill conditioned on commentFigure 12: Question tag and title prediction from the text of a StackOverﬂow question. Regions
highlighted in orange are inﬁll generations from our INCODER-6.7B model.

Figure 13: Zero-shot bidirectional technical jargon translation between Chinese and English. Regions
in orange are inﬁll generations from our INCODER-6.7B model.

25

<|=|>
in q tagstensorflow, gpu How to limit  memory usage  TensorFlowGPU?
 
I am interested in limiting the GPU memory usage in TensorFlow.
I work in an environment in which computational resources are shared, i.e., we have a few server machines equipped with a few Nvidia Titan X GPUs each.
For small to moderate size models, the 12 GB of the Titan X is usually enough for 2–3 people to run training concurrently on the same GPU. If the models are small enough that a single model does not take full advantage of all the computational units of the GPU, this can actually result in a speedup compared with running one training process after the other. Even in cases where the concurrent access to the GPU does slow down the individual training time, it is still nice to have the flexibility of having multiple users simultaneously train on the GPU.
The problem with TensorFlow is that, by default, it allocates the full amount of available GPU memory when it is launched. Even for a small two-layer neural network, I see that all 12 GB of the GPU memory is used up.
Is there a way to make TensorFlow only allocate, say, 4 GB of GPU memory, if one knows that this is enough for a given model?
<|/ q dscore=3 |>
冒泡排序法 -> bubble sort
动态规划法 -> dynamic programming
快速排序法 -> quick sort
数据结构 -> data structure
词嵌入 -> word embedding