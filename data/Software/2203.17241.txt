2
2
0
2

r
a

M
9
2

]

C
O
.
h
t
a
m

[

1
v
1
4
2
7
1
.
3
0
2
2
:
v
i
X
r
a

Bayesian optimization with known experimental and design constraints for chemistry
applications

Riley J. Hickman,1, 2, ∗ Matteo Aldeghi,1, 2, 3, 4, ∗ Florian H¨ase,1, 2, 3, 5 and Al´an Aspuru-Guzik1, 2, 3, 6, 7, 8, †
1Chemical Physics Theory Group, Department of Chemistry,
University of Toronto, Toronto, ON M5S 3H6, Canada
2Department of Computer Science, University of Toronto, Toronto, ON M5S 3G4, Canada
3Vector Institute for Artiﬁcial Intelligence, Toronto, ON M5S 1M1, Canada
4Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge, MA 02139, United States
5Department of Chemistry and Chemical Biology,
Harvard University, Cambridge, MA 02138, United States
6Department of Chemical Engineering & Applied Chemistry,
University of Toronto, Toronto, ON M5S 3E5, Canada
7Department of Materials Science & Engineering,
University of Toronto, Toronto, ON M5S 3E4, Canada
8Lebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON M5G 1Z8, Canada
(Dated: April 1, 2022)

Optimization strategies driven by machine learning, such as Bayesian optimization, are being
explored across experimental sciences as an eﬃcient alternative to traditional design of experi-
ment. When combined with automated laboratory hardware and high-performance computing,
these strategies enable next-generation platforms for autonomous experimentation. However, the
practical application of these approaches is hampered by a lack of ﬂexible software and algorithms
tailored to the unique requirements of chemical research. One such aspect is the pervasive presence
of constraints in the experimental conditions when optimizing chemical processes or protocols, and
in the chemical space that is accessible when designing functional molecules or materials. Although
many of these constraints are known a priori, they can be interdependent, non-linear, and result in
non-compact optimization domains. In this work, we extend our experiment planning algorithms
Phoenics and Gryffin such that they can handle arbitrary known constraints via an intuitive and
ﬂexible interface. We benchmark these extended algorithms on continuous and discrete test func-
tions with a diverse set of constraints, demonstrating their ﬂexibility and robustness. In addition,
we illustrate their practical utility in two simulated chemical research scenarios: the optimization
of the synthesis of o-xylenyl Buckminsterfullerene adducts under constrained ﬂow conditions, and
the design of redox active molecules for ﬂow batteries under synthetic accessibility constraints. The
tools developed constitute a simple, yet versatile strategy to enable model-based optimization with
known experimental constraints, contributing to its applicability as a core component of autonomous
platforms for scientiﬁc discovery.

I.

INTRODUCTION

The design of advanced materials and functional
molecules often relies on combinatorial, high-throughput
screening strategies enabled by high-performance com-
puting and automated laboratory equipment. Despite
the successes of high-throughput experimentation in
chemistry,1,2 biology,3,4 and materials science,5 these
approaches typically employ exhaustive searches that
scale exponentially with the size of the search space.
Data-driven strategies that can adaptively search pa-
rameter spaces without the need for exhaustive explo-
ration are thus replacing traditional design of experi-
ment approaches in many instances. These strategies
use machine-learnt surrogate models trained on all data
generated through the experimental campaign, and are
updated each time new data is collected. One such ap-
proach is Bayesian optimization which, based on the sur-

∗ These authors contributed equally
† alan@aspuru.com

rogate model, deﬁnes a utility function that prioritize ex-
periments based on their expected informativeness and
performance.6–8 These data-driven optimization strate-
gies have already demonstrated superior performance in
chemistry and materials science applications, e.g., in re-
action optimization,9,10 the discovery of magnetic reso-
nance imaging agents,11 the fabrication of organic pho-
tovoltaic materials,12,13 virtual screening of ultra-large
chemical libraries,14 and the design of mechanical struc-
tures with additively manufactured components.15

Machine learning-driven experiment planning strate-
gies can also be combined with automated laboratory
hardware or high-performance computing to create self-
driving platforms capable of achieving research goals
autonomously.16–22 Prototypes of these autonomous re-
search platforms have already shown promise in diverse
applications, including the optimization of chemical reac-
tion conditions,9,10 the design of photocatalysts for the
production of hydrogen from water,23 the discovery of
battery electrolytes,24 the design of nanoporous materi-
als with tailored adsorption properties,25 the optimiza-
tion of multicomponent polymer blend formulations for

 
 
 
 
 
 
organic photovoltaics,12 the discovery of phase-change
memory materials for photonic switching devices,26 and
self-optimization of metal nanoparticle synthesis,27 to
name a few.13,28 While self-driving platforms seem poised
to deliver a next-generation approach to scientiﬁc discov-
ery, their practical application is hampered by a lack of
ﬂexible software and algorithms tailored to the unique
requirements of chemical research.

To provide chemistry-tailored data-driven optimiza-
tion tools, our group has developed Phoenics29 and
Gryffin,30 among others31–33. Phoenics is a linear-
scaling Bayesian optimizer for continuous spaces that
uses a kernel regression surrogate model and natively
supports batched optimization. Gryffin is an exten-
sion of this algorithm to categorical, as well as mixed
continuous-categorical spaces. Furthermore, Gryffin is
able to leverage expert knowledge in the form of descrip-
tors to enhance its optimization performance, which was
found particularly useful in combinatorial optimizations
of molecules and materials.30 As Gryffin is the more
general algorithm, and Phoenics is included within its
capabilities, from here on we will refer only to Gryffin.
These algorithms have already found applications rang-
ing from the optimization of reaction conditions10 and
synthetic protocols,27,34 to that of manufacturing of thin
ﬁlm materials13 and organic photovoltaic devices.12 How-
ever, a number of extensions are still required to make
these tools suitable to the broadest range of chemistry
applications. In particular, Gryffin, like the majority
of Bayesian optimization tools available, does not handle
known experimental or design constraints.

There are often many constraints on the experiment
being performed or molecule being designed. A ﬂexi-
ble data-driven optimization tool should be able to ac-
commodate and handle such constraints. The type of
constraints typically encountered may be separated into
those that aﬀect the objectives of the optimization (e.g.,
reaction yield, desired molecular properties), and those
that aﬀect the optimization parameters (e.g., reaction
conditions). Those aﬀecting the objectives usually arise
in multi-objective optimization, where one would like to
optimize a certain property while constraining another to
be above/below a desired value.31,35,36 For instance, we
might want to improve the solubility of a drug candidate,
while keeping its protein-binding aﬃnity in the nanomo-
lar range. Conversely, parameter constraints limit the
range of experiments or molecules we have access to. De-
pending on the source of the constraints, these may be
referred to as known or unknown. Known constraints
are those we are aware of a priori,37–40 while unknown
ones are discovered through experimentation37–40. For
instance, a known constraint might enforce the total vol-
ume for two liquids to not exceed the available volume
in the container in which they are mixed. While this
poses a restriction on the parameter space, we are aware
of it in advance and can easily compute which regions of
parameter space are infeasible. An unknown constraint
may instead be the synthetic accessibility in a molecular

2

optimization campaign. In this case, we might not know
in advance which areas of chemical space are easily acces-
sible, and have to resort to trial and error to identify fea-
sible and infeasible synthetic targets. While constraints
of the objectives were the subject of our previous work31,
and unknown constraints of the parameters are the sub-
ject of on-going work, this paper focuses on data-driven
optimization with known parameter constraints, which
we will refer to simply as known constraints from here
on.

Generally, known constraints arise due to physical or
hardware restrictions, safety concerns, or user preference.
An example of a physically imposed constraint is the fact
that the temperature of a reaction cannot exceed the boil-
ing temperature of its solvent. As such, one may want
temperature to be varied in the interval 10 < T < 100°C
for experiments using water, and in 10 < T < 66°C
for experiments using tetrahydrofuran. The fact that
the sum of volumes of diﬀerent solutions cannot exceed
that of the container they are mixed in is an example of
a hardware-imposed constraint. In synthetic chemistry,
speciﬁc combinations of reagents and conditions might
need to be avoided for safety reasons instead. Finally,
constraints could also be placed by the researchers to re-
ﬂect prior knowledge about the performance of a certain
protocol. For example, a researcher might know in ad-
vance that speciﬁc combinations of solvent, substrate,
and catalyst will return poor yields. These examples
are not natively handled by Gryffin and the majority
of data-driven optimization tools currently available. In
fact, given any number of continuous or categorical pa-
rameters, their full Cartesian product is assumed to be
accessible by the optimization algorithm. Returning to
the example where solvents have diﬀerent boiling temper-
atures, this means that if the optimization range for the
variable T is set to 10 − 100°C, this range will be applied
to all solvents considered. In practice, known constraints
are often interdependent, non-linear, and can result in
non-compact optimization domains.

In this work, we extend the capabilities of Gryf-
fin to optimization over parameter domains with known
constraints. First, we provide a formal introduction to
the known constraint problem and detail how Gryffin
was extended to be able to ﬂexibly handle such scenar-
ios. Then, we benchmark our new constrained version
of Gryffin on a range of analytical functions subject
to a diverse set of constraints. Finally, we demonstrate
our method on two chemistry applications: the optimiza-
tion of the synthesis of o-xylenyl Buckminsterfullerene
adducts under constrained ﬂow conditions, and the de-
sign of redox active molecules for ﬂow batteries under
synthetic accessibility constraints. Across all tests, we
compare Gryffin’s performance to that of other opti-
mization strategies, such as random search and genetic
algorithms, which can also handle complex constraints.

II. METHODS

An optimization task involves the identiﬁcation of pa-
rameters, x, that yield the most desirable outcome for an
objective f (x). In a chemistry context, these parameters
may be experimental conditions or diﬀerent R groups in
a molecule, while the objectives may be the yield of a re-
action or absorbance at a speciﬁc wavelength. Formally,
for a minimization problem, the solution of the optimiza-
tion is the set of parameters that minimizes the objective
f (x),

x∗ = arg min

f (x),

x∈X

where X is the optimization domain, or parameter
space; i.e., the space of all experimental conditions that
could have been explored during the optimization. In a
Bayesian optimization setting, the objective function f is
considered to be unknown, but can be empirically evalu-
ated at speciﬁc values of x. Evaluating f (x) is assumed
to be expensive and/or time consuming, and its measure-
ment subject to noise. We also assume that we have no
access to gradient information about f .

In a constrained optimization problem, f can be evalu-
ated only for a subset of the optimization domain, C ⊂ X .
A constraint function c(x) determines which parameters
x are feasible and thus in C, and which are not. Contrary
to unknown constraints, known constraints are those for
which we have access to c(x) a priori, that is, we are
aware of them before performing the optimization. Im-
portantly, we can evaluate the functions c(x) and f (x)
independently of one another. The solution of this con-
strained optimization problem may be written formally
as

x∗ = arg min

f (x),

x∈X
s.t. c(x) (cid:55)→ feasible.

The physically meaning of c(x) is case dependent and
domain speciﬁc.
In chemistry, known constraints may
reﬂect safety concerns, physical limits imposed by labo-
ratory equipment, or simply researcher preference. The
types of known constraints described above, and the sub-
ject of this work, are hard constraints. They restrict X
irrefutably and no measurement outside of C is permit-
ted. However, the use of soft constraints has also been
explored. While soft constraints bias the optimization
algorithm away from regions that are thought to yield
undesirable outcomes, they ultimately still allow the full
exploration of X . As such, soft constraints have been
used as means to introduce inductive biases into an op-
timization campaign based on prior knowledge.41,42

The goal of this work is to equip Gryffin with the
ability to satisfy arbitrary constraints, as described by
a user-deﬁned constraint function c(x). This can be

3

achieved by constraining the optimization of the acquisi-
tion function α.43 α(x) deﬁnes the utility (or risk) of eval-
uating the parameters x, and the parameters proposed
by the algorithm are those that optimize α(x). Thus,
constrained optimization of the acquisition function also
constrains the overall optimization problem accordingly.
However, contrary to objective function f , the acquisi-
tion function α is easy to optimize, as its analytical form
is known and can be evaluated cheaply.

Illustration of unconstrained (top row) and con-
Figure 1.
strained (bottom row) acquisition function optimization using
strategies based on the Adam optimizer and a genetic algo-
rithm.
Initial, random samples are shown as black crosses.
Grey crosses represent the updated parameter locations for
these initial samples, while white crosses show the ﬁnal lo-
cations after ten optimization iterations. The purple star
indicates the global optima of the unconstrained acquisition
function, which lies in the infeasible region in the constrained
example.

A. Acquisition optimization in Gyrﬃn

Gryffin’s acquisition function, which is to be mini-

mized, is deﬁned as

α(x) =

(cid:80)n

k=1 fkpk(x) + λpuniform(x)
(cid:80)n
k=1 pk(x) + puniform(x)

,

(1)

where pk(x) are the kernels of the kernel regression
model used as the surrogate, fk are the measured objec-
tive function values, and λ is a user-deﬁned parameter
that determines the exploration-exploitation behavior of

the algorithm. The index k refers to each past observa-
tion, for a total number of observations n.

For continuous parameters, Gryffin uses Gaussian
kernels with prior precision τ sampled from a Gamma
distribution, τ ∼ Γ(a, b), with prior hyperparameters
a = 12n2 and b = 1. For categorical and discrete param-
eters, it uses Gumbel-Softmax44,45 kernels with a prior
temperature parameter of 0.5 + 10n−1. These prior pa-
rameters are, however, updated by a Bayesian neural net-
work in light of the observed data.29 As the prior pre-
cision of the kernels increases with the number of ob-
servations, the surrogate model is encouraged to ﬁt the
observations more accurately as more data is acquired.
Acquisition function optimization in Gryffin gener-
ally follows a two-step strategy in which a global search
is followed by local reﬁnement. First, sets of input pa-
rameters xi are sampled uniformly from the optimiza-
tion domain. By default, the number of samples is set
to be directly proportional to the dimensionality of the
optimization domain. Then, continuous parameters are
optimized with a gradient method for a pre-deﬁned num-
ber of iterations. While early versions of Gryffin em-
ployed second order methods such as L-BFGS, the de-
fault gradient-based approach is now Adam.46 Categori-
cal and discrete parameters are optimized instead follow-
ing a hill-climbing approach (which we refer to as Hill ),
in which each initial sample xi is modiﬁed uniformly at
random, one dimension at a time, and each move is ac-
cepted only if it improves α(xi).

In addition to gradient-based optimization of the ac-
quisition function,
in this work we have implemented
gradient-free optimization via a genetic algorithm. The
population of parameters is ﬁrstly selected uniformly at
random, as described above. Then, each xi in the pop-
ulation is evolved via crossover, mutation and selection
operations, with elitism applied. This approach handles
continuous, discrete, and categorical parameters by ap-
plying diﬀerent mutations and cross-over operations de-
pending on the type. A more detailed explanation of
the genetic algorithm used for acquisition optimization is
provided in SI Sec. S.1.B. This approach is implemented
in Python and makes use of the DEAP library.47,48

Figure 1 provides a visual example of how these two ap-
proaches optimize the acquisition function of Gryffin.
While Adam optimizes each random sample xi (black
crosses) via small steps towards better α(xi) values (grey
to white crosses), the genetic approach does so via larger
stochastic jumps in parameter space.

B. Constrained acquisition optimization with
Adam or Hill

To constrain the optimization of the acquisition func-
tion according to a user-deﬁned constraint function c(x),
we ﬁrst sample a set of feasible parameters xi with re-
jection sampling. That is, we sample x ∼ X uniformly
from the optimization domain and we retain only sam-

4

ples that satisfy the constraint function c(x). Sampling
is performed until the desired number of feasible samples
is drawn. Local optimization of parameters is then per-
formed with Adam, as described above, but it is termi-
nated as soon as an update results in c(xi) (cid:55)→ infeasible
(Fig. 1). For categorical and discrete parameters, Hill
is used rather than Adam, but the constraint protocol
is equivalent. In this case, after rejection sampling, any
move in parameter space is accepted only if it improves
α(xi) and subject to xi ∈ C.

In addition to the above, we also modify the prior pre-
cisions of the kernels used by the surrogate model. In par-
ticular, we substitute the number of observations n with
the observation density ρ = n/VC in the prior hyperpa-
rameters for the Gaussian and Gumbel-Softmax kernels,
where VC is the volume of the feasible region as a fraction
of the overall optimization domain. When no constraints
are used, VC = 1, and the formulation reduces to the
original one used by Gryffin. However, when known
constraints are present, this approach ensures that the
kernels’ bandwidths reﬂect the information density of the
data and avoids underﬁtting. As VC is known and can be
computed in advance, the user is asked to provide it when
using constraints. However, for convenience, and in case
of fairly complex sets of constraints, we extend Gryffin
to also numerically estimate VC. While this approach
to kernel scaling may not be optimal in the presence of
non-isotropic feasible regions, in which the viable opti-
mization domain may expand to diﬀerent extent in dif-
ferent dimensions, it is simple as well as independent of
the details of the constraints, such that it results in no
additional overheads.

C. Constrained acquisition optimization with a
genetic algorithm

The population of the genetic optimization procedure
is initialized with rejection sampling as described above
for gradient approaches. However, to keep the optimized
parameters within the feasible region, a subroutine is
used to project infeasible oﬀsprings onto the feasibility
boundary using binary search (SI Sec. S.1.B). In addi-
tion to guaranteeing that the optimized parameters sat-
isfy the constraint function, this approach also ensures
sampling of parameters close to the feasibility boundary
(Fig. 1). We also use modiﬁed prior kernel precisions as
described in the previous section.

Despite having implemented constraints in Gryffin
independently, when preparing this manuscript we real-
ized that known constraints in Dragonfly49 have been
implemented following a very similar strategy. However,
rather than projecting infeasible oﬀspring solution on the
feasibility boundary, Dragonfly relies solely on rejec-
tion sampling.

A practical advantage of the genetic optimization
strategy is its favourable computational scaling compared
to its gradient-based counterpart. We conducted bench-

marks in which the time needed by Gryffin to optimize
the acquisition function was measured for diﬀerent num-
bers of past observations and parameter space dimen-
sions (SI Sec. S.1.C). In fact, acquisition optimization is
the most costly task in most Bayesian optimization al-
gorithms, including Gryffin. The genetic strategy pro-
vided a speedup of approximately 5× over Adam when
the number of observations was varied, and of approxi-
mately 2.5× when the dimensionality of the optimization
domain was varied. The better time complexity of the
zeroth-order approach is primarily due to derivatives of
α(x) not having to be computed. In fact, our Adam im-
plementation currently computes derivatives numerically.
Future work will focus on deriving the analytical gradi-
ents for Gryffin’s acquisition function, or taking advan-
tage of automatic diﬀerentiation schemes, such that this
gap might reduce or disappear in future versions of the
code.

D. User interface

With user-friendliness and ﬂexibility in mind, we ex-
tended Gryffin’s Python interface such that it can take
a user-deﬁned constraint function among its inputs. As
an example, the following code snippet shows an instan-
tiation of Gryffin where the sum of two volumes is con-
strained.

from gryffin import Gryffin

# set path to optimization configuration
config_file = "config.json"

def my_constraints(params):

vol0 = params["volume_0"]
vol1 = params["volume_1"]
if vol0 + vol1 > 50:
return False

else:

return True

5

mentation handles only inequality constraints. Equal-
ity constraints, such as x1 + x2 = b, eﬀectively change
the dimensionality of the optimization problem and can
be tackled via a re-deﬁnition of the optimization do-
main or analytical transforms. For instance, for the
two-dimensional example above, the constraint can be
satisﬁed simply by optimizing over x1, while setting
x2 = b − x1.

Second, because of the rejection sampling procedure
used (Sec. II.B), the cost of acquisition optimization is
inversely proportional to the fraction of the optimiza-
tion domain that is feasible. The smaller the feasible
volume fraction is, the more samples need to be drawn
uniformly at random before reaching the predeﬁned num-
ber of samples to be collected for acquisition optimiza-
It thus follows that the feasible region, as deter-
tion.
mined by the user-deﬁned constraint function, should
not be exceedingly small (e.g., less than 1% of the op-
timization domain), as in that case Gryffin’s acquisi-
tion optimization would become ineﬃcient. In practice
we ﬁnd that when only a tiny fraction of the overall opti-
mization domain is considered feasible, it is because of a
sub-optimal deﬁnition of the optimization domain, which
can be solved by re-framing the problem. A related is-
sue may arise if the constraints deﬁne disconnected fea-
sible regions with vastly diﬀerent volumes. In this sce-
nario, given uniform sampling, little or no samples may
be drawn from the smaller region. However, this too is
an edge case that is unlikely to arise in practical applica-
tions, as the deﬁnition of completely separate optimiza-
tion regions with vastly diﬀerent scales tends to imply a
scenario where multiple separate optimizations would be
a more suitable solution.

Future work will focus on overcoming these two chal-
lenges. For instance, deep learning techniques like invert-
ible neural networks50 may be used to learn a mapping
from the unconstrained to the constrained domain, such
that the optimization algorithm would be free to oper-
ate on the hypercube while the proposed experimental
conditions would satisfy the constraints.

gryffin = Gryffin(config_file=config_file,

known_constraints=my_constraints)

III. RESULTS AND DISCUSSION

This example could be that of a minimization over two
parameters, x = (x1, x2) ∈ [0, 1]2, subject to the con-
straint that the sum of x1 and x2 does not exceed some
upper bound b,

x∗ = arg min f (x) .
s.t. x1 + x2 ≤ b

E. Current limitations

We note two limitations of our approach to known con-
straints. First, in continuous spaces, the current imple-

In this section we test the ability of Gryffin to
handle arbitrary known constraints. First, we show
how Gryffin can eﬃciently optimize continuous and
discrete (ordered) benchmark functions with a diverse
set of constraints. Then, we demonstrate the practi-
cal utility of handling known constraints on two rele-
vant chemistry examples: the optimization of the syn-
thesis of o-xylenyl Buckminsterfullerene adducts under
constrained ﬂow conditions, and the design of redox ac-
tive molecules for ﬂow batteries under synthetic accessi-
bility constraints.

In addition to testing Gryffin—with both gradient
and evolutionary based-acquisition optimization strate-
gies, referred to as Gryﬃn (Adam/Hill) and Gryﬃn

6

Figure 2. Constrained optimization benchmarks on analytical functions with continuous parameters. The upper row shows
contour plots of the surfaces with constrained regions darkly shaded. Gray crosses show sample observation locations using
the Gryﬃn (Genetic) strategy and purple stars denote the location(s) of unconstrained global optima. The bottom row show
optimization traces for each strategy. Shaded regions around the solid trace represent 95% conﬁdence intervals.

(Genetic), respectively—we also test three other con-
strained optimization strategies amenable to experiment
planning. Speciﬁcally, we use random search (Random),
a genetic algorithm (Genetic), and Bayesian optimiza-
tion with a Gaussian process surrogate model (Drag-
onﬂy). Genetic is the same algorithm developed for
constrained acquisition function optimization, but it is
here employed directly to optimize the objective function
(Sec. II.C). Dragonﬂy uses the Dragonfly package49 for
optimization. Similar to Gryffin, Dragonfly allows
for the speciﬁcation of arbitrarily complex constraints via
a Python function, which is not the case for most other
Bayesian optimization packages. Dragonﬂy was not em-
ployed in the two chemistry examples, due to an imple-
mentation incompatibility with using both constraints
and the multi-objective optimization strategy required
by the applications.

A. Analytical benchmarks

Here, we use eight analytical functions (four continu-
ous and four discrete) to test the ability of Gryffin to
perform sample-eﬃcient optimization while satisfying a
diverse set of user-deﬁned constraints. More speciﬁcally,
we consider the following four two-dimensional continu-
ous surfaces, as implemented in the Olympus package51:
Branin, Schwefel, Dejong, and Discrete Ackley (Fig. 2).
While Branin has three degenerate global minima, we
apply constraints such that only one minimum is present

in the feasible region.
In Schwefel, a highly unstruc-
tured constraint function where the global minimum is
close to the boundary of the infeasible region. In Dejong,
we use a set of constraints that result in a non-compact
optimization domain where the global minimum cannot
be reached, and there is an inﬁnite number of feasible
minima along the feasibility boundary. Discrete Ackley
is a discretized version of the Ackley function, which is
an example of an extremely rugged surface. The four,
two-dimensional discrete surfaces considered are: Slope,
Sphere, Michalewicz, and Camel (Fig. 3). Each variable
in these discrete surfaces is comprised of integer numbers
from 0 to 20, for a design space of 441 options in total.
Results of constrained optimization experiments on
continuous surfaces are shown in Fig. 2. These results
were obtained by performing 100 repeated optimizations
for each of the ﬁve strategies considered, while allowing
a maximum of 100 objective function evaluations. Opti-
mization performance is assessed using the distance be-
tween the best function value found at every iteration
of the optimization campaign and the global optimum, a
metric known as regret, r. The regret after k optimization
iterations is

rk = |f (x∗) − f (x+

k )| .

(2)

x+

k are the parameters associated with best objective
value observed in the optimization campaign after k it-
erations, sometimes referred to as the incumbent point,
i.e. for a minimization problem x+
f (x),

k = arg minx∈Dk

where Dk is the current dataset of observations. x∗ are
the parameters associated with the global optimum of
the function. Sample-eﬃcient algorithms should ﬁnd pa-
rameters that return better (in this case lower) function
values with fewer objective evaluations.

All optimization strategies tested obeyed the known
constraints (Fig. 2, top row). We observe that the combi-
nation of analytical function and user-deﬁned constraint
have great inﬂuence on the relative optimization perfor-
mance of the considered strategies. On all functions, the
performance of Gryffin was insensitive to the acqui-
sition optimization strategy, with Gryﬃn (Adam) and
Gryﬃn (Genetic) performing equally well. On smooth
functions, such as Branin and Dejong, Dragonﬂy dis-
played strong performance, given that its Gaussian pro-
cess surrogate model approximates these functions ex-
tremely well. In addition, Dragonfly appears to pro-
pose parameter points that are closer to previous ob-
servations than Gryffin does, showcasing a more ex-
ploitative character, which in this case is beneﬁcial to
achieve marginal gains in performance when in very close
proximity to the optimum (see SI Sec. S.2.C for addi-
tional details). Although their performance is slightly
worse than Dragonﬂy, Gryﬃn strategies still displayed
improved performance over Random and Genetic on
Branin, and comparable perforamance to Dragonﬂy on
Dejong. Gryﬃn strategies showed improved performance
compared to Dragonﬂy on Schwefel and DiscreteAckley.
These observations are consistent with previous work29
where it was observed that, due to the diﬀerent surrogate
models used, Gryffin returned better performance on
functions with discontinuous character, while Gaussian
process-based approaches better performed on smooth
surfaces. On Schwefel, our Genetic strategy also showed
improved performance over that of Random and Dragon-
ﬂy, comparable with that of Gryﬃn. Finally, note that
in Fig. 2 diﬀerences between strategies are exaggerated
by the use of a logarithmic scale, used to highlight sta-
tistically signiﬁcant diﬀerences. We also report the same
results using a linear scale, which de-emphasizes signif-
icant yet marginal diﬀerences in regret values (e.g. be-
tween Dragonﬂy and Gryﬃn on the Branin function) in
SI Fig. S2.

Results of constrained optimization experiments on
discrete synthetic surfaces are shown in Fig. 3. These re-
sults are also based on 100 repeated optimizations, each
initialized with a diﬀerent random seed. Optimizations
were allowed to continue until the global optimum was
found. As a measure of performance, the number of ob-
jective function evaluations required to identify the op-
timum was used. As such, a more eﬃcient algorithm
should identify the optimum with fewer function evalua-
tions on average.

Here too, all strategies tested correctly obeyed each
constraint function (Fig. 3, top row), and Bayesian op-
timization algorithms (Gryﬃn and Dragonﬂy) outper-
formed Random and Genetic on all four benchmarks
(Fig. 3, bottom row). Random needed to evaluate about

7

half of the feasible space before identifying the optimum.
Genetic required signiﬁcantly fewer evaluations, gener-
ally between a half and a third of those required by
Random. On discrete surfaces, the approaches based on
Gryffin and Dragonfly returned equal performance
overall, with no considerable diﬀerences (Table S1). As it
was observed for continuous surfaces, the performance of
Gryﬃn (Adam) and Gryﬃn (Genetic) was equivalent.

B. Process-constrained optimization of o-xylenyl
C60 adducts synthesis

Compared to their inorganic counterparts, organic so-
lar cells have the advantage of being ﬂexible, lightweight
and easily fabricated.52,53 Bulk heterojunction polymer-
fullerene cells are an example of such devices, in which
the photoactive layer is composed of a blend of poly-
meric donor material and fullerene derivative acceptor
molecules.54 The fullerene acceptor is oftentimes func-
tionalized to tune its optoelectronic properties. In par-
ticular, mono and bis o-xylenyl adducts of Buckmin-
sterfullerene are acceptor molecules which have received
much attention in this regard. On the other hand, higher-
order adducts are avoided as they have a detrimental
impact on the power conversion eﬃciencies of the result-
ing device.55 Therefore, synthetic protocols that provide
accurate control over the degree of C60 funcionalization
are of primary interests for the manufacturing of these
organic photovoltaic devices.

In this example application of constrained Bayesian op-
timization, we consider the reaction of C60 with an excess
of sultine, the cyclic ester of a hydroxy sulﬁnic acid, to
form ﬁrst, second, and third order o-xylenyl adducts of
Buckminsterfullerene (Fig. 4a). This application is based
on previous work by Walker et al.,35 who reported the
optimization of this reaction with an automated, single-
phase capillary-based ﬂow reactor. Because this reaction
is cascadic, a mixture of products is always present, with
the relative amounts of each species depending primarily
on reaction kinetics. The overall goal of the optimization
is thus to tune reaction conditions such that the yield of
ﬁrst- and second-order adducts is maximized and reaches
at least 90%, while the cost of reagents is minimized. We
estimated reagents cost by considering the retail price
listed by a chemical supplier (S.3.B). In eﬀect, we de-
rive a simple estimate of per-minute operation cost of
the ﬂow reactor, which is to be minimized as the second
optimization objective.

The controllable reaction conditions are the tempera-
ture (T ), the ﬂow rate of C60 (FC), and the ﬂow rate
of sultine (FS), which determine the chemical composi-
tion of the reaction mixture and are regulated by syringe
pumps (Fig. 4b). T is allowed to be set anywhere between
100 and 150°C, and ﬂow rates can be varied between 0
and 200 µL/min. However, the values of the ﬂow rates
are constrained. First, the total ﬂow rate cannot exceed
310 µL/min, or be below 10 µL/min. Second, FS can-

8

Figure 3. Constrained optimization benchmarks on analytical functions with discrete, ordered parameters. The upper row
shows heatmaps of the discrete optimization domain. Shaded regions indicate infeasible regions, where constraints have been
applied. The locations of the global optima are indicated by purple stars. Gray crosses show parameters locations that have
been probed in a sample optimization run using Gryﬃn (Genetic) before the optimum being identiﬁed. The bottom row shows,
as superimposed box-and-whisker and swarm plots, the distributions of the number of evaluations needed to identify the global
optimum for each optimization strategy (numerical values can be found in Table S1).

not be more than twice FC, and vice-versa FC cannot be
more than twice FS. More formally, the inequality con-
straints can be deﬁned as 10 < FC + FS < 310 µL/min,
FC < 2FS and FS < 2FC.

The relative concentrations of each adduct—[X1],
[X2], and [X3]
for the mono, bis, and tris adduct,
respectively—are measured via online high performance
liquid chromatography (HPLC) and absorption spec-
troscopy. Following from the above discussion, we would
like to maximize the yield of [X1] and [X2], which are the
adducts with desirable optoelectronic properties. How-
ever, we would also like to reduce the overall cost of the
raw materials. These multiple goals are captured via
the use of Chimera as a hierarchical scalarizing func-
tion for multi-objective optimization.31 Speciﬁcally, we
set the ﬁrst objective as the maximization of [X1] + [X2],
with a minimum target goal of 0.9, and the minimization
of reagents cost as the secondary goal. Eﬀectively, this
setup corresponds to the minimization of cost under the
constraint that [X1] + [X2] ≥ 0.9.

To perform numerous repeated optimizations with dif-
ferent algorithms, as well as collect associated statis-
tics, we constructed a deep learning model to simu-
late the above-described experiment.
In particular, we
trained a Bayesian neural network based on the data
provided by Walker et al.,35 which learns to map re-
action conditions to a distribution of experimental out-
comes (SI Sec. S.3.A). The measurement is thus stochas-

tic, as expected experimentally. This emulator takes
T , FC, and FS as input, and returns mole fractions of
the un- (X0), singly- (X1), doubly- (X2), and triply-
functionalized (X3) C60. The model displayed excellent
interpolation performance across the parameter space for
each adduct type (Pearson coeﬃcient of 0.93 − 0.96 on
the test sets). This enabled us to perform many repeated
optimizations thanks to rapid and accurate simulated ex-
periments.

Results of the constrained optimization experiments
are shown in Fig. 4c. Optimization traces show the ob-
jective values associated with the best scalarized merit
value. The constrained Gryffin strategies obeyed all
ﬂow rate constraints deﬁned. All strategies rapidly iden-
tiﬁed reaction conditions producing high yields. Upon
satisfying the ﬁrst objective, Chimera guided the opti-
mization algorithms towards lowering the cost of the re-
action. Gryffin achieved cheaper protocols faster than
the other strategies tested. In SI Sec. S.3.B we examine
the best reactions conditions identiﬁed by each optimiza-
tion strategy after 100 experiments. We ﬁnd that for the
majority of cases, strategies decreased the C60 ﬂow rate
(the more expensive reagent) to minimize the cost of the
experiment, while simultaneously decreasing the sultine
ﬂow rate to maintain a stoichiometric ratio close to one
and preserve the high (≥ 0.9 combined mole fraction)
yield of X0 and X1 adducts.
In principle, the yield is
allowed to degrade with an accompanying decrease (im-

9

(a) Synthesis of
Figure 4. Experimental setup and results of the process-constrained synthesis of o-xylenyl C60 adducts.
o-xylenyl C60 adducts. 1,4-dihydro-2,3-benzoxathiin 3-oxide (sultine) is converted to o-quinidimethane (oQDM) in situ, which
then reacts with C60 by Diels-Alder cycloaddition to form the o-xylenyl adducts. (b) Schematic of the single-phase capillary-
based ﬂow reactor (as reported by Walker et al.35), along with the optimization parameters, constraints, and objectives. The
C60 and sultine ﬂow rates are modulated by syringe pumps, and the reaction temperature is controlled by a solid state heater.
Online HPLC and absorption spectroscopy is used for analysis. (c) Results of the constrained optimization experiments. Plots
show the mean and 95% conﬁdence interval of the best objective values found after varying numbers of experiments, for each
optimization algorithm studied. The background color indicates the desirability of the objective values, with grey being less,
and white more desirable. For the ﬁrst objective (left-hand side), the grey region below the value of 0.9 indicates values for the
objective that do not satisfy the optimization goal that was set.

provement) in cost. However, we did not see the trade-oﬀ
between yield and cost being required within the exper-
In this case study
imental budget of 100 experiments.
we used GPyOpt, as opposed to Dragonfly, as a rep-
resentative of an established Bayesian optimization algo-
rithm, because its implementation was compatible with
the requirements of this speciﬁc scenario. That is, the
presence of known constraints and the use of an exter-
nal scalarizing function for multi-objective optimization.
Use of Chimera as an external scalarizing function re-
quires updating the entire optimization history at each
iteration. Thus, the optimization library must be used
interface. Although Dragonfly does
via an ask-tell
have such an interface, to the best of our knowledge it
does not yet support constrained and multi-objective op-
In this
timizations with external scalarizing functions.
instance, GPyOpt with expected improvement as the
acquisition function performed similarly to Genetic and
Random on average.

C. Design of redox-active materials for ﬂow
batteries with synthetic accessibility constraints

Long-duration, stationary energy storage devices are
needed to handle the rapid growth in intermittent energy
sources.57 Toward this goal, redox ﬂow batteries oﬀer
a potentially promising solution.58–62 Second-generation
non-aqueous redox ﬂow batteries are based on organic
solvent as opposed to water, which enables a much larger
electrochemical window and higher energy density, the
potential to increase the working temperature window,
as well as the use of cheap, earth abundant redox-active
materials.63 Nevertheless, the multiobjective design of
redox-active materials poses a signiﬁcant challenge in
materials science. Recently, the anolyte redox material
2,1,3-benzothiadiazole was shown to have low redox po-
tential, high stability and promising electrochemial cy-
cling performance.64–66 Furthermore, derivatization of
benzothiadiazole enabled self-reporting of battery health
by ﬂuorescence emission.67

In this application we demonstrate our constrained
optimization approach for the multi-objective design of
retro-synthetically accessible redoxmer molecules. We
utilize a previously reported dataset that comprises 1408
benzothiadiazole derivatives (Fig. 5a) for which the re-
duction potential Ered, solvation free energy Gsolv, and
maximum absorption wavelength λabs were computed
with DFT.68 This example application thus simulates a
DFT-based computational screen69–71 that aims to iden-
tify redoxmer candidates with self-reporting features and
a high probability of synthetic accessibility.

To impose synthetic accessibility constraints we used
RAscore 56, a recently-reported synthetic accessibility
score based on the computer-aided synthesis planner
AiZythFinder 72. RAscore predicts the probability of
AiZythFinder being able to identify a synthetic route for
a target organic molecule. While other measures of syn-

10

thetic accessibility are available,34,73–75 RAscore was cho-
sen for its performance and intuitive interpretation. In
our experiments, we constrained the optimization to can-
didates with RAscore > 0.9 (Fig. 5b, additional details
in SI Sec. S.4.B). This synthetic accessibility constraint
reduces the design space of feasible candidates to a total
of 959 molecules.

We aimed at optimizing three objectives concurrently:
the absorption wavelength λabs, the reduction potential
Ered, and the solvation free energy Gsolv of the candidates
(Fig. 5c). Speciﬁcally, we aimed at identifying molecules
that would (i) absorb in the 350−400 nm range, (ii)
improve upon the reduction potential of the base scaf-
fold (2.04 V against a Li/Li+ reference electrode; SI
Sec. S.4.A), and (iii) provide the lowest possible solva-
tion free energy, here used as a proxy for solubility. The
hierarchical scalarizing function Chimera31 was used to
guide the optimization algorithms toward achieving these
desired properties.

Results of the constrained optimization experiments
are displayed in Fig. 5d. Each optimization strategy was
given a budget of 100 molecules to be tested; the ex-
periments were repeated 100 times and initialized with
diﬀerent random seeds. Optimization traces show the
values of ∆λabs, Ered, and Gsolv associated with the best
molecule identiﬁed among those tested.
In these tests,
the dynamic version of Gryffin was used.30 This ap-
proach can take advantage of physicochemical descrip-
In this case,
tors in the search for optimal molecules.
Gryffin was provided with several descriptors for each
R group in the molecule: number of heavy atoms, number
of hetero atoms, molecular weight, geometric diameter,
polar surface area, polarizability, fraction of sp3 carbons
(SI Sec. S.4.C).76

Constrained Gryffin avoided redoxmer candidates
predicted to be retro-synthetically inaccessible. All op-
timization strategies rapidly identiﬁed candidates with
λabs between 350 and 400 nm, and whose Ered is lower
than that of the starting scaﬀold. However, Gryf-
fin strategies managed to identify candidates with lower
Gsolv faster than Random and Genetic. Consistent with
previous tests, we did not observe a statistically dif-
ferent performance between Gryﬃn (Hill) and Gryﬃn
(Genetic). Overall, Gryﬃn (Hill) and Gryﬃn (Ge-
netic) identiﬁed molecules with better λabs, Ered, and
Gsolv properties than Random or Genetic after probing
100 molecules. Furthermore, within this budget, Gryf-
fin identiﬁed molecules with better properties more ef-
ﬁciently than the competing strategies tested. Without
the aid of physicochemical descriptors, Gryffin’s per-
formance deteriorated, as expected,30 yet it was still su-
perior than that of Random and Genetic (SI Fig. S8).

IV. CONCLUSION

In this work, we extended the capabilities of Gryf-
fin to handle a priori known, hard constraints on the

11

Figure 5. Optimization setup and results for the design of redox-active materials. (a) Markush structure of the benzothiadiazole
scaﬀold and all subtituents considered. The entire design space consists of 1408 candidates (2 R1 × 8 R2 × 8 R3 × 11 R4
(b) Synthetic constraints applied to the optimization. The RAscore 56 was used to score synthesizability. The
options).
histogram shows the distribution of synthesizability scores for all 1408 candidates. We constrain the optimization to those
candidates with an RAscore > 0.9. (c) Objectives of the molecular optimization. (d) Results of the constrained optimization
experiments. Grey shaded regions indicate objective values failing to achieve the desired objectives. Traces depict the objective
values corresponding to the best achieved merit at each iteration, where error bars represent 95% conﬁdence intervals.

parameter domain, a pragmatic requirement for the de-
velopment of autonomous research platforms in chem-
istry. Known constraints constitute an important class
of restrictions placed on optimization domains, and may
reﬂect physical constraints or limitations in laboratory
equipment capabilities.
In addition, known constraints
may provide a straightforward avenue to to inject prior
knowledge or intuition into a chemistry optimization
task. In Gryffin, we allow the user to deﬁne arbitrary
known constraints via a ﬂexible and intuitive Python in-
terface. The constraints are then satisﬁed by constrain-
ing the optimization of its acquisition function.
In all
our benchmarks, Gryffin obeyed the (sometimes com-
plex) constraints deﬁned, showed superior performance
to more traditional search strategies based on random
search and evolutionary algorithms, and was competitive
to state-of-the-art Bayesian optimization approaches. Fi-
nally, we demonstrated the practical utility of handling
known constraints with Gryffin in two research scenar-
ios relevant to chemistry. In the ﬁrst, we demonstrated

how to perform an eﬃcient optimization for the synthesis
of o-xylenyl adducts of Buckminsterfullerene while sub-
jecting the experimental protocol to process constraints.
In the second, we showed how known constraints may be
used to incorporate synthetic accessibility considerations
in the design of redox active materials for non-aqueous
ﬂow batteries. It is our hope that simple, ﬂexible, and
scalable software tools for model-based optimization over
constrained parameter spaces will enhance the applicabil-
ity of data-driven optimization in chemistry and material
science, and will contribute to the operationalization of
self-driving laboratories.

DATA AVAILABILITY

An

open-source

of Gryffin
with known constraints is available on GitHub at
https://github.com/aspuru-guzik-group/gryﬃn,
under
an Apache 2.0 license. The data and scripts used to run

implementation

12

the experiments and produce the plots in this paper are
also available on GitHub at https://github.com/aspuru-
guzik-group/gryﬃn-known-constraints.

ACKNOWLEDGMENTS

The authors thank Dr. Martin Seifrid, Marta Skreta,
Ben Macleod, Fraser Parlane, and Michael Elliott for
valuable discussions. R.J.H. gratefully acknowledges
the Natural Sciences and Engineering Research Council
of Canada (NSERC) for provision of the Postgraduate
Scholarships-Doctoral Program (PGSD3-534584-2019).
M.A. was supported by a Postdoctoral Fellowship of the
Vector Institute. A.A.-G. acknowledges support from the
Canada 150 Research Chairs program and CIFAR, as well
as the generous support of Anders G. Fr¨oseth. This work
relates to the Department of Navy award (N00014-18-S-
B-001) issued by the oﬃce of Naval Research. The United
States Government has a royalty-free license throughout

the world in all copyrightable material contained herein.
Any opinions, ﬁndings, and conclusions or recommenda-
tions expressed in this material are those of the authors
and do not necessarily reﬂect the views of the Oﬃce of
Naval Research. Computations reported in this work
were performed on the computing clusters of the Vec-
tor Institute and on the Niagara supercomputer at the
SciNet HPC Consortium.77,78 Resources used in prepar-
ing this research were provided, in part, by the Province
of Ontario, the Government of Canada through CIFAR,
and companies sponsoring the Vector Institute. SciNet
is funded by the Canada Foundation for Innovation, the
Government of Ontario, Ontario Research Fund - Re-
search Excellence, and by the University of Toronto.

CONFLICTS OF INTEREST

A.A.-G is the Chief Visionary Oﬃcer and founding

member of Kebotix, Inc.

[1] A. McNally, C. K. Prier, and D. W. C. MacMillan, “Dis-
covery of an Amino C–H Arylation Reaction Using the
Strategy of Accelerated Serendipity,” Science, 2011.
[2] K. D. Collins, T. Gensch, and F. Glorius, “Contemporary
screening approaches to reaction discovery and develop-
ment,” Nature Chemistry, vol. 6, pp. 859–871, Oct. 2014.
[3] V. Blay, B. Tolani, S. P. Ho, and M. R. Arkin, “High-
Throughput Screening:
today’s biochemical and cell-
based approaches,” Drug Discovery Today, vol. 25,
pp. 1807–1821, Oct. 2020.

[4] W. Zeng, L. Guo, S. Xu, J. Chen, and J. Zhou, “High-
Throughput Screening Technology in Industrial Biotech-
nology,” Trends in Biotechnology, vol. 38, pp. 888–906,
Aug. 2020.

[5] L. Cheng, R. S. Assary, X. Qu, A. Jain, S. P. Ong,
N. N. Rajput, K. Persson, and L. A. Curtiss, “Accelerat-
ing Electrolyte Discovery for Energy Storage with High-
Throughput Screening,” The Journal of Physical Chem-
istry Letters, vol. 6, pp. 283–291, Jan. 2015.

[6] J. Moˇckus, “On bayesian methods for seeking the ex-
tremum,” in Optimization techniques IFIP technical con-
ference, pp. 400–404, Springer, 1975.

[7] J. Mockus, V. Tiesis, and A. Zilinskas, “The application
of bayesian methods for seeking the extremum,” Towards
global optimization, vol. 2, no. 117-129, p. 2, 1978.

[8] J. Mockus, Bayesian approach to global optimization:
theory and applications, vol. 37. Springer Science & Busi-
ness Media, 2012.

[9] B. J. Shields, J. Stevens, J. Li, M. Parasram, F. Damani,
J. I. M. Alvarado, J. M. Janey, R. P. Adams, and A. G.
Doyle, “Bayesian reaction optimization as a tool for
chemical synthesis,” Nature, vol. 590, pp. 89–96, Feb.
2021.

[10] M. Christensen, L. P. E. Yunker, F. Adedeji, F. H¨ase,
L. M. Roch, T. Gensch, G. dos Passos Gomes, T. Ze-
pel, M. S. Sigman, A. Aspuru-Guzik, and J. E. Hein,
“Data-science driven autonomous process optimization,”

Communications Chemistry, vol. 4, no. 1, 2021.

[11] M. Reis, F. Gusev, N. G. Taylor, S. H. Chung, M. D. Ver-
ber, Y. Z. Lee, O. Isayev, and F. A. Leibfarth, “Machine-
learning-guided discovery of 19f mri agents enabled by
automated copolymer synthesis,” Journal of the Ameri-
can Chemical Society, vol. 143, no. 42, pp. 17677–17689,
2021.

[12] S. Langner, F. H¨ase, J. D. Perea, T. Stubhan, J. Hauch,
L. M. Roch, T. Heumueller, A. Aspuru-Guzik, and C. J.
Brabec, “Beyond ternary opv: High-throughput exper-
imentation and self-driving laboratories optimize multi-
component systems,” Advanced Materials, vol. 32, no. 14,
p. 1907801, 2020.

[13] B. P. MacLeod, F. G. L. Parlane, T. D. Morrissey,
F. H¨ase, L. M. Roch, K. E. Dettelbach, R. Moreira,
L. P. E. Yunker, M. B. Rooney, J. R. Deeth, V. Lai,
G. J. Ng, H. Situ, R. H. Zhang, M. S. Elliott, T. H. Ha-
ley, D. J. Dvorak, A. Aspuru-Guzik, J. E. Hein, and C. P.
Berlinguette, “Self-driving laboratory for accelerated dis-
covery of thin-ﬁlm materials,” Science Advances, vol. 6,
no. 20, p. eaaz8867, 2020.

[14] D. E. Graﬀ, E. I. Shakhnovich, and C. W. Coley,
“Accelerating high-throughput virtual screening through
molecular pool-based active learning,” Chemical Science,
vol. 12, pp. 7866–7881, 2021.

[15] A. E. Gongora, B. Xu, W. Perry, C. Okoye, P. Ri-
ley, K. G. Reyes, E. F. Morgan, and K. A. Brown,
“A bayesian experimental autonomous researcher for
mechanical design,” Science Advances, vol. 6, no. 15,
p. eaaz1708, 2020.

[16] F. H¨ase, L. M. Roch, and A. Aspuru-Guzik, “Next-
Generation Experimentation with Self-Driving Labora-
tories,” Trends in Chemistry, vol. 1, pp. 282–291, June
2019.

[17] L. M. Roch, F. H¨ase, C. Kreisbeck, T. Tamayo-Mendoza,
L. P. E. Yunker, J. E. Hein, and A. Aspuru-Guzik,
“ChemOS: An orchestration software to democratize

autonomous discovery,” PLOS One, vol. 15, no. 4,
p. e0229862, 2020.

[18] J.-P. Correa-Baena, K. Hippalgaonkar, J. van Duren,
S. Jaﬀer, V. R. Chandrasekhar, V. Stevanovic, C. Wa-
dia, S. Guha, and T. Buonassisi, “Accelerating Materi-
als Development via Automation, Machine Learning, and
High-Performance Computing,” Joule, vol. 2, pp. 1410–
1420, Aug. 2018.

[19] H. S. Stein and J. M. Gregoire, “Progress and prospects
for accelerating materials science with automated and au-
tonomous workﬂows,” Chemical Science, vol. 10, no. 42,
pp. 9640–9649, 2019.

[20] E. Stach, B. DeCost, A. G. Kusne, J. Hattrick-Simpers,
K. A. Brown, K. G. Reyes, J. Schrier, S. Billinge,
T. Buonassisi,
I. Foster, C. P. Gomes, J. M. Gre-
goire, A. Mehta, J. Montoya, E. Olivetti, C. Park,
E. Rotenberg, S. K. Saikin, S. Smullin, V. Stanev, and
B. Maruyama, “Autonomous experimentation systems
for materials development: A community perspective,”
Matter, vol. 4, pp. 2702–2726, Sept. 2021.

[21] C. W. Coley, N. S. Eyke, and K. F. Jensen, “Au-
tonomous Discovery in the Chemical Sciences Part I:
Progress,” Angewandte Chemie International Edition,
vol. 59, no. 51, pp. 22858–22893, 2020.

[22] C. W. Coley, N. S. Eyke, and K. F. Jensen, “Au-
tonomous Discovery in the Chemical Sciences Part II:
Outlook,” Angewandte Chemie International Edition,
vol. 59, no. 52, pp. 23414–23436, 2020.

[23] B. Burger, P. M. Maﬀettone, V. V. Gusev, C. M. Aitchi-
son, Y. Bai, X. Wang, X. Li, B. M. Alston, B. Li,
R. Clowes, N. Rankin, B. Harris, R. S. Sprick, and A. I.
Cooper, “A mobile robotic chemist,” Nature, vol. 583,
pp. 237–241, July 2020.

[24] A. Dave, J. Mitchell, K. Kandasamy, H. Wang, S. Burke,
B. Paria, B. P´oczos, J. Whitacre, and V. Viswanathan,
“Autonomous Discovery of Battery Electrolytes with
Robotic Experimentation and Machine Learning,” Cell
Reports Physical Science, vol. 1, p. 100264, Dec. 2020.

[25] A. Deshwal, C. M. Simon, and J. Rao Doppa, “Bayesian
optimization of nanoporous materials,” Molecular Sys-
tems Design & Engineering, vol. 6, no. 12, pp. 1066–1086,
2021.

[26] A. G. Kusne, H. Yu, C. Wu, H. Zhang, J. Hattrick-
Simpers, B. DeCost, S. Sarker, C. Oses, C. Toher, S. Cur-
tarolo, A. V. Davydov, R. Agarwal, L. A. Bendersky,
M. Li, A. Mehta, and I. Takeuchi, “On-the-ﬂy closed-
loop materials discovery via Bayesian active learning,”
Nature Communications, vol. 11, p. 5966, Dec. 2020.
[27] H. Tao, T. Wu, S. Kheiri, M. Aldeghi, A. Aspuru-
Guzik, and E. Kumacheva, “Self-Driving Platform for
Metal Nanoparticle Synthesis: Combining Microﬂuidics
and Machine Learning,” Advanced Functional Materials,
vol. 31, no. 51, p. 2106725, 2021.

[28] P. B. Wigley, P. J. Everitt, A. van den Hengel, J. W.
Bastian, M. A. Sooriyabandara, G. D. McDonald, K. S.
Hardman, C. D. Quinlivan, P. Manju, C. C. N. Kuhn,
I. R. Petersen, A. N. Luiten, J. J. Hope, N. P. Robins,
and M. R. Hush, “Fast machine-learning online optimiza-
tion of ultra-cold-atom experiments,” Scientiﬁc Reports,
vol. 6, May 2016.

[29] F. H¨ase, L. M. Roch, C. Kreisbeck, and A. Aspuru-Guzik,
“Phoenics: A Bayesian Optimizer for Chemistry,” ACS
Central Science, vol. 4, pp. 1134–1145, Sept. 2018.

13

[30] F. H¨ase, M. Aldeghi, R. J. Hickman, L. M. Roch, and
A. Aspuru-Guzik, “Gryﬃn: An algorithm for Bayesian
optimization of categorical variables informed by expert
knowledge,” Applied Physics Reviews, vol. 8, p. 031406,
Sept. 2021.

[31] F. H¨ase, L. M. Roch, and A. Aspuru-Guzik, “Chimera:
enabling hierarchy based multi-objective optimization
for self-driving laboratories,” Chemical Science, vol. 9,
no. 39, pp. 7642–7655, 2018.

[32] M. Aldeghi, F. H¨ase, R. J. Hickman, I. Tamblyn, and
A. Aspuru-Guzik, “Golem: an algorithm for robust ex-
periment and process optimization,” Chem. Sci., vol. 12,
pp. 14792–14807, 2021.

[33] R. J. Hickman, F. H¨ase, L. M. Roch, and A. Aspuru-
Guzik, “Gemini: Dynamic bias correction for au-
tonomous experimentation and molecular simulation,”
2021.

[34] M. Seifrid, R. J. Hickman, A. Aguilar-Granda, C. Lav-
igne, J. Vestfrid, T. C. Wu, T. Gaudin, E. J. Hopkins,
and A. Aspuru-Guzik, “Routescore: Punching the ticket
to more eﬃcient materials development,” ACS Central
Science, vol. 8, no. 1, pp. 122–131, 2022.

[35] B. E. Walker, J. H. Bannock, A. M. Nightingale, and
J. C. deMello, “Tuning reaction products by constrained
optimisation,” Reaction Chemistry & Engineering, vol. 2,
no. 5, pp. 785–798, 2017.

[36] M. A. Gelbart, J. Snoek, and R. P. Adams, “Bayesian op-
timization with unknown constraints,” in Proceedings of
the Thirtieth Conference on Uncertainty in Artiﬁcial In-
telligence, UAI’14, (Arlington, Virginia, USA), pp. 250–
259, AUAI Press, 2014.

[37] R. B. Gramacy and H. K. H. Lee, “Optimization Un-
der Unknown Constraints,” arXiv:1004.4027 [stat], July
2010.

[38] M. A. Gelbart,

and R. P. Adams,
“Bayesian Optimization with Unknown Constraints,”
arXiv:1403.5607 [cs, stat], Mar. 2014.

J. Snoek,

[39] S. Ariafar, J. Coll-Font, D. Brooks, and J. Dy, “AD-
MMBO: Bayesian Optimization with Unknown Con-
straints using ADMM,” Journal of Machine Learning Re-
search, vol. 20, no. 123, pp. 1–26, 2019.

[40] C. Antonio, “Sequential model based optimization of
partially deﬁned functions under unknown constraints,”
Journal of Global Optimization, vol. 79, pp. 281–303, Feb.
2021.

[41] S. Sun, A. Tiihonen, F. Oviedo, Z. Liu, J. Thapa,
Y. Zhao, N. T. P. Hartono, A. Goyal, T. Heumueller,
C. Batali, A. Encinas, J. J. Yoo, R. Li, Z. Ren, I. M.
Peters, C. J. Brabec, M. G. Bawendi, V. Stevanovic,
J. Fisher, III, and T. Buonassisi, “A data fusion approach
to optimize compositional stability of halide perovskites,”
Matter, vol. 4, pp. 1305–1322, Apr. 2021. Publisher: El-
sevier.

[42] Z. Liu, N. Rolston, A. C. Flick, T. Colburn, Z. Ren,
R. H. Dauskardt, and T. Buonassisi, “Machine Learn-
ing with Knowledge Constraints for Process Optimiza-
tion of Open-Air Perovskite Solar Cell Manufacturing,”
arXiv:2110.01387 [physics], Sept. 2021.

[43] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and
N. de Freitas, “Taking the human out of the loop: A re-
view of bayesian optimization,” Proceedings of the IEEE,
vol. 104, no. 1, pp. 148–175, 2016.

[44] E. Jang, S. Gu, and B. Poole, “Categorical reparam-
eterization with gumbel-softmax,” in 5th International

Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track
Proceedings, OpenReview.net, 2017.

[45] Y. W. T. Chris J. Maddison, Andriy Mnih, “The con-
crete distribution: A continuous relaxation of discrete
random variables,” in 5th International Conference on
Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings, Open-
Review.net, 2017.

[46] D. P. Kingma and J. Ba, “Adam: A Method for Stochas-
tic Optimization,” arXiv:1412.6980 [cs], Jan. 2017.
[47] F.-A. Fortin, F.-M. De Rainville, M.-A. Gardner,
M. Parizeau, and C. Gagn´e, “DEAP: Evolutionary al-
gorithms made easy,” Journal of Machine Learning Re-
search, vol. 13, pp. 2171–2175, 2012.

[48] F.-M. De Rainville, F.-A. Fortin, M.-A. Gardner,
M. Parizeau, and C. Gagn´e, “Deap: A python frame-
work for evolutionary algorithms,” in Proceedings of the
14th annual conference companion on Genetic and evo-
lutionary computation, pp. 85–92, 2012.

[49] K. Kandasamy, K. R. Vysyaraju, W. Neiswanger,
B. Paria, C. R. Collins, J. Schneider, B. Poczos, and E. P.
Xing, “Tuning hyperparameters without grad students:
Scalable and robust bayesian optimisation with dragon-
ﬂy.,” J. Mach. Learn. Res., vol. 21, no. 81, pp. 1–27,
2020.

[50] J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duve-
naud, and J.-H. Jacobsen, “Invertible residual networks,”
in Proceedings of the 36th International Conference on
Machine Learning (K. Chaudhuri and R. Salakhutdi-
nov, eds.), vol. 97 of Proceedings of Machine Learning
Research, (Long Beach, California, USA), pp. 573–582,
PMLR, 09–15 Jun 2019.

[51] F. H¨ase, M. Aldeghi, R. J. Hickman, L. M. Roch,
M. Christensen, E. Liles, J. E. Hein, and A. Aspuru-
Guzik, “Olympus: a benchmarking framework for noisy
optimization and experiment planning,” Machine Learn-
ing: Science and Technology, vol. 2, p. 035021, July 2021.
[52] L. X. Chen, “Organic solar cells: Recent progress and
challenges,” ACS Energy Letters, vol. 4, no. 10, pp. 2537–
2539, 2019.

[53] L. Hong, H. Yao, Y. Cui, Z. Ge, and J. Hou, “Recent
advances in high-eﬃciency organic solar cells fabricated
by eco-compatible solvents at relatively large-area scale,”
APL Materials, vol. 8, no. 12, p. 120901, 2020.

[54] C. J. Brabec, S. Gowrisanker, J. J. M. Halls, D. Laird,
S. Jia, and S. P. Williams, “Polymer–fullerene bulk-
heterojunction solar cells,” Advanced Materials, vol. 22,
no. 34, pp. 3839–3856, 2010.

[55] H. Kang, K.-H. Kim, T. E. Kang, C.-H. Cho,
S. Park, S. C. Yoon, and B. J. Kim, “Eﬀect of
Fullerene Tris-adducts on the Photovoltaic Performance
of P3HT:Fullerene Ternary Blends,” ACS Applied Mate-
rials & Interfaces, vol. 5, pp. 4401–4408, May 2013.
[56] A. Thakkar, V. Chadimov´a, E. J. Bjerrum, O. Engkvist,
and J.-L. Reymond, “Retrosynthetic accessibility score
(RAscore) – rapid machine learned synthesizability clas-
siﬁcation from AI driven retrosynthetic planning,” Chem-
ical Science, vol. 12, pp. 3339–3349, Mar. 2021.

[57] T. M. G¨ur, “Review of electrical energy storage technolo-
gies, materials and systems: challenges and prospects for
large-scale grid storage,” Energy & Environmental Sci-
ence, vol. 11, pp. 2696–2767, Oct. 2018.

14

[58] K. Lin, R. G´omez-Bombarelli, E. S. Beh, L. Tong,
Q. Chen, A. Valle, A. Aspuru-Guzik, M. J. Aziz, and
R. G. Gordon, “A redox-ﬂow battery with an alloxazine-
based organic electrolyte,” Nature Energy, vol. 1, pp. 1–8,
July 2016.

[59] P. Leung, A. A. Shah, L. Sanz, C. Flox, J. R. Morante,
Q. Xu, M. R. Mohamed, C. Ponce de Le´on, and F. C.
Walsh, “Recent developments in organic redox ﬂow bat-
teries: A critical review,” Journal of Power Sources,
vol. 360, pp. 243–283, Aug. 2017.

[60] R. Ye, D. Henkensmeier, S. J. Yoon, Z. Huang, D. K.
Kim, Z. Chang, S. Kim, and R. Chen, “Redox Flow Bat-
teries for Energy Storage: A Technology Review,” Jour-
nal of Electrochemical Energy Conversion and Storage,
vol. 15, Sept. 2017.

[61] K. Lourenssen, J. Williams, F. Ahmadpour, R. Clemmer,
and S. Tasnim, “Vanadium redox ﬂow batteries: A com-
prehensive review,” Journal of Energy Storage, vol. 25,
p. 100844, Oct. 2019.

[62] D. G. Kwabi, Y. Ji, and M. J. Aziz, “Electrolyte Lifetime
in Aqueous Organic Redox Flow Batteries: A Critical Re-
view,” Chemical Reviews, vol. 120, pp. 6467–6489, July
2020.

[63] K. Gong, Q. Fang, S. Gu, S. F. Y. Li, and Y. Yan, “Non-
aqueous redox-ﬂow batteries: organic solvents, support-
ing electrolytes, and redox pairs,” Energy & Environmen-
tal Science, vol. 8, pp. 3515–3530, Nov. 2015.

[64] W. Duan, J. Huang, J. A. Kowalski, I. A. Shkrob, M. Vi-
jayakumar, E. Walter, B. Pan, Z. Yang, J. D. Milshtein,
B. Li, C. Liao, Z. Zhang, W. Wang, J. Liu, J. S. Moore,
F. R. Brushett, L. Zhang, and X. Wei, ““Wine-Dark Sea”
in an Organic Flow Battery: Storing Negative Charge in
2,1,3-Benzothiadiazole Radicals Leads to Improved Cy-
clability,” ACS Energy Letters, vol. 2, pp. 1156–1161,
May 2017.

[65] J. Huang, W. Duan, J. Zhang, I. A. Shkrob, R. S. Assary,
B. Pan, C. Liao, Z. Zhang, X. Wei, and L. Zhang, “Sub-
stituted thiadiazoles as energy-rich anolytes for nonaque-
ous redox ﬂow cells,” Journal of Materials Chemistry A,
vol. 6, pp. 6251–6254, Apr. 2018.

[66] J. Zhang, J. Huang, L. A. Robertson, I. A. Shkrob, and
L. Zhang, “Comparing calendar and cycle life stability
of redox active organic molecules for nonaqueous redox
ﬂow batteries,” Journal of Power Sources, vol. 397, Sept.
2018.

[67] L. A. Robertson, I. A. Shkrob, G. Agarwal, Y. Zhao,
Z. Yu, R. S. Assary, L. Cheng, J. S. Moore, and L. Zhang,
“Fluorescence-Enabled Self-Reporting for Redox Flow
Batteries,” ACS Energy Letters, vol. 5, pp. 3062–3068,
Sept. 2020.

[68] G. Agarwal, H. A. Doan, L. A. Robertson, L. Zhang,
and R. S. Assary, “Discovery of Energy Storage Molecu-
lar Materials Using Quantum Chemistry-Guided Multi-
objective Bayesian Optimization,” Chemistry of Materi-
als, vol. 33, pp. 8133–8144, Oct. 2021.

[69] C. d. l. Cruz, A. Molina, N. Patil, E. Ventosa, R. Marcilla,
and A. Mavrandonakis, “New insights into phenazine-
based organic redox ﬂow batteries by using high-
throughput DFT modelling,” Sustainable Energy & Fu-
els, vol. 4, pp. 5513–5521, Oct. 2020.

[70] J. E. Bachman, L. A. Curtiss, and R. S. Assary, “In-
vestigation of the Redox Chemistry of Anthraquinone
Derivatives Using Density Functional Theory,” The Jour-
nal of Physical Chemistry A, vol. 118, pp. 8852–8860,

15

Sept. 2014.

[71] R. S. Assary, F. R. Brushett, and L. A. Curtiss, “Re-
duction potential predictions of some aromatic nitrogen-
containing molecules,” RSC Advances, vol. 4, pp. 57442–
57451, Nov. 2014.

[72] S. Genheden, A. Thakkar, V. Chadimov´a, J.-L. Rey-
mond, O. Engkvist, and E. Bjerrum, “AiZynthFinder:
a fast, robust and ﬂexible open-source software for
retrosynthetic planning,” Journal of Cheminformatics,
vol. 12, p. 70, Nov. 2020.

[73] P. Ertl and A. Schuﬀenhauer, “Estimation of synthetic
accessibility score of drug-like molecules based on molec-
ular complexity and fragment contributions,” Journal of
Cheminformatics, vol. 1, p. 8, June 2009.

[74] C. W. Coley, L. Rogers, W. H. Green, and K. F. Jensen,
“SCScore: Synthetic Complexity Learned from a Reac-
tion Corpus,” Journal of Chemical Information and Mod-
eling, vol. 58, pp. 252–261, Feb. 2018.

[75] M. Vorˇsil´ak, M. Kol´aˇr, I. ˇCmelo, and D. Svozil, “SYBA:
Bayesian estimation of synthetic accessibility of organic
compounds,” Journal of Cheminformatics, vol. 12, p. 35,
May 2020.

[76] H. Moriwaki, Y.-S. Tian, N. Kawashita, and T. Takagi,
“Mordred: a molecular descriptor calculator,” Journal of
Cheminformatics, vol. 10, p. 4, Feb. 2018.

[77] M. Ponce, R. van Zon, S. Northrup, D. Gruner, J. Chen,
F. Ertinaz, A. Fedoseev, L. Groer, F. Mao, B. C.
Mundim, et al., “Deploying a top-100 supercomputer for
large parallel workloads: The niagara supercomputer,” in
Proceedings of the Practice and Experience in Advanced
Research Computing on Rise of the Machines (learning),
pp. 1–8, 2019.

[78] C. Loken, D. Gruner, L. Groer, R. Peltier, N. Bunn,
M. Craig, T. Henriques, J. Dempsey, C.-H. Yu, J. Chen,
et al., “Scinet:
lessons learned from building a power-
eﬃcient top-20 system and data centre,” in Journal of
Physics-Conference Series, vol. 256, p. 012026, 2010.

1

Supplementary Information

Bayesian optimization with known experimental and design constraints for chemistry
applications
Riley J. Hickman,1,2,∗ Matteo Aldeghi,1,2,3,4,∗ Florian H¨ase,1,2,3,5 Al´an Aspuru-Guzik1,2,3,6,7,8

1Chemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON M5S 3H6, Canada
2Department of Computer Science, University of Toronto, Toronto, ON M5S 3G4, Canada
3Vector Institute for Artiﬁcial Intelligence, Toronto, ON M5S 1M1, Canada
4Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge, MA 02139, United States
5Department of Chemistry and Chemical Biology, Harvard University, Cambridge, MA 02138, United States
6Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, ON M5S 3E5, Canada
7Department of Materials Science & Engineering, University of Toronto, Toronto, ON M5S 3E4, Canada
8Lebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON M5G 1Z8, Canada
∗These authors contributed equally

S.1. CONSTRAINED OPTIMIZATION OF THE ACQUISITION FUNCTION

Minima of the acquisition function, α(x) (main text Eq. 1), determine the parameter space point x to be proposed
for objective function measurement, thus, its optimization is an important subroutine in Bayesian optimization.
Acquisition function optimization in Gryffin consists of two main steps. First, a set of N initial parameter space
points are drawn from the domain X using rejection sampling according to constraint function c(x) producing the
set of initial samples Pinit = {xi}N
i=1, where xi ∼ X ; s.t. c(xi) (cid:55)→ feasible. These samples are then reﬁned by one of
several optimization strategies, returning a set of reﬁned proposals Pref = {xi}N
i=1. In the following subsections, we
detail the each acquisition function optimization strategy and compare their computational scaling.

A. Adam/Hill optimizer

For continuous parameters, the Adam optimizer1 is used to optimize the acquisition function, with built in checks for
whether the optimized samples obey the deﬁned known constraints. For discrete and categorical parameter types, we
use a “hill-climbing” strategy, in which each initial sample is randomly updated for a predeﬁned number of iterations,
with the candidate that has the best merit maintained and eventually returned. Algorithm 1 shows this strategy’s
basic pseudocode.

B. Genetic optimizer

Our genetic algorithm implementation is based on the DEAP library2,3 and consists of crossover (C), mutation
(M) and tournament selection (S) operations. The size of the initial population is determined by the number of
initial random samples, N . At each generation, oﬀspring are chosen using tournament-style selection (tournament
size of 3); elitism is applied to 5% of the population. Each parent in the resulting population is given a chance to mate
(using either uniform or two-point crossover, depending on the dimensionality of the parameter space, with a crossover
probability of 0.5), as well as a chance to mutate. We employ a custom mutation operator, M which can handle
continuous, discrete and categorical parameter types. The mutation probability is set to 0.4, with the probability
of mutating each individual parameter set to 0.2. The maximum number of generations is set to 10, but the search
is terminated early if the diversity of the population reaches a speciﬁed threshold. Speciﬁcally, if the population
is concentrated in a small subvolume of parameter space, where each attribute does not span more than 10% of
the allowed range, then the search is terminated. M consists of diﬀerent mutations for each parameter type. For
continuous parameters, a perturbation x(cid:48) is sampled from a Gaussian distribution with scale 0.1, i.e. M(x) = x + x(cid:48),
x(cid:48) ∼ N (0, 0.1). The same is true for discrete parameter types, except x(cid:48) is ﬁrst rounded to the nearest integer value.
For categorical parameters, the mutated oﬀspring are sampled randomly from the set of options for that parameter.
In order to constrain the genetic optimization procedure according to the known constraint function, a subroutine
is used to project infeasible population oﬀspring onto the feasibility boundary using a binary search procedure. After
each application of C or M to parent xp, the feasibility of each resulting oﬀspring xo is tested with c(xo), where
c is the user-deﬁned constraints function.
If c(xo) returns False, i.e. the constraint is not satisﬁed and xo is in
the infeasible region, the following procedure is employed project xo onto the boundary of the feasible region. For

Algorithm 1: Constrained acquisition function optimization with Adam/Hill climbing

Data: initial samples Pinit, acquisition function α(·), constraint function c(·), max iterations imax
Result: reﬁned samples Pref
Pref ← ∅ ;
for xn in Pinit do

2

for i in imax do

n

xn ← AdamStep (cid:0)xcont
xn ← HillStep (cid:0)xcat
xn ← HillStep (cid:0)xdisc
if c(xn) (cid:55)→ infeasible then
+← xi−1
n

(cid:1) ;
(cid:1) ;
(cid:1) ;

Pref

n

n

;

else

Pref

+← xn ;

end

end
Function HillStep(xn):

/* continuous optimization step */
/* categorical optimization step */
/* discrete optimization step */

/* add feasible sample from previous iteration to refined samples */

/* add current sample to refined samples */

ybest ← α (xn);
for zd in xcat
n do
znew
d ← znew
xnew
n
ynew ← α (xnew
if ynew < ybest then
ybest ← ynew ;
xn ← xnew

d ∼ Sd ;
updated with znew
n );

d

;

n

;

/* sample from set of categorical options Sd */

end
return xn

continuous parameters, we consider the midpoint xm of the line segment bounded by xp and xo. If xm is feasible, then
the parent is set to the midpoint; if it is infeasible, the oﬀspring is set to the midpoint. This process is repeated until
the distance between xo and xp is below a tolerance threshold. As a default, we use the criterion (cid:107)xp −xo(cid:107)∞ < 0.01 to
terminate the search, i.e., when we are guaranteed to be within a 1% of relative distance from the feasibility boundary
in all parameter dimensions. Throughout this procedure, xp is guaranteed to always be feasible while xo is always
infeasible. Hence, once the search is terminated, the xo is set equal to xp and is returned. For discrete parameters, the
same process is performed but the search is terminated when the closest point to the feasibility boundary is identiﬁed.
Categorical parameters of infeasible oﬀspring are instead simply reset to those of the feasible parent. When mixed
continuous, discrete, and categorical parameters are present, we (i) set the categorical parameters of xo to those of its
parent, to obtain x(cid:48)
o is feasible, we return it, otherwise we (ii) perform the binary search procedure described
above for the continuous and/or discrete parameters and obtain x(cid:48)(cid:48)
o . Then, we (iii) reset the categorical parameters
of x(cid:48)(cid:48)
o . Given this
approach relies on binary searches, it has a favorable logarithmic scaling and adds little overhead to the optimization
of the acquisition function.

o is feasible, we return it, otherwise we return x(cid:48)(cid:48)

o to their original values in xo, obtaining x(cid:48)(cid:48)(cid:48)

o . If x(cid:48)(cid:48)(cid:48)

o. If x(cid:48)

Algorithm 2 shows the basic pseudocode for our implementation. We show pseudocode for our custom mutation
function M (referred to as Mutation), but we omit deﬁnition of our subroutine which projects infeasible points to
the feasible boundary for brevity. The function is referred to in Algorithm 2 by ProjectToFeasible. We direct the
interested reader to the source code of Gryffin for more details (https://github.com/aspuru-guzik-group/gryﬃn).

C. Empirical time complexity of the Adam and genetic acquisition optimizers

Computational scaling experiments were carried out to compare the relative cost of the Adam and Genetic acquisi-
tion optimization strategies. The time taken to optimize the acquisition function was measured with increasing number
of past observations while keeping the problem dimensionality constant, and with increasing number of parameter
dimensions while keeping the number of observations constant. All parameters were continuous and in [0, 1]. Tests
were performed with and without optimization constraints. When relevant, the constraint used was (cid:80)d
i=1 xi ≤ 0.5d
, where d is dimension of the parameter space and xi are the individual elements of the d-dimensional parameter
vector. That is, we assumed half of the optimization domain to be infeasible. Results of these tests are shown in
Fig. S1. Each datapoint was obtained as the average elapsed time for 60 repeated acquisition function optimizations

Algorithm 2: Constrained acquisition function optimization with genetic algorithm

Data: Pinit, α(·), c(·), imax, crossover operator C with prob pC, custom mutation operator M with prob pM and

3

M , tournament selection operator S

independent prob pindep
Result: reﬁned population Pref
P ← Pinit; f ← α (P) ;
for i in imax do
O ← S (P ) ;
for xi

parent,1, xi

parent,2 in mating pairs do

/* tournament selection of offspring O from population P */

if crossover sample ∼ U (0, 1) < pC then
child,2 ← C (cid:0)xi
xi
child,1, xi
parent,1, xi
child,1 ← ProjectToFeasible (cid:0)xi
xi
child,2 ← ProjectToFeasible (cid:0)xi
xi

(cid:1);
parent,2
child,1, xi
child,2, xi

parent,1

parent,2

end
for xi

parent in O do

if mutant sample ∼ U (0, 1) < pM then

mutant ← M (cid:0)xi
xi
mutant ← ProjectToFeasible (cid:0)xi
xi

parent

(cid:1);

mutant, xi

parent

(cid:1);
(cid:1);

(cid:1);

end
f ← α (O) ;
O +← E ;
P ← O ;

/* evaluate the fitness f of offspring O */

/* add elites E to the offspring O */
/* set population P as the offspring O for next generation */

end
Pref ← P;
Function Mutation(x, pindep

M ):

for xd in x do

if idependent mutation sample ∼ U (0, 1) < pindep

M then

if xd is continuous then

xd ← xd + x(cid:48) ;

/* sample perturbation from unit Gaussian, i.e. x(cid:48) ∼ N (0, 1) */

else if xd is discrete then

xd ← xd + round(x(cid:48), integer) ; /* sample perturbation from unit Gaussian, i.e. x(cid:48) ∼ N (0, 1) */

else if xd is categorical then

xd ← x(cid:48) ;

/* sample x(cid:48) from set of categorical options, i.e. x(cid:48) ∼ Sd */

end
return x

(20 for each λ = {−1, 0, 1}). No appreciable overhead was observed when constraints were present. When keeping the
dimensionality constant, the Genetic strategy showed favourable scaling compared to Adam, being roughly 20% as
expensive as Adam after 100 observations. Similar results were observed in the experiments with a constant number
of observations, where the optimization cost with the Genetic strategy took, on average, ∼ 60% less time than Adam.

Figure S1. Empirical measurements of the time required by Gryffin to optimize its acquisition function. The Adam and
Genetic optimization strategies were compared at varying number of past observations and optimization domain dimensions,
with and without the presence of constraints.

4

S.2. CONSTRAINED OPTIMIZATION OF ANALYTICAL FUNCTIONS

In this section, we provide more details about the constrained analytical functions used for testing Gryffin’s

implementation and performance.

A. Benchmark functions and constraints used

Our synthetic benchmark experiments consisted of four continuous and four discrete surfaces in two dimensions.
The original implementations of the surfaces can be accessed via the Olympus package.4 We used Python wrappers
for each of the surfaces to implement constraints on the parameter space. While the full implementation is available
on GitHub, code snippets are provided here as well to show a user may implement diﬀerent constraint functions to
be used by Gryffin. These constraint functions, called is_feasible(), expect a dictionary, params, containing the
parameter values and evaluate their feasibility; True is returned for feasible, False for infeasible. Note that, while
here we report the deﬁnition of the analytical functions and the input domain typically used, Olympus normalizes
the input domain to the unit hypercube for ease of use (i.e. all analytical function can be expected to be supported
in [0, 1]d). As such, the constraint functions below assume each parameter to be normalized between zero and one.

• Branin This surface is evaluated on the domain x1 ∈ [−5, 10], x2 ∈ [0, 15], and has the form f (x) = a(x2 −
1 + cx1 − r)2 + s(1 − t) cos(x1) + s, with a = 1, b = 5.1/4π2, c = 5/π, t = 1/8π. There are three degenerate
bx2
global minima at (x1, x2) = (−π, 12.275), (π, 2.275) and (9.42478, 2.475). Two of these minima were removed
by the constraints deﬁned below.

def is_feasible(params):
x0 = params['x0']
x1 = params['x1']
y0 = (x0-0.12389382)**2 + (x1-0.81833333)**2
y1 = (x0-0.961652)**2 + (x1-0.165)**2
if y0 < 0.2**2 or y1 < 0.35**2:

return False

else:

return True

• Schwefel : This surface is a complex optimization problem with many local minima.

In d dimensions, it is
evaluated on the hypercube xi ∈ [−500, 500] ∀ i = 1, . . . , d and is described by the expression f (x) = 418.9829d−
(cid:80)d

. The surface has a global optima at x = (420.9687, . . . , 420.9687).

(cid:16)(cid:112)|xi|

(cid:17)

i=1 xi sin

def is_feasible(params):

np.random.seed(42)
N = 20
centers = [np.random.uniform(low=0.0, high=1.0, size=2) for i in range(N)]
radii = [np.random.uniform(low=0.05, high=0.15, size=1) for i in range(N)]
x0 = params['x0']
x1 = params['x1']
Xi = np.array([x0, x1])
for c, r in zip(centers, radii):

if np.linalg.norm(c - Xi) < r:

return False

return True

• Dejong: This surface generalizes a parabola to higher dimensions. It is convex and unimodal an evaluated on
the d-dimensional hypercube xi ∈ [−5, 5], ∀ i = 1, . . . , d. In two dimensions, this surface has a global minimum
at (x0, x1) = (0, 0) with y = 0.

def is_feasible(params):
x0 = params['x0']
x1 = params['x1']

5

y = (x0-0.5)**2 + (x1-0.5)**2
if np.abs(x0-x1) < 0.1:

return False
if 0.05 < y < 0.15:
return False

else:

return True

• DiscreteAckley: This surface is the discrete analogue to the Ackley function.

def is_feasible(self, params):

x0 = params['x0']
x1 = params['x1']

if np.logical_or(0.41 < x0 < 0.46, 0.54 < x0 < 0.59):

return False

if np.logical_or(0.34 < x1 < 0.41, 0.59 < x1 < 0.66):

return False

return True

• Slope This surface generalizes a plane to discrete domains. The surface’s values linearly increase along each

dimension. Constraints form three area elements deﬁned by circles with increasing radii.

def is_feasible(params):
x0 = params['x0']
x1 = params['x1']
y = x0**2 + x1**2
if 5 < y < 25:

return False
if 70 < y < 110:
return False

if 200 < y < 300:

return False

return True

• Sphere: This surfaces generalizes a parabola to discrete spaces. It features a degenerate global minimum if the
number of options along at least one dimension is even,and a well-deﬁned minimum if the number of options
for all dimensions is odd. Constraints remove the same two integer inputs, 9 and 11, from consideration in both
dimensions.

def is_feasible(params):
x0 = params['x0']
x1 = params['x1']
if x0 in [9, 11]:

return False

if x1 in [9, 11]:

return False

return True

• Michalewicz : This surface features a sharper well where the global optimum is located. The number of psuedo-
local minima scales factorially with the number of dimensions. Constraints consist of the area element between
√
a circle centred around (x0, x1) = (14, 10) with radii

30, as well as two rectangular areas.

5 and

√

def is_feasible(params):
x0 = params['x0']
x1 = params['x1']

y = ((x0-14))**2 + (x1-10)**2
if 5 < y < 30:

return False
if 12.5 < x0 < 15.5:
if x1 < 5.5:

return False

if 8.5 < x1 < 11.5:
if x0 < 9.5:

return False

return True

6

• Camel : This surface features a degenerate and pseudo-disconnected global minimum. In 2-dimensions, it has
global minima at (x0, x1) = (7, 11) and (x0, x1) = (14, 10). Constraints are generated by randomly sampling
100 infeasible locations and excluding the (x0, x1) = (7, 11) optima.

def is_feasible(params):

# choose infeasible points at random
num_opts = 21
options = [i for i in range(0,num_opts,1)]
num_infeas = 100
np.random.seed(42)
infeas_arrays = np.array([np.random.choice(options, size=num_infeas,replace=True),

infeas_tuples = [tuple(x) for x in infeas_arrays]

np.random.choice(options, size=num_infeas,

replace=True)]).T

# always exclude the other minima
infeas_tuples.append((7, 11))
infeas_tuples.append((7, 15))
infeas_tuples.append((13, 5))

x0 = params['x0']
x1 = params['x1']
sample_tuple = (x0, x1)
if sample_tuple in infeas_tuples:

return False

return True

B. Results of the constrained optimization benchmarks

Fig. S2 shows the results of the continuous optimization benchmarks where regret is displayed on a linear scale,
which highlights how performance diﬀerences between Gryffin and Dragonfly on Branin and Dejong are marginal.
Table S1 reports the optimization performance achieved by the strategies tested on the discrete surfaces.

−
Random
Genetic
Gryffin (Hill)
Gryffin (Genetic)
Dragonfly

Slope (311)
157.3 ± 9.4
55.2 ± 2.7
12.7 ± 1.0
12.4 ± 1.0
11.0 ± 0.1

Sphere (362)
162.3 ± 9.7
61.5 ± 2.7
19.0 ± 0.8
20.2 ± 0.8
13.6 ± 0.3

Michalewicz (323)
167.8 ± 9.2
47.7 ± 2.4
18.4 ± 0.9
18.7 ± 0.8
29.8 ± 1.2

Camel (347)
171.0 ± 10.8
92.9 ± 6.2
33.8 ± 1.6
36.0 ± 2.6
39.0 ± 2.3

Table S1. Mean and standard error of the number of evaluations needed by each strategy to identify the global optimum of
each constrained discrete surface tested. The integer in parentheses in the header is the number of feasible tiles for the surface
after the constraint is applied, out of a total of 21 × 21 = 442 input combinations.

7

Figure S2. Constrained optimization benchmarks on analytical functions with continuous parameters. The upper row shows
contour plots of the surfaces with constrained regions darkly shaded. Gray crosses show sample observation locations and
purple stars denote the location(s) of unconstrained global optima. The bottom row show optimization traces for each strategy.
Shaded regions around the solid trace represent 95% conﬁdence intervals.

C. Empirical comparison of sampling in Gryﬃn and Dragonﬂy

In this section, we examine the sampling tendencies of Gryffin and Dragonfly on the constrained, continuous
analytical benchmark functions. Speciﬁcally, we compare the tendency of each algorithm to suggest parameter point
which are in close proximity to past observations. The ﬁrst row of Fig. S3 shows the minimum Euclidean distance
between any two parameter points selected during an optimization campaign by each planner (boxplots show this
metric over the 100 independently seeded runs). Dragonﬂy is able to recommend parameter points which are signif-
icantly closer to past observations than Gryﬃn (Adam) or Gryﬃn (Genetic). The greater exploitative tendency of
Dragonﬂy is beneﬁcial on smooth continuous surfaces as it allows for marginal improvement on regret values (main
text Fig. 2). Gryﬃn strategies, on the other hand, contain a self-avoidance routine which biases the search away from
past observations in an attempt to avoid redundant measurements. For practical experimental applications in chem-
istry, the resolution on input parameters is determined by precision of laboratory equipment and/or human error, and
should be considered before commencing the experiment. The bottom two rows show the location of observations for
Gyrﬃn (Adam) and Dragonﬂy strategies around the minima of each surface. Visually, it is apparent that Dragonﬂy
has a greater tendency to recommend parameter points which are considerably closer to past observations than does
Gryﬃn.

S.3. PROCESS-CONSTRAINED OPTIMIZATION OF O-XYLENYL C60 ADDUCTS SYNTHESIS

A. Details of the Bayesian neural network experiment emulator

To emulate the process-constrained synthesis of C60 adducts, we trained a Bayesian neural network (BNN) to return
stochastic outcomes based on a set of controllable parameters. The trained emulator takes a vector containing the
experimental conditions (T , FC60, and FS) and predicts the mole fractions of the products, the un- ([X0]), singly-
([X1]), doubly- ([X2]), and triply-functionalized ([X3]) C60. The BNN consisted of 3 densely-connected variational
layers with reparameterized Monte Carlo estimators5 and was implemented in PyTorch.6. Each hidden layer had
64 nodes and featured a ReLU non-linearity, while the output layer had 4 nodes, one for each of the aformentioned
C60 adducts. The output layer used the softmax activation function, which normalizes the outputs to a probability
distribution where (cid:80)3
i=0[Xi] = 1. Network weights wi and biases bi followed Gaussian distributions whose priors

8

Figure S3. Empirical evaluation of the sampling behaviour of Gryﬃn and Dragonﬂy on constrained continuous surfaces. The
ﬁrst row shows the minimum Euclidean distance between any two parameter points selected by the each optimization strategy.
For each continuous constrained surface, Dragonﬂy allows for recommendation of parameter points which are signiﬁcantly closer
to past observations than does Gryﬃn (Adam) or Gryﬃn (Genetic). The second and third rows shows the location of Gryﬃn
(Adam) and Dragonﬂy samples (grey crosses) in the vicinity of the surface minima (pink star).

Figure S4. Parity plots for each mole fraction predicted by our Bayesian neural network emulator, averaged over 50 network
parameter samples. Horizontal axes plot the true mole fraction, and vertical axes plot the predicted mole fraction. The dashed
diagonal line indicates perfect agreement. The Pearson correlation coeﬃcient and root-mean-square error is given for the
training set (in parentheses) and test set for each mole fraction target. Train (test) set points are shown in blue (orange).

9

were set to have zero mean and unit standard deviation, i.e. wi ∼ N (0, 1), bi ∼ N (0, 1). The network was trained
using variational Bayesian inference. The ELBO loss was minimized using the Adam optimizer1, and resulting
gradients were used to adjust the weights and biases of the network’s parameter distributions during training. Fig. S4
shows parity plots of our model’s predictions against the true C60 adduct mole fractions, where 500 experimental
measurements were used for training and 100 for testing. The BNN displayed excellent interpolation performance
across the parameter space for each adduct type, with Pearson correlation coeﬃcients on the test sets between 0.93
and 0.96.

B. Estimating the experimental cost

The overall goal of the process-constrained optimization of o-xylenyl C60 adducts synthesis is to adjust reaction
conditions such that the combined yield of ﬁrst- and second-order adducts is maximized and reaches at least 90%,
while the cost of reagents is minimized. In order to estimate the cost of the experiments, we considered the listed
price of sultine and C60 by the chemical supplier Sigma-Aldrich. The cost of dibromo-o-xylene cost on Sigma Aldrich
was $191 for 100 g. The cost of C60 was $422 for 5g. In the experiments by Walker et al.7, the concentration of sultine
was 1.4 mg/mL, while the concentration of C60 was 2.0 mg/mL. The amount of C60 used in the experiments will
therefore have much greater inﬂuence on overall experiment cost than sultine. Our optimization experiments target
the adjustment of volume ﬂow rates of each of these chemicals. Thus, we seek a measure of per-unit-time operation
cost to be minimized. Converting to per-litre costs, we have 2.674 $/L for sultine, and 168.8 $/L for C60. Finally,
from the ﬂow rates used in the experiment, FC and FS (with units of µL/min), we obtain an estimate of per-minute
operation cost of the ﬂow-reactor from Walker et al.7 with units of $/min as

cost =

1L
106µL

FC ×

$168.8
L

+

1L
106µL

FS ×

$2.674
L

.

(3)

Fig. S5 shows the mean and 95% conﬁdence interval for parameter values corresponding to the best observed
objective values achieved by each optimization strategy. We report the ﬂow rate in terms of mass per unit time (mass
ﬂow rate) to account for the diﬀerence in concentrations of each reagent and compare the rates on an equal footing.
To improve upon our secondary cost objective, each strategy decreases the FC parameter, as it’s value dominates the
cost in Eq. 3. Decrease in FC is however accompanied by a decrease in FS to preserve the high (≥ 0.9) mol fractions of
the X1 and X2 adducts. For most optimization runs, the temperature of the best performing reactions varies between
116 and 130 ◦C.

Fig. S6 shows distributions of FC − FS values for the best reaction conditions achieved by each optimization
strategy in units of µg/min. For each strategy, we note that this distribution favours positive values, meaning that,
in the majority of the best achieved reaction conditions, the mass ﬂow rate of C60 was greater than that of sultine.
Crucially, the Gryﬃn strategies, which exhibited the best optimization performance on this application, achieved
narrower distributions around FC − FS = 0 than other strategies.

Figure S5. Mean and 95% conﬁdence interval for parameter values corresponding to the best objective values found by each
optimization strategy at each iteration of the optimization campaign. C60 and sultine ﬂow rates are both shown with units of
µg/min.

10

Figure S6. Kernel density estimates show the distribution of FC − FS for the best reactions conditions achieved by each
optimization strategy in units of µg/min. Each distribution is comprised of 100 such values, one for each independently seeded
optimization. Positive values indicate that the best achieved reaction conditions had FC > FS.

S.4. DESIGN OF REDOX-ACTIVE MATERIALS FOR FLOW BATTERIES WITH SYNTHETIC
ACCESSIBILITY CONSTRAINTS

A. Computation of reduction potential tolerance

To set the reduction potential (Ered) upon which we would like to improve, and which is used as an absolute
tolerance in Chimera8, we computed Ered for the base scaﬀold molecule H-AcBzC6.9 We computed Ered with the
same computational protocol used by Agarwal et al.9. The DFT calculation was performed using Gaussian 1610 at
the wb97xd/6-31+G-(d,p)11,12 level of theory. Optimized neutral and anionic geometries were subject to frequency
calculations to compute the free energies. The SMD continuum model13 was used with acetonitrile as the solvent.
The reduction potential was calculated using Eq. 1 in Agarwal et al.9,

Ered =

−∆Gred
nF

− 1.24 V ,

(4)

where ∆Gred = Greduced − Gneutral, n is the number of electrons added to the neutral molecules (n = 1), F is
Faraday’s constant in eV, and 1.24 is a constant subtracted to convert the Gibbs free energy change to reduction
potential (with Li/Li+ reference electrode). The Ered for H-AcBzC6 was computed to be 2.038372 V. Of the 1408
functional derivatives subject to computation by Agarwal et al.9, only 243 had better (lower) Ered.

B. Prediction of the synthetic accessibility of redoxmer candidate molecules

As a constraint on the redoxmer candidates space, we enforce a retrosynthetic accessibility threshold below which
the candidate is considered infeasible. The goal was to have an indication of synthetic accessibility that could be used
to constrain the search space to candidates that likely to be synthesizable in practice.

Fig. S7 shows the distributions of diﬀerent synthetic accessibility scores for the set of 1408 redoxmer candidates
considered in this application. Speciﬁcally, it includes the RAscore 14 predicted by an XGBoost classiﬁer (XGB) and a
neural network (NN), the fragment-based synthetic accessibility score SAscore 15, and the synthetic Bayesian classiﬁer
(SYBA)16. The RAScore is a recently reported synthetic accessibility score that tries to capture the probability
of AiZynthFinder being able to identify a synthetic route for the molecule being evaluated.14 AiZynthFinder is
a retrosynthetic planning tool that can generate synthetic routes for organic molecules.17 Hence, an RAScore of
1 indicates a synthetic path to the desired molecule is likely to exist, while a score of 0 indicates that ﬁnding a
synthetic path is likely to be challenging and potentially impossible. For the purpose of our constrained optimization
experiments, we decided to use the RAscore based on a NN model given its reported performance14 and intuitive
interpretation.

11

Figure S7. Histograms showing the distributions of four synthetic accessibility scores computed for the 1408 redoxmer candi-
dates.

C. Generation of descriptors for benzothiadiazole scaﬀold substituents

In this example application of constrained Bayesian optimization, we employed the Dynamic version of Gryffin
for combinatorial optimization,18 which can take advantage of physicochemical descriptors in the search for optimal
molecules. Speciﬁcally, we provided Gryffin with a total of seven simple descriptors associated with each of the four
substituent groups considered (R1−4 in Fig. 5a). The physicochemical descriptors were computed with the Mordred
Python package.19 As summarized in Table S2, the following descriptors were considered: the number of hetero atoms
(nHetero), molecular weight (MW), topological polar surface area (TopoPSA), number of heavy atoms (nHeavyAtom),
atomic polarizablity (apol), fraction of sp3 hybridized carbons (FCSP3), and geometric diameter (Diameter). All seven
descriptors were used for substituent groups R2−4, but only four of them are used for R1. We eliminate apol, FCSP3
and Diameter from consideration because they each have equal value for both R1 substituent options, and therefore
are not informative. Table S2 sumarizes the Pearson correlation of each descriptor with each objective value over the
entire set of 1408 molecules. N/A entries show the cases where the descriptor is omitted for the R1 substituent. In
addition, Table S2 reports the Pearson correlations between the descriptors for all four R-groups (ρ1, ρ2, ρ3, and ρ4)
and each optimization objective (∆λabs, Ered, and Gsolv). Table S3 reports instead the pairwise correlation between
each descriptor, averaged over all R-groups.

Mordred name

∆λabs

Ered

Gsolv

nHetero
MW
TopoPSA
nHeavyAtom
apol
FCSP3
Diameter

ρ1
0.17
0.17
-0.17
0.17
-0.17
N/A
N/A

ρ2
0.13
0.06
-0.12
0.09
-0.25
-0.11
0.02

ρ3
0.12
0.05
-0.11
0.07
-0.31
-0.06
-0.04

ρ4
0.19
0.15
-0.14
0.20
-0.14
-0.04
0.17

ρ1
0.22
0.22
-0.22
0.22
-0.22
N/A
N/A

ρ2
0.27
0.21
0.13
0.22
-0.26
-0.31
0.16

ρ3
0.26
0.20
0.11
0.21
-0.27
-0.32
0.16

ρ4
0.35
0.19
0.30
0.22
-0.16
-0.36
0.23

ρ1
0.62
0.62
-0.62
0.62
-0.62
N/A
N/A

ρ2
0.24
0.23
0.00
0.22
-0.06
0.13
0.14

ρ3
0.22
0.21
0.01
0.20
-0.04
0.14
0.12

ρ4
-0.12
-0.13
-0.22
-0.21
-0.41
0.02
-0.29

Table S2. Mordred descriptors used to describe R-groups for the battery application optimization. The right most three
columns show the Pearson correlation between each descriptor and each optimization objective for the 1408 redoxmer candidates
considered. The correlations for each of the four R groups are comma separated , i.e. ρR1, ρR3, ρR4, ρR5. The largest correlation
for each objective and R-group is bolded. N/A entries indicate that this descriptor was not considered for this particular R
group. For the R1 group, we do dont consider apol, FCSP3 and Diameter since their values are the same for both R1 options
and therefore provide no additional information.

D. Additional optimization experiments

In addition to Gryffin optimizations taking advantage of physicochemical descriptors (Dynamic Gryﬃn), we also
carried out optimizations without this additional information using Naive Gryﬃn. Fig. S8 shows the optimization

−
nHetero
MW
TopoPSA
nHeavyAtom
apol
FCSP3
Diameter

nHetero
1.00
0.92
0.01
0.91
−0.04
0.2
0.61

MW
0.92
1.00
−0.06
0.93
0.24
0.29
0.71

TopoPSA
0.01
−0.06
1.00
−0.14
0.03
−0.2
−0.13

nHeavyAtom
0.91
0.93
−0.14
1.00
0.32
0.35
0.86

apol
−0.04
0.24
0.03
0.32
1.00
0.47
0.62

FCSP3
0.2
0.29
−0.20
0.35
0.47
1.00
0.32

12

Diameter
0.61
0.71
−0.13
0.86
0.62
0.32
1.00

Table S3. Pairwise Pearson correlations between Mordred descriptors used to describe the R-groups of the redoxmer candidates.

performance of all strategies tested, including the latter. The results show how the use of descriptors provide an
edge to Gryffin to achieve superior performance to all other strategies. Regardless, Naive Gryﬃn still outperforms
model-free optimization strategies Random and Genetic. All these optimizations were constrained to molecules with
high synthetic accessibility scores, as described above.

Figure S8. Results of the constrained optimization experiments for the design of redox-active ﬂow battery materials. Grey
shaded regions indicate objective values failing to achieve the desired objectives. Traces depict the objective values corresponding
to the best achieved merit at each iteration, where error bars represent 95% conﬁdence intervals.

[1] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs], January 2017.
[2] F´elix-Antoine Fortin, Fran¸cois-Michel De Rainville, Marc-Andr´e Gardner, Marc Parizeau, and Christian Gagn´e. DEAP:

Evolutionary algorithms made easy. Journal of Machine Learning Research, 13:2171–2175, 2012.

[3] Fran¸cois-Michel De Rainville, F´elix-Antoine Fortin, Marc-Andr´e Gardner, Marc Parizeau, and Christian Gagn´e. Deap: A
python framework for evolutionary algorithms. In Proceedings of the 14th annual conference companion on Genetic and
evolutionary computation, pages 85–92, 2012.

[4] Florian H¨ase, Matteo Aldeghi, Riley J. Hickman, Lo¨ıc M. Roch, Melodie Christensen, Elena Liles, Jason E. Hein, and Al´an
Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. Machine Learning:
Science and Technology, 2(3):035021, July 2021.

[5] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks, 2015.
[6] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative
style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc.,
2019.

[7] Barnaby E. Walker, James H. Bannock, Adrian M. Nightingale, and John C. deMello. Tuning reaction products by

constrained optimisation. Reaction Chemistry & Engineering, 2(5):785–798, 2017.

0255075100# molecules tested010203040abs [nm]First objective0255075100# molecules tested1.61.82.02.22.42.6Ered [V vs, Li/Li+]Second objectiveRandomGeneticNaive Gryffin (Hill)Naive Gryffin (Genetic)Dynamic Gryffin (Hill)Dynamic Gryffin (Genetic)0255075100# molecules tested0.90.80.70.6Gsolv [eV]Third objective13

[8] Florian H¨ase, Lo¨ıc M. Roch, and Al´an Aspuru-Guzik. Chimera: enabling hierarchy based multi-objective optimization for

self-driving laboratories. Chemical Science, 9(39):7642–7655, 2018.

[9] Garvit Agarwal, Hieu A. Doan, Lily A. Robertson, Lu Zhang, and Rajeev S. Assary. Discovery of Energy Storage Molecular
Materials Using Quantum Chemistry-Guided Multiobjective Bayesian Optimization. Chemistry of Materials, 33(20):8133–
8144, October 2021.

[10] M. J. Frisch, G. W. Trucks, H. B. Schlegel, G. E. Scuseria, M. A. Robb, J. R. Cheeseman, G. Scalmani, V. Barone, G. A.
Petersson, H. Nakatsuji, X. Li, M. Caricato, A. V. Marenich, J. Bloino, B. G. Janesko, R. Gomperts, B. Mennucci, H. P.
Hratchian, J. V. Ortiz, A. F. Izmaylov, J. L. Sonnenberg, D. Williams-Young, F. Ding, F. Lipparini, F. Egidi, J. Goings,
B. Peng, A. Petrone, T. Henderson, D. Ranasinghe, V. G. Zakrzewski, J. Gao, N. Rega, G. Zheng, W. Liang, M. Hada,
M. Ehara, K. Toyota, R. Fukuda, J. Hasegawa, M. Ishida, T. Nakajima, Y. Honda, O. Kitao, H. Nakai, T. Vreven,
K. Throssell, J. A. Montgomery, Jr., J. E. Peralta, F. Ogliaro, M. J. Bearpark, J. J. Heyd, E. N. Brothers, K. N. Kudin,
V. N. Staroverov, T. A. Keith, R. Kobayashi, J. Normand, K. Raghavachari, A. P. Rendell, J. C. Burant, S. S. Iyengar,
J. Tomasi, M. Cossi, J. M. Millam, M. Klene, C. Adamo, R. Cammi, J. W. Ochterski, R. L. Martin, K. Morokuma,
O. Farkas, J. B. Foresman, and D. J. Fox. Gaussian˜16 Revision C.01, 2016.

[11] Jeng-Da Chai and Martin Head-Gordon. Long-range corrected hybrid density functionals with damped atom–atom dis-

persion corrections. Physical Chemistry Chemical Physics, 10(44):6615–6620, November 2008.

[12] Vitaly A. Rassolov, Mark A. Ratner, John A. Pople, Paul C. Redfern, and Larry A. Curtiss. 6-31G* basis set for third-row

atoms. Journal of Computational Chemistry, 22(9):976–984, 2001.

[13] Aleksandr V. Marenich, Christopher J. Cramer, and Donald G. Truhlar. Universal Solvation Model Based on Solute
Electron Density and on a Continuum Model of the Solvent Deﬁned by the Bulk Dielectric Constant and Atomic Surface
Tensions. The Journal of Physical Chemistry B, 113(18):6378–6396, 2009.

[14] Amol Thakkar, Veronika Chadimov´a, Esben Jannik Bjerrum, Ola Engkvist, and Jean-Louis Reymond. Retrosynthetic
accessibility score (RAscore) – rapid machine learned synthesizability classiﬁcation from AI driven retrosynthetic planning.
Chemical Science, 12(9):3339–3349, March 2021.

[15] Peter Ertl and Ansgar Schuﬀenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular

complexity and fragment contributions. Journal of Cheminformatics, 1(1):8, June 2009.

[16] Milan Vorˇsil´ak, Michal Kol´aˇr, Ivan ˇCmelo, and Daniel Svozil. Syba: Bayesian estimation of synthetic accessibility of

organic compounds. Journal of Cheminformatics, 12(1):35, 2021.

[17] Samuel Genheden, Amol Thakkar, Veronika Chadimov´a, Jean-Louis Reymond, Ola Engkvist, and Esben Bjerrum. AiZyn-
thFinder: a fast, robust and ﬂexible open-source software for retrosynthetic planning. Journal of Cheminformatics, 12(1):70,
November 2020.

[18] Florian H¨ase, Matteo Aldeghi, Riley J. Hickman, Lo¨ıc M. Roch, and Al´an Aspuru-Guzik. Gryﬃn: An algorithm for
Bayesian optimization of categorical variables informed by expert knowledge. Applied Physics Reviews, 8(3):031406,
September 2021.

[19] Hirotomo Moriwaki, Yu-Shi Tian, Norihito Kawashita, and Tatsuya Takagi. Mordred: a molecular descriptor calculator.

Journal of Cheminformatics, 10(1):4, February 2018.

