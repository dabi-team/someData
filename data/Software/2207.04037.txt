RASTI 000, 1â€“12 (2022)

Preprint 15 July 2022

Compiled using rasti LATEX style ï¬le v3.0

Bayesian model comparison for simulation-based inference

A. Spurio Mancini
1Mullard Space Science Laboratory, University College London, Dorking, RH5 6NT, UK
2Alan Turing Institute, London, NW1 2DB, UK

,1â˜…â€  M. M. Docherty

,1â€  M. A. Price

1 and J. D. McEwen

1,2

2
2
0
2

l
u
J

4
1

]

O
C
.
h
p
-
o
r
t
s
a
[

2
v
7
3
0
4
0
.
7
0
2
2
:
v
i
X
r
a

Accepted â€”. Received â€”; in original form â€”

ABSTRACT
Comparison of appropriate models to describe observational data is a fundamental task of science. The Bayesian model evidence,
or marginal likelihood, is a computationally challenging, yet crucial, quantity to estimate to perform Bayesian model comparison.
We introduce a methodology to compute the Bayesian model evidence in simulation-based inference (SBI) scenarios (also often
called likelihood-free inference). In particular, we leverage the recently proposed learnt harmonic mean estimator and exploit the
fact that it is decoupled from the method used to generate posterior samples, i.e. it requires posterior samples only, which may
be generated by any approach. This ï¬‚exibility, which is lacking in many alternative methods for computing the model evidence,
allows us to develop SBI model comparison techniques for the three main neural density estimation approaches, including neural
posterior estimation (NPE), neural likelihood estimation (NLE), and neural ratio estimation (NRE). We demonstrate and validate
our SBI evidence calculation techniques on a range of inference problems, including a gravitational wave example. Moreover, we
further validate the accuracy of the learnt harmonic mean estimator, implemented in the harmonic software, in likelihood-based
settings. These results highlight the potential of harmonic as a sampler-agnostic method to estimate the model evidence in both
likelihood-based and simulation-based scenarios.
Key words: machine learning â€“ numerical methods â€“ software â€“ statistics â€“ simulation-based inference

1 INTRODUCTION

Bayesian model comparison provides a robust and principled sta-
tistical framework for the selection of appropriate scientiï¬c models
to describe observational data. The key quantity to perform model
comparison in a Bayesian inference framework is the model evi-
dence, or marginal likelihood, whose estimate allows one to assign
relative weights to diï¬€erent models (see, e.g., Trotta 2008 for a re-
view of Bayesian model comparison, particularly in the context of
cosmology). However, obtaining a precise and accurate estimate of
the Bayesian model evidence is a computationally challenging task,
involving a multi-dimensional integral which may quickly exceed the
available computational resources for parameter spaces of even mod-
erate dimensions. A variety of techniques for computing the Bayesian
model evidence have been proposed (see, e.g., Friel & Wyse 2012;
Llorente et al. 2020 for reviews).

One of the most widely used classes of algorithms for estimating
the model evidence, particularly in astrophysics and cosmology, is
nested sampling (Skilling 2006), a method for which posterior in-
ferences can also be computed as a byproduct (see, e.g., Buchner
2021; Ashton et al. 2022 for reviews of nested sampling). Common
nested sampling algorithms, such as multinest (Feroz & Hobson
2008; Feroz et al. 2009) and polychord (Handley et al. 2015a,b)
have been of remarkable success in multiple research areas. How-
ever, they can struggle in high dimensional parameter spaces. The

â˜… E-mail: a.spuriomancini@ucl.ac.uk
â€  Authors contributed similarly.

Â© 2022 The Authors

recently proposed proximal nested sampling framework scales to
very high dimensional settings (Cai et al. 2021). However, proximal
nested sampling is restricted to log-convex likelihoods. Neverthe-
less, such likelihoods are common and so proximal nested sampling
is likely to be particularly useful for inverse imaging problems. As
the name suggests, nested sampling couples the computation of the
model evidence to the sampling approach, restricting its ï¬‚exibility.
That is, sampling must be performed in a prescribed (i.e. nested)
manner.

The recently proposed learnt harmonic mean estimator (McEwen
et al. 2021) for computation of the model evidence removes this
restriction. While the original harmonic mean estimator (Newton &
Raftery 1994) can fail catastrophically since its variance may become
very large and may not be ï¬nite (Neal 1994), the learnt harmonic
mean solves this problem by learning an approximation of the opti-
mal internal importance sampling target distribution (McEwen et al.
2021). Critically, the learnt harmonic mean estimator requires only
samples from the posterior and so is agnostic to sampling strategy,
aï¬€ording it great ï¬‚exibility, which is crucial to the current work.

The need for eï¬ƒcient and reliable methods for computing the
model evidence applies not only to likelihood-based settings, but
also to simulation-based inference (SBI) frameworks. In the SBI set-
ting (sometimes referred to as likelihood-free inference; LFI), the
likelihood is either not available or too costly to be evaluated, and
the inference process relies solely on the ability to simulate observ-
ables. Approximate Bayesian computation (ABC) is the traditional,
prototypical SBI technique that relies on rejection sampling of pa-
rameter sets on the basis of a similarity metric between the simulated

 
 
 
 
 
 
2

A. Spurio Mancini et al.

observables and the observations (see, e.g., Beaumont 2019). How-
ever, ABC methods can easily require an inaccessible number of
simulations to reach convergence, limiting their applicability. More
recently neural density estimation techniques have been introduced
to surrogate densities directly. Such novel frameworks have seen nu-
merous successful applications in various scientiï¬c areas, carrying
great promise for the future due to their ability to avoid the evalua-
tion of an explicit (and possibly incorrect) likelihood function, while
limiting the number of simulations with clever use of cutting-edge
machine learning algorithms. For a recent review of SBI techniques
we refer the reader to Cranmer et al. (2020). Neural density estima-
tion methods have recently found remarkable success in cosmology,
with general-purpose open-source software readily available (e.g.
pydelfi1 by Alsing et al. 2019; swyft2 by Cole et al. 2021).

The development of new SBI techniques has so far mostly concen-
trated on optimising the task of parameter estimation, while model
selection has received less attention. Nevertheless, model selection
is a critical component of a complete statistical analysis, particularly
in scientiï¬c ï¬elds where selection of the appropriate model is often
the fundamental question. While there has been some consideration
of model selection for SBI, the ï¬eld remains nascent. Brewer et al.
(2011) propose a technique based on diï¬€usive nested sampling to
compute the model evidence for ABC. However, this approach is
restricted to ABC, which as discussed above can be ineï¬ƒcient and
of limited applicability, and is not straightforwardly generalisable to
modern neural density estimation approaches.

In this article we introduce a methodology to compute the model
evidence, in order to facilitate Bayesian model comparison, for mod-
ern neural density estimation approaches to SBI. Our methodology
leverages the learnt harmonic mean estimator (McEwen et al. 2021).
We exploit the fact that the learnt harmonic mean estimator is agnos-
tic to sampling strategy and only requires samples from the posterior
distribution. In some neural density estimation approaches samples
can be generated directly (e.g. by pushing samples from a simple base
distribution, such as a Gaussian, forward through a normalising ï¬‚ow;
Papamakarios et al. 2021). To support such neural density estimation
approaches it is therefore essential that model evidence computation
is decoupled from sampling strategy, as is the case with the learnt
harmonic mean estimator.

The remainder of this article is structured as follows. In Sec. 2 and
Sec. 3 we concisely review, respectively, Bayesian model compari-
son and neural density estimation approaches to SBI. In Sec. 4 we
introduce our methodology to perform Bayesian model comparison
in the context of SBI, leveraging the learnt harmonic mean estima-
tor (McEwen et al. 2021). We present algorithms to compute the
Bayesian model evidence for each of the three main neural density
estimation approaches to SBI that are reviewed by Cranmer et al.
(2020). In Sec. 5 we report the results from numerical experiments
that demonstrate and validate our methodology. Finally, we conclude
in Sec. 6 with a review of our main ï¬ndings.

2 BAYESIAN MODEL COMPARISON

We review here the fundamentals of Bayesian model comparison,
focusing on the challenges associated with estimation of the model
evidence. For a more extensive review we refer the reader to, e.g.,
Trotta (2008). We also summarise the key concepts underlying the

1 https://github.com/justinalsing/pydelfi
2 https://github.com/undark-lab/swyft

RASTI 000, 1â€“12 (2022)

learnt harmonic mean estimator since it is an integral component
of the current work (we refer the reader to McEwen et al. 2021 for
further details).

2.1 Bayesian model evidence

The deï¬nition of model evidence in a Bayesian statistical framework
follows directly from Bayesâ€™ theorem. For a given model M, Bayesâ€™
theorem describes the connection between the conditional probabil-
ities of model parameters ğœ½ and data ğ’…:

ğ‘(ğœ½ | ğ’…, M) =

ğ‘( ğ’…|ğœ½, M) ğ‘(ğœ½ |M)
ğ‘( ğ’…|M)

,

(1)

where ğ‘(ğœ½ | ğ’…, M) is the posterior distribution of the parameters,
given the observed data ğ’… and the assumed model M, ğ‘( ğ’…|ğœ½, M) is
the likelihood function of the data ğ’… given parameters ğœ½ and model
M, ğ‘(ğœ½ |M) is the prior distribution of model parameters ğœ½ for a given
model M, and ğ‘( ğ’…|M) is the model evidence, i.e. the probability of
data ğ’… for a given model M. The Bayesian model evidence is given
by the normalisation factor of the posterior distribution ğ‘(ğœ½ |M, ğ’…):

ğ‘§ = ğ‘( ğ’…|M) =

âˆ«

dğœ½ ğ‘( ğ’…|ğœ½, M) ğ‘(ğœ½ |M).

(2)

Since the model evidence is a normalisation factor for the pos-
terior distribution and is independent of the model parameters, the
evidence is usually disregarded in parameter estimation tasks. How-
ever, for model selection the evidence becomes the crucial quantity
to compute. Being the integral of the likelihood over the prior (cf.
Eq. 2), the evidence allows one to assign relative weights to diï¬€er-
ent models. The evidence ratio between two competing models M1
and M2 enters the expression for the comparison of their posterior
distributions, which again follows from Bayesâ€™ theorem:

.

=

ğ‘( ğ’…|M1) ğ‘(M1)
ğ‘( ğ’…|M2) ğ‘(M2)

ğ‘(M1| ğ’…)
ğ‘(M2| ğ’…)
In many cases a priori probabilities ğ‘(M1) and ğ‘(M2) of the two
models are considered to be equal, hence the ratio of posterior dis-
tributions becomes equivalent to the evidence ratio or Bayes factor

(3)

ğµ12 =

ğ‘( ğ’…|M1)
ğ‘( ğ’…|M2)

=

ğ‘§1
ğ‘§2

.

(4)

For notational brevity, henceforth we drop the explicit conditioning
on models unless there are multiple models under consideration.

2.2 Algorithms for evidence estimation

Computing the evidence for a given model is numerically challeng-
ing due to the multi-dimensional integral in Eq. 2. Many techniques
have been proposed to tackle this challenge, such as thermodynamic
integration (e.g. Beltran et al. 2005; Gregory 2005; Bridges et al.
2006), the Savage-Dickey density ratio (e.g. Trotta 2007), methods
based on ğ‘˜-th nearest-neighbour distances in parameter space (Heav-
ens et al. 2017), nested sampling (Skilling 2006), and others (see,
e.g., Friel & Wyse 2012; Llorente et al. 2020).

Nested sampling reduces the computation of the evidence to the
evaluation of a one-dimensional integral, and as a byproduct provides
samples that can be used to compute posterior inferences, thus sup-
porting both parameter estimation and model selection. Multimodal
nested sampling, implemented in the multinest3 software (Feroz &

3 https://github.com/farhanferoz/MultiNest

Hobson 2008; Feroz et al. 2009; see also Buchner et al. 2014 for the
Python wrapper pymultinest4), has seen enormous success, with
widespread application across multiple research ï¬elds, as has the
slice sampling nested sampling algorithm implemented in the poly-
chord5 software (Handley et al. 2015a,b). Proximal nested sampling
(Cai et al. 2021) is implemented in the proxnest6 software, which
has only been released very recently but is likely to be particularly
useful for inverse imaging problems.

Measuring the model evidence is a numerical process that, if re-
peated multiple times, produces a distribution of values. Ideally,
these distributions would be narrow, and even more importantly they
should provide unbiased estimates of the model evidence. However,
this might not always be the case (Lemos et al. 2022). It is, therefore,
crucial to develop alternative ways to estimate the model evidence, so
as to perform cross-checks on the ï¬nal model selection statements.
Furthermore, as the name nested sampling suggests, sampling must
be performed in a prescribed nested manner, where samples are
drawn from the prior distribution within likelihood level-sets (iso-
contours). This close coupling between sampling strategy and com-
putation of the model evidence reduces ï¬‚exibility. In some settings
sampling may be performed by alternative means and nested sam-
pling approaches to compute the evidence may not be possible.

The recently proposed learnt harmonic mean estimator (McEwen
et al. 2021) for computation of the model evidence removes this
restriction since it only requires samples from the posterior and is
agnostic to how those samples are generated. This is its key property
that we leverage in the current work, hence we concisely review the
learnt harmonic mean estimator below.

2.3 Learnt harmonic mean estimator

The original harmonic mean estimator for computation of the
Bayesian model evidence was ï¬rst introduced by Newton & Raftery
(1994). From Bayesâ€™ theorem it follows that the reciprocal evidence
is related to the harmonic mean of the likelihood by

ğœŒ â‰¡ E ğ‘ (ğœ½ |ğ’…)

(cid:20)

(cid:21)

1
ğ‘( ğ’…|ğœ½)

âˆ«

=

dğœ½

1
ğ‘( ğ’…|ğœ½)

ğ‘(ğœ½ | ğ’…) =

1
ğ‘§

.

(5)

This relation can be used to construct an estimator of the reciprocal
evidence

Ë†ğœŒ =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘–=1

1
ğ‘( ğ’…|ğœ½ğ‘–)

,

ğœ½ğ‘– âˆ¼ ğ‘(ğœ½ | ğ’…),

(6)

using ğ‘ samples {ğœ½ğ‘– } ğ‘
of the posterior distribution ğ‘(ğœ½ | ğ’…). How-
ğ‘–=1
ever, this estimator may present very large or even diverging variance
(Neal 1994).

Gelfand & Dey (1994) proposed a modiï¬cation to the original
harmonic mean estimator, what we call the re-targeted harmonic
mean estimator, introducing a normalised target distribution ğœ‘(ğœ½) to
deï¬ne the modiï¬ed estimator

Ë†ğœŒ =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘–=1

ğœ‘(ğœ½ğ‘–)
ğ‘( ğ’…|ğœ½ğ‘–) ğ‘(ğœ½ğ‘–)

,

ğœ½ğ‘– âˆ¼ ğ‘(ğœ½ | ğ’…),

(7)

from which the original harmonic mean estimator is recovered for
ğœ‘(ğœ½) = ğ‘(ğœ½).

The original harmonic mean estimator can be interpreted as im-
portance sampling, where the importance sampling distribution is

4 https://github.com/JohannesBuchner/PyMultiNest
5 https://github.com/PolyChord/PolyChordLite
6 https://github.com/astro-informatics/proxnest

Bayesian model comparison for SBI

3

the posterior and the target distribution is the prior. It is therefore not
surprising that the original estimator suï¬€ers poor variance properties
since the prior is typically broader than the posterior, whereas im-
portance sampling requires the sampling density to be broader than
the target. By introducing a new target ğœ‘(ğœ½) this issue can be cir-
cumvented provided ğœ‘(ğœ½) is narrower than the posterior. Critically,
however, the introduced target must be a normalised probability dis-
tribution.

The question remains: how does one set the target distribution? A
variety of approaches have been considered previously, however none
have proved completely satisfactory. One approach is to consider a
multivariate Gaussian (Chib 1995); however, such a target typically
has tails that are too broad. An alternative is to consider indicator
functions (Robert & Wraith 2009; van Haasteren 2014); however, for
complicated posterior distributions these typically capture a small
region of parameter space and so are ineï¬ƒcient.

It was recognised by McEwen et al. (2021) that the optimal target

distribution is the normalised posterior

ğœ‘optimal (ğœ½) =

ğ‘( ğ’…|ğœ½) ğ‘(ğœ½)
ğ‘§

.

(8)

While this exact quantity is a priori inaccessible since it involves
knowledge of the evidence ğ‘§ â€” precisely the quantity we are at-
tempting to estimate â€” an approximation of ğœ‘(ğœ½) can be derived
from posterior samples by machine learning techniques. This is the
rationale of the learnt harmonic mean estimator of McEwen et al.
(2021). Moreover, the learnt approximation of the posterior needs
not be highly accurate; but critically it must have narrower tails
than the posterior. Strategies to learn appropriate targets with these
properties are presented in McEwen et al. (2021), setting up an op-
timisation problem to minimise the variance of the result estimator,
while ensuring it is unbiased, and with possible regularisation. The
learnt harmonic mean estimator is implemented in the harmonic7
software.

We conclude this section by highlighting that the learnt harmonic
mean estimator produces estimates of the evidence purely from sam-
ples of the posterior distribution; there is no requirement on the
speciï¬c method used for sampling, i.e. harmonic is agnostic to the
method used to generate posterior samples. As we shall see later
in this work, this is the key property of the learnt harmonic mean
estimator that allows it to be used in a variety of simulation-based
inference scenarios.

3 SIMULATION-BASED INFERENCE (SBI)

We provide a brief overview of the main algorithms used for SBI
(simulation-based inference), referring the reader to Cranmer et al.
(2020) for a more extensive review. The focus of recent SBI devel-
opments and existing literature is on parameter estimation, hence we
discuss SBI in this context. In Sec. 4 we introduce methodologies to
perform Bayesian model comparison in an SBI setting.

The original SBI methodology, based on ABC (approximate
Bayesian computation) (see, e.g., Beaumont 2019), involves sim-
ulating realisations of the observables at each of the explored points
in parameter space, and accepting or rejecting these points based on
their similarity with the observed data, within a tolerance ğœ–. This
rejection sampling method recovers an accurate representation of the
underlying density distribution in the limit ğœ– â†’ 0, at which point the
low simulation eï¬ƒciency makes computational costs infeasibly high

7 https://github.com/astro-informatics/harmonic

RASTI 000, 1â€“12 (2022)

4

A. Spurio Mancini et al.

for inference of parameter spaces with even moderate dimensional-
ity. More recently, neural density estimation techniques have been
introduced to overcome this computational limitation by increasing
simulation eï¬ƒciency. We focus the remainder of this article on neural
density estimation approaches for SBI.

In contrast to ABC, neural density estimation leverages deep neural
networks to approximate conditional probability densities and is able
to speed up inference by orders of magnitude (Papamakarios & Mur-
ray 2016). Neural density estimation involves training a conditional
density estimator ğ‘ğ“, parameterised by weights ğ“, to approximate
a target distribution (either the posterior distribution, the likelihood
function or a density ratio proportional to the likelihood) from a
training set of ğ‘ pairs of (typically) prior samples and simulations
{ğœ½ğ‘–, ğ’…ğ‘– } ğ‘
. Provided the density estimator is suï¬ƒciently expressive,
1=1
ğ‘ğ“ will recover an accurate estimate of the target distribution in the
limit ğ‘ â†’ âˆ.

Neural density estimation workï¬‚ows have three main phases: sim-
ulation, training and inference. The simulation phase generates the
training pairs {ğœ½ğ‘–, ğ’…ğ‘– } that are used in the training phase to tune the
weights of the neural network ğ“ such that ğ‘ğ“ approximates the target
density. In the inference phase, ğ‘ğ“ is then conditioned on a speciï¬c
observation ğ’…0 and parameter inference is performed.

Single runs of the simulation and training phases amortises the
training of the density estimator, allowing oï¬„ine inference to be run
on multiple diï¬€erent observations, aptly named amortised neural
density estimation. However, we are often interested in inference on a
speciï¬c observation ğ’…0. For this, amortised neural density estimation
tends to be ineï¬ƒcient as generating training pairs for the density
estimator across the entire prior parameter support includes many
points in parameter space with very low posterior density ğ‘(ğœ½ | ğ’…0).
To rectify this simulation ineï¬ƒciency one can run sequential neu-
ral density estimation, where multiple rounds of simulation and train-
ing are run sequentially to ensure there is a greater focus on regions
of high posterior density. This is done by generating simulations
from an alternative prior proposal distribution Ëœğ‘(ğœ½). This proposal
distribution is iteratively updated between rounds such that for ğ‘…
rounds, the proposal posterior of the ğ‘–-th round Ëœğ‘ğ‘– (ğœ½ | ğ’…0) becomes
the proposal prior for the subsequent round Ëœğ‘ğ‘–+1 (ğœ½). This sequen-
tial approach can further increase simulation eï¬ƒciency by orders
of magnitude compared to the amortised counterpart (Papamakarios
& Murray 2016), at the expense of forgoing observation-agnostic
ï¬‚exibility.

We brieï¬‚y review the three main approaches to neural density
estimation (Cranmer et al. 2020). When referring to variants of these
implementations we follow the nomenclature of Durkan et al. (2020).

3.1 Neural posterior estimation (NPE)

Neural posterior estimation (NPE) was ï¬rst introduced by Papa-
makarios & Murray (2016) and involves training a conditional den-
sity estimator to approximate the posterior density, such that
ğ‘ğ“ (ğœ½ | ğ’…) â†’ ğ‘(ğœ½ | ğ’…), by minimising the loss function

L (ğ“) = E ğ‘ (ğ’… |ğœ½) ğ‘ (ğœ½)

(cid:2)âˆ’log ğ‘ğ“ (ğœ½ | ğ’…)(cid:3) .

(9)

For sequential neural posterior estimation, iteratively updating the
proposal distribution between inference rounds results in the density
estimator learning a proposal posterior density Ëœğ‘(ğœ½ | ğ’…) that is related
to the true posterior density by

Three variants of NPE have been introduced to recover the true
posterior from the proposal posterior.

The original neural posterior estimation method (NPE-A; Papa-
makarios & Murray 2016) trains a mixture density network to target
the posterior distribution. A post-hoc analytical correction is then
applied to the resulting proposal posterior to recover an approxima-
tion of the true posterior (cf. Eq. 10). NPE-A considers Gaussian or
Gaussian mixture proposal distributions so that the correction factor
can be computed analytically.

To circumvent the requirement for analytical computation, Lueck-
mann et al. (2018) propose a method (NPE-B) where the proposal
correction is embedded as an importance weight in the loss function.
Whilst more ï¬‚exible than NPE-A, this method has been shown to
suï¬€er poor performance as the importance weights in the loss func-
tion are susceptible to high variance, resulting in early termination
of training.

Finally, Greenberg et al. (2019) propose a neural posterior estima-
tion method (NPE-C) that reparameterises the problem to recover a
learnt approximation ğ‘ğ“ (ğœ½ | ğ’…) of the true posterior from a density
estimator Ëœğ‘ğ“ (ğœ½ | ğ’…) of the proposal posterior using a tractable sum of
discrete atomic proposals over the support of the true posterior. This
latter approach allows more ï¬‚exibility in the choice of density estima-
tor, including cutting-edge normalising ï¬‚ow models (Papamakarios
et al. 2017; Durkan et al. 2019). In our subsequent experiments we
only consider this neural posterior estimation method, which we sim-
ply refer to as NPE for the remainder of this article.

NPE learns the posterior density directly, typically for a probabilis-
tic model from which samples can be drawn directly. For example,
samples can be drawn directly from a Gaussian mixture density net-
work or from a normalising ï¬‚ow, where for the latter samples are ï¬rst
drawn from a simple base distribution such as a Gaussian and pushed
forward through the ï¬‚ow to yield samples of the target distribution.
Consequently, generating samples avoids the need for Markov chain
Monte Carlo (MCMC) sampling and can be performed rapidly and
in parallel, signiï¬cantly reducing computation time for inference.

3.2 Neural likelihood estimation (NLE)

Neural likelihood estimation (NLE) was ï¬rst introduced by Papa-
makarios et al. (2019) and involves training a conditional density
estimator to approximate the likelihood function (considering it as a
probability distribution over the data), such that ğ‘ğ“ ( ğ’…|ğœ½) â†’ ğ‘( ğ’…|ğœ½),
by minimising the loss function

L (ğ“) = E ğ‘ (ğ’… |ğœ½) ğ‘ (ğœ½)

(cid:2)âˆ’log ğ‘ğ“ ( ğ’…|ğœ½)(cid:3) .

(11)

In contrast to NPE, sequential NLE can be implemented without a
correction between Ëœğ‘ğ“ ( ğ’…|ğœ½) and ğ‘ğ“ ( ğ’…|ğœ½). In principle, simulations
can be generated for any proposal distribution and, consequently,
simulations from all sequential rounds, not just the latest, can be
used when training (Papamakarios et al. 2019).

This ability to seamlessly optimise simulation eï¬ƒciency, how-
ever, comes at the expense of requiring an external MCMC sampling
stage to generate samples from the surrogate posterior ğ‘ğ“ ( ğ’…|ğœ½) ğ‘(ğœ½)
for inference, which increases inference time and computational cost
signiï¬cantly relative to NPE, where samples can be generated di-
rectly.

3.3 Neural ratio estimation (NRE)

Ëœğ‘(ğœ½ | ğ’…) âˆ

Ëœğ‘(ğœ½)
ğ‘(ğœ½)

ğ‘(ğœ½ | ğ’…).

RASTI 000, 1â€“12 (2022)

(10)

Neural ratio estimation (NRE) was ï¬rst introduced by Hermans et al.
(2019) and involves approximating the posterior density ğ‘(ğœ½ | ğ’…) in-
directly by learning a density ratio ğ‘Ÿğ“ ( ğ’…, ğœ½) that is proportional to

Bayesian model comparison for SBI

5

(a) Neural posterior estimation (NPE)

(b) Neural likelihood estimation (NLE)

(c) Neural ratio estimation (NRE)

Figure 1. Schematic overview of three novel techniques that we introduce to compute the evidence in SBI settings for neural posterior, likelihood and ratio
estimation methods (NPE, NLE, NRE, respectively). The yellow training blocks represent all phases of neural density estimation, where each block can be run
in an amortised or sequential setting.

the likelihood, where ğ“ denotes the model weights. This is done by
training a binary classiï¬er to discriminate samples drawn from the
joint and marginal distributions of training pairs. The classiï¬er then
learns the ratio

ğ‘Ÿğ“ ( ğ’…, ğœ½) =

ğ‘( ğ’…, ğœ½)
ğ‘( ğ’…) ğ‘(ğœ½)

=

ğ‘( ğ’…|ğœ½)
ğ‘( ğ’…)

=

ğ‘(ğœ½ | ğ’…)
ğ‘(ğœ½)

.

(12)

A further NRE variant was devised by Durkan et al. (2020), which
we adopt for our numerical experiments and simply refer to as NRE
for the remainder of this paper.

Similarly to NLE, an additional MCMC sampling step is required
for NRE in order to generate samples from the surrogate posterior
ğ‘Ÿğ“ ( ğ’…, ğœ½) ğ‘(ğœ½). As with NLE, this increases inference time and com-
putation cost signiï¬cantly relative to NPE.

4 BAYESIAN MODEL COMPARISON FOR SBI

We discussed the importance and challenge of computing the model
evidence for Bayesian model selection in Sec. 2, which is a funda-
mental component of many scientiï¬c analyses. Separately in Sec. 3
we discussed three recent neural density estimation techniques for
parameter estimation in an SBI (simulation-based inference) setting,
which oï¬€er great promise for scientiï¬c analyses where the likelihood
is often intractable or too costly to be evaluated. In this section we
unify these two topics by introducing a methodology to compute the
Bayesian model evidence in SBI scenarios.

We do so by presenting three novel techniques to compute the ev-
idence for neural posterior, likelihood, and ratio estimation methods
(NPE, NLE, NRE, respectively). In particular, we leverage the re-
cently proposed learnt harmonic mean estimator and exploit the fact
that it is decoupled from the method used to generate posterior sam-
ples, i.e. it requires posterior samples only, which may be generated
by any approach. This ï¬‚exibility, which is lacking in many alterna-
tive methods for computing the model evidence, allows us to develop
SBI model comparison techniques for all of the three neural density
estimation approaches. The evidence computation technique corre-
sponding to each neural density estimation approach is represented
schematically in Fig. 1. Our approaches support density estimation
training phases run in either an amortised or sequential setting.

4.1 Neural posterior estimation (NPE)

ğ

direct
âˆ¼ ğ‘NPE
ğ

Our approach to compute the evidence for NPE is shown schemati-
cally in Fig. 1(a). We use NPE to learn an approximation ğ‘NPE
(ğœ½ | ğ’…)
of the posterior, parameterised by network weights ğ. This approach
provides the ability to rapidly generate samples directly from the sur-
(ğœ½ | ğ’…). While NPE also provides
rogate posterior, i.e. ğœ½ğ‘–
the ability to evaluate the surrogate normalised posterior, the normal-
isation constant itself, i.e. the model evidence, is not accessible. To
compute the model evidence we therefore adopt the learnt harmonic
mean estimator, using the samples drawn directly from the surrogate
posterior. For the learnt harmonic mean estimator it is also necessary
to evaluate the likelihood at sample positions (see Eq. 7), hence we
adopt NLE to provide a surrogate likelihood. Using NLE we learn an
approximation ğ‘NLE
( ğ’…|ğœ½) of the likelihood, parameterised by a sep-
arate set of network weights ğ“. With a set of posterior samples and
the surrogate likelihood learned by NLE to hand, we use the learnt
harmonic mean estimator to obtain an estimate of the reciprocal of
the model evidence by

ğ“

Ë†ğœŒ =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘–=1

ğ‘NLE
ğ“

ğœ‘(ğœ½ğ‘–)
( ğ’…|ğœ½ğ‘–) ğ‘(ğœ½ğ‘–)

,

ğœ½ğ‘–

direct
âˆ¼ ğ‘NPE
ğ

(ğœ½ | ğ’…).

(13)

The proposed approach to compute the model evidence in the
NPE setting does involve training two neural density estimators, both
NPE and NLE. However, it does not require any MCMC sampling.
Samples can be generated directly from the surrogate posterior learnt
by NPE (e.g. by pushing samples from a simple base distribution
such as a Gaussian through a normalising ï¬‚ow), which is highly
eï¬ƒcient and can also be computed in parallel. This is only possible
due to the ï¬‚exibility of the learnt harmonic mean estimator, which as
commented already is agnostic to sampling strategy, requiring only
posterior samples that can be generated by any technique.

Given trained NPE and NLE surrogate densities, an alternative
naÃ¯ve technique can also be considered to estimate the model ev-
idence. For any model parameter ğœ½ the ratio of the unnormalised
surrogate posterior, formed from the surrogate likelihood and prior,

RASTI 000, 1â€“12 (2022)

Evaluate approx.likelihood at samplesTrainNPE Direct samplingCompute evidenceusing HARMONICTrain  NLETrain  NLEMCMC samplingCompute evidenceusing HARMONICEvaluate approx.likelihood at samplesTrain  NRETrain  NLEMCMC samplingCompute evidenceusing HARMONIC6

A. Spurio Mancini et al.

to the normalised surrogate posterior, i.e.
ğ‘NLE
ğ“

( ğ’…|ğœ½) ğ‘(ğœ½)

,

ğ‘NPE
ğ

(ğœ½ | ğ’…)

(14)

decidedly more eï¬ƒcient than training two such estimators as re-
quired in the NPE and NRE settings (cf. Sec. 4.1 and Sec. 4.3).
However, it does require MCMC sampling to generate samples from
the unnormalised surrogate posterior which is required to compute
the evidence using the learnt harmonic mean estimator.

provides an estimate of the evidence. An estimate of the evidence is
thus recovered for a single parameter ğœ½, which need not be drawn
from any particular distribution. However, such an estimate of the
evidence will be incredibly noisy, i.e. will have an extremely large
variance. Many parameters can be used to generate many estimates of
the evidence that can be averaged. Nevertheless, the resulting estimate
of the evidence remains highly noisy with a very large variance. This
naÃ¯ve estimator relies on the ratio of two approximate quantities,
hence approximation errors compound. Contrast this with the learnt
harmonic mean estimator. While our learnt harmonic mean approach
does use NPE to learn a surrogate posterior ğ‘NPE
(ğœ½ | ğ’…), the density
is never evaluated. We only require samples from the corresponding
distribution. The learnt harmonic mean does require learning the
importance target ğœ‘(ğœ½), and this is indeed learnt to approximate the
posterior, but the target need only be normalised and have tighter
tails than the true posterior ğ‘(ğœ½ | ğ’…) â€” it does not need to be an
accurate approximation of the posterior. Consequently, our proposed
approach to compute the evidence in the NPE setting, based on
the learnt harmonic mean estimator, does not suï¬€er compounding
sources of error and thus provides increased stability over the naÃ¯ve
approach.

ğ

While the focus of the current article is SBI, we also comment that
the ideas presented here can also be applied to accelerate evidence
computation for likelihood-based inference. Crucially, throughout
our approach to compute the evidence in the NPE setting, MCMC
sampling is not required. Posterior samples can be generated directly,
rapidly and in parallel. If a likelihood is available this can simply be
substituted for the surrogate likelihood learnt by NLE. Therefore in
the likelihood-based setting the approach can be altered to leverage
the speed of posterior sample generation of NPE, while adopting
the analytical likelihood function, to obtain a rapid estimate of the
reciprocal evidence without any further computation, as described
by

Ë†ğœŒ =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘–=1

ğœ‘(ğœ½ğ‘–)
ğ‘( ğ’…|ğœ½ğ‘–) ğ‘(ğœ½ğ‘–)

,

ğœ½ğ‘–

direct
âˆ¼ ğ‘NPE
ğ

(ğœ½ | ğ’…).

(15)

Clearly in this setting NLE need not be performed.

4.2 Neural likelihood estimation (NLE)

ğ“

Our approach to compute the evidence for NLE is shown schemat-
ically in Fig. 1(b). We use NLE to learn an approximation of the
likelihood function ğ‘NLE
( ğ’…|ğœ½), parameterised by network weights
ğ“. As is typical for NLE, this approach requires MCMC sampling
to generate samples from the unnormalised surrogate posterior, i.e.
( ğ’…|ğœ½) ğ‘(ğœ½). NLE also provides the ability to eval-
ğœ½ğ‘–
uate the surrogate likelihood. With a set of posterior samples and
the surrogate likelihood learned by NLE to hand, we use the learnt
harmonic mean estimator to compute an estimate of the reciprocal
of the model evidence by

MCMC
âˆ¼

ğ‘NLE
ğ“

Ë†ğœŒ =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘–=1

ğ‘NLE
ğ“

ğœ‘(ğœ½ğ‘–)
( ğ’…|ğœ½ğ‘–) ğ‘(ğœ½ğ‘–)

,

ğœ½ğ‘–

MCMC

âˆ¼ ğ‘NLE
ğ“

( ğ’…|ğœ½) ğ‘(ğœ½).

(16)

This proposed approach to compute the model evidence in the NLE
setting involves training only one neural density estimator, which is

RASTI 000, 1â€“12 (2022)

ğ“

With a trained NLE surrogate density ğ‘NLE

( ğ’…|ğœ½ğ‘–) and the prior
ğ‘(ğœ½) to hand, alternative techniques that only require the likelihood
function and prior could also be considered to compute an estimate
of the evidence. We adopt the learnt harmonic mean estimator since
it decouples evidence estimation from MCMC sampling strategy to
provide greater ï¬‚exibility and has also been shown to be accurate
and robust.

4.3 Neural ratio estimation (NRE)

ğ

MCMC

âˆ¼ ğ‘ŸNRE
ğ

Our approach to compute the evidence for NRE is shown schemat-
ically in Fig. 1(c). We use NRE to indirectly learn an approxima-
tion ğ‘ŸNRE
( ğ’…, ğœ½) ğ‘(ğœ½) of the posterior, parameterised by network
weights ğ. As is typical for NRE, this approach does not provide
the ability for direct sampling and thus requires MCMC sampling
to generate samples from this surrogate normalised posterior, i.e.
( ğ’…, ğœ½) ğ‘(ğœ½). Similarly to our NPE approach, the nor-
ğœ½ğ‘–
malisation constant of this surrogate posterior learned by NRE, i.e.
the model evidence, is not accessible. We therefore adopt the learnt
harmonic mean estimator to compute the model evidence, for which
it is also necessary to evaluate the likelihood at sample positions (see
Eq. 7), hence we adopt NLE to provide a surrogate likelihood. Using
NLE we learn an approximation ğ‘NLE
( ğ’…|ğœ½) of the likelihood, param-
ğ“
eterised by network weights ğ“. With a set of posterior samples and
the surrogate likelihood learned by NLE to hand, we use the learnt
harmonic mean estimator to obtain an estimate of the reciprocal of
the model evidence by

Ë†ğœŒ =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘–=1

ğ‘NLE
ğ“

ğœ‘(ğœ½ğ‘–)
( ğ’…|ğœ½ğ‘–) ğ‘(ğœ½ğ‘–)

,

ğœ½ğ‘–

MCMC

âˆ¼ ğ‘ŸNRE
ğ

( ğ’…, ğœ½) ğ‘(ğœ½).

(17)

The proposed approach to compute the model evidence in the
NRE setting does involve training two neural density estimators, both
NRE and NLE. Furthermore, external MCMC sampling is required
to generate samples from the trained NRE surrogate posterior.

An alternative way to compute the Bayes factor ğµ12 between two
competing models M1 and M2, i.e. the ratio of model evidences
Eq. 4, in the NRE setting is to train an additional NRE model as a
binary classiï¬er to discriminate samples from the joint and marginal
distribution of the two models, respectively. The classiï¬er then learns
the ratio

ğ‘Ÿğ12 ( ğ’…, ğœ½) =

ğ‘( ğ’…, ğœ½ |M1)
ğ‘( ğ’…|M2) ğ‘(ğœ½ |M2)

,

(18)

where ğ12 denotes the network weights for a model trained in such
a manner. Following similar notation, the standard neural ratio for a
. While it is not
single model, say model M1, can be denoted ğ‘Ÿğ11
possible to estimate the evidence of a single model directly, the Bayes
factor comparing the two models, which is the critical quantity for
model comparison, can then be recovered by

ğµ12 =

ğ‘Ÿğ12 ( ğ’…, ğœ½)
ğ‘Ÿğ11 ( ğ’…, ğœ½)

ğ‘(ğœ½ |M2)
ğ‘(ğœ½ |M1)

.

(19)

We understand this method is already known to the SBI community
but were not able to locate any references discussing or applying it.
Since this approach does not lie within the family of methodologies
introduced in the current article, which leverage the learnt harmonic

Bayesian model comparison for SBI

7

Figure 2. Model evidence values estimated with diï¬€erent likelihood-based and simulation-based (likelihood-free) methods for the linear Gaussian example
described in Sec. 5.1, whose analytical truth value is shown in red. For all methods we repeat the evidence estimation exercise 25 times to empirically describe
the statistical distributions of the model evidence estimates, shown by the blue areas in each â€˜violinâ€™. The mean and one standard deviation error bars are
illustrated in purple. The pink section of this plot shows likelihood-based results obtained with multinest, polychord and harmonic (the latter using samples
from emcee). The light brown section of the plot shows results for the simulation-based evidence pipelines summarised in Fig. 1. These are all based on the
use of harmonic to derive evidence estimates from posterior samples obtained with neural posterior estimation (NPE), neural likelihood estimation (NLE) and
neural ratio estimation (NRE), in their amortised and sequential variants.

mean estimate to compute the model evidence from samples of the
surrogate posterior distribution, we leave the analysis of this approach
to further work.

5 NUMERICAL EXPERIMENTS

Here we present the results from our numerical experiments that
demonstrate and validate our SBI evidence calculation techniques.
For validation purposes, for each problem, we compare the value of
the evidence computed by our proposed approach (which we stress
does not include any knowledge of the likelihood) to values com-
puted by likelihood-based approaches, either derived analytically
(when possible) and/or computed numerical by likelihood-based
algorithms (e.g. by harmonic, multinest and/or polychord). Of
course in practical SBI settings, likelihood-based alternatives will
not typically be available. Nevertheless, it is useful to consider prob-
lems here where the likelihood is available so that we can validate
our SBI evidence computation techniques against likelihood-based
alternatives. All of the SBI examples were implemented using the
SBI8 software (Tejero-Cantero et al. 2020).

5.1 Linear Gaussian

The ï¬rst problem we consider is that of a simple simulator which
linearly adds Gaussian noise ğœ–ğ‘– to the value of the parameters ğœƒğ‘–, for

8 https://github.com/mackelab/sbi

ğ‘– = 1, 2, 3:

dğ‘– = ğœƒğ‘– + ğœ–ğ‘–,

ğœ–ğ‘– âˆ¼ N (0, 1).

(20)

This is a standard test problem in the SBI software. The Gaussian
noise has zero mean and unit variance, and for the model parameters
we assume a uniform prior ğœƒğ‘– âˆ¼ U [âˆ’2, 2] for each component
ğ‘– = 1, 2, 3. The likelihood for this model is Gaussian in the parameters
ğœ½ = {ğœƒ1, ğœƒ2, ğœƒ3}:

ğ‘( ğ’…|ğœ½) =

1
(2ğœ‹)3/2

exp

(cid:18)

âˆ’

(cid:0)ğ’… âˆ’ ğœ½(cid:1)2
2

(cid:19)

.

(21)

We assume an observation ğ’…0 = (0, 0, 0). For this model the Bayesian
evidence can be computed analytically:

ğ‘§ =

=

1
64 (2ğœ‹)3/2
âˆš
2(cid:1) (cid:3) 3
(cid:2)erf(cid:0)

64

âˆ« 2

âˆ’2

dğœƒ1

âˆ« 2

âˆ’2

dğœƒ2

âˆ« 2

âˆ’2

dğœƒ3 exp

(cid:18)

âˆ’

2 + ğœƒ2
1 + ğœƒ2
ğœƒ2
3
2

(cid:19)

â‰ˆ 0.01359.

(22)

Fig. 2 summarises the results obtained from our model evidence
estimates for the linear Gaussian problem. The pink background
section shows results for likelihood-based methods for validation
purposes, while results for SBI (likelihood-free) methods are shown
in the light brown region. For all methods we repeat the evidence
estimation exercise 25 times to empirically describe the statistical
distributions of the model evidence estimates, shown by the blue
areas in each â€˜violinâ€™ of Fig. 2. All of the evidence values reported in
Fig. 2 can be compared with the analytical value of Eq. 22, overplotted
in red.

RASTI 000, 1â€“12 (2022)

MultiNestPolychordHarmonicNPEAmortisedNPESequentialNLEAmortisedNLESequentialNREAmortisedNRESequential0.01200.01250.01300.01350.01400.0145EvidenceLikelihood-BasedLikelihood-FreeGround Truth8

A. Spurio Mancini et al.

Figure 3. Model evidence values estimated with diï¬€erent likelihood-based and simulation-based (likelihood-free) methods for the Radiata Pine example
described in Sec. 5.2, whose analytical truth value is shown in red. Colour codes and labels are consistent with Fig. 2.

Likelihood-based approaches include: (a) multinest, which pro-
duces samples and evidence estimates, run with importance sampling
(Feroz et al. 2019), 1000 live points, eï¬ƒciency sampling of 0.3 and
evidence tolerance of 0.01; (b) polychord, which also produces
samples as well as evidence estimates, run using 1000 live points; (c)
harmonic, which produces evidence estimates from posterior sam-
ples, thus we adopt the aï¬ƒne sampler of Goodman & Weare (2010),
implemented in the emcee9 software (Foreman-Mackey et al. 2013),
to generate 105 post burn-in posterior samples from 100 random
walkers and adopt a hypersphere model for the learnt importance tar-
get, with radius equal to the square root of the number of parameters.
All of the three likelihood-based methods provide unbiased av-
erage estimates of the evidence. Notice that for this example the
evidence estimates computed by harmonic have a standard devia-
tion approximately four times smaller than that of multinest and
polychord.

The results for the SBI methods are shown in the light brown sec-
tion of the plot. We report results for NPE, NLE and NRE, and for
each of them we provide an estimate using the amortised as well as
the sequential approach. For NPE and NRE we use 105 simulations
in the amortised approach, while in the sequential one we use 10
rounds with 104 simulations each, thus totalling the same number of
simulations for the two approaches. NLE requires increased compu-
tational cost at training time, therefore for this method we reduce the
number of simulations by an order of magnitude in both amortised
and sequential approaches, following Hermans et al. (2019). For each
SBI method after training a density estimator (in an amortised or se-
quential fashion) we collect a total of 105 posterior samples, either
directly for NPE or by MCMC sampling using 100 emcee random
walkers for NLE and NRE. We train the harmonic importance target
model using 20% of the samples and use the remaining 80% to com-

9 https://github.com/dfm/emcee

RASTI 000, 1â€“12 (2022)

pute the evidence. As explained in Sec. 4.1 and Sec. 4.3, NPE and
NRE require an additional training of an NLE estimator to provide a
surrogate likelihood.

All of the SBI evidence computation techniques provide accurate
estimates of the evidence, with the distribution range capturing the
true analytic evidence. The variances of the SBI estimates are gen-
erally larger than the likelihood-based approaches, which is to be
expected since in contrast to the likelihood-based setting we do not
include any knowledge of the likelihood. We observe that performing
inference using the sequential approach can result in an improvement
in the estimate, but the diï¬€erence is not always signiï¬cant. The NPE
and NRE approaches exhibit some bias, which is nevertheless within
the spread of the distribution of values. The NLE estimates show
excellent agreement with the reference values, despite using fewer
simulations in training. This may be due to the fact that the NLE ap-
proach requires only a single neural density estimator (whereas the
NPE and NRE approaches require two), resulting in fewer sources of
approximation error.

5.2 Radiata pine

The second problem we considered is one of the classic benchmark
examples used to evaluate techniques to estimate the model evidence
(Friel & Wyse 2012; Enderlein 1961). We refer to McEwen et al.
(2021), who demonstrated the eï¬€ectiveness of the learnt harmonic
mean estimator for this example, for a more detailed presentation of
the problem: here we simply report the main points relevant to our
evidence estimation task.

Our dataset is comprised of measurements of Radiata pine trees
of the maximum compression strength parallel to the grain ğ‘¦ğ‘–, for
ğ‘– = 1 . . . 42. The original scientiï¬c problem can be stated in terms
of two models: in Model 1 the density ğ‘¥ğ‘– is assumed as a predictor
for ğ‘¦ğ‘–, while in Model 2 the predictor is assumed to be the resin-
adjusted density ğ‘§ğ‘–. Both predictors are modelled with a Gaussian

Bayesian model comparison for SBI

9

Figure 4. Logarithm of the Bayes factors between source and alternative waveform models estimated with diï¬€erent likelihood-based and simulation-based
(likelihood-free) methods for the gravitational wave example described in Sec. 5.3. Shown in red, an estimate of the true value of the logarithm of the Bayes
factor was obtained using numerical integration. Colour codes and labels are consistent with Fig. 2 and Fig. 3.

linear regression model, for which the value of the evidence can be
derived analytically for each model. For brevity, we report results
only for Model 1; calculations for Model 2 are identical (we did also
experiment with this second model, ï¬nding excellent agreement with
the analytical estimates of the evidence). In Model 1, denoting with
Â¯ğ‘¥ = 1
ğ‘–=1 ğ‘¥ğ‘– the average density across the trees specimens, the
ğ‘›
maximum compression strength ğ‘¦ğ‘– is given by

(cid:205)42

ğ‘¦ğ‘– = ğ›¼ + ğ›½(ğ‘¥ğ‘– âˆ’ Â¯ğ‘¥) + ğœ–ğ‘–,

ğœ–ğ‘– âˆ¼ N (0, ğœâˆ’1).

(23)

The model parameters are {ğ›¼, ğ›½, ğœ}, whose prior distributions are
ğ›¼ âˆ¼ N (ğœ‡ ğ›¼, (ğ‘Ÿ0ğœ)âˆ’1), ğ›½ âˆ¼ N (ğœ‡ğ›½, (ğ‘ 0ğœ)âˆ’1), ğœ âˆ¼ Ga(ğ‘0, ğ‘0),
with (ğœ‡ ğ›¼, ğœ‡ğ›½, ğ‘Ÿ0, ğ‘ 0, ğ‘0, ğ‘0) = (3000, 185, 0.06, 6, 3, 2 Ã— 3002). The
evidence for this model can be computed analytically (cf. McEwen
et al. 2021, Eq. 104); the numerical value of its logarithm is log ğ‘§ =
âˆ’310.12829.

(24)

Fig. 3 summarises our ï¬ndings for the Radiata pine example;
the colour codes are the same as in Fig. 2. We repeat the same
experiments as in the linear Gaussian example, except this time for
simplicity we do not attempt to calculate the evidence with multinest
or polychord (as this would require some eï¬€ort to adapt the prior
function for the Radiata pine model to be compatible with the uniform
distribution on the unit cube required by these nested samplers).
Therefore, for the likelihood-based case we report only numerical
results obtained by applying harmonic to emcee samples, using
a kernel density estimate for the learnt harmonic mean estimator
importance target, with radius 0.02 of the target distribution. As in
the linear Gaussian example, we use 20% of 105 emcee posterior
samples from 100 random walkers to train harmonic, and compute
an estimate of the evidence with the remaining 80%. As we can see in
the pink section of the plot, this provides unbiased and tight estimates
of the evidence.

results for SBI methods. The number of simulations we use to train the
density estimators in the various methods is the same as in the linear
Gaussian example, as is the number of posterior samples used to train
harmonic and derive evidence estimates. All of the SBI evidence
pipelines provide reasonably accurate estimates of the evidence, with
distribution ranges capturing the true analytic evidence. Similar to
the previous linear Gaussian example, we observe that performing
inference using the sequential approach can result in an improvement
in the estimate, but the diï¬€erence is not always signiï¬cant. The NPE
and NRE approaches again exhibit some bias, which is nevertheless
within the spread of the distribution of values. The NLE estimates
again show excellent agreement with the reference values.

5.3 Gravitational waves

The ï¬nal problem we consider is a simulated measurement of a
gravitational wave (GW) signal from a single interferometer. We
consider a merger between two black holes of mass ğ‘€1 = ğ‘€2 =
20ğ‘€(cid:12), following a similar numerical setup to the one considered
by Jeï¬€rey & Wandelt (2020). We compute the noiseless time series
of the strain signal using the pycbc10 software (Biwer et al. 2019).
We only consider the â€˜+â€™ polarization of the detector strain â„+,Ã—.
The duration of the signal is âˆ¼ 0.12s, sampled at steps of duration
âˆ¼ 488ğœ‡s each. We rescale the signal by a multiplicative factor 1021
in order to work with values O (1). For each point in parameter space,
our simulated observable is a noisy gravitational waveform obtained
by adding Gaussian noise with zero mean and standard deviation
ğœ = 0.3 to the noiseless template from pycbc.

For this inference problem we vary the two black hole masses
ğ‘€1, ğ‘€2, each one over a uniform prior U [10, 30] ğ‘€(cid:12). We do not

In the light brown background section of Fig. 3 we can compare

10 https://github.com/gwastro/pycbc

RASTI 000, 1â€“12 (2022)

10

A. Spurio Mancini et al.

consider geometrical properties of the black holes such as spin or
inclination angle, similarly to Jeï¬€rey & Wandelt (2020). As noted
by Hermans et al. (2021), who studied a similar GW SBI problem,
such a simulated experimental setup requires signiï¬cant computa-
tional demand. Hence, for this example we run only 10 repetitions
of each inference method to empirically describe the statistical dis-
tribution of evidence estimates. These estimates mimic a realistic
scenario within GW data analysis pipelines comparing model evi-
dences for two diï¬€erent numerical approximant models assumed in
the generation of the noiseless template waveforms. The ï¬rst of these
two waveform models corresponds to the actual one used to generate
the simulated observation, a reduced-order eï¬€ective-one-body model
(SEOBNR, Taracchini et al. 2014), which we refer as the source model.
The second model we consider is an inspiral-merger-ringdown phe-
nomenological model (IMRPhenom, Hannam et al. 2014), which we
refer to as the alternative model. For this conï¬guration, we expect
to ï¬nd the Bayes factors comparing models to favour the source
SEOBNR model. We verify this numerically, taking advantage of this
problemâ€™s deliberately low-dimensional parameter space, which al-
lows us to compute an estimate of the evidence for each model using
direct numerical integration. We ï¬nd the logarithm of the Bayes fac-
tor computed by direct numerical integration to be âˆ¼ 3.25, favouring
the source model as anticipated.

We run the inference pipeline in multiple likelihood-based con-
texts, namely (a) obtaining samples and evidence estimates with
polychord, using the same conï¬guration for this method as the one
described in Sec. 5.1; (b) sampling the parameter space with emcee,
collecting the same number of posterior samples as in both previous
numerical examples, and using harmonic to obtain an estimate of
the evidence, with a kernel density estimate for the importance target
distribution of radius 0.002 and 0.02 for the source and alternative
models, respectively. Numerical results from these likelihood-based
methods are reported in the pink background section of Fig. 4, where
we show violin plots for log-Bayes factors computed with polychord
and harmonic. The distribution of log-Bayes factors always favours
the true underlying source model and, as in previous examples, we
ï¬nd strong agreement between each likelihood-based approach.

Numerical results from the SBI evidence estimates are shown in
the light brown background section of Fig. 4. The evidence pipelines
we consider are the same as described in Sec. 4 and for each pipeline,
the number of simulations used to train the density estimators are the
same as those used for the linear Gaussian and Radiata pine ex-
amples in Sec. 5.1 and Sec. 5.2, for both amortised and sequential
approaches. We also use the same number of posterior samples to
train harmonic and derive estimates of the evidence. We notice that
all SBI methods produce log-Bayes factor estimates in good agree-
ment with the likelihood-based ones, albeit with a comparatively
larger variance than presented in Fig. 2 and Fig. 3, due to compound-
ing errors from the multiple evidence estimates required to obtain
Bayes factors (cf. Eq. 4). NLE produces more unbiased estimates of
the log-Bayes factors compared to NPE and NRE â€” a similar trend
to that observed in the linear Gaussian and Radiata pine examples
(cf. Fig 2 and Fig. 3).

6 CONCLUSIONS

In this article we introduce methodology to compute the model
evidence for modern neural density estimation approaches to SBI
(simulation-based inference). This approach leverages the learnt har-
monic mean estimator introduced in McEwen et al. (2021); in par-
ticular, its property that it is decoupled from the sampling strategy

RASTI 000, 1â€“12 (2022)

and it only requires samples of the posterior. Exploiting this prop-
erty we develop SBI model comparison techniques for all three main
neural density estimation approaches: NPE (neural posterior esti-
mation), NLE (neural likelihood estimation), and NRE (neural ratio
estimation). Generating samples from the surrogate posterior can be
performed directly for the NPE approach, which can be computed
rapidly and in parallel, whereas MCMC (Markov chain Monte Carlo)
sampling is required for the NLE and NPE approaches.

We demonstrate and validate our SBI evidence calculation tech-
niques on a range of inference problems, using the learnt harmonic
mean estimator as implemented in the harmonic software. The ex-
amples considered include two simple problems for which the analyt-
ical value of the evidence is known, and a more realistic application
to a simulated gravitational wave analysis. For all of these examples,
in addition to using harmonic to provide estimates of the evidence
in an SBI setting, we further test the eï¬€ectiveness of the learnt har-
monic mean estimator in the corresponding likelihood-based setting.
This way, we show that harmonic can be reliably used in both
likelihood-based and simulation-based (likelihood-free) contexts to
provide precise and accurate estimates of the Bayesian evidence.

For likelihood-based approaches we compare the analytical value
of the evidence against estimates obtained with the nested sampling
algorithms multinest and polychord, as well as against estimates
from harmonic (with posterior samples obtained with the aï¬ƒne
sampler emcee). We ï¬nd that harmonic always produces values of
the evidence that are in excellent agreement with the analytical values
and with those computed by multinest and polychord.

Nested sampling algorithms have become the standard way to
compute estimates of the evidence in many scientiï¬c ï¬elds, includ-
ing astrophysics and cosmology. Our results clearly corroborate the
ï¬ndings of McEwen et al. (2021), where the learnt harmonic mean
was introduced and its eï¬€ectiveness demonstrated, and further high-
light the potential of this method to be used as an alternative to nested
sampling for evidence estimation. This is particularly important for
model comparison tasks in cosmological analyses from future sur-
veys, where the ability to cross-check the quoted evidence estimates
is of the utmost importance to ensure robust scientiï¬c statements.

Given the agnostic nature of the learnt harmonic mean estimator
with respect to the method used to obtain samples of the posterior
distribution, it can also be leveraged to compute the model evidence
in a variety of simulation-based (likelihood-free) contexts. In our
experiments we compare the diï¬€erent SBI evidence computation
approaches that we propose, which rely on learning approximations
of either the posterior (NPE), the likelihood (NLE) or the likelihood
ratio (NRE).

We validate all SBI evidence estimator approaches, computing us-
ing harmonic, to those computed by likelihood-based alternatives
(we stress that in a practical SBI setting the likelihood is not avail-
able but for validation purposes it is nevertheless useful to compare
to the likelihood-based setting). As is to be expected, the variances of
the SBI evidence estimates are generally larger than the likelihood-
based approaches (since the latter exploit knowledge of the likeli-
hood, whereas the SBI approaches clearly do not). Furthermore, we
ï¬nd that the NLE evidence calculation techniques typically provide
excellent agreement with the reference values, providing more ac-
curate evidence estimates compared to NPE and NRE. We suggest
that this may be due to the fact that our NLE evidence estimation
approach requires only a single neural density estimator (whereas the
NPE and NRE approaches require two), resulting in fewer sources of
approximation error. This result is particularly encouraging in view
of applications to cosmological scenarios, where it is very common
to perform SBI inference using the pydelfi software (Alsing et al.

2019), which indeed implements (sequential) NLE to sample the
posterior distribution.

Comparing amortised and sequential approaches to SBI inference,
again coupled with harmonic for evidence estimation, we notice
that the sequential approaches lead to either similar or more accurate
estimates of the evidence. This is not surprising as it conï¬rms that the
quality of the approximation performed in applying the SBI technique
does have an important eï¬€ect on the derived quality of the evidence
estimate. This increase in quality of the approximation comes at
the expense of a reduction in the generalisation of the inference
framework, which cannot be re-used for diï¬€erent observations.

To conclude, the methodology and proof-of-concept analysis pre-
sented in this article highlights the potential of harmonic as an
additional tool for Bayesian model selection that can also be suc-
cessfully applied within novel, promising SBI settings. At present, in
order to characterise the errors of the SBI evidence estimators pro-
posed here we recommend bootstrapping, as performed in the simple
numerical experiments presented in this article. Further research will
focus on alternative approaches to estimate the variance of the esti-
mators directly, folding in both sampling variance and neural density
approximate error. Future research will also focus on extending the
applicability of harmonic to larger data and parameter spaces, as
well as leveraging more eï¬€ective learnt importance target models.

ACKNOWLEDGEMENTS

We thank T. Kitching and M. Zaldarriaga for useful discussions.
ASM is supported by the MSSL STFC Consolidated Grant and the
Leverhulme Trust. MMD is supported by the Science and Technology
Facilities Council (STFC) Centre for Doctoral Training (CDT) in
Data Intensive Science (DIS) at UCL. MAP is supported by EPSRC
grant EP/W007673/1. This work used computing facilities provided
by the UCL Cosmoparticle Initiative and also facilities funded by the
Research Capital Investment Fund (RCIF) provided by UKRI, and
partially funded by the UCL Cosmoparticle Initiative and the UCL
CDT in DIS.

DATA AVAILABILITY

harmonic
available
astro-informatics/harmonic.

freely

is

at

https://github.com/

REFERENCES

Alsing J., Charnock T., Feeney S., Wandelt B., 2019, Monthly Notices of the

Royal Astronomical Society

Ashton G., et al., 2022, Nature Reviews Methods Primers, 2
Beaumont M. A., 2019, Annual Review of Statistics and Its Application, 6,

379

Beltran M., Garcia-Bellido J., Lesgourgues J., Liddle A. R., Slosar A., 2005,

Physical Review D, 71, 063532

Biwer C. M., Capano C. D., De S., Cabero M., Brown D. A., Nitz A. H., Ray-
mond V., 2019, Publications of the Astronomical Society of the Paciï¬c,
131, 024503

Brewer B. J., PÃ¡rtay L. B., CsÃ¡nyi G., 2011, Statistics and Computing, 21,

649

Bridges M., Lasenby A., Hobson M., 2006, Monthly Notices of the Royal

Astronomical Society, 369, 1123

Buchner

J.,

Methods,
Nested
doi:10.48550/ARXIV.2101.09675, https://arxiv.org/abs/2101.
09675

Sampling

2021,

Bayesian model comparison for SBI

11

Buchner J., et al., 2014, A&A, 564, A125
Cai X., McEwen
for

J. D., Pereyra M.,
high-dimensional

nested
selection,
sampling
doi:10.48550/ARXIV.2106.03646, https://arxiv.org/abs/2106.
03646

Bayesian model

2021, Proximal

Chib S., 1995, Journal of the American Statistical Association, 90, 1313
Cole A., Miller B. K., Witte S. J., Cai M. X., Grootes M. W.,
Nattino F., Weniger C., 2021, Fast and Credible Likelihood-
Free Cosmology with Truncated Marginal Neural Ratio Estimation,
doi:10.48550/ARXIV.2111.08030, https://arxiv.org/abs/2111.
08030

Cranmer K., Brehmer J., Louppe G., 2020, Proceedings of the National

Academy of Sciences, 117, 30055

Durkan C., Bekasov A., Murray I., Papamakarios G., 2019, Advances in

neural information processing systems, 32

Durkan C., Murray I., Papamakarios G., 2020, On Contrastive Learning for
Likelihood-free Inference, doi:10.48550/ARXIV.2002.03712, https://
arxiv.org/abs/2002.03712

Enderlein G., 1961, Biometrische Zeitschrift, 3, 145
Feroz F., Hobson M. P., 2008, Monthly Notices of the Royal Astronomical

Society, 384, 449

Feroz F., Hobson M. P., Bridges M., 2009, Monthly Notices of the Royal

Astronomical Society, 398, 1601

Feroz F., Hobson M. P., Cameron E., Pettitt A. N., 2019, The Open Journal

of Astrophysics, 2

Foreman-Mackey D., Hogg D. W., Lang D., Goodman J., 2013, Publications

of the Astronomical Society of the Paciï¬c, 125, 306

Friel N., Wyse J., 2012, Statistica Neerlandica, 66, 288
Gelfand A. E., Dey D. K., 1994, Journal of the Royal Statistical Society.

Series B (Methodological), 56, 501

Goodman J., Weare J., 2010, Communications in Applied Mathematics and

Computational Science, 5, 65

Greenberg D., Nonnenmacher M., Macke J., 2019, in Chaudhuri K., Salakhut-
dinov R., eds, Proceedings of Machine Learning Research Vol. 97,
Proceedings of the 36th International Conference on Machine Learn-
ing. PMLR, pp 2404â€“2414, https://proceedings.mlr.press/v97/
greenberg19a.html

Gregory P., 2005, Bayesian Logical Data Analysis for the Physical Sciences:
A Comparative Approach with MathematicaÂ® Support. Cambridge Uni-
versity Press, doi:10.1017/CBO9780511791277

Handley W. J., Hobson M. P., Lasenby A. N., 2015a, Monthly Notices of the

Royal Astronomical Society: Letters, 450, L61

Handley W. J., Hobson M. P., Lasenby A. N., 2015b, Monthly Notices of the

Royal Astronomical Society, 453, 4385

Hannam M., Schmidt P., BohÃ© A., Haegel L., Husa S., Ohme F., Pratten G.,

PÃ¼rrer M., 2014, Phys. Rev. Lett., 113, 151101

Heavens A., Fantaye Y., Mootoovaloo A., Eggers H., Hosenie Z., Kroon
S., Sellentin E., 2017, Marginal Likelihoods from Monte Carlo Markov
Chains, doi:10.48550/ARXIV.1704.03472, https://arxiv.org/abs/
1704.03472

Hermans J., Begy V., Louppe G., 2019, Likelihood-free MCMC with Amor-
tized Approximate Ratio Estimators, doi:10.48550/ARXIV.1903.04057,
https://arxiv.org/abs/1903.04057

Hermans

J., Delaunoy A., Rozet F., Wehenkel A., Louppe G.,
2021, Averting A Crisis
Inference,
In
doi:10.48550/ARXIV.2110.06581, https://arxiv.org/abs/2110.
06581

Simulation-Based

Jeï¬€rey N., Wandelt B. D., 2020, Solving high-dimensional parame-
ter inference: marginal posterior densities and moment networks,
doi:10.48550/ARXIV.2011.05991, https://arxiv.org/abs/2011.
05991

Lemos P., et al., 2022, Robust sampling for weak lensing and clustering anal-
yses with the Dark Energy Survey, doi:10.48550/ARXIV.2202.08233,
https://arxiv.org/abs/2202.08233

Llorente F., Martino L., Delgado D., Lopez-Santiago J., 2020, arXiv preprint

arXiv:2005.08334

Lueckmann J.-M., Bassetto G., Karaletsos T., Macke J. H., 2018, arXiv e-

prints, p. arXiv:1805.09294

RASTI 000, 1â€“12 (2022)

12

A. Spurio Mancini et al.

McEwen J. D., Wallis C. G. R., Price M. A., Docherty M. M., 2021, Machine
learning assisted Bayesian model comparison: learnt harmonic mean esti-
mator, doi:10.48550/ARXIV.2111.12720, https://arxiv.org/abs/
2111.12720

Neal R. M., 1994, JR Stat Soc Ser A (Methodological), 56, 41
Newton M. A., Raftery A. E., 1994, Journal of the Royal Statistical Society.

Series B (Methodological), 56, 3

Papamakarios G., Murray I., 2016, Fast ğœ– -free Inference of Sim-
ulation Models with Bayesian Conditional Density Estimation,
doi:10.48550/ARXIV.1605.06376, https://arxiv.org/abs/1605.
06376

Papamakarios G., Pavlakou T., Murray I., 2017, Advances in neural informa-

tion processing systems, 30

Papamakarios G., Sterratt D., Murray I., 2019, in Chaudhuri K., Sugiyama
M., eds, Proceedings of Machine Learning Research Vol. 89, Proceed-
ings of the Twenty-Second International Conference on Artiï¬cial Intel-
ligence and Statistics. PMLR, pp 837â€“848, https://proceedings.
mlr.press/v89/papamakarios19a.html

Papamakarios G., Nalisnick E., Rezende D. J., Mohamed S., Lakshmi-

narayanan B., 2021, arXiv:1912.02762 [cs, stat]

Robert C. P., Wraith D., 2009, in Aip conference proceedings. pp 251â€“262
Skilling J., 2006, Bayesian Analysis, 1, 833
Taracchini A., et al., 2014, Phys. Rev. D, 89, 061502
Tejero-Cantero A., Boelts J., Deistler M., Lueckmann J.-M., Durkan C.,
GonÃ§alves P. J., Greenberg D. S., Macke J. H., 2020, Journal of Open
Source Software, 5, 2505

Trotta R., 2007, Monthly Notices of the Royal Astronomical Society, 378, 72
Trotta R., 2008, Contemporary Physics, 49, 71
van Haasteren R., 2014, in , Gravitational Wave Detection and Data Analysis

for Pulsar Timing Arrays. Springer, pp 99â€“120

This paper has been typeset from a TEX/LATEX ï¬le prepared by the author.

RASTI 000, 1â€“12 (2022)

