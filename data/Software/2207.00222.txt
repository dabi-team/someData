Bayesian causal inference in automotive
software engineering and online evaluation

Yuchu Liu, David Issa Mattos, Jan Bosch, Helena Holmstr ¨om Olsson, and Jonn Lantz

1

(cid:70)

2
2
0
2

l
u
J

1

]
E
S
.
s
c
[

1
v
2
2
2
0
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Randomised ﬁeld experiments, such as A/B testing, have
long been the gold standard for evaluating software changes. In the
automotive domain, running randomised ﬁeld experiments is not al-
ways desired, possible, or even ethical. In the face of such limita-
tions, we develop a framework BOAT (Bayesian causal modelling for
ObvservAtional Testing), utilising observational studies in combination
with Bayesian causal inference, in order to understand real-world im-
pacts from complex automotive software updates and help software
development organisations arrive at causal conclusions. In this study,
we present three causal inference models in the Bayesian framework
and their corresponding cases to address three commonly experienced
challenges of software evaluation in the automotive domain. We develop
the BOAT framework with our industry collaborator, and demonstrate the
potential of causal inference by conducting empirical studies on a large
ﬂeet of vehicles. Moreover, we relate the causal assumption theories to
their implications in practise, aiming to provide a comprehensive guide
on how to apply the causal models in automotive software engineering.
We apply Bayesian propensity score matching for producing balanced
control and treatment groups when we do not have access to the entire
user base, Bayesian regression discontinuity design for identifying co-
variate dependent treatment assignments and the local treatment effect,
and Bayesian difference-in-differences for causal inference of treatment
effect overtime and implicitly control unobserved confounding factors.
Each one of the demonstrative case has its grounds in practise, and
is a scenario experienced when randomisation is not feasible. With the
BOAT framework, we enable online software evaluation in the automo-
tive domain without the need of a fully randomised experiment.

Index Terms—Automotive Software, Causal Inference, Software Engi-
neering, Bayesian Statistics, Online Experimentation.

1 INTRODUCTION

Randomised online experiments, such as A/B testing, is a
technique for evaluating the impact of software changes
towards real users. With the demonstrated success of online
experiment implementation in Software-as-a-Service (SaaS)
companies [1], [2], [3], the automotive domain starts to
raise interest in adopting such a method, and even starts
to conduct experiments for evaluating embedded software
online [4], [5], [6], [7], [8], [9]

Despite the known advantages and beneﬁts, the automo-
tive domain struggles to scale experimentation activities [4],

Y. Liu is with Volvo Cars, 405 31 G¨oteborg, Sweden and Chalmers University
of Technology, 412 96 G¨oteborg, Sweden (email: yuchu.liu@volvocars.com).
D.I. Mattos is with Volvo Cars, 405 31 G¨oteborg, Sweden.
J. Bosch is with Chalmers University of Technology, 412 96 G¨oteborg.
H. H. Olsson is with Malm¨o University, 211 19 Malm¨o, Sweden.
J. Lantz is with Volvo Cars, 405 31 G¨oteborg, Sweden.

[6], [10]. Some of the identiﬁed challenges are the limited
number of users, limited capability of Over-The-Air (OTA)
software deployment, strict user agreements, safety-critical
and validation constraints among others. These limitations
often create roadblocks that limit the scope and feasibility of
conducting experiments in customer vehicles. To overcome
these limitations, practitioners are looking to leverage col-
lected observational data to understand the causal impact
of a software change [11].

In our previous study [7], we empirically applied and
evaluated the use of causal modelling for software engineer-
ing, in which a Bayesian propensity score matching model
is applied for generating balanced control and treatment
groups in observational software testing. However, as we
have experienced further needs in causal inference in the au-
tomotive domain due to a number of limitations, to address
this need, this paper evaluates the use of three different
Bayesian causal models for treatment effect inference from
observational studies, applied to automotive software de-
velopment. This work extends the method BOAT (Bayesian
causal modelling for ObvservAtional Testing) [7] to include
the Bayesian propensity score matching model for produc-
ing balanced control and treatment groups, the Bayesian
regression discontinuity design for identifying covariate
dependent treatment assignment, and Bayesian difference-
in-differences model for causal inference on treatment effect
over time. While these models have been widely used in
the frequentist setting in other domains of science (such as
medicine [12], trafﬁc and transport [13], social studies [14],
[15]), to the best of our knowledge, this is the ﬁrst paper to
apply and evaluate these models in the Bayesian setting and
in the context of automotive software engineering.

We demonstrate the BOAT method with three cases from
our industrial collaborations, utilising automotive embed-
ded software deployed on real vehicles and users. Compar-
ing with the existing literature, the contribution of this paper
is three-fold.

• We present an overview and discussion of causality
in the potential outcomes framework applied in au-
tomotive software development, along with three il-
lustrative studies that reinforce the need for Bayesian
causal inference in software online evaluation.

• We demonstrate three different Bayesian causal in-
ference models to assess the causal effects in their
corresponding examples. These models are Bayesian

 
 
 
 
 
 
regression discontinuity design, Bayesian difference-
in-differences, and Bayesian propensity score match-
ing.

• We relate the causal assumptions made in causal
inference in relation to online observational studies
conducted on automotive software, and we discuss
their speciﬁc implications.

The rest of this paper is arranged as following. We elab-
orate the importance of causality in automotive software
engineering and present the theory and assumption in the
potential outcome framework in 2. In Section 3, we present
the BOAT framework and our research method. The three
Bayesian causal models and their related cases are described
in Section 4, 5 and 6. The discussion and conclusion are in
Section 7 and 8 respectively. Moreover, we include an online
appendix to share our Bayesian models and their inference.

2 BACKGROUND

In this section, we introduce the concept of randomised ex-
perimentation applied in software engineering, the potential
outcomes framework, and its relevant theories. Moreover,
we give an overview of Bayesian statistics and its inference.

2.1 Randomised experimentation

Randomised experiment methods, such as A/B testing, are
common practises adopted by SaaS companies [1], [2], [3].
In a two-level experiment, the sample group is split into
control and treatment at random and exposed to differ-
ent versions of the same software. When an experiment
is fully randomised, the outcome is independent of the
treatment assignment, this is deﬁned as exchangeability.
In other words, the control and the treatment groups are
interchangeable and do not have any preexisting differ-
ences, thus the observed outcome can only be caused by
the treatment. Therefore, randomised experiments help us at
establishing a causal relationship between the intervention
and the outcome.

Causal knowledge helps us cope with change [16].
Through data analytics, i.e., passive observation, we could
compute a joint distribution of vehicle usage and perfor-
mance – however, such a distribution cannot inform us if
a change in our product would or would not improve the
product performance and user experience. With interven-
tion, randomised experimentation enables direct feedback
from the users and helps organisations answer the ”what-
if” question to software changes. Randomised experiment
has long been the gold standard for evaluating software in
an online and continuous manner.

In recent years, there is an increasing interest of adopt-
ing online experimentation in the automotive domain but
the ability to conduct large scale and fully randomised
experiments is signiﬁcantly more limited, as reported in
[4], [6], [8], [10], [17]. The automotive domain faces many
unique restrictions compared to SaaS companies, such as the
number of hardware and software variants [4], architecture
restrictions [5], safety regulation constraints, number of ve-
hicles available for experimentation [6], driver consent, and
the ability to frequently update software [4], [10]. A combi-
nation of these challenges leads to many situations where a

2

X

t

y

Fig. 1. A simpliﬁed directed acyclic graph showing the relationships of
treatment (t), target variable (y), and covariates (X).

randomised experiment is not possible, such as in limited
samples; desired, such as in highly regulated systems; or
ethical, without explicit consent of the vehicle owners and
users on the complete scope of the new software.

When a randomised experiment is not feasible, there
present confounding factors that can often cause a spurious
correlation and hinder us from drawing causal conclusions
[16], [18], [19]. To address this issue, observational studies
in combination with causal inference models and causal
assumptions need to be applied. We will further discuss
the empirical scenarios from the automotive sector and the
underlying implications of causal assumptions on software
observational testing in the following sections.

2.2 The potential outcomes framework

The potential outcomes framework [19] describes causal
inference from an intervention introduced in randomised
experiments. In this section, we discuss the potential out-
come from both experiments and observational studies,
the later is extensively explored in studies such as [18],
[20]. Potential outcome models quantify the treatment effect
from the intervention introduced, or from known systematic
differences between the control and treatment groups.

A word on notation: In this paper, we will use the fol-
lowing set of notations consistently throughout the different
sections. In this paper, a lower case letter (such as x) denotes
a vector, and when necessary, a superscript xT denotes a
row vector of a random variable. The subscript xj denotes
the length of the vector. The notation of x = {x1, x2, ...xj}
indicates all the instances in the vector, and if a square
bracket is used, we indicate the values in x are bounded,
e.g., x = [0, 42]. We use a bold upper case letter (such as
X, containing a set of vectors x ∈ X) to denote a matrix of
unspeciﬁed dimension. Similarly, subscript Xij denotes the
i number of rows and j number of columns of the matrix X.
We use E[y|x] to express the expectations on variable y given
x, where the | notation represents conditional likelihood.
P (x) or p(x) are used for expressing the probability of x, we
use them interchangeably without making a distinction on if
the probability distribution is continuous or discrete. Finally,
the falsum symbol, ⊥, is dedicated to represent statistical
independence throughout this paper.

Let us consider an experiment in which we introduce
two software variants to two groups, control (Nc) and treat-
ment (Nt). We denote the two levels of software variants as
t = {0, 1}. For each individual in the sample population n ∈
N , we measure a target variable, y, to understand the possi-
ble outcomes, and a set of covariates x ∈ X that are predic-
tive of the outcomes and potentially inﬂuence the treatment.
The covariates X, are also often referred to as context, or
confounding factors. In practise, the two treatment levels
of software could be t = {old version, new version}, for

evaluating a change to an existing software. The potential
outcomes of such an evaluation of an energy management
software, could be reported as y = [150, 300] measured in
Wh/km.

Suppose we are interested in two treatment levels in a
study, t = {0, 1}. In the Rubin potential outcomes frame-
work, the average treatment effect (AT E) can be expressed
as,

AT E = E[y|t = 1] − E[y|t = 0]

(1)

where the E(y) represents the expectation of the outcome
from the samples at different treatment levels. In order
to infer the treatment outcome, an important assumption
made in the potential outcomes framework, is the stable
unit treatment assumption, stating that the treatment only
effect the individual sample the treatment is applied to.

Note that, when the experiment is randomised, the treat-
ment outcome is unconditional to the control and treatment
group assignment. In other words, the two groups are
exchangeable. Therefore, we can explicitly establish a causal
relationship between treatment and the potential outcome
in a randomised experiment. Exchangeability is expressed
as,

[y|t = 0], [y|t = 1]⊥t

(2)

where ⊥ denotes independence, and | means given the

condition, y⊥t|x reads as y is independent of t given x.

The potential outcome and/or treatment assignment in
an observational study is inﬂuenced by covariates X. In
a trivial example, all covariates inﬂuence both the treat-
ment assignment and the outcome, a confounding effect.
We illustrate such an example in a directed acyclic graph
(DAG) in Fig. 1, to infer causality from this DAG, a valid
adjustment set of covariates should block every path from
the treatment t to the target variable y in the DAG. If
such a set of covariates exists and can be identiﬁed in an
observational study, we assume the exchangeability persists,
and it is conditional on the adjustment set (X) given the set
of covariates can be observed and identiﬁed. Formally,

[y|t = 0], [y|t = 1]⊥t|X

(3)

In an observational study, it requires that there shall
be treated and untreated samples in every combination of
the values of the observed confounding factors X [21], the
positivity assumption, is formally expressed as,

0 < P (T = t|X = x|) < 1

(4)

In addition, covariates can be used to estimate the con-
ditional average treatment effect (CAT E) in observational
studies, namely,

CAT E = E[y|t = 1, X = x] − E[y|t = 0, X = x]

(5)

The inference of conditional average treatment effect
is helpful in heterogeneous studies. For example, we can
study the treatment effect in subgroups of vehicle models
or locations, provided the covariates heavily inﬂuence the
treatment outcome.

3

2.3 Bayesian statistics and inference

A short overview of Bayesian statics and inference methods
will be provided in this subsection. To put the Bayesian
causal inference models in context, we present the the basic
principle of Bayes’ theorem and inference methods used
in this study. However, we do not provide a comparison
of frequency and Bayesian statistics, as the difference in
reasoning is beyond the scope of this paper and we refer
such a comparison to other works in software engineering
[22], [23], [24].

In Bayesian statics, the probability of an event is ex-
pressed as a degree of belief which is based on the prior
knowledge of said event. The intuition of Bayesian statistics
is that the degree of belief is updated by observing new data,
evidence, and the sensitivity of the outcome to the prior
reduces as more observations are made. In the application
of causal inference, applying Bayesian statistics allows us to
incorporate available prior knowledge on model parameters
when inferring the counterfactual outcome [25]. The Bayes’
theorem is expressed as,

P (A|B) =

P (B|A)P (A)
P (B)

(6)

where,

• P (A) is the prior, the marginal probability of a
hypothesis before any evidence is observed or pre-
sented. This is often referred to as domain knowl-
edge.

• P (B) is the marginal probability of observing the

event B.

• P (B|A): is the likelihood describing the probably of

observing the evidence given the prior.

• P (A|B): is the posterior probability given the evi-

dence, the observation, and the prior.

In most cases, the exact posterior distribution of the
model parameters cannot be solved analytically, but it can
be approximated numerically with Markov Chain Monte
Carlo (MCMC) or variational inference methods. In this
paper, we approximate the posterior distribution through
the No-U-Turn Sampler (NUTS) in the Hamiltonian Monte
Carlo algorithm. Using a recursive algorithm, NUTS con-
structs a set of possible candidate point spans widely across
the target distribution [26]. NUTS stops automatically if it
retraced its steps, hence the name “No-U-Turn”.

The prior distributions are an integral part of Bayesian
model and they allow researchers the ﬂexibility of incor-
porating domain knowledge of previous research to create
better and more robust models. The priors often act as
constraints of plausible probabilities of parameter values.
With small sample sizes, the prior or the domain knowledge
has a higher inﬂuence. While with larger sample sizes, the
evidence overcomes the impact of the prior in the posterior.
Priors can be speciﬁed to be non-informative, weakly infor-
mative, and informative for their models. A non-informative
prior is based on an unbounded uniform distribution and
does not aggregate any information to the posterior and
are often non proper. Weakly-informative are those that do
not aggregate much information in the posterior parameters
and serves as regularisers in the inference and convergence

of the MCMC solver. An example of such a prior would
be a normal distribution with a large variance compared
to the expected parameter value. Finally, informative priors
are those that incorporate domain knowledge on the subject
and set stricter bounds to parameters in the model. If the
evidence is accordance with the prior, convergence happens
faster due to the smaller search space for the MCMC solver.
If evidence points out to a parameter outside these bounds,
either domain knowledge or data collection should be re-
vised and convergence might be slow.

Comparing to another ﬂavour of statistics, the frequen-
tist statistics, Bayesian statistics has many beneﬁts that have
been discussion in previous literature. By incorporating the
concept of a prior, the stronger the prior information is,
the less sample size Bayesian statistics requires to arrive a
conﬁdent prediction of the posterior distribution. Utilising
this property, Bayesian causal inference models applied in
other areas of science, have reported to be less sensitive to
sample sizes [12], [13], [27], [28]. Moreover, instead of a point
estimate prediction provided by the causal inference models
in the frequentist domain, Bayesian causal inference mod-
els provide a complete posterior distribution, that can be
utilised for the analysis of the treatment effect estimations.

3 THE BOAT FRAMEWORK

In this section, we present an alternative approach to ran-
domised experimentation in software engineering, BOAT
(Bayesian causal modelling for ObvservAtional Testing). In
this framework, we combine the notion of quasi-random
treatment assignment with data obtained from pure ob-
servations, aiming to address the situations where a fully
randomised experiment is not feasible. Different from an
observational study, in which the treatment is inferred from
known systematic differences of the control and the treat-
ment groups, our method allows one to actively intervene
with a treatment group that is not randomly sampled. This
framework enables development organisations to evaluate
their software online without the need of a fully randomised
large scale experiment.

3.1 BOAT

A fully randomised experiment is often challenging to con-
duct in the automotive domain, in the absence of randomi-
sation, it requires a series of causal modelling techniques
to mimic randomisation or to adjust covariates before a
treatment effect can be inferred. To address the challenges
of randomisation, the BOAT framework is induced from
the potential outcomes theory, and is applied and validated
through exemplary cases with our industry collaborator. We
describe the framework in detail as well as the research
method applied for the validation of the BOAT frame-
work. We list the challenges in randomisation and their
corresponding solutions in the BOAT framework as the
following.

The ﬁrst challenge in adopting online experimentation
is the limited access to the entire user base. To start, the
automotive domain has a signiﬁcantly smaller user based
comparing to the SaaS domain, as a result of product
diversity and hardware dependency [4], [6]. Moreover, in

4

Fig. 2. A decision ﬂowchart on which Bayesian causal model from the
BOAT framework to apply when designing an online software evaluation.
(BRDD: Bayesian regression discontinuity, BPSM: Bayesian propensity
score matching, BDID: Bayesian difference-in-differences)

combination with the limitation of safety-critical software
and the lack of explicit user agreements, shipping new
software to the entire ﬂeet is typically undesired, impossible,
or unethical. As a result, the control and the treatment
groups are likely to be unbalanced and the treatment effects
are confounded by one or more unbalanced covariates. In
situations when the treatment effect is confounded by more
than one covariate, it is impossible to balance the control and
treatment groups by stratiﬁcation, therefore, a propensity
score needs to be modelled from all covariates included
in the system. Propensity score matching, ﬁrst proposed
by Rosenbaum and Rubin [18], is a method for matching
samples from the control and treatment group based on the
propensity score calculated from observed covariates, thus
adjusting for covariates and estimating unbias treatment
effects. The Bayesian propensity score matching model is
used for designing a balanced control and treatment group
in trafﬁc safety analysis [13] and in automotive software
engineering [6].

Second, the performance of automotive software func-
tions is often heavily inﬂuenced by temporal factors such
as weather and time of week. Such a seasonality effect
can be observed in software functions related to energy
consumption [29] and crash safety [30], [31]. In practise, if
we want to evaluate an energy management software for
battery electric vehicles, suppose we compare the energy
efﬁciency before and after the software change on the same
vehicles, the conclusion could be bias and confounded by
unobserved temporal factors, e.g., temperature. To address
this particular issue, we suggest to apply the difference-in-
differences model in the Bayesian framework. This model
is ﬁrst presented by Card and Krueger [32] in analysing
the treatment effects of increasing the minimum wage. In
our model, we include a control group of vehicles running
on the old software version through observation. Therefore,
any bias caused by factors common to the control and treat-
ment is implicitly controlled for, even when the confounders

Access to allusers?StartCan randomlysample from allthe users?YesYesAssume allcovariates areknown?NoOne dominantcovariate?YesThe covariate iscontinuous?YesRandomisedexperimentBRDDYesBPSM orStrataNoBPSMNoAnalysis oftreatment effectover time?NoYesNeed to inferunobservedcovariates?NoBDIDare not observed. Besides econometrics, Bayesian difference-
in-differences model is applied to analyse the treatment
effect over time for diabetes patients [12].

Third, we see a need of modelling and analysing soft-
ware studies where the treatment assignments depend on
one continuous covariate. Since many software functions in
vehicles are only activated or beneﬁcial for users around
a certain threshold of a variable, such as speed or trip dis-
tance. This is the case when we are analysing the fuel saving
potential from route prediction of plug-in hybrid vehicles, as
the trip distance heavily inﬂuences the prediction accuracy
and the fuel saving potential. It is believed that the software
is particularly beneﬁcial, when the driver travels slightly
further than the pure electric range of the vehicle on the
daily basis. In this scenario, the regression discontinuity
design [33] could help us model the treatment causal effect
through identifying a threshold of an assignment covariate
where the treatment is mostly inﬂuenced. Bayesian regres-
sion discontinuity design is applied in other areas of science
such as economics [27] and medicine [28].

To further illustrate the use cases of the BOAT frame-
work in automotive software engineering, we provide a
ﬂow chart in Fig. 2. When designing a software online eval-
uation, the ﬁrst assessment criteria is the available sample
size determined by a power analysis of expected size of the
treatment effect. In the scenario of all users can be accessed
and a randomised sampling process can be done, a fully ran-
domised experiment is always more ideal for a strong causal
conclusion. Since randomisation for sampling from the total
user base not only ensures exchangeability, it also ensures
representativeness thus removing sampling bias. The sec-
ond step is to determine if we can safely assume all the
covariates and their relationship to the expected outcome is
known and observable. Under this assumption, there is a
need to identify if there are one or more covariates, since
the balancing of a single and categorical covariate can be
achieved through stratiﬁcation as well. In situations where
there are more than one categorical covariate, a Bayesian
propensity score matching (BPSM) shall be performed to
design a balanced control and treatment group. In the case
of one continuous and dominating covariate, the design
calls for the use of a Bayesian regression discontinuity
(BRDD) model, which utilises the continuous covariate as
an assignment variable for assigning samples to the control
and treatment group. If the assumption of known covariates
cannot be made, which is often the case when evaluating
a novel software functionality, or conducting a longitudi-
nal evaluation. In the former scenario, there is usually no
available data to analyse the causal structure, and in the
latter case, the number of covariates required for identiﬁ-
cation quickly scales as the study is conducted during a
prolonged period of time. The development organisations
need to decide if the unobserved (latent) variables need
to be modelled and inferred. If not, a Bayesian difference-
in-differences (BDID) model can be applied. BDID is an
effective strategy to infer treatment effect overtime, without
the need of observing any time dependent covariates.

Although the BOAT framework provides a structured
guidance for software development organisations for their
design decisions of online software evaluations, and to a
large extend, enables such online evaluation that is oth-

5

erwise challenging or impossible particularly in the auto-
motive domain. We recognise that the framework is not in
anyway complete. For example, if the causal relation is un-
known, a causal discovery process [40] or a graphical model
is needed [9]. To that end, if a latent variable is deemed
to be important and needs to be modelled, methods such as
instrumental variable [34] can be applied, as indicated as the
missing ”yes” action from latent variable inference in Fig.
2. We will further discuss the limitations and the potential
extension of the BOAT framework in the discussion section.

3.2 Research Method

To validate our proposed BOAT framework and its appli-
cability in automotive software engineering, we employ
the design science research method following guidelines
from [41]. The BOAT framework, can be considered as a
design artifact created to address an important research and
organisational problem – in the absence of randomisation,
how do we evaluate software in an online fashion and
infer causality. The relevance of the problem is assessed
and addressed through the challenges of randomisation
experienced in practicse. We list the practices of this re-
search following guidelines from [41] in detail in Table.
1. In Table. 1, we replace the original description from
[41] of the step-wise guideline with the approaches taken
in our research activities. The industry collaborator is an
automotive manufacturer with operations in Europe, China,
and North America, and this research is part of a long term
collaboration. During this study, we worked closely with
three software development teams, these teams are respon-
sible for the conceptualisation, design, and development of
their perspective software features in-house.

Design science is an iterative process containing three
major cycles [42], one, the relevance cycle in which the
problems and requirements are evaluated in the context.
Two, the design cycle, during this step, the design artefacts
and processes are proposed and evaluated. Three, the ﬁnal
design solution which has its base in scientiﬁc theory is
then evaluated empirically. In the ﬁrst cycle of this research
work, we aim to identify relevant challenges and limitation
in the automotive domain for randomised experimentation.
This research cycle is done in two ways. First, to ensure the
relevance, we host weekly workshops and meetings with
the software development teams to discuss and summarise
the challenges experienced in practise. At least one of the
researchers participate the meetings and workshops, and
document the outcome as meeting notes. To improve the
external validity of the conclusions, we then map the chal-
lenges experienced in practise with reports from literature
documenting online experimentation in the domain. Only
challenges that are reported by two or more developers, or
reported by a single developer and also reported in litera-
ture, are included in the ﬁnal conclusion of the relevance
cycle.

The design of the BOAT framework is a combination
of addressing relevant challenges in randomisation, and
solving them with the causal inference models introduced
and applied in other domains. We study the theory of the
causal inference models, as well as their applications and
feasibility studies in other area of science through literature.

TABLE 1
Guidelines of design science research method [34], and practices applied following the guidelines in this research.

6

Guideline 1: Design as an artifact

Guideline

Guideline 2: Problem relevance

Guideline 3: Design evaluation

Guideline 4: Research contributions

Guideline 5: Research rigor

Guideline 6: Design as a search process

Guideline 7: Communication of research

Practise in this research
We present the BOAT framework to three software development organisations, the framework
addresses varies limitations in randomised online experiments, assess practical scenarios to
provide Bayesian causal modelling suggestions. The framework is derived from the theory of
potential outcome, and all of the modelling approaches within the framework are extensively
applied and validated in other areas of science [12], [13], [14], [15], [18], [35], [36].
To ensure the technical solution developed is relevant to the domain, we derive the problem
from existing literature addressing the challenges on online experimentation adaptation in
automotive [4], [5], [6], [7], [8], and literature stating the challenges of online experimentation
in other domains [1], [2], [3]. All of the literature included in the analysis are based on em-
pirical research in their respective domains. Additionally, the scenarios that limit randomised
experimentation are also experienced and reported by our industry collaborator.
The evaluation of the BOAT framework is done quantitatively through three separate empirical
cases. The empirical study are designed together with development organisations as suggested
by [35], [36], [37], and deployed to a selective number of customer vehicles to simulate three
scenarios of online software evaluation in the absent of randomisation. The quantitative data is
collected from the vehicles through telecommunication units onboard. We determine the target
variable and the covariates together with the development teams which also developed the
software changes. Additionally, we assess the validity of the causal assumptions in practise
with domain knowledge provided by experts from the development organisations.
Not to be confused with the research contribution of a publication, the research contribution in
design science is assessed by implementability and representational ﬁdelity. The former criteria
is satisﬁed as there are commonly available tools for computing the the causal models, as well
as the authors of this paper have implemented the code in Pyro [38]. The later is ensured
through the close collaboration with an automotive manufacturer.
Conducting research with design science often requires mathematical formalism to describe
the design artifact [34], [39]. We formulate the Bayesian causal models and assumptions in the
BOAT framework mathematically, as well as providing an algorithmic description for model
implementation. In the BOAT framework, the relationships between factors within the system
is assumed to be known; the inputs, such as covariates, and the outputs are selected based on
the domain knowledge. Claims about the quality of design artifact is dependent on the choice of
performance metrics. Therefore, we evaluate the causal models following advice from existing
literature introducing the models to other areas of science [18], [32], [33].
As a research method, design science is iterative and a way to discover an effective solution
to the problem. In our research, we maintained close collaboration with the development
teams through weekly design meetings, in which we discuss all aspects of the cases including
the potential confounding factors, the expected treatment effects, and etc. The selection of
covariates is done in an iterative manner to ensure the covariates with strong correlation
to the target variable is included in the model. Moreover, the scenarios addressed in the
BOAT framework is derived from literature [4], [10] and validated from the state-of-practise in
automotive software engineering.
The results of our research is communicated in the following three ways; (1) we host
popularised science presentations for our industry collaborators on a regular basis every three
month. The participants of these presentations usually occupy managerial positions such as
product manager, project manager, and product owner. (2) we communicate the BOAT frame-
work, the implemented Python code, and the results to the development organisations through
our weekly meetings, during which we also provide tutorials of the Bayesian modelling
approach. Participants are usually developers and data scientists alike. (3) we communicate
our work to the scientiﬁc community through publications.

We then explain the models with examples from other
domains to the software development teams, both on the
conceptual level and on the theoretical level. In this process,
members of the software development teams provide detail
description of possible use cases of the inference models.
The descriptions are used as inputs for the following two
aspects of the BOAT design, (1) helps us in evaluating
which causal inference models are applicable in the au-
tomotive domain, (2) we summarise the most commonly
mentioned use cases and design our empirical validation
studies around them. Similarly to cycle one, this phase of
the research is conducted through weekly meetings with the
software development teams. At least one of the researchers
participated in those meetings in situ.

Last but not least, the successful adoption of a new
framework in software engineering requires the framework
to be investigated in the business organisations it is applied

in [43], therefore, together with our industry collaborator,
we validate the framework empirically with three software
development organisations. To evaluate the Bayesian causal
models, we design three study cases, we deploy software
and collect empirical data from a total of 1,364 vehicles
driven by real-world customers. The vehicle data collection
took place during an nine-month period, between December
2020 to September 2021. The detailed setup of the three em-
pirical cases are discussed in their corresponding sections.

4 BAYESIAN PROPENSITY SCORE MATCHING

In this section, we present the theory, our observational
study, and the results from the Bayesian propensity score
matching (BPSM) model. The theory and assumption of the
model is presented formally, followed by a discussion on
different matching strategies. Finally, we introduce the setup

7

the treatment or outcome. The average treatment effect
identiﬁed through propensity score matching (AT EP SM )
is conditional to the treatment t and the propensity score
inferred from all observed covariates, e(X). Given the as-
sumption holds, the average treatment effect adjusted with
the propensity score is an unbias estimate of the average
treatment effect,

AT EP SM = E[y|t = 1, e(X)] − E[y|t = 0, e(X)]

(8)

There are two steps in propensity score matching. First,
we estimate the propensity score through a Bayesian logistic
regression, then we perform the matching of samples based
on their propensity score distance. We present the two steps
separately in the following subsections.

4.1.1 Bayesian logistics regression

In a BPSM, the propensity score is estimated with a Bayesian
logistic regression. Sticking to the same notations, the treat-
ment indicator t, follows a Bernoulli distribution,

t ∼ Bernoulli(t|e(X))

where the propensity score e(X) is expressed as,

e(X) =

eα+βX
1 + eα+βX

(9)

(10)

The regression intercept and coefﬁcients, α and β are
latent variables. That is, they are not directly observed but
inferred from other variables X. We normalise the prior
Gaussian distributions for the regression intercept α, as a
result, this prior distribution has a 0 mean and a variance of
λα,

α ∼ N (α|0, λα)

similarly, β has a Gaussian distributions of a 0 mean and

variance of λβ as prior,

β ∼ N (β|0, λβ)

By Bay’s theorem, the posterior distribution of this net-
work is simply the product of the likelihood and the prior.
Therefore, for all samples n ∈ N , the posterior distribution
is a joint probability of t, α, and β marginalised over p(t),
that is,

p(t, α, β|X, λα, λβ)

= p(α|λα) · p(β|λβ) ·

N
(cid:89)

n=1

p(t|α, β, X)

(11)

Bayesian networks are generative models, to generate
the joint probability distribution of the regression model,
the generative process is stated in Algorithm 1.

4.1.2 Matching

The second and ﬁnal step of BPSM is matching samples
from the control and treatment groups based on their
propensity score distance, and to minimise the average
distance for the two groups. The propensity score distance
(δe(X)n) is deﬁned as the absolute difference of the propen-
sity score of each sample (n) in the control and treatment
group,

Fig. 3. An illustration explaining the Propensity Score Matching model.
Note, ﬁgure does not represent real data.

of our observational study utilising BPSM as an identiﬁca-
tion strategy and we present the results.

To estimate the unbias treatment effect in an observa-
tional study is challenging, as the treatment assignment
and effect could be confounded by covariates. We use the
illustration in Fig. 1 to demonstrate this scenario. Propensity
score matching [18] addresses this issue through covariate
adjustment and allows us to generate balanced control and
treatment groups even when the sample size is limited.
The covariates from matched control and treatment group
should form similar empirical distribution, thus reducing
bias in the estimated treatment effect. We illustrate the
concept of propensity score matching in Fig. 3, as can be
seen, samples are matched based on their propensity score
similarity, also called propensity score distance. In previous
literature, propensity score matching has been used for
experiment design when the sample size is small [6], [35],
[36], [44], and for causal effect analysis post facto [11], [13].
When propensity score estimate is done through a Bayesian
network, literature reports the sensitivity to sample size
being lower comparing to the conventional propensity score
model [13].

4.1 Theory
Propensity score, denoted as e(x), is a function of one or
more covariates x ∈ X. Propensity scores are modelled
and matched in the control (t = 0) and treatment (t = 1)
groups, so that the conditional probability distribution of
the covariates given the propensity score p(x|e(x)) is similar
in both groups [18]. The strong exchangeability assumption,
extends from the conditional exchangeability, stating that
the control and treatment outcome pair is assumed to be
independent from treatment assignment, given the observed
covariates, formally,

([y|t = 0], [y|t = 1])⊥t|X

(7)

This assumption implies that the treatment or the out-
come is only confounded by observed covariates and the
covariates that are ignored from the data do not effect

Propensity scoreControlTreatment0.01.0Matched samplesAlgorithm 1 Bayesian logistic regression generative process
Inputs: X covariates, λα prior distribution of α, λβ prior
distribution of β, tn control/treatment indicator
1: Draw α ∼ N (α|0, λα)
2: Draw β ∼ N (β|0, λβ)
3: for each vector of covariate x ∈ X do
4:
5: end for

Draw t ∼ Bernoulli(t|Sigmoid(α + βx))

δe(X)n = |e(X)n,t=1 − e(X)n,t=0|

(12)

Many matching methods have been explored in the lit-
erature, calliper matching [45], 1:1, or n:1 nearest neighbour
matching [44], and full matching [36], [46], just to name
a few. In general, there are two categories of matching
methods, with or without replacement. Matching can be
done with or without replacement. Matching with replace-
ment means one sample in one group can be matched with
multiple samples in another group, an example of such
method is the optimal full matching algorithm [46].

By using matching methods without sample replace-
ment,
the matched control and treatment group will
yield the number of samples. Calliper matching is a
type of matching method, in which a maximum allowed
δe(X)n,max is predetermined and all samples exceeding the
threshold are discarded. Although calliper matching could
result in a reduction on sample size if the calliper is too
ﬁne, it is an intuitive and computationally efﬁcient matching
method [45]. Moreover, the choice of calliper can affect the
result bias [47]. As treated samples are usually more expen-
sive to obtain, discarding those samples could be considered
unfavourable in the automotive application. Therefore, a
second matching method is explored in the study, 1:1 nearest
neighbour matching [44]. In a 1:1 nearest neighbour match-
ing, the algorithm looks for a control sample with the closest
propensity score distance to a given treated sample, thus, no
treated samples will be discarded. In this study, both calliper
matching and 1:1 nearest neighbour matching are applied,
as our objective is to ﬁnd control samples to be compared
with the treated samples.

4.2 Study I: Limited access to users

In the automotive domain, a fully randomised experiment
on a large scale is often impossible due to their already
limited numbers of users and it is often undesired to serve
new software on the entire ﬂeet. When a safety critical
software is the subject of interest – a majority of automotive
software are in safety critical systems – introducing a novel
software feature to a larger number of vehicles might not
be desirable; although the likelihood of catastrophic failure
is low [4], but any minor disturbances at a scale could still
lead to ﬁnancial loss for commercial vehicles. Combine this
with the fact that we often only want to examine software
features on a speciﬁc vehicle model driven in a speciﬁc
region, it further limits the available sample size. As an
alternative to large scale randomised experiments, we pro-
pose a small-scale rollout to a limited number of vehicles.
However, when the control and treatment groups are not

8

randomised, the exchangeability assumption does not hold
and the observed change in the target variable could be
confounded on preexisting differences between the groups
instead of the treatment itself. Thus, we apply Bayesian
propensity score matching for matching comparable treated
and untreated vehicles based on a number of observed
covariates.

We design a study to simulate this scenario and learn the
feasibility of applying BPSM as an identiﬁcation strategy
in observational studies such as this. In Study I, we aim
to analyse treatment effects from an energy management
software. Based on how the vehicle is historically driven,
this software predicts the current trip proprieties such as
expected route, and based on the prediction, energy con-
sumption is optimised through actions such as downshifting
before a hill climb. This software was difﬁcult, if not impos-
sible, to validate in a lab-like environment as the energy
saving potential is strongly dependent on the prediction
accuracy, however, since the software feature involves safety
critical systems such as engine control, it is undesirable to
introduce it to a large group of users for a fully randomised
experiment. Instead, we passively observe 1100 vehicles in
the control group running on an existing software driven
by real-world customers, while only serving the modiﬁed
software to 38 vehicles as our treated samples. The treated
samples, are vehicles leased to employees whom use the
vehicles as their regular cars. This study occurred from
October 2020 to March 2021, and all vehicles are driven
by users residing in Sweden. We discard data generated by
brand new vehicles with mileage less than 100 kilometres.
We also discard trip data in which the average trip speed is
higher than 200 kilometres per hour. After postprocessing,
we have collected data from a total of 421,881 trips made by
1138 vehicles.

A fundamental assumption of applying propensity score
as an identiﬁcation strategy for observational study is the
strong exchangeability assumption (often called ignorability
[18]), implies that the treatment outcome is not dependent
on unobserved covariates. This strong assumption cannot
be checked empirically from pure observational data. Es-
sentially, the assumption implies that all the confounders
that potentially inﬂuence the outcome are observed and it
inherently limits us to draw causal conclusion when no co-
variate is known. In other words, propensity score matching
can only balance covariates that are observed, while full
randomisation can balance all covariates, observed or not
[35]. In practice, to design a study utilising BPSM requires
existing data or knowledge of the software systems, as the
results are strongly dependent on covariate selection [48]. To
this end, an iterative procedure can be applied when select-
ing covariate and performing BPSM [49]. In our study, we
include a total of 14 covariates in the ﬁnal BPSM model as
a outcome of domain knowledge provided by our industry
collaborator and two iterations of model design. We present
the descriptive statistics of the covariates included in the
ﬁnal model in Table. 2.

4.3 Results

In this subsection, we present the results from Study I. First,
we show the propensity score inferred from a Bayesian

TABLE 2
Descriptive statistics of the target variable and covariates as inputs to Bayesian propensity score matching model, and a description of how the
variables are computed. Each variable is aggregated to the vehicle level and min-max scaled.

Variables
Target variable

Variable description

Group

Mean

Std.

9

Fuel consumption [g/km]

total fuel injected in engine / total distance

Covariates

Share of trip start at a high state-of-charge

number of trip where soc start >80%

Share of trip end at a low state-of-charge

number of trip where soc end <21%

Number of trips made on weekdays

number of trips taken place during weekdays

Number of trips made on weekends

number of trips taken place during weekends

Average trip distance [km]

total trip distance / total number of trips

Maximum trip distance [km]

longest trip occurred during the observation period

Average trip speed [km/h]

total trip distance / total trip duration

Maximum trip speed [km/h]

highest trip speed occurred during the observation period

Share of distance on ”hybrid”

distance driven when vehicle is in hybrid mode / total distance

Share of trips with a trailer attached

numbers of trip with trailer attached / total number of trips

Average number of engine starts in a trip

total occurrence of engine RPM >500 / total number of trips

Average ambient temperature [◦C]

average temperature measured at car during the period

Minimum ambient temperature [◦C]

minimum temperature measure at car during the period

Maximum ambient temperature [◦C]

maximum temperature measure at car during the period

Control
Treatment

Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment

0.391
0.354

0.356
0.397
0.258
0.120
0.290
0.225
0.269
0.190
0.301
0.343
0.278
0.240
0.575
0.624
0.637
0.640
0.956
0.987
0.034
0.034
0.169
0.176
0.371
0.372
0.497
0.563
0.388
0.374

0.155
0.123

0.177
0.178
0.155
0.150
0.167
0.153
0.173
0.154
0.132
0.124
0.193
0.186
0.110
0.117
0.125
0.135
0.103
0.033
0.081
0.075
0.112
0.176
0.084
0.071
0.135
0.150
0.101
0.099

network. Second, we present matched control and treatment
group with the two matching strategies discussed in the
previous section.

We implement a Bayesian logistic regression in Pyro [38]
following the generative process described in Algorithm
1. We set up a NUTS sampler for inference to infer the
posterior distribution with a single chain. We generate 3,000
samples of which 200 burn-in. The Brooks-Gelman-Rubin
convergence criteria of ˆR < 1.1 is met, at ˆR = 1.0003.
To triangulate the results, a variational inference model is
used. We use a multivariate normal distribution as a guide
for the variational inference model. We deﬁne 40,000 steps
for optimisation and the solver reaches a stable solution
after the ﬁrst 10,000 steps. Two inference methods return
similar posterior distributions and point estimates. We show
both methods in the online appendix attached and we will
only focus on reporting the inference results from the NUTS
sampler in this paper.

We assign a prior distribution β ∼ N (0, λα = 1) to each
regression coefﬁcient β, and similarly, α ∼ N (0, λβ = 1)
to the regression intercept α. Combining the priors, the
posterior, p(t, α, β|X, λα, λβ), is inferred. Then, the propen-
sity score e(X) is calculated following Equation. 10, before
matching, the mean propensity score in the control and
treatment group is 0.0319 and 0.0633 respectively. In each
group, the standard deviation of the propensity score is
0.0175 and 0.0309. We also quantify the uncertainty of the
propensity score from the posterior distribution. A visuali-
sation of the propensity score distribution before a matching

TABLE 3
Propensity scores in control and treatment groups, before and after a
matching is performed.

Propensity score

Before matching

Calliper matching (calliper = 0.05)

1-1 nearest neighbour matching

Group
Control
Treatment
Control
Treatment
Control
Treatment

Mean
0.0319
0.0633
0.0626
0.0633
0.0627
0.0633

Std.
0.0175
0.0309
0.0300
0.0309
0.0302
0.0309

is performed can be seen in Fig. 4, in which we plot the
propensity score distribution in the control (pc) and treat-
ment (pt) computed from the mean point estimates as well
as random samples from the posterior distribution of the
regression intercept and coefﬁcients.

A matching based on propensity score distance δe(X)n
is performed after the scores are computed. In this paper, we
perform matching with 1:1 nearest neighbour and caliper
matching method, the results are presented in Table 3. As
can be seen from the table, both matching methods return
similar outcome in this study, the mean propensity score
distance is 0.000757 and 0.000608 for the calliper matching
and 1-1 nearest neighbour matching respectively. First, a
calliper matching method is used with a speciﬁed maximum
propensity score distance, δe(X)n,max = 0.05, and every
treated sample returned a matched control sample. The
mean propensity scores in the control group were 0.0626

10

Fig. 4. Kernel density distribution of the propensity scores of the control (pc) and treatment (pt) groups calculated on the mean of posterior
distributions, and twenty-ﬁve values randomly sampled from the posterior distributions representing uncertainties.

after applying caliper matching.

Second, we apply 1-1 nearest neighbour matching
method. Similarly to calliper matching, 1-1 nearest neigh-
bour match will return one-to-one matched pairs, but the
matching is done without specifying maximum allowed
propensity score distance. The algorithm k-nearest neigh-
bours searches for one closest neighbour from the control
samples to the treated samples. The mean propensity score
in the control group is 0.0627 after 1-1 nearest neighbour
matching. Both methods ﬁnd the corresponding control
samples for the treated samples. The covariates balance
is assessed by comparing the descriptive statistics such as
variance and the empirical distribution of covariates in the
two groups. With a calliper matching, we found an average
of 4.1 % reduction in the covariates variance compared to
unmatched groups.

The treatment effect is analysed for both the calliper
matched and 1-1 nearest neighbour matched groups. The
average treatment effect is calculated as the mean difference
of the target variable between the control and treatment
groups, and all values are min-max scaled. The average
target variable is 0.379 and 0.391 for the control group when
matched with calliper and 1-1 nearest neighbour, respec-
tively. The average target variable is 0.355 in the treatment
group. The average treatment effect is -0.024 and -0.036
for the control group when matched with calliper and 1-1
nearest neighbour, respectively.

5 BAYESIAN DIFFERENCE-IN-DIFFERENCES
In this section, Bayesian difference-in-differences (BDID)
theory and our observational study is presented. We de-
scribe the theory and assumptions in the model formally,
we present our study setup and how the model is utilised
as an identiﬁcation strategy for analysing treatment effects
over time.

Fig. 5. An illustration explaining the Difference-in-Differences model and
how the average treatment effect (ATE) is estimated.

Difference-in-differences, proposed by Card and Krueger
[32], is a discrete time dynamic causal model. The model is
designed to identify and control time-dependent covariates
in observational studies, disregarding if the covariates are
measured or not [12], and model the average treatment
effect. Different from cross-sectional treatment effect esti-
mates, where the treatment effect is aggregated over time, or
time-series treatment effect estimates, where time is treated
as a continuous variable. This model can be used to measure
treatment effects in discrete time steps. We demonstrate the
concept of the model in Fig. 5, as shown in the ﬁgure, the
model replies on the parallel trend assumption, which im-
plies the treatment group and control group are assumed to
follow a similar trend for the target variable without a treat-

0.0000.0250.0500.0750.1000.1250.1500.1750.200Propensity score0510152025303540Densitypcpc,meanptpt,meanOutcomePre-treatmentPost-treatmentτ -1ττ +1ControlTreatmentObserved outcome intreatment groupObserved outcome incontrol groupParallel trendassumptionATEObserved state in bothgroups before treatmentX

t

y

τ

Fig. 6. A simpliﬁed directed acyclic graph showing the relationships of
treatment (t), target variable (y), covariates (X), and time dependent
latent variables summarised as τ .

ment. The factual outcome is the observed target variable
after the treatment is applied, the counterfactual outcome,
what would have happened if a treatment is never applied,
is inferred from the parallel trend assumption. Bayesian
difference-in-differences have been applied in studying the
inﬂuence of policy change on diabetes treatment quality
[12]. But there is no documented application of BDID in
software engineering to the best of our knowledge.

5.1 Theory

By including a group of untreated samples through passive
observations from the same time period as the treated sam-
ples, all time dependent covariates are implicitly controlled
for in a DID model, observed or not. Recall the directed
acyclic graph in Fig. 1, we now extend it to include a τ
variable to represent time dependent latent variables (Fig.
6), to conclude causal effect, both the covariates X and the
time dependent latent variables τ need to be adjusted for.
In the difference-in-differences model, the most important
assumption is the parallel trend assumption. It implies the
counterfactual - what would have happened in the absent
of a treatment - is an assumption inherently unobserved.
This assumption supports the exchangeability assumption
as stated in Eq. 2, i.e., the treatment assignment is not based
on the outcome, rather that the outcome is inﬂuenced by the
applied treatment. We can express this assumption formally,
note that we adopt the same notations from Section 3 and
4. Additionally, we use τ = {−1, 0, 1} to denote the time
periods before, during, and after a treatment is applied.

E[y0(1) − y0(−1)|t = 1] = E[y0(1) − y0(−1)|t = 0]

(13)

In Eq. 13, y0(−1) is the target variable with treatment
level 0 at time step τ = −1 (pre-treatment status in Fig.
5), and y0(1) is the target variable with treatment level 0 at
time step τ = 1 without a treatment being applied. We use
the superscript to represent the counterfactual status of the
treatment group, if a treatment is never applied. The parallel
trend assumption states that the target variable measured
from the control and treatment group will follow similar
trend over time, if no treatment is applied at τ , this is often
referred as the counterfactual outcome. The parallel trend
assumption can be check from observational data through
means such as data visualisation.

The

average

identiﬁed through
difference-in-differences (AT EDID) is estimated as the fol-
lowing,

treatment

effect

11

Algorithm 2 Bayesian difference-in-differences generative
process
Inputs: X covariates, λα prior distribution of α, λβ prior
distribution of β, treatment effect yn
1: Draw α ∼ N (α|0, λα)
2: Draw β ∼ N (β|0, λβ)
3: Draw (cid:15) ∼ N ((cid:15)|0, σ2)
4: for each vector of covariate x ∈ X do
5:
6: end for

Draw y ∼ N (y|t + τ + α + βT x, σ2)

AT EDID
= (E[y(1)|t = 1] − E[y(−1)|t = 1])−
(E[y(1)|t = 0] − E[y(−1)|t = 0])

(14)

That is, the difference of the target variables in the
treated group measured at time step τ = −1 and τ = 1,
subtracted with the difference in the control group mea-
sured during the same time period, namely, the difference
in differences. The target variable y can be estimated as a
linear regression from the data observed,

y ∼ t + τ + α + βX + (cid:15)

(15)

In the regression model, we also assign a dummy
variable t indicating if a treatment is applied. The latent
variables regression intercept α and coefﬁcient β have a
Gaussian distribution as prior, formally,

α ∼ N (α|0, λα)

(16)

Let us consider a total j numbers of covariates X, and

regression coefﬁcient is a vector of length j,

βj ∼ N (βj|0, λβj )

(17)

Moreover, since we cannot describe all variations in the
data with a linear model, a error term (cid:15) is included in the
model which represent the observation noise. We have,

(cid:15) ∼ N ((cid:15)|0, σ2)

(18)

The joint distribution is factorised as the equation below,
it is a straight forward application of Bay’s theorem. More-
over, we list the generative process for this join distribution
in Algorithm 2.

p(y, α, β|X, σ, λα, λβ)

= p(t) · p(α|λα) · p(βj|λβj ) ·

p(y|t, α, βj, σ, X)

(19)

N
(cid:89)

n=1

5.2 Study II: Seasonality effect

A large portion of software in the automotive domain is
inﬂuenced by the vehicle operating conditions, e.g., precipi-
tation, temperature, humidity, and icing [29], [30], [31], and
most importantly, the mobility needs of people. In the ﬁrst
case, these operating conditions are often seasonal and from
the diversity of vehicle markets today, they are difﬁcult to
predict beforehand and often requires the software to be

12

TABLE 4
Descriptive statistics of the target variable and covariates as inputs to Bayesian difference-in-differences model, and a description of how the
variables are computed. Each variable is aggregated to the vehicle level and min-max scaled.

Variables
Target variable

Variable description

Group

Mean (τ−1) Mean (τ1)

Energy consumption [Wh/km]

total electrical energy consumed / total distance

Control
Treatment

0.277
0.257

0.296
0.276

Covariates

Time period

Treatment

dummy, 0 for pre-treatment and 1 otherwise

dummy, 0 for control group and 1 for treated group

Average ambient temperature [◦C]

average temperature measured at car

Minimum ambient temperature [◦C]

minimum temperature measure at car

Maximum ambient temperature [◦C]

maximum temperature measure at car

Average trip distance [km]

total trip distance / total number of trips

Maximum trip distance [km]

longest trip occurred during the observation

Average coolant temperature [◦C]

coolant temperature measured at battery outlet

Average voltage battery discharge [Wh]

average battery energy discharge / number of trips

Average starting battery capacity [Wh]

average battery capacity measured at start of a trip

Average state-of-charge change [%]

average displacement of battery state-of-charge

Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment
Control
Treatment

0.667
0.954
0.630
0.557
0.429
0.745
0.257
0.314
0.229
0.270
0.559
0.821
0.612
0.506
0.635
0.531
0.609
0.484

0.318
0.465
0.294
0.272
0.484
0.626
0.192
0.310
0.322
0.392
0.333
0.396
0.587
0.582
0.608
0.616
0.579
0.562

evaluated longitudinally to cover a wider range of operating
conditions and increase conﬁdence in the conclusion. Addi-
tionally, it is naturally reasonable to assume that there are
seasonality effects that cannot be observed in an effective
manner nor can it be predicted, such as public events,
extreme weathers and so on. The travel demand of users can
be largely unpredictable similar to the operating conditions
of the vehicles. To adjust for time-dependent covariates over
a relatively long period of time, requires one to observe
a high number of covariates that are frequently unknown
when the study is designed. For example, external factors
such as cost of fuel, trafﬁc, and parking fare attributes to car
owners’ preferences on the travel mode [50], and most of
these factors are challenging to observe from the perspective
of the vehicle. Thus, in a longitudinal software evaluation,
there is a need for models that can control covariates even
when they are not observed.

Study II is designed to explore the scenario described
above, that is, (a) when the performance of the target
software treatment is highly dependent on external and
seasonal factors such as temperature, (b) and there are
potentially latent variables that cannot be observed in an
effective manner such as travel demands of individuals. The
study is designed to assess the applicability of the BDID
model in addressing the challenge and to verify the causal
assumption in BDID has real-life relevance. The software
deployed in study II is a battery management system for
electric vehicles. The battery inside of an electric vehicle has
an ideal window of operating temperatures, at the start of
a trip, the battery management system will have to either
warm up or cool down the battery into said window of op-
eration, this is done at the cost of driving range. Therefore,
the ideal preconditioning operation shall take place during
charging prior to the trip, utilising the electricity from the

grid instead of from the battery. Moreover, if there is a large
difference between the ambient temperature and ideal oper-
ating temperature, the energy required to precondition the
battery is naturally higher. In other words, it is reasonable
to expect a dynamic seasonality effect on the ﬁnal average
treatment effect. The treatment is a software solution that
controls and optimises the precondition of battery, and it is
expected to improve range as the vehicle no longer needs to
heat up or cool down the battery during driving operations.
Therefore, it decreases the energy consumption (measured
in Wh/km), and if the software performs as expected, the
average treatment effect should lower energy consumption
and extend range.

The study took place between the 1st of August 2021
to the 30th of September 2021, note that during our study,
there is a two-week duration that is a typical vacation period
in Sweden where the vehicles’ users reside. We choose
to conduct the study during this period as the weather
conditions are dynamic as well as the travel demands, to
further demonstrate the power of the BDID model. During
the period, we collected data from 24,286 trips and a total of
616,212 kilometres. The control group, similarly to study I, is
running the existing version of the software and we do not
intervene with the vehicles besides passive data collection.
The treatment group is randomly sampled from a larger
ﬂeet of vehicles leased to company employees, and these
vehicles received the battery preconditioning software as
described above. There are in total 176 vehicles included in
the study, similarly to study I, we discard data generated by
vehicles with odometer less than 100 kilometres and all trips
with average speed higher than 200 kilometres per hour
as it exceeds the digital speed limiter implemented in the
vehicles. We have in total 9 covariates, and their descriptive
statistics are presented in Table. 4. All values are presented

13

Fig. 7. Target variable y measured before the treatment (τ−1), when the
treatment is applied (τ ), and after the treatment (τ1), for the control and
the treatment groups, y is min-max scaled.

min-max scaled due to nondisclosure agreement with our
industry collaborator.

While the Bayesian difference-in-differences model es-
sentially allows a post facto analysis of time dependent treat-
ment effect analysis, it is based on the assumption of parallel
trend, as formally deﬁned in Equation 13. In practise, this
trend states that the control and the treatment group should
follow similar trend over time, if no treatment is applied, im-
plying the difference in between the groups comes from the
unobserved time dependent confounding factors. For BDID
to identity the treatment effect, the assumption needs to
be empirically validated through for example visualisation,
i.e., by comparing the target variable over time between the
treatment and the control group before an intervention is
introduced. Another validation method is to ﬁt the BDID
model before and after the treatment is applied, to test
if the functional form of the counterfactual is correct [51].
Empirically, some observe the samples pre-treatment for as
long as possible, to discovery any unknown or underlying
trend over time [52]. Furthermore, some matching is re-
quired when selecting the control group to compare with the
treated group. In practise, this matching process can be done
by selecting untreated samples that are as similar as possible
to the treated samples, such as vehicle model and engine
types, markets, and so on. To ensure the two groups are
comparable, we select vehicles with the same vehicle type
and have the same battery capacity, and all of the vehicles
are registered and driven in Sweden.

5.3 Results

In this subsection, the results from study II is presented. We
show the BDID regression model inferred from a Bayesian
network, we illustrate the parallel trend assumption, and
the ﬁnal average treatment effect of the software change.

First we inspect the parallel trend assumption through
visualisation and the result is presented in Fig. 7. First,
we compute the average of the target variable before a
treatment is applied, at discrete time step τ−1 for both
control and treatment groups, and the target variable at the
time when the new software is introduced, τ . As can be seen
from the ﬁgure, the target variable from both groups follow

Fig. 8. Posterior distribution of the regression coefﬁcients β, β is ordered
as Table. 4, and the regression intercept α.

a upward trajectory. Last, we compute the average of the
target variable after the treatment is applied at discrete time
step τ1. The visualisation result conﬁrms our assumption
that both the control and treatment groups follow similar
trend over time, if no treatment is applied, and the target
variable observed in the treatment group changes trajectory
after the treatment application.

The BDID regression is implemented in Pyro, similarity
to the Bayesian logistic regression in study I. We follow
the generative process as prescribed in Algorithm 2. The
MCMC NUTS sampler is set in Pyro with 3.000 samples and
200 burn-ins with two chains. The Brooks-Gelman-Rubin
convergence criteria of ˆR < 1.1 is met ( ˆR = 1.0007 for
α, ˆRβ,mean = 1.0066 for all β, and ˆR = 0.999 for the error
term σ). We attach the trace plots in the online appendix.

We assign a weakly informative prior distribution of
α ∼ N (0, 1) and β ∼ N (0, 1) to the regression intercept
and coefﬁcients to introduce scale information to regularise
inference, in this case, min-max scalded covariates. We infer
the posterior distribution p(y, α, β|X, σ, λα, λβ), in Fig. 8,
we present the posterior distribution of the regression inter-
cept (α) and the regression coefﬁcients β. As can be seen,

101Time step0.2600.2650.2700.2750.2800.2850.2900.295Target variablecontroltreatment020beta_1025beta_205beta_3010beta_4010beta_5010beta_6020beta_7010beta_80.02.5beta_90.02.5beta_1002beta_110.60.40.20.00.20.40.605alphaTABLE 5
Average energy consumption (Wh/km) for the control and the
treatment group at each time step.

τ−1
τ1
Change

Control
205.30
218.93
13.627

Treatment
190.45
204.36
13.915

Difference
- 14.85
- 14.57
-0.280

the posterior distribution of the two dummies variables
indicating the time period and if a treatment is applied
(β1 and β2) are informative of the treatment outcome as
expected. While the posterior distributions for covariates
describing the high voltage battery activity level such as
total energy discharge and state-of-charge change (β9 to
β10), contribute positively to average energy consumption,
however, the uncertainty of the effect is high.

Last, we include a difference in differences average
treatment effect analysis following Equation. 14. First, we
compute the difference of expected target variable E[y] at
time step τ−1 between the control (t = 0) and the treatment
(t = 1) groups, this value can be interpreted as the preexist-
ing differences between the groups as a result of unobserved
confounding effects. Second, we calculate the difference of
E[y] between the control and treatment group at time step
τ1, this difference is a sum of the preexisting differences and
the treatment effect if the parallel assumption holds. The
results are presented in Table. 5.

6 BAYESIAN REGRESSION DISCONTINUITY DESIGN

In this section, we present the Bayesian regression disconti-
nuity design (BRDD) theory and observational study III that
is designed to demonstrate the use case of BRDD for evalu-
ating automotive software. The theory and assumptions of
the model is presented formally along with the algorithm
for Bayesian inference. We present the study III, the setup,
data collection method, and how BRDD is used as a strategy
for identifying continuous covariate dependent treatment
assignment.

Regression discontinuity design, proposed by [33], is a
causal modelling approach aiming to determine the treat-
ment effect when the treatment assignment is confounded
by one continuous covariate by assigning a cut-off point.
This continuous covariate is referred as the assignment
variable. In Fig. 9, we illustrate the principle of RDD. Obser-
vations of the target variable is made around the threshold,
in this case, average treatment effect can be inferred without
the need of a randomised experiment. In practise, visualis-
ing of the assignment variable and the target variable is a
simple yet powerful tool to inspect their relationship [53].
While RDD allows inference of treatment effect with the
absent of randomisation, the model alone does not explicitly
or implicitly conclude causality as it does not identify other
unobserved confounding effects. Moreover, the RDD model
essentially infer the treatment effect with a single covariate
X = x, in that perspective, the model has a limited degree
of external validity. However, the RDD model is similar to
a randomised experiment with bias below 0.01 standard
deviations on average especially analysed with the Bayesian

14

Fig. 9. An illustration explaining the Regression Discontinuity Design
model, and how the average treatment effect is estimated.

approach [27], indicating a high internal validity. In our
study III, we investigate the scenario when a software is
only useful in reducing the fuel consumption of the vehicle,
if the average trip distance of the given vehicle is over a
certain threshold, without ex-ante randomisation.

6.1 Theory

In [33], RDD is discussed in the context of regression, and
in this subsection, we will describe it in the potential out-
comes framework provided the conditional exchangeability
assumption holds as formulated in Equation. 3. We illustrate
the relationship between variables in a RDD in Fig. 10. As
can be seen, the outcome y is inﬂuenced by the assignment
variable as well as the predetermined cutoff point c. As
mentioned previously, a RDD does not automatically elim-
inate other confounding factors in the system, we illustrate
that with Z in the directed acyclic graph. The fundamental
concept of RDD is that the treatment assignment is de-
terministic by a covariate X (we call this the assignment
variable) with a ﬁxed threshold, the assignment variable X
is assumed to be correlated to the target variable y, and
their correlation is smooth. Under this assumption, any
discontinuity of the target variable y as a function of X
is interpreted to be the causal treatment effect around the
predetermined threshold. Let X = c be the predetermined
cut-off point of the assignment variable with c being an
arbitrarily determined value, formally,

E[y|X = x, t = 1], and E[y|X = x, t = 0]

(20)

are continuous in x.
This assumption also implies that the probability distri-
bution of y given the covariate X = x is smooth. This as-
sumption is stronger than needed, as the continuity without
a treatment effect is only expected around the cut-off point
X = c and the assumption above covers such a scenario.
Different from a matching problem, the requirement for
overlap requires control and treated samples to have all
possible combinations of the covariates, in a DID model,
for all values of x, the propensity of treatment assignment is
either 0 or 1, i.e., on either side of the cut-off point c. We call

Assignment variableTarget variable ControlTreatmentTreatment  effectControl groupTreatment groupX

c

Z

t

y

Fig. 10. A simpliﬁed directed acyclic graph showing the relationships of
treatment (t), target variable (y), assignment variable (X), the cut-off
point (c), and other confounding factors (Z).

this a sharp design, as opposed to fuzzy design. In practise,
a fuzzy design might be more attractive, as it is reasonable
to include samples close to either side of the cut-off point.

Without the need of extrapolating due to the lack of
overlap, at the cut-off point X = c, we can infer the av-
erage treatment effect from regression discontinuity design
(AT ERDD) as,

AT ERDD = E[y|X = c, t = 1] − E[y|X = c, t = 0]

(21)

That is, the difference between average observed target
variable y with or without the treatment t = {0, 1}, at a
given cut-off point X = c. This average treatment effect can
be estimated as we have made the smoothness assumption
in Equation. 20. We would like to empathise that although
we demonstrate a linear regression for the prediction of
target variable in a BRDD, a polynomial regression can
be applied to handle more complex relations between the
assignment variable and the target variable. When the
smoothness assumption holds, the target variable y can be
predicted using a simple linear regression for the sharp
design at X = c, stated as following centred around the
cut-off point,

y ∼ α + β1(x − c) + β2t + β3(x − c)t + β4Z + (cid:15)

(22)

where, the α is the regression intercept, βj are the regres-
sion coefﬁcients. They are both latent variable inferred from
a Bayesian network. t is a dummy variable indicating if a
treatment has been applied, and (cid:15) represent the linear noise
in the model. We assign a Gaussian distribution as a prior
to the regression intercept and coefﬁcients, namely,

and,

α ∼ N (α|0, λα)

βj ∼ N (βj|0, λβj )

(23)

(24)

An error term (cid:15) is included in the model which represent
the observation noise as a linear model has its limitations for
describing the noisy reality. We have,

(cid:15) ∼ N ((cid:15)|0, σ2)

(25)

The joint distribution is factorised as the blow applying
Baye’s law. We describe the generative process for this join
distribution in Algorithm 3.

p(y, α, βj|Z, t, x, c, σ, λα, λβj )

= p(t) · p(α|λα) · p(βj|λβj ) ·

N
(cid:89)

n=1

p(y|Z, t, x, c, α, βj, σ)

(26)

15

Algorithm 3 Bayesian regression discontinuity design gen-
erative process
Inputs: X covariates, λα prior distribution of α, λβ prior
distribution of β, treatment effect yn
1: Draw α ∼ N (α|0, λα)
2: for each β ∈ βj do
3:
4: end for
5: Draw (cid:15) ∼ N ((cid:15)|0, σ2)
6: Draw y ∼ N (y|α+β1(x−c)+β2t+β3(x−c)t+β4Z, σ2)

Draw β ∼ N (β|0, λβ)

6.2 Study III: Covariate dependent treatment assign-
ment

In absence of randomisation, assuming automotive software
functions to be independent from their operation environ-
ment or usage by the customers is a na¨ıve approach for
estimating the treatment effect of software changes. To that
end, the performance or even the activation of a certain
automotive function is dictated by the usage, in other words,
we frequently run into the situation in which a covariate de-
termines the treatment assignment. To understand the per-
formance of this type of software is important for the follow-
ing two reasons. First, it brings insights on the usefulness
of a given software feature, validating assumptions made
during development against how the product is actually
utilised. Second, it allows the development organisations
to evaluate the software effectiveness in conditions that are
most determining of the effect. In study III, we present a
case that illustrates the importance of causal inference when
the treatment effect is strongly dependent on a covariate.

A plug-in hybrid vehicle, is a type of electriﬁed vehicle
with two sets of propulsion, combustion engine and electric
motors. This type of vehicles usually have limited pure
electrical range, and to maximise the beneﬁt of eclectic
drive such as high efﬁciency for low speed driving and
zero direct emission; automotive manufacturers typically
have a number of control software solutions to optimise
the distribution of the electrical and chemical energy on a
given trip. A simple version of the optimisation strategy is
to prioritise the electrical energy whenever available and
deplete the battery ﬁrst before using the combustion engine.
This type of strategy usually works well when the driver
is expected to travel less distance than the electrical range,
and not on highways where the combustion engine works
more efﬁciently than the electrical motor. Alternatively, the
optimisation can be done through a prediction of trip dis-
tance and destination – if the trip is predicted to be farther
than the electrical range, the car will not prioritise the use
of electrical energy and deplete the battery early in the trip,
with the rationality that the drivers is predicted to enter the
city later where direct emission from the combustion engine
is undesirable. Thus, the assignment of this software is
determined by the trip distance by design. The performance
of this type of software function is highly dependent on how
the cars are driven, more speciﬁcally, on the trip distance.
Thus, to evaluate such a software feature, we chose a cutoff
point of the assignment variable trip distance, at around the
designed electrical range of the vehicle where the software
is expected to have the most impact, and apply the BRDD

model for treatment effect inference and modelling.

Study III is conducted in Sweden, on a ﬂeet of 50 plug-
in hybrid vehicles. The study took place between the 19th
of October 2020 to the 28th of Febuary 2021, and during
which, 12,231 numbers of trips are observed and the vehicles
have driven a total of 191,552 kilometres. The software is
designed so that if the predicted trip distance is less than the
electrical range, the car will prioritise and use the electrical
energy ﬁrst. If not, the vehicle will optimise the energy us-
age between the electrical motor and the combustion engine
according to the predicted trip distance and destination. We
apply the same data post-processing logic as study I and II,
namely, data collected from brand new vehicles and trips
with higher than possible average speed are excluded from
the model. In study III, the target variable is the average
fuel consumption, and the assignment variable is the total
trip distance.

There are two important assumptions in a regression dis-
continuity design. First, we assume unobserved covariates
do not affect the treatment effect or assignment. This is a
strong assumption, similarly to what is previously discussed
for BPSM and most causal inference problems, to satisfy
this assumption, it requires prior knowledge to how the
software interact with the users and inputs from the domain
experts. The second assumption of BRDD is the expectation
of the target variable E[y] is continuous with respect to the
assignment variable X. Mathematically speaking, function
f (x) continuity at x = c can be determined if limx→c f (x)
exist, and limx→c f (x) = f (c). A continuity check should
not be performed on observational data which is per deﬁni-
tion discontinuous, instead, the continuity assumption can
be check with a density test, as suggested by [54]. Last but
not least, the choice of cut-off point X = c, requires that
the assignment mechanism to be known to the develop-
ment teams. In practise, the cut-off point might not be a
sharp differentiation but rather a bandwidth, which can be
determined either through the design intend of the software
or through observational data collected prior to a treatment
is introduced.

6.3 Results

In this section, we present the results from BRDD. As dis-
cussed in the previous subsection, there are other covariates
that could potentially confound the treatment effect, in Z.
In this analysis, to adjust for the covariates, we condition
on one covariant that is the total displaced state-of-charge,
Z ∈ Z. This covariate indicates how much battery is used
during a given trip, naturally, if a trip distance is ﬁxed, the
more battery is used, the less the fuel consumption there
is. Thus, to ensure the trips are comparable disregard the
software treatment, we only look at E[y|Z > 90], they are
trips during which the battery has been depleted. The cut-
off point is arbitrarily determined at c = 60, which is the
approximated pure electrical range of the vehicle model we
are observing. We do not min-max scale the value in Fig.
11, due to that the cut-off point of the assignment variable
represents a physical measurement and we choose to artic-
ulate the physical meaning through presenting the unscaled
value. As a min-max scaled value is difﬁcult to interpret,
to reﬂect the physical meaning of the measurement while

16

Fig. 11. Target variable measured before and after the cut-off point (c =
60), with respect to the assignment variable.

maintaining our conﬁdentiality agreement, we remove the
units of the measured target variable instead. Two linear
regression is ﬁtted on either side of the cut-off point, to
illustrate a discontinuity of the regression line at cut-off as
a representation of the software treatment effect, as can be
seen in Fig. 11, at c = 60.

Following Algorithm 3, we implement the BRDD regres-
sion model in Pyro. The MCMC NUTS solver is set with 2
chains of 2,000 samples each and we discard the ﬁrst 200
steps as warm-up steps. The model, in wide format, has the
following four input features, x − c (so that the regression
is ﬁtted centred around the cut-off point), with x being
the assignment variable of trip distance and c = 60, t =
{0, 1} (to effectively control the regression model), (x − c)t
((x − c)t = 0 for the controlled group, and 1 otherwise),
and the change of the state-of-charge of the battery named
Z. The convergence criteria ˆR < 1.1 is met at ˆR = 1.0004
for α, ˆRβ,mean = 1.0017, ˆR = 1.0000 for the error term
σ. The trace plots are attached in the online appendix. For
the prior distributions, we have a weakly informative prior
of α ∼ N (0, 1), and we select a more informative prior
for β ∼ N (0, 0.5). We choose a weakly regularising prior
for the standard deviation σ ∼ Half-Cauchy(0, 5), as it ap-
proximate uniform distribution and it is weakly informative
near 0. We plan to start with a non-informative prior for the
error term and adjust if the solver does not converge. The
solver meets the convergence criteria with the priors men-
tioned above. The posterior distribution from this Bayesian
network, p(y, α, β|Z, t, x, c, λα, λβ, σ), is inferred. The pos-
terior distribution of the regression intercept, α, returned a
Gaussian distribution centred around αmean = 0.626, with a
standard deviation of αstd = 0.0056. Similarly, the posterior
distribution of the error term (cid:15) is a Gaussian distribution
centred around σ = 0.213 with standard deviation of
0.0010. We present the posterior distributions in Fig. 12.

The outcome for BRDD consists of two linear regression
lines at either side of the cut-off point. Using the posterior
distribution, the regression expression (yc) to the left hand
side of the cut-off point can be expressed as,

404550556065707580Assignment variableTarget variablecontroltreatment17

in the BOAT framework. In order to select samples in the
control group for observation that are as similar as the
treated samples as possible, a matching process is recom-
mended [51]. The judgement on ”as similar as possible”
is a judgement that cannot be made without existing data
on the cohorts and domain knowledge. Likewise, domain
knowledge is required for selecting the cut-off point for the
assignment variable in a regression discontinuity design.

The requirement on domain knowledge implies that
causal inference from observational studies need manual
input when applied in software engineering practises. While
randomised online experiments can be automated to a large
extend, as demonstrated in the SaaS domain [1], [2], [3],
causal inference with observational data is a process that
would potentially require more manual efforts. The extra
overhead of efforts from developers could potentially pose
a challenge in implementing causal inference in combination
with observational studies in automotive software engineer-
ing. Thus, the application of BOAT framework on a larger
scale cannot be done without some form of data-driven
causal discovery methods.

Fig. 12. Posterior distribution of the regression coefﬁcients β, and the
regression intercept α.

yc ∼ α + β1x + β4Z + (cid:15)

(27)

7.2 Extension to BOAT

and the regression expression for the treated samples (yt)
to the right hand side of the cut-off point can be expressed
as,

yt ∼ (α + β2) + (β1 + β3)x + β4Z + (cid:15)

(28)

The average treatment effect AT ERDD is inferred fol-
lowing Equation. 21, we ﬁnd that, at the cut-off point
(X = c), the expected target variable treated and untreated
differ by AT ERDD = yt(x = c)−yc(x = c) = −1.1954. This
is an unbias estimation of the local conditional treatment
effect. As can be seen in Fig. 11, the discontinuity at the
cut-off point can be interpreted as the treatment effect.

7 DISCUSSION

In this section, we provide a discussion on the advantages
and limitations of the BOAT framework from the perspec-
tive of causal inference and piratical applications in the
automotive domain.

7.1 Causal assumptions and domain knowledge

Causality cannot be inferred from observational data alone
[16], as behind every causal conclusion, there are some com-
plementary causal assumptions that might not be testable
empirically. Before adjusting for confounding biases, some
judgements must be made based on domain knowledge as
also discussed by [16], [35], [36], [37].

Take propensity score matching as an example, different
from a randomisation process, in which all covariates will be
balanced in the control and the treatment groups observed
or not, causal inference with covariate adjustment requires
the covariates to be observed [37]. Performing propensity
score matching in combination with observational study, re-
quires a set of carefully chosen covariates which rely heavily
on domain knowledge. To that end, similar requirements on
domain knowledge is also experienced with other models

In this subsection, we offer a short discussion on the poten-
tial outlook of the BOAT framework. As shortly discussed in
Section. 3, the BOAT framework does not cover all scenarios
in causal inference of observational studies in the automo-
tive domain. Since there are a few more techniques that can
be applied when inferring causal treatment effect without
randomised experiments. In this subsection, we offer a dis-
cussion on our decision to why some of the models are not
included in the BOAT framework as if now. These models,
useful in many domains as literature reports, we have yet to
ﬁnd their feasible applications in the automotive sector.

First, the positive decision to whether there is a need
to infer the latent variable is intentionally left out from
the ﬂow chart, when a latent variable needs to be inferred,
methods such as instrumental variable can be applied [34].
Instrumental variable method uses a latent variable to ex-
plain the correlation to the error term. The method should
account for unexpected behaviour between variables, how-
ever, the explanation it provides cannot be interpreted with
a physical meaning. In many automotive software where
interpretability is considered crucial, especially for devel-
opment organisations to take design decisions. Moreover,
instrumental variable method has the tendency to produce
bias results when the sample is small, which is a known
limitation in the automotive domain.

Second, a popular school of causal inference method,
structural causal model and do-calculus [16] offers a com-
prehensive approach to causal inference provided the causal
structure is known. This causal structure is represented in a
DAG, such as the trivial example in Fig. 1. Each component
in a DAG has their graphical and numerical representa-
tions, then through the language of do-calculus, for example
p(Y |do(T ), X), we can represent intervention and infer
treatment effect from a DAG. In order for a structural causal
model to be effective, the structure of the model, i.e., the
DAG, needs to be learnt either through domain-knowledge
or though a data-driven causal discovery process. The for-
mal is time consuming and potentially subjected to biases of

05beta_10100beta_205beta_3050beta_40.20.10.00.10.20.30.40.50100alphaindividuals, the latter requires large amount of data yet does
not address limitations such as sampling bias, measurement
error, and confounding effects [40].

Finally, there are other methods addressing preexisting
differences between the control and the treatment groups,
such as inverse propensity weighting. While achieving sim-
ilar objective as propensity score matching (adjusting for
inverse propensity weighting is a
confounding factors),
parametric method and it is known to be creating imbalance
groups when the sample size is insufﬁcient.

8 CONCLUSION

In this paper, we introduce the BOAT framework for soft-
ware engineering in the automotive domain, enabling online
evaluation of the software in a causal fashion when a fully
randomised experimentation is impossible, undesired, or
unethical. Applying the Bayesian causal inference models,
we demonstrate how a causal conclusion can be drawn
in absent of randomisation utilising the high ﬂexibility of
Bayesian inference towards sample size, as demonstrated
in other areas of science [12], [13], [14], [15]. Combining
theory with practise, we include three illustrative cases
from the automotive domain for further enforce the need
of causal inference in software engineering. The three cases
are conducted together with our industry collaborator, we
introduce three software to a ﬂeet of vehicles driven by
real-world customers. We relate the causal assumptions
to scenarios experienced in practise, aiming to provide a
guideline on when and how to better apply the causal
modelling for inferring the software effects from different
software evaluation needs.

We provide a decision making ﬂowchart along with
the three causal inference models included in the BOAT
framework. The ﬂowchart is design with the objective of
guiding development organisations on which causal infer-
ence models should be used to address their correspond-
ing challenges in real life. In the BOAT framework, we
include three models, they are, (1) Bayesian propensity score
matching for generating balanced control and treatment
groups without randomisation,
(2) Bayesian difference-
in-differences for controlling unobserved seasonal factors
over time, (3) Bayesian regression discontinuity design for
analysing the treatment effect when treatment assignment
is determined by a continuous covariate. All of the three
models and their assumptions have their implications in
practise, as experienced from the automotive domain when
attempting to evaluate software without randomisation, in
this work, we provide a formal discussion of the inference
models, as well as their corresponding real world implica-
tions and applications. The three cases are designed together
with software engineering teams in our case company, to
simulate challenges experienced when evaluating software
online without randomisation. With the development teams,
we introduce new software treatment to a ﬂeet of vehicles,
conduct data collection, and use the empirical data as inputs
to the BOAT framework, additionally, we assess the causal
assumption in relation to the empirical cases. We ﬁnd the
causal models in the BOAT framework to be highly applica-
ble in automotive software engineering, and they enable the
development organisations to evaluate software changes in

18

an online and causal manner. Furthermore, we ﬁnd there is a
strong dependency on domain knowledge when designing
an online observational study as the cause-and-effect is not
always known, and when validating some of the causal
assumptions empirically.

In our future work, we aim to incorporate causal dis-
covery process when designing an online experiment or
observational study, since we cannot always assume the
cause-and-effect of a system is known [55]. Moreover, we
plan to explore data-driven causal discovery methods to
inform and potentially automate the design of experiments,
with the objective of increasing the efﬁciency and the effec-
tiveness of online experimentation in automotive software
engineering.

ONLINE APPENDIX

We attached an online appendix for the Bayesian models.
The online appendix can be found as a Jupyter Notebook
via the following link: github.com/yuchueliu/BOAT.

ACKNOWLEDGEMENT

This work is supported by Volvo Cars, by the Swedish
Strategic vehicle research and innovation programme (FFI),
and by Chalmers University of Technology.

REFERENCES

[1] A. Deng, Y. Xu, R. Kohavi, and T. Walker, “Improving the sensi-
tivity of online controlled experiments by utilizing pre-experiment
data,” in Proceedings of the sixth ACM international conference on Web
search and data mining - WSDM '13. ACM Press, 2013.

[2] D. Tang, A. Agarwal, D. O’Brien, and M. Meyer, “Overlapping
experiment infrastructure: More, better, faster experimentation,”
in Proceedings 16th Conference on Knowledge Discovery and Data
Mining, Washington, DC, 2010, pp. 17–26.

[3] H. Xie and J. Aurisset, “Improving the sensitivity of online con-
trolled experiments,” in Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining.
ACM, aug 2016.

[4] D. I. Mattos, J. Bosch, H. H. Olsson, A. M. Korshani, and J. Lantz,
“Automotive A/B testing: Challenges and lessons learned from
practice,” in 2020 46th Euromicro Conference on Software Engineering
and Advanced Applications (SEAA).

IEEE, aug 2020.

[5] Y. Liu, J. Bosch, H. H. Olsson, and J. Lantz, “An architecture for
enabling A/B experiments in automotive embedded software,”
in 2021 IEEE 45th Annual Computers, Software, and Applications
Conference (COMPSAC).
IEEE, jul 2021.

[6] Y. Liu, D. I. Mattos, J. Bosch, H. H. Olsson, and J. Lantz, “Size
matters? or not: A/B testing with limited sample in automotive
embedded software,” in 2021 47th Euromicro Conference on Software
Engineering and Advanced Applications (SEAA).

IEEE, sep 2021.

[7] ——, “Bayesian propensity score matching in automotive em-
bedded software engineering,” in 2021 28th Asia-Paciﬁc Software
Engineering Conference (APSEC).

IEEE, dec 2021.

[8] F. Giaimo, H. Andrade, and C. Berger, “The automotive take
on continuous experimentation: A multiple case study,” in 2019
45th Euromicro Conference on Software Engineering and Advanced
Applications (SEAA).

IEEE, aug 2019.
[9] D. I. Mattos and Y. Liu, “On the use of causal graphical models for
designing experiments in the automotive domain,” Apr. 2022.
[10] F. Giaimo, H. Andrade, and C. Berger, “Continuous experimen-
tation and the cyber–physical systems challenge: An overview of
the literature and the industrial perspective,” Journal of Systems and
Software, vol. 170, p. 110781, dec 2020.

[11] Y. Xu and N. Chen, “Evaluating mobile apps with A/B and quasi
A/B tests,” in Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, aug
2016.

[12] J. Normington, E. Lock, C. Carlin, K. Peterson, and B. Carlin,
“A bayesian difference-in-difference framework for the impact of
primary care redesign on diabetes outcomes,” Statistics and Public
Policy, vol. 6, no. 1, pp. 55–66, jan 2019.

[13] L. Li and E. T. Donnell, “Incorporating bayesian methods into
the propensity score matching framework: A no-treatment effect
safety analysis,” Accident Analysis & Prevention, vol. 145, p. 105691,
sep 2020.

[14] D. S. Lee and D. Card, “Regression discontinuity inference with
speciﬁcation error,” Journal of Econometrics, vol. 142, no. 2, pp. 655–
674, feb 2008.

[15] S. Chib and L. Jacobi, “Bayesian fuzzy regression discontinuity
analysis and returns to compulsory schooling,” Journal of Applied
Econometrics, vol. 31, no. 6, pp. 1026–1047, aug 2015.

[16] J. Pearl, “Causal inference in statistics: An overview,” Statistics

Surveys, vol. 3, no. none, jan 2009.

[17] D. I. Mattos, J. Bosch, and H. H. Olsson, “Challenges and strate-
gies for undertaking continuous experimentation to embedded
systems: Industry and research perspectives,” in Lecture Notes in
Business Information Processing. Springer International Publishing,
2018, pp. 277–292.

[18] P. R. Rosenbaum and D. B. Rubin, “The central role of the propen-
sity score in observational studies for causal effects,” Biometrika,
vol. 70, no. 1, pp. 41–55, 1983.

[19] P. W. Holland, “Statistics and causal inference,” Journal of the
American Statistical Association, vol. 81, no. 396, pp. 945–960, dec
1986.

[20] D. B. Rubin, “Bayesian inference for causal effects: The role of
randomization,” The Annals of Statistics, vol. 6, no. 1, jan 1978.
[21] D. Westreich and S. R. Cole, “Invited commentary: Positivity in
practice,” American Journal of Epidemiology, vol. 171, no. 6, pp. 674–
677, feb 2010.

[22] C. A. Furia, R. Feldt, and R. Torkar, “Bayesian data analysis in
empirical software engineering research,” IEEE Transactions on
Software Engineering, pp. 1–1, 2019.

[23] R. Torkar, R. Feldt, and C. A. Furia, “Bayesian data analysis in
empirical software engineering: The case of missing data,” in
Contemporary Empirical Methods in Software Engineering. Springer
International Publishing, 2020, pp. 289–324.

[24] D. I. Mattos, J. Bosch, and H. H. Olsson, “Statistical models for the
analysis of optimization algorithms with benchmark functions,”
IEEE Transactions on Evolutionary Computation, vol. 25, no. 6, pp.
1163–1177, 2021.

[25] K. H. Brodersen, F. Gallusser, J. Koehler, N. Remy, and S. L.
Scott, “Inferring causal impact using bayesian structural time-
series models,” The Annals of Applied Statistics, vol. 9, no. 1, mar
2015.

[26] M. D. Hoffman and A. Gelman, “The no-u-turn sampler: Adap-
tively setting path lengths in hamiltonian monte carlo,” The Journal
of Machine Learning Research, 2011.

[27] D. D. Chaplin, T. D. Cook, J. Zurovac, J. S. Coopersmith, M. M.
Finucane, L. N. Vollmer, and R. E. Morris, “The internal and
external validity of the regression discontinuity design: A meta-
analysis of 15 within-study comparisons,” Journal of Policy Analysis
and Management, vol. 37, no. 2, pp. 403–429, feb 2018.

[28] S. Geneletti, A. G. O'Keeffe, L. D. Sharples, S. Richardson, and
G. Baio, “Bayesian regression discontinuity designs: incorporating
clinical knowledge in the causal analysis of primary care data,”
Statistics in Medicine, vol. 34, no. 15, pp. 2334–2352, mar 2015.
[29] N. K. Ahmed and J. Kapadia, “Seasonality effect on electric vehicle
miles traveled in electriﬁed vehicles,” SAE International Journal of
Alternative Powertrains, vol. 6, no. 1, pp. 47–53, mar 2017.

[30] J. Bao, P. Liu, and S. V. Ukkusuri, “A spatiotemporal deep learning
approach for citywide short-term crash risk prediction with multi-
source data,” Accident Analysis & Prevention, vol. 122, pp. 239–254,
jan 2019.

[31] M. B. Kamel and T. Sayed, “Accounting for seasonal effects on
cyclist-vehicle crashes,” Accident Analysis & Prevention, vol. 159, p.
106263, sep 2021.

[32] D. Card and A. Krueger, “Minimum wages and employment: A
case study of the fast food industry in new jersey and pennsylva-
nia,” American Economic Review, vol. 84, 04 1993.

19

[34] G. W. Imbens and J. D. Angrist, “Identiﬁcation and estimation of
local average treatment effects,” Econometrica, vol. 62, no. 2, p. 467,
mar 1994.

[35] D. B. Rubin, “Using propensity scores to help design observational
studies: Application to the tobacco litigation,” Health Services and
Outcomes Research Methodology, vol. 2, no. 3/4, pp. 169–188, 2001.
[36] Z. Xu and J. D. Kalbﬂeisch, “Propensity score matching in ran-
domized clinical trials,” Biometrics, vol. 66, no. 3, pp. 813–823, nov
2009.

[37] E. A. Stuart, “Matching methods for causal inference: A review
and a look forward,” Statistical Science, vol. 25, no. 1, pp. 1–21, feb
2010.

[38] E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer, N. Pradhan,
T. Karaletsos, R. Singh, P. A. Szerlip, P. Horsfall, and N. D.
Goodman, “Pyro: Deep universal probabilistic programming,” J.
Mach. Learn. Res., vol. 20, pp. 28:1–28:6, 2019. [Online]. Available:
http://jmlr.org/papers/v20/18-403.html

[39] K. Peffers, T. Tuunanen, M. Rothenberger, and S. Chatterjee, “A
design science research methodology for information systems
research,” Journal of Management Information Systems, vol. 24, pp.
45–77, 01 2007.

[40] C. Glymour, K. Zhang, and P. Spirtes, “Review of causal discovery
methods based on graphical models,” Frontiers in Genetics, vol. 10,
jun 2019.

[41] Hevner, March, Park, and Ram, “Design science in information
systems research,” MIS Quarterly, vol. 28, no. 1, p. 75, 2004.
[42] M. Carcary, “Design science research: the case of the it capability
maturity framework (it-cmf),” The Electronic Journal of Business
Research Methods, vol. 9, pp. 109–118, 01 2011.

[43] P. Runeson and M. H ¨ost, “Guidelines for conducting and reporting
case study research in software engineering,” Empirical Software
Engineering, vol. 14, no. 2, pp. 131–164, dec 2008.

[44] E. A. Stuart and N. S. Lalongo, “Matching methods for selection
of participants for follow-up,” Multivariate Behavioral Research,
vol. 45, no. 4, pp. 746–765, aug 2010.

[45] P. C. Austin, “Optimal caliper widths for propensity-score match-
ing when estimating differences in means and differences in pro-
portions in observational studies,” Pharmaceutical Statistics, vol. 10,
no. 2, pp. 150–161, mar 2011.

[46] B. B. Hansen, “Full matching in an observational study of coaching
for the SAT,” Journal of the American Statistical Association, vol. 99,
no. 467, pp. 609–618, sep 2004.

[47] J. Wang, “To use or not to use propensity score matching?”

Pharmaceutical Statistics, vol. 20, no. 1, pp. 15–24, aug 2020.

[48] M. A. Brookhart, S. Schneeweiss, K. J. Rothman, R. J. Glynn,
J. Avorn, and T. St ¨urmer, “Variable selection for propensity score
models,” American Journal of Epidemiology, vol. 163, no. 12, pp.
1149–1156, apr 2006.

[49] P. R. Rosenbaum and D. B. Rubin, “Reducing bias in observational
studies using subclassiﬁcation on the propensity score,” Journal of
the American Statistical Association, vol. 79, no. 387, pp. 516–524, sep
1984.

[50] K. Gao, M. Shao, K. W. Axhausen, L. Sun, H. Tu, and Y. Wang, “In-
ertia effects of past behavior in commuting modal shift behavior:
interactions, variations and implications for demand estimation,”
Transportation, jul 2021.

[51] A. Kahn-Lang and K. Lang, “The promise and pitfalls of
differences-in-differences: Reﬂections on 16 and Pregnant and other
applications,” Journal of Business & Economic Statistics, vol. 38,
no. 3, pp. 613–620, apr 2019.

[52] J. D. Angrist and J.-S. Pischke, “The credibility revolution in
empirical economics: How better research design is taking the con
out of econometrics,” Journal of Economic Perspectives, vol. 24, no. 2,
pp. 3–30, may 2010.

[53] G. W. Imbens and T. Lemieux, “Regression discontinuity designs:
A guide to practice,” Journal of Econometrics, vol. 142, no. 2, pp.
615–635, feb 2008.

[54] J. McCrary, “Manipulation of the running variable in the regres-
sion discontinuity design: A density test,” Journal of Econometrics,
vol. 142, no. 2, pp. 698–714, feb 2008.

[55] B. Scholkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner,
A. Goyal, and Y. Bengio, “Toward causal representation learning,”
Proceedings of the IEEE, vol. 109, no. 5, pp. 612–634, may 2021.

[33] D. L. Thistlethwaite

“Regression-
discontinuity analysis: An alternative to the ex post facto experi-
ment.” Journal of Educational Psychology, vol. 51, no. 6, pp. 309–317,
1960.

and D. T. Campbell,

