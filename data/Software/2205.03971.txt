Private Eye: On the Limits of
Textual Screen Peeking via Eyeglass Reï¬‚ections
in Video Conferencing

Yan Longâˆ—, Chen Yanâ€ , Shilin Xiaoâ€ , Shivan Prasadâˆ—, Wenyuan Xuâ€ , and Kevin Fuâˆ—
âˆ—Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA
â€  College of Electrical Engineering, Zhejiang University, Hangzhou, China
{yanlong, shprasad, kevinfu}@umich.edu, {yanchen, xshilin, wyxu}@zju.edu.cn

2
2
0
2

p
e
S
4
1

]

R
C
.
s
c
[

2
v
1
7
9
3
0
.
5
0
2
2
:
v
i
X
r
a

Abstractâ€”Personal video conferencing has become a new norm
after COVID-19 caused a seismic shift from in-person meetings
and phone calls to video conferencing for daily communications
and sensitive business. Video leaks participantsâ€™ on-screen infor-
mation because eyeglasses and other reï¬‚ective objects unwittingly
expose partial screen contents. Using mathematical modeling and
human subjects experiments, this research explores the extent
to which emerging webcams might leak recognizable textual
and graphical
information gleaming from eyeglass reï¬‚ections
captured by webcams. The primary goal of our work is to
measure, compute, and predict the factors, limits, and thresholds
of recognizability as webcam technology evolves in the future.
Our work explores and characterizes the viable threat models
based on optical attacks using multi-frame super resolution
techniques on sequences of video frames. Our models and
experimental results in a controlled lab setting show it is possible
to reconstruct and recognize with over 75% accuracy on-screen
texts that have heights as small as 10 mm with a 720p webcam.
We further apply this threat model to web textual contents
with varying attacker capabilities to ï¬nd thresholds at which
text becomes recognizable. Our user study with 20 participants
suggests present-day 720p webcams are suï¬ƒcient for adversaries
to reconstruct textual content on big-font websites. Our models
further show that the evolution towards 4K cameras will tip
the threshold of text leakage to reconstruction of most header
texts on popular websites. Besides textual targets, a case study
on recognizing a closed-world dataset of Alexa top 100 websites
with 720p webcams shows a maximum recognition accuracy of
94% with 10 participants even without using machine-learning
models. Our research proposes near-term mitigations including a
software prototype that users can use to blur the eyeglass areas of
their video streams. For possible long-term defenses, we advocate
an individual reï¬‚ection testing procedure to assess threats under
various settings, and justiï¬es the importance of following the
principle of least privilege for privacy-sensitive scenarios.

I. INTRODUCTION

Online video calls have become ubiquitous as a remote
communication method, especially since the recent COVID-
19 pandemic that caused almost universal work-from-home
policies in major countries [24], [27], [31] and made video
conference a norm for companies and schools to accommodate
interpersonal communications even after the pandemic [6],
[15], [44], [52].

While video conferencing provides people with the con-
venience and immersion of visual interactions, it unwittingly
reveals sensitive textual information that could be exploited
by a malicious party acting as a participant. Each video

Fig. 1. The optical emanations of the victimâ€™s screen are reï¬‚ected by
eyeglasses, captured by the victimâ€™s webcam, and streamed to the adversary,
which can then be used to reconstruct the screen contents. The experimental
setup (a) with a laptop built-in webcam (b) (red box, 720p), an external
Logitech webcam (c) (green box, 1080p), and a Nikon DSLR (d) (blue box,
4K) helps us predict the future ï¬delity of the attacks as video conferencing
technologies evolve.

participantâ€™s screen could contain private information. The
participantâ€™s own webcam could capture this information when
it is reï¬‚ected by the participantâ€™s eyeglasses and unwittingly
provide the information to the adversary (Figure 1). We refer
to this attack as a webcam peeking attack. Furthermore, it
is important to understand the consequences and limits of
webcam peeking attacks as adversary capability will only
continue to increase with improvements to resolution, frame
rate, and more.

Previous work shows that similar attacks exploiting op-
tical reï¬‚ection oï¬€ nearby objects in controlled setups are
feasible, such as observing teapots on a desk with high-end
digital single-lens reï¬‚ex (DSLR) cameras and telescopes at
a distance [25], [26]. The challenge and characterization of
peeking using the more ubiquitous webcams, however, are
qualitatively diï¬€erent due to the lower quality images of
present-day webcams. The lower-quality webcam images are
caused by unique types of distortions, namely the shot and
ISO noise due to insuï¬ƒcient light reception, and call for new
image-enhancing techniques. In addition, new mathematical
models and analysis frameworks are needed to understand the
threat model of webcam peeking attack. Finally, this new threat
model requires a dedicated evaluation to clarify the potential

 
 
 
 
 
 
threats and mitigations to the average video conference user.
There are many types of media that can leak over optical
reï¬‚ections, including text and graphics. We focus on textual
leakage in this work as itâ€™s a natural starting point for
measurable recognizability and modeling of the fundamental
baseline of information leakage, but also provides insights
into the leakage of non-textual information such as inferring
displayed websites through recognizing graphical contents on
the screen. We seek to answer the following three major
questions: îˆ½1: What are the primary factors aï¬€ecting the
capability of the webcam peeking adversary? îˆ½2: What are the
physical limits of the adversaryâ€™s capability in the present day
and the predictable future, and how can adversaries possibly
extend the limits? îˆ½3: What are the corresponding threats of
webcam peeking against cyberspace targets and the possible
mitigations against the threats?

To answer îˆ½1, we propose a simpliï¬ed yet reasonably accu-
rate mathematical model for reï¬‚ection pixel size. The model
includes factors such as camera resolution and glass-screen
distance and enables the prediction of webcam peeking limits
as camera and video technology evolve. By using the complex-
wavelet structural similarity index as an objective metric for
reï¬‚ection recognizability, we also provide semi-quantitative
analysis for other physical factors including environmental
light intensity that aï¬€ect signal-to-noise ratio of reï¬‚ected light.
To answer îˆ½2, we analyze the distortions in the webcam
images and propose multi-frame super resolution reconstruc-
tion for eï¬€ective image enhancing to extend the limits. We
then gather eyeglass reï¬‚ection data in optimized lab environ-
ments and evaluate the recognizability limits of the reï¬‚ections
through both crowdsourcing workers on Amazon Mechanical
Turk and optical character recognition models. The evaluation
shows it is possible with over 75% accuracy to recognize texts
that have a physical height of 10 mm with a 720p webcam.
To answer îˆ½3, we focus on web textual targets to build
a benchmark that enables meaningful comparisons between
present-day and future webcam peeking threats. We ï¬rst map
the limits derived from the model and evaluations to web
textual contents by surveying previous reports on web text
size and manually inspecting fonts in 117 big-font websites.
Then, we conduct a user study with 20 participants and play
a challenge-response game where one author acts as an adver-
sary to infer HTML contents created by other authors. Results
of the user study suggest that present-day 720p webcams can
peek texts in the 117 big-font websites and future 4K webcams
are predicted to pose threats to header texts from popular
websites. We investigated the underlying factors enabling
easier webcam peeking in the user study by analyzing the
correlation between adversary recognition accuracy and mul-
tiple factors. We found, for example, user-speciï¬c parameters
including browser zoom ratio play a more important role than
the glass-screen distance. Besides texts, we also explored the
feasibility of recognizing websites through graphical contents
with 10 participants and observed accuracies as high as 94% on
recognizing a closed-world dataset of Alexa top 100 websites.
Finally, we discuss possible near-term mitigations including

adjusting environmental lighting and blurring the glass area
in software. We also envision long-term solutions following
an individual reï¬‚ection assessment procedure and a principle
of least privilege. In summary, the goal of this work is to
provide theoretical foundation and benchmark for the study
on emerging webcam peeking threats with evolving webcam
technologies and the development of securer video conferenc-
ing infrastructures. We summarize our main contributions as
follows:

âˆ™ Our work quantiï¬es the limits and primary factors that
predict the degree of information leakage from webcam
peeking by using theoretical modeling and experimenta-
tion. This characterization helps predict future unknown
vulnerabilities tied to the limits of evolving webcam
technologies that do not yet exist.

âˆ™ A benchmark centering on web textual targets that enables
comparisons of webcam peeking threats. Our benchmark-
ing methodology builds upon web text design conventions
and a 20-participant user study on present-day cameras
such that the benchmark can be applied to both hypothet-
ical and emerging cameras in the coming years.

âˆ™ Analysis on near-term mitigations

including using
software-based blurring ï¬lters and changing physical se-
tups as well as possible long-term defenses by proactive
testing and following a principle of least privilege. Our
analysis investigates the potential eï¬€ectiveness and imple-
mentation methods of diï¬€erent protections.

II. THREAT MODEL & BACKGROUND

A. Threat Model

In this work, we study the webcam peeking attack during
online video conference, where the adversary and the victim
are both participants. We assume the device the victim uses
to join the video conference consists of a display screen and
either a built-in or an external webcam that is mounted on the
top of the screen as in most cases, and the victims wear glasses
with a reï¬‚ectance larger than 0, i.e., at least a portion of the
light emanated by the monitor screen can be reï¬‚ected from
the glasses to the webcams. We do not enforce constraints
on the devices used by the adversary. When the adversary
launches the attack, we assume the victim is facing the screen
and webcam in the way that the screen emanated light has a
single-reï¬‚ection optical path into the webcam through glasses
lens outer surface. We do not assume the adversary has any
control or information on the victimâ€™s device.

We assume that the victimâ€™s up-link video stream is enabled
during the attack, and the adversary can acquire the down-link
video stream of the victim. The adversary can achieve that by
either directly intercepting the down-link video stream data,
or recording the victimâ€™s video with the video conferencing
platform being used or even third-party screen recording
services. Since the webcam peeking attack does not require
active interaction between the victim and the adversary, the
adversary does not need to attempt a real-time attack but can
store the video recording and analyze the videos oï¬€-line.

2

B. Glasses

The most common types of glasses that people wear in an
video conferencing setting are prescription glasses [41] and
blue-light blocking (BLB) glasses [11], [51]. BLB glasses
can either have prescription with BLB coating or be non-
prescription (ï¬‚at). The reï¬‚ectance and curvature of glass
lenses are the two most important characteristics in the process
of reï¬‚ecting screen optical emanations.

Reï¬‚ectance. Reï¬‚ectance of a lens surface is the ratio
between the light energy reï¬‚ected and the total energy incident
on a surface [5]. Reï¬‚ectance is wavelength-dependent. The
higher the reï¬‚ectance, the more light can be reï¬‚ected to and
captured by a webcam.

Curvature. Curvature of a lens surface represents how
much it deviates from a plane. The concepts of curvature,
radius, and focal length of a eyeglass lens are used interchange-
ably in diï¬€erent occasions and are related by: ğ¶ğ‘¢ğ‘Ÿğ‘£ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ =
1âˆ•ğ‘…ğ‘ğ‘‘ğ‘–ğ‘¢ğ‘  = 2âˆ•ğ¹ ğ‘œğ‘ğ‘ğ‘™ğ¿ğ‘’ğ‘›ğ‘”ğ‘¡â„. Smaller curvature leads to
larger-size reï¬‚ections. Both the outer and inner surfaces of a
lens can reï¬‚ect, but the outer surface often has smaller curva-
ture and thus produce better quality reï¬‚ections (Appendix A).
This paper refers to the eyeglass lens curvature/radius/focal
length as that of the outer surface if not speciï¬ed otherwise.

C. Digital Camera Imaging System

Digital cameras have sensing units uniformly distributed
on the sensor plane, each of which is a Charge-coupled
Device (CCD) or Complementary Metal-oxide-semiconductor
(CMOS) circuit unit that converts the energy of the photons it
receives within a certain period of time, i.e., the exposure time,
to an amplitude-modulated electric signal. Each sensing unit
then corresponds to a â€œpixelâ€ in the digital domain. The quality
of a digital image to human perception is mainly determined
by its pixel resolution, color representation, the amount of
received light that are of our interests, and various imaging
noise. The 2 key imaging parameters that are closely related
to webcam peeking attack are described below.

Exposure Time. Theoretically,
the more photons will hit

the longer the exposure
time,
the imaging sensors and
thus there can be potentially more light of interest captured.
The images with a longer exposure time will generally be
brighter. The downside of having a longer exposure time is
the aggravated motion blur when imaging an moving object.
ISO Value. The ISO value represents the ampliï¬cation
factor of the photon-induced electrical signals. In darker
conditions, the user can often make the images brighter by
increasing the ISO value. The downside of having higher ISO
is the simultaneous ampliï¬cation of various imaging noise.

D. Text Size Representations

It is important to select proper representations of text size in
both digital and physical domains since the size of the smallest
recognizable texts is the key metric for webcam peeking limits.
When texts are digital, i.e., in the victimâ€™s software such as
browsers and in the webcam image acquired by the adversary,
we use point size and pixel size to represent the text size

Fig. 2.
(Upper) The captured images of the reï¬‚ections. Comparing with
the ideal reï¬‚ections, additional distortions exist that undermine the image
recognizability. (Lower) The estimated ideal reï¬‚ections in the feasibility test
corresponding to letters with a height of 80, 60, 40, 20, 10 mm respectively.
The images are subjected to aliasing when enlarged.

respectively. In the physical domain, i.e., when the texts are
displayed on usersâ€™ screens as physical objects, we use the
cap height of the fonts and the physical unit mm to represent
the size as it is invariant across diï¬€erent computer displays
and enable quantitative analysis of the threats. Cap height is
the uniform height of capitalized letters when font style and
size are speciï¬ed and is thus usually used as a convenient
representation of physical text size and the base for other font
parameters [22], [23].

III. WEBCAM PEEKING THROUGH GLASSES

In this section, we start with a feasibility test that reveals the
3 key building blocks of the webcam peeking threat model,
namely (1) reï¬‚ection pixel size, (2) viewing angle, and (3) light
signal-to-noise ratio (SNR). For the ï¬rst two building blocks,
we develop a mathematical model that quantiï¬es the related
impact factors. For light SNR, we analyze one major factor
it encompasses, i.e., image distortions caused by shot noise,
and investigate using multi-frame super resolution (MFSR)
to enhance reï¬‚ection images. We will analyze other physical
factors that aï¬€ect light SNR in Section IV-D. Experiments are
conducted with the Acer laptop with its built-in 720p webcam,
the pair of BLB glasses, and the pair of prescription glasses
described in Appendix B.

A. Feasibility Test

We conduct a feasibility test of recognizing single alphabet
letters with a similar setup as in Figure 1. A mannequin
wears the BLB glasses with a glass-screen distance of 30
cm. Capital letters with diï¬€erent cap heights (80, 60, 40, 20,
10 mm) are displayed and captured by the webcam. Figure
2 (upper) shows the captured reï¬‚ections. We ï¬nd that the
5 diï¬€erent cap heights resulted in letters with heights of
40, 30, 20, 10, and 5 pixels in the captured images. As
expected,
texts represented by fewer pixels are harder to
recognize. The reï¬‚ection pixel size acquired by adversaries is
thus one key building block of the characteristics of webcam
peeking attack that we need to model. In addition, Figure
2 (lower) shows the ideal reï¬‚ections with these pixel sizes
by resampling the template image. Comparing the two, we
notice small-size texts are subjected to additional distortions
besides the issue of small pixel resolution and noise caused
by the face background, resulting in bad signal-to-noise ratio

3

0.99940.97290.67810.29020.999940 Pixels30 Pixels20 Pixels10 Pixels5 Pixels0.77830.57540.54140.30270.2234TABLE I
PARAMETERS FOR MODELING REFLECTION PIXEL SIZE

Notation
â„ğ‘œ
â„ğ‘ 
ğ‘ ğ‘
â„ğ‘–
ğ‘ƒ
ğ‘
ğ‘Š
ğ‘“
ğ‘‘ğ‘œ
ğ‘‘ğ‘–
ğ‘“ğ‘”

Parameter
Physical size (cap height) of the object on the screen
Physical size of the objectâ€™s projection on the sensor
Pixel size of the imaged object
Physical size of the objectâ€™s virtual image
Physical size of a single imaging sensor pixel
Number of pixels the camera has in the dimension
Physical size of the imaging sensor in the dimension
Camera focal length
Distance between screen and glasses
Distance between glasses and virtual image
Focal length of the glasses convex outer surface

(SNR) of the textual signals. To quantify the diï¬€erence, we
use the complex-wavelet structural similarity (CWSSIM) score
between the template and resampled/captured images as an
objective metric of the reï¬‚ection quality (Appendix C), and
show the scores under each image. The diï¬€erences show that
the SNR of reï¬‚ected light corresponding to the textual targets
is another key building block we need to characterize. Finally,
we notice that when we rotate the mannequin with an angle
exceeding a certain threshold, the webcam images do not
contain the displayed letters on the screen anymore. It suggests
that the viewing angle is another critical building block of the
webcam peeking threat model which acts as an on/oï¬€ function
for successful recognition of screen contents. In the follow
sections, we seek to characterize these three building blocks.

B. Reï¬‚ection Pixel Size

In the attack, the embodiment of textual targets undergoes
a 2-stage conversion process: digital (victim software) â†’
physical (victim screen) â†’ digital (adversary camera). In the
ï¬rst stage, texts speciï¬ed usually in point size in software by
the user or web designers are rendered on the victim screen
with corresponding physical cap heights. In the second stage,
the on-screen texts get reï¬‚ected by the glass, captured by the
camera, digitized, and transferred to the adversaryâ€™s software
as an image with certain pixel sizes. Generally, more usable
pixels representing the texts enable adversaries to recognize
texts more easily. The key is thus to understand the mechanism
of point size â†’ cap height â†’ pixel size conversion.

Point Size â†’ Cap Height. Mapping between digital point
size and physical cap height is not unique but dependent on
user-speciï¬c factors and software. The conversion formula for
most web browsers can be summarized as follows:

â„ğ‘œ =

4
3

ğ‘ğ‘¡ â‹…

ğ»ğ‘ ğ‘ğ‘Ÿ
ğ‘ğ‘œğ‘ 

â‹… ğ‘ ğ‘œğ‘  â‹… ğ‘ ğ‘ â‹… ğ‘Ÿğ‘ğ‘ğ‘

(1)

where â„ğ‘œ is the physical cap height of the text, 4
ğ‘ğ‘¡ is the
3
number of display hardware pixels most web browsers use
to render the text given a point size ğ‘ğ‘¡, ğ»ğ‘ ğ‘ğ‘Ÿ is the physical
height of the screen, ğ‘ğ‘œğ‘  is the screen resolution on the height
dimension set in the OS which can be equal to or smaller than
the maximum supported resolution, ğ‘ ğ‘œğ‘  and ğ‘ ğ‘ are the OS and
browser zoom/scaling ratios respectively, and ğ‘Ÿğ‘ğ‘ğ‘ is the ratio

Fig. 3. The model of reï¬‚ection pixel size. To better depict the objects, the
sizes are not drawn up to scale. The screen overlaps with the webcam lens
and is omitted in the ï¬gure.

[22], [23].

between the cap height and the physical point size which is
on average 2
3

Cap Height â†’ Pixel Size. We would like to remind the
readers that we only use pixel size to represent the size of
texts living in the images acquired by the adversary1. Figure
3 shows the model for this conversion process. To simplify
the model, we assume the glasses lens, screen contents, and
the webcam are aligned on the same line with the same angle.
The result of this approximation is the loss of projective trans-
formation information, which only causes small inaccuracies
for reï¬‚ection pixel size estimation in most webcam peeking
scenarios. Figure 3 only depicts one dimension out of the
horizontal and vertical dimensions of the optical system, but
can be used for both of the dimensions. In this work we focus
on the vertical dimension for analysis, i.e., the reï¬‚ection pixel
size we discuss is the height of the captured reï¬‚ections in
pixels. We summarize the parameters of this optical imaging
system model in Table I. By trigonometry, we know

â„ğ‘ 
ğ‘“

= â„ğ‘–
ğ‘‘ğ‘œ+ğ‘‘ğ‘–

â§
âª
â„ğ‘  = ğ‘ ğ‘ğ‘ƒ â‡’ ğ‘ ğ‘ = â„ğ‘–
â¨
ğ‘‘ğ‘œ+ğ‘‘ğ‘–
âª
ğ‘ƒ = ğ‘Š
â©
ğ‘

â‹… ğ‘“
ğ‘Š

â‹… ğ‘

(2)

As pointed out in Section II-B, the reï¬‚ective outer surface
of glasses are mostly convex mirrors which shrink the size of
the imaginary object â„ğ‘– and also decreases ğ‘‘ğ‘– compared to an
ideal ï¬‚at mirror. To calculate the reï¬‚ection pixel size ğ‘ ğ‘ in
this case, we can use the convex mirror equations [39]

+ 1
(âˆ’ğ‘‘ğ‘–)

= 1
ğ‘‘ğ‘œ

1
(âˆ’ğ‘“ğ‘”)
â„ğ‘–
â„ğ‘œ

= ğ‘‘ğ‘–
ğ‘‘ğ‘œ

â§
âª
â¨
âª
â©

where ğ‘“ğ‘” is the focal length of the convex mirror which is half
of the radius of the glasses lens and is deï¬ned to be positive.
Plugging the above equations into Equation 2 we can then get

ğ‘ ğ‘ =

â„ğ‘œğ‘“ğ‘”
ğ‘‘2
ğ‘œ + 2ğ‘‘ğ‘œğ‘“ğ‘”

â‹…

ğ‘“
ğ‘Š

â‹… ğ‘,

(3)

The term ğ‘“
ğ‘Š

of typical laptop webcams can be estimated
to be in the range 1.1 âˆ’ 1.4 (see Appendix D). With the Acer

1Since web/software designers sometimes also directly specify text size in
ğ‘ƒğ‘¡ in Equation 1), the two pixel sizes can be easily confused

pixel size ( 4
3
without explanation.

4

Imaging  SensorWebcam   LensGlassesOuter SurfaceOn-screen Object     Virtual ImageOn-screen   TexthoOn-screen   TexthoImaging  SensorWebcam   LensGlassesOuter SurfaceOn-screen Object     Virtual ImageTABLE II
THE PREDICTED FEASIBLE ATTACK RANGES FOR THE VIEWING ANGLE.
Type
Pres: All Page + Horizontal
Pres: Center + Horizontal
Pres: All Page + Vertical
Pres: Center + Vertical
BLB: All Page + Horizontal
BLB: Center + Horizontal
BLB: All Page + Vertical
BLB: Center + Vertical

Measurement
Â±17â—¦
Â±8â—¦
Â±13â—¦
Â±5â—¦
Â±25â—¦
Â±13â—¦
Â±19â—¦
Â±10â—¦

Theoretical
Â±15â—¦
Â±5â—¦
Â±9â—¦
Â±3â—¦
Â±20â—¦
Â±10â—¦
Â±14â—¦
Â±8â—¦

laptop and BLB glasses and a glass-screen distance of ğ‘‘ğ‘œ = 30
cm, the estimated vertical pixel size of a 20 mm-tall object
displayed on screen is in the range of 9.2 âˆ’ 11.7 pixels, which
matches with the 10 pixels found in the feasibility test and
veriï¬es the accuracy of the model despite the approximation
we made.

C. Viewing Angle

To model the eï¬€ect of viewing angle and the range of
angles that enables webcam peeking attack, we model the lens
as spherical with a radius 2ğ‘“ğ‘”. A detailed derivation of the
viewing angle model can be found in Appendix E. We consider
two cases of successful peeking with a rotation of the glass
lens. The ï¬rst case All Page claims success as long as there
exists a point on the screen whose emitted light ray can reach
camera. The second case Center claims success only if the
contents at the center of the screen has emitted lights that can
be reï¬‚ected to camera. Table II summarizes the calculated
theoretical angle ranges and the measured values. Both the
theoretical model and measurements show that the webcam
peeking attack is relatively robust to human positioning with
diï¬€erent head viewing angles, which is validated later by the
user study results (Section V-B).

D. Image Distortion Characterization

Generally, the possible distortions are composed of imaging
systemsâ€™ inherent distortions and other external distortions.
Imaging systemsâ€™ inherent distortions mainly include out-of-
focus blur and various imaging noise introduced by non-
ideal camera circuits. Such inherent distortions exist in camera
outputs even when no user interacts with the camera. External
distortions, on the other hand, mainly include factors like
motion blur caused by the movement of active webcam users.
User Movement-caused Motion Blur. When users move
in front of their webcams, reï¬‚ections from their glasses move
accordingly which can cause blurs in the camera images.
User motions can be decomposed into two components,
namely involuntary periodic small-amplitude human tremor
that is always present [33], and intentional non-periodic large-
amplitude movement that is occasionally caused by random
events such as a user moving its head to look aside. By
considering user motions as displacements of â„ğ‘œ and utilizing
Equation 3, the number of blurred pixels ğ›¿ğ‘ can be approxi-
mated by:

ğ›¿ğ‘ =

ğ›¿ğ‘‡ ğ‘“ğ‘”
ğ‘‘2
ğ‘œ + 2ğ‘‘ğ‘œğ‘“ğ‘”

â‹…

ğ‘“
ğ‘Š

â‹… ğ‘

5

where ğ›¿ğ‘‡ is the motion displacement amplitude within the
exposure time of a frame.

For tremor-based motion, existing research suggests the
mean displacement amplitude of dystonia patientsâ€™ head
tremors is under 4 mm with a maximum frequency of about
6 Hz [34]. Since dystonia patients have stronger tremors than
healthy people, this provides an estimation of the tremor am-
plitude upper bound. With the example glass in Section III-B
and a 30 fps camera, the estimated pixel blur is under 1 pixel.
Such a motion blur is likely to aï¬€ect recognition of extremely
small reï¬‚ections. Intentional motion is not a focus of this
work due to its random, occasional, and individual-speciï¬c
characteristics. We will experimentally involve the impacts
of intentional user motions in the user study by letting users
behave normally.

Distortion Analysis. To observe and analyze the dominant
types of distortions, we recorded videos with the laptop
webcam and a Nikon Z7 DSLR [17] representing a higher-
quality imaging system. The setup is the same as the feasibility
test except we tested with both the still mannequin and a
human to analyze the eï¬€ects of human tremor. Figure 13 (a)
shows the comparison between the ideal reï¬‚ection capture and
the actual captures in three consecutive video frames of the
webcam (1st row) and Nikon Z7 (2nd row) when the human
wears the glasses. Empirically, we observed the following
three key features of the video frames in this setup with both
the mannequin and human:

âˆ™ Out-of-focus blur and tremor-caused motion blur are gen-
erally negligible when the reï¬‚ected texts are recognizable.
âˆ™ Inter-frame variance: The distortions at the same position
of each frame are diï¬€erent, generating diï¬€erent noise
patterns for each frame.

âˆ™ Intra-frame variance: Even in a single frame, the distor-

tion patterns are spatially non-uniform.

One key observation is that the captured texts are subjected
to occlusions (the missing or faded parts) caused by shot
noise [19] when there is insuï¬ƒcient number of photons hitting
the sensors. This can be easily reasoned in light of the
short exposure time and small text pixel size causing reduced
photons emitted and received. In addition, other common
imaging noise such as Gaussian noise gets visually ampliï¬ed
by relatively higher ISO values due to the bad light sensitivity
of the webcam sensors. We call such noise as ISO noise.
Both the two types of distortions have the potential to cause
intra-frame and inter-frame variance. The shot and ISO noise
in the webcam peeking attack plays on a see-saw with an
equilibrium point posed by the quality of the camera imaging
sensors. It suggests that the threat level will further increase
(see comparison between the webcam and Nikon Z7â€™s images
in Figure 13) as future webcams get equipped with better-
quality sensors at lower costs.

E. Image Enhancing with MFSR.

The analysis of distortions calls for an image reconstruction
scheme that can reduce multiple types of distortions and
tolerate inter-frame and intra-frame variance. One possible

controlled lab environment and serve as the foundation for
the analysis in Section V.

A. Experimental Setup

Equipment. We collected all data with the aforementioned
Acer laptop as the victim device, and another Samsung laptop
[18] as the adversaryâ€™s device. The two laptops were in a
lab environment with WiFi network connection. The victim
laptop was measured to have an internet download speed of
246 Mbps and upload speed of 137 Mbps while those for the
adversary laptop were 144 Mbps and 133 Mbps respectively.
We used two pairs of glasses, i.e., the pair of BLB glasses and
prescription glasses.

Data Collection. We asked a person to wear the glasses
and sit in front of the victim laptop. The glass-screen distance
was chosen to be 40 cm which was also found to be close
to the average distance in the user study (see Figure 8 (b)).
The screen brightness was 100%. To estimate the limits of
recognition, we used an environmental light intensity of 100
lux to generate the best reï¬‚ections. We then displayed single
capital letters (26 letters) on the victim screen with diï¬€erent
heights ranging from 20 mm to 7 mm. The victim and
adversary laptops had a Zoom [21] session with a video
resolution of 1280Ã—720. For each display of the letters, we
recorded a 3s video of the victimâ€™s images on the adversary
laptop. We then used 8 consecutive frames starting from 1s for
MFSR reconstruction and generated one corresponding image
for each video. We generated 208 images in total for the 2
glasses each with 4 diï¬€erent sizes.

Recognizability Evaluation. In order to evaluate the recog-
nizability of the reconstructed single-letter images and avoid
potential bias introduced by the authorsâ€™ prior knowledge
of the reï¬‚ections, we acquired recognition accuracy by (1)
using multiple SOTA pre-trained deep-learning OCR models
including Google Tesseract and Keras CRNN, and (2) con-
ducting a survey2 (Appendix I) on Amazon Mechanical Turk
(AMT) [9]. For the AMT study, we collected answers from 25
crowdsourcing workers for each reconstructed image, and thus
collected 5200 answers in total. We showed to the workers
all reconstructed images in a randomized manner without
providing them with any information on the original letters
on the screen. We asked the workers to provide 3 best guesses
of the single letter in each reconstructed image. They were
allowed to input the same answer for multiple guesses if they
feel conï¬dent in a guess, or if they have no clue for making
subsequent guesses. The recognizability of the texts in the
reconstructed images is then represented by the recognition
accuracy, i.e., correctly recognized number of letters over the
total number of letters in each case.

(a) Comparison between single frames and the MFSR-reconstructed
Fig. 4.
images with 4 diï¬€erent MFSR approaches. The MFSR images are recon-
structed with the 8 frames shown at
the top. The AKR-based approach
generally produces the best reconstruction results in our task of reï¬‚ection
image reconstruction. (b) The improvement of reï¬‚ection reconstruction quality
as the number of frames used for MFSR increases.

method is to reconstruct a better-quality image from multiple
low-quality frames. Such reconstruction problem is usually
deï¬ned as multi-frame super resolution (MFSR) [59]. The
basic idea is to combine non-redundant information in multiple
frames to generate a better-quality frame.

We tested 3 common light-weight MFSR approaches that
do not require a training phase, including cubic spline in-
terpolation [59], fast and robust MFSR [37], and adaptive
kernel regression (AKR) based MFSR [42]. Tests results on
the reï¬‚ection images show that
the AKR-based approach
generally yields better results than the other two approaches
in our speciï¬c application and setup. All the three approaches
outperform a simple averaging plus upsampling of the frames
after frame registration, which may be viewed as a degraded
form of MFSR. An example of the comparison between the
diï¬€erent methods and the original 8 frames used for MFSR are
shown in Figure 4 (a). We thus use the AKR-based approach
for the following discussions.

One parameter to decide for the use of webcam peeking
is the number of frames used to reconstruct the high-quality
image. Figure 4 (b) shows the CWSSIM score improvement of
the reconstructed image over the original frames with diï¬€erent
number of frames used for MFSR when a human wears the
glasses to generate the reï¬‚ections. Note that increasing the
number of frames do not monotonically increase the image
quality since live usersâ€™ occasional intentional movements can
degrade image registration eï¬€ectiveness in the MFSR process
and thus undermine the reconstruction quality. Based on the
results, we empirically choose to use 8 frames for the following
evaluations. In addition, the improvement in CWSSIM scores
also validates that MFSR-resulted images have better quality
than most of the original frames. We thus only consider
evaluation using the MFSR images in the following sections.

IV. REFLECTION RECOGNIZABILITY & FACTORS

B. Recognizability vs. Size & Letter

In this section, we evaluate the recognizability limits of
reï¬‚ected texts enhanced by the MFSR method given a speciï¬c
set of webcam, glasses, and advantageous environmental con-
ditions. We then investigate the impact of the most signiï¬cant
factors. The evaluations in this section are performed in a

Figure 5 shows the recognition accuracy with the BLB and
prescription glasses respectively with diï¬€erent letter sizes. The
AMT accuracy for each letter size is calculated by including

2Institutional Review Board (IRB) No.HUM00208544

6

OriginalFramesMFSRAKR-basedFast&RobustCub. SplineInterp.Averaging248163264128256512Number of Frames100120140160180CWSSIM ScoreImprovement (%)248163264128256512Number of Frames100120140160180CWSSIM Score Improvement (%)(a)(b)Fig. 5. The recognition accuracy of letters in diï¬€erent sizes with (a) the BLB glasses and (b) the prescription glasses. Although the pair of BLB glasses
has higher reï¬‚ectance than the prescription glasses, the prescription glasses enables reading smaller on-screen texts because of its smaller curvature leading
to larger reï¬‚ection pixel size. Note that the conclusion is device-speciï¬c and cannot be applied to general BLB-prescription glass comparison. Humans are
found more capable in recognizing the reï¬‚ected texts than SOTA OCR models.

all 25 answers for all the 26 letters, i.e., with a denominator of
25 Ã— 26 = 650. We picked 4 representative letter sizes for each
pair of glasses respectively, and show the top 1, 2, and 3 recog-
nition accuracy. we also use error bars to show the standard
deviations. The SOTA OCR models performed considerably
worse than AMT workers. We believe the main reason is that
data distribution in the modelsâ€™ training sets is very diï¬€erent
from the actual data in webcam peeking. After testing diï¬€erent
image data on the models, we found the two main causes
for their bad performance are (1) signiï¬cantly lower contrast,
(2) occlusions caused by insuï¬ƒcient photons. Surprisingly, we
also found the models sensitive to how we crop the images,
which suggests the convolutional layer features and potential
data augmentation schemes employed by these models are
not dealing well with our dataâ€™s distribution. We think future
researchers can potentially utilize these pretrained models and
collect their own webcam peeking dataset to ï¬ne-tune the
model weights to better adapt machine learning recognition
models to this scenario.

The prescription glasses generally yield better results for
the webcam peeking attack, showing that 10 mm texts can
be recognized in the reconstructed images with over 75%
accuracy. Although not as good as the prescription glasses, the
recognition accuracy with the BLB glasses is also high enough
to support eï¬ƒcient peeking attacks against texts of 10-20 mm.
Despite the better reï¬‚ective characteristics of the BLB glasses,
the prescription glasses still generate better results due to its
smaller curvature, highlighting the risks of the peeking attack
even without highly reï¬‚ective glasses.

Intuitively, diï¬€erent letters in the alphabet would be recog-
nized with diï¬€erent levels of hardships due to their structural
characteristics (see Figure 15). For instance, the letter â€œRâ€ and
â€œBâ€ have been found the hardest to recognize in both cases of
the two pairs of glasses. On the other hand, letters such as â€œCâ€,
â€œUâ€, â€œIâ€, and â€œOâ€ have generally the highest recognizability
in all the sizes, which we suspect is due to their simple or
highly symmetric structures that prevent the recognizability
of such letters from dropping too seriously when the texts are
down-sampled and occluded. Furthermore, we found letters
having similar structures are confused with each other more
easily in the recognition. For instance, â€œJâ€ and â€œLâ€ are mostly
recognized as â€œIâ€ when the letter size gets small because the
distortions to the bottom part of â€œJâ€ and â€œLâ€ makes them just
like â€œIâ€ in the reï¬‚ection images.

C. Network Inï¬‚uence

Video conferencing platforms like Zoom cause diï¬€erent
levels of distortions in the images through video encoding and
decoding under various network bandwidths. To analyze the
impact, we compared the quality of the reconstructed images
under diï¬€erent network bandwidths to that when the video is
recorded by the victimâ€™s local device without going through
Zoom. A visual demonstration of the eï¬€ect is shown in Figure
17 which is quantiï¬ed with CWSSIM scores and shown in
Figure 6 (a). We found that when the upload bandwidth is
larger than 10 Mbps, the quality of the reconstructed images
generally remain the same, and is close to the locally-captured
and reconstructed images with a minor degree of added
distortions. An upload bandwidth smaller than 10 Mbps starts
to undermine the reconstructed image quality over Zoom.
When the bandwidth is smaller than 1000 kpbs, the letters get
hard to recognize. Itâ€™s almost unrecognizable with a bandwidth
smaller than 500 kbps. When the bandwidth was larger than
1500 kbps, Zoom was generally able to maintain a 720p video
resolution with a frame rate close to 30 fps (Appendix F).

D. Physical Factors

The recognizability of the reï¬‚ections is a highly complex
multi-variate function over many physical factors. We catego-
rize the factors into 2 groups, namely those mainly aï¬€ecting
the reï¬‚ection pixel size (Section III-B) and those aï¬€ecting the
light SNR. A comprehensive quantitative modeling of light
SNR is very challenging due to the need of accurate imaging
sensor models. Nevertheless, we provide qualitative analysis
and quantify representative cases by calculating changes in
CWSSIM scores (Figure 6).

In light SNR,

the signal portion comes from the light
emanated from the screen, reï¬‚ected by the glasses, and then
captured by the imaging sensors corresponding to the area of
the screen. Other light captured by sensors in this area can be
treated as noise. Counter-intuitively, more reï¬‚ected light does
not always lead to higher reï¬‚ection recognizability as we will
discuss next. Figure 6 (b-e) show the factors that can change
light SNR most signiï¬cantly. (c-e) also inspect how auto
exposure and manual (ï¬xed) exposure can aï¬€ect the light SNR-
recognizability relationships in surprisingly diï¬€erent ways by
using the laptop built-in webcam and the conï¬gurable Nikon
Z7 respectively.

Text Color Contrast. Diï¬€erent colors of texts can aï¬€ect
the reï¬‚ection recognizability because the texts and screen

7

ï¼ˆaï¼‰ï¼ˆbï¼‰Fig. 6. Eï¬€ects of impact factors evaluated by CWSSIM scores. The original score numbers are displayed along with the legend at the bottom, and we plot
the ratio between each score and the highest score in each case as a percentage. Visualizations of the eï¬€ects can be found in the appendix.

background colors produces a certain contrast. We found that
chroma has smaller eï¬€ects than luma and show how luma
aï¬€ects reï¬‚ection quality in Figure 6 (b) (visualization in Figure
17 (b)) by using the absolute diï¬€erence in RGB values of gray-
scale text and background colors to represent the contrast. As
expected, lower contrast (smaller RGB diï¬€erence) undermines
the reï¬‚ection recognizability.

Face Background Reï¬‚ectance. Face background re-
ï¬‚ectance is decided by sub-factors such as skin color. We
tested diï¬€erent background reï¬‚ectance by pasting the inner
side of the glasses with papers of diï¬€erent gray-scale colors
that have the same values for RGB. When the background
has a higher reï¬‚ectance (larger RGB values), more light from
the environment as well as the screen will be reï¬‚ected by
it, increasing the noise portion of the light SNR and thus
undermining recognizability of the reï¬‚ections as shown in
Figure 6 (c) (visualization in Figure 17 (c)).

Environment Light Intensity. Decrease in the environ-
mental light intensity causes a smaller degree of noise and
thus increase the light SNR. This increase, however, does
not necessarily lead to better recognizability in case of we-
bcams which often have auto-exposure control to adjust the
overall brightness of the videos they take. When the overall
environment is too dark, the webcamâ€™s ï¬rmware automatically
increase the exposure time trying to compensate for the dark
environment. This increase in the exposure time can cause an
over-exposure for the reï¬‚ected contents on the glasses which
could have much higher light intensity than the environment,
leading to smaller contrast and thus harder-to-read images.
Such over-exposure is found in multiple participantsâ€™ videos
in the user study (Section V-B). On the other hand,
the
recognizability monotonically increases in the case of manual-
exposure cameras such as the Nikon Z7 in manual mode.
Figure 6 (d) (visualization in Figure 17 (d)) shows the diï¬€erent
behaviors of auto and manual exposure.

Screen Brightness. Screen brightness is the opposite to
environmental light intensity in terms of its impact on the
reï¬‚ection recognizability. When the screen is brighter, the
signal portion in the light SNR increases and can lead to more
readable reï¬‚ections for manual-exposure cameras. However,
auto-exposure of most webcams can again negatively aï¬€ect the
recognizability. Speciï¬cally, if the screen gets too bright com-
pared to the environmental lighting condition, the webcams
will often adjust their exposure time and ISO based on the

8

dominant environmental light condition, and thus cause over-
exposure to the screen reï¬‚ections. Figure 6 (e) (visualization
in Figure 17 (e)) shows the eï¬€ects.

Summary. The results show that variations in physical
conditions can change the actual limits of the attack dra-
matically. The fact
that reï¬‚ection recognizability does not
change monotonically with some factors like environmental
light intensity and screen brightness further challenges the
the possible
attack by making it more diï¬ƒcult
outcomes in uncontrolled settings.

to predict

E. Eyeglass Lens

The diï¬€erence in recognition accuracies between the pair of
BLB and prescription glasses (Figure 5) suggest parameters
of diï¬€erent eyeglass lenses will inï¬‚uence the performance
of webcam peeking. To examine the impact, we analyzed 16
pairs of eyeglasses by inspecting the correlation between their
reï¬‚ection quality quantiï¬ed by CWSSIM scores and several
lens factors. The CWSSIM scores are acquired with the 16
glasses when all other factors are kept the same.

lens focal

The results suggest

length, which determines
the pixel size of reï¬‚ections (Equation 3), has the strongest
inï¬‚uence on the reï¬‚ections with a correlation score of 0.56.
The minimum, mean, and maximum focal length of the 16
pairs of glasses are 10, 268, and 110 cm respectively. With
a correlation score of 0.42, the second strongest factor is
found to be prescription strength (lens power) as lens power
usually has a positive correlation with focal length following
design conventions (see Appendix A for explanation). Lens
reï¬‚ectance and surface coating conditions that mainly aï¬€ect
reï¬‚ection light SNR produce correlation scores of 0.32 and
0.31 respectively. We empirically deï¬ned and added the factor
of lens coating condition that gauges how much the lens
coatings have worn oï¬€ with higher values representing more
intact coating. The motivation is our observation that damage
in lens coating reduces the recognizability of reï¬‚ections (see
Figure 10). We also estimated lens reï¬‚ection spectrum by
calculating the ratio between RGB values of the reï¬‚ections in
the image, but only found correlation scores lower than 0.15.
This suggests the glass type (e.g., BLB or non-BLB) does not
have a strong inï¬‚uence on reï¬‚ection quality. Finally, we expect
the parameters analyzed above have certain relationships with
lens and coating materials, which require specialized optical
equipment to measure and determine.

Local10000 kb/s5000 kb/s1000 kb/s500 kb/s0.44030.36160.32480.29700.1977Nikon Z7r,g,b=30r,g,b=110r,g,b=140r,g,b=180r,g,b=22020 lux100 lux220 lux360 lux450 lux0.51990.49320.48020.28000.14500.49360.48090.47750.45280.42310.19980.37890.37190.19530.14390.34740.34400.33950.33790.3311WebcamNikon Z720%40%60%80%100%0.19910.23540.26180.28660.33370.32300.35390.34270.33080.2969Webcam(a1)(b)(c)(d)WebcamNikon Z7WebcamLocal10000 kb/s5000 kb/s1000 kb/s500 kb/s0.44030.36160.32480.29700.1977Nikon Z7r,g,b=30r,g,b=110r,g,b=140r,g,b=180r,g,b=22020 lux100 lux220 lux360 lux450 lux0.51990.49320.48020.28000.14500.49360.48090.47750.45280.42310.19980.37890.37190.19530.14390.34740.34400.33950.33790.3311WebcamNikon Z720%40%60%80%100%0.19910.23540.26180.28660.33370.32300.35390.34270.33080.2969Webcam(a1)(b)(c)(d)WebcamNikon Z7Webcam30110140180220Face Background RGB020406080100Score (Auto Exp.):0.52, 0.49, 0.48, 0.28, 0.15Score (Manual Exp.):0.49, 0.48, 0.48, 0.45, 0.4220100220360450Env. Light Intensity (Lux)020406080100Score (Auto Exp.):0.20, 0.38, 0.37, 0.20, 0.14Score (Manual Exp.):0.35, 0.34, 0.34, 0.34, 0.3320406080100Screen Brightness (%)020406080100Score (Auto Exp.):0.15, 0.44, 0.33, 0.26, 0.14Score (Manual Exp.):0.30, 0.32, 0.33, 0.34, 0.3525020015010050Txt.-Bg. RGB Absolute Difference020406080100Score (White Bg.):0.38, 0.32, 0.32, 0.19, 0.15Score (White Txt.):0.38, 0.35, 0.33, 0.24, 0.18Local1000050001000500Upload Bandwidth (kb/s)020406080100CWSSIM Score Ratio (%)Score:0.43, 0.39, 0.32, 0.30, 0.20Local1000050001000500Upload Bandwidth (kb/s)020406080100CWSSIM Score Ratio (%)Score:0.43, 0.39, 0.32, 0.30, 0.2030110140180220Face Background RGB020406080100Score (Auto Exp.):0.52, 0.49, 0.48, 0.28, 0.15Score (Manual Exp.):0.49, 0.48, 0.48, 0.45, 0.4225020015010050Txt.-Bg. RGB Absolute Difference020406080100Score (White Bg.):0.38, 0.32, 0.32, 0.19, 0.15Score (White Txt.):0.38, 0.35, 0.33, 0.24, 0.1820100220360450Env. Light Intensity (Lux)020406080100Score (Auto Exp.):0.20, 0.38, 0.37, 0.20, 0.14Score (Manual Exp.):0.35, 0.34, 0.34, 0.34, 0.3320406080100Screen Brightness (%)020406080100Score (Auto Exp.):0.15, 0.44, 0.33, 0.26, 0.14Score (Manual Exp.):0.30, 0.32, 0.33, 0.34, 0.35(a)(b)(c)(d)(e)User

Theoretical

TABLE III
TEXT SIZES OF WEB CONTENTS AND SUSCEPTIBILITY TO PRESENT-DAY AND
FUTURE WEBCAMS
Cap Height (mm)
2.1
2.5
3.2
4.3
3.7
4.3
5.6
7.4
10
14
18
24
35
60

Target
îˆ³
1 P
îˆ³
1 H3
îˆ³
1 H2
îˆ³
1 H1
îˆ³
2 P
îˆ³
2 H3
îˆ³
2 H2
îˆ³
2 H1 (S1)
îˆ³
3 0% (S2)
îˆ³
3 20% (S3)
îˆ³
3 40% (S4)
îˆ³
3 60%
îˆ³
3 80% (S5)
îˆ³
3 95% (S6)

pt
12
14
18
24
21
25
32
42
56
80
102
136
253
340

â—”
â—”
â—”
â—”
â—”
â—“
â—
â—
â—
â—
â—
â—
â—

â—”
â—”
â—”
â—”
â—”
â—“
â—
â—
â—
â—
â—
â—

â—”, â—“, and â— mean the text size can be peeked with 4K, 1080p, and 720p
cameras respectively. The theoretical column contains predictions of the
targetsâ€™ susceptibility based on the reï¬‚ection model and evaluation results in
lab settings. The user column contains those based on the user study.

V. CYBERSPACE TEXTUAL TARGET SUSCEPTIBILITY

The evaluations so far are based on the text physical size and
carried out in controlled environments to better characterize
user-independent components of the reï¬‚ection model as well
as the range of theoretical limits for webcam peeking. In this
section, we start by mapping the limits to common cyberspace
objects in order to understand the potential susceptible targets.
We then conduct a 20-participant user study with both local
and Zoom recordings to investigate the feasibility and chal-
lenges of peeking these targets and various factorsâ€™ impact.

A. Mapping Theoretical Limits to Targets

2, îˆ³

1, îˆ³

We use web texts as an enlightening example of cyberspace
textual targets considering their wide use and the relatively
mature conventions of HTML and CSS. The discussion is
based upon (1) a previous report [49] scraping the most
popular 1000 websites on Alex web ranking [8], and (2)
a manual inspection of 117 big-font websites archived on
SiteInspire [10]. We further divide the inspected web texts
into 3 groups (îˆ³
3, see Appendix H) in order to discuss
separately how the webcam peeking attack with current and
future cameras could have eï¬€ects on them as summarized
in Table III. As pointed out in Section III-B, the conversion
between digital point size and physical cap height is dependent
on speciï¬c user settings such as browser zoom ratio. The cap
height values in Table III are thus measured with the Acer
laptop with default OS and browser settings as a case study.
Based on the results in Figure 5, we hypothesize that the
smallest cap heights adversaries can peek using mainstream
720p cameras is 7-10 mm. We then calculate the correspond-
ing limits with 1080p and 4K cameras with Equation 3 and
show them in the Theoretical column of Table III. Considering
participants are most likely to use 720p cameras, we then
choose point sizes S1-S6 in Table III for evaluations.

B. User Study

The user study3 (Appendix I) is designed in the following
challenge-response way: An author generates HTML ï¬les each
with one randomly selected headline sentence containing 7-9
words 4 from the widely-used â€œA Million News Headlinesâ€
dataset [47]. Only each wordâ€™s ï¬rst letter is capitalized. The
participants display the HTML page in their browsers when
they are recorded, and another author acting as the adversary
tries to recognize the words from the videos containing the 20
participantsâ€™ reï¬‚ections without knowing the HTML contents
by using the same techniques as in Section IV. We then
calculate the percentage of correctly recognized words.

Data Collection. Each participant was given 6 HTML
ï¬les of increasing point sizes from S1 to S6 as shown in
Table III. Note that the 6 sizes are speciï¬ed in point size
in HTML so that user-dependent factors such as screen size
and browser zoom ratio can be studied (Equation 1). The
participants display each HTML ï¬les on their own computer
display in their accustomed rooms and behave normally as
in video conferences. We allow participants to choose their
preferred environmental lighting condition except asking them
to avoid other close light sources besides the screen in front
of their face. The reason is that we found a close frontal
light source can seriously decrease light SNR, which can
potentially be used as a physical mitigation against this attack
but prevents us from examining the impact of all the other
factors. We did not tell the participants to stay stationary and
let them behave normally as in browsing screen contents. Their
webcams record their image for 30 seconds for each HTML.
Network bandwidth and resulted video quality are artifacts
of video conferencing platforms that improve in a rapid way
[4] compared to other user-dependent physical factors. To
study the present-day and possible future impact of video
conferencing platforms, we record the 20 participantsâ€™ videos
both locally and remotely through Zoom. Our experiments
focused on Zoom since it is the mostly used platform and
also provides the most detailed video and network statistics.
We asked the participants to report their user-dependent
parameters including screen resolution (ğ‘ğ‘œğ‘ ), screen physical
size (ğ»ğ‘ ğ‘Ÿ), OS and browser zoom ratio (ğ‘ ğ‘œğ‘ , ğ‘ ğ‘) webcam
resolution in Equation 1, webcam resolution (ğ‘) in Equation
3, and the type of their glasses. Some other physical factors
including environmental light intensity, screen brightness, and
glass-screen distance, and the physical size of displayed texts
are diï¬ƒcult to be measured by the participants themselves and
are not reported. We thus estimated the values of these factors
by utilizing their videos (see Appendix J).

General Adversary Recognition Results. The recognition
results achieved by the adversary with local and remote record-
ings are shown in Figure 7 (upper and lower respectively). Two
participants (4 and 14) did not generate glass reï¬‚ections of
their screens in the video recordings due to the problem of out-

3Institutional Review Board (IRB) No.ZDSYHS-2022-5
4Uniform lengths (e.g., all 8 words) are avoided to prevent the adversary

from guessing the words by knowing how long the sentences are.

9

Fig. 7. The recognition results of textual reï¬‚ections collected with local and Zoom-based remote video recordings from 20 user study participants. Participant
4, 14, and 3, 6, 10, 11 did not generate glass reï¬‚ections that allow successful recognition due to problems of out-of-range viewing angles and very low light
SNR respectively, and are thus omitted from the ï¬gure.

Fig. 8. (a) The degree of inï¬‚uence of diï¬€erent factors on the reï¬‚ection recognition performance evaluated by the correlation scores. Factors highlighted with
boxes are computed with other raw factors according to our model. (b-d) The joint distribution of three factors and the recognition results.

of-range vertical viewing angles as predicted in Section III-B.
Four participants (3, 6, 10, 11) yield 0% textual recognition
accuracy due to a very low light SNR.

With local video recordings,

the percentage out of the
20 participants that are subjected to non-zero recognition
accuracy against S6-S1 are 70%, 60%, 30%, 25%, 15%, and
0% respectively. Videos of participant 7 and 17 using 720p
cameras allowed the adversary to achieve 12.5% and 25%
accuracies on recognizing S2. Videos of participant 16 using
a 480p camera allowed the adversary to achieve an 37.5%
accuracy on recognizing S3. These results translate to the pre-
dicted susceptible targets with cameras of diï¬€erent resolutions
as listed in the User column of Table III, where 720p webcams
pose threats to large-font webs (îˆ³
3) and future 4K cameras
pose threats to various header texts on popular websites (îˆ³
1
and îˆ³
2). As expected, this result is worse than the theoretical
limits in the table that are derived with prescription glass data
in the controlled lab setting (Section IV). Our observations
suggest
the main reasons include: (1) The environmental
lighting conditions of the users are more diverse and less
advantageous to screen peeking than the lab setup, generating
reï¬‚ections with worse light SNR. (2) Texts in the user study
are mostly lower-case and have thus smaller physical sizes than
the upper-case letters used in Section IV. (3) The prescription
glasses used in Section IV have a larger focal length than the
average userâ€™s glasses. (4) More intentional movements exist
in the user study leading to more motion blur.

With Zoom-based remote recordings, the percentage of par-

ticipants with non-zero recognition accuracy against S6-S1 de-
graded to 65%, 55%, 30%, 25%, 5%, and 0% respectively. We
logged the video network bandwidth and resolution reported
by Zoom as shown in Figure 7. The correlation between Zoom
bandwidth, resolution, and their impact on video quality agree
with the observations in Section IV-C. Generally, bandwidths
smaller than 1500 kbps led to 360p resolutions for most of
the time and decreased the recognizable text size by 1 level.
Zoomâ€™s 720p videos also caused degradation in recognition
accuracy but mostly kept the recognizable text size to the same
level as the local recordings, suggesting the same predictions
of susceptible text sizes and corresponding cyberspace targets.
Besides the mostly used platform Zoom, we also acquired
remote recordings of participant 19 with Skype and Google
Meet. The adversary achieved better results with Skype than
Zoom by recognizing S3 and S2 with 89% and 25% accuracies
respectively, which is likely due to Skypeâ€™s capability of
maintaining better-quality video streams with a 1200 kbps
bandwidth. The web-based Google Meet platform provided
the lowest quality videos and only allowed the adversary to
achieve 22% accuracy on recognizing S4.

Underlying Reasons. To ï¬nd out the dominant reasons
enabling easier webcam peeking by analyzing the correlation
between the recognition results and diï¬€erent factors, we turn
each participantâ€™s results (6 sizes) into a single attack score
that is a rectiï¬ed weighted sum of the recognition accuracy
of the six text sizes tested. Figure 8 (a) shows correlation
scores with 11 factors that aï¬€ect reï¬‚ection pixel size (left)

10

720p 720p 720p 720p 720p 720p 720p 720p 640x4801152x648720p 720p 720p 720p 720p 720p 840x480720p 720p 1080p720p 360p N/A720p 1152x648720p 720p 720p 720p 720p 720p 720p 720p 720p 720p 720p 720p 720p 720p 640x480720p 840x4801080pCam Reso.58006300250000000000420032000000000037000000410042000000000053003800000028000000Zoom BW  (kbps).720p 720p720p 360p 360p 720p 720p 360p 360p 720p 360p 720p 360p 360p 360p 720p 720p360p 720p360pZoom Reso.720p 1152x648720p 720p 720p 720p 720p 720p 720p 720p 640x480720p 840x4801080pCam Reso.720p     720p720p 360p 720p 720p 360p 720p 360p 360p   720p360p 720p360pZoom Reso.Zoom kbps58002500120032005300380063001200410011002800130013001200Reflection Pixel Size FactorsLight SNR FactorsReflection Pixel Size FactorsLight SNR Factors(a)and light SNR (right) respectively when ğ‘¤ = 1.5. The
glass type includes prescription (15/20) and prescription with
BLB coatings (5/20). The physical text size and reï¬‚ection-
environment
light ratio highlighted in the boxes are two
compound factors. In short, the physical text size represents
the ratio between the actual physical size of texts displayed on
each participantâ€™s screen and the case study values in Table III
and is calculated with Equation 1 with other raw factors such
as browser zoom ratios. The reï¬‚ection-environment light ratio
represents how strong the screen brightness is compared to
the environmental light intensity and is calculated by dividing
glass luminance by environmental luminance (see Appendix J).
Basically, these two compound factors represent our modelâ€™s
prediction to reï¬‚ection pixel size and light SNR and is found
to generate higher correlation scores than the other raw factors,
which validates the eï¬€ectiveness of our models. Figure 8
(b-d) further show the joint distribution of the attack score
and three representative factors. It can be seen from (b) that
the 40 mm screen-glass distance used in the evaluation of
Section IV is about the average of the participantsâ€™ values, and
distances of these participants actually only have very weak
correlation with the easiness of webcam peeking attack. Figure
8 (d) suggests that when the screen brightness-environmental
light intensity ratio gets lower than a certain threshold, the
likelihood of preventing adversaries from peeking is very high,
which may be considered as a temporary mitigation.

VI. WEBSITE RECOGNITION

The results so far suggest it may still be challenging for
present-day webcam peeking adversaries with mainstream
720p cameras to eavesdrop common textual contents displayed
on userâ€™s screens. During our experimentation, we observed
that recognizing graphical contents such as shapes and layouts
on the screen is generally easier than reading texts. Although
shapes and layouts contain more coarse-grained information
compared to texts, a webcam peeking adversary may still pose
non-trivial threats by correlating such graphical information
with privacy-sensitive contexts. This work further explored to
which degree can a webcam peeking adversary recognize on-
screen websites by utilizing non-textual graphical information.
Data Collection. 10 out of the 20 participants in the
user study participated in the website recognition evaluation.
Following a similar methodology as in [43], we used the Alexa
top 100 websites as a closed-world dataset. Each participant
browsed 25 of the 100 websites while being recorded by their
webcams, with about 10 seconds spent on each website. We
only investigate the recognition of the home page of each
website in this work. [43] shows that other pages of a website
can also lead to the recognition of the website. We believe
the easiness of recognizing a website using diï¬€erent pages
is worth exploring in future works. Same as the procedure
in Section V-B, we allowed the participants to choose their
preferred environments and behave normally. Note that some
participants changed their environment and ambient lighting
compared to the previous textual recognition experiment. Both

Fig. 9. Accuracy of recognizing Alexa top 100 websites from eyeglass
reï¬‚ections. Each participant browsed 25 websites. Participant 0 and 4 did
not yield recognizable reï¬‚ections due to bad light SNR and viewing angles
respectively and are thus omitted.

local and Zoom-based remote recordings were obtained and
recognized by the adversary.

Recognition Results. Figure 9 shows the percentage of
websites (out of 25) correctly recognized by the adversary.
Participant 0 and 4 did not yield recognizable reï¬‚ections due
to bad light SNR and viewing angles respectively. This ratio
of zero recognition (2 out of 10) agrees with that in the textual
recognition test (6 out of 20), suggesting that webcam peeking
may be impossible in 20-30% video conferencing occasions
due to extreme user environment conï¬gurations.

As expected, participants with higher textual recognition
accuracies such as participant 7 generally yield higher website
recognition accuracies too. In addition, we observe that web-
site recognition is more robust to various lighting conditions in
the participantsâ€™ ambient environment. For example, we found
participant 10 who had 0% textual recognition accuracy due
to bad light SNR produced 56% (local) and 36% (remote)
accuracies in website recognition with the same environment
and lighting. The reasons are two-fold. First, solid graphical
contents such as color blocks commonly found on web pages
occupy larger areas than the body of texts and are thus much
easier to identify in low-quality videos. Second, compared
to black texts on white backgrounds which only have two
diï¬€erent colors, the overall web pages with multiple graphical
contents have more colors and contrast,
leading to better
robustness against over- and under-exposure of the usable
screen contents in the webcam videos.

Recognition Easiness and Web Characteristics. Com-
pared to texts, websites feature more abundant and diverse
characteristics. We conducted qualitative and quantitative anal-
ysis to identify the characteristics that make certain websites
more susceptible to webcam peeking. To that end, we ranked
the 100 websites by their easiness of recognition utilizing
recognition accuracies. Figure 16 shows rotated screenshots
of the websites that rank the top and bottom 15 by their
recognition easiness. Visual inspections suggest websites with
higher contrast, larger color blocks, and more salient relative
positions between diï¬€erent color blocks are easier to recog-
nize. Websites that are mostly white with sparse textual and
graphical components on them are the hardest to recognize.
We calculated the correlation scores between the rank of
each website and the average as well as standard deviation
of the websitesâ€™ pixel values. Generally, a higher average

11

12367101316Participant020406080100RecognitionAccuracy (%)Local Rec.Zoom Rec.means the website is closer to a pure white screen; a higher
standard deviation means the website has more abundant high-
contrast textures. The correlation scores obtained are -0.33
and 0.45 respectively, supporting our conclusions in the visual
inspection.

VII. DISCUSSION

A. Proposed Near-Term Mitigations

Given the threats, it is worthwhile exploring feasible mit-
igations that can be applied immediately. A straightforward
approach involves users modifying the dominant physical
factors identiï¬ed in this work to reduce reï¬‚ectionsâ€™ light SNR,
e.g., by placing a lamp facing their face whose light increase
the noise portion of light SNR. For software mitigations, we
notice Zoom provides virtual ï¬lters of non-transparent cartoon
glasses that can completely block the eye areas and thus
eliminate reï¬‚ections. Such features are not found in Skype or
Google Meet. Other software-based approaches that support
better usability involve ï¬ne-tuned blurring of the glass area.
Although none of the platforms supports it now, we have
implemented a real-time eyeglass blurring prototype that can
inject a modiï¬ed video stream into the video conferencing
software. The prototype program5 locates the eyeglass area and
apply a Gaussian ï¬lter to blur the area. Figure 14 demonstrates
the eï¬€ect of using diï¬€erent strengths of Gaussian ï¬ltering by
tuning the ğœ parameter. Stronger ï¬ltering (higher ğœ) reduces
reï¬‚ection quality more but also undermines usability and user
experience to a larger degree as it makes the usersâ€™ eye
areas look more unnatural. We believe the usable strength
also depends on characteristics of speciï¬c glasses. For ex-
ample, Figure 14 show three pairs of glasses with increasing
reï¬‚ectance. Since glasses with higher reï¬‚ectance (e.g., the
3rd row) may already have produced screen reï¬‚ections that
occupy and distort image of usersâ€™ eye areas, applying stronger
ï¬ltering may cause less degradation in user experience in this
case. On the other hand, lower-reï¬‚ectance (e.g., the 1st row)
glasses may require weaker ï¬ltering to maintain the same
degree of usability. In general, we believe it is a good idea
for future platforms incorporating this protection mechanism
to allow users to adjust ï¬ltering strength by themselves.

B. Improve Video-conferencing Infrastructure

Individual Reï¬‚ection Assessment Procedure. Our analysis
and evaluation reveal that diï¬€erent individuals face varying
degrees of potential information leakage when subjected to
webcam peeking. Speciï¬cally, various factors of software
settings, hardware devices, and environmental conditions aï¬€ect
the quality of reï¬‚ections. Even for the same user, the potential
level of threats varies when the user joins video conferences
from diï¬€erent places or at diï¬€erent times of a day. These
factors make it infeasible to recommend or implement a single
set of protection settings (e.g., what glasses/cameras/ï¬lter
strength to use) before the actual user settings are known.

5Details and open-source code of this prototype implementation can be

found at https://github.com/VidConfSec/EyeglassBlurFilter

Providing usable security requires an understanding of how
serious the problem is before trying to eliminate the problem.
In light of this, we advocate an individual reï¬‚ection assessment
procedure that can potentially be provided by future video
conferencing platforms. The testing procedure can be made
optional to users after notifying them of the potential risk
of webcam peeking. The procedure may follow a similar
methodology as the one used in this work by (1) displaying
test patterns such as texts and graphics, (2) collecting webcam
videos for a certain period of time, (3) comparing reï¬‚ection
quality in the video with test patterns to estimate the level
of threats of webcam peeking. With the estimated level of
threats, the platform can then notify the user of the types of
on-screen contents that might be aï¬€ected, and oï¬€ers options
for protection such as ï¬ltering or entering the meeting with
the PoLP principle that will be discussed below.

Principle of Least Pixels. Cameras are getting more capable
than what average users can understandâ€”unwittingly exposing
information beyond what users intend to share. The funda-
mental privacy design challenge with webcam technology is
â€œoversensingâ€ [28] where overly-capable sensors can provide
too much information to downstream processingâ€”more data
than is needed to complete a function, such as a meaningful
face-to-face conversation. This oversensing leads to a violation
to the classic Principle of Least
of the sensor equivalent
Privilege (PoLP) [53]. We believe long-term protection of
users ought to follow a PoLP (perhaps a Principle of Least
Pixels) as webcam hardware and computer vision algorithms
continue to improve. Thus, we recommend that future infras-
tructure and privacy-enhancing modules follow the PoLP not
just for software, but for the camera data streams themselves.
In sensitive conversations, the infrastructure could provide
only the minimal amount of information needed and allow
users to incrementally grant higher access privileges to the
other parties. For example, PoLP blurring techniques might
blur all objects in the video meeting at the beginning and
then intelligently unblur what is absolutely necessary to hold
natural conversations.

C. User Opinion Survey

We collected opinions on our ï¬ndings of webcam peeking
risks and expectations of protections from 60 people including
the 20 people who participated in the user study and 40 people
who did not. We did not ï¬nd apparent diï¬€erences in the two
groupâ€™s opinions. The overall opinions are reported below.

Textual Recognition. For the discovered risk of textual
recognition, 40% of the interviewees found it a larger risk than
what they expected; 48.3% thought it was almost the same as
their expectation; 11.7% expected worse consequences than
what we found. In addition, 76.7% of the interviewees think
this problem needs to be addressed while 23.3% think they
can tolerate this level of privacy leakage.

Website Recognition. 61.7% of the interviewees found it
a larger risk than what they expected; 30% thought it was
almost the same as their expectation; 8.3% expected worse
consequences than what we found. In addition, 86.7% of the

12

interviewees think this problem needs to be addressed while
13.3% think they can tolerate this level of privacy leakage.

Reï¬‚ection Assessment. Regarding the proposed idea of re-
ï¬‚ection assessment procedures that may be provided by video
conferencing platforms in the future, 95% of the interviewees
said they would like to use it; 85%, 68.3%, 45%, and 20% of
the 60 interviewees would like to use it when meeting with
strangers, colleagues, classes, and family/friends respectively.
Glass-blur Filters. Regarding the possible protection of
using ï¬lters to blur the glass area, 83.3% of the interviewees
said they would like to use it; 78.3%, 51.7%, 43.3%, and 11.7%
of the 60 interviewees would like to use it when meeting with
strangers, colleagues, classes, and family/friends respectively.

D. Limitation & Future Work.

This work used human-based recognition to evaluate perfor-
mance limits of reï¬‚ection recognition. Compared to machine
learning-based recognition, human-based recognition helps us
understand the threats posed by a wide range of adversarial
parties including even common users of video conferencing,
and thus provides an estimate of the lower bound of the
limits posed by camera hardware and other factors. We believe
it is always possible to improve the attack performance by
designing a more sophisticated machine learning model with
more parameters, and by increasing the size and diversity of
the training dataset. Further, machine learning recognition is
likely to face the over-ï¬tting and generalizability problems in
webcam peaking due to highly varying personal environment
conditions. Thus, we believe limits posed by a machine learn-
ing recognition back end are subjected to very large variances
and require dedicated future works to quantify

Certain levels of biases have been introduced in the user
study by informing the participants of the studyâ€™s purpose. We
envision a future study may conduct a real-world validation of
this attack by performing it without participantsâ€™ awareness
while carefully following ethical regulations. Alternatively,
public videos on social medias may be analyzed to investigate
how often such information leakage happens. A future study
could also systematically interview professionals in diï¬€erent
types of business and explore information leakage conditions,
frequencies, and concerns. Contextual factors and user atti-
tudes in real-world situations are complementary to this workâ€™s
focus and worth investigating in future research.

VIII. RELATED WORK

The problem of screen reconstruction is a long-studied
challenging problem. In this section, we analyze the past works
that served as the foundations for our thinking in the context
of video conferencing today and in the predicted future.

Screen Peeking Using Cameras. Screen-peeking with cam-
eras through optical emanation reï¬‚ections have been explored
in previous works. In 2008, Backes et al. [26] showed that
adversaries can use oï¬€-the-shelf telescopes and DSLR cameras
to spy victimsâ€™ LCD monitor screen contents from up to 30m
away by utilizing the reï¬‚ective objects that can be commonly
found next to the monitor screen such as teapots placed on

a desk. In 2009, the authors [25] took the attack to the next
level by addressing the challenges of motion blur and out-of-
focus blur by performing deconvolution on the photos with
Point Spread Functions (PSF). Our work diï¬€ers from these
previous works by exploiting the victimsâ€™ own webcams in
video conference for a remote attack. Such changes call for
diï¬€erent imaging enhancing techniques due to the diï¬€erent
types of image distortions. In addition, reï¬‚ective objects on
the desks and human eyes cannot be easily utilized due to
very large curvatures. We thus exploit the glasses people wear
to video conferences as a modern attack vector. [58] proposed
a relevant idea of using adversary-controlled webcam to detect
changes in webpage linksâ€™ colors for inferring visited websites.
It requires the adversary to take control over the victimâ€™s
webcam with malicious web modules and exploits coarse-
grain color variations, while our work studies more natural
attack vectors in video conferencing and investigate the limits
of textual reconstruction.

Screen Content Reconstruction With Other Emanations.
Besides the direct optical emanations from the screen that
we exploit in this work, previous works also explored other
channels such as electromagnetic radiation [45], [46], [56]
and acoustic emanations [38]. Reconstructing screen contents
with such emanations usually requires using additional eaves-
dropping hardware that is placed close to the victims by the
adversary. On the other hand, our work exploits the victimâ€™s
own webcams, making the attack more accessible.

Remote Eavesdropping Via Audio/video Calls. Similar
to our work, such attacks assume the adversary and victim
are both participants of an audio/video conference, and the
adversary can eavesdrop privacy-sensitive information by an-
alyzing the audio/video channels. For example, Voice-over-
IP attacks for keystroke inference eavesdrop the victimâ€™s key-
board inputs by utilizing timing and/or spectrum information
embedded in the keystroke acoustic emanations [29], [30],
[35], [55]. Recently, Sabra et al. [52] proposed works solving
the problem of inferring keystrokes by analyzing the dynamic
body movements embedded in the videos during a video call.
Hilgefort et al. [40] spies victimsâ€™ nearby objects through
virtual backgrounds in video calls by carrying out foreground-
background analysis and accumulating background pixels. In
contrast, our work explores the related problem of the limits
of screen textual contents reconstruction using only the optical
reï¬‚ections from participantsâ€™ glasses embedded in the videos.

IX. CONCLUSION

In this work, we characterized the threat model of the
webcam peeking attack in video conferencing settings. We
developed mathematical models that describe the relation-
ship between the attack limits and diï¬€erent user-dependent
factors. The analysis enables the prediction of future threats
as webcam technology evolves. We conducted experiments
both in controlled lab settings and with a user study. Results
showed that present-day 720p cameras pose threats to the
contents on usersâ€™ screens when users browse certain big-font
websites. Future 4K cameras are predicted to allow adversaries

13

to reconstruct various header texts on popular websites. We
also found adversaries can recognize the website users are
browsing through webcam peeking with 720 cameras. We
analyzed both short-term mitigations and long-term defenses
and collected user opinions on the possible protections through
surveys.

REFERENCES

[1] Converting diagonal ï¬eld of view and aspect

tal and vertical ï¬eld of view.
converting-diagonal-ï¬eld-of-view-and.html, 2013.

ratio to horizon-
http://vrguy.blogspot.com/2013/04/

[2] Webcam Field of View . https://www.telehealth.org.nz/assets/Uploads/

1511-webcam-ï¬eld-of-view.pdf, 2015.

[3] Approximate Focal Length for Webcams and Cell Phone Cameras. https:

//learnopencv.com/approximate-focal-length-for-webcams-and
-cell-phone-cameras/, 2016.

[4] Cisco Annual

Internet Report

(2018â€“2023) White Paper.

https:

//www.cisco.com/c/en/us/solutions/collateral/executive-perspectives/
annual-internet-report/white-paper-c11-741490.html, 2020.

[5] Schott AG: Transmittance of optical glass. https://www.schott.com/d/
advanced_optics/5b1f5065-0587-4b3f-8fc7-e508b5348012/, 2020.
[6] The most maddening part about working from home: video con-
https://www.washingtonpost.com/technology/2020/03/16/

ferences.
remote-work-video-conference-coronavirus/, 2020.

.

[7] Acer

Predator

15.

https://www.acer.com/ac/en/IN/content/

predator-model/NH.Q1YSI.001, 2021.

[8] Alexa SEO and Competitive Analysis Software. https://www.alexa.com/,

2021.

[9] Amazon Mechanical Turk. https://www.mturk.com/, 2021.
[10] Big Type Websites. https://www.siteinspire.com/websites?categories=

22, 2021.

[11] Blue Light Blocking Glasses Market Size 2021 with a CAGR of 7.7% ,
Research by Business Opportunities, Top Companies data report covers,
globally Market Key Facts and Forecast to 2025. https://www.wboc.com/
story/43536337/blue-light, 2021.

[12] Blue Light Blocking Glasses on Amazon. https://www.amazon.com/gp/

product/B07VBFSY33/, 2021.

[13] Cheese. https://wiki.gnome.org/Apps/Cheese, 2021.
[14] Default style sheet for HTML 4. https://www.w3.org/TR/CSS2/sample.

html, 2021.

[15] For better or worse, working from home is here to stay. https://www.
cnbc.com/2021/03/11/one-year-into-covid-working-from-home-is
-here-to-stay.html, 2021.

[16] Letâ€™s Talk About Base Curves.
lets-talk-base-curves/, 2021.

https://opticianworks.com/lesson/

[17] Nikon Z7.

https://www.nikonusa.com/en/nikon-products/product/

[18] Samsung

mirrorless-cameras/z-7.html, 2021.
Notebook
notebook-9-np900x5m-k03/, 2021.

9.

https://www.samsung.com/hk/pc/

[19] Shot Noise. https://en.wikipedia.org/wiki/Shot_noise, 2021.
[20] Web Style Sheets CSS tips & tricks: EM. https://www.w3.org/Style/

Examples/007/units.en.html#units, 2021.

[21] Zoom. https://zoom.us/, 2021.
[22] Aries Arditi. Adjustable typography: an approach to enhancing low

vision text accessibility. Ergonomics, 47(5):469â€“482, 2004.

[23] Aries Arditi and Jianna Cho. Serifs and font legibility. Vision research,

45(23):2926â€“2933, 2005.

[24] Melanie Arntz, Sarra Ben Yahmed, and Francesco Berlingieri. Working
from home and covid-19: The chances and risks for gender gaps.
Intereconomics, 55(6):381â€“386, 2020.

[25] Michael Backes, Tongbo Chen, Markus DÃ¼rmuth, Hendrik PA Lensch,
and Martin Welk. Tempest
in a teapot: Compromising reï¬‚ections
revisited. In 2009 30th IEEE Symposium on Security and Privacy, pages
315â€“327. IEEE, 2009.

[26] Michael Backes, Markus DÃ¼rmuth, and Dominique Unruh. Compromis-
ing reï¬‚ections-or-how to read lcd monitors around the corner. In 2008
IEEE Symposium on Security and Privacy (sp 2008), pages 158â€“169.
IEEE, 2008.

[27] Alexander Bick, Adam Blandin, and Karel Mertens. Work from home

[28] Connor Bolton, Kevin Fu, Josiah Hester, and Jun Han. How to curtail
oversensing in the home. Communications of the ACM, 63(6):20â€“24,
2020.

[29] Stefano Cecconello, Alberto Compagno, Mauro Conti, Daniele Lain, and
Gene Tsudik. Skype & type: Keyboard eavesdropping in voice-over-ip.
ACM Transactions on Privacy and Security (TOPS), 22(4):1â€“34, 2019.
[30] Alberto Compagno, Mauro Conti, Daniele Lain, and Gene Tsudik. Donâ€™t
skype & type! acoustic eavesdropping in voice-over-ip. In Proceedings
of the 2017 ACM on Asia Conference on Computer and Communications
Security, pages 703â€“715, 2017.

[31] Zechuan Deng, RenÃ© Morissette, and Derek Messacar. Running the
economy remotely: Potential for working from home during and after
covid-19. Statistics Canada. 2020.

[32] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli.
quality assessment: Unifying structure and texture similarity.
preprint arXiv:2004.07728, 2020.

Image
arXiv

[33] Rodger J Elble. Tremor. In Neuro-geriatrics, pages 311â€“326. Springer,

2017.

[34] Rodger J Elble, Helge Hellriegel, Jan Raethjen, and GÃ¼nther Deuschl.
Assessment of head tremor with accelerometers versus gyroscopic
transducers. Movement Disorders Clinical Practice, 4(2):205â€“211, 2017.
[35] FÃ¼rkan Elibol, UÄŸur Sarac, and IÅŸin Erer. Realistic eavesdropping attacks
on computer displays with low-cost and mobile receiver system.
In
2012 Proceedings of the 20th European Signal Processing Conference
(EUSIPCO), pages 1767â€“1771. IEEE, 2012.

[36] Leslie G Farkas, Marko J Katic, and Christopher R Forrest.

Inter-
national anthropometric study of facial morphology in various ethnic
groups/races. Journal of Craniofacial Surgery, 16(4):615â€“646, 2005.

[37] Sina Farsiu, M Dirk Robinson, Michael Elad, and Peyman Milanfar. Fast
IEEE transactions on image

and robust multiframe super resolution.
processing, 13(10):1327â€“1344, 2004.

[38] Daniel Genkin, Mihir Pattani, Roei Schuster, and Eran Tromer. Synes-
thesia: Detecting screen content via remote acoustic side channels. In
2019 IEEE Symposium on Security and Privacy (SP), pages 853â€“869.
IEEE, 2019.

[39] Atsuki Higashiyama, Yoshikazu Yokoyama, and Koichi Shimono. Per-
ceived distance of targets in convex mirrors. Japanese Psychological
Research, 43(1):13â€“24, 2001.

[40] Jan Malte Hilgefort, Daniel Arp, and Konrad Rieck. Spying through
In Proceedings of the 14th ACM
virtual backgrounds of video calls.
Workshop on Artiï¬cial Intelligence and Security, pages 135â€“144, 2021.
[41] Brien A Holden, Timothy R Fricke, David A Wilson, Monica Jong,
Kovin S Naidoo, Padmaja Sankaridurg, Tien Y Wong, Thomas J
Naduvilath, and Serge Resnikoï¬€. Global prevalence of myopia and high
myopia and temporal trends from 2000 through 2050. Ophthalmology,
123(5):1036â€“1042, 2016.

[42] Mohammad Moinul Islam, Vijayan K Asari, Mohammed Nazrul Islam,
and Mohammad A Karim. Video super-resolution by adaptive kernel
In International Symposium on Visual Computing, pages
regression.
799â€“806. Springer, 2009.

[43] Marc Juarez, Sadia Afroz, Gunes Acar, Claudia Diaz, and Rachel
Greenstadt. A critical evaluation of website ï¬ngerprinting attacks. In
Proceedings of the 2014 ACM SIGSAC Conference on Computer and
Communications Security, pages 263â€“274, 2014.

[44] Katherine A Karl, Joy V Peluchette, and Navid Aghakhani. Virtual work
meetings during the covid-19 pandemic: The good, bad, and ugly. Small
Group Research, 53(3):343â€“365, 2022.

[45] Markus G Kuhn. Electromagnetic eavesdropping risks of ï¬‚at-panel
displays. In International Workshop on Privacy Enhancing Technologies,
pages 88â€“107. Springer, 2004.

[46] Markus G Kuhn. Security limits for compromising emanations.

In
International Workshop on Cryptographic Hardware and Embedded
Systems, pages 265â€“279. Springer, 2005.

[47] Rohit Kulkarni. A Million News Headlines, 2018.
[48] Chao-Hsien Kuo and Zhen Ye. Sonic crystal lenses that obey the lens-
makerâ€™s formula. Journal of Physics D: Applied Physics, 37(15):2155,
2004.

[49] Michael Li. I studied the fonts of the top 1000 websites. Hereâ€™s what I
learned. https://dribbble.com/stories/2021/04/26/web-design-data-fonts,
2021.

[50] Tony Lindeberg. Scale invariant feature transform. 2012.
[51] Tatsiana Palavets and Mark Rosenï¬eld. Blue-blocking ï¬lters and digital

after the covid-19 outbreak.CEPR Discussion Paper. 2020.

eyestrain. Optometry and Vision Science, 96(1):48â€“54, 2019.

14

[52] Mohd Sabra, Anindya Maiti, and Murtuza Jadliwala. Zoom on the
keystrokes: Exploiting video calls for keystroke inference attacks. arXiv
preprint arXiv:2010.12078, 2020.

[53] Jerome H Saltzer and Michael D Schroeder. The protection of informa-
tion in computer systems. Proceedings of the IEEE, 63(9):1278â€“1308,
1975.

[54] Mehul P Sampat, Zhou Wang, Shalini Gupta, Alan Conrad Bovik, and
Mia K Markey. Complex wavelet structural similarity: A new image
similarity index. IEEE transactions on image processing, 18(11):2385â€“
2401, 2009.

[55] Ilia Shumailov, Laurent Simon, Jeï¬€ Yan, and Ross Anderson. Hearing
your touch: A new acoustic side channel on smartphones. arXiv preprint
arXiv:1903.11137, 2019.

[56] Wim Van Eck. Electromagnetic radiation from video display units: An
eavesdropping risk? Computers & Security, 4(4):269â€“286, 1985.
[57] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli.
Image quality assessment: from error visibility to structural similarity.
IEEE transactions on image processing, 13(4):600â€“612, 2004.

[58] Zachary Weinberg, Eric Y Chen, Pavithra Ramesh Jayaraman, and Collin
Jackson. I still know what you visited last summer: Leaking browsing
In 2011 IEEE
history via user interaction and side channel attacks.
Symposium on Security and Privacy, pages 147â€“161. IEEE, 2011.
[59] Jianchao Yang and Thomas Huang. Image super-resolution: Historical
overview and future challenges. In Super-resolution imaging, pages 1â€“
34. CRC Press, 2017.

[60] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. Fsim: A feature
IEEE transactions on

similarity index for image quality assessment.
Image Processing, 20(8):2378â€“2386, 2011.

APPENDIX A
LENS POWER & FOCAL LENGTH

Power/Diopter of a lens is deï¬ned as the reciprocal of
the lensâ€™ nominal focal length. Diï¬€erent from the ğ‘“ğ‘” used
before, this nominal focal length correspond to the optical
eï¬€ect produced by the combination of the outer and inner
surfaces of the lens, and is related to the radius of the outer
and inner surfaces by the Lens Makerâ€™s Formula [48]:

ğ· =

1
ğ‘“

= (ğ‘› âˆ’ 1)(

1
ğ‘…ğ‘œ

âˆ’

1
ğ‘…ğ‘–

)

where ğ‘…ğ‘œ and ğ‘…ğ‘– are the radius of the outer inner surfaces
respectively, and ğ‘› is the refractive index of lens material.
When the lens power and materials are set, ğ‘…ğ‘œ and ğ‘…ğ‘– can
both be adjusted to produce the desired power. However, ï¬‚atter
outer surface, as known as base curves, are often used for
higher power lenses [16]. This is why we observe a positive
correlation between ğ‘“ğ‘” and the lens power in Section IV-E.

APPENDIX B
LAB SETTING EXPERIMENT EQUIPMENT

The Acer laptop [7] has a screen width of 38 cm and height
of 190 cm and a 720p built-in webcam. The OS is Ubuntu
20.04. The OS and browser zoom ratios are default (100%).
All the photos and videos are collected with the Cheese [13]
webcam application. The photos are in PNG format and the
videos are in WEBM format. The Samsung laptop used as the
attacker device has OS Windows 10 Pro. The recordings are
collected with OBS Studio in MP4 format.

The pair of BLB glasses [12] has lenses with a horizontal
and vertical chord length of 5 cm and 4 cm respectively, and
a focal length (ğ‘“ğ‘”) of 8 cm. The pair of prescription glasses
[12] has lenses with a horizontal and vertical chord length of
6 cm and 5 cm respectively, and a focal length of 50 cm.

Fig. 10. Design conventions suggest that eyeglasses with higher prescription
strength have smaller curvature (larger radius/focal length) on the lens outer
surface, leading to larger-size reï¬‚ections. Besides curvature and reï¬‚ectance,
lens coating conditions can also aï¬€ect reï¬‚ection quality.

Nikon Z7: The photos are in JPEG format (highest quality)
and the videos are in MP4 format. We compared these formats
with the compression-less (raw) photo and video formats
provided by Nikon Z7 while didnâ€™t ï¬nd obvious diï¬€erence
in the image quality.

APPENDIX C
OBJECTIVE METRICS FOR REFLECTION QUALITY

We embody this notion of reï¬‚ection quality in the similarity
between the reï¬‚ected texts and the original templates. We
compared multiple widely-used image structural and textu-
ral similarity indexes including structural similarity Index
(SSIM) [57], complex-wavelet SSIM (CWSSIM) [54], feature
similarity (FSIM) [60], deep image structure and texture
similarity (DISTS) [32] as well as self-built indexes based
on scale invariant feature transform (SIFT) features [50]. We
found CWSSIM, which is robust to various rigid transforms,
produces the best match with human perception results. We
thus use the CWSSIM similarity score between reï¬‚ections
and templates as the reï¬‚ection quality metric, which spans
the interval [0, 1] with larger numbers meaning more similar
and thus higher quality. It is worth noting that the reï¬‚ections
having high CWSSIM scores is only a necessary yet not
suï¬ƒcient condition for them to be recognizable. Through our
experiments, we ï¬nd that in order to be recognizable by either
human or machine learning algorithms, the CWSSIM score
usually needs to be larger than 0.28.

APPENDIX D
WEBCAM PARAMETER ESTIMATION

The manufacturer of the laptop built-in webcams often
do not share information about the webcam focal length ğ‘“
and imaging sensor physical size ğ‘Š . In this case, further
estimation needs to be made. The term ğ‘“
is a function of
ğ‘Š
the vertical ï¬eld-of-view (FoV) of the webcams. Speciï¬cally,
the FoV angle ğ›¼ can be written as

ğ›¼ = 2 tanâˆ’1 ğ‘Š
2ğ‘“

Considering that typical webcams have a diagonal FoV of in
the range 70 âˆ’ 90â—¦, we can convert it to a typical vertical
FoV of about 40 âˆ’ 50â—¦ for a 720p webcam and thus get ğ‘“
ğ‘Š
approximately in the range of 1.1 âˆ’ 1.4 [1]â€“[3].

15

RoRiOuter Surface ReflectionInner Surface ReflectionCoating Wears OffFig. 11. The model of viewing angle.

APPENDIX E
VIEWING ANGLE MODEL

Similar to the pixel size model, we only use 2D modeling
(Figure 11) for simplicity which can represent either horizontal
or vertical rotations, and we only consider one glass lens since
the two lenses are symmetric. The lenses are further modeled
as spherical with a radius 2ğ‘“ğ‘”. We set the origin ğ‘‚ to the
center of the head which is also treated as the rotation center,
and assume the initial orientation without rotation is such that
the center of the glass lens arc ğ‘ƒ1 aligns with the rotation
center and the laptop webcam ğ‘ƒ4 on the X-axis. The distance
between the glass lens center and the rotation center is ğ‘ .
To calculate the maximum feasible angles, we only need to
consider the reï¬‚ections from either one of the two boundary
points of the glass lens since they are symmetric. We label
the bottom boundary point as ğ‘ƒ2. After a rotation of angle ğœƒ,
2 respectively, and the vector âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ—ğ‘ƒ â€²
ğ‘ƒ1, ğ‘ƒ2 are rotated to ğ‘ƒ â€²
ğ‘ƒ â€²
2
1
yields the normal âƒ—ğ‘› at the reï¬‚ection point ğ‘ƒ â€²
2. ğ‘ƒ3 denote the
point source on the screen whose light gets reï¬‚ected to the
camera with an incident angle ğ›½. With ğ¿ğ‘  being the length
of the screen on the dimension, the camera should be able to
peek reï¬‚ections from the glass lens if ğ‘ƒ3 falls in the range of
the screen. ğ¶ denotes the length of the glass lens chord.

, ğ‘ƒ â€²

1

In order to ï¬nd a mapping from the rotation angle ğœƒ to
the light-emission point ğ‘ƒ3 on the screen, the key is to ï¬nd
the slope of line ğ‘ƒ â€²
ğ‘ƒ3 which intersects with the screen. Since
2
ğ‘ƒ4 and ğ‘ƒ â€²
2 bisects ğ‘ƒ â€²
ğ‘ƒ â€²
ğ‘ƒ â€²
ğ‘ƒ3 , we denote the slope of these
2
1
three lines as ğ‘1, ğ‘2, ğ‘3 respectively, and have

2

ğ‘3 =

ğ‘2 âˆ’ 2ğ‘1 âˆ’ ğ‘2
ğ‘2
1
ğ‘2
1 âˆ’ 2ğ‘1ğ‘2 âˆ’ 1

To calculate ğ‘1 and ğ‘2, the coordinate of ğ‘ƒ â€²

1 and ğ‘ƒ â€²

2, ğ‘ƒ4 can

be denoted as,

(
(ğ‘  âˆ’ 2ğ‘“ğ‘”)ğ‘ğ‘œğ‘ ğœƒ, (ğ‘  âˆ’ 2ğ‘“ğ‘”)ğ‘ ğ‘–ğ‘›ğœƒ) â‰œ (ğ¶, ğ·)
(ğ‘¥0ğ‘ğ‘œğ‘ ğœƒ âˆ’ ğ‘¦0ğ‘ ğ‘–ğ‘›ğœƒ, ğ‘¥0ğ‘ ğ‘–ğ‘›ğœƒ + ğ‘¦0ğ‘ğ‘œğ‘ ğœƒ) â‰œ (ğ´, ğµ)

ğ‘ƒ â€²
1 âˆ¶
ğ‘ƒ â€²
2 âˆ¶
2 âˆ¶ (ğ‘  + ğ‘‘, 0) â‰œ (ğ¸, 0)
ğ‘ƒ â€²

â§
âª
â¨
âª
â©

and thus

ğ‘1 =

ğµ âˆ’ ğ·
ğ´ âˆ’ ğ¶

,

ğ‘2 =

ğµ
ğ´ âˆ’ ğ¸

16

Fig. 12. Heat map of observed Zoom video resolutions under diï¬€erent low
bandwidths that resulted in resolutions lower than 720p

The last missing piece is the coordinate of ğ‘ƒ2, which is

denoted as ğ‘ƒ2 âˆ¶ (ğ‘¥0, ğ‘¦0) = (ğ‘Ÿ Ã— ğ‘ğ‘œğ‘ ğ›¼, ğ‘Ÿ Ã— ğ‘ ğ‘–ğ‘›ğ›¼), where

âˆš

( ğ¶
)2 + (
ğ‘Ÿ =
2
ğ›¼ = âˆ’ğ‘ğ‘Ÿğ‘ğ‘ ğ‘–ğ‘›( ğ¶
2ğ‘Ÿ

)

âˆš

ğ‘…2 âˆ’ ( ğ¶
2

â§
âª
â¨
âª
â©

)2 âˆ’ (ğ‘… âˆ’ ğ‘ ))2

We note that the measured ranges in Table II are uniformly
larger than the theoretical values, which could be caused by
a coarse estimation of the distance ğ‘  since the actual distance
between the lens and the rotation center is hard to determine,
and the fact that the model approximates the camera as a point
instead of a surface.

APPENDIX F
VIDEO CONFERENCING PLATFORM BEHAVIORS

Zoom Under Low Bandwidths. When network bandwidth
got smaller than 4 Mbps, we found Zoom will ï¬rst experience
a short period of aggravated packet loss, and then rapidly
decrease the video resolution to compensate for it. Video
resolution will soon be increased again by sacriï¬cing frame
rate as well as compression loss. Zoom will still try to recover
high frame rate later by further increasing the video com-
pression loss. Through our experiments, we noticed that when
the bandwidth was larger than 1500 kbps, Zoom was able to
maintain a 1280*720 resolution with a frame rate very close
to 30 fps. We observed lower resolutions when the bandwidth
is lower than 1500 kbps, as shown in Figure 12. Skype and
Google Meet do not provide statistics like resolution, frame
rate, and bandwidth. But our visual inspection suggests they
take a similar approach as Zoom to handle bandwidth issues.
Video Quality Control. Currently, Zoom and Skype do
not provide an option for users to control video resolution
or quality directly. Google Meet only allows users to switch
between 720p and 360p send and receive video resolutions.
However, users can limit their system or process bandwidths
using software like NetLimiter to decrease video quality even
without the conferencing platform oï¬€ering such an option.

APPENDIX G
DISTORTION ANALYSIS

We taped the inner surface of the glasses lens with black
papers in order to eliminate the impact of the face background
and better characterize the inherent distortions. Eï¬€ects of the
face background is discussed in Section IV-D. The webcam
and Nikon Z7 were set to the same color temperature (3500 K)

1003005007009001100130015001700Average Bandwdith (kbps)1280x7201120x630960x540640x360480x270320x160180x90Video Resolution0%25%50%75%Fig. 13. (a) The ideal capture versus the actual captures in three consecutive
frames by webcam (1st row) and Nikon Z7 (2nd row). The distortions feature
occlusions with inter-frame and intra-frame variance. The webcam yields
larger variance. (b) Photos captured by Nikon Z7 under diï¬€erent exposure
time and ISO settings. Longer exposure time and medium ISO yield smaller
distortions and increase SNR.

and frame rate (30 fps). For the highly conï¬gurable Nikon Z7,
we set the ISO, aperture, and exposure time to 100, ğ¹ 4, and
1
ğ‘  respectively, disabled all active noise-reduction schemes
30
including vibration reduction, and used manual focus mode.
For both cases, we displayed the string â€œTEXTâ€ and adjusted
the size to make sure the captured text in both camerasâ€™ frames
have a size of 10 pixels vertically.

Diï¬€erent from previous works [25], [26], motion blur and
out-of-focus blurs that are theoretically uniform within a single
frame are not the number one limiting factors in the webcam
peeking threat model because of the relatively shorter exposure
time and closer, more constant camera-object distance. Instead,
distortions with intra-frame and inter-frame variance dominate
which suggests the image quality cannot be easily improved
with PSF deconvolution as in [25] and new image enhancing
techniques are needed.

Figure 13 (b) taken with the conï¬gurable Nikon Z7 shows
how these two forms of distortions (shot and ISO noise) aï¬€ect
the images. For the ï¬rst set of images (1st row), we keep ISO at
100 and decrease the exposure time from 1
ğ‘  to show
4
the eï¬€ect of fewer photons hitting the image sensors which
results in increased shot noise occlusions. For the second
set of images (2nd row), we keep the exposure time at 1
ğ‘ 
80
while increasing ISO from 100 to 3200 to show the eï¬€ect of
increased ISO noise.

ğ‘  to 1
80

APPENDIX H
WEB TEXTUAL TARGETS

Web Text Design Conventions. Despite the fact that the
default CSS font sizes are decided by web browser vendors
separately, we ï¬nd many of them follow the W3C recommen-
dation [14], where H1, H2, H3 headersâ€™ font sizes are 2, 1.5,
1.17 em respectively. To brieï¬‚y explain, a text size of ğ‘¥ em
means the size is ğ‘¥ times the current body font size of the
web page [20] which is usually the same as the font size of
paragraph (P) elements. Nevertheless, we note that web design
standards are lacking and designers have a large degree of

Fig. 14. Diï¬€erent strengths of Gaussian ï¬ltering applied on three pairs of
glasses. The reï¬‚ected texts and their CWSSIM scores in each case are shown.
Diï¬€erent glasses require diï¬€erent strengths of ï¬lters to reduce the reï¬‚ection.
We thus advocate an individual reï¬‚ection testing procedure to determine
protection scheme and settings.

freedom of choosing their own text designs. Sometimes bigger
fonts are preferred in order to make the websites more stylish
and eyes-catching. In this section, we thus investigate both
conventional and more stylish web text sizes.

1 and îˆ³

Text Sizes. îˆ³

2: The ï¬rst group represents the median
HTML P, H1, H2, H3 texts of the 1000 websites. [49] reports
that the median size of the P elements is about 12 pt and
H1, H2, H3 sizes are close to the 2, 1.5, 1.17 em ratios
recommended [14]. We thus use these point sizes for îˆ³
1 and
specify the corresponding cap heights in Table III. The second
group represents the largest HTML P, H1, H2, H3 texts of the
1000 websites in [49] with the same recommended em ratios
for the headers. [49] ï¬nds that about 4% of the 1000 websites
uses a P size as large as 21 pt. This results in H1, H2, H3
sizes of 25, 32, and 45 pt respectively.

îˆ³

3: The third group represents the 117 big-font websitesâ€™
texts. We manually inspected all the 427 websites archived
on SiteInspire [10]. The reason for manual analysis rather
than scraping is that many large-font texts on the websites
are embedded in the form of images instead of HTML text
elements in order to create more ï¬‚exible font styles. We then
selected 117 of them based on the following criteria: (1) The
webpage is still active. (2) The largest static texts that enable
an adversary to identify the website through google search has
a cap height of at least 10 mm when displayed on the Acer
laptop. We show the diï¬€erent quantiles of the largest physical
cap heights on the 117 websites and the converted point sizes
in Table III. We ï¬nd that most websites in îˆ³
3 are related to
art, design, and cinema industry which like to present their
stylish design skills but unfortunately make the web peeking
attack easier. About 1/3 of the websites are designersâ€™ or
studiosâ€™ websites that computer science/security researchers
may overlook. Furthermore, 72 out of the 117 websites are
ranked on Alexa from 38 to 8,851,402 with 5 websites among
the top 10,000.

17

Exp. Time: 1/4sExp. Time: 1/20sExp. Time: 1/40sExp. Time: 1/80sISO: 100ISO: 100ISO: 100ISO: 100ISO: 100ISO: 1000ISO: 1600ISO: 3200Exp. Time: 1/80sExp. Time: 1/80sExp. Time: 1/80sExp. Time: 1/80sChangeExposureChangeISOWebcamNikon Z7IdealFrame 1Frame 2Frame 3(a)(b)Fig. 15. The human recognition accuracy of diï¬€erent letters with (a) the BLB glasses and (b) the prescription glasses. Letters such as â€œRâ€ have been found
the most diï¬ƒcult to read in the reï¬‚ections while letters such as â€œCâ€ and â€œUâ€ have high recognizability. The diï¬€erence is mostly due to the simplicity and
symmetry in the lettersâ€™ structures which lead to smaller degradation of recognizability when the reï¬‚ections are subject to distortions.

Fig. 16. A spectrum of Alexa top 100 websites that are found to be the
easiest (upper) and hardest (lower) to recognize in our evaluation of website
recognition under webcam peeking attacks. Screenshots of each website are
rotated by 90 degrees and concatenated horizontally. Correlations scores
between the rank of website recognition easiness and website pixel valuesâ€™
average and standard deviation are -0.33 and 0.45 respectively, suggesting
darker websites with high-contrast graphical contents are easier to recognize.

APPENDIX I
HUMAN SUBJECTS RESEARCH INFORMATION

AMT. The AMT study received IRB waiver from the
authorsâ€™ institutes. The survey results downloaded from AMT
website are de-anonymized by only keeping their answers
and deleting all other information including worker IDs. The
results on the AMT website are deleted. We provided com-
pensations of $18/h for the workers.

User Study. The participants were anonymized with random
orders. No personal information other than the videos and
questionnaires was collected. The HTML ï¬les they used were
created randomly by the authors and do not
involve the
participantsâ€™ private information or contain any unethical or
disrespectful information. The participantsâ€™ videos were used
only for this research and not disclosed to third parties or used
for other purposes.

APPENDIX J
USER STUDY FACTOR ESTIMATION

The overall environmental light intensity and is estimated
by taking the average pixel luminance in the video frames.
The screen brightness is estimated by taking the average pixel
luminance in glass area containing the screen reï¬‚ections. The
glass-screen distance is estimated by the physical average
physiognomical face height [36] and physiognomical pixel
height in the video frames using similar formula as Equa-
tion 2.

Fig. 17. (a) The comparison between reconstructed images when the video is
recorded locally on the victim device and over Zoom with diï¬€erent network
upload bandwidths. (b) Changes of reï¬‚ection recognizability with diï¬€erent
text-background color contrast. (c) Changes of reï¬‚ection recognizability with
diï¬€erent background colors (reï¬‚ectance). We tested gray-scale colors with
the same RGB values, which have relatively uniform reï¬‚ectance on the
visible light spectrum. (d) Changes of reï¬‚ection recognizability under diï¬€erent
environmental light intensity. (e) Changes of reï¬‚ection recognizability with
diï¬€erent screen brightness.

18

ï¼ˆaï¼‰ï¼ˆbï¼‰ABCDEFGHIJKLMNOPQRSTUVWXYZ0255075100RecognitionAccuracy (%)ABCDEFGHIJKLMNOPQRSTUVWXYZ0255075100RecognitionAccuracy (%)Recognition Easiness Rank Top 15WebsitesRecognition Easiness Rank Bottom 15 WebsitesLocal10000 kb/s5000 kb/s2000 kb/s1000 kb/s500 kb/s(a)WebcamAuto-Exp.Nikon Z7Manual-Exp.r,g,b=30r,g,b=110r,g,b=140r,g,b=180r,g,b=220(c)20 lux100 lux220 lux360 lux450 luxWebcamAuto-Exp.Nikon Z7Manual-Exp.(d)20%40%60%80%100%WebcamAuto-Exp.Nikon Z7Manual-Exp.(e)(b)Dark TextWhite Bg.White TextDark Bg.diff=250diff=200diff=150diff=100diff=50