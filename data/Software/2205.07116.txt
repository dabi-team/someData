Practical Insights of Repairing Model Problems on Image
Classification

2
2
0
2

y
a
M
4
1

]

G
L
.
s
c
[

1
v
6
1
1
7
0
.
5
0
2
2
:
v
i
X
r
a

Akihito Yoshii
Susumu Tokumoto
{yoshii.akihito,tokumoto.susumu}@fujitsu.com
Fujitsu Limited
Kawasaki, Kanagawa, Japan

ABSTRACT
Additional training of a deep learning model can cause negative
effects on the results, turning an initially positive sample into a
negative one (degradation). Such degradation is possible in real-
world use cases due to the diversity of sample characteristics. That
is, a set of samples is a mixture of critical ones which should not be
missed and less important ones. Therefore, we cannot understand
the performance by accuracy alone. While existing research aims
to prevent a model degradation, insights into the related methods
are needed to grasp their benefits and limitations. In this talk, we
will present implications derived from a comparison of methods
for reducing degradation. Especially, we formulated use cases for
industrial settings in terms of arrangements of a data set. The
results imply that a practitioner should care about better method
continuously considering dataset availability and life cycle of an
AI system because of a trade-off between accuracy and preventing
degradation.

CCS CONCEPTS
â€¢ Software and its engineering â†’ Software development tech-
niques; â€¢ Computing methodologies â†’ Supervised learning by
classification; Heuristic function construction.

KEYWORDS
AI, machine learning, deep learning, image classification

1 DEGRADATION IN DEEP LEARNING TASKS
A deep learning model can be trained multiple time (retrained) with
additional datasets. Considering a classification task, the model
accuracy is expected to be increased after the retraining.

However, a part of samples can change from a positive (classified
correctly) to a negative (classified incorrectly) even if the accuracy
increases totally. We call such a situation as "degradation" in this
talk. Especially in a real-world setting, available additional datasets
are varying in size, quality and importance; and thus, a model can
be susceptible to a degradation-causing sample.

In a use case that a misclassification of samples leads to seri-
ous accident, those samples should be maintained to be positive.
However, such situation cannot always be measured with accuracy.
This is because even with a highly accurate model which classifies
almost all samples correctly, the model is not acceptable if the most
of â€œimportantâ€ samples are belong to the misclassified ones.

Fuyuki Ishikawa
National Institute of Informatics
Tokyo, Japan
f-ishikawa@nii.ac.jp

2 EXISTING APPROACHES AND OUR

MOTIVATION

In order to reduce the degradation, some works improve retrain-
ing procedure whereas others propose different approaches. Our
existing work, NeuRecover[4] is a non-retraining method which
enables a developer to repair a part of a DNN model weights with
smaller data samples. On the other hand, crafting retraining is
another approach to reduce degradation. For example, Backward
Compatibility ML[1][3] introduces a penalty term to a loss function.
It is designed to prevent new errors which did not exist before a
retraining which affects a user expectation[1].

We need to grasp advantages and limitations of different methods
because they have similar characteristics considering the reduction
of the degradation while they are developed by different focuses.
In this talk, we will present implications on suitable use cases
for each method, derived from a comparison of methods for reduc-
ing degradation. This discussion is important because maintaining
model quality leads to a reliable AI-powered software performance.
Although an accuracy is one of metrics indicating model perfor-
mance, a trade-off exists between an accuracy and certain kinds
of others. Hypothesizing real use cases will be a clue for making
decision under the trade-off and achieving an intended task.

3 EXPERIMENT AND RESULTS
We formulated three use cases from an aspect of dataset arrange-
ment: exclusive (operation-oriented), exclusive (development-oriented)
and inclusive.

In an operation-oriented use case, an update set (ğ·ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ ) is
larger than a training set (ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›) while a development-oriented
case is the opposite. The "Exclusive" assumes no intersection be-
tween ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ·ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ ; on the other hand, ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› is included
by larger ğ·ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ under the "Inclusive" condition.

Figure 1 shows an overall process of the experiment. We exe-
cuted two retraining methods (categorical cross entropy and Back-
ward Compatibility1 ML[1][3]) and one non-retraining method
(NeuRecover[4]) with datasets arranged based on the aforemen-
tioned conditions. We used CIFAR10[2] and Fashion MNIST[5]; and
then split them into ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ·ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ .

We evaluated results with a test set apart from ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ·ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’
in each dataset and metrics proposed in related work. Especially we
compared BR (Break Rate)[4], BEC (Backward Error Compatibility)[3]
and accuracy improvement rate. BR is defined as ğ‘› (ğ‘ƒ1âˆ©ğ‘2)

[4] and

ğ‘› (ğ‘ƒ1)

1The loss function was imported from https://github.com/microsoft/BackwardCompatibilityML
whose
version is
SUM_OVER_BATCH_SIZE.

reduction parameter was

1.4.2, with the

set

to

 
 
 
 
 
 
Akihito Yoshii, Susumu Tokumoto, and Fuyuki Ishikawa

Figure 1: Overall processes

diversity of data samples in real use cases can cause misclassifica-
tion and thus decreases the classification performance of important
data samples. Practitioners should be aware of trade-off between an
accuracy and other non-accuracy metrics and dynamically choose
each method depending on their purpose.

ACKNOWLEDGMENT
This work was partly supported by JST-Mirai Program Grant Num-
ber JPMJMI20B8, Japan.

REFERENCES
[1] Gagan Bansal, Besmira Nushi, Ece Kamar, Dan Weld, Walter Lasecki, and Eric
Horvitz. 2019. Updates in Human-AI Teams: Understanding and Addressing the
Performance/Compatibility Tradeoff. In AAAI Conference on Artificial Intelligence.
https://www.microsoft.com/en-us/research/publication/updates-
AAAI.
in-human-ai-teams-understanding-and-addressing-the-performance-
compatibility-tradeoff/

[2] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.

Technical Report.

[3] Megha Srivastava, Besmira Nushi, Ece Kamar, Shital Shah, and Eric Horvitz. 2020.
An Empirical Analysis of Backward Compatibility in Machine Learning Systems.
In KDD. https://www.microsoft.com/en-us/research/publication/an-empirical-
analysis-of-backward-compatibility-in-machine-learning-systems/

[4] Shogo Tokui, Susumu Tokumoto, Akihito Yoshii, Fuyuki Ishikawa, Takao Nak-
agawa, Kazuki Munakata, and Shinji Kikuchi. 2022. NeuRecover: Regression-
Controlled Repair of Deep Neural Networks with Training History. Proceedings of
the IEEE International Conference on Software Analysis, Evolution and Reengineering
(SANER).

[5] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST: a Novel Image
Dataset for Benchmarking Machine Learning Algorithms. CoRR abs/1708.07747
(2017). arXiv:1708.07747 http://arxiv.org/abs/1708.07747

BEC is defined as ğ‘› (ğ‘1âˆ©ğ‘2)
calculated by (ğ‘€1ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦)
(ğ‘€2ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦)

ğ‘› (ğ‘2)

[3]. Accuracy improvement rate can be

.

The results suggest that NeuRecover has relatively high BEC
and BR while Accuracy improvement rate is lower than others.
The difference of Accuracy improvement rate among methods is
smaller under the ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› : ğ·ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ = 4 : 1 condition than ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› :
ğ·ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ = 1 : 4 condition. On the other hand, BC(ne) keeps stably
higher even when BR is higher and the accuracy improvement is
small.

4 LESSONS LEARNED
We observed following lessons from the results of our experiment.

Methods depending on a purpose and a dataset arrangement
Suitable method can be chosen in an aspect of metrics stated
in the previous section and each experiment condition.
A possible scenario of the "exclusive" situation is that only a
pre-trained model is available for a developer and the pre-
trained model is updated with a dataset prepared by the
developer at the development / operation time. On the other
hand, an "inclusive" situation assumes that the developer has
a dataset available from the beginning without limitation;
therefore the developer can include initial training data in a
dataset for future updates.
If an environment change at an operation time are so signif-
icant that a model need to be retrained, "exclusive (operation-
oriented)" case can be assumed. While an "exclusive (development-
oriented)" is for the situations that an environment is rela-
tively static.

Accuracy does not always earn the highest priority A prac-
titioner should cope with a trade-off between accuracy and
preventing degradation. For example, Bansal et al. are point-
ing out the discrepancy between human expectation of AI
system and model updates [1]. Thus, higher accuracy and
stably higher BEC can be the matter. On the other hand, even
if accuracy is lower than other methods, a method whose
BEC is higher and BR is lower can be a choice when the
improvements of partial important case are needed.

5 CONCLUSION
We compared methods which aim to improve degradation of clas-
sification result. The degradation aspect is important because the

M1: Initially trainedby Dtraintraining setM2: Obtained from M1 beingupdated by Dupdatetraining setTrainâš«Epochs: 5âš«Batch-size:128 (Retrain) /32 (NeuRecover)âš«8-layer CNNPositiveP1NegativeN1PositiveP2NegativeN2UpdateEvaluateEvaluateDtrainDupdateDtestDataset size ratio in the experimentâš«Exclusiveâš«Dtrain:Dupdate=1:4 (operation-oriented)âš«Dtrain:Dupdate=4:1 (development-oriented)âš«Inclusiveâš«Dtrain:Dupdate=1:5âš«Dtrain:Dupdate=4:5