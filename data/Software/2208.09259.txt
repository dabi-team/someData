2
2
0
2

g
u
A
9
1

]
E
S
.
s
c
[

1
v
9
5
2
9
0
.
8
0
2
2
:
v
i
X
r
a

Awaiting for Godot: Stateless Model Checking that
Avoids Executions where Nothing Happens
(Extended Version with Proofs)

Bengt Jonsson
Uppsala University, Sweden
Email: bengt@it.uu.se

Magnus Lång
Uppsala University, Sweden
Email: magnus.lang@it.uu.se

Konstantinos Sagonas
Uppsala University, Sweden and NTUA, Greece
Email: kostis@it.uu.se

Abstract—Stateless Model Checking (SMC) is a veriﬁcation
technique for concurrent programs that checks for safety viola-
tions by exploring all possible thread schedulings. It is highly
effective when coupled with Dynamic Partial Order Reduction
(DPOR), which introduces an equivalence on schedulings and
need explore only one in each equivalence class. Even with
DPOR, SMC often spends unnecessary effort in exploring loop
iterations that are pure,
i.e., have no effect on the program
state. We present techniques for making SMC with DPOR more
effective on programs with pure loop iterations. The ﬁrst is a
static program analysis to detect loop purity and an associated
program transformation, called Partial Loop Purity Elimination,
that inserts assume statements to block pure loop iterations. Sub-
sequently, some of these assumes are turned into await statements
that completely remove many assume-blocked executions. Finally,
we present an extension of the standard DPOR equivalence,
obtained by weakening the conﬂict relation between events. All
these techniques are incorporated into a new DPOR algorithm,
OPTIMAL-DPOR-AWAIT, which can handle both awaits and
the weaker conﬂict relation,
in the sense that it
explores exactly one execution in each equivalence class, and can
also diagnose livelocks. Our implementation in NIDHUGG shows
that these techniques can signiﬁcantly speed up the analysis of
concurrent programs that are currently challenging for SMC
tools, both for exploring their complete set of interleavings, but
even for detecting concurrency errors in them.

is optimal

I. INTRODUCTION

Ensuring correctness of concurrent programs is difﬁcult,
since one must consider all
the different ways in which
actions of different threads can be interleaved. Stateless model
checking (SMC) [9] is a fully automatic technique for ﬁnding
concurrency bugs (i.e., defects that arise only under some
thread schedulings) and for verifying their absence. Given a
terminating program and ﬁxed input data, SMC systematically
explores the set of all thread schedulings that are possible
during program runs. A special runtime scheduler drives the
SMC exploration by making decisions on scheduling whenever
such choices may affect the interaction between threads. SMC
has been implemented in many tools (e.g., VeriSoft [10],
CHESS [20], Concuerror [6], NIDHUGG [2], rInspect [24],
CDSCHECKER [21], RCMC [14], and GENMC [18]), and
successfully applied to realistic programs (e.g., [11] and [17]).
SMC tools typically employ dynamic partial order reduction
(DPOR) [8, 1] to reduce the number of explored schedulings.
DPOR deﬁnes an equivalence relation on executions, which

p

if(x[0] > x[1])

swap(x[0], x[1]);

y := 1;
do b := y
while(b 6= 2);
if(x[0] > x[1])

swap(x[0], x[1])

q
do a := y
while(a 6= 1);
if(x[1] > x[2])

swap(x[1], x[2]);

y := 2

Figure 1: A concurrent program implementing a sorting network. p sorts x[0]
and x[1], and then uses y to signal that x[1] is ready. q waits for y to be 1
and then sorts x[1] and x[2], completing one round of bubble sort. In the
second round, shown in blue, q signals that the next “generation” of x[1] is
ready by setting y to 2, upon which p ﬁnishes the sort by sorting x[0] and
x[1] again. Initially y = 0.

preserves relevant correctness properties, such as reachability
of local states and assertion violations. For correctness, DPOR
needs to explore at least one execution in each equivalence
class. We call a DPOR algorithm optimal if it guarantees the
exploration of exactly one execution per equivalence class.

In SMC, loops have to be bounded if they do not already
terminate in a bounded number of iterations. Loop bounding
may in general not preserve assertion failures. Hence a fairly
large loop bound should be used, but this is often practically
infeasible, and thus loop bounding must strike a balance be-
tween these two concerns. However, for loops whose execution
has no global effects, the number of equivalence classes that
need be explored by SMC can be signiﬁcantly reduced while
still preserving correctness properties, using techniques that
we will present in this paper.

Consider the ﬁrst round of the program snippet in Fig. 1
(shown in black), where thread q executes a loop that waits for
thread p to set the shared variable y to 1. A naïve application
of SMC with DPOR will explore an unbounded number of
executions, since (in the absence of loop bounding) there is an
inﬁnite number of equivalence classes, one for each number of
performed loop iterations. All iterations of this loop, however,
are pure, i.e., they have no effect on the program state. For
such loops, a bound of one will preserve correctness properties.
In our example, the do-while loop of thread q can be rewrit-
ten into the sequence of statements a := y; assume(a = 1),
which will cause the SMC exploration to permanently block
thread q whenever the condition of the assume is violated.

Using assume statements to bound loops causes executions
where the condition of the assume is violated and its cor-
responding thread is blocked to be explored. This happens

 
 
 
 
 
 
even if the condition will eventually be satisﬁed, and the orig-
inal loop will exit, under any fair thread scheduling. Assume-
blocking of a thread can occur in many contexts, each generat-
ing an execution that need not be explored. (We will shortly see
this for the example in Fig. 1.) Furthermore, and perhaps more
seriously, this use of assumes prevents SMC from diagnosing
livelocks in which the loop never exits even under fair thread
scheduling. This is because a blocked execution corresponding
to a livelock can also result from a spurious execution in which
the assume reads a shared variable before it has been written
to by another thread.

Here is where await statements can lead to further reduc-
tions. An await loads from a shared variable, but only if the
loaded value satisﬁes some condition, otherwise it blocks. In
contrast to assume-blocking, await-blocking is not permanent
but can be repealed if the condition is later satisﬁed. Thereby,
executions where blocking occurs by reading “too early” are
avoided. Moreover, such executions can be distinguished from
livelocks, in which the condition is not satisﬁed after some
bounded time. For our example, the rewrite of the do-while
loop into an await(y = 1) statement results in a program for
which SMC would explore only a single execution in which
the await reads the value written by thread p.

Consider now the full program in Fig. 1, which performs
a concurrent sort of a three-element array using a sorting
network. This program can be scaled to larger arrays for
increased available parallelism. Since any network sorting an
array of size n will have at least Ω(n log n) occurrences of
a code snippet which exchanges two values after exiting a
spinloop, exploring such a program with SMC will explore
Ω(2n logn) executions, even after rewriting the spinloops using
assume statements. On the other hand, when using await
statements, all executions fall into the same equivalence class.
Thus, an optimal SMC algorithm that can properly handle
awaits will explore only one execution, thereby achieving
exponential reduction.

In this paper, we present techniques to (i) automatically
transform a program to an intermediate representation that
uses await as a primitive, and (ii) explore its executions
using a provably optimal DPOR algorithm that is await aware
and also uses a conﬂict relation between statements which
is weaker than the standard one. We ﬁrst present a static
program analysis technique to detect pure loop executions
and an associated program transformation, called Partial Loop
Purity (PLP) Elimination, that inserts assume statements which
are then turned into awaits if preceded by the appropriate
load. We prove that PLP is sound in the sense that it pre-
serves relevant correctness properties, including local state
reachability and assertion failures. We also present and prove
conditions under which PLP is guaranteed to remove all pure
executions of a loop. Finally, we prove that our new DPOR
algorithm OPTIMAL-DPOR-AWAIT, which is an extension of
the Optimal-DPOR algorithm of Abdulla et al. [1, 3], is correct
and optimal, also with respect to our weaker conﬂict relation.
All these techniques are available in NIDHUGG, a state-of-
the-art SMC tool, and in the paper’s replication package [13].

p

do a := x
while(a 6= 1);
b := y

q
y := 42;
x := 1

p
a := x;
assume(a = 1);
b := y

p
await(x = 1);
b := y

(a) A program with a spinloop.

(b) p rewritten with assume.

(c) p rewritten with await.

Figure 2: Multi-threaded program illustrating the rewrites; initially, x = y = 0.
For (b) and (c), q is the same as in (a).

Our evaluation, using multi-threaded programs which are cur-
rently challenging for most tools, shows that our techniques
can achieve signiﬁcant (and sometimes exponential) reduction
in the total number of executions that need to be explored.
Moreover, they enable detection of concurrency bugs which
were previously out-of-reach for most concurrency testing
tools.

II. ILLUSTRATION THROUGH EXAMPLES

In this section, we illustrate our contributions through ex-
amples. First, in §II-A we show how assume and await
statements are inserted. In §II-B we illustrate how our optimal
DPOR algorithm handles await statements, and in §II-C how
it handles the weaker conﬂict relation in which atomic fetch-
and-adds on the same variable are not conﬂicting.

We consider programs consisting of a ﬁnite set of
threads that share a ﬁnite set of shared variables (x, y, z).
A thread has a ﬁnite set of local registers (a, b, c), and
runs a deterministic code, built from expressions, atomic
statements, and synchronisation operations, using standard
control ﬂow constructs. Atomic statements read or write
to shared variables and local registers, including atomic
read-modify-write operations, such as compare-and-swap
and fetch-and-add.
include
Synchronisation
locking a mutex and joining another thread. Executions of a
program are deﬁned by an interleaving of statements. We use
sequential consistency in this paper, but we note that some
weak memory models (e.g., TSO and PSO) can be modelled
by an interleaving-based semantics, so our work can be
extended to DPOR algorithms [2] that handle such memory
models. Our loop transformations introduce await statements,
that
take a conditional expression over a global variable
as a parameter and come in several forms: simple awaits
(a := await(x = 0)), and
(await(x = 0)),
(a := xchgawait(x = 0, := 1)). These
exchange-await
operations block until their condition is satisﬁed.

load-await

operations

A. Introducing Await Statements

Let us show an example of how loops are transformed by
introducing assume and await statements. Consider the loop
in Fig. 2a. There, thread p executes a spinloop, waiting for
thread q to set the shared variable x. Each iteration of this
loop, in which the value loaded into a is different from 1, is
pure, i.e., it does not modify shared variables, nor any local
register that may be used after the end of the loop. Therefore
an assume statement is introduced at the point where the thread
can distinguish pure executions from impure ones, i.e., after
a has been loaded. The result of such a rewrite is shown in
Fig. 2b. This program has two traces, one in which the assume

Initially: x = y = 0

p
x := 1;
y := 1

q
x := 2;
y := 2

join threads p and q;
assert(|x - y| < 2)

0,0

q1: x := 2

q1: x := 2

2,0

q2: y := 2

p1: x := 1
1,0

p2: y := 1
1,1

q1: x := 2

2,1

q2: y := 2

2,2

0,0

p1: x := 1
1,0

p2: x := 0

0,0

q1: await(x = 0)

0,0

q2: y := 1

0,1

q1: await(x = 0)

Initially: x = y = 0

p
x := 1;
x := 0

q
await(x = 0);
y := 1

Figure 3: Program with a correctness assertion, and execution trees with the
ﬁrst scheduling of the program; nodes show the values of variables x and y.

succeeds, representing the executions in which the original
loop terminates, and one where thread p gets assume-blocked.
The latter trace will exist even in the case where the original
loop is guaranteed to terminate under a fair scheduler. This
problem is remedied by replacing the load into a and the
following assume statement by an await with a test on the
shared variable from which a reads. Such a rewrite results
in the program in Fig. 2c. In this case, the await statement
may permanently block only if the original loop can livelock
under fair scheduling. In our simple example, the rewritten
program has only a single trace, since the original loop is
guaranteed to terminate and can be replaced by the await.
Programs with more complex loops (e.g., loops that are pure
only along a subset of their paths) are also handled by our
program transformation (§III), but the loop is not eliminated
when assumes or awaits are introduced.

B. OPTIMAL-DPOR-AWAIT by Example

DPOR algorithms are based on regarding executions as
equivalent if they induce the same ordering between executions
of conﬂicting statements. The standard conﬂict relation regards
two accesses to the same variable as conﬂicting if at least
one is a write. We begin by illustrating the Optimal-DPOR
algorithm [3] on the simple program in Fig. 3. There two
threads, p and q, write to two shared variables x and y
in sequence. Optimal-DPOR starts by exploring an arbitrary
interleaved execution of the program. Assume it is p1.p2.q1.q2
as shown in Fig. 3 (we will denote executions by sequences of
thread identiﬁers, possibly subscripted by sequence numbers).
Each explored execution is then analysed to ﬁnd races, i.e.,
pairs of conﬂicting events that are adjacent in the happens-
before order induced by the conﬂict relation. (An event is a
particular execution step of a thread in an execution.) Our ﬁrst
execution contains two races, (p1, q1) and (p2, q2). For each
race, Optimal-DPOR creates a so-called wakeup sequence, i.e.,
a sequence which continues the analysed execution up to the
ﬁrst event in a way which reaches the second event instead
of the ﬁrst event. For the ﬁrst race, the wakeup sequence
is q1, and for the second race, it is p1.q1.q2. The wakeup
sequences are inserted as new branches just before the ﬁrst
event of the corresponding race, thereby gradually building a
tree consisting of the explored executions and added wakeup
sequences. The execution tree after the ﬁrst execution is shown
in Fig. 3.

After processing the ﬁrst execution, Optimal-DPOR then
picks the leftmost unexplored leaf in the tree, and extends it

Figure 4: Exploration of a program with an await with two satisfying writes.

arbitrarily to a full execution, in which races are analysed, etc.
As the algorithm backtracks, it deletes the nodes it backtracks
from in the execution tree. The second execution has two
races, (p1, q1) as well as (p2, q2). However, the corresponding
wakeup sequences will result in executions that are redundant,
i.e., equivalent to already inserted ones, so no further insertion
takes place. The algorithm proceeds in this way until there are
no more unexplored leafs corresponding to wakeup sequences.
In total, there are four executions explored by Optimal-DPOR,
corresponding to the four possible ﬁnal valuations of x and y.
Let us now look at how OPTIMAL-DPOR-AWAIT extends
Optimal-DPOR to work for programs with awaits. Consider
the program in Fig. 4. There, p writes to the global variable x,
ﬁrst updating it to 1, and then back to 0. Assume that the ﬁrst
execution is p1.p2.q1.q2. The analysis of races performed by
Optimal-DPOR must now be extended to consider that await
statements are sometimes blocked. First, the conﬂict between
p2 with q1 will not be handled like a race, since q1 is blocked
just before p2. Therefore, we ﬁnd the closest preceding point
in the execution at which q1 is not blocked, which in this case
is at the beginning. We then construct the wakeup sequence
q1 and insert it at the beginning; cf. Fig. 4. Since this program
only has two traces, OPTIMAL-DPOR-AWAIT will terminate
after exploring the second execution.

C. Handling Atomic Fetch-and-Add Instructions in DPOR

To reduce the number of equivalence classes that need
be explored by a DPOR algorithm, one can weaken the
standard conﬂict relation between statements by considering
two atomic fetch-and-add (FAA) statements on the same
variable as non-conﬂicting if the loaded values are afterwards
unused. In the absence of await statements, many existing
DPOR algorithms like Optimal-DPOR handle this deﬁnition
without modiﬁcation. However, this weakening has a subtle
interaction with await statements that must be handled by
OPTIMAL-DPOR-AWAIT.

Consider the program in Fig. 5. In this program, three
threads, p, q, and r, add atomically to the shared variable
x, and a thread s awaits x having the value 3. We assume
that DPOR considers the FAA statements p1, q1, and r1 to be
non-conﬂicting, but conﬂicting with the statement s1, should
it execute.

Assume that the ﬁrst explored execution is p1.q1.r1. From
this point, we cannot substitute s1 for either of p1, q1, or r1, as
s1 is not enabled after any of q1.r1, p1.r1 or p1.q1, respectively.
Yet, there is another execution in which s1 is enabled. In

p1: x+:=1
1,0

q1: x+:=1

2,0

r1: x+:=3

5,0

0,0

r1: x+:=3
3,0

s1: await(x = 3)

p
x +:= 1

q
x +:= 1

r
x +:= 3

s
await(x = 3);
y := 1

Figure 5: Exploration of a program with fetch-and-adds. Initially, x = y = 0.

order to construct this execution, we must not only schedule
s1 before one of the other events, but before two, both of p1
and q1, so that only r1 remains. Then, we could construct the
wakeup sequence r1.s1. In general, OPTIMAL-DPOR-AWAIT
may need to reorder the sequence of independent FAAs that
precede an await statement and select a subsequence of them,
in order to unblock the await statement. This can be done in
several ways, and OPTIMAL-DPOR-AWAIT is optimised to
avoid enumerating all of them. In §IV-B, we will see how.

III. PARTIAL LOOP PURITY ELIMINATION

In this section, we describe Partial Loop Purity Elimination,
a technique that prevents SMC from exploring executions
with pure loop iterations. It consists of (1) a static analysis
technique which annotates programs with conditions under
which a loop will execute a pure iteration, and (2) a program
transformation which inserts assume statements based on the
analysis.

We consider loops consisting of a set of basic blocks, with a
single header block. Each basic block contains a sequence of
program statements. Blocks are connected via edges, labelled
by conditions. We also consider program representations on
Static Single Assignment (SSA) form, which means that each
register is assigned by exactly one statement. Thus, a register
uniquely identiﬁes the statement that assigns to it. When the
value of a register in one block depends on which predecessor
block was executed, this is expressed using a phi node. For
example, in a block C with predecessors A and B containing
registers a and b, respectively, the statement c := φ(A : a, B : b)
deﬁnes the register c to get the value of a when the previous
basic block was A and of b when the previous block was B.
An execution of a loop iteration is pure if the execution
starts and ends at the header of the loop, and during the
iteration (i) no modiﬁcation of a global variable is performed,
(ii) nor of any local variable that may be used after the end of
the iteration, and (iii) no internal (not to the header) backedge
is taken. In SSA form, modiﬁcation of local variables can be
inferred from the phi nodes in the header. If such a phi node
uses a different value on the backedge to the header than when
ﬁrst entering, then the loop iteration modiﬁed a local variable
that is used on some path after the iteration, and we call the
header impure along the backedge. Our deﬁnition considers
executions that complete inner loop iterations to be non-pure.
However, our PLP transformation will block inner loops from
completing pure iterations.

A register a reaches a program point l if all paths to l
pass a’s deﬁnition. During a loop execution, we say that

. . .

a := x;
b := y

a ≥ 4

a ≥ 4

a 6= 4

a = 4

z := 42

a < 4

. . .

. . .

[a > 4]
a := x;
assume(a ≤ 4);
[a > 4]
b := y
[a > 4]

a 6= 4
[a ≥ 4]

a < 4

. . .

a = 4

[False]
z := 42;
[a ≥ 4]

(a) A loop with non-purity
and conditional branches.

(b) The loop annotated with FPCs and
with the assume that is inserted.

Figure 6: Program snippet illustrating the concepts of the PLP transformation.

an expression over registers is deﬁned-true at some program
point l in the loop, if the expression evaluates to true under
(i) the current valuation of registers that were assigned either
outside the loop or during the current loop iteration, and
(ii) any valuation of all other registers. We now deﬁne a central
concept; that of the Forward Purity Condition.

Deﬁnition 1 (Forward Purity Condition). Let l be a program
point in a loop. Then, a Forward Purity Condition (FPC) at l
is an expression in Disjunctive Normal Form over the registers
such that if an execution, without leaving the loop or taking
an internal backedge, proceeds to a program point l′, at which
the expression is deﬁned-true, then
(i) the execution from l′ will reach the loop header without

taking an internal backedge, and

(ii) the execution from l to the loop header will not modify
any global variables nor any local variable that may be
used after execution has reached the loop header.

We will denote a FPC with brackets, for example [c > 42] or
[False]. A purity condition (PC) of a loop is a FPC of the loop
at the beginning of its header. Thus, whenever a loop iteration
passes a program point where the PC is deﬁned-true, and has
not taken an internal backedge, then that iteration is pure.

We illustrate these concepts for the program snippet in
Fig. 6a. In it, the loop loads x and y into registers a and b, then
branches on the value of a, and along the path where a = 4,
there is a write to z. Since a write to a global variable is
non-pure, the loop is not pure whenever a = 4. The two paths
converge in a common block where a loop condition (a ≥ 4) is
checked. This loop is pure if (i) it takes the backedge, i.e., a ≥ 4
holds, and (ii) the write to z is not performed, i.e., a 6= 4 also
holds. The conjunction of these conditions, a > 4, becomes
a purity condition for the entire loop. We thereafter insert an
assume with the negation of a disjunct of the PC at the earliest
point that it is deﬁned-true, i.e., after the load of x, shown in
blue in Fig. 6b.

Let us now describe the analysis stage for computing purity
conditions. Its ﬁrst step is to compute FPCs at all points in
the loop. Intuitively, the FPC at a point l
is a disjunction
c1 ∨ · · · ∨ cn, where each ci is a (forward) path condition for

reaching the header via a pure execution from l. We compute
FPCs by backwards propagation through statements and basic
blocks. Let FPC(s•) be the FPC immediately after statement
s, let FPC(•s) be the FPC immediately before statement s,
let FPC(•B) be the FPC at the beginning of block B, and let
FPC(B•) be the FPC at the end of block B.

For each statement s, we compute FPC(•s) as FPC(s•) ∧ g,
where g is the condition under which s does not update a
global variable. For instance, g is False for stores, True for
loads, a = 0 for an atomic add of form x +:= a, a = b for an
atomic exchange of form b := xchg(x,a), and c = 1 for an
atomic compare-exchange of form c := cmpxchg(x,a,b).

FPCs for basic blocks are computed as follows. First, for an
edge with condition g from a block A in the loop to a block B,
let FPC(A, B) be the FPC along that edge, deﬁned as follows;

• if B is outside the loop, then FPC(A, B) = [False],
• if B is the header block, then if B is impure along (A, B),
then FPC(A, B) = [False], otherwise FPC(A, B) = [g].
• if B is inside the loop, then FPC(A, B) = [False] if the
edge from A to B is an internal backedge (A, B), otherwise
FPC(A, B) = [FPC(•B) ∧ g],

We propagate FPCs backwards through basic blocks by the
above rules for statements. We then compute the FPC at the
end of a block A with outgoing arcs to B1, . . . , Bk as FPC(A•) =
k
i=1 FPC(A, Bi). We can thereafter calculate FPCs for basic
W
blocks by starting from the edges that leave the loop or go
back to its header. Cycles in the control ﬂow graph are no
issue, since the FPC of a backedge (A, B) does not depend on B.
In Fig. 6b, we can see the FPCs computed by the analysis on
the example.

After the analysis, we insert assume statements. Given a
purity condition of form c1 ∨ c2 ∨ · · · ∨ cn, for each ci we
insert an assume(¬ci) at the earliest point that is textually
after the deﬁnitions of all registers in ci. For registers that do
not reach the insertion location, arbitrary values can be used
when execution does not pass their deﬁnitions. Moreover, if
any memory access along the path corresponding to ci cannot
be statically determined not to segfault, we must not insert ci
before that memory access. For this purpose, we associate an
optional “earliest insertion point” with every ci in each FPC
computed by the analysis. Finally, to exclude paths that took
some internal backedge, a “took internal backedge” boolean
register is introduced, computed by phi-nodes, and included in
the conjunction ci.

Theorem 1, whose proof appears in Appendix A, states two
essential properties of PLP. These properties intuitively say
that PLP removes pure executions while preserving relevant
correctness properties. If σ is a local state occurring in a loop
L of a thread p, we say that L is unavoidably pure from σ
to denote that whenever thread p is in local state σ during
an execution, then p is in the process of completing a pure
iteration of L.

Theorem 1. Let P′ be the program resulting from applying
PLP to P. Then P′ satisﬁes the following properties.

1) Local State Preservation: each local state σ of a thread p
which is reachable in P is also reachable in P′, provided
no loop of p is unavoidably pure from σ.

2) Pure Loop Elimination: no execution of P′ exhibits a

completed pure loop iteration of some thread.

We remark that in the deﬁnition of pure loop iterations,
we assume possibly conservative characterisations of “global
variable” and “local variable that may be used after the end of
the iteration” that can be determined by a standard syntactical
analysis of the program, and hence used in the PLP analysis.

IV. THE OPTIMAL-DPOR-AWAIT ALGORITHM

In this section, we present OPTIMAL-DPOR-AWAIT, a
DPOR algorithm for programs with await statements, which
is both correct and optimal. Given a terminating program on
given input, it explores exactly one maximal execution in each
equivalence class induced by the equivalence relation ≃.

A. Happens-Before Ordering and Equivalence

DPOR algorithms are based on a partial order on the events
in each execution. Given an execution E of a program P, an
event of E is a particular execution step by a single thread; the
i’th event by thread p is identiﬁed by the tuple hp, ii, and
e
b
denotes the thread p of an event e = hp, ii. Let dom(E) denote
the set of events in E. We deﬁne a happens-before relation on
dom(E), denoted hb
−→E , as the smallest transitive relation such
that e hb
(i) e and e′ are performed by the same thread, e spawns the
thread which performs e′, or e′ joins the thread which
performs e, or

−→E e′ if e occurs before e′ in E, and either

(ii) e and e′ access a common shared variable x, at least one
of them writes to x, and they are not both atomic fetch-
and-add operations.

Note that the last condition makes atomic fetch-and-add oper-
ations on the same shared variable independent. It follows that
hb
−→E is a partial order on dom(E). We deﬁne two executions,
E and E ′, as equivalent, denoted E ≃ E ′, if they induce the
same happens-before relation on the same set of events, (i.e.,
dom(E) = dom(E ′) and hb
If E ≃ E ′, then all
variables are modiﬁed by the same sequence of statements,
implying that each thread runs through the same sequence of
local states in E and E ′.

hb
−→E′).

−→E =

B. The Working of the OPTIMAL-DPOR-AWAIT Algorithm

OPTIMAL-DPOR-AWAIT is shown in Algorithm 1. It per-
forms a depth-ﬁrst exploration of executions using the recur-
sive procedure Explore(E), where E is the currently explored
execution, which can also be interpreted as the stack of the
depth-ﬁrst exploration. In addition, for each preﬁx E ′ of E,
the algorithm maintains

• a sleep set sleep(E ′), i.e., a set of threads that should not
be explored from E ′, for the reason that each extension of
form E ′.p for p ∈ sleep(E ′) is equivalent to a previously
explored sequence,

• a wakeup tree wut(E ′), i.e., an ordered tree hB, ≺i, where
B is a preﬁx-closed set of sequences, whose leaves are
called wakeup sequences, and ≺ is the order in which
sequences were added to wut(E ′). For each w ∈ B the se-
quence E ′.w will be explored during the call Explore(E ′)
in the order given by ≺.

All previously explored sequences together with the current
wakeup tree (i.e., all sequences of form E ′.w for w ∈ wut(E ′)
and a preﬁx E ′ of E) form the current execution tree, denoted
E . The branches of E are ordered by the order in which
they were added to the tree. Note that the recursive call to
Explore(E) may insert into wut(E ′) for preﬁxes E ′ of E.

Let v \ p denote the sequence v with the ﬁrst occurrence of
an event by thread p (if any) removed. Let next[E](p) denote
the next event performed by thread p after E. Two important
concepts are races and weak initials.

Deﬁnition 2 (Non-Blocking Races). Let e, e′ be two events in
different threads in an execution E, where e occurs before e′.
Then e and e′ are in a non-blocking race, denoted e -E e′, if
(i) e and e′ are adjacent in hb
−→E e′, and for no
other event e′′ we have e hb
−→E e′), and (ii) e′ cannot
be enabled or disabled by an event in another thread.

−→E (i.e., e hb

−→E e′′ hb

Deﬁnition 3 (Weak Initials). For an execution E.w, the set
of weak initials of w (after E), denoted WI[E](w), is the set
of threads p such that E.w ≃ E.p.(w \ p) if p is in w, and
E.w.p ≃ E.p.w if p is not in w.

Intuitively, p ∈ WI[E](w) if next[E](p) is independent with all
events that precede it in w in the case that p is in w, otherwise
with all events in w. If p ∈ WI[E](w) we say that w is redundant
wrt. E.p, since some extension of E.w is equivalent to some
extension of E.p. An important property of the execution tree
E that is maintained by the algorithm is that an extension w
of an existing sequence E is added only if E does not contain
an execution of form E ′.p such that E ′ but not E ′.p is a preﬁx
of E, and w′.w is redundant wrt. E ′.p, where E ′ is deﬁned by
E = E ′.w′.

For the OPTIMAL-DPOR-AWAIT algorithm, we deﬁne
• pre(E, e) as the preﬁx of E up to but not including e,
• notdep(e, E) as the subsequence of E of events that occur

after e but do not happen-after e.

• u .[E] w to denote that E.u.v ≃ E.w for some v; intuitively

u is a “happens-before preﬁx” of w.

The

algorithm runs

in two phases:

race detection
(lines 3–22) and exploration (lines 24–33). Exploration picks
the next unexplored leaf of the exploration tree and extends it
with arbitrary scheduling to a maximal execution. This leaf is
reached step-by-step: at each step, the current execution E is
extended by the leftmost child of the root of wut(E) and used in
a recursive call to Explore (lines 28–31) in order to perform
the next step. If wut(E) only contains the empty sequence,
an arbitrary thread is chosen for the next step and added to
wut(E) (line 26). This step-by-step extension of the current
execution is continued until a maximal execution is reached.
At each step, the new sleep set after E.p is constructed by

Algorithm 1: OPTIMAL-DPOR-AWAIT

Initial call: Explore(hi) with wut(hi) = h{hi} , /0i, sleep(hi) = /0

1 Explore(E)
2

if enabled(E) = /0 then

foreach e, e′ ∈ dom(E) such that (e -E e′) do

let E′ = pre(E, e)
e′)
let v = (notdep(e, E).
b
if sleep(E′) ∩ WI[E ′](v) = /0 then insert(v, E′)

foreach he′, E′i ∈ ({hnext[E](p), Ei| p is blocked after E}

∪ {he′, pre(E, e′)i | e′ is in E and may block}) do

can-stop := False
foreach e in E′ (starting from the end)

that may enable or disable e′ do

let E′′ = pre(E, e)
let w = notdep(e, E)
if e conﬂicts with all events that may

enable or disable e′ then can-stop := True

did-insert := False
foreach maximal subsequence u of w such that
u .[E ′′] w and e′ is enabled after E′′.u do

did-insert := True
e′
let v = u.
b
if sleep(E′′) ∩ WI[E ′′](v) = /0 then insert(v, E′′)

if can-stop and did-insert then break

else

if wut(E) = h{hi} , /0i then
choose p ∈ enabled(E)
wut(E) := h{p} , /0i
while ∃p ∈ wut(E) do

let p = min≺{p ∈ wut(E)}
sleep(E.p) := {q ∈ sleep(E) | p, q independent after E}
wut(E.p) := subtree(wut(E), p)
Explore(E.p)
add p to sleep(E)
remove all sequences of form p.w from wut(E)

33
34 insert(v, E′)
u := hi
35
let c be the list of children of u in wut(E′) from left to
right

36

foreach sequence u.p in c do

if p ∈ WI[E ′.u](v) then

if p 6∈ v or (v := v \ p) = hi then return
u := u.p
if u is a leaf of wut(E′) then return
goto line 36

add v as a new rightmost descendant of u in wut(E′)
return

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

37

38

39

40

41

42

43

44

taking the elements of sleep(E) that are independent with p.
After a recursive call to E.p, the subtree rooted at E.p can be
removed from the wakeup tree. To remember that we should
not attempt to explore any sequences that are redundant wrt.
E.p, we add p to sleep(E).

The race detection phase is entered when the explored
sequence E is maximal. There we examine E for races and
construct new non-redundant executions. We distinguish be-
tween two types of races: non-blocking races, such as between
a write and a read, handled on lines 3–6, and blocking races,
such as involving an await event, handled on lines 7–22.

For each non-blocking race e -E e′, we let E ′ be the preﬁx
of E that precedes e, and construct a wakeup sequence v by

e′ to the subsequence of events that occur after e
appending
b
in E but do not happen-after e (line 5). By construction, the
sequence E ′.v is an execution. Moreover
e 6∈ WI[E′](v) since
b
the occurrence of e′ in v does not happen-after e. Thus, v is
non-redundant wrt. E ′.
e. If v is also non-redundant wrt. E ′.p
b
for each p ∈ sleep(E ′), then v is inserted into the wakeup tree
at E ′, extending wut(E ′) with a new leaf if necessary.

Races involving events that can be blocked are handled
at lines 7–22. For each such event e′, we extract the preﬁx
E ′ that precedes e′. Then, for each e in E ′ that potentially
conﬂicts with e′, we extract the preﬁx E ′′ preceding e and
the sequence w of events that does not happen-after e. For
each maximal happens-before preﬁx u of w after which e′ is
e′ (line 20),
enabled, we construct a wakeup sequence v as u.
b
which is checked for redundancy and possibly inserted into
the wakeup tree in the same way as for a nonblocking race.
Such preﬁxes can be enumerated by recursively removing the
sufﬁx of one event that may enable or disable e′ at a time,
stopping whenever e′ is enabled by the current preﬁx. As an
optimisation, implemented by the ﬂags can-stop and did-insert,
once the algorithm has found a wakeup sequence that enables
e′ before some event that conﬂicts with every event that may
enable or disable e′, it needs not consider reversing e′ with
even earlier events e, as those reversals will be considered in
a later recursive call.

The function insert(v, E) for inserting a sequence v into a
wakeup tree wut(E ′) is shown in lines 34–44. Starting from the
root, represented by the empty sequence, it traverses wut(E ′)
downwards (the current point being u), always descending
(line 40) to the leftmost child u.p such that p is a weak initial of
the remainder of v until either (i) arriving at a leaf indicating
that v was redundant to begin with and wut(E ′) can be left
unchanged (line 41), (ii) encountering a p which is not in v, or
exhausting v (line 39), or (iii) arriving at a node with no child
passing the test at line 38, and then adding the remainder of v
as a new leaf (line 43), since it was shown to be non-redundant.
Algorithm OPTIMAL-DPOR-AWAIT is correct and optimal
in the sense that it explores exactly one maximal execution
in each equivalence class, as stated in the following theorem
whose proof is in Appendix B.

a

2.

For

terminating

P,
Theorem
OPTIMAL-DPOR-AWAIT has the properties that (i)
for
each maximal execution E of P, it explores some execution
E ′ with E ′ ≃ E, and (ii) it never explores two different but
equivalent maximal executions.

program

V. IMPLEMENTATION AND EVALUATION

We have implemented our techniques on top of the NID-
HUGG tool. NIDHUGG is a state-of-the-art stateless model
checker for C/C++ programs with Pthreads, which works at the
level of LLVM Intermediate Representation (IR), typically pro-
duced by the Clang compiler. We have added our PLP analysis
and transformations, as well as the rewrite from load-assume,
exchange-assume, and compare-exchange-assume pairs into
load-await and exchange-await, as passes over LLVM IR.

NIDHUGG comes with a selection of SMC algorithms. One of
them is Optimal-DPOR, which we have used as a basis for our
implementation of OPTIMAL-DPOR-AWAIT including IFAA,
the optimisation of treating fetch-and-add instructions to the
same memory location as independent. All the techniques in
this paper are now included in upstream NIDHUGG and are
enabled when giving the -optimal ﬂag.

A. Overall Performance

First, we evaluate our technique and compare its perfor-
mance against baseline NIDHUGG and the SAVER [16] tech-
nique, implemented in a recent version of GENMC [18].
SAVER has a similar goal to our PLP transformation, but tries
to identify pure loop iterations dynamically, aborting threads
if they perform a pure loop iteration. SAVER’s approach does
not allow further rewrite with awaits.

For our evaluation, we used a set of real-world benchmarks
similar to those used by the SAVER [16] paper. We note that
all atomic memory accesses in these benchmarks have been
converted to SC, as this is the only common memory model
that both tools support. Where relevant, benchmarks are ran
with the same loop bound as in the SAVER paper. For most
benchmarks, this is one greater than the number of threads.
After the benchmark name, the number of threads are shown
in parentheses. Benchmarks mcslock, qspinlock and seqlock
are tests of data structures from the Linux kernel. Benchmarks
ttaslock and twalock are mockups based on, but not the same as,
the benchmarks in the SAVER paper, because its authors were
not at liberty to share the original benchmark sources. Both
are tests of locking algorithms. Benchmark mpmc-queue tests
a multiproducer-multiconsumer queue algorithm, linuxrwlocks
tests a readers-writers lock algorithm, treiber-stack tests a lock-
free stack algorithm, and ms-queue tests a lock-free queue.
Benchmarks mutex and mutex-musl test two mutex algorithms,
the second one used in the musl C standard library imple-
mentation. Benchmark sortnet is an extended version of the
concurrent sort program from Fig. 1. In this version, the sorting
networks are generated using Batcher’s odd-even mergesort.
The number of elements sorted is twice the number of threads,
so sortnet(6) sorts 12 elements. In our replication package [13],
all the tools and benchmarks are provided, as well as scripts
that can replicate the tables in this section.

We evaluate all techniques based on the number of execu-
tions they explore. In fact, we show this number using an
addition of form T + B, where T is the number of explored
completed executions and B is the number of executions that
are blocked in the sense that either an await is deadlocked
or some thread is blocked for executing assume(false) (in
NIDHUGG) or a pure loop iteration (in SAVER). We remark
that the SAVER paper reports only the T part, but, as we
will see, often the number of blocked executions is signiﬁcant
and outnumbers the number of explored completed executions.
Obviously, both numbers contribute to the time an SMC tool
takes to explore these programs. The evaluation was performed
on a Ryzen 5950X running a July 2022 Arch Linux system.

Table I: Number of (complete+blocked) executions explored by algorithms implemented in GENMC and NIDHUGG on a set of challenging benchmarks, as
well as the execution time (in seconds) taken. The (cid:28) symbol means that the exploration did not ﬁnish in 1h, and † means that the tool crashed.

GENMC

NIDHUGG

Baseline

SAVER

Baseline

PLP

PLP+Await

. . . +IFAA

Execs

Time

Execs

Time

Execs

Time

Execs

Time

Execs

Time

Execs

Time

Benchmark

qspinlock(2)
qspinlock(3)

mcslock(3)
mcslock(4)

twalock(3)
twalock(4)

6+2
564+462

336+426
26232+33432

96+90
6144+7224

mutex-musl(2)
mutex-musl(3)

20+2
136728+12834

mutex(2)
mutex(3)

12+2
9486+1236

0.02
0.06

0.09
42.06

0.02
0.35

0.02
4.74

0.02
0.35

6+2
564+462

336+426
26232+33432

96+90
6144+7224

20+2
136728+12834

12+2
6582+1188

0.02
0.06

0.09
3.95

0.02
0.36

0.01
5.03

0.02
0.25

ms-queue(3)
ms-queue(4)

925+350
0.13
11696504+8399226 2388.57

75+284
10662+192438

0.06
18.35

6+2
564+462

0.06
0.20

6+2
564+462

0.08
0.20

6+2
564+456

336+426
26232+33432

0.20
16.59

336+426
26232+33432

0.23
16.95

336+72
26232+4824

96+90
6144+7224

0.09
1.40

96+90
6144+7224

0.09
1.45

96
6144

0.08
0.21

0.18
9.53

0.08
0.80

6+2
564+456

336+72
26232+4824

96
6144

20+2
25146+93000

0.07
11.89

20+2
25146+93000

0.07
12.04

0.06
25146+81972 10.90

20

20
14736+36846

12+2
9486+1236

901+374
(cid:28)

0.07
1.07

0.58
(cid:28)

6.95
(cid:28)

0.10
4.94

12+2
6582+1188

901+374
(cid:28)

38033+31993
(cid:28)

36+81
576+2308

9+83
88+2769

0.07
0.84

0.58
(cid:28)

7.24
(cid:28)

0.08
0.30

0.10
0.44

12
6582+336

901+374
(cid:28)

38033
(cid:28)

36
576

9+36
88+729

0.07
0.76

0.59
(cid:28)

4.36
(cid:28)

0.07
0.15

0.08
0.20

10
3618+312

901+374
(cid:28)

3840
(cid:28)

36
576

9+36
88+729

0.09
0.20

0.18
9.43

0.08
0.81

0.07
5.29

0.07
0.44

0.60
(cid:28)

0.54
(cid:28)

0.07
0.15

0.09
0.20

linuxrwlocks(3)
linuxrwlocks(4)

ttaslock(3)
ttaslock(4)

seqlock(3)
seqlock(4)

mpmc-queue(3)
mpmc-queue(4)

treiber-stack(3)
treiber-stack(4)

sortnet(4)
sortnet(5)
sortnet(6)

38033+31993
(cid:28)

162+183
20760+29440

3.03
(cid:28)

0.02
1.34

24+59
1060+5518

162+183
20760+29440

0.02
0.22

0.03
1.46

38033+31993
(cid:28)

162+183
20760+29440

147+230
87980+105123

0.04
19.68

9+83
88+2805

147+230
0.02
0.17 87980+104583

0.14
41.58

11206+11612
(cid:28)

1.35

166+987
(cid:28) 39706+1277783

0.09
87.18

11206+8188
(cid:28)

3.35

0.20
0.24
(cid:28) 39706+1123234 226.45 39706+360426 88.29

166+517

166+840

0.17
76+421
5410+114208 24.15

426
1546168+9216

0.04

274+80
217.44 250088+167916

0.04

274+80
33.17 1546168+9216 403.58 250088+167916

0.16

426

0.14
0.15
0.15
98.24 250088+90896 87.92 250088+90896 88.20

274+60

274+60

†
†
†

†
†
†

1+728
1+15231

0.33
10.87
1+163292 140.83

0.48
1+312
1+4517
9.38
1+38285 100.18

1+312
1+4517
1+38285

0.45
9.47
98.82

1
1
1

0.08
0.08
0.08

1
1
1

0.08
0.08
0.08

In Table I, there are four sets of NIDHUGG columns. Base-
line shows the performance of unmodiﬁed NIDHUGG/Optimal.
The PLP columns shows the performance of using unmodiﬁed
NIDHUGG/Optimal together with Partial Loop Purity Elimina-
tion. Pure loops are bounded with assumes. The PLP+Await
columns shows the result of PLP and transforming assumes
into awaits, where possible. Finally, the . . . +IFAA columns re-
port results from when OPTIMAL-DPOR-AWAIT treats atomic
fetch-and-add operations as independent. For the two sets of
GENMC columns, the SAVER columns show the performance
of GENMC v0.6, which implements the SAVER technique,
and the Baseline columns show the performance of GENMC
v0.5.3, which does not. The timeout we have used for these
benchmarks is 1 hour.

Starting at the top of Table I, qspinlock is a benchmark that
does not beneﬁt from SAVER nor PLP, but establishes that the
baseline algorithms of both tools are very similar but GENMC
is faster. In the next four benchmarks (mcslock, twalock, mutex,
and mutex-musl), both PLP and SAVER are ineffective, but
awaits eliminate most of the blocked traces (in mcslock) or
all of them (in the remaining three). Moreover, we see that
IFAA is effective in mutex and mutex-musl, and manages to
almost halve the total number of executions explored.

PLP fails to identify the loop purity in ms-queue. The
restriction on the form of purity conditions imposed by our
implementation in NIDHUGG is underapproximating the purity
condition to [False]. This demonstrates a downside with doing
purity analysis statically, as SAVER never needs to represent

purity conditions in order to eliminate pure loop iterations.

In linuxrwlocks, PLP is ineffective, because this benchmark
does not contain pure loop iterations as we have deﬁned them.
Rather, the loop contains a pair of fetch-and-add and fetch-and-
sub that cancel out, which is called a “zero-net-effect” loop in
the SAVER paper [16]. These are out of scope for a static anal-
ysis, as SAVER has to dynamically undo the elimination if a
read appears to have observed the intermediate effect. Despite
the lack of PLP, OPTIMAL-DPOR-AWAIT signiﬁcantly speeds
up linuxrwlocks.

In ttaslock, we believe some implementation issue is pre-
venting SAVER from eliminating pure loop iterations. PLP
does work, however, and awaits eliminate all the blocked
executions.

In the next three benchmarks (seqlock, mpmc-queue and
treiber-stack), PLP discovers the same pure loop iterations
as SAVER, and permits a rewrite to awaits that signiﬁcantly
reduces the search space, even by an order of magnitude for
seqlock, and on mpmc-queue IFAA further halves it.

Finally, OPTIMAL-DPOR-AWAIT really shines on sortnet.
GENMC cannot take advantage of awaits, and so has to ex-
plore an exponential number of (assume-blocked) traces, where
NIDHUGG can explore the program in just one. Unfortunately,
GENMC v0.5.3 crashes on this benchmark, but we believe it
would yield the same numbers as SAVER, which also explores
a signiﬁcant number of redundant executions.

Table II: Number of (complete+blocked) executions that SMC algorithms in GENMC and NIDHUGG explore on shortened, bug-free versions of safestack.

Benchmark

Baseline

SAVER

Baseline

PLP

PLP+Await

. . . +IFAA

GENMC

NIDHUGG

119+6
928+107
7189+296
121334+12652

119+6
safestack-21(2)
19+1
928+107
safestack-31(2)
56+25
7189+296
safestack-32(2)
463+12
safestack-33(2)
121334+12652
2600+1160
safestack-211(3) 1267120+325932 995224+325932 1259280+324382
962+686
(cid:28)
safestack-311(3)
0+14960
(cid:28) 906529+388117 906529+331337 288057+216830
safestack-321(3)

34+2
103+27
1073+27
6434+1636
2690+1126
0+26536

34+1
103+25
1073+12
6434+1584
2690+928
0+24078

119+6
928+107
7189+296
121334+12652

0+275399108
(cid:28)

0+286818740
(cid:28)

B. Effectiveness on SafeStack

Next, we evaluate the ability of OPTIMAL-DPOR-AWAIT
to expose difﬁcult-to-ﬁnd bugs in real-world code bases.
The benchmark we will use is called safestack. It was ﬁrst
posted to the CHESS forum, and subsequently included in the
SCTBench [23] and SVComp benchmark suites. The original
safestack code attempts to implement a lock-free stack but con-
tains an ABA bug which is quite challenging for concurrency
testing and SMC tools to ﬁnd, in the sense that exposing the
bug requires at least ﬁve context switches. The test harness is
also quite big, containing three threads each performing four
operations on the stack. Let us refer to this original harness
as safestack-444 to indicate that each of its three threads
performs four operations (pop, push, pop, push). We will also
use shortened versions of this harness: four versions with just
two threads, and four versions where each of the three threads
performs fewer operations. The smallest harness that exposes
the bug is safestack-331.

We ﬁrst compare the two SMC tools and their algorithms
on versions of safestack that do not exhibit the bug and thus
require exhaustive exploration of all traces. Table II shows the
results. First, notice that the dynamic technique that SAVER im-
plements is completely or mostly ineffective in these programs;
compare it to the baseline numbers. In contrast, PLP achieves
signiﬁcant reduction of the set of executions that NIDHUGG
explores. Finally, both the transformation of assumes to awaits
and the IFAA optimisation are applicable and result in further
reductions in the number of explored executions. The number
of complete traces is 0 on safestack-311 since the code does
not allow popping the last element, so all traces end up with
one thread livelocking in pop with the queue containing only
one element. For Table II, the timeout used is 10 hours.

With our next and last experiment, using safestack-331, we
can evaluate the tools’ abilities to expose the bug. Neither
GENMC, with or without SAVER, nor baseline NIDHUGG ﬁnd
anything after running for more than 2 000 hours! On the other
hand, if we run NIDHUGG with PLP, awaits, and IFAA, it dis-
covers the bug in just 8 minutes, after exploring 2 + 2 453 474
traces. How much of its search space an SMC tool has to
search before it encounters a bug can be up to “luck”, so to
ensure that this result is not due to luck we “ﬁx” the bug by
commenting out all the assertions in the benchmark and run
NIDHUGG again. This gives us an upper bound on the size of
the search space, i.e., how much would need to be searched to
ﬁnd the bug in the worst case, and also provides an indication

of how long it might take to verify the program after ﬁxing
the bug. On the ﬁxed safestack-331, NIDHUGG terminates in
only 24 minutes after exploring 5 772 + 8 521 721 traces. This
demonstrates how the techniques we presented in this paper
substantially reduce the search space on safestack, allowing the
bug to be found or its absence veriﬁed by an exhaustive SMC
technique. To our knowledge, no other exhaustive technique
has ever been able to discover the bug in safestack.

VI. RELATED WORK

Since SMC tools assume the analysed program to terminate,
they must ﬁrst bound unbounded loops. Several tools [2, 21,
14, 15] have an automatic loop unroller that is parameterised
by a chosen loop bound. Several SMC tools, including NID-
HUGG [2], RCMC [14] and GENMC [15], transform simple
forms of spinloops, such as the one shown in Fig. 2a, to assume
statements, but only transform simple polling loops that can
be recognised syntactically. We are not aware of any tool that
transforms loops into await statements, meaning existing tools
are susceptible to scalability problems for programs like the
sorting networks shown in Fig. 1. An SMC technique that
can diagnose livelocks of spinloops under fair scheduling is
VSYNC [22]. However, to do so it enforces fairness, and cannot
bound the loop even with an assume, thus exploring many
more traces than tools which transform spinloops to assumes.
SAVER [16] also aims to block pure loop iterations by in-
troducing assume statements. It identiﬁes pure loop iterations
dynamically, instead of by static analysis as in our approach.
SAVER’s approach allows to detect a larger class of pure loop
iterations, but it does not allow further rewrite with awaits.
Furthermore, our PLP transformation can block a looping
thread at any point in the loop, not just at the back edge.
SAVER also employs several smaller program transformations,
such as loop rotation and merging of bisimilar control ﬂow
graph nodes, that can increase the number of loops that may
qualify as pure. These transformations are orthogonal to the
detection of pure loop iterations, and could also be used in our
framework.

Checking for purity of loop iterations is an idea that has
appeared in other contexts, such as to verify atomicity for
concurrent data structures [7, 19] and to reduce complexity
for model checking them (e.g., [4]).

The Optimal-DPOR algorithm implemented in NIDHUGG,
handles mutex locks but not await statements. In the jour-
nal article of the Optimal-DPOR algorithm [3], principles
for handling other blocking statements are presented. Our

OPTIMAL-DPOR-AWAIT develops these principles into a
practical and efﬁcient algorithm, which we have also imple-
mented in NIDHUGG. As future work, the Optimal-DPOR with
Observers [5] algorithm, which allows two statements to only
conﬂict in the presence of a third event, could also be extended
(potentially at higher cost) to handle awaits.

VII. CONCLUDING REMARKS

We have presented techniques for making SMC with DPOR
more effective on loops that perform pure iterations, including
a static program analysis technique to detect pure loop execu-
tions, a program transformation to block and also remove them,
a weakening of the standard conﬂict relation, and an optimal
DPOR algorithm which handles the so introduced concepts.
We have implemented the techniques in NIDHUGG, showing
that they can signiﬁcantly speed up the analysis of concurrent
programs with pure loops, and also detect concurrency errors.

ACKNOWLEDGEMENTS

This work was partially supported by the Swedish Research
Council through grants #621-2017-04812 and 2019-05466, and
by the Swedish Foundation for Strategic Research through
project aSSIsT. We thank the anonymous FMCAD reviewers
for detailed comments and suggestions which have improved
the presentation aspects of our work.

REFERENCES
[1] P. Abdulla, S. Aronis, B. Jonsson, and K. Sagonas, “Optimal dynamic
partial order reduction,” in Symposium on Principles of Programming
Languages, ser. POPL 2014. New York, NY, USA: ACM, 2014,
pp. 373–384. [Online]. Available: http://doi.acm.org/10.1145/2535838.
2535845

[2] P. A. Abdulla, S. Aronis, M. F. Atig, B. Jonsson, C. Leonardsson,
and K. Sagonas, “Stateless model checking for TSO and PSO,” in
Tools and Algorithms for the Construction and Analysis of Systems, ser.
LNCS, vol. 9035. Berlin, Heidelberg: Springer, 2015, pp. 353–367.
[Online]. Available: http://dx.doi.org/10.1007/978-3-662-46681-0_28
[3] P. A. Abdulla, S. Aronis, B. Jonsson, and K. Sagonas, “Source sets: A
foundation for optimal dynamic partial order reduction,” Journal of the
ACM, vol. 64, no. 4, pp. 25:1–25:49, Sep. 2017. [Online]. Available:
http://doi.acm.org/10.1145/3073408

[4] P. A. Abdulla, F. Haziza, L. Holík, B. Jonsson, and A. Rezine,
“An integrated speciﬁcation and veriﬁcation technique for highly
concurrent data structures,” Int. J. Softw. Tools Technol. Transf., vol. 19,
no. 5, pp. 549–563, 2017. [Online]. Available: https://doi.org/10.1007/
s10009-016-0415-4

[5] S. Aronis, B. Jonsson, M. Lång, and K. Sagonas, “Optimal dynamic
partial order reduction with observers,” in Tools and Algorithms for the
Construction and Analysis of Systems - 24th International Conference,
ser. LNCS, vol. 10806. Cham: Springer, Apr. 2018, pp. 229–248.
[Online]. Available: https://doi.org/10.1007/978-3-319-89963-3_14
[6] M. Christakis, A. Gotovos, and K. Sagonas, “Systematic testing
in Erlang programs,” in Sixth
for detecting concurrency errors
IEEE International Conference on Software Testing, Veriﬁcation and
Validation, ser. ICST 2013. Los Alamitos, CA, USA: IEEE, Mar.
2013, pp. 154–163. [Online]. Available: https://doi.org/10.1109/ICST.
2013.50

[7] C. Flanagan, S. Freund, and S. Qadeer, “Exploiting purity for atomicity,”
IEEE Trans. Software Eng., vol. 31, no. 4, pp. 275–291, Apr. 2005.
[Online]. Available: https://doi.org/10.1109/TSE.2005.47

[8] C. Flanagan and P. Godefroid, “Dynamic partial-order reduction for
model checking software,” in Principles of Programming Languages,
(POPL). New York, NY, USA: ACM, Jan. 2005, pp. 110–121.
[Online]. Available: http://doi.acm.org/10.1145/1040305.1040315
[9] P. Godefroid, “Model checking for programming languages using
VeriSoft,” in Principles of Programming Languages, (POPL). New

York, NY, USA: ACM Press, Jan. 1997, pp. 174–186.
Available: http://doi.acm.org/10.1145/263699.263717

[Online].

[10] ——, “Software model checking: The VeriSoft approach,” Formal
Methods in System Design, vol. 26, no. 2, pp. 77–101, Mar. 2005.
[Online]. Available: http://dx.doi.org/10.1007/s10703-005-1489-x
[11] P. Godefroid, R. S. Hanmer, and L. Jagadeesan, “Model checking
without a model: An analysis of
the heart-beat monitor of a
telephone switch using VeriSoft,” in Proceedings of the ACM SIGSOFT
International Symposium on Software Testing and Analysis, ser. ISSTA.
[Online].
New York, NY, USA: ACM, Mar. 1998, pp. 124–133.
Available: https://doi.org/10.1145/271771.271800

[12] P. Godefroid, G. J. Holzmann, and D. Pirottin, “State-space caching
revisited,” Formal Methods in System Design, vol. 7, no. 3, pp. 227–241,
1995. [Online]. Available: http://dx.doi.org/10.1007/BF01384077
[13] B. Jonsson, M. Lång, and K. Sagonas, “Replication Package for
Awaiting for Godot: Stateless Model Checking that Avoids Executions
where Nothing Happens,” Aug. 2022, artifact for the FMCAD 2022
paper with the same title. [Online]. Available: https://doi.org/10.5281/
zenodo.6979940

[14] M. Kokologiannakis, O. Lahav, K. Sagonas, and V. Vafeiadis, “Effective
stateless model checking for C/C++ concurrency,” Proc. ACM on
Program. Lang., vol. 2, no. POPL, pp. 17:1–17:32, Jan. 2018. [Online].
Available: https://doi.org/10.1145/3158105

[15] M. Kokologiannakis, A. Raad, and V. Vafeiadis, “Model checking for
weakly consistent libraries,” in Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Implementation,
ser. PLDI 2019. New York, NY, USA: ACM, Jun. 2019, pp. 96–110.
[Online]. Available: https://doi.org/10.1145/3314221.3314609

[16] M. Kokologiannakis, X. Ren, and V. Vafeiadis, “Dynamic partial order
reductions for spinloops,” in Formal Methods in Computer Aided
Design, ser. FMCAD 2021.
IEEE, Oct. 2021, pp. 163–172. [Online].
Available: https://doi.org/10.34727/2021/isbn.978-3-85448-046-4_25

[17] M. Kokologiannakis and K. Sagonas, “Stateless model checking of
the Linux kernel’s hierarchical
read-copy-update (tree RCU),” in
Proceedings of International SPIN Symposium on Model Checking of
Software, ser. SPIN 2017. New York, NY, USA: ACM, 2017, pp.
172–181. [Online]. Available: https://doi.org/10.1145/3092282.3092287
[18] M. Kokologiannakis and V. Vafeiadis, “GenMC: A model checker
for weak memory models,” in Computer Aided Veriﬁcation - 33rd
International Conference, CAV 2021, Proceedings, Part I, ser. LNCS,
vol. 12759.
Springer, Jul. 2021, pp. 427–440. [Online]. Available:
https://doi.org/10.1007/978-3-030-81685-8_20

[19] M. Lesani, T. D. Millstein, and J. Palsberg, “Automatic atomicity
veriﬁcation for clients of concurrent data structures,” in Computer
Aided Veriﬁcation, CAV 2014, ser. LNCS, A. Biere and R. Bloem,
Eds., vol. 8559. Cham: Springer, Jul. 2014, pp. 550–567. [Online].
Available: https://doi.org/10.1007/978-3-319-08867-9_37

[20] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, and
in concurrent
I. Neamtiu, “Finding and reproducing heisenbugs
programs,” in Proceedings of the 8th USENIX Symposium on Operating
Systems Design and Implementation, ser. OSDI ’08. Berkeley, CA,
USA: USENIX Association, Dec. 2008, pp. 267–280.
[Online].
Available: http://dl.acm.org/citation.cfm?id=1855741.1855760

[21] B. Norris and B. Demsky, “A practical approach for model checking
C/C++11 code,” ACM Trans. Program. Lang. Syst., vol. 38, no. 3, pp.
10:1–10:51, May 2016. [Online]. Available: http://doi.acm.org/10.1145/
2806886

[22] J. Oberhauser, R. L. d. L. Chehab, D. Behrens, M. Fu, A. Paolillo,
L. Oberhauser, K. Bhat, Y. Wen, H. Chen, J. Kim, and V. Vafeiadis,
“Vsync: Push-button veriﬁcation and optimization for synchronization
primitives on weak memory models,” in Proceedings of the 26th ACM
International Conference on Architectural Support for Programming
Languages and Operating Systems, ser. ASPLOS 2021. New York,
NY, USA: ACM, 2021, p. 530–545. [Online]. Available: https://doi.org/
10.1145/3445814.3446748

[23] P. Thomson, A. F. Donaldson, and A. Betts, “Concurrency testing
using controlled schedulers: An empirical study,” ACM Trans. Parallel
Comput., vol. 2, no. 4, pp. 23:1–23:37, 2016. [Online]. Available: http://
doi.acm.org/10.1145/2858651

[24] N. Zhang, M. Kusano, and C. Wang, “Dynamic partial order reduction
for relaxed memory models,” in Programming Language Design and
Implementation (PLDI). New York, NY, USA: ACM, Jun. 2015,
pp. 250–259. [Online]. Available: http://doi.acm.org/10.1145/2737924.
2737956

A. Proof of Theorem 1

APPENDIX

We prove Theorem 1, i.e., correctness and completeness of
partial loop purity elimination, restated as Theorems 3 and 4.
Theorem 3 (Local State Preservation). Let P′ be the program
resulting from applying Partial Loop Purity Elimination to P.
Then each local state σ of a thread p which is reachable in P
is also reachable in P′, provided no loop of p is unavoidably
pure from σ.

We start by proving a weaker statement, namely that PLP

only bounds unavoidably pure executions.

Lemma 1. Whenever thread p in state σ executes a false
assume statement in P′ inserted by the transformation from P,
then σ is in an unavoidably pure loop.

Proof. Assume the counterfactual. State σ must be in a loop
L. Let E be the execution of P′ that leads to state σ. The
assumption is that there is a continuation w of E (if we
ignore the assumes), such that E.w either exits the loop without
returning to the header or completes an impure iteration of L
after σ. Also, there must be some path condition ci
that
PLP analysis computed for L in P which was deﬁned-true
at σ. However, by the construction of purity conditions this ci
must include all branch conditions for returning to the header,
so E.w cannot have exited without returning to the header.
Furthermore, ci would force the execution along a speciﬁc path
through the loop, in which no statement changed the value of a
global variable, took an internal backedge or a backedge along
which the header is impure. This contradicts the assumption.
Thus, it must be false, and Lemma 1 true.

Proof of Theorem 3. If σ is in a loop, it was not bounded
by PLP by Lemma 1, given the assumption that no loop
of p is unavoidably pure from σ. If E is an execution of P
that reaches σ, then by removing any complete pure loop
executions in E, we obtain an execution E′ that also reaches σ.
If any ﬁnal states of any threads (other than p) in E are in
unavoidably pure loops, we furthermore remove them, yielding
another sequence E′′. This sequence is an execution that
reaches σ because the removed events, by being in unavoidably
pure loops, cannot have modiﬁed any global variables, and
so p can still read the sequence of values that is required to
reach σ.

For the purposes of proving Property 2 (completeness) of
Theorem 1, we assume that no underapproximation is per-
formed when FPCs are combined using logical connectives.
Additionally, we assume that only two types of simpliﬁcation
of FPCs are performed. Either 1) two conjunctions a ∧ p and
a ∧ ¬p can be merged to a single conjunction a, and 2) two
terms that reference the same register(s) and one imply the
other can be replaced by one of them.
Theorem 4 (Pure Loop Elimination). Let P′ be the program
resulting from applying PLP to P. Then no execution of P′
exhibits a completed pure loop iteration of some thread.

Proof. Assume the counterfactual. Then there is some execu-
tion E of P′ that exhibits a pure loop iteration. Let e be the
ﬁrst event in the pure loop iteration, i.e., the ﬁrst statement of
the header, and let e′ be the last, i.e., after e′,
e will execute
b
the ﬁrst statement of the header again. Following E in reverse,
we can also follow how FPCs would have been propagated
by PLP, starting at e′, i.e., a backedge to the header. We will
show that an assume statement that evaluates to false in E
must have been inserted between e and e′, thus contradicting
the assumption. We show this by showing that the FPCs at each
point along the pure loop iteration have the property that if they
were the purity condition of the loop, a false-evaluating assume
would have been inserted by PLP along the path from e to e′.
We start with FPC(e′•), and then show that this property is
preserved by all the transfer rules, thus showing that it holds for
FPC(•e), and so for the whole loop. Let us call this property
assumes-false(ϕ) for some FPC ϕ.

Note that we do not need to worry about the “took internal
backedge” conjunct in the assume, as pure loop executions
cannot take inner loops. Such a conjunct will always be true
for the purposes of this proof.

The FPC computed before the statement of e′ is given by the
backedge to the header. Since the loop execution is pure, the
FPC will be [g] where g is the backedge condition. We know
that it holds after e′. Furthermore, since all registers mentioned
in g reach e′, if [g] was the loop purity condition the assume
would be inserted somewhere along the path from e to e′ in E.
Now, we will show that this is preserved by all the transfer
rules. First, we consider the rules for some atomic statement s.
Load is trivial. Stores are not possible in a pure loop execution.
For atomic adds of form x +:= a, we add [a = 0] to each
conjunct. But [a = 0] must evaluate to false at s since the
loop execution is pure, and otherwise the global variable x
would have been modiﬁed by s, thus contradicting the purity
of the loop execution. It must also evaluate to true at any later
insertion point, as no more than one deﬁnition of each register
can appear in a pure loop execution. The insertion location
of FPC(•s) might be later than that of FPC(s•), but all other
terms in FPC(s•) must still evaluate to true if moved later.
The transfer rules for atomic exchange and atomic compare-
exchange preserve the property in the same way as the rule
for atomic add, and can be proven similarly.

Now, if s′

is the ﬁrst statement of a block B, and s is
the last statement of a block A immediately preceding it
in E, we know, inductively, that assumes-false(FPC(•B)), and
then we can show assumes-false(FPC(A•)) as follows: Let g
be the condition on the (A, B) edge. It evaluates to true
at s and later. Thus, assumes-false(FPC(A, B)) holds because
FPC(A, B) = g ∧ FPC(•B), similarly as for atomic statements.
The condition FPC(A•) = FPC(A, B) ∨ ϕ for some ϕ by the
transfer rules. As edge guards are mutually exclusive, we
have g =⇒ ¬ϕ. Since g evaluates to true, FPC(A•) must
evaluate to false. If FPC(A•) contains FPC(A, B) as a disjunct,
then trivially assumes-false(FPC(A•)). However, because of
the limitations on how a FPC may be simpliﬁed, for each
disjunct in FPC(A, B), some disjunct in FPC(A•) must imply it

and contain a subset of the registers in it. As removing register
references from a conjunction in an assumes-false FPC cannot
move its insertion location away from the path followed by E,
we have assumes-false(FPC(A•)).

We have now shown inductively that the pure loop execution
in E must contain an assume with a condition that evaluates
to false. This contradicts the assumption that the pure loop
execution in E is complete. Thus, the assumption is false and
Theorem 4 holds.

The extension to segmentation-faulting instructions can be

proven to satisfy Theorem 4 similarly.

B. Proof of Theorem 2

Let us now prove Theorem 2, i.e., the correctness and
optimality of OPTIMAL-DPOR-AWAIT. We begin with cor-
rectness, stated as Theorem 5, whereafter we go to optimality,
stated as Theorem 6.

Throughout, we assume a particular completed invocation
of OPTIMAL-DPOR-AWAIT. This invocation consists of a
number of terminated calls to Explore(E) for some values of E.
Let E denote the set of executions that have been explored in
some call to Explore(.). Deﬁne the ordering ∝ on E by letting
E ∝ E ′ if Explore(E) returned before Explore(E ′). Intuitively,
if one were to draw an ordered tree that shows how the
exploration has proceeded, then E would be the set of nodes
in the tree, and ∝ would be the post-order between nodes in
that tree. We use w, w′, . . . to range over sequences, e, e′, . . . to
range over events, as well as:

• E ⊢ w to denote that E.w is an execution,
• E ′ ≤ E to denote that the sequence E ′ is a preﬁx of the

sequence E,

• dom[E](w) to denote dom(E.w) \ dom(E), i.e., the events

in E.w which are in w,

• e <E e′ to denote that e occurs before e′ in E, i.e., <E is

the total order of events,

• w ≃[E] w′ to denote E.w ≃ E.w′, and
• [E]≃ to denote the equivalence class of E.

For an execution E.w and thread p,

• let p ∈ I[E](w) denote that p ∈ WI[E](w) and p ∈ w, and
• let E ⊢ p♦w denote that p ∈ WI[E](w) and p 6∈ w, i.e., that

next[E](p) is independent of all events in w.

For an arbitrary execution E ∈ E , let ﬁnal_sleep(E) denote the
value of sleep(E) at the point when Explore(E) returns.

We begin by two useful lemmas. The ﬁrst follows from the

involved deﬁnitions.

line 43 in the function insert(·, ·). This step inserts a sequence
of form E ′.u.v after checking that p 6∈ WI[E′.u](v) for all existing
executions of form E ′.u.p. Previous rounds of the loop at
lines 36–42 checked that p 6∈ WI[E′.u′](u′′.v) for all existing
executions of form E ′.u′.p with u = u′.u′′. Also, the test before
insertion at line 6 and line 21 perform the corresponding check
for preﬁxes of E ′.

We can now prove that Algorithm 1 is correct in the sense
that for each maximal execution E, it explores an execution
in [E]≃. This is formalised in Theorem 5 below. Its proof
is by induction over the executions in E , using the order ∝
in which invocations Explore(E) return. The inductive step
for an execution E is proven by contradiction, by making the
assumption that some maximal sequence E.w is unexplored
after the call Explore(E) returns. The proof then arrives at a
contradition through a sequence of claims. First it is shown
that the assumption implies Claim 1, which states that p 6∈
WI[E](w) for all p ∈ ﬁnal_sleep(E). Thereafter, the sequence
of Claims 2–5 establish that the algorithm must have explored
some sequence which exposes a race, which by Claim 6 causes
the algorithm to include a leaf with properties that contradict
the initial assumption in the inductive step, thereby concluding
the proof of the theorem.

Theorem 5 (Correctness of OPTIMAL-DPOR-AWAIT). When-
ever a call to Explore(E) returns during Algorithm 1, then for
all maximal executions E.w, the algorithm has explored some
execution in [E.w]≃.

Since the initial call to the algorithm is Explore(hi), Theo-
rem 5 implies that for all maximal executions E the algorithm
explores some execution in [E]≃.

Proof. By induction on the set of executions E that are
explored during the considered execution, using the ordering
∝ (i.e., the order in which the corresponding calls to Explore
returned).

Base Case: This case corresponds to the ﬁrst sequence E
for which the call Explore(E) returns. By the algorithm, E is
already maximal, so the theorem trivially holds.

InductiveStep: We prove the inductive step for an arbitrary
execution E in E by contradiction. So, we make the assump-
tion that there exists a sequence E such that when the call to
Explore(E) returns, there is a maximal sequence E.w such that
the algorithm has not explored any in [E.w]≃. To do this, we
employ the following inductive hypothesis:

InductiveHypothesis: The theorem holds for all execution

Lemma 2. If p ∈ WI[E](w) and u .[E] w then p ∈ WI[E](u).

sequences E ′ with E ′ ∝ E.

Lemma 3. During the execution of OPTIMAL-DPOR-AWAIT,
a new leaf E is added to the execution tree E only if there
is no previously added execution of form E ′.p with E = E ′.w,
such that E ′ but not E ′.p is a preﬁx of E, and p ∈ WI[E′](w).
Proof. The invariant is established by examining the steps of
Algorithm 1. The only step which inserts a new sequence
into E is the insertion of a new leaf in a wakeup tree at

Let us continue the proof of the inductive step. Let Sleep and
WuT be the values of sleep(E) and wut(E), respectively, when
the call to Explore(E) is performed. Later, just before Claim 2,
we will impose restrictions on how to choose w among the
ones for which E.w is not explored. We will show that this
leads to a contradiction.

For such w to exist, E cannot be maximal, so ﬁnal_sleep(E)

contains at least one thread. For p ∈ ﬁnal_sleep(E), deﬁne

• E ′

p ≤ E, E ′

p.p ∈ E , and E ′

p, such that E ′

p.p is the last
execution of this form that precedes E (w.r.t. ∝). If
E.p ∈ E then E ′
p = E, otherwise if p ∈ ﬁnal_sleep(E) and
E ′
• w′

p is a strict preﬁx of E.
p.w′
p by E = E ′
p.
It follows that p ∈ WI[E′

p](w′

p).

Claim 1. WI[E](w) ∩ ﬁnal_sleep(E) = /0.

Proof. By contradiction. Assume that there is a p ∈ WI[E](w)∩
ﬁnal_sleep(E). Since w is maximal, p ∈ WI[E](w) implies p ∈
w since p is enabled after E and independent with w, therefore
enabled throughout w. Therefore from Deﬁnition 3 there is a
p.p.w′
w′′ such that E.w ≃ E.p.w′′ ≃ E ′
p.w′′. By
the inductive hypothesis applied to E ′
p.p, the algorithm has
p.w′′]≃ = [E.w]≃, which
explored some execution in [E ′
contradicts the initially made assumption about E.w.

p.p.w′′ ≃ E ′

p.p.w′

p.w′

For p ∈ ﬁnal_sleep(E), deﬁne
• wp, as the longest preﬁx of w such that E ⊢ p♦wp,
• ep, as the ﬁrst event in dom[E](w) which is not in wp. Such
an event ep must exist, otherwise wp = w, which implies
E ⊢ p♦w, which implies p ∈ WI[E](w), which contradicts
Claim 1.

Also deﬁne

• q ∈ ﬁnal_sleep(E), such that wq is a longest preﬁx among
wp. If there are several threads p ∈ ﬁnal_sleep(E) such
that wp is the same longest preﬁx, then pick q such that
E ′

q.q is minimal (w.r.t. ∝).

• Sleep′ as the value of sleep(E ′

q.q) when the call

to

Explore(E ′

q.q) is performed.

Without loss of generality, we will assume that among all the
possible w for which the hypothesis in the Inductive Step holds
(i.e., that E.w not explored), we choose w so that wq (chosen
as described above) is as long as possible.

Claim 2. E ′

q ⊢ q.w′

q.wq.

q.wq.

q ⊢ q♦w′

q ⊢ q (because E ′

q.q was actually explored) and
q.wq) (which follows from E ⊢ q♦wq and the fact
q), it follows that

Proof. Since E ′
E ′
q ⊢ q♦(w′
that q ∈ ﬁnal_sleep(E) implies E ′
q ⊢ q.w′
E ′
q.wq) ∩ Sleep′ = /0.
q.q](w′
Claim 3. WI[E′
Proof. Claim 2 has shown that E ′
q.wq. The proof
is then by contradiction: Assume that some thread p is in
WI[E′

q.wq) ∩ Sleep′.

q ⊢ q.w′

q.q](w′

By the construction of Sleep′ (i.e., sleep(E ′

q.q) and satisfy E ′
q.q](w′
q.q.w′

q.wq) implies that p ∈ WI[E′
q.wq ≃ E ′
q.wq)), implies p ∈ WI[E′
q](w′

the thread p must be in sleep(E ′
Explore(E ′
p ∈ WI[E′
using E ′
q♦(w′
WI[E′
w′
before the call to Explore(E ′
q.w′
up in sleep(E ′

q.q)) at line 29,
q) just before the call
q ⊢ p♦q. This together with
q](q.w′
q.wq), which,
q ⊢
q.wq.q), which implies p ∈
q.w′
q, no event in
q) just
q and p will end
q), which

q removes p from the sleep set. Since p was in sleep(E ′

q.wq). Hence, during exploration of E ′

q) and from there in ﬁnal_sleep(E ′

q.wq.q (which follows from E ′

q.q), we have p 6∈ w′

q](w′

q.w′

q.w′

means p ∈ ﬁnal_sleep(E). It then follows that p 6∈ wq, since
otherwise we would have that next[E](p) would not conﬂict
with any event preceding it in wq, hence also in w, contradicting
p ∈ WI[E](w), thereby violating Claim 1.

q](w′

q.wq which by p ∈ WI[E′

Therefore p 6∈ w′
q.wq) entails
E ′
q ⊢ p♦w′
q.wq. By choice of q, we then have necessarily that
ep = eq (otherwise wp would be longer than wq). But since
among the threads p with ep = eq we chose q to be the
ﬁrst one for which a call of the form Explore(E ′
q.p) was
performed, we have that p 6∈ sleep(E ′
q) just before the call to
q.q, Sleep′, ·), whence p 6∈ Sleep′. Thus, we have a
Explore(E ′
contradiction.

q.wq.z′ is
Claim 4. Let z′ be any sequence such that E ′
q.q.w′
maximal (such a z′ can always be found, since E ′
q.q.w′
q.wq
is an execution). Then, the algorithm explores some sequence
q.q.z in [E ′
E ′

q.wq.z′]≃.

q.q.w′

q.q](w′
q.wq.z′) ∩
Proof. From Claim 3, it follows that WI[E′
Sleep′ = /0. Therefore, no execution in [E ′
q.wq.z′]≃ was
q.q.w′
explored before the call to Explore(E ′
q.q), otherwise, there
would be a call Explore(E ′′.p) with E ′′ a preﬁx of E ′
q
and p ∈ Sleep′, and deﬁning w′′ by E ′′.w′′ = E ′
q, we would
have E ′′ ⊢ p♦w′′ and p ∈ WI[E′
q.wq.z′), thus contradicting
q.wq.z′) ∩ Sleep′ = /0. By the inductive hypothesis for
WI[E′
q.wq.z′, the algorithm then explores some
q.q applied to w′
E ′
sequence E ′

q.q](w′

q.q](w′

q.q.z in [E ′

q.wq.z′]≃.

q.q.w′

By the construction of wq, the event next[E′

q](q) conﬂicts

with eq. We have two cases

1) If next[E′

q](q) cannot disable eq, then, letting z′ be eq.z′′
q.wq.eq.z′′ eq. From
q](q) -E′
in Claim 4, we have next[E′
q.q.w′
q.wq.eq.z′′, it follows that the same race
q.q.w′
q.q.z ≃ E ′
E ′
between next[E](q) and eq will also occur in E ′
q.q.z, that is,
q.q.z
we have next[E′
is actually explored by the algorithm, it will encounter
q.q.z eq at line 3. When handling it,
the race next[E′
• E in the algorithm will correspond to E ′
q.q.z in this

q.q.z eq. Since the sequence E ′

q](q) -E′

q](q) -E′

proof,

• e in the algorithm will correspond to next[E′

q](q) in this

proof,

• e′ in the algorithm will correspond to eq in this proof,

and

• v = (notdep(next[E′
quence v at line 5.

q](q), E ′

q.q.z).

eq) will be the se-
b

2) If eq can be blocked by next[E′

q.w′

q.q.w′

q.q.w′

q.wq.w′′

q.wq (but not after E ′

q](q), then eq is possibly
blocked after E ′
q.wq). Let w′′
q
be a shortest sequence such that eq is enabled after
q.q.w′
E ′
q, if such a sequence exists. If eq is not
blocked after E ′
q will be empty. (The
case where no such w′′
q exists will be considered in the
next paragraph.) Then (cid:10)eq, pre(E, eq)(cid:11) is one of the tuples
he′, E ′i constructed at line 8. We note that next[E′
q](q) is
in E ′ and that there is no event in notdep(e, E) that con-
ﬂicts with all events that may enable or disable eq, whence
next[E′
q](q) is one possible choice for e at line 11. Let u

q.wq, then w′′

be a sequence constructed at line 18 with w′
q.wq .[Eq]
q](q), E) after which eq is enabled;
u .[Eq] notdep(next[E′
such a sequence must exist since w′
q.wq itself is a possible
choice. It now follows that u ≃[Eq] w′
q.wq, since otherwise
the sequence u.eq would be a sequence with Eq ⊢ q♦u,
which by construction is not explored after E such that
u is longer than w′
q.wq, thereby contradicting the choice
of w introduced just before Claim 2. The sequence v will
therefore have the same construction as in case 1.
In the case where no such w′′
be blocked after any maximal extension E ′
of E ′
q.q.w′
paragraph.

q exists, the event eq will
q.wq.w′′
q
q.wq. We can then proceed as in the preceding

q.q.w′

Claim 5. w′

q.wq.

eg .[E′
b

q] v.

Proof. Using the same argument as in case 2) in the preceding
paragraph, it can be established that u ≃[Eq] w′
q.wq, due to the
choice of w introduced just before Claim 2. The claim then
follows from the construction of v.

Let wR denote wq.

eq.
b
q) ∩ WI[E′

Claim 6. sleep(E ′

q](w′

q.wR) = /0.

Proof. Assume that some thread p is in WI[E′
consider two cases.

q](w′

q.wR). Let us

1) If p ∈ w′

q, then it has no event happening before it in w′
q,
which implies that it cannot have been in sleep(E ′
q) since
then it could not have been taken out of the sleep set to
q. Thus p 6∈ sleep(E ′
be executed in w′
q](w′

q, then by p ∈ WI[E′
q, which assuming p ∈ sleep(E ′
q.w′

2) If p 6∈ w′
E ′
q ⊢ p♦w′
will still be in the sleep set after E ′
p ∈ ﬁnal_sleep(E). Then:
a) If p ∈ wR, then p ∈ I[E](wR) from which we get p ∈
I[E](w) therefore p ∈ WI[E](w). Since p ∈ ﬁnal_sleep(E)
this contradicts Claim 1.
b) If p 6∈ wR, then from p ∈ WI[E′

q).
q.wR) we have that
q), means that p
q and therefore

q.wR) and p 6∈ w′
q.w′
q.wR, which implies E ′

q.wR
we have that E ′
q ⊢
p♦wR, which is equivalent to E ⊢ p♦wR. But then
wR = wq.
eq is a preﬁx of wp, implying that wp is strictly
b
longer than wq. This contradicts the fact that q was
chosen as the thread in ﬁnal_sleep(E) with the longest
preﬁx wq satisfying E ⊢ q♦wq.

q ⊢ p♦w′

q](w′

Therefore, there can be no such p ∈ WI[E′
is proven.

q](w′

q.wR) and Claim 6

q.u.r ≤ E (if still E ′

q.u < E). We know that E ′

From Claims 5 and 6 and Lemma 2, we get sleep(E ′

q) ∩
WI[E′
q](v) = /0. Thus, the test at line 21 will succeed, and the
sequence v will be inserted into the wakeup tree wut(E ′
q)
(line 21) by the function insert(v, E ′) at lines 34–44. We ﬁrst
claim that during the insertion, the sequence u will always
satisfy Eq.u ≤ E and v will satisfy u′.wq.
eq .[Eq.u] v, where
b
u.u′ = w′
q. This is trivially true initially. To see that it is
preserved by each round of the insertion starting at line 36, we
consider the possible children of form u.p. Let r be the thread
such that E ′
q.u.r is
in E when Explore(E) returns. Furthermore, for each branch
u.p with Eq.u.p ∝ Eq.u.r, we have that p 6∈ WI[E.u](u′.wq.
eq)
b
by the Inductive Hypothesis and the assumption that E.w has
not been explored. On the other hand, r ∈ WI[E.u](u′.wq.
eq),
b
implying that either u.r is already in wut(E ′
q) during the
insertion, in which case the loop will move to the next iteration
with invariants preserved, or u.r is not already in wut(E ′
q),
in which case it must be added during the current insertion
and produce a branch u.v such that u′.wq.
eq .[Eq.u] v. Thus,
b
when insert(v, E ′) returns, the exploration tree will contain an
execution of form E.v with wq.
eq .[E] v, thereby contradicting
b
the assumption that wq is the longest extension of E that has
been explored. This concludes the proof of the inductive step,
and Theorem 5 is proven.

Finally, we also prove that OPTIMAL-DPOR-AWAIT is
optimal in the sense that it never explores two different but
equivalent executions and never encounters sleep set blocking.
The following theorem establishes that sleep sets alone are
sufﬁcient to prevent exploration of two equivalent maximal
executions. It is essentially the same property that Optimal-
DPOR [3] guarantees, and originally appeared as Theorem 3.2
in the paper of Godefroid et al. [12].

Theorem 6. OPTIMAL-DPOR-AWAIT never explores two
maximal executions which are equivalent.

Proof. Assume that E1 and E2 are two equivalent maximal
execution sequences that are explored by the algorithm. Then
they are both in E . Assume, without loss of generality, that
E1 ∝ E2. Let E be their longest common preﬁx, and let
E1 = E.p.v1 and E2 = E.v2. By Lemma 3 and the deﬁnition of
I[E](v2), we have p 6∈ I[E](v2), which contradicts E1 ≃ E2 and
the maximality of E1 and E2.

