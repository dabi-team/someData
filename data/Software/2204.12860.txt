2
2
0
2

r
p
A
7
2

]

C
H
.
s
c
[

1
v
0
6
8
2
1
.
4
0
2
2
:
v
i
X
r
a

Visualization Psychology for Eye Tracking
Evaluation

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

Abstract Technical progress in hardware and software enables us to record gaze
data in everyday situations and over long time spans. Among a multitude of research
opportunities, this technology enables visualization researchers to catch a glimpse
behind performance measures and into the perceptual and cognitive processes of peo-
ple using visualization techniques. The majority of eye tracking studies performed for
visualization research is limited to the analysis of gaze distributions and aggregated
statistics, thus only covering a small portion of insights that can be derived from gaze
data. We argue that incorporating theories and methodology from psychology and
cognitive science will beneﬁt the design and evaluation of eye tracking experiments
for visualization. This book chapter provides an overview of how eye tracking can
be used in a variety of study designs. Further, we discuss the potential merits of
cognitive models for the evaluation of visualizations. We exemplify these concepts
on two scenarios, each focusing on a diﬀerent eye tracking study. Lastly, we identify
several call for actions.

Maurice Koch
University of Stuttgart, Germany, e-mail: Maurice.Koch@visus.uni-stuttgart.de

Kuno Kurzhals
University of Stuttgart, Germany, e-mail: Kuno.Kurzhals@visus.uni-stuttgart.de

Michael Burch
University of Applied Sciences Graubünden, Switzerland, e-mail: Michael.Burch@fhgr.ch

Daniel Weiskopf
University of Stuttgart, Germany, e-mail: Daniel.Weiskopf@visus.uni-stuttgart.de

1

 
 
 
 
 
 
2

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

Fig. 1 Visualization psychology for eye tracking experiments incorporates expertise from psy-
chology and cognitive science to improve the evaluation of visualization techniques by study
methodology, theory integration, and cognitive architectures.

1 Introduction

Eye tracking experiments in visualization research provide insights into how people
interpret and interact with visualizations. In contrast to classic performance anal-
ysis, the analysis of gaze behavior provides information about the distribution of
visual attention over time. Eye tracking further helps understand visual strategies
employed in interpreting a visualization or in working with a complex visual analyt-
ics system. In addition, machine learning, statistics, visualization research, and data
science in general contributed a multitude of new techniques [5, 11] to expand the
spatio-temporal analysis of eye tracking data, verify results, and formulate new hy-
potheses. By combining such state-of-the-art analysis techniques with expertise from
psychology, cognitive science, and eye tracking research, as depicted in Figure 1,
the design and insights gained from eye tracking experiments in visualization can
be signiﬁcantly improved. However, evaluation in visualization still lacks concrete
guidance on such interdisciplinary research. One part of the problem is the increasing
disconnect between psychology and visualization research. For example, in visual
analytics, there is less focus on individual visualizations but on the processes that the
tool is meant to support. Such processes often can be related to diﬀerent scenarios,
such as visual data analysis and reasoning and collaborative data analysis [30], to
name a few. Although visualization research has become more process-centered on
a conceptual level, evaluation today still mostly involves usability testing and bench-
marking based on completion time and error metrics. For this reason, we advocate
that the visualization community broadens their scope toward evaluation methodolo-
gies that better capture the dynamics of complex tool interactions. In a similar sense,

Visualization Psychology forEye Tracking Experimentsinteractive techniquesvisual analyticsuser-based evaluationgaze distributionsscanpathscognitive measuresperception theorystudy methodologycognitive architecturesexplainability of observationstheory-based hypothesesimproved study designVisualization Psychology for Eye Tracking Evaluation

3

we advocate that cognitive psychologists actively participates in that endeavour by
focusing their study on higher-level cognition. Fisher et al. [14] even call for transla-
tional research that bridges pure science and design, with the hope to better support
knowledge transfer between both ﬁelds. A major inspiration for this work has been
Kurzhals et al. [26], who advocated for more interdisciplinary research between the
ﬁelds of psychology, cognitive science and visualization. In this book chapter, we
exemplify how the eye tracking modality could be beneﬁcial to a broader scope of
empirical studies, beyond classical laboratory experiments.

2 Study Designs

In the following, we describe how diﬀerent study designs commonly found in visu-
alization evaluation [10] can beneﬁt from eye tracking methodology. Eye tracking
has become popular in the evaluation of visualizations and there is wide variety of
methods and metrics to evaluate the performance of visualization [15]. Kurzhals
et al. [27] reviewed 368 publications that include eye tracking in a user study and
identiﬁed three main approaches to evaluate visualizations: evaluating the distribu-
tion of visual attention, evaluating sequential characteristics of eye movements, and
comparing the viewing behavior of diﬀerent participant groups. Their review also
shows that user studies with eye tracking have become more common in recent years.
However, the use of eye tracking in evaluation methods has been narrow in the
sense that it is predominantly used in laboratory experiments but infrequently found
in in-the-wild studies. Laboratory experiments oﬀer great control and precise results,
but are primarily suited to study individual factors with predeﬁned hypotheses. In
this section, we outline the current practice of using eye tracking in visualization
research, mostly in the context of controlled experiments. Furthermore, we outline
how eye tracking could be beneﬁcial beyond laboratory experiments. For this, we
include a discussion of in-the-wild studies.

2.1 Controlled Experiments

Eye tracking has become increasingly popular in laboratory experiments. In visual-
ization research, controlled experiments have been mostly conducted for summative
evaluation, such as usability testing and benchmarking. However, such studies often
fail to relate their ﬁndings to the underlying cognitive processes.

Here, we showcase just a few selected eye tracking studies in visualization with a
strong focus on cognitive aspects, such as reasoning, memorability, and perception.
Huang et al. [20] studied how link crossings in graph drawings aﬀect task perfor-
mance. Participants were asked to ﬁnd the shortest path between two speciﬁed nodes
for each drawing. Their eye tracking experiment revealed that link crossings, con-
trary to the common belief, only have minor impact on graph reading performance,

4

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

especially at angles of nearly 90 degrees. Instead, the extra time spent on certain
drawings was due to the tendency of subjects to prefer certain paths at the beginning
of the search task. It was observed that subjects tend to follow links that are close to
an (imaginary) straight line between the target nodes. This can increase the search
time if no such links exist in the graph drawing, and alternative graph lines must be
considered. This behavioral bias during the initial search process in graph drawings
was termed geodesic-path tendency. Körner et al. [28, 29] found that this behavior
can be explained by studying to which extent search and reasoning processes in graph
comprehension are performed concurrently. The two main process involved in such a
task are ﬁrst detecting both speciﬁed nodes in the graph (search) and next ﬁnding the
shortest path between those two nodes (reasoning). Assuming that these processes
occur in parallel, subjects would not show this kind of bias toward certain links in
graph drawings as described by geodesic-path tendency. Körner et al. conducted
eye tracking experiments and found that these two graph comprehension processes
indeed are mostly performed sequentially. This means that subjects can only rely on
local information of the graph drawing to perform reasoning during the search task.
Borkin et al. [6] studied the memorability of visualizations and how well they
are recognized and recalled. Their experiments consists of three phases: encoding,
recognition, and recall. In the encoding phase, subjects were exposed to 100 diﬀerent
visualizations sampled from the MassVis dataset. After the encoding phase of 10
seconds per image, subjects were exposed to the same images plus unseen ﬁller
images as part of the recognition phase. In both phases, eye ﬁxations were collected
to examine the elements in visualizations that facilitate the memorability. In the last
phase, subjects were asked to describe correctly identiﬁed images as best as possible
to understand what elements were easily recalled from memory. The encoding and
recognition phases, eye ﬁxations were analyzed with heatmaps to ﬁnd out what parts
of the visualization draw initial attention to subjects during the encoding phase,
and what elements are required during recognition. During encoding, subjects tend
to perform visual exploration, and ﬁxations are distributed across the image. This
pattern can be observed on most images. Fixations during the recognition phase are
distinct between most recognizable images and least recognizable images. It was
shown that in the most recognizable visualizations, ﬁxations are more biased toward
the center of the image and are generally less widely distributed. This means that
relatively few ﬁxations are needed to recall easily recognizable images from memory,
whereas less recognizable images require more contextual information. Their study
also shows that participant descriptions are of higher quality for visualizations that
are easily recognizable even with a reduced amount of encoding time (such as
one second). Interestingly, prolonged exposure does not change the fact that some
visualizations stay more recognizable.

Hegarty et al. [18] studied how saliency of task-relevant and task-irrelevant infor-
mation on weather maps impacts task performance. Mean proportion of ﬁxation time
was measured to study the level of attention on task-relevant or task-irrelevant infor-
mation before instructions and after-instructions. On the one hand, it was reported
that ﬁxation time signiﬁcantly increases on task-relevant areas after instructions were
given, which shows that attention is strongly driven by top-down inﬂuences. On the

Visualization Psychology for Eye Tracking Evaluation

5

Fig. 2 Conceptual overview of data collection in in-the-wild studies. Diﬀerent modalities such
as camera views, keyboard and mouse logging and eye tracking can be combined to generate a
data-rich description of the study. The collected data can be transformed or extended semantically
with labels provided by human annotators.

other hand, visual salient regions do not draw attention to participants, unless they
correspond to task-relevant areas. These results emphasize that visual salience does
not necessarily facilitate task performance, unless participants are suﬃciently guided
by top-down processes toward task-relevant information.

The aforementioned visualization studies exemplify that eye tracking has become
an established modality to study cognitive processes. Furthermore, many of these
results are directly applicable to the visualization community

2.2 In-the-Wild Studies

As the complexity of visual artefacts increases, it becomes harder to provide holistic
assessments of the eﬀectiveness of complex visualization tools. Field studies oﬀer
more realism by assessing systems within their natural environment like at the
domain expert’s work place. In such settings, it is easier to study processes, like sense-
making, since they tend to be highly context-sensitive. Thus, such processes are more
diﬃcult to capture in controlled experiments that usually impose tight protocols [30].
Many researchers believe that visualization evaluations could beneﬁt from more
qualitative research, for example, by employing ethnographic techniques [38, 13].
In general, social science methods should receive more attention in the community
since individual assessment techniques often fail to capture contextual factors [31].
Ethnographic techniques have been advocated by Sheiderman et al. [38] in the
form of multi-dimensional in-depth long-term case studies (MILCs). MILCs are
performed in-ﬁeld, in a domain experts natural working environment, thus they
are unobtrusive and guarantee more realistic results. Data collected in MILCs is
mostly qualitative and consists of interviews, log books, user maintained diaries, and
usage statistics obtained from the visualization tool. Field studies are often based on

6

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

ethnographical participant observation methods, interviews, surveys, and automated
logging of user activity [38], i.e., they are predominantly qualitative research in
terms of data collection and analysis. Qualitative evaluation often involves thematic
analysis and manual coding, both are inherently subjective processes [10]. There
are multiple problems associated with a primarily quantitative data collection and
analysis approach. First, data collection and analysis are tedious processes that often
involve a lot of manual work. In terms of data analysis, software tools like computer-
assisted qualitative data analysis software (CAQDAS) [3] improve the eﬃciency of
thematic analyses and assist coding, but only to a limited extent. This problem gets
exacerbated in long-term studies where a large amount of diverse data is collected.
For this reason, many MILCs come only with a few interviews and observations, and
during the study, data collection is sparse, at most it consists of user interface logs
that are automatically recorded (in practice, even logging is very uncommon except
for Sheiderman’s MILC study [38]).

The usage of physiological sensors is in particular challenging in ethnographic
studies, where the property of unobtrusiveness must be obeyed (interference by study
coordinators needs to be kept minimal). This is hardly achievable with stand-alone
eye tracking devices and electroencephalogram (EEG), which are highly invasive
and lack mobility. Furthermore, such physiological sensors often require external
supervision and careful setup. This naturally restricts what and how data is collected
in ethnographic studies. However, in regard of eye tracking devices, we have seen
technological progress toward mobile devices that are less invasive and require almost
no external supervision. In this way, eye tracking could act as a quantitative modality
that does not interfere with ethnographic requirements like unobtrusiveness. Figure
2 illustrates the basic idea of collecting data from multiple sources and semantically
and/or algorithmically extending it in subsequent steps.

Whether a modality is considered invasive depends not only on the modality
itself, but also on the situational context. For example, think-aloud protocols can be
elicited either naturally, or they can be imposed externally on request (by a study
coordinator), which could negatively aﬀect reasoning processes [2]. Think-aloud
might also negatively interfere with the natural eye movement, for example, during
attending the screen. To compensate this issue retrospective think-aloud [27] of
screen recordings accompanied by eye tracking data was suggested [12]. In general,
it is important to detect these attention shifts, which also occur naturally without
external stimulation and revalidate the recorded eye movements. Transferring our
studies to virtual reality (VR) could provide non-invasive access to physiological
sensors that are readily available in VR headsets. This could go beyond eye tracking
and further include tracking head/body movements and interface interactions.

The previously discussed scope of in-the-wild studies is on individuals, but can
be easily extended to collaborative settings as well. In that regard, pair analytics [2]
provides an interesting approach to studying social and cognitive processes for the
evaluation of visual analytics tools. Pair analytics studies the interaction between
two human subjects, the subject matter expert and the visual analytics expert, and
visual analytics tools. The visual analytics expert and subject matter expert collab-
orate to solve a speciﬁc domain goal, but both have diﬀerent responsibilities and

Visualization Psychology for Eye Tracking Evaluation

7

roles in that process. The subject matter expert (driver role) is the domain expert that
has the contextual knowledge but not the expertise to use the visual analytics tools,
whereas the visual analytics experts (navigator role) lacks the domain knowledge
but the technical expertise to translate the verbal requests from the subject matter
expert to tool commands. The dialog between the subject matter expert and visual
analytics expert makes the mental models and cognitive processes explicit, thus cap-
tures important cues of the collaborative process. Compared to classical think-aloud
protocols, verbalization during collaborative processes occurs naturally. Aligning
the rich data from think-aloud protocols with eye-movements from the subject mat-
ter expert and visual analytics expert could be a good starting point for in-depth
analysis on social and cognitive processes. Kumar et al. [25] have proposed a similar
type of study, but in the context of pair programming. Data from eye tracking data
and other modalities, like recorded video, are time-synchronized. Having discussed
the merits of in-the-wild studies in the evaluation of visualizations, we also need to
the address the inherent diﬃculties of conducting those studies. As Shneiderman et
al. [38] already mentioned, it is necessary for researchers and participants to allocate
a considerable amount of time into such studies. For example, Valiati et al. [40] per-
formed multiple longitudinal case studies, each took about three to four months. This
complicates recruiting participants, in particular, when domain experts are needed. It
needs to emphasized that this requires an intense level of collaboration and devotion
from both the researchers and domain experts.

2.3 Bridging between Quantitative and Qualitative Research

The aforementioned study designs can be roughly classiﬁed as being either qualita-
tive or quantitative. Quantitative evaluation, often in laboratory experiments, follows
statistical frameworks to make precise inferences about predeﬁned hypotheses. Qual-
itative evaluation provides a richer understanding of the situation that is more holistic
than what quantitative evaluation can capture, but also less precise [10].

Study designs that encompass data collection, analysis, and inferences techniques
from both methodological paradigms, can potentially oﬀset their individual short-
comings. The commonly found dichotomy in quantitative and qualitative inquiry is
too narrow. This motivates the research ﬁeld of mixed methods, which uses methods
from both disciplines to provide a better understanding of the studied phenomena
[23]. One of the hallmarks of mixed methods is to achieve integration by bringing
qualitative and quantitative data together in one study [32]. This integration can
occur at diﬀerent levels such as integration at the study design level, methods, and
interpretation/reporting. An example of integration at study level is an explanatory
sequential design where the quantitative phase informs the follow-up qualitative
phase. For example, a controlled study design with eye tracking could be conducted
to quantitatively evaluate the performance on a visual search tasks with two dif-
ferent visual representations. A follow-up qualitative phase could be justiﬁed for
several reasons. For example, a group of participants could strongly deviate in per-

8

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

formance. The follow-up qualitative phase could try to identify the root of this cause
by performing a retrospect think-aloud protocol where the respective participants
comment on their played-back eye-movements. Think-aloud can also be performed
concurrently to eye tracking experiments, which would correspond to a convergent
mixed methods design.

Integration at the other two levels is more concerned with mixed data analysis and
it is considerably more challenging and less explored [32, 41]. Common strategies
of mixed-data analysis include: data transformation, typology development, extreme
case analysis, and data consolidation [8]. Data consolidation is one of the greatest
challenges of mixed-data analysis since it merges two data sets, which goes beyond
linking. The diﬀerence is that both data sources remain clearly identiﬁable after
data linking while consolidation leads to a genuine new piece of information. These
techniques are not necessarily distinct, for example data transformation could be an
important prepossessing step for data consolation. Data transformation encompass
two data conversion directions, either quantiﬁed data is transformed to qualitative
data (qualtizing) or vice versa (quantizing) [41]. A common way to perform quanti-
zation is by counting codes in an thematic analysis. In that way, quantitative methods
like inferential statistics can be applied indirectly to qualitative data. Qualtizing can
be seen as a semantic transformation of the original quantitative data. This could add
a semantic link to quantitative measurements, which is usually not present in such
measurements beforehand. For example, gaze data in its raw form is just a trajectory
in 2D space without any semantic link to the underlying stimulus. For static stimuli,
this semantic link is easy to provide since there is a one-to-one correspondence
between gaze location and stimuli location. However, such a direct correspondence
it not present in dynamic stimuli where the underlying scene is varying over time.
Providing additional semantics to gaze data with underlying dynamic stimuli, for ex-
ample, by labeling time spans according to the participant’s activity, would increase
the usefulness of these measurements. This form of data consolidation by annotation
of quantitative data can improve the credibility of those measurements and thereby
improve the quality of subsequent mixed data analysis steps.

3 Explainability of Observations

As already outlined in the previous section, building semantic links between gaze
data and contextual factors, like scene information or activity labels, can aid the data
analysis and thereby the explainability of observations.

Areas of Interest

Scanpaths can be transformed to qualitative data by mapping each ﬁxation to a label,
which uniquely identiﬁes an area of interests (AOIs). The usefulness of such a repre-
sentation depends on the semantics of AOIs. For example, AOI grids automatically

Visualization Psychology for Eye Tracking Evaluation

9

generated for static stimuli do not provide much semantic details since an AOI hit
is just still just an indicator of spatial position (spatial quantization), but does not
provide semantic information w.r.t the underlying visual entity. A similar problem
occurs for AOIs induced by automatic clustering of gaze data, where regions with
strong accumulation of gaze positions are deﬁned as AOIs. In contrast to such au-
tomatically generated AOIs, manually AOIs deﬁned based on semantics (images on
web pages; axes on graphs; etc.) can provide more detailed information.

Interpretation and Data Analysis

In Section 2, we have mentioned the challenges in data collection and analysis
in the context mixed-methods research. These kind of challenges are particularly
relevant for in-the-wild studies, such as the previously described long-term ﬁeld
studies in pair analytics. It is challenging to integrate data from heterogeneous data
sources, such as eye tracking and other physiological sensors, as well as hand-written
or verbal protocols. An interesting approach toward these problems is visual data
analysis, sometimes referred to as visualization for visualization (Vis4Vis) [42]. The
vision behind Vis4Vis is to use visualizations to analyze and communicate data from
empirical studies. In the context of eye tracking studies, visual analysis tools have
shown to support the evaluation of studies. For example, Blascheck et al. [4] provide
a comprehensive overview of visualization techniques for eye tracking data. Some
visual analysis approaches have been proposed that integrate eye tracking data with
other data modalities, such as think aloud protocols and interaction logs. Blascheck
et al. [3] proposed a visual analytics system that allow interactive coding and visual
analysis of user activities. Such approaches could be considered as a ﬁrst step
toward visual analysis of data-rich empirical studies with multiple data modalities.
Nonetheless, there is still the need for more scalable visual representations and
automatic analysis techniques to better support the analysis of data from long-term
empirical studies.

4 Cognitive Architectures

One of the overarching goals of empirical studies in visualization is to formulate
guidelines and heuristics that inform the design of future visualizations. However,
many psychological phenomena only apply to speciﬁc aspects of the evaluation,
like Gestalt Laws, but visualization consists of multiple perceptual and cognitive
aspects combined. Thus, guidelines and heuristics on system level would be prefer-
able. However, since they typically involve higher-level cognitive tasks, they are
more inﬂuenced by individual factors, such as knowledge, cultural background, and
cognitive capabilities. Computational models have the potential to generalize across
a wide range of individuals [27] and can provide methods to accurately predict the
eﬀectiveness of visual designs [19]. As shown in Figure 3, such simulation could

10

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

Fig. 3 Cognitive simulation can be performed on multiple levels. Each layer corresponds to one
class of tasks. Each level depends on its lower levels. For example, simulation of collaborative
settings with multiple individuals performing a common task requires successful simulation of
cognitive tasks (level 1 and 2) for individuals.

be performed on multiple levels. On the most fundamental level one, simulation
of human cognition boils down to perceptual simulation that is often highly driven
by the stimulus or more general bottom-up inﬂuences. Early work on that level has
been proposed by Itti and Koch in the context of visual saliency prediction [22].
In general, cognitive simulation on higher levels has been less explored, mostly
due to its complexity and the lack of formal descriptions. Nonetheless, computa-
tional models based on cognitive architectures have been proposed to automate the
evaluation of visualizations on the level of reasoning and decision making. One
example of the application of cognitive architectures like ACT-R [1] is CogTool
(see https://www.cogtool.org), which is deployed for the initial validation of web
designs. Eye ﬁxations can play an important role as a means to train and validate
cognitive models. For example, Raschke et al. [36] propose a cognitive model based
on ACT-R that simulates visual search strategies. Their motivation is to build a
simulation tool similar to CogTool that allows automatic, thus non-empirical, evalu-
ation of visualizations. In contrast to CogTool, that is based on an extended version
of Keystroke-Level-Model [9], their model is trained on eye ﬁxations. Although
their work does not provide any concrete implementation, other researchers have
demonstrated that models based on ACT-R can simulate eye movements on simple
line charts with high conﬁdence [35]. Their model even provides vocal output, thus,
is able to simulate graph comprehension with results close to human level. From
a technical viewpoint, cognitive architectures like ACT-R have some limitations
that prevent their adoption to more complex tasks. For example, Heine et al. [19]
advocate the use of probabilistic models, like Dynamic Bayesian networks, in the
context of modeling human cognition. Probabilistic models could provide a uniﬁed
mathematical model toward human cognition and allows to describe variation of
factors that are not explicitly modeled. This is a strong advantage over ACT-R that
depends on explicit rule-based modeling, which does not scale well for sophisticated
visualizations.

SaliencyPredictionStimulus-drivenMixed-initativeCollaborativeDecisionMakingReasoningData ExplorationLevel1Level2Level3Visualization Psychology for Eye Tracking Evaluation

11

5 Example Scenarios

Visualization evaluation could beneﬁt from the aforementioned study designs, the
explainability of observations, and cognitive architectures. We exemplify this, based
on two previous eye tracking studies. One on the design of metro maps [33] and one
on the evaluation of parallel coordinates plots [34]. We discuss how these studies
could be enhanced and extended by adopting ideas from the previous sections of this
chapter.

5.1 Overview of Scenarios

Scenario 1: Metro Maps

Investigating the readability of metro maps is a challenging ﬁeld of research, but the
gained insights are valuable information on how to ﬁnd design ﬂaws, enhance the
design, and make the maps more understandable to travelers [7]. Netzel et al. [33]
compare color-coded and gray-scale public transport maps with an eye tracking study.
The major outcome is that color is an important ingredient to reduce the cognitive
burden to follow lines. Eye tracking was essential in this study to understand the
strategies participants applied to solve a route ﬁnding task between a start and a
target station (Figure 4). The analysis showed that color maps led to much longer
saccades, and it was hypothesize that colored lines made participants feel safe and,
hence, the route ﬁnding tasks could be answered faster and more reliably. In contrast,
in gray-scale maps, the participants’ eyes moved with signiﬁcantly smaller saccades
to trace a line reliably, which was due to missing color that would otherwise have
helped to visually and perceptually separate the metro lines from each other. A
practical result of this eye tracking experiment for the professional map designer is
that color is crucial for route ﬁnding tasks, hence the much cheaper printed variants
in gray-scale would obviously be counter-productive for the business, although the
costs are much lower.

Scenario 2: Scatter and Parallel Coordinates Plots

The second example of a study investigates the assessment of relative distances
between multi-dimensional data points with scatterplots and parallel coordinates
plots [34] (Figure 5). The authors performed an eye tracking study and showed
that scatterplots are eﬃcient for the interpretation of distances in two dimensions,
but participants performed signiﬁcantly better with parallel coordinates when the
number of dimensions was increased up to eight. With the inclusion of eye tracking,
it was possible to identify diﬀerences in the viewing of the two visualization types
considering ﬁxation durations and saccade lengths. The authors further introduced a
visual scanning model to describe diﬀerent strategies for solving the task. With the

12

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

Fig. 4 Scenario 1: Metro maps in color (left) and in gray scale (right) have been compared for
solving a way ﬁnding task from a start (hand) to a target location. Eye tracking was measured to
identify diﬀerences in the reading behavior of both conditions. Figure reprinted by permission of
Taylor & Francis Ltd from Netzel et al. [33].

Fig. 5 Scenario 2: Eye tracking was applied to compare how people compare distances between
three points (A, B, C) in scatterplots (top) and parallel coordinates plots (bottom). Figure reprinted
by permission of Elsevier from Netzel et al. [34]. Licensed under the CC BY-NC-ND license.

help of eye tracking, a bias toward the center (parallel coordinates plot) and the left
side (scatterplots) of the visualizations could also be measured, which is important
for the design of such plots considering where participants will potentially spend
most of their attention. However, understanding clear visual attention patterns like
following a line as described in the former eye tracking study is not possible here
since either the diagram consists of crowds of points (scatterplot) or a lot of crossing
and partially occluding polylines (parallel coordinates plot). Hence, the reading
behavior is more complex and harder to model than in Scenario 1.

Visualization Psychology for Eye Tracking Evaluation

13

5.2 Potential Extensions

In-the-Wild Studies

As described in Section 2.2, studies in the wild provide a higher realism for experi-
mental outcomes. For Scenario 1, this is highly desirable because the interpretation
of metro maps is a task performed by many people in everyday situations. For the
sake of controlability, stimuli and task were adjusted to ﬁt to a laboratory setting:
People were watching metro maps on a screen with start and goal clearly highlighted.
The situation in a real metro station would diﬀer signiﬁcantly. Numerous confound-
ing factors such as distractions by other people, no clear identiﬁcation of start and
goal, as well as other potential stress inducing factors might inﬂuence the results
how people look at such a map.

Scenario 2, in contrast, involves visualization techniques (i.e., parallel coordinates
plots) that are less known to people. An application in the wild would presumably
take place with domain experts and data scientists rather than a more general audience
of students, as it was the case in the conducted study. Further, the set of performed
tasks would be extended in comparison to the lab study. However, for the hypotheses
of the original experiment, the expertise of the participants was not the determining
factor, since the study aimed to analyze general behavior. For measurements over
longer time periods, the experts could potentially show additional behavior patterns
and learning eﬀects, while general behavior aspects should not change.

Collaborative Studies and Pair Analytics

The investigation of metro maps in Scenario 1 is often an individual task, but is in real
life also performed collaboratively. Similar to the application of the task in the wild,
the analysis of collaborative task solving has the potential to reveal details on how
decision making is performed. Scenario 2 can be imagined for typical analysis tasks
involving domain and visualization experts. In both scenarios, the dialog between
participating people provides valuable information on a qualitative level. Scenario 1
provides the possibility to perform a symmetrical setup where both persons have the
same prerequisites and solve the task together. In Scenario 2, the integration of the
visualizations in a visual analytics framework has the potential to focus more on a
pair analytics approach where people with diﬀerent ﬁelds of expertise (i.e., domain
and visualization expert) work together to solve the task.

Further, measuring the gaze behavior of both persons indicates periods when they
potentially share visual attention, and when they might be confused, e.g., searching
for the region the other person is talking about. Hence, eye tracking helps evaluating
the visualization at hand, but also the interaction between persons.

14

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

Mixed Methods

Qualitative and quantitative evaluation combined provide a more comprehensive
understanding of the research topic than each method on its own. Scenario 1 and 2
mainly focused on the quantitative evaluation of traditional performance measures
and established eye tracking metrics. However, with respect to the analysis of visual
strategies, both studies included visual analysis for the qualitative assessment of
recorded scanpaths. We argue that such observations will become more important
for experiments whenever eye tracking is involved. Furthermore, additional data
(e.g., think aloud, interaction logs) will be necessary to include in a data integration
step to provide a new, more thorough view on the participant’s behavior.

Cognitive Models

Cognitive models to predict the scanpath of a participant and the eﬃciency of
wayﬁnding tasks would be beneﬁcial for the design of metro maps in Scenario 1.
Although diﬀerent strategies for solving the task could be identiﬁed, a generalized
model was not included in the results of the study. The study was one of the ﬁrst in this
domain where it was important to identify general strategies. For a comprehensive
model, additional data for diﬀerent levels of expertise might be necessary. Here, map
designers and map readers are two diﬀerent target groups that potentially focus on
diﬀerent aspects of the map and viewing tasks might diﬀer signiﬁcantly between such
groups. An implicit model of strategies was applied for the manual annotation of
paths, imprecise measures of line tracing. Future models could also consider psycho-
physical measures, for example, just noticeable diﬀerences to be able to separate
close-by metro lines. In the wild, saliency models will also play an important role
for the orientation while searching for start and goal locations.

The design of the study in Scenario 2 was based on some assumptions made
from theory and observations in pilot experiments. Netzel et al. provided a hand-
crafted model (Figure 6) on the diﬀerent strategies during the reading process of the
visualization. This model was guided by the hypotheses of the study. In future re-
search, such models could be generated more systematically, informed by theoretical
perceptual or cognitive models from psychology.

6 Call for Actions

Based on our previous observations, we have identiﬁed the following interesting
points for future development and calls for actions.

Visualization Psychology for Eye Tracking Evaluation

15

Fig. 6 Strategy model for the visual comparison of multidimensional data points with parallel
coordinate plots. Netzel et al. [34] identiﬁed two strategies, i.e., axis-based and interior area
comparison, and comprised them in a hand-crafted behavior model. Figure reprinted by permission
of Elsevier from Netzel et al. [34]. Licensed under the CC BY-NC-ND license.

Translational research

Many early guidelines in visualization were informed by perceptual and cognitive
science, like eﬃcient visual encoding, Gestalt laws [24, 43], or feature integration
theory [16, 39]. However, there is lack of guidelines that inform design decisions for
visual analytics systems [37], since current cognitive models are good at explaining
cognitive processes on well-deﬁned tasks and simple visual stimuli, but are less
applicable to the aforementioned scenarios that have become prevalent in today’s
systems [17]. This line of research oﬀers great potential for translational studies
since psychology and visualization research would equally beneﬁt from such results.
Distributed cognition could be a promising approach toward translational studies of
that kind since it provides a more holistic view of the way humans reason and think.
It acknowledges the fact that humans live in materialistic and social environments,
thus, it emphasizes the importance of contextual factors in human cognition [21].

Best Practices

This book chapter only provided a high-level conceptual view on evaluation strate-
gies. So far, our envisioned evaluation strategies have not yet been implemented
in real world empirical studies. Many challenges are left unanswered, as how to
practically design, conduct, and valuate data-rich empirical studies. It is particularly
important to provide researchers a tool set to perform sophisticated data analysis
with minimal eﬀort. There is also need for the whole community of researchers to
agree upon a proper way to report results of such studies.

16

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

Interdisciplinary Research Venues

Psychologists’ core topics are often disconnected from topics relevant for visualiza-
tion research. Yet, there are some successful examples of combining communities,
for example, at the Symposium on Eye Tracking Research and Applications (ETRA).
Such events provide great opportunities for interdisciplinary discourse and estab-
lishing collaborations. However, publication strategies and research topics might
signiﬁcantly diﬀer between communities. Hence, a fusion of expertise just by project
collaborations might cover some research questions, but from a long-term perspec-
tive, other solutions are necessary. A key question, of course, is: How can we integrate
the expertise from both research ﬁelds in a common research endeavor? We think
that activities such as this workshop or our own experience with the ETVIS work-
shop1 and joint research centers (like SFB-TRR 1612) are a good way to go, but are
alone not suﬃcient and need further action. Building a research area of visualization
psychology could be a viable means, for example, by establishing publication and
other presentation opportunities that work for visualization researchers, psycholo-
gists and social scientists alike, by setting up a canon of teaching new students, and
by lobbying for funding possibilities for such interdisciplinary work.

Psychology Education

Although many design principles are based on perceptual and cognitive theories,
in-depth psychological background knowledge is often not part of the education
for visualization. Researchers starting with eye tracking studies are confronted with
learning eye tracking methodology, which is, starting with proper calibration to a
comprehensive analysis of the data, a complex ﬁeld on its own. As a consequence,
deeper knowledge of a whole new research ﬁeld, i.e., psychology, is hard to achieve
within the short time span of an average PhD student’s career.

References

1. J. R. Anderson, M. Matessa, and C. Lebiere. ACT-R: A theory of higher level cognition and

its relation to visual attention. Human–Computer Interaction, 12(4):439–462, 1997.

2. R. Arias-Hernandez, L. T. Kaastra, T. M. Green, and B. Fisher. Pair analytics: Capturing
In 2011 44th Hawaii International

reasoning processes in collaborative visual analytics.
Conference on System Sciences, pages 1–10, 2011.

3. T. Blascheck, F. Beck, S. Baltes, T. Ertl, and D. Weiskopf. Visual analysis and coding of
data-rich user behavior. In 2016 IEEE Conference on Visual Analytics Science and Technology
(VAST), pages 141–150, 2016.

4. T. Blascheck, K. Kurzhals, M. Raschke, M. Burch, D. Weiskopf, and T. Ertl. State-of-the-art

of visualization for eye tracking data. In EuroVis (STARs), 2014.

1 https://www.etvis.org
2 https://www.sfbtrr161.de/

Visualization Psychology for Eye Tracking Evaluation

17

5. T. Blascheck, K. Kurzhals, M. Raschke, M. Burch, D. Weiskopf, and T. Ertl. Visualization of
eye tracking data: A taxonomy and survey. Comput. Graph. Forum, 36(8):260–284, 2017.
6. M. A. Borkin, Z. Bylinskii, N. W. Kim, C. M. Bainbridge, C. S. Yeh, D. Borkin, H. Pﬁster,
and A. Oliva. Beyond memorability: Visualization recognition and recall. IEEE Transactions
on Visualization and Computer Graphics, 22(1):519–528, 2016.

7. M. Burch, R. Woods, R. Netzel, and D. Weiskopf. The challenges of designing metro maps. In
N. Magnenat-Thalmann, P. Richard, L. Linsen, A. C. Telea, S. Battiato, F. H. Imai, and J. Braz,
editors, Proceedings of the 11th Joint Conference on Computer Vision, Imaging and Computer
Graphics Theory and Applications (VISIGRAPP), pages 197–204. SciTePress, 2016.

8. V. J. Caracelli and J. C. Greene. Data analysis strategies for mixed-method evaluation designs.

Educational Evaluation and Policy Analysis, 15(2):195–207, 1993.

9. S. K. Card, T. P. Moran, and A. Newell. The keystroke-level model for user performance time

with interactive systems. Communications of the ACM, 23(7):396–410, 1980.

10. S. Carpendale. Evaluating information visualizations. In A. Kerren, J. T. Stasko, J. Fekete,
and C. North, editors, Information Visualization - Human-Centered Issues and Perspectives,
volume 4950 of Lecture Notes in Computer Science, pages 19–45. Springer, 2008.

11. A. T. Duchowski. Eye Tracking Methodology - Theory and Practice, Third Edition. Springer,

2017.

12. S. Elling, L. Lentz, and M. de Jong. Retrospective think-aloud method: Using eye movements
as an extra cue for participants’ verbalizations. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, CHI ’11, page 1161–1170, New York, NY, USA, 2011.
Association for Computing Machinery.

13. G. Ellis and A. Dix. An explorative analysis of user evaluation studies in information visualisa-
tion. In Proceedings of the 2006 AVI Workshop on BEyond Time and Errors: Novel Evaluation
Methods for Information Visualization, BELIV ’06, page 1–7, New York, NY, USA, 2006.
Association for Computing Machinery.

14. B. Fisher, T. M. Green, and R. Arias-Hernández. Visual analytics as a translational cognitive

science. Topics in Cognitive Science, 3(3):609–625, 2011.

15. J. H. Goldberg and J. I. Helfman. Eye tracking for visualization evaluation: Reading values on

linear versus radial graphs. Information Visualization, 10(3):182–195, 2011.

16. C. G. Healey and J. T. Enns. Attention and visual memory in visualization and computer
IEEE Transactions on Visualization and Computer Graphics, 18(7):1170–1188,

graphics.
2012.

17. M. Hegarty. The cognitive science of visual-spatial displays: Implications for design. Topics

in Cognitive Science, 3(3):446–474, 2011.

18. M. Hegarty, M. S. Canham, and S. I. Fabrikant. Thinking about the weather: How display
salience and knowledge aﬀect performance in a graphic inference task. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 36(1):37, 2010.

19. C. Heine. Towards modeling visualization processes as dynamic Bayesian networks. IEEE

Transactions on Visualization and Computer Graphics, 27(2):1000–1010, 2021.

20. W. Huang, P. Eades, and Seok-Hee Hong. A graph reading behavior: Geodesic-path tendency.

In 2009 IEEE Paciﬁc Visualization Symposium, pages 137–144, 2009.

21. E. Hutchins. Distributed cognition. International Encyclopedia of the Social and Behavioral

Sciences. Elsevier Science, 138, 2000.

22. L. Itti and C. Koch. Computational modelling of visual attention. Nature Reviews Neuroscience,

2(3):194–203, 2001.

23. R. B. Johnson, A. J. Onwuegbuzie, and L. A. Turner. Toward a deﬁnition of mixed methods

research. Journal of Mixed Methods Research, 1(2):112–133, 2007.
24. K. Koﬀka. Principles of Gestalt Psychology, volume 44. Routledge, 2013.
25. A. Kumar, D. Mohanty, K. Kurzhals, F. Beck, D. Weiskopf, and K. Mueller. Demo of the
EyeSAC system for visual synchronization, cleaning, and annotation of eye movement data. In
ACM Symposium on Eye Tracking Research and Applications, ETRA ’20 Adjunct, New York,
NY, USA, 2020. Association for Computing Machinery.

26. K. Kurzhals, M. Burch, and D. Weiskopf. What we see and what we get from visualization:

Eye tracking beyond gaze distributions and scanpaths, 2020.

18

Maurice Koch, Kuno Kurzhals, Michael Burch, and Daniel Weiskopf

27. K. Kurzhals, B. Fisher, M. Burch, and D. Weiskopf. Eye tracking evaluation of visual analytics.

Information Visualization, 15(4):340–358, 2016.

28. C. Körner. Eye movements reveal distinct search and reasoning processes in comprehension

of complex graphs. Applied Cognitive Psychology, 25(6):893–905, 2011.

29. C. Körner, M. Höﬂer, B. Tröbinger, and I. D. Gilchrist. Eye movements indicate the temporal
organisation of information processing in graph comprehension. Applied Cognitive Psychology,
28(3):360–373, 2014.

30. H. Lam, E. Bertini, P. Isenberg, C. Plaisant, and S. Carpendale. Empirical studies in information
visualization: Seven scenarios. IEEE Transactions on Visualization and Computer Graphics,
18(9):1520–1536, 2012.

31. L. A. McNamara and W. A. Stubbleﬁeld. Workshop proposal: Visualization and the context of
work-qualitative research methods for design deployment evaluation. Technical report, Sandia
National Lab.(SNL-NM), Albuquerque, NM (United States), 2011.

32. E. Moseholm and M. D. Fetters. Conceptual models to guide integration during analysis in
convergent mixed methods studies. Methodological Innovations, 10(2):2059799117703118,
2017.

33. R. Netzel, B. Ohlhausen, K. Kurzhals, R. Woods, M. Burch, and D. Weiskopf. User performance
and reading strategies for metro maps: An eye tracking study. Spatial Cognition & Computation,
17(1-2):39–64, 2017.

34. R. Netzel, J. Vuong, U. Engelke, S. I. O’Donoghue, D. Weiskopf, and J. Heinrich. Comparative
eye-tracking evaluation of scatterplots and parallel coordinates. Visual Informatics, 1(2):118–
131, 2017.

35. D. Peebles. A cognitive architecture-based model of graph comprehension. In 11th Interna-

tional Conference on Cognitive Modeling, Berlin, pages 37–42, 2012.

36. M. Raschke, T. Blascheck, and T. Ertl. Cognitive ergonomics in visualization. In A. Ebert,
G. C. van der Veer, G. Domik, N. D. Gershon, and I. Scheler, editors, Building Bridges: HCI,
Visualization, and Non-formal Modeling, pages 80–94, Berlin, Heidelberg, 2014. Springer.
37. J. Scholtz, C. Plaisant, M. Whiting, and G. Grinstein. Evaluation of visual analytics en-
vironments: The road to the visual analytics science and technology challenge evaluation
methodology. Information Visualization, 13(4):326–335, 2014.

38. B. Shneiderman and C. Plaisant. Strategies for evaluating information visualization tools:
Multi-dimensional in-depth long-term case studies. In Proceedings of the 2006 AVI Workshop
on BEyond Time and Errors: Novel Evaluation Methods for Information Visualization, BELIV
’06, page 1–7, New York, NY, USA, 2006. Association for Computing Machinery.

39. A. Treisman. Preattentive processing in vision. Computer Vision, Graphics, and Image

Processing, 31(2):156–177, 1985.

40. E. R. A. Valiati, C. M. D. S. Freitas, and M. S. Pimenta. Using multi-dimensional in-depth long-
term case studies for information visualization evaluation. In Proceedings of the 2008 Workshop
on BEyond Time and Errors: Novel EvaLuation Methods for Information Visualization, BELIV
’08, New York, NY, USA, 2008. Association for Computing Machinery.

41. S. Vogl. Integrating and consolidating data in mixed methods data analysis: Examples from
focus group data with children. Journal of Mixed Methods Research, 13(4):536–554, 2019.
42. D. Weiskopf. Vis4Vis: Visualization for (Empirical) Visualization Research, pages 209–224.

Springer International Publishing, Cham, 2020.

43. M. Wertheimer. Laws of organization in perceptual forms. A source book of Gestalt Psychology,

1, 1923.

