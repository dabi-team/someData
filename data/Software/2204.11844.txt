From Monolith to Microservices
Static and Dynamic Analysis Comparison

Bernardo Andrade, Samuel Santos and Ant´onio Rito Silva

2
2
0
2

r
p
A
2
2

]
E
S
.
s
c
[

1
v
4
4
8
1
1
.
4
0
2
2
:
v
i
X
r
a

Abstract—One of the most challenging problems in the migra-
tion of a monolith to a microservices architecture is the iden-
tiﬁcation of the microservices boundaries. Several approaches
have been recently proposed for the automatic identiﬁcation
of microservices, which, even though following the same basic
steps, diverge on how data of the monolith system is collected
and analysed. In this paper, we compare the decompositions
generated for two monolith systems into a set of candidate
microservices, when static and dynamic analysis data collection
techniques are used. The decompositions are generated using a
combination of similarity measures and are evaluated according
to a complexity metric to answer the following research question:
which collection of monolith data, static or dynamic analysis,
allows to generate better decompositions? As result of the analysis
we conclude that neither of the analysis techniques, static nor
dynamic, outperforms the other, but the dynamic collection of
data requires more effort.

Index Terms—Microservices, Software Evolution, Static Anal-

ysis, Dynamic Analysis, Software Architecture.

I. INTRODUCTION

Microservices [1] have become main stream in the devel-
opment of large scale and complex systems when companies,
like Amazon and Netﬂix [2], faced constraints on their systems
evolution, due to the coupling resulting from the use of a large
domain model maintained in a shared database. However, the
adoption of this architectural style is not free of problems [3],
where the identiﬁcation of microservices boundaries is one of
the most challenging, because a wrong cut results on the need
to refactor between distributed services, which impacts on the
services interfaces, and cannot have the support of integrated
development environments.

The microservices boundaries identiﬁcation has been ad-
dressed by research, e.g. [4]–[8], in the context of the mi-
gration of monolith systems to a microservices architecture.
Some approaches take advantage of the monolith’s codebase
and runtime behavior to collect data, analyse it, and propose
a decomposition of the monolith. Although each of the ap-
proaches use different techniques, they follow the same basic
steps: (1) Collection: collect data from the monolith system;
(2) Decomposition: deﬁne a decomposition by applying a sim-
ilarity measure and an aggregation algorithm, like a clustering
algorithm, to the data collected in the ﬁrst step; (3) Analysis:
evaluate the quality of the generated decomposition using a
set of metrics.

However, the approaches differ on the techniques applied
in each one of the steps. In terms of the collection of data,

D. Faustino, N. Gonc¸alves and A. Rito Silva are with DPSS - INESC-ID,
Instituto Superior T´ecnico, University of Lisbon, Lisbon, Portugal. E-mail:
{bernardo.andrade, samuel.c.santos, rito.silva}@tecnico.ulisboa.pt

they differ in whether it is collected from the monolith using
static analysis of the code [7], or if they observe the monolith
execution behavior [8].

In this paper we analyse two monolith systems to study
whether these techniques provide signiﬁcant differences when
identifying candidate decompositions. The analysis framework
is built on top of what is considered, by the gray literature,
as one of the main difﬁculties on the identiﬁcation of mi-
croservices boundaries in monolith systems: the transactional
contexts [9, Chapter 5]. Transactional contexts generate a
coupling between domain entities accessed in the context of
the same transaction, due to the complexity of decomposing
a transactional behavior into several distributed transactions,
problem known as the forgetting of the CAP theorem [10].
Therefore, the decomposition to a microservices architecture
should minimize the number of distributed transactions imple-
menting a functionality, i.e., minimize the cost of redesigning
the functionality in the microservices architecture.

Considering this analysis framework, we address the follow-
ing research question: which collection of monolith behavior
data, static or dynamic analysis, allows to generate better
decompositions?

In this section we deﬁned the context of our work. The
next section formalizes our analysis framework. Section III
describes the overall process of automatic identiﬁcation of
candidate microservices and the use of the static and dynamic
data collection techniques in particular. In the evaluation,
Section IV, the analysis framework is applied to 2 systems
in order to answer the research question. Section V presents
related work and Section VI discusses the outcomes of this
work. Finally, Section VII presents the conclusions.

II. SIMILARITY MEASURES AND COMPLEXITY METRIC

A monolith is deﬁned by its set of functionalities which
execute in atomic transactional contexts and, due to the mi-
gration to the microservices architecture, have to be decoupled
into a set of distributed transactions, each one executing in the
context of a microservice.

Therefore, a monolith is deﬁned as a triple (F, E, G), where
F deﬁnes its set of functionalities, E the set of domain entities,
and G a set of call graphs, one for each monolith functionality.
A call graph is deﬁned as a tuple (A, P ), where A = E ×
M is a set of read and write of accesses to domain entities
(M = {r, w}), and P = A × A a precedence relation between
elements of A such that each access has zero or one immediate
predecessors, ∀a∈A#{(a1, a2) ∈ P : a1 = a} ≤ 1, and there
are no circularities, ∀(a1,a2)∈PT (a2, a1) /∈ PT , where PT is

 
 
 
 
 
 
the transitive closure of P . The precedence relation represents
the sequences of accesses associated with a functionality.

A. Similarity Measures

The deﬁnition of similarity measures establishes the dis-
tance between domain entities. Domain entities that are closer,
according to a particular similarity measure, should be in
the same microservice. Therefore, since we are interested in
reducing the number of distributed transactions a functionality
is decomposed in, we intend to deﬁne as close the domain
entities that are accessed by the same functionalities.

The access similarity measure measures the distance be-

tween two domain entities, e1, e2 ∈ E, as:

smaccess(e1, e2) =

#(f unct(e1) ∩ f unct(e2))
#f unct(e1)

where f unct(e) denotes the set of functionalities in the
monolith whose call graph has a read or write access to e.
This measure takes a value in the interval 0..1. When all the
functionalities that access e1 also access e2 then it takes the
value 1.

Since the cost of reading and writing is different in the
context of distributed transactions, because writes introduce
new intermediate states in the decomposition of a functionality,
the next two similarity measures distinguish read from write
accesses in order to reduce the number of write distributed
transactions:

smread(e1, e2) =

smwrite(e1, e2) =

#(f unct(e1, r) ∩ f unct(e2, r))
#f unct(e1, r)

#(f unct(e1, w) ∩ f unct(e2, w))
#f unct(e1, w)

where f unct(e, m) denotes the set of functionalities in the
monolith whose call graph has an access according to mode
m, read or write, respectively. These two measures tend to
include in the same microservice, domain entities that are read
or written together, respectively.

Finally, another similarity measure that is found in the
literature groups domain entities that are frequently accessed
in sequence, in order to reduce the number of remote invo-
cations between microservices, i.e., the domain entities that
are frequently accessed in sequence should be in the same
microservice. Therefore, the sequence similarity measure is
deﬁned:

smsequence(e1, e2) =

sumP airs(e1, e2)
maxP airs

where sumP airs(e1, e2) = Pf ∈F #{(ai, aj) ∈ Gf .P :
(ai.e = e1 ∧ aj.e = e2) ∨ (ai.e = e2 ∧ aj.e = e1)}),
where Gf .P is the precedence relation for functionality f ,
is the number of consecutive accesses of e1 and e2, and
maxP airs = maxei,ej ∈E(sumP airs(ei, ej)) is the max
number of consecutive accesses for two domain entities in
the monolith.

B. Complexity Metric

A decomposition of a monolith is a partition of its domain
entities set, where each element is included in exactly one
subset, a cluster, and a partition of the call graph of each one
of its functionalities. Therefore, given the call graph Gf of a
functionality f , and a decomposition D ⊆ 2E, the partition
call graph of a functionality partition(Gf , D) = (LT, RI) is
deﬁned by a set of local transactions LT and a set of remote
invocations RI, where each local transaction

(i) is a subgraph of the functionality call graph, ∀lt∈LT :

lt.A ⊆ Gf .A ∧ lt.P ⊆ Gf .P ;

(ii) contains only accesses in a single cluster of the domain

entities decomposition, ∀lt∈LT ∃cinD : lt.A.e ⊆ c;
(iii) contains all consecutive accesses in the same cluster,
((ai.e.c = aj.e.c ∧ (ai, aj) ∈
∀ai∈lt.A,aj ∈Gf .A :
Gf .P ) =⇒ (ai, aj) ∈ lt.P ) ∨ ((ai.e.c = aj.e.c ∧
(aj, ai) ∈ Gf .P ) =⇒ (aj, ai) ∈ lt.P ).

From the deﬁnition of local transaction, results the deﬁ-
nition of remote invocations, which are the elements in the
precedence relation that belong to different clusters, RI =
{(ai, aj) ∈ Gf .P : ai.e.c 6= aj.e.c}. Note that, in these
deﬁnitions, we use the dot notation to refer to elements of
a composite or one of its properties, e.g., in aj.e.c, .e denotes
the domain entity in the access, and .c the cluster the domain
entity belongs to.

The complexity for a functionality migration, in the context
of a decomposition, is the effort required in the functionality
redesign, because its transactional behavior is split into several
distributed transactions, which introduce intermediate states
due to the lack of isolation. Therefore, the following aspects
have impact on the functionality migration redesign effort:

• The number of local transactions, because each local

transaction may introduce an intermediate state;

• The number of other functionalities that read domain
entities written by the functionality, because it adds the
need to consider the intermediate states between the
execution of the different local transactions;

• The number of other functionalities that write domain en-
tities read by the functionality, because the functionality
redesign has to consider the different states these domain
entities can be.

This complexity is associated with the cognitive load that
the software developer has to address when redesigning a
functionality. Therefore, the complexity metric is deﬁned in
terms of the functionality redesign.

complexity(f, D) =

X
lt∈partition(Gf ,D)

complexity(lt, D)

The complexity of a functionality is the sum of the com-

plexities of its local transactions.

{fi 6= lt.f : dist(fi, D) ∧ a−1

complexity(lt, D) = #∪ai∈prune(lt)
i ∈ prune(fi, D))}

The complexity of a local transaction is the number of other
distributed functionalities that read, or write, domain entities,
written, or read, respectively by the local transaction. The
auxiliary function dist identiﬁes distributed functionalities,
given the decomposition; a−1
denotes the inverse access, e.g.
(e1, r)−1 = (ei, w); and prune denotes the relevant accesses
inside a local transaction, by removing repeated accesses of
the same mode to a domain entity. If both read and write
accesses occur inside the same local transaction, they are both
considered if the read occurs before the write. Otherwise, only
the write access is considered. These are the only accesses that
have impact outside the local transaction.

i

the number of clusters, and compare them according to the
complexity metric. Additionally, two different decompositions
of the same system can be compared using the MoJoFM [11]
distance metric, which will be use to compare the decompo-
sitions generated using statically and dynamically collected
data.

MoJoFM is a distance measure between two architectures
expressed as a percentage. This measure is based on two key
operations used to transform one decomposition into another:
moves (Move) of entities between clusters, and merges (Join)
of clusters. Given two decompositions, A and B, MoJoFM is
deﬁned as:

III. MONOLITH MICROSERVICES IDENTIFICATION

M oJoF M (A, B) = (1 −

mno(A, B)
max(mno(∀A, B))

) × 100%

The different approaches to the migration of monoliths to
microservices architectures apply, in the Collection step, either
static or dynamic techniques, but there is no evidence in the
literature on whether one of them subsumes the other, whether
they are equivalent, or even whether they are complementary.
Therefore, we collected data using both techniques in order to
address this open problem.

Data was collected from two monolith systems, LdoD1
and Blended Workﬂow (BW)2,
that are implemented us-
ing the Model-View-Controller architectural style, where the
controllers process input events by triggering transactional
changes in the model, thus, corresponding to monolith func-
tionalities. The monolith is designed considering its controllers
as transactions that manipulate a persistent model of domain
entities. Our collection tool was developed to cope with the
Spring-Boot3 framework and the F´enix Framework4 Object-
Relational Mapper (ORM).

As result of the collection, the functionalities accesses are
stored in JSON format. It consists in a mapping between func-
tionality names and functionality objects, where each object
has a traces ﬁeld that consists in a list of trace objects. Each
trace is characterized by a unique identiﬁer and a (compressed)
list of accesses observed for a speciﬁc functionality execution.
An Access is composed by the numeric identiﬁer of the
domain entity and the access type, either read or write.

During the Decomposition step of the migration process, our
tool uses hierarchical clustering (Python SciPy5) to process
the collected data and, according to the 4 similarity measures,
generate a dendrogram of the domain entities. The generated
dendrogram can be cut in order to produce different decom-
positions, given the number of clusters. Our decomposition
tool supports different combinations of similarity measures,
for instance, it is possible to generate a decomposition with
the following weights (30% access, 30% read, 20% write, 20%
sequence).

For the Analysis step our tool generates multiple decom-
positions, by varying the similarity measures weights and

1https://github.com/socialsoftware/edition
2https://github.com/socialsoftware/blended-workﬂow
3https://spring.io/projects/spring-boot
4https://fenix-framework.github.io/
5https://docs.scipy.org/doc/

is

the minimum number of Move
where mno(A, B)
and Join operations needed to transform A into B and
max(mno(∀A, B)) is the number of Move and Join opera-
tions needed to transform the most distant decomposition into
B.

A. Data Collection Tools

Two data collection tools were developed. Spoon [12] is a
static code analysis tool that provides an introspection API
that allows to parse and analyse a Java codebase by simply
giving its folders as input. It was customized to be applied to
identify Spring-Boot controllers and persistent domain entities
implemented using the FenixFramework ORM.

The dynamic data collection is done in a running instance of
the monolith under analysis using Kieker [13]. The monolith
systems were instrumented using AspectJ6 to intercept calls to
the FenixFramework’s data access methods, the ones respon-
sible for manipulating the respective entity’s persistent state.

B. Monolith Monitoring

While for the static data collection it was enough to run
the customized Spoon tool on the monolith codebases, for the
dynamic data collection three different monolith monitoring
strategies were followed: in production, through functional
testing, and by simulation.

Regarding the LdoD system,

it was monitored in three
different environments: production, functional testing and sim-
ulation. The production monitoring lasted 3 weeks and a
total of 490 GB worth of data was collected. Throughout
this period, a tight supervision was necessary to oversee the
impact the monitoring had on the performance of the system’s
functionalities. Since the server hosting the application had a
small free disk space (around 20 GB) and a massive drop in
performance was observed if it was full, it was mandatory to
collect the generated logs from time to time (2-3 days) to not
harm the user experience and to gather fresh logs instead of
discarding them.

Analyzing the collected data presented in Table I, only
44% of the controllers were exercised in production, when

6The

Eclipse
http://www.eclipse.org/aspectj/

Foundation

(2011).

The

AspectJ

Project.

TABLE I
COVERAGE OF DYNAMICALLY COLLECTED DATA

Coverage Controllers (%)
Coverage Entities (%)

LdoD
Tests
96
82

BW
Sim Sim
68
84
86
80

Prod
44
79

TABLE II
COMPARE COLLECTED DATA - AVERAGE OF IDENTIFIED ENTITIES PER
CONTROLLER

AVG(Cov. E/C)

LdoD

BW

Static
95%

Tests
71%

Static
91%

Sim
77%

Static
93%

Sim
78%

compared with the total number of controllers identiﬁed by the
static analysis. Therefore, further processing and evaluation of
this data were abdicated due to the substantial effort required
to process it and the relatively little coverage. Concerning
functional testing, it was achieved by running a suite of 200
integration tests (4.207 lines of code) that exercised 96% of
the controllers and 82% of the domain entities, generating
a few megabytes (<200 MB) of data, while the instruction
coverage, reported by JaCoCo7, was 72% for domain entities
and 82% for controllers. The reduced size of the collected
data is explained by the usage of small subset of the original
database’s data and so, the traces associated with the execution
of functionalities were much shorter. Finally, an expert of the
system simulated, during one hour, the use of functionalities,
using a database with a minimal set of data, and 200 MB of
data was collected and 84% of the controllers and 80% of the
domain entities were exercised.

In what concerns the BW system, it was only simulated
by an expert during an hour and 86% of entities and 68% of
controllers were exercised. In this case, the reduced number of
exercised controllers is justiﬁed by the deprecation of several
controllers that are not reachable through the user interface.

C. Static vs Dynamic Data Collection

The process of data collection obviously differ in the
coverage of controllers between static and dynamic collection,
and they also differ on the identiﬁcation of the domain entities
that each controller accesses.

Table II presents the percentage (average) of domain entities
that each controller accesses when comparing the different data
collection strategies. For instance, in LdoD, static analysis
identiﬁes 95% of the domain entities, when compared with
the identiﬁed through tests, while tests identify 71% of the
domain entities, when compared with the identiﬁed through
static analysis.

Therefore, we can observe that, for the coverage of the
accesses to domain entities in the context of the controllers, in
some cases, dynamic analysis can identify accesses to domain
entities,
the
static collection does not, due to late binding. This is one
of the limitations of the static analysis that may not be able

in the context of a controller execution, that

7https://github.com/jacoco/jacoco

to statically infer the type of a domain entity, in the case
of polymorphic inheritance. The opposite also occurs, static
analysis can identify accesses to domain entities that dynamic
analysis cannot, because depending on the inputs provided
to controllers and data available in the database, some of
the domain entities may not be accessed, both in tests and
simulation.

IV. EVALUATION

The goal of evaluation is to assess which technique, static or
dynamic, provides the best results. First we evaluate whether
the use of static or dynamic analysis allows to identify a
combination of similarity measures that provides better de-
compositions, in terms of complexity. Then, we assess whether
the dynamic analysis produces signiﬁcantly different decom-
positions, when compared to the ones statically generated and
with a source of truth.

In both analysis, the Decomposition step is going to be
applied to the data collected, statically and dynamically.
Therefore, several dendrograms are produced, by varying the
weights of the four existing similarity measures - Access (A),
Write (W), Read (R) and Sequence (S) - in intervals of 10 in
a scale of 0 to 100. For instance (40, 20, 20, 20) represents
a combination of similarity measures where a dendrogram is
generated using hierarchical clustering for the 40% access,
20% write, 20% read, and 20% sequence.

Then several cuts are performed on each dendrogram. Each
cut results in a candidate decomposition of the monolith with
a speciﬁc number of clusters, varying from 3 to 10. For each
generated decomposition, the values for the complexity metric
are calculated. The complexity metric value had to be nor-
malized in order to compare them among the two monoliths,
since they depend on the number of functionalities of each
monolith. The uniform complexity of a given decomposition
d of a monolith is calculated by dividing the complexity of
d by the maxComplexity. The maxComplexity value is
determined by calculating the complexity of a decomposition
of the monolith where each cluster has a single domain
entity. Therefore, the uniform complexity of any monolith
decomposition is a value in the interval 0 to 1.

Therefore, in the experiments, we calculate, for each system,
the uniform complexity of each decomposition generated by
the combination of the 4 similarity measures, each varying in
intervals of 10 and their sum being 100, and the number of
clusters (N), between 3 and 10.

A. Complexity and Similarity Measures Correlation

To assess the correlation between the complexity metric,
the weights given to each similarity measure, and the number
of clusters, a linear regression model was employed using the
Ordinary Least Squares method, as given by:

uComplexity(d) = β1 · d.weightA + β2 · d.weightW
+ β3 · d.weightR + β4 · d.weightS
+ β5 · #d.clusters + cons

To test this regression, a hypotheses was deﬁned as follows:

• H0: β1 = β2 = β3 = β4 = β5 = 0; meaning that the
complexity of a decomposition does not have a relation
with any of the ﬁve parameters

• H1: β1 6= 0 ∨ β2 6= 0 ∨ β3 6= 0 ∨ β4 6= 0 ∨ β5 6=
0; meaning that the complexity of a decomposition does
have a relation with at least one of the ﬁve parameters
The results for systems LdoD and BW are presented in
Tables III and IV. The regression results concerning the impact
of the combination of the similarity measures and number of
clusters on the complexity metric show that the dynamic and
static analysis have statistically signiﬁcant positive correlation
with complexity for the coefﬁcients of the number of clusters.
Regarding the similarity measures, all the analysis show
that, independently of using statically or dynamically collected
data, it is not possible to infer that one similarity measure
by itself is determinant to generate a decomposition with the
lowest complexity, because the magnitude of the coefﬁcients
is not pronounced and some conﬁdence intervals contain the
zero.

The obtained R2 values were considerably high with the
exception of functional testing environment in system LdoD
with just 0.176. This means that, apart from this speciﬁc
environment, the regression model explains most of the data-
set (low variability).

B. Best Complexity Decomposition

Although, it seems that both collection techniques provide
similar insight in terms of the correlation between the simi-
larity measures and the complexity metric, we want to know
whether they produce signiﬁcantly different decompositions.
To assess the results of the two techniques, we compare the
highest quality decompositions, in terms of complexity, from
each approach with a decomposition proposed by a domain
expert, for both systems. In this analysis we consider the expert
decompositions as reference and evaluate, using the MoJoFM
metric, which approach provides closer results to it. Since the
two techniques may miss some domain entities during the
collection phase, we decided that all the unassigned entities
would be put in the biggest cluster, as this strategy conforms
with the incremental decomposition strategy rationale [14,
Chapter 13].

The results from the comparisons are represented in Ta-
ble V, where each cell indicates the MoJoFM percentage value
(0 - 100%) between the lowest complexity decomposition with
N clusters, using a particular collection technique, and the
system’s expert decomposition. Overall, the MoJoFM values
obtained for the different collection approaches were very
similar, for both systems, which leads us to conclude that
there isn’t a collection technique that provides better results.
However, note that, especially on the simulation technique,
the dynamic analysis didn’t cover all controllers during the
collection phase and also missed more entities than the static
II. Therefore, we decided to
approach, see Tables I and
assess if the dynamic analysis approach could surpass the
static analysis if only the common controllers and entities were
considered.

To evaluate this scenario, we re-ran the static analysis on
the two monoliths considering only the common controllers
and domain entities, for each dynamic technique. The results
are represented in Table VI, where we can observe that, on
average, both approaches continue to generate decompositions
almost equally distant to the expert’s, for both systems. The
major noticed difference (7-9%), for system LdoD, is the
average MoJoFM values obtained for the static approach when
evened with the dynamic analysis using the expert simulation
approach. However, a similar impact is not seen for system
BW.

Based on these results, we conclude that, for both systems,
we don’t see signiﬁcant differences between the lowest com-
plexity decompositions obtained using statically and dynami-
cally collected data, and that none of the approaches achieve
identical decompositions to the expert’s, since the average
MoJoFM values obtained vary around 60-70%.

Given the similarities when compared to the expert, we
assessed how far apart the static and dynamic decompositions
were from each other, considering the common controllers and
entities.

Table VII presents the results of applying the MoJo metric
to the best decompositions of LdoD and BW. For LdoD,
the average MoJoFM between the evened static and tests
approaches is 75%, while between the evened static and
simulation approaches is 69%. For BW, the average MoJoFM
between the evened static and simulation approaches was
56%. Therefore, we can observe that the best decompositions
generated by the collection techniques tend to be closer to each
other than to the expert decomposition for monolith LdoD.
However, the same conclusion cannot be drawn for monolith
BW.

We have done an additional analysis, by inspecting the best
decomposition for each one of the evened techniques, and we
could observe that the clusters in the experts decomposition
were more balanced in terms of the number of domain entities
per cluster. This may be an indication that the expert cut
was driven by the structural qualities of the monolith, which
drive the domain model design. Anyway, when comparing the
generated decompositions we found similarities between the
semantics of the clusters.

Overall, this suggests that neither of the analysis techniques
outperforms the other, even though there is space for future
research.

V. RELATED WORK

In recent years, a myriad of approaches to support the
migration of monolith systems to microservices architectures
have been proposed [5], [6], [15]–[24], which use the monolith
speciﬁcation, codebase, services interfaces, runtime behavior,
and project development data to recommend the best decom-
positions [25].

In this paper we address the approaches that use the
monolith codebase or runtime behavior. Although they follow
the same steps, they diverge on what is their main concern and,
consequently, on the similarity measures that they use, such as

TABLE III
COMPARISON OF THE IMPACT OF SIMILARITY MEASURES ON COMPLEXITY FOR BOTH ANALYSIS ON LDOD

Static analysis

Coef.
0.0230
N
A
0.0035
W 0.0041
0.0039
R
S
-0.0002
R2

95% Interval
[0.021, 0.025]
[0.003, 0.004]
[0.004, 0.004]
[0.004, 0.004]
[-0.000,4.66e-05]
0.434

Coef.
0.0253
-0.0003
2.781e-05
-0.0002
0.0002

Tests

95% Interval
[0.023, 0.028]
[-0.001, -9.14e-05]
[-0.000, 0.000]
[-0.000, 7.17e-05]
[-2.19e-05, 0.000]
0.176

Coef.
0.0206
0.0017
0.0079
0.0007
0.0018

Simulation

95% Interval
[0.019, 0.023]
[0.002, 0.002]
[0.008, 0.008]
[0.001, 0.001]
[0.002, 0.002]
0.682

TABLE IV
COMPARISON OF THE IMPACT OF SIMILARITY MEASURES ON
COMPLEXITY FOR BOTH ANALYSIS ON BW

TABLE VI
COMPARING GENERATED WITH EXPERT DECOMPOSITIONS, CONSIDERING
ONLY THE COMMON CONTROLLERS AND ENTITIES

Static analysis

Simulation

Coef.
0.0439
N
A
0.0014
W 0.0019
0.0016
R
S
0.0021
R2

95% Interval
[0.043, 0.045]
[0.001, 0.002]
[0.002, 0.002]
[0.001, 0.002]
[0.002, 0.002]
0.632

Coef.
0.0277
-0.0011
0.0002
-0.0013
0.0019

95% Interval
[0.026, 0.029]
[-0.001, -0.001]
[-9.17e-08, 0.000]
[-0.001, -0.001]
[0.002, 0.002]
0.476

TABLE V
COMPARING GENERATED WITH EXPERT DECOMPOSITIONS

Static
62.12
60.61
56.06
78.79
77.27
83.33
81.82
45.45
68.18

LdoD
Tests
65.15
69.7
68.18
66.67
74.24
72.73
74.24
74.24
70.64

Sim
68.18
66.67
66.67
66.67
68.18
59.09
57.58
56.06
63.64

BW

Static
46.67
44.44
44.44
62.22
66.67
66.67
71.11
71.11
59.17

Sim
44.44
46.67
60.00
57.78
64.44
62.22
62.22
62.22
57.5

N

3
4
5
6
7
8
9
10

avg

accesses [8], reads [4], [7], writes [4], [7], and sequences [4].
On the other hand, some authors use execution traces to collect
the behavior of the monolith, e.g. [8], [24], but there is no
empirical evidence on whether it provides better data than the
static mechanisms, and what is the required effort to collect the
data, although the problem of analysing a large amount of data
was already reported in a another context [26]. Runtime traces
are used in [27] to calculate the percentage of calls between
packages to identify a microservices decomposition, but they
do not discuss the completeness of the data collection. As far
as our knowledge goes, there is no work on the comparison
between the use of static and dynamic analysis in the migration
of monolith systems to a microservices architectures.

Some of approaches also use different metrics to assess
the result of their decompositions. Therefore, we studied the
literature on microservices quality to identify which metrics
to consider. The metric we used for evaluating the complexity
of the decompositions are based on current state of the art
metrics for service-oriented systems [28]. We applied the
complexity metric for the migration of monolith systems to
microservices architecture [29], which was extended to also

LdoD

BW

Static
65.15
51.52
72.73
72.73
75.76
74.24
72.73
68.18
69.13

Tests
59.09
69.7
68.18
54.55
74.24
72.73
74.24
72.73
68.18

Static
63.64
62.12
63.64
68.18
63.64
68.18
57.58
56.06
62.88

Sim
71.21
71.21
66.67
66.67
69.7
59.09
57.58
56.06
64.77

Static
46.67
51.11
53.33
51.11
68.89
66.67
68.89
77.78
60.56

Sim
44.44
46.67
60.00
57.78
64.44
62.22
62.22
62.22
57.5

N

3
4
5
6
7
8
9
10

avg

TABLE VII
COMPARING STATIC WITH DYNAMIC DECOMPOSITIONS, CONSIDERING
ONLY THE COMMON CONTROLLERS AND ENTITIES

LdoD

Static vs Tests
57.41
83.02
78.85
78.85
78.85
80.77
78.85
60.00
74.58

Static vs Sim
80.77
82.35
80.39
78.00
74.00
61.22
48.98
46.94
69.08

BW
Static vs Sim
83.33
63.41
50.00
58.97
57.89
50.00
50.00
37.84
56.43

N

3
4
5
6
7
8
9
10

avg

consider several traces for a functionality, due to the result of
the dynamic collection the data. Other complexity metrics use
the percentage of services with support for transactions [30],
but they lack an integrated perspective that we provide by
deﬁning the transactional complexity of a functionality. An-
other complexity metric considers the number of operations
and services that can be executed in response to an incoming
request [31], while we consider the complexity of implement-
ing a local transaction in the terms of inter-functionalities
interactions, which emphasizes the complexity of cognitive
load, i.e., the total number of other functionalities to consider
when redesigning a functionality.

There is work that integrates static and dynamic analysis.
For instance, in [32], static analysis is used to complement
the incompleteness of dynamic analysis, in order to increase
programming comprehensibility. Recent work on the migration
of microservices also integrates static and dynamic analysis
techniques [33], [34], by complementing the data collected
through static analysis with dynamic analysis collected data.
None of these approaches evaluates or discusses the quality of

data obtained with each one of the techniques.

VI. DISCUSSION

A. Lessons Learned

From this research we learned the following lessons:
• It is not possible to conclude that the decompositions
generated using one of the analysis techniques, static or
dynamic, outperforms the other.

• The effort to collect data dynamically is signiﬁcantly su-
perior than the static collection, specially when collecting
and evaluating data from production, which resulted in a
large amount of collected data and a very low coverage.
On the other hand,
that
achieved better coverage, has a high development cost,
because, contrary to unit tests, which aim to have 100%
coverage, integration tests, which are harder to develop
and maintain, are usually designed to verify the modules
integration, not the execution of all paths.

the use of integration tests,

B. Threats to Validity

1) Internal Validity: Since dynamic analysis adds an extra
layer of computation on top of the monitored systems runtime
behaviour, the assumptions made on the instrumentation, to
minimize the performance degradation perceived by end-users,
can biase the obtained results given that: (i) an iterable object
type is considered to be the type of the ﬁrst element and
(ii) new records are discarded when Kieker’s queue is full.
Concerning (i) it is somehow balanced by the fact that the
static analysis may also not identify the types of objects due
to dynamic binding. In what regards (ii), in the collection done
through tests and simulation the probability of this situation
to occur is low, because it is a single user and the amount of
data in the database is small.

The approach of placing the entities not found during the
collection process into the biggest cluster, when comparing
the static and dynamic decompositions with the expert’s, may
have biased our results, as there is a probability associated
with the expert decomposition that may or may not contain
those entities in the same cluster. However, we also made
the comparisons using other approaches and achieved similar
results, thus, we are conﬁdent in discarding this as a threat.

2) External Validity: Due to the effort associated with the
dynamic collection of data, we only analyzed two systems,
but from the comparison with the decompositions generated
from statically collected data, we may extrapolate that the
quality of one decomposition does not outperforms the other,
though the dynamic analysis of more monoliths is necessary.
Nevertheless, the conclusions about the incompleteness of data
and required effort associated with the dynamic collection of
data are evident and shows that a cost/beneﬁt relation may
tend for the static analysis approaches.

Due to the diversity of metrics that exist for complexity
can our results be generalized? We have done an analysis of
the state of the art on metrics for microservices. Despite this
diversity, we are conﬁdent that the results are relevant because
the several metrics analyse the same elements. Our complexity

metric focus on the complexity introduced by transactions and
the complexity of the interactions, like other metrics do.

As described in the related work, several similarity measures
have been deﬁned to feed the automatic decomposition algo-
rithms. In this works we have focused on the measures that
correlated domain entities access, which cover a signiﬁcant
number of the existing approaches.

C. Future Work

As a consequence of the results of this research and the
learned lessons we identify the following topics for future
work:

• Further explore the results of the dynamic collection of
data, in terms of the frequency of each of the function-
alities, and deﬁne new similarity measures to verify if it
can generate better decompositions;

• Investigate other sequence compression algorithms with
the purpose of decreasing the JSON ﬁle size and also the
time taken to process it.

VII. CONCLUSIONS

The migration of monolith systems to the microservices
architecture is a complex problem that software development
teams have to address when systems become more complex
and larger in scale. Therefore, it
is necessary to develop
the methods and tools that help and guide them on the
migration process. One of the most challenging problems is
the identiﬁcation of microservices. Several approaches have
been proposed to automate such identiﬁcation, which, although
following the same steps, use different monolith analysis
techniques, similarity measures, and metrics to evaluate the
quality of the system.

In this paper, two monolith systems were analysed to study
the impact of applying static and dynamic analysis on the
quality of the automatically generated decompositions as well
as whether a particular combination of similarity measures
provides better decompositions.

As result of the experiments and analysis, we conclude that
different monolith analysis techniques generate decomposi-
tions that do not outperform each other, but, it was clear that
the effort required by the dynamic analysis is much superior
and resulted in less coverage. Although the cost is much
higher, both systems were extensively dynamically analyzed
which, and compared with the static analysis, is a signiﬁcant
effort.

As additional contributions, (i) the gathered data from
the evaluated monolith systems, using dynamic analysis, is
publicly available and can be used by third parties to do
further research, (ii) the data collectors were implemented to
be as conﬁgurable and extensible as possible such that they
can handle a wider variety of code bases with different JAVA
technology stacks.

In terms of future work, due to the different approaches
proposed to the migration of monolith systems into the mi-
croservices architecture, it is necessary to do more studies that
compare static and dynamic collections of data, in the context

of more systems. Additionally, this type of study needs to be
extended to other variations of the approaches, besides the
data collection techniques, like other similarity measures and
quality metrics.

VIII. DATA AVAILABILITY

The data used and produced in this research is available at

http://doi.org/10.5281/zenodo.5675593.

IX. ACKNOWLEDGEMENT

This work was supported by national funds through FCT,
Fundac¸˜ao para a Ciˆencia e a Tecnologia, under project
UIDB/50021/2020.

REFERENCES

[1] M. Fowler and J. Lewis, “Microservices,” 2014. [Online]. Available:

http://martinfowler.com/articles/microservices.html

[2] C. O’Hanlon,

“A conversation with werner

4,

vol.
https://doi.org/10.1145/1142055.1142065

14–22, May

no.

p.

4,

2006.

vogels,” Queue,
[Online]. Available:

[3] D. Taibi and V. Lenarduzzi, “On the deﬁnition of microservice bad

smells,” IEEE Software, vol. 35, no. 3, pp. 56–62, 2018.

[4] M. J. Amiri, “Object-aware identiﬁcation of microservices,” in 2018
IEEE International Conference on Services Computing (SCC), 2018,
pp. 253–256.

[5] J. Fritzsch, J. Bogner, A. Zimmermann, and S. Wagner, “From monolith
to microservices: A classiﬁcation of refactoring approaches,” in Software
Engineering Aspects of Continuous Development and New Paradigms
of Software Production and Deployment, J.-M. Bruel, M. Mazzara, and
B. Meyer, Eds. Cham: Springer International Publishing, 2019, pp.
128–141.

[6] G. Mazlami, J. Cito, and P. Leitner, “Extraction of microservices from
monolithic software architectures,” in Web Services (ICWS), 2017 IEEE
International Conference on.

IEEE, 2017, pp. 524–531.

[7] S. Tyszberowicz, R. Heinrich, B. Liu, and Z. Liu, “Identifying mi-
croservices using functional decomposition,” in Dependable Software
Engineering. Theories, Tools, and Applications, X. Feng, M. M¨uller-
Olm, and Z. Yang, Eds. Cham: Springer International Publishing, 2018,
pp. 50–65.

[8] W. Jin, T. Liu, Y. Cai, R. Kazman, R. Mo, and Q. Zheng, “Service
candidate identiﬁcation from monolithic systems based on execution
traces,” IEEE Transactions on Software Engineering, pp. 1–1, 2019.
[9] N. Ford, R. Parsons, and P. Kua, Building Evolutionary Architectures,

1st ed. O’Reilly, 10 2017.

[10] A. Carrasco, B. v. Bladel, and S. Demeyer, “Migrating towards
microservices: Migration and architecture smells,” in Proceedings of
the 2nd International Workshop on Refactoring, ser. IWoR 2018. New
York, NY, USA: Association for Computing Machinery, 2018, p. 1–6.
[Online]. Available: https://doi.org/10.1145/3242163.3242164

[11] Z. Wen and V. Tzerpos, “An effectiveness measure for software cluster-
ing algorithms,” in Proceedings. 12th IEEE International Workshop on
Program Comprehension, 2004.

IEEE, 2004, pp. 194–203.

[12] R. Pawlak, M. Monperrus, N. Petitprez, C. Noguera, and L. Seinturier,
“Spoon: A library for implementing analyses and transformations of
Java source code,” Software: Practice and Experience, vol. 46, no. 9,
pp. 1155–1179, 2016.

[13] W. Hasselbring
framework
vol.

and A.
for
5,

ing
Impacts,
100019,
http://www.sciencedirect.com/science/article/pii/S2665963820300063

software
p.

engineering

2020.

“Kieker: A monitor-
Software
research,”
[Online]. Available:

van Hoorn,

[14] C. Richardson, Microservices
Manning

Java.
https://books.google.pt/books?id=UeK1swEACAAJ

Publications,

Patterns: With

2019.

[Online].

Examples

in
Available:

[15] M. Ahmadvand and A. Ibrahim, “Requirements reconciliation for scal-
able and secure microservice (de)composition,” in 2016 IEEE 24th
International Requirements Engineering Conference Workshops (REW),
Sep. 2016, pp. 68–73.

[16] M. Gysel, L. K¨olbener, W. Giersche, and O. Zimmermann, “Service
cutter: A systematic approach to service decomposition,” in Service-
Oriented and Cloud Computing, M. Aiello, E. B. Johnsen, S. Dustdar,
and I. Georgievski, Eds. Cham: Springer International Publishing, 2016,
pp. 185–200.

[17] S. Hassan and R. Bahsoon, “Microservices and their design trade-offs:
A self-adaptive roadmap,” in 2016 IEEE International Conference on
Services Computing (SCC), June 2016, pp. 813–818.

[18] L. Baresi, M. Garriga, and A. De Renzis, “Microservices identiﬁcation
through interface analysis,” in Service-Oriented and Cloud Computing,
F. De Paoli, S. Schulte, and E. Broch Johnsen, Eds. Cham: Springer
International Publishing, 2017, pp. 19–33.

[19] S. Klock, J. M. E. M. V. D. Werf, J. P. Guelen, and S. Jansen, “Workload-
based clustering of coherent feature sets in microservice architectures,”
in 2017 IEEE International Conference on Software Architecture (ICSA),
April 2017, pp. 11–20.

[20] R. Nakazawa, T. Ueda, M. Enoki, and H. Horii, “Visualization tool for
designing microservices with the monolith-ﬁrst approach,” in 2018 IEEE
Working Conference on Software Visualization (VISSOFT), Sep. 2018,
pp. 32–42.

[21] L. De Lauretis, “From monolithic architecture to microservices archi-
tecture,” in 2019 IEEE International Symposium on Software Reliability
Engineering Workshops (ISSREW), 2019, pp. 93–96.

[22] M. H. Gomes Barbosa and P. H. M. Maia, “Towards identifying
microservice candidates from business rules implemented in stored
procedures,” in 2020 IEEE International Conference on Software Ar-
chitecture Companion (ICSA-C), 2020, pp. 41–48.

[23] A. Selmadji, A. Seriai, H. L. Bouziane, R. Oumarou Mahamane,
P. Zaragoza, and C. Dony, “From monolithic architecture style to
microservice one based on a semi-automatic approach,” in 2020 IEEE
International Conference on Software Architecture (ICSA), 2020, pp.
157–168.

[24] Y. Zhang, B. Liu, L. Dai, K. Chen, and X. Cao, “Automated microservice
identiﬁcation in legacy systems with functional and non-functional met-
rics,” in 2020 IEEE International Conference on Software Architecture
(ICSA), 2020, pp. 135–145.

[25] F. Ponce, G. M´arquez, and H. Astudillo, “Migrating from monolithic ar-
chitecture to microservices: A rapid review,” in 2019 38th International
Conference of the Chilean Computer Science Society (SCCC), 2019, pp.
1–7.

[26] B. Cornelissen, D. Holten, A. Zaidman, L. Moonen, J. J. van Wijk,
and A. van Deursen, “Understanding execution traces using massive
sequence and circular bundle views,” in 15th IEEE International Con-
ference on Program Comprehension (ICPC ’07), 2007, pp. 49–58.
[27] F. D. Eyitemi and S. Reiff-Marganiec, “System decomposition to
optimize functionality distribution in microservices with rule based
approach,” in 2020 IEEE International Conference on Service Oriented
Systems Engineering (SOSE), 2020, pp. 65–71.

[28] J. Bogner, S. Wagner, and A. Zimmermann, “Automatically measuring
the maintainability of service- and microservice-based systems: A
literature review,” in Proceedings of the 27th International Workshop on
Software Measurement and 12th International Conference on Software
Process and Product Measurement, ser. IWSM Mensura ’17. New
York, NY, USA: Association for Computing Machinery, 2017, p.
107–115. [Online]. Available: https://doi.org/10.1145/3143434.3143443
[29] N. Santos and A. Rito Silva, “A complexity metric for microservices
architecture migration,” in 2020 IEEE International Conference on
Software Architecture (ICSA), 2020, pp. 169–178.

[30] M. Hirzalla, J. Cleland-Huang, and A. Arsanjani, “A metrics suite for
evaluating ﬂexibility and complexity in service oriented architectures,”
in Service-Oriented Computing – ICSOC 2008 Workshops, G. Feuerlicht
and W. Lamersdorf, Eds. Berlin, Heidelberg: Springer Berlin Heidel-
berg, 2009, pp. 41–52.

[31] M. Perepletchikov, C. Ryan, K. Frampton, and Z. Tari, “Coupling
metrics for predicting maintainability in service-oriented designs,” in
2007 Australian Software Engineering Conference (ASWEC’07), 2007,
pp. 329–340.

[32] C. Patel, A. Hamou-Lhadj, and J. Rilling, “Software clustering using
dynamic analysis and static dependencies,” in 2009 13th European
Conference on Software Maintenance and Reengineering, 2009, pp. 27–
36.

[33] A. Krause, C. Zirkelbach, W. Hasselbring, S. Lenga, and D. Kroger,
“Microservice decomposition via static and dynamic analysis of the

monolith,” in 2020 IEEE International Conference on Software Archi-
tecture Companion (ICSA-C), 2020, pp. 9–16.

[34] T. Matias, F. F. Correia, J. Fritzsch, J. Bogner, H. S. Ferreira, and
A. Restivo, “Determining Microservice Boundaries: A Case Study
Using Static and Dynamic Software Analysis,” arXiv e-prints, p.
arXiv:2007.05948, Jul. 2020.

