2
2
0
2

y
a
M
0
3

]
L
M

.
t
a
t
s
[

1
v
7
4
4
5
1
.
5
0
2
2
:
v
i
X
r
a

Holistic Generalized Linear Models

Benjamin Schwendinger
TU Wien

Florian Schwendinger
University of Klagenfurt

Laura Vana
TU Wien

Abstract

Holistic linear regression extends the classical best subset selection problem by adding
additional constraints designed to improve the model quality. These constraints include
sparsity-inducing constraints, sign-coherence constraints and linear constraints. The R
package holiglm provides functionality to model and ﬁt holistic generalized linear models.
By making use of state-of-the-art conic mixed-integer solvers, the package can reliably
solve GLMs for Gaussian, binomial and Poisson responses with a multitude of holistic
constraints. The high-level interface simpliﬁes the constraint speciﬁcation and can be
used as a drop-in replacement for the stats::glm() function.

Keywords: Algorithmic regression, best subset selection, conic programming, holistic con-
straints, optimization, R.

1. Introduction

Selecting a sensible model from the set of all possible models is an important but typically
time-consuming task in the data analytic process. To simplify this process, Bertsimas and
King (2015); Bertsimas and Li (2020) introduce the holistic linear model (HLM). The HLM
is a constrained linear regression model in which the constraints aim to automate the model
selection process.
In particular, this can be achieved by utilizing quadratic mixed-integer
optimization, where the integer constraints are used to place cardinality constraints on the
linear regression model.

Cardinality constraints are used to introduce sparsity in the statistical model, a desirable
property that leads to more interpretability, increase in computational eﬃciency (especially
in high-dimensional settings) and reduction in the variance of the estimates (at the cost of
introducing some bias). Placing a cardinality constraint on the total number of variables
allowed in the ﬁnal model leads to the classical best subset selection problem (Miller 2002)
which, given a response vector y ∈ Rn, a predictor matrix X ∈ Rn×p and a subset size k
between 0 and min{n, p}, ﬁnds the subset of at most k predictors that produces the best ﬁt
in terms of squared error, solving the nonconvex problem

minimize
β

1
2 ky − Xβk2

2

subject to

p
X

i=1

I{βi6=0} ≤ k.

Further cardinality constraints placed on user-deﬁned groups of predictors can be used to e.g.
limit the pairwise multicollinearity, select the best (non-linear) transformation for the predic-
tors, include expert knowledge in the model estimation (such as forcing speciﬁc predictors to

 
 
 
 
 
 
2

Holistic GLMs

stay in the model) or ensure that certain sets of predictors are jointly included (excluded) in
(from) the model.

In addition to cardinality constraints, upper or lower bounds as well as linear constraints on
the coeﬃcients are also relevant for improving interpretability along with introducing sparsity
(e.g. see Slawski and Hein 2013, who introduce non-negativity constraints in linear regression
and show performance similar to other sparsity inducing methods).

In this paper, we introduce the holiglm package, an R package for formulating and ﬁtting
holistic generalized linear models (HGLMs). The package supports holistic constraints such
as mixed-integer-type constraints related to sparsity (e.g. limits on the number of covariates
to be included in the model, group sparsity constraints), linear constraints and constraints
related to multicollinearity. The contribution of the paper is three-fold. First, to the best
of our knowledge, we are the ﬁrst to suggest the use of conic optimization to extend the
results presented for linear regression by Bertsimas and King (2015); Bertsimas and Li (2020)
to the class of generalized linear models and to formulate the most common GLMs as conic
optimization problems. Secondly, we survey the literature related to holistic linear regression
and, more generally, to constrained regression and provide an extensive survey of what should
be considered as holistic constraints. Finally, we provide a ready-to-use implementation of
holistic GLMs in the package holiglm. We exemplify the use of the package for a variety
of statistical problems with the hope that this will encourage the statistical community in
further exploiting the recent advances in conic mixed-integer optimization.

The holiglm package provides a ﬂexible infrastructure for automatically translating con-
strained generalized linear models into conic optimization problems. The optimization
problems are solved by utilizing the R optimization infrastructure package ROI (Theußl,
Schwendinger, and Hornik 2020). Additionally, a high-level interface, which can be used as a
drop-in replacement for the stats::glm() function, is provided. Using ROI makes it possi-
ble for the user to choose between a wide range of commercial and open source optimization
solvers. With recent advancements in conic optimization, it is now possible to routinely solve
conic problems with a large number of variables and/or constraints to proven optimality.
Using conic optimization instead of iteratively reweighted least squares (IRLS) has the ad-
vantages that no starting values are needed, the results are more reliable and the solvers
are designed to handle diﬀerent types of constraints. These advantages come at the cost of
a longer runtime; however, as shown by Schwendinger, Grün, and Hornik (2021) for some
GLMs the speed of the conic formulation is similar to the IRLS implementation.

At the time of writing, there exists no ready-to-use package or library for ﬁtting HGLMs or
HLMs. However, for R (R Core Team 2022) several packages allow the estimation of (gen-
eralized) linear models under linear constraints. The CMLS (Helwig 2018) package can ﬁt
linear regression models under linear constraints by translating the problem into a linear con-
strained quadratic optimization problem, which is solved by quadprog (Turlach, Weingessel,
and Moler 2019). Package colf (Boutaris 2017) can ﬁt linear regression models with lower
and upper bounds on the coeﬃcients. The glmc (Chaudhuri, Handcock, and Rendall 2006)
package can ﬁt generalized linear models while imposing linear constraints on the parame-
ters. Similarly, package restriktor (Vanbrabant 2022) provides functionalities for estimation,
testing and evaluation of linear equality and inequality constraints about parameters and
eﬀects for generalized linear models, where the constraints can be speciﬁed by a text-based
description. Finally, ConsReg (Sallés 2020) provides a similar functionality to restriktor.

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

3

This paper is structured as follows: Section 2 introduces the GLM class and presents the rep-
resentation of speciﬁc family-link combinations as conic optimization models. In Section 3, we
present a list of holistic constraints implemented in the package. Section 4 introduces pack-
age holiglm for R and Section 5 shows the usage of the package in two applications: fairness
constraints in logistic regression and model selection in log-binomial regression. Section 6
concludes.

2. Generalized linear models

A generalized linear model (GLM) is a model for a response variable whose conditional distri-
bution belongs to a one-dimensional exponential family. A GLM consists of a linear predictor

ηi = β0 + β1x1i + . . . + βpxpi = x>

i β

and two functions, namely a twice continuously diﬀerentiable and invertible link function g
that describes how the mean, E(Yi) = µi, depends on the linear predictor (one has g(µi) = ηi),
and a variance function V that describes how the variance depends on the mean VAR(Yi) =
φV (µ) (where the dispersion parameter φ > 0 is a constant).
The most common distributions for the response Yi that can be accommodated in the GLM
setting include the normal, binomial or Poisson. The density of members of the exponential
family can be written as:

f (y; λ, φ) = exp

(cid:26) yλ − b(λ)
φ

+ c(y, φ)

(cid:27)

‚

where λ is called the canonical or natural parameter, b(·) and c(·) are known real-valued
measurable functions that vary from one exponential family to another.
It can be shown that E(y) = b0(λ) and VAR(y) = φb00(λ) holds. Note that g(b0(λ)) = ηi. We
can denote h = (b0)−1 ◦ g−1 so λ(β) = h(η). In case h is the identity, the link g is called
canonical, i.e., b0(·) is the inverse of the canonical link function.
The parameters β can be estimated by maximizing the likelihood function corresponding to
the diﬀerent types of response. Using the distribution of the exponential family, the log-
likelihood for a sample y = (y1, . . . , yn)> is given by:

log L(β; y) =

n
X

i=1

yiλi(β) − b(λi(β))
φi

+ c(yi, φi).

(1)

where φi = φ/ai for ai are user-deﬁned observation weights.
In this paper we leverage the fact that, for the most common family-link combinations, the
maximization of the objective function in Equation 1 can be reformulated as a conic opti-
mization problem. A conic optimization problem is designed to model convex problems by
optimizing a linear objective function over the intersection of an aﬃne hyperplane and a
nonempty closed convex cone. For GLMs, the types of cones relevant for maximum likeli-
hood estimation include the exponential cone Kexp, which can be used to model a variety of
objectives and constraints involving exponentials and logarithms, and the second-order cone
Ksoc, which can be used to model problems involving – directly or indirectly – quadratic

Holistic GLMs

4

Family-
link
Gaussian
Identitya

λi(β), bi(β)

λi(β) = x>
i β ,
bi(β) = (x>

i β)2/2

Binomial
Logitb & λi(β) = x>
Probit

i β
bi(β) = log(1 + exp(x>

i β))

Logb

Poisson

Logb

Identityb

λi(β) = x>
bi(β) = − log (cid:0)1 − exp(x>

i β − log(1 − exp(x>
i β)(cid:1)

i β))

i β

λi(β) = x>
bi(β) = exp(x>

i β)

λi(β) = log(x>
bi(β) = x>

i β

i β)

Objective

−ζ

Conic
constraint

(ζ + 1, ζ − 1, 2(y1 − x>
. . . , 2(yn − x>

1 β),
n β)) ∈ Kn+2
soc

n
P
i=1

yix>

i β − δi

n
P
i=1

yix>

i β + (1 − yi)δi

(δi, 1, γi + 1) ∈ Kexp
i β, 1, γi) ∈ Kexp
(x>

i β ≤ 0
x>
(δi, 1, 1 − γi) ∈ Kexp
i β, 1, γi) ∈ Kexp
(x>

yix>

i β − δi

(x>

i β, 1, δi) ∈ Kexp

n
P
i=1

n
P
i=1

yiδi − x>

i β

(δi, 1, x>
i β) ∈ Kexp
i β ≥ 0
x>

(δi, 1, x>
(ζ + 1, ζ − 1, 2x>
. . . , 2x>

i β) ∈ Kexp
1 β,
n β) ∈ Kn+2
soc

Sq.rootab

λi(β) = 2 log(x>
i β)2
bi(β) = (x>

i β)

n
P
i=1

2yiδi − ζ

Table 1: GLMs as exponential families with conic reformulations. The objective functions
to be maximized are proportional to ∝ Pn
i=1 yiλi(β) − bi(β). The conic constraints with
subscript i should hold for all i = 1, . . . , n. Superscript a marks family-link combinations with
an objective modeled by second-order cones. Superscript b marks family-link combinations
with an exponential cone representation. Note that for the probit link an approximation is
used by scaling the coeﬃcients obtained from the logit link by pπ/8 which is equivalent to
using a simple approximation (Page 1977) for the standard normal distribution.

terms. Table 1 presents the log-likelihood functions for the family-link combinations imple-
mented in package holiglm, and their reformulation as conic problems. More details on these
reformulations can be found in Appendix A.

Inference is based on maximum likelihood, unless the holistic constraints presented in Sec-
tion 3 are binding. In the case of binding constraints, we employ the constrained maximum
likelihood framework in Schoenberg (1997) and apply a correction to the standard errors of
the coeﬃcients.

3. Holistic constraints

Bertsimas and King (2015) and Bertsimas and Li (2020) introduced a set of constraints
designed to improve the quality of linear regression models. These constraints are crafted
based on a survey of modeling recommendations from statistical textbooks and articles. In
this section we survey diﬀerent constraints suggested by the literature and provide an extensive

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

5

overview which includes but is not limited to constraints in Bertsimas and King (2015) and
Bertsimas and Li (2020). Moreover, we provide their formulations as optimization constraints.
Most of the constraints introduced in holiglm are cardinality constraints. For modeling car-
dinality constraints we need p binary variables:

zj ∈ {0, 1},

j = 1, . . . , p.

(2)

These binary variables are only added to the model if needed. Here, the binary variable zj
represents the selection of variable Xj, hence, zj = 0 implies βj = 0. Problems which involve
such binary variables are called mixed-integer optimization problems. This type of cardinality
constraints can be modeled with a so-called big-M constraint.

−M zj ≤ βj ≤ M zj,

j = 1, . . . , p,

(3)

where M is a positive constant. For the big-M constraint it is important to choose a good
constant M . A good M can be characterized by two properties:

• it is chosen big enough, that it does not impact the magnitude of the parameter βi.

• it is chosen as small as possible to improve the speed of the underlying optimization

solver and ensure stability.

3.1. Global sparsity

The global sparsity constraint can be used to model the classical best subset selection problem
(Miller 2002). The best subset selection problem is a combinatorial optimization problem
designed to select a predeﬁned number of kmax covariates out of all available covariates. This
can be accomplished by maximizing the likelihood function while restricting the number of
covariates via integer constraints:

p
X

j=1

zj ≤ kmax,

where kmax ≤ p, kmax ∈ N.

3.2. Group sparsity

Contrary to global sparsity constraints, group sparsity constraints do not restrain all covari-
ates but only speciﬁc local groups of covariates. Group sparsity constraints can again be
subdivided into excluding and including constraints.

• excluding constraints enforce a cardinality constraint on a subset of covariates;

• including constraints enforce that either all covariates of a group are selected or none.

Bertsimas and King (2015) suggest to use excluding constraints for modeling pairwise multi-
collinearity and selecting the best (non-)linear transformation. Including constraints can be
used for modeling categorical variables via one-hot encoding or splines via spline basis func-
tions. In the following we present the formulation of the excluding group sparsity constraints
for restricting pairwise multicollinearity and for selecting the best variable transformation.

6

Holistic GLMs

Limited pairwise multicollinearity

A desired property for a good model is that the covariates exhibit no multicollinearity.
Collinear covariates can cause numerical problems during the estimation, less interpretable
coeﬃcients and misleading standard errors.

The group sparsity constraint can be used to restrict pairwise multicollinearity by excluding
pairs of covariates that exhibit strong pairwise correlation.
Using the previously introduced variables z, the pairwise multicollinearity can be formulated
as,

zj + zk ≤ 1 ∀(j, k) ∈ PC,
here PC = {(j, k) | ρmax < |ρ(j, k)|} denotes the set of pairs of highly correlated covariates.
The general rule of thumb is excluding pairs of covariates where the absolute value of the pair-
wise correlation exceeds the threshold ρmax = 0.7, but other – less as well as more restrictive
– choices are also advocated for in the literature (see Dormann, Elith, Bacher, Buchmann,
Carl, Carré, Marquéz, Gruber, Lafourcade, Leitão et al. 2013, for a review).

Non-linear transformations

In cases where the true relationship between the response and a covariate is non-linear, using
an appropriate transformation can improve model quality. Commonly non-linear transfor-
mations like log(x),
x or x2 are used. When using non-linear transformations, it is often
desirable to only include either the non-transformed variable or only one of the transformed
variables. A group sparsity constraint of the form

√

X

zj ≤ 1,

j∈N L

can be used to select at most one of the transformations. Here N L denotes the set of indices
of applied transformations, including the identity mapping, on a certain covariate.

Including group constraints

In cases where all covariates within a speciﬁed group G should be included in the model (as
in the case of dummy-encoded categorical variables), the including group sparsity constraint
(also referred here as in-out constraint) is translated to equality of all corresponding zi’s
variables:

zj = zk ∀(j, k) ∈ G

3.3. Modeler expertise

Bertsimas and King (2015) point out that, in some situations, the modeler knows the true
importance of a particular covariate, be it some business requirement or other more profound
expert knowledge. In these cases, the modeler can choose to force the inclusion of a particular
covariate. This might be necessary when the automated covariate selection process decides
to discard the covariate due to other global or group sparsity constraints. To achieve this
inclusion, the following constraint can be used:

zj = 1.

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

7

3.4. Bounded domains for covariates

In certain applications, it is of interest to set constraints (bounds) on the value of particular
regression coeﬃcients. For example, McDonald and Diamond (1990) consider the problem
of ﬁnding maximum likelihood estimates of a generalized linear model when some or all
regression parameters are constrained to be non-negative and motivate their approach with
applications in cancer death prediction and demography. Similarly, Slawski and Hein (2013)
propose the use of non-negative least squares in high-dimensional linear models. They show
that this can have similar eﬀects to explicit regularization like LASSO (Tibshirani 1996) or
ridge regression (Hoerl and Kennard 1970) regarding predictive power and sparsity when mild
conditions on the design matrix are met. Moreover, there exist problems where the sign of the
eﬀect of a covariate is known beforehand. This allows the restriction of coeﬃcient domains to
only non-negative or only non-positive values (Carrizosa, Olivares-Nadal, and Ramírez-Cobo
2020). In our conic model approach, for a coeﬃcient βj, we can add bounds (sometimes also
called box constraints) lj and uj by the following constraint:

lj ≤ βj ≤ uj.

3.5. Linear constraints

As Lawson and Hanson (1995) point out, it might also be desirable to enforce linear constraints
of the type Lβ ≤ c for certain covariates (typical constraints include βj + βk ≤ βl, βj ≤
βk or βj = βk = βl). Such constraints might arise in applied mathematics, physics and
economics and usually convey additional information about a problem. Adding additional
convex constraints to a problem will only tighten the set of solutions. One still has to be
careful not to add constraints that make the underlying optimization problem infeasible. If
any infeasibility arises, the underlying optimization solver will detect it.
Note that such linear constraints can also be imposed on the binary variables zi. Said con-
straints can be used to model if-then types of relations. As an example, consider a constraint
of the type “if Xj is included then Xk should also be included in the model”. This would
translate to the constraint zj ≤ zk. Note also that the global and group sparsity constraints
introduced above are special cases of linear constraints on the binary variables.

3.6. Sign coherence constraint

Sign coherence can be a desirable model property which improves interpretability, especially in
the presence of highly correlated covariates. Carrizosa et al. (2020) suggest using a sign coher-
ence constraint instead of the group sparsity constraint to restrict pairwise multicollinearity.
They argue that enforcing sign coherence (forcing strongly positive correlated variables to
have the same sign and strongly negative correlated variables to have opposite signs) is less
restrictive than using a group sparsity constraint, since it allows strongly correlated variables
to be jointly in the model without lowering interpretability due to inconsistent signs. With-
out this restriction, inconsistent signs of strongly correlated variables are often caused by
compensating coeﬃcients (Hahs-Vaughn 2016). Given the set PC of pairs of indices of highly
correlated covariates, we can enforce equal signs of their respective coeﬃcients by adding the
constraints

−M (1 − ujk) ≤ βj, βk ≤ M ujk,

∀(j, k) ∈ PC,

8

Holistic GLMs

Link / Family Gaussian
Q | SOC
Identity
Log
Logit
Probit
Square root

Binomial

LIN & EXP
EXP
EXP

Poisson
LIN & EXP
EXP

SOC & EXP

Table 2: Overview of the cones needed to express speciﬁc GLMs. Here, Q | SOC indicates
that either a quadratic solver or a solver supporting the second-order cone can be used. SOC
& EXP indicates that the exponential cone and second-order cone are used to model the
Poisson model with power link. Note that for the probit link an approximation is used by
scaling the coeﬃcients obtained from the logit link by pπ/8 (Page 1977).

where again M is a large enough constant and ujk is a newly introduced binary variable. One
can see that for ujk = 0 we have −M ≤ βj, βk ≤ 0, while for ujk = 1, we have 0 ≤ βj, βk ≤ M
for an arbitrary pair (j, k) in PC.

4. The holiglm package

Package holiglm allows to reliably and conveniently ﬁt generalized linear models under con-
straints. We aimed to allow for as many family-link combinations and constraints as possible
without reducing the reliability of the solution. To accomplish these goals, the package uses
state-of-the-art mixed-integer (conic) solvers. Using conic optimization, we can reliably solve
convex non-linear mixed-integer problems, given that there exists a combination of cones
that can express the non-linear problem at hand. Luckily, the (log-)likelihood of most GLMs
can be expressed by combinations of the linear (non-negative) cone, the second-order cone
and the exponential cone. Table 2 gives an overview on the cones used to express speciﬁc
GLMs. Additionally, Table 3 provides a list of the solvers available from ROI which allow
for mixed-integer constraints. Information on the diﬀerent types of cones supported by these
solvers is also provided. All the solvers listed in Table 3 internally aim to prove the optimality
of the solution by checking criteria based on the Karush-Kuhn-Tucker optimality conditions.
Consequently, if these solvers signal optimality, the user can be certain that the maximum
likelihood estimate (MLE) was obtained, except for the special case where the MLE does not
exist. In theory, these solvers should be able to provide a certiﬁcate of unboundedness if the
MLE does not exist; however, practically this is often not the case for the exponential cone.
Fortunately, package detectseparation (Kosmidis, Schumacher, and Schwendinger 2022) can
be used to verify the existence of the MLE before the estimation. More information on conic
optimization in general can be found in Boyd and Vandenberghe (2004).
In R, the packages ROI and CVXR (Fu, Narasimhan, and Boyd 2020) provide access to
multiple conic solvers. The CVXR package provides a domain-speciﬁc language (DSL) which
allows to formulate the optimization problems close to their mathematical formulation. To
accomplish this, it ﬁrst translates the problem into an abstract syntax tree and uses disciplined
convex programming (DCP) (Grant, Boyd, and Ye 2006) to verify that the problems are
indeed convex. After the convexity is veriﬁed, the problem is transformed into its matrix
representation and dispatched to the selected solver. Here it is important to note that the
DCP rules are only suﬃcient for convexity, meaning that even if the DCP rules can not verify

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

9

Package

License
Proprietary ROI.plugin.cplex
GPL-3
ROI.plugin.ecos

Solver
CPLEX
ECOS
GUROBI Proprietary ROI.plugin.gurobi
MOSEK Proprietary ROI.plugin.mosek

Mixed

ROI.plugin.neos

Quadratic Objective LIN SOC EXP

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Table 3: Overview of the solvers implemented as plugins in ROI that can handle mixed-integer
constraints in combination with either a quadratic objective with linear constraints or a linear
objective with linear, second-order and exponential conic constraints.

convexity, the problem can still be convex. Based on these properties, from our perspective
the CVXR package is especially well-suited for users unfamiliar with conic optimization or
fast prototyping. The ROI package, on the other hand, oﬀers a uniﬁed solver access to many
solvers, with a simple modeling language designed for users already familiar with R. For the
holiglm package, we choose to rely on the infrastructure of package ROI since we already
know the convexity properties of the likelihood functions and we want to be able to control
the transformation of the likelihood into the optimization problem. Additionally, ROI has
fewer dependencies and, last but not least, the authors are familiar with package ROI.

R> library("holiglm")

4.1. Model ﬁtting

Function hglm is the main function for ﬁtting HGLMs within package holiglm.

hglm(formula, family = gaussian(), data, constraints = NULL,

weights = NULL, scaler = c("auto", "center_standardization",

"center_minmax", "standardization", "minmax", "off"),

scale_response = NULL, big_m = 100, solver = "auto", control = list(),
dry_run = FALSE)

The design of the function arguments resembles that of the stats::glm() function, with
some additional arguments.

constraints The constraints imposed on the GLM. All constraints are of class "hglmc", so

the constraints argument expects a list of "hglmc" constraints.

scaler Especially for constraints which rely on the big-M formulation, scaling is very im-
portant as it helps to make reliable choices in regard to the chosen big-M value.

scale_response Whether the response shall be standardized. Scaling the response is only

possible for family gaussian(), where the default is also to scale it.

big_m An upper bound M for the coeﬃcients, needed for the big-M constraint.

solver By default, the best available optimization solver is chosen automatically. However,
it is possible to force the use of a particular solver by providing the solver name. The
argument is then passed along to ROI::ROI_solve.

10

Holistic GLMs

control The control argument can be used to pass additional arguments along to

ROI::ROI_solve.

dry_run The dry_run argument allows to obtain the underlying optimization problem. This
can be useful if the user wants to impose additional constraints, which are currently not
implemented, directly to the ROI optimization object.

By making use of the formula system provided by the stats package, we ensure that all the
typical operations work as in stats::glm. We illustrate the usage of hglm without constraints
for the log-binomial regression model. This particular family-link combination was chosen as
a ﬁrst example as it can be shown (by simulation) that using conic programming instead of
the iteratively reweighted least squares (IRLS) employed by glm is not only more reliable but
potentially faster (Schwendinger et al. 2021).

R> data("Caesarian", package = "lbreg")
R> fit <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB,
+
+
R> fit

binomial(link = "log"), solver = "ecos",
data = Caesarian, constraints = NULL)

Call:

hglm(formula = cbind(n1, n0) ~ RISK + NPLAN + ANTIB,

binomial(link = "log"), data = Caesarian, constraints = NULL,
solver = "ecos")

Coefficients:
(Intercept)
-1.977

RISK
1.295

NPLAN
0.545

ANTIB
-2.066

Degrees of Freedom: 6 Total (i.e. Null);
Null Deviance:
Residual Deviance: 6.308

83.49

AIC: 31.49

3 Residual

Given that the "hglm" object inherits from class "glm", the output of the summary function
is similar to the one from stats::glm:

R> summary(fit)

Call:
hglm(formula = cbind(n1, n0) ~ RISK + NPLAN + ANTIB,

binomial(link = "log"), data = Caesarian, constraints = NULL,
solver = "ecos")

Deviance Residuals:

1

3
1.06853 -0.34543 -2.21588

2

4
0.20040

5
-0.26605

6
-0.15022

8
0.05687

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

11

Coefficients:

Estimate Std. Error z value Pr(>|z|)

(Intercept) -1.9774
1.2950
RISK
0.5450
NPLAN
ANTIB
-2.0659
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

-5.972 2.35e-09 ***
3.801 0.000144 ***
3.755 0.000174 ***
-7.342 2.11e-13 ***

0.3311
0.3407
0.1451
0.2814

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 83.4914 on 6 degrees of freedom
Residual deviance: 6.3079 on 3 degrees of freedom
AIC: 31.489

Number of iterations: 26

In the above, the number of iterations is the one needed by the solver to reach convergence.

Instead of straight using function hglm for estimating the model, it is also possible to take
a diﬀerent approach, especially if the modeler wants to access the underlying optimization
problem directly:

1. Generate the model with hglm_model.

2. Generate the underlying optimization problem with as.OP.

3. Fit the model with hglm_fit.

To generate the model, the function hglm_model can be used:

R> x <- model.matrix(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, data = Caesarian)
R> model <- hglm_model(x = x, y = with(Caesarian, cbind(n1, n0)),
+

binomial(link = "log"))

Alternatively, hglm with the argument dry_run set to TRUE can be used to obtain the opti-
mization model:

R> model <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, data = Caesarian,
+

binomial(link = "log"), constraints = NULL, dry_run = TRUE)

After the model is set up, the underlying optimization problem can be built by using the
generic function as.OP exported from ROI and solved using function ROI_solve in ROI.

R> op <- as.OP(model)
R> print(op)

12

Holistic GLMs

ROI Optimization Problem:

Minimize a linear objective function of length 18 with
- 18 continuous objective variables,

subject to
- 49 constraints of type conic.

|- 42 conic constraints of type 'expp'
|- 7 conic constraints of type 'nonneg'

- 18 lower and 0 upper non-standard variable bounds.

R> ROI::ROI_solve(op)

Optimal solution found.
The objective value is: 1.109144e+02

Alternatively, the model can be ﬁtted with hglm_fit. Here the solver output is already
converted into a form similar to the output of glm.fit.

R> fit <- hglm_fit(model)
R> fit

Call:

NULL

Coefficients:
(Intercept)
-1.977

RISK
1.295

NPLAN
0.545

ANTIB
-2.066

Degrees of Freedom: 6 Total (i.e. Null);
Null Deviance:
Residual Deviance: 6.308

83.49

AIC: 31.49

3 Residual

4.2. Constraints

The most distinctive feature of the holiglm package is that it allows for the usage of many
predeﬁned constraints. In Section 3 the constraints were introduced. In this section we focus
on the R usage of the constraints. Table 4 gives an overview of all the available constraints
in holiglm.

Combining constraints

In the following example, we
Constraints can be combined by the combine operator c().
ensure that at most 5 covariates are in the ﬁnal model and that the pairwise correlation of
the active variables is less than or equal to 0.8 by combining the two constraints:

R> c(k_max(5), rho_max(0.8))

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

13

Constraint
k_max(k)a

Category
global sparsity

group_sparsity(vars, k = 1L)a

group sparsity

rho_max(rho)a

group sparsity

group_inout(vars)a

group sparsity

Description
Upper bound on the number of
active covariates.
Upper bound on the number of
active covariates within a group
of covariates vars.
Upper bound on the strength of
the pairwise correlation; for pairs
which exceed this bound only one
of the two variables is allowed in
the model.
Forces all covariates within a
group are either in (all coef-
ﬁcients are non-zero) or out
(all coeﬃcients are zero) of the
model.

include(vars)a
lower(kvars)b

modeler expertise Forces the inclusion of covariates.
bounded domains Forces lower bounds on coeﬃ-

cients.

upper(kvars)b

bounded domains Forces upper bounds on coeﬃ-

linear(L, dir, rhs)b

linear constraint

group_equal(vars)b

linear constraint

sign_coherence(vars)a

sign coherence

pairwise_sign_coherence(rho)a

sign coherence

cients.
Imposes linear constraints on the
coeﬃcients or on the z variables
from the big-M constraint.
Forces all covariates in the spec-
iﬁed group to have the same co-
eﬃcient.
Ensures that the coeﬃcients of
the speciﬁed covariates have the
same sign.
Ensures that coeﬃcients of co-
that exhibit pairwise
variates
correlation more than rho (in ab-
solute value) have the same sign.

Table 4: Overview on the pre-implemented constraints. Here only the main arguments are
shown, all arguments can be found in the holiglm manual. The constraints marked with a are
of mixed-integer type, while the ones marked with b are of linear type.

List of Holistic GLM Sparsity Constraints
[[1]]
Holistic GLM Sparsity Constraint of Type 'k_max'
List of 1

$ k_max: int 5

[[2]]

14

Holistic GLMs

Holistic GLM Sparsity Constraint of Type 'rho_max'
List of 4

$ rho_max: num 0.8
$ exclude: chr "(Intercept)"
$ use
$ method : chr "pearson"

: chr "everything"

Global sparsity

The global sparsity constraint k_max can be used to enforce an upper bound on the number
of non-zero covariates contained in the model. The upper bound is enforced on the number
of columns in the model matrix without taking into account the intercept.

R> fit <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, binomial(link = "log"),
+
R> coef(fit)

constraints = k_max(2), data = Caesarian)

(Intercept)
-1.818324

RISK
1.319768

NPLAN
0.000000

ANTIB
-1.774368

As one can see by looking at the coeﬃcients, in contrast to other sparsity-inducing methods,
the coeﬃcients are not just close to zero but exactly zero if not included in the model. The
information about the active coeﬃcients can be obtained by using the active_coefficients
method.

R> active_coefficients(fit)

(Intercept)
TRUE

RISK
TRUE

NPLAN
FALSE

ANTIB
TRUE

Group sparsity

The group sparsity constraint can be used to restrict the number of covariates to be included
from a particular subgroup. The vars argument is a vector containing the names of the
variables for which the constraint should be applied. Note that the names of the variables
should correspond to the column names of the model matrix. As previously mentioned,
a possible use case for this type of constraint is the selection of the best out of diﬀerent
transformations. For illustration, we employ a Poisson regression with log link to model the
number of apprentices moving to Edinburgh from diﬀerent counties. Data is available in
package GLMsData (Dunn and Smyth 2018).

R> data("apprentice", package = "GLMsData")

We specify the following model with group constraints. For the variables distance Dist and
population Pop also the log transformations are considered for inclusion in the model.

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

15

group_sparsity(c("Pop", "log(Pop)")))

R> fo <- Apps ~ Dist + log(Dist) + Pop + log(Pop) + Urban + Locn
R> constraints <- c(group_sparsity(c("Dist", "log(Dist)")),
+
R> fit <- hglm(fo, constraints = constraints, family=poisson(),
+
R> coef(fit)

data = apprentice)

(Intercept)

Pop
7.51921885 0.00000000 -2.05542948 0.00000000

log(Dist)

Dist

log(Pop)
0.98217367

Urban

LocnWest
-0.01705048 0.50439145 -0.10157628

LocnSouth

Indeed, the logarithmically transformed variables are selected for inclusion in the model.

In-out constraints implemented in group_inout force the coeﬃcients of all variables in argu-
ment vars to be jointly zero or diﬀerent than zero. An application for this type of constraints
is dummy-encoded categorical features, where either all dummy variables should be included
in the model or none. The following Poisson regression model is restricted to include three
covariates, whereas all dummies corresponding to the variable Locn (location relative to Ed-
inburgh) should be included or excluded from the model:

R> fo <- Apps ~ log(Dist) + log(Pop) + Urban + Locn
R> constraints <- c(k_max(3), group_inout(c("LocnSouth", "LocnWest")))
R> fit <- hglm(fo, constraints = constraints,
+

family = poisson(), data = apprentice)

To restricting the pairwise collinearity, we provide the constraint rho_max which sets an upper
bound on the pairwise collinearity. For the Boston data set from the MASS package (Venables
and Ripley 2002) we specify the following linear model, where the empirical correlation of
variables tax and rad is more than 0.9:

R> data("Boston", package = "MASS")
R> fit <- hglm(medv ~ rm + rad + dis + lstat + tax + ptratio,
+
R> coef(fit)

data = Boston, constraints = rho_max(0.9))

(Intercept)

lstat
23.790456303 4.323450402 0.000000000 -0.695741701 -0.623392021

rad

dis

rm

ptratio
-0.005568159 -0.846772468

tax

The variable rad is excluded from the model.

Modeler expertise

The function include can be used to force the inclusion of a speciﬁc variable in the model.
This can be useful if a modeler is interested in setting an upper bound on the number of
active variables while ensuring that a certain variable stays in the model. For example, the
following model on the Caesarian data set will include two variables and one of them will be
NPLAN:

16

Holistic GLMs

R> fit <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, binomial(link = "log"),
+
R> coef(fit)

constraints = c(k_max(2), include("NPLAN")), data = Caesarian)

(Intercept)
-1.0056143

RISK
0.0000000

NPLAN
0.5892485

ANTIB
-1.7899819

Bounded domains

The functions lower and upper can be used to set lower and upper bounds on the value of
the coeﬃcients. This can be used, for example, to add non-negativity constraints or any other
bounds on the coeﬃcients.

R> constraints <- c(lower(c(RISK = 3, ANTIB = 1e-3)), upper(c(NPLAN = -1)))
R> fit <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, binomial(link = "log"),
+
R> coef(fit)

constraints = constraints, data = Caesarian)

(Intercept)
-3.67126

RISK
3.00000

NPLAN
-1.00000

ANTIB
0.00100

Note that all three constraints are binding.

Linear constraints

To impose linear constraints on the coeﬃcients, the function linear can be used.
In the
following example we will enforce that βRISK ≤ βANTIB in the log-binomial regression model:

R> risk_leq_antib <- c(RISK = 1, ANTIB = -1)
R> fit <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, binomial(link = "log"),
+
R> coef(fit)

constraints = linear(risk_leq_antib, "<=", 0), data = Caesarian)

(Intercept)
ANTIB
-0.95854528 -0.30446220 0.09905912 -0.30446220

NPLAN

RISK

The next example shows how to enforce that the coeﬃcients of RISK and NPLAN sum to one,
i.e. βRISK + βNPLAN = 1.

R> risk_nplan <- c(RISK = 1, NPLAN = 1)
R> fit <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, binomial(link = "log"),
+
R> coef(fit)

constraints = linear(risk_nplan, "==", 1), data = Caesarian)

(Intercept)
-1.3047691

RISK
0.6600015

NPLAN

ANTIB
0.3399985 -1.9232422

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

17

R> coef(fit)[["RISK"]] + coef(fit)[["NPLAN"]]

[1] 1

To impose both constraints at the same time, we can either combine the linear constraints or
use the matrix notation as shown below.

R> L <- rbind(c(RISK = 1, NPLAN = 0, ANTIB = -1), c(1, 1, 0))
R> fit <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, binomial(link = "log"),
+
constraints = linear(L, c("<=", "=="), c(0, 1)), data = Caesarian)
R> coef(fit)

(Intercept)

RISK
-1.8274949 -0.2560339

NPLAN
1.2560339

ANTIB
-0.2560339

A special case of linear constraint is group_equal, which ensures that the coeﬃcients of the
variables in vars are equal:

R> fit <- hglm(cbind(n1, n0) ~ RISK + NPLAN + ANTIB, binomial(link = "log"),
+
+
R> coef(fit)

constraints = group_equal(c("RISK", "NPLAN", "ANTIB")),
data = Caesarian)

(Intercept)

RISK
-0.9710750 -0.1750262

NPLAN

ANTIB
-0.1750262 -0.1750262

Finally, we showcase how linear constraints can be imposed on the z variables by setting
on_big_m = TRUE in function linear. This can be useful for modeling restrictions of the
type “if variable j is included variable k should also be included”. An application for this
type of constraints is estimating models with polynomial features, where if a higher order of
the polynomial is included in the model, it is desirable that all lower orders are also included.
The following linear model example uses data heatcap from package GLMsData where the
relation of heat capacity for a type of acid and temperature is investigated.

R> data("heatcap", package = "GLMsData")

We consider polynomial features up to degree ﬁve:

R> fo <- Cp ~ poly(Temp, 5)

The linear constraint below corresponds to the constraint

zTemp5 ≤ zTemp4 ≤ zTemp3 ≤ zTemp2 ≤ zTemp,

which ensures that a lower order polynomial feature is included if all preceding (higher order)
ones are included in the model.

18

Holistic GLMs

c(0, 0, -1, 1, 0), c(0, 0, 0, -1, 1))

R> L <- rbind(c(-1, 1, 0, 0, 0), c(0, -1, 1, 0, 0),
+
R> colnames(L) <- colnames(model.matrix(fo, data = heatcap))[-1]
R> lin_z_c <- linear(L, rep("<=", nrow(L)), rep(0, nrow(L)), on_big_m = TRUE)

When additionally setting kmax = 3, we see that all polynomial features up to degree 3 will
be included in the model.

R> constraints <- c(k_max(3), lin_z_c)
R> fit <- hglm(fo, data = heatcap, constraints = constraints)
R> coef(fit)

(Intercept) poly(Temp, 5)1 poly(Temp, 5)2 poly(Temp, 5)3
0.1405174

0.3968900

11.2755556

1.8409086
poly(Temp, 5)4 poly(Temp, 5)5
0.0000000

0.0000000

Sign coherence constraint

We estimate a Poisson model with identity link for the apprentice data where we use in-
teractions of the location with the other covariates. In this case it can be desirable that, for
each covariate, the slopes for all locations have the same sign. Note that the model includes
no intercept:

Locn : Pop + Locn : Urban

sign_coherence(c("LocnNorth:Dist", "LocnSouth:Dist", "LocnWest:Dist")),
sign_coherence(c("LocnNorth:Pop", "LocnSouth:Pop", "LocnWest:Pop")),
sign_coherence(c("LocnNorth:Urban", "LocnSouth:Urban", "LocnWest:Urban")))

R> fo <- Apps ~ 0 + Locn + Locn : Dist +
R> constraints <- c(
+
+
+
R> fit <- hglm(fo, constraints = constraints, family = poisson("identity"),
+
R> coef(fit)

data = apprentice)

LocnNorth
4.929168e-02
LocnSouth:Dist
-2.172583e-01

LocnWest
LocnSouth
1.159558e+01
-1.325961e+01
LocnNorth:Pop
LocnWest:Dist
1.458556e-01
-9.535403e-02
LocnWest:Pop LocnNorth:Urban LocnSouth:Urban
5.071724e-01
9.274395e-03
8.259307e-02

LocnNorth:Dist
-6.226539e-03
LocnSouth:Pop
1.618307e+00
LocnWest:Urban
8.709769e-07

The slope parameters for distance are all negative, while the slope parameters for population
and degree of urbanization are all positive.

Finally, we showcase on the Boston data set how we can restrict the highly correlated covari-
ates to have the same sign using pairwise_sign_coherence:

R> data("Boston", package = "MASS")
R> fit <- hglm(medv ~ rm + rad + dis + lstat + tax + ptratio,
+
R> coef(fit)

data = Boston, constraints = pairwise_sign_coherence(0.8))

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

19

(Intercept)

lstat
2.379045e+01 4.323451e+00 -3.998741e-07 -6.957417e-01 -6.233920e-01

dis

rad

rm

ptratio
-5.568140e-03 -8.467723e-01

tax

Unlike in the unrestricted model, variables rad and tax both have a negative sign. Note also
that, in this example, the coeﬃcient of rad is close to zero, which is in line with the result
obtained when using rho_max constraint.

4.3. Auxiliary functions

Estimate a sequence of models

Function hglm_seq can be used to estimate the sequence of models containing k = 1, . . . , p
covariates. The print method for the resulting "hglm_seq" object includes the AIC, BIC
as well as the vector of coeﬃcients for each model. For the polynomial regression example
introduced before, we can estimate the sequence of models:

R> fit_seq <- hglm_seq(formula = Cp ~ poly(Temp, 5), data = heatcap,
+
R> fit_seq

constraints = lin_z_c)

HGLM Fit Sequence:

k_max

aic

5 -64.52 -58.29
4 -65.60 -60.26
3 -64.04 -59.59
2 -52.30 -48.74
1 -24.42 -21.75

bic (Intercept) poly(Temp, 5)1 poly(Temp, 5)2
0.39689
0.39689
0.39689
0.39689
0.00000

11.27556
11.27556
11.27556
11.27556
11.27556

1.840909
1.840909
1.840909
1.840909
1.840909

poly(Temp, 5)3 poly(Temp, 5)4 poly(Temp, 5)5
0.02653116
0.00000000
0.00000000
0.00000000
0.00000000

-0.05560884
-0.05560884
0.00000000
0.00000000
0.00000000

0.1405174
0.1405174
0.1405174
0.0000000
0.0000000

1
2
3
4
5

1
2
3
4
5

We can observe that the model with polynomial features up to degree 4 is the one preferred
by both AIC and BIC.

Group duplicates

For binomial regression models, if all the covariates are categorical or discrete variables with
few possible values, it is often the case that the model matrix contains duplicated rows. In
this case, it is convenient to aggregate the original data to a data frame containing the unique
values of the covariates in the rows and for each row the number of successes and failures
corresponding to the response. Function agg_binomial can be used for this purpose to reduce
the number of rows from originally 16949 to 74:

20

Holistic GLMs

R> data("Heart", package = "lbreg")
R> heart <- agg_binomial(Heart ~ ., data = Heart, as_list = FALSE)
R> c(nrow(Heart), nrow(heart))

[1] 16949

74

This function should always be used when possible, as it drastically reduces the size of the
underlying optimization problem and therefore, reduces the solving time. The new function
call would be:

R> hglm(cbind(success, failure) ~ ., binomial(link = "log"),
+

data = heart, constraints = NULL)

5. Use cases

This section illustrates the usage of the holiglm package on more realistic real-world examples.

5.1. Fairness in logistic regression

We illustrate how the holiglm package can be employed for estimating the coeﬃcients of
a logistic regression model with the fairness constraints proposed in Zafar, Valera, Gomez-
In the fairness literature, one is typically interested in
Rodriguez, and Gummadi (2019).
building classiﬁcation models free of the so-called disparate impact. A decision-making process
suﬀers from disparate impact if it grants disproportionately large fraction of beneﬁcial (or
positive) outcomes to certain groups of a sensitive feature, such as gender or race. Assuming
we have a binary response variable y and a binary sensitive feature w, a binary classiﬁer is free
of disparate impact if the probability that the classiﬁer assigns an observation to the positive
class is the same for both values of the sensitive feature w: P(ˆy = 1|w = 0) = P(ˆy = 1|w = 1).
If w has more than two classes, this relation should hold for all values of w. Note that
even if the sensitive variable w is not included in the model, the classiﬁer can still suﬀer
from disparate impact (mainly due to the association between the non-sensitive variables x
included in the model and the sensitive variable).

However, for many classiﬁers these probabilities are typically nonconvex functions of the
parameters, which complicates estimation. To provide a more general framework, Zafar et al.
(2019) propose a covariance measure of decision boundary unfairness. This is given by the
covariance between a sensitive binary attribute w and the signed distance from the (non-
sensitive) feature vectors xi to the decision boundary:

COV(w, dβ(x)) = E((w − ¯w)dβ(x)) − E((w − ¯w))E(dβ(x))
}

|

{z
=0

≈

1

n

n
X

(wi − ¯w)dβ(xi).

(4)

i=1

The framework can also accommodate for various binary sensitive variables w(k), k = 1, . . . , K,
where limits are imposed for the above covariance measure for each of the k sensitive variables.
For the logistic regression problem, where the signed distance to the decision boundary is

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

21

proportional to x>

i β, the fairness constrained model is given by:

maximize
β
subject to

Pn

i=1

log(p(yi|xi, β))

1
n
1
n
...
1
n
1
n

Pn

Pn

i=1

i=1

Pn

Pn

i=1

i=1

(w(1)
(w(1)

i − ¯w(1))x>
i − ¯w(1))x>

i β ≤ c1
i β ≥ −c1,

(w(K)
(w(K)

i − ¯w(K))x>
i − ¯w(K))x>

i β ≤ cK
i β ≥ −cK,

(5)

where ck ∈ R+ for k = 1, . . . , K is a given constant which trades oﬀ accuracy and decision
boundary unfairness.
As an illustrative example we employ the AdultUCI data set in package arules (Hahsler,
Buchta, Grün, and Hornik 2021), which contains data extracted from the 1994 U.S. census
bureau database. The response variable in this data set is the binary variable income which
indicates whether the income of the respondents exceeds USD 50 000 per year.

R> data("AdultUCI", package = "arules")

Before estimation, we eliminate all observations with missing values:

R> AdultUCI <- na.omit(AdultUCI)
R> dim(AdultUCI)

[1] 30162

15

The data is rather unbalanced in terms of the distribution of the response:

R> prop.table(table(AdultUCI$income))

small

large
0.7510775 0.2489225

Moreover, this data set is known to be skewed in terms of race and gender.

R> xtabs(~ sex + race, data = AdultUCI)

race

sex

Female
Male

Amer-Indian-Eskimo Asian-Pac-Islander Black Other White
7895
144 18038

1399
1418

294
601

107
179

87

Given the sparsely populated race groups Amer-Indian-Eskimo, Asian-Pac-Islander and
Other, we merge them into the Other group.

R> levels(AdultUCI$race)[c(1, 2)] <- "Other"

22

Holistic GLMs

Clear diﬀerences are observable in the percentage of high income earners in the sex and race
groups:

R> aggregate(income ~ sex + race, data = AdultUCI,
+

function(x) mean(as.numeric(x) - 1))

sex race

income
1 Female Other 0.11475410
2
Male Other 0.26731602
3 Female Black 0.06075768
4
Male Black 0.19816643
5 Female White 0.12298923
Male White 0.32531323
6

We will therefore consider both race and gender as sensitive variables in our use case. We
create one w(k) binary (i.e., dummy) variable for each factor level combination.

R> W <- model.matrix(~ 0 + sex:race, data = AdultUCI)

We consider the following predictive model for the analysis (note that the sensitive variables
are not included in the model formula):

R> form <- "income ~ age + relationship + `marital-status` + workclass +
`education-num` + occupation + I(`capital-gain` - `capital-loss`) +
+
`hours-per-week`"
+

The variable native-country is not included in the model as it was eliminated in a prelimi-
nary stepwise selection procedure.

In order to assess the trade-oﬀ between fairness and prediction accuracy, we follow Zafar
et al. (2019) and choose a grid of values αk ∈ [0, 1] which are used to compute ck = αks(k).
Here s(k) is the empirical covariance (in absolute value) between the sensitive variable w(k)
ˆβ from the unconstrained regression; s(k) serves in this case as an
and the linear predictor x>
i
upper bound on the disparate impact covariance measure. A value αk = 0 represents the case
where the empirical covariance in Equation 4 is restricted to zero while αk = 1 represents
the case with no fairness constraints. We ﬁrst estimate the unconstrained model using the
stats::glm function and the calculate s(k):

R> m0 <- glm(as.formula(form), family = binomial(), data = AdultUCI)
R> s <- apply(W, 2, function(w) abs(cov(w, m0$linear.predictors)))

The left-hand side of the constraints from Equation 5 can be speciﬁed in matrix form Lβ
where L is constructed by:

R> xm <- model.matrix(m0)
R> L <- t(apply(W, 2, function(w) colMeans((w - mean(w)) * xm)))

For a grid of αk values, we estimate the fairness constrained logistic regressions. We set
the big_m argument to 5, which means that the standardized coeﬃcients (see scaler =

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

23

"standardization") are constrained to be at most 5 in absolute value. A value of 5 for
big_m is small enough to ensure a fast convergence of the algorithm and, at the same time,
suﬃciently large to be non-restrictive for the problem at hand (in none of the estimated
models this bound was reached). For each model we store the in-sample predictions, which
will be used for assessing the accuracy versus fairness trade-oﬀ.

ck <- ak * s
model_constrained <- hglm(as.formula(form),

R> library("ROI.plugin.ecos")
R> library("holiglm")
R> K <- nrow(L)
R> alpha <- seq(0, 1, by = 0.05)
R> pred_prob_race_sex <- sapply(alpha, function(ak) {
+
+
+
+
+
+
+
+
+

family = binomial(), data = AdultUCI,
scaler = "standardization", big_m = 5,
constraints = c(linear(L, rep(">=", K), - ck),

phat <- predict(model_constrained, type = "response")
phat

linear(L, rep("<=", K),

ck)))

})

For each model we compute the in-sample accuracy and area under the ROC curve (using
package pROC, Robin, Turck, Hainard, Tiberti, Lisacek, Sanchez, and Müller 2011). Note
that we consider a cut-oﬀ probability threshold of 0.5, i.e., a label of ˆy = 1 will be assigned if
the predicted probability exceeds 50%. To assess the disparate impact of the diﬀerent models
we use the following measure:

DI = P(ˆy = 1|w(k) = 0)
P(ˆy = 1|w(k) = 1)

.

We only consider the dummy variable w(k) which corresponds to the White:Male factor
combination when computing DI but other approaches which take into account all w(k)’s are
available. The DI measure considers the White:Male as the privileged group in this data set
and a value less than one indicates that, in the prediction, the privileged group is favored.
As a rule of thumb, values greater than 0.8 can be considered acceptable. 1

R> obs <- AdultUCI$income
R> res <- apply(pred_prob_race_sex, 2, function(phat) {
+
+
+
+
+
+
+

tab <- table(phat > 0.5, obs)
tab_w <- table(phat > 0.5, W[, "sexMale:raceWhite"])
ptab_w <- prop.table(tab_w)
DI <- ptab_w[2, 1]/ptab_w[2, 2]
acc <- sum(diag(tab))/sum(tab)
c(DI = DI, acc = acc, auc = pROC::roc(obs, phat, quiet = TRUE)$auc)

})

We create the scatterplots of the three measures for the grid of α values:

1See also e.g., guidelines of the US Equal Employment Opportunity Commission regarding hiring practices.

24

Holistic GLMs

Figure 1: This ﬁgure displays the disparate impact, accuracy and area under the ROC curve
for diﬀerent degrees of fairness given by α ∈ [0, 1], where an α of zero implies a fair model in
terms of the speciﬁed covariance measure.

xlab = expression(alpha), ylab = "Disparate impact")

R> par(mfrow = c(1, 3))
R> plot(alpha, res[1,], type = "b",
+
R> plot(alpha, res[2,], type = "b",
+
R> plot(alpha, res[3,], type = "b",
+

xlab = expression(alpha), ylab = "Accuracy")

xlab = expression(alpha), ylab = "AUROC")

The resulting plot can be seen in Figure 1. We observe that the model with α = 0 has a DI
measure of around 0.6, so it still exhibits disparate impact in favor of the privileged group,
even if the covariance measure is restricted to zero in the model building phase. To get a
more accurate picture, one could repeat the analysis by computing the above measures on
a test set. The cut-oﬀ values for the prediction can also be chosen diﬀerently. Finally, one
can see that the trade-oﬀ between fairness and predictive power is not very high, as the loss
in the predictive measures is not dramatic as αk decreases. Such trade-oﬀ analysis can help
the modelers and decision-makers in choosing a setting which is acceptable for the use-case
at hand.

5.2. Model selection in log-binomial regression

In the following example we show how the holiglm package can be used for model selection
for log-binomial regression. For this example we use the icu data set from the aplore3
package (Braglia 2016). In this application, the goal is to model the status of patients at
hospital discharge (i.e. alive versus dead) using a series of patient-speciﬁc covariates.

R> data("icu", package = "aplore3")
R> dim(icu)

0.00.20.40.60.81.00.250.300.350.400.450.500.550.60aDisparate impact0.00.20.40.60.81.00.810.820.830.84aAccuracy0.00.20.40.60.81.00.840.850.860.870.880.890.90aAUROCBenjamin Schwendinger, Florian Schwendinger, Laura Vana

25

[1] 200 21

First, we will check if the MLE exists by making use of the detectseparation package. For
log-binomial regression the existence criterion is slightly diﬀerent from the existence criterion
for logistic regression, for more information we refer to Schwendinger et al. (2021).

R> library("detectseparation")
R> icu <- icu[, setdiff(colnames(icu), "id")]
R> glm(sta ~ ., family = binomial("log"), data = icu,
+

method = "detect_separation")

Data separation in log-binomial models does not necessarily result in infinite

estimates

Implementation: ROI | Solver: lpsolve
Separation: TRUE
Existence of maximum likelihood estimates
genderFemale
0
crnYes
0

(Intercept)
0
serSurgical
0
sys
0
po2<= 60
0
locStupor
0

age
0
canYes
0
hra
0
ph< 7.25
0
locComa
0

raceBlack
0
infYes
0
preYes typeEmergency
0
bic< 18
0

0
pco> 45
0

raceOther
0
cprYes
0
fraYes
0
cre> 2.0
0

0: finite value, Inf: infinity, -Inf: -infinity

Package detectseparation signals that the data has the separation property; however, the
estimates are still all expected to be ﬁnite for the log-binomial regression. Note that for
logistic regression the MLE does not exist (i.e. has inﬁnite components). To verify this, again
detectseparation can be used.

R> glm(sta ~ ., family = binomial("logit"), data = icu,
+

method = "detect_separation")

Implementation: ROI | Solver: lpsolve
Separation: TRUE
Existence of maximum likelihood estimates
age genderFemale
0
crnYes
0

(Intercept)
0
serSurgical
0
sys
0

0
canYes
0
hra
0

raceBlack
-Inf
infYes
0
preYes typeEmergency
0

0

raceOther
0
cprYes
0
fraYes
0

26

Holistic GLMs

Figure 2: AIC, BIC and relative risks (exp βj) for the icu data from the log-binomial re-
gression with constraints on the maximum number of covariates k. RR0 corresponds to the
baseline relative risk exp β0.

po2<= 60
0
locStupor
Inf

ph< 7.25
0
locComa
0

pco> 45
0

bic< 18
0

cre> 2.0
0

0: finite value, Inf: infinity, -Inf: -infinity

For the purpose of model selection, we estimate a sequence of models of diﬀerent size k =
1, . . . , 21 using function hglm_seq. In Figure 2 we present, for each model size k, the AIC and
BIC of the optimized model together with the selected coeﬃcients (which can be interpreted
in terms of relative risks for the log-binomial regression).

R> fits <- hglm_seq(formula = sta ~ ., family = binomial("log"),
+

data = icu, solver = "mosek")

In terms of AIC, models of size 7-9 are preferred, while the size suggested by BIC is around
3-5. Looking at the estimated set of coeﬃcients for each k can provide additional insights
into the model behavior.

For example, the modeler can look at the stability of the magnitude of the coeﬃcients and
how they change when the number of covariates decreases.

6. Conclusion

The holiglm package can be used to ﬁt generalized linear models which are optimal with
respect to a set of speciﬁed constraints. The package provides diﬀerent types of constraints
i) diﬀerent requirements for
which are relevant for statistical modeling purposes such as:
sparsity (e.g.
setting an upper limit on the number of covariates included in the model
globally or from a group of pre-speciﬁed variables, forcing a group of covariates to jointly be
included or excluded from the model), ii) linear constraints, iii) constraints on the pairwise
correlation among covariates, iv) constraints on the sign of the coeﬃcients. The interface
of the package is very similar to the one provided by stats::glm, making it accessible for
users to specify the desired GLMs. Using the fact that the most common GLMs can be
represented as conic programs, holiglm leverages the tremendous advancements in the ﬁeld
of conic programming to provide eﬃcient estimation of constrained GLMs. This also means

kaicbicRR0agegenderFemaleraceBlackraceOtherserSurgicalcanYescrnYesinfYescprYessyshrapreYestypeEmergencyfraYespo2≤60ph<7.25pco>45bic<18cre>2.0locStuporlocComa21172.73245.290.001.030.680.531.400.935.881.040.991.531.001.002.208.711.671.382.520.371.260.764.404.8320170.73239.990.001.030.690.531.410.935.871.041.521.001.002.198.691.671.382.530.371.250.764.364.8319168.74234.710.001.030.680.541.440.935.801.481.001.002.168.711.691.372.610.371.230.774.334.9618166.78229.450.001.030.690.561.345.901.481.001.002.139.151.591.402.700.361.240.774.465.1117164.90224.270.001.030.660.561.386.001.541.002.069.521.461.192.440.401.280.814.094.6416163.11219.180.001.030.690.551.495.711.382.028.901.381.112.370.421.320.904.264.9915161.23214.000.001.030.670.551.516.171.391.999.401.392.240.431.400.914.154.8914159.69209.160.001.030.671.546.301.421.939.551.422.360.431.420.894.194.9913158.26204.440.001.020.640.511.455.411.342.129.031.342.810.434.134.7912156.80199.680.001.020.660.495.321.322.149.041.322.720.434.114.7311155.49195.070.001.030.655.491.322.079.231.352.930.424.164.8410154.42190.700.001.020.665.421.312.099.332.830.424.224.849153.56186.540.001.020.875.291.978.903.300.384.625.298153.32183.010.011.024.721.678.433.110.394.724.727153.99180.370.011.024.248.402.740.404.244.246155.03178.120.011.024.088.970.524.084.085157.53177.310.023.838.060.545.555.554160.47176.960.023.377.755.744.593163.17176.360.045.035.284.222169.78179.680.156.855.481186.29192.890.174.75Benjamin Schwendinger, Florian Schwendinger, Laura Vana

27

that the package will get faster when faster algorithms for conic optimization will become
available. The package relies on ROI as the underlying infrastructure. Through the ROI
plugins, problems with conic objectives and conic- and mixed-integer constraints such as the
ones implemented in holiglm can reliably be computed within R. Package holiglm provides
an automatic way of estimating the model parameters subject to the constraints speciﬁed by
the modeler. As with any automatic approach to statistical modeling, we recommend to use
it complementary to the modeler’s expertise as an exploratory tool which provides guidance
on the structure of the ﬁnal model.

References

Bertsimas D, King A (2015). “An Algorithmic Approach to Linear Regression.” Operations

Research, 64(1), 2–16. doi:10.1287/opre.2015.1436.

Bertsimas D, Li ML (2020). “Scalable Holistic Linear Regression.” Operations Research

Letters, 48(3), 203–208. doi:10.1016/j.orl.2020.02.008.

Boutaris T (2017). colf: Constrained Optimization on Linear Function. R package version

0.1.3, URL https://CRAN.R-project.org/package=colf.

Boyd S, Vandenberghe L (2004). Convex Optimization. Cambridge University Press, New

York, NY, USA. ISBN 0521833787.

Braglia L (2016). aplore3: Datasets From Hosmer, Lemeshow and Sturdivant, “Applied Lo-
gistic Regression” (3rd Ed., 2013). R package version 0.9, URL https://CRAN.R-project.
org/package=aplore3.

Carrizosa E, Olivares-Nadal AV, Ramírez-Cobo P (2020). “Integer Constraints For Enhancing
Interpretability in Linear Regression.” SORT-Statistics and Operations Research Transac-
tions, pp. 67–98. doi:10.2436/20.8080.02.95.

Chaudhuri S, Handcock MS, Rendall MS (2006). glmc: An R Package for Generalized Lin-
ear Models Subject to Constraints. URL http://www.stat.washington.edu/handcock/
combining.

Dormann CF, Elith J, Bacher S, Buchmann C, Carl G, Carré G, Marquéz JRG, Gruber
B, Lafourcade B, Leitão PJ, et al. (2013). “Collinearity: A Review of Methods To Deal
With It and a Simulation Study Evaluating Their Performance.” Ecography, 36(1), 27–46.
doi:10.1111/j.1600-0587.2012.07348.x.

Dunn PK, Smyth GK (2018). GLMsData: Generalized Linear Model Data Sets. R package

version 1.0.0, URL https://CRAN.R-project.org/package=GLMsData.

Fu A, Narasimhan B, Boyd S (2020). “CVXR: An R Package for Disciplined Convex Opti-
mization.” Journal of Statistical Software, 94(14), 1–34. doi:10.18637/jss.v094.i14.

Grant M, Boyd S, Ye Y (2006). Disciplined Convex Programming, chapter 7, pp. 155–210.
Springer-Verlag, Boston, MA. ISBN 978-0-387-30528-8. doi:10.1007/0-387-30528-9_7.

28

Holistic GLMs

Hahs-Vaughn DL (2016). Applied Multivariate Statistical Concepts. Routledge.

Hahsler M, Buchta C, Grün B, Hornik K (2021). arules: Mining Association Rules and
Frequent Itemsets. R package version 1.7-2, URL https://CRAN.R-project.org/package=
arules.

Helwig NE (2018). CMLS: Constrained Multivariate Least Squares. R package version 1.0-0,

URL https://CRAN.R-project.org/package=CMLS.

Hoerl AE, Kennard RW (1970). “Ridge Regression: Biased Estimation For Nonorthogonal

Problems.” Technometrics, 12(1), 55–67. doi:10.1080/00401706.1970.10488634.

Kosmidis I, Schumacher D, Schwendinger F (2022). detectseparation: Detect and Check for
Separation and Inﬁnite Maximum Likelihood Estimates. R package version 0.2.999, URL
https://github.com/ikosmidis/detectseparation.

Lawson CL, Hanson RJ (1995). Solving Least Squares Problems. SIAM.

McDonald JW, Diamond ID (1990). “On the Fitting of Generalized Linear Models with Non-
negativity Parameter Constraints.” Biometrics, 46(1), 201–206. doi:10.2307/2531643.

Miller A (2002). Subset Selection in Regression. CRC Press. doi:10.1201/9781420035933.

Page E (1977). “Approximations to the Cumulative Normal Function and Its Inverse for
Use on a Pocket Calculator.” Journal of the Royal Statistical Society. Series C (Applied
Statistics), 26(1), 75–76. doi:10.2307/2346872.

R Core Team (2022). R: A Language and Environment for Statistical Computing. R Founda-

tion for Statistical Computing, Vienna, Austria. URL http://www.R-project.org.

Robin X, Turck N, Hainard A, Tiberti N, Lisacek F, Sanchez JC, Müller M (2011). “pROC:
An Open-Source Package for R and S+ To Analyze and Compare ROC Curves.” BMC
Bioinformatics, 12, 77. doi:10.1186/1471-2105-12-77.

Sallés JP (2020). ConsReg: Fits Regression & ARMA Models Subject to Constraints to
the Coeﬃcient. R package version 0.1.0, URL https://CRAN.R-project.org/package=
ConsReg.

Schoenberg R (1997). “Constrained Maximum Likelihood.” Computational Economics, 10(3),

251–266. doi:10.1023/a:1008669208700.

Schwendinger F, Grün B, Hornik K (2021). “A Comparison of Optimization Solvers for
Log Binomial Regression Including Conic Programming.” Computational Statistics, 36(3),
1721–1754. doi:10.1007/s00180-021-01084-5.

Slawski M, Hein M (2013). “Non-Negative Least Squares for High-Dimensional Linear Models:
Consistency and Sparse Recovery Without Regularization.” Electronic Journal of Statistics,
7, 3004 – 3056. doi:10.1214/13-ejs868.

Theußl S, Schwendinger F, Hornik K (2020). “ROI: An Extensible R Optimization Infras-

tructure.” Journal of Statistical Software, 94. doi:10.18637/jss.v094.i15.

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

29

Tibshirani R (1996). “Regression Shrinkage and Selection Via the LASSO.” Journal of the
Royal Statistical Society B, 58(1), 267–288. doi:10.1111/j.2517-6161.1996.tb02080.x.

Turlach BA, Weingessel A, Moler C (2019). quadprog: Functions to Solve Quadratic Program-
ming Problems. R package version 1.5-8, URL https://CRAN.R-project.org/package=
quadprog.

Vanbrabant L (2022). restriktor: Restricted Statistical Estimation and Inference for Lin-
ear Models. R package version 0.3-400, URL https://CRAN.R-project.org/package=
restriktor.

Venables WN, Ripley BD (2002). Modern Applied Statistics with S. 4th edition. Springer-
Verlag, New York. ISBN 0-387-95457-0, URL https://www.stats.ox.ac.uk/pub/MASS4/.

Zafar MB, Valera I, Gomez-Rodriguez M, Gummadi KP (2019). “Fairness Constraints: A
Flexible Approach for Fair Classiﬁcation.” Journal of Machine Learning Research, 20(75),
1–42. ISSN 1533-7928. URL http://jmlr.org/papers/v20/18-262.html.

30

Holistic GLMs

A. Conic formulations

In this section we reformulate log-likelihood of the diﬀerent family-link combinations in terms
of Equation 1. Let x>
The family-link combinations implemented in holiglm can be expressed by the second-order
cone:

i β = ηi.

and the primal exponential cone:

Kn
soc

= {(t, x) ∈ Rn|x ∈ Rn−1, t ∈ R, kxk2 ≤ t}

Kexp = {(x, y, z) ∈ R3|y > 0, ye

x

y ≤ z} ∪ {(x, 0, z) ∈ R3|x ≤ 0, z ≥ 0}.

A.1. Gaussian

For the Gaussian family the log-likelihood is proportional to:

log L(β; y1, . . . , yn) ∝

∝

n
X

i=1
n
X

i=1

(yi − g−1(ηi))2
2

−

i − 2yig−1(ηi) + g−1(ηi)2
y2
2

−

∝

n
X

i=1

−

yi g−1(ηi)
| {z }
λi(β)

g−1(ηi)2
2
{z
b(λi(β))

|

}

Identity link
For the identity link we have g−1(ηi) = ηi so

Using an auxiliary variable ζ, the inequality ζ ≥ z2 can be expressed as a second-order cone:

λi(β) = x>

i β,

b(λi(β)) = (x>

i β)2/2.

since

ζ ≥ z2 ⇐⇒ (ζ + 1, ζ − 1, 2z) ∈ K3

soc,

soc ⇐⇒ k(ζ − 1, 2z)k2 ≤ ζ + 1
and by using the deﬁnition of the norm and squaring both sides we get

(ζ + 1, ζ − 1, 2z) ∈ K3

(ζ − 1)2 + (2z)2 ≤ (ζ + 1)2 ⇐⇒ 4z2 ≤ 4ζ.

Using the same technique on z ∈ Rn leads to

ζ ≥

n
X

i=1

i ⇐⇒ (ζ + 1, ζ − 1, 2z1, . . . , 2zn) ∈ Kn+2
z2
soc .

Since max Pn
i=1 yix>
by dropping the constant term Pn

i β − (x>

i β)2
2

can be rewritten into max − Pn

(yi − x>
we obtain the following reformulation:

i=1

1
2

i β)2 − y2

i
2

y2
i
2

i=1

maximize
β,ζ

− ζ

subject to (ζ + 1, ζ − 1, 2(y1 − x>
β ∈ Rp, ζ ∈ R.

1 β), . . . , 2(yn − x>

n β)) ∈ Kn+2
soc

(6)

, and

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

31

A.2. Binomial

For the binomial family the log-likelihood is proportional to:

log L(β; y1, . . . , yn) ∝

∝

n
X

i=1
n
X

i=1

yi log(g−1(ηi)) + (1 − yi) log(1 − g−1(ηi))

yi (log(g−1(ηi)) − log(1 − g−1(ηi)))
{z
}
λi(β)

|

− (− log(1 − g−1(ηi)))
{z
}
b(λi(β))

|

Logit link

For the logit link we have:

λi(β) = log(g−1(ηi)) − log(1 − g−1(ηi)))

= log

(cid:18) exp(ηi)

(cid:19)

1 + exp(ηi)

− log

(cid:18)

1 −

(cid:19)

exp(ηi)
1 + exp(ηi)

= ηi

b(λi(β)) = − log(1 − g−1(ηi)))
exp(ηi)
1 + exp(ηi)

= − log

1 −

(cid:18)

(cid:19)

= log (1 + exp(ηi))

Employing an auxiliary variable δ, the inequality log(1 + exp(x)) can be represented using
the exponential cone. For introducing the representation, note that the exponential function
can be modeled by an exponential cone:

δ ≥ exp(x) ⇐⇒ (x, 1, δ) ∈ Kexp.

Now, the inequality

δ ≤ log(1 + exp(x)) ⇐⇒ exp(δ) ≤ 1 + exp(x)

(7)

(8)

can be reformulated as:

1 + γ ≥ exp(δ) ⇐⇒ (δ, 1, 1 + γ) ∈ Kexp
γ ≥ exp(x) ⇐⇒ (x, 1, γ) ∈ Kexp

The objective is therefore reformulated as:

maximize
β,δ,γ

n
X

i=1

yix>

i β − δi

subject to (δi, 1, 1 + γi) ∈ Kexp for all i ∈ {1, . . . , n}
i β, 1, γi) ∈ Kexp for all i ∈ {1, . . . , n}

(x>
β ∈ Rp, δ ∈ Rn, γ ∈ Rn.

32

Log link

For the log link we have:

Holistic GLMs

λi(β) = log(g−1(ηi)) − log(1 − g−1(ηi)))

= log (exp(ηi)) − log (1 − exp(ηi)) = ηi − log (1 − exp(ηi))

b(λi(β)) = − log(1 − g−1(ηi))) = − log (1 − exp(ηi))

Similar as before, the inequality δ ≤ log(1 − exp(x)) can be represented using the exponential
cone. Now

δ ≤ log(1 − exp(x)) ⇐⇒ exp(δ) ≤ 1 − exp(x)

can be expressed as (using Equation 7):

1 − γ ≥ exp(δ) ⇐⇒ (δ, 1, 1 − γ) ∈ Kexp
γ ≥ exp(x) ⇐⇒ (x, 1, γ) ∈ Kexp.

The objective is therefore reformulated as:

maximize
β,δ,γ

n
X

yix>

i β + (1 − yi)δi

i=1
subject to x>
i β ≤ 0 for all i ∈ {1, . . . , n}
(δi, 1, 1 − γi) ∈ Kexp for all i ∈ {1, . . . , n}
(x>
i β, 1, γi) ∈ Kexp for all i ∈ {1, . . . , n}
β ∈ Rp, δ ∈ Rn, γ ∈ Rn.

A.3. Poisson

For Poisson family the log-likelihood is proportional to:

log L(β; y1, . . . , yn) ∝

n
X

i=1

yi log(g−1(ηi))
{z
}
λi(β)

|

− g−1(ηi)
| {z }
b(λi(β))

Log link

For the log link we have:

Using Equation 7, the objective can be reformulated as:

λi(β) = ηi,

b(λi(β)) = exp(ηi).

n
X

yix>

i β − δi

maximize
β,δ

i=1
subject to (x>

i β, 1, δi) ∈ Kexp for all i ∈ {1, . . . , n}

β ∈ Rp, δ ∈ Rn.

Benjamin Schwendinger, Florian Schwendinger, Laura Vana

33

Identity link

For the identity link we have:

λi(β) = log(ηi),

b(λi(β)) = ηi.

Imposing ηi ≥ 0 for all i = 1, . . . , n and using Equation 7, the objective can be reformulated
as:

maximize
β,δ

n
X

i=1

yiδi − x>

i β

subject to (δi, 1, x>

i β) ∈ Kexp for all i ∈ {1, . . . , n}

x>
i β ≥ 0 for all i ∈ {1, . . . , n}
β ∈ Rp, δ ∈ Rn.

Square root link

For the square root link we have:

Using Equations 6 and 7, the objective can be reformulated as:

λi(β) = 2 log(ηi),

b(λi(β)) = η2
i

maximize
β,δ,ζ

n
X

i=1

2yiδi − ζ

subject to (δi, 1, x>

i β) ∈ Kexp for all i ∈ {1, . . . , n}
n β) ∈ Kn+2
1 β, . . . , 2x>
soc

(ζ + 1, ζ − 1, 2x>
β ∈ Rp, δ ∈ Rn, ζ ∈ R.

Aﬃliation:
Benjamin Schwendinger
Institute of Computer Technology
TU Wien
Gußhausstraße 27-29
1040 Vienna, Austria
E-mail: benjamin.schwendinger@hotmail.com

Florian Schwendinger
Department of Statistics
University of Klagenfurt
Universitätsstraße 65-67
9020 Klagenfurt, Austria
E-mail: FlorianSchwendinger@gmx.at

34

Holistic GLMs

Laura Vana
CSTAT – Computational Statistics
Institute of Statistics and Mathematical Methods in Economics
TU Wien
Wiedner Hauptstraße 7
1040 Vienna, Austria
E-mail: laura.vana.guer@tuwien.ac.at

