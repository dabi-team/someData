MAGPIE: MACHINE AUTOMATED GENERAL PERFORMANCE
IMPROVEMENT VIA EVOLUTION OF SOFTWARE

2
2
0
2

g
u
A
4

]
E
S
.
s
c
[

1
v
1
1
8
2
0
.
8
0
2
2
:
v
i
X
r
a

Aymeric Blot
Department of Computer Science
University College London
London WC1E 6BT, U.K.
a.blot@cs.ucl.ac.uk

Justyna Petke
Department of Computer Science
University College London
London WC1E 6BT, U.K.
j.petke@ucl.ac.uk

ABSTRACT

Performance is one of the most important qualities of software. Several techniques have thus been
proposed to improve it, such as program transformations, optimisation of software parameters, or
compiler ﬂags. Many automated software improvement approaches use similar search strategies to
explore the space of possible improvements, yet available tooling only focuses on one approach at a
time. This makes comparisons and exploration of interactions of the various types of improvement
impractical.
We propose MAGPIE, a uniﬁed software improvement framework. It provides a common edit se-
quence based representation that isolates the search process from the speciﬁc improvement tech-
nique, enabling a much simpliﬁed synergistic workﬂow. We provide a case study using a basic
local search to compare compiler optimisation, algorithm conﬁguration, and genetic improvement.
We chose running time as our efﬁciency measure and evaluated our approach on four real-world
software, written in C, C++, and Java.
Our results show that, used independently, all techniques ﬁnd signiﬁcant running time improve-
ments: up to 25% for compiler optimisation, 97% for algorithm conﬁguration, and 61% for evolving
source code using genetic improvement. We also show that up to 10% further increase in perfor-
mance can be obtained with partial combinations of the variants found by the different techniques.
Furthermore, the common representation also enables simultaneous exploration of all techniques,
providing a competitive alternative to using each technique individually.

Keywords parameter tuning, algorithm conﬁguration, genetic improvement, compiler optimisation.

1

Introduction

Software is never done [49]. It needs to be continuously improved. Hidden bugs, misguided assumptions, outdated
speciﬁcations, code smells, technical debt are all examples of opportunities for software improvement. Bug-free
software is hard to write, and fast, memory and energy-efﬁcient software even more so. With software present in
almost all aspects of our lives its performance has become a key priority. More than ever, software needs to be faster,
more reactive, to be less of a drain on mobile batteries, to use less bandwidth, etc.

There are many ways software performance can be improved. First of all, programming languages are deﬁned, and
shipped with a default compiler or interpreter implementing that language. Alternative compilers or interpreters will
often provide different standard-compliant implementations that may lead to performance changes for a given soft-
ware. They may also expose optimisation related parameters (e.g., the well known -O3 option of C compilers, or
garbage collection parameters) that can be speciﬁcally tuned to improve execution time [25]. Users can also tune the
parameters of the given software itself [48], as software developers will often expose options to allow them to choose
different features to be run. For instance, users might specify a particular output format, a different search algorithm,
or a strategy that favours fast but suboptimal results. Finally, it has been shown that in many cases [55] software can
be improved beyond conﬁguring their exposed parameters, by directly modifying their source code.

 
 
 
 
 
 
MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

Automated tooling has been proposed to help developers in automating the software optimisation process. Exam-
ples include the COLE [33] framework that uses a multi-objective genetic algorithm for GCC compiler optimisation;
ParamILS [37, 36] that utilises local search to navigate the space of algorithm conﬁgurations; and Gin [16], a genetic
improvement framework that applies similar search strategies to explore the space of software mutations, with the aim
of ﬁnding improved software variants.

Albeit originating from completely different ﬁelds, compiler optimisation, algorithm conﬁguration, and genetic im-
provement have all been using similar search strategies, yet have never been compared or combined. It is unclear
whether certain compiler options that improve runtime execution of a program, for instance, would retain the per-
formance gain under different software conﬁguration, that on its own could boost software execution time. It is also
unclear whether such changes are synergistic or not. Finally, to this point there is no evidence that would support
concurrent exploration of the search spaces of compiler or interpreter options, program conﬁgurations, and software
mutations.

To address this gap, we propose MAGPIE — Machine Automated General Performance Improvement through Evo-
lution of software — a framework that enables comparison and combination of various software improvement tech-
niques. At its core, MAGPIE provides a uniﬁed software representation that separates the search strategy from the
improvement technique. It allows a natural reuse of search strategies between improvement techniques and their fair
comparison, as well as an inherent way of investigating the interactions between improvements found at different
granularity levels, e.g., at the level of compiler options or code mutations.

We instantiate MAGPIE with three software improvement techniques: compiler optimisation, algorithm conﬁguration,
and genetic improvement. As the search strategy we use a simple local search as all three techniques have been
previously shown to be efﬁciently solved using dedicated local search strategies. Focusing on a single search strategy
will allow for fair comparison of the magnitude of improvements found by each technique, while future work can
explore more complex or different search strategies.

We use our framework to optimise four large real-world software written in C, C++, and Java. We optimise software
performance in terms of running time in three scenarios, while the last scenario exempliﬁes MAGPIE multi-objective
abilities by simultaneously optimising running time and solution quality. Speciﬁcally, we ﬁrst compare the independent
use of compiler optimisation, algorithm conﬁguration, and genetic improvement, before analysing combinations of
their recommended modiﬁcations, and ﬁnally using MAGPIE to simultaneously explore all types of three techniques.

Our results show signiﬁcant improvements on all four targeted real-world software applications. In particular, we
achieved up to 25% running time improvement using compiler optimisation, up to 97% improvement using algorithm
conﬁguration, and up to 61% improvement for evolving source code using genetic improvement. Automated combi-
nation of the patches yielded by applying different techniques led up to 10% further speedups on speciﬁc scenarios.
Finally, whilst joint use of compiler optimisation, algorithm conﬁguration, and genetic improvement was in most cases
less efﬁcient than separate training, it was still effective and led to a further 15% speedup on one scenario. In summary,
our contributions are:

1. MAGPIE, a uniﬁed abstract framework for comparison and combination of diverse software improvement

techniques.

2. An instantiation of MAGPIE that combines three ways to improve software performance: compiler options,

software parameters, and source code modiﬁcations, through an edit sequence representation.

3. The ﬁrst comparison study of these three software improvement techniques, sharing the same software rep-

resentation and search strategy.

4. We showed, on four large real-world software, that even the simplest local search can yield signiﬁcant im-
provement for all three improvement techniques, comparable with previous work. In particular we obtained
up to 25% speedups with compiler or interpreter ﬂag optimisation, up to 61% for source code modiﬁcations
evolved using genetic improvement, and up to 97% via algorithm conﬁguration.

5. We also reported on the ﬁrst automated exploration of the combination of modiﬁcations yielded by different

techniques, which showed up to 10% further improvements.

6. We also conducted a study on the joint search space yielding up to 15% improvements.

7. Finally, with this paper we provide an implementation of MAGPIE, that provides researchers with quick pro-
totyping and fair comparison of search strategies and improvement techniques, as well as providing practition-
ers with a single tool to easily apply these improvement techniques. (https://github.com/bloa/magpie)

2

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

2 Related Work

Several techniques have been proposed for improvement of software performance. These include, but are not lim-
ited to, modiﬁcation of software’s source code, (e.g., refactoring [51, 1], genetic improvement [55]), application of
optimisations during compilation (e.g., loop transformations [6]), use of alternative algorithms or strategies (e.g., al-
gorithm selection [40, 39] and conﬁguration [30, 34]), making better use of machine speciﬁc hardware (e.g., cache
management [28]), or even adaptation of the hardware itself on which software is to be run (e.g., with FPGAs [29, 52]).

This leaves software users and developers to choose from a plethora of software improvement options and associated
tooling. We note, however, that several of the aforementioned techniques use similar, if not the same, metaheuristic-
based search strategies to navigate the search space of alternative conﬁgurations or software modiﬁcations. In this
section we elaborate on such techniques.

2.1 Compiler Optimisation

Human-written code is seldom executed directly. In most cases, it is interpreted (e.g., with languages such as Python
or Ruby) or compiled into machine code or bytecode (e.g., with software such as C, C++, or Java). The choice of
the interpreter or compiler is a ﬁrst factor that can signiﬁcantly impact software performance, as the default choice
is not guaranteed to be optimal. For speciﬁc applications or environments alternative compilers/interpreters can lead
to signiﬁcant improvements in terms of running time, memory usage, etc. In particular, compilers such as GCC can
offer several hundreds1 of optimisation options which may interact within each other. Even if shortcuts to speciﬁcally
curated standard subsets of optimisation options are provided (e.g., -O3), simply enabling more optimisation doesn’t
always lead to better performance [26].

tackling either optimisation selection (choosing
Research on compiler optimisation follows two major directions:
which optimisation to apply) or phase-ordering (choosing the order in which to apply optimisations). Whilst many
approaches have been proposed for both, mostly based on machine learning [5], many metaheuristics have been
successfully applied to the arguably simpler problem of optimisation selection. For instance, multi-objective Strength
Pareto Evolutionary Algorithm [64] was implemented in COLE [33] and TACT [57], and a single-objective genetic
algorithm was used to navigate the space of compiler ﬂags in ACOVEA [50].

2.2 Algorithm Conﬁguration

Beyond performance improvements due to interpreter or compiler conﬁguration, the other major source of potential
improvements lies in the conﬁguration of the software itself. Indeed most programs expose design choices in the
form of parameters in order to make them adaptable to particular contexts or applications. The default values of
these parameters are usually set to offer overall good performance across all possible inputs. The two major types of
approaches to improve a software conﬁguration are either to ﬁnd optimal parameter values before using the software
(ofﬂine setting) or to update parameter values while using it (online setting) [22, 30].

In this work, we focus on ofﬂine software conﬁguration, usually either called parameter tuning or algorithm con-
ﬁguration. While both terms are mostly interchangeable, tuning is preferred when all parameters are numerical and
continuous (e.g., probability or percentage threshold, number of iterations, population size) leaving conﬁguration for
when it also involves categorical variables (e.g., strategy selection, Boolean parameters). Online parameter control
(and algorithm scheduling [54]) while allowing for further adaptation, comes with the cost of increased search space
and complexity.

Most approaches for automated algorithm conﬁguration are metaheuristics. Well-known automated conﬁgurators
include for example ParamILS [37, 36], based on local search, SMAC [35], based on Bayesian modelling, GGA [4, 3],
based on a genetic algorithm, and irace [48], based on statistical racing. Relevant to our research, irace has been
directly applied on GCC [18] and SMAC on two prominent JavaScript compilers [24], showing that off-the-shelf
algorithm conﬁguration can also be used for compiler optimisation. Another Bayesian approach, BOCA [19], has also
been proposed to tackle compiler autotuning and applied on GCC and LLVM.

2.3 Software Evolution

Automated algorithm conﬁguration is usually applied to parameters explicitly exposed by software developers, repre-
senting different design choices. We present below several techniques that operate directly on the software itself, thus
allowing for even more software variants to be generated.

1https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html

3

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

Software
Benchmark

MAGPIE

Best variant

ﬁtness

software +
edit sequence

Training set

Figure 1: MAGPIE general workﬂow

Genetic improvement (GI) uses metaheuristics to improve software performance by evolving software itself [55].
Changes are most commonly applied at the level of source code, inserting or deleting lines, or nodes of an abstract
syntax tree (AST). Early GI work used genetic programming [41, 46, 58] to evolve the target software. Nowadays the
ﬁeld is expanding, using other types of search approaches, such as local search, recently shown as effective as genetic
programming [14]. GI reuses material already present in the original software, following the plastic surgery hypothe-
sis [8], i.e., that the changes required to obtain improved software can already be found in existing code. Nonetheless,
GI can also use external genetic material, e.g., for automated code transplantation [9, 56]. GI usually mutates lines of
codes or statements. However, it has also been applied to more speciﬁc constructs such as Boolean conditionals [56],
constants [45, 42, 44], arithmetic (such as <=/</==/!=/>=/>), and logical operators (such as ||/&&) [31].

Basios et al. proposed Darwinian evolution [10, 11] which evolves software’s data structures. Their study on Java
software identiﬁed optimal implementations of List containers for speciﬁc variables (e.g., ArrayList, LinkedList).
Wu et al. proposed deep parameter optimisation (DPO) [63, 17, 15, 62] as an alternative solution to overcome the
hurdle that software parameters have to be explicitly exposed by software designers. DPO uses mutation testing
as the underlying tool to discover potential unexposed decision choices, before applying multi-objective search for
improved parameter values. Binkley et al. proposed observation-based program slicing [12] which minimises the
size of software given a set of criteria to fulﬁl. Other approaches include program transformation [23] and code
refactoring [51] which apply more restrictive template-based changes. Another notable example is loop perforation
for which evolutionary strategies have also been tried [7].

3 The MAGPIE Framework

In this section we introduce our conceptual framework MAGPIE — Machine Automated General Performance Im-
provement via Evolution of software — and describe how it can be applied to run compiler optimisation, algorithm
conﬁguration, and genetic improvement. To the best of our knowledge we are the ﬁrst to combine in a uniﬁed frame-
work all three types of approaches.

3.1 MAGPIE Overview

In MAGPIE we propose to represent all changes, from source code changes to conﬁguration changes, as minimal edits
to be applied to the software, with the ﬁnal solution representation being the sequence of edits needed to be applied to
obtain the desired software variant. While more complex representations have been proposed [53], edit sequences are
more easily encoded, constructed, and mutated. They also make MAGPIE scalable to real-world software. In addition,
Weimer et al. showed that shorter patches are easier to understand and thus more likely accepted by reviewers [61].

Starting with an initial software and a benchmark on which to improve its performance, MAGPIE will iteratively create
edits and construct new software variants to be evaluated, and serve as a basis for subsequent variants. A given search
process then navigates the space of available software edits and ultimately returns the sequence of edits describing
how to obtain the best variant found. A basic workﬂow of MAGPIE is provided in Figure 1.

As frequently used in previous work (see Section 2), we recommend the use of metaheuristics, based on local
search [32] or genetic programming [41, 46].

3.2 General-Purpose Software Edits

In MAGPIE each edit is represented as a triplet of elements:

• edit type: the type of operation being applied,
• location: where the operation will be applied to, and
• ingredient: if necessary, a new value or code fragment.

4

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

This type of solution representation has previously been shown to be effective to represent software variants in the
context of genetic improvement. However, it is abstract enough to easily encode other types of software improvement
techniques. Edits can be applied at different granularity levels, summarised as follows.

• Compiler/interpreter level, targeting parameters outside the scope of the software source code, e.g., compiler

or interpreter options.

• Software parameter level, targeting all accessible developer-exposed software parameters.
• Source code level, targeting lines, statements, e.g., their insertion/deletion/replacement; and mutations of

numerical literals, operators, data structures, and others.

We describe below how various software improvement techniques can be represented. To consider more diverse
software in our experiments we focus on edit types that can be applied on most software. For example, we consider
modifying numerical constants but not speciﬁc data structures, as those depend on the software programming language.
Nevertheless, we plan to enrich MAGPIE with such specialised program transformations in future work.

3.3 Compiler/Interpreter Optimisation

Compiler optimisation, as in the selection of the best optimisations to be applied during compilation, is straightforward
using algorithm conﬁguration. Therefore, we use the edit representation described in subsection 3.4. Conﬁguring
other environmental options, such as the parameters of the language interpreter, are also covered by general algorithm
conﬁguration.

In practice edits targeting the conﬁguration of the compiler are used when the software is compiled without interfering
with the edits targeting the interpreter or the software itself.

3.4 Algorithm Conﬁguration

A typical algorithm conﬁguration solution representation is an associative array that maps every parameter to its
corresponding selected parameter value. Instead, we propose to encode the changes to the default conﬁguration rather
than evolving complete copies. Therefore, each replacement edit for algorithm conﬁguration speciﬁes the parameter
it targets (location) together with its new value (ingredient).

Parameter values can be drawn uniformly at random from the range of possible values, but also following other
statistical distributions (e.g., geometric) or combinations of any distributions (e.g., speciﬁc values such as 0 or −1
being treated separately). Special cases such as conditional parameters and forbidden combinations are handled during
ﬁtness evaluation ensuring the validity of the resulting conﬁguration.

The number of possible edits is the sum of the number of possible parameter values for all parameters. This becomes
unreasonably large as soon as a parameter can assume ﬂoating point numerical values or admit large bounds (e.g., an
32-bit unsigned integer admits 232 possible values). However, at any point edits can still be enumerated by subsampling
for each such parameter a set number of possible values.

3.5 Genetic Improvement

Genetic improvement (GI) typically targets source code, mutating either lines of code or statements [55]. For the latter
the abstract syntax tree (AST) representation is used. There are three standard GI edit types: deletion, replacement,
and insertion, each specifying code fragments as both location and ingredient. Deletions result in a linear number of
possible edits, while numbers of possible replacements and insertions grow quadratically.

3.6 Numerical Constants

The space of possible numerical values is too large to simply sample random replacements. Instead, we propose two
edit types, using absolute and relative values. As for conﬁguration edits, edits modifying numerical constants specify
the location of the speciﬁc constant they target and it modiﬁed expression (ingredient).

We single out the values 0, 1, and −1, following Wu et al. [63], as they are used in mutation testing. As for rela-
tive values, previous work showed that ±1 increments in software numerical constants helped in the context of bug
ﬁxing [31]. To enable changes of larger magnitude while still keep replacements with scale related to the original
value, we propose the following simple operations: (•+1), (•-1), (•*2), (•/2), (•*3/2), or (•*2/3). Operations
are applied with correct parenthesising, ensuring that successive edits can stack on top of each other. We enforce the
constraint that a ﬁxed number of new expressions can be applied to every numerical constant. This way the number of
possible edits is linear in the number of constants.

5

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

First stage:

GCC -O0

GCC -O1

GCC -O2

GCC -O3

T

T

T

T

Second stage: (using -O3)

GCC
conﬁg.

software
conﬁg.

GI of
<stmt>

GI of
<num>

SVT

SVT

SVT

SVT

Third stage: (using -O3)

best edits

combined
validation

VT

combined
search

SVT

S: training (search); V: validation; T: test

Figure 2: Experimental protocol (for C/C++ and GCC). In the ﬁrst stage compiler optimisation levels are manually
evaluated. In the second stage four independent runs are conducted. Each only use different types of edits, correspond-
ing to four approaches: compiler optimisation, algorithm conﬁguration, mutation of statements, mutation of numerical
literals. In the third stage we evaluate the combination of the best edits found in the second stage. We also evolve
software using every available edit type, conceptually combining all four approaches.

4 Research Questions

Our main research goal is to show the beneﬁts of a uniﬁed framework for software improvement technologies. With
that in mind, we propose MAGPIE and ask the following RQs:

RQ1: How effective is MAGPIE at optimising compiler or interpreter conﬁgurations? Firstly, we want to mea-
sure the effect of different settings of the compiler (or interpreter, depending on the language). This includes, e.g., for
C code, the choice of the compiler and automated selection of different parameter settings.

RQ2: How effective is MAGPIE at tuning parameters explicitly exposed to the user? We want to investigate what
improvements can be achieved using automated algorithm conﬁguration.

RQ3: How effective is MAGPIE at ﬁnding efﬁciency improvements using genetic improvement? Similarly, we
want to know what type of improvements could be found manipulating the program’s source code, both its statements
and numerical values.

RQ4: What is the impact of combining efﬁciency improving changes from different granularity levels? The
search spaces of compiler optimisation, algorithm conﬁguration, and genetic improvement are diverse in size, struc-
ture, and content. If changes produced by each technique are good in isolation, can performance be further improved
by combining them together?

RQ5: How effective is MAGPIE at simultaneously exploring the joint search space of software edits? Since the
edit sequence solution representation is shared within MAGPIE, despite their differences, can compiler optimisation,
algorithm conﬁguration, and genetic improvement be applied at the same time to evolve software? We also measure
the impact of such improvements found.

5 Empirical Study

In order to answer our research questions we conducted an empirical study on four large real-world software. In this
section we present details of our experimental protocol, the MAGPIE implementation, and the benchmarks used.

6

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

repeated k times using k folds

1/k
training
set

up to 10
training
instances

(k−1)/k
training
set

k-folding
(k = 10)

Training
(local search)

Validation
(subsetting)

100%
test
set

Test

Figure 3: Cross-validation with k-folding and holdout.

5.1 Experimental Protocol

The general workﬂow of the experiments is illustrated in Figure 2. For simplicity, we illustrate our procedure on an
example using a C/C++ software and the GCC compiler.

First of all, we manually investigate a limited number of speciﬁc external settings. For C/C++ software we use the
compiler optimisation parameters -O0 to -O3, whereas for Java software we simply use each JVM with default settings.

Then, we investigate how much performance of a piece of software can change when the parameters of the compiler or
the virtual machine are optimised. Next, we investigate the impact of conﬁguring the software itself. In particular, we
run independent evolution of software’s parameters, statements and numerical values. For C/C++ evolution starts from
-O3 to minimise the length of the experiments: we chose to not use the previously evolved compiler conﬁguration,
even if better performance was found, to avoid dependencies between experimental steps.

Finally, in the last step we evaluate the effect of combining edits from all the different approaches. We investigate
the following two strategies: 1) we simply combine modiﬁcations we obtained during training in the second stage 2)
we re-run MAGPIE on the joint edit search space, at each step of search picking an edit type at random, regardless of
whether it’s applicable to compiler options or numerical constants or other.

We use actual software execution to evaluate the ﬁtness of software variants (Figure 1). To make sure performance
improvements generalise on unseen data, available instances are divided into two disjoint subsets: a training set, to
compare variants to one another; and a test set, subsequently used to conﬁrm the quality of the ﬁnal best software
variant found. In addition, to make sure results are not tainted by the choice of the training set, we use k-fold cross-
validation in addition to simple holdout, as illustrated in Figure 3. Training instances are then separated again into
k subsets so that training is repeated k times. For each trial of our search algorithm only 10 training instances are
actually used due to the very high evaluation cost. At the end of the training step, every instance from the other k − 1
subsets are used to validate the ﬁnal software variant. Finally, after the k training and validation repetitions, the single
best program variant is evaluated on the test set. That scheme is used for every step of the experimental protocol
(Figure 2).

5.2 MAGPIE Implementation

To allow for language-independent experiments, we instrumentalise our implementation with a universal XML rep-
resentation of software on which to apply our edit sequence representation. We chose to use srcML [20] to convert
source code to an XML representation. srcML breaks down a source code ﬁle into its abstract syntax tree (AST)
resulting in an easy to modify structure. It provides provides support for C, C++, C#, as well as Java. This approach
was previously successfully used in the PyGGI 2.0 [2] GI tool.

In addition, to represent compiler or software parameters conﬁgurations, we used dictionary-like linear tree structures
for a natural and seamless integration. We implemented the following edits:

StmtDelete that deletes a statement node from the AST;

StmtReplace that replaces a statement node in the AST by another from the original AST;

StmtInsert that inserts (before or after another statement node) a statement node from the original AST;

ConstantSet, ConstantUpdate: that modiﬁes a numerical constant according to the ingredient content;

ParamSet that assigns to a (either compiler or software) parameter the value passed as ingredient.

7

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

Algorithm 1 First improvement local search

procedure LS ( )

best ← empty mutant
repeat

(cid:46) Append or remove an edit at random
mutant ← mutate(best)
(cid:46) Accept if better
if ﬁtness(mutant) ≤ ﬁtness(best) then

best ← mutant
until training budget exhausted
return best

Following recent results [14], we use a local search procedure (see Algorithm 1). The initial solution is an empty se-
quence, corresponding to the unmodiﬁed software. Then, edits, drawn uniformly at random, are increasingly appended
to the current solution as long as performance is not negatively impacted. Alternatively, mutation can also remove any
edit previously inserted in the edit sequence.

As for the validation step, the edit sequence needs to be reduced to a minimal form, in order to both avoid overﬁtting on
the training set and to provide a simpler ﬁnal patch. With the assumption that edits are mostly independent, especially
when combining edits obtained during separate runs (Figure 2, third step: combined validation), all edits are ﬁrst
independently evaluated and ranked with respect to runtime efﬁciency. Then, they are reintroduced one by one and
appended to the new edit sequence and discarded unless they contribute to further performance improvement [13].
However, due to edit interactions, sometimes such constructed sequence is unable to match the performance of the
naive concatenation of all edits. In this case, we start from the complete sequence and successively remove each edit
from the sequence until no single edit can be removed without performance loss [43].

Training trials were run in parallel using a budget of 1000 mutant evaluations, except for the ﬁnal step that uses a
budget of 4000 evaluations to accommodate for the much larger joint search space. Cross-validation uses the standard
k = 10 repetitions.

5.3 Compiler and VM options

We chose to compare GCC, the defacto-standard linux C compiler, to Clang, a more recent compiler. Whilst by far
most of the literature concentrate on GCC, which exposes a very large number of options, recent work has also been
focusing on LLVM/Clang [21, 19, 27, 47]. Both compilers are known to produce semantically similar yet different
machine code, potentially leading to differences in performance. Whilst both compilers expose different optimisation
options, they both provide the -O0 to -O3 optimisation shortcuts.

Whilst compiler optimisation typically only targets compiled languages such as C, we also compare the OpenJDK and
GraalVM Java virtual machines (JVM) with the expectation that different execution heuristics will also lead to different
performance. While optimisation options are signiﬁcantly less numerous than for C compilers, JVMs provide many
extra options2 that are subject to change without notice and therefore hard to tune manually. The choice of considering
Java software is also motivated by a recent report on the JVM ecosystem highlighting that 36% of surveyed developers
switched from the Oracle JDK to an alternative OpenJDK alternative in 2019.3 Details regarding the two C/C++
compilers and two JVMs as succinctly presented below.

GCC 9.3.1 GCC is the standard GNU and Linux compiler. In addition to the usual -O0 to -O3 options we consider

213 other Boolean and categorical optimisation parameters.

Clang 12.0.0 Clang is a compiler based on the LLVM toolchain, designed to provide a drop-in replacement to GCC.
While Clang has made an explicit design decision not to expose the optimisation pipeline details to con-
sumers, there are 63 transform passes that can be enabled in order to optimise the ﬁnal executable.

OpenJDK 12.0.2 (Oracle) We use a recent (2019) and widespread Java implementation as a baseline. We use 96
parameters shared between both Java implementations, including VM runtime, JIT compiler, and garbage
collection options.

2https://docs.oracle.com/en/java/javase/14/docs/specs/man/java.html
3https://snyk.io/blog/jvm-ecosystem-report-2020/

8

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

OpenJDK 11.0.9 (GraalVM 20.3.0) While based on a slightly older Java version (2018), GraalVM is an alternative
Java VM and JDK that advertise high-performance running time, fast startup and low memory footprint. We
use the same parameters as the Oracle-provided Java implementation.

5.4 Software For Improvement

Our experiments require software on which all three types of software improvement techniques can be applied, more
speciﬁcally C/C++ and Java software with relevant exposed software parameters with reasonable compilation time.
We also want to choose software for which we know there are improvements to be found. To that end, we use the
AClib [38], a benchmark library for software algorithm conﬁguration. We chose to focus on the 3 largest categories:
SAT solving, machine learning and planning software.

The AClib library is dominated by different variants of SAT solvers. Two of these have previously been used as case
studies in literature on genetic improvement [14, 56]. In order to ensure diversity in the selected software we also
chose two software from different AClib categories: planning and machine learning. Details regarding the selected
software is presented below:

MiniSAT (C++) is a well known SAT solver. We use the variant minisat HACK 999ED CSSC-cssc14 as provided
by AClib. It exposes 25 mostly numerical parameters, and during search we evolve the core/Solver.cc
ﬁle. We use the CircuitFuzz dataset, with 247 training and 277 test instances.

LPG (C) is a local-search based planning solver. We use the latest version of LPG (1.2) as provided by AClib. It
exposes 87 numerical and categorical parameters. Evolution focuses on the LocalSearchH relaxed.c ﬁle.
We use the Blocksworld dataset, with 49 training and 25 test instances.

Sat4J (Java) is another SAT Solver, but written in Java. We use the latest version of Sat4J (2.3.1 snapshot
016981c0) with the 10 categorical and numerical parameters provided by AClib. The functions from
minisat/core/Solver.java are evolved during the experiments. We use the same CircuitFuzz instances
as in the MiniSAT scenario.

Weka (Java) is a popular data mining and machine learning Java software. We use the latest version of Weka (3.8
snapshot d5ade95) and focus on the random forest classiﬁer.
Its 10 parameters and implementations of
both the RandomForest and RandomTree classes are optimised. We use the Dexter dataset used by Auto-
Weka [60], with 420 training and 180 test instances. Since Weka already performs cross-validation internally,
training in our experimental protocol uses every training instance.

Our objective is to improve efﬁciency in terms of running time with MAGPIE. Henceforth a technique is considered
effective if it produces a software variant that is faster than original software. However, in practice we chose to
measure the number of CPU instructions, as we found it to be less ﬂaky than raw elapsed time (see subsection 6.1).
Measurements are obtained using the perf Linux kernel performance analysis tool.

Weka computes machine learning models with intrinsic quality in terms of classiﬁcation rate. To ensure our Weka
variants compute both efﬁcient and fast models, we minimise both misclassiﬁcation rate and running time in lexico-
graphical order, thus including a bi-objective scenario in our experiments.

6 Results and Analysis

In this section we present the results of our empirical study and provide answers to our research questions. All
experiments were conducted on a computational cluster running CentOS 7.8.2003 and the Linux 3.10.0 kernel.

6.1 RQ1: Compiler and JVM Optimisation

Table 1 shows, for all four software, the average number of CPU instructions recorded over ten runs (with standard
deviations) when run on the full set of test instances. These are recorded for the default software with standard
compiler and VM options, as well as MAGPIE-evolved compiler and VM conﬁgurations. Additionally, the “best” row
reports the single best ﬁtness produced by MAGPIE.

We ﬁrst note the extremely low coefﬁcients of variation (ratio of standard deviation over mean) for repeated measure-
ments, despite the relatively long CPU times. As an illustration, for MiniSAT with GCC -O0, the ten trials took in
average 1.24 × 104 s with a standard deviation of 3.61 × 102 s, meaning a coefﬁcient of variation of 0.03, ﬁve orders of
magnitude larger than 8.27 × 10−7 when measuring CPU instructions. For that reason, all performance improvements
are reported using ratios of CPU instructions rather than ratios of CPU time. While the most probable explanation is

9

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

Table 1: Average and best ﬁtness on test instances with different compiler and JVM settings. [in CPU instructions]

GCC

Clang

MiniSAT
-O0
-O1
-O2
-O3
MAGPIE

best

LPG
-O0
-O1
-O2
-O3
MAGPIE

best

Sat4j
default
MAGPIE

best

Weka
default
MAGPIE

best

8.43 × 1013 ±7 × 105
8.98 × 1012 ±3 × 106
8.59 × 1012 ±2 × 106
8.56 × 1012 ±2 × 106
8.02 × 1012 ±9 × 1010
7.81 × 1012 (-90.74% -O0)
(-8.80% -O3)

8.61 × 1013 ±2 × 107
4.16 × 1013 ±8 × 105
8.43 × 1012 ±2 × 106
8.68 × 1012 ±2 × 106
8.51 × 1012 ±1 × 1011
8.43 × 1012 (-90.20% -O0)
(-2.91% -O3)

3.48 × 1013 ±1 × 107
1.77 × 1013 ±3 × 105
1.36 × 1013 ±3 × 106
1.39 × 1013 ±4 × 106
1.25 × 1013 ±4 × 1011
1.22 × 1013 (-64.97% -O0)
(-12.11% -O3)

2.79 × 1013 ±2 × 107
1.37 × 1013 ±5 × 106
1.27 × 1013 ±3 × 106
1.27 × 1013 ±4 × 106
1.27 × 1013 ±9 × 109
1.26 × 1013 (-54.61% -O0)
(-0.23% -O3)

Oracle OpenJDK

GraalVM

4.17 × 1013 ±2 × 1011
3.48 × 1013 ±8 × 1011
3.38 × 1013 (-18.8% def.)

4.43 × 1013 ±7 × 1011
3.34 × 1013 ±6 × 1011
3.31 × 1013 (-25.26% def.)

6.51 × 1011 ±2 × 109
6.22 × 1011 ±2 × 109
6.20 × 1011 (-4.86% def.)

6.55 × 1011 ±3 × 109
6.28 × 1011 ±7 × 108
6.27 × 1011 (-4.25% def.)

latency when accessing input ﬁles during execution, accumulations of kernel interrupts, context switches, and hidden
multi-threading may also contribute to noise in time measurements, especially for the JVMs.

Before even running MAGPIE we observe a clear gap between the use of -O0 (default parameter value) and -O2/-O3
for both GCC and Clang, with around 90% speedups for MiniSAT and 40% to 45% speedups for LPG. Improvements
upon -O2 or -O3 using MAGPIE are smaller, up to 12% speedups for GCC and 3% speedups for Clang, the latter
being due to the unfortunate closed nature of the optimisation options of Clang. The magnitude of these results is
in line with previous work using automated algorithm conﬁguration on GCC [18]. For Sat4j the best performance is
achieved by conﬁguring GraalVM, for which a 25% speedup in CPU instruction was achieved. On the more recent
Oracle OpenJDK a 19% speedup was still achieved. For Weka results for both JVMs are similar, with a 5% speedup
obtained for the Oracle OpenJDK.

Details for frequently changed parameters with noticeable impact in isolation are presented in Table 2, as found during
the validation step. Results for Weka and LPG+Clang are omitted as no single individual parameter change had a
> 1% improvement. Interestingly for Java, for which JVM conﬁguration is often advised against, we note a frequent
change in the garbage collection algorithm for Sat4j, and the disabled use of compressed ordinary object pointers
speciﬁcally for GraalVM.

Answer to RQ1: The edit sequence representation is both effective and efﬁcient at optimising the conﬁguration
of both C/C++ compilers and Java VMs. Speedups up to 12% were achieved for GCC when compared with -O3
(91% speedup when compared with -O0), 19% for the Oracle OpenJDK, and 25% for GraalVM.

10

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

Table 2: Frequent compiler and interpreter individual parameter changes with ≥ 1% improvement in ﬁtness.

Scenario

Parameter

MiniSAT GCC

Clang

GCC

LPG

Sat4j

Oracle

-ftree-loop-im
-fira-algorithm=CB

-O2

-ftree-loop-im
-fivopts
-fno-tree-fre

-XX:-UseCompressedOops
-XX:+AggressiveHeap
-batch

#

7
6

7

8
7
6

7
6
5

GraalVM -XX:-UseCompressedOops
-XX:-UseTLAB
-XX:+AggressiveHeap

10
8
5

Imp.

-3.6%
-1.3%

-3.0%

-3.2%
-4.7%
-1.0%

-2.9%
-10.5%
-1.5%

-8.4%
-1.2%
-10.9%

×1013

s
n
o
i
t
c
u
r
t
s
n
i

U
P
C

1.2

1

0.8

s

c
n
n
c

s

1

c
s
n
n
s

c

0

s

c
s
c
n
n

2

s

n

c
n
c

s

3

s

s

c
n

c

4

GCC
s
Clang

c
c

s

9

n

s
s

c
c

8

c

s
s

c

5

s
s
n
n
c
c

6

c
s
s
c
n

7

Figure 4: Algorithm conﬁguration and source code evolution using MAGPIE on the MiniSAT scenario. (c: conﬁgura-
tion, s: statements, n: numerical constants)

Repetition

6.2 RQ2: Algorithm Conﬁguration; RQ3: Genetic Improvement

Figures 4, 5, 6, and 7 report the ﬁtness values on the set of test instances (respectively for the MiniSAT, LPG, Sat4j, and
Weka scenarios) of the ten evolved mutants obtained with MAGPIE using algorithm conﬁguration (denoted by “c”),
GI on statements (“s”) and GI on numeric literals (“n”). Lines connect repeated runs to emphasise differences obtained
between alternative compilers/JVM. Missing data points correspond to overﬁtting mutants — i.e., mutants for which
the validation step failed to produce a single mutant improving the baseline. Data points above the baseline correspond
to mutants both better during training and validation, but failing to properly generalise on the test set. Note that results
may be biased toward algorithm conﬁguration as scenarios and datasets have been lifted from AClib, a conﬁguration
library. Differences with other work may also be due to the speciﬁc conﬁguration encoding, the simple local search
used, or training protocol.

In the MiniSAT scenario (Figure 4) most mutants fail to generalise and very few mutants achieve improvements over
the baseline ﬁtness (using -O3), noticeably two by changing MiniSAT conﬁguration and two by evolving its AST
statements. In the LPG scenario (Figure 5) while all data points show improvements over the baseline, the majority
of GI-evolved mutants overﬁt during the validation step. Even if both GI types of approaches resulted in some very
good mutants, with up to 60% speedup, the best improvements achieved are obtained using algorithm conﬁguration,
In the Sat4j scenario (Figure 6) all three types
with a best ﬁtness of 3.42% over the associated baseline ﬁtness.

11

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

×1013

s
n
o
i
t
c
u
r
t
s
n
i

U
P
C

1.5

1

0.5

0

n
n

c
c
0

s

s

c
c
2

s
s

c
c
1

n
s
n

c
c
3

s

4

s

n
n

c
c
5

Repetition

GCC
Clang

n

s

8

c
c
9

c
c

6

c
c

7

Figure 5: Algorithm conﬁguration and source code evolution using MAGPIE on the LPG scenario. (c: conﬁguration,
s: statements, n: numerical constants)

s
n
o
i
t
c
u
r
t
s
n
i

U
P
C

5.5

5

4.5

4

3.5

×1013

c
s
n

n
c
s

0

c

s
n
c
s
n

1

c

c

s
n

s
n

6

OpenJDK
GraalVM

c
n
s
c
n
s

7

c
s
n

n
s
c

8

s
c
n

n
s

c

9

n
s
c

n
s
c

3

c
n
s

n
c
s

4

s
n
c

c
n
s

2

c
s
n
c
s
n

5

Repetition

Figure 6: Algorithm conﬁguration and source code evolution using MAGPIE on the Sat4J scenario. (c: conﬁguration,
s: statements, n: numerical constants)

of approaches similarly show little improvement over the default setup, with two tuned mutants and one statement-
evolved mutant failing to generalise. Finally Figure 7 reports results for the Weka scenario, showing both the number
of CPU instructions and the misclassiﬁcation rate over the test dataset (both to be minimised). Neither the evolution
of statement or numerical values within the random forest implementation led to any substantial improvement (all
mutants are clustered with the baseline with a misclassiﬁcation rate of 15.5%). On the other hand, tuning of the
algorithm’s parameters led to improvements in both objectives, with in particular variants being simultaneously twice
as fast and twice as efﬁcient as the baseline.

Table 3 reports, within the ten runs of each approach, the single best performance improvement. For Weka, for which
were to be minimised (in lexicographical order) both misclassiﬁcation rate and number of CPU instructions, ﬁtness
values of some additional runs are also reported when relevant. The results relative to the “combined” and “joint”
columns are discussed in the following sections.

Algorithm conﬁguration is the clear winner in the LPG and Weka scenarios. For LPG, it achieves up to 97% im-
provement in number of CPU instructions, i.e., the software variant is around 30 times faster, when GI only produces
up to 61% improvement. For Weka, GI approaches are unable to ﬁnd any improvements. One reason may be its

12

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

s
n
o
i
t
c
u
r
t
s
n
i

U
P
C

1

0.8

0.6

0.4

0.2

0

×1012

c
c

c

c

c

c

c
c

c

c

c

c

c

c
c

c

c
c

c

c

OpenJDK
GraalVM

ssssssssssssssssssssnnnnnnnnnnnnnnnnnnnn

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

Misclassiﬁcation rate

Figure 7: Algorithm conﬁguration and source code evolution using MAGPIE on the Weka scenario. (c: conﬁguration,
s: statements, n: numerical constants)

Table 3: Single best ﬁtness improvement on test instances. [in percentage of CPU instructions]

Scenario

Compiler

Parameters

Statements

Numbers

Combined

Joint

MiniSAT (GCC)
MiniSAT (Clang)
LPG (GCC)
LPG (Clang)
Sat4j (Oracle)
Sat4j (GraalVM)
Weka (Oracle)
Weka (GraalVM)

-9%
-3%
-12%
-0%
-19%
-25%
(-0%, -5%)
(-0%, -4%)

-11%
-17%
-97%
-97%
-13%
-4%
(-50%, -46%)
(-54%, +14%)3

-15%
-0%
-61%
-58%
-1%
-1%
(-0%, -0%)
(-0%, -0%)

-0%
-0%
-36%
-35%
-0%
-1%
(-0%, -0%)
(-0%, -0%)

-25%
-21%
-95%
-95%
-29%
-27%
(-54%, +15%)1
(-54%, +10%)4

-40%
-10%
-94%
-0%
-8%
-24%
(-50%, +13%)2
(-57%, -36%)

For Weka are reported changes for both misclassiﬁcation rate and CPU instructions.
Other notable Weka improvements: 1: (-50%, -49%), 2: (-43%, -24%), 3: (-50%, -48%), 4: (-50%, -49%)

very concise programming style, with very small dedicated functions hard to safely modify. Another reason could
be that improvements found by algorithm conﬁguration may fall outside the scope of the targeted classiﬁer source
code, within the more general code not considered by GI. For MiniSAT all three improvement techniques resulted in
signiﬁcant speedups (9%, 17%, and 15%, respectively). Finally, for Sat4j, both compiler optimisation and algorithm
conﬁguration were able to ﬁnd signiﬁcant speedups, (25% and 13%, respectively).

There is, however, no single technique best in every scenario. By nature, algorithm conﬁguration explores design
choices speciﬁcally exposed by software designers, but are limited to those exclusively. On the other hand, genetic
improvement produces changes from a much larger and much richer search space, well outside what can reasonably be
manually described with parameters, but can easily suffer from focusing on an inadequate part of the ﬁtness landscape.

Answer to RQ2 and RQ3: MAGPIE was successful in ﬁnding large running time improvements, in each of the
three search spaces investigated: on the LPG scenario up to 97% speedup for parameter optimisation, up to 61%
speedup for GI on statements, and up to 36% speedup for GI on numerical constants.

6.3 RQ4: Evaluating Combinations of Mutants

In Table 3 we report the performance on the validation set of software variants obtained by combining edits found by
MAGPIE during training of all four types of approaches. Changes in compiler or interpreter conﬁguration are unques-
tionably not reachable through the other search spaces, as they don’t impact the software source code or execution
semantics directly. As such, it seems that associated improvements directly lead to matching improvements in the

13

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

Table 4: Percentage of edits by origin in the best combined and joint patches from Table 3 (detailing between
StmtDelete/StmtReplace/StmtInsert and ConstantSet/ConstantUpdate)

Scenario

Comp.

Param.

Statements Numbers

Combined validation
MiniSAT (GCC)
MiniSAT (Clang)
LPG (GCC)
LPG (Clang)
Sat4j (Oracle)
Sat4j (GraalVM)
Weka (Oracle)
Weka (GraalVM)

Joint training
MiniSAT (GCC)
MiniSAT (Clang)
LPG (GCC)
LPG (Clang)
Sat4j (Oracle)
Sat4j (GraalVM)
Weka (Oracle)
Weka (GraalVM)

17
6
17
0
48
47
44
44

15
3
0
0
0
29
0
29

9
14
25
34
10
18
9
12

8
11
14
0
33
0
20
43

34 + 9 + 0
43 + 6 + 6
25 +10 + 4
39 + 2 + 5
7 + 3 + 3
0 +12 + 6
9 + 3 + 9
12 + 4 +16

42 + 8 + 5
40 +20 + 0
40 + 0 +11
0 + 0 + 0
0 +33 + 0
14 +14 + 0
7 +27 + 7
0 +14 + 0

16 + 5
9 + 17
7 + 13
10 + 10
0 + 8
6 + 12
13 + 13
8 + 4

14 + 9
17 + 9
11 + 11
0 + 0
0 + 33
14 + 29
20 + 20
14 + 0

combined patch. This is particularly visible in the MiniSAT (Clang), Sat4j, and Weka scenarios, in which there is no
interference between algorithm conﬁguration and genetic improvement.

In the LPG scenario, the combined validation is slightly less efﬁcient than the pure conﬁguration software variant
(from 97% to 95% speedup). In the Weka scenario, it successfully manages to combine the 5% conﬁguration changes
from the JVM optimisation to the improvements found using Weka’s algorithm conﬁguration, resulting in slightly
better variants. In both MiniSAT and Sat4j scenarios, however, it successfully combines the improvement found in the
different search spaces and leads to signiﬁcantly better software variants, ranging from further 2% to 10% speedups.

Table 4 presents the composition of the combined (RQ3) and joint (RQ4) patches reported in Table 3. In the Min-
iSAT and LPG scenarios best improvements come from statement deletion. In the Sat4j and Weka scenarios best
improvements are found through compiler/interpreter conﬁguration, while combined patches make use of every type
of edits.

Answer to RQ4: Edits resulting from different techniques can be effectively combined, leading up to further 10%
speedups when compared to the best individual speedup. Detailed analysis of combined patches composition hints
that optimal software variants may only be reached using a diverse set of edits.

6.4 RQ5: Simultaneous Evolution

Performance of software variants obtained by simultaneously training on all four search spaces — i.e., optimising
compiler/JVM conﬁguration, algorithm conﬁguration, GI on statements and GI on numbers — is also reported in
Table 3, together with composition details in Table 4.

Despite having a similar training budget as the combined approach discussed in the previous section, joint training
produced worse single best results in ﬁve of the eight scenarios. However, in two cases, optimising MiniSAT using
GCC and optimising Weka using GraalVM, the experiment produced a signiﬁcantly better software variant. For
MiniSAT it led to a 40% speedup when the best combined and best individual patches only produced 25% and 15%
speedups, respectively. For Weka joint training produced a variant with both a 57% decrease in misclassiﬁcation rate
(the best achieved in our results) with a 36% speedup. This can be explained as the joint search space is more complex
to navigate, while providing opportunities to ﬁnd otherwise inaccessible combinations of changes. Finally, we note
that ﬁnal patches obtained using joint training seem to use a more diverse set of edits, with more edits associated with
each of the three improvement techniques.

14

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

The results show that the joint training clearly suffers from the size of the combined search space. To alleviate this, a
possible solution may be to change how search spaces are accessible throughout the search. For example, the search
could ﬁrst strongly focus on compiler or interpreter conﬁguration, as those changes seem to be mostly independent
from the others, before introducing other edit types. The relative probabilities to investigate speciﬁc types of changes
may also be speciﬁcally tuned [59] to favour some search spaces over others; follow a set schedule [54]; or use a
multi-armed bandits scheme to maximise expected improvements.

Answer to RQ5: Training on the joint search space proved possible and effective. It resulted in a MiniSAT
software variant 25% faster than the best variant found by any technique applied separately, and still 15% faster
than the variant obtained by combining the best ﬁnal patches. However, it was less efﬁcient in the other scenarios.

7 Conclusions

In this paper we proposed a uniﬁed software performance improvement framework: MAGPIE. It provides a shared
solution representation, a sequence of edits. We implemented three software performance improvement techniques
using the MAGPIE approach: compiler optimisation, algorithm conﬁguration, and genetic improvement. We evaluate
our framework on four large real-world software, targeting running time minimisation and a bi-objective scenario
involving running time and solution quality. Results of our experiments are threefold. First, we showed that even a
simple local search can ﬁnd signiﬁcant performance improvement for all three techniques: up to 25% for compiler
optimisation, 97% for algorithm conﬁguration and 61% for evolving source code using genetic improvement. Then,
we discussed the best variants found, and show that while no single technique is most efﬁcient in all scenarios, partial
recombination of individually evolved changes led to up to 10% further speedups. Finally, we investigated a novel
approach in which all three techniques are combined during the local search itself, and were able to ﬁnd an additional
15% speedup in one speciﬁc scenario.

Overall, we implemented a simple alternative to existing dedicated tooling to automate software performance improve-
ment. Our results show that our framework is both effective and efﬁcient in reproducing signiﬁcant improvements in
line with previous work using other representations and search approaches. It provides a practical interface to conduct
comparison studies of both improvement techniques and their associated search processes, and allows for efﬁcient
prototyping of new research.

In future work we intend to investigate more closely the ﬁtness landscapes associated within the different search spaces.
We also plan to extend our framework with new and more complex search processes, new software improvement
techniques, as well as support for other performance measures. Finally, we intend to simplify and streamline our
framework to provide the community with both a comprehensive and extendable toolkit for researchers and a readily
usable framework for practitioners.

Implementation: https://github.com/bloa/magpie

Acknowledgements: This work was supported by UK EPSRC Fellowship EP/P023991/1.

References

[1] Mansi Agnihotri and Anuradha Chug. 2020. A Systematic Literature Survey of Software Metrics, Code Smells
https:

and Refactoring Techniques. Journal of Information Processing Systems 16, 4 (2020), 915–934.
//doi.org/10.3745/JIPS.04.0184

[2] Gabin An, Aymeric Blot, Justyna Petke, and Shin Yoo. 2019. PyGGI 2.0: Language Independent Genetic
Improvement Framework. In Proceedings of the 27th ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE 2019), Marlon Dumas, Dietmar Pfahl,
Sven Apel, and Alessandra Russo (Eds.). ACM, 1100–1104. https://doi.org/10.1145/3338906.3341184

[3] Carlos Ans´otegui, Yuri Malitsky, Horst Samulowitz, Meinolf Sellmann, and Kevin Tierney. 2015. Model-Based
Genetic Algorithms for Algorithm Conﬁguration. In Proceedings of the 24th International Joint Conference on
Artiﬁcial Intelligence (IJCAI 2015), Qiang Yang and Michael J. Wooldridge (Eds.). AAAI Press, 733–739.

[4] Carlos Ans´otegui, Meinolf Sellmann, and Kevin Tierney. 2009. A Gender-Based Genetic Algorithm for the
Automatic Conﬁguration of Algorithms. In Proceedings of the 15th International Conference on Principles and
Practice of Constraint Programming (CP 2009) (Lecture Notes in Computer Science, Vol. 5732), Ian P. Gent
(Ed.). Springer, 142–157. https://doi.org/10.1007/978-3-642-04244-7_14

15

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

[5] Amir H. Ashouri, John Cavazos, Gianluca Palermo, and Cristina Silvano. 2019. A Survey on Compiler Auto-
tuning using Machine Learning. Comput. Surveys 51, 5 (2019), 96:1–96:42. https://doi.org/10.1145/
3197978

[6] David F. Bacon, Susan L. Graham, and Oliver J. Sharp. 1994. Compiler Transformations for High-Performance

Computing. Comput. Surveys 26, 4 (1994), 345–420. https://doi.org/10.1145/197405.197406

[7] Zorana Bankovic, Umer Liqat, and Pedro L´opez-Garc´ıa. 2015. Trading-off Accuracy vs Energy in Multicore
Processors via Evolutionary Algorithms Combining Loop Perforation and Static Analysis-Based Scheduling. In
Proceedings of the 10th International Conference on Hybrid Artiﬁcial Intelligent Systems (HAIS 2015), Enrique
Onieva, Igor Santos, Eneko Osaba, H´ector Quinti´an, and Emilio Corchado (Eds.). Springer, 690–701. https:
//doi.org/10.1007/978-3-319-19644-2_57

[8] Earl T. Barr, Yuriy Brun, Premkumar T. Devanbu, Mark Harman, and Federica Sarro. 2014. The plastic surgery
hypothesis. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software
Engineering (SIGSOFT FSE 2014), Shing-Chi Cheung, Alessandro Orso, and Margaret-Anne D. Storey (Eds.).
ACM, 306–317. https://doi.org/10.1145/2635868.2635898

[9] Earl T. Barr, Mark Harman, Yue Jia, Alexandru Marginean, and Justyna Petke. 2015. Automated software
transplantation. In Proceedings of the 24th ACM SIGSOFT International Symposium on Software Testing and
Analysis (ISSTA 2015), Michal Young and Tao Xie (Eds.). ACM, 257–269.
https://doi.org/10.1145/
2771783.2771796

[10] Michail Basios, Lingbo Li, Fan Wu, Leslie Kanthan, and Earl T. Barr. 2017. Optimising Darwinian Data Struc-
tures on Google Guava. In Proceedings of the 9th International Symposium on Search Based Software Engi-
neering (SSBSE 2017) (Lecture Notes in Computer Science, Vol. 10452), Tim Menzies and Justyna Petke (Eds.).
Springer, 161–167. https://doi.org/10.1007/978-3-319-66299-2_14

[11] Michail Basios, Lingbo Li, Fan Wu, Leslie Kanthan, and Earl T. Barr. 2018. Darwinian data structure selec-
tion. In Proceedings of the 26th ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE 2018), Gary T. Leavens, Alessandro Garcia, and Corina S.
Pasareanu (Eds.). ACM, 118–128. https://doi.org/10.1145/3236024.3236043

[12] David W. Binkley, Nicolas E. Gold, Mark Harman, Syed S. Islam, Jens Krinke, and Shin Yoo. 2015. ORBS and
the limits of static slicing. In Proceedings of the 15th IEEE International Working Conference on Source Code
Analysis and Manipulation (SCAM 2015), Michael W. Godfrey, David Lo, and Foutse Khomh (Eds.). IEEE,
1–10. https://doi.org/10.1109/SCAM.2015.7335396

[13] Aymeric Blot and Justyna Petke. 2020. Comparing Genetic Programming Approaches for Non-Functional Ge-
netic Improvement – Case Study: Improvement of MiniSAT’s Running Time. In Proceedings of the 23th Eu-
ropean Conference on Genetic Programming (EuroGP 2020) (Lecture Notes in Computer Science, Vol. 12101),
Ting Hu, Nuno Lourenc¸o, Eric Medvet, and Federico Divina (Eds.). Springer, 68–83. https://doi.org/10.
1007/978-3-030-44094-7_5

[14] Aymeric Blot and Justyna Petke. 2021. Empirical Comparison of Search Heuristics for Genetic Improvement of
Software. IEEE Transactions on Evolutionary Computation 25, 5 (2021), 1001–1011. https://doi.org/10.
1109/TEVC.2021.3070271

[15] Mahmoud A. Bokhari, Bobby R. Bruce, Brad Alexander, and Markus Wagner. 2017. Deep Parameter Opti-
misation on Android Smartphones for Energy Minimisation – A Tale of Woe and a Proof-of-Concept. In 3rd
International Workshop on Genetic Improvement, Companion Material Proceedings of the 12th Genetic and
Evolutionary Computation Conference (GI@GECCO 2017 in GECCO 2017 companion), Peter A. N. Bosman
(Ed.). ACM, 1501–1508. https://doi.org/10.1145/3067695.3082519

[16] Alexander E. I. Brownlee, Justyna Petke, Brad Alexander, Earl T. Barr, Markus Wagner, and David R. White.
2019. Gin: Genetic improvement research made easy. In Proceedings of the 14th Genetic and Evolutionary
Computation Conference (GECCO 2019), Anne Auger and Thomas St¨utzle (Eds.). ACM, 985–993. https:
//doi.org/10.1145/3321707.3321841

[17] Bobby R. Bruce, Jonathan M. Aitken, and Justyna Petke. 2016. Deep Parameter Optimisation for Face Detection
Using the Viola-Jones Algorithm in OpenCV. In Proceedings of the 8th International Symposium on Search
Based Software Engineering (SSBSE 2016) (Lecture Notes in Computer Science, Vol. 9962), Federica Sarro and
Kalyanmoy Deb (Eds.). Springer, 238–243. https://doi.org/10.1007/978-3-319-47106-8_18

[18] Leslie P´erez C´aceres, Federico Pagnozzi, Alberto Franzin, and Thomas St¨utzle. 2017. Automatic Conﬁgura-
tion of GCC Using Irace. In Proceedings of the 13th International Conference on Artiﬁcial Evolution, Revised
Selected Papers (EA 2017), Evelyne Lutton, Pierrick Legrand, Pierre Parrend, Nicolas Monmarch´e, and Marc
Schoenauer (Eds.). ACM, 202–216. https://doi.org/10.1007/978-3-319-78133-4_15

16

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

[19] Junjie Chen, Ningxin Xu, Peiqi Chen, and Hongyu Zhang. 2021. Efﬁcient Compiler Autotuning via Bayesian
Optimization. In Proceedings of the 43rd International Conference on Software Engineering (ICSE 2021). IEEE,
1198–1209. https://doi.org/10.1109/ICSE43902.2021.00110

[20] Michael L. Collard and Jonathan I. Maletic. 2016. srcML 1.0: Explore, Analyze, and Manipulate Source Code.
In Proceedings of the 32nd International Conference on Software Maintenance and Evolution (ICSME 2016).
IEEE, 649. https://doi.org/10.1109/ICSME.2016.36

[21] Alessio Colucci, D´avid Juh´asz, Martin Mosbeck, Alberto Marchisio, Semeen Rehman, Manfred Kreutzer, Guen-
ther Nadbath, Axel Jantsch, and Muhammad Shaﬁque. 2020. MLComp: A Methodology for Machine Learning-
based Performance Estimation and Adaptive Selection of Pareto-Optimal Compiler Optimization Sequences.
Computing Research Repository abs/2012.05270 (2020). https://arxiv.org/abs/2012.05270

[22] A. E. Eiben, Robert Hinterding, and Zbigniew Michalewicz. 1999. Parameter control in evolutionary algorithms.
IEEE Transactions on Evolutionary Computation 3, 2 (1999), 124–141. https://doi.org/10.1109/4235.
771166

[23] Deji Fatiregun, Mark Harman, and Robert M. Hierons. 2003. Search Based Transformations. In Proceedings
of the 3rd Genetic and Evolutionary Computation Conference, Part II (GECCO 2003) (Lecture Notes in Com-
puter Science, Vol. 2724), Erick Cant’u-Paz, James A. Foster, Kalyanmoy Deb, Lawrence Davis, Rajkumar Roy,
Una-May O’Reilly, Hans-Georg Beyer, Russell K. Standish, Graham Kendall, Stewart W. Wilson, Mark Har-
man, Joachim Wegener, Dipankar Dasgupta, Mitchell A. Potter, Alan C. Schultz, Kathryn A. Dowsland, Natasa
Jonoska, and Julian F. Miller (Eds.). ACM, 2511–2512. https://doi.org/10.1007/3-540-45110-2_154
[24] Chris Fawcett, Lars Kotthoff, and Holger H. Hoos. 2017. Hot-Rodding the Browser Engine: Automatic
http:

Conﬁguration of JavaScript Compilers. Computing Research Repository abs/1707.04245 (2017).
//arxiv.org/abs/1707.04245

[25] Unai Garciarena and Roberto Santana. 2016. Evolutionary optimization of compiler ﬂag selection by learning
and exploiting ﬂags interactions. In 2nd International Workshop on Genetic Improvement, Companion Material
Proceedings of the 11th Genetic and Evolutionary Computation Conference (GI@GECCO 2016 in GECCO 2016
companion). ACM, 1159–1166. https://doi.org/10.1145/2908961.2931696

[26] Kyriakos Georgiou, Craig Blackmore, Samuel Xavier de Souza, and Kerstin Eder. 2018. Less is More: Exploiting
the Standard Compiler Optimization Levels for Better Performance and Energy Consumption. In Proceedings
of the 21st International Workshop on Software and Compilers for Embedded Systems (SCOPES 2018), Sander
Stuijk (Ed.). ACM, 35–42. https://doi.org/10.1145/3207719.3207727

[27] Kyriakos Georgiou, Zbigniew Chamski, Andr´es Amaya Garc´ıa, David May, and Kerstin Eder. 2022. Lost In
Translation: Exposing Hidden Compiler Optimization Opportunities. Comput. J. 65, 3 (2022), 718–735. https:
//doi.org/10.1093/comjnl/bxaa103

[28] Giovani Gracioli, Ahmed Alhammad, Renato Mancuso, Antˆonio Augusto Fr¨ohlich, and Rodolfo Pellizzoni.
2015. A Survey on Cache Management Mechanisms for Real-Time Embedded Systems. Comput. Surveys 48, 2
(2015), 32:1–32:36. https://doi.org/10.1145/2830555

[29] Kaiyuan Guo, Shulin Zeng, Jincheng Yu, Yu Wang, and Huazhong Yang. 2019. A Survey of FPGA-based Neural
Network Inference Accelerators. ACM Transactions on Reconﬁgurable Technology and Systems 12, 1 (2019),
2:1–2:26. https://doi.org/10.1145/3289185

[30] Youssef Hamadi, Eric Monfroy, and Fr´ed´eric Saubion (Eds.). 2012. Autonomous Search. Springer. https:

//doi.org/10.1007/978-3-642-21434-9

[31] Saemundur O. Haraldsson, John R. Woodward, Alexander E. I. Brownlee, and Kristin Siggeirsdottir. 2017.
Fixing Bugs in Your Sleep – How Genetic Improvement Became an Overnight Success. In 3rd International
Workshop on Genetic Improvement, Companion Material Proceedings of the 12th Genetic and Evolutionary
Computation Conference (GI@GECCO 2017 in GECCO 2017 companion), Peter A. N. Bosman (Ed.). ACM,
1513–1520. https://doi.org/10.1145/3067695.3082517

[32] Holger H. Hoos and Thomas St¨utzle. 2004. Stochastic Local Search: Foundations & Applications. Elsevier /

Morgan Kaufmann.

[33] Kenneth Hoste and Lieven Eeckhout. 2008. COLE: Compiler optimization level exploration. In Proceedings
of the 6th International Symposium on Code Generation and Optimization (CGO 2008), Mary Lou Soffa and
Evelyn Duesterwald (Eds.). ACM, 165–174. https://doi.org/10.1145/1356058.1356080

[34] Changwu Huang, Yuanxiang Li, and Xin Yao. 2020. A Survey of Automatic Parameter Tuning Methods for
Metaheuristics. IEEE Transactions on Evolutionary Computation 24, 2 (2020), 201–216. https://doi.org/
10.1109/TEVC.2019.2921598

17

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

[35] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2011. Sequential Model-Based Optimization for Gen-
eral Algorithm Conﬁguration. In Proceedings of the 5th International Conference on Learning and Intelligent
Optimization, Selected Papers (LION 5) (Lecture Notes in Computer Science, Vol. 6683), Carlos A. Coello Coello
(Ed.). Springer, 507–523. https://doi.org/10.1007/978-3-642-25566-3_40

[36] Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown, and Thomas St¨utzle. 2009. ParamILS: An Automatic
Algorithm Conﬁguration Framework. Journal of Artiﬁcial Intelligence Research 36 (2009), 267–306. https:
//doi.org/10.1613/jair.2861

[37] Frank Hutter, Holger H. Hoos, and Thomas St¨utzle. 2007. Automatic Algorithm Conﬁguration Based on Local
Search. In Proceedings of the 22th AAAI Conference on Artiﬁcial Intelligence (AAAI 2007). AAAI Press, 1152–
1157.

[38] Frank Hutter, Manuel L´opez-Ib´a˜nez, Chris Fawcett, Marius Thomas Lindauer, Holger H. Hoos, Kevin Leyton-
Brown, and Thomas St¨utzle. 2014. AClib: A Benchmark Library for Algorithm Conﬁguration. In Proceedings
of the 8th International Conference on Learning and Intelligent Optimization, Revised Selected Papers (LION
8) (Lecture Notes in Computer Science, Vol. 8426), Panos M. Pardalos, Mauricio G. C. Resende, Chrysaﬁs
Vogiatzis, and Jose L. Walteros (Eds.). Springer, 36–40. https://doi.org/10.1007/978-3-319-09584-4_
4

[39] Pascal Kerschke, Holger H. Hoos, Frank Neumann, and Heike Trautmann. 2019. Automated Algorithm Selec-
tion: Survey and Perspectives. Evolutionary Computation 27, 1 (2019), 3–45. https://doi.org/10.1162/
evco_a_00242

[40] Lars Kotthoff. 2014. Algorithm Selection for Combinatorial Search Problems: A Survey. AI Magazine 35, 3

(2014), 48–60. https://doi.org/10.1609/aimag.v35i3.2460

[41] John R. Koza. 1992. Genetic programming. MIT Press.

[42] Oliver Krauss and William B. Langdon. 2020. Automatically Evolving Lookup Tables for Function Approxima-
tion. In Proceedings of the 23th European Conference on Genetic Programming (EuroGP 2020) (Lecture Notes
in Computer Science, Vol. 12101), Ting Hu, Nuno Lourenc¸o, Eric Medvet, and Federico Divina (Eds.). Springer,
84–100. https://doi.org/10.1007/978-3-030-44094-7_6

[43] William B. Langdon and Mark Harman. 2015. Optimizing Existing Software With Genetic Programming. IEEE
Transactions on Evolutionary Computation 19, 1 (2015), 118–135. https://doi.org/10.1109/TEVC.2013.
2281544

[44] William B. Langdon and Oliver Krauss. 2021. Genetic Improvement of Data for Maths Functions. ACM Trans-
https://doi.org/10.1145/

actions on Evolutionary Learning and Optimization 1, 2 (2021), 7:1–7:30.
3461016

[45] William B. Langdon and Justyna Petke. 2018. Evolving Better Software Parameters. In Proceedings of the 10th
International Symposium on Search Based Software Engineering (SSBSE 2018) (Lecture Notes in Computer
Science, Vol. 11036), Thelma Elita Colanzi and Phil McMinn (Eds.). Springer, 363–369. https://doi.org/
10.1007/978-3-319-99241-9_22

[46] William B. Langdon and Riccardo Poli. 2002. Foundations of genetic programming. Springer.

[47] Shuyue Stella Li, Hannah Peeler, Andrew N. Sloss, Kenneth N. Reid, and Wolfgang Banzhaf. 2022. Genetic
improvement in the shackleton framework for optimizing LLVM pass sequences. In 11th International Workshop
on Genetic Improvement, Companion Material Proceedings of the 16th Genetic and Evolutionary Computation
Conference (GI@GECCO 2022 in GECCO 2022 companion), Jonathan E. Fieldsend and Markus Wagner (Eds.).
ACM, 1938–1939. https://doi.org/10.1145/3520304.3534000

[48] Manuel L´opez-Ib´a˜nez, J´er´emie Dubois-Lacoste, Leslie P´erez C´aceres, Mauro Birattari, and Thomas St¨utzle.
2016. The irace package: Iterated racing for automatic algorithm conﬁguration. Operations Research Perspec-
tives 3 (2016), 43–58. https://doi.org/10.1016/j.orp.2016.09.002

[49] J Michael McQuade, Richard M Murray, Gilman Louie, Milo Medin, Jennifer Pahlka, and Trae Stephens. 2019.
Software is never done: Refactoring the acquisition code for competitive advantage. Technical Report. Wash-
ington DC: Defense Innovation Board.

[50] Dmitry Melnik, Andrey Belevantsev, Dmitry Plotnikov, and Semun Lee. 2010. Case study: Optimizing GCC
on ARM for performance of libevas rasterization library. In Proceedings of the International Workshop on GCC
Research Opportunities (GROW 2010).

[51] Tom Mens and Tom Tourw´e. 2004. A Survey of Software Refactoring. IEEE Transactions on Software Engi-

neering 20, 2 (2004), 126–139. https://doi.org/10.1109/TSE.2004.1265817

18

MAGPIE: Machine Automated General Performance Improvement via Evolution of SoftwareA PREPRINT

[52] Sparsh Mittal. 2020. A survey of FPGA-based accelerators for convolutional neural networks. Neural Computing

and Applications 32, 4 (2020), 1109–1139. https://doi.org/10.1007/s00521-018-3761-1

[53] Vinicius Paulo L. Oliveira, Eduardo Faria de Souza, Claire Le Goues, and Celso G. Camilo-Junior. 2018. Im-
proved representation and genetic operators for linear genetic programming for automated program repair. Em-
pirical Software Engineering 23, 5 (2018), 2980–3006. https://doi.org/10.1007/s10664-017-9562-9
[54] Camille Pageau, Aymeric Blot, Holger H. Hoos, Marie- ´El´eonore Kessaci, and Laetitia Jourdan. 2019. Con-
ﬁguration of a Dynamic MOLS Algorithm for Bi-objective Flowshop Scheduling. In Proceedings of the 10th
International Conference on Evolutionary Multi-Criterion Optimization (EMO 2019) (Lecture Notes in Com-
puter Science, Vol. 11411), Kalyanmoy Deb, Erik D. Goodman, Carlos A. Coello Coello, Kathrin Klamroth,
Kaisa Miettinen, Sanaz Mostaghim, and Patrick Reed (Eds.). Springer, 565–577.
https://doi.org/10.
1007/978-3-030-12598-1_45

[55] Justyna Petke, Saemundur O. Haraldsson, Mark Harman, William B. Langdon, David R. White, and John R.
Woodward. 2018. Genetic Improvement of Software: A Comprehensive Survey. IEEE Transactions on Evolu-
tionary Computation 22, 3 (2018), 415–432. https://doi.org/10.1109/TEVC.2017.2693219

[56] Justyna Petke, Mark Harman, William B. Langdon, and Westley Weimer. 2018. Specialising Software for Dif-
ferent Downstream Applications Using Genetic Improvement and Code Transplantation. IEEE Transactions on
Software Engineering 44, 6 (2018), 574–594. https://doi.org/10.1109/TSE.2017.2702606

[57] Dmitry Plotnikov, Dmitry Melnik, Mamikon Vardanyan, Ruben Buchatskiy, Roman Zhuykov, and Je-Hyung
Lee. 2013. Automatic Tuning of Compiler Optimizations and Analysis of their Impact. In Proceedings of the
2013 International Conference on Computational Science (ICCS 2013) (Procedia Computer Science, Vol. 18),
Vassil N. Alexandrov, Michael Lees, Valeria V. Krzhizhanovskaya, Jack J. Dongarra, and Peter M. A. Sloot
(Eds.). Elsevier, 1312–1321. https://doi.org/10.1016/j.procs.2013.05.298

[58] Riccardo Poli, William B. Langdon, and Nicholas Freitag McPhee. 2008. A Field Guide to Genetic Programming.

lulu.com. http://www.gp-field-guide.org.uk/

[59] Marta Smigielska, Aymeric Blot, and Justyna Petke. 2021. Uniform Edit Selection for Genetic Improvement –
Empirical Analysis of Mutation Operator Efﬁcacy. In Proceedings of the 10th International Workshop on Genetic
Improvement (GI@ICSE 2021). IEEE, 1–8. https://doi.org/10.1109/GI52543.2021.00009

[60] Chris Thornton, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2013. Auto-WEKA: Combined se-
lection and hyperparameter optimization of classiﬁcation algorithms. In Proceedings of the 19th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining (KDD 2013). ACM, 847–855.
https:
//doi.org/10.1145/2487575.2487629

[61] Westley Weimer. 2006. Patches as better bug reports. In Proceedings of the 5th International Conference on
Generative Programming and Component Engineering (GPCE 2006), Stan Jarzabek, Douglas C. Schmidt, and
Todd L. Veldhuizen (Eds.). ACM, 181–190. https://doi.org/10.1145/1173706.1173734

[62] David R. White, Leonid Joffe, Edward Bowles, and Jerry Swan. 2017. Deep Parameter Tuning of Concurrent
Divide and Conquer Algorithms in Akka. In Proceedings of the 20th European Conference on Applications of
Evolutionary Computation, Part II (EvoApp 2017) (Lecture Notes in Computer Science, Vol. 10200), Giovanni
Squillero and Kevin Sim (Eds.). Springer, 35–48. https://doi.org/10.1007/978-3-319-55792-2_3
[63] Fan Wu, Westley Weimer, Mark Harman, Yue Jia, and Jens Krinke. 2015. Deep Parameter Optimisation. In
Proceedings of the 10th Genetic and Evolutionary Computation Conference (GECCO 2015). ACM, 1375–1382.
https://doi.org/10.1145/2739480.2754648

[64] Eckart Zitzler, Marco Laumanns, and Lothar Thiele. 2001. SPEA2: Improving the strength Pareto evolutionary

algorithm. TIK-report 103 (2001).

19

