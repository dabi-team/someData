2
2
0
2

r
a

M
1
3

]

O
L
.
s
c
[

1
v
6
7
0
0
0
.
4
0
2
2
:
v
i
X
r
a

Reachability Logic for Low-Level Programs

Nico Naus1, Freek Verbeek1,2, Marc Schoolderman3, and Binoy Ravindran1

1 Virginia Tech, Blacksburg VA, USA {niconaus,freek,binoy}@vt.edu
2 Open Univeristy, The Netherlands fvb@ou.nl
3 Radboud University Nijmegen, The Netherlands m.schoolderman@cs.ru.nl

Abstract. Automatic exploit generation is a relatively new area of re-
search. Work in this area aims to automate the manual and labor inten-
sive task of ﬁnding exploits in software. In this paper we present a novel
program logic to support automatic exploit generation. We develop a
program logic called Reachability Logic, which formally deﬁnes the re-
lation between reachability of an assertion and the preconditions which
allow them to occur. This relation is then used to calculate the search
space of preconditions. We show that Reachability Logic is a powerful
tool in automatically ﬁnding evidence that an assertion is reachable. We
verify that the system works for small litmus tests, as well as real-world
algorithms. An implementation has been developed, and the entire sys-
tem is proven to be sound and complete in a theorem prover. This work
represents an important step towards formally veriﬁed automatic exploit
generation.

Keywords: Formal veriﬁcation · Formal Methods · Reachability analy-
sis · Automatic Exploit Generation

1 Introduction

Exploit generation is the task of ﬁnding a security vulnerability in a program,
as well as a way to leverage that vulnerability. This is typically seen as a labori-
ous manual task requiring intricate knowledge of the system under investigation.
In contrast, recent studies consider automated exploit generation (AEG) [6,30].
Even though AEG is “still in its infancy” [2], the knowledge obtained by the ex-
posed exploits has already proven to be valuable to guard and improve software.
Exploit generation, from an abstract point of view, is about reasoning over
reachability of unwanted states. For example, a common exploit is a buﬀer over-
ﬂow that overwrites the return address, stored at the top of the stack frame.
For such exploits, the problem of AEG can be reformulated to deciding whether,
just before a return, a state is reachable in which the return address has been
modiﬁed with respect to the initial state. Other types of exploits may aim to
redirect control ﬂow using indirect (dynamically computed) jumps. For such ex-
ploits, the problem of AEG can be reformulated as a decision problem whether
a state is reachable in which an indirection leads to an arbitrary address. To
summarize, a large part of AEG can be reduced to reasoning over reachability
of assertions in low-level programs.

 
 
 
 
 
 
2

Naus et al.

Current program logics are not suitable for reasoning over reachability of
assertions. Hoare Logic [16] reasons over all possible execution paths to prove
correctness of a program with regards to some property. In AEG, a vulnerable
state will typically only occur in some corner case, making Hoare Logic unsuit-
able. Reverse Hoare Logic [29] reasons over total reachability. This means that
it deﬁnes the relation between a state and all execution paths that lead to it.
Listing every possible path that leads to a certain state is diﬃcult, if not unfea-
sible, and makes Reverse Hoare Logic unfeasible for AEG. Knowing that there
is one path that leads to the assertion under investigation is enough.

This paper proposes a novel theoretical foundation for reachability, that can
be used for AEG. The ﬁrst step is to formalize an academic programming lan-
guage similar to the well-known While [16] language. Whereas While is in-
tended to be an abstract model of high-level programming languages, this paper
proposes jump as an abstract model of low-level representations of executable
behavior such as assembly or LLVM IR [22]. Reason is that AEG typically applies
to executables: it considers low-level properties (e.g., stack overﬂows or return-
oriented-programming) on low-level code. The language Jump is characterized
by being low-level, having unstructured control-ﬂow (jumps instead of loops)
and an unstructured ﬂat memory model. Moreover, it is non-deterministic, al-
lowing us to model the uncertainty of the semantics of various constructs found
in executables [14,11].

In a similar fashion to Hoare Logic being deﬁned for While, this paper pro-
poses Reachability Logic (RL) for jump. Reachability Logic consists of triples
with a precondition, a program, and a postcondition. Assuming the postcondi-
tion models a state, a reachability triple states that if the precondition holds,
there exists at least one path to a state that satisﬁes the postcondition. Whereas
Hoare Logic is intended to reason over correctness, RL is thus intended to reason
over reachability.

Hoare Logic comes with a proof methodology (precondition generation) that
allows establishing correctness of a program. In similar fashion, we provide a
methodology for precondition generation for RL. A key observation is that the
nature of precondition generation between the two logics diﬀer. Hoare Logic
requires over-approximative knowledge, e.g., loop invariants and full path explo-
ration, as it is intended for veriﬁcation. Reachability Logic allows formal reason-
ing with under-approximative knowledge: no invariants or full path exploration
is necessary. We show that Reachability Logic deﬁnes a traversable search space.
The precondition that shows reachability, if any paths exist, is somewhere in that
search space. The precondition generation is proven to be sound and complete;
we know that the search space describes only actual reachability evidence, and
that it describes all possible ways to reach the intended state. The cost of hav-
ing both soundness and completeness, is that the search space becomes inﬁnite.
However, ﬁnding one path from assertion to initial state suﬃces and thus there
is no need for full search space traversal to ﬁnd exploits.

We demonstrate applicability of the proposed theory in two ways. First,
we implement a search space generator for RL and demonstrate on various

Reachability Logic for Low-Level Programs

3

Name
Hoare Logic
Reverse Hoare
Reachability Logic hP i p hQi ≡ ∀ σ ·P (σ) =⇒ ∃ σ′

Deﬁnition
{P } p {Q} ≡ ∀ σ ·P (σ) =⇒ ∀ σ′
[P ] p [Q] ≡ ∀ σ′

· σ
) =⇒ ∃ σ · σ
· σ

· Q(σ′

Usage
=⇒ Q(σ′
Correctness
∧ P (σ) Total reachability
∧ Q(σ′
Reachability

)

)

p

p

−−→ σ′
−−→ σ′
−−→ σ′

p

Table 1. Overview of program logics. Arrow
duced by program p.

p

−−→ denotes the transition relation in-

litmus tests how a search space is generated, traversed and preconditions are
found. Second, we consider two realistic assembly implementations (Quicksort
and Karatsuba multiplication) and model them in Jump. We show how evidence
of reachability is automatically generated.

To summarize, the contributions of this paper are:

– A novel logic for reasoning over reachability of assertions in low-level pro-

grams;

– A formally proven correct precondition function that automatically computes

initializations leading to states described by the postcondition;

– The application of this logic to various litmus tests as well as two assembly

implementations.

All results, source code and the formalized proof of correctness in Isabelle /

HOL [24,21,12] are publicly available 4.

Section 2 introduces RL and argues why existing logics are not suitable for
AEG. Section 3 and 4 introduce the jump language and the exploit generation
mechanism for it. Litmus tests and applications are described in Sections 5 and 6.
Section 7 discusses the search space. Related work is discussed in Section 8 before
we conclude in Section 9.

2 Reachability Logic compared to other program logics

Reachability Logic (RL) revolves around triples hP i p hQi, for some program
p. Here, Q is a state-predicate that describes a state to be reached. State-
predicate P represents from where a Q state is reachable: how must the machine
be initialized for Q to be possible? These predicates thus formulate properties
over states, which assign values to memory and registers, including heap, stack
frame, and instruction pointer.

Table 1 provides deﬁnitions for both some existing logics and the proposed
Reachability Logic. Both Hoare Logic (HL) and Reverse Hoare Logic (RHL) can
be extended with a frame rule, producing a “separation” version of the logic [25].
We consider the most elementary versions of these logics. A reachability triple
hP i p hQi expresses that for any state satisfying predicate P there exists some
non-deterministic execution such that a state satisfying Q is reached.

4 https://github.com/niconaus/reachability-logic

4

Naus et al.

Hoare Logic HL [16] shows absence of bugs, with respect to some property,
or, in other words, program correctness. We argue why it cannot be used for
reachability. First, consider an approach based on deriving an HL triple of the
form {P } p {Q}. This would express that all executions of the program lead to a
desired state, which is unrealistic. Typically only a few cases lead to the desired
state and as such this approach would often lead to P = False providing no
information on reachability. Second, consider an approach based on deriving a HL
triple of the form {P } p {¬Q}. The intuition would be to derive preconditions P
that ensure that the state cannot be reached. The negation of such a precondition
may then be an initialization where the exploit does happen.

However, there are various counterarguments to this hypothetical approach.
First, by deﬁnition of Hoare triples, an initial state satisfying ¬P does not neces-
sarily lead to the Q-state, i.e, this approach does not show reachability. Second,
the negation of P can grossly over-approximate the set of initial states that may
lead to the Q-state, even in the case of a hypothetical perfect weakest precondi-
tion generation (which is infeasible in practice due to scalability issues [4]).

As a ﬁnal argument against the use of HL for reachability, we argue that
HL inherently requires reasoning over all execution paths. It requires full path
exploration and invariants over loops, which is known as the Achilles’ heel of
formal veriﬁcation. A logic to be used for AEG should not require any over-
approximative reasoning: it reasons over some path, or some number of iterations
of a loop. A logic for AEG should not require loop invariants.

Reverse Hoare Logic RHL [29] shows total reachability. Speciﬁcally, it shows
that all Q-states are reachable from a P -state. RHL requires P to characterize
all states leading to Q which can easily become infeasible. We illustrate this with
the RHL triple below.

[w ≡ 100] v := w [v > 42]

From precondition w ≡ 100, there is a valid path to a state satisfying postcon-
dition v > 42. However, looking at the deﬁnition of RHL in Table 1, this is not
a valid RHL triple. For all states that satisfy the postcondition, there must be
a state that satisﬁes the precondition that leads to it. For example, v = 43 is a
state that satisﬁes the postcondition, but no state exists that satisﬁes w ≡ 100
leads to it. RHL triples need to specify every state that leads to the postcon-
dition. In the example above, this is trivial, but in more involved cases, this
will quickly become infeasible. In the case of loops, the precondition might be
inﬁnitely big, or require a manually written loop variant. This makes RHL un-
suitable as a foundation for AEG. For the purpose of AEG, it satisﬁes to ﬁnd
one path, and indeed the following RL-triple holds: hw ≡ 100i v := w hv > 42i.
In other words, the universal quantiﬁcation in RHL triples again requires a form
of overapproximative reasoning (speciﬁcally: loop invariants), which should not
be necessary in the case of AEG.

Incorrectness Logic IL [23] has its basis in RHL, but with some changes. The
postcondition has an added exit-condition, indicating if the program terminated

Reachability Logic for Low-Level Programs

5

in an error or not. Like RHL, IL uses manually deﬁned loop variants to deal
with loops. Finally, IL replaces conditionals by so called assumes, that assume
the conditional to hold or not, depending on the selected branch. While these
changes make it easier to work with IL, the triples are still the same as in RHL.
Hoare Logic, Reverse Hoare Logic and Incorrectness Logic are not suitable for
reasoning about reachability in the context of AEG. The key diﬀerence between
Reachability Logic and existing logics is that Reachability Logic is the only logic
that satisﬁes all of the following: 1.) RL is able to reason over the existence
and reachability of paths, 2.) RL allows under-approximative semantics of the
program p under investigation, and 3.) is able to deal with non-determinism.
Reachability Logic instead does allow us to express the relation between a post-
condition to be reached and an initial state conﬁguration. RL is suitable for
not only describing this relation, but also automatically generating precondi-
tions. The coming sections describe RL in more detail, including precondition
generation and examples to illustrate this process.

3 The jump language

The jump language is intended as an abstract representation of low-level lan-
guages such as LLVM IR [22] or assembly. It has no explicit control-ﬂow; instead
it has jumps to addresses. It consists of basic blocks of elementary statements
that end with either a jump or an exit. Blocks are labeled with addresses. Mem-
ory is modeled as a mapping from addresses to values (see Deﬁnition 2 below).
Variables from a set V represent registers and ﬂags. The values stored in variables
or memory are words (bit-vectors) of type W.

The following design decisions have been made regarding jump.

Non-determinism We explicitly include non-determinism through an Obtn
statement that allows to retrieve some value out of a set. Non-determinism
allows modeling of external functions whose behavior is unknown, allows
dealing with uncertain semantics of assembly instructions and allows mod-
eling user-input and IO. The Obtn statement is the only source of non-
determinism in jump.

Unstructured memory Memory essentially consists of a ﬂat mapping of ad-
dresses to values. There is no explicit notion of heap, stack frame, data
section, or global variables. This is purposefully chosen as it allows to reason
over pointer aliasing. For example, it allows Reachability Logic to formulate
statements as “the initial value of this pointer should be equal to the initial
value of register rsp” which is interpreted as a pointer pointing to the return
address at the top of the stack frame.

No structured control-ﬂow All control ﬂow happens through either condi-
tional jumps or indirect jumps. Indirect control ﬂow is typically introduced
by a compiler in case of switch-statements, callbacks, and to implement dy-
namic dispatch. Note that a normal instruction such as the x86 instruction
ret implicitly is an indirect jump as well.

6

Naus et al.

Block
b = s; b | Exit

| Jump e a1 a2 | IndirJmp e

Statement
s = Assign v e

| Obtn v Where e
| Store e v

Expression
e = w | v | !e | e1 ⊕ e2 | ¬ e

Sequence, Exit
Conditional jump, Indirect jump

Variable assignment
Nondeterministic assign
Store v in address e

Value, variable, deref, operation, negate

⊕ ∈ {+, −, ×, %, <, ≤, ≡, 6=, >, ≥, ∧, ∨, . . .} Binary operators

Fig. 1. jump

Deﬁnition 1. A jump program p is deﬁned as the pair (a0, blocks) where a0 is
the entry address, and blocks is a mapping from addresses to blocks. A block is
deﬁned by the grammar in Figure 1.

A block consists of a sequence of zero or more statements, followed by either
a conditional jump, indirect jump or Exit. The conditional jump jumps to the
address a1 only if the given expression evaluates to zero, otherwise to address
a2. The indirect jump calculates the value of e and jumps to the block at that
address. Statements can be assignments or stores. A deterministic assignment
writes the value of expression e to variable v. A nondeterministic assignment
writes some value from a set to variable v, deﬁned by any value w such that
expression e[v := w] evaluates to non-zero. Note that since expressions can read
from memory, an assignment can model a load-instruction. A store writes the
value of variable v into memory. Expressions consist of values, variables, deref-
erencing, binary operations and negation.

The state consists of values assigned to variables and memory. We ﬁrst deﬁne

the memory model.

Deﬁnition 2. The tuple (M, R, write, read) is a memory model, where M is
the type of the memory itself and A is the type of addresses. Function write is of
type A × W × M 7→ M and function read is of type A × M 7→ W. A memory
model must satisfy:

read(a, write(a′, w, m)) =

read(a, m) if a = a′
if a 6= a′
w

(cid:26)

We assume values can bijectively be cast to addresses and we do so freely.

Deﬁnition 3. A state σ is a tuple (mem, vars) where mem is of type M and
vars is of type V 7→ W.

Semantics are expressed through transition relations −→P, −→B and −→S
that respectively deﬁne state transitions induced by programs, blocks, and state-
ments (see Figure 2). For example, notation p : σ −→P σ′ denotes a transition

Reachability Logic for Low-Level Programs

7

prog blocks(a0) : σ −→B σ
(a0, blocks) : σ −→P σ

′

′

jumpTrue σ ⊢ e 6= 0

seq s : σ −→S σ

′

′

′′

−→B σ

b : σ
′′

s;b : σ −→B σ

′

blocks(a1) : σ −→B σ

Jump e a1 a2 : σ −→B σ

jumpFalse σ ⊢ e = 0

blocks(a2) : σ −→B σ

′

indirectJump σ ⊢ e = a

Jump e a1 a2 : σ −→B σ

blocks(a) : σ −→B σ

′

IndirJmp e : σ −→B σ

′

exit

Exit : σ −→B σ

′

′

assign

σ ⊢ e = w
Assign v e : (mem, vars) −→S (mem, vars[v := w])

store

(mem, vars) ⊢ e1 = a

σ ⊢ e2 = w

Store e1 e2 : (mem, vars) −→S (write(a, w, mem), vars)

ndassign

σ ⊢ e[v := w] 6= 0
Obtn v Where e : (mem, vars) −→S (mem, vars[v := w])

load (mem, vars) ⊢ e = a

read(a, mem) = w

(mem, vars) ⊢ !e = w

Fig. 2. Semantics of jump. Rules for evaluation of expressions are omitted, except for
the dereference operator.

induced by program p from state σ to state σ′. Notation σ ⊢ e = w denotes the
evaluation of expression e in state σ to value w.

The semantics are largely straightforward. A program is evaluated by evalu-
ating the block pointed to by the entry address. A conditional jump is evaluated
by evaluating the condition, and then the target block. Indirect jumps are eval-
uated in a similar manner, by evaluating the expression to obtain the block
to jump to. Non-standard is the nondeterministic assignment ndassign, which
evaluates expression e after substituting the variable v for some value w. For
any value w where expression e evaluates to non-zero, a transition may occur.
A store evaluates expression e producing some address a, and writes the value
of variable v to the corresponding region in memory. A load uses function read
to read from memory.

4 Precondition generation

This section formalizes precondition generation functions for Reachability Logic.
The central idea is to formulate a transformation function τ that takes as input
1.) a program p, and 2.) a postcondition Q, and produce as output a disjunctive
set of preconditions. This transformation function follows the recursive structure
of jump, i.e., we formulate functions τP , τB and τS that perform transformations
relative to a program, a block and a statement respectively.

When applied statement-by-statement, these functions populate the Reach-
ability search space. This search space is an acyclic graph, with symbolic predi-
cates as vertices and as root the initial postcondition. It contains a labeled edge
(Q, s, P ) if and only if application of function τS for statement s and postcondi-
tion Q produces a set containing precondition P .

Below, the deﬁnition of predicates that form pre- and postconditions is given.

8

Naus et al.

Predicate

P = ∃ i ∈ e · P | e Existential quantiﬁcation, expression

Predicates P are expressions (true if and only if they evaluate to non-zero),
but can also contain outermost existential quantiﬁers. The predicate ∃i ∈ e · P
means there exists a value w for i such that both e[i := w] and P [i := w] hold.
Given a program p and a postcondition Q deﬁned in the predicate language
above, a transformation is sound if it generates preconditions P that form a
reachability triple. Soundness means that a generated precondition actually rep-
resents an initial state that non-deterministically leads to the Q-state. To deﬁne
soundness, we ﬁrst deﬁne the notion of a reachability triple relative to blocks
(instead of a program as a whole as in Section 2):

Deﬁnition 4. A reachability triple for block b is deﬁned as:

hP i b hQi ≡ ∀ σ ·P (σ) =⇒ ∃ σ′ · Q(σ′) ∧ (b : σ −→B σ′)

We repeat this deﬁnition to stress that a reachability triple over block b intu-
itively means that precondition P leads to the desired state when running the
block and subsequent blocks jumped to, until an exit, i.e, not just running the
instructions within block b itself. This is due to the nature of transition relation
−→B (see Figure 2). A similar deﬁnition can also be made for statements: a
reachability triple hP i s hQi for statement s is deﬁned for transition relation
−→S and thus concerns the execution of the individual statement s only.

Deﬁnition 5. Function τP is sound, if and only if, for any program p and
postcondition Q:

∀P ∈ τP (p, Q) · hP i p hQi

Similarly, soundness is deﬁned for blocks and statements.

Figure 3 shows the transformation functions. Function τ P starts at the entry
block of the program. The program is then traversed in the style of a right
fold [28]: starting at the entry the program is traversed up to an exit point,
from which postcondition transformation happens. Function τ B is identical to
standard weakest-precondition generation in the cases of sequence and exit. In
the case of a conditional jump, two paths are explored. Both could lead to
exploits, as long as the branching conditions remain internally consistent. In case
of an indirect jump, all possible addresses that can be jumped to, are explored.
Function τ S is standard in case of deterministic assignment. In case of nonde-
terministic assignment, according to the execution semantics, some value i needs
to be found that fulﬁlls the condition e. That existentially quantiﬁed value is
substituted for variable v in the postcondition.

In the case of memory assignment, predicate transformation is a bit more

complex. Consider the following example:

Store x 42; Store y 43 h!x ≡ 42i

If x and y alias, then !x will be 43 after execution. The postcondition !x ≡ 42
can only hold if x and y are separate.

Reachability Logic for Low-Level Programs

9

Program:
τP (p, Q)

= τB (blocks(a0), Q)

Block:
τB(s;b, Q)
τB(Jump e a1 a2, Q) = {P1 ∧ e | P1 ∈ τ b(blocks(a1), Q)} ∪ {P2 ∧ ¬e | P2 ∈ τ b(blocks(a2), Q)}
τB(IndirJmp e, Q)
τB(Exit, Q)

= {P ∧ e ≡ a | P ∈ τ b(blocks(a), Q), a ∈ dom(blocks)}
= {Q}

= S{τS(s, P ) | P ∈ τB (b, Q)}

Statement:
τS(Assign v e, Q)
τS(Obtn v Where e, Q) = {∃i ∈ e · Q[v := i]}
τS(Store e v, Q)

= {Q[v := e]}

= {Q′

∧ P | (Q′, P ) ∈ τstore(e, v, Q)}

Fig. 3. Transformation functions for a program p = (a0, blocks).

τstore(a, v, c)
τstore(a, v, x)
τstore(a1, v, !a2)
τstore(a, v, ¬ e)
1 ⊕ e′
τstore(a, v, e1 ⊕ e2)
τstore(a, v, ∃i ∈ e · P) = {(∃i ∈ e′

= {(c, True)}
= {(x, True)}
= {(!a2, a1 6= a2), (v, a1 ≡ a2)}
= {(¬ e′, p) | (e′, p) ∈ τstore(a, v, e)}
2, p1 ∧ p2) | (e′
= {(e′

1, p1) ∈ τstore(a, v, e1), (e′

2, p2) ∈ τstore(a, v, e2)}

· P′, p1 ∧ p2) | (e′, p1) ∈ τstore(a, v, e), (P′, p2) ∈ τstore(a, v, P)}

Fig. 4. Case deﬁnitions for precondition of store

We explicitly encode assumptions about memory separation into the gener-
ated preconditions. The τ store function listed in Figure 4 takes care of this. It
takes as input the address a to which value v is written, and the postcondition
P . It returns a set of tuples (Q, Qmem) where Q is the precondition and Qmem
provides the pointer-relations under which that substitution holds. For exam-
ple, we have τstore(a1, v, !a2 ≡ 42) = {(!a2 ≡ 42, a1 6= a2), (v ≡ 42, a1 ≡ a2)}.
This indicates two possible substitutions when transforming postcondition into
precondition:

h!a2 ≡ 42i Store a1 v h!a2 ≡ 42i if a1 6= a2
hv ≡ 42i Store a1 v h!a2 ≡ 42i if a1 ≡ a2

All other cases of τ store merely propagate the case generation.

There are no special rules for dealing with loops. Instead, loops are unrolled
by the precondition generation. In the case of inﬁnite iterations, the reachability
search space will be inﬁnitely large. To deal with this search space, we order
and prune the space. Theorem 1 states a basic property of exploit triples that
is used for the purpose of pruning. Section 5 describes how the space is ordered
to manage large search spaces.

Theorem 1 (Preservation of unsatisﬁability). For any program p and con-
ditions P and Q such that hP i p hQi,

(∀ σ′ ·Q(σ′) =⇒ False) =⇒ (∀ σ ·P (σ) =⇒ False)

The above can directly be concluded from the deﬁnition of a reachability
triple, as given in Section 2. Once an unsatisﬁable condition is generated, the
precondition generation can be halted, and the condition discarded.

10

Naus et al.

We validate our precondition generation function by proving it is both sound

and complete. Theorems 2 and 3 deﬁne these respective properties.

Theorem 2 (Soundness of precondition generation). Functions τP , τB
and τS are sound.

Theorem 3 (Completeness of precondition generation).

termination(p,P)

no_indirections(p)

hP i p hQi =⇒ ∃P ′.τP (p, Q) ∧ (P =⇒ P ′)

Having both soundness and completeness means that the reachability space
deﬁnes all and only valid preconditions for a certain program and postcondition.
Both theorems, including 1.) the syntax and semantics of jump, 2.) the syn-
tax and semantics of the predicates, and 3.) the functions τ have been for-
mally proven correct in the Isabelle/HOL theorem prover. The proof, including
a small example of exploit generation within Isabelle/HOL, constitutes roughly
1000 lines of code. Proof scripts are publicly available 5. To prove completeness,
Theorem 3 imposes two restrictions. One, we require execution of a program p
under a state described by P to terminate. If a program does not terminate, it is
impossible to construct a P’ for this program, and therefore completeness does
not hold. Two, we show the theorem holds for programs without indirect jumps.
In practice however, this premise has little to no impact. Every jump program
containing indirect jumps, can be manually converted to one with only direct
jumps. Given that P is a precondition for program p and postcondition Q, the
precondition generation will generate a P ′ that is non-strictly weaker than P .
An equivalent of Theorem 3 also holds for τS and τB.

5 Litmus tests

This section presents several litmus tests that demonstrate the functionality
of Reachability Logic. All of the examples have been tested in our prototype
implementation in Haskell, and are available online 6.

The prototype implements the τ functions similar to how they are presented
above. Some changes were made to make the system more user friendly. The
τ functions are deﬁned as non-deterministic functions, building up a tree as a
search space. Branches at the same level originate from a conditional, and deeper
branches indicate a deeper level of jumps. On top of that, a basic simpliﬁcation
step is applied to the generated predicates, to make them more readable.

The reachability search space can be inﬁnitely large. This is why the imple-
mentation builds up the search space as a tree structure. This orders the search
space, making it feasible to search the inﬁnite space in a structured way. Al-
though some rudimentary ordering is done, eﬃciently searching the reachability
space is explicitly left as future work.

Reachability Logic for Low-Level Programs

11

!e ≡ z ∧ e 6= b ∧ e 6= d

!a ≡ z ∧ e ≡ b ∧ e 6= d

!c ≡ z ∧ c 6= b ∧ e ≡ d

!a ≡ z ∧ c ≡ b ∧ e ≡ d

Store b !a;

Store d !c;

Exit

!e ≡ z ∧ e 6= d

!c ≡ z ∧ e ≡ d

!e ≡ z

!e ≡ z

Fig. 5. Precondition generation for double write example

x ≥ y ∧ x < y

(x − 3y) ≥ y ∧ ¬((x − 3y) > y)
∧((x − 2y) > y) ∧ (x − y) ≥ y ∧ ¬(x < y)

(x − y) ≥ y
∧¬((x − y) ≥ y)

(x − 2y) ≥ y
∧¬((x − 2y) > y)
∧((x − y) ≥ y)

(x − 3y) ≥ y ∧ ¬((x − 3y) > y)
∧((x − 2y) > y) ∧ (x − y) ≥ y

(x − y) ≥ y
∧¬((x − y) > y)

(x − 2y) ≥ y
∧¬((x − 2y) > y)
∧((x − y) > y)

· · ·

#0
Jump (x < y) #3 #1

#1
Assign i 1;
Assign x (x − y);
Jump (x ≥ y) #2 #3

#2
Assign x (x − y);
Assign i (i + 1);
Jump(x > y) #2 #3

#3
Exit

x ≥ y

x ≥ y

Fig. 6. Precondition generation for long division example. A dashed arrow leads to an
unsatisﬁable precondition.

Memory: Double store Our ﬁrst litmus test demonstrates how Reachability
Logic deals with symbolic memory access. Figure 5 lists the program on the left.
The program stores the content of memory position a in memory position b, and
then stores the contents of position c in position d. On the right, a schematic
representation of the precondition generation is given for postcondition !e ≡ z.
Precondition generation works from back to front, so we start at the bottom.
The Exit command leaves the condition unaltered. We then arrive at a Store
statement. Here, we split into two cases, one for the case where memory location
e and d are separate, one for the aliasing case. The second Store splits the
conditions up once more, for the separate and aligned cases. At the top of the
tree, we ﬁnd the ﬁnal preconditions.

Inﬁnite exploit space: Long division Our next litmus test demonstrates
conditional jumps, loops, inﬁnite reachability space and postcondition pruning.
Figure 6 lists the program blocks on the left. The blocks are labeled #0 though
#3, with block #0 is the entry point. Variables x and y signify the input. The
program divides x by y, by means of long division. If x is larger than y, the

5 https://github.com/niconaus/reachability-logic/tree/main/isabelle
6 https://github.com/niconaus/reachability-logic

 
 
 
12

Naus et al.

#0 : Obtn d Where 1 < d < n; Jump (n%d ≡ 0) #1 #0

#1 : Exit

∃i.(n%i ≡ 0) ∧ (1 < i) ∧ (i < n)

True

True

∃i.(n%i ≡ 0) ∧ (1 < i) ∧ (i < n) ∧ ∃j.¬((n%j) ≡ 0) ∧ (1 < j) ∧ (j < n)

Fig. 7. Precondition generation for nondeterministic loop example

· · ·

divisor without remainder is returned in variable i. The variable x is updated,
and after execution holds the remainder from division.

In this case, we want to derive that a state is reachable which clearly should
not be, to show that there is a bug in the program. The program behaves incor-
rectly when after execution, the remainder stored in x is equal to or larger than
the divisor y. We use this, x ≥ y, as our exploit postcondition.

The right of Figure 6 represents precondition generation. We start back to
front. Exit does not alter the postcondition, so we just copy it. Then, we either
execute block 0, 1 or 2, depending on what condition holds. If we came directly
from block 0, then x < y must hold, so our precondition is x ≥ y ∧ x < y, which
is false, indicated by the lightning bolt. If we came from block 1, then ¬(x ≥ y)
must have held. Block 1 updates x with x − y, leading to the precondition
(x − y) ≥ y ∧ ¬((x − y) ≥ y). Note that this precondition is unsatisﬁable. By
Theorem 1 we know that we can discard it.

The last block to look at, is block 2. To arrive here, we must have had that
¬(x > y). The body of block 2 updates x, and we end up with (x − y) ≥
y ∧ ¬((x − y) > y). Here, we see the loop unfolding at work. We have executed
the loop body once, and the τ function generates two alternatives. We exit the
loop, indicated by the arrow pointing up, or we run another iteration, indicated
by the arrow pointing right.

Ending the loop at this point again leads to a precondition that is unsatis-
ﬁable, and we can prune it. Running the loop a second time, and then exiting
leads to a precondition that is satisﬁable, and completing the calculation, leads
us to the ﬁrst viable precondition for the postcondition x ≥ y.

The precondition function τ does not stop at this point. It will continue to
unroll the loop an inﬁnite amount of times, making the exploit space inﬁnitely
large. By ordering the space as shown in this example, we can deal with inﬁnity.

Nondeterministic loop A deﬁning feature of both jump and Reachability
Logic, is the ability to deal with nondeterminism. The program listed in Figure 7
is a very small example of such a program. It assigns a random integer value to
the variable d, between 1 and some value n. Then, if n modulo d is zero, the
program jumps to Exit, otherwise it repeats itself.

For this example, we are simply interested in termination. So as a postcon-
dition, we pick True. The Exit block does not change the condition. Executing
the loop once, means that n%d ≡ 0 must hold. The nondeterministic assign-
ment updates d with any value between 1 and n. Translated to precondition,
this means that there exists an i such that (n%i ≡ 0) ∧ (1 < i) ∧ (i < n).

Reachability Logic for Low-Level Programs

13

)

x > 3

x ≡ 9
∧

x ≤ 3

∧
0 > 0
∧ x ≥ 0

x ≡ 1
∧

x ≤ 3

∧
0 > 0
∧ x ≥ 0

x ≡ 0
∧

x ≤ 3

∧
5 > 0
∧ x ≥ 0

x ≡ 2
∧

x ≤ 3

∧
5 > 0
∧ x ≥ 0

∨

0 > 0
∧ (

x < 0

#0 : Assign y 0;

Jump

(x < 0 || x > 3)
#9 #1

#1 : IndirJmp x

#2 : Jump true #4 #4

#3 : Jump true #9 #9

#4 : Assign y 5;

Jump true #9 #9

#9 : Exit

y > 0 ∧ x ≡ 9

y > 0 ∧ x ≡ 3

5 > 0 ∧ x ≡ 2

5 > 0 ∧ x ≡ 4

∧x ≡ 1

∧x ≡ 1

∧x ≡ 1

∧x ≡ 1

5 > 0

y > 0

5 > 0

y > 0

y > 0

Fig. 8. Precondition generation for indirect jump example.

This condition holds if and only if n is not prime. What makes this example
interesting is the contrast with other program logics like Hoare logic. With Hoare
Logic, we are only able to prove that if the program terminates, n was not prime.
Reachability Logic however allows us to show that if n is not prime, then the
program can terminate, which is a much more powerful statement.

Indirect Jumps Our last litmus test demonstrates how Reachability Logic
deals with indirect jumps. Switch-statements consisting of many cases are often
compiled into jump tables. These are typically combined with a guard for values
not handled by the jump table. Figure 8 shows a model of this.

Execution starts at block 0. Here, y is set to 0, and the conditional jump
checks if x is smaller than 0 or larger than 3. If so, we jump to exit. If not, we
jump to block 1, which is the start of our guard. The indirect jump jumps to
the block label stored in x. Blocks 2, 3, and 4 signify the guard options.

The goal of this example is to demonstrate how an indirect jump works with

our precondition generation function. As a postcondition, we select y > 0.

Starting at block 9, we again work our way up the execution back to front.
Block 9 itself has no eﬀect on the postcondition, so it is copied. We can reach
block 9 from four diﬀerent locations in the code, namely block 0, 1, 3 and 4.

Block 0 assigns 0 to y, which leads to a contradiction in our precondition,
namely 0 > 0. Block 1 is the indirect jump. We can only have reached block 9
from here if x was equal to 9. In turn, we could have reached block 1 from itself,
or 0. If we indeed came from itself, we must have made an indirect jump to 1,
requiring x to be equal to 1 as well, which is a contradiction. If we came from 0,
we have the same problem as before, y is set to 0 and so 0 > 0 must hold.

 
 
 
 
 
 
 
14

Naus et al.

Block 3 always jumps to 9, so it has no eﬀect on the condition. The only way
to reach block 3 is from block 1, the indirect jump. This requires x to be equal
to 3. We can reach block 2 from itself and 0. Both lead to a contradiction; if we
came from 1, then x ≡ 1, which is false, if we came from 0, 0 > 0 must hold.

Block 4 is our last hope to ﬁnd a satisﬁable precondition. The block itself
assigns 5 to y, leading to the precondition 5 > 0. Block 4 can be reached from
block 2 or 1. Block 2 is a simple jump, so we copy the condition. Block 2 is only
reachable from block 1, which adds x ≡ 2 to the condition. We can only reach 1
from itself or 0. If we came from itself, we get a contradiction immediately. If we
came from 0, we know that the jump condition was false, so we add its negation
to the condition. This leads to the ﬁrst satisﬁable precondition.

If we came directly from block 1, we know that x ≡ 4 should hold. Analogous
to the case described directly above, we obtain that the only feasible route is via
block 0, which again leads to a satisﬁable precondition.

What is interesting about this example, is the fact that although we have an
indirect jump, the number of paths to explore stays rather limited. Potentially,
an indirect jump can jump to any address, but in practice, these addresses are
limited by the conditions that must hold.

6 Case Studies

We presents results from applying Reachability Logic to two realistic examples.
These case studies were performed using the prototype mentioned in Section 5.

Faulty Partitioning for Quicksort The core of any quicksort algorithm is
the partitioning algorithm. One well-known partitioning algorithm is the one
invented by Tony Hoare [15] which selects a pivot element and then transforms
an input data set into two smaller sets, depending on relative ordering of elements
in the data set to the pivot. This scheme seems superﬁcially very simple, but it is
very easy to get wrong. For instance, the following algorithm has a superﬁcially
plausible variant of this partitioning scheme, which is “nearly correct”.

void quicksort(int a[], size_t N) {

if(N <= 1) return;
int pivot = a[rand()%N];
int i = 0, j = N-1;
while(i <= j) {
while(i < j
&& a[i] <= pivot) i++;
while(i <= j && a[j] >= pivot) j--;
swap(&a[i++], &a[j--]);

}
quicksort(a, j+1);
quicksort(a+i, N-i);}

The partitioning scheme can be translated into a jump program relatively

easily; selection of the pivot can be modeled using a non-deterministic assign.

Reachability Logic for Low-Level Programs

15

We are interested in detecting out-of-bounds memory access. We add bounds
checks to the program, and state triggering one of them as our postcondition.
Running the resultant program through our implementation for an array of size
3 will then generate an exploit-precondition: the program can go out of bounds
if the following condition holds:

∃i.0 ≤ i ≤ 3 ∧ a[i] ≤ a[0] ∧ a[i] ≤ a[1] ∧ a[i] ≤ a[2] ∧ a[0] > a[i]

Informally, this conditions says that a[0] is not the minimal element of the
array. The reason for this is that if the minimal element is chosen as a pivot, and
a[0] is not equal to it, the ﬁrst inner loop will simply fall through, and after
the second loop, i will become −1, pointing outside the array before the swap
occurs. A ﬁx for this would be make the swap conditional, replacing it with:

if(i <= j) swap(&a[i++], &a[j--]);

This will in fact prevent any out-of-bound memory access. However, another
way any version of quicksort can fail dramatically is when the recursive calls
are performed with incorrect parameters. For example if i = 0 or j = N at the
end of the partitioning scheme, we will end up in a inﬁnite recursive loop. If we
specify this as a postcondition of the partitioning scheme, we ﬁnd that the same
preconditions are generated as before.

The functional correctness of the partitioning scheme can also be examined—
that is, is it actually the case that all the elements moved towards the the left-
hand side of the array are less-or-equal to the pivot, and that the elements to the
right are greater-or-equal than the pivot? To examine this, we can specify as an
exploit condition that the input to the ﬁrst recursive invocation of quicksort
contains an element greater than the pivot; this ﬁnds no satisﬁable conditions (as
it is not true). However, specifying this for input sent to the second invocation
of quicksort instead, our prototype will essentially start generating counter-
examples. For example, if the ﬁrst element is the pivot, and strictly less than
the middle element but strictly higher than the third element, partitioning fails.

Karatsuba Several assembly routines for multiplying multi-precision integers
on an 8-bit AVR controller were veriﬁed by Schoolderman [27]. However, it was
also discovered that some of these routines could compute incorrect results if
their arguments aliased with the memory location intended to store the result.
A full veriﬁcation like this appears to require signiﬁcant eﬀort; however, if we are
only interested in ﬁnding aliasing bugs, RL seems ideally suited to ﬁnd these.

We focused on the smallest routine exhibiting the problem: the 48 × 48 →
96-bit multiplication routine as originally developed by Hutter and Schwabe
[19]. This routine computes a product of two 48-bit integers using Karatsuba’s
method, splitting its inputs into two 24-bit halves, and performing a three 24 →
48-bit multiplications with these, combining the results.7 In the process, the

7 To be more precise, this method uses the fact that (2wXh + Xl)(2wYh + Yl) =

(1 + 2w)(2wXhYh + XlYl) − 2w(Xl − Xh)(Yl − Yh)

16

Naus et al.

lowest 24-bits of the result are known early on and written to memory before
the upper half of the inputs is read, causing an aliasing bug.

To model this in jump, registers and the carry ﬂags are modeled as jump
variables, whereas the memory space is modeled using jump addresses. Every
AVR instruction is modeled by a sequence of jump statements. For example,
the instruction ADD a0, a1 can be expressed by the sequence:

Assign tmp (a0 + a1); Assign a0 (tmp mod 256); Assign carry (tmp / 256)

Adding the appropriate binary operators to the syntax of Figure 1, every
instruction required for the program (which are only a handful) can be modeled,
allowing the entire multiplication routine (consisting of 136 instructions) to be
expressed as a jump program. The memory accesses, which operate on three
bytes at a time, were modeled as a single memory operations on a three-byte
memory region.

As seen in Section 5, generated preconditions can be fairly verbose, and we
expected that in this case as well. To remedy this somewhat, we extended the
Haskell implementation with constant folding and other simpliﬁcations to more
eﬃciently manage the search space of possible preconditions, and pruning areas
of the search space which can easily be determined to be impossible. In a more
production-oriented setting, SMT solving and/or a robust expression simpliﬁer
can be used to do this more eﬃciently than our naive Haskell implementation.
For the precondition, we look at the case X · Y where X = Y = 224. Clearly
the expected result should be X · Y = 248, i.e. the 96-bit result should consist of
12 bytes, all of which contain 0, except for the seventh byte which should hold
1. As a postcondition, we therefore specify that this byte does not hold 1.

Running the jump version of the 48-bit Karatsuba code through our analysis
resulted in a handful of preconditions. Some of these simplify to False, as they
express impossible aliasing conditions—an SMT solver would be able to discard
these easily. However, 7 preconditions remained which are completely plausible
and satisﬁable, which fall into three categories:

– X, Y alias, and their high 24-bits overlap with the low 24-bits of the result
– X, Y are disjoint, and of them partially overlaps with the result as before
– X, Z are partially aliased, and one of them partially overlaps with the result

Which are exactly the case we would expect: the issue is being caused by
either (or both) inputs sharing their high 24-bits with the low 24-bits of the
output location. Had we not chosen the ﬁxed input values for X and Y , this
case would have generated more complex preconditions, however, this case shows
that there is an easy instance where these would be satisﬁed.

7 Discussion

Key to our approach is the reachability space. The size and shape of this search
space directly inﬂuences the eﬀectiveness or even feasibility of Reachability Logic.

Reachability Logic for Low-Level Programs

17

State space explosion is a very typical and problematic eﬀect for approaches like
symbolic execution and loop unrolling.

How is this diﬀerent for Reachability Logic? First of all, the goal of RL is not
to say something about all possible execution paths, but instead only one. This
means that instead of proving properties over the entire search space, we just
need to ﬁnd one execution path that is viable. The nature of the reachability
space is such that it only contains paths that are ﬁnite in length, but there may
be inﬁnitely many of them due to loop unrolling and indirect jumps. This means
that we only need to search though this space in width, not depth, to ﬁnd one
path to the assertion. On top of that, the reachability space can be ordered
in such a way to make searching more eﬃcient. For the litmus tests and case
studies, this is already done to improve performance.

8 Related work

Besides the program logics mentioned in Section 2, several other angles have been
explored to (automatically) reason over programs, reachability and exploits.

Brumley et al. [6] generate exploits from patches. Given a program P and
a patched version of the program P ′, they are able to ﬁnd the exploit that
was patched in P ′, but to which P is vulnerable. An obvious drawback of this
approach is that a patched version of the program needs to be available, for this
approach to work.

Avgerinos et al. [2] present an automatic exploit generation system (AEG)
that only requires the source code of the program to be exploited. They generate
LLVM code from the source and analyze this code using symbolic execution and
some safely property to obtain exploits. Their approach uses heuristics, and the
safety property used is not discussed or listed in the paper. No guarantees are
given about the search space, as opposed to our solution, where we formally
verify the reachability search space.

As an alternative to AEG, automatic vulnerability detection is a less strictly
deﬁned line of research into ﬁnding vulnerabilities. Russell et al. [26] use deep
representation learning to do the job. They use the large set of open-source code
available online, labeled with vulnerability information, to train their tool. Their
results are very promising, but as with all machine learning-based solutions, their
approach does not come with any guarantees.

Symbolic execution runs a program with symbols instead of actual input.
Running the program with these symbolic inputs results in a complete overview
of the programs behavior. Symbolic execution is extensively used for software
testing [5,7,8]. Cadar and Sen [9] provide a great overview of the applications
of symbolic execution for this purpose. The biggest downside of symbolic exe-
cution is that it describes the complete program behavior, and therefore quickly
becomes infeasible, due to the may paths to be described.

Symbolic backward execution (SBE) attempts to mitigate the downside of
reasoning over all possible paths by looking at a target location in source code.
Charreteur and Gotlieb present a method for generating test input based on SBE

18

Naus et al.

for Java bytecode [10]. Dinges and Agha augment this approach with concrete
execution as well [13]. Although there are some similarity between SBE and
RL, there are two crucial diﬀerence. First of all, RL is proven to be sound and
complete, providing the user with guarantees over the result. Second, RL is able
to deal with non-determinism, modeling external functions, uncertain semantics
of some instructions, etc. Finally, RL only looks at the postcondition, instead
of a location in code. This means that only those statements that aﬀect the
postcondition, need to be taken into account, making RL much more feasible
than SBE.

There is a wide variety available of static bug-ﬁnding techniques available.
Static analysis has been applied to detect security bugs [18], runtime errors [17]
and even type errors [20]. These analyses are used in practice to detect bugs,
for example the FindBug application [3], which employs simple static techniques
to ﬁnd bugs in Java code. Static analyses have to be tailored to a speciﬁc type
of bugs. Reachability Logic on the other hand, enables the detection of the
reachability of any postcondition.

Anand et al. [1] provide a nice overview of the ﬁeld of test case generation.
They identify ﬁve techniques for test case generation. Symbolic execution, model
based testing, combinatorial testing, adaptive random testing and search-based
testing. In general, our approach is a much more formal one than existing test
case generation. We are interested in formally deﬁning the entire reachability
space in such a way that it is sound and complete. Test case generation has
a diﬀerent goal in mind, namely selecting suitable test cases for a certain pro-
gram. Many of the techniques mentioned do not guarantee full coverage. This is
something that Reachability Logic does have.

9 Conclusion

In this paper, we have presented the novel program logic Reachability Logic. RL
is well suited for proving that an assertion is reachable in a nondeterministic
program. Existing program logics such as HL and RHL have a diﬀerent focus
and are unsuitable for this purpose.

We have developed a low-level language called jump, having jumps instead
of explicit control ﬂow. For this language we presented a precondition genera-
tion function, which we have formally proven to be sound and complete. This
precondition function thus generates a sound and complete reachability space.
To validate our approach, we have presented several litmus tests that illustrate
the precondition generation function, as well as real world case studies that ﬁnd
bugs. An implementation of the system has been developed, in which both the
litmus tests and case studies have been run. The entire system is formally proven
correct in the Isabelle/HOL theorem prover.

Test case generation research oﬀers a plethora of techniques to show errors in
software by means of ﬁnding the right test. While there are many diﬀerent ways
to generate test cases, full coverage is not guaranteed. Finding vulnerabilities
has also been done using machine learning, but similarly this method too does

Reachability Logic for Low-Level Programs

19

not come with any guarantees. Symbolic execution comes closer, since it is able
to prove properties over all paths in the program, but state space explosion
prevents it from being applied to large code bases. Some initial work has been
done on automatic exploit generation. Compared to existing work, we take a
formal approach, so that we are able to give the user the guarantee that all
preconditions are sound.

Acknowledgements This work is supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Agreement No. HR00112090028 and
contract N6600121C4028 and US Oﬃce of Naval Research (ONR) under grant
N00014-17-1-2297.

References

1. Anand, S., Burke, E.K., Chen, T.Y., Clark, J.A., Cohen, M.B., Grieskamp, W.,
Harman, M., Harrold, M.J., McMinn, P.: An orchestrated survey of methodolo-
gies for automated software test case generation. J. Syst. Softw. 86(8), 1978–2001
(2013)

2. Avgerinos, T., Cha, S.K., Hao, B.L.T., Brumley, D.: AEG: automatic exploit gener-
ation. In: Proceedings of the Network and Distributed System Security Symposium,
NDSS 2011, San Diego, California, USA, 6th February - 9th February 2011 (2011)
3. Ayewah, N., Hovemeyer, D., Morgenthaler, J.D., Penix, J., Pugh, W.: Using static

analysis to ﬁnd bugs. IEEE Softw. 25(5), 22–29 (2008)

4. Barnett, M., Leino, K.R.M.: Weakest-precondition of unstructured programs. In:
Proceedings of the 6th ACM SIGPLAN-SIGSOFT workshop on Program analysis
for software tools and engineering. pp. 82–87 (2005)

5. Boyer, R.S., Elspas, B., Levitt, K.N.: Select—a formal system for testing and de-
bugging programs by symbolic execution. ACM SigPlan Notices 10(6), 234–245
(1975)

6. Brumley, D., Poosankam, P., Song, D.X., Zheng, J.: Automatic patch-based exploit
generation is possible: Techniques and implications. In: 2008 IEEE Symposium on
Security and Privacy (S&P 2008), 18-21 May 2008, Oakland, California, USA. pp.
143–157 (2008)

7. Burch, J.R., Clarke, E.M., McMillan, K.L., Dill, D.L., Hwang, L.J.: Symbolic model
checking: 1020 states and beyond. Information and computation 98(2), 142–170
(1992)

8. Cadar, C., Dunbar, D., Engler, D.R., et al.: Klee: unassisted and automatic gen-
eration of high-coverage tests for complex systems programs. In: OSDI. vol. 8, pp.
209–224 (2008)

9. Cadar, C., Sen, K.: Symbolic execution for software testing: three decades later.

Commun. ACM 56(2), 82–90 (2013)

10. Charreteur, F., Gotlieb, A.: Constraint-based test input generation for java byte-
code. In: IEEE 21st International Symposium on Software Reliability Engineering,
ISSRE 2010, San Jose, CA, USA, 1-4 November 2010. pp. 131–140. IEEE Com-
puter Society (2010)

11. Dasgupta, S., Park, D., Kasampalis, T., Adve, V.S., Roşu, G.: A complete formal
semantics of x86-64 user-level instruction set architecture. In: Proceedings of the
40th ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation. pp. 1133–1148 (2019)

20

Naus et al.

12. Dawson, J.: Isabelle theories for machine words. Electronic Notes in Theoretical

Computer Science 250(1), 55–70 (2009)

13. Dinges, P., Agha, G.A.: Targeted test input generation using symbolic-concrete
backward execution.
I., Chechik, M., Grünbacher, P. (eds.)
ACM/IEEE International Conference on Automated Software Engineering, ASE
’14, Vasteras, Sweden - September 15 - 19, 2014. pp. 31–36. ACM (2014)

In: Crnkovic,

14. Heule, S., Schkufza, E., Sharma, R., Aiken, A.: Stratiﬁed synthesis: automatically
learning the x86-64 instruction set. In: Proceedings of the 37th ACM SIGPLAN
Conference on Programming Language Design and Implementation. pp. 237–250
(2016)

15. Hoare, C.A.R.: Algorithm 64: Quicksort. Commun. ACM 4(7), 321 (Jul 1961)
16. Hoare, C.A.R.: An axiomatic basis for computer programming. Commun. ACM

12(10), 576–580 (1969)

17. Hovemeyer, D., Spacco, J., Pugh, W.: Evaluating and tuning a static analysis to
ﬁnd null pointer bugs. In: Proceedings of the 2005 ACM SIGPLAN-SIGSOFT
Workshop on Program Analysis For Software Tools and Engineering, PASTE’05,
Lisbon, Portugal, September 5-6, 2005. pp. 13–19 (2005)

18. Huang, Y., Yu, F., Hang, C., Tsai, C., Lee, D., Kuo, S.: Securing web application
code by static analysis and runtime protection. In: Proceedings of the 13th inter-
national conference on World Wide Web, WWW 2004, New York, NY, USA, May
17-20, 2004. pp. 40–52 (2004)

19. Hutter, M., Schwabe, P.: Multiprecision multiplication on AVR revisited. Journal

of Cryptographic Engineering 5(3), 201–214 (2015)

20. Jensen, S.H., Møller, A., Thiemann, P.: Type analysis for javascript. In: Static
Analysis, 16th International Symposium, SAS 2009, Los Angeles, CA, USA, August
9-11, 2009. Proceedings. pp. 238–255 (2009)

21. Kammüller, F., Wenzel, M., Paulson, L.C.: Locales a sectioning concept for isabelle.
In: International Conference on Theorem Proving in Higher Order Logics. pp. 149–
165. Springer (1999)

22. Lattner, C., Adve, V.S.: LLVM: A compilation framework for lifelong program
analysis & transformation. In: 2nd IEEE / ACM International Symposium on
Code Generation and Optimization (CGO 2004), 20-24 March 2004, San Jose,
CA, USA. pp. 75–88 (2004)

23. O’Hearn, P.W.: Incorrectness logic. Proc. ACM Program. Lang. 4(POPL), 10:1–

10:32 (2020)

24. Paulson, L.C.: Isabelle: A generic theorem prover, vol. 828. Springer Science &

Business Media (1994)

25. Raad, A., Berdine, J., Dang, H., Dreyer, D., O’Hearn, P.W., Villard, J.: Local
reasoning about the presence of bugs: Incorrectness separation logic. In: Computer
Aided Veriﬁcation - 32nd International Conference, CAV 2020, Los Angeles, CA,
USA, July 21-24, 2020, Proceedings, Part II. pp. 225–252 (2020)

26. Russell, R.L., Kim, L.Y., Hamilton, L.H., Lazovich, T., Harer, J., Ozdemir, O.,
Ellingwood, P.M., McConley, M.W.: Automated vulnerability detection in source
code using deep representation learning. In: 17th IEEE International Conference on
Machine Learning and Applications, ICMLA 2018, Orlando, FL, USA, December
17-20, 2018. pp. 757–762 (2018)

27. Schoolderman, M.: Verifying branch-free assembly code in why3. In: Paskevich,
A., Wies, T. (eds.) Veriﬁed Software. Theories, Tools, and Experiments. pp. 66–
83. Springer International Publishing, Cham (2017)

28. Sheard, T., Fegaras, L.: A fold for all seasons. In: Proceedings of the conference on
Functional programming languages and computer architecture. pp. 233–242 (1993)

Reachability Logic for Low-Level Programs

21

29. de Vries, E., Koutavas, V.: Reverse hoare logic. In: Software Engineering and For-
mal Methods - 9th International Conference, SEFM 2011, Montevideo, Uruguay,
November 14-18, 2011. Proceedings. pp. 155–171 (2011)

30. Xu, L., Jia, W., Dong, W., Li, Y.: Automatic exploit generation for buﬀer over-
ﬂow vulnerabilities. In: 2018 IEEE International Conference on Software Quality,
Reliability and Security Companion (QRS-C). pp. 463–468 (2018)

