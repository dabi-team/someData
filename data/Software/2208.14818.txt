PYTORCH IMAGE QUALITY:
METRICS FOR IMAGE QUALITY ASSESSMENT

2
2
0
2

g
u
A
1
3

]

V

I
.
s
s
e
e
[

1
v
8
1
8
4
1
.
8
0
2
2
:
v
i
X
r
a

Sergey Kastryulin
Computational Imaging Laboratory
Skolkovo Institute of Science and Technology
Moscow, Russia
sergey.kastryulin@skoltech.ru

Jamil Zakirov
Independent Researcher
Skolkovo Institute of Science and Technology
Moscow, Russia
jamil.zakirov@skoltech.ru

Denis Prokopenko
Biomedical Engineering Department
School of Biomedical Engineering and Imaging Sciences
King’s College London, London, UK
d.prokopenko@outlook.com

Dmitry V. Dylov
Computational Imaging Laboratory
Skolkovo Institute of Science and Technology
Moscow, Russia
d.dylov@skoltech.ru

ABSTRACT

Image Quality Assessment (IQA) metrics are widely used to quantitatively estimate the extent of
image degradation following some forming, restoring, transforming, or enhancing algorithms. We
present PyTorch Image Quality (PIQ), a usability-centric library that contains the most popular
modern IQA algorithms, guaranteed to be correctly implemented according to their original propo-
sitions and thoroughly veriﬁed. In this paper, we detail the principles behind the foundation of
the library, describe the evaluation strategy that makes it reliable, provide the benchmarks that
showcase the performance–time trade-offs, and underline the beneﬁts of GPU acceleration given
the library is used within the PyTorch backend. PyTorch Image Quality is an open source software:
https://github.com/photosynthesis-team/piq/.

Keywords Image Quality · Metrics · Computer Vision

1

Introduction

With the ever-rising interest towards generation, recovery, and enhancement of images, a number of computational
Image Quality Assessment (IQA) methods have become available. These methods aim to produce a score that would
perfectly correlate with the human perception of the Image Quality (IQ). Given the large number of the assorted IQA
options and frequent mismatch between different implementations of the same metric, a demand for reliable and
community-validated resource has become evident. Speciﬁcally, such a resource should embrace a plethora of possible
modern image metrics, eliminating the need to search, install, or re-implement the algorithms.

Although popular IQA libraries provide the access to some Image Quality Metrics (IQMs) in the form of user-friendly
packages, they do have a set of limitations. Blind Image Quality Toolbox [1] and Image Quality Assessment Toolbox
[2] are veriﬁed MATLAB implementations of no-reference (NR) and full-reference (FR) IQMs. Unfortunately, the
growing popularity of Python makes these solutions less relevant for the hands-on research efforts, where the majority
of image and vision computing experts rely on Python for its well-tested open source repositories. Kornia [3], PIQA [4],
and IQA-PyTorch [5] do provide Python interfaces for the metrics and the IQA-optimization [6], while also focusing on
their use as the loss functions. However, these libraries implement a rather limited number of algorithms1.

This manuscript introduces PyTorch Image Quality (PIQ) open source library that provides an extended collection of
implementations of IQA metrics and the corresponding loss functions, using PyTorch [7] to enable fast and efﬁcient

1Number of IQMs implemented: Kornia - 2, PIQA - 11, IQA-PyTorch - 25.

 
 
 
 
 
 
PyTorch Image Quality: Metrics for Image Quality Assessment

Figure 1: PIQ open source library implements 38+ metrics in three main categories: Full-Reference (FR), No-Reference
(NR), and Distribution-Based (DB). Our implementations enable quantitative estimation of the quality of images from
various domains, including medical scans. Right column shows the counters of the currently implemented metrics.
Note that DB metrics can be used with different feature extractors, further diversifying the pool of available metrics.

computations on the graphics processing unit (GPU). This work is the result of three years of collecting and comparing
the best image quality metrics in one place, with independent implementation, optimization, and testing. At the time
of writing this manuscript, our GitHub project receives over 4000 monthly views, having accumulated 640 stars and
having been forked over 57 open-source projects. This tool has been actively used in the research community, including
the development of new neural network architectures [8] and the large-scale medical image quality study [9].

2 Design principles

We created PyTorch Image Quality to maximise the beneﬁt for the research community working on a plethora of
image-to-image translation problems in a variety of visual data domains (Fig. 1). To do that, we build the library based
on the following design principles:

Be user-friendly. Unfortunately, the complexity inherent to the ﬁeld of Computer Vision is multiplied by that of the
sophisticated algorithms designed to boost the performance of the IQA approaches. We hide this internal complexity
behind easy-to-use APIs that follow the principle of the least astonishment [10] to provide a seamless user experience.

Be reliable. Today, publicly available implementations may produce inconsistent results, which applies even to the
most well-known and the widely used IQMs (e.g. SSIM [11]). PyTorch Image Quality library is focused on a thorough
testing to provide consistency with the formal metrics deﬁnitions and the original implementations proposed by their
authors (if these implementations exist).

Be pragmatic. A majority of modern (and some classical) IQMs are computationally inefﬁcient by their design, which
hinders the ultimate performance. Therefore, PyTorch Image Quality purposely enables an optimization when its extra
complexity is worth delivering a compelling performance. Being inspired by the same principle in PyTorch, we state
that trading 10% of speed for a model that is signiﬁcantly easier to use is acceptable; and 100% is not [7].

3 Usability–centric approach

Currently, PyTorch Image Quality library contains implementations of the following 38 metrics: 21 Full-Reference
(FR) IQMs (PSNR, SSIM [11], MS-SSIM [12], IW-SSIM [13], VIF [14], GMSD [15], MS-GMSD and MS-GMSDc
[16], FSIM and FSIMc [17], SR-SIM and SR-SIMc [18], VSI [19], MDSI [20], HaarPSI [21], Content and Style
Perceptual Scores [22], LPIPS [23], DISTS [24], PieAPP [25], DSS [26]), 2 No-Reference (NR) IQMs (BRISQUE
[27], Total Variation), and 15 Distribution-Based (DB) IQMs (KID [28], FID [29], GS [30], Inception Score (IS) [31],
MSID [32], all implemented with three different feature extractors: Inception Net [33], VGG16, and VGG19 [34]). The
general taxonomy of IQA metrics, their detailed description, and a pertinent discussion can be found in A.

Remark. Throughout the work, we use the term “metrics” to describe the IQA algorithms. Technically, this term is not
mathematically correct, because a metric is a function for which the identity of indiscernibles, the symmetry, and the

2

No-ReferenceDistributionsImage PairsPyTorchImage Quality (PIQ)21 Full-Reference IQMs15 Distribution-Based IQMs2 No-ReferenceIQMsEnhancementSuper-resolutionDeblurringTransformationDemotionDenoising…GenerationPyTorch Image Quality: Metrics for Image Quality Assessment

triangle inequality must hold [?]. For the majority of implemented algorithms, one or several of these attributes do not
hold. However, the term is still used here, which reﬂects the commonplace convention in the community.

Metrics as loss functions

Previous studies of IQMs showed their efﬁciency in reﬂecting the human perception of visual quality [35, 36, 37, 9].
That brings a temptation to use the best-performing differentiable IQMs as the loss functions for a direct optimization
of the image processing models [38]. To enable that possibility, all PIQ metrics are well-integrated with the PyTorch [7]
backend, enabling the automatic computing of the gradients of the differentiable models.

Besides, the use of PyTorch enables the GPU acceleration, yielding a faster computation of the metrics and the losses
(see Fig. 3 for more details). Moreover, our implementation strategy allows a seamless integration with the most
common deep learning pipelines in PyTorch. For instance, the ﬂexible interface enables our metric implementations to
be used as additional layers of a deep neural network.

Integrated feature extractors

DB IQMs are computed on features obtained from images with feature extractors, which are typically represented
by pre-trained convolutional neural networks. Most methods are evaluated using a single feature extractor (typically,
Inception Net [33]). A recent study on IQMs for Magnetic Resonance Imaging [9] showed that the choice of feature
extractor plays a critical role, heavily inﬂuencing the performance of DB metrics. These result may inspire our users to
experiment with various feature extractor options. To address that, we added a possibility to provide their own feature
extractors or choose one of the models integrated into the library (Inception Net [33], VGG16, and VGG19 [34]).

Chromatic versions of luminance based metrics

The majority of methods are designed to be applied to images in RGB color space. However, some of them (e.g. FSIM
[17] and SR-SIM [18]) are designed for grayscale images or for the luminance component of the color images. Because
the chrominance information also affects human visual system (HVS) in understanding the images, better performance
can be expected if the chrominance information is incorporated for color IQA. For that, we follow the common approach
of ﬁrst converting the RGB images into the YIQ colour space. The Y channel is used for the computations of the initial
grayscale variants, while the I and the Q components are added to obtain the chromatic versions of the metrics (e.g.
FSIMc [17] and SR-SIMc [18] respectively).

4 Evaluation

New metrics and measures for IQA that claim to be better than their predecessors emerge every day. To prove that,
authors show IQMs’ performance on speciﬁcally designed IQA datasets consisting of pairs of distorted and reference
images accompanied with similarity scores estimated by human assessors. The availability of such datasets allows
to compute correlations between human scores and metrics’ estimates of quality to ﬁnd algorithms that best reﬂect
judgement of HVS.

We use these results to evaluate the correctness of our implementations. For that, implemented metrics are computed on
selected IQA datasets and obtained values are compared with the ones reported in corresponding research papers.

IQA datasets

IQA datasets are typically designed to evaluate existing IQMs on their ability to answer previously not considered
questions such as IQ from two different view distances (CID dataset [58]), IQ on images perturbed with multiple
types of distortions (MDID [64], MD-IVL [62] and VLC [73] datasets), evaluation of GAN-based image restoration
algorithms (PIPAL [70] and NTIRE 2021 [71] challenges and datasets). Another commonly used datasets are LIVE
[54], TID2013 [55] and KADID-10k [66]. Some works even try to propose a method to construct a general-case IQA
dataset with extremely diverse image characteristics [74].

Subjective annotations

The majority of the listed datasets provide image pairs accompanied by subjective quality scores. Typically, subjective
scores are estimated by human assessors and aggregated in the form of Mean Opinion Scores (MOS) or Differential

3

PyTorch Image Quality: Metrics for Image Quality Assessment

Table 1: Overview of existing IQA datasets

Database

Year Reference Distorted

Subjective Score

IVC [39]
LIVE IQA [40, 41]
A57 [42, 43]
Toyama/MICT [44]
TID2008 [45]
CSIQ [46, 47]
IVC-LAR [48]
WIQ [49]
IRSQ [50]
VCLFER [51, 52]
LIVE MD [53, 54]
TID2013 [55]
MDID2013 [56]
CID2013 [57]
CIDIQ [58]
SIQAD [59]

2005
2006
2007
2008
2008
2009
2009
2009
2012
2012
2013
2013
2014
2013
2014
2015
LIVE in the wild (CLIVE) [60, 61] 2015
2017
2017
2018
2019
2019 140,000
2019
2020
2020

MD-IVL [62, 63]
MDID [64]
KonIQ-10k [65]
KADID-10k [66]
KADIS-700k [67]
PaQ-2-PiQ [68]
SPAQ [69]
PIPAL [70, 71, 72]

10
29
3
14
25
30
8
7
57
23
15
25
12
8
23
20
-
10
20
10,073
81

-
11,125
250

235
779
54
168
1,700
866
120
80
171
552
405
3,000
324
480
690
980
1,162
750
1,600
10,073
10,125
700,000
40,000
-

MOS (1 ∼ 5)
DMOS (0 ∼ 100)
DMOS (0 ∼ 1)
MOS (1 ∼ 5)
MOS (0 ∼ 9)
DMOS (0 ∼ 1)
MOS (1 ∼ 5)
DMOS (0 ∼ 100)
MOS (0 ∼ 5)
MOS (0 ∼ 100)
DMOS (0 ∼ 100)
MOS (0 ∼ 9)
DMOS (0.3 ∼ 0.6)
MOS (0 ∼ 9)
MOS (0 ∼ 9)
DMOS (0 ∼ 100)
MOS (1 ∼ 5)
MOS (0 ∼ 100)
MOS (0 ∼ 9)
MOS (1 ∼ 100)
DMOS (1 ∼ 5)
DMOS (1 ∼ 5)
MOS (0 ∼ 100)
MOS (0 ∼ 100)

29,000 MOS (Elo rating system)

Mean Opinion Scores (DMOS). In some cases, raw scoring data is ﬁrst converted to z-scores averaged and re-scaled
from 0 to 100 to account for different scoring across respondents as proposed in [75].

Datasets selection

A signiﬁcant number of IQA databases have come out over the last 15 years. There is currently no gold standard dataset.
Table 1 shows that IQA datasets use a variety of subjective testing methodologies, number of images, and number of
distortions.

Typically, usage of datasets with a small number of images and distortions results in high variance between evaluation
results. Considering that, our main criteria for selection were the size of the dataset and distortions variety.

We selected TID2013 [55] and KADID-10k [66] databases as one of the most popular in the research community and
PIPAL [70] as the one with a higher variety of introduced distortions. After that, we selected an evaluation criterion
(type of correlation) that can be computed on selected datasets and compared with the reference values.

Evaluation criteria

Among numerous evaluation criteria, PLCC, SRCC, and KRCC are the most commonly used in large-scale studies of
IQ metrics.

Pearson linear correlation coefﬁcient (PLCC) requires produced scores to be linear with respect to subjective ratings.
Previous studies [36, 9] showed non-linear relation between IQM scores and human accessors’ scores. A common
solution is to apply a non-linear regression by adopting the ﬁve-parameter modiﬁed logistic function [40]. Even though
this approach solves the non-linearity problem, we do not use it due to the poor reproducibility of model ﬁtting results
among studies.

4

PyTorch Image Quality: Metrics for Image Quality Assessment

Table 2: Comparison of correlation values (in terms of SRCC) reported in literature with PIQ implementations.

TID2013

KADID-10k

PIPAL

PIQ / Reference PIQ / Reference PIQ / Reference

PSNR
SSIM [11]
MS-SSIM [12]
IW-SSIM [13]
VIFp [14]
GMSD [15]
MS-GMSD [16]
MS-GMSDc [16]
FSIM [17]
FSIMc [17]
SR-SIM [18]
SR-SIMc [18]
VSI [19]
MDSI [20]
HaarPSI [21]
ContentVGG16 [22]
StyleVGG16 [22]
LPIPSVGG16 [23]
DISTS [24]
PieAPP [25]
DSS [26]

0.69 / 0.69 [55]
0.72 / 0.64 [55]
0.80 / 0.79 [55]
0.78 / 0.78 [35]
0.61 / 0.61 [55]
0.80 / 0.80 [16]
0.81 / 0.81 [16]
0.89 / 0.89 [16]
0.80 / 0.80 [55]
0.85 / 0.85 [55]
0.81 / 0.81 [35]
0.87 / −
0.90 / 0.90 [35]
0.89 / 0.89 [20]
0.87 / 0.87 [21]
0.71 / −
0.54 / −
0.67 / 0.67 [24]
0.81 / 0.83 [24]
0.84 / 0.88 [24]
0.79 / 0.79 [35]

No-reference metrics

0.68 / −
0.72 / 0.72 [66]
0.80 / 0.80 [66]
0.85 / 0.85 [66]
0.65 / 0.65 [66]
0.85 / 0.85 [66]
0.85 / −
0.87 / −
0.83 / 0.83 [66]
0.85 / 0.85 [66]
0.84 / 0.84 [66]
0.87 / −
0.88 / 0.86 [66]
0.89 / 0.89 [66]
0.89 / 0.89 [66]
0.72 / −
0.65 / −
0.72 / −
0.88 / −
0.87 / −
0.86 / 0.86 [66]

0.41 / 0.41 [70]
0.50 / 0.53 [70]
0.55 / 0.46 [70]
0.60 / −
0.50 / −
0.58 / −
0.59 / −
0.59 / −
0.59 / 0.60 [70]
0.59 / −
0.54 / −
0.57 / −
0.54 / −
0.59 / −
0.59 / −
0.45 / −
0.34 / −
0.57 / 0.58 [70]
0.62 / 0.66 [70]
0.70 / 0.71 [70]
0.63 / −

BRISQUE [27]

0.37 / 0.84 [35]

0.33 / 0.53 [66]

0.21 / −

Distribution-based metrics

0.42 / −
KIDInceptionV3 [28]
0.67 / −
FIDInceptionV3 [29]
0.37 / −
GSInceptionV3 [30]
0.26 / −
ISInceptionV3 [31]
MSIDInceptionV3 [32] 0.21 / −

0.66 / −
0.66 / −
0.37 / −
0.25 / −
0.32 / −

0.12 / −
0.18 / −
0.02 / −
0.09 / −
0.02 / −

Note 1: Typically, the distance between correlation values obtained with new and reference implementations is used to verify the
accurateness of the former. Zero difference is an indication of correctness. However, we ﬁnd it peculiar to ﬁnd that for some metrics
(e.g., SSIM, MS-SSIM, VSI) the same implementation exactly matches with only one of two reference values.
Note 2: All three considered datasets are widely used in the IQA research community to assess metrics on their ability to estimate
IQ in the same domain - general natural images. However, we observe a signiﬁcant drop of SRCC values for all metrics on the larger
PIPAL dataset compared to smaller TID2013 and KADID10k datasets. While a domain shift caused by larger variety or distortions
introduced in the newer PIPAL dataset could explain the observation, more investigations are required.

Spearman’s rank-order correlation coefﬁcient (SRCC) and Kendall rank correlation coefﬁcient (KRCC) measure
monotonicity between two measured quantities. Despite different calculation methods, KRCC is found to be highly
consistent with the SRCC [35]. Considering that, we use only the most popular SRCC score for further evaluations.

SRCC = 1 −

6 (cid:80)n
i=1 d2
i
n(n2 − 1)

,

(1)

where di is the difference between the i-th image’s ranks in the objective and the subjective ratings and n is the number
of observations.

5

PyTorch Image Quality: Metrics for Image Quality Assessment

Figure 2: Relationship between the computation time of a metric on CPU and their performance in terms of SRCC
score on Natural Images from KADID-10k dataset [66] (top row) and MRI Images [9] (bottom row) for all metrics (left
column) and zoomed-in region indicated in red (right column). Metrics with the best time-quality relation are located in
the top-left corner.

Implementation details of DB IQMs

DB IQMs are not originally designed for pair-wise comparison of images. Instead, they are intended to be used to
compare distributions of two image sets. However, we investigate DB metrics for pair-wise comparison of images
following the computation strategy proposed in [9]. To encounter for the initial setting, we represent each image in
a pair as a set of overlapping patches of size 96 × 96 with stride = 32, which allows us to reformulate the pair-wise
comparison of images as a comparison of patch distributions. After we extract features from two sets of patches, we
proceed with the initial ﬂow of metrics’ computation.

6

0.650.700.750.800.850.90Spearman's Rank Correlation CoefficientPSNRSSIMVIFpLPIPSDISTSPieAPPContentStyleFIDKID3579log(time, ms)0.250.300.35BRISQUEISGSMSID2.53.03.54.04.5log(time, ms)0.800.850.90Spearman's Rank Correlation CoefficientMS-SSIMIW-SSIMGMSDMS-GMSDMS-GMSDcFSIMFSIMcSR-SIMSR-SIMcVSIMDSIHaarPSIDSS0.550.600.650.700.750.80Spearman's Rank Correlation CoefficientPSNRContentStyleLPIPSDISTS357911log(time, ms)0.050.100.150.200.250.300.35SSIMPieAPPBRISQUEKIDFIDGSISMSID2.53.03.54.04.5log(time, ms)0.550.600.650.70Spearman's Rank Correlation CoefficientMS-SSIMIW-SSIMVIFpGMSDMS-GMSDFSIMVSIMDSIHaarPSIDSSPyTorch Image Quality: Metrics for Image Quality Assessment

Figure 3: Relationship between metrics’ computation time on GPU and their performance in terms of SRCC score on
Natural Images from KADID-10k dataset [66] (top row) and MRI Images [9] (bottom row) for all metrics (left column)
and zoomed-in region indicated in red (right column). Metrics with the best time-quality relation are located in the
top-left corner.

Conﬁrmation of implementations correctness

All implementations in the PyTorch Image Quality library are veriﬁed to be consistent with the original implementations
proposed by the authors of each metric on selected IQA datasets. Refer to Table 2 for a detailed comparison.

Typically, IQMs are evaluated on a single dataset. Our veriﬁcation results allow comparing metrics performance across
different image sources to show that even correct implementations (that match correlation values on initial datasets)
may not match values reported on different datasets. It is also worth mentioning that the performance of some feature
extraction-based metrics (e.g. BRISQUE) cannot be fully reproduced on IQA datasets even though the code match the
ofﬁcial implementation of the metric provided by their authors.

7

0.650.700.750.800.850.90Spearman's Rank Correlation CoefficientPSNRSSIMIW-SSIMVIFpLPIPSContentStyleFIDKID2.53.5log(time, ms)0.250.300.35BRISQUE7.58.59.510.5ISGSMSID2.53.03.5log(time, ms)0.800.850.90Spearman's Rank Correlation CoefficientMS-SSIMGMSDMS-GMSDMS-GMSDcFSIMFSIMcSR-SIMSR-SIMcVSIHaarPSIMDSIDISTSPieAPPDSS0.550.600.650.700.750.80Spearman's Rank Correlation CoefficientPSNR2.53.5log(time, ms)0.050.100.150.200.250.300.35SSIMPieAPPBRISQUE7.58.59.5KIDFIDGSISMSID2.252.502.753.003.253.503.754.004.25log(time, ms)0.550.600.650.700.75Spearman's Rank Correlation CoefficientMS-SSIMIW-SSIMVIFpGMSDMS-GMSDFSIMVSIMDSIHaarPSIContentStyleLPIPSDISTSDSSPyTorch Image Quality: Metrics for Image Quality Assessment

5 Performance – complexity trade-off

Figures 2 and 3 describe performance comparison of the implemented metrics in terms of SRCC and computational
time on CPU and GPU for Natural Images from KADID-10k [66] dataset and MRI Images [9]. Benchmarks were
performed on a dedicated instance of NVIDIA DGX Station with Intel Xeon E5-2698 v4 CPU and four NVIDIA V100
32Gb GPUs. A single graphic card was used in all experiments for convenience and simplicity of comparison. Even
though no other processes except from system utilities of the GNU/Linux 5.4.0-91-generic x86-64 operating system
were running during the performance of the benchmarks, we recommend to focus on relative performance of metrics
rather than the exact numbers of their computation time.

Several key observations can be made based on results of the benchmarks. DB IQMs tend to group in the lower-right
corner, steadily showing poor performance both in terms of CPU and GPU computation time and SRCC values (except
from FIDVGG16 on MRI data). Even though GPU acceleration signiﬁcantly speeds up their computation, the computation
time gap remains to be considerable.

The best trade-off between quality and performance is achieved by metrics located in the upper-left corner. In all
experiments there is a group of algorithms that tend to group together, forming a category of methods attractive for
practical application. While the composition of this group varies slightly, there are several metrics that consistently
perform well regardless of the domain and computing device in question such as MDSI, VSI, HaarPSI, DSS, GMSD
and several others. Feature-based FR IQMs such as DISTS and PieAPP show high SRCC values on Natural Images
but take long to be computed on CPU. GPU acceleration plays the key role for these metrics, putting them to the
lower-left group of top performers. Widely used PSNR and SSIM are easy to compute both on CPU and GPU but they
compromise IQA quality on both considered domains.

6 Conclusion and future work

The main contribution of our work is the PyTorch Image Quality (PIQ) Assessment toolbox [76] with a diverse set of
measures, veriﬁed according to their formal deﬁnitions and the original authors’ implementations. The PIQ package
facilitates the performance evaluation of any computer vision solution where an image-to-image task is performed. In
addition, we provide the comparison of common Image Quality Metrics on the image datasets from the general and the
medical domains, assessing the SRCC and the computation time.

The future development of the PyTorch Image Quality library will be aimed at supporting the latest trends and advances
in the objective Image Quality Assessment and at the improvement of the usability and the scalability of the implemented
algorithms.

7 Acknowledgments

We greatly appreciate all contributions from the members of the PIQ community, including questions, discussions,
design suggestions, and technical implementations.

References

[1] Dominik Söllinger. Blind Image Quality Toolbox. https://github.com/dsoellinger/blind_image_

quality_toolbox, 2017. [Online; accessed 15-June-2022].

[2] Qunliang Xing.

Image Quality Assessment Toolbox.

https://github.com/ryanxingql/

image-quality-assessment-toolbox, 2021. [Online; accessed 11-April-2021].

[3] E. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Bradski. Kornia: an open source differentiable computer vision

library for pytorch. In Winter Conference on Applications of Computer Vision, 2020.

[4] François Rozet. PyTorch Image Quality Assessment. https://github.com/francois-rozet/piqa, 2020.

[Online; accessed 15-June-2022].

[5] Chaofeng Chen. IQA PyTorch. https://github.com/chaofengc/IQA-PyTorch, 2021. [Online; accessed

15-June-2022].

[6] Keyan Ding. IQA Optimization. https://github.com/dingkeyan93/IQA-optimization, 2020. [Online;

accessed 15-June-2022].

[7] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary

8

PyTorch Image Quality: Metrics for Image Quality Assessment

DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.

[8] Willian T Lunardi, Martin Andreoni Lopez, and Jean-Pierre Giacalone. Arcade: Adversarially regularized

convolutional autoencoder for network anomaly detection. arXiv preprint arXiv:2205.01432, 2022.

[9] Segrey Kastryulin, Jamil Zakirov, Nicola Pezzotti, and Dmitry V Dylov. Image quality assessment for magnetic

resonance imaging. arXiv preprint arXiv:2203.07809, 2022.

[10] Jon P Smith. What makes a good software library?

https://www.thereformedprogrammer.net/

what-makes-a-good-software-library, 2015. [Online; accessed 15-June-2022].

[11] Z. Wang, A. C. Bovik, H. R Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to

structural similarity. IEEE TIP, 13(4):600–612, 2004.

[12] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale structural similarity for image quality assessment. In The

37th Asilomar Conference on Signals, Systems & Computers, volume 2, pages 1398–1402, 2003.

[13] Z. Wang and Q. Li.

Information content weighting for perceptual image quality assessment.

IEEE TIP,

20(5):1185–1198, 2010.

[14] Hamid R Sheikh and Alan C Bovik. A visual information ﬁdelity approach to video quality assessment. In The
First International Workshop on Video Processing and Quality Metrics for Consumer Electronics, volume 7,
page 2. sn, 2005.

[15] W. Xue, L. Zhang, X. Mou, and A. C. Bovik. Gradient magnitude similarity deviation: A highly efﬁcient

perceptual image quality index. IEEE TIP, 23(2):684–695, 2013.

[16] B. Zhang, P. V. Sander, and A. Bermak. Gradient magnitude similarity deviation on multiple scales for color

image quality assessment. In 2017 IEEE ICASSP, pages 1253–1257, 2017.

[17] L. Zhang, L. Zhang, X. Mou, and D. Zhang. FSIM: A feature similarity index for image quality assessment.

IEEE TIP, 20(8):2378–2386, 2011.

[18] Lin Zhang and Hongyu Li. Sr-sim: A fast and high performance iqa index based on spectral residual. In 2012

19th IEEE international conference on image processing, pages 1473–1476. IEEE, 2012.

[19] L. Zhang, Y. Shen, and H. Li. VSI: A visual saliency-induced index for perceptual image quality assessment.

IEEE Transactions on Image processing, 23(10):4270–4281, 2014.

[20] Hossein Ziaei Nafchi, Atena Shahkolaei, Rachid Hedjam, and Mohamed Cheriet. Mean deviation similarity

index: Efﬁcient and reliable full-reference image quality evaluator. IEEE Access, 4:5579–5590, 2016.

[21] Rafael Reisenhofer, Sebastian Bosse, Gitta Kutyniok, and Thomas Wiegand. A haar wavelet-based perceptual

similarity index for image quality assessment. Signal Process. Image Commun., 61:33–43, 2018.

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV,

pages 694–711, 2016.

[23] R. Zhang, P. Isola, A. A Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a

perceptual metric. In Proceedings of the IEEE Conference CVPR, pages 586–595, 2018.

[24] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli. Image quality assessment: Unifying structure and

texture similarity. arXiv:2004.07728, 2020.

[25] Ekta Prashnani, Hong Cai, Yasamin Mostoﬁ, and Pradeep Sen. Pieapp: Perceptual image-error assessment
through pairwise preference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 1808–1817, 2018.

[26] Amnon Balanov, Arik Schwartz, Yair Moshe, and Nimrod Peleg. Image quality assessment based on dct subband

similarity. In 2015 IEEE ICIP, pages 2105–2109, 2015.

[27] A. Mittal, A. Krishna Moorthy, and A. C. Bovik. No-reference image quality assessment in the spatial domain.

IEEE Transactions on Image Processing, 21(12):4695–4708, 2012.

[28] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD gans. In ICLR 2018, 2018.
[29] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S Hochreiter. Gans trained by a two time-scale update

rule converge to a local nash equilibrium. In Adv Neural Inform Process Syst, pages 6626–6637, 2017.

[30] V. Khrulkov and I. V. Oseledets. Geometry score: A method for comparing generative adversarial networks. In

ICML, volume 80 of Proceedings of Machine Learning Research, pages 2626–2634, 2018.

9

PyTorch Image Quality: Metrics for Image Quality Assessment

[31] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved

techniques for training gans. In Adv Neural Inform Process Syst, pages 2234–2242, 2016.

[32] Anton Tsitsulin et. al. The shape of data: Intrinsic distance for data distributions. In ICLR 2020: Proceedings of

the International Conference on Learning Representations, 2020.

[33] Christian Szegedy et. al. Going deeper with convolutions. In Proceedings of the IEEE CVPR, pages 1–9, 2015.
[34] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR,

2015.

[35] Shahrukh Athar and Zhou Wang. A comprehensive performance evaluation of image quality assessment

algorithms. IEEE Access, 7:140030–140070, 2019.

[36] Allister Mason at. al.;. Comparison of Objective Image Quality Metrics to Expert Radiologists’ Scoring of

Diagnostic Quality of MR Images. IEEE Transactions on Medical Imaging, 39(4):1064–1072, 2020.

[37] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli. Comparison of full-reference image quality models

for optimization of image processing systems. Int. J. Comput. Vis., 129(4):1258–1281, 2021.

[38] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli. Comparison of full-reference image quality models
for optimization of image processing systems. International Journal of Computer Vision, 129(4):1258–1281,
2021.

[39] Patrick Le Callet and Florent Autrusseau. Subjective quality assessment irccyn/ivc database. IEEE transactions

on image processing, 2005.

[40] Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik. A statistical evaluation of recent full reference image

quality assessment algorithms. IEEE Transactions on image processing, 15(11):3440–3451, 2006.

[41] H. R. Sheikh, Z. Wang, L. Cormack, and A. C. Bovik. Live image quality assessment database release 2, 2006.

[42] DM Chandler and SS Hemami. A57 database, 2007.

[43] Damon M Chandler and Sheila S Hemami. Vsnr: A wavelet-based visual signal-to-noise ratio for natural images.

IEEE transactions on image processing, 16(9):2284–2298, 2007.

[44] Yuukou Horita, Keiji Shibata, Yoshikazu Kawayoke, and ZM Parvez Sazzad. Mict image quality evaluation

database. Online], http://mict. eng. u-toyama. ac. jp/mictdb. html, 2011.

[45] Nikolay Ponomarenko, Vladimir Lukin, Alexander Zelensky, Karen Egiazarian, Marco Carli, and Federica
Battisti. Tid2008-a database for evaluation of full-reference visual quality assessment metrics. Advances of
Modern Radioelectronics, 10(4):30–45, 2009.

[46] Eric Cooper Larson and Damon Michael Chandler. Most apparent distortion: full-reference image quality

assessment and the role of strategy. Journal of electronic imaging, 19(1):011006, 2010.

[47] E.C.Larson and D.M.Chandler. Computational and subjective image quality (csiq) database, 2010.

[48] Clément Strauss, François Pasteau, Florent Autrusseau, Marie Babel, Laurent Bédat, and Olivier Déforges.
Subjective and objective quality evaluation of lar coded art images. In Proceedings of the 2009 IEEE International
Conference on Multimedia and Expo, ICME 2009, June 28 - July 2, 2009, New York City, NY, USA, pages
674–677. IEEE, 2009.

[49] U. Engelke, H.-J. Zepernick, and M. Kusuma. Wireless imaging quality database.

https://

computervisiononline.com/dataset/1105138665, 2010. Online; Accessed 03 March 2022.

[50] Lin Ma, Weisi Lin, Chenwei Deng, and King Ngi Ngan. Image retargeting quality assessment: A study of

subjective scores and objective metrics. IEEE J. Sel. Top. Signal Process., 6(6):626–639, 2012.

[51] Andjela Zari´c, Nenad Tatalovi´c, Nikolina Brajkovi´c, Hrvoje Hlevnjak, Matej Lonˇcari´c, Emil Dumi´c, and
Sonja Grgi´c. Vcl@ fer image quality assessment database. AUTOMATIKA: ˇcasopis za automatiku, mjerenje,
elektroniku, raˇcunarstvo i komunikacije, 53(4):344–354, 2012.

[52] Andjela Zari´c. Vclfer image quality assessment database., 2012.

[53] Dinesh Jayaraman, Anish Mittal, Anush K Moorthy, and Alan C Bovik. Objective quality assessment of multiply
distorted images. In 2012 Conference record of the forty sixth asilomar conference on signals, systems and
computers (ASILOMAR), pages 1693–1697. IEEE, 2012.

[54] D Jayaraman, A. Mittal, K. Moorthy, and A. C. Bovik. Live multiply distorted image quality database., 2012.
[55] Nikolay Ponomarenko et. al. Image database tid2013: Peculiarities, results and perspectives. Sig proces: Image

com, 30:57–77, 2015.

10

PyTorch Image Quality: Metrics for Image Quality Assessment

[56] Ke Gu, Guangtao Zhai, Xiaokang Yang, and Wenjun Zhang. Hybrid no-reference quality metric for singly and

multiply distorted images. IEEE Transactions on Broadcasting, 60(3):555–567, 2014.

[57] Toni Virtanen, Mikko Nuutinen, Mikko Vaahteranoksa, Pirkko Oittinen, and Jukka Häkkinen. CID2013:
A database for evaluating no-reference image quality assessment algorithms. IEEE Trans. Image Process.,
24(1):390–402, 2015.

[58] Xinwei Liu, Marius Pedersen, and Jon Yngve Hardeberg. CID: IQ - A new image quality database. In Abderrahim
Elmoataz, Olivier Lezoray, Fathallah Nouboud, and Driss Mammass, editors, Image and Signal Processing - 6th
International Conference, ICISP 2014, Cherbourg, France, June 30 - July 2, 2014. Proceedings, volume 8509 of
Lecture Notes in Computer Science, pages 193–202. Springer, 2014.

[59] Huan Yang, Yuming Fang, and Weisi Lin. Perceptual quality assessment of screen content images. IEEE

Transactions on Image Processing, 24(11):4408–4421, 2015.

[60] Deepti Ghadiyaram and Alan C. Bovik. Massive online crowdsourced study of subjective and objective picture

quality. IEEE Trans. Image Process., 25(1):372–387, 2016.

[61] Deepti Ghadiyaram and Alan C. Bovik. LIVE In the Wild Image Quality Challenge Database. http://live.

ece.utexas.edu/research/ChallengeDB/index.html, 2015. Online; Accessed 03 March 2022.

[62] Silvia Corchs and Francesca Gasparini. A multidistortion database for image quality. In International Workshop

on Computational Color Imaging, pages 95–104. Springer, 2017.

[63] Silvia Corchs and Francesca Gasparini. Multiply distorted database md-ivl., 2017.
[64] Wen Sun, Fei Zhou, and Qingmin Liao. Mdid: A multiply distorted image database for image quality assessment.

Pattern Recognition, 61:153–168, 2017.

[65] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid database for deep
learning of blind image quality assessment. IEEE Transactions on Image Processing, 29:4041–4056, 2020.
[66] Hanhe Lin, Vlad Hosu, and Dietmar Saupe. Kadid-10k: A large-scale artiﬁcially distorted iqa database. In 2019

Tenth Intern Conf on Quality of Multimedia Experience (QoMEX), pages 1–3. IEEE, 2019.

[67] Hanhe Lin, Vlad Hosu, and Dietmar Saupe. Deepﬂ-iqa: Weak supervision for deep iqa feature learning. arXiv

preprint arXiv:2001.08113, 2020.

[68] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, and Alan Bovik. From patches

to pictures (paq-2-piq): Mapping the perceptual space of picture quality, 2019.

[69] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone
photography. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
3677–3686, 2020.

[70] Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy Ren, and Chao Dong. Pipal: a large-scale image
quality assessment dataset for perceptual image restoration. In European Conference on Computer Vision (ECCV)
2020, pages 633–651, Cham, 2020. Springer International Publishing.

[71] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S. Ren, Yu Qiao, Shuhang Gu, Radu Timofte, Manri Cheon,
Sungjun Yoon, Byungyeon Kang, Junwoo Lee, Qing Zhang, Haiyang Guo, Yi Bin, Yuqing Hou, Hengliang Luo,
Jingyu Guo, Zirui Wang, Hai Wang, Wenming Yang, Qingyan Bai, Shuwei Shi, Weihao Xia, Mingdeng Cao,
Jiahao Wang, Yifan Chen, Yujiu Yang, Yang Li, Tao Zhang, Longtao Feng, Yiting Liao, Junlin Li, William Thong,
Jose Costa Pereira, Ales Leonardis, Steven McDonagh, Kele Xu, Lehan Yang, Hengxing Cai, Pengfei Sun,
Seyed Mehdi Ayyoubzadeh, Ali Royat, Sid Ahmed Fezza, Dounia Hammou, Wassim Hamidouche, Sewoong
Ahn, Gwangjin Yoon, Koki Tsubota, Hiroaki Akutsu, and Kiyoharu Aizawa. NTIRE 2021 Challenge on
Perceptual Image Quality Assessment. arXiv, 2021.

[72] Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy Ren, and Chao Dong. Image quality assessment for

perceptual image restoration: A new dataset, benchmark and metric. arXiv preprint arXiv:2011.15002, 2020.

[73] Andela Zaric, Nenad Tatalovic, Nikolina Brajkovic, Hrvoje Hlevnjak, Matej Loncaric, Emil Dumic, and Sonja
Grgic. Vcl@fer image quality assessment database. In Proceedings ELMAR-2011, pages 105–110, 2011.
[74] Hanhe Lin, Vlad Hosu, and Dietmar Saupe. KonIQ-10k: Towards an ecologically valid and large-scale IQA

database. IEEE Transactions on Image Processing, 29:4041–4056, mar 2018.

[75] H. R. Sheikh, M. F. Sabir, and A. C. Bovik. A statistical evaluation of recent full reference image quality

assessment algorithms. IEEE Transactions on Image Processing, 15(11):3440–3451, 2006.

[76] Sergey Kastryulin, Djamil Zakirov, and Denis Prokopenko. PyTorch Image Quality: Metrics and measure
for image quality assessment, 2019. Open source software available from https://github.com/photosynthesis-
team/piq.

11

PyTorch Image Quality: Metrics for Image Quality Assessment

[77] James Mannos and David Sakrison. The effects of a visual ﬁdelity criterion of the encoding of images. IEEE

transactions on Information Theory, 20(4):525–536, 1974.

[78] Z. Wang and A. C. Bovik. Modern image quality assessment. Synthesis Lectures on Image, Video, and Multimedia

Processing, 2(1):1–156, 2006.

[79] Zhou Wang and Alan C Bovik. Mean squared error: Love it or leave it? a new look at signal ﬁdelity measures.

IEEE signal processing magazine, 26(1):98–117, 2009.

[80] Niranjan Damera-Venkata, Thomas D Kite, Wilson S Geisler, Brian L Evans, and Alan C Bovik. Image quality
assessment based on a degradation model. IEEE transactions on image processing, 9(4):636–650, 2000.

[81] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. Advances in neural information processing systems, 25, 2012.

[82] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks
for single image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern
recognition workshops, pages 136–144, 2017.

[83] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew
Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using
a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 4681–4690, 2017.

[84] Yudong Liang, Jinjun Wang, Xingyu Wan, Yihong Gong, and Nanning Zheng. Image quality assessment using

similar scene as reference. In European Conference on Computer Vision, pages 3–18. Springer, 2016.

[85] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint
arXiv:1602.07360, 2016.

[86] Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy Ren, and Chao Dong. Image quality assessment for

perceptual image restoration: A new dataset, benchmark and metric. arXiv preprint arXiv:2011.15002, 2020.

[87] Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Ren Jimmy, and Chao Dong. Pipal: a large-scale image
quality assessment dataset for perceptual image restoration. In European Conference on Computer Vision, pages
633–651. Springer, 2020.

[88] Scott J Daly. Visible differences predictor: an algorithm for the assessment of image ﬁdelity. In Human Vision,

Visual Processing, and Digital Display III, volume 1666, pages 2–15. SPIE, 1992.

[89] Chun-Hsien Chou and Yun-Chin Li. A perceptually tuned subband image coder based on the measure of
just-noticeable-distortion proﬁle. IEEE Transactions on circuits and systems for video technology, 5(6):467–476,
1995.

[90] Guan-Hao Chen, Chun-Ling Yang, Lai-Man Po, and Sheng-Li Xie. Edge-based structural similarity for
image quality assessment. In 2006 IEEE International Conference on Acoustics Speech and Signal Processing
Proceedings, volume 2, pages II–II. IEEE, 2006.

[91] A. Liu, W. Lin, and M. Narwaria. Image quality assessment based on gradient similarity. IEEE Transactions on

Image Processing, 21(4):1500–1512, 2012.

[92] Bo Zhang, Pedro V Sander, and Amine Bermak. Gradient magnitude similarity deviation on multiple scales
for color image quality assessment. In 2017 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 1253–1257. IEEE, 2017.

[93] Rafael Reisenhofer, Sebastian Bosse, Gitta Kutyniok, and Thomas Wiegand. A haar wavelet-based perceptual
similarity index for image quality assessment. Signal Processing: Image Communication, 61:33–43, 2018.

[94] Hossein Ziaei Nafchi, Atena Shahkolaei, Rachid Hedjam, and Mohamed Cheriet. Mean deviation similarity

index: Efﬁcient and reliable full-reference image quality evaluator. Ieee Access, 4:5579–5590, 2016.

[95] Eero P Simoncelli, William T Freeman, Edward H Adelson, and David J Heeger. Shiftable multiscale transforms.

IEEE transactions on Information Theory, 38(2):587–607, 1992.

[96] Vin De Silva and Gunnar E Carlsson. Topological estimation using witness complexes. SPBG, 4:157–166, 2004.

[97] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.

[98] Amnon Balanov, Arik Schwartz, and Yair Moshe. Reduced-reference image quality assessment based on dct
subband similarity. In 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX),
pages 1–6. IEEE, 2016.

12

PyTorch Image Quality: Metrics for Image Quality Assessment

[99] Ke Gu, Guangtao Zhai, Xiaokang Yang, and Wenjun Zhang. A new reduced-reference image quality assessment
using structural degradation model. In 2013 IEEE international symposium on circuits and systems (ISCAS),
pages 1095–1098. IEEE, 2013.

[100] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on

intelligent systems and technology (TIST), 2(3):1–27, 2011.

[101] Ke Gu, Guangtao Zhai, Xiaokang Yang, Wenjun Zhang, and Min Liu. Subjective and objective quality assessment
for images with contrast change. In 2013 IEEE International Conference on Image Processing, pages 383–387.
IEEE, 2013.

[102] Isamu Motoyoshi, Shin’ya Nishida, Lavanya Sharan, and Edward H Adelson. Image statistics and the perception

of surface qualities. Nature, 447(7141):206–209, 2007.

[103] Daniel Zoran and Yair Weiss. Scale invariance and noise in natural images. In 2009 IEEE 12th International

Conference on Computer Vision, pages 2209–2216. IEEE, 2009.

[104] Jinjian Wu, Weisi Lin, Guangming Shi, Leida Li, and Yuming Fang. Orientation selectivity based visual pattern

for reduced-reference image quality assessment. Information Sciences, 351:18–29, 2016.

13

PyTorch Image Quality: Metrics for Image Quality Assessment

A Appendix. Quality Metrics

Figure 4: General taxonomy of IQA metrics. Italic metrics are available with PIQ package. ADD WSPNR, IW-PSNR,
SWDN.

Historically, the ﬁrst works on perceptual full-reference IQA appeared almost half a century ago, with the pioneering
work of Sakrison and Mannos [77] focusing on a class of visual ﬁdelity criterion in the context of image encoding. Over
the past few decades, a number of alternative models mimicking certain functionalities of the Human Visual System
(HVS) were proposed.

Such approaches couldn’t model the real HVS, which is a complex and highly nonlinear system, while most models rely
on simpliﬁcations and strong assumptions (e.g. linearity or quasi-linearity for visual stimuli) and exhibit shortcomings
regarding the deﬁnition of visual quality, quantiﬁcation of suprathreshold distortions, and generalization to natural
images [78]. Hence, IQMs can be classiﬁed and put into a certain category based on their main computation mechanism
as shown in ﬁgure 4.

In our work, we choose to evaluate representative methods from those categories. A short description of the design
philosophies is given below.

Error Based Methods

Point-by-point comparisons between pixels or convolution responses (e.g, wavelets, CNNs) is the simplest way of
measuring perceptual quality.

MSE [11], the Mean Squared Error ((cid:96)2-norm), and closely related PSNR [78], the Peak Signal-to-Noise Ratio, are the
most frequently used quality metrics, which are a de-facto standart way of measuring image quality. MSE is easy to use,
has clear physical meaning (energy of image distortions), satisﬁes the Parseval’s theorem and can be used for algorithm
optimization leading to a closed-form solutions [78, 79]. PSNR is deﬁned as a ratio between the maximum possible
power of a signal and the power of corrupting noise that affects the ﬁdelity of signal representation [78].

MSE and PSNR have been repeatedly shown to poorly correlate with human judgements in controlled experiments
[55, 66]. The main reason for this is four strong underlying assumptions about visual quality [79]: 1. Independence of
spatial relationships between samples, 2. Independence of relationship between signal and error, 3. Independence of
sign of error samples, 4. Equal importance of all signal samples and errors. None of this assumptions hold on in real
life and accurately describe human’s visual system.

14

Image Quality Assessment MetricsFull-ReferenceFused Full-ReferenceReduced-ReferenceNo-ReferenceError BasedStructural Similarity BasedStatistics BasedMixed Strategy BasedEmpirical FusionLearning FusionRank Aggregation FusionOpinion AwareOpinion UnawarePSNRPieAPPSSIMMS-SSIMIW-SSIMFSIM(с)SR-SIM(с)VIFMADHFSIMcMMFBLISSBRISQUE BIQI     CORNIA     HOSA TVRR-DSSReduced-ReferenceNatural Scene StatisticsFIDKIDImproved Precision and RecallDistributionStatisticsVSIDSSHaarPSIMDSIContent/Style ScoreGMSDMS-GMSD(с)LPIPSDISTSMSENQM CSFDVICOMADMDWT-VIFSFFGSMSIDInceptionScoreMMDIFCCISICQMEHISSFMSBIRDRRFSDMRIQMCNIQEILNIQEQACOSVPPyTorch Image Quality: Metrics for Image Quality Assessment

A number of subsequent papers addressed the weaknesses of PSNR and modiﬁed it to better suit for IQA. WSNR [80],
the Weighted Signal-to-Noise Ratio, used contrast sensitivity function to approximate HVS and assign different weights
to signal and noise components, leading to a linear quality measure. NQM [80], the Noise Quality Measure, also used
nonlinear quasi-local processing model of the HVS to accomplish quality assessment.

IW-PSNR [13], the Information Weighted PSNR, is a continued idea of applying additional "weights" to address
importance of different areas. It uses information theoretic principles to add weights for regions of visual content,
which are perceptually more important than others, either due to the visual attention property of the HVS or due to the
inﬂuence of distortions.

MAD [46], the Most Apparent Distortion measure, explicitly models adaptive strategies of the human visual system.
For high quality images with only near-threshold distortions a detection-based strategy is employed, and an appearance-
based strategy is activated if the distortions are clearly visible. The results are then combined into a single score by
weighting scheme, where importance of each strategy is dependent on the distortions strength.

Following the success in image classiﬁcation [81] and recent developments in deep learning [82, 83] usage of con-
volutional neural networks (CNN) became a popular way for constructing IQA methods. A double-path CNN was
introduced by the pioneering work of Liang et al. [84]. Here it was proposed to crop images into small patches and
propagate them through a dedicated network branches to obtain patch scores. The scores determined the ﬁnal overall
image quality score by averaging predicted patch-wise values. The model was trained using regression to MOS scores
from TID2013 [55] database.

Another popular approach, proven to be useful for many image processing tasks, is the usage of generic features
obtained from pre-traind networks, such as AlexNet [81], VGG [34], SqueezeNet [85] and others. Perceptual Loss
and Style Score were introduced by Johnson et al. [22]. Perceptual Loss used a single-path CNN trained on ImageNet
ILSVRC 2012 [81] to extract deep feature representations of reference and distorted images. Those features were
then compared by taking MSE and averaging error between spatial and feature dimensions. Style Score extracts and
compares texture information from the images by computing Gram matrices between features from a CNN and taking
MSE between them.

In LPIPS [23], the Learned Perceptual Image Patch Similarity, Zhang et al. pointed out that methods based on utilization
of deep features signiﬁcantly outperform traditional algorithms on a wide range of distortions. LPIPS computes the
distance between feature representations on multiple levels, similar to Perceptual Score, building on a premise that
different layers represent different structures of the image. To proper combine features, they are ﬁrst unit-normalized to
have similar scales and then summed with weights learned on BAPPS [23] dataset to minimize error between model
and humans preference over two images.

PieAPP [25], the Perceptual Image-Error Assessment through Pairwise Preference metric, uses a pairwise-learning
framework to predict the preference of one distorted image over the other. PieAPP gets features from the feature-
extraction (FE) network and then computes the similarity using a score-computation (SC) network. Both FE and SC are
trained from scratch targeting humans pairwise-preference between images.

SWDN [86], the Space Warping Difference Network, is speciﬁcally designed to handle geometric distortions. When
comparing two image features, a new Space Warping Difference layer takes into account not only pixels on correspond-
ing positions as in all pixel-wise methods, but also looks into a small range around them.

Motivated by the above-mentioned results, a number of other FR-IQA algorithms have been proposed relying on
different deep features, pooling strategies and pre-trained CNNs [87, 24, 86].

Structural Similarity Based Methods

IQMs that use certain properties of HVS can be divided into 2 groups: top-down and bottom-up ones. In previous
section we described bottom-up approaches to IQA design, which used properties of human visual system to modify
error-based MSE and PSNR to better simulate different components of HVS. For example, by adding adaptation to
luminance, contrast sensitivity and contrast masking [88, 89].

In contrary, approaches following top-down IQA design try to mimic the functionality of HVS as a whole and do not
model it by individual components. Structural similarity is a perception-based model that considers image degradation
as perceived change in structural information, while also including luminance and contrast masking terms. These
methods are based on the idea that pixels have strong inter-dependencies especially when they are spatially close.

SSIM [78], the Structural Similarity Index Measure, is the most popular top-down approach which has become a
de-facto standard in the ﬁeld of perceptual image processing (along with PSNR) and has inspired subsequent IQA
models based on feature similarity [79].

15

PyTorch Image Quality: Metrics for Image Quality Assessment

The main idea behind SSIM is to split image distortions into structural and not-structural and focus on the ﬁrst ones, as
the latter are less noticeable by HVS. The comparison is done between luminance (average pixel intensity), contrast
(standard deviation of the local image regions) and structure (cross correlation values between two local image regions)
components, which are later combined into a single quality map by averaging of local quality scores. The main
disadvantage of SSIM is that it takes into account only single image scale, and thus can’t adapt to different sets of
viewing conditions.

MS-SSIM [12], the Multi-scale SSIM, measures SSIM on 5 different scales, computing contrast and similarity on all
levels, while measuring luminance only at the ﬁnal scale. Resulting scores than combined through a weighted product
using weights adjusted on human dataset of mean opinion scores.

IW-SSIM [13], the Information content Weighted SSIM, is an extension of MS-SSIM, that added computation of
additional content weights based on information theoretic principles.

After the success of SSIM, a lot of new works searched for effective image features that be used to describe contrast,
structural information and textures.

ESSIM [90], the Edge-based SSIM, uses ﬁrst order difference operators to compare information between image edges
and claimed, that edges are the most important structure information for the HVS.

GSM [91], the Gradient Similarity Measure, computes image gradients in horizontal and vertical directions to use those
features as an input to the structural similarity computation.

GMSD [15], the Gradient Magnitude Similarity Deviation, is focused on computational efﬁciency of quality predictions.
Based on the idea that global variation of image local quality degradation can reﬂect its overall quality, authors proposed
to compute the standard deviation of the pixel-wise gradient similarity map as an IQA index. This method is, however,
problematic because an image with a large but constant local distortion yields a standard deviation of zero, indicating
the best predicted quality.

MS-GMSD [92], the Multi-scale GMSD, accounts for the variations in viewing conditions by measuring GMSD on 5
different scales, similarly to MS-SSIM. The chromatic version, named MS-GMSDc, additionally measures chrominance
dissimilarity on the last scale, motivated by lower HSV sensitivity to colour distortions.

FSIM [17], the Feature Similarity Index Measure, is built on assumption that low-level features obtained in the
early stage of HVS information processing are used for understanding the image content. Two core features used in
similarity computations are phase congruency, which is a contrast-invariant dimensionless measure of the local structure,
and gradient magnitude maps obtained by Scharr ﬁlter. During pooling, phase congruency component serves as an
adaptive local weighting factor to derive an overall visual quality score. The color-sensitive version of FSIM, is called
FSIMc [17]. It ﬁrst converts RGB images into YIQ color space, and then computes FSIM on luminance channel and
chrominance similarity maps on I and Q channels.

VSI [19], the Visual Saliency Induced quality index, states that the change of salience in degraded areas is the major
predictor of image quality. The visual saliency index is computed using phase congruency and two simple priors (color
temperature and center priors). Gradient magnitude maps are used as an additional feature and pooling strategy mimics
FSIM measure. VSI shows a good correlation with human judgments on localized distortions, such as compression
artefacts or local patch substitutions.

Recently proposed HaarPSI [93], the Haar perceptual similarity index, decomposes both distorted and reference images
into Haar wavelets and computes the structural similarity between magnitudes of high-frequency coefﬁcients. The last
level of Haar wavelet is used to weight the importance of different regions.

MDSI [94], Mean Deviation Similarity Index, combines the gradient similarity, chrominance similarity and deviation
pooling (used in GMSD) into a single IQA model. MDSI contains 7 conﬁgurable parameters that are jointly optimized
on a mean opinion scores dataset to provide best correlation results.

DISTS [24], the Deep Image Structure and Texture Similarity measure, is a deep-learning based IQA algorithm that
follows LPIPS design principles. Image representations are extracted from the pre-trained convolutional neural network
(VGG16 [34]) and combined using SSIM-like structure and texture similarity measurements. It is sensitive to structural
distortions but at the same time robust to texture resampling and modest geometric transformations [37].

DSS, the DCT Subbands Similarity, aims to measure changes in structural information using sub-bands in the discrete
cosine transform (DCT) domain. DSS extracts features in block-based DCT subbands and measures the distances
between corresponding DCT sub-bands. The ﬁnal quality index is computed by pooling those distances together with
weights. The main motivation behind quality assessment in the DCT domain is the observation that the statistics of
DCT coefﬁcients change with the degree and type of image distortion.

16

PyTorch Image Quality: Metrics for Image Quality Assessment

Natural Scene Statistics based

These methods attempt to measure some approximation of the mutual information between the perceived reference and
distorted images as an indication of perceptual image quality. Statistical modeling of the image source, the distortion
process, and the HVS is critical in algorithm development.

Visual Information Fidelity (VIF) [14] uses a Gaussian scale mixture to statistically model the wavelet coefﬁcients of a
steerable pyramid decomposition of an image [95]. After that, it predicts the distorted image quality by quantifying the
amount of preserved information from the reference image. Its spatial domain, named VIFp, computes ﬁdelity on raw
pixels.

Distribution Statistics based

This class of methods originated from the domain of generative modeling, where evaluation by a direct comparison is
not possible and only the distance between model distributions can be measured.

Freshet Inception Distance (FID) [29] considers embeddings of the two data distributions as a product of continuous
multivariate Gaussian. The mean and covariance are estimated for both, and the Fréchet distance between these two
Gaussians (a.k.a Wasserstein-2 distance) is then used to quantify the quality of the generated sample.

Kernel Inception Distance (KID) [28], sometimes referred to as Maximum Mean Discrepancy (MMD) - computes the
dissimilarity between two probability distributions P and Q by measuring squared MMD for some ﬁxed characteristic
kernel function (e.g., Gaussian kernel).

Multi-Scale Intrinsic Distance (MSID) [32] develops an intrinsic and multi-scale method for characterizing and
comparing data manifolds, using a lower-bound of the spectral Gromov-Wasserstein inter-manifold distance, which
compares all data moments.

Geometry Score (GS) [30] constructs a performance measure by comparing geometrical properties of the underlying
data manifolds. In particular, topological approximation for computation of connected components ("holoes") in
homology using witness complex [96] is introduced.

No-Reference Opinion Aware Methods

No-Reference (NR) IQA methods evaluate distorted image’s quality in the absence of any reference information [40].
Thus they are also referred to as blind IQA methods.

Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) operates on locally normalized luminance values in
the spatial domain, called Mean Subtracted Contrast Normalized (MSCN) coefﬁcients. Various features are extracted
from the MSCN coefﬁcients and their pairwise products, which are then used to estimate Generalized Gaussian
Distribution (GGD) and Asymmetric GGD parameters. SVR model is then used to learn to map the features to the
quality score.

Inception score (IS) [31] uses the pre-trained InceptionNet model for feature extraction and captures properties of
generated sample to capture properties of generated samples: diversity with respect to class labels and high classiﬁability.

Improved Inception score (IS’) [97] proposed a different way of feature aggregation, which improved both calculation
and interpretability of the Inception Score.

Reduced Reference Methods

Reduced-reference (RR) image quality assessment methods estimate the quality of distorted images using only
partial information about the reference images when the full-reference approaches are unfeasible. Partial information
represented by extracted features should be relevant to the human judgement of image quality and sensitive to a variety
of image distortions. Consequently, the challenge of a reduced reference metric is to ﬁnd optimal trade-off between the
amount of information represented by extracted features and their correlation with HVS.

RR-DSS [98], a reduced-reference image quality assessment based on DCT sub-band similarity measure. It is an
adapted full-reference DSS quality assessment measure, which assumes that HVS adapts for extracting structural
information. RR-DSS uses only a few lowest frequency sub-bands for the quality assessment. In order to maintain good
IQA results, RR-DSS should use at least 3 to 10 sub-bands. RR-DSS uses spatial down-sampling to reduce amount of
information about reference image. As typical distortions are usually spread over the image, uniform down-sampling of
the local variances preserves important features of the distortions. However, down-sampling makes it impossible to
compute the cross-correlation for the DC sub-band. Therefore, local similarity score for the DC sub-band is computed

17

PyTorch Image Quality: Metrics for Image Quality Assessment

in the same way as for the AC sub-bands. RR-DSS showed high correlation with subjective results on average and
outperformed most other metrics examined in [98], both RR and FR, including SSIM and MS-SSIM. In addition,
the method has a simple implementation and incurs low computational complexity, while the trade-off between side
information and image quality estimation accuracy can be adjusted according to the task.

SDM [99], a reduced-reference IQA using Structural Degradation Model, consists of two main stages: acquiring
structural degradation information for distorted and original images and consecutive aggregation into a single score.
Structural degradation information (SD) can be deﬁned as structural similarity indexes between mean features and
variance features obtained using kernels with various variation values computed for different parts of the images. The
SD information is used to derive distances between distorted and reference images, which are linearly combined using
regression model parameters optimised by training. In particular, one can use SVM [100] for regression as a model and
get an S-SDM score. The SDM approach relies on various spatial responses providing low computational complexity
and fast execution. However, optimisation of model parameters leads to dependency on the dataset used for training,
which might limit the generalisation properties of the metric between different image domains.

RIQMC [101], a reduced-reference image quality metric for contrast-changed images, aims to evaluate image quality
according to contrast features of a picture such as skewness and kurtosis of the image histograms, which correlates
with HVS [102, 103]. RIQMC is based on the ﬁrst four order statistics, which are evaluated for a distorted image,
and values of entropy for distorted and reference images. The ﬁnal value of RIQMC is a linear combination of the
entropy and the ﬁrst four order statistics. Even though the RIQMC metric needs only an entropy of a reference image, it
outperforms full-reference metrics such as PSNR, SSIM, MS-SSIM, IW-SSIM, and MAD, on the CID2013 dataset and
TID2008 subset according to PLCC and SROCC [101]. However, the performance of RIQMC may vary for general
image distortions as the method was proposed exclusively for contrast-changed images.

OSVP [104], an orientation selectivity-based visual pattern IQA, is a reduced-reference measure inspired by the
orientation selectivity (OS) mechanism for visual content extraction by the human visual system. When visual content
is observed, the input signal interacts with the visual cortex depending on the spatial arrangement in local perceptual
ﬁelds generating OS visual patterns. In order to represent the OS mechanism, gradient directions are extracted per pixel,
building the spatial relationship between each pixel and its local neighbours (OSVP). Next, the content of an image is
mapped into a histogram according to spatial relationship patterns. The ﬁnal score is calculated as changes between
the histograms for reference and distorted images. Being inspired by neuroscience ﬁndings in orientation selectivity
mechanism, OSVP RR-IQA showed performance consistent with HVS perception on ﬁve publicly available databases
with limited reference data.

18

PyTorch Image Quality: Metrics for Image Quality Assessment

B Appendix. Additional Benchmarks

The main part of the work considered the performance – complexity trade-off for KADID10k [66] and MRI reconstruc-
tions [9] datasets. Here we present additional benchmarks for two commonly used datasets: TID2013 [55] and PIPAL
[70] of CPU (Fig. 5) and GPU (Fig. 6).

Figure 5: Relationship between metrics’ computation time on CPU and their performance in terms of SRCC score on
Natural Images from TID2013 [55] (top row) and PIPAL [70] (bottom row) datasets for all metrics (left column) and
zoomed-in region indicated in red (right column). Metrics with the best time-quality relation are located in the top-left
corner.

19

3579log(time, ms)0.20.30.40.50.60.70.80.9Spearman's Rank Correlation CoefficientPSNRSSIMVIFpLPIPSDISTSPieAPPContentStyleBRISQUEISFIDGSKIDMSID2.53.03.54.04.5log(time, ms)0.800.850.90Spearman's Rank Correlation CoefficientMS-SSIMIW-SSIMGMSDMS-GMSDMS-GMSDcFSIMFSIMcSR-SIMSR-SIMcVSIMDSIHaarPSIDSS3579log(time, ms)0.00.10.20.30.40.50.60.7Spearman's Rank Correlation CoefficientPSNRLPIPSDISTSPieAPPContentStyleBRISQUEISFIDGSKIDMSID2.53.03.54.04.5log(time, ms)0.500.550.600.65Spearman's Rank Correlation CoefficientSSIMMS-SSIMIW-SSIMVIFpGMSDMS-GMSDMS-GMSDcFSIMFSIMcSR-SIMSR-SIMcVSIMDSIHaarPSIDSSPyTorch Image Quality: Metrics for Image Quality Assessment

Figure 6: Relationship between metrics’ computation time on GPU and their performance in terms of SRCC score on
Natural Images from TID2013 [55] (top row) and PIPAL [70] (bottom row) datasets for all metrics (left column) and
zoomed-in region indicated in red (right column). Metrics with the best time-quality relation are located in the top-left
corner.

20

3579log(time, ms)0.20.30.40.50.60.70.80.9Spearman's Rank Correlation CoefficientPSNRSSIMIW-SSIMVIFpLPIPSContentStyleBRISQUEISFIDGSKIDMSID2.22.42.62.83.03.23.43.63.8log(time, ms)0.800.850.90Spearman's Rank Correlation CoefficientMS-SSIMGMSDMS-GMSDMS-GMSDcFSIMFSIMcSR-SIMSR-SIMcVSIHaarPSIMDSIDISTSPieAPPDSS3579log(time, ms)0.00.10.20.30.40.50.60.7Spearman's Rank Correlation CoefficientPSNRIW-SSIMPieAPPContentStyleBRISQUEISFIDGSKIDMSID2.22.42.62.83.03.23.43.6log(time, ms)0.500.550.600.65Spearman's Rank Correlation CoefficientSSIMMS-SSIMVIFpGMSDMS-GMSDFSIMFSIMcSR-SIMSR-SIMcVSIHaarPSIMDSILPIPSDISTSDSS