Prepared for submission to JINST

Energy reconstruction for large liquid scintillator

detectors with machine learning techniques: aggregated

features approach

ğ‘,ğ‘,1 Yu. Malyshkin,

A. Gavrikov,
ğ‘HSE University, Moscow, Russia
ğ‘Joint Institute for Nuclear Research, Dubna, Russia

F. Ratnikov

ğ‘

ğ‘

E-mail: gavrikov@jinr.ru

Abstract: Large scale detectors consisting of a liquid scintillator (LS) target surrounded by an
array of photo-multiplier tubes (PMT) are widely used in modern neutrino experiments: Borexino,
KamLAND, Daya Bay, Double Chooz, RENO, and upcoming JUNO with its satellite detector TAO.
Such apparatuses are able to measure neutrino energy, which can be derived from the amount of
light and its spatial and temporal distribution over PMT-channels. However, achieving a ï¬ne energy
resolution in large scale detectors is challenging.

In this work, we present machine learning methods for energy reconstruction in JUNO, the
most advanced detector of its type. We focus on positron events in the energy range of 0â€“10 MeV
which corresponds to the main signal in JUNO â€” neutrinos originated from nuclear reactor cores
and detected via an inverse beta-decay channel. We consider Boosted Decision Trees and Fully
Connected Deep Neural Network trained on aggregated features, calculated using information
collected by PMTs. We describe the details of our feature engineering procedure and show that
machine learning models can provide energy resolution ğœ = 3% at 1 MeV using subsets of
engineered features. The dataset for model training and testing is generated by the Monte Carlo
method with the oï¬ƒcial JUNO software. Consideration of calibration sources for evaluation of the
reconstruction algorithms performance on real data is also presented.

Keywords: Large detector-systems performance; Neutrino detectors; Data processing methods

ArXiv ePrint: arxiv

1Corresponding author.

2
2
0
2

n
u
J

7
1

]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[

1
v
0
4
0
9
0
.
6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Contents

1

Introduction

2 Problem statement

3 Data description
3.1 Overview
3.2 Data structure

4 Machine learning approach
4.1 Feature Engineering
4.2 Boosted decision trees

Feature selection

4.2.1
4.2.2 Hyperparameter optimization
4.3 Fully connected deep neural network

5 Results

6 Calibration

7 Conclusions

1

Introduction

1

3

4
4
5

5
5
8
9
10
12

13

15

18

One of the neutrino detection methods consists of observation of optical photons produced by
secondary charged particles from neutrino interactions in the target volume. Although charged
particles can produce Cherenkov light even in water, a class of detectors, e.g. Borexino [1],
KamLAND [2], Daya Bay [3], Double Chooz [4], and RENO [5], used Liquid Scintillators (LS) to
increase the light yield. Modern LS compositions provide ğ‘‚ (104) photons per MeV of deposited
energy, a signiï¬cant part of which can be observed with Photo-Multiplier Tubes (PMT) placed
around the target. Since the amount of emitted photons is deï¬ned by the deposited energy, it can
be easily estimated. Usage of extra information coming from the temporal distribution and spatial
pattern of ï¬red PMTs allows to improve the energy measurement accuracy. However, in the case
of future detectors with a large number of channels (PMTs), the task becomes challenging. We
consider novel approaches based on machine learning for the JUNO detector, the largest detector
of its type currently under construction. These approaches can be also applied to other similar
detectors.

The Jiangmen Underground Neutrino Observatory (JUNO) [6, 7] is a multipurpose neutrino
observatory, under construction, located in southern China. The primary aims of the JUNO
experiment are to determine the mass ordering of neutrino and to precisely measure their oscillation

â€“ 1 â€“

, Î”ğ‘š2

parameters sin2 ğœƒ12, Î”ğ‘š2
31. The main source of neutrinos in JUNO will be reactor neutrinos
21
from the Yangjiang and Taishan nuclear power plants, which are located 53 kilometers from the
detector. Concurrently, JUNO will be able to explore supernovae neutrinos, atmospheric neutrinos,
solar neutrinos and geoneutrinos, as well as rare processes like proton decay.

Figure 1: A schematic view of the JUNO detector and the other main components.

The key requirement for JUNO is to provide energy resolution of ğœğ¸ /ğ¸ = 3%/âˆšï¸ğ¸ (MeV).
The detector construction illustrated in Figure 1 is optimized to meet this requirement. The JUNO
detector consists of the Central Detector (CD), the water Cherenkov detector and the Top Tracker.
CD is an acrylic sphere of 35.4 meters in diameter ï¬lled with 20 kt of liquid scintillator (LS). CD is
held by the stainless steel construction immersed in a water pool. It is equipped with a large number
of photo-multiplier tubes (PMT) of two types: 17 612 large 20-inch tubes and 25 600 small 3-inch.
The former ones provide 75.2% of the sphere coverage and the latter ones ensure an extra 2.7% of
coverage [7]. The water pool is also equipped with 2400 20-inch PMTs to detect Cherenkov light
from muons. The Top Tracker is disposed at the top of the detector, which is used to detect muon
tracks. The Calibration house integrates calibration systems.

JUNO will detect electron antineutrinos via an Inverse Beta-Decay (IBD) channel: ğœˆğ‘’ + ğ‘ â†’
ğ‘’+ + ğ‘›. The positron deposits its energy and annihilates into two 0.511 MeV gammas, forming a
so-called prompt signal. The energy is deposited shortly after the interaction and is the sum of the
positron kinetic energy and the annihilation energy of two 0.511 MeV gammas: ğ¸dep = ğ¸kin + 1.022
MeV. The neutron is captured in LS by hydrogen or carbon nuclei, after approximately 200 Âµs
producing 2.22 MeV (99% cases) or 4.95 MeV (1% cases) de-excitation gammas, for hydrogen and
carbon respectively. This is called the delayed signal. The time coincidence of prompt and delay
signals makes it possible to separate IBD signals from backgrounds. The information collected by

â€“ 2 â€“

Acrylic spherical vessel filled with liquid scintillatorWater poolTop tracker and calibration houseEarth magnetic field compensation coilsPhotomultiplier tubesAcrylic supporting nodesPMTs will be used for energy and vertex reconstruction of neutrino interactions.

Machine Learning (ML) has experienced an extraordinary rise in recent years. High energy
physics (HEP), and in particular neutrino physics, have also proven to be remarkable domains
for ML applications, especially supervised learning, due to the availability of a large amount of
labeled data produced by simulation. There are many examples of using ML approaches in HEP:
in neutrino experiments, in collider experiments, etc. [8â€“10]. We also recommend a living review
of ML techniques to HEP, which tries to include all relevant papers [11].

The paper is organized as follows. Section 2 describes the problem statement. In Section 3
we introduce the data which was used for this analysis. In Section 4 we present machine learning
approaches which we used for this study and their application for the problem. The performance
of the presented ML models is discussed in Section 5. Section 6 is dedicated to evaluating the
performance of models using calibration data. We summarize the study in Section 7.

2 Problem statement

In this work, we continue to study the ML techniques for energy reconstruction for the energy range
of 0â€“10 MeV, covering the region of interest for IBD events from reactor electron antineutrinos. In
general, event energy can be reconstructed from individual PMT signals (charge and time) or using
aggregated information from the whole array of PMTs â€” â€œaggregated featuresâ€. In the ï¬rst paper,
we presented diï¬€erent approaches based on PMT-wise information as well as based on several
basic aggregated features [12]. In the subsequent paper we, using an optimal subset from a large
set of newly engineered aggregated features, demonstrated that this approach can achieve the same
performance as the PMT-wise one [13]. On the other hand, vertex reconstruction requires usage of
granular information both with traditional and ML algorithms, see [14, 15]. Actual research aims
to further investigate the potential of the aggregated features approach and study Boosted Decision
Trees and Fully Connected Deep Neural Network techniques for energy reconstruction in JUNO.

To evaluate the performance of the models we use two metrics: resolution and bias. They are
deï¬ned by a Gaussian ï¬t of the (ğ¸predicted âˆ’ ğ¸dep) distribution (see Section 5). The resolution is
deï¬ned as ğœ/ğ¸dep and the bias as ğœ‡/ğ¸dep, where ğœ and ğœ‡ are the standard deviation and the mean
of the Gaussian ï¬t respectively.

To resolve the primary goal of the JUNO experiment, that is the determination of neutrino
mass ordering at the level of 3 standard deviations, the energy resolution must be better or equal to
ğœ = 3% at 1 MeV, and the energy nonlinearity uncertainty should be less than 1% [6].

For the resolution ğœ/ğ¸dep we also use the following parameterization:
(cid:118)(cid:117)(cid:116)(cid:32)

(cid:33) 2

ğœ
ğ¸dep

=

ğ‘
âˆšï¸ğ¸dep

(cid:19) 2

(cid:18) ğ‘
ğ¸dep

+ ğ‘2 +

,

(2.1)

with parameters ğ‘, ğ‘, and ğ‘ interpreted as follows: ğ‘ is mainly driven by the statistics of the true
accumulated charge on the PMTs, ğ‘ is related to the spatial non-uniformity, and ğ‘ is associated with
the charge from the dark noise (introduced in Section 3.1).

It is practical to have only one parameter to estimate the entire resolution curve. The JUNO
requirements for the appropriate determination of neutrino mass ordering could be translated into
a convenient requirement on an eï¬€ective resolution Ëœğ‘ as [16]:

â€“ 3 â€“

âˆšï¸‚

Ëœğ‘ â‰¡

(ğ‘)2 + (1.6 Ã— ğ‘)2 +

(cid:17) 2

(cid:16) ğ‘
1.6

â‰¤ 3%

(2.2)

3 Data description

3.1 Overview

The analyzed data is generated by the full detector Monte Carlo method using the oï¬ƒcial JUNO
software [17â€“19]. The detector simulation software is based on the Geant4 framework [20, 21],
with the geometry deï¬ned according to the latest design [22], and is implemented as a standalone
application.

The simulation begins with the injection of the positron with kinetic energy in the range from
0 to 10 MeV. As the neutron also produced in the neutrino interaction is going to be used only
for oï¬„ine event selection, it was not simulated for this study. The positron interacts with the
medium of LS losing its energy. After it stops or in-ï¬‚ight, the positron annihilates with an electron
in the medium, producing a pair of 511 keV gammas. These two gammas are usually stopped
by the target, producing secondary Compton electrons. Electromagnetic energy losses of charged
particles, primary positron and secondary electrons in our case, are accompanied by optical light
emission. In general, about 90% of light is produced from the scintillation of the detector target
medium, the other âˆ¼10% is produced from Cherenkov radiation.

The produced photons are then transported through the materials until they are either absorbed,
leave the detector, or hit PMTs. Roughly âˆ¼30% of photons hitting the PMT photo-cathode lead to
a release of photo-electrons which initiate an electric pulse. On average, all PMTs collect about
1500 photo-electrons per 1 MeV of deposited energy at the center of the detector. PMTs also
produce pulses spontaneously, the so-called dark current, which constitute the noise. JUNO will
be equipped with two types of 20-inch PMTs: 25% produced by Hamamatsu and 75% by NNVT.
The measured dark current rate for the former ones is 19.3 kHz, the latter ones have the dark
rate of 49.3 kHz [23]. Diï¬€erent processes during photo-electron collection and further current
ampliï¬cation, as well as intrinsic eï¬€ects of electronics, are also simulated. The introduced spread
of hit times is 1.3 ns and 7.0 ns for Hamamatsu and NNVT respectively [23]. Then the charge
and time information is extracted from PMT pulse shapes and serves as input for reconstruction
algorithms. Using interaction kinematics, the anti-neutrino energy can then be calculated assuming
the following relation: ğ¸ Ëœğœˆğ‘’ â‰ˆ ğ¸dep + 0.8 MeV.

However, if an event happens near the edge of the detector, less than a meter away, one or both
photons can escape the CD without contributing to the light yield. The positron energy will be
underestimated in such cases. There is also a strong radioactive background at the detector edge
which also compromises the energy reconstruction in that area. To avoid these eï¬€ects, we apply ğ‘…
< 17.2 m volume cut, i.e. discarding those events in the outermost 0.5 m layer of the detector.

Figure 2 illustrates one example event for a positron of 6.165 MeV deposited energy, as seen by
the 20-inch PMTs. The ï¬gure shows accumulated charge in the PMT channels (left) and activation
time, i.e. the ï¬rst hit arrival times (right).

â€“ 4 â€“

In this study, event energy reconstruction is based only on information collected by the larger
20-inch PMTs. We do not include the smaller ones (3-inch) so far because their contribution to the
light collection is negligible.

Figure 2: Example of an event for a positron of 6.165 MeV deposited energy as seen by the 20-inch
PMTs. Only ï¬red PMTs are shown. On the left the color represents the accumulated charge: yellow
points show the channels with more hits, red points show the channels with fewer hits. On the right
side the color indicates PMT activation time: the darker blue color shows earlier ï¬rst hit arrivals.
The primary vertex is shown by the gray sphere.

3.2 Data structure

To train models and to evaluate models performances we prepared two datasets: the training dataset
and the testing dataset:

1. Training dataset consists of 5 million events, with a ï¬‚at kinetic energy spectrum ranging
from 0 to 10 MeV and uniformly spread in the volume of the central detector (in LS),
ğ¸kin âˆˆ [0, 10] MeV.

2. Testing dataset consists of 14 subsets with discrete kinetic energies of 0 MeV, 0.1 MeV,
0.3 MeV, 0.6 MeV, 1 MeV, 2 MeV, ..., 10 MeV and uniform spatial distribution. Each subset
contains about 100 thousand events.

4 Machine learning approach

4.1 Feature Engineering

In this study we use aggregated features based on cumulative information from PMTs as input to
machine learning models. This section describes our feature engineering procedure. The basic
aggregated features for energy reconstruction are the following:

1. The accumulated charge on all ï¬red PMTs: AccumCharge. In the ï¬rst order it is proportional

to ğ¸dep.

2. The total number of ï¬red PMTs: nPMTs.

â€“ 5 â€“

LPMT_nPELPMT_FHTEdep = 6.165 MeVEvtID = 1135â–¼3. Coordinate components of the center of charge:

(ğ‘¥cc, ğ‘¦cc, ğ‘§cc) = (cid:174)ğ‘Ÿcc =

(cid:205)ğ‘PMTs
ğ‘–=1

(cid:174)ğ‘ŸPMTğ‘– Â· ğ‘›p.e.,ğ‘–
ğ‘›p.e.,ğ‘–

(cid:205)ğ‘PMTs
ğ‘–=1

and its radial component:

ğ‘…cc = |(cid:174)ğ‘Ÿcc|

,

(4.1)

(4.2)

Coordinate components of the center of charge are rough approximations of the location of
the energy deposition and help to correct the non-uniformity of the detector response. This
non-uniformity is illustrated in Figure 3, which shows the accumulated charge per 1 MeV of
deposited energy as a function of radius cubed ğ‘…3. The sharp decrease of the accumulated
charge per 1 MeV in the region of ğ‘… (cid:38) 16 m (or ğ‘…3 (cid:38) 4000 m3) is caused by the eï¬€ect of the
total internal reï¬‚ection, due to which the photons with a large incident angle never escape the
target.

Figure 3: Accumulated charge on PMTs per 1 MeV of deposited energy as a function of radius
cubed ğ‘…3 for the positrons.

4. Coordinate components of the center of ï¬rst hit time (FHT):

(ğ‘¥cht, ğ‘¦cht, ğ‘§cht) = (cid:174)ğ‘Ÿcht =

1
(cid:205)ğ‘PMTs
ğ‘–=1

ğ‘PMTsâˆ‘ï¸

1
ğ‘¡ht,ğ‘–+ğ‘

ğ‘–=1

(cid:174)ğ‘ŸPMTğ‘–
ğ‘¡ht,ğ‘– + ğ‘

,

and its radial component:

ğ‘…cht = |(cid:174)ğ‘Ÿcht|

(4.3)

(4.4)

Here the constant ğ‘ is required to avoid division by zero. The value of 50 ns was selected
to make the center of FHT closer to the energy deposition vertex. These features give extra
information on the location of the energy deposition.

5. For some ML models it is useful to engineer new synthesized features from the existing

ones [24, 25]. We construct the following synthesized features:

â€“ 6 â€“

010002000300040005000120014001600180020005001000150020002500CountAccumulated charge per MeV, PE/MeVğ›¾cc

ğ‘§ =

ğ‘§cc
cc + ğ‘¦2
cc

âˆšï¸ğ‘¥2

ğ‘¥cc
cc + ğ‘¦2
cc

, ğ›¾cc

ğ‘¥ =

ğ‘¦cc
cc + ğ‘§2
cc

, ğ›¾cc

ğ‘¦ =

âˆšï¸ğ‘¥2

âˆšï¸ğ‘¥2
cc + ğ‘¦2
cc
ğ‘§cc
cc Â· sin ğœƒcc, ğœŒcc =

âˆšï¸ğ‘§2
ğ‘¦cc
ğ‘¥cc
cc,
cc + ğ‘¦2
ğ‘¥2

, ğœ™cc = arctan
âˆšï¸ƒ

;

;

(4.5)

(4.6)

(4.7)

ğœƒcc = arctan

ğ½cc = ğ‘…2

with 7 similar features for the components of the center of FHT.

6. Figure 4 shows the Cumulative Distribution Functions (CDFs) and Probability Density Func-
tions (PDFs) for FHT, i.e. the distribution for the ï¬rst hit time in all ï¬red PMTs. The values
of FHT are the result of the waveform reconstruction and the trigger algorithms embedded in
the JUNO simulation software. The procedure is the same as it will be for real data and does
not rely on the true simulation information in any sense. After an event starts, the main part
of the charge is collected by PMTs in about 200 ns (depending on the distance from the center
of the detector). In the remaining time of the signal, there is a noticeable contribution from
scattered and reï¬‚ected photons as well as from dark noise. There is a clear dependence of
the shape on both energy and radial position. The depression for FHT PDFs at 16.9 m (left,
yellow) is due to the total internal reï¬‚ection of photons having large incidence angles at the
spherical surface of the detector. Near the edge events appear earlier in the readout window
because of the lower time-of-ï¬‚ight correction. The FHT PDFs at the detector center (right)
have similar shapes only diï¬€ering by the relative contribution from dark noise hits and from
annihilation gammas which decreases with increasing energy. To characterize the CDF for
FHT we use the following sets of percentiles as features:

{ht2%, ht5%, ht10%, ht15%, ..., ht90%, ht95%}

as well as mean, standard deviation, skewness and kurtosis:

{htmean, htstd, htskew, htkurtosis}

(4.8)

(4.9)

7. Likewise, we use features extracted from the distribution of charge on PMT channels (nPE).
The corresponding PDFs and CDFs are shown in Figure 5. The nPE PDFs consist of two
prominent peaks corresponding to one or two hits on a single PMT. The higher the energy the
more the second peak is populated. Moreover, for events occurring near the edge there are
PMTs accumulating a large amount of charge and forming the long tail of the distribution.
Therefore, the shape of this distribution delivers information both on the intensity of the light
emission and its position.

{pe2%, pe5%, pe10%, pe15%, ..., pe90%, pe95%}

{pemean, pestd, peskew, pekurtosis}

(4.10)

(4.11)

â€“ 7 â€“

Figure 4: Examples of CDFs and PDFs for FHT distributions with the kinetic energy of ğ¸kin =
1 MeV but diï¬€erent radius (left) and events at the center of the detector (ğ‘… = 0) but diï¬€erent energies
(right). Dashed lines denote PDF mean values.

Figure 5: Examples of CDFs and PDFs for charge distributions with the kinetic energy of ğ¸kin =
1 MeV but diï¬€erent radius (left) and events at the center of the detector (ğ‘… = 0) but diï¬€erent energies
(right). Dashed lines denote PDF mean values.

8. Figure 6 shows the average PDF for FHT distribution for 161 events with ğ¸dep = 2.022 MeV
and ğ‘… = 16 m. There is noticeable variance in the ï¬rst bins. To take this into account we also
use the following diï¬€erences between percentiles since they are more robust to this variance:

{ht5%âˆ’2%, ht10%âˆ’5%, ..., ht95%âˆ’90%}

(4.12)

In total we engineered 91 features.

4.2 Boosted decision trees

Boosted decision trees (BDT) is a supervised machine learning algorithm [27, 28]. Supervised
algorithms learn the mapping of input features to target output, using a data sample with input-
output pairs.

â€“ 8 â€“

02004006008001000120000.250.50.751020040060080010001200050100150200250R = 2.01 mR = 9.0 mR = 16.9 mt, nsF(t)02004006008001000120000.250.50.75102004006008001000120002004006008001000E = 1.02 MeVE = 5.02 MeVE = 9.02 MeVt, nsF(t)012345600.250.50.7510123456050100150R = 2.01 mR = 9.0 mR = 16.9 mnPEF(nPE)012345600.250.50.75101234560100200300E = 1.02 MeVE = 5.02 MeVE = 9.02 MeVnPEF(nPE)Figure 6: Average PDF for FHT distribution with deposited energy ğ¸dep = 2.022 MeV and
ğ‘… = 16 m.

BDT is an ensemble model with Decision Tree (DT) [29] as the base algorithm. DT is a
simple, fast and interpretable model. DT consists of a binary set of splitting rules based on values
of diï¬€erent features of the object. Herewith the single DT is not a powerful enough algorithm,
therefore an ensemble of DTs is commonly used.

DTs in BDT are trained sequentially. Each subsequent DT in BDT is trained to correct errors
of previous DTs in the ensemble. Thus, using a set of weak learners we can build one strong
learner. The tree-based models, including BDT, are an eï¬ƒcient way to work with tabular data.
In this work we use XGBRegressor implementation of BDT from the XGBoost library [30] for
Python. XGBRegressor has a great preset of hyperparameters making it easy to tune the model.
XGBRegressor is stable, fast to train, and fast to make predictions.

4.2.1 Feature selection

Figure 7: 5-fold cross-validation procedure.

Many of the 91 features described in Section 4.1 are highly correlated and we want to keep only
a subset of features which provide the same performance of the model as the full set of features.
The feature selection procedure for the BDT model is described as follows:

1. We train the BDT model on a dataset with 1 million events using 5-fold cross-validation from
the Scikit-learn library [31]. Figure 7 illustrates the cross-validation with the early stopping
condition on an additional hold-out dataset. We evaluate the model performance with Mean
Absolute Percentage Error (MAPE), which is better suited for reconstruction at low energies.
As a result of the cross-validation procedure we obtain mean MAPE on validation datasets
and its standard deviation ğœ€.

â€“ 9 â€“

02004006008001000050100150200250Energy = 2.022 MeV, R = [15.995-16.005] m., Number of events: 161t, nsTrainingValidationMAPEMAPEMAPEMAPEMAPEDataset with 1 million eventsMeanStd 2. We then initialize the empty list and start ï¬lling it with features. On each step we pick a
feature from the full set and train the BDT model using a 5-fold cross-validation procedure
with early stopping. Then we calculate the mean MAPE for each step and add to the list the
one that provides the best performance of the model on this step. We stop when the MAPE
score diï¬€ers from the mean MAPE score, for the model trained on all features, by less than
the standard deviation ğœ€.

Figure 8: Results of the feature selection procedure. Dashed line â€” the average MAPE for BDT
trained on all features after CV with its standard deviation. First ï¬ve features: AccumCharge, ğ‘…cht,
ğ‘§cc, pestd, nPMTs.

Figure 8 shows the results of the above-described feature selection procedure. Thus, the

procedure results to the following set of features:

1) AccumCharge
2) ğ‘…cht
3) ğ‘§cc
4) pestd

5) nPMTs
6) htkurtosis
7) ht25%âˆ’20%
8) ğ‘…cc

9) ht5%âˆ’2%
10) pemean
11) ğ½cht
12) ğœ™cc

13) ht35%âˆ’30%
14) ht20%âˆ’15%
15) pe35%
16) ht30%âˆ’25%

Figure 9 shows the dependence of the eï¬€ective resolution Ëœğ‘ and standard deviation (RMSE) for
reconstructed energy, and true energy obtained with sequentially selected features as input of the
BDT model. The dashed line illustrates Ëœğ‘ for BDT trained on the full set of features and the dark
red area is its error. The decreasing of an error on the validation dataset is consistent and converged
with the improvement of the eï¬€ective energy resolution Ëœğ‘. All models were trained with a maximal
depth of tree equal to 9 and learning rate 0.08, and with the early stopping condition with patience
5.

4.2.2 Hyperparameter optimization

We use the grid search approach for hyperparameter optimization for the BDT model. The space
of hyperparameters is determined and a metric is calculated for each possible combination of

â€“ 10 â€“

1.1851.191.1951.21.2051.211.2151.22Added featureMAPE, %Figure 9: The dependence of the eï¬€ective resolution Ëœğ‘ and RMSE on the validation dataset for
BDT on sequentially selected features of the model. First ï¬ve features: AccumCharge, ğ‘…cht, ğ‘§cc,
pestd, nPMTs.

hyperparameters from this space. The node with the best metric is selected as the optimum.

For grid search we use the BDT model with a learning rate equal to 0.08 trained with a 5-fold
cross-validation on the dataset with 1 million events. We optimize the maximal depth of the tree in
the ensemble. The training of the model stops if the MAPE score on the hold-out validation dataset
has not decreased for 5 successive iterations.

Figure 10 shows the results of the hyperparameter optimization. The best maximum depth of

the tree was found to be 10.

Figure 10: The dependence of minimal optimum number of trees and corresponding MAPE score
for the validation dataset on the maximum depth of tree.

â€“ 11 â€“

2.922.9633.043.088181.58282.5Val. RMSEAdded featureVal. RMSE, KeV56789101112131450010001500200025001.1521.1541.1561.1581.161.1621.1641.166Number of treesMAPEMaximal depth of treeNumber of treesMAPE, %4.3 Fully connected deep neural network

Training a neural network means ï¬tting, based on data, the best parameters ğœ½ for mapping ğ‘“ the
input feature matrix x to the output vector ğ‘¦: ğ‘¦ = ğ‘“ (x; ğœ½).

A fully connected neural network consists of layers with sets of units called neurons. Each
neuron in a layer is connected with each neuron in the next layer. It is called a fully connected deep
neural network (FCDNN) if it has many layers.

Neuron computes a linear combination of its inputs: ğ‘“ (x) = ğ‘Šx + b, where W is a matrix with
learnable weights of the neuron and b is a bias of the neuron. In order for a network to approximate
nonlinear functions, it is required to add nonlinearity â€” activation function â„ â€” for neuron output.
Thus, each neuron computes: ğ‘“ (x) = â„ (ğ‘Šx + b).

Optimization of the hyperparameters for FCDNN is performed using a BayesianOptimization
tuner from the KerasTuner library for Python [32]. To train the model we use TensorFlow [33].
MAPE loss for reconstructed energy and true energy is used as the loss function. All input features
were normalized with a standard score normalization. The training process is performed with early
stopping condition on the validation dataset with patience 25 and with batch size 1024. Table 1
shows the search space and the selected hyperparameters.

Hyperparameter
Units in input layer
Units in hidden layers
Number of hidden layers
Activation [34â€“36]
Optimizer [37, 38]
Learning rate
Scheduler type [39]
Input layer weights initialization
Hidden layers weights initialization

Range
[1, 512]
[1, 512]
[1, 32]
ReLU, ELU, SELU
Adam, SGD, RMSprop
[0.0001, 0.01]
Exponential, None

Selected
256
256
16
ReLU
Adam
0.0016
Exponential

normal, lecun-normal, uniform

normal

Table 1: Hyperparameter search space for FCDNN. Selected hyperparameters are highlighted in
bold.

We ï¬rst select several subsets of features that are reasonable from our previous experience.
Then we pick the following subset of features that provides virtually the same quality of the FCDNN
model as the full set of features:

1) AccumCharge
2) nPMTs
3) ğ‘…cc
4) ğ‘…cht
5) ğœŒcc
11) Percentiles of FHT distribution: {ht2%, ht5%, ht10%, ht15%, ..., ht90%, ht95%}

6) ğœŒcht
7) pemean
8) pestd
9) peskew
10) pekurtosis

It is interesting to note that when using those features selected for the BDT approach, the quality is
within 2ğœ from the optimal quality corresponding to the feature list above.

â€“ 12 â€“

Figure 11 describes the selected architecture of FCDNN and its main selected hyperparameters.

Figure 11: Selected FCDNN architecture and its main selected hyperparameters.

5 Results

(a)

(b)

Figure 12: Examples of distributions of the predicted deposited energy for ğ¸dep = 3.022 MeV (a)
and ğ¸dep = 1.022 MeV (b). The orange line in the left ï¬gure represents the Gaussian ï¬t. The
distributions were obtained, using the BDT model.

As it was mentioned in Section 2, to evaluate the performance of the models we use resolution
and bias metrics. These metrics are obtained from the Gaussian ï¬t of the ğ¸predictedâˆ’ğ¸dep distribution,
as its standard deviation (ğœ) and mean value (ğœ‡). Figure 12a illustrates an example of ğ¸predicted âˆ’ğ¸dep
distribution and its Gaussian ï¬t with ğ¸dep = 3.022 MeV. Note that we exclude edge points with
0 MeV and 10 MeV kinetic energy, corresponding to 1.022 and 11.022 MeV of deposited energy.

â€“ 13 â€“

ReLUReLUReLUReLUReLUReLUReLUReLUReLUlinearAccumChargenPMTsht95%Input layerh1h16EdepMAPE loss30 features256 units256 unitsHidden layersâˆ’0.2âˆ’0.15âˆ’0.1âˆ’0.0500.050.10.150.200.010.020.030.04Î¼ = 0.001 MeV, Ïƒ = 0.051 MeVâˆ’0.2âˆ’0.15âˆ’0.1âˆ’0.0500.050.10.150.200.040.080.120.16The ML models learn that the energy of the event belongs to the energy range from the training
dataset (ğ¸kin 0-10 MeV) and, therefore, the distributions at the edge points are truncated and exhibit
artiï¬cially increased resolution, as shown in Figure 12b.

Figure 13: Energy resolution performance: resolution (upper panel) and bias (lower panel) for
BDT and FCDNN models. Note: the most left point corresponds to 1.122 MeV.

Figure 13 shows the energy reconstruction performance (resolution and bias) of the BDT and
FCDNN models described in Sections 4.2 and 4.3. Parameterization 2.1 with ï¬tted ğ‘, ğ‘, and ğ‘
values is also shown. The bias of the energy reconstruction is very close to zero for both models.
The resolution performance is better for FCDNN in the lower energy region and then converges
with the BDT results at higher energies.

Table 2 lists parameters ğ‘, ğ‘, and ğ‘ of the parameterization 2.1 for the energy resolution curves
and the eï¬€ective resolution Ëœğ‘ for both BDT and FCDNN models. The value of Ëœğ‘ is noticeably better
for FCDNN but for both models, Ëœğ‘ is better than required: Ëœğ‘ < 3%.

ğ‘ Â± Î”ğ‘
Model
2.573 Â± 0.097
BDT
FCDNN 2.316 Â± 0.139

ğ‘ Â± Î”ğ‘
0.763 Â± 0.045
0.827 Â± 0.054

ğ‘ Â± Î”ğ‘
0.990 Â± 0.394
1.474 Â± 0.285

Ëœğ‘ Â± Î” Ëœğ‘
2.914 Â± 0.016
2.822 Â± 0.027

Table 2: Parameters ğ‘, ğ‘, and ğ‘ of the parameterization for the energy resolution curves and the
eï¬€ective resolution Ëœğ‘ for BDT and FCDNN models.

We also investigate how the resolution depends on the diï¬€erent sub-detector regions, see
Figure 14. We used our models, BDT and FCDNN, trained on the dataset with the standard ï¬ducial
volume cut of 17.2 m, i.e. in the 0 âˆ’ 17.2 m range, but tested them on the datasets with varied ğ‘…

â€“ 14 â€“

11.522.5312345678910âˆ’0.200.2BDTFCDNNDeposited energy, MeVResolution, %Bias, %ranges. The R ranges are selected to contain approximately the same number of events. The eï¬€ective
resolution in the ğ‘… regions is correlated with the accumulated charge per MeV (see Figure 3): less
charge means worse eï¬€ective resolution and vice versa. Consequently, the performance of both
models worsens for events close to the edge of the detector.

Besides that, the outer region of the detector is more populated by background events originating
from radioactive decays in materials. Therefore, one may consider reducing the ï¬ducial volume
to increase the quality of data. On another side, excluding outer events decreases statistics which
limits the experiment sensitivity to the neutrino oscillation pattern. Thus, the optimal strategy has
to be found via a comprehensive sensitivity study.

Figure 14: The expected value of the eï¬€ective resolution Ëœğ‘ for equidistant sub-detector regions.
Dashed lines and ï¬lled areas represent the eï¬€ective resolution Ëœğ‘ and its standard deviation for BDT
and FCDNN models respectively. Both models are trained on data with ğ‘…FV < 17.2.

6 Calibration

Models trained and validated on simulated data do not guarantee that their prediction will be
accurate for real experimental data. There are approaches based on domain adaptation methods
that can improve the performance of the reconstruction algorithm on real data, using unlabeled real
data samples [40]. In addition, the accuracy of the models has to be checked, e.g. with calibration
data, that is with signals from well known sources. Calibration data can also be used to ï¬ne tune
reconstruction algorithms trained on synthetic data. Here we present a simple way to compare our
models with the calibration data of JUNO. It includes the data from gamma sources. Since the
models are prepared for positrons a special correction is needed to make the comparison possible.
JUNO has an extensive calibration program: multiple radioactive sources and background

processes are planned to be used. Here we consider three calibration sources, listed in Table 3.

â€“ 15 â€“

R=[0, 8.0]R=[8.0, 10.1]R=[10.1, 11.5]R=[11.5, 12.7]R=[12.7, 13.7]R=[13.7, 14.5]R=[14.5, 15.3]R=[15.3, 16.0]R=[16.0, 16.6]R=[16.6, 17.2]2.72.82.933.13.2BDTFCDNNSub-detector regions, mType Radiation

Source
241Amâˆ’13C ğ›¾
ğ›¾
60Co
ğ‘’+
68Ge

neutron + 6.13 MeV
1.173 + 1.333 MeV
annihilation 0.511 + 0.511 MeV

Table 3: Radioactive sources for calibration.

1. 241Amâˆ’13C source produces both a neutron and a high energy gamma. Then the neutron is
captured, emitting a gamma of 2.22 MeV (on hydrogen atom) or 4.95 MeV (on carbon atom).
We consider only 2.22 MeV gammas here because these are more abundant.

2. 60Co produces two gammas with the energy of 1.173 MeV and 1.333 MeV. We assume they

merge in one signal and reconstruct the sum energy of both gammas.

3. 68Ge decays to 68Ga via electron capture, then 68Ga decays via positron emission (ğ›½+) to 68Zn.
The kinetic energy of the positron is absorbed by the source enclosure, but two annihilation
gammas escape. As a result, we can consider this source as positrons with zero kinetic energy.

These calibration sources are planned to be moved over the central detector volume. The
measured signal is to be used for the improvement of simulation and reconstruction algorithms.
While the kinematics of the calibration sources is well deï¬ned, the deposited energy varies from
event to event. Thus, in practice, only the distributions are known. To evaluate the performance of
the reconstruction algorithms we compare the predictions based on the real calibration data with
these distributions.

(a)

(b)

(c)

Figure 15: Distributions of true deposited energy and predicted energy, using the BDT model, for
calibration sources: a) AmC, b) Co-60, c) Ge-68.

Our models, which are trained on positron sources, are expected to inevitably have weaker
results on gamma sources (241Amâˆ’13C, 60Co) because of diï¬€erent event topology: positrons
deposit their energy in a very compact region, while gammas travel longer distances. Indeed, we
obtain that the predictions for gammas are shifted and has larger variation, see Figure 15. This
ï¬gure shows the distributions of the true simulated energy (in red) and the energy reconstructed
by the BDT model (in green) for the 241Amâˆ’13C, 60Co, and 68Ge sources. To compensate for

â€“ 16 â€“

22.12.22.32.42.500.20.40.60.81AmC:True energyPred. energyPred. energy (corrected)Deposited energy, MeV11.522.500.10.20.30.40.50.60.7Co60:True energyPred. energyPred. energy (corrected)Deposited energy, MeV0.60.811.21.400.10.20.30.40.50.6Ge68:True energyPred. energyDeposited energy, MeV(a)

(b)

(c)

(d)

Figure 16: 2D histograms for true deposited energy and predicted (corrected) energy, using the
BDT model, for calibration sources and positron events: a) AmC, b) Co-60, c) Ge-68, d) ğ‘’+. The
number of events in each cell is given in logarithmic scale. Violet dashed lines show ğ¸true = ğ¸rec.

the bias we extract the necessary correction from values predicted by the model on pure gamma
events. We parameterize the correction as 7.955ğ¸ âˆ’1.418 âˆ’ 0.209 (MeV) and then subtract it from
the reconstructed energy for the gamma sources 241Amâˆ’13C and 60Co (blue in Figure 15).

In future, the agreement between the expected source spectra and the spectra reconstructed from
the real calibration data would indicate that the reconstruction algorithm works correctly. Being
realistic we expect that our reconstructed spectra for calibration sources will not match the measured
ones. We can not foresee what kind of disagreement we will get. However, some information may
be extracted from analyzing the diï¬€erences in the shapes of the reconstructed and may help to spot
the problems in the underlying models. Another possibility is to run an additional round of training
with the data from calibration sources, which are limited but may be enough for ï¬ne tuning.

The ways of using these possible discrepancies are out of the scope of this work and will be

addressed in the future.

Figure 16 shows a 2D correlation between true energy and predicted energy (with the applied
correction for 241Amâˆ’13C and 60Co sources), and complements Figure 15. Figure 16 illustrates that
the correlation is strong. Note that a large bias in the ğ¸dep < 1.022 MeV region for the 68Ge source is
expected because almost all training events have ğ¸dep above 1.022 MeV. The inability of the model
to make proper predictions outside of its training domain explains the complex structure below
1.022 MeV. The 2D correlation for positrons (Figure 16d) demonstrates a good match between
the reconstructed and true energies: ğ¸true = ğ¸rec, also including the regions between the discrete

â€“ 17 â€“

22.12.22.32.42.522.12.22.32.42.51.e01.e0.51.e11.e1.51.e21.e2.51.e31.e3.51.e4CountReconstructed deposited energy, MeVTrue deposited energy, MeVAmC11.522.511.21.41.61.822.22.42.61.e01.e0.51.e11.e1.51.e21.e2.51.e31.e3.51.e4CountReconstructed deposited energy, MeVTrue deposited energy, MeVCo600.60.811.21.40.50.60.70.80.911.11.21.31.41.e01.e0.51.e11.e1.51.e21.e2.51.e31.e3.51.e4CountReconstructed deposited energy, MeVTrue deposited energy, MeVGe68123456789101112345678910111.e01.e0.51.e11.e1.51.e21.e2.51.e31.e3.51.e41.e4.5CountReconstructed deposited energy, MeVTrue deposited energy, MeVe+energies. This conï¬rms the expected behaviour of the model.

7 Conclusions

In this work, we present an application of machine learning techniques for precise energy recon-
struction for the energy range of 0â€“10 MeV. We use two models: Boosted Decision Trees (BDT)
and Fully Connected Deep Neural Network (FCDNN), trained using aggregated features extracted
from Monte Carlo simulation data. We considered the case of the JUNO detector. However, the
approaches are valid for other similar detectors with a large liquid scintillator target surrounded by
an array of photo-sensors. Our dataset is generated with oï¬ƒcial JUNO software for the modeling
of particle interactions, light emission, transport and collection, as well as the realistic response of
photo-multipliers and the readout system.

We design a large set of features and select one features subsets for the BDT model and another
feature subset for the FCDNN model, both provide the same performances as the full set of features.
The requirement on the eï¬€ective resolution to determine the neutrino mass ordering, Ëœğ‘ â‰¤ 3%, is
achieved by both BDT and FCDNN. BDT is a fast and minimalistic model, making predictions 3-4
times faster than FCDNN. On the other hand the latter provides slightly better performance.

We also consider three calibration sources, 241Amâˆ’13C, 60Co, 68Ge, for future evaluation of the
reliability of the reconstruction algorithms. The agreement between the modelsâ€™ prediction made on
real calibration data and on simulated data would conï¬rm the correctness of the reconstruction. A
diï¬€erence in the reconstructed calibration spectra will provide an input for the Monte Carlo tuning.

Acknowledgments

We are very thankful to JUNO collaborators who contributed to the development and validation of
the JUNO simulation software. We thank N. Kutovskiy and N. Balashov for providing an extensive
IT support for computing resources of JINR cloud services [41] and X. Zhang for her work on
producing of the MC samples. We are also grateful to Maxim Gonchar for fruitful discussions.

Fedor Ratnikov is supported by the Russian Science Foundation under grant agreement â„–19-
71-30020. Yury Malyshkin is supported by the Russian Science Foundation under grant agreement
â„–21-42-00023.

References

[1] X. Guo et al. [Borexino Collaboration], Science and technology of Borexino: a real-time detector for

low energy solar neutrinos, Astropart. Phys. 16 (2002) no.3 205-234.

[2] K. Eguchi et al. [KamLAND Collaboration], First results from KamLAND: Evidence for reactor

anti-neutrino disappearance, Phys. Rev. Lett. 90 (2003), 021802.

[3] X. Guo et al. [Daya Bay Collaboration], A Precision Measurement of the Neutrino Mixing Angle ğœƒ13

using Reactor Antineutrinos at Daya Bay, arXiv:hep-ex/0701029.

[4] F. Ardellier et al. [Double Chooz Collaboration], Double Chooz: A Search for the Neutrino Mixing

Angle ğœƒ13, arXiv:hep-ex/0606025.

â€“ 18 â€“

[5] J. K. Ahn et al. [RENO Collaboration], RENO: An Experiment for Neutrino Oscillation Parameter ğœƒ13

Using Reactor Neutrinos at Yonggwang, arXiv:1003.1391.

[6] F. An et al. [JUNO Collaboration], Neutrino Physics with JUNO, J. Phys. G 43 (2016) no.3, 030401.

[7] A. Abusleme et al. [JUNO Collaboration], JUNO Physics and Detector, Prog. Part. Nucl. Phys. 123,

103927 (2022).

[8] D. Bourilkov, Machine and Deep Learning Applications in Particle Physics, Int. J. Mod. Phys. A 34,

no.35, 1930019 (2020).

[9] M. D. Schwartz, Modern Machine Learning and Particle Physics, Harvard Data Science Review

(2021).

[10] D. Guest, K. Cranmer and D. Whiteson, Deep Learning and its Application to LHC Physics, Ann.

Rev. Nucl. Part. Sci. 68, 161-181 (2018).

[11] HEP ML Community. A Living review of machine learning for particle physics.

https://iml-wg.github.io/HEPML-LivingReview/.

[12] Z. Qian, V. Belavin, V. Bokov, R. Brugnera, A. Compagnucci, A. Gavrikov, A. Garfagnini,

M. Gonchar, L. Khatbullina and Z. Li, et al. Vertex and energy reconstruction in JUNO with machine
learning methods, Nucl. Instrum. Meth. A 1010 (2021), 165527.

[13] A. Gavrikov, F. Ratnikov, The use of Boosted Decision Trees for Energy Reconstruction in JUNO

experiment, EPJ Web Conf. 251 (2021), 03014.

[14] Z. Li, Y. Zhang, G. Cao, Z. Deng, G. Huang, W. Li, T. Lin, L. Wen, M. Yu and J. Zou, et al. Event

vertex and time reconstruction in large-volume liquid scintillator detectors, Nucl. Sci. Tech. 32 (2021)
no.5, 49.

[15] Z. Y. Li, Z. Qian, J. H. He, W. He, C. X. Wu, X. Y. Cai, Z. Y. You, Y. M. Zhang and W. M. Luo,

Improving the machine learning based vertex reconstruction for large liquid scintillator detectors with
multiple types of PMTs, arXiv:2205.04039.

[16] A. Abusleme et al. [JUNO Collaboration], Calibration Strategy of the JUNO Experiment, JHEP 03,

004 (2021).

[17] X. Huang et al., Oï¬„ine Data Processing Software for the JUNO Experiment, PoS ICHEP2016

(2017), 1051.

[18] T. Lin et al., The Application of SNiPER to the JUNO Simulation, J. Phys. Conf. Ser. 898 (2017) no.4,

042029.

[19] T. Lin et al., Parallelized JUNO simulation software based on SNiPER, J. Phys. Conf. Ser. 1085

(2018) no.3, 032048.

[20] S. Agostinelli et al. [GEANT4 Collaboration], GEANT4â€“a simulation toolkit, Nucl. Instrum. Meth. A

506 (2003), 250-303.

[21] J. Allison, J. Apostolakis, S. B. Lee, K. Amako, S. Chauvie, A. Mantero, J. I. Shin, T. Toshito,

P. R. Truscott and T. Yamashita, et al. Recent developments in Geant4, J. Nucl. Instrum. Meth. A 835
(2016), 186-225.

[22] K. Li, Z. You, Y. Zhang, J. Zhu, T. Lin, Z. Deng and W. Li, GDML based geometry management

system for oï¬„ine software in JUNO, Nucl. Instrum. Meth. A 908 (2018), 43-48.

[23] A. Abusleme, T. Adam, S. Ahmad, et al. [JUNO Collaboration], Mass Testing and Characterization

of 20-inch PMTs for JUNO, 2022, arXiv:2205.08629.

â€“ 19 â€“

[24] A. Coates, A. Ng, H. Lee, An analysis of single-layer networks in unsupervised feature learning,

Proceedings of the fourteenth international conference on artiï¬cial intelligence and statistics, PMLR
15 (2011) 215-223.

[25] J. Heaton, An empirical analysis of feature engineering for predictive modeling, SoutheastCon, IEEE

(2016), 1-6.

[26] X. Fang, Y. Zhang, G. H. Gong, G. F. Cao, T. Lin, C. W. Yang and W. D. Li, Capability of detecting

low energy events in JUNO Central Detector, JINST 15, no.03, P03020 (2020).

[27] J. Friedman, Stochastic gradient boosting, Computational statistics & data analysis 38 (2002) no.4,

367-378.

[28] J. Friedman, Greedy function approximation: A gradient boosting machine, Ann. Statist. 29 (2001)

1189-1232 no.5.

[29] J. Quinlan, Simplifying decision trees, International Journal of Man-Machine Studies 27 (1987)

221-234 no.3.

[30] T. Chen, C. Guestrin, Xgboost: A scalable tree boosting system, Proceedings of the 22nd acm sigkdd

international conference on knowledge discovery and data mining (2016), 785â€“794.

[31] F. Pedregosa et al., Scikit-learn: Machine Learning in Python, The Journal of Machine Learning

Research 12 (2011) 2825â€“2830.

[32] T. Oâ€™Malley et al., KerasTuner, 2019, https://github.com/keras-team/keras-tuner/

[33] A. Martin et al., TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015,

https://www.tensorï¬‚ow.org/

[34] N. Vinod, G. Hinton, Rectiï¬ed linear units improve restricted boltzmann machines, Proceedings of the
27th International Conference on International Conference on Machine Learning (2010) 807â€“814.

[35] D.-A. Clevert, T. Unterthiner, S. Hochreiter, Fast and Accurate Deep Network Learning by

Exponential Linear Units (ELUs), 2015, arXiv:1511.07289.

[36] G. Klambauer, T. Unterthiner, A. Mayr, S. Hochreiter, Self-Normalizing Neural Networks, Proceedings
of the 31st international conference on neural information processing systems (2017), 972â€“981.

[37] D. Kingma, J. Ba, Adam: A Method for Stochastic Optimization, 2014, arXiv:1412.6980.

[38] S. Ruder, An overview of gradient descent optimization algorithms, 2016, arXiv:1609.04747.

[39] Z. Li, S. Arora, An Exponential Learning Rate Schedule for Deep Learning, 2019, arXiv:1910.07454.

[40] G. N. Perdue et al. [MINERvA Collaboration], Reducing model bias in a deep learning classiï¬er
using domain adversarial neural networks in the MINERvA experiment, JINST 13, no.11, P11020
(2018)

[41] A. Baranov, N. Balashov, N. Kutovskiy and R. Semenov, JINR cloud infrastructure evolution, Phys.

Part. Nucl. Lett. 13 (2016) no.5, 672-675

â€“ 20 â€“

