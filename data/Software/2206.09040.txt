Prepared for submission to JINST

Energy reconstruction for large liquid scintillator

detectors with machine learning techniques: aggregated

features approach

𝑎,𝑏,1 Yu. Malyshkin,

A. Gavrikov,
𝑎HSE University, Moscow, Russia
𝑏Joint Institute for Nuclear Research, Dubna, Russia

F. Ratnikov

𝑎

𝑏

E-mail: gavrikov@jinr.ru

Abstract: Large scale detectors consisting of a liquid scintillator (LS) target surrounded by an
array of photo-multiplier tubes (PMT) are widely used in modern neutrino experiments: Borexino,
KamLAND, Daya Bay, Double Chooz, RENO, and upcoming JUNO with its satellite detector TAO.
Such apparatuses are able to measure neutrino energy, which can be derived from the amount of
light and its spatial and temporal distribution over PMT-channels. However, achieving a ﬁne energy
resolution in large scale detectors is challenging.

In this work, we present machine learning methods for energy reconstruction in JUNO, the
most advanced detector of its type. We focus on positron events in the energy range of 0–10 MeV
which corresponds to the main signal in JUNO — neutrinos originated from nuclear reactor cores
and detected via an inverse beta-decay channel. We consider Boosted Decision Trees and Fully
Connected Deep Neural Network trained on aggregated features, calculated using information
collected by PMTs. We describe the details of our feature engineering procedure and show that
machine learning models can provide energy resolution 𝜎 = 3% at 1 MeV using subsets of
engineered features. The dataset for model training and testing is generated by the Monte Carlo
method with the oﬃcial JUNO software. Consideration of calibration sources for evaluation of the
reconstruction algorithms performance on real data is also presented.

Keywords: Large detector-systems performance; Neutrino detectors; Data processing methods

ArXiv ePrint: arxiv

1Corresponding author.

2
2
0
2

n
u
J

7
1

]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[

1
v
0
4
0
9
0
.
6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Contents

1

Introduction

2 Problem statement

3 Data description
3.1 Overview
3.2 Data structure

4 Machine learning approach
4.1 Feature Engineering
4.2 Boosted decision trees

Feature selection

4.2.1
4.2.2 Hyperparameter optimization
4.3 Fully connected deep neural network

5 Results

6 Calibration

7 Conclusions

1

Introduction

1

3

4
4
5

5
5
8
9
10
12

13

15

18

One of the neutrino detection methods consists of observation of optical photons produced by
secondary charged particles from neutrino interactions in the target volume. Although charged
particles can produce Cherenkov light even in water, a class of detectors, e.g. Borexino [1],
KamLAND [2], Daya Bay [3], Double Chooz [4], and RENO [5], used Liquid Scintillators (LS) to
increase the light yield. Modern LS compositions provide 𝑂 (104) photons per MeV of deposited
energy, a signiﬁcant part of which can be observed with Photo-Multiplier Tubes (PMT) placed
around the target. Since the amount of emitted photons is deﬁned by the deposited energy, it can
be easily estimated. Usage of extra information coming from the temporal distribution and spatial
pattern of ﬁred PMTs allows to improve the energy measurement accuracy. However, in the case
of future detectors with a large number of channels (PMTs), the task becomes challenging. We
consider novel approaches based on machine learning for the JUNO detector, the largest detector
of its type currently under construction. These approaches can be also applied to other similar
detectors.

The Jiangmen Underground Neutrino Observatory (JUNO) [6, 7] is a multipurpose neutrino
observatory, under construction, located in southern China. The primary aims of the JUNO
experiment are to determine the mass ordering of neutrino and to precisely measure their oscillation

– 1 –

, Δ𝑚2

parameters sin2 𝜃12, Δ𝑚2
31. The main source of neutrinos in JUNO will be reactor neutrinos
21
from the Yangjiang and Taishan nuclear power plants, which are located 53 kilometers from the
detector. Concurrently, JUNO will be able to explore supernovae neutrinos, atmospheric neutrinos,
solar neutrinos and geoneutrinos, as well as rare processes like proton decay.

Figure 1: A schematic view of the JUNO detector and the other main components.

The key requirement for JUNO is to provide energy resolution of 𝜎𝐸 /𝐸 = 3%/√︁𝐸 (MeV).
The detector construction illustrated in Figure 1 is optimized to meet this requirement. The JUNO
detector consists of the Central Detector (CD), the water Cherenkov detector and the Top Tracker.
CD is an acrylic sphere of 35.4 meters in diameter ﬁlled with 20 kt of liquid scintillator (LS). CD is
held by the stainless steel construction immersed in a water pool. It is equipped with a large number
of photo-multiplier tubes (PMT) of two types: 17 612 large 20-inch tubes and 25 600 small 3-inch.
The former ones provide 75.2% of the sphere coverage and the latter ones ensure an extra 2.7% of
coverage [7]. The water pool is also equipped with 2400 20-inch PMTs to detect Cherenkov light
from muons. The Top Tracker is disposed at the top of the detector, which is used to detect muon
tracks. The Calibration house integrates calibration systems.

JUNO will detect electron antineutrinos via an Inverse Beta-Decay (IBD) channel: 𝜈𝑒 + 𝑝 →
𝑒+ + 𝑛. The positron deposits its energy and annihilates into two 0.511 MeV gammas, forming a
so-called prompt signal. The energy is deposited shortly after the interaction and is the sum of the
positron kinetic energy and the annihilation energy of two 0.511 MeV gammas: 𝐸dep = 𝐸kin + 1.022
MeV. The neutron is captured in LS by hydrogen or carbon nuclei, after approximately 200 µs
producing 2.22 MeV (99% cases) or 4.95 MeV (1% cases) de-excitation gammas, for hydrogen and
carbon respectively. This is called the delayed signal. The time coincidence of prompt and delay
signals makes it possible to separate IBD signals from backgrounds. The information collected by

– 2 –

Acrylic spherical vessel filled with liquid scintillatorWater poolTop tracker and calibration houseEarth magnetic field compensation coilsPhotomultiplier tubesAcrylic supporting nodesPMTs will be used for energy and vertex reconstruction of neutrino interactions.

Machine Learning (ML) has experienced an extraordinary rise in recent years. High energy
physics (HEP), and in particular neutrino physics, have also proven to be remarkable domains
for ML applications, especially supervised learning, due to the availability of a large amount of
labeled data produced by simulation. There are many examples of using ML approaches in HEP:
in neutrino experiments, in collider experiments, etc. [8–10]. We also recommend a living review
of ML techniques to HEP, which tries to include all relevant papers [11].

The paper is organized as follows. Section 2 describes the problem statement. In Section 3
we introduce the data which was used for this analysis. In Section 4 we present machine learning
approaches which we used for this study and their application for the problem. The performance
of the presented ML models is discussed in Section 5. Section 6 is dedicated to evaluating the
performance of models using calibration data. We summarize the study in Section 7.

2 Problem statement

In this work, we continue to study the ML techniques for energy reconstruction for the energy range
of 0–10 MeV, covering the region of interest for IBD events from reactor electron antineutrinos. In
general, event energy can be reconstructed from individual PMT signals (charge and time) or using
aggregated information from the whole array of PMTs — “aggregated features”. In the ﬁrst paper,
we presented diﬀerent approaches based on PMT-wise information as well as based on several
basic aggregated features [12]. In the subsequent paper we, using an optimal subset from a large
set of newly engineered aggregated features, demonstrated that this approach can achieve the same
performance as the PMT-wise one [13]. On the other hand, vertex reconstruction requires usage of
granular information both with traditional and ML algorithms, see [14, 15]. Actual research aims
to further investigate the potential of the aggregated features approach and study Boosted Decision
Trees and Fully Connected Deep Neural Network techniques for energy reconstruction in JUNO.

To evaluate the performance of the models we use two metrics: resolution and bias. They are
deﬁned by a Gaussian ﬁt of the (𝐸predicted − 𝐸dep) distribution (see Section 5). The resolution is
deﬁned as 𝜎/𝐸dep and the bias as 𝜇/𝐸dep, where 𝜎 and 𝜇 are the standard deviation and the mean
of the Gaussian ﬁt respectively.

To resolve the primary goal of the JUNO experiment, that is the determination of neutrino
mass ordering at the level of 3 standard deviations, the energy resolution must be better or equal to
𝜎 = 3% at 1 MeV, and the energy nonlinearity uncertainty should be less than 1% [6].

For the resolution 𝜎/𝐸dep we also use the following parameterization:
(cid:118)(cid:117)(cid:116)(cid:32)

(cid:33) 2

𝜎
𝐸dep

=

𝑎
√︁𝐸dep

(cid:19) 2

(cid:18) 𝑐
𝐸dep

+ 𝑏2 +

,

(2.1)

with parameters 𝑎, 𝑏, and 𝑐 interpreted as follows: 𝑎 is mainly driven by the statistics of the true
accumulated charge on the PMTs, 𝑏 is related to the spatial non-uniformity, and 𝑐 is associated with
the charge from the dark noise (introduced in Section 3.1).

It is practical to have only one parameter to estimate the entire resolution curve. The JUNO
requirements for the appropriate determination of neutrino mass ordering could be translated into
a convenient requirement on an eﬀective resolution ˜𝑎 as [16]:

– 3 –

√︂

˜𝑎 ≡

(𝑎)2 + (1.6 × 𝑏)2 +

(cid:17) 2

(cid:16) 𝑐
1.6

≤ 3%

(2.2)

3 Data description

3.1 Overview

The analyzed data is generated by the full detector Monte Carlo method using the oﬃcial JUNO
software [17–19]. The detector simulation software is based on the Geant4 framework [20, 21],
with the geometry deﬁned according to the latest design [22], and is implemented as a standalone
application.

The simulation begins with the injection of the positron with kinetic energy in the range from
0 to 10 MeV. As the neutron also produced in the neutrino interaction is going to be used only
for oﬄine event selection, it was not simulated for this study. The positron interacts with the
medium of LS losing its energy. After it stops or in-ﬂight, the positron annihilates with an electron
in the medium, producing a pair of 511 keV gammas. These two gammas are usually stopped
by the target, producing secondary Compton electrons. Electromagnetic energy losses of charged
particles, primary positron and secondary electrons in our case, are accompanied by optical light
emission. In general, about 90% of light is produced from the scintillation of the detector target
medium, the other ∼10% is produced from Cherenkov radiation.

The produced photons are then transported through the materials until they are either absorbed,
leave the detector, or hit PMTs. Roughly ∼30% of photons hitting the PMT photo-cathode lead to
a release of photo-electrons which initiate an electric pulse. On average, all PMTs collect about
1500 photo-electrons per 1 MeV of deposited energy at the center of the detector. PMTs also
produce pulses spontaneously, the so-called dark current, which constitute the noise. JUNO will
be equipped with two types of 20-inch PMTs: 25% produced by Hamamatsu and 75% by NNVT.
The measured dark current rate for the former ones is 19.3 kHz, the latter ones have the dark
rate of 49.3 kHz [23]. Diﬀerent processes during photo-electron collection and further current
ampliﬁcation, as well as intrinsic eﬀects of electronics, are also simulated. The introduced spread
of hit times is 1.3 ns and 7.0 ns for Hamamatsu and NNVT respectively [23]. Then the charge
and time information is extracted from PMT pulse shapes and serves as input for reconstruction
algorithms. Using interaction kinematics, the anti-neutrino energy can then be calculated assuming
the following relation: 𝐸 ˜𝜈𝑒 ≈ 𝐸dep + 0.8 MeV.

However, if an event happens near the edge of the detector, less than a meter away, one or both
photons can escape the CD without contributing to the light yield. The positron energy will be
underestimated in such cases. There is also a strong radioactive background at the detector edge
which also compromises the energy reconstruction in that area. To avoid these eﬀects, we apply 𝑅
< 17.2 m volume cut, i.e. discarding those events in the outermost 0.5 m layer of the detector.

Figure 2 illustrates one example event for a positron of 6.165 MeV deposited energy, as seen by
the 20-inch PMTs. The ﬁgure shows accumulated charge in the PMT channels (left) and activation
time, i.e. the ﬁrst hit arrival times (right).

– 4 –

In this study, event energy reconstruction is based only on information collected by the larger
20-inch PMTs. We do not include the smaller ones (3-inch) so far because their contribution to the
light collection is negligible.

Figure 2: Example of an event for a positron of 6.165 MeV deposited energy as seen by the 20-inch
PMTs. Only ﬁred PMTs are shown. On the left the color represents the accumulated charge: yellow
points show the channels with more hits, red points show the channels with fewer hits. On the right
side the color indicates PMT activation time: the darker blue color shows earlier ﬁrst hit arrivals.
The primary vertex is shown by the gray sphere.

3.2 Data structure

To train models and to evaluate models performances we prepared two datasets: the training dataset
and the testing dataset:

1. Training dataset consists of 5 million events, with a ﬂat kinetic energy spectrum ranging
from 0 to 10 MeV and uniformly spread in the volume of the central detector (in LS),
𝐸kin ∈ [0, 10] MeV.

2. Testing dataset consists of 14 subsets with discrete kinetic energies of 0 MeV, 0.1 MeV,
0.3 MeV, 0.6 MeV, 1 MeV, 2 MeV, ..., 10 MeV and uniform spatial distribution. Each subset
contains about 100 thousand events.

4 Machine learning approach

4.1 Feature Engineering

In this study we use aggregated features based on cumulative information from PMTs as input to
machine learning models. This section describes our feature engineering procedure. The basic
aggregated features for energy reconstruction are the following:

1. The accumulated charge on all ﬁred PMTs: AccumCharge. In the ﬁrst order it is proportional

to 𝐸dep.

2. The total number of ﬁred PMTs: nPMTs.

– 5 –

LPMT_nPELPMT_FHTEdep = 6.165 MeVEvtID = 1135▼3. Coordinate components of the center of charge:

(𝑥cc, 𝑦cc, 𝑧cc) = (cid:174)𝑟cc =

(cid:205)𝑁PMTs
𝑖=1

(cid:174)𝑟PMT𝑖 · 𝑛p.e.,𝑖
𝑛p.e.,𝑖

(cid:205)𝑁PMTs
𝑖=1

and its radial component:

𝑅cc = |(cid:174)𝑟cc|

,

(4.1)

(4.2)

Coordinate components of the center of charge are rough approximations of the location of
the energy deposition and help to correct the non-uniformity of the detector response. This
non-uniformity is illustrated in Figure 3, which shows the accumulated charge per 1 MeV of
deposited energy as a function of radius cubed 𝑅3. The sharp decrease of the accumulated
charge per 1 MeV in the region of 𝑅 (cid:38) 16 m (or 𝑅3 (cid:38) 4000 m3) is caused by the eﬀect of the
total internal reﬂection, due to which the photons with a large incident angle never escape the
target.

Figure 3: Accumulated charge on PMTs per 1 MeV of deposited energy as a function of radius
cubed 𝑅3 for the positrons.

4. Coordinate components of the center of ﬁrst hit time (FHT):

(𝑥cht, 𝑦cht, 𝑧cht) = (cid:174)𝑟cht =

1
(cid:205)𝑁PMTs
𝑖=1

𝑁PMTs∑︁

1
𝑡ht,𝑖+𝑐

𝑖=1

(cid:174)𝑟PMT𝑖
𝑡ht,𝑖 + 𝑐

,

and its radial component:

𝑅cht = |(cid:174)𝑟cht|

(4.3)

(4.4)

Here the constant 𝑐 is required to avoid division by zero. The value of 50 ns was selected
to make the center of FHT closer to the energy deposition vertex. These features give extra
information on the location of the energy deposition.

5. For some ML models it is useful to engineer new synthesized features from the existing

ones [24, 25]. We construct the following synthesized features:

– 6 –

010002000300040005000120014001600180020005001000150020002500CountAccumulated charge per MeV, PE/MeV𝛾cc

𝑧 =

𝑧cc
cc + 𝑦2
cc

√︁𝑥2

𝑥cc
cc + 𝑦2
cc

, 𝛾cc

𝑥 =

𝑦cc
cc + 𝑧2
cc

, 𝛾cc

𝑦 =

√︁𝑥2

√︁𝑥2
cc + 𝑦2
cc
𝑧cc
cc · sin 𝜃cc, 𝜌cc =

√︁𝑧2
𝑦cc
𝑥cc
cc,
cc + 𝑦2
𝑥2

, 𝜙cc = arctan
√︃

;

;

(4.5)

(4.6)

(4.7)

𝜃cc = arctan

𝐽cc = 𝑅2

with 7 similar features for the components of the center of FHT.

6. Figure 4 shows the Cumulative Distribution Functions (CDFs) and Probability Density Func-
tions (PDFs) for FHT, i.e. the distribution for the ﬁrst hit time in all ﬁred PMTs. The values
of FHT are the result of the waveform reconstruction and the trigger algorithms embedded in
the JUNO simulation software. The procedure is the same as it will be for real data and does
not rely on the true simulation information in any sense. After an event starts, the main part
of the charge is collected by PMTs in about 200 ns (depending on the distance from the center
of the detector). In the remaining time of the signal, there is a noticeable contribution from
scattered and reﬂected photons as well as from dark noise. There is a clear dependence of
the shape on both energy and radial position. The depression for FHT PDFs at 16.9 m (left,
yellow) is due to the total internal reﬂection of photons having large incidence angles at the
spherical surface of the detector. Near the edge events appear earlier in the readout window
because of the lower time-of-ﬂight correction. The FHT PDFs at the detector center (right)
have similar shapes only diﬀering by the relative contribution from dark noise hits and from
annihilation gammas which decreases with increasing energy. To characterize the CDF for
FHT we use the following sets of percentiles as features:

{ht2%, ht5%, ht10%, ht15%, ..., ht90%, ht95%}

as well as mean, standard deviation, skewness and kurtosis:

{htmean, htstd, htskew, htkurtosis}

(4.8)

(4.9)

7. Likewise, we use features extracted from the distribution of charge on PMT channels (nPE).
The corresponding PDFs and CDFs are shown in Figure 5. The nPE PDFs consist of two
prominent peaks corresponding to one or two hits on a single PMT. The higher the energy the
more the second peak is populated. Moreover, for events occurring near the edge there are
PMTs accumulating a large amount of charge and forming the long tail of the distribution.
Therefore, the shape of this distribution delivers information both on the intensity of the light
emission and its position.

{pe2%, pe5%, pe10%, pe15%, ..., pe90%, pe95%}

{pemean, pestd, peskew, pekurtosis}

(4.10)

(4.11)

– 7 –

Figure 4: Examples of CDFs and PDFs for FHT distributions with the kinetic energy of 𝐸kin =
1 MeV but diﬀerent radius (left) and events at the center of the detector (𝑅 = 0) but diﬀerent energies
(right). Dashed lines denote PDF mean values.

Figure 5: Examples of CDFs and PDFs for charge distributions with the kinetic energy of 𝐸kin =
1 MeV but diﬀerent radius (left) and events at the center of the detector (𝑅 = 0) but diﬀerent energies
(right). Dashed lines denote PDF mean values.

8. Figure 6 shows the average PDF for FHT distribution for 161 events with 𝐸dep = 2.022 MeV
and 𝑅 = 16 m. There is noticeable variance in the ﬁrst bins. To take this into account we also
use the following diﬀerences between percentiles since they are more robust to this variance:

{ht5%−2%, ht10%−5%, ..., ht95%−90%}

(4.12)

In total we engineered 91 features.

4.2 Boosted decision trees

Boosted decision trees (BDT) is a supervised machine learning algorithm [27, 28]. Supervised
algorithms learn the mapping of input features to target output, using a data sample with input-
output pairs.

– 8 –

02004006008001000120000.250.50.751020040060080010001200050100150200250R = 2.01 mR = 9.0 mR = 16.9 mt, nsF(t)02004006008001000120000.250.50.75102004006008001000120002004006008001000E = 1.02 MeVE = 5.02 MeVE = 9.02 MeVt, nsF(t)012345600.250.50.7510123456050100150R = 2.01 mR = 9.0 mR = 16.9 mnPEF(nPE)012345600.250.50.75101234560100200300E = 1.02 MeVE = 5.02 MeVE = 9.02 MeVnPEF(nPE)Figure 6: Average PDF for FHT distribution with deposited energy 𝐸dep = 2.022 MeV and
𝑅 = 16 m.

BDT is an ensemble model with Decision Tree (DT) [29] as the base algorithm. DT is a
simple, fast and interpretable model. DT consists of a binary set of splitting rules based on values
of diﬀerent features of the object. Herewith the single DT is not a powerful enough algorithm,
therefore an ensemble of DTs is commonly used.

DTs in BDT are trained sequentially. Each subsequent DT in BDT is trained to correct errors
of previous DTs in the ensemble. Thus, using a set of weak learners we can build one strong
learner. The tree-based models, including BDT, are an eﬃcient way to work with tabular data.
In this work we use XGBRegressor implementation of BDT from the XGBoost library [30] for
Python. XGBRegressor has a great preset of hyperparameters making it easy to tune the model.
XGBRegressor is stable, fast to train, and fast to make predictions.

4.2.1 Feature selection

Figure 7: 5-fold cross-validation procedure.

Many of the 91 features described in Section 4.1 are highly correlated and we want to keep only
a subset of features which provide the same performance of the model as the full set of features.
The feature selection procedure for the BDT model is described as follows:

1. We train the BDT model on a dataset with 1 million events using 5-fold cross-validation from
the Scikit-learn library [31]. Figure 7 illustrates the cross-validation with the early stopping
condition on an additional hold-out dataset. We evaluate the model performance with Mean
Absolute Percentage Error (MAPE), which is better suited for reconstruction at low energies.
As a result of the cross-validation procedure we obtain mean MAPE on validation datasets
and its standard deviation 𝜀.

– 9 –

02004006008001000050100150200250Energy = 2.022 MeV, R = [15.995-16.005] m., Number of events: 161t, nsTrainingValidationMAPEMAPEMAPEMAPEMAPEDataset with 1 million eventsMeanStd 2. We then initialize the empty list and start ﬁlling it with features. On each step we pick a
feature from the full set and train the BDT model using a 5-fold cross-validation procedure
with early stopping. Then we calculate the mean MAPE for each step and add to the list the
one that provides the best performance of the model on this step. We stop when the MAPE
score diﬀers from the mean MAPE score, for the model trained on all features, by less than
the standard deviation 𝜀.

Figure 8: Results of the feature selection procedure. Dashed line — the average MAPE for BDT
trained on all features after CV with its standard deviation. First ﬁve features: AccumCharge, 𝑅cht,
𝑧cc, pestd, nPMTs.

Figure 8 shows the results of the above-described feature selection procedure. Thus, the

procedure results to the following set of features:

1) AccumCharge
2) 𝑅cht
3) 𝑧cc
4) pestd

5) nPMTs
6) htkurtosis
7) ht25%−20%
8) 𝑅cc

9) ht5%−2%
10) pemean
11) 𝐽cht
12) 𝜙cc

13) ht35%−30%
14) ht20%−15%
15) pe35%
16) ht30%−25%

Figure 9 shows the dependence of the eﬀective resolution ˜𝑎 and standard deviation (RMSE) for
reconstructed energy, and true energy obtained with sequentially selected features as input of the
BDT model. The dashed line illustrates ˜𝑎 for BDT trained on the full set of features and the dark
red area is its error. The decreasing of an error on the validation dataset is consistent and converged
with the improvement of the eﬀective energy resolution ˜𝑎. All models were trained with a maximal
depth of tree equal to 9 and learning rate 0.08, and with the early stopping condition with patience
5.

4.2.2 Hyperparameter optimization

We use the grid search approach for hyperparameter optimization for the BDT model. The space
of hyperparameters is determined and a metric is calculated for each possible combination of

– 10 –

1.1851.191.1951.21.2051.211.2151.22Added featureMAPE, %Figure 9: The dependence of the eﬀective resolution ˜𝑎 and RMSE on the validation dataset for
BDT on sequentially selected features of the model. First ﬁve features: AccumCharge, 𝑅cht, 𝑧cc,
pestd, nPMTs.

hyperparameters from this space. The node with the best metric is selected as the optimum.

For grid search we use the BDT model with a learning rate equal to 0.08 trained with a 5-fold
cross-validation on the dataset with 1 million events. We optimize the maximal depth of the tree in
the ensemble. The training of the model stops if the MAPE score on the hold-out validation dataset
has not decreased for 5 successive iterations.

Figure 10 shows the results of the hyperparameter optimization. The best maximum depth of

the tree was found to be 10.

Figure 10: The dependence of minimal optimum number of trees and corresponding MAPE score
for the validation dataset on the maximum depth of tree.

– 11 –

2.922.9633.043.088181.58282.5Val. RMSEAdded featureVal. RMSE, KeV56789101112131450010001500200025001.1521.1541.1561.1581.161.1621.1641.166Number of treesMAPEMaximal depth of treeNumber of treesMAPE, %4.3 Fully connected deep neural network

Training a neural network means ﬁtting, based on data, the best parameters 𝜽 for mapping 𝑓 the
input feature matrix x to the output vector 𝑦: 𝑦 = 𝑓 (x; 𝜽).

A fully connected neural network consists of layers with sets of units called neurons. Each
neuron in a layer is connected with each neuron in the next layer. It is called a fully connected deep
neural network (FCDNN) if it has many layers.

Neuron computes a linear combination of its inputs: 𝑓 (x) = 𝑊x + b, where W is a matrix with
learnable weights of the neuron and b is a bias of the neuron. In order for a network to approximate
nonlinear functions, it is required to add nonlinearity — activation function ℎ — for neuron output.
Thus, each neuron computes: 𝑓 (x) = ℎ (𝑊x + b).

Optimization of the hyperparameters for FCDNN is performed using a BayesianOptimization
tuner from the KerasTuner library for Python [32]. To train the model we use TensorFlow [33].
MAPE loss for reconstructed energy and true energy is used as the loss function. All input features
were normalized with a standard score normalization. The training process is performed with early
stopping condition on the validation dataset with patience 25 and with batch size 1024. Table 1
shows the search space and the selected hyperparameters.

Hyperparameter
Units in input layer
Units in hidden layers
Number of hidden layers
Activation [34–36]
Optimizer [37, 38]
Learning rate
Scheduler type [39]
Input layer weights initialization
Hidden layers weights initialization

Range
[1, 512]
[1, 512]
[1, 32]
ReLU, ELU, SELU
Adam, SGD, RMSprop
[0.0001, 0.01]
Exponential, None

Selected
256
256
16
ReLU
Adam
0.0016
Exponential

normal, lecun-normal, uniform

normal

Table 1: Hyperparameter search space for FCDNN. Selected hyperparameters are highlighted in
bold.

We ﬁrst select several subsets of features that are reasonable from our previous experience.
Then we pick the following subset of features that provides virtually the same quality of the FCDNN
model as the full set of features:

1) AccumCharge
2) nPMTs
3) 𝑅cc
4) 𝑅cht
5) 𝜌cc
11) Percentiles of FHT distribution: {ht2%, ht5%, ht10%, ht15%, ..., ht90%, ht95%}

6) 𝜌cht
7) pemean
8) pestd
9) peskew
10) pekurtosis

It is interesting to note that when using those features selected for the BDT approach, the quality is
within 2𝜎 from the optimal quality corresponding to the feature list above.

– 12 –

Figure 11 describes the selected architecture of FCDNN and its main selected hyperparameters.

Figure 11: Selected FCDNN architecture and its main selected hyperparameters.

5 Results

(a)

(b)

Figure 12: Examples of distributions of the predicted deposited energy for 𝐸dep = 3.022 MeV (a)
and 𝐸dep = 1.022 MeV (b). The orange line in the left ﬁgure represents the Gaussian ﬁt. The
distributions were obtained, using the BDT model.

As it was mentioned in Section 2, to evaluate the performance of the models we use resolution
and bias metrics. These metrics are obtained from the Gaussian ﬁt of the 𝐸predicted−𝐸dep distribution,
as its standard deviation (𝜎) and mean value (𝜇). Figure 12a illustrates an example of 𝐸predicted −𝐸dep
distribution and its Gaussian ﬁt with 𝐸dep = 3.022 MeV. Note that we exclude edge points with
0 MeV and 10 MeV kinetic energy, corresponding to 1.022 and 11.022 MeV of deposited energy.

– 13 –

ReLUReLUReLUReLUReLUReLUReLUReLUReLUlinearAccumChargenPMTsht95%Input layerh1h16EdepMAPE loss30 features256 units256 unitsHidden layers−0.2−0.15−0.1−0.0500.050.10.150.200.010.020.030.04μ = 0.001 MeV, σ = 0.051 MeV−0.2−0.15−0.1−0.0500.050.10.150.200.040.080.120.16The ML models learn that the energy of the event belongs to the energy range from the training
dataset (𝐸kin 0-10 MeV) and, therefore, the distributions at the edge points are truncated and exhibit
artiﬁcially increased resolution, as shown in Figure 12b.

Figure 13: Energy resolution performance: resolution (upper panel) and bias (lower panel) for
BDT and FCDNN models. Note: the most left point corresponds to 1.122 MeV.

Figure 13 shows the energy reconstruction performance (resolution and bias) of the BDT and
FCDNN models described in Sections 4.2 and 4.3. Parameterization 2.1 with ﬁtted 𝑎, 𝑏, and 𝑐
values is also shown. The bias of the energy reconstruction is very close to zero for both models.
The resolution performance is better for FCDNN in the lower energy region and then converges
with the BDT results at higher energies.

Table 2 lists parameters 𝑎, 𝑏, and 𝑐 of the parameterization 2.1 for the energy resolution curves
and the eﬀective resolution ˜𝑎 for both BDT and FCDNN models. The value of ˜𝑎 is noticeably better
for FCDNN but for both models, ˜𝑎 is better than required: ˜𝑎 < 3%.

𝑎 ± Δ𝑎
Model
2.573 ± 0.097
BDT
FCDNN 2.316 ± 0.139

𝑏 ± Δ𝑏
0.763 ± 0.045
0.827 ± 0.054

𝑐 ± Δ𝑐
0.990 ± 0.394
1.474 ± 0.285

˜𝑎 ± Δ ˜𝑎
2.914 ± 0.016
2.822 ± 0.027

Table 2: Parameters 𝑎, 𝑏, and 𝑐 of the parameterization for the energy resolution curves and the
eﬀective resolution ˜𝑎 for BDT and FCDNN models.

We also investigate how the resolution depends on the diﬀerent sub-detector regions, see
Figure 14. We used our models, BDT and FCDNN, trained on the dataset with the standard ﬁducial
volume cut of 17.2 m, i.e. in the 0 − 17.2 m range, but tested them on the datasets with varied 𝑅

– 14 –

11.522.5312345678910−0.200.2BDTFCDNNDeposited energy, MeVResolution, %Bias, %ranges. The R ranges are selected to contain approximately the same number of events. The eﬀective
resolution in the 𝑅 regions is correlated with the accumulated charge per MeV (see Figure 3): less
charge means worse eﬀective resolution and vice versa. Consequently, the performance of both
models worsens for events close to the edge of the detector.

Besides that, the outer region of the detector is more populated by background events originating
from radioactive decays in materials. Therefore, one may consider reducing the ﬁducial volume
to increase the quality of data. On another side, excluding outer events decreases statistics which
limits the experiment sensitivity to the neutrino oscillation pattern. Thus, the optimal strategy has
to be found via a comprehensive sensitivity study.

Figure 14: The expected value of the eﬀective resolution ˜𝑎 for equidistant sub-detector regions.
Dashed lines and ﬁlled areas represent the eﬀective resolution ˜𝑎 and its standard deviation for BDT
and FCDNN models respectively. Both models are trained on data with 𝑅FV < 17.2.

6 Calibration

Models trained and validated on simulated data do not guarantee that their prediction will be
accurate for real experimental data. There are approaches based on domain adaptation methods
that can improve the performance of the reconstruction algorithm on real data, using unlabeled real
data samples [40]. In addition, the accuracy of the models has to be checked, e.g. with calibration
data, that is with signals from well known sources. Calibration data can also be used to ﬁne tune
reconstruction algorithms trained on synthetic data. Here we present a simple way to compare our
models with the calibration data of JUNO. It includes the data from gamma sources. Since the
models are prepared for positrons a special correction is needed to make the comparison possible.
JUNO has an extensive calibration program: multiple radioactive sources and background

processes are planned to be used. Here we consider three calibration sources, listed in Table 3.

– 15 –

R=[0, 8.0]R=[8.0, 10.1]R=[10.1, 11.5]R=[11.5, 12.7]R=[12.7, 13.7]R=[13.7, 14.5]R=[14.5, 15.3]R=[15.3, 16.0]R=[16.0, 16.6]R=[16.6, 17.2]2.72.82.933.13.2BDTFCDNNSub-detector regions, mType Radiation

Source
241Am−13C 𝛾
𝛾
60Co
𝑒+
68Ge

neutron + 6.13 MeV
1.173 + 1.333 MeV
annihilation 0.511 + 0.511 MeV

Table 3: Radioactive sources for calibration.

1. 241Am−13C source produces both a neutron and a high energy gamma. Then the neutron is
captured, emitting a gamma of 2.22 MeV (on hydrogen atom) or 4.95 MeV (on carbon atom).
We consider only 2.22 MeV gammas here because these are more abundant.

2. 60Co produces two gammas with the energy of 1.173 MeV and 1.333 MeV. We assume they

merge in one signal and reconstruct the sum energy of both gammas.

3. 68Ge decays to 68Ga via electron capture, then 68Ga decays via positron emission (𝛽+) to 68Zn.
The kinetic energy of the positron is absorbed by the source enclosure, but two annihilation
gammas escape. As a result, we can consider this source as positrons with zero kinetic energy.

These calibration sources are planned to be moved over the central detector volume. The
measured signal is to be used for the improvement of simulation and reconstruction algorithms.
While the kinematics of the calibration sources is well deﬁned, the deposited energy varies from
event to event. Thus, in practice, only the distributions are known. To evaluate the performance of
the reconstruction algorithms we compare the predictions based on the real calibration data with
these distributions.

(a)

(b)

(c)

Figure 15: Distributions of true deposited energy and predicted energy, using the BDT model, for
calibration sources: a) AmC, b) Co-60, c) Ge-68.

Our models, which are trained on positron sources, are expected to inevitably have weaker
results on gamma sources (241Am−13C, 60Co) because of diﬀerent event topology: positrons
deposit their energy in a very compact region, while gammas travel longer distances. Indeed, we
obtain that the predictions for gammas are shifted and has larger variation, see Figure 15. This
ﬁgure shows the distributions of the true simulated energy (in red) and the energy reconstructed
by the BDT model (in green) for the 241Am−13C, 60Co, and 68Ge sources. To compensate for

– 16 –

22.12.22.32.42.500.20.40.60.81AmC:True energyPred. energyPred. energy (corrected)Deposited energy, MeV11.522.500.10.20.30.40.50.60.7Co60:True energyPred. energyPred. energy (corrected)Deposited energy, MeV0.60.811.21.400.10.20.30.40.50.6Ge68:True energyPred. energyDeposited energy, MeV(a)

(b)

(c)

(d)

Figure 16: 2D histograms for true deposited energy and predicted (corrected) energy, using the
BDT model, for calibration sources and positron events: a) AmC, b) Co-60, c) Ge-68, d) 𝑒+. The
number of events in each cell is given in logarithmic scale. Violet dashed lines show 𝐸true = 𝐸rec.

the bias we extract the necessary correction from values predicted by the model on pure gamma
events. We parameterize the correction as 7.955𝐸 −1.418 − 0.209 (MeV) and then subtract it from
the reconstructed energy for the gamma sources 241Am−13C and 60Co (blue in Figure 15).

In future, the agreement between the expected source spectra and the spectra reconstructed from
the real calibration data would indicate that the reconstruction algorithm works correctly. Being
realistic we expect that our reconstructed spectra for calibration sources will not match the measured
ones. We can not foresee what kind of disagreement we will get. However, some information may
be extracted from analyzing the diﬀerences in the shapes of the reconstructed and may help to spot
the problems in the underlying models. Another possibility is to run an additional round of training
with the data from calibration sources, which are limited but may be enough for ﬁne tuning.

The ways of using these possible discrepancies are out of the scope of this work and will be

addressed in the future.

Figure 16 shows a 2D correlation between true energy and predicted energy (with the applied
correction for 241Am−13C and 60Co sources), and complements Figure 15. Figure 16 illustrates that
the correlation is strong. Note that a large bias in the 𝐸dep < 1.022 MeV region for the 68Ge source is
expected because almost all training events have 𝐸dep above 1.022 MeV. The inability of the model
to make proper predictions outside of its training domain explains the complex structure below
1.022 MeV. The 2D correlation for positrons (Figure 16d) demonstrates a good match between
the reconstructed and true energies: 𝐸true = 𝐸rec, also including the regions between the discrete

– 17 –

22.12.22.32.42.522.12.22.32.42.51.e01.e0.51.e11.e1.51.e21.e2.51.e31.e3.51.e4CountReconstructed deposited energy, MeVTrue deposited energy, MeVAmC11.522.511.21.41.61.822.22.42.61.e01.e0.51.e11.e1.51.e21.e2.51.e31.e3.51.e4CountReconstructed deposited energy, MeVTrue deposited energy, MeVCo600.60.811.21.40.50.60.70.80.911.11.21.31.41.e01.e0.51.e11.e1.51.e21.e2.51.e31.e3.51.e4CountReconstructed deposited energy, MeVTrue deposited energy, MeVGe68123456789101112345678910111.e01.e0.51.e11.e1.51.e21.e2.51.e31.e3.51.e41.e4.5CountReconstructed deposited energy, MeVTrue deposited energy, MeVe+energies. This conﬁrms the expected behaviour of the model.

7 Conclusions

In this work, we present an application of machine learning techniques for precise energy recon-
struction for the energy range of 0–10 MeV. We use two models: Boosted Decision Trees (BDT)
and Fully Connected Deep Neural Network (FCDNN), trained using aggregated features extracted
from Monte Carlo simulation data. We considered the case of the JUNO detector. However, the
approaches are valid for other similar detectors with a large liquid scintillator target surrounded by
an array of photo-sensors. Our dataset is generated with oﬃcial JUNO software for the modeling
of particle interactions, light emission, transport and collection, as well as the realistic response of
photo-multipliers and the readout system.

We design a large set of features and select one features subsets for the BDT model and another
feature subset for the FCDNN model, both provide the same performances as the full set of features.
The requirement on the eﬀective resolution to determine the neutrino mass ordering, ˜𝑎 ≤ 3%, is
achieved by both BDT and FCDNN. BDT is a fast and minimalistic model, making predictions 3-4
times faster than FCDNN. On the other hand the latter provides slightly better performance.

We also consider three calibration sources, 241Am−13C, 60Co, 68Ge, for future evaluation of the
reliability of the reconstruction algorithms. The agreement between the models’ prediction made on
real calibration data and on simulated data would conﬁrm the correctness of the reconstruction. A
diﬀerence in the reconstructed calibration spectra will provide an input for the Monte Carlo tuning.

Acknowledgments

We are very thankful to JUNO collaborators who contributed to the development and validation of
the JUNO simulation software. We thank N. Kutovskiy and N. Balashov for providing an extensive
IT support for computing resources of JINR cloud services [41] and X. Zhang for her work on
producing of the MC samples. We are also grateful to Maxim Gonchar for fruitful discussions.

Fedor Ratnikov is supported by the Russian Science Foundation under grant agreement №19-
71-30020. Yury Malyshkin is supported by the Russian Science Foundation under grant agreement
№21-42-00023.

References

[1] X. Guo et al. [Borexino Collaboration], Science and technology of Borexino: a real-time detector for

low energy solar neutrinos, Astropart. Phys. 16 (2002) no.3 205-234.

[2] K. Eguchi et al. [KamLAND Collaboration], First results from KamLAND: Evidence for reactor

anti-neutrino disappearance, Phys. Rev. Lett. 90 (2003), 021802.

[3] X. Guo et al. [Daya Bay Collaboration], A Precision Measurement of the Neutrino Mixing Angle 𝜃13

using Reactor Antineutrinos at Daya Bay, arXiv:hep-ex/0701029.

[4] F. Ardellier et al. [Double Chooz Collaboration], Double Chooz: A Search for the Neutrino Mixing

Angle 𝜃13, arXiv:hep-ex/0606025.

– 18 –

[5] J. K. Ahn et al. [RENO Collaboration], RENO: An Experiment for Neutrino Oscillation Parameter 𝜃13

Using Reactor Neutrinos at Yonggwang, arXiv:1003.1391.

[6] F. An et al. [JUNO Collaboration], Neutrino Physics with JUNO, J. Phys. G 43 (2016) no.3, 030401.

[7] A. Abusleme et al. [JUNO Collaboration], JUNO Physics and Detector, Prog. Part. Nucl. Phys. 123,

103927 (2022).

[8] D. Bourilkov, Machine and Deep Learning Applications in Particle Physics, Int. J. Mod. Phys. A 34,

no.35, 1930019 (2020).

[9] M. D. Schwartz, Modern Machine Learning and Particle Physics, Harvard Data Science Review

(2021).

[10] D. Guest, K. Cranmer and D. Whiteson, Deep Learning and its Application to LHC Physics, Ann.

Rev. Nucl. Part. Sci. 68, 161-181 (2018).

[11] HEP ML Community. A Living review of machine learning for particle physics.

https://iml-wg.github.io/HEPML-LivingReview/.

[12] Z. Qian, V. Belavin, V. Bokov, R. Brugnera, A. Compagnucci, A. Gavrikov, A. Garfagnini,

M. Gonchar, L. Khatbullina and Z. Li, et al. Vertex and energy reconstruction in JUNO with machine
learning methods, Nucl. Instrum. Meth. A 1010 (2021), 165527.

[13] A. Gavrikov, F. Ratnikov, The use of Boosted Decision Trees for Energy Reconstruction in JUNO

experiment, EPJ Web Conf. 251 (2021), 03014.

[14] Z. Li, Y. Zhang, G. Cao, Z. Deng, G. Huang, W. Li, T. Lin, L. Wen, M. Yu and J. Zou, et al. Event

vertex and time reconstruction in large-volume liquid scintillator detectors, Nucl. Sci. Tech. 32 (2021)
no.5, 49.

[15] Z. Y. Li, Z. Qian, J. H. He, W. He, C. X. Wu, X. Y. Cai, Z. Y. You, Y. M. Zhang and W. M. Luo,

Improving the machine learning based vertex reconstruction for large liquid scintillator detectors with
multiple types of PMTs, arXiv:2205.04039.

[16] A. Abusleme et al. [JUNO Collaboration], Calibration Strategy of the JUNO Experiment, JHEP 03,

004 (2021).

[17] X. Huang et al., Oﬄine Data Processing Software for the JUNO Experiment, PoS ICHEP2016

(2017), 1051.

[18] T. Lin et al., The Application of SNiPER to the JUNO Simulation, J. Phys. Conf. Ser. 898 (2017) no.4,

042029.

[19] T. Lin et al., Parallelized JUNO simulation software based on SNiPER, J. Phys. Conf. Ser. 1085

(2018) no.3, 032048.

[20] S. Agostinelli et al. [GEANT4 Collaboration], GEANT4–a simulation toolkit, Nucl. Instrum. Meth. A

506 (2003), 250-303.

[21] J. Allison, J. Apostolakis, S. B. Lee, K. Amako, S. Chauvie, A. Mantero, J. I. Shin, T. Toshito,

P. R. Truscott and T. Yamashita, et al. Recent developments in Geant4, J. Nucl. Instrum. Meth. A 835
(2016), 186-225.

[22] K. Li, Z. You, Y. Zhang, J. Zhu, T. Lin, Z. Deng and W. Li, GDML based geometry management

system for oﬄine software in JUNO, Nucl. Instrum. Meth. A 908 (2018), 43-48.

[23] A. Abusleme, T. Adam, S. Ahmad, et al. [JUNO Collaboration], Mass Testing and Characterization

of 20-inch PMTs for JUNO, 2022, arXiv:2205.08629.

– 19 –

[24] A. Coates, A. Ng, H. Lee, An analysis of single-layer networks in unsupervised feature learning,

Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, PMLR
15 (2011) 215-223.

[25] J. Heaton, An empirical analysis of feature engineering for predictive modeling, SoutheastCon, IEEE

(2016), 1-6.

[26] X. Fang, Y. Zhang, G. H. Gong, G. F. Cao, T. Lin, C. W. Yang and W. D. Li, Capability of detecting

low energy events in JUNO Central Detector, JINST 15, no.03, P03020 (2020).

[27] J. Friedman, Stochastic gradient boosting, Computational statistics & data analysis 38 (2002) no.4,

367-378.

[28] J. Friedman, Greedy function approximation: A gradient boosting machine, Ann. Statist. 29 (2001)

1189-1232 no.5.

[29] J. Quinlan, Simplifying decision trees, International Journal of Man-Machine Studies 27 (1987)

221-234 no.3.

[30] T. Chen, C. Guestrin, Xgboost: A scalable tree boosting system, Proceedings of the 22nd acm sigkdd

international conference on knowledge discovery and data mining (2016), 785–794.

[31] F. Pedregosa et al., Scikit-learn: Machine Learning in Python, The Journal of Machine Learning

Research 12 (2011) 2825–2830.

[32] T. O’Malley et al., KerasTuner, 2019, https://github.com/keras-team/keras-tuner/

[33] A. Martin et al., TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015,

https://www.tensorﬂow.org/

[34] N. Vinod, G. Hinton, Rectiﬁed linear units improve restricted boltzmann machines, Proceedings of the
27th International Conference on International Conference on Machine Learning (2010) 807–814.

[35] D.-A. Clevert, T. Unterthiner, S. Hochreiter, Fast and Accurate Deep Network Learning by

Exponential Linear Units (ELUs), 2015, arXiv:1511.07289.

[36] G. Klambauer, T. Unterthiner, A. Mayr, S. Hochreiter, Self-Normalizing Neural Networks, Proceedings
of the 31st international conference on neural information processing systems (2017), 972–981.

[37] D. Kingma, J. Ba, Adam: A Method for Stochastic Optimization, 2014, arXiv:1412.6980.

[38] S. Ruder, An overview of gradient descent optimization algorithms, 2016, arXiv:1609.04747.

[39] Z. Li, S. Arora, An Exponential Learning Rate Schedule for Deep Learning, 2019, arXiv:1910.07454.

[40] G. N. Perdue et al. [MINERvA Collaboration], Reducing model bias in a deep learning classiﬁer
using domain adversarial neural networks in the MINERvA experiment, JINST 13, no.11, P11020
(2018)

[41] A. Baranov, N. Balashov, N. Kutovskiy and R. Semenov, JINR cloud infrastructure evolution, Phys.

Part. Nucl. Lett. 13 (2016) no.5, 672-675

– 20 –

