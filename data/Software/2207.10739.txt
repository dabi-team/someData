2
2
0
2

l
u
J

1
2

]

G
L
.
s
c
[

1
v
9
3
7
0
1
.
7
0
2
2
:
v
i
X
r
a

BigIssue: A Realistic Bug Localization Benchmark

Paul Kassianik∗
Salesforce Research
pkassianik@salesforce.com

Erik Nijkamp∗
Salesforce Research
erik.nijkamp@salesforce.com

Bo Pang
Salesforce Research
bo.pang@salesforce.com

Yingbo Zhou
Salesforce Research
yingbo.zhou@salesforce.com

Caiming Xiong
Salesforce Research
cxiong@salesforce.com

Abstract

As machine learning tools progress, the inevitable question arises: How can ma-
chine learning help us write better code? With signiﬁcant progress being achieved
in natural language processing with models like GPT-3 and Bert, the applications
of natural language processing techniques to code are starting to be explored.
Most of the research has been focused on automatic program repair (APR), and
while the results on synthetic or highly ﬁltered datasets are promising, such mod-
els are hard to apply in real-world scenarios because of inadequate bug localiza-
tion. We propose BigIssue: a benchmark for realistic bug localization. The goal of
the benchmark is two-fold. We provide (1) a general benchmark with a diversity
of real and synthetic Java bugs and (2) a motivation to improve bug localization
capabilities of models through attention to the full repository context. With the in-
troduction of BigIssue, we hope to advance the state of the art in bug localization,
in turn improving APR performance and increasing its applicability to the modern
development cycle.

1 Introduction

Recent advances in natural language processing (NLP) [2] [5] [22] have increased interest in apply-
ing NLP techniques to code understanding. With the development of code encoders[9] [16], this task
is becoming increasingly more accessible and appealing. As research has jumped ahead into the task
of Automated Program Repair (APR), the results have been not been adequate. Although synthetic
datasets have largely been solved (see Section 2.1), models have been surprisingly underperforming
on real-world datasets, many not even able to repair a quarter of the bugs in the benchmark [24].
This is despite research suggesting that current APR benchmarks suffer from a lack of diversity [8].
As a consequence, many APR models are prone to overﬁtting to speciﬁc datasets [25]. Although
interesting from an academic perspective, such tools would hardly be useful in a real industrial
scenario.

We posit that the three major limitations to APR methods being used today are: (1) training to ﬁx
already located bugs rather than ﬁnding bugs and ﬁxing them, (2) the inability of models to take large
contexts into account, and (3) the reliance on information besides pure code. The ﬁrst limitation is
straightforward: patches have limited context outside of the lines immediately before and after each
patch. It has been shown that APR performance improves signiﬁcantly if a good fault localization
algorithm is used to detect buggy code locations [8] [21]. The second limitation prevents models
from ﬁnding bugs that depend on the context of the program. Even for human readers, many real-
world bugs require a lot of program-speciﬁc context to be detectable. One of the most popular code

∗Equal Contribution

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
encoders today [9] only supports encoding of sequences up to 512 tokens, not nearly enough to
process most Java ﬁles in real-world programs (on average 7.5k tokens with the RoBERTa tokenizer
[22]). The third limitation follows from the fact that the most common method for fault localization
used today (SBFL) [14] is heavily reliant on test cases exposing potentially buggy locations (see
Section 2.2).

In order to advance the state of the art of both BL and APR models, we introduce BigIssue. The
major contributions of BigIssue include:

• A large collection of conﬁrmed real-world bugs with line-level annotations. Each bug
has been reported by live users to the GitHub Issues bug-tracking system and ﬁxed via a
commit or pull request. The dataset contains a total of 23, 924 bugs sourced from 4, 233
Java repositories.

• A very hard synthetic bug collection dataset. It is a long-sequence synthetic bug dataset,

generated by InCoder [11], a state of the art perturbation generation model.

• An empirical demonstration of the hardness of the real benchmark as compared to a syn-
thetic dataset. Even with advanced synthetic bug generation techniques the performance on
real bugs will not be adequate, which calls for further research into realistic bug detection.

By providing a large and diverse dataset of synthetic and real bugs from a multitude of projects
without any extra information outside of code, we hope to push the direction of research towards
line-level long-context bug localization for better performance on APR tasks.

2 Prior Art

2.1 Automatic Program Repair

Since bug localization is fundamentally related to automatic program repair, we provide a brief
survey of existing APR benchmarks and their drawbacks.

Real-world Benchmarks The Defects4J dataset [15] has been widely used in automatic program
repair. It consists of 357 (835 is version 2) bugs sourced from 10 (100) top open-source Java projects.
Bugs are manually reviewed and each bug has at least 1 test case that exposes the bug. APR methods,
however, are not successful enough on this real dataset for the models to be useful in real-world
applications. The most recent state-of-the-art model can only ﬁx 67 out of 357 bugs [33], while the
two previous state-of-the-art models could only ﬁx 44 [24] and 57 [13] bugs. This is despite recent
research that suggests APR methods are overperforming on Defects4J as compared to other similar
benchmarks [8]. Bugs.jar [26] is a similar dataset but with an expanded scope of 8 popular projects
from the Apache foundation.

Another widely used dataset is the ManySStubs4J dataset [17]. It’s a collection of many "stupid"
bugs mined from 100 (1,000) top open-source Java repositories. The collection includes only those
changes where the change is a single line of code and falls into one of the pre-determined 16 cat-
egories of bugs. While convenient, it suffers from a lack of complicated bugs and highly selective
criteria.

Learning-ﬁxes [29] is a collection of about 58,350 short methods mined from GitHub. Each of the
methods was semantically idiomized and presented in the benchmark. The main limitation of this
dataset is that it’s a method-level dataset: each bug should be identiﬁable and ﬁxable based on the
context only present in that particular method. For real bugs, this is usually not the case.

DLFix [20] is another dataset aimed at APR tasks. The dataset consists of almost 5 million meth-
ods, enhanced with metadata, and the corresponding ﬁxed versions of the method for a particular
repository. While interesting for limited cases, the method-level granularity as well as the necessity
of building metadata for each method limits its usefulness, especially on longer methods.

Table 1 presents a comparison of some of the existing APR benchmarks.

Synthetic Benchmarks A natural way to deal with the lack of data diversity in current real-world
benchmarks is to create synthetic benchmarks by perturbing code. Existing work accomplishes
this either via a separate model [19] [6] [16] [32] or via a static oracle (such as a linter) [1]. While

2

Dataset

BigIssue

Defects4J [15]
Bugs.jar [26]

ManySStubs4J [17]
iBugs [4]
Learning-Fixes [29]
DLFix [20]

Size

Gran.

Bug Length Context

# of Repos

Filters

23,924

Line

Multi-line

Repository

4233

Line
Line

Multi-line
Multi-line

357
(835)
1158
10,231
Single-line
(63,923) Line
Multi-line
Line
369
Multi-line
Line
58,350
4,973,000 Method Multi-line

Repository
Repository

5 (17)
8

Repository
Repository
Method
Repository

100 (1000) Yes
No
1
No
-
No
8

No

No
No

Table 1: Comparison of Major Java Bug Detection Datasets.

attractive, there is signiﬁcant evidence that good performance on these benchmarks does not translate
to good performance on real-life bugs [8]. We also perform an experiment 5 that suggests that even
good performance on sophisticated perturbation datasets does not translate well to ﬁxing real bugs.

2.2 Using Existing Benchmarks for Bug Localization

Fault localization and fault prediction have been severely understudied. According to a recent survey
[34] current fault localization and prediction methods can’t even localize half of the bugs in the
Defects4J [15] dataset. The most widely used and best-performaing method for fault localization is
Spectrum-based fault localization (SBFL) [14]. While elementary and simple to implement, it relies
heavily on the quality and quantity of test cases, especially for large programs [18]. The lack of
scalability for this method motivates further research into the problem of bug localization.

3 BigIssue Synthetic Dataset

3.1 Motivation

Error localization in the context of automatic program repair has been a widely studied research topic
[34] [8] [27] [28] [30]. Evaluation of approaches towards error localization requires the construction
of a dataset with known ground-truth. One methodology to create such dataset is to consider existing
code and introduce erroneous perturbations in the form of samples drawn from a generative model.
In prior art [19], synthetic perturbations have been adopted on a function-level granularity with weak
generative models such as small LSTMs. The underlying distribution of such synthetic dataset may
be quite dissimilar to the distribution of realistic bugs, which occur in software engineering [8]. To
decrease this discrepancy, in the following, we will advance this concept to ﬁle-level data and sample
perturbations from a strong generative model.

To evaluate a means of error localization, one requires an evaluation dataset with known ground-
truth errors. Our synthetic dataset adopts the methodology of gathering “real” code as observations
and introducing synthetic perturbations in the observations. Here, the perturbation is in the form
of a rewrite of the original sequence of code into a perturbed sequence of code. In our approach,
a portion of the original code is “masked out” and a generative model is recruited to “ﬁll in” the
masked out code. The “ﬁlled in” portion of the code constitutes the synthetic perturbation. The
perturbation of the original code is assumed to likely to contain “errors”.

While the above approach based on perturbations may appear obvious and trivial, the construction
of such datasets is challenging. This is due to, (1) existing code is not guaranteed to be free of errors,
(2) the deﬁnition or ontology of an “error” or “bug” itself is non-trivial, (3) creating synthetic pertur-
bations which are difﬁcult to discriminate from original observations and yet reﬂect the distribution
of “real” errors is hard.

Prior art addresses these issues (1) by reducing the scope of the code to function or line-level, ef-
fectively reducing the span of code to say 10 lines of code [16] [31] [32] (2) introducing heuristic
perturbations rules or pre-deﬁning a set of categories in which “bugs” fall [16] [6], or (3) perturbing
a single line of code in simple programs [31] [6]. While this over-simpliﬁcation is a reasonable ﬁrst

3

step, the resulting dataset may be quite far from realistic errors in the wild for which localization is
deemed “useful” to a practitioner.

Our work addresses (1) and (2) by doing away with the notion of an “error” and instead shifting the
conceptual thinking toward the distributions of “original” and “perturbed” observations. That is, our
dataset is assumed to contain errors that are not identiﬁed in the ground-truth labels. The task of
error localization is relaxed as the task of localization of perturbations. This relaxation allows us to
consider ﬁle-level observations without the need for a strict deﬁnition of an “error”. In the following,
we will provide details on the creation of such data-set and in particular address (3).

3.2 Dataset Construction

The underlying methodology of the creation of this dataset is (1) for learning and evaluation of
models gather large amounts of ﬁle-level observations (i.e., real code), (2) to introduce synthetic
perturbations from a strong generative models such that discrimination of “original” and “perturbed”
observation is non-trivial, (3) and relax the task of “error localization” to the task of “perturbation
localization”. In the following, we describe the construction of such a dataset.

Observations
In order to obtain large quantities of observations for the learning and evaluation of
localization models, the proposed dataset is a compilation of public, non-personal information from
GitHub consisting of permissively licensed Java code in October 2021. In particular, we gathered
8 million repositories between January 2014 and October 2021 annotated with at least 1 star and
considered the subset of contained ﬁles containing Java code. The ﬁles with are ﬁltered average
lines length of ≤ 100 characters, a maximum line length of 1, 000, and ≥ 90% of the characters
being decimal or hexadecimal digits are removed. Finally, exact duplicates based on their SHA-256
hash are removed, which amounts to a substantial portion of the raw data due to forks and copies of
repositories. The resulting data-set comprises 96.56 GB of raw text.

Perturbations For realistic perturbations, we resort to a method known as “inpainting” for images
or “inﬁlling” for the textual domain. That is, a portion of a giving observation is occluded (or
masked out). Then, the occlusion is reconstructed or “ﬁlled in” by a sample drawn from a generative
model conditional on the non-occluded context. Recently, auto-regressive causal language models
[2] have demonstrated to excel at this task for which the prompt may be treated as context and the
auto-regressive sample conditional on the prompt as the in-painting while preserving the statistical
regularities of the training data. However, the joint distribution over tokens is usually factorized in
a left-to-right order over time, for which the causal mask constraints the inﬁll samples to only take
past context into account, but not future tokens. In our case of sampling realistic perturbations at
random spans within a given observation, we wish to take both the code before and after the masked
out span to be taken account, so that ﬁle-level consistency remains. To address this issue, we recruit
an auto-regressive sampler that re-arranges the input sequence and associated causal masking such
that sampling is conditional on both past and future context [7, 11]. To further reduce the gap
between “real” and “perturbed” sequences, we chose a large-scale language model, InCoder [11]
with 1 billion parameters, and lower the temperature of auto-regressive nucleus sampling to 0.8.
Equipped with such a sampler, a random span in the observation is removed and inﬁlled with a
sample drawn from the InCoder model. The length of the span is drawn from a uniform distribution
with a minimum length of 8 tokens and a maximum length of 64 tokens. The generated sample is
constrained to at most the length of the span.

Task Our proposed “perturbation localization” task can be expressed in the form of a binary classi-
ﬁcation for which each line is labeled as either “original” or “perturbed”. As such, the ground-truth
labels indicate whether the line is a sub-sequence of the observation or was (potentially partially)
perturbed by the sampler. Each ﬁle contains at most one such perturbation. The length of the input
sequence is limited to at most 8, 192 tokens under the RoBERTa tokenizer [22] with at most 512
lines per ﬁle.

3.3 Dataset Examples

To address the aforementioned challenge of creating synthetic perturbations which are difﬁcult to
discriminate from the original distribution and yet reﬂect the distribution of “real” errors, we re-

4

cruited the large-scale InCoder [11] as an auto-regressive sampler with adjusted causal masking to
conditionally sample on a context including future tokens. As a qualitative example, Figure 1 il-
lustrates an original observation on the left-hand side and the perturbed sequence on the right-hand
side.

p a c k a g e com . g i t h u b . y t . m y b a t i s . u t i l s ;

p a c k a g e com . g i t h u b . y t . m y b a t i s . u t i l s ;

i m p o r t com . g i t h u b . y t . b a s e . e x c e p t i o n .

i m p o r t com . g i t h u b . y t . b a s e . e x c e p t i o n .

B a s e E r r o r E x c e p t i o n ;

B a s e E r r o r E x c e p t i o n ;

i m p o r t o r g . a p a c h e . commons . l a n g 3 . S t r i n g U t i l s ;
i m p o r t

j a v a . l a n g . r e f l e c t . F i e l d ;

i m p o r t o r g . a p a c h e . commons . l a n g 3 . S t r i n g U t i l s ;
i m p o r t

j a v a . l a n g . r e f l e c t . F i e l d ;

p u b l i c

c l a s s B e a n U t i l s {

p u b l i c

c l a s s B e a n U t i l s {

p u b l i c

s t a t i c ChainMap < S t r i n g , O b j e c t >

g et V al u eMap ( O b j e c t . . .

o b j s ) {

t r y {

ChainMap < S t r i n g , O b j e c t > map =

new ChainMap < >( ) ;

f o r

( O b j e c t o b j
i f

( n u l l == o b j ) {

: o b j s ) {

c o n t i n u e ;

}
f o r

( C l a s s <?> c = o b j . g e t C l a s s ( ) ;

O b j e c t . c l a s s ! = c ; c = c . g e t S u p e r c l a s s ( ) ) {

p u b l i c

s t a t i c ChainMap < S t r i n g , O b j e c t >

g e t F i e l d M a p ( O b j e c t . . .

o b j s ) {

ChainMap < S t r i n g , O b j e c t > map =

new ChainMap < S t r i n g , O b j e c t > ( ) ;

t r y {
i f

( o b j s

! = n u l l && o b j s . l e n g t h == 1 ) {

O b j e c t o b j = o b j s [ 0 ] ;
f o r ( C l a s s <?> c = o b j . g e t C l a s s ( ) ;
O b j e c t . c l a s s ! = c ; c = c . g e t S u p e r c l a s s ( ) ) {

f o r

( F i e l d f i e l d : c .

f o r

( F i e l d f i e l d : c .

g e t D e c l a r e d F i e l d s ( ) ) {

g e t D e c l a r e d F i e l d s ( ) ) {

f i e l d . s e t A c c e s s i b l e ( t r u e ) ;
O b j e c t v a l u e = f i e l d . g e t ( o b j )

;

;

i f

( n u l l == v a l u e ) {

c o n t i n u e ;

}
i f

( f i e l d . g e t T y p e ( ) .

i s A s s i g n a b l e F r o m ( S t r i n g . c l a s s ) && S t r i n g U t i l s .
i sEm p t y ( ( S t r i n g ) v a l u e ) ) {

c o n t i n u e ;

}
map . p u t ( f i e l d . getName ( ) ,

v a l u e ) ;

}

}

}
r e t u r n map ;

} c a t c h ( E x c e p t i o n e ) {

f i e l d . s e t A c c e s s i b l e ( t r u e ) ;
O b j e c t v a l u e = f i e l d . g e t ( o b j )

i f

( n u l l == v a l u e ) {

c o n t i n u e ;

}
i f

( f i e l d . g e t T y p e ( ) .

i s A s s i g n a b l e F r o m ( S t r i n g . c l a s s ) && S t r i n g U t i l s .
i sEm p t y ( ( S t r i n g ) v a l u e ) ) {

c o n t i n u e ;

}
map . p u t ( f i e l d . getName ( ) ,

v a l u e ) ;

}

}

}
r e t u r n map ;

} c a t c h ( E x c e p t i o n e ) {

t h r o w new B a s e E r r o r E x c e p t i o n ( " O b j e c t

t o

t h r o w new B a s e E r r o r E x c e p t i o n ( " O b j e c t

t o

Map c o n v e r t E r r o r " , e ) ;

Map c o n v e r t E r r o r " , e ) ;

}

}

}

}

}

}

Original observation.

Perturbed observation.

Figure 1: Sampled perturbation introduces a non-trivial rewrite, which may be considered as a “bug”.
Left: Original Java code iterates over a given list of objects (green highlight). Right: Perturbed
Java code only considers the ﬁrst object in the list, if the list contains precisely one element (red
highlight).

Remarkably, both sequences appear to be syntactically correct code. The auto-repressive sampler
took future tokens into account. For example, the type resolution of the object map may be resolved
by the return signature of the function public static ChainMap<...> which was not masked out
and the invocation of map.put(...). While the original code iterates over the list of objects obj,
the perturbed code only considers the ﬁrst element of the list, if the list contains a single element.
Whether the rewrites constitute a "bug" depends on the deﬁnition of the term, as earlier discussed.
However, given the context, one can argue that the rewritten implementation seems less probable to
follow the underlying intent.

3.4 Artifacts

The created data-set contains three partitions, training, validation, and evaluation. The training data
contains 96.22 GB of raw text, which may sufﬁce to train large language models under recent scaling
laws [12]. Details about hosting and accessibility can be found in Appendix A.

5

4 BigIssue Realistic Benchmark

4.1 Motivation

Based on our observations about existing benchmarks from Section 2, we concluded that a new
benchmark was needed to push the state of the art forward. Therefore, we created a benchmark that
prioritized quantity over perceived quality and one that focused speciﬁcally on NL-based line-level
bug localization.

For this benchmark, we deﬁned a line as "buggy" if it has been removed or modiﬁed in the issue
patch. This allows us to avoid using tests as the ground truth for bugs in code. This deﬁnition
also ﬁts well with the usage of code encoders such as CodeBERT [9] for line-level classiﬁcation, as
demonstrated in Section 5.

4.2 Benchmark Construction

First, we considered Java GitHub repositories created between January 2014 and October 2021. To
ensure that we only ﬁlter out repositories that were intended for some form of public use, we only
examined repositories with at least 1 star. We further ﬁltered down the repositories to only those
repositories that had GitHub Issues enabled and had licenses permitting use of their code. That gave
us 4233 repositories.

Using the GitHub API we ﬁltered through closed issues on these repositories. We only used public,
non-personal information available through the API. In order to select issues that corresponded to
bug ﬁxes on that particular repository, we selected issues that either contained "bug", "ﬁx", or "ﬁxed"
as separate words in the title and the body of the issue. We also included issues that contained the
label "bug". We looked at issues with a corresponding "close" event, and we looked at the commit
that was attached to the latest "close" event. This gave us a dataset of 23, 924 total closed issues.

We further subdivide the dataset into single-ﬁle and multi-ﬁle bugs. Single-ﬁle bugs are those that
have exactly one modiﬁed Java ﬁle that doesn’t contain any test code. We set these aside as we
think these bugs will be easier to locate. We therefore get a set of 10905 single-ﬁle bugs and 13019
multi-ﬁle bugs.

To mark buggy lines we examine the data from the hunks in the diff. If a line is (1) removed from the
source ﬁle and (2) is not an import line, it is marked as buggy. In cases where hunks are exclusively
adding code, we mark the two lines in the source before and after the change as buggy.

Test-running frameworks Many of the benchmarks presented above use tests either as assistance
in bug ﬁxing or as a method of ﬁltering bugs. We do not consider testing frameworks and tests as
criteria for whether a commit is a bug or not. Firstly, it was recently shown that unit tests on their
own do not guarantee fewer failures inside the code [3] which implies that there are even more bugs
inside the code that are not exposed by tests. Secondly, we would be severely limiting the diversity
and scope of our benchmark by forcing issues to include an exposing test case.

4.3 Benchmark Examples

In order to show the necessity for long-context models in bug localization, we demonstrate an exam-
ple of a bug that is highly dependent on external context outside of the scope of the ﬁle where the
bug is located. The issue 2 in question is related to a bug in a minecraft plugin. The bug is that the
code calls the global logger instead of the local logger provided via a project-speciﬁc class Varo and
an external library Bukkit3. The sample hunk from the diff is presented in 2.

For a human to understand and debug this issue, the human developer needs to know that the class
Varo exists and is an instance of the Bukkit JavaPlugin class. The human reader must also know
that the JavaPlugin class contains a method called getLogger which presents the user with the
logger one needs to write to to write to the speciﬁc world the plugin instance is active in. For a
model to have a chance at ﬁnding this bug, it must have access to that context. Without the context,
even a human observer cannot reliably mark this as buggy code.

2http://github.com/AlexanderRitter02/Varo-Plugin/issues/25
3https://github.com/Bukkit/Bukkit

6

e n d s i z e = p l u g i n . g e t C o n f i g ( ) . g e t I n t ( " b o r d e r . end − r a d i u s " ) * 2 ;

i n t
d o u b l e s h r i n k A m o u n t P er H o u r = p l u g i n . g e t S e t t i n g s ( ) . g e t B o r d e r S h r i n k P e r H o u r ( ) ;

S y st em . o u t . p r i n t l n ( " W o r l d b o r d e r d i a m e t e r w i l l be s h r u n k e n by " + ( d o u b l e )

s h r i n k A m o u n t P e r H o u r + " b l o c k s

e v e r y " + t i m e i n t e r v a l + "

s e c o n d s

( " + ( d o u b l e )

t i m e i n t e r v a l

/ 3600 + " h o u r s ) . " ) ;

p l u g i n . g e t L o g g e r ( ) . i n f o ( " W o r l d b o r d e r d i a m e t e r w i l l be s h r u n k e n by " + ( d o u b l e )
s e c o n d s

b l o c k s e v e r y " + t i m e i n t e r v a l + "

( " + ( d o u b l e )

t i m e i n t e r v a l

/ 3600 + " h o u r s ) . " ) ;

s h r i n k A m o u n t P er H o u r + "

S t r i n g b o r d e r m s g = " " ;
f o r ( World w o r l d : B u k k i t . g e t W o r l d s ( ) ) {

Wo r l d Bo r d er b o r d e r = w o r l d . g e t W o r l d B o r d e r ( ) ;

Figure 2: Hunk from sample issue from Varo. This bug demonstrates the need for more context than
ﬁle-level information.

4.4 Benchmark Artifacts

For each issue, we provide the unﬁxed and ﬁxed versions, packaged in the .tar.gz format. We
provide the diff information for that commit, as well as information about the issue from the GitHub
API for convenience. Code for processing and obtaining line-level labels for bugs is contained in
the code repository. Details about hosting and accessibility can be found in Appendix A.

5 Synthetic vs Realistic Bug Detection

In this Section, we will conduct a preliminary analysis of the hardness of the BigIssues benchmark.
Since the sequence length exceeds the limitations of most pre-trained language models on code,
we recruit mean pooling to construct a simple baselines. We hypothesize (1) the distribution of
perturbed observations generated by even strong generative models still does not resemble real data,
(2) localization on real data is signiﬁcantly harder, (3) long context is required for accurate bug
localization. Therefore, future research may put increased emphasis on real data. The ﬁndings of
our evaluation with the proposed baseline model conﬁrm this hypothesis.

5.1 Hypothesis

The proposed BigIssue benchmark contains two variants: (1) synthetic rewrites of real code sam-
pled from the a strong generative model, (2) realistic rewrites of real code based on the commits
associated with a closed issue in GitHub.

Recall, for (1) a recent large language model was recruited as sampler which, compared to prior art,
not only is of signiﬁcant size under scaling laws, but furthermore alters the causal masking such
that future tokens can be taken into account as context. We argue that these synthetic rewrites are
non-trivial to detect compared to prior art.

However, our hypothesis is that localization of real bugs is still a signiﬁcantly harder task, which
requires substantial research to be solved. While local, trivial bugs do not require context to be
localized, harder non-local bugs can often only be resolved when taking the entire ﬁle, a set of
imported ﬁles, or the entire repository into account.

Therefore, one would expect reasonable classiﬁcation performance of discriminative models on the
synthetic rewrites, while the real data poses a much harder task.

5.2 Model

To test this hypothesis, we construct a simple baseline classiﬁer. The model should (1) perform
binary classiﬁcation on a line-level granularity, (2) handle variable length sequences of up to 8, 192
tokens and 512 lines, (3) contain a reasonable amount of parameters to have sufﬁcient capacity for
solving the task.

Our architecture partitions a long input sequence into shorter sub-sequences, computes contextu-
alized vectors for each chunk using a bi-directional encoder model, combines the contextualized
vectors into 512 latent vectors with mean-pooling, and ﬁnally projects those vectors to logits for
line-level binary classiﬁcation.

7

Model

Recall↑

Precision↑

F1↑

Short

Long

Realistic

Short

Long

Realistic

Short

Long

Realistic

Random
Pooling
Pooling-Attn

49.58
91.86
97.50

50.51
91.49
97.95

50.99
61.99
52.88

2.68
17.79
27.62

4.71
7.19
21.88

0.96
2.32
2.41

5.08
27.74
43.57

5.99
13.33
35.55

1.88
6.35
4.61

Table 2: Short and Long refer to short and long synthetic datasets. Comparison of the binary classi-
ﬁcation accuracy under various baselines: (1) Random Bernoulli classiﬁer with p = 0.5, (2) Mean
pooling model, (3) Mean pooling model with self-attention between latent vectors.

Consider a sequence x = (x0, x1, . . . , xn) of input tokens with length n = 8, 192. To address the
issue (2) of large n, we partition x into m = 16 equally sized chunks ˜xi with i ∈ {0, . . . , 15}
each containing 512 tokens. To contextualize the embedding vector of the tokens, we recruit the
pre-trained bi-directional encoder f , CodeBERT [10], and compute f (˜xi) for each partition i. Then,
the contextualized partitions are concatenated ˆx = (f (˜x0), f (˜x1), . . . , f (˜xm)). To restore global
position information, we apply additive sinusoidal positional embeddings to ˆx. Mean-pooling is
applied to ˆx with a window length such that the resulting sequence of latent vectors matches the
maximum number of 512 lines. A layer of self-attention integrates the information across partitions
boundaries. A standard linear projection maps each of the line-level latent vectors to logits for binary
classiﬁcation. The resulting model is ﬁne-tuned with binary cross entropy as objective function.

The appeal of the proposed model is to leverage the representations learned by a strong backbone
model and the simplicity in handling variable length including line breaks in the input sequence.
CodeBERT [10] has demonstrated strong empirical performance on down-stream tasks so that the
learned representations should be well suited for bug localization. For simplicity, the mapping of
contextualized vectors to latent vectors allows for variable length input sequences and avoids special
treatment of new line characters. The alignment from lines of the input sequence to latent vectors
for classiﬁcation is implicitly learned by supervision.

5.3 Findings

To evaluate the hardness of the artiﬁcial and realistic BigIssue benchmark, the aforementioned model
is trained on both datasets. For synthetic perturbations, the model is trained on 96.22 GB raw code
with associated line-level binary labels. We train each model (besides Bernoulli baseline) on a single
node with 16 A100 GPUs.

Table 2 summarizes the binary classiﬁcation performance in terms of recall, precision, and F1-score
for three baseline models: (1) A random classiﬁer for which the line-level predictions are modeled
as a Bernoulli random variable per line with probability p = 0.5, (2) the aforementioned mean-
pooling based model for which the self-attention layer between latent vectors is omitted, (3) the
mean-pooling based model including self-attention between latent vectors.

For the synthetic dataset, the mean-pooling model including self-attention with an F1-score of 35.55
signiﬁcantly improves over the random Bernoulli baseline with 5.99. Self-attention to integrate in-
formation across latent vectors improves the score by nearly 22 points, which may indicate that long
context across the partitioning of 512 tokens is crucial. One may assume with further improvements
in modeling, the synthetic dataset is solvable, albeit recruiting a strong generative model to generate
synthetic perturbations.

As hypothesized, real bug detection is a much harder challenge for which synthetic perturbations
may not be a suitable proxy task. It is our hope that this ﬁnding spurs research towards the modeling
of long contexts to approach the task of real bug detection.

6 Conclusion

We propose a new benchmark to be used in assessing line-level bug localization models. The di-
versity and size of the dataset aim to provide a measure with realistic difﬁculty, encouraging larger
context BL modeling that doesn’t rely on project test suites. We also provide a synthetically gen-

8

erated benchmark and show that although the perturbations can be sophisticated and borderline
realistic, success on synthetically generated datasets does not transfer to realistic benchmarks.

We hope that our contributions inspire and push future research into realistic, long-context, NLP-
based bug localization techniques. Advances in this area would bring automatic program repair to a
state that would be useful and transformative to the modern software development process.

9

References

[1] Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. Tﬁx: Learning to ﬁx coding
errors with a text-to-text transformer. In International Conference on Machine Learning, pages
780–791. PMLR, 2021.

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language mod-
els are few-shot learners. Advances in neural information processing systems, 33:1877–1901,
2020.

[3] Efstathia Chioteli, Ioannis Batas, and Diomidis Spinellis. Does unit-tested code crash? a case
study of eclipse. In 25th Pan-Hellenic Conference on Informatics, pages 260–264, 2021.
[4] Valentin Dallmeier and Thomas Zimmermann. Extraction of bug localization benchmarks
In Proceedings of the twenty-second IEEE/ACM international conference on

from history.
Automated software engineering, pages 433–436, 2007.

[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[6] Dawn Drain, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. Deepdebug: Fix-
arXiv preprint

ing python bugs using stack traces, backtranslation, and code skeletons.
arXiv:2105.09352, 2021.

[7] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.
Glm: General language model pretraining with autoregressive blank inﬁlling. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 320–335, 2022.

[8] Thomas Durieux, Fernanda Madeiral, Matias Martinez, and Rui Abreu. Empirical review of
java program repair tools: A large-scale experiment on 2,141 bugs and 23,551 repair attempts.
In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Con-
ference and Symposium on the Foundations of Software Engineering, pages 302–313, 2019.
[9] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and
natural languages. arXiv preprint arXiv:2002.08155, 2020.

[10] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and
natural languages. arXiv preprint arXiv:2002.08155, 2020.

[11] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Incoder: A generative model for code

Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
inﬁlling and synthesis. arXiv preprint arXiv:2204.05999, 2022.

[12] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
[13] Nan Jiang, Thibaud Lutellier, and Lin Tan. Cure: Code-aware neural machine translation for
In 2021 IEEE/ACM 43rd International Conference on Software

automatic program repair.
Engineering (ICSE), pages 1161–1173. IEEE, 2021.

[14] James A Jones and Mary Jean Harrold. Empirical evaluation of the tarantula automatic fault-
localization technique. In Proceedings of the 20th IEEE/ACM international Conference on
Automated software engineering, pages 273–282, 2005.

[15] René Just, Darioush Jalali, and Michael D Ernst. Defects4j: A database of existing faults to
enable controlled testing studies for java programs. In Proceedings of the 2014 International
Symposium on Software Testing and Analysis, pages 437–440, 2014.

[16] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluat-
ing contextual embedding of source code. In International Conference on Machine Learning,
pages 5110–5121. PMLR, 2020.

[17] Rafael-Michael Karampatsis and Charles Sutton. How often do single-statement bugs occur?
In Proceedings of the 17th International Conference on Mining

the manysstubs4j dataset.
Software Repositories, pages 573–577, 2020.

10

[18] Fabian Keller, Lars Grunske, Simon Heiden, Antonio Filieri, Andre van Hoorn, and David Lo.
A critical evaluation of spectrum-based fault localization techniques on a large-scale software
system. In 2017 IEEE International Conference on Software Quality, Reliability and Security
(QRS), pages 114–125. IEEE, 2017.

[19] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and
Percy S Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information
Processing Systems, 32, 2019.

[20] Yi Li, Shaohua Wang, and Tien N Nguyen. Dlﬁx: Context-based code transformation learning
for automated program repair. In Proceedings of the ACM/IEEE 42nd International Conference
on Software Engineering, pages 602–614, 2020.

[21] Kui Liu, Anil Koyuncu, Tegawendé F Bissyandé, Dongsun Kim, Jacques Klein, and Yves
Le Traon. You cannot ﬁx what you cannot ﬁnd! an investigation of fault localization bias in
benchmarking automated program repair systems. In 2019 12th IEEE conference on software
testing, validation and veriﬁcation (ICST), pages 102–113. IEEE, 2019.

[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert
pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint

arXiv:1711.05101, 2017.

[24] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. Co-
conut: combining context-aware neural translation models using ensemble for program repair.
In Proceedings of the 29th ACM SIGSOFT international symposium on software testing and
analysis, pages 101–114, 2020.

[25] S Amirhossein Mousavi, Donya Azizi Babani, and Francesco Flammini. Obstacles in fully

automatic program repair: A survey. arXiv preprint arXiv:2011.02714, 2020.

[26] Ripon K Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, and Mukul R Prasad. Bugs. jar: a
large-scale, diverse dataset of real-world java bugs. In Proceedings of the 15th international
conference on mining software repositories, pages 10–13, 2018.

[27] Qusay Idrees Sarhan and Árpád Beszédes. A survey of challenges in spectrum-based software

fault localization. IEEE Access, 10:10618–10639, 2022.

[28] André Silva, Matias Martinez, Benjamin Danglot, Davide Ginelli, and Martin Monper-
rus. Flacoco: Fault localization for java based on industry-grade coverage. arXiv preprint
arXiv:2111.12513, 2021.

[29] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and
Denys Poshyvanyk. An empirical investigation into learning bug-ﬁxing patches in the wild via
neural machine translation. In Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering, pages 832–837, 2018.

[30] Yue Yan, Shujuan Jiang, Shenggang Zhang, and Ying Huang. Csﬂ: Fault localization on
real software bugs based on the combination of context and spectrum. In International Sympo-
sium on Dependable Software Engineering: Theories, Tools, and Applications, pages 219–238.
Springer, 2021.

[31] Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from di-
agnostic feedback. In International Conference on Machine Learning, pages 10799–10808.
PMLR, 2020.

[32] Michihiro Yasunaga and Percy Liang. Break-it-ﬁx-it: Unsupervised learning for program re-

pair. In International Conference on Machine Learning, pages 11941–11952. PMLR, 2021.

[33] Wei Yuan, Quanjun Zhang, Tieke He, Chunrong Fang, Nguyen Quoc Viet Hung, Xiaodong
Hao, and Hongzhi Yin. Circle: Continual repair across programming languages. arXiv preprint
arXiv:2205.10956, 2022.

[34] Daming Zou, Jingjing Liang, Yingfei Xiong, Michael D Ernst, and Lu Zhang. An empirical
study of fault localization families and their combinations. IEEE Transactions on Software
Engineering, 47(2):332–347, 2019.

11

A Data Description, Hosting Details, and Data Access

We publish the training, evaluation, and validation sets for the synthetic data. We also pub-
lish the realistic benchmark. These items can be accessed in a Google Cloud Storage bucket at
https://console.cloud.google.com/storage/browser/bigissue-research.

Realistic Pre-training data For the realistic Pooling and Pooling-Attention models, we created a
pre-training dataset similar to other projects. We select Java GitHub repositories with 5 stars or more,
we clone the main branch of the repository, while only downloading ﬁles under 2 megabytes. We
then ﬁlter the commits that include the words "error", "bug", "ﬁx", "issue", "mistake", "incorrect",
"fault", "defect", "ﬂaw", or "type", using standard practice in ManySStubs4J project [17] . Since
our models are designed only for single-ﬁle bug localization, we take each modiﬁed ﬁle and apply
the labeling procedure described in the paper to generate the examples and labels. We truncate ﬁles
at 8192 tokens using the CodeBERT paper [9]. In total, we get about 195 GB of data to use for
pre-training.

B Training details

We train all of our models on a single pod with 16 A100 GPUs. We optimized the model with a
linear schedule AdamW [23] optimizer, with a starting learning rate of 5e-5, and 10,000 warmup
steps. We train over 200,000 (50,000 for short synthetic dataset) steps with a batch size of 2(8). We
provide the full training code at https://github.com/salesforce/BigIssue.

Model Checkpoints We
Pooling-Attention models
https://github.com/salesforce/BigIssue.

provide
trained

the model
realistic
on

checkpoints
in

data

the

for
the GitHub

Pooling

and
repository

C Data Collection Ethical Statement

We did not collect any personal information from the GitHub API. We only collect commit informa-
tion and data inside the commits, without taking into the account the origin or the user proﬁle of the
user making the changes.

12

This figure "github_diff.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/2207.10739v1

