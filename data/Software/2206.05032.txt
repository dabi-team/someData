This paper is to appear in Proceedings of the 39th International Conference on Machine Learning, ICML 2022.

Please cite as:

@inproceedings{graph_dgmrf,

author = {Oskarsson, Joel and Sid{\'e}n, Per and Lindsten, Fredrik},
booktitle = {Proceedings of the 39th International Conference on Machine Learning},
title = {Scalable Deep {G}aussian {M}arkov Random Fields for General Graphs},
year = {2022}

}

2
2
0
2

n
u
J

0
1

]
L
M

.
t
a
t
s
[

1
v
2
3
0
5
0
.
6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Scalable Deep Gaussian Markov Random Fields for General Graphs

Joel Oskarsson 1 Per Sid´en 1 2 Fredrik Lindsten 1

Abstract

Machine learning methods on graphs have proven
useful in many applications due to their ability
to handle generally structured data. The frame-
work of Gaussian Markov Random Fields (GM-
RFs) provides a principled way to deﬁne Gaussian
models on graphs by utilizing their sparsity struc-
ture. We propose a ﬂexible GMRF model for
general graphs built on the multi-layer structure
of Deep GMRFs, originally proposed for lattice
graphs only. By designing a new type of layer we
enable the model to scale to large graphs. The
layer is constructed to allow for efﬁcient training
using variational inference and existing software
frameworks for Graph Neural Networks. For a
Gaussian likelihood, close to exact Bayesian infer-
ence is available for the latent ﬁeld. This allows
for making predictions with accompanying uncer-
tainty estimates. The usefulness of the proposed
model is veriﬁed by experiments on a number of
synthetic and real world datasets, where it com-
pares favorably to other both Bayesian and deep
learning methods.

1. Introduction

Graphs are immensely important constructs in scientiﬁc
modeling. They show up as natural representations of tech-
nological networks, such as computer and electricity net-
works, but also of biological and social networks (Stankovi´c
et al., 2020a; 2019). As massive amounts of data are col-
lected about these networks, the area of machine learning on
graphs becomes increasingly relevant. In this ﬁeld we ﬁnd
probabilistic graphical models, that allow for drawing sta-
tistically sound inferences about the quantities in the graph
(Koller & Friedman, 2009). Another, more recent, family
of graph-based models are Graph Neural Networks (GNNs)

1Division of Statistics and Machine Learning, Department
of Computer and Information Science, Link¨oping University,
Link¨oping, Sweden 2Arriver Software AB. Correspondence to:
Joel Oskarsson <joel.oskarsson@liu.se>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

Figure 1. Predictive uncertainties over a large graph. Nodes inside
the rectangles are unobserved, resulting in higher uncertainty (more
red). Values are marginal standard deviations, taken from the
experiment with wind speed data in section 4.4.

(Wu et al., 2020; Kipf & Welling, 2017). These bring
the ﬂexibility and scalability of deep learning to graph-
structured data. Unifying statistically sound methods with
deep learning is an important goal in the ﬁeld of graph-based
methods and in modern machine learning as a whole.

There is a need for methods with a strong statistical ba-
sis, but with the scalability of GNNs. While some graphs,
such as those describing molecules, are comparatively small,
massive graphs can emerge for example from social and traf-
ﬁc networks. We need efﬁcient methods that scale also to
these. In many situations it is additionally desirable not
just to produce accurate predictions, but also some measure
of the uncertainty about the predictions. An example of
this is shown in Figure 1. We are here concerned with the
node-wise regression setting, where a single graph is con-
sidered and nodes are associated with real-valued targets.
We assume the full graph structure to be known. The main
application of interest is prediction for a subset of unob-
served nodes. Such problems can for example arise when
information is missing about some individuals in a social
network or due to partial outages in technological networks.

Bayesian methods provide a principled way to obtain pre-
dictive uncertainty estimates, that properly account for the
uncertainty in latent variables. One type of Bayesian model
that utilizes the sparsity of graphs are Gaussian Markov Ran-
dom Fields (GMRFs) (Rue & Held, 2005). The Deep GMRF
(DGMRF) framework of Sid´en & Lindsten (2020) combines

Scalable Deep Gaussian Markov Random Fields for General Graphs

GMRFs with Convolutional Neural Networks (CNNs) for
the special case of image-structured data. DGMRFs can be
trained efﬁciently and keep all useful properties of GMRFs,
such as exact Bayesian inference for the latent ﬁeld. In this
paper we extend and generalize the DGMRF framework
to the general graph setting. This requires us to design a
new layer construct for DGMRFs based on local operations
over node neighborhoods. Without making any assumptions
on the graph structure we propose methods that allow the
model training to scale to massive graphs.

Our main contributions are: 1) We extend the DGMRF
framework to general graphs by designing a new type of
layer construct based on GNNs. 2) We adapt the DGMRF
training to this new setting, making use of an improved
variational distribution. 3) We propose scalable methods
for performing the log-determinant computations central to
the training. 4) We demonstrate properties of the resulting
model on synthetic data. 5) We experiment on multiple real-
world datasets, for which our model outperforms existing
methods.

2. Background

2.1. GMRFs

G

In graphical models a set of random variables are associ-
ated with the nodes of a graph (Koller & Friedman, 2009;
Bishop, 2006). GMRFs are undirected graphical models
where the nodes jointly follow a Gaussian distribution. More
be an undirected graph with N nodes
speciﬁcally, let
RN . We say that
concatenated in the random vector x
∈
is a GMRF with mean µ and precision ma-
x
µ, Q−
trix Q w.r.t. the graph
= j,
iff Qi,j (cid:54)
where n(i) is the exclusive neighborhood of node i (i /
∈
n(i)). A GMRF is thus a multivariate Gaussian with a pre-
cision matrix as sparse as the graph. Note however that the
covariance matrix can still be fully dense, enabling depen-
dencies between all nodes in the graph.

n(i),

∼ N

= 0

i
∀

⇔

∈

G

(cid:1)

(cid:0)

j

1

(cid:0)

∼ N

0, σ2I

Consider now the common situation where we observe
y = x + (cid:15), (cid:15)
. A GMRF prior on x is conju-
gate to this Gaussian likelihood. We will mainly consider
the application of GMRFs to problems where y is observed
N be a mask vector
only for some nodes. Let m
}
with ones in positions corresponding to the observed nodes,
m and Im = diag(m). The posterior for x in
ym = y
ym ∼ N
this setting is then given by x
, where
|

˜µ, ˜Q−

∈ {

0, 1

(cid:12)

(cid:1)

1

˜Q = Q +

˜µ = ˜Q−
1

1
σ2 Im
Qµ +

(cid:18)

(cid:16)

(cid:17)

(1a)

(1b)

1
σ2

ym

.

(cid:19)

While the posterior is analytically tractable, and again a
GMRF, computing the involved entities explicitly can be a

signiﬁcant computational challenge for large N .

2.2. DGMRFs

Sid´en & Lindsten (2020) note that for an afﬁne map
g: RN

RN a GMRF x can be deﬁned by

→

z = g(x) = Gx + b,

z

(0, I)

(2)

∼ N

RN

×

∈

−

G−

N is a matrix and b some offset vector. This
where G
1b and precision
results in a GMRF with mean µ =
matrix Q = G(cid:124)G. Note how the direction of the mapping
in Eq. 2 makes x implicitly deﬁned, a different setup from
other generative models mapping Gaussian noise to data.
The afﬁne map g can in turn be deﬁned as a combination of
L simpler layers as g = g(L)
g(1), adding
◦
the depth to the Deep GMRF. The value of considering the
layers separately is that they can be implemented implicitly,
using some operation that is known to be afﬁne. Thus,
multiple such operations can be chained without performing
the expensive matrix multiplications to create G.

◦ · · · ◦

g(L

1)

−

Sid´en & Lindsten (2020) consider the special case where
the entries of x are associated with pixels in an image. They
then deﬁne each g(l) as a 2-dimensional convolution with
a ﬁlter containing trainable parameters. Such a DGMRF
is a GMRF w.r.t. a lattice graph (Rue & Held, 2005), a
graph where each pixel is connected to neighboring pixels
within a window determined by the ﬁlter size. The resulting
model shares much of its structure with CNNs. This allows
for utilizing existing deep learning frameworks for efﬁcient
convolution computations, automatic differentiation, and
GPU support.

After observing some data ym inference for the latent ﬁeld
x follows from Eq. 1. To avoid inverting ˜Q, the posterior
mean ˜µ can be computed using the Conjugate Gradient
(CG) method (Hestenes & Stiefel, 1952; Shewchuk, 1994).
ym]
Often, also the posterior marginal variances Var[xi|
are of interest. To avoid computing the covariance matrix
explicitly, an alternative is to use a Monte Carlo estimate
based on a set of samples from the posterior. It is possible
to use the CG method also for efﬁciently drawing posterior
samples (Papandreou & Yuille, 2010).

3. DGMRFs on Graphs

We extend the DGMRF framework of Sid´en & Lindsten
(2020) to general graphs, removing the assumption of a
lattice graph to match the general deﬁnition of a GMRF.
To achieve this we design a new type of layer g(l) without
any assumptions on the graph structure. We then propose
a way to train this new type of DGMRF using scalable
log-determinant computations and a new, more ﬂexible,
variational distribution. An overview of our model is shown
in Figure 2.

(cid:54)
Scalable Deep Gaussian Markov Random Fields for General Graphs

Figure 2. Overview of the graph DGMRF model. The latent ﬁeld x is transformed to z through L afﬁne maps g(1), . . . , g(L). The data y
is a noisy observation of x. In h(1) we illustrate Eq. 5 for a single node i. The node itself is weighted with αldγl
(solid purple arrow).
i
Each node j in the neighborhood is weighted with βlwi,jdγl−1
(dashed orange arrows). The value at node i after the layer is the sum of
all these contributions plus the bias term bl.

i

×

G

∈

RN

be an undirected connected graph with N nodes and
Let
N .
In general we consider
adjacency matrix A
weighted graphs with Ai,j = wi,jI
, where wi,j =
wj,i > 0 is the weight of the edge between nodes i and j.
i, j. We denote the degree
For unweighted graphs wi,j = 1
∀
of node i as di =
in the unweighted
j Ai,j (=
n(i)
|
|
case) and arrange all node degrees in the degree matrix
D = diag([d1, d2, . . . , dN ](cid:124)).

(cid:80)

n(i)

∈

}

{

j

3.1. Graph layer

Generalizing the CNN-based DGMRF layers of Sid´en &
Lindsten (2020) to general graphs requires some special
considerations. It is integral to take into account the varying
node degrees in the graph. To achieve this we look to the
GNN framework and consider a linear version of a message
RN
passing neural network (Gilmer et al., 2017). Let h(l)
be the node values after layer l, with h(0) = x and h(L) =
z. An intuitive way to deﬁne the operation of g(l) would be
to sum over the node neighborhood and weight the center
node by its degree,

∈

i = bl + αldih(l
h(l)

i

1)

−

+ βl

1)

h(l
j

−

,

(3)

n(i)
(cid:88)j
∈

where αl, βl and bl are layer-speciﬁc trainable parameters.
This operation can be viewed as a parametrized version
of the graph Laplacian (Stankovi´c et al., 2020a), a central
construct in graph-based machine learning (Stankovi´c et al.,
2020b; Kipf & Welling, 2017). As a special case of Eq. 3
we also ﬁnd the commonly used Intrinsic GMRF (IGMRF)
model (Rue & Held, 2005). A limitation of Eq. 3 is however
that there are no parameter values that reduce the layer to
an identity mapping. If the model would consist of a single
layer, this would not have been an issue. However, when
stacking multiple layers in a deep architecture this inability
introduces undesirable restrictions, in the sense that the
range of attainable models is not strictly increasing as we
add more layers. To avoid this shortcoming we also consider
an alternative way to deﬁne g(l) by instead taking the mean

over the neighborhood,

i = bl + αlh(l
h(l)

i

1)

−

+ βl

1
di

h(l
j

1)

−

.

(4)

n(i)
(cid:88)j
∈

Unlike Eq. 3, this operation includes the identity mapping
as a special case.

Finally, we propose a layer structure that generalizes these
two ideas, making it possible for the model to learn which
is the better choice for the data at hand. Our proposed layer
is deﬁned by

h(l)
i = bl + αldγl

i h(l

i

1)

−

1
+ βldγl−

i

wi,jh(l
j

−

1)

,

(5)

n(i)
(cid:88)j
∈

]0, 1[ and the
where we introduce another parameter γl ∈
optional edge weights wi,j. Eq. 3 and 4 are special cases as
γl tends to its limits. We empirically verify the usefulness
of this layer construct in Appendix A.1. Note also that when
0 Eq. 5 reduces to
αl = 1, βl = 0, bl = 0 and γl →
an identity mapping. Another motivation for this speciﬁc
layer construct is that it will enable scalable methods for
computing the log-determinant of G, as will be explained in
section 3.3. We additionaly reparametrize the model so that
αl > 0 and
. This avoids degenerate, indeﬁnite
solutions (see Appendix B for details) and will enable our
most scalable method for log-determinant computations.

αl|
|

βl|
|

<

If we consider the entire vector h(l), the layer corresponds
= G(l)h(l
to h(l) = g(l)

1) + b(l) with

h(l

1)

−

−

(cid:0)

(cid:1)

G(l) = αlDγl + βlDγl−

1A

(6)

∈

RN is a vector with all ones.
and b(l) = bl1, where 1
As the operation g(l) corresponds to a layer of a GNN we
can rely on existing software libraries for the model im-
plementation. Such libraries come with a number of use-
ful properties, most importantly automatic differentiation
and GPU-acceleration (Fey & Lenssen, 2019; Grattarola &
Alippi, 2020).

Scalable Deep Gaussian Markov Random Fields for General Graphs

3.2. Variational training

The parameters of a DGMRF can be trained by maximizing
the log marginal likelihood log p(ym|
θ). This is however
often infeasible as it requires computing the determinant of
the posterior precision matrix ˜Q (Sid´en & Lindsten, 2020).
For large N one can instead resort to variational inference,
maximizing the Evidence Lower Bound (ELBO),

ELBO(θ, φ) = E

q(x

φ)[log p(ym, x
|

θ)] + H[q(x
|

φ)]
|

(7)

log p(ym|

θ)

≤

where q is a variational distribution with parameters φ and
] refers to differential entropy. For a DGMRF with a
H[
·
Gaussian likelihood the ﬁrst term of the ELBO is

E

q(x
|
1
E
2

−
+ log

θ)] =

φ)[log p(ym, x
|
g(x) +

(cid:124)

φ)
q(x
|

g(x)
(cid:20)
det(G)
|

| −

M log σ + const.

1
σ2 (ym −

(cid:124)
x)

Im(ym −

x)
(cid:21)

(8)

N
i=1 mi is the number of observed nodes.
where M =
The expectation in Eq. 8 can be estimated using a set of
samples drawn from q. As G = G(L)G(L
1) . . . G(1), the
log-(absolute)-determinant is given by

(cid:80)

−

log

det(G)

|

|

(cid:88)l=1
Computing this efﬁciently is one of the major challenges
with the general graph setting, as will be discussed further
in section 3.3.

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

The full set of model parameters θ are the trainable pa-
rameters of each layer and the noise standard deviation σ.
Maximizing the ELBO w.r.t. θ and φ can be done using
gradient-based stochastic optimization.

3.2.1. VARIATIONAL DISTRIBUTION

A natural and useful way to choose the variational distribu-
ν, SS(cid:124)). This
tion is as another Gaussian q(x
|
|
corresponds to deﬁning q by another afﬁne transformation
in the opposite direction of the DGMRF,

φ) =

(x

N

x = Sr + ν,

r

(0, I).

∼ N

(10)

Note the difference to Eq. 2 as we here parametrize the
covariance matrix instead of the precision matrix. This
parametrization additionally allows for computing gradi-
ents through the sampling process, by the use of the
reparametrization trick (Kingma & Welling, 2014).

Sid´en & Lindsten (2020) use a simple mean ﬁeld approx-
imation with a diagonal S, making all components of x

independent (Bishop, 2006). However, we propose a more
ﬂexible q by choosing

S = diag(ξ) ˜G diag(τ )

(11)

∈

RN are vectors containing positive parameters
where ξ, τ
and ˜G is deﬁned in the same way as the DGMRF layer in
Eq. 6. Including the matrix ˜G in S introduces off-diagonal
elements in the covariance matrix of q, alleviating the inde-
pendence assumption between nodes. Multiple such layers
can also be used, introducing longer dependencies between
nodes in the graph. The full set of variational parameters φ
is then ν, ξ, τ and all trainable parameters from the layer(s)
˜G. In Appendix A.2 we empirically show that DGMRFs
trained using our more ﬂexible variational distribution con-
sistently outperforms those trained using the simple mean
ﬁeld approximation.

With this choice of S the entropy term of the ELBO is

H[q(x

φ)] = log
|

= log

det

det(S)
|
|
N
˜G
(cid:16)

+

i=1
(cid:88)

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

+ const.

log ξi + log τi + const.

(12)

Re-using the DGMRF layer construct in q has the added ben-
eﬁt that the techniques we develop for the log-determinant
computation readily extend also to computing H[q(x

φ)].
|

Computing the necessary log-determinants in Eq. 9 and
12 efﬁciently is a major challenge with the general graph
setting. The CNN-based DGMRF was deﬁned on a lattice
graph, which creates a special structure in G and allows
for ﬁnding efﬁcient closed-form expressions for the log-
determinants (Sid´en & Lindsten, 2020). As we do not make
any such assumptions on the graph structure we here pro-
pose new scalable methods to compute the log-determinants.

3.3.1. EIGENVALUE METHOD

One way to compute the log-determinant is based on the
eigenvalues of the matrix. As the determinant is given by
the product of all eigenvalues,

log

det

G(l)

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:12)
(cid:12)
(cid:12)

det(Dγl )
|

= log

|
+ log

det

αlI + βlD−

1A

(13)

(cid:0)
γl log(di) + log

(cid:12)
(cid:12)

(cid:1)(cid:12)
(cid:12)

λi|
|

N

=

i=1
(cid:88)

λi}
{

N
i=1 are the eigenvalues of αlI + βlD−

1A. It
where
can be shown1 that λi = αl + βlλ(cid:48)i with λ(cid:48)i being the i:th
1For an eigenvector vi of D−1A with eigenvalue λ(cid:48)
i,
ivi ⇒ (αlI + βlD−1A)vi = (αl + βlλ(cid:48)

D−1Avi = λ(cid:48)

i)vi.

L

=

log

det

G(l)

.

(9)

3.3. Computing the log-determinant

Scalable Deep Gaussian Markov Random Fields for General Graphs

1A. Since D−

1A only depends
eigenvalue of the matrix D−
on the graph structure, it does not change during training.
N
This means that we can compute the eigenvalues
i=1 as
a pre-processing step and store these for computing Eq. 13
during training. Note that this is enabled by the speciﬁc
layer construct that we propose. By choosing our layer as
described in section 3.1 we manage to shift the main com-
putational burden of log-determinant computations from the
iterative training to a pre-processing step that only has to be
performed once.

λ(cid:48)i}
{

This method is feasible for moderate N , but computing all
eigenvalues in pre-processing can be unreasonably slow for
very large graphs. To guarantee that the training scales to
massive graphs we propose an alternative method for the
log-determinant computation.

3.3.2. POWER SERIES METHOD

Another way to rewrite the log-determinant is

log

det

G(l)

=N log(αl) +

γl log(di)

N

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:12)
(cid:12)
(cid:12)

(14)

i=1
(cid:88)
I +

βl
αl

˜A

(cid:18)

+ log

det
(cid:12)
(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
where we deﬁne ˜A = D−
2 . For computing the last
2 AD−
term in Eq. 14 we follow the approach of Behrmann et al.
(2019), where an approximation of the log-determinant is
constructed based on a power series,

1

log

det

I +

˜A

=

∞

βl
αl

1
k

−

βl
αl (cid:19)

−

(cid:18)

k

Tr

˜Ak

.

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:17)

˜Ak

(cid:88)k=1
(15)
During training of the model we truncate the series at some
large value k = K, resulting in an approximation. We
note that Tr
only depends on the graph structure,
not the model parameters. This means that the traces for
(cid:17)
can be computed as a pre-processing step.
k
In Appendix C we give further details on this pre-processing
step, show that the series converges and bound the truncation
error.

(cid:16)
1, . . . , K

∈ {

}

3.4. Scalability

The graph DGMRF is deﬁned using the graph
, but it can
be noted that the model is in fact not a GMRF w.r.t. this
graph. Instead, an L-layer DGMRF is a GMRF w.r.t. the
2L, which is deﬁned by connecting all nodes
2L-hop graph
that are at a distance
(ignoring
edge weights). Equivalently, the resulting precision matrix

2L from each other in

≤

G

G

G

Q shares its zero-pattern with (A + I)2L.2

O

We consider now how the model training scales w.r.t. the
number of layers L and the graph structure. With the pro-
posed methods for the log-determinants these can be com-
(N L). The computational complexity is instead
puted in
dominated by the application of G, as this operation scales
with the number of edges in the graph. To simplify the
i, resulting in a graph with
discussion, assume di = d
∀
(N d) edges. Applying L DGMRF layers to this graph
(N dL). This can be compared to explicitly creat-
2L in a pre-processing step, resulting in a graph with
edges. The layer structure is thus central to the
O
scalability of the model. Using multiple layers additionally
adds ﬂexibility in the form of more parameters and longer
node dependencies in the mapping g.

O
is then
ing

O
G
N d2L

(cid:0)

(cid:1)

4. Experiments

The graph DGMRF was implemented3 using PyTorch and
the PyTorch Geometric GNN library (Fey & Lenssen, 2019).
We evaluate the proposed model on a number of synthetic
and real world datasets. All DGMRF training is repeated for
5 different random seeds and the DGMRF results reported
as the mean across these. Details on the datasets and setup
of each experiment are described in Appendix E.

We consider graphs where only a fraction of the nodes are
observed. The model parameters are trained by maximizing
the ELBO using the observed nodes. We generally treat
50% of nodes as unobserved, chosen uniformly at random.
Unless stated otherwise we use the eigenvalue method for
the log-determinant computations, as this is feasible for the
majority of the considered graphs. While the power series
method could also be used in these cases, this would incur
unnecessary approximations. The trained model is then used
to perform inference according to Eq. 1, utilizing the CG
method. We do not compute the full posterior covariance
matrix, but instead draw 100 posterior samples to estimate
the marginal variance of each node. Based on the posterior
mean and marginal variances different evaluation metrics
can then be computed for the unobserved nodes. Using
a consumer-grade GPU the full pre-processing, training,
and inference procedure for a single DGMRF model takes
around 10 minutes for smaller graphs and up to one hour
for the largest and most dense graphs.

(cid:124)

(cid:124)

2Note that G(l) has zeros in the same positions as I + A. Since
G(L) . . . G(1) it has the same zero-pattern as
. . . G(L)
Q = G(1)
(cid:1)Ak. The value of [Ak]i,j is the number
(cid:0)2L
(I + A)2L = (cid:80)2L
k
of graph walks between nodes i and j of length k. So if i and j
are at a distance k(cid:48) ≤ 2L in G, then [Ak(cid:48)
]i,j (cid:54)= 0 and Qi,j (cid:54)= 0.
This implies that Q deﬁnes a GMRF w.r.t. the 2L-hop graph.

k=0

3Our code is available at https://github.com/

joeloskarsson/graph-dgmrf.

Scalable Deep Gaussian Markov Random Fields for General Graphs

Figure 3. MAE between posterior means (left) and between pos-
terior standard deviations (right). The difference is computed
between the true posterior of x and the posterior of each trained
model. Rows represents synthetic data sampled from DGMRFs
with different number of layers. Columns represent DGMRFs with
different number of layers, trained on the synthetic data. For easy
comparison each row has been divided by the error of the model
with L matching the true DGMRF, which makes the diagonal
equal to 1.

4.1. Synthetic data

We start by considering synthetic data sampled from a model
in the same class as our DGMRF. Based on a random graph
with 3000 nodes we deﬁne DGMRF models with 1–4 layers
and some ﬁxed, known parameters (details in Appendix E.1).
A sample of x is then drawn from each such DGMRF and
Gaussian noise added to construct y. For this experiments
we treat 25% of the nodes as unobserved.

Using the synthetic data we train 1–5 layer DGMRF mod-
els. As the graph is reasonably small and we know the
exact parameters of the true model we can here compute
the true posterior. We evaluate the trained models by com-
paring their posteriors to the known true posterior for the
unobserved nodes. The metrics used are Mean Absolute
Error (MAE) between the posterior means and MAE be-
tween the posterior marginal standard deviations. Results
are presented in Figure 3.

The pattern in Figure 3 illustrates how a model speciﬁed
with too few layers fails to learn the true posterior, as rep-
resented by the high error in the bottom left of the ﬁgure.
Without enough layers the precision matrix becomes too
sparse, resulting in the model not capturing complex de-
pendencies between nodes in the graph. On the other hand
we note how the error generally does not increase much
when unnecessarily many layers are used in the model. This
indicates that deeper architectures do not impose unwanted
restrictions on the model class.

We consider an additional synthetic graph datasets sam-
pled from a GMRF that lies outside the model class of our
DGMRF. The precision matrix of this GMRF is speciﬁed by
taking a random linear combination of some other simple

Figure 4. RMSE of DGMRF models with 1–5 layers evaluated on
the Mix synthetic dataset. The small shaded area corresponds to
95% conﬁdence interval across 5 random seeds. The RMSE for
the true posterior mean is also shown.

precision matrices (details in Appendix E.1). We refer to
this as the Mix dataset. Here we compare the Root Mean
Squared Error (RMSE) of the posterior mean to the value
of y for the unobserved nodes. The results for 1–5 layer
DGMRFs are shown in Figure 4. It is clear from the plot
that DGMRFs with more layers perform better. More layers
results in a more ﬂexible model and a more dense precision
matrix, better matching the data at hand. An additional ex-
periment for synthetic data constructed from another GMRF
can be found in Appendix A.3.

4.2. Wikipedia graphs

Next, we conduct experiments with three graphs based on
Wikipedia pages related to different animals (Rozemberczki
et al., 2021). The three graphs vary in sparsity and number
of nodes. In these graphs the nodes represent Wikipedia
pages and edges mutual links between them. The target
attribute y is the log average monthly trafﬁc of each page.

As before we evaluate model predictions by RMSE. To also
take the predictive uncertainty into account we addition-
ally consider the probabilistic metric Continuous Ranked
Probability Score (CRPS) (Gneiting & Raftery, 2007). Let
Fi be the cumulative distribution function of the predictive
distribution for node i. The CRPS is then deﬁned as

CRPS(Fi, yi) =

−

∞

Fi(t)

I
{

−

t
≥

yi}

2

dt

(16)

(cid:90)
−∞(cid:0)

(cid:1)

which can be interpreted as the difference between Fi and a
unit step located at yi. The integral in Eq. 16 has a closed
form solution when the predictive distribution is Gaussian.
We use the negative CRPS, meaning that lower values are
better. The ﬁnal metric is then computed as the mean nega-
tive CRPS over the unobserved nodes. For baseline models
that do not directly yield a predictive distribution we instead
create a crude uncertainty estimate by training an ensemble
of 10 models. The mean and standard deviation across the

12345ModelL1234TrueLMAEµ12345ModelL1234MAEσ1212345LayersL1.551.601.65RMSE×10−2DGMRFTrueposteriorScalable Deep Gaussian Markov Random Fields for General Graphs

Table 1. Results on the three Wikipedia datasets. The best value of each metric is marked with bold and second best underlined. The last
column denotes the number of trainable parameters in each model, not including variational parameters.

CHAMELEON

SQUIRREL

CROCODILE

MODEL

RMSE

CRPS

RMSE

CRPS

RMSE

CRPS

# PARAMETERS

GRAPH GP
LP
IGMRF
DGP (×10)

DGMRF, L = 1
DGMRF, L = 3
DGMRF, L = 5

2.115
2.102
1.805
1.613

1.589
1.511
1.465

1.216
-
1.030
1.067

0.883
0.835
0.804

1.772
1.930
1.718
1.400

1.643
1.493
1.374

1.001
-
0.986
0.984

0.924
0.837
0.765

2.169
2.014
1.526
1.308

1.311
1.228
1.169

1.251
-
0.939
0.786

0.704
0.652
0.614

4
-
2
≈ 104

5
13
21

ensemble is then used for computing the metrics. We denote
such ensembles with (

10) in ﬁgures and tables.

×

In addition to the DGMRF we consider a number of baseline
models. We use a regression version of Label Propagation
(LP) (Zhu et al., 2003; Jia & Benson, 2020), a ﬁrst order
IGMRF (Rue & Held, 2005) and the Graph Gaussian Pro-
cess (GP) model of Borovitskiy et al. (2021). It would be of
interest to include a GNN baseline, but since we do not use
node features this requires special consideration. Inspired
by the CNN-based Deep Image Prior model of Ulyanov
et al. (2018) we use a GNN with random noise as input.
We denote this baseline as Deep Graph Prior (DGP). De-
tailed descriptions of the different baselines can be found in
Appendix D.

The results of DGMRFs and baseline models on the
Wikipedia graphs are presented in Table 1 (standard de-
viations in Appendix A.4). In terms of RMSE the 1-layer
DGMRF performs similar or better to the baseline mod-
els. Adding more layers also seems beneﬁcial, as the best
performance is reached at L = 5. It can be noted that the
diameters of the considered graphs are
10, meaning that
a 5-layer DGMRF corresponds to an almost fully dense
precision matrix. Note that our framework allows us to
deﬁne such a model without ever working with that large
matrix explicitly. With close to exact posterior inference the
DGMRF model also gives principled uncertainty estimates,
which is reﬂected in the favorable CRPS values.

≈

4.3. California housing data

We here experiment on the classical California housing
dataset (Kelley Pace & Barry, 1997). This dataset contains
median house values of 20 640 housing blocks located in
California. Based on their spatial coordinates we create a
sparse graph by Delaunay triangulation (De Loera et al.,
2010). We also weight the graph by the inverse distances
between nodes, here showcasing the ability of our model
to work with a weighted adjacency matrix. The California
housing dataset additionally contains socio-economic fea-

Table 2. Results on the California housing dataset. All metrics are
listed multiplied by a factor 100. When using no features only the
graph or spatial coordinates are utilized.

NO FEATURES

FEATURES

MODEL

RMSE

CRPS

RMSE

CRPS

BAYES LR
MLP (×10)
GCN (×10)
GAT (×10)
SVGP
GRAPH GP
LP
IGMRF

DGMRF, L = 1
DGMRF, L = 2
DGMRF, L = 3

13.319
11.086
8.760
9.166
10.172
11.202
6.989
6.989

6.909
6.853
6.853

7.521
7.915
5.683
6.049
5.689
6.350
-
3.841

3.665
3.651
3.656

8.872
7.094
6.837
6.788
7.287

4.834
4.525
4.273
4.348
3.930

-
-
-

5.894
5.810
5.804

3.078
3.041
3.039

tures associated with the housing blocks. To handle these
node features in the DGMRF we use an auxiliary linear
model, similarly to Sid´en & Lindsten (2020). The linear
model is also given a Bayesian treatment with its own varia-
tional distribution trained jointly with the rest of the model.

For this experiment additional deep learning baselines are
used. We use a standard Multilayer Perceptron (MLP) and
two GNNs, Graph Convolutional Network (GCN) (Kipf
& Welling, 2017) and Graph Attention Network (GAT)
(Veliˇckovi´c et al., 2018). A Bayesian Linear Regression
(LR) baseline is also included. To compare the graph-based
methods to a direct spatial approach we also consider a
Sparse Variational Gaussian Process (SVGP) model (Hens-
man et al., 2015).

In Table 2 we report results with and without using the
node features. The results demonstrate how utilizing both
the graph structure and node features allows our DGMRF
to accurately model the data. Even without node features
the probabilistic predictions of our DGMRF result in the
lowest CRPS. While the main point of this experiment is
to compare purely graph-based methods, it also serves as

Scalable Deep Gaussian Markov Random Fields for General Graphs

Table 3. Results on the wind speed dataset. In Spatial Mask three
rectangular areas were masked out, with nodes inside these treated
as unobserved. For Random Mask half of the nodes were treated
as unobserved, chosen uniformly at random.

SPATIAL MASK RANDOM MASK

MODEL

RMSE

CRPS

RMSE

CRPS

BAYES LR
MLP (×10)
GCN (×10)
GAT (×10)
LP
IGMRF

DGMRF, L = 1
DGMRF, L = 2
DGMRF, L = 3

1.167
1.176
1.116
1.082
0.979
0.980

1.246
1.083
1.075

0.677
0.815
0.719
0.650
-
0.610

0.700
0.616
0.612

0.948
0.653
0.606
0.622
0.311
0.311

0.272
0.273
0.272

0.524
0.396
0.357
0.362
-
0.160

0.123
0.123
0.123

an example of how our DGMRF can be applied to spatial
data. It should however be noted that a graph based on
spatial positions is an approximation. If only the graph is
used by the model, some spatial information is being lost.
Methods working directly with the continuous coordinates
can in some cases be more suitable for this type of problem,
but often come with computational challenges (Hensman
et al., 2015).

4.4. Wind speed data

To test the scalability of our model we experiment on a large
dataset containing wind speeds. The dataset contains the
average wind speed for 126 652 sites around the US, based
on a state-of-the-art meteorological simulation (Draxl et al.,
2015). Similar pre-processing and experiment setup is used
as for the California housing data.

Due to the large size of the graph we here use the power
series method for log-determinant computations. Both a
random and spatial observation mask were considered. The
spatial mask can be seen in Figure 1. Results for the wind
speed dataset are presented in Table 3. The experiment
showcases the ability of our model to scale to large graphs
without sacriﬁcing predictive performance.

4.5. Impact of percentage of observed nodes

In the next experiment we investigate how the model per-
formance is impacted by the percentage of observed nodes.
We repeat the experiment on the Crocodile Wikipedia graph
with 5%–95% of the nodes observed and a 3-layer DGMRF.
The RMSE of the DGMRF and a number of baseline models
are presented in Figure 5. Similar plots for CRPS and for the
synthetic Mix dataset can be found in Appendix A.6. From
Figure 5 we note how the the DGMRF performs well at
20% observed nodes and improves somewhat as even more
are observed. When only 5% of the nodes are observed the

Figure 5. RMSE on the Crocodile Wikipedia graph for different
percentages of observed nodes. The shaded area corresponds to
95% conﬁdence interval, evaluated across different random seeds.

resulting RMSE is substantially higher and the model seems
to become more sensitive to the random seed used.

5. Related Work

Using sparse linear algebra computations, including the
sparse Cholesky decomposition, GMRFs can be scaled to
large graphs (Rue & Held, 2005, Section 2.3–2.4). These
methods do however rely on Q being highly sparse, which
is not always the case for multi-layer DGMRFs. Our GNN-
based framework gives us additional beneﬁts in the form of
automatic differentiation and GPU-acceleration. Another
approach to automatic differentiation for GMRFs is that of
Durrande et al. (2019), where the sparse Cholesky operator
is endowed with a method for reverse-mode differentiation.

GMRFs have close connections to GPs and multiple at-
tempts have been made to also deﬁne GPs on graphs. Venki-
taraman et al. (2020) deﬁne a version of GPs on compara-
bly small graphs, but as with regular GPs scaling to large
datasets is a challenge. Borovitskiy et al. (2021) propose
a more scalable graph GP by making use of variational in-
ference and stochastic optimization. Their method does
however require computing eigenvalues of the graph Lapla-
cian. For our DGMRF this can be circumvented by the
power series method discussed in section 3.3.2, but this
technique does not readily extend to the graph GP case. In
our experiments this graph GP also seems to suffer in pre-
dictive performance due to the approximations necessary to
make it scale to very large graphs. Li et al. (2020) deﬁne
deep GPs (Salimbeni & Deisenroth, 2017) on graphs. They
do however consider a different setting, where multiple sig-
nals are available for the same graph. GPs on graphs can
also be deﬁned as solutions to stochastic partial differential
equations. This approach is used by Nikitin et al. (2022)

95806040205Observednodes(%)1.21.41.61.82.02.2RMSEDGMRFDGP(×10)GraphGPIGMRFScalable Deep Gaussian Markov Random Fields for General Graphs

to construct spatio-temporal graph GPs and by Bolin et al.
(2022) to deﬁne a GP on both nodes and edges of the graph.
Also noteworthy is that GMRFs can be viewed as approx-
imations to GPs in both the euclidean and graph settings
(Lindgren et al., 2011; Borovitskiy et al., 2021).

GNNs are more scalable graph models based on deep learn-
ing (Wu et al., 2020; Kipf & Welling, 2017). While GNNs
have proven to give accurate predictions in many settings
they lack the probabilistic interpretation of GMRFs and GPs.
Our model utilizes the computational beneﬁts of GNNs
without sacriﬁcing the rigorous statistical basis of GMRFs.
Attempts have also been made to combine GNNs with proba-
bilistic approaches, for example by modeling the correlation
of node residuals as jointly Gaussian (Jia & Benson, 2020).

One interpretation of DGMRFs is as a linear version of a nor-
malizing ﬂow model (Sid´en & Lindsten, 2020; Dinh et al.,
2017). Normalizing ﬂows have been deﬁned for graphs, but
mainly for generating the graph structure (Liu et al., 2019;
Kaushalya et al., 2019). Sid´en & Lindsten (2020) deﬁne a
non-linear version of DGMRF (a normalizing ﬂow) in the
CNN-setting, but demonstrate no convincing results on its
usefulness. Preliminary experiments on a non-linear version
of our graph DGMRF indicate similar poor results.

6. Conclusions

In this paper we have presented DGMRFs for general graphs.
Through the computational framework of GNNs and scal-
able log-determinant computations the model can be applied
to large graphs. In experiments the proposed model com-
pares favorably to other both Bayesian and deep learning
methods. We have only considered Gaussian likelihoods,
but extensions to non-conjugate settings could be of interest
(for example node classiﬁcation). Other directions for future
work involve better ways to make use of node features and
methods for jointly learning also the graph structure.

Acknowledgements

This research is ﬁnancially supported by the Swedish Re-
search Council via the project Handling Uncertainty in Ma-
chine Learning Systems (contract number: 2020-04122),
the Wallenberg AI, Autonomous Systems and Software Pro-
gram (WASP) funded by the Knut and Alice Wallenberg
Foundation, and the Excellence Center at Link¨oping–Lund
in Information Technology (ELLIIT).

References

Behrmann, J., Grathwohl, W., Chen, R. T. Q., Duvenaud,
Invertible residual networks.
D., and Jacobsen, J.-H.
In Proceedings of the 36th International Conference on
Machine Learning, pp. 573–582, 09–15 Jun 2019.

Bishop, C. M. Pattern recognition and machine learning.
Information science and statistics. Springer, 2006. ISBN
978-0-387-31073-2.

Bolin, D., Simas, A. B., and Wallin, J. Gaussian
Whittle-Mat´ern ﬁelds on metric graphs. arXiv preprint
arXiv:2205.06163, 2022.

Borovitskiy, V., Azangulov, I., Terenin, A., Mostowsky,
P., Deisenroth, M., and Durrande, N. Mat´ern Gaussian
processes on graphs. In Proceedings of The 24th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics,
pp. 2593–2601, 2021.

De Loera, J. A., Rambau, J., and Santos, F. Triangulations,
volume 25 of Algorithms and Computation in Mathemat-
ics. Springer, 2010. ISBN 978-3-642-12970-4.

Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-
mation using real nvp. In International Conference on
Learning Representations (ICLR), 2017.

Draxl, C., Clifton, A., Hodge, B.-M., and McCaa, J. The
wind integration national dataset (WIND) toolkit. Applied
Energy, 151:355–366, 2015.

Durrande, N., Adam, V., Bordeaux, L., Eleftheriadis, S.,
and Hensman, J. Banded matrix operators for gaussian
markov models in the automatic differentiation era. In
Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics, pp. 2780–2789, 2019.

Fey, M. and Lenssen, J. E. Fast graph representation learning
with PyTorch Geometric. In ICLR Workshop on Repre-
sentation Learning on Graphs and Manifolds, 2019.

Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and
Dahl, G. E. Neural message passing for quantum chem-
istry. In Proceedings of the 34th International Conference
on Machine Learning, pp. 1263–1272, 2017.

Gneiting, T. and Raftery, A. E. Strictly proper scoring
rules, prediction, and estimation. Journal of the American
Statistical Association, 102(477):359–378, 2007.

Grattarola, D. and Alippi, C. Graph neural networks in
tensorﬂow and keras with spektral. In ICLR Workshop
on Graph Representation Learning and Beyond (GRL+),
2020.

Hensman, J., Matthews, A. G. d. G., and Ghahramani, Z.
Scalable variational gaussian process classiﬁcation. In
Proceedings of the 18th International Conference on Ar-
tiﬁcial Intelligence and Statistics, pp. 351–360, 2015.

Hestenes, M. R. and Stiefel, E. Methods of conjugate gradi-
ents for solving linear systems. Journal of Research of
the National Bureau of Standards, 49(6), 1952.

Scalable Deep Gaussian Markov Random Fields for General Graphs

Jia, J. and Benson, A. R. Residual correlation in graph
neural network regression. Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Dis-
covery & Data Mining, pp. 588–598, 2020.

Salimbeni, H. and Deisenroth, M. Doubly stochastic varia-
tional inference for deep Gaussian processes. In Advances
in Neural Information Processing Systems, volume 30,
2017.

Kaushalya, M., Katushiko, I., Kosuke, N., and Motoki, A.
Graphnvp: An invertible ﬂow model for generating molec-
ular graphs. arXiv preprint arXiv:1905.11600, 2019.

Kelley Pace, R. and Barry, R. Sparse spatial autoregressions.
Statistics & Probability Letters, 33(3):291–297, 1997.

Kingma, D. P. and Welling, M. Auto-encoding variational
Bayes. In International Conference on Learning Repre-
sentations (ICLR), 2014.

Kipf, T. N. and Welling, M. Semi-supervised classiﬁca-
tion with graph convolutional networks. In International
Conference on Learning Representations (ICLR), 2017.

Koller, D. and Friedman, N. Probabilistic graphical models:
principles and techniques. Adaptive computation and
machine learning. MIT Press, 2009. ISBN 978-0-262-
01319-2.

Li, N., Li, W., Sun, J., Gao, Y., Jiang, Y., and Xia, S. Stochas-
tic deep Gaussian processes over graphs. In Advances
in Neural Information Processing Systems, volume 33,
2020.

Lindgren, F., Rue, H., and Lindstr¨om, J. An explicit link
between Gaussian ﬁelds and Gaussian Markov random
ﬁelds:
the stochastic partial differential equation ap-
proach. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 73(4):423–498, 2011.

Liu, J., Kumar, A., Ba, J., Kiros, J., and Swersky, K. Graph
normalizing ﬂows. In Advances in Neural Information
Processing Systems, volume 32, 2019.

Nikitin, A. V., John, S., Solin, A., and Kaski, S. Non-
separable spatio-temporal graph kernels via SPDEs. In
International Conference on Artiﬁcial Intelligence and
Statistics, pp. 10640–10660. PMLR, 2022.

Papandreou, G. and Yuille, A. L. Gaussian sampling by
local perturbations. In Advances in Neural Information
Processing Systems, volume 23, pp. 1858–1866, 2010.

Rozemberczki, B., Allen, C., and Sarkar, R. Multi-scale at-
tributed node embedding. Journal of Complex Networks,
9(2), 2021.

Rue, H. and Held, L. Gaussian Markov random ﬁelds:
theory and applications. Number 104 in Monographs on
statistics and applied probability. Chapman & Hall/CRC,
2005. ISBN 978-1-58488-432-3.

Shewchuk, J. R. An introduction to the conjugate gradient
method without the agonizing pain. Technical report,
School of Computer Science, Carnegie Mellon University,
1994.

Sid´en, P. and Lindsten, F. Deep Gaussian Markov random
ﬁelds. In Proceedings of the 37th International Confer-
ence on Machine Learning, pp. 8916–8926, 2020.

Stankovi´c, L., Mandic, D. P., Dakovi´c, M., Kisil, I., Sejdi´c,
E., and Constantinides, A. G. Understanding the basis of
graph signal processing via an intuitive example-driven
approach. IEEE Signal Processing Magazine, 36(6):133–
145, 2019.

Stankovi´c, L., Mandic, D., Dakovi´c, M., Brajovi´c, M.,
Scalzo, B., Li, S., and Constantinides, A. G. Data an-
alytics on graphs part I: Graphs and spectra on graphs.
Foundations and Trends in Machine Learning, 13(1):1–
157, 2020a.

Stankovi´c, L., Mandic, D., Dakovi´c, M., Brajovi´c, M.,
Scalzo, B., Li, S., and Constantinides, A. G. Data analyt-
ics on graphs part III: Machine learning on graphs, from
graph topology to applications. Foundations and Trends
in Machine Learning, 13(4):332–530, 2020b.

Ulyanov, D., Vedaldi, A., and Lempitsky, V. Deep image
prior. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018.

Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A.,
Li`o, P., and Bengio, Y. Graph attention networks. In
International Conference on Learning Representations
(ICLR), 2018.

Venkitaraman, A., Chatterjee, S., and Handel, P. Gaussian
processes over graphs. In ICASSP 2020 - 2020 IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 5640–5644, 2020.

Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S.
A comprehensive survey on graph neural networks. IEEE
Transactions on Neural Networks and Learning Systems,
pp. 1–21, 2020.

Zhu, X., Ghahramani, Z., and Lafferty, J. D.

Semi-
supervised learning using Gaussian ﬁelds and harmonic
functions. In Proceedings of the 20th International Con-
ference on Machine Learning, pp. 912–919, 2003.

Scalable Deep Gaussian Markov Random Fields for General Graphs

A. Additional Results

In this appendix we present additional experiments that verify properties of our model. We also present some supplementary
results from the different experiments in section 4.

A.1. Trainable γl

In section 3.1 we propose a new DGMRF layer that generalizes Eq. 3 and 4 by introducing the γl-parameter. To verify the
usefulness of this parametrization, we perform a number of experiments with ﬁxed γl = 1 (Eq. 3), γl = 0 (Eq. 4), and
with γl being trainable (Eq. 5, our proposed layer). We train and evaluate DGMRFs with 1, 3 and 5 layers on multiple
datasets. To decouple the parametrization of the DGMRF layers from any layers ˜G used in our variational distribution we
here only use a mean ﬁeld approximation. Apart from this we use the same experiment setups as described in section 4 and
Appendix E for each dataset.

Results from the experiments are reported in Table 4. It is clear that DGMRFs with a trainable γl-parameter generally
models the data more accurately. In practice it can also be observed that γl often takes values that are not close to neither 0
nor 1, further indicating the usefulness of the added ﬂexibility. Comparing the models with ﬁxed γl, the version with γl = 0
generally performs better, in particular in multi-layer cases. We have observed that with γl = 1 the training easily becomes
unstable. For the Chameleon data the training of the multi-layer models does not even converge, so these results are left out.
In Table 4 it is also noteworthy that the DGMRF with L = 3 and γl = 1 corresponds to the true model for the Synthetic
DGMRF data. Despite this the DGMRF with trainable γl ends up closer to the true posterior. We believe the reason for this
is the problematic training of the γl = 1 models, preventing the trained DGMRF from reaching parameter values close to the
true ones. On the synthetic DGMRF data the γl = 1 model especially seems to suffer from the independence assumptions in
the mean ﬁeld approximation. If trained using our more ﬂexible variational distribution the error of this model is reduced
substantially4.

A.2. Variational distribution

To evaluate our improved variational distribution we conduct additional experiments on a few of our considered datasets.
We train DGMRF models with 1, 3 and 5 layers on the synthetic 3-layer DGMRF data, the Chameleon Wikipedia data and
the California housing data without features. We train each DGMRF model using three different variational distributions:

• Mean Field (MF) approximation (used by Sid´en & Lindsten (2020)),
• ˜G corresponding to 1 DGMRF layer (ours, see Eq. 11),
• ˜G corresponding to 2 DGMRF layers.

We then use the standard experiment setup for each dataset. As we have seen no use for more than 3-layer models on the
California housing dataset we there exclude the 5-layer DGMRF.

Table 5 and 6 list results for the different variational distributions. Our more ﬂexible distribution shows a clear advantage
over the simple mean ﬁeld approximation. We also report the converged ELBO values of each model. The addition of
DGMRF layers also in the variational distribution results in higher ELBOs overall. This indicates that the learned variational
distribution is closer to the true posterior in terms of Kullback–Leibler divergence. The higher ELBO can also be explained
by a higher log marginal likelihood for the learned model parameters, which would be in line with the improved metric
values. While the use of 1 layer in the variational distribution is an improvement over the mean ﬁeld approximation, there is
no pronounced beneﬁt of 2 layers. This motivates our choice to use 1 layer in remaining experiments.

A.3. Synthetic data

In section 4.1 DGMRF models were trained and evaluated on the synthetic Mix dataset. We present additional results from
this experiment in Figure 6, where we evaluate the CRPS of the model predictions.

We consider another synthetic dataset to evaluate how well multi-layer DGMRFs can model data from a GMRF with a more
3. We then draw a sample from a 1-layer
dense precision matrix. Based on a random graph

we create the 3-hop graph

G

G

4We re-trained the 3-layer model on the synthetic DGMRF data using our variational distribution including 2 DGMRF layers (with
trainable γl). The corresponding MAEµ is then 0.266 ± 0.016 for γl = 1, 0.395 ± 0.000 for γl = 0 and 0.149 ± 0.003 for γl trainable.

Scalable Deep Gaussian Markov Random Fields for General Graphs

Table 4. Results comparing a ﬁxed and trainable γl-parameter. DGMRF models with L ∈ {1, 3, 5} layers have been considered. The
datasets used are: synthetic data sampled from a 3-layer DGMRF (earlier described in section 4.1), the synthetic Mix dataset and the
Chameleon Wikipedia data. For the ﬁrst of these we compare the model to the true posterior using MAE of posterior mean and marginal
standard deviations. All metrics are reported as mean±standard deviation across 5 random seeds. For readability some metrics are listed
multiplied by a factor 100.

SYNTH. DGMRF (L = 3)

MIX

CHAMELEON

DGMRF

γl

MAEµ (×100) MAEσ (×100)

RMSE (×100)

CRPS (×100)

RMSE

CRPS

L = 1

L = 3

L = 5

γl = 1
γl = 0
TRAINABLE

γl = 1
γl = 0
TRAINABLE

γl = 1
γl = 0
TRAINABLE

0.335±0.000
0.441±0.000
0.335±0.000

0.370±0.008
0.404±0.000
0.163±0.002

0.935±0.056
0.414±0.000
0.184±0.005

0.355±0.004
0.440±0.005
0.355±0.006

0.446±0.024
0.372±0.010
0.240±0.010

0.904±0.034
0.384±0.005
0.229±0.002

1.752±0.001
1.689±0.000
1.687±0.000

1.945±0.002
1.587±0.000
1.584±0.001

5.188±0.698
1.581±0.000
1.578±0.001

0.988±0.000
0.955±0.000
0.954±0.000

1.050±0.001
0.896±0.000
0.894±0.000

1.753±0.035
0.892±0.000
0.890±0.001

1.841±0.001
1.617±0.001
1.617±0.001

-
1.634±0.001
1.527±0.002

-
1.584±0.001
1.489±0.063

1.065±0.001
0.899±0.001
0.899±0.001

-
0.904±0.001
0.845±0.001

-
0.861±0.000
0.813±0.039

Table 5. Results comparing models trained using different variational distributions. DGMRF models with L ∈ {1, 3, 5} layers have been
trained on synthetic data sampled from a 3-layer DGMRF. We compare the models to the true posterior using MAE of posterior mean and
marginal standard deviations. All values are reported as mean±standard deviation across 5 random seeds. Note that higher ELBO values
are better.

SYNTH. DGMRF (L = 3)

DGMRF VI LAYERS MAEµ (×100) MAEσ (×100)

ELBO

L = 1

L = 3

L = 5

0 (MF)
1
2

0 (MF)
1
2

0 (MF)
1
2

0.335±0.000
0.323±0.001
0.323±0.000

0.163±0.002
0.151±0.005
0.149±0.003

0.184±0.005
0.151±0.007
0.153±0.005

0.355±0.006
0.366±0.007
0.364±0.004

0.240±0.010
0.221±0.007
0.226±0.013

0.229±0.002
0.206±0.006
0.212±0.005

2.056±0.000
2.080±0.000
2.080±0.000

2.107±0.000
2.129±0.000
2.129±0.000

2.115±0.000
2.132±0.000
2.133±0.000

Table 6. Results comparing models trained using different variational distributions. DGMRF models with L ∈ {1, 3, 5} layers have been
trained on the Chameleon Wikipedia data and the California housing data (without features). All values are reported as mean±standard
deviation across 5 random seeds. Note that higher ELBO values are better.

DGMRF VI LAYERS

RMSE

CRPS

ELBO

RMSE (×100)

CRPS (×100)

ELBO

CHAMELEON

CALIFORNIA HOUSING, NO FEATURES

L = 1

L = 3

L = 5

0 (MF)
1
2

0 (MF)
1
2

0 (MF)
1
2

1.617±0.001
1.589±0.000
1.589±0.001

1.527±0.002
1.511±0.006
1.515±0.014

1.489±0.063
1.465±0.033
1.453±0.020

0.899±0.001
0.883±0.000
0.883±0.000

0.845±0.001
0.835±0.003
0.837±0.007

0.813±0.039
0.804±0.023
0.795±0.015

-1.026±0.001
-1.000±0.000
-0.999±0.000

-1.002±0.001
-0.981±0.008
-0.986±0.010

-0.987±0.044
-0.963±0.011
-0.961±0.009

7.074±0.001
6.909±0.000
6.897±0.002

6.914±0.001
6.853±0.000
6.853±0.001

3.743±0.001
3.665±0.000
3.661±0.001

3.702±0.002
3.656±0.001
3.654±0.001

0.438±0.000
0.488±0.000
0.490±0.000

0.491±0.000
0.500±0.000
0.500±0.000

-
-
-

Scalable Deep Gaussian Markov Random Fields for General Graphs

Figure 6. CRPS of DGMRF models with 1–5 layers evaluated on the Mix synthetic dataset. The shaded area corresponds to 95%
conﬁdence interval across 5 random seeds. The CRPS for the true posterior is also shown.

(a) RMSE

(b) CRPS

Figure 7. RMSE and CRPS of DGMRF models with 1–5 layers evaluated on the Dense synthetic datasets. The shaded area corresponds to
95% conﬁdence interval across 5 random seeds. The metric values for the true posterior is also shown.

G

3. Gaussian noise is then added and 25% of the nodes treated as unobserved. We call this the Dense
DGMRF deﬁned w.r.t.
synthetic dataset. Similar to the experiment on the Mix data we train 1–5 layer DGMRFs. Note that the models we train are
deﬁned on
.
G
Results for the Dense dataset are presented in Figure 7. With more layers the DGMRF can come close to the true posterior,
even when deﬁned on a more sparse graph. The performance of the DGMRF models clearly improves up until 3 layers, at
which point the sparsity of the precision matrix matches that of the true model.

3. The true model for the data also here lies outside the model class of DGMRFs deﬁned on

rather than

G

G

A.4. Wikipedia graphs

For the experiments on Wikipedia graphs (section 4.2) we list the standard deviations across 5 random seeds in Table 7. The
random seeds only affect the model training and the set of unobserved nodes is kept ﬁxed.

A.5. California housing and wind speed data

For the experiments on the California housing and wind speed data (section 4.3 and 4.4) we list the standard deviations
across 5 random seeds in Table 8.

A.6. Impact of percentage of observed nodes

Here we present additional results for the experiment in section 4.5, where model performance for different percentages
of observed nodes is investigated. Figure 8 shows for the Crocodile Wikipedia graph how the CRPS changes with the

12345LayersL8.89.09.29.4CRPS×10−3DGMRFTrueposterior12345LayersL2.242.262.282.30RMSE×10−2DGMRFTrueposterior12345LayersL1.261.28CRPS×10−2DGMRFTrueposteriorScalable Deep Gaussian Markov Random Fields for General Graphs

Table 7. Standard deviations of DGMRF results for the Wikipedia experiment. Each standard deviation is computed across the results of 5
models trained using different random seeds.

CHAMELEON

SQUIRREL

CROCODILE

DGMRF

RMSE

CRPS

RMSE

CRPS

RMSE

CRPS

L = 1
L = 3
L = 5

0.000
0.006
0.033

0.000
0.003
0.023

0.000
0.034
0.022

0.000
0.021
0.013

0.000
0.031
0.030

0.000
0.018
0.019

Table 8. Standard deviations of DGMRF results for the the California housing and wind speed data. Each standard deviation is computed
across the results of 5 models trained using different random seeds. Values for the California housing data are multiplied by a factor 100
to match the scale in Table 2.

CAL., NO FEATURES CAL., FEATURES WIND, SPATIAL MASK WIND, RANDOM MASK

DGMRF

RMSE

L = 1
L = 2
L = 3

0.000
0.000
0.000

CRPS

0.000
0.001
0.001

RMSE

CRPS

RMSE

0.001
0.001
0.001

0.001
0.001
0.001

0.013
0.011
0.017

CRPS

0.008
0.007
0.011

RMSE

0.000
0.001
0.001

CRPS

0.000
0.000
0.000

percentage of observed nodes. The same experiment was also repeated using the synthetic Mix dataset and the results from
this can be found in Figure 9.

B. Reparametrization

, as this would lead to a G(l) with determinant
When parametrizing the model we want to avoid the situation where
0 and a non-deﬁnite precision matrix Q. Even if this exact situation would likely be avoided during optimization, this
represents an unstable area of the parameter space as the log-determinant goes towards
. We solve this problem by
βl
< 1. We choose this direction of the inequality as it will guarantee the
enforcing the inequality
αl
convergence of the power series that we use in one of our methods for the log-determinant computation (see Appendix C). It
also makes intuitive sense that the central node should have a larger inﬂuence in each layer than it’s neighbors. We also
parametrize the model so that αl > 0, which is no restriction on the model class. See this for example in how Eq. 2 is
invariant to the sign of the right hand side.

βl| ⇔

βl|
|

αl|

αl|

−∞

=

>

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

|

|

|

Figure 8. CRPS on the Crocodile Wikipedia graph for different percentages of observed nodes. The shaded area corresponds to 95%
conﬁdence interval, evaluated across different random seeds.

95806040205Observednodes(%)0.60.81.01.21.4CRPSDGMRFDGP(×10)GraphGPIGMRFScalable Deep Gaussian Markov Random Fields for General Graphs

(a) RMSE

(b) CRPS

Figure 9. RMSE and CRPS on the Mix synthetic graph for different percentages of observed nodes. The shaded area corresponds to 95%
conﬁdence interval, evaluated across different random seeds.

To achieve these inequalities, as well as γl ∈

]0, 1[, we reparametrize the model as

αl = exp

θ(l)
1

(cid:16)

βl = αl tanh

(cid:17)
θ(l)
2

(cid:16)
γl = sigmoid

(cid:17)
θ(l)
3

(17)

(18)

(19)

2 , θ(l)

1 , θ(l)

where θ(l)
restrictions on G(l). In a similar way we reparametrize σ = exp(θσ) > 0 with a free parameter θσ ∈

R are free parameters. This allows us to use unconstrained optimization while preserving the desired

3 ∈

R.

(cid:16)

(cid:17)

C. Details on the Power Series Log-Determinant Computation

To make our graph DGMRF scale to large graphs the computation of log
has to be both efﬁcient and differentiable
w.r.t. all layer parameters. We follow the approach of Behrmann et al. (2019), where a stochastic approximation of the
log-determinant is constructed based on a power series.

det

(cid:0)

G(l)

(cid:1)(cid:12)
(cid:12)

(cid:12)
(cid:12)

We start by rewriting G(l) as

G(l) = Dγl (αlI + βlD−

1A)

= Dγl−

1

2 (αlD

1

1

2 + βlD−

1

2 AD−

1
2 )D

1
2

(20)

2 D−
βl
αl

I +

˜A

1
2

D

(cid:19)

= αlDγl−

1
2

(cid:18)

where we again use the deﬁnition ˜A = D−

1

2 AD−

1
2 . Then

log

det

G(l)

= log

det

αlDγl−

1
2

I +

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:12)
(cid:12)
(cid:12)

= log

(cid:18)

(cid:18)
1
l det(D)γl−
αN
2 det

(cid:19)

(cid:19)(cid:12)
(cid:12)
(cid:12)
det(D)
(cid:12)

˜A

1
2

(21)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

= N log(αl) + γl log(det(D)) + log

˜A

1
2

D

βl
αl

(cid:19)
βl
αl

I +

(cid:18)

(cid:19)

det

I +

(cid:18)

βl
αl

(cid:12)
(cid:19)
(cid:12)
(cid:12)
˜A
(cid:12)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we have used that αl > 0 and det(D) > 0 as well as a number of properties of determinants (see for example
Petersen & Pedersen (2012)). All involved quantities are simple and efﬁcient to compute, except for log

det

˜A

I + βl
αl

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:12)
(cid:12)
(cid:12)

95806040205Observednodes(%)1.61.82.02.22.42.6RMSE×10−2DGMRFDGP(×10)IGMRFGraphGP95806040205Observednodes(%)0.81.01.21.41.61.8CRPS×10−2DGMRFDGP(×10)IGMRFGraphGPScalable Deep Gaussian Markov Random Fields for General Graphs

which involves the adjacency matrix of the graph. To tackle this challenge, we will formulate this quantity as a power series
and apply a stochastic approximation.
We ﬁrst note a few things about the matrix ˜A. Firstly, the eigenvalues of ˜A lie in [
1, 1]. This follows for example from
˜A has eigenvalues in [0, 2] (Stankovi´c et al., 2020a; Bolla & Tusn´ady,
the fact that the normalized graph Laplacian I
1994). Secondly, since ˜A is symmetric we can also conclude that its singular values are given by the absolute values of its
eigenvalues5. In particular, the maximum singular value σmax( ˜A)

−

−

To construct our approximation, we ﬁrst show that det
eigenvector wi. Then

βl
αl
˜λi is an eigenvalue to I + βl
αl

I +

(cid:18)

˜A

so ψi = 1 + βl
αl

I + βl
αl

≤
˜A
(cid:17)
˜Awi =

1.
> 0. Let ˜λi ∈
βl
αl

1 +

˜λi

wi,

(cid:19)

(cid:16)
wi = wi +

βl
αl

(cid:19)
(cid:18)
˜A. By our parametrization βl

1, 1] be an eigenvalue of ˜A with

[

−

(22)

αl ∈

]

1, 1[ , which implies that ψi > 0

−

i. Hence

∀

det

I +

(cid:18)

βl
αl

N

˜A

=

(cid:19)

i=1
(cid:89)

ψi > 0

⇒

log

det

I +

˜A

= log

det

I +

βl
αl

˜A

.

(cid:19)(cid:19)

(23)

(cid:18)

(cid:18)

βl
αl

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Similarly to Behrmann et al. (2019) we next formulate the log-determinant in terms of the power series

log

det

I +

˜A

= Tr

log

I +

βl
αl

(cid:18)

(cid:18)

(cid:88)k=1
Using the properties of ˜A discussed earlier we can show that

(cid:18)

(cid:18)

(cid:19)(cid:19)

(cid:19)(cid:19)

˜A

∞

=

βl
αl

Tr

1)k+1

(cid:18)(cid:16)

k

˜A
(cid:17)

βl
αl

k

(cid:19)

=

1
k

−

∞

(cid:88)k=1

k

βl
αl (cid:19)

−

(cid:18)

Tr

˜Ak

.

(24)

(cid:16)

(cid:17)

(
−

βl
αl

˜A
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

βl
αl (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜A
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

βl
αl (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜A
(cid:17)

(cid:16)

=

σmax

< 1

(25)

which implies that the power series in Eq. 24 converges. We can then truncate this series at some large k = K to get an
approximation, as discussed in section 3.3.2.

Using the same reasoning as Behrmann et al. (2019), the truncation error of the series can be bounded. We note that

N

N

Tr

˜Ak

=

˜λk
i

≤

(cid:16)

(cid:17)(cid:12)
(cid:12)
(cid:12)
i is an eigenvalue to ˜Ak and that

(cid:12)
(cid:12)
(cid:12)

(cid:12)
i=1
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we have used that each ˜λk
truncated at K can then be bounded as

≤

N

(26)

(cid:12)
(cid:12)
(cid:12)
1. The absoute error EK of the series in Eq. 24

EK ≤

N

(cid:32)−

log

1

−

βl
αl (cid:12)
(cid:12)
(cid:12)
(cid:12)

1
k

k

.
(cid:33)

βl
αl (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
We refer to Behrmann et al. (2019) for a proof of this bound. Our case is fully analogues and can be derived by replacing
(cid:12)
(cid:12)
. For the majority of parameter values this bound decreases very quickly with K, as shown
their Lipschitz constant by

(cid:88)k=1

(cid:18)

(cid:19)

βl
αl

(cid:12)
in Figure 10. It is only when
(cid:12)
(cid:12)

(cid:12)
βl
(cid:12)
αl
(cid:12)

is close to 1 that the rate of decrease slows down.

(cid:12)
As the traces in Eq. 24 only depend on the graph structure, their values can be pre-computed and stored for use during the
(cid:12)
(cid:12)
can for example be done using sparse linear algebra, multiplying together k sparse
model training. Computing Tr
matrices. For large values of k these matrix products do however become increasingly dense and the computation inefﬁcient.
An alternative approach is to use the fact that

(cid:12)
(cid:12)
(cid:12)
˜Ak

(cid:17)

(cid:16)

˜λk
i

i=1(cid:12)
(cid:88)
(cid:12)
(cid:12)
˜λk
i
≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

K

−

(27)

(28)

Tr

˜Ak

= E

p(u)

(cid:16)

(cid:17)

u(cid:124) ˜Aku
(cid:104)

(cid:105)

5Let vi and λi be the i:th eigenvector and eigenvalue of a symmetric matrix B. Then B(cid:124)Bvi = B2vi = λiBvi = λ2
i = |λi| is a singular value of B.

(cid:112)λ2

i vi and

Scalable Deep Gaussian Markov Random Fields for General Graphs

Figure 10. The quantity − log
values.

(cid:16)

1 −

(cid:12)
(cid:12)
(cid:12)

βl
αl

(cid:17)

(cid:12)
(cid:12)
(cid:12)

− (cid:80)K

k=1

(cid:12)
(cid:12)
(cid:12)

1
k

βl
αl

k

(cid:12)
(cid:12)
(cid:12)

from the bound in Eq. 27, plotted as a function of K for different parameter

for any distribution p(u) with zero mean and covariance matrix I (Hutchinson, 1990). Taking a Monte Carlo (MC) estimate
of Eq. 28 yields the Hutchinson’s trace estimator. We note that while the estimator is unbiased, we here only compute its
value during pre-processing and then reuse it during training. This gives an approximation of the trace, but since we do not
have to re-compute it during training we can use a high value for K and many samples for the MC estimate. We choose
N , resulting in a minimum variance estimator. Computing this estimate only
p(u) as the uniform distribution over
requires applying ˜A to vectors sampled from p(u). As the multiplication with ˜A corresponds to a single GNN layer we
implement this using the same GNN framework as the rest of the model.

1, 1
}

{−

D. Baseline Models

In this appendix we describe the baseline models used in our experiments.

D.1. Label Propagation (LP)

The label propagation baseline follows the basic framework of (Zhu et al., 2003), but using real-valued labels (Jia & Benson,
2020). This approach boils down to ﬁnding a joint solution to the equation

yi =

1
di

yj

n(i)
(cid:88)j
∈

(29)

for all i corresponding to unobserved nodes. Such a solution exists, but requires the inversion of a potentially very large
matrix. We utilize the CG method for this.

The LP baseline does not give a predictive distribution that can be used for computing CRPS. As there is no randomness in
the method we can not apply the ensemble method for uncertainty estimation here.

D.2. Intrinsic GMRF (IGMRF)

4 for the Wikipedia graphs and (cid:15) = 10−

The IGMRF baseline is a GMRF with mean µ = 0 and precision matrix Q = κ(D
A) + (cid:15)I. Here κ is a parameter of the
model and (cid:15) a small value added to the diagonal in order to ensure positive deﬁniteness and numerical stability. We use
6 for all other experiments. Similarly to our DGMRF, this GMRF deﬁnes
(cid:15) = 10−
a prior over x and we then use a Gaussian likelihood with noise variance σ2. The parameters κ and σ are optimized by
maximizing the log marginal likelihood log p(ym). For this we use a simple grid-based optimization where log p(ym) is
and 20 log-spaced values of κ in the range [0.01, 1000]. To
evaluated for each combination of σ

−

0.001, 0.01, 0.1, 1
}

∈ {

20406080100K0.00.51.01.52.02.53.03.5EKN≤(cid:12)(cid:12)(cid:12)βlαl(cid:12)(cid:12)(cid:12)=0.99(cid:12)(cid:12)(cid:12)βlαl(cid:12)(cid:12)(cid:12)=0.95(cid:12)(cid:12)(cid:12)βlαl(cid:12)(cid:12)(cid:12)=0.9(cid:12)(cid:12)(cid:12)βlαl(cid:12)(cid:12)(cid:12)=0.5(cid:12)(cid:12)(cid:12)βlαl(cid:12)(cid:12)(cid:12)=0.1Scalable Deep Gaussian Markov Random Fields for General Graphs

compute the log marginal likelihood we use the fact that

holds for any x(cid:48). We then utilize sparse linear algebra computations, such as the sparse Cholesky decomposition, to compute
all quantities involved (Rue & Held, 2005). In contrast to our DGMRF this is feasible here since Q is very sparse.

p(ym) =

p(ym|
p(x
|

x)p(x)
ym)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x=x(cid:48)

(30)

D.3. Graph GP

For the Graph GP baseline we use the reference implementation made available6 by the authors (Borovitskiy et al., 2021).
In order to make the model scale to large graphs we use a number of approximation techniques, all described in the original
paper. For training the model we use doubly stochastic variational inference with a batch size of 128 and the proposed
interdomain inducing variables. The kernel matrix is approximated using the 500 smallest eigenvalues of the graph Laplacian
and corresponding eigenvectors. When applicable we use the weighted adjacency matrix also for the Graph GP baseline.

D.4. Sparse Variational Gaussian Process (SVGP)

The SVGP (Hensman et al., 2015) implementation is from the GPyTorch library (Gardner et al., 2018). We use a Mat´ern
kernel with ν = 3/2, automatic relevance determination and a batch size of 256. The positions of 500 inducing points are
learned jointly with the model parameters. A Gaussian variational distribution is used, where we parametrize the Cholesky
factor of the covariance matrix directly. When more features than the spatial coordinates are included, the features and
coordinates are concatenated and together used as kernel input.

D.5. Bayesian Linear Regression (Bayes LR)

The Bayesian linear regression model is a default Bayesian Ridge model from the scikit-learn library (Pedregosa et al.,
2011). It uses a Gaussian prior on the regression coefﬁcients and uninformative gamma priors on regularization parameters.

D.6. Multilayer Perceptron (MLP)

In comparison to other baseline models, the deep learning baselines require some tuning of the network architectures. To
achieve this we reserve 20% of the training data (observed nodes) as a validation set. No such validation set is used for
DGMRFs or other baseline models. The validation set is used to decide the network architecture and for regularization by
early stopping.

For the MLP baseline we consider the layer conﬁgurations (number of hidden layers
1
conﬁguration. The layer conﬁguration resulting in the lowest validation loss is then used for the ensemble.

128,
. We use an MSE loss and the Adam optimizer (Kingma & Ba, 2015) to train one model of each
}

dimensionality)

128, 2

512, 2

∈ {

512

×

×

×

×

×

1

D.7. GNNs (Graph Convolutional Network (GCN) and Graph Attention Network (GAT))

The GNN models are trained in the same way as the MLP, also using 20% of the observed nodes for validation. The full
graph is fed to the GNN at each iteration and the MSE loss computed for all nodes designated for training. We here consider
the layer conﬁgurations (number of hidden layers
dimensionality)
32,
1
. We experiment with two GNN baselines, the Graph Convolutional Network (GCN) of Kipf &
5
}
Welling (2017) and the Graph Attention Network (GAT) of Veliˇckovi´c et al. (2018). For the GAT model we use single-head
attention.

128, 5

128, 7

64, 3

64, 5

64, 3

32, 3

32, 1

128

∈ {

×

×

×

×

×

×

×

×

×

×

D.8. Deep Graph Prior (DGP)

As earlier noted the DGMRF model can be related to the message passing formulation of GNNs. An interesting question
arises about how much of the performance of these types of models can be attributed to any inductive biases of the GNN
layer formulation. We ask, does the GNN architecture in itself restrict the model class to a set of models that are highly
useful for common types of signals on graphs? Ulyanov et al. (2018) investigate a similar question for CNNs and natural
images in their Deep Image Prior model. For the in-painting setting, random noise is fed to the network and the model

6https://github.com/spbu-math-cs/Graph-Gaussian-Processes

Scalable Deep Gaussian Markov Random Fields for General Graphs

Table 9. Properties of the different graph datasets. The density of an undirected graph with N nodes and Ne edges is computed as
2Ne/N (N − 1). This is equal to the fraction of possible edges present, compared to the complete graph. The (unweighted) diameter of a
graph is the longest shortest path between any two nodes in the graph.

GRAPH

NODES

EDGES

DENSITY

DIAMETER UNOBSERVED NODES

SYNTHETIC DGMRF
DENSE
MIX
WIKIPEDIA CHAMELEON
WIKIPEDIA SQUIRREL
WIKIPEDIA CROCODILE
CALIFORNIA HOUSING
WIND SPEED, SPATIAL MASK
WIND SPEED, RANDOM MASK

3 000
3 000
5 000
2 277
5 201
11 631
20 536
126 652
126 652

8 977
8 974
14 970
31 371
198 353
170 773
61 585
379 933
379 933

0.0020
0.0020
0.0012
0.0121
0.0147
0.0025
2.92 × 10−4
4.74 × 10−5
4.74 × 10−5

32
32
41
11
10
11
57
101
101

25%
25%
50%
50%
50%
50%
50%
9.12%
50%

then trained on the observed pixels of a single picture. The idea is that any inductive biases in the model architecture can
still steer the training to good solutions. As the model is massively overparametrized the training should not be allowed
to converge, but stopped at some earlier point that still results in a good solution. We hypothesize that GNNs can exhibit
similar properties as the CNNs investigated by Ulyanov et al. (2018) and implement a graph version that we denote DGP.

For the DGP model we use the same setup for training GNNs as described above. We use GCN layers, tune the layer
conﬁguration, and use early stopping in the same way as before. A single Gaussian sample is drawn and used as the model
input throughout the whole training.

The DGP model allows for applying GNNs to graphs without input features. As DGP has proven useful in some of our
experiments this indicates that our earlier stated hypothesis has some ground to it. Exploring this type of model and the
inductive biases of GNNs in general is an interesting direction for further research.

E. Details on Experiments and Datasets

In the interest of reproducability we present additional details about the experiment setups and datasets in this appendix. For
training our DGMRF we use a learning rate of 0.01 and the Adam optimizer (Kingma & Ba, 2015) in all experiments. The
model has not been observed to be sensitive to these choices so no extensive tuning has been done. Note that overﬁtting is
not a considerable problem here. If necessary the learning rate can be tuned to make the ELBO converge, just using the
training data (observed nodes). On synthetic data we train the model for 50 000 iterations, on the Wikipedia and California
housing datasets 80 000 iterations (150 000 for 5-layer DGMRFs) and for the wind speed data 150 000 iterations. These
numbers are large enough for the ELBO to converge and often unnecessarily high, meaning that runtimes could be slightly
reduced with a more carfeful choice. In all experiments we use one DGMRF layer for ˜G in the variational distribution q (see
Eq. 11). At each iteration of training we draw 10 samples from q to estimate the expectation in the ELBO.

An overview of the different graphs used in experiments is presented in Table 9.

E.1. Synthetic data

E.1.1. SYNTHETIC DGMRF DATA

The synthetic data used for the experiment in Figure 3 was sampled from 1–4 layer DGMRFs based on a synthetic generated
graph. A graph with 3000 nodes was generated by ﬁrst sampling node positions uniformly over [0, 1]2 and then forming
the graph using a Delaunay triangulation (De Loera et al., 2010). The positions of the nodes were only used to construct
1.0, γl = 1, and bl = 0 for all
the graph and for visualization purposes. DGMRFs with parameters αl = 1.2, βl =
layers were then deﬁned on this graph. The ﬁnal data y was constructed by drawing a single sample x from a DGMRF and
adding Gaussian noise with σ = 0.01. Observation masks were additionally created by randomly setting 25% of nodes as
unobserved.

−

Scalable Deep Gaussian Markov Random Fields for General Graphs

E.1.2. DENSE

For the synthetic Dense data (discussed in Appendix A.3), a random graph was generated in a similar way as above. Based
on this random graph the corresponding 3-hop graph was then created. A sample x was drawn from a 1-layer DGMRF with
αl = 1.2, βl =

1.0, γl = 1, and bl = 0 deﬁned on this 3-hop graph. Gaussian noise with σ = 0.01 was again added.

−

E.1.3. MIX

For the Mix data a graph with 5000 nodes was created in the same way as above. A GMRF was deﬁned on this graph with
(cid:124)
4
mean µ = 0 and precision matrix Q =
i Gi, where each Gi was deﬁned as a DGMRF layer according to Eq. 6. We
i=1 G
i (in the same way as the Dense dataset). The
deﬁned each Gi not directly on the generated graph, but on the i-hop graph
G
0.1]) and γl ﬁxed to 1. The data was
([0.5, 1.5]), βl ∼ U
parameters of each Gi were sampled randomly as αl ∼ U
([
then sampled in the same way as above.

1.1,

(cid:80)

−

−

E.2. Wikipedia graphs

The Wikipedia graphs were created and made available7 by Rozemberczki et al. (2021). In our experiments we do not use
the accompanying node features and we use the logarithm of target values. We pre-processed the data by removing duplicate
edges and self-loops (edges with the same node as both endpoints).

After tuning the DGP model the architecture used for the ensemble was 5
Chameleon and Squirrel data.

×

128 on the Crocodile data and 7

128 on the

×

E.3. California housing data

The California housing dataset was loaded directly through the scikit-learn library8 (Pedregosa et al., 2011). We used the
logarithm of target values and performed the same feature transformation as described in the original paper (Kelley Pace &
Barry, 1997). Outliers in the data were removed by the elliptic envelope method, determining the contamination degree by
visual inspection. Features and targets were ﬁnally normalized to [0, 1].

An equirectangular projection was used on the latitude and longitude of each housing block to create the node positions.
Based on these positions the graph was created as a Delaunay triangulation. The graph creation was done using the built in
Delaunay computation in PyTorch Geometric (Fey & Lenssen, 2019), which in turn relies on the Qhull library9. We note
that many choices exist for how to create such a spatial graph, but the Delaunay method has in our experiments shown to
work well while maintaining a highly sparse graph. Considering different algorithms for graph creation is outside the scope
1 based on the euclidean distance ρi,j between nodes i and j.
of this work. The graph was weighted with wi,j = (ρi,j + (cid:15))−
A small (cid:15) is included to prevent division by zero for nodes assigned to the same position.

For handling features in the DGMRF model we follow the approach of Sid´en & Lindsten (2020) and use an auxiliary
Bayesian linear regression model. An uninformative
prior and a mean-ﬁeld variational distribution are used for
the coefﬁcients of the linear model. For posterior inference the coefﬁcients can be integrated out.

0, 108I

N

(cid:0)

(cid:1)

When no features are used the baseline models in Table 2 only utilize the node positions as inputs. When features are used
both the node positions and the pre-processed socio-economic features are used. The Graph GP, LP and IGMRF baselines
only utilize the graph and no node positions or other features. There is no obvious way to adapt these methods to also use
node features, so this was deemed outside the scope of this paper.

The resulting layer conﬁgurations after tuning for the deep learning baselines are listed in Table 10 together with the number
of trainable parameters. These numbers can be compared to the signiﬁcantly smaller 4L + 1 trainable parameters in an
L-layer DGMRF.

7https://github.com/benedekrozemberczki/MUSAE
8https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset
9http://www.qhull.org/

Scalable Deep Gaussian Markov Random Fields for General Graphs

Table 10. Layer conﬁgurations (number of hidden layers × dimensionality) and corresponding number of trainable parameters of the deep
learning models. Each layer conﬁguration is the ﬁnal one decided on from hyperparameter tuning on the validation data.

CAL., NO FEATURES

CAL., FEATURES

WIND, SPATIAL MASK WIND, RANDOM MASK

MODEL

LAYERS

# PARAM.

LAYERS

# PARAM.

LAYERS

# PARAM.

LAYERS

# PARAM.

MLP
GCN
GAT

2 × 512
7 × 128
5 × 128

264 705
99 585
67 841

2 × 512
3 × 64
3 × 128

268 801
9 089
35 329

2 × 512
7 × 128
5 × 32

266 241
99 969
4 769

2 × 512
7 × 128
5 × 64

266 241
99 969
17 729

E.4. Wind speed data

The wind speed data originates from the Wind Integration National Dataset Toolkit10. The mean wind speed in the summary
statistics for the different sites was used as our target. To somewhat limit ourselves to continental US we only included sites
24.4◦. Also here an equirectangular projection was used and the graph then created in the same way as for
at a latitude
≥
the California housing data.

The LP and IGMRF baseline models only utilize the graph in this experiment. All other baselines (not the DGMRFs) use
the node positions as features, but here expanded to second degree polynomial features in order to capture some non-linear
trends. Deep learning architectures are listed in Table 10. We were not able to apply the Graph GP to this dataset as the
required pre-computation of eigenvalues and eigenvectors does not scale to a graph of this size.

For the DGMRF we here use the power series method of section 3.3.2 for the log-determinant computations. We truncate the
power series in Eq. 15 at K = 50 terms and use the Hutchinson’s trace estimator with 1000 MC samples (see Appendix C).

E.5. Impact of percentage of observed nodes

In this experiment we only changed the mask specifying which nodes were observed in the Mix and Wikipedia Crocodile
graphs. Such masks were created for 5%, 20%, 40%, 60%, 80% and 95% of nodes being observed. Each set of observed
nodes was chosen to be a superset of the previous one, with the additional nodes sampled uniformly at random.

In this experiment we use 5 random seeds also for the DGP baseline, in order to compute conﬁdence intervals. This means
that the whole ensemble of 10 models is re-trained for each random seed, in total creating 50 models for each observation
mask. To make this feasible we ﬁx the GNN architecture used. Based on a preliminary tuning experiment we found good
choices to be the 7

32 architecture for the Wikipedia Crocodile data.

128 architecture for the Mix data and the 5

×

×

Appendix References

Bolla, M. and Tusn´ady, G. Spectra and optimal partitions of weighted graphs. Discrete Mathematics, 128(1):1–20, 1994.

Gardner, J. R., Pleiss, G., Bindel, D., Weinberger, K. Q., and Wilson, A. G. Gpytorch: Blackbox matrix-matrix gaussian
process inference with gpu acceleration. In Advances in Neural Information Processing Systems, volume 31, 2018.

Hutchinson, M. A stochastic estimator of the trace of the inﬂuence matrix for Laplacian smoothing splines. Communications

in Statistics - Simulation and Computation, 19(2):433–450, 1990.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2015.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R.,
Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.

Petersen, K. B. and Pedersen, M. S. The matrix cookbook, nov 2012.

10https://www.nrel.gov/grid/wind-toolkit.html

