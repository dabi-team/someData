Towards Eﬀicient Sparse Matrix Vector Multiplication
on Real Processing-In-Memory Systems

Christina Giannoula1,2

Ivan Fernandez1,3

Juan Gómez-Luna1

Nectarios Koziris2 Georgios Goumas2 Onur Mutlu1

1ETH Zürich

2National Technical University of Athens

3University of Malaga

2
2
0
2

r
p
A
2

]

R
A
.
s
c
[

1
v
0
0
9
0
0
.
4
0
2
2
:
v
i
X
r
a

Abstract
Sparse Matrix Vector Multiplication (SpMV) has been character-
ized as one of the most thoroughly studied scientiﬁc computation
kernels, because it is a fundamental linear algebra kernel for impor-
tant applications from the scientiﬁc computing, machine learning,
and graph analytics domains. SpMV performs indirect memory ref-
erences as a result of storing the sparse matrix in a compressed for-
mat, and irregular memory accesses to the input vector due to the
sparsity pattern of the input matrix [6, 13, 27]. Therefore, in com-
modity processor-centric systems, SpMV is a primarily memory-
bandwidth-bound kernel for the majority of real sparse matrices,
and is bottlenecked by data movement between memory and pro-
cessors [6, 9, 10].

One promising way to alleviate the data movement bottleneck
is the Processing-In-Memory (PIM) paradigm [1, 2, 7–10, 14, 15,
17]. PIM moves computation close to application data by equipping
memory chips with processing capabilities [1, 14, 15]. To provide
large aggregate memory bandwidth for the in-memory processors,
several manufacturers have already started to commercialize near-
bank PIM designs [1, 9, 10, 19, 20]. Near-bank PIM designs tightly
couple a PIM core with each DRAM bank, exploiting bank-level
parallelism to expose high on-chip memory bandwidth of standard
DRAM to processors. Three real near-bank PIM architectures are
Samsung’s FIMDRAM [19], SK hynix’s GDDR6-AiM [20] and the
UPMEM PIM system [1, 9, 10].

Most real near-bank PIM architectures [1, 9, 10, 19, 20] sup-
port several PIM-enabled memory chips connected to a host CPU
via memory channels. Each memory chip comprises multiple low-
power PIM cores with relatively low computation capability [9, 10],
and each of them is located close to a DRAM bank [9, 10, 19, 20].
Each PIM core can access data located on its local DRAM bank,
and typically there is no direct communication channel among PIM
cores. Overall, near-bank PIM systems provide high levels of paral-
lelism and very large memory bandwidth. As such, they are a very
promising computing platform to accelerate memory-bound ker-
nels. Recent works leverage near-bank PIM architectures to pro-
vide high performance and energy beneﬁts on bioinformatics [5,
9, 10], skyline computation [22], compression [11] and neural net-
work [9, 10, 16, 19] kernels. A recent study [9, 10] provides PrIM
benchmarks [25], which are a collection of 16 kernels for evaluat-
ing near-bank PIM architectures. However, there is no prior work
to thoroughly study the widely used, memory-bound SpMV kernel
on a real PIM system.

Our work is the ﬁrst to eﬃciently map the SpMV kernel on near-
bank PIM systems, and understand its performance implications
on a real-world PIM system. We make two key contributions. First,
we design eﬃcient SpMV algorithms to accelerate the SpMV kernel
in current and future PIM systems, while covering a wide variety

of sparse matrices with diverse sparsity patterns. Second, we pro-
vide the ﬁrst comprehensive analysis of SpMV on a real PIM archi-
tecture. Speciﬁcally, we conduct our rigorous experimental analy-
sis of SpMV kernels in the UPMEM PIM system, the ﬁrst publicly-
available real-world PIM architecture.

We present the freely and openly available SparseP library [26]
that includes 25 SpMV kernels for real PIM systems. SparseP sup-
ports (1) the most popular compressed matrix formats (i.e., CSR,
COO, BCSR, BCOO formats), (2) a wide range of data types (i.e., 8-
bit integer, 16-bit integer, 32-bit integer, 64-bit integer, 32-bit ﬂoat
and 64-bit ﬂoat data types), (3) two types of well-crafted data par-
titioning techniques of the sparse matrix to PIM-enabled memory,
(4) various load balancing schemes across PIM cores, (5) various
load balancing schemes across threads of a multithreaded PIM core,
and (6) three synchronization approaches among threads within
multithreaded PIM core.

We conduct an extensive characterization and analysis of SparseP
kernels on the UPMEM PIM system [9, 10]. We analyze the SpMV
execution (1) using one single multithreaded PIM core, (2) using
thousands of PIM cores, and (3) comparing its performance and en-
ergy consumption with that achieved on conventional processor-
centric CPU and GPU systems. Our extensive evaluation provides
programming recommendations for software designers, and sug-
gestions and hints for hardware and system designers of future
PIM systems.

We highlight our most signiﬁcant recommendations for PIM

software designers:
(1) Design algorithms that provide high load balance across threads
of a multithreaded PIM core in terms of computations, loop con-
trol iterations, synchronization points and memory accesses. In
SpMV, we ﬁnd that when the parallelization scheme used causes
high disparity in the non-zero elements/blocks/rows processed
across threads of a PIM core, or the number of lock acquisi-
tions/lock releases/DRAM memory accesses performed across
threads, performance severely degrades in low-area PIM cores
with relatively low computation capabilities [9, 10].

(2) Design compressed data structures that can be eﬀectively parti-
tioned across DRAM banks, with the goal of providing high com-
putation balance across PIM cores. We observe that the com-
pressed matrix format used to store the input matrix in SpMV
determines the data partitioning across DRAM banks of PIM-
enabled memory, thereby aﬀecting the load balance across PIM
cores with corresponding performance implications.

(3) Design adaptive algorithms that trade oﬀ computation balance
across PIM cores for lower data transfer costs to PIM-enabled mem-
ory, and adapt the software strategies to the particular patterns
of each input given, as well as the characteristics of the PIM hard-
ware. Our analysis demonstrates that the best-performing SpMV

 
 
 
 
 
 
execution on the UPMEM PIM system is achieved using algo-
rithms that (i) trade oﬀ computation for lower data transfer
costs, and (ii) select the load balancing strategy and data parti-
tioning policy based on the particular sparsity pattern of the in-
put matrix and the characteristics of the underlying PIM hard-
ware.
We highlight our most signiﬁcant suggestions for PIM hardware

and system designers:
(1) Provide low-cost synchronization support and hardware support
to enable concurrent memory accesses by multiple threads to the
local DRAM bank to increase parallelism in a multithreaded PIM
core. For instance, ﬁne-grained locking approaches in SpMV
to increase parallelism in critical sections do not improve per-
formance over coarse-grained approaches. This is because con-
current DRAM accesses performed by multiple threads are se-
rialized by the UPMEM PIM hardware. To improve parallelism,
subarray level parallelism [23] or multiple DRAM banks per
PIM core could be supported in the PIM hardware, along with
lightweight synchronization schemes for PIM cores [2].

(2) Optimize the broadcast collective operation in data transfers from
main memory to PIM-enabled memory to minimize overheads of
copying the input data into all DRAM banks in the PIM system.
When the sparse matrix is horizontally partitioned across PIM
cores and the whole input vector is copied into the DRAM bank
of each PIM core, SpMV cannot scale up to a large number of
PIM cores. This is because it is severely limited by data transfer
costs to broadcast the input vector into each DRAM bank of
PIM-enabled DIMMs. Such data transfers incur high overheads,
because they take place via the narrow oﬀ-chip memory bus.
(3) Optimize the gather collective operation at DRAM bank granular-
ity for data transfers from PIM-enabled memory to the host CPU
to minimize overheads of retrieving the output results. When the
sparse matrix is split in 2D tiles, each of them is assigned to
each PIM core, SpMV is severely limited by data transfers to
retrieve results for the output vector from DRAM banks of PIM-
enabled memory. This is due to two reasons: (i) PIM cores cre-
ate a large number of partial results that need to be gathered
from PIM-enabled memory to the host CPU via the narrow
memory bus in order to assemble the ﬁnal output vector, and
(ii) the current implementation of the UPMEM PIM system has
the limitation that the transfer sizes from/to all DRAM banks
involved in the same parallel transfer need to be the same, and
therefore a large amount of padding with empty bytes is per-
formed in such SpMV kernels.

(4) Design high-speed communication channels and optimized libraries
for data transfers to/from thousands of DRAM banks of PIM-enabled
memory. We ﬁnd that SpMV execution on the memory-centric
UPMEM PIM system achieves a much higher fraction of the
machine’s peak performance (on average 51.7% for the 32-bit
ﬂoat data type), compared to that on processor-centric CPU
and GPU systems. However, its end-to-end performance is still
signiﬁcantly limited by data transfer overheads on the narrow
memory bus. Thus, the software stack of real PIM systems needs
to be enhanced with fast data transfers to/from PIM-enabled
memory modules, and/or the PIM hardware needs to be en-
hanced to support eﬃcient direct communication among PIM
cores [12, 18, 21, 24].

For more information about our thorough characterization on
the SpMV PIM execution, results, insights and the open-source
SparseP software package [26], we refer the reader to the full ver-
sion of the paper [3, 4]. We hope that our work can provide valu-
able insights to programmers in the development of eﬃcient sparse
linear algebra kernels and other irregular kernels from diﬀerent ap-
plication domains tailored for real PIM systems, and enlighten ar-
chitects and system designers in the development of future memory-
centric computing systems. The SparseP software package is pub-
licly and freely available at https://github.com/CMU-SAFARI/SparseP.
Keywords
high-performance computing, sparse matrix-vector multiplication,
SpMV library, multicore, processing-in-memory, near-data process-
ing, memory systems, data movement bottleneck, DRAM, bench-
marking, real-system characterization, workload characterization
References
[1] F. Devaux, The True Processing In Memory Accelerator, Hot Chips, 2019.
[2] Christina Giannoula et al., SynCron: Eﬃcient Synchronization Support for Near-

Data-Processing Architectures, HPCA, 2021.

[3] Christina Giannoula et al., SparseP: Towards Eﬃcient Sparse Matrix Vector Mul-
tiplication on Real Processing-In-Memory Architectures, Proc. ACM Meas. Anal.
Comput. Syst., 2022.

[4] Christina Giannoula et al., SparseP: Towards Eﬃcient Sparse Matrix Vector Multi-

plication on Real Processing-In-Memory Systems, CoRR, 2022.

[5] Dominique Lavenier et al., Variant Calling Parallelization on Processor-in-

Memory Architecture, BIBM, 2020.

[6] Georgios Goumas et al., Performance Evaluation of the Sparse Matrix-Vector Mul-

tiplication on Modern Architectures, J. Supercomput., 2009.

[7] Ivan Fernandez et al., NATSA: A Near-Data Processing Accelerator for Time Series

Analysis, ICCD, 2020.

[8] Junwhan Ahn et al., A Scalable Processing-In-Memory Accelerator for Parallel

Graph Processing, ISCA, 2015.

[9] Juan Gómez-Luna et al., Benchmarking a New Paradigm: An Experimental Anal-

ysis of a Real Processing-in-Memory Architecture, CoRR, 2021.

[10] Juan Gómez-Luna et al., Benchmarking Memory-Centric Computing Systems:

Analysis of Real Processing-In-Memory Hardware, IGSC, 2021.

[11] Joel Nider et al., Processing in Storage Class Memory, HotStorage, 2020.
[12] Kevin Chang et al., Low-Cost Inter-Linked Subarrays (LISA): Enabling Fast Inter-

Subarray Data Movement in DRAM, HPCA, 2016.

[13] Konstantinos Kanellopoulos et al., SMASH: Co-Designing Software Compression
and Hardware-Accelerated Indexing for Eﬃcient Sparse Matrix Operations, MICRO,
2019.

[14] Onur Mutlu et al., Processing Data Where It Makes Sense: Enabling In-Memory

Computation, MICPRO, 2019.

[15] Onur Mutlu et al., A Modern Primer on Processing in Memory, Emerging Comput-

ing: From Devices to Systems - Looking Beyond Moore and Von Neumann, 2020.

[16] Peng Gu et al., iPIM: Programmable In-Memory Image Processing Accelerator Us-

ing Near-Bank Architecture, ISCA, 2020.

[17] Saugata Ghose et al., Processing-in-Memory: A Workload-Driven Perspective, IBM

JRD, 2019.

[18] Seyyed Hossein SeyyedAghaei Rezaei et al., NoM: Network-on-Memory for Inter-

Bank Data Transfer in Highly-Banked Memories, IEEE CAL, 2020.

[19] Sukhan Lee et al., Hardware Architecture and Software Stack for PIM Based on

Commercial DRAM Technology: Industrial Product, ISCA, 2021.

[20] Seongju Lee et al., A 1ynm 1.25V 8Gb, 16Gb/s/pin GDDR6-based Accelerator-in-
Memory Supporting 1TFLOPS MAC Operation and Various Activation Functions
for Deep-Learning Applications, ISSCC, 2022.

[21] Vivek Seshadri et al., RowClone: Fast and energy-eﬃcient in-DRAM bulk data copy

and initialization, MICRO, 2013.

[22] Vasileios Zois et al., Massively Parallel Skyline Computation for Processing-in-

Memory Architectures, PACT, 2018.

[23] Yoongu Kim et al., A Case for Exploiting Subarray-Level Parallelism (SALP) in

DRAM, ISCA, 2012.

[24] Yaohua Wang et al., FIGARO: Improving System Performance via Fine-Grained

In-DRAM Data Relocation and Caching, MICRO, 2020.

[25] SAFARI

Research

Group,

PrIM

Benchmark

Suite,

https:// github.com/ CMUSAFARI/ prim-benchmarks, 2021.

[26] SAFARI

Research

Group,

SparseP

Software

Package,

https:// github.com/ CMU-SAFARI/ SparseP, 2022.

[27] A. Smith, 6 New Facts About Facebook, http:// mediashift.org, 2019.

