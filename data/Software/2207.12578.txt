2
2
0
2

l
u
J

6
2

]
E
S
.
s
c
[

1
v
8
7
5
2
1
.
7
0
2
2
:
v
i
X
r
a

A Retrospective on ICSE 2022

CAILIN WINSTON, CALEB WINSTON, CHLOE WINSTON, CLARIS WINSTON, CLEAH WIN-
STON∗, Paul G. Allen School of Computer Science, University of Washington, Seattle, USA

INTRODUCTION

1
The 44th International Conference on Software Engineering (ICSE 2022) was held in person from May 22 to May
27, 2022 in Pittsburgh, PA, USA. Since ICSE was held as a solely virtual conference for the last two years, the
opportunity to interact with other members of the software engineering community in person and to engage in
insightful discussions in a physical room was greatly welcomed.

Each day was organized into paper sessions, poster sessions, and Birds of a Feather (BoF) sessions, in addition
to plenty of time for networking. Each paper session consisted of around 6-10 5 minute talks and time for
questions for the authors. The Birds of a Feather sessions allowed for a broader discussion on a topic; the sessions
varied in terms of topics and format.

In this document, we summarize themes of research that we observed at the conference. We organized these

under the following topics (in no particular order):

(1) Autonomous Vehicles (Section 2)
(2) API Misuse (Section 3)
(3) Machine Learning Libraries (Section 4)
(4) Machine Learning Automating Software Engineering (Section 5)
(5) Communities (Section 6)
(6) Education (Section 7)

2 AUTONOMOUS VEHICLES
With companies such as Tesla, Waymo, and NVIDIA making advances in autonomous vehicle technology, it
is important for the software engineering community to discuss the current pressing challenges in developing
and testing these hardware-software systems and improve the testing methods. Autonomous vehicles are highly
safety-critical technologies, and thus reliability must be ensured.

At ICSE 2022, the most pressing challenges were discussed. Issues around who/what is liable for bugs and skill
decrement of future drivers were mentioned broadly, but these remain open issues to be tackled in a diﬀerent
forum. The issues of little eﬀective regulation were quickly laid aside, and the discussion was focused on the need
for a benchmark for safety/reliability and the challenges of testing these safety-critical systems. A benchmark
for safety and reliability is lacking because it is hard to deﬁne what is "suﬃcient dependability" for autonomous
vehicles, and it is hard to test a system to prove that reliability criteria are satisﬁed. Furthermore, diﬀerent
deﬁnitions of safety might exist. Hu et al. [11] addressed this challenge by deﬁning reliability requirements
for machine vision components (MVCs), such as those critical in autonomous vehicle systems, based on the
baseline of human performance and by developing methods for specifying correctness and for checking whether
requirements are satisﬁed. These requirements ensure that the performance of an MVC should not be aﬀected
by image transformations that would not aﬀect a human’s vision and judgment.

Furthermore, software is oftentimes the easiest component to change in hardware-software systems, so it
sometimes becomes a catch-all for bugs that arise, and thus the need to address the challenges in testing these

∗All authors contributed equally to this document.

Author’s address: Cailin Winston, Caleb Winston, Chloe Winston, Claris Winston, Cleah Winston, cailinw@cs.washington.edu, Paul G.
Allen School of Computer Science, University of Washington, Seattle, Seattle, USA.

 
 
 
 
 
 
2

• Cailin Winston, Caleb Winston, Chloe Winston, Claris Winston, Cleah Winston

complex systems is important. Woodlief et al. [29] presented a mutation-based approach to test machine vision
components that are used in autonomous vehicles by generating mutations based on semantics from an im-
age dataset. Regarding testing the entire system, simulators exist, but methods for testing the simulators have
not been developed. However, the priority for the large companies that are developing these technologies is
maximizing revenue, which may not always align with the ideal criteria for safety.

FIGHTING MISUSE

3
Research presented at this year’s ICSE on API misuse detection addressed one key issue at large: false positives in
mined instances of API misuse. API misuse occurs when an API has some inner workings that is misunderstood
by the user of the API and this results in code that may result in an error. API misuse detection relies on a large
set of mined instances of API misuse. Unfortunately, as Lamothe et al. discovered, mined instances of API misuse
can turn out to be mostly false positives [15].

Lamothe et al. addressed this problem of false positives with a data augmentation strategy by automatically
producing complementary examples of API usage. Kang et al. addressed this problem with an active learning
strategy where diﬀerent API usage examples are selected for manual inspection based on their informativeness
[12]. Since API misuse can lead to vulnerabilities or failures, it is exciting to see work to more eﬀectively mine
instances of API misuse.

4 MACHINE LEARNING LIBRARIES
While there has been a larger trend of machine learning (ML) with and for software engineering (SE), several
papers tackled the more speciﬁc issue of testing and debugging ML libraries. While some past approaches for
detecting bugs in deep learning libraries use diﬀerential testing (i.e., testing whether similar functions have the
same functionality between diﬀerent libraries), a noted issue with this is the reliance on multiple libraries for the
same set of functionality. As an example, both Pytorch and Tensorﬂow and popular deep learning libraries that
provide similar functionality but with diﬀerent APIs. Several within-library diﬀerential testing techniques were
suggested. EAGLE [31] deﬁnes equivalence rules and develops equivalent graphs that should yield the same
results. The functionality of these equivalent graphs is veriﬁed to be identical. FreeFuzz takes the approach of
mining usage of functions from open source code and carries out diﬀerential testing between similar concepts
(e.g., CPU vs GPU computation should yield the same results) [33]. EAGLE and FreeFuzz are able to develop
bugs, but another issue with debugging machine learning libraries is that these are frequently updated, and
thus, the expected functionality of various methods may change over time. DiﬀWatch [2] addresses this issue
by identifying diﬀerential tests and automatically detecting changes in deep learning libraries that might aﬀect
the expected outcome of the tests. This can prompt developers to update their tests accordingly.

DeepStability tackles a subtler type of bugs - those having to do with mathematical instability. Kloberdanz et
al. studied a number of commit messages in Pytorch that indicated a bug related to numerical instability [14].
They manually veriﬁed what type of commit it was, whether it was related to numerical instability, and whether
it was a new unit test or a ﬁx to a previous bug. The authors characterized the root causes and the patches for the
discovered issues and used this information to identify numerical instability in other functions such as cosine
similarity.

Another set of research addressed the issue of debugging the use of deep learning libraries. Keeper [30] is a
tool for testing software using cognitive ML APIs that utilizes pseudo-inverse functions, symbolic execution, and
automatic generation of test inputs to fulﬁll branch coverage and identify root causes of bugs. DeepDiagnosis
[32] is an approach for debugging DNN programs by identifying and localizing faults, such as an exploding tensor
or loss not decreasing during training, as well as suggesting ﬁxes for the root causes of the faults. Another work

A Retrospective on ICSE 2022

•

3

[28] proposes enabling ML libraries to search for hyperparameter conﬁgurations that encourage learning a fair
model, given the dataset.

5 ML IS AUTOMATING SE
The automation of various software engineering (SE) tasks by using machine learning (ML) has been a research
trend for many years. At ICSE 2022, we observed speciﬁc themes of work around explainability, variables, and
automatic program repair.

5.1 Explainable Models of Code
One topic that came up multiple times was the topic of explainability. The idea was often that if we really want
to use ML for SE applications, we need to be able to explain to humans why an ML model’s prediction about
some code should be trusted.

Regarding ML models for program repair, Noller et al. surveyed developers and discovered that not only do
most current program repair tools not produce high-quality patches but also developers do not seem interested
in human-in-the-loop interaction to collaboratively develop better patches [20]. Regarding summarizing code
diﬀs, Cito et al. develop a method for producing counterfactual explanations for models of code [5]. The focus of
this work is models for automated code review that predict some metric such as performance given a code diﬀ.
In automated code review, counterfactual explanations are critical because humans would want to understand
what aspect of the diﬀ causes a predicted performance regression for example.

5.2 Variables
There was also quite a bit of research centered on leveraging variables and their names to develop eﬀective
models of code. Yang et al. focused on adversarially training models of code by changing variable names [37].
Variable names are changed by greedily selecting more tokens that were more inﬂuential for correct predictions.
Chen et al. focused more speciﬁcally on pretraining the representations of individual variable names [4]. They
determined similarity of variable names and apply contrastive loss to learn a more useful representation of
variable names. Finally, Patra et al. presented work on using a neural classiﬁcation model to classify learned
representations of variable names and values as consistent or inconsistent [21].

5.3 Automatic Program Repair
There have been multiple successful approaches to automatic program repair (APR). Given this, Noller et al.
interviewed software practitioners to determine the usability and acceptability of current APR approaches [20].
The results of the survey indicated several issues with current APR approaches, listed below:

• Finding patches automatically can be time-consuming. Software practitioners prefer patches to be found

within 2 hours.

• Software practitioners prefer fewer than ﬁve patches to be generated to reduce the amount that needs to

be reviewed.

• While software practitioners generally want some interaction, they prefer low-interaction methods.
• APR methods may generate patches that overﬁt to a speciﬁc test suite.
• The inability to explain patches makes software practitioners reluctant to trust them. Additional tests

generated by APR methods would help practitioners trust the accuracy of the patch.

These ﬁndings will help direct future research and development of APR techniques, and several other papers
presented at the conference were directly targeted towards these issues. PropR incorporates property-based
testing into APR. Property-based tests are tests deﬁned by properties of input-output pairs [8]. Thus, an inﬁnite

4

• Cailin Winston, Caleb Winston, Chloe Winston, Claris Winston, Cleah Winston

number of tests can theoretically be produced, where the properties of the output are veriﬁed. PropR is imple-
mented in Haskell and creates patches faster, and importantly patches that do not overﬁt due to the additional
property testing.

PropR utilizes underlying semantics-based repair techniques. However, deep learning approaches have been
explored too. The complexity of these approaches though present several challenges, out of which two were
directly addressed by papers this year.

• Deep learning approaches to this point have only been capable of generating single-line patches. DEAR,
however, uses a divide-and-conquer approach to successfully create multi-line and multi-hunk patches
[16].

• Deep learning approaches require a loss function to minimize. The typical loss function used for APR
techniques is based on syntactical diﬀerences between generated and “ground-truth” patches. This means
that patches that do not even compile may have a lower computed loss than slightly incorrect patches
that do compile. This is likely one of the reasons behind the low-quality patches generated through deep
learning approaches. RewardRepair developed a new loss function that executes predicted patches and
penalizes patches that do not compile and thereby enables improvement over state-of-the-art approaches
[38].

These results are promising; however, a discussion with the authors of the above papers revealed that there

are a number of uncertainties in the future of deep learning-based APR methods.

• Deep learning is fundamentally data-driven. Thus, large quantities of data are needed to train these models,
and this is diﬃcult to obtain because it requires large quantities of bugs and associated patches. This being
said, the ideas of property testing as used in PropR can be applied to augment training datasets.

• As mentioned above, deep learning approaches so far do not perform well on APR tasks, yet DEAR and

RewardRepair have taken steps towards tackling these issues.

• There is also a concern about the sustainability of deep learning in general due to the resource consumption

required for training models. This is an active area of research though in deep learning.

• Deep learning approaches may also learn a limited type of patch based on the types of patches in the

training dataset. For this issue as well, the ideas behind PropR may be applicable.

Overall, it is possible that a combination of symbolic, semantics, and deep learning based approaches may prove
eﬀective. For example, deep learning approaches could be used to repair programs viewed from the semantics,
whereas program synthesis approaches are used to generate code from the repaired semantics.

6 SOFTWARE ENGINEERING COMMUNITIES
A theme of research presented at ICSE 2022 was studying the communities surrounding software engineering.
Examples of communities included GitHub, StackOverﬂow, and Twitter.

6.1 Open Source Software (GitHub)
GitHub represents some of the largest software engineering communities. Given its open nature, it has proven
particularly accessible to researchers to investigate SE communities.

It’s important to foster a safe and positive environment in open source GitHub projects. Miller et al. and Qiu et
al. explored the toxicity that often surrounds open source communities such as on GitHub [18, 23]. The authors
have described the attitude of unreasonably high expectations towards open source developers, and analyzed
toxic GitHub issue discussions and interpersonal conﬂict in code review which result from this. The toxicity that
exists in OSS is shown to be diﬀerent from the toxicity that exists in other social media platforms such as Reddit,
so doing more analysis and ﬁnding ways to detect toxic behavior in OSS, is a ﬁrst step to being able to mitigate
this attitude.

A Retrospective on ICSE 2022

•

5

On a similar note, positively welcoming newcomers into such OSS projects is very important. Guizani et al
has worked on creating a maintainer dashboard that works to attract and retain new contributors by highlight-
ing achievements of contributors encouraging maintainers to recognize new contributors [9]. Currently, many
GitHub repositories have bots for pull requests. Russel et al. discussed the noisiness of these kinds of bots, and
how they can be overwhelming for newcomers to GitHub repositories - instead they suggested creating better
ways to represent the information of bots, such as clearer summaries [34].

Another aspect that was discussed is gender diversity in OSS. Studies demonstrated the current gender
imbalance in open source contributions. Rossi et al. and Prana et al. conducted a large global study of how the
gender gap of contributions in OSS has changed over the years, and in diﬀerent worldwide regions [22, 24]. They
found that over the last 20 years, the gender gap has been gradually decreasing worldwide. However, over the
COVID-19 pandemic, while the contributions by male participants has not changed much, there was a signiﬁcant
decrease in the contributions of female participants to OSS, increasing the gender gap. Again, these patterns were
consistent across worldwide regions. While the general trends were similar, they found that certain regions such
as regions in Africa have been experiencing a growth in gender diversity faster than others. They noted while
there has been much positive change in the last decade or two, there is still much work to be done to reduce the
gender gap in OSS.

Much of this research leverages open-source repositories but doesn’t venture into closed-source or internal
software. A question was raised to the above authors in Q&A following their presentations about how their
research might translate to closed-source projects. Separately, Qiu et al. compared GitHub open-source projects
and Google internal code review to determine how detecting toxic behavior varies between open- and closed-
source [23]. They found that both have platform speciﬁc features so you can’t use the exact same model when
ﬁnding toxic comments in open source projects or pushback in internal code review.

6.2 Platforms for Discussion (StackOverflow and Twitter)
StackOverﬂow is a popular resource for programmers, and there were a couple papers that delved into how
StackOverﬂow is used. Some research discussed how expert programmers use stack overﬂow in a diﬀerent and
usually more eﬀective way than novices - based on this, they suggested that it may be worthwhile to consider
teaching students how to properly look for online help on sites such as stack overﬂow. Somewhat relatedly to
StackOverﬂow, Xu et al. proposed Post2Vec which leveraged tags and other metadata to learn better representa-
tions of StackOverﬂow [36]. This work could also help developers more easily discover posts on StackOverﬂow.

6.3 Remote Work
Another theme of discussion was how to move to and improve remote work, emphasizing that having a positive
setup to help with accessibility in case some people need to work remotely. Research studied how remote work
aﬀected people’s productivity - the same aspects of remote work that are advantageous to some people, such
as schedule ﬂexibility, may be disadvantageous to others. Some suggestions to help ease the challenges that
come with remote work were creating a 10 minute rule time limit for meeting (starting meetings 10 minutes
after the hour and ending 10 minutes before to thesaurus to give everyone at least a 20 minute break between
meetings, and help with productivity during the meeting), making sure to end virtual meetings on time, and
chatting about personal things virtually to help maintain connection. Another challenge discussed by Santos et
al., is coordination in remote environments - they discussed how coordination depends on cohesion [26].

6

• Cailin Winston, Caleb Winston, Chloe Winston, Claris Winston, Cleah Winston

7 EDUCATION
Instructors from diﬀerent universities share techniques that they have used to teach software engineering and
design. Many papers at ICSE 2022 emphasized the importance of bringing software engineering into the class-
room and described techniques that can be used when teaching software engineering and design.

7.1 Techniques for Teaching Software Engineering
Through various papers and discussion sessions, many points were brought up about techniques to teach soft-
ware engineering and design.

• Peer feedback. Providing peer feedback to students can assist them in learning software design. As Ruk-
mono et al. discusses, students appreciate both positive feedback and critical feedback that is speciﬁc with
examples [25]. They also learned that students should be taught how to provide useful peer review to
other student’s projects and that students who are more engaged with the course material are more likely
to implement feedback from peer review.

• Enforcing certain guiding principles in design. One professor suggested thinking about the inclusiv-
ity, security, and fairness of the user as a guiding principle. Another suggestion was to encourage students
to think more about users when designing software, and to carefully consider any assumptions made about
them. Students should be encouraged to think about the end user of the software, as well as maintain code
that is readable by everyone.

• Some professors discussed an assignment, where students would look at a real system that is currently
used, and document its software design. While it can be challenging to ﬁnd the necessary information to
complete such a project, the professors found that it beneﬁted the students to see how software engineering
is applied in the real world.

• Hackathons. Hackathons could also be useful in teaching software engineering. Aﬃa et al. suggests
introducing hackathons to provide students with a means of proving their knowledge of course topics [1].
They also propose spacing out the hackathons through the period of the course instead of hosting one non
required hackathon at the end of the course.

7.2 Broader Ideas for Software Engineering Course Designs
There were also broader ideas for software engineering courses that can be designed.

• Robotic Systems. One idea discussed was teaching more speciﬁc topics with a software engineering
and design focus. Hildebrandt et al. developed a course for undergraduates on software development for
robotics [10]. Students learned about robotics and how to build systems with it, and they were also involved
in focused sessions/assignments for debugging. They suggested that teaching more speciﬁc engineering
topics such as robotics in a focused classroom setting, with an emphasis on software engineering.

• Coding Camps. Moster et al. created a summer coding camp for autistic high school students [19]. In
the course, students learned soft skills, such as kindness, teamwork, proactive communication, and scrum.
These skills, which are often lacked by autistic students, are critical for learning through the remote learn-
ing environment. The researchers chose to used written and pre-recorded videos because students pro-
gressed through course at diﬀerent paces.

• Capstone Courses. Capstone projects are a useful learning complement to the learning environment
because they allow students to understand the link between the concepts they learn in a software en-
gineering class and how they can be utilized in real-world scenarios. However, as Bütt et al. explains,
capstone projects remove the experience of risks experienced by the real-world [3]. Thus, they suggest
introducing an entrepreneurship aspect to capstone projects.

A Retrospective on ICSE 2022

•

7

8 PRESSING CHALLENGES TO SE RESEARCH COMMUNITY
Through paper presentations, follow-up discussions, and Birds of a Feather sessions, several challenges in soft-
ware engineering (SE) research became apparent.

Given the number of papers on SE for ML as well as ML for SE, there were several conversations on what ML
robustness really means from a software engineering and testing point of view. Given the almost inﬁnite space
of inputs and the non-deterministic nature of ML models, testing machine learning systems is challenging, and
it is hard to deﬁne the suﬃciency of testing to achieve robustness.

Another broad issue and concern that was discussed was the issue of replicability. Many recognized that many
published experimental artifacts are not easily reproducible. This is because of a wide range of reasons, including
lack of eﬀort in creating and documenting replication packages and lack of maintenance of these packages. PhD
students mentioned that the incentive is often to publish more papers and to explore new research directions
instead of spending time to create higher quality replication packages and to maintain them after graduation.
Several papers also addressed this concern. Timperley et al. discussed the challenges that aﬀect the quality of
published artifacts and proposed suggestions to mitigate these [27]. Daoudi et al. investigated the reproducibility
of published Android malware detection software and discussed how barriers to reproduction can be broken
down [6]. On a similar note, these challenges also arise for researchers/domain experts in various ﬁelds and
data scientists [7] who may lack software engineering experience. Thus future research to understand how the
software engineering community can help to bridge this gap is important.

Another broad topic discussed was around what is required for industry to adopt tools developed at ICSE
2022 and how academic research can have a larger impact in software engineering. Maybe software engineering
researchers should have a foot in industry to better understand the risks and challenges of adopting new tools.
On the other hand, maybe more software developers and project managers from industry settings should attend
research conferences such as ICSE. Those working in industry could also host grand challenges or otherwise
spread word of challenged that need to be overcome in practice. An issue with this though was that this could
bias the research space to a subset of all the problems that need to be addressed. Maybe industry needs to be
more willing to adopt new tools and technologies.

These challenges are important for SE researchers to consider in their future research, but from these discus-

sions, it is also apparent that the solutions to these challenges are still unclear.

9 RESEARCH FROM UW
UW researchers contributed to ICSE 2022 in the areas of reliability and safety, SE communities, and mutation
testing. Winston et al. investigated repairing machine-learning-based brain-computer interfaces with fault lo-
calization [35], Liang et al. explored mining OSS skills from GitHub [17], and Kaufman et al. presented work on
prioritizing mutants in mutation testing [13].

REFERENCES
[1] Abasi-amefon Obot Aﬃa, Alexander Nolte, and Raimundas Matulevičius. 2022. Integrating Hackathons into an Online Cybersecurity

Course. arXiv preprint arXiv:2202.06018 (2022).

[2] Jinqiu Yang Alexander Prochnow. 2022. Diﬀ Watch: Watch Out for the Evolving Diﬀerential Testing in Deep Learning Libraries. In 2022

IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion).

[3] Ethan Bütt, Suzette Person, and Christopher Bohn. 2022. Student-Sponsored Projects in a Capstone Course. (2022).
[4] Qibin Chen, Jeremy Lacomis, Edward J Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues. 2021. VarCLR: Variable

Semantic Representation Pre-training via Contrastive Learning. arXiv preprint arXiv:2112.02650 (2021).

[5] Jürgen Cito, Isil Dillig, Vijayaraghavan Murali, and Satish Chandra. 2021. Counterfactual Explanations for Models of Code. arXiv

preprint arXiv:2111.05711 (2021).

[6] Nadia Daoudi, Kevin Allix, Tegawendé F Bissyandé, and Jacques Klein. 2021. Lessons learnt on reproducibility in machine learning

based Android malware detection. Empirical Software Engineering 26, 4 (2021), 1–53.

8

• Cailin Winston, Caleb Winston, Chloe Winston, Claris Winston, Cleah Winston

[7] Will Epperson, April Yi Wang, Robert DeLine, and Steven M Drucker. 2022. Strategies for Reuse and Sharing among Data Scientists in

Software Teams. (2022).

[8] Matthías Páll Gissurarson, Leonhard Applis, Annibale Panichella, Arie van Deursen, and David Sands. 2022. PropR: Property-Based

Automatic Program Repair. In The 44th IEEE/ACM In-ternational Conference on Software Engineering (ICSE). IEEE/ACM.

[9] Mariam Guizani, Thomas Zimmermann, Anita Sarma, and Denae Ford. 2022. Attracting and Retaining OSS Contributors with a Main-

tainer Dashboard. arXiv preprint arXiv:2202.07740 (2022).

[10] Carl Hildebrandt, Meriel von Stein, Trey Woodlief, and Sebastian Elbaum. 2022. Preparing Software Engineers to Develop Robot

Systems. (2022).

[11] Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, and Marsha Chechik. 2022. If a Human Can See It,

So Should Your System: Reliability Requirements for Machine Vision Components. arXiv preprint arXiv:2202.03930 (2022).

[12] Hong Jin Kang and David Lo. 2021. Active learning of discriminative subgraph patterns for API misuse detection. IEEE Transactions

on Software Engineering (2021).

[13] Samuel Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, and René Just. 2022. Prioritizing mutants to guide mutation testing.

(2022).

[14] Eliska Kloberdanz, Kyle G Kloberdanz, and Wei Le. 2022. DeepStability: A Study of Unstable Numerical Methods and Their Solutions

in Deep Learning. arXiv preprint arXiv:2202.03493 (2022).

[15] Maxime Lamothe, Heng Li, and Weiyi Shang. 2021. Assisting Example-based API Misuse Detection via Complementary Artiﬁcial

Examples. IEEE Transactions on Software Engineering (2021).

[16] Yi Li, Shaohua Wang, and Tien N Nguyen. 2022. DEAR: A Novel Deep Learning-based Approach for Automated Program Repair. arXiv

preprint arXiv:2205.01859 (2022).

[17] Jenny Liang, Thomas Zimmermann, and Denae Ford. 2022. Towards Mining OSS Skills from GitHub Activity. (2022).
[18] Courtney Miller, Sophie Cohen, Daniel Klug, Bogdan Vasilescu, and Christian Kästner. 2022. “Did You Miss My Comment or What?”

Understanding Toxicity in Open Source Discussions. In In 44th International Conference on Software Engineering (ICSE’22).

[19] Makayla Moster, Ella Kokinda, Matthew Re, James Dominic, Jason Lehmann, Andrew Begel, and Paige Rodeghero. 2022. “Can You

Help Me” An Experience Report of Teamwork in a Game Coding Camp for Autistic High School Students. (2022).

[20] Yannic Noller, Ridwan Shariﬀdeen, Xiang Gao, and Abhik Roychoudhury. 2022. Trust Enhancement Issues in Program Repair. In

Proceedings of the ACM/IEEE 44th International Conference on Software Engineering.

[21] Jibesh Patra and Michael Pradel. 2021. Nalin: Learning from Runtime Behavior to Find Name-Value Inconsistencies in Jupyter Note-

books. arXiv preprint arXiv:2112.06186 (2021).

[22] Gede Artha Azriadi Prana, Denae Ford, Ayushi Rastogi, David Lo, Rahul Purandare, and Nachiappan Nagappan. 2021. Including every-
one, everywhere: Understanding opportunities and challenges of geographic gender-inclusion in oss. IEEE Transactions on Software
Engineering (2021).

[23] Huilian Sophie Qiu, Bogdan Vasilescu, Christian Kästner, Carolyn Denomme Egelman, Ciera Nicole Christopher Jaspan, and Emer-
son Rex Murphy-Hill. 2022. Detecting Interpersonal Conﬂict in Issues and Code Review: Cross Pollinating Open-and Closed-Source
Approaches. (2022).

[24] Davide Rossi and Stefano Zacchiroli. 2022. Worldwide Gender Diﬀerences in Public Code Contributions. arXiv preprint arXiv:2202.07278

(2022).

[25] Satrio Adi Rukmono and Michel RV Chaudron. 2022. Guiding Peer-feedback in Learning Software Design using UML. (2022).
[26] Ronnie E Santos and Paul Ralph. 2022. A Grounded Theory of Coordination in Remote-First and Hybrid Software Teams. arXiv preprint

arXiv:2202.10445 (2022).

[27] Christopher S Timperley, Lauren Herckis, Claire Le Goues, and Michael Hilton. 2021. Understanding and improving artifact sharing

in software engineering research. Empirical Software Engineering 26, 4 (2021), 1–41.

[28] Saeid Tizpaz-Niari, Ashish Kumar, Gang Tan, and Ashutosh Trivedi. 2022. Fairness-aware Conﬁguration of Machine Learning Libraries.

arXiv preprint arXiv:2202.06196 (2022).

[29] Kevin Sullivan Trey Woodlief, Sebastian Elbaum. 2022. Semantic Image Fuzzing of AI Perception Systems. In 2022 IEEE/ACM 44th

International Conference on Software Engineering: Companion Proceedings (ICSE-Companion).

[30] Chengcheng Wan, Shicheng Liu, Sophie Xie, Yifan Liu, Henry Hoﬀmann, Michael Maire, and Shan Lu. 2022. Automated Testing of

Software that Uses Machine Learning APIs. (2022).

[31] Jiannan Wang, Thibaud Lutellier, Shangshu Qian, Hung Viet Pham, and Lin Tan. 2022. EAGLE: Creating Equivalent Graphs to Test

Deep Learning Libraries. (2022).

[32] Mohammad Wardat, Breno Dantas Cruz, Wei Le, and Hridesh Rajan. 2021. DeepDiagnosis: Automatically Diagnosing Faults and

Recommending Actionable Fixes in Deep Learning Programs. arXiv preprint arXiv:2112.04036 (2021).

[33] Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang. 2022. Free Lunch for Testing: Fuzzing Deep-Learning Libraries from

Open Source. arXiv preprint arXiv:2201.06589 (2022).

A Retrospective on ICSE 2022

•

9

[34] Mairieli Wessel, Ahmad Abdellatif, Igor Wiese, Tayana Conte, Emad Shihab, Marco A Gerosa, and Igor Steinmacher. 2022. Bots for Pull
Requests: The Good, the Bad, and the Promising. In Proceedings of the 44th ACM/IEEE International Conference on Software Engineering
(ICSE’22), Vol. 26. ACM/IEEE, 16.

[35] Cailin Winston, Caleb Winston, Chloe Winston, Claris Winston, and Cleah Winston. 2022. Repairing Brain-Computer Interfaces with

Fault-based Data Acquisition. arXiv preprint arXiv:2203.10677 (2022).

[36] Bowen Xu, Thong Hoang, Abhishek Sharma, Chengran Yang, Xin Xia, and David Lo. 2021. Post2vec: Learning distributed representa-

tions of Stack Overﬂow posts. IEEE Transactions on Software Engineering (2021).

[37] Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022. Natural Attack for Pre-trained Models of Code. arXiv preprint arXiv:2201.08698

(2022).

[38] He Ye, Matias Martinez, and Martin Monperrus. 2021. Neural Program Repair with Execution-based Backpropagation. arXiv preprint

arXiv:2105.04123 (2021).

