SN Computer Science (2021) 2:470
https://doi.org/10.1007/s42979-021-00885-1

Neural Networks with `A La Carte Selection of Activation
Functions

Moshe Sipper

2
2
0
2

n
u
J

4
2

]
E
N
.
s
c
[

1
v
6
6
1
2
1
.
6
0
2
2
:
v
i
X
r
a

Received: June 27, 2022/ Accepted: date

Abstract Activation functions (AFs), which are pivotal to the success (or fail-
ure) of a neural network, have received increased attention in recent years, with
researchers seeking to design novel AFs that improve some aspect of network per-
formance. In this paper we take another direction, wherein we combine a slew of
known AFs into successful architectures, proposing three methods to do so ben-
eﬁcially: 1) generate AF architectures at random, 2) use Optuna, an automatic
hyper-parameter optimization software framework, with a Tree-structured Parzen
Estimator (TPE) sampler, and 3) use Optuna with a Covariance Matrix Adapta-
tion Evolution Strategy (CMA-ES) sampler. We show that all methods often pro-
duce signiﬁcantly better results for 25 classiﬁcation problems when compared with
a standard network composed of ReLU hidden units and a softmax output unit.
Optuna with the TPE sampler emerged as the best AF architecture-producing
method.

Keywords Artiﬁcial neural network · Activation function

`a la carte: according to a menu or list that prices items separately
cf. table d’hˆote: a meal served to all guests at a stated hour and ﬁxed price

— Merriam-Webster Dictionary

1 Introduction

Activation functions (AFs) are crucial to the success (or failure) of a neural network
(witness the rise in recent years of the ReLU function at the expense of the sigmoid
function—see Section 2). With the growing success of deep neural networks over
the past decade, AFs have been receiving increased attention, with researchers
seeking to design better ones and create networks with better AF layers.

M. Sipper
Department of Computer Science, Ben-Gurion University, Beer Sheva 84105, Israel
E-mail: sipper@gmail.com, http://www.moshesipper.com/
©The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd 2021

 
 
 
 
 
 
2

Moshe Sipper

Herein, we propose to take a diﬀerent route: We then seek ways by which to
build AF layers that can outperform a standard neural-network architecture. We
call our approach `a la carte, for activation layers: choose, arrange, train, et voil`a.

We propose three methods by which AFs can be combined: 1) randomly,
2) through a state-of-the-art hyper-parameter optimizer with a Tree-structured
Parzen Estimator (TPE) sampler, and 3) via said optimizer with a Covariance
Matrix Adaptation Evolution Strategy (CMA-ES) sampler.

In the next section we survey the literature on AF design, followed by a pre-
sentation of our experimental setup in Section 3. In Section 4 we delineate the
results and discuss them. Finally, we oﬀer concluding remarks in Section 5.

2 Activation Functions: Previous Work

A recent survey compared trends in AFs both in research and practice of deep
learning [18]. They examined the following 21 AFs: Sigmoid, HardSigmoid, SiLU,
dSiLU, Tanh, Hardtanh, Softmax, Softplus, Softsign, ReLU, LReLU, PReLU,
RReLU, SReLU, ELU, PELU, SELU, Maxout, Swish, ELiSH, and HardELiSH (for
precise deﬁnitions the reader is referred to [18]). Highly popular architectures—
including AlexNet, ZFNet, VGGNet, SegNet, GoogleNet, SqueezeNet, ResNet,
ResNeXt, MobileNets, and SeNet—were shown to have a common AF setup, with
Rectiﬁed Linear Units (ReLUs) in the hidden layers and Softmax units in the out-
put layer. Indeed, in the next section we will set this as our standard architecture
for comparison purposes. [18] stated that, “The most notable observation on the
use of AFs for DL applications is that the newer activation functions seem to out-
perform the older AFs like the ReLU, yet even the latest DL architectures rely on
the ReLU function.” [18]

AF approaches can be divided into three categories: tunable or learnable AFs,

ensemble methods, and theoretically motivated AFs.

Tunable AFs. A very recent survey focused on modern trainable AFs [3]. They pro-
posed a two-class taxonomy comprising ﬁxed-shape AFs and trainable AFs. The
former included all the classical AFs, such as sigmoid, tanh, ReLU, and so forth,
while the latter class contained AFs whose shape is learned during the training
phase. The ﬁxed-shape class was further divided into classic and rectiﬁed-based,
and the trainable class was further divided into parameterized standard AFs and
AFs based on ensemble methods. Finally, they looked at trainable non-standard
neuron deﬁnitions, which “change the standard deﬁnition of neuron computation
but are considered as neural network models with trainable activation functions
in the literature” (e.g., maxout). They noted that in many cases improved perfor-
mance could be gained by using trainable AFs rather than ﬁxed-shape AFs.

[27] introduced SPLASH units (Simple Piecewise Linear and Adaptive with
Symmetric Hinges), a class of learnable AFs. SPLASH units are continuous, grounded,
and use symmetric hinges that are placed at ﬁxed locations, derived from the data.
Compared to nine other learned and ﬁxed AFs, SPLASH units showed superior
performance across three datasets and four architectures.

[6] presented an eﬃcient computational solution to train deep neural networks

with learnable AFs, speciﬁcally focusing on deep spline networks.

Neural Networks with `A La Carte Selection of Activation Functions

3

[13] observed that small variations to activation distributions could greatly
aﬀect the semantic feature representations in 1-bit CNNs, and introduced gen-
eralized AFs with learnable coeﬃcients. Their proposed network attained good
performance at substantially lower computational cost.

[1] designed a novel form of piecewise linear AF that was learned independently
for each neuron using gradient descent, and showed improvement over deep neural
networks composed of static rectiﬁed linear units.

[12] described an extreme learning machine (ELM) with a tunable AF that was
tuned by means of diﬀerential evolution based on the input data. The tunable AF
was of the form G(S, α), with S being the inner product of input and weights com-
bined with bias, and α being the tunable parameter. They demonstrated improved
performance over previous versions of ELM.

[7] presented trained AFs, where an AF was trained for each particular neuron
by linear regression. A diﬀerent AF was generated for each neuron in the hidden
layer. The approach was used in random weight artiﬁcial neural networks (RWNs),
attaining success rates surpassing RWNs that used traditional AFs.

[24] introduced a novel family of ﬂexible AFs based on an inexpensive kernel
expansion at every neuron. They showed that their kernel activation function
(KAF) networks outperformed competing approaches on a number of benchmark
problems.

Some older papers include [25], who introduced a multi-output neural model
with a tunable, AF, f(S, a), with S being the inner product of input and weights,
and a being a tunable parameter that could change during training. [22] presented
a Look-Up-Table (LUT) AF, oﬀering a preliminary study of an adaptive LUT-
based neuron (L-neuron) over some (now-considered) simple problems (XOR, 4-bit
parity, and 8-bit parity).

Ensemble methods. [14] recently presented an ensemble of convolutional neural net-
works (CNNs) trained with diﬀerent AFs, and also introduced a new AF—the
Mexican Linear Unit. They showed that an ensemble of multiple CNNs that only
diﬀered in the AFs outperformed the results of the single CNNs and of naive
ensembles made of ReLU networks.

[17] proposed AF ensembling by majority voting, wherein a single neural net-
work is trained with ﬁve diﬀerent AFs. They showed that model accuracy could
be improved for four datasets—MNIST, Fashion MNIST, Semeion, and ARDIS
IV—over methods including CNN, RNN (Recurrent Neural Network), and SVM
(Support Vector Machine).

Theoretically motivated AFs. [11] introduced a number of new AFs, including: gen-
eralized swish, mean-swish, ReLU-swish, triple-state swish, sigmoid-algebraic, triple-
state sigmoid, exponential swish, sinc-sigmoid, and derivative of sigmoid AFs.
They compared the proposed AFs with some well-known and recently proposed
AFs. They found that, “New AFs can combine the advantages of predeﬁned ones
and so they perform better than the older ones.”

[23] introduced the tent AF and showed that tents make deep learning mod-
els more robust to adversarial attacks. The constraining tent AF has the form:
f(x; δ) = max(0, δ − |x|); as noted by the authors, “it is basically built from two
ReLUs where a ReLU and a horizontally ﬂipped ReLU share the same distance
(δ) from the origin.”

4

Moshe Sipper

[19] investigated the approximation ability of deep neural networks with a
broad class of frequently used AFs. They derived the required depth, width, and
sparsity of a deep neural network to approximate any H¨older smooth function up
to a given approximation error.

[15] proposed a novel AF called Mish, deﬁned as f(x) = x · tanh(softplus(x)),

and showed its merits over other AFs on a number of challenging datasets.

[9] proposed a novel method to train neural networks with AFs that strongly
saturate when their input is large. This was achieved by injecting noise to the AF
in its saturated regime and learning the level of noise. They showed that their
noisy AFs were easier to optimize, and also attained better test errors, since the
noise injected to the activations also regularized the model.

[8] proposed three new AFs: complementary log-log, probit, and log-log. They
compared networks using these AFs to networks using classical AFs, over ﬁnancial
time series datasets. They concluded with recommendations of when to apply these
new AFs given the type of time series data.

Having examined the literature we conclude that research into activation func-
tions, much of it very recent, clearly shows a thriving interest in veering away from
traditional, popular AFs, attempting to ﬁnd better networks through improved
AFs and AF layers. This line of research is mostly concerned with proposing new
forms of AFs, with interesting properties that might improve network performance.
Rather than designing a new AF we take another direction, proposing to com-

bine a slew of known AFs, and showing how this can be done advantageously.

3 Experimental Setup

We used Optuna, a state-of-the-art automatic hyper-parameter optimization soft-
ware framework [2] (we also considered Hyperopt [5], but it is more dated and less
potent). Optuna oﬀers a deﬁne-by-run style user API where one can dynamically
construct the search space, and an eﬃcient sampling algorithm and pruning algo-
rithm. Moreover, our experience has shown it to be fairly easy to set up. Optuna
formulates the hyper-parameter optimization problem as a process of minimizing
or maximizing an objective function given a set of hyper-parameters as an input.
As noted by [2], there are generally two types of sampling methods: relational
sampling, which exploits correlations amongst parameters, and independent sam-
pling, which samples each parameter independently. Optuna oﬀers two main sam-
plers, both of which we used herein. Covariance Matrix Adaptation Evolution
Strategy (CMA-ES) performs relational sampling, while Tree-structured Parzen
Estimator (TPE) performs independent sampling. Optuna also provides pruning:
automatic early stopping of unpromising trials [2].

We implemented our experiments in PyTorch, a popular deep learning frame-
work [20]. We selected 44 potential AFs, some of which are “oﬃcially” deﬁned in
torch.nn as AFs (e.g., ReLU and Sigmoid), while the others are simply mathe-
matical functions over tensors (e.g., Abs and Sin). Note that where the latter are
concerned there might be some question as to their diﬀerentiability (PyTorch per-
forms automatic diﬀerentiation—indeed this feature is one of its main strengths).
However, in practice, this does not pose a problem since the number of points at
which the function is non-diﬀerentiable is usually (inﬁnitely) small. For example,

Neural Networks with `A La Carte Selection of Activation Functions

5

PyTorch AFs: ELU, Hardshrink, Hardtanh, LeakyReLU, LogSigmoid, PReLU, ReLU, ReLU6,
RReLU, SELU, CELU, GELU, Sigmoid, Softplus, Softshrink, Softsign, Tanh, Tanhshrink,
Softmin, Softmax, LogSoftmax

PyTorch Math: Abs, Acos, Angle, Asin, Atan, Ceil, Cos, Cosh, Digamma, Erf, Erfc,
Exp, Floor, Frac, GumbelSoftmax, Log, Log10, Neg, Round, Sin, Sinh, Tan, Trunc

New AFs culled from the literature:
Mish: f (x) = x · tanh(softplus (x)) [15]
Generalized Swish: f (x) = x · sigmoid(e−x) [11]
Sigmoid Derivative: f (x) = e−x · (sigmoid (x))2 [11]
CLogLogM: f (x) = 1 − 2 · e−0.7·ex

[8]

Fig. 1 List of 48 AFs used herein. For full descriptions the reader is referred to pytorch.org
and the respective cited papers.

ReLU is non-diﬀerentiable at zero, however, in practice, it is rare to stumble often
onto this value, and when we do, an arbitrary value can be assigned (e.g., zero),
with little to no eﬀect on performance.

We also added four top-performing AFs from the literature discussed in Sec-
tion 2. This served the twofold purpose of testing newly proposed AFs and demon-
strating the ease with which our framework can accommodate new AF ideas. The
full list of AFs is given in Figure 1.

For our experiments we selected classiﬁcation datasets from OpenML.org [28],
which boasts over 21,000 datasets (and is constantly growing). Upon examination
we found 465 classiﬁcation datasets with at least 10,000 samples. Of these we
selected 25 datasets, such that we ended up with a variety of number of samples,
features, and classes.

For each dataset we performed 30 replicate runs with 5-layer networks and 30
replicate runs with 10-layer networks. While Optuna could have been tasked with
optimizing the number of layers as well, we chose not to increase the already-large
search space, and focus on AFs alone—our main interest in this paper. We thus
opted for 5- and 10-layer networks, values chosen so as to eschew shallow networks
while still aﬀording extensive testing of our approach through multiple replicates
over multiple datasets.

Each network computational layer was fully connected, and performed the
standard linear computation, y = W x+b, with W being the weight matrix, x being
the input vector, and b being the bias (torch.nn.Linear). This was followed by the
AF (layer) computation. We used the Adam optimizer [10] with a learning rate
of 0.001, cross entropy loss, and a single batch (i.e., the entire training set). The
hidden-layer sizes were set to 64 nodes, with the preceding input and succeeding
output dimensions depending on the particular dataset.

The pseudo-code of the experimental setup is given in Algorithm 1. Each repli-
cate run began with a 70%-30% training/test split of the dataset. We ﬁt scikit-
learn’s [21] StandardScaler to the training set and applied the ﬁtted scaler to the
test set. This ensured that features had zero mean and unit variance (often helpful
where neural networks are concerned).

We then executed and compared four diﬀerent AF architectures:

1. The standard architecture (as seen in Section 2): hidden ReLU units with a
Softmax output. Train the network over the training set and record the score

6

Moshe Sipper

Algorithm 1 Experimental setup (per dataset)

Input:

dataset ← dataset to be used

Output:

Test scores (for each network type, over all replicates)

1: for rep ← 1 to 30 do
2:
3:
4:
5:

Randomly split dataset into 70% training set and 30% test set
Fit StandardScaler to training set and apply ﬁtted scaler to test set
Train a standard network over training set and test over test set
Independently generate 1000 networks with random AFs, train each one over training

set, test each one over test set, retain top test score

6:

7:

Run Optuna with TPE sampler for 1000 trials over training set, test returned best

model over test set

Run Optuna with CMA-ES sampler for 1000 trials over training set, test returned best

model over test set

over the test set as the replicate’s score. The test-set score equalled prediction
accuracy, with the best value being 1 and the worst value being 0.

2. Generate 1000 random networks. A random network is generated by selecting
at random with uniform probability 5 or 10 AFs (for 5 or 10 layers, respectively)
from the list of 48 AFs (Figure 1). Train each random network over the training
set and test it over the test set. Record the best test score of the 1000 random
networks as the replicate’s score.

3. Run Optuna for 1000 trials with a TPE sampler. Set Optuna to optimize as
hyper-parameters the (5 or 10) AFs (for 5 or 10 AF layers). A single trial com-
prises training the network with a particular set of hyper-paremeters supplied
by Optuna (a list of AFs) over the training set. Test the best network returned
by Optuna (after 1000 trials) over the test set, and record this score as the
replicate’s score.

4. As in the previous item, except use Optuna with the CMA-ES sampler.

A network was trained for up to 300 epochs, with early stopping: exit if accu-
racy improvement from one epoch to the next is less than 0.001 for 10 successive
epochs. The code is available at github.com/moshesipper.

[4] came closest to what we propose above. They suggested the use of a genetic
algorithm to combine 11 AFs through the use of 8 operators: addition, subtraction,
multiplication, division, exponentiation, minimum, maximum, and function com-
position. They showed that simple combinations of AFs could be evolved. Their
approach diﬀered in the following from ours: 1) A network was tested with a single
AF, whereas our method is based on a combination of AFs; 2) they tested their
approach on a smaller number of datasets (3 vs. our 25); (3) our set of potential
AFs is signiﬁcantly larger, with far more mathematical operators (which turn out
to be useful, as we shall see below); (4) our AF-layout design employs both a less
powerful method (random) and a more sophisticated method (Optuna) than the
genetic algorithm used therein.

Neural Networks with `A La Carte Selection of Activation Functions

7

4 Results and Discussion

Table 1 shows the results of all 50 experiments, each row summarizing 30 replicate
runs. We performed two tests to asses statistical signiﬁcance: 1) a 10,000-round
permutation test comparing the medians of the respective method’s scores (random
AFs or Optuna-designed) with the standard network’s scores; and 2) a 10,000-
round permutation test comparing the medians of the top method’s scores with
the second-best network’s scores.

At least one of our newly proposed methods (random network, Optuna with
TPE sampler, Optuna with CMA-ES sampler) attained statistically signiﬁcant
improvement in 42 of the 50 experiments. In most cases all three methods were
better. Moreover, in several cases improvement was not merely statistically signif-
icant, but indeed twofold, threefold, and even more.

A statistically signiﬁcant number-one ranking (when compared with the second-
best) was attained by Optuna using a TPE sampler 17 times, and by the random
network 5 times. AF layout design with Optuna would seem to be an excellent
choice when one wishes to improve a deep learner’s performance.

In addition to the results given in Table 1, we were now in possession of 4500
architectures (25 datasets × 30 replicates × 2 layer counts × 3 non-standard
methods). Given the large search space of architectures (485 for 5 layers and 4810
for 10 layers) it was not surprising that almost all 4500 architectures were unique
(the only exceptions were 2 5-layer networks that appeared twice). We could,
however, take a closer at the 33750 AFs within these architectures, and examine
their frequencies—which are given in Table 2.

Examining Table 2, we can make several interesting observations. Firstly, we
note what is not there: the commonly used ReLU and Softmax functions (they do
appear lower down the list, not shown for brevity). We note the preponderance
of usually overlooked mathematical functions, which would seem to be a good
choice for AF candidates, e.g., sine, hyperbolic sine, negative, and exponential.
The diversity of AFs chosen by the methods presented herein is also apparent.
Providing a large set of AF candidates as grist for the AF layout-design mill is
beneﬁcial and can be fruitfully applied. Of the four newly proposed AFs (Figure 1)
only Generalized Swish made it into one of the top-10 rankings.

We asked whether for particular datasets speciﬁc AFs appeared more fre-
quently, an inquiry that led to Table 3. We note that for some datasets certain
AFs were selected with a notably higher frequency. And once again we note the
preponderance of AFs not often used.

The freedom to choose from a large set of possible AFs and compose AF layouts
can produce better-performing networks. It would seem to help reduce error from
a model by avoiding overﬁtting to the training set. As such, it might be acting
as a regularizer, performing a beneﬁcial tradeoﬀ by reducing variance signiﬁcantly
without increasing bias (substantially). This regularization eﬀect of `a la carte is a
conjecture at this point.

5 Concluding Remarks

Presenting deep networks `a la carte, we showed that it is possible to signiﬁcantly
improve performance by randomly choosing non-standard AF architectures. And

8

Moshe Sipper

Table 1 Experimental results. All scores shown are those of the test sets. dataset: OpenML
dataset id; samp: number of samples; feat: number of features; cls: number of classes; lay:
number of network layers; standard: median score over 30 replicates of standard network;
random: median score over 30 replicates of best random network score (of 1000) per replicate;
tpe: median score over 30 replicates of best network per replicate found by Optuna using TPE
sampler; cmaes: median score over 30 replicates of best network per replicate found by Optuna
using CMA-ES sampler; top: method that produced the top median score over 30 replicates.
For random, tpe, cmaes: we performed a 10,000-round permutation test comparing the medians
of the respective method’s scores with the standard network’s scores. For top: we performed
a 10,000-round permutation test comparing the medians of the top method’s scores with the
second-best network’s scores. A ‘!!’ denotes a p-value < 0.001 and a ‘!’ denotes a p-value < 0.05
(otherwise p-value >= 0.05).

dataset

samp

feat

6

20000

32

10992

279

45164

1044

10936

1459

10218

1471

14980

16

16

74

27

7

14

1476

13910

128

1477

13910

129

1478

10299

561

1481

28056

1531

10176

1532

10668

1533

10386

1534

10190

1536

10130

1537

28626

1568

12958

4154

14240

4534

11055

40985

45781

41027

44819

41526

15547

41671

20000

42641

18982

42670

18982

6

3

3

3

3

3

3

8

30

30

2

6

60

20

79

79

cls

26

10

11

3

10

2

6

6

6

18

5

5

5

5

5

5

4

2

2

20

3

2

5

5

5

lay
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10

standard
0.119
0.04
0.436
0.103
0.509
0.509
0.547
0.387
0.206
0.121
0.551
0.55
0.964
0.21
0.975
0.212
0.964
0.192
0.149
0.127
0.962
0.961
0.964
0.964
0.964
0.963
0.963
0.962
0.962
0.962
0.974
0.975
0.649
0.329
0.998
0.998
0.938
0.555
0.062
0.063
0.52
0.514
0.489
0.486
0.559
0.559
0.858
0.278
0.855
0.276

random
0.924 !!
0.829 !!
0.986 !!
0.979 !!
0.961 !!
0.566 !!
0.591 !!
0.513 !!
0.687 !!
0.59 !!
0.878 !!
0.721 !!
0.991 !!
0.983 !!
0.991 !!
0.982 !!
0.98 !!
0.971 !!
0.637 !!
0.463 !!
0.966 !!
0.962
0.968 !!
0.964
0.968 !!
0.964
0.967 !!
0.962
0.967 !!
0.963
0.976 !
0.975
0.985 !!
0.979 !!
1.0 !!
0.998
0.958 !!
0.938 !!
0.068 !!
0.067 !!
0.825 !!
0.776 !!
0.511 !!
0.514 !!
0.569 !!
0.56
0.952 !!
0.904 !!
0.952 !!
0.894 !!

tpe
0.931 !!
0.896 !!
0.986 !!
0.982 !!
0.966 !!
0.623 !!
0.53
0.374 !
0.737 !!
0.625 !!
0.859 !!
0.727 !!
0.991 !!
0.988 !!
0.992 !!
0.988 !!
0.974 !!
0.971 !!
0.699 !!
0.549 !!
0.966 !!
0.962
0.967 !
0.964
0.967 !!
0.964
0.966 !
0.962
0.966 !!
0.963
0.975 !
0.975
0.992 !!
0.992 !!
0.999 !!
0.998
0.96 !!
0.946 !!
0.061 !
0.062 !
0.855 !!
0.795 !!
0.51 !!
0.513 !!
0.581 !!
0.56
0.955 !!
0.928 !!
0.953 !!
0.941 !!

cmaes
0.923 !!
0.817 !!
0.984 !!
0.976 !!
0.96 !!
0.574 !!
0.483
0.422 !!
0.675 !!
0.577 !!
0.852 !!
0.723 !!
0.989 !!
0.983 !!
0.99 !!
0.983 !!
0.976 !!
0.969 !!
0.642 !!
0.41 !!
0.965 !!
0.962
0.967 !!
0.964
0.967 !
0.964
0.965 !
0.962
0.966 !!
0.962
0.975
0.975
0.984 !!
0.978 !!
0.999 !!
0.998
0.956 !!
0.936 !!
0.062
0.062
0.824 !!
0.773 !!
0.511 !!
0.513 !!
0.572 !!
0.56
0.95 !!
0.89 !!
0.951 !!
0.902 !!

top
tpe !
tpe !!
random
tpe !
tpe !!
tpe
random !!
random !!
tpe !!
tpe !
random
tpe
tpe
tpe !!
tpe
tpe !!
random !!
random
tpe !!
tpe !!
tpe
tpe
random
cmaes
random
cmaes
random
tpe
random
tpe
random
tpe
tpe !!
tpe !!
random
standard
tpe
tpe !
random !!
random !!
tpe !!
tpe !
random
random
tpe
random
tpe !
tpe
tpe
tpe !

even better results can be obtained by using a state-of-the-art hyper-parameter
optimizer.

One might inquire as to the cost of searching for better AF layouts. Deep
networks come with a plethora of hyper-parameters that need to be tuned, and
adding AFs into the mix could therefore be cost-eﬀective when compared with

Neural Networks with `A La Carte Selection of Activation Functions

9

Table 2 Frequencies of top-10 AFs in the best networks found by the three methods discussed
in the text (random, Optuna with TPE sampler, Optuna with CMA-ES sampler). Shown by
category: AFs in input layer, AFs in hidden layers, and AFs in output layer.

input
AF
Softshrink
Sinh
Abs
SELU
Angle
Neg
Tanhshrink
Hardshrink
Exp
Hardtanh

5 layers
hidden
AF
Freq
Softshrink
6.7%
6.3%
Abs
5.8%
Sinh
4.9% Hardshrink
Hardtanh
4.5%
4.4%
Exp
3.7%
Sin
Erf
3.5%
3.5%
SELU
3.4%
ReLU6

output
AF
Freq
Exp
6.2%
Sinh
5.0%
4.5%
Cosh
4.5% LogSoftmax
Neg
4.5%
Abs
4.1%
Softplus
4.0%
PReLU
3.8%
SELU
3.6%
RReLU
3.2%

Freq
11.1%
8.1%
7.5%
4.4%
4.0%
4.0%
3.7%
3.6%
3.6%
3.5%

input

AF
Neg
Sinh
Exp
Angle
Sin
SELU
Hardtanh
Erf
Tanh
GeneralizedSwish

10 layers
hidden
Freq
AF
SELU
3.8%
3.8%
Sin
Neg
3.7%
3.5%
Erf
3.4% Hardtanh
3.3%
Exp
CELU
3.3%
3.2%
Tanh
3.1%
Atan
Sinh
2.9%

output
Freq
AF
3.7% GumbelSoftmax
Exp
3.5%
LogSoftmax
3.2%
CELU
3.2%
ELU
3.1%
PReLU
3.0%
Cosh
3.0%
SELU
3.0%
Neg
3.0%
RReLU
3.0%

Freq
9.1%
5.4%
4.2%
4.0%
4.0%
3.8%
3.7%
3.7%
3.5%
3.3%

the possible beneﬁts. Our experience has shown that Optuna is eﬃcient at hyper-
parameter optimization in general, and at optimizing AF layouts in particular.
Ultimately, whether the cost of `a la carte is worthwhile or not depends on how
important it is for the user to improve performance for a particular task.

We believe this work may have broad implications as it provides an automated
means to attain better deep-learning models, with little input from the user (ex-
cept, possibly, adding AF candidates to our own plethora). We can further suggest
a number of avenues for future research:

– Devise other methods for designing AF architectures.
– Seek a better understanding of the dynamics involved. For example, does the
approach indeed perform some form of regularization, as suggested in Sec-
tion 4?

– While we noted that virtually all AF layouts were unique, and were able to
perform an analysis of AF frequencies, it would be of interest to carry this
further. No AF is an island: possibly, certain AFs perform better in the presence
of certain others.

– There might be additional functions that could be beneﬁcially added to the

“bag of AFs” (Figure 1).

– Apply `a la carte to other types of networks, e.g., convolutional neural networks

and recurrent neural networks.

– Go beyond AF selection—to AF construction. As we saw in Section 2, re-
searchers have suggested tailored combinations of previous AFs (or functions
that are not AFs), and [4] showed that evolving simple, new AFs is possible.
Perhaps this AF-design process could be further enhanced, as we did for dif-

10

Moshe Sipper

Table 3 Frequencies of topmost AF in the best networks found by all three methods, by
dataset. ds: OpenML dataset id; lay: number of network layers. Shown by category: AFs in
input layer, AFs in hidden layers, and AFs in output layer.

ds

6

32

279

1044

1459

1471

1476

1477

1478

1481

1531

1532

1533

1534

1536

1537

1568

4154

4534

40985

41027

41526

41671

42641

42670

lay

5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10
5
10

AF
input
Sinh
Tanh
Exp
Sinh
Sinh
Sin
Round
ReLU
Angle
Angle
Angle
ReLU6
Abs
LeakyReLU
Abs
Erf
Cos
Cos
Sinh
CELU
Softshrink
Trunc
Tanhshrink
Sinh
Tanhshrink
Exp
Tanhshrink
Cosh
Tanhshrink
Cosh
Sinh
Exp
Hardtanh
Exp
Softshrink
Abs
Abs
RReLU
Tan
Digamma
Exp
Hardtanh
Softshrink
Softshrink
SELU
Cosh
Softshrink
Angle
Softshrink
Angle

Freq

13.3%
10.0%
18.9%
10.0%
7.8%
5.6%
17.8%
6.7%
43.3%
7.8%
13.3%
5.6%
28.9%
11.1%
31.1%
7.8%
30.0%
12.2%
23.3%
7.8%
13.3%
6.7%
12.2%
8.9%
14.4%
5.6%
11.1%
7.8%
12.2%
6.7%
13.3%
8.9%
8.9%
8.9%
16.7%
11.1%
40.0%
5.6%
22.2%
11.1%
21.1%
8.9%
34.4%
13.3%
12.2%
6.7%
10.0%
8.9%
11.1%
10.0%

AF
hidden
Exp
SELU
Exp
Exp
Sinh
Digamma
Abs
SELU
Exp
Angle
Abs
Abs
Erfc
SELU
Abs
ELU
Cos
Abs
Sin
Exp
Softshrink
Digamma
Softshrink
Digamma
Hardshrink
Ceil
Softshrink
Digamma
Hardshrink
Digamma
Softshrink
Digamma
Abs
Atan
Hardshrink
Neg
Abs
Sin
Hardtanh
Digamma
Softshrink
Erf
Softshrink
Softshrink
SELU
GumbelSoftmax
Abs
Erf
Abs
SELU

Freq

11.9%
5.7%
12.2%
5.3%
8.1%
5.3%
15.2%
4.4%
9.6%
6.8%
11.9%
5.1%
10.0%
6.2%
9.6%
5.1%
6.3%
6.4%
10.7%
4.6%
11.9%
3.9%
11.5%
5.1%
10.0%
3.5%
12.6%
3.8%
10.7%
5.1%
9.6%
3.6%
13.3%
4.7%
12.2%
3.8%
11.9%
6.0%
8.1%
5.6%
8.9%
4.7%
22.6%
12.2%
9.6%
6.1%
6.7%
6.1%
8.9%
7.4%

AF
output
Exp
CELU
Sinh
CELU
Exp
SigmoidDerivative
CELU
GumbelSoftmax
Softplus
CELU
Atan
GumbelSoftmax
Exp
Exp
Exp
ELU
Exp
GumbelSoftmax
Exp
Cosh
SELU
CELU
Neg
Mish
Abs
ELU
PReLU
Softshrink
LogSoftmax
Softshrink
ReLU
ReLU
Sinh
Exp
GumbelSoftmax
Floor
Abs
GumbelSoftmax
GumbelSoftmax
GumbelSoftmax
Neg
GumbelSoftmax
Softmin
GumbelSoftmax
GumbelSoftmax
GumbelSoftmax
Exp
Exp
Exp
PReLU

Freq

24.4%
12.2%
14.4%
11.1%
25.6%
11.1%
11.1%
28.9%
20.0%
8.9%
11.1%
40.0%
16.7%
11.1%
18.9%
11.1%
15.6%
11.1%
16.7%
15.6%
8.9%
5.6%
8.9%
7.8%
6.7%
6.7%
10.0%
6.7%
10.0%
6.7%
8.9%
6.7%
27.8%
11.1%
20.0%
5.6%
12.2%
35.6%
23.3%
18.9%
11.1%
40.0%
5.6%
11.1%
16.7%
8.9%
28.9%
12.2%
22.2%
11.1%

ﬁcult design problems, such as designing game strategies [26] and designing
novel statistics [16].

Acknowledgements

I thank Zvika Haramaty, Raz Lapid, and Snir Vitrack Tamam for helpful com-
ments.

References

1. Agostinelli, F., Hoﬀman, M.D., Sadowski, P.J., Baldi, P.: Learning activation functions to
improve deep neural networks. In: Y. Bengio, Y. LeCun (eds.) 3rd International Conference

Neural Networks with `A La Carte Selection of Activation Functions

11

on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop
Track Proceedings, pp. 1–9 (2015)

2. Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M.: Optuna: A next-generation hy-
perparameter optimization framework. In: Proceedings of the 25th ACM SIGKDD inter-
national conference on knowledge discovery & data mining, pp. 2623–2631 (2019)

3. Apicella, A., Donnarumma, F., Isgr`o, F., Prevete, R.: A survey on modern trainable acti-

vation functions. Neural Networks (2021)

4. Basirat, M., Roth, P.M.: The quest for the golden activation function. arXiv preprint

arXiv:1808.00783 (2018)

5. Bergstra, J., Yamins, D., Cox, D.D., et al.: Hyperopt: A python library for optimizing the
hyperparameters of machine learning algorithms. In: Proceedings of the 12th Python in
science conference, vol. 13, p. 20. Citeseer (2013)

6. Bohra, P., Campos, J., Gupta, H., Aziznejad, S., Unser, M.: Learning activation functions
IEEE Open Journal of Signal Processing 1, 295–309

in deep (spline) neural networks.
(2020)

7. Ertu˘grul, ¨O.F.: A novel type of activation function in artiﬁcial neural networks: Trained

activation function. Neural Networks 99, 148–157 (2018)

8. Gomes, G.S.d.S., Ludermir, T.B., Lima, L.M.: Comparison of new activation functions in
neural network for forecasting ﬁnancial time series. Neural Computing and Applications
20(3), 417–439 (2011)

9. Gulcehre, C., Moczulski, M., Denil, M., Bengio, Y.: Noisy activation functions. In: Inter-

national Conference on Machine Learning, pp. 3059–3068. PMLR (2016)
10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization.

arXiv preprint

arXiv:1412.6980 (2014)

11. Ko¸cak, Y., S¸iray, G. ¨U.: New activation functions for single layer feedforward neural net-

work. Expert Systems with Applications 164, 113977 (2021)

12. Li, B., Li, Y., Rong, X.: The extreme learning machine learning algorithm with tunable

activation function. Neural Computing and Applications 22(3), 531–539 (2013)

13. Liu, Z., Shen, Z., Savvides, M., Cheng, K.T.: ReActNet:: Towards precise binary neural
In: European Conference on Computer

network with generalized activation functions.
Vision, pp. 143–159. Springer (2020)

14. Maguolo, G., Nanni, L., Ghidoni, S.: Ensemble of convolutional neural networks trained
with diﬀerent activation functions. Expert Systems with Applications 166, 114048 (2021)
arXiv

15. Misra, D.: Mish: A self regularized non-monotonic neural activation function.

preprint arXiv:1908.08681 4 (2019)

16. Moore, J.H., Olson, R.S., Chen, Y., Sipper, M.: Automated discovery of test statistics
using genetic programming. Genetic programming and evolvable machines 20(1), 127–137
(2019)

17. Nandi, A., Jana, N.D., Das, S.: Improving the performance of neural networks with an en-
semble of activation functions. In: 2020 International Joint Conference on Neural Networks
(IJCNN), pp. 1–7. IEEE (2020)

18. Nwankpa, C., Ijomah, W., Gachagan, A., Marshall, S.: Activation functions: Comparison
of trends in practice and research for deep learning. arXiv preprint arXiv:1811.03378
(2018)

19. Ohn, I., Kim, Y.: Smooth function approximation by deep neural networks with general

activation functions. Entropy 21(7), 627 (2019)

20. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
Gimelshein, N., Antiga, L., et al.: PyTorch: An imperative style, high-performance deep
learning library. arXiv preprint arXiv:1912.01703 (2019)

21. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D.,
Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research 12, 2825–2830 (2011)

22. Piazza, F., Uncini, A., Zenobi, M.: Neural networks with digital LUT activation functions.
In: Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya,
Japan), vol. 2, pp. 1401–1404. IEEE (1993)

23. Rozsa, A., Boult, T.E.: Improved adversarial robustness by reducing open space risk via

tent activations. arXiv preprint arXiv:1908.02435 (2019)

24. Scardapane, S., Van Vaerenbergh, S., Totaro, S., Uncini, A.: Kafnets: Kernel-based non-
parametric activation functions for neural networks. Neural Networks 110, 19–32 (2019)

12

Moshe Sipper

25. Shen, Y., Wang, B., Chen, F., Cheng, L.: A new multi-output neural model with tunable
activation function and its applications. Neural Processing Letters 20(2), 85–104 (2004)
26. Sipper, M., Azaria, Y., Hauptman, A., Shichel, Y.: Designing an evolutionary strategiz-
IEEE Transactions on Systems, Man, and

ing machine for game playing and beyond.
Cybernetics, Part C (Applications and Reviews) 37(4), 583–593 (2007)

27. Tavakoli, M., Agostinelli, F., Baldi, P.: SPLASH: Learnable activation functions for im-

proving accuracy and adversarial robustness. Neural Networks (2021)

28. Vanschoren, J., van Rijn, J.N., Bischl, B., Torgo, L.: OpenML: Networked science in ma-
chine learning. SIGKDD Explorations 15(2), 49–60 (2013). DOI 10.1145/2641190.2641198.
URL http://doi.acm.org/10.1145/2641190.2641198

