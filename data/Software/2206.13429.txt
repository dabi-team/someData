Incivility Detection in Open Source Code Review and Issue Discussions

Isabella Ferreiraa,∗, Ahlaam Raﬁqb, Jinghui Chenga

aDepartment of Computer and Software Engineering, Polytechnique Montréal, Montréal, Quebec, Canada
bDepartment of Physics, Indian Institute of Technology, Guwahati, Assam, India

2
2
0
2

n
u
J

7
2

]
E
S
.
s
c
[

1
v
9
2
4
3
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

Given the democratic nature of open source development, code review and issue discussions may be uncivil. Incivility, deﬁned
as features of discussion that convey an unnecessarily disrespectful tone, can have negative consequences to open source com-
munities. To prevent or minimize these negative consequences, open source platforms have included mechanisms for removing
uncivil language from the discussions. However, such approaches require manual inspection, which can be overwhelming given
the large number of discussions. To help open source communities deal with this problem, in this paper, we aim to compare six
classical machine learning models with BERT to detect incivility in open source code review and issue discussions. Furthermore,
we assess if adding contextual information improves the models’ performance and how well the models perform in a cross-platform
setting. We found that BERT performs better than classical machine learning models, with a best F1-score of 0.95. Furthermore,
classical machine learning models tend to underperform to detect non-technical and civil discussions. Our results show that adding
the contextual information to BERT did not improve its performance and that none of the analyzed classiﬁers had an outstanding
performance in a cross-platform setting. Finally, we provide insights into the tones that the classiﬁers misclassify.

Keywords:
incivility, code review, github issues, open source, bert, machine learning

1. Introduction

Open source software (OSS) development provides abun-
dant opportunities for public discussions, which happen within
the context of issue tracking, bug report, code review, and user
feedback, to just name a few. These opportunities character-
ize the democratic essence of open source development by al-
lowing anyone who has the relevant knowledge to contribute
to the development process and shape the project one way or
another. However, as in all types of public discussions, con-
versations that happen in open source development can become
uncivil. Incivility in such contexts is deﬁned as features of dis-
cussion that convey an unnecessarily disrespectful tone toward
the discussion forum, its participants, or its topics [1]. This
phenomenon, along with related concepts such as toxicity, is a
topic that has recently attracted close attention in the software
engineering community [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12].

Previous research [1, 8] have indicated that negative expe-
riences involving uncivil encounters may be caused by various
factors such as the violation of community conventions, inap-
propriate solutions proposed by developers, inappropriate feed-
back provided by maintainers, personal opinions with the OSS
project, diﬀerent points of view about technical concerns, or
disagreement about politics and/or OSS ideology. Furthermore,

∗Corresponding author
Email addresses: isabella.ferreira@polymtl.ca (Isabella

Ferreira), arafiq@iitg.ac.in (Ahlaam Raﬁq),
jinghui.cheng@polymtl.ca (Jinghui Cheng)

uncivil expressions can have important impacts on the commu-
nication and the discussion participants, resulting in escalated
incivility, discontinued conversation, or disengaged contribu-
tors. As such, many major software engineering platforms such
as Stack Overﬂow and GitHub have incorporated mechanisms
for labeling and removing oﬀensive and toxic languages [13,
14]. Many of these approaches involve manual inspection, which
requires considerable human eﬀorts given the large amount of
content generated daily in those platforms. Hence, automated
techniques for detecting uncivil communication in software en-
gineering platforms would be helpful for open source commu-
nities.

Developing such automated techniques, however, involves
major challenges. First, although their impacts are not neglectable,
uncivil exchanges in open source communities can be infre-
quent [1, 4]. The lack of uncivil cases poses challenges in cre-
ating datasets for training and evaluating the automated tech-
niques. Second, incivility can be manifested in various ways.
For example, previous work has identiﬁed many characteristics
of discussion that can be seen as uncivil. Among them, there
are straightforward features such as name calling and vulgarity.
But at the same time, incivility can be manifested through dis-
cussion characteristics such as irony, mocking, and threat that
are diﬃcult to detect automatically [1]. As a result, many ex-
isting software engineering sentiment analysis tools do not per-
form well when detecting incivility [1]. Finally, incivility can
be “very much in the eye of the beholder” [15]. Thus, the dis-
cussion context can have strong indications on whether a com-
ment is uncivil. So analyzing the text in isolation may lead to

Preprint submitted to Journal of Systems and Software

June 28, 2022

 
 
 
 
 
 
inaccurate results.

In this paper, we aim to detect incivility by addressing the

aforementioned problems. More speciﬁcally,
RQ1. How well can the BERT-based model detect incivility
compared to classical machine learning models?

To the best of our knowledge, none of the previous research
has built classiﬁers to detect incivility in open source code re-
view or issue discussions. Hence, it is unknown if incivility can
be automatically detected with a good performance in such dis-
cussions. This research question seeks to address this gap by
comparing the performance of classical machine learning mod-
els (such as Naive Bayes and Support Vector Machine) with the
BERT deep learning model in detecting incivility. BERT [16] is
known for outperforming classical natural language processing
(NLP) techniques in various problems, including text classiﬁ-
cation [17]. It is being widely used in the software engineering
domain, especially for sentiment analysis [18, 19, 20]. Based
on the results of previous research on other tasks, we hypothe-
size that BERT can detect incivility in open source code review
and issue discussions more accurately than the classical ma-
chine learning models.

RQ2. To what extent does the context help to detect incivil-
ity in code review and issue discussions?

Ferreira et al. [1] have shown that the context is an impor-
tant factor that should be considered when detecting incivility.
However, Murgia et al. [21] found that contexts did not help hu-
man raters reach an agreement on assessing emotion expressed
in discussions on issue tracking systems. Hence, in RQ2, we
aim to assess if adding the context helps to improve the per-
formance of automated incivility detection techniques. Partic-
ularly, we considered the previous email or comment for the
detection of incivility of the current email or comment. We
hypothesize that the context improves the classiﬁers’ perfor-
mance.

RQ3. How well do the incivility detection techniques work
in a cross-platform setting?

Building a manually annotated gold standard for incivility
detection on a particular platform is a time-consuming task.
Currently, only two datasets are available in the literature [1, 4],
focusing on code review discussions and issue discussions, re-
spectively. Furthermore, discussions that happened on diﬀerent
platforms could have characteristics that indicate incivility in
diﬀerent ways. Hence, in RQ3, we aim to assess if it is fea-
sible to use BERT and classical machine learning models to
detect incivility in a cross-platform setting. This information
will help us assess the performance of incivility detection on a
new dataset when a gold standard is not available.

Our results show that BERT performs better than classical
machine learning techniques to detect incivility in code review
and issue discussions regardless of which class balancing tech-
nique is used. Furthermore, we found that classical machine
learning models tend to underperform to classify non-technical
code review emails and issue comments as well as civil sen-
tences in both datasets. Our ﬁndings also indicated that adding

2

the context did not improve the incivility classiﬁcation. Finally,
we found that none of the classiﬁers have had an outstanding
performance to detect incivility in a cross-platform setting. In
summary, we make the following contributions:

• To the best of our knowledge, this is the ﬁrst study propos-
ing and comparing BERT with six classical machine learning
models to detect incivility in open source discussions.

• Based on our results, we provide insights on the tones that
misclassiﬁed by each classiﬁer when detecting incivility in
open source code review and issue discussions.

• We make a replication package1 available containing the data,
features, and scripts used to detect incivility in open source
discussions.

2. Background & Related Work

In this section, we discuss the background information and
related work on (i) machine learning for text classiﬁcation, (ii)
automated detection of unhealthy discussions in online commu-
nication platforms, and (iii) automated detection of unhealthy
discussions in software engineering.

2.1. Machine Learning for Text Classiﬁcation

Text classiﬁcation is a classical natural language processing
(NLP) problem that aims at assigning labels to textual docu-
ments, such as sentences or paragraphs [22]. Currently, there
are two kinds of machine learning approaches for automatic text
classiﬁcation, namely classical machine learning-based models
and neural network-based approaches.

Common classical machine learning-based models include
classiﬁcation and regression tree (CART), k-nearest-neighbors
(KNN), logistic regression, naive Bayes, random forest, and
support vector machine (SVM), among others. They were ap-
plied in various general text classiﬁcation tasks [23, 24, 25, 26],
as well as for software engineering tasks in speciﬁc [5, 27, 28,
29]. To use these models, features need to be ﬁrst deﬁned and
extracted from textual documents, then fed into the classiﬁer
for prediction. Popular features for textual data include bag
of words (BoW) and frequency–inverse document frequency (tf-
idf). Although widely used, classical machine learning clas-
siﬁers have a major limitation. That is, choosing the proper
features for each domain requires extensive domain knowledge;
thus it is hard to deﬁne cross-domain or cross-task features [22].
In this work, we assess if it is feasible to use the six aforemen-
tioned classical machine learning-based models to detect inci-
vility in code review and issue discussions.

To solve the aforementioned challenges, neural network-
based approaches have been widely explored in the literature
to address text classiﬁcation tasks [16, 30, 31, 32, 33, 34, 35].
In 2018, Devlin et al. [16] proposed BERT (Bidirectional En-
coder Representations from Transformers), which is currently

1https://github.com/isabellavieira/incivility_detection_oss_discussions

the state of the art embedding model [22]. The BERT base
model consists of 110M parameters and has been trained on
BookCorpus [36] and English Wikipedia [37], which include a
total of 3.3 billion words. BERT is trained with two objectives:
masked language modeling (MLM) and next sentence predic-
tion (NSP). MLM allows the model to learn a bidirectional rep-
resentation of the sentence by randomly masking 15% of the
words in the input and then training the model to predict the
masked words. For NSP, the model concatenates two masked
sentences as inputs during the pretraining phase and then pre-
dicts if the two sentences are continuous in the text or not.

Many variants have been made to BERT since it was pro-
posed [38, 39, 40, 41, 42]. RoBERTa [38], for example, is
a more robust implementation of BERT, trained with a much
larger amount of data. ALBERT [39] optimizes BERT by low-
ering its memory consumption and increasing its training speed.
DistillBERT [40] uses knowledge distillation, i.e., a compres-
sion technique in which a compact model is trained to repro-
duce the behavior of a larger model, to reduce the size of the
BERT model. SpanBERT [41] is a pre-trained method to better
represent and predict spans of text. CodeBERT [42] is a pre-
trained language model for both programming languages and
natural languages. In many NLP [17] and software engineering
problems [6, 18, 19, 20], BERT has demonstrated to have better
performance than classical machine learning models. Thus, we
investigate BERT’s ability to detect incivility in code review
and issue discussions. To simplify this initial exploration, we
used the original BERT model instead of its variants.

2.2. Automated Detection of Incivility in Online Communica-

tion Platforms

By using either classical machine learning-based models
or neural network-based approaches, many authors have tried
to automatically detect incivility on online platforms, such as
in news discussions [43, 44] and Twitter [45]. Daxenberge
et al. [43], for example, sought to understand incivility (deﬁned
as “expressions of disagreement by denying and disrespecting
opposing views”) on user comments on Facebook pages of nine
German public and private media outlets. By using a logistic
regression classiﬁer, they found that incivility can be identiﬁed
with an overall F1-score of 0.46. To assess how well machine
learning models are able to detect incivility (deﬁned as “fea-
tures of discussion that convey an unnecessarily disrespectful
tone towards the discussion forum, its participants, or its top-
ics”) in a cross-platform setting, Sadeque et al. [44] trained
diﬀerent machine learning models on an annotated newspaper
dataset and tested them on Russian troll tweets. As a result,
Recurrent Neural Network (RNN) with Gated Recurrent Units
(GRU) outperformed the other analyzed models with an F1-
score of 0.51 for name calling and 0.48 for vulgarity. On Twit-
ter, incivility (deﬁned as “the act of sending or posting mean
text messages intended to mentally hurt, embarrass or humil-
iate another person using computers, cell phones, and other
electronic devices”) detection with character-level bidirectional
long short-term memory (bi-LSTM) and character-level con-
volutional neural networks (CNNs) with a rectiﬁed linear unit

(ReLU) outperformed the best baseline model with a F1-score
of 0.82 [45].

In our literature review, we were not able to ﬁnd previous
research investigating automated detection of incivility in soft-
ware engineering settings, although some recent work focused
on detecting unhealthy discussions that we review in the next
section. In this paper, we thus address this gap by leveraging
the concepts and the datasets established in our previous work
about incivility in code review [1] and issue discussions [4].

2.3. Automated Detection of Unhealthy Discussions in Soft-

ware Engineering

Unhealthy interactions are often characterized in software
engineering (SE) discussions as toxicity [7, 8, 46, 47], oﬀen-
sive language [6, 9], heated discussions [4, 5], hate speech [2],
pushback [3, 47], and negative sentiments [10, 11, 12]. Al-
though these diﬀerent terminologies might share similarities
with incivility, they only cover one dimension of incivility, i.e.,
language that harms other people [1]. Incivility, however, is an
umbrella with diﬀerent dimensions that focus on unnecessar-
ily disrespectful tones toward the discussion forum, its partici-
pants, or its topics, hurting, therefore, a constructive conversa-
tion [1]. Table 1 presents the studies proposing models to detect
diﬀerent kinds of unhealthy interactions. We compare our study
with the literature with respect to the model implemented, the
used dataset, and the techniques to improve the models’ perfor-
mance, such as cross-validation, data augmentation, class bal-
ancing, and hyperparameter optimization.

Previous study identiﬁed that open source contributors might
have diﬀerent communication styles; some may have negative
impacts. For example, a Naive Bayes classiﬁer identiﬁed that
the leaders of the Linux Kernel Mailing List (LKML) have dif-
ferent communication styles (F1-score = 0.96) [48]; some used
more impolite, rude, aggressive, or oﬀensive words. Oﬀen-
sive language (deﬁned as “communication that contains gut-
ter language, swearing, racist, or oﬀensive content”) can also
be identiﬁed in other platforms such as GitHub, Gitter, Slack,
and Stack Overﬂow, with more than 97% of accuracy using
BERT [6].

In addition to having diﬀerent communication styles, con-
tributors might also demonstrate their sentiments and emotions
when expressing themselves in open source discussions. Anger,
for example, can be accurately identiﬁed in Jira discussions
with SVM (F1-score = 0.81), J48 decision tree (F1-score =
0.77), and Naive Bayes (F1-score = 0.72) [49]. Similarly, sen-
timent polarity of Stack Overﬂow posts can be better identiﬁed
with BERT (F1-score = 0.84) than with Recurrent Neural Net-
work (RNN) (F1-score = 0.66) [18]. BERT and its variation
RoBERTa also achieve a good performance (F1-score>0.90)
when identifying the sentiment polarity of GitHub issues [19].
Since BERT achieves a good performance in many NLP tasks,
Wu et al. [20] compared the performance of diﬀerent existing
sentiment analysis tools with BERT. They found that BERT
achieved the best score among all sentiment analysis tools for
Stack Overﬂow posts (F1-score = 0.64 for the positive and neg-
ative classes, and 0.93 for the neutral class), API reviews, Jira
issues, and Gerrit code reviews (with F1-score of about 0.9 for

3

Table 1: Methods available in the literature to automatically detect unhealthy discussions in the software engineering domain.

Authors

Goal

Model

Dependent variables

Dataset

Schneider et al. [48]

Identify the discourse patterns of the leaders of the LKML.

Naive Bayes

Email sent by Linus Torvalds
Email sent by Greg Kroah-Hartman

Code reviews

Gachechiladze et al. [49] Detect anger towards self, others, and objects.

Weka implementation: SVM, J48, Naive Bayes

Biswas et al. [18]

Assess how much improvement can be made to sentiment analysis
for the SE domain.

BERT4SentiSE, RNN4SentiSE

Egelman et al. [3]

Detect the feelings pushback in code reviews, i.e., the perception of unnecessary
interpersonal conﬂicts in code review while a reviewer is blocking a
change request.

Logit Regression Model

Anger towards self
Anger towards others
Anger towards objects

Positive
Negative
Neutral

Interpersonal conﬂict
Feeling that acceptance was withheld for too long
Reviewer asked for excessive changes
Feeling negative about future code reviews
Frustration

Raman et al. [2]

Detect toxic language, i.e., hate speech and microaggressions.

SVM

Sarker et al. [9]

Evaluate diﬀerent tools to detect toxicity.

Perspective API, STRUDEL Toxicity Detector,
Deep Pyramid Convolutional Neural Networks,
BERT with fast.ai,
Hate Speech Detection

Batra et al. [19]

Assess how much improvement can be made to sentiment analysis for
the SE domain.

BERT base model, RoBERTa, ALBERT

Cheriyan et al. [6]

Detect and classify oﬀensive language, i.e., communication that contains
gutter language, swearing, racist, or oﬀensive content.

Random Forest, SVM, BERT

Wu et al. [20]

Detect sentiment in diﬀerent kinds of SE discussions.

BERT, SentiStrength, NLTK, StanfordCoreNLP,
SentiStrength-SE, SentiCR, Senti4SD

Our work

Detect incivility, i.e., features of discussion that convey an unnecessarily
disrespectful tone toward the discussion forum, its participants, or its
topics in code review and issue discussions.

CART, KNN, Logistic Regression,
Naive Bayes, Random Forest, SVM, BERT

Toxic
Non-toxic

Toxic
Non-toxic

Positive
Negative
Neutral

Oﬀensive
Non-oﬀensive

Positive
Negative
Neutral

Technical
Non-technical
Civil
Uncivil

Jira issues

Stack Overﬂow posts

Code reviews

GitHub issues

Code reviews,
Gitter messages

GitHub commits,
Jira issues,
Stack Overﬂow posts

GiHub,
Gitter,
Slack,
Stack Overﬂow

Jira issues,
API reviews,
Stack Overﬂow,
Gerrit Codereview,
GitHub pull requests ,
Stack Overﬂow posts

Code reviews
GitHub issues

Techniques
Cross-validation: (cid:51)
Data augmentation: (cid:55)
Class balancing: (cid:55)
Hyperparameter optimization: (cid:55)
Cross-validation: (cid:51)
Data augmentation: (cid:55)
Class balancing: (cid:55)
Hyperparameter optimization: (cid:51)
Cross-validation: (cid:51)
Data augmentation: (cid:55)
Class balancing: (cid:51)
Hyperparameter optimization: (cid:51)

Cross-validation: (cid:55)
Data augmentation: (cid:55)
Class balancing: (cid:55)
Hyperparameter optimization: (cid:55)

Cross-validation: (cid:51)
Data augmentation: (cid:55)
Class balancing: (cid:55)
Hyperparameter optimization: (cid:51)
Cross-validation: (cid:55)
Data augmentation: (cid:55)
Class balancing: (cid:55)
Hyperparameter optimization: (cid:55)
Cross-validation: (cid:55)
Data augmentation: (cid:51)
Class balancing: (cid:55)
Hyperparameter optimization: (cid:55)
Cross-validation: (cid:55)
Data augmentation: (cid:51)
Class balancing: (cid:55)
Hyperparameter optimization: (cid:55)

Cross-validation: (cid:55)
Data augmentation: (cid:55)
Class balancing: (cid:55)
Hyperparameter optimization: (cid:55)

Cross-validation: (cid:51)
Data augmentation: (cid:51)
Class balancing: (cid:51)
Hyperparameter optimization: (cid:51)

the positive and negative classes in all those cases). Interest-
ingly, BERT also has better performance than sentiment analy-
sis tools in a cross-platform setting (F1-scores are above 0.9 for
positive, neutral and negative polarities) [20].

Another emotion that might emerge in code review discus-
sions, more speciﬁcally, is the feeling of pushback, which is
characterized by interpersonal conﬂicts, impatience, disappoint-
ment, and frustration [3]. In Google’s code review discussions,
a logistic regression model found that code review authors are
between 3.0 and 4.1 times more likely to experience the feeling
of pushback for at least once and between 7.0 and 13.7 times
more likely to experience it multiple times when compared to
code reviews that were not ﬂagged with a potential feeling of
pushback.

Finally, toxic language, i.e., hate speech and microaggres-
sions, can be identiﬁed with automated methods. For example,
using the SVM model Raman et al. detected toxicity in GitHub
issues with a precision of 0.75, but a low recall of 0.35 [2].
Sarker et al. [9] also tested the SVM classiﬁer on other 100k
randomly sampled GitHub issues; they found that the precision
decreased to 0.50, demonstrating that the model might be over-
ﬁtting to the training set. Similarly, toxicity can be identiﬁed in
Gerrit code review and Gitter discussions, with the STRUDEL
toxicity detector having a F1-score of 0.49 and 0.73, respec-
tively [9]. Interestingly, Sarker et al. [9] found that toxicity de-
tectors tend to perform worse on more formal SE discussions,
such as code reviews, than on informal conversations such as
Gitter messages.

Our work diﬀers from the previous works in several ways.
First, this is the ﬁrst study proposing to detect incivility in SE
discussions. This study builds upon our previous work on char-

acterizing incivility on code review discussions of rejected patches
from the LKML and GitHub issue discussions locked as too
heated. We chose to compare six classical machine learning
models (Classiﬁcation and Regression Tree (CART), k-Nearest
Neighbors (KNN), Logistic Regression, Naive Bayes, Random
Forest, and SVM) with BERT. Additionally, we use four strate-
gies to augment our data (i.e., synonym replacement, random
insertion, random swap, and random deletion) and compare
three class balancing techniques, i.e., random undersampling,
random oversampling, and SMOTE. We also perform hyperpa-
rameter optimization with Grid Search on the classical machine
learning models’ hyperparameters and Bayesian Optimization
on BERT’s hyperparameters to improve the models’ performance.
Finally, the performance of our models is evaluated in a 5-fold
cross-validation. On top of evaluating the performance of the
machine learning models in detecting incivility, we also ana-
lyze the impact of the discussion context and the feasibility of
detecting incivility in a cross-platform setting.

3. Datasets and Data Preprocessing

The general goal of this study is to assess the extent to
which incivility can be detected in code review and issue dis-
cussions. To the best of our knowledge, only two incivility
datasets are available in the literature, i.e., a code review dataset
comprising code review emails of rejected patches that were
sent to Linux Kernel Mailing List (LKML) [1] and an issues
dataset comprising GitHub issues locked as too heated [4]. We
used both datasets to train our classiﬁers.

For each dataset, the natural language emails (in the case
of the code review dataset) and comments (in the case of the

4

GitHub issues dataset) were ﬁrst labeled as technical or non-
technical. Following the deﬁnition used by the datasets, the
technical class comprises emails and comments that are purely
focused on technical discussions, i.e., none of their sentences
convey a mood or style of expression [1]. On the contrary, non-
technical code review emails or issue comments are those in
which at least one sentence expresses a tone-bearing discussion
feature (TBDF). The datasets use the concept of TBDF to in-
dicate “conversational characteristics demonstrated in a written
sentence that convey a mood or style of expression” [1].
In
total, there are 1,365 technical emails and 168 non-technical
emails in the code review dataset; there are 4,793 technical
comments and 718 non-technical comments in the issues dataset.
Next, sentences in non-technical emails and comments were
then further categorized as civil or uncivil. Civil sentences are
those that contain positive, neutral, or negative (but not uncivil)
TBDFs, such as excitement, friendly joke, or sadness, as deﬁned
in the dataset [1, 4]. Conversely, the uncivil class contains sen-
tences that demonstrate at least one uncivil TBDF, such as bitter
frustration, impatience, mocking, or vulgarity. There are 117
civil sentences and 276 uncivil sentences in the non-technical
emails of the code review dataset and there are 353 civil sen-
tences and 896 uncivil sentences in the non-technical comments
of the issues dataset.

Our classiﬁcation tasks are thus two-layered: ﬁrst, we aim
to classify code review emails/issue comments into technical
or non-technical (CT1); then, for non-technical contents, we
aim to classify sentences into civil or uncivil (CT2). The goal
to separate these two classiﬁcation tasks is to assess if there is
a diﬀerence to predict incivility according to the granularity of
the text, i.e., the technical/non-technical classiﬁcation is more
coarse-grained since it is in the email/comment level and the
civil/uncivil classiﬁcation is more ﬁne-grained since it is done
in the sentence level. Furthermore, in a concrete scenario in
which open source contributors would use our classiﬁers to as-
sess whether their comments are uncivil or not, ﬁrst we would
detect if the text is technical or not. If it is non-technical, then
we would detect (in)civility.

3.1. Data Preprocessing

We consider a series of steps to reduce noise on the datasets
described above. First, we exclude sentences coded with civil
and uncivil TBDFs from the civil dataset because similar in-
stances with diﬀerent target classes can cause the models to
perform poorly.

1. We manually remove the source code (including variable
names, function names, stack traces, etc.), words other than
English, emojis, and GitHub username mentions (such as
@username) from the text;

2. We automatically remove the header of code review emails,
including the ﬁrst line that follows the regex pattern “On
(.*?)

wrote:”;

3. We automatically remove greetings such as “Hi [person_name]”
and statements such as “Reviewed by [person_name]” or
“Tested by [person_name]”;

5

4. We automatically remove any signature statement that is in
the following list of words: “warm regards”, “kind regards”,
“regards”, “cheers”, “many thanks”, “thanks”, “sincerely”,
“best”, “thank you”, “talk soon”, “cordially”, “yours truly”,
“all the best”, “best regards”, “best wishes”, “looking for-
ward to hearing from you”, “sincerely yours”, “thanks again”,
“with appreciation”, “with gratitude”, and “yours sincerely”;

5. We automatically remove all reply quotes, usually repre-

sented by “<”;

6. We automatically remove stop words and punctuation; we

perform stemming on each remaining word.

3.2. Feature Extraction for Classical ML Classiﬁers

This step consists in extracting features to detect incivility
in code review emails/issues comments and sentences using su-
pervised techniques. We created two sets of features for both
classiﬁcation tasks, namely textual features and conversational
features, which were inspired by the work of Arya et al. [27]
and adapted to our context.

Textual features: We consider n-grams as textual features.
First, we perform text vectorization by transforming each word
of the text into one feature.
In short, we create feature vec-
tors that are based on the absolute terms frequencies. Second,
we use n-grams that represent the appearance of n tokens se-
quences. Then, we use weighted TF-IDF to transform both
features into numerical representations, i.e., the frequency of
words and n-grams in the text are multiplied by their inverse
In this work, we consider 1-gram
document frequency [27].
(i.e., word frequency) and 2-gram. We tune our models by con-
sidering only the n-gram conﬁguration that yields the best result
for each model.

Conversational features describe the participants, length,
structural, and temporal attributes of code review and issue dis-
cussions. Each one of these features are described in Table 2,
along with the classiﬁcation tasks in which they are used. The
conversational features include the following categories.

• Participant features include features describing the discus-
sion participants, i.e., authors who wrote the code review
email or issue comment as well as is the author is a main-
tainer or a developer.

• Length features concern the length of emails, comments, or
sentences. These speciﬁc features indicate length in terms of
the number of characters in the email/comment or the sen-
tence, as well as the relative number of words with respect to
other emails, comments, or sentences.

• Structural features describe the location of an email, com-
ment, or sentence in relation to the entire email thread, issue
thread, or the current email/comment itself.

• Temporal features concern the time that the email/comment
was sent with respect to the immediately previous and next
email/comment as well as the beginning and the end of the
email/issue thread.

Table 2: Conversational features of code review and issue discussions

Feature type Classiﬁcation task

Feature name

Description

Participant

CT1, CT2

AUTHOR_ROLE

CT1, CT2

FIRST_AUTHOR

Length

Structural

Temporal

CT1
CT1
CT2
CT2
CT2

CT1

CT2

CT2

CT1, CT2

CT1, CT2
CT1, CT2
CT1, CT2
CT1, CT2

CHAR_TEXT
LEN_TEXT
CHAR_SENT
LEN_SENT_T
LEN_SENT_C

POS_TEXT_T

POS_SENT_E

POS_SENT_T

LAST_COMMENT

TIME_FIRST_COMMENT
TIME_TEXT_LAST
TIME_PREVIOUS_COMMENT
TIME_TEXT_NEXT

Email author’s role in the Linux kernel. We ﬁrst group identities that have the same names or the same email addresses. The
author is considered a maintainer if one of those identities appears in the MAINTAINERS ﬁle [50], and a developer otherwise.
Flag if the email/comment author also sent the ﬁrst email/comment of thread (i.e., original patch/issue description).
Number of characters in the email/comment.
Number of words in the email/comment divided by that of the longest email/comment in the thread.
Number of characters in the sentence.
Number of words in sentence divided by that of longest sentence in the thread.
Number of words in sentence divided by that of longest sentence in the email/comment.
Position of email/comment in the thread divided by the number of emails/comments in thread.
Position of sentence in email/comment divided by the number of sentences in email/comment. Sentences are identiﬁed based
on the following regular expression: (? <= [.!?]).
Position of sentence in thread divided by the number of sentences in thread. Sentences are identiﬁed based on the following
regular expression: (? <= [.!?]).
Flag if it is the last email/comment or not.
Time from ﬁrst email/comment to current email/comment divided by the total time of the thread.
Time from current email/comment to last email/comment divided by the total time of the thread.
Time from previous email/comment to current email/comment divided by the total time of the thread.
Time from current email/comment to next email/comment divided by total time of the thread.

Values

{Maintainer, Developer}

{True, False}
R≥0 = {x ∈ R|x≥0}
(0,1]
(0,1]
(0,1]
(0,1]

(0,1]

(0,1]

(0,1]

{True, False}

[0, 1]
[0, 1]
[0, 1]
[0, 1]

Note: CT1 = classiﬁcation task 1 on technical and non-technical emails/comments, CT2 = classiﬁcation task 2 on civil and uncivil sentences.

4. Data Augmentation and Class Balancing

Our datasets, especially the ones for civil/uncivil classiﬁca-
tion, are relatively small. To increase the training set and to
boost performance for both classiﬁcation tasks, we used the
Easy Data Augmentation (EDA) [51] techniques to augment
the current datasets; the EDA techniques are known to con-
tribute to performance gains of classiﬁcations when the dataset
is small [51]. Additionally, the datasets we use are highly im-
balanced, skewing toward technical emails/comments and un-
civil sentences. Machine learning classiﬁers are well known for
underperforming when the data is skewed toward one class [52,
53]. To address this issue, we explored and evaluated three class
balancing techniques that we describe in this section.

4.1. Easy Data Augmentation Techniques (EDA)

In this study, we use the Easy Data Augmentation (EDA)
Techniques [51] to increase the size of our datasets. EDA is
composed of four operations:

• Synonym Replacement (SR) consists of randomly choosing
n words (excluding stop words) from the text and replacing
them with a random synonym.

• Random Insertion (RI) consists of ﬁnding a synonym of a
random word in the text (excluding stop words) and inserting
the synonym in a random position in the text. This process is
repeated n times.

• Random Swap (RS) is when two words are randomly cho-
sen and their positions are swapped. This is repeated n times.

that indicates the percentage of words in a text to be changed.
Furthermore, for each original email/comment/sentence, it is
possible to generate naug augmented emails/comments/sentences.
We evaluated diﬀerent combinations of hyperparameters to aug-
ment the training set, as shown in Table 3. The hyperparameter
values were chosen based on the training set size and thresholds
that result in high performance for each EDA operation, as rec-
ommended by Wei and Zou [51]. The hyperparameter tuning
process is described in detail in Section 5.

4.2. Class Balancing Techniques

To address the class imbalance problem of our dataset [52],
we explored three class balancing techniques: random over-
sampling, random undersampling, and Synthetic Minority Over-
sampling Technique (SMOTE). We implemented these techniques
using the Python library imblearn to compare their results
when answering RQ1. The class balancing techniques are ap-
plied after the datasets are augmented by EDA.

• Random oversampling aims at taking random samples for
the minority class and duplicating them until it reaches a size
comparable to the majority class [55].

• Random undersampling selects random samples from the
majority class and removes them from the dataset until it
reaches a size comparable to the minority class [53].

• SMOTE is a method in which the minority class is over-
sampled by creating new samples and the majority class is
undersampled [56].

• Random Deletion (RD) consists of randomly removing words

5. Training and Evaluating the Classiﬁers

in a sentence with probability p.

To ﬁnd a synonym to perform the SR and RI operations,
we use the NLTK wordnet corpus and the function synsets
(word) to lookup for the word’s synonym. Furthermore, we
use the NLTK’s list of English stopwords [54] to exclude stop
words from the text. To mitigate the threat of long texts having
more words than short texts, Wei and Zou [51] suggest varying
the number of words n for SR, RI, and RS based on the text
length l with the formula n = αl, where α is a hyperparameter

Figure 1 depicts the key components in the pipeline of the
incivility classiﬁers explored in this study. To answer our re-
search questions, we implemented six classical machine learn-
ing models (Section 5.1) and one deep learning model (Sec-
tion 5.2). After preprocessing the data (Section 3.1) and ex-
tracting the features for the classical machine learning models
(Section 3.2), we stratify our dataset into train, test, and valida-
tion sets. Then, we augment our training set (Section 4.1) and
balance our classes (Section 4.2). During training, to obtain

6

Classiﬁcation task

α_S R

α_RI

α_RS

p_RD

naug code review dataset

naug GitHub issues dataset

Table 3: EDA hyperparameters search space

CT1

CT2

0.2
0.2
0.2
0.2
0.05
0.05
0.05
0.05

0.2
0.2
0.2
0.2
0.05
0.05
0.05
0.05

0.1
0.1
0.05
0.05
0.1
0.1
0.05
0.05

0.1
0.1
0.05
0.05
0.1
0.1
0.05
0.05

0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.05

0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.05

0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.05

0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.05

4
8
4
8
4
8
4
8

8
16
8
16
8
16
8
16

4
-
4
-
4
-
4
-

8
16
8
16
8
16
8
16

Note: CT1 = classiﬁcation task 1 on technical and non-technical emails/comments, CT2 = classiﬁcation task 2 on civil and uncivil sentences.

the optimal models, we perform hyperparameter tuning to ﬁnd
out the best set of hyperparameters. Finally, we test the trained
classiﬁers on the test set and assess the performance of each
classiﬁer according to four performance metrics (Section 5.3).

5.1. Classical Machine Learning Models

We consider six classical classiﬁers to detect incivility in
code review and issues discussions (RQ1). The classiﬁers were
implemented using the sklearn Python library.

• Classiﬁcation and Regression Tree (CART) is a binary tree
that aims at producing rules that predict the value of an out-
come variable [26].

• k-nearest neighbors (KNN) assumes that similar datapoints
are close to each other. Hence, the algorithm relies on dis-
tance metrics for classiﬁcation. The resulting class is the one
that has the nearest neighbors [23].

• Logistic Regression (LR) uses a logistic function to model
the dependent variable. The goal of the algorithm is to ﬁnd
the best ﬁtting model to describe the relationship between the
dependent and independent variable [23].

• Naive Bayes (NB) is a probabilistic classiﬁer based on the
Bayes theorem.
It assumes that the presence (or absence)
of a particular feature of a class is unrelated to the pres-
In this work,
ence (or absence) of any other feature [25].
our text classiﬁcation is performed using the Multinomial
Naive Bayes model that has improved performance over the
Bernoulli model for text classiﬁcation [57].

• Random Forest (RF) is a group of decision trees whose
nodes are deﬁned based on the training data [23]. The most
frequent label found by the trees from the forest is the result-
ing class [27].

• Support Vector Machine (SVM) is a linear model that cre-

ates a line or a hyperplane separating the data into two classes [58].

7

During the training process, we performed nested cross-
validation with grid search [59] to test a combination of hyper-
parameters and evaluate the models’ performance. Speciﬁcally,
we ﬁrst split the dataset into train and test sets in the outer strat-
iﬁed 5-fold cross-validation for model evaluation. The train-
ing set obtained from the outer cross-validation is then further
split into training (for training the models) and validation (for
selecting the best hyperparameters) sets in the inner stratiﬁed 5-
fold cross-validation. In addition to the EDA hyperparameters,
Table 4 presents the search space for the additional hyperpa-
rameters of each model, which were deﬁned according to the
literature. Note that we used the default values for hyperparam-
eters not described in Table 4. In each outer cross-validation
fold, we used the performance metrics presented in Section 5.3
to evaluate the performance of the classiﬁer.

5.2. BERT Base Model

We use the uncased BERT base model, pretrained on the
English language [60], to detect incivility. We chose to use
the uncased model (i.e., the model does not make a diﬀerence
between “english” and “English”) because the case informa-
tion is not relevant in our classiﬁcation tasks. Furthermore,
due to the large number of parameters in this model (approx-
imately 110 million parameters), we did not pretrain it from
scratch to reduce the risk of overﬁtting [61]. The classiﬁca-
tion task is done using the Transformers PyTorch library with
AutoModelForSequenceClassification [62], which has a
classiﬁcation head.

We split the input dataset into train, test, and evaluation
datasets in a 70-15-15 ratio stratiﬁed along the labels. To opti-
mize BERT’s hyperparameters, we run bayesian optimization [63]
with 50 trials for each one of the EDA parameter settings (see
Section 4.1), i.e., eight times. BERT’s hyperparameter opti-
mization was done using the hyperparameter_search() func-
tion [64] from the Trainer class with optuna as the backend.
The bayesian optimization takes in the training and evaluation

Figure 1: Key components and main pipeline of incivility classiﬁers

sets as inputs; the former is used to train the model with dif-
ferent hyperparameters and the latter is used to select the best
hyperparameters. The search space for the hyperparameters is
presented in Table 4; we used the default values of hyperparam-
eters not described in Table 4.

After obtaining the best set of hyperparameters, we perform
a 5-fold cross-validation to train and test BERT. For that, we
use the Trainer [65] class from the Transformers library.
The training adopts an epoch evaluating strategy, i.e., evaluat-
ing BERT’s performance at the end of each epoch using the
performance metrics described in Section 5.3.

5.3. Performance metrics

To compare the performance of our classiﬁers, we evaluate
their performances using the confusion matrix: TP is the num-
ber of true positives, FN is the number of false negatives, FP is
the number of positives, and TN is the number of true negatives.
Based on this matrix, we ﬁrst computed two well-known
metrics, namely precision and recall [69]. The precision of
a given target class (i.e., technical, non-technical, civil, or un-
civil) is deﬁned by the ratio of emails, comments, or sentences
for which a given classiﬁer correctly predicted that target class,
i.e., precision = T P/(T P + FP). The precision value is always
between 0 (poor model) and 1 (perfect model). In each classiﬁ-
cation task of our experiments, we ﬁrst calculated the precision
in each class, then the macro-average metric across both classes
to represent the overall precision of the models.

The recall of a given target class is the ratio of all emails,
comments, or sentences with that target class that a given clas-
siﬁer was able to ﬁnd, i.e., recall = T P/(T P + FN). The recall
value is always between 0 (poor model) and 1 (perfect model).
Similar to precision, we calculated per-class and macro-averaged
recall metrics for each classiﬁcation task.

Then, to have a single value representing the goodness of
the models, we computed the F1-score, which is the harmonic
mean of precision and recall, i.e., F1 = 2 · precision·recall
precision+recall . The
F1-score is independent of the number of true negatives and it is
highly inﬂuenced by classes labeled as positive. The F1-score is
always between 0 (low precision and low recall) and 1 (perfect
precision and perfect recall). In our experiments, we calculated
per-class and macro-averaged F1 metrics.

T P·T N−FP·FN
(T P+FP)(T P+FN)(T N+FP)+(T N+FN)

Finally, we computed the Matthews Correlation Coeﬃ-
cient (MCC) [70], which is a single-value classiﬁcation metric
that is more interpretable and robust to changes in the prediction
goal [71] because it summarizes the results of all four quadrants
of a confusion matrix, i.e., true positive, false negative, true neg-
ative, and false positive [72]. The MCC metric is calculated as
MCC =
and its value is always
between -1 (poor model) and 1 (perfect model); 0 suggesting
that the model’s performance is equal to random prediction. To
calculate TP, TN, FP, and FN for MCC, we considered civil and
technical as positive classes and uncivil and non-technical as
negative classes, although this selection does not inﬂuence the
end results. To be able to compare the MCC scores with the
other performance metrics (i.e., macro-averaged precision, re-
call, and F1-score), we normalized the MCC values to the [0, 1]
interval. Therefore, the normalized MCC (nMCC) is deﬁned
by nMCC = MCC+1
[71, 73]. We also use nMCC as the primary
metric during hyperparameter evaluation (Section 5.1).

√

2

6. Experimental Design to Answer the RQs

In this section, we present the experimental design to an-
swer each research question. All data and scripts used in our
experiments are available in our replication package2.

2https://github.com/isabellavieira/incivility_detection_oss_discussions

8

Data  PreprocessingBlue-colored boxes indicate components with hyperparameters.Classical ML Models 5-fold nested cross-validationClass BalancingSMOTERandom UndersamplingRandom OversamplingOne of the following:   Data augmentation All of the following, on the training sets:Random DeletionRandom InsertionSynonym ReplacementRandom SwapFeatures ExtractionConversational  FeaturesTextual  FeaturesLogistic RegressionNaive BayesRandom ForestSVMCARTKNNBERT Base Model5-fold cross-validation with Bayesian optimization of hyperparametersClass BalancingRandom OversamplingOne of the following:   Random UndersamplingNo Class BalancingData augmentation All of the following, on the training sets:Random DeletionRandom InsertionSynonym ReplacementRandom SwapBERT base model (uncased)Two Classification Tasks: CivilUncivilCT2:TechnicalNon-technicalCT1:Datasets GitHub issuesCode review emailsPrecisionRecallF1-scoreMCCPerformance Metrics Model

BERT

CART

KNN

Table 4: Search space for hyperparameter tuning

Hyperparameter

learning_rate
per_device_train_batch_size
warmup_steps
num_train_epochs
weight_decay

Default value
5−5
8
[0, 5−5]
3
0.0

Searched values
log([1−5, 1−3])
{4, 8, 16}
[0, 500]
[2, 10]
log([0.01, 0.3])

max_depth
min_samples_leaf

n_neighbors
leaf_size

Logistic Regression

C

Multinomial Naive Bayes

alpha

Random Forest

SVM

n_estimators
max_features

C
gamma

None
1

5
30

1.0

1.0

100
auto

1.0
scale

{1, 2, 5, 10, 20, 30, 40, 50}
{1, 2, 4, 8, 16, 32}

[1, 10]
[10, 100]

{0.01, 0.1, 1.0, 10.0}

[0.0, 1.0]

{10, 50, 100}
{2, 5, 10}

[2−5, 215]
[2−15, 23]

Steps Reference

-
-
1
1
-

-
-

1
10

-

0.1

-
-

2−2
2−2

[66]
[66]

[67]
[67]

[27]

[67]

[27]
[27]

[68]
[68]

6.1. Detecting Incivility in Code Reviews and Issues Discus-

sions (RQ1)
In RQ1, we have two classiﬁcation tasks for each dataset,
i.e., (1) classiﬁcation of code review emails and issue comments
into technical and non-technical and (2) classiﬁcation of code
review sentences and issue sentences into civil and uncivil.

For each classiﬁcation task and for each dataset, we com-
pare BERT with six classical machine learning models. We
assess BERT with three class balancing conditions: no class
balancing, random oversampling, and random undersampling.
It was not possible to run SMOTE with BERT because the cur-
rent SMOTE implementations need to convert textual features
to numerical vectors (via tokenization to a form suitable for
SMOTE) [74] and cannot be applied to textual features that
are used for BERT. The classical machine learning models are
assessed with three class balancing techniques: random over-
sampling, random undersampling, and SMOTE. Thus, for each
classiﬁcation task and each dataset, we have 21 experimental
conditions (7 classiﬁers * 3 balancing techniques). The hyper-
parameters are tuned separately for each combination of classi-
ﬁcation task, dataset, classiﬁer, and class balancing technique.
In this paper, we report the results related to hyperparameters
that had the best averaged nMCC score across all outer folds.

6.2. Using the Context to Predict Incivility (RQ2)

To answer RQ2, we considered the previous email/comment
in the same thread, ordered by date/time, as the context in the
classiﬁcation tasks. Speciﬁcally, for each email/comment in-
cluded in the datasets, we retrieve the previous email/comment
from the same email or issue thread and concatenate it with the
original email/comment to create a “context dataset”.

We could not consider the classical classiﬁers in RQ2 be-
cause the extracted features rely solely on the current email/
comment (see Section 3.2). Hence, RQ2 focuses only on BERT.
We use the hyperparameters, the EDA parameter conﬁguration,
and the imbalance handling technique that obtained the best
nMCC score in RQ1 for BERT in each classiﬁcation task for
each dataset.

9

To analyze if the context helps to detect incivility, we com-
pare the diﬀerence between the performance scores of RQ1
and RQ2. Hence, for each performance metric (PM), ∆PM =
PMRQ2 − PMRQ1. We deﬁne whether the context helps to detect
incivility or not according to the following conditions:

∆PM =





< 0
> 0
= 0

: the context does not help
: the context helps
: the context does not make a diﬀerence

6.3. Detecting Incivility in a Cross-Platform Setting (RQ3)

To answer RQ3, we train our models and test them in the
other dataset for each classiﬁcation task; i.e., we train our clas-
siﬁers on the code review dataset and test them on the GitHub
issues dataset, and vice versa. For that, we use the hyperpa-
rameters, the EDA parameters conﬁguration, and the imbalance
handling techniques that obtained the best nMCC score in RQ1
for the dataset used to train the classiﬁers. Because the best
hyperparameters can diﬀer among the ﬁve folds for the classi-
cal machine learning models, we pick the hyperparameters that
were chosen most frequently across all ﬁve folds. If there is a
tie between two sets of hyperparameters, we then choose the
hyperparameter from the fold that had the highest nMCC score.

7. Results

In this section, we present the results for each research ques-
tion for the technical/non-technical and civil/uncivil classiﬁca-
tion tasks. We only report the results of the hyperparameters
that yielded the best results.

7.1. Models’ Performance on Incivility Detection (RQ1)

7.1.1. Classiﬁcation into Technical and Non-Technical

Figure 2 presents the average performance scores for each
experiment condition for the code reviews (left) and issues (right)
datasets and Figure 3 shows the performance scores per target
class.

Figure 2: Average performance scores per class balancing technique and classiﬁer for the classiﬁcation of technical and non-technical emails/comments (CT1).

Figure 3: Performance scores per target class for the classiﬁcation of technical and non-technical emails/comments (CT1).

For the code reviews dataset, BERT without class bal-
ancing and with random undersampling has the best perfor-
mance compared to the classical classiﬁers, with F1 = 0.92
and 0.94, and nMCC = 0.91 and 0.94, respectively. Classi-
cal machine learning models underperform to classify non-
technical code review emails. The nMCC scores for the classi-
cal classiﬁers and for BERT with random oversampling (rang-
ing from 0.50 to 0.64) are very low compared to BERT’s nMCC
scores with random understamplinng and without class balanc-
ing (0.94 and 0.91, respectively), showing that such classiﬁers

are not eﬀective to detect non-technical code review emails.
Figure 3 (left) conﬁrms this result, indicating that the non-technical
class (red color) having overall lower precision and recall than
the technical class (green color) for the underperforming clas-
siﬁers. We also observe that among the classical classiﬁers,
Random Forest (RF) achieved the highest precision of approxi-
mately 0.7 regardless of the class balancing technique, but with
a low recall of approximately 0.5; on the contrary, the Logis-
tic Regression (LR) classiﬁer achieved the highest recall of ap-
proximately 0.7, but a relatively low precision of about 0.6.

10

For the issues dataset, BERT also performs better than
the classical classiﬁers in all class balancing conditions. Sim-
ilar to the code reviews dataset, BERT is able to precisely clas-
sify technical and non-technical issue comments (precision ≈
0.9), ﬁnding a substantial number of issue comments (recall ≈
0.9), and eﬀectively classifying the technical and non-technical
issue comments (nMCC ≈ 0.9), in all class balancing condi-
tions. Furthermore, classical classiﬁers also underperform
to classify non-technical issue comments. The nMCC scores
range from 0.57 to 0.69 demonstrating that non-technical issue
comments are not eﬀectively detected with classical machine
learning models (see Figure 3 (right)), except for RF with ran-
dom oversampling and SVM with random undersampling, in
which their precision metrics are better for the non-technical
class (precisionRF = 0.91 and precisionS V M = 1.0) than the
technical class (precisionRF = 0.87 and precisionS V M = 0.87);
yet their recalls are very low for the non-technical class (recallRF =
0.07, recallS V M = 0.03).

It is surprising that BERT has a good performance overall
even without any class balancing technique. This result is
conﬁrmed by Figure 3, which shows that even without a class
balancing technique both technical and non-technical classes
have a good F1-score for both the code reviews dataset (non −
technical = 0.84 and technical = 0.98) and the issues dataset
(non − technical = 0.86 and technical = 0.98).

7.1.2. Classiﬁcation into Civil and Uncivil

Figure 4 illustrates the performance metrics for each exper-
iment setting and for both the code reviews (left) and issues
(right) datasets. Similarly, Figure 5 presents the performance
metrics per target class for both datasets.

BERT is the best performing classiﬁer regardless of the
class balancing technique for the code reviews dataset. We
observe that, similar to the technical/non-technical classiﬁca-
tion task, BERT has the best performance (precision ≈ 0.9,
recall ≈ 0.9, F1 ≈ 0.9, nMCC ≈ 0.9) to classify civil and un-
civil code review sentences. However, classical machine learn-
ing techniques tend to perform better in the classiﬁcation of
civil/uncivil sentences than in technical/ non-technical emails,
with nMCC scores ranging from 0.58 to 0.77 (compared to
between 0.50 and 0.64 for the technical/ non-technical classi-
ﬁcation). Additionally, the classical models underperform
when classifying civil code review sentences (see Figure 5
(left)). The Logistic Regression, Naive Bayes, and Random
Forest classiﬁers have overall promising results though, with
precision ≈ 0.8 and recall ≈ 0.7.

For the issue comments dataset, BERT is also the best
classiﬁer for detecting incivility regardless of which class
balancing technique is used (precision ≈ 0.9, recall ≈ 0.9).
Although classical techniques also tend to underperform when
classifying civil issue sentences (see Figure 5 (right)), Logistic
Regression and Random Forest have good precision (≈ 0.71 for
LR and ≈ 0.75 for RF) and recall (≈ 0.72 for LR and ≈ 0.67 for
RF) overall.

Summary RQ1: BERT performs better than the classical
machine learning classiﬁers regardless of the class balanc-
ing technique for technical/non-technical and civil/uncivil
classiﬁcation in both datasets. Classical machine learn-
ing techniques tend to underperform when classifying the
non-technical and civil classes.

7.2. Incivility Detection Using the Context (RQ2)

Figure 6 (a) and (b) present the diﬀerence of performance
metrics between the BERT results considering the context (RQ2)
and without the context (RQ1), for the technical/non-technical
classiﬁcation and the civil/uncivil classiﬁcation, respectively.

We found that the context does not help to classify tech-
nical and non-technical code review emails and issue com-
ments. We observe that, for both datasets when detecting tech-
nical and non-technical contents, ∆PM is negative overall, with
the non-technical class results having most drastically decreased
with the context information (Figure 6 (a)). BERT’s perfor-
mance on the code reviews dataset, more speciﬁcally, gets worse
by ≈ −0.2 for the non-technical class, having its precision de-
creased from 0.88 to 0.67 and its recall from 0.92 to 0.71. Sim-
ilarly, on the issues dataset, BERT’s precision for non-technical
comments decreased by 0.14, going from 0.92 to 0.78; and the
recall decreased by 0.04, from 0.83 to 0.79.

Overall, the context also does not help to classify civil
and uncivil code review and issue sentences. Concerning the
classiﬁcation into civil and uncivil using the context informa-
tion, our results show that the civil class results have signif-
icantly decreased, especially for the issues dataset (Figure 6
(b)). Although the precision did not change for the code re-
views dataset, its recall decreased by 0.04, going from 0.92 to
0.88). For the issues dataset, the precision and recall decreased
by 0.09 and 0.03, respectively.

Summary RQ2: Adding the previous code review email
and issue comment makes the prediction worse for both
technical/non-technical and civil/uncivil classiﬁcation. The
eﬀect is stronger for the non-technical class.

7.3. Incivility Detection in a Cross-Platform Setting (RQ3)

7.3.1. Classiﬁcation into Technical and Non-technical

Figure 7 (a) presents the performance scores for technical
and non-technical classiﬁcation when (i) training on the code
reviews dataset and testing on the issues dataset and (ii) training
on the issues dataset and testing on the code reviews one.

The classiﬁers’ performances degraded to classify non-
technical discussions in a cross-platform setting. When train-
ing our classiﬁers on the code reviews dataset and testing them
on the issues dataset, we observe that BERT is the best clas-
siﬁer, with a nMCC score of 0.62. Our results show that the
classiﬁers’ performances are not satisfactory to precisely clas-
sify non-technical discussions (red color), with precision scores
ranging from 0.10 (SVM) to 0.39 (BERT and RF). Interestingly,
the Logistic Regression (LR) and Naive Bayes (NB) classiﬁers
can retrieve a signiﬁcant percentage of non-technical discus-
sions (recallLR = 0.88, recallNB = 0.85); even though they fail

11

Figure 4: Average performance scores per class balancing technique and classiﬁer for the classiﬁcation of civil and uncivil sentences (CT2).

Figure 5: Performance scores per target class for the classiﬁcation of civil and uncivil sentences (CT2).

to precisely classify such cases (precisionLR = 0.16, precisionNB =
0.17). We also observe a similar pattern when training on the is-
sues dataset and testing on the code reviews dataset. The MCC
scores ranged from 0.50 (KNN and SVM) to 0.62 (BERT).
In this setting, BERT is better (precision = 0.51) than in the
previous setting (precision = 0.39) to precisely classify non-
technical discussions, yet the coverage is lower (recall = 0.10
in this setting, recall = 0.30 in the previous setting). Surpris-
ingly, the Logistic Regression classiﬁer has a similar recall for
both target classes (recalltechnical = 0.61, recallnon_technical =

12

0.64).

7.3.2. Classiﬁcation into Civil and Uncivil

Figure 7 (b) presents the classiﬁcation results for the civil
and uncivil classiﬁcation when (i) training on the code reviews
dataset and testing on the issues dataset and (ii) training on the
issues dataset and testing on the code reviews dataset.

The classiﬁers’ performances are also degraded to clas-
sify civil sentences in a cross-platform setting. Concerning
the classiﬁcation into civil and uncivil and training on code re-

(a) Technical and non-technical classiﬁcation (CT1)

(b) Civil and uncivil classiﬁcation (CT2)

Figure 6: Diﬀerence of BERT’s performance scores between RQ1 (without context information) and RQ2 (with context information).

(a) Technical and non-technical classiﬁcation (CT1)

(b) Civil and uncivil classiﬁcation (CT2)

Figure 7: Performance scores for classiﬁcation in a cross-platform setting.

views and testing on issues, we observe that all classiﬁers are
able to precisely classify uncivil discussions with precision ≈
0.7 with a good coverage (recall ≈ 0.8). However, all classi-
ﬁers have low precision (ranging from 0.35 to 0.53) and low
recall (ranging from 0.29 to 0.56). When training on issues and
testing on code reviews, we observe the same pattern as in the
aforementioned conﬁguration, i.e., all classiﬁers can precisely
classify the uncivil class (precision ≈ 0.8) with a recall ≈
0.7. Interestingly, the Logistic Regression classiﬁer has a re-
call higher for the civil class (recall = 0.82) than the uncivil
class (recall = 0.42).

Summary RQ3: None of the classiﬁers are eﬀective to
classify non-technical and civil discussions in a cross- plat-
form setting. However, all classiﬁers were able to perform
well when classifying the technical and uncivil classes in
a cross-platform setting.

8. Discussion

Our results show that BERT performs better than classi-
cal machine learning models in both technical/non-technical
and civil/uncivil classiﬁcation on code review emails and is-
sue comments, with a F1-score higher than 0.9. This result is

13

−0.02−0.2−0.11−0.01−0.14−0.07−0.01−0.22−0.11−0.02−0.04−0.03−0.02−0.21−0.11−0.01−0.08−0.05−0.11−0.05Code reviews datasetIssues datasetTechnicalNon−technicalAverageTechnicalNon−technicalAveragePrecisionRecallF1nMCCBERTBERTBERTBERTBERTBERT−0.20−0.15−0.10−0.050.000.05−0.20−0.15−0.10−0.050.000.05−0.20−0.15−0.10−0.050.000.05−0.20−0.15−0.10−0.050.000.05Δ PMΔ PM =<0>00−0.02−0.02−0.090.01−0.04−0.04−0.01−0.03−0.03−0.01−0.02−0.03−0.02−0.02−0.060−0.03−0.02−0.03Code reviews datasetIssues datasetCivilUncivilAverageCivilUncivilAveragePrecisionRecallF1nMCCBERTBERTBERTBERTBERTBERT−0.20−0.15−0.10−0.050.000.05−0.20−0.15−0.10−0.050.000.05−0.20−0.15−0.10−0.050.000.05−0.20−0.15−0.10−0.050.000.05Δ PMΔ PM =<0>0Train on code reviews and test on issuesTrain on issues and test on code reviewsPrecisionRecallF1nMCCBERTCARTKNNLRNBRFSVMBERTCARTKNNLRNBRFSVM0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00Performance scoresTarget classTechnicalNon−technicalAverageTrain on code reviews and test on issuesTrain on issues and test on code reviewsPrecisionRecallF1nMCCBERTCARTKNNLRNBRFSVMBERTCARTKNNLRNBRFSVM0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00Performance scoresTarget classCivilUncivilAveragesimilar to the ones found in the literature for the classiﬁcation
of sentiments [18, 19, 20] and oﬀensive language [6] in dif-
ferent software engineering artifacts (such as Stack Overﬂow
posts, GitHub issues, API reviews, Jira issues, Gerrit code re-
views, Gitter, and Slack). Hence, this paper contributes to the
literature by demonstrating that BERT can also be used to clas-
sify incivility in code review emails and issue discussions. Fur-
thermore, our results demonstrate that classical machine learn-
ing techniques tend to underperform when classifying non-
technical code review emails and issue comments and civil
sentences in both datasets. Since BERT has a F1-score greater
than 0.90 when identifying both of these target classes, it is un-
clear what are the cases that classical machine learning models
miss and that BERT does not.

Given that BERT had the best performance of all the an-
alyzed classiﬁers, we then assessed whether BERT could be
further improved if the context was added to the text to be
classiﬁed. However, we found that adding the previous code
review email and issue comment makes the prediction of
technical/non-technical code review emails and issue com-
ments and civil/uncivil sentences worse, if not unchanged.
This result echoes Murgia et al. [21], in which the authors found
that the context does not play a signiﬁcant role when classify-
ing emotions in issue comments. But at the same time, this
result is counterintuitive to us, since based on our experience
of manually classifying incivility in code reviews and issue dis-
cussions [1, 4], we would expect that adding the context would
improve the classiﬁers’ performance. One explanation for this
is that such conversations are not “ﬂat” or “linear”; i.e., the con-
text is more complex than the immediate previous email. We
plan to examine ways to capture this complexity in future work.
Finally, we investigated if BERT and classical machine learn-
ing models can be used in a cross-platform setting. Based on
our analysis, we found that the classiﬁers’ performance de-
graded in a cross-platform setting, with BERT being the best
performing model with F1 and nMCC scores below 0.7. Simi-
larly, Qiu et al. [47] also found that classiﬁers have performance
degradation when training on toxic issues and code review com-
ments and testing on pushback in code reviews and vice versa.
While performance degradation is expected, BERT’s classiﬁca-
tion results are way better than random, especially for the tech-
nical and uncivil classes. However, whether this performance
is satisfactory in practice needs future investigation.

Based on these results, we would like to further investi-
gate why BERT performs better than classical machine learn-
ing models (RQ1), why the context makes BERT’s performance
scores worst (RQ2), and why the incivility detectors cannot be
accurately generalized to other platforms (RQ3). To answer
those questions and to have a better understanding of the cases
that each classiﬁer performs better or worse, in the next sections
we aim to assess if there are any tone-bearing discussion fea-
tures (TBDFs) [1] that the analyzed models are unable to
precisely classify. Since the non-technical emails/comments
(CT1) are split into civil and uncivil sentences (CT2) and the
sentence classiﬁcation depends on the tone that they demon-
strate [1], we will then assess the percentage of sentences that
were misclassiﬁed by each classiﬁer per TBDF in CT2. The

14

misclassiﬁed sentences were extracted from the test sets in the
outer fold cross-validation (see Section 5.1). Furthermore, as
demonstrated by Ferreira et al., the TBDFs named confusion,
criticizing oppression, dissatisfaction, and expectation were only
encountered in issue discussions [4] and not in code review dis-
cussions [1].

Figure 8: Percentage of misclassiﬁed sentences per TBDF per classiﬁer.

8.1. Analysis of misclassiﬁed TBDFs per incivility classiﬁer

Contrary to the classical machine learning models, BERT
is able to correctly classify more than 70% of the sentences
for all civil and uncivil TBDFs for the code reviews and is-
sues datasets. As Figure 8 shows, in the code reviews dataset
BERT mostly misclassiﬁes sentences demonstrating the civil
TBDFs friendly joke (28.57%), commanding (22.22%), and
sadness (14.29%) and the uncivil TBDFs irony (16.67%),
threat (15.28%), and vulgarity (15.28%). Although BERT
misses 22.22% of sentences expressing commanding, the clas-
sical machine learning models are worst in classifying this TBDF
(varying from 44.44% for LR and SVM to 88.89% for KNN).
Concerning the friendly joke TBDF, although SVM is as good
as BERT (both models miss 28.57% of sentences with this TBDF),
the other machine learning models misclassify from 42.86%
(for CART, LR, and NB) to 71.43% (for KNN AND RF) sen-
tences. The same happens for commanding and sadness. In-
terestingly, the classical machine learning models perform
better than BERT to identify the uncivil TBDFs that BERT
misses in the code reviews dataset. That is, KNN and SVM
misclassify 14.29% of sentences (instead of 16.67% for BERT)
demonstrating irony; CART and KNN misclassify 12.50% of
sentences (instead of 15.38% for BERT) showing threat; CART,
LR, and SVM do not miss any sentence demonstrating vulgar-
ity, while BERT misses 15.38%.

15.79%52.63%21.05%10.53%10.53%31.58%26.32%66.67%88.89%44.44%55.56%55.56%77.78%44.44%77.78%55.56%61.11%61.11%61.11%72.22%55.56%42.86%71.43%42.86%42.86%42.86%71.43%28.57%50.00%75.00%75.00%75.00%75.00%75.00%75.00%56.76%56.76%35.14%24.32%24.32%40.54%37.84%66.67%66.67%0.00%66.67%66.67%66.67%33.33%42.86%85.71%71.43%71.43%71.43%71.43%85.71%30.77%76.92%15.38%15.38%15.38%15.38%30.77%5.56%22.22%5.88%28.57%0.00%5.26%0.00%14.29%0.00%22.22%21.11%17.78%12.22%12.22%6.67%8.89%25.00%27.50%15.00%12.50%12.50%15.00%12.50%57.14%14.29%28.57%42.86%42.86%42.86%14.29%25.64%12.82%17.95%20.51%20.51%12.82%10.26%21.69%13.25%9.64%10.84%10.84%4.82%8.43%12.50%12.50%25.00%25.00%25.00%37.50%37.50%0.00%11.11%0.00%11.11%11.11%11.11%0.00%4.76%0.00%16.67%3.13%0.91%15.38%15.38%1.72%51.72%62.07%22.41%32.76%36.21%86.21%53.19%59.57%55.32%74.47%74.47%85.11%74.47%5.00%25.00%22.50%20.00%17.50%22.50%27.50%50.00%58.33%33.33%61.67%61.67%66.67%78.33%17.65%82.35%64.71%52.94%58.82%47.06%82.35%63.64%72.73%39.39%75.76%75.76%71.21%81.82%27.27%50.00%15.91%34.09%34.09%31.82%75.00%56.52%60.87%43.48%78.26%78.26%60.87%82.61%0.00%66.67%66.67%33.33%33.33%33.33%66.67%37.50%59.38%34.38%59.38%59.38%40.63%84.38%55.56%66.67%55.56%88.89%88.89%88.89%77.78%35.00%67.50%27.50%30.00%30.00%42.50%85.00%31.03%58.62%20.69%27.59%27.59%41.38%68.97%1.72%8.51%5.26%13.56%17.65%12.90%4.44%26.09%0.00%12.50%11.11%9.52%3.23%31.10%22.97%27.92%14.13%14.13%10.25%9.19%25.93%27.78%22.22%12.96%12.96%12.96%12.96%27.87%31.15%27.87%26.23%26.23%11.48%18.03%23.78%28.65%23.78%14.59%14.59%9.19%11.35%25.86%27.59%12.07%17.24%17.24%4.60%12.64%20.00%26.67%20.00%26.67%26.67%13.33%6.67%41.94%41.94%25.81%12.90%12.90%16.13%22.58%2.17%1.89%4.76%1.66%2.20%0.00%3.03%Code reviews datasetIssues datasetCivilUncivilBERTCARTKNNLRNBRFSVMBERTCARTKNNLRNBRFSVMSincere apologiesSadnessOppressionHumilityHope to get feedbackFriendly jokeExpectationDissatisfactionCriticizing oppressionConsideratenessConfusionCommandingAppreciation and excitementVulgarityThreatName callingMockingIronyImpatienceAnnoyance and bitter frustrationTBDFs0255075100% misclassified sentences per TBDFFor the issues dataset, BERT mainly misclassiﬁes the fol-
lowing civil TBDFs: friendly joke (26.09%), criticizing op-
pression (17.65%), considerateness (13.56%), and dissatis-
faction (12.90%). None of the uncivil TBDFs had more than
10% of misclassiﬁed sentences for this dataset. Furthermore,
none of the classical machine learning models can classify the
aforementioned TBDFs better than BERT.

Observation 1: For each TBDF, BERT can correctly clas-
sify more than 70% of the sentences in both datasets. The
TBDFs Friendly joke and commanding are among the most
diﬃcult civil TBDFs to classify, while irony and vulgarity
are the most diﬃcult uncivil TBDFs.

8.2. Analysis of BERT’s misclassiﬁed TBDFs considering the

context
Figure 9 presents the percentage of misclassiﬁed sentences
per TBDF for BERT considering the context for both datasets.
Surprisingly, for the code reviews dataset, commanding, friendly
joke, sadness, and threat that were most frequently misclassi-
ﬁed by BERT without the context (see Section 8.1) now have
100% of the sentences correctly classiﬁed. Irony has a slightly
decreased number of misclassiﬁed sentences, going from 16.67%
to 14.29%, and vulgarity has an increased number of misclassi-
ﬁed sentences, by 9.62% (from 15.38% to 25%). Additionally,
appreciation and excitement, sincere apologies, impatience, and
mocking were more frequently misclassiﬁed considering the con-
text than without the context, increasing the number of misclas-
siﬁed sentences by 9.82%, 11.11%, 7.14%, and 7.98%, respec-
tively.

Figure 9: Percentage of misclassiﬁed sentences per TBDF for BERT consider-
ing the context.

For the issues dataset, the number of misclassiﬁed sentences
was decreased by 16.09% for the friendly joke TBDF and by

15

1.67% for the considerateness TBDF. Criticizing oppression
has an increased number of misclassiﬁed sentences by 11.76%,
and dissatisfaction by 0.29%. Furthermore, expectation, sad-
ness, and sincere apologies were the most impacted TBDFs, in-
creasing the number of misclassiﬁed sentences by 8.38%, 22.62%,
and 14.16%, respectively.

Observation 2: Even though some TBDFs that were mis-
classiﬁed by BERT without context have improved, many
others have deteriorated.

8.3. Analysis of misclassiﬁed TBDFs in cross-platform settings
When training on code reviews and testing on issues,
BERT misclassiﬁes more than 50% of the sentences demon-
strating confusion, dissatisfaction, oppression, criticizing op-
pression, commanding, considerateness, and sadness. Fig-
ure 10 presents the percentage of misclassiﬁed sentences per
TBDF in cross-platform settings.
It is expected that BERT’s
performance is decreased for confusion, criticizing oppression,
dissatisfaction, and expectation, since those TBDFs are not present
in the code reviews dataset; hence, BERT never saw examples
of these TBDFs in the training set. Interestingly, in this set-
ting, BERT classiﬁes all instances of hope to get feedback cor-
rectly, and it misses up to 16% for name calling (7.14%), an-
noyance and bitter frustration (11.09%), impatience (13.58%),
and sincere apologies (16.13%). The classical machine learn-
ing models tend to misclassify more sentences than BERT,
except for the vulgarity with KNN, and irony and mocking
with LR.

When training on issues and testing on code reviews,
BERT tends to misclassify more than 50% of the sentences
classiﬁed as sadness, friendly joke, and considerateness. Sim-
ilar to the other cross-platform setting, BERT classiﬁes all in-
stances of hope to get feedback correctly and it misses only
9.23% of sentences related to vulgarity, 11.88% related to mock-
ing, and 12.36% related to name calling. Interestingly, in this
setting classical machine learning models are better than
BERT to classify various TBDFs, such as commanding (CART
and LR), friendly joke (LR), humility (LR), oppression (LR),
sincere apologies (LR), annoyance and bitter frustration (RF),
impatience (RF), irony (CART, KNN, RF, and SVM), mock-
ing (RF and SVM), name calling (SVM), and threat (NB and
SVM).

Observation 3: In a cross-platform setting, the accuracy
of all TBDFs degraded for BERT. The TBDFs that are the
most challenging to correctly classify are commanding,
considerateness, oppression, and sadness for both datasets.
For the issues dataset, BERT also fails to classify friendly
joke.

9. Threats to Validity

We discuss threats to the study validity [75] as follows.

15.38%0.00%7.14%0.00%0.00%5.88%0.00%0.00%11.11%0.00%7.14%11.11%8.89%0.00%25.00%14.29%3.45%15.00%8.33%11.88%29.41%13.19%12.82%10.00%0.00%15.15%10.00%32.14%17.39%2.74%0.00%1.33%3.07%2.87%0.00%0.00%Code reviews datasetIssues datasetCivilUncivilBERTBERTSincere apologiesSadnessOppressionHumilityHope to get feedbackFriendly jokeExpectationDissatisfactionCriticizing oppressionConsideratenessConfusionCommandingAppreciation and excitementVulgarityThreatName callingMockingIronyImpatienceAnnoyance and bitter frustrationTBDFs0255075100% misclassified sentences per TBDFing search spaces deﬁned in the literature. Additionally, our
model evaluation is based on the average metric values of a 5-
fold cross-validation.

Conclusion Validity. All our validations (either to ﬁnd the best
hyperparameter or to compare the models) are based on the
nMCC metric, which is known to be more interpretable and
to have more robust results than other performance metrics.

External Validity. Our incivility classiﬁers are limited to code
reviews of rejected patches of the Linux Kernel Mailing List
and to GitHub issues locked as too heated. Hence, our results
may not be generalizable to other software engineering commu-
nication artifacts; this includes the results of cross-platform per-
formance. Concerning the features used by the classical tech-
niques, we have experience with incivility studies and we man-
ually coded the data in our previous work [1, 4]. Hence, we
were able to assess if the features are accurate to the incivility
domain. Finally, our results are conﬁned to the models imple-
mented in this study. It is unknown if other models that would
perform better for incivility classiﬁcation.

Figure 10: Percentage of misclassiﬁed sentences per TBDF in cross-platform
settings.

10. Conclusion

Construct Validity. The incivility dataset from Ferreira et al. [1,
4] might contain noise (such as the source code, words other
than English, special characters, etc..) that can aﬀect the mod-
els’ performance. To mitigate this threat, we followed strict
steps to preprocess the text (Section 3.1). Hence, we expect to
have removed the noise in the data. Furthermore, the set of fea-
tures used for the classical models might not represent all con-
founding factors in incivility. We minimize this threat by adopt-
ing the features from a previous work focused on characterizing
sentences in issue discussions [27]. Finally, when assessing if
the contextual information helps to detect incivility, the pres-
ence of civil or uncivil words and technical or non-technical
words in the previous code review email/issue comment can af-
fect the models’ performance. To mitigate this threat, we com-
puted the number of previous emails and comments that are
technical → non-technical and civil → uncivil and vice versa.
For CT1, we found up to 7.76% code review emails and up to
8.05% issues comments in this situation. For CT2, we found
up to 5.36% code review sentences and up to 12.68% of issue
sentences in this situation. Given the relative low number of
datapoints in such a situation, we think that our results will not
be highly aﬀected by that.

Internal Validity. The models might overﬁt due to the small
number of labeled datapoints. To address this problem, we im-
plemented four data augmentation techniques with eight com-
binations of hyperparameters to ensure optimal results. Addi-
tionally, the imbalance in the datasets may lead to poor per-
formance. To minimize this threat, we compared three class
balancing techniques and assessed their performance. Finally,
the choice of hyperparameters might aﬀect the results. For that,
we did hyperparameter optimization on all seven models us-

16

Open source communities have developed mechanisms for
handling uncivil discourse. However, the current mechanisms
require considerable manual eﬀorts and human intervention.
Automated techniques to detect uncivil conversations in open
source discussions can help alleviate such challenges. In this
paper, we compared six classical machine learning techniques
with BERT when detecting incivility in open source code re-
view and issue discussions. Furthermore, we assessed if adding
contextual information improves the classiﬁers’ performance
and if the seven classiﬁers can be used in a cross-platform set-
ting to detect incivility. In our analysis, we identiﬁed BERT as
the best model to detect incivility in both code review and issue
discussions. Furthermore, our results show that classical ma-
chine learning models tend to underperform when classifying
non-technical and civil conversations. Finally, we found that
adding the context does not improve BERT’s performance and
that the classiﬁers’ performance degraded in a cross-platform
setting. We provide three insights on the tones that the classi-
ﬁers misclassify when detecting incivility. These insights will
help future work that aims at leveraging discussion tones in au-
tomated incivility detection applications, as well as improving
cross-platform incivility detection performance.

Acknowledgements

The authors would like to thank Calcul Québec for the com-
puting hardware that enabled to them run the experiments of
this study. The authors also thank the Natural Sciences and En-
gineering Research Council of Canada for funding this research
through the Discovery Grants Program [RGPIN-2018-04470].

31.03%60.34%67.24%56.90%62.07%58.62%91.38%71.49%42.55%59.57%68.09%63.83%70.21%70.21%83.33%66.67%58.33%50.00%66.67%41.67%91.67%70.51%50.00%75.00%68.33%38.33%60.00%55.00%76.47%70.59%82.35%70.59%76.47%88.24%94.12%81.29%37.88%71.21%62.12%50.00%63.64%54.55%47.11%56.82%56.82%86.36%59.09%61.36%65.91%43.48%78.26%60.87%73.91%47.83%82.61%65.22%0.00%66.67%100.00%33.33%66.67%100.00%66.67%37.50%46.88%81.25%46.88%21.88%21.88%43.75%40.63%77.78%77.78%77.78%77.78%33.33%77.78%55.56%60.00%47.50%77.50%60.00%35.00%60.00%45.00%16.13%58.62%55.17%51.72%17.24%27.59%34.48%11.09%48.06%26.15%25.09%34.28%26.50%29.33%13.58%50.00%27.78%22.22%33.33%29.63%33.33%38.73%42.62%32.79%18.03%39.34%26.23%32.79%23.31%45.41%22.16%15.14%35.14%22.16%22.70%7.14%48.28%25.29%20.69%33.91%22.41%25.86%24.00%33.33%33.33%26.67%40.00%40.00%33.33%29.70%48.39%19.35%38.71%38.71%32.26%45.16%30.00%57.89%68.42%31.58%57.89%94.74%89.47%46.67%33.33%66.67%33.33%55.56%77.78%88.89%60.00%72.22%66.67%38.89%100.00%83.33%100.00%71.43%57.14%85.71%14.29%71.43%85.71%100.00%0.00%50.00%100.00%25.00%75.00%100.00%75.00%45.79%48.65%70.27%13.51%67.57%64.86%91.89%40.00%33.33%100.00%0.00%33.33%100.00%66.67%80.00%71.43%85.71%57.14%57.14%85.71%100.00%25.71%15.38%61.54%0.00%30.77%7.69%92.31%20.63%31.11%21.11%42.22%16.67%7.78%8.89%33.50%45.00%32.50%50.00%35.00%5.00%7.50%43.33%28.57%28.57%57.14%57.14%28.57%28.57%11.88%30.77%30.77%33.33%15.38%7.69%7.69%12.36%27.71%25.30%32.53%14.46%7.23%3.61%18.46%50.00%25.00%87.50%0.00%12.50%0.00%9.23%55.56%22.22%11.11%22.22%22.22%11.11%Train on code reviews and test on issuesTrain on issues and test on code reviewsCivilUncivilBERTCARTKNNLRNBRFSVMBERTCARTKNNLRNBRFSVMSincere apologiesSadnessOppressionHumilityHope to get feedbackFriendly jokeExpectationDissatisfactionCriticizing oppressionConsideratenessConfusionCommandingAppreciation and excitementVulgarityThreatName callingMockingIronyImpatienceAnnoyance and bitter frustrationTBDFs0255075100% misclassified sentences per TBDFReferences

[1] I. Ferreira, J. Cheng, B. Adams, The “shut the f** k up" phenomenon:
Characterizing incivility in open source code review discussions, Pro-
ceedings of the ACM on Human-Computer Interaction 5 (CSCW2)
(2021) 1–35.

[2] N. Raman, M. Cao, Y. Tsvetkov, C. Kästner, B. Vasilescu, Stress and
burnout in open source: Toward ﬁnding, understanding, and mitigating
unhealthy interactions, in: Proceedings of the ACM/IEEE 42nd Interna-
tional Conference on Software Engineering: New Ideas and Emerging
Results, ICSE-NIER ’20, Association for Computing Machinery, New
York, NY, USA, 2020, p. 57–60.

[3] C. D. Egelman, E. Murphy-Hill, E. Kammer, M. M. Hodges, C. Green,
C. Jaspan, J. Lin, Predicting developers’ negative feelings about code re-
view, in: Proceedings of the ACM/IEEE 42nd International Conference
on Software Engineering, ICSE ’20, Association for Computing Machin-
ery, New York, NY, USA, 2020, p. 174–185.

[4] I. Ferreira, B. Adams, J. Cheng, How heated is it? understanding github
locked issues, in: Proceedings of the 19th International Conference on
Mining Software Repositories, 2022.

[5] A. Rahman, L. Williams, Source code properties of defective infrastruc-
ture as code scripts, Information and Software Technology 112 (2019)
148–163.

[6] J. Cheriyan, B. T. R. Savarimuthu, S. Craneﬁeld, Towards oﬀensive lan-
guage detection and reduction in four software engineering communities,
in: Evaluation and Assessment in Software Engineering, 2021, pp. 254–
259.

[7] K. D. A. Carillo, J. Marsan, "the dose makes the poison"-exploring the

toxicity phenomenon in online communities (2016).

[8] C. Miller, S. Cohen, D. Klug, B. Vasilescu, C. Kästner, "did you miss
my comment or what?" understanding toxicity in open source discussions
(2022).

[9] J. Sarker, A. K. Turzo, A. Bosu, A benchmark study of the contempo-
rary toxicity detectors on software engineering interactions, in: 2020 27th
Asia-Paciﬁc Software Engineering Conference (APSEC), IEEE, 2020,
pp. 218–227.

[10] B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza, R. Oliveto, Senti-
ment analysis for software engineering: How far can we go?, in: Proceed-
ings of the 40th international conference on software engineering, 2018,
pp. 94–104.

[11] N. Novielli, A. Serebrenik, Sentiment and emotion in software engineer-

ing, IEEE Software 36 (5) (2019) 6–23.

[12] N. Novielli, F. Calefato, F. Lanubile, Love, joy, anger, sadness, fear, and
surprise: Se needs special kinds of ai: A case study on text mining and
se, IEEE Software 37 (3) (2020) 86–91.

[13] Heat detector, https://github.com/SOBotics/HeatDetector, last

access: 2022-02-13.

[14] Locking

conversations,

2014-06-09-locking-\conversations/,
13.

https://github.blog/
2022-02-

last access:

[15] K. Coe, K. Kenski, S. A. Rains, Online and uncivil? patterns and determi-
nants of incivility in newspaper website comments, Journal of Communi-
cation 64 (4) (2014) 658–679.

[16] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
bidirectional transformers for language understanding, arXiv preprint
arXiv:1810.04805 (2018).

[17] S. González-Carvajal, E. C. Garrido-Merchán, Comparing bert
against traditional machine learning text classiﬁcation, arXiv preprint
arXiv:2005.13012 (2020).

[18] E. Biswas, M. E. Karabulut, L. Pollock, K. Vijay-Shanker, Achieving
reliable sentiment analysis in the software engineering domain using bert,
in: 2020 IEEE International Conference on Software Maintenance and
Evolution (ICSME), IEEE, 2020, pp. 162–173.

[19] H. Batra, N. S. Punn, S. K. Sonbhadra, S. Agarwal, Bert-based sentiment
analysis: A software engineering perspective, in: International Confer-
ence on Database and Expert Systems Applications, Springer, 2021, pp.
138–148.

[20] J. Wu, C. Ye, H. Zhou, Bert for sentiment classiﬁcation in software en-
gineering, in: 2021 International Conference on Service Science (ICSS),
IEEE, 2021, pp. 115–121.

[21] A. Murgia, P. Tourani, B. Adams, M. Ortu, Do developers feel emotions?
an exploratory analysis of emotions in software artifacts, in: Proceedings

17

of the 11th working conference on mining software repositories, 2014,
pp. 262–271.

[22] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,
J. Gao, Deep learning–based text classiﬁcation: a comprehensive review,
ACM Computing Surveys (CSUR) 54 (3) (2021) 1–40.

[23] K. Shah, H. Patel, D. Sanghvi, M. Shah, A comparative analysis of logis-
tic regression, random forest and knn models for the text classiﬁcation,
Augmented Human Research 5 (1) (2020) 1–16.

[24] T. Pranckeviˇcius, V. Marcinkeviˇcius, Comparison of naive bayes, ran-
dom forest, decision tree, support vector machines, and logistic regres-
sion classiﬁers for text reviews classiﬁcation, Baltic Journal of Modern
Computing 5 (2) (2017) 221.

[25] W. Dai, G.-R. Xue, Q. Yang, Y. Yu, Transferring naive bayes classiﬁers

for text classiﬁcation, in: AAAI, Vol. 7, 2007, pp. 540–545.

[26] R. J. Lewis, An introduction to classiﬁcation and regression tree (cart)
analysis, in: Annual meeting of the society for academic emergency
medicine in San Francisco, California, Vol. 14, Citeseer, 2000.

[27] D. Arya, W. Wang, J. L. Guo, J. Cheng, Analysis and detection
of information types of open source software issue discussions,
in:
2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE), IEEE, 2019, pp. 454–464.

[28] M. Chouchen,

J. Olongo, A. Ouni, M. W. Mkaouer, Predicting
code review completion time in modern code review, arXiv preprint
arXiv:2109.15141 (2021).

[29] A. Uchôa, C. Barbosa, D. Coutinho, W. Oizumi, W. K. Assunçao, S. R.
Vergilio, J. A. Pereira, A. Oliveira, A. Garcia, Predicting design impactful
changes in modern code review: A large-scale empirical study, in: 2021
IEEE/ACM 18th International Conference on Mining Software Reposito-
ries (MSR), IEEE, 2021, pp. 471–482.

[30] Y. Bengio, R. Ducharme, P. Vincent, A neural probabilistic language

model, Advances in Neural Information Processing Systems 13 (2000).

[31] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed rep-
resentations of words and phrases and their compositionality, Advances
in neural information processing systems 26 (2013).

[32] S. Ili´c, E. Marrese-Taylor, J. A. Balazs, Y. Matsuo, Deep contextual-
ized word representations for detecting sarcasm and irony, arXiv preprint
arXiv:1809.09795 (2018).

[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural
information processing systems 30 (2017).

[34] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving lan-

guage understanding by generative pre-training (2018).

[35] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models
are few-shot learners, Advances in neural information processing systems
33 (2020) 1877–1901.

[36] Bookcorpus, https://yknzhu.wixsite.com/mbweb,

last access:

2022-02-13.

[37] English wikipedia, https://en.wikipedia.org/wiki/English_

Wikipedia, last access: 2022-02-13.

[38] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pretrain-
ing approach, arXiv preprint arXiv:1907.11692 (2019).

[39] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, Albert:
A lite bert for self-supervised learning of language representations, arXiv
preprint arXiv:1909.11942 (2019).

[40] V. Sanh, L. Debut, J. Chaumond, T. Wolf, Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter, arXiv preprint arXiv:1910.01108
(2019).

[41] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, O. Levy, Span-
bert: Improving pre-training by representing and predicting spans, Trans-
actions of the Association for Computational Linguistics 8 (2020) 64–77.
[42] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang, et al., Codebert: A pre-trained model for programming
and natural languages, arXiv preprint arXiv:2002.08155 (2020).

[43] J. Daxenberger, M. Ziegele, I. Gurevych, O. Quiring, Automatically de-
tecting incivility in online discussions of news media, in: 2018 IEEE 14th
International Conference on e-Science (e-Science), IEEE, 2018, pp. 318–
319.

[44] F. Sadeque, S. Rains, Y. Shmargad, K. Kenski, K. Coe, S. Bethard, Inci-
vility detection in online comments, in: Proceedings of the eighth joint

[69] R. Baeza-Yates, B. Ribeiro-Neto, et al., Modern information retrieval,

Vol. 463, ACM press New York, 1999.

[70] B. W. Matthews, Comparison of the predicted and observed secondary
structure of t4 phage lysozyme, Biochimica et Biophysica Acta (BBA)-
Protein Structure 405 (2) (1975) 442–451.

[71] D. Chicco, G. Jurman, The advantages of the matthews correlation coeﬃ-
cient (mcc) over f1 score and accuracy in binary classiﬁcation evaluation,
BMC genomics 21 (1) (2020) 1–13.

[72] R. Croft, D. Newlands, Z. Chen, M. A. Babar, An empirical study of
rule-based and learning-based approaches for static application security
testing, in: Proceedings of the 15th ACM/IEEE International Symposium
on Empirical Software Engineering and Measurement (ESEM), 2021, pp.
1–12.

[73] D. Chicco, V. Starovoitov, G. Jurman, The beneﬁts of the matthews cor-
relation coeﬃcient (mcc) over the diagnostic odds ratio (dor) in binary
classiﬁcation assessment, IEEE Access 9 (2021) 47112–47124.

[74] H. He, Y. Ma, Imbalanced learning foundations, algorithms, and applica-

tions, IEEE Press, Wiley, 2013.

[75] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, A. Wesslén,
Experimentation in software engineering, Springer Science & Business
Media, 2012.

conference on lexical and computational semantics (* SEM 2019), 2019,
pp. 283–291.

[45] S. K. Maity, A. Chakraborty, P. Goyal, A. Mukherjee, Opinion conﬂicts:
An eﬀective route to detect incivility in twitter, Proceedings of the ACM
on Human-Computer Interaction 2 (CSCW) (2018) 1–27.

[46] K. D. A. Carillo, J. Marsan, B. Negoita, Towards developing a theory of
toxicity in the context of free/open source software & peer production
communities, SIGOPEN 2016 (2016).

[47] H. S. Qiu, B. Vasilescu, C. Kästner, C. D. Egelman, C. N. C. Jaspan, E. R.
Murphy-Hill, Detecting interpersonal conﬂict in issues and code review:
Cross pollinating open-and closed-source approaches (2022).

[48] D. Schneider, S. Spurlock, M. Squire, Diﬀerentiating communication
styles of leaders on the linux kernel mailing list, in: Proceedings of the
12th International Symposium on Open Collaboration, 2016, pp. 1–10.

[49] D. Gachechiladze, F. Lanubile, N. Novielli, A. Serebrenik, Anger and
its direction in collaborative software development, in: 2017 IEEE/ACM
39th International Conference on Software Engineering: New Ideas and
Emerging Technologies Results Track (ICSE-NIER), IEEE, 2017, pp. 11–
14.

[50] Linux kernel’s list of maintainers, https://github.com/torvalds/

linux/blob/master/MAINTAINERS, last access: 2022-02-13.

[51] J. Wei, K. Zou, Eda: Easy data augmentation techniques for boosting
performance on text classiﬁcation tasks, arXiv preprint arXiv:1901.11196
(2019).

[52] N. Japkowicz, S. Stephen, The class imbalance problem: A systematic

study, Intelligent data analysis 6 (5) (2002) 429–449.

[53] G. E. Batista, R. C. Prati, M. C. Monard, A study of the behavior
of several methods for balancing machine learning training data, ACM
SIGKDD explorations newsletter 6 (1) (2004) 20–29.

[54] Nltk’s

list of

english stopwords,

https://gist.github.com/

sebleier/\554280, last access: 2021-07-23.

[55] C. Padurariu, M. E. Breaban, Dealing with data imbalance in text classi-

ﬁcation, Procedia Computer Science 159 (2019) 736–745.

[56] N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer, Smote: syn-
thetic minority over-sampling technique, Journal of artiﬁcial intelligence
research 16 (2002) 321–357.

[57] J. D. Rennie, L. Shih, J. Teevan, D. R. Karger, Tackling the poor assump-
tions of naive bayes text classiﬁers, in: Proceedings of the 20th interna-
tional conference on machine learning (ICML-03), 2003, pp. 616–623.

[58] M. Goudjil, M. Koudil, M. Bedda, N. Ghoggali, A novel active learning
method using svm for text classiﬁcation, International Journal of Automa-
tion and Computing 15 (3) (2018) 290–298.

[59] J. Bergstra, R. Bardenet, Y. Bengio, B. Kégl, Algorithms for hyper-
parameter optimization, Advances in neural information processing sys-
tems 24 (2011).

[60] T. Hugging Face, Bert-base-uncased model, https://huggingface.

co/bert-base-uncased, last access: 2022-03-10.

[61] I. Turc, M.-W. Chang, K. Lee, K. Toutanova, Well-read students learn
better: On the importance of pre-training compact models, arXiv preprint
arXiv:1908.08962v2 (2019).

Face,

[62] T. Hugging
quence
transformers/v4.19.2/en/model_doc/auto#transformers.
AutoModelForSequenceClassification, last access: 2022-03-10.

se-
Auto model
https://huggingface.co/docs/

classiﬁcation,

classes:

Auto

for

[63] J. Snoek, H. Larochelle, R. P. Adams, Practical bayesian optimization of
machine learning algorithms, Advances in neural information processing
systems 25 (2012).

[64] T. Hugging Face, Hyperparameter search, https://huggingface.co/
docs/transformers/main_classes/trainer#transformers.
Trainer.hyperparameter_search, last access: 2022-03-10.

[65] T. Hugging Face, Trainer class, https://huggingface.co/docs/
transformers/main_classes/trainer, last access: 2022-03-10.
[66] Q. Liu, X. Wang, X. Huang, X. Yin, Prediction model of rock mass
class using classiﬁcation and regression tree integrated adaboost algo-
rithm based on tbm driving data, Tunnelling and Underground Space
Technology 106 (2020) 103595.

[67] R. Shu, T. Xia, L. Williams, T. Menzies, Better security bug re-
port classiﬁcation via hyperparameter optimization, arXiv preprint
arXiv:1905.06872 (2019).

[68] C.-W. Hsu, C.-C. Chang, C.-J. Lin, et al., A practical guide to support

vector classiﬁcation (2003).

18

