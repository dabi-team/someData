2
2
0
2

g
u
A
3

]
E
S
.
s
c
[

1
v
1
7
9
1
0
.
8
0
2
2
:
v
i
X
r
a

API Usage Recommendation via Multi-View
Heterogeneous Graph Representation Learning

1

Yujia Chen†, Xiaoxue Ren‡, Cuiyun Gao∗†, Yun Peng‡, Xin Xia§, and Michael R. Lyu‡
†Harbin Institute of Technology, Shenzhen, China
‡The Chinese University of Hong Kong, Hong Kong, China
§Software Engineering Application Technology Lab, Huawei, China
gaocuiyun@hit.edu.cn, yujiachen@stu.hit.edu.cn
{ypeng, xiaoxueren, lyu}@cse.cuhk.edu.hk, xin.xia@acm.org

Abstract—Developers often need to decide which APIs to use for the functions being implemented. With the ever-growing number of
APIs and libraries, it becomes increasingly difﬁcult for developers to ﬁnd appropriate APIs, indicating the necessity of automatic API
usage recommendation. Previous studies adopt statistical models or collaborative ﬁltering methods to mine the implicit API usage
patterns for recommendation. However, they rely on the occurrence frequencies of APIs for mining usage patterns, thus prone to fail for
the low-frequency APIs. Besides, prior studies generally regard the API call interaction graph as homogeneous graph, ignoring the rich
information (e.g., edge types) in the structure graph. In this work, we propose a novel method named MEGA for improving the
recommendation accuracy especially for the low-frequency APIs. Speciﬁcally, besides call interaction graph, MEGA considers another
two new heterogeneous graphs: global API co-occurrence graph enriched with the API frequency information and hierarchical structure
graph enriched with the project component information. With the three multi-view heterogeneous graphs, MEGA can capture the API
usage patterns more accurately. Experiments on three Java benchmark datasets demonstrate that MEGA signiﬁcantly outperforms the
baseline models by at least 19% with respect to the Success Rate@1 metric. Especially, for the low-frequency APIs, MEGA also
increases the baselines by at least 55% regarding the Success Rate@1 score.

Index Terms—API recommendation, multi-view heterogeneous graphs, graph representation learning

(cid:70)

1 INTRODUCTION

In the daily software development process, develop-
ers often use the application programming interface (API)
provided by some libraries to reduce development time
when implementing a function. For instance,
the API
Buf f eredInputStream.read() provides an efﬁcient way to
read data from an input stream and store the data in a buffer
array. However, it is difﬁcult for developers to be familiar
with all APIs, because APIs are extensive in quantity and
rapidly evolving [1], [2]. In the past two decades, the num-
ber of Java Development Kit (JDK) APIs has increased more
than 20 times (from 211 classes in the ﬁrst version of 1996 to
4,403 classes in 2022) [3] [4]. Therefore, when selecting APIs,
developers often refer to ofﬁcial technical documentation,
raise questions on sites (e.g., Stack Overﬂow), or query
on search engines (e.g., Google), etc. Obviously, the whole
process relies on developers’ experience, and could be time-
consuming since useful information is usually buried in
massive contents [5], [6].

Regarding the issues above, previous studies [7], [8] pro-
pose to automatically recommend a list of API candidates
according to previously-written code, which is demon-
strated to be beneﬁcial for improving the API searching
process and facilitating software development. For example,
MAPO [9] and UP-Miner [10] are based on mining frequent
patterns clusters from collected projects to obtain common
API usage patterns. PAM [11] uses probabilistic modelling

* corresponding author.

technique in API call sequence to mine usage patterns.
FOCUS [12] uses a context-aware collaborative-ﬁltering [13]
technique to recommend APIs, relying on the similarity
between methods and projects. GAPI [14] applies graph
neural networks [15] based collaborative ﬁltering to exploit
the relationship between methods and APIs. However, these
techniques focus on recommending commonly-used APIs,
and tend to fail to mine the usage patterns of the low-
frequency APIs. According to our analysis in Section 2, the
low-frequency APIs occupy a signiﬁcant proportion of all
APIs. According to the statistics, the rarely-appeared APIs
account for 76% of the whole APIs in the SHL dataset [12].
Nevertheless, the recommendation success rate of rare APIs
(7.9%) is much lower than that of common APIs (54.2%).
Thus, how to effectively learn low-frequency APIs usage
patterns is a great yet under-explored challenge [16].
Besides, the existing techniques highly rely on the homoge-
neous interaction information between APIs and methods,
ignoring the rich contextual information in source code (e.g.,
co-occurring APIs and hierarchical structure in projects and
packages). In fact, APIs under the same package are more
likely to be called together (e.g., ﬁle.open() and ﬁle.close())
are under the package java.io), which is important external
information for API recommendation. Therefore, how to
involve contextual information in API recommendation
is another challenge.

In this work, we propose MEGA, a novel API usage
recommendation method with Multi-view hEterogeneous
Graph representAtion learning. Different from the prior

 
 
 
 
 
 
studies, MEGA employs heterogeneous graphs, which are
constructed from multiple views, i.e., method-API interac-
tion from local view, API-API co-occurrence from global
view, and project structure from external view. Speciﬁcally,
MEGA builds upon three heterogeneous graphs, i.e., the
common call interaction graph, and two new graphs, i.e.,
global API co-occurrence graph and hierarchical structure graph.
The call interaction graph establishes the relations between
methods and corresponding called APIs, and is commonly
adopted by previous approaches [11], [12], [14], [17], [18].
Models based on only such graph cannot well capture the
representations of the APIs with rare called frequencies. To
improve the API representations, the global API co-occurrence
graph is introduced to build the relations between APIs
with the co-occurrence frequencies incorporated. To enrich
the representations of APIs and methods with contextual
structure information, MEGA also involves the called infor-
mation by projects and packages, composing the hierarchical
structure graph. A graph representation model is then pro-
posed to learn the matching scores between methods and
APIs based on the multi-view graphs. To integrate the multi-
view knowledge, a frequency-aware attentive network and
a structure-aware attentive network are proposed to encode
the co-occurrence information and hierarchical structure,
respectively.

We evaluate the effectiveness of MEGA on three Java
benchmark datasets consisting of 610 Java projects from
GitHub and 868 JAR archives from the Maven Central
Repository. In addition, we also simulate the real develop-
ment scenario [12] where a developer has already called
some APIs in a method. Then MEGA recommends APIs
based on the called APIs by client methods, and calculates
the evaluation metrics. The experimental results show that
MEGA outperforms the baseline approaches (PAM [11],
FOCUS [12] and GAPI [14]) by at least 19% with respect
to the Success Rate@1 metric. For the low-frequency APIs,
MEGA also achieves an increased rate at more than 55%
compared to the baselines.

In summary, our main contributions in this paper are as

follows:

• To the best of our knowledge, we are the ﬁrst work
to construct multi-view heterogeneous graphs for more
accurate API usage recommendation.

• We propose a novel API recommendation approach
named MEGA, which designs a graph representation
model with a frequency-aware attentive network and
a structure-aware attentive network to generate en-
hanced representations of methods and APIs.

• We perform experiments on three benchmark datasets,
and the results demonstrate that MEGA outperforms
the state-of-the-art API usage recommendation ap-
proaches, even for the low-frequency APIs.

Outline. The rest of paper is organized as follows: Section 2
introduces details of our motivation. Section 3 presents the
overall workﬂow of MEGA and architecture of the graph
representation model in MEGA. Section 4 and Section 5 are
the settings and results of evaluation, respectively. Section 6
analyzes some implications and threats to validity. Section 7
succinctly describes related works. In the end, in Section 8,
we conclude the whole work.

2 MOTIVATION

2

Fig. 1: The distribution of APIs called frequency in SHS,
SHL and MV datasets.

Fig. 2: API recommendation performance of FOCUS [12],
GAPI [14] and our proposed MEGA (corresponding to the
APIs with different frequencies on the SHL dataset).

Figure 1 shows the distribution of APIs with different
occurrence frequencies in three benchmark datasets [12], i.e.,
SHS, SHL and M V , with detailed statistics of the datasets
shown in Table 2. Obviously, APIs with lower occurrence
frequencies (i.e., ≤ 3) account for signiﬁcant proportions
(i.e., > 65%) among all the APIs in each dataset. Although
appearing less frequently, the large proportion of such APIs
indicates developers’ strong demands for speciﬁc functions,
and accurately recommending the APIs is critical for facili-
tating their daily programming.

Figure 3 illustrates an example of a client method which
queries the low-frequency APIs. In this scenario, the de-
veloper is working on a method to get the name of a
JAR package, but is not sure which APIs to use next.
The “true API calls” in Figure 3 list the APIs in ground
truth, in which both “JAXBContext.createUnmarshalle()” and
“Unmarshaller.unmarshal(java.io.InputStream)” are rarely ap-
pear in the datasets. Both popular models including FO-
CUS [12] and GAPI [14] learn the API representations
ineffectively, and fail to recommend the APIs. Figure 2
depicts the API recommendation performance of the two
models corresponding to APIs with different frequencies on
the SHL dataset with respect to the SuccessRate@10 score.

SHSSHLMV0.44.536.47.912.154.231.548.364.10500010000150002000025000010203040506070rare APInormal APIcommon APIAPI TypeNumberFOCUSGAPIMEGA(Ours)SuccessRate@10(%)API Number≤3≥103~103

Fig. 4: The overall workﬂow of MEGA.

TABLE 1: Symbols and corresponding descriptions.

Notation
U,I
E,R
GI
GC
GH
(i, f, j)
(h, r, t)
u, i
L
E l
u
S l
u
u , e(o)
e(o)
i
e(c)
u , e(c)
i
u , e(h)
e(h)
i
eu, ei

Description
The set of methods, APIs
The set of entities, relations
The call interaction graph
The global API co-occurrence graph
The hierarchical structure Graph
The co-occurrence triple
The structure triple
The client method, the target API
The hop number of information encoding
The l-hop entity set of u
The l-hop triple set of u
The local-view representation of the method u,i
The global-view representation of u,i
The external-view representation of u,i
The ﬁanl representation of u,i

class), so it successfully recommends the rare API “JAXB-
Context.CreateUnmarshaller()”.

3 METHODOLOGY

3.1 Workﬂow of MEGA

Figure 4 illustrates the MEGA’s workﬂow which in-
cludes three main stages, i.e., constructing multi-view het-
erogeneous graphs, training a graph representation model
and recommending APIs with the trained model. In the
ﬁrst stage, we construct three heterogeneous graphs, i.e.,
call interaction graph, global API co-occurrence graph, and hi-
erarchical structure graph. The nodes of these graphs include
APIs, methods, classes, projects, and packages. We extract
the relations between nodes from the source code. Then,
in the second stage, a graph representation model is pro-
posed to encode the three graphs and integrate the graph
representations for recommendation. In the last stage, we
employ the trained model to return a ranked list of API
usage recommendation according to the code snippet of
the current client method. The details of our approach are
explained in the following parts. For facilitating readers’
understanding of the proposed approach, we list the key
notations in Table 1.

3.2 Constructing Multi-View Heterogeneous Graphs

In this section, we present the graph construction pro-
cess of the multi-view graphs. Following the formulation
of mainstream recommendation systems [20], we treat all

Fig. 3: An Example of API usage recommendation. (The
true API calls and TOP-3 APIs recommended by FOCUS [12],
GAPI [14] and our proposed MEGA, respectively.)

We ﬁnd that the APIs appearing rarely, e.g., ≤ 3, present
signiﬁcantly poor performance than the APIs appearing
frequently, e.g., ≥ 10. The results show that the existing
models are difﬁcult to recommend the low-frequency APIs.
Besides, existing approaches [10], [12], [14], [18], [19]
generally regard the API call interaction graph as homoge-
neous graph, ignoring the rich heterogeneous information
(e.g., edge types) in the graph. For example, the state-of-
the-art models, FOCUS [12] and GAPI [14] are based on
collaborative ﬁltering for measuring the similarities between
all methods to recommend APIs. The learning process in the
models tends to rely on the commonly-used APIs in meth-
ods, resulting in ineffective API recommendation. As the
example shown in Figure 3, the API recommended by FO-
CUS for improving the speed and efﬁciency of byte stream
operations comes from the BufferedReader class, which is a
very general yet function-irrelevant operation for the cur-
rent client method.

Our approach. To address the above limitations of the
existing models, we try to exploit the rich heterogeneous
information in source code from multiple views, includ-
ing method-API interaction from local view, API-API co-
occurrence from global view, and project structure from ex-
ternal view, respectively. Speciﬁcally, we build three hetero-
geneous graphs from each view, i.e., call interaction graph,
global API co-occurrence graph and hierarchical structure
graph. Moreover, two new attentive networks are designed
for encoding frequency-based co-occurrence information
and structure-based hierarchical information during learn-
ing the representations of APIs and methods. As the exam-
ple shown in Figure 3, MEGA captures the co-occurring pat-
tern with the API “JAXBContext.NewInstance( ¨POM. 4 0¨)”
and their similar structural information (i.e., under the same

Client Method:privateString getJarName(String path, JAXBElement<Model> fhElement){Model model = fhElement.getValue();Modules ms = model.getModules();String originalName = null;if(ms == null) originalName = model.getArtifactId();elseoriginalName = ms.getModule();JAXBContext jaxbContext = JAXBContext.newInstance("pom._4_0");// which APIs are next?     }True API Calls : üJAXBContext.createUnmarshaller()üFileInputStream.FileInputStream(java.lang.String) üUnmarshaller.unmarshal(java.io.InputStream) API Calls  Returned by  FOCUS : lBufferedReader.BufferedReader()lBufferedReader.close()lBufferedReader.readLine()API Calls  Returned by  GAPI : lStreamResult.StreamResult()lDate.getTime()lClass.forName()API Calls  Returned by  OURS : lJAXBContext.createUnmarshaller()lModel.getVersion()lUnmarshaller.unmarshal(java.io.InputStream) SourceCodeExtractCall Interaction GraphHierarchical Structure GraphGlobal API Co-occurrence Grapha.  ConstructingMulti-View Heterogeneous Graphs  clientmethodb. Training a Graph Representation Model  c.RecommendingAPIs withthe trainedmodel TrainedModelclientmethodtargetAPIAPIlistGraph Representation Learning Model4

Fig. 5: Training and recommending process of the graph representation learning model in MEGA.

Algorithm 1: Global API Co-occurrence Graph
Construction(S, ε)

input : An API sequence set S and an integer ε
output: A global API co-occurrence graph

1 Let V and E represent the set of vertices and edges in

the co-occurrence graph respectively;

2 V ← ∅, E ← ∅;
3 Let ω represent a weight function : E → R≥0, where

each edge e in E has a weight ω(e);

4 foreach A in S do
5

append all API nodes in A into V ;
foreach ai in A do

6

7

E ← E ∪ {(ai, aj)|aj ∈ A ∧ j ∈ {j, j + ε}} ;

8 foreach e in E do
9

ω(e) ← ω(e) + 1 ;

10 GC ← build co-occurrence graph with V , E and ω ;
11 return GC ;

methods and APIs in projects as user set U , and item set I.
The graphs are constructed from local view (i.e., method-
API interaction), global view (i.e., API co-occurrence in-
formation), and external view (i.e., hierarchical structure),
respectively.

1) Call Interaction Graph GI. It represents the call
relations between methods and APIs, denoted as a bipartite
graph GI = {(u, yui, i)|u ∈ U, i ∈ I}, where yui = 1
indicates a method u calls an API i. For example, [my-
File.createFile(), 1, java.io.File.exists()] indicates that a method
myFile.createFile() calls an API java.io.File.exists(). The call in-
teraction graph reﬂects the basic relations between APIs and
methods, and is commonly adopted by prior studies [11],
[12], [14], [17], [18].

2) Global API Co-occurrence Graph GC. It records the
co-occurrence relations between APIs, e.g., the two API
ﬁle.open() and ﬁle.close() are connected since they ever ap-
peared together in some methods. Algorithm 1 shows the
pseudo-code for global API co-occurrence graph construc-
tion. The graph is built based on a set of API sequences S
and an integer ε. Speciﬁcally, we ﬁrst initialize the set of
vertices V =∅ and the set of edges E=∅ in the co-occurrence
graph (line 2). Then, for each API sequence A in S, we
append all API nodes in A into V and collect all edges such

Fig. 6: Examples of Hierarchical Structure Graph.

that each edge (ai, aj) s.t. ∀ai ∈ A and ∃j ∈ {i, i + ε},
aj ∈ A (lines 4-7). Next, for each edge e in E, we update
the ω(e) by counting the occurrence frequencies (lines 8-9).
Finally, we build the co-occurrence graph GC based on V ,
E and ω, and return the co-occurrence graph GC (lines 10-
11). GC is denoted as {(i, f, j)|i, j ∈ I, f ∈ T }, where each
triplet describes that API i and API j are invocated together
f times. For example, [ﬁle.open(), 10, ﬁle.close()] indicates that
ﬁle.open() and ﬁle.close() appear together 10 times. The co-
occurrence graph is beneﬁcial for enriching the APIs with
frequency information, which can also implicitly comple-
menting the representations of methods.

3) Hierarchical Structure Graph GH. The hierarchical
information, e.g., the belonging projects/packages, implies
the functionality of APIs and methods, thereby helpful for
API recommendation. We consider both project-level and
package-level information, i.e., the projects where methods
are declared and packages that APIs belong to, for con-
structing the hierarchical structure graph. We construct the
graph as a directed graph, denoted as GH = {(h, r, t)|h, t ∈
E, r ∈ R}, in which each triplet (h, r, t) represents there
is a relation r from head entity h to tail entity t, E is the
set of all entities, including API, method, class, project, and
package, and R is the set of relations including belong-to-
class, belong-to-project and belong-to-package. As the example
depicted in Figure 6, projects and packages are organized
as a tree structure. In the example shown in Figure 6
(a), [keyBasedTweents(), belong-to-class, twitter] denotes that
the method keyBasedTweents() belongs to the class twitter;
[twitter, belong-to-project, com.orange] denotes that the class
twitter belongs to the project com.orange.

Embedding Networkinitialset client  methodtarget  APIcall interaction graph………initialset initial set 𝓔𝒖𝒖𝒊global API co-occurrencegraphhierarchical   structuregraph…co-occurrence set 𝓔𝒖𝒄𝒍…structureset 𝓔𝒖𝒉𝒍…co-occurrence set 𝓔𝒊𝒄𝒍…structureset 𝓔𝒊𝒉𝒍concatenate(1)Call Interaction Encoding(2)Co-occurrence Information Encoding(3)Hierarchical StructureEncoding(4)Fusion & PredictionFrequency-aware Attentive NetworkStructure-aware Attentive Networkmethod   embeddingAPI embedding#𝒚𝒖𝒊𝒖𝒊𝓔𝒊𝒉𝓔𝒊𝒄PC…Mcom.orangetwitter…PCAjava.io…BufferedWritercloseKeyBasedTweets(a) project(b) package3.3 Training Graph Representation Model and Recom-
mendation

This section introduces how MEGA trains a graph rep-
resentation model based on the constructed multi-view
graphs, and utilizes the trained model to make API rec-
ommendation, corresponding to the second stage and third
stage in Figure 4, respectively.

Figure 5 illustrates the whole process of training and
recommendation, including three graph encoding modules,
i.e., call interaction encoding, co-occurrence information encoding
and hierarchical structure encoding, as well as one fusion and
prediction module. Given a client method, a target API, and
the three heterogeneous graphs as input, the graph repre-
sentation model aims to predict the probability of the target
API invocated by the client method. In the ﬁrst module, an
Embedding Network is employed to encode basic interaction
information into local-view representations of the client
method and the target API, as shown in Figure 5 (1). Then, in
the second module, as illustrated in Figure 5 (2), a Frequency-
aware Attentive Network is designed to encode frequency-
based co-occurrence information into representations of the
client method and the target API from global view. Next,
in the third module, a Structure-aware Attentive Network is
designed to encode structure-based hierarchical information
into representations of the client method and the target API
from external view, as shown in Figure 5 (3). Finally, in the
last module, the local-view, global-view and external-view
representations are concatenated as the ﬁnal representations
of the client method and the target API.

3.3.1 Call Interaction Encoding

Call interaction reﬂects basic information of the client
method u and the target API i, respectively. We utilize
them to generate local-view representations following the
prior study [21]. Speciﬁcally, for each client method u, the
called API set is denoted as Eu = {i | i ∈ {i | (u, yui, i) ∈
GI and yui = 1}}. We then obtain the client method u rep-
ei
resentation according to its called API set: e(o)
,
where ei is the embedding of API i and |Eu| is the set size.
Similarly, we obtain the target API i representation e(o)

u =

i∈Eu
|Eu|

(cid:80)

.

i

3.3.2 Co-occurrence Information Encoding

Global co-occurrence reﬂects the frequency-enriched in-
formation of the client method u and the target API i.
According to the deﬁnition of global API co-occurrence graph
in Section 3.2, the value of the edge between a pair of
API nodes denotes their co-occurring frequency, which also
implies their relevant connection. To encode the global-view
information of the client method u and the target API i,
we design a frequency-aware attentive network, as shown in
Figure 7(a). Algorithm 2 shows the pseudo-code for the
encoding. For the client method u, the APIs co-occurred
with its called APIs reveal the method’s potential call need.
Thus, we utilize the API set Eu obtained from section 3.3.1 as
the initial set E (0)
for the ﬁrst-hop propagation on GC (line
u
2). After initialization, we conduct information encoding to
generate co-occurrence representation in each hop.(line 4-8).
Information Encoding (line 4-8). For each triple in
global API co-occurrence graph, i.e., (i, f, j) ∈ GC , we
deﬁne the l-hop triple set based on the entity set E (l)
as:
u

Algorithm 2: Encoding Process of GC and GH

input : a constructed graph G, a number of max-hop L,

AttentiveNetwork, an entity set E

output: a list of representation Ke

5

1 let N (v, G) be the set of v’s neighbors in G;
2 let E be the ﬁrst-hop entity set E (0);
3 Ke ← ∅;
4 for l=0, 1 · · · , L do
5

S(l) ← {(i, e(i, j), j)|i ∈ E (l) ∧ j ∈ N (i, G)};
e(l) ← AttentiveNetwork(S(l));
E (l+1) ← { j |i ∈ E (l) ∧ j ∈ N (i, G)};
append e(l) into Ke;

6

7

8

9 return Ke;

(a) Frequency-aware Attentive
Network

(b) Structure-aware Attentive
Network

Fig. 7: Illustration of two attentive networks in the graph
representation model.

u = {(i, f, j)|i ∈ E l
S l
u} (l begins with 0). Following previous
works [22], [23], we sample a ﬁxed-size triple set instead of
using a full-size set to reduce the computation overhead.
Based on sampled co-occurrence associations,

i.e.,
u, we learn l-hop co-occurrence representation

(i, f, j) ∈ Sl
of the client method u:

e(l)
u =

(cid:88)

π(i, f, j) ej,

(i,f,j)∈S l
u

where coefﬁcient π(i, f, j) is attentively calculated as:

π(i, f, j) =

(cid:80)

exp(mlp(((ei (cid:12) ej)||ef ))

(i(cid:48),f (cid:48),j(cid:48))∈S l
u

exp(mlp((ei(cid:48) (cid:12) ej(cid:48))||ef (cid:48)))

(1)

,

(2)

where the notation (cid:12) denotes the element-wise multiplica-
tion operation, and || denotes the concatenation operation.
ei and ej are the embeddings of API i and its co-occurred
API j, respectively. ef is the embedding of frequency f .
mlp(·) is a three-layer MLP with Relu [24] as the nonlinear
activation function. The attention mechanism for encoding
the l-hop co-occurrence representation (i.e., Equ. (1) and (2))
explicitly introduces co-occurred frequency f into calculat-
ing the inﬂuence of co-occurred API i on the representation
of API j.

After performing L-hop information encoding, where L
is the max hop number, we obtain the global-view repre-
sentations e(c)
u of the client method u by appending the
u , · · · , e(L)
representations from all hops: e(c)
u }.
Similarly, we obtain the global-view representation e(c)
of

u = {e(0)

u , e(1)

i

𝐂𝐨𝐧𝐜𝐚𝐭𝐞𝐧𝐚𝐭𝐞𝐅𝐂𝐬………(a)Frequency-aware Attentive Network𝐂𝐨𝐧𝐜𝐚𝐭𝐞𝐧𝐚𝐭𝐞𝐅𝐂𝐬………(b) Structure-aware Attentive Network𝐂𝐨𝐧𝐜𝐚𝐭𝐞𝐧𝐚𝐭𝐞𝐅𝐂𝐬………(a)Frequency-aware Attentive Network𝐂𝐨𝐧𝐜𝐚𝐭𝐞𝐧𝐚𝐭𝐞𝐅𝐂𝐬………(b) Structure-aware Attentive Networkthe target API i. The global-view representations e(c)
u and
e(c)
captures frequency-enriched co-occurrence information
i
for enhancing the semantic representations of the client
method and the target API, respectively.

3.3.3 Hierarchical Structure Encoding

Hierarchical structure reﬂects external contextual infor-
mation of the client method u and the target API i. Ac-
cording to the deﬁnition of hierarchical structure graph, as
described in Section 3.2, different head entities and relations
endow tail entities with different semantics. To obtain the
representations of the client method u and the target API
i from external view, we design a structure-aware attentive
network, as shown in Figure 7(b).

The encoding process for the hierarchical structural
graph is similar to the encoding process of the API co-
occurrence information, as illustrated in Section 3.3.2, except
for the design of the attentive network. Speciﬁcally, for
the l-hop triple set S l
u} in hierarchical
structure graph, we learn l-hop structure representation for
client method u by:

u = {(h, r, t)|h ∈ E l

e(l)
u =

(cid:88)

π(h, r) et,

(h,r)∈S l
u

(3)

where coefﬁcient π(h, r) is attentively calculated as:

π(h, r) =

(cid:80)

exp(mlp(eh||er))

(h(cid:48),r(cid:48),t(cid:48))∈S l
u

exp(mlp(eh(cid:48)||er(cid:48)))

,

(4)

where eh, et are the embeddings of head entity h and tail
entity t, respectively. er is the embedding of relation r.
The structure-aware attention mechanism (i.e., Equ. (3) and
Equ. (4)) explicitly endows the relevance calculation of tail
entity t with the relation r. Based on the structure encoding,
we ﬁnally obtain the external-view representations e(h)
u and
e(h)
for the client method u and the target API i, respectively.
i

3.3.4 Fusion and Prediction

u ||e(c)

and ei = e(o)

We obtain the ﬁnal representation for the client method u
and the target API i by concatenating the multi-view repre-
sentations, i.e., eu = e(o)
||e(h)
u ||e(h)
.
u
i
The ﬁnal representations eu and ei incorporate call inter-
action information, frequency-enriched co-occurrence infor-
mation and structure-based hierarchical information, for ac-
curately capturing the similarly between the client method
u and the target API i. During prediction, we conduct inner
product of eu and ei for calculating the call probability:
ˆyui = e(cid:62)

||e(c)
i

i

u ei.

4 EXPERIMENTAL SETUP

In this section, we conduct extensive experiments to
evaluate the proposed approach with the aim of answering
the following research questions:
• RQ1: How does MEGA perform compared with the state-

of-the-art API usage recommendation approaches?

• RQ2: What is the impact of the three encoding compo-
nents (i.e., Call Interaction Encoding, Co-occurrence Infor-
mation Encoding and Hierarchical Structure Encoding) in
the graph representation model on the performance of
MEGA?

6
TABLE 2: Statistics of the three datasets: SHS, SHL and
MV. The Call-Avg means the average calls per method.

# Projects
# Packages
# Classes
# Methods
# APIs
# Calls
# Call-Avg

SHS
200
253
4,285
4,530
5,351
27,312
6

SHL
610
714
91,060
191,532
30,576
1,027,644
5

M V
868
340
23,207
32,987
22,054
343,010
10

• RQ3: How does MEGA perform on low-frequency APIs?
• RQ4: How do different hyper-parameter settings affect

MEGA’s performance?

4.1 Dataset Description

To evaluate the effectiveness of MEGA, we utilize three
publicly available benchmark datasets: SHS, SHL, and
M V :
• SHL contains 610 java projects, ﬁltered from 5,147 ran-
domly downloaded java projects retrieved from GitHub
via the Software Heritage archive [25].

• SHS is comprised of 200 java projects with small ﬁle sizes
extracted from SHL. It is designed to evaluate some time-
consuming baselines such as PAM [11].

• MV consists of 868 JAR archives collected from the
Maven Central repository. There are 3,600 JAR archives in
the original dataset, and 1,600 JAR archives remain after
being deduplicated by the previous work [12], [14]. While
through our manual inspection, we ﬁnd that the cleaned
dataset still contains highly similar projects. For example,
some projects have snapshot versions during the devel-
opment process and a release version at the end, such
as commons-1.0.2.RELEASE.jar and commons-1.0.2.BUILD-
SNAPSHOT.jar. Besides, some projects may have their re-
named versions, such as eclipse.equinox.common-3.6.200.jar
and common-3.6.200.jar. In these cases, the two projects are
nearly identical. Too many similar projects in a dataset
may introduce bias in evaluation [12]. Therefore, we de-
cided further clean this dataset by removing duplicated
project versions, i.e., the projects with snapshot versions
or renamed versions. We ﬁnally obtain 868 JAR archives
from 3,600 JAR archives for the M V dataset.

From the source code in datasets, we extract the method
declarations and corresponding API calls, and hierarchical
structure of the projects and packages containing method-
s/APIs. We summarize the detailed statistics of the three
datasets in Table 2.

4.2 Baselines

To demonstrate the effectiveness, we compare our pro-
posed MEGA with one statistic-based method (PAM),
two collaborative ﬁltering(CF)-based methods (FOCUS and
GAPI), as follows:
• PAM [11] is a statistical method to mine API usage
patterns, which mainly adopts a probabilistic model to
infer the patterns with the highest probabilities from client
code.

• FOCUS [12] leverages collaborative ﬁltering to recom-
mend API usage patterns. It measures the similarity
between methods via a context-based rating matrix to
recommend potential APIs.

• GAPI [14] is the state-of-the-art CF-based method that
employs graph neural networks to capture high-order
connectivity between methods and APIs from a uniﬁed
graph. We re-implement this model according to the orig-
inal paper.

4.3 Evaluation Metrics

[12],

adopt

Following previous approaches
recommendation, we

[14] on API
successRate@K,
usage
P recision@K and Recall@K to evaluate the quality
of
top-K API usage recommendation. Given a top-K
ranked recommendation list RECk(m) for a test method
m ∈ M and the ground-truth set GT (m), we adopt
M AT CHk(m) = RECk(m) ∩ GT (m) to present
the
correctly predicted API set. The SuccessRate@K(SR@K),
P recision@K(P @K), and Recall@K(R@K) are deﬁned
as follows:
• SuccessRate@K is the proportion of at least one success-

ful match among the top-K APIs.

SR@K =

countm∈M(M AT CHk(m) > 0)
|M|

(5)

• P recision@K is the proportion of correctly predicted

APIs amongst the top-K APIs.

P @K =

|M AT CHk(m)|
k

(6)

• Recall@K is the proportion of correctly predicted APIs

amongst the ground-truth APIs.

R@K =

|M AT CHk(m)|
|GT (m)|

(7)

4.4 Implementation Details

Following the real development scenario settings simu-
lated by FOCUS, we take the last method of each project as
the test method, i.e., the active method that the developer
is working on. The ﬁrst four APIs in this method are
considered as visible context, added to the training set, and
the rest is used as ground truth, added to testing set.

We implement MEGA in PyTorch. The embedding size
is set to 64 for the model in MEGA. We employ the bi-
nary cross-entropy loss as the loss function. To initialize
the model parameters, we utilize the default Xavier ini-
tializer [26]. Also, we choose Adam optimizer [27] to train
our model, with a learning rate of 0.002, a coefﬁcient of L2
normalization of 10−5, a batch size of 1024 and an epoch
number equal to 40 ﬁxed for all datasets.

Following previous work [28], we set the maximum dis-
tance of adjacent APIs (cid:15) as 3 in constructing GC . Considering
that the edge attribute in GC is a continuous variable, we
adopt the equidistant bucket discretization method, and re-
gard the bucket number as the edge type. The optimal num-
ber of buckets |T | in discretization, the max hop number L
and the size of triple set |S l
u| in each hop l on three datasets
are determined based on the experimental performance. The

best settings of the hyper-parameters for all the baseline
approaches are deﬁned following the original papers. All
approaches are trained on NVIDIA Tesla V100 GPU.

7

5 RESULTS

5.1 Effectiveness of MEGA Compared with Baselines
(RQ1)

Table 3 presents overall results of all baselines along
with MEGA in terms of SR@K metric, and the comparison
curves of P @K and R@K on three datasets (with K = 1, 5,
10, 20) are shown in Figure 8 and Figure 9, respectively. In-
tuitively, MEGA consistently achieves the best performance
on all datasets. Note that, we only test PAM on SHS due to
its long execution time and scaling poorly in a large dataset.
Detailed observations are as follows:

Comparison of SR@K on a single dataset. Without loss
of generality, we take the SHS dataset as an example to
illustrate the comparison here, and similar trends can also
be observed on other datasets. In the SHS dataset, MEGA
improves over the state-of-the-art baseline GAPI w.r.t SR@1,
SR@5, SR@10, and SR@20 by 125.1%, 85.12%, 65.76% and
39.33% respectively. This demonstrates the effectiveness of
MEGA on various Top-K settings. What’s more, from Table 3
we can see that SR@K of MEGA increases to 0.836 when K
increases to 20. This means that in most cases, MEGA can
identify the correct API in the Top-20 results, while other
baseline models can only identify about 60% of correct APIs
in the Top-20 results.

Comparison of SR@1 on multiple datasets. To evaluate
the performance of MEGA among multiple datasets com-
pared with baseline models, we choose the SR@1 metric
as it considers both whether a correct API can be included
and whether a correct API can get a higher rank. Overall,
in terms of SR@1, MEGA improves 125.1% and 77.65%
compared to the best baseline GAPI on SHS and SHL,
and 19.85% compared to the best baseline FOCUS on M V .
This demonstrates that MEGA is more effective on multiple
datasets than baselines. We notice that GAPI underperforms
FOCUS on M V . One possible explanation is that when the
ﬁrst-order call interactions are abundant enough, high-order
connectivity introduces more noise into the representation
of methods and APIs instead, leading to a negative effect.

It is worth noting that MEGA has more prominent
results in SHS, which is the smallest dataset with the
minimum number of average interactions for per method.
Speciﬁcally, the SR@1 of FOCUS and GAPI is 0.161 and
0.195, and for PAM, it is even lower at 0.08, which means
that all baselines fail to provide the correct API at the ﬁrst
position for more than 80% of cases. Whereas, the same met-
ric for MEGA is 0.439, meaning that MEGA can successfully
recommend the correct API in the Top-1 result for nearly
half of the client methods. The signiﬁcant improvement of
MEGA on SR@1 veriﬁes our approach of encoding diverse
information into ﬁnal representation, especially when his-
torical call interactions are sparse in small dataset.

Different from SR@K and R@K that increases when K
increases, we ﬁnd that P @K decreases when K increases.
Because in most cases, the number of correct APIs is much
fewer than the candidate list size K. While increasing the

TABLE 3: The performance comparison of SR@K between MEGA and the baselines on three datasets.

8

Method

SHS

SHL

M V

SR@1

SR@5

SR@10

SR@20

SR@1

SR@5

SR@10

SR@20

SR@1

SR@5

SR@10

SR@20

PAM
FOCUS
GAPI

0.080
0.161
0.195

0.150
0.256
0.363

0.275
0.328
0.479

0.335
0.422
0.600

-
0.188
0.163

-
0.292
0.402

-
0.349
0.532

-
0.388
0.670

-
0.549
0.260

-
0.709
0.569

-
0.769
0.714

-
0.819
0.837

MEGA

0.439

0.672

0.794

0.836

0.334

0.544

0.641

0.731

0.658

0.810

0.840

0.875

TABLE 4: The performance comparison of SR@K between MEGA and its variants on three datasets.

Method

SHS

SHL

M V

SR@1

SR@5

SR@10

SR@20

SR@1

SR@5

SR@10

SR@20

SR@1

SR@5

SR@10

SR@20

MEGA w/o H&C
MEGA w/o HS
MEGA w/o CO

0.333
0.349
0.423

0.566
0.614
0.651

0.688
0.704
0.757

0.751
0.773
0.815

0.142
0.167
0.311

0.291
0.316
0.521

0.413
0.429
0.629

0.558
0.578
0.728

0.521
0.533
0.614

0.694
0.704
0.746

0.756
0.784
0.780

0.802
0.842
0.816

MEGA

0.439

0.672

0.794

0.836

0.334

0.544

0.641

0.731

0.658

0.810

0.840

0.875

K
@
P

0.4

0.3

0.2

0.1

MEGA
FOCUS

GAPI
PAM

0.4

0.3

0.2

0.1

K
@
P

MEGA
FOCUS

GAPI

K
@
P

0.6
0.5
0.4
0.3
0.2
0.1

MEGA
FOCUS

GAPI

1

5

K

(a) SHS

10

20

1

5

10

20

1

5

K

10

20

K

(b) SHL

(c) M VS

Fig. 8: The performance comparison of P @K between MEGA and the baselines on three datasets.

candidate list size helps MEGA ﬁnd the correct API, it also
involves many irrelevant APIs.

The above experimental results show that pattern-based
methods, such as PAM, relying on mining frequent subse-
quences generally perform worse than learning-based meth-
ods such as GAPI and MEGA. This indicates the signiﬁcance
of exploring high-order connections and making full use of
external information.

Answer to RQ1: MEGA is quite effective on API us-
age recommendation, outperforming the state-of-the-
art approaches in terms of Success Rate, Precision and
Recall, respectively. Besides, MEGA achieves consis-
tent performance on all datasets.

5.2 Ablation Study (RQ2)

To investigate the effectiveness of each component of the
graph representation model in MEGA, we perform ablation
studies by considering the following three variants.

• MEGAw/o CO: This variant deletes the co-occurrence in-
formation encoding module from the model to investigate
the impact of global information obtained between APIs.
• MEGAw/o H&C: This variant only preserves the call in-
teraction encoding in the model to gain the primary
representations of the method and the API, without any
supplementary information.

The experimental results are shown in Table 4. We ﬁnd
that the performance of MEGA drops in three variants com-
pared with the complete model, which demonstrates the
effectiveness of the hierarchical structure encoding module
and co-occurrence information encoding module.

MEGAw/o H&C performs worst since the variant only
utilizes historical call information. Moreover, we notice that
the performance degradation is most signiﬁcant on SHL.
For example, SR@1 decreases from 0.311 to 0.142. Note that,
in SHL, the average number of call interactions is 5, which
is the smallest among all datasets. This demonstrates the
prominent advantage of appending co-occurrence informa-
tion and structure information to the ﬁnal representation
when the interactions are insufﬁcient.

• MEGAw/o HS: This variant removes the hierarchical struc-
ture encoding module from the model to study the effect
of external information derived from the project and pack-
age.

In addition, the performance of MEGAw/o HS is better
than MEGAw/o CO, meaning that hierarchical structure in-
formation is more critical than co-occurrence information.
One possible reason is that the external project/package

MEGA
FOCUS

GAPI
PAM

K
@
R

0.4

0.3

0.2

0.1

MEGA
FOCUS

GAPI

0.4

0.3

0.2

0.1

K
@
R

K
@
R

0.5

0.4

0.3

0.2

0.1

9

MEGA
FOCUS

GAPI

1

5

K

(a) SHS

10

20

1

5

10

20

1

5

K

10

20

K

(b) SHL

(c) M VS

Fig. 9: The performance comparison of R@K between MEGA and the baselines on three datasets.

TABLE 5: The performance comparison over low-frequency
APIs on three datasets.

SR@1

SR@10

P@1

P@10

R@1

R@10

SHS

FOCUS
GAPI
MEGA

0.017
0.020
0.081

FOCUS
GAPI
MEGA

0.003
0.040
0.081

FOCUS
GAPI
MEGA

0.002
0.009
0.014

0.156
0.081
0.263

0.004
0.079
0.315

0.003
0.018
0.041

0.017
0.020
0.081

0.016
0.013
0.046

0.009
0.007
0.029

0.043
0.034
0.176

SHL

0.003
0.040
0.081

0.004
0.012
0.040

0.002
0.020
0.057

0.015
0.047
0.236

M V

0.003
0.009
0.014

0.002
0.003
0.008

0.001
0.006
0.006

0.007
0.016
0.029

structure can provide more contextual information instead
of just internal call relation, which is more beneﬁcial for
capturing the semantic match between methods and APIs.

Answer to RQ2: Encoding both API co-occurrence
information and project/package hierarchical structure
information into the ﬁnal representations of methods
and APIs is beneﬁcial to MEGA’s performance im-
provements. Especially, project/package hierarchical
structure information is more critical, beneﬁting from it
contains contextual information of methods and APIs.

5.3 Performance of MEGA on low-frequency APIs(RQ3)

As stated in Section 2, we design MEGA to alleviate the
problem that current approaches on low-frequency APIs. To
verify our design, we conduct experiments on APIs called
by methods less than or equal to 3 times.

Table 5 shows SR@K, P @K and R@K (K = {1, 10})
for all approaches on three datasets. To sum up, that MEGA
greatly outperforms other approaches in all metrics. In
detail, SR@10 is improved by 298.7%, P @10 is improved
by 233.3%, and R@10 is improved by 402.1% compared
to the latest approach GAPI on SHL. Looking into the
performance of baselines, GAPI obtains better performance
indicating the effectiveness of incorporat-
than FOCUS,
ing complicated connectivity information in enriching the

representation of APIs with fewer direct interactions. Al-
though MEGA presents a signiﬁcant improvement on the
recommendation performance, the results of low-frequency
APIs are still quite limited, which may be attributed to the
functional particularity of the APIs and needs more future
research.

Answer to RQ3: MEGA consistently outperforms
the state-of-the-art baselines for recommending low-
frequency APIs on the three benchmark datasets.
low-
Despite the superior performance of MEGA,
frequency recommendation is still challenging and
needs more future research.

5.4 Parameter Sensitivity Study (RQ4)

We conduct experiments to analyze the impact of fol-
lowing hyper-parameters with different settings on MEGA’s
performance.

Impact of max hop number L. We vary the number of
hops in propagating to observe the performance change of
MEGA. Figure 11(a) depicts the results in terms of SR@10.
We observe that MEGA achieves the best results with one
hop on three datasets, and the performance gradually de-
creases with the hop number increases.

One possible explanation is that, in the graph, short-
distance nodes have a strong correlation with the original
node, while the relevance decays as the distance increases.
Consequently, the positive impact of short-distance prop-
agation is greater, while long-distance propagation brings
more noise than useful signals.

Impact of bucket number |T |. To study the impact
of bucket numbers, we conduct different experiments by
setting different bucket numbers. The experimental results
in terms of SR@10 are presented in Figure 11(b), which
shows that for SHS, SHL, and M V , the best performance
is achieved when the number of buckets is 15, 10, and 15,
respectively.

One possible reason for this phenomenon is that when
the number of buckets is too small, i.e., few relation types,
the graph contains less information, which compromises the
trained model’s expressiveness. While a large number of
buckets, i.e., many relation types, makes the information in
the graph too rich, leading to over-ﬁtting the model.

Impact of triple set size |S l

u| in each hop l. We change
the number of neighbors selected by the client method and

0.81

0.78

0.75

0.72

0.69

0
1
@
R
S

0.9

0.8

0.7

0.6

0.5

0
1
@
R
S

10

8

16

32

64

8

16

32

64

0.65

0
1
@
R
S

0.63

0.61

0.59

8

16

32

64

0.85

0
1
@
R
S

0.83

0.81

0.79

8

32
16
triple set size

(a) SHS

64

8

32
16
triple set size

(b) SHL

64

8

32
16
triple set size

64

(c) M VS

Fig. 10: The results of SR@10 on three datasets along with different sizes of triple set.

SHS

SHL

M VS

SHS

SHL

M VS

0.9

0.8

0.7

0.6

0
1
@
R
S

1

2

3

4

hop number

1

5
15
10
bucket number

20

(a) The effect of hop numbers

(b) The effect of bucket numbers

Fig. 11: The parameter sensitivity study of hop and bucket
numbers.

the target API in each hop to explore the effects of triple
set size on MEGA’s performance. The results of SR@10 on
the SHS, SHL and M V are demonstrated in Figure 10(a),
Figure 10(b) and Figure 10(c), respectively.

Jointly analyzing the three sub-ﬁgures, when the size
increases, the results get better ﬁrst and then worse. This
means that when the size is moderately large, the beneﬁts
of more information included improves the performance.
However, when the size is extremely large, the noise in-
troduced outweighs the useful information introduced and
thus it can hurt the performance. Overall, 16 or 32 is the
suitable size of triple set in each hop for both the method
and the API on three datasets.

Answer to RQ4: The larger hop number has negative
effect on MEGA; the bucket number has stable effect
on MEGA; the triple set size has ﬂuctuant effect on
MEGA.

6 DISCUSSION
6.1 Implications

The limited results may be attributed to the functional par-
ticularity of the low-frequency APIs, and could impact the
practical usage of current API recommendation tools. Thus,
we suggest researchers working on API recommendation to
focus more on the recommendation of low-frequency APIs
by combining external knowledge such as API documenta-
tion or exploring data augmentation techniques.

Software Developers. According to our coarse analysis
of the benchmark datasets, low-frequency APIs are usually
not associated with API documentation. API documentation
which contains usage samples and instructions is helpful
for learning the representations of APIs [29], [30]. Thus, we
encourage developers to write some descriptions or usage
examples for facilitating the API recommendation task.

6.2 Threats to Validity

Internal Validity. In this paper, following [22], [23] we
sample a ﬁxed-size of neighbors on graphs instead of us-
ing a full size triple sets for the trade off of computation
overhead. This may slightly inﬂuence the performance of
MEGA. To alleviate the impact of this threat, we conduct
each experiment ﬁve times and obtain average performance
as shown in Section 5. Furthermore, our experiments on
parameter sensitivity also demonstrates that different sizes
of triple set inﬂuence the performance of MEGA slightly.

External Validity. We evaluate MEGA under Java
datasets, while MEGA may show different performance
on datasets in other programming languages. To reduce
the impact from different programming languages, when
designing three multi-view graphs, we try to exclude the
language-speciﬁc information and only maintain the struc-
ture information such as call relationships and deﬁnition
relationships. We believe our design can be easily adapted
to most programming languages.

In this section, we discuss the implications that would be

helpful for software researchers and software developers.

Software Researchers. In section 5, we achieve that the
heterogeneous information in source code is greatly ben-
eﬁcial for improving the recommendation performance of
APIs including the low-frequency APIs. However, we also
ﬁnd that the results of low-frequency APIs are still quite
limited, presenting a large gap with those of common APIs.

7 RELATED WORK

In this section, we review existing work about API usage
recommendation. The work on API usage recommendation
can be divided into two categories: pattern-based methods
and learning-based methods. Pattern-based methods utilize
traditional statistical methods to capture usage patterns
from API co-occurrences. Learning-based methods leverage
deep learning models to automatically learn the potential

usage patterns from a large code corpus and then use them
to recommend patterns.

Pattern-based methods. Zhong et al. propose MAPO [9]
to cluster and mine API usage patterns from open source
repositories, and then recommends the relevant usage pat-
terns to developers. Wang et al. improve MAPO and build
UP-Miner [10] by utilizing a new algorithm based on
SeqSim to cluster the API sequences. Nguyen et al. propose
APIREC [31], which uses ﬁne-grained code changes and
the corresponding changing contexts to recommend APIs.
Fowkes et al. propose PAM [11] to tackle the problem that
the recommended API lists are large and hard to under-
stand. PAM mines API usage patterns through an almost
parameter-free probabilistic algorithm and uses them to
recommend APIs. Liu et al. propose RecRank [32] to improve
the top-1 accuracy based on API usage paths. Nguyen et
al. propose FOCUS [12], which mines open-source reposi-
tories and analyzes API usages in similar projects to rec-
ommend APIs and API usage patterns based on context-
aware collaborative-ﬁltering techniques. Previous pattern-
based methods only consider one or two relationships be-
tween APIs, however, MEGA considers call interactions,
API co-occurrences, project/package hierarchical structure
to comprehensively capture the contexts surrounding APIs.
Learning-based methods. Nguyen et al. propose a
graph-based language model GraLan [17] to recommend
API usages. Gu et al. propose DeepAPI [33]. They refor-
mulate API recommendation task as a query-API transla-
tion problem and use an RNN Encoder-Decoder model to
recommend API sequences. Ling et al. propose GeAPI [19].
GeAPI automatically constructs API graphs based on source
code and leverages graph embedding techniques for API
representation. Gu et al. propose Codekernel [18] by repre-
senting code as object usage graphs and clustering them to
recommend API usage examples. Zhou et al. build a tool
named BRAID [34] to leverage learning-to-rank and active
learning techniques to boost recommendation performance.
Previous learning-based methods fail to recommend usage
patterns for low-frequency APIs due to the data-driven
feature, MEGA encodes API frequency with global API co-
occurrence graph to alleviate this problem.

8 CONCLUSION

In this paper, we propose a novel approach MEGA
for automatic API usage recommendation. MEGA employs
heterogeneous graphs, which are constructed from multiple
views, i.e., method-API interaction from local view, API-
API co-occurrence from global view, and project structure
from external view. A graph representation model with a
frequency-aware attentive network and a structure-aware
attentive network is then proposed to learn the matching
scores between methods and APIs based on the multi-
view graphs. Experiment demonstrates MEGA’s effective-
ness both on overall API usage recommendation and low-
frequency API usage recommendation. For future work, in
addition to the information extracted from projects, some
information from API ofﬁcial documentation or Q&A sites
also contributes to mining API usage patterns. Therefore,
we plan to design some new modules that encode more
information from different sources.

REFERENCES

11

[1] D. Hou and X. Yao, “Exploring the intent behind API evolution:
A case study,” in 18th Working Conference on Reverse Engineering,
WCRE 2011, Limerick, Ireland, October 17-20, 2011, M. Pinzger,
D. Poshyvanyk, and J. Buckley, Eds.
IEEE Computer Society,
2011, pp. 131–140.

[2] Z. Yu, C. Bai, L. Seinturier, and M. Monperrus, “Characterizing the
usage, evolution and impact of java annotations in practice,” IEEE
Trans. Software Eng., vol. 47, no. 5, pp. 969–986, 2021.
I. Gvero, “Core java volume I: fundamentals, 9th edition by cay
s. horstmann and gary cornell,” ACM SIGSOFT Softw. Eng. Notes,
vol. 38, no. 3, p. 33, 2013.

[3]

[4] Oracle, “Jdk 18 documentation,” https://docs.oracle.com/en/

java/javase/18/books.html, 2022.

[5] M. P. Robillard, “What makes apis hard to learn? answers from

[6]

developers,” IEEE Softw., vol. 26, no. 6, pp. 27–34, 2009.
S. M. Nasehi, J. Sillito, F. Maurer, and C. Burns, “What makes a
good code example?: A study of programming q&a in stackover-
ﬂow,” in 28th IEEE International Conference on Software Maintenance,
ICSM 2012, Trento, Italy, September 23-28, 2012.
IEEE Computer
Society, 2012, pp. 25–34.

[7] M. Acharya, T. Xie, J. Pei, and J. Xu, “Mining API patterns as
partial orders from source code: from usage scenarios to spec-
iﬁcations,” in Proceedings of the 6th joint meeting of the European
Software Engineering Conference and the ACM SIGSOFT International
Symposium on Foundations of Software Engineering, 2007, Dubrovnik,
Croatia, September 3-7, 2007, I. Crnkovic and A. Bertolino, Eds.
ACM, 2007, pp. 25–34.

[8] R. P. L. Buse and W. Weimer, “Synthesizing API usage examples,”
in 34th International Conference on Software Engineering, ICSE 2012,
June 2-9, 2012, Zurich, Switzerland, M. Glinz, G. C. Murphy, and
M. Pezz`e, Eds.

IEEE Computer Society, 2012, pp. 782–792.

[9] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei, “MAPO: mining
and recommending API usage patterns,” in ECOOP 2009 - Object-
Oriented Programming, 23rd European Conference, Genoa, Italy, July
6-10, 2009. Proceedings, ser. Lecture Notes in Computer Science,
S. Drossopoulou, Ed., vol. 5653. Springer, 2009, pp. 318–343.
[10] J. Wang, Y. Dang, H. Zhang, K. Chen, T. Xie, and D. Zhang,
“Mining succinct and high-coverage API usage patterns from
source code,” in Proceedings of the 10th Working Conference on
Mining Software Repositories, MSR ’13, San Francisco, CA, USA, May
18-19, 2013, T. Zimmermann, M. D. Penta, and S. Kim, Eds.
IEEE
Computer Society, 2013, pp. 319–328.

[11] J. M. Fowkes and C. Sutton, “Parameter-free probabilistic API
mining across github,” in Proceedings of the 24th ACM SIGSOFT
International Symposium on Foundations of Software Engineering, FSE
2016, Seattle, WA, USA, November 13-18, 2016, T. Zimmermann,
J. Cleland-Huang, and Z. Su, Eds. ACM, 2016, pp. 254–265.
[12] P. T. Nguyen, J. D. Rocco, D. D. Ruscio, L. Ochoa, T. Degueule,
and M. D. Penta, “FOCUS: a recommender system for mining API
function calls and usage patterns,” in Proceedings of the 41st Interna-
tional Conference on Software Engineering, ICSE 2019, Montreal, QC,
Canada, May 25-31, 2019, J. M. Atlee, T. Bultan, and J. Whittle, Eds.
IEEE / ACM, 2019.

[13] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl, “Item-based
collaborative ﬁltering recommendation algorithms,” in Proceedings
of the Tenth International World Wide Web Conference, WWW 10, Hong
Kong, China, May 1-5, 2001, V. Y. Shen, N. Saito, M. R. Lyu, and
M. E. Zurko, Eds. ACM, 2001, pp. 285–295.

[14] C. Ling, Y. Zou, and B. Xie, “Graph neural network based col-
laborative ﬁltering for API usage recommendation,” in 28th IEEE
International Conference on Software Analysis, Evolution and Reengi-
neering, SANER 2021, Honolulu, HI, USA, March 9-12, 2021.
IEEE,
2021, pp. 36–47.

[15] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Mon-
fardini, “The graph neural network model,” IEEE Trans. Neural
Networks, vol. 20, no. 1, pp. 61–80, 2009.

[16] Y. Peng, S. Li, W. Gu, Y. Li, W. Wang, C. Gao, and M. R. Lyu,
“Revisiting, benchmarking and exploring API recommendation:
How far are we?” CoRR, vol. abs/2112.12653, 2021.

[17] A. T. Nguyen and T. N. Nguyen, “Graph-based statistical language
model for code,” in 37th IEEE/ACM International Conference on
Software Engineering, ICSE 2015, Florence, Italy, May 16-24, 2015,
Volume 1, A. Bertolino, G. Canfora, and S. G. Elbaum, Eds.
IEEE
Computer Society, 2015, pp. 858–868.

[18] X. Gu, H. Zhang, and S. Kim, “Codekernel: A graph kernel
based approach to the selection of API usage examples,” in 34th
IEEE/ACM International Conference on Automated Software Engineer-
ing, ASE 2019, San Diego, CA, USA, November 11-15, 2019.
IEEE,
2019, pp. 590–601.

[19] C. Ling, Y. Zou, Z. Lin, and B. Xie, “Graph embedding based
API graph search and recommendation,” J. Comput. Sci. Technol.,
vol. 34, no. 5, pp. 993–1006, 2019.

[20] A. Chen, “Context-aware collaborative ﬁltering system: Predicting
the user’s preference in the ubiquitous computing environment,”
in Location- and Context-Awareness, First International Workshop,
LoCA 2005, Oberpfaffenhofen, Germany, May 12-13, 2005, Proceedings,
ser. Lecture Notes in Computer Science, T. Strang and C. Linnhoff-
Popien, Eds., vol. 3479. Springer, 2005, pp. 244–253.

[21] G. Zhou, X. Zhu, C. Song, Y. Fan, H. Zhu, X. Ma, Y. Yan, J. Jin,
H. Li, and K. Gai, “Deep interest network for click-through
rate prediction,” in Proceedings of
the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining,
KDD 2018, London, UK, August 19-23, 2018, Y. Guo and
F. Farooq, Eds. ACM, 2018, pp. 1059–1068. [Online]. Available:
https://doi.org/10.1145/3219819.3219823

[22] H. Wang, F. Zhang, J. Wang, M. Zhao, W. Li, X. Xie, and M. Guo,
“Ripplenet: Propagating user preferences on the knowledge graph
for recommender systems,” in Proceedings of the 27th ACM Interna-
tional Conference on Information and Knowledge Management, CIKM
2018, Torino, Italy, October 22-26, 2018, A. Cuzzocrea, J. Allan, N. W.
Paton, D. Srivastava, R. Agrawal, A. Z. Broder, M. J. Zaki, K. S.
Candan, A. Labrinidis, A. Schuster, and H. Wang, Eds. ACM,
2018, pp. 417–426.

[23] Z. Wang, G. Lin, H. Tan, Q. Chen, and X. Liu, “CKAN: collabo-
rative knowledge-aware attentive network for recommender sys-
tems,” in Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval, SIGIR 2020,
Virtual Event, China, July 25-30, 2020, J. Huang, Y. Chang, X. Cheng,
J. Kamps, V. Murdock, J. Wen, and Y. Liu, Eds. ACM, 2020, pp.
219–228.

[24] A. F. Agarap, “Deep learning using rectiﬁed linear units (relu),”

arXiv preprint arXiv:1803.08375, 2018.

[25] R. D. Cosmo and S. Zacchiroli, “Software heritage: Why and
how to preserve software source code,” in Proceedings of the 14th
International Conference on Digital Preservation, iPRES 2017, Kyoto,
Japan, September 25-29, 2017, S. Hara, S. Sugimoto, and M. Goto,
Eds., 2017.

[31] A. T. Nguyen, M. Hilton, M. Codoban, H. A. Nguyen, L. Mast,
E. Rademacher, T. N. Nguyen, and D. Dig, “API code recommen-
dation using statistical learning from ﬁne-grained changes,” in
Proceedings of the 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering, FSE 2016, Seattle, WA, USA,

12

[26] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training
deep feedforward neural networks,” in Proceedings of the Thirteenth
International Conference on Artiﬁcial Intelligence and Statistics, AIS-
TATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, ser.
JMLR Proceedings, Y. W. Teh and D. M. Titterington, Eds., vol. 9.
JMLR.org, 2010, pp. 249–256.

[27] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” in 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Y. Bengio and Y. LeCun, Eds., 2015.

[28] Z. Wang, W. Wei, G. Cong, X. Li, X. Mao, and M. Qiu, “Global
context enhanced graph neural networks for session-based rec-
ommendation,” in Proceedings of the 43rd International ACM SIGIR
conference on research and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020, J. Huang, Y. Chang,
X. Cheng, J. Kamps, V. Murdock, J. Wen, and Y. Liu, Eds. ACM,
2020, pp. 169–178.

[29] F. Thung, S. Wang, D. Lo, and J. Lawall, “Automatic recom-
mendation of API methods from feature requests,” in 2013 28th
IEEE/ACM International Conference on Automated Software Engi-
neering, ASE 2013, Silicon Valley, CA, USA, November 11-15, 2013,
IEEE, 2013, pp. 290–300.
E. Denney, T. Bultan, and A. Zeller, Eds.
[30] Q. Huang, X. Xia, Z. Xing, D. Lo, and X. Wang, “API method
recommendation without worrying about the task-api knowledge
gap,” in Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering, ASE 2018, Montpellier, France,
September 3-7, 2018, M. Huchard, C. K¨astner, and G. Fraser, Eds.
ACM, 2018, pp. 293–304.
November 13-18, 2016, T. Zimmermann, J. Cleland-Huang, and
Z. Su, Eds. ACM, 2016, pp. 511–522.

[32] X. Liu, L. Huang, and V. Ng, “Effective API recommendation
without historical software repositories,” in Proceedings of the 33rd
ACM/IEEE International Conference on Automated Software Engineer-
ing, ASE 2018, Montpellier, France, September 3-7, 2018, M. Huchard,
C. K¨astner, and G. Fraser, Eds. ACM, 2018, pp. 282–292.

[33] X. Gu, H. Zhang, D. Zhang, and S. Kim, “Deep API learning,” in
Proceedings of the 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering, FSE 2016, Seattle, WA, USA,
November 13-18, 2016, T. Zimmermann, J. Cleland-Huang, and
Z. Su, Eds. ACM, 2016, pp. 631–642.

[34] Y. Zhou, H. Jin, X. Yang, T. Chen, K. Narasimhan, and H. C.
Gall, “BRAID: an API recommender supporting implicit user
feedback,” in ESEC/FSE ’21: 29th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Soft-
ware Engineering, Athens, Greece, August 23-28, 2021, D. Spinellis,
G. Gousios, M. Chechik, and M. D. Penta, Eds. ACM, 2021, pp.
1510–1514.

