2
2
0
2

g
u
A
3

]
E
S
.
s
c
[

1
v
1
7
9
1
0
.
8
0
2
2
:
v
i
X
r
a

API Usage Recommendation via Multi-View
Heterogeneous Graph Representation Learning

1

Yujia Chen‚Ä†, Xiaoxue Ren‚Ä°, Cuiyun Gao‚àó‚Ä†, Yun Peng‚Ä°, Xin Xia¬ß, and Michael R. Lyu‚Ä°
‚Ä†Harbin Institute of Technology, Shenzhen, China
‚Ä°The Chinese University of Hong Kong, Hong Kong, China
¬ßSoftware Engineering Application Technology Lab, Huawei, China
gaocuiyun@hit.edu.cn, yujiachen@stu.hit.edu.cn
{ypeng, xiaoxueren, lyu}@cse.cuhk.edu.hk, xin.xia@acm.org

Abstract‚ÄîDevelopers often need to decide which APIs to use for the functions being implemented. With the ever-growing number of
APIs and libraries, it becomes increasingly difÔ¨Åcult for developers to Ô¨Ånd appropriate APIs, indicating the necessity of automatic API
usage recommendation. Previous studies adopt statistical models or collaborative Ô¨Åltering methods to mine the implicit API usage
patterns for recommendation. However, they rely on the occurrence frequencies of APIs for mining usage patterns, thus prone to fail for
the low-frequency APIs. Besides, prior studies generally regard the API call interaction graph as homogeneous graph, ignoring the rich
information (e.g., edge types) in the structure graph. In this work, we propose a novel method named MEGA for improving the
recommendation accuracy especially for the low-frequency APIs. SpeciÔ¨Åcally, besides call interaction graph, MEGA considers another
two new heterogeneous graphs: global API co-occurrence graph enriched with the API frequency information and hierarchical structure
graph enriched with the project component information. With the three multi-view heterogeneous graphs, MEGA can capture the API
usage patterns more accurately. Experiments on three Java benchmark datasets demonstrate that MEGA signiÔ¨Åcantly outperforms the
baseline models by at least 19% with respect to the Success Rate@1 metric. Especially, for the low-frequency APIs, MEGA also
increases the baselines by at least 55% regarding the Success Rate@1 score.

Index Terms‚ÄîAPI recommendation, multi-view heterogeneous graphs, graph representation learning

(cid:70)

1 INTRODUCTION

In the daily software development process, develop-
ers often use the application programming interface (API)
provided by some libraries to reduce development time
when implementing a function. For instance,
the API
Buf f eredInputStream.read() provides an efÔ¨Åcient way to
read data from an input stream and store the data in a buffer
array. However, it is difÔ¨Åcult for developers to be familiar
with all APIs, because APIs are extensive in quantity and
rapidly evolving [1], [2]. In the past two decades, the num-
ber of Java Development Kit (JDK) APIs has increased more
than 20 times (from 211 classes in the Ô¨Årst version of 1996 to
4,403 classes in 2022) [3] [4]. Therefore, when selecting APIs,
developers often refer to ofÔ¨Åcial technical documentation,
raise questions on sites (e.g., Stack OverÔ¨Çow), or query
on search engines (e.g., Google), etc. Obviously, the whole
process relies on developers‚Äô experience, and could be time-
consuming since useful information is usually buried in
massive contents [5], [6].

Regarding the issues above, previous studies [7], [8] pro-
pose to automatically recommend a list of API candidates
according to previously-written code, which is demon-
strated to be beneÔ¨Åcial for improving the API searching
process and facilitating software development. For example,
MAPO [9] and UP-Miner [10] are based on mining frequent
patterns clusters from collected projects to obtain common
API usage patterns. PAM [11] uses probabilistic modelling

* corresponding author.

technique in API call sequence to mine usage patterns.
FOCUS [12] uses a context-aware collaborative-Ô¨Åltering [13]
technique to recommend APIs, relying on the similarity
between methods and projects. GAPI [14] applies graph
neural networks [15] based collaborative Ô¨Åltering to exploit
the relationship between methods and APIs. However, these
techniques focus on recommending commonly-used APIs,
and tend to fail to mine the usage patterns of the low-
frequency APIs. According to our analysis in Section 2, the
low-frequency APIs occupy a signiÔ¨Åcant proportion of all
APIs. According to the statistics, the rarely-appeared APIs
account for 76% of the whole APIs in the SHL dataset [12].
Nevertheless, the recommendation success rate of rare APIs
(7.9%) is much lower than that of common APIs (54.2%).
Thus, how to effectively learn low-frequency APIs usage
patterns is a great yet under-explored challenge [16].
Besides, the existing techniques highly rely on the homoge-
neous interaction information between APIs and methods,
ignoring the rich contextual information in source code (e.g.,
co-occurring APIs and hierarchical structure in projects and
packages). In fact, APIs under the same package are more
likely to be called together (e.g., Ô¨Åle.open() and Ô¨Åle.close())
are under the package java.io), which is important external
information for API recommendation. Therefore, how to
involve contextual information in API recommendation
is another challenge.

In this work, we propose MEGA, a novel API usage
recommendation method with Multi-view hEterogeneous
Graph representAtion learning. Different from the prior

 
 
 
 
 
 
studies, MEGA employs heterogeneous graphs, which are
constructed from multiple views, i.e., method-API interac-
tion from local view, API-API co-occurrence from global
view, and project structure from external view. SpeciÔ¨Åcally,
MEGA builds upon three heterogeneous graphs, i.e., the
common call interaction graph, and two new graphs, i.e.,
global API co-occurrence graph and hierarchical structure graph.
The call interaction graph establishes the relations between
methods and corresponding called APIs, and is commonly
adopted by previous approaches [11], [12], [14], [17], [18].
Models based on only such graph cannot well capture the
representations of the APIs with rare called frequencies. To
improve the API representations, the global API co-occurrence
graph is introduced to build the relations between APIs
with the co-occurrence frequencies incorporated. To enrich
the representations of APIs and methods with contextual
structure information, MEGA also involves the called infor-
mation by projects and packages, composing the hierarchical
structure graph. A graph representation model is then pro-
posed to learn the matching scores between methods and
APIs based on the multi-view graphs. To integrate the multi-
view knowledge, a frequency-aware attentive network and
a structure-aware attentive network are proposed to encode
the co-occurrence information and hierarchical structure,
respectively.

We evaluate the effectiveness of MEGA on three Java
benchmark datasets consisting of 610 Java projects from
GitHub and 868 JAR archives from the Maven Central
Repository. In addition, we also simulate the real develop-
ment scenario [12] where a developer has already called
some APIs in a method. Then MEGA recommends APIs
based on the called APIs by client methods, and calculates
the evaluation metrics. The experimental results show that
MEGA outperforms the baseline approaches (PAM [11],
FOCUS [12] and GAPI [14]) by at least 19% with respect
to the Success Rate@1 metric. For the low-frequency APIs,
MEGA also achieves an increased rate at more than 55%
compared to the baselines.

In summary, our main contributions in this paper are as

follows:

‚Ä¢ To the best of our knowledge, we are the Ô¨Årst work
to construct multi-view heterogeneous graphs for more
accurate API usage recommendation.

‚Ä¢ We propose a novel API recommendation approach
named MEGA, which designs a graph representation
model with a frequency-aware attentive network and
a structure-aware attentive network to generate en-
hanced representations of methods and APIs.

‚Ä¢ We perform experiments on three benchmark datasets,
and the results demonstrate that MEGA outperforms
the state-of-the-art API usage recommendation ap-
proaches, even for the low-frequency APIs.

Outline. The rest of paper is organized as follows: Section 2
introduces details of our motivation. Section 3 presents the
overall workÔ¨Çow of MEGA and architecture of the graph
representation model in MEGA. Section 4 and Section 5 are
the settings and results of evaluation, respectively. Section 6
analyzes some implications and threats to validity. Section 7
succinctly describes related works. In the end, in Section 8,
we conclude the whole work.

2 MOTIVATION

2

Fig. 1: The distribution of APIs called frequency in SHS,
SHL and MV datasets.

Fig. 2: API recommendation performance of FOCUS [12],
GAPI [14] and our proposed MEGA (corresponding to the
APIs with different frequencies on the SHL dataset).

Figure 1 shows the distribution of APIs with different
occurrence frequencies in three benchmark datasets [12], i.e.,
SHS, SHL and M V , with detailed statistics of the datasets
shown in Table 2. Obviously, APIs with lower occurrence
frequencies (i.e., ‚â§ 3) account for signiÔ¨Åcant proportions
(i.e., > 65%) among all the APIs in each dataset. Although
appearing less frequently, the large proportion of such APIs
indicates developers‚Äô strong demands for speciÔ¨Åc functions,
and accurately recommending the APIs is critical for facili-
tating their daily programming.

Figure 3 illustrates an example of a client method which
queries the low-frequency APIs. In this scenario, the de-
veloper is working on a method to get the name of a
JAR package, but is not sure which APIs to use next.
The ‚Äútrue API calls‚Äù in Figure 3 list the APIs in ground
truth, in which both ‚ÄúJAXBContext.createUnmarshalle()‚Äù and
‚ÄúUnmarshaller.unmarshal(java.io.InputStream)‚Äù are rarely ap-
pear in the datasets. Both popular models including FO-
CUS [12] and GAPI [14] learn the API representations
ineffectively, and fail to recommend the APIs. Figure 2
depicts the API recommendation performance of the two
models corresponding to APIs with different frequencies on
the SHL dataset with respect to the SuccessRate@10 score.

SHSSHLMV0.44.536.47.912.154.231.548.364.10500010000150002000025000010203040506070rare APInormal APIcommon APIAPI TypeNumberFOCUSGAPIMEGA(Ours)SuccessRate@10(%)API Number‚â§3‚â•103~103

Fig. 4: The overall workÔ¨Çow of MEGA.

TABLE 1: Symbols and corresponding descriptions.

Notation
U,I
E,R
GI
GC
GH
(i, f, j)
(h, r, t)
u, i
L
E l
u
S l
u
u , e(o)
e(o)
i
e(c)
u , e(c)
i
u , e(h)
e(h)
i
eu, ei

Description
The set of methods, APIs
The set of entities, relations
The call interaction graph
The global API co-occurrence graph
The hierarchical structure Graph
The co-occurrence triple
The structure triple
The client method, the target API
The hop number of information encoding
The l-hop entity set of u
The l-hop triple set of u
The local-view representation of the method u,i
The global-view representation of u,i
The external-view representation of u,i
The Ô¨Åanl representation of u,i

class), so it successfully recommends the rare API ‚ÄúJAXB-
Context.CreateUnmarshaller()‚Äù.

3 METHODOLOGY

3.1 WorkÔ¨Çow of MEGA

Figure 4 illustrates the MEGA‚Äôs workÔ¨Çow which in-
cludes three main stages, i.e., constructing multi-view het-
erogeneous graphs, training a graph representation model
and recommending APIs with the trained model. In the
Ô¨Årst stage, we construct three heterogeneous graphs, i.e.,
call interaction graph, global API co-occurrence graph, and hi-
erarchical structure graph. The nodes of these graphs include
APIs, methods, classes, projects, and packages. We extract
the relations between nodes from the source code. Then,
in the second stage, a graph representation model is pro-
posed to encode the three graphs and integrate the graph
representations for recommendation. In the last stage, we
employ the trained model to return a ranked list of API
usage recommendation according to the code snippet of
the current client method. The details of our approach are
explained in the following parts. For facilitating readers‚Äô
understanding of the proposed approach, we list the key
notations in Table 1.

3.2 Constructing Multi-View Heterogeneous Graphs

In this section, we present the graph construction pro-
cess of the multi-view graphs. Following the formulation
of mainstream recommendation systems [20], we treat all

Fig. 3: An Example of API usage recommendation. (The
true API calls and TOP-3 APIs recommended by FOCUS [12],
GAPI [14] and our proposed MEGA, respectively.)

We Ô¨Ånd that the APIs appearing rarely, e.g., ‚â§ 3, present
signiÔ¨Åcantly poor performance than the APIs appearing
frequently, e.g., ‚â• 10. The results show that the existing
models are difÔ¨Åcult to recommend the low-frequency APIs.
Besides, existing approaches [10], [12], [14], [18], [19]
generally regard the API call interaction graph as homoge-
neous graph, ignoring the rich heterogeneous information
(e.g., edge types) in the graph. For example, the state-of-
the-art models, FOCUS [12] and GAPI [14] are based on
collaborative Ô¨Åltering for measuring the similarities between
all methods to recommend APIs. The learning process in the
models tends to rely on the commonly-used APIs in meth-
ods, resulting in ineffective API recommendation. As the
example shown in Figure 3, the API recommended by FO-
CUS for improving the speed and efÔ¨Åciency of byte stream
operations comes from the BufferedReader class, which is a
very general yet function-irrelevant operation for the cur-
rent client method.

Our approach. To address the above limitations of the
existing models, we try to exploit the rich heterogeneous
information in source code from multiple views, includ-
ing method-API interaction from local view, API-API co-
occurrence from global view, and project structure from ex-
ternal view, respectively. SpeciÔ¨Åcally, we build three hetero-
geneous graphs from each view, i.e., call interaction graph,
global API co-occurrence graph and hierarchical structure
graph. Moreover, two new attentive networks are designed
for encoding frequency-based co-occurrence information
and structure-based hierarchical information during learn-
ing the representations of APIs and methods. As the exam-
ple shown in Figure 3, MEGA captures the co-occurring pat-
tern with the API ‚ÄúJAXBContext.NewInstance( ¬®POM. 4 0¬®)‚Äù
and their similar structural information (i.e., under the same

Client Method:privateString getJarName(String path, JAXBElement<Model> fhElement){Model model = fhElement.getValue();Modules ms = model.getModules();String originalName = null;if(ms == null) originalName = model.getArtifactId();elseoriginalName = ms.getModule();JAXBContext jaxbContext = JAXBContext.newInstance("pom._4_0");// which APIs are next?     }True API Calls : √ºJAXBContext.createUnmarshaller()√ºFileInputStream.FileInputStream(java.lang.String) √ºUnmarshaller.unmarshal(java.io.InputStream) API Calls  Returned by  FOCUS : lBufferedReader.BufferedReader()lBufferedReader.close()lBufferedReader.readLine()API Calls  Returned by  GAPI : lStreamResult.StreamResult()lDate.getTime()lClass.forName()API Calls  Returned by  OURS : lJAXBContext.createUnmarshaller()lModel.getVersion()lUnmarshaller.unmarshal(java.io.InputStream) SourceCodeExtractCall Interaction GraphHierarchical Structure GraphGlobal API Co-occurrence Grapha.  ConstructingMulti-View Heterogeneous Graphs  clientmethodb. Training a Graph Representation Model  c.RecommendingAPIs withthe trainedmodel TrainedModelclientmethodtargetAPIAPIlistGraph Representation Learning Model4

Fig. 5: Training and recommending process of the graph representation learning model in MEGA.

Algorithm 1: Global API Co-occurrence Graph
Construction(S, Œµ)

input : An API sequence set S and an integer Œµ
output: A global API co-occurrence graph

1 Let V and E represent the set of vertices and edges in

the co-occurrence graph respectively;

2 V ‚Üê ‚àÖ, E ‚Üê ‚àÖ;
3 Let œâ represent a weight function : E ‚Üí R‚â•0, where

each edge e in E has a weight œâ(e);

4 foreach A in S do
5

append all API nodes in A into V ;
foreach ai in A do

6

7

E ‚Üê E ‚à™ {(ai, aj)|aj ‚àà A ‚àß j ‚àà {j, j + Œµ}} ;

8 foreach e in E do
9

œâ(e) ‚Üê œâ(e) + 1 ;

10 GC ‚Üê build co-occurrence graph with V , E and œâ ;
11 return GC ;

methods and APIs in projects as user set U , and item set I.
The graphs are constructed from local view (i.e., method-
API interaction), global view (i.e., API co-occurrence in-
formation), and external view (i.e., hierarchical structure),
respectively.

1) Call Interaction Graph GI. It represents the call
relations between methods and APIs, denoted as a bipartite
graph GI = {(u, yui, i)|u ‚àà U, i ‚àà I}, where yui = 1
indicates a method u calls an API i. For example, [my-
File.createFile(), 1, java.io.File.exists()] indicates that a method
myFile.createFile() calls an API java.io.File.exists(). The call in-
teraction graph reÔ¨Çects the basic relations between APIs and
methods, and is commonly adopted by prior studies [11],
[12], [14], [17], [18].

2) Global API Co-occurrence Graph GC. It records the
co-occurrence relations between APIs, e.g., the two API
Ô¨Åle.open() and Ô¨Åle.close() are connected since they ever ap-
peared together in some methods. Algorithm 1 shows the
pseudo-code for global API co-occurrence graph construc-
tion. The graph is built based on a set of API sequences S
and an integer Œµ. SpeciÔ¨Åcally, we Ô¨Årst initialize the set of
vertices V =‚àÖ and the set of edges E=‚àÖ in the co-occurrence
graph (line 2). Then, for each API sequence A in S, we
append all API nodes in A into V and collect all edges such

Fig. 6: Examples of Hierarchical Structure Graph.

that each edge (ai, aj) s.t. ‚àÄai ‚àà A and ‚àÉj ‚àà {i, i + Œµ},
aj ‚àà A (lines 4-7). Next, for each edge e in E, we update
the œâ(e) by counting the occurrence frequencies (lines 8-9).
Finally, we build the co-occurrence graph GC based on V ,
E and œâ, and return the co-occurrence graph GC (lines 10-
11). GC is denoted as {(i, f, j)|i, j ‚àà I, f ‚àà T }, where each
triplet describes that API i and API j are invocated together
f times. For example, [Ô¨Åle.open(), 10, Ô¨Åle.close()] indicates that
Ô¨Åle.open() and Ô¨Åle.close() appear together 10 times. The co-
occurrence graph is beneÔ¨Åcial for enriching the APIs with
frequency information, which can also implicitly comple-
menting the representations of methods.

3) Hierarchical Structure Graph GH. The hierarchical
information, e.g., the belonging projects/packages, implies
the functionality of APIs and methods, thereby helpful for
API recommendation. We consider both project-level and
package-level information, i.e., the projects where methods
are declared and packages that APIs belong to, for con-
structing the hierarchical structure graph. We construct the
graph as a directed graph, denoted as GH = {(h, r, t)|h, t ‚àà
E, r ‚àà R}, in which each triplet (h, r, t) represents there
is a relation r from head entity h to tail entity t, E is the
set of all entities, including API, method, class, project, and
package, and R is the set of relations including belong-to-
class, belong-to-project and belong-to-package. As the example
depicted in Figure 6, projects and packages are organized
as a tree structure. In the example shown in Figure 6
(a), [keyBasedTweents(), belong-to-class, twitter] denotes that
the method keyBasedTweents() belongs to the class twitter;
[twitter, belong-to-project, com.orange] denotes that the class
twitter belongs to the project com.orange.

Embedding Networkinitialset client  methodtarget  APIcall interaction graph‚Ä¶‚Ä¶‚Ä¶initialset initial set ùìîùíñùíñùíäglobal API co-occurrencegraphhierarchical   structuregraph‚Ä¶co-occurrence set ùìîùíñùíÑùíç‚Ä¶structureset ùìîùíñùíâùíç‚Ä¶co-occurrence set ùìîùíäùíÑùíç‚Ä¶structureset ùìîùíäùíâùíçconcatenate(1)Call Interaction Encoding(2)Co-occurrence Information Encoding(3)Hierarchical StructureEncoding(4)Fusion & PredictionFrequency-aware Attentive NetworkStructure-aware Attentive Networkmethod   embeddingAPI embedding#ùíöùíñùíäùíñùíäùìîùíäùíâùìîùíäùíÑPC‚Ä¶Mcom.orangetwitter‚Ä¶PCAjava.io‚Ä¶BufferedWritercloseKeyBasedTweets(a) project(b) package3.3 Training Graph Representation Model and Recom-
mendation

This section introduces how MEGA trains a graph rep-
resentation model based on the constructed multi-view
graphs, and utilizes the trained model to make API rec-
ommendation, corresponding to the second stage and third
stage in Figure 4, respectively.

Figure 5 illustrates the whole process of training and
recommendation, including three graph encoding modules,
i.e., call interaction encoding, co-occurrence information encoding
and hierarchical structure encoding, as well as one fusion and
prediction module. Given a client method, a target API, and
the three heterogeneous graphs as input, the graph repre-
sentation model aims to predict the probability of the target
API invocated by the client method. In the Ô¨Årst module, an
Embedding Network is employed to encode basic interaction
information into local-view representations of the client
method and the target API, as shown in Figure 5 (1). Then, in
the second module, as illustrated in Figure 5 (2), a Frequency-
aware Attentive Network is designed to encode frequency-
based co-occurrence information into representations of the
client method and the target API from global view. Next,
in the third module, a Structure-aware Attentive Network is
designed to encode structure-based hierarchical information
into representations of the client method and the target API
from external view, as shown in Figure 5 (3). Finally, in the
last module, the local-view, global-view and external-view
representations are concatenated as the Ô¨Ånal representations
of the client method and the target API.

3.3.1 Call Interaction Encoding

Call interaction reÔ¨Çects basic information of the client
method u and the target API i, respectively. We utilize
them to generate local-view representations following the
prior study [21]. SpeciÔ¨Åcally, for each client method u, the
called API set is denoted as Eu = {i | i ‚àà {i | (u, yui, i) ‚àà
GI and yui = 1}}. We then obtain the client method u rep-
ei
resentation according to its called API set: e(o)
,
where ei is the embedding of API i and |Eu| is the set size.
Similarly, we obtain the target API i representation e(o)

u =

i‚ààEu
|Eu|

(cid:80)

.

i

3.3.2 Co-occurrence Information Encoding

Global co-occurrence reÔ¨Çects the frequency-enriched in-
formation of the client method u and the target API i.
According to the deÔ¨Ånition of global API co-occurrence graph
in Section 3.2, the value of the edge between a pair of
API nodes denotes their co-occurring frequency, which also
implies their relevant connection. To encode the global-view
information of the client method u and the target API i,
we design a frequency-aware attentive network, as shown in
Figure 7(a). Algorithm 2 shows the pseudo-code for the
encoding. For the client method u, the APIs co-occurred
with its called APIs reveal the method‚Äôs potential call need.
Thus, we utilize the API set Eu obtained from section 3.3.1 as
the initial set E (0)
for the Ô¨Årst-hop propagation on GC (line
u
2). After initialization, we conduct information encoding to
generate co-occurrence representation in each hop.(line 4-8).
Information Encoding (line 4-8). For each triple in
global API co-occurrence graph, i.e., (i, f, j) ‚àà GC , we
deÔ¨Åne the l-hop triple set based on the entity set E (l)
as:
u

Algorithm 2: Encoding Process of GC and GH

input : a constructed graph G, a number of max-hop L,

AttentiveNetwork, an entity set E

output: a list of representation Ke

5

1 let N (v, G) be the set of v‚Äôs neighbors in G;
2 let E be the Ô¨Årst-hop entity set E (0);
3 Ke ‚Üê ‚àÖ;
4 for l=0, 1 ¬∑ ¬∑ ¬∑ , L do
5

S(l) ‚Üê {(i, e(i, j), j)|i ‚àà E (l) ‚àß j ‚àà N (i, G)};
e(l) ‚Üê AttentiveNetwork(S(l));
E (l+1) ‚Üê { j |i ‚àà E (l) ‚àß j ‚àà N (i, G)};
append e(l) into Ke;

6

7

8

9 return Ke;

(a) Frequency-aware Attentive
Network

(b) Structure-aware Attentive
Network

Fig. 7: Illustration of two attentive networks in the graph
representation model.

u = {(i, f, j)|i ‚àà E l
S l
u} (l begins with 0). Following previous
works [22], [23], we sample a Ô¨Åxed-size triple set instead of
using a full-size set to reduce the computation overhead.
Based on sampled co-occurrence associations,

i.e.,
u, we learn l-hop co-occurrence representation

(i, f, j) ‚àà Sl
of the client method u:

e(l)
u =

(cid:88)

œÄ(i, f, j) ej,

(i,f,j)‚ààS l
u

where coefÔ¨Åcient œÄ(i, f, j) is attentively calculated as:

œÄ(i, f, j) =

(cid:80)

exp(mlp(((ei (cid:12) ej)||ef ))

(i(cid:48),f (cid:48),j(cid:48))‚ààS l
u

exp(mlp((ei(cid:48) (cid:12) ej(cid:48))||ef (cid:48)))

(1)

,

(2)

where the notation (cid:12) denotes the element-wise multiplica-
tion operation, and || denotes the concatenation operation.
ei and ej are the embeddings of API i and its co-occurred
API j, respectively. ef is the embedding of frequency f .
mlp(¬∑) is a three-layer MLP with Relu [24] as the nonlinear
activation function. The attention mechanism for encoding
the l-hop co-occurrence representation (i.e., Equ. (1) and (2))
explicitly introduces co-occurred frequency f into calculat-
ing the inÔ¨Çuence of co-occurred API i on the representation
of API j.

After performing L-hop information encoding, where L
is the max hop number, we obtain the global-view repre-
sentations e(c)
u of the client method u by appending the
u , ¬∑ ¬∑ ¬∑ , e(L)
representations from all hops: e(c)
u }.
Similarly, we obtain the global-view representation e(c)
of

u = {e(0)

u , e(1)

i

ùêÇùê®ùêßùêúùêöùê≠ùêûùêßùêöùê≠ùêûùêÖùêÇùê¨‚Ä¶‚Ä¶‚Ä¶(a)Frequency-aware Attentive NetworkùêÇùê®ùêßùêúùêöùê≠ùêûùêßùêöùê≠ùêûùêÖùêÇùê¨‚Ä¶‚Ä¶‚Ä¶(b) Structure-aware Attentive NetworkùêÇùê®ùêßùêúùêöùê≠ùêûùêßùêöùê≠ùêûùêÖùêÇùê¨‚Ä¶‚Ä¶‚Ä¶(a)Frequency-aware Attentive NetworkùêÇùê®ùêßùêúùêöùê≠ùêûùêßùêöùê≠ùêûùêÖùêÇùê¨‚Ä¶‚Ä¶‚Ä¶(b) Structure-aware Attentive Networkthe target API i. The global-view representations e(c)
u and
e(c)
captures frequency-enriched co-occurrence information
i
for enhancing the semantic representations of the client
method and the target API, respectively.

3.3.3 Hierarchical Structure Encoding

Hierarchical structure reÔ¨Çects external contextual infor-
mation of the client method u and the target API i. Ac-
cording to the deÔ¨Ånition of hierarchical structure graph, as
described in Section 3.2, different head entities and relations
endow tail entities with different semantics. To obtain the
representations of the client method u and the target API
i from external view, we design a structure-aware attentive
network, as shown in Figure 7(b).

The encoding process for the hierarchical structural
graph is similar to the encoding process of the API co-
occurrence information, as illustrated in Section 3.3.2, except
for the design of the attentive network. SpeciÔ¨Åcally, for
the l-hop triple set S l
u} in hierarchical
structure graph, we learn l-hop structure representation for
client method u by:

u = {(h, r, t)|h ‚àà E l

e(l)
u =

(cid:88)

œÄ(h, r) et,

(h,r)‚ààS l
u

(3)

where coefÔ¨Åcient œÄ(h, r) is attentively calculated as:

œÄ(h, r) =

(cid:80)

exp(mlp(eh||er))

(h(cid:48),r(cid:48),t(cid:48))‚ààS l
u

exp(mlp(eh(cid:48)||er(cid:48)))

,

(4)

where eh, et are the embeddings of head entity h and tail
entity t, respectively. er is the embedding of relation r.
The structure-aware attention mechanism (i.e., Equ. (3) and
Equ. (4)) explicitly endows the relevance calculation of tail
entity t with the relation r. Based on the structure encoding,
we Ô¨Ånally obtain the external-view representations e(h)
u and
e(h)
for the client method u and the target API i, respectively.
i

3.3.4 Fusion and Prediction

u ||e(c)

and ei = e(o)

We obtain the Ô¨Ånal representation for the client method u
and the target API i by concatenating the multi-view repre-
sentations, i.e., eu = e(o)
||e(h)
u ||e(h)
.
u
i
The Ô¨Ånal representations eu and ei incorporate call inter-
action information, frequency-enriched co-occurrence infor-
mation and structure-based hierarchical information, for ac-
curately capturing the similarly between the client method
u and the target API i. During prediction, we conduct inner
product of eu and ei for calculating the call probability:
ÀÜyui = e(cid:62)

||e(c)
i

i

u ei.

4 EXPERIMENTAL SETUP

In this section, we conduct extensive experiments to
evaluate the proposed approach with the aim of answering
the following research questions:
‚Ä¢ RQ1: How does MEGA perform compared with the state-

of-the-art API usage recommendation approaches?

‚Ä¢ RQ2: What is the impact of the three encoding compo-
nents (i.e., Call Interaction Encoding, Co-occurrence Infor-
mation Encoding and Hierarchical Structure Encoding) in
the graph representation model on the performance of
MEGA?

6
TABLE 2: Statistics of the three datasets: SHS, SHL and
MV. The Call-Avg means the average calls per method.

# Projects
# Packages
# Classes
# Methods
# APIs
# Calls
# Call-Avg

SHS
200
253
4,285
4,530
5,351
27,312
6

SHL
610
714
91,060
191,532
30,576
1,027,644
5

M V
868
340
23,207
32,987
22,054
343,010
10

‚Ä¢ RQ3: How does MEGA perform on low-frequency APIs?
‚Ä¢ RQ4: How do different hyper-parameter settings affect

MEGA‚Äôs performance?

4.1 Dataset Description

To evaluate the effectiveness of MEGA, we utilize three
publicly available benchmark datasets: SHS, SHL, and
M V :
‚Ä¢ SHL contains 610 java projects, Ô¨Åltered from 5,147 ran-
domly downloaded java projects retrieved from GitHub
via the Software Heritage archive [25].

‚Ä¢ SHS is comprised of 200 java projects with small Ô¨Åle sizes
extracted from SHL. It is designed to evaluate some time-
consuming baselines such as PAM [11].

‚Ä¢ MV consists of 868 JAR archives collected from the
Maven Central repository. There are 3,600 JAR archives in
the original dataset, and 1,600 JAR archives remain after
being deduplicated by the previous work [12], [14]. While
through our manual inspection, we Ô¨Ånd that the cleaned
dataset still contains highly similar projects. For example,
some projects have snapshot versions during the devel-
opment process and a release version at the end, such
as commons-1.0.2.RELEASE.jar and commons-1.0.2.BUILD-
SNAPSHOT.jar. Besides, some projects may have their re-
named versions, such as eclipse.equinox.common-3.6.200.jar
and common-3.6.200.jar. In these cases, the two projects are
nearly identical. Too many similar projects in a dataset
may introduce bias in evaluation [12]. Therefore, we de-
cided further clean this dataset by removing duplicated
project versions, i.e., the projects with snapshot versions
or renamed versions. We Ô¨Ånally obtain 868 JAR archives
from 3,600 JAR archives for the M V dataset.

From the source code in datasets, we extract the method
declarations and corresponding API calls, and hierarchical
structure of the projects and packages containing method-
s/APIs. We summarize the detailed statistics of the three
datasets in Table 2.

4.2 Baselines

To demonstrate the effectiveness, we compare our pro-
posed MEGA with one statistic-based method (PAM),
two collaborative Ô¨Åltering(CF)-based methods (FOCUS and
GAPI), as follows:
‚Ä¢ PAM [11] is a statistical method to mine API usage
patterns, which mainly adopts a probabilistic model to
infer the patterns with the highest probabilities from client
code.

‚Ä¢ FOCUS [12] leverages collaborative Ô¨Åltering to recom-
mend API usage patterns. It measures the similarity
between methods via a context-based rating matrix to
recommend potential APIs.

‚Ä¢ GAPI [14] is the state-of-the-art CF-based method that
employs graph neural networks to capture high-order
connectivity between methods and APIs from a uniÔ¨Åed
graph. We re-implement this model according to the orig-
inal paper.

4.3 Evaluation Metrics

[12],

adopt

Following previous approaches
recommendation, we

[14] on API
successRate@K,
usage
P recision@K and Recall@K to evaluate the quality
of
top-K API usage recommendation. Given a top-K
ranked recommendation list RECk(m) for a test method
m ‚àà M and the ground-truth set GT (m), we adopt
M AT CHk(m) = RECk(m) ‚à© GT (m) to present
the
correctly predicted API set. The SuccessRate@K(SR@K),
P recision@K(P @K), and Recall@K(R@K) are deÔ¨Åned
as follows:
‚Ä¢ SuccessRate@K is the proportion of at least one success-

ful match among the top-K APIs.

SR@K =

countm‚ààM(M AT CHk(m) > 0)
|M|

(5)

‚Ä¢ P recision@K is the proportion of correctly predicted

APIs amongst the top-K APIs.

P @K =

|M AT CHk(m)|
k

(6)

‚Ä¢ Recall@K is the proportion of correctly predicted APIs

amongst the ground-truth APIs.

R@K =

|M AT CHk(m)|
|GT (m)|

(7)

4.4 Implementation Details

Following the real development scenario settings simu-
lated by FOCUS, we take the last method of each project as
the test method, i.e., the active method that the developer
is working on. The Ô¨Årst four APIs in this method are
considered as visible context, added to the training set, and
the rest is used as ground truth, added to testing set.

We implement MEGA in PyTorch. The embedding size
is set to 64 for the model in MEGA. We employ the bi-
nary cross-entropy loss as the loss function. To initialize
the model parameters, we utilize the default Xavier ini-
tializer [26]. Also, we choose Adam optimizer [27] to train
our model, with a learning rate of 0.002, a coefÔ¨Åcient of L2
normalization of 10‚àí5, a batch size of 1024 and an epoch
number equal to 40 Ô¨Åxed for all datasets.

Following previous work [28], we set the maximum dis-
tance of adjacent APIs (cid:15) as 3 in constructing GC . Considering
that the edge attribute in GC is a continuous variable, we
adopt the equidistant bucket discretization method, and re-
gard the bucket number as the edge type. The optimal num-
ber of buckets |T | in discretization, the max hop number L
and the size of triple set |S l
u| in each hop l on three datasets
are determined based on the experimental performance. The

best settings of the hyper-parameters for all the baseline
approaches are deÔ¨Åned following the original papers. All
approaches are trained on NVIDIA Tesla V100 GPU.

7

5 RESULTS

5.1 Effectiveness of MEGA Compared with Baselines
(RQ1)

Table 3 presents overall results of all baselines along
with MEGA in terms of SR@K metric, and the comparison
curves of P @K and R@K on three datasets (with K = 1, 5,
10, 20) are shown in Figure 8 and Figure 9, respectively. In-
tuitively, MEGA consistently achieves the best performance
on all datasets. Note that, we only test PAM on SHS due to
its long execution time and scaling poorly in a large dataset.
Detailed observations are as follows:

Comparison of SR@K on a single dataset. Without loss
of generality, we take the SHS dataset as an example to
illustrate the comparison here, and similar trends can also
be observed on other datasets. In the SHS dataset, MEGA
improves over the state-of-the-art baseline GAPI w.r.t SR@1,
SR@5, SR@10, and SR@20 by 125.1%, 85.12%, 65.76% and
39.33% respectively. This demonstrates the effectiveness of
MEGA on various Top-K settings. What‚Äôs more, from Table 3
we can see that SR@K of MEGA increases to 0.836 when K
increases to 20. This means that in most cases, MEGA can
identify the correct API in the Top-20 results, while other
baseline models can only identify about 60% of correct APIs
in the Top-20 results.

Comparison of SR@1 on multiple datasets. To evaluate
the performance of MEGA among multiple datasets com-
pared with baseline models, we choose the SR@1 metric
as it considers both whether a correct API can be included
and whether a correct API can get a higher rank. Overall,
in terms of SR@1, MEGA improves 125.1% and 77.65%
compared to the best baseline GAPI on SHS and SHL,
and 19.85% compared to the best baseline FOCUS on M V .
This demonstrates that MEGA is more effective on multiple
datasets than baselines. We notice that GAPI underperforms
FOCUS on M V . One possible explanation is that when the
Ô¨Årst-order call interactions are abundant enough, high-order
connectivity introduces more noise into the representation
of methods and APIs instead, leading to a negative effect.

It is worth noting that MEGA has more prominent
results in SHS, which is the smallest dataset with the
minimum number of average interactions for per method.
SpeciÔ¨Åcally, the SR@1 of FOCUS and GAPI is 0.161 and
0.195, and for PAM, it is even lower at 0.08, which means
that all baselines fail to provide the correct API at the Ô¨Årst
position for more than 80% of cases. Whereas, the same met-
ric for MEGA is 0.439, meaning that MEGA can successfully
recommend the correct API in the Top-1 result for nearly
half of the client methods. The signiÔ¨Åcant improvement of
MEGA on SR@1 veriÔ¨Åes our approach of encoding diverse
information into Ô¨Ånal representation, especially when his-
torical call interactions are sparse in small dataset.

Different from SR@K and R@K that increases when K
increases, we Ô¨Ånd that P @K decreases when K increases.
Because in most cases, the number of correct APIs is much
fewer than the candidate list size K. While increasing the

TABLE 3: The performance comparison of SR@K between MEGA and the baselines on three datasets.

8

Method

SHS

SHL

M V

SR@1

SR@5

SR@10

SR@20

SR@1

SR@5

SR@10

SR@20

SR@1

SR@5

SR@10

SR@20

PAM
FOCUS
GAPI

0.080
0.161
0.195

0.150
0.256
0.363

0.275
0.328
0.479

0.335
0.422
0.600

-
0.188
0.163

-
0.292
0.402

-
0.349
0.532

-
0.388
0.670

-
0.549
0.260

-
0.709
0.569

-
0.769
0.714

-
0.819
0.837

MEGA

0.439

0.672

0.794

0.836

0.334

0.544

0.641

0.731

0.658

0.810

0.840

0.875

TABLE 4: The performance comparison of SR@K between MEGA and its variants on three datasets.

Method

SHS

SHL

M V

SR@1

SR@5

SR@10

SR@20

SR@1

SR@5

SR@10

SR@20

SR@1

SR@5

SR@10

SR@20

MEGA w/o H&C
MEGA w/o HS
MEGA w/o CO

0.333
0.349
0.423

0.566
0.614
0.651

0.688
0.704
0.757

0.751
0.773
0.815

0.142
0.167
0.311

0.291
0.316
0.521

0.413
0.429
0.629

0.558
0.578
0.728

0.521
0.533
0.614

0.694
0.704
0.746

0.756
0.784
0.780

0.802
0.842
0.816

MEGA

0.439

0.672

0.794

0.836

0.334

0.544

0.641

0.731

0.658

0.810

0.840

0.875

K
@
P

0.4

0.3

0.2

0.1

MEGA
FOCUS

GAPI
PAM

0.4

0.3

0.2

0.1

K
@
P

MEGA
FOCUS

GAPI

K
@
P

0.6
0.5
0.4
0.3
0.2
0.1

MEGA
FOCUS

GAPI

1

5

K

(a) SHS

10

20

1

5

10

20

1

5

K

10

20

K

(b) SHL

(c) M VS

Fig. 8: The performance comparison of P @K between MEGA and the baselines on three datasets.

candidate list size helps MEGA Ô¨Ånd the correct API, it also
involves many irrelevant APIs.

The above experimental results show that pattern-based
methods, such as PAM, relying on mining frequent subse-
quences generally perform worse than learning-based meth-
ods such as GAPI and MEGA. This indicates the signiÔ¨Åcance
of exploring high-order connections and making full use of
external information.

Answer to RQ1: MEGA is quite effective on API us-
age recommendation, outperforming the state-of-the-
art approaches in terms of Success Rate, Precision and
Recall, respectively. Besides, MEGA achieves consis-
tent performance on all datasets.

5.2 Ablation Study (RQ2)

To investigate the effectiveness of each component of the
graph representation model in MEGA, we perform ablation
studies by considering the following three variants.

‚Ä¢ MEGAw/o CO: This variant deletes the co-occurrence in-
formation encoding module from the model to investigate
the impact of global information obtained between APIs.
‚Ä¢ MEGAw/o H&C: This variant only preserves the call in-
teraction encoding in the model to gain the primary
representations of the method and the API, without any
supplementary information.

The experimental results are shown in Table 4. We Ô¨Ånd
that the performance of MEGA drops in three variants com-
pared with the complete model, which demonstrates the
effectiveness of the hierarchical structure encoding module
and co-occurrence information encoding module.

MEGAw/o H&C performs worst since the variant only
utilizes historical call information. Moreover, we notice that
the performance degradation is most signiÔ¨Åcant on SHL.
For example, SR@1 decreases from 0.311 to 0.142. Note that,
in SHL, the average number of call interactions is 5, which
is the smallest among all datasets. This demonstrates the
prominent advantage of appending co-occurrence informa-
tion and structure information to the Ô¨Ånal representation
when the interactions are insufÔ¨Åcient.

‚Ä¢ MEGAw/o HS: This variant removes the hierarchical struc-
ture encoding module from the model to study the effect
of external information derived from the project and pack-
age.

In addition, the performance of MEGAw/o HS is better
than MEGAw/o CO, meaning that hierarchical structure in-
formation is more critical than co-occurrence information.
One possible reason is that the external project/package

MEGA
FOCUS

GAPI
PAM

K
@
R

0.4

0.3

0.2

0.1

MEGA
FOCUS

GAPI

0.4

0.3

0.2

0.1

K
@
R

K
@
R

0.5

0.4

0.3

0.2

0.1

9

MEGA
FOCUS

GAPI

1

5

K

(a) SHS

10

20

1

5

10

20

1

5

K

10

20

K

(b) SHL

(c) M VS

Fig. 9: The performance comparison of R@K between MEGA and the baselines on three datasets.

TABLE 5: The performance comparison over low-frequency
APIs on three datasets.

SR@1

SR@10

P@1

P@10

R@1

R@10

SHS

FOCUS
GAPI
MEGA

0.017
0.020
0.081

FOCUS
GAPI
MEGA

0.003
0.040
0.081

FOCUS
GAPI
MEGA

0.002
0.009
0.014

0.156
0.081
0.263

0.004
0.079
0.315

0.003
0.018
0.041

0.017
0.020
0.081

0.016
0.013
0.046

0.009
0.007
0.029

0.043
0.034
0.176

SHL

0.003
0.040
0.081

0.004
0.012
0.040

0.002
0.020
0.057

0.015
0.047
0.236

M V

0.003
0.009
0.014

0.002
0.003
0.008

0.001
0.006
0.006

0.007
0.016
0.029

structure can provide more contextual information instead
of just internal call relation, which is more beneÔ¨Åcial for
capturing the semantic match between methods and APIs.

Answer to RQ2: Encoding both API co-occurrence
information and project/package hierarchical structure
information into the Ô¨Ånal representations of methods
and APIs is beneÔ¨Åcial to MEGA‚Äôs performance im-
provements. Especially, project/package hierarchical
structure information is more critical, beneÔ¨Åting from it
contains contextual information of methods and APIs.

5.3 Performance of MEGA on low-frequency APIs(RQ3)

As stated in Section 2, we design MEGA to alleviate the
problem that current approaches on low-frequency APIs. To
verify our design, we conduct experiments on APIs called
by methods less than or equal to 3 times.

Table 5 shows SR@K, P @K and R@K (K = {1, 10})
for all approaches on three datasets. To sum up, that MEGA
greatly outperforms other approaches in all metrics. In
detail, SR@10 is improved by 298.7%, P @10 is improved
by 233.3%, and R@10 is improved by 402.1% compared
to the latest approach GAPI on SHL. Looking into the
performance of baselines, GAPI obtains better performance
indicating the effectiveness of incorporat-
than FOCUS,
ing complicated connectivity information in enriching the

representation of APIs with fewer direct interactions. Al-
though MEGA presents a signiÔ¨Åcant improvement on the
recommendation performance, the results of low-frequency
APIs are still quite limited, which may be attributed to the
functional particularity of the APIs and needs more future
research.

Answer to RQ3: MEGA consistently outperforms
the state-of-the-art baselines for recommending low-
frequency APIs on the three benchmark datasets.
low-
Despite the superior performance of MEGA,
frequency recommendation is still challenging and
needs more future research.

5.4 Parameter Sensitivity Study (RQ4)

We conduct experiments to analyze the impact of fol-
lowing hyper-parameters with different settings on MEGA‚Äôs
performance.

Impact of max hop number L. We vary the number of
hops in propagating to observe the performance change of
MEGA. Figure 11(a) depicts the results in terms of SR@10.
We observe that MEGA achieves the best results with one
hop on three datasets, and the performance gradually de-
creases with the hop number increases.

One possible explanation is that, in the graph, short-
distance nodes have a strong correlation with the original
node, while the relevance decays as the distance increases.
Consequently, the positive impact of short-distance prop-
agation is greater, while long-distance propagation brings
more noise than useful signals.

Impact of bucket number |T |. To study the impact
of bucket numbers, we conduct different experiments by
setting different bucket numbers. The experimental results
in terms of SR@10 are presented in Figure 11(b), which
shows that for SHS, SHL, and M V , the best performance
is achieved when the number of buckets is 15, 10, and 15,
respectively.

One possible reason for this phenomenon is that when
the number of buckets is too small, i.e., few relation types,
the graph contains less information, which compromises the
trained model‚Äôs expressiveness. While a large number of
buckets, i.e., many relation types, makes the information in
the graph too rich, leading to over-Ô¨Åtting the model.

Impact of triple set size |S l

u| in each hop l. We change
the number of neighbors selected by the client method and

0.81

0.78

0.75

0.72

0.69

0
1
@
R
S

0.9

0.8

0.7

0.6

0.5

0
1
@
R
S

10

8

16

32

64

8

16

32

64

0.65

0
1
@
R
S

0.63

0.61

0.59

8

16

32

64

0.85

0
1
@
R
S

0.83

0.81

0.79

8

32
16
triple set size

(a) SHS

64

8

32
16
triple set size

(b) SHL

64

8

32
16
triple set size

64

(c) M VS

Fig. 10: The results of SR@10 on three datasets along with different sizes of triple set.

SHS

SHL

M VS

SHS

SHL

M VS

0.9

0.8

0.7

0.6

0
1
@
R
S

1

2

3

4

hop number

1

5
15
10
bucket number

20

(a) The effect of hop numbers

(b) The effect of bucket numbers

Fig. 11: The parameter sensitivity study of hop and bucket
numbers.

the target API in each hop to explore the effects of triple
set size on MEGA‚Äôs performance. The results of SR@10 on
the SHS, SHL and M V are demonstrated in Figure 10(a),
Figure 10(b) and Figure 10(c), respectively.

Jointly analyzing the three sub-Ô¨Ågures, when the size
increases, the results get better Ô¨Årst and then worse. This
means that when the size is moderately large, the beneÔ¨Åts
of more information included improves the performance.
However, when the size is extremely large, the noise in-
troduced outweighs the useful information introduced and
thus it can hurt the performance. Overall, 16 or 32 is the
suitable size of triple set in each hop for both the method
and the API on three datasets.

Answer to RQ4: The larger hop number has negative
effect on MEGA; the bucket number has stable effect
on MEGA; the triple set size has Ô¨Çuctuant effect on
MEGA.

6 DISCUSSION
6.1 Implications

The limited results may be attributed to the functional par-
ticularity of the low-frequency APIs, and could impact the
practical usage of current API recommendation tools. Thus,
we suggest researchers working on API recommendation to
focus more on the recommendation of low-frequency APIs
by combining external knowledge such as API documenta-
tion or exploring data augmentation techniques.

Software Developers. According to our coarse analysis
of the benchmark datasets, low-frequency APIs are usually
not associated with API documentation. API documentation
which contains usage samples and instructions is helpful
for learning the representations of APIs [29], [30]. Thus, we
encourage developers to write some descriptions or usage
examples for facilitating the API recommendation task.

6.2 Threats to Validity

Internal Validity. In this paper, following [22], [23] we
sample a Ô¨Åxed-size of neighbors on graphs instead of us-
ing a full size triple sets for the trade off of computation
overhead. This may slightly inÔ¨Çuence the performance of
MEGA. To alleviate the impact of this threat, we conduct
each experiment Ô¨Åve times and obtain average performance
as shown in Section 5. Furthermore, our experiments on
parameter sensitivity also demonstrates that different sizes
of triple set inÔ¨Çuence the performance of MEGA slightly.

External Validity. We evaluate MEGA under Java
datasets, while MEGA may show different performance
on datasets in other programming languages. To reduce
the impact from different programming languages, when
designing three multi-view graphs, we try to exclude the
language-speciÔ¨Åc information and only maintain the struc-
ture information such as call relationships and deÔ¨Ånition
relationships. We believe our design can be easily adapted
to most programming languages.

In this section, we discuss the implications that would be

helpful for software researchers and software developers.

Software Researchers. In section 5, we achieve that the
heterogeneous information in source code is greatly ben-
eÔ¨Åcial for improving the recommendation performance of
APIs including the low-frequency APIs. However, we also
Ô¨Ånd that the results of low-frequency APIs are still quite
limited, presenting a large gap with those of common APIs.

7 RELATED WORK

In this section, we review existing work about API usage
recommendation. The work on API usage recommendation
can be divided into two categories: pattern-based methods
and learning-based methods. Pattern-based methods utilize
traditional statistical methods to capture usage patterns
from API co-occurrences. Learning-based methods leverage
deep learning models to automatically learn the potential

usage patterns from a large code corpus and then use them
to recommend patterns.

Pattern-based methods. Zhong et al. propose MAPO [9]
to cluster and mine API usage patterns from open source
repositories, and then recommends the relevant usage pat-
terns to developers. Wang et al. improve MAPO and build
UP-Miner [10] by utilizing a new algorithm based on
SeqSim to cluster the API sequences. Nguyen et al. propose
APIREC [31], which uses Ô¨Åne-grained code changes and
the corresponding changing contexts to recommend APIs.
Fowkes et al. propose PAM [11] to tackle the problem that
the recommended API lists are large and hard to under-
stand. PAM mines API usage patterns through an almost
parameter-free probabilistic algorithm and uses them to
recommend APIs. Liu et al. propose RecRank [32] to improve
the top-1 accuracy based on API usage paths. Nguyen et
al. propose FOCUS [12], which mines open-source reposi-
tories and analyzes API usages in similar projects to rec-
ommend APIs and API usage patterns based on context-
aware collaborative-Ô¨Åltering techniques. Previous pattern-
based methods only consider one or two relationships be-
tween APIs, however, MEGA considers call interactions,
API co-occurrences, project/package hierarchical structure
to comprehensively capture the contexts surrounding APIs.
Learning-based methods. Nguyen et al. propose a
graph-based language model GraLan [17] to recommend
API usages. Gu et al. propose DeepAPI [33]. They refor-
mulate API recommendation task as a query-API transla-
tion problem and use an RNN Encoder-Decoder model to
recommend API sequences. Ling et al. propose GeAPI [19].
GeAPI automatically constructs API graphs based on source
code and leverages graph embedding techniques for API
representation. Gu et al. propose Codekernel [18] by repre-
senting code as object usage graphs and clustering them to
recommend API usage examples. Zhou et al. build a tool
named BRAID [34] to leverage learning-to-rank and active
learning techniques to boost recommendation performance.
Previous learning-based methods fail to recommend usage
patterns for low-frequency APIs due to the data-driven
feature, MEGA encodes API frequency with global API co-
occurrence graph to alleviate this problem.

8 CONCLUSION

In this paper, we propose a novel approach MEGA
for automatic API usage recommendation. MEGA employs
heterogeneous graphs, which are constructed from multiple
views, i.e., method-API interaction from local view, API-
API co-occurrence from global view, and project structure
from external view. A graph representation model with a
frequency-aware attentive network and a structure-aware
attentive network is then proposed to learn the matching
scores between methods and APIs based on the multi-
view graphs. Experiment demonstrates MEGA‚Äôs effective-
ness both on overall API usage recommendation and low-
frequency API usage recommendation. For future work, in
addition to the information extracted from projects, some
information from API ofÔ¨Åcial documentation or Q&A sites
also contributes to mining API usage patterns. Therefore,
we plan to design some new modules that encode more
information from different sources.

REFERENCES

11

[1] D. Hou and X. Yao, ‚ÄúExploring the intent behind API evolution:
A case study,‚Äù in 18th Working Conference on Reverse Engineering,
WCRE 2011, Limerick, Ireland, October 17-20, 2011, M. Pinzger,
D. Poshyvanyk, and J. Buckley, Eds.
IEEE Computer Society,
2011, pp. 131‚Äì140.

[2] Z. Yu, C. Bai, L. Seinturier, and M. Monperrus, ‚ÄúCharacterizing the
usage, evolution and impact of java annotations in practice,‚Äù IEEE
Trans. Software Eng., vol. 47, no. 5, pp. 969‚Äì986, 2021.
I. Gvero, ‚ÄúCore java volume I: fundamentals, 9th edition by cay
s. horstmann and gary cornell,‚Äù ACM SIGSOFT Softw. Eng. Notes,
vol. 38, no. 3, p. 33, 2013.

[3]

[4] Oracle, ‚ÄúJdk 18 documentation,‚Äù https://docs.oracle.com/en/

java/javase/18/books.html, 2022.

[5] M. P. Robillard, ‚ÄúWhat makes apis hard to learn? answers from

[6]

developers,‚Äù IEEE Softw., vol. 26, no. 6, pp. 27‚Äì34, 2009.
S. M. Nasehi, J. Sillito, F. Maurer, and C. Burns, ‚ÄúWhat makes a
good code example?: A study of programming q&a in stackover-
Ô¨Çow,‚Äù in 28th IEEE International Conference on Software Maintenance,
ICSM 2012, Trento, Italy, September 23-28, 2012.
IEEE Computer
Society, 2012, pp. 25‚Äì34.

[7] M. Acharya, T. Xie, J. Pei, and J. Xu, ‚ÄúMining API patterns as
partial orders from source code: from usage scenarios to spec-
iÔ¨Åcations,‚Äù in Proceedings of the 6th joint meeting of the European
Software Engineering Conference and the ACM SIGSOFT International
Symposium on Foundations of Software Engineering, 2007, Dubrovnik,
Croatia, September 3-7, 2007, I. Crnkovic and A. Bertolino, Eds.
ACM, 2007, pp. 25‚Äì34.

[8] R. P. L. Buse and W. Weimer, ‚ÄúSynthesizing API usage examples,‚Äù
in 34th International Conference on Software Engineering, ICSE 2012,
June 2-9, 2012, Zurich, Switzerland, M. Glinz, G. C. Murphy, and
M. Pezz`e, Eds.

IEEE Computer Society, 2012, pp. 782‚Äì792.

[9] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei, ‚ÄúMAPO: mining
and recommending API usage patterns,‚Äù in ECOOP 2009 - Object-
Oriented Programming, 23rd European Conference, Genoa, Italy, July
6-10, 2009. Proceedings, ser. Lecture Notes in Computer Science,
S. Drossopoulou, Ed., vol. 5653. Springer, 2009, pp. 318‚Äì343.
[10] J. Wang, Y. Dang, H. Zhang, K. Chen, T. Xie, and D. Zhang,
‚ÄúMining succinct and high-coverage API usage patterns from
source code,‚Äù in Proceedings of the 10th Working Conference on
Mining Software Repositories, MSR ‚Äô13, San Francisco, CA, USA, May
18-19, 2013, T. Zimmermann, M. D. Penta, and S. Kim, Eds.
IEEE
Computer Society, 2013, pp. 319‚Äì328.

[11] J. M. Fowkes and C. Sutton, ‚ÄúParameter-free probabilistic API
mining across github,‚Äù in Proceedings of the 24th ACM SIGSOFT
International Symposium on Foundations of Software Engineering, FSE
2016, Seattle, WA, USA, November 13-18, 2016, T. Zimmermann,
J. Cleland-Huang, and Z. Su, Eds. ACM, 2016, pp. 254‚Äì265.
[12] P. T. Nguyen, J. D. Rocco, D. D. Ruscio, L. Ochoa, T. Degueule,
and M. D. Penta, ‚ÄúFOCUS: a recommender system for mining API
function calls and usage patterns,‚Äù in Proceedings of the 41st Interna-
tional Conference on Software Engineering, ICSE 2019, Montreal, QC,
Canada, May 25-31, 2019, J. M. Atlee, T. Bultan, and J. Whittle, Eds.
IEEE / ACM, 2019.

[13] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl, ‚ÄúItem-based
collaborative Ô¨Åltering recommendation algorithms,‚Äù in Proceedings
of the Tenth International World Wide Web Conference, WWW 10, Hong
Kong, China, May 1-5, 2001, V. Y. Shen, N. Saito, M. R. Lyu, and
M. E. Zurko, Eds. ACM, 2001, pp. 285‚Äì295.

[14] C. Ling, Y. Zou, and B. Xie, ‚ÄúGraph neural network based col-
laborative Ô¨Åltering for API usage recommendation,‚Äù in 28th IEEE
International Conference on Software Analysis, Evolution and Reengi-
neering, SANER 2021, Honolulu, HI, USA, March 9-12, 2021.
IEEE,
2021, pp. 36‚Äì47.

[15] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Mon-
fardini, ‚ÄúThe graph neural network model,‚Äù IEEE Trans. Neural
Networks, vol. 20, no. 1, pp. 61‚Äì80, 2009.

[16] Y. Peng, S. Li, W. Gu, Y. Li, W. Wang, C. Gao, and M. R. Lyu,
‚ÄúRevisiting, benchmarking and exploring API recommendation:
How far are we?‚Äù CoRR, vol. abs/2112.12653, 2021.

[17] A. T. Nguyen and T. N. Nguyen, ‚ÄúGraph-based statistical language
model for code,‚Äù in 37th IEEE/ACM International Conference on
Software Engineering, ICSE 2015, Florence, Italy, May 16-24, 2015,
Volume 1, A. Bertolino, G. Canfora, and S. G. Elbaum, Eds.
IEEE
Computer Society, 2015, pp. 858‚Äì868.

[18] X. Gu, H. Zhang, and S. Kim, ‚ÄúCodekernel: A graph kernel
based approach to the selection of API usage examples,‚Äù in 34th
IEEE/ACM International Conference on Automated Software Engineer-
ing, ASE 2019, San Diego, CA, USA, November 11-15, 2019.
IEEE,
2019, pp. 590‚Äì601.

[19] C. Ling, Y. Zou, Z. Lin, and B. Xie, ‚ÄúGraph embedding based
API graph search and recommendation,‚Äù J. Comput. Sci. Technol.,
vol. 34, no. 5, pp. 993‚Äì1006, 2019.

[20] A. Chen, ‚ÄúContext-aware collaborative Ô¨Åltering system: Predicting
the user‚Äôs preference in the ubiquitous computing environment,‚Äù
in Location- and Context-Awareness, First International Workshop,
LoCA 2005, Oberpfaffenhofen, Germany, May 12-13, 2005, Proceedings,
ser. Lecture Notes in Computer Science, T. Strang and C. Linnhoff-
Popien, Eds., vol. 3479. Springer, 2005, pp. 244‚Äì253.

[21] G. Zhou, X. Zhu, C. Song, Y. Fan, H. Zhu, X. Ma, Y. Yan, J. Jin,
H. Li, and K. Gai, ‚ÄúDeep interest network for click-through
rate prediction,‚Äù in Proceedings of
the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining,
KDD 2018, London, UK, August 19-23, 2018, Y. Guo and
F. Farooq, Eds. ACM, 2018, pp. 1059‚Äì1068. [Online]. Available:
https://doi.org/10.1145/3219819.3219823

[22] H. Wang, F. Zhang, J. Wang, M. Zhao, W. Li, X. Xie, and M. Guo,
‚ÄúRipplenet: Propagating user preferences on the knowledge graph
for recommender systems,‚Äù in Proceedings of the 27th ACM Interna-
tional Conference on Information and Knowledge Management, CIKM
2018, Torino, Italy, October 22-26, 2018, A. Cuzzocrea, J. Allan, N. W.
Paton, D. Srivastava, R. Agrawal, A. Z. Broder, M. J. Zaki, K. S.
Candan, A. Labrinidis, A. Schuster, and H. Wang, Eds. ACM,
2018, pp. 417‚Äì426.

[23] Z. Wang, G. Lin, H. Tan, Q. Chen, and X. Liu, ‚ÄúCKAN: collabo-
rative knowledge-aware attentive network for recommender sys-
tems,‚Äù in Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval, SIGIR 2020,
Virtual Event, China, July 25-30, 2020, J. Huang, Y. Chang, X. Cheng,
J. Kamps, V. Murdock, J. Wen, and Y. Liu, Eds. ACM, 2020, pp.
219‚Äì228.

[24] A. F. Agarap, ‚ÄúDeep learning using rectiÔ¨Åed linear units (relu),‚Äù

arXiv preprint arXiv:1803.08375, 2018.

[25] R. D. Cosmo and S. Zacchiroli, ‚ÄúSoftware heritage: Why and
how to preserve software source code,‚Äù in Proceedings of the 14th
International Conference on Digital Preservation, iPRES 2017, Kyoto,
Japan, September 25-29, 2017, S. Hara, S. Sugimoto, and M. Goto,
Eds., 2017.

[31] A. T. Nguyen, M. Hilton, M. Codoban, H. A. Nguyen, L. Mast,
E. Rademacher, T. N. Nguyen, and D. Dig, ‚ÄúAPI code recommen-
dation using statistical learning from Ô¨Åne-grained changes,‚Äù in
Proceedings of the 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering, FSE 2016, Seattle, WA, USA,

12

[26] X. Glorot and Y. Bengio, ‚ÄúUnderstanding the difÔ¨Åculty of training
deep feedforward neural networks,‚Äù in Proceedings of the Thirteenth
International Conference on ArtiÔ¨Åcial Intelligence and Statistics, AIS-
TATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, ser.
JMLR Proceedings, Y. W. Teh and D. M. Titterington, Eds., vol. 9.
JMLR.org, 2010, pp. 249‚Äì256.

[27] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimiza-
tion,‚Äù in 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Y. Bengio and Y. LeCun, Eds., 2015.

[28] Z. Wang, W. Wei, G. Cong, X. Li, X. Mao, and M. Qiu, ‚ÄúGlobal
context enhanced graph neural networks for session-based rec-
ommendation,‚Äù in Proceedings of the 43rd International ACM SIGIR
conference on research and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020, J. Huang, Y. Chang,
X. Cheng, J. Kamps, V. Murdock, J. Wen, and Y. Liu, Eds. ACM,
2020, pp. 169‚Äì178.

[29] F. Thung, S. Wang, D. Lo, and J. Lawall, ‚ÄúAutomatic recom-
mendation of API methods from feature requests,‚Äù in 2013 28th
IEEE/ACM International Conference on Automated Software Engi-
neering, ASE 2013, Silicon Valley, CA, USA, November 11-15, 2013,
IEEE, 2013, pp. 290‚Äì300.
E. Denney, T. Bultan, and A. Zeller, Eds.
[30] Q. Huang, X. Xia, Z. Xing, D. Lo, and X. Wang, ‚ÄúAPI method
recommendation without worrying about the task-api knowledge
gap,‚Äù in Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering, ASE 2018, Montpellier, France,
September 3-7, 2018, M. Huchard, C. K¬®astner, and G. Fraser, Eds.
ACM, 2018, pp. 293‚Äì304.
November 13-18, 2016, T. Zimmermann, J. Cleland-Huang, and
Z. Su, Eds. ACM, 2016, pp. 511‚Äì522.

[32] X. Liu, L. Huang, and V. Ng, ‚ÄúEffective API recommendation
without historical software repositories,‚Äù in Proceedings of the 33rd
ACM/IEEE International Conference on Automated Software Engineer-
ing, ASE 2018, Montpellier, France, September 3-7, 2018, M. Huchard,
C. K¬®astner, and G. Fraser, Eds. ACM, 2018, pp. 282‚Äì292.

[33] X. Gu, H. Zhang, D. Zhang, and S. Kim, ‚ÄúDeep API learning,‚Äù in
Proceedings of the 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering, FSE 2016, Seattle, WA, USA,
November 13-18, 2016, T. Zimmermann, J. Cleland-Huang, and
Z. Su, Eds. ACM, 2016, pp. 631‚Äì642.

[34] Y. Zhou, H. Jin, X. Yang, T. Chen, K. Narasimhan, and H. C.
Gall, ‚ÄúBRAID: an API recommender supporting implicit user
feedback,‚Äù in ESEC/FSE ‚Äô21: 29th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Soft-
ware Engineering, Athens, Greece, August 23-28, 2021, D. Spinellis,
G. Gousios, M. Chechik, and M. D. Penta, Eds. ACM, 2021, pp.
1510‚Äì1514.

