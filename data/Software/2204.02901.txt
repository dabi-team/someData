2
2
0
2

r
p
A
6

]

C
O
.
h
t
a
m

[

1
v
1
0
9
2
0
.
4
0
2
2
:
v
i
X
r
a

Visualizing Multidimensional Linear
Programming Problems

Nikolay A. Olkhovsky and Leonid B. Sokolinsky(B)[0000−0001−9997−3918]

South Ural State University (National Research University)
76, Lenin prospekt, Chelyabinsk, Russia, 454080
olkhovskiiNA@susu.ru, leonid.sokolinsky@susu.ru

Abstract. The article proposes an n-dimensional mathematical model
of the visual representation of a linear programming problem. This model
makes it possible to use artiﬁcial neural networks to solve multidimen-
sional linear optimization problems, the feasible region of which is a
bounded non-empty set. To visualize the linear programming problem,
an objective hyperplane is introduced, the orientation of which is de-
termined by the gradient of the linear objective function: the gradient
is the normal to the objective hyperplane. In the case of searching a
maximum, the objective hyperplane is positioned in such a way that
the value of the objective function at all its points exceeds the value
of the objective function at all points of the feasible region, which is a
bounded convex polytope. For an arbitrary point of the objective hy-
perplane, the objective projection onto the polytope is determined: the
closer the objective projection point is to the objective hyperplane, the
greater the value of the objective function at this point. Based on the
objective hyperplane, a ﬁnite regular set of points is constructed, called
the receptive ﬁeld. Using objective projections, an image of a polytope is
constructed. This image includes the distances from the receptive points
to the corresponding points of the polytope surface. Based on the pro-
posed model, parallel algorithms for visualizing a linear programming
problem are constructed. An analytical estimation of its scalability is
performed. Information about the software implementation and the re-
sults of large-scale computational experiments conﬁrming the eﬃciency
of the proposed approaches are presented.

Keywords: Linear programming · Multydimensional visualization · Math-
ematical model · Parallel algorithm · BSF-skeleton

1

Introduction

The rapid development of big data technologies [12,11] has led to the emergence
of mathematical optimization models in the form of large-scale linear program-
ming (LP) problems [24]. Such problems arise in industry, economics, logistics,
statistics, quantum physics, and other ﬁelds [4,8,22,3,25]. In many cases, the
conventional software is not able to handle such large-scale LP problems in an

 
 
 
 
 
 
2

N.A. Olkhovsky and L.B. Sokolinsky

acceptable time [2]. At the same time, in the nearest future, exascale supercom-
puters will appear [6], which are potentially capable of solving such problems. In
accordance with this, the issue of developing new eﬀective methods for solving
large-scale LP problems using exascale supercomputing systems is urgent.

Until now, the class of algorithms proposed and developed by Dantzig based
on the simplex method [5] is one of the most common ways to solve the LP
problems. The simplex method is eﬀective for solving a large class of LP prob-
lems. However, the simplex method has some fundamental features that limit its
applicability to large LP problems. First, in the worst case, the simplex method
traverses all the vertices of the simplex, which results in exponential time com-
plexity [35]. Second, in most cases, the simplex method successfully solves LP
problems containing up to 50,000 variables. However, a loss of precision is ob-
served when the simplex method is used for solving large LP problems. Such a
loss of precision cannot be compensated even by applying such computational
intensive procedures as “aﬃne scaling” or “iterative reﬁnement” [34]. Third, the
simplex method does not scale well on multiprocessor systems with distributed
memory. A lot of attempts to parallelize the simplex method were made, but
they all failed [19]. In [14], Karmarkar proposed the inner point method having
polynomial time complexity in all cases. This method eﬀectively solves prob-
lems with millions of variables and millions of constraints. Unlike the simplex
method, the inner point method is self-correcting. Therefore, it is robust to the
loss of precision in computations. The drawbacks of the interior point method
are as follows. First, the interior point method requires careful tuning of its pa-
rameters. Second, this method needs a known point that belongs to the feasible
region of the LP problem to start calculations. Finding such an interior point can
be reduced to solving an additional LP problem. An alternative is the iterative
projection-type methods [23,26,31], which are also self-correcting. Third, like the
simplex method, the inner point method does not scale well on multiprocessor
systems with distributed memory. Several attempts at eﬀective parallelization
for particular cases have been made (see, for example, [10,15]). However, it was
not possible to make eﬃcient parallelization for the general case. In accordance
with this, the research directions related to the development of new scalable
methods for solving LP problems are urgent.

A possible eﬃcient alternative to the conventional methods of LP is optimiza-
tion methods based on neural network models. Artiﬁcial neural networks [20,21]
are one of the most promising and rapidly developing areas of modern informa-
tion technology. Neural networks are a universal tool capable of solving problems
in almost all areas. The most impressive successes have been achieved in image
recognition and analysis using convolutional neural networks [18]. However, in
the scientiﬁc periodicals, there are almost no works devoted to the use of con-
volutional neural networks for solving linear optimization problems [17]. The
reason is that convolutional neural networks are focused on image processing,
but there are no works on the visual representation of multidimensional linear
programming problems in the scientiﬁc literature. Thus, the issue of developing

Visualizing LP Problems

3

new neural network models and methods focused on linear optimization remains
open.

In this paper, we have tried to develop a n-dimensional mathematical model
of the visual representation of the LP problem. This model allows us to employ
the technique of artiﬁcial neural networks to solve multidimensional linear op-
timization problems, the feasible region of which is a bounded nonempty set.
The visualization method based on the described model has a high computa-
tional complexity. For this reason, we propose its implementation as a parallel
algorithm designed for cluster computing systems. The rest of the paper is orga-
nized as follows. Section 2 is devoted to the design of a mathematical model of
the visual representation of multidimensional LP problems. Section 3 describes
the implementation of the proposed visualization method as a parallel algorithm
and provides an analytical estimation of its scalability. Section 4 presents infor-
mation about the software implementation of the described parallel algorithm
and discusses the results of large-scale computational experiments on a clus-
ter computing system. Section 4 summarizes the obtained results and provides
directions for further research.

2 Mathematical Model of the LP Visual Representation

The linear optimization problem can be stated as follows

¯x = arg max

{h
Rm×n, and c

c, x

Ax 6 b, x

Rn

,

}

∈

(1)

i|

Rn, A

where c, b
stands for the
dot product of vectors. We assume that constraint x > 0 is also included in the
system Ax 6 b in the form of the following inequalities:

= 0. Here and below,

h·

∈

∈

·i

,

x1 + 0 +
−
0

x2 + 0 +

· · · · · · · · ·
· · ·

−
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
xn 6 0.
+ 0

· · ·
0 +

+ 0 6 0;
+ 0 6 0;

· · · · · · · · ·

−

The vector c is the gradient of the linear objective function

f (x) = c1x1 + . . . + cnxn.

Let M denote the feasible region of problem (1):

M =

x
{

∈

Rn

|

Ax 6 b

.

}

(2)

(3)

We assume from now on that M is a nonempty bounded set. This means that
M is a convex closed polytope in space Rn, and the solution set of problem (1)
is not empty.
Let ˜ai

Rn be a vector formed by the elements of the ith row of the matrix A.
Then, the matrix inequality Ax 6 b is represented as a system of the inequalities

∈

˜ai, x
i
h

6 bi, i = 1, . . . , m.

(4)

6
4

N.A. Olkhovsky and L.B. Sokolinsky

We assume from now on that

for all i = 1, . . . , m. Let us denote by Hi the hyperplane deﬁned by the equation

˜ai

= 0.

(5)

Thus,

˜ai, x
i

h

= bi (1 6 i 6 m).

Hi =

x
{

∈

Rn

˜ai, x
i

|h

= bi

.

}

(6)

(7)

Deﬁnition 1. The half-space H +
space deﬁned by the equation

i generated by the hyperplane Hi is the half-

H +

i =

x

{

∈

Rn

˜ai, x
i

|h

6 bi

.

}

From now on, we assume that problem (1) is non-degenerate, that is

= j : Hi

= Hj (i, j

i
∀

1, . . . , m

∈ {

) .

}

(8)

(9)

Deﬁnition 2. The half-space H +
with respect to vector c if

i generated by the hyperplane Hi is recessive

x

Hi,

λ

R>0 : x

λc

H +

i ∧

x

λc /
∈

Hi.

∀

∈

−
In other words, the ray coming from the hyperplane Hi in the direction opposite
to the vector c lies completely in H +

i , but not in Hi.

−

∈

∈

∀

(10)

Proposition 1. The necessary and suﬃcient condition for the recessivity of the
half-space H +

i with respect to the vector c is the condition

i
Proof. Let us prove the necessity ﬁrst. Let the condition (10) hold. Equation (7)
implies

˜ai, c
h

> 0.

(11)

(12)

(13)

By virtue of (5),

x =

λ =

bi˜ai
˜ai

k

k

1
˜ai

k

k

Hi.

2 ∈

R>0.

2 ∈

Comparing (10) with (12) and (13), we obtain

bi˜ai
˜ai
k
k
bi˜ai
˜ai

k

k

2 −

2 −

1
˜ai
k
1
˜ai

k

k

k

2 c

∈

H +
i ;

2 c /
∈

Hi.

6
6
6
In view of (7) and (8), this implies

Visualizing LP Problems

5

1
k˜aik2 c

< bi.

(14)

˜ai, bi ˜ai
k˜aik2

−

D

E

Using simple algebraic transformations of inequality (14), we obtain (11). Thus,
the necessity is proved.

Let us prove the suﬃciency by contradiction. Assume that (11) holds, and

there are x

∈

Hi and λ > 0 such that

−
In accordance with (7) and (8), this implies

−

x

λc /
∈

H +

i ∨

x

λc

∈

Hi.

˜ai, x
h

−

λc

i

> bi

that is equivalent to

i −
Since λ > 0, it follows from (11) that

˜ai, x
h

λ

˜ai, c
h

i

> bi.

˜ai, x
i

h

> bi,

but this contradicts our assumption that x

Hi.

Deﬁnition 3. Fix a point z

∈
c =

H +

∈
Rn such that the half-space

x

{

∈

Rn

c, x

|h

z

i

−

6 0

}

⊓⊔

(15)

includes the polytope M :

H +
c .
In this case, we will call the half-space H +
c
hyperplane Hc, deﬁned by the equation

M

⊂

the objective half-space, and the

the objective hyperplane.

Hc =

x
{

∈

Rn

c, x

z

i

−

= 0

,

}

|h

(16)

Denote by πc(x) the orthogonal projection of point x onto the objective hyper-
plane Hc:

πc(x) = x

−

z

i

c.

c, x
h
c

k

−
2
k

Here,
the objective hyperplane Hc as follows:

k · k

stands for the Euclidean norm. Deﬁne distance ρc(x) from x

ρc(x) =

πc(x)

k

.

x
k

−

(17)

H +

c to

∈

(18)

Comparing (15), (17) and (18), we ﬁnd that, in this case, the distance ρc(x) can
be calculated as follows:

ρc(x) = h

.

(19)

c, z

x
i

−
c
k

The following Proposition 2 holds.

k

6

N.A. Olkhovsky and L.B. Sokolinsky

Proposition 2. For all x, y

H +
c ,

∈
ρc(x) 6 ρc(y)

Proof. Equation (19) implies that

ρc(x) 6 ρc(y)

⇔

c, z

c, z
c,

−
+
i
x
i
−
>
c, x
i

⇔ h

⇔ h
⇔ h

⇔ h

6

x
i
c,
h
6

−
c,
h
c, y
h

i

c, z
h
6
x
i
y
−
.

i

c, x
i

⇔ h

>

c, y
h

i

.

c, z
h

x
i

k
−

−
c
k
y
i
c, z
h

c, z

6 h

k

y

i

−
c
k

+

c,
h

y

i

−

i

Proposition 2 says that problem (1) is equivalent to the following problem:

¯x = arg min

ρc(x)
|

{

x

∈

M

.

}

⊓⊔

(20)

Deﬁnition 4. Let the half-space H +
The objective projection γi(x) of a point x
H +
is a point deﬁned by the equation
i

∈

i be recessive with respect to the vector c.
Rn onto the recessive half-space

γi(x) = x

σi(x)c,

−

(21)

where

σi(x) = min

σ

R>0

x

σc

−

∈

H +
i

.

∈

Examples of objective projections in R2 are shown in Figure 1.

(cid:8)

(cid:9)

(cid:12)
(cid:12)

=

(cid:4593)(cid:4593)

(cid:1869)

(cid:1876)(cid:1314)(cid:1314)

(cid:1855)

(cid:1876)(cid:1314)

(cid:1869)(cid:1314)

Fig. 1. Objective projections in space R2: γi(x

(cid:2878)
(cid:1834)(cid:3036)

′

) = q

′

; γi(x

′′

) = q

′′

= x

′′

.

The following Proposition 3 provides an equation for calculating the objective
projection onto a half-space that is recessive with respect to the vector c.

Visualizing LP Problems

7

Proposition 3. Let half-space H +

i deﬁned by the inequality

be recessive with respect to the vector c. Let

˜ai, x
i
h

6 bi

H +
i .

g /
∈

Then,

γi(g) = g

−

bi

c.

˜ai, g
h

i −
˜ai, c
i

h

Proof. According to deﬁnition 4, we have

γi(g) = g

σi(g)c,

−

where

σi(x) = min

σ

R>0

∈

σc

x

−

∈

H +
i

.

Thus, we need to prove that

(cid:8)

(cid:12)
(cid:12)

(cid:9)

˜ai, g

h

i −
˜ai, c
i
h

bi

= min

σ

R>0

∈

σc

x

−

∈

H +
i

.

(cid:9)
Consider the strait line L deﬁned by the parametric equation

(cid:8)

(cid:12)
(cid:12)

Let the point q be the intersection of the line L with the hyperplane Hi:

L =

g + τ c
{

|

τ

∈

R

.

}

Then, q must satisfy the equation

q = L

Hi.

∩

q = g + τ ′c

(22)

(23)

(24)

(25)

(26)

(27)

for some τ ′
instead of x:

∈

It follows that

R. Substitute the right side of equation (27) into equation (6)

˜ai, g + τ ′c

h

= bi.

i

+ τ ′
i
bi

˜ai, c
h
˜ai, g

˜ai, g

h
τ ′ =

= bi,

i
.

(28)

i

− h
˜ai, c
h

i

Substituting the right side of equation (28) into equation (27) instead of τ ′, we
obtain

q = g +

bi

˜ai, g

− h
˜ai, c
h

i

i

c,

8

N.A. Olkhovsky and L.B. Sokolinsky

which is equivalent to

q = g

−

bi

c.

˜ai, g
h

i −
˜ai, c
i

h

Since, according to (26), q

∈

Hi, equation (25) will hold if

σ

∀

∈

R>0 : σ < h

˜ai, g

i −
˜ai, c
i

h

bi

g

σc /
∈

−

⇒

H +
i

holds. Assume the opposite, that is, there exist σ′ > 0 such that

σ′ < h

˜ai, g

bi

i −
˜ai, c
i
h

and

Then, it follows from (22) and (32) that

σ′c

g

−

∈

H +
i .

˜ai, g
h

−

σ′c

i

6 bi.

This is equivalent to

Proposition 1 implies that

˜ai, g
h
˜ai, c
h

i

bi 6 σ′

˜ai, c

.

i −
> 0. Hence, equation (33) is equivalent to

i

h

σ′ > h

bi

.

˜ai, g

i −
˜ai, c
i
h

Thus, we have a contradiction with (31).

(29)

(30)

(31)

(32)

(33)

⊓⊔

Deﬁnition 5. Let g
the polytope M is the point deﬁned by the following equation:

Hc. The objective projection γM (g) of the point g onto

∈

γM (g) = g

σM (g)c,

−

(34)

σM (g) = min

σ
{

∈

R>0|

g

σc

M

.

}

∈

−

where

If

then we set γM (g) = ~
∞
the polytope M .

∈
¬∃
, where ~
∞

σ

R>0 : g

σc

−

∈

M,

stands for a point that is inﬁnitely far from

Examples of objective projections onto polytope M in R2 are shown in Figure 2.

Visualizing LP Problems

9

(cid:1859)(cid:1314)(cid:1314)

(cid:1855)

(cid:1878)

(cid:1859)(cid:1314)

(cid:1869)(cid:1314)

(cid:1834)(cid:3030)

(cid:955)

(cid:1839)

Fig. 2. Objective projections onto polytope M in R2: γM (g

′

′

) = q

; γM (g

′′

) = ~∞.

Deﬁnition 6. A receptive ﬁeld G(z, η, δ)
ter z
∈
conditions:

Hc and rank η

∈

R>0 with cen-
N is a ﬁnite ordered set of points satisfying the following

Hc of density δ

⊂

∈

G(z, η, δ);
G(z, η, δ) :

g

z
−
k
G(z, η, δ) : g′

∈
G(z, η, δ)
g′′
∃
Co(G(z, η, δ))

z

∈
g
∀
∈
g′, g′′
∀
g′
∀
x

∈

∀

∈

6 ηδ√n;
g′

k
= g′′
⇒ k
G(z, η, δ) :
∈
g
∃

−
g′
k
G(z, η, δ) :

∈

g′′

> δ;

k
g′′
k
x
k

−

−
g

k

= δ;
6 1

2 δ√n.

(35)
(36)

(37)
(38)

(39)

The points of the receptive ﬁeld will be called receptive points.

Here, Co(X) stands for the convex hull of a ﬁnite point set X =

x(1), . . . , x(K)

K

K

(cid:8)

Rn:

⊂

(cid:9)

Co(X) =

λix(i)

λi

R>0,

λi = 1

.

∈

)

(

i=1
X

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
In deﬁnition 6, condition (35) means that the center of the receptive ﬁeld belongs
to this ﬁeld. Condition (36) implies that the distance from the central point z to
each point g of the receptive ﬁeld does not exceed ηδ√n. According to (37), for
any two diﬀerent points g′
= g′′ of the receptive ﬁeld, the distance between them
cannot be less than δ. Condition (38) says that for any point g′ of a receptive
ﬁeld, there is a point g′′ in this ﬁeld such that the distance between g′ and g′′ is
equal to δ. Condition (39) implies that for any point x belonging to the convex
hull of the receptive ﬁeld, there is a point g in this ﬁeld such that the distance
between x and g does not exceed 1
2 δ√n. An example of a receptive ﬁeld in the
space R3 is presented in Figure 3.

6
6
10

N.A. Olkhovsky and L.B. Sokolinsky

(

,

,

)

(cid:2372)(cid:3030)

(cid:1878)

(cid:2015)

(cid:2012)

(cid:2015)(cid:2012)

(cid:2012)

(cid:1878)

Fig. 3. Receptive ﬁeld in space R3.

Let us describe a constructive method for building a receptive ﬁeld. Without
= 0. Consider the following set of vectors:

loss of generality, we assume that cn

c(0) = c = (c1, c2, c3, c4, . . . , cn−1, cn);
n

1
c1

c2
i , c2, c3, c4, . . . , cn−1, cn

−

i=2
(cid:16)
(1, 0, . . . , 0), if c1 = 0;
n

X

0,

1
c2

−

i=3
(cid:16)
(0, 1, 0, . . . , 0), if c2 = 0;

X

c2
i , c3, c4, . . . , cn−1, cn

, if c1 6

= 0;

(cid:17)

, if c2 6

= 0;

(cid:17)

0, 0,

−

1
c3

n

i=4

c2
i , c4, . . . , cn−1, cn

(cid:16)
(0, 0, 1, 0, . . . , 0), if c3 = 0;

X

, if c3 6

= 0;

(cid:17)

c(1) =

c(2) =

c(3) =














. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

0, . . . , 0,

1
cn−2

n

i=n−1

c2
i , cn−1, cn

−

X

(cid:16)
(0, . . . , 0, 1, 0, 0), if cn−2 = 0;
c2
n
cn−1 , cn

, if cn−1 6
(cid:16)
(0, . . . , 0, 0, 1, 0), if cn−1 = 0.

0, . . . , 0,

−

(cid:17)

= 0;

, if cn−2 6

= 0;

(cid:17)

c(n−2) =

c(n−1) =








It is easy to see that


i, j

0, 1, . . . , n

1

, i

= j :

c(i), c(j)

= 0.

−
This means that c0, . . . , cn−1 is an orthogonal basis in Rn. In particular,

∈ {

E

D

∀

}

i = 1, . . . , n

∀

−

1 :

c, c(i)

= 0.

(40)

D

E

1)
The following Proposition ?? shows that the linear subspace of dimension (n
generated by the orthogonal vectors c1, . . . , cn−1 is a hyperplane parallel to the
hyperplane Hc.

−

6
6
11

1)

−

(41)

(42)

⊓⊔

(43)

Proposition 4. Deﬁne the following linear subspace Sc of dimension (n
in Rn:

n−1

Visualizing LP Problems

Sc =

(

Then,

s
∀

∈

Proof. Let s

Sc, that is

∈

λic(i)

λi

R

.

)

∈

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Sc : s + z

Hc.

∈

s = λ1c(1) + . . . + λn−1c(n−1).

Then,

c, (s + z)

z

i
h
In view of (40), this implies

−

= λ1

c, c(1)

+ . . . + λn−1

c, c(n−1)

.

D

E

D

E

c, (s + z)
h

−

z

i

= 0.

Comparing this with (16), we obtain s + z

Hc.

∈

Deﬁne the following set of vectors:

e(i) =

c(i)
c(i)

(i = 1, . . . , n

1).

−

It is easy to see that the set

k
e1, . . . , en−1}
This algorithm constructs a receptive ﬁeld G(z, η, δ) consisting of

is an orthonormal basis of subspace Sc.
The procedure for constructing a receptive ﬁeld is presented as Algorithm 1.

{

k

KG = (2η + 1)n−1

(44)

−

points. These points are arranged at the nodes of a regular lattice having the form
of a hypersquare (a hypercube of dimension n
1) with the edge length equal to
2ηδ. The edge length of the unit cell is δ. According to Step 13 of Algorithm 1
and Proposition 4, this hypersquare lies in the hyperplane Hc and has the center
at the point z. The drawback of Algorithm 1 is that the number of nested for
loops depends on the dimension of the space. This issue can be solved using the
function G, which calculates a point of the receptive ﬁeld by its ordinal number
(numbering starts from zero; the order is determined by Algorithm 1). The
implementation of the function G is represented as Algorithm 2. The following
Proposition 5 provides an estimation of time complexity of Algorithm 2.

Proposition 5. Algorithm 2 allows an implementation that has time complex-
ity1

cG = 4n2 + 5n

9,

−

(45)

where n is the space dimension.

12

N.A. Olkhovsky and L.B. Sokolinsky

Algorithm 1 Building receptive ﬁeld G(z, η, δ)
Require: z ∈ Hc, η ∈ N, δ ∈ R>0
1: G := ∅
2: for in−1 = 0 . . . 2η do
sn−1 := in−1δ − ηδ
3:
for in−2 = 0 . . . 2η do
4:
sn−2 := in−2δ − ηδ
5:
. . .
6:
for i1 = 0 . . . 2η do
7:
s1 := i1δ − ηδ
8:
s := 0
9:
for j = 1 . . . n − 1 do
10:
s := s + sje(j)
11:
12:
13:
14:
15:
16: end for

end for
G := G ∪ {s + z}

end for

end for

Proof. Consider Algorithm 3 representing a low-level implementation of Al-
gorithm 2. Values calculated in Steps 1–2 of the algorithm 3 do not depend
on the receptive point number k and therefore can be considered constants. In
Steps 3–8, the repeat/until loop runs (n
1)
operations. In steps 13–16, the nested repeat/until loop runs n times and re-
quires c13:16 = 4n operations. In steps 10–18, the external repeat/until loop
runs (n
1) operations.
In total, we obtain

1) times and requires c10:18 = (4+c13−16)(n

1) times and requires c3:8 = 5(n

1) = 4(n2

−

−

−

−

−

cG = c3:8 + c10:18 = 4n2 + 5n

9.

−

Corollary 1. Time complexity of Algorithm 2 can be estimated as O(n2).

⊓⊔

R>0. The image I(z, η, δ) generated
N, δ
Deﬁnition 7. Let z
by the receptive ﬁeld G(z, η, δ) is an ordered set of real numbers deﬁned by the
equation

Hc. Fix η

∈

∈

∈

I(z, η, δ) =

ρc(γM (g))
|
{

g

∈

G(z, η, δ)
}

.

(46)

The order of the real numbers in the image is determined by the order of the
respective receptive points.

1 Here, time complexity refers to the number of arithmetic and comparison operations

required to execute the algorithm.

Visualizing LP Problems

13

for j = (n − 1) . . . 1 do
ij := (cid:4)k/(2η + 1)j−1(cid:5)
k := k mod (2η + 1)j−1

Algorithm 2 Function G calculates a receptive point by its number k
Require: z ∈ Hc, η ∈ N, δ ∈ R>0
1: function G(k, n, z, η, δ)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end function

end for
g := z
for j = 1 . . . (n − 1) do

g := g + (ij δ − ηδ)e(j)

end for
G := g

The following Algorithm 4 implements the function I(z, η, δ) building an image
as a list of real numbers.

Algorithm 4 Building image I(z, η, δ)
Require: z ∈ Hc, η ∈ N, δ ∈ R>0
1: function I(z, η, δ)
2:
3:
4:
5:
6:
7: end function

I :=[ ]
for k = 0 . . . ((2η + 1)n−1 − 1) do

gk := G(k, n, z, η, δ)
I := I ++ [ρc(γM (gk))]

end for

Here, [ ] stands for the empty list, and ++ stands for the operation of list con-
catenation.
Let

is recessive with respect
M . Assume
to the vector c (see Proposition 1). Let there be a point u
∩
that we managed to create an artiﬁcial neural network DNN, which receives the
image I(πc(u), η, δ) as an input, and outputs the point u′ such that

> 0. This means that the half-space H +
i

˜ai, c
h

Hi

∈

i

u′ = arg min

ρc(x)
|
{

x

∈

Hi

M

.

}

∩

Then, we can build the following Algorithm 5 solving the linear programming
problem (20) using DNN.

14

N.A. Olkhovsky and L.B. Sokolinsky

lj := ⌊k/h⌋
k := k mod h
h := h/p
j := j − 1

Algorithm 3 Low-level implementation of Algorithm 2
1: p := 2η + 1; r := ηδ; h := pn−2; g := z
2: j := n − 1
3: repeat
4:
5:
6:
7:
8: until j = 0
9: j := 1
10: repeat
11:
12:
13:
gi := gi + wje(j)
14:
i := i + 1
15:
16:
until i > n
j := j + 1
17:
18: until j = n

wj := lj δ − r
i := 1
repeat

i

Algorithm 5 Linear programming using DNN
Require: u(1) ∈ Hi ∩ M, h˜ai, ci > 0, z ∈ Hc; η ∈ N, δ ∈ R>0
1: k := 1
2: repeat
I := I(u(k), η, δ)
3:
u(k+1) := DNN(I)
4:
k := k + 1
5:
6: until u(k) 6= u(k−1)
7: ¯x := u(k)
8: stop

Only an outline of the forthcoming algorithm is presented here, which needs
further formalization, detalization and reﬁnement.

3 Parallel Algorithm for Building LP Problem Image

When solving LP problems of large dimension with a large number of constraints,
Algorithm 4 of building the LP problem image can incur signiﬁcant runtime

Visualizing LP Problems

15

overhead. This section presents a parallel version of Algorithm 4, which sig-
niﬁcantly reduces the runtime overhead of building the image of a large-scale
LP problem. The parallel implementation of Algorithm 4 is based on the BSF
parallel computation model [28,27]. The BSF model is intended for a cluster
computing system, uses the master/worker paradigm and requires the repre-
sentation of the algorithm in the form of operations on lists using higher-order
functions Map and Reduce deﬁned in the Bird–Meertens formalism [1]. The BSF
model also provides a cost metric for the analytical evaluation of the scalability
of a parallel algorithm that meets the speciﬁed requirements. Examples of the
BSF model application can be found in [7,31,30,32,33].

Let us represent Algorithm 4 in the form of operations on lists using higher-
order functions Map and Reduce. We use the list of ordinal numbers of inequal-
ities of system (4) as a list, which is the second parameter of the higher-order
function Map:

Designate R∞ = R

∪ {∞}

map = [1, . . . , m] .

L

. We deﬁne a parameterized function

(47)

which is the ﬁrst parameter of a higher-order function Map, as follows:

Fk :

1, . . . , m
{

} →

R∞,

Fk(i) =

ρc (γi(gk)) , if

˜ai, c

> 0 and γi(gk)

(

, if

∞

˜ai, c
h

i

h

i
6 0 or γi(gk) /
∈

M.

M ;

∈

(48)

where gk = G(k, n, z, η, δ) (see Algorithm 2), and γi(gk) is calculated by equa-
tion (24). Informally, the function Fk maps the ordinal number of half-space H +
i
to the distance from the objective projection to the objective hyperplane if H +
i
is recessive with respect to c (see Proposition 1) and the objective projection
.
belongs to M . Otherwise, Fk returns the special value
∞
The higher-order function Map transforms the list
map into the list
L
by applying the function Fk to each element of the list

map:

reduce

L

L

reduce = Map (Fk,

L

map) = [Fk(1), . . . , Fk(m)] = [ρ1, . . . , ρm] .

L

Deﬁne the associative binary operation

(cid:13)↓

R∞ as follows:

: R∞ →
=
;
∞
= α;

β = min(α, β).

α

∈

∀
α, β

∀

∈

∞ (cid:13)↓ ∞
(cid:13)↓ ∞

R : α
R : α

(cid:13)↓

Informally, the operation

calculates the minimum of two numbers.

(cid:13)↓
The higher-order function Reduce folds the list
R∞ by sequentially applying the operation

L

to the entire list:

reduce to the single value

ρ

∈

Reduce(

(cid:13)↓

,

reduce) = ρ1 (cid:13)↓
L

. . .

ρm = ρ.

(cid:13)↓

(cid:13)↓
ρ2 (cid:13)↓

16

N.A. Olkhovsky and L.B. Sokolinsky

Algorithm 6 Building the image I by Map and Reduce
Require: z ∈ Hc, η ∈ N, δ ∈ R>0
1: input n, m, A, b, c, z, η, δ
2: I :=[ ]
3: Lmap :=[1, . . . , m]
4: for k = 0 . . . ((2η + 1)n−1 − 1) do
Lreduce := Map(Fk, Lmap)
5:
ρ := Reduce((cid:13)↓ , Lreduce)
6:
I := I ++ [ρ]
7:
8: end for
9: output I
10: stop

The Algorithm 6 builds the image I of LP problem using higher-order func-
tions Map and Reduce. The parallel version of Algorithm 6 is based on algorith-
mic template 2 in [28]. The result is presented as Algorithm 7.

Algorithm 7 Parallel algorithm of building image I

Master

lth Worker (l =0,. . . ,L-1 )

1: input n
2: I :=[ ]
3: k := 0
4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: until exit
15: output I
16: stop

SendToWorkers k

RecvFromWorkers [ρ0, . . . , ρL−1]
ρ := Reduce ((cid:13)↓ , [ρ0, . . . , ρL−1])
I := I ++ [ρ]
k := k + 1
exit := (cid:0)k > (2η + 1)n−1(cid:1)
SendToWorkers exit

RecvFromMaster k
Lreduce(l) := Map (cid:0)Fk, Lmap(l)(cid:1)
ρl := Reduce (cid:0)(cid:13)↓ , Lreduce(l)(cid:1)
SendToMaster ρl

1: input n, m, A, b, c, z, η, δ
2: L := NumberOfWorkers
3: Lmap(l) :=[lm/L, . . . , ((l + 1)m/L) − 1]
4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: until exit
15:
16: stop

RecvFromMaster exit

Let us explain the steps of Algorithm 7. For simplicity, we assume that the
number of constraints m is a multiple of the number of workers L. We also as-
sume that the numbering of inequalities starts from zero. The parallel algorithm

Visualizing LP Problems

17

includes L + 1 processes: one master process and L worker processes. The master
manages the computations. In Step 1, the master reads the space dimension n.
In Step 2 of the master, the image variable I is initialized to the empty list.
Step 3 of the master assigns zero to the iteration counter k. At Steps 4–14,
the master organizes the repeat/until loop, in which the image I of the LP
problem is built. In Step 5, the master sends the receptive point number gk
to all workers. In Step 8, the master is waiting the particular results from all
workers. These particular results are folded to a single value, which is added
to the image I (Steps 9–10 of the master). Step 11 of the master increases
the iteration counter k by 1. Step 12 of the master assigns the logical value
k > (2η + 1)n−1
to the Boolean variable exit. In Step 13, the master sends the
value of the Boolean variable exit to all workers. According to (44), exit = f alse
(cid:0)
means that not all the points of the receptive ﬁeld have been processed. In this
case, the control is passed to the next iteration of the external repeat/until
loop (Step 14 of the master). After exiting the repeat/until loop, the master
outputs the constructed image I (Step 15) and terminates its work (Step 16).

(cid:1)

All workers execute the same program codes but with diﬀerent data. In Step 3,
the lth worker deﬁnes its own sublist. In Step 4, the worker enters the re-
peat/until loop. In Step 5, it receives the number k of the next receptive point.
In Step 6, the worker processes its sublist
Lmap(l) using the higher-order func-
tion Map, which applies the parameterized function Fk, deﬁned by (48), to each
Lreduce(l), which includes the
element of the sublist. The result is the sublist
distances Fk(i) from the objective hyperplane Hc to the objective projections
of the receptive point gk onto hyperplanes Hi for all i from the sublist
Lmap(l).
In Step 7, the worker uses the higher-order function Reduce to fold the sublist
,
Lreduce(l) to the single value of ρl, using the associative binary operation
(cid:13)↓
which calculates the minimum distance. The computed particular result is sent
to the master (Step 8 of the worker). In Step 13, the worker is waiting for the
master to send the value of the Boolean variable exit. If the received value is
false, the worker continues executing the repeat/until loop (Step 14 of the
worker). Otherwise, the worker process is terminated in Step 16.

Let us obtain an analytical estimation of the scalability bound of the parallel
algorithm 7 using the cost metric of the BSF parallel computation model [28].
Here, the scalability bound means the number of workers at which maximum
speedup is achieved. The cost metric of the BSF model includes the following cost
parameters for the repeat/until loop (Steps 4–14) of the parallel algorithm 7:

m :
D :

tc

:

tMap :

ta

:

L

map;

length of the list
latency (the time taken by the master to send one byte message
to a single worker);
the time taken by the master to send the coordinates of the receptive
point to a single worker and receive the computed value from it
(including latency);
the time taken by a single worker to process the higher-order function
Map for the entire list
the time taken by computing the binary operation

map;

L

.

(cid:13)↓

18

N.A. Olkhovsky and L.B. Sokolinsky

According to equation (14) from [28], the scalability bound of the Algorithm 7
can be estimated as follows:

Lmax =

1
2 s(cid:18)

tc
ta ln 2

2

+

(cid:19)

tMap
ta

+ 4m

tc
ta ln 2

.

−

(49)

Calculate estimations for the time parameters of equation (49). To do this, we
will introduce the following notation for a single iteration of the repeat/until
loop (Steps 4–14) of Algorithm 7:

cc

:

cMap :

ca

:

the quantity of numbers sent from the master to the worker and
back within one iteration;
the quantity of arithmetic and comparison operations computed in
Step 5 of the sequential algorithm 6;
the quantity of arithmetic and comparison operations required
to compute the binary operation

.
(cid:13)↓

At the beginning of every iteration, the master sends each worker the receptive
point number k. In response, the worker sends the distance from the receptive
point gk to its objective projection. Therefore,

In the context of Algorithm 6

cc = 2.

cMap = (cG + cFk ) m,

(50)

(51)

where cG is the number of operations taken to compute coordinates of the
point gk, and cFk is the number of operations required to calculate the value
of Fk(i), assuming that the coordinates of the point gk have already been calcu-
lated. The estimation of cG is provided by Proposition 5. Let us estimate cFk .
2) arith-
According to (24), calculating the objective projection γi(g) takes (6n
1)
metic operations. It follows from (19) that the calculation of ρc(x) takes (5n
arithmetic operations. Inequalities (4) imply that checking the condition x
M
1) arithmetic operations and m comparison operations. Hence,
takes m(2n
3) operations. Hence,
Fk(i) takes a total of (2mn + 11n

−
∈

−

−

−

cFk = 2mn + 11n

3.

−

(52)

Substituting the right-hand sides of equations (45) and (52) in (51), we obtain

cMap = 4n2m + 2m2n + 16nm

12m.

−

(53)

To perform the binary operation

(cid:13)↓

, one comparison operation must be executed:

ca = 1.

(54)

Visualizing LP Problems

19

Let τop stand for average execution time of arithmetic and comparison opera-
tions, and τtr stand for average time of sending a single real number (excluding
the latency). Then, using equations (50), (53), and (54) we obtain

tc = ccτtr + 2D = 2(τtr + D);
tMap = cMapτop = (4n2m + 2m2n + 16nm
ta = caτop = τop.

−

12m)τop;

(55)
(56)

(57)

Substituting the right-hand sides of equations (55) – (57) in (49), we obtain the
following estimations of the scalability bound of Algorithm 7:

Lmax =

1
2 s(cid:18)

2(τtr + D)
τop ln 2

2

(cid:19)

+ 4n2m + 2m2n + 16nm

12m

−

2(τtr + D)
τop ln 2

.

−

where n is the space dimension, m is the number of constraints, D is the latency.
For large values of m and n, this is equivalent to

Lmax

≈

O(

2n2m + m2n + 8nm

6m).

−

If we assume that m = O(n), then it follows from (58) that

p

Lmax

≈

O(n√n),

(58)

(59)

where n is the space dimension. The estimation (59) allows us to conclude that
Algorithm 7 scales very well 2. In the following section, we will verify the ana-
lytical estimation (59) by conducting large-scale computational experiments on
a real cluster computing system.

4 Computational Experiments

We performed a parallel implementation of the algorithm 7 in the form of the
ViLiPP (Visualization of Linear Programming Problem) program in C++ us-
ing a BSF-skeleton [29]. The BSF-skeleton based on the BSF parallel com-
putation model encapsulates all aspects related to parallelization of the pro-
gram using the MPI [9] library and the OpenMP [13] programming interface.
The source code of the ViLiPP program is freely available on the Internet at
https://github.com/nikolay-olkhovsky/LP-visualization-MPI. Using the paral-
lel program ViLiPP, we conducted experiments to evaluate the scalability of
Algorithm 7 on the cluster computing system “Tornado SUSU” [16], the char-
acteristics of which are presented in Table 1.

To conduct computational experiments, we constructed three random LP
problems using the FRaGenLP problem generator [32]. The parameters of these

2 Let Lmax = O(nα). We say: the algorithm scales perfectly if α > 1; the algorithm
scales well if α = 1; the algorithm demonstrates limited scalability if 0 < α < 1; the
algorithm does not scale if α = 0.

20

N.A. Olkhovsky and L.B. Sokolinsky

Table 1. Speciﬁcations of the “Tornado SUSU” computing cluster

Value

Parameter
Number of processor nodes 480
Processor
Processors per node
Memory per node
Interconnect
Operating system

Intel Xeon X5680 (6 cores, 3.33 GHz)
2
24 GB DDR3
InﬁniBand QDR (40 Gbit/s)
Linux CentOS

Table 2. Parameters of test LP problems

Problem
ID
LP 7
LP 6
LP 5

Dimen-
sion
7
6
5

Number of
constraints
4016
4014
4012

Non-zero
values in A
100%
100%
100%

Receptive ﬁeld
cardinality
15 625
3 125
625

problems are given in Table 2. In all cases, the number of non-zero values of the
matrix A of problem (1) was 100%. For all problems, the rank η of the receptive
ﬁeld was assumed to be equal to 2. In accordance to equation (44), the receptive
ﬁeld cardinality demonstrated an exponential growth with an increase of space
dimension.

The results of computational experiments are presented in Table 3 and in
Fig. 4. In all runs, a separate processor node was allocated for each worker.
One more separate processor node was allocated for the master. Computational
experiments shows that the ViLiPP program scalability bound increases with the
increase of the problem dimension. For LP5, the maximum of speedup curve is
reached around 190 nodes. For LP6, the maximum is located around 260 nodes.
For LP7, the scalability bound is approximately equal to 326 nodes. At the same
time, there is an exponential increase of the runtime of building the LP problem
image. Building the LP5 problem image takes 10 seconds on 11 processor nodes.
Building the LP7 problem image takes 5 minutes on the same number of nodes.
An additional computational experiment showed that building an image of the
problem with n = 9 takes 1.5 hours on 11 processor nodes.

The conducted experiments show that on the current development level of
higgh-performance computing, the proposed method is applicable to solving LP
problems that include up to 100 variables and up to 100 000 constraints.

Conclusion

The main contribution of this work is a mathematical model of the visual rep-
resentation of a multidimensional linear programming problem of ﬁnding the
maximum of a linear objective function in a feasible region. The central element
of the model is the receptive ﬁeld, which is a ﬁnite set of points located at the

Visualizing LP Problems

21

Table 3. Runtime of building LP problem image (sec.)

Number of pro-
cessor nodes
11
56
101
146
191
236
281
326

LP5

9.81
1.93
1.55
1.39
1.35
1.38
1.45
1.55

LP6

54.45
10.02
6.29
4.84
4.20
3.98
3.98
4.14

LP7

303.78
59.43
33.82
24.73
21.10
19.20
18.47
18.30

p
u
d
e
e
p
S

18

16

14

12

10

8

6

4

2

0

LP7
LP6
LP5

11

56

146

101

236
Number(cid:3)of(cid:3)processor(cid:3)nodes

191

281

326

Fig. 4. ViLiPP parallel program speedup for LP problems of various sizes.

nodes of a square lattice constructed inside a hypercube. All points of the recep-
tive ﬁeld lie in the objective hyperplane orthogonal to the vector c = (c1, . . . , cn),
which is composed of the coeﬃcients of the linear objective function. The tar-
get hyperplane is placed so that for any point x from the feasible region and
holds. We
any point z of the objective hyperplane, the inequality
can say that the receptive ﬁeld is a multidimensional abstraction of the digital
camera image sensor. From each point of the receptive ﬁeld, we construct a ray
parallel to the vector c and directed to the side of the feasible region. The point
at which the ray hits the feasible region is called the objective projection. The
1), in
image of a linear programming problem is a matrix of dimension (n
which each element is the distance from the point of the receptive ﬁeld to the
corresponding point of objective projection.

c, x
i
h

c, z

<

−

i

h

22

N.A. Olkhovsky and L.B. Sokolinsky

The algorithm for calculating the coordinates of a receptive ﬁeld point by its
ordinal number is described. It is shown that the time complexity of this algo-
rithm can be estimated as O(n2), where n is the space dimension. An outline of
the algorithm for solving the linear programming problem by an artiﬁcial neu-
ral network using the constructed images is presented. A parallel algorithm for
constructing an image of a linear programming problem on computing clusters
is proposed. This algorithm is based on the BSF parallel computation model,
which uses the master/workers paradigm and assumes a representation of the
algorithm in the form of operations on lists using higher-order functions Map
and Reduce. It is shown that the scalability bound of the parallel algorithm ad-
mits the estimation of O(n√n). This means that the algorithm demonstrates a
good scalability.

The parallel algorithm for constructing the multidimensional image of a lin-
ear programming problem is implemented in C++ using the BSF–skeleton that
encapsulates all aspects related to parallelization by the MPI library and the
OpenMP API. Using this software implementation, we conducted large-scale
computational experiments on constructing images for random multidimensional
linear programming problems with a large number of constraints on the “Tor-
nado SUSU” computing cluster. The conducted experiments conﬁrm the validity
and eﬃciency of the proposed approaches. At the same time, it should be noted
that the time of image construction increases exponentially with an increase in
the space dimension. Therefore, the proposed method is applicable for prob-
lems with a number of variables not exceeding 100. However, the number of
constraints can theoretically be unbounded.
Future research directions are as follows.

1. Develop a method for solving linear programming problems based on the

analysis of its images and prove its convergence.

2. Develop and implement a method for the training data set generation to cre-
ate a neural network that solves linear programming problems by analyzing
their images.

3. Develop and train an artiﬁcial neural network solving multidimensional lin-

ear programming problems.

4. Develop and implement a parallel program on a computing cluster that con-
structs multidimensional images of a linear programming problem and cal-
culates its solution using the artiﬁcial neural network.

Funding

The reported study was partially funded by the Russian Foundation for Basic
Research (project No. 20-07-00092-a) and the Ministry of Science and Higher
Education of the Russian Federation (government order FENU-2020-0022).

Visualizing LP Problems

23

References

1. Bird, R.S.: Lectures on Constructive Functional Programming. In: M. Broy (ed.)
Constructive Methods in Computing Science. NATO ASI Series F: Computer and
Systems Sciences, vol. 55, pp. 151–216. Springer, Berlin, Heidelberg (1988)

2. Bixby, R.: Solving Real-World Linear Programs: A Decade and More of Progress.

Operations Research 50(1), 3–15 (2002). doi:10.1287/opre.50.1.3.17780

3. Brogaard, J., Hendershott, T., Riordan, R.: High-Frequency Trading and Price
doi:

Review of Financial Studies 27(8), 2267–2306 (2014).

Discovery.
10.1093/rfs/hhu032

4. Chung, W.: Applying large-scale linear programming in business analytics. In: 2015
IEEE International Conference on Industrial Engineering and Engineering Man-
agement (IEEM), pp. 1860–1864. IEEE (2015). doi:10.1109/IEEM.2015.7385970
5. Dantzig, G.: Linear programming and extensions. Princeton university press,

Princeton, N.J. (1998)

6. Dongarra, J., Gottlieb, S., Kramer, W.: Race to Exascale. Computing in Science

and Engineering 21(1), 4–5 (2019). doi:10.1109/MCSE.2018.2882574

7. Ezhova, N.A., Sokolinsky, L.B.: Scalability Evaluation of Iterative Algorithms Used
for Supercomputer Simulation of Physical processes. In: Proceedings - 2018 Global
Smart Industry Conference, GloSIC 2018, art. no. 8570131, p. 10. IEEE (2018).
doi:10.1109/GloSIC.2018.8570131

8. Gondzio, J., Gruca, J.A., Hall, J., Laskowski, W., Zukowski, M.: Solving large-scale
optimization problems related to Bell’s Theorem. Journal of Computational and
Applied Mathematics 263, 392–404 (2014). doi:10.1016/j.cam.2013.12.003

9. Gropp, W.: MPI 3 and Beyond: Why MPI Is Successful and What Challenges It
Faces. In: J. Traﬀ, S. Benkner, J. Dongarra (eds.) Recent Advances in the Message
Passing Interface. EuroMPI 2012. Lecture Notes in Computer Science, vol. 7490,
pp. 1–9. Springer, Berlin, Heidelberg (2012). doi:10.1007/978-3-642-33518-1 1
10. Hafsteinsson, H., Levkovitz, R., Mitra, G.: Solving large scale linear program-
ming problems using an interior point method on a massively parallel SIMD
computer. Parallel Algorithms and Applications 4(3-4), 301–316 (1994). doi:
10.1080/10637199408915470

11. Hartung, T.: Making Big Sense From Big Data. Frontiers in Big Data 1, 5 (2018).

doi:10.3389/fdata.2018.00005

12. Jagadish, H.V., Gehrke, J., Labrinidis, A., Papakonstantinou, Y., Patel, J.M., Ra-
makrishnan, R., Shahabi, C.: Big data and its technical challenges. Communica-
tions of the ACM 57(7), 86–94 (2014). doi:10.1145/2611567

13. Kale, V.: Shared-memory Parallel Programming with OpenMP.

In: Par-
allel Computing Architectures and APIs, chap. 14, pp. 213–222. Chapman
and Hall/CRC, Boca Raton (2019). doi:10.1201/9781351029223-18/SHARED-
MEMORY-PARALLEL-PROGRAMMING-OPENMP-VIVEK-KALE

14. Karmarkar, N.: A new polynomial-time algorithm for linear programming. Com-

binatorica 4(4), 373–395 (1984). doi:10.1007/BF02579150

15. Karypis, G., Gupta, A., Kumar, V.: A parallel formulation of interior point algo-
rithms. In: Proceedings of the 1994 ACM/IEEE conference on Supercomputing
(Supercomputing’94), pp. 204–213. IEEE Computer Society Press, Los Alamitos,
CA, USA (1994). doi:10.1109/SUPERC.1994.344280

16. Kostenetskiy, P., Semenikhina, P.: SUSU Supercomputer Resources for Indus-
In: Proceedings - 2018 Global Smart Indus-
doi:

try and fundamental Science.
try Conference, GloSIC 2018, art. no. 8570068, p. 7. IEEE (2018).
10.1109/GloSIC.2018.8570068

24

N.A. Olkhovsky and L.B. Sokolinsky

17. Lachhwani, K.: Application of Neural Network Models for Mathematical Program-
ming Problems: A State of Art Review. Archives of Computational Methods in
Engineering 27, 171–182 (2020). doi:10.1007/s11831-018-09309-5

18. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444

(2015). doi:10.1038/nature14539

19. Mamalis, B., Pantziou, G.: Advances in the Parallelization of the Simplex Method.
In: C. Zaroliagis, G. Pantziou, S. Kontogiannis (eds.) Algorithms, Probability, Net-
works, and Games. Lecture Notes in Computer Science, vol. 9295, pp. 281–307.
Springer, Cham (2015). doi:10.1007/978-3-319-24024-4 17

20. Prieto, A., Prieto, B., Ortigosa, E.M., Ros, E., Pelayo, F., Ortega, J., Rojas, I.:
Neural networks: An overview of early research, current frameworks and new chal-
lenges. Neurocomputing 214, 242–268 (2016). doi:10.1016/j.neucom.2016.06.014

21. Schmidhuber, J.: Deep learning in neural networks: An overview. Neural Networks

61, 85–117 (2015). doi:10.1016/j.neunet.2014.09.003

22. Sodhi, M.: LP modeling for asset-liability management: A survey of choices
doi:

Operations Research 53(2), 181–196 (2005).

and simpliﬁcations.
10.1287/opre.1040.0185

23. Sokolinskaya, I.: Parallel Method of Pseudoprojection for Linear Inequalities. In:
L. Sokolinsky, M. Zymbler (eds.) Parallel Computational Technologies. PCT 2018.
Communications in Computer and Information Science, vol. 910, pp. 216–231.
Springer, Cham (2018). doi:10.1007/978-3-319-99673-8 16

24. Sokolinskaya, I., Sokolinsky, L.B.: On the Solution of Linear Programming Prob-
lems in the Age of Big Data. In: L. Sokolinsky, M. Zymbler (eds.) Parallel Com-
putational Technologies. PCT 2017. Communications in Computer and Informa-
tion Science, vol. 753., pp. 86–100. Springer, Cham (2017). doi:10.1007/978-3-319-
67035-5 7

25. Sokolinskaya, I., Sokolinsky, L.B.: Scalability Evaluation of NSLP Algorithm for
Solving Non-Stationary Linear Programming Problems on Cluster Computing Sys-
tems. In: V. Voevodin, S. Sobolev (eds.) Supercomputing. RuSCDays 2017. Com-
munications in Computer and Information Science, vol. 793, pp. 40–53. Springer,
Cham (2017). doi:10.1007/978-3-319-71255-0 4

26. Sokolinskaya, I.M., Sokolinsky, L.B.: Scalability Evaluation of Cimmino Algo-
rithm for Solving Linear Inequality Systems on Multiprocessors with Distributed
Memory. Supercomputing Frontiers and Innovations 5(2), 11–22 (2018). doi:
10.14529/jsﬁ180202

27. Sokolinsky, L.B.: Analytical Estimation of the Scalability of Iterative Numerical Al-
gorithms on Distributed Memory Multiprocessors. Lobachevskii Journal of Math-
ematics 39(4), 571–575 (2018). doi:10.1134/S1995080218040121

28. Sokolinsky, L.B.: BSF: A parallel computation model for scalability estimation of
iterative numerical algorithms on cluster computing systems. Journal of Parallel
and Distributed Computing 149, 193–206 (2021). doi:10.1016/j.jpdc.2020.12.009
29. Sokolinsky, L.B.: BSF-skeleton: A Template for Parallelization of Iterative Nu-
merical Algorithms on Cluster Computing Systems. MethodsX 8, Article number
101,437 (2021). doi:10.1016/j.mex.2021.101437

30. Sokolinsky, L.B., Sokolinskaya, I.M.: Scalable Method for Linear Optimization
of Industrial Processes.
In: Proceedings - 2020 Global Smart Industry Con-
ference, GloSIC 2020, pp. 20–26. Article number 9267,854. IEEE (2020). doi:
10.1109/GloSIC50886.2020.9267854

31. Sokolinsky, L.B., Sokolinskaya, I.M.: Scalable parallel algorithm for solving non-
stationary systems of linear inequalities. Lobachevskii Journal of Mathematics
41(8), 1571–1580 (2020). doi:10.1134/S1995080220080181

Visualizing LP Problems

25

32. Sokolinsky, L.B., Sokolinskaya, I.M.: FRaGenLP: A Generator of Random Lin-
In: L. Sokolinsky,
ear Programming Problems for Cluster Computing Systems.
M. Zymbler (eds.) Parallel Computational Technologies. PCT 2021. Communi-
cations in Computer and Information Science, vol. 1437, pp. 164–177. Springer,
Cham (2021). doi:10.1007/978-3-030-81691-9 12

33. Sokolinsky, L.B., Sokolinskaya, I.M.: VaLiPro: Linear Programming Validator for
Cluster Computing Systems. Supercomputing Frontiers and Innovations 8(3), 51–
61 (2021). doi:10.14529/js 210303

34. Tolla, P.: A Survey of Some Linear Programming Methods. In: V.T. Paschos (ed.)
Concepts of Combinatorial Optimization, 2 edn., chap. 7, pp. 157–188. John Wiley
and Sons, Hoboken, NJ, USA (2014). doi:10.1002/9781119005216.ch7

35. Zadeh, N.: A bad network problem for the simplex method and other minimum
cost ﬂow algorithms. Mathematical Programming 5(1), 255–266 (1973). doi:
10.1007/BF01580132

