2
2
0
2

y
a
M
2
1

]

A
N
.
h
t
a
m

[

2
v
6
3
1
5
0
.
5
0
2
2
:
v
i
X
r
a

A matrix–free high–order solver for the numerical
solution of cardiac electrophysiology

P. C. Africaa,∗, M. Salvadora, P. Gervasiob, L. Dede’a, A. Quarteronia,c

aMOX – Dipartimento di Matematica, Politecnico di Milano,
P.zza Leonardo da Vinci, 32, 20133 Milano, Italy
bDICATAM, Universit`a degli Studi di Brescia,
Via Branze, 38, 25123 Brescia, Italy
cMathematics Institute, ´Ecole Polytechnique F´ed´erale de Lausanne,
Av. Piccard, CH-1015 Lausanne, Switzerland (Professor Emeritus)

Abstract

We propose a matrix–free solver for the numerical solution of the cardiac elec-
trophysiology model consisting of the monodomain nonlinear reaction–diﬀusion
equation coupled with a system of ordinary diﬀerential equations for the ionic
species. Our numerical approximation is based on the high–order Spectral El-
ement Method (SEM) to achieve accurate numerical discretization while em-
ploying a much smaller number of Degrees of Freedom than ﬁrst–order Finite
Elements. We combine sum–factorization with vectorization, thus allowing for
a very eﬃcient use of high–order polynomials in a high performance computing
framework. We validate the eﬀectiveness of our matrix–free solver in a variety
of applications and perform diﬀerent electrophysiological simulations ranging
from a simple slab of cardiac tissue to a realistic four–chamber heart geometry.
We compare SEM to SEM with Numerical Integration (SEM–NI), showing that
they provide comparable results in terms of accuracy and eﬃciency. In both
cases, increasing the local polynomial degree p leads to better numerical results
and smaller computational times than reducing the mesh size h. We also im-
plement a matrix–free Geometric Multigrid preconditioner that entails better
performance in terms of linear solver iterations than state–of–the–art matrix–
based Algebraic Multigrid preconditioners. As a matter of fact, the matrix–free
speed–up with respect to a conventional
solver here proposed yields up to 50
matrix–based solver.

×

Keywords: Cardiac electrophysiology, Matrix–free solver, Spectral Element
Method, High Performance Computing, Geometric Multigrid

∗Corresponding author.
Email address: pasqualeclaudio.africa@polimi.it (P. C. Africa)

 
 
 
 
 
 
1. Introduction

Mathematical and numerical modeling of cardiac electrophysiology provides
meaningful tools to address clinical problems in silico, ranging from the cellular
to the organ scale [1, 2, 3, 4, 5]. For this reason, several mathematical models
and methods have been designed to perform electrophysiological simulations [6,
7]. Among these, we consider the monodomain equation coupled with suitable
ionic models, which describes the space–time evolution of the transmembrane
potential and how chemical species move across ionic channels [8].

This set of combined partial and ordinary diﬀerential equations describes
solutions that resemble those of a wavefront propagation problem, i.e. mani-
festing very steep gradients. Despite being extensively used [9, 10, 11, 12], the
Finite Element Method (FEM) with ﬁrst order polynomials does not seem to
be the most suitable to properly capture the physical processes underlying car-
diac electrophysiology [13]. Indeed, in such cases, a very ﬁne mesh resolution
is required to obtain fully convergent numerical results [14], which calls for an
overwhelming computational burden.

High–order numerical methods come into play to tackle this speciﬁc issue:
Spectral Element Method (SEM) [15, 16, 17], high–order Discontinuous Galerkin
(DG) [18, 19], Finite Volume Method (FVM) [20], or Isogeometric Analysis
(IGA) [21] account for small numerical dispersion and dissipation errors while al-
lowing for converging numerical solutions with less Degrees of Freedom (DOFs)
[22, 23, 24, 25]. However, the use of high–order polynomials in matrix–based
solvers for complex scenarios has been hampered by several numerical chal-
lenges, which are mostly related to the stiﬀness of the discretized monodomain
problem [26].

In this context, we develop and implement a high–order matrix–free nu-
merical solver that can be readily employed for CPU–based, massively parallel,
large–scale numerical simulations. Since there is no need to assemble any matrix,
all the ﬂoating point operations are associated with matrix–vector products that
represent the most demanding computational kernels at each iteration of itera-
tive solvers. Thanks to vectorization [27], which enables algebraic operations on
multiple mesh cells at the same time, and sum–factorization [28, 29], the higher
the polynomial degree, the higher the computational advantages provided by the
matrix–free solver [23, 30]. Moreover, the small memory occupation required
by the matrix–free implementation allows for its exploitation in GPU–based
cardiac solvers [31, 32, 33].

In this manner, we obtain very accurate and eﬃcient numerical simulations
for cardiac electrophysiology, even if the linear solver remains unpreconditioned.
Additionally, we implement a matrix–free Geometric Multigrid (GMG) precon-
ditioner that is optimal for the values of h (mesh size) and p (polynomial degree)
considered in this paper when continuous model properties (i.e a single ionic
model and a continuous set of conductivity coeﬃcients) are employed through-
out the computational domain.

We present diﬀerent benchmark problems of increasing complexity for car-
diac electrophysiology, ranging from the Niederer benchmark on a slab of cardiac

2

tissue [34] to a whole–heart numerical simulation. We focus on two high–order
discretization methods, namely, we compare SEM to SEM with Numerical Inte-
gration (SEM–NI), following the notations introduced in [17]. These two meth-
ods diﬀer in the use of quadrature formulas: Legendre–Gauss formulae for SEM,
Legendre–Gauss–Lobatto for SEM–NI. Numerical results of Section 5 show that
the two methods feature a similar behaviour in terms of both accuracy and com-
putational costs. In both cases, choosing a higher polynomial degree p leads to
a fairly more beneﬁcial ratio between accuracy and computational costs than
reducing the mesh size h. For instance, working with two discretizations with
the same number of DOFs on the Niederer benchmark, the solution computed
with Q4 (local polynomials of degree 4 with respect to each spatial coordinate)
and average mesh size havg = 0.48 mm is more accurate than the one obtained
with Q1 and average mesh size havg = 0.12 mm. Moreover, the former one has
been computed at a computational cost that is about 40% of the latter one.

We also evaluate the performance of our matrix–free solver: a 50

speed–up
is achieved with respect to the matrix–based solver. Furthermore, while the
assembling and solving phases of the monodomain problem take more than 70%
of the total computational time with the matrix–based implementation, this
value drops to approximately 20% with the matrix–free solver.

×

The mathematical models and the numerical methods contained in this pa-
per have been implemented in lifex (https://lifex.gitlab.io/), a high-
performance C++ library developed within the iHEART project and based on
the deal.II (https://www.dealii.org) Finite Element core [35].

The outline of the paper is as follows. We describe the monodomain model
in Section 2. We address its space and time discretizations in Section 3. We
propose the matrix–free solver for cardiac electrophysiology and the matrix–free
GMG preconditioner in Section 4. Finally, the numerical results in Section 5
prove the high eﬃciency of our high–order SEM matrix–free solver against the
low–order FEM matrix–based one.

2. Mathematical model

For the mathematical modeling of cardiac electrophysiology, we consider the

monodomain equation coupled with suitable ionic models ([6, 8])






(cid:19)

(cid:18)

∂u
∂t
u)

·

+

− ∇ ·

n = 0,

u) = χ

(DM∇

Iion(u, w, z)

χ
Cm
(DM∇
dw
dt
dz
dt
u(x, 0) = u0(x), w(x, 0) = w0(x), z(x, 0) = z0(x),

= H(u, w, z),

= G(u, w, z),

Iapp(x, t),

(0, T ],

(0, T ],

×

(0, T ],

(0, T ],

in Ω

×
on ∂Ω

in Ω

in Ω

×

×

in Ω.

(1)

The unknowns are: the transmembrane potential u, the vector w = (w1, . . . , wM )
of the probability density functions of M gating variables, which represent the

3

fraction of open channels across the membrane of a single cardiomyocyte, and
the vector z = (z1, . . . , zP ) of the concentrations of P ionic species. For the
sake of simplifying the notation, in the following the membrane capacitance per
unit area Cm and the membrane surface–to–volume ratio χ are set equal to 1.
The mathematical expressions of the functions H(u, w, z) and G(u, w, z),
which describe the dynamics of gating variables and ionic concentrations re-
spectively, and the ionic current
Iion(u, w, z) strictly depend on the choice of
the ionic model. Here, the TTP06 [36] ionic model is adopted for the slab and
ventricular geometries, while the CRN [37] ionic model is employed for the atria.
The action potential is triggered by an external applied current

Iapp(x, t).

The diﬀusion tensor DM is expressed as follows

n0,

s0 + σnn0 ⊗

DM = σlf0 ⊗

f0 + σts0 ⊗
where the vector ﬁelds f0, s0 and n0 express the ﬁber, the sheetlet and the
sheet–normal (cross–ﬁber) directions, respectively. We also deﬁne longitudinal,
R+, respectively [38].
transversal and normal conductivities as σl, σt, σn ∈
R3 is represented either by a
In this paper, the computational domain Ω
slab of cardiac tissue or by the Zygote geometry [39]. Homogeneous Neumann
boundary conditions are prescribed on the whole boundary ∂Ω to impose the
condition of electrically isolated domain, n being the outward unit normal vector
to the boundary.

(2)

⊂

3. Space and time discretizations

In order to discretize in space the system (1), we adopt SEM [15, 16, 17, 40,
41], a high-order method that can be recast in the framework of the Galerkin
method [13].

We consider a family of hexahedral conforming meshes, satisfying standard
assumption of regularity and quasi-uniformity [13], and let h > 0 denote the
mesh size.

At each time, we look for the discrete solution belonging to the space of
globally continuous functions that are the tensorial product of univariate piece-
1 with
wise (on each mesh element) polynomial functions of local degree p
respect to each coordinate. The local ﬁnite element space is referred to as Qp,
while we denote by Vh,p the global ﬁnite dimensional space.

≥

−

When using SEM, the univariate basis functions are of Lagrangian (i.e.,
nodal) type and their support nodes xi are the Legendre–Gauss–Lobatto quadra-
ture nodes (see, e.g., [42, Ch. 2]), suitably mapped from the reference interval
[

1, 1] to the local 1D elements.
One of the main features of SEM is that, when the data are smooth enough,
the induced approximation error decays more than algebraically fast with re-
Indeed, it is said that SEM features
spect to the local polynomial degree.
exponential or spectral convergence. At the same time, the convergence with
H s(Ω),
respect to the mesh size h behaves as in FEM. More precisely, if u

∈

4

with s > 3
lem in a Lipschitz domain Ω and uSEM ∈
following error estimate holds

2 , denotes the exact solution of a linear second–order elliptic prob-
Vhp is its SEM approximation, the

u
(cid:107)

uSEM(cid:107)H 1(Ω) ≤
SEM can be considered as a special case of hp
nodal basis functions and conforming hexahedral meshes.

Chmin(p+1,s)−1p1−s

−

−

(cid:107)H s(Ω).
u
(cid:107)
FEM ([41, 43, 44]) with

(3)

Typically, when using SEM, the integrals appearing in the Galerkin formu-
lation of the diﬀerential problem (1) are computed by the composite Legendre–
Gauss (LG) quadrature formulas (see [17, 42]). In principle, one can choose LG
formulas of the desired order of exactness to guarantee a highly accurate com-
putation of all the integrals appearing in (1). However, a typical choice is to use
LG formulas with (p + 1) quadrature nodes, which guarantees that the entries
of both the mass matrix and the stiﬀness matrix with constant coeﬃcients are
computed exactly while keeping the computational costs not too large [30, 45].
A considerable improvement in reducing the computational times of evaluat-
ing the integrals consists of using Legendre–Gauss–Lobatto (LGL) quadrature
formulas (instead of LG ones), again with (p + 1) nodes that now coincide with
the support nodes of the Lagrangian basis functions.

−

This results into the so–called SEM–NI method (NI standing for numerical
integration). Since the Lagrangian basis functions are mutually orthogonal with
respect to the discrete L2
inner product induced by the LGL formulas, the mass
matrix of the SEM–NI method is diagonal, although not integrated exactly; this
is a great strength of SEM–NI in solving time–dependent diﬀerential problems
through explicit methods when the mass matrix is assembled. On the other
hand, as the degree of exactness of LGL quadrature formulas using (p + 1)
nodes along each direction is 2p
1, the integrals associated with the nonlinear
terms of the diﬀerential problem may introduce quadrature errors and aliasing
eﬀects that are as signiﬁcant as the nonlinearities.
SEM is equivalent to Q1−

SEM–NI is
FEM in which the integrals are approximated by the trapezoidal

We remark that Q1−

FEM, while Q1−

−

in fact Q1−
quadrature rule [13].

We choose the same local polynomial degree p (and then the same ﬁnite
dimensional space) for approximating the transmembrane potential u, the gating
variables wi (for i = 1, . . . , M ) and the ionic concentrations zi (for i = 1, . . . , P )
at each time t

(0, T ].

All the time derivatives appearing in Eq. (1) have been approximated by the
2nd–order Backward Diﬀerentiation Formula (BDF2) over a discrete set of time
steps tn = n∆t, n = 0, . . . , N , being ∆t the time step size.

∈

Regardless of the quadrature formula (LG or LGL), the algebraic counter-
part of the monodomain problem (1) reads: given w0, z0 and u0, and suitable
1, ﬁnd wn+1, zn+1 and un+1
initializations for w1, z1 and u1, then, for any n

≥

5

by solving the following partitioned scheme:






3wn+1

3zn+1

−

4wn + wn−1
2∆t
4zn + zn−1
2∆t

−

= H(u∗, wn+1, zn+1),

= G(u∗, wn+1, zn+1),

(4)

3un+1

M

4un + un−1
2∆t

−

+ Kun+1 = sn+1 + fn+1.

The arrays un+1 and wn+1 and zn+1 contain the SEM or SEM–NI DOFs of
the transmembrane potential, gating variables and ionic concentrations, respec-
tively, M and K are the SEM or SEM–NI mass and stiﬀness matrices, respec-
tively, and fn+1
Iapp(xi, tn+1). The entries of sn+1 are computed with Ionic
Current Interpolation (ICI) [46], i.e.,

=

i

sn+1
i =

(cid:90)

−

Ω



(cid:88)



j



Iion(u∗

j , wn+1
j

, zn+1
j

)ϕj

 ϕi,

(5)

with ϕi the ith Lagrange basis function of the ﬁnite dimensional space Vh,p.
We remark that, when SEM–NI with LGL quadrature formulas are employed,
ICI coincides with Lumped–ICI [47], as the lumping of the SEM mass matrix
coincides with the SEM–NI mass matrix.

If we set u∗ = un+1, then we recover the fully implicit BDF2 scheme. Nev-
ertheless, we highlight that the function
Iion is typically strongly nonlinear. To
overcome the drawbacks of this nonlinearity, we adopt the extrapolation formula
un−1 of un+1, that is second–order accurate with respect to ∆t. The
u∗ = 2un
resulting semi–implicit scheme is 2nd–order accurate in time when ∆t
0 (see,
e.g., [48]).

→

−

The ordinary diﬀerential equations (4)1,2 are associated with the ionic model
and provide both the gating variables and the ionic species, while equation (4)3
is the discretization of the monodomain equation and its solution at the generic
time step n

1 is obtained by solving the linear system

≥

where

Aun+1 = bn+1,

(6)

A =

M + K,

3
2∆t

1
∆t
Solving the system (6) represents the most computationally demanding part of
(4). We refer to Section 5, in particular to Table 8, for further details about
this speciﬁc aspect.

un−1) + sn+1 + fn+1.

bn+1 =

M(2un

(7)

−

4. Matrix–free and matrix–based solvers

As in FEM, the matrix A based on either SEM or SEM–NI has a very sparse
structure, thus iterative methods are the natural candidates to solve the linear

6

system (6). Since A is symmetric and positive deﬁnite, we have adopted the
Conjugate Gradient (CG) method or its preconditioned version (PCG).

Excluding the preconditioner step, the most expensive part of one CG–
iteration is the evaluation of a matrix–vector product Av, where v is a given
vector.

Typically, in a conventional matrix–based solver, the matrix A is assembled
and stored in sparse format, then referenced whenever the matrix–vector prod-
uct has to be evaluated, i.e. during each iteration of the CG. The matrix–based
solver aims at minimizing the number of ﬂoating points operations required for
FEM discretization for which
such evaluation and is a winning strategy in Q1−
the band of the matrix A is small.

When SEM (or SEM–NI) of local degree p are employed in the discretization
process, each cell counts (p + 1)3 DOFs. It follows that the typical bandwidth
of SEM (or SEM–NI) stiﬀness matrices is about C(p + 1)3 (where C is the
maximum number of cells sharing one node of the mesh) and it exceeds widely
FEM stiﬀness matrices. The large bandwidth of the SEM matrix
that of Q1−
A can worsen the computational times of accessing the matrix entries, thus
deteriorating the eﬃciency of the iterative solver.

Moreover, in modern processors, access to the main memory has become the
bottleneck in many solvers for partial diﬀerential equations: a matrix–vector
product based on matrices requires far more time waiting for data to arrive
from memory than on actually doing the ﬂoating point operations. Thus, it is
proved to be more eﬃcient to recompute matrix entries – or rather, the action
of the diﬀerential operator represented by these entries on a known vector, cell
by cell – rather than looking up matrix entries in the memory, even if the former
approach requires a signiﬁcant number of additional ﬂoating point operations
[30].

This approach is referred to as matrix–free.

In practice, shape functions
values and gradients are pre-computed for each basis function on the reference
cell, for each quadrature node. Then, the Jacobian of the transformation from
the real to the reference cell is cached, thus improving the computational cost
of the evaluation.

A matrix–free solver can also beneﬁt from vectorization, that brings an ad-
ditional speedup. In FEM solvers (and, similarly, in SEM ones), the cell–wise
computations are typically exactly the same for all cells, and hence a Single–
Instruction, Multiple–Data (SIMD) stream can be used to process several values
at once (see Figure 1). Vectorization is a SIMD concept, that is, one CPU in-
struction is used to process multiple cells at once. Modern CPUs support SIMD
instruction sets to diﬀerent extents, i.e. one single CPU instruction can simul-
taneously process from two doubles (four ﬂoats) up to eight doubles (or sixteen
ﬂoats), depending on the underlying architecture [49]. Vectorization can be eas-
ily applied to a parallel framework [50], which in our case results in the scheme
shown in Figure 2.

Vectorization is typically not used explicitly in matrix–based Finite Element
codes as it is beneﬁcial only in arithmetic intensive operations, whereas addi-
tional computational power becomes useless when the workload bottleneck is

7

Figure 1: Comparison between scalar and vectorized operations.

(a) Original mesh.

(b) Parallel partitioning of
mesh cells.

(c) Vectorization of cell oper-
ations on each parallel unit.

Figure 2: Vectorization in a parallel framework: when the original mesh is partitioned among
multiple computational units, vectorization is applied at the level of each process. Here,
diﬀerent colors refer to diﬀerent parallel units and light/dark variations represent diﬀerent
vectorized batches (in this example, vectorization acts on 8 cells).

8

Scalarop=Vectorizedop=the memory bandwidth.

When vectorization is available, it can be more convenient to repeat the
arithmetic operations to compute the action of the matrix on a known vector
every time that it is needed, instead of accessing the matrix entries to compute a
matrix–vector product. Moreover, thanks to the fact that the multivariate SEM
Lagrange basis is of tensorial type, in order to reduce the computational com-
plexity of one evaluation of the product Av, sum–factorization can be exploited
[28, 29, 51]; in this way, the matrix–vector product in the generic three dimen-
sional cell requires only 9(p + 1) ﬂoating point operations instead of (p + 1)3 per
(cid:0)9(p + 1)4(cid:1) instead of
degree of freedom, resulting in a complexity equal to
(cid:0)(p + 1)6(cid:1), and this plays in favor of repeating the computation rather than

O
of accessing the memory (see [52, Sect. 2.3.1] and [42, Sect. 4.5.1]).

O

On these bases, a high–order matrix–free solver is more eﬃcient both in
terms of memory occupation (no system matrix is assembled and stored globally)
and computational time [30], as we will also show in Sect. 5.

To precondition the CG method we have chosen Multigrid preconditioners.
For the matrix–based solver, the Algebraic Multigrid (AMG) preconditioner
[53, 54] turns out to be a very eﬃcient choice. Nevertheless, its implementation
requires the explicit knowledge of the entries of the matrix A. For this reason,
this preconditioner cannot be used in a matrix–free context.

To overcome this drawback, in the latter case we have adopted a Geomet-
ric Multigrid (GMG) preconditioner, more precisely the high–order h–multigrid
preconditioner [55], which uses p–degree interpolation and restriction among
geometrically coarsened meshes. GMG methods are among the most eﬃcient
solvers for linear systems arising from the discretization of elliptic partial dif-
(n) in the number of un-
ferential equations, oﬀering an optimal complexity
knowns n, and they are often used as very eﬃcient preconditioners (see [32, 53,
56, 57] and the literature cited therein).

O

GMG turns out to be very eﬃcient in a matrix–free context because all
its computational kernels, including the Chebyshev smoother and the trans-
fer between diﬀerent grid levels, are based on matrix–vector products involv-
ing suitable collections of mesh cells [58].
In our implementation, the GMG
preconditioner exploits the hierarchical meshes that are built by the recursive
subdivision of each cell into 8 subcells, starting from a coarse mesh T0 of size
h0, as shown in Figure 3.

5. Numerical results

We present several numerical simulations of cardiac electrophysiology. First,
we consider a benchmark problem on a slab of cardiac tissue [34], in order to
compare SEM against SEM–NI and matrix–free against matrix–based in terms
of computational eﬃciency and mathematical accuracy. Then, we employ the
Zygote left ventricle geometry and we analyze the sole impact of increasing p,
i.e. the local polynomial degree, on the numerical solution. Finally, for the sake
of completeness, we show the capability of our matrix–free solver by presenting

9

Figure 3: Schematization of multigrid methods. Starting from the real mesh T , the action of
the restriction and prolongation operators is shown, down to the coarse mesh T0.

Variable

Value

Unit Variable

Value

Unit

Conductivity tensor
σl
σt
σn

0.7643
0.3494
0.1125

10−4 m2 s−1
10−4 m2 s−1
10−4 m2 s−1

·
·
·

Applied current
max
(cid:101)
app
I
tapp

10−3

3

15 V s−1
s

·

Table 1: Parameters of the electrophysiological model. For the CRN and TTP06 ionic models,
we adopt the parameters reported in the original papers [36, 37] for epicardial cells.

a detailed Zygote four–chamber heart electrophysiological simulation in sinus
rhythm.

For the time discretization, we use the BDF2 scheme with a time step ∆t =
0.1 ms. The ﬁnal time T diﬀers with the speciﬁc test case. We employ T = 0.2 s
in the Niederer benchmark [34], while T = 0.6 s and T = 0.8 s are considered
for the left ventricle and whole–heart geometries, respectively.

Regarding the linear algebra back–end, we use the PCG solver with a stop-
ping criterion based on the absolute residual with tolerance 10−15. In the two
test cases involving the slab and left ventricle, we employ the GMG (AMG,
resp.) preconditioner for the matrix–free (matrix–based, resp.) solver. On the
other hand, no preconditioner is introduced in the four–chamber heart numerical
simulation, due to the presence of diﬀerent ionic models in the computational
domain, namely the CRN model ([37]) for atria and the TTP06 one ([36]) for
ventricles.

Fiber generation is performed by means of the Laplace–Dirichlet rule–based
methods proposed in [38, 59], while in Table 1 we report the parameters of the
monodomain equation.
The external current

(0, tapp] in a cuboid
for the Niederer benchmark (as described in [34]), otherwise in diﬀerent spheres
for the ventricle and whole–heart test cases (we can deduce them from the
numerical results shown in Figures 4, 10 and 12).

max
app is applied for t
Iapp(x, t) = (cid:101)
I

∈

All the numerical simulations were performed by using one cluster node
endowed with 56 cores (two Intel Xeon Gold 6238R, 2.20 GHz), which is available

10

T0TRestrictionProlongationFigure 4: Niederer benchmark. The geometry and an example of mesh (left) and the associated
simulated activation map (right). The blue color represents the region where the cubic stimulus
is initially applied; the red one is associated with the corresponding diagonally opposite vertex,
which is activated as last.

at MOX, Dipartimento di Matematica, Politecnico di Milano.

5.1. Slab of cardiac tissue

The computational domain with an example of mesh (left) and the associ-
ated numerical simulation (right) for the Niederer benchmark [34] is depicted
in Figure 4. An external stimulus of cubic shape is applied at one vertex, the
electric signal propagates through the slab, and the diagonally opposite vertex is
activated as the last point. The domain is discretized by means of a structured,
uniform hexahedral mesh.

We present a systematic comparison between SEM and SEM–NI for several
values of both the mesh size h and the local polynomial degree p, in order to un-
derstand which is the best formulation in terms of accuracy and computational
cost. Moreover, we compare the eﬃciency of the matrix–free and matrix–based
solvers for SEM.

In Figures 5 and 6 we show the action potential and the calcium concentra-
tion computed with SEM and SEM–NI, respectively, over time. More precisely,
the minimum, average, maximum and point values are plotted, where the max,
min, and mean functions are evaluated on the set of nodes of the mesh. We
notice that the convergence is faster for increasing p rather than for vanishing
h.

At each node xi of the mesh, we also compute the activation time τ as the
time instant when the approximation of the transmembrane potential u presents
maximum derivative, i.e.

τ (xi) = arg max

t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂u
∂t

(xi, t)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(8)

In the formula above t spans over the discrete set of time steps and the time
derivative is approximated via the same scheme used for the time discretization
of problem (1).

In Figure 7 we show the activation times along the slab diagonal, for diﬀerent
choices of the local space (from Q1 to Q4) and mesh reﬁnements. As the error

11

Figure 5: Niederer benchmark. Minimum (top), average (second), maximum (third) and point
values (bottom) of the action potential u and intracellular calcium concentration Ca2+ over
time for a slab of cardiac tissue. P is a random point within the computational domain
away from the initial stimulus. We consider diﬀerent hp combinations: SEM Q1 to Q4 and
havg = 0.48 mm to havg = 0.06 mm (havg is the average mesh size).

12

0.000.050.100.150.20Time[s]−500minΩuhp[mV]Actionpotential0.040.050.060.000.050.100.150.20Time[s]0.10.20.3minΩCa2+hp[µM]Intracellularcalciumconcentration0.050.060.070.080.000.050.100.150.20Time[s]−75−50−25025meanΩuhp[mV]Actionpotential0.030.040.050.060.000.050.100.150.20Time[s]0.10.20.30.4meanΩCa2+hp[µM]Intracellularcalciumconcentration0.040.060.000.050.100.150.20Time[s]−50050maxΩuhp[mV]Actionpotential0.020.040.000.050.100.150.20Time[s]0.10.20.30.4maxΩCa2+hp[µM]Intracellularcalciumconcentration0.050.100.150.000.050.100.150.20Time[s]−500uhp(P)[mV]Actionpotential0.0020.0040.000.050.100.150.20Time[s]0.10.20.30.4Ca2+hp(P)[µM]Intracellularcalciumconcentration0.020.04SEM(Q1,havg=0.48mm)SEM(Q1,havg=0.24mm)SEM(Q1,havg=0.12mm)SEM(Q1,havg=0.06mm)SEM(Q2,havg=0.48mm)SEM(Q3,havg=0.48mm)SEM(Q4,havg=0.48mm)Figure 6: Niederer benchmark. Minimum (top), average (second), maximum (third) and point
values (bottom) of the action potential u and intracellular calcium concentration Ca2+ over
time for a slab of cardiac tissue. P is a random point within the computational domain away
from the initial stimulus. We consider diﬀerent hp combinations: SEM-NI Q1 to Q4 and
havg = 0.48 mm to havg = 0.06 mm (havg is the average mesh size).

13

0.000.050.100.150.20Time[s]−75−50−25025minΩuhp[mV]Actionpotential0.040.050.060.000.050.100.150.20Time[s]0.10.20.3minΩCa2+hp[µM]Intracellularcalciumconcentration0.050.060.070.080.000.050.100.150.20Time[s]−75−50−25025meanΩuhp[mV]Actionpotential0.030.040.050.060.000.050.100.150.20Time[s]0.10.20.30.4meanΩCa2+hp[µM]Intracellularcalciumconcentration0.040.060.000.050.100.150.20Time[s]−500maxΩuhp[mV]Actionpotential0.020.040.000.050.100.150.20Time[s]0.10.20.30.4maxΩCa2+hp[µM]Intracellularcalciumconcentration0.050.100.150.000.050.100.150.20Time[s]−500uhp(P)[mV]Actionpotential0.0020.0040.000.050.100.150.20Time[s]0.10.20.30.4Ca2+hp(P)[µM]Intracellularcalciumconcentration0.020.04SEM-NI(Q1,havg=0.48mm)SEM-NI(Q1,havg=0.24mm)SEM-NI(Q1,havg=0.12mm)SEM-NI(Q1,havg=0.06mm)SEM-NI(Q2,havg=0.48mm)SEM-NI(Q3,havg=0.48mm)SEM-NI(Q4,havg=0.48mm)(a) SEM.

(b) SEM–NI.

Figure 7: Niederer benchmark. Activation times computed along the diagonal of the slab
(see Figure 4), with diﬀerent choices of the local space (Q1 to Q4) and mesh reﬁnements
(havg = 0.48 mm to havg = 0.06 mm).

accumulates over the diagonal, the inset plots show a zoom around the right
endpoint. Such results demonstrate that high polynomial degrees p, even with
a coarse mesh size h, lead to a faster convergence rate compared to the small–p,
small–h scenario.

To better investigate the comparison between SEM and SEM–NI, in Figure 8

14

0.00.51.01.52.0Arclength[cm]10203040AT[ms]2.002.052.102.154045SEM(Q1,havg=0.48mm)SEM(Q1,havg=0.24mm)SEM(Q1,havg=0.12mm)SEM(Q1,havg=0.06mm)SEM(Q2,havg=0.48mm)SEM(Q3,havg=0.48mm)SEM(Q4,havg=0.48mm)0.00.51.01.52.0Arclength[cm]01020304050AT[ms]2.002.052.102.154045SEM-NI(Q1,havg=0.48mm)SEM-NI(Q1,havg=0.24mm)SEM-NI(Q1,havg=0.12mm)SEM-NI(Q1,havg=0.06mm)SEM-NI(Q2,havg=0.48mm)SEM-NI(Q3,havg=0.48mm)SEM-NI(Q4,havg=0.48mm)we show the quantities

(cid:32)

errmax =

∆t

(cid:32)

errmin =

∆t

(cid:32)

errmean =

∆t

(cid:88)
n |

(cid:88)
n |

(cid:88)
n |

max
x

uhp(x, tn)

max
x

−

uref (x, tn)

2
|

(cid:33)1/2

min
x

uhp(x, tn)

min
x

uref (x, tn)
|

−

2

(cid:33)1/2

(cid:33)1/2

mean
x

uhp(x, tn)

mean
x

−

uref (x, tn)

2
|

(cid:32)

errP =

∆t

(cid:88)
n |

uhp(P, tn)

uref (P, tn)
|

−

2

(cid:33)1/2

(9)

(10)

(11)

(12)

versus the total number of mesh points, for both SEM and SEM–NI solution
FEM on a very
uhp. Our reference solution uref has been computed with Q1−
ﬁne grid with average mesh size havg = 0.06 mm, for a total of 11’401’089 mesh
points. P is a random point within the computational domain away from the
initial stimulus. The max, min, and mean functions are evaluated on the set
of nodes of the mesh. The number of mesh points increases by reducing h for
both “SEM h” and “SEM–NI h”, while it increases with p for both “SEM p”
and “SEM–NI p”.

The numerical results conﬁrm the typical behaviour of SEM and SEM–NI
discretizations, i.e. the errors decrease faster by increasing p rather than de-
creasing h. Moreover, we notice that SEM and SEM–NI errors behave quite
similarly, with a slight advantage for SEM.

In Tables 2–5 we report the CPU time required by the linear solver for the
whole numerical simulation (the times are cumulative over all time steps), for
SEM and SEM–NI discretizations, matrix–free and matrix–based solvers. Fur-
thermore, in Figure 9 we plot the errors (9)–(12) versus the CPU time required
to solve all the linear systems along the whole numerical simulation. For SEM–
NI we only report the times relative to the matrix–free solver, while for SEM we
report the times for both the matrix–free and matrix–based solvers. The same
symbol (circle, square, diamond, and cross) refers to the numerical simulations
carried out on the meshes with the same number of DOFs. If we compare the
SEM,
errors and the CPU–times of Q1−
havg = 0.48 mm (these two conﬁgurations share the same number 1’449’665 of
mesh nodes), we notice that the errors of Q4−
SEM are at most about 1/3 – 1/2
SEM and the ratio of the corresponding CPU times is about 40%.
of that of Q1−
SEM outperforms Q1−
Thus, we conclude that Q4−
For the comparison between matrix–free and matrix–based solvers, we notice
that the former one is always faster, and the gain of matrix–free over matrix–
based solver increases with the polynomial degree p. More precisely, the speed–
up factors are shown in Table 6 when havg = 0.48 mm, and in Table 7 when
p = 1.

SEM, havg = 0.12 mm with those of Q4−

SEM (that is Q1−

FEM).

15

Figure 8: Niederer benchmark. Errors for action potential u and intracellular calcium concen-
tration Ca2+ versus the number of mesh points (dofs) used in a slab of cardiac tissue.

16

105106dofs10−1100errorminActionPotential105106dofs10−310−2errorminIntracellularCalciumConcentration105106dofs10−1100errormeanActionPotential105106dofs10−3errormeanIntracellularCalciumConcentration105106dofs10−1100errormaxActionPotential105106dofs10−410−3errormaxIntracellularCalciumConcentration105106dofs10−210−1errorPActionPotential105106dofs10−510−4errorPIntracellularCalciumConcentrationSEMhSEM-NIhSEMpSEM-NIpMesh points
number

Cells
number

Local
space

Linear solver
SEM [s]

Linear solver
SEM–NI [s]

Assemble rhs
SEM [s]

Assemble rhs
SEM–NI [s]

25’025
187’425
618’529
1’449’665

21’888
21’888
21’888
21’888

Q1
Q2
Q3
Q4

10.769
14.694
37.733
91.380

8.031
12.383
36.419
90.370

0.784
4.710
14.867
33.899

0.766
4.645
14.542
32.920

Table 2: Niederer benchmark. Computational times for SEM and SEM–NI, with a ﬁxed
average mesh size havg = 0.48 mm and p ranging from 1 to 4. Matrix–free solver.

Mesh points
number

Cells
number

havg
[mm]

Linear solver
SEM [s]

Linear solver
SEM–NI [s]

Assemble rhs
SEM [s]

Assemble rhs
SEM–NI [s]

25’025
187’425
1’449’665
11’401’089

21’888
175’104
1’400’832
11’206’656

0.48
0.24
0.12
0.06

10.769
29.157
256.295
2137.329

8.031
27.951
270.959
2158.751

0.784
5.771
42.783
336.272

0.766
5.656
43.548
336.641

Table 3: Niederer benchmark. Computational times for SEM and SEM–NI, with an average
mesh size havg ranging from 0.48 mm to 0.06 mm and p = 1. Matrix–free solver.

Mesh points
number

Cells
number

Local
space

Linear solver
matrix–free [s]

Assembly phase
matrix–free [s]

Linear solver
matrix–based [s]

Assembly phase
matrix–based [s]

25’025
187’425
618’529
1’449’665

21’888
21’888
21’888
21’888

Q1
Q2
Q3
Q4

10.769
14.694
37.733
91.380

0.784
4.710
14.867
33.899

5.570
47.655
556.041
2796.981

17.150
176.375
919.298
3598.244

Table 4: Niederer benchmark. Computational times for matrix–free and matrix–based solvers,
SEM Qp with a ﬁxed average mesh size havg = 0.48 mm and p ranging from 1 to 4.

Mesh points
number

Cells
number

havg
[mm]

Linear solver
matrix–free [s]

Assembly phase
matrix–free [s]

Linear solver
matrix–based [s]

Assembly phase
matrix–based [s]

25’025
187’425
1’449’665
11’401’089

21’888
175’104
1’400’832
11’206’656

0.48
0.24
0.12
0.06

10.769
29.157
256.295
2137.329

0.784
5.771
42.783
336.272

5.570
11.331
138.514
1328.839

17.150
141.349
1089.666
8857.580

Table 5: Niederer benchmark. Computational times for matrix–free and matrix–based, SEM
Q1 with an average mesh size havg ranging from 0.48 mm to 0.06 mm.

Local space Q1

Q2

Q3

Q4

(CPU time)mb
(CPU time)mf ∼

2

7

∼

30

∼

50

∼

Table 6: Niederer benchmark. Speed–up of the matrix–free solver over the matrix–based one
when havg = 0.48 mm.

17

Figure 9: Niederer benchmark. Errors of action potential u and intracellular calcium concen-
tration Ca2+ versus CPU time required to solve all the linear systems in a slab of cardiac
tissue. Equal symbols identify the same number of mesh points: (
618’529, (cid:3) 1’449’665.

) 25’025,

187’425,

♦

×

◦

18

101102103CPUtime[s]10−1100errorminActionPotential101102103CPUtime[s]10−310−2errorminIntracellularCalciumConcentration101102103CPUtime[s]10−1100errormeanActionPotential101102103CPUtime[s]10−3errormeanIntracellularCalciumConcentration101102103CPUtime[s]10−1100errormaxActionPotential101102103CPUtime[s]10−410−3errormaxIntracellularCalciumConcentration101102103CPUtime[s]10−210−1errorPActionPotential101102103CPUtime[s]10−510−4errorPIntracellularCalciumConcentrationmatrix-freeSEMhmatrix-freeSEM-NIhmatrix-freeSEMpmatrix-freeSEM-NIpmatrix-basedSEMhmatrix-basedSEMphavg
(CPU time)mb
(CPU time)mf

0.48

0.24

0.12

0.06

2

∼

4

∼

4

∼

4

∼

Table 7: Niederer benchmark. Speed–up of the matrix–free solver over the matrix–based one
when p = 1.

Solver

Monodomain solver Monodomain assembly

Ionic model solver

Matrix–based (Q1, havg = 0.12 mm)
Matrix–free (Q4, havg = 0.48 mm)

16.61 %
14.95 %

56.98 %
3.36 %

26.41 %
81.69 %

Table 8: Niederer benchmark. Matrix–free and matrix–based percentages for the assembling
and solving phases. Note that in the matrix–free case we only need to assemble the right–
hand side vector, as there is no matrix. Moreover, these percentages are computed without
taking into account all other phases of the numerical simulation, such as mesh allocation, ﬁber
generation and output of the results.

Moreover, from Table 8 we observe that, in a matrix–based electrophysiologi-
cal simulation, most of the computational time is spent to solve the linear system
associated with the monodomain equation. On the contrary, in the matrix–free
solver most of the computational time is devoted to the ionic model. This means
that the cost for solving the linear system has been highly optimized.

Finally, we compare the performance of the AMG and GMG preconditioners,
used by the matrix–based and matrix–free solvers, respectively. In Tables 9 and
10 we show the average number of iterations required by the PCG method to
solve the linear system (6). We notice that, for the values of h and p considered
here, the GMG preconditioner appears optimal in the number of PCG iterations
versus both h and p. As a matter of fact, the average number of iterations is
about 1.8 for all hp conﬁgurations. On the contrary, the AMG preconditioner
is optimal versus the mesh size h (even if it requires a slightly larger number of
iterations than GMG), while it is not against the polynomial degree p. Indeed,
in this case the number of iterations is proportional to the polynomial degree.
All the numerical results shown in this section highlight how much advanta-
geous the matrix–free solver with SEM or SEM–NI is for cardiac electrophysiol-

Mesh points
number

Cells
number

Local
space

Matrix–free (SEM)
GMG preconditioner

Matrix–free (SEM–NI)
GMG preconditioner

Matrix–based (SEM)
AMG preconditioner

25’025
187’425
618’529
1’449’665

21’888
21’888
21’888
21’888

Q1
Q2
Q3
Q4

1.6362
1.8056
1.7906
1.7371

1.8126
1.8581
1.8161
1.7826

2.5777
5.9255
8.3363
10.5187

Table 9: Niederer benchmark. Average number of CG iterations for matrix–free (SEM, SEM–
NI) and matrix–based (SEM) solvers, Qp with a ﬁxed average mesh size havg = 0.48 mm and
p ranging from 1 to 4.

19

Mesh points
number

Cells
number

havg
[mm]

Matrix–free (SEM)
GMG preconditioner

Matrix–free (SEM–NI)
GMG preconditioner

Matrix–based (SEM)
AMG preconditioner

25’025
187’425
1’449’665
11’401’089

21’888
175’104
1’400’832
11’206’656

0.48
0.24
0.12
0.06

1.6362
1.5757
1.4448
1.4468

1.8126
1.6847
1.5937
1.4923

2.5777
3.0535
2.9400
3.2094

Table 10: Niederer benchmark. Average number of CG iterations for matrix–free (SEM,
SEM–NI) and matrix–based (SEM) solvers, Q1 with an average mesh size havg ranging from
0.48 mm to 0.06 mm.

Mesh points
number

159’149
1’172’919
3’879’415
9’116’741

Cells
number

139’684
139’684
139’684
139’684

Local
space

PCG iterations
GMG preconditioner

Q1
Q2
Q3
Q4

2.0770
1.9628
1.9455
1.9440

Table 11: Zygote left ventricle. Average number of PCG iterations for the matrix–free solver
with SEM discretization, Qp with a ﬁxed average mesh size havg = 2.0 mm and p ranging
from 1 to 4.

ogy simulations, with respect to the matrix–based solver with low–order FEM.
Since the matrix–free implementation outperforms the matrix–based one,
while SEM and SEM–NI provide comparable results in terms of accuracy and
eﬃciency, we will employ the matrix–free solver with just the SEM formulation
for the numerical simulations that we are going to present in the next sections.

5.2. Left ventricle

We report the results for the electrophysiological simulations performed with
the Zygote left ventricle geometry. The settings of this test case are resumed
at the beginning of Sect. 5. We consider a mesh with havg = 2.0 mm and
polynomial degree p from 1 to 4.

In Table 11 we report the number of mesh nodes, the number of cells and the
average number of iterations required by the PCG method to solve the linear
system (6). As for the Niederer benchmark, the GMG preconditioner turns out
to be optimal also for these numerical simulations. Indeed, the number of PCG
iterations is about 2 along the whole time history for any polynomial degree
between 1 and 4.

In Figure 10 we depict the activation maps for diﬀerent choices of the local
space (from Q1 to Q4). By looking at the isolines, we observe that the Q3
solution is very close to the Q4 solution, that means we reach convergence for
p = 3, even with such a relatively low mesh resolution havg = 2.0 mm. Whereas,
it is a well–established result in the literature that ﬁrst order Finite Elements
would reach convergence for a value of havg that is about 100 times smaller – i.e.
for a much higher number of DOFs (see, e.g., [14]). DOFs refer to the degrees
of freedom of the action potential, disregarding both gating variables and ionic
species, then it coincides with the number of mesh nodes.

20

159(cid:48)149 DOFs
(Q1)

1(cid:48)172(cid:48)919 DOFs
(Q2)

3(cid:48)879(cid:48)415 DOFs
(Q3)

9(cid:48)116(cid:48)741 DOFs
(Q4)

Figure 10: Zygote left ventricle. Three views of the activation maps computed with Qp
elements (p = 1, . . . , 4) and a ﬁxed average mesh size havg = 2.0 mm.

The same conclusions hold when considering Figure 11, where we show the
minimum, average, and maximum pointwise values of both the action potential
u and the intracellular calcium concentration Ca2+ over time.

5.3. Whole–heart

The aim of this section is to show that our matrix–free solver can be suc-
cessfully applied even in a complex framework. For this purpose we perform a
numerical simulation in sinus rhythm with the Zygote four–chamber heart. The
settings of this test case can be found at the beginning of Sect. 5. We consider
diﬀerent ionic models, namely CRN and TTP06 for atria and ventricles, re-
spectively. Furthermore, we model the valvular rings as non-conductive regions
of the myocardium. The mesh is endowed with 355’664 cells and 10’355’058
SEM, this
nodes (havg = 1.6 mm). We employ the matrix–free solver and Q3−
choice is motivated by the numerical results obtained for the Niederer bench-

21

Figure 11: Zygote left ventricle. Minimum (top), average (mid), maximum (bottom) pointwise
values of action potential u and intracellular calcium concentration Ca2+ over time, Qp with
a ﬁxed average mesh size havg = 2.0 mm and p ranging from 1 to 4.

22

0.00.10.20.30.40.50.6Time[s]−100−75−50−25025minΩV[mV]Actionpotential0.100.150.200.00.10.20.30.40.50.6Time[s]0.050.100.150.200.25minΩCa2+[µM]Intracellularcalciumconcentration0.100.150.200.00.10.20.30.40.50.6Time[s]−75−50−25025meanΩV[mV]Actionpotential0.100.150.200.00.10.20.30.40.50.6Time[s]0.10.20.3meanΩCa2+[µM]Intracellularcalciumconcentration0.100.150.200.00.10.20.30.40.50.6Time[s]−50050maxΩV[mV]Actionpotential0.100.150.200.00.10.20.30.40.50.6Time[s]0.10.20.30.4maxΩCa2+[µM]Intracellularcalciumconcentration0.100.150.20SEM(Q1,havg=2mm)SEM(Q2,havg=2mm)SEM(Q3,havg=2mm)SEM(Q4,havg=2mm)t=0.1 s

t=0.2 s

t=0.3 s

t=0.4 s

t=0.5 s

t=0.6 s

t=0.7 s

t=0.8 s

Figure 12: Zygote whole–heart. Time evolution of the transmembrane potential u during one
heartbeat.

mark (Sect. 5.1) and the convergence test performed on the Zygote left ventricle
geometry (Sect. 5.2).

We depict in Figure 12 the evolution of the transmembrane potential over
time on the whole–heart geometry. The electric signal initiates at the sinoatrial
node in the right atrium and then propagates to the left atrium and ventricles by
means of preferential conduction lines, such as the Bachmann’s and His bundles
[60]. The wavefront propagation appears very smooth, while accounting for
small dissipation and dispersion throughout the heartbeat [22].

6. Conclusions

We developed a matrix–free solver for cardiac electrophysiology tailored to
the eﬃcient use of high–order numerical methods. We employed the mon-
odomain equation to model the propagation of the transmembrane potential
and physiologically–based ionic models (CRN and TTP06) to describe the be-
haviour of diﬀerent chemical species at the cells level.

We run several electrophysiological simulations for three diﬀerent test cases,
namely a slab of cardiac tissue, the Zygote left ventricle and the Zygote whole–
heart to demonstrate the eﬀectiveness and generality of our matrix–free solver
in combination with Spectral Element Methods. SEM and SEM–NI provided
comparable numerical results in terms of both accuracy and eﬃciency. Further-

23

more, we showed the importance of considering high–order Finite Elements for
this class of mathematical problems, i.e. when sharp wavefronts are involved.

Our matrix–free solver outperforms state–of–the–art matrix–based solvers in
terms of computational costs and memory requirements. This is true even when
matrix–vector products are computed without any matrix–free preconditioner,
thanks to both vectorization and sum–factorization. Finally, the small mem-
ory usage of the matrix–free implementation may allow for the development of
GPU–based solvers of the cardiac function.

Acknowledgments

This research has been funded partly by the European Research Council
(ERC) under the European Union’s Horizon 2020 research and innovation pro-
gramme (grant agreement No. 740132, iHEART “An Integrated Heart Model
for the simulation of the cardiac function”, P.I. Prof. A. Quarteroni) and partly
by the Italian Ministry of University and Research (MIUR) within the PRIN
(Research projects of relevant national interest 2017 “Modeling the heart across
the scales: from cardiac cells to the whole organ” Grant Registration number
2017AXL54F).

Authors’ contribution

All authors contributed to the study conception and design of the work. PCA
and MS developed the computer code, performed the numerical simulations
and post-processed the numerical results. PG conceptualized the goals and
the theoretical background of the research and took the lead in designing the
experiments and interpreting the results. LD contributed to the analysis of
results and coordinated the research activity planning. The initial draft of this
manuscript was written by PCA, MS and PG. All authors read and approved
this manuscript.

References

[1] T. Gerach, S. Schuler, J. Fr¨ohlich, L. Lindner, , E. Kovacheva, R. Moss,
E. W¨ulfers, G. Seemann, C. Wieners, A. Loewe, Electro-Mechanical Whole-
Heart Digital Twins: A Fully Coupled Multi-Physics Approach, Mathemat-
ics 9 (11).

[2] R. Gray, P. Pathmanathan, Patient-Speciﬁc Cardiovascular Computational
Modeling: Diversity of Personalization and Challenges, Journal of Cardio-
vascular Translational Research 11 (2018) 80–88.

[3] R. Piersanti, F. Regazzoni, M. Salvador, A. Corno, L. Dede’, C. Vergara,
A. Quarteroni, 3D-0D closed-loop model for the simulation of cardiac biven-
tricular electromechanics, Computer Methods in Applied Mechanics and
Engineering 391 (2022) 114607.

24

[4] M. Potse, D. Krause, W. Kroon, R. Murzilli, S. Muzzarelli, F. Regoli,
E. Caiani, F. Prinzen, R. Krause, A. Auricchio, Patient-speciﬁc modelling
of cardiac electrophysiology in heart-failure patients, Europace 16 (2014)
v56–iv61.

[5] M. Strocchi, C. Augustin, M. Gsell, E. Karabelas, A. Neic, K. Gillette,
O. Razeghi, A. Prassl, E. Vigmond, J. Behar, J. Gould, B. Sidhu, C. Ri-
naldi, M. Bishop, G. Plank, S. Niederer, A publicly available virtual cohort
of four-chamber heart meshes for cardiac electro-mechanics simulations,
PLOS ONE 15 (2020) 1–26.

[6] A. Quarteroni, L. Ded`e, A. Manzoni, C. Vergara, Mathematical Modelling
of the Human Cardiovascular System: Data, Numerical Approximation,
Clinical Applications, Cambridge University Press, 2019.

[7] N. A. Trayanova, R. Winslow, Whole-Heart Modeling: Applications to Car-
diac Electrophysiology and Electromechanics, Circulation Research 108 (1)
(2011) 113–128.

[8] P. Colli Franzone, L. Pavarino, S. Scacchi, Mathematical cardiac electro-

physiology, Vol. 13, Springer, 2014.

[9] H. Arevalo, F. Vadakkumpadan, E. Guallar, A. Jebb, P. Malamas, K. Wu,
N. Trayanova, Arrhythmia risk stratiﬁcation of patients after myocardial in-
farction using personalized heart models, Nature Communications 7 (2016)
11437.

[10] J. Bayer, V. Sobota, A. Moreno, P. Jais, E. Vigmond, The Purkinje network
plays a major role in low-energy ventricular deﬁbrillation, Computers in
Biology and Medicine 141 (2022) 105133.

[11] K. Gillette, M. Gsell, A. Prassl, E. Karabelas, U. Reiter, G. Reiter,
T. Grandits, C. Payer, D. ˇStern, M. Urschler, J. Bayer, C. Augustin,
A. Neic, T. Pock, E. Vigmond, G. Plank, A Framework for the generation
of digital twins of cardiac electrophysiology from clinical 12-leads ECGs,
Medical Image Analysis 71 (2021) 102080.

[12] C. Mendonca Costa, A. Neic, E. Kerfoot, B. Porter, B. Sieniewicz, J. Gould,
B. Sidhu, Z. Chen, G. Plank, C. Rinaldi, M. Bishop, S. Niederer, Pacing in
proximity to scar during cardiac resynchronization therapy increases local
dispersion of repolarization and susceptibility to ventricular arrhythmoge-
nesis, Heart Rhythm 16 (10) (2019) 1475–1483.

[13] A. Quarteroni, A. Valli, Numerical Approximation of Partial Diﬀerential

Equations, Springer Verlag, Heidelberg, 1994.

[14] L. Woodworth, B. Cansız, M. Kaliske, A numerical study on the eﬀects
of spatial and temporal discretization in cardiac electrophysiology, Inter-
national Journal for Numerical Methods in Biomedical Engineering 37 (5)
(2021) e3443.

25

[15] A. Patera, A spectral element method for ﬂuid dynamics: laminar ﬂow in
a channel expansion, Journal of Computational Physics 54 (1984) 468–488.

[16] Y. Maday, A. Patera, Spectral element methods for the incompressible
Navier-Stokes equations, in: State-of-the-Art Surveys on Computational
Mechanics, A.K. Noor and J. T. Oden, 1989, pp. 71–143.

[17] C. Canuto, M. Hussaini, A. Quarteroni, T. Zang, Spectral Methods. Evolu-
tion to Complex Geometries and Applications to Fluid Dynamics, Springer,
Heidelberg, 2007.

[18] D. Arnold, F. Brezzi, B. Cockburn, L. Marini, Uniﬁed analysis of discontin-
uous Galerkin methods for elliptic problems, SIAM Journal on Numerical
Analysis 39 (5) (2001) 1749–1779.

[19] B. Cockburn, C.-W. Shu, The local discontinuous galerkin method for time-
dependent convection-diﬀusion systems, SIAM Journal on Numerical Anal-
ysis 35 (6) (1998) 2440–2463.

[20] R. LeVeque, Finite volume methods for hyperbolic problems, Cambridge
Texts in Applied Mathematics, Cambridge University Press, Cambridge,
2002.

[21] J. A. Cottrell, T. J. R. Hughes, Y. Bazilevs, Isogeometric Analysis: Toward

Integration of CAD and FEA, Wiley, 2009.

[22] M. Bucelli, M. Salvador, L. Ded`e, A. Quarteroni, Multipatch Isogeometric
Analysis for electrophysiology: Simulation in a human heart, Computer
Methods in Applied Mechanics and Engineering 376 (2021) 113666.

[23] C. Cantwell, S. Yakovlev, R. Kirby, N. Peters, S. Sherwin, High-order spec-
tral/hp element discretisation for reaction–diﬀusion problems on surfaces:
Application to cardiac electrophysiology, Journal of Computational Physics
257 (2014) 813–829.

[24] Y. Coudi`ere, R. Turpault, Very high order ﬁnite volume methods for car-
diac electrophysiology, Computers & Mathematics with Applications 74 (4)
(2017) 684–700.

[25] J. Hoermann, C. Bertoglio, M. Kronbichler, M. Pfaller, R. Chabiniok,
W. Wall, An adaptive hybridizable discontinuous Galerkin approach for
cardiac electrophysiology, International Journal for Numerical Methods in
Biomedical Engineering 34 (5).

[26] K. Vincent, M. Gonzales, A. Gillette, C. Villongco, S. Pezzuto, J. Omens,
M. Holst, A. McCulloch, High-order ﬁnite element methods for cardiac
monodomain simulations, Frontiers in Physiology 6 (Aug).

26

[27] D. Arndt, N. Fehn, G. Kanschat, K. Kormann, M. Kronbichler, P. Munch,
W. Wall, J. Witte, ExaDG: High-Order Discontinuous Galerkin for the
Exa-Scale, in: Software for Exascale Computing - SPPEXA 2016-2019,
Springer International Publishing, Cham, 2020, pp. 189–224.

[28] S. Orszag, Spectral methods for problem in complex geometries, Journal of

Computational Physics 37 (1980) 70–92.

[29] J. Melenk, K. Gerdes, C. Schwab, Fully discrete hp

ﬁnite elements: Fast
quadrature, Computer Methods in Applied Mechanics and Engineering
190 (32-33) (2001) 4339 – 4364.

−

[30] M. Kronbichler, K. Kormann, A generic interface for parallel cell-based
ﬁnite element operator application, Computers and Fluids 63 (2012) 135–
147.

[31] Y. Xia, K. Wang, H. Zhang, Parallel Optimization of 3D Cardiac Electro-
physiological Model Using GPU, Computational and Mathematical Meth-
ods in Medicine 2015 (2015) 862735.

[32] M. Kronbichler, K. Ljungkvist, Multigrid for Matrix-Free High-Order Fi-
nite Element Computations on Graphics Processors, ACM Transactions on
Parallel Computing 6 (1) (2019) 1–32.

[33] G. Del Corso, R. Verzicco, F. Viola, A fast computational model for the
electrophysiology of the whole human heart, Journal of Computational
Physics 457 (2022) 111084.

[34] S. A. Niederer, E. Kerfoot, A. P. Benson, M. O. Bernabeu, O. Bernus,
C. Bradley, E. M. Cherry, R. Clayton, F. H. Fenton, A. Garny, E. Heiden-
reich, S. Land, M. Maleckar, P. Pathmanathan, G. Plank, J. F. Rodr´ıguez,
I. Roy, F. B. Sachse, G. Seemann, O. Skavhaug, N. Smith, Veriﬁcation of
cardiac tissue electrophysiology simulators using an N-version benchmark,
Philosophical Transactions of the Royal Society A: Mathematical, Physical
and Engineering Sciences 369 (1954) (2011) 4331–51.

[35] D. Arndt, W. Bangerth, B. Blais, T. Clevenger, M. Fehling, A. Grayver,
T. Heister, L. Heltai, M. Kronbichler, M. Maier, P. Munch, J. Pelteret,
R. Rastak, I. Tomas, B. Turcksin, Z. Wang, D. Wells, The deal.II library,
Version 9.2, Journal of Numerical Mathematics 28 (3) (2020) 131–146.

[36] K. ten Tusscher, A. Panﬁlov, Alternans and spiral breakup in a human
ventricular tissue model, American Journal of Physiology Heart and Cir-
culation Physiology 291 (2006) 1088–1100.

[37] M. Courtemanche, R. Ramirez, S. Nattel, Ionic mechanisms underlying
human atrial action potential properties:
insights from a mathematical
model, American Journal of Physiology Heart and Circulation Physiology
275 (1) (1998) H301–H321.

27

[38] R. Piersanti, P. Africa, M. Fedele, C. Vergara, L. Ded`e, A. Corno, A. Quar-
teroni, Modeling cardiac muscle ﬁbers in ventricular and atrial electrophys-
iology simulations, Computer Methods in Applied Mechanics and Engineer-
ing 373 (2021) 113468.

[39] Zygote Media Group Inc., Zygote Solid 3D heart Generation II, Develop-

ment Report (2014).

[40] C. Bernardi, Y. Maday, Spectral, spectral element and mortar element
in: Theory and numerics of diﬀerential equations (Durham,

methods,
2000), Universitext, Springer, Berlin, 2001, pp. 1–57.

[41] G. Karniadakis, S. Sherwin, Spectral/hp Element Methods for Computa-

tional Fluid Dynamics, Oxford University Press, 2005, 2nd ed.

[42] C. Canuto, M. Hussaini, A. Quarteroni, T. Zang, Spectral Methods. Fun-

damentals in Single Domains, Springer, Heidelberg, 2006.

[43] B. Szab´o, I. Babuˇska, Finite Element Analysis, John Wiley & sons, New

York, 1991.

[44] C. Schwab, p
Oxford, 1998.

−

and hp

−

ﬁnite element methods, Oxford University Press,

[45] N. Fehn, W. Wall, M. Kronbichler, A matrix-free high-order discontinuous
Galerkin compressible Navier-Stokes solver: A performance comparison of
compressible and incompressible formulations for turbulent incompressible
ﬂows, International Journal for Numerical Methods in Fluids 89 (3) (2019)
71–102.

[46] F. Regazzoni, M. Salvador, P. Africa, M. Fedele, L. Dede’, A. Quarteroni, A
cardiac electromechanical model coupled with a lumped-parameter model
for closed-loop blood circulation, Journal of Computational Physics 457
(2022) 111083.

[47] A. Quarteroni, T. Lassila, S. Rossi, R. Ruiz-Baier, Integrated Heart-
Coupling multiscale and multiphysics models for the simulation of the car-
diac function, Computer Methods in Applied Mechanics and Engineering
314 (2017) 345–407.

[48] P. Gervasio, F. Saleri, A. Veneziani, Algebraic fractional step schemes with
spectral methods for the incompressible Navier-Stokes equations, Journal
of Computational Physics 214 (1) (2006) 347–365.

[49] J. M. Cebrian, L. Natvig, M. Jahre, Scalability analysis of AVX-512 exten-

sions, The Journal of Supercomputing 76 (3) (2020) 2082–2097.

[50] D. Zhong, Q. Cao, G. Bosilca, J. Dongarra, Using long vector extensions

for MPI reductions, Parallel Computing 109 (2022) 102871.

28

[51] M. Kronbichler, K. Kormann, Fast matrix-free evaluation of discontinu-
ous Galerkin ﬁnite element operators, ACM Transactions on Mathematical
Software 45 (3) (2019) 1–40.

[52] C. Cantwell, S. Sherwin, R. Kirby, P. Kelly, From h to p eﬃciently: Strategy
selection for operator evaluation on hexahedral and tetrahedral elements,
Computers and Fluids 43 (1) (2011) 23 – 28.

[53] B. Janssen, G. Kanschat, Adaptive Multilevel Methods with Local Smooth-
Conforming High Order Finite Element Methods,

ing for H 1
SIAM Journal on Scientiﬁc Computing 33 (4) (2011) 2095–2114.

and H curl

−

−

[54] J. Xu, L. Zikatanov, Algebraic multigrid methods, Acta Numerica 26 (2017)

591 – 721.

[55] H. Sundar, G. Stadler, G. Biros, Comparison of multigrid algorithms for
high-order continuous ﬁnite element discretizations, Numerical Linear Al-
gebra with Applications 22.

[56] U. Trottenberg, C. Oosterlee, A. Sch¨uller, Multigrid, Elsevier Academic

Press, London, UK, 2001.

[57] T. Clevenger, T. Heister, G. Kanschat, M. Kronbichler, A Flexible, Parallel,
Adaptive Geometric Multigrid Method for FEM, ACM Transactions on
Mathematical Software 47 (1).

[58] M. Adams, M. Brezina, J. Hu, R. Tuminaro, Parallel multigrid smoothing:
Polynomial versus Gauss-Seidel, Journal of Computational Physics 188 (2)
(2003) 593 – 610.

[59] P. Africa, R. Piersanti, M. Fedele, L. Ded`e, A. Quarteroni, lifex – heart
module: a high-performance simulator for the cardiac function. Package 1:
Fiber generation, arXiv preprint arXiv:2108.01907.

[60] R. Harrington, J. Narula, Z. Eapen, Hurst’s the Heart, MacGraw-Hill, 2011.

29

