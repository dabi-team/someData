MCTensor: A High-Precision Deep Learning Library
with Multi-Component Floating-Point

2
2
0
2

g
u
A
9
2

]

G
L
.
s
c
[

2
v
7
6
8
8
0
.
7
0
2
2
:
v
i
X
r
a

Tao Yu * 1 Wentao Guo * 1 Jianan Canal Li * 1 2 Tiancheng Yuan * 3 Christopher De Sa 1

Abstract

In this paper, we introduce MCTensor, a library
based on PyTorch for providing general-purpose
and high-precision arithmetic for DL training.
MCTensor is used in the same way as PyTorch
Tensor: we implement multiple basic, matrix-
level computation operators and NN modules for
MCTensor with identical PyTorch interface. Our
algorithms achieve high precision computation
and also beneﬁts from heavily-optimized PyTorch
ﬂoating-point arithmetic. We evaluate MCTensor
arithmetic against PyTorch native arithmetic for a
series of tasks, where models using MCTensor in
ﬂoat16 would match or outperform the PyTorch
model with ﬂoat32 or ﬂoat64 precision.

1. Introduction

High precision computations are of interest in many areas.
For example, an emerging trend in studying dynamical sys-
tems is to use Taylor methods with high-precision arithmetic
(Bailey & Borwein, 2015), and delaunay triangulation in
computational geometry (Schirra, 1998). Recently, high pre-
cision computations are even desired for some deep learning
tasks, e.g. hyperbolic deep learning, so as to use hyperbolic
space stably in practice (Yu & De Sa, 2019; 2021).

As of now, most high precision algorithms can be divided in
two categories: (1) the standard multiple-digit “BigFloat"
format using a sequence of digits coupled with a single
exponent term; (2) the multiple-component ﬂoating-point
format (MCF) using an unevaluated sum of multiple ordi-
nary ﬂoating-point numbers (e.g. ﬂoat16, ﬂoat32). Some
examples of the ﬁrst approach include the GNU Multiple
Precision (GMP) Arithmetic Library (Granlund & the GMP

*Equal contribution

1Department of Computer Science,
2Biological Engineering, 3and System Engineering, Cornell Uni-
versity. Correspondence to: Tao Yu <tyu@cs.cornell.edu>,
Wentao Guo <wg247@cornell.edu>,
Jianan Canal Li
<jl3789@cornell.edu>, Tiancheng Yuan <ty373@cornell.edu>.

Workshop on Hardware Aware Efﬁcient Training. The 39 th Inter-
national Conference on Machine Learning, Baltimore, Maryland,
USA, 2022. Copyright 2022 by the author(s).

development team, 2012) and Julia’s BigFloat type (Bezan-
son et al., 2017). The idea of multiple-component ﬂoat
approach (also referred as “expansion" in some literature),
dates back to priest’s works (Priest, 1991), and some ex-
ample implementations include multi-component library on
C++ (Hida et al., 2007) and MATLAB (Jiang et al., 2016).
The multiple-digit approach can represent compactly a much
larger range of numbers, whereas the multiple-component
approach still adopts the limited precision ﬂoats to achieve
high precision computations with error guarantees (Jolde¸s
et al., 2015). However, the multiple-component approach
has an advantage in speed over the multiple-digit approach,
as it uses standard ﬂoats such as ﬂoat16 and ﬂoat32 and
hence better takes advantage of existing ﬂoating-point ac-
celerators (Hida et al., 2007; Richard Shewchuk, 1997).

In the emerging deep learning area, there is a demand for
high precision computations in some deep learning appli-
cations (e.g. hyperbolic deep learning (Nickel & Kiela,
2017; Liu et al., 2019)). Currently, there is no such deep
learning library for efﬁcient high precision computations.
Popular deep learning frameworks such as PyTorch (Paszke
et al., 2019), TensorFlow (Abadi et al., 2015) and Caffe (Jia
et al., 2014) only supports training with standard ﬂoats (e.g.
ﬂoat16, ﬂoat32 and ﬂoat64). Though high precision compu-
tations are enabled in Julia using multiple-digit “BigFloat",
deep learning libraries built on top of Julia such as Flux
(Innes et al., 2018; Innes, 2018), MXNet.jl (Chen et al.,
2015), and TensorFlow.jl (Malmaud & White, 2018) do not
support training with BigFloat. Furthermore, BigFloat in Ju-
lia can only exist on CPUs, but not on GPUs, which greatly
limits its usages.

In this work, we develop a multiple-component ﬂoating-
point library, MCTensor, for general-purpose, fast, and high-
precision deep learning. We build MCTensor on top of
PyTorch and it can be used in the same way as the PyTorch
Tensor object. We hope MCTensor library can beneﬁt ap-
plications in the following areas: (1) high precision training
with low precision numbers. Training with low precision
numbers is an emerging deep learning area on tasks like
mobile computer vision (Tulloch & Jia, 2017) and leverag-
ing hardware accelerator like Google TPU (Jouppi et al.,
2017), where the deep learning model computes with low

 
 
 
 
 
 
MCTensor: A High-Precision Deep Learning Library

precision numbers such as ﬂoat8 and ﬂoat16. A large quan-
tization error using low precision arithmetic would affect
the convergence (Wu et al., 2018) and may degrade the per-
formance of the model; (2) numerical accurate and stable
hyperbolic deep learning, where the non-Euclidean hyper-
bolic space is used in place of Euclidean space for many
deep learning tasks and models due to its non-Euclidean
properties. For example, graph embedding (Nickel & Kiela,
2017; 2018) and many developed hyperbolic networks, in-
cluding hyperbolic neural networks (Ganea et al., 2018) and
hyperbolic GCN (Chami et al., 2019; Yu & De Sa, 2022).
However, the numerical error of computing with standard
ﬂoating-point numbers in hyperbolic space is unbounded,
even with ﬂoat64, characterized as the “NaN" problem (Sala
et al., 2018; Yu & De Sa, 2019). A high precision compu-
tation in the hyperbolic space suggested using MCF (Yu &
De Sa, 2021) would be helpful. Our main contributions are
as follows:

• We implement MCTensor in the same way as PyTorch
Tensor with corresponding basic and matrix-level opera-
tions using MCF.

• We enable learning with MCTensor by developing the
MCModule layers, MCOptimizers and etc with the same
programming interface as PyTorch’s counterparts.

• We demonstrate the performance of MCTensor for both
high precision training with low precision numbers and
on some hyperbolic tasks.

2. Methodologies

x = (x0, x1, · · · , xnc−1) = x0 + x1 + ... + xnc−1

Here we introduce some basics of our MCTensor library,
built on top of PyTorch (Paszke et al., 2019) 1 that employs
multi-component ﬂoating-point as its underlying tensor rep-
resentation. Each MCTensor x is represented as an expan-
sion, an unevaluated sum of multiple tensors as follows:
(1)
where each xi, as a component of x, can be a PyTorch
ﬂoating-point Tensor in any precision, and nc is the number
of components for MCTensor x. It’s required that all com-
ponents to be ordered in a decreasing magnitude (with x0
being the largest and xnc−1 being the smallest). In this way,
MCTensor allows roundoff error to be propagated to the
later components and thus offers better precision compared
to a standard PyTorch Tensor2.

implement basic operators add, subtract,
We ﬁrst
. . . for MCTensor with MCF arith-
multiply, divide,
metic and further vectorize them to matrix-level opera-
tors dot, mm, . . . with same semantics as their PyTorch
counterparts. These operators then allow us to implement

1Our PyTorch version is 1.11.0
2unless otherwise speciﬁed, the PyTorch tensor, or “Tensor", is
referred to a PyTorch tensor with an arbitrary ﬂoating point data
type in this paper

higher-level MCModule, MCOptim as the counterpart for
torch.nn.Module and torch.optim so that we can use them
for any deep learning applications.

2.1. MCTensor Object with Basic Operators

[MCTensor object] A MCTensor x can be abstracted as
an object with speciﬁcation x{fc, tensor, nc}. Specif-
ically, x.tensor has nc components of PyTorch tensors
x0, x1, · · · , xnc−1 in the last dimension, and it has shape
as (∗x0.shape, nc). The x.f c data term in MCTensor is a
view of x0, keeps track of the gradient for x, and if needed,
serves as an approximate tensor representation of x.

∂f

∂(x0+x1+···+xnc−1) = ∂f

[Gradient] Because a MCTensor is an unevaluated sum
of Tensors, then the gradient of a function f w.r.t. x is
∂f
∂x =
, which is same as the the
gradient of f w.r.t. any component xi. So we only keep
track of the gradient information of x.fc for a MCTensor x,
which can then be computed naturally by PyTorch’s auto-
differentiation engine to get x.fc.grad as x.grad.

∂xi

In order to develop more advanced operations of MCTen-
sor, we develop two most basic functions ﬁrst, Two−Sum and
Two−Prod, whose inputs are two PyTorch Tensors and re-
turns the result as a MCTensor with 2 components in the
form of (result, error).

[Basic Operators] We develop binary MCTensor operators
to support input types (1) a MCTensor and a Tensor, (2) a
Tensor and a MCTensor, and (3) a MCTensor and a MCTen-
sor. For unary operators, we only accept a MCTensor as
an input. The output for all these operators is a MCTen-
sor with the same nc as the input(s). We implement MCF
algorithms for basic arithmetic, with the full list of their
algorithm statements available in the appendix:

• MCTensor: Exp−MCN (exp), Square−MCN (square)
• MCTensor and Tensor: Grow−ExpN (add), ScalingN

(multiply), DivN (divide)

• MCTensor and MCTensor: Add−MCN (add), Div−MCN

(divide), Mul−MCN (multiply)

For example, the addition between a MCTensor and a Ten-
sor, or Grow−ExpN (Grow Expansion with Normalization),
is given in Alg. 1.

Algorithm 1 Grow-ExpN

Input: nc-MCTensor x, PyTorch Tensor v
initialize Q ← v
for i = 1 to nc do
k ← nc + 1 − i
(Q, hk) ← Two-Sum(xk−1, Q)

end for
h ← (Q, h1, · · · , hnc)
Return: Simple-Renorm(h, nc)

MCTensor: A High-Precision Deep Learning Library

tails of them are provided in the appendix.

For matrix operators, we leverage the broadcastability and
vectorization from native PyTorch operations embedded in
our basic MCF operators across each nc, and then apply
sequential error propagation. For example, for MV−MCN with
input as x and v, we ﬁrst use ScalingN to compute the
broadcasted product of x and v for each nc, and then we
sequentially sum up the results and propagate errors with
the Add−MCN operator. In this way, we can make our multi-
plication part (ScalingN) independent of the input size and
only employ for-loop for addition part (Add−MCN) since error
propagation in the addition is sequential by the algorithm.
Theoretically, accurate addition of N MCTensors is of order
at least O(nc · log N ).

MCTensor operators will be slower because of the need to
propagate errors in computation, and have more memory
burden than PyTorch operators because of the nature of
MCF representation.
In Table 1, we can see a tradeoff
between program speed and precision. However, we would
like to note that there is still much space to optimize these
algorithms for better timing in practice. This work aims
to provide ML community with the possibility to do high-
precision computations for learning with MCTensor over
GPUs. More details can be found in A.3.

2.3. MCModule, MCActivation, and MCOptim

We enable learning with MCTensor by developing neu-
ral network basic modules (MCModule), activation func-
tions (MCActivation) and optimizers (MCOptim), in the
exact programming interfaces as their PyTorch coun-
terparts in (torch.nn.Module, torch.nn.functional and
torch.optim). The semantics are identical to that of Py-
Torch’s module and optimizer, except that we are using
MCTensor arithmetic. Speciﬁcally, we give some examples:

• MCModule: MCLinear, MCEmbedding, MCSequential
• MCOptim: MC−SGD, MCAdam
• MCActivation: MCSoftmax, MCReLU, MC−GELU

Here Fig.2 is the MCTensor implementation of linear layer,
where weights and biases are represented by MCTensors.
Similar to its PyTorch peer, a MCLinear takes input of size of
input shape, in_features, size of output shape, out_features,
learn with or without biases (boolean: bias) and number of
component nc for setting up the underlying MCTensors. To
handle the operations within MCLinear layer, we override
PyTorch’s nn.functional.linear to perform matrix level
multiplication (Algo. 22) between a MCTensor weight and
the Tensor input, then the addition of the product with bias
(Algo.9) if needed. The output of the MCLinear layer is a
MCTensor with the same nc.

Since we implemented MCModule, MCActivation, and
MCOptimizer to follow the same speciﬁcation as their Py-

Figure 1. Relative Errors for Multiplication Mul−MCN(x,y), Com-
pared with High Precision Julia BigFloat (3000 bits precision).
Order of magnitudes for x and y are the same.

Grow−ExpN takes a nc-component MCTensor x and a normal
Tensor v as input. The approximated result Q of this addi-
tion, is initialized to v. The Grow Expansion happens ﬁrst
in the for loop, starting with the last component of x, xnc−1,
to the ﬁrst component x0. The algorithm Two−Sum sums Q
and a component xk−1, resulting in the updated approxima-
tion Q and the error term hk. The resulting h is therefore
grown into nc + 1 components, which are naturally ordered
in a decreasing manner, except that there could be some
intermediate zero components. Hence, in order to meet the
requirements of a MCTensor, we use Simple−Renorm algo-
rithm to move zeros backwards and output a nc components
MCTensor.

We also implement two different algorithms for Mul−MCN:
a fast version that uses two Div−MCN operations to perform
multiplication, and a slower version that have better error
bounds. However, in practice we see little difference be-
tween them, so we run all our experiments with the fast
version. Details of these basic MCF algorithms are provided
in the appendix.

To demonstrate how a MCTensor’s precision increases with
the number of components nc, we plot the relative numerical
errors for MCTensor with different nc and PyTorch Tensor.
We set the data type for both MCTensors and Tensors to be
Float32, and compute the errors w.r.t. the results derived
by high precision Julia BigFloat with number of bits of
the signiﬁcand set to 3000. As can be seen from Figure 1,
even with nc = 2, the relative error is orders of magnitude
smaller than PyTorch Tensors.

2.2. MCTensor Matrix Operators

After deﬁning the basic MCF arithmetic, we are able
to implement commonly used matrix level operators for
MCTensor including AddMM−MCN (torch.addmm), Dot−MCN
(torch.dot),
MM−MCN (torch.mm),
BMM−MCN (torch.bmm) and Matmul−MCN (torch.matmul). De-

MV−MCN (torch.mv),

MCTensor: A High-Precision Deep Learning Library

class MCLinear ( MCModule ):

def __init__ (self , in_features , out_features , nc , bias=True ):

super (MCLinear , self ). __init__ ()
self. in_features = in_features
self. out_features = out_features
self.nc = nc
self. weight = MCTensor ( out_features , in_features , nc=nc , requires_grad =True)
if bias:

self.bias = MCTensor ( out_features , nc=nc , requires_grad =True)

else:

self.bias = None

def forward (self , input ):

return F. linear (input , self.weight , self.bias)

Figure 2. An Example for Implementing MCLinear

Operators

Inputs sizes

FloatTensor

1-MCTensor

2-MCTensor

3-MCTensor

Dot-MCN
MV-MCN
Matmul-MCN ( 500× 200), ( 200× 50)

5000, 5000
(5000 × 500), 500

1.61µs ± 3.29ns
157µs ± 4.32µs
97.3µs ± 1.1µs

442µs ± 5.61µs
320ms ± 5.78ms
495ms ± 10.8ms

656µs ± 1.16µs
460ms ± 10.7ms
735ms ± 21.7ms

858µs ± 12.2µs
580ms ± 12.1ms
934ms ± 28ms

Table 1. MCTensor Matrix Operators Running Time (mean ± sd)

Torch counterparts, building a MCTensor model is the same
way as one would build a PyTorch module. The only dif-
ference to the users is the need to specify the number of
components nc. An demonstration of our MCTensor model
optimization programming paradigm can be found in Fig. 3.

mc_model = MCModel (nc=nc)
mc_optimizer = MCSGD ( mc_model . parameters ())

for x, y in train_dataset :

mc_optimizer . zero_grad ()
y_hat = mc_model (x)
loss = loss_fn (y_hat , y)
loss. backward ()
mc_optimizer .step ()

Figure 3. MCTensor model optimization programming paradigm

2.4. Error Analysis

Principally, one can achieve arbitrary precision using
MCTensor by simply increasing the number of components,
however, note that each component of a MCTensor is a
standard ﬂoat, which has a natural range. Take Float16
for example, the minimum representable strictly positive
value is 2−24 ≈ 5.96 × 10−8, i.e., the smallest error that
can be captured by MCFloat16 is 2−24. Hence in practice,
2-MCFloat16 is usually sufﬁcient and similar performances
are observed when more components of MCFloat16 were
adopted. While simply adding an appropriate scale factor
2−k (depending on the precision requirements) to smaller
components can help capture even smaller errors, it is out
of scope of this paper.

To validate improved precision of MCTensor models, we
consider the linear regression task since it is possible to
obtain loss arbitrarily close to zero. We use a single
MCLinear without bias term and the Mean Squared Error
(MSE) loss on fully observed synthetic data: L(W ) =

Figure 4. Loss Curves on Linear Regression Task.

MSELoss(y, XW T ), where X is a (10000×2) matrix with
each entries sampled from x ∼ N (−0.5, 0.52), and y is cal-
culated from y = XW ∗T where W ∗ is the (10000 × 1)
target weight sampled from w∗ ∼ N (−0.5, 0.52). We use
gradient descent with lr = 0.05 for optimization.

In Figure 4, we plot the training loss curves for the model
with the same structure and initialization, but with MCTen-
sor or Tensor as data structure. The comprehensive results
can be seen in Table 6. The ﬁnal train loss for 2-MCFloat16
is orders of magnitudes smaller than Float16.

Since by just using 2-MCFloat16, we can achieve much
better precision than Float16 Tensor (HalfTensor), we run
all following experiments in Float16 with nc =2 or 3 expect
for Hyperbolic MCEmbedding.

3. Experiments

A MCTensor is able to achieve improved precision com-
pared with a PyTorch Tensor with the same data type. If
higher precision implies better results under the same exper-

MCTensor: A High-Precision Deep Learning Library

Figure 5. Training Losses for Logistic Regression Models on the
Breast Cancer Dataset

iment settings, we would expect our n-MCTensor (n ≥ 2)
model can achieve better performance than a native PyTorch
model with same precision. We demonstrate the increasing
precision under the same data type for MCTensor by car-
rying out experiments in two settings: (1) high precision
learning with low precision numbers, this include logis-
tic regression model and multi-layer perceptrons (MLP) in
Section 3.1; and (2) high precision hyperbolic embedding
reconstruction task in Section 3.2. For all these experiments,
MCTensor models use the same initialization as the Tensor
models, and use MCModule as its PyTorch’s nn.Module coun-
terpart, with MCTensors as layer weights. Our code and
models are open-source and available at github 3.

3.1. High-precision Computations with Low Precision

Low precision machine learning employs models that com-
pute with low precision numbers (e.g. ﬂoat8 and ﬂoat16),
which become popular in many edge-applications (Hubara
et al., 2017; Zhang et al., 2019). However, the quantization
error inherent with low precision arithmetic could affect the
convergence and performance of the model (Wu et al., 2018;
De Sa et al., 2018). As a matter of fact, most current low
precision learning frameworks including (Courbariaux et al.,
2014; De Sa et al., 2018) adopt high precision numbers dur-
ing gradient computations and optimizations, but pursue a
better way to convert the results to low-precision numbers
with less quantization errors. In comparison, with MCTen-
sor, one can get accurate update with purely low-precision
numbers thoroughly without even touching high precision
arithmetics. This is particularly helpful on devices where
only low-precision arithmetics are supported.

Logistic Regression. We conduct a logistic regression
task on a synthetic dataset and the cancer dataset, both
are datasets with binary labels. The synthetic dataset
consists of 1, 000 data points, where each data point
is constructed
contains two features.

This dataset

3https://github.com/ydtydr/MCTensor

Figure 6. Test Accuracy for MC-MLP on Breast Cancer Dataset.
Notice that the curve for MC-MLP model with nc=2, ﬂoat16
essentially overlaps with the curve for PyTorch MLP model with
ﬂoat32.

Figure 7. Test Accuracy for MC-MLP on Reduced MNIST Dataset.
Notice that the curve for MC-MLP model with nc=2, ﬂoat16
essentially overlaps with the curve for PyTorch MLP model with
ﬂoat32.

through the make_classification (Guyon, 2003) function
from scikit-learn package with both n_informative and
n_clusters_per_class are set to 1. The breast cancer
dataset (MurtiRawat et al., 2020) consists of 569 data points
and each data point contains 30 features. More details can
be found in A.5.

Multi-layer Perceptron. We also construct a Multi-layer
Perceptron using MCTensor (MC-MLP) and evaluate it on
classiﬁcation tasks on the breast cancer dataset and a re-
duced MNIST, which has only 10,000 data points in total,
with 1,000 images sampled randomly per class (Deng, 2012).
The MC-MLP consists of three MCLinear layers with model
weight in MCFloat16. After each MCLinear layer, the result-
ing MCTensor is transformed into a normal tensor, passed it
to activation function and fed to the next layer.

We experiment on both the Breast Cancer dataset and the
Reduced MNIST dataset, and as demonstrated in Table 2
and Figure 7, in both experiments, MC-MLP outperforms
PyTorch ﬂoat16 models, and arrive at a lower training loss
and a higher test accuracy (same as PyTorch ﬂoat32 and

MCTensor: A High-Precision Deep Learning Library

Model

Training Loss

Testing accuracy

Model

MAP (mean ± sd) MR (mean ± sd)

MLP Float16
MLP Float32
MLP Float64

MC-MLP (nc=1)
MC-MLP (nc=2)
MC-MLP (nc=3)

0.144
0.124
0.124

0.144
0.124
0.124

90.35
91.23
91.23

90.35
91.23
91.23

Halfspace (f32)
Halfspace (f64)

MC-Halfspace (f64 nc=1)
MC-Halfspace (f64 nc=2)
MC-Halfspace (f64 nc=3)

91.91% ± 0.64%
92.79% ± 0.41%

93.02% ± 0.40%
92.77% ± 0.28%
93.31% ± 0.75%

1.399 ± 0.04
1.340 ± 0.07

1.296 ± 0.02
1.304 ± 0.02
1.282 ± 0.03

Table 3. Performance of Hyperbolic Models

Table 2. MC-MLP Models on Breast Cancer Dataset with MCSGD

ﬂoat64 models). Notice that for both datasets, after nc
exceeds certain value (nc = 2), adding extra more nc would
not lead to further improvement. More details can be found
in Section A.7.

3.2. Hyperbolic Embedding

Another use case that beneﬁts from MCTensor is hyperbolic
deep learning, where the non-Euclidean hyperbolic space
is adopted in place of Euclidean space for various purposes.
It has been shown in (Nickel & Kiela, 2017; Chami et al.,
2019; Yu & De Sa, 2022) that hyperbolic space is better
suited for processing hierarchical data (e.g. trees, acyclic
graphs). However, representing hyperbolic space with or-
dinary ﬂoating-points (even ﬂoat64) leads to unbounded
representation error, particularly when points get far away
from the origin, known as the “NaN" problem (Yu & De Sa,
2019) that not-a-number error occurs during computations.
With MCTensor, we can achieve high-precision computa-
tions and avoid the “NaN" problem by simply using more
components.

Following Yu & De Sa (2021), we conduct hyperbolic em-
bedding experiment on the WordNet Mammals dataset with
1181 nodes and 6541 edges. We use Poincaré upper-half
space (Halfspace) model to embed its transitive closure
for reconstruction. The Poincaré Halfspace model is the
manifold (U n, gu), where U n is the upper half space of the
n-dimensional Euclidean space, with the metric tensor and
distance function being:

gu(x) = gE
x2
n

,

du(x, y) = arcosh

(cid:16)

1 + ||x−y||2
2xnyn

(cid:17)

,

where gE is the Euclidean metric tensor. For the observed
edges D = {(x, y)}, we learn the embeddings Θ for all
nodes, subject to minimizing the following loss function

L(Θ) = (cid:80)

(x,y)∈D log

e−du (x,y)
y(cid:48) ∈N (x) e−du (x,y(cid:48) ) ,

(cid:80)

where N (x) are randomly chosen 50 negative examples in
addition to the positive example (x, y). We report the results
in Table 3, where MAP is the mean averaged precision and
MR is the mean rank metric.

4. Conclusion

We introduce MCTensor based on PyTorch to achieve high-
precision arithmetic while leveraging the beneﬁts of heavily-
optimized ﬂoating-point arithmetic. We verify its capability
of high precision computations using low precision numbers
and relieving the NaN problem in hyperbolic space. We
hope this library could interest researchers to use MCTensor
for ML applications requiring high-precision computations.

A promising future work is to design and optimize MCF
algorithms to granularize the tradeoff between efﬁciency and
precision to make MCTensor competent even for less-noisy
and general tasks. We hope this library can address the need
for fast high-precision library absent for DL community
and prompt DL practitioners to rethink the concept of high-
precision.

Acknowledgement

This work is supported by NSF IIS-2008102.

References

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,
M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-
enberg, J., Mané, D., Monga, R., Moore, S., Murray, D.,
Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,
I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M.,
Wicke, M., Yu, Y., and Zheng, X. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015.
URL https://www.tensorflow.org/. Software
available from tensorﬂow.org.

Bailey, D. H. and Borwein, J. M. High-precision arithmetic
in mathematical physics. Mathematics, 3(2):337–367,
2015.

Bezanson, J., Edelman, A., Karpinski, S., and Shah, V. B.
Julia: A fresh approach to numerical computing. SIAM
review, 59(1):65–98, 2017.

Chami, I., Ying, Z., Ré, C., and Leskovec, J. Hyperbolic
graph convolutional neural networks. Advances in neural
information processing systems, 32, 2019.

MCTensor: A High-Precision Deep Learning Library

Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao,
T., Xu, B., Zhang, C., and Zhang, Z. Mxnet: A ﬂexible
and efﬁcient machine learning library for heterogeneous
distributed systems. arXiv preprint arXiv:1512.01274,
2015.

Courbariaux, M., Bengio, Y., and David, J.-P. Training deep
neural networks with low precision multiplications. arXiv
preprint arXiv:1412.7024, 2014.

De Sa, C., Leszczynski, M., Zhang, J., Marzoev, A.,
Aberger, C. R., Olukotun, K., and Ré, C. High-accuracy
low-precision training. arXiv preprint arXiv:1803.03383,
2018.

Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. IEEE
signal processing magazine, 29(6):141–142, 2012.

Ganea, O., Bécigneul, G., and Hofmann, T. Hyperbolic neu-
ral networks. Advances in neural information processing
systems, 31, 2018.

Granlund, T. and the GMP development team. GNU MP:
The GNU Multiple Precision Arithmetic Library, 5.0.5
edition, 2012. http://gmplib.org/.

Guyon, I. Design of experiments of the nips 2003 variable
selection benchmark. In NIPS 2003 workshop on feature
extraction and feature selection, volume 253, pp. 40,
2003.

Hida, Y., Li, X. S., and Bailey, D. H. Library for double-
double and quad-double arithmetic. NERSC Division,
Lawrence Berkeley National Laboratory, pp. 19, 2007.

Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and
Bengio, Y. Quantized neural networks: Training neural
networks with low precision weights and activations. The
Journal of Machine Learning Research, 18(1):6869–6898,
2017.

Innes, M. Flux: Elegant machine learning with julia. Jour-
nal of Open Source Software, 2018. doi: 10.21105/joss.
00602.

Innes, M., Saba, E., Fischer, K., Gandhi, D., Rudilosso,
M. C., Joy, N. M., Karmali, T., Pal, A., and Shah, V. Fash-
ionable modelling with ﬂux. CoRR, abs/1811.01457,
URL https://arxiv.org/abs/1811.
2018.
01457.

Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J.,
Girshick, R., Guadarrama, S., and Darrell, T. Caffe:
Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093, 2014.

Jiang, H., Du, P., Li, K., and Peng, L. The implementation
of multi-precision package in multiple-component format
in matlab. SCAN 2016, pp. 66, 2016.

Jolde¸s, M., Marty, O., Muller, J.-M., and Popescu, V. Arith-
metic algorithms for extended precision using ﬂoating-
point expansions. IEEE Transactions on Computers, 65
(4):1197–1210, 2015.

Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,
G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,
A., et al. In-datacenter performance analysis of a tensor
processing unit. In Proceedings of the 44th annual inter-
national symposium on computer architecture, pp. 1–12,
2017.

Liu, Q., Nickel, M., and Kiela, D. Hyperbolic graph neural
networks. Advances in Neural Information Processing
Systems, 32, 2019.

Malmaud, J. and White, L. Tensorﬂow. jl: An idiomatic
julia front end for tensorﬂow. Journal of Open Source
Software, 3(31):1002, 2018.

MurtiRawat, R., Panchal, S., Singh, V. K., and Panchal,
Y. Breast cancer detection using k-nearest neighbors,
logistic regression and ensemble learning. In 2020 In-
ternational Conference on Electronics and Sustainable
Communication Systems (ICESC), pp. 534–540, 2020.
doi: 10.1109/ICESC48915.2020.9155783.

Nickel, M. and Kiela, D. Poincaré embeddings for learning
hierarchical representations. Advances in neural informa-
tion processing systems, 30, 2017.

Nickel, M. and Kiela, D. Learning continuous hierarchies
in the lorentz model of hyperbolic geometry. In Interna-
tional Conference on Machine Learning, pp. 3779–3788.
PMLR, 2018.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. Advances in neural information
processing systems, 32, 2019.

Priest, D. M. Algorithms for arbitrary precision ﬂoating
point arithmetic. University of California, Berkeley, 1991.

Priest, D. M. On properties of ﬂoating point arithmetics:
numerical stability and the cost of accurate computations.
PhD thesis, Citeseer, 1992.

Richard Shewchuk, J. Adaptive precision ﬂoating-point
arithmetic and fast robust geometric predicates. Discrete
& Computational Geometry, 18(3):305–363, 1997.

MCTensor: A High-Precision Deep Learning Library

Sala, F., De Sa, C., Gu, A., and Ré, C. Representation
tradeoffs for hyperbolic embeddings. In International
conference on machine learning, pp. 4460–4469. PMLR,
2018.

Schirra, S. Robustness and precision issues in geometric

computation. 1998.

Tulloch, A. and Jia, Y. High performance ultra-low-
precision convolutions on mobile devices. arXiv preprint
arXiv:1712.02427, 2017.

Wu, S., Li, G., Chen, F., and Shi, L. Training and inference
with integers in deep neural networks. In International
Conference on Learning Representations, 2018.

Yu, T. and De Sa, C. Hyla: Hyperbolic laplacian features for
graph learning. arXiv preprint arXiv:2202.06854, 2022.

Yu, T. and De Sa, C. M. Numerically accurate hyperbolic
embeddings using tiling-based models. Advances in Neu-
ral Information Processing Systems, 32, 2019.

Yu, T. and De Sa, C. M. Representing hyperbolic space
accurately using multi-component ﬂoats. Advances in
Neural Information Processing Systems, 34, 2021.

Zhang, T., Lin, Z., Yang, G., and De Sa, C. Qpytorch: A low-
precision arithmetic simulation framework. In 2019 Fifth
Workshop on Energy Efﬁcient Machine Learning and
Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp.
10–13. IEEE, 2019.

MCTensor: A High-Precision Deep Learning Library

A. Appendix

A.1. Julia BigFloat error

We further conduct experiments to evaluate the numerical errors of basic MCTensor arithmetic. Speciﬁcally, we ﬁrst compute
Add−MCN(x,y) of two random MCTensor x,y of roughly the same magnitude m. x,y are sampled by ﬁrst sampling two
random Julia BigFloat numbers with high precision (e.g. 3000 precision) using the equation (10−N (0, 1))m, then converted
to their corresponding MCTensors in Float32. In order to get the exact numerical error, we transform the MCTensor result to
a Julia BigFloat number, then compute the relative error of it to the high precision addition of x,y (and not the addition of
the two BigFloat numbers initially sampled) in Julia. In the same way, we compute the numerical errors of Mult−MCN(x,y)
and ScalingN(x,y) with y being MCTensor in the former case and standard (PyTorch) tensor in the later case. x,y are
sampled in the same way throughout the three cases.

Figure 8. Relative Error of MCTensor arithmetic with different number of components, compared with high precision Julia BigFloat
results (i.e. 3000 precision). Left: Add−MCN(x,y), middle: ScalingN(x,y) and right: Mult−MCN(x,y). Order of magnitudes for x and
y are kept the same.

For a thorough comparison, we also derive below the same numerical errors when the order of magnitudes for x varies and
order of magnitudes for y is kept at 2.

Figure 9. Relative Error of MCTensor arithmetic with different number of components, compared with high precision Julia BigFloat
results (i.e. 3000 precision). Left: Add−MCN(x,y), middle: ScalingN(x,y) and right: Mult−MCN(x,y). Order of magnitudes for x
varies and order of magnitudes for y is kept at 2.

MCTensor: A High-Precision Deep Learning Library

A.2. MCTensor Operators

A.2.1. BASIC OPERATORS

The input of Two−Sum is two PyTorch Tensors with same
precision, a and b. Algorithm 2 returns the sum s = ﬂ(a+b)
and the error, err(a + b).

Algorithm 5 Two-Prod-fma

Input: PyTorch Tensors a, b
Requires: Machine supports FMA instructions set
p ← ﬂ(a · b)
e ← torch.addcmul(−p, a, b)
Return: (p, e)

Algorithm 2 Two-Sum

Input: PyTorch Tensors a, b
x ← a + b
bvirtual ← ﬂ(x − a)
avirtual ← ﬂ(x − bvirtual)
broundoff ← ﬂ(b − bvirtual)
aroundoff ← ﬂ(a − avirtual)
y ← ﬂ(aroundoff + broundoff)
Return: (x, y)

The constraint of decreasing magnitude and non-
overlapping across nc might be temporarily violated in
computation, so MCTensor must be renormalized during
computation. Users can specify the target nc after renor-
malization, rnc, but by default we keep them the same. The
Renormalize function is this paper is a variant of the Priest’s
algorithm 6. (Priest, 1991).

Algorithm 6 Renormalize (Priest, 1992)

The Split Algorithm 3 takes a standard PyTorch ﬂoating
point value with p-bit signiﬁcand and splits it into its high
and low parts, both with p

2 -bit of signiﬁcand.

Algorithm 3 Split

Input: PyTorch Tensor a
if a.dtype is HalfTensor (ﬂoat16) then

constant ← 6

else if a.dtype is FloatTensor (ﬂoat32) then

constant ← 12

else if a.dtype is DoubleTensor (ﬂoat64) then

constant ← 26

end if
t ← ﬂ(2constant + 1) · a
ahi ← ﬂ(t − ﬂ(t − a))
alo ← ﬂ(a − ahi)
Return: (ahi, alo)

Based on Split, the following Algorithm 4 computes and
returns p = ﬂ(a × b) and e = err(a × b).

Algorithm 4 Two-Prod

Input: PyTorch Tensors a, b
p ← ﬂ(a · b)
(ahi, alo) ← Split(a)
(bhi, blo) ← Split(b)
err1 ← ﬂ(p − ﬂ(ahi · bhi))
err2 ← ﬂ(err1 − ﬂ(alo · bhi))
err3 ← ﬂ(err2 − ﬂ(ahi · blo))
e ← ﬂ(ﬂ(alo · blo) − err3)
Return: (p, e)

Fuse multiply-add, or FMA, is a ﬂoating-point operation
that performs multiplication and addition in one step. With
proper hardware, this Algorithm 5 can speed up TwoProd.

Input: nc-MCTensor x, rnc
Requires: rnc < nc
initialize s ← x0, k ← 0, t0 ← 0
for i = 1 to nc do

(s, ti) ← Two-Sum(xi, s)

end for
for i = 0 to nc − 1 do

(s, e) ← Two-Sum(s, ti)
if e (cid:54)= 0 then
bk ← s
s ← e
k ← k + 1

end if
end for
Return: (b0, b1, · · · , brnc−1)

There is a simple and fast implementation of Renormalize
as Simple−Renorm, which extracts all non-zero values from
a non-renormalized MCTensor and puts together a new
MCTensor. Note that this Simple−Renorm does not have the
same guarantee as Renormalize. Therefore in our Mult−MCN,
we still use the original version of Renormalize. But for
most other operations, we still utilize Simple−Renorm for
fast computation.

Algorithm 7 Simple-Renorm
Input: nc-MCTensor x, rnc
Requires: rnc < nc
k ← 0; (b0, b1, · · · , brnc−1) ← (0, 0, · · · , 0)
for i = 0 to rnc − 1 do

if xi (cid:54)= 0 then
bk ← xi
k ← k + 1

end if
end for
Return: (b0, b1, · · · , brnc−1)

MCTensor: A High-Precision Deep Learning Library

Algorithm
(Richard Shewchuk, 1997)

10

Div-MCN,

modiﬁed

from

Input: nc-MCTensor x, y
initialize: q ← ﬂ(x0/y0), h0 ← q
for i = 1 to nc do

r ← Add-MCN(x, −ScalingN(y, q, False))
x ← r
q ← ﬂ(x0/y0)
hi ← q

end for
h ← (h0, h1, · · · , hnc)
Return: Simple-Renorm(h, nc)

Algorithm 11 Mult-MCN

Input: nc-MCTensor x, y
initialize: z0 ← 1, z1 = · · · = znc−1 ← 0
z ← (z0, z1, · · · , znc−1)
y−1 ← Renormalize(Div-MCN(z, y))
h ← Renormalize(Div-MCN(x, y−1))
Return: h

Algorithm 12 Mult-MCN-Slow

Input: nc-MCTensor x, y
initialize: p ← ﬂ(x0 · y0), h0 ← p
for i = 1 to nc do

e ← Add-MCN(x, −DivN(p, y))
x ← e
p ← ﬂ(x0 · y0)
hi ← p

end for
h ← (h0, h1, · · · , hnc)
Return: Simple-Renorm(h, nc)

Algorithm 13, DivN takes input of a PyTorch Tensor and a
MCTensor and computes the Div−MCN results by appending
zero-value components to a PyTorch Tensor and making it
MCTensor.
Algorithm 13 DivN

Input: PyTorch Tensor x0, nc-MCTensor y,
initialize: x1 = · · · = xnc−1 ← 0
x ← (x0, x1, · · · , xnc−1)
Return: Div-MCN(x, y)

Algorithm 8 describes the multiplication of a MCTensor
with a PyTorch Tensor. In our implementation, the user
can specify whether the algorithm can return an expanded
results with nc + 1, or a MCTensor with same nc.

Algorithm 8 ScalingN, modiﬁed from (Richard Shewchuk,
1997)

Input: nc-MCTensor x, PyTorch Tensor v, expand
initialize e ← 0
for i = 0 to nc − 1 do

(hp, e1) ← Two-Prod(xi, v)
(hi, e2) ← Two-Sum(hp, e)
e ← ﬂ(e1 + e2)

end for
h ← (h0, · · · , hnc−1, e)
if expand is True then

Return: Simple-Renorm(h, nc + 1)

else

Return: Simple-Renorm(h, nc)

end if

Add−MCN, Div−MCN, Mult−MCN are operators for addition,
division, and multiplication of two nc-MCTensors. Here
we have two versions of multiplication, Mult−MCN and
Mult−MCN−Slow. Algorithm 11 is implemented by taking
the inverse of the second MCTensor ﬁrst, and then the ﬁrst
MCTensor is divided by the second MCTensor’s inverse.
This division would give the result of multiplication. Al-
gorithm 12 follows the same pattern of our deﬁnition of
Div−MCN and provides better error bounds, but it is rarely
used as the computational cost is too high.

Algorithm
(Richard Shewchuk, 1997)

9

Add-MCN,

modiﬁed

from

Input: nc-MCTensor x, y
initialize: e ← 0
for i = 0 to nc − 1 do

(hp, e1) ← Two-Sum(xi, yi)
(hi, e2) ← Two-Sum(hp, e)
e ← ﬂ(e1 + e2)

end for
h ← (h0, · · · , hnc−1, e)
Return: Simple-Renorm(h, nc)

MCTensor: A High-Precision Deep Learning Library

The following Algorithm 14 describes the exponential func-
tion for a MCTensor.

Algorithm 14 Exp-MCN

Input: nc-MCTensor x
initialize h ← exp(x0)
for i = 1 to nc − 1 do

h ← ScalingN(h, exp(hi), True)

end for
Return: h

Algorithm 18 MM-MCN

Input: nc-MCTensor x, PyTorch Tensor v
Requires: x is 2D matrix of size (n × m) and v is 2D
Matrix of size (m × p)
x ← x.unsqueeze(−1)
v ← v.transpose(−1, −2)
scaled ← ScalingN(x, v, False)
h ← scaled[..., 0]
for i = 1 to m − 1 do

h ← Add-MCN(h, scaled[..., i])

end for
Return: h

The following Algorithm 15 describes the square of a
MCTensor.

Algorithm 15 Square-MCN

Input: nc-MCTensor x
initialize h0 ← ﬂ(2x0 ), h1 = · · · = hnc−1 ← 0
h ← (h0, h1, · · · , hnc−1)
h ← Grow-ExpN(h, ﬂ(2 · x0 · x1))
Return: h

A.2.2. MATRIX OPERATORS

on

the

and

MCTensor

dimensions

Based
put
Dot−MCN, MV−MCN, MM−MCN, BMM−MCN and 4DMM−MCN
are implemented for calculating the matrix-level multiplica-
tion results. All operations are identical with the PyTorch
implementations.

in-
Tensor,

PyTorch

of

Algorithm 16 Dot-MCN

Input: nc-MCTensor x, PyTorch Tensor v
Requires: x and v both 1D array
h ← ScalingN(x, v, False)
htensor = h0 + h1 + · · · + hnc−1
Return: htensor

Algorithm 17 MV-MCN

Input: nc-MCTensor x, PyTorch Tensor v
Requires: x is 2D matrix of size (n × m) and v is 1D
array of size m
scaled ← ScalingN(x, v, False)
h ← scaled[..., 0]
for i = 1 to m − 1 do

h ← Add-MCN(h, scaled[..., i])

end for
Return: h

Algorithm 19 BMM-MCN

Input: nc-MCTensor x, PyTorch Tensor v
Requires: x is 3D matrix of size (b × n × m) and v is
3D Matrix of size (b × m × p)
x ← x.unsqueeze(−1)
v ← v.unsqueeze(1).transpose(−1, −2)
scaled ← ScalingN(x, v, False)
h ← scaled[..., 0]
for i = 1 to m − 1 do

h ← Add-MCN(h, scaled[..., i])

end for
Return: h

Algorithm 20 4DMM-MCN

Input: nc-MCTensor x, PyTorch Tensor v
Requires: x is 4D matrix of size (a × b × n × m) and v
is 4D Matrix of size (c × d × m × p); two sizes can be
broadcasted in torch.matmul
x ← x.unsqueeze(−1)
v ← v.unsqueeze(2).transpose(−1, −2)
scaled ← ScalingN(x, v, False)
h ← scaled[..., 0]
for i = 1 to m − 1 do

h ← Add-MCN(h, scaled[..., i])

end for
Return: h

The following Algorithm 21 computes the matrix multi-
plication of a nc-MCTensor with a PyTorch Tensor ﬁrst,
then times a constant α. Then the product of another nc-
MCTensor times a constant β is added to the former multi-
plication result.

MCTensor: A High-Precision Deep Learning Library

Algorithm 21 AddMM-MCN

Input: nc-MCTensor x, nc-MCTensor y, PyTorch Ten-
sor v, β, α
Requires: x is 2D matrix of size (n × m), and v is 2D
Matrix of size (m × p); y is 2D matrix of size (n × m)
or y is 1D array of size m
h ← ScalingN(MM-MCN(x, v), α)
bias ← ScalingN(y, β)
h ← Add-MCN(h, bias)
Return: h

This Algorithm 22, Matmul−MCN is the central function
for handling all matrix level multiplications of one nc-
MCTensor and one PyTorch Tensor.

Algorithm 22 Matmul-MCN

Input: nc-MCTensor x, PyTorch Tensor y
xd, yd ← x.dim(), y.dim()
if xd = 1 and yd = 1 then

Return: Dot-MCN(x, y)
else if xd = 2 and yd = 2 then
Return: MM-MCN(x, y)
else if xd = 2 and yd = 1 then
Return: MV-MCN(x, y)
else if xd > 2 and yd = 1 then
Return: ScalingN(x, y)

else if xd = yd and xd = 3 then
Return: BMM-MCN(x, y)
else if xd = yd and xd = 4 then
Return: 4DMM-MCN(x, y)

end if

MCTensor: A High-Precision Deep Learning Library

A.3. MCF Basic and Matrix Operators

We run on a AMD Ryzen 7 5800X CPU with 64 GB memory.

For all the basic operators, we repeat PyTorch addition for 7e3 runs and 1-, 2-, 3-MCTensor for 7 runs.

For vector-vector (torch.dot/Dot-MCN) product, we repeat PyTorch addition for 7e6 runs and 1-, 2-, 3-MCTensor for 7e3
runs. For matrix-vector (torch.mv/MV-MCN) product and batched matrix-matrix (torch.bmm/BMM-MCN) product, we
repeat PyTorch addition for 7e3 runs and 1-, 2-, 3-MCTensor for 7 runs. For matrix-matrix (torch.matmul/Matmul-MCN)
product and bias-matrix-matrix (torch.addmm/AddMM-MCN) addition-product, we repeat PyTorch addition for 7e4 runs
and 1-, 2-, 3-MCTensor for 7 runs.

The timing for both matrix and basic operators are given in Table 4 and Table 5.

Operators

Inputs sizes

FloatTensor

1-MCTensor

2-MCTensor

3-MCTensor

(1000 × 1000)
Add-MCN
(1000 × 1000)
ScalingN
Mult-MCN (1000 × 1000)
(1000 × 1000)
Div-MCN

497 µs ± 6.77 µs
490 µs ± 9.69 µs
514 µs ± 15.2 µs
510 µs ± 10.6 µs

26.7 ms ± 486 µs
33.7 ms ± 402 µs
218 ms ± 4.03 ms
80.3 ms ± 770 µs

44.7 ms ± 379 µs
57.5 ms ± 842 µs
667 ms ± 11.6 ms
243 ms ± 3.24 ms

64.4 ms ± 385 µs
84.7 ms ± 1.78 ms
1.4 s ± 21.6 ms
498 ms ± 8.26 ms

Table 4. MCTensor Basic Operators Running Time (mean ± sd)

Operators

Dot-MCN
MV-MCN
Matmul-MCN
AddMM-MCN
BMM-MCN

Inputs sizes

FloatTensor

1-MCTensor

2-MCTensor

3-MCTensor

5000, 5000
(5000 × 500), 500
( 500× 200), ( 200× 50)
100, ( 500× 200), ( 200× 100),
(16 × 500× 200), (16 ×200× 50)

1.61 µs ± 3.29 ns
157 µs ± 4.32µs
97.3 µs ± 1.1 µs
153 µs ± 869 ns
1.5 ms ± 9.32 µs

442 µs ± 5.61 µs
320 ms ± 5.78ms
495 ms ± 10.8 ms
750 ms ± 21.9 ms
4.84 s ± 43.1 ms

656 µs ± 1.16 µs
460 ms ± 10.7ms
735 ms ± 21.7 ms
1.12 s ± 28.4 ms
8.03 s ± 68.4 ms

858 µs ± 12.2 µs
580 ms ± 12.1ms
934 ms ± 28 ms
1.44 s ± 49.1 ms
11.2 s ± 74.1 ms

Table 5. MCTensor Matrix Operators Running Time (mean ± sd)

A.4. Linear Regression Results

Table 6 describes the ﬁnal training loss of the linear regression task in section 2.4.

Model

Train Loss

PyTorch ﬂoat16
PyTorch ﬂoat32
PyTorch ﬂoat64
MCTensor ﬂoat16, nc = 1
MCTensor ﬂoat32, nc = 2
MCTensor ﬂoat64, nc = 3

1.99e-4
2.64e-12
8.02e-18
1.80e-4
1.95e-7
1.95e-7

Table 6. Final Training Loss Results of Linear Regression Task

MCTensor: A High-Precision Deep Learning Library

A.5. Logistic Regression

We apply logistic regression on a synthetic dataset and a breast cancer dataset. The synthetic dataset consists of 1,000 data
points, where each data point contains two features, while the breast cancer dataset consists of 569 data points and each data
point contains 30 numeric features. A single MCLinear layer with ﬂoat16 and nc between 1, 2 and 3 are used for logistic
regression with Binary Cross Entropy loss.

L(W ) = BCELoss(y, sigmoid(XW T ))

For both datasets, we randomly split 80% of the data for training and 20% of the data for testing. As both datasets are
small in scale, we run them in full batch with MC-SGD and SGD. For the synthetic dataset, we set learning rate to be 3e-3
and run for 4000 epochs. For the breast cancer dataset, we set learning rate to be 1e-4 and momentum as 0.9, and run for
3000 epochs. Below is the results for both dataset and ﬁgures for MCLinear results on breast cancer dataset with MC-SGD
optimizer. As can be seen, even with as few as 2 components, MCTensor in ﬂoat16 can match with the training loss of
ﬂoat32 Tensor.

Figure 10. Testing Accuracy for MCTensor and PyTorch Logistic Regression on the Breast Cancer Dataset

Model

Training Loss Testing Accuracy (%)

0.1940
0.1042
0.1042
0.1941
0.1041
0.1041
Table 7. Final Training and Testing Results for Logistic Regression on Synthetic Dataset

Tensor ﬂoat16
Tensor ﬂoat32
Tensor ﬂoat64
1-MCTensor ﬂoat16
2-MCTensor ﬂoat16
3-MCTensor ﬂoat16

100
100
100
100
100
100

Model

Training Loss Testing Accuracy (%)

0.1944
0.1528
0.1528
0.1944
0.1527
0.1527
Table 8. Final Training and Testing Results for Logistic Regression on Breast Cancer Dataset

Tensor ﬂoat16
Tensor ﬂoat32
Tensor ﬂoat64
1-MCTensor ﬂoat16
2-MCTensor ﬂoat16
3-MCTensor ﬂoat16

92.11
92.98
92.98
92.11
92.98
92.98

MCTensor: A High-Precision Deep Learning Library

A.6. MCTensor Deep Learning models

As a demonstration for the ease of using MCTensor to build a MCTensor deep learning model, we provide an example code
for MCTensor MLP (MC-MLP) model for the multi-class classiﬁcation tasks. Essentially, the only differences visible to the
users are the need to set the number of components, and the need to convert MCTensor output to Tensor approximation
between different layers. The need for Tensor approximation is explained below.

class MC − MLP( MCModule ):

def __init__ (self , input_dim , hidden1 , hidden2 , num_classes =10, nc =2):

super (MCMLP , self ). __init__ ()
self.fc1 = MCLinear (input_dim , hidden1 , nc=nc)
self.fc2 = MCLinear (hidden1 , hidden2 , nc=nc)
self.fc3 = MCLinear (hidden2 , num_classes , nc=nc)
self. dropout = nn. Dropout (0.2)

def forward (self , x):

x = F.relu(self.fc1(x))
x = self. dropout (x. tensor .sum ( −1))
x = F.relu(self.fc2(x))
x = self. dropout (x. tensor .sum ( −1))
x = self.fc3(x)
return F. log_softmax (x. tensor .sum ( −1) , dim =1)

Figure 11. MC-MLP Code Example

In MCModule, all network layers take Tensor as inputs, keep MCTensor as their weights, and through MCTensor matrix
operations, produces MCTensor as output. If the MCTensor outputs need to be taken as input for the next MCModule
layer, there need to be new layers that will perform multiplication on two MCTensor matrices. As can be seen from
Table 4, ScalingN, the multiplication between a MCTensor and a PyTorch Tensor, is more than a hundred times faster
than Mult-MCN, the multiplication between two MCTensor. To avoid this forbidden cost of computation, we convert
the unevaluated sums into an evaluated sum as shown above (x.tensor.sum(−1)). Although there might some losses of
precision due to summation, the loss is marginal and we trade it for an orders of magnitude smaller execution time.

A.7. MC-MLP Experiment Details

Using MC-MLP, we perform multi-class classiﬁcation task on the Reduced MNIST dataset and binary classiﬁcation task on
the Breast Cancer dataset. The training details for both tasks are shown in Table 9 and Table 10.

Parameter

Value

MLP ﬁrst hidden layer dim
MLP second hidden layer dim
Batch size
Optimizer
Learning rate
Epoch

150
150
full batch (569)
MC-SGD (GD with full batch)
6e-3
1000

Table 9. Training Details for the Breast Cancer dataset

Parameter

MLP ﬁrst hidden layer dim
MLP second hidden layer dim
Batch size
Optimizer
Learning rate
Momentum
Epoch

Value

50
50
128
MC-SGD (SGD)
2e-3
0.8
100

Table 10. Training Details for the Reduced MNIST Dataset

Figure 12 shows the training loss curves for MC-MLP on breast cancer dataset with MC-SGD optimizer.

MCTensor: A High-Precision Deep Learning Library

Figure 12. Training Loss Curves for MLP on Breast Cancer Dataset

Figure 13 shows the training loss curves for MC-MLP on reduced MNIST dataset with MC-SGD optimizer. The hyperpa-
rameters used in training are shown in Table 10, and the ﬁnal results for testing accuracy is shown in Table 11.

Figure 13. Training Loss Curves for MLP on Reduced MNIST Dataset

Model

Training Loss

Testing accuracy

MCMLP (f16 nc=1)
MCMLP (f16 nc=2)
MCMLP (f16 nc=3)
PyTorch MLP (f16)
PyTorch MLP (f32)
PyTorch MLP (f64)

0.424
0.349
0.349
0.412
0.343
0.343

91.40
92.03
91.98
91.40
92.00
92.00

Table 11. MC-MLP Models on Reduced MNIST Dataset with MCSGD

