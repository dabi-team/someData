Safety and Performance, Why not Both? Bi-Objective Optimized
Model Compression toward AI Software Deployment

Jie Zhu1,2, Leye Wang1,2, Xiao Han3∗
1Key Lab of High Confidence Software Technologies (Peking University), Ministry of Education, China
2School of Computer Science, Peking University, Beijing, China
3Shanghai University of Finance and Economics, Shanghai, China
zhujie@stu.pku.edu.cn,leyewang@pku.edu.cn,xiaohan@shufe.edu.cn

2
2
0
2

p
e
S
9

]

G
L
.
s
c
[

2
v
9
6
9
5
0
.
8
0
2
2
:
v
i
X
r
a

ABSTRACT
The size of deep learning models in artificial intelligence (AI) soft-
ware is increasing rapidly, which hinders the large-scale deploy-
ment on resource-restricted devices (e.g., smartphones). To mitigate
this issue, AI software compression plays a crucial role, which aims
to compress model size while keeping high performance. How-
ever, the intrinsic defects in the big model may be inherited by
the compressed one. Such defects may be easily leveraged by at-
tackers, since the compressed models are usually deployed in a
large number of devices without adequate protection. In this pa-
per, we try to address the safe model compression problem from
a safety-performance co-optimization perspective. Specifically, in-
spired by the test-driven development (TDD) paradigm in software
engineering, we propose a test-driven sparse training framework
called SafeCompress. By simulating the attack mechanism as the
safety test, SafeCompress can automatically compress a big model
to a small one following the dynamic sparse training paradigm.
Further, considering a representative attack, i.e., membership infer-
ence attack (MIA), we develop a concrete safe model compression
mechanism, called MIA-SafeCompress. Extensive experiments are
conducted to evaluate MIA-SafeCompress on five datasets for both
computer vision and natural language processing tasks. The results
verify the effectiveness and generalization of our method. We also
discuss how to adapt SafeCompress to other attacks besides MIA,
demonstrating the flexibility of SafeCompress.

CCS CONCEPTS
• Security and privacy → Software security engineering; •
Computing methodologies → Artificial intelligence.

KEYWORDS
AI software safe compression, test-driven development, member-
ship inference attack

∗Corresponding author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX

ACM Reference Format:
Jie Zhu, Leye Wang, and Xiao Han. 2022. Safety and Performance, Why
not Both? Bi-Objective Optimized Model Compression toward AI Software
Deployment. In Proceedings of the 37th IEEE/ACM International Conference
on Automated Software Engineering (ASE ’22). ACM, New York, NY, USA,
13 pages. https://doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION
In the last decade, artificial intelligence (AI) software, especially
those based on deep neural networks (DNN), has set a huge storm [33].
Currently, AI software, with DNN as representatives, is recognized
as an emerging type of software artifact (sometimes known as “soft-
ware 2.0” [71]). Notably, the size of DNN-based AI software has
increased rapidly in recent years (mostly because of the trained deep
neural network model). For instance, the state-of-the-art computer
vision model contains more than 15 billion parameters [55]. The re-
cent natural language model, GPT-3, is even larger, surpassing 175
billion parameters; this requires nearly 1TB of space to store only
the model [5]. Such a huge model hinders realistic applications such
as autonomous driving when the software requires to be deployed
in resource-restricted devices like wearable devices or edge nodes.
To this end, a new branch is derived from the traditional software
compression area [16, 17, 52, 65], called AI software compression
1), and has attracted a lot of
(especially DNN model compression
research interest to date.

Model compression aims to compress a large DNN model to a
smaller one given specific requirements e.g., parameter numbers,
model sparsity, and compression rate. Rashly compressing a model
may lead to severe degeneration in the AI software’s task perfor-
mance such as classification accuracy. To balance memory storage
and task performance, many compression methods have been pro-
posed and deployed [2, 8, 20, 21, 24, 30]. For example, Han et al. [21]
prune AlexNet [33] and reduce its size by 9 times while losing only
0.01% accuracy in image classification. Jiao et al. [30] reduce the
size of BERT [13] by about 2 times via knowledge distillation while
losing a 0.1% average score in the GLUE [63] official benchmark.

While the compressed model aims to mimic the original model’s
behavior, its defects may also be inherited. As a representative case,
big deep models are verified to be able to memorize training data [6],
thus leading to private data leakage when facing threats such as
membership inference attacks [59]; such a vulnerability would
probably remain in the compressed model. More seriously, model
compression is often used for AI software deployment on a large

1In the rest of the paper, without incurring the ambiguity, we will use ‘model com-
pression’ to indicate the ‘DNN model compression in AI software’ for clarity.

 
 
 
 
 
 
ASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Jie Zhu, Leye Wang, and Xiao Han

number of edge devices (smartphones, wearable devices, etc.) [11,
39]; compared to the big model (often stored in a well-maintained
server), attackers thereby have much more opportunities to access
to compressed models and attack them(e.g., an attacker may act as
a normal user to download the compressed model in her/his own
device). In a word, compressed models will inherit the vulnerabilities
2.
of big models, while facing even higher risks of being attacked
Hence, studying how to do safe model compression is urgently
required.

An intuitive solution to the safe model compression is directly
combining two streams of techniques, forming a two-step solution: (i)
model compression and (ii) model protection. For instance, we can
first obtain a small model using existing compression techniques,
and then apply protection techniques (e.g., differential privacy [1]
and knowledge distillation [58]) to improve the model safety against
certain attacks. However, the two-step solution may suffer from poor
model performance
and low safety due to lack of consideration of
the interaction between the two techniques. For example, Yuan &
Zhang [68] find that pruning makes the divergence of prediction
confidence and sensitivity increase and vary widely among different
classes. This may not be sensed by defenders for lack of interaction
but can be manipulated by attackers.

3

In this paper, we try to address the safe model compression prob-
lem from a performance-safety co-optimization perspective. More
specifically, inspired by test-driven development [3] and dynamic
sparse training [46], we propose a test-driven sparse training frame-
work for safe model compression, called SafeCompress. By simulat-
ing the attack mechanism to defend, SafeCompress can automati-
cally compress a big model into a small one (with required sparsity)
to optimize both model performance and safety. SafeCompress gen-
erally follows an iterative optimization approach. For initialization,
SafeCompress randomly prunes a big model to a sparse one, which
serves as the input of the first iteration. In each iteration, various
compression strategies are applied to the input model to derive more
sparse models; then, SafeCompress launches a performance-safety
co-optimization mechanism that applies both task performance and
simulated-attack-based safety tests on the derived sparse models to
select the best compression strategy. Then, the sparse model with
the selected (best) strategy becomes the input of the next iteration.
The iterative process will terminate after a predefined maximum
number of iterations or a new model has little improvement in
performance and safety tests.

Based on the SafeCompress framework, we design and implement
a concrete safe model compression mechanism against membership
inference attacks (MIAs) [59], denoted as MIA-SafeCompress. In
MIA-SafeCompress, we further propose an entropy-based regularizer
that increases the uncertainty of model outputs to protect the model
from MIAs. Note that we choose MIA as the attack example because
MIA is a representative method for evaluating AI model safety,
especially from the privacy leakage aspect [50, 66].
Our contribution can be summarized as follows:

2Besides accessing to compressed models more easily for attackers, we also find that
some compressed models have higher attack accuracy in our experiments, which
means these compressed models are more vulnerable.
3We use ‘performance’ as a general term to describe any specific task-dependent metric.
For instance, if a model is developed for image classification, the model performance
can be measured by the classification accuracy metric.

• To the best of our knowledge, this is one of the pioneering
efforts toward the safe model compression problem, which is critical
for today’s large-scale AI software deployment on edge devices
such as smartphones.

• To address the safe model compression problem, we propose
a general framework called SafeCompress, which can be config-
ured to protect against a pre-specified attack mechanism. In brief,
SafeCompress adopts a test-driven process to iteratively update the
model compression strategies to co-optimize model performance
and safety.

• Considering MIA as a representative attack mechanism [49], we
develop a concrete instance of SafeCompress, i.e., MIA-SafeCompress.
We further enhance the iterative model compression process with a
new entropy-based regularizer to increase the model safety against
MIA. We also discuss how to adapt SafeCompress to other attacks.
• Using MIA-SafeCompress as a showcase of SafeCompress, we
conduct extensive experiments on five datasets of two domains
(three computer vision tasks and two natural language processing
tasks). Results verify that our method significantly outperforms
baseline solutions that integrate state-of-the-art compression and
MIA defense techniques. The code of MIA-SafeCompress is available
as open source at https://github.com/JiePKU/MIA-SafeCompress.

2 BACKGROUND
2.1 Test-Driven Development
Test-driven development (TDD) [3] is a programming paradigm
where test codes play a vital role during the whole software develop-
ment process. With TDD, before writing the codes for the software
functionality, programmers would write the corresponding test
suite in advance; then, the test suite can justify whether the soft-
ware functionality is implemented properly or not. In general, TDD
leads to an iterative coding-testing process to improve the correct-
ness and robustness of the software; it has become a widely-adopted
practice in software development (e.g., SciPy [18]). Inspired by TDD,
we develop a test-driven safe model compression framework called
SafeCompress. Similar to the iterative coding-testing process in
TDD, SafeCompress adopts an iterative compressing-testing process.
In particular, by specifying the attacks to fight, SafeCompress can
automatically update the compression strategies step by step to
optimize model performance and safety simultaneously.

2.2 Dynamic Sparse Training
Dynamic sparse training (DST) is a sparse-to-sparse training para-
digm to learn a sparse (small) DNN model based on a dense (big)
one [45, 46]. Specifically, it starts training with a sparse model
structure initialized from a dense model. As training progresses, it
modifies the architecture iteratively by pruning some neural net-
work connections and growing new connections based on certain
strategies. This enables neural networks to explore self-structure
until finding the most suitable one for the training data. Note that
SafeCompress’s iterative updating strategy for optimizing the com-
pressed model structure is just inspired by the DST paradigm.

2.3 Membership Inference Attacks
Prior research has extensively verified that DNN models often ex-
hibit different behaviors on training data records (i.e., members)

Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software DeploymentASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

versus test data records (i.e., non-members); the main reason is that
models can memorize training data during the repeated training
process lasting for a large number of epochs [6, 50]. For instance, a
DNN model would generally give a higher prediction confidence
score to a training data record than a test one, as the model may
remember training data’s labels. Based on such observations, mem-
bership inference attacks (MIAs) [59] are proposed to build attack
models to infer whether one data record belongs to training data or
not. When it comes to sensitive data, personal privacy is exposed
to a great risk. For example, if MIA learns that a target user’s elec-
tronic health record (EHR) data is used to train a model related to
a specific disease (e.g., to predict the length of stay in ICU [44]),
then the attacker knows that the target user has the disease. In this
paper, we use MIA as an attack instance to verify the feasibility
and effectiveness of SafeCompress, because MIA has become one of
the representative attack methods to evaluate the safety of DNN
models both theoretically and empirically [50, 66].

3 PROBLEM FORMULATION
Given a big model F (; 𝜃 ) parameterized by 𝜃 , we aim to find a
sparse model F (; ^𝜃 ) (most elements in ^𝜃 are zero) under certain
memory restriction Ω and the sparse model can defend against
to denote the
a pre-specified attack mechanism 𝑓𝐴. We use 𝐺 𝑓𝐴
attack gain of 𝑓𝐴. We restrict the compression ratio, or called model
sparsity, below Ω (i.e., the percentage of non-zero parameters in
the sparse model over the original model). We aim to minimize
over the
both the task performance loss L and the attack gain 𝐺 𝑓𝐴
sparse model F (; ^𝜃 ):

min
^𝜃

min
^𝜃

s. t.

∑︁

L (F (𝑥; ^𝜃 ), 𝑦)

𝑥,𝑦
𝐺 𝑓𝐴 (F (; ^𝜃 ))

∥ ^𝜃 ∥0
∥𝜃 ∥

≤ Ω,

(1)

(2)

(3)

where 𝑥 is a sample and 𝑦 is the corresponding label, and L rep-
resents a task-dependent loss function. ∥·∥0 counts the number of
non-zero elements, and ∥·∥ calculates the number of all the ele-
ments. Note that it is a bi-objective optimization problem regarding
both model performance (Eq. 1) and safety (Eq. 2).

The above formulation is a general one without specifying the
attack mechanism 𝑓𝐴. In this research, we use MIA as an example
of 𝑓𝐴. The gain 𝐺 𝑓𝐴

for MIA is then formulated as follows.

MIA gain. MIA infers whether a sample 𝑥 is in the training
dataset or not. To this end, MIA usually trains a binary classifica-
tion model 𝑓𝐴. In this work, we consider 𝑓𝐴 as a neural network
classifier with the black-box setting [59] (i.e., only model outputs
are available to 𝑓𝐴).4 Then, given training samples (𝐷𝑡𝑟 ) and non-
training samples (𝐷 ¬𝑡𝑟 ), the expected gain of 𝑓𝐴 is:

𝐺 𝑓𝐴 (F (; ^𝜃 )) =

∑︁

1(𝑥 ∈ 𝐷𝑡𝑟 ) log(𝑓𝐴 (F (𝑥; ^𝜃 ), 𝑦))+

𝑥,𝑦
1(𝑥 ∈ 𝐷¬𝑡𝑟 ) log(1 − (𝑓𝐴 (F (𝑥; ^𝜃 ), 𝑦))),

(4)

where 1(𝑥 ∈ 𝑍 ) is 1 if the sample 𝑥 belongs to 𝑍 , otherwise 0.

4Other MIA settings, e.g., white-box [50], can also be supported by our SafeCompress
framework. Sec. 4.4 includes a detailed discussion.

4 METHOD
4.1 Key Design Principles
We clarify two key principles driving our design, i.e., attack config-
urability and task adaptability before elaborating on design details.
Attack Configurability. In reality, adversaries may conduct
various types of attacks [22]. Hence, a practical solution should
be able to do safe compression against an arbitrary pre-specified
attack mechanism. In other words, the proposed solution should
be easily configured to fight a given attack mechanism.

Task Adaptability. AI (especially deep learning) techniques
have been applied to various task domains including computer
vision (CV), natural language processing (NLP), etc. To this end, a
useful solution is desired to be able to adapt to heterogeneous AI
tasks (e.g., CV or NLP) very easily.

Our design of SafeCompress just follows these two principles,
thus ensuring practicality in various AI software deployment sce-
narios. Next, we describe the design details.

4.2 SafeCompress: A General Framework for

Safe Model Compression

As shown in Figure 1, SafeCompress contains three stages.

Stage 1. Sparsity-Aware Model Initialization. The sparsity
(i.e., compression ratio) needs to be considered as a priority. We
follow the dynamic sparse training paradigm to restrict the sparsity
in model initialization. Specifically, based on a given big model (an
arbitrary deep model for various tasks like CV or NLP), we initialize
a sparse one to meet memory requirements. After initialization, the
sparse model is sent to Stage 2.

Stage 2. Candidate Sparse Model and Simulated Attacker
Generation. During this stage, the sparse model is firstly trained
until reaching the stopping criteria. Then, the trained model is fed
to two branches. The first branch is called dynamic sparse update
where different combinations of pruning and growth strategies are
performed on the input model, producing new candidate model
variants with diverse sparse architectures. Note that the number of
removed connections equals that of the reactivated ones, guaran-
teeing the sparsity unchangeable. The second branch is the attack
mechanism simulation. In this branch, we simulate an external at-
tacker that aims to attack candidate sparse models. The simulated
attacker and candidate sparse models are sent to Stage 3.

Stage 3. Safety Test-driven Model Selection. A safety test is
performed on these input sparse models by the simulated attacker.
The one that performs the best in this test will be selected and sent
back to Stage 2, starting a new iteration. The whole process will
terminate after running for a predefined number of iterations.

The pseudo-code of SafeCompress is in Algorithm 1. Note that
we do not restrict the type of attacks in SafeCompress. It thus has a
potential to prevent various attacks toward AI software and models.
Next, we would instantiate the SafeCompress framework for MIA.
Afterward, we discuss how to adapt SafeCompress to other attacks.

ASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Jie Zhu, Leye Wang, and Xiao Han

Figure 1: An overview of our framework SafeCompress.

Algorithm 1 SafeCompress Framework Procedure
Input: A big model M𝐿; A sparsity requirement Ω; A set of update strate-
gies U and the size is 𝑁 ; Training stopping criteria for a sparse model
T; Total epochs for termination 𝐸𝑝𝑠;
Output: A well-trained sparse model M𝑆 ;
1: Initialize M𝐿 as a sparse one to meet the sparsity requirement Ω; We

denote this sparse model as M𝑆

Update M𝑆 via U𝑖 denoted as M𝑖
𝑆

2: Train M𝑆 until condition T is satisfied then do:
3: for each U𝑖 in U do:
4:
5: end for
6: Obtain a candidate sparse model set C={M1
7: Simulate an external attacker as A (with the help of M𝑆 )
8: M𝑏𝑒𝑠𝑡
9: M𝑆 ← M𝑏𝑒𝑠𝑡
10: if not achieve total epochs 𝐸𝑝𝑠:
11:
12: return M𝑆

𝑆 ← Pick the best from {safety test(C, A)}

𝑆 ...M𝑁
𝑆 }

go to 2

𝑆

4.3 MIA-SafeCompress: Defending

Membership Inference Attacks based on
SafeCompress

We implement a concrete safe model compression mechanism
against MIA, called MIA-SafeCompress based on SafeCompress.

Stage 1. Sparsity-Aware Model Initialization. Given a big
4.3.1
model, we adopt the Erdös–Rényi [7, 47] initialization method to
reach a predefined sparsity requirement, i.e., removing a number
of model connections (assigning zeros to the connection weights).
Specifically, for the 𝑘-th layer with 𝑛𝑘 neurons, we collect them in
𝑛𝑘 ]. Usually, 𝑉 𝑘 in
a vector and denote it as 𝑉 𝑘 = [𝑣𝑘
1
the 𝑘-th layer is connected with last layer 𝑉 𝑘−1 via a weight matrix
𝑊 𝑘 ∈ R𝑛𝑘 ×𝑛𝑘−1 . In the sparse setting, the matrix degenerates to a

, ......𝑣𝑘

, 𝑣𝑘
2

, 𝑣𝑘
3

Erdös–Rényi random graph, where the probability of connection
between neuron 𝑉 𝑘 and 𝑉 𝑘−1 is decided by:

𝑃 (𝑊𝑖 𝑗 ) =

𝜖 × (𝑛𝑘 + 𝑛𝑘−1)
𝑛𝑘 × 𝑛𝑘−1

,

(5)

where 𝜖 is a coefficient that is adjusted to meet a target sparsity.
In general, this distribution is inclined to allocate higher sparsity
(more zeros) to the layers with more parameters as the probability
tends to decline while parameters increase.5

4.3.2
Stage 2. Candidate Sparse Model and Simulated Attacker Gen-
eration. During this stage, we train the input sparse model for a
predefined number of iterations (following the setting in [38], the
iteration number is set to 4,000). Once finished, the well-trained
sparse model, denoted as M𝑆 , is fed into two branches.

Branch 1. Dynamic sparse update. In the first branch, i.e., the
dynamic sparse update branch, we apply two state-of-the-art prun-
ing strategies and two growth strategies to operate on M𝑆 , leading
to four (2∗2) different sparse typologies. The two pruning strategies
are magnitude-based pruning [10, 20] and threshold-based prun-
ing [21]. Magnitude-based pruning removes the connections with
the smallest weight magnitudes; threshold-based pruning removes
the connections whose weight magnitudes are below a given thresh-
old. The two growing strategies are gradient-based growth [12, 19]
and random-based growth [46]. The gradient-based growth reac-
tivates connection (weight) that has a large gradient | 𝜕 L
𝜕𝑤 |, while

5For the weights of remained connections, we can keep those of the big model, or
simply do random initialization. In our experiments, we find that keeping big model
weights performs not better than random initialization. Hence, we adopt random
initialization in our implementation.

PruneGrowUpdatePruneGrowUpdateStrategy 1Strategy 1Strategy MStrategy MStrategy NStrategy NStrategy 1Strategy MStrategy N× NDynamic sparse updateCandidate Sparse Model and Simulated Attacker GenerationSafety Test-driven Model SelectionSparse Model TrainingSparsity-Aware Model InitializationSparsity-Aware Model InitializationRetrain1) ……2) ……SelectSafety & Perfor-mance  TestsSafety & Perfor-mance  TestsRecord............Model FlowMerge FlowUpdate StrategySimulation  MethodInitializeInitializeAttack mechanism simulationAttackerAttackerSimulateAttack mechanism simulationAttackerSimulateSafety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software DeploymentASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Then, by considering the attack accuracy (safety) and task perfor-
mance together, we can select the best candidate model. In our
implementation, we use a newly-defined TM-score (i.e., Task per-
formance divided by MIA attack accuracy, details in Sec. 4.3.4) for
model selection. The candidate sparse model with the highest TM-
score is selected. The selected model is then sent back to Stage 2
and a new iteration starts. The whole process will terminate after
repeating a predefined number of iterations.

4.3.4 Metric. For performance, we evaluate model accuracy on
classification tasks, represented by Task Acc. The higher the Task
Acc is, the better the model performs. For safety, we adopt the
membership inference attack accuracy, denoted as MIA Acc, to
reflect defense ability. The lower the MIA Acc is, the stronger the
model’s defense ability is. Besides, we take both performance and
safety into consideration, and design a metric called TM-score that
aims to directly evaluate the performance-safety trade-off:

𝑇 𝑀-𝑠𝑐𝑜𝑟𝑒 =

(𝑇 𝑎𝑠𝑘 𝐴𝑐𝑐)𝜆
𝑀𝐼𝐴 𝐴𝑐𝑐

,

(6)

where 𝜆 is a coefficient to control this trade-off and we set 𝜆 = 1 in
our method for simplicity. If a model keeps high Task Acc and low
MIA Acc, the resulted TM-score will be high. In brief, this metric is
in line with our goal to seek a model with both good performance
and strong safety.

4.3.5 Enhancement with Entropy-based Regularization. MIA mech-
anisms usually perform inference attac ks by taking the target
model’s output probabilities as inputs. Consequently, we consider
to conceal the discriminative information by increasing the uncer-
tainty in output probability. In information theory, entropy has
been employed to describe the degree of uncertainty. For a c-class
classification task, given its output probability distribution, the
entropy is formulated as:

𝐸 = −

𝑐−1
∑︁

𝑖=0

𝑝𝑖 log(𝑝𝑖 )

(7)

It is easy to know that 𝐸 gets the maximum value when 𝑝𝑖 is equal
to 1
𝑐 . Intuitively, it is hard for a classifier to distinguish the right
category if the output possibility distribution is not biased. In the
worst case when each output class has an equal probability, it
degenerates to a random guess. Fortunately, this behavior meets
our expectations when defending against MIA. In other words,
increasing entropy could enhance the target model’s ability to
defend against MIA. Hence, we define a new task model training
loss as follows:

L = L𝑟 − 𝛽 ·

1
𝑁

𝑁 −1
∑︁

𝑗=0

𝐸 𝑗 ,

(8)

where L𝑟 is a classification loss (e.g., cross-entropy), 𝛽 is a coeffi-
cient to weight the entropy term, and 𝑁 is the batch size. We name
this loss re1. We set 𝛽 to 0.1 and 𝑁 to 128 in our implementation.
Further, we propose another loss variant called re2. Inspired by
previous works [34, 51], we design a fine-grained regularization
term that considers the classification result of each sample, while
re1 just operates on every sample no matter whether it is classified
correctly. Specifically, we denote Φ as a set where misclassified

Figure 2: The architecture of the simulated MIA neural net-
work model. Each layer is fully connected to its subsequent
layer. 𝐶 is the category number.

the random-based growth randomly reactivates connection. After-
ward, we fine-tune these derived sparse models and generate four
𝑆 .6
candidate sparse models M1
𝑆 , ..., M4

Branch 2. Attack mechanism simulation. We try to simulate
an external attacker in preparation for safety tests in the second
branch. Specifically, we follow the previous MIA work [49] and
simulate the MIA attacker with a fully connected neural network,
as depicted in Figure 2. The simulated attacker contains three parts:
probability stream, label stream, and fusion stream. The probability
stream processes the output probability from the target sparse
model M𝑆 (𝑥). The label stream deals with the label 𝑦 of the sample
𝑥. Then, the fusion stream fuses the features extracted from the
above two streams and outputs a probability to indicate whether the
sample is used in training or not. Note that to improve the attacker
simulation efficiency, we do not independently train an attacker
for each candidate sparse model M𝑖
𝑠 in Branch 1. Instead, we first
train a simulated attacker A based on M𝑆 ; then, for each M𝑖
𝑠 , we
fine-tune A for several epochs to ensure its attack effectiveness
toward M𝑖
𝑠 . This training acceleration strategy is sensible as M𝑖
𝑠
often deviates from M𝑠 only marginally.

Finally, four sparse model variants (Branch 1) and corresponding
well-trained simulated attackers (Branch 2) are sent to the next
stage for safety tests.

Stage 3. Safety Test-driven Model Selection. Safety test-driven
4.3.3
model selection aims to choose the candidate sparse model with
the best trade-off in task performance and safety protection. Specif-
ically, we employ the simulated attacker to conduct MIA safety
tests on four candidate sparse models and then record the attack
accuracy. Subsequently, we can simply select the one whose attack
accuracy is the lowest (strong defense ability). However, this man-
ner ignores the task performance, thereby missing a comprehensive
consideration. Hence, we also conduct task performance tests on
four sparse models (e.g., image classification accuracy for CV tasks).

6As more advance pruning or growth strategies may be proposed in the future, Safe-
Compress is easy to incorporate them by adding to/replacing existing strategies.

·Concat128 × 256C × 10241024 × 512512 × 64C × 10241024 × 512512 × 64512 × 64C × 512512 × 64C × 512256 × 6464 × 1256 × 6464 × 1Probability StreamLabel StreamFusion Stream𝑓(𝑥) y·Concat128 × 256C × 10241024 × 512512 × 64512 × 64C × 512256 × 6464 × 1Probability StreamLabel StreamFusion Stream𝑓(𝑥) yASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Jie Zhu, Leye Wang, and Xiao Han

samples are collected in a batch 𝑁 . Thus, re2 is formulated as:

Table 1: Number of samples in dataset splits.

L = L𝑟 − 𝛽 ·

1
∥Φ∥

∑︁

𝑗 ∈Φ

𝐸 𝑗 ,

(9)

Datasets

where ∥Φ∥ is the size of set Φ. We keep the same setting as re1 for
𝛽 and 𝑁 .

4.4 Configuring SafeCompress to Other

Attacks

To adapt MIA-SafeCompress to other attacks, the main modification
is using another attack mechanism instead of MIA in Branch 2 of
Stage 2. Some examples are listed as follows.

White-box MIA. Different from our implemented black-box
MIA, white-box MIA assumes that attackers have extra knowledge
about the target model beyond outputs (e.g., hidden layer parame-
ters [56] and gradient descents in training epochs [50]). Then, to
implement a white-box MIA mechanism, we can just include the
extra information as inputs to simulate the attacker mechanism A.
In Appendix, we have conducted a preliminary experiment to verify
the feasibility of extending our framework to white-box MIA.

Attribute Inference Attack. Attribute inference attack (AIA)
aims to infer private information of a user (sample), such as age
and location [22]. Suppose that the original big model (e.g., auto-
encoder [4]) is trained to obtain a representation from a user’s
movie ratings to serve downstream tasks like recommendation; this
representation may leak the user’s private information [31]. Then,
we can simulate the attacker A as a specific AIA method (e.g., logis-
tic regression [31]), or a basket of multiple AIA methods [22]. Note
that the safety and performance metrics also need to be modified.
For safety, the metric depends on the private attribute — e.g., to
protect age inference, the metric could be set to mean absolute error.
For performance, we can list several downstream tasks and then
evaluate the learned representations’ effects on these tasks.

5 EXPERIMENTAL SETUP
5.1 Datasets and Models
We conduct experiments on five datasets. For each dataset, we select
one model widely used for the dataset task as the original big model.
The training/test data partition setting follows the reference papers.
CIFAR10 and CIFAR100 [32] are two benchmark datasets for
image classification. Both of them have 50, 000 training images and
10, 000 test images. CIFAR10 has 10 categories while CIFAR100
has 100 categories. The size of every image is 32 × 32. We adopt
AlexNet [33] for CIFAR10 and VGG16 [60] for CIFAR100.

Tinyimagenet [35] is another image dataset that contains 200
categories. Each category includes 500 training images and 50 test
images. The size of each image is 64 × 64. We adopt ResNet18 [23]
for Tinyimagenet.

Yelp-5 [69] is a review dataset for sentiment classification (5
categories). It includes 130, 000 training and 10, 000 test texts per
class. We adopt BERT [14] for Yelp-5.

AG-News [70] is a topic classification dataset. It has 4 classes.
For each class, it contains 30,000 training and 1,900 test texts. We
adopt Roberta [40] for AG News.

Dataset Splits for Attacker Simulation. We split experimen-
tal datasets as shown in Table 1 to evaluate the model defense ability.

Attack Evaluation

Attack Training
𝐷𝑘𝑛𝑜𝑤𝑛
𝐷𝑘𝑛𝑜𝑤𝑛
𝑡𝑒𝑠𝑡
𝑡𝑟𝑎𝑖𝑛
25,000
25,000
50,000

5,000
5,000
5,000

𝐷𝑢𝑛𝑘𝑛𝑜𝑤𝑛
𝑡𝑟𝑎𝑖𝑛
25,000
25,000
50,000

325,000
60,000

25,000
3,800

325,000
60,000

𝐷𝑢𝑛𝑘𝑛𝑜𝑤𝑛

𝑡𝑒𝑠𝑡

5,000
5,000
5,000

25,000
3,800

CIFAR10
CIFAR100
Tinyimagenet

Yelp-5
AG-News

Table 2: Two-step baselines.

Model Compression

MIA Defense

Pruning

KD

(cid:33)
(cid:33)
(cid:33)

(cid:33)
(cid:33)
(cid:33)

DP

(cid:33)

(cid:33)

AdvReg

DMP

(cid:33)

(cid:33)

(cid:33)

(cid:33)

Baselines

Pr-DP
Pr-AdvReg
Pr-DMP
KD-DP
KD-AdvReg
KD-DMP

Following previous works [58, 64, 72], we assume the simulated
attacker knows 50% of the (target) model’s training data and 50%
of the test data (non-training data), which are used for training the
attack model. The remaining datasets are adopted for evaluation.

5.2 Baselines
While few existing studies focus on safe model compression, we
thus formulate several two-step baseline methods by combining
state-of-the-art model compression and MIA defense techniques.
In the first step, we need to choose one compression method
C. We consider two widely used and effective choices to compress
big models, i.e., pruning [20] and knowledge distillation (KD) [24].
For pruning, We adopt the ‘pretrain→prune→fine-tune’ paradigm.
Firstly, we pretrain a big model from scratch. Then we leverage
magnitude-based pruning and compress this full model to a certain
sparsity. Then we fine-tune the pruned model to recover model
performance. For KD, we design a small dense model as the student
model given the sparsity requirement. Then, we use the well-trained
big model as the teacher model to help the student model’s training.
In the second step, we should select one MIA defense method
D. We test three defense algorithms including differential privacy
(DP) [1], adversary regularization (AdvReg) [49], and distillation for
membership privacy (DMP) [58]. DP adds noise to gradients while
training. AdvRes adds membership inference gain 𝐺𝐴 of the attacker
to the loss function of the target model as a regularization term,
thereby forming a min-max game. DMP first trains an unprotected
(vanilla) teacher model. Then, it utilizes extra unlabeled data and
adopts knowledge distillation to force the student i.e., the target
model, to simulate the output of this teacher.

Finally, we group two-step techniques by enhancing the training
or fine-tuning process in C with the defense technique in D. For ex-
ample, the Pr-DP baseline adopts the ‘pretrain→prune→DP-based
fine-tune’ process; KD-AdvReg trains the student compressed model

Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software DeploymentASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Table 3: Comparison with different regularization methods.
The best results are marked in bold.

Regularization no

re1

re2

L2

Task Acc
MIA Acc

TM-score

69.52% 69.89% 69.91% 68.86 %
51.75% 51.94% 51.54% 53.33%
1.34

1.29

1.35

1.35

Figure 3: Visualization of the learning procedure.

with the help of the big teacher model in a min-max game manner.
All the six two-step baselines are listed in Table 2.

Baseline 7 (MIA-Pr [64]). This baseline is originally proposed
to defend MIA, and its main technique is pruning. Hence, although
the paper [64] does not specify the usage for model compression,
it can naturally reduce the model size. We use this method as a
baseline to prune the big model until satisfying the sparsity require-
ment. Note that when pruning the big model, MIA-Pr optimizes
the MIA defense effect, while the two-step baseline Pr-X (the first
step is pruning) optimizes the task performance (e.g., classification
accuracy).

5.3 Implementation Details
We perform all the experiments using Pytorch 1.8. on Ubuntu 20.04.
For CV tasks, we use NVIDIA 1080Ti with CPU of Intel Xeon Gold
5118 (4 cores, 2.3GHz) and 16GB memory. We train big models with
batch size 128 for 200 epochs. For NLP tasks, we use NVIDIA 3090
with CPU of Intel Xeon Gold 5218 (6 cores, 2.3GHz) and 40 GB
memory. The batch size is 256 and the training lasts for 10 epochs.
In MIA-SafeCompress, we adopt SGD as the optimizer to train
the target sparse model. We set the learning rate to 0.1 and adjust it
via a multi-step decay strategy. To simulate the attacker, following
previous work [49], we adopt ReLu as the activation function in our
attack neural network model. All the network weights are initialized
with normal distribution with mean 0 and standard deviation 0.01,
and all biases are set to 0 by default. The batch size is 128. We use
the Adam optimizer with the learning rate of 0.001 all the time and
we train the attack model 100 epochs. During the training process,
we ensure that every training batch contains the same number
of member and non-member data samples, aiming to prevent the
attack model from being biased toward either side.

6 EXPERIMENTS
6.1 Learning Procedure Visualization
To better understand the learning procedure of MIA-SafeCompress,
we train VGG16 on CIFAR100 with sparsity set to 0.05, and depict
how loss, Task Acc, and MIA Acc vary when the model training and
updating continues, as shown in Figure 3. For each sparse model, we
train about 10 epochs (i.e., about 4,000 iterations with batch size 128)
and then update its sparse structure according to safety tests. That is,
10-epoch training usually triggers one update. We can observe that
the model gradually converges as training progresses. Specifically,
by following the DST manner, MIA-SafeCompress enables the target
model to explore as many candidate sparse structures as possible.
Then, with the help of safety tests, the sparse model structure can
generally evolve into better ones (with higher Task Acc and lower
MIA Acc) step by step.

6.2 Regularization Term Selection
To decide the regularization term adopted in MIA-SafeCompress, we
conduct the experiments on CIFAR100. The results are reported in
Table 3. We also compare our method with widely-used L2 regular-
ization. Results show that our proposed re2 regularization works
the best. Therefore, we choose re2 as an optional regularization
term and report its results in our following experiments.

6.3 Main Results on CV Datasets
The results of three CV datasets are illustrated as follows.

6.3.1 CIFAR100 (VGG16). Firstly, we report the Task Acc and MIA
Acc on CIFAR100 with sparsity 0.05 using VGG16. As illustrated in
Figure 4, the results indicate that MIA-SafeCompress outperforms
seven baselines in Task Acc while alleviating MIA risks remarkably.
Specifically, MIA-SafeCompress produces 51.75% for MIA Acc (just
a bit higher than random guess), decreasing by 28.27%, 10.9%, 6.03%,
26.21%, 4.86%, 17.22%, and 11.13% compared with seven baselines,
respectively. Moreover, equipping MIA-SafeCompress with re2 leads
to an additional reduction of 0.21% in MIA Acc while Task Acc
rises 0.39%. Interestingly, when compared to the uncompressed
VGG16, SafeCompress reduces MIA risks to a large extent while
sacrificing Task Acc only a little. This highlights the practicality of
SafeCompress in generating a small and safe model with competitive
task performance.

Further, to validate the effectiveness on different sparsity require-
ments, we report the results in Figure 5 with sparsity 0.1 and 0.2.
Consistent with the results of sparsity 0.05, MIA-SafeCompress still
beats all the baselines by achieving higher Task Acc and lower MIA
Acc. We also observe that when sparsity increases, Task Acc of
MIA-SafeCompress increases from 69.52% (sparsity 0.05) to 71.96%
(sparsity 0.2); meanwhile, MIA Acc also goes up from 51.75% (spar-
sity 0.05) to 58.85% (sparsity 0.2), indicating that the compressed
model becomes more vulnerable with increasing sparsity. Such a
phenomenon may be incurred by the extra information (i.e., more
parameters) kept in the compressed model when sparsity rises —
some of the extra information may be generalizable so that task
performance is enhanced; other information might be specific to
training data, thus leading to higher MIA risks.

Finally, to overview the performance-safety trade-off in one
shot, we report TM-score (Task Acc divided by MIA Acc, Eq. (6))
in Table 4. Clearly, MIA-SafeCompress (re2) outperforms all the
baselines significantly and consistently under three sparsity settings
by making a better trade-off between performance and safety.

6.3.2 CIFAR10 (AlexNet). The results on CIFAR10 are presented
in Table 5. It can be seen that MIA-SafeCompress obtains almost
the best performance in Task Acc and maintains a pretty strong

                    D   H S R F K V                / R V V             7 D V N  $ F F                         0 , $  $ F F                   b   H S R F K V c  update times  5ASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Jie Zhu, Leye Wang, and Xiao Han

experiment results), the time consumption of MIA-SafeCompress is
generally acceptable in practice. Note that we only need to run MIA-
SafeCompress once to get the compressed model, and the model can
then be deployed in hundreds of thousands of devices repeatedly.

6.5 Flexibility with Other Training Tricks
It is worth noting that SafeCompress is also flexible to incorporate
other training tricks not mentioned in previous experiments. To il-
lustrate this flexibility, we try to incorporate five other widely-used
training tricks in MIA-SafeCompress, including dropout [62], data
augmentation (Aug, including random cropping, resizing, and flip-
ping), adversary regularization (AdvReg), sparse model initialization
with the big model’s parameter values (BigPara), and knowledge distil-
lation (KD) where the sparse model’s prediction score approximates
the big model’s [24]. Table 8 shows the results of CIFAR100.

Compared to our MIA-SafeCompress implementation in main
experiments (i.e., re2), we find that most training tricks achieve sim-
ilar TM-scores. This indicates that SafeCompress is compatible with
most training tricks. Then, in practice, given a specific dataset/task,
we could enumerate different combinations of training tricks and
find the best one in the SafeCompress framework.

Another interesting observation is that data augmentation sig-
nificantly increases MIA Acc (i.e., reduces safety) for the final com-
pressed model. The possible reason is that data augmentation allows
training samples to be memorized more easily, as the variants of
original samples (e.g., flipping and resizing) are also used for train-
ing. Notably, while not focusing on compressed sparse models, a
recent paper [67] also points out that data augmentation may lead
to a significantly higher MIA risk. Inspired by [67], SafeCompress
may also be extended to a useful framework to benchmark different
training tricks’ impacts on compressed models’ safety, which would
be an interesting future direction.

6.6 Results on NLP Datasets
To further validate the effectiveness and generalization of MIA-
SafeCompress, we conduct experiments on two NLP datasets.

6.6.1 Yelp-5 (BERT). The results are reported in Table 9. It can
be seen that MIA-SafeCompress produces a competitive Task Acc.
At the same time, it decreases MIA Acc by 6.78% compared to the
uncompressed model, outperforming all the baselines. When attach-
ing re2 to MIA-SafeCompress, we get the lowest MIA Acc. Besides,
MIA-SafeCompress achieves the highest TM-score, indicating its
ability to balance task performance and safety.

Figure 4: Results on CIFAR100 with sparsity 0.05 (MS: MIA-
SafeCompress, dash line: uncompressed VGG16).

Table 4: TM-score on CIFAR100. The best results are marked
in bold.

Method
VGG16 (uncompressed)

Pr-DP
Pr-AdvReg
Pr-DMP
KD-DP
KD-AdvReg
KD-DMP
MIA-Pr

MIA-SafeCompress
+ re2

Sparsity=0.05 Sparsity=0.1 Sparsity=0.2

1.08

0.39
1.08
1.10
0.47
1.13
0.91
1.09

1.34
1.35

1.08

0.41
0.99
1.22
0.47
1.13
0.93
1.06

1.35
1.37

1.08

0.47
1.06
1.21
0.48
0.90
0.84
1.05

1.22
1.28

defensive ability when sparsity is 0.05. Thanks to the excellent ef-
fectiveness in both aspects, MIA-SafeCompress produces the highest
TM-score, showing its outstanding ability to make the performance-
safety trade-off. When the sparsity is set to 0.1, our method produces
85.31% for Task ACC, slightly inferior to the best result (86.84%)
produced by KD-AdvReg. However, MIA-SafeCompress decreases
MIA Acc to 52.37%, much lower than KD-AdvReg (55.59%), leading
to the highest TM-score again.

6.3.3 Tinyimagenet (ResNet18). We present the results of Tinyima-
genet in Table 6. Excitingly, MIA-SafeCompress produces the best
accuracy for Task Acc in both sparsity 0.05 and 0.1, outperforming
all the baselines by a large margin. Also, MIA-SafeCompress obtains
competitive MIA Acc in both sparsity settings compared to the
best defense effects of baselines. Specifically, MIA-SafeCompress
achieves 52.46% (sparsity 0.05) and 52.79% (sparsity 0.1) in MIA
Acc; The best MIA Acc results among baselines are 52.32% (sparsity
0.05) and 52.44% (sparsity 0.1). The gaps are tiny (0.14% and 0.35%).
When employing re2, the MIA Acc of MIA-SafeCompress declines
further, leading to the lowest MIA Acc. Then, MIA-SafeCompress
(re2) achieves the best TM-score in both sparsity settings.

6.4 Time Consumption
Table 7 reports the time consumption for each method to get a
compressed model (sparsity = 0.05) on CIFAR100. In general, al-
most all the methods (except Pr-DP) can finish compressing in 1–5
hours, and the time consumption of MIA-SafeCompress is compa-
rable to that of others. Considering that MIA-SafeCompress can
achieve the best performance-safety balance (from our previous

6.6.2 AG-News (Roberta). As indicated in Table 10, MIA-SafeCompress
achieves 87.5% for Task Acc, slightly inferior to the highest 88.10%
(KD-AdvReg). However, our method decreases MIA Acc to 55.28%,
much lower than KD-AdvReg (56.56%). Moreover, MIA-SafeCompress
outperforms all the baselines in MIA Acc, thus resulting in a high
TM-score (1.58). Moreover, incorporating MIA-SafeCompress with
re2 leads to a further enhancement in all the metrics.

31.5580.0267.9662.6563.5357.7836.7477.9664.156.6162.9268.9768.7362.8869.5251.7569.9151.54Task Acc %MIAs Acc %VGG16 on CIFAR100 with Sparsity 0.05 Pr-DPPr-AdvRegPr-DMPKD-DPKD-AdvRegKD-DMPMIA-PrMSMS+re272.6467.33Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software DeploymentASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Figure 5: Results on CIFAR100 with sparsity 0.1 and 0.2 (MS: MIA-SafeCompress, dash line: uncompressed VGG16).

Table 5: Task Acc (performance), MIA Acc (safety) and TM-score results on CIFAR10. The best results are marked in bold.

Method

AlexNet (uncompressed)

Sparsity=0.05

Sparsity=0.1

Task Acc
87.41%

MIA Acc
58.34%

TM-score
1.50

Task Acc
87.41%

MIA Acc
58.34%

TM-score
1.50

Pr-DP
Pr-AdvReg
Pr-DMP
KD-DP
KD-AdvReg
KD-DMP
MIA-Pr

MIA-SafeCompress
+ re2

54.57%
82.72%
81.66%
72.09%
80.47%
82.23%
82.26%

83.94%
84.00%

68.35%
52.74%
55.63%
59.94%
54.38%
61.63%
53.15%

52.97%
53.12%

0.80
1.57
1.47
1.20
1.48
1.33
1.55

1.59
1.58

62.87%
86.19%
83.73%
73.27%
86.84%
86.26%
86.11%

85.31%
85.38%

66.49%
54.83%
54.05%
60.42%
55.59%
63.45%
56.12%

52.37%
53.17%

0.95
1.57
1.55
1.21
1.56
1.36
1.53

1.63
1.61

Table 6: Task Acc (performance), MIA Acc (safety) and TM-score results on Tinyimagenet. The best results are marked in bold.

Method

ResNet18 (uncompressed)

Sparsity=0.05

Sparsity=0.1

Task Acc
65.48%

MIA Acc
69.73%

TM-score
0.94

Task Acc
65.48%

MIA Acc
69.73%

TM-score
0.94

Pr-DP
Pr-AdvReg
Pr-DMP
KD-DP
KD-AdvReg
KD-DMP
MIA-Pr

MIA-SafeCompress
+ re2

19.26%
60.10%
55.56%
17.10%
52.34%
53.71%
58.36%

63.81%
63.45%

71.07%
57.62%
60.03%
52.32%
53.27%
57.18%
57.92%

52.46%
51.27%

0.27
1.04
0.92
0.33
0.98
0.94
1.01

1.22
1.24

24.56%
61.16%
59.61%
17.45%
54.48%
57.16%
60.91%

65.15%
65.15%

74.17%
63.93%
64.56%
52.44%
53.60%
55.65%
61.03%

52.79%
52.32%

0.33
0.96
0.92
0.33
1.02
1.03
1.00

1.24
1.25

Table 7: Time consumption on CIFAR100. VGG16 is the uncompressed model. Others are compressed with sparsity 0.05.

Method
Time (h)

VGG16 (uncompressed)
1.05

Pr-DP

Pr-AdvReg

Pr-DMP

KD-DP

KD-AdvReg

KD-DMP MIA-Pr MIA-SafeCompress (re2)

16.90

2.92

4.75

4.89

2.22

1.42

2.37

3.98

7 RELATED WORK
7.1 Membership Inference Attacks and

Defenses

Membership inference attacks (MIAs) [59], which aim to infer
whether a data record is used to train a model or not, have the

39.878570.7366.5267.7355.9938.3679.7368.2575.5467.3180.1669.1765.9971.9658.8571.6755.94Task Acc %VGG16 on CIFAR100 with Sparsity 0.2 MIA Acc %34.9984.468.8569.4665.8153.8437.2579.7165.6658.1164.5569.7269.8965.7471.6753.1371.652.24Task Acc %VGG16 on CIFAR100 with Sparsity 0.1 MIA Acc %72.6472.6467.3367.3367.3367.33ASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Jie Zhu, Leye Wang, and Xiao Han

Table 8: Using other training tricks in MIA-SafeCompress.

Table 9: Task Acc, MIA Acc, and TM-score on Yelp-5.

Trick
Task Acc 69.43%
MIA Acc 53.94%
TM-score 1.29

Dropout Aug

AdvReg BigPara KD

re2

62.03% 68.86% 68.52% 70.02% 69.91%
65.46% 52.96% 52.05% 53.10% 51.54%
0.95

1.30

1.32

1.32

1.35

potential to raise severe privacy risks to individuals. A prevalent
attack fashion is to train a neural network via multiple shadow train-
ing [49, 59], converting the task of recognizing member and non-
member of training datasets to a binary classification problem. Un-
like binary classifier-based MIA, another attack form, metric-based
MIA that just computes metrics (e.g., prediction correctness [66]
or confidence [57], entropy [61], etc.) is simpler and less computa-
tional. However, the performance of metric-based attacks is inferior
to classifier-based attacks. Recently, as more and more efforts have
been devoted to the field, various defense methods have been pre-
sented. For example, differential privacy [1, 53] (known as DP)
interrupts the attack model by adding noise to the learning object
or output of the target model. But the cost between utilization
and defense is unacceptable [28]. Adversarial regularization [49],
known as AdvReg, combines target model training process with
attack model, formulating a min-max game. To improve the model
utility, Jia et al. introduce Memguard [29] which adds carefully
computed noise to output of a model, aiming to defend the attack
model while keeping performance. However, it is vulnerable to the
threshold-based attack [61]. More recently, model compression tech-
nologies (e.g., knowledge distillation [24] and pruning [20]) have
been employed to protect member privacy. Based on knowledge
distillation, Shejwalkar and Houmansadr [58] propose Distillation
For Membership Privacy (DMP) defense method, where a teacher
is trained to label an unlabeled reference dataset, and those with
low prediction entropy are selected to train the target model. So it
requires an extra unlabeled reference dataset. Further, Zheng et al.
propose complementary knowledge distillation (CKD) and pseudo
complementary knowledge distillation (PCKD) [72], eliminating
the need for additional public data by just transferring knowledge
from a private training set. However, as knowledge distillation is
an indirect learning strategy [9], some critical information may be
lost during mimicking the teacher. Afterward, in [64], pruning is
adopted to mitigate MIA while reducing model size simultaneously.
But it mainly focuses on preventing membership inference attacks
without explicitly considering the memory restriction. Besides,
Yuan & Zhang [68] show that pruning makes the divergence of
prediction confidence and prediction sensitivity increase and vary
widely among the different classes of member and no-member data.
This characteristic may be manipulated by attackers. Our method is
different from these works that mainly focus on defending against
MIA. This research tries to address safe model compression prob-
lem. More specifically, we aim to decrease the risk of privacy attack
(e.g. MIA) and keep excellent performance when compressing a
model. It is a bi-objective optimization problem.

7.2 Model Compression
Due to limited memory and computation resources, model compres-
sion [20] plays a crucial role, especially when transformer-based big
models [14, 15, 43] become the mainstream. To alleviate the issue,

Method

BERT (Uncompressed)

Pr-DP
Pr-AdvReg
Pr-DMP
KD-DP
KD-AdvReg
KD-DMP
MIA-Pr

MIA-SafeCompress
+ re2

Sparsity=0.5

Task Acc

MIA Acc

TM-score

62.21%

60.77%
61.17%
61.32%
58.21%
61.38%
59.69%
61.80%

61.66%
61.61%

71.15%

64.79%
73.34%
66.41%
65.36%
67.40%
70.25%
73.85%

64.37%
64.24%

0.87

0.94
0.83
0.92
0.89
0.91
0.85
0.84

0.96
0.96

Table 10: Task Acc, MIA Acc, and TM-score on AG-News.

Method

Sparsity=0.5

Task Acc MIA Acc

TM-score

Roberta (Uncompressed)

89.20%

57.59%

Pr-DP
Pr-AdvReg
Pr-DMP
KD-DP
KD-AdvReg
KD-DMP
MIA-Pr

MIA-SafeCompress
+ re2

87.28%
87.24%
86.38%
82.64%
88.10%
87.24%
87.91%

87.50%
87.61%

56.07%
57.58%
56.31%
56.13%
56.56%
56.64%
57.04%

55.28%
54.79%

1.55

1.56
1.52
1.53
1.47
1.56
1.54
1.54

1.58
1.60

various methods have been proposed. For example, pruning [20, 42],
as a direct and effective method, removes unimportant weights or
structures according to certain criteria (e.g., weight magnitude).
Knowledge distillation [24, 30], known as KD, transforms knowl-
edge from a big model (we call it teacher) to a small model (we call
it student) during training. Determining how to design a student
model and distill knowledge are two essential questions to be an-
swered while applying KD. Quantification [8, 21] is much simpler,
which converts a long storage width in memory to a shorter one.
For example, it could convert float (64 bit) to an 8-bit integer [26],
even a binary (1 bit) [25, 36]. In addition, dynamic sparse training
(denoted as DST), as a new compression method, is first proposed
in [46] and achieves surprising performance, attracting much atten-
tion from researchers. Follow-up works further introduce weight
redistribution [12, 48], gradient-based weight growth [12, 19], and
extra weights update in the backward pass [27, 54] to improve
the sparse training performance. More recently, by developing an
independent framework, a truly sparse neural network without
masks with over one million neurons can be trained on a typical
laptop [37]. Hence, the DST paradigm has shown great potential
to deploy neural network models into edge devices at a large scale.
However, while there exist numerous works in model compression
that balance size and performance, privacy safety is not well con-
sidered. Differing from these works, our work considers both safety
and performance during model compression.

Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software DeploymentASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

8 LIMITATIONS AND FUTURE WORK
As one of the pioneering studies toward safe model compression,
we have also identified several future work possibilities that may
attract more effort to this direction.

Integrating heterogeneous attacks. In practice, AI software
would face various types of attacks. The SafeCompress framework
follows the attack configurability principle to be able to be config-
ured against different attacks, but it does not include an integration
component to simultaneously consider heterogeneous attacks cur-
rently. A straightforward way is introducing multiple safety tests
regarding heterogeneous attacks, and then using the weighted sum
to combine these test measurements; the pitfall is the hardness of
deciding the weights. More advanced strategies, such as bootstrap-
ping according to different attacks’ severity and commonality [22],
may deserve future research.

Benchmarking performance-safety trade-offs between mod-

els. In this work, for any dataset, we only use one state-of-the-art
model as the original input big model. Prior research has pointed out
that, when two models do the same task with similar performance,
the model with more parameters may face higher MIA risks [50].
Then, if we compress the two models with the same sparsity restric-
tions, would the statement still stand or be overturned? Analyzing
this question would significantly help the model selection process
in AI software deployment.

9 CONCLUSION
In this paper, we present a performance-safety co-optimization
framework, called SafeCompress, to address the safe model com-
pression problem, as it is critical for current large-scale AI soft-
ware deployment. SafeCompress is a test-driven sparse training
framework, which can be easily configured to fight a pre-specified
attack mechanism. Specifically, By simulating the attack mecha-
nism, SafeCompress performs both safety and performance tests,
and iteratively updates the compressed sparse model. Based on
SafeCompress, we consider the membership inference attack (MIA),
a representative attack form, and implement a concrete instance
called MIA-SafeCompress against MIA. Extensive experiments have
been conducted using five datasets including three computer vision
tasks and two natural language processing tasks. The results verify
the effectiveness and generalization of our method. We also try
to incorporate other training tricks into MIA-SafeCompress and
elaborate on how to adopt SafeCompress to other attacks, showing
the flexibility of SafeCompress. As a pioneering study toward the
safe model compression problem, we expect that our research can
attract more effort to this promising direction in the new era when
AI software is becoming more and more prevalent.

10 ACKNOWLEDGEMENT
We thank the anonymous reviewers for their constructive com-
ments. We also thank Ruiqing Ding, Guanghong Fan, Duo Zhang,
and Xusheng Zhang for useful discussions. This work is supported
by the NSFC Grants no. 72071125 and 61972008.

Figure 6: The architecture of the simulated white-box MIA
neural network model. FC is fully connection layer. Conv
means convolution layer.

11 APPENDIX
In this section, we try to configure SafeCompress to another attack
called white-box membership inference attack. We name this in-
stance of SafeCompress framework as WMIA-SafeCompress. It is
worth noting that white-box MIA is different from black-box attack.
Specifically, the white-box setting allows an attacker to access to ex-
tra knowledge about the target model beyond outputs (e.g., hidden
layer parameters [56] and gradient descents in training epochs [50]).
Hence, it is regarded as a stronger attack manner.

Attacker Setting. We follow the configuration instruction pro-
vided in section 4.4. Concretely, we simulate the white-box at-
tacker proposed in previous works [41, 56] while maintaining other
settings in our paper. As illustrated in Figure 6, the simulated at-
tacker contained five parts: probability stream, loss stream, gradient
stream, label stream, and fusion stream. The first four streams are
designed to process four different inputs respectively. They are
the target sample’s ranked posteriors, loss value (e.g., classification
loss), gradients of the parameters of the target model’s last layer,
and one-hot encoding of its true label. The fifth part, as the name
suggests, fuses the extracted features from the first four streams
and outputs a probability to indicate whether the sample is used
in training or not. We use ReLU as the activation function for the
attack model.

Results. We choose the widely-used dataset CIFAR100 and the
widely-concerned neural model VGG16 in this field to conduct
the experiment with model sparsity set to 0.05. The results are
reported in Table 11. Our method produces 67.51% for Task Acc,
achieving a pretty competitive classification accuracy among all the
baselines. In addition, WMIA-Safecompress also decreases the MIA
Acc by 15.36% compared to the uncompressed VGG16. Although it
is a little bit inferior (2.21% lower) in MIA Acc than Pr-DMP, our

FCFCFCFCFCFCFCFCFCFCFCFCConvFCFCFCConvFCFCFCFCFCFCFCFCFCFCFCProbability StreamLoss StreamLabel StreamGradient StreamFusion StreamCatASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

Jie Zhu, Leye Wang, and Xiao Han

Table 11: Task Acc, MIA Acc, and TM-score on VGG16.

Method

Sparsity=0.05

Task Acc MIA Acc

TM-score

VGG16 (Uncompressed)

72.64%

71.58%

Pr-DP
Pr-AdvReg
Pr-DMP
KD-DP
KD-AdvReg
KD-DMP
MIA-Pr

WMIA-SafeCompress

31.55%
67.96%
63.53%
36.74%
64.10%
62.92%
68.73%

67.51%

84.18%
67.05%
54.01%
77.40%
61.27%
63.30%
66.77%

56.22%

1.01

0.37
1.01
1.18
0.47
1.05
0.99
1.03

1.20

method maintains more performance (3.98% higher) than Pr-DMP.
Finally, we also calculate TM-score for each method to show its
trade-off degree. It is observed that our method obtains the highest
TM-score (1.20). Such results are reasonable as our framework,
SafeCompress, always targets bi-objective (safety and performance)
optimization. Simultaneously, this experiment also indicates that
our framework truly enables generalization to other, even stronger
attacks.

REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In
Proceedings of the 2016 ACM SIGSAC conference on computer and communications
security. 308–318.

[2] Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep? Advances

in neural information processing systems 27 (2014).

[3] Kent L. Beck. 2003. Test-driven Development - by example. In The Addison-Wesley

signature series.

[4] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation
learning: A review and new perspectives. IEEE transactions on pattern analysis
and machine intelligence 35, 8 (2013), 1798–1828.

[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.

[6] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019.
The secret sharer: Evaluating and testing unintended memorization in neural
networks. In 28th USENIX Security Symposium (USENIX Security 19). 267–284.
[7] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang.
2021. Chasing sparsity in vision transformers: An end-to-end exploration. Ad-
vances in Neural Information Processing Systems 34 (2021).

[8] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.
2015. Compressing neural networks with the hashing trick. In International
conference on machine learning. PMLR, 2285–2294.

[9] Elliot J. Crowley, Gavin Gray, and Amos J Storkey. 2018. Moonshine: Distilling
with Cheap Convolutions. In Advances in Neural Information Processing Systems,
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(Eds.), Vol. 31. Curran Associates, Inc.

[10] Xiaoliang Dai, Hongxu Yin, and Niraj K. Jha. 2019. NeST: A Neural Network
Synthesis Tool Based on a Grow-and-Prune Paradigm. IEEE Trans. Comput. 68,
10 (2019), 1487–1497.

[11] By Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. 2020. Model
Compression and Hardware Acceleration for Neural Networks: A Comprehensive
Survey. Proc. IEEE 108 (2020), 485–532.

[12] Tim Dettmers and Luke Zettlemoyer. 2019. Sparse networks from scratch: Faster
training without losing performance. arXiv preprint arXiv:1907.04840 (2019).
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).

[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).

[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is
Worth 16x16 Words: Transformers for Image Recognition at Scale. in International
Conference on Learning Representations (ICLR) (2021).

[16] Milenko Drinić, Darko Kirovski, and Hoi Vo. 2007. PPMexe: Program compression.
ACM Transactions on Programming Languages and Systems (TOPLAS) 29, 1 (2007),
3–es.

[17] Jens Ernst, William S. Evans, Christopher W. Fraser, Todd A. Proebsting, and

Steven E. Lucco. 1997. Code compression. In PLDI ’97.

[18] Pauli Virtanen et al. 2020. SciPy 1.0: fundamental algorithms for scientific com-

puting in Python. Nature Methods 17 (2020), 261 – 272.

[19] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.
2020. Rigging the lottery: Making all tickets winners. In International Conference
on Machine Learning. PMLR, 2943–2952.

[20] Song Han, Huizi Mao, and William J Dally. 2016. Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman coding.
(2016).

[21] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights
and connections for efficient neural network. Advances in neural information
processing systems 28 (2015).

[22] Xiao Han, Hailiang Huang, and Leye Wang. 2019. F-PAD: Private attribute disclo-
sure risk estimation in online social networks. IEEE Transactions on Dependable
and Secure Computing 16, 6 (2019), 1054–1069.

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.

[24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the knowledge

in a neural network. arXiv preprint arXiv:1503.02531 2, 7 (2015).

[25] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Ben-
gio. 2016. Binarized neural networks. Advances in neural information processing
systems 29 (2016).

[26] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew
Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and
training of neural networks for efficient integer-arithmetic-only inference. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
2704–2713.

[27] Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen.
2020. Top-kast: Top-k always sparse training. Advances in Neural Information
Processing Systems 33 (2020), 20744–20754.

[28] Bargav Jayaraman and David Evans. 2019. Evaluating differentially private ma-
chine learning in practice. In 28th USENIX Security Symposium (USENIX Security
19). 1895–1912.

[29] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang
Gong. 2019. Memguard: Defending against black-box membership inference at-
tacks via adversarial examples. In Proceedings of the 2019 ACM SIGSAC conference
on computer and communications security. 259–274.

[30] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,
and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.
arXiv preprint arXiv:1909.10351 (2019).

[31] Michal Kosinski, David Stillwell, and Thore Graepel. 2013. Private traits and
attributes are predictable from digital records of human behavior. Proceedings of
the national academy of sciences 110, 15 (2013), 5802–5805.

[32] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features

from tiny images. (2009).

[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. Advances in neural information
processing systems 25 (2012).

[34] Agostina Larrazabal, Cesar Martinez, Jose Dolz, and Enzo Ferrante. 2021. Maxi-
mum Entropy on Erroneous Predictions (MEEP): Improving model calibration
for medical image segmentation. arXiv preprint arXiv:2112.12218 (2021).
[35] Ya Le and Xuan Yang. 2015. Tiny imagenet visual recognition challenge. CS 231N

7, 7 (2015), 3.

[36] Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks. arXiv preprint

arXiv:1605.04711 (2016).

[37] Shiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram
Matavalam, Yulong Pei, and Mykola Pechenizkiy. 2021. Sparse evolutionary
deep learning with over one million artificial neurons on commodity hardware.
Neural Computing and Applications 33, 7 (2021), 2589–2604.

[38] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. 2021.
Do we actually need dense over-parameterization? in-time over-parameterization
in sparse training. In International Conference on Machine Learning. PMLR, 6989–
7000.

[39] Yong Liu, Shuo Gao, Anbiao Huang, Jie Zhu, Lijun Xu, and Arokia Nathan. 2020.
Ensemble learning-based technique for force classifications in piezoelectric touch
panels. IEEE Sensors Journal 20, 16 (2020), 9540–9549.

[40] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A

Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software DeploymentASE ’22, October 10–14, 2022, Ann Arbor, Michigan, USA.

[65] Andrew Wolfe and Alex Chanin. 1992. Executing compressed programs on an

embedded RISC architecture. In MICRO 25.

[66] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy
risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE
31st computer security foundations symposium (CSF). IEEE, 268–282.

[67] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. 2021. How Does
Data Augmentation Affect Privacy in Machine Learning?. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 35. 10746–10753.

[68] Xiaoyong Yuan and Lan Zhang. 2022. Membership Inference Attacks and De-
fenses in Neural Network Pruning. arXiv preprint arXiv:2202.03335 (2022).
[69] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional
networks for text classification. Advances in neural information processing systems
28 (2015).

[70] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional
networks for text classification. Advances in neural information processing systems
28 (2015).

[71] Ziqi Zhang, Yuanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Xiangqun Chen,
Yao Guo, and Yunxin Liu. 2022. ReMoS: Reducing Defect Inheritance in Transfer
Learning via Relevant Model Slicing. In 44th International Conference on Software
Engineering (ICSE).

[72] Junxiang Zheng, Yongzhi Cao, and Hanpin Wang. 2021. Resisting membership
inference attacks through knowledge distillation. Neurocomputing 452 (2021),
114–126.

robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[41] Yugeng Liu, Rui Wen, Xinlei He, Ahmed Salem, Zhikun Zhang, Michael Backes,
Emiliano De Cristofaro, Mario Fritz, and Yang Zhang. 2021. ML-Doctor: Holistic
risk assessment of inference attacks against machine learning models. arXiv
preprint arXiv:2102.02551 (2021).

[42] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Chang-
shui Zhang. 2017. Learning efficient convolutional networks through network
slimming. In Proceedings of the IEEE international conference on computer vision.
2736–2744.

[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using
shifted windows. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 10012–10022.

[44] Liantao Ma, Xinyu Ma, Junyi Gao, Xianfeng Jiao, Zhihao Yu, Chaohe Zhang,
Wenjie Ruan, Yasha Wang, Wen Tang, and Jiangtao Wang. 2021. Distilling
knowledge from publicly available online EMR data to emerging epidemic for
prognosis. In Proceedings of the Web Conference 2021. 3558–3568.

[45] Decebal Constantin Mocanu et al. 2017. Network computations in artificial intelli-

gence. Technische Universiteit Eindhoven.

[46] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen,
Madeleine Gibescu, and Antonio Liotta. 2018. Scalable training of artificial neural
networks with adaptive sparse connectivity inspired by network science. Nature
communications 9, 1 (2018), 1–12.

[47] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen,
Madeleine Gibescu, and Antonio Liotta. 2018. Scalable training of artificial neural
networks with adaptive sparse connectivity inspired by network science. Nature
communications 9, 1 (2018), 1–12.

[48] Hesham Mostafa and Xin Wang. 2019. Parameter efficient training of deep convo-
lutional neural networks by dynamic sparse reparameterization. In International
Conference on Machine Learning. PMLR, 4646–4655.

[49] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine learning with
membership privacy using adversarial regularization. Proceedings of the ACM
Conference on Computer and Communications Security (2018), 634–646.

[50] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy
analysis of deep learning: Passive and active white-box inference attacks against
centralized and federated learning. In 2019 IEEE symposium on security and privacy
(SP). IEEE, 739–753.

[51] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey
Hinton. 2017. Regularizing neural networks by penalizing confident output
distributions. arXiv preprint arXiv:1701.06548 (2017).

[52] William W. Pugh. 1999. Compressing Java class files. In PLDI ’99.
[53] Md Atiqur Rahman, Tanzila Rahman, Robert Laganière, Noman Mohammed, and
Yang Wang. 2018. Membership Inference Attack against Differentially Private
Deep Learning Model. Trans. Data Priv. 11, 1 (2018), 61–79.

[54] Md Aamir Raihan and Tor Aamodt. 2020. Sparse weight activation training.
Advances in Neural Information Processing Systems 33 (2020), 15625–15638.
[55] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe
Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. Scaling
vision with sparse mixture of experts. Advances in Neural Information Processing
Systems 34 (2021).

[56] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and
Hervé Jégou. 2019. White-box vs black-box: Bayes optimal strategies for mem-
bership inference. In International Conference on Machine Learning. PMLR, 5558–
5567.

[57] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and

Michael Backes. 2019. In Network and Distributed System Symposium.

[58] Virat Shejwalkar and Amir Houmansadr. 2021. Membership privacy for machine

learning models through knowledge transfer. AAAI (2021).

[59] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership Inference Attacks Against Machine Learning Models. Proceedings - IEEE
Symposium on Security and Privacy (2017), 3–18.

[60] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-

works for Large-Scale Image Recognition. CoRR abs/1409.1556 (2015).

[61] Liwei Song and Prateek Mittal. 2021. Systematic evaluation of privacy risks of
machine learning models. In 30th USENIX Security Symposium (USENIX Security
21). 2615–2632.

[62] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.
[63] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R
Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural
language understanding. arXiv preprint arXiv:1804.07461 (2018).

[64] Yijue Wang, Chenghong Wang, Zigeng Wang, Shanglin Zhou, Hang Liu, Jinbo Bi,
Caiwen Ding, and Sanguthevar Rajasekaran. 2021. Against Membership Inference
Attack: Pruning is All You Need. In Proceedings of the Thirtieth International Joint
Conference on Artificial Intelligence, IJCAI-21, Zhi-Hua Zhou (Ed.). International
Joint Conferences on Artificial Intelligence Organization, 3141–3147. Main Track.

