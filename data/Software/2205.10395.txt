2
2
0
2

y
a
M
0
2

]

V
C
.
s
c
[

1
v
5
9
3
0
1
.
5
0
2
2
:
v
i
X
r
a

Assessing visual acuity in visual prostheses through
a virtual-reality system

Melani Sanchez-Garcia*1, Roberto Morollon-Ruiz*2, Ruben
Martinez-Cantin3, Jose J. Guerrero3, Eduardo
Fernandez-Jover2
1Department of Computer Science. University of California, Santa Barbara, CA, USA
2Instituto de Bioingenier´ıa. Universidad de Miguel Hernandez, Spain
3Instituto de Investigaci´on en Ingenier´ıa de Arag´on, (I3A). Universidad de Zaragoza,
Spain

E-mail: mesangar@ucsb.edu, rmorollon@umh.es

Abstract. Objective. Current visual implants still provide very low resolution and
limited ﬁeld of view, thus limiting visual acuity in implanted patients. Developments of
new strategies of artiﬁcial vision simulation systems by harnessing new advancements
in technologies are of upmost priorities for the development of new visual devices.
Approach. In this work, we take advantage of virtual-reality software paired with a
portable head-mounted display and evaluated the performance of normally sighted
participants under simulated prosthetic vision with variable ﬁeld of view and number
of pixels. Our simulated prosthetic vision system allows simple experimentation in
order to study the design parameters of future visual prostheses. Ten normally
sighted participants volunteered for a visual acuity study. Subjects were required to
identify computer-generated Landolt-C gap orientation and diﬀerent stimulus based
on light perception, time-resolution, light location and motion perception commonly
used for visual acuity examination in the sighted. Visual acuity scores were recorded
across diﬀerent conditions of number of electrodes and size of ﬁeld of view. Main
results. Our results showed that of all conditions tested, a ﬁeld of view of 20◦ and
1000 phosphenes of resolution proved the best, with a visual acuity of 1.3 logMAR.
Furthermore, performance appears to be correlated with phosphene density, but
showing a diminishing return when ﬁeld of view is less than 20◦. Signiﬁcance. The
development of new artiﬁcial vision simulation systems can be useful to guide the
development of new visual devices and the optimization of ﬁeld of view and resolution
to provide a helpful and valuable visual aid to profoundly or totally blind patients.

Keywords: Visual prosthesis, visual acuity, virtual-reality, prosthetic vision, computer
vision, simulated prosthetic vision.

 
 
 
 
 
 
1. Introduction

Low vision or blindness are major health issues for the individual’s quality of life.
The leading causes of blindness are primarily age-related eye diseases such as age-
related macular degeneration (AMD) [1, 2], cataract, retinitis pigmentosa (RP) [3] and
glaucoma [4]. The loss of photoreceptors due to degeneration is a major cause of vision
loss, resulting in dysfunctional light detection, transduction, and transmission [5–7].
In the case of RP, these inherited disorders can aﬀect either rods or cone primarily.
The most common form of RP is characterized initially by night blindness, followed
by progressive loss in the peripheral ﬁeld of view in daylight, eventually leading to
blindness after several decades. The case of AMD is characterized by sudden acuity
loss. Currently, there is no cure for RP or AMD [8, 9]. However, great eﬀorts have been
devoted to restoring the resulting poor visual function, based primarily on gene therapy,
stem cell transplantation, or visual prosthesis [10–15].

Visual prostheses are presently the most viable technology for the treatment of
low vision and there are many types being proposed because of its potential for the
development of various types of devices with existing technologies (see Figure 1). The
basic concept of a visual prosthesis is ”electrically stimulating nerve tissues associated
with vision (such as the retina) to help transmit electrical signals with visual information
to the brain” [16]. Thus, several research groups are focusing their eﬀorts on the
development of new approaches for artiﬁcial vision based on electric stimulation of
the retina [17–19], optic nerve [20–22], lateral geniculate nucleus [23, 24], or visual
cortex [25–30], as can be seen in Figure 1. All of these prosthetic devices work by
exchanging information between the electronic devices and diﬀerent types of neurons,
and although most of them are still in development, they show promise of restoring
vision in many forms of blindness.

At present, retinal prostheses are the most successful approach in this ﬁeld [17, 19].
In a retinal implant a multielectrode array is set up on the retinal surface and stimulates
the retina from the top side with electrodes. All this current implantable system are
designed with electrodes implanted in the body working together with several devices
worn outside the body. Thus, a visual prosthesis incorporates an external video camera
for image acquisition, an image processor converting the image to a suitable pattern
of electrical stimulation, and ﬁnally the electrical stimulation array on the retina
itself [31–34].
In spite of reports showing retinal prostheses capable of helping some
participants perform simple tasks of daily living, such as detecting lights, recognizing
objects and even reading large letters, there are still physiological and technological
limitations of the information received by implanted patients. The number of electrodes
and implant size limit the maximum amount of information that can be provided by
the stimulating array. This fact has restricted the degree of visual resolution (up to
1500 phosphenes) and dynamic range of the visual perception (8 grey levels) that can
be delivered to the user. Besides, current systems such as retinal implants provide a
ﬁeld of view (FOV) of approximately 18◦×11◦ in the retinal area, which correspond to

2

Figure 1. Main approaches for the design of a visual prosthesis. A) Schematic
diagram of a LGN prosthesis. B) Cortical prosthesis. C) Retinal prosthesis. D) Optic
nerve prosthesis. In general, all the approaches share a common set of components: a
camera to capture images, generally mounted on glasses; a video processing unit (VPU)
that transform the visual scene into patterns of electrical stimulation and transmits this
information through a radio-frequency link to the implanted device, and an electrode
array implanted at some level in the visual pathways which has to be located near the
target neurons.

the FOV subtended by the electrode implant on the retina. Moreover, the visual acuity
of existing devices is very low, which means that crucial skills such as facial recognition
or navigation in unknown environments are not yet possible.

The visual acuity in prosthetic vision is limited by various factors from both
engineering and physiological perspectives [35]. One of the main causes of low visual
acuity is the limited spatial resolution that can be achieved by electrical stimulation
with existing visual implants. For example, the size of the electrodes in today’s retinal
implants is often much larger than the size of the neurons in the retina, and the number
of electrodes is low [36]. However, it is not entirely true that visual perception will
improve by just increasing the number of electrodes [37]. In addition, it has been shown
that increased FOV is associated with a signiﬁcant improvement in visual acuity [38].
Several retinal prostheses have been tested in clinical trials [17, 39–42] but although the
results are very encouraging, still have to provide higher visual acuities to allow blind
users to perform daily activities [43,44]. These devices have been shown to restore vision
up to a visual acuity of 1.8 logMAR and 1.44 logMAR, respectively. Regardless, the
optimal number of the electrodes and FOV to provide adequate prosthetic vision is still
an open question and an important design parameter needed to develop better implants.
In order to gain a better understanding of the potential beneﬁts of low resolution

3

visual prostheses we can use Simulated Prosthetic Vision (SPV). The SPV system is a
standard procedure for non-invasive evaluation using normal vision subjects. Generally,
in SPV, a low-resolution image of the view is presented on a computer screen [45] or on a
head-mounted display [46–48] glasses to a normally sighted user (see Figure 2). It allows
researchers to rigorously investigate the minimal requirements for a functional visual
prosthesis and to explore which variables are important in the development of a visual
prosthesis. However, the phosphenes perceived by people with visual prostheses are not
yet fully understood. Realistic perception simulations with visual implants would be
useful for the development and evaluation of future visual prosthetic systems. Avraham
et al. [49] implemented a retinal prosthetic vision simulation, including temporal aspects
such as persistence and perceptual fading of phosphenes and the electrode activation
rate. Other previous works have focused on development of computational models to
describe some of these distortions for a small number of behavioral observations in either
space [50] or time [51]. Beyeler et al. [52] go further and simulate spatial distortions,
temporal nonlinearities, and spatio-temporal interactions reported across a wide range
of conditions, devices, and patients.

A major outstanding challenge is predicting what people ‘see’ when they use their
devices or how the implanted subjects will see with the future devices. Another challenge
is addressing the narrow FOV found in most devices as well as the number and size of
electrodes. Some SPV studies attempt to address this using computer monitors but this
requires patients to scan the environment with head movements while trying to piece
together the information, which is diﬃcult to measure with static monitors. To address
these challenges, SPV can be also assessed in controlled, real, or virtual environments.
For instance, a SPV model in immersive virtual-reality (VR) allowing sighted subjects
to act as implanted patients. In this setup, the visual input about to be rendered to
a head-mounted display (HMD) mimics the external camera of a retinal implant. This
input can come from the HMD’s camera or can be simulated in a virtual environment.
This allows sighted subjects to ‘see’ through the eyes of a retinal prosthesis patient,
taking into account their head. Some researchers have used the combination of SPV
with a VR system for experimentation. Sanchez et al. [46] analyzed the inﬂuence of
ﬁeld of view with respect to resolution in visual prostheses through a study with a SPV
setup using a VR system. Kasowski et al. [53] proposed to embed biologically realistic
models of SPV in immersive VR so that sighted subjects can act as virtual patients in
real-world tasks. Thorn et al. [54] implemented prosthetic vision in a VR environment
in order to simulate the real-life experience of using a retinal prosthesis and investigated
the interaction between the ﬁeld of view and the pixel number. Moreover, prosthetic
visual acuity for rectangular and hexagonal phosphene grids was also tested using a
virtual reality simulation [48]. Similarly, Chen et al. [47] examined visual acuity of
prosthetic vision under VR simulation measuring parameters such as ﬁltering scheme,
ﬁlter aperture and the phosphene matrix.

In this work, we take advantage of virtual-reality software paired with a portable
head-mounted display and evaluated the performance of normally sighted participants

4

under simulated prosthetic vision with variable ﬁeld of view and number of pixels. In
our system, the head-mounted display mimics the external camera of a visual implant
subjects and allows simple experimentation in order to study the design parameters of
future visual prostheses.

2. Methods

We examined visual acuity on a stimuli recognition task using SPV through a VR system.
The SPV system is a standard procedure for non-invasive evaluation using normal vision
subjects. This methodology allows controlled evaluation of normally sighted subject
response and task performance which is fundamental to know the way humans perceive
and interpret phosphenized renderings. SPV also oﬀers the advantage of adapting
implant designs to improve the perceptual quality without involving implanted subjects.

2.1. Participants

Ten subjects with normal vision volunteered for the formal experiment. The subjects
(four females and six males) were between 22 and 35 years old. Every subject used a
computer daily.

2.1.1. Ethical statement The research process was conducted according to the ethical
recommendations of the Declaration of Helsinki. The research protocol used for this
study is non-invasive, purely observational, with absolutely no-risk for any participant.
There was no personal data collection or treatment and all subjects were volunteers.
Subjects gave their informed written consent after explanation of the purpose of the
study and possible consequences. The consent allowed the abandonment of the study
at any time. All data were analyzed anonymously. The experiment was approved by
the Aragon Autonomous Community Research Ethics Committee (CEICA, see Ethical
Statement for additional details).

2.2. Simulated Prosthetic Vision (SPV)

This section describes the SPV system including the hardware speciﬁcations, software
components and phosphene generation.

2.2.1. Hardware The experiment was conducted on an HTC VIVE PRO powered by a
computer (Intel(R) Core(TM) i9-9900KF CPU 3.60GHz, NVIDIA GeForce RTX 2080
Ti). The VR system is composed by two lenses, two screens, SteamVR Tracking, G-
sensor, gyroscope, proximity and Eye Comfort Setting (IPD). It contains dual AMOLED
3.5” diagonal screen with a resolution of 1440 x 1600 pixels per eye (2880 x 1600 pixels
combined), covering a visual ﬁeld of approximately 110 degrees. In our experiments we
mostly use the central part of the display which remains undistorted. The representation
with simulated phosphenes was displayed on the VR system worn by the participants

5

Figure 2. Data process. From the images obtained by the HTC cameras we cut
the central area of the raw images to eliminate some kind of edge distortion. The
selected area is projected on the two HTC displays. Finally, we convert the images
into simulated phosphenes. We only project the phosphenic image to the right eye,
simulating an implant in the right eye, and lack of vision on the left.

as well as on the computer screen for the experimenter to check the progress. During
the experiment, participants were seated in a backless chair allowing them to scan the
entire scene with head rotation movements.

2.2.2. Software The implementation was done in C++, using OpenVR for HTC VIVE
Pro to connect with the VR system and OpenCV for image processing. Our software
is compatible with the Windows operating system. Figure 2 shows the data process
designed to generate the stimuli for the VR system. From the images obtained by the
HTC cameras we cut the central area of the raw images to eliminate some kind of edge
distortion. The selected area is projected on the two HTC displays. Finally, we convert
the images into simulated phosphenes. We only project the phosphenic image to the
right eye, simulating an implant in the right eye, and lack of vision on the left (see
Figure 2).

Our phosphene map conﬁguration is similar to the framework of Sanchez et al. [46].
We approximate the phosphenes as circular dots with a Gaussian luminance proﬁle
—each phosphene has maximum intensity at the center and gradually decays to the
periphery, following a Gaussian function–. The intensity of a phosphene is directly
extracted from the intensity of the same region in the image. For our experiments, each
phosphene has eight intensity levels. The size and brightness are directly proportional to
the quantiﬁed sampled pixel intensities. The phosphene map is calculated and updated
with respect to head orientation in real time.

2.3. Procedure

The experiment was conducted using a selection of stimulus from Balm [55] and
Freiburg [56] tests, adapted to our SPV system, which are automated procedures for self-
administered measurement of visual acuity. The images were presented to the subjects

6

Figure 3. Stimuli conditions in the experiment. The six possible stimulus
conditions are depicted for the ‘Landolt-C orientation test’.
‘FOV-Resolution’: C1:
10◦-100 phosphenes, C2: 10◦-1000 phosphenes, C3: 20◦-100 phosphenes, C4: 20◦-1000
phosphenes, C5: 50◦-100 phosphenes and C6: 50◦-1000 phosphenes.

using diﬀerent stimuli conditions based on two resolutions (100 and 1000 phosphenes)
and three FOVs (10, 20 and 50 degrees), as can be seen in Figure 3. We selected
these particular resolutions and FOVs based on current visual prostheses [15,17,39–42],
although our VR platform allows to quickly change those parameters.

‘light location’,

‘time recognition’,

The participants performed ﬁve tests based on diﬀerent types of stimuli, described
as ‘light perception’,
‘motion perception’ and
‘Landolt-C orientation’ (see Figure 4). The ﬁrst stimulus corresponded to the ‘light
perception’ and is the simplest stimulus of the experiment that tests the basic perception
of light. The subjects’ task was to decide whether they see the light appear after the
warning tone or not. The second stimulus corresponded to ‘time resolution’, which
assesses one basic aspect of time resolution—namely, whether one or two ﬂashes occur
after an indicator beep. The third stimulus corresponded to the ‘light location’ that
tests the projection of light. A light disc appeared that the subject must center in
the limited visual ﬁeld. After a pre-set delay, simultaneously with a warning tone, a
wedge appeared directed up, down, right, or left from the ﬁxation disc. The fourth
stimulus corresponded to ‘motion perception’. A random hexagonal pattern of light
and dark elements appeared. After an acoustic signal, it began to move in one of four
directions (up-down-right-left). The subject indicated the motion’s direction. The last
stimulus corresponded to the ‘Landolt-C orientation’ test. Subjects had to indicate the
orientation in one of four directions (up-down-right-left) of the gap in the Landolt-C,
which is a standard international symbol for testing visual acuity. In all tests subjects
responded via corresponding joystick positions. The number of trials was set to 24 for
all the tests.

Subjects sat in a chair that was adjustable in height and were instructed to look
straight at the middle of the screen. A HMD was used in order to immerse the subject

7

Figure 4. Subject and trial setup. Subjects view through the HMD what is shown
in the picture on the monitor. Subjects scanned the underlying picture with their head
motion. They used the joystick to indicate the orientation of the stimuli for subject
response. For the experiment, we used ﬁve stimuli from Balm [55] and Freiburg [56]
tests: Light, Time, Location, Motion, and Landolt C. Each test consisted of 24 stimuli.
The stimuli in each test were randomly selected.

into a virtual-reality environment. Subjects view through the HMD what was shown
in the picture on the computer monitor. Subjects scanned the underlying picture with
their head motion. The image was projected to the right eye only, simulating an implant
in the right eye, and lack of vision on the left (see Figure 2). The subjects were shielded
from as much ambient light as possible and lights were turned oﬀ in the room. The
computer generated graphics were simulated to be stationary in space; head motion
corresponded to scanning an image, sampling with the phosphenized view at diﬀerent
locations of the visual space.

Before the start of each trial, the subjects were informed about that stimulus
functions and the valid choices and they carried out a training test with the 5 stimuli
with normal vision to become familiar with the tasks and the environment. To avoid
errors during the experiment, we trained the subjects to enter their responses via a
keypad with four keys (left, right, top, and bottom). The keypad was a commercial
light, left key; no
USB-connected entry pad. The four keys coded the test response:
light, right key; one ﬂash, left key; two ﬂashes, right key. For the ‘light location’,
‘motion perception’ and ‘Landolt-C orientation’ tests, the keys corresponded to the
observed direction. They were encouraged to respond within the time limit, even when
uncertain. No head or eye tracking was used during the testing to account for ﬁxation.

8

(a)

(b)

Figure 5. Light perception. Performance and reaction time for the ‘light
perception’ test. (a) Bar-plot for performance. (b) Bar-plot for time.

2.4. Statistical analysis

Data were analyzed using two-way ANOVA and post hoc-test with Tukey’s method to
evaluate simultaneously the eﬀect of the two grouping variables (resolution and FOV)
on the response variables performance and reaction time with p = 0.05, ∗ < 0.05 ,
∗∗ < 0.01, ∗ ∗ ∗ < 0.001 and ns not signiﬁcant.

3. Results

The results are summarized from Figure 5 to Figure 10. All ﬁgures show box-plots
of data distribution with 25, 50 and 75th quartiles for performance and reaction time
(mean ± standard deviation) for aggregated data from all subjects. The performance
(in percentage) is deﬁned as number of correct responses. The reaction time (in seconds)
is the time from the subject’s ﬁrst response. We also performed a test to determine if
the mean diﬀerence between speciﬁc pairs of conditions are statistically signiﬁcant using
Tukey’s method with a signiﬁcant level α = 0.05.

3.1. Light perception

Figure 5 shows the performance and reaction time for the ‘light perception’ test. This
test is the simplest of the ﬁve tests carried out in the experiment. All subjects were able
to perceive light or not in almost all trials. All the conditions obtain high values above
95%, as can be seen in Figure 5(a). There was no signiﬁcant diﬀerence between the
stimulus conditions. The reaction time for all conditions are similar close to 1.6s (see
Figure 5(b)). The lowest reaction time was 1.47±0.34, corresponding to the condition of
20◦ and 1000 phosphenes. For the resolution of 100, no signiﬁcant diﬀerence was found
for 10-20 FOVs (p=0.9964), 20-50 FOVs (p=0.7723) and 10-50 FOVs (p=0.8174). For

9

(a)

(b)

Figure 6. Time resolution. Performance and reaction time for the ‘time resolution’
test. (a) Bar-plot for performance. (b) Bar-plot for time.

the resolution of 1000, nor signiﬁcant diﬀerence was found for 10-20 FOVs (p=0.2497),
20-50 FOVs (p=0.7680) and 10-50 FOVs (p=0.6223). There was no signiﬁcant diﬀerence
between the two resolutions (p > 0.05). Some outliers can be observed in both graphs
corresponding to moments in which the subjects were distracted during the task.

3.2. Time resolution

Figure 6 shows the performance and reaction time for the ‘time resolution’ test. For the
resolution of 100 phosphenes, the average performance is 96.15 ± 4.53, 96.70 ± 4.64 and
79.55 ± 11.78 for 10, 20 and 50 degrees respectively (see Figure 6(a)). No signiﬁcant
diﬀerence was found for 10-20 FOVs (p=0.9863). However, signiﬁcant diﬀerence was
found for 10-50 and 20-50 FOVs. For the resolution of 1000, the average performance
is 95.05 ± 4.82, 98.35 ± 2.66 and 97.80 ± 2.84 for 10, 20 and 50 degrees respectively.
No signiﬁcant diﬀerence was found for 10-20 FOVs (p=0.1163), 20-50 FOVs (p=0.2160)
and 10-50 FOVs (p=0.9369).

Figure 6(b) shows the reaction time for the ‘time resolution’ test. For the resolution
of 100 phosphenes, the average reaction time is 1.44 ± 0.31, 1.46 ± 0.42 and 2.15 ± 0.43
for 10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was found for 10-20
FOVs (p=0.9947). For the resolution of 1000, the average reaction time is 1.33 ± 0.32,
1.31 ± 0.30 and 1.48 ± 0.14 for 10, 20 and 50 degrees respectively. No signiﬁcant
diﬀerence was found for 10-20 FOVs (p=0.9877), 20-50 FOVs (p=0.3160) and 10-50
FOVs (p=0.3909).

3.3. Light location

Figure 7 shows the performance and reaction time for the ‘light location’ test. The
performance increases as the FOV and the resolution increases (see Figure 7(a)). For the

10

(a)

(b)

Figure 7. Light location. Performance and reaction time for the ‘light location’
test. (a) Bar-plot for performance. (b) Bar-plot for time.

resolution of 100 phosphenes the average performance is 70.00 ± 21.79, 87.75 ± 14.37 and
73.63 ± 26.15 for 10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was found
for 10-20 FOVs (p=0.1697), 20-50 FOVs (p=0.3158) and 10-50 FOVs (p=0.9237). For
the resolution of 1000 phosphenes the average performance is 81.00 ± 19.89, 94.13 ± 9.70
and 98.38 ± 2.64 for 10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was
found for 20-50 FOVs (p=0.7512).

Figure 7(b) shows the reaction time for the three FOVs and two resolutions. For the
resolution of 100 phosphenes the average reaction time is 16.25 ± 8.63, 8.00 ± 6.82 and
9.40 ± 10.56 for 10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was found
for 10-20 FOVs (p=0.1099), 20-50 FOVs (p=0.9335) and 10-50 FOVs (p=0.2089). For
the resolution of 1000 phosphenes the average reaction time is 14.62 ± 10.98, 4.47 ± 3.27
and 2.24 ± 1.17 for 10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was
found for 20-50 FOVs (p=0.6704). Comparing the performance for the same FOV, the
reaction time decreases with increasing number of phosphenes.

3.4. Motion perception

Figure 8 shows the performance and reaction time for the ‘motion perception’ test. For
the same resolution, the performance decreases as the FOV increases (see Figure 8(a)).
For the resolution of 100 phosphenes the average performance is 88.63 ± 6.57, 87.38 ±
11.64 and 32.63±9.76 for 10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was
found for 10-20 FOVs (p=0.9540). For the resolution of 1000 phosphenes the average
performance is 99.50 ± 1.58, 99.50 ± 1.58 and 92.88 ± 7.17 for 10, 20 and 50 degrees
respectively. No signiﬁcant diﬀerence was found for 10-20 FOVs (p=1.000).

Figure 8(b) shows the reaction time for the ‘motion perception’ test. For the same
resolution, the reaction time increases as the FOV increases. For the resolution of 100

11

(a)

(b)

Figure 8. Motion perception. Performance and reaction time for the ‘motion
perception’ test. (a) Bar-plot for performance. (b) Bar-plot for time.

phosphenes the average reaction time is 5.84 ± 2.73, 5.80 ± 1.94 and 10.59 ± 7.13 for
10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was found for 10-20 FOVs
(p=0.9998), 20-50 FOVs (p=0.0647) and 10-50 FOVs (p=0.0675). For the resolution of
1000 phosphenes the average reaction time is 3.64 ± 1.72, 2.64 ± 0.75 and 3.71 ± 0.85
for 10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was found for 10-20
FOVs (p=0.1591), 20-50 FOVs (p=0.1265) and 10-50 FOVs (p=0.9915). Comparing
the performance for the same FOV, the reaction time decreases with increasing number
of phosphenes.

3.5. Landolt-C orientation

Figure 9 shows the performance and reaction time for the ‘Landolt-C orientation’
test. For the resolution of 100 phosphenes the average performance is 43.89 ± 23.78,
59.44±12.02 and 52.78±11.19 for 10, 20 and 50 degrees respectively (see Figure 9(a)). No
signiﬁcant diﬀerence was found for 10-20 FOVs (p=0.1141), 20-50 FOVs (p=0.6499) and
10-50 FOVs (p=0.4732). For the resolution of 1000 phosphenes the average performance
is 60.65 ± 10.94, 63.33 ± 13.41 and 957.22? ± 7.43 for 10, 20 and 50 degrees respectively.
No signiﬁcant diﬀerence was found for 10-20 FOVs (p=0.8366), 20-50 FOVs (p=0.4314)
and 10-50 FOVs (p=0.7738).

For the resolution of 100 phosphenes the average reaction time is 15.92 ± 6.53,
8.92 ± 3.93 and 9.24 ± 2.70 for 10, 20 and 50 degrees, respectively. No signiﬁcant
diﬀerence was found for 20-50 FOVs (p=0.9874) (see Figure 9(b)). For the resolution of
1000 phosphenes the average reaction time is 8.29 ± 2.91, 8.17 ± 3.97 and 7.46 ± 4.16 for
10, 20 and 50 degrees respectively. No signiﬁcant diﬀerence was found for 10-20 FOVs
(p=0.9970), 20-50 FOVs (p=0.9041) and 10-50 FOVs (p=0.8711).

Figure 10 shows the values of visual acuity (in logMAR) for the ‘Landolt-C

12

(a)

(b)

Figure 9. Landolt-C orientation. Performance and reaction time for the ‘Landolt-
C orientation’ test. (a) Bar-plot for performance. (b) Bar-plot for time.

orientation’ test obtained for each condition. Comparing the same FOV, we obtain
higher visual acuity for 1000 resolution than for 100. However, the most signiﬁcant
diﬀerence between the two resolutions was found for the 10 FOV. The best visual acuity
is obtained for the condition of 20◦ and 1000 resolution, with a visual acuity value of
1.3 logMAR. This visual acuity is considered the limit of blindness.

4. Discussion

During the past decade, much eﬀort has been put into developing visual prosthesis
devices that help restore vision in blind people [17–30]. However, much less has been
spent on ﬁnding acceptable procedures to assess the functionality of diﬀerent visual
implant technologies and maximising the beneﬁts of artiﬁcial vision.

One of the most common methods used in order to gain a better understanding of
the potential beneﬁts of low resolution visual prostheses is through simulated prosthetic
vision (SPV). Generally, in a SPV experiment, a real-time, low-resolution image of the
view is presented on a HMD to a normally sighted subject. The image of the scene is
captured by a head mounted camera, digitalized by a computer, and a sub-sampled low
resolution image is presented on the HMDs. In this way, a variety of tasks have been
evaluated using SPV and encouraging performance results were reported on reading
speed [57–60], navigation [61–63], object recognition [45,64], hand–eye coordination [65]
and face recognition [57,66], among others. However, visual acuity tests are the principle
quantitative measures used to assess the eﬃcacy and cost eﬀectiveness of procedures
designed to improve or restore vision [67, 68].
In a visual acuity test a patient is
required to report the identity of diﬀerent patterns presented in various sizes (spatial
frequency), and the resulting visual acuity is deﬁned by the smallest shape that can be
correctly identiﬁed by the observer. Visual acuities of 0.5 logMAR and 1.0 logMAR are

13

Figure 10. Visual acuity for ‘Landolt-C orientation’ test. Values of visual
acuity (in logMAR) for the ‘Landolt-C orientation’ test obtained for each condition.

considered moderate and severe visual impairment, respectively. A visual acuity greater
than 1.3 logMAR is considered total blindness. The theoretical visual acuity achievable
by present-day retinal implants such as Argus II with a FOV of approximately 20◦ is 2.5
logMAR (20/6325), and the highest acuity is shown to be 1.8 logMAR (20/1262) [40,69]
in several basic tasks, such as those concerning grating visual acuity, square localization,
and movement detection [69–72]. Clinical trials for the Alpha-IMS assessed the visual
acuity and object recognition of the restored vision in the context of daily living and
mobility [41]. Three patients were able to read letters with a visual angle between 5◦
and 10◦, demonstrating the highest visual acuity of 1.44 logMAR in the Landolt-C test.
The visual acuity achieved by the Alpha-IMS is not signiﬁcantly higher than that of the
Argus II, given its 25-fold higher number of stimulating channels. These results may
imply that the patterned stimulation generated from all the 1500 individual channels
of the Alpha-IMS could not be perfectly discriminated by the retinal cells, lowering the
perceived spatial.

Visual acuity has already been used in the past both in clinical trials of visual
neuroprosthesis [73,74] but also in simulation experiments of prosthetic vision [47,75,76].
The SPV software samples the image to match the resolution of the retinal prosthesis
devices. By using SPV, researchers evaluated whether and under what conditions a
measured visual acuity level is a true indication that the visual prosthesis provides a
patterned image. Visual acuity is classically measured by optotypes such as letters,
numbers or Landolt C-rings. Just as the interest in developing a visual prosthesis
intensiﬁed in recent years, some researchers published their SPV study ﬁndings regarding
In 2004,
the number of phosphenes required for comparable-to-normal visual acuity.
Chen et al. [47] examined visual acuity under virtual-reality (VR) in SPV using diﬀerent
ﬁltering schemes. The best mean score recorded by the subjects was 1.55 logMAR.
Later, they tested visual acuity for both rectangular and hexagonal phosphene grids
using the Freiburg test [56]. The visual acuity scores ranged from 1.45 to 1.80 logMAR

14

Table 1. Pixel density on the phosphenic image. Amount of pixels needed to
form one phosphene on each of the conditions.

Resolution

Field of view
(degrees)

Pixel density
(Pixels/Phos)

100

1000

10
20
50

10
20
50

9
36
361

1
4
36

depending on subject. Similarly, Cha et al. [75] measured visual acuity as a function of
the number of pixels and their spacing. They concluded that 625 electrodes implanted
in a 1x1 cm area near the foveal representation of the visual cortex should produce
a phosphene image with a visual acuity of approximately 20/30, 0.17 logMAR. Hayes
et al. [76] simulate three retinal implants and test the functionality of this vision with
four-choice orientation discrimination of a Sloan letter E. Subjects were found to have
visual acuities of 1.96, 1.82, and 1.32 logMAR with the 4x4, 6x10, and 16x16 electrode
arrays, respectively.

We found that the recognition of the diﬀerent stimuli is well achieved with low
resolution and restricted FOV. As can be seen in Figures 5, 6, 7, 8 and 9, for almost
all tests a signiﬁcant improvement in task performance was obtained for a 20 FOV
and a resolution of 1000. Besides, participants took less time to recognize the stimuli
with this condition. Generally, participants took less time to recognize stimuli with
the 20 FOV than the 50 FOV. This seems counterintuitive since with a narrower FOV
the global reference of the image is lost. However, the narrower the FOV, the higher
the angular resolution and therefore the greater the image detail (higher frequencies).
Contrary, to generate one phosphene in the largest FOV, a larger number of pixels is
averaged and therefore more information on image details is lost (see Table 1). Thus,
the widest FOV allows to cover the widest area of the image but it only allows to see the
gist of the image (low spatial frequency). On the other hand, the higher visual acuity
was also obtained with the condition of 20 FOV and 1000 resolution (see Figure 10).
For this condition, subjects obtained a visual acuity of 1.3 logMAR. If the number of
phosphenes is reduced to 100 for the 20 FOV condition, visual acuity decreases to 1.86
logMAR. We can compare this visual acuity value with those obtained by some studies
with Argus II using similar conditions of FOV and resolution [40, 69]. Furthermore, for
the experimental condition of 10◦ FOV and 1000 resolution which can be compared to
the Alpha-IMS implant [41], subjects obtained a visual acuity of 1.43 logMAR.

However, visual acuity is not the only important parameter nor the spatial details

15

of visual scenes. There are many other relevant aspects in visual scenes such as shape,
color and movement that would allow the extraction of complex information, for example
identifying human faces, from relatively poor-quality images by using speciﬁc cues and
multiple visual features [77] or obstacle detection from depth information and motion
cues to facilitate the safe movement of the user in complex or unfamiliar environments
[78]. This suggests that besides image resolution, we should try to pay attention to
other relevant visual attributes such as receptive ﬁeld size, localization, orientation, or
movement [37]. In addition, depending on the subjects, there is one need or another. For
example, some people focus more on identifying objects or people, while others prefer
orientation and mobility. The key issue is to encode and send useful information that
can be translated into functional gains for activities of daily living. Furthermore, it has
been observed that there may be subtle diﬀerences in perceived visual ﬁeld or encoding
between subjects. Therefore, future advanced systems for interacting with the brain of
people with low vision should allow the customization of functions to meet the needs of
each subject.

5. Conclusions

Visual acuity tests are the main quantitative measures used to evaluate the eﬀectiveness
and cost-eﬀectiveness of procedures designed to improve or restore vision. However,
ﬁnding acceptable procedures for evaluating the functionality of visual
implant
technologies and maximizing the beneﬁts of prosthetic vision is still under study.
The present work constitutes a ﬁrst essential step towards immersive virtual-reality
simulations of prosthetic vision which has the potential to accelerate the prototyping of
new devices. Via a head-mounted display, subjects were aﬀorded simulated prosthetic
vision (phosphene images) and required to recognise diﬀerent stimuli normally used to
measure visual acuity. Of all conditions tested, a FOV of 20◦ and 1000 phosphenes
of resolution proved optimal, with higher visual acuity of 1.3 logMAR. Our simulated
prosthetic vision system allows simple experimentation in order to study the design
parameters of future visual prostheses. This work is a step toward the design of more
eﬀective electrode arrays that we hope will beneﬁt the blind through neuroprosthesis.

Acknowledgments

This work was supported by project RTI2018-096903-B-I00 (MINECO/FEDER, UE)
and BES-2016-078426 (MINECO). The authors thank Violeta Estepa Ramos for
collaborating in the development of the simulation environment.

References

[1] VanNewkirk M R, Nanjan M B, Wang J J, Mitchell P, Taylor H R and McCarty C A 2000

Ophthalmology 107 1593–1600

16

[2] Vingerling J R, Dielemans I, Hofman A, Grobbee D E, Hijmering M, Kramer C F and de Jong

P T 1995 Ophthalmology 102 205–210

[3] Hartong D T, Berson E L and Dryja T P 2006 The Lancet 368 1795–1809
[4] Steinmetz J D, Bourne R R, Briant P S, Flaxman S R, Taylor H R, Jonas J B, Abdoli A A, Abrha
W A, Abualhasan A, Abu-Gharbieh E G et al. 2021 The Lancet Global Health 9 e144–e160
[5] Curcio C A, Owsley C and Jackson G R 2000 Investigative ophthalmology & visual science 41

2015–2018

[6] Busskamp V, Duebel J, Balya D, Fradot M, Viney T J, Siegert S, Groner A C, Cabuy E, Forster

V, Seeliger M et al. 2010 science 329 413–417

[7] Yue L, Weiland J D, Roska B and Humayun M S 2016 Progress in retinal and eye research 53

21–47

[8] Richer S, Stiles W, Statkute L, Pulido J, Frankowski J, Rudy D, Pei K, Tsipursky M and Nyland

J 2004 Optometry-Journal of the American Optometric Association 75 216–229

[9] Heier J S, Brown D M, Chong V, Korobelnik J F, Kaiser P K, Nguyen Q D, Kirchhof B, Ho A,

Ogura Y, Yancopoulos G D et al. 2012 Ophthalmology 119 2537–2548
[10] Stieger K, Chauveau C and Rolling F 2010 Current gene therapy 10 389–403
[11] Huang Y, Enzmann V and Ildstad S T 2011 Stem Cell Reviews and Reports 7 434–445
[12] Wong I Y H, Poon M W, Pang R T W, Lian Q and Wong D 2011 Graefe’s Archive for Clinical

and Experimental Ophthalmology 249 1439–1448

[13] Beltran W A, Cideciyan A V, Lewin A S, Iwabe S, Khanna H, Sumaroka A, Chiodo V A, Fajardo
D S, Rom´an A J, Deng W T et al. 2012 Proceedings of the National Academy of Sciences 109
2132–2137

[14] Takahashi V K, Takiuti J T, Jauregui R and Tsang S H 2018 Ophthalmic genetics 39 560–568
[15] Bloch E, Luo Y and da Cruz L 2019 Therapeutic advances in ophthalmology 11 2515841418817501
[16] Farnum A and Pelled G 2020 Frontiers in neuroscience 14 36
[17] da Cruz L, Dorn J D, Humayun M S, Dagnelie G, Handa J, Barale P O, Sahel J A, Stanga P E,

Hafezi F, Safran A B et al. 2016 Ophthalmology 123 2248–2254

[18] Lorach H, Wang J, Lee D Y, Dalal R, Huie P and Palanker D 2016 Biomedical optics express 7

13–21

[19] Stingl K, Schippert R, Bartz-Schmidt K U, Besch D, Cottriall C L, Edwards T L, Gekeler F,

Greppmaier U, Kiel K, Koitschev A et al. 2017 Frontiers in neuroscience 11 445

[20] Duret F, Brel´en M E, Lambert V, G´erard B, Delbeke J and Veraart C 2006 Restorative neurology

and neuroscience 24 31–40

[21] Lu Y, Yan Y, Chai X, Ren Q, Chen Y and Li L 2013 Journal of neural engineering 10 036022
[22] Gaillet V, Cutrone A, Artoni F, Vagni P, Pratiwi A M, Romero S A, Di Paola D L, Micera S and

Ghezzi D 2020 Nature biomedical engineering 4 181–194

[23] Vurro M, Crowell A M and Pezaris J S 2014 Frontiers in human neuroscience 8 816
[24] Killian N J, Vurro M, Keith S B, Kyada M J and Pezaris J S 2016 Scientiﬁc reports 6 1–16
[25] Fernandez E, Pelayo F, Romero S, Bongard M, Marin C, Alfaro A and Merabet L 2005 Journal

of neural engineering 2 R1

[26] Normann R A, Greger B A, House P, Romero S F, Pelayo F and Fernandez E 2009 Journal of

neural engineering 6 035001

[27] Kane S R, Cogan S F, Ehrlich J, Plante T D, McCreery D B and Troyk P R 2013 IEEE Transactions

on Biomedical Engineering 60 2153–2160

[28] Normann R A and Fernandez E 2016 Journal of neural engineering 13 061003
[29] Fernandez E 2018 Bioelectronic medicine 4 1–8
[30] Niketeghad S, Muralidharan A, Patel U, Dorn J D, Bonelli L, Greenberg R J and Pouratian N

2019 Journal of neurosurgery 132 2000–2007

[31] Chader G J, Weiland J and Humayun M S 2009 Progress in brain research 175 317–332
[32] Dowling J 2009 Eye 23 1999–2005
[33] Sachs H G and Gabel V P 2004 Graefe’s archive for clinical and experimental ophthalmology 242

17

717–723

[34] Weiland J D, Liu W and Humayun M S 2005 Annu. Rev. Biomed. Eng. 7 361–401
[35] Shim S, Eom K, Jeong J and Kim S J 2020 Micromachines 11 535
[36] Tong W, Stamp M, Apollo N V, Ganesan K, Meﬃn H, Prawer S, Garrett D J and Ibbotson M R

2019 Journal of neural engineering 17 016018

[37] Fern´andez E, Alfaro A and Gonz´alez-L´opez P 2020 Frontiers in Neuroscience 14 681
[38] Ameri H, Ratanapakorn T, Ufer S, Eckhardt H, Humayun M S and Weiland J D 2009 Journal of

neural engineering 6 035002

[39] Rizzo S, Belting C, Cinelli L, Allegrini L, Genovesi-Ebert F, Barca F and Di Bartolo E 2014

American journal of ophthalmology 157 1282–1290

[40] Humayun M S, Dorn J D, Da Cruz L, Dagnelie G, Sahel J A, Stanga P E, Cideciyan A V, Duncan

J L, Eliott D, Filley E et al. 2012 Ophthalmology 119 779–788

[41] Stingl K, Bartz-Schmidt K U, Besch D, Chee C K, Cottriall C L, Gekeler F, Groppe M, Jackson

T L, MacLaren R E, Koitschev A et al. 2015 Vision research 111 149–160

[42] Zrenner E, Bartz-Schmidt K U, Benav H, Besch D, Bruckmann A, Gabel V P, Gekeler F,
Greppmaier U, Harscher A, Kibbel S et al. 2011 Proceedings of the Royal Society B: Biological
Sciences 278 1489–1497

[43] Stronks H C and Dagnelie G 2014 Expert review of medical devices 11 23–30
[44] Sahel J, Mohand-Said S, Stanga P, Caspi A, Greenberg R, Group A I S et al. 2013 Investigative

Ophthalmology & Visual Science 54 1389–1389

[45] Sanchez-Garcia M, Martinez-Cantin R and Guerrero J J 2020 Plos one 15 e0227677
[46] Sanchez-Garcia M, Martinez-Cantin R, Bermudez-Cameo J and Guerrero J J 2020 Journal of

Neural Engineering 17 056002

[47] CHEN S, LOVELL N and SUANING G 2004 Eﬀect of prosthetic vision acuity by ﬁltering
schemes, ﬁlter cut-oﬀ frequency and phosphene matrix: A virtual reality simulation 26th Annual
International Conference of the IEEE-EMBS, San Francisco, CA

[48] Chen S, Hallum L, Lovell N and Suaning G J 2005 Journal of Neural Engineering 2 S135
[49] Avraham D, Jung J H, Yitzhaky Y and Peli E 2021 Journal of Neural Engineering 18 0460d9
[50] Nanduri D, Fine I, Horsager A, Boynton G M, Humayun M S, Greenberg R J and Weiland J D

2012 Investigative ophthalmology & visual science 53 205–214

[51] Horsager A, Greenwald S H, Weiland J D, Humayun M S, Greenberg R J, McMahon M J, Boynton

G M and Fine I 2009 Investigative ophthalmology & visual science 50 1483–1491

[52] Beyeler M, Boynton G M, Fine I and Rokem A 2017 BioRxiv 148015
[53] Kasowski J, Wu N and Beyeler M 2021 arXiv preprint arXiv:2102.10678
[54] Thorn J T, Migliorini E and Ghezzi D 2020 Journal of Neural Engineering 17 056019
[55] Bach M, Wilke M, Wilhelm B, Zrenner E and Wilke R 2010 Investigative ophthalmology & visual

science 51 1255–1260

[56] Bach M et al. 1996 Optometry and vision science 73 49–53
[57] Ho E, Boﬀa J and Palanker D 2019 Journal of vision 19 22–22
[58] Paraskevoudi N and Pezaris J S 2021 Scientiﬁc Reports 11 1–17
[59] Abraham C, Farah N, Gerbi-Zarfati L, Harpaz Y, Zalvesky Z and Mandel Y 2019 Biomedical optics

express 10 1081–1096

[60] Mandel Y 2019 Investigative Ophthalmology & Visual Science 60 2843–2843
[61] Sanchez-Garcia M, Perez-Yus A, Martinez-Cantin R and Guerrero J J 2021 arXiv preprint

arXiv:2109.14957

[62] Lo Valvo A, Croce D, Garlisi D, Giuliano F, Giarr´e L and Tinnirello I 2021 Sensors 21 3061
[63] Vergnieux V, Mac´e M J M and Jouﬀrais C 2017 Artiﬁcial organs 41 852–861
[64] Li H, Su X, Wang J, Kan H, Han T, Zeng Y and Chai X 2018 Artiﬁcial intelligence in medicine

84 64–78

[65] Titchener S A, Shivdasani M N, Fallon J B and Petoe M A 2018 Translational vision science &

technology 7 2–2

18

[66] Irons J L, Gradden T, Zhang A, He X, Barnes N, Scott A F and McKone E 2017 Vision research

137 61–79

[67] Rosenfeld P J, Brown D M, Heier J S, Boyer D S, Kaiser P K, Chung C Y and Kim R Y 2006

New England Journal of Medicine 355 1419–1431

[68] Kobelt G, Lundstr¨om M and Stenevi U 2002 Journal of Cataract & Refractive Surgery 28 1742–

1749

[69] Ho A C, Humayun M S, Dorn J D, Da Cruz L, Dagnelie G, Handa J, Barale P O, Sahel J A,

Stanga P E, Hafezi F et al. 2015 Ophthalmology 122 1547–1554

[70] Zhou D D, Dorn J D and Greenberg R J 2013 The argus® ii retinal prosthesis system: An
overview 2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)
(IEEE) pp 1–6

[71] Humayun M S, Dorn J D, Ahuja A K, Caspi A, Filley E, Dagnelie G, Salzmann J, Santos A,
Duncan J, Mohand-Said S et al. 2009 Preliminary 6 month results from the argus tm ii epiretinal
prosthesis feasibility study 2009 Annual International Conference of the IEEE Engineering in
Medicine and Biology Society (IEEE) pp 4566–4568

[72] Ahuja A K, Dorn J, Caspi A, McMahon M, Dagnelie G, Stanga P, Humayun M, Greenberg R,

Group A I S et al. 2011 British Journal of Ophthalmology 95 539–543

[73] Humayun M S, De Juan E, Dagnelie G, Greenberg R J, Propst R H and Phillips D H 1996 Archives

of ophthalmology 114 40–46

[74] Dobelle W H 2000 ASAIO journal 46 3–9
[75] Cha K, Horch K and Normann R A 1992 Annals of biomedical engineering 20 439–449
[76] Hayes J S, Yin V T, Piyathaisere D, Weiland J D, Humayun M S and Dagnelie G 2003 Artiﬁcial

Organs 27 1016–1028

[77] Sinha P 2002 nature neuroscience 5 1093–1097
[78] Perez-Yus A, Bermudez-Cameo J, Lopez-Nicolas G and Guerrero J J 2017 Depth and motion cues
with phosphene patterns for prosthetic vision Proceedings of the IEEE International Conference
on Computer Vision Workshops pp 1516–1525

19

