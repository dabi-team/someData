Open World Learning Graph Convolution for
Latency Estimation in Routing Networks

Yifei Jin
KTH Royal Institute of Technology & Ericsson AB
Division of Theoretical and Computer Science & Ericsson Research
Stockholm, Sweden
yifeij@kth.se

Marios Daoutis
Ericsson AB
Ericsson Research
Stockholm, Sweden
marios.daoutis@ericsson.com

Sarunas Girdzijauskas
KTH Royal Institute of Technology
Division of Software and Computer Systems
Stockholm, Sweden
sarunasg@kth.se

Aristides Gionis
KTH Royal Institute of Technology
Division of Theoretical and Computer Science
Stockholm, Sweden
argioni@kth.se

Abstract‚ÄîAccurate routing network status estimation is a key
component in Software DeÔ¨Åned Networking. However, existing
deep-learning-based methods for modeling network routing are
not able to extrapolate towards unseen feature distributions. Nor
are they able to handle scaled and drifted network attributes
in test sets that include open-world inputs. To deal with these
challenges, we propose a novel approach for modeling network
routing, using Graph Neural Networks. Our method can also
be used for network-latency estimation. Supported by a domain-
knowledge-assisted graph formulation, our model shares a stable
performance across different network sizes and conÔ¨Ågurations of
routing networks, while at the same time being able to extrapolate
towards unseen sizes, conÔ¨Ågurations, and user behavior. We show
that our model outperforms most conventional deep-learning-
based models, in terms of prediction accuracy, computational
resources, inference speed, as well as ability to generalize towards
open-world input.

Index Terms‚ÄîGraph Convolution, Software DeÔ¨Åne Networks,

Open World Learning

I. INTRODUCTION

Software-deÔ¨Åned networking (SDN) is a state-of-the-art
approach to network management, which permits dynamic and
programmatic network conÔ¨Ågurations (e.g., routing), aimed
at improved network performance and monitoring, abstracted
away from individual network elements into a centralized net-
work control layer. Consequently, the orchestration and SDN
control software is expected to operate on and adapt to unseen
network deployments and topologies. Precise routing-network
modeling is a prerequisite for SDN to efÔ¨Åciently estimate and
forecast network state. As a result, in certain scenarios, such
as during network expansion, it is expected that: (i) several
network features (possibly used in model training) become
greatly imbalanced; (ii) unseen attribute values are expected
to be out-of-distribution; (iii) varying, often larger, network
sizes are encountered; and (iv) relational dependencies expand
among longer node chains. An illustrative example of above is
shown in Fig. 1. A successful approach to deal with the above-
mentioned challenges, is to employ ideas from the domain

Figure 1: Schematic Representation of SDN Network State
Estimation Problem. Directed line denotes network trafÔ¨Åc
Ô¨Çows with different throughput in the given topology, where
one can see that there exist different distributions of line type
between training and validation/test time, as mentioned in (i).
Undirected line denote the link communicating pair of network
elements, with the line‚Äôs strength denoting the link‚Äôs capacity
level.

that

of open-world machine learning (OWL) [1]. Compared with
traditional machine learning, OWL is expected to extrapolate
towards unseen input distribution, classes, and scaling in
the training set. Note,
the problem of estimating the
network state is EXPTIME-complete, as shown by [2], and
has been extensively studied in the literature [3], where most
commonly conventional deep-learning methods are employed
in the context of reinforcement learning (RL) based network
orchestration as well as deep-learning-based service optimiza-
tion.

In this paper we consider the problem of estimating the
latency between a source and a destination network node
(denoted as OD pair) in a routing network. The problem setting
is to learn a model on smaller networks and extrapolate the
predictions on larger networks assuming open-world input, as
illustrated in Fig. 1.

2
2
0
2

l
u
J

8

]
I

N
.
s
c
[

1
v
3
4
6
4
1
.
7
0
2
2
:
v
i
X
r
a

Training timeProposed SolutionSmaller NetworkNetwork TopologyTraffic & Routing Configper OD-pair latency(ùëñùëñùëñ)(ùëñùëñ)Prediction(ùëñùë£)Larger NetworkValidation/Test time 
 
 
 
 
 
Conventional deep learning (DL) methods have focused in
predictive user behavior modeling on service demand and data
usage. In this setting, a line of work focuses on leveraging deep
neural networks for adaptive optimisation of routing schemes,
as by [4], which typically require similar data distribution in
the test environment. Only few papers have considered to ex-
trapolate their proposed model on a larger network [5], some-
thing which, this far, has been considered infeasible in real-
world scenarios. Moreover, successful extrapolation would
require introducing task-speciÔ¨Åc non-linearity (i.e., queuing
theory and network calculus in the routing network [6]) in the
model [7], something which, this far, has not been considered
in state-of-the-art DL solutions.

Graph Neural Networks (GNNs) have been used success-
fully for incorporating domain knowledge into different prob-
lem settings, which in turn, help to devise robust models better
suited for OWL in learning complex-system representations.
Moreover, our work is primarily inspired by [8] who reveal
that by solely observing key nodes in the SDN topology, the
deep network can produce effective embeddings capable of
predicting network performance.

Our study is among the Ô¨Årst approaches that attempt to
the routing network state snapshot by learning the
model
given topology structure for the task of estimating the net-
work latency using open-world input. Our proposed solution
transforms the task of estimating the latency between source
and destination nodes, to a link-attribute prediction task, i.e.,
predicting each link that can potentially contribute to trafÔ¨Åc
delays occurring in the routing trajectory. Estimating every
link‚Äôs latency contribution requires three aspects: (i) the trafÔ¨Åc
amount passing through the link; (ii) the capacity of the
link; and (iii) link‚Äôs structural features. The link trafÔ¨Åc and
capacity jointly denote the data throughput, and the congestion
state of the given link. The link‚Äôs structural features can
be learned from the network topology and they serve two
purposes: (i) to identify key links that can become bottleneck
of the network performance; and (ii) to compute pair-wise
similarities, which are used to discover similar links. We train
our model on distributions of a particular network size, yet,
the model achieves at least 75% reduction in mean absolute
percentage error (MAPE) measured in different sized networks
during inference, i.e., without explicit training using the larger
(up to 6√ó) network topologies. Last but not least, our solution
requires no GPU resource while still ensures a faster training
as well as 3‚Äì7 times faster inference speeds, with only 6-
25% embedding size required, when compared, for example,
with the best-performing benchmark models. The remaining
sections are organised as follows: In Sec. II we discuss the
related approaches in the domain of size generalisation of
GNNs and graph-based ML in SDN routing, while in Sec. III
we present our problem formulation in more detail. In Sec. IV
we present our methodology followed by Sec. V, which
contains the experimental evaluation of our algorithm. Finally
Sec. VI concludes with a discussion of our main contributions
and considerations for future work.

II. RELATED WORK

Graph Convolutional Networks (GCN) is a known feature-
extraction technique for non-Euclidean spaces [9]. GCNs are
capable of learning the graph structure by aggregating each
node‚Äôs embedding with its neighbors. In the conventional
architecture, GCN is proven to be capable of incorporating
node and edge information in a undirected graph scope. We
are currently witnessing an increasing number of publications
on GCN models, which, on the one hand reveal the success
of this model, and on the other hand expose its limitations.

GCN models are constrained due to aggregating only local
neighbors. For large enough graphs, GCN may fail to capture
distant (long-range) dependencies entirely. Efforts have been
made towards both building deeper GNN models [10] and
building expressive GNN layers on long-range dependencies.
For example, [11] proposed the Ô¨Årst effective method to
encode long-range dependencies, between any two nodes, of
the graph in the node embedding. Furthermore, [12] pro-
pose to apply graph attention network (GAT), to compute
shortest-path-to-node attention, based on a transformer [13]
architecture. This work is one of the most related approaches
to the one presented here. Contrary to the aforementioned
contributions, which are based on transductive settings, our
work focuses instead on an inductive setting. In our model, we
are incorporating path-to-node attention, but passively between
the path‚Äôs structurally similar node and the targeting node,
to tackle longer chains of dependencies in the test data.
In addition, we do not build very deep GNN solutions to
enlarge the receptive Ô¨Åeld up to the length of dependency,
for balancing between model accuracy, resource usage and
inference speed trade-off.

In a survey by [7], the extrapolation ability of GNNs regards
mainly two aspects: the ability to extrapolate towards unseen
structural features as well as to extrapolate towards out-of-
distribution attributes. [14] conclude that the size general-
ization can only be applied on graphs with certain structural
features. Moreover, they suggest that the expressiveness of the
GNN model must be still sufÔ¨Åcient for applying it to larger
graphs. [15] further elaborate from the perspective frequency
response, pointing out that spectral graph convolution is inher-
ently transferable for graphs with similar degree distribution.
In our model, we leverage the conclusions from the afore-
mentioned works and cast the graph-size-generalization prob-
lem to a transferred formulation of graph, which is considered
to be learnable by spectral graph-convolution methods. On the
other hand, extrapolation towards out-of-distribution data is a
fundamental challenge for most neural networks. For out-of-
distribution attributes, the extrapolation ability relies more on
the embedding and readout function of the GNN architecture.
[7] have shown the multi-layered perception models (MLP)
can only extrapolate if the test set
is expanded from the
training set in all geometrical directions in the latent space.
Both [16] and [17] have studied on approximating simple
arithmetic computation (+, ‚àí, √ó, and /) when extrapolate
towards drifted numerical values. In our proposed model, we

utilize building blocks from [16] to encourage the model to
learn the arithmetic impact, when tackling drifted numerical
input features.

More recent related works focus on learning the tuned graph
representation of routing networks. [18] propose RouteNet,
which is regarded as a Ô¨Årst work that introduces message
passing in deep-learning-based network modeling, in a bi-
partite graph formulation. [19] and [20] extend RouteNet to
be adaptive to heterogeneous scheduling policies and routing
scheme, while improving its accuracy and inference speed on
similar topology structures. Finally, [5] present their work on
topology size generalization for latency estimation of Origin-
Destination (OD) pairs, the same problem we focus here.
Through improving RouteNet by including queue occupancy
state per link, beside the path and link nodes, they formulate a
tripartite message passing scheme, which introduces size gen-
eralization ability to the model. Yet, none of aforementioned
works utilize the topology‚Äôs structural feature in network-state
embedding, as we address in this work.

III. PROBLEM FORMULATION
The task we consider in this paper is to perform mean
latency estimation on every OD pair p in the network. We
summarize the deÔ¨Åned notation in Table. I. The input to our
problem is the following.

1) A sequence of attributed size-varying networks G =
{Gi(Vi, Ei; Xi, Yi)}, where Vi denotes the |Vi| = ni
network nodes and Ei denotes the |Ei| = mi
links
between pairs of nodes in Vi. For each node v ‚àà Vi,
a set of node attributes x(v) are provided to describe the
heterogeneous routing conÔ¨Åguration on the network node
level. Similarly, a set of link attributes y(e) is introduced
for each link e ‚àà Ei to denote link conÔ¨Åguration attributes
with respect to routing. The set of all node attributes in
Gi is denoted by Xi, and the set of all link attributes is
denoted by Yi. Each network Gi includes ki OD pairs
Pi = {pi,j}, with j = 1, . . . , ki.

2) A sequence of trafÔ¨Åc matrices T = {Ti}, where each
Ti corresponds to a network Gi. Each trafÔ¨Åc matrix Ti
consists by ki vectors {ti,j}, with j = 1, . . . , ki, where
each ti,j ‚àà R2 denotes the mean throughput and peak
throughput features for the OD pair pi,j. Note that there
exist OD pairs with same source and destination nodes
but different throughput features.

3) A sequence of routing matrices R = {Ri}, where each
Ri with dimension Rni√óni, with 0‚Äôs on the diagonal
elements, corresponds to network Gi. Each element in Ri,
denoted by Ri(c,d) = ri(vc, vd); vc, vd ‚àà Vi represents the
next-hop per OD pair, given the current node vc and the
destination node vd. Note that Ri contains information
for all possible routing pairs, regardless of the given OD
pair elements in Ti.

4) The ground truth: A sequence of performance matrices
Q = {Qi}, where each Qi corresponds to network Gi.
Each performance matrix Qi consists of ki vectors {qi,j},
with j = 1, . . . , ki, denoting the performance of each

OD pair pi,j in global (mean latency) and local (per-
link occupancy) metrics. The task is to estimate the mean
latency value of each OD pair, denote as an element in
qi,j = Qi(pi,j), with j = 1, . . . , ki.

Previous works [5], [18]‚Äì[20] formulate the problem deÔ¨Åned
above as a multi-step prediction problem. More speciÔ¨Åcally,
given network snapshots (Gi, Ti, Ri), the goal is to predict the
future ‚àÜ steps of the network state [Q(cid:48)1
i , . . . , Q(cid:48)‚àÜ
] and pool
i
the result of each step for a prediction Q(cid:48)
i. In contrast, in our
model, we consider this problem as a node attribute prediction
problem. We introduce a modiÔ¨Åed problem formulation:

1) Given the input to our problem speciÔ¨Åed by a sequence
of tuples (Gi(Vi, Ei; Xi, Yi), Ti, Ri), we Ô¨Årst transform
Gi(Vi, Ei; Xi, Yi) to a directed graph, where each undi-
rected edge (u, v) ‚àà Ei is replaced by two directed edges
(u, v) and (v, u), and then transform it to a line graph
i ). We deÔ¨Åne a projection L‚àí1, to project the
GL
representation (i.e: vi ‚àà V L
i ) in line graph GL
i ‚àà EL
i , eL
i ,
back to its representation in Gi,

i , EL

i (V L

i of the line graph GL
i

2) The edge set EL
is directed. The
node set V L
includes only valid routings in Ri, i.e., for
i
all v ‚àà V L
its respective edge representation L‚àí1(v) =
i
e(vx, vy) is in Ei. Thus, there exists vd ‚àà Vi such that
ri(vx, vd) = vy holds.

3) We can compute the summed trafÔ¨Åc T s

i per-link over all
OD trajectory pairs passing through, for all ni nodes.
Each element is denoted by ts(i, v) = T s
i (v), for all
i . Given node v‚Äôs edge representation L‚àí1(v) =
v ‚àà V L
ev ‚àà Ei, We deÔ¨Åne the node attributes X L
i as
follows:

i (v) in GL

X L

i (v) =

ts(i, v)
c(Yi(ev))

and ts(i, v) =

(cid:88)

Ti(pk),

pk;v‚ààpk

(1)

for v ‚àà V L
i

and ev ‚àà Ei and L‚àí1(v) = ev

where c(Yi(ev)) ‚àà Yi(Ei), denotes the capacity per-link
in Gi.

4) For each edge e ‚àà EL

i connecting node vs to node vd in
i . We introduce corresponding edge weight wi(vs, vd)

GL
as follows:

wi(vs, vd) =

(cid:80)

Ti(pk)

pk;vs,vd‚ààpk
ts(i, vs)

Yi(evs)
Yi(evd )
for vs, vd ‚àà V L
and evs, evd ‚àà Ei,
i
and L‚àí1(vs) = evs, L‚àí1(vd) = evd ,

¬∑

,

(2)

The motivation behind this re-formulation is multi-faceted:
(i) Using directed links in GL
introduces OD trajectory pairs
i
information into the graph structure. The new formulation
separates the link‚Äôs adjacency into in-degree and out-degree,
which will be used to represent the link‚Äôs in-coming trafÔ¨Åc
(in-trafÔ¨Åc) and out-going trafÔ¨Åc (out-trafÔ¨Åc). (ii) For the task
of estimating path latency Qi(pi,j),
the problem can be
decomposed into estimating the latency contribution of each
hop v ‚àà V L
Qi(v). According to queuing theory,
i
such latency contribution further reduces to estimating v‚Äôs
queue occupancy state O(i, v) ‚àà Qi. Thus, in the graph GL
i ,

into (cid:80)

v;v‚ààpk

Table I: Notation Table

Training/testing set of Topology

Network Topology Feature Notation:
G = {Gi}
Gi(Vi, Ei; Xi, Yi) Topology snapshot graph
Vi = {vi}
ni = |Vi|
Ei = {ei}
Xi = {x(vi)}
Yi
c(Y (ei))

Network nodes in Gi
Size of Gi
Communication links between network nodes
Network node conÔ¨Åguration of Gi
Communication link conÔ¨Åguration of Gi
ConÔ¨Ågured link capacity of ei ‚àà Ei

Network Service Feature Notation:
OD pairs in Gi
Pi = {pi,j }
j-th OD-pair in Gi
pi,j
Number of OD pairs in Gi
ki
T = {Ti}
Training/testing set of user trafÔ¨Åc
User trafÔ¨Åc matrix for all Pi
Ti = {ti,j }
User trafÔ¨Åc features for pi,j ‚àà Pi
ti,j

Network Routing Feature Notation:
R = {Ri}
Ri
Ri(c,d)

Training/testing set of routing matrices
Routing matrices corresponds to network Gi
Next-hop of current node and destination being vc, vd ‚àà
Vi

Ground Truth Notation:
Q = {Qi}
Qi
qi,j

Training/testing set of performance matrix
Performance matrix correspond to (Gi, Ti, Ri)
Performance measurement (i.e: latency) of pi,j

Novel Feature Notation in reformulating the problem :
GL
L‚àí1

i , EL
i )

i (V L

i to its correspond-

Line graph transformation of Gi(Vi, Ei)
Projection from representation in GL
ing representation in Gi
Matrix of summed trafÔ¨Åc of pi,j , who share the same
node v ‚àà V L
i
Summed trafÔ¨Åc for all pi,j passing through node v ‚àà
V L
i
Line graph node feature, introduced in Equ. 1
Line graph edge weight, introduced in Equ. 2
node v‚Äôs queue occupancy state (ground truth), for v ‚àà
V L
i

T s
i

ts(i, v)

i (v)

X L
wi
O(i, v)

(cid:96)

and ASout
, which denote Ô¨Årst-order proximity, second-order
in-proximity and second-order out-proximity, as shown in
Fig. 3b. Given the weighted adjacency matrix A(cid:96) of the line
graph GL

i , one can compute the aforementioned quantities:

AF

(cid:96) = (A(cid:96) + AT

(cid:96) )/2,

(3)

ASin
(cid:96)(i,j)

=

(cid:88)

k

A(cid:96)(k,i)A(cid:96)(k,j)
(cid:80)
v A(cid:96)(k,v)

and ASout
(cid:96)(i,j)

=

(cid:88)

k

,

A(cid:96)(i,k) A(cid:96)(j,k)
(cid:80)
v A(cid:96)(v,k)
for (i, j) ‚àà V L
i
(4)

We apply graph convolution to the aforementioned adjacency
matrix, with adding self-loops, denoted by ÀúAF
(cid:96) + I,
ÀúASin = ASin +I,
ÀúASout = ASout +I. We concatenate (denote
as ||) each result with a train-able weight (Œ±, Œ≤). Œò is a train-
able matrix. We deÔ¨Åne

(cid:96) = AF

fx(X, Ax) = ÀúDx

(‚àí 1

2 ) ÀúAx ÀúDx

(‚àí 1
2 )

XŒò,

(5)

Figure 2: Comparative Example of the Problem Formulation,
with Ground Truth being Red. Left: Previous works‚Äô formu-
lation. Right: The proposed formulation

the problem is reduced to a node attribute prediction task.1
(iii) Inspired by previous work, identifying key link that is
shared by many OD pairs could have a dominating impact on
the whole network performance. In the graph GL
i , such feature
is disclosed within the ego-networks of nodes v ‚àà V L
i , which
can be jointly formed by the trajectories of all paths passing
through v. Thus, one can discover the importance of node v in
network topology by its role in GL
i . (iv) The features computed
in Eq. 1 and Eq. 2 are supported by queuing theory;2 here
we consider domain knowledge to be the key in producing a
size-invariant graph signal on GL
i . An illustrative example of
re-formulation, comparing with the previous works‚Äô is given
in Fig. 2.

IV. METHODOLOGY

Fig. 3 provides an overview of our proposed model for
the prediction task. In the Ô¨Årst phase (Fig. 3a), we transform
the input graph Gi (Fig.3a (i), (ii)) into the line graph GL
i
(Fig.3a (iii), (iv)) for the model to learn in a latent space
that generalizes well on different
input graph sizes. Next
(Fig. 3b), we separate the graph into a directed sub-graph
and a un-directed sub-graph that capture the neighboring
sequential relation as well as the impact of key nodes. We
design the corresponding components that capture each impact
and handle out-of-distribution features, so as to get consistent
results across different graph sizes. Below, we discuss each
component separately.

A. Directed Spectral Graph Convolution

To capture the sequential relation between nodes consisting
of the OD pair‚Äôs trajectory, as well as the OD pair‚Äôs direction,
we use the architecture designed by [21]. It is a spectral
convolution block with its reception Ô¨Åeld encoded with direc-
tion information, named Directed Graph Convolution Network
(DGCN). Given the processed directed GL
i , we separate the
(cid:96) , ASin
adjacency matrix into three different formations: AF

(cid:96)

1A detailed transform procedure is shown in Appendix.A. Link to the
appendix: https://anonymous.4open.science/r/RoutingGNN Appendix-F5ED/
IEEE Graph Covolution for Routing Appendix .pdf

2A detailed proof is given Appendix.B-C, Link to the appendix same as

above.

2143(ùëù!,#,ùë°!,#,ùëû!,#)(ùëù!,$,ùë°!,$,ùëû!,$)ùëü!2,4=ùë£%ùëü!1,4=ùë£$ùëü!1,3=ùë£$ùëü!2,3=ùë£&ùëã!‚Äô(ùë£#,$,ùë°(ùëñ,ùë£#,$,ùëÇ(ùëñ,ùë£#,$))1,22,32,4ùëù!,$ùëù!,#ùëã!‚Äô(ùë£$,&,ùë°(ùëñ,ùë£$,&,ùëÇ(ùëñ,ùë£$,&))ùëã!‚Äô(ùë£$,%,ùë°(ùëñ,ùë£$,%,ùëÇ(ùëñ,ùë£$,%))ùë§(ùë£#,$,ùë£$,%)remain equal, unless packet drop happens. [16] have veriÔ¨Åed
its superiority in numerical extrapolation in neural network
readout.

C. Routing Role Recognition based Attention

Inspired by the work of [8], identiÔ¨Åcation of key nodes in the
SDN is essential to estimate the whole networks performance.
For this purpose, we involve role recognition [23] during the
pre-processing phase (denote as different colors in Fig. 3a
(iv)). By deÔ¨Åning vs, vd ‚àà V L
i , we consider
i
their role to be R(vs), R(vd). We deÔ¨Åne the role adjacency
matrix AR of GL
i :

as nodes in GL

AR(s,d) =

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

1,

if R(vs) = R(vd), and exists p ‚àà Pi,
such that vs, vd ‚àà p

0, otherwise

We consider the pair-wise similarity between the role of a
node and its neighbors to be crucial, since an OD pair‚Äôs source
or destination can bring its impact towards close neighbours
of nodes of interest and formulate their embedding as an
‚Äúaugmented source.‚Äù GAT is reported to be a well-suited
solution to capture such pair-wise similarity impact. Thus,
we apply GAT layers on node embedding between each role
adjacency pair. The motivation is to capture the long-range
dependency of in-trajectory of OD pair, without collecting all
hops‚Äô feature in the trajectory. For the aforementioned A(cid:96), one
can consider that denser connected nodes in GL
i are usually
congested. Thus, its state (e.g., occupancy) is dependant on
its neighbors‚Äô states. For less connected nodes, one can infer
that they have few OD‚Äôs pairs passing through. Thus, their
states are more dependent on the few passing through OD
pair‚Äôs demanding trafÔ¨Åc. This information might be submerged
for those well-connected nodes in the OD pair trajectory, but
more obvious on the weakly-connected nodes sharing the same
trajectory. Such pair-wise state similarity forms AR, which
enlarges the receptive Ô¨Åeld of the model beyond topological
neighbors. Our results show that this approach improves the
model performance as well as it introduces generalization
ability when training and inference takes place using samples
with longer chains of trajectories.

V. EXPERIMENTS

A. Dataset and Baselines

Dataset. We verify the performance of the model on a sim-
ulated dataset provided by [24]. The training, validation and
test sets are produced by a packet level simulator (OMNeT++
v5.5.1 [25]). Though lacking of other benchmark dataset, this
dataset includes a summary of patterns of real-world network
topologies from The Internet Topology Zoo [26]. The training
set includes a variety of networks sized from 25 to 50 network
nodes, while the test and validation set are sized from 50 to
300 network nodes. The larger network brings the following
attribute changes: (1) Network topology samples introducing
a few longer OD pair‚Äôs trajectories; (2) Network topology
samples introducing larger capacity attribute per link, as well

(a)

(b)

Figure 3: (a) Pre-processing: Incorporate OD Information in
Node‚Äôs Local Structural Feature; (b) Learning and Inference:
Extract Neighboring Impact and Long Range Dependencies

j( ÀúAxij) is a diagonal matrix. The result of

where ÀúDxii = (cid:80)
each DGCN block can be formulated as:
H (l+1) = ||(fF (H (l), ÀúAF

(cid:96) ), Œ±fSin (H (l), ÀúASin),
Œ≤fSout (H (l), ÀúASout )).

(6)

(7)

We expect fF , fSin and fSout
to capture the global trafÔ¨Åc
dependency, in-trafÔ¨Åc dependency and out-trafÔ¨Åc dependency
respectively. One could foresee that the deeper the model is
build, such dependency can be more precisely captured. We
used EdgeDrop [22] as well as skip-connection to avoid over-
smoothing, both of which have been found to help building
deeper networks during the experimentation.

B. Embedding and Readout Function with Extrapolation Abil-

ity

For handling the out of distribution attribute value per-
node in larger graph, we have used NALU [16] to replace
MLP in common GNN setup. Another motivation is to build
equivalence in the embedding space between a node‚Äôs out-
trafÔ¨Åc attribute and its in-trafÔ¨Åc attributes‚Äô from its neighbors.
This is because the in-trafÔ¨Åc and out-trafÔ¨Åc of a node should

ùëù!ùëù"ùëù#ùë£$!ùë£$"ùë£$#ùë£$%ùë£$&ùë£$‚Äôùë£$(ùë£$)(ùëñ)(ùëñùëñ)(ùëñùëñùëñ)(ùëñùë£)ùëù!ùëù"ùëù#ùëù!ùëù"ùëù#ùë£*"ùë£*#ùë£*%ùë£*&ùë£*(ùë£*+ùë£*!ùë£*!ùë£*+ùë£*"ùë£*#ùë£*%ùë£*&ùë£*(DGCNEdgeDropDGCNGATNALUùê¥!ùê¥"‚Ä¶NALUFCN[ùë•#"ùë£$",ùë•%"ùë£$",‚Ä¶,ùë•&"(ùë£$")]GATDGCN‚Ä¶ùë†ùëòùëñùëù‚àíùëêùëúùëõùëõùëíùëêùë°ùëñùëúùëõùê¥"‚Äôùê¥"(!"ùê¥"(#$%‚Ä¶as different ‚Äúnext-hop‚Äù choice encoded in Ri, for OD pairs
to generate their trajectory in a different routing preference;
(3) Network topology samples introducing both sampling
methods (1) and (2), as described above. A description of the
used dataset is shown in Table II.3
Baselines. We have considered several state-of-the-art models
to compare our proposed model.

1) RouteNet [18] is a message-passing-based GNN solution
to estimate per OD pair‚Äôs latency in SDN on bipartite
graph problem formulation.

2) RouteNet-Salzburg [19] is an optimization model based
on RouteNet that scales to different routing policies on
same sized network topologies on estimating per OD
pair‚Äôs latency. This is a winning solution of [24].

3) RouteNet-GA4

is a Ô¨Åne-tuned solution based on
RouteNet with better generalised performance on various
OD pair trajectory length, this is a winning solution of
[24].

4) DGCN [21] is a graph spectral convolution solution gen-
eralized for directed graph scenario. In our experiment,
we apply NALU as its embedding and readout function,
as well as same hyper-parameters and pre-processing
scheme as the proposed model for a fair comparison.
5) Scalable-RouteNet [5], [27]5 is a RouteNet based im-
proved solution to estimate per OD pair‚Äôs latency in size-
variant SDN on tripartite graph problem formulation.

B. Experiment Settings

We have implemented our proposing solution with Keras
as well as tf-geometric [28]. Furthermore, our solution
requires no-GPU, whereas for the other benchmark solu-
tions that do require GPU, we have been training using
one Nvidia Geforce 1080 Ti and GNU-Parallel
for resource management. For training the comparative bench-
mark models, we used the recommended parameter sets re-
ported by the authors, but with the uniÔ¨Åed learning rate r =
10‚àí3, epoch number = 250, epoch size = 4000, loss function
as MAPE as well as input feature space for a fair comparison.
We train Ô¨Åve independent random seeds for each model and

3Detailed information on feature distribution as well as comparison of the
training and testing dataset is given in Appendix.D. Link to the appendix same
as above.
4Code

https://github.com/ITU-AI-ML-in-5G-Challenge/

available

at

PS-014.2-GNN-Challenge-Gradient-Ascent

5We report their published results, however, we have not been given access

to their implementation to reproduce their results.

Table II: Network Topology Graph Set Statistics

Dataset

Training Set

Validation & Test set

Number of Samples
Graph size
Average Degree (mean, std)
Average
(mean, std)
Diameter (mean, std)
Cluster CoefÔ¨Åcient (mean, std)

Shortest

Path

120000
[25, 50]
(9.778, 0.9491)
(2.379, 0.1372)

3120
[50, 300]
(9.523, 1.268)
(3.384, 0.456)

(4.458, 0.6441)
(0.131, 0.0243)

(6.667, 1.280)
(0.0427, 0.0342)

i (V L

i , EL

report the averaged solution. The proposed method, based on
aforementioned preprocessing and formulation, are taking the
GL
i (v) and edge weight
wi in its input layer, with a 32-dim output layer for each node
embedding in V L
i . A detailed list of training parameters is
published on https://github.com/bluelancer/GNNET.git.

i ) as well as node feature X L

C. Experiment Result

We evaluate the performance of our model in the following
experiments: (1) Table III reports the overall performance of
all the test set and we note their difference in MAPE through
Delta ; (2) Fig. 4 reports model performance on each interval
of size samples from the test set; (3) Table IV reports mean
inference speed on a selection of certain sized samples from
test set. Here we are only concerned with the time interval of
inference, without any pre/post-processing time, as they can
be parallelized in an actual application. We do not include
results from Scalable-RouteNet due to the use of different
hardware. (4) Fig. 5 reports an ablation study towards the
decision of adopting NALU instead of MLP in proposed model
when extrapolating towards different size of graph. Fig. 6
reports a sensitivity study result on most affecting parameter
and conÔ¨Åguration in training the proposed model.
Table III: Average Performance (%)

Model

Our model
RouteNet
RouteNet-Salzburg
RouteNet-GA
DGCN
Scalable-RouteNet6

MAPE

2.317
274.5
596.8
557.8
2.935
10

Delta

‚àí
272.2
594.5
555.5
0.618
7.683

Table IV: Inference Speed (ms) per Topology vs. Topology
Size

Topology Size

50

100

200

300

Our model
RouteNet
RouteNet-Salzburg
RouteNet-GA
DGCN

161.5
485.3
0.41 hr
1363
105.5

373.3
1972
0.4 hr
4786
220.3

1115
6084
0.39 hr
19913
861.9

2718
21264
0.44 hr
71767
1770

Based on the above results, one can see that our model
inference
outperforms all baselines in terms of accuracy,
speed and generalization ability to open-world input. For
Table III, we observed that our model outperforms in terms
of accuracy. We consider this veriÔ¨Åes the validity of both
our modiÔ¨Åed problem formulation, and the superiority of the
proposed model. One can observe that other well-studied GNN
benchmark (i.e:DGCN) can also outperform greatly against
the carefully designed task-speciÔ¨Åc model, which can be
concluded into the effect of problem formulation.
is faster
For Table IV, one can conclude that our model
than most
task-speciÔ¨Åc benchmark models (i.e: Routenet,
RouteNet-Salzburg, and RouteNet-GA). The proposed model‚Äôs
inference time remains tolerable for SDN routing estimation
component, even for the largest network we have (300 nodes).

6The reported result is referenced from [5]

(a) Proposed Model

(b) DGCN

(c) Scalable-RouteNet6

(d) RouteNet

(e) RouteNet-GA

(f) RouteNet-Salzburg

Figure 4: Model Generalization Ability Towards Open-World Input

DGCN performs marginally faster as it is one of the building
blocks of the proposed model, while the proposed model is
more accurate and has a more stable generalization perfor-
mance against open-world input.
For Figure 4, we conclude our proposed solution performs
best in generalization ability to open-world input. Note that
comparing solely DGCN and our model, we could see AR not
only introduces more accurate estimation, but also increases
the performance stability. We believe that this improvement is
due to introducing role inference in our model, which provides
an easy way to diffuse long-range dependencies.
For Figure 5, we veriÔ¨Åed the beneÔ¨Åt that we bringing in NALU
instead of MLP in common GNN architecture, we observed
using NALU obtains marginal gain on median accuracy but
more on variance of the accuracy. This veriÔ¨Åed the conclusion
from [16] that NALU assists in modeling numerical extrapo-
lation in larger routing network samples.

VI. CONCLUSION

Figure 5: Ablation Study of the Proposed Model w.r.t NALU
and MLP Embedding & Readout Function

orchestration scheme, which eventually results in a dynamic-
routing topology. Generalization to dynamic and size-variant
graphs with GNN models remain to be studied in future work.

The paper offers the following contributions from the state
of art: (i) We proposed a novel formulation of the latency
prediction task in SDN. (ii) We proposed a solution that
incorporates domain knowledge in SDN optimization aimed
at an open-world learning setup. (iii) Our proposed model
achieves better accuracy, inference speed and generalization
ability beyond state of the art, all while using less computa-
tional resources.A limitation in our approach is that we do
not incorporate the scenarios of peak hours, where unseen
trafÔ¨Åc triggers the network
amount and duration of burst

ACKNOWLEDGEMENT

This work was partially supported by the Wallenberg AI,
Autonomous Systems and Software Program (WASP) funded
by the Knut and Alice Wallenberg Foundation. We thank
Dr. Pedro Batista and Dr. Alessandro Previti from Ericsson
Research for the insightful discussions.

REFERENCES

[1] J. Parmar, S. S. Chouhan, and S. S. Rathore, ‚ÄúOpen-world machine
learning: Applications, challenges, and opportunities,‚Äù arXiv preprint
arXiv:2105.13448, 2021.

[50,99][100,149][150,199][200,249][250,300]Topology Size02468Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]Topology Size02468Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]TopologySize0%5%10%15%20%AbsoluteRelativeError[50,99][100,149][150,199][200,249][250,300]Topology Size02004006008001000Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]Topology Size05001000150020002500Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]Topology Size05001000150020002500Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]Topology Size0246810Absolute Relative Error (%)funcNALU Embedding & ReadoutMLP Embedding & Readout[20] Y. Kong, D. Petrov, V. R¬®ais¬®anen, and A. Ilin, ‚ÄúPath-link graph neural
network for ip network performance prediction,‚Äù in 2021 IFIP/IEEE
International Symposium on Integrated Network Management (IM).
IEEE, 2021, pp. 170‚Äì177.

[21] Z. Tong, Y. Liang, C. Sun, D. S. Rosenblum, and A. Lim, ‚ÄúDirected
graph convolutional network,‚Äù arXiv preprint arXiv:2004.13970, 2020.
[22] Y. Rong, W. Huang, T. Xu, and J. Huang, ‚ÄúDropedge: Towards deep
graph convolutional networks on node classiÔ¨Åcation,‚Äù arXiv preprint
arXiv:1907.10903, 2019.

[23] K. Henderson, B. Gallagher, T. Eliassi-Rad, H. Tong, S. Basu,
L. Akoglu, D. Koutra, C. Faloutsos, and L. Li, ‚ÄúRolx: structural role
extraction & mining in large graphs,‚Äù in Proceedings of the 18th ACM
SIGKDD, 2012, pp. 1231‚Äì1239.

[24] J. Su¬¥arez-Varela et al., ‚ÄúThe graph neural networking challenge: a
worldwide competition for education in ai/ml for networks,‚Äù ACM
SIGCOMM Computer Communication Review, vol. 51, no. 3, pp. 9‚Äì
16, 2021.

[25] A. Varga, ‚ÄúDiscrete event simulation system,‚Äù in Proc. of the European

Simulation Multiconference (ESM‚Äô2001), 2001, pp. 1‚Äì7.

[26] S. Knight, H. X. Nguyen, N. Falkner, R. Bowden, and M. Roughan,
‚ÄúThe internet topology zoo,‚Äù IEEE Journal on Selected Areas in Com-
munications, vol. 29, no. 9, pp. 1765‚Äì1775, 2011.

[27] M. Ferriol-Galm¬¥es, K. Rusek, J. Su¬¥arez-Varela, S. Xiao, X. Cheng,
P. Barlet-Ros, and A. Cabellos-Aparicio, ‚ÄúRoutenet-erlang: A graph
neural network for network performance evaluation,‚Äù 2022.

[28] J. Hu, S. Qian, Q. Fang, Y. Wang, Q. Zhao, H. Zhang, and C. Xu,

‚ÄúEfÔ¨Åcient graph deep learning in tensorÔ¨Çow with tf-geometric,‚Äù 2021.

Figure 6: Sensitivity Study on the criteria of Mean Absolute
Relative Error (MAE) w.r.t a Selection of Most Performance-
affecting Hyper-parameter

[2] D. Gamarnik and S. K. Gupta, ‚ÄúAlgorithmic challenges in the theory of

queueing networks,‚Äù 2011.

[3] R. Amin, E. Rojas, A. Aqdus, S. Ramzan, D. Casillas-Perez, and J. M.
Arco, ‚ÄúA survey on machine learning techniques for routing optimization
in sdn,‚Äù IEEE Access, 2021.

[4] V. Srivastava and R. S. Pandey, ‚ÄúMachine intelligence approach: To
solve load balancing problem with high quality of service performance
for multi-controller based software deÔ¨Åned network,‚Äù Sustainable Com-
puting: Informatics and Systems, vol. 30, p. 100511, 2021.

[5] M. Ferriol-Galm¬¥es, J. Su¬¥arez-Varela, K. Rusek, P. Barlet-Ros, and
A. Cabellos-Aparicio, ‚ÄúScaling graph-based deep learning models to
larger networks,‚Äù arXiv preprint arXiv:2110.01261, 2021.

[6] F. Ciucu and J. Schmitt, ‚ÄúPerspectives on network calculus: no free
lunch, but still good value,‚Äù in Proceedings of the ACM SIGCOMM 2012
conference on Applications, technologies, architectures, and protocols
for computer communication, 2012, pp. 311‚Äì322.

[7] K. Xu, M. Zhang, J. Li, S. S. Du, K.-i. Kawarabayashi, and S. Jegelka,
‚ÄúHow neural networks extrapolate: From feedforward to graph neural
networks,‚Äù arXiv preprint arXiv:2009.11848, 2020.

[8] P. Sun, J. Li, Z. Guo, Y. Xu, J. Lan, and Y. Hu, ‚ÄúSinet: Enabling scalable
network routing with deep reinforcement learning on partial nodes,‚Äù
in Proceedings of the ACM SIGCOMM 2019 Conference Posters and
Demos, 2019, pp. 88‚Äì89.

[9] T. N. Kipf and M. Welling, ‚ÄúSemi-supervised classiÔ¨Åcation with graph
convolutional networks,‚Äù arXiv preprint arXiv:1609.02907, 2016.
[10] G. Li, M. M¬®uller, B. Ghanem, and V. Koltun, ‚ÄúTraining graph neural

networks with 1000 layers,‚Äù arXiv preprint arXiv:2106.07476, 2021.

[11] B. Chen, R. Barzilay, and T. Jaakkola, ‚ÄúPath-augmented graph trans-

former network,‚Äù arXiv preprint arXiv:1905.12712, 2019.

[12] Y. Yang, X. Wang, M. Song, J. Yuan, and D. Tao, ‚ÄúSpagan: Shortest
path graph attention network,‚Äù arXiv preprint arXiv:2101.03464, 2021.
[13] P. VeliÀáckovi¬¥c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Ben-
gio, ‚ÄúGraph attention networks,‚Äù arXiv preprint arXiv:1710.10903, 2017.
[14] G. Yehudai, E. Fetaya, E. Meirom, G. Chechik, and H. Maron, ‚ÄúFrom
local structures to size generalization in graph neural networks,‚Äù in
ICML. PMLR, 2021, pp. 11 975‚Äì11 986.

[15] M. Balcilar, G. Renton, P. H¬¥eroux, B. Ga¬®uz`ere, S. Adam, and P. Honeine,
‚ÄúAnalyzing the expressive power of graph neural networks in a spectral
perspective,‚Äù in ICLR, 2020.

[16] A. Trask, F. Hill, S. E. Reed, J. W. Rae, C. Dyer, and P. Blunsom,

‚ÄúNeural arithmetic logic units,‚Äù in NeurIPS, 2018.

[17] A. Madsen and A. R. Johansen, ‚ÄúNeural arithmetic units,‚Äù in ICLR,

2019.

[18] K. Rusek, J. Su¬¥arez-Varela, P. Almasan, P. Barlet-Ros, and A. Cabellos-
Aparicio, ‚ÄúRoutenet: Leveraging graph neural networks for network
modeling and optimization in sdn,‚Äù IEEE Journal on Selected Areas
in Communications, vol. 38, no. 10, pp. 2260‚Äì2270, 2020.

[19] M. Happ, M. Herlich, C. Maier, J. L. Du, and P. DorÔ¨Ånger, ‚ÄúGraph-
neural-network-based delay estimation for communication networks
with heterogeneous scheduling policies,‚Äù ITU Journal on Future and
Evolving Technologies, vol. 2, no. 4, 2021.

DGCNGCNBackBone Network Selection0123MAE (%)0.30.50.8Edge_Drop_Rate012MAE (%)1234GCN layers0123MAE (%)24816Attention_Heads024MAE (%)