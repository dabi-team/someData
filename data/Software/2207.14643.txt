Open World Learning Graph Convolution for
Latency Estimation in Routing Networks

Yifei Jin
KTH Royal Institute of Technology & Ericsson AB
Division of Theoretical and Computer Science & Ericsson Research
Stockholm, Sweden
yifeij@kth.se

Marios Daoutis
Ericsson AB
Ericsson Research
Stockholm, Sweden
marios.daoutis@ericsson.com

Sarunas Girdzijauskas
KTH Royal Institute of Technology
Division of Software and Computer Systems
Stockholm, Sweden
sarunasg@kth.se

Aristides Gionis
KTH Royal Institute of Technology
Division of Theoretical and Computer Science
Stockholm, Sweden
argioni@kth.se

Abstractâ€”Accurate routing network status estimation is a key
component in Software Deï¬ned Networking. However, existing
deep-learning-based methods for modeling network routing are
not able to extrapolate towards unseen feature distributions. Nor
are they able to handle scaled and drifted network attributes
in test sets that include open-world inputs. To deal with these
challenges, we propose a novel approach for modeling network
routing, using Graph Neural Networks. Our method can also
be used for network-latency estimation. Supported by a domain-
knowledge-assisted graph formulation, our model shares a stable
performance across different network sizes and conï¬gurations of
routing networks, while at the same time being able to extrapolate
towards unseen sizes, conï¬gurations, and user behavior. We show
that our model outperforms most conventional deep-learning-
based models, in terms of prediction accuracy, computational
resources, inference speed, as well as ability to generalize towards
open-world input.

Index Termsâ€”Graph Convolution, Software Deï¬ne Networks,

Open World Learning

I. INTRODUCTION

Software-deï¬ned networking (SDN) is a state-of-the-art
approach to network management, which permits dynamic and
programmatic network conï¬gurations (e.g., routing), aimed
at improved network performance and monitoring, abstracted
away from individual network elements into a centralized net-
work control layer. Consequently, the orchestration and SDN
control software is expected to operate on and adapt to unseen
network deployments and topologies. Precise routing-network
modeling is a prerequisite for SDN to efï¬ciently estimate and
forecast network state. As a result, in certain scenarios, such
as during network expansion, it is expected that: (i) several
network features (possibly used in model training) become
greatly imbalanced; (ii) unseen attribute values are expected
to be out-of-distribution; (iii) varying, often larger, network
sizes are encountered; and (iv) relational dependencies expand
among longer node chains. An illustrative example of above is
shown in Fig. 1. A successful approach to deal with the above-
mentioned challenges, is to employ ideas from the domain

Figure 1: Schematic Representation of SDN Network State
Estimation Problem. Directed line denotes network trafï¬c
ï¬‚ows with different throughput in the given topology, where
one can see that there exist different distributions of line type
between training and validation/test time, as mentioned in (i).
Undirected line denote the link communicating pair of network
elements, with the lineâ€™s strength denoting the linkâ€™s capacity
level.

that

of open-world machine learning (OWL) [1]. Compared with
traditional machine learning, OWL is expected to extrapolate
towards unseen input distribution, classes, and scaling in
the training set. Note,
the problem of estimating the
network state is EXPTIME-complete, as shown by [2], and
has been extensively studied in the literature [3], where most
commonly conventional deep-learning methods are employed
in the context of reinforcement learning (RL) based network
orchestration as well as deep-learning-based service optimiza-
tion.

In this paper we consider the problem of estimating the
latency between a source and a destination network node
(denoted as OD pair) in a routing network. The problem setting
is to learn a model on smaller networks and extrapolate the
predictions on larger networks assuming open-world input, as
illustrated in Fig. 1.

2
2
0
2

l
u
J

8

]
I

N
.
s
c
[

1
v
3
4
6
4
1
.
7
0
2
2
:
v
i
X
r
a

Training timeProposed SolutionSmaller NetworkNetwork TopologyTraffic & Routing Configper OD-pair latency(ğ‘–ğ‘–ğ‘–)(ğ‘–ğ‘–)Prediction(ğ‘–ğ‘£)Larger NetworkValidation/Test time 
 
 
 
 
 
Conventional deep learning (DL) methods have focused in
predictive user behavior modeling on service demand and data
usage. In this setting, a line of work focuses on leveraging deep
neural networks for adaptive optimisation of routing schemes,
as by [4], which typically require similar data distribution in
the test environment. Only few papers have considered to ex-
trapolate their proposed model on a larger network [5], some-
thing which, this far, has been considered infeasible in real-
world scenarios. Moreover, successful extrapolation would
require introducing task-speciï¬c non-linearity (i.e., queuing
theory and network calculus in the routing network [6]) in the
model [7], something which, this far, has not been considered
in state-of-the-art DL solutions.

Graph Neural Networks (GNNs) have been used success-
fully for incorporating domain knowledge into different prob-
lem settings, which in turn, help to devise robust models better
suited for OWL in learning complex-system representations.
Moreover, our work is primarily inspired by [8] who reveal
that by solely observing key nodes in the SDN topology, the
deep network can produce effective embeddings capable of
predicting network performance.

Our study is among the ï¬rst approaches that attempt to
the routing network state snapshot by learning the
model
given topology structure for the task of estimating the net-
work latency using open-world input. Our proposed solution
transforms the task of estimating the latency between source
and destination nodes, to a link-attribute prediction task, i.e.,
predicting each link that can potentially contribute to trafï¬c
delays occurring in the routing trajectory. Estimating every
linkâ€™s latency contribution requires three aspects: (i) the trafï¬c
amount passing through the link; (ii) the capacity of the
link; and (iii) linkâ€™s structural features. The link trafï¬c and
capacity jointly denote the data throughput, and the congestion
state of the given link. The linkâ€™s structural features can
be learned from the network topology and they serve two
purposes: (i) to identify key links that can become bottleneck
of the network performance; and (ii) to compute pair-wise
similarities, which are used to discover similar links. We train
our model on distributions of a particular network size, yet,
the model achieves at least 75% reduction in mean absolute
percentage error (MAPE) measured in different sized networks
during inference, i.e., without explicit training using the larger
(up to 6Ã—) network topologies. Last but not least, our solution
requires no GPU resource while still ensures a faster training
as well as 3â€“7 times faster inference speeds, with only 6-
25% embedding size required, when compared, for example,
with the best-performing benchmark models. The remaining
sections are organised as follows: In Sec. II we discuss the
related approaches in the domain of size generalisation of
GNNs and graph-based ML in SDN routing, while in Sec. III
we present our problem formulation in more detail. In Sec. IV
we present our methodology followed by Sec. V, which
contains the experimental evaluation of our algorithm. Finally
Sec. VI concludes with a discussion of our main contributions
and considerations for future work.

II. RELATED WORK

Graph Convolutional Networks (GCN) is a known feature-
extraction technique for non-Euclidean spaces [9]. GCNs are
capable of learning the graph structure by aggregating each
nodeâ€™s embedding with its neighbors. In the conventional
architecture, GCN is proven to be capable of incorporating
node and edge information in a undirected graph scope. We
are currently witnessing an increasing number of publications
on GCN models, which, on the one hand reveal the success
of this model, and on the other hand expose its limitations.

GCN models are constrained due to aggregating only local
neighbors. For large enough graphs, GCN may fail to capture
distant (long-range) dependencies entirely. Efforts have been
made towards both building deeper GNN models [10] and
building expressive GNN layers on long-range dependencies.
For example, [11] proposed the ï¬rst effective method to
encode long-range dependencies, between any two nodes, of
the graph in the node embedding. Furthermore, [12] pro-
pose to apply graph attention network (GAT), to compute
shortest-path-to-node attention, based on a transformer [13]
architecture. This work is one of the most related approaches
to the one presented here. Contrary to the aforementioned
contributions, which are based on transductive settings, our
work focuses instead on an inductive setting. In our model, we
are incorporating path-to-node attention, but passively between
the pathâ€™s structurally similar node and the targeting node,
to tackle longer chains of dependencies in the test data.
In addition, we do not build very deep GNN solutions to
enlarge the receptive ï¬eld up to the length of dependency,
for balancing between model accuracy, resource usage and
inference speed trade-off.

In a survey by [7], the extrapolation ability of GNNs regards
mainly two aspects: the ability to extrapolate towards unseen
structural features as well as to extrapolate towards out-of-
distribution attributes. [14] conclude that the size general-
ization can only be applied on graphs with certain structural
features. Moreover, they suggest that the expressiveness of the
GNN model must be still sufï¬cient for applying it to larger
graphs. [15] further elaborate from the perspective frequency
response, pointing out that spectral graph convolution is inher-
ently transferable for graphs with similar degree distribution.
In our model, we leverage the conclusions from the afore-
mentioned works and cast the graph-size-generalization prob-
lem to a transferred formulation of graph, which is considered
to be learnable by spectral graph-convolution methods. On the
other hand, extrapolation towards out-of-distribution data is a
fundamental challenge for most neural networks. For out-of-
distribution attributes, the extrapolation ability relies more on
the embedding and readout function of the GNN architecture.
[7] have shown the multi-layered perception models (MLP)
can only extrapolate if the test set
is expanded from the
training set in all geometrical directions in the latent space.
Both [16] and [17] have studied on approximating simple
arithmetic computation (+, âˆ’, Ã—, and /) when extrapolate
towards drifted numerical values. In our proposed model, we

utilize building blocks from [16] to encourage the model to
learn the arithmetic impact, when tackling drifted numerical
input features.

More recent related works focus on learning the tuned graph
representation of routing networks. [18] propose RouteNet,
which is regarded as a ï¬rst work that introduces message
passing in deep-learning-based network modeling, in a bi-
partite graph formulation. [19] and [20] extend RouteNet to
be adaptive to heterogeneous scheduling policies and routing
scheme, while improving its accuracy and inference speed on
similar topology structures. Finally, [5] present their work on
topology size generalization for latency estimation of Origin-
Destination (OD) pairs, the same problem we focus here.
Through improving RouteNet by including queue occupancy
state per link, beside the path and link nodes, they formulate a
tripartite message passing scheme, which introduces size gen-
eralization ability to the model. Yet, none of aforementioned
works utilize the topologyâ€™s structural feature in network-state
embedding, as we address in this work.

III. PROBLEM FORMULATION
The task we consider in this paper is to perform mean
latency estimation on every OD pair p in the network. We
summarize the deï¬ned notation in Table. I. The input to our
problem is the following.

1) A sequence of attributed size-varying networks G =
{Gi(Vi, Ei; Xi, Yi)}, where Vi denotes the |Vi| = ni
network nodes and Ei denotes the |Ei| = mi
links
between pairs of nodes in Vi. For each node v âˆˆ Vi,
a set of node attributes x(v) are provided to describe the
heterogeneous routing conï¬guration on the network node
level. Similarly, a set of link attributes y(e) is introduced
for each link e âˆˆ Ei to denote link conï¬guration attributes
with respect to routing. The set of all node attributes in
Gi is denoted by Xi, and the set of all link attributes is
denoted by Yi. Each network Gi includes ki OD pairs
Pi = {pi,j}, with j = 1, . . . , ki.

2) A sequence of trafï¬c matrices T = {Ti}, where each
Ti corresponds to a network Gi. Each trafï¬c matrix Ti
consists by ki vectors {ti,j}, with j = 1, . . . , ki, where
each ti,j âˆˆ R2 denotes the mean throughput and peak
throughput features for the OD pair pi,j. Note that there
exist OD pairs with same source and destination nodes
but different throughput features.

3) A sequence of routing matrices R = {Ri}, where each
Ri with dimension RniÃ—ni, with 0â€™s on the diagonal
elements, corresponds to network Gi. Each element in Ri,
denoted by Ri(c,d) = ri(vc, vd); vc, vd âˆˆ Vi represents the
next-hop per OD pair, given the current node vc and the
destination node vd. Note that Ri contains information
for all possible routing pairs, regardless of the given OD
pair elements in Ti.

4) The ground truth: A sequence of performance matrices
Q = {Qi}, where each Qi corresponds to network Gi.
Each performance matrix Qi consists of ki vectors {qi,j},
with j = 1, . . . , ki, denoting the performance of each

OD pair pi,j in global (mean latency) and local (per-
link occupancy) metrics. The task is to estimate the mean
latency value of each OD pair, denote as an element in
qi,j = Qi(pi,j), with j = 1, . . . , ki.

Previous works [5], [18]â€“[20] formulate the problem deï¬ned
above as a multi-step prediction problem. More speciï¬cally,
given network snapshots (Gi, Ti, Ri), the goal is to predict the
future âˆ† steps of the network state [Q(cid:48)1
i , . . . , Q(cid:48)âˆ†
] and pool
i
the result of each step for a prediction Q(cid:48)
i. In contrast, in our
model, we consider this problem as a node attribute prediction
problem. We introduce a modiï¬ed problem formulation:

1) Given the input to our problem speciï¬ed by a sequence
of tuples (Gi(Vi, Ei; Xi, Yi), Ti, Ri), we ï¬rst transform
Gi(Vi, Ei; Xi, Yi) to a directed graph, where each undi-
rected edge (u, v) âˆˆ Ei is replaced by two directed edges
(u, v) and (v, u), and then transform it to a line graph
i ). We deï¬ne a projection Lâˆ’1, to project the
GL
representation (i.e: vi âˆˆ V L
i ) in line graph GL
i âˆˆ EL
i , eL
i ,
back to its representation in Gi,

i , EL

i (V L

i of the line graph GL
i

2) The edge set EL
is directed. The
node set V L
includes only valid routings in Ri, i.e., for
i
all v âˆˆ V L
its respective edge representation Lâˆ’1(v) =
i
e(vx, vy) is in Ei. Thus, there exists vd âˆˆ Vi such that
ri(vx, vd) = vy holds.

3) We can compute the summed trafï¬c T s

i per-link over all
OD trajectory pairs passing through, for all ni nodes.
Each element is denoted by ts(i, v) = T s
i (v), for all
i . Given node vâ€™s edge representation Lâˆ’1(v) =
v âˆˆ V L
ev âˆˆ Ei, We deï¬ne the node attributes X L
i as
follows:

i (v) in GL

X L

i (v) =

ts(i, v)
c(Yi(ev))

and ts(i, v) =

(cid:88)

Ti(pk),

pk;vâˆˆpk

(1)

for v âˆˆ V L
i

and ev âˆˆ Ei and Lâˆ’1(v) = ev

where c(Yi(ev)) âˆˆ Yi(Ei), denotes the capacity per-link
in Gi.

4) For each edge e âˆˆ EL

i connecting node vs to node vd in
i . We introduce corresponding edge weight wi(vs, vd)

GL
as follows:

wi(vs, vd) =

(cid:80)

Ti(pk)

pk;vs,vdâˆˆpk
ts(i, vs)

Yi(evs)
Yi(evd )
for vs, vd âˆˆ V L
and evs, evd âˆˆ Ei,
i
and Lâˆ’1(vs) = evs, Lâˆ’1(vd) = evd ,

Â·

,

(2)

The motivation behind this re-formulation is multi-faceted:
(i) Using directed links in GL
introduces OD trajectory pairs
i
information into the graph structure. The new formulation
separates the linkâ€™s adjacency into in-degree and out-degree,
which will be used to represent the linkâ€™s in-coming trafï¬c
(in-trafï¬c) and out-going trafï¬c (out-trafï¬c). (ii) For the task
of estimating path latency Qi(pi,j),
the problem can be
decomposed into estimating the latency contribution of each
hop v âˆˆ V L
Qi(v). According to queuing theory,
i
such latency contribution further reduces to estimating vâ€™s
queue occupancy state O(i, v) âˆˆ Qi. Thus, in the graph GL
i ,

into (cid:80)

v;vâˆˆpk

Table I: Notation Table

Training/testing set of Topology

Network Topology Feature Notation:
G = {Gi}
Gi(Vi, Ei; Xi, Yi) Topology snapshot graph
Vi = {vi}
ni = |Vi|
Ei = {ei}
Xi = {x(vi)}
Yi
c(Y (ei))

Network nodes in Gi
Size of Gi
Communication links between network nodes
Network node conï¬guration of Gi
Communication link conï¬guration of Gi
Conï¬gured link capacity of ei âˆˆ Ei

Network Service Feature Notation:
OD pairs in Gi
Pi = {pi,j }
j-th OD-pair in Gi
pi,j
Number of OD pairs in Gi
ki
T = {Ti}
Training/testing set of user trafï¬c
User trafï¬c matrix for all Pi
Ti = {ti,j }
User trafï¬c features for pi,j âˆˆ Pi
ti,j

Network Routing Feature Notation:
R = {Ri}
Ri
Ri(c,d)

Training/testing set of routing matrices
Routing matrices corresponds to network Gi
Next-hop of current node and destination being vc, vd âˆˆ
Vi

Ground Truth Notation:
Q = {Qi}
Qi
qi,j

Training/testing set of performance matrix
Performance matrix correspond to (Gi, Ti, Ri)
Performance measurement (i.e: latency) of pi,j

Novel Feature Notation in reformulating the problem :
GL
Lâˆ’1

i , EL
i )

i (V L

i to its correspond-

Line graph transformation of Gi(Vi, Ei)
Projection from representation in GL
ing representation in Gi
Matrix of summed trafï¬c of pi,j , who share the same
node v âˆˆ V L
i
Summed trafï¬c for all pi,j passing through node v âˆˆ
V L
i
Line graph node feature, introduced in Equ. 1
Line graph edge weight, introduced in Equ. 2
node vâ€™s queue occupancy state (ground truth), for v âˆˆ
V L
i

T s
i

ts(i, v)

i (v)

X L
wi
O(i, v)

(cid:96)

and ASout
, which denote ï¬rst-order proximity, second-order
in-proximity and second-order out-proximity, as shown in
Fig. 3b. Given the weighted adjacency matrix A(cid:96) of the line
graph GL

i , one can compute the aforementioned quantities:

AF

(cid:96) = (A(cid:96) + AT

(cid:96) )/2,

(3)

ASin
(cid:96)(i,j)

=

(cid:88)

k

A(cid:96)(k,i)A(cid:96)(k,j)
(cid:80)
v A(cid:96)(k,v)

and ASout
(cid:96)(i,j)

=

(cid:88)

k

,

A(cid:96)(i,k) A(cid:96)(j,k)
(cid:80)
v A(cid:96)(v,k)
for (i, j) âˆˆ V L
i
(4)

We apply graph convolution to the aforementioned adjacency
matrix, with adding self-loops, denoted by ËœAF
(cid:96) + I,
ËœASin = ASin +I,
ËœASout = ASout +I. We concatenate (denote
as ||) each result with a train-able weight (Î±, Î²). Î˜ is a train-
able matrix. We deï¬ne

(cid:96) = AF

fx(X, Ax) = ËœDx

(âˆ’ 1

2 ) ËœAx ËœDx

(âˆ’ 1
2 )

XÎ˜,

(5)

Figure 2: Comparative Example of the Problem Formulation,
with Ground Truth being Red. Left: Previous worksâ€™ formu-
lation. Right: The proposed formulation

the problem is reduced to a node attribute prediction task.1
(iii) Inspired by previous work, identifying key link that is
shared by many OD pairs could have a dominating impact on
the whole network performance. In the graph GL
i , such feature
is disclosed within the ego-networks of nodes v âˆˆ V L
i , which
can be jointly formed by the trajectories of all paths passing
through v. Thus, one can discover the importance of node v in
network topology by its role in GL
i . (iv) The features computed
in Eq. 1 and Eq. 2 are supported by queuing theory;2 here
we consider domain knowledge to be the key in producing a
size-invariant graph signal on GL
i . An illustrative example of
re-formulation, comparing with the previous worksâ€™ is given
in Fig. 2.

IV. METHODOLOGY

Fig. 3 provides an overview of our proposed model for
the prediction task. In the ï¬rst phase (Fig. 3a), we transform
the input graph Gi (Fig.3a (i), (ii)) into the line graph GL
i
(Fig.3a (iii), (iv)) for the model to learn in a latent space
that generalizes well on different
input graph sizes. Next
(Fig. 3b), we separate the graph into a directed sub-graph
and a un-directed sub-graph that capture the neighboring
sequential relation as well as the impact of key nodes. We
design the corresponding components that capture each impact
and handle out-of-distribution features, so as to get consistent
results across different graph sizes. Below, we discuss each
component separately.

A. Directed Spectral Graph Convolution

To capture the sequential relation between nodes consisting
of the OD pairâ€™s trajectory, as well as the OD pairâ€™s direction,
we use the architecture designed by [21]. It is a spectral
convolution block with its reception ï¬eld encoded with direc-
tion information, named Directed Graph Convolution Network
(DGCN). Given the processed directed GL
i , we separate the
(cid:96) , ASin
adjacency matrix into three different formations: AF

(cid:96)

1A detailed transform procedure is shown in Appendix.A. Link to the
appendix: https://anonymous.4open.science/r/RoutingGNN Appendix-F5ED/
IEEE Graph Covolution for Routing Appendix .pdf

2A detailed proof is given Appendix.B-C, Link to the appendix same as

above.

2143(ğ‘!,#,ğ‘¡!,#,ğ‘!,#)(ğ‘!,$,ğ‘¡!,$,ğ‘!,$)ğ‘Ÿ!2,4=ğ‘£%ğ‘Ÿ!1,4=ğ‘£$ğ‘Ÿ!1,3=ğ‘£$ğ‘Ÿ!2,3=ğ‘£&ğ‘‹!â€™(ğ‘£#,$,ğ‘¡(ğ‘–,ğ‘£#,$,ğ‘‚(ğ‘–,ğ‘£#,$))1,22,32,4ğ‘!,$ğ‘!,#ğ‘‹!â€™(ğ‘£$,&,ğ‘¡(ğ‘–,ğ‘£$,&,ğ‘‚(ğ‘–,ğ‘£$,&))ğ‘‹!â€™(ğ‘£$,%,ğ‘¡(ğ‘–,ğ‘£$,%,ğ‘‚(ğ‘–,ğ‘£$,%))ğ‘¤(ğ‘£#,$,ğ‘£$,%)remain equal, unless packet drop happens. [16] have veriï¬ed
its superiority in numerical extrapolation in neural network
readout.

C. Routing Role Recognition based Attention

Inspired by the work of [8], identiï¬cation of key nodes in the
SDN is essential to estimate the whole networks performance.
For this purpose, we involve role recognition [23] during the
pre-processing phase (denote as different colors in Fig. 3a
(iv)). By deï¬ning vs, vd âˆˆ V L
i , we consider
i
their role to be R(vs), R(vd). We deï¬ne the role adjacency
matrix AR of GL
i :

as nodes in GL

AR(s,d) =

ï£±
ï£´ï£²

ï£´ï£³

1,

if R(vs) = R(vd), and exists p âˆˆ Pi,
such that vs, vd âˆˆ p

0, otherwise

We consider the pair-wise similarity between the role of a
node and its neighbors to be crucial, since an OD pairâ€™s source
or destination can bring its impact towards close neighbours
of nodes of interest and formulate their embedding as an
â€œaugmented source.â€ GAT is reported to be a well-suited
solution to capture such pair-wise similarity impact. Thus,
we apply GAT layers on node embedding between each role
adjacency pair. The motivation is to capture the long-range
dependency of in-trajectory of OD pair, without collecting all
hopsâ€™ feature in the trajectory. For the aforementioned A(cid:96), one
can consider that denser connected nodes in GL
i are usually
congested. Thus, its state (e.g., occupancy) is dependant on
its neighborsâ€™ states. For less connected nodes, one can infer
that they have few ODâ€™s pairs passing through. Thus, their
states are more dependent on the few passing through OD
pairâ€™s demanding trafï¬c. This information might be submerged
for those well-connected nodes in the OD pair trajectory, but
more obvious on the weakly-connected nodes sharing the same
trajectory. Such pair-wise state similarity forms AR, which
enlarges the receptive ï¬eld of the model beyond topological
neighbors. Our results show that this approach improves the
model performance as well as it introduces generalization
ability when training and inference takes place using samples
with longer chains of trajectories.

V. EXPERIMENTS

A. Dataset and Baselines

Dataset. We verify the performance of the model on a sim-
ulated dataset provided by [24]. The training, validation and
test sets are produced by a packet level simulator (OMNeT++
v5.5.1 [25]). Though lacking of other benchmark dataset, this
dataset includes a summary of patterns of real-world network
topologies from The Internet Topology Zoo [26]. The training
set includes a variety of networks sized from 25 to 50 network
nodes, while the test and validation set are sized from 50 to
300 network nodes. The larger network brings the following
attribute changes: (1) Network topology samples introducing
a few longer OD pairâ€™s trajectories; (2) Network topology
samples introducing larger capacity attribute per link, as well

(a)

(b)

Figure 3: (a) Pre-processing: Incorporate OD Information in
Nodeâ€™s Local Structural Feature; (b) Learning and Inference:
Extract Neighboring Impact and Long Range Dependencies

j( ËœAxij) is a diagonal matrix. The result of

where ËœDxii = (cid:80)
each DGCN block can be formulated as:
H (l+1) = ||(fF (H (l), ËœAF

(cid:96) ), Î±fSin (H (l), ËœASin),
Î²fSout (H (l), ËœASout )).

(6)

(7)

We expect fF , fSin and fSout
to capture the global trafï¬c
dependency, in-trafï¬c dependency and out-trafï¬c dependency
respectively. One could foresee that the deeper the model is
build, such dependency can be more precisely captured. We
used EdgeDrop [22] as well as skip-connection to avoid over-
smoothing, both of which have been found to help building
deeper networks during the experimentation.

B. Embedding and Readout Function with Extrapolation Abil-

ity

For handling the out of distribution attribute value per-
node in larger graph, we have used NALU [16] to replace
MLP in common GNN setup. Another motivation is to build
equivalence in the embedding space between a nodeâ€™s out-
trafï¬c attribute and its in-trafï¬c attributesâ€™ from its neighbors.
This is because the in-trafï¬c and out-trafï¬c of a node should

ğ‘!ğ‘"ğ‘#ğ‘£$!ğ‘£$"ğ‘£$#ğ‘£$%ğ‘£$&ğ‘£$â€™ğ‘£$(ğ‘£$)(ğ‘–)(ğ‘–ğ‘–)(ğ‘–ğ‘–ğ‘–)(ğ‘–ğ‘£)ğ‘!ğ‘"ğ‘#ğ‘!ğ‘"ğ‘#ğ‘£*"ğ‘£*#ğ‘£*%ğ‘£*&ğ‘£*(ğ‘£*+ğ‘£*!ğ‘£*!ğ‘£*+ğ‘£*"ğ‘£*#ğ‘£*%ğ‘£*&ğ‘£*(DGCNEdgeDropDGCNGATNALUğ´!ğ´"â€¦NALUFCN[ğ‘¥#"ğ‘£$",ğ‘¥%"ğ‘£$",â€¦,ğ‘¥&"(ğ‘£$")]GATDGCNâ€¦ğ‘ ğ‘˜ğ‘–ğ‘âˆ’ğ‘ğ‘œğ‘›ğ‘›ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ´"â€™ğ´"(!"ğ´"(#$%â€¦as different â€œnext-hopâ€ choice encoded in Ri, for OD pairs
to generate their trajectory in a different routing preference;
(3) Network topology samples introducing both sampling
methods (1) and (2), as described above. A description of the
used dataset is shown in Table II.3
Baselines. We have considered several state-of-the-art models
to compare our proposed model.

1) RouteNet [18] is a message-passing-based GNN solution
to estimate per OD pairâ€™s latency in SDN on bipartite
graph problem formulation.

2) RouteNet-Salzburg [19] is an optimization model based
on RouteNet that scales to different routing policies on
same sized network topologies on estimating per OD
pairâ€™s latency. This is a winning solution of [24].

3) RouteNet-GA4

is a ï¬ne-tuned solution based on
RouteNet with better generalised performance on various
OD pair trajectory length, this is a winning solution of
[24].

4) DGCN [21] is a graph spectral convolution solution gen-
eralized for directed graph scenario. In our experiment,
we apply NALU as its embedding and readout function,
as well as same hyper-parameters and pre-processing
scheme as the proposed model for a fair comparison.
5) Scalable-RouteNet [5], [27]5 is a RouteNet based im-
proved solution to estimate per OD pairâ€™s latency in size-
variant SDN on tripartite graph problem formulation.

B. Experiment Settings

We have implemented our proposing solution with Keras
as well as tf-geometric [28]. Furthermore, our solution
requires no-GPU, whereas for the other benchmark solu-
tions that do require GPU, we have been training using
one Nvidia Geforce 1080 Ti and GNU-Parallel
for resource management. For training the comparative bench-
mark models, we used the recommended parameter sets re-
ported by the authors, but with the uniï¬ed learning rate r =
10âˆ’3, epoch number = 250, epoch size = 4000, loss function
as MAPE as well as input feature space for a fair comparison.
We train ï¬ve independent random seeds for each model and

3Detailed information on feature distribution as well as comparison of the
training and testing dataset is given in Appendix.D. Link to the appendix same
as above.
4Code

https://github.com/ITU-AI-ML-in-5G-Challenge/

available

at

PS-014.2-GNN-Challenge-Gradient-Ascent

5We report their published results, however, we have not been given access

to their implementation to reproduce their results.

Table II: Network Topology Graph Set Statistics

Dataset

Training Set

Validation & Test set

Number of Samples
Graph size
Average Degree (mean, std)
Average
(mean, std)
Diameter (mean, std)
Cluster Coefï¬cient (mean, std)

Shortest

Path

120000
[25, 50]
(9.778, 0.9491)
(2.379, 0.1372)

3120
[50, 300]
(9.523, 1.268)
(3.384, 0.456)

(4.458, 0.6441)
(0.131, 0.0243)

(6.667, 1.280)
(0.0427, 0.0342)

i (V L

i , EL

report the averaged solution. The proposed method, based on
aforementioned preprocessing and formulation, are taking the
GL
i (v) and edge weight
wi in its input layer, with a 32-dim output layer for each node
embedding in V L
i . A detailed list of training parameters is
published on https://github.com/bluelancer/GNNET.git.

i ) as well as node feature X L

C. Experiment Result

We evaluate the performance of our model in the following
experiments: (1) Table III reports the overall performance of
all the test set and we note their difference in MAPE through
Delta ; (2) Fig. 4 reports model performance on each interval
of size samples from the test set; (3) Table IV reports mean
inference speed on a selection of certain sized samples from
test set. Here we are only concerned with the time interval of
inference, without any pre/post-processing time, as they can
be parallelized in an actual application. We do not include
results from Scalable-RouteNet due to the use of different
hardware. (4) Fig. 5 reports an ablation study towards the
decision of adopting NALU instead of MLP in proposed model
when extrapolating towards different size of graph. Fig. 6
reports a sensitivity study result on most affecting parameter
and conï¬guration in training the proposed model.
Table III: Average Performance (%)

Model

Our model
RouteNet
RouteNet-Salzburg
RouteNet-GA
DGCN
Scalable-RouteNet6

MAPE

2.317
274.5
596.8
557.8
2.935
10

Delta

âˆ’
272.2
594.5
555.5
0.618
7.683

Table IV: Inference Speed (ms) per Topology vs. Topology
Size

Topology Size

50

100

200

300

Our model
RouteNet
RouteNet-Salzburg
RouteNet-GA
DGCN

161.5
485.3
0.41 hr
1363
105.5

373.3
1972
0.4 hr
4786
220.3

1115
6084
0.39 hr
19913
861.9

2718
21264
0.44 hr
71767
1770

Based on the above results, one can see that our model
inference
outperforms all baselines in terms of accuracy,
speed and generalization ability to open-world input. For
Table III, we observed that our model outperforms in terms
of accuracy. We consider this veriï¬es the validity of both
our modiï¬ed problem formulation, and the superiority of the
proposed model. One can observe that other well-studied GNN
benchmark (i.e:DGCN) can also outperform greatly against
the carefully designed task-speciï¬c model, which can be
concluded into the effect of problem formulation.
is faster
For Table IV, one can conclude that our model
than most
task-speciï¬c benchmark models (i.e: Routenet,
RouteNet-Salzburg, and RouteNet-GA). The proposed modelâ€™s
inference time remains tolerable for SDN routing estimation
component, even for the largest network we have (300 nodes).

6The reported result is referenced from [5]

(a) Proposed Model

(b) DGCN

(c) Scalable-RouteNet6

(d) RouteNet

(e) RouteNet-GA

(f) RouteNet-Salzburg

Figure 4: Model Generalization Ability Towards Open-World Input

DGCN performs marginally faster as it is one of the building
blocks of the proposed model, while the proposed model is
more accurate and has a more stable generalization perfor-
mance against open-world input.
For Figure 4, we conclude our proposed solution performs
best in generalization ability to open-world input. Note that
comparing solely DGCN and our model, we could see AR not
only introduces more accurate estimation, but also increases
the performance stability. We believe that this improvement is
due to introducing role inference in our model, which provides
an easy way to diffuse long-range dependencies.
For Figure 5, we veriï¬ed the beneï¬t that we bringing in NALU
instead of MLP in common GNN architecture, we observed
using NALU obtains marginal gain on median accuracy but
more on variance of the accuracy. This veriï¬ed the conclusion
from [16] that NALU assists in modeling numerical extrapo-
lation in larger routing network samples.

VI. CONCLUSION

Figure 5: Ablation Study of the Proposed Model w.r.t NALU
and MLP Embedding & Readout Function

orchestration scheme, which eventually results in a dynamic-
routing topology. Generalization to dynamic and size-variant
graphs with GNN models remain to be studied in future work.

The paper offers the following contributions from the state
of art: (i) We proposed a novel formulation of the latency
prediction task in SDN. (ii) We proposed a solution that
incorporates domain knowledge in SDN optimization aimed
at an open-world learning setup. (iii) Our proposed model
achieves better accuracy, inference speed and generalization
ability beyond state of the art, all while using less computa-
tional resources.A limitation in our approach is that we do
not incorporate the scenarios of peak hours, where unseen
trafï¬c triggers the network
amount and duration of burst

ACKNOWLEDGEMENT

This work was partially supported by the Wallenberg AI,
Autonomous Systems and Software Program (WASP) funded
by the Knut and Alice Wallenberg Foundation. We thank
Dr. Pedro Batista and Dr. Alessandro Previti from Ericsson
Research for the insightful discussions.

REFERENCES

[1] J. Parmar, S. S. Chouhan, and S. S. Rathore, â€œOpen-world machine
learning: Applications, challenges, and opportunities,â€ arXiv preprint
arXiv:2105.13448, 2021.

[50,99][100,149][150,199][200,249][250,300]Topology Size02468Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]Topology Size02468Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]TopologySize0%5%10%15%20%AbsoluteRelativeError[50,99][100,149][150,199][200,249][250,300]Topology Size02004006008001000Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]Topology Size05001000150020002500Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]Topology Size05001000150020002500Absolute Relative Error (%)[50,99][100,149][150,199][200,249][250,300]Topology Size0246810Absolute Relative Error (%)funcNALU Embedding & ReadoutMLP Embedding & Readout[20] Y. Kong, D. Petrov, V. RÂ¨aisÂ¨anen, and A. Ilin, â€œPath-link graph neural
network for ip network performance prediction,â€ in 2021 IFIP/IEEE
International Symposium on Integrated Network Management (IM).
IEEE, 2021, pp. 170â€“177.

[21] Z. Tong, Y. Liang, C. Sun, D. S. Rosenblum, and A. Lim, â€œDirected
graph convolutional network,â€ arXiv preprint arXiv:2004.13970, 2020.
[22] Y. Rong, W. Huang, T. Xu, and J. Huang, â€œDropedge: Towards deep
graph convolutional networks on node classiï¬cation,â€ arXiv preprint
arXiv:1907.10903, 2019.

[23] K. Henderson, B. Gallagher, T. Eliassi-Rad, H. Tong, S. Basu,
L. Akoglu, D. Koutra, C. Faloutsos, and L. Li, â€œRolx: structural role
extraction & mining in large graphs,â€ in Proceedings of the 18th ACM
SIGKDD, 2012, pp. 1231â€“1239.

[24] J. SuÂ´arez-Varela et al., â€œThe graph neural networking challenge: a
worldwide competition for education in ai/ml for networks,â€ ACM
SIGCOMM Computer Communication Review, vol. 51, no. 3, pp. 9â€“
16, 2021.

[25] A. Varga, â€œDiscrete event simulation system,â€ in Proc. of the European

Simulation Multiconference (ESMâ€™2001), 2001, pp. 1â€“7.

[26] S. Knight, H. X. Nguyen, N. Falkner, R. Bowden, and M. Roughan,
â€œThe internet topology zoo,â€ IEEE Journal on Selected Areas in Com-
munications, vol. 29, no. 9, pp. 1765â€“1775, 2011.

[27] M. Ferriol-GalmÂ´es, K. Rusek, J. SuÂ´arez-Varela, S. Xiao, X. Cheng,
P. Barlet-Ros, and A. Cabellos-Aparicio, â€œRoutenet-erlang: A graph
neural network for network performance evaluation,â€ 2022.

[28] J. Hu, S. Qian, Q. Fang, Y. Wang, Q. Zhao, H. Zhang, and C. Xu,

â€œEfï¬cient graph deep learning in tensorï¬‚ow with tf-geometric,â€ 2021.

Figure 6: Sensitivity Study on the criteria of Mean Absolute
Relative Error (MAE) w.r.t a Selection of Most Performance-
affecting Hyper-parameter

[2] D. Gamarnik and S. K. Gupta, â€œAlgorithmic challenges in the theory of

queueing networks,â€ 2011.

[3] R. Amin, E. Rojas, A. Aqdus, S. Ramzan, D. Casillas-Perez, and J. M.
Arco, â€œA survey on machine learning techniques for routing optimization
in sdn,â€ IEEE Access, 2021.

[4] V. Srivastava and R. S. Pandey, â€œMachine intelligence approach: To
solve load balancing problem with high quality of service performance
for multi-controller based software deï¬ned network,â€ Sustainable Com-
puting: Informatics and Systems, vol. 30, p. 100511, 2021.

[5] M. Ferriol-GalmÂ´es, J. SuÂ´arez-Varela, K. Rusek, P. Barlet-Ros, and
A. Cabellos-Aparicio, â€œScaling graph-based deep learning models to
larger networks,â€ arXiv preprint arXiv:2110.01261, 2021.

[6] F. Ciucu and J. Schmitt, â€œPerspectives on network calculus: no free
lunch, but still good value,â€ in Proceedings of the ACM SIGCOMM 2012
conference on Applications, technologies, architectures, and protocols
for computer communication, 2012, pp. 311â€“322.

[7] K. Xu, M. Zhang, J. Li, S. S. Du, K.-i. Kawarabayashi, and S. Jegelka,
â€œHow neural networks extrapolate: From feedforward to graph neural
networks,â€ arXiv preprint arXiv:2009.11848, 2020.

[8] P. Sun, J. Li, Z. Guo, Y. Xu, J. Lan, and Y. Hu, â€œSinet: Enabling scalable
network routing with deep reinforcement learning on partial nodes,â€
in Proceedings of the ACM SIGCOMM 2019 Conference Posters and
Demos, 2019, pp. 88â€“89.

[9] T. N. Kipf and M. Welling, â€œSemi-supervised classiï¬cation with graph
convolutional networks,â€ arXiv preprint arXiv:1609.02907, 2016.
[10] G. Li, M. MÂ¨uller, B. Ghanem, and V. Koltun, â€œTraining graph neural

networks with 1000 layers,â€ arXiv preprint arXiv:2106.07476, 2021.

[11] B. Chen, R. Barzilay, and T. Jaakkola, â€œPath-augmented graph trans-

former network,â€ arXiv preprint arXiv:1905.12712, 2019.

[12] Y. Yang, X. Wang, M. Song, J. Yuan, and D. Tao, â€œSpagan: Shortest
path graph attention network,â€ arXiv preprint arXiv:2101.03464, 2021.
[13] P. VeliË‡ckoviÂ´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Ben-
gio, â€œGraph attention networks,â€ arXiv preprint arXiv:1710.10903, 2017.
[14] G. Yehudai, E. Fetaya, E. Meirom, G. Chechik, and H. Maron, â€œFrom
local structures to size generalization in graph neural networks,â€ in
ICML. PMLR, 2021, pp. 11 975â€“11 986.

[15] M. Balcilar, G. Renton, P. HÂ´eroux, B. GaÂ¨uz`ere, S. Adam, and P. Honeine,
â€œAnalyzing the expressive power of graph neural networks in a spectral
perspective,â€ in ICLR, 2020.

[16] A. Trask, F. Hill, S. E. Reed, J. W. Rae, C. Dyer, and P. Blunsom,

â€œNeural arithmetic logic units,â€ in NeurIPS, 2018.

[17] A. Madsen and A. R. Johansen, â€œNeural arithmetic units,â€ in ICLR,

2019.

[18] K. Rusek, J. SuÂ´arez-Varela, P. Almasan, P. Barlet-Ros, and A. Cabellos-
Aparicio, â€œRoutenet: Leveraging graph neural networks for network
modeling and optimization in sdn,â€ IEEE Journal on Selected Areas
in Communications, vol. 38, no. 10, pp. 2260â€“2270, 2020.

[19] M. Happ, M. Herlich, C. Maier, J. L. Du, and P. Dorï¬nger, â€œGraph-
neural-network-based delay estimation for communication networks
with heterogeneous scheduling policies,â€ ITU Journal on Future and
Evolving Technologies, vol. 2, no. 4, 2021.

DGCNGCNBackBone Network Selection0123MAE (%)0.30.50.8Edge_Drop_Rate012MAE (%)1234GCN layers0123MAE (%)24816Attention_Heads024MAE (%)