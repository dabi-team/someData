2
2
0
2

l
u
J

0
1

]
I

A
.
s
c
[

1
v
5
1
5
4
0
.
7
0
2
2
:
v
i
X
r
a

Developing an AI-enabled IIoT platform -
Lessons learned from early use case validation(cid:63)

Holger Eichelberger1, Gregory Palmer2, Svenja Reimer3, Tat Trong Vu2,
Hieu Do2, Soﬁane Laridi2, Alexander Weber1, Claudia Nieder´ee2, and
Thomas Hildebrandt4

1 University of Hildesheim, Software Systems Engineering, Universit¨atsplatz 1, 31141
Hildesheim, Germany
2 University of Hannover, L3S, Appelstraße 9a, 30617 Hannover, Germany
3 University of Hannover, IFW, An der Universit¨at 2, 30832 Garbsen, Germany
4 Phoenix Contact, Flachsmarktstraße 8, 32825 Blomberg, Germany

Abstract. For a broader adoption of AI in industrial production, ad-
equate infrastructure capabilities are crucial. This includes easing the
integration of AI with industrial devices, support for distributed deploy-
ment, monitoring, and consistent system conﬁguration.
Existing IIoT platforms still lack required capabilities to ﬂexibly inte-
grate reusable AI services and relevant standards such as Asset Admin-
istration Shells or OPC UA in an open, ecosystem-based manner. This is
exactly what our next level Intelligent Industrial Production Ecosphere
(IIP-Ecosphere) platform addresses, employing a highly conﬁgurable low-
code based approach.
In this paper, we introduce the design of this platform and discuss an
early evaluation in terms of a demonstrator for AI-enabled visual quality
inspection. This is complemented by insights and lessons learned during
this early evaluation activity.

Keywords: IIoT · Industry 4.0 · Platform · Artiﬁcial Intelligence · Asset
Administration Shells.

1

Introduction

The ﬁeld of artiﬁcial intelligence (AI) has made signiﬁcant progress in recent
years, in particular thanks to leveraging advances in the training of deep neural
networks [20]. In numerous areas AI performance is comparable to human per-
formance, e. g., for object detection and image classiﬁcation tasks [26,34]. As a
result AI-based methods are increasingly being applied in a large variety of ap-
plication domains for supporting automated decision processes [36]. An econom-
ically highly relevant application domain for AI is industrial production [1,29].
Here AI-based methods can increase eﬀectiveness, improve quality and reduce

(cid:63) Supported by the German Ministry of Economics and Climate Action (BMWK)

under grant numbers 01MK20006A and 01MK20006D.

 
 
 
 
 
 
2

H. Eichelberger, G. Palmer, S. Reimer, et al.

costs as well as energy consumption [28,32]. The most prominent example is AI-
based condition monitoring, which intelligently reduces maintenance costs and
down times [6]. Further promising application areas include AI-based quality
control [21,27], job-shop scheduling [8,39] and smart assembly systems [22].

A number of tools and libraries exist for easing the development and train-
ing of AI models – further augmented by the availability of powerful pre-trained
models[13]. However, other steps in the data science process for intelligent pro-
duction are less well supported. This includes, for example, data acquisition
(often not supported by the existing, long-lived and expensive legacy produc-
tion systems) and the deployment of AI methods within close proximity of the
target machines, i. e., on industrial edge devices.

Operating an AI solution in an industrial context requires many support-
ing capabilities, including components for monitoring, resource management,
data storage, integration with production and business processes, and conﬁg-
uration [35]. Although several Industrial Internet of Things (IIoT) platforms
(also known as Industry 4.0 platforms) do exist, e.g., Siemens MindSphere, PTC
ThingWorkx or AWS IoT, they usually fall short in some of the aforementioned
aspects. Furthermore, they lack in ﬂexibility, openness, freedom of installation
(cloud vs. on-premise) or timely support for relevant standards [33], which are
crucial for the demand-driven development and the operation of evolving, long-
lived production systems. Researching and developing concepts for a ﬂexible,
open, standard-enabled and yet vendor-neutral IIoT platform is one of the core
aims of the IIP-Ecosphere project (next level Intelligent Industrial Production
ecosphere)5. Our IIP-Ecosphere platform aims for a low-code approach combin-
ing model-based conﬁguration, code-generation techniques with relevant indus-
trial standards, e.g. OPC UA (Companion Specs) or the currently trending Asset
Administration Shells (AAS) [17,30]. An AAS follows an approach to describe
assets, e.g., products or tools, in a machine-readable and vendor-independent
manner, thus, smoothing information exchange and enabling innovative cross-
company processes.

In this paper, we introduce the IIP-Ecosphere platform for intelligent indus-
trial production and an early validation in terms of a demonstrator use case on
AI-enabled visual quality inspection. We presented our use case at the Hannover
Messe 2022, one of the world’s largest trade fairs, exhibiting the state-of-the-art
(SOTA) w. r. t. innovative technological developments in industry. Besides tech-
nical aspects, the speciﬁc contribution of this paper is on the identiﬁcation of
challenges imposed by such an application and lessons learned for the engineer-
ing of intelligent IIoT platforms. The remainder of this paper is structured as
follows. First, we discuss related work in Section 2 and then introduce the IIP-
Ecosphere platform in Section 3. In Section 4 we deﬁne our use case and then
focus on the technical use case realization in Section 5. In Section 6, we discuss
the lessons that we learned from this early demonstrator-based validation and
in Section 7 we conclude with an outlook on future work.

5 https://www.iip-ecosphere.de/

Developing an AI-enabled IIoT platform - An early use case validation

3

2 Related Work

In [33], we analyzed 21 industrial IIoT platforms. The platforms of highest indus-
trial relevance were selected for this analysis from the large and evolving set of
existing IIoT platforms - some sources mention more than 450 diﬀerent software
platforms and [19] found more than 1200 vendors. We focused on 16 topics that
are relevant for AI application in production, including AI capabilities, edge sup-
port, conﬁgurability, cloud vs. on-premise installation, use of standards including
AAS integration. While most of the platforms provide some AI capabilities, the
support is currently rather diverse, ranging from advanced service integrations to
plain Python AI frameworks. Except for one platform, which allows on-premise
installation, all other reviewed platforms are cloud-based. Most of the analyzed
platforms do not allow a deployment of own (AI) services on edge devices and
recent standards such as OPC UA are rarely supported or, for AAS, not sup-
ported at all. As surveyed in [25] with 75 companies, only 50% utilize cloud,
while only 30% operate a platform. For the platform users, complexity, unclear
license and cost models limit or even prevent the adoption of an IIoT platform.
On the scientiﬁc side, an ongoing SLR reveals that researched platforms usu-
ally focus on a narrow inclusion of standards (MQTT, OPC UA), very limited
conﬁguration approaches, or non-industrial edge devices such as Raspberry PI.
Notable platform concepts are discussed for protocols in [31], edge usage [7,31],
on-premise option, [12,24,31], conﬁgurability [12,24] or AI capabilities [7,12,31].
In contrast, we present an AI-enabled IIoT platform that addresses identi-
ﬁed gaps and industry-relevant capabilities. In particular, our platform focuses
on openness, standard support, interoperability, conﬁgurability, industrial edge
devices and production-relevant AI capabilities. Furthermore, our platform is to
our knowledge the ﬁrst to rely on a deep integration of AAS, e.g., to represent
(runtime) component information or to steer platform operations.

With respect to the AI-services, we evaluate our platform using SOTA con-
volutional neural networks (CNNs), which have enabled a signiﬁcant number of
recent breakthroughs in the ﬁelds of computer vision and image processing [15].
CNNs’ strength is their ability to automatically learn feature representations
from raw image data [3]. With ever more advanced architectures emerging,
CNNs are increasingly being utilized within an Industry 4.0 context – from
end-of-the-line visual inspection tasks [27] to predictive maintenance and fault
detection [37]. In addition, while our current use case focuses on visual inspec-
tion, approaches where CNNs process encoded time-series data as images are
increasingly being used in intelligent manufacturing [14,18,38].

3

IIoT Platform

The IIP-Ecosphere platform is planned as a virtual platform, which can easily
be added to existing production environments and augments existing function-
ality with AI capabilities, particularly through standardized protocols. During
an intensive and interactive requirements collection for the platform [10], our
industrial stakeholders expressed in particular three core demands, namely

4

H. Eichelberger, G. Palmer, S. Reimer, et al.

1. the use of AAS to facilitate interoperability between products, processes and
platforms. While the standardization of formats for AAS is still in progress,
the aim here is to explore the capabilities and limitations of AAS, while also
considering more advanced uses, e.g., for software components. This early
validation is also used to feed back requirements into the development of the
AAS standard.

2. the vendor-independent deployment of AI and data processing components
within close proximity of the machines. This involves exploiting heteroge-
neous industrial edge devices in an open manner. In our case, this is smoothly
integrated with the overall AAS approach of the platform by equipping the
edge devices with a (vendor-provided or own) AAS providing deployment
control operations.

3. a loose integration of AI tools, in particular for the development of an AI
approach and for AI training. In other words, the platform shall support
a data scientist in the development of the solution while not limiting the
freedom of choice regarding AI and data science tools.

Based on more than 150 top-level requirements [10], we designed a layered,
service-oriented architecture for our platform. As we have to deal with a wide
variety of service realizations including Java classes, Python scripts and even
binary executables, we employ a rather open notion of the term service 6. Services
can be application-speciﬁc or generic, adaptable ones shipped with the platform.

Fig. 1: Platform component overview.

Figure 1 illustrates the main building blocks. The lowest layer 1○ focuses on
the communication, i.e., the data transport among components (such as MQTT
or AMQP), but also with machines or other platforms through pluggable con-
nectors (e.g., OPC UA, MQTT, AAS).

6 In more details, for us a service is a function with deﬁned input- and output
data/management interfaces, which can process data either in synchronous or asyn-
chronous fashion, and can optionally be distributable.

Edge, Cloud, ServerInst. PlatformTransport, machine / platform connectorsData integrationDeployment UnitAIAnoAIDeploymentConfigurationData StoreSecurity, data protection, data sharingAnoApplicationsConfiguration modelManagement UILegend: Platform componentDeployable serviceExternal componentAASData flowService interfaces, resource management, monitoring①②③④⑤⑥⑦⑧⑨⑩⑪AI toolkitDeveloping an AI-enabled IIoT platform - An early use case validation

5

The services layer 2○ deﬁnes the service management interfaces and realizes
mechanisms for resource management and monitoring. Moreover, this compo-
nent contains language-speciﬁc execution environments to ease the realization,
integration and execution of individual services, e.g., for Java or Python.

The security services layer 3○ realizes additional platform-provided function-
ality that allows for enhancing security through secure data processing and data
sharing. Here, one example service is a generic anonymization and pseudonymiza-
tion service that can safeguard the processing of data about persons.

The components in the next layer realize advanced services. For the data
processing, these are a semantic data integration 4○ of multiple data sources as
well as production-speciﬁc AI methods in terms of an AI toolkit 5○. Besides an
integration of open source components such as Python packages, the platform
can also integrate commercial components, e.g., the RapidMiner Real Time Scor-
ing Agent (RTSA)7, a generic execution environment for AI workﬂows. In the
same layer, two core components of the platform are located, the mechanisms
for consistent conﬁguration and control of the heterogeneous service deployment
to execution resources 6○. The conﬁguration component is based on a cross-
cutting conﬁguration model 7○, which integrates classical software product line
concepts [23] like optionalities, alternatives and constraints with topological ca-
pabilities [9] for modeling the graph-based data ﬂows of an IIoT application.

The top-most layer contains the integrated applications as well as a web-
based user interface 8○ for managing the platform. An application 9○ consists
of contributions to the conﬁguration model deﬁning the data types and generic
or application-speciﬁc services to be used as well as their data ﬂows.

Depending on the application, connectors and services can be distributed
to the executing resources 10○, e.g., edge devices in virtualized manner, i.e., as
(Docker) containers. Therefore, a part of the platform is running on the device,
which executes deployment and service commands issued by the central man-
agement parts of the platform. Moreover, the platform can communicate with
already installed software or platforms via connectors 11○. Many tasks of cre-
ating such an application are automated in a model-based, generative fashion
based on the conﬁguration model. Examples are the generation of interfaces and
basic implementations of application-speciﬁc services, the adaptation of generic
services to application-speciﬁc data formats by glue code, the generation of ma-
chine connectors based on low-code speciﬁcations in the conﬁguration model, or
integration and packaging of the application.

As mentioned above, the platform supports actual standards in the IIoT
domain and, in particular, the currently evolving AAS. As indicated in Figure 1,
components that are provided by third parties such as services, applications
or devices/machines are described in terms of an own AAS, e.g., through a
(pragmatic use of) the nameplate for industrial devices [2]. The platform also
maintains an encompassing AAS for itself. This consists of 11 submodels, which
references the third-party AAS and provides additional elements, e.g., runtime
monitoring properties [4] or management operations.

7 https://docs.rapidminer.com/9.4/scoring-agent/deploy-rts/

6

H. Eichelberger, G. Palmer, S. Reimer, et al.

For the development of the open source IIP-Ecosphere platform8, we build
upon and integrate at the time of writing 23 open source components, in par-
ticular from the Eclipse IoT ecosystem such as BaSyx9 for the realization of
AAS. All integrated components (including BaSyx) are treated as optionalities
or alternatives [23] and utilized in the platform only through platform interfaces
and code adapting the component to the respective interface. On the one hand,
this facilitates openness and systematic variability, e.g., of the platform trans-
port protocol. On the other hand, an interface-based integration eases coping
with unexpected external changes of components, as in most situations only the
implementation of the interface must be adjusted accordingly.

4 Demonstrator Use-case Description

For a ﬁrst validation of our platform we have selected a use case consisting of
a simple AI-based quality inspection for individualized products. The use case
utilizes industrial components such as a cobot and an industrial edge device for
realistic validation results. Therefore, despite being of small scale, the use case
is tailored to investigate the following questions:

Q1 What are the practical challenges when bringing AI-based quality inspection
close to the machines, i.e., onto an edge device? In more detail, What is
the ability of the platform to automatically deploy AI services on the edge
irrespective of model speciﬁc dependencies?

Q2 How can AAS be best exploited in support of quality inspection in lot-size-

one (individualized) production settings?

Q3 How well does the platform support integrating diﬀerent involved technolo-
gies (AI, cobot, edge, etc.) and developing the respective IIoT application?
Q4 How can an IIoT application be developed by an interdisciplinary team as
required in such a setting? Often, interdisciplinarity is seen as a special chal-
lenge when realizing, e.g., data analysis [16] or cyber-physical systems [11].

As products we are using small aluminum cars, which are produced for this
purpose. The cars vary w. r. t. product properties such as wheel color or engrav-
ings. Accompanying production, AAS capturing those properties are generated
and made available to the platform via an AAS registry. Quality inspection is
performed based on three position images taken of the car models using a 5 axis
cobot arm with a mounted camera. AI models trained for this purpose are used
for quality control and deployed onto the edge for this purpose. Based on the
AI results, the quality is measured along two axis: a) conformance with produc-
tion speciﬁcation (as given by the product’s AAS) and b) quality of production
(existence of scratches).

8 https://github.com/iip-ecosphere
9 https://www.eclipse.org/basyx/

Developing an AI-enabled IIoT platform - An early use case validation

7

5 Demonstrator Realization

The demonstrator use case has been sucessfully implemented. In the following,
we provide insights into the employed hardware, the interaction of the compo-
nents/services, the role of our platform and describe the AI service.

The IIoT hardware of the demonstrator consists of a Universal Robots UR5e
cobot10 equipped with a Robotiq wrist cam11 as well as a Phoenix Contact AXC
3152, a combined Programmable Logic Controller (PLC) and edge device12 with
an Intel Atom processor, 2 GByte RAM, and 32 GByte SD-based hard drive. In
addition, a usual PC plays the role of the central IT running our platform and
a tablet is used to present the application user interface.

The software side consists of four main components, the Cam source as image
input, the Python AI, the Action Decider controlling the overall process and
the App AAS as data sink. Furthermore, two connectors, thr OPC UA connector
and the AAS connector integrate the hardware as well as the external car AAS.
As illustrated in Figure 2, those components together form the demonstra-
tor application13. We brieﬂy describe the design of the components and their
interactions in terms of a normal execution sequence14.

The quality inspection process is initiated by pressing a physical button con-
nected to the AXC 1○, which reﬂects the button status on its OPC UA server.
A customized OPC UA connector provided by our platform observes the change
and informs the Action Decider. Alternatively, the user can press a UI button
on the tablet. The UI is based on an application AAS provided by the App AAS
service, which provides processing results as well as operations, such as starting
the inspection process.

Fig. 2: Demonstrator components and data ﬂows.

10 https://www.universal-robots.com/de/produkte/ur5-roboter/
11 https://www.universal-robots.com/plus/products/robotiq/robotiq-wrist-camera/
12 https://www.phoenixcontact.com/de-de/produkte/steuerung-axc-f-3152-1069208
13 We plan to make the source code of the demonstrator available as open source.
14 A video showing our setup is available at: https://youtu.be/36Xtw1L2XkQ.

OPC UA ConnectorAAS ConnectorApp AASCam SourceActionDeciderPython AIProductAAS RegistryLegend: Main data flowControl flowPhysical connectionApplication serviceGeneric serviceAXC 3152UR5e①②③④⑤⑥⑦IIP-Ecosphere platform8

H. Eichelberger, G. Palmer, S. Reimer, et al.

The Action Decider 2○ initiates a movement of the robot arm to one of the
three visual scanning positions (QR position, left or right car side) and requests
the Cam Source 3○ to take a picture from the wrist cam 4○. In the QR position,
the Cam Source performs a QR-code detection and augments the input data by
the QR payload. Pictures are subsequently taken for the left and right car side.
Each picture is streamed to the Python AI, which aims to detect the product
properties 5○. The AI results including the actual picture form the input to
the Action Decider. Based on the QR payload, the Action Decider obtains
the product AAS 6○ and compares the information detected by the AI and the
conﬁgured information.

The comparison results as well as trace information on all intermediary steps
are handed over to the App AAS 7○, which is presented by the Angular-based
application UI on the tablet. The UI application is entirely based on AAS. It
enables following the entire quality inspection process, visually browsing back-
ward and forward on demand. Furthermore, is confronts in an illustrative way
the picture of the product according to the speciﬁcation with a picture of the
product as it is seen by the AI. This has proven to be very useful for the demon-
stration of the use case on the trade fair. Moreover, it allows the inspection of
all underlying services running on the platform through their individual AAS.

With respect to the platform, the application as depicted in Figure 2 is mod-
eled as data ﬂow in the platform conﬁguration model. The application instan-
tiation process created application-speciﬁc interfaces of the three application-
speciﬁc services (Cam Source, Python AI, Action Decider), customized generic
connectors and services (OPC UA connector, AAS connector and App AAS) for
their use in the application, generated glue code for the distributed stream-based
service execution and performed the Maven-based integration and packaging of
the application. In summary, the application (except for the application UI)
consists of about 8.2 KLOC, whereby 51% are generated and 49% are manu-
ally written (30% production and 19% testing code). At runtime, the platform
cares in particular for the distribution of the services to the target devices, the
execution of services and container as well as for the runtime monitoring.

Below, we detail the most complex component of the application, the Python-
based AI service. Our use-case presents us with four AI-related challenges: a) tire
color detection; b) detection of an engraving towards the rear of the vehicle, if
present, then both on right rear and left rear sides; c) classiﬁcation of the num-
ber windows [1, 3]; and d) the detection of scratches (drawn on using a black
non-permanent marker pen, also allowing for an interaction opportunity with
the audience). Task a) is trivially solved using a range based color threshold ap-
proach, where ranges for each of the four color types (red, green, yellow, black)
are found upon converting our training images into HSV color-space. For the re-
maining tasks we utilized deep CNNs. We ﬁnd that each problem beneﬁts from
a custom preprocessing step, prior to feeding the image into the model. Further-
more, given the limited capacity of the edge device, we train a distinct model for
each task that can be loaded when required, instead of training one big model
for all three tasks. Given the small size of our dataset (consisting of 200 images),

Developing an AI-enabled IIoT platform - An early use case validation

9

common dataset augmentation techniques (e. g., random ﬂipping, rotation and
zooming in/out) are utilized to avoid overﬁtting our models. We describe each
model and respective preprocessing steps in detail below. Hyperparameters that
are consistent across our models include: an Adam optimizer; a learning rate of
1e-4; β1 = 0.9; β2 = 0.999; (cid:15) =1e-07; 1000 training epochs; a batch size of 20;
Relu activation layers; and a training / test ratio of 9:1. All models reach ∼ 99%
and ∼ 95% accuracy on the training and testing datasets respectively.

Scratches detection: For preprocessing we ﬁrst apply the same color range
based thresholding approach to eliminate colors within the range of the car’s
color (see Figure 3). The CNN for this task consists of separable convolutional
layers [15], representing a light weight alternative to normal convolutional layers
without signiﬁcantly impacting performance. Separable convolutional layers are
therefore well suited for edge devices with limited capacity w. r. t. memory. The
model is trained using a binary cross entropy loss function (sigmoid).

Engraving detection: For this task we used OpenCV’s edge detection to ﬁnd
a mask that can highlight the engraving (see Figure 4 for examples). The rest of
the process is the same as for the scratch detection outlined above.

Windows detection: This task is more challenging than the others, as win-
dows can be confused with “scratches” (see Figure 5). To overcome this challenge
we utilize a model that receives two inputs: 1.) a mask that highlights both the
car windows and the scratches; and 2.) the mask from the ﬁrst task (scratches
detection). Feeding in both masks allows the model to learn to recognise anoma-
lies. A categorical cross entropy loss function (softmax) is used for training the
model, since we have three diﬀerent window conﬁgurations.

To gain ﬁrst insights into the performance of executing the AI on the em-
ployed hardware, we pragmatically measure the inference time per image in the
target setup. Per image, the Python AI consumes 0.4-0.5 s on the PC and 3-5 s
on the edge device. A performance drop is not surprising here, as the edge is
a resource-constrained device, which dedicates parts of its CPU and memory
to hard realtime processes. Moreover, the hard drive is SD-based, which may
limit ﬁle transfer, e.g., when reading AI models. However, the measurements of
both devices exceed by far the required production machine pace of 8 ms [10].
Thus, further optimizations are needed, e.g., by employing a GPU or TPU co-
processor (a TPU module is available for the AXC 3152) or programming level
optimizations, e.g., to avoid re-loading models in a streaming setup.

Fig. 3: Result of applying our preprocessing prior to scratch detection.

10

H. Eichelberger, G. Palmer, S. Reimer, et al.

Fig. 4: Two examples of before/after preprocessing for detecting the engraving.

(a) Original Image

(b) Mask 1

(c) Mask 2

Fig. 5: Window detection task example, where a window is drawn onto the ve-
hicle. Both mask from Sub-Figures 5b and 5c are fed into the model, the ﬁrst
highlighting both windows and scratches, while Mask 2 emphasises the scratch.

6 Lessons Learned

We implemented our use case and successfully demonstrated it and the under-
lying platform at Hannover Messe 2022 – a large international industrial trade
fair. In addition, in realizing the use case we collected a number of experiences
and encountered a diverse set of unexpected issues.

In this section, we summarize and discuss the most relevant experiences and
issues as lessons learned for the target community. In this context, we revisit
also the questions Q1-Q4 from Section 4.

IIoT integration: Incorporating the cobot, the edge, the camera as well
as the IT components and the AI models into an IIoT application (and make
them interact “smoothly”) was well supported by the platform. This is espe-
cially due to integrated protocols, generated connectors as well as data ﬂows
(Q3). However, it also proved to be much more challenging than expected. A
major reason for this is that several technological borders have to be bridged,
where each of those technologies comes with their own paradigms, languages, and
(only partially documented) limitations. We especially experienced this, when
incorporating the cobot (edge - cobot interaction) and the camera (edge - camera
interaction).

For the edge-cobot communication, several integration paths have been ex-
plored. This included ProﬁNet, direct wiring and a software solution based on
the robot operating system ROS15. Although ProﬁNet is a standardized network

15 https://www.ros.org/

Developing an AI-enabled IIoT platform - An early use case validation

11

protocol and the Phoenix Contact PLCnext programming environment provides
a function brick for the integration of UR cobots, the establishment of a reliable
bi-directional communication between edge and cobot was hindered by a lack of
suﬃcient documentation. Alternatively, direct wiring of digital in- and outputs
between edge and cobot is a feasible approach, but limited in ﬂexibility due to
the number of supported wires and diﬃcult in realization, as here low-level signal
details must be considerd for the programming. In the end a ROS-based solution
was selected due to its easier integration with the platform.

From the integration of the camera with the edge we learned that the capacity
of the involved devices might depend upon the way they are connected - and
not all devices support all types of connections. For the case of the camera,
for example, some functionalities such as explicitly controlling the focal length,
lighting, or image preprocessing - although supported when connected via USB
- are not accessible via other communication paths. Since the employed edge
does not support USB, it had to be connected indirectly via a Web Server and
networking limiting the available functionalities.

In summary, we learned that integrating IIoT devices requires cross-domain
expertise (Q4), e.g., in our case from the automation domain, the robot manufac-
turer and the application/software side. Here, more plug-and-play opportunities
or agreed protocols such as OPC UA are desirable. However, this seems to re-
quire further standardization eﬀorts as the actual OPC companion speciﬁcation
for robots only allows for reading the robot state rather than taking control over
a robot.

Container virtualization on constrained resources: To support het-
erogeneous devices, the platform encourages the virtualization of the distributed
service execution through container technology. However, building a container
for an edge device requires a careful software selection, consideration of processor
capacities, memory and disk spaces as well as subsequent optimization of the con-
tainer (Q1). For achieving the container-based AI deployment on the employed
edge, for example, several iterations were required. The initially used Tensor-
Flow is shipped as a monolithic 2 GByte Python package. However, installing
all required components results in a container with a 4.2 GByte disk footprint,
which can easily exceed the capacity of the employed edge device. Furthermore,
TensorFlow did not support the Atom processor of the edge device. We there-
fore switched to TensorFlow light, which requires an upfront conversion of the
AI model and, in eﬀect, leads to a container size of about 2.6 GB. An alterna-
tive is to utilize tailored compilations for such devices, if available. Here, better
modularization of such libraries, e.g., an inference-only TensorFlow as well as
oﬃcial, easy-accessible builds of native software for diﬀerent processor versions
would be desirable. However, inference only models are at a disadvantage within
non-stationary domains that require online learning.

AI in Production Context: Our experience regarding AI for production is
twofold. On the one hand, it is important to carefully assess whether complex AI
methods really bring beneﬁts over algorithmic methods, e.g., a threshold-based
method for wheel color detection. This is in particular important when deploying

12

H. Eichelberger, G. Palmer, S. Reimer, et al.

AI to resource constrained devices. On the other hand, AI tasks in an industrial
production context, are in many cases more complex than expected from theory,
because the physical environment of the AI application may have a strong inﬂu-
ence, e.g., light, heat, vibration. In our case, AI needed to be tuned due to light
reﬂection on polished aluminium and changing lighting conditions in diﬀerent
settings (lab, trade fair ground). Furthermore, careful data preprocessing also
proved to be very eﬀective already in our relatively controlled environment.

Service Testing Sandbox: Besides platform components, it is essential
to test application-speciﬁc services and components. Setting up such tests is a
complex task, depending on the frameworks, techniques and conventions em-
ployed by the platform. We experienced this when integrating the Python-based
AI service as it implicitly involves diﬀerent programming languages as well as
the underlying stream processing environment, in our case Spring Cloud Stream
(SCS)16. In other words, such a test requires setting up a SCS-based service
test, which calls the managing Java stream component and, through the Python
service environment, the actual Python service code. Moreover, it is desirable to
enable testing for application developers, who do not (yet) have deep knowledge
about platform internals (Q4). To ease testing for platform users and based on
the integration experiences that we made, we plan to generate a test basis for
each application service, which transparently sets up the SCS environment and
also feeds the service with test data in a uniform manner.

Asset Administration Shells: The exploitation of AAS in the context
of lot size 1 production has proven very useful (Q2). AAS provide a manage-
able means to capture, update and check individualized production information
across individual processes (in our case production and quality control) and
across unit and organizational borders in an interoperable way17. However, it
has also shown that accompanying tools and infrastructure are crucial to make
the use of AAS feasible (see also the discussion of BaSyx below). This is espe-
cially true, when AAS instances are required for individual products (not only for
machines). In this case, an AAS editor, such as the frequently mentioned AASX
package explorer18, may be helpful for AAS templates, but it is of little use for
creating instances, as it is meant for supporting manual rather than for mass
creation of AAS instances. Here, the automatic generation of AAS instances has
proven to be a viable and eﬀective way to create individual AAS based on pro-
duction data. This is also very much in line with the low-code approach of our
platform, which heavily relies on generated software artifacts (e.g., service code).
BaSyx Experience: Overall, our experience with BaSyx was mostly posi-
tive, as it allows to eﬀectively realize complex, distributed AAS-based applica-
tions or our platform AAS. However, it also became obvious that BaSyx is still
in development and lacks some stability. For example, adding many elements
(more than 1000 in a few seconds) to a submodel or removing such elements

16 https://spring.io/projects/spring-cloud-stream
17 In the future, AAS formats that are currently in standardization will probably fur-

ther facilitate interoperability)

18 https://github.com/admin-shell/aasx-package-explorer

Developing an AI-enabled IIoT platform - An early use case validation

13

(e.g., by parallel processes) caused excessive CPU consumption. In our case, we
mitigated these problems by aggressive ﬁltering to focus on relevant informa-
tion. Moreover, a more ﬁne-grained modularization of BaSyx for application on
resource-constrained devices would be desirable (although not causing issues on
the same scale as, e.g., TensorFlow).

Platform Beneﬁts: Overall, using the IIP-Ecosphere platform allowed us
to build the demonstrator in relatively short time (within four weeks from ﬁrst
conception, including planning, discussions, workarounds and bug ﬁxing). To a
large degree, we attribute this to the low-code model-based approach, which
implies guidelines and structuring principles. For example, as an initial model
of the application must exist before services can be implemented, the involved
parties are motivated to agree on an application design ﬁrst strengthening their
joint vision and understanding (Q4). From a technical and an organizational
perspective, the service interfaces that we derived next, led to a clear separation
of interests but also to ownerships, which contributes to Q3 (here into AI, UI,
robot control and supporting services). Moreover, the platform approach also
gives structural guidance, e.g., on the organization of code artifacts for diﬀerent
programming languages so that packaged application artifacts can be deployed
and executed by the platform. Further, the code generation helped us speeding
up the development. In particular, the complete generation of OPC UA and
AAS connectors from the model saved considerable amount of time. As already
mentioned in Section 4, more than half of the application code was generated,
relying on frameworks that were already tested and integrated into the platform.
Further, the code generation even enabled us to quickly realize changes to the
processing in an agile and consistent manner. Although the standardization of
AAS is currently in progress, the existing AAS modeling concepts are already
beneﬁcial in data exchange as they lead to a uniform representation of data
structures that are helpful, e.g., when realizing an application UI. A further
beneﬁt is that a platform provides an environment to realize services more easily,
even without knowing details on machine connectors, data transport, monitoring
or deployment. Here, an adequate abstraction of a functional unit, e.g., a service,
allows to deploy and to distribute, e.g., AI, in a ﬂexible and easy manner, e.g.,
to edge devices (Q1).

As a more general lesson learned, it is deﬁnitively worth while to validate
complex software systems such as a platform with realistic use cases, since it also
brings improvements for the platform itself. Although our platform is subject
to regression tests on diﬀerent levels of granularity, the demonstrator involved
several unexpected situations. For example, some of the control ﬂow communi-
cation paths in the demonstrator that failed without (observable) reason. An
initial workaround for the demonstrator ﬁnally led to a general revision of the
code generation for these paths in the platform. Another example are services
that need longer for their startup than expected by the tests, such as a Python
script utilizing TensorFlow-light AI models. As the service startup was assigned
to the wrong service lifecycle phase, these services were accidentally started mul-

14

H. Eichelberger, G. Palmer, S. Reimer, et al.

tiple times by the service framework due to timeouts. Correcting the assignment
for all services helped to further stabilize the platform.

7 Future work & Conclusion

In this paper, we provided an overview of our IIoT platform approach – the open
source IIP-Ecosphere platform – and introduced its architecture. While design-
ing and developing such a platform is a technical challenge in itself, although
supported through more than 20 integrated open source components, it is im-
portant to validate and evaluate the approach with industrial devices and use
cases. As a ﬁrst step in this direction, we presented an AI-based visual quality
inspection, which combines industrial devices (edge, cobot), recent and upcom-
ing standards (OPC UA, AAS) with actual AI methods. From the realization
of the demonstrator, we derived and discussed lessons learned, which pinpoint
actual problems but also indicate positive experiences that can be helpful for
other work in the ﬁeld.

As future work, we plan to investigate some of the open problems from our
lessons learned discussion and further applications, e.g., selection and optimiza-
tion of AI for resource-constrained devices, federated AI models [5] architecture
in a federated multi-device setup, automated container building for Industry
4.0 applications or AAS-based plug-and-play for industrial edge devices during
onboarding into an IIoT platform. Besides these topics, we will improve and
stabilize the realization of our platform, which already raised various industrial
interests outside the consortium, as a basis for deeper evaluations and further
demonstrators.

References

1. Angelopoulos, A., Michailidis, E.T., Nomikos, N., Trakadas, P., Hatziefremidis,
A., Voliotis, S., Zahariadis, T.: Tackling faults in the industry 4.0 era—a survey of
machine-learning solutions and key aspects. Sensors 20(1), 109 (2019)

2. Bader, S., Bedenbecker, H., Billmann, M., et al.: Generic Frame for Technical
Data for Industrial Equipment in Manufacturing (Version 1.1) (2020), https://bit.
ly/3NpuIOF

3. Bansod, G., Khandekar, S., Khurana, S.: Analysis of convolution neural network
architectures and their applications in industry 4.0. In: Metaheuristic Algorithms
in Industry 4.0, pp. 139–162. CRC Press (2021)

4. Casado, M.G., Eichelberger, H.: Industry 4.0 Resource Monitoring - Experiences
With Micrometer and Asset Administration Shells. In: CEUR-WS Proceedings of
Symposium on Software Performance 2021 (SSP’21) (2021)

5. Chang, Y., Laridi, S., Ren, Z., Palmer, G., Schuller, B.W., Fisichella, M.: Robust
federated learning against adversarial attacks for speech emotion recognition. arXiv
preprint arXiv:2203.04696 (2022)

6. Chen, J., Lim, C.P., Tan, K.H., Govindan, K., Kumar, A.: Artiﬁcial intelligence-
based human-centric decision support framework: an application to predictive
maintenance in asset management under pandemic environments. Annals of Op-
erations Research pp. 1–24 (2021)

Developing an AI-enabled IIoT platform - An early use case validation

15

7. Chen, S., Li, Q., Zhang, H., Zhu, F., Xiong, G., Tang, Y.: An IoT Edge Com-
puting System Architecture and its Application. In: International Conference on
Networking, Sensing and Control (ICNSC). pp. 1–7 (2020)

8. Denkena, B., Dittrich, M.A., Fohlmeister, S., Kemp, D., Palmer, G.: Scalable coop-
erative multi-agent-reinforcement-learning for order-controlled on schedule manu-
facturing in ﬂexible manufacturing systems. Simulation in Produktion und Logistik
2021: Erlangen, 15.-17. September 2021 p. 305 (2021)

9. Eichelberger, H., Qin, C., Sizonenko, R., Schmid, K.: Using IVML to Model the
Topology of Big Data Processing Pipelines. In: International Systems and Software
Product Line Conference. pp. 204 – 208 (2016)

10. Eichelberger, H., Stichweh, H., Sauer, C.: Requirements for an AI-enabled Industry
4.0 Platform – Integrating Industrial and Scientiﬁc Views. In: Intl. Conference on
Advances and Trends in Software Engineering. pp. 7–14 (2022)

11. Feichtinger, K., Meixner, K., Rinker, F., Koren, I., et al.: Industry Voices on Soft-
ware Engineering Challenges in Cyber-Physical Production Systems Engineering.
In: ETFA’22 (2022), https://t.co/sqr3XfVMyN, accepted, preprint

12. Foukalas, F.: Cognitive IoT platform for fog computing industrial applications.

Computers & Electrical Engineering 87, 106770 (2020)

13. Han, X., Zhang, Z., Ding, N., Gu, Y., Liu, X., Huo, Y., Qiu, J., Yao, Y., Zhang,
A., Zhang, L., et al.: Pre-trained models: Past, present and future. AI Open 2,
225–250 (2021)

14. Hoang, D.T., Kang, H.J.: Rolling element bearing fault diagnosis using convolu-
tional neural network and vibration image. Cognitive Systems Research 53, 42–50
(2019), advanced Intelligent Computing

15. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-
dreetto, M., Adam, H.: Mobilenets: Eﬃcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)

16. Hummel, O., Eichelberger, H., Giloj, A., Werle, D., Schmid, K.: A Collection of
Software Engineering Challenges for Big Data System Development. In: Euromicro
Conf. on Software Engineering and Advanced Applications. pp. 362–369 (2018)
17. Kannoth, S., Hermann, J., Damm, M., R¨ubel, P., Rusin, D., Jacobi, M., Mittels-
dorf, B., Kuhn, T., Antonino, P.O.: Enabling SMEs to Industry 4.0 Using the
BaSyx Middleware: A Case Study. In: Software Architecture. pp. 277–294 (2021)
18. Kiangala, K.S., Wang, Z.: An eﬀective predictive maintenance framework for con-
veyor motors using dual time-series imaging and convolutional neural network in
an industry 4.0 environment. Ieee Access 8, 121033–121049 (2020)

19. Krause, T., Strauß, O., Scheﬄer, G., Kett, H., Lehmann, K., Renner, T.: IT-
Plattformen f¨ur das Internet der Dinge (IoT) (2017), https://bit.ly/3amlXaG
20. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. nature 521(7553), 436–444

(2015)

21. Lee, S.M., Lee, D., Kim, Y.S.: The quality management ecosystem for predictive
maintenance in the industry 4.0 era. International Journal of Quality Innovation
5(1), 1–11 (2019)

22. Lin, C.H., Wang, K.J., Tadesse, A.A., Woldegiorgis, B.H.: Human-robot collabo-
ration empowered by hidden semi-markov model for operator behaviour prediction
in a smart assembly system. Journal of Manufacturing Systems 62, 317–333 (2022)
23. van der Linden, F., Schmid, K., Rommes, E.: Software Product Lines in Action -

The Best Industrial Practice in Product Line Engineering. Springer (2007)

24. Lins, T., Oliveira, R.A.R., Correia, L.H.A., Silva, J.S.: Industry 4.0 Retroﬁtting.
In: Brazilian Symp. on Computing Systems Engineering (SBESC). pp. 8–15 (2018)

16

H. Eichelberger, G. Palmer, S. Reimer, et al.

25. Nieder´ee, C., Eichelberger, H., Schmees, H.D., Broos, A., Schreiber, P.: KI in der

Produktion – Quo vadis? (2021), https://doi.org/10.5281/zenodo.6334521

26. Pal, S.K., Pramanik, A., Maiti, J., Mitra, P.: Deep learning in multi-object detec-

tion and tracking: state of the art. Applied Intelligence 51(9), 6400–6429 (2021)

27. Palmer, G., Schnieders, B., Savani, R., Tuyls, K., Fossel, J., Flore, H.: The auto-
mated inspection of opaque liquid vaccines. In: ECAI 2020, pp. 1898–1905 (2020)
28. Patalas-Maliszewska, J., Pajkak, I., Skrzeszewska, M.: Ai-based decision-making
model for the development of a manufacturing company in the context of industry
4.0. In: International Conference on Fuzzy Systems (FUZZ-IEEE). pp. 1–7 (2020)
29. Peres, R.S., Jia, X., Lee, J., Sun, K., Colombo, A.W., Barata, J.: Industrial artiﬁ-
cial intelligence in industry 4.0 - systematic review, challenges and outlook. IEEE
Access 8, 220121–220139 (2020)

30. Platform Industrie 4.0: Details of the Asset Administration Shell, https://bit.ly/

3nPPAUU

31. Raileanu, S., Borangiu, T., Morariu, O., Iacob, I.: Edge Computing in Industrial
IoT Framework for Cloud-based Manufacturing Control. In: International Confer-
ence on System Theory, Control and Computing (ICSTCC). pp. 261–266 (2018)

32. Reimer, J., Wang, Y., Laridi, S., Urdich, J., Wilmsmeier, S., Palmer, G.: Identifying
cause-and-eﬀect relationships of manufacturing errors using sequence-to-sequence
learning. arXiv preprint arXiv:2205.02827 (2022)

33. Sauer, C., Eichelberger, H., Ahmadian, A.S., Dewes, A., J¨urjens, J.: Current In-
dustrie 4.0 Platforms – An Overview (2021), https://zenodo.org/record/4485756
34. Schnieders, B., Luo, S., Palmer, G., Tuyls, K.: Fully convolutional one-shot object
segmentation for industrial robotics. In: International Conference on Autonomous
Agents and MultiAgent Systems. pp. 1161–1169 (2019)

35. Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaudhary,
V., Young, M., Crespo, J.F., Dennison, D.: Hidden Technical Debt in Machine
Learning Systems. In: Advances in Neural Information Processing Systems. vol. 28,
pp. 2503–2511 (2015)

36. Shinde, P.P., Shah, S.: A review of machine learning and deep learning applications.
In: International conference on computing communication control and automation
(ICCUBEA). pp. 1–6 (2018)

37. Silva, W., Capretz, M.: Assets predictive maintenance using convolutional neural
networks. In: International Conference on Software Engineering, Artiﬁcial Intelli-
gence, Networking and Parallel/Distributed Computing (SNPD). pp. 59–66 (2019)
38. Wang, H., Li, S., Song, L., Cui, L.: A novel convolutional neural network based
fault recognition method via image fusion of multi-vibration-signals. Computers in
Industry 105, 182–190 (2019)

39. Zhuang, C., Liu, J., Xiong, H.: Digital twin-based smart production management
and control framework for the complex product assembly shop-ﬂoor. The interna-
tional journal of advanced manufacturing technology 96(1), 1149–1163 (2018)

