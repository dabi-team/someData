Astromorphic Self-Repair of Neuromorphic Hardware Systems

Zhuangyu Han,1 Naﬁul Islam, 1 Abhronil Sengupta 1
1 Department of Electrical Engineering
Penn State University
University Park, PA 16802
zfh5141@psu.edu, naﬁul@psu.edu, sengupta@psu.edu

2
2
0
2

p
e
S
5
1

]
E
N
.
s
c
[

1
v
8
2
4
7
0
.
9
0
2
2
:
v
i
X
r
a

Abstract

While neuromorphic computing architectures based on Spik-
ing Neural Networks (SNNs) are increasingly gaining inter-
est as a pathway toward bio-plausible machine learning, at-
tention is still focused on computational units like the neu-
ron and synapse. Shifting from this neuro-synaptic perspec-
tive, this paper attempts to explore the self-repair role of glial
cells, in particular, astrocytes. The work investigates stronger
correlations with astrocyte computational neuroscience mod-
els to develop macro-models with a higher degree of bio-
ﬁdelity that accurately captures the dynamic behavior of the
self-repair process. Hardware-software co-design analysis re-
veals that bio-morphic astrocytic regulation has the potential
to self-repair hardware realistic faults in neuromorphic hard-
ware systems with signiﬁcantly better accuracy and repair
convergence for unsupervised learning tasks on the MNIST
and F-MNIST datasets.

Introduction
As a pathway to enable next-generation neuromorphic intel-
ligence, Spiking Neural Networks (SNNs) are emerging as a
disruptive computing paradigm where the information trans-
mitted and processed in the artiﬁcial neurons of the comput-
ing system is embodied in a temporal series of binary spikes
which mimic the action potentials propagating between neu-
rons in the mammalian brain. SNNs have demonstrated sig-
niﬁcant advantages in comparison to traditional non-spiking
computational architectures in several aspects, such as low
latency and low energy consumption (Davies et al. 2018;
Merolla et al. 2014; Sengupta and Roy 2017; Diehl and
Cook 2015; Lu and Sengupta 2020; Neftci, Mostafa, and
Zenke 2019).

Recent research in neuromorphic computing has started
focusing on other cellular components in the brain that might
contribute to cognition in addition to neuronal spiking be-
havior and synaptic plasticity. Speciﬁcally, this work ex-
plores the contribution of one such component - glial cells,
in particular, the astrocytes (Oberheim et al. 2006). It has
been observed that neurotransmitters released from a neu-
ron can trigger diffusing Ca2+ waves inside the cytoplasm
of neighboring astrocytes, which later regulates the strength
of the synapses surrounded by these astrocytes (Cornell-
Bell et al. 1990; Perea and Araque 2007). Previous works
have considered the regulation mechanism of the astrocytes

as a self-repair capability of artiﬁcial spiking neural net-
works (Wade et al. 2012; Liu et al. 2017, 2018; Rastogi
et al. 2021) with synaptic faults. This is especially timely
given the signiﬁcant advancements in Artiﬁcial Intelligence
(AI) memristive hardware over the past few years enabled
by devices where the core physics serve as a natural hard-
ware substrate for designing compact, energy-efﬁcient neu-
romorphic hardware accelerators (Sung, Hwang, and Yoo
2018; Park et al. 2022). However, such devices often suf-
fer from non-idealities and faults (Liu et al. 2014; Radetzki
et al. 2013) and therefore mitigation of such issues is criti-
cal to ensure minimal accuracy degradation in AI hardware
platforms. While astrocytic regulation of synaptic transmis-
sion probability offers motivation toward autonomous self-
repair of faulty neuromorphic hardware, current work sig-
niﬁcantly falls short of this goal due to a lack of hardware
insights. The neuroscience inspired algorithm frameworks
are often loosely bio-inspired and depend on global network
level parameters like synaptic weight percentiles (Rastogi
et al. 2021) which are not practical to calculate in real-time
under energy and resource constraints in edge AI systems.

This work forges stronger connections with neuroscience
computational models of astrocytes to develop self-repair
algorithmic frameworks driven by a software-hardware co-
design perspective. The key distinguishing factors of our
work against prior proposals are:

(i) Neuroscience Inspired Self-Repair Learning Algo-
rithm Formulation: Based on the bio-inspiration premise
that self-repair learning algorithm formulations need to have
stronger neuroscience correlation in order to be hardware
efﬁcient and compatible, we design a self-repair rule by
theoretically analyzing the dynamic temporal behavior of
the self-repair process enabled by astrocytes. We develop a
macro-model of the self-repair process by benchmarking it
to astrocyte computational neuroscience models. As shown
in the paper, the bio-inspiration route enables us to construct
self-repair-based synaptic learning rules that captures the
dynamic temporal repair process while simultaneously de-
pending on local network level parameters that make it hard-
ware compatible. Performance evaluation of the proposed
learning rule is demonstrated for an unsupervised learning
framework on the MNIST and F-MNIST datasets.

(ii) Algorithm-Hardware Co-Design: Going beyond
software simulated faults where synapse transmission prob-

 
 
 
 
 
 
abilities (PR) are assumed to be zero, we will analyze the
impact of non-idealities occurring in the neuromorphic hard-
ware, which are signiﬁcantly more complex than stuck-at-
zero faults. In particular, we focus on synaptic weight drift
effects observed in memristive AI hardware (Ambrogio et al.
2019), in addition to stuck-at faults. This can therefore lead
to the development of highly adaptive neuromorphic sys-
tems that can self-repair faults (device level non-idealities)
in an autonomous fashion.

Methods

Astrocyte Mediated Synaptic Dynamics
Astrocytes modulate the transmission characteristics, funda-
mentally the spike transmission probability, of the synapses
they ensheathe (Wade et al. 2012). Endocannabinoids-
mediated synaptic potentiation (e-SP)
(Navarrete and
Araque, 2010) and Depolarization-induced suppression of
excitation (DSE) (E. Alger 2002) are two dynamic modu-
lating factors that inﬂuence the PR evolution of synapses.
Through a signal pathway inside the astrocyte cytoplasm, e-
SP increases the PR after each post-synaptic neuron ﬁring
event. In contrast, DSE provides direct negative feedback
to the ﬁring rate of post-synaptic neurons. Prior work on
computational models of astrocytes have outlined the quan-
titative change of 2-arachidonly glycerol (2-AG) as the ﬁrst
process initiating PR modulation (Wade et al. 2012). The
post-synaptic neuron releases 2-AG every time it ﬁres, and
the 2-AG decays exponentially when there is no release from
the dendrite of the post-synaptic neuron. The dynamics of 2-
AG can be described as:

d(AG)
dt

=

−AG
τAG

+ rAGδ(t − tsp)

(1)

where, AG is the quantity of 2-AG released by the post-
synaptic neuron, τAG is the decay time constant of 2-AG,
rAG is the production rate of 2-AG and tsp is the time
when the post-synaptic neuron spikes. Note that 2-AG is a
neuron-speciﬁc variable, which implies that the spikes of all
synapses of a neuron will contribute to the evolution of 2-
AG of this neuron.

The binding process of 2-AG to CB1R receptors on the
pre-synaptic neuron triggers DSE. The relationship between
2-AG and DSE can be described by a simple linear equation:

DSE = −AG × KAG

(2)

where, AG is the quantity of 2-AG released by the post-
synaptic neuron and KAG is a linear scaling factor. DSE is
also a neuron-speciﬁc variable.

Concurrently, 2-AG also binds to the CB1Rs on the
plasma membrane of the astrocyte, which subsequently trig-
gers the generation of IP3 inside the cytoplasm of astro-
cytes. IP3 later binds on the IP3 receptor in the Endoplasmic
Reticulum (ER), which releases Ca2+ into the cytoplasm.
The Ca2+ dynamics, described by the Li-Rinzel model (Li
and Rinzel 1994), involves three Ca2+ currents which are
Jchan (the current through Ca2+ channel controlled by mu-
tual gating of Ca2+ and IP3), Jleak (the leakage current

from ER into the cytoplasm) and Jpump (the pumping cur-
rent where the Ca2+ is absorbed into ER through Sacro-
Endoplasmic-Reticulum Ca2+-ATPase (SERCA) pumps).
Interested readers are referred to Refs. Wade et al. (2012)
and De Pitt`a et al. (2009) for details on the computational
model. The intracellular Ca2+ dynamics can be described
as:

dCa2+
dt

= Jchan + Jleak − Jpump

(3)

The exocytosis of glutamate is regulated by Ca2+, which
can be described as:

d(Glu)
dt

=

−Glu
τGlu

+ rGluδ(t − tCa2+)

(4)

where, Glu is the total quantity of glutamate released by the
astrocyte, τGlu is the glutamate decay time constant, rGlu is
the ﬁxed amount of glutamate release each time the Ca2+
concentration crosses the threshold and tCa2+ is the time in-
stant when the Ca2+ concentration crosses the threshold.

Glutamate binding is the ﬁnal stage of potentiation
where the synapse has its PR increased when the group I
metabotropic Glutamate Receptors (mGluRs) on the mem-
brane of the pre-synaptic neuron receives the glutamate,
whose dynamics is described by:

τeSP

d(eSP)
dt

= −eSP + meSPGlu(t)

(5)

where, eSP is the intensity of e-SP, τeSP is the decay time
constant of eSP, meSP is a scaling factor and Glu(t) is the
released glutamate quantity at time t. Please note here that
eSP is an astrocyte-speciﬁc variable which inﬂuences all the
synapses of all the neurons ensheathed by the astrocyte.

Potentiation provides positive feedback to synaptic ﬁring,
while DSE depresses synaptic ﬁring. The PR dynamics as
a function of time can be described as a combined effect of
DSE and eSP as follows:

PR(t) = PR(0) + PR(0) × (

DSE(t) + eSP(t)
100

)

(6)

where, PR(0) is the PR of the synapse before any ﬁring.

As described by the dynamic astrocyte model, DSE is es-
tablished by the direct binding of 2-AG to the pre-synaptic
neuron, which produces a restricted effect on a single neu-
ron. However, the e-SP feedback through the cytoplasm of
the astrocyte is globally effective for all synapses ensheathed
by the corresponding astrocyte. During the self-repair pro-
cess enabled by the astrocyte, when the PR of a synapse
drops due to faults, the magnitude of DSE of that speciﬁc
neuron (considering that DSE is always negative) will im-
mediately decline caused by the decreasing 2-AG. However,
the global e-SP maintains its intensity due to the continu-
ous ﬁring of the neighboring neurons. Consequently, the PR
of healthy synapses for the affected neuron with faults will
increase based on Equation 6 and recover the spiking ac-
tivity of the neuron to the initial baseline value. The next
section discusses formulation of a benchmarked astrocyte
macro-model that captures the dynamical repair process for
hardware realistic faults based on this computational neuro-
science model.

Macro-model Formulation for Hardware Realistic
Synaptic Fault Repair

Figure 1: Two neurons interacting with a common astrocyte.

is discussed in detail

Neuron-Astrocyte Structure Conﬁguration The two-
simulation model
computational
neuron-one-astrocyte
(Wade et al. 2012), shown in Figure 1, is adopted in this
paper. In such a model, there are two neurons, N1 and N2,
with identical biological characteristics and one astrocyte
functioning as the e-SP pathway for both neurons. The
neuron ﬁring model
in the next
sub-section. The two neurons have the same number of
synapses, and each synapse receives a Poisson spike train
with a globally constant rate parameter, i.e. 10 spikes per
second. Please note that Figure 1 shows only one dendrite
for each neuron, for simplicity, which implies that the DSE
shown in the ﬁgure is not restricted to the single synpase,
instead it is effective neuron-wide. The initial PR values
of each synapse are initialized randomly and are sampled
from a uniform distribution whose mean is pre-deﬁned.
The signalling pathway of the astrocyte is computationally
realized by a group of global variables, such as the quantity
of IP3, Ca2+ and glutamate. The interaction between the
neurons and the astrocyte is represented by the change of
variable 2-AG and glutamate quantity.

The total simulation duration is 400 s and a resolution of
1 ms is used for modelling the temporal dynamics. The Pois-
son spike trains are continuously received by all the synapses
for the entire duration of the simulation. A speciﬁc amount
of current (6650 pA in this paper) is injected into the neuron
if a spike is successfully transmitted through the synapse.

Neuron Firing Model The biological attributes of the
neurons are critical to the dynamics of the neuron-astrocyte
structure. For the computational simulations performed in
this paper, the Leaky-Integrate-Fire model is used for mod-
eling the neuron dynamics. The evolution of neuron mem-
brane potential follows the equation below:

τv

dv(t)
dt

= −(v(t) − vres) + I(t)

(7)

where, v is the membrane potential, τv is the decay time
constant of the membrane potential, vres is the resting state
potential and I(t) is the injected current which is the total

Figure 2: Schematic of the network mapped to a memristive
crossbar array. Each weight is encoded by the conductance
of a pair of PCM devices. Additional peripheral circuitry is
omitted.

current the neuron receives from all the synapses. When the
membrane potential reaches a threshold potential, vth, the
neuron ﬁres once and immediately the membrane potential
is set to a pre-determined value, vreset. No refractory period,
δref , is considered in this computational model.

Memristive Hardware Modelling and Fault Simulation
At the hardware level, the PR values (equivalently synap-
tic weights for artiﬁcial spiking neural networks used for
machine intelligence) of the network are mapped to the
conductance states of the memristive devices. In this pa-
per, we speciﬁcally consider device characteristics corre-
sponding to a representative memristive technology - Phase
Change Memory (PCM) (Fong, Neumann, and Wong 2017).
Organized in a crossbar fashion, as shown in Figure 2, these
arrays of devices can implement the dot product operation.
The input voltage Vi coming from the pre-synaptic neurons
to each row of synapses are modulated by the conductance
of the synaptic devices and are summed along the columns
by Kirchhoff’s current law. Thus, the post-synaptic current
fed to the post-synaptic neuron j is given by:

Ij =

n
(cid:88)

i=1

Gi,jVi

(8)

where n is the total number of rows, i.e. total number of pre-
synaptic neurons. Note, as conductance values are always
positive, in order to represent both negative and positive
weights, each weight is actually mapped to a pair of PCM
devices (G+,G−, shown in Figure 2 inset) where the posi-
tive (negative) conductance encodes the positive (negative)
weight value while the other conductance is set to a high
OFF resistive state. For sake of simplicity, in our following
discussions we will consider the conductance to be given by
G = G+ −G−. The intrinsic device physics of PCM devices
can be exploited to implement synaptic learning rules like
Spike Timing Dependent Plasticity (STDP) (Sebastian et al.
2018) while the array structure enables the area and energy-
efﬁcient implementation of synaptic dot-product computa-
tion required in artiﬁcial neural networks (Fong, Neumann,
and Wong 2017).

v ∼ N (µv, σv)

(10)

=

However, memristive devices suffer from several non-
idealities. While previous works on astrocyte enabled self-
repair in neural networks (Wade et al. 2012; Liu et al.
2017, 2018; Rastogi et al. 2021) have considered stuck-at-
faults, hardware realistic faults are much more complex. In
this work, we consider two speciﬁc hardware non-idealities:
stuck-at-fault and weight drift. The stuck-at-fault is a type
of fault where a pre-determined percentage of synapses are
faulty or disabled, i.e. PR = 0, and no longer affected
by any dynamics. The weight drift scenario occurs in the
healthy synapses and is representative of PCM device tech-
nology. Therefore, along with accounting for a portion of
the synapses to be disabled (representing the simulation
of stuck-at-faults) in our modelling, the PRs of healthy
synapses drift with a randomly generated drift ratio. The
drift fault pattern is extracted from the temporal PCM device
characteristics (Ambrogio et al. 2019). The temporal decay
of the conductance of a PCM device can be described as:
G = G0t−v

(9)

norm

where, G0 is the initial conductance, tnorm is the normalized
time, which is greater than 1, and v is the log scale decay
slope. The value rdecay = G
norm is called the drift
G0
ratio. Also, instead of a constant parameter, the log scale
decay slope v is sampled from a normal distribution for each
individual PCM device. i.e.

= t−v

where, µv and σv represent the mean and standard deviation
of the distribution of v. In both scenarios mentioned above,
the fault is instantly injected to neuron N2 at time tfault =
200 s. Neuron N1 is not affected by any fault. For the stuck-
at-fault scenario, the impacted PRs drop to 0 at tfault and
remain at that value. For modelling both the stuck-at-fault
and drift process, at tfault, the PRs of disabled synapses are
set to 0 in the same way as the former. At the same time, each
healthy synapse PR is multiplied by its individual rdecay. In
the following sections, without explicit declaration, the PRs
are the PR values of neuron N2, as N1 is not inﬂuenced by
fault.

Self-Repair Macro-model Formulation In this subsec-
tion, we develop a macro-model capturing the temporal dy-
namics of the astrocyte induced self-repair process by uti-
lizing the detailed computational model discussed previ-
ously. For simplicity, the macro-model is developed based
on the response of the computational model to stuck-at-
faults while performance evaluation on machine learning
tasks (discussed in next section) is performed for both stuck-
at-faults and weight drift non-idealities. Based on Equation
6, it can be observed that PR(t) is a product of PR(0) and
a global variable 1 + (DSE(t) + eSP(t))/100. Due to the
stochastic nature of PR initialization and fault injection, the
amount of self-repair varies in different simulation runs. In
general, the PRs of healthy synapses increase after the oc-
currence of fault and ﬁnally stabilize at a certain level which
is regulated by the competition between direct and indirect
signal feedback. From Figure 3, where a sample of the tem-
poral evolution of PR values is plotted, it can be observed

Figure 3: An example of temporal evolution of PR value of
neuron N2 under astrocytic inﬂuence, where the total num-
ber of synapses is 10.

that the time cost for self-repair, which is deﬁned as the time
difference between tfault and the time the PR takes to reach
its repaired stable value, is similar for each synapse. In fact,
we can prove that all the PRs keep their relative magnitude
unchanged at any time during the simulation window unless
there is any particular PR hitting the upper bound 1. At any
given time t1 and t2, the ratio between two arbitrary PRs,
say PRi and PRj, is

PRi(0)(1 + DSE(t1)+eSP(t1)
PRj(0)(1 + DSE(t1)+eSP(t1)

100

)

)

100

=

PRi(t1)
PRj(t1)
PRi(0)(1 + DSE(t2)+eSP(t2)
PRj(0)(1 + DSE(t2)+eSP(t2)

100

100

)

)

=

PRi(t2)
PRj(t2)

(11)

Therefore all the synapses ﬁnish their self-repair simultane-
ously. Also, from Equation 11, we can conclude that at any
given time t1 and t2 and two arbitrary PRs, say PRi and
PRj, the following relation holds:

PRi(t1)
PRi(t2)

=

PRj(t1)
PRj(t2)

(12)

In this work, the PR self-repair ratio q of a synapse, say
synapse i, is deﬁned as the ratio of the PR value after self-
repair and the stable PR value before fault injection:

qi =

PRi(AS)
PRi(BF)

(13)

where, PRi(AS) is the mean (averaged over multiple simu-
lation runs) of stable PR value of synapse i after self-repair
and PRi(BF) stands for the mean of stable PR value of
synapse i just before fault injection. The intervals of AS and
BF are shaded in Figure 3. It is easy to infer from Equa-
tion 12 that q1 = q2 = .. = qm where m is the number of
synapses of neuron N2. Therefore, we will denote q without
a subscript for neuron N2 for the remainder of the text.

Based on the computational model simulations, we in-
ferred that the self-repair ratio q is a function of the initial
PRs and the fault magnitude. By statistics, the larger the per-
centage of synapses affected, larger is the value of q. Based
on data collected from 400 simulation runs with indepen-
dent randomly sampled PR initialization and fault injection,
it was conﬁrmed that q is strongly correlated to the severity

Astromorphic Learning Algorithm Formulation

Figure 5: SNN network architecture used for unsupervised
learning. Lateral inhibitory connections are only shown for
one neuron in the output layer.

To demonstrate our proposal, we consider an SNN trained
via the STDP learning algorithm where the synaptic weights
are updated according to the timing of spikes of pre-synaptic
and post-synaptic neurons. For detailed information about
the learning algorithm and training process, interested read-
ers are directed to (Diehl and Cook 2015). An STDP net-
work for image recognition has a structure as elaborated in
Figure 5. The dimension of the input layer, ninput, is dic-
tated by the dimension of the input images for recognition
while the main network consists of nneuron neurons. To pre-
vent single neurons from dominating the ﬁring pattern dur-
ing training, homeostasis is included in the neuron model:
a neuron spike causes a constant threshold increment (θ+)
over the baseline threshold. The threshold increment decays
exponentially, according to decay time constant (τθ), with
time when the neuron does not ﬁre. Such a mechanism pre-
vents a neuron from ﬁring continuously, which enables the
entire network to learn uniformly. The homeostasis effect is
also balanced by a lateral inhibitory effect where any neu-
ron spike in the output layer triggers negative spikes to all
the other neurons (where the weights for all inhibitory con-
nections is a global constant, winh) and prevents them from
ﬁring, thereby promoting competitive learning. The repre-
sentative counterpart of synaptic PR from the computational
model is the set of synaptic weights in the SNN connecting
the input and output neuron layers. There will be a positive
weight update whenever the post-synaptic neuron ﬁres af-
ter the pre-synaptic neuron and vice versa. The trace of a
pre/post-synaptic neuron is set to 1 at the instant it generates
a spike, and the trace decays exponentially afterward. The
trace of a neuron measures the temporal closeness of the
current time and its immediate previous spike. The STDP
learning rule based on post/pre-synaptic neuron spike traces
can be formulated as:

∆w(t) =

(cid:26)ηpost × xpre(t)

−ηpre × xpost(t)

∆t > 0
∆t < 0

(19)

where, ηpost/ηpre are the post/pre-synaptic neuron learning
rates and xpre/xpost are the traces of the pre/post-synaptic
neurons. ∆t is the time difference between post-synaptic and
pre-synaptic spikes, i.e. ∆t = tpost − tpre. Practically, the

Figure 4: Quantitative relationship between self-repair ratio,
q, and fault severity, z.

of the fault. Here, the severity of the fault z is deﬁned as:

z =

=

sum of initial PRs of non-disabled synapses
sum of initial PRs of all the synapses
non-disabled synapses PRi(0)
(cid:80)
all synapses PRi(0)

(cid:80)

(14)

where, PRi(0) is the initial PR of synapse i. In the context
of terminology to be used in the next section for learning
algorithm formulation in self-repair of neuromorphic hard-
ware systems, a non-disabled synapse is a synapse whose
PR is not stuck to zero and therefore includes synaptic
weights with a drift factor. Quantitatively, the strong correla-
tion found between q and z from the astrocyte computational
model simulations is:

1.03
z + 0.04
which is also displayed in Figure 4.

q ≈

For simplicity, an inverse proportion relation is consid-
ered in our algorithm design (to be discussed in next sec-
tion), i.e.

(15)

(16)

q ←

1
z

Such a strong correlation between q and z implies that the
astrocyte self-repair mechanism will drive the PR of healthy
synapses of a faulty neuron to increase until the sum of
PR of the remaining healthy synapses approaches the orig-
inal synaptic PR sum before fault. The temporal increment
of PR of healthy synapses after fault injection follows an
exponential-like trend since the self-repair becomes rela-
tively slow as the PR of healthy synapses approach their tar-
get value. i.e.,

PRi(t) ≈ PRi(AS)(1 − e− (t−tfault+tb )

τ

)

= q × PRi(BF)(1 − e− (t−tfault+tb )
∀ t > tfault

τ

)

where

tb = −τ log(

q − 1
q

)

(17)

(18)

is the temporal intercept and τ is the self-repair time con-
stant. The value of the temporal intercept can be determined
from the initial condition that at t = tfault, the transmission
probability starts to increase from an initial value PRi(BF).
The next section discusses astromorphic learning algorithm
formulation for unsupervised SNNs inspired by the formu-
lated macro-model.

∆t > 0 case is executed when the post-synaptic neuron ﬁres
and the ∆t < 0 case is executed when the pre-synaptic neu-
ron ﬁres.

Astrocyte feedback causes the modulation of synaptic
plasticity in the presence of faults, in turn, modulating the
STDP learning rule. Inspired by the inverse proportion re-
lationship between the self-repair ratio and the fault ratio,
the self-repair learning rule is expected to recover the orig-
inal sum of synaptic PRs of a faulty neuron. The target re-
paired value of healthy synaptic weights of a faulty neuron
is therefore determined by q. We note that an exponential
increasing temporal pattern (exhibited by the dynamic re-
pair process in astrocyte computational model in Equation
(17)) is commonly represented by the following equation:
dx(t)
τx
dt = xtarget − x(t) , where xtarget is the value that the
objective x converges to and τx is the convergence time con-
stant. The slope of increment of the objective at any given
time t is proportional to the difference between the target
value and the current value. Therefore, the astrocyte modu-
lated STDP learning rule can be described by the following
equation where the rate of weight change is directly propor-
tional to the difference between the target synaptic PR and
the current weight:

∆w(t) =

(cid:40)

ηpost × xpre(t) × qw0−w(t)
−ηpre × xpost(t)

τ

∆t > 0
∆t < 0

(20)

where, w0 is the weight of the synapse before fault injec-
tion, w(t) is the current weight, τ is the self-repair time con-
stant (which can be combined with the learning rate as a sin-
gle hyperparameter), q is the self-repair ratio mentioned in
the previous section, Here, the self-repair ratio q is approxi-
mated by 1
z , where z is a function of the sum of the synaptic
weights of the corresponding neuron. It is therefore worth
pointing out here that the resultant computational model in-
spired learning rule is local where the synaptic weight up-
dates are dependent on the current weight (local to synapse)
and sum of weights of the corresponding neuron (local to
neuron). This enables our algorithm to be memristive hard-
ware compatible (for instance, sum of weights of a particular
neuron can be evaluated by applying all-one voltages along
the rows of the crossbar array in Figure 2) unlike prior pro-
posals (Rastogi et al. 2021) where the temporal dynamics of
the weight updates during self-repair are governed by global
parameters like percentile of the weight distribution of the
entire network and are also lacking in bio-ﬁdelity from mod-
elling perspective. The term qw0 in Equation 20 is the self-
repair target value of the corresponding weight. Note that
qeﬀ is not constant with time, therefore the target value of
each synaptic PR/weight can change during the re-training
process.

The next section evaluates the performance of our pro-
posed astromorphic learning rule against prior proposals in
unsupervised SNNs for hardware-realistic faults. For clar-
ity, we will refer to the basic spike-timing dependent synap-
tic plasticity as STDP, astrocyte mediated plasticity in prior
proposals (Rastogi et al. 2021) as A-STDP (global) and our
proposal as A-STDP (local).

Results

Experiment Setup
The proposed A-STDP (local) learning rule is implemented
using the BindsNET (Hazan et al. 2018) framework - an
open-source SNN simulation platform based on PyTorch
(https://pytorch.org/). The algorithms were run on the hard-
ware environment consisting of one Intel(R) Xeon(R) Silver
4210 CPU, one NVIDIA GeForce RTX 2080 Ti GPU with
11264 MBytes graphics memory, 187 GBytes RAM and
CentOS Linux 7 operating system. The SNNs are trained
using STDP on two recognition tasks, namely the MNIST
(LeCun and Cortes 2010) and Fashion MNIST (Xiao, Ra-
sul, and Vollgraf 2017) datasets. The intensity of the pixels
of the input images are converted into Poisson spike trains
(the ﬁring rates for different datasets are noted in Table II).
The weights between the input layer and the output layer
are randomly initialized with a uniform distribution of 0-0.3
range. Two types of hardware realistic faults (stuck-at-zero
and weight drift) mentioned previously are injected in the
baseline trained networks. Performance assessment in terms
of accuracy and repair speed is performed for three genera-
tions of self-repair learning rules - STDP, A-STDP (global)
(Rastogi et al. 2021) and A-STDP (local).

First, the baseline networks (which do not possess any
fault) are prepared by training a randomly initialized net-
work with the STDP learning rule. The baseline net-
works, consisting of 400 neurons, are individually trained
on MNIST/Fashion MNIST dataset with a ﬁnal accuracy
of 90.43%/77.60%. Next, stuck-at-faults are injected in the
weights of the baseline network with a certain fault proba-
bility pfault. i.e. each weight in the baseline network is set
to zero with probability pfault. Five values for pfault = 0.5,
0.6, 0.7, 0.8 and 0.9 are tested to mimic varying fault sever-
ity. The weight drift involves three parameters: tnorm, µv
and σv (mentioned in prior sections). Subsequently, STDP,
A-STDP (local) and A-STDP (global) self-repair re-training
is performed for comparative performance assessment. The
best accuracy and number of data samples consumed before
reaching best accuracy are recorded and analyzed. Interested
readers can ﬁnd the network weight maps before and after
the introduction of non-idealities (stuck-at-faults and weight
drift), including the self-repaired weights, in the supplemen-
tary material. The detailed information regarding all simula-
tion hyperparameters is included in Table 2 and typical abla-
tion studies for important hyperparameters of the astromor-
phic learning model is also included in the supplementary
material.

Like previous works (Rastogi et al. 2021), the weights of

all neurons are normalized during the training process:

WeightSUMi ← WeightSUM

(21)

where, WeightSUMi represents the sum of weights of neu-
ron i and WeightSUM is the network-wide average of the
sum of weights for all the neurons. The normalization op-
eration is applied after each batch of STDP learning, which
guarantees that all the neurons have the same chance to be
trained. Since the STDP algorithm updates the weights only
when there is any spike and highly severe faults can result

Dataset

pfault

Acc. Norm.
(%)

Acc. STDP
(%)

# Steps STDP
(104 steps)

MNIST
(Baseline: 90.43%)

Fashion MNIST
(Baseline: 77.60%)

0.5
0.6
0.7
0.8
0.9
0.5
0.6
0.7
0.8
0.9

27.61 (0.96)
27.18 (0.16)
27.02 (0.76)
26.33 (1.16)
27.68 (0.25)
32.40 (1.42)
32.18 (1.40)
31.12 (0.38)
30.30 (0.65)
29.90 (0.38)

83.22 (0.39)
79.92 (0.62)
70.11 (0.90)
51.19 (0.86)
39.41 (1.56)
75.75 (0.25)
74.17 (0.47)
72.63 (0.64)
67.38 (0.58)
43.48 (1.65)

8.8 (2.5)
9.0 (1.9)
7.3 (1.0)
5.0 (1.2)
0.7 (0.6)
19.9 (4.9)
21.4 (1.6)
22.9 (0.8)
19.5 (2.7)
0.7 (0.6)

Acc. A-STDP
(global)
(%)
76.26 (0.61)
73.62 (0.78)
70.83 (0.50)
66.93 (0.31)
65.42 (0.86)
67.30 (0.56)
65.76 (0.12)
64.49 (0.40)
61.79 (1.01)
57.75 (0.53)

# Steps A-STDP
(global)
(104 steps)
7.7 (1.8)
5.8 (1.2)
6.9 (1.8)
8.0 (1.7)
9.3 (1.4)
17.5 (3.4)
15.1 (6.0)
18.0 (4.8)
19.6 (5.0)
23.4 (0.7)

Acc. A-STDP
(local)
(%)
87.31 (0.34)
84.89 (0.32)
82.45 (0.51)
77.68 (0.85)
68.58 (0.55)
76.14 (0.30)
74.74 (0.23)
72.67 (0.53)
69.76 (0.43)
64.54 (0.69)

# Steps A-STDP
(local)
(104 steps)
7.9 (1.6)
6.2 (2.1)
7.4 (2.5)
6.3 (1.3)
9.3 (1.2)
1.5 (0.9)
2.5 (1.1)
2.1 (1.1)
2.2 (1.6)
4.3 (2.6)

Table 1: Self-repair accuracy and convergence speed comparison among STDP, A-STDP (global) and our proposed A-STDP
(local) learning rules for 400 neuron network trained on MNIST and Fashion-MNIST datasets. Statistics is averaged over 5
runs.

Parameters
Simulation time duration per image, time
Simulation time-step size, ∆t
Membrane potential decay time const., τv
Refractory period, δref
Resting membrane potential, vres
Reset membrane potential, vreset
Threshold membrane potential, vth
Adaptive threshold increment, θ+
Adaptive threshold decay time const., τθ
A-STDP (global) weight-percentile, α (Rastogi et al. 2021)
A-STDP (global) non-linearity, σ (Rastogi et al. 2021)
A-STDP (local) self-repair time const. (MNIST), τ
A-STDP (local) self-repair time const. (Fashion MNIST), τ
A-STDP (local) normalization period
Weight sum lower bound (MNIST), LBWeightSUM
Weight sum lower bound (Fashion MNIST), LBWeightSUM

Values
100 ms
1 ms
100 ms
5 ms
-65 mV
-60 mV
-52 mV
0.05 mV
107 ms
98
2
10−2
4×10−3
1 batch
0.17
0.22

Parameters
No. of output neurons, nneuron
No. of input neurons, ninput
Batch size
Spike trace decay time const.
STDP weight normalization factor
Maximum input spike rate (MNIST)
Maximum input spike rate (Fashion MNIST)
Inhibitory synaptic weight (MNIST), winh
Inhibitory synaptic weight (Fashion MNIST), winh
Post-synaptic learning rate (MNIST), ηpost
Post-synaptic learning rate (Fashion MNIST), ηpost
Pre-synaptic learning rate (MNIST), ηpre
Pre-synaptic learning rate (Fashion MNIST), ηpre
Weight drift normalized time, tnorm
Weight drift log slope mean, µv
Weight drift log slope standard derivation, σv

Values
400
784
16
20 ms
78.4
128 /s
45 /s
-120
-250
10−2
4×10−3
10−4
4×10−5
104
1
0.2258

Table 2: Self-repair retraining hyperparameters.

in very low spiking activity in the network, a lower bound
of WeightSUM, namely LBWeightSUM, is conﬁgured as a
network hyperparameter to improve the training efﬁciency.

Accuracy and Convergence Speed Analysis

Table 1 reports the best accuracy recorded in the self-repair
process for hardware-realistic faults in the network (stuck-
at-faults and weight drift). The results have been averaged
over 5 independent runs of the network. In table 1, Acc.
Norm represents the network accuracy after fault injec-
tion and weight normalization. Acc. STDP/Acc. A-STDP
(global)/Acc. A-STDP (local) is the accuracy measured af-
ter re-training using STDP/A-STDP (global)/A-STDP (lo-
cal) learning rules respectively. # Steps describes the num-
ber of training samples consumed before reaching the max-
imum accuracy during the self-repair process. The stan-
dard deviation of each measurement is included in paren-
thesis. A-STDP (local) signiﬁcantly outperforms A-STDP
(global) in terms of self-repaired accuracy. Further, while
the convergence speed of A-STDP (local) is comparable
to A-STDP (global) in case of the MNIST dataset, it is
signiﬁcantly faster over the more complex Fashion-MNIST

dataset. Training convergence graphs are also provided in
the supplementary material.

Discussion

This work strengthens the foundation for enabling astro-
morphic self-repair of hardware realistic faults in a neuro-
morphic system by forging stronger correlations with as-
trocyte neuroscience models. The key distinguishing aspect
of the work lies in the neuroscience-hardware-software co-
design aspect that delves into learning algorithm formula-
tion that is not only neuroscience inspired but also is hard-
ware compatible. Performance assessment is provided in un-
supervised learning tasks for hardware realistic faults going
beyond simple stuck-at-faults considered in prior works. Fu-
ture work can focus on understanding other aspects of as-
trocyte functionality in the self-repair process. For instance,
there is a pair of feedback pathway in the astrocyte compu-
tational model based on the ﬁring rate of the corresponding
neuron, whose details have not been fully explored in this
work. Furthermore, the modulation of inhibitory synapses
through astrocytes (Kang et al. 1998) for neuromorphic
computing still need more research.

Acknowledgments
The work was supported in part by the National Science
Foundation grants BCS #2031632, ECCS #2028213 and
CCF #1955815.

References
Ambrogio, S.; Kumar, A.; Chen, A.; Burr, G. W.; Gallot, M.;
Spoon, K.; Tsai, H.; Mackin, C.; Wesson, M.; Kariyappa, S.;
Narayanan, P.; and Liu, C.-C. 2019. Reducing the Impact of
Phase-Change Memory Conductance Drift on the Inference
of large-scale Hardware Neural Networks. In 2019 IEEE In-
ternational Electron Devices Meeting (IEDM), 6.1.1–6.1.4.
San Francisco, CA, USA: IEEE. ISBN 978-1-72814-032-2.
Cornell-Bell, A. H.; Finkbeiner, S. M.; Cooper, M. S.; and
Smith, S. J. 1990. Glutamate induces calcium waves in
Science,
cultured astrocytes: long-range glial signaling.
247(4941): 470–473. Publisher: American Association for
the Advancement of Science.
Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.;
Choday, S. H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.;
Liao, Y.; Lin, C.-K.; Lines, A.; Liu, R.; Mathaikutty, D.; Mc-
Coy, S.; Paul, A.; Tse, J.; Venkataramanan, G.; Weng, Y.-H.;
Wild, A.; Yang, Y.; and Wang, H. 2018. Loihi: A Neuro-
morphic Manycore Processor with On-Chip Learning. IEEE
Micro, 38(1): 82–99.
Diehl, P. U.; and Cook, M. 2015. Unsupervised learning
of digit recognition using spike-timing-dependent plasticity.
Frontiers in Computational Neuroscience, 9.
E. Alger, B. 2002. Retrograde signaling in the regulation of
synaptic transmission: focus on endocannabinoids. Progress
in Neurobiology, 68(4): 247–286.
Fong, S. W.; Neumann, C. M.; and Wong, H.-S. P. 2017.
Phase-change memory—Towards a storage-class memory.
IEEE Transactions on Electron Devices, 64(11): 4374–
4385.
Hazan, H.; Saunders, D. J.; Khan, H.; Patel, D.; Sanghavi,
D. T.; Siegelmann, H. T.; and Kozma, R. 2018. BindsNET:
A Machine Learning-Oriented Spiking Neural Networks Li-
brary in Python. Frontiers in Neuroinformatics, 12: 89.
Kang, J.; Jiang, L.; Goldman, S. A.; and Nedergaard, M.
1998. Astrocyte-mediated potentiation of inhibitory synap-
tic transmission. Nature Neuroscience, 1(8): 683–692.
LeCun, Y.; and Cortes, C. 2010. MNIST handwritten digit
database.
Li, Y.-X.; and Rinzel, J. 1994. Equations for InsP3 Receptor-
mediated [Ca2+]i Oscillations Derived from a Detailed Ki-
netic Model: A Hodgkin-Huxley Like Formalism. Journal
of Theoretical Biology, 166(4): 461–473.
Liu, J.; Harkin, J.; Li, Y.; and Maguire, L. 2014. Online
trafﬁc-aware fault detection for networks-on-chip. Journal
of Parallel and Distributed Computing, 74(1): 1984–1993.
Liu, J.; Harkin, J.; Maguire, L. P.; McDaid, L. J.; and Wade,
J. J. 2017. SPANNER: A self-repairing spiking neural net-
IEEE transactions on neural
work hardware architecture.
networks and learning systems, 29(4): 1287–1300.
Liu, J.; McDaid, L. J.; Harkin, J.; Karim, S.; Johnson, A. P.;
Millard, A. G.; Hilder, J.; Halliday, D. M.; Tyrrell, A. M.;
and Timmis, J. 2018. Exploring self-repair in a coupled spik-
ing astrocyte neural network. IEEE transactions on neural
networks and learning systems, 30(3): 865–875.

Lu, S.; and Sengupta, A. 2020. Exploring the Connection
Between Binary and Spiking Neural Networks. Frontiers in
Neuroscience, 14: 535.
Merolla, P. A.; Arthur, J. V.; Alvarez-Icaza, R.; Cassidy,
A. S.; Sawada, J.; Akopyan, F.; Jackson, B. L.; Imam, N.;
Guo, C.; Nakamura, Y.; Brezzo, B.; Vo, I.; Esser, S. K.; Ap-
puswamy, R.; Taba, B.; Amir, A.; Flickner, M. D.; Risk,
W. P.; Manohar, R.; and Modha, D. S. 2014. A million
spiking-neuron integrated circuit with a scalable communi-
cation network and interface. Science, 345(6197): 668–673.
Navarrete, M.; and Araque, A. 2010. Endocannabinoids Po-
tentiate Synaptic Transmission through Stimulation of As-
trocytes. Neuron, 68(1): 113–126.
Neftci, E. O.; Mostafa, H.; and Zenke, F. 2019. Surrogate
Gradient Learning in Spiking Neural Networks: Bringing
the Power of Gradient-Based Optimization to Spiking Neu-
ral Networks. IEEE Signal Processing Magazine, 36(6): 51–
63.
Oberheim, N. A.; Wang, X.; Goldman, S.; and Nedergaard,
M. 2006. Astrocytic complexity distinguishes the human
brain. Trends in Neurosciences, 29(10): 547–553.
Park, T. J.; Deng, S.; Manna, S.; Islam, A. N.; Yu, H.;
Yuan, Y.; Fong, D. D.; Chubykin, A. A.; Sengupta, A.;
Sankaranarayanan, S. K.; et al. 2022. Complex oxides for
brain-inspired computing: A review. Advanced Materials,
2203352.
Perea, G.; and Araque, A. 2007. Astrocytes Potentiate
Transmitter Release at Single Hippocampal Synapses. Sci-
ence, 317(5841): 1083–1086.
Radetzki, M.; Feng, C.; Zhao, X.; and Jantsch, A. 2013.
Methods for fault tolerance in networks-on-chip. ACM Com-
puting Surveys, 46(1): 1–38.
Rastogi, M.; Lu, S.; Islam, N.; and Sengupta, A. 2021. On
the Self-Repair Role of Astrocytes in STDP Enabled Unsu-
pervised SNNs. Frontiers in Neuroscience, 14: 603796.
Sebastian, A.; Le Gallo, M.; Burr, G. W.; Kim, S.; Bright-
Sky, M.; and Eleftheriou, E. 2018. Tutorial: Brain-inspired
computing using phase-change memory devices. Journal of
Applied Physics, 124(11): 111101.
Sengupta, A.; and Roy, K. 2017. Encoding neural and synap-
tic functionalities in electron spin: A pathway to efﬁcient
neuromorphic computing. Applied Physics Reviews, 4(4):
041105.
Sung, C.; Hwang, H.; and Yoo, I. K. 2018. Perspective: A
review on memristive hardware for neuromorphic computa-
tion. Journal of Applied Physics, 124(15): 151903.
Wade, J.; McDaid, L.; Harkin, J.; Crunelli, V.; and Kelso,
S. 2012. Self-repair in a Bidirectionally Coupled Astrocyte-
Neuron (AN) System based on Retrograde Signaling. Fron-
tiers in Computational Neuroscience, 6.
Xiao, H.; Rasul, K.; and Vollgraf, R. 2017. Fashion-MNIST:
a Novel Image Dataset for Benchmarking Machine Learning
Algorithms. Number: arXiv:1708.07747 arXiv:1708.07747
[cs, stat].

Supplementary Material

Self Repair Time Constant Hyperparameter
Tuning
While the majority of hyperparameter choices in the algo-
rithm simulations were guided by prior literature, the self
repair time constant τ in the proposed A-STDP (local) learn-
ing rule had a signiﬁcant effect on the convergence speed. τ
also controls the amount of weight update in a single repair
step. The larger the value of τ is, the slower the convergence
speed will be. Ablation studies involving several values of
τ were performed on the MNIST dataset where the self-
repaired accuracy and mean and standard deviation of sam-
ples required to reach optimal accuracy are shown in Fig-
ure 6. The MNIST training set (60000 samples in total) was
randomly split into 50000 samples for training and 10000
samples for validation in order to perform the ablation stud-
ies. 10 individual runs were used to extract the statistics of
Figure 6.

Figure 6: Mean self-repaired accuracy with standard devia-
tion (statistics from 10 runs) of A-STDP (local) algorithm
retrained for 2 epochs on MNIST training dataset with dif-
ferent τ settings, where pfault is 0.8 including weight drift.
Mean accuracy value of each setting is shown on top of the
bar. The lower portion of the bar shows the mean and stan-
dard deviation of the number of training samples required to
reach optimal accuracy.

Based on the above simulation studies, τ = 0.01 is se-
lected as the optimal setting for the self repair time constant
hyperparameter.

Weight Maps
Plotting the neuronal weights of the network as images intu-
itively illustrates the effect of faults and self-repair. Figure 7
shows how the synapses’ PRs are repaired after the injection
of faults and weight drift, including the reference of healthy
weights trained by STDP algorithm. From Figure 7b, it can
be observed that the non-idealities degrade the weight maps
for each neuron signiﬁcantly. Figure 7c shows that A-STDP
(local) retraining process is able to recover the weight maps
to a reasonable degree.

Convergence Plots
Convergence plots depicted in Figure 8 show that the accu-
racy of A-STDP (local) reaches its maximum value during

7474.57575.57676.57777.578Accuracy (%)76.8675798.4(15786.2) = 0.00877.1270598.4(20804.5) = 0.0176.8677398.4(17229.4) = 0.0276.7177200.0(20026.6) = 0.176.7699190.4(1024.5) = 1*the re-training process within 50 thousand samples for Fash-
ion MNIST dataset. However, A-STDP (global) converges
much slowly and the ﬁnal accuracy is lower as well.

(a) Network with healthy weights

Figure 8: Accuracy of 5 runs of A-STDP (global) and A-
STDP (local) re-training processes for 4 epochs on Fashion
MNIST dataset, where pfault is 0.7 along with weight drift.
The downward-pointing triangles mark the number of sam-
ples required for the algorithm to converge to the optimal
accuracy for each run.

(b) Network after fault injection, weight
drift and normalization

(c) Network after self-repair

Figure 7: Weight maps of all neurons before and after
faults/self-repair with pfault = 0.7 including weight drift.
For better illustration, weights are normalized to [0, 1] indi-
vidually for each neuron.

00.511.52Number of samples1055560657075Accuracy (%)A-STDP (local)A-STDP (global)