2
2
0
2

y
a
M
1
1

]
L
C
.
s
c
[

1
v
9
3
3
5
0
.
5
0
2
2
:
v
i
X
r
a

An Eﬃcient Summation Algorithm for the
Accuracy, Convergence and Reproducibility of
Parallel Numerical Methods

Farah Benmouhoub1, Pierre-Loic Garoche2, and Matthieu Martel1,3

1 LAMPS Laboratory, University of Perpignan, France.
2 ENAC, Toulouse, France.
3 Numalis, Montpellier, France.
1{first.last}@univ-perp.fr,2pierre-loic.garoche@enac.fr

Abstract. Nowadays, parallel computing is ubiquitous in several appli-
cation ﬁelds, both in engineering and science. The computations rely on
the ﬂoating-point arithmetic speciﬁed by the IEEE754 Standard. In this
context, an elementary brick of computation, used everywhere, is the
sum of a sequence of numbers. This sum is subject to many numerical
errors in ﬂoating-point arithmetic. To alleviate this issue, we have intro-
duced a new parallel algorithm for summing a sequence of ﬂoating-point
numbers. This algorithm which scales up easily with the number of pro-
cessors, adds numbers of the same exponents ﬁrst. In this article, our
main contribution is an extensive analysis of its eﬃciency with respect
to several properties: accuracy, convergence and reproducibility. In order
to show the usefulness of our algorithm, we have chosen a set of represen-
tative numerical methods which are Simpson, Jacobi, LU factorization
and the Iterated power method.

Keywords: ﬂoating-point arithmetic, accurate summation, numerical
accuracy, numerical methods, convergence, reproducibility.

1

Introduction

Scientiﬁc computing relies heavily on ﬂoating-point arithmetic as deﬁned by the
IEEE754 Standard [1,8,17]. It is therefore sensitive to round-oﬀ errors, and this
problem tends to increase with parallelism. In ﬂoating-point computations, in
addition to rounding errors, the order of the computations aﬀects the accuracy of
the results. For example, let us calculate in IEEE754 single precision (Binary32)
the sum of three values x, y and z, where x = 109, y = −109 and z = 10−9. We
obtain

((x + y) + z) = ((109 − 109) + 10−9) = 10−9,

(x + (y + z)) = (109 + (−109 + 10−9)) = 0.

(1)

(2)

Equations (1) and (2) show that for the same mathematical expression, a sum
of three operands, diﬀerent orderings of the computations yield diﬀerent results.
In the ﬂoating-point arithmetic, we note that, for the same values of x, y and z,

 
 
 
 
 
 
and for the same arithmetic operation, we obtain two diﬀerent results because
of parsing the three values diﬀerently. In fact, many summation algorithms exist
in the literature. Some of them are based on compensated summation meth-
ods [13,16,18,21,22,23] with or without the use of the error-free transformations
to compute the error introduced by each accumulation. Others are based on ma-
nipulating the exponent and/or the mantissa of the ﬂoating-point numbers in
order to split data before starting computations [5,6].

In a similar approach, we proposed in [2] a new algorithm for accurately
summing n ﬂoating-point numbers. This algorithm performs computations only
within working precision, requiring only an access to the exponents of the values.
The idea is to compute the summands according to their exponents without
increasing the complexity. More precisely, the complexity of the algorithm is
linear in the number of elements, just like the naive summation algorithms. The
main contribution of the present article is to show that this algorithm improves
simultaneously the parallel execution time, the reproducibility and convergence
of computations through the increase of their numerical accuracy as follows:

1. numerical accuracy improvement is illustrated on computations of MPI im-

plementations of Simpson’s rule and the LU factorization method.

2. convergence acceleration is showcased on both Jacobi’s method and the it-
erated power method compared to versions of these methods which use a
simple summation algorithm. Past results [4] show that improving the ac-
curacy of computation also leads to accelerate the convergence of iterative
sequential algorithms. Our motivation is therefore to parallelize these two
methods focusing ﬁrst on accuracy and obtaining, as a side eﬀect, a better
convergence.

3. Last, reproducibility of numerical computation in the context of parallel sum-
mation is supported by the reduction on numerical errors. Indeed, the com-
bination of the non-associativity of ﬂoating-point operations like addition
and computations done in parallel may aﬀect reproducibility. The intuitive
solution used to ensure reproducibility is to determine a deterministic or-
der of computation. Another method is based on reduction or elimination of
round-oﬀ errors, i.e. by improving the numerical accuracy of computations
that we will further see in this article. This is illustrated on Simpson’s rule
and a simple matrix multiplication.

A last contribution is an experimental comparison of the execution time
of our algorithm with respect to the similar approach proposed by Demmel
and Hida [5]. Indeed, Demmel and Hida algorithm [5] has a time complexity of
O(n log n) because of an additional sorting step, compared to our summation
algorithm which involve no explicit sorting and has a complexity in O(n). Our
solution has a greater space complexity but this can be addressed with sparse
datastructures. Execution time of the two approaches are compared in Sections
3.1 and 3.2.

The rest of the paper is organized as follows. Section 2 recalls elements
of ﬂoating-point arithmetic and the related work on some existing summation
methods proposed to improve the numerical accuracy of computations. We also

2

present our parallel summation algorithm. Section 3 focuses on the improvement
of numerical accuracy, based on two experiments, Simpson’s rule and a LU fac-
torization. Section 4 focuses on the impact of accuracy on the convergence speed.
It is based on experiments on Jacobi’s method and the Iterated Power Method.
Last, we focus on reproducibility in Section 5, based on two experiments: the
Simpson’s rule and a matrix multiplication. We conclude in Section 6.

2 Background

This section introduces some useful notions used in the remainder of this ar-
ticle. Section 2.1 provides some background of the ﬂoating-point arithmetic as
deﬁned by the IEEE754 standard [1,8,17]. Section 2.2 discusses related work.
Our algorithm introduced in [2], is presented in Section 2.3.

2.1 Floating-Point Arithmetic

Following the IEEE754 Standard, a ﬂoating-point number x in base β is deﬁned
by

x = s · m · βexp−f +1

(3)

where

– s ∈ {−1, 1} is the sign,
– m = d0d1....df −1 is the mantissa with digits 0 ≤ di < β, 0 ≤ i ≤ f − 1,
– f is the precision,
– exp is the exponent with expmin ≤ exp ≤ expmax.

The IEEE754 Standard deﬁnes binary formats with some particular values for
f , expmin and expmax which are summarized in Table 1. Moreover, the IEEE754
Standard deﬁnes four rounding modes for elementary operations over ﬂoating-
point numbers. These modes are towards +∞, towards −∞, towards zero and
towards nearest, denoted by ◦+∞, ◦−∞, ◦0 and ◦∼, respectively.

The behavior of the elementary operations (cid:5) ∈ {+, −, ×, ÷} between ﬂoating-

point numbers is given by

v1 (cid:5)◦ v2 = ◦(v1 (cid:5) v2)

(4)

where ◦ denotes the rounding mode such as ◦ ∈ {◦+∞, ◦−∞, ◦0, ◦∼}. By Equa-
tion (4), we illustrate that, in ﬂoating-point computations, performing an ele-
mentary operation (cid:5)◦ with rounding mode ◦ returns the same result as the one
obtained by an exact operation (cid:5), then rounding the result using ◦. The IEEE754
Standard also speciﬁes how the square root function must be rounded in a simi-
lar way to Equation (4) but does not specify the round-oﬀ of other functions like
sin, log, etc. In this article, without loss of generality, we consider that β = 2.
We assume the rounding mode to the nearest. In ﬂoating-point computations,
absorption and cancellation may aﬀect the numerical accuracy of computations.
An absorption occurs when adding two ﬂoating-point numbers with diﬀerent or-
ders of magnitude. The small value is absorbed by the large one. A cancellation
occurs when two nearly equal numbers are subtracted and the most signiﬁcant
digits cancel each other.

3

Format
Half precision
Single precision
Double precision
Quadruple precision

#total bits
16 bits
32 bits
64 bits
128 bits

f bits
11
24
53
113

exp bits
5
8
11
15

expmax
expmin
+15
−14
+127
−126
−1122
+1223
−16382 +16383

Table 1: Binary formats of the IEEE754 Standard.

2.2 Related Work

Summation of ﬂoating-point numbers is one of the most basic tasks in nu-
merical analysis. Research work has focused on improving the numerical accu-
racy [5,6,13,16,18,21] or reproducibility [7] of the computations involving sum-
mations. There are many sequential and parallel algorithms for this task. Surveys
of them being presented in [11,12]. Floating-point summation is often improved
by compensated summation methods [13,16,18,21,22,23] with or without the use
of error-free transformations to compute the error introduced by each accumu-
lation. We detail some of the compensated summation algorithms further in this
section. The accuracy of summation algorithms can also be improved by manip-
ulating the exponent and the mantissa of the ﬂoating-point numbers in order to
split data before starting computations [5,6]. This approach is the one employed
by our algorithm and it is explained in details in Section 2.3.

Compensated summation methods: The idea is to compute the exact round-
ing error after each addition during computations [13]. Compensated summation
algorithms accumulate these errors and add the result to the result of the sum-
mation. The compensation process can be applied recursively yielding cascaded
compensated algorithm. Malcolm [16] describes cascading methods based on the
limited exponent range of ﬂoating-point numbers. He deﬁnes an extended preci-
sion array ei where each component corresponds to an exponent. To extract and
scale the exponent, Malcolm uses an integer division, without requiring the divi-
sion to be a power of 2. If the extended precision has 53 + k bits in the mantissa,
then, obviously, no error occurs for up to 2k summands and (cid:80)n
i=1 ei.
The summands pi are added with the respect to decreasing order into the array
element corresponding to their exponent. Note that such an algorithm requires
twice as much running time compared to our algorithm.

i=1 pi = (cid:80)n

Rump et al. [18,21,22] proposed several algorithms for summation and dot
product of ﬂoating point numbers. These algorithms are based on iterative appli-
cation of compensations. An extension of the compensation of two ﬂoating-point
numbers to vectors of arbitrary length is also given and used to compute a result
as if computed with twice the working precision. Various applications of com-
pensated summation method have been proposed [9,10]. Th´evenoux et al. [24]
implement an automatic code transformation to derive a compensated programs.
Also, we mention the accurate ﬂoating-point summation algorithms intro-
duced by Demmel and Hida [5,6]. Given two precision f and F with F > f , Dem-
mel and Hida’s algorithms use a ﬁxed array accumulators A0, ....AN of precision
F for summing n ﬂoating-point numbers of precision f such that S = (cid:80)n
i=1 si.

4

These algorithms require accessing the exponent ﬁeld of each si to decide to
which accumulator Aj to add it. More precisely, each Aj accumulating the sum
of the si where e leading bits are j. Then, these Aj are sorted in decreasing
order to be summed. Consequently, complexity of these algorithms is equal to
O(n log n), because of the sorting step.

Parallel approaches: In addition to the existing sequential algorithms, many
other parallel algorithms have been proposed. Leuprecht and Oberaigner [15]
describe parallel algorithms for summing ﬂoating-point numbers. The authors
propose a pipeline version of sequential algorithms [3,20] dedicated to the com-
putation of exact rounding summation of ﬂoating-point numbers. In order to
ensure the reproducibility, Demmel and Nguyen [7] introduce a parallel ﬂoating-
point summation method based on a technique called pre-rounding to obtain
deterministic rounding errors. The main idea is to pre-round the ﬂoating-point
input values to a common base, according to some boundary, so that their sum
can later be computed without any rounding error. The error depends only on
both input values and the boundary, contrary to the intermediate results which
depend on the order of computations.

2.3 Accurate Summation Algorithm

In this section, we describe our summation algorithm introduced in [2] for accu-
rately summing n ﬂoating-point numbers. Our algorithm enjoys the following set
of properties. First, it improves the numerical accuracy of computations without
increasing the cost of complexity compared to the naive algorithm. Second, it
performs all the computations in the original working precision without using
accumulators of higher precision. Last, using this new algorithm, we increase the
numerical accuracy and, as a side eﬀect and shown in the next sections, we also
improve the execution time and reproducibility of summation.

For the algorithm detailed hereafter, we assume that we have P processors
and n summands (with n (cid:29) P ). We assign n/P summands to each proces-
sor. For computing the sum S = (cid:80)n
i=1 si, ∀0 ≤ i < P processor i computes
(cid:80)(i+1)×n/P −1
sj. Then a reduction – a last sum – is done to compute the ﬁnal
j=i×n/P
result. First of all, Algorithm 1 allocates an array called sum by exp which is
created and initialized at 0 for all its elements before starting the summations.
The array sum by exp has exp max − exp min + 1 elements. Let us assume that
the exponents of summands range from exp min to exp max. The main idea is
to sum all the summands whose exponent is 2i in the cell sum by exp[i + bias]
such as bias = −exp min, this avoids most absorptions while avoiding to sort
explicitly the array. Let exp (si) denote the exponent of si in base 2. For com-
puting the sum S = (cid:80)n
i=1 si, each value si is added to the appropriate cell
sum by exp[exp si + bias] according to its exponent. For parallelism, each pro-
cessor has an sum by exp array to sum locally. In order to obtain its local ﬁnal
result, we add these values in increasing order. Once the ﬁnal local sums are
computed, a reduction is done and the processor receiving the result of the re-
duction gets the total sum. To emphasize the property regarding the cost of

5

Algorithm 1 Accurate summation with local sum at each processor

exp s i=getExp(s[i])+bias

1: Initialization of the array sum by exp
2: total sum=0.0
3: for i=0 : p row do
4:
5:
6:
7: local sum=0.0
8: for i=0 to exp max-exp min+1 do
9:

local sum=local sum+sum by exp[i]

in base 2
sum by exp[exp s i]=sum by exp[exp s i]+s[i]

(cid:46) p row: rows number of each processor
(cid:46) getExp: function used to compute exponent

(cid:46) Summing locally in order of increasing exponents

(cid:46) Total sum by processor 0
10: MPI Reduce(&local sum, &total sum, 1, Mpi ﬂoat, Mpi sum, 0,Mpi comm world)

complexity mentioned above, we note that the cost of Algorithm 1 is O(n), no
sorting being performed and the access to the data to be summed being done
only once.

Let us mention that another reﬁned implementation of Algorithm 1 has been
proposed in [2]. This second implementation shares a common idea with Algo-
rithm 1 which is related to the way that the summands are added and diﬀers
from Algorithm 1 in that the ﬁnal sum is not carried out in the same way.
The advantage of this other implementation is that the summation results are
more accurate than those of Algorithm 1. Concerning its drawback, the cost
of complexity will be higher in the constant of the O(n) while being linear. In
the following sections, we evaluate the simpler and most cost-eﬀective version
of Algorithm 1 regarding accuracy, convergence speed for iterative schema and
reproducibility. We note that our implementations are done in the C program-
ming language with MPI, compiled with MPICC 3.2, and made them run on
an Intel i5 with 7.7 GB memory. Let us also note that for our experiments, we
report numerical values relying on thousands separators, typesetting 1234567.89
as 1, 234, 567.89.

3 Numerical Accuracy

In this section, we ﬁrst evaluate the numerical accuracy of our algorithm intro-
duced in Section 2.3. Secondly, we address the issue of the compromise between
numerical accuracy and running time of the studied algorithms. We take into
consideration two examples, namely Simpson’s rule and the LU factorization
method. For each example, we have implemented two parallel versions, using
MPI [19]. The ﬁrst one, called original program uses simple sums: to sum n val-
ues x1, ...., xn it computes (((x1 + x2) + x3) + .... + xn). The second program uses
Algorithm 1 and is called, the accurate program. The experiments are carried
out for several numbers of processors.

6

3.1 Simpson’s Rule

Our ﬁrst example computes the integral C ×(cid:82) b
0 f (x)dx of mathematical functions
f using Simpson’s rule. The Simpson’s rule is a numerical method that approxi-
mates the value of a deﬁnite integral of a function f using quadratic polynomials.
We measure the eﬃciency of our algorithm on this example by computing the
absolute errors between both the results of the original and accurate programs
with respect to the analytical solution of the integral, as shown in Figure 1. We
integrated the following functions C × cos(x), C × (1/x2 + 1) and C × tanh(x)
with C = 106, and b ranging in [2; 5]. The number of processors P ranges in
[2; 8]. Each processor computes a part of the integral.

Fig. 1: The absolute errors between the original program and the accurate one for
the integral computation of three diﬀerent functions (C × cos(x), C × (1/x2 + 1)
and C × tanh(x)) with the corresponding analytical result by varying the upper
bound of the integral b = 2, 3, 4, 5.

For the ﬁrst experimentation, let us take the function C × cos(x) as an exam-
ple. As it is observed in Figure 1, the absolute errors of the original program
are larger than those of the accurate program of several order of magnitude. To
better illustrate, let us consider the value 3 of the x-axis corresponding to the
upper bound of the integral. We notice that the absolute errors of the original
program are 392, 700.198, 411, 541.22875 and 414, 048.5725 for P = 2, P = 4
and P = 8, respectively. In contrast to the accurate program where the ab-
solute errors computed for the same example are 3, 238.3225, 32, 419.6975 and
77, 805.5725, respectively. In the same way, we note that the results of the second
function C × (1/x2 + 1) and the third function C × tanh(x) are similar to those
of the ﬁrst function C × cos(x). Moreover, the results of the C × tanh(x) function
show that the results computed by the original Simpson’s rule performed on a

7

 0 50 100 150 200 250 300 350 400 450234523452345Absolute error [x104]Upper bound of the integralOriginal program,proc=2Accurate program,proc=2Original program,proc=4Accurate program,proc=4Original program,proc=8Accurate program,proc=8Original program,proc=2Accurate program,proc=2Original program,proc=4Accurate program,proc=4Original program,proc=8Accurate program,proc=8Original program,proc=2Accurate program,proc=2Original program,proc=4Accurate program,proc=4Original program,proc=8Accurate program,proc=8f(x)=tanh(x)*Cf(x)=(1/x2+1)*Cf(x)=cos(x)*Clarge upper bound of the integral are those which have larger absolute errors. In-
deed, for P = 2 the absolute errors computed for the upper bound equal to 2,3,4
and 5 are 621, 315.3125, 1, 253, 797.375, 3, 166, 450.745625 and 4, 130, 976.360625
respectively.

The second experiment measures the execution time (in seconds) of the orig-
inal program, accurate program and another program based on sorting. The
choice of this last program is motivated by the main idea of Demmel and Hida
algoritms [5]. Let us consider the third function C × tanh(x) for P = 8, Figure 2
displays the running time in seconds taken by each program (original program,
accurate program and summation by sorting) to compute the integral of this
function. The results show that the summation program based on sorting like
Demmel and Hida algorithms [5] need more time to compute the integral of
the function C × tanh(x). Contrarily to the summation by sorting program, our
algorithm called accurate program requires much less time and a little more
than the original program. For example, to compute the integral of the function
C ×tanh(x) for b = 2 it takes 24s using the summation by sorting program, while
this computation takes only 0.37s and 1.08s using the original and the accurate
programs, respectively. It is well known that the summations based on sorting
performed on the summands before computations are more accurate but these
computations take more time (49s for b = 5) to performed than our accurate
algorithm (1.15s for the same value of b) where no sorting is performed.

Fig. 2: Execution time of the original program, accurate program and summation
algorithm by sorting for the integral computation of the function C × tanh(x).

8

 0 5 10 15 20 25 30 35 40 45 50 2 2.5 3 3.5 4 4.5 5Times in secondsUpper bound of the integralOriginal program,proc=8Accurate program,proc=8Summation by sorting,proc=83.2 LU Factorization

Our second example is the parallel LU factorization method. This method con-
sists in rewriting a matrix A as the product of a lower triangular matrix L and
an upper triangular matrix U such that A = LU . The LU factorization method
is a very common algorithm which can be used e.g. to solve linear systems or
to compute the determinant of a matrix. In the parallel case, the matrix A is
divided into blocks of rows and each processor performs its computations on a
given block. For our experiments we generated square matrices of various dimen-
sions n ∈ [200, 800] with increment of 100. These matrices contain values chosen
to introduce ill-conditioned sums [14]. In our case, we consider 30% of large
values among small and medium. By small, medium and large values we mean
respectively, of the order of 10−7, 100 and 107. This is motivated by the IEEE754
single precision arithmetic. Also, we take vectors x with the same proportions
of large values among small and medium as for the matrices.

Fig. 3: The absolute errors of the LU factorization by the original and accurate
algorithm for matrix of diﬀerent sizes.

The ﬁrst experimentation consists of comparing the numerical accuracy of the
LU factorization carried out using the original and accurate programs. Let us
consider a matrix A and vector x. We start by computing the solution vector
given by Ax = b. This vector is considered as the exact solution. Next, we apply
the original LU factorization program to the matrix A with P = 16 processors in
order to obtain Lorig and Uorig. In the same way, we factorize the matrix A using
the accurate LU factorization into Lacc and Uacc. We compare the new vector
solutions borig = Lorig × Uorig × x and bacc = Lacc × Uacc × x with the exact so-
lution b. Figure 3 represents the absolute errors between the computed solutions

9

 40 50 60 70 80 90 100200300400500600700800Absolute errors [x105]Matrix sizesOriginal LU factorizationAccurate LU factorizationafter factorization and the exact solution. These experiments show signiﬁcant
improvements: while the diﬀerence between the absolute errors of the original
and the accurate program are already up to an order of 105 for our smallest
matrices (n = 200), it reaches up to an order of 107 for large ones (n = 600).
More precisely, for n = 200 the absolute errors are 452, 1984 and 445, 6448 for
the original and the accurate LU factorization, respectively. We obtain 996, 1472
and 655, 3600 for n = 600. Thus, we conclude from this experimentation that
the accurate program shows its eﬃciency in terms of numerical accuracy when
we handle large matrices, i.e. when various types of absorptions and cancellation
have been introduced.

By the second experimentation, we want to show the running time taken by
each LU factorization program for a set of matrices of size varying from 200
to 800 with P = 16. Figure 4 summarizes the execution time taken by each
algorithm (original program, accurate program and summation by sorting) to
compute the LU factorization.

Fig. 4: Execution time of the original program, accurate program and summation
algorithm by sorting of the LU factorization.

We notice in Figure 4 that the summation algorithm based on sorting requires
much more time to compute the LU factorization. Besides, accurate program
needs much less time than summation algorithm by sorting and a little more than
the original algorithm. To better illustrate, let us take a matrix of size n = 300.
We remark that the execution takes only 66s and 90s with original program
and accurate program, respectively, while the summation by sorting program

10

 0 2000 4000 6000 8000 10000 12000 200 300 400 500 600 700 800Times in secondsMatrix sizesOriginal program,proc=16Accurate program,proc=16Summation by sorting,proc=16requires 224s for the same computation. Let us also remark that the running
time obtained for the summation by sorting program for the large matrices are
much larger than those obtained for the smallest one. In fact, the execution
time increase from 101s to approximately 2h for matrices sizes 200 and 800,
respectively, using the summation algorithm by sorting.

4 Convergence of Iterative Methods

In this section, we focus on the impact of accuracy on the number of iterations
required by numerical iterative methods to converge. For our experiments, we
consider two iterative methods: Jacobi’s method and Iterated Power method. As
in the previous section, we implemented two versions of the same algorithm, the
original one and our accurate version. We observed the impact on the conver-
gence, comparing their respective number of iterations.

4.1 Jacobi’s Method

The Jacobi’s method is a well known numerical method used to solve linear
systems of the form Ax = b. In this method, an initial guess, an approximate
solution x0, is selected and is iteratively updated until ﬁnding the solution xk of
the linear system. More precisely, this method iterates until |x(k+1)
i | < ε.
In our case, the parallelization of the Jacobi’s method is done according to the
row-wise distribution. Jacobi’s method is stable whenever the matrix A is strictly
diagonally dominant, i.e. its satisﬁes the property of Equation (5).

− xk

i

∀i ∈ 1, . . . , n,

|aii| >

(cid:88)

j(cid:54)=i

|aij|.

(5)

We examine the impact of accuracy on the convergence speed for the systems
of sizes 10 and 100. While the chosen systems were stable with respect to the
suﬃcient condition of the stability given by Equation (5), they are close to un-
stability with ∀i ∈ 1, . . . , n, |aii| ≈ (cid:80)
j(cid:54)=i |aij|. Figures 5 represents the diﬀerence
between the number of iterations of the original and the accurate programs.
Let us take the ﬁrst system of size n = 10. We notice that for various values
of ε varying from 10−2 to 10−5, the convergence speed in terms of number of
iterations increases from 59 to 2, 029. For the second system when n = 100, we
remark that the number of iterations reduced are larger than those computed for
the system n = 10 for the same values of ε. For instance the number of iterations
reduced for ε = 10−2 and ε = 10−5 are 861 and 10, 946, respectively. From these
two examples, we conclude that the smallest values of ε are those which have a
large diﬀerence between the original and the accurate programs in terms of the
number of iterations. Also, for a given value of ε, the accurate program shows its
eﬃciency on the convergence speed on large matrices compared to small ones.

11

Fig. 5: Diﬀerence between number of iterations necessary for the original and the
accurate programs to achieve the convergence of the Jacobi method.

4.2 Iterated Power Method

The Iterative Power method is particularly useful for estimating numerically
the largest eigenvalue and its corresponding eigenvector. The idea is to ﬁx an
arbitrary initial vector x(0) which contains a single non-zero elements. Then, we
build an intermediary vector y(1) such that Ax(0) = y(1). In order to obtain
the vector x(1), we renormalize y(1) so that the selected component is again
equal to 1. For the next iteration, we use x(1) as a selected vector. The iterative
process is repeated until convergence. We assume that, the parallelization of the
Iterated Power method is done according to the row-wise distribution. Let us
take a square matrix A of the form:

A =








d a12 · · · a1j
a21 d · · · a2j
...
...
ai1 ai2 · · · d








We assume that aij = 0.01 and d ∈ [300.0, 500.0] following the methodology
introduced in [4]. Figure 6 summarizes the diﬀerence between the number of
iterations of the original and the accurate Iterated Power method. As it is ob-
served in Figure 6, the accurate program accelerates the convergence speed of
the Iterative Power method by reducing the number of iterations needed to con-
verge. Indeed, for the matrix size n = 100 with various values of the diagonal
and using P = 4 we show that the number of iterations reduced increases from
205 to 340.

12

 0 2000 4000 6000 8000 10000 1200010e-210e-310e-410e-510e-210e-310e-410e-5Number of iterationsEpsilonNumber of iterations reducedNumber of iterations reducedSize of matrix n=100Size of matrix n=10Fig. 6: Diﬀerence between number of iterations of original and accurate Iterated
Power method (n = 100, d ∈ [300, 500] with increment of 20).

5 Reproducibility

In this section, we aim at evaluating the eﬃciency of Algorithm 1 on the im-
provement of reproducibility. Figures 7 and 8 give results of reproducibility for
Simpson’s rule and Matrix Multiplication, respectively. During our experiments,
we consider the original and the accurate programs of each method. We compare
the results of each of them on several processors and their respective results with
only one processor.

5.1 Simpson’s Rule

Let’s take again the example of Simpson’s rule already introduced in Section 3.
In practice, improving the numerical accuracy often improves reproducibility. We
measure the eﬃciency of our algorithm on this example by computing the abso-
lute errors between both the results of the original and the accurate programs
by varying the number of processors from 2 to 8 and their respective program
with only one processor, as shown in Figure 7. Let us consider several mathe-
matical functions C × cos(x), C × (1/x2 + 1) and C × tanh(x) with C = 106,
and b ranging in [2; 5]. For example, for f (x) = C × cos(x) with P = 2, the
results show that the absolute errors of the original program are larger (between
105, 553.117187 and 703, 687.4375) than those of the accurate program (between
47, 938.1875 and 195, 067.296875) as it is observed in Figure 7. Also, we can ob-
serve in Figure 7 that the results of integrals computed by the original program
performed on a large number of processors P = 8 are those which have a larger
absolute errors. As an example, the absolute errors computed for the function
C × tanh(x) with P = 8 are between 233, 122.109375 and 3, 637, 171.1875 while

13

 200 220 240 260 280 300 320 340300320340360380400420440460480500Number of iterationsThe value of the diagonal 'd'Diﬀerence between the original and the accurate Iterated Power methodthe absolute errors computed for the same example with P = 2 are between
87, 960.921875 and 1, 221, 541.8750.

Fig. 7: The reproducibility of the integral computations using Simpson method
of the original program and the accurate one depending on the number of pro-
cessors.

5.2 Matrix Multiplication

The computation of the matrix-matrix multiplication based on ﬂoating-point
addition and multiplication which are non-associative operations is prone to
accuracy problems. Moreover, the out of order execution of arithmetic operations
on diﬀerent or even similar parallel architectures are diﬀerent, reproducibility
of the results is not guaranteed. In this context, we address the problem of
reproducibility in the case of matrix multiplication. To parallelize this method,
we assume that each matrix is divided into sub-matrices of the size n/P . For our
experiments, we consider square matrices of various dimensions n ∈ [200, 800]
with increment of 200. These matrices contain a variety of ﬂoating-point values
chosen with diﬀerence in magnitude. More precisely, they are made of 50% of
large values (of the order of 107) among small (of the order of 100) and medium
(of the order of 10−7). Figure 8 represents the percentage of accuracy computed
between the original and the accurate programs carried out using P = 8 and their
respective result using only one processor. The results show that for diﬀerent
matrix sizes, the percentage of accuracy of the original program ranges from 3%
to 13%. Contrarily to the original program, the percentage linked to the accurate
program described in this article is equal to 100% for each matrix which conﬁrms
the usefulness of Algorithm 1.

14

 0 50 100 150 200 250 300 350 400234523452345Absolute error [x104]Upper bound of the integralOriginal program,proc=2Accurate program,proc=2Original program,proc=4Accurate program,proc=4Original program,proc=8Accurate program,proc=8Original program,proc=2Accurate program,proc=2Original program,proc=4Accurate program,proc=4Original program,proc=8Accurate program,proc=8Original program,proc=2Accurate program,proc=2Original program,proc=4Accurate program,proc=4Original program,proc=8Accurate program,proc=8f(x)=tanh(x)*Cf(x)=(1/x2+1)*Cf(x)=cos(x)*CFig. 8: The reproducibility of the matrix-matrix multiplication for diﬀerent size
of matrices.

6 Conclusion and Future Work

In this article, we have focused on the impact of accuracy on both the repro-
ducibility and convergence speed of numerical algorithms. The originality of this
article is to study the impact that a new accurate summation algorithm has on
the accuracy of numerical methods involving sums. In this work, we have experi-
mented our algorithm on several representative numerical methods by comparing
the original and the accurate programs of each of them. The experiments show
the usefulness of our algorithm on the improvement of reproducibility. Also, the
results obtained show the eﬃciency of our algorithm to reduce the number of
iterations required by numerical iterative methods to converge. More precisely,
the accurate program converge more quickly than the original one without loss
of accuracy. In a future work, we would like to reﬁne our algorithm by adding
a test phase after line 4 of Algorithm 1, i.e. before adding the value si to the
appropriate cell of the array sum by exp. Indeed, if we have a large set of values
to be summed with the same exponent, the result produced can have a larger
exponent than the initial one. Therefore, a loss of accuracy can be caused during
the computations of the local sums. An interesting perspective consists in feeding
our algorithms [2] with a static analysis. Our idea is to rely on static analysis
to detect the exponents range of a given set of values that will be summed.
Once this range is accurately computed, one can use our summation algorithm
and obtain more accurate ﬂoating-point results. Further, it will be interesting
to explore the impact of our algorithms in the context of neural networks. Our
goal is to improve the numerical accuracy of computations by using the accurate
summation algorithms [2] as a replacement for their summation algorithms.

15

 0 10 20 30 40 50 60 70 80 90 100200400600800% of accuracyMatrix sizesOriginal matrix multiplicationAccurate matrix multiplicationAcknowledgments This work was supported by a regional funding (Region
Occitanie) and partially by project ANR-17-CE25-0018 FEANICSES.

References

1. ANSI/IEEE. IEEE Standard for Binary Floating-Point Arithmetic. SIAM, 2008.
2. Farah Benmouhoub, Pierre-Loic Garoche, and Matthieu Martel. Parallel Accurate
and Reproducible Summation. Number 20 in Advances in Intelligent Systems and
Computing, London, UK, 2021. Springer.

3. Gerd Bohlender. Floating-point computation of functions with maximum accuracy.

IEEE Trans. Comput., 26(7):621–632, July 1977.

4. Nasrine Damouche, Matthieu Martel, and Alexandre Chapoutot.

Impact of
accuracy optimization on the convergence of numerical iterative methods.
In
M. Falaschi, editor, LOPSTR 2015, volume 9527 of Lecture Notes in Computer
Science, pages 143–160. Springer, 2015.

5. James Demmel and Yozo Hida. Accurate ﬂoating point summation. Technical
Report UCB/CSD-02-1180, EECS Department, University of California, Berkeley,
May 2002.

6. James Demmel and Yozo Hida. Accurate and eﬃcient ﬂoating point summation.

SIAM J. Sci. Comput., 25(4):1214–1248, April 2003.

7. James Demmel and Hong Diep Nguyen. Parallel reproducible summation. IEEE

Transactions on Computers, 64(7):2060–2070, 2015.

8. David Goldberg. What every computer scientist should know about ﬂoating-point

arithmetic. ACM Comput. Surv., 23(1):5–48, March 1991.

9. Stef Graillat, Philippe Langlois, and Nicolas Louvet. Algorithms for Accurate,
Validated and Fast Polynomial Evaluation. Japan Journal of Industrial and Applied
Mathematics, 26(2-3):191–214, 2009.

10. Stef Graillat and Val´erie M´enissier-Morain. Compensated Horner scheme in com-
plex ﬂoating point arithmetic. In Proceedings, 8th Conference on Real Numbers
and Computers, pages 133–146, Santiago de Compostela, Spain, July 2008.

11. Nicholas Higham. The accuracy of ﬂoating point summation. SIAM Journal on

Scientiﬁc Computing, 14(4):783–799, 1993.

12. Nicholas Higham. Accuracy and Stability of Numerical Algorithms. Society for

Industrial and Applied Mathematics, USA, 1996.

13. William Kahan. A survey of error analysis. In IFIP Congress, 1971.
14. Philippe Langlois, Matthieu Martel, and Laurent Th´evenoux. Accuracy versus
time: A case study with summation algorithms.
In Proceedings of the 4th In-
ternational Workshop on Parallel and Symbolic Computation, PASCO ’10, page
121–130, New York, NY, USA, 2010. Association for Computing Machinery.
15. H. Leuprecht and W. Oberaigner. Parallel algorithms for the rounding exact sum-

mation of ﬂoating point numbers. Computing, 28(2):89–104, 1982.

16. Michael Malcolm. On accurate ﬂoating-point summation. Commun. ACM,

14(11):731–736, November 1971.

17. J.M. Muller, N. Brisebarre, F. De Dinechin, C.P. Jeannerod, V. Lef`evre,
G. Melquiond, N. Revol, D. Stehl´e, and S. Torres. Handbook of Floating-Point
Arithmetic. Birkh¨auser, 2010.

18. Takeshi Ogita, Siegfried Rump, and Shin’ichi Oishi. Accurate sum and dot product.

SIAM Journal on Scientiﬁc Computing, 26(6):1955–1988, 2005.

16

19. Peter Pacheco. An Introduction to Parallel Programming. Morgan Kaufmann

Publishers Inc., San Francisco, CA, USA, 1st edition, 2011.

20. Mich`ele Pichat. Correction d’une somme en arithmetique a virgule ﬂottante. Nu-

mer. Math., 19(5):400–406, October 1972.

21. Siegfried Rump. Ultimately fast accurate summation. SIAM Journal on Scientiﬁc

Computing, 31(5):3466–3502, 2009.

22. Siegfried Rump and Takeshi Ogita. Fast high precision summation. Nonlinear

Theory and Its Applications, IEICE, 1, 01 2010.

23. Siegfried Rump, Takeshi Ogita, and Shin’ichi Oishi. Accurate ﬂoating-point sum-
mation part I: faithful rounding. SIAM J. Scientiﬁc Computing, 31(1):189–224,
2008.

24. Laurent Th´evenoux, Philippe Langlois, and Matthieu Martel. Automatic source-
to-source error compensation of ﬂoating-point programs: code synthesis to opti-
mize accuracy and time. Concurrency and Computation: Practice and Experience,
29(7):e3953, 2017.

17

