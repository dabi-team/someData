Astronomy & Astrophysics manuscript no. main
June 22, 2022

©ESO 2022

2
2
0
2

n
u
J

9
1

]

M

I
.
h
p
-
o
r
t
s
a
[

1
v
1
0
5
9
0
.
6
0
2
2
:
v
i
X
r
a

DoG-HiT: A novel VLBI Multiscale Imaging Approach

H.Müller1 and A.P. Lobanov1

Max-Planck-Institut für Radioastronomie, Auf dem Hügel 69, Bonn, 53121, Germany
e-mail: hmueller@mpifr-bonn.mpg.de, e-mail: alobanov@mpifr-bonn.mpg.de

Received September 15, 1996; accepted March 16, 1997

ABSTRACT

Context. Reconstructing images from very long baseline interferometry (VLBI) data with sparse sampling of the Fourier domain (uv-
coverage) constitutes an ill-posed deconvolution problem. It requires application of robust algorithms maximizing the information
extraction from all of the sampled spatial scales and minimizing the inﬂuence of the unsampled scales on image quality.
Aims. We develop a new multiscale wavelet deconvolution algorithm DoG-HiT for imaging sparsely sampled interferometric data
which combines the diﬀerence of Gaussian (DoG) wavelets and hard image thresholding (HiT). Based on DoG-HiT, we propose a
multi-step imaging pipeline for analysis of interferometric data.
Methods. DoG-HiT applies the compressed sensing approach to imaging by employing a ﬂexible DoG wavelet dictionary which is
designed to adapt smoothly to the uv-coverage. It uses closure properties as data ﬁdelity terms only initially and perform non-convex,
non-smooth optimization by an amplitude conserving and total ﬂux conserving hard thresholding splitting. DoG-HiT calculates a
multiresolution support as a side product. The ﬁnal reconstruction is reﬁned through self-calibration loops and imaging with amplitude
and phase information applied for the multiresolution support only.
Results. We demonstrate the stability of DoG-HiT and benchmark its performance against image reconstructions made with CLEAN
and Regularized Maximum-Likelihood (RML) methods using synthetic data. The comparison shows that DoG-HiT matches the
superresolution achieved by the RML reconstructions and surpasses the sensitivity to extended emission reached by CLEAN.
Conclusions. Application of regularized maximum likelihood methods outﬁtted with ﬂexible multiscale wavelet dictionaries to imag-
ing of interferometric data matches the performance of state-of-the art convex optimization imaging algorithms and requires fewer
prior and user deﬁned constraints.

Key words. Techniques: interferometric - Techniques: image processing - Techniques: high angular resolution - Methods: numerical
- Galaxies: jets - Galaxies: nuclei

1. Introduction

In very long baseline interferometry (VLBI), signals recorded at
individual radio antennas are combined (correlated) in order to
sample angular scales inversely proportional to pairwise antenna
separations projected onto the plane of the incoming wavefront.
Described by the van Cittert-Zernike theorem, the correlation
product (visibility) of the signals recorded at two antennas at
a given time is given by a single spectral harmonic correspond-
ing to a single spatial frequency of the Fourier transform of the
observed sky brightness distribution (see Thompson et al. 1994).
From a complete sampling of spatial frequencies, the true image
could be revealed by the inverse Fourier transform. However, the
practical limitations on the number of antennas, observing band-
width and observing time often result in situation when VLBI
data provide only sparse sampling (uv-coverage) of the spatial
frequencies (or "Fourier domain"), below the Nyquist-Shannon
sampling rate.

The development of powerful imaging algorithms such as
CLEAN (Högbom 1974) and their decade long successful appli-
cation in VLBI studies demonstrated that a reliable reconstruc-
tion of the true sky brightness distribution is still possible under
a strong assumption about the sky brightness distribution being
compressible as a sum of point sources. CLEAN and its many
variants (e.g. Clark 1980; Schwab 1984) work well not only for
compact structures but also for extended emission. CLEAN is
still broadly used, mainly because it is practical. However, front-

line VLBI applications such as millimetre or space VLBI de-
mand better imaging tools which would alleviate the known lim-
itations of CLEAN and provide superresolution, multiscalar de-
compositions, and high dynamic range.

Multiresolution imaging routines based on the greedy match-
ing pursuit procedure inherent to CLEAN have been developed
for decades now (Wakker & Schwarz 1988; Starck et al. 1994;
Bhatnagar & Cornwell 2004; Cornwell 2008; Rau & Cornwell
2011). These studies build up on the great success of compressed
sensing theory (e.g. Candès et al. 2006; Donoho 2006), i.e. that
an image can be sparsely represented in a suitable set of ba-
sis functions (atoms). Even the CLEAN algorithm (sparsity in
pixel basis) Lannes et al. (1997) and total variation regulariza-
tion methods (sparsity of the Haar wavelet) could be understood
in this way.

Imaging algorithms based on wavelets attract close atten-
tion of the astronomy community because they stand out as ex-
tremely helpful in the analysis and compression of image fea-
tures on multiple scales (Starck & Murtagh 2006; Starck et al.
2015; Mertens & Lobanov 2015; Line et al. 2020). Both ex-
tended emission features and small scale structures are well com-
pressible with wavelets. Moreover, wavelets of varying scales are
sensitive to diﬀerent ranges of visibilities, allowing the user to
incorporate information about the radially distributed positions
of gaps in the uv-coverage in the imaging procedure. Hence,
sparsity in the wavelet domain is a strong and interesting image
prior for the radio aperture synthesis imaging problem.

Article number, page 1 of 20

 
 
 
 
 
 
A&A proofs: manuscript no. main

The past ﬁve decades saw an ongoing development of regu-
larized maximum likelihood methods (RML) for interferometric
imaging, in particular with the development of image entropy
regularizers such as the maximum entropy method (MEM) (e.g.
Frieden 1972; Narayan & Nityananda 1986; Wiaux et al. 2009;
Li et al. 2011; Garsden et al. 2015; Thiébaut & Young 2017).
The RML methods have been applied particularly extensively
for imaging with the EHT1, (e.g. Ikeda et al. 2016; Akiyama
et al. 2017b,a; Chael et al. 2018; Event Horizon Telescope Col-
laboration et al. 2019). In a typical RML application, the im-
age is recovered by minimizing simultaneously a data ﬁdelity
term which measures the proximity of the recovered solution to
the true data, i.e visibilities and/or closure properties, and a set
of regularization terms which measure the feasibility of the re-
covered solution. It has been demonstrated that l1-penalty terms
promote sparsity in the image domain. Hence, RML algorithms
and the progress in convex optimization (Beck & Teboulle 2009;
Combettes & Pesquet 2009) provide a powerful framework of
respecting sparsity during image deconvolution.

However, the deﬁciencies of uv-coverages inherent to such
interferometric instruments as the EHT or the space VLBI mis-
sion RadioAstron pose additional challenges. Compressed sens-
ing approaches applied to data from such arrays are capable,
in principle, of recovering the signiﬁcant structure of the tar-
get (achieving small data ﬁdelity terms) while suppressing any
additional noise-induced image features and sidelobes (achiev-
ing small penalty terms). However, in VLBI observations the
sidelobes and the true image structure often become compara-
ble in their magnitudes. The suppression of structure due to im-
age sparsity aﬀects the recovered data signiﬁcantly. A more ad-
vanced treatment of image features, i.e. a more advanced diﬀer-
entiation between observed emission and noisy uv-gap induced
structures, and an amplitude conserving optimization strategy
are needed. Furthermore, an unsupervised approach for blind
imaging is desired.

Random and systematic noise factors in the ﬁnal image can
be induced at various steps of the analysis. In particular, errors
resulting from uv-coverage deﬁciencies and antenna based noise
factors (calibration issues, thermal noise) depend on location of
the trace of the antenna pair in the uv-plane. Hence, these errors
are scale and direction dependent. We need a novel algorithm
that can deal with this, i.e. that can automatically decompose
noisy features from signal features. This is a task that is suitable
for wavelets in the ﬁrst instance since they decompose the im-
age into a sequence of scales. Direction dependent information
is more diﬃcult to compress and will not be addressed in this
paper.

In this paper, we present a new multiscalar wavelet imag-
ing algorithm built upon the compressed sensing approach. Our
method extends over standard sparsity promoting imaging al-
gorithms by applying a more stringent separation of signiﬁcant
image features from noise contributions by using an adaptive
wavelet dictionary and suppressing the noise-induced artifacts
in a novel amplitude-conserving hard thresholding algorithm.
This algorithm is well suited for dealing with high level side-
lobes such as the ones typically found in the data from EHT or
space VLBI observations.

An important feature of the algorithm is that the initial selec-
tion of the scales in the wavelet dictionary derives from the uv-
coverage of observations and not from any assumptions about
structure of the target source. We utilize current state of the
art optimization algorithms for solving the resulting RML min-

1 Event Horizon Telescope

Article number, page 2 of 20

imization problem, but amend the imaging pipeline by a hard
thresholding sparsity term based on the multiresolution support,
which allows us to retain necessary image information while
suppressing noisy scales. We deal with potential residual calibra-
tion deﬁciencies of the data by ﬁrst using only the gain-invariant
closure quantities for imaging and then, after identifying and
suppressing noise contributions, imaging the full data with an
optimized, ﬁxed multiresolution support. The resulting objective
functional for minimization is not convex and not smooth, which
requires employing non-convex and non-smooth optimization
strategies. We present a ﬁnal imaging pipeline which is immedi-
ately applicable to VLBI data. This imaging pipeline takes con-
siderably fewer parameters than typical RML pipelines, thereby
presenting a viable step towards a more unsupervised imaging
approach.

We test our pipeline routine on test images recently used
to verify the modern generation of RML image routines (Tiede
et al. 2020). For incomplete uv-coverages, our algorithm per-
forms better than the canonical CLEAN and its multiscale vari-
ants, owing to the ﬂexibility of the dictionary (allowing to adapt
it to a speciﬁc uv-coverage of the array), the sparse representa-
tion of astronomical images in the wavelet basis (compared to
the representation with CLEAN or MS-CLEAN components),
and the correct treatment of scale-dependent noise properties.

2. Theory

This section summarizes the relevant theory and background for
diﬀerent aspects of the new algorithm, focusing primarily on
application of wavelets for deconvolution in aperture synthesis
and on speciﬁc aspects of optimization procedures applied to
sparsely sampled data.

2.1. Aperture Synthesis

In interferometric observations, every antenna in the array
records the electromagnetic ﬁeld of an incoherent sky bright-
ness distribution I(x, y), where x and y are angular coordinates
on the sky. Following the van Cittert-Zernike theorem, the cross
correlation between the signals recorded by two antennas over
a baseline (u, v) (spatial frequencies in units of wavelengths) is
given by the Fourier transform of I(x, y) at this baseline:

(cid:90) (cid:90)

V(u, v) =

e−2πi(xu+yv)I(x, y)dxdy ,

(1)

where V is the complex visibility. This relation holds under as-
sumptions of a ﬂat wavefront and small ﬁeld of view approxi-
mation. Every antenna pair at a ﬁxed time gives rise to a spe-
ciﬁc baseline. The projection of a baseline on a plane orthog-
onal to the direction to the target the Earth rotates baselines
smoothly shift by time describing the typical elliptical traces in
uv-coverages. However, due to the small number of antennas in
VLBI arrays the coverage of measurements in the uv-domain
remains sparse. In particular, gaps in the uv-coverage introduce
sidelobes and artifacts in the recovered image. When inverting
the Fourier transform (to produce ID) the result can be written as
convolution:

ID = B ∗ I,

(2)

where ID is the dirty image, i.e. the inverse Fourier transform
of the (tapered) observed visibilities, and B is the dirty beam,
i.e. the inverse Fourier transform of the (tapered) projection onto
measured baselines in the Fourier codomain.

H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

Aperture synthesis imaging is the problem of recovering the
true distribution I(x, y) from a discrete sparse set of observed
visibilities. This procedure could also be understood as a decon-
volution problem, see Eq. (2) . The incomplete uv-coverage in-
troduces direction- and scale-dependent sidelobe patterns in the
dirty image and the dirty beam. Deconvolution in this case be-
comes an ill-posed inverse problem. In particular, the solution to
the imaging problem in Eq. (2) is strictly speaking not unique as
there are Fourier harmonics missing from the observation (his-
torically called the “invisible distributions”). A successful de-
convolution method must be able to identify and categorize these
invisible distribution and minimize their impact on the restored
image.

Image restoration is further complicated by the variable ther-
mal noise and signal-to-noise ratio (SNR) of visibility measure-
ments. The visibility SNR is systematically reduced at long base-
lines. As the antenna sensitivity enters the reconstruction at spe-
ciﬁc scales and directions, determined by the position of the
baseline corresponding to a given antenna pair, the noise be-
comes scale and direction dependent.

Various calibration issues also need to be addressed during
image restoration. Systematic direction-independent calibration
errors can be factorized into multiplicative station based gains gi
(where the index i denotes the antenna in the array) aﬀecting the
relation between the observed visibilities Vij and the true visibil-
ities Vij:
Vij ≈ gig(cid:63)

j Vij + Nij,

(3)

where Nij denotes thermal noise on the baseline. In particular,
phase information is typically only available after a calibration
by an ad hoc initial model. In standard imaging approaches
(e.g., in CLEAN), the problem of calibration is typically ad-
dressed through a hybrid imaging approach. In this case, an ini-
tial image is ﬁrst produced using the a priori set of instrumen-
tal gains and then the gain terms are solved for as in Eq. (3)
in order to enforce consistency with the current image guess
(with the solution typically obtained by a gradient descent ap-
proach or self-calibration), and these two steps are repeated it-
eratively until the desired image quality is reached. In this way
alternating self-calibration and imaging steps converges to a self-
contained model description consistent with the observed and
self-calibrated data.

Some of the calibration issues can be circumvented by em-
ploying closure quantities computed from combinations of vis-
ibilities that are independent of antenna-based gain errors. The
closure phase, Ψijk, is the phase over a triangle of antennas i, j, k,
i.e.:
Ψijk = arg

VijVjkVki

(4)

(cid:16)

(cid:17)

.

The closure amplitude, A, is the ratio of amplitudes over a square
of antennas i, j, k, l:

Aijkl =

|Vij||Vkl|
|Vik||Vjl|

.

(5)

Not all closure triangles and closure squares are independent,
which leads to reducing the number of total observables. Let us
assume that at a speciﬁc time N antennas are observing simul-
taneously. This gives rise to N(N − 1)/2 independent baselines,
while there are only (N −1)(N −2)/2 independent closure phases
and N(N − 3)/2 independent closure amplitudes (Chael et al.
2018). Hence the number of observables is reduced by a frac-
tion of 1 − 2/N for closure phases and 1 − 2/(N − 1) for closure
amplitudes.

2.2. Deconvolution

Historically, the imaging problem described by Eq. (2) has been
addressed through inverse modelling, i.e. by CLEAN (Högbom
1974) which can be classiﬁed as a greedy, matching pursuit algo-
rithm. The problem is ﬁrst translated into a deconvolution prob-
lem by taking the inverse Fourier transform of the visibilities.
Hence, CLEAN requires performing this inversion on calibrated
complex visibilities at every stage. The deconvolution problem
is therefore solved by inverse modelling: CLEAN searches iter-
atively for the position of the maximum in the residual image,
stores this in a list of delta-components, and updates the resid-
ual by subtracting the rescaled and shifted dirty beam from the
residual image. In multiscale variants of CLEAN the delta com-
ponents are replaced by more sophisticated extended basis func-
tions (Bhatnagar & Cornwell 2004; Cornwell 2008; Rau & Corn-
well 2011). Recent years saw a continued development of imag-
ing by forward modelling (e.g. Garsden et al. 2015; Akiyama
et al. 2017b; Chael et al. 2018) in which Eq. (1) is solved by
ﬁtting a model solution to the visibilities by minimizing the er-
ror in some cost functional (data ﬁdelity term). With this forward
modelling approach, RML methods can work directly on the clo-
sure quantities or a mix of data products in order to reduce the
inﬂuence of calibration errors on the reconstruction. Regulariza-
tion and missing information are dealt with by simultaneously
minimizing a penalization term which promotes desired image
features, i.e. sparsity, smoothness or small entropy. The result-
ing minimization problem is then solved by standard numerical
optimization algorithms, e.g. by a gradient descent algorithm.

A major advantage of the work presented in this paper is
the use of novel basis functions (i.e. wavelets). We will discuss
them in more detail in Sec. 2.3. These wavelets are extended and
allow a more thorough analysis of the uv-coverage of the ob-
servations. The basis functions used in (MS-)CLEAN and RML
are typically not oﬀering this kind of analysis. Standard CLEAN
(Högbom 1974) models the image as a set of delta functions.
It’s multiscalar variants use some version of truncated Gaus-
sian functions (i.e. see the discussions in Cornwell 2008). RML
methods are utilizing pixel grids.

2.3. Wavelets

The continuous wavelet transform (CWT) could be understood
as an extension of Fourier transform (Starck et al. 2015) in which
the Fourier decomposition in frequency domain is amended by
a windowing of the measurement domain with a specially de-
signed analyzing wavelet function. In the deﬁnition of Gross-
mann et al. (1989), the CWT related to an analyzing wavelet
Φ(t) operates in one dimension on the space of square integrable
functions so that

I (cid:55)→ W(a, b) = 1
√
a

(cid:90)

I(t)Φ∗

(cid:33)

(cid:32)

t − b
a

dt = I ∗ ˜Φa(b),

(6)

Φ∗( −t

where ˜Φa(t) = 1√
a ), a is the scale parameter and b is the po-
a
sition parameter. Hence, the CWT performs eﬀectively a number
of convolutions with dilated versions of the analyzing wavelet Φ.
There are diﬀerent choices for the analyzing wavelet functions
around including the Morlet wavelet (Goupillaud et al. 1984;
Coupinot et al. 1992), the Haar wavelet (Stollnitz et al. 1994),
the Mexican-hat wavelets (Murenzi 1989) and discrete versions
(e.g. see Mallat 1989; Starck et al. 2015, and references therein).
In this work we are using Diﬀerence of Gaussian (DoG)
wavelets that are commonly applied to approximate Mexican-

Article number, page 3 of 20

A&A proofs: manuscript no. main

exp

hat wavelets (Gonzalez & Woods 2006; Assirati et al. 2014):


−r(x, y)2
DoG (x, y) = 1


Φσ1,σ2
2πσ2
2σ2
1
1
= Gσ1 − Gσ2 ,

(7)
where necessarily σ1 ≤ σ2 and Gσ j denotes a Gaussian with
standard deviation σ j.

−r(x, y)2
2σ2
2

1
2πσ2
2


 −

exp




Wavelets in image domain (convolution) directly translate to

masks in Fourier domain (pointwise multiplication).

F Φσ1,σ2

DoG (u, v) ∝ exp

(cid:16)
−2π2σ2

1q(u, v)2(cid:17)

− exp

(cid:16)

−2π2σ2

2q(u, v)2(cid:17)

,
(8)

where q(u, v) denotes the radius in Fourier domain.

Of special interest for image compression is the discrete
wavelet transform, in particular the a-trou wavelet transform
(also called starlet transform). In a nutshell, the a-trou wavelet
transform aims to compute a sequence of smoothing scales c j
by convolving the image with a discretized smoothing kernel di-
lated by 2 j pixels, where j labels the scale and ranges from 0 up
to a ﬁnal smoothing scale J. Wavelet scales are deﬁned as the
diﬀerence of two smoothing scales:
ω j = c j − c j+1
The last smoothing scale cJ is added to the set of wavelet scales
resulting in the set: [ω0, ω1, ..., ωJ−1, cJ]. This set decomposes
the initial image into subbands ω j, each of them containing in-
formation on spatial scales from 2 jρ to 2 j+1ρ, where ρ is the
smallest scale in the image, i.e. the width of the smoothing ker-
nel (which is often chosen to be close to the pixel scale). The set
is complete in the sense that the image at the limiting resolution
c0 can be recovered by summing all scales:
c0 =

ω j + cJ .

(10)

(cid:88)

(9)

j

The a-trou wavelet transform has a wide range of applica-
tions and was successfully applied to radio interferometry before
(e.g. Li et al. 2011; Garsden et al. 2015). However, the a-trou
wavelet decomposition by construction allows only for scales
with the widths of 20, 21, 22, 23, ... pixels. In this study, we are
interested in getting a more ﬂexible selection of scales to adapt
the scales to the uv-coverage in order to diﬀerentiate better be-
tween well and poorly constrained spatial scales.

Therefore, we propose to construct a continuous wavelet
decomposition out of DoG wavelets in the same way as the
a-trou wavelet transform was constructed out of a discretized
smoothing kernel. We select an ascending sequence of widths
σ0 ≤ σ1 ≤ ... ≤ σJ and compute the smoothing scales c j by
convolution with Gaussians with widths σ j, i.e. c j = I ∗ Gσ j.
The wavelet scales ω j are then set by
ω j = c j − c j+1 = I ∗ Φσ j,σ j+1
(11)
which approximates suﬃciently well the Mexican hat wavelet
scales.

DoG

,

We call a set of basis functions in compressed sensing a dic-
tionary, while the basis functions itself are called atoms of the
dictionary. The term dictionary is also used for the linear map-
ping that evaluates a coeﬃcient array of these atoms (Starck et al.
2015). The set of DoG wavelet functions Φσ j,σ j+1
together with
the last smoothing scale GσJ builds a multiscalar dictionary Γ:

DoG

The atoms of the dictionary Γ are the wavelets Φσ j,σ j+1
and GσJ .
By construction, see also Eq. (10), all atoms in the dictionary
sum to Gσ0 , which is (given that σ0 should be chosen very small,
i.e. Gσ0 is a delta peak at the pixel scale) indicating that the dic-
tionary Γ has full rank.

DoG

Another crucial property of the dictionary Γ is that the in-
is vanishing. Hence, only the ﬁnal

tegral of the atoms Φσ j,σ j+1
smoothing scale GσJ transports total ﬂux in the image.

DoG

The subbands I j hold the information of the image at a re-
spective scale described by σ j and σ j+1. We will denote the col-
lection of subbands of an image I by I = {I1, I2, ..., IJ} for the
rest of the paper. However, even if I = Γ(I1, I2, ..., IJ) holds, it is
usually I j (cid:44) ω j due to the non-orthogonality of the DoG wavelet
functions. However, ω j should provide a reasonable initial guess
if one tries to ﬁnd an array I = {I1, I2, ..., I j} which satisﬁes
I = ΓI .

2.4. Sparsity Promoting Regularization

We apply sparsity promoting regularization in the generalized
Tikhonov framework:

ˆI ∈ argminI [S (FΓI , V) + αR(I )] ,

(13)

where S is the data ﬁdelity term which measures the proximity
between the recovered visibilities FΓI and the observed visibil-
ity data, V. The term F denotes mapping of the image intensity
onto the visibilities, i.e. it computes a tapered and weighted pro-
jection of the Fourier transform of x on a discrete and ﬁxed sam-
pling. The term R denotes the regularization term which mea-
sures the feasibility of the guess I . The parameter α controls
the bias between both terms. The ﬁnal recovered image solution
is then:

ˆI = Γ ˆI .

(14)

The data ﬁdelity terms used for this paper are introduces as
follows. Let V = FΓI denote the visibility data predicted from
the current guess. We quantify the proximity between the pre-
dicted and measured visibilities by the eﬀective χ2-distance be-
tween them,

S vis(V , V) = 1
Nvis

Nvis(cid:88)

i=1

|Vi − Vi|2
Σ2
i

,

(15)

where Nvis is the number of visibilities and Σi the estimated ther-
mal noise of a given visibility. This χ2 corresponds directly to
a log-Likelihood, given uncorrelated Gaussian thermal noise on
the diﬀerent baselines. In addition to this, we also use similar
distances deﬁned for three additional quantities. The between the
measured and predicted visibility amplitudes,

S amp(V , V) = 1
Nvis

Nvis(cid:88)

i=1

(|Vi| − |Vi|)2
Σ2
i

.

(16)

The distance between the measured and predicted closure
phases,

Γ : (I0, I1, I2, ..., IJ) (cid:55)→

J−1(cid:88)

j=0

Φσ j,σ j+1
DoG

∗ I j + GσJ ∗ IJ.

(12)

S cph(V , V) = 1
Ncph

Ncph(cid:88)

i=1

|Ψi(V ) − Ψi(V)|2
Σ2

cph,i

,

(17)

Article number, page 4 of 20

H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

where Ncph is the number of closure phase combinations, Σcph,i
the noise on a closure phase Ψi(V), and Ψi(V ) denotes the re-
spective closure phase computed from the array of predicted vis-
ibilities, V . And ﬁnally, the distance between measured and pre-
dicted closure amplitudes,

S cla(V , V) = 1
Ncla

Ncla(cid:88)

i=1

| ln Ai(V ) − ln Ai(V)|2
Σ2

cla,i

,

(18)

with similar conventions as for the closure phases. We would
like to note here that Eq. (17) and Eq. (18) are only approximate
expressions for the correct log-likelihoods for closure products.
(e.g. Blackburn et al. 2020; Arras et al. 2022). These approxi-
mations and combinations of them are applied in Sec. 4 for the
analysis of test data.

It is known that sparsity is promoted by convex pseudonorm
functionals as regularization terms (Starck et al. 2015), e.g. by a
term of the form:

Rl0 (I ) = (cid:107)I (cid:107)l0

=

J(cid:88)

(cid:88)

j=0

i

w j|Ii

j|0,

(19)

with weights w j = max Ψσ j,σ j+1
and i referring to the pixels in the subbands.

DoG

(see our discussion in Sec. 3.1)

Another type of regularization terms used for this work are

characteristic functions, incorporating a total ﬂux f constraint

Rﬂux(I, f ) =

(cid:40)

total ﬂux of I = f

0
∞ else,

or a multiresolution support, M, such that

Rmrs(I , M) =

(cid:40)

0 I (cid:44) 0 only in M
∞ else.

(20)

(21)

The multiresolution support M is a subdomain of the parame-
ter space occupied by I = {I1, I2, ..., IJ}, and it comprises the
coeﬃcients in I that are allowed to be unequal to zero. In this
sense, Rmrs could be understood as a compact ﬂux constraint (i.e.
all coeﬃcients in the subbands I1, I2, ..., IJ outside of a compact
core region are constrained to zero), a multiscale constraint (i.e.
all coeﬃcients within one uncovered subband I j are set to zero)
or a combination of both.

2.5. Optimization

We use a ﬂexible dictionary of DoG-wavelets and minimize Eq.
(13) directly with convex optimization algorithms. Generally, a
gradient descent algorithm could be used for this task as long as
the data ﬁdelity term and the penalty term are smooth (i.e. pos-
sess a gradient). However, for sparsity promoting algorithms the
penalty term is typically non-smooth, i.e. the l0-norm is not dif-
ferentiable. In numerical optimization, it is common practice to
use the l1-norm as a convex approximation to the non-convex l0-
functional stated above (e.g. Starck et al. 2015). As the l1-norm
is also not smooth (prohibiting gradient descent algorithms from
use), powerful optimization strategies were developed in numer-
ical mathematics that typically outperform smooth approxima-
tions to the l1-norm. Several of such optimization strategies have
been recently applied to aperture synthesis as well (Li et al.
2011; Carrillo et al. 2012, 2014; Garsden et al. 2015; Girard
et al. 2015; Onose et al. 2016; Mouri Sardarabadi et al. 2016;
Akiyama et al. 2017b,a; Onose et al. 2017; Cai et al. 2018a,b;
Chael et al. 2018; Pratley et al. 2018; Event Horizon Telescope

Collaboration et al. 2019). These algorithms depend on the prox-
imal point operator instead of the gradient. However, in the
present work we are addressing a slightly more advanced prob-
lem of maintaining suﬃcient contrast in the image, and hence
we are interested in the l0 functional instead of its common con-
vex approximation l1. Moreover, this will allow us to construct a
multiresolution support later on. Relying on the overall success
of proximal-point based algorithms in dealing with this kind of
optimization problems, we nevertheless attempt to address our
minimization problem by a proximal point based optimization.
In the following we describe basic properties of the prox-
imal point operator. If H is a proper, convex and lower semi-
continuous functional on a Hilbert space X, then the proximity
operator of H is deﬁned as the mapping (Moreau 1962):

proxτ,H(z) = argmins∈X

(cid:40)

H(s) + 1
2τ

(cid:41)

(cid:107)s − z(cid:107)X

,

(22)

and proxτ,H is well deﬁned (i.e. there is a unique single-valued
minimum). For a convex, proper and lower semi-continuous ob-
jective functional, such as the right hand side of Eq. (22), the zero
element is in the subdiﬀerential of the functional at the point of
the minimum. Hence, ˆs := proxτ,H(z) satisﬁes:

z − ˆs ∈ τ∂H[ ˆs].

(23)

The power of proximal operators comes from their ﬁxed-point
property. It follows directly from Eq. (22) and Eq. (23), that:

ˆs ∈ argminsH(s) ⇐⇒ ˆs = proxτ,H( ˆs)

(24)

independently of τ ≥ 0, for a sketch of the proof see Appendix
A.

Hence, we can solve the minimization in Eq. (13) by ﬁxed-
point iterating the proximity operator. This procedure is exact
in the sense that convergence proofs are available (e.g. Mar-
tinet 1972). For a combination of a smooth term (data-ﬁdelity
term) and a non-smooth term (penalty term) one ends up at a
two step splitting minimization strategy consisting of a gradient
descent step for the data ﬁdelity term and one proximity step
for the penalty term (Combettes & Pesquet 2009). The forward-
backward splitting algorithm is outlined in its general framework
in Table 1. The two-step splitting is realized during the last step
of the algorithm, when the current guess is updated by a proxi-
mal step and a gradient descent step.

Interestingly, despite being derived in the context of convex
optimization, there are also local convergence proofs available
for the case when S and R are not convex, but the penalty term
remains lower semicontinuous, proper and satisﬁes the technical
Kurdyka-Łojasiewicz property, e.g. see Attouch et al. (2013);
Ochs et al. (2014); Xiao et al. (2015); Bo¸t et al. (2016); Liang
et al. (2016) or Bao et al. (2016) for a connection to wavelets.
This is of special interest for radio aperture synthesis as the
data ﬁdelity terms S amp, S cph and S cla are indeed not convex.
Local convergence to a steady point is known and under some
circumstances even global convergence could be proven (com-
pare the discussion in Liang et al. 2016). Application in practice
shows that, given a reasonable initial guess, local minima could
be avoided (Starck et al. 2015).

One may wonder, if the diﬃculty with convex non-smooth
penalty-functionals is now just transported to the probably trou-
blesome minimization problem in the deﬁnition of the proximity
operator in Eq. (22). But the proximity operator is known for a
large number of examples and the computation is often not more
time consumptive than one Landweber iteration. For example

Article number, page 5 of 20

A&A proofs: manuscript no. main

Table 1. Forward-Backward Splitting for the minimization of S + R

Input: S , R : X (cid:55)→ R (convex)
Input: grad S is L-Lipschitz continuous

Step size: τ ∈ (0, 2/L), typical choice: τ = 1/ (cid:107)grad(S )(cid:107)2
Initial guess: x0 ∈ X
while i = 0, 1, 2, ... do

xi+1 = proxτ,R(xi − τgradS (xi))

for the l0-functional the proximity operator is (e.g. Starck et al.
2015):

proxτ,(cid:107)·(cid:107)l0

(z) =





z
sign(z)[0, z]
0

√

|z| >
|z| = 0
√
|z| <

2τ

2τ

,

(25)

where signum and absolute value are meant to be evaluated
pointwise. This not always single valued since the l0 norm is not
convex. The proximal point operator of characteristic functions
is the projection on the support of the characteristic function: i.e.
in the case of the multiresolution support the function that nul-
liﬁes all coeﬃcients outside the multiresolution support, and (in
the case of the total ﬂux) the function that projects the current
guess to the guess with the correct total ﬂux.

3. Pipeline

We use the same notations as in the former subsections: V are the
observed visibilities, f the prior compact total ﬂux, Γ the dictio-
nary of composed of DoG wavelets, and F the linear mapping of
the image intensity to the tapered visibilities.

3.1. Outline

The core of our imaging method concerns solving the following
optimization problem:

ˆI ∈ argminI

(cid:104)
S cph(FΓI , V) + S cla(FΓI , V)
+α · Rl0(I ) + Rﬂux(I , f )(cid:3) ,

(26)

where we choose the maximum of the corresponding DoG
wavelet function as weights ωi. We have only one regulariza-
tion parameter α that controls the amount of suppression by hard
thresholding. We like to emphasize the main motivations behind
this optimization problem:

– We use the more ﬂexible DoG dictionary here, see Eq. (12).
This allows us to adapt the dictionary to the uv-coverage by
separating scales that are well covered by observations from
those are less accurate constrained by observations. This will
allows us to better suppress the signal from the latter one.
– We initially use the closure properties as data ﬁdelity term
as a measure to reduce the eﬀect of possible antenna-based
calibration errors. Chael et al. (2018) demonstrated that this
information is suﬃcient to recover the image when using
strong regularization priors. In later imaging rounds, i.e. after
several self-calibration steps, we are also starting to include
amplitude and phase information.

Article number, page 6 of 20

– We use hard thresholding (l0 pseudonorm regularization).
This promotes sparsity. In the few works addressing multi-
scalar imaging for radio aperture synthesis (Li et al. 2011;
Carrillo et al. 2012, 2014; Garsden et al. 2015; Onose et al.
2016; Mouri Sardarabadi et al. 2016; Onose et al. 2017; Prat-
ley et al. 2018) often the l1-norm is used as a convex approxi-
mation to Rl0 . This is standard for sparsity promoting inverse
problems (e.g. Starck et al. 2015). However, the l1-norm sup-
presses both, image features and noisy structures. As it is im-
portant to preserve the amplitude on the well covered scales,
we resort to using the non-convex l0-pseudonorm as penal-
ization. We weight the l0 pseudonorms by the maximal peak
of corresponding DoG wavelet basis function. This is done
to avoid that the scale selection would have a strong eﬀect
on the choice of the best regularization parameter. In princi-
ple these weighting parameters could be considered as free
regularization parameters as well. However, to meet our re-
quirement of constructing an algorithm that is as unbiased
and data-driven as possible, we restrict them in this work to
the choice that seems most reasonable.

– It should be noted that S cph, S cla and Rl0 are invariant against
rescaling the coeﬃcients x (atoms) by a scale factor λ ∈ R.
To select the most feasible solution along this line, we select
the one that matches the prior compact total ﬂux.

– There are more possible regularization terms available, for
example the total variation or the total squared variation
terms that are applied for the EHT imaging (Event Hori-
zon Telescope Collaboration et al. 2019). However, ﬁnding
suitable weighting parameters for the diﬀerent data terms
and penalty terms is somewhat unintuitive for such diﬀer-
ent types of regularizations. This task often requires large
parameter surveys with feasible synthetic data. We aim to
ﬁnd a largely unsupervised algorithm with only a few free
parameters.
Our optimization problem diﬀers signiﬁcantly from previous
multiscalar RML imaging approaches (e.g. Li et al. 2011; Car-
rillo et al. 2012, 2014; Garsden et al. 2015; Girard et al. 2015;
Onose et al. 2016; Mouri Sardarabadi et al. 2016; Onose et al.
2017; Cai et al. 2018a,b; Pratley et al. 2018). They used the star-
let transform as dictionary (replaced by DoG dictionary), the dis-
tance of observed and predicted visibilities as data ﬁdelity term
(replaced by closure properties), and l1 penalty terms (replaced
by l0 penalization).

Nevertheless, our algorithm shares some similarities with
RML reconstructions. The unpenalized minimization of the data
ﬁdelity terms would yield a high resolving reconstruction which
ﬁts the observed data points with a (too) high ﬁdelity, but pro-
vides clearly unphysical highly oscillating ﬁts of the visibilities
in the gaps of the uv-coverage. Total variation and total squared
variation penalization eﬀectively smooth the recovered model
to a reasonable extent, where the amount of smoothing is con-
trolled by the trade-oﬀ between data ﬁdelity term and penaliza-
tion term. We achieve a similar eﬀect by modeling the brightness
density distribution with (as few as possible) smooth, extended
basis functions.

3.2. Pipeline

The data ﬁdelity terms S cph, S cla and the regularization term
Rl0 are not convex. Hence, the minimization problem stated in
Eq. (26) strictly speaking may not have a single-valued mini-
mum. Therefore, a careful imaging pipeline helping global con-
vergence is needed. Note also that the representation of the im-
age in wavelet scales is an overcomplete representation. Due to

H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

the resulting large arrays, computation could be slow. Computa-
tion time can in principle be reduced when starting from a rea-
sonable initial guess instead of a ﬂat image or a Gaussian prior.
On the other hand, the solution of Eq. (26) returns an ade-
quate calibration image ˆI = Γ ˆx and computes, on the ﬂy, a mul-
tiresolution support M (all the pixels that are unequal to zero).
Further imaging rounds, including the self-calibrated visibilities,
allow the solution only to vary in the multiresolution support
and hence could sharpen the image further while respecting the
sparsity assumption due to the multiresolution support. This ap-
proach is realized within the following imaging pipeline:

1. Single Scalar ﬂux constraining imaging:

We minimize the term:

ˆI1 ∈ argminIS amp(FI, V) + S cph(FI, V)
+ S cla(FI, V) + Rﬂux(I, f ),

4. Multiresolution imaging with full visibilities:

After another self-calibration step, we solve the imaging
problem:

ˆI 4 ∈ argminI S vis(FΓI , V) + Rmrs(I , M),

(29)

by a gradient descent algorithm only varying coeﬃcients
in the multiresolution support analog to the third imaging
round.

5. Single Scalar visibility imaging: Finally, we set all pixels
with negative ﬂux to zero ﬂux and increase the match to the
observed visibilities by a gradient descent algorithm mini-
mizing S vis(FI, V) in the pixel scale starting from ˆI5 = Γˆ5.

The last three imaging rounds (in particular round 5) are optional
and only reﬁne the reconstruction. This will be discussed in our
demonstration on synthetic data in Sec. 4.

(27)

where no dictionary is involved. We do this by the fast min-
imization method available in the scipy package. In fact this
imaging round is similar to the ﬁrst imaging round with the
ehtim imaging package for Event Horizon Telescope Collab-
oration et al. (2019). This imaging round is used for ﬁnding a
reasonable initial guess in order to reduce the overall compu-
tation time. We convolve the result with the instrument clean
beam (to avoid local minima) and only use a few iterations,
i.e. an incomplete decomposition. Finally, we have to ﬁnd
some wavelet coeﬃcient array ˆI 1 that satisﬁes ˆI1 = Γ ˆI 1.
To satisfy Eq. (10), we copy the intensity ˆI1 in every scale
ˆI 1
j

= ˆI1, where j denotes the scale in use.

2. Multiscalar closure property Hard Thresholding imag-

ing:
This imaging round is the heart of the new algorithm. We
solve Eq. (26) by a forward-backward splitting approach.
ˆI 1 computed in the ﬁrst imag-
We start from the initial guess
ing round and compute a scale discrete guess in order to min-
imize Eq. (26). We start from the largest scales only (set all
other subbands to zero), successively add smaller scales and
larger thresholds. We stop at the scale at which the functional
(26) is minimal, i.e. at the smoothing when accuracy of the
ﬁt and sparsity penalization balance. Lastly, we reestimate
the thresholds for each scale individually starting from the
smallest scales.
We then minimize, starting from this initial guess, the func-
tional with a forward-backward splitting strategy. We will
explain this forward-backward splitting minimization strat-
egy in Sec. 3.3. An outline of the round 2 imaging algorithm
is presented in Tab. 2.

3. Multiresolution imaging with visibility amplitudes:

We self-calibrate the data with the image guess derived in the
second image round. Moreover, we compute the multireso-
ˆI 2 of the second imag-
lution support M from the result
ing round, i.e. we choose all non-zero elements of the mul-
tiscalar coeﬃcient array ˆI 2 as multiresolution support. We
now solve the problem:

ˆI 2 ∈ argminI S amp(FΓI , V) + S cph(FΓI , V)
+ S cla(FΓI , V) + Rmrs(I , M).

(28)

This is solved by a simple gradient descent algorithm starting
ˆI 3 in which only the gradient with re-
from the initial guess
spect to the coeﬃcient in the multiresolution support is com-
puted.

3.3. Minimization Algorithm

We now discuss the minimization algorithm used to minimize
Eq. (26). All other imaging rounds are based on smooth gradient
descent imaging algorithms (rounds 3-5) or a smooth Newton
type minimization (round 1). But Eq. (26) is neither convex nor
smooth. However, the data ﬁdelity terms are smooth with Lip-
schitz continuous derivatives and the l0 pseudonorm is proper,
lower-semicontinuous and satisﬁes the Kurdyka-Lojasiewicz
property (e.g. Liang et al. 2016). Thus, the Forward-Backward
Splitting algorithm 1 remains applicable, see our discussion in
Sec. 2.5. Additionally, recall that S cph, S cla and Rl0 are invariant
against rescaling the coeﬃcient array by a scalar factor λ. We
therefore propose the following iterative scheme:

We ﬁrst minimize S cph(FΓx, V) + S cla(FΓx, V) + α · Rl0 (x)
by a ﬁxed number of forward-backward splitting iterations, then
we rescale the coeﬃcient array by a scale factor such that I =
Γx has a total ﬂux matching the prior compact ﬂux (letting the
data ﬁdelity terms and regularization terms unaﬀected), then we
proceed with our forward-backward splitting algorithm, doing
rescaling again and so on. The complete procedure is outlined in
Tab. 2.

The needed proximal operator for the l0 pseudonorm is com-

puted in Eq. (25).

Iterative reweighted l1-regularization proposed by Candès
et al. (2007) provides an alternative approach to solve optimiza-
tion problems with non-convex l0-terms and is more common
than our forward backward scheme. However, our rescaling ap-
proach to match the total ﬂux would aﬀect the reweighting step
of the reweighted l1-regularization method. So, it would intro-
duce an additional layer of complexity in solving the optimiza-
tion problem. This would fail our requirement of a preferably
simple imaging algorithm with a small number of parameters to
specify.

3.4. Selection of scales

Our DoG-wavelet dictionary is ﬂexible in the sense that the
Gaussian widths could be chosen to adapt to the uv-coverage.
Hence, the selection of scales is data driven (e.g. by the uv-
coverage) and should be performed automatically. We discuss
in this section the automatic scale-width selection and outline
the key points of this approach.

The Fourier transform of a two-dimensional DoG wavelet
is a ring shaped mask, recall Eq. (8). It is reasonable to select
the masks such that well covered regions of the uv-space and

Article number, page 7 of 20

A&A proofs: manuscript no. main

Table 2. Wavelet Forward-Backward-Splitting: Pipeline round 2

Input: Visibilities: V
Input: Stepsize: τ (chosen artiﬁcially, such that algorithm converges)
Input: Regularization Parameter: α
Input: Total ﬂux: f

Deﬁne a dictionary of basis functions(wavelets): Γ
Deﬁne a forward operator: G : I (cid:55)→ F ΓI (note G is linear)
Deﬁne a data-ﬁdelity functional: d f : I (cid:55)→ S lca(V, GI ) + S cph(V, ΓI )
Precompute gradient of data-ﬁdelity functional: d f (cid:48)[I ]
Deﬁne a penalty term: pen : I (cid:55)→ |support(x)| (l0-norm)
Precompute proximal operator of penalty term: proxτ (hard thrinkage operator, Eq. (25))
I = initialguess

(cid:46) Precompute needed data terms and operators

(cid:46) Find initial image thresholding by minimizing Eq. (26) on a predeﬁned grid of thresholds

Deﬁne grid of possible thresholds: ti
for i = 1, 2, 3, ... do

Hard thresholding: testi = proxti(I )
mini = d f (testi) + αpen(testi)

Find minimum i and update initial guess I = proxti(I )
mintot = mini
for j = 0, 1, 2, ..., J do
for i = 1, 2, 3, ... do

Hard thresholding single scale: testi, j = {I1, ..., proxti(I j), ..., IJ}
mini, j = d f (testi, j) + αpen(testi, j)
if mini, j < mintot then
mintot = mini, j
I j = proxti(I j)

while stopping-rule 1 do

while stopping-rule 2 do
I = I − τ · d f (cid:48)[I ]
I = proxτ·α(I )

I = I · f /sum(ΓI )

Compute Multiresolution support M = {I (cid:44) 0}

Output: I is aprroximate minimizer to Eq. (26)
Output: ˆI = ΓI is an approximation to the true sky brightness distribution
Output: As a byproduct M is a reasonable multi-resolution support

(cid:46) Start forward-backward iterations from this guess

poorly covered regions are separated. However, for very sparse
arrays, there are no really well covered scales. In this situation,
our selection should be also driven by the assertion that all the
data points belonging to the same antenna pair should be covered
in one scale.

is given in Tab. 3. We also mention in Tab. 3 which scale is most
sensitive to which antenna pair, i.e. what was the selection crite-
rion to this scale. As all DoG-wavelets satisfy the zero integral
property of wavelets, then the only ﬂux-transporting scale is the
smoothing scale GσJ .

We present a sketch of our automatic scale selection in Fig.
1. We unpack the array of uv-distances of the full array, sort it in
increasing order (black dots in Fig. 1) and search for jumps be-
tween two consecutive data points that exceed a certain thresh-
old. These jumps clearly appear at gaps in the uv-coverage (most
visible between the blue and orange line in Fig. 1, respectively
between the green and the red line). We store the uv-distances at
which these gaps appear and select the DoG wavelet widths by
the mean of consecutive distances (represented by colored hori-
zontal lines in Fig. 1).

As a demonstration, we apply this procedure to the EHT
2017 array. In Fig. 2, we show our masks and the data points in
uv-space. The widths information of the scales shown in Fig. 2

Article number, page 8 of 20

The smallest scale in our set has a width of 9.96 µas which
corresponds to 5.02 pixels in our discretization. For the sake of
completing our dictionary of wavelet functions so that Eq. (10)
remains satisﬁed, we complete our sets of scales down to the
pixel size by adding DoG wavelets according to the widths of 1,
2 and 4 pixels. This, however, will turn out to be less relevant as
these scales will be suppressed by the algorithm automatically,
see Sec. 4.2.

H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

The ring has radius of 22 µas and a total ﬂux of 0.6 Jy. The

ring is convolved with a Gaussian with FWHM of 10 µas.

We simulate visibility data from the test images with the help
of the ehtim package, using the EHT 2017 array at 229 GHz.
We mimic the observation with the observe_same option assum-
ing the same systematic noise levels, observation intervals and
correlation times as for the EHT observations (Event Horizon
Telescope Collaboration et al. 2019). We assume phase and gain
calibration, but add thermal noise.

We aim to study the image on a 128x128 pixel grid with
1µas-pixels. However, to avoid boundary eﬀects in the com-
putation (the largest chosen Gaussian has a FWHM of already
93.75 µas), we widen the ﬁeld of view by a factor of two. More-
over, we use 129 pixels instead of 128 pixels to discretize narrow
central Gaussians correctly. We have deﬁned 12 diﬀerent wavelet
scales. Thus, we are attempting to solve for 12·129·129 ≈ 2·105
parameters in the multiscale imaging rounds.

4.2. Imaging Pipeline

In this subsection, we use with the crescent image to demon-
strate the stability of our imaging pipeline and present some key
features.

We show in Fig. 3 the imaging results obtained from the cres-
cent test data after diﬀerent imaging rounds. The image after the
second imaging round is shown in the upper right panel, the ﬁ-
nal image after the ﬁfth imaging round in the lower right panel.
The essential image structure is already recovered after the sec-
ond imaging round (multiscalar imaging with closure proper-
ties). This indicates that the multiscalar imaging approach might
also be applicable to badly calibrated data and that satisfactory
image quality could be achieved even without self-calibration
loops. Nevertheless, the use of the amplitudes and full visibil-
ity data (imaging rounds 3-5, lower panels) reﬁnes the recovered
structures and increases coincidence with observed visibilities.
Moreover, the steady improvement of the image quality shown in
Fig. 3 demonstrates that our amplitude conserving hard thresh-
olding approach works in the way intended. We observe a strong
contrast between the ring feature and the inner depression (due
to sparsity) while the amplitude and total ﬂux is conserved. This
would not be available with soft thresholding.

We demonstrate in Fig. 4 that our ﬁnal image ﬁts the ob-
served visibilities well. The hard thresholding approach sup-
presses emission that is not signiﬁcant for ﬁtting the visibilities,
but it does not break the ﬁt to the observed data as soft threshold-
ing would do. In fact, we successfully separated between signif-
icant image structures (ﬁtting the visibilities) and noise induced
features (very small sidelobe level in the ﬁnal image).

We present the multiscalar composition of the image in more
detail in Fig. 5. The panels of Fig. 5 suggest that diﬀerent scales
are sensitive to diﬀerent parts of the ﬁnal image, e.g. an extended
Gaussian component (bottom right panel), the ring feature with a
central negative peak to compensate for this extended emission
(e.g. middle panels) or the asymmetry of the crescent (bottom
left panel). The ﬁnal high resolution and high contrast image is
only visible when summing all the single scale images. Addi-
tionally, we present in Fig. 4 the ﬁt to the data from the sin-
gle scales only for some selected single scales, i.e. the ones that
have the largest signal according to Fig. 5. The various scales
are in Fourier domain mostly sensitive to varying parts of the
uv-coverage, from the short baselines (scales 9 and 11), over the
middle baselines (scale 6) to the longest PV-SMA/JCMT base-
lines (scale 4) as designed. Moreover, Fig. 5 demonstrates that
there are certain scales that are completely suppressed due to

Article number, page 9 of 20

Fig. 1. Sketch od automatic scale selection. The sorted array of uv-
distances is plotted with black points. This array has clearly visible
jumps (gaps in radial uv-coverage). We identify these jumps and assign
scalar widths to it (colored horizontal lines).

Scale σ1 → σ2 (µas)
Main Sensitivity
0.84 → 1.69
Unresolved
1.69 → 3.37
Unresolved
3.37 → 4.23
Unresolved
PV-SMA/JCMT
4.23 → 5.78
5.78 → 6.66
Gap
AA/AP-JCMT/SMA, AA/AP-PV
6.66 → 7.06
AA/AP-SMT, SMT-PV, LMT-PV
7.06 → 12.18
12.18 → 14.13 AA/AP-LMT, JCMT/SMA-LMT
14.13 → 17.55
17.55 → 33.69
33.69 → 39.81
39.81

JCMT/SMA-SMT
Gap
LMT-SMT
AA-AP, JCMT-SMA

0
1
2
3
4
5
6
7
8
9
10
11

Table 3. Widths of DoG wavelets and their main sensitivity to the uv-
coverage, i.e. which antenna pair is mainly covered by these scales. Dif-
ferenet scales are most sensitive either to speciﬁc baselines or the gaps
in the uv-coverage. The three smallest scales were added to complete the
dictionary down to the pixel size and compress unresolved structures.

4. Tests with synthetic data

4.1. Testdata

We test our algorithm on the same set of synthetic data that were
recently used for testing feature extraction from the EHT data
(Tiede et al. 2020). In particular, we use a crescent, a disk, a
double Gaussian, and a ring structure.

The crescent is described by the Equation (Tiede et al. 2020):

I(r, θ) = I0(1 − s cos(θ − ξ))

δ(r − r0)
2πr0
We use ξ = 180◦, r0 = 22 µas, s = 0.46 and I0 = 0.6 Jy. The
crescent is then convolved with a Gaussian with the full width at
half maximum (FWHM) of 10 µas.

(30)

.

The disk is a disk of diameter 70 µas. The disk is then con-

volved with a Gaussian with FWHM 10 µas.

The double Gaussian image consists of two Gaussian peaks
of FWHM 20 µas. The ﬁrst Gaussian is placed at the origin and
has a ﬂux of 0.27 Jy. The second Gaussian is placed 30 µas to the
East and 12 µas to the South. It has a ﬂux of 0.33 Jy.

A&A proofs: manuscript no. main

Fig. 2. Observed uv-coverage (black/red points) of the EHT data array (observation of M 87 on 5 April 2017) and masks deﬁned by the DoG
wavelets listed in Tab. 3 (color maps). The masks are the Fourier transform of the respective wavelets and they deﬁne ring-like ﬁlters in Fourier
domain. The visibilities highlighted by a speciﬁc ﬁlter are plotted in red

.

the sparsity promoting imaging pipeline (the smallest scales, top
panels). Consequently there is no signal at these scales in Fig.
5. This is reasonable as these scales are sensitive mainly to ﬁne
structures which could only be sampled at baselines longer than
the maximum baseline in the data. Moreover, it is noticeable
that the scale that is most sensitive to the longest baselines (PV-
SMA/JCMT, the fourth scale in Fig. 5) is completely suppressed.
That, however, does not necessarily mean these data points do
not aﬀect the reconstruction anymore. As can be seen in the ring-

Article number, page 10 of 20

like masks presented in Fig. 2, these data points in fact aﬀect all
other scales as well (as the Fourier masks are no steep Heaviside
functions), but with reduced importance. However, the suppres-
sion of this scale could be a hint that further improvement of the
method may be available by treating the weighting coeﬃcients
w j in Eq. (19) as free parameters.

H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

Fig. 3. Imaging results of the crescent at various steps of the imaging pipeline. Upper left: Gaussian prior image. Upper middle: Initial guess, result
after round 1 blurred by the 20 µas beam. Upper right: After imaging round 2. Bottom left: After Imaging round 3. Bottom middle: After imaging
round 4. Bottom right: Final image after imaging round 5.

4.3. Proof of concept

One of the principal ideas of this paper is to deﬁne a ﬂexible
wavelet dictionary which adapts smoothly to the uv-coverage.
We now prove this concept. We present in Fig. 6 a reconstruc-
tion with the complete pipeline with the selection of scales spec-
iﬁed in Tab. 3 and with a coarser grid that would be available
for instance with the less ﬂexible a-trou wavelet transform (right
panel): ˜Σ = [1, 4, 8, 32] (in units of 1.98 µas pixels). We used
only every second power of two here for demonstration purposes
to enhance the eﬀect of a less ﬁne grid of scales.

The crescent structure is much more robustly recovered with
our selection of scales. This is expected, as illustrated by Fig. 5.
The smaller scales respond to diﬀerent aspects of the ﬁne struc-
ture of the crescent test image, such as the ring like emission,
the narrow central ring line or the southern emission peak. The
larger scales compress the extended emission. The ﬁnal high res-
olution image is only visible by the sum of all these scales. The
artiﬁcial selection of scales ˜Σ has a less complex separation of
scales. The complex conglomerate of multiple structure features
has to be compressed in only one or two scales. Due to the coarse
gridding of widths in ˜Σ, the algorithm is forced to utilize to small
scales which are not able to compensate the bad ﬁtting of the un-
constrained minimization. Our automatic scale selection outper-
forms over this rigid choice of scales because of a more suitable
smoothing and thresholding due to adaptive steps in the scale

Article number, page 11 of 20

Fig. 4. Observed amplitudes (black) and recovered visibilities (yellow)
as function of uv-radius for the crescent test data. Moreover, we show
the ﬁt of single scales for some selected scales (blue, red, purple, green).

A&A proofs: manuscript no. main

Fig. 5. Multiresolution image after imaging round 4. Each panel shows the recovered images from one scale only. The scales are computed with
the DoG method with the widths deﬁned in Tab. 3. The images is shown for every scale in increasing order from the upper left to the lower right.

selection, and hence a more rigorous compression of structure
information.

4.4. Regularization Parameter

That said, it should be mentioned again that the wavelet
dictionaries are complete regardless of the selection of scales.
Hence, theoretically the same image can be represented by both
wavelet dictionaries regardless of the special choice of scales.
The dependency of the reconstruction on the selection of scales
is induced by the imaging pipeline (recall that the objective func-
tional is not convex and hence only convergence to a local min-
imum can be assured). It is easier to recover the image feature
at a speciﬁc scale, if this scale is well covered by measurements
which helps global convergence with our imaging pipeline. On
the other hand, a deconvolution at a less well covered scale is
more uncertain and possibly fails in the reconstruction of some
features.

One may ask now whether progressively reﬁning of the grid
of scales should further increase the accuracy of image restora-
tion. while it is principally expected, it also comes with the cost
of increased computation time and requires more complexity. In
this regard, our automatic scale selection may be viewed as a
viable optimum and data-driven approach.

Article number, page 12 of 20

Our algorithm depends on signiﬁcantly fewer critical parame-
ters that need to be speciﬁed by the user. The user only needs to
deﬁne the regularization parameter α controlling the size of the
penalty term, in contrast to the RML methods requiring mul-
tiple penalty terms (e.g. with MEM, l1, TV, TSV ... penalty
terms) balanced by the term weightings. All other parameters
in DoG-HiT are determined automatically from data: the widths
of the DoG-dictionary are deﬁned by the automatic procedure
described in Sec. 3.4 and the total ﬂux could be identiﬁed with
the zero-spacing ﬂux which can be measured or estimated. Pa-
rameters corresponding to the numerical minimization methods
(stepsize, number of iterations, relative tolerance) have only a
minor impact on the ﬁnal result as long as convergence is as-
sured. We present a more quantitative analysis of the impact of
the regularization parameter α on the reconstruction in Appendix
B. In a nutshell, if the regularization parameter is too small, the
visibilities are overﬁtted by a greedy model with a high back-
ground level. For higher regularization parameters, the penalty
term becomes more important: the background ﬂux level is de-
creased and the greedy, blobby model becomes more uniform.
The best ﬁt is achieved. On the other side, if the regularization

H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

Fig. 6. Reconstruction of the crescent image. Left panel: True image. Middle panel: Reconstruction with the selection of scales speciﬁed in Tab.
(3). Right panel: Reconstruction with scale widths that are a power of two (discrete wavelet transform).

parameter is chosen to big, the sparsity penalization dominates
the objective functional. The hard thresholding suppresses sig-
niﬁcant image information and the image is badly ﬁtted with a
small number of large wavelet scales.

5. Comparison to alternative imaging algorithms

We compare our image reconstruction with the image recon-
structions by standard Högbom CLEAN and the RML method
available in the ehtim software package. We utilize the weight-
ing of the data terms for RML reconstructions that was used for
Event Horizon Telescope Collaboration et al. (2019) and apply
their four-round imaging pipeline published in the EHT data re-
lease2. The CLEAN reconstructions are performed with the cir-
cular window available in the EHT data release 3 and are restored
with a 20 µas restoring beam. It is worth noting that the RML
scripts used for this imaging were extensively optimized for the
observations of M87 with the EHT, so excellent reconstructions
are expected for this comparison for RML. On the other hand,
in contrast to DoG-HiT, those excellent reconstructions required
many diﬀerent parameters to be speciﬁed.

5.1. Qualitative Comparison

We show in Fig. 7 our test image reconstructions on the set
of test data presented in Sec. 4.1. Our image reconstruction
shows a greater resolution than the CLEAN images. Moreover,
we achieve a greater contrast between image features and back-
ground noise levels than the CLEAN algorithm, i.e. sharper
edges in the recovered images.

Compared to the powerful RML imaging method, our al-
gorithm achieves comparable resolutions. This comes somehow
surprising as we probe the observed images with extended ba-
sis functions. In particular, we are able to recover some of the
ﬁne structure that is not visible in the RML reconstructions. We
ﬁnd the correct crescent-shaped North-South asymmetry in the
crescent image, the ﬁne ring ridgeline in the ring image and the
correct peak values in the double Gaussian image. Moreover, we

2 Available
eventhorizontelescope/2019-D01-02
3 https://github.com/eventhorizontelescope/
2019-D01-02

under

https://github.com/

ﬁnd a greater contrast between the ring-like features in the ring
and crescent images and the central depression, compared to that
observed in RML image. However, the inner ’no-emission’ ra-
dius is smaller than in the true images with DoG-HiT while the
spherical shape remains better recovered. This region is signiﬁ-
cantly better recovered by the RML algorithm. Moreover, RML
appears to perform better in resolving the ring and crescent fea-
tures transversely.

Notably, our algorithm also succeeds in the reconstruction
of smooth extended emission, e.g. of the disk image. The re-
construction of the disk is quite accurate and comparable to the
reconstruction with CLEAN. It does not manifest the greedy im-
age disk features or background emission present in the RML re-
construction. The ring image demonstrates that DoG-HiT is able
to ﬁt uniform emission (ring extension) and sharp features (ring
edges) simultaneously. The CLEAN reconstruction of the ring
lacks the proper reconstruction of the sharp ring edges and the
central depression. The RML reconstruction ﬁts the central de-
pression well, but the ring brightness distribution is less homoge-
neous than in the DoG-HiT reconstruction. In this way DoG-HiT
combines the major advantages of RML reconstructions (super-
resolving structures) and CLEAN (high dynamic range sensitiv-
ity to extended structures), at the same time also reducing the
drawbacks of either of these two methods. It should therefore
be well suited for imaging problems arising in the context of
EHT observations in which the demand on recovery of informa-
tion contained on smallest accessible scales requires simultane-
ous robust imaging of extended structures (jet). Performance of
Dog-HiT under these conditions will be discussed in in Sec. 6
using simulated data with a wide range of spatial scales.

5.2. Quantitative Comparison

We now compare the various imaging algorithms in a more quan-
titative way using a measure of the their relative error,

err =

(cid:13)(cid:13)(cid:13)recovered solution − exact image
(cid:13)(cid:13)(cid:13)exact image

(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)

.

(31)

We present the relative errors of the reconstructions in Tab. 4.
The comparison may be somewhat unfair for CLEAN given the
large beam size compared to the size of the structures, but a ﬁnal

Article number, page 13 of 20

A&A proofs: manuscript no. main

Fig. 7. Comparison of the reconstructions with various imaging algorithms. We show in the upper line the true images (crescent, disk, double,
ring). In the second to fourth line we present the image reconstructions with DoG-HiT, RML and CLEAN respectively.

convolution with a synthetic point spread function is the com-
mon standard in radio astronomy. We present the relative error
of the reconstruction both without blurring (as it is standard for
RML and DoG-HiT) and with blurring by 1/2 of the beam size
and the full beam size (as it is standard for CLEAN). The super-
resolving DoG-HiT reconstructions are getting worse with larger
restoring beam, while for CLEAN the opposite is true. DoG-HiT
tops the challenge for three of the four test images (crescent,
disk, ring) and performs similar to RML for narrow structures
(crescent, double). Overall, we can conclude that DoG-HiT is
able to achieve a similar precision as current imaging algorithms,

but alleviates some of the limitations of both CLEAN (no super-
resolution) and RML methods (sensitivity to smooth extended
features).

We present in Fig. 8 the residuals of the reconstructions of
the ring feature with RML and with DoG-HiT. The residuals for
both imaging methods are ring-shaped and spatially correlated,
indicating that there is still unrecovered structure. However, the
histograms of the residuals in the lower panels of Fig. 8 demon-
strate overall a very good reconstruction. The pixel residual dis-
tribution is well approximated by a narrow Gaussian distribution
in both cases. Nevertheless, the residual distribution for DoG-

Article number, page 14 of 20

H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

Blurring

0 µas

10 µas

20 µas

DoG-HiT
RML
CLEAN
DoG-HiT
RML
CLEAN
DoG-HiT
RML
CLEAN

Crescent Disk Double
0.167
0.138
0.164
0.266
1.427
1.282
0.191
0.144
0.234
0.219
0.658
0.568
0.402
0.203
0.443
0.275
0.556
0.156

0.156
0.16
1.121
0.219
0.238
0.294
0.414
0.433
0.399

Ring
0.139
0.211
1.082
0.215
0.245
0.285
0.411
0.441
0.396

Table 4. Relative errors of the reconstructions shown in Fig. 7.

HiT is slightly more narrow and less skewed, which agress well
with the overall slightly smaller relative error listed in Table 4.

5.3. Transverse Resolution

We study the transverse resolution of the algorithms with the
crescent image in this section. We present in Fig. 9 the proﬁles of
the true (blue) and the recovered crescent images in North-South
direction at central right ascension. We recover the correct dou-
ble peak structure with North-South asymmetry both with the
RML method and with DoG-HiT. CLEAN is not able to repro-
duce this ﬁne structure suﬃciently. Regarding transverse reso-
lution of the ring features and the central depression, RML and
DoG-HiT perform equally well, recovering approximately the
correct widths of the Gaussian blurred ring and the correct depth
of the central depression. However, DoG-HiT recovers a zero-
ﬂux central depression which is not captured in the true image.
We computed the blurring beam size that maximizes the correla-
tion between the (blurred) true image and the recovered images
to quantify the resolutions. The largest correlation between the
DoG-HiT reconstruction and the true image was achieved if the
true image is blurred by a beam with widths 6.1 µas. That means
that DoG-HiT was able to reproduce image features down to a
resolution of approximately 6 µas. For the RML reconstruction
we found a maximal correlation for a beam of 5.2 µas similar to
DoG-HiT. This resolution is expected due to the reverse taper
of 5 µas applied in the ehtim pipeline. For CLEAN we found a
widths of 19.7 µas coinciding well with the applied point spread
function of the array.

5.4. Simplicity and Performance

The ﬁve imaging steps presented in Sec. 3.2 may not appear as a
simple approach to imaging. However, one should recall that this
strategy resembles typical steps in the imaging of interferomet-
ric data with CLEAN: imaging in several loops of cleaning and
self calibration. Hence, our lengthy pipeline is not more complex
than automatic cleaning scripts.

More importantly, DoG-HiT only takes into account a very
limited number of regularization parameters, namely only the
prior guess for the total ﬂux and the biasing parameter α which
controls the weight of the hard thresholding regularization term
(see Eq. (26)). Apart from these parameters, only solver related
choices such as stepsize or relative tolerances need to be speci-
ﬁed. This is a ﬁrst step towards a more unsupervised imaging al-
gorithm in which crucial choices for the imaging procedure (i.e.
selection of window or regularization parameters) come directly
from data and are not manually selected doing the analysis.

The down side of this simplicity is that DoG-HiT is also less
ﬂexible compared to RML methods. RML methods combine dif-

ferent type of penalizations and prior assumptions that could be
relatively weighted according to speciﬁcations of a certain data
set. While it is a promising news that wavelet sparsity promot-
ing algorithms are similar or, in some settings, even better per-
forming than RML methods, this conclusion cannot be automati-
cally generalized. In particular, more advanced calibration issues
could add an additional layer of complexity to the problem. Fur-
thermore, the hard thresholding method used in DoG-HiT may
limit the dynamic range of the reconstruction, i.e. the minimal
ﬂux that can be recovered. A more rigorous study of this draw-
back should be made in subsequent works and applications.

Furthermore, it is a serious disadvantage of our algorithm
that it presently requires considerably more time and comput-
ing resources than the fast RML methods, because the image is
overcompletely represented by wavelet scales.

6. Physical source model

To demonstrate the performance of the algorithm on structures
covering a wider range of spatial scales, we present here a
DoG-HiT image reconstruction made from synthetic data from
the ﬁrst ngEHT Analysis Challenge 4, which emulate the black
hole shadow and the jet base in M 87 as observed with a pos-
sible ngEHT conﬁguration (Roelofs et. al. 2022). The ngEHT
is a planned, but not ﬁnally proposed future global VLBI ar-
ray designed to produce real time movies of the dynamics in
the extreme vicinity of a black hole and the innermost jet re-
gion (Doeleman et al. 2019). The ngEHT builds up on the enor-
mous success of the EHT and will extend the EHT science with
higher dynamic ranges, sensitivity and resolution. It is believed
to deliver novel groundbreaking results for the formation of jets,
accretion physics and general relativity tests (Doeleman et al.
2019). In particular the dense uv-coverage (including short base-
lines) and high sensitivity of the ngEHT as compared to the cur-
rent EHT allow for the reconstruction of the extended jet emis-
sion. The reconstruction and the true image are compared in
Fig. 10 in linear (left column) and logarithmic (middle and right
column) scales, with the latter employed for highlighting the ex-
tended emission. The simulated source structure is taken from a
MAD GRMHD simulation of a rapid spinning black hole sur-
rounded by an accretion disk with electron heating from recon-
nection (Roelofs et. al. 2022; Mizuno et al. 2021; Fromm et al.
2022). The simulated visibilities are calculated for a possible
template ngEHT conﬁguration at 230 GHz that is used through-
out the ngEHT Analysis Challenge (Roelofs et. al. 2022) and
might be realized in the ﬁnal concept of the array. It contains
the eleven current EHT sites (ALMA, APEX, GLT, IRAM-30
m, JCMT, KP, LMT, NOEMA, SMA, SMT, SPT) and ten ad-
ditional stations from the list of Raymond et al. (2021) (BAR,
OVRO, BAJA, NZ, SGO, CAT, GARS, HAY, CNI, GAM). HAY,
OVRO, and GAM are 37, 10.4, and 15 m antennas respectively.
All of the remaining additional antennas are assumed to be of
6 m in diameter. The synthetic visibilities are simulated with a
10 sec averaging time and with alternating 10 min observation
scans and 10 min gaps. The resulting uv-coverage is presented
in Fig. 11. For more information on the generation of the ground
truth image and the synthetic observation we refer the interested
reader to the description of the ngEHT Analysis Challenge avail-
able at the link listed in footnote 4.

The DoG-HiT reconstruction in Fig. 10 represents accurately
the central ring-like structure. This is expected, judging from the
successful reconstructions obtained in Sec. 5.1 on the compact

4 Available under https://challenge.ngeht.org/challenge1/

Article number, page 15 of 20

A&A proofs: manuscript no. main

Fig. 8. Residuals of the reconstructions with DoG-HiT (left) and RML (right) for the ring image. Upper panels: True image subtracted from the
reconstructed image. Lower panels: Histogram of the residual distribution.

crescent images from a much sparser synthetic observation. In
addition to this, Fig. 10 demonstrates that DoG-HiT also repro-
duces very well the extended emission from the jet base (middle
and right panels). The structural details of the jet base are com-
pressed on much larger scales than the smaller ring feature and
are less bright (hence only visible in logarithmic scale). This re-
sult demonstrates the ability of DoG-HiT to work on images with
a wide range of spatial scales and large dynamic range.

7. Conclusion

In this paper, we presented a novel interferometric imaging al-
gorithm which is capable of adapting to the Fourier domain cov-
erage of observations and particularly applicable to sparse uv-
coverages. Our imaging algorithm models the image as a sum
of diﬀerence of Gaussians wavelet functions. This wavelet dic-
tionary is more ﬂexible than the usual discrete a-trou wavelet
transform and allows us to select the scales to adapt to the uv-
coverage.

Article number, page 16 of 20

H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

cies (e.g. a limited dynamic range) into the reconstruction, and
this needs to be addressed in future works.

Software Availability

We will make our imaging pipeline and our software available
soon in a suitable way. Our software makes use of the pub-
licly available ehtim (Chael et al. 2018), regpy (Regpy 2019)
and WISE software packages (Mertens & Lobanov 2015).

Acknowledgements

We thank F. Roelofs, C. Fromm, L. Blackburn, G. Lindahl, A.
Raymond, S. Doeleman and the team of the ngEHT Analysis
Challenge for providing their data set and for useful discussions.
HM received ﬁnancial support for this research from the Inter-
national Max Planck Research School (IMPRS) for Astronomy
and Astrophysics at the Universities of Bonn and Cologne.

References

Akiyama, K., Ikeda, S., Pleau, M., et al. 2017a, AJ, 153, 159
Akiyama, K., Kuramochi, K., Ikeda, S., et al. 2017b, ApJ, 838, 1
Arras, P., Frank, P., Haim, P., et al. 2022, Nature Astronomy, 6, 259
Assirati, L., Silva, N. R., Berton, L., Lopes, A. A., & Bruno, O. M. 2014, Journal

of Physics: Conference Series, 490, 012020

Attouch, H., Bolte, J., & Svaiter, B. 2013, Mathematical Programming, 137, 91
Bao, C., Dong, B., Hou, L., et al. 2016, Inverse Problems, 32, 115004
Beck, A. & Teboulle, M. 2009, SIAM J. Imaging Sciences, 2, 183
Bhatnagar, S. & Cornwell, T. J. 2004, A&A, 426, 747
Blackburn, L., Pesce, D. W., Johnson, M. D., et al. 2020, ApJ, 894, 31
Bo¸t, R., Csetnek, E., & Szilard Csaba, L. 2016, EURO Journal on Computational

Optimization, 4, 3

Cai, X., Pereyra, M., & McEwen, J. D. 2018a, MNRAS, 480, 4154
Cai, X., Pereyra, M., & McEwen, J. D. 2018b, MNRAS, 480, 4170
Candès, E., Romberg, J., & Tao, T. 2006, IEEE Trans. Information Theory, 52,

489

Candès, E., Wakin, M., & Boyd, S. 2007, Journal of Fourier Analysis and Appli-

cations, 14, 877

Carrillo, R. E., McEwen, J. D., & Wiaux, Y. 2012, MNRAS, 426, 1223
Carrillo, R. E., McEwen, J. D., & Wiaux, Y. 2014, MNRAS, 439, 3591
Chael, A. A., Johnson, M. D., Bouman, K. L., et al. 2018, ApJ, 857, 23
Clark, B. G. 1980, A&A, 89, 377
Combettes, P. & Pesquet, J. 2009, Fixed-Point Algorithms for Inverse Problems

in Science and Engineering, 49

Cornwell, T. J. 2008, IEEE Journal of Selected Topics in Signal Processing, 2,

793

Coupinot, G., Hecquet, J., Auriere, M., & Futaully, R. 1992, A&A, 259, 701
Doeleman, S., Blackburn, L., Dexter, J., et al. 2019, in Bulletin of the American

Astronomical Society, Vol. 51, 256

Donoho, D. 2006, IEEE TRans. Information Theory, 52, 128
Event Horizon Telescope Collaboration, Akiyama, K., Alberdi, A., et al. 2019,

ApJ, 875, L4

Frieden, B. R. 1972, Journal of the Optical Society of America (1917-1983), 62,

511

Fromm, C. M., Cruz-Osorio, A., Mizuno, Y., et al. 2022, A&A, 660, A107
Garsden, H., Girard, J. N., Starck, J. L., et al. 2015, A&A, 575, A90
Girard, J. N., Garsden, H., Starck, J. L., et al. 2015, Journal of Instrumentation,

10, C08013

Gonzalez, R. & Woods, R. 2006, Digital Image Processing (3rd Edition)
Goupillaud, P., Grossmann, A., & Morlet, J. 1984, Geoexploration, 23, 85, seis-

mic Signal Analysis and Discrimination III

Grossmann, A., Kronland-Martinet, R., & Morlet, J. 1989, In Wavelets: Time-

Frequency Methods and Phase Space, -1, 2

Högbom, J. A. 1974, A&AS, 15, 417
Ikeda, S., Tazaki, F., Akiyama, K., Hada, K., & Honma, M. 2016, PASJ, 68, 45
Lannes, A., Anterrieu, E., & Marechal, P. 1997, A&AS, 123, 183
Li, F., Cornwell, T. J., & de Hoog, F. 2011, A&A, 528, A31
Liang, J., Fadili, J., & Peyré, G. 2016, in Advances in Neural Information Pro-
cessing Systems, ed. D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, & R. Gar-
nett, Vol. 29 (Curran Associates, Inc.), 4035–4043

Line, J. L. B., Mitchell, D. A., Pindor, B., et al. 2020, PASA, 37, e027
Mallat, S. G. 1989, IEEE Transactions on Pattern Analysis and Machine Intelli-

gence, 11, 674

Article number, page 17 of 20

Fig. 9. Proﬁles of the recovered crescent images in North-South direc-
tion and central right ascension.

We formulate the imaging problem as an optimization prob-
lem with an objective functional consisting of the reduced χ2
of the recovered closure properties (closure phase and logarith-
mic closure amplitudes) and an l0-pseudonorm sparsity term in
the wavelet domain. As this objective functional is still invari-
ant against rescaling of the image guess, we also add a total ﬂux
constraint. The resulting objective functional is non-smooth and
non-convex, but could be solved by an iterative hard thresholding
splitting algorithm for which local convergence to a steady point
is known. Due to non-convexity, global convergence cannot be
assured, but practice shows that local minima could be avoided
by proper initial guesses. Our algorithm is amplitude- and total
ﬂux-conserving, in contrast to schemes using soft thresholding.
Together with a more thorough separation of image features and
sidelobes by a ﬂexible wavelet dictionary analysis, this is ex-
pected to bring signiﬁcant improvements in imaging of VLBI
data with strongly varying and scale-dependent noise.

We present a complete imaging pipeline ready for appli-
cation. Our imaging pipeline consists of ﬁve imaging rounds,
where we reﬁne the initial imaging results from the closure prop-
erties in an iterative imaging/self-calibration loop which uses the
amplitude and phase information. We apply for the ﬁrst time a
multiresolution constraint for these reﬁnement steps. Moreover,
we prove stability of our pipeline in practice on synthetic data.

Comparisons of imaging performance on the synthetic data
show that DoG-HiT achieves super-resolution and outperforms
CLEAN in the reconstruction of ﬁne structure (super-resolving)
and that it comparable to RML methods in terms of accuracy
of reconstruction. DoG-HiT succeeds in the reconstruction of
smooth extended emission components, where it outperforms
RML. It eﬀectively combines the strengths of CLEAN and RML
methods and reduces their speciﬁc weaknesses. DoG-HiT should
therefore be well suited for application to targets with a wide
range of spatial scales, for which it may be outperforming cur-
rent RML reconstructions in the context of better recovering
smoother emission on large scales. We have demonstrated this
capability on a synthetic data set from the ﬁrst ngEHT chal-
lenge, with excellent reconstructions achieved for of both the
small scale inner ring-like structure and the faint, larger scale
emission from the jet base. It should also be noted that the DoG-
HiT reconstruction accurately reproduces features with a strong
contrast between emission and the background. At the same
time, DoG-HiT presently introduces some systematic inaccura-

A&A proofs: manuscript no. main

Fig. 10. Reconstruction of synthetic M87 observation with a possible ngEHT array taken from the ngEHT Analysis Challenge (Roelofs et. al.
2022). The true image is presented in the upper panels, the reconstruction with DoG-HiT in the lower panels. The left panels show the ground
truth and the recovered images in linear scale, the middle panels in logarithmic scale (i.e. highlighting the extended emission from the jet basis)
and the right panels compare the ground truth an the recovered image both smoothed with a restoring beam of 20µas.

Mertens, F. & Lobanov, A. 2015, A&A, 574, A67
Mizuno, Y., Fromm, C. M., Younsi, Z., et al. 2021, MNRAS, 506, 741
Moreau, J. 1962, Comptes Rendus Hebdomadaires des Séances de l’Académie

des Sciences, Paris, 255

Mouri Sardarabadi, A., Leshem, A., & van der Veen, A.-J. 2016, A&A, 588, A95
Murenzi, R. 1989, in Wavelets. Time-Frequency Methods and Phase Space, ed.

J.-M. Combes, A. Grossmann, & P. Tchamitchian, 239

Narayan, R. & Nityananda, R. 1986, ARA&A, 24, 127
Ochs, P., Chen, Y., Brox, T., & Pock, T. 2014, SIAM Journal on Imaging Sciences

[electronic only], 7, 1388–1419

Onose, A., Carrillo, R. E., Repetti, A., et al. 2016, MNRAS, 462, 4314
Onose, A., Dabbech, A., & Wiaux, Y. 2017, MNRAS, 469, 938
Pratley, L., McEwen, J. D., d’Avezac, M., et al. 2018, MNRAS, 473, 1038
Rau, U. & Cornwell, T. J. 2011, A&A, 532, A71
Raymond, A. W., Palumbo, D., Paine, S. N., et al. 2021, ApJS, 253, 5
Regpy. 2019, "regpy: Python tools for regularization methods", https://

github.com/regpy/regpy

Roelofs et. al. 2022, "The ngEHT Analysis Challenges", to be published in the

special ngEHT issue of Galaxies

Schwab, F. R. 1984, AJ, 89, 1076
Starck, J.-L., Bijaoui, A., Lopez, B., & Perrier, C. 1994, A&A, 283, 349
Starck, J. L. & Murtagh, F. 2006, Astronomical image and data analysis

(Springer)

Starck, J.-L., Murtagh, F., & Fadili, J. 2015, Sparse image and signal processing:
Wavelets and related geometric multiscale analysis, second edition, 1–423
Stollnitz, E., Derose, T., & Salesin, D. 1994, IEEE Computer Graphics and Ap-

plications, 15

Thiébaut, É. & Young, J. 2017, Journal of the Optical Society of America A, 34,

904

Thompson, A., Moran, J., & Swenson, G. 1994, Interferometry and Synthesis

inRadio Astronomy (Krieger Publishing Company)

Tiede, P., Broderick, A. E., & Palumbo, D. C. M. 2020, arXiv e-prints,

arXiv:2012.07889

Wakker, B. P. & Schwarz, U. J. 1988, A&A, 200, 312
Wiaux, Y., Jacques, L., Puy, G., Scaife, A. M. M., & Vandergheynst, P. 2009,

MNRAS, 395, 1733

Xiao, J., Ng, M. K.-P., & Yang, Y.-F. 2015, IEEE Transactions on Image Pro-

Fig. 11. uv-coverage of a synthetic ngEHT observation of M87 at
230 GHz. The uv coverage with the EHT 2017 antennas only is plot-
ted in red. For more details see Roelofs et. al. (2022).

Martinet, B. 1972, Comptes Rendus de l’Academie des Sciences de Paris, 1274,

cessing, 24, 1587

163

Article number, page 18 of 20

True ImageDoG-HiT0.0000.0050.010Jy / pixel−4−2log10(Jy / pixel)−4−2log10(Jy / pixel)H.Müller and A.P. Lobanov: DoG-HiT: A novel VLBI Multiscale Imaging Approach

0
0.345
0.202

α
crescent
disk

10−2
0.148
0.154
Table B.1. Relative error of the DoG-HiT reconstruction with varying
assumptions on the regularization parameter α.

10−1
0.148
0.137

10−3
0.17
0.164

100
0.169
0.138

101
0.254
0.231

Appendix A: Fixed point property of proximity

operators

Let ˆx ∈ argminsH(x) and let x ∈ X. Then it is:
(cid:107)s − ˆs(cid:107)X ≥ H( ˆs) + 1
H(x) + 1
2τ
2τ

(cid:107) ˆs − ˆs(cid:107)X,

(A.1)

as ˆs is in the argmin of H and τ ≥ 0. Vice versa, let ˆs be the
solution to ˆs = proxτ,H( ˆs), then it follows, see Eq. (23):
0 = ˆs − ˆs ∈ τ∂H[ ˆs],

(A.2)

which suﬃces to show for a convex, proper and lower semicon-
tinuous functional that ˆx is in the argmin of H.

Appendix B: Variation of regularization parameter

We discuss in this subsection the impact of the regularization pa-
rameter α. We show in Fig. B.1 the reconstruction of the crescent
image and of the disk image with varying regularization param-
eter α. The most left panels show the true image, the second left
panels the unconstrained reconstructions, e.g. α = 0. The middle
panels show from left to right the reconstruction results obtained
with increasing values of α. We present in Tab. B.1 the relative
precisions (31) of the diﬀerent reconstructions. The reconstruc-
tions are worse for too small α and too big α. The best ﬁt value
lies somewhere in between.

The reconstructions for very small α show a greedy and too-
ﬁne resolving model that diﬀers signiﬁcantly from the true im-
age. Moreover, fainter sidelobes and background emission is vis-
ible in the reconstruction. These models overﬁt the observed vis-
ibilities, i.e. the observed visibilities are ﬁtted exactly, but the
gaps in the uv-coverage are ﬁlled by high oscillating ﬁts.

For intermediate α the reconstruction is best. The uncon-
strained reconstruction is modeled with few (due to sparsity)
extended, smooth wavelet functions. This approach eﬀectively
smoothes the ﬁt to the visibilities and ﬁlls the gaps in the uv-
coverage with smooth ﬁts. In this spirit the sparsity approach in
the wavelet basis has a similar eﬀect on the data such as total
variation, total squared variation and maximum entropy penal-
izations. Moreover, sidelobes in the image are suppressed by the
hard thresholding.

On the other end of the table the reconstructions worsen
again for too large reconstruction parameters. In these cases the
penalty term dominates the objective functional in the forward-
backward minimization. The image is modelled with too few
wavelet scales. The result is a blurry reconstruction. Moreover,
the hard thresholding minimization procedure cuts signiﬁcant
fainter features, e.g. the northern emission in the crescent test
image (upper most right panel in Fig. B.1).

Article number, page 19 of 20

A&A proofs: manuscript no. main

Fig. B.1. Reconstructions with varying regularization parameter α. Most left panels: True images. Middle panels from left to right: α ∈
{0, 10−3, 10−2, 10−1, 100, 101}.

Article number, page 20 of 20

