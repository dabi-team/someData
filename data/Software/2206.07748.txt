Noname manuscript No.
(will be inserted by the editor)

Immersion Metrics for Virtual Reality

Matias N. Selzer* · Silvia M. Castro

2
2
0
2

n
u
J

5
1

]

C
H
.
s
c
[

1
v
8
4
7
7
0
.
6
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Technological advances in recent years have
promoted the development of virtual reality systems
that have a wide variety of hardware and software
characteristics, providing varying degrees of immersion.
Immersion is an objective property of the virtual reality
system that depends on both its hardware and software
characteristics. Virtual reality systems are currently
attempting to improve immersion as much as possible.
However, there is no metric to measure the level of
immersion of a virtual reality system based on its
characteristics. To date, the inﬂuence of these hardware
and software variables on immersion has only been
considered individually or in small groups. The way
these system variables simultaneously aﬀect immersion
has not been analyzed either. In this paper, we propose
immersion metrics for virtual reality systems based on
their hardware and software variables, as well as the
development process that led to their formulation. From
the conducted experiment and the obtained data, we
followed a methodology to ﬁnd immersion models based
on the variables of the system. The immersion metrics
presented in this work oﬀer a useful tool in the area
of virtual reality and immersive technologies, not only
to measure the immersion of any virtual reality system
but also to analyze the relationship and importance of
the variables of these systems.

Keywords Virtual Reality · Immersion · Presence

* Corresponding author.

Matias N. Selzer, and Silvia M. Castro
Engineering
Science
for Computer
Institute
(UNS–CONICET), and VyGLab Research Laboratory
(UNS-CICPBA), Department of Computer Science and
Engineering, Universidad Nacional del Sur, Bah´ıa Blanca,
Argentina.
E-mail: matias.selzer@cs.uns.edu.ar, smc@cs.uns.edu.ar

and

1 Introduction

Virtual Reality (VR) systems are sophisticated human -
computer interaction interfaces that are used today
in a wide variety of application areas
such as
education [1,2, 3,4], medicine [5,6] and training [7,
8, 9,10], among others. Some of the most popular
VR application areas today include entertainment
and video games [11,12,13] and each application has
diﬀerent objectives, requiring diﬀerent hardware and
software implementations.

For decades, it has been discussed in the literature
which variables of a VR system are related to
the perceived level of presence and immersion. The
inﬂuence of these hardware and software variables on
immersion has only been considered individually or
taking into account small groups of these. To date,
the way they all simultaneously aﬀect immersion has
not been analyzed. Also, the inﬂuence of all these
variables have not been compared with each other.
This motivates the generation of metrics developed to
calculate the perceived immersion of a VR system that
include all its variables with their respective levels of
incidence.

The main contribution of this work is the design and
development of immersion metrics that calculate the
level of immersion of a given VR system, based on its
hardware and software characteristics. This can be also
used to compare the immersion of diﬀerent commercial
or ad hoc VR systems. In addition, these metrics can be
considered an extremely useful design tool to measure
the immersion of prototypes, allowing to adjust, as
much as possible, the values of the variables involved
in the system. To achieve this goal, we followed the
methodology presented in section 3.

 
 
 
 
 
 
2

Matias N. Selzer*, Silvia M. Castro

2 Background and Related Work

2.1 Presence and Immersion

this reason, in a similar manner as the Bouchard test,
we used a single-item questionnaire for immersion.

[14],

Immersion is a relevant concept in VR that has
generated a lot of confusion regarding its similarity
to the concept of presence. The feeling of presence is
a subjective measure that depends on the sensation
and personal experience of each user. On the contrary,
according to Slater et al.
immersion refers to
an objective characteristic of a virtual environment
that is strongly linked to both hardware and software
components. According to this, the wider the sensory
bandwidth of a system, the more immersive the system
would be. For example, a system that includes 3D
spatial sound should be more immersive than a system
that does not include sound at all, or a system with a
ﬁeld-of-view (FOV) of 150º should be more immersive
than a system with a FOV of 100º. Also, in this context,
two diﬀerent users experiencing exactly the same VR
system should perceive the same level of immersion.
However, this is not as simple as just increasing the
variable values as much as possible. If we consider the
sound volume, for example, a higher value (i.e. a louder
volume) might not always produce a higher level of
immersion.

In this work, we study the relationship between vari-
ables and follow Slater’s deﬁnition of immersion, i.e.,
immersion is considered as an objective characteristic
of a VR system that can be measured.

2.2 Measuring Immersion

There are many questionnaires and surveys to measure
presence and immersion through causal factors and
diﬀerent variables. However, only a small number have
been validated and are used regularly. In 2004, Baren
and Ijsselsteijn [15] presented a complete list of existing
measurement methods, although today this list is out
of date.

One of the most used tools to measure presence and
immersion in virtual environments is the questionnaire.
Each type of questionnaire has its advantages and dis-
advantages. While questionnaires with many items can
provide a detailed assessment of multiple dimensions
of presence, single-item questionnaires, such as the
test presented by Bouchard et al. [16], allow a rapid
assessment and are less prone to memory impairment
after exposure to the experience. The Bouchard test has
been used successfully in previous works [17,18,19]. In
this work, since the user must perform as many trials as
possible (see section 4.1), we required a questionnaire
that was easy to understand and quick to complete. For

2.3 Variables Contributing to Immersion

The literature presents an extensive work related to
the variables that may contribute to a higher sense of
presence in VR. This relates to the characteristics of the
user and those of the system. The user characteristics
refer to the psychological and subjective characteristics
that inﬂuence the degree of perceived presence, and
those of the system refer to the technical characteristics
of the system that inﬂuence the perceived level of
immersion.

Previous works present several variables related to
the immersion and the visual features provided by
the system. These include the ﬁeld-of-view [20,21],
the screen resolution [20,22], the stereopsis [20,22],
the response time or latency [23], brightness, contrast,
saturation, and sharpness [24], the level of detail of the
3D models [25], the lighting of the virtual environment
[26], and the use of dynamic shadows [26]. Regarding
the variables related to audio, these include the use
of sound compared to not using sound [27, 28], the
ambient sound [29], the 3D spatial sound [30, 31], the
use of headphones compared to the use of speakers [31],
and the echo or reverberation [29]. Finally, regarding
the variables related to the user’s tactile system and
tracking, these include the sensory bandwidth [32], the
level of body tracking [33], the degrees of freedom [29],
the aﬀordance of the controls [34], the response time
or latency of the tracking [35], the locomotion mode
used to navigate through the virtual environment [36],
and the temperature and wind [30]. The supplementary
material presents the complete and extensive list of
variables studied for this work.

3 Methodology

In order to generate an immersion metric, we followed
a methodology that can be divided into two main parts
(see Fig. 1). The ﬁrst part deals with the analysis
of variables and the dataset population. It is very
common for some variables to be named diﬀerently in
diﬀerent studies. Hence, the ﬁrst step in this part of the
methodology is the study and classiﬁcation of all these
variables. The variables selected for the experiment are
presented in section 4.1.4.

Once the variables are selected, we required a
method to quantify the level of immersion produced
by a VR system,
for the diﬀerent values that the
variables can take. For this reason, we designed a user

Immersion Metrics for Virtual Reality

3

example, considered all the variables of the experiment
and others considered only the statistically signiﬁcant
variables. In Stage 2, feature selection techniques were
applied to reduce the number of variables of the
models. This process is explained in section 5.2. All the
candidate models (the models generated in Stage 1 and
Stage 2) went through a validation process in Stage 3
(see section 5.3).

Finally, the best models in terms of predictive
power, number of terms and coeﬃcients were selected as
immersion metrics. This is explained in detail in section
6.

4 Dataset Population and Analysis of Variables

4.1 User Study

We designed an experiment in which the participants
have to perform a certain task within a virtual envi-
ronment and report the perceived level of immersion.
That virtual environment is detailed in section 4.1.3
but its visual, auditory and tactile composition depends
on the values taken by the independent variables in
each trial. That is, each time a new trial is run, the
independent variables take a new value, modifying the
virtual environment completely (see section 4.1.5).

4.1.1 Participants

Initially, this experiment was intended to be performed
each one
by as many participants as possible,
performing as many trials as possible,
in order to
populate a dataset. In that scenario, with a reasonably
big dataset, more accurate analyses could be performed
related to the variables and the diﬀerences between
the participants’ characteristics. However, due to the
isolation related to COVID-19,
conditions of social
we decided to carry out the experiment focused on a
single user. This type of experiment is better known as
single-subject design and is often used in ﬁelds such as
psychology, education, and human behavior. It is also
often used to assess the eﬀect of a variety of applied
research interventions.

The present experiment was conducted by a
30-year-old male self-perceived gender participant.
The participant had experience playing video games
and using VR systems. Although the developed
methodology has been used in the context of single-
subject design,
it constitutes the basis for use with
multiple users (see section 8).

Fig. 1 Methodology followed in this work to generate
immersion metrics based on the hardware and software
characteristics of the VR system. The ﬁrst part deals with
the analysis of the variables and the population of a dataset
in a user experiment. The second part deals with the creation
of immersion models by using techniques of linear regression,
feature selection and validation. 1

study in which the user explores and interacts with a
virtual environment and reports the level of perceived
immersion. In each trial, this virtual environment is
generated based on the values taken by the variables
of the system. Hence, each trial contributes to a
new sample in a dataset that stores the relationship
between the VR system variables and the immersion
perceived by the user. After that, statistical analyses
were performed to this dataset to ﬁnd which variables
are statistically signiﬁcant. This process is detailed in
section 4.

The second part of this methodology deals with the
generation of immersion models, and is divided into
3 stages. In Stage 1, diﬀerent regression models for
immersion are generated, based on the dataset obtained
in Part 1 and the statistically signiﬁcant variables.
for
This is detailed in section 5.1. Some models,

4

4.1.2 Hardware

The experiment was conducted using a desktop
computer with an i5-7500 3.40GHz CPU, with 16GB
of RAM, and a GeForce GTX 1060 6GB GPU video
card. There was no performance degradation that could
have compromised the experience. Visual stimulation
and interactions were carried out using the Oculus
Rift CV1 1 system. The binocular ﬁeld-of-view of the
system is approximately 110º. Its display has a 60Hz
refresh rate and a resolution of 2160 × 1200 for both
eyes. Head orientation and position are recorded by the
system’s integrated gyroscope and accelerometer. The
optical cameras of the system were used to track the
participant. The system also has a mechanism to adjust
the participant’s visual disparity. Finally, the system’s
integrated headphones were used to deliver the audio.

4.1.3 Virtual Environment

According to Makransky et al.
[37], to obtain the
most accurate measurement possible, the participant
must remain entertained and motivated throughout the
experience. If the participant becomes bored and begins
to ramble, this can negatively inﬂuence the accuracy
of the results. For this reason, we designed a game to
keep the participant motivated during the test. In this
game, the participant must survive a zombie attack for
a certain period of time. To keep the user motivated,
the diﬃculty of the scenario varies depending on the
remaining playing time. That is, the frequency with
which new enemies appear and their speed increase as
time goes by.

The participant is located at the center of a
crossroad between two corridors. The enemies appear
at the end of those corridors and start walking towards
the participant, who can only walk through a delimited
(virtual) zone of 3m × 3m (ﬁgure 2). If the enemies get
too close to the participant, the game ends.

To evaluate the participant’s movements, diﬀerent
obstacles were placed to obstruct the vision between
the participant and the enemies. Hence, the participant
needs to move to shoot the enemies.

The participant has a gun in each hand to shoot the
enemies (ﬁgure 3). The bullets are unlimited. The right
side of the guns shows the remaining time and the left
side the locomotion mode.

The delivered audio includes other

in
addition to the ambient background sound. When
the participant shoots, a shooting sound is generated
from the gun. In addition, the enemies produce three
diﬀerent sounds: a sound when they appear at the end

sounds

1 https://www.oculus.com/rift

Matias N. Selzer*, Silvia M. Castro

of a corridor, another sound when they are close to the
participant, and another sound when they die.

4.1.4 Independent and Dependent Variables

The independent variables are those established by
the system in each test and do not depend on other
variables. The variables considered for the experiment
are listed in table 1.

On the other hand, the dependent variables are
those that depend on the independent variables. These
variables are rated, on a scale from 1 to 100, with
a questionnaire at the end of each test. As in the
Bouchard questionnaire [38], we measured immersion
with a speciﬁc question: “How much did you feel
immersed in the experience? i.e., how much did you
feel that you SAW, HEARD and NAVIGATED like you
do in real life?”. The participant was given a thorough
explanation on the question and also the opportunity
to ask questions.

4.1.5 Procedure

Each time a new trial begins, the characteristics of
the scene related to all the independent variables are
modiﬁed. For numerical variables, a random real value
is computed within the established range and,
for
categorical variables, a random integer value associated
with one of the categories of that variable is computed.
The virtual scenario is then generated based on these
variables and their computed values. Hence, for each
trial, the participant would perceive a completely
diﬀerent experience. Figure 4 shows two examples of
diﬀerent dynamically generated virtual scenes. The
supplementary material presents examples of the eﬀect
of the other variables on the virtual scene.

Each trial ends either when the participant survives
for the speciﬁed time or when an enemy gets close
enough. Following the principles proposed by Slater
et al. [39], it is important to take measurements as
soon as possible after the experience. Immediately after
the trial ends, the enemies that are still in the scene
disappear and a ﬂoating screen appears for the user
to answer the question related to the perceived total
immersion.
Finally,

the
participant took a 5-minute break between trials. No
noticeable symptoms of cybersickness occurred at any
time.

to mention that

important

is

it

4.1.6 Results

The data from the experiment was saved into a dataset
for later analysis. This dataset is represented by a

Immersion Metrics for Virtual Reality

5

Fig. 2 Top aerial view of the virtual environment (left) and a close-up view (right). The user is at the center. Enemies emerge
from the 4 corridors’ ends and walk towards the center.

Fig. 3 The left side of each gun shows the the walking mode (left). The right side of each gun shows the remaining playing
time (right).

Table 1 Independent variables considered in this study. These variables are arranged in categories, namely: Trial Variables,
Visual Conﬁguration Variables, Audio Conﬁguration Variables and Locomotion Conﬁguration Variables. For each variable, a
brief description is presented.

Category
Trial
Variables

Variable Name

Description

Duration Time

from 120 to 1200 seconds (2 to 20 minutes)

Screen Resolution (Width and Height)
Field-of-View (FOV)
Frame Rate (FPS)
Stereopsis
Antialiasing (MSAA)
Textures
Illumination
Saturation
Brightness
Contrast
Sharpness
Shadows
Reﬂections
3D Models Detail
Depth-of-Field
Particles
Sound System
Ambient Sound
Reverberation
3D Spatial Sound

from 0.1 to 1.0 multiplied by the device max resolution (2160x1200 for the Oculus Rift CV1)
from 30% to 100% of the device max FOV
from 8 to 60 FPS
Enabled or Disabled
Enabled or Disabled
Enabled or Disabled
Ambient Light with No Shading, or Point Lights with Realistic Shading
from -1.0 (no saturation at all) to 1.0 (extremely saturated image)
from -0.8 to 0.8. Higher or lower values create completely dark or white scenes
from -0.8 to 0.8
from 0.0 to 1.0
Shadow Strength from 0.0 to 1.0
(Specular Coeﬃcient of Materials) Enabled or Disabled
Low-Poly Models or High-Poly Models
Enabled or Disabled
Enabled or Disabled
No Sound, Speakers, or Headphones
Enabled or Disabled
Enabled or Disabled
Enabled or Disabled

Locomotion Mode

Real Walking, Teleportation, Joystick Movement, or Walking-in-Place (WIP)

Visual
Conﬁguration

Audio
Conﬁguration

Locomotion
Conﬁguration

6

Matias N. Selzer*, Silvia M. Castro

Fig. 4 Scene with a low value of 3D models detail, no textures and a large ﬁeld-of-view (left). Scene with a high value of 3D
models detail, textures activated, and a narrow ﬁeld-of-view (right).

table, where each row corresponds to a sample and
each column to a variable. The data collected during
each trial constitutes a sample in this dataset. For this
experiment, the participant performed 401 successful
trials, thus generating 401 rows in the dataset. The
dataset is public and available online [40].

4.2 Statistical Analysis

Based on the obtained dataset, we performed statistical
analyses to evaluate the relationship between the
diﬀerent variables and the perceived immersion. We
present the most relevant results of the analyses relating
total immersion.

We performed Kolmogorov-Smirnov tests for nor-
mality, which showed that the data did not follow
a normal distribution. For this reason, we used non-
parametric tests for statistical analysis, i.e., we em-
ployed non-parametric Kruskal-Wallis tests to evaluate
the statistical diﬀerences of the independent variables
on immersion. We used Dunn’s pairwise comparison
with Bonferroni correction to identify where the
diﬀerences occurred. In all these cases, a conﬁdence
interval of 95% was considered. Finally, correlation
analyses were performed to study possible relationships
between the independent variables and the perceived
immersion. We used Spearman correlations for ordinal
for continuous
variables and Pearson correlations
variables.

Considering the visual variables with respect to
total immersion, a small correlation was found with
screen width (r(401) = 0.276, p < 0.01), frames per
second (r(401) = 0.148, p < 0.01) (ﬁgure 5 left), and
contrast (r(401) = 0.125, p = 0.012). Also, a signiﬁcant
diﬀerence was found between using textures and not
using textures ( χ2 = 65.017, p < 0.01) (ﬁgure 5 right).
For the audio variables, a statistically signiﬁcant
diﬀerence was found between the diﬀerent audio output

modes (χ2 = 8.222, p = 0.02). According to Dunn’s
test, this diﬀerence is found between the group with no
sound and the group with headphones (ﬁgure 6 left).

Regarding the relationship with the locomotion
variables, a statistically signiﬁcant diﬀerence was found
between the navigation modes (χ2 = 28, 074, p < 0.01)
(ﬁgure 6 right). Subsequent analysis with Dunn’s test
revealed that the diﬀerence occurs between all groups.

5 Generation of Models

In this work, we carried out a process to ﬁnd the
best regression models for immersion based on the 22
independent variables of the experiment. This process,
organized in 3 stages, is described below.

5.1 Stage 1: Direct Models

In the ﬁrst stage, we ﬁtted regression models using
the variables from the experiment. Five models were
generated, detailed in the subsections below and
summarized in table 2. Each model is represented by
a Total Immersion (TI) function, being n the number
of variables (x) and m the number of coeﬃcients (β).

5.1.1 Simple Linear Model

This model consists of a linear regression between total
immersion and the 22 independent variables. From
these results, we can generate a function of the form:

T I =β0 + β1 × x1 + β2 × x2 + β3 × x3 + ... + βm × xn

(1)

5.1.2 Simple Model with Interactions

Since the interaction between variables can aﬀect
the ﬁnal result, this multivariate model considers the
interactions between each pair of independent variables.

Immersion Metrics for Virtual Reality

7

Fig. 5 Relationship between frames per second and total immersion (r(401) = 0.148, p < 0.01) (left). Boxplot for the
relationship between the use of textures and total immersion (χ2 = 65.017, p < 0.01) (right).

Fig. 6 Boxplot for the relationship between audio output and total immersion (χ2 = 8.222, p = 0.02) (left), and between
navigation mode and total immersion (χ2 = 28.074, p < 0.01) (right).

Then, the function corresponding to this model has the
following form:

5.1.5 Manual Model

T I =β0 + β1 × x1 + β2 × x2 + β3 × x1 × x2 + ...+

βm × xn−1 × xn

(2)

5.1.3 Complete Model without Interactions

In this case, we included all the variables and also the
variables of order 2. The interaction between variables
is not considered. The function corresponding to this
model has the following form:

T I =β0 + β1 × x1 + β2 × x2

1 + β3 × x2 + β4 × x2

2 + ...+

βm × x2
n

5.1.4 Complete Model

(3)

The Complete Model
is the model that, in addition
includes both the
the variables,
to including all
interactions between each pair of independent variables,
as well as
the order 2 variables. The function
corresponding to this model has the following form:
T I =β0 + β1 × x1 + β2 × x2

1 + β3 × x2 + β4 × x2

2+

β5 × x1 × x2 + ... + βm−1 × x2

n + βm × xn−1 × xn

(4)

Generally, researchers rely on theory and experience to
decide which candidate variables should be included
in a regression model. In this sense, some techniques
recommend that the set of predictor variables included
in the ﬁnal regression model be based on a priori data
analysis.

In section 4.2, we analyzed the statistical relation-
ship between each of the independent variables and
the total immersion. Hence, we propose another model
that considers only the variables that aﬀected the total
immersion in a statistically signiﬁcant way. For this
model, we included the variables screen width, frames
per second, contrast, duration time, textures, audio
output, and navigation mode. For these variables, we
have also included in the model the order 2 variables
and the interactions between each pair of independent
variables.

5.2 Stage 2: Feature Selection

Feature selection techniques help to identify a more
condensed set of variables that feed the model
in
a meaningful way. These techniques iteratively add
or remove potential variables, testing for statistical
signiﬁcance after each iteration.

8

Matias N. Selzer*, Silvia M. Castro

Table 2 Stage 1 Models Comparison.

Model Name
Simple Linear
Simple with Interaction
Complete without Interaction
Complete
Manual

R2Adjusted AIC Predictors Coeﬃcients

0.4121
0.5647
0.4423
0.5999
0.4182

3303
3208
3288
3155
3289

22
22
22
22
7

25
299
34
308
14

includes

The Complete Model presented in the previous
section has 308 coeﬃcients and it
the
combination of all the studied variables. We performed
feature selection to this model as a way to reduce the
number of variables, therefore the complexity of the
model. However, decreasing the number of variables of
the model can negatively aﬀect the model’s predictive
power.

The literature presents many feature selection
techniques. We used Stepwise Regression because this
technique provides the ability to manage large amounts
of potential predictor variables, and ﬁne-tuning the
model to choose the best predictor variables from the
available options. The Stepwise Regression technique
allows us to establish a target p-value. The smaller the
p-value, the smaller the number of variables that will
fulﬁll that value, thus obtaining a smaller model. Hence,
we deﬁned 4 groups based on 4 diﬀerent target p-values:
Model A (p = 0.05), Model B (p = 0.01), Model C (p
= 0.005), and Model D (p = 0.001). Finally, for each
of these 4 groups, 3 models were generated: one with
Forward Selection, another with Backward Selection,
and another with Stepwise Selection. Therefore, 12 new
models were obtained, presented in table 3.

All the models that used the Forward Selection
technique resulted to be equal to the Complete Model
from stage 1. This indicates that the algorithm did
not stop until all variables were included. On the other
hand, both the A Backward model and the A Stepwise
model, as well as the D Backward model and the D
Stepwise model, are also equal to each other. Two
models are equal when they have the same coeﬃcients,
predictors and prediction values.

5.3 Stage 3: Validation

We validated all the models created (i.e. the ones from
stage 1 and stage 2) using cross-validation with k
iterations with repetition. Ten repetitions were used.
In summary, the k iteration cross-validation procedure
with k = 10 divides the dataset into 10 subsets. It
uses 9 of these 10 subsets to train the model and the
remainder one to test it. Thus, a prediction error is
obtained. This process is repeated for all the 10 subsets,

and the total prediction error is the average of the 10
individual errors.

In this case, we also use repetition, that is, the entire
process described above is carried out 10 times. Hence,
the ﬁnal prediction error is the result of averaging the
10 runs. This is done for each model, thus obtaining the
values of RMSE, R2 adjusted, and MAE.

All the models are arranged in table 4, ordered
according to the number of coeﬃcients. This table
groups the models that are equal. As mentioned before,
the best prediction can be deﬁned by the highest
adjusted R2 or the lowest RMSE or MAE values. In
this work, we follow the value of R2 to decide which
model is “better” in terms of predictive power.

6 Immersion Metrics: Selected Models and
Functions

From among the obtained models, our goal was to
ﬁnd the one (or ones) that were most closely related
to the intended use of the model. A model with a
high predictive power would provide a better immersion
approximation based on the variables of the VR system.
A model with fewer predictors requires fewer variables
of the VR system. A model with fewer coeﬃcients
can be computed faster. Hence, when selecting the
best models, we need to consider the trade-oﬀ between
predictive power, number of coeﬃcients, and number of
predictors.

Of the resulting models presented in table 4, the
model with the best predictive power, based on R2, is
the A Backward or A Stepwise model, both with 177
coeﬃcients and R2 = 0.5973. The table also presents
models with a similar R2 and with fewer coeﬃcients.
Therefore,
in the search of the best models, we
discarded the model with 177 coeﬃcient and analyzed
in detail the models with 42, 40, and 39 coeﬃcients that
have, respectively, an R2 equal to 0.5542, 0.5297, and
0.5314.

Regarding the B Stepwise, B Backward and C
Stepwise models, none of them include the variables
reﬂections, reverberation and 3D spatial sound. The
B Backward model also does not include the variable
saturation. It is interesting that the 3D spatial sound,

Immersion Metrics for Virtual Reality

Table 3 Stage 2 Models Comparison.

9

Model Name R2Adjusted AIC Predictors Coeﬃcients
3155
A Forward
3040
A Backward
3040
A Stepwise
3155
B Forward
3191
B Backward
3172
B Stepwise
3155
C Forward
3244
C Backward
3187
C Stepwise
3155
D Forward
3277
D Backward
3277
D Stepwise

0.5999
0.7604
0.7604
0.5999
0.5704
0.5925
0.5999
0.492
0.5741
0.5999
0.4362
0.4362

308
177
177
308
40
42
308
24
39
308
15
15

22
22
22
22
19
18
22
13
18
22
9
9

Table 4 Comparison and Validation of all models. The grouped models are exactly the same. The Coeﬃcients column is
highlighted to emphasize that the coeﬃcients are sorted from highest to lowest.

Model Name
Complete
A Forward
B Forward
C Forward
D Forward
Simple with
Interactions
A Backward
A Stepwise
B Stepwise
B Backward
C Stepwise
Complete without
Interactions
Simple Linear
C Backward
D Backward
D Stepwise
Manual

Model Information

Validation Information

R2Adjusted AIC Predictors Coeﬃcients RMSE R2Adjusted MAE

0.5999

3155

0.5647

3208

0.7604

0.5925
0.5704
0.5741

0.4423

0.412
0.492

0.4362

0.4182

3040

3172
3191
3187

3290

3303
3244

3277

3289

22

22

22

18
19
18

22

22
13

9

7

308

32.18

0.1403

25.47

299

177

42
40
39

34

25
24

15

14

30.56

0.1393

24.23

12.71

12.62
12.99
12.94

14.71

14.86
13.78

14.36

14.55

0.5973

0.5542
0.5297
0.5314

0.4037

0.3869
0.4691

0.4235

0.4091

10.23

10.29
10.59
10.68

12.01

12.16
11.36

11.75

11.94

Table 5 Selected Immersion Models.

Model Information
Model Name Coeﬃcients Predictors

Model 1
Model 2
Model 3

42
24
15

18
13
9

Validation Information
R2Adjusted
0.5542
0.4691
0.4235

which according to the literature is a variable widely
inﬂuential, was not considered by these models. On the
other hand, some models did not consider the variables
reﬂections, reverberation and saturation. Taking this
into account, we consider that the B Stepwise model,
with 42 coeﬃcients, is the best of these three models
since, although it has more coeﬃcients, it has greater
predictive power.

Then, the Complete Model without Interaction,
with 34 coeﬃcients, and the Simple Linear Model, with
25 coeﬃcients, were discarded. Both include all the 22
variables, and their predictive power is lower than the

other models.

We consider that the C Backward Model, with
24 coeﬃcients, is also one of the best models since
it includes 13 of the 22 variables, even though its
predictive power is lower than the other models
the 22
with more coeﬃcients and variables. Of
variables, this model does not include the stereopsis,
antialiasing,
illumination mode, saturation, shadow
strength, reﬂections, depth of ﬁeld, reverberation or 3D
spatial sound. We consider that the variables stereopsis,
lighting mode, and 3D spatial sound are relevant since,
according to the literature, they have a signiﬁcant

10

Matias N. Selzer*, Silvia M. Castro

inﬂuence on immersion and presence.

predictive power of each model.

TotalImmersion = −52.795864+
screenWidth × 0.023127+
ﬁeldOfView × 0.233013+
framesPerSecond × 1.524708+
stereopsisActivated × −3.741471+
antialiasingActivated × −14.463670+
textureModeWithTextures × −3.558318+
illuminationModeLightsAndShading × 1.792554+
brightness × −5.050710+
contrast × 15.784897+
sharpness × 26.061258+
shadowStrength × 14.743503+
modelsDetailHigh × 1.604808+
depthOfFieldActivated × 7.430725+
particlesActivated × 4.942608+
audioOutputModeSpeakers × 1.870371+
audioOutputModeHeadphones × 11.826967+
ambientSoundActivated × 11.696136+
locomotionModeJoystick × 18.260037+
locomotionModeWalkInPlace × 2.375830+
durationTime × 0.002978+
screenWidth2 × −0.000008+
framesPerSecond2 × −0.017609+
screenWidth × illuminationModeLightsAndShading × 0.007121+
ﬁeldOfView × contrast × −0.227759+
ﬁeldOfView × sharpness × −0.431735+
stereopsisActivated × shadowStrength × −13.691010+
stereopsisActivated × durationTime × 0.015409+
antialiasingActivated × durationTime × 0.017004+
textureModeWithTextures × contrast × 8.744551+
textureModeWithTextures × modelsDetailHigh × 10.788037+
textureModeWithTextures × durationTime × 0.019434+
shadowStrength × particlesActivated × −14.702403+
ambientSoundActivated × durationTime × −0.017771+
locomotionModeJoystick × durationTime × −0.014104+
locomotionModeWalkInPlace × durationTime × 0.016911+
modelsDetailHigh × depthOfFieldActivated × −8.055026+
particlesActivated × ambientSoundActivated × 6.549401+
antialiasingActivated × audioOutputModeSpeakers × 7.244033+
antialiasingActivated × audioOutputModeHeadphones × −2.993224+
antialiasingActivated × illuminationModeLightsAndShading×
− 6.452346+
textureModeWithTextures × illuminationModeLightsAndShading×
− 7.164925

(5)

Finally, the models with 15 and 14 coeﬃcient are
very similar in terms of predictive power, number of
coeﬃcients, and number of predictors. The model with
15 coeﬃcients includes 3 variables that the model
with 14 coeﬃcients does not. These are ﬁeld-of-view,
deﬁnition and models detail. On the other hand, the
model with 14 coeﬃcients includes a variable that the
model with 15 coeﬃcients does not, which is contrast.
According to the statistical analysis, the variable
contrast inﬂuences the total immersion, although very
slightly. However, according to the literature, the
variables ﬁeld-of-view and detail of the models are more
signiﬁcant and inﬂuential than contrast. For this reason,
we selected the model with 15 coeﬃcients instead of the
one with 14.

After this process, three models were selected, which
are presented in table 5. For clarity, we will call these
models “Model 1”, “Model 2”, and “Model 3”. The
table details the number of coeﬃcients, the number
of predictors and the adjusted R2,
indicating the

The functions for Model 1, Model 2 and Model 3
are presented in equations 5, 6 and 7, respectively.
These functions can be used to estimate the level of
immersion of a given VR system based on its hardware
and software features.

Total Immersion = −38.16095974+
screenW idth × 0.008504384+
f ieldOf V iew × 0.196812152+
f ramesP erSecond × 1.541130003+
textureM odeW ithT extures × −5.46407892+
brightness × −4.085710982+
contrast × 19.38644806+
sharpness × 23.33455116+
modelsDetailHigh × −1.670228672+
particlesActivated × −3.069366777+
audioOutputM odeSpeakers × 5.297975701+
audioOutputM odeHeadphones × 10.45405873+
ambientSoundActivated × −2.809009675+
locomotionM odeJoystick × 18.95378116+
ocomotionM odeW alkInP lace × 1.551341478+
durationT ime × 0.012916461+
f ramesP erSecond2 × −0.018563525+
f ieldOf V iew × contrast × −0.222598953+
f ieldOf V iew × sharpness × −0.38192204+
textureM odeW ithT extures × modelsDetailHigh × 8.765077901+
textureM odeW ithT extures × durationT ime × 0.019749586+
particlesActivated × ambientSoundActivated × 7.579476366+
locomotionM odeJoystick × durationT ime × −0.015178069+
locomotionM odeW alkInP lace × durationT ime × 0.015704389

(6)

Total Immersion = −44.78322466+
screenW idth × 0.008237546+
f ieldOf V iew × 0.227429898+
f ramesP erSecond × 1.608568062+
textureM odeW ithT extures × 9.717910348+
sharpness × 26.4102586+
modelsDetailHigh × −3.064720396+
audioOutputM odeSpeakers × 5.519798682+
audioOutputM odeHeadphones × 10.19128742+
locomotionM odeJoystick × 5.692683516+
locomotionM odeW alkInP lace × 13.37931564+
durationT ime × 0.017860572+
f ramesP erSecond2 × −0.019223957+
f ieldOf V iew × sharpness × −0.431998513+
textureM odeW ithT extures × modelsDetailHigh × 9.859037709
(7)

7 Immersion in Commercial Devices

We tested the immersion metrics on three of today’s
most popular commercial VR systems, with very
diﬀerent hardware and software characteristics each.
These are the Oculus Rift S2, the Oculus Quest 23 and
the Oculus GO4.

To carry out this analysis, we used the application
Beat Saber which can run on all three devices. Beat
Saber 5 is a rhythm game developed exclusively for VR

2 https://www.oculus.com/rift-s/
3 https://www.oculus.com/quest/
4 https://www.oculus.com/go/
5 https://beatsaber.com/

Immersion Metrics for Virtual Reality

11

Table 6 Immersion calculated for the three types of devices
for the Beat Saber game, using the 3 immersion metrics.

Oculus Rift S Oculus Quest Oculus GO

Model 1
Model 2
Model 3

57.95020
60.74711
74.07102

54.40024
63.46851
76.70703

48.17092
52.51633
58.41741

that has become one of the most popular games in
recent years. The game is developed for the three VR
devices we are considering, hence we can use it for
the immersion calculation using our metrics. All three
viewers can run the game at 60 frames per second, as
indicated by the game speciﬁcation.

To use our immersion metrics, we need to know the
game’s software speciﬁcations (for example if shadows
are being used or not, or the level of brightness or
saturation). Since some of these variables are not
speciﬁed, we had to estimate them by analyzing
gameplays, images, and videos of the game.

The results of the immersion calculation are shown
in table 6 for the three models and the three devices.
The Oculus GO presented the lowest immersion for
the three models. This is consistent with the technical
speciﬁcations of the device, as well as users’ ratings
over the past few years. This viewer was Oculus’ ﬁrst
attempt to make a viewer completely independent of a
PC, and its tracking system and visual quality are more
basic than the other devices.

On the other hand, the Oculus Rift S and Oculus
Quest turned out to be very similar in terms of
immersion. Based on Model 1, the Oculus Rift S
presented more immersion than the Oculus Quest, but
for Models 2 and 3, the Oculus Quest outperformed the
Oculus Rift S. This small diﬀerence between these two
devices was expected since, based on the variables used
by the metrics, the only diﬀerence between the two was
the screen resolution, which the Oculus Quest narrowly
exceeds.

8 Discussion

The present work studied the diﬀerent variables of the
VR system and how they relate to the perceived level of
immersion. We performed a user study and statistical
analyses following a methodology designed to generate
immersion metrics. This section presents a discussion
about the obtained results, the limitations of the study,
and some directions for future work.

8.1 Variables

The statistical analyses described in section 4.2
provided interesting results. Some visual variables

presented small correlations with immersion, namely
the screen width, the frames per second, and the
contrast. The screen resolution and the frames per
second are variables widely studied in the literature,
and it is suggested that a bigger screen resolution and
faster frames per second are clearly related to a higher
level of immersion. On the other hand, it was interesting
to see that the contrast aﬀected, albeit slightly, the level
of immersion. This can be related to the role of contrast
in detecting the edge and details of objects

The use of textures signiﬁcantly aﬀected the level
of immersion. This suggests that the user felt more
immersed when the objects and the environment
presented a convincing material,
to what
happens in the real world. Most objects in the real world
present some kind of deﬁned texture or material. This
might explain why a lower immersion was perceived
when seeing objects with only solid colors and no
textures.

similar

Some visual variables

that, according to the
literature, signiﬁcantly aﬀect the immersion, were not
relevant in the statistical analysis. For instance, we
expected the ﬁeld-of-view to highly inﬂuence the
perceived immersion. Nowadays, every modern VR
headset seeks to improve the ﬁeld-of-view, among
other variables. In addition, the stereopsis was another
variable that did not aﬀect the immersion signiﬁcantly.
This is a variable directly related to depth perception,
both in the real and virtual environment. It should
be considered that there are people who have a
deﬁciency in stereoscopic vision and yet perceive depth.
This is due to diﬀerent depth cues in a scene. The
result obtained is consistent with this and undoubtedly
arises when analyzing the diﬀerent parameters as a
whole. Therefore, in future work it would be extremely
interesting to study the inﬂuence and relationship
between the variables that provide depth information
in more detail.

Regarding the audio, the results are consistent with
the literature. As expected, the use of headphones
presented the higher level of immersion, followed by
the use of speakers, and the absence of sound. The
headphones deliver the audio to each one of the user’s
ears, occluding the external noise, and thus improving
the immersion, no matter whether the 3D spatial sound,
ambient sound, or reverberation were active or not.
It is interesting to note that these three variables did
not signiﬁcantly aﬀected the immersion, since based on
the literature, they are relevant. The 3D spatial sound,
for instance, is not clearly perceived unless the user
is wearing headphones. Future work will consider the
analysis of the audio variables in more detail.

Regarding the locomotion mode, there was a clear

12

Matias N. Selzer*, Silvia M. Castro

diﬀerence between all groups, being the walk in place
the most immersive technique, followed by the use of
joystick, and ﬁnally by teleportation. In this study,
due to physical constrains, the real walking technique
could not be used. However, the results are consistent
with the literature, suggesting that the physical body
movement of walking did inﬂuence the ﬁnal perceived
immersion.

We have relied on the literature and on our previous
knowledge to select and study the variables that were
used in this work. However, the study of immersion
should not be limited only to these variables. As
technology advances, new variables will emerge that
must be considered, studied, and incorporated into the
metrics.

8.2 User Study

As mentioned in section 4.1.1, this study was intended
to be performed by many participants. However, due
to the conditions of social isolation related to COVID-
19, we decided to conduct the experiment focused of a
single user. Hence, the experiment results would only
relate to a target population with the characteristics
of the single user who performed the experiment.
Despite this, the results are still very interesting and,
above all, it is extremely important to have a way to
consider both the relevance of each variable and the
relationships between them in relation to the immersion
of the system. Future work will therefore consider this
study by conducting further experiments with more
participants.

In this study, we have used a single-item measure to
assess immersion. There are other questionnaires that
provide more information about the diﬀerent factors
that shape presence and immersion but, because they
are much larger or complex, participants can get bored
and lead to wrong results. Future work should consider
the use other measures to gather more information
about the relationship between immersion and the
variables of the VR system.

8.3 Generation of the Metrics

In this work, we followed a speciﬁc methodology to
generate immersion metrics, i.e., through the use of
regression models in addition to feature selection and
validation techniques. Other alternatives or techniques
can be considered in the diﬀerent parts of the metrics
generation process to get more insight about the
relationship between the variables and the eﬀect on the
immersion. We have made the dataset available online

for the public. Future work, therefore, should consider
the study and application of other techniques.

After generating the immersion models, a selection
process was carried out to determine which one (or
ones) of these could be considered the best models.
In this process (described in section 6), we made
decisions to discard some models in favor of others. For
this purpose, we focused on the predictive power, the
number of coeﬃcients and the number of predictors of
the models, without considering the particular variables
of each model. However, for a particular system or
application, it might be interesting to favor the model
that includes a particular variable such as, for example,
3D spatial audio. This should be considered in future
work

Based on these results, the Complete Model turned
out to be not as powerful as it seemed on stage
1, now obtaining only a R2 = 0.1403. This could
most likely be due to overﬁtting. It is highly probable
that a model that uses all the variables and all the
combinations between them will be adjusted to very
speciﬁc characteristics of the training data that have
no causal relationship with the objective function.

Our immersion metrics are intended to work with
any VR system, based on its hardware and software
characteristics. However, as mentioned above, some
of the studied variables depend on both the virtual
scenario being used, as well as the speciﬁc task being
performed. In this sense,
future work should also
consider the evaluation of immersion metrics in various
case studies and diﬀerent application domains.

9 Conclusions

Currently, the development of new VR systems with
diﬀerent hardware and software characteristics has
been accelerated. Every system tries to outperform the
others, but most of them rely only on technological ad-
vances to improve the user’s immersion and experience.
However, not only the most common hardware variables
(such as the ﬁeld-of-view or the screen resolution)
should be considered.

VR systems consider both hardware and software
variables that inﬂuence the total
immersion of the
system. It is necessary to know which variables are
if we
most inﬂuential and how. Thus, for example,
need to select among diﬀerent variables to include in a
new VR system, we may choose those with the highest
impact on the level of immersion. The inﬂuence of
these hardware and software variables on immersion
has only been considered individually or taking into
account small groups of these. To date, the way they all

Immersion Metrics for Virtual Reality

13

simultaneously aﬀect immersion has not been analyzed.
The motivation of this study is based on the study and
application of these hardware and software variables of
the VR system and their relationship to construct an
immersion metric. In this way, the level of immersion
of any VR system can be estimated without the need
of user tests.

The work we carried out has been highly chal-
lenging. The obtained results contribute to the area
of
immersive technologies and more speciﬁcally to
the area of VR. Commercial VR systems developed
in recent years are based on the assumption that
the better the hardware the higher the immersion
and therefore, the better the experience. Even though
upgrading the hardware can help to improve immersion,
this is not the only issue to be considered. To truly
improve immersion, the combination of variables to be
considered must be improved. Immersion metrics can
be designed to consider these characteristics of a VR
system and help to decide which variables to favor both
when designing a new VR system and when estimating
the immersion of existing VR systems. This allows the
comparison between diﬀerent systems, being able to
choose the best alternative according to the task to be
performed.

References

1. Laura Freina and Michela Ott. A literature review on
immersive virtual reality in education: state of the art and
perspectives.
In The International Scientiﬁc Conference
eLearning and Software for Education, volume 1, page 133.
” Carol I” National Defence University, 2015.

2. Teresa Monahan, Gavin McArdle,

and Michela
Bertolotto. Virtual reality for collaborative e-learning.
Computers & Education, 50(4):1339–1353, 2008.

3. Ioannis Messinis, Dimitrios Saltaouras, Panayotis Pinte-
las, and Tassos Mikropoulos. Investigation of the relation
between interaction and sense of presence in educational
virtual environments.
In 2010 International Conference
on e-Education, e-Business, e-Management and e-Learning,
pages 428–431. IEEE, 2010.

4. Jia Qiao, Jing Xu, Lu Li, and Yan-Qiong Ouyang. The
integration of immersive virtual reality simulation in
interprofessional education: A scoping review. Nurse
Education Today, page 104773, 2021.

5. Samuel P Ang, Michael Montuori, Yuriy Trimba, Nicole
Maldari, Divya Patel, and Qian Cece Chen. Recent
applications of virtual reality for the management of pain
in burn and pediatric patients. Current Pain and Headache
Reports, 25(1):1–8, 2021.

6. Peter R Swiatek, Joseph A Weiner, Daniel J Johnson,
Philip K Louie, Michael H McCarthy, Garrett K Harada,
Niccole Germscheid, Jason PY Cheung, Marko H Neva,
Mohammad El-Sharkawi, et al. Covid-19 and the rise
of virtual medicine in spine surgery: a worldwide study.
European Spine Journal, pages 1–10, 2021.

7. Teodor P Grantcharov, VB Kristiansen, Jørgen Bendix,
L Bardram, J Rosenberg, and Peter Funch-Jensen.

Randomized clinical trial of virtual reality simulation for
laparoscopic skills training. British journal of surgery,
91(2):146–150, 2004.

8. Lu´ıs Barbosa, Pedro Monteiro, Manuel Pinto, Hugo
Coelho, Miguel Melo, and Maximino Bessa. Multisensory
virtual environment for ﬁreﬁghter training simulation:
Study of the impact of haptic feedback on task execution.
In 2017 24º Encontro Portuguˆes de Computa¸c˜ao Gr´aﬁca e
Intera¸c˜ao (EPCGI), pages 1–7. IEEE, 2017.

9. Florence A¨ım, Guillaume Lonjon, Didier Hannouche, and
Remy Nizard. Eﬀectiveness of virtual reality training
in orthopaedic surgery.
the journal of
arthroscopic & related surgery, 32(1):224–232, 2016.
10. Sayali Joshi, Michael Hamilton, Robert Warren, Danny
Faucett, Wenmeng Tian, Yu Wang, and Junfeng
Ma. Implementing virtual reality technology for safety
training in the precast/prestressed concrete industry.
Applied ergonomics, 90:103286, 2021.

Arthroscopy:

11. Michael Zyda. From visual simulation to virtual reality

to games. Computer, 38(9):25–32, 2005.

12. Adrian David Cheok, Michael Haller, Owen Noel Newton
Fernando, and Janaka Prasad Wijesena. Mixed reality
entertainment and art. The International Journal of
Virtual Reality, 8(2):83–90, 2009.

13. S Graceline Jasmine, L Jani Anbarasi, Modigari
Narendra, and Benson Edwin Raj. Augmented and
virtual reality and its applications.
In Multimedia and
Sensory Input for Augmented, Mixed, and Virtual Reality,
pages 68–85. IGI Global, 2021.

14. Mel Slater. Measuring presence: A response to the witmer
and singer presence questionnaire. Presence, 8(5):560–
565, 1999.

15. Joy Van Baren, WA IJsselsteijn, Panos Markopoulos,
Natalia Romero, and Boris De Ruyter. Measuring
systems
aﬀective beneﬁts and costs of awareness
supporting intimate social networks. In CTIT workshop
proceedings series, volume 2, pages 13–19, 2004.

16. S Bouchard, G Robillard, J St-Jacques, S Dumoulin,
M J Patry, and P Renaud. Reliability and validity
In
of a single-item measure of presence in VR.
Proceedings. Second International Conference on Creating,
Connecting and Collaborating through Computing, pages
59–61. ieeexplore.ieee.org, October 2004.

17. Daniel Gromer, Oct´avia Madeira, Philipp Gast, Markus
Nehﬁscher, Michael Jost, Mathias M¨uller, Andreas
M¨uhlberger, and Paul Pauli. Height simulation in a
virtual reality cave system: validity of fear responses and
eﬀects of an immersion manipulation. Frontiers in human
neuroscience, 12:372, 2018.

18. Sebastian Oberd¨orfer, Martin Fischbach, and Marc Erich
Latoschik.
Eﬀects of ve transition techniques on
presence, illusion of virtual body ownership, eﬃciency,
and naturalness.
In Proceedings of the Symposium on
Spatial User Interaction, pages 89–99. ACM, 2018.

19. Matias N Selzer, Nicolas F Gazcon, and Martin L Larrea.
Eﬀects of virtual presence and learning outcome using
low-end virtual reality systems. Displays, 59:9–15, 2019.
20. Kwanguk Kim, M Zachary Rosenthal, David J Zielinski,
and Rachael Brady.
Eﬀects of virtual environment
platforms on emotional responses. Computer methods and
programs in biomedicine, 113(3):882–893, 2014.

21. Guy Wallis and Jennifer Tichon. Predicting the eﬃcacy
of simulator-based training using a perceptual judgment
task versus questionnaire-based measures of presence.
Presence, 22(1):67–85, 2013.

22. Dohyun Ahn, Youngnam Seo, Minkyung Kim,
Joung Huem Kwon, Younbo Jung, Jungsun Ahn,

14

Matias N. Selzer*, Silvia M. Castro

and Doohwang Lee. The eﬀects of actual human size
display and stereoscopic presentation on users’ sense of
being together with and of psychological immersion in a
virtual character. Cyberpsychology, Behavior, and Social
Networking, 17(7):483–487, 2014.

31. Ilias Bergstr¨om, S´ergio Azevedo, Panos Papiotis, Nuno
Saldanha, and Mel Slater. The plausibility of a string
quartet performance in virtual reality. IEEE transactions
on visualization and computer graphics, 23(4):1352–1359,
2017.

23. Juno Kim, Wilson Luu, and Stephen Palmisano.
Multisensory integration and the experience of scene
instability, presence and cybersickness in virtual environ-
ments. Computers in Human Behavior, 113:106484, 2020.
24. Tomasz Mazuryk and Michael Gervautz. Virtual reality
- history, applications, technology and future. Journal,
1996.

25. Torben Volkmann, Daniel Wessel, Tim Ole Caliebe, and
Nicole Jochems. What you see isn’t necessarily what you
get: testing the inﬂuence of polygon count on physical
and self-presence in virtual environments. In Proceedings
of the Conference on Mensch und Computer, pages 119–
128, 2020.

26. Mel Slater, Bernhard Spanlang, and David Corominas.
Simulating virtual environments within virtual environ-
ments as the basis for a psychophysics of presence. ACM
Transactions on Graphics (TOG), 29(4):92, 2010.

27. David Zeltzer. Autonomy,

interaction, and presence.
Presence: Teleoperators & Virtual Environments, 1(1):127–
132, 1992.

28. Sandra Poeschl, Konstantin Wall, and Nicola Doering.
Integration of
sound in immersive virtual
environments an experimental study on eﬀects of spatial
sound on presence. In Virtual Reality (VR), 2013 IEEE,
pages 129–130. IEEE, 2013.

spatial

29. Bimal Balakrishnan and S Shyam Sundar. Where am i?
how can I get there? impact of navigability and narrative
transportation on spatial presence. Human–Computer
Interaction, 26(3):161–204, August 2011.

30. Ant´onio S´ergio Azevedo, Joaquim Jorge, and Pedro
Campos. Combining eeg data with place and plausibility
responses as an approach to measuring presence in
outdoor virtual environments. Presence: Teleoperators and
Virtual Environments, 23(4):354–368, 2014.

32. Michael P Snow. Charting presence in virtual environments
and its eﬀects on performance. PhD thesis, Virginia Tech,
1998.

33. Alessandra Gorini, Claret S Capideville, Gianluca
De Leo, Fabrizia Mantovani, and Giuseppe Riva. The
role of immersion and narrative in mediated presence: the
virtual hospital experience. Cyberpsychology, Behavior,
and Social Networking, 14(3):99–105, 2011.

34. Kevin D Williams.

The eﬀects of video game
controls on hostility, identiﬁcation, and presence. Mass
communication and Society, 16(1):26–48, 2013.

35. Kevin W Arthur, Kellogg S Booth, and Colin Ware.
Evaluating 3d task performance for ﬁsh tank virtual
worlds.
ACM Transactions on Information Systems
(TOIS), 11(3):239–265, 1993.

36. Matias Nicol´as Selzer. Interacci´on humano computadora
en ambientes virtuales. Tesis de Magister, Universidad
Nacional del Sur, 2018.

37. Guido Makransky, Thomas S Terkildsen, and Richard E
Mayer. Adding immersive virtual reality to a science
lab simulation causes more presence but less learning.
Learning and Instruction, 60:225–236, 2019.

38. St´ephane Bouchard, Julie St-Jacques, Genevi`eve Robil-
lard, and Patrice Renaud. Anxiety increases the feeling
of presence in virtual reality. Presence: Teleoperators and
Virtual Environments, 17(4):376–391, 2008.

39. Mel Slater. Presence and the sixth sense. Presence:
Teleoperators & Virtual Environments, 11(4):435–439,
2002.

40. Matias N. Selzer, Silvia M. Castro, and Martin L. Larrea.
Virtual reality immersion dataset. Mendeley Data, V2,
doi: 10.17632/kj79vpcsc5.2, 2022.

