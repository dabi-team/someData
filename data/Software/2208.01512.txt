2
2
0
2

g
u
A
2

]
E
S
.
s
c
[

1
v
2
1
5
1
0
.
8
0
2
2
:
v
i
X
r
a

Human Aspect of Threat Analysis: A Replication

Katja Tuma
k.tuma@vu.nl
Vrije Universiteit Amsterdam
The Netherlands

Winnie Mbaka
w.mbaka@vu.nl
Vrije Universiteit Amsterdam
The Netherlands

ABSTRACT
Background: Organizations are experiencing an increasing de-
mand for security-by-design activities (e.g., STRIDE analyses) which
require a high manual eﬀort. This situation is worsened by the cur-
rent lack of diverse (and suﬃcient) security workforce and inconclu-
sive results from past studies. To date, the deciding human factors
(e.g., diversity dimensions) that play a role in threat analysis have
not been suﬃciently explored.
Objective: To address this issue, we plan to conduct a series of ex-
ploratory controlled experiments. The main objective is to empiri-
cally measure the human-aspects that play a role in threat analysis
alongside the more well-known measures of analysis performance.
Method: We design the experiments as a diﬀerentiated replication
of past experiments with STRIDE. The replication design is aimed
at capturing some similar measures (e.g., of outcome quality) and
additional measures (e.g., diversity dimensions). We plan to con-
duct the experiments in an academic setting.
Limitations: Obtaining a balanced population (e.g., wrt gender)
in advanced computer science courses is not realistic. The exper-
iments we plan to conduct with MSc level students will certainly
suﬀer this limitation.

There is an increasing need to perform such architectural secu-
rity analyses (e.g., latest BSIMM study reports an increased invest-
ment by more than 65% [10]) as the threat landscape evolves. How-
ever, threat analysis requires a high manual eﬀort [31], demands
the involvement of security and domain experts [6], and has been
proven time and again diﬃcult to fully automate [39].

Threat analysis practices are set back by a globally recorded
shortage of the security workforce [4, 7]. In addition, the current se-
curity workforce is not diverse (e.g., with respect to gender) which
may be viewed as an opportunity for a change.

Risk decisions (which are core to threat analysis) are made in
face of uncertainty [3], thus there is space for subjective (and pos-
sibly biased) judgement [5, 15]. Empirical evidence of threat anal-
ysis performance indicators is a crucial piece of the puzzle to im-
prove the situation. But, past empirical studies were either incon-
clusive about some performance indicators [38] or have focused
on measuring performance indicators irrespective of the human
factors [31, 37, 40]. Yet measuring such human factors is pivotal
to understanding how to close the security workforce gap in the
future.

To address these issues, we plan to conduct a series of exploratory
controlled experiments with the aim of empirically measuring the
human-aspects that play a role in threat analysis. In particular, we
design a diﬀerentiated replication [21], where we capture some
similar measures used in previous experiments [38] but also dif-
ferent measures (e.g., participant gender, nationality, type of out-
comes).

Pre-print

ACM Reference Format:
Katja Tuma and Winnie Mbaka. 2022. Human Aspect of Threat Analysis: A
Replication. In Proceedings of ACM Conference (Conference’17). ACM, New
York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

KEYWORDS
Threat Analysis, Human Aspects, Empirical Software Engineering,
Replication, Controlled Experiment

1 INTRODUCTION
Security-by-design techniques [8, 30] have been used to prevent
costly security ﬁxes to software in later stages of the development
life-cycle by analyzing security already during the design phase.
Practitioners use threat analysis [36] to look for potential secu-
rity threats in their product’s software architecture. For instance,
STRIDE [33] is a popular technique developed by Microsoft.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

2 RELATED WORK
We positioned our contributions with respect to existing literature
on empirical studies of STRIDE and related replication studies.
Empirical studies of threat analysis. Several works have inves-
tigated STRIDE empirically. Scandariato et al. [31] performed a de-
scriptive analysis to quantify the cost and eﬀectiveness of STRIDE
by measuring the productivity, precision, and recall in an academic
setting. Their study reports students having higher rates in preci-
sion and recall with lower productivity rates.

Two studies [2, 6] conducted case studies investigating the chal-
lenges of STRIDE. Bernsmed et al. [6] performed an exploratory
case study with the goal of investigating the challenges facing adop-
tion of threat modeling using the Microsoft approach with STRIDE.
The study was done in a company comprising ﬁve agile develop-
ment projects. Their analysis elicited 21 challenges to threat mod-
elling which were then mapped to existing literature and concluded
that proper understanding of threat elicitation is required in order
to actualise the functions of STRIDE especially in agile develop-
ment.

Stevens et al. [34] conducted a qualitative case study to evaluate
the impact of introducing threat modeling to an organization that

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

K Tuma and W Mbaka

had previously not used it by applying the Center of Gravity (CoG)
framework. The CoG is a risk-ﬁrst threat analysis technique that
has not been extensively used to analyze software security. The au-
thors goal was to measure eﬀectiveness and eﬃciency of CoG, us-
ing surveys and classroom sessions that involved 25 practitioners.
The authors reported a very high accuracy from the participant’s
results, similar to other studies that have been conducted with ex-
perts. Diﬀerent attack and risk-centric threat analysis techniques
(such as Attack trees, CORAS, MUCs, to name a few) have also
been investigated with empirical rigour. We point the interested
reader to a systematic review for more details [36].
Replications. We frame our plan as a series of experimental repli-
cations. Generally, the goal of replication studies is to validate the
experimental procedures of the original study using a diﬀerent par-
ticipation pool. This studies are aimed at generating new data [29]
(as opposed to re-analyzing the same data in reproduction studies).
We brieﬂy mention some related replication studies.

In their original study, Labunets et al [20], compared two risk
assessment methods, a visual and a textual method and reported
that the visual method was more eﬀective for identifying threats
than the textual one. The same study was replicated in [19], apply-
ing similar procedures. In contrast to the original study, the replica-
tion reported that the two methods being investigated were (statis-
tically) equivalent with regards to the quality of identiﬁed threats
and security controls.

Several studies have empirically compared [17, 18, 23] and con-
ducted replications [16, 24, 26] requirement engineering techniques
(e.g., requirements elicitation). For brevity, we direct the interested
reader a comprehensive review by Ambreen et al. [1].

3 RESEARCH QUESTIONS
Due to the academic setting we limit this study on observing gen-
der, background, and nationality diversity dimensions (and exclude
seniority). The main goal of this study is to measure the existence
(or absence) of diversity eﬀects on the actual and perceived anal-
ysis outcomes. Accordingly, we developed two research questions
and hypotheses about each measure.

RQ1. What is the eﬀect of gender, background, and nationality on
the actual threat analysis outcomes?

To investigate RQ1, we pose hypotheses about the equivalence

of the sample means for the analysis outcomes.
𝐻 11 : 𝐶𝑜𝑚𝑝.𝑆𝑐𝑖𝐹 = 𝐶𝑜𝑚𝑝.𝑆𝑐𝑖𝑀
Regarding gender, we expect that the outcomes reported by women
are equivalent to the outcomes reported by men. Studies of risk
perception suggest that women perceive certain risks diﬀerently
compared to men. Though we do not foresee strong diﬀerences,
we might ﬁnd some eﬀects when it comes to risk priority.
𝐻 12 : 𝐶𝑜𝑚𝑝.𝑆𝑐𝑖1 = 𝐶𝑜𝑚𝑝.𝑆𝑐𝑖2
Regarding education, we expect that the students of various spe-
cialization tracks report equivalent outcomes for the same system
under analysis.
𝐻 13 : 𝐶𝑜𝑚𝑝.𝑆𝑐𝑖𝑁 𝑎 = 𝐶𝑜𝑚𝑝.𝑆𝑐𝑖𝑁𝑏
We expect that the students of various race and nationality report
statistically equivalent outcomes.

RQ2. What is the eﬀect of gender, background, and nationality on
the perceived threat analysis outcomes?

To investigate RQ2, we pose hypotheses about the equivalence

of the sample means for the perceived analysis outcomes.
𝐻 21 : 𝑃𝑒𝑟𝑐 (𝐶𝑜𝑚𝑝.𝑆𝑐𝑖𝐹 ) < 𝑃𝑒𝑟𝑐 (𝐶𝑜𝑚𝑝.𝑆𝑐𝑖𝑀 )

Due to low conﬁdence levels of female computer science students,
we expect that the perceived quality of outcomes reported by women
is lesser compared to the perceived quality of outcomes reported
by men (regardless of the actual outcomes by both groups).
𝐻 22 : 𝑃𝑒𝑟𝑐 (𝐶𝑜𝑚𝑝.𝑆𝑐𝑖1) = 𝑃𝑒𝑟𝑐 (𝐶𝑜𝑚𝑝.𝑆𝑐𝑖2)

Regarding education, we expect that overall the students of vari-
ous specialization tracks do not diﬀer in their perceived quality of
the outcomes they produced. We may ﬁnd higher conﬁdence lev-
els of perceived quality for students that are following a security
specialization track.
𝐻 23 : 𝑃𝑒𝑟𝑐 (𝐶𝑜𝑚𝑝.𝑆𝑐𝑖𝑁 𝑎) = 𝑃𝑒𝑟𝑐 (𝐶𝑜𝑚𝑝.𝑆𝑐𝑖𝑁𝑏)
We expect that the students of various nationality do not diﬀer in
their perceived quality of the outcomes they produced.

4 REPLICATION PROTOCOL
4.1 Variables
Table 1 shows the variables of the study.

Independent. Gender is an individual’s identity based on
4.1.1
their sex, which is typically, man and woman, but can also be non-
binary. Several studies [25] [13] [41] found evidence of bias against
women in some software engineering communities, and sometimes
negative perceptions about women working in teams. Thus, this is
an interesting dimension to further investigate in the context of
security.
Education is an individuals’ level of achievement within a speciﬁc
area of specialization (e.g., computer security vs AI) in academic
studies. Typically, risk-based decisions in organizations have been
made by persons in managerial positions who are assumed to have
a better understanding of the product. However, technical skills of
security experts or engineers should be taken into consideration
during critical decision-making. Therefore, it is interesting to in-
vestigate the eﬀect of education by including participants from dif-
ferent academic backgrounds (e.g., in communication sciences). Al-
though in [14] study, education was found to be a non-signiﬁcant
variable, it is not clear whether this dimension has an impact in
performing a RA task.
Nationality is generally used to refer to someone’s country of
origin, however, it may be coupled with other identifying aspects,
such as the culture and language. On the other hand, race is a so-
cial construct that is associated with an individuals physical ap-
pearance, such as their skin colour. Determining the eﬀect of na-
tionality bias in security practices is to date an open question. In
a previous study, Thomas et al. [35] conducted semi-structured in-
terviews with 11 Black women in computing and report that Black
women experienced a number of challenges, such as discrimina-
tion, expectations from others that are too high or too low, isola-
tion, sexism, and racism. However, it is not clear whether some of

Pre-print

Human Aspect of Threat Analysis: A Replication

Conference’17, July 2017, Washington, DC, USA

and risk mitigations). The second part of the training will prepare
the students to actually perform a threat analysis using one of the
technique variants. The third part of the training will introduce
the participants to the case study which will be the object of their
analysis.
Case study documentation. We will use the same case study as
in the original study. The home monitoring system (HomeSys) is
an automated surveillance system designed for residential places.
Its main objective is to enable the home-owner to remotely moni-
tor their property. A detailed documentation of the case (require-
ments, architectural design, etc) will be made available to the par-
ticipants.
Ground truth analysis. We will use one ’golden standard’ data
ﬂow diagram and its’ corresponding ground truth STRIDE analy-
sis of the HomeSys case study from [38]. Since we do not aim to
measure the quality of the diagrams created, and the DFD building
is less time consuming compared to threat identiﬁcation, we will
provide a model to the participants. This will signiﬁcantly simply
the comparison of the identiﬁed security threats. Similarly, we will
provide the ground truth analysis to the participants that will be
prioritizing threats and identifying security mitigations.

Pre-print

4.3 Task
The participants will be asked to individually ﬁll-in a survey. The
survey consists of three parts. First, a few questions about the stu-
dents gender, background, nationality. Half of the participants will
be asked to perform a STRIDE analysis (i.e., identify security threats).
In contrast to previous studies, our participants will be given the
same graphical model of HomeSys to analyze and they will ana-
lyze it using the same STRIDE technique. The other half of the
participants will be asked to prioritize a list of security threats and
identify security mitigations to high-priority threats. In contrast to
previous studies, our participants will be given the graphical model
and the list of security threats. To guide the threat identiﬁcation the
participants will use the documentation of STRIDE. Similar to the
past studies, we will hand out a threat template csv to standard-
ize the format of the outcomes reported. The participants will sub-
mit the ﬁles using the same survey. Finally, they will be asked a
few questions regarding their perception of the task. Time taken
to complete the task was captured using an online survey tool.

these challenges were as a result of their gender, race or national-
ity). But, few studies have focused on nationality diversity in the
software engineering discipline [25].

4.1.2 Dependent. Since the quality of analysis lacks a formalised
deﬁnition (e.g., often natural language is used to describe attack
scenarios and informal notations are used for modeling [36]), we
will use measures that can be easily reproduced. Namely, we can
observe how diversity dimensions eﬀect the type of analysis out-
comes. Table 1 (dependant variables) shows various outcomes types
that we observe.
Threats. We use the STRIDE threat categories to distinguish dif-
ferent type of threats. Tuma et al. [37, 38] noticed that expert anal-
ysis tend to be more balanced in terms of their review of diﬀerent
threat categories, while non-experts tend to report a high number
of tampering, denial of service and information disclosure threats.
We are interested to observe whether other diversity dimensions
have an eﬀect on category distribution of identiﬁed threats.
Assumptions. Assumptions are statements about the domain that
may or may not be true. Assumptions are often implicit and dy-
namic in nature (i.e., they can be invalidated and modiﬁed as the
project evolves). Van Landuyt and Joosen [40] ﬁnd that the major-
ity of assumptions (created by students during STRIDE) were used
to either justify an existence of threats or are used to eliminate
threats. In [40] a substantial subset (78%) of the assumptions was in
direct reference to security-related concepts (i.e., security assump-
tions), however also domain assumptions (statements about com-
ponent functionalities) were made. Thus, we are interested to in-
vestigate the eﬀect of diversity dimensions on the type of assump-
tions.
Attack surface. deﬁning an attacker proﬁle and the attack surface
are essential in determining the feasibility of an attack scenario. To
this end, security analysis are expected to make these distinction
prior to performing a threat analysis.
Risk priority. We refer to risk as a product of threat probability
and impact. How individuals assess risk priorities may be related to
their risk perception which is already well understood [11]. Since
the number of identiﬁed threats explodes in realistic projects, prac-
titioners must choose which threats are most urgent to mitigate.
Thus they prioritize them based on estimations of risk.
Mitigations. There are diﬀerent approaches that can be applied
while mitigating security risks. Preventative (e.g., implementation
of two-factor authentication), detective/reactive (e.g., using intru-
sion detection and access revocation techniques) and corrective
(e.g., maintaining audit trails or restoring from a secure state). Se-
curity analysts can implement multiple strategies depending on
domain-related factors, such as the cost associated with a speciﬁc
mitigation strategy. Some diversity dimension (e.g., gender) may
underestimate the ease with which a mitigation is actually imple-
mented, as observed in [42]. Thus, it is interesting to observe how
other diversity dimensions, including gender eﬀect the type of mit-
igations that are identiﬁed during threat analysis.

4.2 Material
Training. In the ﬁrst part of the training the participants will be
introduced to some key security topics (such as CIAA triad, se-
curity threats, attack surface and vulnerabilities, security controls

4.4 Participants
Our population is computer science students, with some diﬀer-
ences in the elective courses and program choices (e.g., we plan to
include students from various master programs, such as IA, com-
puter Security, and Software Engineering). All participants are stu-
dents enrolled in a course taught by the experimenters. At the be-
ginning of the course we plan to hand out an entry survey to mea-
sure participants’ background and areas of expertise relevant to
the study. We expect most to be new to secure design techniques
(e.g STRIDE, threat modeling, Data Flow Diagrams, misuse cases,
attack trees etc). In addition, we expect the participants are unfa-
miliar with architectural modeling techniques (e.g sequence, com-
ponent and deployment diagrams).

Conference’17, July 2017, Washington, DC, USA

K Tuma and W Mbaka

Name

Description

Scale

Operationalization

Independent variables (design)

Gender
Background

Nationality

Dependent variables

obtained from the gender of participants
the program specialization and extra curriculum activi-
ties
obtained from the nationality of participants

nominal multiple choice
nominal multiple choice

nominal multiple choice

**Diﬀerent measures compared to existing literature**
Type of identiﬁed threats

Type of assumptions

Type of attacks surface

Risk priorities

Type of mitigations

Treated/Measured variables

Time spent on task

Perceived precision (PP)

Perceived recall (PR)

Perceived usefulness (PU)
Experience with security and modeling

Experience with STRIDE

Experience with domain of application

**Diﬀerent measures compared to existing literature**
Perceived cognitive load

Perceived quality and eﬃcacy

distribution of categories of threats (spooﬁng, tamper-
ing, information disclosure, denial of service, elevation
of privilege) that have been identiﬁed by the partici-
pants
distribution type of assumptions (domain, security) that
have been reported by the participants
distribution attack surfaces (physical, close-proximity,
remote) of the identiﬁed threats
distribution of risk priorities (high, medium, low) as-
signed to identiﬁed threats
distribution of type of identiﬁed mitigations (preventa-
tive, detective/reactive, corrective)

Pre-print

self-reported ratio between the number of correctly
identiﬁed threats and all threats identiﬁed
self-reported ratio between the number of correctly
identiﬁed threats and all existing threats identiﬁed
self-reported usefulness of the prescribed technique
self-reported experience in number of years or previ-
ously completed courses
self-reported experience in number of years or previ-
ously completed courses
self-reported experience in number of years or previ-
ously completed courses

time (in hours) to complete the task using the prescribed
technique

the reported cognitive load (complexity) of the task us-
ing the prescribed technique
the reported quality and self eﬃcacy of work conducted

nominal see Section 4.2

nominal see Section 4.2

nominal see Section 4.2

nominal see Section 4.2

nominal see Section 4.2

ordinal

ordinal

automatically
measured
by
submission tool
5-point Likert scale

the

ordinal

5-point Likert scale

ordinal
ordinal

5-point Likert scale
5-point Likert scale

ordinal

5-point Likert scale

ordinal

5-point Likert scale

ordinal

5-point Likert scale

ordinal

5-point Likert scale

Table 1: Variables of the diﬀerentiated replication experiments

4.5 Execution plan
Work division. The participants will be randomly divided into
two groups (A and B). Group A will be tasked with analysing a pro-
vided data ﬂow diagram of the HomeSys case study using STRIDE.
Group B will be tasked with prioritizing a provided list of secu-
rity threats and identifying security mitigations for high-priority
threats. These treatment groups are formed only to divide the work
to avoid overloading the participants performing an overly com-
plex task individually.
Training. The participants will undergo an obligatory training lec-
tures (about 3 hours) covering the topics mentioned above.

Hand-outs. After the training, participants will be given digital
copies of all the support material (inc. lecture slides, case docu-
mentation, technique documentation, etc).
Physical labs. The experiment will be conducted during a four
hour physical lab. The participants will be separated into diﬀerent
classrooms depending on their treatment group (to avoid spillover
eﬀects). Each classroom will be supervised by either a teaching as-
sistant or the experimenters. Only questions about the experiment
protocol will be answered.
Reports. The data will be collected through an online survey tool.

Human Aspect of Threat Analysis: A Replication

Conference’17, July 2017, Washington, DC, USA

4.6 Analysis plan
Data cleaning. We will perform a preliminary check of the col-
lected data. This will include removing submissions for which we
did not get explicit consent by the participant. Second, we will re-
move clearly insincere submission attempts (if any).
TOST analysis of equivalence. We will use both diﬀerence and
equivalence statistical tests. It is likely that we do not obtain nor-
mally distributed samples, thus we plan to use a non-parametric,
Mann-Whitney test. TOST was initially proposed by [32] and is
widely used in pharmacological and food sciences to check whether
two treatments are equivalent within a speciﬁed range 𝛿 [9, 22].
Wherever possible (e.g., for Likert-scale questions) we will deﬁne
the delta empirically. For instance by pooled variance 𝜎𝑝 across
several samples reported in the literature on security risk analy-
sis (e.g., in a four year interval) on variables ranging over a 5-item
Likert scale for demographic statistics as to account for natural
variability of the data.
Validity threats. There is typically around 20% (or less) female
students enrolled in computer science programs. We are aware of
the validity threats caused by an unbalanced population sample,
which is omnipresent in all gender diversity studies in STEM disci-
plines [25]. To partially mitigate this threat, we will rally female
computer scientist students towards participation through local
steminist groups and similar community organized channels.

Since we do not include practitioners in this study, we can not
observe the full complexity of the diversity eﬀects (e.g., including
seniority) that are actually present in organizations where threat
analysis is routinely performed. Still, studies have shown [12, 27,
28] that the diﬀerences between the performance of professionals
and graduate students are often limited.

We considered the threat of overloading the participants with a
complex task. We mitigate this threat by splitting the participants
into two groups, so individual participants get to either only focus
on ﬁnding threats or focus on mitigating risks.

[10] Synopsys Software Integrity Group. 2021. Building Security In Maturity Model

(BSIMM12). Retrieved April 20, 2022 from https://www.bsimm.com

[11] Per E Gustafsod. 1998. Gender Diﬀerences in risk perception: Theoretical and

methodological erspectives. Risk analysis 18, 6 (1998), 805–811.

[12] Martin Höst, Björn Regnell, and Claes Wohlin. 2000. Using students as sub-
jects—a comparative study of students and professionals in lead-time impact
assessment. Empirical Software Engineering 5, 3 (2000), 201–214.

[13] Nasif Imtiaz, Justin Middleton, Joymallya Chakraborty, Neill Robson, Gina Bai,
and Emerson Murphy-Hill. 2019.
Investigating the eﬀects of gender bias on
GitHub. In 2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 700–711.

[14] Eric Jardine. 2020. The Case against Commercial Antivirus Software: Risk Home-
ostasis and Information Problems in Cybersecurity. Risk Analysis 40, 8 (2020),
1571–1588.

[15] Johannes G Jaspersen and Gilberto Montibeller. 2015. Probability elicitation un-
der severe time pressure: A rank-based method. Risk Analysis 35, 7 (2015), 1317–
1335.

[16] J. Jung, K. Hoeﬁg, D. Domis, A. Jedlitschka, and M. Hiller. 2013. Experimental
International
comparison of two safety analysis methods and its replication.
Symposium on Empirical Software Engineering and Measurement (2013), 223–232.
https://doi.org/10.1109/ESEM.2013.59

[17] Peter Karpati, Andreas L Opdahl, and Guttorm Sindre. 2011. Experimental com-
parison of misuse case maps with misuse cases and system architecture diagrams
for eliciting security vulnerabilities and mitigations. In Availability, Reliability
and Security (ARES), 2011 Sixth International Conference on. IEEE, 507–514.
[18] Peter Karpati, Guttorm Sindre, and Raimundas Matulevicius. 2012. Comparing
misuse case and mal-activity diagrams for modelling social engineering attacks.
International Journal of Secure Software Engineering (IJSSE) 3, 2 (2012), 54–73.

[19] Katsiaryna Labunets, Fabio Massacci, and Federica Paci. 2017. On the equiva-
lence between graphical and tabular representations for security risk assessment.
In International Working Conference on Requirements Engineering: Foundation for
Software Quality. Springer, 191–208.

[20] Katsiaryna Labunets, Fabio Massacci, Federica Paci, et al. 2013. An experimental
comparison of two risk-based security methods. In 2013 ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement. IEEE, 163–172.
[21] R Murray Lindsay and Andrew SC Ehrenberg. 1993. The design of replicated

studies. The American Statistician 47, 3 (1993), 217–228.

[22] Michael Meyners. 2012. Equivalence tests–A review. Food quality and preference

[23] Andreas L Opdahl and Guttorm Sindre. 2009. Experimental comparison of at-
Information and

tack trees and misuse cases for security threat identiﬁcation.
Software Technology 51, 5 (2009), 916–932.

[24] M. Riaz, J. King, J. Slankas, L. Williams, F. Massacci, C. Quesada-López, and M.
Jenkins. 2017. Identifying the implied: Findings from three diﬀerentiated repli-
cations on the use of security requirements templates. Empirical Software Engi-
neering 22, 4 (2017), 2127–2178. https://doi.org/10.1007/s10664-016-9481-1
[25] Gema Rodríguez-Pérez, Reza Nadri, and Meiyappan Nagappan. 2021. Perceived
diversity in software engineering: a systematic literature review. Empirical Soft-
ware Engineering 26, 5 (2021), 1–38.

[26] S. Rueda, J.I. Panach, and D. Distante. 2020. Requirements elicitation methods
based on interviews in comparison: A family of experiments. Information and
Software Technology 126 (2020). https://doi.org/10.1016/j.infsof.2020.106361
[27] Per Runeson. 2003. Using students as experiment subjects–an analysis on grad-
uate and freshmen student data. In Proceedings of the International Conference
on Empirical Assessment in Software Engineering. 95–102.

Pre-print

26, 2 (2012), 231–245.

5 ACKNOWLEDGMENTS
REFERENCES
[1] Talat Ambreen, Naveed Ikram, Muhammad Usman, and Mahmood Niazi. 2018.
Empirical research in requirements engineering: trends and opportunities. Re-
quirements Engineering 23, 1 (2018), 63–95.

[2] Karin Bernsmed and Martin Gilje Jaatun. 2019. Threat modelling and agile soft-
ware development: Identiﬁed practice in four Norwegian organisations. In 2019
International Conference on Cyber Security and Protection of Digital Services (Cy-
ber Security). IEEE, 1–8.

[3] Vicki Bier. 2020. The Role of Decision Analysis in Risk Analysis: A Retrospective.

Risk Analysis 40, S1 (2020), 2207–2217.

[4] Borka Jerman Blažič. 2021. Cybersecurity Skills in EU: New Educational Con-
cept for Closing the Missing Workforce Gap. In Cybersecurity Threats with New
Perspectives. IntechOpen.

[5] Mario P Brito and Ian GJ Dawson. 2020. Predicting the validity of expert judg-
ments in assessing the impact of risk mitigation through failure prevention and
correction. Risk analysis 40, 10 (2020), 1928–1943.

[6] Daniela Soares Cruzes, Martin Gilje Jaatun, Karin Bernsmed, and Inger Anne
Tøndel. 2018. Challenges and experiences with applying microsoft threat model-
ing in agile development projects. In 2018 25th Australasian Software Engineering
Conference (ASWEC). IEEE, 111–120.

[7] CyberSeek. 2019. Cybersecurity Supply/Demand Heat Map. Retrieved April 21,

2022 from https://www.cyberseek.org/heatmap.html

[8] Chad Dougherty, Kirk Sayre, Robert C Seacord, David Svoboda, and Kazuya To-
gashi. 2009. Secure Design Patterns. Technical Report. Carnegie-Mellon Univer-
sity Pittsburgh, Software Engineering Institute.

[9] Food and Drug Administration. 2001. Guidance for industry: Statistical ap-

proaches to establishing bioequivalence.

[28] Iﬂaah Salman, Ayse Tosun Misirli, and Natalia Juristo. 2015. Are students rep-
resentatives of professionals in software engineering experiments?. In Proceed-
ings of the International Conference on Software Engineering-Volume 1. IEEE Press,
666–676.

[29] Adrian Santos, Sira Vegas, Markku Oivo, and Natalia Juristo. 2019. A procedure
and guidelines for analyzing groups of software engineering replications. IEEE
Transactions on Software Engineering 47, 9 (2019), 1742–1763.

[30] Joanna C. S. Santos, Katy Tarrit, and Mehdi Mirakhorli. 2017. A Catalog of Se-
curity Architecture Weaknesses. In Proceedings of the International Conference
on Software Architecture Workshops (ICSAW). IEEE Computer Society, 220–223.
https://doi.org/10.1109/ICSAW.2017.25

[31] Riccardo Scandariato, Kim Wuyts, and Wouter Joosen. 2015. A descriptive study
of Microsoft’s threat modeling technique. Requirements Engineering 20, 2 (2015),
163–180.

[32] DL Schuirmann. 1981. On hypothesis-testing to determine if the mean of a
normal-distribution is contained in a known interval. Biometrics 37, 3 (1981),
617–617.

[33] Adam Shostack. 2014. Threat modeling: Designing for security. John Wiley &

Sons.

[34] R Stevens, D Votipka, and E.M. Redmiles. 2018. The Battle for New York: A Case
Study of Applied Digital Threat Modeling at the Enterprise Level. In SEC’18:
Proceedings of the 27th USENIX Conference on Security Symposium. USENIX As-
sociation, 621–637.

Conference’17, July 2017, Washington, DC, USA

K Tuma and W Mbaka

[35] Jakita O Thomas, Nicole Joseph, Arian Williams, Jamika Burge, et al. 2018. Speak-
ing truth to power: Exploring the intersectional experiences of Black women in
computing. In 2018 Research on Equity and Sustained Participation in Engineering,
Computing, and Technology (RESPECT). IEEE, 1–8.

[36] Katja Tuma, Gül Calikli, and Riccardo Scandariato. 2018. Threat analysis of soft-
ware systems: A systematic literature review. Journal of Systems and Software
144 (2018), 275–294.

[37] Katja Tuma, Christian Sandberg, Urban Thorsson, Mathias Widman, Thomas
Herpel, and Riccardo Scandariato. 2021. Finding security threats that matter:
Two industrial case studies. Journal of Systems and Software 179 (2021), 111003.
[38] Katja Tuma and Riccardo Scandariato. 2018. Two architectural threat analysis
techniques compared. In European Conference on Software Architecture. Springer,
347–363.

[39] Katja Tuma, Laurens Sion, Riccardo Scandariato, and Koen Yskout. 2020. Au-
tomating the early detection of security design ﬂaws. In Proceedings of the 23rd
ACM/IEEE International Conference on Model Driven Engineering Languages and
Systems. 332–342.

[40] Dimitri Van Landuyt and Wouter Joosen. 2021. A descriptive study of assump-
tions in STRIDE security threat modeling. Software and Systems Modeling (2021),
1–18.

[41] Yi Wang and David Redmiles. 2019. Implicit gender biases in professional soft-
ware development: An empirical study. In 2019 IEEE/ACM 41st International
Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS).
IEEE, 1–10.

[42] George Wright, Fergus Bolger, and Gene Rowe. 2002. An empirical test of the rel-
ative validity of expert and lay judgments of risk. Risk Analysis: An International
Journal 22, 6 (2002), 1107–1122.

Pre-print

