2
2
0
2

r
p
A
8
2

]

G
L
.
s
c
[

1
v
6
6
6
3
1
.
4
0
2
2
:
v
i
X
r
a

Schr¨odinger’s FP: Dynamic Adaptation of
Floating-Point Containers for Deep Learning
Training

Miloˇs Nikoli´c
University of Toronto
milos.nikolic@mail.utoronto.ca

Enrique Torres Sanchez
University of Toronto
enrique.torres@mail.utoronto.ca

Jiahui Wang
University of Toronto
eduardojh.wang@mail.utoronto.ca

Ali Hadi Zadeh
University of Toronto
hadizade@ece.utoronto.ca

Mostafa Mahmoud
University of Toronto
mostafa.mahmoud@mail.utoronto.ca

Ameer Abdelhadi
University of Toronto
ameer.abdelhadi@utoronto.ca

Andreas Moshovos
University of Toronto
moshovos@ece.utoronto.ca

Abstract—We introduce a software-hardware co-design ap-
proach to reduce memory trafﬁc and footprint during training
with BFloat16 or FP32 boosting energy efﬁciency and execution
time performance. We introduce methods to dynamically adjust
the size and format of the ﬂoating-point containers used to
store activations and weights during training. The different value
distributions lead us to different approaches for exponents and
mantissas. Gecko exploits the favourable exponent distribution
with a loss-less delta encoding approach to reduce the total
exponent footprint by up to 58% in comparison to a 32 bit ﬂoating
point baseline. To content with the noisy mantissa distributions,
we present two lossy methods to eliminate as many as possible
least signiﬁcant bits while not affecting accuracy. Quantum Man-
tissa, is a machine learning-ﬁrst mantissa compression method
that taps on training’s gradient descent algorithm to also learn
minimal mantissa bitlengths on a per-layer granularity, and ob-
tain up to 92% reduction in total mantissa footprint. Alternatively,
BitChop observes changes in the loss function during training
to adjust mantissa bit-length network-wide yielding a reduction
of 81% in footprint. Schr¨odinger’s FP implements hardware
encoders/decoders that guided by Gecko/Quantum Mantissa or
Gecko/BitChop transparently encode/decode values when trans-
ferring to/from off-chip memory boosting energy efﬁciency and
reducing execution time.

.

I. INTRODUCTION

Training most state-of-the-art neural networks has become
an exascale class task [1], [2] requiring many graphics
processors [3] or specialized accelerators such as the TPU [4],
Gaudi [5], DaVinci [6], or Cerebras CS1 [7]. The need for
energy efﬁcient and fast inference further increases training
costs as additional processing is needed during training to
best tune the resulting neural network. Techniques such as
quantization [8]–[11], pruning [12]–[14], network architecture
search [15], and hyperparameter exploration [16] are examples
of such training-time tuning techniques. The need for improving
training speed and energy efﬁciency is not limited to server-
class installations. Such methods are desirable also at the “edge”
where training can be used to reﬁne an existing model with
user-speciﬁc information and input.

Improving training speed and energy efﬁciency has been
receiving the attention it deserves throughout
the soft-
ware/hardware stack. Distributed training reduces training
time by exploiting model, data, and pipeline parallelism [17],
[18]. Dataﬂow optimizations such as data blocking and reuse,
and communication and computation overlapping improve
performance and energy efﬁciency by best allocating and
using compute, memory and communication resources within
and across computing nodes [19]–[21]. Transfer learning, can
reduce training time and customization of a neural network by
utilizing another previously training network as the basis upon
which to train a reﬁned model [22].

Regardless of whether training is done using a single node,
or is distributed across several such as in federated [23]
and distributed learning, single node performance and energy
efﬁciency remains critically important. While training is both
computationally and data demanding, it is memory transfers
that dominate overall execution time and energy [24]. As
reviewed in Section III, energy and time are dominated by
data transfers for stashing (saving and much later recovering)
activation and weight tensors [24]. Chen et al., have proposed
recomputing activation tensors rather than stashing them [25]
shifting the energy costs to compute rather than memory.
Data blocking the form of micro-batching improves memory
hierarchy utilization [26].

Another class of relevant solutions revisit the datatype used
during training. Initially, training used single precision ﬂoating-
point data and arithmetic as the focus was on demonstrating that
neural networks can indeed tackle challenging problems. Since
then, more compact datatypes such as half precision ﬂoating-
point FP16, BFloat16 [27]–[29], dynamic ﬂoating-point [30],
and ﬂexpoint [31] have been introduced reducing overall trafﬁc,
and improving energy efﬁciency for computations and memory.
Rather than using a single datatype throughout, which has to
provide the precision needed by all data and computations,
other methods explored using a combination of datatypes where
more compact datatype are used for some if not most data and

1

 
 
 
 
 
 
computations [30], [32]–[34]. Finally, methods that use low
bitlength arithmetic have also been proposed [35].

Complementing the aforementioned methods are lossless
and lossy compression approaches that use fewer bits to
represent stashed tensor content thus reducing overall trafﬁc
and footprints while improving energy efﬁciency and overall
execution time [24], [36]–[38]. By boosting the effective
capacity of each node’s memory compression techniques
can also reduce inter-node trafﬁc during distributed training.
The methods target speciﬁc value patterns such as zeros, or
take advantage of the underlying mathematical properties of
training to use efﬁcient representations of certain tensors (such
as the outputs of ReLU or pool layers). Gist demonstrated
that some models can be trained using tighter ﬂoating-point
representations where the mantissa and exponent are using
fewer bits compared to Bﬂoat16 or FP32 [24]. Gist does not
offer a method for determining which representation to use in
advance. The user has to try to train the network to determine
whether it can still converge. Furthermore, Gist does not adjust
the representation which remains the same throughout training.
This work explores Schr¨odinger’s FP, a family of lossy
compression methods that complement prior aforementioned
training acceleration methods. Schr¨odinger’s FP explores
whether it is possible to dynamically and continuously adjust the
mantissa bitlength (how many factional bits) and the container
(how many bits overall) for ﬂoating-point values for stashed
tensors and to do so as much as possible transparently (without
anyone looking). That is, Schr¨odinger’s FP is designed to work
with existing training methods where it encodes values as they
are being stashed to off-chip memory, and decodes them to
their original format (we study FP32 and BFloat16) as they
are being read back. As a result, Schr¨odinger’s FP can be used
with any training hardware without requiring changes to the
existing on-chip memory hierarchy and compute units.

Fundamentally, compression relies on non-uniformity in the
data value distribution statically and temporally. It is instructive
to compare and contrast the behavior of ﬂoating-point values
during training with the ﬁxed-point values commonly used
during inference as many techniques have capitalized on the
properties of the value stream during inference [36], [39]–
[44]. During inference: 1) Zeros are very common especially
in models using ReLU. 2) The distribution of weights and
activations during inference tend to be heavily biased around a
centroid (often zero), with very few values having a magnitude
close to the extremes of the value range and many “medium
magnitude” values being exceedingly rare. This results in many
values having long bit preﬁxes of 0 or 1 making it proﬁtable
to adapt the container bitlength accordingly. 3) The weights
are statically known and can be pre-encoded. 4) Activations
tend to be used quickly after production and may not need to
be stored to off-chip memory. 5) It is possible to trim some of
the least signiﬁcant bits which are “noisy” yet not useful [45]
further skewing the value distribution.

In contrast to inference, adapting the container and bitlength
of ﬂoating-point values during training has to content with
the following challenges: 1) be it weights or activations, the

values are changing continuously, 2) the initial values are
random, 3) ﬂoating-point values comprise a sign, exponent,
and a normalized mantissa where a skewed value distribution
does not necessarily translate into a skewed distribution at the
bit-level, 4) training often does minute updates to larger values
and any method has to accommodate those updates which
cannot be discarded as noise, and 5) activation values have to
be stashed over much longer periods of time (produced during
the forward pass and consumed during the backward pass).

Since the values are continuously changing at runtime, a
dynamic method is desirable. Promisingly, while the initial
values are random, training relatively quickly modiﬁes them
resulting in overall distributions that are non-uniform thus
giving compression an opportunity to be effective. However,
a challenge remains as this non-uniformity is obstructed by
the use of a normalized mantissa coupled with an exponent.
Speciﬁcally, even though the values produced after a while
resemble the non-uniform distributions seen during inference,
the normalized mantissa lacks the 0 or 1 bit preﬁxes that are
common in ﬁxed-point. At the same time, a ﬁxed-point repre-
sentation lacks the range and potentially the bitlengths needed
to support the wide range of updates that stochastic gradient
decent performs. Finally, with carefully chosen dataﬂow most
of the gradients can be produced and consumed on-chip.

To address the aforementioned challenges Schr¨odinger’s FP
uses different approaches for the mantissa and exponent which
we study with FP32 and BFloat16. Schr¨odinger’s FP dynam-
ically adjusts mantissa bitlengths in order to store and read
fewer bits per number in off-chip memory. This work explores
explores two such mantissa bitlength adaptation methods. The
ﬁrst, Quantum Mantissa harnesses the training algorithm itself
to learn, on-the-ﬂy the mantissa bitlengths that is needed per
tensor/layer and continuously adapts those bitlengths per batch.
Quantum Mantissa introduces a single learning parameter per
tensor/layer and a loss function that measures the effects of the
current bitlength selection. Learning these bitlength paramaters
incurs a negligible overhead compared to the savings from the
resulting reduction in off-chip trafﬁc. The experiments with
Quantum Mantissa show that: 1) it is capable to reduce the
mantissa bitlengths considerably (for example, down to 4 to 1
bits depending on the layer and epoch with BFloat16), and 2)
the reductions are achieved fairly soon in the training process
and remain relatively stable till the end (e.g., after epoch 5).
However, the bitlengths vary per layer/tensor and ﬂuctuate
throughout capturing beneﬁts that wouldn’t be possible with a
static choice of bitlengths.

Motivated by the success of Quantum Mantissa, we explore
the second mantissa adjustment method, BitChop, which is
a history-based, hardware-driven approach that requires no
additional loss function and parameters. BitChop interfaces
with the otherwise unmodiﬁed training system only in that it
needs to be notiﬁed of per batch updates to the existing loss.
Using an exponential moving average of these changes to the
loss, to adjust mantissa bitlength for the whole network. As long
as the network seems to be improving, BitChop will attempt
to use a shorter mantissa, otherwise keep it the same or even

2

increase it. The method proves effective, albeit it achieves lower
reductions in mantissa length compared to Quantum Mantissa.
This is expected since: 1) Quantum Mantissa harnesses the
existing training process to continuously learn what bitlengths
to use, and 2) Quantum Mantissa adjusts bitlengths per layer
whereas BitChop uses one bitlength for all layers.

For the exponents we corroborate past ﬁndings that most
of the exponents during training exhibit a heavily biased
distribution [46]. Accordingly, Schr¨odinger’s FP uses a value-
based approach where it stores exponent using only as many bits
as necessary to represent their magnitude plus sign. Metadata
encode the number of bits used. To reduce the overhead of
this metadata, Schr¨odinger’s FP encodes exponents in groups.
compres-
sors/decompressors that operate on groups of otherwise
unmodiﬁed ﬂoating-point values be it FP32 or BFloat16.
The compressors accept an external mantissa length signal
and pack the group of values using the aforementioned
compression methods for the mantissas and exponents. The
decompressors expand such compressed blocks back to into
the original ﬂoating-point format.

Section V presents

hardware

efﬁcient

We study Schr¨odinger’s FP with ResNet18 [47] and Mo-
bileNet V3 [48] trained on ImageNet [49]. For clarity, we
report detailed results with ResNet18 with BFloat16 throughout
the paper, concluding with overall performance and energy
efﬁciency measurements for all models.

We highlight the following experimental ﬁndings:
• Schr¨odinger’s FP compression techniques ﬁnd the neces-
sary mantissa and exponent bitlengths to reduce memory
footprint without noticeable loss of accuracy: SFP QM
reduces ResNet18 down to 14.7% and MobileNet V3
to 23.7% and SFP BC to 24.9% and 27.2%,
Small
respectively

• Schr¨odinger’s FP compressor/decompressor contributions
exploit the reduced footprint to obtain 2.34× and 2.12×
performance improvement for SFP QM and SFP BC ,
respectively

• Crucially, Schr¨odinger’s FP excels at squeezing out energy
savings with on average, 5.17× and 3.77× better energy
efﬁciency for SFP QM and SFP BC , respectively

II. RELATED WORK
Schr¨odinger’s FP will generally work in conjunction with
methods that partition, distribute, and reschedule the training
work. Accordingly, we limit attention to methods that reduce
the container size or the datatype used during training.

Gist demonstrated that the bulk of the memory transfers
during training is due to stashed activations [24]. Gist’s two
main compression methods target two classes of activations:
those between a ReLU and a Pooling layer, and those between
a ReLU and a Convolutional layer. For the ﬁrst class it shows
that one bit is enough, whereas for the second it takes advantage
of the sparsity caused by ReLU and avoids storing the resulting
zeros. As a third method, Gist explores whether using a reduced
length ﬂoating-point format — which is chosen in advance
and kept the same throughout — still allows the network to

3

converge. It shows that some models can still converge and the
bitlength needed varies across models. Gist determines this per
model bitlengths post-mortem, that is only after performing
a full training run. Regardless, this choice is useful when
a network has to be routinely retrained. In contrast to Gist,
Schr¨odinger’s FP discovers on-the-ﬂy which representation to
use and does so continuously throughout the training process
adapting as needed. Moreover, for exponents it takes advantage
of their skewed value distribution adapting the number of bits
used to their actual values instead of using a one-size-ﬁts-all.
Mixed-precision methods use a mix of ﬁxed-point and
ﬂoating-point operations and values [30], [34]. The methods
still use pre-decided and ﬁxed during the training process
data formats. Furthermore, they require modiﬁcations to the
training implementation to take advantage of this capability.
Schr¨odinger’s FP does not modify the training implementation,
adjusts the data type and containers continuously, and encodes
exponents based on their content. Xiao et al., demonstrated
that training some models is possible performing the bulk of
computations over extremely narrow formats using 4b (forward)
and 8b (backward) albeit at some reduction in accuracy (roughly
2% absolute for MobileNet V2/ImageNet) [50]. In principle,
given Schr¨odinger’s FP’s dynamic and content-based approach,
it should be possible to combine it with such training methods
boosting overall beneﬁts. However, further study is needed to
determine whether this is possible and effective.

Sakr et al., present an analytical method for predicting
the accumulation bitlengths needed per layer/block with the
main goal of reducing computation costs at
the ﬂoating-
point units [51]. The method produces tight, yet ﬁxed-size
formats for the whole training process. Schr¨odinger’s FP
is a dynamic approach that also takes advantage of value
content for exponents and that does not modify the training
implementation. Studying the interaction and potential synergy
with such analytical methods is interesting future work.

III. THE TRAINING PROCESS

Our goal is to make training more efﬁcient. Accordingly,
we will ﬁrst brieﬂy cover how the training process enfolds and
indicate where the most costly actions are. Furthermore, one
of our methods involves modifying the training process and
that discussion will learn on the information in this section.

language processing, recommendation systems,

A task of a neural network can be almost anything, from
to
natural
computer vision [52]. While many tasks are best solved with
feed-forward networks, some perform better with feed-back
connections. However, feed-back connections can be unraveled
in time to produce feed-forward networks [52]. Consequently,
we focus on feed-forward networks.

The goal of training a network is to determine the parameters
that will best solve the given task. Without loss of generality,
we will focus on image classiﬁcation networks as they are
simplest to explain, were the ﬁrst ones drive the resurgence
in interest in deep learning, and remain as the most common
benchmark for hardware evaluation.

Fig. 1: Training process and its memory transfers. Numbers represent the equations that are computed. Blue - Activations
that are typically saved to off-chip memory during forward pass and retrieved during backward pass, Red - Weights that are
typically stored and loaded once from off-chip memory, Gray - Updates and Gradients – through mini-batching during the
backward pass they can often ﬁt on-chip.

In order to explain the training process, we will describe Gra-
dient Descent by introducing the loss function in Section III-A,
the forward pass in which we evaluate the performance
of the network in Section III-B and the backward pass in
which the parameters are updated in Section III-C. Finally, in
Section III-D we will explain the scale of the problem. Figure 1
provides an overview of the training process illustrating tensor
transfers.

A. The Loss Function

Training explicitly targets improving the value of a Loss
Function. This is a function that acts as a proxy for the accuracy
at solving the desired task. The requirements of this function
is that it is differentiable and that the value of the function
reduces as the output network approaches the desired value and
the certainty of that output increases. In this case, the smaller
the loss function, the more desirable the network performance.
If these conditions are satisﬁed, it is fairly simple to devise
updates for all parameters. The partial derivative of the loss
function in respect of each parameter indicates how a change of
the parameter will affect the loss function. Since our goal is the
minimize the loss function, we want to bump each parameter
in the opposite direction of the partial derivative:

wi

l = wi

l − LR ×

δL
δwi

l

(1)

where wi

l represents the i − th weight of layer l, LR
represents the learning rate and L represents the Loss function.
We repeat this procedure for every parameter, for many
inputs to slowly reach the weights at which the loss function is
minimal, and as a result the accuracy is maximized. Crucially, if
we want to optimize some property, say weight values, number
of operations or memory footprint, we can include this in our
Loss function and use the same procedure. In order to use this
procedure to update the weight, every operation we use in the
network must be differentiable.

B. Forward Pass

In the forward pass, we input the training example and
calculate the Loss Function by sequentially calculating all the
activations in order. This involves simply computing the output
of each layer l sequentially:

ai

l+1 = F l

(cid:33)

wi,j

l × ai,j

l

(cid:32)

(cid:88)

i

(2)

4

WgtsWgtsWgtsWgtsWgtsWgtsWgtsWgtsForward PassBackward PassUpdUpdUpdUpd(2)(2)(2)(2)(3)(4)(4)(4)(3)(3)(3)(3)Activations in Off Chip MemoryGradsGradsGradsGradsGradsActsActsActsActsActsActsActsActsActsLossInputwhere ai

l and ai,j

l+1 represents the i − th activation of layer l + 1,
l represent the weights and activations of layer
wi,j
l+1 and F l is a non-linear
l that are directly connected to ai
differentiable activation function of the l − th layer. The input
activation of each layer, for every input, needs to be saved to
in order to compute the backward pass.

C. Backward Pass

The backward pass utilizes the chain rule to progressively
ﬁnd the partial derivative of the loss function with respect
to the weights. In this case, the reuse of calculated values is
maximized. The process involves two steps for every layer,
applied in reverse order, from the last to the ﬁrst:

1) Weight Update: First we can compute the input the

updated weights of the current layer:

wi

l = wi

l − LR ×

ai,j

l ×

(cid:88)

j

δL

l+1

δaj

(3)

where LR is the learning rate, ai,j
connected to the weight and
from the l + 1 − th layer.

δaj

l are the activations
δL
l+1 is the activation gradient

2) Propagate Gradient Backward: Second, we propagate
the gradient backward into the previous layer by computing:

δL
δai

l =

(cid:88)

j

wi,j

l ×

δL
l+1 ×

δaj

δF l
l
δaj

l(cid:1)

(cid:0)aj

(4)

l and × δL
δaj

where wi,j
l+1 are all weight/output gradient pairs
connected to the input gradient and F l is the activation function
of layer l. Only at this point can the activations of this layer
be discarded.

We repeat these steps all the way to the input layer. Once
this is done we have completed one weight tensor update. We
repeat this for every input batch, for many epochs.

D. So, why is training expensive?

While training is expensive both compute- and memory-
wise, off-chip memory accesses dominate the energy and time
costs [24] since computing the weight updates, necessitates
retrieving the activations from the forward pass. This is a
massive amount of data. For ImageNet, with reasonable batch
sizes, the volume of activations is on the order of gigabytes
far exceeding practical on-chip capacities.

E. Gradient Descent

The core of training remains the Gradient Descent procedure:
slowly, step-by-step moving against the gradient to ﬁnd the
parameters that produce the minimum loss. This algorithm is
being continuously improved to ﬁnd better minima quicker
and cheaper. However, the basics and hardware implications
remain the same; lots of compute and even more memory.

5

F. Our Contribution

Our goal is to reduce the energy and time cost of training
by leveraging machine learning and hardware techniques to
reduce memory footprint and trafﬁc. We do this by selecting
an elastic datatype and container coupled with light-weight,
custom encoder/decoder hardware that exploits it to reduce
off-chip trafﬁc and footprint. We propose:

1) two lossy, but controlled, techniques to reduce the number
of mantissa bits. Since mantissas are normalized and almost
uniformly distributed, our approach is to trim the mostly noisy,
least signiﬁcant bits with one of two, real-time novel techniques:
• Quantum Mantissa: A machine learning technique to ﬁnd
required mantissa bitlength introduced in Section IV-A.
This technique involves a low-overhead modiﬁcation of
gradient descent to “learn” ﬁne-grained (per tensor/layer)
mantissa requirements during training.

• BitChop: A heuristic based technique that ﬁnds the
activation mantissas in Section IV-B. This method tracks
the current loss function and deciding whether to add,
remove, or keep the same the activation mantissa bitlength
at network-level granularity.

2) Gecko: A loss-less compression method for exponents by
exploiting their favorable normal distribution (Section IV-C).
This method relies on delta encoding and a ﬁne-grained
approach to signiﬁcantly reduce the exponent footprint of both
weights and activation.

3) A hardware architecture to exploit our custom datatype
in Section V and deliver energy and performance beneﬁts for
neural network training.

IV. ADJUSTING VALUE CONTAINERS DURING TRAINING

The ﬁrst step to reducing the energy and time cost of training
is to deﬁne an efﬁcient datatype. In general, training requires a
ﬂoating-point approach in order to maintain accuracy on most
real-world tasks. Floating-point formats consist of three distinct
segments: a mantissa, an exponent, and a sign bit. Mantissas
and exponents are differently distributed, so they require
different approaches. The greatest challenge is compressing
mantissas since they are uniformly distributed across the
domain whereas compression exploits non-uniformity. We will
present two approaches to compresses mantissas, an machine
learning approach in Section IV-A and a hardware design
inspired approach in Section IV-B. In contrast exponents can
be compressed with fairly simple hardware techniques as shown
in Section IV-C. We will brieﬂy comment on the implications
of the sign bit in Section IV-D. Finally, Section VI-A will
bring it all together and show the full effects of our approach.

A. Mantissa: Quantum Mantissa

Quantum Mantissa involves procedures for both the forward
and backward passes of training. We begin by deﬁning a con-
ventional quantization scheme for integer mantissa bitlengths
in the forward pass and then expand it it to the non-integer
domain and then describe how this interpretation allows
bitlengths to be learned using gradient descent. Subsequently,
we introduce a parameterizable loss function, which enables

Quantum Mantissa to penalize larger bitlengths. We then
brieﬂy touch on the compute overhead of our method and
the plan for ﬁnal selection of mantissa bitlengths. Ultimately,
we demonstrate the beneﬁts of Quantum Mantissa on memory
footprint during ImageNet training.

1) Quantization: The greatest challenge for

learning
bitlengths is that they represent discrete values over which
there is no obvious differentiation. To overcome this challenge
we deﬁne a quantization method on non-integer bitlengths. We
start with an integer quantization of the mantissa M with n
bits by zeroing out all but the the top n bits:

Q(M, n) = M ∧ (2n − 1) << (m − n)

(5)

where Q(M, n) is the quantized mantissa with bitlength n, m
is the maximum number of bits and ∧ represents bitwise AND.
Throughout training, we represent the integer quantization as
Q(M, n). This quantization scheme does not allow the learning
of bitlengths with gradient descent due to its discontinuous
and non-differentiable nature. To expand the deﬁnition to real-
valued n = (cid:98)n(cid:99) + {n}, the values used in inference during
training are selected as a random choice between the nearest
two integers with probabilities {n} and 1 − {n}:

Q(M, n) =

(cid:40)

Q(M, (cid:98)n(cid:99)),
Q(M, (cid:98)n(cid:99) + 1), with probability {n}

with probability 1 − {n}

(6)
where (cid:98)n(cid:99) and {n} are ﬂoor and fractional parts of n,
respectively. The scheme can be, and in this work is, applied to
activations and weights separately. Since the minimum bitlength
per value is 0, n is clipped at 0. This presents a reasonable
extension of the meaning of bitlength in continuous space and
allows for the loss to be differentiable with respect to bitlength.
During the forward pass the above formulae are applied to
both activations and weights. The quantized values are saved
and used in the backward pass. During the backward pass
we use the straight-through estimator [53], [54] to prevent
propagating zero gradients that result from the discontinuity’s
discreteness, however, we use the quantized mantissas for all
calculations. This efﬁcient quantization during the forward pass
reduces the footprint of the whole process.

2) Loss Function: On top of ﬁnding the optimal weights,
the modiﬁed loss function penalizes mantissa bitlengths by
adding a weighted average (with weights λi not be confused
with the model’s weights) of the bits mi required for mantissas
of weights and activations. We deﬁne total loss L as:

L = Ll + γ

(cid:88)

(λi × ni)

(7)

where Ll is the original loss function, γ is the regularization
coefﬁcient used for selecting how aggressive the quantization
should be, λi is the weight corresponding to the importance of
the ith group of values (one per tensor), and ni is the bitlength
of the activations or weights in that tensor.

This loss function can be used to target any quantiﬁable
criteria by a suitable selection of the λi parameters. Since our
goal is to minimize the total footprint of a training run, we
weight each layer’s tensors according to their footprint.

6

3) Computational and Memory Overheads: Quantum Man-
tissa adds minimal computational and memory overhead to
the forward and backward passes. In the forward pass, random
numbers need to be created at a chosen granularity to determine
the quantized values. Ideally this would be done per value,
however, our experiments show that per tensor/layer is sufﬁcient
and is a negligible cost.

To update the bitlength parameters in the backward pass, we
need to compute their gradients. These are a function of the
weight values and gradients, which will be calculated as part
of regular backward pass. As a result, the extra calculations
for each bitlength is on the order of O(n), where n is the
number of values quantized to that bitlength. This overhead is
negligible in comparison to the total number of computations.
In the case of our experiments, the overhead is less than 2%
and 0.5% for MobileNet V3 and ResNet18, respectively.

On the memory side, the only extra parameters that need to
be saved are the bitlengths, two ﬂoats per layer (bitlength for
weights and activations), again negligible in comparison with
the total footprint. All other values are consumed as they are
produced without need for off-chip stashing.

4) Bitlength Selection: The above training method will
produce non-integer bitlengths and requires stochastic inference
(given a fractional bitlength we choose one of the surrounding
integer bitlengths at random per tensor). We prefer the network
not to have this requirement when deployed. For this reason,
we round up the bitlengths and ﬁx them for some training time
to ﬁne-tune the network to this state. While our experiments
show that bitlengths converge quickly and ﬁnal ones can be
determined within a couple of epochs avoiding the small
overhead for most of training, we aim to delay this action
so that bitlengths have the ability to increase if needed during
training. Our experiments show that this unnecessary for the
models studied, however, the overhead is so small that we
leave it on as a safety mechanism. We round up the bitlengths
for the last 10 epochs to let the network regain any accuracy
that might have been lost due to Quantum Mantissa. Quantum
Mantissa still reduces trafﬁc during these epochs.
Evaluation: BitLengths and Accuracy: We report measure-
ments for per-layer weights and activations quantized separately
using a loss function weighted to minimize total memory
footprint. We train ResNet18 [55] on the ImageNet [49] dataset
over 90 epochs, with 0.1 weight decay, setting the regularizer
strength to 0.1, 0.01 and 0.001 respectively at epochs 0, 30,
and 60.

The goal of Quantum Mantissa is to tap onto the learn-
ing algorithm to minimize the memory footprint whilst not
introducing accuracy loss. It excels at this goal. Figure 2
shows that throughout training Quantum Mantissa introduces
minimal changes in validation accuracy. In the end, the network
converges to a solution within 0.4% of the FP32 baseline.

Figure 3 shows how Quantum Mantissa quickly (within a
couple of epochs) reduces the required mantissas for activations
and weights down to 1 − 2 bits on average. Throughout
training, the total cumulative memory footprint is reduced
to 7.8% and 25.5% of the FP32 and BFloat16 mantissa

Fig. 2: Quantum Mantissa: ResNet18 validation accuracy
throughout training

Fig. 4: Quantum Mantissa: Mantissa bitlengths for each layer
of ResNet18 at the end of each epoch. Darker dots represent
the latter epochs

Fig. 3: Quantum Mantissa: ResNet18 weighted mantissa
bitlengths with their spread throughout training

footprint, respectively. The ﬁgure further shows that there
is a large spread across different layers, indicating that a
granular, per-layer, approach is the right choice to maximize
beneﬁts. Quantum Mantissa generally targets the activation
bitlengths more aggressively than the weights as the activations
are responsible for the majority of the memory footprint. This
is contrary to the case when the bitlengths are not weighted
according to their importance and the weights end up being
smaller than the activations.

The spread of mantissa bitlengths across the network and
time is shown in Figure 4. While most layers quickly settle at
1 or 2 bits, there are a couple of exceptions that require more,
at times up to 4b. Because of this spread, a network-scale
datatype would have to use the largest datatype and leave a
lot of the potential untapped. For ResNet18, the maximum
bitlength is over 2× larger than the weighted average.

B. Mantissa: BitChop

While Quantum Mantissa leverages training itself to greatly
reduce mantissa lengths, having a method that does not require
introducing an additional loss function and parameters would be
appealing. BitChop is a run-time, heuristic method to reduce the
number of mantissa bits for the forward and backward passes.
At a high-level, BitChop monitors how training progresses
adjusting the mantissa length accordingly: as long as the
training seems to be improving the network, BitChop will
attempt to use a shorter mantissa, otherwise it will try to

increase its bitlength. Figure 5 illustrates BitChop’s “observe
and adjust” approach. BitChop conceptually splits the training
process into periods, where a period is deﬁned as processing
N batches. BitChop adjusts the mantissa at the end of each
period using information about network progress.

Fig. 5: BitChop’s Observe/Decide Approach

Fig. 6: BitChop: ResNet18/ImageNet validation accuracy
throughout BFloat16 training

The ideal scenario for BitChop is one where past obser-
vation periods are good indicators of forthcoming behavior.
Macroscopically, network accuracy improves over time during
training, which appear to be a good ﬁt. Microscopically,
however, training is a fairly noisy process, a reality that BitChop
has to content with. Fortunately, training is a relatively long
process based on “trial-and-error” which may be forgiving for
momentary lapses in judgement.

There are three major design decisions that impact how
successful BitChop can be: 1) what information to use as a
proxy for network progress, 2) how long the period should
be, and 3) at what granularity to adjust mantissa lengths. The
resulting method should strike a balance between capturing as

7

much as possible of the opportunity to reduce bitlength, while
avoiding over-clipping as this can hurt learning progress and
ultimately the ﬁnal accuracy that will be achieved.

We have experimented with several options and arrived at
the following choices: 1) Using an exponential moving average
of the loss as a proxy for network progress, and 2) using a
short period where N = 1, that is a single batch. Moreover,
rather than attempting to adjust mantissas at the tensor/layer
level, BitChop uses the same mantissa for the whole model.
In more detail, to monitor network progress, BitChop uses
the loss which is calculated per batch as part of the regular
training process. While the loss improves over time, when
observed over short periods of time, it exhibits non-monotonic
behavior which is sometimes erratic. To compensate for this
volatility, BitChop uses an exponential moving average Mavg
which it updates at the end of each period:

M avg = M avg + α ∗ (Li − M avg)

(8)

Fig. 7: BitChop: ResNet18 average mantissa bitlengths per
epoch throughout training, on BFloat16 and FP32.

where Li is the loss during the last period and α is an
exponential decay factor which can be adjusted to assign more
or less signiﬁcance to older loss values. This smooths the loss
over time while giving importance to the most recent periods.
At the end of each period i, BitChop decides what the
mantissa bitlenth, ni+1, should be for the next period i + 1 as
follows:

ni+1 =






ni − 1, when M avgi > Li + (cid:15)
ni,
ni + 1, when M avgi < Li − (cid:15)

when Li − (cid:15)i ≤ M avgi ≤ Li + (cid:15)i

(9)

Fig. 8: BitChop: ResNet18 - Bﬂoat16, distribution of BitChop’s
mantissa bitlengths throughout the 5005 batches of epoch 45
in ImageNet training.

where Li and ni are respectively the loss and the mantissa
length of period i, and (cid:15)i is a threshold calculated with the
average relative error between Li and M avgi. Full precision
is used during LR changes, as the network is more sensitive.
BitChop is implemented as a simple hardware controller which
is notiﬁed of the loss via a user-level register in memory. The
only modiﬁcation to the training code is for updating this
register once per period.
Evaluation: BitLengths and Accuracy: We measure the
effectiveness of BitChop by reporting its effect on memory
footprint and accuracy during full training sessions of ResNet18
as described for Quantum Mantissa. Presently, BitChop adjusts
the mantissa only for the activations. Figure 6 shows that with
BitChop, the network achieves the same validation accuracy as
with the baseline training. For clarity, the ﬁgure shows results
for BFloat16 only (results with FP32 were similar and accuracy
was unaffected). Throughout the training process, validation
accuracy under BitChop exhibits more pronounced swings
compared to the baseline and to Quantum Mantissa. However,
in absolute terms, these swings are small as the validation
accuracy under BitChop closely follows that observed with
baseline training.

Figure 7 shows that BitChop reduces mantissa bitlengths to
4 - 5 bits on average when used over BFloat16 and to 5 - 6
bits on average when used over FP32. However, the mantissa
bitlength may vary per batch depending on the loss. This is
illustrated in Figure 8, through a histogram of the bitlengths

Fig. 9: ResNet18: Distribution of exponent values

used throughout a sample epoch (epoch 45) for the BFloat16
run. This shows that while BitChop is consistently reducing the
mantissa bitlength to 2 - 4 bits, the training process sometimes
requires the entire range of Bﬂoat16. All across the training
process, BitChop reduces the total mantissa footprint of the
BFloat16 baseline to 64.3%, and over FP32 BitChop reduces
mantissa footprint to 56.1%.

While BitChop’s network-level granularity might miss po-
tential bitlength reductions, the method does not require extra
trainable parameters or any modiﬁcation to the training process.

8

the remaining exponents are stored using the bitlength chosen.
Alternatively, a ﬁxed bias can be used to encode the exponents.
In this case, a predetermined bias b is used to encoded the
exponents in memory were an exponent E is stored as E − b
and with as many bits as necessary to store the magnitude of
this difference. The bias can be programmed up-front or can
be ﬁxed in the design. A bias of 127 was found to be best for
the models studied. When this scheme is used, exponents can
be encoded in smaller groups. We have found that a group of
8 works very well in practice.
Evaluation: BitLength: We measure how many bits are
needed to encode the exponents using Gecko and for the
duration of training of ResNet18 as described previously. For
clarity, we limit attention to BFloat16. As a representative
measurement, Figure 10 reports the cumulative distributions of
exponent bitlength for one batch across 1) all layers, and 2) for
a single layer, and separately for weights and activations. After
delta encoding, almost 90% of the exponents become lower
than 16. Further, 20% of the weight exponents and 40% of the
activation exponents end up being represented using only 1 bit.
Across the whole training process, the overall compression ratio
for the weight exponents is 0.56 and 0.52 for the activation
exponents. The ratio is calculated as (M + C)/O where M
the bits used by the per group bitlength ﬁelds, C the bits used
to encode the exponent magnitudes after compression, and O
the bits used to encode exponents in the original format.

D. Sign

One of the simplest and most common activation functions
used for CNNs is the Linear Rectiﬁer (ReLU), which zeroes
out all negative values. As a result, ReLU outputs can be stored
without the sign bit.

V. OUR HARDWARE APPROACH

This section presents the Schr¨odinger’s FP hardware en-
coder/decoder units that efﬁciently exploit the potential created
by our quantization schemes. Without the loss of generality
we describe compressors/decompressors that process groups
of 64 FP32 values.

A. Compressor

Figure 11a shows that the compressor contains 8 packer units
(Figure 11c). The compressor accepts one row (8 numbers)
per cycle, for a total of 8 cycles to consume the whole group.
Each column is treated as a subgroup whose exponents are to
be encoded using the ﬁrst element’s exponent as the base and
the rest as deltas. Accordingly, the exponents of the ﬁrst row
are stored as-is via the packers. For the every subsequent row,
the compressor ﬁrst calculates deltas prior to passing them to
the packers.

The length of the mantissa is the same for all values and
is provided by the mantissa quantizer method be it Quantum
Mantissa or BitChop. Each row uses a container whose bitlength
is the sum of the mantissa bitlength (provided externally) plus
the bitlength needed to store the highest exponent magnitude
cross the row. To avoid wide crossbars when packing/unpacking,

Fig. 10: ResNet18: Post-encoding distribution of exponent
bitlength

Nor does it introduce any explicit overhead, making it a plug-in,
albeit ad-hoc adjustment to the training process.

C. Exponent: Gecko

The exponents of BFloat16 and FP32 are 8b biased ﬁxed-
point numbers. Except for a few early batches, we ﬁnd that
during training the exponent values exhibit a heavily biased
distribution centered around 127 which represents 0. This is
illustrated in Figure 9 which reports the exponent distribution
throughout training of Resnet18 after epoch 10 (the ﬁgure
omits gradients which are even more biased as those can
be kept on-chip). Taking advantage of the relatively small
magnitude of most exponents we adopt a variable length
lossless exponent encoding. The encoding uses only as many
bits as necessary to represent the speciﬁc exponent magnitude
rather than using 8b irrespective of value content. Since we
use variable-sized exponents, a metadata ﬁeld speciﬁes the
number of bits used. Having a dedicated bitlength per value
would negate any beneﬁts or worse become an overhead. To
amortize the cost of this metadata several exponents share
a common bitlength which is long enough to accommodate
the highest magnitude within the group. We further observed
that especially for weights, the values often exhibit spatial
correlation, that is values that are close-by tend to have similar
magnitude. Encoding differences in value skews the distribution
closer to zero beneﬁting our encoding scheme.

In the implementation we study the encoding scheme Gecko
used is as follows: Given a tensor, Gecko ﬁrst groups the
values in groups of 64 (padding as needed) which it treats
conceptually as an 8x8 matrix. Every column of 8 exponents
is a group which shares a common base exponent. The base
exponent per column is the exponent that appears in the ﬁrst
row of incoming data. The base exponent is stored in 8b.
The remaining 7 exponents are stored as deltas from the base
exponent. The deltas are stored as [magnitude, sign] format
and using a bitlength to accommodate the highest magnitude
among those per row. A leading 1 detector determines how
many bits are needed. The bitlength is stored using 3b and

9

values remain within the conﬁnes of their original format bit
positions as per the method proposed in Proteus [56]. In contrast
to Proteus, however, here every row uses a different bitlength,
the values are ﬂoating-point, the bitlengths vary during runtime
and per row, and we target training. The exponent lengths need
to be stored as metadata per row. These are stored separately
necessitating two write streams per tensor both however are
sequential thus DRAM-friendly. The mantissa lengths are either
tensor/layer- or network-wide and are stored along with the
other metadata for the model.

Each packer (Figure 11c) takes a single FP32 number in
[exponent, sign, mantissa] format, masks out unused exponent
and mantissa bits, and rotates the remain bits to position to ﬁll in
the output row. The mask is created based on the exp width and
man width inputs. The rotation counter register provides the
rotation count which is updated to (exp width+man width+1)
every cycle. The (L,R) register pair, is used to tightly pack the
encoded values into successive rows. There are needed since a
value may now be split across two memory rows. When either
register, its 32b (or 16b for BFloat16) are drained to memory.
This arrangement effectively packs the values belonging to this
column tightly within a column of 32b in memory. Since each
rows the same total bitlength, the 8 packers operate in tandem
ﬁlling their respective outputs at exactly the same rate. As a
result, the compressor produces 8x32b at a time. The rate at
which the outputs are produced depends on the compression
rate achieved, the higher the compression, the lower the rate.

B. Decompressor

As Figure 11b shows, the decompressor mirrors the compres-
sor. It takes 8 3-bit exponent widths and a mantissa length from
the system, and 8x32 bits of data per cycle. Every column of
32b is fed into a dedicated unpacker per column. The unpacker
(Figure 11d reads the exponent length for this row and the
global mantissa length, takes the correct number of bits, and
extends the data to [exponent, sign, mantissa] format.

Each unpacker handles one column of 32b from the incoming
compressed stream. The combine-and-shift will combine the
input data and previous data in register then shift to the left.
The number of shifted bits is determined by the exponent and
mantissa lengths of this row. The 32-bit data on the left of the
register are taken out and shifted to the right (zero extending
the exponent). Finally, the unpacker reinserts the mantissa bits
that were trimmed during compression. Since each row of data
uses the same total bitlength, the unpackers operate in tandem
consuming data at the same rate. The net effect is that external
memory see wide accesses on both sides.

VI. EVALUATION – PUTTING ALL TOGETHER

We study the following two Schr¨odinger’s FP variants:
Gecko/Quantum Mantissa (SFP QM ) and Gecko/BitChop
(SFP BC ) which are combinations of our exponent and mantissa
compression methods and the interaction with GIST++ [24], a
slightly modiﬁed version of Gist that uses sparsity encoding
only for those tensors where doing so reduces footprint avoiding

10

(a) Compressor

(b) Decompressor

(c) Packer

(d) Unpacker

Fig. 11: Schr¨odinger’s FP Compressors/Decompressors

We perform full

the increase in trafﬁc that would occur otherwise. This is
especially useful for MobileNet V3 that does not use ReLU.
training runs for ResNet18 [47] and
MobileNet V3 Small [48] over ImageNet [49] using an
RTX3090/24GB while using PyTorch v1.10. We implement
Quantum Mantissa by modifying the loss function and adding
the gradient calculations for the per tensor/layer parameters. We
simulate BitChop in software. For both methods, we faithfully
emulate mantissa bitlength effects by truncating the mantissa

bits at the boundary of each layer using PyTorch hooks and
custom layers. We also implement Gecko in software via
PyTorch hooks. The above enhancements allow us to measure
the effects our methods have on trafﬁc and model accuracy.

A. Memory Footprint Reduction

First we report activation and weight footprint reduction
on ResNet18. Combined, our compression techniques excel at
reducing memory footprint during training with little affect on
accuracy. Table I shows the cumulative total memory reduction
in comparison with FP32 and BFloat16 baselines and the
corresponding validation accuracies. Our compression methods
signiﬁcantly reduce memory footprint for all tested networks,
whilst not signiﬁcantly affecting accuracy.

1) SFP QM : : Figure 12 shows the relative footprint of
each part of the datatype with SFP QM in comparison with
the FP32 and Bﬂoat16 baseline. Even though our methods
are very effective at reducing the weight footprint (91% for
mantissas and 54% for exponents), this effect is negligible in
the grand scheme of things due to Amdhal’s law and the fact
that weights are a very small part of all three footprints. For
the same reason, the reductions in activation footprint (92%
for mantissas, 63% for exponents and 98% for sign) have a
far greater effect. Because of the effectiveness of Quantum
Mantissa mantissa compression, the mantissas are reduced
from the top contributor in FP32 (70%), to a minor contributor
(38%). While exponents are signiﬁcantly reduced too, they start
to dominate with 59% of the footprint in comparison with the
FP32 baseline at 24%. Similar conclusions are reached when
comparing with Bﬂoat16 except for the fact that in Bﬂoat16
mantissas and exponents have similar footprint.

2) SFP BC : Figure 12 also shows the relative footprint of
the datatype components under SFP BC when compared to
the FP32 and Bﬂoat16 baselines. While BitChop does reduce
mantissa bitlength for the network’s weights, this does not
have a great effect in the total memory footprint reduction
due to the small size of weights when compared to activations.
Although mantissa weight footprint is not reduced, weight
exponent footprint is by 56%. This is why the focus on
the activations’ mantissa bitlengths yields a signiﬁcant total
memory footprint reduction when compared to FP32 (mantissa
footprint is reduced by 81%, exponent footprint by 63% and
sign by 98% in activations), and a smaller but still signiﬁcant
reduction when compared with Bﬂoat16 (36% for mantissa and
63% for exponents). The reductions are not as great as with
Quantum Mantissa due to the network-wise limitation of the
method and activation mantissas stay as the major contributor
of footprint.

B. Relative compression

Finally we compare Schr¨odinger’s FP compression against
Bﬂoat16, GIST++ and JS, a simple sparse Bﬂoat16 zero-
compression method in Figure 13. JS uses an extra bit per
value to avoid storing zeros. We limit attention to activations
since we have since that overall weights represent a small
fraction of overall footprint and trafﬁc. All methods beneﬁt

11

Fig. 12: ResNet18 relative weight and activation footprint:
FP32, BF16, SFP BC and SFP QM . ImageNet with batch size
256

Fig. 13: Schr¨odinger’s FP: Comparison of cumulative activation
footprint with BF16, sparsity only and GIST++ compression

from the use of a 16 bit base. On ResNet18, JS and GIST++
beneﬁt from the 30% reduction due to high sparsity induced
by ReLu. GIST++ beneﬁts even further because of its efﬁcient
compression of maximum pooling. SFP BC does even better
just by ﬁnding a smaller datatype outperforming all of them,
whereas SFP QM proves even better by adjusting the datatype
per layer. However, SFP BC and SFP QM only target the
reduced datatype and there is opportunity to build on it with
same ideas that power JS and GIST++. When combined
this further improves compression ratios to 10× and 8× for
modiﬁed SFP QM and SFP BC .

MobileNet V3 small poses a bigger challenge since it
sparsely uses ReLu and uses no max pooling. Accordingly,
there is little potential for JS and GIST++ to exploit. SFP QM
and SFP BC still get another 2× compression over Bﬂoat16,
JS, and GIST++. Application of ideas from JS and GIST++ to
Schr¨odinger’s FP compression offers marginal gains.

C. Performance and Energy Efﬁciency

Since training these networks takes several days on actual
hardware, cycle-accurate simulation of the full process is im-
practical. To estimate performance and energy, we analytically
model the time and energy used per layer per pass of a
baseline accelerator using trafﬁc and compute counts collected
during the aforementioned full training runs. We record these
counts each time a layer is invoked using PyTorch hooks. We

TABLE I: SFP BC , SFP QM , BF16: Accuracy and total memory reduction vs. FP32.

Network
ResNet18
MobileNet V3 Small

FP32
Accuracy
69.94
65.60

Footprint
100%
100%

BF16
Footprint
50%
50%

SFP QM
Accuracy
69.54
65.26

Footprint
14.7%
24.9%

SFP BC
Accuracy
69.95
65.21

Footprint
23.7%
27.2%

TABLE II: Performance and Energy Efﬁciency gains in comparison with the FP32 baseline. Higher is better

Network
ResNet18
MobileNet V3 Small

Bﬂoat 16
1.53×
1.72×

Performance
SFP QM
2.30×
2.37×

SFP BC
2.15×
2.32×

Energy Efﬁciency
SFP QM
6.12×
3.95×

Bﬂoat 16
2.00×
2.00×

SFP BC
4.54×
3.84×

SFP QM may offer bigger performance beneﬁts if coupled
with higher computational performance hardware that would
reduce the computation time of the layers. Regardless, while
a reduction in trafﬁc may not yield a direct improvement in
performance, it does improve energy efﬁciency.

Table II also shows the improvement in energy efﬁciency
of Bﬂoat16, SFP QM and SFP BC over the FP32 baseline.
SFP QM and SFP BC excel at improving energy efﬁciency by
signiﬁcantly reducing DRAM trafﬁc. The energy consumption
of DRAM accesses greatly outclasses that of computation, and
due to DRAM accesses not being limited by compute bound
network layers, SFP QM and SFP BC are more impactful in
energy efﬁciency than in performance improvements, achieving
an average of 5.0× and 4.1× energy efﬁciency improvements
respectively. The dominance of DRAM access energy consump-
tion over computation can also be seen in Bﬂoat16, where the
reduction to half the footprint, the use of 16-bit compute units
and the compute layers being no longer a limiting factor gives
Bﬂoat16 a 2× energy efﬁciency improvement.

VII. CONCLUSION

We explored methods that dynamically adapt the bitlengths
and containers used for ﬂoating-point values during training.
The different distributions of the exponents and mantissas led
us to tailored approaches for each. In this work we targeted
the largest contributors to off-chip trafﬁc during training
activations and also the weights. There are several directions for
improvements and further exploration including expanding the
methods to also target the gradients and reﬁning the underlying
policies they use to adapt mantissa lengths. Regardless, this
work has demonstrated that the methods are effective. The
key advantages of our methods are: 1) they are dynamic and
adaptive, 2) they do not modify the training algorithm, 3) they
take advantage of value content for the exponents.

ACKNOWLEDGMENTS

This work was supported by a grant from the Samsung
Advanced Institute of Technology, the NSERC COHESA Strate-
gic Research Network and an NSERC Discovery Grant. This
manuscript was previously submitted to the 2022 International
Symposium on Computer Architecture.

model an accelerator with 8K units each capable of performing
4 MACs per cycle and with a 500MHz clock for a peak
computer bandwidth of 16TFLOPS. We consider two baseline
accelerators using respectively FP32 and BFloat16. Both have
8 channels of LPDDR4-3200 DRAM memory and 32MB of on-
chip buffers. We model time and energy for memory accesses
via DRAMSIM3 [57]. For modeling on-chip structures we
use CACTI [58] for the buffers and layout measurements for
the compute units and the Gecko compressors/decompressors.
We use a commercial 65nm process to model the processing
units and Gecko hardware. We implement the units in Verilog
and perform synthesis via the Synopsys Design Compiler
and layout using Cadence Innovus with a target frequency
of 500MHz. Synthesis uses Synopsys’ commercial Building
Block IP library for the target tech node. We estimate power
via Innovus using traces over a representative sample. There
are two Gecko compressor/decompressor units per channel.

To take advantage of data reuse where possible we perform
the forward pass in a layer-ﬁrst order per batch. This allows
us to read the weights per layer only once per batch. For the
backward pass, we utilize the on-chip buffers for mini-batching
with a layer-ﬁrst order over a mini-batch of samples. Mini-
batching reduces overall trafﬁc by processing as many samples
as possible in a layer-ﬁrst order avoiding either having to spill
gradients or reading and writing weights per sample per layer.
The number of samples that can ﬁt in a mini-batch depends
on the layer dimensions and the size of the on-chip buffer.

Performance improvements of Bﬂoat16, SFP QM , and
SFP BC over the FP32 baseline are shown in Table II. On
average, SFP QM and SFP BC produce a speed up of 2.3×
and 2.2× respectively, while Bﬂoat 16 speeds up by 1.6×.
Both SFP QM and SFP BC signiﬁcantly outperform both the
FP32 baseline and the Bﬂoat 16 on all networks. However,
performance does not scale linearly even though SFP QM and
SFP BC reduce the memory footprint to 14.7% and 23.7%
respectively. This is due to layers that were previously memory
bound during the training process now becoming compute
bound because of the reduction in memory footprint. This
is also the reason why even though Bﬂoat16 reduces the
data type to half, it does not achieve 2× speedup . This
transition of most layers from memory bound to compute bound
also affects the improvements in performance that SFP QM
can offer, as even though the method consistently achieves a
lower footprint than SFP BC , this only offers an advantage
for performance in the few layers that remain memory bound.

12

REFERENCES

[1] S. Venkataramani, A. Ranjan, S. Banerjee, D. Das, S. Avancha, A. Jagan-
nathan, A. Durg, D. Nagaraj, B. Kaul, P. Dubey, and A. Raghunathan,
“ScaleDeep: A scalable compute architecture for learning and evaluating
deep networks,” in Proceedings of
the 44th Annual International
Symposium on Computer Architecture, ISCA ’17, (New York, NY, USA),
pp. 13–26, ACM, 2017.

[2] D. Amodei, D. Hernadez, G. Sastry, J. Clark, G. Brockman, and

I. Sutskever, “Open AI Blog.”

[3] “NVIDIA

Tesla

Achitecture,”
V100
https://images.nvidia.com/content/volta-architecture/pdf/volta-
architecture-whitepaper.pdf, NVIDIA, 2017.

GPU

in

[4] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
S. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P.-l. Cantin,
C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb,
T. V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho,
D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski,
A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar, S. Lacy,
J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin,
G. MacKean, A. Maggiore, M. Mahony, K. Miller, R. Nagarajan,
R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda,
A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn,
G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan,
G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter,
W. Wang, E. Wilcox, and D. H. Yoon, “In-datacenter performance
analysis of a tensor processing unit,” in Proceedings of the 44th Annual
International Symposium on Computer Architecture, ISCA ’17, (New
York, NY, USA), pp. 1–12, ACM, 2017.

[5] “Gaudi

training platform white paper,” in https://habana.ai/wp-

content/uploads/2019/06/Habana-Gaudi-Training-Platform-
whitepaper.pdf, Habana, 2019.

[6] H. Liao, J. Tu, J. Xia, and X. Zhou, “DaVinci: A scalable architecture
for neural network computing,” in 2019 IEEE Hot Chips 31 Symposium
(HCS), pp. 1–44, 2019.

[7] “Cerebras CS1,” in https://www.cerebras.net/product/, Cerebras, 2019.
[8] M. Courbariaux, Y. Bengio, and J.-P. David, “Binaryconnect: Training
deep neural networks with binary weights during propagations,” in
Advances in Neural Information Processing Systems, pp. 3123–3131,
2015.

[9] M. Nikoli´c, M. Mahmoud, and A. Moshovos, “Characterizing sources
of ineffectual computations in deep learning networks,” in 2018 IEEE
International Symposium on Workload Characterization (IISWC), pp. 86–
87, 2018.

[10] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “HAQ: hardware-aware

automated quantization,” CoRR, vol. abs/1811.08886, 2018.

[11] M. Nikoli´c, G. B. Hacene, C. Bannon, A. D. Lascorz, M. Courbariaux,
Y. Bengio, V. Gripon, and A. Moshovos, “Bitpruning: Learning bitlengths
for aggressive and accurate quantization,” 2020.

[12] Y. L. Cun, J. S. Denker, and S. A. Solla, “Optimal brain damage,”
in Advances in Neural Information Processing Systems, pp. 598–605,
Morgan Kaufmann, 1990.

[13] T.-J. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convolu-

tional neural networks using energy-aware pruning,” 2017.

[14] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding,” in 4th International Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings (Y. Bengio and Y. LeCun, eds.), 2016.

[15] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A

survey,” Arxiv preprint arxiv:1808.05377, 2018.

[16] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian opti-
mization of machine learning algorithms,” in Proceedings of the 25th
International Conference on Neural Information Processing Systems -
Volume 2, NIPS2012, p. 2951...2959, 2012.

[17] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le,
M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y.
Ng, “Large scale distributed deep networks,” in Proceedings of the 25th
International Conference on Neural Information Processing Systems -
Volume 1, NIPS’12, (USA), pp. 1223–1231, Curran Associates Inc., 2012.
[18] R. Mayer and H. Jacobsen, “Scalable deep learning on distributed infras-
tructures: Challenges, techniques and tools,” CoRR, vol. abs/1903.11314,
2019.

13

[19] Y.-H. Chen, J. Emer, and V. Sze, “Eyeriss: A spatial architecture for
energy-efﬁcient dataﬂow for convolutional neural networks,” in ACM
SIGARCH Computer Architecture News, vol. 44, pp. 367–379, IEEE
Press, 2016.

[20] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li, “TernGrad:
Ternary gradients to reduce communication in distributed deep learning,”
in Advances in Neural Information Processing Systems 30 (I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, eds.), pp. 1509–1519, Curran Associates, Inc., 2017.
[21] K. Hegde, J. Yu, R. Agrawal, M. Yan, M. Pellauer, and C. W. Fletcher,
“UCNN: Exploiting computational reuse in deep neural networks via
weight repetition,” in Proceedings of the 45th Annual International
Symposium on Computer Architecture, ISCA ’18, (Piscataway, NJ, USA),
pp. 674–687, IEEE Press, 2018.

[22] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transactions
on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345–1359,
2010.

[23] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N.
Bhagoji, K. A. Bonawitz, Z. Charles, G. Cormode, R. Cummings,
R. G. L. D’Oliveira, H. Eichner, S. E. Rouayheb, D. Evans, J. Gardner,
Z. Garrett, A. Gasc´on, B. Ghazi, P. B. Gibbons, M. Gruteser, Z. Harchaoui,
C. He, L. He, Z. Huo, B. Hutchinson, J. Hsu, M. Jaggi, T. Javidi,
G. Joshi, M. Khodak, J. Koneˇcn´y, A. Korolova, F. Koushanfar, S. Koyejo,
T. Lepoint, Y. Liu, P. Mittal, M. Mohri, R. Nock, A. ¨Ozg¨ur, R. Pagh,
H. Qi, D. Ramage, R. Raskar, M. Raykova, D. Song, W. Song, S. U.
Stich, Z. Sun, A. T. Suresh, F. Tram`er, P. Vepakomma, J. Wang, L. Xiong,
Z. Xu, Q. Yang, F. X. Yu, H. Yu, and S. Zhao, “Advances and open
problems in federated learning,” Found. Trends Mach. Learn., vol. 14,
no. 1-2, pp. 1–210, 2021.

[24] A. Jain, A. Phanishayee, J. Mars, L. Tang, and G. Pekhimenko, “Gist:
Efﬁcient data encoding for deep neural network training,” in Proceedings
of the 45th Annual International Symposium on Computer Architecture,
ISCA ’18, (Piscataway, NJ, USA), pp. 776–789, IEEE Press, 2018.
[25] T. Chen, B. Xu, C. Zhang, and C. Guestrin, “Training deep nets with

sublinear memory cost,” CoRR, vol. abs/1604.06174, 2016.

[26] Y. Huang, Y. Cheng, D. Chen, H. Lee, J. Ngiam, Q. V. Le, and
Z. Chen, “Gpipe: Efﬁcient training of giant neural networks using pipeline
parallelism,” CoRR, vol. abs/1811.06965, 2018.

[27] S. Wang and P. Kanwar, “BFloat16: The secret to high performance
on cloud TPUs,” in https://cloud.google.com/blog/products/ai-machine-
learning/bﬂoat16-the-secret-to-high-performance-on-cloud-tpus, Google,
2019.

[28] D. D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee,
S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen,
J. Yang, J. Park, A. Heinecke, E. Georganas, S. Srinivasan, A. Kundu,
M. Smelyanskiy, B. Kaul, and P. Dubey, “A study of BFLOAT16 for
deep learning training,” CoRR, vol. abs/1905.12322, 2019.

[29] Google, “Using bﬂoat16 with tensorﬂow models.” https://cloud.google.

com/tpu/docs/bﬂoat16.

[30] D. Das, N. Mellempudi, D. Mudigere, D. D. Kalamkar, S. Avancha,
K. Banerjee, S. Sridharan, K. Vaidyanathan, B. Kaul, E. Georganas,
A. Heinecke, P. Dubey, J. Corbal, N. Shustrov, R. Dubtsov, E. Fomenko,
and V. O. Pirogov, “Mixed precision training of convolutional neural
networks using integer operations,” in 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings, 2018.

[31] U. K¨oster, T. J. Webb, X. Wang, M. Nassar, A. K. Bansal, W. H.
Constable, O. H. Elibol, S. Gray, S. Hall, L. Hornof, A. Khosrowshahi,
C. Kloss, R. J. Pai, and N. Rao, “Flexpoint: An adaptive numerical
format for efﬁcient training of deep neural networks,” in Proceedings
of the 31st International Conference on Neural Information Processing
Systems, NIPS’17, (USA), pp. 1740–1750, Curran Associates Inc., 2017.
[32] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garc´ıa,
B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu,
“Mixed precision training,” in 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
2018, Conference Track Proceedings, 2018.

[33] NVIDIA, “Training with mixed precision.” https://docs.nvidia.com/

deeplearning/sdk/mixed-precision-training/index.html.

[34] M. Drumond, T. Lin, M. Jaggi, and B. Falsaﬁ, “Training DNNs with
hybrid block ﬂoating point,” in Proceedings of the 32Nd International
Conference on Neural Information Processing Systems, NIPS’18, (USA),
pp. 451–461, Curran Associates Inc., 2018.

[53] Y. Bengio, N. L´eonard, and A. Courville, “Estimating or propagating
gradients through stochastic neurons for conditional computation,” arXiv
preprint arXiv:1308.3432, 2013.

[54] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio,
“Binarized neural networks,” in Advances in neural information processing
systems, pp. 4107–4115, 2016.

[55] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 770–778, 2016.

[56] P. Judd, J. Albericio, T. Hetherington, T. M. Aamodt, N. E. Jerger, and
A. Moshovos, “Proteus: Exploiting numerical precision variability in deep
neural networks,” in Proceedings of the 2016 International Conference
on Supercomputing, ICS ’16, (New York, NY, USA), pp. 23:1–23:12,
ACM, 2016.

[57] S. Li, Z. Yang, D. Reddy, A. Srivastava, and B. Jacob, “Dramsim3:
A cycle-accurate, thermal-capable dram simulator,” IEEE Computer
Architecture Letters, vol. 19, no. 2, pp. 106–109, 2020.

[58] HewlettPackard, “CACTI.” https://github.com/HewlettPackard/cacti.

[35] C. De Sa, M. Leszczynski, J. Zhang, A. Marzoev, C. R. Aberger,
K. Olukotun, and C. R´e, “High-accuracy low-precision training,” arXiv
preprint arXiv:1803.03383, 2018.

[36] M. Rhu, M. O’Connor, N. Chatterjee, J. Pool, Y. Kwon, and S. W.
Keckler, “Compressing dma engine: Leveraging activation sparsity for
training deep neural networks,” in 2018 IEEE International Symposium
on High Performance Computer Architecture (HPCA), pp. 78–91, 2018.
[37] B. Zheng, N. Vijaykumar, and G. Pekhimenko, “Echo: Compiler-
based GPU memory footprint reduction for LSTM RNN training,” in
Proceedings of the 47th Annual International Symposium on Computer
Architecture, ISCA ’20, ACM, 2020.

[38] R. D. Evans, L. F. Liu, and T. Aamodt, “JPEG-ACT: A frequency-
domain lossy DMA engine for training convolutional neural networks,”
in Proceedings of the 47th Annual International Symposium on Computer
Architecture, ISCA ’20, ACM, 2020.

[39] S. Han, H. Mao, and W. J. Dally, “Deep Compression: Compressing
Deep Neural Networks with Pruning, Trained Quantization and Huffman
Coding,” arXiv:1510.00149 [cs], Oct. 2015. arXiv: 1510.00149.
[40] Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne,
“Eyeriss: An Energy-Efﬁcient Reconﬁgurable Accelerator for Deep
Convolutional Neural Networks,” in IEEE International Solid-State
Circuits Conference, ISSCC 2016, Digest of Technical Papers, pp. 262–
263, 2016.

[41] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and
W. J. Dally, “EIE: efﬁcient
inference engine on compressed deep
neural network,” in 43rd ACM/IEEE Annual International Symposium
on Computer Architecture, ISCA 2016, Seoul, South Korea, June 18-22,
2016, pp. 243–254, 2016.

[42] S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y. Xie, Y. Chen, and
T. Chen, “Cambricon: An instruction set architecture for neural networks,”
2016 ACM/IEEE 43rd Annual International Symposium on Computer
Architecture (ISCA), vol. 00, no. undeﬁned, pp. 393–405, 2016.
[43] A. D. Lascorz, S. Sharify, I. Edo, D. M. Stuart, O. M. Awad, P. Judd,
M. Mahmoud, M. Nikoli´c, K. Siu, Z. Poulos, and A. Moshovos,
“Shapeshifter: Enabling ﬁne-grain data width adaptation in deep learning,”
in Proceedings of the 52nd Annual IEEE/ACM International Symposium
on Microarchitecture, MICRO 52, (New York, NY, USA), Association
for Computing Machinery, 2019.

[44] I. Edo Vivancos, S. Sharify, D. Ly-Ma, A. Abdelhadi, C. Bannon,
M. Nikoli´c, M. Mahmoud, A. Delmas Lascorz, G. Pekhimenko, and
A. Moshovos, “Boveda: Building an on-chip deep learning memory
hierarchy brick by brick,” in Proceedings of Machine Learning and
Systems (A. Smola, A. Dimakis, and I. Stoica, eds.), vol. 3, pp. 1–20,
2021.

[45] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. Enright Jerger, and
A. Moshovos, “Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network
Computing,” in Intl’ Symp. on Computer Architecture, 2016.

[46] O. M. Awad, M. Mahmoud, I. Edo, A. H. Zadeh, C. Bannon, A. Jayarajan,
G. Pekhimenko, and A. Moshovos, “Fpraker: A processing element
for accelerating neural network training,” in MICRO ’21: 54th Annual
IEEE/ACM International Symposium on Microarchitecture, Virtual Event,
Greece, October 18-22, 2021, pp. 857–869, ACM, 2021.

[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image

recognition,” CoRR, vol. abs/1512.03385, 2015.

[48] A. Howard, M. Sandler, G. Chu, L. Chen, B. Chen, M. Tan, W. Wang,
Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, and H. Adam, “Searching for
mobilenetv3,” CoRR, vol. abs/1905.02244, 2019.

[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei,
“ImageNet Large Scale Visual Recognition Challenge,” arXiv:1409.0575
[cs], Sept. 2014. arXiv: 1409.0575.

[50] X. Sun, N. Wang, C.-Y. Chen, J. Ni, A. Agrawal, X. Cui, S. Venkatara-
mani, K. El Maghraoui, V. V. Srinivasan, and K. Gopalakrishnan, “Ultra-
low precision 4-bit training of deep neural networks,” in Advances in
Neural Information Processing Systems (H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin, eds.), vol. 33, pp. 1796–1807,
Curran Associates, Inc., 2020.

[51] C. Sakr, N. Wang, C.-Y. Chen, J. Choi, A. Agrawal, N. R. Shanbhag,
and K. Gopalakrishnan, “Accumulation bit-width scaling for ultra-low
precision training of deep networks,” in 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019, OpenReview.net, 2019.

[52] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

pp. 436–444, 05 2015.

14

