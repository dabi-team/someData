2
2
0
2

r
a

M
6
1

]

C
D
.
s
c
[

1
v
5
4
3
4
1
.
3
0
2
2
:
v
i
X
r
a

PUBNUMBER
March 29, 2022

Evolution of HEP Processing Frameworks

Christopher D. Jones and Kyle Knoepfel

Fermi National Accelerator Laboratory, Batavia, IL 60510, USA

Paolo Calafiura, Charles Leggett, and Vakhtang Tsulaia

Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA

ABSTRACT

HEP data-processing software must support the disparate physics needs of many
experiments. For both collider and neutrino environments, HEP experiments
typically use data-processing frameworks to manage the computational com-
plexities of their large-scale data processing needs.

Data-processing frameworks are being faced with new challenges this decade.
The computing landscape has changed from the past three decades of homoge-
neous single core x86 batch jobs running on grid sites. Frameworks must now
work on a heterogeneous mixture of diﬀerent platforms: multi-core machines,
diﬀerent CPU architectures, and computational accelerators; and diﬀerent com-
puting sites: grid, cloud and high-performance computing.

We describe these challenges in more detail and how frameworks may con-
front them. Given their historic success, frameworks will continue to be critical
software systems that enable HEP experiments to meet their computing needs.
Frameworks have weathered computing revolutions in the past; they will do so
again with support from the HEP community.

Submitted to the Proceedings of the US Community Study
on the Future of Particle Physics (Snowmass 2021)

1

 
 
 
 
 
 
1 Executive Summary

The recent changes to computational hardware and to the kinds of sites hosting that
hardware in the past decade calls for renewed research and development into HEP data-
processing frameworks in order fully exploit these resources. Computational resources of
the previous decades were dominated by the world-wide research grid composed of com-
modity x86 CPUs running the Linux operating system. Experiments just needed to target
that hardware with single-threaded applications in order to meet their computational needs.
The advent of multi-core systems, the prevalence of non-CPU based computational hard-
ware, and the growth in use of non-grid computational sites all require new research and
development at the framework level. Multi-core systems necessitate exposing and exploiting
software concurrency. New kinds of computational hardware require new ways to schedule
work in the application and potentially new third party library interfaces to accommodate.
High Performance Computing and commercial cloud sites bring unique requirements and
possibilities which will shape additional aspects of frameworks.

Modifying experiments software frameworks to support multi-processing and multi-
threading event processing models signiﬁcantly improves utilization of shared system re-
sources on modern many-core architectures, and also increases the overall event process-
ing throughput. Current software frameworks of many HENP experiments are capable
of concurrently processing multiple events either within a single multi-threaded applica-
tion or by coordinating event processing across multiple processes. To further optimize
resource utilization of certain workloads, some experiments have implemented hybrid mul-
tiprocess/multithreaded processing models. The ﬁrst results obtained in this area are en-
couraging, and further research and development should be made into exploring the possi-
bility of a generalized hybrid framework that can combine the strengths of the two parallel
processing models.

Computational accelerators (e.g., GPUs, FPGAs) are becoming increasingly prevalent
in most new computing facilities. To port HEP applications to these architectures, signif-
icant technical challenges must be addressed, such as eﬃciently scheduling work on both
accelerators and CPUs, accommodating the various programming models of accelerators
and CPUs, and adjusting experiment data models to be more compatible with accelerator
expectations.

While the world-wide computing grid continues to be an important computational re-
source for HEP, it is expected that in the future, experiments will rely more on HPC sites
and commercial cloud-based resources. Each of these resource types requires some special-
ization by the frameworks to achieve optimal utilization. Cloud-computing sites also bring
a new set of requirements for frameworks, as such sites are likely to be used on an oppor-
tunistic basis. This may require that the event processing jobs must be able to terminate
on a short notice and preserve all work done so far, or they must periodically save the event
processing results.

Framework developers will continue to rely on the ﬁrm support they have receive from

HEP experiments as they confront the computing challenges of the next decade.

2

2

Introduction

Although the term framework can be used in multiple ways, we use it to denote a software
system that HEP experiments use to simulate or reconstruct detector data with the goal
of extracting physics quantities of interest—or data products. HEP frameworks provide
non-trivial data-processing functionality that includes:

• The ability to schedule nearly arbitrary user-deﬁned algorithms for a program’s exe-

cution.

• A provenance subsystem that records the processing history of data products.

• Organizing data into physically meaningful datasets or periods of time.

• Managing iteration over events and other data groupings.

Because physicists rely on these features as part of the analysis process, frameworks have
become “an essential part of HEP experiments’ software stacks” [1].

The ways in which frameworks have provided these capabilities, however, have changed
over time. Not only do newer computing technologies necessitate rethinking long-established
programming paradigms, but changes in the amount of data to be processed—and the
hardware on which the processing occurs—also warrant reassessing the status quo of HEP
computing.

In section 3 of this document we discuss various models of parallel event processing (e.g.,
multithreaded, multiprocess, hybrid) adopted by the experiments in order to eﬃciently
utilize computing resources of modern many-core nodes. Section 4 describes the challenges
posed to experiments’ software frameworks by the need to support wide range of available
CPU architectures. In section 5 we describe the diﬀerences between heterogeneous (e.g.,
GPU, FPGA) and traditional CPU architectures, we also discuss various GPU programming
models available today on the market, and the challenges related to oﬄoading computations
from the frameworks to accelerators. Finally, section 6 describes three main categories of
computational sites—grid, HPCs and commercial clouds—and discusses some speciﬁcs of
running experiments’ data-processing applications on these sites.

3 Many-Core

For decades, it was suﬃcient for an experiment’s data-processing framework to execute on
only one CPU core. Computing clusters and large-scale batch systems were deployed and
have achieved impressive data-processing throughput using only commodity CPU hardware.
HEP experiments also took advantage of the “perfect parallelization” of event-processing
made possible by the statistical independence of each recorded event. This parallelization
was achieved by having many independent single-threaded processes each independently

3

processing their own group of events. However, as CPU clock rates have plateaued, com-
puting nodes have changed to including multiple integrated CPU cores, performing com-
putations in parallel to keep up with Dennard scaling [2]. Eﬃciently utilizing nodes with
large numbers of cores can be challenging for traditional computing models where each job
runs independently on a single core. This can lead to very large memory footprints, and
ineﬃcient use of system resources. Modifying these frameworks to support multiprocess-
ing and multithreading on many-core architectures can signiﬁcantly improve utilization of
shared system resources. In addition, a single multi-core job can process a larger number
of events within the same amount of time, with all the events being written to the same
ﬁle. The resultant larger ﬁles can then either limit or completely eliminate the need to later
merge output ﬁles together in order to generate ﬁles of suﬃcient size to be eﬃciently stored
to archival media, such as tape. In addition, the total number of multi-core jobs needed
to complete a large workﬂow is substantially less than the number of single-threaded jobs
needed to do the same workﬂow. This decrease in total number of jobs to manage can
signiﬁcantly decrease the pressure placed on the workﬂow management system.

Within the last 10 years, many High Energy and Nuclear Physics experiments have
gained the ability to use multiple cores within a single job on a machine when doing
production work (such experiments include ATLAS and LHCb through the Gaudi frame-
work [3], CMS [4], ALICE, Mu2e through the art framework [5], GlueX through the JANA2
framework [6], and Jeﬀerson Laboratory’s CLARA framework [7]). These experiments’
frameworks are now able to concurrently process multiple events within one multithreaded
application or by coordination across multiple processes.

3.1 Multithreaded Processing Model

Multithreading sees a single process running multiple threads of execution where all threads
share the same memory space but each thread performs its own operations. Communication
across threads is extremely fast (as it is just read/write to shared memory) but unintended
sharing (known as race conditions) can be very diﬃcult to diagnose and ﬁx. The memory
sharing across threads allows a single, multithreaded program to consume less CPU mem-
ory than the equivalent number of concurrently running, single-threaded applications. The
memory savings were a major reason for ATLAS and CMS to move to a multithreaded
framework. multithreading technologies make it easy to schedule tasks for concurrent ex-
ecution on a machine’s available CPU cores. By splitting the work into a large number of
small tasks, the concurrency opportunities increase even further.

3.2 Multiprocess Processing Model

In contrast to multithreading, multiprocess involves executing multiple intercommunicat-
ing processes on one or more compute nodes. These processes each have their own mutable
memory space which avoids the unintended sharing diﬃculties of multithreading. It is possi-
ble to avoid redundant copies of immutable data (e.g. detector geometry descriptions) either
by sharing the memory via the operating system’s “copy-on-write” feature, or by partition-

4

ing the tasks assigned to each process such that all tasks using the same immutable data
are in the same process. Avoiding such redundancies can greatly lower overall memory con-
sumption. As just mentioned, the individual processes are usually assigned a speciﬁc set of
tasks they will perform. Communication across processes is, in general, slower than within
a process (as data transferred between processes often needs to be packed, transferred, and
then unpacked). This tends to require that, in order to amortize communication time, the
tasks performed by each process do more work and run longer than tasks in a multithreaded
framework. Given the restriction on tasks done by each process, it can be more challeng-
ing to eﬃciently use the available CPU cores when using multiprocess implementations as
compared to multithreading.

3.3 Hybrid Processing Models

Just as it is possible to have multithreaded processes be part of a multiprocess based system,
it is also possible for a single, multithreaded application to control multiple external pro-
cesses. This is used by the CMS experiment to run concurrent single-threaded instances of
Monte Carlo generators where the generators are not safe to run multithreaded. In this case,
the main threaded process has a task associated with each generator instance and when the
main application wants to run that generator, the associated task pauses the thread in the
main application and then permits the generator application to “take over” that core for
its own execution. Once the generator ﬁnishes processing the event, the generator process
pauses and the main application’s task wakes up, transfers the data between processes and
then returns control to the main application. In this way, the external process is essentially
scheduled just like a task within the main application.

Another approach to the hybrid processing model is to implement a multiprocess ar-
chitecture where each event-processing unit is a multithreaded process. The usage of this
approach is being considered by the ATLAS experiment for some workloads with high mu-
table memory footprint (hence not suitable for running in pure multiprocess mode) that
have not demonstrated good scaling of event-processing throughput due to lock contention
among threads. The exact conﬁguration of threads and processes can be ﬁne-tuned to
speciﬁc hardware and job requirements.

Further research and development should be made into exploring the possibility of a
generalized hybrid multiprocess/multithreaded HEP framework system that can combine
the strengths of the two processing models.

3.4 Further Considerations

Many applications now also support concurrent processing of tasks at the sub-event level.
For most HEP experiments, this level of concurrency is more limited than event-level par-
allelism as the number of tasks tends to top out around 10,000 independent tasks (e.g.
the number of charged-particle tracks in an extreme collision) versus the 100s of billions
of independent events that are typically processed by the LHC experiments. In addition,

5

the time required to execute intra-event tasks in parallel tends to be a small fraction of
the total time needed to process an event; by contrast, concurrently processing separate
events naturally ﬁlls an event’s entire processing time. For neutrino experiments such as
DUNE, however, the trade-oﬀs between inter- and intra-event parallelism may be diﬀerent
as the number of events to process at a given time may be less than the available cores. In
addition, trying to process as many concurrent events as cores on a machine may not be
possible if the mutable memory needed per event exceeds the available memory per core on
a standard machine. Given the diﬀerent kinds of exploitable concurrency, it may therefore
be necessary to develop and maintain diﬀerent frameworks based on characteristics of the
data being collected and the tasks to be performed.

As frameworks apply more cores to processing data from the same source (e.g. a ﬁle
on disk) and writing data to the same storage (e.g. another ﬁle on disk), the serial nature
of traditional I/O systems becomes a limit to the concurrency scaling. CMS has already
found that attempting to scale beyond 8 threads per job while doing reconstruction leads
to an overall CPU eﬃciency of less than 80% because of the need to sequentially call into
ROOT for I/O. A component of the US DOE CCE project [8] is researching ways to allow
greater concurrency while doing I/O but additional research will probably needed in this
area in order to exploit a greater number of cores.

4 CPU Architectures

The dominance of commodity Intel x86 compatible hardware as the major source of HEP
computing resources appears to be coming to an end. As more HEP computational resources
come from high-performance computing (HPC) sites, the types of CPU architectures frame-
works must support will grow.

One of the driving factors in the design of HPC sites is the computational energy
eﬃciency—i.e. the energy utilization per ﬂoating-point operation. The use of traditional
x86-based homogeneous CPU designs for leadership class HPC facilities has proved unfea-
sible due to their electrical demands.
Instead, increasingly large fractions of silicon are
being devoted to more energy-eﬃcient computational accelerators such as GPUs, as well as
a shift towards lower-power traditional CPUs such as ARM-based chips. Examples of these
are Frontier at ORNL, where more than 90% of the computing power comes from AMD
GPUs, and Fugaku in Japan, the current leader of the TOP500 supercomputers, which is
based on a Fujitsu AF64 ARM 8.2A core. Manufacturers have also started oﬀering less
monolithic chips, where a single CPU is composed of multiple cores of diﬀerent power and
performance, or where one or more traditional CPUs are bound together with a GPU or
FPGA accelerator in a chiplet array, oﬀering much higher bandwidth between components
than with a traditional add-in accelerator card.

The validation of physics results across these diﬀerent architectures may require software
experts to assist in proper handling of portable ﬂoating-point evaluations to guarantee
identical results. Experiments may, otherwise, have to give up on bit-wise reproducibility
of results and favor statistical equivalency instead. Either way, the frameworks will need to

6

Figure 1: Current HPC landscape.

record information about what architectures were used for each generated data product to
assist in debugging code.

In order to get the best usage of diﬀerent CPU architectures, we may be required to com-
pile our code either with speciﬁc architectural compiler directives or with speciﬁc compilers.
This approach will likely lead to many variants of an experiment’s code base which will have
to be managed and distributed. Presently, one complete build of CMSSW (including all
third party libraries) requires approximately 100GB of storage space. Having multiples of
these for each software architecture, and then within a given architecture multiple variants
for each sub-hardware conﬁguration, is likely to be unsustainable. To mitigate this problem,
development eﬀort is needed to explore viable solutions, perhaps including:

• Fat binaries, where one build contains diﬀerent machine code versions of the same
function, and the language or operating system automatically uses the correct version
at run-time.

• Selective optimization, where the developer identiﬁes which framework components
would beneﬁt the most from architecture-speciﬁc builds and the framework would
dynamically load the selected components built for speciﬁc hardware.

• On-site builds, where the code is built directly on the site that will run it. Traditional
build mechanisms have made such an idea extremely diﬃcult. With the advent of
better build tools, such as Spack [9], this approach becomes more feasible, although
the size required for each build may still be very large.

For each of the above options, the size and number of diﬀerent builds that may be needed to
support all the workﬂows planned to run at a site, and the validation of the physics results
for those builds are still challenges that would need to be confronted.

7

Figure 2: Portability solutions for heterogeneous architectures.

5 Computational Accelerators

Computational accelerators, such as GPUs, and to a lesser extent FPGAs, are becoming
increasingly prevalent in most new computing facilities (see Figure 1) due to their reduced
energy consumption. There exist major challenges, however, in porting current HEP work-
ﬂows to these architectures. Traditional programming paradigms commonly found in HEP
programs do not map well onto GPU architectures.

GPUs and FPGAs are fundamentally diﬀerent from traditional CPU architectures. Al-
though aﬀording massive parallelism, the memory access patterns and high latency of GPUs
require a fundamentally diﬀerent programming model than that of CPUs. Other acceler-
ators such as TPUs are optimized for speciﬁc tasks, such as matrix manipulations, that
are heavily used in machine learning workﬂows. While these can enormously accelerate the
computations for which they were designed, they are very ineﬃcient as a general purpose
processor. Field programmable devices such as FPGAs and ASICS are also very eﬃcient
for the speciﬁc algorithms for which they are designed, but require time consuming repro-
gramming to reconﬁgure for a diﬀerent task.

The situation is further complicated by the fact that the three major producers of GPUs,
NVIDIA, AMD and Intel, each promote their own programming model and development
environment, which tend to be incompatible with each other. This use of device speciﬁc
languages requires the re-coding or translation of source code for the appropriate device,
which can be very time consuming given the large code base that HEP experiments have
developed. The human cost of developing, validating, and maintaining multiple versions
of source code for each architecture is too burdensome for any experiment to currently
contemplate. A number of portability layers are available (see Figure 2), such as Kokkos [10],
Alpaka [11], OpenMP [12], and SYCL [13], and standards are slowly emerging that will
eventually percolate into the C++ language standard. However, even with a portability
layer that can target multiple diﬀerent hardware backends from the same source code,
diﬀerent libraries, compiler options, and sometimes even diﬀerent compilers are currently
needed to generate the binaries, thus increasing build complexity and code distribution
overheads.

8

Integrating computational accelerators into traditional frameworks adds an extra level
of complexity. The optimal technique of executing code on accelerators is usually via asyn-
chronous oﬄoading, where once a task has been sent to the accelerator by the framework
executing on the CPU, it does not need to wait idly for the results, and can instead execute
other useful work. Once the accelerator has ﬁnished its task, the framework must then be
informed so that the results can be retrieved and downstream execution can continue. This
asynchronous model can often require signiﬁcant modiﬁcations to existing frameworks that
have been developed for traditional CPUs. Furthermore, the amount of “work” that can be
oﬄoaded to an accelerator from a single event is often rather small, leading to ineﬃcient use
of the device. In order to maximize accelerator performance, data may need to be batched
between multiple events, which is a signiﬁcant complication for frameworks that normally
consider each event to be completely independent.

Many modern frameworks have evolved complex event data models (EDMs) that make
extensive use of object-oriented features such as multiple levels of polymorphic inheritance.
These types of data structures cannot be used eﬃciently on accelerators, which prefer ﬂat
structures where threads access sequential or regularly stepped memory locations. This has
caused the need to reformulate data objects that can be accessed on accelerators, either
by explicitly changing the EDM, which may involve modifying large amounts of associated
code, or by converting them at run-time as they are transferred to and from the device,
which can impose a signiﬁcant performance penalty. Further development is needed in this
area to ﬁnd the best trade-oﬀs between performance and usability.

6 Computational Sites

For several decades HEP computing has relied on a world-wide computational grid to do the
bulk of computing for the experiments. The grid has been composed of machines running
compatible versions of Linux on x86-compatible hardware. This large-scale homogeneity
has made it easy to compile an experiment’s code once and then use it everywhere. The
grid will continue to be an important computational resource for HEP, but in the future,
experiments will rely more on HPC sites and commercial Cloud-based resources. Each of
these resource types will require some specialization by the frameworks in order to achieve
optimal utilization.

6.1 Grid sites

The world-wide grid uses batch job schedulers at each site.
In general, sites prefer to
schedule many independent jobs. These jobs are given a ﬁxed share of resources (number of
CPU threads and total CPU memory) they are allowed to use. The ﬁxed resource sharing is
needed as sites usually prefer to be able to schedule multiple jobs onto the same computing
node. Typically, the computing nodes will have access to the internet as well as their own
local disks. Sites also tend to have a site-wide storage system which can be used by workﬂows
to store the ﬁles containing the ﬁnal results of computation before moving those ﬁles to an

9

archival storage site. Some sites have special agreements with experiments and allow those
experiments to run permanent services at the site, for example a database-caching layer for
conditions storage. For frameworks, this means they need internal mechanisms to restrict
the number of CPU cores being utilized at a single time and they could rely upon fast local
storage during processing.

6.2 HPC sites

Unlike grid sites where a “job slot” in the system constitutes a fraction of a compute node,
an HPC “job slot” is typically a cluster of hundreds or thousands of compute nodes that
can be used by the program simultaneously. This cluster of compute nodes is allotted a
certain amount of time as a group to be run on the site. It is the responsibility of the user
to ensure that a site’s compute nodes are well-utilized as poor utilization can negatively
impact future resource requests. This translates into a requirement that a large number
of nodes must ﬁnish work at approximately the same time, which is diﬃcult to accomplish
when the processing unit over which work is parallelized is a ﬁle containing thousands of
events. Further research and development will thus be needed for extending frameworks to
eﬃciently operate in such an environment. This might entail scheduling work for chunks
of events not only across all processing elements of a computing node, but also across all
nodes that have been assigned to that job. A further challenge is to keep track of the
progress of many independent applications processing small chunks of events, where the
applications have the ability to stop work after any chunk has ﬁnished or use some other
mechanism to deal with early termination. One way to achieve this goal is to integrate
the application framework with a workﬂow management system as demonstrated by the
Raythena/EventService project [14].

In addition, HPC sites vary widely, not only in their computational hardware but also
in their network access and storage systems. Many HPC sites do not allow direct internet
connections from their worker nodes. Some of these sites do allow internet connections on
special edge nodes which, in turn, communicate with the worker nodes. These restrictions
can require additional development in order to handle access to resources that are normally
shared across computing sites, such as conditions databases. For storage, sites might have
local storage on the compute nodes, such as hard disks, SSDs, or maybe even RAM disk, or
might not have any local storage. The site-wide storage systems of an HPC site may not be
designed to handle small writes from large numbers of processes and, instead, be designed
for large writes from a very small number of applications. Further research is likely needed
to be able to use the storage on HPC sites eﬀectively.

6.3 Cloud computing

Commercial cloud computing oﬀers new possibilities for HEP computing. Experiments
could use these resources either to handle a large time-critical data-processing need (e.g.
Monte Carlo generation for an upcoming conference) or to supplement their standard com-
puting resources at times when the commercial cloud resources are less expensive. The

10

availability of a large number of compute nodes on short notice could allow experiments to
provision their standard computing resources for steady-state processing instead of for peak
processing, thereby reducing the total number of compute resources they must maintain.

Utilizing cloud-computing sites brings a new set of requirements for frameworks. Cloud-
computing sites are likely to be used on an opportunistic basis. If the job is being run in
a “spot market” and will only run for as long as the price to use the CPU is below some
threshold, this may require that either the job must terminate very quickly and preserve all
work done so far, or the job must periodically store its results into completed ﬁles so that
intermediate results can be used for later processing. One such approach is to distribute
the events of a production job across many semi-independent application workers, each
of which process a small chunk of events as described above in section 6.2. This allows
the preemption of a production job at any time, losing only the work done on the last
few events being processed. Correctly handling a job that has only partially completed
will likely require additional development of the workﬂow management system, which is
responsible for distributing the jobs across processing sites.

6.4 Further comments

Experiments have observed penalties when transferring data into and out of cloud and HPC
sites. Addressing this issue could require running all intermediate processing steps for a
complete workﬂow on a single site, rather than distributing results from diﬀerent processing
steps to diﬀerent computing sites for later processing.

The extreme heterogeneity of computational platforms described in sections 4 and 5,
together with the variety of site execution policies, favoring (e.g.) large-scale, distributed
applications on HPC systems, and preemptible applications on commercial clouds, present
signiﬁcant new challenges for HEP computing. Developing data-processing framework mod-
ules that run eﬃciently on accelerators, ideally across multiple platforms, is an eﬀort being
pursued by the HEP community in collaboration with HPC experts [8]. Assuming a suf-
ﬁcient number of such modules is available, the challenge for framework developers is to
match the resources needed by a module with those provided by one or more computing
sites. A further challenge for the framework scheduler is to minimize and “hide” data trans-
mission latencies from the memory of one processing element to another. Similar problems
have been addressed by distributed application frameworks like Ray [15] and microservice
architectures like Dapr [16], albeit with diﬀerent performance requirements.

7 Conclusion

The recent changes to the computing landscape over the last decade have the potential to
meet the computational needs for HEP experiments in the decades to come. To realize
that potential, the HEP community will need to continue investing in the development of
its data-processing frameworks as fundamental building blocks for exploiting the available
computational resources.

11

References

[1] P. Calaﬁura, M. Clemencic, H. Grasland, C. Green, B. Hegner, C. Jones et al., HEP
Software Foundation Community White Paper Working Group – Data Processing
Frameworks, https://arxiv.org/abs/1812.07861, 2019.

[2] R. Dennard, F. Gaensslen, H.-N. Yu, V. Rideout, E. Bassous and A. LeBlanc, Design
of ion-implanted mosfet’s with very small physical dimensions, IEEE Journal of
Solid-State Circuits 9 (1974) 256.

[3] M. Clemencic, D. Funke, B. Hegner, P. Mato, D. Piparo and I. Shapoval, Gaudi
components for concurrency: Concurrency for existing and future experiments,
Journal of Physics: Conference Series 608 (2015) 012021.

[4] C.D. Jones, L. Contreras, P. Gartung, D. Hufnagel and L. Sexton-Kennedy, Using the

CMS threaded framework in a production environment, Journal of Physics:
Conference Series 664 (2015) 072026.

[5] C. Green, J. Kowalkowski, M. Paterno, M. Fischler, L. Garren and Q. Lu, The art

framework, Journal of Physics: Conference Series 396 (2012) 022020.

[6] Lawrence, David, Boehnlein, Amber and Brei, Nathan, Jana2 framework for event

based and triggerless data processing, EPJ Web Conf. 245 (2020) 01022.

[7] V Gyurgyan et al, Clara: Clas12 reconstruction and analysis framework, J. Phys.:

Conf. Ser. 762 (2016) .

[8] “High Energy Physics – Center for Computational Excellence.”

https://www.anl.gov/hep-cce.

[9] T. Gamblin, M. LeGendre, M.R. Collette, G.L. Lee, A. Moody, B.R. de Supinski
et al., The Spack Package Manager: Bringing Order to HPC Software Chaos,
Supercomputing 2015 (SC’15), (Austin, Texas, USA), November 15-20, 2015, DOI.

[10] C.R. Trott, D. Lebrun-Grandi´e, D. Arndt, J. Ciesko, V. Dang, N. Ellingwood et al.,

Kokkos 3: Programming model extensions for the exascale era, IEEE Transactions on
Parallel and Distributed Systems 33 (2022) 805.

[11] E. Zenker, B. Worpitz, R. Widera, A. Huebl, G. Juckeland, A. Kn¨upfer et al., Alpaka
- an abstraction library for parallel kernel acceleration, IEEE Computer Society, May,
2016, http://arxiv.org/abs/1602.08477 [1602.08477].

[12] “OpenMP.” https://www.openmp.org.

[13] “SYCL.” https://www.khronos.org/api/index 2017/sycl.

[14] Muskinja, Miha, Calaﬁura, Paolo, Leggett, Charles, Shapoval, Illya and Tsulaia,
Vakho, Raythena: a vertically integrated scheduler for ATLAS applications on
heterogeneous distributed resources, EPJ Web Conf. 245 (2020) 05042.

12

[15] Ray, Scaling Python made simple, for any workload, https://ray.io/, 2021.

[16] Dapr, APIs for building portable and reliable microservices, https://dapr.io/, 2022.

13

