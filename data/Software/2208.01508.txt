2
2
0
2

g
u
A
2

]
E
S
.
s
c
[

1
v
8
0
5
1
0
.
8
0
2
2
:
v
i
X
r
a

MEMO: Coverage-guided Model Generation For Deep Learning
Library Testing

Meiziniu Li
The Hong Kong University of Science
and Technology, China
China
mlick@cse.ust.hk

Jialun Cao
The Hong Kong University of Science
and Technology
Guangzhou HKUST Fok Ying Tung
Research Institute
China
jcaoap@cse.ust.hk

Yongqiang Tian
University of Waterloo
The Hong Kong University of Science
and Technology
Canada
yongqiang.tian@uwaterloo.ca

Tsz On Li
The Hong Kong University of Science
and Technology
China
toli@connect.ust.hk

Ming Wen*
Huazhong University of Science and
Technology
China
mwenaa@hust.edu.cn

Shing-Chi Cheung*
The Hong Kong University of Science
and Technology
Guangzhou HKUST Fok Ying Tung
Research Institute
China
scc@cse.ust.hk

ABSTRACT
Recent deep learning (DL) applications are mostly built on top of
DL libraries. The quality assurance of these libraries is critical to
the dependable deployment of DL applications. A few techniques
have thereby been proposed to test DL libraries by generating
DL models as test inputs. Then these techniques feed those DL
models to DL libraries for making inferences, in order to exercise
DL libraries modules related to a DL model’s execution. However,
the test effectiveness of these techniques is constrained by the
diversity of generated DL models. Our investigation finds that
these techniques can cover at most 11.7% of layer pairs (i.e., call
sequence between two layer APIs) and 55.8% of layer parameters
(e.g., “padding” in Conv2D). As a result, we find that many bugs
arising from specific layer pairs and parameters can be missed by
existing techniques.

In view of the limitations of existing DL library testing tech-
niques, we propose MEMO to efficiently generate diverse DL models
by exploring layer types, layer pairs, and layer parameters. MEMO:
(1) designs an initial model reduction technique to boost test effi-
ciency without compromising model diversity; and (2) designs a
set of mutation operators for a customized Markov Chain Monte
Carlo (MCMC) algorithm to explore new layer types, layer pairs,
and layer parameters. We evaluate MEMO on seven popular DL
libraries, including four for model execution (TensorFlow, PyTorch
and MXNet, and ONNX) and three for model conversions (Keras-
MXNet, TF2ONNX, ONNX2PyTorch). The evaluation result shows
that MEMO outperforms recent works by covering 10.3% more
layer pairs, 15.3% more layer parameters, and 2.3% library branches.
Moreover, MEMO detects 29 new bugs in the latest version of DL
libraries, with 17 of them confirmed by DL library developers, and
5 of those confirmed bugs have been fixed.

* Corresponding author.

Figure 1: Overview of Testing DL Library by DL Models.

1 INTRODUCTION
Deep learning (DL) has been increasingly adopted for mission-
critical applications such as authentication [13, 14], medical treat-
ment [25], pandemic control [7, 8], and autonomous driving [9, 32,
33]. Current DL applications are mostly built on top of popular
DL libraries such as TensorFlow [6], PyTorch [30], MXNet [10],
and ONNX [3]. These libraries were, however, found to commonly
suffer from coding bugs [20]. Early detection of these bugs is crucial
to the development of reliable DL applications.

Promising results have been reported by testing DL libraries or
DL inference engines using DL models as the test inputs [15, 17, 27,
31, 37]. By loading and inferencing with the DL models, these test-
ing techniques [15, 17, 27, 31, 37] aim to test DL libraries through
a sequence of function calls for model construction and model
inference. To obtain a large amount of DL models as test inputs,
existing works mainly follow two practices: (1) generate mutants of
published DL models or their datasets [17, 27, 31, 37] or (2) gener-
ate models directly from scratch using predefined model structure
templates (e.g., chain structure) [15]. In particular, CRADLE [31] col-
lects popular DL models such as ResNet [18] and InceptionNet [35]
as test inputs to test DL libraries. LEMON [37] and Audee [17]
generate DL models by iteratively mutating the parameters and
structures of published DL models. Luo et al. [27] abstract DL mod-
els as a computation graph and iteratively mutate the structures,

Bugs DetectionNumericCrashPerformanceKerasTensorFlowMXNetDL ModelDL LibrariesPyTorchModelConstructionModel InferenceONNX 
 
 
 
 
 
Woodstock 2022, November, 2022, Woodstock, NY

Li, et al.

Figure 2: An Example of Keras Script for Model Construc-
tion

Figure 3: A Crash Bug of ONNXRuntime

parameters, and data inputs in the computation graph.1 Muffin [15]
generates DL models with diverse layer types from scratch using
predefined chain and cell-based structure templates.

Existing techniques target at generating DL models to either
enlarge the prediction inconsistency in differential testing [17, 37],
increase the coverage of layer APIs [15] or increase the operator-
level coverage (i.e., the combination of graph structures, operator
parameters, and data input) in the computation graph of DL mod-
els [27]. However, they are limited in generating diverse DL models
to expose bugs that reside in specific API calls (e.g., specific API
call sequences or API parameters). Take a model generation script
of Keras as an example. The diversity of API calls triggered by
the generated model depends on the varieties of the model’s layer
types (e.g., Conv2D, BatchNormalization in Figure 2), layer pairs
which determine the data transformation between adjacent layers
(e.g., Conv2D→BatchNormalization), and layer parameters (e.g.,
filters, kernel_size highlighted in orange). However, neither
increasing the prediction inconsistency [17, 37], nor increasing ex-
isting coverages [15, 27] can targetedly help diversify the model’s
layer parameters and layer pairs.

As a result, we find that the models generated by existing tech-
niques are weak in covering layer pairs and layer parameters, i.e.,
only at most 11.7% layer pairs and 55.8% layer parameters have
been covered by existing techniques. Consequently, bugs residing
at specific parameter settings or API call sequences cannot be effec-
tively detected. Figure 3 shows a bug that illustrates the weaknesses
of existing techniques. It is a crash bug triggered by a variant of the
ResNet model, leading to a core dump in ONNXRuntime,2 which is
a DL library for model inference and training with over 6.5K stars
in GitHub. The bug is triggered by a specific layer pair and layer
parameters (as shown on the left with the units of Dense layer set
to zero). It is hard for existing techniques to generate DL models
that are diversified enough to manifest the bug-inducing structure.
To address the weaknesses of existing work, we propose MEMO,
which aims to generate diverse DL models for DL library testing.
Similar to LEMON [37] and Audee [17], MEMO uses published
DL models as the initial seeds. DL models are generated by iter-
atively mutating the seeds with new layer types, layer pairs, and
layer parameters. Two challenges arise in the generation of di-
verse DL models. First, the search space of model generation is

1We refer this technique as GraphFuzz in the rest of our paper
2https://github.com/microsoft/onnxruntime

huge. Popular published DL models typically comprise a lengthy,
ordered sequence of architecture with numerous layer parameters.
For example, mutating the InceptionResnetV2 model involves the
numerous possible perturbation of 782 network layers with 30,400
layer parameters. Second, the runtime overhead in the generation
and execution of a DL model is expensive. As reported by an ear-
lier work [38], testing techniques based on model generation and
execution can be cost-prohibitive. For instance, it can take over 10
minutes to generate a new DL model from the InceptionResNetV2
seed model and execute the new model on an Intel GPU server. It-
eratively mutating these published models, which are mostly large
in size, incurs non-trivial runtime overhead in model generation.
The insight of MEMO to address these two challenges is based
on our observation: increasing the diversity of layer pairs and
parameters in DL models are likely to invoke different function
calls, thus can achieve more comprehensive testing on DL libraries.
In particular, we define three coverage criteria (namely layer type
coverage, layer pair coverage, and layer parameter coverage) to
guide the mutation of DL models, in order to targetedly enhance
these models’ diversity in layer pairs and parameters. Different from
previous coverage criteria (e.g., operator-level coverage proposed
by GraphFuzz [27]), targeting at these three coverage criteria offers
a more optimal granularity in measuring the model’s diversity.
Based on these three coverages, we further design a set of mutation
operators to diversify the types of layers, layer pairs, and layer
parameters in the seed models. We also adapt the Monte Carlo
Markov Chain (MCMC) algorithm with a novel fitness function to
iteratively mutate seed models for diversity. To address the second
challenge, we propose a model reduction strategy to reduce the size
of initial seed models while retaining their model diversity. The
strategy significantly reduces the runtime overhead incurred by
model generation and execution.

We evaluate the performance of MEMO on seven popular DL
libraries: TensorFlow [6], PyTorch [30], MXNet [10], ONNXRun-
time [4], Keras-MXNet [1], TF2ONNX [5], ONNX2PyTorch [2]. In
particular, we apply our generated models to test the model con-
struction and inference modules in four DL libraries (i.e., Tensor-
Flow, PyTorch, MXNet, and ONNXRuntime). To adopt differential
testing for bug detection, we use three DL libraries (i.e., Keras-
MXNet, TF2ONNX, ONNX2PyTorch) model conversion across dif-
ferent DL libraries. Therefore, our generated models can also man-
ifest bugs inside these model conversion libraries. We compare
MEMO with three state-of-the-art DL library testing techniques as

Conv2DBatchNormalizationInputOutputimportkeras# Define The Modelx =keras.layers.Input((28,28,3))y =keras.layers.Conv2D(# Layer Parametersfilters=2,kernel_size=3,activation='relu',padding="same")(x)y =keras.layers.BatchNormalization(# Layer Parametersaxis=-1,momentum=0.99)(y)# Construct The Modelmodel =keras.models.Model(x,y)How to motivate our work by existing work’s limitation (by essence):As is shown on the left, a DL models defined by a set of layers with a specific layer configurations and layer call sequence detemining the data flow of the input tensor. However, existing model generation techniques are either driven by the output inconsistency across DL libraries or driven by the layer type diversity.  The diversity of layer configurations and layer call sequences are ignored. As a result, the state-of-the-art testing techniques fail to detect any new bugs residing in the modules related to model loading and model execution in the latest version of TensorFlow library.DenseReLUDotInputOutputMutateUnits: 100 -> 0Empty TensorCore Dump!MEMO: Coverage-guided Model Generation For Deep Learning Library Testing

Woodstock 2022, November, 2022, Woodstock, NY

baselines: CRADLE [31], LEMON [37], and Muffin [15]. For Graph-
Fuzz [27] and Audee [17], since no executables or source code are
available, we do not include them as baselines. To make our eval-
uation comprehensive, we include Muffin, the latest technique to
be published as a baseline in our evaluation. The evaluation results
show that MEMO’s mutation operators and its search algorithm
can significantly increase the diversity of generated DL models, out-
performing baselines by covering 15.3% more layer parameters and
10.3% more layer pairs. Moreover, MEMO can cover at least 2.3%
more branches than baselines. MEMO successfully finds 29 new DL
library bugs. So far, 17 of them have been confirmed by DL library
developers, and 5 of the confirmed ones have already been fixed. In
summary, our work makes the following major contributions.

models either result in NaN or the inconsistency of their outputs
over different DL libraries can be boosted. GraphFuzz [27] is further
proposed to mutate the computation graph in DL model for DL
library testing. It proposes an operator-level coverage to quantify
the model diversity and further design a set of mutation operators
to explore combinations of model structures, parameters and data
inputs. Muffin [15] is the most recent DL library testing work that
generates DL models driven by layer diversity. In particular, Muffin
abstracts the DL model as a Directed Acyclic Graph (DAG) and de-
fines two types of commonly used graph structures: chain struture
and cell-based structure to generate the structure information (i.e.,
the topology of how layers are connected), it further generates DL
models towards increasing the coverage of layer APIs.

• Originality. We proposed MEMO, a coverage-guided model
generation framework for DL library testing. It is the first
work that points out the importance of test input (i.e., DL
model) diversity on DL library testing. To boost the model
diversity, we utilize three effective layer coverage criteria, a
set of mutation operators, and a Monte Carlo Markov Chain-
based search algorithm to generate diversed model, and thus
enhance the testing performance.

• Effectiveness. Our evaluation shows that MEMO outper-
forms the state-of-the-art work by coverage 10.3% more layer
pairs and 15.3% layer parameters.

• Efficiency. We significantly boost the efficiency by propos-
ing a model reduction technique based on our coverage crite-
ria. As a result, the average generation time for each model
can be reduced by 83.9%.

• Usefulness. We applied MEMO to seven popular DL li-
braries. It successfully detected 29 new bugs that caused
crash, NaN and inconsistent outputs. 17 out of these 29 new
bugs have been confirmed by the developers, and 5 of those
confirmed bugs have been fixed.

2 BACKGROUND AND MOTIVATION
2.1 Testing DL Libraries by DL Models
Prior testing techniques commonly use the availability of high-level
libraries such as Keras to construct a DL model. By feeding these
models to DL libraries for making inference, code in the loading and
inferencing modules will be executed. For test oracle design, they
adopt differential testing to apply the same DL model on multiple
different DL libraries with model conversion tools or shared high-
level APIs. These techniques complement the unit test suites of a DL
library by invoking a sequence of library APIs for the end-to-end
completion of a machine learning.

CRADLE [31] is the earliest technique proposed to test DL li-
braries by collecting publicly-available models and datasets to de-
tect inconsistent outputs across DL libraries. It successfully detects
implementation differences of Keras across different library back-
ends. Based on these existing DL models, two model generation
techniques: LEMON [37] and Audee [17] were later proposed to
generate DL models driven by output inconsistency. These two
techniques propose a set of mutation operators such as mutating
model’s weights, model’s architectures, and layer’s parameters to
explore library code. They further propose some heuristic-based
fitness functions to guide DL model mutations so that the generated

2.2 Model Generation Strategies
Essentially, existing techniques as mentioned in Section 2.1 adopt
different strategies in model generation. For example, LEMON
generates models to enlarge the output inconsistency across DL
libraries. Besides output inconsistency, Audee favors generating
models that can trigger the Not-A-Number value in DL libraries.
GraphFuzz designs an operator-level coverage which is defined as a
weighted sum of five coverage criteria (i.e., operator type coverage,
input degree coverage, output degree coverage, single edge cov-
erage, shape&parameter coverage) derived from graph theory. It
further iteratively mutates models towards a higher operator-level
coverage result. Muffin adopts the strategy of maximizing layer type
coverage (i.e., the percentage of invoked layer APIs in all the pre-
defined layer APIs) in model generation. However, these strategies
are limited in several aspects. First, enlarging output inconsistency
has little help in boosting models’ diversity. Second, only focusing
on layer type coverage is insufficient since it can easily reach 100%
while increasing operator-level coverage proposed by GraphFuzz
is limited to increasing the diversity of API calls (e.g., we observe
that increasing the shape coverage in GraphFuzz rarely triggers a
new API call sequence or new control flows in the API).

Like Muffin and GraphFuzz, in this paper, we also adopt cov-
erage strategies to generate diverse DL models that can exercise
possible computation of models by the library. As illustrated by
the Keras script in Figure 2, the decision of layer types, layer pairs
(i.e., a layer calling sequence from one layer to another layer), and
layer parameters greatly affect the underlying computation of a
generated model. This motivates us to define a more optimal model
generation strategy for DL library testing guided by three coverage
criteria including two (layer type coverage and layer pair coverage)
adopted from existing works [15, 27] and a newly proposed one
(layer parameter coverage).

• Layer Type Coverage: the percentage of invoked layer
APIs in all the pre-defined layer APIs (Same as Muffin and
GraphFuzz).

• Layer Pair Coverage: the percentage of invoked call se-
quence between two layer APIs among all the possible call
sequences between pre-defined layer APIs (Same as edge
coverage defined by GraphFuzz.

• Layer Parameter Coverage: the percentage of invoked
layer parameters (e.g., the “filters” in Conv2D layer API)
among all possible layer parameters.

Woodstock 2022, November, 2022, Woodstock, NY

Li, et al.

Based on these three types of coverage, we consider a model as
a diverse model if it contains either new layer types, new layer pairs,
or new layer parameters compared with a set of existing models.
Accordingly, the objective of our paper is to comprehensively test
DL libraries by increasing the diversity of the generated models,
and thus higher layer type coverage, layer pair coverage, and layer
parameter coverage can be achieved.

2.3 Limitations of Existing Techniques
To explore the limitations of baselines [15, 31, 37], we conducted
an experiment on the layer type coverage, layer pair coverage, and
layer parameter coverage achieved by existing works. For each
technique, we run their tool with their default hyperparameters for
six hours and further analyze the coverage results.

The experimental results reveal low coverage with respect to
layer pairs and layer parameters, suggesting the limited effective-
ness of existing works in terms of generating diverse models. Specif-
ically, only at most 11.7% layer pairs (428/3660) and 55.8% layer
parameters (1038/1860) have been covered by existing techniques.
As is shown in Figure 3, achieving high layer type coverage alone
is ineffective in detecting bugs residing in the specific layer call-
ing sequence and layer parameters. Motivated by such limitations
of existing techniques, we propose MEMO to better generate di-
verse models that result in a higher layer pair and layer parameter
coverage.

Table 1: Various Coverages by Existing Techniques

Technique

Layer Type Layer Pair Layer Parameter

Coverage

Coverage Coverage

MUFFIN 80.3%

11.7%

LEMON

39.3%

CRADLE

27.9%

5.3%

1.5%

55.8%

10.9%

8.3%

3 METHODOLOGY
3.1 Overview
As discussed in Section 1, the effective generation of diverse DL
models needs to address the challenges of runtime overhead in-
curred by large models and the large search space for model gen-
eration. Figure 4 presents the 3-step workflow of MEMO: model
reduction, model generation, and library testing. To address the
first challenge, we propose an algorithm (in Section 3.2) to reduce
the size of published models while retaining their coverage in layer
types, layer pairs and layer parameters (as defined in Section 2.2).
To address the second challenge, we propose a set of mutation op-
erators (in Section 3.3) to facilitate the search for diverse models.
The search is further empowered by an MCMC-based algorithm
(in Section 3.4), guided by a fitness function that favors diversity in
layer types, layer pairs and layer parameters. Finally, we perform
differential testing using these models on multiple DL libraries. Any

behavioral inconsistency (i.e., library crashes or output inconsis-
tency across libraries) is regarded as an indicator of potential bugs
in DL libraries.

3.2 Reduction Algorithm for Original Model
We observe that large DL models are typically formed by duplicated
layers and parameters, reducing the degree of such duplication
does not affect the coverage in layer types, layer pairs, and layer
parameters. However, arbitrary removal of duplicated layers and
parameters does not work well. As shown in Figure 5, layers in
an off-the-shelf model are not arbitrarily connected. Indeed, these
layers are carefully connected to fulfill constraints in tensor shapes
and dimensions. The shape constraint captures a specific assump-
tion made by a merging layer, such as an Add layer, on the tensor
shape. The dimension constraint captures a specific assumption
made by a layer on the tensor dimension (e.g., the Conv2D layer
can only accept four-dimensional tensor as input). We observe that
existing DL models are usually composed of several blocks, which
is a connection between multiple layers. These blocks are further se-
quentially connected to construct the DL model. Take the structure
of InceptionResNetV2 as an example (in Figure 6). The Inception-
ResNetV2 is formed by eight types of blocks and their repetitions.
Therefore, model reduction can be achieved by reducing the scale
of each block (intra-block reduction), followed by the removal of
repeated blocks (inter-block reduction). MEMO therefore removes
a duplicated layer from a block in intra-block reduction only if its
removal does not change the tensor shape or tensor dimension.
For the inter-block reduction, since all blocks are sequentially con-
nected with no merging layer, only the dimension constraint shall
be considered. Therefore, MEMO iterates each block and removes
the duplicated block only if removing those duplicated blocks will
not change the tensor dimension. Algorithm 1 shows the logic of
original model reduction. MEMO collects pre-defined blocks from
the original model (line 2), visits each block in turn to perform
intra-block reduction (line 5-15), and finally removes duplicated
blocks to perform inter-block reduction (line 16-20).

Figure 6 shows a reduced model of InceptionResNetV2. After
model reduction, the total number of layers in the original model
can be reduced from 782 to 69 and the total number of parameters
can be reduced from 55 million to 3 million. As a result, the runtime
overhead for TensorFlow to load and inference on the reduced
model can be lessen from 65 seconds to six seconds while the layer
types, layer pairs and layer parameters are preserved.

3.3 Model Mutation Operators
Existing model mutation operators are mainly designed to generate
mutants by changing the structure or weights of given seed DL
models. The design, however, does not consider the diversity of
mutants. Examples of these operators are inverting the activation
state of neurons and layer duplication. However, these operators
fail to introduce new layer types, layer pairs, or layer parameters
to the generated mutants.

In this section, we propose seven mutation operators to generate
diverse model mutants for the three coverage criteria. As shown
in Table 2, these operators can be classified into structure-level

MEMO: Coverage-guided Model Generation For Deep Learning Library Testing

Woodstock 2022, November, 2022, Woodstock, NY

Figure 4: Overview of MEMO

Algorithm 1: Original Model Reduction

Input: m: an original model
Output: m′: reduced model

1 def ModelReduction(𝑚):
𝐵 ← collect block(𝑚)
2
𝐵𝑑𝑖𝑣𝑒𝑟𝑠𝑒 ← [] // initiate diverse block list
foreach 𝑏𝑖 in 𝐵 do

3

4

Figure 5: Example of Invalid Model that Violates Tensor
Constraints

Figure 6: Illustration of Model Reduction on InceptionRes-
NetV2

mutations and layer-level mutations. In addition, we include a Not-
A-Number (NaN) mutation operator to mutate the input of layer to
NaN or Inf for robustness testing.
Structure-Level Mutation. Four mutation operators (i.e., NLA,
LResh, LMerge, and LConn) are designed to mutate the structure of
DL models.

The designs of NLA, LResh and LMerg aim at adding new layers
with different functionality to the seed model. Intuitively, if a layer
API has not been invoked during the previous model generation,
it has a higher priority to be chosen. However, we find that the
layer type coverage can be easily accomplished, where most layer
APIs have been invoked. When it occurs, mutation operators would

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

// intra-block reduction
𝑉 , 𝐸 ← [], [] // initiate diverse node, edge list
𝐿 ← collect layer(𝑏𝑖 )
foreach 𝑙𝑖 in 𝐿 do

𝑣, 𝑒 ← extract layer and edge property from 𝑙𝑖
if 𝑣 ∉ 𝑉 OR 𝑒 ∉ 𝐸 then

𝑉 .update(𝑣)
𝐸.update(𝑒)

else

𝐿.pop(𝑙𝑖 )

𝑏 ′
𝑖 ← connect layer(𝐿)
// inter-block reduction
if 𝑏 ′

𝑖 ∉ 𝐵 then
𝐵𝑑𝑖𝑣𝑒𝑟𝑠𝑒 .update(𝑏 ′
𝑖 )

else

𝐵.pop(𝑏𝑖 )

𝑚′ ← connect block(𝐵)
return 𝑚′

favor those layer APIs that have not been invoked by the current
seed model. The LConn operator focuses on adding new layer pairs
to the seed model, following a similar principle to prioritize those
layer pairs that have not been invoked before. The four mutation
operators work as follows:

• New Layer Addition (NLA): Given a seed model, NLA ran-
domly inserts 𝑛 (a random number range from 1 to 5) new
layer APIs that have not been invoked before to the seed
model. If the total number of these APIs (i.e., 𝑚) is less than
𝑛, NLA randomly chooses the rest 𝑛 − 𝑚 layer APIs from

Original Model ReductionDiverse Model GenerationMutation Operators OpSeed Pools < M ×Op >Select Seed(#!, ()!)New Model *+"($")Bug DetectionUpdateBugsDifferential TestingInitial ModelsReduced Models MDL Library ADL Library BDL Library CShape ConstraintDimension ConstraintAddTensor (batch, 10, 10, 3)Tensor (batch, 10, 12, 3)Conv2DTensor (batch, 10, 3)Original Model (782 layers, 55 M parameters)Reduced Model (69 layers, 3 M parameters)Block 1Block 4Block 6Block 8Block 2Block 3Block 5Block 710X20X10XInputOutputOutputInputWoodstock 2022, November, 2022, Woodstock, NY

Li, et al.

Table 2: Diversity-Driven Mutation Operators

Type

Names

Description

New Layer Addition (NLA) Add n new layers

Layer Reshaping (LResh)

Add n new reshape layers

Layer Merging (LMerge)

Merge two layers

Layer Connecting (LConn)

Connect two layers

Change Shape (CShape)

Change the shape parameter of a layer

Change Mode (CMode)

Change the mode parameter of a layer

Structure

Layer

Others

NaN (NaN)

Change the input of layer to infinity or NaN

the set of layer APIs that have not been covered by this seed
model instead.

• Layer Reshaping (LResh): Given a seed model, LResh ran-
domly inserts 𝑛 (a random number range from 1 to 5) reshap-
ing layer APIs that have not been invoked before to the seed
model. If the total number of these APIs (i.e., 𝑚) is less than
𝑛, NLA randomly chooses the rest 𝑛 − 𝑚 reshaping layer
APIs from the set of reshaping layer APIs that have not been
covered by this seed model instead.

• Layer Merging (LMerg): Given a seed model, LMerg ran-
domly chooses a merging layer to merge two layer APIs in
the seed model that has the same shape.

• Layer Connecting (LConn): Given a seed model, LConn
analyzes all possible layer pairs on the seed model and ran-
domly chooses one that has not been invoked before. If no
new layer pair is identified, LConn randomly connects two
layers.

Layer-Level Mutation. Two mutation operators, i.e., CMode and
CShape, are designed to increase the model diversity on layer pa-
rameters.

To mutate layer parameters, we categorize our target layers’
parameters into two types: (1) mode parameters such as the layer
initialization mode and the layer regularization mode controlling
the computation scheme in a layer API, and (2) shape parameters
such as filters or kernel_sizes determining the shape of ten-
sor to be computed by layers. The two mutation operators work as
follows:

• Change Mode Param (CMode): Given a seed model, CMode
randomly chooses 𝑛 (a random integer between 1 and 5) layer
APIs and changes their mode parameters to a new option
that has not been invoked before (for example, changing
kernel_initializer from “Zeros” to “Ones” or changing
filters from 10 to 20). If the mode parameter has invoked
all possible options, CMode further change the target param-
eter to a different one encouraging the exploration of a new
parameter combination in the target layer.

• Change Shape Param (CShape): Similar as CMode, CShape
randomly chooses 𝑛 layer APIs and change their shape pa-
rameters to a new option with the intention to increase the
coverage of shape parameters

In addition, we design a mutation operator named NaN to change
the input of a layer to Not-A-Number (NaN) or infinity, this muta-
tion operator is designed to change the input value of layer APIs
for robustness testing.

3.4 Diversity-Driven Model Generation
Since not all mutation operators and seed models are equally effec-
tive in generating diverse models, random selection of mutation
operators and seed models may not help. To guide the model gen-
eration, we adopt the Metropolis-Hasting algorithm, an MCMC
(Markov Chain Monte Carlo) sampling method for the exploration
of diverse DL models. Specifically, we design a diversity based fit-
ness function to quantify the potential effectiveness of mutation
operators and seed models. In each iteration of model generation,
MEMO selects the best mutation operator and seed model based
on their fitness value.

Fitness Function. In the MCMC sampling process, we define
the seed as a combination of one model (𝑚) and one mutation op-
erator (𝑜𝑝). The combination allows tracking the useful mutation
operator that works well for a seed model. Based on each seed
𝑠 = ⟨𝑚, 𝑜𝑝⟩, MEMO is able to generate a new model by applying the
selected mutation operator 𝑜𝑝 to the seed model 𝑚. The formulation
is motivated by an observation that not all mutation operators can
generate diverse models from the same seed model. We quantify
the effectiveness of a pair of mutation operator and seed model as
follows: for each pair of 𝑜𝑝 and 𝑚, if applying 𝑜𝑝 to 𝑚 has success-
fully generated diverse models, the pair will have a higher chance
to be chosen for future model generation. Following Section 2.2, we
use Equation 1 to measure the diversity of a generated model 𝑚′.
Specifically, we compare the coverage achieved by 𝑚′ ∪ 𝑀 and 𝑀
where 𝑀 is the set of previously generated models, if 𝑚′ contains
new layer types, layer pairs or layer parameters, 𝑑𝑖𝑣𝑒𝑟𝑠𝑒 (𝑚′) will
return a positive number indicating that a diverse model is found;
otherwise 𝑑𝑖𝑣𝑒𝑟𝑠𝑒 (𝑚′) is zero.

𝑑𝑖𝑣𝑒𝑟𝑠𝑒 (𝑚′) = 𝐶𝑜𝑣 (𝑚′ ∪ 𝑀) − 𝐶𝑜𝑣 (𝑀)

(1)

After collecting the total number of diverse models generated by
each mutation operator 𝑜𝑝 and seed model 𝑚, we further quantify
their performance based on the total number of diverse models
generated as Equation 2.

𝑠𝑐𝑜𝑟𝑒 (𝑚) = # of diverse models generated from 𝑚
𝑠𝑐𝑜𝑟𝑒 (𝑜𝑝) = # of diverse models generated using 𝑜𝑝

(2)

Therefore, during each iteration, MEMO is able to prioritize mu-
tation operators and seed models. After quantifying each mutation
operator and each seed model’s performance, we can define the
fitness function for each seed 𝑠 as a sum of the mutation operator’s
effectiveness 𝑠𝑐𝑜𝑟𝑒 (𝑜𝑝) and seed model’s effectiveness 𝑠𝑐𝑜𝑟𝑒 (𝑚) by
Equation 3.

𝑓 𝑖𝑡𝑛𝑒𝑠𝑠 (𝑠 = ⟨𝑚, 𝑜𝑝⟩) =

𝑠𝑐𝑜𝑟𝑒 (𝑚) + 𝑠𝑐𝑜𝑟𝑒 (𝑜𝑝)
# total times selected + 1

(3)

Based on Equation 3, a seed 𝑠, i.e., ⟨𝑚, 𝑜𝑝⟩, is more likely to be
selected if the mutation operator 𝑜𝑝 and the seed model 𝑚 have
higher score. However, following existing work [11], if a seed 𝑠 is
frequently selected but fail to introduce diverse models in many

MEMO: Coverage-guided Model Generation For Deep Learning Library Testing

Woodstock 2022, November, 2022, Woodstock, NY

iterations, we should lower its priority so other seeds are more
likely to be chosen, therefore, we also consider the total times each
seed 𝑠 has been selected in our fitness function 3.

Seed Selection. MEMO adopts the MH algorithm, a Markov
Chain Monte Carlo (MCMC) algorithm which has been commonly
used by existing testing techniques to guide the seed selection [12,
37]. The MH algorithm is designed for obtaining random samples
from a probability distribution. It works by generating a sequence
of samples whose distribution closely approximates the designed
distribution. Samples are produced iteratively, where the acceptance
of the next sample (say 𝑠2) depends only on the current one (say
𝑠1). In our setting, a sample corresponds to a seed, i.e., a pair of a
mutation operator and a model

Like existing works [11, 37], we use the geometric distribution as
the desired distribution. The geometric distribution is the probabil-
ity distribution of the number X of Bernoulli trials needed to obtain
one success. Specifically, if the probability of success on each trial
is 𝑝, the probability that the 𝑘𝑡ℎ trial is the first success is given by
𝑃𝑟 (𝑋 = 𝑘) = (1 − 𝑝)𝑘−1𝑝, where 𝑝 ∈ (0, 1) is a hyperparameter
related to the total number of seeds [11]. We sort all seeds based
on their fitness scores computed by Equation 3 from the highest to
the lowest. By mapping the sorted seeds into the geometric distri-
bution, the sampling process is thereby able to prefer those with
high fitness score than those with low fitness score. Specifically,
suppose the last selected seed is 𝑠1, the probability to accept seed
𝑠2 is calculated by Equation 4. In particular, if the 𝑘2 (the index
of 𝑠2 after we sort all seeds) is smaller than 𝑘1 (the index of 𝑠1),
i.e., the fitness score of 𝑠2 is larger than that of 𝑠1, 𝑃 (𝑠1 → 𝑠2) = 1
and we directly accept 𝑠2. Otherwise, we accept 𝑠2 with a certain
probability (1 − 𝑝)𝑘2−𝑘1 .

𝑃 (𝑠1 → 𝑠2) = 𝑚𝑖𝑛

(cid:18)

1,

(cid:19)

𝑃𝑟 (𝑠2)
𝑃𝑟 (𝑠1)

(cid:16)

= 𝑚𝑖𝑛

1, (1 − 𝑝)𝑘2−𝑘1

(cid:17)

(4)

Algorithm 2 shows the detailed process for MEMO to generate
models. Given the reduced initial model pool 𝑀 and mutation oper-
ators 𝑂𝑝, MEMO first constructs the initial seed pool 𝑆 by iterating
all possible combinations between 𝑀 and 𝑂𝑝 (in line 3). In each
iteration, MEMO uses the MH algorithm for seed selection (in func-
tion SeedSelection), and further generates new models based on the
selected seed (in line 5-7). If the generated model is a diverse one,
i.e., triggers new code branch in DL libraries, MEMO updates the
original model by the new one (in line 8-10, function UpdateSeeds).

3.5 Bug Detection
In each iteration, the generated model will be sent to different DL
libraries for testing. For bug detection, we follow existing works [15,
17, 27, 31, 37] to adopt the differential testing as the test oracle. Over-
all, MEMO targets at three types of bugs: crash bugs, Not-A-Number
(NaN) bugs and inconsistency bugs. For crash bugs and NaN bugs, we
consider a bug is revealed if some DL libraries crash or output NaN
value when loading and executing the model while some libraries
do not. However, because of the computation randomness during
the DL library, two libraries’ output on the same model and same
input are not exactly the same. Therefore, we adopt a well-defined
inconsistency metric, i.e., 𝐷_𝑀𝐴𝐷, proposed by CRADLE [31] for
inconsistency bug detection. In particular, 𝐷_𝑀𝐴𝐷 calculates the

Algorithm 2: Diverse Model Generation Algorithm

Input: M: a model pool; Op: a mutation operator pool;

P: stopping signal
Output: M′: All generated models

1 def ModelGeneration(𝑃, 𝑀, 𝑂𝑝):
2

𝑀 ′ ← 𝑀
𝑆 ← 𝑀 × 𝑂𝑝 // initialize seed pool S
while ! 𝑃 do

𝑠𝑖 ← SeedSelection (𝑆)
𝑚𝑖, 𝑜𝑝𝑖 ← 𝑠𝑖
𝑚′
if cov(𝑚′

𝑖 ← 𝑜𝑝𝑖 (𝑚𝑖 )

𝑖 ∪ 𝑀 ′) > cov(𝑀 ′) then
𝑖 can increase coverage

// If 𝑚′
𝑚𝑖 ← 𝑚′
𝑖
UpdateSeeds(𝑆, 𝑚𝑖, 𝑚′
𝑖 )

𝑀 ′ ← 𝑀 ′ ∪ {𝑚′
𝑖 }

return 𝑀 ′
13
14 def SeedSelection(𝑆):
15

// select candidate based on MH algorithm
𝑆𝑠𝑜𝑟𝑡𝑒𝑑 ← SortSeed(𝑆) // Using Fitness Function (Eq. 3)
𝑝𝑟𝑜𝑏 ← 0
𝑘1 ← get the index of last selected seed
while New Rand() ≥ 𝑝𝑟𝑜𝑏 do

𝑠𝑖 ← random.Choice(𝑆𝑠𝑜𝑟𝑡𝑒𝑑 )
𝑘2 ← 𝑆𝑠𝑜𝑟𝑡𝑒𝑑 .𝑖𝑛𝑑𝑒𝑥 (𝑠𝑖 )
𝑝𝑟𝑜𝑏 ← (1 − 𝑝)𝑘2−𝑘1

return 𝑠𝑖

23
24 def UpdateSeeds(𝑆, 𝑚𝑖, 𝑚′
𝑖 ):
25

foreach 𝑠 𝑗 ∈ 𝑆 do
if 𝑚𝑖 ∈ 𝑠 𝑗 then
𝑚𝑖, 𝑜𝑝 𝑗 ← 𝑠 𝑗
𝑠 𝑗 ← 𝑚′
𝑖, 𝑜𝑝 𝑗

3

4

5

6

7

8

9

10

11

12

16

17

18

19

20

21

22

26

27

28

inconsistency distance by comparing the relative distance of model
outputs from the ground-truth labels [31]. Specifically, given two
predicted vectors 𝑌 , 𝑌 ′, and the ground truth 𝑂, 𝛿𝑌 ,𝑂 and 𝛿𝑌 ′,𝑂 are
calculated to measure the distance between the predicted vectors
and the ground truth using Equation 5, and the distance between
two predicted vectors are then measured by Equation 6.

𝛿𝑌 ,𝑂 =

1
𝑁

𝑁
∑︁

𝑖=1

|𝑌𝑖 − 𝑂𝑖 |

𝐷_𝑀𝐴𝐷𝑂,𝑌 ,𝑌 ′ =

|𝛿𝑌 ,𝑂 − 𝛿𝑌 ′,𝑂 |
𝛿𝑌 ,𝑂 + 𝛿𝑌 ′,𝑂

(5)

(6)

4 EVALUATION
We evaluated the performance of MEMO from three perspectives:
bug detection performance, model generation diversity, and model
generation efficiency. In particular, we studied four research ques-
tions (RQs):

Woodstock 2022, November, 2022, Woodstock, NY

Li, et al.

Table 3: Performance on Bug Detection (The number of confirmed bugs is parenthesized.)

DL Libraries

ONNXRuntime MXNet Keras-MXNet TF2ONNX ONNX2PyTorch Keras TensorFlow PyTorch

Crash

NaN

3 (2)

2 (2)

Inconsistency

0 (0)

Subtotal

5 (4)

4 (0)

1 (1)

0 (0)

5 (1)

2 (0)

0 (0)

0 (0)

2 (0)

5 (3)

1 (1)

0 (0)

6 (4)

3 (1)

0 (0)

0 (0)

3 (1)

3 (3)

4 (3)

0 (0)

0 (0)

0 (0)

1 (1)

7 (6)

1 (1)

0 (0)

0 (0)

0 (0)

0 (0)

• RQ1: Can MEMO detect new bugs? We first demonstrate the
bug-revealing capability of MEMO to reveal its usefulness. If
real new bugs can be detected by exploring model diversity, the
outstanding of our work may be indicated.

• RQ2: Can MEMO outperform existing techniques? To fur-
ther demonstrate the advantage of our diversity-driven approach,
we then compare MEMO against state-of-the-art techniques in
terms of the generated models’ diversity (i.e., layer type cov-
erage, layer pair coverage, and layer parameter coverage). We
also compare the branch coverage result to evaluate MEMO’s
effectiveness in exploring library code.

• RQ3: Can test efficiency be boosted with reduction of ini-
tial models? We also conduct the experiment to show the impact
of the reduction of the initial model on the test efficiency by illus-
trating the trends of coverage growth achieved with and without
model reduction.

• RQ4: To what extent do the proposed set of mutation op-
erators and the search algorithm contribute to the perfor-
mance of MEMO? Finally, to further dissect the secrets of MEMO,
we conduct an ablation study to analyze two key components
(i.e., model mutation operators and the MCMC-based search al-
gorithm) in MEMO, examining their usefulness in generating
diverse models and exercising various computations.

4.1 Experiment Setup
Experiments for RQ1 and RQ2 were conducted on a machine pow-
ered by Intel i9 with one Titan RTX and two 2080Ti GPU cards. In
parallel, experiments for RQ3 and RQ4 were conducted on another
machine powered by dual Xeon with eight 2080Ti GPU cards.

Baselines. We considered three state-of-the-art DL testing tech-
niques as baselines in our experiment: CRADLE [31], LEMON [37]
and Muffin [15]. Since no executables or sources of GraphFuzz [27]
and Audee [17] are available, we did not include them as baselines.
The three baselines are differently designed. CRADLE focuses on
detecting the inconsistency issue exposed by existing DL models.
Since it does not involve model generation, we consider the total
layer types, layer pairs, and layer parameters in the initial models
as their overall performance. For LEMON and Muffin, we used their
default settings to generate models. Also, since other baselines do
not consider the training phase during testing, so for Muffin which
targets at both training and inference phases, we only considered
the inference phase for the fairness of the comparison.

Benchmarks and Implementations. We selected 10 popular
DL models with different model structures as initial seed mod-
els. They cover diverse application domains across both image
data and sequential data. They have also been used by the four
baselines in the evaluation. In particular, eight DL models (i.e.,
LeNet, AlexNet, DenseNet121, InceptionResNetV2, InceptionV3,
MobileNetV2, ResNet50, Xception) are image classification models
trained on MNIST [23], CIFAR-10 [21], and ImageNet [22]. The
remaining two models are LSTM-based models trained on Sine-
Wave dataset and Stock-Price dataset by existing work [37]. All DL
models are online-available on our project website.3 The setting
of hyperparameter 𝑝 in our algorithm depends on the total size of
seeds in our MH algorithm [11]. Since our benchmark contains 10
initial DL models and 7 mutation operators, the total size of initial
seeds is 70 (10 initial models × 7 mutation operators). Following the
setting recommended by existing work [11], the range of 𝑝 should
be within [0.042, 0.056]. In our implementation, we set 𝑝 to 0.05.

DL Libraries Under Test. We evaluated the bug detection per-
formance of MEMO on the latest release of eight popular DL li-
braries: Keras 2.8.0, TensorFlow 2.9.0, PyTorch 1.10.0, MXNet 1.9.0,
ONNXRuntime 1.10.0, Keras-MXNet 2.2.4.3, TF2ONNX 1.9.3, and
ONNX2Pytorch 0.4.1. Following earlier practice [15, 31, 37], MEMO
leverages Keras APIs to generate models uniformly for the testing
of different DL libraries. The generated models are converted for
MXNet, ONNX, and PyTorch using three libraries, Keras-MXNet,
TF2ONNX and ONNX2Pytorch. The setup helps to control the
variation of models used for different libraries.

Unique Bugs identification. Following the practice of existing
work [37], we manually analyzed each detected bugs and identify
the unique ones. We consider a crash bug unique if it has a distinct
stack trace. For NaN and inconsistency bugs, we manually located
the layer that triggers the bugs and analyze the root cause. If multi-
ple bugs are caused by the same root cause, we consider them as
duplicated bugs and only consider one of them in the subsequent
analysis.

4.2 RQ1: Effectiveness of Bug Detection
Table 3 demonstrates the new bugs detected by MEMO across
various DL libraries, categorized by their bug symptoms. In total,
MEMO detected 29 new bugs in the latest release version of evalu-
ated DL libraries, 17 of them have been confirmed by DL library

3https://github.com/MEMOASE/MEMO

MEMO: Coverage-guided Model Generation For Deep Learning Library Testing

Woodstock 2022, November, 2022, Woodstock, NY

developers, and 5 out of these confirmed bugs have been fixed. The
remaining 12 bugs have been reported and are waiting for confir-
mation. The detected bugs can be categorized according to three
different symptoms.

Crash Bugs. Among the bugs detected, 20 are crash bugs. Nine
of them have been confirmed by developers, including two fixed. We
further analyzed the nine confirmed crash bugs. Four occurred at
model conversion (three in TF2ONNX library and one in ONNX2Py-
Torch library). The remaining five crash bugs occurred at model
execution (two bugs in ONNXRuntime and three bugs in Keras).
We noticed that only one crash bug is caused by a single layer with
its default parameters. Eight of the nine crash bugs were mani-
fested by either specific layer pairs (e.g., a layer pair between two
ThresholdReLU layers will cause TF2ONNX crashes) or specific
layer parameters (e.g., failing to set either strides or dilation_
rate in SeparableConv2D to 1 can result in the wrong shape in-
ference, causing Keras to crash). The experiment result affirms the
importance of generating models with diverse layer parameters
and layer pairs. Moreover, MEMO detected one severe crash bug
that directly leads to a core dump in ONNXRuntime library (as is
shown in Figure 3). This bug is caused by a specific layer pair (i.e.,
Dense-Dot) and a special layer parameter (i.e., units=0). For this
issue, developers instantly created a pull request to fix it after our
reporting.

NaN Bugs. Eight of the confirmed bugs are Not-A-Number (NaN)
bugs. Seven of them have been confirmed by developers. Three of
the confirmed NaN bugs were manifested by some special layer
parameters, such as setting the max_value in the ReLU layer to a
concrete value instead of “None” (the default option). One NaN bug
was caused by a specific layer pair (i.e., Conv2D+Multiply) and the
remaining three NaN bugs were exposed when a NaN value was
passed to a layer with its default parameter. Since the manifestation
of the majority of NaN bugs requires a specific layer parameter
or layer pair, generating models by covering diverse layer pairs or
layer parameters are important for detecting NaN bugs.

Inconsistency Bugs. One of the confirmed bugs is inconsis-
tency bug between TensorFlow library and ONNXRuntime library.
We find that this bug lies in the model conversion phase in Ten-
sorFlow. Specifically, when TensorFlow converts model from the
Keras format to TensorFlow Protobuf format (a private format of
DL model inside TensorFlow library), the bug will be triggered if
the axis parameter in the Softmax layer is set to “-2”.

In conclusion, our experiment shows that MEMO is effective in
detecting bugs in DL libraries. More importantly, we found that 13
out of 17 confirmed bugs can only be triggered by deliberated layer
pairs or layer parameters, while only four bugs can be manifested
by directly calling a layer API alone. The finding further supports
the insight of MEMO, i.e., models with diverse layer parameters
and layer pairs are useful in detecting DL library bugs.

4.3 RQ2: Comparison with Baselines
We compare the performance of MEMO with three baselines: CRA-
DLE, LEMON, and Muffin. Specifically, we evaluated their per-
formance by running them on the same initial DL models (see
Section 4.1) for six hours. Note that CRADLE tests DL libraries
by directly collecting publicly-available DL models. We, therefore,

Table 4: Coverage by MEMO and Baselines

Technique

Branch

Layer Type Layer Pair Layer Parameter

Coverage Coverage

Coverage Coverage

MEMO

18.4%

100%

22.0%

71.1%

MUFFIN 16.1%

LEMON

12.1%

CRADLE

11.6%

80.3%

39.3%

27.9%

11.7%

5.3%

1.5%

55.8%

10.9%

8.3%

Table 5: Comparison of Original Models and Their Reduced
Models

Original Models

Reduced Models

Average Number of Layers

103.25

Average Number of Weights

30.528 M

Average Inference Time

33 sec.

11.0

0.595 M

2.5 sec.

referred to the coverage results of initial DL models to capture CRA-
DLE’s performance. Since Muffin does not require initial DL models
for model generation, we ran it for six hours using its default setting
to capture the coverage result. Table 4 presents the layer type cover-
age, layer pair coverage, and layer parameter coverage achieved by
the baselines. We also include the branch coverage of TensorFlow’s
model construction and model execution modules as an additional
performance metric beyond the three proposed ones for reference.
The results suggest correlation between the branch coverage and
the diversity of generated models in terms of layer types, layer pairs
and layer parameters. MEMO outperforms baselines by covering at
least 2.3% more new branches in TensorFlow’s model construction
and execution modules. Nevertheless, the issue of relatively low
branch coverage is common to fuzzing-based techniques. It is an
interesting problem for future research.

4.4 RQ3: Efficiency of Model Reduction
In this research question, we aim to study the effectiveness of the
model reduction algorithm. To perform the study, we prepared two
sets of initial seed models based on the original DL models collected
in Section 4.1 and the models reduced from these original models.
As shown in Table 5, original models with an average of 100 layers
and 30 million weights can be reduced to models with an average
of 11 layers and 0.595 million weights. As a result, the average
inference time after using the reduced models was shortened from
33 to 2.5 seconds.

The experiment result shows that reducing the collected original
models can significantly boost test efficiency. Specifically, we ran
MEMO’s model generation algorithm separately with the original
models and reduced models for six hours. The coverage result in
Figure 7 demonstrates clear improvement in terms of efficiency af-
ter model reduction. In particular, the layer pair coverage increases
from 4.5% to 14.1% and the layer parameter coverage increases by

Woodstock 2022, November, 2022, Woodstock, NY

Li, et al.

Figure 7: Coverage by using Original Models and Their Re-
duced Models

Figure 8: Comparison of MEMO, MEMO_o and MEMO_r

from 20.7% to 43.4%. We also compared the average model genera-
tion overhead on the original DL models and reduced DL models.
On average, it took MEMO around 45 seconds to generate and
test each DL model mutated from the reduced DL models, while
it takes around 279 seconds using the original DL models without
reduction.

4.5 RQ4: Ablation Study on Mutation
Operators and Search Algorithm

We further evaluate the effectiveness of our mutation operators and
MCMC-based search algorithm. To conduct an ablation study of the
proposed MCMC-based search algorithm, we constructed a base-
line named as MEMO_r. MEMO_r is the same as MEMO, except that
MEMO_r adopts a random mutation strategy instead of the MCMC-
based search algorithm. In other words, MEMO_r randomly selects
a mutation operator and a seed model in each iteration for model
generation. To conduct an ablation study of the proposed mutation
operators, we constructed another baseline named as MEMO_o. In-
stead of using our proposed mutation operators, MEMO_o adopts
the existing mutation operators proposed by LEMON [37], Deep-
Mutation [28], and Audee [17].

Same as RQ3, we ran MEMO, MEMO_o and MEMO_r for six hours
and compared their effectiveness in terms of the three coverage
criteria. We present our result in Figure 8. Our result shows that,
compared to MEMO_o and MEMO_r, MEMO can effectively increase
the diversity of generated models in terms of layer type coverage,
layer pair coverage and layer parameter coverage. The improve-
ment arises from the use of our proposed mutation operators is
especially significant (e.g., increase the layer parameter coverage
by 31.6%). The improvement arises from the use of our proposed
search algorithm is also noticeable (e.g., increase the layer parame-
ter coverage by 9.5%). The result of the ablation study shows that
both our mutation operators and search algorithm contribute to
generating diverse DL models.

5 THREATS TO VALIDITY
Overall, there are three threats which may affect the experimental
results. We ran experiments with a time budget of six hours for
each baseline on the same machine in turn. Yet, the amount of
available system resources for the experiments may vary across
24 hours. To alleviate the influence caused by system computing
performance, we set the total number of threads/GPU cards used in
each experiment to be the same. The second threat is the number
of new bugs detected by MEMO. For those unconfirmed crash bugs,
following existing work [37], we used the stack trace to distinguish
them, however, it is possible that one crash bug can have different
stack trace. To alleviate this problem, we manually checked the
root cause of each unconfirmed crash bugs to reduce false positives.
The third threat is introduced by the non-determinism during the
model generation. MEMO adopts MCMC search algorithm, where
the randomness is involved. To alleviate the threat, for every ex-
periment, we ran 10 times and presented the average result in our
evaluation.

6 RELATED WORKS
6.1 Testing of DL Library
There are five state-of-the-art techniques related to our work, namely,
CRADLE [31], Audee [17], LEMON [37], GraphFuzz [27] and Muf-
fin [15]. They either generate mutants of published DL models or
their datasets [17, 27, 31, 37] or generate models from scratch using
predefined model structure templates [15]. They propose a set of
criteria to measure the quality of generated models, such as incon-
sistent output across different libraries or maximizing layer types.
Three of them [15, 17, 31] also propose localization methods aiming
at locating the faulty layers or faulty API parameters. Muffin also
applies the generated model to test training related modules in DL
libraries. In contrast, we propose to generate models by diversify-
ing their layer types, layer pairs, and layer parameters. There are
other studies on a related testing problem with different focus. Doc-
Ter [24] extracts constraints of arguments of the functions from the
documentation of DL libraries and leverages them to guide the test
case generation. Predoo [39] proposes a fuzzing technique to test
the operators in DL library. It aims to estimate and detect individual
DL operator’s precision errors. FreeFuzz [38] fuzzes DL libraries
via mining from open source, FreeFuzz obtains code/models from
code snippets in the library documentation, developer tests, and
DL models in the wild. Furthermore, TVMFuzz [34] focuses on
the testing of the Deep Learning compilers. The major difference
between our works and these works is that MEMO generates DL
models as test inputs to test DL library modules related to a DL
model’s execution.

6.2 Empirical Study for DL Libraries
There are also empirical studies conducted on DL libraries. Jia et
al. [19] report an empirical study to understand the characteristics
of the bugs in TensorFlow. They further study the symptoms, causes
and repair patterns of TensorFlow bugs [20]. Nejadgholi et al [29]
study the test oracle in the unit tests of deep learning libraries.
Liu et al [26] and Guo et ai [16] propose the use of inconsistency
patterns for testing across different DL libraries. Tambon et al [36]

05000100001500020000Times (S)0.00.20.40.60.81.0CoverageLayer Type Coverage05000100001500020000Times (S)0.000.050.100.150.20CoverageLayer Pair Coverage05000100001500020000Times (S)0.00.10.20.30.40.5CoverageLayer Param. CoverageReduced ModelOriginal Model05000100001500020000Times (S)0.00.20.40.60.81.0CoverageLayer Type Coverage05000100001500020000Times (S)0.000.050.100.150.20CoverageLayer Pair Coverage05000100001500020000Times (S)0.00.10.20.30.40.5CoverageLayer Param. CoverageMEMOMEMO_oMEMO_rMEMO: Coverage-guided Model Generation For Deep Learning Library Testing

Woodstock 2022, November, 2022, Woodstock, NY

studied a typical bug in DL library, i.e., silent bugs. The silent bugs
refer to bugs that lead to wrong behavior, but no apparent bug
symptoms (e.g., system crash, hang, or exceptions raised with error
messages). Instead of focusing on the root causes and symptoms of
library bugs, we focus on the generation of diverse DL models in
terms of their layer types, layer pairs and layer parameters.

7 CONCLUSION
In this paper, we propose a novel technique named MEMO to test
DL libraries by generating diverse DL models as test inputs. Mo-
tivated by the model generation script of Keras, MEMO utilizes
three coverage criterias to measure the model diversity: layer type
coverage, layer pair coverage, and layer parameter coverage. Driven
by these coverages, MEMO proposes a model reduction technique
to significantly boost the efficiency during model generation by
addressing the challenge of runtime overhead incurred by large
models. It also designs a set of novel mutation operators and a
coverage-guided search algorithm to search for diverse DL mod-
els to achieve more comprehensive testing on DL libraries. Our
evaluation of popular DL libraries shows that MEMO significantly
outperforms the state-of-the-art DL library testing techniques in
both effectiveness and efficiency. In total, there are 29 new bugs
were detected by MEMO, among which, 17 of them were confirmed
by developers.

REFERENCES
[1] 2022. keras-apache-mxnet. https://github.com/awslabs/keras-apache-mxnet
[2] 2022. onnx2pytorch. https://github.com/ToriML/onnx2pytorch
[3] 2022. Open Neural Network Exchange. https://onnx.ai/
[4] 2022. Optimize and Accelerate Machine Learning Inferencing and Training.

https://onnxruntime.ai/

[5] 2022. tensorflow-onnx. https://github.com/onnx/tensorflow-onnx
[6] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, et al.
2016. Tensorflow: A system for large-scale machine learning. In 12th {USENIX}
symposium on operating systems design and implementation ({OSDI} 16). 265–283.
[7] A. Ardakani, Alireza Rajabzadeh Kanafi, U. Acharya, Nazanin Khadem, and A.
Mohammadi. 2020. Application of deep learning technique to manage COVID-19
in routine clinical practice using CT images: Results of 10 convolutional neural
networks. Computers in Biology and Medicine 121 (2020), 103795 – 103795.
[8] S. Bhattacharya, Praveen Kumar Reddy Maddikunta, Quoc-Viet Pham, T.
Gadekallu, S SivaRamaKrishnan, et al. 2020. Deep learning and medical im-
age processing for coronavirus (COVID-19) pandemic: A survey. Sustainable
Cities and Society 65 (2020), 102589 – 102589.

[9] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. 2015. Deepdriving:
Learning affordance for direct perception in autonomous driving. In Proceedings
of the IEEE International Conference on Computer Vision. 2722–2730.

[10] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, et al. 2015. MXNet: A
Flexible and Efficient Machine Learning Library for Heterogeneous Distributed
Systems. arXiv:cs.DC/1512.01274

[11] Yuting Chen, Ting Su, Chengnian Sun, Zhendong Su, and Jianjun Zhao. 2016.
Coverage-Directed Differential Testing of JVM Implementations. In Proceedings
of the 37th ACM SIGPLAN Conference on Programming Language Design and
Implementation (PLDI ’16). Association for Computing Machinery, New York, NY,
USA, 85–99. https://doi.org/10.1145/2908080.2908095

[12] Yuting Chen, Ting Su, Chengnian Sun, Zhendong Su, and Jianjun Zhao. 2016.
Coverage-Directed Differential Testing of JVM Implementations. SIGPLAN Not.
51, 6 (June 2016), 85–99. https://doi.org/10.1145/2980983.2908095

[13] Rajshekhar Das, Akshay Gadre, Shanghang Zhang, Swarun Kumar, and Jose MF
Moura. 2018. A deep learning approach to IoT authentication. In 2018 IEEE
International Conference on Communications (ICC). IEEE, 1–6.

[14] Aidin Ferdowsi and W. Saad. 2019. Deep Learning for Signal Authentication and
Security in Massive Internet-of-Things Systems. IEEE Transactions on Communi-
cations 67 (2019), 1371–1387.

[15] Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang. 2022. Muffin: Testing
Deep Learning Libraries via Neural Architecture Fuzzing. In 2022 IEEE/ACM 44th
International Conference on Software Engineering (ICSE). IEEE.

[16] Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, et al. 2019. An empiri-
cal study towards characterizing deep learning development and deployment

across different frameworks and platforms. In 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 810–822.

[17] Qianyu Guo, Xiaofei Xie, Yi Li, Xiaoyu Zhang, Yang Liu, et al. 2020. Audee: Auto-
mated testing for deep learning frameworks. In 2020 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 486–498.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity mappings
in deep residual networks. In European conference on computer vision. Springer,
630–645.

[19] Li Jia, Hao Zhong, Xiaoyin Wang, Linpeng Huang, and Xuansheng Lu. 2020. An
Empirical Study on Bugs Inside TensorFlow. In Database Systems for Advanced
Applications, Yunmook Nah, Bin Cui, Sang-Won Lee, Jeffrey Xu Yu, Yang-Sae
Moon, et al. (Eds.). Springer International Publishing, Cham, 604–620.

[20] Li Jia, Hao Zhong, Xiaoyin Wang, Linpeng Huang, and Xuansheng Lu. 2021. The
symptoms, causes, and repairs of bugs inside a deep learning library. Journal of
Systems and Software 177 (2021), 110935. https://doi.org/10.1016/j.jss.2021.110935
[21] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features

from tiny images. (2009).

[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. Advances in neural information
processing systems 25 (2012), 1097–1105.

[23] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning
applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–2324. https:
//doi.org/10.1109/5.726791

[24] Li, Yitong. 2020. Documentation-Guided Fuzzing for Testing Deep Learning API

Functions. Master’s thesis. http://hdl.handle.net/10012/16589

[25] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso
Setio, Francesco Ciompi, et al. 2017. A survey on deep learning in medical image
analysis. Medical image analysis 42 (2017), 60–88.

[26] Ling Liu, Yanzhao Wu, Wenqi Wei, Wenqi Cao, Semih Sahin, et al. 2018. Bench-
marking Deep Learning Frameworks: Design Considerations, Metrics and Be-
yond. In 2018 IEEE 38th International Conference on Distributed Computing Systems
(ICDCS). 1258–1269. https://doi.org/10.1109/ICDCS.2018.00125

[27] Weisi Luo, Dong Chai, Xiaoyue Run, Jiang Wang, Chunrong Fang, et al. 2021.
Graph-based Fuzz Testing for Deep Learning Inference Engines. In 43rd IEEE/ACM
International Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30
May 2021. IEEE, 288–299. https://doi.org/10.1109/ICSE43902.2021.00037
[28] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, et al. 2018. DeepMutation:
Mutation Testing of Deep Learning Systems. In 2018 IEEE 29th International
Symposium on Software Reliability Engineering (ISSRE). 100–111. https://doi.org/
10.1109/ISSRE.2018.00021

[29] Mahdi Nejadgholi and Jinqiu Yang. 2019. A Study of Oracle Approximations in
Testing Deep Learning Libraries. In 2019 34th IEEE/ACM International Conference
on Automated Software Engineering (ASE). 785–796. https://doi.org/10.1109/ASE.
2019.00078

[30] Adam Paszke, S. Gross, Francisco Massa, Adam Lerer, James Bradbury, et al.
2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library.
In NeurIPS.

[31] Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan. 2019. CRADLE:
cross-backend validation to detect and localize bugs in deep learning libraries.
In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).
IEEE, 1027–1038.

[32] Ahmad El Sallab, Mohammed Abdou, E. Perot, and S. Yogamani. 2017. Deep Rein-
forcement Learning framework for Autonomous Driving. ArXiv abs/1704.02532
(2017).

[33] S. Shalev-Shwartz, Shaked Shammah, and A. Shashua. 2016. Safe, Multi-Agent,
Reinforcement Learning for Autonomous Driving. ArXiv abs/1610.03295 (2016).
[34] Qingchao Shen, Haoyang Ma, Junjie Chen, Yongqiang Tian, Shing-Chi Che-
ung, et al. 2021. A Comprehensive Study of Deep Learning Compiler Bugs. In
Proceedings of the 29th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE
2021). Association for Computing Machinery, New York, NY, USA, 968–980.
https://doi.org/10.1145/3468264.3468591

[35] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed,
et al. 2014. Going Deeper with Convolutions. CoRR abs/1409.4842 (2014).
arXiv:1409.4842 http://arxiv.org/abs/1409.4842

[36] Florian Tambon, Amin Nikanjam, Le An, Foutse Khomh, and Giuliano Antoniol.
2021. Silent Bugs in Deep Learning Frameworks: An Empirical Study of Keras
and TensorFlow. https://doi.org/10.48550/ARXIV.2112.13314

[37] Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, and Dongdi Zhang. 2020. Deep
learning library testing via effective model generation. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 788–799.

[38] Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang. 2022. Free
Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source. arXiv
preprint arXiv:2201.06589 (2022).

[39] Xufan Zhang, Ning Sun, Chunrong Fang, Jiawei Liu, Jia Liu, et al. 2021. Predoo:
Precision Testing of Deep Learning Operators. Association for Computing Machin-
ery, New York, NY, USA, 400–412. https://doi.org/10.1145/3460319.3464843

