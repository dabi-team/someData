2
2
0
2

y
a
M
2
2

]

O
R
.
s
c
[

1
v
6
0
9
0
1
.
5
0
2
2
:
v
i
X
r
a

Monitoring of Perception Systems:
Deterministic, Probabilistic, and Learning-based
Fault Detection and Identiﬁcation(cid:63)

Pasquale Antonantea,∗, Heath Nilsena, Luca Carlonea

aMassachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139

Abstract

This paper investigates runtime monitoring of perception systems. Percep-

tion is a critical component of high-integrity applications of robotics and au-

tonomous systems, such as self-driving cars.

In these applications, failure of

perception systems may put human life at risk, and a broad adoption of these

technologies requires the development of methodologies to guarantee and mon-

itor safe operation. Despite the paramount importance of perception, currently

there is no formal approach for system-level perception monitoring. In this pa-

per, we formalize the problem of runtime fault detection and identiﬁcation in

perception systems and present a framework to model diagnostic information

using a diagnostic graph. We then provide a set of deterministic, probabilis-

tic, and learning-based algorithms that use diagnostic graphs to perform fault

detection and identiﬁcation. Moreover, we investigate fundamental limits and

provide deterministic and probabilistic guarantees on the fault detection and

identiﬁcation results. We conclude the paper with an extensive experimental

evaluation, which recreates several realistic failure modes in the LGSVL open-

source autonomous driving simulator, and applies the proposed system monitors

to a state-of-the-art autonomous driving software stack (Baidu’s Apollo Auto).

(cid:63)This work was partially funded by the NSF CAREER award “Certiﬁable Perception for

Autonomous Cyber-Physical Systems”

∗Corresponding author
Email addresses: antonap@mit.edu (Pasquale Antonante), hnilsen@mit.edu (Heath

Nilsen), lcarlone@mit.edu (Luca Carlone)

Preprint submitted to Artiﬁcial Intelligence Journal

May 24, 2022

 
 
 
 
 
 
The results show that the proposed system monitors outperform baselines, have

the potential of preventing accidents in realistic autonomous driving scenarios,

and incur a negligible computational overhead.

Keywords: Autonomous Vehicles, Perception, Safety, Runtime Monitoring.

1. Introduction

The number of Autonomous Vehicles (AVs) on our roads is increasing rapidly,

with major players in the space already oﬀering autonomous rides to the pub-

lic [1]. Self-driving cars promise a deep transformation of personal mobility and

have the potential to improve safety, eﬃciency (e.g., commute time, fuel), and

induce a paradigm shift in how entire cities are designed [2]. One key factor that

drives the adoption of such technology is the capability of ensuring and moni-

toring safe operation. Consider Uber’s fatal self-driving crash [3] in 2018: the

report from the National Transportation Safety Board states that “inadequate

safety culture” contributed to the fatal collision between the autonomous vehicle

and the pedestrian. In a recent survey [4], the American Automobile Associa-

tion (AAA) reports that vehicles with autonomous driving features consistently

failed to avoid crashes with other cars or bicycles. An analysis by Business In-

sider [5] found that the number of accidents involving AVs surged in 2021. This

is a clear sign that the industry needs a sound methodology, embedded in the

design process, to guarantee safety and build public trust.

Safe operation requires AVs to correctly understand their surroundings, in

order to avoid unsafe behaviors. In particular, AVs rely on onboard perception

systems to provide situation awareness and inform the onboard decision-making

and control systems. The perception system uses sensor data and prior knowl-

edge (e.g., high-deﬁnition maps) to create an internal representation of the

surrounding environment, including estimates for the positions and velocities of

other vehicles and pedestrians, or the presence of traﬃc signs and traﬃc lights.

Modern perception systems use both data-driven and classical methods. While

classical methods are well-rooted in signal processing and estimation theory and

2

have been extensively studied in robotics and computer vision, they may still

have unexpected failure modes in practice, e.g., local convergence in the Itera-

tive Closest Point for 3D object pose estimation [6] or premature termination

of robust estimation techniques as RANSAC [7], among many other examples.

The use of data-driven methods further exacerbates the problem of ensuring cor-

rectness of the perception outputs, since current neural network architectures

are still prone to creating unexpected and often unpredictable failure modes [8].

Ensuring and monitoring the correct operation of the perception system of

an AV is a major challenge. Industry heavily relies on simulation and testing to

provide evidence of safety. Although there is an increasing interest in the area of

safety certiﬁcation and runtime monitoring, the literature lacks a system-level

framework to organize and reason over the diagnostic information available at

runtime for the purpose of detecting and identifying potential perception-system

failures. Reliable runtime monitoring would enable the vehicle to have a better

understanding of the conditions it operates in, and would give it enough notice

to take adequate actions to preserve safety (i.e., switch to fail-safe mode or

hand over the control to a human operator) in case of severe failures. In this

paper, we use the term “failure” (or “fault”) in a general sense, to also denote

failures of the intended functionality [9, 10]. For instance, a neural network can

execute correctly (e.g., without errors in the implementation or in the hardware

running the network) but can still fail to produce a correct prediction for out-

of-distribution inputs. Then, fault detection is the problem of detecting the

presence of a fault in the system, while fault identiﬁcation is the problem of

inferring which components of the system are faulty. The latter is particularly

important since (i) not every fault has the same severity, hence understanding

which component is failing may lead to diﬀerent responses, (ii) a designer can

use fault statistics to decide to focus research and development eﬀorts on certain

components, and (iii) a regulator can use information about speciﬁc faults to

trace the steps or even determine responsibilities after an accident.

Most of the existing literature (which we review more extensively in Sec-

tion 2) has focused on detecting failures of speciﬁc modules or speciﬁc algo-

3

rithms, like localization [11, 12], semantic segmentation [13], or obstacle detec-

tion [13]. These methodologies often use a white-box approach (the monitor

knows how the monitored algorithm works to some extent), and are sometimes

computationally expensive to run [13]. However, the literature still lacks a

framework for system-level monitoring of perception systems, which is able to

detect and identify failures in complex systems involving both classical and

data-driven (possibly asynchronous, multi-modal)1 perception algorithms.

Contribution. This paper addresses this gap and provides methodolo-

gies for runtime monitoring (in particular, fault detection and identiﬁcation) of

complex perception systems. Our ﬁrst contribution is to formalize the problem

(Section 3) and to present a framework (Section 4) to organize heterogeneous

diagnostic tests of a perception system into a graphical model, the diagnostic

graph. In particular, we present diﬀerent mathematical models (including both

deterministic and probabilistic models) to describe common diagnostic tests.

Then, we introduce the concept of diagnostic graph, and extend it to capture

asynchronous information over time (leading to temporal diagnostic graphs).

Our framework adopts a black-box approach, in that it remains agnostic to

the inner workings of the perception algorithms, and only focuses on collecting

results from diagnostic tests that check the validity of their outputs.

Our second contribution (Section 5) is a set of algorithms that use diagnostic

graphs to perform fault detection and identiﬁcation. For the deterministic case,

we provide optimization-based methods that ﬁnd the smallest set of faults that

explain the test results. For the probabilistic case, we transform a diagnostic

graph into a factor graph and perform inference to ﬁnd the set of faulty modules.

Finally, we propose a learning-based approach based on graph neural networks

that learns to predict failures in a diagnostic graph.

Our third contribution (Section 6) is to investigate fundamental limits and

provide deterministic and probabilistic guarantees on the fault detection and

1Modern perception systems rely on data from multiple sensors and are implemented in

multi-threaded architectures, where each algorithm may be executed at a diﬀerent rate.

4

identiﬁcation results. In the deterministic case, we draw connections between

perception system monitoring and the literature on diagnosability in multipro-

cessor systems, and in particular the PMC model [14]. This allows us to estab-

lish formal guarantees on the maximum number of faults that can be uniquely

identiﬁed in a given perception system, leading to the notion of diagnosability.2

In the probabilistic case, we develop Probably Approximate Correctly (PAC)

bounds on the expected number of mistakes our runtime monitors will make.

Finally, we show that our framework is eﬀective in detecting and identifying

faults in a real-world perception pipeline for obstacle detection (Section 7).

In particular, we perform experiments using a realistic open-source autonomous

driving simulator (the LGSVL Simulator [15]) and a state-of-the-art autonomous

driving software stack (Baidu’s Apollo Auto [16]). Our experiments show that

(i) some of our algorithms outperform common baselines in terms of accuracy,

(ii) they allow detecting failures and provide enough notice to stop the vehicle

before an accident occurs in realistic scenarios, and (iii) their runtime is typically

below ﬁve milliseconds, incurring a negligible overhead in practice. A video

showcasing the execution of the proposed runtime monitors can be found at

https://www.mit.edu/~antonap/videos/AIJ22PerceptionMonitoring.mp4.
We have also released an open-source version of our code at https://github.
com/MIT-SPARK/PerceptionMonitoring.

2. Related Work

This section reviews related work on runtime monitoring and AV safety

assurance, spanning both industrial practice (Section 2.1) and academic research

(Section 2.2).

2As discussed in Section 6, diagnosability is related to the level of redundancy within the

system and provides a quantitative measure of robustness.

5

2.1. State of Practice

The automotive industry currently uses four classes of methods to claim the

safety of an AV [17], namely: miles driven, simulation, scenario-based testing,

and disengagement. Each of these methods has well-known limitations. The

miles driven approach is based on the statistical argument that if the probability

of crashes per mile is lower in autonomous vehicles than for humans, then AVs

are safer; however, such an analysis would require an impractical amount (i.e.,

billions) of miles to produce statistically-signiﬁcant results [18, 17].3 The same

approach can be made more scalable through simulation, but unfortunately

creating a life-like simulator is an open problem, for some aspects even more

challenging than self-driving itself. Scenario-based testing is based on the idea

that if we can enumerate all the possible driving scenarios that could occur,

then we can simply expose the AV (via simulation, closed-track testing, or on-

road testing) to all these scenarios and, as a result, be conﬁdent that the AV will

only make sound decisions. However, enumerating all possible corner cases (and

perceptual conditions) is a daunting task. Finally, disengagement is deﬁned as

the moment when a human safety driver has to intervene in order to prevent a

hazardous situation. However, while less frequent disengagements indicate an

improvement of the AV behavior, they do not give evidence of the system safety.

An established methodology to ensure safety is to develop a standard that

every manufacturer has to comply with. In the automotive industry, the stan-

dard ISO 26262 [19] is a risk-based safety standard that applies to electronic

systems in production vehicles. A key issue is that ISO 26262 mostly focuses on

electronic systems rather than algorithmic aspects, hence it does not readily ap-

ply to fully autonomous vehicles [20]. The recent ISO 21448 [9], which extends

the scope of ISO 26262 to cover autonomous vehicles functionality, primarily

considers mitigating risks due to unexpected operating conditions, and provides

3Moreover, the analysis should cover all representative driving conditions (e.g., driving

on a highway is easier than driving in urban environment) and should be repeated at every

software update, quickly becoming impractical.

6

high-level considerations on best-practice for the development life-cycle. Both

ISO 26262 and ISO/PAS 21448 are designed for self-driving vehicles supervised

by a human [21]. Koopman and Wagner [22] propose a standard called UL

4600 [23] speciﬁcally designed for high-level autonomy (levels 4 and 5). This

standard focuses on ensuring that a comprehensive safety case is created, but

it is technology-agnostic, meaning that it requires evidence of system safety

without prescribing the use of any speciﬁc approach or technology to achieve it.

2.2. State of the Art

Related work tries to tackle the problem of safety assurance using diﬀerent

strategies. Formal methods [24, 25, 26, 27, 28, 29, 30, 31, 32] have been re-

cently used as a tool to study safety of autonomous systems. These approaches

have been successful for decision systems, such as obstacle avoidance [33], road

rule compliance [34], high-level decision-making [35], and control [36, 37], where

the speciﬁcations are usually model-based and have well-deﬁned semantics [38].

However, they are challenging to apply to perception systems, due to the com-

plexity of modeling the physical environment [39], and the trade-oﬀ between

evidence for certiﬁcation and tractability of the model [40]. One common ap-

proach is ﬁnding an example where the system fails (i.e., falsiﬁcation). Current

approaches [41, 42, 43] consider high-level abstractions of perception [17, 44, 45]

or rely on simulation to assert the true state of the world [41, 42, 46]. Other

approaches focus on adversarial attacks for neural-network-based object detec-

tion [47, 48, 49]; these methods derive bounds on the magnitude of the perturba-

tion that induces incorrect detection result, and are typically used oﬀ-line [50].

Previous works on runtime fault detection and identiﬁcation focused

on components of the perception system [51]. Miller et al. [52] propose a frame-

work for quantifying false negatives in object detection. For semantic segmen-

tation, Besnier et al. [13] propose an out-of-distribution detection mechanism,

while Rahman et al. [53] propose a failure detection framework to identify pixel-

level misclassiﬁcations. Lambert and Hays [54] propose cross-modality fusion

algorithm to detect changes in high-deﬁnition map. Liu and Park [55] propose a

7

methodology to analyze the consistency between camera image data and LiDAR

data to detect perception errors. Sharma et al. [56] propose a framework for

equipping any trained deep network with a task-relevant epistemic uncertainty

estimate. Several GPS/RTK integrity monitors have been proposed [11, 12]

to detect localization errors. Another line of works leverages spatio-temporal

information to detect failures. You et al. [57] use spatio-temporal informa-

tion from motion prediction to verify 3D object detection results. Balakrish-

nan et al. [44, 58] propose the Timed Quality Temporal Logic (TQTL) to reason

about desiderable spatio-temporal properties of a perception algorithm.

Kang et al. [59] use model assertions, which similarly place a logical con-

straint on the output of a module to detect anomalies. Fault-tolerant architec-

tures [60] have been also proposed to detect and potentially recover from a faulty

state, but these eﬀorts mostly focus on implementing watchdogs and monitors

for speciﬁc modules, rather than providing tools for system-level analysis and

monitoring.

Fault identiﬁcation and anomaly detection have been extensively stud-

ied in other areas of engineering. Bayesian networks, Hidden Markov Mod-

els [61, 62], and deep learning [63] have been used to enable fault identiﬁcation,

but mainly in industrial systems instrumented to detect component failures.

Graph-neural networks have been used for anomaly detection (see [64] for a

comprehensive survey). In this context, “anomaly detection is the data mining

process that aims to identify the unusual patterns that deviate from the ma-

jorities in a dataset” [64].

In order to detect anomalies, objects (i.e., nodes,

edges, or sub-graphs) are usually represented by features that provide valuable

information for anomaly detection, and when a feature considerably diﬀers from

the others (or the training data), the object is classiﬁed as anomalous. De Kleer

and Williams [65] propose a methodology to detect failures by comparing ob-

servations with a predicted output. The dissimilarities are then used to search

for potential failures that explain the measurements. The work assumes the

availability of a model that predicts the behavior of the system, and —after

collecting intermediate results of each component— it searches for the smallest

8

set of failing components that explains the wrong measurements. Preparata,

Metze, and Chien [14] study the problem of fault diagnosis in multi-processor

systems, introducing the concept of diagnosability; their work is then extended

by subsequent works [66, 67, 68]. Sampath et al. [69] propose the concept of

diagnosability for discrete-event systems [70, 71]. The system is modeled as a

ﬁnite-state machine, and is said to be diagnosable if and only if a fault can be

detected after a ﬁnite number of events.

The present paper extends this literature in several directions. First, we

take a black-box approach and remain agnostic to the inner workings of the per-

ception system we aim to monitor (relaxing assumptions in related work [65]).

Second, we develop a fault identiﬁcation framework that reasons over the consis-

tency of heterogeneous and potentially asynchronous perception modules (going

beyond the homogeneous, synchronous, and deterministic framework in [14]).

Third, the framework is applicable to complex real-world perception systems

(not necessarily modeled as discrete-event systems [70, 71]). The present pa-

per also extends our previous work on perception-system monitoring [72], which

only focuses on the deterministic case and considers a simpliﬁed model.

3. Problem Statement: Fault Detection and Identiﬁcation

in Perception Systems

3.1. Perception System: Modules and Outputs

A perception system S comprises a ﬁnite set of interconnected modules M =
{m1, m2, . . . m|M|}; for instance, the perception system of a self-driving car
may include modules for lane detection, camera-based object detection, LiDAR-

based motion estimation, ego-vehicle localization, etc. Each module m ∈ M
produces an output. For instance, the lane detection module may produce an

estimate of the 3D location of the lane boundaries, while the pedestrian detection

module may produce an estimate of the pose and velocity of pedestrians in

the surroundings. Some of these outputs provide inputs for other perception

modules, while other are the outputs of the perception system and feed into

9

other systems (e.g., to planning and control). The set of modules’ outputs are

disjoint (i.e., each output is produced by a single module), and the set of all

outputs is denoted by O. We model the perception system as a graph of modules

and outputs.

Deﬁnition 1 (Perception System). A perception system S is a directed graph

S = (M ∪ O, E), where the set of nodes M ∪ O describes modules and outputs

in the system, while the set of edges E describes which module produces or con-

sumes a certain output. In particular, an edge (mi, oj) ∈ E with mi ∈ M and

oj ∈ O models the fact that module mi produces output oj. Similarly, and edge

(oj, mi) ∈ E with oj ∈ O and mi ∈ M models the fact that module mi uses

output oj.

We treat each module as a black-box and remain agnostic to the algorithms

they implement. This allows our framework to generalize to complex perception

systems, possibly including a combination of classical and data-driven methods.

While we will consider more complex examples of perception systems in the

experimental section, Fig. 1 shows a simple example of perception system to

ground the discussion. The system comprises three modules: a LiDAR-based

obstacle detector, a camera-based obstacle detector, and a sensor fusion module.

Both the LiDAR-based and the camera-based obstacle detectors generate a set

of obstacles detected in the environment, namely, the LiDAR obstacles and

camera obstacles. The sensor fusion algorithm combines the two sets of obstacles

to produce a new set of objects, called fused obstacles.

Remark 2 (Modules vs. Outputs). Our system model treats modules and

outputs as separate nodes. This is convenient for two reasons. First, fault

identiﬁcation at the modules and outputs may serve diﬀerent purposes: output

fault identiﬁcation is more useful at runtime to identify unreliable information

from the perception system and prevent accidents; module fault identiﬁcation

is typically more informative for designers and regulators. Second, in practi-

cal applications we can rarely measure if a module is failing (indeed developing

algorithms that can “self-diagnose” their failures is an active area of research,

10

Figure 1: Perception system including 3 modules (rectangles) and 3 outputs (circles). Modules

are connected by edges describing which module produces or consumes a given output. The

failure modes of each module (resp. output) are represented by red dots.

see work on certiﬁable algorithms [73]). On the other hand, we can directly

measure the outputs of the modules and develop diagnostic tests to check if an

output is plausible and consistent with other outputs in the system.

3.2. Fault Detection and Fault Identiﬁcation

Each module in S might fail at some point, jeopardizing the system per-
formance or even its safety. In particular, each module m ∈ M is assumed to
have a set of failure modes. While the list of failures can include any software

and hardware failures, in this paper we particularly focus on failures of the in-

tended functionality. For example, a neural-network-based camera-based object

detection module might experience the failure mode “out-of-distribution sam-

ple” when it processes an input image, which indicates that while the module’s

code executed successfully, the resulting detection is expected to be incorrect.

Similarly, each output o ∈ O has an associated set of failure modes. For
instance, the output of the camera-based object detector might experience a

“mis-detection” failure mode if it fails to detect an object, or a “mis-classiﬁcation”

failure mode if the object is detected but misclassiﬁed. A module’s failure mode

typically causes a failure in one of its outputs. Examples of failure modes are

given in Fig. 1. For each module and output, the ﬁgure lists a potential failure

mode: for instance, the LiDAR-based obstacle detection output may fail if it

misdetects an obstacle, while the sensor fusion module may fail it it incorrectly

11

LiDAR-basedObstacle DetectorLiDARObstaclesCameraObstaclesCamera-basedObstacle DetectorSensor FusionAlgorithmFusedObstaclesOut-of-distribution sampleOut-of-distribution sampleMisdetectionMisdetectionMisdetectionMisassociationassociates the input obstacles.

Deﬁnition 3 (Failure Modes). At each time instant, the i-th failure mode
fi ∈ {INACTIVE, ACTIVE} ∼= {0, 1} is either ACTIVE (also 1) if such failure
is occurring, or INACTIVE (also 0). A module or an output is failing if at

least one of its failure modes is ACTIVE. If we stack the status (ACTIVE/IN-

ACTIVE) of all failure modes into a single binary vector, the fault state vector

f ∈ {0, 1}Nf (where Nf is the number of failure modes), then f is all zeros if

there are no faults, or has entries equal to ones for the active failure modes.

The goal of this paper is then to address the following problems:

Fault Detection decide whether the system is working in nominal conditions

or whether a fault has occurred (i.e., infer if there is at least an active

failure mode in f );

Fault Identiﬁcation identify the speciﬁc failure mode the system is experi-

encing (i.e., infer which failure mode is active in f ).

Fault detection is the easiest between the two problems, as it only requires

specifying the presence of at least a fault, without specifying which modules or

outputs are incorrect. Mathematically, this reduces to identifying whether the

unknown vector f has at least an entry equal to 1. Fault identiﬁcation goes one
step further by explicitly indicating the set of active failure modes. Mathemat-

ically, this reduces to identifying exactly which entries of the unknown vector f
are equal to 1. Identifying which module is faulty is particularly important to
inform regulators (e.g., to trace the steps that that led to an accident caused by

an autonomous vehicle) and system designers (e.g., to highlight modules that

are likely to fail and require further development). Moreover, not all faults are

equally problematic: for instance, a failure in localizing a car in the opposite

lane of a divided highway is less consequential that failing to detect a pedestrian

in front of the car. Note that solving fault identiﬁcation implies a solution for

fault detection (i.e., whenever we declare one or more modules to be faulty, we

12

essentially also detected there is a failure), hence in the rest of this paper we

focus on the design of a monitoring system for fault identiﬁcation.

Remark 4 (Assumptions and Terms of Use). We assume that the potential

failure modes of the system are known to the system designer. In practice, these

can be discovered using some form of hazard analysis, such as Failure Modes

and Eﬀects Analysis (FMEA) [74] or Fault tree analysis (FTA) [75]. Moreover,

one can always add a generic “unknown failure mode” to the list of failure modes

of a module or output, hence this assumption is not restrictive. We also remark

that our monitoring system’s objective is to diagnose potential failures, while

it does not prescribe what are the actions that need to be taken in response to

each failure ( e.g., whether to stop the car, provide a warning to the passenger,

etc.), which is failure and system-dependent. Investigation how to respond to or

mitigate failures is left to future work.

4. Modeling Fault Identiﬁcation with Diagnostic Graphs

This section develops a framework to model fault identiﬁcation problems in

perception systems. In the previous section we have discussed how the goal is to

identify the set of active failure modes associated to modules and outputs in a

system. Here we introduce the concept of diagnostic graphs to study fault iden-

tiﬁcation: diagnostic graph will allow developing fault identiﬁcation algorithms

(Section 5) and understanding fundamental limits (Section 6).

The intuition is that in a perception system we can perform a number of

diagnostic tests that check the validity of the output of certain modules. For

instance, we can compare the outputs of diﬀerent modules to ensure they are

consistent (e.g., compare the obstacles detected by the LiDAR-based obsta-

cle detection against the camera-based obstacle detection), or inspect that the

output a certain module respects certain requirements (e.g., the vision-based

ego-motion module is tracking a suﬃcient number of features). Then, we can

model these checks as edges in a bipartite graph, the diagnostic graph, which

13

can be used for fault identiﬁcation. In the following, we formalize the notions

of diagnostic tests and diagnostic graphs.

4.1. Diagnostic Tests

In our fault identiﬁcation framework, the system is equipped with a set of

diagnostic tests that can (possibly unreliably) provide diagnostic information

about the state of a subset of failure modes. Each diagnostic test is a function

t : S → {PASS, FAIL}, where S ⊆ {1, . . . , Nf } is a subset of the failure modes
that the test is checking, called the scope of the test, and the test returns a
value z ∈ {PASS, FAIL} ∼= {0, 1}, called the outcome of the test. A diagnostic
test returns PASS (also denoted with 0) if there is no active failure mode in its
scope, FAIL (also denoted with 1) otherwise. In general, tests can be unreliable,
meaning that they can both fail to detect active failures or incorrectly detect

failures as active (i.e., false alarms).

While in the experimental section we will describe more complex tests (and

provide an open-source framework4 to easily code new tests), it is instructive

to consider a simple test between the outputs of the LiDAR-based obstacle de-

tection and the camera-based obstacle detection in Fig. 2. The test in Fig. 2

compares the two sets of objects detected by the two detectors; whenever an in-

consistency arises, the test returns FAIL. However, if both detectors are subject

to the same failure, e.g., they both misdetect an obstacle, the test might still

pass, thus exhibiting unreliable behavior. We remark that a single test does not

suﬃce for fault identiﬁcation: for instance, if the test in Fig. 2 fails, we can only

conclude that one of the two detectors had a failure (or that the test was a false

alarm); therefore, we typically need to collect a number of tests and perform

some inference process to draw conclusions about which modules failed. The

collection of the outcomes of multiple diagnostic tests is called a syndrome.

Deﬁnition 5 (Syndrome). Assuming we have Nt diagnostic tests, the vector
collecting the test outcomes z ∈ {PASS, FAIL}Nt is called a syndrome.

4Code available at https://github.com/MIT-SPARK/PerceptionMonitoring.

14

In the following, we describe how to mathematically model the relation be-

tween the failure modes and the test outcomes; this will be instrumental in

solving the inverse problem of identifying the failure mode from a given syn-

drome. We provide a deterministic and a probabilistic model for the tests below.

Deterministic Tests. Deterministic diagnostic tests encode the set of

possible test outcomes, by establishing a deterministic relation between failure

modes in the test’s scope and the test outcome. We discuss potential models

for deterministic diagnostic test below.

Ideally we would like the test to return FAIL if and only if at least one

of the failure modes in its scope is active. This leads to the deﬁnition of a

“Deterministic OR” test.

Deﬁnition 6 (Deterministic OR). A diagnostic test t(fscope(t)) is a determin-

istic OR if its test outcome z is






z =

PASS if (cid:107)fscope(t)(cid:107)1= 0

FAIL otherwise

(1)

This kind of tests can be hard to implement in practice. For example,

imagine a diagnostic test that compares the output of two object classiﬁers:

if one of them produces a wrong label, it is easy to detect there is a failure;

however, if both classiﬁers are trained on similar data and both report the

incorrect label there is no way to detect the failure.

In this case, the test

outcome is unreliable. The following deﬁnition introduces a type of unreliable

test.

Deﬁnition 7 (Deterministic Weak-OR). A test t(fscope(t)) is a deterministic

Weak-OR if its test outcome z is






z =

PASS

if 0 < (cid:107)fscope(t)(cid:107)1< |scope(t)|

PASS or FAIL if (cid:107)fscope(t)(cid:107)1= |scope(t)|

(2)

FAIL

otherwise

This kind of test is consistent with the tests used in [76].

Intuitively, a

“Deterministic Weak-OR” may return PASS even if all failure modes are active,

15

since the test might fail to detect an inconsistency if all faults are consistent

with each others (again, think about two object classiﬁers failing in the same

way). Even though the Weak-OR test may pass or fail when all failure modes

are active, its outcome remains deterministic.

Finally, an even weaker type of deterministic test is what we call the Deter-

ministic Weaker-OR (this is the easiest test to implement in practice).

Deﬁnition 8 (Deterministic Weaker-OR). A diagnostic test t(fscope(t)) is a

Deterministic Weaker-OR if its test outcome z is




PASS or FAIL if (cid:107)fscope(t)(cid:107)1> 0



PASS

if (cid:107)fscope(t)(cid:107)1= 0

z =

(3)

In other words, the tested is designed to pass in nominal conditions (i.e.,

when no failure mode is active), but it can have arbitrary outcomes otherwise.

The types of deterministic tests presented above are not the only possible

deterministic tests. Other examples include, for instance, diagnostic tests that

fail to detect speciﬁc sets of failure modes. Deterministic tests can be designed

using formal methods tools or certiﬁable perception algorithms [77, 78, 79],5 see

also Remark 10 below.

Probabilistic Tests. Deterministic tests might not capture the complexity

of real world diagnostic tests. Most practical tests are likely to incorrectly detect

faults (i.e., produce false positive) or fail to detect faults (i.e., produce false

negatives) with some probability. For this reason, in this paper, we also allow

for an arbitrary probabilistic relationship between test outcomes and failure

modes in the test scope.

A simple-yet-expressive way to formalize a probabilistic test is to use what

we call a “Noisy-OR” model. In particular, the Noisy-OR model represents the

probability of a diagnostic test outcome as a conditional probability distribution

5Certiﬁable perception algorithms are a class of model-based perception algorithms that

provide a soundness certiﬁcate at runtime, allowing one to directly measure the presence (or

absence) of certain failure modes, see [79, 73].

16

Scope

Test outcome z

f1

f2

OR

0

0

1

1

0

1

0

1

0

1

1

1

Noisy-OR

0 with prob. (1 − pa,1)(1 − pa,2)

1 with prob. pa,1 + pa,2 − pa,1pa,2

0 with prob. (1 − pa,1)(1 − pd,2)

1 with prob. pa,1 + pd,2 − pa,1pd,2

0 with prob. (1 − pd,1)(1 − pa,2)

1 with prob. pd,1 + pa,2 − pd,1pa,2

0 with prob. (1 − pd,1)(1 − pd,2)

1 with prob. pd,1 + pd,2 − pd,1pd,2


















Figure 2: A test comparing

Table 1: Table of possible outcomes for the Deterministic OR and

two outputs, LiDAR Obsta-

the probabilistic Noisy-OR version of a test with scope f1 and f2.

cles and Camera Obstacles

over the failure modes in its scope Pr(z | fscope(t)) as deﬁned below.

Deﬁnition 9 (Noisy-OR). A diagnostic test t(fscope(t)) is a probabilistic Noisy-

OR if its test outcome z follows

Pr(z = PASS | fscope(t)) =

(cid:89)

i∈scope(t)

Pr(z = PASS | fi)

(4)

where Pr(z | fi) denotes the conditional probability of the test outcome (PASS/-

FAIL) conditioned on the status (ACTIVE/INACTIVE) of the failure mode fi.

Clearly, Pr(z = FAIL | fscope(t)) = 1 − Pr(z = PASS | fscope(t)).

Now suppose each test has a probability pd,i of correctly identifying failure
fi (detection probability), and a probability pa,i of false alarm for fi. Exploiting
the fact that fi ∈ {0, 1}, we can write Eq. (4) as:

Pr(z = PASS | fscope(t)) =

(cid:89)

i∈scope(t)

(1 − pd,i)fi(1 − pa,i)1−fi

(5)

An example of probabilistic test outcome is given in Table 1.

Similarly to the deterministic case, the Noisy-OR model is not the only pos-

sible model. However, Section 7 shows that this model is particularly eﬀective

in modeling fault identiﬁcation problems in practice. In Section 5, we discuss

how to learn the probabilities involved in probabilistic tests (i.e., pd,i and pa,i

17

TestLiDARObstaclesCameraObstaclesMisdetectionMisdetectionin Eq. (5)) given a training dataset, and how to use the test outcomes to infer

the most likely failure modes. Towards that goal, we need to group diagnostic

tests into a suitable graph structure, called a diagnostic graph, which we present

in the following section. We conclude this section with a remark.

Remark 10 (From diagnostic tests to fault identiﬁcation). The diagnostic tests

we introduced in this section are not dissimilar from the typical diagnostic tests

or watchdogs considered in prior work or used by practitioners. Our goal here

is to formalize these tests and use the test outcomes to infer the most likely

set of system-wide failures. In this sense, our fault identiﬁcation framework is

designed to capitalize on (rather than replace) existing diagnostic tools used in

practice. For example the detection mechanism proposed by Liu and Park [55],

which is based on the idea of projecting the 3D LiDAR points onto camera

images, and then checking whether objects detected from LiDAR and images

match each other, can be formulated as a diagnostic tests with the camera and

LiDAR misdetection in its scope, such that the test outcome is the output of

the algorithm in [55]. Also, out-of-distribution detection based on epistemic

uncertainty, e.g., [56], can be formulated as a diagnostic tests with the module’s

“out-of-distribution sample” failure mode in its scope, such that the test outcome

is FAIL if the estimated uncertainty is above a threshold.

4.2. Diagnostic Graph

A diagnostic graph is a structure deﬁned over a perception system and has

the goal of describing the diagnostic tests (as well as more general relations

among failure modes) and their scope. We provide a formal deﬁnition below.

Deﬁnition 11 (Diagnostic Graph). A diagnostic graph is a bipartite graph
D = (V, R, E) where the nodes are split into variable nodes V, corresponding
to the failure modes in the system, and relation nodes R, where each relation

φk(f ) ∈ R is a function over a subset of failure modes f . Then an edge in E

exists between a failure mode fi ∈ V and a relation φk ∈ R, if fi is in the scope
of the relation φk ( i.e., if the variable fi appears in the function φk).

18

Relations capture constraints among the variables induced by the test out-

comes or from prior knowledge we might have about the failure modes. We

describe the two main types of relations below and for each we describe their

implementation in the deterministic and probabilistic case.

Deﬁnition 12 (Test-driven Relations). A test-driven relation φk describes

whether —for a test tk— a given set of failure mode assignments might have

produced a certain test outcome zk. More formally, for a deterministic test tk,

a test-driven relation is a boolean function:

φk(f ) = φ(fscope(tk); zk) = 1 (cid:2)zk = t (cid:0)fscope(tk)

(cid:1)(cid:3)

(6)

where 1 is the indicator function that returns 1 if the condition is satisﬁed or
0 otherwise. The function Eq. (6) checks if an assignment of failure modes

f may have produced the test outcome zk and where the notation φk(f ) =

φ(fscope(tk); zk) clariﬁes that the function φk only involves a subset of failure

modes fscope(tk) (the ones in the scope of test tk) and depends on the (given)

test outcome zk. Similarly, for a probabilistic test tk, a test-driven relation is a

real-valued function:

φk(f ) = φ(fscope(tk); zk) = Pr(zk|fscope(tk))

(7)

which returns the likelihood of the test outcome zk given an assignment f .

Deﬁnition 13 (A Priori Relations). An a priori relation describes whether

a given set of failure modes is plausible, considering a priori knowledge about

the system. More formally, in the deterministic case, an a priori relation is

a boolean function φk(f ) that returns 1 if the assignment of f is plausible or

0 otherwise. Similarly, in the probabilistic case, an a priori relation is a real-

valued function φk(f ) that returns the likelihood of a given assignment f .

In the following we will denote the set of Test-driven Relations as Rtest while

the set of A Priori Relations as Rprior. Therefore, R = Rtest ∪ Rprior.

While we have provided several examples of diagnostic tests in the previous

section, we now provide examples of a priori relations. For instance, in the

19

deterministic case, some failure modes of a module can be mutually exclusive

(e.g., “too many outliers”, “not enough features” in the Lidar-based ego-motion

estimation) or one can imply another (e.g., if a module is experiencing an “out-of-

distribution sample” failure mode, then its outputs will have at least an active

failure mode). Not all relations are deterministic, for example in Fig. 3, the

failure modes of the sensor fusion algorithm may have a complex probabilistic

relationship with the failure modes of the lidar and camera obstacles failure

modes. Note that the main diﬀerence between test-driven relations and a priori

relations is that the former provides a measurable test outcome, while the latter

relies on a priori knowledge about the system (i.e., no outcome is measured).

We elucidate on the notion of diagnostic graph with two examples below.

Example 1: Multi-sensor Obstacle Detection. Consider the perception

system in Fig. 1. We can associate a diagnostic graph to the system where the

variable nodes of the diagnostic graph are the failure modes of modules and

outputs in the system. The diagnostic graph, shown in Fig. 3, also includes two

diagnostic tests and a priori relations encoding input/output relationship be-

tween modules and outputs. Each diagnostic test compares a pair of outputted

obstacles, namely LiDAR obstacles and camera obstacles (with failures f4 and
f5), and camera obstacles and fused obstacles (with failures f4 and f6).

Figure 3: A diagnostic graph for the perception system example in Fig. 1. Red circles rep-

resent variable nodes (failure modes) while squares represent relations. Test-driven Relations

are shown in blue, while a priori relations are shown in black.

Example 2: LiDAR-based Ego-motion Estimation. We provide a

second example that also includes singleton diagnostic tests (having a single

failure mode in their scope) and includes explicit tests over modules. The ex-

ample consists of a LiDAR-based odometry system that computes the rela-

20

tive motion between consecutive LiDAR scans using feature-based registration,

see e.g., [80, 81]. The system S comprises two modules, a feature extraction

module and a point-cloud registration module, as depicted in Fig. 4(left). The

feature extraction module extracts 3D point features from input LiDAR data,

while the point-cloud registration module uses the features to estimate the rel-

ative pose between two consecutive LiDAR scans. Suppose that the feature

extraction module is based on a deep neural network and that it can experience

an “out-of-distribution sample” failure, which causes the corresponding output

to potentially experience “too-many outliers” or “few features” failures. Simi-

larly, the module point-cloud registration can experience the failure “suboptimal

solution”, which leads its outputs, the relative pose, to experience a “wrong rel-

ative pose” failure. Fig. 4(right) shows a diagnostic graph for the system. The

system is equipped with three diagnostic tests. A diagnostic test detects if the

failure mode “few features” is active by checking the cardinality of the feature

set.

If the point-cloud registration module is a certiﬁable algorithm [73], we

can attach a diagnostic test to the point-cloud registration module that uses

the module’s certiﬁcate to detect if the module is experiencing a “suboptimal

solution” failure. Another diagnostic test detects if the relative pose is wrong by

checking that the relative pose does not exceed some meaningful threshold given

the vehicle dynamics. Finally, another test checks if under the computed rela-

tive pose, the feature extractor has “too many outliers”. This can be achieved

by counting the number of features that are correctly aligned after applying the

estimated relative pose. The diagnostic graph also contains a priori relations

encoding constraints on the input/output relationships.

4.2.1. Temporal Diagnostic Graph

So far, we have considered a diagnostic graph as a representation of the di-

agnostic information available at a speciﬁc instant of time (e.g., the examples

above include tests and relations involving the behavior of modules and out-

puts at a certain time instant). However, perception systems evolve over time,

and considering the temporal dimension oﬀers further opportunities for fault

21

Figure 4:

(Left) Example of the LiDAR-based ego-motion estimation system S. The system

is composed by two modules (rectangles), each producing one output (circles). (Right) The

corresponding diagnostic graph, where red circles represent variable nodes (failure modes)

while squares represent relations (test-driven Relations in blue, a priori relations in black).

identiﬁcation, e.g., by monitoring the temporal evolution of the outputs.

Suppose we have a collection of diagnostic graphs T = {D(t), . . . , D(t+K)},
collected over and interval of time. We could think of stacking these diagnos-

tic graphs, into a new temporal diagnostic graph D[K]. The temporal graph

preserves the failure mode, relations and edges of each sub-graph D(k) ∈ T .

However, since D[K] includes outputs produced at multiple time instants, we

can also augment the graph to include temporal diagnostic tests and temporal

relationships. For example, we might check that an obstacle does not disappear

from the scene (unless it goes out of the sensor ﬁeld of view), or that the pose

of the ego-vehicle does not change too much over time. As we will see, the

use of temporal diagnostic graph leads to slightly improved fault identiﬁcation

performance. An example of temporal diagnostic graph is given in Fig. 5.

The algorithms and results presented in the rest of this paper apply to both

regular and temporal diagnostic graph, unless speciﬁed otherwise.

Remark 14 (Temporal Diagnostic Tests). Temporal diagnostic tests are used to

monitor the evolution of the system over time. For example the Timed Quality

Temporal Logic in [45] can be implemented with a temporal diagnostic test that

spans multiple D(t)’s. More speciﬁcally, the test example considered in [45]

requires that “At every time step, for all the objects in the frame, if the object

22

Too-many OutliersNot enough featuresWrong Relative PoseSuboptimal SolutionOut-of-distribution samplePoint-CloudRegistrationRelativePoseFeaturesFeatureExtractorFigure 5: Example of Temporal Diagnostic Graph composed by two identical sub-graphs.

We added temporal relations (both test-driven and a priori) between the two sub-graphs.

class is cyclist with probability more than 0.7, then in the next 5 frames the

same object should still be classiﬁed as a cyclist with probability more than 0.6”.

This can be modeled as a diagnostic test that spans 5 diagnostic graphs and that

returns FAIL if the predicate is false.

5. Algorithms for Fault Identiﬁcation

This section shows how to perform fault identiﬁcation over a diagnostic

graph.

In particular, we present algorithms to infer which failure modes are

active, given a syndrome. We study fault identiﬁcation with deterministic tests

in Section 5.1 and then extend it to the probabilistic case in Section 5.2. Fi-

nally, we present a graph-neural-network approach for fault identiﬁcation in

Section 5.3.

5.1. Inference in the Deterministic Model

In the deterministic case, our inference algorithm looks for the smallest set

of active failure modes that explains a given syndrome. In Section 6, we will

show that such approach is guaranteed to correctly identify the faults as long

23

as the tests provide a suﬃcient level of redundancy, an insight we will formalize

through the notion of “diagnosability”.

Looking for the smallest set of active failures that explains the test outcomes

(and more generally, the relations) in a diagnostic graph can be formulated as

the following optimization problem (given a syndrome z):

minimize
f ∈{0,1}Nf

(cid:107)f (cid:107)1

subject to φk(fscope(tk); zk) = 1,

i = 1, . . . , Nt,

(D-FI)

φj(f ) = 1,

j = 1, . . . , Nr,

where φk(fscope(tk); zk) are the Nt test-driven relations in the diagnostic graph,
while φj(f ) are the Nr a priori relations in the graph. In words, Eq. (D-FI)
looks for binary decisions (ACTIVE/INACTIVE) for the failure modes f , and
looks for the smallest set of faults (the objective (cid:107)f (cid:107)1 counts the number of AC-
TIVE failure modes) such that the faults satisfy the relations in the diagnostic

graph. Eq. (D-FI) is our Deterministic Fault Identiﬁcation algorithm.

The optimization in Eq. (D-FI) can be solved using standard computa-

tional tools from Integer Programming [82] or Constraint Satisfaction Program-

ming [83]. While integer programming is better suited to ﬁnd the solution to

the minimization problem, constraint programming also allows ﬁnding all the

solutions in the feasible set. The choice between the two depends on the ap-

plication and the expression for the relations. In our experiments, we solve it

using Integer Programming. We remark that while Integer Programming is NP

complete, our problems typically only involve tens to hundreds of failure modes,

and can be solved eﬃciently in practice.

The model presented above is generic and valid for any deterministic test

and a priori relations. In the following, we provide an example to ground the

discussion and show how to instantiate the optimization problem in practice.

Example 3: Deterministic Inference with Weaker-OR and Module-

Output Relations. We consider a diagnostic graph with Deterministic Weaker-

OR tests. Moreover, as a priori relations, we assume that whenever the output

24

of a module has a failure, then also the module itself must have at least an active

failure mode. This is also the setup we use in our experiments in Section 7.

In Weaker-OR diagnostic tests, the PASS outcome is unreliable, meaning

that if a test returns PASS it might have 0 or more failure modes active in its
scope. However, when it the test returns FAIL, we know there must be at least

one failure mode active. This can be easily enforced in the optimization by

imposing the constraint:

(cid:107)fscope(ti)(cid:107)1≥ 1

∀ti ∈ {1, . . . , Nt} such that zi = FAIL,

We then have to enforce the relation that if an output has an active failure

mode, then the module that produced it must have at least one active failure

mode as well. Towards this goal, let F(oi) ⊆ {1, . . . , Nf } be the set of failure
modes associated to outputs of module mi and F(mi) be the set of failure modes
associated to mi; then the a priori relation can be enforced via the constraint:

(cid:107)fF (mi)(cid:107)1 ≥

1
|F(oi)|

(cid:107)fF (oi)(cid:107)1

Intuitively, when there is no active failure in the outputs (i.e., (cid:107)fF (oi)(cid:107)1= 0)
the constraint is trivially satisﬁed, while when there is at least an output failure

(i.e., (cid:107)fF (oi)(cid:107)1> 0) then (cid:107)fF (mj )(cid:107)1 is forced to be at least 1. The resulting
optimization problem ﬁnally becomes:

minimize
f ∈{0,1}Nf

(cid:107)f (cid:107)1

subject to (cid:107)fscope(ti)(cid:107)1≥ 1

∀ti ∈ {1, . . . , Nt} such that zi = FAIL,

(8)

(cid:107)fF (mi)(cid:107)1≥

1
|F(oi)|

(cid:107)fF (oi)(cid:107)1

∀mi ∈ M.

5.2. Inference in the Probabilistic Model

This section shows how to use the formalism of factor graphs to ﬁnd the

most likely active failure modes that explain a given syndrome in a diagnostic

graph with probabilistic tests.

Factor graphs are a powerful class of probabilistic graphical models. Prob-

abilistic graphical models allow describing relationships between multiple vari-

ables using a concise language. In particular, they describe joint or conditional

25

distributions over a set of unknown variables and a set of known observations,

and can be used to infer the values of the unknown variables. In this work we

limit ourselves to factor graphs over discrete (binary) variables. We start from

the deﬁnition of a factor graph.

Deﬁnition 15 (Factor Graph). A factor graph is a bipartite graph F = (V, Φ, E)

consisting of a set V of variable nodes, a set Φ of factor nodes, and a set E ⊆

V × Φ of edges having one endpoint at a variable node and the other at a factor

node. Let N (φ) the set of variables to which a factor node φ is connected, then,

the factor graph deﬁnes a family of distributions that factorize according to

µ(f | z) =

1
Z

(cid:89)

φ∈Φ

φ(fN (φ); z)

(9)

where the normalization factor Z, also known as the partition function, ensures
that µ(f ) is a valid distribution:6

Z(z) =

(cid:88)

(cid:89)

f

φ∈Φ

φ(fN (φ); z)

(10)

The notation φ(fN (φ); z) emphasizes the fact that each factor is a function of a

subset fN (φ) of the failure modes f , for given observed z.

The factor graph F and the diagnostic graph D have a similar structure. In
fact we can choose the set of variables V in the factor graph to be the same

as the set of variables in the diagnostic graph, namely the set of failure modes.

Then, we can choose the set of factors Φ to be the relations R of D, and the set
of edges to be the same. Therefore, for a given diagnostic graph D, it is easy to

devise the corresponding factor graph as:

µ(f | z) =

1
Z

(cid:89)

φk∈Rtest

φk(fscope(tk); zk)

(cid:89)

φj ∈Rprior

φj(fN (φj ))

(11)

where we have simply observed that the probability distributions induced by

the relations in the diagnostic graph naturally factorize into factors, each one

corresponding to a (test-driven or a priori) relation in the diagnostic graph.

6The notation (cid:80)

f means “sum over all possible values of f .”

26

Maximum a Posteriori Inference. Given a factor graph, a natural ques-

tion to ask is what is the most likely assignment of variables that maximizes the

probability distribution induced by the factor graph (e.g., in our case, this is the

most likely set of faults in the system). This leads to the concept of maximum a

posteriori (MAP) inference, which —given a factor graph and a syndrome z—

looks for the most likely variables f (cid:63), that maximize the posterior distribution:

f (cid:63) = arg max
f ∈{0,1}Nf

µ(f | z)

(FG-FI)

Computing a MAP estimate is known to be NP-hard for general factor graphs [84],

therefore it is common to use approximate methods. In our experiments we used

belief propagation(Sec. 3 in [85]) to solve the MAP inference, which ﬁnds the

optimal solution for tree-structured factor graphs, and is known to empirically

return good approximations for the MAP estimate in general factor graphs.

Learning the Factor Graph Parameters. While in the deterministic
case we know the expression of the relations φk, in the probabilistic case the
probabilistic tests might depend on unknown parameters, cf. the expression in
Eq. (5) that requires specifying the parameters pd,i (probability that a fault is
not detected) and pa,i (probability of a false alarm). There are several paradigms
to learn the factor graph parameters. In our experiments we use a method called

structured support vector machine (SSVM) or maximum margin learning (Sec.

19.7 in [86]).

5.3. Graph Neural Networks for Fault Identiﬁcation

The factor graph framework introduced in the previous section learns the

factor graph parameters from training data, and then performs maximum a

posteriori inference at runtime for fault identiﬁcation. In this section, we propose

a learning-based framework that is also trained on a dataset, but then learns

directly how to predict which failure mode is active at runtime. In particular,

we use Graph Neural Networks (GNN) to learn to identify active faults in a

diagnostic graph.

27

GNNs provide a general framework for learning using graph-structured data,

and have empirically achieved state-of-the-art performance in many tasks such

as node classiﬁcation, link prediction, and graph classiﬁcation [87]. The fault

identiﬁcation problem considered in this paper can be phrased as a node clas-
siﬁcation problem. In node classiﬁcation, given a undirected graph G = (V, E)
where each node i ∈ V has an (unknown) label yi, the objective is to learn
a representation vector ei of node i such that label yi can be predicted as a
function of the node embeddings ei.

In the following, we recall common GNN architectures (Section 5.3.1) and

then we discuss how to transform our diagnostic graph into a structure that can

be fed to a GNN to predict active faults (Section 5.3.2).

5.3.1. Graph Neural Network Preliminaries

A GNN is an extension of recurrent neural networks that operates on graph-

structured data. GNNs are based on the concept of neural message passing in

which real-valued vector messages are exchanged between nodes of a graph —

not dissimilarly to the belief propagation we used in Section 5.2— but were the

messages (and node updates) are built using diﬀerentiable functions encoded as

neural networks. To understand the basic idea of neural message passing con-

sider an undirected graph G = (V, E). At the beginning, each node is assigned a
feature vector e(0)
for each i ∈ V. Then, during each message-passing iteration
i
k = 1, 2, . . ., the embedding e(k)
is updated by aggregating the embeddings of
node i’s neighborhood N (i)

i

e(k+1)
i

= update

= update

(cid:16)

(cid:16)

e(k)
i

, aggregate

(cid:16)(cid:110)

e(k)
j

| j ∈ N (i)

(cid:111)(cid:17)(cid:17)

(cid:17)

e(k)
i

, a(k)
i

(12)

(13)

Where aggregate(·) and update(·) are two learned diﬀerentiable functions (i.e.,
neural networks). At each iteration k, the aggregate(·) function takes the embed-
dings of node i’s neighbors and generates a message a(k)
. Then, the update(·)
function combines the message with the previous embedding of node i, generat-
ing the new embedding of node i. The ﬁnal embedding is obtained by running

i

28

the neural message passing for K iterations. Finally, the node label is predicted
by a learned diﬀerentiable function of the node embeddings:

yi = READOUT(e(K)

i

)

(GNN-FI)

The literature on GNN oﬀers a number of potential choices for the update(·)

and aggregate(·) functions. We review four popular choices below.

Graph Convolutional Networks (GCNs). One of the most popular

graph neural network architectures is the graph convolutional network (GCN) [88].

The GCN model implements the update and aggregate function as:

e(k+1)
i

= σ


W (k+1) (cid:88)

j∈N (i)∪{i}

e(k)
j
(cid:112)|N (i)||N (j)|





(14)

where W (k+1) is a trainable weight matrix and σ(·) is a nonlinear activation
function. Note that Eq. (14) can also be written in a matrix form

E(k+1) = σ

(cid:16)

ˆP E(k)W (k+1)(cid:17)

where ˆP = ˆD− 1
original graph, and ˆD is its diagonal degree matrix.

2 (A + I) ˆD− 1

2 , the matrix A is the adjacency matrix of the

Graph Convolutional Network via Initial residual and Identity

mapping (GCNII). The GCN is aﬀected by the over-smoothing problem [89],

where after several iterations of GNN message passing, the nodes’ embeddings

become very similar to each another; over-smoothing prevents the use of deeper

GNN models, which in turn prevents the GNN from leveraging longer-term de-

pendencies in the graph. To solve this problem, Chen et al. [90] propose the

GCNII, where the update of the embedding vectors becomes:

E(k+1) = σ

(cid:16)(cid:16)

(1 − αk) ˆP E(k) + αkE(0)(cid:17) (cid:16)

(1 − βk)I + βkW (k)(cid:17)(cid:17)

(15)

and where αk and βk are two hyper-parameters. GCNII improves on the basic
GCN by adding a smoothed representation ˆP E(k) with an initial residual con-
nection to the ﬁrst layer E(0), and adds an identity mapping to the k-th weight
matrix W (k).

29

Graph Sample and Aggregate (GraphSAGE). GraphSAGE is another

approach for node classiﬁcation [91]. The aggregate function takes the form

a(k+1)
i

= σ

(cid:16)

W · g

(cid:16)

{e(k)
j

: j ∈ N (i) ∪ {i}}

(cid:17)(cid:17)

(16)

where g(·) is an aggregator function like the element-wise mean or max pool-
ing. Then, the update function is a function over the concatenation of the old

embedding and the message a(k)

i

:

e(k+1)
i

= σ

(cid:16)

W [e(k)
i

, a(k+1)
i

(cid:17)
]

(17)

Graph Isomorphism Network (GIN). The Graph Isomorphism Network

(GIN) [92] is deﬁned by the following aggregation function

a(k+1)
i

= (1 + (cid:15)(k+1))e(k)

i +

(cid:88)

e(k)
j

j∈N (i)

(18)

where (cid:15)(k) is a trainable (or ﬁxed) parameter. The update function in GIN is

e(k+1)
i

= ζ (k+1)(a(k+1)

i

)

(19)

where ζ(·) is also a neural network.

5.3.2. From Diagnostic Graphs to Graph Neural Networks

In order to apply GNNs to our diagnostic graph D, we need to convert
D = (VD, RD, ED) into an undirected graph G = (VG, EG). Towards this goal,
we take the set of nodes VG to be both the set of failure modes and diagnostic test
outcomes. Note that we add the diagnostic test outcomes as nodes in the graph

since this allows attaching the test outcomes as features to these nodes. For each

test tk we form a clique7 involving the set of nodes in the test’s scope and the
variable corresponding to the test zk, namely the set scope(tk) ∪ {zk}. We then
form another clique for each a priori relation φj ∈ Rprior using the set of failure
modes N (φk) connected to φj. For example if we have a factor φ(f1, f2; z2)

7A clique is a subset of vertices of an undirected graph such that every two distinct vertices

in the clique share an edge.

30

we add the following (undirected) edges to EG: (f1, f2), (f1, z2), (f2, z2). We
attach a feature vector to each node in the graph. For the test nodes, we use a

one-hot encoding describing the test outcome as node feature. For the module

nodes, we use the failure probability (computed from the training data) as node

features. We provide more details on the node features in Section 7.

Figure 6: Example of conversion of the diagnostic graph in Fig. 3 into an undirected graph.

Learning to Identify Active Faults. In order to train the GNN to iden-

tify active faults, we use a supervised learning approach. In particular, we use

a softmax classiﬁcation function and negative log-likelihood training loss, which

is available in standard libraries, such as PyTorch [93].

6. Fundamental Limits

Given a diagnostic graph it is natural to ask if there is a maximum number

of failure modes that can be correctly identiﬁed as active. In order words, for a

given system, can we guarantee that our algorithms are able to correctly identify

all faults? Under which conditions? We answer these questions in this section,

where we introduce the concept of diagnosability. We discuss the deterministic

case (i.e., where the tests are assumed to be unreliable deterministic tests)

in Section 6.1. Then we obtain more general guarantees for the probabilistic

case (which also apply to our learning-based algorithms) in Section 6.2.

6.1. Deterministic Diagnosability

In this section, we assume diagnostic graphs with deterministic relations

and present theoretical results on the maximum number of faults that can be

correctly identiﬁed. Towards this goal, we borrow and extend results from fault

31

identiﬁcation in multi-processor systems [14], which were partially presented in

our previous work [76]. In particular, Lemma 17 and Theorem 18 below are a

direct application of results in [14], while the others are our extensions.

We start with the deﬁnition of deterministic diagnosability.

Deﬁnition 16 (κ-diagnosability [14, 76]). A diagnostic graph D is κ-diagnosable

if, given any syndrome, all active failure modes can be correctly identiﬁed, pro-

vided that the number of active failure modes in the system does not exceed κ.

The idea behind κ-diagnosability is that the number of failures that can be
correctly identiﬁed is an intrinsic property of a system and its diagnostic graph,

and somehow it measures if the system has enough redundancy to unambigu-

ously identify the cause of certain failures.

Example 4: Multi-sensor Obstacle Detection (Fig. 1 and Fig. 3).

Consider the example in Fig. 1 and assume that an output fails if and only if the

module producing it fails. Also assume that the sensor fusion algorithm does

not necessarily fail if its inputs are wrong (thus removing φ5(f3, f4, f5), or set-
ting it to be always TRUE). If both diagnostic tests behave like Deterministic
ORs, and they both return FAIL, we would not know if the state of the fail-

ure mode (f1, f2, f3, f4, f5, f6) was (0, 1, 0, 0, 1, 0), (0, 1, 1, 0, 1, 1), (1, 0, 1, 1, 0, 1),
(1, 1, 0, 1, 1, 0) or (1, 1, 1, 1, 1, 1). In fact, all these failures would generate the
same syndrome (FAIL,FAIL). However, if we impose that the maximum number

of active failure mode is 2 (i.e., κ = 2), the number of feasible candidates drops
In other words, if we have at most two
to only one, namely (0, 1, 0, 0, 1, 0).
failures in the system, the two tests would allow us to uniquely identify which

failure mode is active without any doubt.

After deﬁning the notion of diagnosability in Deﬁnition 16, we are left with

the question: can we develop an algorithm to compute the diagnosability of a

certain diagnostic graph? It has been noted in [94] that a system is κ-diagnosable
if the set of possible syndromes uniquely encodes the set of active failure modes.

Such observation is formalized by the following lemma.

32

Lemma 17 (Diagnosability and Syndromes). Let syndrome(A) be the set of all

possible syndromes produced by a set of active failure modes A ⊆ {1, . . . , Nf }.

A diagnostic graph D is κ-diagnosable if and only if, given any A1, A2 ⊆

{1, . . . , Nf }, such that |A1|, |A2|≤ κ (with A1 (cid:54)= A2), we have syndrome(A1) ∩

syndrome(A2) = ∅.

Proof. We prove “κ-diagnosability ⇒ syndrome(A1) ∩ syndrome(A2) = ∅” and
its reverse implication below. In both, we deﬁne X = {A ⊆ {1, . . . , Nf } | |A|≤
κ} to be the set of subsets of {1, . . . , Nf } of cardinality no larger than κ.

⇒ Suppose D is κ-diagnosable. Suppose by contradiction that there exists a
syndrome z such that z ∈ syndrome(A1) ∩ syndrome(A2), with A1, A2 ∈ X
and A1 (cid:54)= A2. Since z ∈ syndrome(A1) and z ∈ syndrome(A2), we are unable
to say if the syndrome z is produced by the set of active failure modes is A1 or
A2, contradicting the deﬁnition of κ-diagnosability of D.

⇐ Call Y = (cid:83)
A∈X syndrome(A) the set of all possible syndromes assuming
there are less than κ active failure modes. From the assumptions we know that
any two A1, A2 ∈ X have syndrome(A1)∩syndrome(A2) = ∅, which means that
we can uniquely map a syndrome to any set A. This is exactly the deﬁnition of
κ-diagnosability.

The lemma intuitively establishes that for a κ-diagnosable system, two dif-
ferent sets of κ faults must produce diﬀerent syndromes, such that for any given
syndrome, there is no ambiguity on which set of active failure modes generated

it, and we can perform fault identiﬁcation without any mistake.

Lemma 17 suggests an algorithmic way to check if a diagnostic graph is

κ-diagnosable, which however requires checking every subset of failure modes
of cardinality up to κ (and their syndromes). In the following, we reﬁne the
result, showing that, under technical assumptions, one can directly compute

the diagnosability by only looking at the topology of the diagnostic graph.

33

Theorem 18 (Characterization of κ-diagnosability [66]). Let

H(f )

.
= {t | t ∈ {1, . . . , Nt}, f ∈ scope(t)}

be the set of tests involving a failure mode f , and let

Γ(f )

.
=

(cid:91)

scope(t) \ {f }

t∈H(f )

be the set of failure modes that share a test with f . Also deﬁne Γ(X)
(cid:83)

f ∈X Γ(f ) \ X the extension of Γ to a set of failure modes. Now assume that all
tests follow the Deterministic Weak-OR model and have scope of cardinality 2.

.
=

Then D is κ-diagnosable if all the following conditions are satisﬁed:

i. κ ≤ (Nf − 1)/2

ii. κ ≤ mini∈{1,...,Nf }|H(fi)|

iii. for each q ∈ N with 0 ≤ q < κ, and each X ⊂ {1, . . . , Nf } with |X|=

Nf − 2κ + q we have |Γ(X)|> q

Proof. The assumption on the cardinality allows us to transform our general

diagnostic graph into an undirected graph akin to the one used in [66, 72]. Then,

the conditions (i), (ii) and (iii) are a straightforward application of Theorem 2

in [66] to the resulting graph.

Theorem 18 also shows that the diagnosability of a system depends on the

amount of redundancy in the systems and how well the tests are able to cap-

ture it. The connection is particularly visible in condition (iii):
of possible set X of active failure modes (of appropriate size), there must be
a suﬃcient number the tests, that —using information coming from diﬀerent

for each set

modules/outputs— give an opinion on the state of the failure modes in X.

Let us now move our attention to temporal diagnostic graphs. Denote with

κ(D) the maximum value of κ for the diagnostic graph D. Then the following
result characterizes the diagnosability of temporal diagnostic graphs.

34

Theorem 19 (Diagnosability in Temporal Diagnostic Graphs). Let D[K] a tem-

poral diagnostic graph built by stacking a set of K regular diagnostic graphs

D(1), . . . , D(K). Then κ(D[K]) ≥ mini∈{1,...,K} κ(D(i)).

Proof. Let z be a syndrome for the temporal diagnostic graph D[K], generated
by a set of active failure mode A, such that |A|= m ≤ mini∈{1,...,K} κ(D(i)).
Clearly, each element of A is a variable node of one of the regular diagnostic
graphs D(1), . . . , D(K) that compose D[K], therefore we can split A into the
variables nodes of each regular diagnostic graph, obtaining A(1), . . . , A(K) (these
i=1A(i) = A). Similarly, we can
sets are non-overlapping and are such that ∪K
project the syndrome z into K sub-syndromes z(1), . . . , z(K) each containing
only the test outcomes of the corresponding regular diagnostic graphs (notice

that doing the projection we lose the temporal tests, if any). By construction

|A(i)| ≤ m for each i = 1, . . . , K. From the assumption, we know that each
sub-graph D(i) is m-diagnosable. Therefore, each sub-graph D(i) will be able to
correctly identify the set of active failure modes A(i) from the syndrome z(i).
This means that D[K] is at least m-diagnosable, concluding the proof.

As an immediate result we have the following corollary, which characterizes

the diagnosability of “homogeneous” temporal diagnostic graph, obtained by

stacking multiple identical diagnostic graphs over time.

Corollary 20 (Diagnosability in Homogeneous Temporal Diagnostic Graphs).

The diagnosability of the composition of identical diagnostic graph is monoton-

ically increasing.

This means that by stacking diagnostic graphs over time, we have the op-

portunity to increase the diagnosability, without any risk of harming it.

6.2. Probabilistic Diagnosability

The deterministic notion of κ-diagnosability introduced in the previous sec-
tion imposes a strong condition on D, as it requires that any syndrome un-

equivocally encodes all possible conﬁgurations of failure modes. When the

35

tests are probabilistic, such a condition becomes too stringent: intuitively, since

with some probability each test can produce diﬀerent outcomes it is unlikely

that Lemma 17 will be satisﬁed for any κ > 0. In other words, κ-diagnosability
deals with the worst case over all possible test outcomes, which becomes too

conservative when every outcome is possible (with some probability). For this

reason, in this section, we extend the deﬁnition of diagnosability to deal with

the case where the diagnostic graph includes probabilistic tests.

Towards deﬁning a probabilistic notion of diagnosability, we introduce the

Hamming distance h(f , f (cid:48)) between two binary vector f and f (cid:48) as follows:

h(f , f (cid:48)) =

l
(cid:88)

i=1

1[fi (cid:54)= f (cid:48)
i ]

(20)

where l is the length of f , and 1 is the indicator function. Assuming that f is
the binary vector describing the active failures in the system, and that f (cid:48) is an

estimated vector of the fault states, the Hamming distance simply counts the

number of mis-identiﬁed faults. We are now ready to introduce the following

probabilistic deﬁnition of diagnosability.

Deﬁnition 21 ((Probably Approximately Correct) PAC-Diagnosability). Con-

sider a fault identiﬁcation algorithm ΨD applied to a diagnostic graph D. The di-

agnostic graph D is (γ, p)-PAC-diagnosable under ΨD, if, for some 1 ≤ γ ≤ Nf

Pr
(z,f )∼F

[h (ΨD(z), f ) ≤ γ] ≥ p

(21)

where F is the joint distribution of potential failures and test outcomes.

This deﬁnition simply says that a given fault identiﬁcation algorithm applied

to the diagnostic graph D is (γ, p)-PAC-diagnosable if it expected to make less
than γ mistakes with probability at least p. We observe that Deﬁnition 21
depends on the diagnostic graph, but also on the fault identiﬁcation algorithm.

Clearly, since the outcome of the tests is a random variable, so is the Ham-

ming distance h(ΨD(z), f ). Therefore, we can deﬁne its expected value as:

hF (ΨD) = E(z,f )∼F [h(ΨD(z), f )]

36

This quantity is the number of mistakes that the fault identiﬁcation algorithm

ΨD is expected to make. Let us suppose we have a dataset W of i.i.d. samples
of the underlying faults distribution F. Let

ˆhW (ΨD) =

1
|W|

(cid:88)

(z,f )∈W

h(ΨD(z), f )

(22)

be the empirical number of mistakes the fault identiﬁcation algorithm ΨD makes
on W. For instance, if we are given a (labeled) dataset W describing the system

execution, with the corresponding ground truth failure modes states f , we can
test our algorithm ΨD and calculate the empirical number of mistakes ˆhW (ΨD)
it makes. Then, we can use the following result to bound the expected number

of mistakes our algorithm will make in expectation over all future scenarios.

Theorem 22 (Fault Identiﬁcation Error Bound). Consider a dataset W of

i.i.d. samples of the underlying faults distribution F, and a fault identiﬁcation

algorithm ΨD over D. Then, for any δ > 0, the following inequality holds with

probability at least 1 − δ:

hF (ΨD) ≤ ˆhW (ΨD) + Nf

(cid:115)

log(2/δ)
2|W|

(23)

Proof. For each sample (z(i), f (i)) in W, the result of each Hamming distance
will less or equal than Nf . From the Hoeﬀding’s inequality we have that

(cid:104)

Pr

|hF (ΨD) − ˆhW (ΨD)|≥ (cid:15)

(cid:105)

(cid:18)

≤ 2exp

−

(cid:19)

2(cid:15)2|W|
Nf

2

(24)

Setting the right-hand side of Eq. (24) to be equal to δ and solving for (cid:15) yields:
(cid:115)

(cid:15) = Nf

log(2/δ)
2|W|

After setting the right-hand side to δ, Eq. (24) can be rewritten as:

(cid:104)

Pr

|hF (ΨD) − ˆhW (ΨD)|≤ (cid:15)

(cid:105)

≥ δ

Combining (25) and (26) and removing the absolute value we get:

(cid:34)
hF (ΨD) − ˆhW (ΨD) ≤ Nf

Pr

(cid:115)

(cid:35)

≥ δ

log(2/δ)
2|W|

from which the result easily follows.

37

(25)

(26)

(27)

The previous result essentially says that the expected number of mistakes

the algorithm ΨD makes stays close to the empirical mean ˆhW (ΨD), and the
distance from the empirical mean gets smaller when the training dataset gets

larger (i.e., for larger |W|), but gets larger for larger number of failure modes
(i.e., for larger Nf ). The following corollary easily follows.

Corollary 23 (Characterization of PAC-diagnosability). For a given dataset W

of i.i.d. samples of the underlying faults distribution F, and a fault identiﬁcation

algorithm ΨD over D, the diagnostic graph D is (γ, p)-PAC-diagnosable with p

satisfying the following inequality:

p ≥ 1 − 2e

−2

(cid:16) γ−ˆhW
Nf

(cid:17)2

|W|

(28)

Proof. Let γ = hF (ΨD) and p = 1 − δ, substituting into Eq. (23), and solving
for p yield the result.

Remark 24 (Diagnosability over Subgraphs). Given a diagnostic graph D, we

might be interested in running fault identiﬁcation algorithms over a subgraph

¯D ⊆ D. Analyzing the diagnosability of certain subgraphs of D might suggest

weaknesses of the perception pipeline. For example the system might have suﬃ-

cient redundancy to be able to correctly identify the faults in the obstacle detec-

tion subgraph with low errors and high conﬁdence, but might lack of redundancy

to detect and identify faults in the traﬃc light recognition.

7. Experimental Evaluation

This section shows that diagnostic graphs are an eﬀective model to de-

tect and identify failures in complex perception systems.

In particular, we

show that the proposed monitors (i) outperform baselines in terms of fault

identiﬁcation accuracy, (ii) allow detecting failures and provide enough notice

to prevent accidents in realistic test scenarios, and (iii) run in milliseconds,

adding minimal overhead. A video showcasing the execution of the proposed

runtime monitors can be found at https://www.mit.edu/~antonap/videos/
AIJ22PerceptionMonitoring.mp4.

38

We test our runtime monitors in several scenarios, speciﬁcally designed to

stress-test the perception system. The scenarios are simulated using the LGSVL

Simulator [15], an open-source autonomous driving simulator. The simulator

also generates ground-truth data, e.g., ground-truth obstacles and active failure

modes, and seamlessly connects to the perception system through the Cyber RT

Bridge interface [15]. We apply our monitors to a state-of-the-art perception

system. In particular, we use Baidu’s Apollo Auto [16] version 7 [95]. Baidu’s

Apollo is an open-source, sate-of-the-art, autonomous driving stack that includes

all the relevant functionalities for level 4 autonomous driving.

Section 7.1 provides more details about Apollo Auto and its perception sys-

tem. Section 7.2 describes the diagnostic tests we design for Apollo Auto’s per-

ception system. Section 7.3 discusses implementation details for the proposed

monitors. Section 7.4 describes our test scenarios. Section 7.5 provides quan-

titative fault detection and identiﬁcation results, including an ablation study

of the diﬀerent GNN architectures. Section 7.6 provides qualitative results and

discussion for a key test scenario.

7.1. Apollo Auto

Baidu’s Apollo Auto [16] uses a ﬂexible and modularized architecture for

the autonomy stack based on the sense-plan-act framework. The stack includes

seven subsystems: (i) the localization subsystem provides the pose of the ego

vehicle; (ii) the high-deﬁnition map provides a high-resolution map of the en-

vironment, including lanes, stop signs, and traﬃc signs; (iii) the perception

subsystem processes sensory information (together with the localization data)

and creates a world model; (iv) the prediction subsystem predicts future evolu-

tion of the world state; (v) the motion planning subsystems and (vi) the routing

subsystem generate a feasible trajectory for the ego vehicle, and ﬁnally, (vii)

the control subsystem generates low-level control signals to move the vehicle.

In our experiments, we focus on the perception subsystem, to which we apply

our runtime monitors. In the following, we brieﬂy review the key aspects of the

Apollo Auto perception system.

39

7.1.1. Apollo Auto Perception System

Apollo Auto’s perception system is tasked with the detection and classi-

ﬁcation of obstacles and traﬃc lights.8 The perception module is capable of

using multiple cameras, radars, and LiDARs to recognize obstacles. There is

a submodule for each sensor modality, that independently detects, classiﬁes,

and tracks obstacles. The results from each sub-module are then fused using a

probabilistic sensor fusion algorithm.

Obstacle Detection. Obstacles such as cars, trucks, bicycles, are detected

using an array of radars, LiDARs, and cameras. Each obstacle is represented

by a 3D bounding-box in the world frame, the class of the object, a conﬁdence

score, together with other sensor-speciﬁc information (e.g., the velocity of the

obstacle). Each sensor is processed as follows:

Camera: The camera-based obstacle detection network is based on the monoc-

ular object detection SMOKE [96] and trained on the Waymo Open Dataset [97].

The network predicts 2D and 3D information about each obstacle, and

then a post-processing step predicts the 3D bounding box of each ob-

stacle by minimizing the reprojection error of available templates for the

predicted obstacle class;

LiDAR: The LiDAR-based obstacle detection network, called Mask-Pillars is

based on PointPillars [98], but enhanced with a residual attention module

to improve detection in case of occlusion;

Radar: Apollo Auto uses directly the obstacles detections reported by the radar

(assumed to have an embedded detector [99]), that are post-processed to

be transformed to the world frame.

8Note that our monitors can be also applied to other perception-related subsystems, such

as the localization and high-deﬁnition map subsystem, see [72] for an example.

40

LiDAR

Camera

Radar

Figure 7: Vehicle conﬁguration and sensor ﬁeld-of-view (FOV). LiDAR FOV is shown in

green, the camera FOV in blue and the radar FOV in orange.

7.1.2. Vehicle Conﬁguration

The simulated vehicle is a Lincoln MKZ with one Velodyne VLS-128 LiDAR,

one front-facing camera with a ﬁeld-of-view of 50◦, one front-facing telephoto
camera (pointed 4◦ upwards) for traﬃc light detection and recognition, one
Continental ARS 408-21 front-facing radar, GPS, and IMU.

We ran the Baidu’s Apollo AV stack on a computer with an Intel i9-9820X

(4.1 GHz) processor, 64 GB of memory and two NVIDIA GeForce RTX 2080Ti.
The simulator ran on a computer with 11th Generation Intel i7-11700F (4.8 GHz)
processor, 16 GB of memory, and an NVIDIA GeForce RTX 3060. The two com-
puters were connected using a Gigabit Ethernet cable.

7.2. Diagnostic Graph

We focused our attention on the obstacle detection pipeline. The system we

aim to monitor, together with the failure modes considered, is shown in Fig. 8

The system is composed of four modules:

• Lidar-based Obstacle detector, based on a deep learning algorithm, subject

to out-of-distribution sample failure mode;

• Camera-based Obstacle detector, based on a deep learning algorithm, sub-

ject to out-of-distribution sample failure mode;

• Radar-based Obstacle detector subject to misdetection failure mode;

41

LiDARCamera100°60°20°Radar• Sensor Fusion subject to misassociation failure mode.

Each module produces a set of detected obstacles. We identiﬁed three failure

modes for each set of detected obstacles:

• misdetection: the module detected a ghost obstacle or is missing an ob-

stacle in the scene;

• misposition: the module detected the obstacle correctly, but its position

is incorrect (i.e., more than 2.5m error in our tests);

• misclassiﬁcation: the module detected the obstacle correctly but the ob-

stacle’s semantic class is incorrect.

We equipped the obstacle detection system with 18 diagnostic tests. For each

pair of modules’ outputs, namely (Lidar, Camera), (Radar, Camera), (Lidar,

Sensor Fusion), (Radar, Sensor Fusion), (Lidar, Radar), and (Camera, Sen-

sor Fusion), there is a test that compares the outputs to diagnose each of the

output’s failure modes (i.e., misdetection, misposition, and misclassiﬁcation).

Intuitively, each test compares the two sets of obstacles coming from the cor-

responding modules, and if they are diﬀerent, it reports if the inconsistency

was due to a misdetection, misposition, or misclassiﬁcation. Moreover, we in-

cluded a priori relation between every module and its output.

In particular,

the modules are assumed to fail if their outputs have at least one active failure

mode. In the probabilistic diagnostic graph we also added an a priori relation

for each module’s failure mode, indicating the prior probability of that failure

mode being active.

7.2.1. Diagnostic Tests

We now describe the logic for the diagnostic tests we implemented. Consider

two sets of synchronized detected obstacles9, say A and B, produced by two
modules, using some sensor data. Let Ω be the region deﬁned by the intersection
of both sensor ﬁelds of view and a region of interest (e.g., a region close to a

9By synchronized we mean that the two outputs are produced at the same time instant.

42

Figure 8: Perception system considered in our experiments. Modules are shown as rectangular

blocks, outputs are shown as rounded boxes, while failure modes are denoted with red dots.

drivable area10). Denote by AΩ and BΩ the set of obstacles restricted to the
region Ω, namely AΩ ⊆ A such that for each obstacle o in A, o is in AΩ if and
only if o is inside the region deﬁned by Ω. The same relation holds for BΩ. Then
the diagnostic test checking for misdetections is deﬁned as follows:

tmisdetection =






FAIL

if |AΩ| (cid:54)= |BΩ|

PASS

otherwise

Note that if the two sets of obstacles have a diﬀerent cardinality —when re-

stricted to the area co-visible by both sensors— it means that one of the two

sets contains a ghost obstacle or one of the two sets is missing an obstacle. From

a single test, we are not able to say which of the two sets is experiencing the

misdetection, but we know at least one output did.

Let us now move our attention to the misposition failure mode. Let C be
the set of matched obstacles, that is, a pair of obstacles (l, r) —with l ∈ AΩ and
r ∈ BΩ— is in C, if l and r represent the same obstacles. A common approach
for ﬁnding the set of matches is to select all the pairs that are closest to each

10In our experiments, the region of interest is the area within 5 meters from a drivable lane.

43

LiDAR-basedObstacle detectorLiDARObstaclesCameraObstaclesCamera-basedObstacle detectorSensor FusionAlgorithmFusedObstaclesRadarObstaclesRadar-basedObstacle detectorMisdetectionMisassociationOut-of-distribution sampleOut-of-distribution sampleMisdetectionMispositionMisclassificationMisdetectionMispositionMisclassificationMisdetectionMispositionMisclassificationMisdetectionMispositionMisclassificationother (i.e., solving an assignment problem)11. The diagnostic test checking for

mispositioned obstacles is deﬁned as follows:

tmisposition =




FAIL



PASS

∃(l, r) ∈ C such that |pos(l) − pos(r)|≥ θ

otherwise

where pos(·) is the position of an obstacle and θ is an error threshold, chosen
as θ = 2.5 m in our experiments.

Finally, the test checking for misclassiﬁed obstacles is deﬁned as follows:

tmisclassiﬁcation =




FAIL



PASS

∃(l, r) ∈ C such that cls(l) (cid:54)= cls(r)

otherwise

where cls(·) is the class of the obstacle, i.e., the test fails if associated obstacles
are assigned diﬀerent semantic classes.

7.2.2. Temporal Diagnostic Graph

To build a temporal diagnostic graph we stack 2 regular diagnostic graphs
into a temporal diagnostic graph. In the probabilistic case, each module failure

mode is connected to its successive (in time) via a priori relationships, which

represent the transition probability between states in consecutive time steps. No

temporal a priori relations are added in the deterministic case. We also added

temporal tests. The logic of the tests presented in Section 7.2 is applicable to

temporal tests with small changes. In temporal tests, the sets A and B are not

time-synchronized anymore (e.g., they are obstacles detected by the same sensor

at consecutive time stamps), therefore the position of each obstacle in each set

must be adjusted for the distance the obstacle traveled between consecutive

detections. To use the tests described earlier in the temporal domain we used

the following approach. If the obstacle is equipped with an estimated velocity

vector, since the time diﬀerence between detections is usually below 30 ms, we

11We matched obstacles using a generalization of the Hungarian algorithm [100], with the

cost of each match being the Euclidean distance between obstacles.

44

assume constant speed and integrate the speed over the time interval to ﬁnd an

approximate position of each obstacle. When the velocity is not available, we

use the average speed of an obstacle (for a given obstacle’s class) and adapt the

misposition threshold θ to account for the uncertainty.

7.3. Fault Identiﬁcation: Implementation Details

Deterministic Fault Identiﬁcation. For the tests with the deterministic

model, we assumed the Weaker-OR model for the diagnostic tests as described

in Eq. (3). We used this model for both the regular diagnostic graph and the

temporal diagnostic graph, and solved the optimization problem in Eq. (8) using

Google OR-Tools [101] Integer Programming Solver.

Probabilistic Fault Identiﬁcation. To perform probabilistic inference

on the diagnostic graph, we transformed it into a factor graph and trained

the potentials for each relation using the maximum margin learning algorithm

described in Section 5.2 on the training dataset. We used the Hamming distance

deﬁned in Eq. (20) as the loss function L. We set the regularization parameter
to λ = 10; see [85].12 For each diagnostic graph, we perform inference using
the max-product algorithm for a ﬁxed number of iteration (100 iterations). In
our implementation, we use the Grante library [102] to perform learning and

inference over the factor graph.

Graph-Neural-Network-based Fault Identiﬁcation. In Section 5.3 we

saw that a graph neural network requires a feature for each node in the graph to

perform neural message passing. We now discuss how we set the feature vector

for each node in the graph. Recall that the GNN uses a pairwise undirected

graph, where a node is either a failure mode or a test outcome. The feature

xtk ∈ R2 for a test tk is set as the one-hot encoding of the test outcome (i.e.,
[1 0] if the test passed, [0 1] if it failed). For the failure mode nodes we do not
have any measurable quantity at runtime; we therefore use the training dataset

12In our experiment we noticed that the performance of the learning algorithm are not

sensitive to the choice of λ.

45

to compute the feature vectors. In particular the feature vector xfi ∈ R2 for
let ρi be the empirical probability
a failure mode fi is computed as follow:
1[fi = ACTIVE]; then the feature
that fi is ACTIVE, i.e., ρi = 1
|W|
T . Intuitively, the feature describes the prior
vector is chosen as xfi = [1 − ρi, ρi]
probability of the failure mode fi’s state.

(z,f )∈W

(cid:80)

We now discuss the architecture of the GNN. Our GNN is composed by a

linear layer that embeds the feature vectors in R16, followed by a ReLU function.

The output is then passed to a stack of graph convolution layers interleaved with

ReLU activation functions. We tested four diﬀerent graph convolution layers

• in the case of GCN, we stack 3 layers with 16 hidden channels each;
• in the case of GCNII, we stack 64 layers with 16 hidden channels each

with α = 0.1, β = 0.4;

• in the case of GIN, we stack 3 layers with 16 hidden channels each with the
function ζ (k)(·) (cf. Eq. (19)) being a 2-layer perceptron for k = 1, . . . , 3;
• in the case of GraphSAGE, we stack 3 (and 6 for temporal diagnostic

graphs) layers with mean aggregator and 16 hidden channels each.

Finally, the readout function that converts the graph embedding to node labels

is a linear layers followed by a softmax pooling. We perform an ablation of the

diﬀerent GNN architectures in Section 7.5.

We implemented the GNNs in PyTorch [93] and trained them on the train-

ing dataset for 100 epochs using the Adam optimizer. To reduce the amount
of guesswork in choosing an initial learning rate, we used the learning rate

ﬁnder available in the PyTorch Lightning library [103]. The procedure is based

on [104]: the learning rate ﬁnder does a small training run where the learn-

ing rate is increased after each processed batch and the corresponding loss is

logged. Then, the learning rate is chosen to be the point with the steepest

negative gradient.

Baselines. We compared the proposed monitors against two simple base-

lines. In the ﬁrst baseline (label: “Baseline”), whenever a diagnostic test returns

FAIL, all failure modes in its scope are considered active. In the second baseline

46

(label: “Baseline (w/rel. scores)”), modules are ordered by a reliability score de-

ﬁned by the system designer. In our experiments we considered the radar to be

more reliable than the sensor fusion, which is more reliable than the LiDAR,

which in turn is more reliable than the camera. When a diagnostic test fails,

this second baseline labels all the failure modes in the test scope associated to

the least reliable module (and its outputs) as ACTIVE. For example if a diag-

nostic test comparing camera and LiDAR obstacles returns FAIL, the failure

modes associated with the camera are the ones that are labeled active because

the camera is considered less reliable than the LiDAR. Both baselines label a

module’ failure modes as active if at least one of the module’s outputs is failing.

7.4. Scenarios

We designed a set of challenging scenarios to stress-test the Apollo Auto

perception system. These scenarios were created using the LGSVL Simula-

tor Visual Scenario Editor, which allows the user to create scenarios using a

drag-and-drop interface. The vehicle behavior is tested on each scenario in a

multitude of situations including diﬀerent time of day (noon, 6 PM, 9 PM) or

weather condition (rain and fog). The scenarios are described in Table 2.

Table 2: Scenarios.

Fault-free.

Fault-detected.

Scene

Fault Detection Results

Hidden Pedestrian. A pedestrian, initially occluded by a track parked on the

right-hand side of the street, steps in front of the ego vehicle.

47

Overturned Truck. The ego vehicle encounters an overturned truck occupying the

lane it is driving in. The scenario recreates an accident occurred in Taiwan where a

Tesla hit an overturned truck on a highway [105].

Stopped Vehicle. While driving, the car in front of the ego vehicle makes a lane

change to avoid the stationary car that is in their lane. This leaves the ego vehicle

with little to no time to react to the stationary car.

Cut Oﬀ Left. While driving in the right lane on a three-lane road, a vehicle from

the left lane cuts the ego vehicle oﬀ.

48

Cut Oﬀ Right. While driving in the left lane on a two-lane road, a vehicle from the

right lane cuts the ego vehicle oﬀ while turning into a parking lot.

School Bus Intersection. The ego vehicle drives through an intersection. A school

bus crosses the intersection coming from the left-hand side. As the ego vehicle crosses

the intersection, a pedestrian steps into the intersection from the left-hand side.

Car in Front. A car is still in front of the ego vehicle preventing it to move forward.

49

Cones in the Lane. The ego vehicle is driving on a lane partially delimited by traﬃc

cones, while another vehicle is driving in the opposite lane. After passing traﬃc cones,

another vehicle exits a parking lot and merges right in front of the ego vehicle.

Cyclist. The ego vehicle is stopped at an intersection and as it starts driving through

the intersection, a cyclist enters the ﬁeld of view from the left-hand side of the inter-

section and rides right in front of the ego vehicle.

Turkeys. While driving on a straight road, the ego vehicle must avoid a collision

with two turkeys that suddenly walk in front of the ego vehicle.

7.4.1. Dataset generation

We executed the diagnostic tests described in Section 7.2 every 0.3 sec-
onds, and used the corresponding test outcomes to perform fault identiﬁcation.

Time synchronization of the modules’ output is achieved by pairing outputs

that are closest in time to each other. Ground-truth labels for the outputs’

failure modes are generated using the ground-truth detections provided by the

simulator. In particular, to generate the label for each failure mode of an out-

put, we used the three diagnostic tests described in Section 7.2.1 comparing

the set of obstacles to the ground-truth detections. For a module m instead,
since all modules have only one failure mode, the associated failure mode fm

50

Algorithm

Factor Graph

Deterministic

93.30

91.06

Baseline (w/rel. scores)

92.39

Baseline

GCN

GCNII

GIN

GraphSage

84.85

92.27

87.61

91.89

92.84

Regular

Temporal

All

Outputs Modules

All

Outputs Modules

96.72

93.69

94.65

89.09

96.01

93.94

96.06

96.46

83.03

83.18

85.61

72.12

81.06

68.64

79.39

81.97

93.60

89.26

90.18

83.90

91.79

92.60

93.21

92.71

96.88

92.33

92.69

87.73

96.06

96.01

96.47

96.42

83.74

80.06

82.67

72.39

78.99

82.36

83.44

81.60

Table 3: Fault identiﬁcation accuracy. Best accuracy if highlighted in green, second-best is

highlighted in yellow.

is labeled as ACTIVE if and only if any failure mode if its output is ACTIVE.

We collected 1650 regular diagnostic graphs, and split them into 1320 (80%)
training samples, 165 (10%) testing samples, and 165 validation samples. We
also collected 1590 temporal diagnostic graphs, and split them into 1272 (80%)
training samples, 159 (10%) testing samples, and 159 validation samples.

7.5. Fault Detection and Identiﬁcation Results

We used three metrics to evaluate the performance for both the fault detec-

tion and identiﬁcation problems:

Accuracy is the percentage of correctly detected (resp. identiﬁed) failures over

the total number of samples;

Precision measures the percentage of correct identiﬁcations over the number

of failures the fault identiﬁcation system reported; a monitor achieves high

precision if it has a low rate of false alarms;

Recall measures the percentage of correct identiﬁcations over the number of

failures the system experienced; a monitor has high recall if it is able to

catch a large fraction of failures occurring in the perception system.

51

Figure 9: Precision/Recall for regular diagnostic graphs. (Left) Modules, (Right) Outputs.

Figure 10: Precision/Recall for temporal diagnostic graphs. (Left) Modules, (Right) Outputs.

7.5.1. Fault Identiﬁcation Results

Table 3 reports the accuracy of all compared techniques, averaged across all

test scenarios in Table 2. The ﬁrst and fourth columns report the overall ac-

curacy (“All”) when using regular and temporal diagnostic graphs, respectively.

The remaining columns report a breakdown of the accuracy in terms of mod-

ules and outputs. The overall accuracy results suggest that factor-graph-based

probabilistic fault identiﬁcation outperforms all other algorithms and achieves

96.72 % accuracy when using regular diagnostic graphs and 96.88 % with tempo-
ral diagnostic graphs. GNNs architectures achieve the second-best performance

(GraphSAGE in the regular case, GIN in the temporal case). If we now look at

the breakdown of the fault identiﬁcation results between modules and outputs,

we notice two trends. First, the factor graph still performs the best across the

spectrum, but it is slightly slightly inferior than a baseline in the regular case.

52

4050607080Precision(%)405060708090Recall(%)FactorGraphDeterministicBaseline(w/rel.scores)BaselineGCNGCNIIGINGraphSageOutputs405060708090Precision(%)20406080100Recall(%)FactorGraphDeterministicBaseline(w/rel.scores)BaselineGCNGCNIIGINGraphSageModules405060708090Precision(%)405060708090Recall(%)FactorGraphDeterministicBaseline(w/rel.scores)BaselineGCNGCNIIGINGraphSageOutputs50607080Precision(%)2030405060708090Recall(%)FactorGraphDeterministicBaseline(w/rel.scores)BaselineGCNGCNIIGINGraphSageModulesAs we will see shortly, the baselines tend to make quite conservative decisions

(i.e., they tend to detect more failures than the ones actually present in the sys-

tem), which increases accuracy (and recall) at the expense of precision. Second,

output fault identiﬁcation has higher accuracy than module fault identiﬁcation;

this is expected, since most of our tests directly involve outputs, while we can

only indirectly infer module failures via the a priori relations. Note that the

two statistics (output fault identiﬁcation vs. module fault identiﬁcation) are

typically used for diﬀerent purposes, as discussed in Remark 2.

Fig. 9 shows precision-recall trade-oﬀs when using regular diagnostic graphs.

Best results are near the top-right corner of each ﬁgure, where both precision

and recall are high. The ﬁgure conﬁrms that while the baselines have large recall

(due to the fact that are conservative in detecting failure modes as active), their

precision is relatively low (i.e., they have a large number of false alarms). On the

other side of the spectrum, GNN architectures (with the exception of GCNII)

achieve high prediction (87.25 % for GraphSAGE) but low recall (60.96 % for
GraphSAGE). The deterministic fault identiﬁcation struggles to mark failure

modes as active, achieving low precision and recall in the output space; this is

due to the fact that it disregards PASS results (which do not even appear in

the optimization Eq. (8)). Factor graph inference again achieves a reasonable

trade-oﬀ, with 85.22 % precision and 67.12 % recall.

Fig. 10 shows precision-recall trade-oﬀs when using temporal diagnostic

graphs. Compared to the regular diagnostic graph we see a steep increase in

precision in the output space. The best-performing model goes from around

90 % precision of the regular graph to 97 % of the temporal diagnostic graph.

PAC-Diagnosability. Fig. 11 and Fig. 12 show the PAC-Diagnosability

bound deﬁned in Eq. (23) for each of the compared techniques. The bound

represents the number of fault identiﬁcation mistakes each algorithm is expected

to make with a given conﬁdence (δ in Eq. (23)). The plots show that with high
probability, most of the algorithms are expected to make less than 1 mistake in
the fault identiﬁcation (i.e., false alarms or false negatives). The factor graph

has the lowest bound of all methods in both the regular and temporal diagnostic

53

graphs; the only exception is Fig. 11(right), where the baseline with reliability

score has the lowest bound for module fault identiﬁcation.

Figure 11: PAC-diagnosability bounds for regular diagnostic graphs. (Left) Modules, (Right)

Outputs. Lower is better.

Figure 12: PAC-diagnosability bounds for temporal diagnostic graphs.

(Left) Modules,

(Right) Outputs. Lower is better.

κ-diagnosability. Let us now discuss the deterministic diagnosability of
the perception system considered in our experiments (Fig. 8). If the tests be-

have as a Deterministic OR, the diagnostic graph used in our experiments is

5-diagnosable. This means that if there are up to 5 active failure modes the
deterministic fault identiﬁcation will be able to correctly identify them. If we

instead assume the tests behave as a Weak-OR, which might fail when all the

failure modes in its scope are active, the diagnostic graph is 3-diagnosable. It’s
worth noticing that this does not mean that if there are more than 3 (or 5)
active failure modes the fault identiﬁcation will surely fail, but rather that we

54

9092949698100Conﬁdence(%)1.01.52.02.5BoundFactorGraphDeterministicBaseline(w/rel.scores)BaselineGCNGCNIIGINGraphSage9092949698100Conﬁdence(%)0.81.01.21.41.6Bound9092949698100Conﬁdence(%)0.751.001.251.501.752.002.252.50BoundFactorGraphDeterministicBaseline(w/rel.scores)BaselineGCNGCNIIGINGraphSage9092949698100Conﬁdence(%)0.81.01.21.4Bounddo not have the guarantee that it will not make any mistake. When using De-

terministic Weaker-OR tests, the diagnosability drops to zero, meaning that the

fault identiﬁcation guarantees vanish.

Extra diagnosability results. To show the eﬀectiveness of the deter-
ministic and probabilistic diagnosability we generated a random 4-diagnosable
diagnostic graph with 10 independent failure modes and Weak-OR tests and col-
lected the fault identiﬁcation results (using the deterministic model) for every

syndrome and every possible fault assignment. The results are shown in Fig. 13.

The ﬁgure reports the average number of incorrect fault identiﬁcation results

(i.e., the Hamming distance between the estimated and actual vector of active

faults) for increasing number of active faults. The vertical dashed line represents

the deterministic diagnosability value: by Deﬁnition 16, the fault identiﬁcation

is guaranteed to correctly identify the active failure modes provided that there

are less than 4 active failure modes. In fact, from the plot we see that the fault
identiﬁcation algorithm does not make any mistake in the fault identiﬁcation

when there are less than 4 faults. The horizontal dashed line instead represents
the probabilistic diagnosability value, in particular it is the ceiling of the bound

in Eq. (23), computed with very high conﬁdence (1 − 1 × 10−12). The bound
guarantees that with high probability the average number of mistakes (the av-

erage Hamming distance) the fault identiﬁcation algorithm is going to make is

less that 2; this is again consistent with the numerical results.

Timing. The runtime of each method is shown in Table 4. All algorithms
perform inference in less than 4 ms, except for GCNII which averages at around
20 ms. This is likely due to the fact that GCNII uses a deep architecture, which
incurs an increased computational cost. The best performing algorithm, i.e.,

the factor graph, can be executed in real-time as its runtime averages around

0.8 ms for regular graphs and 3.8 ms for temporal graphs.

7.5.2. Fault Detection Results

Recall that fault detection is the problem of deciding whether the system is

working in normal conditions or whether at least a fault has occurred. Table 5

55

Figure 13: Average Hamming distance between the estimated and actual vector f of fault

states in a randomly generated 4-diagnosable diagnostic graph with 10 independent failure

modes and Weak-OR tests. The vertical dashed line represents the deterministic diagnosability

bound: if the system is experiencing less than 4 active failure modes, the fault identiﬁcation

is guaranteed to be correct (0 Hamming distance). The horizontal dashed line represents the

ceiling of the PAC-diagnosability bound in Eq. (23): with very high probability the average

number of mistakes (average Hamming distance) is less than the PAC-diagnosability bound.

and Fig. 14 show accuracy, precision, and recall. Fig. 14 shows that most of

the algorithms for inference presented in this paper (as well as the baselines)

attain similar performance with precision above 90 % and recall above 80 %; this
conﬁrms that fault detection is a somewhat easier problem compared to fault

identiﬁcation. Table 5 shows that the deterministic approach and the baselines

do particularly well for fault detection: they both detect failure as soon as a

single test fails, which makes their accuracy high. On the other hand, the factor

graph approach may prefer explaining a failed test as a false alarm. Therefore,

while factor graphs would be the go-to approach for fault identiﬁcation, a simpler

baseline approach suﬃces for fault detection.

7.6. Example Scenario: Using Monitoring to Prevent Accidents

We conclude the experimental section by showing how fault detection and

identiﬁcation can be eﬀectively used to prevent dangerous situations. To this

aim, we developed an additional scenario (not included in Table 2) where a deer

crosses the road while the ego vehicle cruises on a straight road (Fig. 15).

The scenario is novel to the identiﬁcation algorithm, i.e., not used for train-

ing, test, or validation. The results of the failure identiﬁcation are shown

56

012345678910NumberofActiveFailureModes0.00.51.01.52.0AverageHammingDistancei

c
i
t
s
i
n
m
r
e
t
e
D

e
n

i
l
e
s
a
B

)
s
e
r
o
c
s

.
l
e
r
/
w
(

r
o
t
c
a
F

h
p
a
r
G

e
n

i
l
e
s
a
B

N
C
G

I
I

N
C
G

I

N
G

e
g
a
S
h
p
a
r
G

r Avg.
a
l
u
g
e
R

Std.

0.79

3.25

0.10

0.10

0.63

19.88

0.48

0.59

(0.17)

(0.14)

(0.06)

(0.06)

(0.01)

(0.10)

(0.01)

(0.02)

l Avg.

2.53

3.68

0.27

0.26

0.68

24.56

0.50

0.85

Std.

(0.04)

(0.46)

(0.17)

(0.16)

(0.01)

(0.33)

(0.01)

(0.01)

a
r
o
p
m
e
T

Table 4: Average runtime (“Avg.”) and standard deviation (“Std.”) for fault identiﬁcation, in

milliseconds.

Algorithm

Factor Graph

Deterministic

76.67

89.09

Baseline (w/rel. scores)

89.09

Baseline

GCN

GCNII

GIN

GraphSage

89.09

71.82

68.48

83.94

76.67

Regular

Temporal

All

Outputs Modules

All

Outputs Modules

88.48

89.09

89.09

89.09

86.06

87.88

86.06

89.09

64.85

89.09

89.09

89.09

57.58

49.09

81.82

64.24

81.60

93.25

85.28

85.28

80.06

78.83

83.13

79.14

91.41

93.25

85.28

85.28

90.18

85.89

92.64

89.57

71.78

93.25

85.28

85.28

69.94

71.78

73.62

68.71

Table 5: Fault detection accuracy. Best accuracy if highlighted in green, second-best is

highlighted in yellow.

57

Figure 14: Fault detection in diagnostic graphs. (Left) Regular, (Right) Temporal.

Figure 15: Example scenario involving a deer crossing the road in front of the ego vehicle.

in Fig. 16, where we used the probabilistic fault identiﬁcation.

Initially, the

monitor detects no failure (rightmost green section). As the ego vehicle gets

closer to the undetected obstacle, the radar detects the obstacle but the camera

does not. The inconsistency between the two sets of obstacles causes the test

between camera and radar to return FAIL. Given the test’s outcomes, the factor

graph correctly detects and identiﬁes the failure, triggering an alarm (rightmost

red section). As the ego vehicle gets even closer, the deer goes out of the ﬁeld-

of-view of the radar while entering the LiDAR ﬁeld-of-view. For a few meters,

both camera and LiDAR fail to detect the deer Fig. 17, but since it is out of

the ﬁeld-of-view of the radar, the diagnostic test fails to report the failure13.

13This could be solved by improving the logic of the diagnostic test; for instance, it could

58

80859095Precision(%)7580859095Recall(%)FactorGraphDeterministicBaseline(w/rel.scores)BaselineGCNGCNIIGINGraphSageTemporal5060708090Precision(%)80859095100Recall(%)FactorGraphDeterministicBaseline(w/rel.scores)BaselineGCNGCNIIGINGraphSageRegularFigure 16: Fault identiﬁcation results for the example scenario in Fig. 15. The car travels

from right to left. Initially, the monitor detects no failure (rightmost, green section). As the

ego vehicle gets closer to the obstacle, the LiDAR-based and camera-based obstacle detectors

fail to detect the deer while the radar-based obstacle detector correctly locates the obstacle;

as a result the fault identiﬁcation/detection triggers an alarm (red sections).

As the obstacle re-enters the ﬁeld-of-view of the radar, the diagnostic test again

returns FAIL, signaling the presence of a failure.

The ﬁrst alarm is raised 7.19 s before the collision, ﬂagging the camera mis-
detection as an active failure mode. Before the collision, the AV has a speed of

8.43 m/s. The car can reach a maximum deceleration of 6 m s−2. As result, the
car would need 1.4 s to come to a complete stop. We note that after detecting
the fault, for a short interval of time the monitor detects no failure: this is

due to the fact that the deer goes out of the radar ﬁeld-of-view, and no other

obstacle detector is capable of detecting it, thus lacking redundancy to diagnose

the failure; see the visualization and explanation in Fig. 18.

To gather statistical evidence of the eﬀectiveness of the fault detection, we

run the same scenario 10 times at diﬀerent times of the day (sun, twilight, and

night) and diﬀerent weather conditions (including fog and rain). The proba-

bilistic fault detection approach never raised false alarms in these tests, and the

average time between the alarm and the collision was 7.54 s. The car traveled
at an average speed of 6.16 m/s, requiring 1.03 s to come to a complete stop.

predict that —while the obstacle moved outside the ﬁeld-of-view— it is unlikely it disappeared.

59

Figure 17: Camera Image for the scenario in Fig. 15. Blue bounding box is the ground truth

detection. The camera fails to detect the deer crossing the road (misdetection failure).

The radar detects the obstacle,
but the camera fails to do so

Camera and LiDAR fail to
detect the obstacle while it is
outside the radar ﬁeld-of-view

Figure 18: Two snapshots from the example scenario of Fig. 15. Shaded areas represent the

sensor ﬁeld-of-view (FOV): green, blue, and orange represent the LiDAR, camera, and radar

FOVs, respectively. On the left, the deer is outside the LiDAR FOV (so the LiDAR obstacle

detector is not supposed to detect the obstacle); the radar detects the obstacle, while the

camera fails to detect it even if it is inside its FOV. Since the corresponding diagnostic test

fails, our monitors can detect the failure. On the right, the deer is outside the radar FOV;

in this case, both the camera and the LiDAR fail to detect the obstacles (even though it is

within their FOVs), hence no diagnostic test fails and our monitor fails to detect the fault.

The fault identiﬁcation exhibited an average accuracy of 93.75 %.

60

8. Conclusions

This paper investigated runtime monitoring of complex perception systems

and presented a novel framework to collect and organize diagnostic information

for fault detection and identiﬁcation in perception systems. Toward this goal,

we formalized the concept of diagnostic tests, a generalization of runtime mon-

itors, that return diagnostic information about the presence of failure modes.

We then introduced the concept of diagnostic graph, as a structure to organize

diagnostic information and its relations with the monitored perception system.

We then provided a set of deterministic, probabilistic, and learning-based algo-

rithms that use diagnostic graphs to perform fault detection and identiﬁcation.

In addition to the algorithms, we investigated fundamental limits and provided

deterministic and probabilistic guarantees on the fault detection and identiﬁca-

tion results. These include results about the maximum number of faults that

can be correctly identiﬁed in a given perception system as well as PAC-bounds

on the number of mistakes our fault identiﬁcation algorithms are expected to

make. We conclude the paper with an extensive experimental evaluation, which

recreates several realistic failure modes in the LGSVL open-source autonomous

driving simulator, and applies the proposed system monitors to a state-of-the-

art autonomous driving software stack (Baidu’s Apollo Auto). The results show

that the proposed system monitors outperform baselines in terms of fault identi-

ﬁcation accuracy, have the potential of preventing accidents in realistic scenarios,

and incur a negligible computational overhead.

This work opens a number of avenues for future work. First, we plan to test

our monitors on real-world datasets (rather than realistic simulations) and to

provide more examples of the proposed approach applied to other perception

subsystems (e.g., localization, lane segmentation). Second, we plan to add a risk

metric to the fault identiﬁcation process that could help the decision layer to

make more informed decisions. Finally, in this paper, we used simple diagnostic

tests. Moving forward, it would be desirable to use more advanced diagnostic

tests available in the literature.

61

References

[1] Google’s

self-driving startup Waymo is

introducing fully driver-

less

rides

to San Francisco,

https://www.businessinsider.com/

waymo-testing-fully-automated-cars-san-francisco-2022-3,
cessed: 2022-05-14.

ac-

[2] G. Silberg, R. Wallace, G. Matuszak, J. Plessers, C. Brower, D. Subrama-

nian, Self-driving cars: The next revolution, White paper, KPMG LLP &

Center of Automotive Research 9 (2) (2012) 132–146.

[3] NTSB, Collision between vehicle controlled by developmental automated

driving system and pedestrian, tempe, arizona (2018).

URL

https://www.ntsb.gov/investigations/AccidentReports/

Reports/HAR1903.pdf

[4] American Automobile Association,

Active

driving

assistance

system

performance,

https://newsroom.aaa.com/asset/
active-driving-assistance-system-performance-may-2022/ (2022).

[5] Waymo

and Cruise

self-driving

cars

took over San Francisco

streets at

record levels

in 2021 — so did collisions with other

cars,

scooters,

and

bikes,

https://www.businessinsider.com/

self-driving-car-accidents-waymo-cruise-tesla-zoox-san-francisco-2022-1,
accessed: 2022-05-14.

[6] H. Yang, J. Shi, L. Carlone, TEASER: Fast and Certiﬁable Point Cloud

Registration, arXiv preprint arXiv: 2001.07715(pdf).

[7] M. Fischler, R. Bolles, Random sample consensus: a paradigm for model

ﬁtting with application to image analysis and automated cartography,

Commun. ACM 24 (1981) 381–395.

[8] R. Salay, R. Queiroz, K. Czarnecki, An analysis of iso 26262: Us-

ing machine learning safely in automotive software, arXiv preprint

arXiv:1709.02435.

62

[9] ISO Standard, Road vehicles — safety of the intended functionality, iSO/-

PAS 21448:2019(en) (2019).

[10] Aptiv, Audi, B. Apollo, BMW, Continental, Daimler, F. Group, Here,

Inﬁneon, Intel, Volkswagen, Safety First for Automated Driving (2019).

URL

https://www.daimler.com/innovation/case/autonomous/

safety-first-for-automated-driving-2.html

[11] H. Jing, Y. Gao, S. Shahbeigi, M. Dianati, Integrity monitoring of gnss/ins

based positioning systems for autonomous vehicles: State-of-the-art and

open challenges, IEEE Transactions on Intelligent Transportation Sys-

tems.

[12] O. A. Hafez, G. D. Arana, M. Joerger, M. Spenko, Quantifying robot local-

ization safety: A new integrity monitoring method for ﬁxed-lag smoothing,

IEEE Robotics and Automation Letters 5 (2) (2020) 3182–3189.

[13] V. Besnier, A. Bursuc, D. Picard, A. Briot, Triggering failures: Out-of-

distribution detection by learning from local adversarial attacks in se-

mantic segmentation, in: Proceedings of the IEEE/CVF International

Conference on Computer Vision, 2021, pp. 15701–15710.

[14] F. P. Preparata, G. Metze, R. T. Chien, On the connection assignment

problem of diagnosable systems, IEEE Transactions on Electronic Com-

puters (6) (1967) 848–854.

[15] LG, LGSVL Simulator.

URL https://www.lgsvlsimulator.com

[16] Baidu, Apollo Auto.

URL https://apollo.auto/

[17] S. Shalev-Shwartz, S. Shammah, A. Shashua, On a formal model of safe

and scalable self-driving cars, ArXiv abs/1708.06374.

63

[18] N. Kalra, S. M. Paddock, Driving to safety: How many miles of driving

would it take to demonstrate autonomous vehicle reliability?, Transporta-

tion Research Part A: Policy and Practice 94 (2016) 182–193.

[19] ISO Standard, Road vehicles – functional safety, iSO 26262-1:2011 (2011).

[20] P. Koopman, M. Wagner, Challenges in autonomous vehicle testing and

validation, SAE Int. J. Trans. Safety 4 (1).

[21] F. Concas, J. K. Nurminen, T. Mikkonen, S. Tarkoma, Validation Frame-

works for Self-Driving Vehicles: A Survey, Springer, 2021.

[22] P. Koopman, U. Ferrell, F. Fratrik, M. Wagner, A safety standard ap-

proach for fully autonomous vehicles, in:

International Conference on

Computer Safety, Reliability, and Security, Springer, 2019, pp. 326–332.

[23] Underwriters Laboratories, ANSI/UL 4600 Standard for Safety for the

Evaluation of Autonomous Products.

URL https://ul.org/UL4600

[24] F. Ingrand, Recent trends in formal validation and veriﬁcation of au-

tonomous robots software, in: 2019 Third IEEE International Conference

on Robotic Computing (IRC), 2019, pp. 321–328.

[25] A. Desai, T. Dreossi, S. Seshia, Combining model checking and runtime

veriﬁcation for safe robotics, in:

International Conference on Runtime

Veriﬁcation, Springer, 2017, pp. 172–189.

[26] B. Hoxha, G. Fainekos, Planning in dynamic environments through tem-

poral logic monitoring, in: AAAI Workshop: Planning for Hybrid Sys-

tems, Vol. 16, 2016, p. 12.

[27] C.-I. Vasile, J. Tumova, S. Karaman, C. Belta, D. Rus, Minimum-violation

scLTL motion planning for mobility-on-demand, in: IEEE Intl. Conf. on

Robotics and Automation (ICRA), 2017, pp. 1481–1488.

64

[28] S. Dathathri, R. Murray, Decomposing GR(1) games with singleton live-

ness guarantees for eﬃcient synthesis, arXiv abs/1709.07094.

[29] S. Ghosh, D. Sadigh, P. Nuzzo, V. Raman, A. Donzé, A. L. Sangiovanni-

Vincentelli, S. S. Sastry, S. A. Seshia, Diagnosis and repair for synthesis

from signal temporal logic speciﬁcations, in: Proceedings of the 19th In-

ternational Conference on Hybrid Systems: Computation and Control,

HSCC ’16, ACM, 2016, pp. 31–40.

[30] W. Li, L. Dworkin, S. A. Seshia, Mining assumptions for synthesis, in:

Ninth ACM/IEEE International Conference on Formal Methods and Mod-

els for Codesign (MEMPCODE2011), 2011, pp. 43–50.

[31] W. Li, D. Sadigh, S. Sastry, S. Seshia, Synthesis for human-in-the-loop

control systems, in: Intl. Conf. on Tools and Algorithms for the Construc-

tion and Analysis of Systems (TACAS), 2014.

[32] M. Kloetzer, C. Belta, A fully automated framework for control of linear

systems from temporal logic speciﬁcations, IEEE Trans. on Automatic

Control 53 (1) (2008) 287–297.

[33] S. Mitsch, K. Ghorbal, D. Vogelbacher, A. Platzer, Formal veriﬁcation

of obstacle avoidance and navigation of ground robots, The International

Journal of Robotics Research 36 (12) (2017) 1312–1340.

[34] N. Roohi, R. Kaur, J. Weimer, O. Sokolsky, I. Lee, Self-driving vehicle

veriﬁcation towards a benchmark, arXiv preprint arXiv:1806.08810.

[35] R. C. Cardoso, M. Farrell, M. Luckcuck, A. Ferrando, M. Fisher, Hetero-

geneous veriﬁcation of an autonomous curiosity rover (2020) 353–360.

[36] S. Jha, V. Raman, D. Sadigh, S. Seshia, Safe autonomy under percep-

tion uncertainty using chance-constrained temporal logic, Journal of Au-

tomated Reasoning 60 (2017) 43–62.

65

[37] F. Pasqualetti, F. Dörﬂer, F. Bullo, Attack detection and identiﬁcation in

cyber-physical systems, IEEE Transactions on Automatic Control 58 (11)

(2013) 2715–2729.

[38] M. Foughali, B. Berthomieu, S. Dal Zilio, P.-E. Hladik, F. Ingrand,

A. Mallet, Formal veriﬁcation of complex robotic systems on resource-

constrained platforms, in: 2018 IEEE/ACM 6th International FME Work-

shop on Formal Methods in Software Engineering (FormaliSE), 2018, pp.

2–9.

[39] S. Seshia, D. Sadigh, Towards veriﬁed artiﬁcial

intelligence, ArXiv

abs/1606.08514.

[40] M. Luckcuck, M. Farrell, L. A. Dennis, C. Dixon, M. Fisher, Formal spec-

iﬁcation and veriﬁcation of autonomous robotic systems: A survey, ACM

Computing Surveys (CSUR) 52 (5) (2019) 1–41.

[41] T. Dreossi, D. Fremont, S. Ghosh, E. Kim, H. Ravanbakhsh, M. Vazquez-

Chanlatte, S. Seshia, VERIFAI: A toolkit for the design and analysis of

artiﬁcial intelligence-based systems, ArXiv:1902.04245.

[42] D. J. Fremont, T. Dreossi, S. Ghosh, X. Yue, A. L. Sangiovanni-

Vincentelli, S. A. Seshia, Scenic: a language for scenario speciﬁcation and

scene generation, in: Proceedings of the 40th ACM SIGPLAN Conference

on Programming Language Design and Implementation, 2019, pp. 63–78.

[43] K. Leahy, E. Cristofalo, C. Vasile, A. Jones, E. Montijano, M. Schwager,

C. Belta, Control in belief space with temporal logic speciﬁcations using

vision-based localization, Intl. J. of Robotics Research 38.

[44] A. Balakrishnan, A. G. Puranic, X. Qin, A. Dokhanchi, J. V. Deshmukh,

H. Ben Amor, G. Fainekos, Specifying and evaluating quality metrics for

vision-based perception systems, in: Design, Automation Test in Europe

Conference Exhibition (DATE), 2019, pp. 1433–1438.

66

[45] A. Dokhanchi, H. B. Amor, J. Deshmukh, G. Fainekos, Evaluating per-

ception systems for autonomous vehicles using quality temporal logic, in:

Intl. Conf. on Runtime Veriﬁcation (RV), 2018.

[46] T. Dreossi, S. Ghosh, A. Sangiovanni-Vincentelli, S. Seshia, Systematic

testing of convolutional neural networks for autonomous driving, ArXiv

abs/1708.03309.

[47] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A. Chen,

K. Fu, Z. M. Mao, Adversarial sensor attack on lidar-based perception in

autonomous driving, in: Proceedings of the 2019 ACM SIGSAC conference

on computer and communications security, 2019, pp. 2267–2281.

[48] A. Boloor, K. Garimella, X. He, C. Gill, Y. Vorobeychik, X. Zhang, At-

tacking vision-based perception in end-to-end autonomous driving models,

Journal of Systems Architecture 110 (2020) 101766.

[49] H. Delecki, M. Itkina, B. Lange, R. Senanayake, M. J. Kochenderfer,

How do we fail? stress testing perception in autonomous vehicles, arXiv

preprint arXiv:2203.14155.

[50] N. Akhtar, A. Mian, Threat of adversarial attacks on deep learning in

computer vision: A survey, Ieee Access 6 (2018) 14410–14430.

[51] Q. M. Rahman, P. Corke, F. Dayoub, Run-time monitoring of machine

learning for robotic perception: A survey of emerging trends, IEEE Access

9 (2021) 20067–20075.

[52] D. Miller, P. Moghadam, M. Cox, M. Wildie, R. Jurdak, What’s in the

black box? the false negative mechanisms inside object detectors, arXiv

preprint arXiv:2203.07662.

[53] Q. M. Rahman, N. Sünderhauf, P. Corke, F. Dayoub, Fsnet: A failure

detection framework for semantic segmentation, Vol. 7, 2022, pp. 3030–

3037. doi:10.1109/LRA.2022.3143219.

67

[54] J. Lambert, J. Hays, Trust, but verify: Cross-modality fusion for hd map

change detection, in: Thirty-ﬁfth Conference on Neural Information Pro-

cessing Systems Datasets and Benchmarks Track (Round 2), 2021.

[55] J. Liu, J.-M. Park, “seeing is not always believing”: Detecting perception

error attacks against autonomous vehicles, IEEE Transactions on Depend-

able and Secure Computing 18 (5) (2021) 2209–2223.

[56] A. Sharma, N. Azizan, M. Pavone, Sketching curvature for eﬃcient out-

of-distribution detection for deep neural networks, in: Uncertainty in Ar-

tiﬁcial Intelligence, PMLR, 2021, pp. 1958–1967.

[57] C. You, Z. Hau, S. Demetriou, Temporal consistency checks to detect lidar

spooﬁng attacks on autonomous vehicle perception, in: Proceedings of the

1st Workshop on Security and Privacy for Mobile AI, 2021, pp. 13–18.

[58] A. Balakrishnan, J. Deshmukh, B. Hoxha, T. Yamaguchi, G. Fainekos,

Percemon: Online monitoring for perception systems, in: International

Conference on Runtime Veriﬁcation, Springer, 2021, pp. 297–308.

[59] D. Kang, D. Raghavan, P. Bailis, M. Zaharia, Model assertions for debug-

ging machine learning, in: NIPS, 2018.

[60] A. Santamaria-Navarro, R. Thakker, D. D. Fan, B. Morrell, A. akbar

Agha-mohammadi, Towards resilient autonomous navigation of drones

(2020). arXiv:2008.09679.

[61] B. Cai, L. Huang, M. Xie, Bayesian networks in fault diagnosis, IEEE

Transactions on industrial informatics 13 (5) (2017) 2227–2240.

[62] A. Abdollahi, K. R. Pattipati, A. Kodali, S. Singh, S. Zhang, P. B. Luh,

Probabilistic graphical models for fault diagnosis in complex systems,

in: Principles of Performance and Reliability Modeling and Evaluation,

Springer, 2016, pp. 109–139.

68

[63] Y. Lei, B. Yang, X. Jiang, F. Jia, N. Li, A. K. Nandi, Applications of

machine learning to machine fault diagnosis: A review and roadmap, Me-

chanical Systems and Signal Processing 138 (2020) 106587.

[64] X. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong, L. Akoglu,

A comprehensive survey on graph anomaly detection with deep learning,

IEEE Transactions on Knowledge and Data Engineering.

[65] J. De Kleer, B. C. Williams, Diagnosing multiple faults, Artiﬁcial intelli-

gence 32 (1) (1987) 97–130.

[66] S. L. Hakimi, A. T. Amin, Characterization of connection assignment of

diagnosable systems, IEEE Transactions on Computers 100 (1) (1974)

86–88.

[67] K. Bhat, Algorithms for ﬁnding diagnosability level and t-diagnosis in a

network of processors, in: Proceedings of the ACM’82 conference, 1982,

pp. 164–168.

[68] A. T. Dahbura, System-level diagnosis: A perspective for the third decade,

in: Concurrent Computations, Springer, 1988, pp. 411–434.

[69] M. Sampath, R. Sengupta, S. Lafortune, K. Sinnamohideen, D. Teneket-

zis, Diagnosability of discrete-event systems, IEEE Transactions on auto-

matic control 40 (9) (1995) 1555–1575.

[70] J. Zaytoon, S. Lafortune, Overview of fault diagnosis methods for discrete

event systems, Annual Reviews in Control 37 (2) (2013) 308–320.

[71] T. M. Tuxi, L. K. Carvalho, E. V. Nunes, A. E. da Cunha, Diagnosability

veriﬁcation using ltl model checking, Discrete Event Dynamic Systems

(2022) 1–35.

[72] P. Antonante, D. Spivak, L. Carlone, Monitoring and diagnosability of

perception systems, in: IEEE/RSJ Intl. Conf. on Intelligent Robots and

Systems (IROS), 2021, (pdf).

69

[73] H. Yang, L. Carlone, Certiﬁable outlier-robust geometric perception:

Exact semideﬁnite relaxations and scalable global optimization, arXiv

preprint arXiv: 2109.03349(pdf).

[74] J. Yang, M. Ward, J. Akhtar, The development of safety cases for an

autonomous vehicle: A comparative study on diﬀerent methods, Tech.

rep., SAE Technical Paper (2017).

[75] R. Yan, S. J. Dunnett, L. M. Jackson, Reliability modelling of automated

guided vehicles by the use of failure modes eﬀects and criticality analysis,

and fault tree analysis, in: 5th student conference on operational research

(SCOR 2016), Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.

[76] P. Antonante, D. Spivak, L. Carlone, Monitoring and diagnosability of

perception systems, arXiv preprint arXiv: 2011.07010(pdf).

[77] H. Yang, L. Carlone, One ring to rule them all: Certiﬁably robust

geometric perception with outliers, in: Conference on Neural Information

Processing Systems (NeurIPS), Vol. 33, 2020, pp. 18846–18859, (pdf).

URL

https://proceedings.neurips.cc/paper/2020/file/

da6ea77475918a3d83c7e49223d453cc-Paper.pdf

[78] H. Yang, L. Carlone, In perfect shape: Certiﬁably optimal 3D shape re-

construction from 2D landmarks, in: IEEE Conf. on Computer Vision and

Pattern Recognition (CVPR), 2020, arxiv version: 1911.11924, (pdf).

[79] H. Yang, L. Carlone, A polynomial-time solution for robust registration

with extreme outlier rates, in: Robotics: Science and Systems (RSS),

2019, (pdf), (video), (media), (media), (media).

[80] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,

I. Reid, J. Leonard, Past, present, and future of simultaneous localization

and mapping: Toward the robust-perception age, IEEE Trans. Robotics

32 (6) (2016) 1309–1332, arxiv preprint: 1606.05830, (pdf). doi:10.1109/
TRO.2016.2624754.

70

[81] H. Yang, J. Shi, L. Carlone, TEASER: Fast and Certiﬁable Point Cloud

Registration, IEEE Trans. Robotics 37 (2) (2020) 314–333, extended arXiv

version 2001.07715 (pdf).

[82] L. A. Wolsey, Integer programming, John Wiley & Sons, 2020.

[83] F. Rossi, P. Van Beek, T. Walsh, Handbook of constraint programming,

Elsevier, 2006.

[84] S. E. Shimony, Finding maps for belief networks is np-hard, Artiﬁcial

intelligence 68 (2) (1994) 399–410.

[85] S. Nowozin, C. H. Lampert, Structured learning and prediction in com-

puter vision, Vol. 6, Now publishers Inc, 2011.

[86] K. P. Murphy, Machine learning: a probabilistic perspective, MIT press,

2012.

[87] W. L. Hamilton, Graph representation learning, Synthesis Lectures on

Artiﬁcal Intelligence and Machine Learning 14 (3) (2020) 1–159.

[88] T. Kipf, M. Welling, Semi-supervised classiﬁcation with graph convolu-

tional networks, in: Intl. Conf. on Learning Representations (ICLR), 2017.

[89] Q. Li, Z. Han, X.-M. Wu, Deeper insights into graph convolutional net-

works for semi-supervised learning, in: Thirty-Second AAAI conference

on artiﬁcial intelligence, 2018.

[90] M. Chen, Z. Wei, Z. Huang, B. Ding, Y. Li, Simple and deep graph con-

volutional networks, in: International Conference on Machine Learning,

PMLR, 2020, pp. 1725–1735.

[91] W. H. L., R. Ying, J. Leskovec, Inductive representation learning on large

graphs, in: Advances in Neural Information Processing Systems (NIPS),

2017, p. 1025–1035.

71

[92] K. Xu, W. Hu, J. Leskovec, S. Jegelka, How powerful are graph neural

networks?, in: Intl. Conf. on Learning Representations (ICLR), 2019.

[93] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,

T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An im-

perative style, high-performance deep learning library, Advances in neural

information processing systems 32.

[94] A. Sengupta, A. T. Dahbura, On self-diagnosable multiprocessor systems:

diagnosis by the comparison approach, IEEE Transactions on Computers

41 (11) (1992) 1386–1396.

[95] Baidu, Apollo Auto.

URL https://github.com/ApolloAuto/apollo

[96] Z. Liu, Z. Wu, R. Tóth, SMOKE: Single-stage monocular 3d object de-

tection via keypoint estimation, arXiv preprint arXiv:2002.10111.

[97] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,

J. Guo, Y. Zhou, Y. Chai, B. Caine, et al., Scalability in perception

for autonomous driving: Waymo open dataset, in: Proceedings of the

IEEE/CVF Conference on Computer Vision and Pattern Recognition,

2020, pp. 2446–2454.

[98] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, O. Beijbom, Pointpillars:

Fast encoders for object detection from point clouds, in: Proceedings of

the IEEE/CVF Conference on Computer Vision and Pattern Recognition,

2019, pp. 12697–12705.

[99] Google’s self-driving startup Waymo is introducing fully driverless

rides

to San Francisco,

https://www.continental-automotive.

com/getattachment/5430d956-1ed7-464b-afa3-cd9cdc98ad63/
ARS408-21_datasheet_en_170707_V07.pdf.pdf, accessed: 2022-05-15.

72

[100] D. F. Crouse, On implementing 2d rectangular assignment algorithms,

IEEE Transactions on Aerospace and Electronic Systems 52 (4) (2016)

1679–1696.

[101] Google, Google OR-Tools.

URL https://developers.google.com/optimization

[102] Grante Library for Inference and Estimation on Discrete Factor Graph

Model, http://www.nowozin.net/sebastian/grante/, accessed: 2022-
05-15.

[103] W. Falcon,

et

al.,

Pytorch

lightning,

https://github.com/

PytorchLightning/pytorch-lightning (2019).

[104] L. N. Smith, Cyclical learning rates for training neural networks,

in:

2017 IEEE winter conference on applications of computer vision (WACV),

IEEE, 2017, pp. 464–472.

[105] The Guardian, Tesla driver dies

in ﬁrst

fatal crash while us-

ing autopilot mode,

www.theguardian.com/technology/2016/jun/
30/tesla-autopilot-death-self-driving-car-elon-musk, accessed:
2022-05-15 (2016).

73

