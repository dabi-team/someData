2
2
0
2

y
a
M
9
2

]

Y
C
.
s
c
[

1
v
1
0
6
4
1
.
5
0
2
2
:
v
i
X
r
a

YASM (Yet Another Surveillance Mechanism)

Kaspar Rosager Ludvigsen1, Shishir Nagaraja2, and Angela Daly3

1Department of Computer and Information Sciences, University of Strathclyde,
kaspar.rosager-ludvigsen@strath.ac.uk
2Department of Computer and Information Sciences, University of Strathclyde,
shishir.nagaraja@strath.ac.uk
3Leverhulme Research Centre for Forensic Science and Dundee Law School,
adaly001@dundee.ac.uk

May 31, 2022

Abstract

Many types of surveillance exist on anything from smart-
phones to IoT devices, but most of them are not as ubiqui-
tous and intrusive as Client Side Scanning (CSS) for Child
Sexual Abuse Material Detection (CSAMD). Apple pro-
posed to scan their software and hardware for such im-
agery. While CSAMD was since pushed back, the Euro-
pean Union has decided to propose forced CSS to combat
and prevent child sexual abuse via a new regulation, and
deliberately weaken encryption on all messaging services.
CSS represents mass surveillance of personal property, in
this case pictures and text, proposed by Apple without
proper consideration of privacy, cybersecurity and legal
consequences. We ﬁrst argue why CSS should be lim-
ited or not used at all, and brieﬂy discuss some clear is-
sues with the way pictures cryptographically are handled
and how the CSAMD claims to preserve privacy. After-
wards, in the second part, we analyse the possible human
rights violations which CSS in general can cause within
the regime of the European Convention on Human Rights.
The focus is the harm which the system may cause to indi-
viduals, and we also comment on the proposed European
Union Regulation. We ﬁnd that CSS by itself is prob-
lematic because they can rarely fulﬁl the purposes which
they are built for. This comes to down to how software is

not “perfect”, as seen with antivirus software. Secondar-
ily, the costs for attempting to solve issues such as CSAM
far outweigh the beneﬁts, and this is not likely to change
regardless of how the technology develops. We further-
more ﬁnd the CSAMD as proposed is not likely to pre-
serve the privacy or security in the way of which it is de-
scribed in Apple’s own materials. We also ﬁnd that the
CSAMD system and CSS in general would likely violate
the Right to a Fair Trial, Right to Privacy and Freedom of
Expression. This is because the pictures could have been
obtained in a way that could make any trial against a le-
gitimate perpetrator inadmissible or violate their right for
a fair trial, the lack of any safeguards to protect privacy
on national legal level, which would violate the Right for
Privacy, and it is unclear if the kind of scanning which
would be done here could pass the legal test which Free-
dom of Expression requires, making it likely violate this
as well. Finally, we ﬁnd signiﬁcant issues with the pro-
posed Child Abuse Regulation. This is because it relies
on techno-solutionist arguments without substance, disre-
gards conventional knowledge on cybersecurity and does
not justify the independence and power of a “centre” to
help solve the problem.

 
 
 
 
 
 
1 Introduction

Mass surveillance has been around for long [72], and and
a modern way to commit and complete the task now ex-
ists through digital mass surveillance [65], but recklessly
implementing and using this without proper cybersecu-
rity (security) considerations can have devastating con-
sequences, like the destruction of privacy through back-
doors which criminals can use as easily as the state, or
signiﬁcant economic consequences [2]. Furthermore, us-
ing these digital tools do not seem to decrease complexity
for the authorities [22]. Opponents argue that surveillance
is necessary and beneﬁts from secrecy [34], while basic
cybersecurity engineering practices speak directly against
this [54, 3]. With the advent of a fully digital society [30],
surveillance is conducted on almost all hardware and soft-
ware, and this increased during the COVID-19 crisis via
digital health surveillance [57, 50, 40].

Our focus of this paper is Client-side scanning (CSS),
which is an example of a digital tool used for surveillance.
The cybersecurity community broadly distinguishes be-
tween two types of scanning; server-side and CSS. CSS
used on individuals has in particular been heavily criti-
cized [1]. Most current surveillance is done in the for-
mer format, while existing software such as antivirus
programs do the latter. Furthermore, CSS analyses the
content before it is encrypted, unlike server-side, which
creates many additional cybersecurity risks and potential
failures. CSS therefore adds a whole new level of surveil-
lance; these systems track everything in a given area real-
time. Analogy wise, it will be like going from an agent
occasionally tracking you and your house, to the agent
living in your house and constantly going through your
belongings. Types of malware have and still use what is
essentially CSS to achieve their objectives [44]. Along-
side this, the enabling of digital surveillance is continu-
ously expanded on a worldwide scale. T

his can be seen in a country like the UK, through the sti-
ﬂing of protests, inability to sue the state [61] and disable
certain groups from participating in a democratic society.
Whether or not these measures become reality matters lit-
tle, when the purpose of them clearly is to increase the
opportunities to do surveillance and decrease the amount
of rights for individuals. Such acts has been met with in-
creased usage of encryption and other evasive measures,
by everything from “unwanted parties” [63] to dissidents,

making national issues into international. But having en-
cryption seems to be worth far more than not having it [2,
41].

In this paper, we analyse whether CSS will violate hu-
man rights through the European Convention on Human
Rights (ECHR) [21]. In it, The Right to Remain Silent
and Not Incriminate Oneself can lead to failed prosecu-
tions of potential criminals and a widespread waste of re-
sources if violated. Surveillance systems are well known
to violate rights, but CSS present systems which will do
this routinely or constantly, which is why we ﬁnd them
to be dangerous and cannot justify by the goals they aim
to serve. The same applies to antivirus, which can never
fully eradicate all malware [44].

CSS will do this and also likely violate a myriad of hu-
man rights. CSS will likely be used regardless, which is
why this paper includes comments on both technical and
human rights issues. Usage of digital mass surveillance
requires access to systems, done through lawful autho-
risation or outright illegal means [37]. We have seen a
move to large corporations performing surveillance in the
platform economy, which either enables them to act as
gatekeepers or collaborators with national states [31], or
lets them act entirely for their own beneﬁt [70]. Both in-
clude snooping on sounds, intercepting messages on the
software they own and run, infer or read location data and
access to personal ﬁles or pictures. These mirror the past
types of physical surveillance.

CSS can be exempliﬁed by Apple’s Child Sexual Abuse
Material detection system (CSAMD). This system is post-
poned, but this does not make the discussion of it and fu-
ture similar systems any less important.
It would have
applied to all IPhones and ICloud. Apple is not the only
company to have such a system, but they are, as of the
time of writing, the only company to have publicly re-
leased proofs and system summaries [10]. CSAMD was
created to ﬁght CSAM, which is broadly considered a
crime in a majority of jurisdictions [45]. Fighting such
material is vital, but this is usually left to authorities of
individual national states or cooperation in legal entities
such as the European Union, Europol or Interpol. Out-
side of criminal law, mental health surveillance is fought
through apps [19] or through traditional means of data
gathering [29], but CSS goes beyond these. While the
spread of CSAM is a major issue, preventing or mitigat-
ing the actions leading to the material being created is

much more vital [5], and is not something any private en-
tity can ever do by itself. An additional danger which CSS
brings in the ﬁeld of CSAM detection and management,
is to prematurely exhaust police resources, analogous to
the damage which sensor-based monitoring systems did
to chemical plants in the past [39, 32].

Regrettably, there is no doubt that states in the future
will use CSS systems that apply the same kind of tech-
niques as the CSAMD to other situations. This will likely
not contribute to more security or more privacy[1], and
is not a new or unexpected development [53, 26, 59]
and have been used by adversaries in various forms for
decades through malware [64, 49]. The European Union
has joined this list, with its new proposal for legislation,
which frames CSS as the solution to all problems regard-
ing child sexual abuse, and may mandate backdoors in
all encryption on all communication platforms going for-
ward, see Section 5 on page 10. CSS and data protection
and privacy is a known angle [15], but research on the
rights of individuals is needed. Since CSS will affect al-
most everyone, we chose to focus on human rights as they
are laid out via the ECHR, as it covers the EU and more.

The paper is as follows: Section 2 includes arguments
against CSS, section 3 deﬁnes terms used in the paper,
section 4 analyses human rights, with the case being
ECHR, speciﬁcally Article 6, 8, 10 and 11. Section 5 dis-
cusses the proposed Regulation, section 6 includes future
considerations and section 7 is the conclusion.

We ﬁnd that CSS has serious issues on a fundamental
level, which cannot be solved since they exist because of
their designs. Furthermore, we ﬁnd that Apple’s CSAMD
was inadequate both in terms of its privacy preserving
functions, NeuralHashing and even in its premise. It will
lead to future CSS systems which can use other datasets
and for other purposes, such as catching dissidents or
protesters. To that extent, we ﬁnd that CSS systems will
violate several rights within the European Convention of
Human Rights, but our analysis is not exhaustive. They
will likely violate the Right to a Fair Trial, in particular
the Right to Remain Silent and Not Incriminate Oneself,
Right to Privacy, and if implemented further than current
examples, Freedom of Assembly and Association as well.

2 CSS as an Impossibility

It is a widely held belief that CSS with the right safe-
guards and privacy enhancing technologies can be done
“properly”, while others have concluded that they can
never be secure [1, p. 34] or justiﬁable in their current
form [53], and calling CSS to be impossible is not far
off. Not impossible in its practical implementation, but
the impossibility to genuinely and rationally live up to its
own spirit. We ﬁrst present a simple proof to make that
point, then we continue will an alternative way to show
the same.

2.1 Proof 1

We make two assumptions: 1. Assume that there exists a
perfect CSS technique and that this is implemented into a
system, and it could, e.g., be used to detect CSAM.

2. For CSAM, there is no perfect deﬁnition of CSAM
which can be expressed as a general rule to identify it as
a type of image. There exists legal deﬁnitions, but these
do not constitute the rules which a CSS system can make
decisions from, such as Article 34 in the United Nations
Convention on the Rights of the Child 1989 and the Con-
vention on the Protection of Children against Sexual Ex-
ploitation and Sexual Abuse. An adversary will upload
an image which is scanned by the CSS. If the CSS de-
tects that this is CSAM, the adversary will delete the im-
age, and discontinue their usage of the service. By doing
so, the CSS will never detect any CSAM from this ad-
versary ever again, and therefore become redundant. This
means, that no matter how perfect you make the detec-
tion of CSAM or other changes, the CSS is not capable of
fully fulﬁlling its purposes, as any adversary can entirely
circumvent its entire existence by using other avenues.

Assumption 1 has the same ﬂaw as the idea of a ’perfect

antivirus program’ [44] [16].

Beyond CSAM, the same circumvention is possible.
Because of this, CSS will in most cases not be able to
fulﬁll the purposes for which it was created for, and at
best fails, but will at worst be created and continue to
function for entirely different purposes like surveillance-
capitalism.

2.2 Proof 2

3 Client-side Scanning

We make the following assumptions about such systems:
1. The aims of a system must be able to reach their goals,
idealised or not.

CSS is on-device analysis of data [1], which is different
from analysis from information gathered elsewhere. We
will go through one example of CSS.

2. The proposed goals must not conﬂict directly with

the capabilities of a system.

3.1 CSAMD

3. There is no such thing as a perfect defence.

4. The risk of circumvention cannot be fully mitigated.

These three assumptions cover purposes for the sys-
tem, lack of justiﬁcations for its goals and therefore its
existence, and its impossibility. CSS contains in its very
notion constant surveillance upon the system, and unlike
pure logging, attempts to oversee all events within a given
framework. This makes it very similar to software like
antivirus, which we cannot be “perfect” [14, 44, 16] as
the deﬁnition of malicious software can never deﬁne all
the types in existence. CSS is also malicious against the
user [1]. Therefore, CSS is impossible because its exis-
tence is adversarial to the user, regardless of whether they
violate what the CSS searches for or not, as CSS by its
existence creates a security and a human rights’ hazard.
Two wrongs do not make a right. Essentially, the idea of
CSS as a positive or constructive force does not exist be-
cause of its function. The goal of these systems is usually
to aid in investigations or otherwise provide information,
but by doing so, CSS systems inherently violate their own
goals. They commit to surveillance, which can be mis-
used by everyone that can guess keys or otherwise attack
the system, creating the same problems as they are trying
to solve. The goals of any CSS will therefore be in con-
ﬂict with its capabilities at all times. Furthermore, there
is no way to guarantee against misuse and therefore viola-
tion of the very purpose of the systems. One must assume
that there will be a certain risk of adversarial failures, and
that there is no way to ever mitigate everything. If there
is always a risk, constantly scanning what is done live on
a system by itself means that CSS acts more like a weak-
ness, than in does as a tool to fulﬁll its goals. All three
arguments combined show that the held beliefs and argu-
ments for CSS do not hold up when faced with the aims,
goals and capabilities of the system. This will not change
going forward, regardless of technical advancements.

Apple documents its proposed system in a variety of on-
line documents [4, 10, 8]. There are direct issues with
how it preserves privacy. The system does this through
hashing and PSI, and theoretically only lets humans-in-
the-loop access them after certain measures. But not only
is this construction vulnerable to adversarial attackers, the
way they “identify” CSAM could lead to scores of false
positives or other issues. Since the subject of CSAM is
grave, it is prudent to make a safe and secure system
that does not potentially label thousands of individuals for
possessing materials they do not in reality.

A less overarching but more detailed analysis of the
system has been done elsewhere [48], its NeuralHash im-
age tool has been reverse engineered several times [66],
and another image tool akin to it has also been reverse
engineered and found vulnerable [7].

The proposed detection system is based on a range of
techniques: private set intersection (PSI) [52], Cockoo ta-
bles [46], Secret sharing, Difﬁe-Helman problems [47]
and hardness, Naor-Reingold Difﬁe-Hellman [43] random
self reducibility, Coppersmith and Sudan Algorithm, In-
terleaved Reed-Solomon code under random noise and tu-
ples. The idea of private set intersection refers a method
primary set theory properties like A ∪ B.

The CSAMD uses secret sharing to ﬁnd this intersec-
tion, which is done between the user, Apple and the sys-
tem. It is evident that letting Apple control the system tool
which one relies on for secret sharing is problematic. You
do not want to share the real intersection, so you will use
A′ ∪ B′ instead.

The CSAMD uses Shamir Secret Sharing, which means
that the secret is an element of a ﬁnite ﬁeld (s ∈ F) , of
which a polynomial with a certain degree is picked. Be-
cause of the qualities of the ﬁnite ﬁeld, shares of the se-
cret can be created which rely on the polynomial, and then
there exists a threshold for which the polynomial can be
guessed.

But if the system cannot get the necessary amount of
shares to reach the threshold, only the user and Apple will
know the secret. In the CSAMD, this is supposedly fur-
ther modiﬁed so that not even the system nor Apple will
know through synthetic shares, which are periodically up-
loaded by the user [4]. But will the system know when it
is handling synthetic or real matches? As the data sent
by the users is put into cuckoo tables to hash the data.
Cuckoo hashing uses two functions with a set size and
two tables, where the key (in this case) can exist in both
tables but never both at the same time [46]. Each function
will be sent every time, regardless of whether the key is in
one or the other. In this context, Difﬁe-Hellman hardness
is used to assert how negligible the success of an adver-
sarial attacker in guessing different elements of the tuple
and a property of the tuples is, and Difﬁe-Hellman ran-
dom self-reducibility is a means to use the fact that Difﬁe-
Hellman tuples are not random tuples [48]. The latter is
a core part of the creation of the key mentioned above.
The shares (central to PSI) which are correct among the
synthetic ones can be detected with the interleaved Reed-
Solomon under random noise algorithm [11, 48], which
relies on the relationship between the values, and these
are kept under control by always including a version of
the picture in lower resolution, limiting the size to one
which the algorithm satisﬁes its parameters.

3.1.1 Safety Comments and Criticism

There has been public criticism by prominent authors of
the system [42], explicitly of NeuralHash [66] and similar
future schemes [1].

Some developers [42] still supports this type of surveil-
lance, but such systems have similar human rights issues
as below.

CSAMD may fail to detect CSAM, because it is not
within its known data set, adversary abuses NeuralHash-
ing or by other measures. This could be embedded within
a picture. This can be done both to abuse the system (to
i.e.
target individuals) or to hide CSAM as such [66].
Assumption 2 from Proof 1 still applies here, there is no
perfect way to identify and otherwise always ﬁnd CSAM.
Reversely, any machine or other learning can therefore
be abused or circumvented. CSAMD is a system that is
likely made for the beneﬁt of Apple, regardless of its oth-
erwise altruistic intentions that clash with existing views

[55]. There a very real risk for false positives with poten-
tial consequences for the physical person targeted, but its
choice of PETs is questionable and the risk of many types
of adversarial failures is great.

Struppek et al.

[66] present compelling arguments
which show that NeuralHash is not ﬁt for purpose. 90
to 100 percent success rate [66] of an adversarial attack,
show that NeuralHash is not robust against simple image
processing software. This may indicate problems with
deep perceptual hashing employed in this manner in gen-
eral, and not just one images. Struppek et al. also showed
that NeuralHash leaks from its classiﬁers, which is not
unexpected, but further justiﬁes not using any technique
similar to NeuralHash when handling potentially sensitive
or private information, as many adversaries will be able to
infer information or perhaps more without having access
to the entire image. The use of Neural Networks will al-
ways enable the risk of the attacks mentioned by Struppek
et al., which means they should not be used until suitable
defences are found. We agree, but philosophical argu-
ments are not built around conclusive statements, “not ﬁt
for purpose” does not argue strongly enough against using
such systems when they seem this unﬁt. Future authors
will give more cohesive and rigorous arguments as to why
CSS can be unethical within deﬁned ethical frameworks.
A key to this is cause and effect, which through the lens
of applied ethics like consequentialism [12] makes CSS
hard to justify. The increased surveillance must equally
increase safety, and must cause more efﬁcient sanctioning
and perhaps increased justice, but none of this is likely to
happen [1].

Finally, there is no point in discussing “perfect” pri-
vacy, if the supposed materials are later de-encrypted and
then decided on by a human in the loop who is not part
of law enforcement of any state. This is regardless of the
supposed safeguards which NGOs [69] and other struc-
tures that may include specialists, as they will still not be
faced with public ethical and legal rules of conduct or en-
joy the legitimacy of being hired by the state in question.

4 Human and Constitutional Rights

CSS systems bring up human rights issues. Privacy vio-
lations seem clear from its inception and contextual pri-
vacy and clearer consent rules will not solve these is-

sues [24]. The potential which systems inspired by the
CSAMD have in the future present a violations of other
rights.

To some, questions of human rights or rights derived
from constitutions seem unrealistic, abstract, or perhaps
even redundant. Unrealistic, because the individual or
even a given company may never reach the procedural
point where it must be proven or refuted in court [33].
Abstract, because the concepts seemingly have nothing to
do with the private or professional lives of those that have
this belief, but this is misleading [73]. And redundant,
because it may seem like those rights are built in or part
of the human nature, which does not seem to be so [20].
The last century gave us the basis of these rights because
of the atrocities that were committed to prevent further
deprivation of these rights, even with cultural differences
[13].

The ECHR is an instrument was created by the Coun-
cil of Europe, which includes EU Member States, and
additional countries. The EU has the Charter of Funda-
mental Rights of the European Union, which uses and ex-
pands the ECHR. The ECHR exists above national legal
systems, which is necessary for it to function [9]. For a
citizen in these countries, the European Court of Human
Rights (ECtHR) is the last resort. Before going to the EC-
tHR, it is up to the national judges, civil or otherwise, to
apply it properly. Not every case can be appealed to the
ECtHR, as its case law has already answered the common
questions. It is within this substance that judges and the
systems as such must make these rights into reality. Hu-
man rights exist at all times, but can only be negatively or
positively accessed in court. “Negatively refers” to what
should not be done or inaction, and “positively” refers to
what has to explicitly be done or acted upon. Any com-
ments we make or analysis of how the CSAMD system
may work, are in the context of hypothetical issues in
the future. This does not mean it will only apply to the
CSAMD, it will apply to any kind of automated mass per-
sonal data CSS analysis tool that any state may use in the
future via function creep [35].

The following section is therefore a direct analysis as
to when CSS systems can breach the ECHR and how. Our
analysis cannot amount to a justiﬁcation as to whether
they should ever be used for these purposes, but serves an
evaluation of legality of the systems within each article.

4.1 Right to a Fair Trial

CSS may impact the Right to a Fair Trial (Article 6), if
evidence gathered by CSS is used in court or during in-
vestigation. Many content moderating companies have in-
ternal or external measures to share with law authorities,
but this may not be clear in all jurisdictions, and regard-
ing CSAMD, Apple has not made it clear how it would
complete its duties on a world wide scale. Article 6 ap-
plies if the individual is “charged with a criminal offense”.
Charged does not only refer to when criminal proceedings
have begun, but also before or when the suspicion is in-
ternal.1

4.1.1 The Right to Remain Silent and Not Incrimi-

nate Oneself

The Right to Remain Silent and Not Incriminate Oneself
is not mentioned explicitly in Article 6, but is accepted
through case law2.

The right implies that suspects have the right not to
speak and not give information which would incriminate
them, and applies outside of criminal law too [28]. The
negative limit is to prevent evidence obtained through co-
ercion or oppression3 from being used. The exception
is where the evidence is obtained through compulsory
powers (e.g., lawful authorisation), of which the evidence
must have an independent existence of the subject (e.g.
blood).

CSAMD makes use of pictures which do not have an
independent existence of the subject. Whether this applies
to the NeuralHash which CSAMD has set up to not have
a human in the loop until later, remains to be seen, and
cannot be answered from existing verdicts.

Many jurisdictions likely violate self incrimination
through current practice like the analysis of social me-
dia posts of potential refugees in the application process
[38]. Pictures which are synonymous with private prop-
erty would also ﬁt this category, denying its independent
existence of the subject.

If lawful authorisations are not systematically given ac-
cess CSS systems, a test must be done by national courts
to consider whether the individual retains the right:

131816/08, Stirmanov v. Russia, § 39.
210828/84, Funke v. France, § 44.
319187/91, Saunders v. the United Kingdom, § 68 - 69

The nature and the degree of the compulsion, the exis-
tence of relevant safeguards in the procedure, and the use
of the obtained material.

It is unlikely that the retrieved ﬁles or data can be the
sole evidence using to convict the individual, regardless
of the kind of comprehensive scanning client-side based
systems can do or jurisdiction.

If used, there must exist specialised legal and practical
safeguards, which has yet to materialise in any members
of the Council of Europe. Legal and practical safeguards
refer to CSS being used fairly, a explicit public versus in-
dividual interest evaluation, but it cannot be the argument
for why the Right to Remain Silent and Not Incriminate
Oneself must be extinguished4 which has been ﬁrmly es-
tablished by the ECtHR in various case law5.

Essentially, public interest cannot extinguish the very
essence of the right. CSS systems will likely require
costly national implementation to not violate The Right to
Remain Silent and Not Incriminate Oneself. CSAMD and
similar systems have, as of the time of writing, not done
so, and the proposed Regulation we brieﬂy look at later
does not change this. Following this, we include further
details within Article 6.

4.1.2 Admissibility

The evidence must be admissible, which refers to whether
evidence can be used at a trial or not. Some states have
rules against evidence obtained wrongfully or by other
means of evaluation, which can lead to situations where
the ﬁles or just the detection of them cannot be used in
proceedings [68]. The Convention does not consider these
aspects, as these must be done in national law and by
the relevant court6, but it evaluates whether the case was
“fair” or not. The fairness test is individual and must be
done entirely on a trial to trial basis.

If

the case is not deemed fair because of non-
admissibility, some jurisdictions will close the current
case, and others will continue but very likely lead to the
accused being found not guilty.

454810/00, Jalloh v. Germany, § 97.
534720/97, Case of Heaney and McGuiness v. Ireland, §§ 57 - 58.
628490/95, Hulki G¨unes v. Turkey.

4.1.3 Planted Evidence and Entrapment

Relating explicitly to the Right to a Fair Trial, a trial can
never be fair if the accused cannot question the evidence,
which will be very difﬁcult if relying on CSS or black
box systems. This is due to the nature of the scanning
constantly on the device of the accused, and because of
the hidden or deliberately complicated or obfuscated ap-
proach to the coding.

A trial cannot be fair either, if the evidence used was
planted7. This can be done by an adversary, who sends
the accused a ﬁle containing something that triggers the
scanning. Regarding CSAMD, steganography [67] is not
likely to be able to pass NeuralHash, but for future CSS
systems this may not case. Normally planted evidence
refers to it being planted by the state, but this deﬁnition is
expanded because of the increase possibilities of interfer-
ence by third parties that have no physical access to the
accused. Lastly, a trial cannot be fair if the evidence is
obtained through unlawful secret surveillance8, which as
a general rule will make such evidence inadmissible.

Entrapment is worth mentioning in the same vein.
This refers to various means to either frame or otherwise
through “traps” to ﬁnds reasons to investigated individu-
als9. This could very well be done with CSAMD or sim-
ilar systems, with the exact same techniques as an adver-
sarial attacker. Entrapment has happened in many states
of the Council of Europe, including Croatia10, Germany11
Lithuania12, United Kingdom13 and more.

4.2 Right to Respect for Private and Family
Life, Home and Correspondence

Article 8 has to do with the integrity and status of one’s
private pictures. It includes the protection of privacy14.
In the case law of the ECtHR, privacy is either protected
positively or made to protect against the state negatively,
but there exists a right for the individual to be protected

722062/07, Layijov v. Azerbaijan, § 64.
835394/97, Khan v. the United Kingdom, § 34.
959696/00, Khudobin v. Russia, § 128.
1047074/12, Grba v. Croatia.
1140495/15, Akbay and Others v. Germany.
1274420/01, Ramanauskas v. Lithuania.
1367537/01, Shannon v. the United Kingdom
1440660/08 and 60641/08, Von Hannover v. Germany (no. 2), § 95.

against other private parties15.

This right for protection against private parties is a
recognition of their new role in infringing privacy system-
atically. This allows Article 8 to be applied horizontally
under certain circumstances, which enables individuals to
sue to parties which are not just the state, or at the very
minimum sue the state over its lack of action against the
private parties.

We see that the Right to Privacy could potentially ren-
der the use of CSS illegal within national states. This
comes down to the wording of Article 8, which in part
1, sets out the right and is qualiﬁed in part 2. The Right
to Privacy is therefore not an absolute right, but instead
relies on the entirety of Article 8. Interfering in privacy
requires a legitimate aim, and “ﬁghting” CSAM would ﬁt
under “prevention of order or crime”, “for the protection
of health and morals” and “for the protection of the rights
and freedoms of others”, usually not an issue to prove in
court16, but other CSS systems might not pass. The lat-
ter could be because of their purpose not being related to
criminal investigations.

The other test from part 2 of Article 8 however, is
whether violating the right is “necessary in a democratic
society”. This is by the court interpreted directly into
“pressing social need”, which the interference must jus-
tify precisely, and it must be proportionate to the problem
which needs to be solved17. Apple would then need to jus-
tify the surveillance through these rules, and for CSAMD,
this has not been done adequately. There is no documen-
tation that proves or makes it likely that there exists the
huge amount of CSAM which would justify CSS on all
devices, as the measure would only be proportionate if
the pressing social need warranted the breach of privacy.
There is a chance that such documentation can be brought
to light in court proceedings.

The same can be said about CSS for location data, e.g.
contact tracing applications [71], which so far would not
be able to justify its existence via part 2 either. Note that
this can be done without invoking data protection legis-
lation. This does not mean that the surveillance will be
criminalized, but practice indicates that it could be possi-

1561496/08, B˘arbulescu v. Romania [GC], §§ 112 - 112, to be under-

stood in an expanded manner.

1643835/11, S.A.S. v. France [GC], § 114
1720071/07, Piechowicz v. Poland, § 212

ble18, even if civil litigation is more likely19.

Like many types of existing surveillance, the ECtHR
is not willing to directly require the removal of the tech-
niques, most clearly seen with the techniques of entrap-
ment from Section 4.1.3 earlier, which are allowed nar-
rowly20.

4.2.1 Violation by Existence

Questions could then arise as to how such surveillance
systems could violate privacy of individuals by merely
existing, such as through the medium. Depictions and
pictures of individuals are inherently protected by Article
821.

This means that

the individual must be protected
against state and other private actors intruding on this
right22.

This is not absolute, and no case law that relates di-
rectly to the protection outright without any criterion23
exists. There is no case law which guarantees that any
surveillance could be justiﬁed if sufﬁciently “privacy en-
hanced”24, as the court leaves no room for exceptions if
the surveillance is disproportionate and violates any de-
gree of appreciation the state had or fails the tests above.
We narrowly interpret this as Article 8 never allowing
privacy enhancing technologies to justify the measures.
CSS systems like the CSAMD must therefore be analysed
on the basis of protecting the depiction of the individual,
since this is the subject of the surveillance.

The question then becomes how far it reaches. Gen-
erally, the state must positively protect this right through
criminal or civil law provisions25. It could mean that to
use CSS, there should be legal safeguards that force Ap-
ple or other companies to use state of the art encryption
and demand local representatives that act as humans in the
loop in their system. Other authors have suggested similar
policy proposals [1].

1838435/13, B.V. and Others v. Croatia, § 151.
1925163/08, 2681/10 and 71872/13, Noveski v. the former Yugoslav

Republic of Macedonia, § 61.

2074420/01, Ramanauskas v. Lithuania [GC], § 51
211874/13 and 8567/13, L´opez Ribalda and Others v. Spain [GC], §§

87-91.

2240660/08 and 60641/08, Von Hannover v. Germany, §§ 50-53.
2318068/11, Dupate v. Latvia, §§ 49-76.
2430562/04 and 30566/04, S. and Marper v. U.K, § 125.
255786/08, S¨oderman v. Sweden.

4.2.2 Defamation

The protection of individual reputation is also included
in Article 8. An attack of a certain level of seriousness
must have occurred on the individual and it must have
harmed the personal enjoyment for the right to respect pri-
vate life26. This could be the planting of CSAM or false
positives.

If CSS systems ends up causing loss of reputa-
tion through the media or through companies/authorities
themselves accusing the user, Article 8 can apply. The
most important concept in this aspect is whether the loss
of reputation was caused by user’s foreseeable actions27,
or if the loss of reputation was caused by a criminal con-
viction, which the court does not accept28.

From case law, this includes any part of the chain, and
countries like the UK will allow this to be the basis for
tort law or reimbursement law, with the company or state
responsible as the potentially liable party [27].

4.2.3 State Surveillance

Because CSS systems may be part of secret state surveil-
lance systems, similar safeguards as normal surveillance
must be given29. These should include appeal processes
and right to access. This is outside of what would be given
through data protection rules, as access to it is required to
receive the process of a fair trial. In an EU context, such
data would not be covered by the GDPR, but instead the
Law Enforcement Directive, which limits the potential in-
formation which the individual would be able to obtain to
a limited amount, and which requires the surveillance to
fulﬁll a range of criteria, including authorisation [51].

ECtHR practice shows that state surveillance via CSS
without safeguards would be in violation of Article 830,
but there is one issue with this approach. Getting a case
before a national court or the ECtHR requires that the
state surveillance is perceived or discovered.

up it is up the surveillance chain. This is difﬁcult, as it
involves discovering and understanding the CSS which is
embedded in systems used on a daily basis by those under
surveillance.

4.3 Freedom of Expression

Article 10 applies to any medium31. CSS systems like
the CSAMD will not immediately impact the Freedom of
Expression of any individual. Derived (chilling) effects
seen in other types of surveillance systems will regard-
less of the intention lead to diminished rights, especially
freedom of speech [58, 62] or theoretically any type of
freedom [56].

4.3.1 The three “tests”

Like the other ECHR rights discussed, it is made in a pos-
itive manner. As the exception, there exists a test which
the courts have to take regarding situations where Free-
dom of Speech can be suppressed [25]. This consists of
three parts, which must be cumulatively fulﬁlled for the
state to be able to justify violating Freedom of Speech:
Lawfulness of the interference (1), legitimacy of the aim
pursued by the interference (2), necessity of the interfer-
ence in a democratic society (3). Failure at any stage will
make the surveillance be in violation with Article 10.

1. Lawfulness of the surveillance system in interfer-
ing Freedom of Expression is attained with positively de-
scribing and implementing it in law, with the latter requir-
ing precise wording and literal usage and meaning of the
text32. The Court recognises that technology changes, and
that wording can be made to be vague, but usage matters,
and in this sense “what” the surveillance actually does33.
This catches the issue of lawfulness, that it must be fore-
seeable for the user or individual whose Freedom of Ex-
pression is infringed.

If the information provided by the client side scanning
systems is the source of the secret surveillance, discover-
ing it will be much harder because of how much further

Future systems require proper implementation into law
to not threaten Freedom of Expression through picture
recognition of people and so on. This rules out any kind of

2676639/11, Denisov v. Ukraine, § 112.
2725527/13, Vicent Del Campo v. Spain
2876639/11, Denisov v. Ukraine [GC], § 98.
295029/71, Klass and Others v. Germany, § 36.
304647/98, Peck v. the United Kingdom, § 59.

3110572/83, Markt intern Verlag GmbH and Klaus Beermann v. Ger-

many, § 26.

3224973/15, Cangi v. Turkey, §§ 39 and 42.
3321279/02 and 36448/02, Lindon, Otchakovsky-Laurens and July v.

France, § 41.

private surveillance, which states would then have to pro-
tect its citizens against, making private versions of such
systems illegal. It is not foreseeable for the individual that
all their storage or other digital devices suddenly become
live CSS systems, at least not within current case law of
the ECtHR.

2. The surveillance must pursue a legitimate aim. This
can be passed depending on the lack of constitutional pro-
tection of the system, or through popular movements, but
they must be clear34.

3. The third test is the most divisive, and the case law
for this decides what should be done, not the wording as
such. There must be a “pressing social need”. Pressing
here refers to an actual need, and may not be twisted in
practice or other through legislation35, though this does
not make it indispensable. CSS could therefore be rele-
vant during an insurrection or other emergencies or due to
other extreme circumstances. Secondly, the assessment of
the severity of the system must not lead to the assumption
that it causes censoring36. Any kind of sanctions associ-
ated with the system must be proportional37 and properly
legislated and justiﬁed38. Thirdly, national courts can be
part of the problem if they are not able to sufﬁciently and
properly assess Article 10 in regards to this issue39.

4.4 Freedom of Assembly and Association

CSS can be used long term to control or infringe given
Article 11 rights. This can be done through recognition of
pictures, locations, people or speciﬁc subjects. The aims
for this kind of surveillance could be suppression of po-
litical or non-political parties, speciﬁc public ﬁgures or
unions [6, 60].

Because of the nature of the surveillance, we focus on
Freedom of Association. Article 11 and Article 10 are
not in competition, rather the opposite, and Freedom of
Assembly is clearly as important as Freedom of Expres-
sion40. The freedom to associate with any political party

3467667/09 and others, Bayev and Others v. Russia, §§ 64 and 83.
356538/74, The Sunday Times v. the United Kingdom, § 59.
3656925/08, B´edat v. Switzerland, § 79.
3713444/04, Glor v. Switzerland, § 94.
38Bayev and Others v. Russia, supra, § 83.
3923954/10, Uj v. Hungary, §§ 25-26.
4020652/92, Djavit An v. Turkey, 2003, § 56

is crucial41 as is any other form of group42. To be included
in the protection, the ’association’ must have a private
character43, but the state cannot speculate in nationalis-
ing it on purpose to remove the Article 11 protection44, or
in reality prevent any ineffective exercise of the right45.
Article 11 states that any intervention with the right must
“prescribed by law”, pursue legitimate aims and be “nec-
essary in a democratic society”. These are not deﬁned in
the same manner as Article 10. To be prescribed by law,
the intervention of the Right of Assembly must be posi-
tively described in legislation, must be available for those
affected and must be foreseeable46.

Any CSS systems used for these purposes must be
included in national legislation and the public must be
clearly warned that they are able to use it to essentially
prevent assembly. Preventing assembly could be in the
form of dissolving organisations (political, labour or oth-
erwise) through identiﬁcation and arrests or milder eco-
nomic sanctions.

The core point is that the CSS would be used for iden-
tiﬁcation, as pictures contain metadata be themselves or
through the system, or clear location or other data, and
that they can contain elements that could link the user to
the organisation that the state wants to quash. CSAMD
is likely capable of being re-purposing the system to look
for these factors and through the same methods, alert a
company (or the state using the system) which can then
lead to further investigation and potentially litigate. Fu-
ture systems are likely to be able to do this much easier
and be designed for it.

5 The Child Sexual Abuse Regula-

tion

The proposed EU Regulation laying down rules to pre-
vent and combat child sexual abuse [17] (CSA) has issues

41133/1996/752/951, United Communist Party of Turkey and Others.

v. Turkey, 1998, § 25.

4248848/07, Association Rhino and Others v. Switzerland, 2011 § 61.
437601/76 and 7806/77, Young, James and Webster v.
the United

Kingdom, 1979, Commission’s report, § 167.

4442117/98, Bollan v. the United Kingdom, 2000.
4570945/11 et al., Magyar Kereszt´eny Mennonita Egyh´az and Others

v. Hungary, 2014, § 78.

4639748/98, Maestri v. Italy [GC], 2004, §25 - 42.

In its impact assessment
which we must comment on.
of the proposed legislation, under its analysis of loss of
fundamental rights, the European Commission claims on
page 14 claims that CSS is “often the only possible way
to detect it”, foregoing all other types of preventive and
criminal measures Member States can take against CSA.
There is no argumentation via logic or backed up by
literature. This makes the statement and perhaps the
whole Regulation techno-solutionist, a term which much
good research [23] illustrates, which implies the argument
merely want to only solve a problem with a speciﬁc tech-
nology without regarding other factors. The European
Commission furthermore disregards and does not analyse
the potential consequences either CSS or server-side scan-
ning would have on cybersecurity and privacy, while they
justify the victim’s potential positive outcomes outweigh-
ing the negative of everyone else. The main tools of the
Regulation are:

Providers must conduct risk assessments (Article 3)

and providers must mitigate risks (Article 4).

Force app stores to prevent children from using inap-
propriate (not well deﬁned) apps (Article 6), and force
CSS or server-side scanning for all providers of commu-
nication services (this includes Signal) (Article 10) and
backdoors, combined with potentially creating unlimited
preservation of data if requested (Article 22).

Enforcement powers, which includes 6 percent of an-
nual turnover or global income based ﬁne (Article 35) and
forcibly physically shutting servers down (Article 28 and
29).

The creation of a EU Centre to facilitate technology and
support providers and Member States, and have databases
that are not well speciﬁed, without any requirements for
its own staff and no assurance that EUROPOL will not de
facto control or otherwise inﬂuence it.

We know very well that self-regulation requires teeth if
not complied with, which the Regulation does have, but
its requirements end up putting the entire sector at risk
from how zealous it looks or through its attempts to solve
its goals in Article 1 (impossible only with these tools, see
Section 2).

Depending on the interpretation, Article 10 may com-
promise all communication platforms which are used
by EU citizens, and the European Commission has not
learned from the mistakes of the past regarding key es-
crow, nor have they taken heed from existing literature

such as [1].

6 Future Considerations

If these systems get implemented into our lives through
all digital infrastructures, keeping check on their inﬂu-
ence and consequences and what can be done to use them
is of utmost importance, which means increased amounts
of research into their actions upon the lives of those af-
ﬂicted by them, whether natural or legal persons. This
research includes judicial, cybersecurity and structural re-
views, as well as more empirically based research through
interviews of those that develop or are affected by them.
Plenty of existing research into other systems is usable
[65, 41], but because of the character of these new surveil-
lance structures, the reverberations of the actions of the
system will be that much greater.

We hope that there will be an increasing focus in the
research on the downsides and or the costs of CSS, in
essence the proportionality of the exercise. This ques-
tion could be answered with an analysis focused on pro-
portionality as a principle within law or philosophy, and
would ﬁt as an extension of existing research [15, 53], and
the development of the EU CSA Regulation. A continua-
tion of showing the impact on human rights of every kind
by CSS is needed as well.

7 Conclusion

If you want to dig for gold, you predict accurately where
it is. What you usually do not do, is to dig up the entire
crust of the surface of the earth. CSS systems and mass
surveillance represent the latter.

In this paper, we argue that CSS cannot be justiﬁed to
be used in the way systems like CSAMD is presented.
We do so on the basis of two different kinds of proofs,
but with the same outcome: Because CSS cannot reach
its own purpose, either due to assumptions on universal
circumventability or purely because it cannot technically
or practically reach the goals, it will not be ﬁt for ﬁghting
CSAM by itself.

We are aware of ongoing developments such as En-
crochat , but that speciﬁc case represents a situation where
the entire infrastructure was compromised, and was more

of a trap than a CSS. There are far more elaborate and
safer alternatives than the PSI and surrounding system
which Apple chose for CSAMD, and this could in retro-
spect be considered the ﬁrst major “failure” of the system
[1]. Instead of relying on these safer alternatives, Apple
decided to create an infrastructure which beckons to suf-
fer adversarial failures from the start [66].

While this may change, this does not prevent the sec-
ond “failure” of the system. We show in this paper that
CSS like CSAMD will have the potential to not just vi-
olate one human right, but most likely ≦ 2 rights at the
same time. CSS such as CSAMD is very likely to vio-
late the EHRC Article 6 derived Right to Remain Silent
and Not Incriminate Oneself, which could cause serious
admissibility issues in courts, potentially preventing trials
against criminals or otherwise disrupt legal systems. CSS
opens endless possibilities for states to use entrapment,
potentially causing the prosecution of innocent individu-
als.

Another violated right is Article 8, which includes a
right and protection of privacy. CSS will not pass the
test of ”necessary in a democratic society”, exactly be-
cause these systems rarely will be proportionate in their
infringements to their goals. We discussed two more po-
tential violations, which CSS may cause, but these were
both hypothetical. This was Article 10, Freedom of Ex-
pression, and Article 11, Freedom of Assembly and Asso-
ciation. We note that CSS systems would rarely meet the
thresholds to justify violating the rights, but what is im-
portant to acknowledge is the very powerful ways which
they could already do so. Surveillance systems seen in
China are likely already capable of this [18, 36].

We comment on the proposal for a Regulation by the
European Union to ﬁght Child Sexual Abuse, which wor-
ryingly it does through mandating CSS and breaking the
encryption of all messaging services which can be identi-
ﬁed as such. We show in our proofs that CSS is by no
means the right or adequate solution, and the proposal
does not justify how this could be achieved without fun-
damentally breaking everyone’s right to privacy and fair
trial.

References

[1] Hal Abelson et

al.

“Bugs

The

Risks

Client-Side
of
ets:
2110.07450.
arXiv:
ning”.
http://arxiv.org/abs/2110.07450.

2021.

in our Pock-
Scan-
URL:

[2] Harold Abelson et al. “Keys under doormats: Man-
dating insecurity by requiring government access
to all data and communications”. In: Journal of Cy-
bersecurity 1.1 (2015), pp. 69–79. ISSN: 20572093.
DOI: 10.1093/cybsec/tyv009.

[3] Ross Anderson. Security engineering: a guide to
building dependable distributed systems. John Wi-
ley & Sons, 2020.

[4] Apple

Inc.

“CSAM

Detection
2021.

-
URL:

Technical
https://www.apple.com/child-safety/pdf/CSAM%7B%5C_%7DDetection%7B%5C_%7DTechnical%7B%5C_%7DSummary.pdf.

Summary”.

[5] Luciana C. Assini-Meytin, Rebecca L. Fix,
and Elizabeth J. Letourneau. “Child Sexual
Abuse: The Need for a Perpetration Preven-
tion Focus”. In: Journal of Child Sexual Abuse
ISSN: 15470679. DOI:
29.1 (2020), pp. 22–40.
10.1080/10538712.2019.1703232.
URL:
https://doi.org/10.1080/10538712.2019.1703232.

[6] Valerie Aston. “State surveillance of protest and
the rights to privacy and freedom of assembly: a
comparison of judicial and protester perspectives”.
In: European Journal of Law and Technology 8.1
(2017), pp. 1–19.

[7] Anish Athalye. Inverting PhotoDNA. 2021. URL:

https://www.anishathalye.com/2021/12/20/inverting-photodna/.

[8] Mihir Bellare. A Concrete-Security Analysis of the
Apple PSI Protocol. Tech. rep. Apple, 2021. URL:
https://www.apple.com/child-safety/pdf/Alternative%7B%5C_%7DSecurity%7B%5C_%7DProof%7B%5C_%7Dof%7B%5C_%7DApple%7B%5C_%7DPSI%7B%5C_%7DSystem%7B%5C_%7DMihir%7B%5C_%7DBellare.pdf.

[9] Samantha Besson. “European human rights, supra-
national judicial review and democracy : thinking
outside the judicial box”. In: Human rights protec-
tion in the European legal order : The interaction
between the European and the national courts. In-
tersentia, 2011, pp. 97–145.

[10] Abhishek

Bhowmick

et

al.
2021.

“The
URL:

Apple
https://www.apple.com/child-safety/pdf/Apple%7B%5C_%7DPSI%7B%5C_%7DSystem%7B%5C_%7DSecurity%7B%5C_%7DProtocol%7B%5C_%7Dand%7B%5C_%7DAnalysis.pdf.

System”.

PSI

[11] Daniel Bleichenbacher, Aggelos Kiayias, and Moti
Yung. “Decoding of Interleaved Reed Solomon
Codes over Noisy Data”. In: Automata, Languages
and Programming. Vol. Lecture No. 2003, pp. 97–
108. ISBN: 3540666664.

[12] Dallas Card and Noah A. Smith. “On Consequen-
tialism and Fairness”. In: Frontiers in Artiﬁcial
Intelligence 3.May (2020), pp. 1–11.
ISSN:
26248212. DOI: 10.3389/frai.2020.00034.
arXiv: 2001.00329.

[13] Munamato Chemhuru. “African Communitarian-
ism and Human Rights, Towards a Compatibilist
View”. In: Theoria 65.157 (2018), pp. 37–56.

[14] David M. Chess and Steve R. White. “An Unde-
tectable Computer Virus”. In: Proceedings of Virus
Bulletin Conference 5 (2000).

[15]

and the prospects

Jennifer Cobbe. “Data protection , ePrivacy
,
for Apple’s on-device
CSAM Detection system in Europe”. 2021. URL:
https://osf.io/preprints/socarxiv/rhw8c/.

[16] Fred Cohen. “Computer Viruses - Theory and Ex-
periment”. In: Computers & Security 6.1 (1987),
pp. 22–35. URL: all.net/books/virus/.

[22] Peter Fussey and Ajay Sandhu. “Surveillance ar-
bitration in the era of digital policing”. In: Theo-
retical Criminology 26.1 (2022), pp. 3–22. ISSN:
14617439. DOI: 10.1177/1362480620967020.

[23]

John Gardner and Narelle Warren. “Learning
from deep brain stimulation:
the fallacy of
techno-solutionism and the need for ‘regimes of
care’”. In: Medicine, Health Care and Philoso-
phy 22.3 (2019), pp. 363–374. ISSN: 15728633.
DOI:
URL:
10.1007/s11019-018-9858-6.
http://dx.doi.org/10.1007/s11019-018-9858-6.

[24] Mohamad Gharib. “Privacy and Informational
Informed Con-
Self-determination Through
In: Lecture Notes
sent: The Way Forward”.
subseries
Science
in Computer
Lecture Notes
Intelligence and
in Artiﬁcial
Lecture Notes in Bioinformatics) 13106 LNCS
(2022), pp. 171–184.
ISSN: 16113349. DOI:
10.1007/978-3-030-95484-0_11.

(including

[25] Gehan Gunatilleke. “Justifying Limitations on the
Freedom of Expression”. In: Human Rights Review
22.1 (2021), pp. 91–108. ISSN: 18746306. DOI:
10.1007/s12142-020-00608-8.

[18] Corinne Reichert. China reportedly scans tourists’
installing malware. 2019. URL:

phones by
https://www.cnet.com/tech/mobile/china-is-reportedly-scanning-tourists-phones-with-malware/.

[27] Kirsty Hughes and Neil M. Richards. “The Atlantic
divide on privacy and free speech”. In: Compara-
tive defamation and privacy law. Cambridge Uni-
versity Press, 2016.

[19] Lisa Cosgrove et al. “Digital phenotyping and
digital psychotropic drugs: Mental health surveil-
lance tools that threaten human rights”. In: Health
and Human Rights 22.2 (2020), pp. 33–40. ISSN:
10790969.

[20]

Jack Donnelly. “Human Rights as Natural Rights”.
In: Human Rights Quarterly 4.3 (1982), p. 391.
ISSN: 02750392. DOI: 10.2307/762225.

[28] Tuomas Hupli. “To Remain or Not to Remain
Silent: The Evolution of The Privilege against Self-
incrimination Ten Years After Marttinen v. Fin-
land”. In: Bergen Journal of Criminal Law and
Criminal Justice 6.2 (2018), pp. 136–151.

for

Johnson

surveillance or

and Tanina Rostain.
[29] Rebecca A.
spotlight on in-
“Tool
In: An-
equality? Big data and the law”.
nual Review of Law and Social Science 16
(2020), pp. 453–472.
ISSN: 15503585. DOI:
10.1146/annurev-lawsocsci-061020-050543.

[21] Council
vention
https://www.echr.coe.int/Documents/Convention%7B%5C_%7DENG.pdf.

European
Rights.

Europe.
Human

Con-
URL:

of
on

[17] European Commission. Proposal

for a Reg-
the European Parliament and of
ulation of
the Council
to prevent
laying down rules
and combat child sexual abuse. 2022. URL:
https://ec.europa.eu/home-affairs/system/files/2022-05/Proposal%20for%20a%20Regulation%20laying%20down%20rules%20to%20prevent%20and%20combat%20child%20sexual%20abuse%7B%5C_%7Den%7B%5C_%7D0.pdf.

[26] Yao Wen Huang et al. “Non-detrimental Web ap-
plication security scanning”. In: Proceedings -
International Symposium on Software Reliability
Engineering, ISSRE (2004), pp. 219–230. ISSN:
10719458. DOI: 10.1002/0471654787.ch9.

[42]

and Anunay Kulshrestha.
Jonathan Mayer
Opinion: We built a system like Apple’s to
ﬂag child sexual abuse material — and con-
cluded the tech was dangerous. 2021. URL:
https://www.washingtonpost.com/opinions/2021/08/19/apple-csam-abuse-encryption-security-privacy-dangerous/.

[43] Thierry Mefenza et al. “Polynomial

interpo-
lation of
the generalized Difﬁe–Hellman and
Naor–Reingold functions”. In: Designs, Codes and
Cryptography 87 (2019), pp. 75–85.

[30] Rikke Frank Jørgensen. “Data and rights in
the case of Den-
the digital welfare state:
Information Communication and
mark”.
Society 0.0 (2021), pp. 1–16.
ISSN: 14684462.
DOI: 10.1080/1369118X.2021.1934069. URL:
https://doi.org/10.1080/1369118X.2021.1934069.

In:

[40] David Lyon. Pandemic Surveillance. John Wiley &

Sons, 2021. ISBN: 978-1-5095-5030-2.

[41] Monique Mann, Angela Daly, and Adam Molnar.
“Regulatory arbitrage and transnational surveil-
lance: australia’s extraterritorial assistance to ac-
cess encrypted communications”. In: Internet Pol-
icy Review 9.3 (2020), pp. 1–20. ISSN: 21976775.
DOI: 10.14763/2020.3.1499.

[31] Lina M. Khan. “Sources of Platform Power”. In:
Georgetown Law Technology Review 325 (2018),
pp. 325–334.

[32] Trevor A. Kletz. What Went Wrong?: Case
Histories of Process Plant Disasters and How
They Could Have Been Avoided. Butterworth-
Heinemann, 2009.

[33] Francesca Klug. “THE LONG ROAD TO HU-
MAN RIGHTS COMPLIANCE”. In: Nothern Ire-
land Legal Quaterly 57.1 (2006), pp. 186–204.

[34] Mateusz Kolaszy´nski. “Secret

surveillance in
Poland after Snowden: between secrecy and trans-
parency.” In: Trust and Transparency in an Age of
Surveillance. Routledge, 2022. Chap. 7.

[35] Bert

Jaap Koops. “The concept of

function
Innovation and Technology
In: Law,
creep”.
13.1 (2021), pp. 29–56.
ISSN: 1757997X.
DOI: 10.1080/17579961.2021.1898299. URL:
https://doi.org/10.1080/17579961.2021.1898299.

[37] St´ephane Leman-Langlois. “State Mass Spy-
In: Critical Criminology
ISSN: 15729877.
URL:

Illegalism”.
ing as
26.4 (2018), pp. 545–561.
DOI:
https://doi.org/10.1007/s10612-018-9421-z.

10.1007/s10612-018-9421-z.

[47]

[38] Koen Leurs. “Communication rights from the
margins: politicising young refugees’ smartphone
pocket archives”. In: International Communica-
tion Gazette 79.6-7 (2017), pp. 674–698. ISSN:
17480493. DOI: 10.1177/1748048517727182.

[39] Nancy G. Leveson. Safeware: System Safety and
Computers. 1. Addison-Wesley Publishing Com-
pany, Inc., 1995, p. 680. ISBN: 0-201-11972-2.

[36] Lorand Laskai

and Adam Segal. The En-
2021 Up-
rep. March. Carnegie Endow-
2021. URL:

in

for

China:

cryption Debate
date. Tech.
ment
https://carnegieendowment.org/files/202104-Germany%7B%5C_%7DCountry%7B%5C_%7DBrief.pdf.

[46] Rasmus Pagh and Flemming Friche Rodler.
In: BRICS Report Se-
DOI:
121–133.
URL:

“Cuckoo Hashing”.
ries. August.
10.1007/3-540-44676-1_10.
http://link.springer.com/10.1007/3-540-44676-1%7B%5C_%7D10.

International Peace,

2001,

pp.

[44] Ori Or-Meir et al. “Dynamic malware analysis
in the modern era—A state of the art survey”.
In: ACM Computing Surveys 52.5 (2019). ISSN:
15577341. DOI: 10.1145/3329786.

[45] Abhilash Nair. “Internet Content Regulation:
Is a Global Community Standard a Fallacy or
International Re-
the Only Way Out?1”.
view of Law, Computers & Technology 21.1
(2007), pp. 15–25.
ISSN: 1360-0869. DOI:
10.1080/13600860701281655.

In:

Jiaxin Pan, Chen Qian, and Magnus Ringerud.
“Signed Difﬁe-Hellman Key Exchange with Tight
Security”. In: Topics in Cryptology – CT-RSA 2021.
2021, pp. 201–227. ISBN: 9783540792628.

Pinkas.
(PSI)

[48] Benny
section
CSAM Detection
https://decentralizedthoughts.github.io/2021-08-29-the-private-set-intersection-psi-protocol-of-the-apple-csam-detection-system/.

Inter-
Apple
URL:

Set
the
2021.

The
Protocol

Private
of

System.

[49] Gopalakrishnan

Prakash

and Marimuthu
Parameswari. “On reviewing the implications
of Rogue Antivirus”. In: Journal of Information
Ethics 25.2 (2016), pp. 128–139. ISSN: 10619321.

[50] Ramiro Alvarez Ugarte; Naomi Appelman;
Lilana Arroyo Moliner; Istv´an B¨or¨ocz; Magda
Brewczy´nska; Julienne Chen; Julie E. Cohen;
Arely Cruz-Santiago; Angela Daly; Marisa Duarte;
Lilian Edwards; Helen Eenmaa-Dimitrieva; Rafael
Evangelista; Ronan ´O Fathaigh; Data Justice
and Covid-10: Global Perspectives. Linnet Tay.
Meatspace Press, 2020. ISBN: 9781913824006.

[51]

recognition
Isadora Neroni Rezende. “Facial
in police hands: Assessing the
‘Clearview
case’ from a European perspective”. In: New
of European Criminal Law 11.3
Journal
ISSN: 2032-2844. DOI:
(2020), pp. 375–389.
10.1177/2032284420948161.

[60]

[52] Peter Rinda and Mike Rosulek. “Malicious-
Secure private set
intersection via dual exe-
the ACM Confer-
cution”. In: Proceedings of
ence on Computer and Communications Secu-
rity (2017), pp. 1229–1242. ISSN: 15437221. DOI:
10.1145/3133956.3134044.

[53] Paul Rosenzweig. “The Law and Policy of Client-
Side Scanning”. In: Joint PIJIP/TLS Research Pa-
per Series 58 (2020).

[54]

Jerome H. Saltzer and Michael D. Schroder. “The
Protection of Information in Computer Systems”.
In: Proceedings of the IEEE (1975).

[55] L. Sanchez et al. “A Practitioner Survey Exploring
the Value of Forensic Tools, AI, Filtering, &
Safer Presentation for Investigating Child Sexual
Abuse Material (CSAM)”. In: Digital Investi-
gation 29 (2019), S124–S142. ISSN: 17422876.
DOI:
URL:
10.1016/j.diin.2019.04.005.
https://doi.org/10.1016/j.diin.2019.04.005.

[56] Paul M. Schwartz. “Privacy and Democracy in
Cyberspace”. In: Vanderbilt Law Review 52.6
ISSN: 00422533. DOI:
(1999), pp. 1609–1670.
10.2139/ssrn.205449.

[57] Sharifah Sekalala et al. “Analyzing the human
rights impact of increased digital public health
surveillance during the COVID-19 crisis”. In:
Health and Human Rights 22.2 (2020), pp. 7–20.
ISSN: 10790969. DOI: 10.17615/8e8a-m957.

[58] Amartya Sen. “Speaking of Freedom”. In: The Lit-

tle Magazine: Listen (2002), pp. 9–16.

[59] Hossain Shahriar, Sarah North,

and Wei
Chuen Chen. “Client-side detection of SQL
In: Lecture Notes in Busi-
injection attack”.
ness
Information
LNBIP
148
(2013), pp. 512–517.
ISSN: 18651348. DOI:
10.1007/978-3-642-38490-5_46.

Processing

Ilia Siatitsa. “Freedom of assembly under attack:
General and indiscriminate surveillance and in-
terference with internet communications”. In: In-
the Red Cross 102.913
ternational Review of
(2020), pp. 181–198.
ISSN: 16075889. DOI:
10.1017/S1816383121000047.

[63] Chad M.S. Steel et al. “An integrative re-
technology and counter-
view of historical
measure usage trends in online child sexual
In: Foren-
exploitation material offenders”.
Investiga-
sic Science
tion 33 (2020), p. 300971.
ISSN: 26662817.
DOI: 10.1016/j.fsidi.2020.300971. URL:
https://doi.org/10.1016/j.fsidi.2020.300971.

International: Digital

[64] Brett Stone-Gross et al. “The Underground
Software”.
of
Security
(2013), pp. 55–78. DOI:

Economy
In:
and Privacy
III
10.1007/978-1-4614-1981-5_4.

Fake Antivirus

Information

Economics

of

[65] Elizabeth Stoycheff, G. Scott Burgess, and Maria
Clara Martucci. “Online censorship and digital
surveillance:
the relationship between suppres-
sion technologies and democratization across

[61] Haroon Siddique. Law Society sounds warn-
judicial review bill. 2021. URL:

ing against
https://www.theguardian.com/law/2021/jul/21/law-society-sounds-warning-against-judicial-review-bill.

[62] Daniel J. Solove. Nothing to hide: The false
tradeoff between privacy and security. 2011,
pp.
DOI:
10.5860/choice.49-2979.

9780300172317.

1–246.

ISBN:

countries”. In: Information Communication and
Society 23.4 (2020), pp. 474–490. ISSN: 14684462.
DOI: 10.1080/1369118X.2018.1518472. URL:
https://doi.org/10.1080/1369118X.2018.1518472.

[66] Lukas Struppek et al.

“Learning to Break
Deep Perceptual Hashing: The Use Case Neu-
ralHash”.
arXiv: 2111.06628. URL:
https://arxiv.org/abs/2111.06628v1.

2021.

[67] Nandhini

Subramanian

et

Steganography: A Review of
Access
Advances”.
pp.
10.1109/ACCESS.2021.3053998.

23409–23423.

ISSN:

IEEE

In:

“Image
al.
the Recent
(2021),
9
DOI:

21693536.

[68] Erik Voeten. “The impartiality of international
judges: Evidence from the European court of hu-
man rights”. In: American Political Science Review
102.4 (2008), pp. 417–433. ISSN: 00030554. DOI:
10.1017/S0003055408080398.

[69] Ben Wagner. “The Politics of

Internet Fil-
tering: The United Kingdom and Germany
In: Politics
in a Comparative Perspective”.
34.1 (2014), pp. 58–71.
ISSN: 02633957. DOI:
10.1111/1467-9256.12031.

[70] Emily West. “Amazon: Surveillance as a ser-
In: Surveillance and Society 17.1-2
14777487. DOI:

vice”.
(2019), pp.
10.24908/ss.v17i1/2.13008.

27–33.

ISSN:

[71] Lucie White

“Without
fail?”

and Philippe Van Basshuy-
corona
a
Journal of Medical Ethics
ISSN: 14734257. DOI:

sen.
apps
47.12 (2021), E83.
10.1136/medethics-2020-107061.

trace: Why

did

In:

[72] Yuval Yekutieli. “Is somebody watching you? An-
cient surveillance systems in the southern Judean
desert”. In: Journal of Mediterranean Archaeol-
ogy 19.1 (2006), pp. 65–89. ISSN: 09527648. DOI:
10.1558/jmea.2006.19.1.65.

[73] Hans-Georg Ziebertz, Susanne D¨ohnert, and
Alexander Unser. “Predictors of Attitudes To-
wards Human Dignity: An Empirical Analy-
sis Among Youth in Germany”. In: Religion
and civil human rights in empirical perspec-
tive. Cham u. a. Springer, 2018, pp. 17–60. DOI:
10.1007/978-3-319-59285-5_2.

