GreyConE: Greybox fuzzing+Concolic execution
guided test generation for high level design

Mukta Debnath∗‡, Animesh Basak Chowdhury†‡, Debasri Saha§ and Susmita Sur-Kolay∗
∗ Advanced Computing and Microelectronics Unit, Indian Statistical Institute, Kolkata, India
†Centre for Cybersecurity, New York University, United States
§A. K. Choudhury School of IT, U. of Calcutta, India
‡Equal contribution

2
2
0
2

l
u
J

3
1

]
E
S
.
s
c
[

3
v
7
4
0
4
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—Exhaustive testing of high-level designs pose an
arduous challenge due to complex branching conditions, loop
structures and inherent concurrency of hardware designs. Test
engineers aim to generate quality test-cases satisfying various
code coverage metrics to ensure minimal presence of bugs in a
design. Prior works in testing SystemC designs are time inefﬁcient
which obstruct achieving the desired coverage in shorter time-
span. We interleave greybox fuzzing and concolic execution in a
systematic manner and generate quality test-cases accelerating
test coverage metrics. Our results outperform state-of-the-art
methods in terms of number of test cases and branch-coverage
for some of the benchmarks, and runtime for most of them.

Index Terms—SystemC, High-level Synthesis, Greybox fuzzing,

Symbolic Execution, Concolic Execution, Code Coverage

I. INTRODUCTION
SystemC is the latest de-facto standard for early-stage
development of hardware designs having a complex design
architecture. Detecting functional and security bugs at early
stage design development is crucial to eliminate functional in-
consistencies and vulnerabilities. A SystemC design provides
cycle-accurate behavioural modeling of a given speciﬁcation.
Therefore, testing SystemC design focuses more on functional
level correctness, inconsistency in speciﬁcations and unknown
(possibly erroneous) behaviour rather than defects arising
from low-level hardware. Hardware engineers prefer to check
logical inconsistencies in early-stage design as defects caught
post manufacturing stage are costly. This motivates veriﬁcation
and test engineers to develop a good test generation framework
for detecting bugs for designs at higher abstraction level.

Veriﬁcation and testing SystemC designs (and, in general
high level designs) for bug detection have a rich literature [1],
[2], [3], [4]. Prior works adopt two mainstream approaches
techniques, e.g. model
to tackle the problem: (a) formal
checking [2] and symbolic execution [3]; and (b) simulation-
based techniques such as black-box testing, white-box [5], [6]
and grey-box [7], [8] fuzzing. There are a plethora of works
that have critically identiﬁed the issues like scalability and
exhaustive testing, and addressed them by proposing a hybrid
approach — concolic testing. A very recent work [9] has
applied concolic testing to alleviate the problem of scalibility
arising from symbolic execution. Although the approach is
quite promising, we ask two unaddressed questions:

1) Concolic testing relies heavily on initial test-cases. How
can one generate better quality test-cases (in a minimal-

istic time frame) such that concolic execution can focus
only on “hard-to-reach” state of design?

2) Guided fuzzing-based techniques quickly explore the
design space but often get stuck in complex branch con-
ditions. How can one mitigate this problem in fuzzing?
In this work, we propose GreyConE: an end-to-end test-
generation framework for high-level designs outputting high-
quality test-cases and quickly achieves better coverage metrics.
In short, we make the following contributions:

1) We developed GreyConE by interleaving greybox fuzzing
and concolic execution. We leverage the power of grey-
box fuzzing by quickly covering ”easy-to-reach” states
ﬁrst and pass on “hard-to-reach” state to concolic engine.
2) GreyConE outperforms state-of-the-art Greybox fuzzer
(AFL [7]) and concolic test generation (S2E [10]) in
terms of branch coverage achieved and runtime for many
of the benchmark SystemC designs.

The rest of the paper is organized as follows: Section II
outlines the background and prior related works. In Section III,
we present our proposed framework and show the efﬁcacy of
results in Section IV. Section VI has the concluding remarks.

II. BACKGROUND

A. Test-case generation for SystemC Designs

Recent works like SESC [11] and CTSC [9] use symbolic
and concolic execution respectively to test SystemC designs.
SESC adapted symbolic execution with signiﬁcant code cov-
erage but limited to only a subset of SystemC features. Later,
CTSC covered most SystemC semantics but failed to improve
efﬁcacy in terms of performance and scalability compared to
SESC. Further, both the work depends on the availability of
initial test-cases for the symbolic engine.

B. Greybox fuzzing

Fuzz testing is a well known technique for detecting bugs
in software. Greybox fuzzing [7], [12] generates interesting
test-cases using genetic algorithm on instrumented executable.
Instrumentation injects markers in the code at every basic-
block which can track post compilation and execution whether
a test-case has reached the marker location. A ﬁtness function
is used in order to evaluate the quality of a test-case. Typically,
greybox fuzzing is used to improve branch-pair coverage of

i

 
 
 
 
 
 
1
2

3
4
5
6
7
8
9
10
11
12
13
14
15

int foo(int i, int j){
//i and j symbolic
variables

int x = 0, y = 0, z;
z = 2*j;
if(z == i){

if (j < 5){

x = -2; y = 2;

}
else

{ x = 2; y = 2;}

}
else

{ x = 2; y = 2;}

return 0;

}

Fig. 1: (a) Example code snippet. (b) Symbolic and concolic
execution ﬂow

the design. A test-case is “interesting”, if it covers a previously
unexplored basic block, or covers it for a unique number of
times. The fuzzer maintains a history-table for every basic
block covered so far, and retains interesting test cases for
further mutation/crossover. The test generation process halts
is achieved. Popular
once the used-deﬁned coverage goal
coverage-guided greybox fuzzing (CGF) engines like AFL [7]
are able to detect countless hidden vulnerabilities in well-
tested software.

C. Symbolic and Concolic Execution

Symbolic execution is a formal technique for generating
quality test-cases. It typically assigns program inputs as sym-
bolic values (as opposed to concrete values) and forks two
threads at each condition; a boolean formula evaluating to True
and the negated formula. Thus, symbolic execution forms an
execution tree (Figure 1(b)). At each branching condition, a
SAT/SMT solver is invoked to generate test-case which in turn
ﬁnds a set of concrete values assigned to symbolic variables
reaching the branching condition. The size of the execution
tree grows exponentially in terms of branching conditions
resulting in state-space explosion.

One major drawback of symbolic execution is treating all
program inputs as symbolic variables. This results in complex
boolean formulae within a few nested conditions resulting in
longer runtime for generating test-cases. Concolic execution
solves this problem by assigning concrete values to a set
of program inputs for quicker search space exploration. This
helps covering the branching conditions which are “easy-to-
reach”, and pass on the simpliﬁed formulae for “hard-to-reach”
conditions to SAT/SMT solver. Post executing the program
with concrete values, constraints at each condition are negated
and solved to generate a new test case covering the unexplored
path. The path constraints generated have reduced the number
of clauses and variables aiding solvers to penetrate deeper
program states quickly.

We demonstrate concolic execution with a simple example
(Figure 1a). A set of program inputs are treated as symbolic
variables; the inputs i and j have symbolic values i = a and
j = b. We choose a random concrete input (i = 2, j = 1)
and obtain the execution trace (dotted lines, Figure 1b). The
alternative path constraints to be explored symbolically are

collected along the execution path guided by concrete inputs,
forking the side branches. At each condition, constraints are
negated and solved to generate new test cases. The concolic
engine terminates after all conditions are covered. In the next
section, we present Greycone, which interleaves concolic exe-
cution and greybox fuzzing to accelerate test-case generation.

Algorithm 1: FUZZER(DU Tins−exe, Tinitial)
Data: DU Tins−exe, Tinitial, timecut−of f
Result: Tf uzzed
Tf uzzed ← Tinitial
while time ≤ timecut−of f do
for τ ∈ Tf uzzed do

K ← CALCULATE ENERGY(τ )
for i ∈ {1, 2, . . . , K} do

τ (cid:48) ← MUTATE-SEED(τ )
if IS-INTERESTING(DU Tins−exe, τ (cid:48)) then

Tf uzzed ← Tf uzzed ∪ τ (cid:48)

end

end

end

end
return Tf uzzed

III. GREYCONE FRAMEWORK

The workﬂow of GreyConE (Fig. 2) comprises (1) Greybox

fuzzing, (2) Concolic execution, and 3) Coverage evaluator.

A. Generating instrumented binary

First, we convert a high-level hardware design to a low-level
intermediate representation (IR) in the form of LLVM bitcode
[13]. The compiler inserts a marker at the top of every basic
block in the IR to generate an instrumented executable which
is passed to both greybox fuzzing engine and concolic engine
to generate test-cases and track uncovered basic blocks.

B. Greybox fuzzing by AFL

We outline the overall ﬂow of Greybox fuzzing by American
fuzzy lop (AFL) [7] in Algorithm 1. The instrumented ex-
ecutable DU Tins−exe and a test-set Tinitial are fed to the
fuzzing framework. The function CALCULATE-ENERGY as-
signs energy to every test-case of Tinitial based on runtime be-
haviour, e.g. execution time and obtained coverage. It assigns
more energy to a test-case with faster execution time, covering
more branches and penetrating deeper code segments. AFL
uses Tinitial to perform deterministic and havoc mutations
to generate newer test-cases (MUTATE-SEED). AFL uses
branch-pair coverage as a ﬁtness metric to determine the
quality of a test-input. For each branch-pair, it maintains a
hash-table entry to record hit counts. It retains a test-input
for further exploration if it covers an unseen new-branch
pair or has unique hit-counts on already covered branch-pair
(IS-INTERESTING). The fuzz engine retains interesting test-
cases for further exploration. The algorithm terminates post
covering all branching conditions or reaching a user-deﬁned
timecut−of f . AFL maintains all interesting test-inputs in the
queue Tf uzzed.

ii

func (i=a, j=b)Concrete seed i=2, j=1z = 2b 2b != a2b == a2b == a &&b < 5  x = -2,y = 2 2b == a &&b >= 5 ExitExit    x = 2,     y = 2 Exit  x = - 2,y = - 2 Fig. 2: GreyConE test generation framework

C. Concolic testing by S2E

For

concolic

testing, we

employ symbolic

executer
(S2E) [10] which has two main components: 1) a virtual
machine based on QEMU [14] and 2) a symbolic execu-
tion engine based on KLEE [15]. The interesting inputs
in the design can be marked as symbolic using S2E API
s2e make smbolic() function. It executes the DU Tins−exe
with concrete test-inputs from Tinitial (CONC-EXEC) gen-
erating concrete execution traces. It maintains the state of
execution tree DU TexecT ree and identiﬁes uncovered branch-
pairs using Tinitial. COND-PREDICATE constructs the path
constraints for the uncovered branch pair, forks new thread and
invokes SAT-solver (CONSTRAINT-SOLVER) to generate the
test-cases. It terminates either after a user-deﬁned timeout is
reached or covers all branch-pairs of DU TexecT ree. Tconcolic
stores the test cases generated by the concolic engine. We
outline a typical concolic execution approach in Algorithm 2.

D. Interleaved fuzzing and concolic testing

We interleave greybox fuzzing and concolic execution ex-
tenuating the problems associated with standalone techniques.
Concolic engine usually performs depth-ﬁrst search for search-
space exploration. A set of random test-cases lead to frequently
invoking the SAT/SMT solver for generating test-cases to
cover unexplored conditions in DU TexecT ree. We perform
fuzzing of the DUT to avoid this huge slowdown and invoke
concolic engine for “hard-to-cover” scenarios with the fuzzer
generated test-cases. As shown in Fig. 2, we ﬁrst perform
lightweight instrumentation on all conditions of design-under-
test (DUT) and generate an instrumented executable. We start

iii

Algorithm 2: CONCOL-EXEC(DU Tins−exe,Tinitial)
Data: DU Tins−exe, Tinitial
Result: Tconcolic
DU TexecT ree ← φ
for τ ∈ Tinitial do

Ptrace ← CONC-EXEC(DU Tins−exe,τ )
DU TexecT ree ← DU TexecT ree ∪ Ptrace

end
for uncovered cond c ∈ DU TexecT ree do

pc ← COND-PREDICATE(c)
ti ← CONSTRAINT-SOLVER(pc)
Tconcolic ← Tconcolic ∪ ti

end
return Tconcolic

our fuzz-engine (FUZZER) with a set of initial test-cases.
The fuzz-engine generates interesting test-cases using genetic
algorithm and explores various paths in the design. Once a
user-deﬁned time period timethresholdf expires, we invoke
the concolic engine (CONCOL-EXEC) with fuzzed test-cases
for unseen path exploration. CONCOL-EXEC identiﬁes un-
covered conditions in DU TexecT ree and forks new threads
for symbolic execution on such conditions using depth-ﬁrst
search. The concolic engine generates new test-cases satisfying
the runtime of
complex conditional statements. We limit
concolic execution engine to avoid scalability bottleneck by
timethresholdc, that monitors the time elapsed since the last
test case generated. Test-cases generated by concolic engine
are fed back to the fuzz engine for quicker exploration once
the “hard-to-cover” conditions are explored. This process halts
when either a user-deﬁned target coverage is achieved or a
user-deﬁned time limit timecutof f is reached.

, , Program Initial Test-cases ()Pre- ProcessingProgram ExecutableTarget coverage achieved?ExitContinueLLVM bitcodegenerationLightweight InstrumentationInstrumented executable = Program executionTest-cases time >  ?Mutation + CrossoverTest-case Priortization Coverage Improved?1) Update  2) Reset  3) Call coverage evaluator time >  ?Invoke Concolic engineDiscard test-caseExitUpdate DUT execTreeLocate uncover- ed conditionsPath predicate generation ()SAT-based constraintsolving1) Update  2) Reset  3) Measure coverage > ?Invoke Fuzz EngineTest-cases GreyConE test generation frameworkCoverage evaluatorYesNoYesYesNoNoNoYesPre- ProcessingGreybox fuzz engineConcolic engineTABLE I: GreyConE execution phases

TABLE II: Comparing GreyConE(GCE) with baselines

Benchmarks LOC

ADPCM
AES
FFT ﬁxed
IDCT
MD5C
Filter FIR
Interpolation
Decimation
Kasumi
UART
Quick sort

270
429
334
450
467
176
231
422
415
160
204

# Test-cases

Branch cov. (%)

Time(in s)

Benchmarks LOC

# Test-cases

Branch cov.(%)

Time taken (s)

f uzz1 conc1 f uzz2 f uzz1 conc1 f uzz2 f uzz1 conc1 f uzz2

5
3
3
62
5
3
44
3
31
2
7

6
4
3
137
3
2
-
2
23
15
-

-
4
-
-
2
7
-
-
-
4
-

93.3
100
79.2 83.3
81.2 96.9
64.8
100
87.5 90.6
76.8 87.5
-
100
100
96.8
93.3
100
81.2 84.1
-
100

-
91.7
-
-
100
93.8
-
-
-
88.5
-

9
76
29
7
9
7
3
12
10
334
14

39
29
137
222
34
23
-
22
46
136
-

-
51
-
-
7
37
-
-
-
248
-

ADPCM
AES
FFT ﬁxed
IDCT
MD5C
Filter FIR
Interpolation
Decimation
Kasumi
UART
Quick sort

270
429
334
450
467
176
231
422
415
160
204

30
23
7

AFL S2E GCE AFL S2E GCE AFL S2E GCE
48
156
166
229
50
67
3
34
56
718
14

100
374
124
91.7 1745
314
96.9 1215 2051
100
422
29
100 1329 1214
89
551
93.8
3
37
100
372 1065
100
233
58
100
88.5
730
334
14
248
100

11 96.9 94.4
11 88.7 82.4
6 96.9 96.9
199 74.1 84.2
10 90.6
70
12 93.8 93.8
100 100
44
100 100
5
54
100 100
21 81.2 85.7
7
100 100

21
8
46
110 146
45
11
36
11
5
44
5
4
76
60
54
4
7
13

IV. EXPERIMENTAL SETUP AND DESIGN

A. Experimental setup

We implement GreyConE with state-of-the-art software test-
ing tools: AFL (v2.52b)[7] and S2E[10]. For robust coverage
measurements, we cross-validated our results with coverage
measurement tools: lcov-1.13 [16], and gcov-7.5.0 [17]. Ex-
periments were on a 3.20 GHz 16 GB RAM i5 linux machine.

B. Benchmark characteristics

We evaluate GreyConE on a wide spectrum of available
SystemC benchmarks: SCBench[18] and S2CBench[19], se-
lected from a wide variety of application domains covering
many open-source hardware designs. The benchmarks con-
sidered have diverse characterization: ADPCM, FFT, IDCT
(all
image processing cores), AES, MD5C (cryptographic
cores), Quick sort (data manipulation), Decimation (ﬁlters)
and UART (communication protocols).

C. Design of experiments

We evaluate the efﬁcacy of GreyConE in two aspects: (a)
coverage improvement and (b) run-time speedup and compare
our results with state-of-the-art testing techniques: fuzz-testing
based approach (AFL) and concolic execution (S2E).

Baseline 1 (Fuzz testing): We run AFL on the benchmarks
with default setting, and randomly generated initial seed
inputs.

Baseline 2 (Concolic execution): Similarly, we run S2E
on the benchmarks with default conﬁgurations and randomly
generated concrete seed inputs.

GreyConE: We run GreyConE on the SystemC benchmarks
starting with randomly generated input test-cases. We set the
user deﬁned parameters tthresholdf = 5s and tthresholdc = 10s
for AFL and S2E respectively, excluding the execution time
needed to generate the ﬁrst seed for each test engine.

For our experiments, we set timecutof f to 2 hours and
compare baseline methods and GreyConE. Next, we discuss
the performance of GreyConE in terms of branch-coverage
improvement and run-time speedup.

Fig. 3: Coverage improvement (left) and timing speed-up
(right) by GreyConE

V. RESULTS AND EVALUATION

GreyConE invokes fuzz-engine and concolic engine inter-
changeably. We have annotated each phase of run incremen-
tally where f uzzk denotes kth execution phase of fuzz-engine.
To show the effectiveness of GreyConE, we present branch
coverage achieved in each phase along with a number of
test-cases generated in Table I. In Table II, we report the
number of test-cases generated; achievable branch coverage
within timecutof f and the earliest time taken to reach that
coverage. We compare our results with AFL [7] and S2E [10]
which are open-sourced implementations. Due to unavailabil-
ity of implementations of SESC [11] and CTSC [9], we used
S2E [10] to demonstrate the performance of concolic execution
on SystemC designs by adopting necessary changes.
1) Coverage achieved by GreyConE: Higher branch cover-
age indicates greater probability of test-cases detecting bugs
hidden in deeper program segments. As depicted in Table I,
for certain designs, GreyConE does not require conocolic
engine at all. This indicates that such benchmarks lack com-
plex “branching” conditions where fuzzer can get stuck. For
every design, GreyConE achieves maximum possible coverage
post one call to concolic engine. Here, we claim GreyConE
achieved “maximum” possible coverage as we independently
validated that “uncovered” branches are unreachable codes
(AES, FFT ﬁxed, Filter FIR and UART). We compare the
branch coverage obtained by GreyConE with other techniques
in Table II. We observe that GreyConE outperforms baseline
techniques AFL and S2E in terms of coverage achieved (Fig-
ure 3). As illustrated, the test-cases produced by GreyConE
signiﬁcantly improves the branch coverage by 3%-25.9%

iv

)

%

(

e
g
a
r
e
v
o
C

100
80
60
40
20
0

)

%

(

e
g
a
r
e
v
o
C

100
80
60
40
20
0

GreyConE
S2E
AFL

1

10

100 1,000 10,000

Time(s)

(a) ADPCM

GreyConE
S2E
AFL

1

10

100 1,000 10,000

Time(s)

(e) MD5C

100
80
60
40
20
0

100
80
60
40
20
0

GreyConE
S2E
AFL

1

10

100 1,000 10,000

Time(s)

(b) AES

GreyConE
S2E
AFL

1

10

100 1,000 10,000

Time(s)

(f) ﬁlter-FIR

100
80
60
40
20
0

100
80
60
40
20
0

GreyConE
S2E
AFL

1

10

100 1,000 10,000

Time(s)

(c) FFT-ﬁxed

GreyConE
S2E
AFL

1

10

100 1,000 10,000

Time(s)

(g) decimation

100
80
60
40
20
0

80
60
40
20
0

GreyConE
S2E
AFL

1

10

100 1,000 10,000

Time(s)

(d) IDCT

GreyConE
S2E
AFL

1

10

100 1,000 10,000

Time(s)

(h) UART

Fig. 4: Branch coverage obtained on SystemC designs while running GreyConE, S2E, AFL over a time period of two hours

compared to AFL and 2.8%-30% compared to S2E within
the two hours’ time limit. Fig. 4 provides a detailed coverage
analysis over the entire time period of two hours.
2) Analyzing run-time speedup: We measure the time taken
to obtain best achievable coverage by baseline techniques and
compare run-time speedup by GreyConE to achieve the same.
From Figure 4, the time taken to achieve a certain branch
coverage is lower bounded by GreyConE compared to AFL
and S2E. In Figure 3, we show the run-time speedup of
GreyConE over AFL and S2E for the designs where every
technique has achieved the same branch coverage within the
time-limit. We observe that GreyConE is signiﬁcantly faster
to reach a certain branch coverage. The speed-up achieved by
GreyConE are in line with our design approach: (1) GreyConE
quickly identiﬁes the region where fuzz engine gets stuck
and invoke concolic engine to solve the complex conditions;
(2) GreyConE avoids expensive path exploration by concolic
execution by using fuzzer generated seeds leading to faster
exploration and test-case generation.
3) Analyzing test-case quality: We report the number of
test-cases preserved by each technique until it reaches user-
deﬁned timeout (or, till the maximum coverage is achieved)
in Table II. A closer analysis reveal that the number of test-
cases generated by GreyConE is same as by AFL where
AFL alone sufﬁced in reaching the target coverage without
getting stuck for tthresholdf time. But, for cases where AFL
crossed the tthresholdf , GreyConE invokes concolic engine
for generating quality test-cases quickly. Similarly when S2E
gets stuck for tthresholdc, GreyConE invokes the fuzz engine.
Finally, GreyConE needs fewer test-cases than both AFL and
S2E indicating good quality test-case generation.

VI. CONCLUSION

We proposed GreyConE: an end-to-end test-generation
framework penetrating into deeper program segments of Sys-

temC designs. Our results show scalable generation of test
cases with better branch coverage and accelerated design space
exploration compared to state-of-the-art techniques. GreyConE
has alleviated the drawbacks of fuzzing and concolic execu-
tion by interleaving them. Future works include enhancing
GreyConE to low-level netlist (RTL/gate-level) and uncover
hardware speciﬁc bugs in design.

REFERENCES

[1] D. Karlsson et al., “Formal veriﬁcation of systemc designs using a petri-

net based representation,” in DATE, 2006, pp. 1228—-1233.

[2] P. Herber et al., “Model checking systemc designs using timed au-

tomata,” in CODES+ISSS, 2008, pp. 131—-136.

[3] A. Vafaei et al., “Symba: Symbolic execution at c-level for hardware

trojan activation,” in ITC, 2021, pp. 223–232.

[4] V. Herdt et al., “Verifying systemc using stateful symbolic simulation,”

in DAC, 2015, pp. 1–6.

[5] P. Godefroid et al., “Dart: Directed automated random testing,” in PLDI,

2005, pp. 213—-223.

[6] P. Godefroid, “SAGE: Whitebox fuzzing for security testing,” Queue,

vol. 10, 2012.

[7] M. Zalewski, “American fuzzy lop,” http://lcamtuf.coredump.cx/aﬂ.
[8] T. Trippel et al., “Fuzzing hardware like software,” IEEE S&P, 2021.
[9] B. Lin et al., “Concolic testing of systemc designs,” in ISQED, 2018,

pp. 1–7.

[10] V. Chipounov et al., “The s2e platform: Design, implementation, and

applications,” ACM TCS, vol. 30, 2012.

[11] B. Lin et al., “Generating high coverage tests for SystemC designs using

symbolic execution,” in ASPDAC, 2016, pp. 166–171.

[12] A. B. Chowdhury et al., “Verifuzz: Program aware fuzzing,” in TACAS,

2019, pp. 244–249.
[13] “LLVM,” https://llvm.org/.
[14] F. Bellard, “QEMU, a fast and portable dynamic translator.” in USENIX

FREENIX, 2005, p. 46.

[15] C. Cadar et al., “KLEE: unassisted and automatic generation of high-

coverage tests for complex systems programs,” in OSDI, 2008.

[16] “LCOV,” http://ltp.sourceforge.net/coverage/lcov.php.
[17] “GCov,” https://gcc.gnu.org/onlinedocs/gcc/Gcov.html.
[18] B. Lin et al., “Scbench: A benchmark design suite for systemc veriﬁ-

cation and validation,” 2018.

[19] B. C. Schafer et al., “S2cbench: Synthesizable systemc benchmark suite

for high-level synthesis,” IEEE ESL, vol. 6, 2014.

v

