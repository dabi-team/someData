2
2
0
2

l
u
J

4

]
L
F
.
s
c
[

1
v
6
1
5
1
0
.
7
0
2
2
:
v
i
X
r
a

LEARNING STATE MACHINES VIA EFFICIENT HASHING OF
FUTURE TRACES

Robert Baumgartner, Sicco Verwer
Intelligent Systems
Delft University of Technology
Delft, The Netherlands
{r.baumgartner-1, s.e.verwer}@tudelft.nl

ABSTRACT

State machines are popular models to model and visualize discrete systems such as software systems,
and to represent regular grammars. Most algorithms that passively learn state machines from data
assume all the data to be available from the beginning and they load this data into memory. This
makes it hard to apply them to continuously streaming data and results in large memory requirements
when dealing with large datasets. In this paper we propose a method to learn state machines from
data streams using the count-min-sketch data structure to reduce memory requirements. We apply
state merging using the well-known red-blue-framework to reduce the search space. We implemented
our approach in an established framework for learning state machines, and evaluated it on a well
know dataset to provide experimental data, showing the effectiveness of our approach with respect to
quality of the results and run-time.

Keywords State Machine Learning · Count-Min-Sketch · Data Streams

1

Introduction

State machines are a well known means to model discrete systems and regular grammars. When learned from data,
they provide a means to describe the underlying dynamics, as well as a method to visualize the behavior [1]. State
machines can be learned in different manners from data. One of the most well known ways to identify state machines
is the evidence-driven-state-merging algorithm, a method using statistical evidence to ﬁnd similarities in states of a
state machine based on heuristics. We assume familiarity with standard algorithms for learning state machines from
data. We refer the reader to [2] for an introduction. A drawback of most state machine learning algorithms, including
evidence-driven-state-merging, is the fact that they require all input data at once in one pass, thus they cannot be learned
in an adaptive manner. This also leads to a large memory footprint for large inputs, since data has to be stored all at once.
Some of the few works tackling these two issues are from [3, 4, 5]. In this paper we propose an alternative approach to
overcome these two limitations, and introduce a heuristic that makes use of count-min-sketches to efﬁciently learn state
machines from data-streams in an adaptive manner.

In each state, a count-min-sketch stores counts of hashes of observed futures. These futures are either subsequences
starting in that state or sliding windows. Storing only counts allows us to process data streams, as we can forget most of
the future sequences that are possible after reaching a state. The count-min-sketch should of course provide a sufﬁciently
good estimate of a states’ future behavior. We therefore only allow merges between states with sufﬁcient counts,
determined by a threshold. To reduce memory, we only store the red core, the blue fringe, and the ﬁrst layer of white
states. As a consequence, the state-merging methods are much more efﬁcient as every merge only induces a handful of
additional ones due to the determinization/folding routines. The behavior in all other states is estimated by the sketches.
Although this greatly reduces the number of states in memory, we show in experiments on the PAutomaC dataset [6]
that this provides good estimates. All of our source code will be released open source as part of the FlexFringe tool [7].

 
 
 
 
 
 
Learning state machines via efﬁcient hashing of future traces

2 Related Work

In the literature, several approaches exist for learning state machines. Active learning learns a model via actively asking
queries [8, 9], where the learner can actively make queries to the system to extract information and pose a hypothesis.
A drawback of this method however is that it assumes the presence of an oracle that processes and answers the queries
asked. An efﬁcient approach to learn from traces is [10], where they reformulated the problem and used SAT-solvers to
learn deterministic ﬁnite automata (DFA). The method is shown to solve the problem exactly, however, it becomes
inefﬁcient on larger datasets. Another approach to learn state machines from input traces is by means of state merging.
In this case a preﬁx tree, the so called Augmented Preﬁx Tree Acceptor (APTA), is built, which describes the set of
input traces exactly. In line with Occam’s razor the goal of state merging then is to minimize the APTA while still
representing the set of input traces. The algorithm achieves this via ﬁnding pairs of states that show similar behavior
and then merging them. Although shown to be NP-hard [11], much research uses a state merging method, see e.g., [12].

A popular approach is the evidence-driven-state-merging algorithm (EDSM) [13]. The classical Alergia algorithm [14]
is a version of state merging that uses statistical tests to compare and merge. The k-tails algorithm performs merges but
requires identical futures up to a given depth k [15]. Updated search procedures improve the quality of state machines
as well as improve run-time [16, 17, 18]. Other types of state machines can be learned via specialized algorithms,
such as timed automata via the RTI algorithm [19] or a likelihood-ratio test [20], or learning extended ﬁnite state
machines [21] or guarded ﬁnite state machines [22]. Vodenˇcarevi´c et al. proposed an algorithm that learns a hybrid
state machine model by taking different aspects of a system into account during the learning process [23].

Despite the vast amount of research done on learning state machines, most of the state merging procedures assume
that the complete data is available at the start of the program. Little work can be found on learning state machines in
a streamed fashion. In [5] a method is presented that uses frequent pattern data stream mining techniques to make a
streaming state machine learner. [3] learns state machines using modiﬁed Space-Saving sketches, and prove properties
such as convergence and memory consumption. This work was extended with a parameter search strategy in [4]. In
[24] a streamed merging method method was implemented in the Apache framework, also based on count-min-sketches
to hash futures. Our approach is largely inspired by these two works.

3 Background

3.1 Probabilistic deterministic ﬁnite automata

A PDFA is a tuple A = {Σ, Q, q0, δ, S, F }, where Σ is a ﬁnite alphabet, Q is a ﬁnite set of states, q0 ∈ Q is a
unique starting state, δ : Q × Σ → Q ∪ {0} is the transition function, S : Q × Σ → [0, 1] is the symbol probability
function, and F : Q → [0, 1] is the ﬁnal probability function, such that F (q) + (cid:80)
a∈Σ S(q, a) = 1 for all q ∈ Q.
Given a sequence of symbols s = a1, . . . , an, a PDFA can be used to compute the probability for the given sequence:
A(s) = S(q0, a1) · S(q1, a2) · . . . · S(qn−1, an) · F (qn), where δ(qi, ai+1) = qi+1 for all 0 ≤ i ≤ n − 1. A PDFA
is called probabilistic because it assigns probabilities based on the symbol S and ﬁnal F probability functions. It is
called deterministic because the transition function δ (and hence its structure) is deterministic. A PDFA computes a
probability distribution over Σ∗, i.e., (cid:80)

s∈Σ∗ A(s) = 1.

3.2 Learning State Machine Models

We focus on learning state machines via state merging [13]. Classical state merging starts by constructing a tree
representing the input data, also called the APTA. The APTA is a tree accepting the input traces exactly. The goal
of state merging then is to minimize this APTA iteratively while still representing the data. This is analogous to the
generalization process in other machine learning algorithms. Minimization is done via comparing pairs of states on
behavior and merging states with similar futures. Since in each step multiple merges can be possible, the heuristic
computes a score. The highest scoring possible merge will be performed. In our work, we also employ the red-blue
framework. The red-blue framework maintains a core of red states, with initially only the root state red. Non-red states
that have a direct transition from a red state are called blue states, states that are neither red nor blue are called white. A
candidate merge pair can then only be a pair of a red and a blue state.

2

Learning state machines via efﬁcient hashing of future traces

4 Methodology

4.1 The heuristic

In order to efﬁciently store the future of states, our algorithm uses the count-min-sketch (CMS) data structure [25]. The
main idea is to store the states’ future with constant memory footprint per state, at the potential cost of approximation
errors. It works via using a matrix, and the columns represent counts of hashed items. Since the hashing can collide,
multiple rows are instantiated, each with its own associated hash function. Upon retrieving counts of an element e off
the CMS, e is hashed again for each row, and the minimum count in the columns is considered the best approximation
of the counts. Fig. 1 shows the storage of a CMS. An element, in this case a tuple 4, 3, is being stored and thrown into
each row’s hash function respectively. The increased counts are highlighted in red.

(a) Before store.

(b) After store.

Figure 1: A CMS before and after storing an element. The indices the hash functions hash the element onto are shown
to the left.

We use nfutures CMS per state, one per future. If for example we take state S8 from Fig. 2 and set nfutures to 3, the 3
sketches would look like the following: Sketch 1 would store a 2 and a 3, sketch 2 would store a 4 and a 15, sketch 3
would store a 3 and a 1. Additionally to the normal outgoing symbols, we save an extra dedicated bin in the sketch
for terminating sequences. Whenever a sequence terminates in a state, we store a "sequence termination" on the last
column of the sketch, and only terminations can be stored in that column.

Figure 2: An excerpt of a preﬁx tree.

Whenever we want to merge a state, all we have to do them is to compare the corresponding sketch pairs, i.e., sketch
1 of the red state with sketch 1 of the blue state, sketch 2 of the red state with sketch 2 of the blue state, and so on.
We consider the rows of the CMS distributions, and perform the Hoeffding-bound similar to [14] (Eq. 1) for each
corresponding pair of rows of the two sketches. In this equation, xi
is the relative frequency of element i in distribution
n1
1, and yi
it’s frequency in distribution 2. α is a hyperparameter to be set. Apart from checking whether two states are
n2
can be merged, we also need a score function. In order to assign a score to a merge, we consider the rows of the CMS
r
l and (cid:126)vi
as vectors, and then compute the sum of the cosine-similarities of each corresponding vector (row) pair (cid:126)vi
(Eq. 2) in between two sketches, averaged over the number of rows each sketch has. Furthermore, when performing or
undoing a merge on two states, we need to update the state information. To this end, we simply treat the sketches of the
individual states as matrices, which we sum up on a merge.

3

Learning state machines via efﬁcient hashing of future traces

Algorithm 1: Streaming
Input: Set of sequences X, batch size b, threshold t
Output: A hypothesis (automaton) H

1 H ← root node;
2 c ← 0;
3 foreach xi ∈ X do
n ← root node;
4
foreach sj ∈ xi do

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

// sj are the individual symbols of string xi;
n.size ← n.size + 1;
if transition from n with sj exists then

n = δ(n, sj);

else if n is red or n is blue then

create new node satisfying δ(n, sj);
else if n.parent is red and n.size == t then

mark n blue;

else

// this is a white state;
do nothing;

c ← c + 1;
if c == b then

while merges_possible(H) do
perf orm_merges(H);

c ← 0;

(cid:115)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

xi
n1

−

yi
n2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

<

1
2

log

(cid:18) 2
α

(cid:19) (cid:18) 1
√
n1

+

1
√
n2

(cid:19)

,

cosine_sim

(cid:16)

(cid:126)vi

l, (cid:126)vi

r(cid:17)

=

r

l · (cid:126)vi
(cid:126)vi
l||||(cid:126)vi
||(cid:126)vi

,

r||

(1)

(2)

4.2 Streaming

In order to stream state machines we decided for an approach similar to the one in [3]. We adopt the red-blue framework
and include an evidence threshold sink_count. Our streaming starts with the root node as a red node. Differently to
the normal batch-wise approach we only create new states with direct transitions coming from red or blue states. We
furthermore introduce a threshold for states. Every time an incoming sequence passes a state Si, we increase a counter
on that state by one. Only when the counter of a state passes a threshold can that state become a blue state and be
considered for a merge. Merges happen only in batches. Once the batch size is reached, we perform merges until no
more are possible, then we read the next batch. The streaming procedure is described in Alg. 1.

5 Experiments and results

In order to evaluate our approach, we implemented our streaming and our heuristic as modules in Flexfringe [7, 26].
In order to compare our approach, we compared it with a baseline approach, namely FlexFringe’s implementation of
the Alergia algorithm. The streaming procedure described in Alg. 1 is generic enough to use any merge heuristic that
supports a consistency_check and an assign_score procedure, and thus our implementation allows us to exchange
the sketching and Alergia heuristic very easily. The dataset we used is the well known PAutomaC dataset [6]. We set
the batchsize b to 500 and the threshold t to 100 to give the statistical tests enough evidence, and ran both the sketching
with different future lengths nfutures , and the Alergia algorithm. In order to have a better comparison, we augmented
the Alergia algorithm with the well known k-tails algorithm [15], from which we use the k parameter. Since we only
append to red and blue states, a value of k greater than 1 would be meaningless, hence we only used k = 0 and k = 1.

4

Learning state machines via efﬁcient hashing of future traces

(a) Alergia with varying k.

(b) Sketching heuristic with different nfutures values.

(c) Sketching heuristic with nfutures = 2 vs. Alergia with
k = 1.

(d) Sketching heuristic with nfutures = 3 in stream mode vs.
Alergia with k = 2 in batch mode.

Figure 3: Perplexity score errors on all experiments. The x-axis indicates the dataset’s scenarios from 1 to 48

Our performance metric is the difference in perplexities of the original automaton and our learned automaton, as
described in [6]. Fig. 3(a) shows the streamed Alergia results with the two different values for k. It is clear that k == 1
performs better. In order to check how the sketching algorithm scales with respect to nfutures , we plot the perplexities
in a similar fashion, where we only vary the nfutures parameter this time. The plots are shown in Fig. 3(b). Increasing
nfutures from 1 to 2 has a large impact, similar to increasing k from 0 to 1 in Alergia. Increasing the value to 3 improves
some problems minimally, however we noticed that from nfutures = 4 on the quality of the resulting state machine
starts to degrade. Also note that the sketching and Alergia have peaks at the same problems in their weaker settings, and
both improve drastically on the same problems when giving them more information. In a next step we compare the
sketching with Alergia in Fig. 3(c). We can see that the sketching performs better than Alergia on most problems,
which is also evidenced by the average error of 2.34 for the sketching in this setting and 3.14 for Alergia. Increasing
nfutures to 3 would result in an average error of 2.29 in our experiment for the sketching, where it performed always
equal or minimally better than the same with nfutures = 2.

We also compared runtime, where we found almost identical times. While sketching took 36.8s in total on all problems
with nfutures = 2 on our machine (Ubuntu 20.4, Intel i7@2.60Ghz and 16GB RAM), Alergia took 36.0s with k = 1.
Increasing nfutures to 3 increased the runtime to 38.8s, the k parameter had little effect on runtime from 0 to 1, with a
runtime of 35.3s at k = 0. Last but not least it is worth noting that while Alergia’s performance increased with larger k,
doing the same with the sketching approach actually resulted in worse results. In order to compare the batch mode
with the stream mode we ran Alergia in k = 2 in batch mode and the sketching with nfutures = 3 in stream mode. The
comparison is depicted in Fig. 3(d). It can be seen that the streamed version compares worse than the batched version,
which is also imminent from the average error of 0.63 for the batch-mode. However, this comes at the cost of much
larger memory and runtime consumption. While the streamed version ran for less than 40s, the batch mode took 258s,
or more than 4 minutes. Memory consumption wise the batched version is also much more costly, as can be seen in
Table 1, where we compare memory footprint of batch mode Alergia and stream mode sketching on a few selected
PAutomaC scenarios. To get the consumption we measured dynamic memory allocation via the Massif tool from the
Valgrind toolkit.

5

Learning state machines via efﬁcient hashing of future traces

Method
Alergia batch
Sketching stream

Scenario 8
1GB
95MB

Scenario 9
58.47MB
17.4MB

Scenario 20
957.7MB
35.53MB

Scenario 21
2.049GB
56.58MB

Table 1: Batch vs. stream mode memory consumption per maximum heap size for a few selected scenarios.

6 Discussion and limitations

At ﬁrst we expect the gain in performance for Alergia with increasing k. In batch mode, up to a value of k = 2
performance slightly increases, then it enters a plateau. Apparently, for the PAutomaC dataset, it is sufﬁcient to know
the next 3 steps ahead. The decrease in the performance of our sketching with nfutures > 3 can be possibly explained
by the fact that the algorithm will prefer non-optimal merges, since the sketches "pool" the future subtrees together for
the score computation.

The advantage of our sketching approach lies in the fact that we can look ahead further than other heuristics like Alergia
while still discarding information, i.e., the states themselves. We also approximate infrequently used symbols, unlike
[3], who use a modiﬁcation of the Space-Saving-Algorithm to approximate the most frequent features only. A drawback
of our sketching is though that with too large alphabet size our approximations can collide too much, leading to worse
results. We are not sure yet why our method’s results degrade with higher k larger than 0. We can only provide a
hypothesis at this stage. In order to do ktails and with our way of streaming, pairs of white states with transitions
coming out of blue states have to be compared. Those states can, since they are usually infrequent, possibly not provide
much evidence, leading to incorrect results on our statistical tests. Limitations of this work are mainly the nature of the
dataset, which appears simple enough that not many lookaheads and simple methods are enough to get good results.
But our results to demonstrate that the approach works. Another limitation is the size of the dataset, which are small
for streaming algorithms. In the future, we aim to test our method on different data sets such as network trafﬁc and
software logs, which are typically too large to ﬁt into memory.

7 Conclusion

In this work we introduced a new approach for streaming state machines and showed ﬁrst experimental results. We
compared our sketching approach with a conventional method and pointed out advantages and disadvantages based on
the results we got. We conclude that our method works as expected, and we expect the advantages to show better on
larger datasets where deeper lookaheads into the future of a state are necessary to make good predictions. The streaming
also enables the processing of really large data in a cost effective manner. A drawback of our method however is clearly
that the sketches have a ﬁxed size when starting the algorithm, and thus the number of potential symbols the traces may
contain must be known or estimated before running the algorithm. When set too small, the results could get impaired by
collisions from inside the CMS.

8 Ackknowledgements

This work is supported by NWO TTW VIDI project 17541 - Learning state machines from infrequent software traces
(LIMIT).

References

[1] Christian Hammerschmidt, Sicco Verwer, Qin Lin, and Radu State. Interpreting ﬁnite automata for sequential

data. 12 2016.

[2] Colin de la Higuera. Grammatical Inference: Learning Automata and Grammars. Cambridge University Press,

2010.

[3] Borja Balle, Jorge Castro, and Ricard Gavaldà. Bootstrapping and learning pdfa in data streams. In Jeffrey Heinz,
Colin Higuera, and Tim Oates, editors, Proceedings of the Eleventh International Conference on Grammatical
Inference, volume 21 of Proceedings of Machine Learning Research, pages 34–48, University of Maryland,
College Park, MD, USA, 05–08 Sep 2012. PMLR.

[4] Borja Balle, Jorge Castro, and Ricard Gavaldà. Adaptively learning probabilistic deterministic automata from data

streams. Mach Learn, 96:99–127, 2014.

6

Learning state machines via efﬁcient hashing of future traces

[5] Jana Schmidt and Stefan Kramer. Online induction of probabilistic real-time automata. Journal of Computer

Science and Technology, 29(3):345–360, 2014.

[6] Sicco Verwer, Rémi Eyraud, and Colin De La Higuera. PAutomaC: A probabilistic automata and hidden Markov

models learning competition. Machine Learning, 96(1-2):129–154, oct 2014.

[7] Sicco Verwer and Christian A. Hammerschmidt. ﬂexfringe: A passive automaton learning package. In 2017 IEEE

International Conference on Software Maintenance and Evolution (ICSME), pages 638–642, 2017.

[8] Dana Angluin. Queries and concept learning. Machine Learning, 2:319–342, 1988.
[9] Frits Vaandrager. Model learning. Communications of the ACM, 60(2):86–95, 2017.
[10] Marijn J. H. Heule and Sicco Verwer. Exact dfa identiﬁcation using sat solvers. In José M. Sempere and Pedro
García, editors, Grammatical Inference: Theoretical Results and Applications, pages 66–79, Berlin, Heidelberg,
2010. Springer Berlin Heidelberg.

[11] E Mark Gold. Complexity of automaton identiﬁcation from given data. Information and Control, 37(3):302–320,

1978.

[12] Colin de la Higuera. A bibliographical study of grammatical inference. Pattern Recognition, 38(9):1332–1348,

2005. Grammatical Inference.

[13] Kevin J. Lang, Barak A. Pearlmutter, and Rodney A. Price. Results of the abbadingo one DFA learning competition
and a new evidence-driven state merging algorithm. In Lecture Notes in Computer Science (including subseries
Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), volume 1433, pages 1–12. Springer
Verlag, 1998.

[14] Rafael C. Carrasco and Jose Oncina. Learning stochastic regular grammars by means of a state merging method.
In Rafael C. Carrasco and Jose Oncina, editors, Grammatical Inference and Applications, pages 139–152, Berlin,
Heidelberg, 1994. Springer Berlin Heidelberg.

[15] A. W. Biermann and J. A. Feldman. On the synthesis of ﬁnite-state machines from samples of their behavior.

IEEE Transactions on Computers, C-21(6):592–597, 1972.

[16] A.L. Oliveira and J.P.M. Silva. Efﬁcient search techniques for the inference of minimum size ﬁnite automata. In
Proceedings. String Processing and Information Retrieval: A South American Symposium (Cat. No.98EX207),
pages 81–89, 1998.

[17] John Abela, François Coste, and Sandro Spina. Mutually compatible and incompatible merges for the search of

the smallest consistent dfa. volume 3264, pages 28–39, 10 2004.

[18] Kevin Lang. Faster algorithms for ﬁnding minimal consistent dfas. 01 2000.
[19] Sicco Verwer, Mathijs Weerdt, and Cees Witteveen. Efﬁciently identifying deterministic real-time automata from

labeled data. Machine Learning, 86:295–333, 03 2012.

[20] Sicco Verwer, M. D. Weerdt, and Cees Witteveen. A likelihood-ratio test for identifying probabilistic deterministic

real-time automata from positive data. In ICGI, 2010.

[21] Neil Walkinshaw, Ramsay Taylor, and John Derrick. Inferring extended ﬁnite state machine models from software

executions. Empirical Software Engineering, 21(3):811–853, 2016.

[22] Leonardo Mariani, Mauro Pezzè, and Mauro Santoro. Gk-tail+ an efﬁcient approach to learn software models.

IEEE Transactions on Software Engineering, 43(8):715–738, 2017.

[23] Asmir Vodenˇcarevi´c, Hans Kleine Bürring, Oliver Niggemann, and Alexander Maier. Identifying behavior models

for process plants. In ETFA2011, pages 1–8, 2011.

[24] Hans Schouten. Learning State Machines from data streams and an application in network-based threat detection.

Technical report, 2018.

[25] Graham Cormode and S. Muthukrishnan. An improved data stream summary: the count-min sketch and its

applications. Journal of Algorithms, 55(1):58–75, 2005.

[26] Sicco Verwer and Christopher Hammerschmidt. Flexfringe. https://github.com/tudelft-cda-lab/

FlexFringe.

7

