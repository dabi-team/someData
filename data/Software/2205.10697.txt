2
2
0
2

p
e
S
6
2

]
L
M

.
t
a
t
s
[

4
v
7
9
6
0
1
.
5
0
2
2
:
v
i
X
r
a

The Selectively Adaptive Lasso

Alejandro Schuler, Yi Li, Mark van der Laan

September 28, 2022

Abstract

Machine learning regression methods allow estimation of functions without unrealistic parametric
assumptions. Although they can perform exceptionally in prediction error, most lack theoretical con-
vergence rates necessary for semi-parametric eﬃcient estimation (e.g. TMLE, AIPW) of parameters like
average treatment eﬀects. The Highly Adaptive Lasso (HAL) is the only regression method proven to
converge quickly enough for a meaningfully large class of functions, independent of the dimensionality
of the predictors. Unfortunately, HAL is not computationally scalable.
In this paper we build upon
the theory of HAL to construct the Selectively Adaptive Lasso (SAL), a new algorithm which retains
HAL’s dimension-free, nonparametric convergence rate but which also scales computationally to mas-
sive datasets. To accomplish this, we prove some general theoretical results pertaining to empirical loss
minimization in nested Donsker classes. Our resulting algorithm is a form of gradient tree boosting
with an adaptive learning rate, which makes it fast and trivial to implement with oﬀ-the-shelf software.
Finally, we show that our algorithm retains the performance of standard gradient boosting on a diverse
group of real-world datasets. SAL makes semi-parametric eﬃcient estimators practically possible and
theoretically justiﬁable in many big data settings.

1 Introduction

In regression our task is to ﬁnd a function that maps features to an outcome or score such that the
expected loss is minimized [1]. In the past decades a huge number of ﬂexible regression methods have
been developed that eﬀectively search over high- or inﬁnite-dimensional function spaces (these are often
collectively called “machine learning” methods for regression).

In statistical inference, our task is more broadly to estimate the value of some aspect of an unknown
probability distribution (e.g. a mean) and to quantify a measure of uncertainty about our estimate
that comes from having drawn random data from this distribution. Modern statistical theory provides
methods for doing this without imposing unrealistic assumptions on the data-generating mechanism (e.g.
normality, linearity) [2]. Moreover, under certain conditions, some of these methods provide optimality
in the sense that no other estimator has less sampling variance; such estimators are called "eﬃcient",
or "semi/non-parametric eﬃcient" when eﬃciency holds without parametric assumptions [3, 4]. When
combined with causal identiﬁcation results, these methods allow us to most eﬃciently answer important
real-world causal questions from complex data structures: for example, assessing whether a drug improves
cancer survival despite self-selection into treatment and some outcome censoring.

In general, however, the beneﬁcial properties of these estimators can only be guaranteed if certain
regressions can be made to converge quickly to their true optima. For example, when estimating the
average treatment eﬀect from observational data, a suﬃcient condition for nonparametric eﬃciency of
the TMLE and AIPW estimators is that the propensity score and outcome regression models converge
to their respective truths in root-mean-square generalization error at a rate of oP (n−1/4) [3, 4]. Thus
the development of fast-converging, nonparametric regression methods is critical for eﬃcient statistical
inference.

Without putting unrealistic smoothness conditions on the regression functions, it is diﬃcult to prove
rates faster than oP (n−1/4). In fact, there are general theorems that show that attaining such a rate is im-
possible if we assume unrestricted high-dimensional function spaces without such smoothness conditions
[5]. This is often referred to as the curse of dimensionality [6, 7].

This curse of dimensionality made eﬃcient inference in observational studies theoretically diﬃcult to
guarantee in a general nonparametric setting. In 2016, however, Benkeser and van der Laan [8] showed

1

 
 
 
 
 
 
that dimension-free oP (n−1/4) rates (or better) are attainable under nonrestrictive assumptions with
a regression method called the Highly Adaptive Lasso (HAL). Instead of assuming smoothness, HAL
assumes that the true function is cadlag of bounded sectional variation. Roughly speaking, this restricts
the amount the true function can go up and down over its domain in a total, global sense, instead of
restricting its behavior locally. HAL therefore imposes no scientiﬁcally meaningful assumptions because
(a) this variation norm bound can be arbitrarily large and (b) cadlag functions are extremely general
(e.g. even allowing discontinuities). To our knowledge, HAL is the only method that provides rates fast
enough for eﬃcient inference in any realistic nonparametric setting [9].

Unfortunately, as we will shortly see, the HAL algorithm is practically infeasible even for moderately
sized problems. So, somewhat ironically, statistical scalability comes at the cost of computational scal-
ability. On the other hand, many popular machine learning methods are massively scalable but come
without the required theoretical guarantees. Gradient-boosted trees in particular have proven consis-
tently successful in real-world prediction settings [10, 11, 12].

Our task in this paper is to marry these two kinds of advantages. We build upon the theory of HAL
to construct a new gradient-boosted tree algorithm that retains HAL’s convergence rate in the same
nonparametric setting but which scales computationally to massive datasets. To accomplish this, we
prove two general theoretical results pertaining to empirical loss minimization in nested Donsker classes.

Outline We begin by reviewing the mechanics and theory of HAL (section 2). We then motivate and
state our new theoretical results (sections 3.1, 3.1.2) and explore their implications in the construction
of a new algorithm (SAL) which retains the HAL rate (section 3.2). Lastly, we demonstrate SAL’s
performance in real-world datasets (section 4) and conclude with a brief discussion (section 5).

P

f (Z)dP and Pnf = 1
n

Notation and Preliminaries Throughout the paper we adopt the empirical process notation P f =
i f (Zi). In this notation these operators do not average over any potential
randomness in f , so, for example, P fn is a random variable if fn is a random function learned from data.
R
We reserve ˜Pnfn for the case where fn is random, but independent from the random measure ˜Pn (e.g.
when ˜Pn is the empirical measure of a validation set). We use
P f 2 unless
otherwise noted. To indicate the probability measure of a set we use braces, e.g. P

f
k
R be IID across i and with a generic X, Y that have some joint distribution
Let Xi, Yi
= [0, 1]p without loss of generality for applications with bounded covariates. Let
P . We will take
f = arg minf :X →R P L(f ; X, Y ) for some loss L (e.g. mean-squared error), which from now on we invoke
in abbreviated form L(f ) or Lf . This is the standard regression setup where our goal is to estimate the
function f from n samples of the vector of predictors X and the outcome Y .

to indicate an L2 norm
Z < c

∈ X ×
X

.
p
}

k

{

Throughout, we presume that the truth f is a cadlag function of sectional variation norm (deﬁned in
(M ). We omit notating the bound

what follows) bounded by M and refer to this set of functions as
M where it is clear from context or not important to the immediate argument.

F

Much of our theory relies on ﬁnding the a function that exactly minimizes empirical loss (i.e. train-
ing error) in a class of functions. This is easy to do in parametric function classes but diﬃcult in
nonparametric ones. Fortunately we are sometimes able to solve the latter problem with the for-
are two sets and we have that
mer by establishing that two classes share a minimizer.

If

arg minf ∈F PnLf

F
arg ming∈G PnLg we will use the notation

.

G

F

“loss-based dissimilarity”) P (Lfn

⊂
are minimizers in
In our theoretical results we give convergence rates of some estimator fn in terms of the divergence (or
2 =
k
f )2 of regressions in order to prove eﬃciency in estimating some target parameter. The reader
should note that convergence in divergence typically implies L2 convergence at the same rate under very
p
mild conditions on the loss function1 which are satisﬁed in all standard applications [13].

Lf ). However, we are usually interested in the L2 norm

fn
k

⊂ G

P (fn

−

−

−

F

f

to indicate that minimizers in

and
G
↓PnL

2 Highly Adaptive Lasso (HAL)

We begin with a description of HAL, which is fairly simple in its mechanics yet rich in theory.

1When the loss is mean-squared-error, we see directly that P (Lfn − Lf ) = P (fn − f )2 via iterated expectation (conditioning

on X) and recognizing f = E[Y |X] in this case.

2

2.1 Algorithm

Mechanically, HAL is just lasso [14], but using a special, transformed set of covariates. That is, instead
of ﬁtting lasso with the data (X, Y )n
Kn is a particular
}
function that expands the covariates X into a larger set of binary predictors. Hn is not ﬁxed a-priori
(thus the subscript n), but is instead adaptively determined by the full dataset (X)n
i , as we will describe
shortly.

i , we pass in (Hn(X), Y )n

i where Hn :

X → {

0, 1

Let hs,c :

X → {

0, 1

}

represent one of the Kn elements of Hn so that Hn(X) = [hs1,c1 (X), . . . hsKn ,cKn (X)]⊤.

≥

We refer to these hs,c as basis functions, or just bases. Each basis is indexed by a “section” s
which is just some subset of the covariates, and a knot point c
hs,c(X) = Πj∈s1(Xj
of the s-section of the knot c. For a single knot c we can construct up to 2p
corresponds to a diﬀerent possible subset of the raw covariates.

,
}
. The (s, c) basis is given by
cj ). Geometrically, hs,c(X) tells us whether or not X is “up and to the right”
1 bases, each of which

1 . . . p

∈ X

⊂ {

Alternatively, we can index these bases with a vector-valued knot point c

)p in
x) where the inequality is taken to hold in each dimension. Bases of
which case we have hc = 1(c
this kind can be thought of as indicators of the positive orthant in Cartesian space, translated by some
(possibly inﬁnite) amount in each coordinate. For that reason, we’ll refer to these as “positive orthant
bases”.

∪ {±∞}

¯Rp = (R

−

≤

∈

For HAL, we deﬁne Hn as the set of bases

. In other words, we take all
bases that can be constructed using each observed Xi as a knot point, using all possible sections. This
generates a maximum of Kn = n(2p
1) unique bases which includes an intercept h∅,·. In this way the
n
1 and is thus data-adaptive.
precise set of bases depends on the observed values
}

When we apply the transformation Hn row-wise to the covariate matrix (X)n

output (Hn(X))n
that we then feed into the standard lasso algorithm, which produces the estimated function

i we can represent the
i as a binary matrix with n rows and Kn columns. These are the transformed predictors

hs,c : s
{

Xi
{

1 . . . p

⊂ {

, c

−

∈

}

(X)n
i }

fn = arg min
f ∈Fn(M )

PnLf

n(M ) =

F

Hn(x)⊤β
β
s.t.
1
k

k

(cid:26)

M

≤

(cid:27)

(1)

(2)

for a vector of coeﬃcients β

RKn and a desired L1 bound M that directly corresponds to a
). The user
regularization parameter λ in a penalized regression fn = arg minf ∈Fn(∞) (PnLf + λ
|
must specify the data and the desired regularization λ. HAL can therefore be easily implemented as a
wrapper around existing lasso software, e.g. glmnet [15].

β(f )

∈

k

2.2 Theory

F

Although HAL is beguilingly simple in its mechanics, it is extremely powerful theoretically. To understand
this we must return to our class

of cadlag functions of bounded sectional variation norm.

“Cadlag” (continu à droite, limites à gauche) means right-continuous with left limits. So, for example,
all continuous functions are cadlag. Cadlag functions correspond 1-to-1 with signed measures the same
way that cumulative distribution functions correspond with probability measures. The sectional variation
norm2 of a cadlag function f on [0, 1]p is given by
f
where the xs
k
notation means that all elements of x not in the set of coordinates s are ﬁxed at 0. Intuitively, a variation
norm is a way of measuring how much a function goes up and down in total over its entire domain. For
a function of one variable, the variation norm is the total “elevation gain” plus “elevation loss” over the
domain, up to a constant.

ν = f (0) +
k

1s
0s |
R

df (xs)

s∈{1...p}

P

|

F

(M ) is therefore an extremely rich class of functions: f

(M ) may be highly nonlinear and
(M ) tend to be pathological, e.g. f (x) = cos(1/x).
even discontinuous. Functions that are not in
(M ) generally does not constitute a scientiﬁcally meaningful restriction of
That is why assuming f
possibilities for large enough M . However,
(M ) happens to be “small” enough that it is a Donsker
class, meaning that empirical processes indexed by this class of functions converge in distribution to a
limit that is a tight Gaussian process [2]. This is key to proving HAL’s fast convergence rate.

∈ F

∈ F

F

F

Benkeser and van der Laan [8] and Fang et al. [7] give a concise argument that the lasso solution fn

as deﬁned above (eq. 3) is in fact one of the exact solutions of the functional optimization problem:

2The sectional variation norm is also called “Hardy-Krause variation” and is distinct from, but related to the more common

notion of total variation norm (also known as “Vitali variation”) [7].

3

f ∗
n = arg min
f ∈F (M )

PnLf

(M ) =

F

f
s.t.

∈ F
f
ν
k
k

(

M

≤

(3)

(4)

↓PnL

Or, in our notation,

(M ). This result follows from an expanded representation of
the sectional variation norm for cadlag functions. Thus HAL elegantly reduces the functional (inﬁnite-
dimensional) optimization problem to a ﬁnite-dimensional one. If the loss L corresponds to a negative
log-likelihood, HAL is a nonparametric maximum likelihood estimator [16].

⊂ F

n(M )

F

F

Since

(M ) is Donsker and fn is the empirical minimizer of the loss in the class, generic results from
empirical process theory establish that fn converges in mean-squared error to f at a root-n rate provided
(M ) [8, 2]. Speciﬁcally,
that L is mean-squared error loss and M is chosen large enough that f
Lf )2 = oP (n−1/2). A more involved proof based on the entropy integral of the
we have that P (Lfn
(M ) shows that this rate can even be improved to OP (n−1/3(log n)2(p−1)/3) [13]. These
function class
rates also generalize to most other commonly-used loss functions [13]. Astoundingly, the convergence
speed does not depend on the covariate dimension p (or does, but only through a log factor), meaning
that HAL eﬀectively breaks the curse of dimensionality.

∈ F

−

F

HAL may not be the only algorithm that accomplishes the goal of fast rates on nonparametric
regression (although it is the only one to date). To see why, it helps to break down HAL’s success into
three properties of the class of cadlag functions of bounded sectional variation norm:

1. This class is nonparametric and expressive

2. This class is Donsker

3. There exists an algorithm that can perform empirical loss minimization in this class

(1) means that almost any function of interest in the natural world is contained in this class so we
are essentially always correct in assuming that it contains the true regression function. (2) means that if
it were possible to do empirical loss minimization in this class, we’d be able to get a rate of OP (n−1/2)
or faster by classical results in empirical process theory. (3) Gives us a way to do that minimization in
practice.

Taken together, this shows that fast rates on regression are attainable if we have any nonparametric
Donsker class that admits a practical algorithm for empirical loss minimization. The class of cadlag
functions of bounded sectional variation is one such example but there may be others!

Notwithstanding future developments, HAL may also have some beneﬁcial properties that are unique
to it. Recent work has shown how HAL by itself can be leveraged for inference without the aid of
bias-correcting schemes like TMLE [17]. It is well known that unbiased plug-in estimation is generally
not possible with other regression methods [18]. HAL succeeds in this task because it asymptotically
solves all score equations in a semiparametric model where the functional components are cadlag and of
bounded variation.

3 Selectively Adaptive Lasso (SAL)

HAL boasts impressive statistical properties that make it stand alone among methods for supervised
learning. Unfortunately, it is not computationally feasible to use HAL for high dimensional data because
it involves solving a lasso problem with up to n(2p
1) predictors. For example, consider a moderately
−
sized problem where we would like to use electronic health records from 2,000 patients to predict expected
survival time from a panel of 20 continuous biomarkers. The number of constructed predictors for this
problem numbers over two billion. That makes it diﬃcult to instantiate the required matrix in memory,
let alone compute on it.

The point of our new algorithm, which we call the Selectively Adaptive Lasso (SAL) is to retain all of
the impressive statistical properties of HAL without paying the computational price. The fundamental
idea that makes SAL computationally practical and performant is early-stopping validation with data-
adaptive, expressive bases. We’ll ﬁrst explore what we mean by these concepts and then state powerful
theorems that together let us carry over HAL’s impressive convergence rate to SAL despite the algorithmic
diﬀerences.

4

3.1 Early-stopping validation is rate-preserving for HAL

The point of early-stopping validation is to ﬁt a sequence of nested models and to stop once error
on a validation set begins to increase. This “early stopping” strategy is commonly employed as a model
selection heuristic that prevents having to ﬁt larger and larger models ad inﬁnitum [19, 20]. For example,
several popular packages for gradient boosting allow the user to ﬁt models without specifying the number
of trees a-priori: given a validation dataset, the software continues adding trees until validation loss
begins to increase. More generally this can done in an ad-hoc fashion: a user might manually expand
the upper/lower bound value of a hyperparameter during grid search model selection if it looks like a
minimum has not been reached. Often these hyperparameters directly correspond to moving upwards in
a series of nested statistical models (e.g. increasing number or depth of trees in tree ensembles, loosening
regularization in elastic net).

Our concern here is to propose a sequence of nested models that makes sense in our setting and to

prove that early-stopping validation over this sequence does not sacriﬁce HAL’s desirable properties.

|

s
≡ |

As a heuristic, [21] proposed a nested sequence of models for HAL that starts with one-way bases
= 2), etc. In practice, that
(hs,c where D
= 1), then adds two-way interactions (hs,c where D
|
means solving a lasso problem with np predictors, then one with n(p + p
), and so on until validation
loss increases. The general theory we develop in what follows covers this case and proves that this
algorithm retains HAL’s convergence rate. However, we also propose an improved algorithm for data-
adaptive basis selection to grow the statistical model which oﬀers better ﬁne-tuning and computational
advantages. This algorithm is also covered by the theory we develop here.

s
≡ |
2
−p
2

Our main analytical result is that early-stopping validation of an estimator that minimizes empirical
loss in a nested sequence of models embedded in a Donsker class preserves the convergence rate of that
estimator in the parent Donsker class. The proof is given in the appendix and relies only on generic
results from empirical process theory.

Theorem 1 (Early-stopping validation is rate-preserving). Consider a nested, possibly random or data-
dependent, sequence of function classes

F
all contained in some Donsker class

(

1

2

⊂ F
. Let

F

. . .

k

F

⊂

. . .

F

Kn )n

⊂

⊂ F

• fk,n = arg minf ∈Fk,n
• fn = arg minf ∈F PnLf be an empirical minimizer of L in the parent Donsker class
• f = arg minφ∈F P Lφ be the true, unknown minimizer.

PnLf be an empirical minimizer of L in each model

F

Assume that
Kn,n
by minimizing empirical loss in

⊂ F

F

↓PnL

Kn,n for all n.

F

. In other words, empirical loss minimization in

can be accomplished exactly

F

Deﬁne the diﬀerence in validation-set error from one iteration to the next to be ∆k,n = ˜PnLfk+1,n
−
˜PnLfk,n and choose k∗ to be the smallest value k such that ∆k,n > ǫn for some positive ǫn that is allowed
to shrink no faster than O(n−1/2).

−

−

→

Lf ) = oP (rn).

Lf ) = oP (rn) for some rn

0, then P (Lfk∗,n
If P (Lfn
The way the stopping point k∗ is chosen means that the analyst never needs to ﬁt past fk∗+1,n. In
contrast, consider the standard (cross-)validation scheme where k∗ is chosen as the global minimizer of
validation loss ˜PnLfk,n over all k in some ﬁnite set. Existing results show that this scheme preserves
the convergence rate of the fastest estimator in the ensemble, even if it grows at slower than a given
rate in n [22]. The downside is that computing the global minimum means ﬁtting fk,n for all k. With
early-stopping validation, we are instead satisﬁed with the ﬁrst local minimum we ﬁnd as we increase
k. This means we don’t have to ﬁt any models past that point, saving computation and allowing us to
select among massive numbers of models. This has been heuristically justiﬁed in the past by arguing
that larger models will continue to overﬁt more and more, so we are often right in guessing that the ﬁrst
local minimum is also the global minimum. That is not always true in ﬁnite samples, but our result
establishes that is true in an asymptotic sense.

The trade-oﬀ is that standard (cross-)validation can handle completely arbitrary estimators, whereas
with early stopping the estimators must be empirical loss minimizers of a nested series of models em-
bedded in a Donsker class. This is mitigated by the fact that we are still perfectly justiﬁed in using an
outer, standard validation setup to select among a ﬁnite ensemble of arbitrary estimators, one of which
happens to be the estimator selected via an inner, early-stopping validation procedure of the kind we
describe here.

5

F

F

=

=

(v2)

(v1)

k∗,n 6

k∗,n 6

As we previously stated, our result covers the case where higher-order interactions are sequentially
k is allowed to be random, this result also covers

added into partial HAL models. Since the sequence
other even more sophisticated schemes of the kind we will discuss shortly.

F
Theorem 1 easily extends to a early-stopping V -fold cross-validation scheme where k∗ is chosen based
on the ﬁrst local minimum of the validation error averaged over the folds 1
In this
V
setting, the ﬁnal estimator will typically be re-ﬁt for k∗ iterations using the full data and in general
k∗,n for any two validation sets v1, v2. In other words, the exact sequence of bases in
F
each validation sample will be diﬀerent. This may seem unsettling, but the fact that our result uniformly
covers cases where the sequence of statistical models is chosen adaptively or randomly means that we can
select the number of iterations k∗ without regard to the precise models and still obtain our conclusion.
An important corollary of this result (formalized as corollary 3 in the appendix) is that nested early
stopping schemes in Donsker models are also rate-preserving. For example, consider ﬁxing the sectional
variation norm bound M and then using early stopping to ﬁnd k∗ for a nested sequence inside the
Donsker class
(M ). One might be might be unsure that M is a large enough bound and be tempted
to repeat the process with a larger M (for which k∗(M ) might be diﬀerent) and continue on in such a
fashion until validation error increases from one M to the next. Corollary 3 establishes that this kind of
model selection is asymptotically justiﬁed as long as there is some a-priori ﬁnite sequence for M .

n Lf (−v)
k,n .

˜P (v)

P

F

v

tle explanation. For this we can consider the parent Donsker class

This also covers early stopping in interaction depth D (even jointly with M ), which bears a lit-
: f (x) =
of variationally-bounded cadlag functions that only have up to D-way interac-
D′, we can invoke theorem 1 and corollary 3
(M ′, D′) for any M

tions. Since
P
≤
to prove that an outer early-stopping validation loop in these hyperparameters is rate preserving.

{|s|=D} fs(xs1 . . . xsD )

(M, D) =

}
⊂ F

M ′, D

(M, D)

∈ F

f
{

≤

F

F

It is also important to point out that ǫn may be taken as a ﬁxed constant for all n (e.g. this can be
baked into the algorithm as a default setting). But having ǫn be allowed to shrink with the sample size
is extremely beneﬁcial: if we were to use a ﬁxed cutoﬀ ǫn = ǫ and apply our early-stopping algorithm
to larger and larger samples, we would expect the discovered stopping point k∗ to increase because
larger samples combat variance (overﬁtting). However, we don’t want k∗ to get too large in practice
because that increases computational intensity. By allowing ǫn to shrink with sample size, we can be
more and more sensitive to increases in validation loss as n increases. That makes us more aggressive
about stopping early in large samples, thus partially slowing down the rate at which k∗ grows while still
preserving the asymptotics. We expect users may choose ǫ to facilitate computation, meaning that in
diﬀerent problems they may use diﬀerent values to keep runtimes reasonable. In other words, users will
naturally choose shrinking ǫn, even if they do not realize it. Allowing ǫn to shrink (but not too fast)
helps protect the statistical properties from this aggressive user behavior. In the appendix (corollary 2)
we also cover an extension of theorem 1 that allows ǫn to shrink even faster (up to o(rn)) if additional
conditions are met.

Our theory also trivially covers any early stopping schemes that are less aggressive than checking for
the ﬁrst time that ∆k,n > ǫn. For example, to try to get over local minima and improve empirical ﬁnite-
sample performance we might stop only after ∆k,n > ǫn some number of iterations in a row. Schemes of
this kind are common in machine learning applications that use early stopping.

3.1.1 Adaptive Basis Selection

A naive implementation of early stopping might order the positive orthant bases at random. However,
this is a poor choice because the vast majority of bases are not useful for ﬁtting the data.3 If bases were
ordered at random it would take many, many iterations for any useful ﬁtting to occur. Early stopping
could kick in very quickly since the vast majority of bases would fail to produce any improvement in
validation error. The consequence would be a rather poor ﬁt in practice. To solve that problem we must
therefore ensure that the ordering of the bases is good in the sense that training error generally falls
“quickly” at ﬁrst and then “slowly” thereafter.

The optimal choice of basis at each iteration would be the one that decreases training error the most.
Thankfully, our early-stopping theorem covers data-adaptive orderings of the basis functions so we are
allowed to look at training error when deciding which basis to use next. Eﬀectively, what we have is a
variable selection problem for a (lasso) regression model and we are choosing to solve it with forward
selection [23]. At each iteration we add in the unused basis that has the maximum correlation with the
current residual.

3It is known mathematically that the number of nonzero lasso coeﬃcients cannot exceed n. On the other hand, the total
number of positive orthant bases generated by HAL is n(2p − 1). Therefore only a tiny fraction of a percent of these (on the
order of 2−p) appear in the ﬁnal lasso solution.

6

Unfortunately, ﬁnding this basis requires a computationally infeasible exhaustive search at each iter-
ation. Even a partial explicit search is infeasible for high-dimensional problems because the full set of
HAL predictors Hn is so large that it cannot even be instantiated in memory. So instead of ﬁnding the
optimal basis we have to settle for ﬁnding one that does a “good enough” job without needing an exhaus-
tive search. Moreover we need to be able to “search” through the bases without explicitly constructing
the vast majority of them. The precise optimality doesn’t matter anyways: all we want is some sensible
ordering of the bases.

To execute an “good enough” on-the-ﬂy search we translate the search problem into a regression
problem. The trick is to view the search as a functional gradient descent step. Given a current ﬁt f ,
we’re looking for the basis h that minimizes PnL(f + h). If the norm of all h are about equal and small
relative to f then this is like looking for the “direction” h that most decreases the empirical loss PnL.
Thus we would like to set h(x)
≈ −∇f (x)PnL to achieve a gradient descent step. For example, under
squared error loss L = (Y
f (X)).
Therefore we want h(X) to approximate R = 2(Y
f (X)). We only know what this is at the observed
−
data (Xi, Yi) so to get values for h at unobserved X we have to regress R1...n onto X1...n. The only
remaining complication is that we have to do this with a regression algorithm that always outputs a
learned function h that is in our set of bases. For positive orthant bases, a modiﬁed version of the
standard recursive partitioning algorithm does the trick. However, the theory we develop in the next
section will let us use standard regression trees instead.

f (X))2, the derivative of the loss w.r.t. the prediction f (X) is

2(Y

−

−

−

The idea we describe in the previous paragraph is known as functional gradient boosting [10]. Us-
ing this technique we can generate each basis on-the-ﬂy and simultaneously ensure that training error
decreases rapidly.

3.1.2 Expressive Bases

Our early stopping result implies that we can iteratively build up a HAL model: we add a positive
orthant basis and reﬁt the lasso at each iteration until validation error begins to increase. But even
with an intelligent, data-adaptive ordering, this algorithm scales very poorly. Only very simple functions
In large
can be ﬁt well by linear combinations of tens or even hundreds of positive orthant bases.
high-dimensional settings we will therefore pass through a huge number of bases before early stopping
is achieved, and we will solve larger and larger lasso problems along the way. Instead of solving one
impossibly large lasso problem we must solve a very long series of increasingly large lasso problems – this
is still computationally impractical.

Therefore we would prefer to use bases that can individually absorb more unexplained variance. For
example, if we could use some (possibly random) subset of all bounded regression trees that could be
generated from our data as bases then we might hit early stopping faster than we would with positive
orthants. However, we need to show that doing this would not compromise our convergence rate.

Thankfully, this is easy to do in conjunction with early stopping. Under some easily satisﬁable
conditions we can greatly extend the set of bases we consider in the (boosted) early-stopping procedure.
Corollary 1 (Using more expressive bases). Let ˜H(X)
set of Kn cadlag basis functions of sectional variation bounded by B. Let

(B) be a (possibly random or data-adaptive)

⊂ F

• ˜
F

•

F

k,n(M ) =

n(M ) =

˜H1:k(x)⊤β :
{
H(x)⊤β :
β
{

k

k
1
k

β

1
k

≤

M

}

≤

M

}

where H(x) are the HAL positive orthant bases. Then the result of theorem 1 holds if we use sequences
of the form

˜
1(M )
F

⊂

˜
2(M )
F

⊂

. . . ˜
F

k(M )

⊂

. . . ˜
F

Kn (M )

h(cid:16)

n 6⊂ F

(cid:17)

n(BM )

(BM )

⊂ F

i

The (brief) proof is shown in the appendix.
Our idea here is to start with an expressive basis ˜h1, then add in another expressive basis ˜h2, etc.
with the intention of building up to doing lasso over the full set of bases ˜H. However, empirical loss
minimization over that set of bases is not necessarily the same as empirical loss minimization in some
parent Donsker class so we can’t immediately use the conclusion of theorem 1. The trick is to show that
this function class is contained inside of another one which does share an empirical minimizer with some
parent Donsker class of interest. Thus all we need to do is continue the theoretical validation process one
step past the lasso with full set of bases ˜Hn and bound M and at the very end solve the original HAL

7

problem but with the bound set to BM instead of M . If we do that, then we can use theorem 1 and get
the result we want: early-stopping validation using expressive bases preserves the HAL rate.

The requirement that we include a full HAL model (with larger bound) at the end of our sequence
of models is completely theoretical: in practice we expect to stop adding bases far before we reach the
full set of expressive bases ˜Hn.

This result is only useful if we can ﬁnd some interesting set of bases ˜Hn that satisﬁes the conditions of
the corollary: namely that it consists of cadlag functions uniformly bounded in terms of variation norm.
Here we will argue that rectangular indicators, bounded regression trees of ﬁxed depth, and even tree
ensembles all ﬁt the bill.

Consider an indicator for a rectangular region r(x) = 1(a

Rp and the
x < b) where a, b, x
inequalities must hold in each coordinate (some or all elements of a, b are allowed to be
). Such a
function can always be constructed as a linear combination of 2p positive orthant indicators (one per
2p. By construction, any positive orthant basis h has
corner of the rectangle): r =
sectional variation norm 1. Thus
P

2p αh where

∈
±∞

α
|

| ≤

≤

r
k

ν =
k

2p

αh

α
|

|k

h

ν = 2p
k

≤

ν

X

2p

X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(5)

A bounded regression tree t(x) of maximum depth D has at most 2d leaves, each of which is a rect-
angular region in which the function takes some value c
B. Note that the depth of the regression trees
used either by themselves or as part of a GBM learner correspond exactly to the maximum interaction
depth D of our positive orthant bases. Thus we can write the tree as a ﬁnite, bounded linear combination
of rectangular indicators. Taking the norm,

≤

t
k

ν =
k

2D

cr

X

2D

≤

ν

X

r
c
|k
|

ν
k

≤

B2D sup

r
k

ν = B2D2p
k

2

(6)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

where we’ve used our above result. By exactly the same argument, any weighted ensemble g of
a ﬁxed number of trees T where the weights are bounded (say by B again) has norm bounded by

g
k

ν
k

≤

T B22Dp

2

.

3.2 Algorithm

In the previous sections we’ve explained why early stopping and adaptive, expressive bases are necessary
to turn HAL into a computationally feasible algorithm. We presented powerful theoretical results that
ensure that the HAL rate is preserved despite these changes to the algorithm in a very broad range
of settings. Now we are ready to put it all together and propose a new, practical algorithm that is
guaranteed by our theory to achieve HAL’s convergence rate. We call this new algorithm the Selectively
Adaptive Lasso (SAL) because it selectively constructs ﬂexible bases to add into the model on-the-ﬂy;
unlike the Highly Adaptive Lasso, which includes the full “kitchen sink” of bases.

When implemented, SAL is essentially a gradient-boosted tree ensemble with lasso run over it in
chunks and early stopping used for model selection. This can also be understood as a gradient-boosted
tree algorithm with a special kind of adaptive learning rate. As a consequence, SAL is trivial to implement
with oﬀ-the-shelf software.

Description SAL works as follows. The user is required to specify

1. training data (Xt, Yt) and validation data (Xv, Yv)
2. a full set of GBT hyperparameters “γ” (e.g. number of trees, learning rate, tree depth, etc.)

3. the lasso regularization strength λ

4. the number of bases desired.
We begin each iteration with the current residual Rt, which for the ﬁrst iteration is the vector of
training outcomes Yt. We ﬁt a GBT model h
gbt(Xt, Rt; γ) using the speciﬁed hyperparameters
←
[H ⊤, h]. Now we compute the basis matrix H ⊤(Xt) for
γ and append it to the vector of bases H ⊤
the training data by getting predictions from each basis h and concatenating them as columns. We use
lasso with regularization strength λ to regress Yt onto H ⊤(Xt) and produce the current coeﬃcients
H(Xt)⊤β and proceed to the
β

coef(lasso(Xt, Yt; λ)). Lastly, we update the residual Rt

Yt

←

←

←

−

8

next iteration. After the ﬁnal iteration, the vector of bases and ﬁnal coeﬃcients are returned. Predictions
for new data X are given by H(X)⊤β.

If a maximum number of bases is not given, early stopping may be employed to set this parameter
data-adaptively. At the end of each iteration, we compute the validation set loss L(Yv, H(Xv)⊤β). If
this loss is greater than that of the previous iteration plus some small ﬁxed ǫ we stop and return the set
of bases and coeﬃcients from the previous iteration. An outer early stopping loop can also be used to
set the depth of each tree in the GBT ﬁts (increasing) and the lasso regularization strength (decreasing).

Simple example python code implementing SAL can be found at tinyurl.com/4te5wcb3.

When used with early stopping, this algorithm is guaranteed4 by the theory presented in the previous
sections to attain the HAL rate. In order to ensure that theorem 1 applies, k must be tuned via early-
stopping validation. If there is no applicable a-priori knowledge of λ or D, these parameters must be
tuned with early stopping as well. Corollary 3 establishes that this can be done in a separate outer loop.
Readers who are familiar with gradient boosting should recognize SAL as a boosting algorithm. If
[1 . . . 1]⊤ we obtain the
the line of code implementing β
standard GBT algorithm. Many existing GBT packages even allow the user to impose an L1 regulariza-
tion rate on the leaf values of the trees, but this is always approximated in a stagewise fashion [12]. For
our theory to hold the lasso solution must be recomputed exactly in each iteration.

coef(lasso(Xt, Yt; λ)) is replaced with β

←

←

By running the lasso over chunks of trees (or single trees, or leaf values), we are eﬀectively introducing
a data-adaptive learning rate. For example, imagine the user speciﬁes that the SAL bases will be GBTs
with learning rate η = 0.1 and 10 trees each. If we denote the prediction of each tree as bj(x), the ﬁnal
20
j=11 ηbj (x). If we move βk into each
prediction after 2 SAL iterations would be β1
sum, we see that the eﬀect is to obtain a learning rate of β1η for the ﬁrst 10 trees and a learning rate of
β2η for the next 10, etc. Since β is determined based on the training data and current predictions, this
eﬀectively creates a data-adaptive learning rate that can even look back and modify the learning rate for
long-past iterations if necessary. SAL thus elegantly bridges HAL and gradient-boosted trees.

10
j=1 ηbj (x) + β2

P

P

Many variations of this algorithm (e.g. cross-validated early stopping) are possible and are supported
by our general theory. However, our goal here is to present a canonical algorithm, support it with rigorous
theory, and demonstrate practical utility. Further experimentation is certainly warranted in the future.

4 Demonstration

Here we demonstrate how SAL performs relative to GBT and lasso on 12 real-world regression datasets
from the UCI Machine Learning Repository [24].

We randomly5 split each dataset into training-validation (90%) and test sets (10%) and then further
randomly split the training-validation into training (80%) and validation (20%). For each learner, we
performed model selection using the training and validation data according to the model selection strategy
for each learner (detailed shortly). Once optimal hyperparameters were determined we reﬁt each learner
using those hyperparameters and the full training-validation data. Finally, we calculated the test-set
RMSE for each ﬁt learner. We repeated this entire experiment 10 times6 for each dataset, averaged the
RMSEs across repetitions, and computed the standard deviation of the RMSEs.

For all SAL ﬁts we used GBT bases of 100 trees with a learning rate of 0.2 (ﬁt using the xbgoost
package [12]). The number of bases (k) was determined based on early stopping using the validation data.
We used an outer early-stopping loop to jointly set λ and the tree depth D for the GBT bases. λ was
set to 1e-5 initially and divided by 10 in each iteration whereas D was set to 2 initially and incremented
by 2 in each iteration. We used histogram-based trees with 27 bins and all other GBT hyperparameters
left to their defaults in xgboost. Lasso solutions were provided by glmnet [15]. ǫ was set to machine
ﬂoating-point precision in all early stopping loops.

For all GBT ﬁts we again used a learning rate of 0.2. We set the number of trees with early stopping
on the validation data (evaluating validation-set loss every 100 trees). Tree depth D was set using an

4There are two minor details that were not included in the description above so as to not distract from the main ideas.
These details are: 1) the trees used in the GBT implementation must be bounded (to satisfy cn,ij ≤ c in thm. 1) and 2) some
large, maximum number of trees must be speciﬁed after which the algorithm solves a HAL problem. Condition (1) is easily
satisﬁed by trimming the tree predictions to an arbitrarily large value. This should generally be a no-op and therefore may be
practically omitted. Condition (2) is also practically a no-op because early stopping will kick in far before the maximum number
of bases is reached. We recommend ignoring these details in practice, but note that a fully theory-compliant implementation
should attend to them.

5excepting the yearmsd dataset, which has a temporal ordering implying a ﬁxed test set of the last 51629 rows (∼10%)
6excepting the two largest datasets (yearmsd, slice) for each of which we only ran a single experiment

9

dataset

yacht
energy
boston
concrete
wine
power
kin8nm
naval
protein
blog
slice
yearmsd

p

6
8
13
8
11
4
8
17
9
280
384
90

n

SAL

GBT

LASSO

308
768
506
1030
1599
9568
8192
11934
45730
52397
53500
515345

0.682 ± 0.137
0.325 ± 0.0544
3.25 ± 0.561
3.75 ± 0.316
0.623 ± 0.0282
3.08 ± 0.134
0.115 ± 0.00333
0.000818 ± 2.53e-05
1.87 ± 0.053
26.7 ± 1.56
1.34 ± NaN
9.05 ± NaN

0.646 ± 0.127
0.325 ± 0.0529
3.23 ± 0.503
3.68 ± 0.35
0.623 ± 0.0304
3.09 ± 0.125
0.114 ± 0.00313
0.000818 ± 2.53e-05
1.87 ± 0.048
26.5 ± 1.59
1.33 ± NaN
9.04 ± NaN

11.4 ± 1.71
2.96 ± 0.195
4.54 ± 0.577
10.6 ± 0.41
0.671 ± 0.0219
5 ± 0.101
0.748 ± 0.00305
0.00266 ± 3.26e-05
4.22 ± 0.0435
33.3 ± 1.62
8.62 ± NaN
134 ± NaN

Table 1: Mean RMSEs (±1.96× standard deviation of RMSEs) across repetitions for all experiments.

outer early-stopping loop, once again set to 2 initially and incremented by 2 in each iteration. Again
we used histogram-based trees with 27 bins and all other GBT hyperparameters left to their defaults in
xgboost.

λ in our lasso learners was tuned using the built-in cross-validation in the glmnet package using all

the default options.

Our results are shown in table 1. SAL and GBT perform almost identically and in all cases are much
better than lasso. The pattern is the same across the dimensions of the data p and n. All experiments
ran on a MacBook Air in a single afternoon.

5 Discussion

We have demonstrated a regression algorithm, the Selectively Adaptive Lasso, which is massively scal-
able while maintaining a OP (n−1/3(log n)2(p−1)/3) L2 convergence rate in nonparametric settings. The
algorithm is a form of gradient tree boosting with an adaptive learning rate, which makes it trivial
to implement with oﬀ-the-shelf software and fast-running. We showed that our algorithm retains the
performance of standard gradient boosting on a diverse group of real-world datasets. SAL makes semi-
parametric eﬃcient estimators practically possible and theoretically justiﬁable in many settings.

Along the way, we proved several general results pertaining to empirical loss minimization in nested
Donsker classes. These may be of independent interest for the development of future statistically-justiﬁed
machine learning methods.

Our results also shed some light on why GBT is so successful by itself. The standard approach of
regularizing via a small learning rate η precisely mimics L1 shrinkage as η
0, assuming a globally
optimal tree can be ﬁt in each iteration [23, 25, 19]. Our early-stopping result makes it clear that, at
least asymptotically, trees can be added in a suboptimal order without compromising the rate. This
helps justify the use of recursive partitioning.

→

Future experiments are warranted to explore variations of SAL and its performance in diﬀerent
settings. With trivial modiﬁcations these results extend to probabilistic classiﬁcation for categorical
outcomes. Undersmoothed SAL by itself (like HAL) also satisﬁes the theoretical criteria required to be
an eﬃcient plug-in estimator for certain estimands [17] but this has yet to be demonstrated in practice
with simulations. It should also be possible to perform early-stopping undersmoothing in the number of
bases as well as in the regularization parameter.

Acknowledgments

The authors are eternally grateful to Tianyue Zhou for notational corrections throughout the manuscript.

10

References

[1] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2. Overview of Supervised Learning. In
The Elements of Statistical Learning, page 1 34. Springer New York, New York, NY, January 2009.

[2] A W van der Vaart. Asymptotic Statistics. Cambridge University Press, June 2000.

[3] A Tsiatis. Semiparametric theory and missing data, 2007.

[4] Mark J van der Laan, M J Laan, and James M Robins. Uniﬁed Methods for Censored Longitudinal

Data and Causality. Springer Science & Business Media, January 2003.

[5] Charles J Stone. Optimal Global Rates of Convergence for Nonparametric Regression. Ann. Stat.,

10(4):1040 1053, December 1982.

[6] James Robins, Lingling Li, Eric Tchetgen, and Aad van der Vaart. Higher order inﬂuence functions
and minimax estimation of nonlinear functionals. In Probability and Statistics: Essays in Honor
of David A. Freedman, Probability and Statistics: Essays in Honor of David A. Freedman, pages
335–421. Institute of Mathematical Statistics, Beachwood, Ohio, USA, 2008.

[7] Billy Fang, Adityanand Guntuboyina, and Bodhisattva Sen. Multivariate extensions of isotonic
regression and total variation denoising via entire monotonicity and Hardy-Krause variation. March
2019.

[8] David Benkeser and Mark van der Laan. The highly adaptive lasso estimator. Proc Int Conf Data

Sci Adv Anal, 2016:689–696, December 2016.

[9] Mark van der Laan. A generally eﬃcient targeted minimum loss based estimator based on the highly

adaptive lasso. Int. J. Biostat., 13(2), October 2017.

[10] J H Friedman. Greedy function approximation: a gradient boosting machine. Ann. Stat., 2001.

[11] Casper Solheim Bojer and Jens Peder Meldgaard. Kaggle forecasting competitions: An overlooked

learning opportunity. Int. J. Forecast., 37(2):587–603, April 2021.

[12] Tianqi Chen and Carlos Guestrin. XGBoost. In Proceedings of the 22nd ACM SIGKDD International

Conference on Knowledge Discovery and Data Mining, New York, NY, USA, August 2016. ACM.

[13] Aurélien F Bibaut and Mark J van der Laan. Fast rates for empirical risk minimization over càdlàg

functions with bounded sectional variation norm. July 2019.

[14] Robert Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Series B Stat.

Methodol., 58(1):267–288, 1996.

[15] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear

models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010.

[16] Mark J van der Laan and Sherri Rose. Why machine learning cannot ignore maximum likelihood

estimation. October 2021.

[17] Mark J van der Laan, David Benkeser, and Weixin Cai. Eﬃcient estimation of pathwise diﬀerentiable

target parameters with the undersmoothed highly adaptive lasso. August 2019.

[18] Peter J Bickel, Chris A J Klaassen, Ya’acov Ritov, and Jon A Wellner. Eﬃcient and Adaptive

Estimation for Semiparametric Models.

[19] Ye Luo and Martin Spindler. High-Dimensional L2Boosting: Rate of convergence. arXiv, 2016.

[20] Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. August 2005.

[21] Nima Hejazi, Jeremy Coyle, and Mark van der Laan. hal9001: Scalable highly adaptive lasso

regression in R. J. Open Source Softw., 5(53):2526, September 2020.

[22] Sandrine Dudoit and Mark J van der Laan. Asymptotics of cross-validated risk estimation in

estimator selection and performance assessment. Stat. Methodol., 2(2):131 154, 2005.

11

[23] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. aos,

32(2):407–499, April 2004.

[24] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.

[25] Peng Zhao and Bin Yu. Boosted lasso. https://statistics.berkeley.edu/sites/default/files/tech-reports/678.pdf.

Accessed: 2022-1-26.

A Lemmas

Before beginning the proofs of our results we sketch the proof for a generic result from empirical process
theory that we will use several times:

Lemma 1. If

F

is Donsker, (Pn

−

P )fn = OP (n−1/2) for any (possibly random) sequence fn

.

∈ F

F

F

is Donsker, √n(Pn

with sample paths in l∞(

P )   G where G is a tight Gaussian process indexed
Proof. By deﬁnition, since
to R, equipped with supremum norm).
by
Tightness means that with high probability the sample paths of G are in a compact set and can thus
be covered with a ﬁnite number of balls. The center of each of these balls is a bounded function since
all sample paths are bounded. Therefore all functions in this high-probability set are also bounded, and
moreover are bounded uniformly since the number of balls is ﬁnite.

) (bounded functions from

−

F

F

That establishes the fact that supf ∈F √n(Pn

P )f = OP (1), since for any level of probability δ we
can eventually ﬁnd a high-probability set where all the contained functions are bounded. From there we
P )f = OP (n−1/2) as desired.
get that (Pn

−

supf ∈F (Pn
Note: as a trivial corollary we also have that ( ˜Pn

P )fn

≤

−

−

P )fn

−

≤

supf ∈F ( ˜Pn

−

P )f = OP (n−1/2).

We also remind ourselves of an elementary asymptotic result:

Lemma 2. Let Un

P
→
Proof. By deﬁnition of convergence in probability we must show that P
ǫ > 0. But this is clear because

0. Then anUn

and assume P

Un = 1
{

} →

0, 1

∈ {

}

0 for any an.

anUn > ǫ
{

} →

0 for any ﬁxed

P

anUn > ǫ
{

}

= P

Un = 0
anUn > ǫ
}
|
{

P

Un = 0
}
{

+ P

Un = 1
anUn > ǫ
}
|
{

P

Un = 1
}
{

0

P
|

Un = 1
{

{z
} →

0

≤

}

And lastly a fact about minimizers over nested sets

Lemma 3. Let A

C and B

⊂

⊂

C be classes of functions and let B

↓PnL

⊂

C. Then B

↓PnL

⊂

A

B

∪

↓PnL

⊂

C.

Proof. Since B already includes the empirical minimizer in C, adding more choices from C (e.g. those
options in A) cannot result in a better minimum.

B Proof of Theorem 1

We can now proceed to our proof of theorem 1.

Theorem 1 (Early-stopping validation is rate-preserving). Consider a nested, possibly random or data-
dependent, sequence of function classes

F
all contained in some Donsker class

(

1

2

⊂ F
. Let

F

. . .

k

F

⊂

. . .

F

Kn )n

⊂

⊂ F

• fk,n = arg minf ∈Fk,n
• fn = arg minf ∈F PnLf be an empirical minimizer of L in the parent Donsker class
• f = arg minφ∈F P Lφ be the true, unknown minimizer.

PnLf be an empirical minimizer of L in each model

F

12

Assume that
Kn,n
by minimizing empirical loss in

⊂ F

F

↓PnL

Kn,n for all n.

F

. In other words, empirical loss minimization in

can be accomplished exactly

F

Deﬁne the diﬀerence in validation-set error from one iteration to the next to be ∆k,n = ˜PnLfk+1,n
−
˜PnLfk,n and choose k∗ to be the smallest value k such that ∆k,n > ǫn for some positive ǫn that is allowed
to shrink no faster than O(n−1/2).

If P (Lfn

−

Lf ) = oP (rn) for some rn

0, then P (Lfk∗,n

Lf ) = oP (rn).

−

→

Proof. The overall arc of the proof is to show that the early-stopping solution asymptotically converges
to the empirical minimizer in the parent Donsker class (e.g. the HAL solution when
is the class of
bounded-variation cadlag functions) and thus retains its rate. To do this we will show that in large-
enough samples, the validation error starts to trend monotonically downward in k with high probability.
Therefore early stopping does not kick in and with high probability we obtain the empirical minimizer
in the parent class.

F

To reduce visual clutter we omit n subscripts on sequences of k as is convenient throughout the proof.

We begin by decomposing

P (Lfk∗,n

−

Lf ) = P (Lfk∗,n

−

Lfn) + P (Lfn

Lf )

−

(7)

oP (rn) by assumption

The proof is complete if the ﬁrst term on the right-hand side is also oP (rn); that is, if we can show
that the early-stopping solution fk∗,n converges in expected loss to the HAL solution fn at a rate of
oP (rn) or faster. To accomplish that we decompose

|

{z

}

P (Lfk∗,n

Lfn) =

−

≤

Kn

1k(k∗)P (Lfk,n

Lfn)

−

Xk
1(k∗ = Kn)✭✭✭✭✭✭✭✭

P (LfKn,n

Lfn) + 1(k∗ < Kn) max
k<Kn

−

(8)

(9)

P (Lfk,n

Lfn)

−

The cancellation comes by our knowledge that fn = fKn,n (i.e. the HAL solution is an empirical loss
1(k∗ < Kn) we therefore establish the following suﬃcient

). Using that 1(k∗ = Kn) = 1
minimizer in
conditions to ensure that the left-hand side above is oP (rn):

−

F

1. maxk<Kn P (Lfk,n
−
2. 1(k∗ < Kn) = oP (rn)

Lfn) = OP (1)

Condition (1) To establish condition (1) note that the sequence of models

¯k,n for

F

¯k = arg max

P (Lfk,n

k<Kn

Lfn)

−

deﬁnes one possible path through the random models
k we have

F

k,n as n increases. For an arbitrary ﬁxed sequence

P (Lfk,n

Lfn) =

(Pn

−

−

−

P )(Lfk,n

−

Lfn)

+Pn(Lfk,n

Lfn)

−

OP (n−1/2)

In the ﬁrst term, (Pn

which is Donsker.

P )(Lfk,n

−

−

|
Lfn) = OP (n−1/2) follows from lemma 1 because fk,n, fn

{z

}

(10)

F

∈

≤

To show the LHS is Op(1) it therefore suﬃces to show that the remaining term Pn(Lfk,n

Lfn) =
OP (1). We can bound PnLfk,n
PnLf0,n for any k (where f0,n(x) = arg minc∈R PnLc). The idea is that
an empirical minimizer in any nontrivial function class must have better training error than a constant
ﬁt. So our term of interest is bounded above by PnLf0,n = (Pn
P )Lf0,n + P Lf0,n. The ﬁrst term there
is Op(n−1/2) by lemma 1 and the second term may be bounded under very weak conditions (e.g. Y is
bounded and loss function is non-negative).
We have thus established that P (Lfk,n

Lfn) = OP (1) uniformly over any arbitrary sequence k and
Lfn) = OP (1) as desired.

thus this must also hold for the special case k = ¯k. Thus maxk<Kn P (Lfk,n

−

−

−

−

13

k∗ < Kn
{

Condition (2) Now we can proceed to show condition (2), which was that 1(k∗ < Kn) = oP (rn). This
follows from lemma 2 as long as P
0, i.e. the probability of early stopping goes to zero as the
sample size increases. By deﬁnition this can only happen if the probability of validation error increasing
from one iteration to the next goes to zero at every iteration. Formally, we need P
maxk≤Kn ∆k,n >
{
ǫn

n maxk≤Kn ∆k,n = oP (1). This is what we will show.

0 which certainly is the case if ǫ−1

} →
As before, consider an arbitrary sequence k: the particular (random) sequence ¯¯k = arg maxk≤Kn ∆k,n
n ∆k,n = oP (1) for any sequence k which thus implies what we want

is one special case. We will show ǫ−1
as one example. Expanding from the deﬁnition of ∆k,n, we have

} →

∆k,n = ˜Pn(Lfk+1,n

Lfk,n)

−

( ˜Pn

=

−

−

P )(Lfk+1,n

−

OP (n−1/2)

Lfk,n)

+P (Lfk+1,n

Lfk,n)

−

(11)

(12)

The ﬁrst term is Op(n−1/2) by lemma 1. Since ǫn is O(n−1/2) or slower by assumption, we just need

|

{z

}

the second term here to be OP (n−1/2) or faster to get ǫ−1

n ∆k,n = oP (1).

Deﬁne fk = arg minf ∈Fk,n

P Lf , i.e.

the minimizer of the true loss in the (random) model

Continuing from above we expand the second term to

P (Lfk+1,n

−

Lfk,n) = P (Lfk+1,n

Lfk+1) + P (Lfk+1

Lfk)

+P (Lfk

Lfk,n)

−

−

−

≤0

P (Lfk+1,n
≤ |
= P (Lfk+1,n

−

+
|

P (Lfk,n
Lfk+1)
{z
|
|
Lfk+1) + P (Lfk,n

Lfk)
}
−
Lfk)

|

−

−

k,n.

F

(13)

(14)

(15)

The term P (Lfk+1

Lfk)

0 by the deﬁnition of fk and the fact that

k,n

≤
any sequence. In the last line we exploit the fact that P (Lfk,n
is the minimizer of the true loss.

−

−

F
Lfk) > 0 since fk, fk,n

⊂ F

k+1,n at each k in
k,n and fk

∈ F

To contend with the two remaining terms, note that for any arbitrary sequence k we have

P (Lfk,n

Lfk) =

(Pn

−

−

−

P )(Lfk,n

(Pn

−

≤ −

P )(Lfk,n

Lfk) + Pn(Lfk,n

Lfk

)

−

Lfk)

|

≤0

{z

}

−

−

(16)

(17)

which by lemma 1 is OP (n−1/2). However, we can also exploit the fact that

F to get the
Lf ) = oP (rn) by assumption. We’ll assume without loss of

k,n

⊂

F

better rate oP (rn), since we know P (Lfn
generality that rn is a rate faster than n−1/2.

−

Therefore we have that ∆k,n = OP (n−1/2) + oP (rn), which gives the rate OP (n−1/2). Thus we can
n ∆k,n = oP (1). This is satisﬁed by

aﬀord ǫn shrinking at any rate slower than n−1/2 and still get ǫ−1
assumption

C Corollaries

Corollary 1 (Using more expressive bases). Let ˜H(X)
set of Kn cadlag basis functions of sectional variation bounded by B. Let

⊂ F

(B) be a (possibly random or data-adaptive)

• ˜
F

•

F

k,n(M ) =

n(M ) =

˜H1:k(x)⊤β :
{
H(x)⊤β :
β
{

k

k
1
k

β

1
k

≤

M

}

≤

M

}

where H(x) are the HAL positive orthant bases. Then the result of theorem 1 holds if we use sequences
of the form

˜
1(M )
F

⊂

˜
2(M )
F

⊂

. . . ˜
F

k(M )

⊂

. . . ˜
F

Kn (M )

h(cid:16)

n 6⊂ F

(cid:17)

n(BM )

(BM )

⊂ F

i

14

Proof. First we verify that indeed ˜
F

k,n(M )

⊂ F

(BM ). Pick f

˜
k,n(M ).
F

∈

f
k

ν =
k

βk˜hk

˜hk

sup

k k

≤

ν
k

βk
|

| ≤

BM

X

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

X

Moreover f is cadlag since the set of cadlag functions is closed under ﬁnite linear combinations. Thus

f

(BM ) as desired.

∈ F

Now we verify the claim. This is easy because all of the assumptions of theorem 1 are already satisﬁed
n(BM ) as the ﬁnal

except for the fact that ˜
n(BM ). Instead, consider using ˜
F
F
6⊂ F
class. Now the set inclusion is trivially satisﬁed but we have to show that

Kn (M )

Kn (M )

∪ F

to use our theorem. This is clear from lemma 3. The same lemma also implies

˜
Kn (M )
F

∪ F

n(BM )

↓PnL

⊂ F

(BM )

(cid:16)

(cid:17)

so there is no need to do empirical loss minimization over the union:

n(BM )

F

↓PnL

⊂

˜
Kn (M )
F

∪ F

n(BM )

(cid:16)

(cid:17)

F

n(BM ) by itself suﬃces.

Corollary 2 (Sample-size dependent early stopping). If
0 for all k, we can
relax the presumption that ǫn shrink no faster than o(n−1/2) and instead require that it shrinks no faster
than O(n−1/2δ1/2
n ) (presuming that is faster than o(n−1/2)) and still obtain the conclusion of theorem 1.
However, in no case can ǫ shrink faster than o(rn).

fk+1,n
k

< δn

fk,n

→

−

k

Proof. In eq. 12 we decompose ∆k,n into two terms, one of which is OP (n−1/2) and the other which
is oP (rn). Again presuming without loss of generality that rn is faster than n−1/2, we immediately see
that if the OP (n−1/2) term is what is gating the speed of convergence. If we could improve that rate,
then ∆k,n would converge more quickly and that would give more wiggle room for ǫn. However, once
that rate improves past o(rn), it is the other term that gates the convergence, which shows that o(rn) is
the best we can do by ﬁnding fast bounds on δn.
The OP (n−1/2) term in question is ( ˜Pn

P )(Lfk+1,n

−

Lfk,n). Using a bound based on entropy
Lfn) = OP (n−1/2δ1/2
n )

P )(Lgn

integrals, Bibaut and van der Laan [13] invoke a result that shows ( ˜Pn
for gn, fn

< δn. The result follows immediately.

as long as

fn

−

−

∈ F

gn
k

−

k

−

A comment on corollary 2: The condition

0 may be diﬃcult to show for any
particular data-adaptive method of constructing the nested models
Kn . However, one may
have intuition that as n increases, we get Kn increasing and perhaps the diﬀerence between subsequent
models becomes smaller and smaller such that the distance between empirical loss minimizers in adjacent
models decreases. Alternatively, one might presume that the ordering of the models is good enough that
most of the “ﬁtting” happens early on (i.e. validation error decreases sharply and then tapers oﬀ), in
which case the required condition would hold for k greater than some k0, perhaps.

fk+1,n
k

< δn

fk,n

. . .

→

−

⊂

F

F

k

1

In any case, the implication is that for sensibly constructed adaptive sequences we may be able to

tolerate being slightly more sensitive to increases in validation error in larger and larger samples.

Corollary 3 (Nested early-stopping validation). Consider the setting of theorem 1, but replace
a Donsker class
thus replace

m nested in a sequence of Donsker classes (

k,m and k∗ becomes k∗(m).

M ) with f

G
k with

m . . .

1 . . .

∈ G

with
F
M . We must

Deﬁne the diﬀerence in validation-set error from one estimate to the next to be ∆m,n = ˜PnLfk∗(m+1),n−
˜PnLfk∗(m),n with k∗(m) chosen as in theorem 1. Choose m∗ to be the smallest value m such that
∆m,n > ǫn for some positive ǫn that is allowed to shrink no faster than o(n−1/2).

F

F

G

G

G

If P (Lfn

−

Lf ) = oP (rn) for some rn

0, then P (Lfk∗(m∗),n −

→

Lf ) = oP (rn).

Proof. Follow the outline of the proof of theorem 1 exactly, substituting f m
fk. Whenever the term P (Lf m
follows.

Kn for
Kn ) arises invoke theorem 1 to prove it is oP (rn) and the result

k∗(m∗),n for fk∗,n and f m

k∗(m),n −

Lf m

∗

15

