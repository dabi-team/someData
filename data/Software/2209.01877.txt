Performance optimization and analysis of the
unstructured discontinuous Galerkin solver on
multi-core and many-core architectures

1st Zhe Dai
Computational Aerodynamics Institute
China Aerodynamics Research and Development Center
Mianyang, China
daizhe cardc@163.com

2nd Liang Deng*
Computational Aerodynamics Institute
China Aerodynamics Research and Development Center
Mianyang, China
dengliang11@nudt.edu.cn

3rd Yueqing Wang
Computational Aerodynamics Institute
China Aerodynamics Research and Development Center
Mianyang, China
yqwang2013@163.com

4th FangWang
State Key Laboratory of Aerodynamics
China Aerodynamics Research and Development Center
Mianyang, China
wangfang@cardc.cn

5th Ming Li
Computational Aerodynamics Institute
China Aerodynamics Research and Development Center
Mianyang, China
liming@cardc.cn

6th Jian Zhang
Computational Aerodynamics Institute
China Aerodynamics Research and Development Center
Mianyang, China
zhangjian@cardc.cn

Abstract—The discontinuous Galerkin (DG) algorithm is a
representative high order method in Computational Fluid Dy-
namics (CFD) area which possesses considerable mathemati-
cal advantages such as high resolution,
low dissipation, and
dispersion. However, DG is rather computationally intensive
to demonstrate practical engineering problems. This paper
discusses the implementation of our in-house practical DG
application in three different programming models, as well as
some optimization techniques, including grid renumbering and
mixed precision to maximize the performance improvements in
a single node system. The experiment on CPU and GPU shows
that our CUDA, OpenACC, and OpenMP-based code obtains a
maximum speedup of 42.9x, 35.3x, and 8.1x compared with serial
execution by the original application, respectively. Besides, we
systematically compare the programming models in two aspects:
performance and productivity. Our empirical conclusions facili-
tate the programmers to select the right platform with a suitable
programming model according to their target applications.

Index Terms—Unstructured grid, DG, GPU, performance,

productivity

I. INTRODUCTION

The high-order scheme receives considerable attention in
Computational Fluid Dynamics (CFD) due to its low nu-
merical dissipation for Large-Eddy Simulation (LES) [1] and
the capability to capture the ﬁne-grained structure in compli-
cated ﬂows. Discontinuous Galerkin (DG) [2] is a popular

*Corresponding author:dengliang11@nudt.edu.cn. This work was sup-

ported by the National Numerical Wind Tunnel of China.

algorithm that possesses good mathematical properties in
conservation, stability, and convergence in complex ﬂow prob-
lems. However, DG suffers from severe computing overhead
comparing with the low-order scheme. For example, solving
the Euler equation with three-dimensional fourth-order DG
method needs one hundred variables per grid element [3],
[4], while the only single-digit variable is required in low-
order methods, not to mention the additional volume and
area integral computations in every grid element, which is
not needed at all in low-order method.

To make computationally expensive DG algorithm avail-
able in practical engineering, many researchers have made
considerable achievements on both GPU and CPU hared-
wares. Xia accomplishes the unstructured grid DG algorithm
based on OpenACC Programming model [5]. Pazner em-
ployed a parallel-in-time strategy to compute the Runge-
Kutta stages simultaneously under DG discretizations [6].
Duan applied total variation diminishing Runge–Kutta scheme
coupled with the multigrid strategy to improve the parallel
efﬁciency of DG method on CPUs [7]. Maurice presented a
parallel computing strategy for a hybridizable discontinuous
Galerkin nested geometric multigrid solver on many-core
hardware, attaining 80% of peak performance for higher order
polynomials [8]. Kronbichler integrated optimized DG code
into a scalable framework of ﬁve supercomputers using both
CPU and GPU hardwares [9].

The former studies usually focus on the algorithm optimiza-

2
2
0
2

p
e
S
5

]
S
M

.
s
c
[

1
v
7
7
8
1
0
.
9
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
tion and performance achievement with either CPU or GPU.
However, few articles consider the performance portability
across multi-core CPU and many-core accelerators, as they
are generally used in high performance computing (HPC)
system. The latest TOP500 list shows that six out of the top
ten supercomputers utilize many-core accelerators, this trend
brings not only a good opportunity to accelerate DG algorithm
on GPU, but also barriers in programming to migrate code
on diverse computing architectures.

Therefore, this work aims to assess three programming
models, namely OpenMP, OpenACC, and CUDA on both
multi-core CPU and many-core accelerator, in the context
of an unstructured grid high-order DG application called
HOUR2D. The aspects to be compared are the implemen-
tational effort and performance evaluation, as well as the
optimizations during code migration. Finally, we evaluate the
perforamnce and portability to give insights on appropriate
language selection for different purposes. The work provides
novelty from several perspectives:

• We give a high-performance migration of HOUR2D
application in OpenMP, OpenACC, and CUDA, respec-
tively, the parallelization in different programming mod-
els is elaborately chosen to adapt unique data structures
and computing formats.

• We perform the grid renumbering method to optimize
irregular memory access issue caused by the unstructured
grid, then carry out the mixed ﬂoating-point method to
improve computational performance, at last, the Uniﬁed
Memory(UM) method are tested in OpenACC for work-
load reduction.

• We evaluate the performance and productivity, ﬁrstly the
speedup of three programming models is listed using six
different grid meshes, then some insights are indicated
by using the rooﬂine model, at last, a comprehensive as-
sessment is displayed together with productivity metric.
This work is organized as follows: Section 2 gives a brief
review of the mathematical method and typical data pattern
used in HOUR2D. Section 3 displays the implementation to
achieve high performance for three prrgramming languages
and the evaluation approaches. Section 4 presents the exper-
iment of performance and productivity in HOUR2D code.
Section 5 summarizes our whole research and gives insights.

II. HOUR2D MATHEMATICAL METHOD

HOUR2D can solve steady or unsteady ﬁeld Navier-Stoke
and Euler problems by using the explicit Runge-Kutta high-
order DG algorithm, and it’s mainly composed of three parts:
preprocessing, ﬂow ﬁeld calculation, and output postprocess-
ing. The application uses the same high-order DG numerical
method to discretize the Euler equation and Navier-Stokes
equation, the governing equation, and the detailed numerical
discretization process are as follows.

regard to volume force and external heating are:

∂Q
∂t

+

∂E
∂x

+

∂F
∂y

= ivis · (

∂Ev
∂x

+

∂Fv
∂x

)

(1)

where Q is conserved variable, while (E, F )/(Ev, Fv) are
inviscid/viscous ﬂux in x,y direction respectively, eq. (1) is
Euler equation when ivis = 0, otherwise is NS equation when
ivis = 1. The detailed formation is











Q =

, E =

, F =

ρ
ρµ
ρν
e









ρµ
ρµ2 + p
ρµν
(e + p)µ









ρν
ρνµ
ρν2 + p
(e + p)ν









(2)

Eν =







0
τxx
τxy
µτxx + ντxy − qx







, Fν =







0
τyx
τyy
µτyx + ντyy − qy







(3)

where ρ is the density, and µ,ν is the velocity in x, y
direction,p is the pressure, and e is total ﬂuid energy per unit
volume written as:
(cid:20)

e = ρ

U +

(cid:21)
(µ2 + ν2)

=

1
2

p
γ − 1

1
2

+

ρ(µ2 + ν2)

(4)

while U is the internal energy per unit mass of ﬂuid:

U = CνT = RT / (γ − 1) = p/ρ/ (γ − 1)

(5)

where Cν = R/(γ − 1) is speciﬁc heat capacity at
constant volume,T is the temperature obtained from state
equation p = ρRT ,R is gas constant,γ is speciﬁc heat
ratio.τxx, τxy, τyx, τyy are the weights of viscous stress tensor,
Stokes’ hypothesis for a Newtonian, isotropic ﬂuid can be
written as:
(cid:20)τxx
τyx

(cid:20)2/3(2µx − νy)
µy + µx

µy + µx
2/3(2νy − νx)

τxy
τyy

= µ

τ =

(cid:21)

(cid:21)

(6)

where ν is dynamic viscosity coefﬁcient.

B. DG Discretization

The governing equation in eq. (1) can be abbreviated as

Qt + (cid:53) ·

−→
F (Q) − (cid:53) ·

−→
Fν (Q, (cid:53)Q) = 0

(7)

compared with the second-order ﬁnite volume (FVM) dis-
cretization method, all the physical quantities of every el-
ement obey high-order polynomial distribution in DG dis-
cretization way, which means the corresponding distribution
polynomial coefﬁcients are needed, scilicet, degrees of free-
dom.

Q (t, x, y) =

cj (t) bj (x, y)

(8)

(cid:88)

j

A. Governing Equation

In the Cartesian coordinate system, the two-dimensional
conserved form Navier-Stokes or Euler equations without

cj in eq. (8) is the physical quantity coefﬁcient in element
which varies over time, bj is basis function, then introducing
auxiliary variables based on eq. (8):

Qt + (cid:53) ·

−→
F (Q) − (cid:53) ·

−→
Z (Q) = (cid:53)Q
−→
−→
Z ) = 0
Fv(Q,

(9)

afterward multiplying again by basis function bj, at last, we
get a discrete form of the discontinuous ﬁnite element using
integration by parts :

(cid:90)

−

Q · (cid:53)bjdΩ +

−→
Z bjdΩ =

(cid:90)

Ω

(cid:90)

i

ˆQ (cid:0)Q−, Q+(cid:1) −→n bjdΓ
(cid:90)

Ω

(cid:90)

+

ˆQb (cid:0)Q−, Qb(cid:1) −→n bjdΓ

QtbjdΩ

(cid:90)

−

b
(cid:16)−→

F (Q) −

(cid:16)

−→
Fv

Q,

−→
Z

Ω
(cid:17)(cid:17)

· (cid:53)bjdΩ

Ω

(cid:90)

i
(cid:90)

+

+

−→
Hc

(cid:0)Q−, Q+, −→n (cid:1) bjdΓ

(10)

−→
H b
c

(cid:0)Q−, Qb, −→n (cid:1) bjdΓ

(cid:90)

−

(cid:16)

−→
Hv

Q−,

b
−→
Z −, Q+,

−→
Z +, −→n

(cid:17)

bjdΓ

Fig. 1. The major work ﬂows for HOUR2D application.

i
(cid:90)

b

−

(cid:16)

−→
H b
v

Q−,

−→
Z −, Qb,

−→
Z b, −→n

(cid:17)

bjdΓ

= 0

The spatial integral in eq. (10) is a Gaussian Numerical
Integral of which the subscripts Ω, i, b represents the element
integral, the interior surface integral, and the boundary surface
integral respectively, while the superscript−, +, b represents
the physical quantity inside the element, outside the element,
and at the boundary conditions. Hc denotes inviscid ﬂux
which is in Roe format [10], besides Hν is viscous ﬂux in
BR1 format [11].

III. PERFORMANCE IMPLEMENTATION

Time iterations is the hotspot which occupies more than
95% of total running time which is shown detailedly in ﬁg. 1,
including time step, right-hand-side(RHS), and ﬁle output,
the RHS procedure involves inviscid ﬂux, gradient, viscous
ﬂux, and residual calculation. The RHS step is counted twice
in every time step because of the two-order explicit Runge-
Kutta method, and it consumes approximately 90% of total
running time, so it is the computationally intensive part of
the whole application indeed, therefore the primary concern
for performance optimization is RHS.

A. Data Structure and Computing Formats

The geometric topology of HOUR2D’s unstructured mesh
consists of vertices, elements, and cells as shown in ﬁg. 2,
Ei is the face unit in which stores the ﬂux variable, Ci is
the cell unit in which stores the intermediate variables that
include density, velocity, and pressure, besides the above data
is organized as the array of structure(AOS) format.

Fig. 2. Data structure and major computational modes. C is the body center
of the cell, and E is the face unit where the data is stored in AOS format.
The dotted box shows two major modes in our computations, the red dotted
arrow means the data access ﬂows in memory.

box shows. The ﬁrst one indicates the ﬂux computation which
is parallelized through elements, for every time step, the
program reads the intermediate variables from neighbor cell
units, then writes back to the current element unit after cal-
culations. The other one is the physical quantity calculations
that are stored in cells, which requires ﬂux variable around
adjacent cells at the present step, data race occurs if the
element parallelization model is still utilized.

B. Grid Renumbering

The second computing formats mentioned in the former
subsection involves the data access aroud the neighbor cells in
every central cell. However, the number of these adjacent cells
are not naturally continuous as the toplogy of unstructured
grid is irregular. Hence, resulting in indirect memory access
patterns when iterating over the cells of the unstructured grid
for the ﬂux, gradient, and residual computations.

We display two major computational modes that affect
memory access pattern and parallelization format as the dotted

To relieve the irregular memory access issue, we reorder
the grid to reduce the cache miss and improve the data

Output?YNPostprocessingPreprocessingExit the loop?NYTime stepRHSRHSInviscous FluxGradientViscous FluxResidualData I/Olocality with Reverse Cuthill Mckee (RCMK) algorithm [12].
The RCMK is developed to minimize the bandwidth in the
adjacent graph by assembling the non-zero values as close
as possible to the main diagonal [13]. So we reorder the
topological adjacency derived from the computational grid
to align the cell numbers. The optimization result is shwon
in table I, it displays the bandwidth of four unstructured grids
with different number of cells. The last two columns are the
bandwidth of cell-based adjacency matrix which shows the
degree of aggregation around the diagonal, the bandwidth is
greatly reduced according to the results of all four grids, it
can be infered that bandwidth reduction obtains 2-3 orders of
magnitude. A more detailed plot about the cell aggregation in
shown in ﬁg. 3, it exhibits the fairly good result by comparing
the adjacency before and after applying RCMK.

TABLE I
THE BANDWIDTH OF DIFFERENT COMPUTATIONAL GRIDS WHEN USING
RCMK ALGORITHM. THE ELEMENT IS THE NUMBER OF GRID CELLS,
BANDWIDTH SHOWS THE MAX AGGREGATION OF ADJACENT ELEMENTS,
THE BANDWIDTH IS GREATLY REDUCED AFTER APPLYING RCMK.

Grids

Grid1
Grid2
Grid3
Grid4

Number
of cell
2688
4032
16128
64512

Original
bandwidth
1427
3992
12410
49016

RCMK
bandwidth
69
35
67
131

C. Mixed Precision

it

The physical variables use the double-precision (DP) for-
mat to maintain good convergence and computational preci-
sion through CFD simulations compared with single-precision
(SP), however,
increases the amount of memory data
required for every ﬂoating-point operation, and the memory
wall issue is further aggravated and restricts the CFD perfor-
mance efﬁciency, under the conditions that off-chip memory
bandwidth is considered to be the constraining resource in
system performance for the foreseeable future [14]. The
mathematical evidence proves that there’s no need to use DP
format in the all physical parameters or the whole calculation
procedures, even if taking into account the constraint of given
computational precision [15], [16], so CFD application turn to
use lower ﬂoating-point precision variables to achieve higher
performance, naturally some mixed-precision(MP) strategies
emerged [17], [18].

One of the methods is called the iterative reﬁnement
it employs SP parameters in previous
approach, at ﬁrst,
iterations to improve the computational performance, then
uses DP format immediately when the convergence rebound
occurs, so convergence, precision, and high performance are
guaranteed at the same time. We verify this approach in
HOUR2D by using a NACA airfoil mesh grid as the ﬁg. 4
shows, ﬁg. 4 illustrates the L2 residual convergence, in which
MP-30k stands for the initial 30 thousand steps are computed
in SP format, while the rest of the steps are computed in DP,
it’s obvious that the precision of the output result decline as
the SP step increases.

Fig. 3.
The intuitive illustration for bandwidth reductions of four com-
putional grids. The upper ﬁgures show the original adjacency, while the lower
ﬁgures are adjacency after applying RCMK.

Following the renumbering, the cell is arranged in the order
of monotonical index, and a consecutive rank is performed
index according to the ﬁrst constant cell
on the element
index, so the second index reference will be visited in as-
cending order. The staple beneﬁts of the optimizations above
can be summarized as enhancing both spatial and temporal
locality particularly when cells are referenced contiguously
in memory and exploiting the available memory bandwidth.
What’s more, the renumbering technique is executed only
once before the time iteration starts for every process, hence
it has a negligible effect on the whole application execution
not
to mention the time iteration is usually a fairly big
number. The overall performance improvement by using the
grid renumbering method is about 20%-30%.

Fig. 4. Mixed precision approach veriﬁcation using NACA grid. The
comparison of the residual convergence in MP methodology with different
time steps allocated in SP and DP, for example, MP-30k means the initial
30000 iterations are executed in SP format, the rest of the iterations are
computed in DP.

We assess the performance improvement of our MP method
after the correctness veriﬁcation with two grid examples as
table I shows. The experiment result is shown in table II, in
which the total time steps are 100 thousand, too. DP shows
the benchmark of our test, the run time decreases as the SP
step increases. The last row SP indicates the most optimal
improvement with no DP format variables are used at all, we
can conclude from the table that the MP method achieves

grid1grid2grid3grid415% performance improvement at most compared with the
DP format. However, the SP format is only comparable in
performance, as the computational precision is not strictly
guaranteed to meet
in diverse grids from
mathematical point of view, the SP format are usually not used
in engineering computations, so the most optimal speedup is
usually only a theoretical conclusion.

the constraint

TABLE II
CLOCK TIME OF OUR MIXED PRECISION APPROACH RUNNING ON TWO
GRIDS. THE DP MEANS THE DOUBLE PRECISION CALCULATION, AND SP
IS SINGLE PRECISION FORMAT, THE MP-20K MEANS THE STARTING
20000 ITERATIONS ARE EXECUTED IN SP FORMAT, FOLLOWED BY THE
REST OF THE ITERATIONS COMPUTED IN DP.

Time
DP
MP-20k
MP-50k
MP-80k
SP

Grid1
51.87
49.68
48.30
46.68
43.87

Grid2
151.91
145.83
140.90
135.95
131.40

D. Heterogeneous Implementations

We port HOUR2D code using both Compute Uniﬁed De-
vice Architecture(CUDA) and OpenACC on Nvidia GPU to
get an apparent comparison in performance and productivity.
CUDA usually gains a better performance than OpenACC
due to the ﬁne-grained manipulation of hardware resources
this generally requires the time-
within kernels, however,
consuming task of code revising to make use of the resources
for developers. On the contrary, OpenACC is a declarative
model using compiler pragmas which is not as effective as
CUDA in computing performance.

The minimization of data transmission between CPU and
GPU hardware is at prior consideration in HOUR2D as
it’s an expensive operation over the time iterations, so the
covered data is transported to GPU memory in the grid
initialization stage to avoid the potential bottleneck caused by
the frequent data transfer. We accommodate the grid data with
the appropriate component from the GPU memory hierarchy
like shared memory, constant memory, and global memory
to achieve better memory access performance. For example,
HOUR2D high-order algorithm needs 393 geometrical con-
stants to accomplish element-integral and cell-integral in each
time iteration step, so we place it in the constant memory to
reduce memory transactions launched by thread warp.

As for the code migration, we port the whole iteration code
into GPU as the blue dotted box shows in ﬁg. 1, on the one
hand, the parallelization in RHS step can be implemented
naturally in the way of ﬁg. 2 shows without any data racing,
the element-based or cell-based loop can be mapped to the
one-dimensional thread in GPU directly.

On the other hand, the time step computation involves the
minimal value of all local time steps in cell data, making
data racing if it’s executed in direct parallelization. To achieve
effective performance in entire computing procedures, we use
GPU reduction to get minimal value by taking advantage of

shared memory which enables synchronization within thread
blocks. Furthermore, we design the consecutive reduction
method to realize robust reduction under the restriction of
max thread number in GPU block.

Porting the code by using OpenACC is much more
than CUDA by using compiler pragmas to
convenient
the time
parallelize compute-intensive code, for example,
step reduction mentioned above only needs
to use
#pragma acc parallel reduction instead, and the speciﬁed
process will be done by PGI compiler, but the performance
also highly depends on the data layout in PGI compiler and
the effectiveness of code generation.

UM is a pragmatic technology that can be used by CUDA
6.X or higher version, it manages the data between CPU and
GPU independently which was relied on programmers, hence
explicit function calls and data migration can be decreased
evidently. We adopt the UM method during heterogeneous
code migration by simply adding the managed compilation
option so that memory space between host and device is
established.

UM leads to possible performance loss due to the ad-
ditional memory transmission and memory page fault [19],
which is also observed in our OpenACC code, generally,
the performance loss decreases as the grid size increases,
eventually stabilizing below 3% especially when the grid’s
element exceeds sixteen thousand, this may be caused the
intensiﬁcation of discontinuous memory access as grid size
increases.

IV. EXPERIMENTS AND ANALYSIS

The HOUR2D code is fulﬁlled at the same level of op-
timization in three different programming models: OpenMP,
OpenACC, and CUDA, while the computing hardware in-
cludes a computing node consisting of an Intel Xeon E5-2698
@2.20GHz CPU and an Nvidia A100 GPU, as for compiler
toolchain, we use GCC − 5.4.0 for CPU code with optimiza-
tion ﬂag −O3, and the GPU compiler N V CC − 10.2.89 for
CUDA and OpenACC. The computational grid is illustrated in
table III which includes two steady NACA airfoil grids(grid1
and grid2), four unsteady front step grids (grid3-grid6), and
an unsteady ﬂow around double cylindrical(grid7). The latter
grid is four times larger than the former one for those possess
same aerodynamic shape, the last column is the grid type of
every element.

A. Evaluation Approach

As the main hotspot of HOUR2D is time iteration that
performs almost the same operation, and tens of thousands
of time steps are usually required to reach a convergent
result in practical CFD simulation, the preprocessing and
postprocessing parts should be removed for the seek precise
evaluation. We apply Dual-phase measurement [20] method
to exclude the non-primary impacts during the performance
experiments. Speciﬁcally, the application is tested twice with
different iterations, the subtraction data between two iterations
shows the difference of exact iteration computations. Hence,

Fig. 5.
steps . (b) The pressure of double cylindrical, with Mach number=0.1, Reynolds number=100 at 400 thousand time steps.

Streamline diagram of the real computing results. (a) The density of grid2, with Mach number=0.3, Reynolds number=5000 at 400 thousand time

(a)

(b)

TABLE III
DETAILED INFORMATION OF COMPUTATIONAL GRIDS USED IN
SUBSEQUENT EXPERIMENTS. NODES AND CELLS SHOW THE
COMPUTATIONAL SCALE, THE DIFFERENCE OF GRID SHAPE REFLECTS
THE NUMBER OF FACES IN CELL.

Grid
grid1
grid2
grid3
grid4
grid5
grid6
grid7

Nodes
378
1428
4193
16449
65153
259329
248215

Cells
672
2688
4032
16128
64512
258048
495432

Grid shape
triangular
triangular
quadrilateral
quadrilateral
quadrilateral
quadrilateral
quadrilateral

the performance metric like Flops, DRAM bytes and run time
of the can be computed precisely in this way.

We implement the rooﬂine model to give some performance
insights into memory bandwidth and ﬂoating-point opera-
tions. Rooﬂine [21], [22] is a bound and bottleneck analysis
function, it chooses the minimum value of either current peak
machine performance, or peak DRAM bandwidth multiply
arithmetic intensity, so that it provides insights into the pri-
mary factors affecting the performance of computer systems
[23].

To measure the workload during the porting process, we ap-
ply productivity metric to quantify how efﬁciently HOUR2D
code is developed with different programming models [24],
speciﬁcally the ”code divergence” is the average of the
pairwise distances between the applications in different code
versions A,

D(A) =

(cid:19)−1

(cid:18)|A|
2

(cid:88)

d(ai, aj), ({ai, aj} ⊂ A)

(11)

in which da,b originates from the same code in the number of
Source Line of Code(SLOC) [25] normalized to the smaller
application, the detailed equation is shown in eq. (12).

d(a, b) =

|SLOC(a) − SLOC(b)|
min(SLOC(a), SLOC(b))

(12)

B. Performance Results

We use the complete grids to verify the correctness of
output ﬂow ﬁeld, two intuitive shapes are choosen to display
the stable ﬂow ﬁeld as ﬁg. 5 shows. The subﬁgure ﬁg. 5a
displays the ﬁnal denisty of NACA airfoil illustration, the
various color refers to differentiated values. While ﬁg. 5b
is the pressure of ﬂow around double cylindrical, the ﬂow
clusters clusters in front of the ﬁrst circular and forms
relatively high pressure, the reversed side of the circular forms
low pressure.

We choose OpenMP-based code as a benchamrk to assess
the performance of other two languages. Thread scalability
is ﬁrst evaluted with ﬁrst six computational grids for the
convenience of comparision, the run time is displayed in
ﬁg. 6, and the number of thread varies from one to twenty.
It can be inferred from the ﬁgure that
the code obtains
considerable scalability as the growth rate of the running
time is roughly the same as that of the grid size. Besides,
twenty-thread parallelization achieves a speedup of 6.0x -
8.1x comparing with serial execution according to different
grids,
the different speedup of grid may be affected by
several reasons like insuffcient parallelism for small-scale
grid, memroy access exacerbation as grid size increases and
so on.

The comparisons for three languages is applied after eval-
uating OpenMP code base, we analyze the speedup based on
the serial execution for ﬁrst six grids as ﬁg. 7 shows. The
colored columnars are presented under the same optimiation
conditions, grid renumbering and mixed precision strategy are
not included. The GPU-based code shows relatively lower
speedup than OpenMP in small-scale grids, it’s due to the
fact that the grid data is insufﬁcient in parallelism gain to
cover the data movement cost. As the grid size increases,
GPU-based code shows priority to OpenMP’s, particularly,
CUDA is much better than OpenACC owe to the ﬁne-tuning
of thread scheduling, better memory hierarchy control, and
some memory access optimizations. Generally, OpenACC’s

this may be the increasing deterioration of frequent access
to multi-dimensional array for element and cell integral, and
the most important, non-consecutive memory access caused
by the unstructured grid which is generally acknowledged as
a tough issue for HPC researchers in CFD ﬁeld. What’s more,
the achieved Flops can be ranked as CUDA ¿ OpenACC ¿
OpenMP due to the abundant thread concurrency in GPU,
particularly for the larger grids.

Fig. 6. The run time of OpenMP-based code with six different grid meshes

speedup reaches 75% to 90% of CUDA’s, besides, UM
approach is enabled in OpenACC-based code. The CUDA,
OpenMP, OpenACC based code obtains a maximal speedup
of 42.9x, 35.3x, and 8.1x among six grids comparing with
serial execution, respectively.

Fig. 7.
comparing with serial execution of original code.

The speedup of the OpenMP, CUDA, OpenACC based code

To further explore the computing performance, we build
the rooﬂine model of all three programming models to ﬁnd
the possible bottleneck with ﬂoating-point operations and
memory access, the execution details in the CPU are collected
by Intel Vtune Proﬁler, while the Nvidia Nsight Proﬁler is
utilized in GPU. The ﬁnal chart is shown in ﬁg. 8, the abscissa
is the arithmetic intensity which represents the amount of data
required(transported from DRAM to cache ) for each ﬂoating-
point operation, and the ordinate is the double-precision
ﬂoating-point operations achieved per second, besides, the red
and blue attributes belong to V100 GPU and Intel Xeon E5
CPU respectively, the solid line is the theoretical limit of the
hardware unit, and the scatter points are the measured values
through our experiments, moreover, the red dot shows the
execution of CUDA-based code while the red star belongs
to OpenACC, the different scatter points of the same color
display the analysis of different grids.

We can infer from ﬁg. 8 that the HOUR2D executions of
all three models are faced with memory access bottleneck
even if some memory optimization methods are applied,
especially for CUDA-based code when grid size extends,

Fig. 8. The illustration of rooﬂine performance model for three programming
models

C. Productivity Evaluation

Although the CUDA-based code shows the absolute per-
formance advantage over OpenMP and OpenACC, mean-
while CUDA certainly consumes the most effort on code
tunning. So we use quantitative metric detailed in eq. (12)
to further evaluate our workload as table IV shows, in which
the second column is the porting workload from desktop
version to current code, and the desktop is taken as the
benchmark version with 2900 total source lines. The metric
d(current,desktop) displays the relative workload porting from
the desktop version to the current version as the eq. (10)
shows, the fourth and ﬁfth row stands for the situation of
the bigger metric d
whether to use UM method or not,
represents more porting workload and lower productivity.
So under the basis of OpenMP-based code, the workload
of the CUDA is 8.2 times larger and the OpenACC using
UM is 2.7 times larger, however, this quantiﬁed number has
an underlying assumption that the development difﬁculty of
different programming models keeps consistent.

TABLE IV
PORTABILITY EVALUATIONS WHEN PORTING FROM DESKTOP CODE TO
THREE PROGRAMMING MODELS

Current Version
OpenM P
CU DA
OpenACCUM
OpenACCnonUM

SLOC
34
280
94
161

dcurrent,desktop(%)
1.17
9.67
3.24
5.56

The relative ef f ort time productivity(RDT P ) con-
cept connects the performance and productivity which is

Ψrelative=speedup/relative effort, it’s useful under the con-
dition of the same optimization level for three programming
models, so the sort through Ψ according to the metric above
is: ΨOpenM P > ΨCU DA > ΨOpenACC. The OpenMP per-
forms best because of the fairly high productivity, and CUDA
is better than OpenACC. It’s worth noting that RDT P takes
performance and productivity equally important which is not
always true in reality, striking the appropriate balance between
the performance and productivity relies on the ultimate use
of the code [26].

V. CONCLUSIONS AND FUTURE WORK

We have presented the migration of a practical high-order
DG application named HOUR2D into three programming
models running on two architectures, performance analysis,
and portability are further evaluated.

We ﬁrst discuss some optimization methods in the light
of data structure and computing format, the grid renumber-
ing approach is used to relieve the discontinuous memory
access, then the mixed-precision method to maximize the
ﬂoating-point operations. We also capture the performance
on CPU and GPU platforms with seven grids, concretely
the CUDA, OpenMP, and OpenACC based code achieves
roughly a speedup of 42.9x, 8.1x, and 35.3x based on the
serial execution, then we ﬁnd the memory access issue by
using the rooﬂine model which needs further research, the
productivity of different programming models are measured
with quantiﬁed metric.

We plan to carry out our future work in two aspects. On
the one hand, we improve the memory bandwidth and the
ﬂoating-point efﬁciency of the high-order algorithm. On the
other hand, we evaluate the performance portability of our
algorithm in more programming models to offer a practical
assessment to the CFD developers.

REFERENCES

[1] N. Grube, E. Taylor, and P. Martin, “Assessment of weno methods
with shock-conﬁning ﬁltering for les of compressible turbulence,” in
18th AIAA Computational Fluid Dynamics Conference, p. 4198, 2007.
[2] W. H. Reed and T. R. Hill, “Triangular mesh methods for the neu-
tron transport equation,” tech. rep., Los Alamos Scientiﬁc Lab., N.
Mex.(USA), 1973.

[3] M. Li, W. Liu, L. Zhang, and X. He, “Applications of high order
hybrid dg/fv schemes for two-dimensional rans simulations,” Procedia
Engineering, vol. 126, pp. 628–632, 2015. Frontiers in Fluid Mechanics
Research.

[4] L. Zhang, L. Wei, H. Lixin, D. Xiaogang, and Z. Hanxin, “A class of
hybrid dg/fv methods for conservation laws ii: Two-dimensional cases,”
Journal of Computational Physics, vol. 231, no. 4, pp. 1104–1120,
2012.

[5] Y. Xia, L. Luo, and H. Luo, “Openacc-based gpu acceleration of a
3-d unstructured discontinuous galerkin method,” in 52nd Aerospace
Sciences Meeting, p. 1129, 2014.

[6] W. Pazner and P.-O. Persson, “Stage-parallel fully implicit rungekutta
solvers for discontinuous galerkin ﬂuid simulations,” Journal of Com-
putational Physics, vol. 335, pp. 700–717, 2017.

[7] D. Zhijian and X. Gongnan, “Parallel discontinuous galerkin ﬁnite
element method for computing hyperbolic conservation law on unstruc-
tured meshes,” International Journal of Numerical Methods for Heat
& Fluid Flow, vol. 31, 2021.

[8] M. S. Fabien, M. G. Knepley, R. T. Mills, and B. M. Rivi`ere, “Manycore
parallel computing for a hybridizable discontinuous galerkin nested
multigrid method,” SIAM Journal on Scientiﬁc Computing, vol. 41,
no. 2, pp. C73–C96, 2019.

[9] M. Kronbichler, N. Fehn, P. Munch, M. Bergbauer, K.-R. Wichmann,
C. Geitner, M. Allalen, M. Schulz, and W. A. Wall, “A next-generation
discontinuous galerkin ﬂuid dynamics solver with application to high-
resolution lung airﬂow simulations,” in Proceedings of the International
Conference for High Performance Computing, Networking, Storage and
Analysis, SC ’21, (New York, NY, USA), Association for Computing
Machinery, 2021.

[10] P. L. Roe, “Approximate riemann solvers, parameter vectors, and
difference schemes,” Journal of computational physics, vol. 43, no. 2,
pp. 357–372, 1981.

[11] B. Cockburn and C.-W. Shu, “The runge–kutta discontinuous galerkin
method for conservation laws v: multidimensional systems,” Journal of
Computational Physics, vol. 141, no. 2, pp. 199–224, 1998.

[12] E. Cuthill and J. McKee, “Reducing the bandwidth of sparse symmet-
ric matrices,” in Proceedings of the 1969 24th national conference,
pp. 157–172, 1969.

[13] D. Burgess and M. B. Giles, “Renumbering unstructured grids to
improve the performance of codes on hierarchical memory machines,”
Advances in Engineering Software, vol. 28, no. 3, pp. 189–201, 1997.
[14] D. A. Patterson, “Latency lags bandwith,” Communications of the ACM,

vol. 47, no. 10, pp. 71–75, 2004.

[15] A. Abdelfattah, H. Anzt, E. G. Boman, E. Carson, T. Cojean, J. Don-
garra, A. Fox, M. Gates, N. J. Higham, X. S. Li, et al., “A survey of
numerical linear algebra methods utilizing mixed-precision arithmetic,”
The International Journal of High Performance Computing Applica-
tions, vol. 35, no. 4, pp. 344–369, 2021.

[16] F. Tisseur, “Newton’s method in ﬂoating point arithmetic and iterative
reﬁnement of generalized eigenvalue problems,” SIAM Journal on
Matrix Analysis and Applications, vol. 22, no. 4, pp. 1038–1057, 2001.
[17] I. Kampolis, X. Trompoukis, V. Asouti, and K. Giannakoglou, “Cfd-
based analysis and two-level aerodynamic optimization on graphics
processing units,” Computer Methods in Applied Mechanics and Engi-
neering, vol. 199, no. 9-12, pp. 712–722, 2010.

[18] P. Gomes, T. D. Economon, and R. Palacios, “Sustainable high-
performance optimizations in su2,” in AIAA Scitech 2021 Forum,
p. 0855, 2021.

[19] W. Li, G. Jin, X. Cui, and S. See, “An evaluation of uniﬁed memory
technology on nvidia gpus,” in 2015 15th IEEE/ACM international
symposium on cluster, cloud and grid computing, pp. 1092–1098, IEEE,
2015.

[20] Y. Che, L. Zhang, Y. Wang, C. Xu, W. Liu, and Z. Wang, “Microar-
chitectural performance comparison of intel knights corner and intel
sandy bridge with cfd applications,” The Journal of Supercomputing,
vol. 70, no. 1, pp. 321–348, 2014.

[21] S. Williams, A. Waterman, and D. Patterson, “Rooﬂine: an insightful
visual performance model for multicore architectures,” Communications
of the ACM, vol. 52, no. 4, pp. 65–76, 2009.

[22] M. Harris, “Mapping computational concepts to gpus,” in ACM SIG-

GRAPH 2005 Courses, pp. 50–es, 2005.

[23] E. D. Lazowska, J. Zahorjan, G. S. Graham, and K. C. Sevcik, Quan-
titative system performance: computer system analysis using queueing
network models. Prentice-Hall, Inc., 1984.

[24] S. L. Harrell, J. Kitson, R. Bird, S. J. Pennycook, J. Sewall, D. Jacobsen,
D. N. Asanza, A. Hsu, H. C. Carrillo, H. Kim, et al., “Effective
performance portability,” in 2018 IEEE/ACM International Workshop
on Performance, Portability and Productivity in HPC (P3HPC), pp. 24–
36, IEEE, 2018.

[25] V. Nguyen, S. Deeds-Rubin, T. Tan, and B. Boehm, “A sloc counting

standard,” in Cocomo ii forum, vol. 2007, pp. 1–16, Citeseer, 2007.

[26] J. , M. Oivo, and A. Jedlitschka, “Software productivity and effort
estimation,” Journal of Software: Evolution and Process, vol. 27, no. 7,
pp. 465–466, 2015.

