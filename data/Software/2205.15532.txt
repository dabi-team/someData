2
2
0
2

y
a
M
1
3

]
E
S
.
s
c
[

1
v
2
3
5
5
1
.
5
0
2
2
:
v
i
X
r
a

Dataset Bias in Android Malware Detection

1st Yan Lin
Beijing University of Posts and Telecommunications
linyan@bupt.edu.cn

2nd Tianming Liu
Monash University
Tianming.Liu@monash.edu

3rd Wei Liu
Tsinghua University
lw21@mails.tsinghua.edu.cn

4th Zhigaoyuan Wang
Beijing University of Posts and Telecommunications
wangzgy@bupt.edu.cnD

5th Li Li
Monash University
li.li@monash.edu

6thGuoai Xu
Beijing University of Posts and Telecommunications
xga@bupt.edu.cn

6thHaoyu Wang
Huazhong University of Science and Technology
haoyuwang@hust.edu.cn

Abstract—Researchers have proposed kinds of malware de-
tection methods to solve the explosive mobile security threats.
We argue that the experiment results are inﬂated due to the
research bias introduced by the variability of malware dataset.
We explore the impact of bias in Android malware detection
in three aspects, the method used to ﬂag the ground truth,
the distribution of malware families in the dataset, and the
methods to use the dataset. We implement a set of experiments
of different VT thresholds and ﬁnd that the methods used to
ﬂag the malware data affect the malware detection performance
directly. We further compare the impact of malware family types
and composition on malware detection in detail. The superiority
of each approach is different under various combinations of
malware families. Through our extensive experiments, we showed
that the methods to use the dataset can have a misleading impact
on evaluation, and the performance difference can be up to
over 40%. We argue that these research biases observed in
this paper should be carefully controlled/eliminated to enforce
a fair comparison of malware detection techniques. Providing
reasonable and explainable results is better than only reporting
a high detection accuracy with vague dataset and experimental
settings.

Index Terms—Malware detection, Dataset bias, Mobile App,

Android.

I. INTRODUCTION

With the explosion of smartphones and mobile apps, the
number of mobile malware has been growing rapidly as
well [1]. Millions of malicious apps were identiﬁed every year,
with complex malicious payload, and sophisticated evasion
techniques. The ofﬁcial Android app market, Google Play, was
reported to be affected by malware from time to time [2], [3],
which causes great risks to mobile users.

The increasing mobile security threats have attracted a large
number of research efforts in our community. Researchers
have proposed different kinds of malware detection tech-
niques, e.g., signature-based approaches [4]–[7], behavior-
based approaches [8]–[11], and machine-learning based ap-
proaches [12]–[15], etc. Almost all the existing studies have
reported promising results. For example, Drebin [13] was
reported to achieve a detection rate of 94% in Android
malware detection, ICCDetector [16] was reported to achieve

an accuracy of 97.4%, and MADAM [17] reports to achieve
an overall malware detection accuracy of 100%.

Is Android malware detection a solved problem? Consider-
ing the high accuracy reported in previous studies, it seems
that there exists very little room for improvement. However,
in this paper, we argue that
the effectiveness of malware
detection approaches is highly related to the dataset used in
the evaluation, and the experiment results are inﬂated due
to research bias introduced by the variability of the malware
dataset and the methods to use the dataset.

Representative malware benchmarks. There are some
widely used malware datasets in the research community. For
example, MalGenome dataset [18] was constructed in 2012,
and it contains over 1,000 malware samples, which were
used by hundreds of studies to evaluate their effectiveness
on malware detection. Drebin [13] is another widely used
benchmark, with over 5,000 malware samples. AMD [19] is
one of the largest malware datasets used in the research com-
munity, with over 24 thousand samples covering 71 malware
families in total. RmvDroid [20] takes advantage of the app
maintenance behaviors of Google Play to help label malware,
which contains 9,133 malware samples across 56 malware
families. Besides, a number of papers [21]–[23] have created
their own datasets based on existing anti-virus engines for
malware analysis, detection, and classiﬁcation.

Research Bias introduced by dataset. Existing malware
datasets were created by different research groups, and they
were constructed using ad-hoc approaches followed by dif-
ferent criteria (e.g.,
labeling methods, family distribution,
and splitting methods of training/testing), which may cause
potential research bias1.

First, the methods used to ﬂag the ground truth are
ad-hoc and usually vary greatly. Besides MalGenome [18],
which was manually created by analyzing the security reports
released by security companies, almost all
the other mal-
ware datasets were created based on the detection results of

1We use the term ’bias’ to refer to, different criteria to create and use the

malware dataset can have a misleading impact (bias) on evaluations.

1

 
 
 
 
 
 
TABLE I: A list of widely used Android malware benchmarks. The method/source indicates the method used to ﬂag the ground truth of
each dataset. The # Families indicates the number of malware families in each dataset, either reported by the original author or classiﬁed by
AVClass [24]. The top-3 families and top-3 types indicate the most popular malware families and types in each dataset. The column adware
indicates whether each dataset treats adware as a kind of malware.

Dataset

Time

# Malware Method/Source

# Families

# Top-3 Families

# Top-3 Types

Adware

MalGenome [18]

2010-2012

1,234

Security Reports

Drebin [13]

2013

5,560

VT (2 of 10 engines)

Piggybacking [25]

2016

1,136

VT (>=1 engine)

AMD [19]

2010-2016

24,553

VT (>=28 engines)

RmvDroid [20]

2014-2018

9,133

VT (>=10 engines)
& Removed by Google Play

49

179

100

71

56

DroidKungfu (38%)
Basebridge (25%)
Geinimi (5%)
FakeInstaller (17%)
DroidKungfu (12%)
Plankton (11%)
Dowgin (24%)
Kuguo (22%)
Gingermaster (6%)
Droidkungfu (20%)
Airpush (8%)
Ginmaster (8%)
Airpush (32%)
Mecor (11%)
Plankton (9%)

Trojan (94%)
Exploit (3%)
Spyware (1%)
Trojan (76%)
Malware (18%)
Exploit (2%)
Adware (64%)
Trojan (25%)
Spyware (2%)
Trojan (42%)
Adware (34%)
Exploit (11%)
Adware (79%)
Trojan (13%)
Spyware (5%)

NO

NO

(cid:88)

(cid:88)

(cid:88)

VirusTotal 2, a widely-used online malware detection service
containing over 60 anti-virus engines. The general method is
to collect mobile apps in the wild (e.g., Google Play and
third-party app markets), and then use the detection results
of VirusTotal to label them. However, there are no standards
on how to take advantage of the detection results to label
malware. In this context, researchers use their intuition and
adopt ad-hoc methods to label the apps and release the dataset
to the research community as benchmarks. As there are over
60 engines that report the detection results, they usually use
different thresholds of detection engines on VirusTotal to label
malware samples. For example, Drebin [13] was created based
on the results of 10 engines on VirusTotal, i.e., one sample is
selected as long as two of the 10 engines ﬂagged the sample as
malicious. The Piggybacking [25] dataset used 1 engine as the
threshold, the TESSERACT [22] sets the threshold as 4, and
AMD [19] used 28 engines (over 50% of the engines) as the
threshold. It is unknown to us to what extent do the malware
labelling methods affect the malware detection results.

Second,

the malware

families distributed in the
datasets are unbalanced and vary greatly. For example,
MalGenome [18] dataset covers 49 families, and each family
contains 1 to 309 malware samples. The top-3 families occupy
roughly 70% of the overall dataset, while over 30 families
have less than 10 samples. The distribution suggested that,
as long as the detection approach can successfully detect
the top families, the overall result will be good enough. As
machine-learning based malware detection approaches highly
rely on the training dataset, it is unknown to us how does
the construction of the dataset affect the malware detection
results.

Third,

the methods to use the dataset may affect
the detection results. Traditionally, machine learning-based
approaches are assessed in a cross-validation scenario that
validates the classiﬁcation model by assessing how the result
will generalize to an independent dataset. To estimate how the

2https://www.virustotal.com

2

prediction model will perform in practice, a cross-validation
scenario partitions the sample data into 2 subsets. The ﬁrst
subset is used for learning analysis, i.e., building the model
during the training phase. The second subset is used to validate
the model. To reduce the variability of the results, multiple
rounds are performed and the results are averaged over the
rounds. A well-known type of cross-validation is the 10-Fold
cross-validation, i.e., the sample data was randomly partitioned
into 10 subsets, 9 of which are used for training and 1 for
validation. However, no previous work has analyzed the bias
introduced by “randomly” partitions. For example, there are
10 malware families in a malware dataset. When performing
malware detection (note here, we mention malware detection,
not classiﬁcation), the ideal case should be that, for each kind
of malware, we have known its 90% of samples, and predict
the remaining 10%. However, the extreme case might be that,
training use samples of 9 families and the remaining 1 family
is used for prediction. Almost all previous studies have not
carefully explained how they use the dataset, thus we argue
that there might be research bias when evaluating different
approaches.

As aforementioned, almost all the existing malware detec-
tion studies directly use the labelled malware datasets in an ad-
hoc way, without considering the research bias and potential
impact introduced by the malware dataset.

In this paper, we take a data-driven method to research the
research bias introduced by the variability of malware dataset.
We mainly focus on three aspects, the method used to ﬂag
the ground truth, the distribution of malware families in the
dataset, and the methods to use the dataset. We ﬁrst resort to
the Androzoo dataset to obtain a malware corpus containing
690,544 samples. Then we create a number of experimental
datasets under different conditions to investigate the potential
bias introduced by malware datasets.

First, we explore the impact of VirusTotal threshold on
malware detection. We constructed 23 sets of malware bench-
marks including the VT numbers from 1 to 30+. We further

apply csbd, drebin, and mamadroid to these benchmarks.
Interestingly, we observe that the VT threshold could affect the
malware detection directly. The malware detection results of
the algorithm increase with the increasing of VT thresholds
at ﬁrst, and then stabilize to some extent. The evaluation of
different algorithms may also differ when labeling malware
with different VT thresholds.

Second, we compare the impact of malware family numbers,
types and composition on malware detection in detail. We ran-
domly select 10 popular malware families from the malware
corpus. We then curate a number of malware datasets with
different numbers or different compositions of malware fam-
ilies. We ﬁnd that the performance of the malware detection
method is very related to the constructed dataset. The malware
detection results of an algorithm can vary when different
datasets are constructed.

Third, we explore the methods to use the dataset when
evaluating the malware detection methods. To be able to
present the results more visually, we consider the ideal case
and the extreme case of the sample partition methods in cross-
validation. We compare the ideal case with the extreme case
using 5-Fold cross validation Through our extensive experi-
ments, we showed that the methods to use the dataset can
have a misleading impact on evaluation, and the performance
difference can be up to over 40%.

Our research work’s contributions are summarized as the

followings:

• We make the ﬁrst systematic study of the dataset bias in
Android malware detection. Speciﬁcally, we focus on the
variability of the malware dataset and the methods to use
the dataset.

• We make effort to curate a number of malware bench-
marks under different conditions and evaluate their per-
formance gap of malware detection using existing tech-
the detection results can even
niques. We found that
be manipulated by controlling the dataset to make one
speciﬁc approach achieve the best result.

• We measure the inﬂuence of inconsistent datasets on
malware detection assessments. We believe that a ma-
chine learning fairness framework for malware detection
is needed to highlight pros and cons of each approach,
and provide reasonable and explainable results.

This paper is organized as follows. In Section II, we
present the related work and background on Android malware
benchmarks and Android Malware Detection. In Section III,
we present our research questions and the experimental setup
in detail. We analyze the impact of malware labelling methods
in Section IV. We then further characterize how the variation
of malware families affects the malware detection (seeSection
V). In Section VI, we investigate the methods to use the
dataset in the malware detection, focusing on the ”randomly”
partitions of the cross validation. In Section VII, we discuss
the implication and limitation of our paper. Finally, we draw
our conclusion in Section VIII.

II. RELATED WORK AND BACKGROUND

A. Android Malware Benchmarks

Our community has created a number of datasets to facilitate
Android malware research. We have summarized some of them
in Table I.

MalGenome dataset was created in 2012 based on man-
ually examining the security reports released by anti-virus
companies. It has over 1,200 malware samples, covering 49
families. Over 94% of the samples in the dataset are Trojan,
and adware is not considered to be malware in this dataset.
The top-2 families, i.e., DroidKungFu and Basebridge occupy
over 60% of the samples. MalGenome was widely used by
our research community as the benchmark to perform malware
detection [26]–[28]. Drebin is another widely used benchmark,
which was created in 2013. It has over 5,000 malware samples,
and it was created based on the detection results of 10 famous
anti-virus engines on VirusTotal. One sample is considered
to be malicious as long as two of the 10 engines ﬂagged the
sample as malicious. Over 76% of the samples are Trojan, and
adware is also not considered in this dataset. Piggybacking
dataset was created based on the app clone detection results
(i.e., malicious apps were created on top of popular legitimate
apps) in 2016. Apps will be regarded as malicious as long
as they are ﬂagged by at least one engine on VirusTotal. It
contains 1,136 samples, and adware occupies a large portion
of the samples (64%). The AMD dataset was created in
2016, with a large number of malware samples. It contains a
considerable number of samples overlapped with MalGenome
and Drebin, as it collected samples from multiple sources
including existing malware datasets. RmvDroid was created in
2019, in order to overcome the challenge of malware sample
labelling. They rely on both VirusTotal and Google Play’s app
maintenance practice to create the dataset. For the apps ﬂagged
by over 20 engines on VirusTotal and further be removed from
Google, they will regard them as malware. RmvDroid also
considers adware as malicious, which occupies 79% of the
samples.

Besides,

there are some other datasets created by the
research community using the similar approach,
including
Android PRAGuard dataset [29], AAGM dataset [30], the
Android Malware Dataset [31], and KronoDroid dataset [32]
etc.

AAGM dataset [30] was constructed in 2017. Applications
in AAGM dataset were labelled as malware as they were
ﬂagged by more than two Anti-Virus products in Virusto-
tal web service.It has 1900 applications including benign
and 12 different families. It contains 400 malware samples,
adware(250) and general malware(150). Android PRAGuard
dataset [29] contains 10479 samples. It has over 50 malware
families. They did not use VirusTotal or AndroTotal because of
their limitations for their study. The Android Malware Dataset
[31], is a public dataset consisting of 24,553 malware samples.
Each app in the Android Malware Dataset is scanned by 55
antivirus products from VirusTotal. The sample is labelled
as malware when over 28 anti-virus products used in the

3

VirusTotal recognize it as a malware. It has 71 malware
families and collected between 2010 and 2016. KronoDroid
dataset [32] is a hybrid-featured Android dataset. The malware
dataset of KronoDroid is composed of Drebin, AMD, Virus-
Total Academic Malware Samples and VirusShare. Thus, all
malware samples are labeled by the result of Virustotal with
different Virustotal threasholds. It has two public datasets. The
emulator dataset is composed of 28,745 malware from 209
malware families and the real device dataset contains 41,382
malware, belonging to 240 malware families.

As aforementioned, these datasets were created under dif-
ferent criteria, including malware labelling methods, dataset
construction, etc. We argue that these different criteria may
introduce research bias in malware detection, which is the
main focus of this paper.In order to analyze the bias of datasets
with different criteria, we did not use these public datasets
directly. We reconstructed many datasets according to these
criteria metioned above for the study in this paper.

B. Android Malware Detection

Many research efforts were focused on malware detection in
our community. They mainly could be classiﬁed as signature-
[4]–[7], behavior-based approaches [8]–
based approaches
[11], and machine-learning based approaches [12]–[15], etc.
DroidAPIMiner [15] performs malware detection based
on features generated at API level. Drebin is a lightweight
detection method that uses static analysis to gather the most
important characteristics of Android applications including
permissions, API calls, components, etc. It uses SVM algo-
rithm to detect whether a given sample is malicious or not.
Csbd [33] was proposed to detect malware using structural
features, i.e., CFG signatures. It constructs CFGs of individ-
ual methods and encodes them as text-signatures. Then, a
Random Forest classiﬁer is trained with these signatures to
detect malware. DroidSIFT [6] builds contextual API depen-
dency graphs that provide an abstracted view of the possible
behaviors of malware and employs machine learning and
graph similarity to detect malicious applications. MudFlow
[34] leverages the analysis of ﬂows between APIs to detect
malware. DroidDetector [35] build Deep Belief Networks for
Android malware detection using human engineered features,
including required permissions, sensitive API calls, and certain
dynamic behaviors. Deep4maldroid [36] constructs weighted
directed graphs from Linux kernel system calls and use
them to train deep neural networks for malware detection.
Mclaughlin et al. [14] proposed to detect malware by applying
a deep neural network to the raw op-codes extracted from
the dex bytecode. Mamadroid [11] is an Android malware
detection system based on modeling the sequences of API
calls as Markov chains. This detection system depends on app
behavior and builds a model in the form of a Markov chain.
Although previous studies have reported promising results
on malware detection,
is still unknown to us how the
results vary across datasets constructed under different criteria.
In this paper, we select three representative works, csbd,

it

drebin, and mamadroid to revisit their malware detection
performance across different datasets we constructed.

III. STUDY DESIGN

A. Research Questions

In this paper, to unravel the confusion mentioned in Section

II, we seek to answer the following research questions:
RQ1 To what extent does VirusTotal threshold in malware la-
belling affect the malware evaluation results? VirusTotal
has been widely used in the research community to label
Android malware. As there are over 60 anti-virus engines
on VirusTotal, it is hard to deﬁne an accurate threshold
to label malware. As previous work has applied different
ad-hoc thresholds, it is interesting to investigate Whether
such thresholds impact malware evaluation results.
RQ2 How does the malware family distribution in the dataset
affect the detection result? As shown in Table I, the
malware family distribution in the widely used dataset
is highly imbalanced. Thus, it is unknown to us whether
the imbalance would introduce research bias during the
evaluation of malware detection approaches.

RQ3 How does the sample partition methods in evaluation af-
fect the detection results? No previous work has analyzed
the bias introduced by the so-called “randomly” parti-
tions in the evaluation (e.g., cross-validation) scenario.
We want to explore the impact introduced by partition
methods used in malware detection.

Fig. 1: The overview of our experimental setup.

B. Experimental Setup

As shown in Fig. 1, we prepare the malware corpus based on
Virustotal thresholds, family distribution, and sample partion
methods. In this section, we illustrate our experimental setup
in detail.

4

1) Malware Detection Approaches: To investigate the three
RQs, in our experiment, we apply three widely used machine-
learning based malware detection approaches (see Section
II-B), csbd, drebin, and mamadroid, as they are open-
sourced, widely used in our community, and proven to be
effective. We reused the implementation from github for
csbd3, drebin4 and mamadroid5, respectively.

2) Malware Corpus: To prepare our dataset for malware
detection, we resort to the well-known Androzoo dataset [37],
which contains over ten million Android apks from different
app markets including Google Play. Androzoo provides the
scanned result of each apk using VirusTotal. According to
these VT results, we are able to obtain 876,235 potential
malicious apps that are originated from Google Play with at
least one VT engine ﬂagging them as positive by the time of
this study. As previous work [38] suggested that the detection
results of VirusTotal would change with time, thus we further
obtain their latest VT reports. After fetching the latest VT
results, the number of malware apks is reduced to 690,544,
which means that almost 200K malware apks are not ﬂagged
as positive by any VT engines in the latest VT results. It further
suggests the dynamic of VT and the potential inaccurate results
in malware detection introduced by the unsteadiness of the
dataset.

Based on these latest VT reports, we further obtain the
family type and family name of each (potential) malicious
app with Euphony [39]. The 690,544 malicious apps, together
with the VT number, family name, and family type of each
malware, constitute the potential malware corpus in our exper-
iment. Note that, apps in this malware corpus are not necessary
to be malicious. We did not use all the samples as our ground-
truth to evaluate the malware detection techniques. Based on
the different criteria (e.g., VT threshold) and factors (e.g.,
malware family distribution) we considered, we will select
apps from the corpus to construct different testing datasets.

the benign dataset

3) Testing Datasets: To investigate the potential bias in-
troduced by malware datasets, we have curated a number
of experimental datasets under different conditions. We ﬁrst
prepare a benign dataset of 1,000 apps, with no VT engines
ﬂagging them as positive in Androzoo all the time. Note
that
is used repeatedly throughout our
experiments. For the malware dataset, we select 1,000 apps
from our malware corpus based on different criteria (the
selection process will be introduced in the following sections),
and then combine them with the 1,000 apps in the benign
dataset, forming a new experiment set of 2,000 apps. We
then perform our experiments on these apps with a fully
random 5-Fold cross-validation. Note that in each iteration, our
three malware detection approaches are applied respectively.
Therefore, after one group of the experiment is conducted,
we can obtain the accuracy, precision, and recall for the three
approaches. The only variable in each group of our experiment

3https://github.com/MLDroid/csbd
4https://github.com/MLDroid/drebin
5https://bitbucket.org/gianluca students/mamadroid code

is how we select and construct the 1,000 malicious apps. We
will elaborate on that in the following sections.

(a) Overall accuracy with different VT thresholds.

(b) Precision with different VT thresholds.

(c) Recall with different VT thresholds.

Fig. 2: Overall accuracy, precision, and recall of malware prediction
for each group of experiments in RQ1.

IV. THE IMPACT OF MALWARE LABELLING

A. Experimental Design

We ﬁrst investigate the impact of VT threshold on mal-
ware detection. To provide a comprehensive study, we have
constructed 23 sets of malware benchmarks, and each set
contains 1,000 (considered) malicious apps based on different
VT threshold. Note that, when the VT threshold is over 20,
the number of malware samples in our malware corpus is less
than 1,000 for a single VT number. To ensure the same size
of malware dataset, we extract the ones labeled as malware
by 21-25 VT engines as a dataset. It is the same when the
VT number is 26-30 and more than 31. The VT numbers of
the 23 groups dataset range from 1 to 20 in different groups,
with three additional groups of from 21 to 25, from 26 to 30
and more than 31. As aforementioned, we further apply csbd,
drebin, and mamadroid to these benchmarks.

5

B. Analysis

Intra-approach Analysis As shown in Figure 2(a), for each
approach, the malware detection accuracy grows logarithmicly
with the VT threshold increases from 1 to 6, and then the
accuracy of csbd and drebin remains relatively stable from
6 to 31+, the accuracy of mamadroid remains relatively
stable form 6 to 12. The difference between the highest and
the lowest accuracy reaches up to 11.25% for csbd, 21.5%
for drebin, and 13.33% for mamadroid under different
VT thresholds, which indicates that the criteria to construct
the dataset would lead to the great diversity of the detection
results.

This is reasonable since a higher VT number represents
an app having a higher possibility of being malicious, while
a lower VT number may jeopardize the reliability of the
ground truth in the experiment dataset, which in turn causes
the decrease of the overall accuracy.

Inter-approach Analysis In general, csbd has the high-
est accuracy, followed by drebin and then mamadroid.
However, it is interesting to see that, the comparison of the
three malware detection approaches on some metrics can show
some inconsistent results introduced by the VT threshold. For
example, as to the malware detection precision, the perfor-
mance of mamadroid is greater than that of drebin when
the threshold is lower than 12, while the result shows a quick
turnaround with a threshold higher than 12 (see Figure 2(b)).
Further, the performance gaps among these approaches are
highly sensitive to the threshold. For example, on the dataset
of V T = 21 − 25, drebin achieves 12.12% of accuracy
higher than mamadroid (90.75% VS. 78.63%), while they
achieve similar results on the dataset of V T = 10.

Brief Summary

The thresholds for malware labelling can directly affect the
performance of malware detection. For a single approach,
the impact could be signiﬁcant since the difference between
the worst case and the best case reach over 20%. Further,
the malware labelling threshold can lead to controversial
or inconsistent results when comparing different malware
detection approaches.

V. THE IMPACT OF DATASET COMPOSITION

A. Experiment Design

1) Research Questions: To explore the impact of malware
family types and composition on malware detection in com-
prehensive, we further divide RQ2 into the following sub-
questions.

RQ2.1 Does the number of malware families in a given dataset

affect the detection results?

RQ2.2 Assuming the number of malware families in a given
dataset is stable, does the composition of malware fami-
lies affect the malware detection results?

RQ2.3 Considering the uneven distribution of malware families
in the existing dataset (see Table I), does the emphasis
on certain families affect the overall detection results?

TABLE II: 10 Families Selected in RQ2

Family Name
artemis
generisk
genbl
hifrm
plankton

Percentage% Family Name

8.48%
3.23%
2.34%
1.71%
0.84%

fakeapp
scamapp
mobeleader
hamob
stopsms

Percentage%
0.60%
0.27%
0.26%
0.22%
0.22%

2) Target Malware Families: We randomly select 10 pop-
ular malware families from the malware corpus mentioned in
Section III to investigate the impact of the types and compo-
sitions of malware families on malware detection. Note that,
we did not consider adware related malware families in this
study. Table II shows the family name of the selected malware
and its proportion in our malware corpus. These malware
families have diverse malicious behaviors and different levels
of prevalence in the wild. Note that in the sets of experiments
in RQ2, all the selected malware samples were tagged by at
least ﬁve engines on VirusTotal to eliminate the potential bias
introduced by the labelling thresholds (see Section IV).

3) Dataset Construction: To answer RQ2.1 and RQ2.2,
we have curated a number of malware datasets with different
numbers of malware families. Each malware family is evenly
distributed in the dataset. The malware datasets include only
one family (10 datasets of 10 selected malware families),
two families (select 2 families from the 10 families, and
(cid:1)) groups of experiments), ﬁve families (select
perform 45 ((cid:0) 2
5 families from the 10 families, and perform 252 ((cid:0) 5
(cid:1)) groups
of experiments) and ten families (all 10 selected families).
Based on these 308 different malware datasets, we explore
the impact of the malware family distribution on malware
detection.

10

10

10

To investigate the uneven distribution of malware families
raised in RQ2.3, we further construct malware datasets that
are dominated by some malware families. To be speciﬁc, for
the selected 10 families, we randomly select two families
(cid:1)) groups of experiments) and regard them as the
(45 ((cid:0) 2
two major families, i.e., the two major families occupy 60%
of the malware samples in the dataset (each occupies 30%
of the malware samples), and the remaining eight families
occupy 40% of the dataset (each family occupies 5% of the
dataset). We explore whether the imbalance distribution affects
the results of malware detection based on these datasets.

B. RQ2.1 Does the number of malware families matter?

Figure 3 shows the distribution of the accuracy of the exper-
iment results for the different number of malware families in
the dataset. With the number of the malware families growing,
the average detection accuracy of each malware detection
method decreases. On average, malware detection works best
in the case when the dataset contains only one malware family.
The trends of the three approaches are similar. The difference
between accuracy caused by the number of malware families
in the dataset is 2.61%, 3.01%, and 7.49% respectively for the
three algorithms.

6

Fig. 3: The malware detection results for different number of malware families in a given dataset.

Brief Summary

On the condition of the number of samples in a malware
dataset is stable, the number of families matters. More
families, more diversity. In general, with the increasing of
the number of malware families, the accuracy of machine-
learning based classiﬁers would decrease. Different algo-
rithms have different sensitivities to the number of malware
families in a dataset.

Fig. 4: The accuracy of malware detection on different families.

C. RQ2.2 Does the composition of malware families matter?

RQ2.1 and RQ2.2 use the same dataset but do not focus on
the same issues. RQ2.1 focuses on the impact of the number
of malware in the dataset, while RQ2.2 pays more attention
on the composition of malware families in the dataset. We
consider the case that the number of families in the malware
dataset is 1, 2, and 5. We further investigate the differences in
their performances based on our constructed datasets.

1) Single Family: As shown in Figure 4, for the constructed
10 malware datasets (with each having one single-family),
the detection accuracy of the trained classiﬁers has noticeable
differences. The gap of the malware detection accuracy could
be up to 4.07%, 9.64%, and 18.9% respectively for csbd,
drebin, and mamadroid, respectively. We further perform

7

inter-approach analysis. All these three detection approaches
have their “favorable” families. For example, as shown in
Figure 4, it is apparently that drebin performs better than
mamadroid in most cases. Nevertheless, the malware detec-
tion accuracy of mamadroid is higher than drebin on the
scamapp dataset. Thus, for a malware dataset dominated by
a single family, the evaluation results might be quite bias.

2) Two Families: We next investigate the malware dataset
composed of two families. Figure 5 shows the overall results.
Deeper color represents higher accuracy. We can observe that
different combinations of families result in huge differences in
accuracy under the same malware detection method. The gap
of the malware detection accuracy could be over 20% even
for the same malware detection approach (4.15%, 9.95%, and
20.09% respectively between the highest and the lowest for
csbd, drebin, and mamadroid). We further notice that all
of these three approaches can achieve the best results under
different combinations. For the classiﬁers trained using csbd,
the one that relies on the dataset contains scamapp and
mobeleader families performs the best. As for drebin,
the dataset containing mobeleader and hamob families per-
forms the best. The dataset containing hamob and scamapp
families performs the best for mamadroid. It further indicates
that the evaluation of the malware detection approach is highly
relevant to the constructed dataset.

3) Five Families: We next apply the malware detection
approaches to datasets which are composed of ﬁve malware
families, with 252 groups of malware benchmarks in total.
Figure 6 shows the overall distribution of the results. It is
surprising to see that, for a single approach, the difference
between the best and the worst results could achieve 4.8%
for csbd, 8.1% for drebin, and 19.04% for mamadroid
(the best and the worst results are highlighted on Figure 6).
For example, Table III shows the top-5 classiﬁers with the
highest and the lowest accuracy for classiﬁers trained using
mamadroid. The malware detection performance of csbd
is relatively stable, and the variance of accuracy is 0.82%.
The variances of drebin and mamadroid are 2.58% and
8.92%, respectively.

We further perform inter-approach analysis. Csbd always
performs the best under the 252 groups of malware bench-
marks. In most cases, the accuracy of drebin is higher

Fig. 5: Experiment results on malware dataset composed of two families. Each cell represents the overall accuracy of the malware detection
approach on the dataset composed of the corresponding two families (x-axis and y-axis).

TABLE III: 5 sets of families with the highest/lowest accurary for mamadroid

Highest Five

Lowest Five

Accuracy%
98.19%
94.87%
94.22%
93.82%
93.57%

Families
fakeapp,hifrm,mobeleader,scamapp,hamob
hifrm,mobeleader,scamapp,hamob,stopsms
fakeapp,hifrm,scamapp,hamob,stopsms
fakeapp,hifrm,mobeleader,hamob,stopsms
plankton,fakeapp,mobeleader,scamapp,hamob

Accuracy%
79.15%
79.95%
80.31%
80.71%
80.91%

Families
plankton,generisk,fakeapp,artemis,genbl
plankton,generisk,artemis,genbl,hamob
plankton,generisk,artemis,genbl,stopsms
plankton,generisk,artemis,genbl,scamapp
plankton,generisk,hifrm,artemis,genbl

Brief Summary

Our ﬁndings highlight that, in the research of malware
detection, it is easy to ignore the dataset bias of family
composition, which leads to an excessive interpretation of
malware detection approaches. The composition of mal-
ware families can have great inﬂuences on the malware
detection performance when the number of malware fami-
lies in a given dataset is stable. The superiority of malware
detection approach is different under various combinations
of malware families. One can even manipulate the compo-
sition of the dataset to make one speciﬁc approach achieve
the best results.

D. RQ2.3 Does the imbalance of malware families matter?

As aforementioned, we construct malware datasets that are
dominated by two random malware families and perform 45
groups of experiments. The results are presented in Figure 7.
1) Intra-approach Analysis: As each dataset has two dom-
inating families, each cell in the Matrix indicates the corre-
sponding major families. For the considered three approaches,
the difference between the highest and the lowest accuracy is
3.3% (csbd), 5.15% (drebin) and 8.73% (mamadroid),
respectively.

For csbd, as shown in Figure 8, 32 of 45 combinations
are higher than the average distribution datasets. The minimum
accuracy difference is -1.25%, and the maximum is 2.05%. For
drebin, 18 of them are higher than the average distribution
datasets, and the minimum accuracy difference is -2.9%, and
the maximum is 2.25%. For mamadroid, 30 of them are

Fig. 6: Experiment results on malware dataset composed of ﬁve
families. Note that we have created 225 groups of datasets in total.

than that of mamadroid. However, although the accuracy
of mamadroid is the most unstable, mamadroid could
also perform better than drebin in some cases. For ex-
ample, on the dataset of the composition of generisk,
hifrm, artemis, mobeleader and genbl, drebin
achieves 8.25% of accuracy higher than mamadroid (82.87%
VS. 91.12%), while on the dataset of the composition of
fakeapp, hifrm, mobeleader, scamapp and hamob,
mamadroid achieves 2.69% of accuracy higher
than
drebin (98.19% VS. 95.5%)

8

Fig. 7: The impact of the imbalance of malware families.

higher than the average distribution datasets, and the minimum
accuracy difference is -3.96%, and the maximum is 4.77%.

2) Inter-approach Analysis: In general, csbd performs the
best, follewed by drebin and then mamadroid. Further,
the performance gaps among these approaches are highly
related to the dominating families. For example, when the two
dominating families of the dataset are hifrm and hamob,
drebin achieves 1.9% of accuracy higher than mamadroid
(90.7% VS. 88.8%), while drebin could achieve 13.3% of
accuracy higher than mamadroid (93.7% VS. 80.4%) when
the two dominating families of the dataset are generisk and
artemis.

Brief Summary

The emphasis on certain families can affect malware detec-
tion performance. The impact on the same malware detec-
tion differs when the malware families with a large share
in the dataset are different. Also, the effect of increasing or
decreasing the percentage of some certain malware families
in the dataset does not have the same effect with different
algorithms. Therefore, the imbalance of malware families
can be an evaluation bias of malware detection.

VI. THE IMPACT OF SAMPLE PARTITION

A. Experimental Design

We further design experiments to evaluate the impact of
the sample partition methods in cross-validation. We consider
the ideal case and the extreme case of the sample partition
methods in cross-validation to present the comparisons more
visually. In the ideal case, each malware family in the dataset
is evenly distributed, and the types and numbers of malware
families in the training and validation sets are identical. It
can ensure that the data distribution of the malware detection
methods in the training and testing set is completely consistent.
As comparison, one extreme case is that the veriﬁcation set
does not even contain the malware families in the training set.
We compare the ideal case with the extreme case using 5-Fold
cross validation. We re-use the same 252 groups of malware
benchmarks from RQ2.2. For the extreme case, the malware

dataset is partitioned into 5 sub-sets, and each sub-set contains
a single family, which means it does not have malware family
overlap with other sub-sets. Figure 9 shows the overall results
for the three approaches.

B. Analysis

Intra-approach Analysis If the sample partition methods
in cross validation is not randomly partitioned, all the three
methods could not ensure the malware detection performance.
The malware detection methods could not be fair evaluated if
the bias of sample partition exists. Figure 9 shows the overall
results. As expected, for all the three malware detection ap-
proaches, their performance decreased greatly on the extreme
case. The gap between the worst case and the ideal case could
be 37.29%, 31.16%, and 44.58% for csbd, drebin, and
mamadroid respectively.

Inter-approach Analysis The sample partition method has
a huge inﬂuence on the comparison of different malware
detection approaches. In the ideal case, csbd always performs
the best when comparing the accuracy of the three algorithms.
In the worst case, it is hard to tell which malware detection
approach performs the best. It is found that drebin performs
the best 179 times and csbd performs the best 73 times in
the worst case.

Brief Summary

Our exploration suggests that even using the same dataset,
data partition methods used in the cross-validation could
introduce substantial impact to the overall results. One can
even manipulate the training/testing data partition method
to make one speciﬁc approach perform the best. We argue
that there might be research bias when evaluating their
approaches.

VII. DISCUSSION

A. Implication

Our observations in this paper can provide practical impli-
cations. First, we observe that the effectiveness of malware
detection approaches is highly related to the dataset used in

9

Fig. 8: The accuracy difference with between imbalance distribution datasets and average distribution datasets.

bution is unlimited and other situations of dataset bias exist.
Nevertheless, we believe our design has covered most of the
situations of dataset bias. Second, we randomly select 10 fami-
lies to do experiments in this paper. However, various malware
families exist in the industry environment. We believe that
similar conclusions can be obtained by using other families.
Finally, due to the resource limitations, our research is mainly
based on three malware detection methods. We make efforts
to investigate the differences between the three methods to
form a general law of the dataset bias in Android malware
detection.

VIII. CONCLUSION
In this paper, we make the ﬁrst systematic study of the
dataset bias in Android malware detection. Speciﬁcally, we
focus on the variability of the malware dataset and the methods
to use the dataset. We make effort to curate a number of
malware benchmarks under different conditions and evaluate
their performance gap of malware detection using existing
techniques. The experiment results suggest that, using different
criteria to create and use the malware dataset can have a
misleading impact on evaluations. We argue that a machine
learning fairness framework for malware detection is needed
to highlight the pros and cons of each approach. Providing
reasonable, meaningful and explainable results is better than
only reporting a high detection accuracy with vague dataset
and experimental settings.

REFERENCES

[1] H. Wang, Z. Liu, J. Liang, N. Vallina-Rodriguez, Y. Guo, L. Li,
J. Tapiador, J. Cao, and G. Xu, “Beyond google play: A large-scale
comparative study of chinese android app markets,” in Proceedings of
the Internet Measurement Conference 2018, 2018, pp. 293–307.

[2] “New google android malware warning issued to 8 million play store
https://www.forbes.com/sites/kateoﬂahertyuk/2019/10/24/new-

users,”
google-android-malware-warning-issued-to-8-million-play-store-
users/#2915ff0a1235, 2019.
android

[3] “Joker

snowballs

malware

google

play,”

on

Fig. 9: The impact of sample partition methods.

the evaluation and the experiment results are inﬂated due to
research bias introduced by the variability of the malware
dataset and the methods to use the dataset. The results can
be inconsistent and even misleading, i.e., the superiority of
malware detection methods under different criteria is different.
In other words, evaluating whether a method is good enough
is closely relevant to the chosen dataset. This, however, has
never been mentioned and carefully explained in the previous
thousands of malware detection papers. We admit that different
malware detection approach may have their own advantages
and their usage scenarios, however, we argue that, a machine
learning fairness framework for malware detection is needed
to highlight the pros and cons of each approach. Providing
reasonable, meaningful and explainable results is better than
only reporting a high detection accuracy with vague dataset
and experimental settings.

B. Limitation

Our study carries several limitations. First, we have inves-
tigated three aspects of the dataset bias in Android malware
detection. Although it is still possible that the family distri-

https://threatpost.com/joker-androids-malware-ramps-volume/151785/,
2020.

[4] P. Faruki, V. Ganmoor, V. Laxmi, M. S. Gaur, and A. Bharmal,
“Androsimilar: robust statistical feature signature for android malware
detection,” in Proceedings of
the 6th International Conference on
Security of Information and Networks, 2013, pp. 152–159.

10

[24] “Avclass: A tool for massive malware labeling,” in International Sym-
posium on Research in Attacks, Intrusions, and Defenses, 2016.
[25] L. Li, D. Li, T. F. Bissyand´e, J. Klein, Y. Le Traon, D. Lo, and
L. Cavallaro, “Understanding android app piggybacking: A systematic
study of malicious code grafting,” IEEE Transactions on Information
Forensics and Security, vol. 12, no. 6, pp. 1269–1284, 2017.

[26] F. Shen, J. Del Vecchio, A. Mohaisen, S. Y. Ko, and L. Ziarek, “Android
malware detection using complex-ﬂows,” IEEE Transactions on Mobile
Computing, vol. 18, no. 6, pp. 1231–1245, 2018.

[27] S. Y. Yerima and S. Sezer, “Droidfusion: A novel multilevel classiﬁer
fusion approach for android malware detection,” IEEE transactions on
cybernetics, vol. 49, no. 2, pp. 453–466, 2018.

[28] T. Kim, B. Kang, M. Rho, S. Sezer, and E. G. Im, “A multimodal deep
learning method for android malware detection using various features,”
IEEE Transactions on Information Forensics and Security, vol. 14, no. 3,
pp. 773–788, 2018.

[29] D. Maiorca, D. Ariu, I. Corona, M. Aresu, and G. Giacinto, “Stealth
attacks: An extended insight into the obfuscation effects on android
malware,” Computers & Security, vol. 51, pp. 16–31, 2015.

[30] A. H. Lashkari, A. F. A. Kadir, H. Gonzalez, K. F. Mbah, and A. A.
Ghorbani, “Towards a network-based framework for android malware
detection and characterization,” in 2017 15th Annual conference on
privacy, security and trust (PST).

IEEE, 2017, pp. 233–23 309.

[31] F. Wei, Y. Li, S. Roy, X. Ou, and W. Zhou, “Deep ground truth analysis
of current android malware,” in International Conference on Detection
of Intrusions and Malware, and Vulnerability Assessment.
Springer,
2017, pp. 252–276.

[32] A. Guerra-Manzanares, H. Bahsi, and S. N˜omm, “Kronodroid: Time-
based hybrid-featured dataset for effective android malware detection
and characterization,” Computers & Security, vol. 110, p. 102399, 2021.
[33] K. Allix, T. F. Bissyand´e, Q. J´erome, J. Klein, Y. Le Traon et al.,
“Empirical assessment of machine learning-based malware detectors for
android,” Empirical Software Engineering, vol. 21, no. 1, pp. 183–211,
2016.

[34] V. Avdiienko, K. Kuznetsov, A. Gorla, A. Zeller, S. Arzt, S. Rasthofer,
and E. Bodden, “Mining apps for abnormal usage of sensitive data,”
in 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, vol. 1.
IEEE, 2015, pp. 426–436.

[35] Z. Yuan, Y. Lu, and Y. Xue, “Droiddetector: android malware char-
acterization and detection using deep learning,” Tsinghua Science and
Technology, vol. 21, no. 1, pp. 114–123, 2016.

[36] S. Hou, A. Saas, L. Chen, and Y. Ye, “Deep4maldroid: A deep learning
framework for android malware detection based on linux kernel system
call graphs,” in 2016 IEEE/WIC/ACM International Conference on Web
Intelligence Workshops (WIW).

IEEE, 2016, pp. 104–111.

[37] L. Li, J. Gao, M. Hurier, P. Kong, T. F. Bissyand´e, A. Bartel, J. Klein, and
Y. Le Traon, “Androzoo++: Collecting millions of android apps and their
metadata for the research community,” arXiv preprint arXiv:1709.05281,
2017.

[38] S. Zhu, J. Shi, L. Yang, B. Qin, Z. Zhang, L. Song, and G. Wang,
“Measuring and modeling the label dynamics of online anti-malware
engines,” in Proceedings of the 29th USENIX Conference on Security
Symposium. USA: USENIX Association, 2020.

[39] M. Hurier, G. Suarez-Tangil, S. K. Dash, T. F. Bissyand´e, Y. Le Traon,
J. Klein, and L. Cavallaro, “Euphony: Harmonious uniﬁcation of ca-
cophonous anti-virus vendor labels for android malware,” in 2017
IEEE/ACM 14th International Conference on Mining Software Reposi-
tories (MSR).

IEEE, 2017, pp. 425–435.

[5] M. Zheng, M. Sun, and J. C. Lui, “Droid analytics: a signature based
analytic system to collect, extract, analyze and associate android mal-
ware,” in 2013 12th IEEE International Conference on Trust, Security
and Privacy in Computing and Communications.
IEEE, 2013, pp. 163–
171.

[6] M. Zhang, Y. Duan, H. Yin, and Z. Zhao, “Semantics-aware android mal-
ware classiﬁcation using weighted contextual api dependency graphs,”
in Proceedings of the 2014 ACM SIGSAC conference on computer and
communications security, 2014, pp. 1105–1116.

[7] Y. Feng, S. Anand, I. Dillig, and A. Aiken, “Apposcopy: Semantics-
based detection of android malware through static analysis,” in Pro-
ceedings of
the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering, 2014, pp. 576–587.

[8] I. Burguera, U. Zurutuza, and S. Nadjm-Tehrani, “Crowdroid: behavior-
based malware detection system for android,” in Proceedings of the 1st
ACM workshop on Security and privacy in smartphones and mobile
devices, 2011, pp. 15–26.

[9] A. Saracino, D. Sgandurra, G. Dini, and F. Martinelli, “Madam: Effective
and efﬁcient behavior-based android malware detection and prevention,”
IEEE Transactions on Dependable and Secure Computing, vol. 15, no. 1,
pp. 83–97, 2016.

[10] S. K. Dash, G. Suarez-Tangil, S. Khan, K. Tam, M. Ahmadi, J. Kinder,
and L. Cavallaro, “Droidscribe: Classifying android malware based
on runtime behavior,” in 2016 IEEE Security and Privacy Workshops
(SPW).

IEEE, 2016, pp. 252–261.

[11] E. Mariconti, L. Onwuzurike, P. Andriotis, E. De Cristofaro, G. Ross,
and G. Stringhini, “Mamadroid: Detecting android malware by building
markov chains of behavioral models,” arXiv preprint arXiv:1612.04433,
2016.

[12] S. Chen, M. Xue, Z. Tang, L. Xu, and H. Zhu, “Stormdroid: A
streaminglized machine learning-based system for detecting android
malware,” in Proceedings of the 11th ACM on Asia Conference on
Computer and Communications Security, 2016, pp. 377–388.

[13] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, K. Rieck, and
C. Siemens, “Drebin: Effective and explainable detection of android
malware in your pocket.” in Ndss, vol. 14, 2014, pp. 23–26.

[14] N. McLaughlin, J. Martinez del Rincon, B. Kang, S. Yerima, P. Miller,
S. Sezer, Y. Safaei, E. Trickel, Z. Zhao, A. Doup´e et al., “Deep android
malware detection,” in Proceedings of the Seventh ACM on Conference
on Data and Application Security and Privacy, 2017, pp. 301–308.
[15] Y. Aafer, W. Du, and H. Yin, “Droidapiminer: Mining api-level features
for robust malware detection in android,” in International conference on
security and privacy in communication systems.
Springer, 2013, pp.
86–103.

[16] K. Xu, Y. Li, and R. H. Deng, “Iccdetector: Icc-based malware detection
on android,” IEEE Transactions on Information Forensics and Security,
vol. 11, no. 6, pp. 1252–1264, 2016.

[17] G. Dini, F. Martinelli, A. Saracino, and D. Sgandurra, “Madam: a
multi-level anomaly detector for android malware,” in International
Conference on Mathematical Methods, Models, and Architectures for
Computer Network Security. Springer, 2012, pp. 240–253.

[18] Y. Zhou and X. Jiang, “Dissecting android malware: Characterization
and evolution,” in 2012 IEEE symposium on security and privacy. IEEE,
2012, pp. 95–109.

[19] F. Wei, Y. Li, S. Roy, X. Ou, and W. Zhou, “Deep ground truth analysis
of current android malware,” in International Conference on Detection
of Intrusions and Malware, and Vulnerability Assessment.
Springer,
2017, pp. 252–276.

[20] H. Wang, J. Si, H. Li, and Y. Guo, “Rmvdroid: towards a reliable android
malware dataset with app metadata,” in 2019 IEEE/ACM 16th Interna-
tional Conference on Mining Software Repositories (MSR). IEEE, 2019,
pp. 404–408.

[21] M. Fan, J. Liu, X. Luo, K. Chen, Z. Tian, Q. Zheng, and T. Liu, “Android
malware familial classiﬁcation and representative sample selection via
frequent subgraph analysis,” IEEE Transactions on Information Foren-
sics and Security, vol. 13, no. 8, pp. 1890–1905, 2018.

[22] F. Pendlebury, F. Pierazzi, R. Jordaney, J. Kinder, and L. Cavallaro,
“{TESSERACT}: Eliminating experimental bias in malware classiﬁca-
tion across space and time,” in 28th {USENIX} Security Symposium
({USENIX} Security 19), 2019, pp. 729–746.

[23] G. Suarez-Tangil and G. Stringhini, “Eight years of rider measurement
in the android malware ecosystem: evolution and lessons learned,” arXiv
preprint arXiv:1801.08115, 2018.

11

