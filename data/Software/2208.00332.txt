This is Authors version of the paper under review at a  journal. 

GitHub  Marketplace  for  Practitioners  and  Researchers  to  Date 

A Systematic Analysis of the Knowledge Mobilization Gap in Open 
Source Software Automation 

Sk  Golam  Saroar  ·  Waseefa  Ahmed  ·  Maleknaz 
Nayebi 

Received:  date  /  Accepted:  date 

Abstract  Marketplaces  for  distributing  software  products  and  services  have  been  getting  in- 
creasing popularity. GitHub, which is most known for its version control functionality through 
Git,  launched  its  own  marketplace  in  2017.  GitHub  Marketplace  hosts  third  party  apps  and 
actions to automate workflows in software teams. Currently, this marketplace hosts 440 Apps 
and 7,878 Actions across 32 different categories. Overall, 419 Third party developers released 
their apps on this platform which 111 distinct customers adopted. The popularity and accessi- 
bility of GitHub projects have made this platform and the projects hosted on it one of the most 
frequent  subjects  for  experimentation  in  the  software  engineering  research.  A  simple  Google 
Scholar search shows that 24,100 Research papers have discussed GitHub within the Software 
Engineering field since 2017, but none have looked into the marketplace. The GitHub Market- 
place provides a unique source of information on the tools used by the practitioners in the Open 
Source Software (OSS) ecosystem for automating their project’s workflow. In this study, we (i) 
mine and provide a descriptive overview of the GitHub Marketplace, (ii) perform a systematic 
mapping of research studies in automation for open source software, and (iii) compare the state 
of  the  art  with  the  state  of  the  practice  on  the  automation  tools.  We  conclude  the  paper  by 
discussing the potential of GitHub Marketplace for knowledge mobilization and collaboration 
within the field. This is the first study on the GitHub Marketplace in the field. 

Keywords  Software  Engineering,  GitHub  Marketplace,  Open  Source,  Software  Automation, 
Systematic Mapping Study, Knowledge Mobilization 

Sk Golam Saroar 
York University 
E-mail:  saroar@yorku.ca

Waseefa Ahmed 
York  University 
E-mail:  waseefa@yorku.ca

Maleknaz Nayebi 
York University 
E-mail:  mnayebi@yorku.ca

2 

1 Introduction 

Sk Golam Saroar et al. 

GitHub Marketplace provides software teams with tools and technologies to improve their devel- 
opment workflows. Third-party developers provide these automation tools on the marketplace 
to reduce the number of repetitive tasks within a workflow and increase the productivity of a 
software team. The software engineering research community has also been concerned with the 
automation of development workflows and providing decision support to assist software teams 
in improving teams’ productivity and products value. Yet, the GitHub Marketplace has never 
been  the  subject  of  an  investigation  to  evaluate  the  alignment  or  difference  between  research 
efforts (the published studies) and the practice (GitHub automation tools). 

Software Marketplaces have been studied since 2012 from the engineering perspective. Har- 
man et al. [16] provided an insight into the importance of marketplace ecosystem for software 
engineering  practices  by  investigating  the  mobile  app  stores.  Martin  et al.  [29] synthesize  the 
research and development directions in mobile app stores. Mobile app stores serve not only as 
huge collections of apps but also enable developers to produce and distribute content while es- 
tablishing a communication channel between users and developers via ratings and reviews [29]. 
The results of their synthesis in 2017 showed that the research in mobile app stores covers the 
topics of extracting requirements and analyzing user feedback, release and distribution of apps, 
security  and  vulnerabilities,  and  API  analysis,  as  well  as  the  discussion  about  the  ecosystem 
and the marketplace. Similar to mobile app stores, apps and actions on the GitHub Marketplace 
also have technical and non-technical attributes. 

GitHub Marketplace is a software repository that allows developers to provide free or paid 
tools to GitHub users for automating their workflows1, in particular, when hosted on a GitHub 
repository. Apps and Actions are the two types of tools in this marketplace. While both provide 
ways to build automation and workflow tools, they each have strengths2 that make them useful 
in different ways. GitHub actions are designed for automation of customized individual tasks 
within a workflow 3 including the automation for CI pipelines, building, testing or deploying 
software applications. They are written in 
container or 
snippet. Github actions are free for users. GitHub  apps  are applications that 
a 
GitHub users can install on user or organization accounts. Apps can access certain repositories 
when explicitly specified and allowed by the user4. Apps can be free or paid, and often each 
app has multiple paywalls depending on its functionality. GitHub apps are indeed actions that 
are hosted and run by GitHub. It is recommended that users and organizations use apps if the 
desired action needs extended permission, or if the user wishes to manage and run the action by 
themselves, or when users need further architecture and memory support for analyzing data. 

files and can be built as a 

Javascript 

Docker 

YAML 

Considering the widespread popularity of GitHub in regards to hosting software repositories 
and managing product releases and the number of tools for workflow automation, we believe 
that the provided set of tools is highly relevant to real-world software engineering challenges. 
Hence, the tools are a unique source of data and provide the opportunity to observe practices 
and understand pain points and trends in software automation. We compared research studies 
published  in  the  software  engineering  community  and  the  automation  tools  devised  by  the 
researchers,  and  focused  on  studying  the  gap  between  research  and  practice  in  open  source 
software tools. 

1

2

https://docs.GitHub.com/en/developers/GitHub-marketplace/GitHub-marketplace-overview/ 

about-GitHub-marketplace 

3

https://docs.github.com/en/actions/creating-actions/about-custom-actions# 

strengths-of-github-actions-and-github-apps 

4

https://docs.GitHub.com/en/actions/creating-actions/about-actions 
https://docs.GitHub.com/en/developers/apps/getting-started-with-apps/about-apps 

 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

3 

To the best of our knowledge, this study is the first on the analysis and mining of GitHub 
marketplace. Our study consists of three main parts. First, we performed an exploratory analysis 
to understand and evaluate the GitHub Marketplace by mining and synthesizing the technical 
and non-technical attributes of apps and actions in this ecosystem. Second, we systematically 
gathered  literature  on  automation  tools  and  techniques.  Finally,  we  mapped  state  of  the  art 
to the state of practice, compared the existing research direction with the automation tools in 
the GitHub Marketplace, and performed a gap analysis. In particular, we are answering three 
research questions (RQs): 

RQ1  (Descriptive)  -  What  are  the  characteristics  of  the  GitHub  Marketplace  and  what  tools 

are offered for automating development workflows? 
Why  and  How:  To  provide  a  better  understanding  of  the  attributes  and  data  structure 
within this repository, we mined the characteristics of apps and actions in the GitHub Mar- 
ketplace  and  explained  the  status  quo  using  all  the  available  fields  in  the  repository  and 
performed a comparative study wherever appropriate. 

RQ2  (Descriptive)  -  How  have  the  automation  tools  for  open  source  software  evolved  in  the 

research community over the years? 
Why  and  How:  We  systematically  gathered  literature  on  automation  tools  and  techniques 
within the open source ecosystem and compared the papers over the years. Further, we mapped 
the topic of these academic studies into the marketplace tool categories to enable comparison 
between research and current practice. 

RQ3  (Diagnostic)  -  How  do  the  automation  tools  in  the  marketplace  compare  with  software 

engineering literature and what are the gaps? 
Why  and  How: We compared the state of the art with the state of the practice. To this end, 
we first gathered apps and actions information and extracted a list of categories and frequency 
of coverage from the GitHub Marketplace. We then performed a systematic literature analysis 
and mapped the papers retrieved from our systematic search into the marketplace categories 
and discussed the gap and trends. 

The rest of this paper is organized as follows. Section 2 presents the related work. Section 3 
describes the research methodology, including data extraction, synthesis, and analysis. Section 4 
presents  the  observations  from  mining  GitHub  Marketplace.  Section  5  shows  results  of  the 
systematic mapping study. Section 6 provides the gap analysis. Section 7 discusses the threats 
to validity. Section 8 provides discussion and future implications. Section 9 concludes this paper. 

2 Related Work 

So far, the analysis of marketplaces in software engineering has been limited to mobile apps store 
analysis. However, marketplaces, and in particular, the GitHub Marketplace has been studied 
in fields other than the software engineering community from the economical and management 
points of view. 

2.1  GitHub Marketplace 

Kallis et al.  [19, 20] proposed a GitHub app called Issue Tagger  which  uses machine learning 
to  analyze  the  title  and  textual  description  of  issues  and  automatically  assign  labels  such  as 

 
 
 
 
 
 
 
 
4 

Sk Golam Saroar et al. 

bug report, feature request, and question. Experimenting on about 30,000 GitHub issues, they 
showed that the Ticket Tagger could identify the correct labels with reasonably high effective- 
ness.  They  also  presented  the  tool’s  architecture  and  provided  all  the  necessary  information 
for  installing  and  using  the  app  on  custom  GitHub  repositories.  To  improve  the  quality  of 
software,  Souza  et  al.  [44]  proposed  to  analyze  code  inspection  tools  available  on  the  Github 
Marketplace. They selected four tools for the 
system and found more than ten thousand failures. Subsequently, they analyzed the individual 
GLPI 
PHP 
feedback for that tool. Kinsman et al. [23] examined how actions were used by developers and 
how various activity metrics changed as a result of their adoption. They showed that the use of 
GitHub actions raised the monthly total of rejected pull requests while lowering the monthly 
total  of  commits  on  merged  pull  requests.  They  also  found  that  developers  had  a  favourable 
opinion of GitHub actions although the technology had only been used by a small percentage 
of projects thus far. Their findings are especially important for practitioners to understand and 
prevent undesirable effects on their projects. 

programming language to inspect the 

There are also multi-sided marketplaces such as Uber, where the platforms have customers 
on  both  the  demand  side  and  the  supply  side.  Mehrotra  et  al.  [30]  presented  some  research 
problems in developing a recommendation framework for such marketplaces, for example, intro- 
ducing a multi-objective ranking that optimizes the different objectives for multi-stakeholders. 

2.2  Software Bots 

Many projects use bots to automate a wide range of tasks such as refactoring source code [51], 
fixing bugs [33], updating project dependencies [31], supporting communication and decision- 
making  [45],  etc.  Wessel  et.  al  investigated  the  usage  and  impact  of  such  bots  in  [48].  They 
classified  bots  and  collected  metrics  from  before  and  after  bot  adoption  in  93  projects  on 
GitHub.  Although  the  surveyed  developers  believed  that  bots  were  useful  for  maintenance 
tasks,  the  authors  found  no  consistent  and  statistically  significant  difference  between  before 
and  after  bot  adoption  in  terms  of  number  of  comments,  commits,  changed  files,  and  time 
to close pull requests. They also asked developers about their desired improvements for bots. 
Developers wanted smarter bots that would decrease code review effort, decrease time to close 
pull-requests,  automate  continuous  integration  tasks,  among  others.  Many  of  the  apps  and 
actions in the GitHub Marketplace today automate these tasks. ‘Continuous integration’ (
), 
and ‘Code review’ (
) are among the most prevalent topics for actions in the Marketplace (see 
Table 8). 

#1

To assist students in checking code style and functioning, Hu et al. [17] built up a static code 
analyzer and a continuous integration service on GitHub. The authors implemented three bots 
and  showed  that  more  than  70%  of  students  believed  the  bots’  advise  was  valuable  and  that 
they could deliver much more feedback than teaching professionals. In another research, Wessel 
et al. [49] addressed some of the problems with existing GitHub bots, for example, the human- 
bot interaction on the pull-requests can be disruptive, spam-like, and eventually be ignored by 
the  contributors.  In  their  paper  [49],  they  envisioned  the  concept  of  a  meta-bot  which  would 
act as a middleman between the contributors and the existing bots. Instead of handling specific 
tasks  on  pull  requests,  the  meta-bot  would  provide  a  centralized  control  by  integrating  and 
orchestrating the task-oriented bots, and thus adding more value to the interaction of already 
existing bots. 

#6

 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

5 

2.3  Analyzing Marketplaces in Software Engineering 

In recent years, app marketplaces have drawn a lot of attention among researchers within the 
software  engineering  community.  In  [16],  Harman  et  al.  pointed  out  that  app  marketplaces 
were different from traditional ones (for example, they do not have source code), and provided 
a framework for app marketplace analysis. Their framework consisted of extracting and refining 
data, extracting feature information from textual descriptions, as well as computing metrics for 
analysis such as correlation analysis. They also analyzed the technical, customer, and business 
aspects  of  some  apps  in  the  BlackBerry  app  store.  Chen  et  al.  [8]  proposed 
,  a  novel 
framework for app review mining, in order to extract valuable information from user reviews 
with  minimal  human  efforts.  After  filtering  out  non-informative  reviews  using  a  pre-trained 
classifier, they employed topic modeling and a ranking scheme to automatically prioritize the 
informative reviews. 

ARMiner

Al-Subaihin  et  al.  [1]  studied  the  app  store  from  the  developers’  perspective.  Through 
developer interviews and surveys, they tried to better understand developers’ practices when 
making  apps  in  order  to  determine  the  extent  to  which  information  from  app  stores  affected 
developers’ decision-making and to highlight open issues and challenges. They pointed out that 
researchers and practitioners from several software engineering sub-fields such as requirements, 
testing, software repository mining, etc could benefit from the findings of the survey. Nayebi et 
al. [36] showed that considering only app stores for review mining lacked a significant amount 
of feedback, rather multiple sources must be considered to get more critical and objective views 
of  apps.  For  example,  in  their  study,  they  found  strong  correlations  between  the  number  of 
reviews  and  the  number  of  tweets  for  most  apps.  Maalej  et  al.  [28]  discussed  the  importance 
of user feedback and its relation with the software requirements and features. These feedback 
and the user experiences could inspire test cases, alternate flows, or even new features. Krüger 
et al. [24] described a preliminary analysis of the Eclipse marketplace with a view to providing 
a  glance  on  open  marketplaces  as  well  as  initiating  further  research.  They  proposed  to  mine 
marketplace data to address questions such as, who contributes to successful plug-ins, in order 
to  identify  leading  developers  and  communities,  leading  to  collaborations  and  new  research 
directions. Li et al. [26] performed a performance and usability evaluation of five open-source 
IDE  plugins  that  identify  and  report  security  vulnerabilities.  They  investigated  the  types  of 
vulnerabilities these plugins could detect, quality of detection, and user-friendliness of plugin 
outputs. They found a mismatch between the claimed and actual coverage of the plugins, while 
most plugins had a high false-positive rate, and were not developer-friendly. 

2.4  Mobile App Stores 

Mojica et al. [32] presented a study where they examined whether an app rating accurately re- 
flects how the users perceive it. Their findings indicated that the app store’s current metric fails 
to capture the users’ changing levels of satisfaction as the app evolves. Ali et al. [3] conducted a 
large-scale comparative study of cross-platform apps collected from the Apple and Google Play 
app  stores.  The  goal  was  to understand  the  differences in  how  users  perceived the same  app 
distributed through different platforms. Comparing app-store attributes, such as stars, versions, 
and prices, and measuring the aggregated user-perceived ratings, they found many differences 
across the platforms. Nayebi et al. [34, 39] conducted surveys with users and developers to un- 
derstand  the  release  strategies  used  for  mobile  apps  and  found  that  an  app’s  strategy  affects 
its success and how it is perceived by the users. Carreño et al. [7] extracted user requirements 
from comments on Google Play using a sentiment-aware topic model [18]. Their method aided 

 
 
 
 
 
 
6 

Sk Golam Saroar et al. 

requirements summarisation with significantly less effort than manual identification. Khalid et 
al.  [21]  studied  the  user’s  perspective  of  iOS  apps  by  analyzing  app  reviews.  They  found  12 
types of user complaints including functional errors, requests for additional features, and app 
crashes.  They  also  highlighted  the  importance  of  regression  testing  before  updating  apps  by 
showing that almost 11% of the studied complaints were reported after a recent update. 

3 Research  Methodology 

Our  research  methodology  consists  of  three  major  parts.  First,  for  RQ1,  we  used  empirical 
protocols  to  retrieve  information  from  the  GitHub  Marketplace  as  a  software  repository  and 
explored  and  derived  observations.  This  part  mainly  consists  of  statistical  methods  for  de- 
scriptive analytics. Second, for RQ2, we employed empirical protocols for systematic mapping 
studies and performed a comprehensive literature analysis to extract the spectrum of software 
engineering research for automation tools in open source. Finally, for RQ3, we compared the 
state  of  the  art  with  the  state  of  the  practice  by  using  our  findings  from  RQ1  and  RQ2  and 
discussing the gaps and trends. In Section 3.1 we discuss our data retrieval and analysis method 
from the marketplace. Next, we discuss the mapping protocol in Section 3.2. The gap analysis 
is presented in section 3.3. 

3.1  Process of Gathering Data from the Marketplace 

To gather apps and actions from the GitHub Marketplace, we used 
5 (for automated 
6  (as  the  main  web  page  crawler). 
loading  of  the  JavaScript  components)  along  with 
Each  web  page  loads  only  1,000  Apps  or  actions.  While  this  had  no  implication  for  the  apps 
data (since there are less than a 1,000 Apps in the marketplace), this restriction meant that we 
could extract only a fraction of the actions data from a static page. 

Selenium

Scrapy

To  avert  this  issue,  we  designed  a  set  of  knitted  crawlers  that  not  only  started  from  the 
marketplace’s  main  page  but  also  took  each  action  category  as  a  scrapping  root.  For  most 
categories,  there  were  less  than  1,000  Actions  on  the  marketplace,  thus  we  did  not  face  any 
issue in gathering all the information. However, for four categories, there were more than 1,000 
Actions.  As  a  result,  we  used  the  embedded  marketplace  data  sorting  options-  ‘Best  Match’, 
‘Recently added’, and ‘Most installed/starred’ (see Figure 1). Each sorting method displayed a 
different set of data, allowing us to store additional actions not previously found. 

Apps and actions detail pages differ from each other. Also, actions have a free outline and 
style  using  GitHub  tools  which  each  developer  can  design.  To  ensure  that  we  are  gathering 
header 
all the existing metadata for apps and actions, we parsed through the different 
layers for apps and actions. As developers provide different levels of detail about each action, 
we created a unified list of all the data entries provided across different actions on GitHub. 

HTML 

We  could  uniformly  gather  11  Action  attributes  and  13  App  attributes  across  the  market- 
place.  Figure  2  includes  details  of  these  attributes.  Some  of  the  apps  and  actions  had  invalid 
URLs to their detail page, resulting in missing details for those items. However, we kept record 
of such items in our data collection process. Figure 4 shows the categories for apps and actions. 
The results of this analysis is presented in Section 4. 

5

6
https://www.selenium.dev/ 
https://scrapy.org/ 

 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

7 

Verified 

Fig.  1  Screenshot  of  the  GitHub  marketplace  with  the  meta  data  fields  (Types,  Categories,  Filters, 
Verification status, Number of Installs, Number of Stars, and Developers’ name) annotated. 

3.2  Protocols Used for Systematic Mapping Study 

To  answer  RQ2,  we  conducted  a  systematic  mapping  study  with  a  view  to  comparing  cur- 
rent trends in the open source research community with the ones in practice. To perform this 
systematic  study,  we  followed  the  guidelines  of  Peterson  et  al.  [40].  In  its  nature,  systematic 
mapping is aimed at performing a general analysis of the topics and trends in the field, which 
perfectly  serves  the  purpose  of  comparison  between  state  of  the  art  and  state  of  practice  we 
formulate  in  the  RQ3.  An  overview  of  the  process  is  shown  in  Figure  3.  Our  initial  search 
resulted in 11,365 Research papers. By applying our exclusion criteria, we narrowed it down to 
365 Papers, which we used in our final mapping. Next, we detail the process of paper selection 
along with the inclusion and exclusion process. 

3.2.1  Source Selection 

Our objective was to map the state of the art software engineering to the current categories of 
apps  and  actions  in  GitHub Marketplace. As  such,  we  chose  broad search  terms  to  gather  all 
the papers focusing on open source software engineering. Our search query was as following: 

(“Software Engineering” OR “SE”) AND (“GitHub” OR “OSS” OR “open source” OR “open- 

source.”) 

 
 
 
 
 
 
 
 
 
8 

Sk Golam Saroar et al. 

Name 

Developer 

Number of installs 

Short description 

GitHub Verification 

Number of stars 

Full description 

Recommended 

Contributors 

Categories 

Supported languages 

Developer Links 

Verified domains 

Pricing 

Releases 

Developer  Attributes 

Platform  Attributes 

User Attributes 

Fig.  2  Attributes  on  the  GitHub  Marketplace  for  apps  (gray),  actions  (green),  or  both  (white).  Each 
attribute is either defined by the developer, platform, or user. 

Following previous studies, we gathered papers from five publication sources, namely ACM 
Digital  Library,  IEEE  Xplore,  ScienceDirect,  Scopus,  and  Inspec.  Table  1  shows  the  initial 
number of papers retrieved from each of these sources based on our search query. To ensure 
search quality, we applied two broad Inclusion criteria while performing search in these libraries: 
Inclusion Criteria I: Studies must have been published between 2000 and 2021, 
Inclusion Criteria II: Studies must have been published in journals or conference proceedings. 
Overall, our initial search resulted in 11,365 Publications. 

3.2.2  Exclusion  Criteria 

To ensure quality of the publications, we used a set of heuristic exclusion criteria and narrowed 
down the search. We excluded the below papers: 
Exclusion Criteria I: Studies not presented in English, 
Exclusion Criteria II: Duplicate studies, 
Exclusion Criteria III:  Studies  not  presented  in  the  top  20  Publication  venues  according  to 

Table  1  The  initial  number  of  studies  retrieved  from  five  libraries  using  our  search  query  with  the 
applied inclusion criteria. 

  Publication  source  Num.  of  Studies   
ACM Digital Library 
IEEE  Xplore 
ScienceDirect 
Scopus 
Inspec 

8,817 
399 
1,154 
354 
619 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

9 

Step 1 

Step 2 

Step 3 

Step 4 

Searching 
databases 
with inclusion 
criteria 

Exclusion 
based on 
duplicated 
studies 

Exclusion 
based on top 20 
software engi- 
neering venues 

Exclusion 
based on title, 
abstract, 
keywords 

11,365 Papers 

10,927 Papers 

2,115 Papers 

365 Papers 

Fig. 3  The process of our systematic mapping with the number of retrieved papers at each step. 

Google Scholar’s ranking7. The list of the venues we used as for 2021 is provided in Table 2, 
Exclusion  Criteria  IV:  Studies  that  do  not  have  software  and  its  variations  in  title,  abstract, 
keywords and automation  and its variations in title, abstract, and keywords. 

We  retrieved  a  total  of  365  Papers  after  applying  the  exclusion  criteria.  Majority  of  the 
papers (10.96%) were published in 2019 followed by 10.32% published in 2017. The details of 
the number of publications are provided in Table 2. 

Table  2  Selected  publication  venues  from  Google  Scholar  as  used  for  exclusion  Criteria  III  and  the 
final number of papers studied from each library. 

Rank  Publication 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 

ACM/IEEE Inter. Conference on Software Engineering 
Journal of Systems and Software 
IEEE Transactions on Software Engineering 
Information  and  Software  Technology 
ACM SIGSOFT Inter. Symposium on Foundations of Software Engineering 
Empirical Software Engineering 
IEEE  Software 
ACM  SIGPLAN  Conference  on  Programming  Language  Design  and  Implementation  (PLDI) 
Mining  Software  Repositories  (MSR) 
IEEE/ACM  Inter.  Conference  on  Automated  Software  Engineering  (ASE) 
ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL) 
Inter.  Conference  on  Software  Analysis,  Evolution,  and  Reengineering  (SANER) 
Proceedings  of  the  ACM  on  Programming  Languages 
Software  &  Systems  Modeling 
Inter.  Symposium  on  Software  Testing  and  Analysis 
IEEE Inter. Conference on Software Maintenance and Evolution 
Software: Practice and Experience 
Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS) 
IEEE Inter. Requirements Engineering Conference 
ACM  SIGPLAN  Symposium  on  Principles  &  Practice  of  Parallel  Programming  (PPOPP) 

0 
#Papers 
106 
101 
111 
0 
28 
12 
0 
0 
0 
0 
0 
0 
3 
0 
0 
4 
0 
0 
0 

3.2.3  Data  Extraction  and  Synthesis 

For the classification scheme, we used the GitHub Marketplace categories. In the marketplace, 
the  product  owner  defined  one  or  more  categories  for  each  app  and  action.  Categories  are 
a  common  practice  in  marketplaces  that  offer  end  users  a  convenient  way  to  find  products 

7

https://scholar.google.ca/citations?view_op=top_venues&hl=en&vq=eng_softwaresystems 

 
 
 
 
 
 
 
 
 
 
 
 
10 

Sk Golam Saroar et al. 

  and services and possibly compare and choose among different alternatives. GitHub apps and 

actions can be classified in one or more categories. 

We  followed  a  protocol  for  the  manual  categorization  of  the  papers.  First  two  co-authors 
of  the  paper  each  carefully  reviewed  365  Paper  abstracts  and  categorized  them  up  to  three 
categories.  The  classification  results  were  compared,  and  a  total  of  79  Disagreements  were 
found  in  at  least  one  of  the  categories.  The  two  authors  discussed  these  conflicts  and  could 
converge their opinion for 14 of these papers. In the case that no agreement could be made, the 
third co-author was involved. The third co-author was provided with the list of 65 Papers and 
independently  categorized  them.  Then,  the  categorized  results  were  reviewed  by  the  authors 
and  the  list  was  finalized.  As  the  result  of  this  process,  53  Papers  were  identified  by  all  the 
authors as irrelevant to the scope of this study and could not be mapped into any marketplace 
category. Of the remaining 312 Papers, the authors identified 20 Papers as systematic literature 
reviews and systematic mapping studies and excluded them from the mapping study. Hence, 
the  answer  to  RQ2  as  detailed  in  Section  5  is  based  on  292  Papers. 

3.3  Gap  Analysis 

We performed quantitative and qualitative comparison to diagnose the gap between research 
and  practice.  First,  we  compared  the  population  of  researchers  and  practitioners  publishing 
papers  or  tools  related  to  a  particular  category.  We  calculated  the  number  of  distinct  paper 
authors  and  distinct  app  and  action  developers  in  our  data  of  RQ1  and  RQ2.  Although 
on  GitHub  we  identified  developers  with  their  unique  user  IDs,  we  found  several  different 
formats for author names from the studies in our mapping study (RQ2). Hence, we performed 
to  ensure  that  we  identify  these  different  formats  and  avoid  counting 
duplicate  authors.  On  the  other  hand,  there  can  be  multiple  authors  having  the  same  name. 
substring  matching 
In order to distinguish between authors with identical names, we performed manual inspection 
of their research profile on the internet, compared their affiliations, co-authors, and then made 
a decision. For each category in the marketplace, we then compared the number of authors to 
the number of developers. We highlighted categories where a higher proportion of researchers 
are working compared to developers and vice versa. We also identified the categories with the 
least difference between the proportion of researchers and developers. 

Second, we counted the number of contributions (i.e the apps and actions in the marketplace 
(RQ1)  and  the  number  of  papers  in  literature  (RQ2))  to  discuss  the  extent  of  topic  coverage. 
We compared the absolute number and the percentage (for example, proportion of the number 
of papers in each category to overall number of papers). 

Third,  we  compared  the  popularity  of  studies  with  the  tools  in  the  marketplace.  As  for 
the popularity in practice, we used the average number of installs (for apps), and the average 
number  of  stars  (for  actions)  per  each  category  as  the  proxy.  For  apps,  we  got  the  average 
number of installs per category by taking the total number of installs for all apps in a category 
and dividing it by the total number of apps in that category. We calculated the average number 
of stars per category in a similar manner. For the academic popularity, we calculated the average 
number  of  citations  per  year  across  all  the  papers.  We  used  Google  Scholar’s  API  to  gather 
the number of citations for each paper. In order to get the average number of citations per year, 
we divided the number of citations of a paper by the number of years from the time they were 
published. We used this number instead of the total number of citations so that the papers that 
were published earlier did not get an unfair advantage. We then calculated the average number 
of citations per category in the same way as we calculated the other two metrics for apps and 
actions. We ranked each category based on these popularity proxies. 

 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

11 

Fourth, we compared co-occurrence and overlap between categories among the marketplace 
tools (RQ1) and the mapped studies (RQ2). Developers can categorise their apps and actions 
in  more  than  one  category  within  the  Marketplace.  Similarly,  within  our  mapping  study  we 
categorized the papers in up to three categories. Often, new technologies and innovations ap- 
pear from the synergy between topics. Hence, we compared the perspective of researchers and 
practitioners on the relevance and connection between different categories. Also, we identified 
the most active and inactive intersection between the categories. 

Fifth,  we compared  the content  and  description  of  marketplace  tools  with  the  state  of the 
art studies. We used the short descriptions and full descriptions of apps and actions to extract 
the key topics in the marketplace (RQ1). For literature, we extract these topics from the paper 
title and abstracts in RQ2. 

4 RQ1: Observations from Mining GitHub Marketplace 

As  the  first  study  on  analyzing  the  GitHub  Marketplace,  our  first  research  question  (RQ1)  is 
focused on reporting observations from this platform. To this end, we performed an explorative 
study on all the available data attributes and performed group comparisons whenever possible. 
In  contrast  to  the  known  app  stores  for  the  software  engineering  research  community,  the 
GitHub Marketplace provides moderate degree of customization for the developers. This results 
in non-unified information across apps and actions within the marketplace. For instance, only 
5.01%  of  apps  provide  the  information  of  their  customers.  In  this  study,  we  focused  on  the 
information that is available for all the apps and actions. 

We scraped data for 440 Apps and 7,878 Actions. However, we could not retrieve the details 
of  one  app  and  32  Actions  due  to  HTTP  status  code  errors.  As  a  result,  the  following  results 
are  based  on  analysis  of  439  Apps  and  7,846  Actions.  We  first  compare  differences  between 
apps and actions. Then, we discuss the results of our observations based on the characteristics 
of the information (attributes) provided by the platform, developers, and users (see Figure 2). 

4.1  Apps Versus Actions in GitHub Marketplace 

GitHub apps are a particular type of GitHub action that are hosted and run by GitHub. Apps 
are mostly used in the case that: 

–  Accessing the team’s workflow requires further permissions to the repository, 
–  Repository owner would need to host and run the automation, or 
–  There is a need for a solid architecture of the hosted data. 

GitHub  actions  are  dependent  on  runners  to  execute  GitHub  action  workflows.  One  can 
host their own runner and customize the environment for GitHub action workflows (self-hosted 
runners) or use GitHub virtual machines to run workflows which contains tools, packages, and 
settings available for GitHub actions (GitHub-hosted runners). In comparison to GitHub hosted 
runners, the self-hosted runners offer more control of hardware, operating system, and tools8,9. 

Different steps are required for adding apps and actions to the GitHub Marketplace. To be 

successfully  listed  on  the  marketplace,  apps  must  meet  user  experience  requirements,  branding 

8

9

https://docs.github.com/en/actions/using-github-hosted-runners/ 

about-github-hosted-runners 

https://docs.github.com/en/actions/hosting-your-own-runners/ 

about-self-hosted-runners 

 
 
 
 
 
 
 
12 

Sk Golam Saroar et al. 

  requirements,  in  addition  to  billing  requirements  for  paid  apps.  The  user  experience  requirements 
state  that  apps  must  not  sway  users  away  from  GitHub,  must  integrate  with  the  platform 
beyond authentication, and must be publicly available on the marketplace. Also, listings must 
include valid contact information for the publisher, who should set up webhook events to get 
notified about GitHub plan changes or cancellations through the Marketplace API. 

Moreover, apps must have a relevant description and must specify a pricing plan. According 
to  the  branding  requirements,  apps  must  have  a  logo,  feature  card,  screenshot  images,  and  a 
well written description which are aligned with the recommendations provided by GitHub. Paid 
apps also have additional requirements. To publish a paid plan for an app (or an app that offers 
at least one paid plan) on the GitHub Marketplace, the app must be owned by an organization 
and  the  app  developer  must  have  owner  permissions  in  the  organization.  Furthermore,  the 
organization must be verified. At least 100 Installations and 200 Users are required for GitHub 
apps  respectively  to  be  listed  as  a  paid  app.  Under  the  billing  requirements, 
apps  and 
all paid apps need to handle GitHub Marketplace purchase events to manage new purchases, 
upgrades, downgrades, cancellations, and free trials. 

OAuth 

To publish an action, a developer first needs to create the action in their repository. They 
need  to  ensure  that  the  repository  only  includes  the  metadata  file,  code,  and  files  necessary 
for the action. This allows the developer to tag, release, and package the code in a single unit. 
The  action  must  be  in  a  public  repository  and  the  repository  must  contain  a  single  action. 
The action’s metadata file should have a unique name and be located in the root directory of 
the  repository.  The  name  cannot  match  an  existing  GitHub  Marketplace  category  as  GitHub 
reserves the names of GitHub features. If these requirements are met, the developer can add the 
action to the GitHub Marketplace by tagging it as a new release and publishing it immediately 
on the GitHub Marketplace. 

GitHub Marketplace offers apps and actions that provide various attributes of developers as 
users. While in the mobile app stores, the user is often a general public, in this marketplace, 
the users of the offered products are software developers as well. 

4.2  Developers  Attributes 

As developers publish their products (apps and actions) on the marketplace, they define several 
attributes for their products. In this section, we discuss these developer attributes. 

4.2.1  Name 

Names are the unique identifier for products (apps and actions) on the marketplace. The name is 
limited to 255 Characters. Products cannot have the same name as an existing GitHub account 
unless it is the developer’s own user or organization name. For actions, the name should not 
match an existing marketplace category nor can it have the same name as a published action. 

4.2.2  Categories 

According to GitHub documentation, each app and action in the marketplace can be listed in 
one or two (primary and secondary) categories by its developer. The primary category should 
best  represent  the  main  functionality  of  the  product,  while  the  optional  secondary  category 
should also fit the product. While this is true for actions, we have observed that apps sometimes 

 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

65 

76 

81 

80 

26 
27 

25 

35 

21 

26 

43 

6 

4 

12 

5 

2 

9 

7 

118 

s
e
i
r
o
g
e
t
a
C

Code quality 
Continuous  integration 
Code review 
Mobile CI 
Testing 
Reporting 
Support 
Project management 
Open Source management 
GitHub Created 
Deployment 
API management 
Utilities 
AI Assisted 
Container CI   
Security 
Backup Utilities 
Localization 
Time tracking 
Monitoring 
Publishing 
IDEs 
Code Scanning Ready 
Learning 
Dependency management 
Chat 
Desktop tools 
Mobile 

s
e
i
r
o
g
e
t
a
C

Content Attachments API  1 

Code search 

28 

18 

10 

16 

20 

12 
14 

3 

5 

5 

API management 
Testing 
Reporting 
Utilities 
Continuous  integration 
Publishing 
Support 
Project management 
Code review 
Deployment 
Chat 
Community 
Container CI 
Dependency management 
AI Assisted 

Open Source management 
Security 
Monitoring 
Code quality 
Localization 
Backup Utilities 
Mobile 
Mobile CI 
Learning 
Desktop tools 
IDEs 
Code search 
Time tracking 
Game CI 
Code Scanning Ready 

176 

315 

662 

1058 

507 

684 

126 

224 

155 

864 

58 

560 

401 

244 

345 

188 

46 
22 

65 
91 
66 

20 
24 
37 
33 
3 
7 

13 

2374 

2554 

1800 

0 

20 

40 

60 

80 

100 

120 

0 

500 

1000 

1500 

2000 

2500 

Number  of  Apps 

(a) 

Number of Actions 

(b) 

Fig.  4  Each  app  or  action  is  listed  in  one  or  more  categories  by  its  developer.  (a)  shows  the  categories 
and  frequencies  for  apps,  and  (b)  shows  the  categories  and  frequencies  for  actions. 

have more than two categories. 39 Apps have three categories and two apps have four categories. 
Currently  32  Categories  exist  in  the  marketplace  where  two  of  them  are  only  exclusive  to 
apps  (‘Content  attachments  API’,  and  ‘GitHub  Created’)  and  two  are  exclusive  to  actions 
(‘Community’, and ‘Game CI’). The detailed list of all these categories is provided in Table 3, 
which shows that 87.5% of the categories are mutual between apps and actions. 

Figure 4 shows the number of apps and the number of actions in each category. We used these 
categories  to  further  answer  RQ2  and  systematically  map  literature  on  open  source  software 
and automation into these categories. For apps, categories ‘Utilities’ (118 Apps), ‘Code Review’ 
(81), and ‘Project Management’ (80) has the highest population. While for actions, ‘Continuous 
Integration’ (2,554 Actions), ‘Utilities’ (2,374), and ‘Deployment’ (1,800) are the categories with 
the most population. 

As  a  means  to  understand  the  similarity  between  the  automated  tools  provided  on  the 
marketplace, we were interested in examining how often two categories appeared together and 
how  many  apps  or  actions  the  categories  involved.  Figure  5  demonstrates  the  frequency  of 

Table 3 Categories of apps and actions on the GitHub Marketplace. *We will refer to ‘API management 
and Checking’ as ‘API management’ in the subsequent discussions 

ID  Type  Category 

ID  Type  Category 

Security 

C1  Both  API management and Checking*  C17  Both 
C2  Both 
Testing 
C3  Both  Utilities 
C4  Both  Reporting 
C5  Both  Continuous integration 
Publishing 
C6  Both 
Support 
C7  Both 
C8  Both 
Project  management 
C9  Both  Code  review 
C10  Both  Deployment 
C11  Both  Chat 
C12  Action  Community 
C13  Both  Container CI 
C14  Both  Dependency  management 
C15  Both  AI Assisted 
C16  Both  Open  Source  management 

C18  Both  Monitoring 
C19  Both  Code  quality 
Localization 
C20  Both 
C21  Both  Desktop  tools 
C22  Both  Mobile 
C23  Both 
C24  Both  Mobile CI 
C25  Both  Code  search 
C26  Both  Code  Scanning  Ready 
Learning 
C27  Both 
C28  Both 
Time  tracking 
C29  Action  Game CI 
C30  Both 
C31  App 
C32  App 

Backup Utilities 
Content Attachments API 
GitHub Created 

IDEs 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
14 

Sk Golam Saroar et al. 

d
e
t
s
i
s
s
A

I
A

(b) 

Reporting 

L
e
a
r
n
n
g

i

(a) 

Fig.  5  Frequency  of  categories  appearing  together  for  (a)  apps  and  (b)  actions  on  the  marketplace. 

categories  chosen  together  for  apps  in  Figure  5-(a)  and  actions  in  Figure  5-(b).  For  apps,  the 
biggest  intersection  belongs  to  ‘Code  Quality’  and  ‘Code  Review’  categories  with  31  mutual 
apps. This is followed by ‘Utilities’ and ‘Project management’ with 26 Apps. ‘Utilities’ pairs with 
23 out of 29 other app categories, followed by ‘Project Management’ (with 19 other categories), 
‘Continuous  integration’  (with  19  other  categories),  ‘Code  review’  (with  17  other  categories), 
and  ‘Security’  (with  15  other  categories).  On  the  other  hand,  ‘Code  Scanning  Ready’  and 
‘Content Attachments API’ each pair with only one other app category, ‘Security’ and ‘Project 
management’, respectively. 

For  actions  (Figure  5-(b)),  ‘Continuous  integration’  and  ‘Deployment’  appear  together  for 
603 Actions followed by ‘Continuous integration’ and ‘Utilities’ with 466 Actions. The category 
‘Utilities’ pairs with all other (28 out of 29 Categories) action categories except ‘Code Scanning 
Ready’.  It  also  appears  that  ‘Continuous  integration’  (with  27  other  categories),  ‘Code  qual- 
ity’  (with  25  other  categories),  ‘Publishing’  (with  25  other  categories),  and  ‘Testing’  (with  25 
other  categories)  has  high  number  of  intersections  and  synergy.  ‘Game  CI’  (with  three  other 
categories),  ‘Code  Scanning  Ready’  (with  four  other  categories)  and  ‘IDEs’  (with  seven  other 
categories) pair with the least number of other categories. 

Developers classify their provided tools in up to two of the the 32 Categories provided in the 
marketplace. There are two categories exclusive for apps and two are exclusive to actions. 
‘Utilities’ include most number of apps while ‘Continuous Integration’ has the most number 
of actions. We use these categories for mapping studies in RQ2. 

4.2.3  Short description 

GitHub allows product owners to add brief descriptions for their products in the marketplace to 
explain the product’s main functionality. GitHub suggests keeping short descriptions at 40-80 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

15 

s
r
e
t
c
a
r
a
h
C

f
o
r
e
b
m
u
N

120 

100 

80 

60 

40 

20 

0 

Fig.  6  Number  of  characters  in  short  descriptions  for  apps  and  actions  on  the  GitHub  Marketplace 

Apps 

Actions 

Characters. GitHub also recommends that short descriptions do not include complete sentences 
or more than one sentence10. 

Figure  6  shows  that  on  average  apps  have longer  short  descriptions compared  to  actions. 
On  average,  apps  have  short  descriptions  of  68.72  Characters  and  10.54  Words  and  actions 
have  short  descriptions  of  55.13  Characters  and  8.97  Words.  18.64%  of  the  apps  (82  out  of 
440)  have  short  descriptions  of  less  than  40  Characters  while  33.64%  of  the  apps  (148  out  of 
440)  have  short  descriptions  that  exceed  80  Characters.  This  means  that  52.28%  Apps  on  the 
marketplace have a short description length that falls outside the GitHub recommended range 
of  40-80  characters.  For  actions,  28.90%  and  15.65%  have  a  short  description  length  of  below 
40 Characters and above 80 Characters, respectively. There is a very weak correlation between 
short description lengths and number of stars for actions (correlation =  0.04) and number of 
installs for apps (correlation = −0.01). 

We also were interested to see if specific (key)words and bigrams in the product description 

with 

ngram_range 

TfidfVectorizer 

sublinear  tf  scaling

contributed to higher number of installs (apps) or stars (actions). With this goal, we 
, which 
the app short description texts using 
vectorized 
considers the logarithm of the term frequency, and 
set to (1, 2), meaning that we 
want unigrams and bigrams. We then fit a linear regression model with these vectors against 
the number of installs. The regression model determines the optimal weights to best fit the 
data. Observations that have relatively larger weights have more influence in the analysis than 
observations that have smaller weights. Finally, we used the Eli5 11 python package to inspect 
model parameters and understand how the model works globally. Specifically, we used the 
function from the package which takes our model and vectorizer as inputs and 
returns an explanation of model parameters (weights). In other words, this function gives us the 
show_weights 
words that contribute most to the number of installs, either positively or negatively. Words such 
as ‘learn’, ‘connect’, ‘deploy’ have the largest positive weights while words such as ‘machine’, 
‘help’, ‘open source’ negatively affect the number of installs. We also ran the same experiment 
with action descriptions. Table 4 lists the top words and bigrams that most affect the number 
of installs and number of stars for apps and actions, respectively. 

4.2.4  Full description 

Each product has a dedicated page and URL on the marketplace which includes an elaborate 
description of the app or action. For actions, GitHub does not provide guidelines regarding the 

10

11

https://docs.github.com/en/developers/github-marketplace/listing-an-app-on-github-marketplace/ 

writing-a-listing-description-for-your-app#very-short-description 

https://eli5.readthedocs.io/en/latest/index.html 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
16 

Sk Golam Saroar et al. 

  Table  4  Words  and  bigrams  with  the  highest  positive  and  negative  weights  impacting  number  of 
installs  (apps)  and  number  of  stars  (actions).  Green  shows  positive  and  red  shows  negative  weights 
while darker shade indicates greater amounts. 

Apps 

Actions 

Word/Bigram 
learn 
connect 
deploy 
repo 
speed 
code 
apps 
open source 
help 
machine 

Weight  Word/Bigram 
linters 
validate yml 
detect bug 
git history 
process project 
option 
smell program 
bash 
ssh rsync 
jupyter notebook 

+392817.486 
+72359.028 
+58359.672 
+35675.965 
-33828.643 
-41012.257 
-45310.336 
-50067.428 
-67253.429 
-131001.443 

Weight 
+2639.525 
+1527.648 
+996.803 
+900.346 
+813.082 
+682.447 
+678.411 
+624.838 
-620.961 
-2071.856 

format or length of this description. However, for apps, this full description should consist of 
two parts: a required ‘Introductory description’, and an optional ‘Detailed description’12. 

The ‘Introductory description’ is displayed at the top of the app’s landing page. Although 
developers are free to use more characters, GitHub advises writing a one or two sentence high- 
level summary between 150-250 Characters in the ‘Introductory description’ field. Developers 
can provide further details in the ‘Detailed description’ section, which consists of three to five 
highlights about their products’ value (or defined by GitHub as the ‘value propositions’), each 
described in  no  more  than  two  sentences. A  value  proposition is  a  short  statement  that  sum- 
marizes why a GitHub user should use a particular app. GitHub recommends that developers 
include  a  title and  a  paragraph  of  description  for  each  value  proposition  and  avoid  complete 
sentences or more than one sentence in value proposition titles. For detailed descriptions, de- 
velopers can use up to 1,000 Characters. Developers can also add up to five screenshots of their 
app to the landing page. After they’ve been uploaded, developers can rearrange the screenshots 
and add captions to each. GitHub also provides guidelines for screenshots13. 

html 

To  understand  the  features  offered  by  the  tools  in  each  category,  we  mined  the  long  de- 
scriptions  of  all  the  tools  in  each  category.  Similar  to  the  work  of  Al-Subaihin  et  al.  [2]  on 
mining features from mobile app descriptions, we refer to a feature as a claimed functionality 
mined from the product description. We pre-processed the text by first removing URLs, emo- 
jis, 
tags,  and  punctuation  from  the  descriptions.  We  also  built  a  thesaurus  for  mapping 
abbreviation and replaced each abbreviation with a full text. For this, we manually built a list 
of abbreviations by scanning through the most frequent 500 Words in long descriptions. From 
this  preprocessed  data,  we  extracted  the  top  words  and  bigrams  in  each  category,  the  latter 
often  resembling  product  features.  We compared  the  extracted  bigrams  to  further  investigate 
the  overlap  between  the  categories.  Table  5  shows  the  top  features  from  each  category  and 
the frequency in which they appear in product descriptions. The most frequent feature of the 
.  Categories  ‘Project  management’ 
products  in  the  GitHub  Marketplace  is 
(127  Mentions  of  this  feature),  ‘Open  Source  management’  (54),  ‘Utilities’  (53),  ‘AI  Assisted’ 
(21), and ‘Support’ (10) all have products that offer this feature to some extent such as open or 
close issue, label issue, find issue, or comment on issue using bot. Among these, labeling issue 
also appears  frequently 
is  the  most  prevalent  feature among  products.  The  feature 

issue  management

12

unit  test 

13

https://docs.github.com/en/developers/github-marketplace/listing-an-app-on-github-marketplace/ 

writing-a-listing-description-for-your-app#listing-details 

https://docs.github.com/en/developers/github-marketplace/listing-an-app-on-github-marketplace/ 

writing-a-listing-description-for-your-app#product-screenshots 

 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

17 

  Table  5  Top  features  in  each  marketplace  category  (extracted  from  the  long  description  of  products). 
The  numbers  in  parenthesis  stand  for  the  frequencies  of  each  feature  appearing  in  the  product  descrip- 
tions. 

Category 

Top features 

AI Assisted 

API  management 

Backup Utilities 

Chat 

Code quality 

Code  review 

scanning 

Code 
ready 
Code search 

Community 

Container CI 

Content  Attach- 
ments API 
Continuous  
gration 
Dependency  man- 
agement 

inte- 

Deployment 

Desktop tools 

Game CI 

GitHub Created 

IDEs 

Learning 

Localization 

Mobile 

Mobile CI 

Monitoring 

Open  Source  man- 
agement 
Project  manage- 
ment 

Publishing 

Reporting 

Security 

Support 

Testing 

Time  tracking 

Utilities 

label  issue  (21),  unit  test  (12),  dependency  conflict  (11),  search  code  (4),  code 
enhance  (3) 
api specification (15), api documentation (13), swagger ui (13), performance test 
(5), schema change (3) 
backup  repository  (5),  data  retention  (4),  daily  backup  (3),  run  workflow  (3), 
cloud  storage  (2) 
send message (110), slack notification (76), custom message (34), discord webhook 
(30), slack bot (29) 
code coverage (113), run test (82), static analysis (72), code review (27), automate 
code (5) 
linting process  (64), code coverage (59), automatically merge (6), test coverage 
(5), automate code (5) 
infrastructure  code  (4),  code  scan  (4),  check  vulnerability  (4),  automatically 
merge (2), automate deployment (2) 
issue create (7), unit test (5), code enhance (3), best practice (3), search code (3) 
api key (25), push branch (20), create issue (17), open source (17), issue comment 
(14) 
container  registry  (126),  run  docker  (82),  build  image  (74),  push  image  (61), 
machine learning (3) 
real  time  (3),  account  login  (2),  embed  issue  (2),  team  collaboration  (2),  collab- 
oration leanboard (2) 
docker image (347), run test (320), run workflow (206), docker container (160), 
unit test (36) 
run test (54), project documentation (31), dependency conflict (11), fix vulnera- 
bility (7), update dependency (6) 
docker image (226), run build (136), cloud deploy (4), easy build (3), automatic 
deployment (3) 
cake script (12), UI test (5), desktop app (5), quick filter (5), issue tracker (2) 
resource pack (11), optimize resource (5), build renpy (2), generate zip (2), file 
distribution (2) 
 jira issue (4), friendly bot (3), learn skill (3), post comment (2), connect jira (2) 
code editor (4), smart IDE (3), code deploy (2), app inspection (2), sass solution 
(2) 
update  readme  (8),  execute  workflow  (7),  smart  IDE  (3),  learn  skill  (3),  share 
knowledge  (3) 
check spelling (5), generate translation (4), machine translation (4), google trans- 
late (4), continuous localization (2) 
run test (26), android emulator (9), unit test (9), android app (8), deploy apps 
(3) 
run android (14), android emulator (8), android CI (7), upload artifact (6), deploy 
apps (3) 
send  notification  (19),  run  lighthouse  (16),  html  report  (14),  graphql  inspector 
(5), crash reporting (3) 
create workflow (31), close issue (25), label issue (29), post comment (5), merge 
pull-request  (3) 
create  issue  (68),  label  issue  (59),  create  release  (45),  kanban  board  (9),  track 
progress (7) 
create release (266), release note (178), docker image (132), build deploy (102), 
release tag (94) 
send  message  (52),  unit  test  (50),  slack  notification  (46),  test  report  (33),  see 
dashboard (2) 
security scan (53), scan repositories (51), security analysis (36), fix vulnerability 
(25), static analysis (21) 
support request (14), development support (13), label issue (10), powerful ana- 
lytics (3), customer service (3) 
unit  test  (103),  test  playbook  (88),  code  coverage  (66),  test  report  (47),  test 
automation (6) 
todoist api (10), update readme (9), todo list (8), task organization  (2),  produc- 
tivity growth (2) 
project documentation (159), docker image (128), create workflow (108), depen- 
dency conflict (11), pull-request review (8) 

 
 
 
18 

Sk Golam Saroar et al. 

in  products  from  the  categories  ‘Testing’  (103),  ‘Reporting’  (50),  ‘AI  Assisted’  (12),  ‘Mobile’ 
(9),  ‘Continuous  integration’  (6),  ‘Code  search’  (5),  and  ‘Deployment’  (3).  Apps  and  actions 
in  the  marketplace  which  facilitate 
mainly  fall  in  ‘Code  quality’  (72),  ‘Se- 
curity’  (21),  ‘Code  review’  (3),  and  ‘Continuous  integration’  (3).  The  categories  ‘Dependency 
management’  (17),  ‘AI  Assisted’  (11),  ‘Utilities’  (11),  and  ‘Open  Source  management’  (6)  are 
also  similar  in  that  they  all  have  products  that  help  with 
.  These  prod- 
ucts mainly aid software developers in updating dependencies, resolving dependency conflicts, 
creating  dependency  graphs  and  documentations.  A  variety  of  products  in  overlapping  cate- 
gories also help developers with code coverage, API documentation, automating deployment, 
detecting and fixing vulnerabilities in software, and building and maintaining 

managing  dependency

static  analysis 

images. 

Developers provide long and short descriptions along with their tools on the marketplace. 
Analysis of these texts for extracting software features shows a significant overlap between 
functionalities for different tool categories. 

Docker 

4.2.5  Apps’ Supported languages 

If an app only works with specific programming languages, its developer can select up to ten 
programming languages the app supports. These languages are displayed on the app’s listing 
page.  Filling  this  field  is  optional  for  app  developers  and  is  not  applicable  for  actions.  We 
identified 80 Distinct programming languages that are supported by apps on the marketplace. 
However,  234  Apps  (53.30%)  did  not  document  their  supported  languages.  Among  the  rest, 

with  71.22%  (146  out  of  205  apps), 

with  59.51%  (122  out  of  205), 

Ruby 

with  57.56%  (118  out  of  205), 
JavaScript 
Python 
of 205) are the top five languages supported by the apps. 

with  46.83%  (96  out  of  205),  and 

with  42.93%  (88  out 

Java 

and 

appear  together  in  107  Apps  followed  by 

Some  apps  support  more  than  one  programming  language.  The  chord  diagram  of  Fig- 
ure  7  demonstrates  the  co-occurrence  of  these  languages  among  the  apps.  Most  frequently, 
which 
were jointly supported in 106 Apps. 77 out of the 80 distinct languages identified on the mar- 
Javascript 
,  and 
ketplace  are  supported  by  apps  that support  at least  one  other  language. 
pairs  with 
are  the  only  languages  not  associated  with  any  other  languages. 
(with 
, 
PHP 
) and each pair of languages 
reStructuredText

60 out of 79 other languages, followed by 
OCaml 
55), 
and 
is supported by only one app14. 

ABAP
(pairing with 58 other languages), 
Javascript 

each pair with only one other language (
Ruby 

(with  51).  On  the  other  hand, 

(with  52),  and 

Javascript 

LookML

Python 

Python 

Dhall

Java 

and 

and 

Go 

, 

, 

Go 

Java 
RMarkdown 

Markdown

4.2.6  App Developer Links 

The  app  listing  page  on  the  marketplace  includes  required  and  optional  URLs.  The  two  re- 
quired  URLs  are  the  Customer  support  URL  and  Privacy  policy  URL.  According  to  GitHub 
documentation15,  the  privacy URL should display  the  app’s  privacy  policy  while  the  support 
URL  should  redirect clients  to  a  web  page  for  getting  technical  help,  product  information,  or 
account information. 

The  optional  URLs  include  company  URL,  status  URL,  and  documentation  URL.  The 
company URL is a link to the company’s website while the status URL is a link to a web page 

14These  are  apps  for  a  real-time  collaborative  Markdown  editor 
15

https://docs.github.com/en/developers/github-marketplace/listing-an-app-on-github-marketplace/ 

writing-a-listing-description-for-your-app#listing-urls 

 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

19 

Y
A
M
L

T
E
N

.

a
i
l
u
J

Go 

Fig.  7  Frequency  of  languages  appearing  together  for  apps 

that  shows  the  app’s  status.  Current  and  historical  incident  reports,  web  application  uptime 
status, and scheduled maintenance can all be seen on status pages. Finally, the documentation 
URL leads to documentation that demonstrates how to use the app. 

4.2.7  App Verified domains 

In Section 4.3.2, we mentioned that before app developers can offer paid plans for their apps or 
include a marketplace badge in their app listing, they are required to complete the publisher’s 
verification  process  for  their  organization.  As  part  of  this  process,  the  publisher  must  ensure 
that  their  organization  has  verified  ownership  of  their  domain.  43.51%  (191  out  of  439)  Apps 
on the marketplace have verified domains. 

4.2.8  Apps’  Pricing 

GitHub  actions  are  free  for  both  GitHub  hosted  runners  and  self-hosted  runners  (see  Section 
4.1).  For  instance,  users  can  choose  to  create  a  custom  hardware  configuration  with  more 
processing power in order to run larger jobs using a self-hosted runner. For self-hosted runners, 
each  GitHub  account  receives  a  certain  amount  of  free  minutes  and  storage.  For  example, 
GitHub Free has  500 MB  of  storage  and  2,000  Minutes/Month,  compared  to  1 GB  storage  and 
3,000  Minutes/Month  for  GitHub Pro16.  Customers  are  billed  for  additional  usage  of  GitHub 
actions beyond the storage or minutes included in their account. 

16

https://docs.github.com/en/billing/managing-billing-for-github-actions/ 

about-billing-for-github-actions#included-storage-and-minutes 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
20 

Sk Golam Saroar et al. 

For  apps,  GitHub  Marketplace  pricing  plans  can  be  free,  flat  rate,  or  per-unit.  Free  plans 
are completely free for users and developers are encouraged (not enforced) to offer free plans as 
a way to promote open source services. There are two types of paid pricing plans- flat rate and 
per-unit. Flat rate pricing plans charge a fixed fee on a monthly and yearly basis. On the other 
hand,  per-unit  pricing  plans  charge  a  fixed  fee  on  either  a  monthly  or  yearly  basis  for  a  unit 
that the publisher specifies such as a user, seat, or per developer. Up to 10 Pricing plans can be 
offered in the marketplace listing of each app. These pricing plans allow publishers to provide 
their app with different levels of service or resources. If an app has multiple plan options, the 
publisher can set up corresponding pricing plans. For example, if the app has two plan options, 
an open source plan and a pro plan, the publisher can set up a free pricing plan for the open 
source plan and a flat pricing plan for the pro plan. Different publishers choose different units 
or  combination  of  units  for  their  pricing  plans.  This  variety  of  possible  monetization  plans 
along with the freedom to choose the name for the offered plan creates a very inhomogeneous 
monetization data for repository mining. 

Currently, there are 223 different pricing plan names on the marketplace, 51 of which are 
variations  of  Free,  for  example,  Free,  Open  source,  Basic,  Default,  Starter,  Hobby,  Just  enjoy 
it, etc. On the marketplace, 96.81% (425) Apps have a free tier compared to 19.36% (85) Apps 
with  at  least  one  paid  plan.  80.64%  (354)  Apps  have  only  the  free  plan  in  contrast  to  3.19% 
(14) Apps that have only paid plans. Finally, 16.17% (71) Apps have both free and paid plans. 
The publisher may also want to provide free 14-day trials to customers. Whether the free trial 
is for the flat-rate or per-unit pricing plan can be specified while setting up the pricing plans 
for the app on the marketplace. Customers can purchase apps without leaving GitHub by using 
GitHub’s billing API to pay for the service with the payment method that is already attached 
to their GitHub account. 

Most apps have a free tier while 3.19% Apps offer only paid plans. There is no uniform unit 
for apps’ pricing plans on the marketplace which results into heterogeneous data for 
monetization analysis. 

4.2.9  Actions’ Releases 

Actions include release information in their page and users can view and choose to use older 
releases if available. GitHub documentation does not mention if there is a limit to the maximum 

0 
0

2
2 

4
4 

Number  of  Releases 

Numbe 

eleases 

6 

6 
r of R 

8
8 

0 
1
0
1

Fig.  8  Number of  releases  vs  number of  actions  in  the  marketplace 

 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

21 

number of releases that can be displayed on the action landing page. However, we observed that 
the number of releases range from 1 to 10. For each value n within this range, Figure 8 shows 
the number of actions that have n releases. Majority of the actions have more than one release. 
42.64% of the actions only have one release while 1.43% of the actions have nine releases. 
However, the Pearson correlation coefficient (r) between number of releases and number of 
stars is rather weak (= 0.19). 

Apps and actions each provide a set of unique attributes which can be quantified. In 
particular, majority of actions provide a detailed release history over multiple versions. 

4.3  Platform Attributes 

Al-Subaihin et al. [1] have explored the impact of platforms on software engineering practices. 
While  the  study  is  only  limited  to  the  mobile  app  development,  the  impact  of  platforms  on 
software is non-trivial. In this study, by ‘platform attributes’ we are referring to the attributes 
defined and provided by the GitHub Marketplace. 

4.3.1  Developers 

The marketplace provides developer information for apps and actions. Overall, there are 5,928 
Developers hosting their apps or actions in the marketplace. Among them, 368 published only 
apps, 5,509 published only actions, and 51 has both types of products on the platform. 20.70% 
(1,227) of the developers have more than one product (app or action) in the marketplace. Azure 
has the most number of products (One app and 35 Actions). This is also the highest number of 
actions by one provider in GitHub Marketplace. This is followed by reviewdog with 25 Actions 
but  with  no  app  on  the  marketplace.  Devbotsxyz  owns  the  highest  number  of  apps  with  five 
apps but one action. 

Figure  9  shows  the  number  of  unique  developers  working  on  apps  and  actions  in  each 
marketplace  category.  ‘Continuous  Integration’  (C5),  ‘Utilities’  (C3),  and  ‘Deployment’  (C10) 
are  the  top  three  categories  that  have  the  highest  number  of  active developers.  This  does  not 
come as a surprise as these three categories also have the most number of apps and actions in 

C30 

C29 

C28 

C27 

C 

26 

C 

25 

C 

24 

C23 

C22 

C21 

C32  C1 

C31  4096 

C2 

C3 

1024 

256 
64 

16 

4 

1 

C4 

C5 

C6 

C7 

C8 

C9 

C10 

C11 

C20 

C19 

C18

C17  C16 

C12 

C13 

C14 

C15 

Fig.  9  Number  of  developers  (apps  and  actions)  per  category 

Number  of  Developers 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
22 

Sk Golam Saroar et al. 

the marketplace (see Table 8). On average, each developer has 1.40 Products (apps or actions) 
on the marketplace. For each category, if we consider the ratio between number of developers 
to  the  number  of  products  (i.e  apps  or  actions),  ‘Continuous  Integration’  (C5,  ratio  =  1.30), 
‘Utilities’  (C3, r =  1.29),  and ‘Code  Quality’  (C19,  r =  1.25)  are  ranked  highest  among  all  the 
categories.  The  very  close  ratios  for  all  the  categories  implies  that  majority  of  the  developers 
(79.30%) on the marketplace tend to publish only one tool regardless of the category. 

4.3.2  GitHub Verification of creators and products 

GitHub Marketplace has two separate processes for verifying creators and verifying products 
(i.e apps or actions). 

To offer paid plans for apps or to include a marketplace badge in the app listing, the app 
developer must complete the publisher’s verification process for their organization and become 
a  ‘verified  creator’.  As  part  of  this  process,  the  publisher  must  ensure  that  their  basic  profile 
information is accurately filled out, including an email address for support and updates from 
GitHub.  Two-factor  authentication  also  needs  to  be enabled  for  the  organization.  The  organi- 
zation must have verified ownership of their domain and ensure that a ‘Verified’ badge displays 
on the organization’s profile page. GitHub will review the details and inform the organization 
once their publisher verification is complete. Once the organization has been verified, they can 
publish paid plans for their apps. 

Actions  with  the  verified creator badge  indicate  that  GitHub  has  verified  the  creator  of  the 
action as a partner organization. However, there are two possible levels of verification for apps: 

–  App  meets  the  requirements  for  listing:  These  apps  meet  the  listing  requirements  but  the 
publisher has not been verified. Apps with this badge cannot change their pricing plan until 
the publisher successfully applies for verification. Or, 

–  Publisher domain and email verified: These are granted to the apps that are owned by an 
organization  that  has  verified  ownership  of  their  domain,  confirmed  their  email  address, 
and required two-factor authentication for their organization. In other words, the app went 
through the publisher verification process and was successfully granted a ‘verified creator’ 
status. 

There  are  3.67%  (304  out  of  8,225)  Verified  creators  in  the  marketplace.  2.67%  (209  out  of 
7,846)  of  the  actions  on  the marketplace  have verified  creators.  69.09% (304  out  of  440)  of  the 
apps in the marketplace are not verified by GitHub and does not have any level of verification 
badge. Among the 136 Verified apps, 41 has the ‘App meets the requirements for listing’ status 
while 95 holds ‘Publisher domain and email verified’ status. 

4.3.3  GitHub Recommended Apps 

15.0% (66 out of 440) of the apps on the GitHub Marketplace have a label stating Recommended. 
The number of installs for these apps are not provided. GitHub does not provide any official 
description  of  the  eligibility  criteria  for  this  tag.  We  checked  the  list  of  recommended  apps 
across  different  users  as  well  as  for  three  users  over  time.  The  set  of  recommended  apps  are 
static and is not curated based on a user or usage over time. Further, we hypothesized that this 
could be relevant to apps verifiability and authentications. However, our observations showed 
that these recommended apps have a lower percentage (18.18%) of verification. Only 22.72% of 
these recommended apps have a verified domain compared to 47.06% of the non-recommended 
apps with at least one verified domain. ‘Localization’ (33.33%), ‘Time Tracking’ (28.57%), and 
‘Support’ (25.0%) are the categories with the highest proportion of recommended apps. 

 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

23 

Majority (79.3%) of the developers have published only one tool on the marketplace. Only 
2.67% Action developers and 21.59% App developers are verified creators on the 
marketplace. Despite flexible pricing plans for the tools on the marketplace, only verified 
creators can distribute paid tools. 

4.4  User and Usage Attributes 

In  the  GitHub  Marketplace,  the  attributes  provided  for  apps  and  actions  are  not  similar.  For 
instance, the actions have information about the ‘number of stars’ while for apps, the ‘number 
of  installs’  are  provided.  We  will  refer  to  the  apps  with  the  fewest  number  of  installs  as  the 
least popular apps. Similarly, the least popular actions are the ones with the fewest number of 
stars. The inaccuracy of these ratings have been discussed by Ruiz et al. [32] for mobile apps. 
In the context of our study, while we use the number of stars and the number of installs as the 
most  accessible  user  feedback,  it  is  essential  that  the  reader  considers  the  limitations  to  this 
interpretation. 

4.4.1  Apps’ Number of Installs 

Cracking the code of success for products is the ultimate wish of product owners. In the context 
of platform-mediated software products (particularly mobile app stores), success analysis has 
been discussed qualitatively by gathering developer [34, 1] and user perceptions [22] or mining 
and observing quantitative measures for extracting the confounding factors to the app’s rating 
and number of installs [46]. Following that footstep, we performed a heuristic search to mine 
confounding factors for the popularity of products on the GitHub Marketplace. Due to GitHub’s 
mechanism of masking the number of installs for the ‘recommended apps’ (see Section 4.3.3), 
we only based our analysis in this section on 85% of the apps (374 apps out of 440). 

Figure  10-(a)  is  the  Boxplot  distribution  of  the  ‘number  of  installs’  for  apps.  The  highest 
number of installs was 1.5 Million which belonged to 
followed by Travis 
is  the third  most  installed  app with  207,000 Installs. 
CI  with  314,000  Installs. 
Among  the  top  quartile  of  most  installed  apps,  three  are  developed  by  GitHub.  The  average 
number of installs per app on the marketplace is 7,530.12. However, 92.25% of apps have less 
than  the  average  number  of  installs.  The  median  number  of  installs  is  only  143.5.  Among 
the  apps,  the  highest  average  number  of  installs  belong  to  categories  ‘GitHub  Created’  (with 
594,500  Installs),  ‘Learning’  (115,748.08),  and  ‘API  Management’  (75,320). 

GitHub  Learning  Lab 

Slack  +  GitHub 

4.4.2  Actions’ Number of Stars 

GitHub  users  star  repositories  to  keep  track  of  the  projects  they  find  interesting  as  well  as 
to  show  appreciation  to  the  repository  maintainer  for  their  efforts.  GitHub  may  recommend 
related  content  to  a  user  on  their  personal  dashboard  based  on  their  starred  repositories.  For 
this  reason,  users  also  star  repositories  in  order  to  find  similar  projects  on  GitHub.  Many  of 
GitHub’s repository rankings depend on the number of stars a repository has17. In our analysis, 
we also use the number of stars as a popularity metric for the actions on the marketplace. 

The highest number of stars for actions belongs to 

developed by 

developed by 

Super-Linter 

with 
with 3,900 
github 

6,800 stars followed by 

17

yq  -  portable  yaml  processor 

mikefarah 

https://docs.github.com/en/get-started/exploring-projects-on-github/ 

saving-repositories-with-stars 

 
 
 
 
 
 
 
 
 
 
 
24 

Sk Golam Saroar et al. 

s
l
l
a
t
s
n
i

f
o
#

106 

105 

104 

103 

102 

101 

104 

103 

102 

f
o
#

101 

Apps 

(a) 

Actions 

(b) 

Fig.  10  Distribution  of  number  of  installs  for  (a)  apps  and  number  of  stars  for  (b)  actions  on  the 
marketplace 

has the most number of actions (35 Actions) followed by 

stars.  Figure  10-(b)  demonstrates  the  Boxplot  distribution  of  the  star  ratings  among  actions. 
(25 Actions). When 
looking into the proportion of number of actions in a category and the number of stars, ‘Time 
Azure 
tracking’ and ‘Community’ are the top two categories while ‘Game CI’ and ‘Backup Utilities’ 
are  the  least  popular.  Also,  39  out  of  the  40  Actions  with  the  least  number  of  stars  belong  to 
the ‘API Management’ category. 

reviewdog 

4.4.3  Actions’ Contributors 

Additionally,  the  marketplace  also  lists  all  the  contributors  of  an  action.  Contributors  are  the 
software developers who open an issue, propose a Pull Request, or commit any type of change 
to  the  default  branch  of  an  action  repository.  A  developer  may  or  may  not  be  a  contributor. 
7,111  Actions  (90.63%)  in  the  marketplace  have  at  least  one  contributor.  61.81%  actions  have 
only one contributor. On the other hand, a total of 145 Actions have 12 Contributors, which is 
the  largest  number  of  contributors  for  actions  on  GitHub.  4,915  out  of  7,878  Actions  (62.4%) 
involved their developer as a contributor. However, we found a weak negative point-biserial 
correlation  (r  =  -0.32)  between  the  number  of  contributors  and  the  action’s  developer  also 
being a contributor. 

We measured the popularity of apps and actions based on the number of installs and the 
number of stars, respectively. The apps created by GitHub (C32) and the apps on ‘Learning’ 
(C27) are the top two app categories with the highest average number of installs. ‘Time 
tracking’ (C28) and ‘Community’ (C12) have the highest average number of stars among the 
actions. 

4.5  Relation Between the Attributes 

66.8%  (250)  of  the  374  Apps  that  display  their  number  of  installs  are  not  verified  by  GitHub. 
The  average  number  of  installs  for  these  apps  is  436.7  compared  to  an  average  of  21,831.36 
Installs for the apps that are verified by GitHub. Similarly, the average number of installs for 
the 191 Apps that have at least one verified domain is 11,711.18. On the other hand, the average 
installs for apps without a verified domain is only 2,336.41 for 248 Apps. This phenomenon also 
holds true for actions, although only 2.67% (209) of the actions on the marketplace have verified 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
GitHub Marketplace for Practitioners and Researchers to Date 

25 

Is GitHub Verified 

Has Documentation 

Has Free Trial 

Has Verified Domain(s) 

s
l
l
a
t
s
n
I

f
o

r
e
b
m
u
N

106 

105 

104 

103 

102 

101 

False 

True 

False 

True 

False 

True 

False 

True 

Fig. 11  Distribution of the number of installs in regards to different marketplace properties for apps 

creators. The average number of stars for these 209 actions is 115, which is significantly more 
than 17.35, the average stars for actions without a verified creator. 2,199 Actions have 0 (zero) 
stars, of which only 0.14% (3 out of 2,199) have a verified creator. 

One  common  thing  about  these  least  popular  apps  on  the  marketplace  is  their  lack  of 
verification. None of the ten least popular apps are verified by GitHub and only one of them 
has  a  verified  domain,  in  contrast  to  the  overall  percentage  of  43.51%  for  apps  with  verified 
domains. Three of these ten Apps belong to the ‘Utilities’ category. Only 20% among these ten 
apps have a link to the documentation, which is far below the overall percentage of 64.44% on 
the  marketplace.  Notably,  none  of  the  ten  apps  have  a  paid  version.  However,  79.68%  of  the 
apps on GitHub do not have a paid version. Therefore, this might not be a distinctive feature 
of  the  apps  with  the  least  number  of  installs.  Figure  11  shows  box-plot  distributions  of  the 
number of installs in regards to different marketplace properties for apps. The median number 
of  installs  is  higher  for  apps  that  are  verified  by  GitHub  compared  to  the  apps  that  are  not 
verified. Similarly, this median value is higher for apps that have documentation, free trial, or 
verified domains. However, we did not find any strong correlation between number of installs 
and these app properties. The point-biserial correlation coefficients between number of installs 
and  (i)  is  GitHub  verified  (0.12)  (ii)  has  documentation  (-0.04)  (iii)  has  free  trial  (0.01)  (iv) 
has verified domain(s) (0.06) are trivial. 

Verification is a contributing factor for higher number of stars for the actions as well. Only 
2.66% Actions have a verified creator on the marketplace. However, 24 out of the top 100 Actions 
with the most number of stars have a verified creator. Five out of the top ten actions belong to 
the  ‘Utilities’  category.  Three  among  these  ten  actions  share  the  categories  ‘Deployment’  and 
‘Publishing’. 

number of  installs 
recommended 
description  length 
verified  by  github 
has  verified  domain 
has  documentation 

1 
-0.04 
-0.01 
0.12 
0.06 
-0.03 

has  free  trial 

0.01 

1 
-0.08
-0.12   
-0.18   
-0.16 
  -0.01 

0.0016  0.0001  0.0144  0.0036  0.0009  0.0001 
  0.0064  0.0144  0.0324  0.0256  0.0001 
  0.0004  0.0004  0.0049  0.0009 
  0.2304  0.0729  0.1849 
  0.0729  0.0441 

1 
description  length 
0.04
number  of  stars 
0.06 
has  verified  creator 
0.05
number  of  releases 
number of contributors  0.05 

  0.0016  0.0036  0.0025  0.0025 
  0.0016  0.0361 

0.16 
0.0036  0.0961 
1 
0.39 

  0.1521 
1   

1 
0.02 
0.02 
0.07 
0.03 

d
e
d
n
e
m
m
o
c
e
r

h
t
g
n
e
l

n
o
i
t
p
i
r
c
s
e
d

s
l
l
a
t
s
n

i

f
o

r
e
b
m
u
n

1 
0.48
0.27 
0.43

1 
0.27
0.21 

b
u
h
t
i
g

y
b
d
e
i
f
i
r
e
v

1 
0.2

n
i
a
m
o
d

d
e
i
f
i
r
e
v

s
a
h

l
a
i
r
t

e
e
r
f

s
a
h

  0.04 
n
1   
o
i
t
a
t
n
e
m
u
c
o
d

s
a
h

1 
  0.04 
0.19
h
  0.4 
t
g
n
e
l

n
o
i
t
p
i
r
c
s
e
d

s
r
a
t
s

f
o

r
e
b
m
u
n

1 
0.06
0.31 

r
o
t
a
e
r
c

d
e
i
f
i
r
e
v

s
a
h

s
e
s
a
e
l
e
r

f
o

r
e
b
m
u
n

s
r
o
t
u
b
i
r
t
n
o
c

f
o

r
e
b
m
u
n

(a) Apps 

(b)  Actions 

Fig.  12  Correlation matrix between different attributes for (a) apps and (b) actions. The upper triangle 
shows  the  effect  size.  As  effect  size,  we  use  the  coefficient  of  determination  (goodness  of  fit). 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
 
 
   
 
 
 
 
 
 
     
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
 
 
 
   
 
26 

Sk Golam Saroar et al. 

We analyzed the relationships between a number of attributes and the popularity of apps and 
actions. Among all, the verification status demonstrated the most strong correlation. 

5 RQ2: Mapping Studies of Studies in Open Source Software Engineering 

In  the  previous  section,  we  explored  the  status  quo  of  the  GitHub  Marketplace,  which  are 
the  tools  practitioners  use  in  the  open  source  ecosystem  (RQ1).  To  address  the  knowledge 
mobilization gap (RQ3), we performed a systematic mapping of software engineering studies 
in RQ2. We discuss the results of the mapping study in this section. 

As  discussed  in  Section  3.2,  we  gathered  relevant  papers  on  automation  within  the  open 
source software ecosystem between the years 2000 to 2021. More importantly, we were interested 
in gathering and synthesizing the status of automation techniques and tools offered for the open 
source community. Recent years have seen an increase in the number of articles published in this 
field. Figure 13 shows the number of publications per year relevant to ‘automation’ and ‘open 
source software engineering’ in the top 20 software engineering venues. There is a steep increase 
in  the  number  of  publications  starting  from  the  year  2009,  and  there  is  another  peak  in  2015 
onward.  There  were  32  publications  in  2021,  compared  to  only  six  publications  in  2011  (ten 
years period), strengthening  the case that researchers are recently more active in this domain 
than ever before. 

Following the systemic protocol procedure described earlier (see Section 3.2), two authors 
independently mapped the papers included in the study into the app and action categories (32 
Categories overall). Table 6 demonstrates the results of this mapping. 

5.1  Quantitative Analysis of the Mapped Studies 

As shown in Figure 14, ‘Code quality’ was the most popular category in software engineering 
literature  with  97  out  of  292  papers.  We  also  examined  how  often  two  categories  appeared 
together and how many papers the categories involved. Figure 16 demonstrates the frequency 
of categories chosen together for papers. The biggest intersection belongs to ‘Code quality’ and 
‘Code search’ with 24 Papers sharing these two categories. This is followed by ‘Code quality’ 
and ‘Code review’ with 20 Papers. ‘Utilities’ pairs with 22 out of 26 other categories for papers, 
followed by ‘Code quality’ (with 20 other categories), and ‘Support’ (with 20 other categories). 

s
r
e
p
a
P
f
o
r
e
b
m
u
N

35 

30 

25 

20 

15 

10 

5 

0 

31    31 

32 

28 

26 

26 

19 

13 12 

14 15 

13

6 

6 

1  1  2  2  3  4  3  4 

0
0
0
2

1
0
0
2

2
0
0
2

3
0
0
2

4
0
0
2

5
0
0
2

6
0
0
2

7
0
0
2

8
0
0
2

9
0
0
2

0
1
0
2

1
1
0
2

2
1
0
2

3
1
0
2

4
1
0
2

5
1
0
2

6
1
0
2

7
1
0
2

8
1
0
2

9
1
0
2

0
2
0
2

1
2
0
2

Year 

Fig.  13  Number  of  categorized  papers  within  our  systematic  mapping  study  (RQ2)  in  each  year 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

27 

Table 6  Papers mapped to GitHub Marketplace categories 

Category 

ID 

API  management 
and Checking 

P16, P29, P64, P140, P186, P261, P340, P361 

Testing 

Utilities 

Reporting 

Continuous 
gration 

inte- 

Publishing 

Support 

Project  manage- 
ment 

Code  review 

Deployment 
Chat 

Community 

Dependency  man- 
agement 

AI  Assisted 

Open Source man- 
agement 
Security 

Monitorin

Code quality 

Localization 
Desktop tools 
Mobile 
IDEs 
Mobile CI 

Code search 

scanning 

Code 
ready 
Learning 
Time tracking 

P14, P18, P28, P60, P71, P76, P79, P83, P89, P91, P92, P95, P107, P114, P128, 
P130, P131, P132, P137, P138, P160, P164, P168, P170, P173, P176, P177, P179, 
P185, P188, P192, P193, P203, P209, P217, P221, P222, P227, P249, P260, P262, 
P263, P264, P266, P273, P276, P277, P279, P282, P283, P284, P288, P303, P323, 
P331, P341, P350, P357, P360, P363, P364 
P8, P13, P36, P38, P42, P51, P62, P64, P73, P100, P103, P108, P117, P120, P130, 
P139, P140, P150, P151, P154, P155, P159, P163, P164, P166, P168, P172, P175, 
P189, P194, P195, P207, P231, P236, P259, P267, P270, P301, P340 
P21, P44, P45, P50, P65, P67, P69, P71, P81, P99, P100, P105, P109, P174, P180, 
P182, P216, P220, P234, P239, P240, P287, P327, P334, P350, P359, P364 

P37, P83, P98, P194, P224 

P6, P25, P66, P98, P121, P186, P194, P215, P238, P242, P289, P291, P292, P304, 
P305, P313, P355, P356 
P8,  P15,  P24,  P28,  P42,  P48,  P55,  P72,  P77,  P80,  P82,  P84,  P87,  P98,  P101, 
P105, P107, P117, P119, P121, P126, P135, P157, P168, P169, P170, P175, P197, 
P207, P215, P216, P229, P240, P242, P248, P265, P267, P272, P275, P286, P290, 
P294, P301, P305, P311, P328, P345 
P5, P6, P10, P15, P25, P37, P47, P57, P69, P88, P116, P121, P127, P135, P148, 
P152, P158, P182, P187, P238, P244, P248, P252, P256, P257, P259, P265, P269, 
P271, P278, P286, P289, P290, P291, P292, P298, P300, P313, P322, P330, P334, 
P344, P345, P352, P355, P356, P359 
P1, P2, P22, P29, P36, P51, P59, P68, P70, P78, P80, P82, P93, P99, P104, P106, 
P111, P117, P122, P125, P133, P140, P144, P155, P156, P165, P166, P187, P205, 
P230, P233, P247, P250, P289, P299, P314, P317 
P10, P79, P163, P189, P291, P292, P311, P326 
P35, P39, P74, P220, P236 
P5,  P25,  P27,  P35,  P37,  P39,  P47,  P50,  P59,  P72,  P73,  P74,  P94,  P113,  P118, 
P119, P134, P242, P275, P305 
P7, P17, P78, P143, P159, P161, P171, P180, P193, P196, P201, P202, P215, P221, 
P225, P228, P230, P235, P243, P253, P254, P255 
P10, P17, P22, P36, P56, P61, P65, P75, P81, P92, P102, P112, P127, P136, P142, 
P146, P147, P153, P155, P157, P158, P161, P163, P166, P171, P182, P195, P212, 
P225, P227, P228, P232, P235, P244, P248, P249, P250, P252, P253, P254, P255, 
P256, P262, P266, P290, P298, P300, P307, P311, P316, P325, P336, P347, P354 
P14, P29, P47, P57, P72, P73, P74, P77, P83, P84, P94, P108, P118, P119, P148, 
P220, P223, P236, P244 
P55,  P136,  P148,  P172,  P191,  P196,  P201,  P279,  P299,  P315,  P361 
P80, P110, P116, P124, P132, P139, P143, P151, P152, P158, P167, P171, P174, 
P177, P196, P252, P265, P270, P278, P326 
P1,  P2,  P16,  P28,  P44,  P60,  P62,  P68,  P70,  P75,  P78,  P89,  P90,  P93,  P95,  P97, 
P101, P102, P103, P104, P106, P109, P111, P112, P114, P120, P122, P124, P125, 
P127, P129, P133, P137, P139, P142, P143, P144, P145, P146, P147, P150, P151, 
P153, P154, P156, P157, P160, P165, P167, P170, P172, P178, P180, P181, P184, 
P187, P188, P189, P193, P195, P203, P204, P206, P214, P217, P223, P224, P229, 
P230, P231, P238, P247, P254, P258, P266, P268, P272, P273, P278, P279, P281, 
P287, P288, P293, P302, P306, P309, P312, P313, P314, P317, P323, P330, P332, 
P346, P351, P365 
P16,  P145,  P146,  P147,  P150,  P161,  P202,  P221,  P225,  P228,  P235,  P253,  P255 
P4,  P100,  P108,  P169,  P240 
P50,  P76,  P79,  P109,  P128,  P183,  P260 
P42, P51, P205 
P76 
P1,  P7,  P14,  P22,  P27,  P44,  P64,  P68,  P70,  P90,  P93,  P95,  P101,  P104,  P106, 
P112, P114, P120, P125, P133, P137, P153, P154, P156, P164, P165, P185, P197, 
P209, P212, P239, P264, P268, P272, P273, P302 

P2, P90, P102, P142, P145, P202, P209, P216, P239, P267, P288, P299 

P5, P45, P122, P126, P174, P205, P275, P294, P327, P364 
P35,  P57,  P152,  P250 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
28 

Sk Golam Saroar et al. 

0
0
0
2

1
0
0
2

2
0
0
2

3
0
0
2

4
0
0
2

5
0
0
2

6
0
0
2

8
0
0
2

9
0
0
2

0
1
0
2

1
1
0
2

2
1
0
2

3
1
0
2

4
1
0
2

5
1
0
2

6
1
0
2

7
1
0
2

8
1
0
2

9
1
0
2

0
2
0
2

1
2
0
2

l
a
t
o
T

7
0
0
2

2 

API management 

Testing 
Utilities   

1 

1   

2 
1  3   

4 

2  2 

1 

5 

2 

3 

1 

4 

1 

6 

2 

3 

6 

1 

8 

8 

3 

10 

61 

2  1  1  1  2  3  2  7  3  3  6  3  39 

Reporting  1  1  1   

1   

1  1  1   

3   

1  3  1  4  5  3  27 

Continuous  integration   
Publishing   

Support  1   

2 

1   

1 

1   

1   

1   
2  2  1  1  1  3  4  3  6  5  5  7  3  47 

2   

1  1   
1  2  3  3  4  18 

2  1 

5 

1  3  2  2  1  2  2  5  5  6  4  4  5  4  47 

Project  management   
Code review   
Deployment   
Chat   
Community   
Dependency  management   
AI Assisted   
Open Source management   
Security   
Monitoring   
Code quality   
Localization   
Desktop tools  1   

1   

Mobile   
IDEs 
Mobile CI   
Code search   
Code Scanning Ready   
Learning   
Time tracking   

1   

1  1  1   
1   

1   

1   

1  1   
2   

1   
2   

1   
1  1  1   
1   

4  3  5  3  6  5  5  37 
1  1  2  1   
2   
1   
6  1  4  2  1  20 

1  2 

5 

8 

1   
1  3   

2  3  1  2  1  2  1  3  1  1  21 

4  3  2  7  5  7  5  5  4  7  54 

1   

1  1   

1  4  3  2  2  3  19 

1   

1   

1  1   
1   

1   
1  1   

1  1   
1   

2  1   
2  1   
3  2  2  3  4   

1   

11 

20 

1  3  3  1  3  3  5  2  4  4  4  5  7  10  7  10  11  13  97 
4   
1  1   

1  1  2   

1   

2   

13 

1   

2   

1  1   

1  3   

1 

1  1   
1   

1   

5 

7 

3 

1 

1   
1  1   

1   
1  1   

1  1  2  1  3  1   

1   

1   

1  1   

1   

1   

4  2  6  1  4  4  5  36 
1  1  2   

1  1  2  12 

1  1  1  1   
1   

1   

10 

1 

4 

Fig.  14  Number  of  papers  in  each  year  per  category.  Categories  with  zero  number  of  paper  were 
excluded from the heatmap. 

On the other hand, ‘Mobile CI’ pairs with only two other categories while ‘IDEs’ intersect with 
four other categories and ‘Time tracking’ appear together with five other categories. 

We looked into the number of distinct authors per category as an indicator for the number 
of contributors in each domain. We counted a total of 871 distinct authors for 292 Papers in our 
mapping study. Figure 15 shows the number of individual authors in each marketplace category. 
‘Code  quality’  (C19),  ‘Testing’  (C2),  and  ‘AI  Assisted’  (C15)  are  the  top  three  categories  that 
have the highest number of active researchers. These are also the top three categories with the 
most  number  of  papers  (see Table  8)18.  Similarly,  the category  ‘Mobile CI’  (C24)  has  the  least 
number of papers (one) as well as distinct authors (three). 25.68% of the papers (75 out of 292) 
have three authors while the median number of authors per paper is 3.5. 13 Papers out of 292 
(4.45%) have only one author. 

‘Code  quality’  has  the  highest  number  of  mapped  studies  among  all  categories.  Papers  in  this 
category  also  appear  frequently  in  ‘Code  search’  (24  times)  and  ‘Code  review’  (20  times). 
‘Code  quality’  also  has  the  highest  number  of  (299)  distinct  authors,  followed  by  ‘Testing’ 
(208) and ‘AI Assisted’ (206). 

18There is a strong positive correlation (r = 0.99) between the number of papers and the number of 

unique authors researching in a category across all the papers. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

29 

C32  C1    C2 

729 

C31 

243 

81 
27 

9 

3 

1 

C30 

C29 

C28 

C27 

C26 

C25 

C24 

C23 

C22 

C21 

C3 

C4 

C5 

C6 

C7 

C8 

C9 

C10 

C11 

C12 

C13 

C20 

C19 

C14 

C15 

C18 

C17 

C16 

Number of Authors 

Fig.  15  Number  of  authors  per  category 

5.2  Qualitative Analysis of the Mapped Studies 

Software programs often do not perform in isolation. GitHub Marketplace defines ‘API man- 
agement’ (C1) as the tools to structure API infrastructure to enable various internet gateways 
to interact with offered services. On the other side, ‘Dependency Management’ (C14) tools help 
secure and manage third-party dependencies. Literature in both categories discuss approaches 
and tools for bug detection and requirement traceability. 

The  marketplace  encourages  developers  to  categorize  tools  intended  to  help  them  elimi- 
nate  bugs  and  ship  their  products  in  the  ‘Testing’  (C2)  category.  The  category  involves  test 
management tools, logging tools, and benchmarking tools. Further, the ‘Code Scanning Ready’ 
(C26)  category  is  dedicated  to  static  analysis,  dynamic  analysis,  container  scanning,  linting, 
and fuzzing tools. These tools are mostly integrated with GitHub Code Scanning SARIF19 Up- 
load. Our mapping resulted in 61 Studies that suggested automation in the ‘Testing’ category, 
and  18  of  them  offered  tool  support.  Others  focused  on  proposing  models  and  algorithms  or 
benchmarking the performance of different techniques for improved testing practices and tested 
these models using open source projects. 

GitHub  Marketplace  offers  ‘Utilities’  (C3)  as  a  set  of  auxiliary  tools  to  enhance  the  de- 
velopers’  experience  on  GitHub.  The  category  includes  tools  for  backup,  bots,  pull  request 
automation,  backup  tools,  visualization  tools,  and  environment  setup  tools.  We  mapped  39 
Studies  in  the  ‘Utilities’  category  and  similar  to  the  marketplace  products,  these  studies  are 
distributed  across  different  phases  of  the  software  life  cycle.  The  ‘Monitoring’  (C18)  category 
provides tools to monitor the impact of code changes and help developers to measure perfor- 
mance,  track  errors,  and  analyze  applications.  We  mapped  20  Studies  in  this  category  which 
discuss  topics  such  as  crash  localization,  crash  reproduction,  reporting  problems,  code-smell 
detection, among others. When it comes to ‘Reporting’ (C4), the GitHub Marketplace consists 
of tools that provide insight into the team’s development activities. We categorized 41 Papers 
into the ‘Reporting’ category. This category involves variety of studies which assist the devel- 
opers to get further insight into their project through different medias such as measurement or 
visualization. However, majority of the studies that fell into this category were mainly focused 
on mining repositories and emphasizing the need for automation tools. 

19SARIF stands for Static Analysis Results Interchange Format 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
30 

Sk Golam Saroar et al. 

D
e
p
e
n
d
e
n
c
y
m
a
n
a
g
e
m
e
n
t

Fig.  16  Frequency  of  categories  appearing  together  for  the  papers  in  our  analysis. 

To automatically build and test code when pushing it to GitHub and preventing bugs from 
being  deployed  to  production,  the  GitHub  Marketplace  introduces  ‘Continuous  Integration’ 
(C5), with subcategories ‘Container CI’ (C13), ‘Mobile CI’ (C24), and ‘Game CI’ (C29). Within 
our mapping of papers relevant to ‘Continuous integration’, we only considered studies directly 
relevant to containers, mobile, or games as relevant to each subcategory C13, C24, or C29. Our 
search did not result in any publications within the defined protocol for continuous integration 
of containers or open source games. 

In  2015,  Vasilescu  et  al.  [47]  tapped  into  the  opportunity  of  accessing  various  projects  on 
GitHub, which were at different stages of process integration and automation, to pick out the 
effect of continuous integration. Their study showed that by using continuous integration, the 
core developers  of  each team  were  more  effective  at  merging  pull  requests  and  finding more 
bugs. Since then, a number of studies have focused on this topic. However, when it comes to 
automation  of the  continuous  integration  process  for  open  source  software,  we  retrieved  and 
mapped five papers since 2017 (see Figure 14 for the yearly numbers). 

Marketplace’s ‘Publishing’ category (C6) consists of tools for getting sites ready including 
release  tools  for  production  while  ‘Deployment’  (C10)  tools  refer  to  streamline  code  deploy- 
ments. We mapped 18 Papers in the ‘Publishing’ category and only eight papers being primarily 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

31 

relevant to the ‘Deployment’ category. Studies from both categories discuss cloud deployment 
and  evaluating  deployments.  Similar  to  the  marketplace,  the  mapping  in  state  of  the  art  has 
a  high  overlap  with  the  ‘Continuous  Integration’  (C5)  and  ‘Testing’  (C2)  categories.  Among 
[6]  is  a  tool  that  uses  static  analysis  to  detect  faults  at  runtime  and  guarantees 
them, 
the  developer  can  deploy  (or  test)  the  code  without  additional  manual  effort.  The  rest  either 
offered a framework or algorithm for deployment and the analysis stays non pragmatic. 

CCBot 

When it comes to community support, GitHub provides a set of tools. The apps and actions 
in the GitHub’s ‘Support’ (C7) category are designated to help with the needs of the team and 
the customers. ‘Localization’ (C20) consists of tools to extend software’s reach20 and to localize 
and translate continuously from GitHub. The tools in the ‘Chat’ (C11) category, in particular, 
are designed to bring GitHub into the conversations. This is while the marketplace’s definition of 
‘Community’ (C12) tools is quite brief as ‘Tools for the community’. We also decided to briefly 
describe  studies  relevant  to  the  ‘Learning’  (C27)  category  along  with  these  as  they  support 
individuals to get the skills they need for leveling up. In our mapping study, we identified two 
main categories among the studies where the focus is either on adopting the engineering process 
to support the end users or to support the productivity of the development team. 

To  assist  software  teams  in  organizing,  managing,  and  tracking  projects,  GitHub  Market- 
place  offers  tools  in  the  ‘Project  management’  (C8)  category  that  build  on  top  of  issues  and 
pull  requests.  Also,  GitHub  acknowledges  the  difficulties  teams  might  face  when  managing 
their open source projects and provides tools under ‘Open Source management’ (C16) to make 
managing  more  ‘fun  and  manageable’21.  Our  analysis  resulted  in  mapping  47  Studies  in  the 
‘Project management’ (C8) category. Similar to the marketplace, the items within this category 
are  of  a  diverse  nature.  Among  them,  a  few  are  particularly  addressing  automation  tools  for 
developers in the open source environment. Only five studies are particularly proposing a tool 
set  while  the  rest  are  offering  insight  on  the  importance  of  the  automation  in  the  context  of 
project management. The marketplace offers ‘Time Tracking’ (C28) tools for tracking progress 
and predicting the length of tasks based on team’s coding activities. We mapped four studies 
in  this  category  which  mainly  discuss  reducing  code  review  effort  and  documentation  effort, 
prioritizing documentation, and issue management i.e. detecting duplicate issues. 

GitHub  has  a  specified  category,  ‘AI  Assisted’  (C15),  for  the  tools  that  are  enabled  by 
artificial intelligence (AI). While many studies in the software engineering literature are using 
components of machine learning, we discuss the ones that we primarily mapped to this category. 
These  studies  have  focused  on  bug  localization,  automatic  bug  reporting,  finding  traceability 
links,  vulnerability  detection,  automatic  code  generation,  to  name  a  few  areas.  To  find,  fix, 
and prevent security vulnerabilities, the marketplace offers ‘Security’ (C17) tools. Following the 
same direction for literature, we mapped a number of studies that offered tools for vulnerability 
detection, code analysis, finding and fixing broken dependencies, and more. 

GitHub Marketplace defines ‘Code Quality’ (C19) as the category to ‘Automate your code 
review with style, quality, security, and test-coverage checks when you need them.’ ‘Code Re- 
view’  (C9)  is  defined  as  the  set  of  tools  that  ensure  code  meets  quality  standards,  resulting 
in developers shipping software with confidence. Along the same line, ‘Code search’ (C25) is 
the  category  dedicated  to  the  tools  that  query,  index,  or  hash  the  semantics  of  source  code. 
We mapped 97, 37, and 36 Papers in C19, C9, and C25, respectively. Literature in these areas 
focus on static and dynamic analysis, automatic refactoring, test coverage, bug localization and 
reporting, program comprehension, traceability link detection, technical debt analysis, among 
other  tasks.  ‘Desktop  Tools’  (C21)  are  offered  by  the  marketplace  to  assist  developers  to  run 

20This  should  not  be  mistaken  with  the  bug  localization  in  software  research. 
21

https://github.com/marketplace/category/open-source-management 

 
 
 
32 

Sk Golam Saroar et al. 

tools  natively  on  their  local  machines.  The  ‘Mobile’  (C22)  tools  are  designed  to  improve  the 
workflow for the small screen. ‘IDEs’ (C23) is a category of tools that help developers in find- 
ing the right interface to build, debug, and deploy their source code. These are relatively less 
popular  categories  in  literature.  We  mapped  five,  seven,  and  three  studies  in  C21,  C22,  and 
C23 respectively. 

We extracted bigrams from paper titles and abstracts for all the papers in a category. These 
bigrams are presented in Table 7. We followed the same protocol and process as for extracting 
bigrams  for  tools  on  the  marketplace  (see  Table  5).  These  bigrams  represent  the  key  features 
or phrases. 

bug  reporting

One  of  the  common  features  in  literature  is 

,  appearing  in  seven  categories 
namely  ‘AI  Assisted’  (32  Mentions  of  this  feature),  ‘Project  management’  (32),  ‘Code  quality’ 
(24), ‘Reporting’ (22), ‘Code search’ (14), ‘Code review’ (11), and ‘Support’ (8). Another feature 
.  ‘Code  review’  (10), 
that  is  widely  shared  by  multiple  categories  is 
‘Dependency management’ (10), ‘Utilities’ (10), ‘IDEs’ (10), ‘AI Assisted’ (9), and ‘Localization’ 
(8) all have papers on the topic of 
. The categories ‘Code quality’ 
(7), ‘Code Scanning ready’ (6), ‘Utilities’ (6), and ‘Deployment’ (4) are similar in that they 
is discussed in 
all have papers that facilitate 
papers that fall under the categories ‘Testing’ (13), ‘Mobile’ (4), ‘Mobile CI’ (4), and ‘Open 
source management’ (2). Table 7 shows the top features per category along with the frequency 
in which these features appear in paper titles and abstracts. 

requirement  traceability
. The topic of 

requirement  traceability

test  automation 

static  analysis

Studies in different categories often discuss similar topics/features. The most common 
feature in literature is ‘bug reporting’ (appearing in seven categories) followed by 
‘requirement traceability’ (six categories), ‘static analysis’ (four categories), and ‘test 
automation’ (four categories). 

6 RQ3: The Gap 

We performed a systematic study of the marketplace and the literature to perform a gap analysis 
in  RQ3.  The  category  ‘GitHub  Created’  is  excluded  from  this  analysis  as  this  category  only 
applies to actions and not apps or papers. 

6.1  The Interest of Researchers and Practitioners Across Categories 

There  are  871  distinct  authors  and  5,937  distinct  developers  in  our  dataset  of  292  Papers  and 
8,318 GitHub products (440 Apps and 7,878 Actions). We presented the number of developers 
(Figure 9) and the number of authors (Figure 15) per category in RQ1  and RQ2, respectively. 
By comparing the two, we found that researchers and practitioners are interested in different 
subject  categories,  as  shown  in  Figure  17.  23.65%  (206)  Researchers  are  publishing  in  ‘AI  As- 
sisted’, which is the third most popular category in terms of number of authors per category. In 
contrast,  only 1.15%  (68)  Developers  are  building  apps  and  actions  in  this  category. The  Per- 
centage Point (pp) difference is 22.5. We have similarly striking gaps between the percentage 
of authors and developers working in the ‘Code quality’ (21.8 pp more authors) and ‘Support’ 
(19.43 pp more authors) categories. 20 Categories out of 32 have a higher percentage (ranging 
from 0.54 pp to 22.51 pp higher) of authors compared to the percentage of developers working 
in those categories. On the other hand, only 1.72% (15) of the authors are working in the ‘Con- 
tinuous integration’ category compared to 34.23% (2,032) of the developers contributing to the 

 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

33 

Table  7  Top  features  in  each  category  (extracted  from  paper  titles  and  abstracts).  The  numbers  in 
parenthesis  stand  for  the  frequencies  of  each  feature  appearing  in  literature. 

Category 

Top features 

AI Assisted 

API  management 

Chat 

Code  quality 

Code  review 

Code 
ready 

scanning 

Code  search 

Community 

inte- 

Continuous 
gration 
Dependency  man- 
agement 

Deployment 

Desktop tools 

IDEs 

Learning 

Localization 

Mobile 
Mobile CI 

Monitoring 

Open  Source  man- 
agement 
Project  manage- 
ment 

Publishing 

Reporting 

Security 

Support 

Testing 

Time  tracking 

Utilities 

bug report (32), fault localization (13), traceability link (9), vulnerability detection 
(6), code generation (5) 
code search (5), natural-language query (5), api documentation (3), api specification 
(2), api library (2) 
developer discussion (4), issue report (4), bug fix (4), communication challenge (2), 
gain insight (2) 
technical  debt  (26),  bug  report  (24),  fault  localization  (12),  automate  refactoring 
(11), static analysis (7) 
technical  debt  (16),  program  comprehension  (14),  bug  report  (11),  trace  link  (10), 
code  analysis  (8) 
static analysis (6), dynamic analysis (6), defect identification (5), statistical debug 
(4), security vulnerability (4) 
technical  debt  (24),  bug  report  (14),  bug  localization  (10),  mutation  analysis  (8), 
natural-language  query  (5) 
open source (9), manage assignee (7), follow recommendation (6), developer discus- 
sion (4), issue report (4) 
CI build (9), CI specification (5), automate build (2), failure prediction  (2), model 
develop (2)) 
bug detection (9), traceability link (8), verification validation (7), co-change depen- 
dency (4), requirement traceability (2) 
cloud deployment (5), static analysis (4), deployment evaluation (4), code generation 
(3), mobile apps (3) 
uml  profile  (5),  case  tool  (3),  modelling  language  (2),  support  editor  (2),  embed 
system (2) 
program  comprehension  (13),  trace  link  (10),  link  creation  (5),  data  platform  (2), 
ide plug-in (2) 
program comprehension (12), open source (3), similarity measurement (2), developer 
study (2), knowledge acquisition (2) 
bug  detection  (9),  verification  validation  (7),  traceability  link  (6),  bug  track  (2), 
requirement  traceability (2) 
mobile apps (13), android apps (6), automate test (4), gui test (4), visual test (3) 
mobile apps (6), automate test (4) 
severity label (6), problem report (6), code-smell detection (5), crash reproduction 
(5), crash localization (3) 
issue report (9), mutation analysis (8), identify duplicate (5), bug fix (4), automate 
test (2) 
bug report (32), bug assignment (13), resolve merge-conflict (6), improve maintain- 
ability (5), support tool (4) 
CI  build  (9),  CI  specification  (5),  cloud  deployment  (5),  release  schedule  (4),  de- 
ployment evaluation (4) 
bug report (22), log change (9), tag recommendation  (5), program repair (4), tool 
support  (3) 
vulnerability detection (6), code analysis (5), static verification (3), refactoring edits 
(3), broken dependency (2) 
CI  build  (9),  bug  report  (8),  analysis  tool  (5),  bug  localization  (5),  code  analysis 
(4) 
test  suite  (34),  fault  localization  (33),  mutation  analysis  (13),  automate  test  (13), 
unit test (10) 
duplicate issue (8), documentation effort (5), issue report (5), prioritize documen- 
tation (3), review effort (3) 
trace link (10), static analysis (6), tool support (5), code search (5), code design (3) 

 
 
 
 
34 

Sk Golam Saroar et al. 

marketplace in this category. Glaring gaps such as this, where a higher percentage of developers 
are involved compared to researchers, also exist in other categories such as ‘Deployment’ (22.29 
pp more developers) and ‘Utilities’ (17.03 pp more developers). Categories with the least dif- 
ference between the proportion of researchers and developers are ‘API Management’ (0.54 pp 
more researchers), ‘IDEs’ (0.87 pp more researchers), ‘Mobile’ (0.91 pp more researchers), and 
‘Mobile CI’ (1.04 pp more developers). Figure 18 shows the percentage point difference between 
the contribution of researchers and practitioners for each category. For eight categories, there 
is  more  than  10  Percentage  Point  (PP)  difference  between  the  percentage  of  researchers 
and practitioners. 

Besides  having  a  developer,  most  actions  (90.63%)  in  the  marketplace  have  one  or  more 
contributors. 64.49% of these contributors are working in the ‘Continuous integration’ category 
compared  to  1.72%  of  the  authors.  Categories  ‘Utilities’  (39.7  pp),  ‘Deployment’  (38.71  pp), 
and  ‘Publishing’  (17.82  pp)  have  considerably  more  percentage  of  contributors  compared  to 
authors.  On  the  other  hand,  ‘AI  Assisted’  (22.36  pp),  ‘Support’  (18.87  pp),  and  ‘Code  search’ 
(13.34 pp) have a higher percentage of authors than contributors on the marketplace. 

Although  there  are  30  Action  categories  in  the  marketplace  (at  the  time  of  writing  this 
paper),  GitHub  actions  was  first  introduced  as  an  alternative  to  CI/CD  services  for  GitHub 

Table  8  Ranking  of  categories  based  on  the  number  of  apps,  actions,  and  papers  in  that  category 
(sorted by number of papers) 

ID  Type  Category 

#  Apps  #  Actions #  Papers  RankApps  RankActions  RankPapers 

Testing 

C19  Both  Code  quality 
C2  Both 
C15  Both  AI assisted 
Project  management 
C8  Both 
C7  Both 
Support 
C3  Both  Utilities 
C9  Both  Code  review 
C25  Both  Code  search 
C4  Both  Reporting 
C14  Both  Dependency  management 
C18  Both  Monitoring 
C12  Action  Community 
C16  Both  Open source  management 
Publishing 
C6  Both 
Localization 
C20  Both 
C26  Both  Code  scanning  ready 
Security 
C17  Both 
C27  Both 
Learning 
C10  Both  Deployment 
C1  Both  API  management 
C22  Both  Mobile 
C5  Both  Continuous integration 
C21  Both  Desktop  tools 
C11  Both  Chat 
C28  Both 
C23  Both 
C24  Both  Mobile CI 
C13  Both  Container CI 
C29  Action  Game CI 
C30  Both 
C31  App 
C32  App 

Backup Utilities 
Content Att. API 
GitHub created 

Time  tracking 
IDEs 

65 
26 
26 
80 
12 
118 
81 
5 
27 
20 
28 
N/A 
25 
18 
9 
3 
43 
16 
35 
15 
10 
76 
14 
12 
7 
10 
6 
5 
N/A 
2 
1 
4 

864 
662 
58 
507 
126 
2,388 
684 
37 
315 
401 
188 
155 
244 
1,058 
46 
7 
345 
66 
1800 
176 
135 
2,554 
20 
224 
33 
24 
91 
560 
3 
22 
N/A 
N/A 

97 
61 
54 
47 
47 
39 
37 
36 
27 
21 
20 
20 
19 
18 
13 
12 
11 
10 
8 
8 
7 
5 
5 
5 
4 
3 
1 
0 
0 
0 
0 
0 

5 
10 
11 
3 
19 
1 
2 
25 
9 
13 
8 
N/A 
12 
14 
22 
28 
6 
15 
7 
16 
20 
4 
17 
18 
23 
21 
24 
26 
N/A 
29 
30 
27 

5 
7 
22 
9 
19 
2 
6 
24 
12 
10 
15 
17 
13 
4 
23 
29 
11 
21 
3 
16 
18 
1 
28 
14 
25 
26 
20 
8 
30 
27 
N/A 
N/A 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 

 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

35 

repositories  [10].  For  this  reason,  it  comes  as  no  surprise  that  ‘Continuous  Integration’,  and 
‘Deployment’  are  two  of  the  top  three  categories  based  on  the  number  of  developers.  The 
other  category,  ‘Utilities’,  also  includes  marketplace  products  related  to  CI/CD.  These  three 
categories also pair frequently between them for the products in the marketplace (see section 
6.4).  This  results  in  a  lot  of  developers  being  involved  in  these  three  categories.  While  the 
practitioners are focusing on getting their products on the marketplace as quickly and seamlessly 
as possible, thanks to the abundance of CI/CD products, the researchers are concerned more 
about facilitating development with the help of AI assisted technologies, improved code quality, 
and support to both the development team and the end users. 

25% (eight out of 32) of the categories have more than 10 percentage point difference in the 
percentage of active individuals between researchers and practitioners in the open source 
community. The researchers are mostly active in ‘Code quality’ category while practitioners 
largely focus on ‘Continuous integration’. 

6.2  The Distribution of Papers and Marketplace Products Across Categories 

The  distribution  of  papers  and  marketplace  products  vary  greatly  across  categories.  For  ex- 
Category for actions and apps, respectively. 
ample, ‘Continuous Integration’ is the 
However, this category contains only five (1.71%) Papers and is ranked 
out of 27 Categories 
#4 
with at least one paper. On the other hand, 20.89% (61) Papers in our mapping study fall under 
the ‘Testing’ category, while only 5.92% (26) Apps and 8.44% (662) Actions on the marketplace 
fall under this category (see Table 8). 

and 

#22 

#1 

We calculated the percentage of the papers and the marketplace products (apps and actions) 
in  each  category  and  plotted  them  in  Figure  19.  In  particular,  there  are  11  Categories  with  a 
higher percentage of marketplace products compared to the percentage of papers. This differ- 
ence  is  more  than  15  Percentage  Point  (pp)  for  categories  ‘Continuous  integration’  (30.03 
pp),  ‘Deployment’  (19.41  pp),  and  ‘Utilities’  (16.89  pp).  In  contrast,  ‘Code  quality’  (22.01  pp), 
‘AI  Assisted’  (17.48  pp),  ‘Support’  (14.43),  ‘Testing’  (12.59  pp),  ‘Code  search’  (11.82  pp)  and 

C32  C1 

1 
0.333 
0.111 
0.037 

0.012 

0.004 
0.001 
0 

C31 

C30 

C29 

C28 

C27 

C 

26 

C 

25 

C 

24 

C23 

C22 

C21 

C2 

C3 

C4 

C5 

C6 

C7 

C8 

C9 

C10 

C11 

C12 

C13 

C14 

C15 

C20 

C19 

C18

C17  C16 

Fig.  17  Number  of  Authors  vs  Number  of  Developers  (after  normalizing)  per  marketplace  category 

Number of Authors 

Number of Developers 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
36 

Sk Golam Saroar et al. 

Testing (C2) 

Reporting (C4) 
Continuous Integration (C5) 

Support (C7) 

Code review (C9) 

Community (C12) 

Dependency Management (C14) 

Open source management (C16) 

Monitoring (C18) 

Localization  (C20) 

Mobile (C22) 
IDEs (C23) 

Code search (C25) 

s
e
i
r
o
g
e
t
a
C

API management (C1) 

Utilities (C3) 

Publishing (C6) 

Project management (C8) 

Chat (C11) 

Container CI (C13) 

Deployment (C10) 

AI Assisted (C15) 

Security (C17) 

Code quality (C19) 

Desktop tool (C21) 

Mobile CI (C24) 

Code scanning ready (C26) 

Learning (C27) 

Game CI (C29) 

Time tracking (C28) 

Backup utilities (C30) 

Content Attachments API (C31) 

-30 

-20 

-10 

0 

10 

20 

30 

40 

Percentage  point  difference 

Fig.  18  Difference  (in  percentage  point)  between  the  number  of  practitioners  and  the  number  of 
researchers working in each category. Red demonstrates a higher percentage of researchers working 
in a category, while Green bars indicate a higher percentage of practitioners. 

15 other categories have a higher percentage of mapped papers into these categories compared 
to the percentage of marketplace products. 

There  were  four22  categories  on  the  GitHub  Marketplace  for  which  we  did  not  find  any 
papers within our systematic mapping study, namely ‘Container CI’, ‘Game CI’, ‘Backup Util- 
ities’,  and  ‘Content  Attachments  API’  (see  Table  8).  Barring  these  four  categories,  ‘API  Man- 
agement’, ‘IDEs’, ‘Mobile’, ‘Mobile CI’, ‘Time tracking’, and ‘Security’ had the least percentage 
point  difference  (ranging  from  0.43  pp  to  0.91  pp)  between  the  proportion  of  the  papers  and 
the marketplace products (Figure 19). 

11 categories have a higher percentage of marketplace products compared to 20 categories 
with a higher percentage of mapped studies. In terms of percentage point difference, 
literature is lacking the most in ‘Continuous integration’ while the marketplace falls behind 
the most in ‘Code quality’ products. The difference is less that 1 pp for six categories and 
more than 10 pp for eight other categories. 

6.3  Popular Categories in Literature and in the Marketplace 

We discussed the interest of researchers and practitioners in developing and contributing to a 
subject  matter  in  the  previous  section.  Here,  we  compare  the  usage  and  popularity  of  these 
topics.  Figure  20  shows  a  bump  chart  that  compares  the  rankings  of  categories  based  on  the 
average number of paper citations, app installs, and action stars. The more popular categories 
in each column are shown on top while the least popular categories are on the bottom. 

22excluding  ‘GitHub  Created’  which  we  do  not  consider  for  the  gap  analysis  for  aforementioned 

reason 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

37 

e
g
a
t
n
e
c
r
e
P

35 

30 

25 

20 

15 

10 

5 

0 

T
e
s
t
i
n
g

U

t
i
l
i
t
i
e
s

R
e
p
o
r
t
i
n
g

C
o
n
t
.

i

n
t
e
g
r
a
t
i
o
n

P
u
b

l
i
s
h
n
g

i

S
u
p
p
o
r
t

P
r
o
j
e
c
t

m
a
n
a
g
e
m
e
n
t

C
h
a
t

C
o
d
e

r
e
v
i
e
w

D
e
p
l
o
y
m
e
n
t

C
o
m
m
u
n
i
t
y

C
o
n
t
a
i
n
e
r
C

I

A
I
A
s
s
i
s
t
e
d

D
e
p
e
n
d
e
n
c
y
m
g
m

t

O
p
e
n
S
o
u
r
c
e
m
g
m

t

I

D
E
s

M
o
b

i
l
e

S
e
c
u
r
i
t
y

M
o
n
i
t
o
r
i
n
g

C
o
d
e

q
u
a
l
i
t
y

L
o
c
a
l
i
z
a
t
i
o
n

D
e
s
k
t
o
p
T
o
o
l

Category 

M
o
b

i
l
e
C

I

i

T
m
e
t
r
a
c
k
n
g

i

C
o
d
e

s
e
a
r
c
h

L
e
a
r
n
n
g

i

C
o
d
e
S
c
a
n
n
n
g

i

r
e
a
d
y

G
a
m
e
C

I

B
a
c
k
u
p
U

t
i
l
i
t
i
e
s

C
o
n
t
e
n
t
A
t
t
a
c
h

.

A
P
I

Fig. 19  The percentage of the marketplace products (green) and papers (red) in each category. 

) compared to practitioners (

The category ‘Mobile CI’ displays the most striking gap in popularity between researchers 
and  practitioners.  It  is  ranked 2nd and 3rd in  terms  of  average  installs  and  stars,  respectively, 
while appearing at the 26th position in the average citation ranking. This is because we catego- 
rized only one paper in ‘Mobile CI’ compared to six apps and 91 Actions in this category in the 
marketplace.  ‘Time  tracking’  is  another  category  that  is  significantly  less  popular  among  re- 
searchers (
among the app users). 
‘Container CI’, ‘Desktop tools’, ‘Security’, and ‘Monitoring’ are other categories that are more 
favored by practitioners when compared to researchers. On the other hand, categories ‘Testing’, 
‘Code  search’,  ‘AI  Assisted’,  ‘Support’,  and  ‘IDEs’  are  more  popular  among  researchers  than 
both app and action users. The category ‘Game CI’ ranks 
(out of 31) in all three metrics, 
making it one of the least popular categories both in research and in practice. ‘Content Attach- 
ments API’ (
) are also unpopular among researchers and action 
users  although  the  categories  rank  9th  and  19th,  respectively,  in  terms  of  average  number  of 
installs. 

) and ‘Backup Utilities’ (

among action users and 

#30 

#23

#29

#31

#5 

#1 

Among categories that are more popular with practitioners, ‘Mobile CI’ shows the biggest 
gap compared to popularity with researchers. On the flip side, ‘Testing’ is more popular 
among researchers with the largest difference compared to popularity among practitioners. 
‘Game CI’ is equally less popular among both researchers and practitioners. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
38 

Sk Golam Saroar et al. 

6.4  Overlap Between Categories in Literature and in the Marketplace 

As  discussed  in  section  4.2.2,  apps  and  actions  in  the  marketplace  can  be  listed  in  more  than 
one category. We looked into how these categories pair up in the marketplace compared to our 
own categorization of the open-source software engineering literature. 

Most frequently, ‘Continuous integration’ and ‘Deployment’ appear together in 7.51% (622 
out of 8,285) Products in the marketplace, followed by ‘Continuous integration’ and ‘Utilities’ 
which are shared by 5.65% (468 out of 8,285) Products. However, in literature we found no paper 
that  falls  under  both  ‘Continuous  integration’  and  ‘Deployment’.  In  other  words,  while  there 
are  products  in  the  marketplace  that  are  offering  automation  both  for  continuous  integration 
and deployment, research is lacking at the intersection of these two fields. On the other hand, 
‘Code quality’ and ‘Code search’ are most frequently shared by studies in literature, appearing 
together  in  8.22%  (24  out  of  292)  Papers  (see  Figure  16).  In  contrast,  there  is  no  app  in  the 
marketplace that falls under both of these categories, and only 0.04% Actions share these two 
categories. 

Figure  21  shows  the  top  three  intersections  of  categories  in  the  marketplace  and  in  the 
literature.  The  categories  ‘Continuous  integration’,  ‘Utilities’,  and  ‘Deployment’  make  up  the 
top  three  pairs  of  categories  shared  among  the  marketplace  products.  These  three  categories 
are  also  the  top  three  categories  based  on  the  number  of  actions  per  category.  Similarly,  the 
three  categories  that  feature  in  the  most  frequently  shared  categories  in  the  literature-  ‘Code 
quality’,  ‘Code  review’,  and  ‘Code  search’  are  ranked 
,  respectively,  based  on  the 
number of papers per category (see Table 8). 

, 

, 

#1

#7

#8

Fig.  20  Ranking  based  on  the  average  number  of  paper  citations,  app  installs,  and  action  stars  per 
category 

 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

39 

Cont.  integration 

Code  quality 

1540 

622 

468  

1646 

238 

1129 

53 

20 

24 

6 

11 

1 

Deployment 

Code  search 

Utilities 

Code  review 

(a) 

(b) 

Fig.  21  The intersection between categories are not equal in the state of the art and state of practice. 
The left Venn diagram (green) shows the largest overlaps between the marketplace categories (RQ1). 
The right diagram (red) shows the biggest overlaps between the mapped studies (RQ2). 

The overlap between categories vary greatly in the marketplace and in literature. ‘Continuous 
integration’ and ‘Deployment’ has the largest intersection in marketplace (appearing together 
622 Times) while never overlapping in literature. In contrast, ‘Code quality’ and ‘Code 
search‘ overlap frequently in literature (24 Times) but scarcely in the marketplace (3 Times). 

6.5  Qualitative Comparison of Automation Tools in Research and Practice 

The marketplace products under the ‘AI Assisted’ category help with 
(3 
Mentions in product descriptions) while the studies mapped under this category discuss the 
(5). Code enhancement tasks include improving code readability, fix- 
task of 
ing bugs in code, adding code comments, etc. On the other hand, code generation helps with 
producing situation-aware and business aware applications or creating software for embedded 
systems, among other use cases. For ‘API Management’, both literature and marketplace prod- 
ucts assist with 
. The marketplace offers prod- 
and 
ucts under ‘Dependency management’ that aid developers in finding and resolving 

code  enhancement 

code  generation 

(11) as well as 

API  documentation 

API  specification

(6). Similarly, literature in this category propose 
dependency 

and 

unit  test 

code  scanning 

(6) techniques, they both help with 

(4) compared to papers that propose 

tools that help with 
conflict 
The features 

updating  dependency 
co-changing  dependency 
test  automation 

(4). 
are common among both literature and mar- 
ketplace products that fall under the ‘Testing’ category. While ‘Code Scanning ready’ has prod- 
(6) 
ucts that facilitate 
and 
. 
Products  and  papers  under  the  ‘Publishing’  category  help  developers  with  their  software  re- 
checking security-vulnerability
(266), 
leases. While the marketplace offers apps and actions that facilitate 
(94), the literature discusses more effi- 
. 
. As for the ‘Code 
cloud  deployment
pull requests (6), 
static  analysis
(64),  and  a  variety  of  other  use-cases.  Literature,  on  the 
automatically  merging 
(11), 

cient 
generating  release-notes 
‘Code quality’ also has products and papers that help with 
release  scheduling 
review’ category, the marketplace has products for 

(178), and 
(4). Both products and papers offer tools for 

dynamic analysis 

release  creation 

static  analysis 

release  tags 

(14), 

(8), 

(5), 

other hand, discuss 
test  coverage 
etc under this category. 

code  linting 

For  ‘Community’,  the  marketplace  offers  products  for  automatic 

code  analysis 

program  comprehension 

bug  reporting 
and 

on  issues.  On  the  other  hand,  the  literature  discusses  using  issue  reports  (4)  to 
bot 

issue  creation 

comments 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
40 

Sk Golam Saroar et al. 

issue reports, or to 

. For example, Di Sorbo et al. [43] an- 
alyzed over 6,000 Issue reports in 279 GitHub projects to investigate the nature of ‘wontfix 
identify  duplicate 
label  issues
issues’ and experimented with predicting ‘wontfix issues’ based on the title and description of 
issue reports. In the ‘Learning’ category, marketplace helps with 

and 

, in comparison to literature which helps with 

learning  skills 

(2). In the 
sharing 

‘Monitoring’ category, there are tools in the marketplace that help with 
knowledge
or 

. The mapped studies focus more on 
(5). For ‘Security’, marketplace has tools for 

knowledge  acquisition 

(3) and 
crash  reporting 
, and 

sending  notification

while the literature also discusses 

(6). 

crash  reproduction 
fixing  vulnerability 
vulnerability 
test,  release,  monitoring,  and  deployment  of  projects  for 

detecting 
‘Mobile CI’ tools on the marketplace are CI tools that are particularly automating the build, 
, 
, 
projects.  The  marketplace  also  offers  actions 

code  analysis 

, 

, 

crash  localization 

(5) and 
security  analysis

, 
for  integration  with  different  platforms  such  as 
Apache  Cordova
Xamarin 

Octodroid

Ionic

,  or 

, 

as  well  as  integration  to  deploy,  label,  and  upload  signed  releases  to  the 

Appknox 
Visual  Studio
. We mapped only one paper [42] under 
store or building and publishing 
security 
GooglePlay 
this category which discusses automated tests for mobile apps. The authors found that tests for 
connectivity, GUI, sensors, and multiple configurations were scarce and there was no correlation 
between automated tests and app popularity. 

Android 

files on 

GitHub

APK 

Android

emulator, 

iOS

Flutter

,  or 

Firebase

‘Game CI’ includes ‘tools for building CI pipeline for game development’23. Actions in this 
category are mostly focused on package management and automation to run builds using dif- 
ferent criteria or targets. Several of these actions are integrating 
build engine commands 
in these processes. In addition, a few of the actions offer notifications on the new releases or set 
up  static data products  that host  game  assets. As for  ‘Container  CI’,  the  GitHub  Marketplace 
offers tools for creating, retrieving, and registering container images across multiple platforms. 
The actions in this category help with automating tasks such as pushing and running the builds 
or  code  files  and  setting  up  stream  processing  platforms  such  as 
as  well  as  starting  databases.  Other  than  that,  actions  under  ‘Continuous  integration’  offer 
Cordova 
integration of different tools such as 
into GitHub repositories 
, 
to  facilitate  developers’  activities.  We  did  not  find  any  paper  directly  related  to  either  ‘Game 
Pipenv 
CI’ or ‘Container CI’ within our systematically retrieved list of studies. 

Apache  Kafka 

Maven  CLI

Armvirt

Unity 

, and 

and 

The ‘Time tracking’ category includes apps to extract metadata from developers’ activities 
automatically and use different metrics to monitor or manage projects. The monitoring related 
apps provide reports on developers’ time spent on different projects, using different program- 
ming languages, IDEs, workflows, issues, pull requests, commit statuses, and also offer a variety 
of  visualizations  or  to-do  lists.  ‘Time  tracking’  tools  intersecting  with  ‘Project  management’ 
tools are mostly intended to gather billable hours of software developers by tracking the time 
spent across different desktop and web applications. The actions in this category are being used 
to block activities such as continuous integration during particular times, posting reminders for 
different to-do activities, updating README files of projects using stats, providing insight to 
developers’ productivity, automatic decision making on abandoned tasks (such as closing issues 
which  have  not  been  addressed  in  a  specified  time  period),  and  sending  reports  to  different 
tools and mediums. Studies in this category discuss topics such as using git hours to get effort 
estimations  for  projects  based  on  git  hours  per  commit,  the  evolution  of  projects  and  arti- 
facts, supporting global software developments by converting time and showing productivity of 
developers in mornings or nights, automatic tools for adding files changed in pull requests, etc. 

23

https://github.com/marketplace/category/game-ci 

 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

41 

  7 Threats  to  Validity 

This  study  attempts  to  obtain  a  picture  of  the  current  state  of  research  and  practice  in  the 
software engineering community. We have designed measures to elaborate on the state of the 
art  and  practice  and  the  mobilization  gap.  These  measures,  however,  have  some  threats  to 
validity that we discuss below. We refer to the validity types introduced by Wohlin et al. [50] 
to structure these limitations. 

As  for  the  conclusion  validity,  the  threat  of  drawing  wrong  conclusions  for  RQ1  and  RQ2 
are  low  as  the  nature  of  questions  are  descriptive.  We  used  conventional  statistical  testing 
and visualizations to summarize our observations. We used categories as the main unit for our 
side by side comparison. As we used the marketplace categories and their definitions for our 
mapping study, we expect these mapping do not pose a huge validity threat. However, the area 
of  categorization  is  fuzzy  and  one  contribution  (either  the  papers  or  apps  and  actions)  can 
be categorized in multiple areas. To provide the readers with further insight, we provided an 
analysis of the intersection between the categories in Section 6.4. 

When it comes to our diagnostic evaluation of the gap between academia and the industry 
(RQ3),  there  are  a  number  of  potential  threats.  We  used  the  number  of  papers  and  authors, 
as well as the number of apps, actions and their developers as a proxy for the extent of interest 
from  the  community.  However,  these  measures  might  not  be  an  accurate  proxy.  Moreover, 
as  time  passes,  the  number  of  citations,  in  particular,  can  be  affected  extensively.  Similarly, 
the  popularity  (or  the  perceived  value  by  the  user)  is  inaccurate  and  only  acts  as  a  proxy. 
To  mitigate  the  risk,  we  used  the  average  number  of  citations  among  all  the  papers  and  the 
average number of installs and stars for apps and actions within each category. These proxies 
and comparisons might pose a threat on the validity of the conclusions we have drawn. 

As per the construct validity, the exclusion criteria applied to the systematic mapping may 
have caused us to ignore some relevant papers in the field. For instance, we collected the papers 
for  our  mapping  study  from  the  top  20  venues  listed  in  Google  Scholar’s  ranking.  This  may 
have excluded some relevant papers from our study. Yet, we believe the search was extensive 
enough  to  cover  important  aspects  for  our  comparison  purpose.  We  also  used  the  categories 
on GitHub Marketplace to categorize the 365 Papers from our dataset. However, the academic 
and industry terminology are not consistent. For instance, in the marketplace, ‘Localization’ is 
to  ‘Extend  your  software’s  reach.  Localize  and  translate  continuously  from  GitHub.‘24.  While 
in academia, localization often refers to bug localization. Thus, using just the category name to 
categorize the research papers and then mapping them to apps and actions on the marketplace 
could be error-prone. We also rely on the developers to choose appropriate categories for their 
products on GitHub. However, there could be developers having different perceptions of the 
categories,  making  this  subjective  to  the  developers’  perception.  To  mitigate  this  risk,  before 
mapping  the  studies  into  the  categories  in  RQ2,  all  the  annotators  familiarized  themselves 
with the official category definitions in the marketplace. They looked into samples of apps and 
actions within a category and discussed within the group to have similar understandings of the 
category definitions. 

Further, we used NLP techniques on app and action descriptions to generate word clouds for 
each category (a sample of word clouds for the ‘Localization’ and ‘Utilities’ categories are shown 
in Figure 22). While this was not good enough for allowing us to automate the categorization 
process, it gave us a better idea as to what the categories meant in the context of the GitHub 
Marketplace. 

24

https://github.com/marketplace?category=localization&query=&type=&verification= 

 
 
 
42 

Sk Golam Saroar et al. 

(a) 

(b) 

Fig.  22  Word  clouds  for  the  categories  (a)  Localization  and  (b)  Utilities  generated  using  full  descrip- 
tions of apps in the categories 

When  it  comes  to  the  internal  validity,  in  RQ2,  we  categorized  the  papers  manually.  Al- 
though  the  process  was  independent,  after  it  was  done,  we  discussed  the  disagreements  and 
used a mediator as to solicit an external opinion and adjusted the categories. Still, there could 
be bias and misunderstanding that threat the internal validity. It would be ideal to find ways 
to automate the categorization process, which would make this work free from unintentional 
research  bias.  However,  as  we  followed  the  established  protocols  of  the  empirical  study  and 
systematic mappings, we expect these biases are minimal. 

We  also  used a  combination of  an  automatic  and  a  manual  process  to  identify  individual 
researchers.  We  used  a  combination  of  names,  surnames, and  affiliations  to  identify  the  indi- 
vidual  authors  in  the  field.  We  followed  the  process  by  manually  inspecting  scholar  profiles 
and LinkedIn accounts. Yet, there is a small chance that some individuals were not identified 
properly and a few redundancies exist. 

As  per  the  external  validity,  for  the  academic  studies  (RQ2),  the  publication  data  and 
citations over the time was available. However, the marketplace does not provide date for apps 
and actions over the time (neither in terms of the publication time nor their popularity status 
over the time). As a result, we could not perform a time based similar analysis or any analysis 
about  evolution  of  categories  over  time.  This  lack  of  historical  data  makes  it  hard  to  predict 
upcoming trends in the field. Hence, it is hard to comment on the generalization of the study 
beyond the scope of this empirical investigation. We expect that in the long run new categories 
would be introduced or be eliminated from the marketplace. 

8 Discussion and Implications 

To summarize, this paper involves three main contributions. 

First,  we  performed  the  very  first  study  on  analyzing  the  GitHub  Marketplace.  The  study  of 
marketplaces have been of interest in the software engineering community since the emergence 
of  mobile  app  stores.  While  there  has  been  intense  ongoing  research  and  case  studies  in 
the domain of open source software engineering, the marketplace, which is the platform for 
officially  sharing  automated  tools  for  the  open  source  ecosystem,  has  never  been  studied. 
By analyzing the marketplace, we believe the research community can invest in mining this 
repository to gain more insight into different interactions and requirements within the open 
source software community. 

Second,  we presented the results of a systematic mapping study of the literature in automation 
within the open source community. While there are multiple studies in the context of open 

 
 
 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

43 

source [25][15][9], ours particularly provides a mapping to the perspective as seen officially by 
GitHub and the open source developers. 

Third,  we performed a comprehensive comparison between our findings from mining the mar- 
ketplace and performing the mapping study. This comparison of state of the art and state of 
the practice demonstrates and quantifies the gap. 

GitHub  Marketplace  is  a  software  repository  that  has  not  been  mined  and  presented  in 
the software engineering research community. The marketplace is forming a two sided platform 
(similar to the mobile app stores) where it provides visibility to both app developers and their 
users [37],[35, 38]. The marketplace provides a combination of qualitative and quantitative data 
that can be of interest for many in the community. Most interestingly, in contrast to the mobile 
app  stores,  not  only  are  the  providers  software  developers  but  also  the  users  are  in  software 
teams and are developers as well. 

While this study retrieves information from a new perspective, it should be noted that the 
community has known and acknowledged the gap between academia and industry. Lo et al. [27] 
gathered  571  Papers  from  ICSE  and  ESEC/FSE  venues  and  asked  practitioners  to  annotate 
their relevancy. The results showed that for 71% of all ratings, the publications were considered 
essential or worthwhile while emphasizing on the room to improve relevance and actionability of 
the research. This study later was replicated by French et al. [14]. On the other hand, Devanbu 
et al. [11] invested and demonstrated that developers often hold strong a priori opinions about 
several  technical  aspects  which  are  not  well  supported  by  evidence.  Such  beliefs  formed  by 
subjective understanding and personal experience are error-prone. In a follow up article [12], 
they echoed the concern of many researchers, “Our life’s work, embodied in research papers, 
counted so little toward forming the opinions of professional practitioners at one of the world’s 
leading software companies?”. Further, a more recent study by Shrikanth et al. [41] pointed to 
the disconnection between beliefs of practitioners and what we achieve with empirical research. 
The authors advise practitioners and researchers to routinely evaluate their beliefs. 

To address the gap, multiple efforts were placed to address the "right question". Buse and 
Zimmermann [5] empirically evaluated the diverse information needed from stakeholders. Bagel 
and Zimmermann [4] performed a holistic survey with developers at Microsoft to understand 
the questions of interest in software teams and listed 145 Questions that could be of interest to 
the community. Nayebi et al. [35] evaluated the requirements from release engineers for mobile 
app analysis. These efforts are often employing surveys to understand developers’ perceptions. 
Fernandez  et  al.  [13]  performed  a  large-scale  survey  with  practitioners  to  identify  the  pain 
points of software requirements engineering. These are only a few samples of such surveys to 
identify the studies relevant to real-world practices. The community’s emphasis on performing 
relevant research for practitioners and lack of access to data from industrial players, open source 
community, and projects have been the subject of many studies in the field. The research has 
been  looking  into  the  open  source  software  hosted  on  GitHub.  Tools  such  as  GHTotrrent25 
have been designed to facilitate research in the field. We have been hopeful that working on 
open  source  has  at  least  gotten  us  closer  to  real-world  issues.  While  many  of  the  studies  start 
with  a  descriptive  analysis  of  a  sample  of  open  source  repositories,  researchers  have  not  directly 
evaluated  the  relevance  of  research  in  open  source.  Our  analysis  of  RQ3  does  indeed  show  the 
gap between the researchers’ effort and the practitioners’ work in the open source community. 
We  believe  that  the  marketplace  has  the  potential  to  help  knowledge  mobilization  if  we 
focus our efforts in that direction. The analysis and mining of the GitHub Marketplace (such as 
the  one  done  in  our  study)  can  provide  researchers  with  access  to  a  diverse  set  of  perceptions. 
A deep look into the developers’ ways of automating their tasks on repositories and observing 

25

https://ghtorrent.org 

 
 
 
44 

Sk Golam Saroar et al. 

their actions can further reduce the bias of questioning and increase our leverage in narrowing 
the  gap.  The  conference  tool  tracks  can  encourage  authors  who  are  researching  in  open  source 
to  provide  their  tools  and  techniques  as  an  action  within  the  GitHub  Marketplace.  We  think 
this  is  a  straightforward  and  compatible  way  to  make  a  real-world  impact  and  move  above 
silos  of  research,  each  performing  a  case  study  on  a  random  set  of  open  source  repositories. 
Further, actions being released on the marketplace can also address the issue with replicability 
and reproducibility of software engineering research. 

9 Conclusion 

GitHub  Marketplace  is  a  software  repository  that  provides  tools  for  automation  based  on 
GitHub  events.  We  mined  and  analyzed  the  apps  and  actions  on  the  GitHub  Marketplace 
and mapped our findings with literature in open source software engineering. The marketplace 
provides a plethora of information around the tools provided on this platform. These include 
developers’,  platform’s,  and  users’  usage  data  on  the  tools.  We  also  presented  a  systematic 
mapping  study  on  literature  about  automation  in  open  source.  We  mapped  365  Papers  into 
the  categories  from  the  marketplace  and  summarized  our  findings.  We  found  that  the  litera- 
ture does not cover the automation tools in the GitHub Marketplace very well. Although some 
of  the  automation  topics  in  literature  are  widely  used  in  practice,  they  have  not  evolved  in 
a  direction  to  be  aligned  with  the  state  of  practice.  We  quantified  and  highlighted  the  gaps 
helping  researchers  and  practitioners  to identify  opportunities  and  synergies  to  work  toward 
reducing the gap. There is significant research work on topics such as ‘Testing’, ‘Code quality’ 
and ‘Continuous integration’ while practitioners are still not making full use of them. 

By introducing this software repository, we enabled the community to take further steps in 
understanding  and  analyzing  different  automation  tools. The  summary  of  the  state  of the  art 
helps the researchers to build on top of the existing work while practitioners can navigate the 
technology and find the synergies. We hope this leads to more vigorous knowledge mobilization 
within software teams. 

References 

1.  A. A. Al-Subaihin, F. Sarro, S. Black, L. Capra, and M. Harman.  App store effects on software 

engineering practices.  IEEE Transactions on Software Engineering, 47(2):300–319, 2021. 

2.  A.  A.  Al-Subaihin,  F.  Sarro,  S.  Black,  L.  Capra,  M.  Harman,  Y.  Jia,  and  Y.  Zhang.  Clustering 
mobile  apps  based  on  mined  textual  features.  In  Proceedings  of  the  10th  ACM/IEEE  International 
Symposium  on  Empirical  Software  Engineering  and  Measurement,  ESEM  ’16,  New  York,  NY,  USA, 
2016. Association for Computing Machinery. 

3.  M. Ali, M. E. Joorabchi, and A. Mesbah.  Same app, different app stores: A comparative study. 
In  2017  IEEE/ACM  4th  International  Conference  on  Mobile  Software  Engineering  and  Systems 
(MOBILESoft), pages 79–90, 2017. 

4.  A. Begel and T. Zimmermann. Analyze this! 145 questions for data scientists in software engineering. 
In  Proceedings  of  the  36th  International  Conference  on  Software  Engineering,  pages  12–23,  2014. 
5.  R. P. Buse and T. Zimmermann. Information needs for software development analytics. In 2012 

34th  International  Conference  on  Software  Engineering  (ICSE), pages 987–996. IEEE, 2012. 

6.  S. A. Carr, F. Logozzo, and M. Payer. Automatic contract insertion with ccbot. IEEE Transactions 

on Software Engineering, 43(8):701–714, 2016. 

7.  L. V. G. Carreño and K. Winbladh. Analysis of user comments: An approach for software require- 
ments  evolution.  In  2013  35th  International  Conference  on  Software  Engineering  (ICSE),  pages 
582–591, 2013. 

 
 
 
 
 
 
 
 
GitHub Marketplace for Practitioners and Researchers to Date 

45 

8.  N.  Chen, J.  Lin,  S.  C.  H.  Hoi,  X. Xiao,  and  B. Zhang.  Ar-miner:  Mining  informative  reviews 
for developers from mobile app marketplace. In Proceedings of the 36th International Conference 
on Software Engineering, ICSE 2014, page 767–778, New York, NY, USA, 2014. Association for 
Computing Machinery. 

9.  V. Cosentino, J. L. C. Izquierdo, and J. Cabot. A systematic mapping study of software development 

with github.  IEEE Access, 5:7173–7192, 2017. 

10.  A.  Decan,  T.  Mens,  P.  R.  Mazrae,  and  M.  Golzadeh.  On  the  Use  of  GitHub  Actions  in  Software 

Development Repositories, June 2022. 

11.  P. Devanbu, T. Zimmermann, and C. Bird. Belief & evidence in empirical software engineering. In 
2016  IEEE/ACM  38th  International  Conference  on  Software  Engineering  (ICSE),  pages  108–119. 
IEEE, 2016. 

12.  P. Devanbu, T. Zimmermann, and C. Bird. Belief and evidence: How software engineers form their 

opinions.  IEEE Software, 35(06):72–76, 2018. 

13.  D.  M.  Fernández,  S.  Wagner,  M.  Kalinowski,  M.  Felderer,  P.  Mafra,  A.  Vetrò,  T.  Conte,  M.- 
T.  Christiansson,  D.  Greer,  C.  Lassenius,  et  al.  Naming  the  pain  in  requirements  engineering. 
Empirical software  engineering, 22(5):2298–2338, 2017. 

14.  X. Franch, D. M. Fernández, M. Oriol, A. Vogelsang, R. Heldal, E. Knauss, G. H. Travassos, J. C. 
Carver, O. Dieste, and T. Zimmermann. How do practitioners perceive the relevance of requirements 
engineering  research?  an  ongoing  study.  In  2017 IEEE 25th International Requirements Engineering 
Conference  (RE),  pages  382–387.  IEEE,  2017. 

15.  O.  Franco-Bedoya,  D.  Ameller,  D.  Costal,  and  X.  Franch.  Open  source  software  ecosystems:  A 

systematic  mapping.  Information  and  software  technology,  91:160–185,  2017. 

16.  M. Harman, Y. Jia, and Y. Zhang. App store mining and analysis: Msr for app stores. In 2012 9th 

IEEE Working Conference on Mining Software Repositories (MSR), pages 108–111, 2012. 

17.  Z.  Hu  and  E.  Gehringer.  Use  bots  to  improve  github  pull-request  feedback.  In  Proceedings of the 
50th  ACM  Technical  Symposium  on  Computer  Science  Education,  SIGCSE  ’19,  page  1262–1263, 
New York, NY, USA, 2019. Association for Computing Machinery. 

18.  Y.  Jo  and  A.  H.  Oh.  Aspect  and  sentiment  unification  model  for  online  review  analysis.  In 
Proceedings of the Fourth ACM International Conference on Web Search and Data Mining,  WSDM 
’11, page 815–824, New York, NY, USA, 2011. Association for Computing Machinery. 

19.  R. Kallis, A. Di Sorbo, G. Canfora, and S. Panichella. Ticket tagger: Machine learning driven issue 
classification.  In  2019  IEEE  International  Conference  on  Software  Maintenance  and  Evolution 
(ICSME), pages 406–409, 2019. 

20.  R.  Kallis,  A.  Di  Sorbo,  G.  Canfora,  and  S.  Panichella.  Predicting  issue  types  on  github.  Science of 

Computer Programming, 205:102598, 12 2020. 

21.  H. Khalid. On identifying user complaints of ios apps. In 2013 35th International Conference on 

Software Engineering (ICSE), pages 1474–1476, 2013. 

22.  H. Khalid, E. Shihab, M. Nagappan, and A. E. Hassan.  What do mobile app users complain about? 

IEEE  software,  32(3):70–77,  2014. 

23.  T.  Kinsman,  M.  Wessel,  M.  A.  Gerosa,  and  C.  Treude.  How  do  software  developers  use  github 

actions to automate their workflows?, 2021. 

24.  J.  Krüger, N.  Corr,  I. Schröter, and  T. Leich.  Digging into the  eclipse  marketplace.  pages 60–65, 

04 2017. 

25.  V. Lenarduzzi, D. Taibi, D. Tosi, L. Lavazza, and S. Morasca. Open source software evaluation, 
selection,  and  adoption:  a  systematic  literature  review.  In  2020 46th Euromicro Conference on 
Software Engineering and Advanced Applications (SEAA), pages 437–444. IEEE, 2020. 

26.  J. Li, S. Beba, and M. M. Karlsen.  Evaluation of open-source ide plugins for detecting security 
vulnerabilities.  Proceedings of the Evaluation and Assessment on Software Engineering, 2019. 
27.  D. Lo, N. Nagappan, and T. Zimmermann.  How practitioners perceive the relevance of software 
engineering  research.  In  Proceedings  of  the  2015  10th  Joint  Meeting  on  Foundations  of  Software 
Engineering, pages 415–425, 2015. 

28.  W.  Maalej,  M.  Nayebi,  T.  Johann,  and  G.  Ruhe.  Toward  data-driven  requirements  engineering. 

IEEE  Software,  33(1):48–54,  2016. 

29.  W. Martin, F. Sarro, Y. Jia, Y. Zhang, and M. Harman. A survey of app store analysis for software 

engineering.  IEEE Transactions on Software Engineering, 43(9):817–847, 2017. 

30.  R.  Mehrotra  and  B.  Carterette.  Recommendations  in  a  marketplace.  In  Proceedings  of  the  13th 
ACM  Conference  on  Recommender  Systems,  RecSys  ’19,  page  580–581,  New  York,  NY,  USA,  2019. 
Association for Computing Machinery. 

31.  S. Mirhosseini and C. Parnin.  Can automated pull requests encourage software developers to up- 
grade  out-of-date  dependencies?  In  2017  32nd  IEEE/ACM  International  Conference  on  Automated 
Software  Engineering  (ASE), pages 84–94, 2017. 

 
 
46 

Sk Golam Saroar et al. 

32.  I.  J.  Mojica  Ruiz,  M.  Nagappan,  B.  Adams,  T.  Berger,  S.  Dienst,  and  A.  E.  Hassan.  Examining 

the rating system used in mobile-app stores.  IEEE  Software, 33(6):86–92, 2016. 

33.  M. Monperrus.  Explainable software bot contributions: Case study of automated bug fixes.  In 
2019  IEEE/ACM  1st  International  Workshop  on  Bots  in  Software  Engineering  (BotSE).  IEEE, 
may 2019. 

34.  M.  Nayebi,  B.  Adams, and  G.  Ruhe.  Release  practices  for  mobile apps –  what  do  users  and 
developers  think?  In  2016  IEEE  23rd  International  Conference  on  Software  Analysis,  Evolution, 
and Reengineering (SANER), volume 1, pages 552–562, 2016. 

35.  M. Nayebi, B. Adams, and G. Ruhe. Release practices for mobile apps–what do users and developers 
think?  In  2016 ieee 23rd international conference on software analysis, evolution, and reengineering 
(saner), volume 1, pages 552–562. IEEE, 2016. 

36.  M. Nayebi, H. Cho, and G. Ruhe. App store mining is not enough for app improvement. Empirical 

Software Engineering, 23, 10 2018. 

37.  M.  Nayebi,  H.  Farahi,  and  G.  Ruhe.  Which  version  should  be  released  to  app  store?  In 
2017  ACM/IEEE  International  Symposium  on  Empirical  Software  Engineering  and  Measurement 
(ESEM), pages 324–333. IEEE, 2017. 

38.  M. Nayebi, H. Farrahi, and G. Ruhe. Analysis of marketed versus not-marketed mobile app releases. 
In Proceedings  of  the  4th  International  Workshop  on  Release  Engineering, pages 1–4, 2016. 
39.  M. Nayebi and G. Ruhe. An open innovation approach in support of product release decisions. 
In  Proceedings  of  the  7th  International  Workshop  on  Cooperative  and  Human  Aspects  of  Software 
Engineering, pages 64–71, 2014. 

40.  K. Petersen, S. Vakkalanka, and L. Kuzniarz. Guidelines for conducting systematic mapping studies 

in  software  engineering:  An  update.  Information  and  software  technology,  64:1–18,  2015. 

41.  N. Shrikanth, W. Nichols, F. M. Fahid, and T. Menzies. Assessing practitioner beliefs about software 

engineering.  Empirical  Software  Engineering,  26(4):1–32,  2021. 

42.  D. B. Silva, M. M. Eler, V. H. Durelli, and A. T. Endo.  Characterizing mobile apps from a source 

and  test  code  viewpoint.  Information  and  Software  Technology,  101:32–50,  2018. 

43.  A.  D.  Sorbo,  J.  Spillner,  G.  Canfora,  and  S.  Panichella.  "won’t  we  fix  this  issue?"  qualitative 
characterization and automated identification of wontfix issues on github. CoRR, abs/1904.02414, 
2019. 

44.  I. Souza, L. Campello, E. Rodrigues, G. Guedes, and M. Bernardino.  An Analysis of Automated 
Code  Inspection  Tools  for  PHP  Available  on  Github  Marketplace,  page  10–17.  Association  for 
Computing Machinery, New York, NY, USA, 2021. 

45.  M.-A. Storey and A. Zagalsky. Disrupting developer productivity one bot at a time. In Proceedings 
of  the  2016  24th  ACM  SIGSOFT  International  Symposium  on  Foundations  of  Software  Engineer- 
ing, FSE 2016, page 928–931, New York, NY, USA, 2016. Association for Computing Machinery. 
46.  Y. Tian, M. Nagappan, D. Lo, and A. E. Hassan. What are the characteristics of high-rated apps? 
a  case  study  on  free  android  applications.  In  2015  IEEE  international  conference  on  software 
maintenance and evolution (ICSME), pages 301–310. IEEE, 2015. 

47.  B. Vasilescu, Y. Yu, H. Wang, P. Devanbu, and V. Filkov.  Quality and productivity outcomes 
relating  to  continuous  integration  in  github.  In  Proceedings  of  the  2015  10th  joint  meeting  on 
foundations of software engineering, pages 805–816, 2015. 

48.  M. Wessel, B. M. de Souza, I. Steinmacher, I. S. Wiese, I. Polato, A. P. Chaves, and M. A. Gerosa. 
The power of bots: Characterizing and understanding bots in oss projects.  Proc. ACM Hum.- 
Comput. Interact., 2(CSCW), nov 2018. 

49.  M.  Wessel  and  I.  Steinmacher.  The  Inconvenient  Side  of  Software  Bots  on  Pull  Requests,  page 

51–55.  Association for Computing Machinery, New York, NY, USA, 2020. 

50.  C.  Wohlin,  P.  Runeson,  M.  Höst,  M.  C.  Ohlsson,  B.  Regnell,  and  A.  Wesslén.  Experimentation in 

software engineering.  Springer Science & Business Media, 2012. 

51.  M. Wyrich and J. Bogner. Towards an autonomous bot for automatic source code refactoring. In 
2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering (BotSE),  pages  24–
28, 2019. 

 
 
GitHub Marketplace for Practitioners and Researchers to Date 

47 

Appendix I 

[P1]  Robles,  G.,  Gonzalez-Barahona,  J.  M.,  Merelo,  J.  J.  (2006).  Beyond  source  code:  the  importance 
of  other  artifacts  in  software  development  (a  case  study).  Journal  of  Systems  and  Software,  79(9), 
1233-1248. 

[P2]  Souter, Amie L., and Lori L. Pollock. ‘Characterization and automatic identification of type infea- 

sible call chains.’ Information and Software Technology 44, no. 13 (2002): 721-732. 

[P3]  Tolosa,  J.  B.,  Sanjuán-Martínez,  O.,  García-Díaz,  V.,  G-Bustelo,  B.  C.  P.,  Lovelle,  J.  M.  C. 
(2011). Towards the systematic measurement of ATL transformation models. Software: Practice and 
Experience, 41(7), 789-815. 

[P4]  Dalton,  A.  R.,  Hallstrom,  J.  O.  (2009).  nAIT:  A  source  analysis  and  instrumentation  framework 

for nesC. Journal of Systems and Software, 82(7), 1057-1072. 

[P5]  Uhl,  A.  (2008).  Model-driven  development  in  the  enterprise.  IEEE  software,  25(1),  46-49. 
[P6]  Lago,  P.,  Muccini,  H.,  Van  Vliet,  H.  (2009).  A  scoped  approach  to  traceability  management. 

Journal of Systems and Software, 82(1), 168-182. 

[P7]  Strasunskas,  D.,  Hakkarainen,  S.  E.  (2012).  Domain  model-driven  software  engineering:  A  method 

for  discovery  of  dependency  links.  Information  and  Software  Technology,  54(11),  1239-1249. 

[P8] Lizcano, D., Alonso, F., Soriano, J.,  López, G. (2015). Automated end user-centred adaptation 
of web components through automated description logic-based reasoning. Information and Software 
Technology, 57, 446-462. 

[P9]  Idri,  A.,  azzahra  Amazal,  F.,  Abran,  A.  (2015).  Analogy-based  software  development  effort  esti- 

mation: A systematic mapping and review. Information and Software Technology, 58, 206-230. 

[P10]  Patel,  P.,  Cassou,  D.  (2015).  Enabling  high-level  application  development  for  the  Internet  of 

Things.  Journal  of  Systems  and  Software,  103,  62-84. 

[P11]  Kitchenham,  B.  A.,  Brereton,  P.,  Turner,  M.,  Niazi,  M.  K.,  Linkman,  S.,  Pretorius,  R.,  Budgen, 
D.  (2010).  Refining the  systematic literature review process—two participant-observer case studies. 
Empirical Software Engineering, 15(6), 618-653. 

[P12]  Kitchenham,  B.,  Pretorius,  R.,  Budgen,  D.,  Brereton,  O.  P.,  Turner,  M.,  Niazi,  M.,    Linkman, 
S.  (2010).  Systematic  literature  reviews  in  software  engineering–a  tertiary  study.  Information  and 
software technology, 52(8), 792-805. 

[P13]  Gallardo,  M.  M.,  Merino,  P.,  Panizo,  L.,  Linares,  A.  (2011).  A  practical  use  of  model  checking 
for synthesis: generating a dam controller for flood management. Software: Practice and Experience, 
41(11), 1329-1347. 

[P14]  Smith,  B.  H.,  Williams,  L.  (2009).  On  guiding  the  augmentation  of  an  automated  test  suite  via 

mutation analysis. Empirical software engineering, 14(3), 341-369. 

[P15]  Calinescu,  R.,  Grunske,  L.,  Kwiatkowska,  M.,  Mirandola,  R.,  Tamburrelli,  G.  (2010).  Dynamic 
QoS  management and optimization in  service-based systems. IEEE Transactions on Software  Engi- 
neering, 37(3), 387-409. 

[P16]  Wang,  X.,  Zhang,  L.,  Xie,  T.,  Mei,  H.,  Sun,  J.  (2012).  Locating  need-to-externalize  constant 
strings for software internationalization with generalized string-taint analysis. IEEE Transactions on 
Software Engineering, 39(4), 516-536. 

[P17]  De  Caso,  G.,  Braberman,  V.,  Garbervetsky,  D.,  Uchitel,  S.  (2010).  Automated  abstractions  for 

contract validation. IEEE Transactions on Software Engineering, 38(1), 141-162. 

[P18]  Kasoju,  A.,  Petersen,  K.,  Mäntylä,  M.  V.  (2013).  Analyzing  an  automotive  testing  process  with 

evidence-based software engineering. Information and Software Technology, 55(7), 1237-1259. 

[P19]  Kosar, T., Bohra, S.,  Mernik, M. (2016). Domain-specific languages: A systematic mapping study. 

Information and Software Technology, 71, 77-91. 

[P20]  Giuffrida, R.,  Dittrich, Y. (2013). Empirical studies on the use of social software in global software 
development–A systematic mapping study. Information and Software Technology, 55(7), 1143-1164. 
[P21]  Winter, J., Rönkkö, K.,  Rissanen, M. (2014). Identifying organizational barriers—a case study of 
usability work when developing software in the automation industry. Journal of Systems and Software, 
88, 54-73. 

[P22]  White, J., Doughtery, B.,  Schmidt, D. C. (2010). Ascent: An algorithmic technique for designing 

hardware and software in tandem. IEEE Transactions on Software Engineering, 36(6), 838-851. 

[P23]  Preuveneers,  D.,  Heyman,  T.,  Berbers,  Y.,  Joosen,  W.  (2016).  Systematic  scalability  assessment 

for feature oriented multi-tenant services. Journal of Systems and Software, 116, 162-176. 

[P24]  Quirchmayr, T., Paech, B., Kohl, R., Karey, H.,  Kasdepke, G. (2018). Semi-automatic rule-based 
domain terminology and software feature-relevant information extraction from natural language user 
manuals. Empirical Software Engineering, 23(6), 3630-3683. 

[P25] Zhang, J., Wang, Y.,  Xie, T. (2019). Software feature refinement prioritization based on online 

user review mining. Information and Software Technology, 108, 30-34. 

 
 
 
48 

Sk Golam Saroar et al. 

[P26]  de  Magalhães,  C.  V.,  da  Silva,  F.  Q.,  Santos,  R.  E.,  Suassuna,  M.  (2015).  Investigations  about 
replication of empirical studies in software engineering: A systematic mapping study. Information and 
Software Technology, 64, 76-101. 

[P27]  Colanzi,  T.  E.,  Vergilio,  S.  R.,  Assuncao,  W.  K.  G.,  Pozo,  A.  (2013).  Search  based  software 
engineering: Review and analysis of the field in Brazil. Journal of Systems and Software, 86(4), 970- 
984. 

[P28]  Grano,  G.,  Palomba,  F.,  Di  Nucci,  D.,  De  Lucia,  A.,  Gall,  H.  C.  (2019).  Scented  since  the 
beginning: On the diffuseness of test smells in automatically generated test code. Journal of Systems 
and Software, 156, 312-327. 

[P29]  Kim, M., Notkin, D., Grossman, D.,  Wilson, G. (2012). Identifying and summarizing systematic 

code changes via rule inference. IEEE Transactions on Software Engineering, 39(1), 45-62. 
[P30]  Faragardi, H. R., Lisper, B., Sandström, K.,  Nolte, T. (2018). A resource efficient framework to 
run automotive embedded software on multi-core ECUs. Journal of Systems and Software, 139, 64-83. 
[P31]  Švogor, I.,  Crnković,  I.,  Vrček,  N.  (2019).  An  extensible  framework  for  software  configuration 
optimization  on  heterogeneous  computing  systems:  Time  and  energy  case  study.  Information  and 

software technology, 105, 30-42. 

[P32]  Zavala, E., Franch, X.,  Marco, J. (2019). Adaptive monitoring: A systematic mapping. Information 

and software technology, 105, 161-189. 

[P33]  Gacitúa,  R.,  Sepúlveda,  S.,  Mazo,  R.  (2019).  FM-CF:  A  framework  for  classifying  feature model 

building approaches. Journal of Systems and Software, 154, 1-21. 

[P34]  Agh,  H.,  Ramsin,  R.  (2016).  A  pattern-based  model-driven  approach  for  situational  method 

engineering. Information and Software Technology, 78, 95-120. 

[P35]  Mohamad,  M.,  Liebel,  G.,  Knauss,  E.  (2017).  LoCo  CoCo:  Automatically  constructing  coordi- 
nation  and  communication  networks  from  model-based  systems  engineering  data.  Information  and 
Software Technology, 92, 179-193. 

[P36]  Falessi,  D.,  Di  Penta,  M.,  Canfora,  G.,  Cantone,  G.  (2017).  Estimating  the  number  of  remaining 

links in traceability recovery. Empirical Software Engineering, 22(3), 996-1027. 

[P37]  Ståhl, D., Mårtensson, T.,  Bosch, J. (2017). The continuity of continuous integration: Correlations 

and consequences. Journal of Systems and Software, 127, 150-167. 

[P38]  Borg,  M.,  Wnuk,  K.,  Regnell,  B.,  Runeson,  P.  (2016).  Supporting  change  impact  analysis  using  a 
recommendation  system:  An  industrial  case  study  in  a  safety-critical  context.  IEEE  Transactions  on 
Software Engineering, 43(7), 675-700. 

[P39]  Barua, A., Thomas, S. W.,  Hassan, A. E. (2014). What are developers talking about? an analysis 

of topics and trends in stack overflow. Empirical Software Engineering, 19(3), 619-654. 

[P40]  Jagroep, E., van der Ent, A., van der Werf, J. M. E., Hage, J., Blom, L., van Vliet, R.,  Brinkkem- 
per, S. (2018). The hunt for the guzzler: Architecture-based energy profiling using stubs. Information 
and Software Technology, 95, 165-176. 

[P41]  Ali,  N.  B.,  Usman,  M.  (2018).  Reliability  of  search  in  systematic  reviews:  Towards  a  quality 
assessment framework for the automated-search strategy. Information and Software Technology, 99, 
133-147. 

[P42]  Osvaldo  Jr,  S.  S.,  Lopes,  D.,  Silva,  A.  C.,  Abdelouahab,  Z.  (2017).  Developing  software  systems 
to  Big  Data  platform  based  on  MapReduce  model:  An  approach  based  on  Model  Driven  Engineering. 
Information and Software Technology, 92, 30-48. 

[P43]  Da  Silva,  F.  Q.,  Santos,  A.  L.,  Soares,  S.,  França,  A.  C.  C.,  Monteiro,  C.  V.,  Maciel,  F.  F. 
(2011). Six years of systematic literature reviews in software engineering: An updated tertiary study. 
Information and Software Technology, 53(9), 899-913. 

[P44]  Li, H., Shang, W., Zou, Y.,  E Hassan, A. (2017). Towards just-in-time suggestions for log changes. 

Empirical Software Engineering, 22(4), 1831-1865. 

[P45]  Ebert, C., Heidrich, J., Martínez-Fernández, S.,  Trendowicz, A. (2019). Data science: technologies 

for better software. IEEE software, 36(6), 66-72. 

[P46]  Sharafi,  Z.,  Soh,  Z.,  Guéhéneuc,  Y.  G.  (2015).  A  systematic  literature  review  on  the  usage  of 

eye-tracking in software engineering. Information and Software Technology, 67, 79-107. 

[P47]  Jiang, J., Lo, D., Ma, X., Feng, F.,  Zhang, L. (2017). Understanding inactive yet available assignees 

in GitHub. Information and Software Technology, 91, 44-55. 

[P48]  Xiong, Y., Zhang, H., Hubaux, A., She, S., Wang, J.,  Czarnecki, K. (2014). Range fixes: Interactive 
error resolution for software configuration. Ieee transactions on software engineering, 41(6), 603-619. 

[P49]  Hamilton,  M.  H. (2018).  What  the  Errors Tell  Us.  IEEE  Software,  35(5),  32-37. 
[P50]  Martens,  D.,  Maalej,  W.  (2019).  Towards  understanding  and  detecting  fake  reviews  in  app  stores. 

Empirical  Software  Engineering,  24(6),  3316-3355. 

[P51]  Hübner, P.,  Paech, B. (2020). Interaction-based creation and maintenance of continuously usable 
trace links between requirements and source code. Empirical Software Engineering, 25(5), 4350-4377. 

 
 
GitHub Marketplace for Practitioners and Researchers to Date 

49 

[P52]  Carbonnel,  J.,  Huchard,  M.,  Nebut,  C.  (2019).  Modelling  equivalence  classes  of  feature  models 
with  concept  lattices  to  assist  their  extraction  from  product  descriptions.  Journal  of  Systems  and 
Software, 152, 1-23. 

[P53]  Barricelli,  B.  R.,  Cassano,  F.,  Fogli,  D.,  Piccinno,  A.  (2019).  End-user  development,  end-user 
programming and end-user software engineering: A systematic mapping study. Journal of Systems 
and Software, 149, 101-137. 

[P54]  Ciolek,  D.,  Braberman,  V.,  D’Ippolito,  N.,  Piterman,  N.,  Uchitel,  S.  (2016).  Interaction  mod- 
els  and  automated  control  under  partial  observable  environments.  IEEE  Transactions  on  Software 
Engineering, 43(1), 19-33. 

[P55]  Caivano,  D.,  Fogli,  D.,  Lanzilotti,  R.,  Piccinno,  A.,  Cassano,  F.  (2018).  Supporting  end  users  to 
control their smart home: design implications from a literature review and an empirical investigation. 
Journal of Systems and Software, 144, 295-313. 

[P56]  Tawosi,  V.,  Jalili,  S.,  Hasheminejad,  S.  M.  H.  (2015).  Automated  software  design  using  ant  colony 

optimization  with  semantic  network  support.  Journal  of  Systems  and  Software,  109,  1-17. 

[P57]  Rakha, M. S., Shang, W.,  Hassan, A. E. (2016). Studying the needed effort for identifying dupli- 

cate issues. Empirical Software Engineering, 21(5), 1960-1989. 

[P58]  de Oliveira Neto, F. G., Torkar, R., Feldt, R., Gren, L., Furia, C. A.,  Huang, Z. (2019). Evolution 
of  statistical  analysis  in  empirical  software  engineering  research:  Current  state  and  steps  forward. 
Journal of Systems and Software, 156, 246-267. 

[P59]  Sharma,  S.,  Sodhi,  B.  (2019).  Using  Stack  Overflow  content  to  assist  in  code  review.  Software: 

Practice and Experience, 49(8), 1255-1277. 

[P60]  Drave,  I.,  Hillemacher,  S.,  Greifenberg,  T.,  Kriebel,  S.,  Kusmenko,  E.,  Markthaler,  M.,  ...  Wort- 
mann, A. (2019). SMArDT modeling for automotive software testing. Software: Practice and Experi- 
ence, 49(2), 301-328. 

[P61]  Islam, M. R.,  Zibran, M. F. (2018). SentiStrength-SE: Exploiting domain specificity for improved 

sentiment analysis in software engineering text. Journal of Systems and Software, 145, 125-146. 

[P62]  Cornu,  B.,  Barr,  E.  T.,  Seinturier,  L.,  Monperrus,  M.  (2016).  Casper:  Automatic  tracking  of  null 

dereferences to inception with causality traces. Journal of Systems and Software, 122, 52-62. 

[P63]  Haghighatkhah,  A.,  Banijamali,  A.,  Pakanen,  O.  P.,  Oivo,  M.,  Kuvaja,  P.  (2017).  Automotive 

software engineering: A systematic mapping study. Journal of Systems and Software, 128, 25-55. 
[P64]  Zhang,  F.,  Niu,  H.,  Keivanloo,  I.,  Zou,  Y.  (2017).  Expanding  queries  for  code  search  using 
semantically related api class-names. IEEE Transactions on Software Engineering, 44(11), 1070-1082. 
[P65]  Bao, L., Li, J., Xing, Z., Wang, X., Xia, X.,  Zhou, B. (2017). Extracting and analyzing time-series 

HCI data from screen-captured task videos. Empirical Software Engineering, 22(1), 134-174. 

[P66]  Nayebi,  M.,  Ruhe,  G.,  Zimmermann,  T.  (2019).  Mining  treatment-outcome  constructs  from  se- 

quential software engineering data. IEEE Transactions on Software Engineering, 47(2), 393-411. 

[P67]  Haghighatkhah, A., Oivo, M., Banijamali, A.,  Kuvaja, P. (2017). Improving the state of automo- 

tive software engineering. IEEE Software, 34(5), 82-86. 

[P68]  Li,  H.,  Shang,  W.,  Hassan,  A.  E.  (2017).  Which  log  level  should  developers  choose  for  a  new 

logging  statement?.  Empirical  Software  Engineering,  22(4),  1684-1716. 

[P69]  Zou, W., Lo, D., Chen, Z., Xia, X., Feng, Y.,  Xu, B. (2018). How practitioners perceive automated 

bug report management techniques. IEEE Transactions on Software Engineering, 46(8), 836-862. 

[P70]  Mahmoud,  A.,  Bradshaw,  G. (2017).  Semantic topic  models  for source code analysis.  Empirical 

Software Engineering, 22(4), 1965-2000. 

[P71]  Arcuri,  A.  (2018).  An  experience  report  on  applying  software  testing  academic  results  in  industry: 

we  need  usable  automated  test  generation.  Empirical  Software  Engineering,  23(4),  1959-1981. 

[P72]  Jiang,  J.,  Lo,  D.,  He,  J.,  Xia,  X.,  Kochhar,  P.  S.,  Zhang,  L.  (2017). Why  and  how  developers fork 

what from whom in GitHub. Empirical Software Engineering, 22(1), 547-578. 

[P73]  Gousios,  G.,  Spinellis,  D.  (2014).  Conducting  quantitative  software  engineering  studies  with 

Alitheia Core. Empirical Software Engineering, 19(4), 885-925. 

[P74]  Huang,  Q.,  Xia,  X.,  Lo,  D.,  Murphy,  G.  C.  (2018).  Automating  intention  mining.  IEEE  Transac- 

tions on Software Engineering, 46(10), 1098-1119. 

[P75]  AlOmar,  E.  A.,  Mkaouer,  M.  W.,  Ouni,  A.  (2021).  Toward  the  automatic  classification  of  self- 

affirmed refactoring. Journal of Systems and Software, 171, 110821. 

[P76]  Silva,  D.  B.,  Eler,  M.  M.,  Durelli,  V.  H.,  Endo,  A.  T.  (2018).  Characterizing  mobile  apps  from  a 

source and test code viewpoint. Information and Software Technology, 101, 32-50. 

[P77]  LaToza,  T.  D.,  Di  Lecce,  A.,  Ricci,  F.,  Towne,  W.  B.,  Van  der  Hoek,  A.  (2018).  Microtask 

programming. IEEE Transactions on Software Engineering, 45(11), 1106-1124. 

[P78]  de  Oliveira,  M.  C.,  Freitas,  D.,  Bonifácio,  R.,  Pinto,  G.,  Lo,  D.  (2019).  Finding  needles  in  a 
haystack:  Leveraging  co-change  dependencies  to  recommend  refactorings.  Journal  of  Systems  and 
Software, 158, 110420. 

 
 
50 

Sk Golam Saroar et al. 

[P79]  Cruz,  L.,  Abreu,  R.,  Lo,  D.  (2019).  To  the  attention  of  mobile  software  developers:  guess  what, 

test your app!. Empirical Software Engineering, 24(4), 2438-2468. 

[P80]  Nelson, N., Brindescu, C., McKee, S., Sarma, A.,  Dig, D. (2019). The life-cycle of merge conflicts: 

processes, barriers, and strategies. Empirical Software Engineering, 24(5), 2863-2906. 

[P81]  Zhou, P., Liu, J., Liu, X., Yang, Z.,  Grundy, J. (2019). Is deep learning better than traditional ap- 
proaches  in  tag  recommendation  for  software  information  sites?.  Information  and  software  technology, 
109, 1-13. 

[P82]  Zanjani, M. B., Kagdi, H.,  Bird, C. (2015). Automatically recommending peer reviewers in modern 

code review. IEEE Transactions on Software Engineering, 42(6), 530-543. 

[P83]  Munir,  H.,  Linåker,  J.,  Wnuk,  K.,  Runeson,  P.,  Regnell,  B.  (2018).  Open  innovation  using  open 

source tools: A case study at Sony Mobile. Empirical Software Engineering, 23(1), 186-223. 

[P84]  Sbai,  N.,  Lenarduzzi,  V.,  Taibi,  D.,  Sassi,  S.  B.,  Ghezala,  H.  H.  B.  (2018).  Exploring  information 
from  OSS  repositories  and  platforms  to  support  OSS  selection  decisions.  Information  and  Software 
Technology, 104, 104-108. 

[P85]  Franco-Bedoya, O., Ameller, D., Costal, D.,  Franch, X. (2017). Open source software ecosystems: 

A Systematic mapping. Information and software technology, 91, 160-185. 

[P86]  Graaf,  B.,  Lormans,  M.,  Toetenel,  H.  (2003).  Embedded  software  engineering:  the  state  of  the 

practice. IEEE software, 20(6), 61-69. 

[P87]  Krall, J., Menzies, T.,  Davies, M. (2015). Gale: Geometric active learning for search-based software 

engineering. IEEE Transactions on Software Engineering, 41(10), 1001-1018. 

[P88]  Di Sorbo, A., Canfora, G.,  Panichella, S. (2019). ’ Won’t We Fix this Issue?’ Qualitative Character- 
ization and Automated Identification of Wontfix Issues on GitHub. arXiv preprint arXiv:1904.02414. 
[P89]  Zhang,  M.,  Li,  Y.,  Li,  X.,  Chen,  L.,  Zhang,  Y.,  Zhang,  L.,  Khurshid,  S.  (2019).  An  empirical 
study  of  boosting  spectrum-based  fault  localization  via  pagerank.  IEEE  Transactions  on  Software 

Engineering, 47(6), 1089-1113. 

[P90]  Soremekun, E., Kirschner, L., Böhme, M.,  Zeller, A. (2021). Locating faults with program slicing: 

an empirical analysis. Empirical Software Engineering, 26(3), 1-45. 

[P91]  Kazerouni,  A.  M.,  Davis,  J.  C.,  Basak,  A.,  Shaffer,  C.  A.,  Servant,  F.,  Edwards,  S.  H.  (2021). 
Fast and accurate incremental feedback for students’ software tests using selective mutation analysis. 
Journal of Systems and Software, 175, 110905. 

[P92]  Ghosh,  D.,  Singh,  J.  (2021).  Spectrum-based  multi-fault  localization  using  Chaotic  Genetic  Al- 

gorithm. Information and Software Technology, 133, 106512. 

[P93]  Almhana,  R.,  Kessentini,  M.,  Mkaouer,  W.  (2021).  Method-level  bug  localization  using  hybrid 

multi-objective search. Information and Software Technology, 131, 106474. 

[P94]  Tamburri,  D.  A.,  Palomba,  F.,  Kazman,  R.  (2019).  Exploring  community  smells  in  open-source: 

An automated approach. IEEE Transactions on software Engineering, 47(3), 630-652. 

[P95]  Xiaobo, Y., Bin,  L.,  Shihai,  W. (2021).  A Test Restoration  Method  based on  Genetic  Algorithm  for 
effective  fault  localization  in  multiple-fault  programs.  Journal  of  Systems  and  Software,  172,  110861. 
[P96]  Panichella,  A.  (2021).  A  Systematic  Comparison  of  search-Based  approaches  for  LDA  hyperpa- 

rameter tuning. Information and Software Technology, 130, 106411. 

[P97]  Ye,  H.,  Martinez,  M.,  Durieux,  T.,  Monperrus,  M.  (2021).  A  comprehensive  study  of  automatic 

program repair on the QuixBugs benchmark. Journal of Systems and Software, 171, 110825. 

[P98]  Saidani,  I.,  Ouni,  A.,  Chouchen,  M.,  Mkaouer,  M.  W.  (2020).  Predicting  continuous  integration 

build failures using evolutionary search. Information and Software Technology, 128, 106392. 

[P99]  Panichella,  S.,  Zaugg,  N.  (2020).  An  empirical  investigation  of  relevant  changes  and  automation 

needs  in  modern  code  review.  Empirical  Software  Engineering,  25(6),  4833-4872. 

[P100]  Wei,  R.,  Zolotas,  A.,  Hoyos  Rodriguez,  H.,  Gerasimou,  S.,  Kolovos,  D.  S.,  Paige,  R.  F.  (2020). 
Automatic generation of UML profile graphical editors for Papyrus. Software and Systems Modeling, 
19(5), 1083-1106. 

[P101]  Alizadeh,  V.,  Kessentini,  M.,  Mkaouer,  M.  W.,  Ocinneide,  M.,  Ouni,  A.,  Cai,  Y.  (2018).  An  in- 
teractive  and  dynamic  search-based  approach  to  software  refactoring  recommendations.  IEEE  Trans- 
actions  on  Software  Engineering,  46(9),  932-961. 

[P102]  Zhang, Y., Jin, D., Xing, Y.,  Gong, Y. (2020). Automated defect identification via path analysis- 

based features with transfer learning. Journal of Systems and Software, 166, 110585. 

[P103]  Koyuncu, A., Liu, K., Bissyandé, T. F., Kim, D., Klein, J., Monperrus, M.,  Le Traon, Y. (2020). 
Fixminer: Mining relevant fix patterns for automated program repair. Empirical Software Engineering, 
25(3), 1980-2024. 

[P104]  de Freitas Farias, M. A., de Mendonça Neto, M. G., Kalinowski, M.,  Spínola, R. O. (2020). Iden- 
tifying self-admitted technical debt through code comment analysis with a contextualized vocabulary. 
Information and Software Technology, 121, 106270. 

 
 
GitHub Marketplace for Practitioners and Researchers to Date 

51 

[P105]  Chen,  D.,  Zhang,  R.  (2020).  Catla-HS:  An  Open  Source  Project  for  Tuning  and  Analyzing 

MapReduce Performance on Hadoop and Spark. IEEE Software. 

[P106]  Janke,  M.,  Mader,  P.  (2020).  Graph  Based  Mining  of  Code  Change  Patterns  from  Version 

Control Commits. IEEE Transactions on Software Engineering. 

[P107]  Guizzo,  G.,  Sarro,  F.,  Krinke, J.,  Vergilio,  S.  R.  (2020).  Sentinel:  A  hyper-heuristic for  the 

generation of mutant reduction strategies. IEEE Transactions on Software Engineering. 

[P108]  Zolotas, A., Hoyos Rodriguez, H., Hutchesson, S., Sanchez Pina, B., Grigg, A., Li, M., ...  Paige, 
R.  F.  (2020).  Bridging  proprietary  modelling  and  open-source  model  management  tools:  the  case  of 
PTC  integrity  modeller  and  epsilon.  Software  and  Systems  Modeling,  19(1),  17-38. 

[P109]  Zeng,  Y.,  Chen,  J.,  Shang,  W.,  Chen,  T.  H.  P.  (2019).  Studying  the  characteristics  of  logging 
practices in mobile apps: a case study on f-droid. Empirical Software Engineering, 24(6), 3394-3434. 
[P110]  Avelino,  G.,  Passos,  L.,  Petrillo,  F.,  Valente,  M.  T.  (2018).  Who  Can  Maintain  This  Code?: 
Assessing the Effectiveness of Repository-Mining Techniques for Identifying Software Maintainers. 
IEEE Software, 36(6), 34-42. 

[P111]  Mohan, M.,  Greer, D. (2019). Using a many-objective approach to investigate automated refac- 

toring. Information and Software Technology, 112, 83-101. 

[P112]  Chen,  H.,  Huang,  Y.,  Liu,  Z.,  Chen,  X.,  Zhou,  F.,  Luo,  X.  (2019).  Automatically  detecting  the 

scopes of source code comments. Journal of Systems and Software, 153, 45-63. 

[P113]  Tamburri,  D.  A.,  Palomba,  F.,  Serebrenik,  A.,  Zaidman,  A.  (2019).  Discovering  community 
patterns in open-source: a systematic approach and its evaluation. Empirical Software Engineering, 
24(3), 1369-1417. 

[P114]  Xie,  X.,  Chen,  B.,  Zou,  L.,  Liu,  Y.,  Le,  W.,  Li,  X.  (2017).  Automatic  loop  summarization  via 

path dependency analysis. IEEE Transactions on Software Engineering, 45(6), 537-557. 

[P115]  Rahman,  A.,  Mahdavi-Hezaveh,  R.,  Williams,  L.  (2019).  A  systematic  mapping  study  of  infras- 

tructure  as  code  research.  Information  and  Software  Technology,  108,  65-77. 

[P116]  Choetkiertikul,  M.,  Dam,  H.  K.,  Tran,  T.,  Ghose,  A.,  Grundy,  J.  (2017).  Predicting  delivery 
capability in iterative software development. IEEE Transactions on Software Engineering, 44(6), 551- 
573. 

[P117]  Li,  Y.,  Zhu,  C.,  Rubin,  J.,  Chechik,  M.  (2017).  Semantic  slicing  of  software  version  histories. 

IEEE Transactions on Software Engineering, 44(2), 182-201. 

[P118]  Koo,  H.  M.,  Ko,  I.  Y.  (2017).  Construction  and  utilization  of  problem-solving  knowledge  in  open 

source  software  environments.  Journal  of  Systems  and  Software,  131,  402-418. 

[P119]  Kapitsaki,  G.  M.,  Kramer,  F.,  Tselikas,  N.  D.  (2017).  Automating  the  license  compatibility 

process in open source software with SPDX. Journal of systems and software, 131, 386-401. 

[P120]  Morales,  R.,  Soh,  Z.,  Khomh,  F.,  Antoniol,  G.,  Chicano,  F.  (2017).  On  the  use  of  developers’ 
context  for  automatic  refactoring  of  software  anti-patterns.  Journal  of  systems  and  software,  128, 
236-251. 

[P121]  Choetkiertikul, M.,  Dam, H. K., Tran, T.,  Ghose,  A.  (2017). Predicting the delay of issues with 

due dates in software projects. Empirical Software Engineering, 22(3), 1223-1263. 

[P122]  Beller,  M.,  Zaidman,  A.,  Karpov,  A.,  Zwaan,  R.  A.  (2017).  The  last  line  effect  explained. 

Empirical Software Engineering, 22(3), 1508-1536. 

[P123]  Xiao,  L.,  Yu,  Z.,  Chen,  B.,  Wang,  X.  (2017).  How  robust  is  your  development  team?.  IEEE 

Software, 35(1), 64-71. 

[P124]  Tian,  Y.,  Ali,  N.,  Lo,  D.,  Hassan,  A.  E.  (2016).  On  the  unreliability  of  bug  severity  data. 

Empirical Software Engineering, 21(6), 2298-2323. 

[P125]  Mohan, M., Greer, D.,  McMullan, P. (2016). Technical debt reduction using search based auto- 

mated refactoring. Journal of Systems and Software, 120, 183-194. 

[P126]  Fu,  W.,  Menzies,  T.,  Shen,  X.  (2016).  Tuning  for  software  analytics:  Is  it  really  necessary?. 

Information and Software Technology, 76, 135-146. 

[P127]  Jonsson,  L.,  Borg,  M.,  Broman,  D.,  Sandahl,  K.,  Eldh,  S.,  Runeson,  P.  (2016).  Automated  bug 
assignment: Ensemble-based machine learning in large scale industrial contexts. Empirical Software 
Engineering, 21(4), 1533-1578. 

[P128]  Amalfitano, D., Fasolino, A. R., Tramontana, P., Ta, B. D.,  Memon, A. M. (2014). MobiGUITAR: 

Automated model-based testing of mobile apps. IEEE software, 32(5), 53-59. 

[P129]  Ouni,  A.,  Kessentini,  M.,  Sahraoui,  H.,  Inoue,  K.,  Hamdi,  M.  S.  (2015).  Improving  multi- 
objective  code-smells  correction  using  development  history.  Journal  of  Systems  and  Software,  105, 
18-39. 

[P130]  Fraser,  G.,  Arcuri,  A.  (2015).  1600  faults  in  100  projects:  automatically  finding  faults  while 

achieving high coverage with evosuite. Empirical software engineering, 20(3), 611-639. 

[P131]  Iqbal, M. Z., Arcuri, A.,  Briand, L. (2015). Environment modeling and simulation for automated 

testing of soft real-time embedded software. Software  Systems Modeling, 14(1), 483-524. 

 
 
52 

Sk Golam Saroar et al. 

[P132]  Cornu, B., Seinturier, L.,  Monperrus, M. (2015). Exception handling analysis and transformation 
using fault injection: Study of resilience against unanticipated exceptions. Information and Software 
Technology, 57, 66-76. 

[P133]  Insa,  D.,  Silva,  J.  (2015).  Automatic  transformation  of  iterative  loops  into  recursive  methods. 

Information and Software Technology, 58, 95-109. 

[P134]  Schall,  D.  (2014).  Who  to  follow  recommendation  in  large-scale  online  development  communities. 

Information and Software Technology, 56(12), 1543-1555. 

[P135]  Alam, O., Adams, B.,  Hassan, A. E. (2012). Preserving knowledge in software projects. Journal 

of systems and software, 85(10), 2318-2330. 

[P136]  Shahmehri,  N.,  Mammar,  A.,  De  Oca,  E.  M.,  Byers,  D.,  Cavalli,  A.,  Ardi,  S.,  Jimenez,  W. 
(2012). An advanced approach for modeling and detecting software vulnerabilities. Information and 
Software Technology, 54(9), 997-1013. 

[P137]  Woungang,  I.,  Akinladejo,  F.  O.,  White,  D.  W.,    Obaidat,  M.  S.  (2012).  Coding-error  based 
defects  in  enterprise  resource  planning  software:  Prevention,  discovery,  elimination  and  mitigation. 
Journal  of  systems  and  software,  85(7),  1682-1698. 

[P138] Zhang, L., Ma, X., Lu, J., Xie, T., Tillmann, N., De Halleux, P. (2011). Environmental modeling 

for automated cloud application testing. IEEE software, 29(2), 30-35. 

[P139]  Deissenboeck,  F.,  Juergens,  E.,  Hummel,  B.,  Wagner,  S.,  y  Parareda,  B.  M.,  Pizka,  M.  (2008). 

Tool support for continuous quality control. IEEE software, 25(5), 60-67. 

[P140]  Xing,  Z.,  Stroulia,  E.  (2007).  API-evolution  support  with  Diff-CatchUp.  IEEE  Transactions  on 

Software Engineering, 33(12), 818-836. 

[P141]  Poshyvanyk, D., Gueheneuc, Y. G., Marcus, A., Antoniol, G.,  Rajlich, V. (2007). Feature location 
using probabilistic ranking of methods based on execution scenarios and information retrieval. IEEE 
Transactions on Software Engineering, 33(6), 420-432. 

[P142]  Demsky,  B.,  Rinard, M.  C.  (2006). Goal-directed  reasoning for  specification-based data  structure 

repair. IEEE Transactions on Software Engineering, 32(12), 931-951. 

[P143]  Di  Penta,  M.,  Neteler,  M.,  Antoniol,  G.,  Merlo,  E.  (2005).  A  language-independent  software 

renovation  framework.  Journal  of  Systems  and  Software,  77(3),  225-240. 

[P144]  Tsantalis, N., Chatzigeorgiou, A.,  Stephanides, G. (2005). Predicting the probability of change 

in object-oriented systems. IEEE Transactions on Software Engineering, 31(7), 601-614. 

[P145]  Williams, C. C.,  Hollingsworth, J. K. (2005). Automatic mining of source code repositories to 

improve bug finding techniques. IEEE Transactions on Software Engineering, 31(6), 466-480. 

[P146]  McLaughlin, L. (2004). Automated bug tracking: the promise and the pitfalls. IEEE Software, 

21(1), 100-103. 

[P147]  Liu,  H.,  Liu,  Q.,  Niu,  Z.,  Liu,  Y.  (2015).  Dynamic  and  automatic  feedback-based  threshold 

adaptation for code smell detection. IEEE Transactions on Software Engineering, 42(6), 544-558. 

[P148]  Dashevskyi, S., Brucker, A. D.,  Massacci, F. (2018). A screening test for disclosed vulnerabilities 

in foss components. IEEE Transactions on Software Engineering, 45(10), 945-966. 

[P149]  Galster,  M.,  Weyns,  D.,  Tofan,  D.,  Michalik,  B.,  Avgeriou,  P.  (2013).  Variability  in  software 
systems—a systematic literature review. IEEE Transactions on Software Engineering, 40(3), 282-306. 
[P150]  Moha,  N.,  Guéhéneuc,  Y.  G.,  Duchien,  L.,  Le  Meur,  A.  F.  (2009).  Decor:  A  method  for  the 
specification  and  detection  of  code  and  design  smells.  IEEE  Transactions  on  Software  Engineering, 

36(1), 20-36. 

[P151]  Liu,  H.,  Guo,  X.,  Shao,  W.  (2013).  Monitor-based  instant  software  refactoring.  IEEE  Transac- 

tions on Software Engineering, 39(8), 1112-1126. 

[P152]  McBurney, P. W., Jiang, S., Kessentini, M., Kraft, N. A., Armaly, A., Mkaouer, M. W.,  McMillan, 

C.  (2017).  Towards  prioritizing  documentation  effort.  IEEE  Transactions  on  Software  Engineering, 
44(9), 897-913. 

[P153]  da  Silva Maldonado, E.,  Shihab, E.,  Tsantalis, N. (2017). Using natural language processing to 
automatically detect self-admitted technical debt. IEEE Transactions on Software Engineering, 43(11), 
1044-1062. 

[P154]  Guéhéneuc,  Y.  G.,    Antoniol,  G.  (2008).  Demima:  A  multilayered  approach  for  design  pattern 

identification. IEEE transactions on software engineering, 34(5), 667-684. 

[P155]  Ye,  X.,  Bunescu,  R.,  Liu,  C.  (2015).  Mapping  bug  reports  to  relevant  files:  A  ranking  model,  a 
fine-grained benchmark, and feature evaluation. IEEE Transactions on Software Engineering, 42(4), 
379-402. 

[P156]  Yan, M., Xia, X., Shihab, E., Lo, D., Yin, J.,  Yang, X. (2018). Automating change-level self- 
admitted  technical  debt  determination.  IEEE  Transactions  on  Software  Engineering,  45(12),  1211- 
1229. 

[P157]  Arnaoudova,  V.,  Eshkevari,  L.  M.,  Di  Penta,  M.,  Oliveto,  R.,  Antoniol,  G.,    Guéhéneuc,  Y. 

G.  (2014).  Repent:  Analyzing  the  nature  of  identifier  renamings.  IEEE  Transactions  on  Software 
Engineering, 40(5), 502-532. 

 
 
GitHub Marketplace for Practitioners and Researchers to Date 

53 

[P158]  Duran-Limon, H. A., Garcia-Rios, C. A., Castillo-Barrera, F. E.,  Capilla, R. (2015). An ontology- 
based product architecture derivation approach. IEEE Transactions on Software Engineering, 41(12), 
1153-1168. 

[P159]  Canal,  C.,  Poizat,  P.,  Salaün,  G.  (2008).  Model-based  adaptation  of  behavioral  mismatching 

components. IEEE Transactions on Software Engineering, 34(4), 546-563. 

[P160]  Athanasiou,  D.,  Nugroho,  A.,  Visser, J.,  Zaidman,  A.  (2014). Test code  quality and its relation 

to issue handling performance. IEEE Transactions on Software Engineering, 40(11), 1100-1125. 

[P161]  Ali,  N.,  Guéhéneuc,  Y.  G.,  Antoniol,  G.  (2012).  Trustrace:  Mining  software  repositories  to 
improve the accuracy of requirement traceability links. IEEE Transactions on Software Engineering, 
39(5), 725-741. 

[P162]  Brosch,  F.,  Koziolek,  H.,  Buhnova,  B.,  Reussner,  R.  (2011).  Architecture-based  reliability  pre- 
diction with the palladio component model. IEEE Transactions on Software Engineering, 38(6), 1319- 
1339. 

[P163]  de Jonge, M. (2005). Build-level components. IEEE Transactions on Software Engineering, 31(7), 

588-600. 
[P164]  Sakti, A., Pesant, G.,  Guéhéneuc, Y. G. (2014). Instance generator and problem representation 
to improve object oriented code coverage. IEEE Transactions on Software Engineering, 41(3), 294-313. 
[P165]  Tsantalis, N., Mazinanian, D.,  Krishnan, G. P. (2015). Assessing the refactorability of software 

clones. IEEE Transactions on Software Engineering, 41(11), 1055-1090. 

[P166]  Fowkes,  J.,  Chanthirasegaran,  P.,  Ranca,  R.,  Allamanis,  M.,  Lapata,  M.,  Sutton,  C.  (2017). 
Autofolding for source code summarization. IEEE Transactions on Software Engineering, 43(12), 1095- 
1109. 

[P167]  Egyed,  A.  (2010).  Automatically  detecting  and  tracking  inconsistencies  in  software  design  models. 

IEEE  Transactions  on  Software  Engineering,  37(2),  188-204. 

[P168]  Kästner, C.,  Dreiling,  A.,  Ostermann, K. (2013).  Variability  mining:  Consistent semi-automatic 

detection of product-line features. IEEE Transactions on Software Engineering, 40(1), 67-82. 

[P169]  Stewart,  D.  B.,  Arora,  G.  (2003).  A  tool  for  analyzing  and  fine  tuning  the  real-time  properties 

of an embedded system. IEEE Transactions on Software Engineering, 29(4), 311-326. 

[P170]  Porter,  A.,  Yilmaz,  C.,  Memon,  A.  M.,  Schmidt,  D.  C.,  Natarajan,  B.  (2007).  Skoll:  A  pro- 
cess and infrastructure for distributed continuous quality assurance. IEEE Transactions on Software 
Engineering, 33(8), 510-525. 

[P171]  Emberson, P.,  Bate, I. (2009). Stressing search  with scenarios for flexible solutions to real-time 

task allocation problems. IEEE Transactions on Software Engineering, 36(5), 704-718. 

[P172]  Lin,  Z.,  Zhang,  X.,  Xu,  D.  (2009).  Reverse  engineering  input  syntactic  structure  from  program 

execution and its applications. IEEE Transactions on Software Engineering, 36(5), 688-703. 

[P173]  Lavezzoli, P. (2006). The dawn of Indian music in the West. AC Black. 
[P174]  Prechelt, L.,  Unger, B. (2001). An experiment measuring the effects of personal software process 

(PSP) training. IEEE transactions on software engineering, 27(5), 465-472. 

[P175]  Dorn, J., Lacomis, J., Weimer, W.,  Forrest, S. (2017). Automatically exploring tradeoffs between 
software output fidelity and energy costs. IEEE Transactions on Software Engineering, 45(3), 219-236. 
[P176]  DO, Hyunsook, MIRARAB, Siavash, TAHVILDARI, Ladan, et al. The effects of time constraints 
on test case prioritization: A series of controlled experiments. IEEE Transactions on Software Engi- 

neering, 2010, vol. 36, no 5, p. 593-617. 

[P177]  Chen,  N.,  Kim,  S.  (2014).  Star:  Stack  trace  based  automatic  crash  reproduction  via  symbolic 

execution. IEEE transactions on software engineering, 41(2), 198-220. 

[P178]  Han, A. R.,  Cha, S. (2017). Two-phase assessment approach to improve the efficiency of refac- 

toring identification. IEEE Transactions on Software Engineering, 44(10), 1001-1023. 

[P179]  Nguyen, B. N.,  Memon, A. M. (2014). An observe-model-exercise* paradigm to test event-driven 
systems with undetermined input spaces. IEEE Transactions on Software Engineering, 40(3), 216-234. 
[P180]  Dagenais, B.,  Robillard, M. P. (2014). Using traceability links to recommend adaptive changes 

for documentation evolution. IEEE Transactions on Software Engineering, 40(11), 1126-1146. 

[P181]  Tsantalis, N.,  Chatzigeorgiou, A. (2009). Identification of move method refactoring opportunities. 

IEEE  Transactions  on  Software  Engineering,  35(3),  347-367. 

[P182]  Fan,  Y.,  Xia,  X.,  Lo,  D.,  Hassan,  A.  E.  (2018).  Chaff  from  the  wheat:  Characterizing  and 

determining valid bug reports. IEEE transactions on software engineering, 46(5), 495-525. 

[P183]  Liu,  Y.,  Xu,  C.,  Cheung,  S.  C.,  Lü,  J.  (2014).  Greendroid:  Automated  diagnosis  of  energy 
inefficiency for smartphone applications. IEEE Transactions on Software Engineering, 40(9), 911-940. 
[P184]  Wang,  X.,  Dang,  Y.,  Zhang,  L.,  Zhang,  D.,  Lan,  E.,  Mei,  H.  (2014).  Predicting  consistency- 
maintenance requirement of code clonesat copy-and-paste time. IEEE Transactions on Software En- 

gineering, 40(8), 773-794. 

 
 
54 

Sk Golam Saroar et al. 

[P185]  McMinn, P., Harman, M., Lakhotia, K., Hassoun, Y.,  Wegener, J. (2011). Input domain reduction 
through irrelevant variable removal and its effect on local, global, and hybrid search-based structural 
test data generation. IEEE Transactions on Software Engineering, 38(2), 453-477. 

[P186]  Lee, S., Wu, R., Cheung, S. C.,  Kang, S. (2019). Automatic detection and update suggestion for 
outdated api names in documentation. IEEE Transactions on Software Engineering, 47(4), 653-675. 
[P187]  Ghiotto,  G.,  Murta,  L.,  Barros,  M.,  Van  Der  Hoek,  A.  (2018).  On  the  nature  of  merge  con- 
flicts: A study of 2,731 open source Java projects hosted by GitHub. IEEE Transactions on Software 
Engineering, 46(8), 892-915. 

[P188]  Xuan, J., Martinez, M., Demarco, F., Clement, M., Marcote, S. L., Durieux, T., ...  Monperrus, M. 
(2016). Nopol: Automatic repair of conditional statement bugs in java programs. IEEE Transactions 
on Software Engineering, 43(1), 34-55. 

[P189]  Carr,  S.  A.,  Logozzo,  F.,  Payer,  M.  (2016).  Automatic  contract  insertion  with  ccbot.  IEEE 

Transactions on Software Engineering, 43(8), 701-714. 

[P190]  Wen, M., Wu, R.,  Cheung, S. C. (2018). How well do change sequences predict defects? sequence 

learning from software changes. IEEE Transactions on Software Engineering, 46(11), 1155-1175. 

[P191]  Brun,  Y.,  young  Bang,  J.,  Edwards,  G.,  Medvidovic,  N.  (2015).  Self-adapting  reliability  in 

distributed software systems. IEEE Transactions on Software Engineering, 41(8), 764-780. 

[P192]  Meng, Y., Gay, G.,  Whalen, M. (2018). Ensuring the observability of structural test obligations. 

IEEE Transactions on Software Engineering, 46(7), 748-772. 

[P193]  Fraser, G.,  Zeller, A.  (2011).  Mutation-driven generation of unit  tests  and oracles. IEEE Trans- 

actions on Software Engineering, 38(2), 278-292. 

[P194]  Gallaba, K.,  McIntosh, S. (2018). Use and misuse of continuous integration features: An empirical 
study of projects that (mis) use travis ci. IEEE Transactions on Software Engineering, 46(1), 33-50. 
[P195]  Wang,  Y.,  Yu,  H.,  Zhu,  Z.,  Zhang,  W.,  Zhao,  Y.  (2017).  Automatic  software  refactoring  via 
weighted  clustering  in  method-level  networks.  IEEE  Transactions  on  Software  Engineering,  44(3), 
202-236. 

[P196]  Mirakhorli, M.,  Cleland-Huang, J.  (2015). Detecting, tracing, and monitoring architectural tac- 

tics in code. IEEE Transactions on Software Engineering, 42(3), 205-220. 

[P197]  Chen, T. H., Shang, W., Jiang, Z. M., Hassan, A. E., Nasser, M.,  Flora, P. (2016). Finding and 
evaluating the performance impact of redundant data access for applications that are developed using 
object-relational mapping frameworks. IEEE Transactions on Software Engineering, 42(12), 1148-1161. 
[P198]  Jiang, Y., Liu, H., Zhu, J.,  Zhang, L. (2018). Automatic and accurate expansion of abbreviations 

in parameters. IEEE Transactions on Software Engineering, 46(7), 732-747. 

[P199]  Delahaye,  B.,  Katoen,  J.  P.,  Larsen,  K.  G.,  Legay,  A.,  Pedersen,  M.  L.,  Sher,  F.,  Wasowski,  A. 
(2011, June). New results on abstract probabilistic automata. In 2011 Eleventh International Confer- 
ence on Application of Concurrency to System Design (pp. 118-127). IEEE. 

[P200]  Hrischuk,  C.  E.,  Woodside,  C.  M.  (2002).  Logical  clock  requirements  for  reverse  engineering 

scenarios from a distributed system. IEEE Transactions on Software Engineering, 28(4), 321-339. 

[P201]  Desmet,  L.,  Verbaeten,  P.,  Joosen,  W.,  Piessens,  F.  (2008).  Provable  protection  against  web 
application vulnerabilities related to session data dependencies. IEEE transactions on software engi- 
neering, 34(1), 50-64. 

[P202]  Bagheri, H., Tang, C.,  Sullivan, K. (2016). Automated synthesis and dynamic analysis of tradeoff 
spaces for object-relational mapping. IEEE Transactions on Software Engineering, 43(2), 145-163. 
[P203]  Chavez, H. M., Shen, W., France, R. B., Mechling, B. A.,  Li, G. (2015). An approach to checking 
consistency between UML class model and its Java implementation. IEEE Transactions on software 
engineering, 42(4), 322-344. 

[P204]  Reder,  A.,  Egyed,  A.  (2013).  Determining  the  cause  of  a  design  model  inconsistency.  IEEE 

Transactions on Software Engineering, 39(11), 1531-1548. 

[P205]  Xia,  X.,  Bao,  L.,  Lo,  D.,  Xing,  Z.,  Hassan,  A.  E.,  Li,  S.  (2017).  Measuring  program  comprehen- 
sion: A large-scale field study with professionals. IEEE Transactions on Software Engineering, 44(10), 
951-976. 

[P206]  Buse,  R.  P.,  Weimer,  W.  R.  (2009).  Learning  a  metric  for  code  readability.  IEEE  Transactions 

on software engineering, 36(4), 546-558. 

[P207]  Mäder,  P.,  Kuschke,  T.,  Janke,  M.  (2019).  Reactive  auto-completion  of  modeling  activities. 

IEEE Transactions on Software Engineering, 47(7), 1431-1451. 

[P208]  Itzik,  N.,  Reinhartz-Berger,  I.,  Wand,  Y.  (2015).  Variability  analysis  of  requirements:  Consid- 
ering behavioral differences and reflecting stakeholders’ perspectives. IEEE Transactions on Software 
Engineering, 42(7), 687-706. 

[P209]  Bate,  I.,  Burns,  A.,  Davis,  R.  I.  (2016).  An  enhanced  bailout  protocol  for  mixed  criticality 

embedded software. IEEE Transactions on Software Engineering, 43(4), 298-320. 

 
 
GitHub Marketplace for Practitioners and Researchers to Date 

55 

[P210]  Bennaceur,  A.,  Issarny,  V.  (2014).  Automated  synthesis  of  mediators  to  support  component 

interoperability. IEEE Transactions on Software Engineering, 41(3), 221-240. 

[P211]  Bontemps, Y., Heymans, P.,  Schobbens, P. Y. (2005). From live sequence charts to state machines 

and back: A guided tour. IEEE Transactions on Software Engineering, 31(12), 999-1014. 

[P212]  Nadi, S., Berger, T., Kästner, C.,  Czarnecki, K. (2015). Where do configuration constraints stem 
from? an extraction approach and an empirical study. IEEE  Transactions  on Software Engineering, 
41(8), 820-841. 

[P213]  Ravindran, B. (2002). Engineering dynamic real-time distributed systems: Architecture, system 
description language, and middleware. IEEE Transactions on Software Engineering, 28(1), 30-57. 
[P214]  Le Goues, C.,  Weimer, W. (2011). Measuring code quality to improve specification mining. IEEE 

Transactions on Software Engineering, 38(1), 175-190. 

[P215]  Cunha, J., Fernandes, J. P., Mendes, J.,  Saraiva, J. (2014). Embedding, evolution, and validation 

of model-driven spreadsheets. IEEE Transactions on Software Engineering, 41(3), 241-263. 

[P216]  Chalin,  P.  (2009).  Engineering  a  sound  assertion  semantics  for  the  verifying  compiler.  IEEE 

Transactions on Software Engineering, 36(2), 275-287. 

[P217]  Grano,  G.,  Palomba,  F.,  Gall,  H.  C.  (2019).  Lightweight  assessment  of  test-case  effectiveness 
using source-code-quality indicators. IEEE Transactions on Software Engineering, 47(4), 758-774. 
[P218]  Zuberi, K. M., Pillai, P.,  Shin, K. G. (1999, December). EMERALDS: a small-memory real-time 
microkernel.  In  Proceedings  of  the  seventeenth  ACM  symposium  on  Operating  systems  principles  (pp. 
277-299). 

[P219]  Haran, M., Karr, A., Last, M., Orso, A., Porter, A. A., Sanil, A.,  Fouche, S. (2007). Techniques for 
classifying executions of deployed software to support software engineering tasks. IEEE Transactions 
on Software Engineering, 33(5), 287-304. 

[P220]  Ramírez-Mora,  S.  L.,  Oktaba,  H.,  Gómez-Adorno,  H.,  Sierra,  G.  (2021).  Exploring  the  commu- 
nication  functions  of  comments  during  bug  fixing  in  Open  Source  Software  projects.  Information  and 
Software Technology, 136, 106584. 

[P221]  Nam,  J.,  Wang,  S.,  Xi,  Y.,  Tan,  L.  (2019).  A  bug  finder  refined  by  a  large  set  of  open-source 

projects. Information and Software Technology, 112, 164-175. 

[P222]  Lakhotia,  K.,  Harman,  M.,  Gross,  H.  (2013).  AUSTIN:  An  open  source  tool  for  search  based 

software testing of C programs. Information and Software Technology, 55(1), 112-125. 

[P223]  Kawaguchi,  S.,  Garg,  P.  K.,  Matsushita,  M.,  Inoue,  K.  (2006).  Mudablue:  An  automatic  cate- 

gorization system for open source repositories. Journal of Systems and Software, 79(7), 939-953. 

[P224]  Saidani,  I.,  Ouni,  A.,  Mkaouer,  M.  W.,  Palomba,  F.  (2021).  On  the  impact  of  Continuous 
Integration on refactoring practice: An exploratory study on TravisTorrent. Information and Software 
Technology, 138, 106618. 

[P225]  Liu,  B.,  Rong,  G.,  Dong,  L.,  Zhang,  H.,  Chen,  D.,  Chen,  T.,  ...  Zhang,  T.  (2019).  What  are  the 
factors affecting the handover process in open source development?. Journal of Systems and Software, 
153, 238-254. 

[P226]  Watanabe,  W.  M.,  Felizardo,  K.  R.,  Candido  Jr,  A.,  de  Souza,  É.  F.,  de  Campos  Neto,  J. 
E.,  Vijaykumar, N. L. (2020). Reducing efforts of software engineering systematic literature reviews 
updates using text classification. Information and Software Technology, 128, 106395. 

[P227]  Garousi,  V.,  Keleş,  A.  B.,  Balaman,  Y.,  Güler,  Z.  Ö.,  Arcuri,  A.  (2021).  Model-based  testing  in 
practice:  An  experience  report  from  the  web  applications  domain.  Journal  of  Systems  and  Software, 
180, 111032. 

[P228]  Bian, Y., Koru, G., Su, X.,  Ma, P. (2013). SPAPE: A semantic-preserving amorphous procedure 

extraction method for near-miss clones. Journal of Systems and Software, 86(8), 2077-2093. 

[P229]  Marcilio, D., Furia, C. A., Bonifácio, R.,  Pinto, G. (2020). SpongeBugs: Automatically generating 
fix  suggestions  in response  to  static  code  analysis  warnings.  Journal  of  Systems  and  Software,  168, 
110671. 

[P230]  Sülün,  E.,  Tüzün,  E.,  Doğrusöz,  U.  (2021).  RSTrace+:  Reviewer  suggestion  using  software 

artifact traceability graphs. Information and Software Technology, 130, 106455. 

[P231]  Iammarino, M., Zampetti, F., Aversano, L.,  Di Penta, M. (2021). An empirical study on the co- 
occurrence between refactoring actions and self-admitted technical debt removal. Journal of Systems 
and Software, 178, 110976. 

[P232]  Hassler, E. E.,  Hale,  D. P.,  Hale, J.  E. (2018).  A comparison of automated training-by-example 
selection algorithms for Evidence Based Software Engineering. Information and Software Technology, 
98, 59-73. 

[P233]  Wang,  D.,  Kula,  R.  G.,  Ishio,  T.,  Matsumoto,  K.  (2021).  Automatic  patch  linkage  detection 
in code review using textual content and file location features. Information and Software Technology, 
139, 106637. 

 
 
56 

Sk Golam Saroar et al. 

[P234]  Tsai, J. J.,  Liu, A. (2009). Experience on knowledge-based software engineering: A logic-based 
requirements language and its industrial applications. Journal of Systems and Software, 82(10), 1578- 
1587. 

[P235]  Lizcano, D., Soriano, J., López, G.,  Gutiérrez, J. J. (2017). Automatic verification and validation 
wizard in web-centred end-user software engineering. Journal of Systems and Software, 125, 47-67. 
[P236]  Golzadeh, M., Decan, A., Legay, D.,  Mens, T. (2021). A ground-truth dataset and classification 
model for detecting bots in GitHub issue and PR comments. Journal of Systems and Software, 175, 
110911. 

[P237]  Cordy,  J.  R.,  Dean,  T.  R.,  Malton,  A.  J.,  Schneider,  K.  A.  (2002).  Source  transformation  in 
software engineering using the  TXL transformation system. Information and Software Technology, 
44(13), 827-837. 

[P238]  Salman, H. E. (2021). Feature-based insight for forks in social coding platforms. Information and 

Software Technology, 140, 106679. 

[P239]  Moseler,  O.,  Lemmer,  F.,  Baltes,  S.,  Diehl,  S.  (2021).  On  the  diversity  and  frequency  of  code 
related to mathematical formulas in real-world Java projects. Journal of Systems and Software, 172, 
110863. 

[P240]  Grundy, J., Mugridge, W.,  Hosking, J. (2000).  Constructing  component-based  software  engi- 
neering environments: issues and experiences. Information and Software Technology, 42(2), 103-114. 
[P241]  Magdaleno, A. M., Werner, C. M. L.,  De Araujo, R. M. (2012). Reconciling software development 

models: A quasi-systematic review. Journal of Systems and Software, 85(2), 351-369. 

[P242] Li, C., Huang, L., Ge, J., Luo, B.,  Ng, V. (2018). Automatically classifying user requests in 

crowdsourcing requirements engineering. Journal of Systems and Software, 138, 108-123. 

[P243]  Saied, M. A., Ouni, A., Sahraoui, H., Kula, R. G., Inoue, K.,  Lo, D. (2018). Improving reusability 
of software libraries through usage pattern mining. Journal of Systems and Software, 145, 164-179. 
[P244]  de Lima Júnior, M. L., Soares, D. M., Plastino, A.,  Murta, L. (2018). Automatic assignment of 
integrators  to  pull  requests:  The  importance  of  selecting  appropriate  attributes.  Journal  of  Systems 
and Software, 144, 181-196. 

[P245]  Auch, M., Weber, M., Mandl, P.,  Wolff, C. (2020). Similarity-based analyses on software appli- 

cations: A systematic literature review. Journal of Systems and Software, 168, 110669. 

[P246]  Chong, C. Y.,  Lee, S. P. (2017). Automatic clustering constraints derivation from object-oriented 
software using weighted complex network with graph theory analysis. Journal of Systems and Software, 
133, 28-53. 

[P247]  Rahman,  A.,  Williams,  L.  (2019).  Source  code  properties  of  defective  infrastructure  as  code 

scripts. Information and Software Technology, 112, 148-163. 

[P248]  Montandon, J. E., Valente, M. T.,  Silva, L. L. (2021). Mining the technical roles of github users. 

Information and Software Technology, 131, 106485. 

[P249]  Miryeganeh,  N.,  Hashtroudi,  S.,  Hemmati,  H.  (2021).  GloBug:  Using  global  data  in  Fault  Lo- 

calization. Journal of Systems and Software, 177, 110961. 

[P250]  Wang,  S.,  Bansal,  C.,  Nagappan,  N.  (2021).  Large-scale  intent  analysis  for  identifying  large- 

review-effort code changes. Information and Software Technology, 130, 106408. 

[P251]  Imtiaz,  J.,  Sherin,  S.,  Khan,  M.  U.,  Iqbal,  M.  Z.  (2019).  A  systematic  literature  review  of  test 

breakage prevention and repair techniques. Information and Software Technology, 113, 1-19. 

[P252]  Meqdadi,  O.,  Alhindawi,  N.,  Alsakran,  J.,  Saifan,  A.,  Migdadi,  H.  (2019).  Mining  software 
repositories for adaptive change commits using machine learning techniques. Information and Software 
Technology, 109, 80-91. 

[P253]  Fu,  Y.,  Yan,  M.,  Zhang,  X.,  Xu,  L.,  Yang,  D.,  Kymer,  J.  D.  (2015).  Automated  classification 
of software change messages by semi-supervised latent dirichlet allocation. Information and Software 
Technology, 57, 369-377. 

[P254]  Newman,  C.  D.,  AlSuhaibani,  R.  S.,  Decker,  M.  J.,  Peruma,  A.,  Kaushik,  D.,  Mkaouer,  M.  W., 
Hill,  E.  (2020).  On  the  generation,  structure,  and  semantics  of  grammar  patterns  in  source  code 
identifiers. Journal of Systems and Software, 170, 110740. 

[P255]  Guerra, E., Alves, F., Kulesza, U.,  Fernandes, C. (2013). A reference architecture for organizing 
the internal structure of metadata-based frameworks. Journal of Systems and Software, 86(5), 1239- 
1256. 

[P256]  Yan,  M.,  Fu,  Y.,  Zhang,  X.,  Yang,  D.,  Xu,  L.,  Kymer,  J.  D.  (2016).  Automatically  classifying 
software  changes  via  discriminative  topic  model:  Supporting  multi-category  and  cross-project.  Journal 
of Systems and Software, 113, 296-308. 

[P257]  Sarkar,  S.,  Maskeri,  G.,  Ramachandran,  S.  (2009).  Discovery  of  architectural  layers  and  mea- 
surement of layering violations in source code. Journal of Systems and Software, 82(11), 1891-1905. 
[P258]  Lima,  P.,  Guerra,  E.,  Meirelles,  P.,  Kanashiro,  L.,  Silva,  H.,  Silveira,  F.  F.  (2018).  A  metrics 

suite for code annotation assessment. Journal of Systems and Software, 137, 163-183. 

 
 
GitHub Marketplace for Practitioners and Researchers to Date 

57 

[P259]  Mo,  R.,  Yin,  Z.  (2020).  Exploring  software  bug-proneness  based  on  evolutionary  clique  modeling 

and analysis. Information and Software Technology, 128, 106380. 

[P260]  Coppola,  R.,  Ardito,  L.,  Torchiano,  M.,  Alégroth,  E.  (2021).  Translation  from  layout-based  to 
visual  android  test  scripts:  An  empirical  evaluation.  Journal  of  Systems  and  Software,  171,  110845. 
[P261]  Qiu, D., Li, B.,  Leung, H. (2016). Understanding the API usage in Java. Information and software 

technology, 73, 81-100. 

[P262]  Xiao,  X.,  Pan,  Y.,  Zhang,  B.,  Hu,  G.,  Li,  Q.,  Lu,  R.  (2021).  ALBFL:  A  novel  neural  ranking 
model  for  software  fault  localization  via  combining  static  and  dynamic  features.  Information  and 
Software Technology, 139, 106653. 

[P263]  Lakhotia, K., McMinn, P.,  Harman, M. (2010). An empirical investigation into branch coverage 

for C programs using CUTE and AUSTIN. Journal of Systems and Software, 83(12), 2379-2391. 

[P264]  Pradhan,  D.,  Wang,  S.,  Yue,  T.,  Ali,  S.,  Liaaen,  M.  (2019).  Search-based  test  case  implantation 

for  testing  untested  configurations.  Information  and  Software  Technology,  111,  22-36. 

[P265]  Banerjee, S., Syed, Z., Helmick, J., Culp, M., Ryan, K.,  Cukic, B. (2017). Automated triaging 

of very large bug repositories. Information and software technology, 89, 1-13. 

[P266]  Shippey,  T.,  Bowes,  D.,  Hall,  T.  (2019).  Automatically  identifying  code  features  for  software 

defect prediction: Using AST N-grams. Information and Software Technology, 106, 142-160. 

[P267]  Şora,  I.,  Chirila,  C.  B.  (2019).  Finding  key  classes  in  object-oriented  software  systems  by  tech- 

niques based on static analysis. Information and Software Technology, 116, 106176. 

[P268]  Huang,  Y.,  Hu,  X.,  Jia,  N.,  Chen,  X.,  Zheng,  Z.,  Luo,  X.  (2020).  CommtPst:  Deep  learning 

source code for commenting positions prediction. Journal of Systems and Software, 170, 110754. 

[P269]  Yan,  M.,  Zhang,  X.,  Liu,  C.,  Xu,  L.,  Yang,  M.,  Yang,  D.  (2017).  Automated  change-prone  class 
prediction on unlabeled dataset using unsupervised method. Information and Software Technology, 
92, 1-16. 

[P270]  Gu, Y., Xuan, J., Zhang, H., Zhang, L., Fan, Q., Xie, X.,  Qian, T. (2019). Does the fault reside 
in a stack trace? assisting crash localization by predicting crashing fault residence. Journal of Systems 
and Software, 148, 88-104. 

[P271]  Malek, S., Krishnan, H. R.,  Srinivasan, J. (2010). Enhancing middleware support for architecture- 
based development through compositional weaving of styles. Journal of Systems and Software, 83(12), 
2513-2527. 

[P272]  Li,  Z.,  Jiang,  Z.,  Chen,  X.,  Cao,  K.,  Gu,  Q.  (2021).  Laprob:  A  label  propagation-based  software 

bug  localization  method.  Information  and  Software  Technology,  130,  106410. 

[P273]  Lemos,  O.  A.  L.,  Bajracharya,  S.,  Ossher,  J.,  Masiero,  P.  C.,  Lopes,  C.  (2011).  A  test-driven 
approach to code search and its application to the reuse of auxiliary functionality. Information and 
Software Technology, 53(4), 294-306. 

[P274]  Mazhelis, O., Tyrväinen, P.,  Frank, L. (2013). Vertical software industry evolution: The impact 
of software costs and limited customer base. Information and Software Technology, 55(4), 690-698. 
[P275]  Ras, E.,  Rech, J. (2009).  Using Wikis to support the Net Generation  in  improving knowledge 

acquisition in capstone projects. Journal of Systems and Software, 82(4), 553-562. 

[P276]  Yu, K., Lin, M., Chen, J.,  Zhang, X. (2012). Towards automated debugging in software evolution: 
Evaluating  delta  debugging  on  real  regression  bugs  from  the  developers’  perspectives.  Journal  of 
Systems and Software, 85(10), 2305-2317. 

[P277]  Zhu,  L.,  Bui,  N.  B.,  Liu,  Y.,  Gorton,  I.  (2007).  MDABench:  Customized  benchmark  generation 

using MDA. Journal of Systems and Software, 80(2), 265-282. 

[P278]  Haitzer, T., Navarro, E.,  Zdun, U. (2017). Reconciling software architecture and source code in 

support of software evolution. Journal of Systems and Software, 123, 119-144. 

[P279]  Chen, Z., Guo, H. F.,  Song,  M.  (2018). Improving regression  test efficiency with an awareness 

of refactoring changes. Information and Software Technology, 103, 174-187. 

[P280]  Li,  S.,  Zhang,  H.,  Jia,  Z.,  Li,  Z.,  Zhang,  C.,  Li,  J.,  ...  Shan,  Z.  (2019).  A  dataflow-driven 
approach to identifying microservices from monolithic applications. Journal of Systems and Software, 
157, 110380. 

[P281]  Haitzer,  T.,  Zdun,  U.  (2015).  Semi-automatic  architectural  pattern  identification  and  documen- 

tation  using  architectural  primitives.  Journal  of  Systems  and  Software,  102,  35-57. 

[P282]  Pill, I.,  Wotawa, F. (2018). Automated generation of (F) LTL oracles for testing and debugging. 

Journal of Systems and Software, 139, 124-141. 

[P283]  Eler, M. M., Endo, A. T.,  Durelli, V. H. (2016). An empirical study to quantify the characteristics 
of Java programs that may influence symbolic execution from a unit testing perspective. Journal of 
Systems and Software, 121, 281-297. 

[P284]  Xiaobo,  Y.,  Bin,  L.,  Shihai,  W.,  Dong,  A.,  Feng,  Z.,    Yelin,  Y.  (2021).  Efilter:  An  effective 
fault  localization  based  on  information  entropy  with  unlabelled  test  cases.  Information  and  Software 
Technology, 134, 106543. 

 
 
58 

Sk Golam Saroar et al. 

[P285]  Schmidt, H. (2003). Trustworthy components––compositionality and prediction. Journal of Sys- 

tems and Software, 65(3), 215-225. 

[P286]  Ovaska, E., Evesti, A., Henttonen, K., Palviainen, M.,  Aho, P. (2010). Knowledge based quality- 
driven architecture design and evaluation. Information and Software Technology, 52(6), 577-601. 
[P287]  Liu,  K.,  Li,  L.,  Koyuncu,  A.,  Kim,  D.,  Liu,  Z.,  Klein,  J.,  Bissyandé,  T.  F.  (2021).  A  critical 
review on the evaluation of automated program repair systems. Journal of Systems and Software, 171, 
110817. 

[P288]  Xuan,  J.,  Cornu,  B.,  Martinez,  M.,  Baudry,  B.,  Seinturier,  L.,  Monperrus,  M.  (2016).  B- 
Refactoring: Automatic test code refactoring to improve dynamic analysis. Information and Software 
Technology, 76, 65-80. 

[P289]  Linsbauer,  L.,  Schwägerl,  F.,  Berger,  T.,  Grünbacher,  P.  (2021).  Concepts  of  variation  control 

systems. Journal of Systems and Software, 171, 110796. 

[P290]  Erdemir, U.,  Buzluca, F. (2014). A learning-based module extraction method for object-oriented 

systems. Journal of Systems and Software, 97, 156-177. 

[P291]  Kochovski,  P.,  Drobintsev,  P.  D.,  Stankovski,  V.  (2019).  Formal  quality  of  service  assurances, 
ranking and verification of cloud deployment options with a probabilistic model checking method. 
Information and Software Technology, 109, 14-25. 

[P292]  Ruiz-Rube,  I.,  Dodero,  J.  M.,  Colomo-Palacios,  R.  (2015).  A  framework  for  software  process 

deployment and evaluation. Information and Software Technology, 59, 205-221. 

[P293]  Al  Dallal,  J.,  Briand,  L.  C.  (2010).  An  object-oriented  high-level  design-based  class  cohesion 

metric. Information and software technology, 52(12), 1346-1361. 

[P294]  Vujošević-Janičić, M., Nikolić, M., Tošić, D.,  Kuncak, V. (2013). Software verification and graph 
similarity for automated evaluation of students’ assignments. Information and Software Technology, 
55(6), 1004-1016. 

[P295]  Dietrich,  R.,  Schmitt,  F.,  Grund,  A.,  Stolle,  J.  (2017).  Critical-blame  analysis  for  OpenMP  4.0 

offloading on Intel Xeon Phi. Journal of Systems and Software, 125, 381-388. 

[P296] Chong, C. Y., Lee, S. P.,  Ling, T. C. (2013). Efficient software clustering technique using an 
adaptive and preventive dendrogram cutting approach. Information and Software Technology, 55(11), 
1994-2012. 

[P297]  Staples,  M.,  Niazi,  M.  (2007).  Experiences  using  systematic  review  guidelines.  Journal  of  Systems 

and Software, 80(9), 1425-1437. 

[P298]  Shokripour,  R.,  Anvik,  J.,  Kasirun,  Z.  M.,  Zamani,  S.  (2015).  A  time-based  approach  to  auto- 

matic bug report assignment. Journal of Systems and Software, 102, 109-122. 

[P299]  Goseva-Popstojanova,  K.,  Perhinschi,  A.  (2015).  On  the  capability  of  static  code  analysis  to 

detect security vulnerabilities. Information and Software Technology, 68, 18-33. 

[P300]  Zhang, T., Chen, J., Yang, G., Lee, B.,  Luo, X. (2016). Towards more accurate severity prediction 

and fixer recommendation of software bugs. Journal of Systems and Software, 117, 166-184. 

[P301]  Ruiz, F. J. B., Ramón, Ó. S.,  Molina, J. G. (2017). A tool to support the definition and enactment 

of model-driven migration processes. Journal of Systems and Software, 128, 106-129. 

[P302]  Lukins,  S.  K.,  Kraft,  N.  A.,  Etzkorn,  L.  H.  (2010).  Bug  localization  using  latent  dirichlet  allo- 

cation. Information and Software Technology, 52(9), 972-990. 

[P303]  Chan, W. K., Cheung, S. C., Ho, J. C.,  Tse, T. H. (2009). PAT: a pattern classification approach 
to automatic reference oracles for the testing of mesh simplification programs. Journal of Systems and 
Software, 82(3), 422-434. 

[P304]  Kessentini,  W.,  Sahraoui,  H.,  Wimmer,  M.  (2019).  Automated  metamodel/model  co-evolution: 

A  search-based  approach.  Information  and  Software  Technology,  106,  49-67. 

[P305]  Wohlrab, R., Knauss, E.,  Pelliccione, P. (2020). Why and how to balance alignment and diversity 
of requirements engineering practices in automotive. Journal of Systems and Software, 162, 110516. 
[P306]  Al Dallal, J. (2012). Constructing models for predicting extract subclass refactoring opportunities 
using  object-oriented  quality  metrics.  Information  and  Software  Technology,  54(10),  1125-1141. 
[P307]  Cirilo, E., Nunes, I., Kulesza, U.,  Lucena, C. (2012). Automating the product derivation process 

of multi-agent systems product lines. Journal of Systems and Software, 85(2), 258-276. 

[P308]  Feldmann,  S.,  Kernschmidt,  K.,  Wimmer,  M.,  Vogel-Heuser,  B.  (2019).  Managing  inter-model 
inconsistencies  in  model-based  systems  engineering:  Application  in  automated  production  systems 
engineering.  Journal  of  Systems  and  Software,  153,  105-134. 

[P309]  Eichelberger, H.,  Schmid, K. (2009). Guidelines on the aesthetic quality of UML class diagrams. 

Information and Software Technology, 51(12), 1686-1698. 

[P310]  Nadal, S., Herrero, V., Romero, O., Abelló, A., Franch, X., Vansummeren, S.,  Valerio, D. (2017). 
A  software  reference  architecture  for  semantic-aware  Big  Data  systems.  Information  and  software 
technology, 90, 75-92. 

 
 
GitHub Marketplace for Practitioners and Researchers to Date 

59 

[P311]  Teixeira, S., Agrizzi, B. A., Pereira Filho, J. G., Rossetto, S., Pereira, I. S. A., Costa, P. D., ... 
Martinelli, R. R. (2020). LAURA architecture: Towards a simpler way of building situation-aware and 
business-aware IoT applications. Journal of Systems and Software, 161, 110494. 

[P312]  O’Keeffe, M.,  Cinnéide, M. O. (2008). Search-based refactoring for software maintenance. Journal 

of Systems and Software, 81(4), 502-516. 

[P313]  Kuttal,  S.  K.,  Sarma,  A.,  Rothermel,  G.,  Wang,  Z.  (2018).  What  happened  to  my  application? 
Helping end users comprehend evolution through variation management. Information and Software 
Technology, 103, 55-74. 

[P314]  Blouin,  A.,  Lelli,  V.,  Baudry,  B.,  Coulon,  F.  (2018).  User  interface  design  smell:  Automatic 

detection and refactoring of Blob listeners. Information and Software Technology, 102, 49-64. 

[P315]  Abdullah,  I.  S.,  Menascé,  D.  A.  (2013).  The  Meta-Protocol  framework.  Journal  of  Systems and 

Software, 86(11), 2711-2724. 

[P316]  Al-Hroob,  A.,  Imam,  A.  T.,  Al-Heisa,  R.  (2018).  The  use  of  artificial  neural  networks  for  ex- 
tracting  actions  and  actors  from  requirements  document.  Information  and  Software  Technology,  101, 
1-15. 

[P317]  Han, A. R., Bae, D. H.,  Cha, S. (2015). An efficient approach to identify multiple and independent 

move method refactoring candidates. Information and Software Technology, 59, 53-66. 

[P318]  Giaimo, F., Andrade, H.,  Berger, C. (2020). Continuous experimentation and the cyber–physical 
systems challenge: An overview of the literature and the industrial perspective. Journal of Systems 
and Software, 170, 110781. 

[P319]  Vogel-Heuser,  B.,  Fay,  A.,  Schaefer,  I.,  Tichy,  M.  (2015).  Evolution  of  software  in  automated 
production systems: Challenges and research directions. Journal of Systems and Software, 110, 54-84. 
[P320]  Cortellessa,  V.,  Di  Pompeo,  D.  (2021).  Analyzing  the  sensitivity  of  multi-objective  software 
architecture  refactoring  to  configuration  characteristics.  Information  and  Software  Technology,  135, 

106568. 

[P321]  Magalhaes,  A.  P.  F.,  Andrade,  A.  M.  S.,  Maciel,  R.  S.  P.  (2019).  Model  driven  transformation 
development (MDTD): An approach for developing model to model transformation. Information and 
Software Technology, 114, 55-76. 

[P322]  Shafiee,  S.,  Wautelet,  Y.,  Hvam,  L.,  Sandrin,  E.,  Forza,  C.  (2020).  Scrum  versus  Rational  Uni- 
fied  Process  in  facing  the  main  challenges  of  product  configuration  systems  development.  Journal  of 
Systems and Software, 170, 110732. 

[P323]  Boucher,  A.,  Badri,  M.  (2018).  Software  metrics  thresholds  calculation  techniques  to  predict 

fault-proneness:  An  empirical  comparison.  Information  and  Software  Technology,  96,  38-67. 

[P324]  Pira, E., Rafe, V.,  Nikanjam, A. (2018). Searching for violation of safety and liveness properties 
using knowledge discovery in complex systems specified through graph transformations. Information 
and Software Technology, 97, 110-134. 

[P325] Boes, J.,  Migeon, F. (2017). Self-organizing multi-agent systems for the control of complex 

systems. Journal of Systems and Software, 134, 12-28. 

[P326]  Heinrich,  R.,  Koch,  S.,  Cha,  S.,  Busch,  K.,  Reussner,  R.,  Vogel-Heuser,  B.  (2018).  Architecture- 
based change impact analysis in cross-disciplinary automated production systems. Journal of Systems 
and Software, 146, 167-185. 

[P327] Smidts, C., Huang, X.,  Widmaier, J. C. (2002). Producing reliable software: an experiment. 

Journal of Systems and Software, 61(3), 213-224. 

[P328]  Lin, Q., Low, C. P., Ng, J. M., Bu, J.,  Liu, X. (2003). Multiuser collaborative work in virtual 

environment based CASE tool. Information and Software Technology, 45(5), 253-267. 

[P329]  Butting,  A.,  Kautz,  O.,  Rumpe,  B.,  Wortmann,  A.  (2019).  Continuously  analyzing  finite, 
message-driven, time-synchronous component  connector systems during architecture evolution. Jour- 
nal of Systems and Software, 149, 437-461. 

[P330]  Han,  A.  R.,  Bae,  D.  H.  (2013).  Dynamic  profiling-based  approach  to  identifying  cost-effective 

refactorings.  Information  and  Software  Technology,  55(6),  966-985. 

[P331] Ferrer, J., Chicano, F.,  Alba, E. (2013). Estimating software testing complexity. Information 

and Software Technology, 55(12), 2125-2139. 

[P332]  Alkhazi,  B.,  Abid,  C.,  Kessentini,  M.,  Wimmer,  M.  (2020).  On  the  value  of  quality  attributes 
for  refactoring  ATL  model  transformations:  A  multi-objective  approach.  Information  and  Software 
Technology, 120, 106243. 

[P333]  Autili,  M.,  Mostarda,  L.,  Navarra,  A.,  Tivoli,  M.  (2008).  Synthesis  of  decentralized  and  concur- 
rent  adaptors  for  correctly  assembling  distributed  component-based  systems.  Journal  of  Systems  and 
Software, 81(12), 2210-2236. 

[P334]  Staron,  M.,  Meding,  W.  (2016).  Mesram–a  method  for  assessing  robustness  of  measurement 
programs in large software development organizations and its industrial evaluation. Journal of Systems 
and Software, 113, 76-100. 

 
 
60 

Sk Golam Saroar et al. 

[P335]  Bellettini, C., Damiani, E.,  Fugini, M. G. (2001). Software reuse in-the-small: automating group 

rewarding. Information and Software Technology, 43(11), 651-660. 

[P336]  Abushark,  Y.,  Thangarajah,  J.,  Harland,  J.,  Miller,  T.  (2017).  A  framework  for  automatically 

ensuring the conformance of agent designs. Journal of Systems and Software, 131, 266-310. 

[P337]  Vogel-Heuser, B., Fischer, J., Feldmann, S., Ulewicz, S.,  Rösch, S. (2017). Modularity and archi- 
tecture of  PLC-based software for automated production Systems: An analysis in industrial companies. 
Journal of Systems and Software, 131, 35-62. 

[P338]  Edagawa, T. A. Y. S. S. T. T., Akaike, T., Higo, Y., Kusumoto, S., Hanabusa, S.,  Shibamoto, 
T. (2011). Function point measurement from Web application source code based on screen transitions 
and database accesses. Journal of Systems and Software, 84(6), 976-984. 

[P339]  Bracciali, A., Brogi, A.,  Canal, C. (2005). A formal approach to component adaptation. Journal 

of Systems and Software, 74(1), 45-54. 

[P340]  Izquierdo,  J.  L.  C.,  Jouault,  F.,  Cabot,  J.,  Molina,  J.  G.  (2012).  API2MoL:  Automating  the 
building of bridges between APIs and Model-Driven Engineering. Information and Software Technol- 
ogy, 54(3), 257-273. 

[P341]  Delgado-Perez, P.,  Chicano, F. (2020). An experimental and practical study on the equivalent 
mutant  connection:  An  evolutionary  approach.  Information  and  Software  Technology,  124,  106317. 
[P342]  Shahin, M., Liang, P.,  Babar, M. A. (2014). A systematic review of software architecture visu- 

alization techniques. Journal of Systems and Software, 94, 161-185. 

[P343]  Guo, J., White, J., Wang, G., Li,  J.,  Wang, Y. (2011).  A  genetic algorithm for optimized feature 
selection with resource constraints in software product lines. Journal of Systems and Software, 84(12), 
2208-2221. 

[P344]  Wood,  S.,  Michaelides,  G.,    Thomson,  C.  (2013).  Successful  extreme  programming:  Fidelity  to 

the  methodology  or  good  teamworking?.  Information  and  Software  Technology,  55(4),  660-672. 

[P345]  Staron,  M.,  Meding,  W.,  Nilsson,  C.  (2009).  A  framework  for  developing  measurement  systems 

and its industrial evaluation. Information and Software Technology, 51(4), 721-737. 

[P346]  Rebai, S., Kessentini, M., Wang, H.,  Maxim, B. (2020). Web service design defects detection: A 

bi-level multi-objective approach. Information and Software Technology, 121, 106255. 

[P347]  Parra,  E.,  Dimou,  C.,  Llorens,  J.,  Moreno,  V.,  Fraga,  A.  (2015).  A  methodology  for  the  clas- 
sification  of  quality  of  requirements  using  machine  learning  techniques.  Information  and  Software 
Technology, 67, 180-195. 

[P348]  Pelliccione,  P.,  Tivoli,  M.,  Bucchiarone,  A.,    Polini,  A.  (2008).  An  architectural  approach  to 

the  correct  and  automatic  assembly  of  evolving  component-based  systems.  Journal  of  Systems  and 
Software, 81(12), 2237-2251. 
[P349]  Nguyen, P. H., Kramer, M., Klein, J.,  Le Traon, Y. (2015). An extensive systematic review on 
the model-driven development of secure systems. Information and Software Technology, 68, 62-81. 
[P350]  Pančur, M.,  Ciglarič, M. (2011). Impact of test-driven development on productivity, code and 

tests: A controlled experiment. Information and Software Technology, 53(6), 557-573. 

[P351]  Kebir,  S.,  Borne,  I.,  Meslati,  D.  (2017).  A  genetic  algorithm-based  approach  for  automated 

refactoring of component-based software. Information and Software Technology, 88, 17-36. 

[P352]  Pernstål,  J.,  Gorschek,  T.,  Feldt,  R.,  Florén,  D.  (2015).  Requirements  communication  and  bal- 
ancing in large-scale software-intensive product development. Information and Software Technology, 
67, 44-64. 

[P353]  Stamelos,  I.,  Vlahavas,  I.,  Refanidis,  I.,  Tsoukiàs,  A.  (2000).  Knowledge  based  evaluation  of 

software systems: a case study. Information and Software Technology, 42(5), 333-345. 

[P354]  Fortino, G.,  Russo, W. (2012). ELDAMeth: An agent-oriented methodology for simulation-based 

prototyping of distributed agent systems. Information and Software Technology, 54(6), 608-624. 

[P355]  Spanoudakis,  G.,  Zisman,  A.,  Pérez-Minana,  E.,  Krause,  P.  (2004).  Rule-based  generation  of 

requirements traceability relations. Journal of systems and software, 72(2), 105-127. 

[P356]  Szőke,  Á.  (2011).  Conceptual  scheduling  model  and  optimized  release  scheduling  for  agile  envi- 

ronments.  Information  and  software  technology,  53(6),  574-591. 

[P357]  Briand,  L.  C.,  Labiche,  Y.,  He,  S.  (2009).  Automating  regression  test  selection  based  on  UML 

designs. Information and Software Technology, 51(1), 16-30. 

[P358]  Etemaadi,  R.,  Lind,  K.,  Heldal,  R.,  Chaudron,  M.  R.  (2013).  Quality-driven  optimization  of 
system  architecture:  Industrial  case  study  on  an  automotive  sub-system.  Journal  of  Systems  and 
Software, 86(10), 2559-2573. 

[P359]  Winter, J., Rönkkö, K.,  Rissanen, M. (2014). Identifying organizational barriers—a case study of 
usability work when developing software in the automation industry. Journal of Systems and Software, 
88, 54-73. 

[P360]  Ribeiro,  J.  C.  B.,  Zenha-Rela,  M.  A.,  de  Vega,  F.  F.  (2009).  Test  case  evaluation  and  input 
domain reduction strategies for the evolutionary testing of object-oriented software. Information and 
Software Technology, 51(11), 1534-1548. 

 
 
GitHub Marketplace for Practitioners and Researchers to Date 

61 

[P361]  Chang, T. K.,  Hwang, G. H. (2007). The design and implementation of an application program 

interface for securing XML documents. Journal of Systems and Software, 80(8), 1362-1374. 

[P362]  Pernstål,  J.,  Feldt,  R.,  Gorschek,  T.  (2013).  The  lean  gap:  A  review  of  lean  approaches  to 

large-scale software systems development. Journal of Systems and Software, 86(11), 2797-2821. 

[P363]  Hotomski, S.,  Glinz, M. (2019). GuideGen: An approach for keeping requirements and acceptance 
tests aligned via automatically generated guidance. Information and Software Technology, 110, 17-38. 
[P364]  George, B.,  Williams, L. (2004). A structured experiment of test-driven development. Informa- 

tion and software Technology, 46(5), 337-342. 

[P365]  WOO, Gyun, CHAE, Heung Seok, CUI, Jian Feng, et al. Revising cohesion measures by consid- 
ering the impact of write interactions between class members. Information and Software Technology, 
2009, vol. 51, no 2, p. 405-417. 

 
 
