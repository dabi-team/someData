Anchoring Code Understandability Evaluations Through Task
Descriptions

Marvin Wyrich
marvin.wyrich@iste.uni-stuttgart.de
University of Stuttgart
Stuttgart, Germany

Lasse Merz
lasse.merz@web.de
University of Stuttgart
Stuttgart, Germany

Daniel Graziotin
daniel.graziotin@iste.uni-
stuttgart.de
University of Stuttgart
Stuttgart, Germany

2
2
0
2

r
a

M
5
2

]
E
S
.
s
c
[

1
v
5
0
7
3
1
.
3
0
2
2
:
v
i
X
r
a

ABSTRACT
In code comprehension experiments, participants are usually told
at the beginning what kind of code comprehension task to expect.
Describing experiment scenarios and experimental tasks will influ-
ence participants in ways that are sometimes hard to predict and
control. In particular, describing or even mentioning the difficulty
of a code comprehension task might anchor participants and their
perception of the task itself.
In this study, we investigated in a randomized, controlled exper-
iment with 256 participants (50 software professionals and 206
computer science students) whether a hint about the difficulty of
the code to be understood in a task description anchors participants
in their own code comprehensibility ratings. Subjective code evalu-
ations are a commonly used measure for how well a developer in a
code comprehension study understood code. Accordingly, it is im-
portant to understand how robust these measures are to cognitive
biases such as the anchoring effect.
Our results show that participants are significantly influenced by
the initial scenario description in their assessment of code com-
prehensibility. An initial hint of hard to understand code leads
participants to assess the code as harder to understand than partic-
ipants who received no hint or a hint of easy to understand code.
This affects students and professionals alike. We discuss examples
of design decisions and contextual factors in the conduct of code
comprehension experiments that can induce an anchoring effect,
and recommend the use of more robust comprehension measures
in code comprehension studies to enhance the validity of results.

CCS CONCEPTS
• Software and its engineering → Software organization and
properties; • Human-centered computing → Empirical studies
in HCI .

KEYWORDS
code comprehension, anchoring effect, empirical study design, soft-
ware metrics

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICPC 2022, May 21–22, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00

ACM Reference Format:
Marvin Wyrich, Lasse Merz, and Daniel Graziotin. 2022. Anchoring Code
Understandability Evaluations Through Task Descriptions. In ICPC ’22:
IEEE/ACM International Conference on Program Comprehension, May 21–22,
2022, Pittsburgh, Pennsylvania, United States. ACM, New York, NY, USA,
8 pages.

1 INTRODUCTION
Thinking about what you tell participants at the beginning of your
study matters — at least if your concern is to produce a study
design with high validity and reduce the probability of biased results.
Consider the following scenario. Caroline is a developer who wants
to take part in an advertised study to research the influence of code
comments on source code comprehensibility. On site, the study
leader explains the study process to her. She would have to look at
a source code snippet and rate its comprehensibility. Caroline is a
little nervous, but the study leader instinctively reassures her that
the code will not be too difficult to understand. The study leader has
good intentions here, yet there might be unattended consequences
for this action.

The reader might start seeing, at this point, the internal valid-
ity of the fictitious study threatened, since the assessment of code
comprehensibility was most likely influenced at this moment by
the words of the study leader. The introduction to a study should
strictly follow a predefined script. The Standards for Educational
and Psychological Testing enlist several recommendations to as-
semble and present instructions to administer a test, including the
instructions presented to test takers so that “it is possible for others
to replicate the administration conditions under which the data on
reliability, validity [...] were obtained” [2][p. 90]. Additionally, by
following a predefined script, experimenter expectancies can be
avoided [31]. We share this sentiment, and yet, from the researchers’
perspective, it does not always turn out to be that simple to control
for any external influences on subjective assessments.

Subjective code comprehension assessments are a common mea-
sure in code comprehension studies because of their simplicity [19].
At the same time, we know from more than forty years of research
on the anchoring effect [12, 30] that even inconspicuous environ-
mental factors are sufficient to influence people in their estima-
tion [8]. The anchoring effect denotes that an initial value is insuf-
ficiently adjusted so that “different starting points yield different
estimates, which are biased toward the initial values” [30]. It is
one of the most robust cognitive biases [12] and, in the context of
software engineering, the most studied [16].

A recent study by Wyrich et al. [33] showed that a single code
comprehensibility metric next to the source code being evaluated is

 
 
 
 
 
 
ICPC 2022, May 21–22, 2022, Pittsburgh, PA, USA

Wyrich et al.

sufficient to significantly anchor developers in their evaluation of
the code. So far, however, we do not know whether these findings
are also applicable to scenario descriptions presented to experi-
mental participants before the actual code comprehension task and
whether prior information about the code snippet leads to anchor-
ing participants in their code comprehension assessments.

For the design of code comprehension studies, confirmation of
such an effect would imply that subjective ratings should only be
used when contextual factors can be controlled for with a high de-
gree of certainty for all participants, thus minimizing the risk of an-
choring individual participants and biasing the experiment results.
In any case, additional insights on the influence of scenario descrip-
tions on subjective code comprehension ratings provide a useful
basis for design decisions, and can partially counteract existing
uncertainty about what constitutes a good empirical study [27, 28].
For these reasons, we conduct a controlled experiment with 256

participants to investigate the following research question:

Does specific information available in advance about
a code snippet influence developers in their subjective
assessment of the code’s comprehensibility?

The following section summarizes related work on the anchoring
effect, in and outside of software engineering. In section 3, we
describe our research design, the underlying conceptual model, and
how we analyzed the collected data. Sections 4 and 5 present and
discuss the results to the research question and its implications.
Section 6 concludes the paper.

2 RELATED WORK
Cognitive biases refer to systematic deviations from optimal deci-
sions and judgment, which can potentially jeopardize the success
of a project [22]. In a systematic mapping study, Mohanani et al.
[16] show that cognitive biases can be found in many forms in
software engineering: 37 different cognitive biases were identified
in 65 articles. The most frequently studied in the SE literature is
the anchoring effect.

In a recent field study, Chattopadhyay et al. [5] found that de-
velopment is often disrupted by cognitive biases, such that biased
actions are significantly more likely to be reversed later. Actions
associated with cognitive biases of the fixation category, which
includes the anchoring effect, were most likely to be reversed.

When Tversky and Kahneman [30] investigated the anchoring
effect, they demonstrated the effect by spinning a wheel of for-
tune with numbers between 0 and 100. The resulting number influ-
enced how high or low participants estimated the share of African
countries in the United Nations. Note that there is apparently no
relationship between the number on a wheel of fortune and UN
countries.

In the decades that followed, numerous other studies on the
anchoring effect were carried out [12], so that today we know,
for example, that environmental anchors can be much more subtle
without losing their effect [8]. In addition, the effect is not limited to
laymen, but experienced people are also anchored in corresponding
contexts [12].

It has also been shown that the anchor does not necessarily have
to be a number. For example, Allen and Parsons [1] found that
SQL queries can serve as anchors, and while developers in a query

formulation task are faster when modifying existing SQL queries
instead of rewriting them from scratch, solutions were as well less
accurate and overconfidence in the results increased.

While the anchoring effect generally receives ample attention,
studies to better understand its actual influence and occurrence in
the context of program comprehension experiments are still lacking.
Yet, this is precisely where these studies are needed. Code compre-
hension and code comprehensibility are often measured in code
comprehension studies by participants’ subjective ratings [19]. Po-
tential biases in these measures due to uncontrolled environmental
factors pose a validity threat.

Wyrich et al. [33] showed in a recent experiment that displaying
different values of a made-up code comprehensibility metric sig-
nificantly anchored study participants in their subjective ratings
of source code comprehensibility. Participants were assigned to
one of two groups and saw three code snippets one after the other,
which they had to understand and evaluate in terms of their under-
standability. All participants were shown a supposedly validated
metric next to the code snippets, which however displayed a dif-
ferent value depending on the treatment group (either 4 or 8) and
was intended to anchor the participants in this way. The observed
anchoring effect was significant (𝑝 < .01) with a large effect size
(𝑑 = −1.29).

The experiment by Wyrich et al. [33] is closest to ours, but differs
decidedly in some respects, which is why we do not refer to ours
as a replication study. Wyrich et al. [33] anchored the participants
by displaying the anchor, i.e., the numeric value for the metric,
during the code comprehension task, next to the code snippets,
thus much more prominently than we do. We study the anchoring
of experiment participants at the beginning of the study using a
description of the expected code snippet, hence, displayed before
showing the coding snippet. Our choice is based on the limitation
discussed by Wyrich et al. [33] that showing the anchoring element
during the code comprehension task might be unusual. We sought
to verify that the results still hold true when we were closer to
realistic scenarios, some of which we discuss in this paper.

Other differences in design characteristics between the two stud-
ies are outlined in Table 1. Nevertheless, similar to a reproduction
study1, we build on the work by Wyrich et al. [33] to make both
studies as comparable as possible and thus contribute to a common
overall picture.

In summary, the anchoring effect is present in software engi-
neering and its investigation follows not only current efforts to
debias software engineering practice eventually [5, 16, 22], but also
to identify confounding factors in the context of scientific studies
and thus enable the design of valid studies.

3 METHODOLOGY
The goal of our study is to investigate the effect of the presence
and content of information about a code snippet at the beginning
of a scientific study on the participant’s rating of the comprehensi-
bility of that code snippet. To this end, we formulated the research
question given in the introduction.

1https://www.acm.org/publications/policies/artifact-review-badging

Anchoring Code Understandability Evaluations

ICPC 2022, May 21–22, 2022, Pittsburgh, PA, USA

Table 1: Comparison of experiment characteristics with the study by Wyrich et al. [33]

Wyrich et al. [33]

Our study

Scenario presentation
Metric values (scenarios)
Comprehension task
Participants
Setting

next to code snippet, metric only
4, 8
determine output and rate understandability
43 students
onsite, code shown on screen

prior to code snippet, metric and snippet details
3, 8, none
rate understandability
206 students, 50 professionals
remote, code shown on screen

3.1 Research Design
We conducted a controlled and randomized between-subjects ex-
periment with 3x2 factorial design. Each participant was assigned
to one of three scenario groups and had to understand one of two
code snippets. A schematic representation of the research design is
provided in Fig. 1.

The experiment took place online via a self-hosted instance of
LimeSurvey, and participation was entirely anonymous. Partici-
pants first confirmed that their consent to participate was informed
and agreed to the data and privacy policy. On the next page, partic-
ipants saw a description of the task that awaited them in the next
step. Depending on the randomly assigned group, this description
included information about the code comprehensibility rating of a
fictitious expert system. Then, each participant had to understand
one randomly selected code snippet and provide their code com-
prehensibility rating. Finally, a demographic data questionnaire
concluded the study.

3.2 Experimental Materials
Scenarios. All participants saw a textual scenario description
3.2.1
of the expected code comprehension task. The wording of this
description was the same for all three groups, but one of three
paragraphs was not shown to the control group, and there was a
hint to Treatment Groups 1 and 2 for a value of either 3 (easy) or
an 8 (hard) as a comprehensibility score by the expert system. The
description was as follows:

You will look at a Java code snippet in a bit. The only
information that we provide you about the snippet is
the following: The code snippet is between 20 and 30
lines long and tests a String, a sequence of characters,
for a criterion.

⟨⟨Begin Treatment Group text snippet⟩⟩
We developed an expert system to rate the under-
standability of code based on multiple metrics. This
system rated the code snippet you will look at in a
few moments as
⟨⟨Treatment Group 1⟩⟩ a 3 out of 10.
⟨⟨Treatment Group 2⟩⟩ an 8 out of 10.
The system uses a scale from 1 to 10, where 1 is very
easy and 10 is very hard to understand.
⟨⟨ End Treatment Group text snippet⟩⟩

Your only task is to judge its understandability on a
scale from 1 to 10, where 1 is very easy to understand
and 10 is very hard to understand. The code is fully

functioning and bug-free. You will have unlimited
time to look at the code snippet. Feel free to rate
the code snippet whenever you think you have an
adequate impression to judge its understandability.

3.2.2 Code Snippets. Since the influence of a scenario description
on anchoring in source code comprehensibility ratings might de-
pend on the actual difficulty of the code snippet, we were interested
in studying both an easy and a hard snippet in the study.

Every developer has a slightly different idea of how understand-
able certain code is. Therefore, we pre-selected five Java code snip-
pets and invited eight software developers to assess their compre-
hensibility. The pre-selected snippets all met the criteria that no
domain knowledge is necessary for understanding and that they are
neither too long to be displayed in full on a screen nor too trivial to
be understood after just a few seconds. In pre-selecting functions
that are rather complex, we, like Wyrich et al. [33], used the cogni-
tive complexity metric [4], which has been shown to correlate in
particular with subjective evaluations by developers [17].

Through this preliminary evaluation, we were able to identify
one easy and one difficult code snippet that we subsequently used
in our study. The easy code snippet is a method from the apache
commons-lang StringUtils class and checks if a given character
sequence contains both uppercase and lowercase characters. The
difficult code snippet is the solution to a coding challenge [32] to
find the longest palindromic substring within a string. Both code
snippets are included in the supplemental materials.

3.2.3 Questionnaire. The experiment was completed by a short
demographic questionnaire, which asked about the current main
occupation of the participant. The choices were Student, IT pro-
fessional, Researcher and Other (with the possibility to specify the
occupation). We then clarified the specific intent of our study and
again listed ways to contact us with potential questions or com-
ments.

3.3 Participants
We invited a convenience sample of software professionals and
computer science university students to take part in our study. To
disseminate the invitation, we used social media and asked personal
contacts to draw attention to the study in their software companies.
Computer science university students (software engineering cur-
riculum, BSc and MSc) were asked to participate in the study, for
example to fulfill course requirements to participate in scientific
studies.

We assured the participants of anonymity, and they could drop
out of the study at any time or not participate at all and still fulfill

ICPC 2022, May 21–22, 2022, Pittsburgh, PA, USA

Wyrich et al.

Figure 1: Schematic representation of the research design. Participants are randomly assigned to one of three groups that
provide different information about the code snippet to be understood next. Each participant then randomly sees exactly one
from a pool of two snippets, either an easy one or a difficult one. A survey that is the same for all concludes the experiment.

their requirements. The only requirement for participation was a
basic understanding of the Java programming language.

We encouraged participation with the low amount of time com-
mitment of 10 to 15 minutes for the whole study. Furthermore, we
pledged to donate €5 to a good cause for every participant among
the software professionals that completed the study. Subjects could
choose between three charity projects on different topics, or split
the donation evenly among the three projects.

Following the goal-setting theory of motivation [15], we also
invited participants with the clear goal of assessing the compre-
hensibility of a particular code snippet. Setting such a specific and
challenging goal may lead to increased effort and persistence, which
is desirable for completing the study. Our purpose in doing so was
to pique the interest of developers who want to demonstrate their
ability to understand code.

3.4 Conceptual Model
We aim to build on the study by Wyrich et al. [33] by developing
the conceptual model of Figure 2, which summarizes all variables
and their hypothesized relationships graphically. We investigate,
like Wyrich et al. [33], whether the anchoring effect is confirmed in
the form that a Scenario description, which controls for information
presented at the beginning of the study about the code snippet to
be understood, influences Perceived Code Comprehensibility (H1). In
our model, we introduce two other factors that we theorize to have
an effect on Perceived Code Comprehensibility. The Code Snippet
Difficulty will influence the Perceived Code Comprehensibility (H2).
Wyrich et al. [33] selected the code snippets for their study to be
of comparable complexity to control for the potential influence of
snippet complexity in this way. We appreciate this design decision,
but we want to understand the extent to which an easy or a hard
task affects the Perceived Code Comprehensibility, and we argue
that this influence will be stronger than the one provided by the
Scenario.

Furthermore, Wyrich et al. [33] had only students as participants
in their study. Much discussion has happened in the literature on

Figure 2: Conceptual model for the study. Dashed arrows
represent moderators.

Table 2: Assignable values for the variables of the model.

Variable

Values

Scenario
Code Snippet Difficulty
Role
Perceived Code Comprehensibility

{baseline, easy, hard}
{easy, hard}
{student, professional}
[1.. 10]

differences between students and professionals [11] in terms of pro-
ductivity, performance, and software quality, with some claiming
or finding that there are little to none [26, e.g.], others that there
are [29, e.g.]. Wyrich et al. [33] themselves discuss this circum-
stance as a potential limitation of their study, although literature
suggest that the anchoring effect is not restricted to inexperienced
people [12, 30]. We avoid any debate and investigate whether a Role
influences the Perceived Code Comprehensibility (H3).

Control GroupTreatment Group 1Treatment Group 2... Java snippet ... 20 to 30 lines ...... Java snippet ... 20 to 30 lines ...rated 3/10 difficulty to understand ...... Java snippet ... 20 to 30 lines ...rated 8/10 difficulty to understand ...Easy CodeHard CodeSurveyH12H2Code SnippetDifﬁcultyH13H3RoleH1ScenarioPerceived CodeComprehensibilityAnchoring Code Understandability Evaluations

ICPC 2022, May 21–22, 2022, Pittsburgh, PA, USA

Finally, given the absence of prior literature, we want to test for
the influence that these factors have with each other. The interac-
tion between Scenario and Code Snippet Difficulty (H12) as well as
the interaction between Scenario and Role (H13) are further mod-
eled as moderators2 on the influence that Scenario has on Perceived
Code Comprehensibility. Investigating the moderators will enable
us to better characterize a potential anchoring effect of Scenario
on Perceived Code Comprehensibility.

Table 2 provides a summary of the values that can be assigned to
each of the four variables in the conceptual model. Due to the small
number of six self-identified researchers, we decided to combine
them with the 44 software professionals into one group for the
analysis. We still consider that a meaningful distinction can be
made between students and professionals. Nevertheless, we will
discuss the potential consequences of this design decision in 5.2.

3.5 Analysis Procedure
The data violated at least one assumption of most of the commonly
used modelling techniques3, and it also presented evidence for non-
normality on the dependent variable (Shapiro-Wilk Test, 𝑊 = 0.97,
𝑝 < .00001).

We thus opt for a Partial Least Squares Structural Equation Model
(PLS-SEM). PLS-SEM was recently introduced to the discipline
by Russo and Stol [25] as part of the SEM statistical technique fam-
ily for causal-predictive approaches in the behavioral sciences. We
direct readers to Russo and Stol’s work [25] for an introduction
and overview but, in short, PLS-SEM is suited for testing a theoret-
ical framework from a prediction perspective, when the structural
model is complex, and when distribution issues are a concern [14].
We model the factors of Figure 2, as defined in 3.4, as a PLS-
SEM in R 4.1.2 [21] and SEMinR 2.2.1 [23]. The output will provide
us with statistics on the predictive power and significance of the
relationship between the factors4.

4 RESULTS
We recruited 256 participants, of which 206 were students and 50
were professional software developers (see Section 3.3).

Table 3 provides descriptive statistics for the Perceived Code
Comprehensibility (PCC) grouped by factorial assignment condi-
tion and role. Students and professionals were overall very close or
identical in their median PCC for the same scenario and code snip-
pet. The code snippet we considered easy was actually perceived by
participants as easier to understand than the snippet predicted to
be perceived harder to understand, regardless of scenario and role.
Participants provided the highest median PCC ratings for the com-
bination of hard scenario and hard code snippet (a median value
of 7 and 7.5 from students and professionals). For the easy code
snippet, the scenario seems to have had a smaller overall impact
on the PCC ratings.

2A variable w is called a moderator when the relationship between other two variables
A and B is influenced by w, and it is modelled as interaction effect [7].
3Including linear regression models, methods from the various ANOVA families with
applied data transformation techniques, and ordinal logistic regression methods. The
latter could not be applied because of proportional odds violation.
4Readers familiar with SEM will notice that we are applying PLS-SEM as an analysis
technique to estimate single-item indicators. That is, we rely on PLS-SEM robustness to
perform a “regression job” instead of using the technique for its intended psychometric
purposes. We elaborate on these issues in the limitations section.

Table 3: Descriptive statistics and group assignment for the
study. CSD = Code Snippet Difficulty, n = group size, PCC =
Perceived Code Comprehensibility, M = mean, SD = standard
deviation, Mdn = median.

CSD Role

n

PCC M PCC SD PCC Mdn

Scenario: baseline

easy

hard

student
professional

student
professional

Scenario: easy

easy

hard

student
professional

student
professional

Scenario: hard

easy

hard

student
professional

student
professional

35
10

32
10

25
9

33
5

32
2

49
14

4.0
3.3

5.25
5.6

2.56
2.89

4.67
6.0

3.25
1.0

6.22
6.86

3.33
2.79

2.05
2.88

1.94
2.67

1.49
2.65

2.90
0.0

2.05
2.35

2
2

5
6

2
2

4
7

2
1

7
7.5

In Table 4 we show the results of the statistical analysis for each
hypothesis. We find a significant path from Scenario to Perceived
Code Comprehensibility (path coefficient5 𝛽 = 0.165, 𝑝 < .001) con-
firming the presence of the anchoring effect. Code snippet difficulty
had the expected strong influence on PCC (𝛽 = 0.418, 𝑝 < .001),
but, furthermore, seems not to be a moderator of the relationship
between Scenario and PCC (𝛽 = 0.076, 𝑝 > .10). The paths from
Role to PCC and the moderating effect of Role on the path from
Scenario to PCC were insignificant. Thus, our results support H1
and H2; they do not support H12, H3 and H13.

The model explains 𝑅2 = .223 of the variance in Perceived Code
Comprehensibility. Bootstrapped heterotrait-monotrait ratio of cor-
relations (HTMT) are all below zero, bootstrapped loadings and
weights are all approximately 1.00, all variance inflation factors
(VIF) are 1.00, and all reliability coefficients are 1.00. All this is
expected with single item constructs that are assumed to be fully
independent.

5 DISCUSSION
We can answer the research question whether specific information
available in advance about a code snippet influences developers

5Path coefficients are expressed as standardized regression coefficients in terms of
standard deviations; see, e.g., [13].

ICPC 2022, May 21–22, 2022, Pittsburgh, PA, USA

Wyrich et al.

Table 4: Estimated and bootstrapped path coefficients with 95% CI, model explanatory power expressed as R^2 and adjusted
R^2.
∗ = 𝑝 ≤ .10, ∗∗ = 𝑝 ≤ .01, ∗ ∗ ∗ = 𝑝 ≤ .001.

Perceived Code Comprehensibility Bootstrap Mean Bootstrap SD T Stat.

95% CI

H1: Scenario
H2: Code Snippet Difficulty
H3: Role
H12: Scenario*Code Snippet Difficulty
H13: Scenario*Role
R^2
AdjR^2

0.165***
0.418***
0.033
0.076
-0.013
0.223
0.208

in their subjective assessment of code’s comprehensibility as fol-
lows: The anchoring effect is, once again, significant. Information
about the expected complexity of a code snippet to be understood
influences developers in their supposedly independent code eval-
uations. Students and professionals are equally affected, which is
in line with the investigation of Wyrich et al. [33] who measured
programming experiences instead of role, and found programming
experience to likely not play a role in anchoring. The finding is
further in line with the body of literature on the anchoring effect,
which states that the anchoring effect is not limited to laymen or
those inexperienced in an activity [12, 30].

The choice of code snippet also has a significant impact on per-
ceived code comprehensibility. While this is not too surprising, nor
is it the central point of our work, it is still good to have data points
that show that the choice of code snippets for code comprehension
studies should not be underestimated. Studies seeking to ensure
that code snippets of different tasks are of comparable difficulty
should invest effort in, e.g., a pilot study to evaluate appropriate
snippets.

What is interesting for the characterization of the observed an-
choring effect, however, is that the complexity of a code snippet did
not have an influence on the strength of the anchoring effect (H12).
Based on our results, we suspect that a snippet must be complex
enough so that PCC ratings do not concentrate too much on the
lower end of the scale. Apart from that, the actual complexity of a
snippet does not seem to be too important for anchoring.

5.1 Implications
We know, regarding the anchoring effect, that the specific anchor
can take many forms and does not necessarily always have anything
to do with the situation being assessed [12, 30]. When designing
and conducting code comprehension studies, researchers have a lot
of freedom and just as much potential to inadvertently set anchors.
In the introduction, we described a scenario in which the instructor
mentioned, to encourage the participant, that the code snippets to
be assessed were not too difficult to understand. Similarly, however,
even in an online experiment without a human instructor, many
examples of potential anchors can be found.

For example, explicitly mentioning in a study description that
it is a study for novice programmers could lead to code snippets
being rated as easier to understand. Communicated time limits
for the code comprehension tasks can convey to participants how

0.169
0.418
0.031
0.074
-0.017

0.049
0.063
0.058
0.055
0.058

3.357
6.673
0.564
1.384
-0.228

[0.071, 0.265]
[0.293, 0.541]
[-0.08, 0.148]
[-0.034, 0.181]
[-0.124, 0.096]

complex the tasks are supposed to be. A validation study of a code
comprehensibility metric that displays the metric to developers
and asks whether they agree with it or how they would assess the
code instead already anchored them in their own judgments, as our
study and that of Wyrich et al. [33] have empirically demonstrated.
One can easily find further examples in which study participants
would be anchored. We see great research potential to empirically
investigate these scenarios and to find solutions for affected stud-
ies, which for example cannot disregard subjective evaluations
because they are relevant for their research intentions. Yet, for
all study designs that allow to dispense with subjective assess-
ments, we recommend more reliable and objective measurements
to draw conclusions about how well a developer has understood
code. Corresponding tasks and measures are available for this pur-
pose [9, 10, 19].

5.2 Limitations
With our design, we were able to overcome a number of limitations
discussed in the work by Wyrich et al. [33]. Our experiment had a
much larger and more heterogeneous sample of developers, a less
prominent presentation of the anchor, and we had a control group
that allowed us to measure the perceived code comprehensibility
for participants who were not explicitly anchored. Still, the results
of our study should be seen in the light of some limitations.

A confounding factor for the assessment of the code snippets’
understandability might be diverse understandings of what consti-
tutes comprehensible code between the participants. We discussed,
while designing the study, whether to provide a definition of under-
standability at the beginning of the survey. However, since there
exists neither an agreement in the literature nor does it seem real-
istic that developers all share the same view on understandability,
we decided against it. It might be possible that participants in one
group share similar views on what constitutes understandable code,
while participants in other groups differ. On the other hand, the
randomized assignment of participants to scenario and code snippet
should have mitigated this threat.

Regarding the generalizability of our results, we would like to
emphasize that the scope clearly lies on the evaluation of individual
code snippets and participants applied a comprehension process
commonly referred to as bottom-up code comprehension [18]. We
see much value in reproductions of our experiment with larger
software systems to be assessed.

Anchoring Code Understandability Evaluations

ICPC 2022, May 21–22, 2022, Pittsburgh, PA, USA

We conducted our experiment remotely to make the study acces-
sible to as many developers as possible and to minimize contacts
due to the pandemic. The context in which the participants took
part in the study could therefore not be controlled, which could
have caused individual participants to be distracted during the con-
duct of the study or not to complete the study conscientiously. We
do not know what influence a potential distraction can have on
a code evaluation, but at least we think that the activity can be
resumed after an interruption. Participants who spent less than 20
seconds viewing and rating the code snippet’s understandability
were excluded from the analysis (average time for this task was
around four minutes; a total of 10 participants were excluded based
on this criterion).

Apart from this, running the experiment remotely worked well,
and we are pleased to have recruited 50 software professionals as
participants in addition to students. We have previously disclosed
that this experiment group included both software professionals
and six participants who identified themselves as researchers. This
could be seen as a threat to construct validity, since software de-
velopment experience might be less pronounced among software
engineering researchers than among full-time software engineering
professionals.

We assume that researchers in computer science, who hold either
a related MSc or a PhD, have comparable experience compared to
software professionals to be able to deal with our tasks. Yet, we
repeated the statistical analysis without the six researchers, and
there was no change in the significance of the results. The path
coefficients for the significant hypotheses would be 𝛽_𝐻1 = 0.161
and 𝛽_𝐻2 = 0.408 (change ≤ 0.10).

Further, when designing the experiment, we took care to mini-
mize the time required for participation to achieve a correspond-
ingly high response rate. This is the practical reason why we did
not measure actual code comprehension with additional tasks. Fur-
thermore, Wyrich et al. [33] found evidence for a strong anchoring
effect in perceived code comprehensibility ratings, but also that
actual code comprehension was not affected by the anchoring ef-
fect. We assume that by our random assignment of a sample six
times larger than that of Wyrich et al. [33] the absence of effect
anchoring/actual understanding still holds.

As anticipated in Section 3.5, we make use of PLS-SEM in ways
that reduce it to a simpler regression modelling tool with single-item
constructs. That is, we use PLS-SEM to understand the relationship
between variables rather than typical reflective and/or formative
constructs in structural and measurement models. As disclosed
in the same section, we were driven to this choice because we
preferred not to rely on robustness against assumption violations
of other analysis and regression tools. PLS-SEM has just a few
assumptions that our data did not violate, and it suited our needs.
Furthermore, we tested our results with an alternative model
specification and report coherent results. We fit an equivalent mul-
tilevel mixed effects model with lme4 1.1-27.1 [3]. Relying on its
robustness against non-randomness of residuals, which is the case
of our data, we find that (1) the same path coefficients that are found
as significant in our PLS-SEM are also significant in the multilevel
mixed effects model, (2) the model provides comparable explana-
tory power (𝑅2 = .22 and adjusted 𝑅2 = .22). We are thus confident
in our chosen analysis tools and its results. For elaborating on the

limitations, we opted for multilevel mixed effects models based
on Clark’s report [6] on their positive comparability with SEM. We
still opted not to use multilevel mixed effects models for our main
analysis because of cautiousness: the residuals are not randomly
distributed.

The other related issue lies in using single item scales to rep-
resent our underlying constructs. PLS-SEM allows for single item
constructs, and it is also used this way in information systems re-
search [20]. This use is, however, encouraged only when no other
alternatives are available since psychometric properties of single
item constructs are subject of debate of providing low psychometric
validity and low reliability [20, 24]. We argue that we fall within
the recommendation of employing PLS-SEM for single item con-
structs. First, our independent variables are experimental groups
represented by either two or three ordinal or categorical options.
Furthermore, they provide highly observable validity and reliability
(e.g., role = student indeed represents students, and scenario
= easy really represents the easy scenario). Only the dependent
variable, Perceived Code Comprehensibility, is operationalized by
a single item, and the results might indeed suffer from low psycho-
metric validity and reliability.

The Perceived Code Comprehensibility is a variable of which
scale we inherited by reproducing Wyrich et al.’s study [33]. This
was a deliberate choice to be able to compare results and build on
their study. A psychometrically validated multi-item scale to assess
the Perceived Code Comprehensibility would allow for a richer and
better explanation for the variance therein and offer (more) valid
and reliable interpretation of its values. Such a scale, alas, does not
exist, to the best of our knowledge. We report on this issue that is
shared by most, if not all, studies that investigate Perceived Code
Comprehensibility and call for future research on more robust ways
to assess it.

6 CONCLUSION
The conduction of program comprehension studies with human
participants all usually start in the same way: with an introduction
of the participants to the study context. We have shown in a con-
trolled experiment with 256 participants that a subtle cue about
the difficulty of the code to be understood anchors participants in
their subjective assessments of code comprehensibility. This result
is consistent with prior research and is one reason to design code
comprehension studies somewhat differently in the future, as many
studies today still rely on subjective assessments of source code as
a proxy for how well a participant understood code. Such a com-
prehension measure is tempting because it provides an indicator of
code understanding in a relatively time efficient manner. However,
it is just as easy for this assessment to be biased by contextual fac-
tors, which is why we recommend the use of more robust measures
such as the correctness of comprehension questions.

Future work should explore ways to predict and mitigate the
anchoring effect as a confounding factor in code comprehension
studies interested in perceived code comprehensibility. Debiasing
software engineering through empirical studies is only possible
if we also debias empirical studies themselves. On a related note,
we think it is worthwhile to investigate other, even more subtle,
potential anchors. In section 5.1, we have discussed a few examples

ICPC 2022, May 21–22, 2022, Pittsburgh, PA, USA

Wyrich et al.

[18] Michael P. O’Brien, Jim Buckley, and Teresa M. Shaft. 2004. Expectation-based,
inference-based, and bottom-up software comprehension. Journal of Software
Maintenance and Evolution: Research and Practice 16, 6 (2004), 427–447. https:
//doi.org/10.1002/smr.307

[19] Delano Oliveira, Reydne Bruno, Fernanda Madeiral, and Fernando Castor. 2020.
Evaluating Code Readability and Legibility: An Examination of Human-centric
Studies. In 2020 IEEE International Conference on Software Maintenance and Evo-
lution (ICSME). IEEE, 348–359.

[20] Maria Petrescu. 2013. Marketing research using single-item indicators in struc-
Journal of Marketing Analytics 1, 2 (2013), 99–117.

tural equation models.
https://doi.org/10.1057/jma.2013.7

[21] R Core Team. 2021. R: A Language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.
org/

[22] Paul Ralph. 2011. Toward a theory of debiasing software development. In Eu-

roSymposium on Systems Analysis and Design. Springer, 92–105.

[23] Soumya Ray, Nicholas Patrick Danks, and André Calero Valdez. 2021. seminr:
Building and Estimating Structural Equation Models. https://CRAN.R-project.
org/package=seminr R package version 2.2.1.

[24] Ringle, Sarstedt, and Straub. 2012. Editor’s Comments: A Critical Look at the
Use of PLS-SEM in “MIS Quarterly”. MIS Quarterly 36, 1 (2012), iii. https:
//doi.org/10.2307/41410402

[25] Daniel Russo and Klaas-Jan Stol. 2021. PLS-SEM for Software Engineering
Research: An Introduction and Survey. ACM Computing Surveys (CSUR) 54, 4
(2021), 1–38.

[26] Iflaah Salman, Ayse Tosun Misirli, and Natalia Juristo. 2015. Are students represen-
tatives of professionals in software engineering experiments?. In 2015 IEEE/ACM
37th IEEE International Conference on Software Engineering (ICSE), Vol. 1. IEEE,
666–676.

[27] Janet Siegmund. 2016. Program comprehension: Past, present, and future. In
2016 IEEE 23rd International Conference on Software Analysis, Evolution, and
Reengineering (SANER), Vol. 5. IEEE, 13–20.

[28] Janet Siegmund, Norbert Siegmund, and Sven Apel. 2015. Views on internal and
external validity in empirical software engineering. In 2015 IEEE/ACM 37th IEEE
International Conference on Software Engineering (ICSE), Vol. 1. IEEE, 9–19.
[29] Dag IK Sjoberg, Bente Anda, Erik Arisholm, Tore Dyba, Magne Jorgensen, Amela
Karahasanovic, Espen Frimann Koren, and Marek Vokác. 2002. Conducting realis-
tic experiments in software engineering. In Proceedings international symposium
on empirical software engineering (ISESE). IEEE, 17–26.

[30] Amos Tversky and Daniel Kahneman. 1974. Judgment under uncertainty: Heuris-

tics and biases. science 185, 4157 (1974), 1124–1131.

[31] Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, and
Anders Wesslén. 2012. Experimentation in software engineering. Springer Science
& Business Media.

[32] Marvin Wyrich, Daniel Graziotin, and Stefan Wagner. 2019. A theory on individ-
ual characteristics of successful coding challenge solvers. PeerJ Computer Science
5 (2019), e173.

[33] Marvin Wyrich, Andreas Preikschat, Daniel Graziotin, and Stefan Wagner. 2021.
The Mind Is a Powerful Place: How Showing Code Comprehensibility Metrics
Influences Code Understanding. In 2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE). IEEE, 512–523.

that could affect not only code comprehension studies, but empirical
studies in software engineering in general.

7 DATA AVAILABILITY
We publicly provide the code snippets used, the data collected, and
the analysis script as supplementary material at https://doi.org/10.
5281/zenodo.5877313.

ACKNOWLEDGMENTS
We are grateful for the many participants who volunteered to take
part in our study. We thank three anonymous reviewers for their
constructive and insightful feedback.

REFERENCES
[1] Gove Allen and B Jeffrey Parsons. 2006. A little help can be a bad thing: Anchor-
ing and adjustment in adaptive query reuse. 2006 International Conference on
Information Systems (ICIS), 1–21.

[2] American Educational Research Association, American Psychological Associa-
tion, National Council on Measurement in Education, and Joint Committee on
Standards for Educational and Psychological Testing (U.S.). 2014. Standards for ed-
ucational and psychological testing. American Educational Research Association,
Washington, DC.

[3] Douglas Bates, Martin Mächler, Ben Bolker, and Steve Walker. 2015. Fitting
Linear Mixed-Effects Models Using lme4. Journal of Statistical Software 67, 1
(2015), 1–48. https://doi.org/10.18637/jss.v067.i01

[4] G. Ann Campbell. 2018. Cognitive Complexity: An Overview and Evaluation.
In Proceedings of the 2018 International Conference on Technical Debt (TechDebt)
(Gothenburg, Sweden) (TechDebt ’18). Association for Computing Machinery,
New York, NY, USA, 57–58. https://doi.org/10.1145/3194164.3194186

[5] Souti Chattopadhyay, Nicholas Nelson, Audrey Au, Natalia Morales, Christopher
Sanchez, Rahul Pandita, and Anita Sarma. 2020. A tale from the trenches: cog-
nitive biases and software development. In Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering. Association for Computing
Machinery, New York, NY, USA, 654–665.

[6] Michael Clark. 2016. SEM vs. Mixed. Technical Report. University of Michigan. 1
pages. https://m-clark.github.io/docs/mixedModels/growth_vs_mixed_old.html
[7] Jacob Cohen, Patricia Cohen, Stephen G. West, and Leona S. Aiken. 2013. Ap-
plied Multiple Regression/Correlation Analysis for the Behavioral Sciences (3 ed.).
Routledge, New York, NY, USA. 536 pages.

[8] Clayton R Critcher and Thomas Gilovich. 2008. Incidental environmental anchors.

Journal of Behavioral Decision Making 21, 3 (2008), 241–251.

[9] Sarah Fakhoury. 2018. Moving towards objective measures of program compre-
hension. In Proceedings of the 2018 26th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE). Association for Computing Machinery, New York, NY, USA, 936–939.
[10] Dror G Feitelson. 2021. Considerations and Pitfalls in Controlled Experiments
on Code Comprehension. In 2021 IEEE/ACM 29th International Conference on
Program Comprehension (ICPC). IEEE, 106–117.

[11] Robert Feldt, Thomas Zimmermann, Gunnar R Bergersen, Davide Falessi, Andreas
Jedlitschka, Natalia Juristo, Jürgen Münch, Markku Oivo, Per Runeson, Martin
Shepperd, et al. 2018. Four commentaries on the use of students and professionals
in empirical software engineering experiments. Empirical Software Engineering
23, 6 (2018), 3801–3820.

[12] Adrian Furnham and Hua Chu Boo. 2011. A literature review of the anchoring

effect. The journal of socio-economics 40, 1 (2011), 35–42.

[13] Joe F. Hair, Christian M. Ringle, and Marko Sarstedt. 2011. PLS-SEM: Indeed a
Silver Bullet. Journal of Marketing Theory and Practice 19, 2 (April 2011), 139–152.
https://doi.org/10.2753/mtp1069-6679190202

[14] Joseph F. Hair, Jeffrey J. Risher, Marko Sarstedt, and Christian M. Ringle. 2019.
When to use and how to report the results of PLS-SEM. European Business Review
31, 1 (2019), 2–24. https://doi.org/10.1108/ebr-11-2018-0203

[15] Edwin A Locke and Gary P Latham. 2002. Building a practically useful theory of
goal setting and task motivation: A 35-year odyssey. American psychologist 57, 9
(2002), 705.

[16] Rahul Mohanani, Iflaah Salman, Burak Turhan, Pilar Rodríguez, and Paul Ralph.
2018. Cognitive biases in software engineering: a systematic mapping study.
IEEE Transactions on Software Engineering 46, 12 (2018), 1318–1339.

[17] Marvin Muñoz Barón, Marvin Wyrich, and Stefan Wagner. 2020. An Empirical
Validation of Cognitive Complexity as a Measure of Source Code Understandabil-
ity. In Proceedings of the 14th ACM / IEEE International Symposium on Empirical
Software Engineering and Measurement (ESEM) (Bari, Italy). Association for Com-
puting Machinery, New York, NY, USA, Article 5, 12 pages.

