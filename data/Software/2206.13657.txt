DigiTac: A DIGIT-TacTip Hybrid Tactile Sensor for Comparing
Low-Cost High-Resolution Robot Touch

Nathan F. Lepora, Yijiong Lin, Ben Money-Coomes, John Lloyd

www.lepora.com/digitac, www.github.com/nlepora/digitac-design

1

2
2
0
2

l
u
J

8
1

]

O
R
.
s
c
[

2
v
7
5
6
3
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Deep learning combined with high-resolution tactile
sensing could lead to highly capable dexterous robots. However,
progress is slow because of the specialist equipment and expertise.
The DIGIT tactile sensor offers low-cost entry to high-resolution
touch using GelSight-type sensors. Here we customize the DIGIT
to have a 3D-printed sensing surface based on the TacTip family
of soft biomimetic optical tactile sensors. The DIGIT-TacTip
(DigiTac) enables direct comparison between these distinct tactile
sensor types. For this comparison, we introduce a tactile robot
system comprising a desktop arm, mounts and 3D-printed test
objects. We use tactile servo control with a PoseNet deep learning
model to compare the DIGIT, DigiTac and TacTip for edge- and
surface-following over 3D-shapes. All three sensors performed
their constructions led to
similarly at pose prediction, but
differing performances at servo control, offering guidance for
researchers selecting or innovating tactile sensors. All hardware
and software for reproducing this study will be openly released.

Index Terms—Force and Tactile Sensing, Open-source Robotics

I. INTRODUCTION

Advances in deep learning combined with innovation in
high-resolution tactile sensing give a plausible route to robots
with the dexterous capabilities of the human hand. Consider,
for example, the revolution in computer vision where 1000s of
researchers have ready access to deep learning infrastructure
and vast amounts of openly-shared visual data. In contrast,
research on robot touch for dexterity requires access to spe-
cialist equipment such as industrial arms integrated with tactile
sensing hardware. This need for specialised equipment and
expertise is a barrier to entering the ﬁeld and reproducing
research ﬁndings. Consequently, progress is slow and narrow
in scope from just a few research laboratories.

Researchers in Meta AI have developed the DIGIT as a
low-cost high-resolution tactile sensor [1]. They have open-
sourced the sensor design and in partnership with the company
GelSight commercially manufacture the sensor [2]. To lower
the barrier of entry to using machine learning (ML) with
tactile sensors like the DIGIT, an accompanying library of
ML models and functionality (PyTouch) was also released [3]
alongside a simulation environment (TACTO) [4]. The overall
concept is to provide a complete ecosystem for teaching robots
to perceive, understand and interact through touch [5].

For low-cost high-resolution robot touch to progress, there
needs to be a comparison of tactile sensing technologies and

NL and JL were supported by a Leadership Award from the Leverhulme

Trust on ‘A biomimetic forebrain for robot touch’ (RL-2016-39).

All authors are with the Department of Engineering Mathematics and

Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.

Corresponding author: n.lepora@bristol.ac.uk
Project website: www.lepora.com/digitac
Project repository: www.github.com/nlepora/digitac-design
Shared data: www.doi.org/10.5523/bris.110f0tkyy28pa2joru2pxxbrxd

Test System: Low-Cost Tactile Sensor on Desktop Arm

Figure 1: Left: the DIGIT-TacTip (DigiTac) mounted on an
affordable desktop robot arm (Dobot MG400, ∼£2k). Right:
close-up of sensor and a captured high-resolution tactile image.

a greater diversity of tactile sensing hardware, ML models
and test scenarios. The TacTip is a distinct high-resolution
optical tactile sensor with a 3D-printed biomimetic structure
of papillae that amplify the movement of markers on their
tips [6], [7], which also has body of research on using ML
models for tactile perception and control. In this paper, we
customize the DIGIT to have a TacTip skin to make the ﬁrst
direct comparison between the GelSight and TacTip families
of tactile sensors using common software and hardware. For
this comparison, we make use of recent progress in pose-based
servo control using ML models on tactile images [8]–[10].

Our main contributions are:

1) We introduce the DIGIT-TacTip (DigiTac) that combines
the DIGIT base (lighting, camera board, housing) with a
TacTip sensing module (3D-printed skin/mount, window, soft
gel). This raises challenges in adapting a design intended for
GelSight-type sensors to the TacTip sensing mechanism. In
particular, the DIGIT is designed for lateral illumination of
a ﬂat sensing surface, whereas the TacTip works best with a
curved skin under direct lighting.
2) We apply recent developments in pose-based tactile servo
control [9], [10] to compare the sensors on a common afford-
able desktop robot system (Fig. 1), by learning to slide over the
edges and surfaces of unknown 3D objects while maintaining
safe, delicate contact. This raises several challenges, including
how to apply methods developed with industrial robots to
lower-capability desktop robots, how to use and compare
distinct high-resolution optical tactile sensors with the same
hardware/software infrastructure, and how to conduct
the
experiments in a manner that facilitates reproducible research.
All hardware and software components including 3D-

printable designs for the DigiTac will be openly released.

 
 
 
 
 
 
2

(a) TacTip (round)

(b) TacTip (ﬂat)

(c) DigiTac

(d) DIGIT

(e) TacTip (ﬁngertip)

Figure 2: Comparative form factors of (a) TacTip (round tip), (b) TacTip (ﬂat tip), (c) DigiTac (curved TacTip skin); (d) DIGIT
(ﬂat GelSight skin) and (e) ﬁngertip-sized TacTip [11]. The background grid spacing is 10 mm.

II. BACKGROUND AND RELATED WORK

A beneﬁt and constraint of the DIGIT-TACTO-PyTouch
ecosystem is the focus on one type of high-resolution optical
tactile sensor: the GelSight [12], [13]. The GelSight images
indentation of an elastomer from the surface shading on a re-
ﬂective coating illuminated by multiple internal light sources.
This design concept has diversiﬁed into a family of reﬂection-
based optical tactile sensors [14]–[17] including the DIGIT [1].
The ﬁrst application of deep learning to optical tactile sensing
used the GelSight [18], since leading to many ML models
including progress in integrating vision and touch [19].

Another type of high-resolution optical tactile sensor uses
the morphology of the skin to transduce contact
into the
motion of markers [6], [7]. The TacTip is unique in using an
array of 3D-printed biomimetic internal pins to amplify the
contact due to surface indentation and shear [7]. The design is
based on human skin that is structured around dermal papillae
whose motion is signalled by nearby mechanoreceptors [20],
with neural signals found recently to match the TacTip out-
put [21]. The tactile images from the TacTip are well-suited
to ML models, leading to applications including tactile servo
control [8], [10], pushing manipulation [22] and sim-to-real
transfer of tactile pushing, rolling and servoing [23]. A review
of these capabilities was published recently [7].

More broadly, there have been many tactile sensor designs
utilizing markers [24], with recent innovation in embedding
markers within and underneath the tactile skin, e.g. [25]–[28].
Some GelSight-type sensors also use markers on the reﬂective
coating [13], [17] to enable sensitivity to shear contact that is
not apparent from imaging reﬂection alone.

All tactile sensors have their pros and cons. The GelSight
and TacTip have a common reliance on imaging an internally-
illuminated skin. However, there are differences in their elas-

ticity, robustness, friction and sensitivity to contact features
that may make their operation in practise rather different. Our
work here aims to bring together knowledge across the ﬁeld
to make it possible to trade-off aspects of these distinct tactile
sensors to progress towards tactile-enabled robot dexterity.

III. TACTILE SENSORS

A. Tactile Sensor Comparison

The aim of this paper is to compare low-cost high-resolution
tactile sensors from the GelSight and TacTip families. For
a GelSight-type sensor, we use the DIGIT (Fig. 2d), which
has a ﬂat sensing surface of 25×19 mm and VGA camera.
We compare this with a domed TacTip (Fig. 2a) of 40 mm
diameter and full-HD camera (ELP 1080p USB module).
Table I compares the hardware properties of the sensors.

However,

there is an issue with comparing the sensing
capabilities of the DIGIT and TacTip. Any differences in
performance may be attributable to the cameras or sensor
shapes, rather than the tactile sensing. Therefore, it is impor-
tant to standardize the hardware and software methods as far as
possible to isolate differences due to the sensing mechanism.
How can two distinct sensors be standardized? Although the
TacTip and GelSight-type sensors are superﬁcially similar in
both relying on internal cameras, they use different internal
illumination. The DIGIT/GelSight has lateral lighting into a
thick acrylic window that acts as a light-guide to accentuate
shading from the surface indentation. The TacTip has forward-
facing lighting situated to both contrast the reﬂective markers
against a black skin and reduce glare on a thin internal window
that holds a soft optically-clear gel below the 3D-printed skin.
Investigations indicated that the DIGIT could be customized
into a TacTip by moving the acrylic window back from the
light-source and having a rounded sensing surface. The details
of this DIGIT-TacTip customization are described next.

Table I. Comparison of DIGIT, DigiTac and TacTip low-cost
high-resolution tactile sensors (each is less than £100 to make).

B. DigiTac Design and Fabrication

size l × w × h [mm]
weight [g]
sensing ﬁeld [mm]
resolution [pix]
frame rate [per sec]
biomimetic markers

DIGIT
36×26×33
20
25×19
640×480
60
-

DigiTac
36×26×39
20
25×19
640×480
60
140

TacTip
48 dia.×55
65
40 dia.
1920×1080
120
331

The design of the DIGIT optical tactile sensor is based
on [1]: (i) being sufﬁciently compact to ﬁt as an array of
end effectors/ﬁngertips on multi-ﬁngered robot hands or arms;
(ii) having a sensing surface (gel) that is robust and easily
interchanged; and (iii) easing the fabrication process by using
off-the-shelf components and a snap-ﬁt assembly.

3

Figure 3: TacTip sensing surface integrated with DIGIT base.
A) 3D-printed TacTip, B) acrylic window, C) lighting PCB,
D) plastic housing, E) camera PCB, F) back housing.

Figure 5: Pose-based tactile servo control with PoseNet con-
volutional neural network model [10, Fig. 2][9, Fig. 4].

Table II. Tactile image data collection for sliding contacts:
top: pose parameter ranges; bottom: slide ranges to pose. 5000
uniformly-random samples are collected per stimulus.

stimulus
edge
surface

x-position
0 mm
0 mm

y-position

pose parameter ranges
z-position
[−5,+5] mm [−1,+1] mm [−45◦,+45◦]
[−30◦,+30◦]
[−5,−1] mm
0 mm

angle

Figure 4: 3D-printable design and fabricated DigiTac sensing
surface with press-ﬁt housing.

slide parameter ranges

stimulus
edge
surface

x-position

y-position

[−5,+5] mm [−5,+5] mm
[−5,+5] mm

0 mm

z-position
0 mm
0 mm

angle
[−5◦,+5◦]
[−5◦,+5◦]

In practise, the DIGIT has two distinct assemblies:

(A1) The base unit comprising the internal lighting PCB,
camera PCB and housing (Fig. 3C-F same as [1, Fig. 2D-G]);
(A2) The sensing surface on a thick acrylic window/light-
guide and housing that snap ﬁts onto the base. This sensing
surface based on the GelSight family of sensors, comprising
a painted elastomer, 6 mm-thick acrylic window and snap-
ﬁt holder [1, Fig. 2A-C]. Several elastomers were trialled,
including coated markers and a transparent surface [1, Fig. 4].
Here we adapt the DIGIT sensing surface into a TacTip

sensing surface by:
(D1) The plastic snap-ﬁt holder is redesigned to support a
curved compliant skin on the underside of which are 140
2 mm-papillae tipped by markers (Fig 4). The entire piece
is fabricated on a multi-material 3D-printer (Stratasys J826),
using a rubber polymer (Agilus30, black) for the skin/papillae
and acrylic resin (VeroWhite) for the holder/markers.
(D2) The acrylic window is narrowed to 1 mm thick and
moved above the light source to just below the complient
skin. The original window acted as a light guide for lateral
illumination of the GelSight-type sensing surface. However,
the main function of the window for the TacTip is to contain
a compliant gel (see D3 below). A thinner window allows
more direct marker illumination for a sufﬁciently curved skin,
while also avoiding spurious reﬂections and saving weight.
(D3) The enclosure between the skin and window is ﬁlled with
a soft optically-clear gel as in other TacTip sensors [6], [7].
Here we used a silicone gel (Techsil, Shore A Hardness 15),
which was injected through a small (1.5 mm-dia.) ﬁlling hole
near the retaining screw on the front of the sensing surface.
To stop the gel leaking, the ﬁlling hole is then sealed with a
small plug, which is also made by 3D-printing.

The DigiTac sensing surface is assembled in a similar way
as other TacTip sensors (see the Soft Robotics Toolkit for a
step-by-step guide). The use of multi-material 3D-printing will
ease future customization of the design, having already led to
a wide range of TacTip form-factors and integrated robotic
systems (Fig 2; see also [7, Fig. 4]). There is a competitive
online marketplace for 3D-printing, making it easy to obtain
the DigiTac sensing surface from the open-sourced designs.

The DigiTac differs from the DIGIT sensor by having a
thicker, curved sensing surface (Fig. 2 and Table I), which
we will see later is far softer than the original elastomer.
The DIGIT and DigiTac are sufﬁcently small and light to be
compatible with some grippers (e.g. the Allegro hand shown
in [1]). However, they are too bulky for anthropomorphic robot
hands, where a custom TacTip (Fig. 2E) is currently the only
integrated high-resolution optical tactile sensor.

IV. EXPERIMENTS

A. Task: Tactile Servo Control on a Contour and Surface

Here we use servo control around a contour and over a
surface to test and compare the various tactile sensors. Tactile
servo control is analogous to visual servo control, with tactile
data used in a feedback loop to control the robot motion.
In pose-based tactile servo control [10], the pose error SER
between the sensor pose FPS and a reference pose FPR (in a
local feature frame F ) drives the feedback loop (Fig. 5 top).
For high-resolution optical tactile sensors, the sensor pose
FPS in the feature frame (here of an edge or surface) can
be predicted using a suitably-trained ‘PoseNet’ convolutional
neural network on a tactile image [9] (Fig. 5). For robust pose
predictions while interacting with the object, it is necessary to

4

train the model to be insensitive to contact shear motion by
sliding the sensor along the object to its labelled pose [8]–[10].
Here we use two tactile servo control tasks adapted from
the fully 3D-tasks reported in earlier work [10] to 2D-tasks
implementable on lower-capability desktop robot arms:
Task 1: Edge-following is around horizontal ﬂat shapes (here
we use circle, square and circular-wave objects described
in [10]). The PoseNet model is trained on a straight edge of
the square block. During training, the sensor pose is varied in
its perpendicular distance from the edge and across a range of
orientations, using a horizontal sliding motion to move it into
position (Table II). Unlabelled variations in contact depth (z)
are introduced to train the model to be insensitive to sub-mm
variations in object height due to imperfect levelling of the
robot (which is very difﬁcult to avoid in practise).
Task 2: Surface-following is carried out around the walls of
the shapes described above with the tactile sensor oriented so
the sensing surface is vertical. The PoseNet model is trained on
a straight vertical wall of the square block. The labelled pose
is varied in distance, normal to the surface, and over a range
of surface angles, following a 2D sliding motion tangential to
the surface (Table II). A subtlety for servoing over surfaces, is
that the tool centre point (TCP) needs adjusting so the z-axis
of the end-effector frame coincides with the tip of the sensor.
The PoseNet models are trained on 75% of 5000 random
poses (uniformly distributed), with test accuracies reported
on the remaining 25%. Network hyperparameters are from
ref. [10, Table II] without further tuning, except the input lay-
ers were changed to correspond to 160×120 pixel subsampled
tactile images for the DIGIT and DigiTac and 128×128 pixels
for the TacTip, with the latter two binarized using adaptive
thresholding. Networks were trained over 100 epochs (∼30
minutes on a standard PC with 6Gb GPU).

The tactile sensors are compared using the model perfor-
mance on the edge and surface test data and the deviation of
the controlled trajectories from the known object perimeters.
These tests also probe the model generalization for each sensor
to curved edges and surfaces not experienced in training.

B. Low-cost test platform

To aid in making the robot test platform accessible to other
laboratories, we use a low-cost desktop robot along with laser-
cut and 3D-printed accessories for all comparison experiments.
To give some context, typically the hardware and software
infrastructure required for researching robot touch has been:
(I1) Robot manipulators, such as ABB and Universal Robotics
industrial robot arms. These are highly capable and accurate
(e.g. ABB-IRB120 repeatability ±0.01 mm), but are large,
expensive items of equipment
that must be installed and
operated safely in a laboratory or industrial setting.
(I2) Software libraries for capturing tactile data, processing
with ML models and controlling the robots in real-time. Dif-
ferent laboratories have various solutions. In Bristol Robotics
Laboratory, we have developed Python libraries, VSP (Visual
Stream Processor) and CRI (Common Robot Interface) for
asynchronous tactile image capture and interfacing with pro-
prietary robot APIs, available on GitHub.

Figure 6: Schematic of test platform: A) desktop robot arm;
B) tactile sensor on mount; C) 3D-printed stimulus on mount;
D) robot mounting plate; E) base plate with mounting grid.

Here we introduce a test platform to ease the setup and
repeatability of tactile experiments using a low-cost safe
alternative to industrial robot arms (Fig. 6):
(P1) Desktop robot arm, for which we use a Dobot MG400
4-axis arm designed for affordable automation. The base and
control unit has footprint 190 mm×190 mm, payload 750 g,
maximum reach 440 mm and repeatability ±0.05 mm. As we
describe later, the accuracy of tactile models trained using
this arm is similar to larger industrial robot arms. The main
constraint is that only the (x, y, z)-position and rotation around
the z-axis of the end effector are actuated.
(P2) Base plate with a mounting plate for the desktop robot
and a grid of holes for mounting stimuli in precise locations
relative to the robot. The base plate was laser cut from an
acrylic sheet of size 600×400×10 mm, with 25 mm grid-
spacing. The robot mounting plate was also laser cut from
10 mm-thick acrylic then screwed to the base plate.
(P3) Normal and right-angle mounts for the DIGIT and TacTip
sensors so they can be used as end effectors. These were 3D-
printed from designs adapted from an end-ﬂange supplied with
the MG400. A choice of mounts allows the tactile sensing
surface to be oriented in a horizontal or vertical plane, as the
end effector can rotate only around a vertical axis.
(P4) Test stimuli with mounts to attach to the base plate. Here
we use circular, square and circular-wave shapes of ∼100 mm
diameter with 30 mm walls, 3D-printed in ABS.

V. RESULTS

A. Appraisal of DIGIT, DigiTac and TacTip

For an initial qualitative comparison of the three optical
tactile sensors, we pressed them against a 3D-printed straight
edge and captured the tactile images (Fig. 7).

From the external appearance of the deformation, the Digi-
Tac and TacTip (Figs 7b,c) are much softer than the DIGIT
(Fig. 7a), with a large compression of several millimeters over
the contact region and a corresponding bulging of part of the
sensing surface not in contact. In contrast, the DIGIT sensing
surface is fairly stiff with sub-mm deformation. The maximum

(a) DIGIT

(b) DigiTac

(c) TacTip

DigiTac: Edge pose – displacement and contact angle

5

DigiTac: Surface pose – depth and contact angle

(d) RGB image

(e) BW image

(f) BW image

Figure 7: Top: views of the DIGIT, DigiTac and TacTip (round)
pressing on an edge. Bottom: corresponding tactile images.

compression of the DigiTac or TacTip is about 6-8 mm before
damage will occur and for the DIGIT about 1-2 mm.

Another difference is that the DIGIT sensing surface has
higher friction than the DigiTac. It is fairly easy to slide a
DigiTac or TacTip over a 3D-printed ﬂat surface with slight
texture, whereas the DIGIT tends to stick then slip.

From the tactile images, the edge is clearly visible with the
DIGIT as a shaded line at the point of contact (Fig. 7d). For
the DigiTac and TacTip, the edge can be located by where
the markers are more widely spaced (Figs 7e,f). Thus, it is
more difﬁcult to see edge location by eye; however, we will
see later that the edge can be precisely localized by using a
convolutional neural network on the entire tactile image.

The DIGIT is sensitive enough for the texture of the 3D-
print to be just visible (Fig 7d, right of edge). This information
is absent from static touch with the DigiTac/TacTip. In this
respect, the DIGIT is superior to the human sense of touch,
which does not have the sensitivity to feel that the 3D-print is
textured by statically holding a ﬁngertip on the surface.

B. Sensor Comparison using Edge Following

Next, we compare the tactile sensors on predicting edge
position and angle during sliding contacts, then use this pose
model for tactile servo control around the edges of three test
objects: a circular disk, square block and circular-wave.

For ofﬂine edge-pose prediction on the test data, both the
DigiTac and TacTip are similarly accurate with edge position
error 0.16 mm and angle error ∼1.6◦ (Table III). These errors
are ∼1.5% of the ranges (10 mm and 90◦), with little scatter
when the predictions are plotted against ground truths (Fig. 8).
It appears the smaller size of the DigiTac and fewer markers
(Table I) does not affect its capability for predicting edge pose.
In comparison, the DIGIT has similar edge position error
(0.19 mm vs 0.16 mm) and angle (1.82◦ vs ∼1.6◦) compared
with the DigiTac. To isolate the effects of sensor surface
material and geometry from subsequent processing, we used
the same neural network architectures and hyperparameters
with binary tactile images for the DigiTac and shaded images
for the DIGIT. While these results could change with a

Figure 8: Pose prediction with the DigiTac. Top: edge displace-
ment and angle. Bottom: surface depth and angle. Predictions
are under random unknown sliding contacts (Table II).

Table III. Accuracy (MAE) of pose estimation for the DIGIT,
DigiTac and TacTip on the straight edge and ﬂat surface.
Predictions are under random sliding contacts (Table II).

sensor

stimulus

DIGIT

DigiTac

TacTip

edge
surface
edge
surface
edge
surface

position y
MAE / range
0.19 mm / 10 mm
0.22 mm / 1 mm
0.16 mm / 10 mm
0.06 mm / 4 mm
0.16 mm / 10 mm
0.11 mm / 4 mm

angle
MAE / range
1.83◦ / 90◦
2.47◦ / 60◦
1.64◦ / 90◦
0.51◦ / 60◦
1.66◦ / 90◦
0.73◦ / 60◦

different choices of neural networks, all accuracies are sub-
mm and around 1-2 degrees, which are appropriate for the
contour-following experiments reported below.

When applied to edge following on the three test shapes, the
DIGIT, DigiTac and TacTip all accurately traced around the
shapes to sub-mm accuracy (Fig. 9, two left columns). For all
three sensors and all three shapes, the mean absolute position
errors were mainly sub-millimeter (Table IV, edge stimuli),
with the TacTip the most accurate (0.4-0.5 mm), followed by
both the DIGIT and DigiTac (0.6-1.2 mm). Overall, the circular
wave is most demanding, followed by the square (because of
the corners) and the circle is the easiest. All position errors
are larger than the test predictions (∼ 0.15 mm), as expected
because the servo control task introduces factors such as the
corners on the square and curves of the circular-wave that
differ from the straight edges used in training.

The angle prediction performance showed more variability
in how accurately the sensor maintains normality to the edge
during servoing (Fig. 9; Table IV). For just the circle and
square, the TacTip has the lowest angle error ((cid:46)5◦), followed
by both the DIGIT and DigiTac (10-15◦), which we attribute
to the larger domed TacTip behaving better under shear. All
sensors were inaccurate on the circular-wave (20-30◦), because
of the difﬁculty of contour following around the shape.

6

DIGIT: Edge Following

DigiTac: Edge Following

DigiTac: Surface Following

TacTip: Edge following

TacTip: Surface following

Figure 9: Edge and surface following with the DIGIT, DigiTac
and TacTip for the disk, square and circular-wave shapes.

Table IV. Accuracy of edge and surface following for the
DIGIT, DigiTac and TacTip.

sensor
DIGIT

DigiTac

TacTip

stimulus
edge
edge
edge
edge
edge
edge
surface
surface
surface
edge
edge
edge
surface
surface
surface

shape
circle
square
circular-wave
circle
square
circular-wave
circle
square
circular-wave
circle
square
circular-wave
circle
square
circular-wave

position MAE
0.6 mm
0.9 mm
1.2 mm
0.6 mm
1.0 mm
1.1 mm
0.7 mm
1.5 mm
1.0 mm
0.4 mm
0.5 mm
0.4 mm
0.6 mm
1.5 mm
1.6 mm

angle MAE
10.7◦
12.7◦
29.7◦
15.1◦
15.0◦
26.2◦
12.9◦
10.2◦
27.9◦
1.6◦
4.6◦
19.6◦
15.4◦
14.7◦
23.0◦

C. Sensor Comparison using Surface Following

The ﬁnal comparison of the tactile sensors is on predicting
surface angle and contact depth during sliding contacts, which
is then used for tactile servo control around the vertical walls
of the same three test objects used above.

For ofﬂine surface-pose prediction on the test data, both
the DigiTac and TacTip accurately predict surface pose, with
the DigiTac giving the best contact depth error (0.06 mm) and
angle error (0.5◦) (Table III). These correspond to 1.5% and
∼1% of the ranges (4 mm and 60◦), with little scatter on
the prediction vs ground truth plots (Fig. 8). The accuracy
in contact depth is similar to the repeatability (0.05 mm) of
the desktop robot, which may be a limiting factor.

For the DIGIT, the angle error (2.47◦) and contact depth
error (0.22 mm) are high compared to those for DigiTac and
TacTip (Table III). It was not possible to successfully follow a
surface with this model. We attribute this difﬁculty as due to
the much stiffer sensing surface, which allowed only a narrow
(1 mm) range of training depths and during surface following
left little tolerance to errors during the servo control.

When applied to surface following on the three test shapes,
both the DigiTac and TacTip had similar accuracy (0.6-
1.5 mm), with the circle the most accurately traced at ∼0.5 mm
error (Fig. 9, Table IV). Both sensors struggled to turn the
corners of the square, and for the DigiTac we needed to ad-
vance the angular set point to 5◦ for the task to complete. Like
edge-following, the circular-wave was the most demanding
shape to accurately trace. The angle errors were similar for the
DigiTac/TacTip, being least for the circle and square (10◦-15◦)
and largest on the circular-disk (∼25◦).

VI. DISCUSSION

In this paper, we compared low-cost high-resolution tactile
sensors from the GelSight and TacTip families, by adapting
the DIGIT (GelSight) to have a TacTip sensing surface, The
DIGIT-TacTip, or DigiTac allowed comparison of these two
tactile sensor types with the same hardware and software.

Overall,

the TacTip, DigiTac and DIGIT optical

tactile
sensors all performed well at pose prediction and pose-
based tactile servo control, with pose-prediction accuracy of
∼ 0.1 mm and ∼1-2◦, and servo control accuracy typically

better than 1 mm. There was some biasing of contact angle
during the servo control tasks, mainly for the DIGIT and
DigiTac. However, they still performed accurately at edge and
surface following, except the DIGIT was not suited for sliding
over surfaces because of its relatively ﬂat and stiff elastomer.
The most signiﬁcant difference between the sensors is
their material properties and construction. The DIGIT sensing
surface is ﬂat and fairly inelastic compared to the soft curved
sensing surface of the TacTip. GelSight-type sensors have ﬂat
sensing surfaces comprised of a molded elastomer illuminated
laterally, which are highly effective at imaging ﬁne surface
detail. In contrast, TacTip-type sensors come in various shapes
and sizes (Fig. 2) with a ﬂexible 3D-printed skin and compli-
ance from a soft gel. For the servo control tasks considered
here, a soft curved tactile sensor was more practical to ﬁt into
curved surface and had greater tolerance for safe contact.

The robustness of the sensors also depends on their material
properties and construction, which is important because touch
involves contact with the potential for damage. Our tests were
demanding because: (i) thousands of sliding contacts were
needed to train the pose-prediction models; (ii) surface and
edge following shears the sensor surface as it rubs against the
object; (iii) surface following can also lead to collisions with
the test object. During the tests, we broke one DigiTac by
ripping the skin where it joins the housing and one DIGIT by
shearing off the elastomer from a collision.

Because our aim in comparing tactile sensor technologies is
to encourage innovation in the ﬁeld, we will open-source the
DigiTac sensing surface and software infrastructure including
the robot interface and libraries for the experiments. We will
also release all data and models used in this paper. As a
ﬁnal comment, we encourage others to adopt a similarly open
perspective, and look forward to seeing further comparison
and open release of other low-cost optical tactile sensors.

Acknowledgements: We thank Mike Lambeta and Roberto
Calandra for donating the DIGIT sensors, and Stephen Red-
mond for discussions including suggesting the name ‘DigiTac’.

REFERENCES

[1] M. Lambeta, P. Chou, S. Tian, B. Yang, B. Maloon, V. Most, D. Stroud,
R. Santos, A. Byagowi, G. Kammerer, D. Jayaraman, and R. Calandra.
DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tac-
tile Sensor With Application to In-Hand Manipulation. IEEE Robotics
and Automation Letters, 5(3):3838–3845, July 2020.

[2] DIGIT. http://www.digit.ml/.
[3] M. Lambeta, H. Xu, J. Xu, P. Chou, S. Wang, T. Darrell, and R. Ca-
landra. PyTouch: A Machine Learning Library for Touch Processing.
In 2021 IEEE International Conference on Robotics and Automation
(ICRA), pages 13208–13214, Xi’an, China, May 2021.

[4] S. Wang, M. Lambeta, P. Chou, and R. Calandra. TACTO: A Fast,
Flexible, and Open-Source Simulator for High-Resolution Vision-Based
IEEE Robotics and Automation Letters, 7(2):3930–
Tactile Sensors.
3937, April 2022.

[5] Teaching robots to perceive, understand, and interact through touch.
https://ai.facebook.com/blog/teaching-robots-to-perceive-understand-
and-interact-through-touch/.

[6] B. Ward-Cherrier, N. Pestell, L. Cramphorn, B. Winstone, M. E.
Giannaccini, J. Rossiter, and N. Lepora. The TacTip Family: Soft
Optical Tactile Sensors with 3D-Printed Biomimetic Morphologies. Soft
Robotics, 5(2):216–227, 2018.

[7] N. Lepora. Soft Biomimetic Optical Tactile Sensing With the TacTip:
A Review. IEEE Sensors Journal, 21(19):21131–21143, October 2021.

7

[8] N. Lepora, A. Church, C. de Kerckhove, R. Hadsell, and J. Lloyd.
From Pixels to Percepts: Highly Robust Edge Perception and Contour
Following Using Deep Learning and an Optical Biomimetic Tactile
Sensor. IEEE Robotics and Automation Letters, 4(2):2101–2107, 2019.
[9] N. Lepora and J. Lloyd. Optimal Deep Learning for Robot Touch:
IEEE

Training Accurate Pose Models of 3D Surfaces and Edges.
Robotics & Automation Magazine, 27(2):66–77, June 2020.

[10] N. Lepora and J. Lloyd. Pose-Based Tactile Servoing: Controlled Soft
Touch Using Deep Learning. IEEE Robotics & Automation Magazine,
28(4):43–55, December 2021.

[11] N. Lepora, A. Stinchcombe, C. Ford, A. Brown, J. Lloyd, M. Catalano,
M. Bianchi, and B. Ward-Cherrier. Towards integrated tactile senso-
rimotor control in anthropomorphic soft robotic hands. In 2021 IEEE
International Conference on Robotics and Automation (ICRA), Xi’an
China, May 2021.

[12] M. K. Johnson and E. H. Adelson. Retrographic sensing for the
In IEEE Conference on

measurement of surface texture and shape.
Computer Vision and Pattern Recognition, pages 1070–1077, 2009.

[13] W. Yuan, S. Dong, and E. Adelson.

GelSight: High-Resolution
Robot Tactile Sensors for Estimating Geometry and Force. Sensors,
17(12):2762, 2017.

[14] D. Gomes, Z. Lin, and S. Luo. GelTip: A Finger-shaped Optical Tactile
In 2020 IEEE/RSJ International
Sensor for Robotic Manipulation.
Conference on Intelligent Robots and Systems (IROS), pages 9903–9909,
Las Vegas, NV, USA, October 2020.

[15] B. Romero, F. Veiga, and E. Adelson. Soft, Round, High Resolution
Tactile Fingertip Sensors for Dexterous Robotic Manipulation. In 2020
IEEE International Conference on Robotics and Automation (ICRA),
pages 4796–4802, Paris, France, May 2020.

[16] A. Padmanabha, F. Ebert, S. Tian, R. Calandra, C. Finn, and S. Levine.
OmniTact: A Multi-Directional High-Resolution Touch Sensor. In 2020
IEEE International Conference on Robotics and Automation (ICRA),
pages 618–624, Paris, France, May 2020.

[17] I. Taylor, S. Dong, and A. Rodriguez. GelSlim3.0: High-Resolution
Measurement of Shape, Force and Slip in a Compact Tactile-Sensing
In 2022 IEEE International Conference on Robotics and
Finger.
Automation (ICRA), pages 10781–10787, Philadelphia, PA, USA, May
2022.

[18] W. Yuan, C. Zhu, A. Owens, M. Srinivasan, and E. Adelson. Shape-
independent hardness estimation using deep learning and a GelSight
tactile sensor. In 2017 IEEE International Conference on Robotics and
Automation (ICRA), pages 951–958, Singapore, Singapore, May 2017.
[19] S. Luo, N. Lepora, U. Martinez-Hernandez, Joao Bimbo, and H. Liu.
Editorial: ViTac: Integrating Vision and Touch for Multimodal and
Cross-Modal Perception. Frontiers in Robotics and AI, 8, 2021.
[20] C. Chorley, C. Melhuish, T. Pipe, and J. Rossiter. Development of
a tactile sensor based on biologically inspired edge encoding.
In
International Conference on Advanced Robotics, pages 1–6, 2009.
[21] N. Pestell, T. Grifﬁth, and N. Lepora. Artiﬁcial SA-I and RA-I afferents
for tactile sensing of ridges and gratings. Journal of The Royal Society
Interface, 19(189):20210822, April 2022.

[22] J. Lloyd and N. Lepora. Goal-Driven Robotic Pushing Using Tactile and
Proprioceptive Feedback. IEEE Transactions on Robotics, 38(2):1201–
1212, April 2022.

[23] A. Church, J. Lloyd, R. Hadsell, and N. Lepora. Tactile Sim-to-Real
Policy Transfer via Real-to-Sim Image Translation. In Proceedings of the
5th Conference on Robot Learning, pages 1645–1654. PMLR, October
2021.

[24] K. Kamiyama, H. Kajimoto, Naoki Kawakami, and Susumu Tachi.
In IEEE International
Evaluation of a vision-based tactile sensor.
Conference on Robotics and Automation (ICRA), pages 1542–1547
Vol.2, New Orleans, LA, USA, 2004.

[25] G. Zhang, Y. Du, H. Yu, and M. Wang. DelTact: A Vision-based Tactile
Sensor Using Dense Color Pattern. arXiv preprint arXiv:2202.02179,
February 2022.

[26] X. Lin and M. Wiertlewski. Sensing the Frictional State of a Robotic
IEEE Robotics and Automation

Skin via Subtractive Color Mixing.
Letters, 4(3):2386–2392, July 2019.

[27] W. Li, J. Konstantinova, Y. Noh, Z. Ma, A. Alomainy, and K. Althoefer.
An Elastomer-based Flexible Optical Force and Tactile Sensor. In 2019
2nd IEEE International Conference on Soft Robotics (RoboSoft), pages
361–366, Seoul, Korea (South), April 2019.

[28] C. Sferrazza and R. D’Andrea. Design, Motivation and Evaluation of
a Full-Resolution Optical Tactile Sensor. Sensors, 19(4):928, February
2019.

