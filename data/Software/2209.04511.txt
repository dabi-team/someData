Empirical Software Engineering manuscript No.
(will be inserted by the editor)

Pitfalls and Guidelines for Using Time-Based Git
Data

Samuel W. Flint · Jigyasa Chauhan ·
Robert Dyer

2
2
0
2

p
e
S
9

]
E
S
.
s
c
[

1
v
1
1
5
4
0
.
9
0
2
2
:
v
i
X
r
a

Received: March 13, 2022 / Accepted: date

Abstract Many software engineering research papers rely on time-based data
(e.g., commit timestamps, issue report creation/update/close dates, release dates).
Like most real-world data however, time-based data is often dirty. To date, there
are no studies that quantify how frequently such data is used by the software engi-
neering research community, or investigate sources of and quantify how often such
data is dirty. Depending on the research task and method used, including such dirty
data could aﬀect the research results. This paper presents an extended survey of
papers that utilize time-based data, published in the Mining Software Repositories
(MSR) conference series. Out of the 754 technical track and data papers published
in MSR 2004–2021, we saw at least 290 (38%) papers utilized time-based data.
We also observed that most time-based data used in research papers comes in the
form of Git commits, often from GitHub. Based on those results, we then used the
Boa and Software Heritage infrastructures to help identify and quantify several
sources of dirty Git timestamp data. Finally we provide guidelines/best practices
for researchers utilizing time-based data from Git repositories.

Keywords literature review · time data · mining software repositories

1 Introduction

The Mining Software Repositories (MSR) conference has been around since 2004
as a workshop, working conference, and ﬁnally a full conference. During those

This paper is a revised and extended version of Flint et al. (2021a).

Corresponding author: Samuel W. Flint
University of Nebraska–Lincoln
E-mail: swﬂint@huskers.unl.edu

Jigyasa Chauhan
University of Nebraska–Lincoln
E-mail: jchauhan2@huskers.unl.edu

Robert Dyer
University of Nebraska–Lincoln
E-mail: rdyer@unl.edu

 
 
 
 
 
 
2

Flint, Chauhan, and Dyer

18 years, there have been over 600 research and over 100 data showcase papers
published in MSR proceedings. The majority of the research in MSR relies on
analyzing existing data, including data from version control systems (CVS, Sub-
version, Git), issue/bug reports, discussion forums (emails, Stack Overﬂow), pull
requests (PRs), continuous build/test systems, etc. Often these data sources in-
clude time components indicating when events occurred, such as the timestamp
of a code commit or when a pull request was opened or closed.

Depending on the source of the data, there may be errors or inconsistencies
in the time components. For example, the Git version control system (VCS) al-
lows users to specify both the authored and committed dates when creating new
commits. It also allows editing the existing commit graph (rebasing) which allows
for modiﬁcation of the timestamps of older commits. Similarly, Subversion adds
properties to revisions for things like the author and revision date. While they are
added automatically, they can be altered or even removed later. There are also
more general issues with time data, for example dealing with inconsistent time
zones, clock skews, or more generally, incorrectly set computer clocks.

To date, no survey has been performed to investigate how MSR researchers
utilize time-based data in their research. This work thus surveys 754 MSR technical
research and data showcase papers from 2004–2021 to determine how many rely
on time-based data and what techniques are utilized to control for potential errors
in that data. We utilize keyword searches of the papers and then manually inspect
to determine that at least 209 technical research papers and 81 data showcase
papers rely on or provide time-based data. This accounts for at least 38% of the
papers in MSR’s history. Thus we conclude that time-based data is widely used in
MSR research.

Based on the results of the survey indicating that VCS is the most used data
kind incorporating time-based data, and that GitHub is the most used data source,
we then investigate potential problems with time-based Git data from GitHub.
Since Robles (2010) previously showed that many research papers in MSR are dif-
ﬁcult to reproduce, often due to missing data, we chose to not directly investigate
the time-based data used in the prior MSR papers found in the survey. Instead, we
utilized the Boa (Dyer et al., 2013, 2021) and Software Heritage (Cosmo and Za-
cchiroli, 2017; Software Heritage developers, 2020) infrastructures and attempted
to quantify how frequently some types of errors occur in those datasets. We also
attempted to infer the potential source(s) of commonly observed errors. Based on
this investigation, we observed a couple of potential pitfalls when utilizing time-
based Git data, and try to quantify how frequently one might encounter such errors
if using Git data derived from GitHub.

The results show that almost 50k commits have timestamps that are suspi-
ciously too old (even before the initial release of CVS, 19 November 1990), out of
around 54m total commits (around 0.09%). Many of those bad timestamps were
the result of tools such as git-svn. We also discovered over 80k commits from over
57k projects where one (or more) of the commit’s parent commits had a times-
tamp that was newer than the commit itself—a situation that does not make sense.
Again, many of these were the result of automated tools or a small set of users.
A replication package containing all of the data and scripts used in our analysis is
also publicly available (Flint et al., 2021b).

To help show the potential impact of bad time-based data, we investigate
several datasets containing Git repositories from MSR data showcase papers. These

Pitfalls and Guidelines for Using Time-Based Git Data

3

papers collectively have over a hundred citations already. We intersect the projects
in those datasets with the projects from Boa’s datasets and ﬁnd over 15k commits
with bad timestamps. Since those papers already contain over a hundred citations,
it shows the potential impact of such bad data propagating to other research and
highlights the importance of properly sanitizing data, especially when building
reusable datasets.

Finally, we propose some guidelines for researchers utilizing time-based Git
data to help escape these pitfalls. These include ﬁltering out older projects (based
on our analysis, we would recommend anything before 2014), ﬁltering out certain
projects or users that seem to have a lot of bad commit timestamps, or prefer-
ably running speciﬁc analyses to automatically verify and reject commits with
suspicious timestamps. We hope future SE researchers follow these guidelines.

Note that this study is an extension of our prior paper (Flint et al., 2021a).
Compared to the previous paper, this work adds the following additional contri-
butions:

– We update the survey to include MSR 2021 papers.
– We extended the survey analysis to see if there were trends over time in the

use of kinds of mined time-based data or data sources.

– The original paper analyzed a single dataset consisting of Java projects from
GitHub. In this work, we analyze two additional GitHub datasets with Kotlin
and Python projects, to see if choice of programming language might inﬂuence
how often bad time data occurs.

– We also analyze a SourceForge dataset from Boa to investigate if CVS and
Subversion-based projects from multiple programming languages have bad time
data. This allows us to generalize the results beyond just Git data and beyond
the three programming languages studied from GitHub.

– For commits that are out-of-order (at least one parent commit is newer than the
commit itself), we quantify how far apart the commit is from its out-of-order
parent to give some insights into the potential causes.

– Finally, we try to quantify the potential impact of such bad time-based data, by
analyzing 11 previously published MSR dataset papers and intersecting their
data with our datasets to quantify if those datasets contain potentially bad
commits.

In the next section we discuss related prior research. In Section 3 we detail
our survey on the use of time-based data in MSR research. Then in Sections 4–5
we attempt to identify and quantify some examples of problems with time-based
data in Git/GitHub. In Section 6 we look at the potential impact such time-based
data problems might have on the ﬁeld by analyzing some re-usable datasets. We
discuss implications of the study and present best practice guidelines in Section 7.
Threats to the validity of our study are then discussed in Section 8. Finally, we
conclude in Section 9.

2 Previous Studies

In this section we discuss prior works that either performed surveys of MSR re-
search or propose guidelines for future MSR researchers to follow.

4

Flint, Chauhan, and Dyer

Demeyer et al. (2013) explored 10 years of MSR papers to determine what soft-
ware projects were studied and the frequency of studies on the given projects, as
well as the infrastructure behind mining. In particular, they noted that the most
common source of data were version control systems, including the then-increasing
popularity of Git, and infrequency of use of VCSes other than CVS, Subversion or
Git. They also noted that few of the studies at the time had considered industrial
cases and instead most were over open source software. While their work identi-
ﬁes common sources of data (of various kinds), our survey speciﬁcally focuses on
common sources of time-based data.

Kalliamvakou et al. (2014, 2016) addressed various characteristics of GitHub
repositories. They note several possible problems with GitHub data, such as con-
taining personal and inactive projects or that many pull requests are not marked
as merged despite being so. They provide guidelines for software engineering re-
searchers on how to use GitHub data in their research more eﬀectively. Our work is
somewhat complementary to theirs. While they do not focus on time-based issues,
that is the focus of our work and our recommendations.

Cosentino et al. (2016) reviewed the use of GitHub data in prior studies and
structured data archives. In particular, they looked at how GitHub data was used,
how the data was collected, and what, if any, limitations were reported in the
studies. The operation of the GitHub API at the time, particularly in terms of
request limits and inconsistent responses, was noted as a limitation. Further, the
lack of availability of fresh data was considered as a potential issue, due to reliance
on commonly curated data sources. Finally, they also described potential issues
with sampling of datasets, suggesting that better sampling methods are needed.
Similar to their work identifying potential problems with GitHub data, we identify
time-based problems with Git data sources and suggest possible methods to avoid
such problems when building datasets.

Robles (2010) was concerned with the replication of MSR studies and observed
very few papers were “replication friendly.” Replication requires the availability
of datasets and tools, as well as an adequate description of techniques used to
ﬁlter and analyze those datasets. The tools and descriptions that are preserved for
replication may ﬁlter using time, yet this particular class of ﬁltering criteria is only
one of many which must be considered for replication. Like Robles (2010), Ghezzi
and Gall (2013) studied replication of MSR studies. In particular, they described
a web service to gather and analyse software repository data, which was then used
to replicate MSR studies from 2004–2011. They found that, of the studies in those
years, 51% could not be fully replicated. While these works looked at prior papers
to estimate replicability, our work looks at prior papers to investigate the use of
time-based data. We also rely on this work to help motivate portions of our study.
Kotti and Spinellis (2019) investigated the use and impact of datasets published
as MSR data showcase papers via a systematic review. They noted that a number
of further work has built upon the MSR data showcase papers, with over 65% being
used in other studies and one having 157 (at time of their publication) citations.
In particular, they clearly show the impact of the data showcase papers, however,
they do not investigate potential issues with the re-use of this data or possible
problems with the datasets. In our work, we look at potential problems with some
of these datasets. We rely on their results to motivate that investigation.

Hemmati et al. (2013) described a set of best-practices, a “cookbook”, for
mining software repositories researchers. This included suggestions regarding social

Pitfalls and Guidelines for Using Time-Based Git Data

5

data, statistical and analytical techniques, and the sharing of tools. They discuss
the issue of VCS noise and the potential lack of granularity in VCS-recorded
changes, however, they do not discuss the potential causes of discontinuities in
time data, nor ways they may be handled. In this work we not only identify several
examples of problems with time-based data, but also propose some guidelines on
how to clean and ﬁlter data to avoid those problems.

Gasser et al. (2004), early in MSR’s history, evaluated the needs of researchers
in the ﬁeld and the data and artifacts to be studied. They proposed a set of char-
acteristics for studies to have, and discussed issues with data and how these issues
may be addressed. In particular, they discussed the frequent need to normalize
data as part of the analysis and data collection process. They did not however
focus on time-based data, which is the focus of this study.

Bird et al. (2009) discussed mining one VCS in particular, Git, and the po-
tential issues that may occur in mining repositories using it. This work describes
a number of issues, in particular, the existence of rebasing, which allows users
to rewrite a repository’s history, re-using commits in a diﬀerent order than the
commit timestamps may suggest. Thus their work helps identify potential sources
of bad time-based data, while in our work we propose some guidelines to avoid
including bad time-based Git data.

Many of the prior works mentioned here either performed surveys to look at
prior studies or they discuss common issues studies face and provide guidelines.
These works, unlike our study, were not focused on time-based data, or on provid-
ing guidance on how to deal with potentially bad time-based data.

3 Survey on the Use of Time-Based Data

This work ﬁrst investigates the following research question:

RQ1 Does prior software mining research rely on time-based data?
We focus on the mining software repositories (MSR) conference series, as
it is the preeminent conference for mining repositories. Thus we investi-
gate: is time-based data commonly utilized in published MSR technical
and data showcase papers?

Based on the results of this research question showing a large number of papers
using time-based data, we then investigate two other questions by analyzing the
subset of papers that utilize time-based data:

RQ2 What kinds of commonly mined software data include time?
When looking for time-based data, are there common kinds and sources
for that data? This can give insight into what problems might occur and
where researchers should focus their eﬀorts to ensure the data is properly
sanitized.

RQ3 What ﬁltering or cleaning techniques are used with time-based
data? Do papers already use ﬁltering or cleaning techniques for time-
based data, and if so are such techniques common? Can we infer recom-
mendations based on the existing approaches?

We begin by surveying published MSR proceedings. We select papers to review,
then from these, classify what kind of time-related data is used, how it is ﬁltered
or cleaned in published work, and the sources of time-based data.

6

Flint, Chauhan, and Dyer

3.1 RQ1: Does prior software mining research rely on time-based data?

For this study we focused only on papers published in MSR proceedings from 2004
to 2021. All technical track papers (short and long) and Data Showcase papers were
considered. Data Showcase papers were included as they are potential data sources
for other (future) research papers. Mining Challenge papers were excluded, as all
papers in this category for a given year typically use the same challenge dataset,
which may skew results towards a particular kind of data in that year. This gave
us a corpus of 754 papers to inspect.

One author used a keyword search to ﬁlter papers from this corpus, retaining
for further study those papers that contained any of the following time-related
keywords1: time, date, epoch, record, month, year, hour, minute, second, period,
week, chronolog, day, past, and interval. This retained 346 of the 754 papers
(45.89%).

After papers were initially ﬁltered, two authors independently analyzed each
paper to determine what kinds of time-based data were used, the source(s) of
the data, and any methods used to ﬁlter, clean or normalize the time-based data.
During this process, if either author found that a paper did not ﬁt the study,
it was voted for removal and removed if a second author agreed. For example,
papers which used “runtime” as a performance metric, or “epoch” as a measure of
training time were considered as irrelevant to the study and removed if no other
time-based data was used.

After the ﬁrst round, if the two authors disagreed on the kinds of data, data
sources, or ﬁltering techniques, they discussed this disagreement until consensus
was reached. This aﬀected a total of 16 papers (for source of data), 12 papers (for
kinds of data), and 13 papers (for ﬁltering techniques) for a total of 38 papers (3
papers had several disagreements). This data is described in more detail in the
following subsections.

A total of 56 papers (7.4%) had a matching keyword but were removed, leaving
290 papers. Thus the simple keyword search yielded a precision of 84%. The results
of this selection process are shown in Table 1 and the spreadsheet including all
considered and selected papers and human judgements made by the authors is
available in the replication package (Flint et al., 2021b). A full citation of all
selected papers is available in Appendix B.

The results show that every year of MSR had papers relying on time-based
data, ranging from 9-80% of all papers in a given year. Both the technical and
data showcase tracks have papers in every year relying on time-based data.

Finding 1: Time-based data is prevalent in MSR research papers, accounting for anywhere
from 9-80% of the papers in a given year. On average, 38% of all MSR papers utilize time-
based data.

3.2 RQ2: What kinds of commonly mined software data include time?

Since so many papers utilize time-based data, we now investigate if there are
common kinds and sources of time-based data. We found that across the 290

1All authors brainstormed potential keywords and helped create the ﬁnal list.

Pitfalls and Guidelines for Using Time-Based Git Data

7

Table 1 Published and selected MSR papers, by year. The full list of considered papers
and the human judgements made by the authors are available in an Excel spreadsheet in
our replication package (Flint et al., 2021b). The full list of selected papers is also shown in
Appendix B.

Technical Papers Data Showcase Percent
Selected
selected / total

selected / total

2004
2005
2006
2007
2008
2009
2010
2011
2012

2013
2014
2015
2016
2017
2018
2019
2020
2021

5 / 26
2 / 22
6 / 28∗
8 / 27
7 / 31
9 / 23
6 / 22
9 / 27
9 / 31

10 / 37
13 / 39
12 / 42
11 / 42†
13 / 43
13 / 48
16 / 47
20 / 45
40 / 48

19%
9%
21%
30%
23%
39%
27%
33%
29%

37%
46%
43%
31%
38%
37%
28%
53%
80%

9 / 14
12 / 15
13 / 16
4 / 7
6 / 7
10 / 15
2 / 17
14 / 19
11 / 16

Total

209 / 628

81 / 126

38%

(∗2006 had 2 papers listed in the program that do not appear in the proceedings, which were
excluded)
(†2016 had 1 paper not listed in the program that appears in the proceedings, it was included)

papers selected, 37 diﬀerent kinds of time-including data were used. From these,
all data kinds used by more than one paper are shown in Table 2. In particular,
we found that VCS data (diﬀs, commit lineage, commit logs, authors, etc.) are the
most commonly used. In addition, we found that issues and their related metadata
were used frequently as well.

We keep some similar kinds, such as Mailing List, Developer Q&A (e.g., Stack-
Exchange, StackOverﬂow), and Chat Logs separate as although each serve a some-
what similar purpose, researchers tend to ask diﬀerent questions about them. We
kept several categories of logs separate (e.g., “General Logs” from logging frame-
works or servers, user interaction logs, chat logs) for similar reasons.

We also investigated if the kinds of data used in MSR papers has changed over
time. We show a graph of the eight most common data kinds in Figure 1. This
graph has time on the x-axis (in years) and for each year, then plots the percent
of papers utilizing that particular data kind. Note that the totals may add up to
more than 100% as some papers utilize multiple data kinds.

When visually inspecting the graph, several trends emerge. We identiﬁed three
diﬀerent time periods with diﬀerent trends. The ﬁrst period is from 2004–2010,
where a large percent of papers utilize VCS data, but there was also several other
data kinds frequently used such as issue data and mailing list data.

The second period was from 2010–2016. During this period, VCS data use (as a
percentage) dropped down. Now we were starting to see more papers that focused
on a speciﬁc, non-VCS form of data such as issues, releases, or pull requests. In

8

Flint, Chauhan, and Dyer

Table 2 Common kinds of data used in MSR papers. Only data kinds used by more than one
paper are listed here.

Num. Papers Data Kind

188 (64.83%)
81 (27.93%)
46 (15.86%)
42 (14.48%)
19 (6.55%)
19 (6.55%)
18 (6.21%)
13 (4.48%)
13 (4.48%)
7 (2.41%)
6 (2.07%)
3 (1.03%)
2 (0.69%)
2 (0.69%)

VCS
Issues
Releases
Forge Metadata
Mailing List
Pull Requests
Developer Q&A
Continuous Improvement Logs
General Logs
Interaction Logs
Common Vulnerabilities and Exposures (CVEs)
Time Cards
Chat Logs
File Dates

particular, issue data was quite popular (about as popular as VCS data) during
this period.

The third period is from 2016–2021. During this period, we can see a clear
shift back toward a focus on VCS data. While other forms of data still appear, a
large percentage (60%+) of papers every year rely on VCS data while other forms
of data only appear in about a quarter or less of papers. The popularity of data
kinds such as issues decreased. This helps show the importance of having good
VCS data, as bad data may aﬀect many future papers.

Fig. 1 Kinds of data used over time (top 8). Each data point represents the percent of papers
that year that utilized that kind of data.

Pitfalls and Guidelines for Using Time-Based Git Data

9

Table 3 Common data sources used by MSR papers. Only data sources used by more than
one paper are listed here. Top individual data sources are indicated by bold.

Source
Category

FOSS projects

forges

anonymized
operating systems

social

app stores
issue trackers
CI systems
package repositories

security related

messaging

Total
Number

Examples

21 (7.24%)

85 (29.31%)

24 (8.28%)
23 (7.93%)

103 (35.52%) Eclipse (23, 7.93%), Apache (19, 6.55%),
Mozilla, Firefox, PostgreSQL, OpenStack, Ar-
goUML, GCC, Python, Chrome
GitHub (81, 27.93%), GHTorrent (18,
6.21%), GitLab, SourceForge, BitBucket,
Software Heritage Archive, DockerHub,
Google Code, Boa
various
Linux Kernel, RedHat, Debian, Gentoo, Fe-
dora, BSD
Stack Overﬂow (16, 5.52%), Twitter, De-
vpost
Google Play, F-Droid
Jira, Gerrit, BugZilla
Travis, Jenkins, TeamCity
Maven Central Repository, Comprehensive R
Archive Network, PyPi, Comprehensive Perl
Archive Network, Node Package Manager
National Vulnerability Database, Common
Platform Enumeration, CAPE, Common Vul-
nerabilities and Exposures
Slack, Gitter

9 (3.10%)
8 (2.76%)
6 (2.07%)
6 (2.07%)

2 (0.69%)

5 (1.72%)

Finding 2: Except for a period from around 2010–2016, VCS data seems to be the most
popular kind of time-including data used by MSR researchers.

We also investigate the sources of data, that is, where the data is gathered from
(as opposed to what the data is) and found roughly 209 diﬀerent sources were used,
with proprietary and anonymous repositories listed as a single, “anonymized”
source. We then categorized these sources to provide a higher-level overview of
some of the most common and to group them together by their (perceived) sim-
ilarity to each other. These categories and some of the more common sources
within the category are shown in Table 3. Perhaps unsurprisingly, GitHub is the
most common single data source, but FOSS project repositories, such as Eclipse,
Apache, and Mozilla, are also quite common. It is also notable that the GHTor-
rent (Gousios and Spinellis, 2012) dataset is frequently used as well.

To help show overlap between sources and data kinds, we present in Table 4
the top 10 source/kind pairs, and the percent the pair is seen in the studied papers
as well as in all MSR published papers. In particular, we note that around 23% of
the papers studied (or about 9% of published MSR papers) use time-based VCS
data and speciﬁcally get data from GitHub. Forge metadata from GitHub is also
common at about 3% of all MSR papers, with the combination of Issues and either
Eclipse or GitHub sources the third-most common combination.

Similar to the analysis of data kinds over time, we visually analyzed the data
sources over time, shown in Figure 2. Here the data was a bit noisier, but the data
falls roughly into two time periods: before and after 2015.

Before 2015, the sources of data were much larger than after 2015. There did
not seem to be a consensus on which dataset(s) to use for MSR papers, as things

10

Flint, Chauhan, and Dyer

Table 4 Top 10 data source/data kind pairs.

VCS

Forge Metadata

Issues

GitHub
Eclipse
GHTorrent
anonymized
Apache

22.76% 8.97%
4.83%
5.52% 3.79%
5.17%
3.45%

6.21%
6.21%

3.10%

like the Apache repositories or the Mozilla repositories saw huge swings from 40%+
use in one year to 0% a year or two later. Notably, GitHub did not exist for half
of that period and did not really start gaining in popularity until around 2012.

The second period, after 2015, is where we see GitHub’s popularity starting to
aﬀect the choice of data MSR researchers used. You can see a steady incline in the
percent of MSR papers using GitHub as a (direct) data source. While other sources
are still used, they typically account for a small subset of papers in any given year.
Note here that when we graph GitHub, we are graphing direct uses of GitHub
data – not indirect uses through aggregated data such as GHTorrent (Gousios and
Spinellis, 2012) (shown separately). This shows that MSR researchers are greatly
preferring to use data from GitHub, and thus we need to understand what issues
may be present in that data.

Fig. 2 Data sources used over time (top 8). Each data point represents the percent of papers
that year that utilized that data source.

Finding 3:
It appears many popular sources in the past, such as Eclipse and Apache, are
seeing declining use while GitHub is becoming the clear leading source of time-based data.

Pitfalls and Guidelines for Using Time-Based Git Data

11

Table 5 Common ﬁltering/cleaning techniques used by MSR papers. Only techniques used
by more than one paper are listed here.

Number of Papers Filtering Technique

192 (66.21%)
30 (10.35%)
24 (8.28%)
12 (4.14%)
5 (1.72%)
5 (1.72%)
4 (1.38%)

none explicitly mentioned
time window
date cutoﬀ
custom condition
changeset coalesence
date format correction
CVSAnalY

3.3 RQ3: What ﬁltering or cleaning techniques are used with time-based data?

Finally we investigated any ﬁltering or cleaning techniques used by the selected
papers. Our goal was to see if there are any commonly used methods, so that we can
inform future researchers of such techniques. We identiﬁed 54 diﬀerent methods
of cleaning or ﬁltering the time-based data (considering all custom conditions as
one method for the purposes of counting). We show any used by more than one
paper in Table 5.

Among the various time-based ﬁltering and cleaning techniques found in MSR
papers, we found six used by more than one paper. The majority of these are
ﬁltering techniques of some form, with a single cleaning technique described. It
is important to note, however, the majority of papers utilizing time-based data
(192, 66.21%) do not explicitly describe any ﬁltering or cleaning methods used,
although they might have performed some ﬁltering or cleaning. We discuss each
of the six mentioned techniques in more detail here.

3.3.1 Time Window

A number of studies select data from a source that was added between two dates or
other markers in time (e.g., releases). This was by far the most common explicitly
described method, being found in 30 studies. Some of these studies provided full
dates (Durieux et al., 2020; Pimentel et al., 2019), others only partial dates out
to year or month (Hayashi et al., 2019; Ahasanuzzaman et al., 2016), or version
numbers of releases (Antoniol et al., 2005).

3.3.2 Date Cutoﬀ

All studies retrieved data from before or after a speciﬁc date (the date of the
study). But whether the study date is used or some other date is used must be
considered. In particular, some papers describe what their cutoﬀ date for data
inclusion is, while others do not. This method is used in particular by Wang et al.
(2020); Karampatsis and Sutton (2020); Zhu and Wei (2019); Cito et al. (2017).

3.3.3 Custom Condition

A custom condition speciﬁes some method for ﬁltering a data source using time.
These were frequently employed to ensure that commits or issues were studied that

12

Flint, Chauhan, and Dyer

matched some temporal condition relating the two, or to ensure that commits were
in order, as well as for other purposes.

Liu et al. (2020) describe the use of a particular time-based condition to select
commits to study. They were interested in ﬁnding commits between the open
and close of a particular issue (in other words, looking for ﬁxing commits). This
condition is issuecreate < commitcreate < issueclose, and uses time components
from both issues and commits.

Kikas et al. (2016) use commit time and a forge’s repository creation time to
remove forks of original projects so that only the originals may be studied. We
note in particular that this method may inappropriately remove projects which
have changed forges.

Finally, Steﬀ and Russo (2012) construct a commit graph such that, for each
commit node, it is only connected to nodes preceding it in time which also share
ﬁles in common, that is, for two commits (t1, F1) and (t2, F2), (t1, F1) → (t2, F2)
if and only if t1 < t2 and F1 ∩ F2 (cid:54)= ∅.

3.3.4 Changeset Coalescence

Further techniques used include changeset coalescence or commit reconstruction.
This technique is useful in CVS or RCS repositories where changes are only made
to individual ﬁles. Most of these methods operate by collecting changes made in
a small window (3 minute) by one user into a single changeset; they may also be
aided by the use of ChangeLogs to collect such changes. This technique was used
by Zimmermann and Weißgerber (2004); Walker et al. (2006); Kagdi et al. (2006);
D’Ambros et al. (2010).

3.3.5 CVSAnalY

CVSAnalY2 is a tool to extract information from VCS logs of various kinds. It
supports CVS, Subversion, and Git. When operating on Subversion repositories, it
skips over commits it considers invalid, with one condition being the lack of date3.
Otherwise, it performs a sort of date format correction, storing all dates as Unix
timestamps with associated time zones.

In particular, this tool sees use on Git repositories (Gonzalez-Barahona et al.,
2015; Robles et al., 2014; Goeminne et al., 2013), as well as Subversion reposito-
ries (Sadowski et al., 2011), where the ﬁltering may be most apparent.

3.3.6 Date Format Normalization

Due to the diversity of data sources and systems used, date and time data often
must be normalized, that is, put into a standard format. This may include time
zone conversion or other actions, and presents a single, uniﬁed view of time for
analysis and further ﬁltering. This process makes it easier to further process since

2https://github.com/MetricsGrimoire/CVSAnalY
3Note that: “While Subversion automatically attaches properties (svn:date, svn:author,
svn:log, and so on) to revisions, it does not presume thereafter the existence of those properties,
and neither should you or the tools you use to interact with your repository.” https://svnbook.
red-bean.com/en/1.7/svn.advanced.props.html

Pitfalls and Guidelines for Using Time-Based Git Data

13

all data will be in the same format and thus is considered a cleaning technique.
As an example, it is speciﬁcally used by Claes and M¨antyl¨a (2020); Xu and Zhou
(2018); Baysal et al. (2012).

These are some of the most common time-based data ﬁltering techniques used.
In the next section we investigate and attempt to quantify how frequently problems
occur in time-based data.

3.3.7 Summary

33.79% of papers using time-based data describe using some sort of ﬁltering or
cleaning method: mining from within speciﬁc windows, mining from before a spe-
ciﬁc date, or using date format correction. Twelve papers describe using custom,
specialized conditions or methods based on the needs of the study. Further, the
papers that use CVS or RCS describe using a method to coalesce changes into
changesets, and several papers used CVSAnalY to perform their mining.

Finding 4: Filtering and cleaning techniques are used in less than 34% of MSR papers
using time data. Some of the more generally applicable techniques used include ﬁltering to
a time window, using a speciﬁc date cutoﬀ, or normalizing dates to a standard format.

4 The Pitfalls of Time-Based VCS Data

In the last section, we surveyed the MSR literature to determine how often time-
based data is used. Our results showed that time-based data is frequently used–by
at least 38% of MSR papers surveyed. We then looked at what the most frequently
used data kinds and data sources were. Based on those results, we observed that
VCS data (often Git) is the most used data kind and, speciﬁcally, data from
GitHub tends to be the most used data source. Based on these results, we identiﬁed
an additional research question to investigate:

RQ4 How frequently does bad time-based VCS data occur? Based
on the results of the prior research questions, we investigate Git data
from GitHub to quantify how frequently bad time-based data occurs.
Additionally, we investigate Subversion/CVS data from SourceForge to
show that these problems may exist in data from sources other than Git.

Since the previous section showed a large number of MSR research papers rely
on time-based data, it is important to get a feel for how often such data might
be bad. To date, no study has investigated how frequently bad time-based data
occurs and thus we do not know if existing (or future) studies are relying on a
large amount of possibly bad data.

In the remainder of this section, we outline our methodology. We do not directly
analyze the papers studied in the prior section, as prior research showed that many
research papers are not possible to replicate, often because of missing data (Robles,
2010; Ghezzi and Gall, 2013). Given this fact, we instead opt to directly analyze
a large number of open-source repositories and see if we can quantify how often
bad time-based data occurs in the wild. In a later section, we then investigate how
often those repositories with bad data are used by some MSR data papers.

14

Flint, Chauhan, and Dyer

4.1 Datasets Studied

To attempt to quantify problems with time-based data, we utilize the Boa infras-
tructure (Dyer et al., 2013, 2021). We use Boa as it provides many large, diverse
datasets which have been pre-processed to ease analysis and we are already famil-
iar with it. Because Boa provides these large, pre-processed datasets, we are not
required to re-collect and build a dataset, speeding our analyses and enabling eas-
ier replication. We rely on four diﬀerent datasets: “2013 September/SourceForge”,
“2019 October/GitHub” (containing Java project commits), “2021 Aug/Kotlin”,
and “2021 Aug/Python”. An overview of these datasets is shown in Table 6, listing
the total number of projects and commits in each dataset.

Table 6 Summary of dataset characteristics.

SourceForge
(SVN)

Java
(Git)

Kotlin
(Git)

Python
(Git)

Projects
Commits

65,934
15,063,073

282,781
23,229,406

499,645
11,022,118

100,940
5,427,215

The ﬁrst of these is composed of open-source projects from SourceForge, the
remaining three datasets contain open-source projects from GitHub. The Source-
Forge dataset contains around 65k Subversion (SVN) projects with at least one
revision from over 50 programming languages, including over 23k Java projects,
9k C++ projects, 4k C projects, 4k C# projects, and 900 PHP projects. The
Java dataset contains over 200k projects, the Kotlin dataset contains almost 500k
projects 4, and the Python dataset contains around 100k projects. In total, these
four datasets provide over 54 million total commits.

For some of the analyses, we needed to know a commit’s list of parent commit
hashes. Some of the older Boa datasets did not directly provide that information,
so we utilize the GitHub API to obtain it. Since some of the repositories in the
Boa dataset also no longer exist on GitHub, we would not be able to utilize their
API to obtain that information. As such, we also utilized the Software Heritage
Archive (Cosmo and Zacchiroli, 2017; Software Heritage developers, 2020) to at-
tempt to locate any repositories in the Boa datasets that were deleted from GitHub
since the Boa dataset was built.

We identify and remove exact duplicate commits (same commit hash) from the

GitHub datasets5. All analyses in this paper utilize the de-duplicated datasets.

4.2 Query Approach

Figure 3 shows the relevant Boa query used to collect data for our investigation.
Note that Boa stores commits in a topologically sorted array based on the commit
parent(s). This means traversals on the commits (called Revisions in Boa) are
performed in topological order.

4The Kotlin dataset contains some projects which may exist in the Java dataset.
5The SF.net dataset contained Subversion projects, which store commit IDs as integers

and thus are not unique across projects and can not be easily deduplicated.

Pitfalls and Guidelines for Using Time-Based Git Data

15

1 P: output collection[string][string] of time;
2 P2: output collection[string][string] of time;
3 P3: output collection[string][string] of time;

4 cvs_release_date := T"Mon Nov 19 00:00:00 UTC 1990";
5 boa_dataset_date := T"Thu Oct 31 00:00:00 UTC 2019";
6 last: Revision;

before r: Revision -> {

7 visit(input, visitor {
8
9
10
11
12

if (r.commit_date < cvs_release_date)

P[input.project_url][r.id] << r.commit_date;

else if (r.commit_date > boa_dataset_date)

P2[input.project_url][r.id] << r.commit_date;

if (def(last)

&& r.commit_date < last.commit_date
&& !match("merge", lowercase(last.log))
&& !match("merge", lowercase(r.log)))

P3[input.project_url][r.id] << r.commit_date;

last = r;

13
14
15
16
17
18
}
19
20 });

Fig. 3 Boa query to ﬁnd bad commit timestamps in the Java dataset. This query is the
combination of http://boa.cs.iastate.edu/boa/?q=boa/job/public/90164, http://boa.cs.
iastate.edu/boa/?q=boa/job/public/90169, and http://boa.cs.iastate.edu/boa/?q=boa/
job/public/90973 for presentation purposes. Similar queries were built for the other datasets
by changing the date on line 5.

The query outputs the project URL, the commit ID, and any suspicious commit
timestamp. The query looks for three possible kinds of bad time data indexed by
project URL and revision ID. First, it looks for suspicious commit timestamps
that seem too old (lines 9–10). We deﬁne a commit as being suspiciously old if
it occurred prior to the release date of CVS (Nov 19, 1990), as our datasets are
based on SVN and Git and post-date even CVS’s release.

Second, it looks for suspicious commit timestamps that seem too new (lines
11–12). Here we use the date of the dataset itself as the deﬁnition of the current
time, and look for any commit in the “future.” This date is changed for each
dataset to match the date of the dataset.

Third, it looks for commits that have a parent that is newer than themselves
(lines 13–18). Since the commits are ordered topologically based on time, a commit
should always have a timestamp that is not older than its parent’s timestamp.

In the next section, we investigate how frequently bad time-based VCS data

occurs in the studied datasets.

5 RQ4: How frequently does bad time-based VCS data occur?

In this section we investigate some potential problems with Git, SVN, and CVS
timestamps and attempt to quantify how frequently such problems occur in the
wild.

16

Flint, Chauhan, and Dyer

Table 7 Suspiciously old commit timestamps in the Java dataset

Count

Timestamp Date/Time

1
3576
1
1
1
1
1
1
1
1
1
1
1
2
3
2
1
1
1
11
1
1
1

-2044178335000000
0
730000000
956000000
1585000000
1601000000
1627000000
3495000000
3523000000
7403000000
7558000000
7923000000
88210000000
88211000000
88212000000
88213000000
127771000000
179895000000
255447000000
1000000000000
315772873000000
566635987000000
589770257000000

03/23/1905, 12:41:05 PM
01/01/1970, 12:00:00 AM
01/01/1970, 12:12:10 AM
01/01/1970, 12:15:56 AM
01/01/1970, 12:26:25 AM
01/01/1970, 12:26:41 AM
01/01/1970, 12:27:07 AM
01/01/1970, 12:58:15 AM
01/01/1970, 12:58:43 AM
01/01/1970, 02:03:23 AM
01/01/1970, 02:05:58 AM
01/01/1970, 02:12:03 AM
01/02/1970, 12:30:10 AM
01/02/1970, 12:30:11 AM
01/02/1970, 12:30:12 AM
01/02/1970, 12:30:13 AM
01/02/1970, 11:29:31 AM
01/03/1970, 01:58:15 AM
01/03/1970, 10:57:27 PM
01/12/1970, 13:46:40 PM
01/03/1980, 06:41:13 PM
12/16/1987, 06:53:07 AM
09/09/1988, 02:04:17 AM

5.1 Looking for Suspicious Commit Timestamps

First we investigated to see if there were suspicious commit timestamps within the
studied repositories. For Git repositories, one might expect the commit timestamps
to be after the initial release of Git (around 2005). It is however possible some
repositories were in a diﬀerent version control system (such as CVS or Subversion)
and converted to Git. For the sake of this study, we decided to investigate any
commit timestamp prior to the release of CVS version 1.0 (19 November 1990).
The relevant part of the Boa query in Figure 3 is lines 9–10.

The result of this query found 3,612 suspicious commit timestamps from 51
projects in the Java dataset. For those projects, this represents 4.20% of their total
commits. For the full dataset, this represents 0.02% of the commits. In total, there
were 23 unique suspicious timestamps (note: Boa timestamps are given as Unix
timestamps with milliseconds), listed in Table 7 along with the number of times
they occurred and their conversion to a human readable date format.

SourceForge had a similar large number of older commits (46,266). For that
data, the majority of them were the timestamp -1 and there were no commits with
timestamp 0. Kotlin and Python however showed diﬀerent results. Kotlin only had
6 old commits and Python only had 20 old commits. This might be due to the
fact these are newer languages (Kotlin) or recently becoming popular (Python)
and thus many projects might have started directly with Git.

As can be seen from the Java results above, the majority of the suspicious
timestamps are the value 0. There are however a handful of other suspicious
timestamps. For example, the 8 timestamps on January 2, 1970 at 12:30 all come

Pitfalls and Guidelines for Using Time-Based Git Data

17

Table 8 Table of frequent tokens appearing in suspicious commit messages, excluding commits
containing the frequent term “git-svn-id”. English stop words were removed.

n Token

309 process
309 http
18 ad
18 add
14 merg
13 move
13 ﬁx
12 commit
12 bug
10 git

n Token

10 enter
10 rc

7 empti
6 check
6 make
6 initi
6 thi
6 minor
6 function
6 chang

from a single project that was ported over from Microsoft’s CodePlex.6 Most likely
there was a problem in that porting process.

In fact, many of these suspicious timestamped commits seem to come from
tools, such as git-svn.7 This tool was popular in the period between when Sub-
version was more common and people were starting to move to Git. It allows
maintaining a Git clone of a Subversion repository but required inserting ‘git-svn-
id’ tags into the commit messages to properly track the SVN repository. We were
able to verify 3,153 of the commit logs (for Java, Kotlin, and Python) via GitHub’s
API, and 2,847 of those commits (90%) contain a git-svn-id tag in the message.
Since that tool accounted for such a large portion of the commits, we inves-
tigated all the remaining commits, to look for other possible common tools, by
generating a table of frequently occurring words in the commit logs. We generated
this table by using NLTK8 to ﬁrst tokenize the commit messages, then removing
all English stop words, and lemmatizing and stemming the remaining words. We
then group and sort the remaining tokens, shown in Table 8. Note that we also
removed any commits containing “git-svn-id”. What remains does not seem to
indicate any additional tools accounting for a large portion of the commits.

We also investigated dates that might be in the future. For this we used a
cutoﬀ time of the Boa dataset’s release date (e.g., 31 October 2019 for the Java
dataset). Any commit with a time later than the release date was output. This
analysis yielded 11 commits from 3 projects in the Java dataset where the dates
were in the years 2025, 2027, and 2037. Clearly these commits have invalid dates.
A manual inspection of these commits showed the commits were (based on the
Git graph) in between commits with dates that appear valid, indicating the years
were oﬀ. Most likely these invalid dates were generated through either user error
or misconﬁgured clocks.

The other three datasets all had similarly small number of future commits,
with 75 commits in 1 project for the SourceForge dataset, 0 commits for Kotlin,
and 4 commits in 3 projects for Python. In general it appears future dates are
relatively rare, regardless of the programming language or forge.

6https://archive.softwareheritage.org/browse/origin/log/?origin_url=https:

//github.com/KevinHoward/Irony&timestamp=2015-07-29T09:07:18Z

7https://archive.softwareheritage.org/browse/origin/log/?origin_url=https:

//github.com/maodouzi/PY&timestamp=2015-08-07T07:29:54Z

8https://www.nltk.org/

18

Flint, Chauhan, and Dyer

Finding 5: We found both suspiciously old and suspiciously new commit timestamps in
all datasets. Old timestamps (such as 0) are much more common, while suspiciously new
timestamps far out in the future are less common.

5.2 Finding Out-of-order Commits

Another possible problem with VCS tools allowing users and tools to set the
commit date is that the date speciﬁed might seem valid, but actually be wrong.
This could lead to a graph where a particular node has a commit date that is
actually older than its parent node. Obviously such a case should not make sense.
This might be due to a misconﬁgured clock on a particular computer9, specifying
the wrong time zone10, or any other number of causes.11 We call these out-of-order
commits.

In this section, we investigate how frequently such out-of-order commits occur
in Boa’s dataset. To do this, we traverse the revision list of each code repository
in order and compare the commit date of a revision to the commit date of the
previous revision. Due to how Boa linearizes the commit graph using a topological
sort, this might not technically be a parent (indeed, commit nodes might also have
multiple parents due to branching) but it can give us insight into this problem.

In the ﬁrst attempt at writing this query, we noticed a lot of results where one
of the two commits were explicitly marked (in the log) as a merge commit. We
decided to ﬁlter those out as merging behavior might induce a lot of false positives.
The relevant part of the Boa query in Figure 3 is lines 13–18.

Running this query on the Java dataset gave us 18,685 suspicious commits from
4,275 projects. For those projects, this represents 0.59% of their total commits. For
the full dataset, this represents 0.12% of the commits. We used the GitHub API to
download the JSON metadata for as many of the commits as possible and for any
missing commits attempted to obtain JSON metadata from Software Heritage.
This left us with 18,379 commits, which we then veriﬁed their commit timestamp
against each of their parent commit timestamps. That process indicated a total of
13,611 commits from 3,967 projects had at least one parent that was newer than
the commit itself.

Kotlin had a similar number of out-of-order commits, totalling 2,635 commits
from 1,754 projects. Python actually had about three times more than Java (per
project) with 12,275 commits from 1,376 projects. And SourceForge had substan-
tially more, with 65,238 commits from 53,820 projects.

From these out-of-order commits in all three datasets, we examine just how far
the children are committed before their parent. If the distance is short, it might
indicate clock skew issues. If the distance is far, it might indicate tools causing
the problem. In terms of seconds, we show the summary statistics in Table 9. This
shows that the median out-of-order commit is around 28k seconds before its parent
in Java, 9k seconds before its parent in Kotlin, and almost 40k seconds before its
parent in Python. All of these values are less than one day. In addition to the

9https://stackoverflow.com/questions/633353
10https://stackoverflow.com/questions/52507279
11https://stackoverflow.com/questions/16259105

Pitfalls and Guidelines for Using Time-Based Git Data

19

Table 9 For out-of-order commits, how far in the future (in seconds) is the parent commit
compared to the child commit?

dataset

Java
Kotlin
Python

mean

std min

25% / 50% /

75%

max

66,336,948.24
3,176,263.51
1,680,997.88

279,541,949.43
47,386,198.06
33,655,968.10

1
0
0

2,726 / 28,418 / 313,125.25
1,208 / 9,688 / 40,054.00
12,357 / 39,944 / 86,400.00

1,395,555,801
1,585,965,306
1,485,020,157

Fig. 4 Histogram of parent − child diﬀerences for out-of-order commits.

median (50%), we present mean, standard deviation, minimum , 25th and 75th
percentiles, and maximum.

We also present a histogram of the data as eleven buckets in Figure 4, from
fairly small time spans (≤ 30s) to medium-size (≤ 1d) to much longer (> 1y).
Now you can clearly see that many appear to occur within one day, hinting at a
potential misconﬁgured timezone, and those with a diﬀerence less than a minute
are likely some level of clock skew. However this is just speculation, as we can’t
tell from the commits themselves what accounts for these diﬀerences.

Finding 6: All four datasets contained a large number (2k-18k) of out-of-order commits.

In the next sections we look at some common tools, users, and projects observed

in the out-of-order dataset.

5.2.1 Common Tools

We further processed the commits suspected to be out of order. Having done so, we
collected all commit messages and removed English stop words to produce a table
of frequent words (Table 10). We also generated a word cloud (not shown) of all
words, allowing us to visually analyze terms frequently used in the bad commits.
We used the word cloud to get a feel for some commonly occurring problems

20

Flint, Chauhan, and Dyer

Table 10 Twenty most common tokens appearing in bad commit messages. English stop
words were removed.

n Token

3264 updat
2744 ﬁx
1846 ad
1473 add
1440 http
1185 chang
1148 git-svn-id
1099 test

906 creat
891 use

n Token

868 ﬁle
828 commit
800 reviewed-bi
770 gener
744 thi
731 remov
685 bug
637 code
574 d0ab736e-dc22-4aeb-8dc9-08def0aa14fd
573 work

without limiting ourselves to just the most frequently occurring words. By doing
so, we were able to note a handful of tools that have a tendency to produce bad
commit timestamps.

Review systems like Gerrit12 seem to be a frequent contributor to bad commits,
as found by the Change-Id commit footer (511 times). We suspect this is due to
the “push, review, commit/rebase, force push, GOTO review” method that is used
by many participants in the code review process.

We also frequently found other commit log footers, like Reviewed-by (920 times
total). These are used in other review processes, which involve either rebasing to
edit commit messages to include them, or passing patch sets via email.

We also noticed another mixed VCS, namely hg-git13, which allows a Mercu-
rial user to manipulate a Git repository using Mercurial commands. In particular,
we note the addition of metadata to commits, with the rebase source footer (seen
351 times), which is likely a result of rebases on Git repositories using Mercurial
or similar tools. Mercurial’s abbreviation, hg is also found 523 times.

Google produced a tool, MOE14 (Make Open Easy) which is used to synchro-
nize two repositories, one internal, and one open to the public. This tool can syn-
chronize, translate content between kinds of repositories, and scrub content from
a repository. Because of these features, we suspect that use of this tool produced a
mismatch between repositories, where an open-source repository received patches
from an internal repository after receiving patches from other contributors. We
see MOE related messages show up 465 times across the bad commits.

Finding 7: There are several commonly used tools that seem to cause bad Git timestamps,
often tools that convert or interoperate with other version control systems.

5.2.2 Commits On and Oﬀ GitHub

Speciﬁcally when we inspected the Kotlin dataset, we noticed a lot of commonly oc-
curring commit messages along the lines of “Created ﬁle.ext” or “Updated ﬁle.ext”.

12https://gerritcodereview.com
13https://www.mercurial-scm.org/wiki/HgGit
14https://opensource.google/projects/moe

Pitfalls and Guidelines for Using Time-Based Git Data

21

Fig. 5 An example of veriﬁed and non-veriﬁed commits that are out-of-order (from https:
//github.com/HamzaHix/first_tabBar_andFragments/commits/master).

We inspected a couple of these results to see what might be happening and quickly
realized those repositories had a mixture of veriﬁed and unveriﬁed commits.

For example, consider the commits shown in Figure 5. The parent commit has
a time of 6:39PM CST. The child commit has an older time of 5:56PM CST. But
notice the child is marked as a veriﬁed commit. This commit states “This commit
was created on GitHub.com and signed with GitHub’s veriﬁed signature.” The
parent commit does not indicate it was a veriﬁed commit, and thus we can safely
assume it was created on a diﬀerent machine and pushed to GitHub.

This seems to indicate the user’s machine had the wrong time set: either the
time was just bad or there was some timezone misconﬁguration causing it to be oﬀ
by one or more hours. We investigated how often the out-of-order commits in our
dataset were marked as veriﬁed by inspecting the commit’s JSON data to look for
the verified attribute. In total, we found 864 out-of-order commits were marked
as veriﬁed.

Finding 8: Machine time diﬀerences, between the GitHub servers and individual user’s
machines, can often introduce bad Git timestamps.

5.2.3 Common Authors and Repositories

We were interested to see if a few number of authors or repositories contributed
a large number of bad commits. If this is the case, it could make ﬁltering much
easier in cases where a small number of bad commits might still be acceptable.

Using the collected commit data, we analyzed commit author information and
counted the number of commits made by the top-20 committers of the bad commits
in our dataset. From this, we found that 9,383 commits (26% of all bad commits)
were made by the top 20 committers (with all ‘(no name)’ committers grouped as
one). We keep commits with an unknown committer to help understand possible
causes for time issues. We suspect that a commit made without a committer name
is more likely to be using a mis-conﬁgured tool. Since we observed the use of
common tools as a common source of bad commit timestamps, it makes sense
there may be some committers (who utilize those tools) with a larger number of
bad commits.

Similarly, we collected the repository each bad commit belonged to and found
that the top-20 repositories, shown in Table 11, contributed 10,103 commits (59%
of all bad commits). Note that some of these projects appear to be clones (e.g.,
openjdk8-hotspot and openjdk8-jdk), where the original repository most likely
also contains bad commit timestamps. Boa only contains repositories not explicitly

22

Flint, Chauhan, and Dyer

Table 11 Top 20 projects with the most faulty commits.

Bad Commits Project

rawbinz/pythonsnippets
3347
jacksyen/pyastd
3314
771
kylenapped/shashy
571 HansiChan/SoccerPredictor
securesystemslab/zippy
535
ghaseminya/commiter
459
joehzli/seattle
195
uditrugman/openjdk8-jdk
143
113
xapi-project/xen-api
100 mytskine/mupdf-unoﬃcial

uditrugman/openjdk8-hotspot

80
69 D5rkUnl0ck3r/test
68
60
56
51
50
43
41
37

gisce/openobject-addons-extra
cjashfor/LinuxToolsProjectPatches
iw3hxn/server
cylc/cylc-ﬂow
igloosec/hue
tandong8888/topsun
talknomoney66/TalkNoMoneyShare
cbeust/kobalt

marked as forks (what Pietri et al. (2020) call “forge forks”), so these projects most
likely cloned and uploaded the repository without utilizing GitHub’s fork feature.

Finding 9: These results indicate a small number (20) of authors and repositories tend
to account for a relatively large percentage (26% and 59%, respectively) of bad commits.

5.3 Summary

To summarize, we were able to ﬁnd thousands of bad commit timestamps, as
shown in Table 12. All four datasets contained bad timestamps, with the older
SourceForge data having more than the newer Git datasets. All programming
languages seem to contain commits with bad time data.

Table 12 Summary of commits exhibiting “bad” timestamps.

Out-of-order
Commits Projects Commits Projects Commits Projects

Future

Old

SourceForge
Java
Kotlin
Python

46,266
3,612
6
20

11
51
6
8

65,238
13,599
2,635
12,275

53,820
3,967
1,754
1,376

75
11
0
4

1
3
0
3

Many of these commits seem to originate from tool use, especially tools that
migrate or synchronize between two version control systems. In addition, we saw
a small number of committers and projects seem to contribute a large number of
the bad commit timestamps.

Pitfalls and Guidelines for Using Time-Based Git Data

23

6 Potential Impact of Time Problems

To investigate the potential impact the time issues we observed might have, we
chose to look at the published MSR data papers in detail to see if there may be
time data problems with those datasets. We did not investigate the data from
the technical track papers, as many research papers (especially older ones) lack
making the data available or the link to their data is no longer valid (Robles,
2010). Additionally, the data papers have potential for lots of reuse, as was shown
by Kotti and Spinellis (2019). Thus if there are problems with the data in the data
papers, potentially many other papers might be aﬀected.

6.1 Additional Filtering

First we looked through the 81 data papers and identiﬁed any additional ﬁltering
(beyond time-based ﬁltering) they might have performed, as such a ﬁlter might po-
tentially weed out bad time-based data as well. Two of the authors independently
read each paper’s approach and evaluation sections to look for any mention of
potential ﬁltering, such as ﬁltering projects based on metadata or commits based
on deduplication. They then discussed any diﬀering results to reach an agree-
ment. Based on that analysis, it seems many data papers (48 out of 81) do not
explicitly mention any ﬁltering. The top three ﬁltering techniques observed were:
deduplication (at various levels) in 7 (8.64%) papers, popularity (stargazer count
on GitHub) in 5 (6.17%) papers, and selecting speciﬁc programming languages
(often, Java) in 3 (3.70%) papers.

The datasets we analyzed in this paper were all deduplicated and spanned
several programming languages, including Java, Kotlin, and Python. Despite that,
we still observed quite a few bad commits, so it seems these two ﬁltering techniques
are probably not suﬃcient to avoid bad time-based data. Given that result, we
focus here on using stargazer counts to ﬁlter bad time-based data.

Based on a range of noted minimum star counts from the selected dataset
papers, we report the number of commits remaining when we ﬁlter projects by a
minimum of stars across the three GitHub-derived Boa datasets. The results are
shown in Table 13.

Table 13 Bad commits remaining in projects when ﬁltered by stars.

Min. Stars

Java

Kotlin

Python

1
2
5
10
50
100
500
1000

967 / 37% 3,740 / 30%
11,460 / 51%
672 / 25% 2,949 / 24%
10,339 / 46%
474 / 18% 2,001 / 16%
8,663 / 39%
367 / 14% 1,787 / 15%
5,844 / 26%
1,538 / 13%
2,805 / 13%
219 / 8%
895 / 7%
2,365 / 11% 165 / 6%
52 / 2%
120 / 1%
36 / 1%

385 / 2%
216 / 1%

86 / 0.7%

These results show that even with the highest stargazer ﬁlter we found in the
data papers (100, from Kim et al. (2021)), thousands of commits with time-based

24

Flint, Chauhan, and Dyer

problems still remain. In general, it seems possible to ﬁlter a large percent of
bad commits using stars, but would require a higher cutoﬀ value than typically
observed (such as 500). We have personally observed some research papers that
select the top-K projects based on highest star counts.

Such an approach may be eﬀective, for example in Java the top-1k projects
by star count would yield projects with over 2k stars each and, in theory, ﬁlter
a large amount of bad time-based data. This however is no guarantee, as one of
the projects in our datasets that contained bad time data (the Python project
scrapy15, with 17 bad commits) actually has over 41k stars.

Finding 10: Filtering on non-time based criteria, such as stargazer count, often is not
eﬀective. It can be eﬀective, if the cutoﬀ is suﬃciently high (500+) or when selecting the
top-K projects sorted by star count.

6.2 Investigating Data Papers

Since it seems likely the data papers are not employing a ﬁltering technique that
would ﬁlter out bad time data and since we have several Git-based datasets, here
we look at all data papers that used Git data. This gave us 34 papers out of the 81
published data papers using time-based data. From that set of 34, we then checked
if the dataset is still accessible and if it provides raw Git repositories. This left us
with 11 out of 81 data papers (13.58%) for our analysis.

For each of the 11 data papers, we intersected the projects in their dataset
against the Java, Kotlin, and Python datasets to see if we have similar projects.
The results are shown in Table 14, where we list the paper, how many citations
it has at the time of writing (based on Google Scholar), which Boa dataset(s)
intersected with it, and the “Projects Covered” column contains two numbers: the
ﬁrst is the number of projects intersecting the Boa dataset(s) and the second is
the total number of GitHub projects in the original data paper.

Then, based on the list of projects we found, we look to see how many commits
in those projects contained time errors. The results are shown in Table 15. As in
the previous sections, these results are all deduplicated commits.

First, we notice that in 9 of the 11 datasets we were able to ﬁnd bad commits.
This does not mean the other 2 datasets lack bad time data, simply that the
intersection was very small and contained no bad commits and thus we are unable
to easily identify problems without inspecting each project in those datasets. All of
the other 9 projects contained out-of-order commits and none of them contained
future commits. 5 of the 9 contained old commits. The fact we found so many
problems with such a small sample of the datasets (anywhere from 0.36% to 40.04%
of the projects) hints that there could be more bad commits in those datasets.

These 9 papers have been cited 164 times according to Google Scholar at the
time of writing this paper. Thus there are a large number of research papers
that potentially used these datasets and may have relied on these bad commit
timestamps. This shows that it is especially important to carefully handle (ﬁl-
ter and/or clean) commit data when building a reusable research dataset as any
potential problems with the data could propagate to other research.

15https://github.com/scrapy/scrapy

Pitfalls and Guidelines for Using Time-Based Git Data

25

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0
3

1
2

1

4
9

1
4

4
1
1

0
0
5

4
2
4

3
5
5

0

0

1
2
9

3
2
7

4

9
9

5
6
2

7
3
3

0

0

2
4
8
,
3

5
0
0
,
2

3
9
5
,
2

0

0

0

0

1

1

2
1

8

4
1

0

0

0

0

0

0

1

2

0

0

5
8
9

7
2
1

6
9
8
,
2

e
r
u
t
u
F

r
e
d
r
o
-
f
o
-
t
u
O

d
l
O

s
t
c
e
j
o
r
P

s
t
i

m
m
o
C

s
t
c
e
j
o
r
P

s
t
i

m
m
o
C

s
t
c
e
j
o
r
P

s
t
i

m
m
o
C

)
0
2
0
2

,
.
l
a

t
e

s
i
l
l
e
n
p
S
(

i

S
S
O
n
e
v
i
r
d
-
e
s
i
r
p
r
e
t
n
E

)
8
1
0
2

,
.
l
a

t
e

n
n
a
m
r
e
h
c
S
(

r
e
k
c
o
D

)
a
0
2
0
2

,
.
l
a

t
e

i

u
L
(

o
o
Z
o
r
d
n
A

)
7
1
0
2

,
.
l
a

t
e

u
h
Z
(
C
S
o
D

)
1
2
0
2

,
.
l
a

t
e

x
u
e
i
r
u
D
(

S
T
E
U
D

)
1
2
0
2

,
.
l
a

t
e

c
i
b
a
D
(
T
R
A
E
S

)
0
2
0
2

,
.
l
a

t
e

`o
r
o
C
(
C
e
T
J

)
1
2
0
2

,
i

h
c
s
e
n
a
v
l
a
S

d
n
a

i

n
a
d
n
a
k
s
E
(

s
s
e
l
r
e
d
n
o
W

)
1
2
0
2

,
.
l
a

t
e

u
l
o
v
a
g
a
V
(

6
2
5
E
G

)
8
1
0
2

,
.
l
a

t
e

r
e
g
i
e
G

(

i

e
n
h
c
a
M
e
m
T
d
i
o
r
d
n
A

i

)
5
1
0
2

,
.
l
a

t
e

u
c
s
e
l
i
s
a
V
(

y
t
i
s
r
e
v
i
D

r
e
p
a
P
t
e
s
a
t
a
D

3
3
0
,
8
9
/
3
5
3

4
7
4
,
3
2
/
9
3
5
,
1

6
1
2
,
8
/
7
8
0
,
1

0
1
/
6

5
5
2
,
7
1
/
7
8
4

7
1
2
,
0
5
/
5
6
5
,
5

0
1
5
,
8
3
9
/
5
0
7
,
3
1

1
9
9
,
7
4
1
/
2
7
1
,
3
1

2
3
2
,
1
3
/
6
0
9
,
2
1

7
7
8
,
1
/
3

1
0
1
/
2

n
o
h
t
y
P

,

n

i
l
t
o
K

,
a
v
a
J

n

i
l
t
o
K

,
a
v
a
J

n
o
h
t
y
P

,

n

i
l
t
o
K

,
a
v
a
J

n
o
h
t
y
P

,

n

i
l
t
o
K

,
a
v
a
J

n

i
l
t
o
K

,
a
v
a
J

n
o
h
t
y
P

,

n

i
l
t
o
K

,
a
v
a
J

n
o
h
t
y
P

,

n

i
l
t
o
K

,
a
v
a
J

n
o
h
t
y
P

,

n

i
l
t
o
K

,
a
v
a
J

n
o
h
t
y
P

,

n

i
l
t
o
K

,
a
v
a
J

a
v
a
J

a
v
a
J

9
7

7
3

0
2

4
1

6

4

2

1

1

0

0

)
0
2
0
2

,
.
l
a

t
e

s
i
l
l
e
n
p
S
(

i

S
S
O
n
e
v
i
r
d
-
e
s
i
r
p
r
e
t
n
E

)
8
1
0
2

,
.
l
a

t
e

n
n
a
m
r
e
h
c
S
(

r
e
k
c
o
D

)
a
0
2
0
2

,
.
l
a

t
e

i

u
L
(

o
o
Z
o
r
d
n
A

)
7
1
0
2

,
.
l
a

t
e

u
h
Z
(
C
S
o
D

)
1
2
0
2

,
.
l
a

t
e

x
u
e
i
r
u
D
(

S
T
E
U
D

)
1
2
0
2

,
.
l
a

t
e

c
i
b
a
D
(
T
R
A
E
S

)
0
2
0
2

,
.
l
a

t
e

`o
r
o
C
(
C
e
T
J

)
1
2
0
2

,
i

h
c
s
e
n
a
v
l
a
S

d
n
a

i

n
a
d
n
a
k
s
E
(

s
s
e
l
r
e
d
n
o
W

)
1
2
0
2

,
.
l
a

t
e

u
l
o
v
a
g
a
V
(

6
2
5
E
G

)
8
1
0
2

,
.
l
a

t
e

r
e
g
i
e
G

(

i

e
n
h
c
a
M
e
m
T
d
i
o
r
d
n
A

i

)
5
1
0
2

,
.
l
a

t
e

u
c
s
e
l
i
s
a
V
(

y
t
i
s
r
e
v
i
D

t
e
s
a
t
a
D

l
a
n
i
g
i
r
O
/
a
o
B
n
I

d
e
r
e
v
o
C
s
t
c
e
j
o
r
P

)
s
(
t
e
s
a
t
a
D
a
o
B

s
n
o
i
t
a
t
i
C

r
e
p
a
P
t
e
s
a
t
a
D

.
a
t
a
d

e
m

i
t

d
a
b

y
l
l
a
i
t
n
e
t
o
p

r
o
f

d
e
z
y
l
a
n
a

s
r
e
p
a
p

a
t
a
d
R
S
M

4
1

e
l
b
a
T

.
s
r
e
p
a
p

a
t
a
d
R
S
M
d
e
i
d
u
t
s

n

i

s
p
m
a
t
s
e
m

i
t

”
d
a
b
“

g
n

i

i
t
i
b
h
x
e

s
t
i

m
m
o
c

f
o

y
r
a
m
m
u
S

5
1

e
l
b
a
T

26

Flint, Chauhan, and Dyer

Table 16 Percent of faulty commits removed by ﬁltering commits from or before a given year.

Filter Date Removed Bad Commits Remaining

≤ 2015
≤ 2014
≤ 2013
≤ 2012
≤ 2011
≤ 2010
≤ 2009
≤ 2008
≤ 2007
≤ 2006
≤ 2005
≤ 2004
≤ 2003

≤ 1992

≤ 1970

99.45%
99.29%
97.73%
70.38%
50.69%
36.17%
24.86%
18.14%
12.84%
11.05%
10.03%
8.95%
8.15%

6.63%

6.63%

...

...

11
14
45
593
987
1,278
1,504
1,639
1,745
1,781
1,801
1,823
1,839

1,869

1,869

Finding 11: When looking at 0.36% to 40.04% of the projects contained in 11 previously
published dataset papers, we found 9 of the 11 papers that are cited 164 times contain bad
time data.

7 Discussion and Guidelines

We showed that time-based data is utilized by a large number of MSR research (at
least 38% of papers). We then described some possible problems with timestamps
in Git data, the most used data kind, and attempted to quantify how often those
problems occur in the most used data source, GitHub. In this section, we discuss
some guidelines for handling time-based data.

When order is a component of an analysis, handling suspicious commits is
recommended. To do this, we recommend that any commit with a timestamp less
than 1 is removed. For the data we observed, this ﬁlter would remove about 98%
of suspicious commits.

In general, we also recommend searching the commit logs for projects that

contain a ‘git-svn-id’ tag and consider removing matching projects.

To handle the problem of out-of-order commits, we recommend four strategies:
1. ﬁltering commits before a certain date; 2. ﬁltering commits belonging to certain
projects; 3. ﬁltering only commits which are out-of-order; and/or, 4. using a robust
method of mining commits with rebasing. We will discuss each of these in turn,
including the beneﬁts and the potential problems each brings to the table.

7.1 Filtering Before a Speciﬁc Date

The ﬁrst ﬁltering method we suggest is removing commits from before a speciﬁc
date. We suggest this method due to its relative simplicity, as well as its eﬀect (see
Table 16). In particular, we suggest removing all commits before 1 January 2014,

Pitfalls and Guidelines for Using Time-Based Git Data

27

as doing so could remove 97.73% of all bad commits. Even ﬁltering this much data
still leaves (at the time of writing) eight years of historical data to study.

If the research question requires longer history, this method may not be feasible

and the other ﬁltering methods mentioned later are recommended.

7.2 Filtering Speciﬁc Projects

Given the exceedingly wide range of projects available on GitHub and similar sites,
the removal of projects known to have a large number of out-of-order commits
still leaves a large available corpus. The beneﬁt is that a longer history can be
maintained. This is a bit more complicated than simply ﬁltering by a speciﬁc date
though, as a list of so called “bad” projects would need to be known. The MSR
community could work toward maintaining such a list.

7.3 Filtering only Out-of-Order Commits

Another recommendation is to ﬁlter the out-of-order commits. The speciﬁc anal-
ysis would have to decide which commit(s) to remove in this case, and how that
might aﬀect the analysis results. For example, the simplest solution would be to
remove all out-of-order commits. However if an analysis relies on pairs of commits,
e.g. to determine co-changes, such a study may have to remove additional neigh-
boring commits. Similarly, if a study looks at the full history of ﬁles, then any
ﬁles modiﬁed in the removed commits may trigger additional removals of other
commits modifying the same set of ﬁles. In such a case, it may be easier to simply
reject any project with one or more out-of-order commit.

This ﬁltering method is the most computationally expensive, requiring each
commit to be examined in turn. However, this method has the beneﬁt of not re-
moving other history and could be used to enable study of a repository that other-
wise may be problematic, or has substantial pre-2014 history. While some previous
work, such as Steﬀ and Russo (2012), built graphs with commit timestamps and
performed topological sorts on them and could very easily have identiﬁed such
out-of-order commits, they did not identify such a problem or suggest it as a
solution.

7.4 Ordering on Committer Date

Git commits store two timestamps: the author date and the committer date.
The author date is the date the commit was originally made. The committer date is
essentially the last date the commit was modiﬁed. Certain commands, e.g. cherry
picking, amending, and rebasing, modify the commits and thus will change the
committer date. Most of the time, the two timestamps are identical.

In this work we looked for out-of-order commits using the committer date. As
Git is a graph, those committer timestamps should be in non-decreasing order as
you walk the graph. The author timestamps however might be out of order – and
that is to be expected.

28

Flint, Chauhan, and Dyer

We recommend most papers utilize the committer date, and then look for out-
of-order issues in a project. For any work that needs to know about rebasing, e.g.
when analyzing the code review process, they may have to apply more complicated
techniques, especially if the project relies on a rebase-heavy workﬂow, such as those
utilizing Gerrit or similar review platforms.

7.5 Using Topological Order

Another possibility to handle out-of-order commits would be to ignore the
timestamps entirely and rely only on the actual structure of the graph. A topo-
logical ordering of that graph would provide the correct order of the commits,
regardless of their individual timestamps. Depending on the particular analysis
used, this strategy might be suﬃcient. This would only work if the analysis does
not rely on the speciﬁc timestamps and only needs to know the relationship among
the commits (parent-child, etc). For example, any analysis that simply needs to
know about the diﬀ between two versions of source ﬁles (e.g., if mining for refac-
torings).

7.6 Filtering by Star Counts

As we observed in Section 6, star counts are not always a good method for ﬁltering
out bad time data. It can be tricky to pick a suﬃciently high cutoﬀ value, without
selecting a high value of 1000 or more.

One method that probably works well however is to simply select the top-K
projects, based on star counts. Often, as long as the researcher is only selecting
the top 1k or so projects, those projects will have a high enough star count to
ﬁlter out much of the bad data.

7.7 Summary

The exact method(s) utilized will depend highly on the speciﬁc research questions
being answered. For example, if the research questions require a long history then
ﬁltering by date might not be the best approach. Additionally, researchers need to
decide if it is acceptable to simply drop the bad commits, or if projects with even
a single bad commit should be excluded entirely.

8 Threats to Validity

A threat to construct validity is the use of keyword-based search technique to
identify MSR papers using time-based data. We ﬁnd this technique is sound, as
we manually veriﬁed the results. It is however not complete, as a paper might
have utilized time-based data without using any of the selected keywords. Thus
the 38% of papers identiﬁed is a lower bound and the actual number of papers
utilizing time-based data might be even higher. The percentage we found is still
high enough to indicate this is an important problem.

Pitfalls and Guidelines for Using Time-Based Git Data

29

Another threat to construct validity is the choice to exclude merge commits
when looking for out-of-order commits. This was done as we observed quite a few
false-positives. However, many merge commits might actually contain out-of-order
timestamps and thus our choice might lead to under counting the amount of bad
time-based commits. We chose to err on the side of precision at the expense of
recall.

The notes for the “2019 October/GitHub” dataset indicate that the data was
actually collected in 201516. Because of the age of the data, many projects no longer
exist on GitHub. Even with the newer Kotlin and Python datasets there are several
projects no longer available. To mitigate this threat, we utilized the Software
Heritage Archive (Cosmo and Zacchiroli, 2017; Software Heritage developers, 2020)
to attempt to locate these repositories which have been deleted.

Boa’s datasets exclude explicitly marked forks (what Pietri et al. (2020) call a
“forge fork”). However, forks created oﬀ the website remain in the dataset causing
some commits to be duplicated. We attempted to mitigate this threat by identify-
ing and removing exact duplicate commits (same commit hash) from the GitHub
datasets.

A threat to internal validity is that some of the commits might actually suﬀer
from multiple problems. To quantify how many commits have a bad timestamp and
also are out of order, we intersect the two results. There are 877 (4.09%) commits
in Boa’s dataset that potentially suﬀered from both the out-of-order error as well
as being suspiciously old. Most of those (all but 11) are timestamps of 0.

For Python we actually see slightly diﬀerent results, as only 6 (0.042%) commits
in Boa’s dataset potentially suﬀered from both the out-of-order error as well as
being suspiciously old. Out of those 6, only 1 is the timestamp 0.

For Kotlin the results are similar to Python, as only 4 (0.14%) commits in
Boa’s dataset potentially suﬀered from both the out-of-order error as well as being
suspiciously old. None of those commits are the timestamp 0. Overall, it appears
that very few commits in the datasets we studied suﬀer from multiple problems.
A threat to internal validity is that Boa’s datasets tend to include a lot of dupli-
cated data (exact code clones). This often occurs due to a fork occurring outside of
GitHub that is not a “forge fork” (using the terminology of Pietri et al. (2020)). We
attempted to mitigate this issue by removing exactly duplicated commits (com-
mits with the same hash) and then reporting results for only the deduplicated
data. We were only able to do this for the three GitHub-based datasets, so the
SourceForge dataset might have duplicated commits in it.

Another threat to internal validity is that 917 out of the 7,569 projects (12.12%)
identiﬁed with time data problems no longer exist on GitHub (as of 8 October
2021). These projects are however still in the released Boa dataset, so we main-
tained their results in this study. We attempted to mitigate this threat by validat-
ing the data directly with GitHub using their API, and for the projects that were
missing we utilized the Software Heritage Archive (all projects were found). Note
however that the times Boa and Software Heritage indexed the projects might
diﬀer, and thus there were some commits found by Boa that we were not able
to verify. Such commits were removed from the dataset. Of the 38,624 total Git
commits found by Boa, 33,898 (87.76%) were still on GitHub, 3,572 (9.25%) were
found on Software Heritage, and 1,154 (2.99%) were excluded. We do not believe

16http://boa.cs.iastate.edu/boa/?q=content/dataset-notes-october-2019

30

Flint, Chauhan, and Dyer

this is a problem however, as the point of the analysis was to see if bad time data
exists, not to fully account for all such cases.

A threat to external validity is that we only originally studied Git reposito-
ries. However, although we did not quantify it explicitly, Subversion also allows
developers to modify (and even remove!) the commit date: svn propset -rXXX
--revprop svn:date. This was why we also analyzed an older SourceForge dataset
that contains CVS and SVN repositories and saw similar issues as observed in the
GitHub datasets. The results however may not generalize to any VCS that disal-
lows modifying commit timestamps.

Another threat to external validity is that certain tools that cause time prob-
lems are only available for certain programming languages so there is a need to
study commit data from multiple languages. To mitigate this threat, we stud-
ied three datasets from GitHub from three popular languages: Java, Kotlin, and
Python. We also studied SourceForge, which contains commit data from over 50
programming languages, including over 23k Java projects, 9k C++ projects, 4k C
projects, 4k C# projects, and 900 PHP projects.

9 Conclusion

The use of time-based data in MSR studies is wide-spread, in at least 38% of MSR
papers. Properly handling this time-based data is thus very important. However,
the diversity of tools and workﬂows used to generate the time-based data can
present challenges. In particular, ensuring that time data is consistent and main-
tains linearity is important. Further, we have found that many papers do not
describe cleaning or ﬁltering of time data, with those that do describe ﬁltering
tending towards simple techniques like selection from a deﬁned time span or selec-
tion of data before a certain date. Some papers have used more robust or rigorous
techniques, and may thus avoid some of the time-related problems found in data
such as Git repositories.

To remedy potential time-based issues in VCS data, such as that coming from
GitHub, we recommend a simple ﬁlter to drop any timestamp less than 1 as well as
a more complex ﬁltering to remove out-of-order commits. Ideally, each repository
would be analyzed to detect and remove the out-of-order commits, but depending
on the need a simple cutoﬀ ﬁlter removing commits prior to 2014 might suﬃce.
Applying both ﬁlters (the second ﬁlter actually implies the ﬁrst) is very simple
and would remove around 98% of all observed bad commits.

We also observed 9 (out of 11 studied) MSR data papers that exhibit bad time
data, showing that the problems identiﬁed occur in practice. These papers already
have over 150 citations, indicating further research has utilized this potentially
bad data.

In the future, we would like to investigate potential problems in other kinds
of time data, such as issue reports. We would also like to investigate how time-
based data is used when training machine learning models and if issues arise from
training on later observed data and then classifying on older data.

Acknowledgements The authors would like to thank Yijia Huang, Tien N. Nguyen, and
Hridesh Rajan for insightful discussions that inspired this paper. We also thank the anonymous
MSR’21 and EMSE reviewers for many suggestions that substantially improved this paper.

Pitfalls and Guidelines for Using Time-Based Git Data

31

Conﬂict of interest

The authors declare that they have no conﬂict of interest.

References

Ahasanuzzaman M, Asaduzzaman M, Roy CK, Schneider KA (2016) Mining dupli-
cate questions in Stack Overﬂow. In: Proceedings of the 13th International Con-
ference on Mining Software Repositories, Association for Computing Machinery,
New York, NY, USA, MSR ’16, p 402–412, DOI 10.1145/2901739.2901770

Antoniol G, Rollo VF, Venturi G (2005) Linear predictive coding and cepstrum
coeﬃcients for mining time variant information from software repositories. In:
Proceedings of the 2005 International Workshop on Mining Software Reposito-
ries, Association for Computing Machinery, New York, NY, USA, MSR ’05, p
1–5, DOI 10.1145/1083142.1083156

Baysal O, Holmes R, Godfrey MW (2012) Mining usage data and development ar-
tifacts. In: 2012 9th IEEE Working Conference on Mining Software Repositories
(MSR), pp 98–107, DOI 10.1109/MSR.2012.6224305

Bird C, Rigby PC, Barr ET, Hamilton DJ, German DM, Devanbu P (2009) The
promises and perils of mining Git. In: 2009 6th IEEE International Working
Conference on Mining Software Repositories, pp 1–10, DOI 10.1109/MSR.2009.
5069475

Cito J, Schermann G, Wittern JE, Leitner P, Zumberi S, Gall HC (2017) An empir-
ical analysis of the Docker container ecosystem on GitHub. In: 2017 IEEE/ACM
14th International Conference on Mining Software Repositories (MSR), IEEE,
DOI 10.1109/msr.2017.67

Claes M, M¨antyl¨a MV (2020) 20-MAD: 20 years of issues and commits of Mozilla
and Apache development. In: Proceedings of the 17th International Conference
on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR ’20, p 503–507, DOI 10.1145/3379597.3387487

Cosentino V, Izquierdo JLC, Cabot J (2016) Findings from GitHub: Methods,
datasets and limitations. In: 2016 IEEE/ACM 13th Working Conference on
Mining Software Repositories (MSR), pp 137–141

Cosmo RD, Zacchiroli S (2017) Software Heritage: Why and how to preserve soft-
ware source code. In: iPRES 2017: 14th International Conference on Digital
Preservation, Kyoto, Japan

D’Ambros M, Lanza M, Robbes R (2010) An extensive comparison of bug pre-
diction approaches. In: 2010 7th IEEE Working Conference on Mining Software
Repositories (MSR 2010), pp 31–41, DOI 10.1109/MSR.2010.5463279

Demeyer S, Murgia A, Wyckmans K, Lamkanﬁ A (2013) Happy birthday! A trend
analysis on past MSR papers. In: Proceedings of the 10th Working Conference
on Mining Software Repositories, IEEE Press, MSR ’13, p 353–362

Durieux T, Le Goues C, Hilton M, Abreu R (2020) Empirical study of restarted and
ﬂaky builds on Travis CI. In: Proceedings of the 17th International Conference
on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR ’20, p 254–264, DOI 10.1145/3379597.3387460

Dyer R, Nguyen HA, Rajan H, Nguyen TN (2013) Boa: A language and infras-
tructure for analyzing ultra-large-scale software repositories. In: Proceedings of

32

Flint, Chauhan, and Dyer

the 2013 International Conference on Software Engineering, IEEE Press, ICSE
’13, pp 422–431, DOI 10.5555/2486788.2486844

Dyer R, Nguyen HA, Rajan H, Nguyen TN (2021) Boa: Mining ultra-large-scale
software repositories. http://boa.cs.iastate.edu/boa/, accessed: 2021-10-14
Flint SW, Chauhan J, Dyer R (2021a) Escaping the time pit: Pitfalls and
guidelines for using time-based Git data. In: 2021 IEEE/ACM 18th Interna-
tional Conference on Mining Software Repositories (MSR), pp 85–96, DOI
10.1109/MSR52588.2021.00022

Flint SW, Chauhan J, Dyer R (2021b) Replication package for ”Pitfalls and
Guidelines for Using Time-Based GitData From Java, Kotlin, and Python
Projects”. DOI 10.5281/zenodo.7065577, URL https://doi.org/10.5281/
zenodo.7065577

Gasser L, Ripoche G, Sandusky RJ (2004) Research infrastructure for empirical
science of F/OSS. In: Proceedings of the 1st International Workshop on Mining
Software Repositories

Ghezzi G, Gall HC (2013) Replicating mining studies with SOFAS. In: Proceedings
of the 10th Working Conference on Mining Software Repositories, IEEE Press,
MSR ’13, p 363–372

Goeminne M, Claes M, Mens T (2013) A historical dataset for the Gnome ecosys-
tem. In: 2013 10th Working Conference on Mining Software Repositories (MSR),
pp 225–228, DOI 10.1109/MSR.2013.6624032

Gonzalez-Barahona JM, Robles G, Izquierdo-Cortazar D (2015) The MetricsGri-
moire database collection. In: Proceedings of the 12th Working Conference on
Mining Software Repositories, IEEE Press, MSR ’15, p 478–481

Hayashi J, Higo Y, Matsumoto S, Kusumoto S (2019) Impacts of daylight sav-
ing time on software development. In: Proceedings of the 16th International
Conference on Mining Software Repositories, IEEE Press, MSR ’19, p 502–506,
DOI 10.1109/MSR.2019.00076

Hemmati H, Nadi S, Baysal O, Kononenko O, Wang W, Holmes R, Godfrey MW
(2013) The MSR cookbook: Mining a decade of research. In: Proceedings of the
10th Working Conference on Mining Software Repositories, IEEE Press, MSR
’13, p 343–352

Kagdi H, Yusuf S, Maletic JI (2006) Mining sequences of changed-ﬁles from ver-
sion histories. In: Proceedings of the 2006 International Workshop on Mining
Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR ’06, p 47–53, DOI 10.1145/1137983.1137996

Kalliamvakou E, Gousios G, Blincoe K, Singer L, German DM, Damian D
(2014) The promises and perils of mining GitHub. In: Proceedings of the
11th Working Conference on Mining Software Repositories, Association for
Computing Machinery, New York, NY, USA, MSR 2014, pp 92–101, DOI
10.1145/2597073.2597074

Kalliamvakou E, Gousios G, Blincoe K, Singer L, German DM, Damian D (2016)
An in-depth study of the promises and perils of mining GitHub. Empirical Softw
Engg 21(5):2035–2071, DOI 10.1007/s10664-015-9393-5

Karampatsis RM, Sutton C (2020) How often do single-statement bugs occur? In:
Proceedings of the 17th International Conference on Mining Software Reposito-
ries, ACM, DOI 10.1145/3379597.3387491

Kikas R, Dumas M, Pfahl D (2016) Using dynamic and contextual features to
predict issue lifetime in GitHub projects. In: Proceedings of the 13th Interna-

Pitfalls and Guidelines for Using Time-Based Git Data

33

tional Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR ’16, p 291–302, DOI 10.1145/2901739.
2901751

Kotti Z, Spinellis D (2019) Standing on shoulders or feet? The usage of the MSR
data papers. In: Proceedings of the 16th International Conference on Mining
Software Repositories, IEEE Press, MSR ’19, p 565–576, DOI 10.1109/MSR.
2019.00085

Liu Y, Lin J, Cleland-Huang J (2020) Traceability support for multi-lingual soft-
ware projects. In: Proceedings of the 17th International Conference on Mining
Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR ’20, p 443–454, DOI 10.1145/3379597.3387440

Pietri A, Rousseau G, Zacchiroli S (2020) Forking without clicking: On how to
identify software repository forks. In: Proceedings of the 17th International Con-
ference on Mining Software Repositories, Association for Computing Machinery,
New York, NY, USA, p 277–287

Pimentel JaF, Murta L, Braganholo V, Freire J (2019) A large-scale study about
quality and reproducibility of Jupyter notebooks. In: Proceedings of the 16th
International Conference on Mining Software Repositories, IEEE Press, MSR
’19, p 507–517, DOI 10.1109/MSR.2019.00077

Robles G (2010) Replicating MSR: A study of the potential replicability of pa-
pers published in the Mining Software Repositories proceedings. In: 7th IEEE
Working Conference on Mining Software Repositories, MSR ’10, pp 171–180,
DOI 10.1109/MSR.2010.5463348

Robles G, Gonz´alez-Barahona JM, Cervig´on C, Capiluppi A, Izquierdo-Cort´azar
D (2014) Estimating development eﬀort in free/open source software projects
by mining software repositories: A case study of OpenStack. In: Proceedings
of the 11th Working Conference on Mining Software Repositories, Association
for Computing Machinery, New York, NY, USA, MSR 2014, p 222–231, DOI
10.1145/2597073.2597107

Sadowski C, Lewis C, Lin Z, Zhu X, Whitehead EJ (2011) An empirical analysis
of the FixCache algorithm. In: Proceedings of the 8th Working Conference on
Mining Software Repositories, Association for Computing Machinery, New York,
NY, USA, MSR ’11, p 219–222, DOI 10.1145/1985441.1985475

Software Heritage developers (2020) Software Heritage archive. https://archive.

softwareheritage.org/, accessed: 2020-12-28

Steﬀ M, Russo B (2012) Co-evolution of logical couplings and commits for defect
estimation. In: Proceedings of the 9th IEEE Working Conference on Mining
Software Repositories, IEEE Press, MSR ’12, p 213–216

Walker RJ, Holmes R, Hedgeland I, Kapur P, Smith A (2006) A lightweight ap-
proach to technical risk estimation via probabilistic impact analysis. In: Pro-
ceedings of the 2006 International Workshop on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR ’06, p 98–104,
DOI 10.1145/1137983.1138008

Wang P, Brown C, Jennings JA, Stolee KT (2020) An empirical study on regular
expression bugs. In: Proceedings of the 17th International Conference on Mining
Software Repositories, ACM, DOI 10.1145/3379597.3387464

Xu Y, Zhou M (2018) A multi-level dataset of Linux kernel patchwork. In: Pro-
ceedings of the 15th International Conference on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR ’18, p 54–57,

34

Flint, Chauhan, and Dyer

DOI 10.1145/3196398.3196475

Zhu J, Wei J (2019) An empirical study of multiple names and email addresses
in OSS version control repositories. In: 2019 IEEE/ACM 16th International
Conference on Mining Software Repositories (MSR), IEEE, DOI 10.1109/msr.
2019.00068

Zimmermann T, Weißgerber P (2004) Preprocessing CVS data for ﬁne-grained
analysis. In: Proceedings of the 1st International Workshop on Mining Software
Repositories, MSR ’04, pp 2–6

Pitfalls and Guidelines for Using Time-Based Git Data

35

A List of Boa jobs

In this section we list public links to all of the Boa queries utilized by our study. Full details
as well as all data (including generated data based on the Boa outputs) is available in our
replication package (Flint et al., 2021b).

A.1 Java Queries

All Boa queries were run on the ‘2019 October/GitHub‘ dataset.

Suspiciously ’old’ commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/90164
Suspiciously ’future’ commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/90973
Out-of-order commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/90169

A.2 Kotlin Queries

All Boa queries were run on the ‘2021 Aug/Kotlin‘ dataset.

Suspiciously ’old’ commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95104
Suspiciously ’future’ commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95113
Out-of-order commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95107

A.3 Python Queries

All Boa queries were run on the ‘2021 Aug/Python‘ dataset.

Suspiciously ’old’ commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95105
Suspiciously ’future’ commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95112
Out-of-order commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95106

A.4 SourceForge Queries

All Boa queries were run on the ‘2013 September/SF‘ dataset.

Suspiciously ’old’ commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95171
Suspiciously ’future’ commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95173
Out-of-order commits: http://boa.cs.iastate.edu/boa/?q=boa/job/public/95170

B List of Selected Papers

In this section, we list all papers selected for inclusion in the study (see Table 1).

2004 Selected Papers

Germ´an DM (2004) Mining CVS repositories, the softChange experience. In: MSR
Howison J, Crowston K (2004) The perils and pitfalls of mining SourceForge. In: MSR
Jensen C, Scacchi W (2004) Data mining for software process discovery in open source software

development communities. In: MSR

Liu Y, Stroulia E, Wong K, German D (2004) Using CVS historical information to understand

how students develop software. In: MSR

Zimmermann T, Weißgerber P (2004) Preprocessing CVS data for ﬁne-grained analysis. In:

MSR

36

Flint, Chauhan, and Dyer

2005 Selected Papers

Antoniol G, Rollo VF, Venturi G (2005) Linear predictive coding and cepstrum coeﬃcients
for mining time variant information from software repositories. In: Proceedings of the
2005 International Workshop on Mining Software Repositories, Association for Comput-
ing Machinery, New York, NY, USA, MSR ’05, p 1–5, DOI 10.1145/1083142.1083156, URL
https://doi.org/10.1145/1083142.1083156

´Sliwerski J, Zimmermann T, Zeller A (2005) When do changes induce ﬁxes? In: Proceedings
of the 2005 International Workshop on Mining Software Repositories, Association for Com-
puting Machinery, New York, NY, USA, MSR ’05, p 1–5, DOI 10.1145/1083142.1083147,
URL https://doi.org/10.1145/1083142.1083147

2006 Selected Papers

German DM, Rigby PC, Storey MA (2006) Using evolutionary annotations from change logs
to enhance program comprehension. In: Proceedings of the 2006 International Workshop
on Mining Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR ’06, p 159–162, DOI 10.1145/1137983.1138020, URL https://doi.org/10.1145/
1137983.1138020

Kagdi HH, Yusuf S, Maletic JI (2006) Mining sequences of changed-ﬁles from version histories.

In: MSR ’06

Knab P, Pinzger M, Bernstein A (2006) Predicting defect densities in source code ﬁles with
decision tree learners. In: Proceedings of the 2006 International Workshop on Mining Soft-
ware Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’06, p
119–125, DOI 10.1145/1137983.1138012, URL https://doi.org/10.1145/1137983.1138012
Parnin C, G¨org C, Rugaber S (2006) Enriching revision history with interactions. In: Pro-
ceedings of the 2006 International Workshop on Mining Software Repositories, Asso-
ciation for Computing Machinery, New York, NY, USA, MSR ’06, p 155–158, DOI
10.1145/1137983.1138019, URL https://doi.org/10.1145/1137983.1138019

Robles G, Gonzalez-Barahona JM, Michlmayr M, Amor JJ (2006) Mining large software
compilations over time: Another perspective of software evolution. In: Proceedings of the
2006 International Workshop on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR ’06, p 3–9, DOI 10.1145/1137983.1137986, URL
https://doi.org/10.1145/1137983.1137986

Walker RJ, Holmes R, Hedgeland I, Kapur P, Smith A (2006) A lightweight approach to
technical risk estimation via probabilistic impact analysis. In: Proceedings of the 2006 In-
ternational Workshop on Mining Software Repositories, Association for Computing Ma-
chinery, New York, NY, USA, MSR ’06, p 98–104, DOI 10.1145/1137983.1138008, URL
https://doi.org/10.1145/1137983.1138008

2007 Selected Papers

Anvik J, Murphy GC (2007) Determining implementation expertise from bug reports. In:
Fourth International Workshop on Mining Software Repositories (MSR’07:ICSE Workshops
2007), pp 2–2, DOI 10.1109/MSR.2007.7

Bird C, Gourley A, Devanbu P (2007) Detecting patch submission and acceptance in OSS
projects. In: Fourth International Workshop on Mining Software Repositories (MSR’07:ICSE
Workshops 2007), pp 26–26, DOI 10.1109/MSR.2007.6

Canfora G, Cerulo L, Penta MD (2007) Identifying changed source code lines from version
repositories. In: Proceedings of the Fourth International Workshop on Mining Software
Repositories, IEEE Computer Society, USA, MSR ’07, p 14, DOI 10.1109/MSR.2007.14,
URL https://doi.org/10.1109/MSR.2007.14

Hindle A, Godfrey MW, Holt RC (2007) Release pattern discovery via partitioning: Method-
ology and case study. In: Fourth International Workshop on Mining Software Repositories
(MSR’07:ICSE Workshops 2007), pp 19–19, DOI 10.1109/MSR.2007.28

Pitfalls and Guidelines for Using Time-Based Git Data

37

Minto S, Murphy GC (2007) Recommending emergent teams. In: Proceedings of the Fourth
International Workshop on Mining Software Repositories, MSR ’07, pp 5–5, DOI 10.1109/
MSR.2007.27

Mizuno O, Ikami S, Nakaichi S, Kikuno T (2007) Spam ﬁlter based approach for ﬁnding fault-
prone software modules. In: Fourth International Workshop on Mining Software Repositories
(MSR’07:ICSE Workshops 2007), pp 4–4, DOI 10.1109/MSR.2007.29

Robbes R (2007) Mining a change-based software repository. In: Fourth International Work-
shop on Mining Software Repositories (MSR’07:ICSE Workshops 2007), pp 15–15, DOI
10.1109/MSR.2007.18

Zimmermann T (2007) Mining workspace updates in CVS. In: Fourth International Workshop
on Mining Software Repositories (MSR’07:ICSE Workshops 2007), pp 11–11, DOI 10.1109/
MSR.2007.22

2008 Selected Papers

Hata H, Mizuno O, Kikuno T (2008) An extension of fault-prone ﬁltering using precise training
and a dynamic threshold. In: Proceedings of the 2008 International Working Conference
on Mining Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR ’08, p 89–98, DOI 10.1145/1370750.1370772, URL https://doi.org/10.1145/
1370750.1370772

Holmes R, Begel A (2008) Deep Intellisense: A tool for rehydrating evaporated information. In:
Proceedings of the 2008 International Working Conference on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR ’08, p 23–26, DOI
10.1145/1370750.1370755, URL https://doi.org/10.1145/1370750.1370755

Layman L, Nagappan N, Guckenheimer S, Beehler J, Begel A (2008) Mining software eﬀort
data: Preliminary analysis of Visual Studio Team System data. In: Proceedings of the 2008
International Working Conference on Mining Software Repositories, Association for Com-
puting Machinery, New York, NY, USA, MSR ’08, p 43–46, DOI 10.1145/1370750.1370762,
URL https://doi.org/10.1145/1370750.1370762

Pattison DS, Bird CA, Devanbu PT (2008) Talk and work: A preliminary report. In: Pro-
ceedings of the 2008 International Working Conference on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR ’08, p 113–116, DOI
10.1145/1370750.1370776, URL https://doi.org/10.1145/1370750.1370776

Ratzinger J, Sigmund T, Gall HC (2008) On the relation of refactorings and software defect
prediction. In: Proceedings of the 2008 International Working Conference on Mining Soft-
ware Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’08,
p 35–38, DOI 10.1145/1370750.1370759, URL https://doi.org/10.1145/1370750.1370759
Thomson C, Holcombe M (2008) Correctness of data mined from CVS. In: Proceedings of the
2008 International Working Conference on Mining Software Repositories, Association for
Computing Machinery, New York, NY, USA, MSR ’08, p 117–120, DOI 10.1145/1370750.
1370777, URL https://doi.org/10.1145/1370750.1370777

Weißgerber P, Neu D, Diehl S (2008) Small patches get in! In: Proceedings of the 2008 Inter-
national Working Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR ’08, p 67–76, DOI 10.1145/1370750.1370767, URL
https://doi.org/10.1145/1370750.1370767

2009 Selected Papers

Anbalagan P, Vouk M (2009) On mining data across software repositories. 2009 6th IEEE

International Working Conference on Mining Software Repositories pp 171–174

Bajracharya S, Lopes C (2009) Mining search topics from a code search engine usage log.
In: 2009 6th IEEE International Working Conference on Mining Software Repositories, pp
111–120, DOI 10.1109/MSR.2009.5069489

Bird C, Rigby PC, Barr ET, Hamilton DJ, German DM, Devanbu P (2009) The promises
and perils of mining git. In: 2009 6th IEEE International Working Conference on Mining
Software Repositories, pp 1–10, DOI 10.1109/MSR.2009.5069475

38

Flint, Chauhan, and Dyer

Boogerd C, Moonen L (2009) Evaluating the relation between coding standard violations
and faults within and across software versions. In: 2009 6th IEEE International Working
Conference on Mining Software Repositories, pp 41–50, DOI 10.1109/MSR.2009.5069479
German DM, Di Penta M, Gueheneuc YG, Antoniol G (2009) Code siblings: Technical and legal
implications of copying code between applications. In: 2009 6th IEEE International Working
Conference on Mining Software Repositories, pp 81–90, DOI 10.1109/MSR.2009.5069483
Happel H, Maalej W (2009) From work to word: How do software developers describe
their work? In: 2009 6th IEEE International Working Conference on Mining Software
Repositories. MSR 2009, IEEE Computer Society, Los Alamitos, CA, USA, pp 121–130,
DOI 10.1109/MSR.2009.5069490, URL https://doi.ieeecomputersociety.org/10.1109/
MSR.2009.5069490

Hattori L, Lanza M (2009) Mining the history of synchronous changes to reﬁne code ownership.
In: 2009 6th IEEE International Working Conference on Mining Software Repositories, pp
141–150, DOI 10.1109/MSR.2009.5069492

Kuhn A (2009) Automatic labeling of software components and their evolution using log-
likelihood ratio of word frequencies in source code. In: 2009 6th IEEE International Working
Conference on Mining Software Repositories, pp 175–178, DOI 10.1109/MSR.2009.5069499
Mockus A (2009) Amassing and indexing a large sample of version control systems: Towards the
census of public source code history. In: 2009 6th IEEE International Working Conference
on Mining Software Repositories, pp 11–20, DOI 10.1109/MSR.2009.5069476

2010 Selected Papers

D’Ambros M, Lanza M, Robbes R (2010) An extensive comparison of bug prediction ap-

proaches. In: MSR ’10, pp 31–41, DOI 10.1109/MSR.2010.5463279

Ibrahim WM, Bettenburg N, Shihab E, Adams B, Hassan AE (2010) Should I contribute to

this discussion? In: MSR ’10, pp 181–190, DOI 10.1109/MSR.2010.5463345

J´unior MC, Mendon¸ca M, Farias M, Henrique P (2010) OSS developers context-speciﬁc pre-
ferred representational systems: A initial neurolinguistic text analysis of the Apache mailing
list. In: MSR ’10, pp 126–129, DOI 10.1109/MSR.2010.5463339

Maalej W, Happel H (2010) Can development work describe itself? In: MSR ’10, pp 191–200,

DOI 10.1109/MSR.2010.5463344

Nussbaum L, Zacchiroli S (2010) The ultimate Debian database: Consolidating bazaar meta-
data for quality assurance and data mining. In: MSR ’10, pp 52–61, DOI 10.1109/MSR.
2010.5463277

Rahman F, Bird C, Devanbu P (2010) Clones: What is that smell? In: MSR ’10, pp 72–81,

DOI 10.1109/MSR.2010.5463343

2011 Selected Papers

Bhattacharya P, Neamtiu I (2011) Bug-ﬁx time prediction models: Can we do better? In:
Proceedings of the 8th Working Conference on Mining Software Repositories, Association
for Computing Machinery, New York, NY, USA, MSR ’11, p 207–210, DOI 10.1145/1985441.
1985472, URL https://doi.org/10.1145/1985441.1985472

Bradley AW, Murphy GC (2011) Supporting software history exploration. In: Proceedings of
the 8th Working Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR ’11, p 193–202, DOI 10.1145/1985441.1985469, URL
https://doi.org/10.1145/1985441.1985469

Canfora G, Cerulo L, Cimitile M, Di Penta M (2011) Social

interactions around cross-
system bug ﬁxings: The case of FreeBSD and OpenBSD. In: Proceedings of the 8th
Working Conference on Mining Software Repositories, Association for Computing Ma-
chinery, New York, NY, USA, MSR ’11, p 143–152, DOI 10.1145/1985441.1985463, URL
https://doi.org/10.1145/1985441.1985463

Davies J, German DM, Godfrey MW, Hindle A (2011) Software bertillonage: Finding the
provenance of an entity. In: Proceedings of the 8th Working Conference on Mining Software
Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’11, p
183–192, DOI 10.1145/1985441.1985468, URL https://doi.org/10.1145/1985441.1985468

Pitfalls and Guidelines for Using Time-Based Git Data

39

Eyolfson J, Tan L, Lam P (2011) Do time of day and developer experience aﬀect commit
bugginess? In: Proceedings of the 8th Working Conference on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR ’11, p 153–162, DOI
10.1145/1985441.1985464, URL https://doi.org/10.1145/1985441.1985464

Giger E, Pinzger M, Gall HC (2011) Comparing ﬁne-grained source code changes and code
churn for bug prediction. In: Proceedings of the 8th Working Conference on Mining Software
Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’11, p 83–92,
DOI 10.1145/1985441.1985456, URL https://doi.org/10.1145/1985441.1985456

Sadowski C, Lewis C, Lin Z, Zhu X, Whitehead EJ (2011) An empirical analysis of the Fix-
Cache algorithm. In: Proceedings of the 8th Working Conference on Mining Software Repos-
itories, Association for Computing Machinery, New York, NY, USA, MSR ’11, p 219–222,
DOI 10.1145/1985441.1985475, URL https://doi.org/10.1145/1985441.1985475

Thomas SW, Adams B, Hassan AE, Blostein D (2011) Modeling the evolution of topics in
source code histories. In: Proceedings of the 8th Working Conference on Mining Software
Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’11, p
173–182, DOI 10.1145/1985441.1985467, URL https://doi.org/10.1145/1985441.1985467
Zeltyn S, Tarr P, Cantor M, Delmonico R, Kannegala S, Keren M, Kumar AP, Wasserkrug
S (2011) Improving eﬃciency in software maintenance. In: Proceedings of the 8th Working
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR ’11, p 215–218, DOI 10.1145/1985441.1985474, URL https://doi.
org/10.1145/1985441.1985474

2012 Selected Papers

Artho C, Suzaki K, Di Cosmo R, Treinen R, Zacchiroli S (2012) Why do software packages
conﬂict? In: 2012 9th IEEE Working Conference on Mining Software Repositories (MSR),
pp 141–150, DOI 10.1109/MSR.2012.6224274

Baysal O, Holmes R, Godfrey MW (2012) Mining usage data and development artifacts. In:
2012 9th IEEE Working Conference on Mining Software Repositories (MSR), pp 98–107,
DOI 10.1109/MSR.2012.6224305

Bird C, Nagappan N (2012) Who? Where? What? Examining distributed development in two
large open source projects. In: 2012 9th IEEE Working Conference on Mining Software
Repositories (MSR), pp 237–246, DOI 10.1109/MSR.2012.6224286

Gousios G, Spinellis D (2012) Ghtorrent: Github’s data from a ﬁrehose. In: 2012 9th IEEE
Working Conference on Mining Software Repositories (MSR), pp 12–21, DOI 10.1109/MSR.
2012.6224294

Hindle A (2012) Green mining: A methodology of relating software change to power consump-
tion. In: 2012 9th IEEE Working Conference on Mining Software Repositories (MSR), pp
78–87, DOI 10.1109/MSR.2012.6224303

Khomh F, Dhaliwal T, Zou Y, Adams B (2012) Do faster releases improve software quality? an
empirical case study of Mozilla Firefox. In: 2012 9th IEEE Working Conference on Mining
Software Repositories (MSR), pp 179–188, DOI 10.1109/MSR.2012.6224279

Rodr´ıguez-Bustos C, Aponte J (2012) How Distributed Version Control Systems impact open
source software projects. In: 2012 9th IEEE Working Conference on Mining Software Repos-
itories (MSR), pp 36–39, DOI 10.1109/MSR.2012.6224297

Souza R, Chavez C (2012) Characterizing veriﬁcation of bug ﬁxes in two open source IDEs.
In: 2012 9th IEEE Working Conference on Mining Software Repositories (MSR), pp 70–73,
DOI 10.1109/MSR.2012.6224301

Steﬀ M, Russo B (2012) Co-evolution of logical couplings and commits for defect estimation. In:
2012 9th IEEE Working Conference on Mining Software Repositories (MSR), pp 213–216,
DOI 10.1109/MSR.2012.6224283

2013 Selected Papers

Alali A, Bartman B, Newman CD, Maletic JI (2013) A preliminary investigation of using age
and distance measures in the detection of evolutionary couplings. In: 2013 10th Working

40

Flint, Chauhan, and Dyer

Conference on Mining Software Repositories (MSR), pp 169–172, DOI 10.1109/MSR.2013.
6624024

Fu Q, Lou JG, Lin Q, Ding R, Zhang D, Xie T (2013) Contextual analysis of program logs
for understanding system behaviors. In: 2013 10th Working Conference on Mining Software
Repositories (MSR), pp 397–400, DOI 10.1109/MSR.2013.6624054

Ghezzi G, Gall HC (2013) Replicating mining studies with SOFAS. In: 2013 10th Working
Conference on Mining Software Repositories (MSR), pp 363–372, DOI 10.1109/MSR.2013.
6624050

Goeminne M, Claes M, Mens T (2013) A historical dataset for the Gnome ecosystem. In:
2013 10th Working Conference on Mining Software Repositories (MSR), pp 225–228, DOI
10.1109/MSR.2013.6624032

Gousios G (2013) The GHTorent dataset and tool suite. In: Proceedings of the 10th Working

Conference on Mining Software Repositories, IEEE Press, MSR ’13, p 233–236

Guzzi A, Bacchelli A, Lanza M, Pinzger M, van Deursen A (2013) Communication in open
source software development mailing lists. In: 2013 10th Working Conference on Mining
Software Repositories (MSR), pp 277–286, DOI 10.1109/MSR.2013.6624039

Hamasaki K, Kula RG, Yoshida N, Cruz AEC, Fujiwara K, Iida H (2013) Who does what
during a code review? datasets of OSS peer review repositories. In: Proceedings of the 10th
Working Conference on Mining Software Repositories, IEEE Press, MSR ’13, p 49–52

Jiang Y, Adams B, German DM (2013) Will my patch make it? and how fast? case study
on the Linux kernel. In: 2013 10th Working Conference on Mining Software Repositories
(MSR), pp 101–110, DOI 10.1109/MSR.2013.6624016

Lamkanﬁ A, P´erez J, Demeyer S (2013) The Eclipse and Mozilla defect tracking dataset: A
genuine dataset for mining bug information. In: 2013 10th Working Conference on Mining
Software Repositories (MSR), pp 203–206, DOI 10.1109/MSR.2013.6624028

MacLean AC, Knutson CD (2013) Apache commits: Social network dataset. In: 2013 10th
Working Conference on Mining Software Repositories (MSR), pp 135–138, DOI 10.1109/
MSR.2013.6624020

Mukherjee D, Garg M (2013) Which work-item updates need your response? In: 2013 10th
Working Conference on Mining Software Repositories (MSR), pp 12–21, DOI 10.1109/MSR.
2013.6623998

Nadi S, Dietrich C, Tartler R, Holt RC, Lohmann D (2013) Linux variability anomalies: What
causes them and how do they get ﬁxed? In: 2013 10th Working Conference on Mining
Software Repositories (MSR), pp 111–120, DOI 10.1109/MSR.2013.6624017

Naguib H, Narayan N, Br¨ugge B, Helal D (2013) Bug report assignee recommendation us-
ing activity proﬁles. In: Proceedings of the 10th Working Conference on Mining Software
Repositories, IEEE Press, MSR ’13, p 22–30

Raemaekers S, van Deursen A, Visser J (2013) The Maven repository dataset of metrics,
changes, and dependencies. In: 2013 10th Working Conference on Mining Software Reposi-
tories (MSR), pp 221–224, DOI 10.1109/MSR.2013.6624031

Robbes R, R¨othlisberger D (2013) Using developer interaction data to compare expertise
metrics. In: 2013 10th Working Conference on Mining Software Repositories (MSR), pp
297–300, DOI 10.1109/MSR.2013.6624041

Squire M (2013a) Apache-aﬃliated Twitter screen names: A dataset. In: 2013 10th Working
Conference on Mining Software Repositories (MSR), pp 305–308, DOI 10.1109/MSR.2013.
6624043

Squire M (2013b) Project roles in the Apache Software Foundation: A dataset. In: 2013 10th
Working Conference on Mining Software Repositories (MSR), pp 301–304, DOI 10.1109/
MSR.2013.6624042

Wagstrom P, Jergensen C, Sarma A (2013) A network of Rails a graph dataset of Ruby on Rails
and associated projects. In: 2013 10th Working Conference on Mining Software Repositories
(MSR), pp 229–232, DOI 10.1109/MSR.2013.6624033

Wang S, Khomh F, Zou Y (2013) Improving bug localization using correlations in crash reports.
In: 2013 10th Working Conference on Mining Software Repositories (MSR), pp 247–256,
DOI 10.1109/MSR.2013.6624036

Pitfalls and Guidelines for Using Time-Based Git Data

41

2014 Selected Papers

Baldassari B, Preux P (2014) Understanding software evolution: The Maisqual ant data set. In:
Proceedings of the 11th Working Conference on Mining Software Repositories, Association
for Computing Machinery, New York, NY, USA, MSR 2014, p 424–427, DOI 10.1145/
2597073.2597136, URL https://doi.org/10.1145/2597073.2597136

Bloemen R, Amrit C, Kuhlmann S, Ord´o˜nez–Matamoros G (2014) Gentoo package dependen-
cies over time. In: Proceedings of the 11th Working Conference on Mining Software Repos-
itories, Association for Computing Machinery, New York, NY, USA, MSR 2014, p 404–407,
DOI 10.1145/2597073.2597131, URL https://doi.org/10.1145/2597073.2597131

Chen TH, Nagappan M, Shihab E, Hassan AE (2014) An empirical study of dormant bugs. In:
Proceedings of the 11th Working Conference on Mining Software Repositories, Association
for Computing Machinery, New York, NY, USA, MSR 2014, p 82–91, DOI 10.1145/2597073.
2597108, URL https://doi.org/10.1145/2597073.2597108

Erfani Joorabchi M, Mirzaaghaei M, Mesbah A (2014) Works for me! characterizing non-
reproducible bug reports. In: Proceedings of the 11th Working Conference on Mining Soft-
ware Repositories, Association for Computing Machinery, New York, NY, USA, MSR 2014,
p 62–71, DOI 10.1145/2597073.2597098, URL https://doi.org/10.1145/2597073.2597098
Farah G, Tejada JS, Correal D (2014) OpenHub: A scalable architecture for the analysis of
software quality attributes. In: Proceedings of the 11th Working Conference on Mining
Software Repositories, Association for Computing Machinery, New York, NY, USA, MSR
2014, p 420–423, DOI 10.1145/2597073.2597135, URL https://doi.org/10.1145/2597073.
2597135

Fujiwara K, Hata H, Makihara E, Fujihara Y, Nakayama N, Iida H, Matsumoto K (2014)
Kataribe: A hosting service of historage repositories. In: Proceedings of the 11th Working
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR 2014, p 380–383, DOI 10.1145/2597073.2597125, URL https://doi.
org/10.1145/2597073.2597125

Gousios G, Zaidman A (2014) A dataset for pull-based development research. In: Proceedings of
the 11th Working Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR 2014, p 368–371, DOI 10.1145/2597073.2597122,
URL https://doi.org/10.1145/2597073.2597122

Gousios G, Vasilescu B, Serebrenik A, Zaidman A (2014) Lean GHTorrent: GitHub data on
demand. In: Proceedings of the 11th Working Conference on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR 2014, p 384–387, DOI
10.1145/2597073.2597126, URL https://doi.org/10.1145/2597073.2597126

Gupta M, Sureka A, Padmanabhuni S (2014) Process mining multiple repositories for soft-
ware defect resolution from control and organizational perspective. In: Proceedings of the
11th Working Conference on Mining Software Repositories, Association for Computing Ma-
chinery, New York, NY, USA, MSR 2014, p 122–131, DOI 10.1145/2597073.2597081, URL
https://doi.org/10.1145/2597073.2597081

Hanam Q, Tan L, Holmes R, Lam P (2014) Finding patterns in static analysis alerts: Improving
actionable alert ranking. In: Proceedings of the 11th Working Conference on Mining Software
Repositories, Association for Computing Machinery, New York, NY, USA, MSR 2014, p
152–161, DOI 10.1145/2597073.2597100, URL https://doi.org/10.1145/2597073.2597100
Kalliamvakou E, Gousios G, Blincoe K, Singer L, German DM, Damian D (2014) The promises
and perils of mining GitHub. In: Proceedings of the 11th Working Conference on Mining
Software Repositories, Association for Computing Machinery, New York, NY, USA, MSR
2014, p 92–101, DOI 10.1145/2597073.2597074, URL https://doi.org/10.1145/2597073.
2597074

Khodabandelou G, Hug C, Deneck`ere R, Salinesi C (2014) Unsupervised discovery of inten-
tional process models from event logs. In: Proceedings of the 11th Working Conference
on Mining Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR 2014, p 282–291, DOI 10.1145/2597073.2597101, URL https://doi.org/10.
1145/2597073.2597101

Lazar A, Ritchey S, Sharif B (2014a) Generating duplicate bug datasets. In: Proceedings of
the 11th Working Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR 2014, p 392–395, DOI 10.1145/2597073.2597128,
URL https://doi.org/10.1145/2597073.2597128

42

Flint, Chauhan, and Dyer

Lazar A, Ritchey S, Sharif B (2014b) Improving the accuracy of duplicate bug report detection
using textual similarity measures. In: Proceedings of the 11th Working Conference on Mining
Software Repositories, Association for Computing Machinery, New York, NY, USA, MSR
2014, p 308–311, DOI 10.1145/2597073.2597088, URL https://doi.org/10.1145/2597073.
2597088

Linares-V´asquez M, Bavota G, Bernal-C´ardenas C, Oliveto R, Di Penta M, Poshyvanyk D
(2014) Mining energy-greedy API usage patterns in Android apps: An empirical study. In:
Proceedings of the 11th Working Conference on Mining Software Repositories, Association
for Computing Machinery, New York, NY, USA, MSR 2014, p 2–11, DOI 10.1145/2597073.
2597085, URL https://doi.org/10.1145/2597073.2597085

Mitropoulos D, Karakoidas V, Louridas P, Gousios G, Spinellis D (2014) The bug catalog of
the Maven ecosystem. In: Proceedings of the 11th Working Conference on Mining Software
Repositories, Association for Computing Machinery, New York, NY, USA, MSR 2014, p
372–375, DOI 10.1145/2597073.2597123, URL https://doi.org/10.1145/2597073.2597123
Passos L, Czarnecki K (2014) A dataset of feature additions and feature removals from the
Linux kernel. In: Proceedings of the 11th Working Conference on Mining Software Reposi-
tories, Association for Computing Machinery, New York, NY, USA, MSR 2014, p 376–379,
DOI 10.1145/2597073.2597124, URL https://doi.org/10.1145/2597073.2597124

Ponzanelli L, Bavota G, Di Penta M, Oliveto R, Lanza M (2014) Mining StackOverﬂow to turn
the IDE into a self-conﬁdent programming prompter. In: Proceedings of the 11th Working
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR 2014, p 102–111, DOI 10.1145/2597073.2597077, URL https://doi.
org/10.1145/2597073.2597077

Robles G, Arjona Reina L, Serebrenik A, Vasilescu B, Gonz´alez-Barahona JM (2014a) FLOSS
2013: A survey dataset about free software contributors: Challenges for curating, sharing,
and combining. In: Proceedings of the 11th Working Conference on Mining Software Repos-
itories, Association for Computing Machinery, New York, NY, USA, MSR 2014, p 396–399,
DOI 10.1145/2597073.2597129, URL https://doi.org/10.1145/2597073.2597129

Robles G, Gonz´alez-Barahona JM, Cervig´on C, Capiluppi A, Izquierdo-Cort´azar D (2014b)
Estimating development eﬀort in free/open source software projects by mining software
repositories: A case study of OpenStack. In: Proceedings of the 11th Working Conference
on Mining Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR 2014, p 222–231, DOI 10.1145/2597073.2597107, URL https://doi.org/10.
1145/2597073.2597107

Steidl D, Hummel B, Juergens E (2014) Incremental origin analysis of source code ﬁles. In:
Proceedings of the 11th Working Conference on Mining Software Repositories, Association
for Computing Machinery, New York, NY, USA, MSR 2014, p 42–51, DOI 10.1145/2597073.
2597111, URL https://doi.org/10.1145/2597073.2597111

Valdivia Garcia H, Shihab E (2014) Characterizing and predicting blocking bugs in open source
projects. In: Proceedings of the 11th Working Conference on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR 2014, p 72–81, DOI
10.1145/2597073.2597099, URL https://doi.org/10.1145/2597073.2597099

Williams JR, Di Ruscio D, Matragkas N, Di Rocco J, Kolovos DS (2014) Models of OSS
project meta-information: A dataset of three forges. In: Proceedings of the 11th Working
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR 2014, p 408–411, DOI 10.1145/2597073.2597132, URL https://doi.
org/10.1145/2597073.2597132

Zhang C, Hindle A (2014) A green miner’s dataset: Mining the impact of software change on
energy consumption. In: Proceedings of the 11th Working Conference on Mining Software
Repositories, Association for Computing Machinery, New York, NY, USA, MSR 2014, p
400–403, DOI 10.1145/2597073.2597130, URL https://doi.org/10.1145/2597073.2597130
Zhang F, Mockus A, Keivanloo I, Zou Y (2014) Towards building a universal defect prediction
model. In: Proceedings of the 11th Working Conference on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR 2014, p 182–191, DOI
10.1145/2597073.2597078, URL https://doi.org/10.1145/2597073.2597078

Pitfalls and Guidelines for Using Time-Based Git Data

43

2015 Selected Papers

Ahmed TM, Shang W, Hassan AE (2015) An empirical study of the copy and paste behavior

during development. In: MSR ’15, p 99–110

Altinger H, Siegl S, Dajsuren Y, Wotawa F (2015) A novel

industry grade dataset for
fault prediction based on model-driven developed automotive embedded software. In: 2015
IEEE/ACM 12th Working Conference on Mining Software Repositories, pp 494–497, DOI
10.1109/MSR.2015.72

Barik T, Lubick K, Smith J, Slankas J, Murphy-Hill E (2015) Fuse: A reproducible, extendable,
internet-scale corpus of spreadsheets. In: 2015 IEEE/ACM 12th Working Conference on
Mining Software Repositories, pp 486–489, DOI 10.1109/MSR.2015.70

Bird C, Carnahan T, Greiler M (2015) Lessons learned from building and deploying a code

review analytics platform. In: MSR ’15, p 191–201

Burlet G, Hindle A (2015) An empirical study of end-user programmers in the computer music

community. In: MSR ’15, p 292–302

Choetkiertikul M, Dam HK, Tran T, Ghose A (2015) Characterization and prediction of issue-

related risks in software projects. In: MSR ’15, p 280–291

Claes M, Mens T, Di Cosmo R, Vouillon J (2015) A historical analysis of Debian package

incompatibilities. In: MSR ’15, p 212–223

German DM, Adams B, Hassan AE (2015) A dataset of the activity of the git super-repository
of Linux in 2012. In: Proceedings of the 12th Working Conference on Mining Software
Repositories, IEEE Press, MSR ’15, p 470–473

Gonzalez-Barahona JM, Robles G, Izquierdo-Cortazar D (2015) The MetricsGrimoire database
collection. In: 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,
pp 478–481, DOI 10.1109/MSR.2015.68

Habayeb M, Miranskyy A, Murtaza SS, Buchanan L, Bener A (2015) The Firefox temporal
dataset. In: 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,
pp 498–501, DOI 10.1109/MSR.2015.73

Jiang Y, Adams B (2015) Co-evolution of infrastructure and source code - an empirical study.
In: 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories, pp 45–55,
DOI 10.1109/MSR.2015.12

Krutz DE, Mirakhorli M, Malachowsky SA, Ruiz A, Peterson J, Filipski A, Smith J (2015) A
dataset of open-source Android applications. In: 2015 IEEE/ACM 12th Working Conference
on Mining Software Repositories, pp 522–525, DOI 10.1109/MSR.2015.79

Lin Z, Whitehead J (2015) Why power laws? an explanation from ﬁne-grained code changes.
In: Proceedings of the 12th Working Conference on Mining Software Repositories, IEEE
Press, MSR ’15, p 68–75

Linares-V´asquez M, White M, Bernal-C´ardenas C, Moran K, Poshyvanyk D (2015) Mining
Android app usages for generating actionable GUI-based execution scenarios. In: MSR ’15,
p 111–122

Mauczka A, Brosch F, Schanes C, Grechenig T (2015) Dataset of developer-labeled commit
messages. In: Proceedings of the 12th Working Conference on Mining Software Repositories,
IEEE Press, MSR ’15, p 490–493

Moura I, Pinto G, Ebert F, Castor F (2015) Mining energy-aware commits. In: 2015
IEEE/ACM 12th Working Conference on Mining Software Repositories, pp 56–67, DOI
10.1109/MSR.2015.13

Ohira M, Kashiwa Y, Yamatani Y, Yoshiyuki H, Maeda Y, Limsettho N, Fujino K, Hata
H, Ihara A, Matsumoto K (2015) A dataset of high impact bugs: Manually-classiﬁed issue
reports. In: 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,
pp 518–521, DOI 10.1109/MSR.2015.78

Ray B, Nagappan M, Bird C, Nagappan N, Zimmermann T (2015) The uniqueness of changes:

Characteristics and applications. In: MSR ’15, p 34–44

Sawant AA, Bacchelli A (2015) A dataset for API usage. In: 2015 IEEE/ACM 12th Working

Conference on Mining Software Repositories, pp 506–509, DOI 10.1109/MSR.2015.75

Spinellis D (2015) A repository with 44 years of Unix evolution. In: 2015 IEEE/ACM 12th
Working Conference on Mining Software Repositories, pp 462–465, DOI 10.1109/MSR.2015.
64

Vasilescu B, Serebrenik A, Filkov V (2015) A data set for social diversity studies of GitHub
teams. In: Proceedings of the 12th Working Conference on Mining Software Repositories,
IEEE Press, MSR ’15, p 514–517

44

Flint, Chauhan, and Dyer

Wermelinger M, Yu Y (2015) An architectural evolution dataset. In: 2015 IEEE/ACM 12th
Working Conference on Mining Software Repositories, pp 502–505, DOI 10.1109/MSR.2015.
74

Yu Y, Wang H, Filkov V, Devanbu P, Vasilescu B (2015) Wait for it: Determinants of pull

request evaluation latency on GitHub. In: MSR ’15, p 367–371

Zacchiroli S (2015) The Debsources dataset: Two decades of Debian source code metadata. In:
2015 IEEE/ACM 12th Working Conference on Mining Software Repositories, pp 466–469,
DOI 10.1109/MSR.2015.65

Zanjani MB, Kagdi H, Bird C (2015) Using developer-interaction trails to triage change re-

quests. In: MSR ’15, p 88–98

2016 Selected Papers

Ahasanuzzaman M, Asaduzzaman M, Roy CK, Schneider KA (2016) Mining duplicate ques-
tions of Stack Overﬂow. In: 2016 IEEE/ACM 13th Working Conference on Mining Software
Repositories (MSR), pp 402–412

Beyer S, Pinzger M (2016) Grouping android tag synonyms on Stack Overﬂow. In: Pro-
ceedings of the 13th International Conference on Mining Software Repositories, Asso-
ciation for Computing Machinery, New York, NY, USA, MSR ’16, p 430–440, DOI
10.1145/2901739.2901750, URL https://doi.org/10.1145/2901739.2901750

Damevski K, Chen H, Shepherd D, Pollock L (2016) Interactive exploration of developer
interaction traces using a Hidden Markov Model. In: Proceedings of the 13th International
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR ’16, p 126–136, DOI 10.1145/2901739.2901741, URL https://doi.
org/10.1145/2901739.2901741

Dilshener T, Wermelinger M, Yu Y (2016) Locating bugs without looking back. In: MSR ’16,

p 286–290, DOI 10.1145/2901739.2901775

G´omez M, Rouvoy R, Adams B, Seinturier L (2016) Mining test repositories for automatic
detection of ui performance regressions in android apps. In: Proceedings of the 13th In-
ternational Conference on Mining Software Repositories, Association for Computing Ma-
chinery, New York, NY, USA, MSR ’16, p 13–24, DOI 10.1145/2901739.2901747, URL
https://doi.org/10.1145/2901739.2901747

Kikas R, Dumas M, Pfahl D (2016) Using dynamic and contextual features to predict issue
lifetime in GitHub projects. In: 2016 IEEE/ACM 13th Working Conference on Mining
Software Repositories (MSR), pp 291–302

Moslehi P, Adams B, Rilling J (2016) On mining crowd-based speech documentation. In: MSR

’16, p 259–268, DOI 10.1145/2901739.2901771

Ortu M, Murgia A, Destefanis G, Tourani P, Tonelli R, Marchesi M, Adams B (2016) The
emotional side of software developers in JIRA. In: Proceedings of the 13th International
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR ’16, p 480–483, DOI 10.1145/2901739.2903505, URL https://doi.
org/10.1145/2901739.2903505

Rahman MT, Querel LP, Rigby PC, Adams B (2016) Feature toggles: Practitioner practices
and a case study. In: 2016 IEEE/ACM 13th Working Conference on Mining Software Repos-
itories (MSR), pp 201–211

Rozenberg D, Beschastnikh I, Kosmale F, Poser V, Becker H, Palyart M, Murphy GC (2016)
Comparing repositories visually with repograms. In: Proceedings of the 13th International
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR ’16, p 109–120, DOI 10.1145/2901739.2901768, URL https://doi.
org/10.1145/2901739.2901768

Squire M (2016) Data sets: The circle of life in Ruby hosting, 2003-2015. In: Proceedings of the
13th International Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR ’16, p 452–459, DOI 10.1145/2901739.2903509, URL
https://doi.org/10.1145/2901739.2903509

Yang D, Hussain A, Lopes CV (2016a) From query to usable code: An analysis of stack overﬂow
code snippets. In: Proceedings of the 13th International Conference on Mining Software
Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’16, p
391–402, DOI 10.1145/2901739.2901767, URL https://doi.org/10.1145/2901739.2901767

Pitfalls and Guidelines for Using Time-Based Git Data

45

Yang X, Kula RG, Yoshida N, Iida H (2016b) Mining the modern code review repositories: A
dataset of people, process and product. In: Proceedings of the 13th International Conference
on Mining Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR ’16, p 460–463, DOI 10.1145/2901739.2903504, URL https://doi.org/10.1145/
2901739.2903504

Zagalsky A, Teshima CG, German DM, Storey MA, Poo-Caama˜no G (2016) How the R com-
munity creates and curates knowledge: A comparative study of stack overﬂow and mail-
ing lists. In: 2016 IEEE/ACM 13th Working Conference on Mining Software Repositories
(MSR), pp 441–451

Zhu J, Zhou M, Mei H (2016) Multi-extract and multi-level dataset of Mozilla issue tracking
history. In: 2016 IEEE/ACM 13th Working Conference on Mining Software Repositories
(MSR), pp 472–475

2017 Selected Papers

Aivaloglou E, Hermans F, Moreno-Leon J, Robles G (2017) A dataset of Scratch programs:
Scraped, shaped and scored. In: 2017 IEEE/ACM 14th International Conference on Mining
Software Repositories (MSR), pp 511–514, DOI 10.1109/MSR.2017.45

Bao L, Xing Z, Xia X, Lo D, Li S (2017) Who will leave the company? a large-scale industry
study of developer turnover by mining monthly work report. In: Proceedings of the 14th
International Conference on Mining Software Repositories, IEEE Press, MSR ’17, p 170–181,
DOI 10.1109/MSR.2017.58, URL https://doi.org/10.1109/MSR.2017.58

Beller M, Gousios G, Zaidman A (2017) Oops, my tests broke the build: An explorative analysis
of Travis CI with GitHub. In: 2017 IEEE/ACM 14th International Conference on Mining
Software Repositories (MSR), pp 356–367, DOI 10.1109/MSR.2017.62

Cito J, Schermann G, Wittern JE, Leitner P, Zumberi S, Gall HC (2017) An empirical analysis
of the Docker container ecosystem on GitHub. In: 2017 IEEE/ACM 14th International
Conference on Mining Software Repositories (MSR), pp 323–333, DOI 10.1109/MSR.2017.67
Claes M, M¨antyl¨a M, Kuutila M, Adams B (2017) Abnormal working hours: Eﬀect of rapid
releases and implications to work content. 2017 IEEE/ACM 14th International Conference
on Mining Software Repositories (MSR) pp 243–247

Dehghan A, Neal A, Blincoe K, Linaker J, Damian D (2017) Predicting likelihood of require-
ment implementation within the planned iteration: An empirical study at IBM. In: 2017
IEEE/ACM 14th International Conference on Mining Software Repositories (MSR), pp
124–134, DOI 10.1109/MSR.2017.53

Gharehyazie M, Ray B, Filkov V (2017) Some from here, some from there: Cross-project code
reuse in github. In: 2017 IEEE/ACM 14th International Conference on Mining Software
Repositories (MSR), pp 291–301, DOI 10.1109/MSR.2017.15

Madeyski L, Kawalerowicz M (2017) Continuous defect prediction: The idea and a related
dataset. In: 2017 IEEE/ACM 14th International Conference on Mining Software Reposito-
ries (MSR), pp 515–518, DOI 10.1109/MSR.2017.46

Molderez T, Stevens R, De Roover C (2017) Mining change histories for unknown systematic
edits. In: 2017 IEEE/ACM 14th International Conference on Mining Software Repositories
(MSR), pp 248–256, DOI 10.1109/MSR.2017.12

Rausch T, Hummer W, Leitner P, Schulte S (2017) An empirical analysis of build failures in the
continuous integration workﬂows of java-based open-source software. In: 2017 IEEE/ACM
14th International Conference on Mining Software Repositories (MSR), pp 345–355, DOI
10.1109/MSR.2017.54

Robles G, Ho-Quang T, Hebig R, Chaudron MRV, Fernandez MA (2017) An extensive dataset
of UML models in GitHub. In: Proceedings of the 14th International Conference on Mining
Software Repositories, IEEE Press, MSR ’17, p 519–522, DOI 10.1109/MSR.2017.48, URL
https://doi.org/10.1109/MSR.2017.48

Sadat M, Bener AB, Miranskyy A (2017) Rediscovery datasets: Connecting duplicate reports.
In: 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),
IEEE, pp 527–530

Silva D, Valente MT (2017) Refdiﬀ: Detecting refactorings in version histories. In: 2017
IEEE/ACM 14th International Conference on Mining Software Repositories (MSR), pp
269–279, DOI 10.1109/MSR.2017.14

46

Flint, Chauhan, and Dyer

Tiwari NM, Upadhyaya G, Nguyen HA, Rajan H (2017) Candoia: A platform for building and
sharing mining software repositories tools as apps. In: 2017 IEEE/ACM 14th International
Conference on Mining Software Repositories (MSR), pp 53–63, DOI 10.1109/MSR.2017.56
Wan Z, Lo D, Xia X, Cai L (2017) Bug characteristics in blockchain systems: A large-scale
empirical study. In: 2017 IEEE/ACM 14th International Conference on Mining Software
Repositories (MSR), pp 413–424, DOI 10.1109/MSR.2017.59

Watanabe T, Akiyama M, Kanei F, Shioji E, Takata Y, Sun B, Ishi Y, Shibahara T, Yagi T,
Mori T (2017) Understanding the origins of mobile app vulnerabilities: A large-scale mea-
surement study of free and paid apps. In: 2017 IEEE/ACM 14th International Conference
on Mining Software Repositories (MSR), pp 14–24, DOI 10.1109/MSR.2017.23

Xu L, Dou W, Gao C, Wang J, Wei J, Zhong H, Huang T (2017) Spreadcluster: Re-
covering versioned spreadsheets through similarity-based clustering. In: 2017 IEEE/ACM
14th International Conference on Mining Software Repositories (MSR), pp 158–169, DOI
10.1109/MSR.2017.28

Yamashita A, Abtahizadeh SA, Khomh F, Gu´eh´eneuc YG (2017) Software evolution and
quality data from controlled, multiple, industrial case studies. In: 2017 IEEE/ACM 14th
International Conference on Mining Software Repositories (MSR), pp 507–510, DOI 10.
1109/MSR.2017.44

Zhu C, Li Y, Rubin J, Chechik M (2017) A dataset for dynamic discovery of semantic changes
in version controlled software histories. In: 2017 IEEE/ACM 14th International Conference
on Mining Software Repositories (MSR), pp 523–526, DOI 10.1109/MSR.2017.49

2018 Selected Papers

Accioly P, Borba P, Silva L, Cavalcanti G (2018) Analyzing conﬂict predictors in open-source

Java projects. In: MSR ’18, p 576–586, DOI 10.1145/3196398.3196437

Arima R, Higo Y, Kusumoto S (2018) A study on inappropriately partitioned commits: How
much and what kinds of IP commits in Java projects? In: MSR ’18, p 336–340, DOI
10.1145/3196398.3196406

Baltes S, Dumani L, Treude C, Diehl S (2018) SOTorrent: Reconstructing and analyzing the
evolution of Stack Overﬂow posts. In: MSR ’18, p 319–330, DOI 10.1145/3196398.3196430
Benkoczi R, Gaur D, Hossain S, Khan MA (2018) A design structure matrix approach for
measuring co-change-modularity of software products. In: Proceedings of the 15th Inter-
national Conference on Mining Software Repositories, Association for Computing Ma-
chinery, New York, NY, USA, MSR ’18, p 331–335, DOI 10.1145/3196398.3196409, URL
https://doi.org/10.1145/3196398.3196409

Bernardo JaH, da Costa DA, Kulesza U (2018) Studying the impact of adopting continuous
integration on the delivery time of pull requests. In: MSR ’18, p 131–141, DOI 10.1145/
3196398.3196421

Calciati P, Kuznetsov K, Bai X, Gorla A (2018) What did really change with the new re-
lease of the app? In: 2018 IEEE/ACM 15th International Conference on Mining Software
Repositories (MSR), pp 142–152

Chatzidimitriou K, Papamichail M, Diamantopoulos T, Tsapanos M, Symeonidis A (2018)
npm-miner: An infrastructure for measuring the quality of the npm registry. In: 2018
IEEE/ACM 15th International Conference on Mining Software Repositories (MSR), pp
42–45

Claes M, M¨antyl¨a M, Kuutila M, Farooq U (2018) Towards automatically identifying paid

open source developers. In: MSR ’18, p 437–441, DOI 10.1145/3196398.3196447

Geiger FX, Malavolta I, Pascarella L, Palomba F, Di Nucci D, Bacchelli A (2018) A graph-
based dataset of commit history of real-world Android apps. In: 2018 IEEE/ACM 15th
International Conference on Mining Software Repositories (MSR), pp 30–33

Markovtsev V, Long W (2018) Public Git Archive: A big code dataset for all. In: 2018
IEEE/ACM 15th International Conference on Mining Software Repositories (MSR), pp
34–37

Martins P, Achar R, V Lopes C (2018) 50k-c: A dataset of compilable, and compiled, Java
projects. In: 2018 IEEE/ACM 15th International Conference on Mining Software Reposito-
ries (MSR), pp 1–5

Nayebi M, Kuznetsov K, Chen P, Zeller A, Ruhe G (2018) Anatomy of functionality deletion:
An exploratory study on mobile apps. In: Proceedings of the 15th International Conference

Pitfalls and Guidelines for Using Time-Based Git Data

47

on Mining Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR ’18, p 243–253, DOI 10.1145/3196398.3196410, URL https://doi.org/10.1145/
3196398.3196410

Nayrolles M, Hamou-Lhadj A (2018) CLEVER: Combining code metrics with clone detection
for just-in-time fault prevention and resolution in large industrial projects. In: MSR ’18, p
153–164, DOI 10.1145/3196398.3196438

Paixao M, Krinke J, Han D, Harman M (2018) CROP: Linking code reviews to source code
changes. In: 2018 IEEE/ACM 15th International Conference on Mining Software Reposito-
ries (MSR), pp 46–49

Rath M, Lo D, M¨ader P (2018) Analyzing requirements and traceability information to improve

bug localization. In: MSR ’18, p 442–453, DOI 10.1145/3196398.3196415

Sanchez BA, Barmpis K, Neubauer P, Paige RF, Kolovos DS (2018) Restmule: Enabling re-
silient clients for remote APIs. In: 2018 IEEE/ACM 15th International Conference on Min-
ing Software Repositories (MSR), pp 537–541

Schermann G, Zumberi S, Cito J (2018) Structured information on state and evolution of Dock-
erﬁles on GitHub. In: 2018 IEEE/ACM 15th International Conference on Mining Software
Repositories (MSR), pp 26–29

Spinellis D (2018) Documented Unix facilities over 48 years. 2018 IEEE/ACM 15th Interna-

tional Conference on Mining Software Repositories (MSR) pp 58–61

Wang H, Li H, Li L, Guo Y, Xu G (2018) Why are Android apps removed from Google Play?
a large-scale empirical study. In: 2018 IEEE/ACM 15th International Conference on Mining
Software Repositories (MSR), pp 231–242

Widder D, Vasilescu B, Hilton M, K¨astner C (2018) I’m leaving you, Travis: A continuous
integration breakup story. In: 2018 IEEE/ACM 15th International Conference on Mining
Software Repositories (MSR), pp 165–169

Xu Y, Zhou M (2018) A multi-level dataset of Linux kernel patchwork. In: Proceedings of the
15th International Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR ’18, p 54–57, DOI 10.1145/3196398.3196475, URL
https://doi.org/10.1145/3196398.3196475

Yamashita A, Petrillo F, Khomh F, Gu´eh´eneuc YG (2018) Developer interaction traces backed
by IDE screen recordings from think aloud sessions. In: Proceedings of the 15th International
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR ’18, p 50–53, DOI 10.1145/3196398.3196457, URL https://doi.org/
10.1145/3196398.3196457

Yu Y, Li Z, Yin G, Wang T, Wang H (2018) A dataset of duplicate pull-requests in Github.
In: Proceedings of the 15th International Conference on Mining Software Repositories,
Association for Computing Machinery, New York, NY, USA, MSR ’18, p 22–25, DOI
10.1145/3196398.3196455, URL https://doi.org/10.1145/3196398.3196455

2019 Selected Papers

Ahmad M, Cinn´eide MO (2019) Impact of Stack Overﬂow code snippets on software cohesion:

A preliminary study. In: MSR ’19, p 250–254, DOI 10.1109/MSR.2019.00050

Chren S, Micko R, Buhnova B, Rossi B (2019) STRAIT: A tool for automated software relia-

bility growth analysis. In: MSR ’19, p 105–110, DOI 10.1109/MSR.2019.00025

Gote C, Scholtes I, Schweitzer F (2019) Git2net: Mining time-stamped co-editing networks
from large git repositories. In: Proceedings of the 16th International Conference on Mining
Software Repositories, IEEE Press, MSR ’19, p 433–444, DOI 10.1109/MSR.2019.00070,
URL https://doi.org/10.1109/MSR.2019.00070

Hayashi J, Higo Y, Matsumoto S, Kusumoto S (2019) Impacts of daylight saving time on
software development. In: Proceedings of the 16th International Conference on Mining Soft-
ware Repositories, IEEE Press, MSR ’19, p 502–506, DOI 10.1109/MSR.2019.00076, URL
https://doi.org/10.1109/MSR.2019.00076

Hoang T, Dam HK, Kamei Y, Lo D, Ubayashi N (2019) DeepJIT: An end-to-end deep learning
framework for just-in-time defect prediction. In: MSR ’19, p 34–45, DOI 10.1109/MSR.2019.
00016

Kiehn M, Pan X, Camci F (2019) Empirical study in using version histories for change risk

classiﬁcation. In: MSR ’19, p 58–62, DOI 10.1109/MSR.2019.00018

48

Flint, Chauhan, and Dyer

Ma Y, Bogart C, Amreen S, Zaretzki R, Mockus A (2019) World of Code: An infrastructure
for mining the universe of open source VCS data. In: MSR ’19, p 143–154, DOI 10.1109/
MSR.2019.00031

Mitropoulos D, Louridas P, Salis V, Spinellis D (2019) Time present and time past: Analyzing
the evolution of JavaScript code in the wild. In: MSR ’19, p 126–137, DOI 10.1109/MSR.
2019.00029

Mondal S, Rahman MM, Roy CK (2019) Can issues reported at Stack Overﬂow questions be
reproduced? an exploratory study. In: Proceedings of the 16th International Conference on
Mining Software Repositories, IEEE Press, MSR ’19, p 479–489, DOI 10.1109/MSR.2019.
00074, URL https://doi.org/10.1109/MSR.2019.00074

Pietri A, Spinellis D, Zacchiroli S (2019) The Software Heritage graph dataset: Public software

development under one roof. In: MSR ’19, p 138–142, DOI 10.1109/MSR.2019.00030

Pimentel JF, Murta L, Braganholo V, Freire J (2019) A large-scale study about quality and
reproducibility of Jupyter notebooks. In: 2019 IEEE/ACM 16th International Conference
on Mining Software Repositories (MSR), pp 507–517, DOI 10.1109/MSR.2019.00077

Schipper D, Aniche M, van Deursen A (2019) Tracing back log data to its log statement:
From research to practice. In: 2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR), pp 545–549, DOI 10.1109/MSR.2019.00081

Serra D, Grano G, Palomba F, Ferrucci F, Gall HC, Bacchelli A (2019) On the eﬀectiveness
of manual and automatic unit test generation: Ten years later. In: 2019 IEEE/ACM 16th
International Conference on Mining Software Repositories (MSR), pp 121–125, DOI 10.
1109/MSR.2019.00028

van Tonder R, Trockman A, Le Goues C (2019) A panel data set of cryptocurrency development
activity on GitHub. In: 2019 IEEE/ACM 16th International Conference on Mining Software
Repositories (MSR), pp 186–190, DOI 10.1109/MSR.2019.00037

Treude C, Wagner M (2019) Predicting good conﬁgurations for GitHub and Stack Overﬂow
topic models. In: Proceedings of the 16th International Conference on Mining Software
Repositories, IEEE Press, MSR ’19, p 84–95, DOI 10.1109/MSR.2019.00022, URL https:
//doi.org/10.1109/MSR.2019.00022

Yang AZH, da Costa DA, Zou Y (2019) Predicting co-changes between functionality speciﬁ-
cations and source code in behavior driven development. In: Proceedings of the 16th In-
ternational Conference on Mining Software Repositories, IEEE Press, MSR ’19, p 534–544,
DOI 10.1109/MSR.2019.00080, URL https://doi.org/10.1109/MSR.2019.00080

Zhai H, Casalnuovo C, Devanbu P (2019) Test coverage in Python programs. In: MSR ’19, p

116–120, DOI 10.1109/MSR.2019.00027

Zhu J, Wei J (2019) An empirical study of multiple names and email addresses in OSS version
control repositories. In: Proceedings of the 16th International Conference on Mining Software
Repositories, IEEE Press, MSR ’19, p 409–420, DOI 10.1109/MSR.2019.00068, URL https:
//doi.org/10.1109/MSR.2019.00068

2020 Selected Papers

Abdellatif A, Costa D, Badran K, Abdalkareem R, Shihab E (2020) Challenges in chatbot
development: A study of Stack Overﬂow posts. In: MSR ’20, p 174–185, DOI 10.1145/
3379597.3387472

Barmpis K, Neubauer P, Co J, Kolovos D, Matragkas N, Paige RF (2020) Polyglot and
distributed software repository mining with Crossﬂow. In: MSR ’20, p 374–384, DOI
10.1145/3379597.3387481

Bello-Jim´enez L, Escobar-Vel´asquez C, Mojica-Hanke A, Cort´es-Fern´andez S, Linares-V´asquez
M (2020) Hall-of-apps: The top Android apps metadata archive. In: Proceedings of the
17th International Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR ’20, p 568–572, DOI 10.1145/3379597.3387497, URL
https://doi.org/10.1145/3379597.3387497

Chatterjee P, Damevski K, Kraft NA, Pollock L (2020) Software-related Slack chats with
disentangled conversations. In: Proceedings of the 17th International Conference on Mining
Software Repositories, Association for Computing Machinery, New York, NY, USA, MSR
’20, p 588–592, DOI 10.1145/3379597.3387493, URL https://doi.org/10.1145/3379597.
3387493

Pitfalls and Guidelines for Using Time-Based Git Data

49

Chen Y, Santosa AE, Yi AM, Sharma A, Sharma A, Lo D (2020) A machine learning approach

for vulnerability curation. In: MSR ’20, p 32–42, DOI 10.1145/3379597.3387461

Claes M, M¨antyl¨a MV (2020) 20-mad: 20 years of issues and commits of Mozilla and Apache de-
velopment. In: Proceedings of the 17th International Conference on Mining Software Repos-
itories, Association for Computing Machinery, New York, NY, USA, MSR ’20, p 503–507,
DOI 10.1145/3379597.3387487, URL https://doi.org/10.1145/3379597.3387487

Cor`o F, Verdecchia R, Cruciani E, Miranda† B, Bertolino A (2020) Jtec: A large collection
of Java test classes for test code analysis and processing. In: Proceedings of the 17th In-
ternational Conference on Mining Software Repositories, Association for Computing Ma-
chinery, New York, NY, USA, MSR ’20, p 578–582, DOI 10.1145/3379597.3387484, URL
https://doi.org/10.1145/3379597.3387484

Dey T, Mousavi S, Ponce E, Fry T, Vasilescu B, Filippova A, Mockus A (2020) Detecting
and characterizing bots that commit code. In: MSR ’20, p 209–219, DOI 10.1145/3379597.
3387478

Diamantopoulos T, Papamichail MD, Karanikiotis T, Chatzidimitriou KC, Symeonidis AL
(2020) Employing contribution and quality metrics for quantifying the software development
process. In: Proceedings of the 17th International Conference on Mining Software Reposi-
tories, Association for Computing Machinery, New York, NY, USA, MSR ’20, p 558–562,
DOI 10.1145/3379597.3387490, URL https://doi.org/10.1145/3379597.3387490

Durieux T, Le Goues C, Hilton M, Abreu R (2020) Empirical study of restarted and ﬂaky

builds on Travis CI. In: MSR ’20, p 254–264, DOI 10.1145/3379597.3387460

El Zarif O, Da Costa DA, Hassan S, Zou Y (2020) On the relationship between user churn and

software issues. In: MSR ’20, p 339–349, DOI 10.1145/3379597.3387456

Fan J, Li Y, Wang S, Nguyen TN (2020) A C/C++ code vulnerability dataset with code
changes and CVE summaries. In: Proceedings of the 17th International Conference on Min-
ing Software Repositories, Association for Computing Machinery, New York, NY, USA, MSR
’20, p 508–512, DOI 10.1145/3379597.3387501, URL https://doi.org/10.1145/3379597.
3387501

Golubev Y, Eliseeva M, Povarov N, Bryksin T (2020) A study of potential code borrowing and
license violations in Java projects on GitHub. In: MSR ’20, p 54–64, DOI 10.1145/3379597.
3387455

Gonzalez D, Zimmermann T, Nagappan N (2020) The state of the ML-universe: 10 years of
artiﬁcial intelligence & machine learning software development on GitHub. In: MSR ’20, p
431–442, DOI 10.1145/3379597.3387473

Henkel J, Bird C, Lahiri SK, Reps T (2020) A dataset of Dockerﬁles. In: Proceedings of the
17th International Conference on Mining Software Repositories, Association for Computing
Machinery, New York, NY, USA, MSR ’20, p 528–532, DOI 10.1145/3379597.3387498, URL
https://doi.org/10.1145/3379597.3387498

Hung CS, Dyer R (2020) Boa views: Easy modularization and sharing of MSR analyses. In:

MSR ’20, p 147–157, DOI 10.1145/3379597.3387480

Karampatsis RM, Sutton C (2020) How often do single-statement bugs occur? the
ManySStuBs4J dataset. Association for Computing Machinery, New York, NY, USA, MSR
’20, p 573–577, DOI 10.1145/3379597.3387491, URL https://doi.org/10.1145/3379597.
3387491

Liu P, Li L, Zhao Y, Sun X, Grundy J (2020a) Androzooopen: Collecting large-scale open
source Android apps for the research community. In: Proceedings of the 17th International
Conference on Mining Software Repositories, Association for Computing Machinery, New
York, NY, USA, MSR ’20, p 548–552, DOI 10.1145/3379597.3387503, URL https://doi.
org/10.1145/3379597.3387503

Liu Y, Lin J, Cleland-Huang J (2020b) Traceability support for multi-lingual software projects.

In: MSR ’20, p 443–454, DOI 10.1145/3379597.3387440

Mockus A, Spinellis D, Kotti Z, Dusing GJ (2020) A complete set of related git repositories
identiﬁed via community detection approaches based on shared commits. In: Proceedings of
the 17th International Conference on Mining Software Repositories, Association for Comput-
ing Machinery, New York, NY, USA, MSR ’20, p 513–517, DOI 10.1145/3379597.3387499,
URL https://doi.org/10.1145/3379597.3387499

Mujahid S, Abdalkareem R, Shihab E, McIntosh S (2020) Using others’ tests to identify break-

ing updates. In: MSR ’20, p 466–476, DOI 10.1145/3379597.3387476

Muse BA, Rahman MM, Nagy C, Cleve A, Khomh F, Antoniol G (2020) On the prevalence,
impact, and evolution of SQL code smells in data-intensive systems. In: MSR ’20, p 327–338,

50

Flint, Chauhan, and Dyer

DOI 10.1145/3379597.3387467

Nakamaru T, Matsunaga T, Yamazaki T, Akiyama S, Chiba S (2020) An empirical study of

method chaining in Java. In: MSR ’20, p 93–102, DOI 10.1145/3379597.3387441

Parra E, Ellis A, Haiduc S (2020) Gittercom: A dataset of open source developer commu-
nications in Gitter. In: Proceedings of the 17th International Conference on Mining Soft-
ware Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’20, p
563–567, DOI 10.1145/3379597.3387494, URL https://doi.org/10.1145/3379597.3387494
Pietri A, Rousseau G, Zacchiroli S (2020) Forking without clicking: On how to identify software

repository forks. In: MSR ’20, p 277–287, DOI 10.1145/3379597.3387450

Politowski C, Petrillo F, Ullmann GC, de Andrade Werly J, Gu´eh´eneuc YG (2020) Dataset
of video game development problems. In: Proceedings of the 17th International Conference
on Mining Software Repositories, Association for Computing Machinery, New York, NY,
USA, MSR ’20, p 553–557, DOI 10.1145/3379597.3387486, URL https://doi.org/10.1145/
3379597.3387486

Rodrigues IM, Aloise D, Fernandes ER, Dagenais M (2020) A soft alignment model for bug

deduplication. In: MSR ’20, p 43–53, DOI 10.1145/3379597.3387470

Spinellis D, Kotti Z, Kravvaritis K, Theodorou G, Louridas P (2020) A dataset of enterprise-
driven open source software. In: Proceedings of the 17th International Conference on Mining
Software Repositories, Association for Computing Machinery, New York, NY, USA, MSR
’20, p 533–537, DOI 10.1145/3379597.3387495, URL https://doi.org/10.1145/3379597.
3387495

Svitkov S, Bryskin T (2020) Visualization of methods changeability based on VCS data. In:

MSRC ’20, pp 477–480

Walden J (2020) The impact of a major security event on an open source project: The case of

OpenSSL. In: MSR ’20, p 409–419, DOI 10.1145/3379597.3387465

Wang P, Brown C, Jennings JA, Stolee KT (2020) An empirical study on regular expression

bugs. In: MSR ’20, p 103–113, DOI 10.1145/3379597.3387464

Wu Y, Zhang Y, Wang T, Wang H (2020) An empirical study of build failures in the Docker

context. In: MSR ’20, p 76–80, DOI 10.1145/3379597.3387483

Xavier L, Ferreira F, Brito R, Valente MT (2020) Beyond the code: Mining self-admitted
technical debt in issue tracker systems. In: MSR ’20, p 137–146, DOI 10.1145/3379597.
3387459

Zhang X, Rastogi A, Yu Y (2020) On the shoulders of giants: A new dataset for pull-based
development research. In: Proceedings of the 17th International Conference on Mining Soft-
ware Repositories, Association for Computing Machinery, New York, NY, USA, MSR ’20, p
543–547, DOI 10.1145/3379597.3387489, URL https://doi.org/10.1145/3379597.3387489

2021 Selected Papers

(2021) Self-admitted technical debt in R packages: An exploratory study. In: 2021 IEEE/ACM
18th International Conference on Mining Software Repositories (MSR), pp 179–189, DOI
10.1109/MSR52588.2021.00030

Al Alamin MA, Malakar S, Uddin G, Afroz S, Haider TB, Iqbal A (2021) An empirical study
of developer discussions on low-code software development challenges. In: 2021 IEEE/ACM
18th International Conference on Mining Software Repositories (MSR), pp 46–57, DOI
10.1109/MSR52588.2021.00018

Albonico M, Malavolta I, Pinto G, Guzman E, Chinnappan K, Lago P (2021) Mining energy-
related practices in robotics software. In: 2021 IEEE/ACM 18th International Conference
on Mining Software Repositories (MSR), pp 483–494, DOI 10.1109/MSR52588.2021.00060
Alfadel M, Costa DE, Shihab E, Mkhallalati M (2021) On the use of Dependabot security pull
requests. In: 2021 IEEE/ACM 18th International Conference on Mining Software Reposi-
tories (MSR), pp 254–265, DOI 10.1109/MSR52588.2021.00037

Alghamdi M, Hayashi S, Kobayashi T, Treude C (2021) Characterising the knowledge about
primitive variables in Java code comments. In: 2021 IEEE/ACM 18th International Confer-
ence on Mining Software Repositories (MSR), pp 460–470, DOI 10.1109/MSR52588.2021.
00058

Ciniselli M, Cooper N, Pascarella L, Poshyvanyk D, Di Penta M, Bavota G (2021) An em-
pirical study on the usage of BERT models for code completion. In: 2021 IEEE/ACM

Pitfalls and Guidelines for Using Time-Based Git Data

51

18th International Conference on Mining Software Repositories (MSR), pp 108–119, DOI
10.1109/MSR52588.2021.00024

Codabux Z, Vidoni M, Fard FH (2021) Technical debt in the peer-review documentation of
R packages: a rOpenSci case study. In: 2021 IEEE/ACM 18th International Conference on
Mining Software Repositories (MSR), pp 195–206, DOI 10.1109/MSR52588.2021.00032
Cˆandido J, Haesen J, Aniche M, van Deursen A (2021) An exploratory study of log placement
recommendation in an enterprise system. In: 2021 IEEE/ACM 18th International Confer-
ence on Mining Software Repositories (MSR), pp 143–154, DOI 10.1109/MSR52588.2021.
00027

Dabic O, Aghajani E, Bavota G (2021) Sampling projects in GitHub for MSR studies. In:
2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),
pp 560–564, DOI 10.1109/MSR52588.2021.00074

Ding ZY, Le Goues C (2021) An empirical study of OSS-Fuzz bugs. In: 2021 IEEE/ACM
18th International Conference on Mining Software Repositories (MSR), pp 131–142, DOI
10.1109/MSR52588.2021.00026

Durieux T, Soto-Valero C, Baudry B (2021) Duets: A dataset of reproducible pairs of Java
library-clients. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp 545–549, DOI 10.1109/MSR52588.2021.00071

Eng K, Hindle A (2021) Revisiting Dockerﬁles in open source software over time. In: 2021
IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pp
449–459, DOI 10.1109/MSR52588.2021.00057

Eskandani N, Salvaneschi G (2021) The Wonderless dataset for serverless computing. In: 2021
IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pp
565–569, DOI 10.1109/MSR52588.2021.00075

Flint SW, Chauhan J, Dyer R (2021) Escaping the time pit: Pitfalls and guidelines for using
time-based git data. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp 85–96, DOI 10.1109/MSR52588.2021.00022

Fournier Q, Aloise D, Azhari SV, Tetreault F (2021) On improving deep learning trace analysis
with system call arguments. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 120–130, DOI 10.1109/MSR52588.2021.00025

Gholamian S, Ward PAS (2021) On the naturalness and localness of software logs. In: 2021
IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pp
155–166, DOI 10.1109/MSR52588.2021.00028

Gote C, Zingg C (2021) gambit – an open source name disambiguation tool for version control
systems. In: 2021 IEEE/ACM 18th International Conference on Mining Software Reposito-
ries (MSR), pp 80–84, DOI 10.1109/MSR52588.2021.00021

Haben G, Habchi S, Papadakis M, Cordy M, Le Traon Y (2021) A replication study on the
usability of code vocabulary in predicting ﬂaky tests. In: 2021 IEEE/ACM 18th International
Conference on Mining Software Repositories (MSR), pp 219–229, DOI 10.1109/MSR52588.
2021.00034

Hora A (2021a) Googling for software development: What developers search for and what they
ﬁnd. In: 2021 IEEE/ACM 18th International Conference on Mining Software Repositories
(MSR), pp 317–328, DOI 10.1109/MSR52588.2021.00044

Hora A (2021b) What code is deliberately excluded from test coverage and why? In: 2021
IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pp
392–402, DOI 10.1109/MSR52588.2021.00051

Imam A, Dey T (2021) Tracking hackathon code creation and reuse. In: 2021 IEEE/ACM
18th International Conference on Mining Software Repositories (MSR), pp 615–617, DOI
10.1109/MSR52588.2021.00085

Imran MM, Ciborowska A, Damevski K (2021) Automatically selecting follow-up questions
for deﬁcient bug reports. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 167–178, DOI 10.1109/MSR52588.2021.00029

Kim M, Kim Y, Lee E (2021) Denchmark: A bug benchmark of deep learning-related software.
In: 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),
pp 540–544, DOI 10.1109/MSR52588.2021.00070

Kinsman T, Wessel M, Gerosa MA, Treude C (2021) How do software developers use GitHub
actions to automate their workﬂows? In: 2021 IEEE/ACM 18th International Conference
on Mining Software Repositories (MSR), pp 420–431, DOI 10.1109/MSR52588.2021.00054
Malavolta I, Chinnappan K, Swanborn S, Lewis GA, Lago P (2021) Mining the ROS ecosystem
for green architectural tactics in robotics and an empirical evaluation. In: 2021 IEEE/ACM

52

Flint, Chauhan, and Dyer

18th International Conference on Mining Software Repositories (MSR), pp 300–311, DOI
10.1109/MSR52588.2021.00042

Manes SS, Baysal O (2021) Studying the change histories of Stack Overﬂow and GitHub snip-
pets. In: 2021 IEEE/ACM 18th International Conference on Mining Software Repositories
(MSR), pp 283–294, DOI 10.1109/MSR52588.2021.00040

Marcilio D, Furia CA (2021) How Java programmers test exceptional behavior. In: 2021
IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pp
207–218, DOI 10.1109/MSR52588.2021.00033

Mondal S, Uddin G, Roy CK (2021) Rollback edit inconsistencies in developer forum. In:
2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),
pp 380–391, DOI 10.1109/MSR52588.2021.00050

Nielebock S, Blockhaus P, Kr¨uger J, Ortmeier F (2021) Androidcompass: A dataset of Android

compatibility checks in code repositories. 2103.09620

Opdebeeck R, Zerouali A, De Roover C (2021) Andromeda: A dataset of Ansible Galaxy roles
and their evolution. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp 580–584, DOI 10.1109/MSR52588.2021.00078

Papoutsoglou M, Wachs J, Kapitsaki GM (2021) Mining DEV for social and technical insights
about software development. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 415–419, DOI 10.1109/MSR52588.2021.00053

Pei J, Wu Y, Qin Z, Cong Y, Guan J (2021) Attention-based model for predicting question re-
latedness on Stack Overﬂow. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 97–107, DOI 10.1109/MSR52588.2021.00023

Pfeiﬀer RH (2021) Identifying critical projects via PageRank and truck factor. In: 2021
IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pp
41–45, DOI 10.1109/MSR52588.2021.00017

Pornprasit C, Tantithamthavorn CK (2021) Jitline: A simpler, better, faster, ﬁner-grained just-
in-time defect prediction. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 369–379, DOI 10.1109/MSR52588.2021.00049

Quaranta L, Calefato F, Lanubile F (2021) Kgtorrent: A dataset of python Jupyter note-
books from Kaggle. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp 550–554, DOI 10.1109/MSR52588.2021.00072

Santos F, Wiese I, Trinkenreich B, Steinmacher I, Sarma A, Gerosa MA (2021) Can i solve it?
identifying APIs required to complete OSS tasks. In: 2021 IEEE/ACM 18th International
Conference on Mining Software Repositories (MSR), pp 346–257, DOI 10.1109/MSR52588.
2021.00047

Schuler A, Kotsis G (2021) Mining API interactions to analyze software revisions for the
evolution of energy consumption. In: 2021 IEEE/ACM 18th International Conference on
Mining Software Repositories (MSR), pp 312–316, DOI 10.1109/MSR52588.2021.00043
Scoccia GL, Migliarini P, Autili M (2021) Challenges in developing desktop web apps: a study of
Stack Overﬂow and GitHub. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 271–282, DOI 10.1109/MSR52588.2021.00039

Sharma T, Kessentini M (2021) QScored: A large dataset of code smells and quality metrics. In:
2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),
pp 590–594, DOI 10.1109/MSR52588.2021.00080

Sri-iesaranusorn P, Kula RG, Ishio T (2021) Does code review promote conformance? a study of
OpenStack patches. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp 444–448, DOI 10.1109/MSR52588.2021.00056

Sridharan M, Mantyla M, Rantala L, Claes M (2021) Data balancing improves self-admitted
technical debt detection. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 358–368, DOI 10.1109/MSR52588.2021.00048

Sviridov N, Evtikhiev M, Kovalenko V (2021) Tnm: A tool for mining of socio-technical data
from git repositories. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp 295–299, DOI 10.1109/MSR52588.2021.00041

Svyatkovskiy A, Lee S, Hadjitoﬁ A, Riechert M, Franco JV, Allamanis M (2021) Fast and
memory-eﬃcient neural code completion. In: 2021 IEEE/ACM 18th International Confer-
ence on Mining Software Repositories (MSR), pp 329–340, DOI 10.1109/MSR52588.2021.
00045

Tu H, Papadimitriou G, Kiran M, Wang C, Mandal A, Deelman E, Menzies T (2021) Mining
workﬂows for anomalous data transfers. In: 2021 IEEE/ACM 18th International Conference
on Mining Software Repositories (MSR), pp 1–12, DOI 10.1109/MSR52588.2021.00013

Pitfalls and Guidelines for Using Time-Based Git Data

53

Uchˆoa A, Barbosa C, Coutinho D, Oizumi W, Assun¸c˜ao WKG, Vergilio SR, Pereira JA,
Oliveira A, Garcia A (2021) Predicting design impactful changes in modern code review: A
large-scale empirical study. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 471–482, DOI 10.1109/MSR52588.2021.00059

Vagavolu D, Agrahari V, Chimalakonda S, Venigalla ASM (2021) GE526: A dataset of open-
source game engines. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp 605–609, DOI 10.1109/MSR52588.2021.00083

Wendland T, Sun J, Mahmud J, Mansur SMH, Huang S, Moran K, Rubin J, Fazzini M
(2021) Andror2: A dataset of manually-reproduced bug reports for Android apps. 2021
IEEE/ACM 18th International Conference on Mining Software Repositories (MSR) DOI
10.1109/msr52588.2021.00082, URL http://dx.doi.org/10.1109/MSR52588.2021.00082
Yin L, Zhang Z, Xuan Q, Filkov V (2021) Apache Software Foundation Incubator Project sus-
tainability dataset. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp 595–599, DOI 10.1109/MSR52588.2021.00081

Yitagesu S, Zhang X, Feng Z, Li X, Xing Z (2021) Automatic part-of-speech tagging for security
vulnerability descriptions. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp 29–40, DOI 10.1109/MSR52588.2021.00016

Young JG, Casari A, McLaughlin K, Trujillo MZ, H´ebert-Dufresne L, Bagrow JP (2021)
Which contributions count? analysis of attribution in open source. In: 2021 IEEE/ACM
18th International Conference on Mining Software Repositories (MSR), pp 242–253, DOI
10.1109/MSR52588.2021.00036

Zerouali A, Vel´azquez-Rodr´ıguez C, De Roover C (2021) Identifying versions of libraries used
in Stack Overﬂow code snippets. In: 2021 IEEE/ACM 18th International Conference on
Mining Software Repositories (MSR), pp 341–345, DOI 10.1109/MSR52588.2021.00046

