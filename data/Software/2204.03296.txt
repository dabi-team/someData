1 

Deep Learning for Real Time Satellite Pose 
Estimation on Low Power Edge TPU 

Alessandro Lotti, Dario Modenini, Paolo Tortora, Massimiliano Saponara, and Maria A. Perino  

Abstract‚Äî Pose estimation of an uncooperative space resident 
object is a key asset towards autonomy in close proximity opera-
tions. In this context monocular cameras are a valuable solution 
because of their low system requirements. However, the associated 
image processing algorithms are either too computationally expen-
sive for real time on-board implementation, or not enough accu-
rate. In this paper we propose a pose estimation software exploit-
ing neural network architectures which can be scaled to different 
accuracy-latency trade-offs. We designed our pipeline to be com-
patible with Edge Tensor Processing Units to show how low power 
machine learning accelerators could enable Artificial Intelligence 
exploitation in space. The neural networks were tested both on the 
benchmark Spacecraft Pose Estimation Dataset, and on the  pur-
posely  developed  Cosmo  Photorealistic  Dataset,  which  depicts  a 
COSMO-SkyMed satellite in a variety of random poses and steer-
able solar panels orientations. The lightest version of our architec-
ture  achieves  state-of-the-art  accuracy  on  both  datasets  but at  a 
fraction of networks complexity, running at 7.7 frames per second 
on a Coral Dev Board Mini consuming just 2.2W. 

I.  Introduction 

V 

ision based navigation is a key technology for next gener-
ation of On-Orbit Servicing (OOS) and Active Debris Re-
moval (ADR) missions. In these scenarios,  guidance and con-
trol laws shall be fed with the relative chaser-to-target pose (i.e. 
position  and attitude)  which  might  be  conveniently  estimated 
from monocular images as these sensors are simple, light, and 
consume little power. Traditionally, Image Processing (IP) al-
gorithms  are  divided  in  i)  hand-crafted  features  [1,2]  and  ii) 
Deep Learning (DL) based [3-14]. However, the former are af-
fected by low robustness against typical space imagery charac-
teristics such as low Signal-to-Noise-Ratio (SNR), severe, and 
rapidly varying illumination conditions, and backgrounds. Neu-
ral Networks (NNs) could overcome such weaknesses through 
proper training but often result in a high computational burden, 
hardly compatible with typical onboard processing power.  

In recent years deep learning has been shown to aid space-
craft monocular pose estimation at different levels. Sharma et 
A. Lotti is with the Department of Industrial Engineering, Alma Mater Stu-
(e-mail:  ales-

diorum  Universit√†  di  Bologna,  Forl√¨,  47121, 
sandro.lotti4@unibo.it). 

Italy 

D. Modenini, and P. Tortora are with the Department of Industrial Engineer-
ing and with the Interdepartmental Centre for Industrial Research, Alma Mater 
Studiorum Universit√† di Bologna, Forl√¨, 47121, Italy (e-mail: {dario.modenini, 
paolo.tortora}@unibo.it). 

M. Saponara and M. A. Perino are with Thales Alenia Space Italia, Turin, 
10146,  Italy  (e-mail:  {massimiliano.saponara,  mariaantonietta.perino}  @tha-
lesaleniaspace.com). 

This  work  was  partially  supported  by  Thales  Alenia  Space  Italia,  in  the 
framework of the project ‚ÄúA testbed for deep learning in support of docking-
grasping operations‚Äù. 

al [3] addressed the problem as a classification task by discre-
tizing  the  pose  space.  Later,  Sharma  and  D‚ÄôAmico  [4],  pro-
posed a solution based on joint classification and regression. In 
this last work, the authors also presented  the Spacecraft PosE 
Estimation Dataset (SPEED), providing synthetic and real high 
resolution  (1920x1200  px)  images  of  the  Tango  spacecraft. 
SPEED has been adopted as a benchmark in the first Satellite 
Pose Estimation Challenge (SPEC) [15] co-hosted by the Euro-
pean  Space  Agency  (ESA) and  Stanford‚Äôs  Space  Rendezvous 
Laboratory  (SLAB).  Three  out  the  four  top-scoring  works 
adopted a three stage approach leveraging Convolutional Neu-
ral Networks (CNNs) to detect the target on the image first, and 
to  regress  the  locations  of  some  predefined  keypoints  later 
which are then fed into off-the-shelf Perspective-n-Point (PnP) 
solvers [5,6] for pose estimation. On the other hand, the third 
classified  [7]  proposed  an  end-to-end  CNN  based  pipeline 
which however required a huge model to achieve competitive 
accuracy. Indeed, submissions were ranked solely on a pose re-
gression error, regardless of their computational burden. Only 
the  SLAB  baseline  [6]  addressed  this  issue  by  adopting  light 
networks which however led to a pose estimation error signifi-
cantly higher than the best one.  

Later works addressed the pose estimation accuracy-latency 
trade-off.  To  this  aim,  Hu  et  al.  [8]  proposed  a  single  stage 
method leveraging a 3D loss to make pose estimation less sen-
sitive to scale variations. Wang et al. [9] exploited Transform-
ers for direct keypoints coordinates retrieval upon a CNN based 
detection step, while Piazza et al. [10] adopted a light model for 
target detection. Carcagn√¨ et al. [11] revisited the method in [5] 
by replacing the landmarks regression network backbone with 
a lighter variant. Similarly, Posso et al. [12] revisited the end-
to-end approach proposed in [7] in a lite manner, reaching how-
ever an accuracy that is far from the top submissions at SPEC.  
These  works  have  been  tested  on  high  end  desktop  GPUs 
only and they are not optimized for low power embedded de-
vices which typically perform well on limited sets of operations 
as a consequence of hardware specialization. Black et al. [13] 
contributed  remarkably  in  this  respect,  by  developing  a  light 
pipeline that achieves real time inference on an Intel Joule 570x 
board consuming 3.7 W.  

In  this  context,  our  work  focuses  on  further  improving  the 
pose  estimation  accuracy-latency  trade-off  on  low  power  de-
vices from both the software and hardware points of view.  

First, we propose a pose estimation pipeline based on NNs 
optimized for embedded devices, with an architecture that can 
be scaled to the available computational power. We investigate 
optimizations  through  TensorFlow  Lite  (TFLite)  conversion 
and  quantization.  The  former  is  a  Machine  Learning (ML) li-
brary  for  on-device  inference  while  the  latter  consists  of 

 
 
 
converting  NNs‚Äô  weights to  integers,  yielding to  a  significant 
runtime advantage at deployment.  

Second, we investigate high performing ML coprocessors for 
low  power  integer-only  inference,  namely  Edge  Tensor  Pro-
cessing  Units  (TPUs),  which  are  gaining  the  attention  of  the 
space community [16]. We thus test our models on a Coral Dev 
Board Mini equipped with both a TPU and a 1.5 GHz quadcore 
CPU. We evaluate our algorithms on both SPEED [4] and on a 
new  dataset  developed  as  part  of  this  work,  named  COSMO 
Photorealistic  Dataset  (CPD),  depicting  a  COSMO  SkyMed 
satellite.  Our  results  show  how  model  optimization  and  edge 
processors can enable sub-degree and centimeter-level real time 
pose  estimation  compatible  with  typically  available  onboard 
power levels.  

II.  Cosmo Photorealistic Dataset 
Moving components, such as antennas and solar arrays, are 
common for large spacecrafts, i.e. the ones that would benefit 
the most from OOS. The need to validate pose estimation pipe-
lines even in this scenario, drove the design of a new dataset, 
named COSMO Photorealistic Dataset (CPD), depicting a sat-
ellite from the SkyMed Earth observation constellation in het-
erogeneous combinations of poses, solar arrays configurations, 
lighting conditions, and backgrounds. To this end, the 3D com-
puter graphics software Blender was selected because of its na-
tive  support  to  Physically  Based  Rendering  (PBR).  Most  as-
sumptions  adopted  for  setting  the  spacecraft  pose  distribution 
and image post-processing follow that of SPEED [4] for ease of 
benchmarking, as detailed in the following. 

A.  Blender Scene 

The Blender scene consists of three concentric highly polyg-
onal spheres representing Earth, clouds, and atmosphere plus a 
CAD  model  of  the  COSMO  spacecraft.  Similarly  to  [7],  the 
Earth  was  textured  with  a  high  resolution  map  further  aug-
mented  with  an  ocean  mask1  and  topography  data  from  the 
NASA‚Äôs  Blue  Marble collection2  which also provided clouds 
texture. A third-party shader3 was applied to increase the real-
ism of the clouds providing at the same time transparency, dif-
fusion, and reflection. Atmospheric scattering was emulated ex-
ploiting Blender‚Äôs volumetric rendering in combination with a 
second  shader, from  the  same  package,  which implements an 
exponential density model. A Blender‚Äôs sun lamp emulates the 
Sun by providing collimated light at a blackbody temperature 
of 5778 K. 
  The  main  exterior  features  of  COSMO  spacecraft  are  the 
Multi-Layer  Insulation  (MLI)  and  the  solar  panels.  A  faithful 
representation  of  the  MLI  material  was  obtained  thanks  to  a 
crumpled  normal  texture4  mapped  to  the  spacecraft  body  for 
emulating the typical random reflections. The solar arrays have 
been equipped with a solar cell texture providing base color and 
displacement,  while  surface  reflection  has  been  obtained 
through a Blender‚Äôs glossy shader. 

B.  Pose Distribution 

The  distance  is  randomly  selected  from  a  standard  normal 
distribution  ùëÅ(ùúá  =  36ùëö,  ùúé  =  10ùëö),  rejecting  all  the  values 

2 

above 70m and below 36m. The x-y offsets on the image plane 
are  uncorrelated  random  values  selected  from  a  multivariate 
normal  distribution,  constrained  to  guarantee  that  the  satellite 
almost always lies entirely in the image frame. The attitude is 
randomly  sampled  from a  uniform distribution  of rotations in 
the SO(3) space.  

Pose  distribution  and  camera-satellite  alignment  are  gov-
erned by the Starfish library [14]. Domain variation is provided 
through Blender Python API, by rotating Earth and clouds be-
neath  the  satellite  before  each  rendering.  Geometrical  con-
straints on the Sun-satellite-Earth angle and Sun-satellite-cam-
era angle are prescribed to avoid dark images due to a non-illu-
minated  camera  view.  Besides  that,  lighting  direction  is  ran-
domized across the dataset to provide a comprehensive range of 
illumination conditions. Sun-tracking rotation of solar panels is 
added through a Blender ‚Äúlocked track‚Äù constraint which allow 
them to rotate about their longitudinal axis for tracking the Sun 
direction. 

C.  Render Setup and Post Processing 

A  total  of  15000 images  have  been  rendered  with the  PBR 
Cycles engine through a pinhole camera model. The resolution 
has been set to 1920x1200 px. Post-processing steps include a 
glare node, meant to replicate the bloom effect, grayscale con-
version, and addition of Gaussian noise and Gaussian blurring 
to emulate shot noise and depth of field. Fig. 1 depicts a close-
up render of the satellite and two samples from the dataset, prior 
to grayscale conversion and noise addition. 

Fig. 1 Close-up preview of the COSMO SkyMed satellite (top), sample 
images from CPD prior to postprocessing (bottom) 

III.  Methods 
Our  software  pipeline  is  based  on  three  stages,  namely  A) 
spacecraft detection, B) landmarks regression, and C) pose es-
timation. For this work, we hold the assumption that the target 
is  known  by  the  chaser,  i.e.  a  wireframe  model  is  available. 
Since this information was not included in the SPEED [4] da-
taset, we reconstructed a 3D model through multiview triangu-
lation. We then retrieved training and testing labels (i.e. bound-
ing boxes and landmarks coordinates) for both datasets by pro-
jecting the 3D satellites‚Äô keypoints, highlighted in Fig. 2, onto 
the image plane exploiting the known target pose. 

1 Tom Patterson, www.shadedrelief.com 
2 NASA, Visible Earth, visibleearth.nasa.gov 

3 F. Lasse, Physically Correct Atmosphere Shader, gumroad.com/l/JlNTt 
4 NASA, 3D Resources, nasa3d.arc.nasa.gov/detail/Sentinel-6 

 
 
  
  
  
3 

Target  position  and  orientation  are  estimated  through  an 
EPnP solver [19] with RANSAC exploiting the known 2D-3D 
correspondences. The resulting pose is eventually refined with 
a Levenberg‚ÄìMarquardt optimization step9. 

IV.  Experiments and Results 
Our models are first compared with state-of-the-art methods 
using the floating point, non-optimized NNs configuration. We 
then illustrate the benefits obtained through TensorFlow Lite10 
conversion first and quantization later. These modifications en-
able  the  deployment  of  the  models  on  a  low  power  machine 
learning accelerator, namely a TPU, which can further reduce 
the inference time. 

A.  Training Setup 

For SPEED dataset, since the ground truth poses for the test 
sets have not been released at the time of this writing, only syn-
thetic  images  from  the  SPEC  training  set  were  used.  Both 
SPEED and CPD have been divided into train and test clusters 
through uniform random sampling (Table 2). 

The DN is trained for 50000 steps with momentum optimizer 
starting from COCO [20] pretrained weights, applying random 
crops and horizontal flips to avoid overfitting. RNs have been 
trained with Adam optimizer [21] and mean absolute error loss 
for 550 epochs on SPEED and 450 epochs on CPD. Efficient-
Net-Lite backbones were initialized with Imagenet [22] check-
points. Applied data augmentations include random image ro-
tations, bounding box enlargements and shifts, random bright-
ness, and contrast adjustments.  

The learning rate was gradually reduced according to a co-
sine decay law after a warmup phase, lasting 2000 steps for the 
DN  and  10 epochs  for the  RNs,  where it  grows linearly  from 
0.15 to 0.45 and from 1e-4 to 3e-3 respectively. The batch size 
was set to 256 for both DN and RNs. 

The  trained  networks  were  first  deployed  on the  CPU  of  a 
Coral Dev Board Mini for performance assessment. Later, all 
RNs have been re-trained exploiting Quantization Aware Train-
ing11 (QAT) which emulates quantization along with NNs pa-
rameters tuning, to reduce accuracy loss at conversion. The only 
exception is the DN which is trained only once for each of the 
two  datasets  directly  applying  QAT,  to  reduce  development 
time. 

The results provided in next paragraphs refer to the SPEED 

[4] dataset, unless otherwise stated. 

Dataset 
SPEED 
CPD 

Table 2 Datasets partitioning 
Train images 
9728 
12032 

Test images 
2272 
2968 

Fig. 2 Wireframe models: COSMO (left), Tango (right) 

A.  Spacecraft Detection 

Direct  processing  of  high  resolution  images  would  prevent 
real-time inference on low power embedded devices, due to the 
need of large NNs and a high memory footprint. The purpose 
of the Detection Network (DN) is therefore to identify a Region 
of Interest (ROI), by detecting the satellite on the image. 

To this end, we employed a MobileDet Edge TPU optimized 
model  [17]  from the TensorFlow  (TF)  Object  Detection  API5 
with  an  input  shape  of  320x320  px  and about  3.3  millions of 
parameters.  The  predicted  bounding  box  is  made  squared,  to 
avoid distortion, and enlarged by a factor 1.15 to increase the 
chance that the satellite lies within its margins. The ROI accu-
racy  is  defined  as  the  percentage  of  times  the  ground  truth 
bounding box is contained within the regressed one. In addition, 
the matching between the true and estimated ROI is assessed in 
terms  of  Intersection  Over  Union  (IoU).  In  case  the  ROI  is 
smaller than the second NN input size, the bounding box is fur-
ther expanded, otherwise the cropped image is resized to fulfil 
that requirement.  

B.  Landmarks Regression 

The  resulting  ROI  is  fed  into  a  second  CNN  which  is  in 
charge of detecting a set of predefined keypoints. We propose 
a Regression Network (RN) architecture which can be scaled to 
different accuracy-latency trade-offs. It consists of a fully con-
volutional model built on top of EfficientNet-Lite backbones6,7, 
which are obtained by removing operations not well supported 
by mobile accelerators from the original EfficientNets [18]. The 
regression  head  consists  of  a  sequence  of  four  convolutions 
(Table 1) which gradually reduce the feature map channels and 
dimensions through learnt operations rather than pooling. The 
network returns a vector containing normalized coordinates of 
the landmarks in a predefined order. 

Model scaling is inherent to the adoption of EfficientNet-Lite 
backbones  which  are  developed  to  this  purpose,  each  model 
featuring a prescribed combination of network‚Äôs width, depth, 
and input resolution8. In this work, we tested EfficientNet-Lite 
versions 0 to 4, for a total of 5 variants. 

# 
1 
2 
3 
4 

Type 
Standard 
Separable  
Standard 
Standard 

Table 1 Regression head structure 
Activation 
Relu 
Relu 
- 
- 

Kernel size 
1 
3 
1 
#3 resolution 

Padding 
Valid 
Same 
Valid 
Valid 

#filters 
128 
128 
kpts x2 
kpts x2 

B.  Comparison with State of the Art 

To  assess  the  accuracy  of  our  methods  we  adopt  the  same 
metric of SPEC [15]. This is based on the sum of a normalized 
position (ùëíùë°ÃÖ ) and rotation (ùëíùëû) errors averaged over all the N test 
images: 

5github.com/tensorflow/models/blob/master/research/object_detec-
tion/g3doc/tf1_detection_zoo.md 
6blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-effi-
cientnet-lite.html 
7github.com/sebastian-sz/efficientnet-lite-keras 

8github.com/tensorflow/tpu/blob/master/models/official/efficientnet/lite/effi-
cientnet_lite_builder.py#L38-L45 
9docs.opencv.org/3.4.12/d9/d0c/group__calib3d.html 
10tensorflow.org/lite?hl=en 
11tensorflow.org/model_optimization/guide/quantization/training?hl=en 

 
 
 
ùëíùë°ùëñ = |ùíïùê∫ùëá ùëñ ‚àí ùíïùê∏ùëÜùëá ùëñ|
2
ùëÅ

ùëíùë°ÃÖ =

1
ùëÅ

ùëíùë°ùëñ
|ùíïùê∫ùëá ùëñ|
2

‚àë

ùëñ=1

ùëíùëû =

1
ùëÅ

ùëÅ
‚àë 2 arccos(|‚ü®ùííùê∫ùëáùëñ,ùííùê∏ùëÜùëáùëñ
ùëñ=1

‚ü©|)

ùê∏ = ùëíùë°ÃÖ + ùëíùëû 

Where ùíïùê∫ùëá, ùííùê∫ùëá, and ùíïùê∏ùëÜùëá, ùííùê∏ùëÜùëá denote the ground truth and 
estimated position and attitude quaternion respectively. Table 3 
displays the error metrics attained by all of our pipeline variants 
along with those of the best ranked submissions at SPEC and 
the embedded method proposed in [13]. Even though a direct 
comparison with would require to train and test the NNs on the 
same images subsets, our results clearly indicate how state-of-
the art accuracy can be achieved with remarkably less NNs pa-
rameters.  

Table 3 Comparison with state of the art [6-8,13,15] 

Model 

ùë¨ 

ùíÜùíí[¬∞] 

ùíÜùíï [ùíé] 

UniAdelaide [5] 
Our_lite4 
Our_lite3 
Our_lite2 
Our_lite1 
Our_lite0 
EPFL_cvlab12 
Black, et al. [13] 

0.0094 
0.0119 
0.0124 
0.0131 
0.0134 
0.0149 
0.0215 
0.0409 
pedro_fairspace [7]  0.0571 
SLAB_baseline [6]  0.0626 

0.41 ¬± 1.50 
0.52 ¬± 0.52 
0.55 ¬± 0.53 
0.58 ¬± 0.57 
0.59 ¬± 0.57 
0.65 ¬± 0.58 
0.91 ¬± 1.29 
- 
2.49 ¬± 3.02 
2.62 ¬± 2.90 

0.032 ¬± 0.095 
0.034 ¬± 0.069 
0.033 ¬± 0.062 
0.036 ¬± 0.075 
0.037 ¬± 0.068 
0.040 ¬± 0.073 
0.073 ¬± 0.587 
- 
0.145 ¬± 0.239 
0.209 ¬± 1.133 

#Parame-
ters 
176.2 M 
15.4 M 
10.5 M 
8.4 M 
7.7 M 
6.9 M 
89.2 M 
6.9 M 
‚âà 500 M 
11.2 M 

C.  Conversion to TensorFlow Lite 

As TF is not optimized for on-device inference, we converted 
all NNs to TFLite. In this work, we exploited the 2.7.0 version 
of tflite-runtime. Performance comparisons are reported in Ta-
ble  4  and  Table  5,  highlighting  latency  reduction  and  persis-
tence of accuracy. Note that the frames per second (fps) data do 
not include image loading time from memory. 

Table 4 DN performances 

DN 
TF 
TFLite 

IoU mean 
0.9426 
0.9408 

IoU median 
0.9600 
0.9575 

ROI accuracy  
96.88 % 
97.40 % 

Table 5 RN variants details and TF vs TFLite pipelines performances 
comparison 

RNs 

Lite0 
Lite1 
Lite2 
Lite3 
Lite4 

Input 
size [px] 
224x224 
240x240 
260x260 
280x280 
300x300 

Pipeline fps 

Pose error ùë¨ 

TF 
0.69 
0.61 
0.56 
0.46 
0.33 

TFLite 
0.76 
0.68 
0.62 
0.51 
0.39 

TF 
0.0149 
0.0134 
0.0131 
0.0124 
0.0119 

TFLite 
0.0150 
0.0136 
0.0131 
0.0126 
0.0118 

D.  Quantization: CPU vs TPU Inference 

Quantization, which is needed for deploying the networks on 
the  TPU,  allows  compressing  the  file  sizes  up  to  the  75% 
thereby significantly reducing latency. Accuracy drop is inevi-
table; however, to minimize it, RNs have been retrained apply-
ing  QAT.  All  networks  have  been  fully  quantized  except  for 
input and output tensors. A comparison between CPU and TPU 

12indico.esa.int/event/319/attachments/3561/4754/pose_gerard_segmenta-
tion.pdf 

4 

performances is provided in Table 6 and Table 7, highlighting 
the superior fps attainable by the latter, which is up to the 253% 
higher than with the CPU. In addition, the TPU allows reducing 
power consumption at inference: our tests revealed that the av-
erage absorption drops from the 3 W required by CPU to 2.2 W 
for TPU. 

Table 6 CPU vs TPU quantized NNs runtime 

DN Runtime [ms] 

RN Runtime [ms] 

CPU 
303.54 
300.58 
296.76 
319.60 
304.23 

TPU 
69.82 
97.92 
128.26 
139.39 
144.15 

CPU 
136.19 
194.29 
261.83 
425.67 
658.96 

TPU 
36.77 
63.26 
92.45 
177.49 
414.77 

Table 7 CPU vs TPU quantized pipelines performances 

Pose 
error, ùë¨ 

0.0179 
0.0156 
0.0151 
0.0152 
0.0147 

Board Tempera-
ture [¬∞C ] 

Pipeline speed [fps]  

CPU 

83.82 
84.63 
84.80 
84.79 
85.39 

TPU 

51.86 
51.66 
51.43 
48.95 
46.71 

CPU  TPU 

2.17 
1.94 
1.72 
1.30 
1.01 

7.66 
5.38 
4.06 
2.92 
1.70 

Gain 
[%] 
+253% 
+177% 
+136% 
+125% 
+68% 

RNs 

Lite0 
Lite1 
Lite2 
Lite3 
Lite4 

RNs 

Lite0 
Lite1 
Lite2 
Lite3 
Lite4 

E.  Performances on CPD 

Finally, we tested our TPU pipelines on CPD. The good per-
formance  of  the  DN  (Table  8)  demonstrates  its  robustness 
against variable solar panels orientation. The mean pose error E 
is similar to that on SPEED, even though few outliers are pre-
sent, as highlighted in Table 9. The worst results are obtained 
on images where the solar panels occlude a large portion of the 
SAR  antenna,  images  taken  in  near  eclipse  conditions,  and 
those characterized by extreme blooming. 

Table 8 DN performances on CPD 

Quantized DN 

IoU mean 
0.9417 

IoU median 
0.9537 

ROI accuracy  
99.2 % 

Table 9 TPU pipelines performances on CPD 

Quantized 
RNs 
Lite0 
Lite1 
Lite2 
Lite3 
Lite4 

ùë¨ 

0.0170 
0.0153 
0.0159 
0.0141 
0.0140 

ùíÜùíí [¬∞] 

ùíÜùíï [ùê¶] 

0.741 ¬± 0.804 
0.667 ¬± 0.638 
0.654 ¬± 1.648 
0.613 ¬± 0.589 
0.580 ¬± 0.660 

0.180 ¬± 0.355 
0.161 ¬± 0.230 
0.202 ¬± 2.644 
0.152 ¬± 0.273 
0.173 ¬± 1.598 

V.  Conclusion 

We introduced a new photorealistic satellite dataset includ-
ing steerable solar panels and we discussed neural models ena-
bling real-time spacecraft pose estimation from monocular im-
ages on low power embedded hardware. 

Our pipelines perform on par with the state of the art while 
using extremely lite networks. Switching from CPU to TPU al-
lows increasing the fps by a factor of ‚âà 2 to 3 (up to 7.7 fps) 
while reducing the measured power consumption of 25% (from 
3W down to 2.2 W).  

When evaluated on our CPD, the algorithms exhibit accura-
cies in line with SPEED, although with higher sensitivity to oc-
clusions, which deserves further investigation.  

 
 
 
 
 
 
 
 
 
5 

[17]   Xiong, Y., Liu, H., Gupta, S., Akin, B., Bender, G., Wang, Y., Kinder-
mans, P.-J., Tan,  M.,  Singh, V., and Chen, B., ‚ÄúMobileDets: Searching 
for  Object  Detection  Architectures  for  Mobile  Accelerators,‚Äù  Proceed-
ings  of  the  IEEE/CVF  Conference  on  Computer  Vision  and  Pattern 
Recognition, IEEE CVF, Rockville MD, 2021, pp. 3824-3833, 2021. doi: 
10.1109/CVPR46437.2021.00382 

[18]   Tan, M., and Le, Q., ‚ÄùEfficientNnet: Rethinking model scaling for con-
volutional neural networks,‚Äù International conference on machine learn-
ing, PMLR, 2019, pp. 6105-6114. 

[19]   Lepetit, V.,  Moreno-Noguer, F., and Fua, P.,   ‚ÄùEpnp: An accurate o(n) 
solution  to  the  pnp  problem,‚Äù International  journal  of  computer  vi-
sion, Vol.  81,  No.  2,  2009,  pp.  155-166.  doi:  doi.org/10.1007/s11263-
008-0152-6 

[20]   Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., 
Doll√°r,  P.,  and  Zitnick,  C.  L.,  ‚ÄúMicrosoft  COCO:  Common  objects  in 
context,‚Äù European  conference  on  computer  vision, Springer,  Zurich, 
2014, pp. 740-755. 

[21]   Kingma, P.M., and Ba, J., ‚ÄúAdam: A  Method for Stochastic Optimiza-
tion,‚Äù 3rd International Conference on Learning Representations, Com-
putational and Biological Learning Society, San Diego, CA, 2015, pp.1-
15. 

[22]   Krizhevsky, A., Sutskever, I., and G. E. Hinton, G. E., ‚ÄúImageNet classi-
fication with deep convolutional neural networks,‚Äù Advances in Neural 
Information Processing Systems, Vol. 25, No. 2, 2012, pp. 1106-1114. 
doi:10.1145/3065386. 

Future developments include testing the NNs on real imagery 

to investigate domain gap. 

References 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

 Sharma, S., Ventura, J., and D‚ÄôAmico, S., ‚ÄúRobust Model-Based Monoc-
ular  Pose  Initialization  for  Noncooperative  Spacecraft  Rendezvous.‚Äù 
Journal of Spacecraft and Rockets, Vol. 55, No. 6, 2018, pp. 1414‚Äì1429. 
doi: doi.org/10.2514/1.A34124 
 Capuano, V., Cuciniello, G.,  Pesce, V., Opromolla, R., Sarno,  S.,  Lav-
agna, M., Grassi,  M., Corraro,  F.,  Capuano, G., Tabacco, P., Meta,  F., 
Battagliere, M., and Tuozzi, A., ‚ÄúVINAG: A highly integrated system for 
autonomous on-board absolute and relative spacecraft navigation,‚Äù The 
4S Symposium 2018. 
 Sharma, S., Beierle, C., and D'Amico, S., ‚ÄúPose estimation for non-coop-
erative  spacecraft  rendezvous  using  convolutional  neural  networks,‚Äù 
2018 IEEE Aerospace Conference, IEEE, Big Sky Montana, 2018, pp. 1-
12. 
 Sharma,  S.,  and  D‚ÄôAmico,  S.,  ‚ÄúPose  Estimation  for  Non-Cooperative 
Rendezvous  Using  Neural  Networks,‚Äù  2019  AAS/AIAA  Astrodynamics 
Specialist Conference, AAS/AIAA, Portland, 2019, AAS 19-350, pp. 1-
20. 
 Chen, B.,  Cao, J., Parra, A., and Chin, T. J., ‚ÄúSatellite pose estimation 
with deep landmark regression and nonlinear pose refinement,‚Äù Proceed-
ings  of  the  IEEE/CVF  International  Conference  on  Computer  Vision 
Workshops, 
IEEE  CVF,  Seoul,  2019,  pp.  2816-2824.  doi: 
10.1109/ICCVW.2019.00343 
 Park, T.H., Sharma, S. and D‚ÄôAmico, S., ‚ÄúTowards robust learning based 
pose estimation of noncooperative spacecraft,‚Äù 2019 AAS/AIAA Astrody-
namics  Specialist  Conference,    AAS/AIAA,  Portland,  2019,  AAS  19-
840, pp. 1-20. 
 Proen√ßa, P. F., and Gao, Y., ‚ÄúDeep Learning for Spacecraft Pose Estima-
tion  from  Photorealistic  Rendering,‚Äù  2020  IEEE  International  Confer-
ence on Robotics and Automation, IEEE ICRA, (virtual), 2020, pp. 6007-
6013. doi: 10.1109/ICRA40945.2020.9197244 
 Hu, Y., Speierer, S., Jakob, W., Fua, P., and Salzmann, M., ‚ÄúWide-depth-
range 6d object pose estimation in space,‚Äù Proceedings of the IEEE/CVF 
Conference  on  Computer  Vision  and  Pattern  Recognition,  IEEE  CVF, 
Rockville 
doi: 
10.1109/CVPR46437.2021.01561 
 Wang, Z., Zhang, Z., Sun, X., Li, Z., and Yu, Q., ‚ÄúRevisiting Monocular 
Satellite Pose Estimation with Transformer.‚Äù IEEE Transactions on Aer-
ospace 
doi: 
Systems, 
doi.org/10.1109/TAES.2022.3161605 

15870-15879. 

Electronic 

2022, 

2021, 

MD, 

1‚Äì1. 

and 

pp. 

pp. 

[10]   Piazza,  M., Maestrini,  M., and Di Lizia,  P., ‚ÄúMonocular Relative Pose 
Estimation Pipeline for Uncooperative Resident Space Objects.‚Äù Journal 
Information  Systems,  2022,  pp.  1‚Äì20.  doi: 
of  Aerospace 
doi.org/10.2514/1.I011064 

[11]   Carcagn√¨, P., Leo, M., Spagnolo, P., Mazzeo, P. L., and Distante, C., ‚ÄúA 
Lightweight Model for Satellite Pose Estimation.‚Äù In Image Analysis and 
Processing ‚Äì ICIAP 2022 (S. Sclaroff, C. Distante, M. Leo, G. M. Fari-
nella,  and  F.  Tombari,  eds.),  Springer  International  Publishing,  Cham, 
2022, pp. 3‚Äì14. 

[12]   Posso, J., Bois, G., and Savaria, Y., ‚ÄúMobile-URSONet: An Embeddable 
Neural Network for Onboard Spacecraft Pose Estimation.‚Äù Accepted at 
IEEE ISCAS 2022. http://arxiv.org/abs/2205.02065. 

[13]   Black, K., Shankar, S., Fonseka, D., Deutsch, J., Dhir, A., and Akella, M. 
R., ‚ÄúReal-Time, Flight-Ready, Non-Cooperative Spacecraft Pose Estima-
tion Using Monocular Imagery,‚Äù 31st. AAS/AIAA Space Flight Mechan-
ics Meeting, AAS/AIAA, (virtual), 2021, AAS 21-283, pp. 1-16. 
[14]   Schubert, C., Black, K., Fonseka, D., Dhir, A., Deutsch, J., Dhamani, N., 
Martin, G., and Akella, M., ‚ÄúA Pipeline for Vision-Based On-Orbit Prox-
imity  Operations  Using  Deep  Learning  and  Synthetic  Imagery,‚Äù  2021 
IEEE Aerospace Conference, Big Sky, Montana, IEEE, pp. 1-15. 
[15]   Kisantal,  M.,  Sharma,  S.,  Park,  T.  H.,  Izzo,  D.,  M√§rtens,  M.,  and 
D‚ÄôAmico, S., ‚ÄúSatellite pose estimation challenge: Dataset, competition 
design,  and  results,‚Äù IEEE  Transactions  on  Aerospace  and  Electronic 
Systems, Vol. 
doi: 
10.1109/TAES.2020.2989063  

4083-4098. 

56,  No. 

2020, 

11, 

pp. 

[16]   Goodwill, J., Crum, G., MacKinnon, J., Brewer, C., Monaghan, M., Wise, 
T., and Wilson, C., ‚ÄúNASA SpaceCube Edge TPU SmallSat Card for Au-
tonomous Operations and Onboard Science-Data Analysis,‚Äù Proceedings 
of the Small Satellite Conference, AIAA, Utah, 2021, No. SSC21-VII-08. 

 
 
