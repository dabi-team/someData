2
2
0
2

g
u
A
7
1

]
S
M

.
s
c
[

1
v
0
3
5
8
0
.
8
0
2
2
:
v
i
X
r
a

Survey of Methods for Solving Systems of Nonlinear Equations, Part I:
Root-ﬁnding Approaches

Ilias S. Kotsireas
ikotsire@wlu.ca
Wilfrid Laurier University
Canada

Panos M. Pardalos
ppardalos@toxeus.org
Toxeus Systems LLC
Orlando, Florida, USA

Alexander Semenov
asemenov@uﬂ.edu
University of Florida
Gainesville, Florida, USA

William T. Trevena
wtrevena@uﬂ.edu
University of Florida
Gainesville, Florida, USA

Michael N. Vrahatis
vrahatis@math.upatras.gr
University of Patras
Patras, Greece

August 19, 2022

Abstract

This paper presents a comprehensive survey of methods which can be utilized to search for solutions to
systems of nonlinear equations (SNEs). Our objectives with this survey are to synthesize pertinent literature
in this ﬁeld by presenting a thorough description and analysis of the known methods capable of ﬁnding one or
many solutions to SNEs, and to assist interested readers seeking to identify solution techniques which are well
suited for solving the various classes of SNEs which one may encounter in real world applications.

To accomplish these objectives, we present a multi-part survey. In part one, we focus on root-ﬁnding ap-
proaches which can be used to search for solutions to a SNE without transforming it into an optimization
problem. In part two, we will introduce the various transformations which have been utilized to transform a
SNE into an optimization problem, and we discuss optimization algorithms which can then be used to search
for solutions. In part three, we will present a robust quantitative comparative analysis of methods capable of
searching for solutions to SNEs.

Keywords: systems of nonlinear equations, localization of zeros, computation of roots, topological degree, total
number of solutions and extrema, interval methods, symbolic computation, tensor methods, homotopy methods

1

Introduction

This paper presents part one of a survey on methods for ﬁnding one or many solutions to a system of nonlinear
equations (SNE):

Fm(x) = Θm ≡ (0, 0, . . . , 0)(cid:62) ⇐⇒

(1)






f1(x1, x2, . . . , xn) = 0,

f2(x1, x2, . . . , xn) = 0,
...
fm(x1, x2, . . . , xn) = 0,

where Fm = (f1, f2, . . . , fm) : Dn ⊂ Rn → Rm, where f1, f2, . . . , fm are real-valued continuous or continuously
diﬀerentiable functions on the domain Dn, and where at least one of f1, f2, . . . , fm is nonlinear. For example,
consider the system of transcendental equations

F2(x) = Θ2 ≡ (0, 0)(cid:62) ⇐⇒

(cid:40)f1(x1, x2) = x1 − x1 sin(x1 + 5x2) − x2 cos(5x1 − x2) = 0,

f2(x1, x2) = x2 − x2 sin(5x1 − 3x2) + x1 cos(3x1 + 5x2) = 0,

(2)

which is comprised of two transcendental equations of two unknowns (See Figure 1).

Finding one or more solutions to a SNE is a challenging and ubiquitous task faced in many ﬁelds including
chemistry [1, 2, 3], chemical engineering [4], automotive steering [5], power ﬂow [6, 7], large-scale integrated circuit

1

 
 
 
 
 
 
Figure 1: An example of a SNE with two transcendental equations of two unknowns as introduced by Eq. (2):
(Blue): f1(x1, x2) = x1 − x1 sin(x1 + 5x2) − x2 cos(5x1 − x2) = 0;
(Red): f2(x1, x2) = x2 − x2 sin(5x1 − 3x2) +
x1 cos(3x1 + 5x2) = 0. Solutions to this SNE are deﬁned as the points where the blue and red contours intersect.
Finding all of the points within a certain region which satisfy both equations is a challenging task.

designs [8], climate modeling [9], materials engineering [10], robotics [11, 12, 13, 14], nuclear engineering [15],
image restoration [16], protein interaction networks [8], neurophysiology [17], economics [18], ﬁnance [19], applied
mathematics [20], physics [21], ﬁnding string vacua [22], machine learning [23, 24], geometric constraint solving
(used in computer aided design) [25], and geodesy [26, 27] among others. The problem of solving even a system
of polynomial equations has been proven to be NP-hard [28]. Furthermore, it has also been proven [29] that no
general algorithm exists for determining whether an integer solution exists for a polynomial equation with a ﬁnite
number of unknowns and only integer coeﬃcients. The latter has been known as Hilbert’s 10th problem.

1.1 Notation / Scientiﬁc Style
Throughout this paper, we utilize x = (x1, x2, . . . , xn)(cid:62) ∈ D ⊂ Rn to denote a real vector within the bounded
n)(cid:62) ∈ D ⊂ Rn to denote a real solution to a SNE such that all
1, x∗
domain D. Furthermore, we utilize x∗ = (x∗
n)(cid:62) ∈
equations in the SNE are satisﬁed (Fm(x∗) = 0). In an iterative method, we utilize xk = (xk
D ⊂ Rn for k = 0, 1, . . . to denote the vector found during the k−th iteration of the iterative method. Here, xk
i
denotes the i−th coordinate of the vector xk.

2, . . . , xk

2, . . . , x∗

i , . . . xk

1, xk

1.2 Terminology

Although we refer to Eq. (1) as a system of nonlinear equations (SNE), such systems have been referred to in a
variety of diﬀerent ways in literature. For example, articles [30, 31, 32, 33, 34, 35] utilize the abbreviation “SNLE”
to refer to a system of nonlinear equations, and article [36] uses the abbreviation “SoNE”. Other papers refer to
Eq. (1) as a nonlinear system of equations, and use the abbreviations “NSE” [37, 38] and “NLS” [39]. Eq. (1) has
also been referred to as a nonlinear equation system (NES) [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54].
The survey in article [55] uses the terminology Nonlinear Equations (NEs) to refer to a system of one or more
nonlinear equations.

When m > n, a SNE can be referred to as an overdetermined SNE, and when n > m, a SNE can be referred to
as an underdetermined SNE. When m = n, a SNE can be referred to as a square SNE [56]. Furthermore, a SNE is
considered to be consistent if a solution exists which satisﬁes all equations [25].

For a square]/ SNE, a solution x∗ = (x∗

1, x∗

2, . . . , x∗

n)(cid:62) of the SNE Fn(x) = Θn or equivalently a zero x∗ of

2

the function Fn(x) or a root x∗ of the function Fn(x) is called simple if for the determinant of the corresponding
Jacobian matrix:

JFn (x) ≡ F (cid:48)

n(x)ij ≡

(cid:27)

(cid:26) ∂fi(x)
∂xj

ij

≡














∂f1(x)
∂x1

∂f2(x)
∂x1
...
∂fn(x)
∂x1

∂f1(x)
∂x2

∂f2(x)
∂x2
...
∂fn(x)
∂x2

· · ·

· · ·
. . .

· · ·














,

∂f1(x)
∂xn

∂f2(x)
∂xn
...
∂fn(x)
∂xn

(3)

at x∗ it holds that det JFn (x∗) (cid:54)= 0, otherwise it is called multiple. The problem of conservation and decomposition
of a multiple root into simple roots in the case of systems of homogeneous algebraic equations has been tackled
in [57]. This approach can be applied to high dimensional CAD where it is sometimes required to compute the
intersection of several hypersurfaces that are a perturbation of a set of original unperturbed hypersurfaces.

When Fn satisﬁes the monotonicity condition:

(cid:0)Fn(x) − Fn(y)(cid:1)(cid:62)

(x − y) (cid:62) 0, ∀ x, y ∈ Rn,

(4)

the corresponding SNE can be referred to as a system of monotone nonlinear equations [58]. Furthermore, Fn is
considered to be Lipschitz continuous if there exists L > 0 such that

(cid:107)Fn(x) − Fn(y)(cid:107)2 (cid:54) L(cid:107)x − y(cid:107)2, ∀ x, y ∈ Rn.
Many of the root ﬁnding methods described in Section 3 are guaranteed to converge to a solution when applied to
SNEs that satisfy both the monotonicity and Lipchitz continuity conditions (the hybrid spectral methods introduced
in [59] for example).

(5)

1.3 Comparison to other surveys

Other surveys discussing solution techniques for SNEs include [60] and [55]. We have decided to conduct this
comprehensive literature review because many new solution techniques for SNEs have been introduced since the
publication of [60] in 1994, and because the recent survey presented in [55] focuses mainly on methods which ﬁrst
convert a SNE into an optimization problem, and then search for multiple solutions to the optimization problem
using Intelligent Optimization Algorithms (IOAs). The IOAs discussed in article [55] are primarily metaheuristics
for global optimization. Although the survey in article [55] provides a very nice discussion of IOAs for solving SNEs
reformulated as optimization problems, many of the IOAs they discuss are only introduced at a very high level,
only eight IOAs were tested in their computational study, and the IOAs were evaluated on SNEs comprised of 20
equations or less. Also, article [55] only brieﬂy mentions methods which can be used to search for solutions to SNEs
without transforming them into optimization problems.

We would like to present a broader survey which covers in detail the large set of methods which can be used
to solve a SNE without transforming it into an optimization problem (i.e. homotopy and symbolic computation
methods). These methods are our main focus in part one of this survey. In part two, we will expand upon article
[55] by introducing additional reformulation techniques and optimization algorithms which have been used to solve
SNEs, and by discussing in much more detail many optimization algorithms which were only brieﬂy introduced in
article [55]. This will allow us to appropriately set the stage for the comprehensive empirical study we will present
in part three of this survey. Furthermore, we believe it is imperative to introduce the reader to a technique for
determining the number of solutions to a SNE that exist within a bounded domain. Such techniques are of critical
practical importance for those interested in ﬁnding all solutions to a SNE that exist within a domain of interest.

1.4 Organization of this survey

We begin this survey by introducing a method which can be used to determine the total number of solutions to
a SNE that exist within a given bounded domain. By determining the number of solutions to a SNE which exist
within a bounded domain of interest, in the event that no solutions exist, one can avoid spending time and resources
searching for solutions all together. Alternatively, if one knows that a speciﬁc number of solutions exist to a SNE
within a bounded domain of interest, one can continue to search for solutions until the desired number of solutions
are found. Next, we introduce root-ﬁnding methods which have been utilized in literature to search for solutions
to a SNE without transforming it into an optimization problem. We conclude our paper by introducing additional
methods which have been used to attempt to solve SNEs, and by highlighting promising areas for future research.

3

2 Determining the number of solutions to a SNE in a bounded domain

The knowledge of all the solutions of a system of nonlinear equations and/or all the extrema of a function is of
major importance in various ﬁelds. The total number of the solutions of a system of nonlinear equations can be
obtained by computing the topological degree. Suppose that the function Fn = (f1, f2, . . . , fn) : Dn ⊂ Rn → Rn is
deﬁned and is two times continuously diﬀerentiable in a bounded domain Dn of Rn with boundary b(Dn). Suppose
further that the solutions of Fn(x) = Θn are not located on b(Dn), and that they are simple (that the determinant
of the Jacobian of Fn at these solutions is non-zero). Then the topological degree of Fn at Θn relative to Dn is
denoted by deg[Fn, Dn, Θn] and can be deﬁned by the following relation:

deg[Fn, Dn, Θn] =

(cid:88)

x∈F −1

n (Θn)

sgn det JFn(x),

where det JFn(x) denotes the determinant of the Jacobian matrix and sgn deﬁnes the three-valued sign function.
The above deﬁnition can be generalized when Fn is only continuous [61].

It is evident that, since deg[Fn, Dn, Θn] is equal to the number of simple solutions of Fn(x) = Θn which give
positive determinant of the Jacobian matrix, minus the number of simple solutions which give negative determinant
of the Jacobian matrix, then the total number N s of simple solutions of Fn(x) = Θn can be obtained by the value
of deg[Fn, Dn, Θn] if all these solutions have the same sign of the determinant of the Jacobian matrix. Thus, Picard
considered the following extensions of the function Fn and the domain Dn [62, 63]:

Fn+1 = (f1, f2, . . . , fn, fn+1) : Dn+1 ⊂ Rn+1 → Rn+1,

(6)

where fn+1 = y det JFn , Rn+1 : x1, x2, . . . , xn, y, and Dn+1 is the direct product of the domain Dn with an
arbitrary interval of the real y-axis containing the point y = 0. Then the solutions of the following system of
equations:

fi(x1, x2, . . . , xn) = 0,
y det JFn(x1, x2, . . . , xn) = 0,

i = 1, 2, . . . , n,

are the same simple solutions of Fn(x) = Θn provided that y = 0. Obviously, the determinant of the Jacobian
matrix obtained for the function (6) is equal to (det JFn(x))2 which is always positive. Thus, the total number N s
of the solutions of the system Fn(x) = Θn can be obtained by the following value of the topological degree:

N s = deg[Fn+1, Dn+1, Θn+1].

For example, in the one dimensional case, using the above Picard’s extensions it is proved that the total number
of simple solutions N s of the equation f (x) = 0, where f : (a, b) ⊂ R → R is twice continuously diﬀerentiable in a
predetermined interval (a, b), is given by the following relation [62, 63]:

N s = −

(cid:34)

ε

1
π

(cid:90) b

a

f (x) f (cid:48)(cid:48)(x) − f (cid:48)2(x)
f 2(x) + ε2f (cid:48)2(x)

dx + arctan

(cid:19)

(cid:18) εf (cid:48)(b)
f (b)

− arctan

(cid:18) εf (cid:48)(a)
f (a)

(cid:19)(cid:35)

,

(7)

where ε is a small positive constant. Note that N s was shown to be independent of the value of ε. Also, the above
approach can be applied for computing the number of multiple solutions. Obviously, the total number N e of the
extrema of f ∈ C 3 i.e. x ∈ (a, b) such that f (cid:48)(x) = 0 can be obtained using the above formula (7). For details of the
topological degree we refer the interested reader to the books [64, 65, 66, 67, 61, 68]. Details of the computation of
the value of the topological degree and its usefulness as well as some applications and issues related to the number
of zeros can be found for example in [69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83].

Article [84] also discusses utilizing the topological degree to determine the existence of robust solutions to a

SNE.

3 Root ﬁnding methods

This section discusses root ﬁnding methods which have been utilized to search for solutions to SNEs. Speciﬁcally,
Section 3.1 - Section 3.10 discuss methods which have been utilized to search for a single solution to a SNE, and
Section 3.11 discusses deﬂation techniques which can be used to compute further solutions. Some of the root ﬁnding
methods introduced in this section are discussed in more detail than others, and after we introduce fundamental
root ﬁnding methods, we place a particular emphasis on promising methods which do not appear to be widely

4

discussed in recent literature. We also brieﬂy introduce some of the more well known root ﬁnding methods such as
Quasi-Newton methods, and we cite relevant literature for the interested reader to refer to.

Although many of the root ﬁnding methods described in this section are guaranteed to converge to a solution
when applied to SNEs that satisfy certain conditions such as monotonicity (Eq. (4)) and Lipschitz continuity (Eq.
(5)), none of the methods described in this section are guaranteed to converge to a solution on all classes of SNEs.
For example, the root ﬁnding method proposed in article [58] was proven to converge on systems of monotone
nonlinear equations which satisfy additional regularity conditions, but the proposed method is not guaranteed to
converge on all SNEs.

Many of the root ﬁnding methods discussed in this section are specialized at solving particular classes of SNEs.
For example, Section 3.5 discusses a tensor-free Chebyshev-Halley method designed to solve SNEs that are large,
sparse, and which may have ill-conditioned or singular Jacobian matrices at a solution [85].

3.1 Classical Newton’s and Broyden’s methods for ﬁnding a single solution

Newton’s method [86, 87, 61, 88] for ﬁnding roots is a well known approach which can be utilized to ﬁnd a single
solution to a SNE. Starting at a “good” initial guess or approximate solution x0 = (x0
n)(cid:62) of a system of
n nonlinear real equations in n real unknowns, when Fn(x) is Lipschitz continuous and the Jacobian matrix JFn(x)
of Fn(x) is available, Newton’s method can be used to iteratively ﬁnd elements of a sequence converging towards a
n)(cid:62) with a quadratic rate of convergence. Newton’s method attempts to calculate
1, x∗
true solution x∗ = (x∗
a new approximate solution xk+1 from xk via

2, . . . , x∗

2, . . . , x0

1, x0

xk+1 = xk − JFn

(cid:0)xk(cid:1)−1

(cid:0)xk(cid:1),

Fn

k = 0, 1, 2, . . .

Alternatively, instead of calculating the inverse of the Jacobian matrix, one often can solve the system of linear
equations:

(cid:0)xk(cid:1),
to ﬁnd the Newton step sk = xk+1 − xk. Thus, Newton’s method using a initial guess x0 for each iteration
k = 0, 1, 2, . . . performs the following steps:

(cid:0)xk(cid:1) sk = −Fn

k = 0, 1, 2, . . .

JFn

(8)

(a) Solve: the system of linear equation JFn

(cid:0)xk(cid:1)sk = −Fn

(cid:0)xk(cid:1) for sk.

(b) Set: xk+1 = xk + sk.

In many cases, Step (b) is given by xk+1 = xk + λsk where λ is selected to guarantee decrease in (cid:107)Fn(cid:107) (see [87]).
Many expansions upon Newton’s method for ﬁnding roots have been introduced in literature, and most of these
expansions are focused on accelerating the rate of convergence. For example, articles [89, 90, 91, 92, 93, 94] present
new techniques based upon Newton’s method for ﬁnding roots and quadrature rules to accelerate convergence.
Newton’s method can also be augmented with higher order information to accelerate convergence towards an
approximation of a true solution x∗. Overall, Newton’s method for ﬁnding roots seeks to approximate the solution
of a nonlinear system of equations by solving iteratively a sequence of systems of linear equations. In cases where
the Jacobian matrix is singular, we refer the interested reader to articles [95, 96]. Convergence results for SNEs
which have a Jacobian with a constant rank in the vicinity of a solution are presented in articles [97, 98]. Dimension-
reducing modiﬁcations of Newton’s method that are able to tackle eﬃciently and eﬀectively, among others, almost
linear systems can be found in [99, 100, 101, 102, 103].

In a similar approach, Broyden’s method [104, 86, 87, 61, 88] using an initial guess x0 and an initial matrix B0

such that det B0 (cid:54)= 0, for each iteration k = 0, 1, 2, . . . performs the following steps:

(a) Solve: the system of linear equations Bksk = −Fn

(cid:0)xk(cid:1) for sk.

(b) Set: xk+1 = xk + sk.

(c) Set: yk = Fn

(cid:0)xk(cid:1).

(cid:0)xk+1(cid:1) − Fn
1
(cid:0)sk(cid:1)(cid:62)

(d) Set: Bk+1 = Bk +

(cid:0)yk − Bksk(cid:1)(cid:0)sk(cid:1)(cid:62)

.

sk
(cid:0)x0(cid:1). Obviously, by avoiding this choice Broyden’s method does not require
A good “choice” of B0 is B0 = JFn
the computation of the Jacobian matrix. The eﬀectiveness of Broyden’s method and Newton’s method on diﬀerent

5

classes of SNEs has been studied, for example, article [105] compares the eﬀectiveness of Newton’s method, Broyden’s
method, and other methods at solving sparse SNEs.

There are many techniques that can be utilized to solve the system of linear equations produced by each
iteration of Newton’s or Broyden’s method for ﬁnding roots. Two widely used methods for solving a system of
linear equations are described in the following sections, and we refer the interested reader to [61] for more methods
and details regarding these methods. For a visualization of the basins of convergence for Newton’s and Broyden’s
methods, we refer the interested reader to article [106].

3.1.1 Generalized Minimum Residual method (GMRES)

Consider a system of linear equations of the form Ax = b. The GMRES method [107] is an iterative method
which at the k−th iteration uses the Arnoldi iteration [108] to ﬁnd an approximate solution xk in the k−th Krylov
subspace Kk which minimizes the residual (cid:107)b − Axk(cid:107)2. The k-th Krylov subspace of the GMRES method is

Kk = Kk(A, r0) = span{r0, Ar0, A2r0, . . . , Ak−1r0},

where r0 = b − Ax0 is the residual at the initial point x0.

GMRES assumes that the matrix A is invertible (and hence square) and that b is normalized such that (cid:107)b(cid:107)2 = 1.
It is worth mentioning that because A is assumed to be invertible, that this technique in its original form can not be
utilized with Newton’s method to ﬁnd an approximate solution to a SNE that has more equations than unknowns
(m > n).

3.1.2 Successive Overrelaxation method (SOR)

The SOR method is another iterative method for solving a system of linear equations of the form Ax = b. If we
assume that the diagonal elements aii of A are all non-zero, at the k−th iteration we can utilize the SOR iteration
to ﬁnd a new approximate solution xk+1 to Ax = b by the following update:

xk+1 = xk − ω(D − ωL)−1(Axk − b),

where ω > 1 is a relaxation parameter, and A = D −L−U where D, L, and U are diagonal, strictly lower triangular,
and strictly upper triangular matrices respectively. Here, L and U have zero diagonal elements, and the assumption
that the diagonal elements of A are nonzero ensures that (D − L)−1 exists [61]. The Gauss-Seidel iteration is a
special case of the SOR where ω = 1.

Though the SOR method and the corresponding Jacobi method [61] can be utilized to solve the series of linear
systems produced by Newton’s method, the SOR and Jacobi methods can also be extended to ﬁnd a single solution
to a SNE.

3.2 Solving a SNE using generalizations of iterative methods for linear systems

The well known and widely used Gauss-Seidel iterative method for solving a linear system of equations of the form
Ax = b can be generalized for solving SNEs. Thus, if Fn = (f1, f2, . . . , fn) : D ⊂ Rn → Rn, then the basic step of the
nonlinear Gauss-Seidel (NGS) iteration is to solve (in analogy to linear case), the i−th nonlinear one-dimensional
equation:

, . . . , xk+1

i−1 , xi, xk

i+1, . . . , xk

fi(xk+1
1
for xi, and to set xk+1
= xi. To obtain xk+1 from xk, we solve successively the n one-dimensional nonlinear
equations (9) for i = 1, 2, . . . , n. If relaxation parameters ωk are used, we may set xk+1
i ) and
the corresponding method is called Nonlinear Successive Overrelaxation (NSOR) method. In an analogous way,
the k−th step of the Nonlinear Jacobi (NJ) iterative scheme consists of solving the i−th nonlinear one-dimensional
equation:

i + ωk(xi − xk

i = 1, 2, . . . , n.

k = 0, 1, . . . ,

n) = 0,

= xk

(9)

i

i

fi(xk

1, . . . , xk

i−1, xi, xk

i+1, . . . , xk

n) = 0,

k = 0, 1, . . . ,

i = 1, 2, . . . , n.

(10)

for xi and setting xk+1
can be easily parallelized.

i = xi, for i = 1, 2, . . . , n. The main characteristic of the nonlinear Jacobi approach is that it

In contrast to the linear case, in general, the analytic solutions of Eqs. (9) and (10) are not available and an
one-dimensional rootﬁnding method must be applied that terminates after a suitable number of steps. Any type

6

of one-dimensional methods can be used leading to a large variety of combined methods. On the other hand if
many steps of these one-dimensional methods are applied the whole procedure becomes cumbersome and thus in
practice, in many cases, a few steps or at least one step of these methods is applied. In this case, for example, if we
apply the one-dimensional Newton’s method, the derivative-free one-dimensional secant method or the derivative-
free one-dimensional Steﬀensen method to Eqs. (9) and (10), we obtain respectively the following methods for
solving SNEs [61]:

(a) the one-step SOR-Newton method:

xk+1
i = xk

i − ωk

fi(xk,i)
∂ifi(xk,i)

,

k = 0, 1, . . . ,

i = 1, 2, . . . , n.

where xk,i = (xk+1
, xk+1
i , xk
2
tion fi(x) with respect to the variable xi.

, . . . , xk+1

i−1 , xk

1

i+1, . . . , xk

n)(cid:62) and where ∂ifi(x) denotes the partial derivative of the func-

(b) the one-step Jacobi Newton method:

xk+1
i = xk

i − ωk

fi(xk)
∂ifi(xk)

,

k = 0, 1, . . . ,

i = 1, 2, . . . , n.

(c) the derivative-free one-step SOR secant method:

xk+1
i = xk

i − ωk

fi(xk,i) − fi

i − xk−1
xk
i
(cid:0)xk,i + (xk−1

i − xk

i )ei (cid:1) fi

(cid:0)xk,i(cid:1),

k = 0, 1, . . . ,

i = 1, 2, . . . , n,

where ei indicates the i-th column of the identity matrix In.

(d) the derivative-free one-step Jacobi secant method:

xk+1
i = xk

i − ωk

fi(xk) − fi

i − xk−1
xk
i
(cid:0)xk + (xk−1

i − xk

i )ei (cid:1) fi(xk),

k = 0, 1, . . . ,

i = 1, 2, . . . , n.

(e) the derivative-free one-step SOR Steﬀensen method:

xk+1
i = xk

i − ωk

(cid:0)xk,i(cid:1)2
fi
(cid:0)xk,i − fi(xk,i)ei (cid:1) ,
fi(xk,i) − fi

k = 0, 1, . . . ,

i = 1, 2, . . . , n.

(f) the derivative-free one-step Jacobi Steﬀensen method:

xk+1
i = xk

i − ωk

(cid:0)xk(cid:1)2
fi
(cid:0)xk − fi(xk)ei (cid:1) ,
fi(xk) − fi

k = 0, 1, . . . ,

i = 1, 2, . . . , n.

(11)

Furthermore, since in many problems of practical interest the functions values are known only imprecisely, the
traditional and widely applied one-dimensional bisection method can be used for a suitable number of steps for
computing solutions of Eqs. (9) and (10). Speciﬁcally, for the computation of a zero of a continuous function
f : [a, b] ⊂ R → R the one-dimensional bisection method has be given in [80] by the following sequence:

xk+1 = xk + sgnf (x0) sgnf (xk) (b − a)/2k+1,

x0 = a,

k = 0, 1, . . . .

Similarly, instead of the above sequence we can also use the following one:

xk+1 = xk − sgnf (x0) sgnf (xk) (b − a)/2k+1,

x0 = b,

k = 0, 1, . . . .

(12)

(13)

Obviously, the sequences (12) and (13) converge to a root r ∈ (a, b) if for some xk, k = 1, 2, . . .
it holds that
sgnf (x0) sgnf (xk) = −1. Also, the number of iterations ν, that are required in obtaining an approximate root r∗
such that |r − r∗| (cid:54) ε for some ε ∈ (0, 1) is given by:

ν = (cid:6)log2

(cid:0)(b − a) ε−1(cid:1)(cid:7).

(14)

7

The one-dimensional bisection method always converges within the given interval (a, b) and is a globally convergent
method. Furthermore, it has a great advantage since it is worst-case optimal; i.e., it possesses asymptotically the
best possible rate of convergence in the worst case [109, 68]. Thus, it is guaranteed to converge to an approximate
root with a predetermined accuracy using the predeﬁned number of iterations given by Eq. (14). Notice that no
other method has this characteristic. This method actually requires only the signs of function values (and the
gradient values for the optimization case) to be correct. Thus, it requires only one bit of information, namely the
sign of a function value and consequently it can be applied to problems with imprecise function values. Also, this
method can be generalized to tackle SNEs and optimization problems. In [80] a straightforward generalization of
the bisection method, named characteristic bisection method has been presented. For a few details of the bisection
and the generalized bisection methods and for some applications we refer the interested reader to [110, 111, 71, 74,
112, 79, 113, 80, 114, 82, 115, 116].

The main characteristic of all the above brieﬂy described methods in this section is that they can tackle n-
dimensional SNEs using only one-dimensional rootﬁnding methods. This issue is very important in cases where
the dimension n is large. Also, the corresponding algorithms are very simple to implement with a few lines of
code. Notice that, the convergence properties of all the above methods are well studied and analyzed (see for
example [61]). On the other hand, although the nonlinear iterative rootﬁnding methods have been extensively
studied, the optimization case has not been thoroughly studied and analyzed. Details and some applications of the
above described methods can be found for example in [117, 118, 119, 114, 120].

3.3 Chebyshev-Halley methods

Although Newton’s method only has a quadratic rate of convergence, Halley’s method possesses a cubic rate of
convergence. Halley’s method, also known as the method of tangent hyperbolas, utilizes information from the tensor
of second derivatives of Fn(x) to accelerate convergence towards a local minimizer x∗ of Fn(x) from an initial guess
x0. Halley’s method attempts to calculate a new approximate solution xk+1 to a SNE from xk using the following
scheme:
LFn (xk) (cid:2)I − αLFn (xk)(cid:3)−1(cid:111)
n (x)F (cid:48)

n(x)−1Fn(x) is the degree of logarithmic convexity

where α is a real parameter, and LFn (x) = F (cid:48)
[121, 122, 123]. Therefore, Halley’s method is applicable to cases where F (cid:48)(cid:48)

n(xk)−1Fn(xk),
F (cid:48)

xk+1 = xk −

n(x)−1F (cid:48)(cid:48)

n (x) can be computed.

Chebyshev’s method refers to the case where α = 0, the classical Halley’s method refers to the case where α = 1
2 ,
and the super-Halley method refers to the case where α = 1. Details about Chebyshev-like methods for solving SNEs
can be found in [124].

I +

1
2

(cid:110)

3.4 Tensor methods utilizing higher order derivatives

There exists a class of tensor methods which utilizes information from higher order derivatives to accelerate con-
vergence to a single solution of a SNE. In [125], Steihaug and Suleiman utilize the model:

(cid:107)M k(d)(cid:107)2 =

(cid:13)
(cid:13)Fn(xk) + JFn(xk)d +
(cid:13)

1
2

T kdd

(cid:13)
(cid:13)
(cid:13)2

(cid:54) ηk(cid:107)Fn(xk)(cid:107)2.

(15)

which seeks to determine a step dk at each iteration. In this model, T k = F (cid:48)(cid:48)
n (xk) is the tensor of second derivatives
of Fn(xk), and ηk ∈ [0, 1). Steihaug and Suleiman proved that any method that can be used to ﬁnd dk in Eq. (15)
while satisfying (cid:107)dk(cid:107)2 = O((cid:107)Fn(xk)(cid:107)2) is locally convergent. Furthermore, Steihaug and Suleiman showed that the
rate of convergence is at least:

(a) Q-super-linear when ηk → 0.

(b) Q-quadratic when ηk = O((cid:107)Fn(xk)(cid:107)2).

(c) Q-cubic when ηk = O((cid:107)Fn(xk)(cid:107)2
(d) Q-order min(cid:8)ˆp, 3(cid:9) when ηk = O((cid:107)Fn(xk)(cid:107) ˆp−1

2).

2

), 1 < ˆp.

Furthermore, in order to solve Eq. (15), Steihaug and Suleiman in [125] introduce a class of inexact Chebyshev-Halley
methods which under some assumptions are locally convergent satisfying Eq. (15) and (cid:107)dk(cid:107)2 = O((cid:107)Fn(xk)(cid:107)2).

8

3.5 Tensor-free Chebyshev-Halley method

Instead of calculating the tensor term T kdd as performed in Eq. (15), Eustaquio et al. in [85] avoid calculating the
tensor term altogether by introducing a general tensor-free Chebyshev-Halley method for solving SNEs. Therefore,
though Eq. (15) requires calculating F (cid:48)(cid:48)
n (x)
does not exist. Also, this method does not require the inequality in Eq. (15) to be satisﬁed. The general framework
for the inexact tensor-free Chebyshev-Halley class is the following:

n (x), the method of Eustaquio et al. can be utilized in cases where F (cid:48)(cid:48)

1. Given the SNE Fn(x) = Θn, the machine precision εM , and k = 0, select α ∈ R, a step length h > 0, an

initial point x0 ∈ Rn, a forcing term limit ˜η ∈ (0, 1), and a mapping C : Rn → Rn×n.

2. while Fn(xk) (cid:54)= Θn do

1 ∈ [0, ˜η) and ηk

(a) Select forcing term tolerances ηk
2 ∈ [0, ˜η).
(cid:13)Fn(xk)(cid:13)
(cid:13)
(b) Compute d1 such that (cid:13)
(cid:13)JFn (xk)d1 + Fn(xk)(cid:13)
(cid:54) ηk
(cid:13)2.
(cid:13)2
1
(cid:13)
(c) Compute d2 such that (cid:13)
(cid:0)JFn(xk) + α C(xk)(cid:1)d2 + 1
2 C(xk)d1
(cid:13)2
(cid:13)
(d) Set xk+1 = xk + d1 + d2.

(cid:54) ηk
2

(cid:13)
(cid:13) 1
2 C(xk)d1

(cid:13)
(cid:13)2.

(e) Set k = k + 1.

3. end while

(cid:14)(cid:107)d1(cid:107)2 ∈
where the authors utilized the mapping C(xk) = 1
h
(cid:9) ∈
(εM , 0.5) and the forcing terms ηk
(2εM , 10−8] to control the level of accuracy of the approximate solution to the SNE. In comparison to Eq. (15),
the mapping C(xk) replaces the tensor term T kd while still preserving a third-order rate of convergence. As shown
in the algorithm above, each iteration only requires approximately solving two linear systems. Eustaquio et al.
utilized Saad and Schultz’s Generalized Minimum Residual method (GMRES) [107] to solve the two linear systems
and obtain the inexact Newton steps dk

(cid:0)JFn (xk+hd1)−JFn (xk)(cid:1) with step length h =
(cid:9) ∈ (2εM , 10−8] and ηk

√
2 = min(cid:8)10−8, (cid:107)Fn(xk)(cid:107)2

1 = min(cid:8)10−8, (cid:107)Fn(xk)(cid:107)2

εM (cid:107)xk(cid:107)2

2

1 and dk
2.

The authors proved that any method belonging to their class of inexact tensor-free Chebyshev-Halley methods is
locally convergent. Note that the system solved in step 2(b) is analogous to system (8) solved in Newton’s method
where d1 = xk+1 − xk.

Eustaquio et al. conducted a rigorous quantitative comparative analysis of their method against the Inexact
Newton method and the tensor methods utilizing higher order derivatives presented by Steihaug and Suleiman
[125]. Eustaquio et al. illustrated that the class of tensor-free Chebyshev-Halley methods are capable of eﬃciently
handling problems that have ill-conditioned or singular Jacobian matrices at the solution.

Furthermore, Jarrat’s method [126] is deﬁned as a speciﬁc case of the tensor-free Chebyshev-Halley method
2 = 0, h = 2/3, and the mapping C(xk) = h−1(cid:0)JFn (xk + h d1) − JFn (xk)(cid:1). Jarrat’s method
where α = 1, ηk
has a convergence rate of four. Articles using Jarratt-like methods to solve SNEs can be found for example in
[30, 127, 128].

1 = 0, ηk

3.6 Tensor methods not utilizing higher order derivatives

In addition to the class of tensor methods which utilize a tensor of higher order derivatives, other tensor methods
exist and have demonstrated eﬀectiveness at solving large, sparse, and ill-formed SNEs with singular Jacobian
matrices [129, 130, 131, 132, 133, 134, 135, 136, 137]. In [129] Frank and Schnabel introduced tensor strategies for
solving SNEs based around solving the quadratic model:

min
d∈Rn

(cid:107)M k(d)(cid:107)2 = min
d∈Rn

(cid:13)
(cid:13)Fn(xk) + JFn (xk)d +
(cid:13)

1
2

T kdd

(cid:13)
(cid:13)
(cid:13)2

,

(16)

where JFn(x) is the Jacobian matrix of Fn(x), d is the step size, and T k ∈ Rn×n×n is a carefully chosen tensor.

Expanding upon this formulation, in [137], Bader presents three Krylov-based methods for iteratively solving
n function

Eq, (16) to a speciﬁed tolerance. Bader’s method selects T k such that the model interpolates p (cid:54)
values from its most recent history of iterates. By selecting p = 1, Bader reduces the tensor model about xk to

√

min
d∈Km

(cid:107)M k(d)(cid:107)2 = min
d∈Km

(cid:13)
(cid:13)Fn(xk) + JFn (xk)d +
(cid:13)

1
2

ak(cid:0)(sk)(cid:62)d(cid:1)2(cid:13)
(cid:13)
(cid:13)2

,

(17)

9

where Km is a m-dimensional Krylov subspace, and where ak, sk ∈ Rn are given as follows:

ak =

2(cid:0)Fn(xk−1) − Fn(xk) − JFn (xk)sk)
((sk)(cid:62)sk(cid:1)2

,

sk = xk−1 − xk.

The linear Krylov subspace method ﬁnds an approximate solution xm to the linear system Ax = b from an m-
dimensional aﬃne subspace x0 + Km where

Km(A, r0) = span{r0, Ar0, A2r0, . . . , Am−1r0},

r0 = b − Ax0,

and r0 is the residual at the initial point x0.

The three techniques presented by Bader in [137] select Km in diﬀerent ways as a subroutine within the following

algorithm:

1. Given the SNE Fn(x), select an initial point x0 and a maximum number of iterations kmax.

2. For k = 0, 1, 2, . . . , kmax, do:

(a) Select a forcing term tolerance ηk ∈ [0, 1).

(b) If k = 0:

i. Calculate the Newton-GMRES [107] step dN based on the tolerance ηk.
ii. Proceed to step 2e.

(c) Form the local tensor model (16).
(d) Calculate the approximate tensor step dT according to ηk by solving one of the three methods presented

for selecting Km.

(e) Set xk+1 = xk + αd where a linesearch strategy using the directions dT and / or dN is used to select d

and α.

(f) If xk+1 is an acceptable approximate root of Fn(x):

i. Stop.

Bader’s methods demonstrated eﬀectiveness at solving large-scale SNEs, especially those which have ill-conditioned

or singular Jacobians at the solution.

3.7 Quasi-Newton method for ﬁnding roots

Although Newton’s method for ﬁnding roots requires the Jacobian JFn(x), if JFn (x) is not available, Quasi-Newton
methods can be utilized. A Quasi-Newton method is any method that utilizes an approximation of JFn (x) instead of
the exact JFn (x). A classiﬁcation of Quasi-Newton methods for solving SNEs is presented in article [138]. Examples
of Quasi-Newton methods include the methods discussed in articles [139, 140].

One of the most popular Quasi-Newton methods is the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method
[141, 142, 143, 144]. Popular extension of BFGS method is limited memory BFGS (L-BFGS) [145]. Article [58]
combines a memoryless variant of the BFGS method with a projection technique for solving SNEs, and the authors
demonstrated the eﬃciency of the method by solving large SNEs with up to 5000 dimensions in the order of
milliseconds or seconds, depending on the problem instance. Other Quasi-Newton methods include the Symmetric
Rank 1 (SR1) method and the Davidon–Fletcher–Powell (DFP) method among others. Quasi-Newton methods
were utilized to ﬁnd solutions to SNEs in articles [146, 147, 148, 149, 150, 151, 152, 153, 154].

3.8 Spectral methods

Spectral methods are another class of derivative-free methods which have been used to solve SNEs, and they are
particularly well suited for solving large SNEs because they have a relatively low storage requirement [59]. Spectral
methods often utilize spectral parameters and coeﬃcients to help determine the search direction dk in the update
step xk+1 = xk + αkdk. For example, article [59] presents two derivative-free hybrid spectral methods for solving
SNEs, and proves that the proposed methods will converge to a solution when applied to SNEs that satisfy the
conditions of monotonicity (Eq. (4)) and Lipschitz continuity (Eq. (5)). Other articles which propose applying
spectral methods for solving SNEs include [155, 156].

10

3.9 Hermitian and skew-Hermitian splitting (HSS) based methods

First introduced in [157] for solving linear systems of equations, Hermitian and skew-Hermitian splitting (HSS)
based methods have been shown to be eﬀective at solving sparse SNEs [158] and SNEs which can be decomposed
into a linear part Ax and a nonlinear part ζ(x) such that Fm(x) = Ax − ζ(x) [159]. When the linear part Ax is
dominant over the nonlinear part ζ(x), Fm(x) can be referred to as a weakly nonlinear system. By separating the
linear and nonlinear elements of Fm(x), certain specialized techniques can potentially be utilized to ﬁnd solutions to
Fm(x) more quickly than by utilizing traditional root ﬁnding methods. Some of these techniques are derivative-free,
including the derivative-free HSS based method introduced in [160] which is guaranteed to converge to a solution
on SNEs that satisfy certain conditions.

3.10 Levenberg-Marquardt method

The Levenberg-Marquardt algorithm [161, 162] was designed to solve a nonlinear least squares problem that can
i=1[fi(x)]2 where ϕ : Rn → R. The Levenberg-Marquardt algorithm is an
be in general expressed as ϕ(x) = 1
2
iterative procedure which at each iteration calculates an updated solution using the rule xk+1 = xk + λd, where the
search direction d is found by solving equations of the form

(cid:80)m

(cid:0)J (cid:62)
Fm

JFm + µ In

(cid:1)d = −J (cid:62)
Fm

Fm,

where JFm ∈ Rm×n is the Jacobian matrix of Fm, µ ∈ R, In is the identity matrix, and Fm : Rn → Rm. In article
[163], Fletcher proposed the following modiﬁcation to make the solution scale invariant:

(cid:0)J (cid:62)
Fm

JFm + µ diag{J (cid:62)
Fm

JFm }(cid:1)d = −J (cid:62)

Fm

Fm,

where the identity matrix In is replaced with the diagonal matrix consisting of the diagonal elements of J (cid:62)
JFm .
Fm
Article [164] proposed to use the Levenberg-Marquardt method for solving a SNE modeling a port mooring structure.
The Levenberg-Marquardt algorithm is widely used in many applications, including artiﬁcial neural network training
(for example, see article [165]).

3.11 Deﬂation techniques for the computation of further solutions

As illustrated above, there is a plethora of methods for obtaining a single solution of a system of n nonlinear real
equations in n real unknowns. Brown and Gearhart in [166] proposed deﬂation techniques for the computation
of further solutions of a system of nonlinear equations. Speciﬁcally, these techniques proceed as follows: “Once a
solution of a system of nonlinear equations has been obtained a modiﬁed system is formed in such a way that it
retains those solutions of the original system which remain to be computed except the solutions that has been already
computed”. This procedure may be applied sequentially until all solutions of the original system are obtained.

For example, assume a system of nonlinear equations Fn(x) = Θn, where Fn = (f1, f2, . . . , fn) : Dn ⊂ Rn → Rn,
then to deﬂate out the p already computed roots r1, r2, . . . , rp in order to compute additional roots, the following
norm deﬂated function [166] is applied:

ˆfi(x) =

1
j=1 (cid:107)x − rj(cid:107)

(cid:81)p

fi(x),

i = 1, 2, . . . , n,

or alternatively the following inner product deﬂated function [166] can be used:

˜fi(x) =

(cid:81)p

j=1

1

(cid:10)∇fi(rj), (x − rj)(cid:11) fi(x),

i = 1, 2, . . . , n.

The above inner product deﬂated function has proven to be useful in practice when Newton’s method for root
ﬁnding is used. The deﬂation techniques can also be extended for the computation of multiple solutions. A study
of the deﬂation techniques for the one dimensional case can be found in article [167]. Also, additional techniques
for the optimization case related to the above deﬂation techniques as well as some applications can be found for
example in the papers [168, 169, 170, 171, 172, 173].

11

4 Symbolic computation methods

Exact algorithmic methods for solving systems of nonlinear polynomial equations have been developed in the
realm of Symbolic Computation, also called Computational Algebra, or Computer Algebra. Broadly speaking, this
particular subarea of Symbolic Computation can in fact be interpreted as a constructive version of some parts of
the well-established mathematical theories of Commutative Algebra and Algebraic Geometry. We begin this section
by a brief presentation (via examples) of the main Symbolic Computation methods for solving systems of nonlinear
polynomial equations, namely resultants and Gr¨obner bases. We end this section by citing a number of books that
discuss extensively all the developments in the area of Symbolic Computation exact methods for solving systems of
nonlinear polynomial equations.

4.0.1 Resultants

Historically, the theory of resultants was developed with the aim to provide a systematic means of elimination of
variables/unknowns from a system of nonlinear polynomial equations.

The resultant of two univariate polynomials of degrees m, n respectively,

p(x) = pmxm + · · · + p1 + p0,

q(x) = qnxn + · · · + q1 + q0,

is deﬁned as the determinant of the (n + m) × (n + m) Sylvester matrix associated to p(x) and q(x), namely:

resx

(cid:0)p(x), q(x)(cid:1) = det















am am−1
0
0
0
bn
0
0
0

am
0
0
bn−1
bn
0
0















.

am−1
am
0

bn−1
bn
0

am−1
am

bn−1
bn

One of the main properties of the resultant is that it is equal to zero, if and only if the two polynomials have a
common root.

The resultant of two bivariate polynomials p = p(x, y), q = q(x, y) with respect to the variable x, or with respect
to the variable y, is the determinant of their associated Sylvester matrix, when p and q are considered as polynomials
in x, or in y respectively.

Let us illustrate the concept of the resultant by a simple but instructive example.

Example 1 Consider the system of two polynomials f1(x1, x2) = 0, f2(x1, x1) = 0 in two variables x1, x2, given
by:

f1(x1, x2) = x2
2 − 1,
f2(x1, x2) = x1 − x2 − 1.
The resultant of f1, f2, with respect to x1 is the determinant of their 3 × 3 Sylvester matrix when considered as
polynomials in x1, namely, f1 = 1 · x2

2 − 1), f2 = 1 · x1 + (−x2 − 1),

1 + 0 · x1 + (x2

1 + x2

(18)

resx1(f1, f2) = det







1

0

x2

2 − 1

1 −x2 − 1

0

0

1

−x2 − 1







= 2x2

2 + 2x2.

By factorizing the resultant and setting it equal to zero, we obtain x2 = 0 and x2 = −1. By substituting each of
these two values of x2 back into (18), we obtain the corresponding values of x1 as: x1 = 1 and x1 = 0. Therefore,
we conclude that the system (18) possesses the two solutions:

(x1 = 1, x2 = 0), (x1 = 0, x2 = −1),

which can easily be veriﬁed to be correct. In terms of geometric interpretation, the ﬁrst equation f1(x1, x2) represent
the unit circle and the second equation f1(x1, x2) represents a straight line, that intersects the unit circle. The two
solutions of the system (18) are the two points of intersection of the (red) unit circle and this (green) straight line
in Figure 2 below.

12

Figure 2: Geometric interpretation of System (18)

4.0.2 Gr¨obner bases

The theory of Gr¨obner bases was developed as a multivariate analogue of the classical Gaussian elimination for
systems of linear equations. Again we illustrate the concept with an example and refer to the aforementioned books
for additional details and technicalities.

Example 2 Consider the system of three polynomials f1(x1, x2, x3) = 0, f2(x1, x2, x3) = 0, f3(x1, x2, x3) = 0 in
three variables x1, x2, x3, given by:

f1(x1, x2, x3) = x2
f2(x1, x2, x3) = x1x3 + x2
f3(x1, x2, x3) = x1x2 + x2

1 + x2x3 − 2,
2 − 3,
3 − 5.

(19)

The lexicographical Gr¨obner basis of the ideal generated by these three polynomials, with respect to the lexico-
graphical ordering induced by x2 > x1 > x3 is given by the three polynomials:

8x8
88x7
152x7

2 − 60x6
2 − 680x5
2 − 1132x5

2 + 142x4

2 − 172x2

2 + 1,

2 + 1674x3

2 − 2081x2 + 117x1,

2 + 2700x3

2 − 3403x2 + 117x3.

(20)

Now we notice that the ﬁrst polynomial in (20) depends only on x2 and is of degree 8. In addition, the second
polynomial in (20) depends linearly on x1 and the third polynomial in (20) depends linearly on x3. This allows us
to express x1 and x3 as polynomials in x2. Therefore, the solution process starts by ﬁnding the 8 roots of the ﬁrst
polynomial in (20) and for each one of those roots, we ﬁnd the unique values of x1 and x3, given by the last two
polynomials in (20). Here are the eight complex roots (four real roots and two pairs of complex conjugate roots) of
the ﬁrst polynomial in (20)

−2.16624183202470499
2.16624183202470499
−0.07643337499454630
0.07643337499454630

−1.329681781358029 − 0.606033421098925 i
−1.329681781358029 + 0.606033421098925 i
1.329681781358029 − 0.606033421098925 i
1.329681781358029 + 0.606033421098925 i

13

Note that the four real roots and the four complex roots come in pairs of the form (r, −r), a consequence of the
fact that the ﬁrst polynomial in (20) contains only even powers of x2. The presence of only even powers of the
indeterminate x2 in the ﬁrst polynomial in (20) is captured by the fact that the order of the Galois group of this
polynomial is smaller than the order of the Galois group of a more “random/generic” polynomial of degree 8.

A very readable, self-contained and pedagogical introduction to Gr¨obner bases can be found in chapter 10 of [174].

4.0.3 Symbolic computation software systems

One of the major outcomes of the research area of Symbolic Computation (Computer Algebra) in the past few
decades, is the advent of the so-called Computer Algebra Systems (CAS). A CAS is deﬁned as a piece of mathe-
matical software that makes advanced functionalities available to the user, in a transparent manner. Such advanced
functionalities include univariate and multivariate polynomial factorization, primality testing, integer factorization,
root ﬁnding, numerical and exact integration, visualization, number theory, linear algebra, commutative algebra,
commutative and non-commutative settings, tools for optimization, graph theory, group theory, coding theory,
combinatorics, discrete mathematics and so forth. There is a number of commercial and free (open source) CAS
available today. Some of the most well-known early CAS (such as Axiom and Macsyma) are largely deprecated
today. We focus our attention on those CAS that currently feature a signiﬁcant user base. We note that CAS
are used by millions of research mathematicians, physicists, chemists, astronomers, engineers, practitioners and
educators at academic, public, private and government institutions worldwide. In what follows, we provide a brief
summary of the main commercial CAS in use today.

(a) Maple is the ﬂagship Canadian product in the area of mathematical software. It is produced, maintained and
distributed by the company Maplesoft, based in Waterloo, Ontario, Canada https://www.maplesoft.com/.
Maple features an easy-to-learn underlying programming language as well as more than 150 additional packages
that signiﬁcantly expand its core functionalities.

(b) Magma is the ﬂagship Australian product in the area of mathematical software. It is produced, maintained and
distributed by the University of Sydney http://magma.maths.usyd.edu.au/. It provides a mathematically
rigorous environment for deﬁning and working with structures such as groups, rings, ﬁelds, modules, algebras,
schemes, curves, graphs, designs, codes and many others. Magma also supports a number of databases
designed to aid computational research in those areas of mathematics which are algebraic in nature.

(c) Mathematica is a ﬂagship American product in the area of mathematical software. It is produced, maintained
and distributed by the company Wolfram Research, based in Champaign, Illinois, United States https:
//www.wolfram.com/. Mathematica’s stated purposes include injecting computational intelligence at every
level, on every project by unifying algorithms, data, notebooks, linguistics and deployment—enabling powerful
workﬂows across desktop, cloud, server and mobile.

(d) Matlab is a ﬂagship American product in the area of mathematical software. https://www.mathworks.com.
It is produced, maintained and distributed by the company MathWorks, based in California, United States.
MATLAB’s strong points include matrix manipulations, plotting of functions and data, implementation of
algorithms, creation of user interfaces, and interfacing with programs written in other languages.

Some of the most popular non-commercial mathematical software packages include Sage, Singular/Plural, Co-
CoA, Macaulay 2 and so forth. Each one has their own strengths and areas of particular focuses. For more
information on CAS in general, we refer the interested reader to the comprehensive book [175].

4.0.4 Symbolic computation books

In this section, we present a selection of books in Symbolic computation and related areas. While there is inevitably
some overlap among these books, there are also diﬀerent aspects of polynomial system solving and its applications
discussed in each one of them.

1. The classic books [174, 176, 177] are general references for Computer Algebra.

2. The series of books by Teo Mora [178, 179, 180, 181] is an encyclopedic reference to the subject of polynomial

system solving.

14

3. The theory of Gr¨obner bases, see [182, 183, 184, 185], was initially developed by Bruno Buchberger and further
extended by several other researchers. The Buchberger algorithm to compute Gr¨obner bases of polynomial
systems is implemented in every major Symbolic Computation software today. This includes commercial
software, such as Maple, Magma and Mathematica and open source software, such as Singular [186], Plural
[187], CoCoA [188, 189, 190, 191] and the Macaulay2 software system [192].

4. The book [193] discusses aspects of systems of polynomial equations from the viewpoints of computational
commutative algebra, discrete geometry, elimination theory, real geometry, as well as their applications in
various domains such as partial diﬀerential equations, economics, probability, and statistics.

5. The book [194] and its second edition [195], describe useful algorithmic aspects of systems with symmetries,
treated using Invariant Theory. The book [196] is concerned with polynomial systems with symmetries, that
arise in the context of dynamical systems. The french-language book [197] is devoted exclusively to polynomial
system solving methods. The french-language book [198] contains a series of chapters on polynomial system
solving.

6. The books [199] and [200] describe the emerging area of Algebraic Statistics, while the book [201] focuses on
numerical aspects of polynomial system solving. On the other hand, the book [202] focuses on theoretical
aspects of polynomial system solving. Furthermore, the books trilogy [203, 204, 205] examine the foundation
of polynomial system solving from the algebraic geometry standpoint as well as the applications standpoint.
The Galois group of a polynomial is inextricably linked with the more general Galois Theory, we mention the
three books [206, 207, 208].

5 Homotopy / Continuation methods

Homotopy methods, also referred to as continuation methods, may be used for ﬁnding solutions to polynomial
equations and systems of polynomial equations [209, 210, 211]. Polynomial equations often arise in kinematics
and robotics related problems [14, 12]; papers [6, 7] apply homotopy methods to solve systems of power ﬂow
equations; large-scale integrated circuit designs and protein-protein interaction equation are solved using homotopy
method in [8], [27] applies homotopy methods for solving SNEs arising in geodesy;
[22] applies homotopy for
ﬁnding string vacua, [4] applies homotopy methods in chemical engineering. Homotopy (or deformation) of a
system of equations Fn(x) = Θn ≡ (0, 0, . . . , 0)(cid:62) (cf. Eq. (1)) is a function Hn such that Hn(x, 1) = Gn(x), and
Hn(x, 0) = Fn(x), where the roots of Gn(x) are known. For example, it is possible to choose a convex homotopy,
Hn(x, λ) = λGn(x) + (1 − λ)Fn(x), where λ ∈ [0, 1] and trace the curve, called the homotopy path, from a starting
point (x, 1) to solution point (x, 0). Thus, gradually deforming solutions of a starting system Gn(x) = Θn into
the solutions to the target system Fn(x) = Θn. At each step of the process, a solution of the current system
Hn(x, ˜λ) = Θn is used as a starting solution to the next system Hn(x, ˜λ + ∆λ) = Θn, which is solved using Newton-
type methods that require an invertible Jacobian. In the case of polynomial systems it is trivial to ﬁnd solutions
of the starting system Gn(x) = Θn. Common approaches for deﬁning Gn(x) include ﬁxed point homotopy, where
Gn(x) = x − x0 (where x0 is a starting solution) and Newton homotopy, where Gn(x) = Fn(x) − Fn(x0). The
latter is also referred to a global homotopy, where Hn(x, λ) = Fn(x) − λFn(x0), where x0 is a starting solution [209].
An adaptive method for selecting the steps of the homotopy path is presented in [212]. As per [213], advantages
of homotopy methods are 1) handling of singular solutions, 2) possibility to obtain multiple solutions using one
homotopy path, and 3) preserving Morse indices for gradient systems. Paper [12] introduces a collision-based
homotopy continuation technique. Article [214] discusses the problem of divergent homotopy paths and proposes
an algorithm which performs projective path tracking.

Article [215] describes the software package PHCpack for solving polynomial systems using the homotopy
method. Other software packages implementing homotopy continuation methods include Bertini [216] and Hom-
Lab [217]. Paper [218] describes a monodromy-based solver.

6

Interval methods

Interval arithmetic was introduced in article [219]. Books on interval methods include [220] and [221]. A real
interval X is deﬁned as a set of real numbers between lower and upper bounds

X = [a, b] = {x ∈ R | a (cid:54) x (cid:54) b}.

15

Interval arithmetic is a set of operations such as addition, subtraction, multiplication, and division, deﬁned on
In
the intervals. Further, interval functions, domain and range of which are the intervals, have been deﬁned.
addition to that, interval diﬀerentiation and integration have also been proposed. The most important beneﬁt of
interval analysis is its accountability for rounding errors due to limited machine precision. If a value is represented
as a single number, rounding errors occurring during the computations may accumulate, thus leading to a wrong
result.
In interval arithmetic a value is represented by a lower and upper bound which provide reliable results
during the computations. Further, interval arithmetic can be used to model uncertainty, often arising in practical
problems; for example, uncertainty due to imprecise measurements [222]. Interval arithmetic was standardized by
the IEEE in 2015 [223]. Global optimization problems can be solved using the interval branch-and-bound method
which iteratively splits the search space, and removes its parts that do not contain a global solution; multiple
splitting schemes have been proposed in the literature [224]. In this case, application of interval arithmetic allows
to guarantee, if a solution exists within a region of interest. Paper [225] proposes to use interval methods for
inclusion and exclusion tests, where inclusion tests check existence of the solution within an interval, and exclusion
tests check its non-existence. The interval Newton method [226] is a generalization of a Newton method for interval
arithmetic; it can be used to ﬁnd zeros of a function. Interval Newton method attempts to iteratively narrow down
the new interval solution [xk+1] from [xk] via

[xk+1] = [xk] ∩ (cid:0)[xk] − Jfn

(cid:0)[xk](cid:1)−1

(cid:0)[xk](cid:1)(cid:1),

fn

k = 0, 1, 2, . . .

Paper [227] presents an interval branch-and-prune algorithm that is capable of ﬁnding all solutions of a polynomial
system, and the article evaluates the proposed method on large systems with up to 320 variables. An interval
method was also eﬀectively applied to solve large polynomial systems with up to 2500 variables in [228]. Article
[229] provides a modiﬁcation of the interval branch-and-bound method and applies it to solving systems of equations.
A method of solving nonlinear equations by using interval arithmetic was patented [230]. Article [231] presents
details of implementation of a parallel interval optimization algorithm and its application for solving systems of
nonlinear equations. A software package for solving equations using interval methods is presented in [232] and [233].
Article [234] provides interval extensions to Halley’s method (discussed in Section 3.3 of this paper), and applies it
to ﬁnding roots of a single nonlinear equation. Interval methods are utilized to ﬁnd all solutions of the kinematics
SNEs in [235, 236]. Articles [237, 238] suggests to use linear programming (LP) for ﬁnding all solutions of the SNE;
the method in the paper is best suited to SNEs consisting of linear equations with relatively few nonlinear terms.
The method is based on the surrounding nonlinear equations by rectangles, based on interval arithmetic. Then, LP
can be used to remove parts of the solution space that do not contain the solutions. Paper [239] proposes to extend
this method and utilize LP narrowing, that is capable to solve large scale (n = 50 000) separable SNEs. Article
[240] proposes an extension of the interval method for a SNE based on its transformation to separable form. Paper
[241] describes the UniCalc SNE solver software which is based on interval methods. Another software package for
interval optimization, that can also be used to solve SNEs is RealPlayer [242].

7 Synopsis and concluding remarks

This article presents part one of a survey on methods for solving a system of nonlinear equations (SNE). In part one
we have presented a comprehensive survey of methods which can be utilized to search for solutions to a SNE without
transforming a SNE into an optimization problem. Since many of the SNEs that arise in real world applications
are considered over a ﬁnite bounded domain D, we ﬁrst introduced a technique which can be utilized to determine
the number of solutions to a SNE that exist within D. Then, we introduced a diverse set of root-ﬁnding methods
which can be used to search for solutions to a SNE. Next, we described additional methods which have been used
to search for solutions to SNEs including methods from symbolic computation, homotopy / continuation methods,
and interval methods.

Analyzing this literature has led us to conclude that although there are a variety of root-ﬁnding methods which
are guaranteed to converge to a solution when applied to SNEs that satisfy certain conditions (such as monotonicity
(Eq. 4) and Lipschitz continuity (Eq. 5)), for general SNEs, there is no guarantee that any of the methods described
in this paper will converge to a solution in ﬁnite time. However, a new taxonomy of SNEs is needed to facilitate
the identiﬁcation of new classes of tractable problems, and to compare the performance of the methods that are
most capable of solving them. We are actively working on the development of such a taxonomy. Additionally, we
are actively exploring methods capable of solving systems of nonlinear equations and inequalities.

In part one of this survey, we discussed methods for solving SNEs without transforming them into optimization
problems. In part two of this survey, we will describe various transformations which can be utilized to transform a

16

SNE into an optimization problem, and we will discuss optimization algorithms which can then be used to search
for solutions. In part three of this survey, we will present a robust quantitative comparative analysis of methods
capable of searching for solutions to SNEs.

References

[1] Christodoulos A. Floudas. Global optimization in design and control of chemical process systems. Journal of

Process Control, 10(2):125–134, 2000.

[2] Wagner F. Sacco and N´elio Henderson. Finding all solutions of nonlinear systems using a hybrid metaheuristic

with fuzzy clustering means. Applied Soft Computing, 11(8):5424–5432, 2011.

[3] Astrid Holstad. Numerical solution of nonlinear equations in chemical speciation calculations. Computational

Geosciences, 3(3):229–257, Dec 1999.

[4] Hugo Jim´enez-Islas, Gloria M. Mart´ınez-Gonz´alez, Jos´e L. Navarrete-Bola˜nos, Jos´e E. Botello- ´Alvarez, and
J. Manuel Oliveros-Mu˜noz. Nonlinear homotopic continuation methods: A chemical engineering perspective
review. Industrial & Engineering Chemistry Research, 52(42):14729–14742, Oct 2013.

[5] N´elio Henderson, Wagner F. Sacco, and Gustavo Mendes Platt. Finding more than one root of nonlinear equa-
tions via a polarization technique: An application to double retrograde vaporization. Chemical Engineering
Research and Design, 88(5):551–561, 2010.

[6] Hsiao-Dong Chiang, Tian-Qi Zhao, Jiao-Jiao Deng, and Kaoru Koyanagi. Homotopy-enhanced power ﬂow
methods for general distribution networks with distributed generators. IEEE Transactions on Power Systems,
29(1):93–100, 2014.

[7] Dhagash Mehta, Hung Dinh Nguyen, and Konstantin Turitsyn. Numerical polynomial homotopy continuation
method to locate all the power ﬂow solutions. IET Generation, Transmission & Distribution, 10(12):2972–
2980, 2016.

[8] Hsiao-Dong Chiang and Tao Wang. Novel homotopy theory for nonlinear networks and systems and its
applications to electrical grids. IEEE Transactions on Control of Network Systems, 5(3):1051–1060, 2018.

[9] Chao Yang, Jianwen Cao, and Xiao-Chuan Cai. A fully implicit domain decomposition algorithm for shallow

water equations on the cubed-sphere. SIAM Journal on Scientiﬁc Computing, 32(1):418–438, 2010.

[10] Matti Schneider, Daniel Wicht, and Thomas B¨ohlke. On polarization-based schemes for the ﬀt-based compu-
tational homogenization of inelastic materials. Computational Mechanics, 64(4):1073–1095, Oct 2019.

[11] Yunong Zhang. A set of nonlinear equations and inequalities arising in robotics and its online solution via a

primal neural network. Neurocomputing, 70(1):513–524, 2006.

[12] Amir Salimi Lafmejani, Ahmad Kalhor, and Mehdi Masouleh. A new development of homotopy continuation
method, applied in solving nonlinear kinematic system of equations of parallel mechanisms. 2015 3rd RSI
International Conference on Robotics and Mechatronics (ICROM), 10 2015.

[13] David A Cox, John Little, and Donal O’Shea. Robotics and automatic geometric theorem proving. In Ideals,

Varieties, and Algorithms, pages 291–343. Springer, 2015.

[14] Ping Ji and Hongtao Wu. Kinematics analysis of an oﬀset 3-upu translational parallel robotic manipulator.

Robotics Auton. Syst., 42:117–123, 2003.

[15] Strachimir Cht. Mavrodiev and Maksym Deliyergiyev. Modiﬁcation of the nuclear landscape in the inverse
problem framework using the generalized bethe-weizs¨acker mass formula. arXiv e-prints, pages arXiv–1602,
2016.

[16] Sani Aji, Poom Kumam, Punnarai Siricharoen, Auwal Bala Abubakar, and Mahmoud Muhammad Yahaya.
A modiﬁed conjugate descent projection method for monotone nonlinear equations and image restoration.
IEEE Access, 8:158656–158665, 2020.

17

[17] Jan Verschelde, Pierre Verlinden, and Ronald Cools. Homotopies exploiting Newton polytopes for solving

sparse polynomial systems. SIAM Journal on Numerical Analysis, 31(3):915–930, 1994.

[18] Crina Grosan and Ajith Abraham. A new approach for solving nonlinear equations systems. IEEE Transac-

tions on Systems, Man, and Cybernetics - Part A: Systems and Humans, 38(3):698–714, 2008.

[19] Ahmad Golbabai, Davood Ahmadian, and Mariyan Milev. Radial basis functions with application to ﬁnance:

American put option under jump diﬀusion. Mathematical and Computer Modelling, 55(3):1354–1362, 2012.

[20] Guang Zhang and Liang Bai. Existence of solutions for a nonlinear algebraic system. Discrete dynamics in

nature and society, 2009, 2009.

[21] Karol Kowalski and Karol Jankowski. Towards complete solutions to systems of nonlinear equations of many-

electron theories. Phys. Rev. Lett., 81:1195–1198, Aug 1998.

[22] Dhagash Mehta. Numerical polynomial homotopy continuation method and string vacua. Advances in High

Energy Physics, 2011:263937, Oct 2011.

[23] Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Nonlinear equation solving: A faster alternative

to feedforward computation. arXiv preprint arXiv:2002.03629, 2020.

[24] Yunfeng Cai and Ping Li. Solving the robust matrix completion problem via a system of nonlinear equations.
In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference
on Artiﬁcial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 4162–
4172. PMLR, 26–28 Aug 2020.

[25] Stepan Yu. Gatilov. Properties of nonlinear systems and convergence of the Newton-Raphson method in

geometric constraint solving. volume 32, pages 57—-75. NCC, 2011.

[26] B´ela Pal´ancz, Piroska Zaletnyik, Joseph L Awange, and Erik W Grafarend. Dixon resultant’s solution of

systems of geodetic polynomial equations. Journal of Geodesy, 82(8):505–511, 2008.

[27] B´ela Pal´ancz, Joseph L. Awange, Piroska Zaletnyik, and Robert H. Lewis. Linear homotopy solution of

nonlinear systems of equations in geodesy. Journal of Geodesy, 84(1):79, Sep 2009.

[28] Christian Jansson. An np-hardness result for nonlinear systems. Reliable Computing, 4(4):345–350, Nov 1998.

[29] Yuri V. Matiyasevich. Hilbert’s Tenth Problem. MIT Press, Cambridge, MA, USA, 1993.

[30] Mustafa Q Khirallah and MA Haﬁz. Solving system of nonlinear equations using family of Jarratt methods.

International Journal of Diﬀerential Equations and Applications, 12(2), 2013.

[31] MQ Khirallah and MA Haﬁz. Novel three order methods for solving a system of nonlinear equations. Bulletin

of Mathematical Sciences & Applications, 1(2):01–14, 2012.

[32] J Alikhani Koupaei and Seyed Mohammad Mehdi Hosseini. A new hybrid algorithm based on chaotic maps

for solving systems of nonlinear equations. Chaos, Solitons & Fractals, 81:233–245, 2015.

[33] M. A. El-Shorbagy and Adel M. El-Refaey. Hybridization of grasshopper optimization algorithm with genetic

algorithm for solving system of non-linear equations. IEEE Access, 8:220944–220961, 2020.

[34] Y Ramu Naidu and Akshay Kumar Ojha. Solving multiobjective optimization problems using hybrid co-
operative invasive weed optimization with multiple populations. IEEE Transactions on Systems, Man, and
Cybernetics: Systems, 48(6):821–832, 2016.

[35] Angel Kuri. Solution of simultaneous non-linear equations using genetic algorithms. WSEAS Transactions

on Systems, 2, 01 2003.

[36] Lin Xiao, Zhijun Zhang, and Shuai Li. Solving time-varying system of nonlinear equations by ﬁnite-time
recurrent neural networks with application to motion tracking of robot manipulators. IEEE Transactions on
Systems, Man, and Cybernetics: Systems, 49(11):2210–2220, 2018.

18

[37] Huan-Tong Geng, Yi-Jie Sun, Qing-Xi Song, and Ting-Ting Wu. Research of ranking method in evolution
strategy for solving nonlinear system of equations. In 2009 First International Conference on Information
Science and Engineering, pages 348–351, 2009.

[38] Jun Pei, Zorica Draˇzi´c, Milan Draˇzi´c, Nenad Mladenovi´c, and Panos M Pardalos. Continuous variable
neighborhood search (c-vns) for solving systems of nonlinear equations. INFORMS Journal on Computing,
31(2):235–250, 2019.

[39] Abd allah Mousa and Islam Eldesoky. Genls: Co-evolutionary algorithm for nonlinear system of equations.

Applied Mathematics and Computation, 197:633–642, 04 2008.

[40] Wenyin Gong, Yong Wang, Zhihua Cai, and Shengxiang Yang. A weighted biobjective transformation tech-
nique for locating multiple optimal solutions of nonlinear equation systems. IEEE Transactions on Evolu-
tionary Computation, 21(5):697–713, 2017.

[41] Wu Song, Yong Wang, Han-Xiong Li, and Zixing Cai. Locating multiple optimal solutions of nonlinear
equation systems based on multiobjective optimization. IEEE Transactions on Evolutionary Computation,
19(3):414–431, 2015.

[42] Sha Qin, Sanyou Zeng, Wei Dong, and Xi Li. Nonlinear equation systems solved by many-objective hype. In

2015 IEEE Congress on Evolutionary Computation (CEC), pages 2691–2696, 2015.

[43] Jinjin Guo, Binbin Qiu, and Yunong Zhang. New-type dtz model for solving discrete time-dependent nonlinear
equation system with robotic-arm application. In 2020 10th International Conference on Information Science
and Technology (ICIST), pages 153–162, 2020.

[44] Zuowen Liao, Wenyin Gong, and Ling Wang. Memetic niching-based evolutionary algorithms for solving

nonlinear equation system. Expert Systems with Applications, 149:113261, 2020.

[45] Zuowen Liao, Wenyin Gong, Xuesong Yan, Ling Wang, and Chengyu Hu. Solving nonlinear equations system
with dynamic repulsion-based evolutionary algorithms. IEEE Transactions on Systems, Man, and Cybernetics:
Systems, 50(4):1590–1601, 2020.

[46] Weifeng Gao, Yuting Luo, Jingwei Xu, and Shengqi Zhu. Evolutionary algorithm with multiobjective opti-

mization technique for solving nonlinear equation systems. Information Sciences, 541, 07 2020.

[47] Wenyin Gong, Yong Wang, Zhihua Cai, and Ling Wang. Finding multiple roots of nonlinear equation systems
via a repulsion-based adaptive diﬀerential evolution. IEEE Transactions on Systems, Man, and Cybernetics:
Systems, 50(4):1499–1513, 2020.

[48] Weifeng Gao, Genghui Li, Qingfu Zhang, Yuting Luo, and Zhenkun Wang. Solving nonlinear equation systems
by a two-phase evolutionary algorithm. IEEE Transactions on Systems, Man, and Cybernetics: Systems,
51(9):5652–5663, 2021.

[49] Wei He, Wenyin Gong, Ling Wang, Xuesong Yan, and Chengyu Hu. Fuzzy neighborhood-based diﬀerential
evolution with orientation for nonlinear equation systems. Knowledge-Based Systems, 182:104796, 2019.

[50] Jianye Wu, Wenyin Gong, and Ling Wang. A clustering-based diﬀerential evolution with diﬀerent crowding

factors for nonlinear equations system. Applied Soft Computing, 98:106733, 2021.

[51] Weifeng Gao and Yu Li. Solving a new test set of nonlinear equation systems by evolutionary algorithm.

IEEE Transactions on Cybernetics, pages 1–10, 2021.

[52] Aijuan Song, Guohua Wu, and Witold Pedrycz. Integrating variable reduction strategy with evolutionary

algorithm for solving nonlinear equations systems. CoRR, abs/2008.04223, 2020.

[53] Zuowen Liao, Wenyin Gong, Ling Wang, Xuesong Yan, and Chengyu Hu. A decomposition-based diﬀerential
evolution with reinitialization for nonlinear equations systems. Knowledge-Based Systems, 191:105312, 2020.

[54] Shi Cheng, Junfeng Chen, Xiujuan Lei, and Yuhui Shi. Locating multiple optima via brain storm optimization

algorithms. IEEE Access, 6:17039–17049, 2018.

19

[55] Wenyin Gong, Zuowen Liao, Xianyan Mi, Ling Wang, and Yuanyuan Guo. Nonlinear equations solving with

intelligent optimization algorithms: A survey. Complex System Modeling and Simulation, 1(1):15–32, 2021.

[56] Masoud Ahookhosh, Keyvan Amini, and Somayeh Bahrami. Two derivative-free projection approaches for

systems of large-scale nonlinear monotone equations. Numerical Algorithms, 64, 10 2013.

[57] Susumu Tanab´e and Michael N. Vrahatis. On perturbation of roots of homogeneous algebraic systems.

Mathematics of Computation, 255:1383–1402, 2006.

[58] Najib Ullah, Jamilu Sabi’u, and Abdullah Shah.

A derivative-free scaling memoryless broyden–
ﬂetcher–goldfarb–shanno method for solving a system of monotone nonlinear equations. Numerical Linear
Algebra with Applications, 28(5):e2374, 2021.

[59] Sani Aji, Poom Kumam, Aliyu Muhammed Awwal, Mahmoud Muhammad Yahaya, and Wiyada Kumam.
Two hybrid spectral methods with inertial eﬀect for solving system of nonlinear monotone equations with
application in robotics. IEEE Access, 9:30918–30928, 2021.

[60] Jos´e Mario Mart´ınez. Algorithms for Solving Nonlinear Systems of Equations, pages 81–108. Springer Nether-

lands, Dordrecht, 1994.

[61] James M. Ortega and Werner C. Rheinboldt. Iterative Solution of Nonlinear Equations in Several Variables.

Society for Industrial and Applied Mathematics, January 2000.

[62] ´Emile Picard.

Sur le nombre des racines communes `a plusieurs ´equations simultan´ees.

Journal de

Math´ematiques Pures et Appliqu´ees, 8(4e s´erie):5–24, 1892.

[63] ´Emile Picard. Trait´e d’Analyse. Gauthier-Villars, Paris, 1922.

[64] Jane Cronin. Fixed Points and Topological Degree in Nonlinear Analysis. American Mathematical Society,

Providence, Rhode Island, 1964.

[65] Noel G. Lloyd. Degree Theory. Oxford University Press, New York, 1978.

[66] Donal O’Regan, Yeol Je Cho, and Yu-Qing Chen. Topological Degree Theory and Applications. Taylor &

Francis Group, Boca Raton, FL, 2006.

[67] Enrique Outerelo and Jes´us M. Ruiz. Mapping Degree Theory. American Mathematical Society, Providence,

Rhode Island, 2009.

[68] Krzysztof A. Sikorski. Optimal Solution of Nonlinear Equations. Oxford University Press, New York, 2001.

[69] Jeroen M. Bergamin, Tassos C. Bountis, and Michael N. Vrahatis. Homoclinic orbits of invertible maps.

Nonlinearity, 15(5):1603–1619, 2002.

[70] Ioannis Z. Emiris, Bernard Mourrain, and Michael N. Vrahatis. Sign methods for counting and computing
real roots of algebraic systems. Technical report, Rapport de Recherche No.3669, INRIA (Institut National
de Recherche en Informatique et en Automatique), Sophia Antipolis, France, April 1999.

[71] Dimitris J. Kavvadias and Michael N. Vrahatis. Locating and computing all the simple roots and extrema of

a function. SIAM Journal on Scientiﬁc Computing, 17(5):1232–1248, 19965.

[72] R. Baker Kearfott. An eﬃcient degree-computation method for a generalized method of bisection. Numerische

Mathematik, 32:109–127, 1979.

[73] Bernard Mourrain, Nicos G. Pavlidis, Dimitris K. Tasoulis, and Michael N. Vrahatis. Determining the number
of real roots of polynomials through neural networks. Computers and Mathematics with Applications, 51(3-
4):527–536, 2006.

[74] Bernard Mourrain, Michael N. Vrahatis, and Yakoubsohn Jean-Claude. On the complexity of isolating real
roots and computing with certainty the topological degree. Journal of Complexity, 18(2):612–640, 2002.

[75] Vassilis P. Plagianakos, Nicos K. Nousis, and Michael N. Vrahatis. Locating and computing in parallel all
the simple roots of special functions using pvm. Journal of Computational and Applied Mathematics, 133(1-
2):545–554, 2001.

20

[76] Chronis Polymilis, Graziano Servizi, Charalampos Skokos, Giorgio Turchetti, and Michael N. Vrahatis. Topo-

logical degree theory and local analysis of area preserving maps. Chaos, 13(1):94–104, 2003.

[77] Frank Stenger. Computing the topological degree of a mapping in Rn. Numerische Mathematik, 25:23–38,

1975.

[78] Martin Stynes. An algorithm for numerical calculation of topological degree. Applicable Analysis, 25:23–38,

1979.

[79] Michael N. Vrahatis. An eﬃcient method for locating and computing periodic orbits of nonlinear mappings.

Journal of Computational Physics, 119:105–119, 1995.

[80] Michael N. Vrahatis. Solving systems of nonlinear equations using the nonzero value of the topological degree.

ACM Transactions on Mathematical Software, 14(4):312–329, Dec. 1988.

[81] Michael N. Vrahatis. A short proof and a generalization of Miranda’s existence theorem. Proceedings of the

American Mathematical Society, 107(3):701–703, 1989.

[82] Michael N. Vrahatis and Kosmas I. Iordanidis. A rapid generalized method of bisection for solving systems

of nonlinear equations. Numerische Mathematik, 49(2-3):123–138, 1986.

[83] Michael N. Vrahatis, Graziano Servizi, Giorgio Turchetti, and Tassos C. Bountis. A procedure to compute
the ﬁxed points and visualize the orbits of a 2D map. Technical report, CERN SL/93-06 (AP), CERN-SL
Division, European Organization for Nuclear Research (CERN), Geneva, Switzerland, February 1993.

[84] Peter Franek and Marek Krˇc´al. Robust satisﬁability of systems of equations. J. ACM, 62(4), sep 2015.

[85] Rodrigo G Eustaquio, Ademir A Ribeiro, and Miguel A Dumett. A new class of root-ﬁnding methods in rn:
the inexact tensor-free chebyshev–halley class. Computational and Applied Mathematics, 37(5):6654–6675,
2018.

[86] John E. Dennis and Robert B. Schnabel. Numerical methods for unconstrained optimization and nonlinear

equations. SIAM, 1996.

[87] Carl T. Kelley. Solving nonlinear equations with Newton’s method. SIAM, 2003.

[88] Werner C. Rheinboldt. Methods for solving systems of nonlinear equations. SIAM, 1998.

[89] Marco Frontini and E Sormani. Third-order methods from quadrature formulae for solving systems of nonlinear

equations. Applied Mathematics and Computation, 149(3):771–782, 2004.

[90] MT Darvishi and Ali Barati. A fourth-order method from quadrature formulae to solve systems of nonlinear

equations. Applied Mathematics and Computation, 188(1):257–261, 2007.

[91] Hari M Srivastava, Javed Iqbal, Muhammad Arif, Alamgir Khan, Yusif S Gasimov, and Ronnason Chinram. A
new application of gauss quadrature method for solving systems of nonlinear equations. Symmetry, 13(3):432,
2021.

[92] Muhammad Aslam Noor and Muhammad Waseem. Some iterative methods for solving a system of nonlinear

equations. Computers & Mathematics with Applications, 57(1):101–106, 2009.

[93] Adi Ben-Israel. A modiﬁed newton-raphson method for the solution of systems of equations. Israel Journal

of Mathematics, 3(2):94–98, Jun 1965.

[94] N. E. Yudin. Adaptive gauss–newton method for solving systems of nonlinear equations. Doklady Mathematics,

104(2):293–296, Sep 2021.

[95] Adi Ben-Israel. A newton-raphson method for the solution of systems of equations. Journal of Mathematical

Analysis and Applications, 15(2):243–252, 1966.

[96] Jos´e L. Hueso, Eulalia Mart´ınez, and Juan R. Torregrosa. Modiﬁed newton’s method for systems of nonlinear

equations with singular jacobian. Journal of Computational and Applied Mathematics, 224(1):77–83, 2009.

21

[97] Stepan Yu. Gatilov. Using low-rank approximation of the Jacobian matrix in the Newton-Raphson method

to solve certain singular equations. J. Comput. Appl. Math., 272:8–24, 2014.

[98] Xiubin Xu and Chong Li. Convergence of newton’s method for systems of equations with constant rank

derivatives. Journal of Computational Mathematics, 25(6):705–718, 2007.

[99] Theodoula N. Grapsa and Michael N. Vrahatis. The implicit function theorem for solving systems of nonlinear

equations in R2. International Journal of Computer Mathematics, 28(1-4):171–181, 1989.

[100] Theodoula N. Grapsa and Michael N. Vrahatis. A dimension-reducing method for solving systems of nonlinear

equations in Rn. International Journal of Computer Mathematics, 32(3-4):205–216, 1990.

[101] Theodoula N. Grapsa and Michael N. Vrahatis. A new dimension-reducing method for solving systems of

nonlinear equations. International Journal of Computer Mathematics, 55:235–244, 1995.

[102] Theodoula N. Grapsa and Michael N. Vrahatis. A dimension-reducing method for unconstrained optimization.

Journal of Computational and Applied Mathematics, 66:239–253, 1996.

[103] Theodoula N. Grapsa, Michael N. Vrahatis, and Tassos C. Bountis. Solving systems of nonlinear equations in
Rn using a rotating hyperplane in Rn+1. International Journal of Computer Mathematics, 35(1-4):133–151,
1990.

[104] Charles G. Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics of

Computation, 19(92):577–593, 1965.

[105] M´arcia A. Gomes-Ruggiero, Jos´e Mario Mart´ınez, and Antonio Carlos Moretti. Comparing algorithms for solv-
ing sparse nonlinear systems of equations. SIAM Journal on Scientiﬁc and Statistical Computing, 13(2):459–
483, March 1992.

[106] Michael G. Epitropakis and Michael N. Vrahatis. Studying the basin of convergence of methods for computing

periodic orbits. International Journal of Bifurcation and Chaos, 21(08):2079–2106, 2011.

[107] Youcef Saad and Martin H. Schultz. Gmres: A generalized minimal residual algorithm for solving nonsym-

metric linear systems. SIAM Journal on Scientiﬁc and Statistical Computing, 7(3):856–869, 1986.

[108] Walter E. Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.

Quarterly of Applied Mathematics, 9:17–29, 1951.

[109] Krzysztof A Sikorski. Bisection is optimal. Numerische Mathematik, 40:111–117, 1982.

[110] Dimitris J. Kavvadias, Frosso S. Makri, and Michael N. Vrahatis. Locating and computing arbitrarily dis-

tributed zeros. SIAM Journal on Scientiﬁc Computing, 21(3):954–969, 1999.

[111] Dimitris J. Kavvadias, Frosso S. Makri, and Michael N. Vrahatis. Eﬃciently computing many roots of a

function. SIAM Journal on Scientiﬁc Computing, 27(1):93–107, 2005.

[112] Michael N. Vrahatis. An error estimation for the method of bisection in Rn. Bulletin of the Greek Mathematical

Society, 27:161–174, 1986.

[113] Michael N. Vrahatis. CHABIS: A mathematical software package for locating and evaluating roots of systems

of nonlinear equations. ACM Transactions on Mathematical Software, 14(4):330–336, Dec. 1988.

[114] Michael N. Vrahatis, George S. Androulakis, and George E. Manoussakis. A new unconstrained optimization
method for imprecise function and gradient values. Journal of Mathematical Analysis and Applications,
197(2):586–607, 1996.

[115] Michael N. Vrahatis, Heinz Isliker, and Tassos C. Bountis. Structure and breakdown of invariant tori in a 4-D
mapping model of accelerator dynamics. International Journal of Bifurcation and Chaos, 7(12):2707–2722,
1997.

[116] Dimitra-Nefeli A. Zottou, Dimitris J. Kavvadias, Frosso S. Makri, and Michael N. Vrahatis. Manbis–a C++
mathematical software package for locating and computing eﬃciently many roots of a function: Theoretical
issues. ACM Transactions on Mathematical Software, 44(3):1–7, April 2018.

22

[117] Stamatios-Aggelos N. Alexandropoulos, Panos M. Pardalos, and Michael N. Vrahatis. Dynamic search tra-
jectory methods for global optimization. Annals of Mathematics and Artiﬁcial Intelligence, 88(1-3):3–37,
2020.

[118] Bassilis Boutsinas and Michael N. Vrahatis. Artiﬁcial nonmonotonic neural networks. Artiﬁcial Intelligence,

132(1):1–38, 2001.

[119] Michael N. Vrahatis, George S. Androulakis, John N. Lambrinos, and George D. Magoulas. A class of
gradient unconstrained minimization algorithms with adaptive stepsize. Journal of Computational and Applied
Mathematics, 114(2):367–386, 2000.

[120] Michael N. Vrahatis, George D. Magoulas, and Vassilis P Plagianakos. Globally convergent modiﬁcation of

the quickprop method. Neural Processing Letters, 12(2):159–169, 2000.

[121] M.A. Hern´andez and M.A. Salanova. Indices of convexity and concavity. application to halley method. Applied

Mathematics and Computation, 103(1):27–49, 1999.

[122] MA Hern´andez and MA Salanova. A family of chebyshev-halley type methods.

International Journal of

Computer Mathematics, 47(1-2):59–63, 1993.

[123] J.M. Guti´errez and M.A. Hern´andez. A family of chebyshev-halley type methods in banach spaces. Bulletin

of the Australian Mathematical Society, 55(1):113–130, 1997.

[124] DKR Babajee, MZ Dauhoo, MT Darvishi, A Karami, and Ali Barati. Analysis of two chebyshev-like third or-
der methods free from second derivatives for solving systems of nonlinear equations. Journal of Computational
and Applied Mathematics, 233(8):2002–2012, 2010.

[125] Trond Steihaug and Sara Suleiman. Rate of convergence of higher order methods. Applied Numerical Math-

ematics, 67:230–242, 2013. NUMAN 2010.

[126] Ioannis K. Argyros, Dong Chen, and Qian Qingshan. The jarratt method in banach space setting. Journal

of Computational and Applied Mathematics, 51(1):103–106, May 1994.

[127] Fayyaz Ahmad, Emran Tohidi, Malik Zaka Ullah, and Juan A Carrasco. Higher order multi-step jarratt-like
method for solving systems of nonlinear equations: Application to pdes and odes. Computers & Mathematics
with Applications, 70(4):624–636, 2015.

[128] Thugal Zhanlav and Khuder Otgondorj. Higher order jarratt-like iterations for solving systems of nonlinear

equations. Applied Mathematics and Computation, 395:125849, 2021.

[129] Paul David Frank. Tensor methods for solving systems of nonlinear equations (numerical analysis, optimiza-

tion). PhD thesis, University of Colorado at Boulder, 1984.

[130] Robert B Schnabel and Paul D Frank. Solving systems of nonlinear equations by tensor methods. Technical

report, Department of Computer Science, Colorado University at Boulder, 1986.

[131] Ali Bouaricha. Solving large sparse systems of nonlinear equations and nonlinear least squares problems using

tensor methods on sequential and parallel computers. PhD thesis, University of Colorado at Boulder, 1992.

[132] Ali Bouaricha and Robert B Schnabel. Tensor methods for large, sparse systems of nonlinear equations.
Preprint MCS-P473-1094, Mathematics and Computer Science Division, Argonne National Laboratory, 1994.

[133] Dan Feng and Thomas H Pulliam. Tensor-gmres method for large systems of nonlinear equations. SIAM

Journal on Optimization, 7(3):757–779, 1997.

[134] Ali Bouaricha and Robert B Schnabel. Algorithm 768: TENSOLVE: A software package for solving systems
of nonlinear equations and nonlinear least-squares problems using tensor methods. ACM Transactions on
Mathematical Software (TOMS), 23(2):174–195, 1997.

[135] Ali Bouaricha and Robert B Schnabel. Tensor methods for large sparse systems of nonlinear equations.

Mathematical programming, 82(3):377–400, 1998.

[136] Brett W Bader and Robert B Schnabel. On the performance of tensor methods for solving ill-conditioned

problems. SIAM Journal on Scientiﬁc Computing, 29(6):2329–2351, 2004.

23

[137] Brett W. Bader. Tensor-Krylov methods for solving large-scale systems of nonlinear equations. SIAM Journal

on Numerical Analysis, 43(3):1321–1347, 2005.

[138] Claude Brezinski. A classiﬁcation of quasi-Newton methods. Numerical Algorithms, 33:123–135, 2004.

[139] Guangye Li. Successive column correction algorithms for solving sparse nonlinear systems of equations.

Mathematical Programming, 43(1):187–207, Jan 1989.

[140] Jos´e Mario Mart´ınez and M´ario C. Zambaldi. An inverse column-updating method for solving large–scale

nonlinear systems of equations. Optimization Methods and Software, 1(2):129–140, 1992.

[141] Charles G. Broyden. The convergence of a class of double-rank minimization algorithms 1. General consider-

ations. IMA Journal of Applied Mathematics, 6(1):76–90, 03 1970.

[142] Roger Fletcher. A new approach to variable metric algorithms. The Computer Journal, 13(3):317–322, 01

1970.

[143] Donald Goldfarb. A family of variable-metric methods derived by variational means. Mathematics of Com-

putation, 24(109):23–26, 1970.

[144] David F Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of compu-

tation, 24(111):647–656, 1970.

[145] Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathe-

matical Programming, 45(1):503–528, Aug 1989.

[146] Ernesto G Birgin, Nataˇsa Kreji´c, and Jos´e Mario Mart´ınez. Globally convergent inexact quasi-newton methods

for solving nonlinear systems. Numerical algorithms, 32(2):249–260, 2003.

[147] MK Dauda, AS Magaji, US Shehub, MA Usman, and MY Waziri. A simple conjugate gradient type method for
solving large-scale systems of nonlinear equations. Malaysian Journal of Computing and Applied Mathematics,
3(2):25–35, 2020.

[148] David Ek. Approaches to accelerate methods for solving systems of equations arising in nonlinear optimization.

PhD thesis, KTH Royal Institute of Technology, 2020.

[149] K Kamfa, MY Waziri, IM Sulaiman, M Mamat, and Hery Ibrahim. A quasi-newton like method via modiﬁed
rational approximation model for solving system of nonlinear equation. Journal of Advanced Research in
Dynamical and Control Systems, 2020.

[150] Wah June Leong, Malik Abu Hassan, and Muhammad Waziri Yusuf. A matrix-free quasi-newton method for

solving large-scale nonlinear systems. Computers & Mathematics with Applications, 62(5):2354–2363, 2011.

[151] Ya-Zhong Luo, Guo-Jin Tang, and Li-Ni Zhou. Hybrid approach for solving systems of nonlinear equations

using chaos optimization and quasi-newton method. Applied Soft Computing, 8(2):1068–1073, 2008.

[152] Mustafa Mamat, MK Dauda, MY Waziri, Fadhilah Ahmad, and Fatma Susilawati Mohamad. Improved quasi-
newton method via psb update for solving systems of nonlinear equations. In AIP Conference Proceedings,
volume 1782, page 030009. AIP Publishing LLC, 2016.

[153] Jos´e Mario Mart´ınez. Practical quasi-newton methods for solving nonlinear systems. Journal of Computational
and Applied Mathematics, 124(1):97–121, 2000. Numerical Analysis 2000. Vol. IV: Optimization and Nonlinear
Equations.

[154] Rosana P´erez and V´era Lucia Rocha Lopes. Recent applications and numerical implementation of quasi-
newton methods for solving nonlinear systems of equations. Numerical Algorithms, 35(2):261–285, 2004.

[155] Alessandra Papini, Margherita Porcelli, and Cristina Sgattoni. On the global convergence of a new spectral
residual algorithm for nonlinear systems of equations. Bollettino dell’Unione Matematica Italiana, pages 1–12,
2020.

[156] William La Cruz, Jos´e Mario Mart´ınez, and Marcos Raydan. Spectral residual method without gradient
information for solving large-scale nonlinear systems of equations. Mathematics of Computation, 75(255):1429–
1449, April 2006.

24

[157] Zhong-Zhi Bai, Gene H. Golub, and Michael K. Ng. Hermitian and skew-hermitian splitting methods for non-
hermitian positive deﬁnite linear systems. SIAM Journal on Matrix Analysis and Applications, 24(3):603–626,
2003.

[158] Qingbiao Wu and Minhong Chen. Convergence analysis of modiﬁed newton-hss method for solving systems

of nonlinear equations. Numerical Algorithms, 64(4):659–683, 2013.

[159] Zhong-Zhi Bai and Xi Yang. On hss-based iteration methods for weakly nonlinear systems. Applied numerical

mathematics, 59(12):2923–2936, 2009.

[160] Abdolreza Amiri, Alicia Cordero, Mohammad Taghi Darvishi, and Juan R Torregrosa. A fast algorithm to
solve systems of nonlinear equations. Journal of Computational and Applied Mathematics, 354:242–258, 2019.

[161] Kenneth Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of

Applied Mathematics, 2(2):164–168, 1944.

[162] Donald Marquardt. An algorithm for least-squares estimation of nonlinear parameters. SIAM Journal on

Applied Mathematics, 11(2):431–441, 1963.

[163] Roger Fletcher. A modiﬁed marquardt subroutine for nonlinear least squares. Technical report, Atomic

Energy Research Establishment, Harwell, England (United Kingdom), 1971.

[164] Valentina Kuzina and Alexander Koshev. Modiﬁcation of the levenberg–marquardt algorithm for solving
complex computational construction problems. In IOP Conference Series: Materials Science and Engineering,
volume 960, page 032039. IOP Publishing, 2020.

[165] Jos´e de Jes´us Rubio. Stability analysis of the modiﬁed levenberg–marquardt algorithm for the artiﬁcial neural
network training. IEEE Transactions on Neural Networks and Learning Systems, 32(8):3510–3524, 2021.

[166] Kenneth M. Brown and William B. Gearhart. Deﬂation techniques for the calculation of further solutions of

a nonlinear system. Numerische Mathematik, 16, 1971.

[167] James H. Wilkinson. Rounding errors in algebraic processes. Dover Publications, Inc., New York, 1994.

[168] Vassilis S. Kalantonis, Efstathios A. Perdios, Angela E. Perdiou, Omiros Ragos, and Michael N. Vrahatis.
Deﬂation techniques for the determination of periodic solutions of a certain period. Astrophysics and Space
Science, 288(2):591–599, 2003.

[169] Konstantinos E. Parsopoulos, Vasssilis P. Plagianakos, George D. Magoulas, and Michael N. Vrahatis. Ob-
jective function “stretching” to alleviate convergence to local minima. Nonlinear Analysis – Theory, Methods
& Applications, 47(5):3419–3424, 2001.

[170] Konstantinos E. Parsopoulos and Michael N. Vrahatis. Recent approaches to global optimization problems

through particle swarm optimization. Natural Computing, 1(2-3):235–306, 2002.

[171] Konstantinos E. Parsopoulos and Michael N. Vrahatis. On the computation of all global minimizers through

particle swarm optimization. IEEE Transactions on Evolutionary Computation, 8(3):211–224, 2004.

[172] Konstantinos E. Parsopoulos and Michael N. Vrahatis. Particle swarm optimization and intelligence: Advances

and applications. Information Science Publishing (IGI Global), Hershey, PA, USA, 2010.

[173] Konstantinos E. Parsopoulos and Michael N. Vrahatis. Deﬂection and stretching techniques for detection of
multiple minimizers in multimodal optimization problems. In Metaheuristics for Finding Multiple Solutions,
M. Preuss, M.G. Epitropakis, J.E. Fieldsend and X. Li (eds.), Chapter 6, pages 129–144. Natural Computing
Series, Springer Nature Switcherland AG, 2021.

[174] K. O. Geddes, S. R. Czapor, and G. Labahn. Algorithms for computer algebra. Kluwer Academic Publishers,

Boston, MA, 1992.

[175] Michael J. Wester. Computer Algebra Systems: A Practical Guide. John Wiley & Sons, Chichester, United

Kingdom, 1999. https://math.unm.edu/~wester/cas/book/contents.html.

25

[176] J. H. Davenport, Y. Siret, and E. Tournier. Computer algebra. Academic Press, Ltd., London, second edition,
1993. Systems and algorithms for algebraic computation, With a preface by Daniel Lazard, Translated from
the French by A. Davenport and J. H. Davenport, With a foreword by Anthony C. Hearn.

[177] Joachim von zur Gathen and J¨urgen Gerhard. Modern computer algebra. Cambridge University Press,

Cambridge, third edition, 2013.

[178] Teo Mora. Solving polynomial equation systems. I, volume 88 of Encyclopedia of Mathematics and its Appli-

cations. Cambridge University Press, Cambridge, 2003. The Kronecker-Duval philosophy.

[179] Teo Mora. Solving polynomial equation systems. II, volume 99 of Encyclopedia of Mathematics and its Appli-
cations. Cambridge University Press, Cambridge, 2005. Macaulay’s paradigm and Gr¨obner technology.

[180] Teo Mora. Solving polynomial equation systems. Vol. III. Algebraic solving, volume 157 of Encyclopedia of

Mathematics and its Applications. Cambridge University Press, Cambridge, 2015.

[181] Teo Mora. Solving polynomial equation systems. Vol. IV. Buchberger theory and beyond, volume 158 of

Encyclopedia of Mathematics and its Applications. Cambridge University Press, Cambridge, 2016.

[182] William W. Adams and Philippe Loustaunau. An introduction to Gr¨obner bases, volume 3 of Graduate Studies

in Mathematics. American Mathematical Society, Providence, RI, 1994.

[183] Thomas Becker and Volker Weispfenning. Gr¨obner bases, volume 141 of Graduate Texts in Mathematics.
Springer-Verlag, New York, 1993. A computational approach to commutative algebra, In cooperation with
Heinz Kredel.

[184] Ralf Fr¨oberg. An introduction to Gr¨obner bases. Pure and Applied Mathematics (New York). John Wiley &

Sons, Ltd., Chichester, 1997.

[185] Franz Winkler. Polynomial algorithms in computer algebra. Texts and Monographs in Symbolic Computation.

Springer-Verlag, Vienna, 1996.

[186] Wolfram Decker and Christoph Lossen. Computing in algebraic geometry, volume 16 of Algorithms and
Computation in Mathematics. Springer-Verlag, Berlin; Hindustan Book Agency, New Delhi, 2006. A quick
start using SINGULAR.

[187] Wolfram Decker and Gerhard Pﬁster. A ﬁrst course in computational algebraic geometry. African Institute

of Mathematics (AIMS) Library Series. Cambridge University Press, Cambridge, 2013.

[188] Martin Kreuzer and Lorenzo Robbiano. Computational commutative algebra. 1. Springer-Verlag, Berlin, 2000.

[189] Martin Kreuzer and Lorenzo Robbiano. Computational commutative algebra. 2. Springer-Verlag, Berlin, 2005.

[190] Martin Kreuzer and Lorenzo Robbiano. Computational commutative algebra 1. Springer-Verlag, Berlin, 2008.

Corrected reprint of the 2000 original.

[191] Martin Kreuzer and Lorenzo Robbiano. Computational linear and commutative algebra. Springer, Cham,

2016.

[192] Daniel R. Grayson and Michael E. Stillman. Macaulay2, a software system for research in algebraic geometry.

Available at https://math.uiuc.edu/Macaulay2/.

[193] Bernd Sturmfels. Solving systems of polynomial equations, volume 97 of CBMS Regional Conference Series
in Mathematics. Published for the Conference Board of the Mathematical Sciences, Washington, DC; by the
American Mathematical Society, Providence, RI, 2002.

[194] Bernd Sturmfels. Algorithms in invariant theory. Texts and Monographs in Symbolic Computation. Springer-

Verlag, Vienna, 1993.

[195] Bernd Sturmfels. Algorithms in invariant theory. Texts and Monographs in Symbolic Computation. Springer

Wien, New York, Vienna, second edition, 2008.

[196] Karin Gatermann. Computer algebra methods for equivariant dynamical systems, volume 1728 of Lecture

Notes in Mathematics. Springer-Verlag, Berlin, 2000.

26

[197] Mohamed Elkadi and Bernard Mourrain.
Math´ematiques et Applications 59, 2007.

Introduction `a la r´esolution des syst`emes polynomiaux.

[198] Alin Bostan, Fr´ed´eric Chyzak, Marc Giusti, Romain Lebreton, Gr´egoire Lecerf, Bruno Salvy, and ´Eric Schost.

Algorithmes Eﬃcaces en Calcul Formel. HAL Archives Ouvertes, 2017.

[199] Giovanni Pistone, Eva Riccomagno, and Henry P. Wynn. Algebraic statistics, volume 89 of Monographs on
Statistics and Applied Probability. Chapman & Hall/CRC, Boca Raton, FL, 2001. Computational commutative
algebra in statistics.

[200] Seth Sullivant. Algebraic statistics, volume 194 of Graduate Studies in Mathematics. American Mathematical

Society, Providence, RI, 2018.

[201] Hans J. Stetter. Numerical polynomial algebra. Society for Industrial and Applied Mathematics (SIAM),

Philadelphia, PA, 2004.

[202] Alicia Dickenstein and Ioannis Z. Emiris. Solving Polynomial Equations. Foundations, Algorithms, and Ap-
plications, volume 14. Algorithms and Computation in Mathematics, Springer, Berlin, Heidelberg, 2005.

[203] David A. Cox. Applications of polynomial systems, volume 134 of CBMS Regional Conference Series in

Mathematics. American Mathematical Society, Providence, RI, 2020.

[204] David A. Cox, John Little, and Donal O’Shea. Using algebraic geometry, volume 185 of Graduate Texts in

Mathematics. Springer, New York, second edition, 2005.

[205] David A. Cox, John Little, and Donal O’Shea. Ideals, varieties, and algorithms. Undergraduate Texts in
Mathematics. Springer, Cham, fourth edition, 2015. An introduction to computational algebraic geometry
and commutative algebra.

[206] David A. Cox. Galois theory. Pure and Applied Mathematics (Hoboken). John Wiley & Sons, Inc., Hoboken,

NJ, second edition, 2012.

[207] Ian Stewart. Galois theory. CRC Press, Boca Raton, FL, fourth edition, 2015.

[208] Jean-Pierre Tignol. Galois’ theory of algebraic equations. World Scientiﬁc Publishing Co. Pte. Ltd., Hacken-

sack, NJ, second edition, 2016.

[209] Eugene L. Allgower and Kurt Georg. Introduction to Numerical Continuation Methods. SIAM, 2003.

[210] Layne T. Watson. Globally convergent homotopy algorithms for nonlinear systems of equations. Nonlinear

Dynamics, 1(2):143–191, Mar 1990.

[211] Alexander Morgan. Solving Polynomial Systems Using Continuation for Engineering and Scientiﬁc Problems.

SIAM, 2009.

[212] Werner C Rheinboldt. An adaptive continuation process for solving systems of nonlinear equations. University

of Maryland, 1975.

[213] Tianran Chen and Tien-Yien Li. Homotopy continuation method for solving systems of nonlinear and poly-

nomial equations. Communications in Information and Systems, 15(2):119–307, 2015.

[214] Tianran Chen and Tien-Yien Li. Spherical projective path tracking for homotopy continuation methods.

Communications in Information and Systems, 12(3):195–220, 2012.

[215] Jan Verschelde. Algorithm 795: Phcpack: A general-purpose solver for polynomial systems by homotopy

continuation. ACM Trans. Math. Softw., 25(2):251–276, jun 1999.

[216] Daniel J. Bates, Jonathan D. Haunstein, Andrew J. Sommese, and Charles W. Wampler. Numerically Solving

Polynomial Systems with Bertini. Society for Industrial and Applied Mathematics, USA, 2013.

[217] Andrew Sommese and Charles Wampler. The Numerical Solution of Systems of Polynomials Arising in

Engineering and Science. 03 2005.

27

[218] Timothy Duﬀ, Cvetelina Hill, Anders Jensen, Kisun Lee, Anton Leykin, and Jeﬀ Sommars. Solving polynomial
systems via homotopy continuation and monodromy. IMA Journal of Numerical Analysis, 39(3):1421–1446,
04 2018.

[219] Ramon E Moore. Interval analysis. Prentice-Hall, 1966.

[220] Arnold Neumaier. Interval Methods for Systems of Equations. Encyclopedia of Mathematics and its Applica-

tions. Cambridge University Press, 1991.

[221] Ramon E. Moore, R. Baker Kearfott, and Michael J. Cloud. Introduction to Interval Analysis. Society for

Industrial and Applied Mathematics, USA, 2009.

[222] Vladik Kreinovich. Solving equations (and systems of equations) under uncertainty: how diﬀerent practical
problems lead to diﬀerent mathematical and computational formulations. Granular Computing, 1(3):171–179,
Sep 2016.

[223] IEEE. IEEE 1788-2015 – IEEE standard for interval arithmetic. https://standards.ieee.org/standard/

1788-2015.html, 2015.

[224] Helmut Ratschek and Jon G. Rokne. Interval global optimization. In Christodoulos A. Floudas and Panos M.

Pardalos, editors, Encyclopedia of Optimization, Second Edition, pages 1739–1757. Springer, 2009.

[225] Matthew Stuber, V. Kumar, and P. Barton. Nonsmooth exclusion test for ﬁnding all solutions of nonlinear

equations. BIT Numerical Mathematics, 50:885–917, 12 2010.

[226] G. William Walster and Eldon R. Hansen. Global optimization using interval analysis: Revised and expanded.

2007.

[227] Pascal Van Hentenryck, David McAllester, and Deepak Kapur. Solving polynomial systems using a branch

and prune approach. SIAM Journal on Numerical Analysis, 34(2):797–827, 1997.

[228] Kiyotaka Yamamura and Koki Suda. An eﬃcient algorithm for ﬁnding all solutions of separable systems of

nonlinear equations. BIT Numerical Mathematics, 47:681–691, 2007.

[229] Bartlomiej Jacek Kubica. Excluding regions using sobol sequences in an interval branch-and-prune method

for nonlinear systems. Reliab. Comput., 19(4):385–397, 2013.

[230] G. William Walster and Eldon R. Hansen. Us6915321b2: Method and apparatus for solving systems of

nonlinear equations using interval arithmetic, 2002.

[231] Yves Papegay, David Daney, and Jean-Pierre Merlet. Parallel implementation of interval analysis for equations
solving. In Jack Dongarra, Domenico Laforenza, and Salvatore Orlando, editors, Recent Advances in Parallel
Virtual Machine and Message Passing Interface, pages 555–559, Berlin, Heidelberg, 2003. Springer Berlin
Heidelberg.

[232] Max E. Jerrell. Finding and verifying all solutions of a system of nonlinear equations. IFAC Proceedings
Volumes, 31(16):327–332, 1998. IFAC Symposium on Computation in Economics, Finance and Engineering:
Economic Systems, Cambridge, UK, 29 June - 1 July.

[233] Max E Jerrell and Wendy A Campione. Finding and verifying all solutions of a system of nonlinear equations

using public domain software. 2002.

[234] Tahereh Eftekhari. Interval extensions of the halley method and its modiﬁed method for ﬁnding enclosures
of roots of nonlinear equations. Computational Methods for Diﬀerential Equations, 8(2):222–235, 2020.

[235] Olivier Didrit, Michel Petitot, and Eric Walter. Guaranteed solution of direct kinematic problems for general
conﬁgurations of parallel manipulators. IEEE Transactions on Robotics and Automation, 14(2):259–266, 1998.

[236] A Castellet and F Thomas. An algorithm for the solution of inverse kinematics problems based on an interval

method. In Advances in Robot Kinematics: Analysis and Control, pages 393–402. Springer, 1998.

[237] Kiyotaka Yamamura, Hitomi Kawata, and Ai Tokue. Interval solution of nonlinear equations using linear

programming. BIT Numerical Mathematics, 38(1):186–199, Mar 1998.

28

[238] Kiyotaka Yamamura and Tsuyoshi Fujioka. Finding all solutions of nonlinear equations using the dual simplex
method. Journal of Computational and Applied Mathematics, 152(1):587–595, 2003. Proceedings of the
International Conference on Recent Advances in Computational Mathematics.

[239] Kiyotaka Yamamura, Koki Suda, and Naoya Tamura. Lp narrowing: A new strategy for ﬁnding all solutions

of nonlinear equations. Applied Mathematics and Computation, 215(1):405–413, 2009.

[240] Lubomir V Kolev. An improved method for global solution of non-linear systems. Reliable Computing,

5(2):103–111, 1999.

[241] A. Babichev, O. Kadyrova, T. Kashevarova, A. Leshchenko, and A. Semenov. Unicalc, a novel approach to

solving systems of algebraic equations. Interval Computations, 1993, 01 1993.

[242] Laurent Granvilliers and Fr´ed´eric Benhamou. Algorithm 852: Realpaver: An interval solver using constraint

satisfaction techniques. ACM Trans. Math. Softw., 32(1):138–156, mar 2006.

29

