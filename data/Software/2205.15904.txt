2
2
0
2

n
u
J

3

]
E
S
.
s
c
[

2
v
4
0
9
5
1
.
5
0
2
2
:
v
i
X
r
a

SYNTHESIZING CONFIGURATION TACTICS FOR EXERCISING
HIDDEN OPTIONS IN SERVERLESS SYSTEMS

PREPRINT OF https://doi.org/10.1007/978-3-031-07481-3_5

Jörn Kuhlenkamp
ISE, TU Berlin, Germany
jk@ise.tu-berlin.de

Sebastian Werner∗
ISE, TU Berlin, Germany
sw@ise.tu-berlin.de
Stefan Tai
ISE, TU Berlin, Germany
st@ise.tu-berlin.de

ABSTRACT

Chin Hong Tran
ISE, TU Berlin, Germany
ct@ise.tu-berlin.de

A proper conﬁguration of an information system can ensure accuracy and efﬁciency, among other
system objectives. Conversely, a poor conﬁguration can have a signiﬁcant negative impact on
the system’s performance, reliability, and cost. Serverless systems, which are comprised of many
functions and managed services, especially risk exposure to misconﬁgurations, with many provider-
and platform-speciﬁc, often intransparent and ‘hidden’ settings. In this paper, we argue to pay close
attention to the conﬁguration of serverless systems to exercise options with known accuracy, cost
and time. Based on a literature study and long-term serverless systems development experience, we
present nine tactics to unlock potentially neglected and unknown options in serverless systems.

Keywords serverless system · conﬁguration methods · design tactics

1

Introduction

Serverless computing is a new cloud provisioning model appealing to cloud consumers due to increased automation of
operational tasks, instant scalability with incoming workload, and no costs for idle computing resources. Consequently,
serverless computing promises developers the means for delivering more features for applications in shorter lead times
at a lower cost [16].

In the serverless programming model [9], developers build serverless systems as event-driven compositions of (cloud)
functions and additional, managed cloud services, such as object storage, messaging queues, and databases. Cloud func-
tions run in a Function-as-a-Service platform such as AWS Lambda, Google Cloud Functions, or Apache OpenWhisk.
However, academia and industry report that client-side quality in terms of performance, reliability, and execution costs
frequently and signiﬁcantly mismatches developers’ expectations due to poor conﬁgurations [14, 15] making ignoring
conﬁguration particularly risky.

Ironically, conﬁguring serverless systems "accurately" requires developers to understand a cloud platform’s quality-
sensitive conﬁguration options hidden by the programming model’s high abstraction. To this end, related work proposes
conﬁguration methods [19, 8, 2, 6]. However, these conﬁguration methods may not live up to developers’ expectations,
as they focus on accuracy but tend to neglect required cost and time. Furthermore, accuracy with these methods
typically depends on implicit assumptions regarding a system’s architecture and deployment environment. We ﬁnd it
not convincing to assume that developers that select the serverless computing paradigm to leverage cost and lead-time
beneﬁts buy into a conﬁguration approach that demands unknown cost and time investments.

In this context, we deﬁne the research question: How can developers conﬁgure serverless systems with predictable
accuracy, cost, and time? Towards this, we present two contributions, (i) a literature review that indicates that
accuracy, cost and time of (current) conﬁguration methods are too in-transparent, and (ii) nine tactics for designing
future conﬁguration methods with known accuracy, cost and time effectively synthesizing the state-of-the-art.

1

 
 
 
 
 
 
2 Conﬁguration Fundamentals

Through introducing fundamentals of system conﬁguration methods’, we lay the foundation for reviewing existing
related work (Sec. 3), introducing tasks, and quality dimensions, see also Figure 1.

M e m ory
Batch

Fusion

Place m ent

C oncurrency
B ounds

W eights

Isolated

C hain

C o m plex

(cid:51)

Saha[19]
-
Christoforou[5] (cid:51) (cid:51)
(cid:51)
-
Ali[3]
(cid:51)
-
Elgamal[8]
-
-
Schuler[22]
(cid:51)
-
Akhtar[2]
-
-
Tariq[25]
-
-
Sánchez-A.[20]
(cid:51)
-
Eismann[7]
Sedefouglu[23] (cid:51)
-
First-Author

-
-
-
-
-
-
(cid:51) (cid:51)
-
-
(cid:51)
-
-
-
-
-
-
-
-
-

-
-
(cid:51)
-
(cid:51)
-
(cid:51)
-
(cid:51)
-
(cid:51)
-
(cid:51) (cid:51)
(cid:51) (cid:51)
-
-
-
-

(cid:51)
-
-
(cid:51)
-
-
(cid:51)
-
-
(cid:51) (cid:51)
-
(cid:51)
-
-
(cid:51) (cid:51)
-
(cid:51) (cid:51)
-
-
-
-
(cid:51) (cid:51)
-
(cid:51)
-
-

-
-
-
(cid:51)
-
-
(cid:51)
(cid:51)
-
-

Knob

Goal

Composition

Table 1: Comparison of conﬁguration meth-
ods.

Figure 1: Exemplary conﬁguration method on Lambda.

2.1 Tasks

Developers conﬁgure systems to meet quality goals. To that extent, a conﬁguration method takes as input a system
under conﬁguration (SUC), knobs, e.g., a conﬁguration parameter, and goals, e.g., a target throughput, and outputs a
policy, e.g., a set of conﬁguration parameters that should meet desired goals. In Figure 1, we show a running example of
sizing "memory" for three functions implementing matrix multiplication on AWS Lambda, with two goals: client-side
request-response latency below 90s and marginal execution cost below 0.1 USD.

To meet both goals, a conﬁguration methods needs to ﬁnd values for knobs, e.g., the memory size of the three functions
in our example. A SUC can expose knobs on the platform- or application-level. However, application-level knobs often
require developers to modify code, e.g., changing libraries or the function composition. With increasing maturity, a
platform typically assimilates relevant application-level knobs to expose them as a feature to foster accessibility [26].

A goal is an expression of a developer’s preferences for a system quality, e.g., a target latency range. Note that
conﬁguration methods assume that a SUC allows observing system qualities included in the goals. A conﬁguration
method comprises three high-level tasks: match, model, and sample resulting in a policy that ﬁts a goal. Matching
ﬁnds a policy that ﬁts provided goals accurately. Thus, matching requires searching all possible conﬁgurations. To
assess how accurately a policy matches goals, matching uses a system quality model that captures the cause-effect
relationship between a policy and goals, e.g., cost and latency as a function of memory size. Modelling is the task of
creating such a system quality model. Computing a realistic system quality model typically requires collecting samples
under realistic assumptions. Sampling is the task of obtaining such samples. A sample is a data record including a
policy and measured system quality, e.g., 8s latency and 0.2 USD cost for 1024MB memory size. While it is possible to
create samples using different approaches, reliable results require experimentation, i.e., making observations on a SUC
in a controlled environment.

2.2 Quality Dimensions

Quality dimensions quantify a conﬁguration method’s quality, and are hence not to confuse with system qualities
used in specifying goals. In order to compare and evaluate conﬁguration methods, we present the three most relevant
conﬁguration method qualities in detail.

Accuracy (A) quantiﬁes how closely a conﬁguration method’s computed policys ﬁt goals. Goals including multiple
system qualities typically require an aggregation function such as a weighted distances function for comparison. Another
aspect is stability, which describes the ability of a conﬁguration method to generalize, i.e., maintaining accuracy for
changing SUCs. Costs (C) comprises all monetary costs for executing a conﬁguration method. It is vital to evaluate
not only matching cost but also the modelling and sampling cost. Otherwise, a conﬁguration method implicitly claims
that system quality models are reusable and generalize well over arbitrary SUCs. Time (T) is the difference between
applying a computed policy and starting a conﬁguration method. A conﬁguration method that is fully automated through
a conﬁguration system typically allows to easily quantify time for conﬁguration requests. Time for all tasks is relevant
because a conﬁguration method can cache samples and system quality models.

2

Matrix Multiplication on AWS LambdaMemory3x{128,4000 MB}Memory128 MB512 MB4000 MBSampleMemoryLatencyModelMatchConfiguration Method Qualities:oCostoTime oAccuracyServerless System (SUC):Knobs:Quality Goals:Latency <90s Cost <0.1$ConfigurationMethodPolicy:3 Literature Review

Building on Sec. 2, we analyse existing proposals for conﬁguring serverless systems in a structured literature review
(SLR) [12], extending the scope of serverless computing’s literature reviews on other topics [13, 17, 24, 21]. We
seed our search for publications in existing serverless computing literature datasets [21, 13] including only scientiﬁc
publications from the years 2018-2021. Each publication includes at least one conﬁguration method in the context of
serverless computing. Table 1 summarizes the ﬁnal set of publications [19, 5, 3, 8, 20, 22, 2, 7] in chronological order
of appearance.

3.1 Results

SUC A single client’s event typically triggers the execution of multiple downstream functions that form a function
composition. We differentiate between approaches for isolated functions [19, 3, 7, 23], function chains [8, 2], and
complex compositions [5]. While a function chain executes multiple functions sequentially, a complex composition
includes switching and parallel executions. Approaches that conﬁgure functions in isolation and neglect the composition
logic are inclined to propose solutions that do not closely match goals. Complex compositions include perfectly
parallelizable execution ﬂows.Some proposals are tightly coupled to a speciﬁc system,e.g., [20] is tightly coupled to the
concrete implementation of serverless sorting and [22] to the KNative platform [11] hurting general applicability of
approaches. First standalone conﬁguration systems begin to emerge, i.e., Tariq et al. [25] provide a middleware called
Sequoia for their matching algorithm. Such approaches seems promising because they make fewer assumptions on a
SUCs software and deployment architecture.

Knobs Related work addresses ﬁve classes of knobs: memory, batch, fusion, placement, and concurrency. The memory
knob universally represents setting computing resources for a function (sizing) [19, 5, 8, 2, 7, 23]. Some approaches use
small domain size [7, 23] hiding platform options. The batch knob comes in the two variants event baching and input
batching. Event batching [3] collects multiple events before sending them to a single slot, i.e., VM or container. Input
batching [5] collects various inputs in a single event before execution in a single slot. The fusion knob [8] merges a set
of function handlers into a single one. In contrast, the placement [8, 2] knob sets the target deployment environment of
a function, e.g., a data center and edge location. Finally, the concurrency knob determines the number of events that a
function executes in parallel, including two variants. Function concurrency [5, 25] determines the sum of events that all
slots can process at the same time. Slot concurrency [22] determines the number of events that a single slot can process
concurrently, i.e., similarly to multi-threading.

Goals From a developer’s perspective, all tuning knobs can inﬂuence multiple system qualities, including execution
latency, cost, and reliability, but existing proposals commonly focus on a subset of these qualities including throughput
[22], execution latency [19, 5, 3, 8, 20, 2, 7, 23], execution cost [5, 3, 8, 2, 7, 23], and reliability [25]. Absolute quality
bounds [8, 2, 23] and relative quality weights [7] are both usefully means for expressing developer’s quality preferences
that are not considered in combination. Hence, we argue that approaches (currently) limit developers’ ability to express
quality preferences holistically.

Quality Dimensions Not all proposals explicitly discuss quality dimensions [19]. Multiple proposals evaluate accuracy
without cost and time [3, 5, 8, 20, 22]. For example, [5] report high accuracy by quantitative comparisons with an
exhaustive search. We argue that extensive sampling and proﬁling implies high cost [3], and potentially time [5].
Schuler et al. [22] use reinforcement learning that requires 150-600 iterations to stabilize in a policy. For sampling,
each iteration executes 500 requests in ﬂight for 30 seconds. Hence, we infer a stabilization time between 75min and
5h and cost of stabilization time multiplied by average throughput. Tarek et al. [8] propose the knob function fusion
that implies changing the application logic of a SUC, which makes estimating cost and time of the approach difﬁcult.
Most proposals do not discuss how they handle changes to a SUC. Akhtar et al. [2] generate separate quality models
for individual functions using Bayesian optimization, and, in a next step, ﬁnd a suitable conﬁguration for a function
chain leveraging Integer Linear Programming. Notably, modelling only requires 5-15 samples. In contrast, Eismann
et al. [7] require signiﬁcant ex-ante sampling and modelling enabling quick matching at the risk of reduced accuracy
under system changes.

3.2 Discussion

Conﬁguration promises matching an information system’s quality goals accurately at a cost and time investment. To that
extent, conﬁguration methods employ three tasks sampling, modeling, and matching. However, conﬁguration methods
can signiﬁcantly differ in (i) when and how they execute each task and (ii) supported knobs, system qualities, and goals.

3

The fundamental design decisions that make up each conﬁguration method are typically justiﬁed by improving accuracy
or reducing the time or cost of conﬁguration. However, our review indicates that design decisions’ negative impacts
on other conﬁguration method qualities are often hidden. This predominantly manifests in two ways. First, shifting
negative impacts to other tasks. For example, reporting low costs and time for matching at the expense of increased cost
and time for sampling and modeling. Second, making speciﬁc assumptions about the conﬁgured serverless system. For
example, assuming a speciﬁc workload, function composition, or FaaS platform feature to reduce efforts for sampling
and modeling.

Therefore, we believe that it is beneﬁcial to make more explicit the tradeoffs between accuracy, time, and cost that
are hidden in the design decisions of current conﬁguration methods. In this way, tradeoffs and assumptions for a
conﬁguration method as a whole and for individual design decisions become known to researchers and practitioners,
promoting (re)usability. To this end, we propose a modular approach that allows the design decisions of different
conﬁguration methods to be combined to meet the requirements of a particular system context. As a ﬁrst step in this
new direction, we propose the use of tactics, which we present in the next section.

4 Tactics

This section presents nine concrete tactics for engineering the quality dimensions of conﬁguration methods. While
closely related to design patterns, we rely on the word "tactic" because they are not exclusively usable in conﬁguration
systems but also in semi-automated or manual conﬁguration methods. We group the tactics into platform-, knob-, and
application-centric tactics. We use a short-hand notation to indicate a tactic’s impacts on quality dimensions: accuracy
(A), cost (C), and time (T).

Platform-centric Tactics Platform-centric tactics leverage idiosyncrasies of serverless platforms to improve a conﬁgu-
ration method.

T1 - Isolate Executions Processing multiple events concurrently in the same slot/runtime can signiﬁcantly impact
accuracy (A) per event by adding noise to each collected sample, thus, requiring a large number of runs to ﬁlter out
this noise. Therefore, this tactic assumes that a slot only performs a single (isolated) execution at a time. A correct
assumption can reduce the number of runs (C, T) without sacriﬁcing accuracy (A); however, a false assumption runs
the danger of reducing accuracy. We observed that several FaaS platforms fulﬁl the assumptions of this tactic [1, 15].

T2 - Automate Operational Tasks Different runs, i.e, observations, require changing the deployment and conﬁgura-
tion of a serverless system. This tactic uses a FaaS platform’s capabilities to automate associated operational tasks.
Different FaaS platforms require a few seconds to minutes to converge to a new target deployment [14]. If a deployment
change converges quickly, the lag between runs shortens, reducing the overall time for sampling (T). As a downside,
clients can temporarily observe inconsistent deployments in speciﬁc FaaS platforms [14]. Not accounting for this
behaviour risks of mixing samples (A).

T3 - Manifold Testbeds This tactic uses a FaaS platform’s ability to scale multiple deployments independently and
isolated with incoming events. Thus, on these platforms, we can deploy policy-variants in parallel and conduct multiple
runs simultaneously, similar to Joyner et al. [10]. A potential beneﬁt is reducing the overall time of experimentation
(T) without increasing costs (C) due to serverless platforms’ common work-based billing model. However, a FaaS
platform’s default limits, e.g., regarding the maximum number of concurrent executions, as well as cold-starts, can
result in runtime bottlenecks and misleading observations impairing accuracy (A).

Knob-centric Tactics Knob-centric tactics leverage knowledge on the relationship between a knob and its impact on
quality to reduce runs while maintaining high accuracy (A).

T4 - Constant Quality Function This tactic assumes that a system’s quality does not signiﬁcantly change for
different values of a knob. A conﬁguration method can exploit this behaviour by randomly selecting a value for the
knob and thus omitting runs. This assumption typically holds for multiple knobs exposed by FaaS platforms. For
example, conﬁguring functions-tags never impacts system qualities such as latency. Thus, omitting these knobs when
considering new conﬁguration options reduces the time (T) and cost (C). However, validation of these assumptions is
critical to avoid system quality degradation.

T5 - Monotonic Quality Function This tactic assumes that values of a knob’s domain have an inherent order, that
quality changes with this order monotonically, and that a conﬁguration method supports bounds in the goal deﬁnition.

4

The method can exploit this knowledge by omitting runs after observing quality outside of a predeﬁned bound without
reducing accuracy (A). While this tactic can reduce cost (C), it implies that runs execute sequentially, making this tactic
mutually exclusive with (T3). For example, all elements in the domain of the memory knob are ordered based on their
numeric value. For some applications, latency will decrease monotonically for larger memory values. Alternatively, if a
predeﬁned quality bound states that end-to-end latency must be smaller than 1s, a method can omit runs for all smaller
memory values after observing a run with a latency over 1s.

T6 - Quality Function Type This tactic assumes that quality is a function of a knob’s values that follows a known
function type, e.g., a linear or an exponential function. A conﬁguration method can exploit this by not observing a SUC
under all possible values of a knob’s domain but only estimating the parameters of the known function type, which is
typically possible using observations from fewer runs. For example, Akhtar et al. [2] assume that execution latency as a
function of memory follows an exponential decay function.

T7 - Quality Function This tactic assumes that the function type (see T6), including all its parameters, are known.
Consequently, a conﬁguration method can omit runs because the relationship between a knob and quality is known
already. In other words, this tactic (re-)uses an existing quality model entirely, omitting the tasks sampling and
modelling.

Application-centric Tactics Application-centric tactics leverage knowledge on these decisions to reduce modelling
and sampling efforts (C,T) maintaining accuracy (A).

T8 - Composition Type
In a composition, the conﬁguration space rapidly increases with the number of functions. As
an example, if we consider a simple sequence of three functions, the conﬁguration space for the memory knob would be
101123 for AWS Lambda. However, making assumptions on the type of composition allows reducing the conﬁguration
space. In the context of the example, a function chain [4], enables observing each function in isolation, followed by a
suitable aggregation. Precisely, the sum of the individual functions’ execution latencies becomes an estimate of the
composition’s latency, thus reducing cost (C) and time (T). This tactic ﬁnds implicit usage in [8] and [2].

T9 - Workload It is possible to observe signiﬁcantly different qualities for runs with different valid event inputs.
Consequently, maintaining high accuracy requires observing a system under different workloads, therefore, increasing
the number of runs. This tactic leverages knowledge on the impact of an event-inputs on system qualities to reduce the
number of runs and beneﬁt-cost (C) and time (T). Making wrong assumptions reduces the accuracy of conﬁgurations (A).

5 Sizing Middleware

This section presents a novel conﬁguration system (Sizer) that applies tactics from the previous section to support short
lead times of serverless development.

5.1 Requirements

To guide the design and selection of appropriate tactics, we ﬁrst deﬁne the following requirements: A sizing request
outputs a policy proposing slot sizes, a request can comprise goals as bounds and preferences for the four system
qualities throughput, latency, reliability, and platform consumption. Quality preferences are a vector of relative weights
that sum to one and quality bounds using comparison operators, e.g., RLat≤900ms2. The Sizer shall only make minimal
assumptions on a SUC, maintain high accuracy, deﬁned in terms of the sum of weighted distances from an optimal
policy p∗ (sec 2.2) using the recent proposal by Pallas et al. [18], allowing different cloud platforms, composition types,
handler implementations, and workloads. A request shall enable short lead times by returning within minutes with a
small cost per sizing.

5.2 Tactic-based Design

Figure 2 gives a high-level overview of the Sizer’s architecture leverging tactics (T1, T2, T3, T5, T7, T8, T9). To
utilize system information, a Sizer Client submits bounds, preferences, a workload model (T9), and a reference to a
target SUC3 (T8) or an existing quality model (T7) as inputs.

2total request-response latency less then 900ms
3For example, in the case of AWS, a references can be (i) a AWS StepFunction reference or (ii) an reference to a single Lambda

function.

5

ß

Figure 2: High-level Architecture of Sizing Middleware

As a central component, the Sizing Controller ﬁrst determines if an existing quality model can be used or if an experiment
is needed to obtain a suitable quality model. For experiments, the Experiment Controller coordinates the generation of
new samples leveraging a sampling strategy that prioritized time and cost,i.e., exploiting T3 by selecting nsizes ∈ N
sizes with a maximum spacing between them, instead of for example a more accurate but time-consuming approach
such as Bayesian Optimization like Akthar et al. [2]. The FaaS Controller will enact the sampling and experimentations
managed by the Experiment Controller using the Workload Controller which emulates a SUC’s clients respecting T9.
By further applying T1 we can lift the need to observe different target throughputs reducing cost and time, assuming
that a FaaS platform provides high elasticity [15]. The Workload Controller also validates responses and invalidates
deployments (T2). We primarily invalidate response of cold starts the latency introduced by cold starts is not only
inﬂuenced by sizing but also other factors outside of the developers control and thus, only hurts the generated sizing
model. The Observation Controller is than used to augment responses with added telemetry from a cloud platform, e.g.,
a cold start indicator.

Finally, the Sizing Controller invokes the Model Builder that computes quality models for each function (T8), and
provided workload class (T9) and stores them in the Models database. This includes learning ELat as a function
of size assuming an exponential decay function similar to Akhtar et.al. [2].For compositions, observation models
for each function use an aggregation function to compute the ﬁnal sizing model. An aggregation function allows to
analytically determining an operation’s quality only using the quality models of individual functions (T1). For cost
and reliability, the aggregation of compositions is straightforward, however for end-to-end latency, we need to sum
the latency of sequential parts and adding the maximum latency of parallel parts of the execution ﬂow.

Lastly, the Sizing Controller returns the proposed policy to the Sizer Client and optionally forwards it to the FaaS
Controller that applies the policy to a target deployment.

5.3

Implementation

We implemented the design (sec 5.2) in Python targeting GCF and primarily AWS to showcase applicability for
FaaS platforms exposing ﬁne-grained conﬁguration options. We provide a standalone executable with an integrated
web-based frontend and a library for easy integration in a deployment pipeline. Furthermore, the Sizer’s components
can be used individually, such as the Model Builder and Size Finder.

We ensured that all models are ﬁle-based and storable in version-controlled systems such as Git. The FaaS-Controller
leverages the Python-Lambda SDK for AWS, and the Observation Controller uses the AWS Cloud-Watch SDK. We
implemented custom Python logic for the Workload Controller and Composition Model and multiple Size Finder and
Model Builder variants that the Sizing Controller can select.
We implemented a Regression Sizer that ﬁts the exponential decay function to samples using the scipy.optimize4
package. For the Size-Finder, we implemented a weighted sum function (ZF ) to ﬁnd Pareto-optimal solutions of the

4https://docs.scipy.org/doc/scipy/reference/optimize.html

6

Sizing ControllerExperiment ControllerFaaS ControllerWorkload ControllerModel BuilderSize FinderObservation CollectorModelsSamplesTelemetryOperationsInvocationFaaS Platform InterfacesSizing Middlewaremulti-objective optimization problem regarding provided the quality goals. For example, we assume the weights for
ELat5 (w1) and ECostt6 (w2) and obtain for the observed requests R for a size the ZF:

ZF (R) =

1
|R|

(cid:88)

r∈R

w1 · ELatr
maxa∈R(ELata)

+

w2 · ECostr
maxb∈R(ECostb)

For sizing compositions, the system combines quality models of individual functions to ﬁnd the best policy using
the scipy.optimize package, speciﬁcally the generalized simulated annealing algorithm [27]. We published the
implementation of the Sizer GitHub7.

6 Conclusion

Serverless systems require conﬁguration methods to deal with quality-sensitive conﬁguration options. Results from
a literature review indicate that industry and research propose isolated conﬁguration methods that focus on accuracy
but come with implicit assumptions on a system’s architecture and unclear developers’ time and cost investments. We
synthesized the results into nine general tactics to aid developers in the design and evaluation of conﬁguration methods.
We do not claim completeness and invite fellow researchers to add more tactics. For future work, we propose a modular
approach that allows easily combining different tactics to meet the requirements of a particular system context.

References

[1] Alexandru Agache et al., Firecracker: Lightweight virtualization for serverless applications, 17th Symposium on

Networked Systems Design and Implementation, NSDI ’2020, USENIX Association, 2 2020.

[2] Nabeel Akhtar et al., Cose: Conﬁguring serverless functions using statistical learning, IEEE INFOCOM 2020,

2020.

[3] Ahsan Ali et al., Batch: Machine learning inference serving on serverless platforms with adaptive batching,

Proc. SC ’20, SC ’20, IEEE Press, 2020.

[4] Ioana Baldini et al., The serverless trilemma: Function composition for serverless computing, Proc. ACM

SIGPLAN 2017, Onward! 2017, ACM, 2017.

[5] Andreas Christoforou and Andreas S. Andreou, An effective resource management approach in a faas environment,

ESSCA 2018, ESSCA’18, vol. 2330, 2018.

[6] Simon Eismann et al., Predicting the costs of serverless workﬂows, (2020).
[7] Simon Eismann et al., Sizeless: Predicting the optimal size of serverless functions, 2021, preprint.
[8] Tarek Elgamal et al., Costless: Optimizing cost of serverless computing through function fusion and placement,

SEC, SEC’2018, IEEE, 10 2018.

[9] Eric Jonas et al., Cloud programming simpliﬁed: A berkeley view on serverless computing, 2 2019.
[10] Shannon Joyner and other, Ripple: A practical declarative programming framework for serverless compute, 2020.
[11] Nima Kaviani et al., Towards serverless as commodity: A case of knative, Proc. 5th WOSC 2019, ACM, 2019.
[12] Barbara A. Kitchenham, Guidelines for performing systematic literature reviews in software engineering, Tech.

report, 2007.

[13] Jörn Kuhlenkamp and Sebastian Werner, Benchmarking FaaS platforms: Call for community participation,

Proc. 4th WoSC 2018, ACM, 12 2018.

[14] Jörn Kuhlenkamp et al., An evaluation of faas platforms as a foundation for serverless big data processing,

Proc. UCC 2019, ACM, 2019.

[15]

, All but one: Faas platform elasticity revisited, SIGAPP Appl. Comput. Rev. 20 (2020), no. 3.
, The ifs and buts of less is more: A serverless computing reality check, Proc. IC2E 2020, IEEE, 2020.

[16]
[17] Philipp Leitner et al., A mixed-method empirical study of function-as-a-service software development in industrial

practice, Journal of Systems and Software 149 (2018).

5Execution Latency
6Execution Cost
7https://github.com/tawalaya/cat-sizer

7

[18] Frank Pallas, Dimitri Staufer, and Jörn Kuhlenkamp, Evaluating the accuracy of cloud nlp services using
ground-truth experiments, 2020 IEEE International Conference on Big Data (Big Data), 2020, pp. 341–350.
[19] Aakanksha Saha and Sonika Jindal, Emars: Efﬁcient management and allocation of resources in serverless, Proc .

11th CLOUD 2018, 7 2018.

[20] Marc Sánchez-Artigas et al., Primula: A practical shufﬂe/sort operator for serverless computing, Proc. 21st

Middleware 2020, Middleware ’20, ACM, 2020, p. 31–37.

[21] Joel Scheuner and Philipp Leitner, Function-as-a-service performance evaluation: A multivocal literature review,

Systems and Software (2020).

[22] Lucia Schuler et al., Ai-based resource allocation: Reinforcement learning for adaptive auto-scaling in serverless

environments, 2020, preprint.

[23] Özgür Sedefouglu and Hasan Sözer, Cost minimization for deploying serverless functions, Proc. 36th SAC 2021,

ACM, 2021.

[24] Davide Taibi et al., Patterns for serverless functions (function-as-a-service): A multivocal literature review,

Proc. 10th CLOSER 2020, INSTICC, SciTePress, 2020.

[25] Ali Tariq et al., Sequoia: Enabling quality-of-service in serverless computing, Proc. 11th SoCC 2020, ACM, 2020.
[26] Sebastian Werner and Stefan Tai, Application-platform co-design for serverless data processing, Proc. 19th

ICSOC 2021, 2021.

[27] Y Xiang, D.Y Sun, W Fan, and X.G Gong, Generalized simulated annealing algorithm and its application to the

thomson model, Physics Letters A 233 (1997), no. 3, 216–220.

8

