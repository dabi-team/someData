2
2
0
2

y
a
M
3

]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[

1
v
0
5
4
1
0
.
5
0
2
2
:
v
i
X
r
a

New Monitoring Interface for the AMS Experiment

Raheem Karim Hashmania,∗, Maxim Konyushikhinb, Baosong Shanc, Xudong Caib and
Melahat Bilge Demirköza

aMiddle East Technical University (METU), Ankara 06800, Turkey
bMassachusetts Institute of Technology (MIT), Cambridge, Massachusetts 02139, USA
cBeihang University (BUAA), Beijing, 100191, China

A R T I C L E I N F O

A B S T R A C T

Keywords:
Alpha Magnetic Spectrometer
Monitoring data
Time series data analytics
Grafana
InﬂuxDB

The Alpha Magnetic Spectrometer (AMS) is constantly exposed to harsh condition on the ISS.
As such, there is a need to constantly monitor and perform adjustments to ensure the AMS
operates safely and eﬃciently. With the addition of the Upgraded Tracker Thermal Pump System,
the legacy monitoring interface was no longer suitable for use. This paper describes the new
AMS Monitoring Interface (AMI). The AMI is built with state-of-the-art time series database
and analytics software. It uses a custom feeder program to process AMS Raw Data as time
series data points, feeds them into InﬂuxDB databases, and uses Grafana as a visualization
tool. It follows modern design principles, allowing client CPUs to handle the processing work,
distributed creation of AMI dashboards, and up-to-date security protocols. In addition, it oﬀers
a more simple way of modifying the AMI and allows the use of APIs to automate backup and
synchronization. The new AMI has been in use since January 2020 and was a crucial component
in remote shift taking during the COVID-19 pandemic.

1. Introduction

The Alpha Magnetic Spectrometer (AMS) is a general-purpose high-energy particle physics detector. It was
installed on the International Space Station (ISS) on 19 May 2011 to conduct a unique, long-duration mission of
fundamental physics research in space. Its main objectives include searching for antimatter, investigating dark matter,
and analyzing cosmic rays using the 6 detectors onboard. These detectors include the gaseous Xe/CO2 Transition
Radiation Detector (TRD), the nine layer Silicon Tracker, the lead/scintillator ﬁber Electromagnetic Calorimeter
(ECAL), two Time of Flight Detectors (ToF), and the aerogel/NaF Ring Imaging Cherenkov (RICH). At the very
center is a permanent Nd-Fe-B magnet which generates a magnetic ﬁeld of 0.15 T [1]. AMS has been continuously
collecting cosmic ray particles since its installation onboard the ISS and has collected over 200 billion cosmic ray
events.

Due to the extreme conditions in space, a myriad of sensors were installed onboard the AMS that measure the
temperature, pressure, current, voltage, and other health data of its various components. This monitoring data is
transmitted together with the cosmic ray data to the ground with an average transmission speed of 10 MBit/sec [2].

The detector and its components are continuously exposed to space radiation, extreme temperature variations, solar
eﬀects, and changes in the ISS orientation. One of the most challenging areas of the AMS operations is the continuous
real-time monitoring and adjustment of temperatures, voltages, currents, and pressures. Due to the ISS orbit lasting
~93 minutes, these values change drastically, going from maximal to minimal and back again within that time span.
Certain measurements, such as temperature, can change from -60° C to +80° C within a single orbit without the use
of controlled heaters and thermal blankets. As such, real-time monitoring is required to prevent certain values from
going beyond their range of operational values.

The legacy AMS Monitoring Interface (AMI) is a set of software designed to allow quick access to the health
status of the AMS and its various components. Experts at the CERN Payload Operations and Control Center (POCC),
at CERN, Geneva, and the AMS Asia POCC in Taiwan use this interface to monitor the AMS 24 hours a day.

In recent years, the performance of the legacy AMI had degraded and become ineﬃcient, taking upwards of 45
seconds to show certain complex plots. The design philosophy had become outdated, generating static plots from the
backend and pushing them to the frontend website. It used only one of the incoming data streams from the ISS at

∗Corresponding author

raheem.hashmani@cern.ch (R.K. Hashmani); baosong.shan@cern.ch (B. Shan)

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 1 of 16

 
 
 
 
 
 
New Monitoring Interface for the AMS Experiment

any given time and was therefore not taking advantage of the redundancy measures of the AMS’s two main streams:
real-time and buﬀered. It was built with a heavy reliance on scripting and required changes be done by speciﬁc people
who knew how to rework the scripts. Lastly, it had to be supplemented with various other custom-built monitoring
console software to provide additional capabilities. Discussion to replace it with a modern, up-to-date monitoring
system had started to take place since launch.

The ﬁnal push came in 2019. In the latter half of 2014, the old pumps on the mechanically pumped two-phase
CO2 cooling loop [3], which circulated coolant to the Silicon Tracker, reached their designed lifetime. In order to
prolong the lifetime of the AMS experiment, an upgraded cooling system known as the Upgraded Tracker Thermal
Pump System (UTTPS) was designed, manufactured, tested, space qualiﬁed, and deployed to the ISS in November
2019. This upgrade brought along additional sensors to be installed onboard the AMS, increasing the number of data
sources.

Due to the design philosophy of the legacy AMI, large reworks of the program’s scripts would need to be done in
order to accommodate the new data sources. This was the key trigger that lead to the decision to build a new AMS
Monitoring Interface.

The new AMI described in this paper is a state-of-the-art monitoring interface that combines the key features of
the previous software into a uniﬁed system. The goal was to build a fast and modern system using the new tools and
design philosophies developed in recent years. It eﬃciently manages the large amounts of data generated by the AMS,
provides a noticeable speedup in plot generation compared to the legacy AMI, uses resources eﬃciently, allows for
scalability and future-prooﬁng, and enables an easier way to add new data sources without needing large reworks in the
future. In addition to health status, it also supports access to scientiﬁc data such as trigger and detector performance,
including calibration, occupation, and noise levels amongst other statistics, eliminating the need for most additional
monitoring console programs. Finally, it is designed to have all features be fully accessible via a transportable user
interface, allowing complete remote monitoring.

This paper discusses the design philosophies and software used to develop this new AMS Monitoring Interface.

2. AMS Monitoring Data
2.1. Data Delivery

Figure 1 shows an overview of the ﬂight and ground control operations carried out at the POCC. After being
collected, monitoring data is streamed to the ground via the High Rate Data Link (HRDL) to ISS avionics. The ISS
avionics devices then send this data to the Tracking and Data Relay Satellites (TDRS), using the Ku-band antenna,
which then transmit it to ground at White Sands, NM, where it is relayed to AMS computers at NASA’s Marshall Space
Flight Center (MSFC), AL. From there it is then ﬁnally delivered to the AMS Payload Operations and Control Center
(POCC) at CERN, Geneva, Switzerland.

To guarantee reliable delivery from the ISS to the ground, as well as to achieve real-time delivery for some of the
monitoring data, the data is split into two main streams, real-time and buﬀered. The real-time stream’s bandwidth is
low and mostly contains data from the various AMS sensors. The buﬀered stream has a larger bandwidth and contains
both this sensor data and all of the science data. However, it is not real-time and has a time delay. A copy of all the
data is also stored on the AMS laptop onboard the ISS, for backup purposes in case the ISS Ku-band antenna is not
locked on a TRDS satellite or there is data lost during the high rate data link transfer.

At POCC, the monitoring data on the ground appears as tree-like hierarchy of nodes and unique data types
associated with each node. AMS was built with a 4-fold redundancy, so there are many similar, but independent,
sensors onboard. As such, they are categorized using sensor type (denoting measurement type such as temperature),
sensor ID (distinguishing between sensors of the same type), and sensor name (unique names for each of the sensors).
All streamed data is stored at the POCC servers at CERN as binary ﬁles, called AMS Raw Data, and ordered into ﬁle
structures by time.

2.2. Data Types

Diﬀerent data types describe the diﬀerent measurement results from the detector. Alongside science data collected
from the subdetectors, onboard sensors collect various health data measured from these subdetectors and their
electronics. This health data, called housekeeping data, includes temperatures, pressures, pump speeds, voltages,
currents, and data transfer rates. When taken together with select information from science data used to help with

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 2 of 16

New Monitoring Interface for the AMS Experiment

Figure 1: Overview of AMS Operations. The data is sent down following the path clockwise from the upper left, while
commands from the POCC follow this path in reverse (counterclockwise, from the lower left).

monitoring, such as calibration data, occupancy levels, and trigger rates, it is collectively referred to as monitoring
data.

AMS temperatures are measured by 1118 temperature sensors plus an additional 78 sensors added during the
UTTPS upgrade. Their purpose is to ensure that critical hardware temperatures stay within certain ranges of acceptable
values. Due to the AMS’s constant exposure to radiation, extreme temperature variations, changes in ISS orbit and
orientation, solar eﬀects, and docking/undocking of spacecraft, temperature ﬂuctuations must be constantly monitored
so that POCC can assess situations immediately and take action if necessary. Various subsystem cooling/heating
mechanisms can be adjusted to regulate temperature. Figure 2 gives an example of a temperature variation throughout
24 hours for the sensors measuring the temperatures on the outside of the tracker radiators, Pt8bS and Pt11bS, using
one of the new AMI’s Grafana panels.

In addition to temperature sensors, there are a number of voltage and current sensors that ensure large variations
in values do not damage the electronics and power distribution systems. Other housekeeping data generated includes
pressure sensors, such as for the TRD subdetector (CO2 diﬀusion can aﬀect signal generated for events [4]) where the
pressure within the tubes is constantly monitored to determine when CO2 should be resupplied. Another instance is
within the UTTPS, where CO2 liquid to vapor percentage is monitored to ensure optimal cooling. The UTTPS has
additional monitoring as well, such as the pump rotation speeds to ensure optimal performance. If the running pump
suddenly slows, experts at POCC should be alerted to take action and ﬁnd the cause of the slow down, preventing
possible permanent damage.

In addition to the housekeeping data types mentioned, subdetector speciﬁc housekeeping data, ﬁrmware state, and
data rates are also monitored and sent to ground with a time granularity on the order of one second to few minutes.
This data then needs to be processed, visualized and analyzed by the AMS Monitoring Interface.

2.3. Addition of UTTPS

Electronics for the central part of the AMS Silicon Tracker (six out of nine layers) produce approximately 144 W of
heat which need to be withdrawn and dissipated into space [5]. The original Thermal Tracker Cooling System (TTCS)
was a two-phase CO2 system and was mechanically pumping CO2 in its liquid phase. It was designed to maintain the
Tracker temperature between -20° C to +20° C. There were four redundant pumps and two redundant sets of pumping
loops and control electronics.

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 3 of 16

New Monitoring Interface for the AMS Experiment

Figure 2: Variation over the course of 24 hours on 6 December 2021 for two sensors measuring the temperatures at the
center of the tracker radiators. The sensors on the Wake radiator (Pt11bS) and Ram Radiator (Pt8bS) show a variance of
approximately 23.25 °C and 17.37 °C throughout a single ISS orbit, respectively.

In 2014, the TTCS reached its expected lifetime [6]. In order to continue functionality and improve the cooling of
the Tracker, a new Upgraded Thermal Pump System (UTTPS) was designed and built. Towards the end of 2019 and
early 2020, NASA arranged for four extravehicular activities and the UTTPS was successfully installed.

The UTTPS added an additional 78 temperature sensors and 6 absolute pressure sensors. Alongside this, there are
3 hull sensors on each pump to measure the RPM and feedback from the pump controller electronics is monitored for
the voltage and current measurements. Adding these measurements required a complete rework of the existing AMS
Monitoring Interface.

3. Legacy AMS Monitoring Interface

The legacy AMI [7], which was operated until the end of 2019, was a collection of monitoring consoles and
a Relational Database Management System. The relational database was used as the backend, a Web2py Database
Abstraction Layer (DAL) was used to hold the database structure description, a Remote Procedure Call (RPC) interface
allowed client programs to interact with the database, an AMI web interface showcased RRDtool [8] generated static
plots, and a Scanner utility ﬁlled in the database using the RPC interface.

Figure 3 shows an outline of the legacy AMI’s structure. The feeder utility, made using C++, would run individually
for each of the monitoring consoles at POCC, with an additional one being run for the relational database. It scanned
AMS Raw Data and optionally a single stream of data via the POCC multicast. It searched for address numbers and
determined the sensor type, sensor name, sensor ID, and data type before feeding it into the relational database using
the RPC interface. A simpliﬁed layout of the relational database is shown in Figure 4.

The web interface allowed the user to browse, by name, the extended data type table for a given sensor type and
select a time window to query the data points within that time frame. A custom Python script within Web2Py would
then extract the requested data from the relational database and use RRDtool to generate a plot of it, with respect to
the time, as a PNG image. This was done completely on the server side with the images being uploaded to the web
interface. Certain pages on the web interface could show multiple plots either within a single image or with diﬀerent
images in order to show relationships and give a comprehensive view of a subsystem. Such multi-plot pages were made
by modifying the Python script and the DAL interface was used to develop an internal set of functions that would grant
access to the data. Additional monitoring consoles were only accessibly physically inside the POCC and not on the
web interface.

Due to the heavy reliance on scripting, any kind of edit or new plot would require the maintainer of the interface to
modify the code. In addition, the reliance upon the Relational Database Management system and the constant parsing
through the AMS Raw Data showcased an outdated model of monitoring critical AMS health data. Parsing through
the Raw Data ﬁles multiple times for each console, storing it on various tables in a relational database, creating static
PNG images upon refresh took a considerable toll on computing resources, was slow, and overall ineﬃcient. Much of

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 4 of 16

New Monitoring Interface for the AMS Experiment

Figure 3: Structure of the legacy AMS Monitoring Interface. Each monitoring console on the POCC had its own dedicated
feeder which would parse through the Raw Data to get the information the console needed. One of the feeders was
dedicated to feeding the relational database, which powered the old AMI’s web client.

Figure 4: Simpliﬁed layout of the legacy AMI’s Relational Database Management System, adapted from [7].

the processing, both from the backend (retrieving the data) and frontend (generating the plots to show the data) took
place on the server side, which further represented an outdated model for monitoring data.

Additionally, implementing certain functions such as extracting data points as a CSV ﬁle for further processing,
emailing warning alerts when temperatures were reaching their limits, or building heatmaps for certain housekeeping
data needed custom monitoring consoles to be built each time. Furthermore, the UTTPS upgrade brought new
measurements, which required massive rewrites in the scripts to integrate the data into pre-existing pages and create
entirely new pages on the web interface.

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 5 of 16

AMS Raw DataFeeder 3Feeder NFeeder 2Feeder N+1Feeder 1Monitoring Console 1 @ POCCMonitoring Console 2 @ POCCMonitoring Console 3 @ POCCMonitoring Console N @ POCCRelational Database @ POCCWeb ClientNew Monitoring Interface for the AMS Experiment

The UTTPS upgrade, along with the general slowness and ineﬃciency of the legacy AMI model, prompted the

need for an upgrade to the AMS Monitoring Interface system.

4. New AMI System Structure

Figure 5 outlines the structure of the new AMI. The new AMI uses the open-source, publicly available, time series
database InﬂuxDB as its backend and the time series data analytics software, Grafana, as the frontend. It is designed
such that Raw Data is parsed only once by the feeder and then subsequently passed to two independent InﬂuxDB
databases, one hosted on CERN’s Database on Demand (DBOD) [9] servers and an identical one hosted on POCC’s
servers, for dual redundancy. Two Grafana instances are then independently connected to each of the InﬂuxDBs, but
the plots and dashboards are kept the same via Git synchronization occurring once a day. The CERN version of Grafana
is hosted on the OpenShift Platform [10] while the POCC version is hosted on the POCC servers. The CERN version
therefore acts as a web client for the general AMS members while the identical POCC version is used as a replacement
for the monitoring consoles at POCC.

Figure 5: Structure of the new AMS Monitoring Interface. The Raw Data is passed to the feeder, which then ﬁlls two
InﬂuxDB databases, one stored on the CERN servers, one stored on the POCC’s servers, for redundancy. Both have
independent Grafana instances that display the data, with both Grafanas using Git to synchronize once per day. The CERN
instance can then be accessed by the internet for the general AMS members while the POCC instance is used for the
monitoring consoles at POCC.

5. Backend Design

A new feeder program was developed using C that can parse through AMS raw ﬁles or a data stream via the POCC’s
multicast, identify key sensor IDs, extract sensor data, sensor type, data type, their respective values and timestamp,
and send the values to the InﬂuxDB database. Figure 6 shows an outline of the new AMI database structure.

The C program routinely scans for new raw ﬁles, with each ﬁle representing one minute of the data streams, and
parses through them to ﬁnd the relevant data. It sets the ﬁeld key to be the measurement, such as temperature, voltage,
or pressure, with the ﬁeld value being the corresponding measurement’s value, and sets various key tags identifying
the name, id, or type of measurement.

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 6 of 16

AMS Raw DataFeederInﬂuxDB @ CERNInﬂuxDB @ POCCGrafana @ CERNGrafana @ POCCWeb ClientMonitoring Consoles @ POCCSyncNew Monitoring Interface for the AMS Experiment

Figure 6: Simpliﬁed outline of the new AMI InﬂuxDB database. A single database or multiple databases can be used
simultaneously. Each of these databases contain time series data in the form of data points, which have 3 major categories:
Data Type which includes the measurement name (for example, temperature) and value, the corresponding timestamp,
and indexed keys, which are ﬂexible tags that allow the querying and displaying of entire groups of data points, such as
"all temperature sensors".

To send a data point with accompanying timestamp, ﬁeld, and keys, InﬂuxDB’s HTTPS line protocol is used with
the CERN and POCC databases listening for https requests on select ports. The C program creates a string with the
relevant information, identiﬁes which database server to send the data to, takes in credentials and a port number and
ﬁnally uses the line protocol to submit this information. The database then receives it and stores the information.

Once this process is complete, the raw ﬁles no longer need to be processed and are kept in storage. Data from
the database can then be accessed directly via commands to the database by users at POCC, or via a time series data
analytics software (see Section 6).

5.1. Beneﬁts of using a Time Series Database

A time series database is optimized for storing data points indexed with an inherent temporal order, otherwise
known as a time series, with associated pairs of times and a corresponding value generated at that time, as formalized
in 2002 [11]. Since then, time series and related databases have been used in a variety of ﬁelds including medicine [12],
weather prediction [13, 14], computer generated graphics [15], and even computing resource management at ATLAS
[16].

With relational databases, data with similar features are organized into tables and linked with relationships, as
shown with the legacy AMI. They are most eﬃcient when data points have large correlation with one another and
there is a need to edit data points after insertion. AMS monitoring data, however, has little need to be edited after
insertion and has high cardinality. This causes relational databases to perform poorly, since long tables can no longer
be cached resulting in data querying taking signiﬁcantly long periods of time.

With the time series data format, however, the issue of cardinality is reduced because the format is optimized
to handle data points with time stamps. Time series data have natural temporal ordering with relationships deﬁned
by simple time measurements, data values, ﬁelds describing data types, and keys adding metadata. While this limits
the type of relationship data points can have, it simpliﬁes the database structure layout (see Figure 6) and increases
eﬃciency by allowing chunks of data, or shards, within a selected time frame to be located and prepared for querying,
allowing faster information retrieval [17].

Additional beneﬁts of time series data include the ability to aggregate and downsample data points over time,
track and monitor time series, process series for further tasks, perform various math operations (depending upon the

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 7 of 16

Database 1Data Point 2Indexed KeysData TypesDatabase nData Point 1Data Point nSensor ID IntSensor Name StringSensor Type StringTimestampMeasurementStringValue DoubleNew Monitoring Interface for the AMS Experiment

exact time series database used), export data for further processing (see Section 7.2), and display them using various
monitoring tools.

Due to the ease in which the database can be ﬁlled, the addition of future new measurements of data, similar to how
the UTTPS upgrade brought additional measurements, is a straightforward process. Large rewrites of the scripts are
no longer necessary, and instead, simple additional clauses in the feeding scripts can allow it to search for additional
new measurements and add them to the database with corresponding ﬁeld-key pairs and metadata tags.

Treating housekeeping and certain science data as a time series puts the emphasis on the timestamp of a datum.
This allows the seamless merging of the diﬀerent data streams from the ISS, which originally start as a single stream.
The smaller and faster real-time stream is processed ﬁrst to ﬁll in the database while any gaps between data points
are ﬁlled later as the slower, buﬀered stream arrives and is processed. Additionally, duplicated data points can easily
be overwritten based on their timestamp, negating the need for an extensive search of the original data. Finally, time
series database structure allows users to query only points of interest, selected by the ﬁeld-key pairs and metadata tags
and bounded by the time range chosen.

This method reﬂects a more up-to-date form of processing and storing monitoring data.

5.2. Resources Beneﬁt

With the new feeding mechanism and time series database, AMS raw ﬁles are processed only once to extract the
relevant data points to store within the dual redundant databases. Additional AMS raw ﬁles do not require the processing
of previous ﬁles to get more cohesive outputs, unlike the legacy AMI. This signiﬁcantly reduces the number of times
the large raw ﬁles are parsed, reducing load on the servers and preventing the need for storing the large raw ﬁles on
the server, saving hard disk space.

5.3. Beneﬁts of InﬂuxDB over Alternatives

InﬂuxDB is an open source time series database written in the Go programming language with no external
dependencies [18], allowing for the eﬃcient development of custom pre-compiled binaries and easy deployment
on various servers and operating systems. It uses a custom-built Time-Structured Merge Tree storage engine which
compresses series data in a columnar format, storing only the diﬀerences between values in a series. This makes it
highly eﬃcient for both storage and querying of data and allows AMS to eﬀortlessly store half a terabyte of raw data
per year on a single server. It uses an SQL-like query language, making it simple to retrieve data, and supports a large
number of aggregation and math functions during querying [18].

When compared to other time series databases, previous studies have shown InﬂuxDB to be far simpler to use
and more eﬃcient even on low powered devices [17]. While InﬂuxDB does not have the fastest insertion time, it
has the fastest data querying compared to competing time series databases such as TimescaleDB [19]. This is more
advantageous for the AMS Monitoring Interface as inserting data has been reduced to a single pass per AMS raw ﬁle,
but data is constantly queried every few seconds. Finally, InﬂuxDB has consistently ranked as the most popular time
series database on DB Engines rankings, outshining competitors like Prometheus and Graphite [20].

6. Frontend Design

The new AMI was designed with the intent to build one frontend to eventually replace the various monitoring
consoles at POCC. Grafana was chosen as a suitable candidate due to its focus on time-series data analytics, support
for various plugins, and ability to display data in a variety of formats (see section 7).

6.1. Beneﬁts of using Time Series Data Analytics Software

Plots and similar visualizations are used to better understand the monitoring data, make comparisons, understand
trends, and spot abnormalities. Grafana is the open source, web-based, interactive visualization and dashboard data
analytics software selected for this task [21]. It has built-in support for InﬂuxDB, along with many other database
software, and allows for easy reading of their data without the need for programming.

While the legacy AMI relied upon a single script manager to modify the plots and pages that organize them,
due to the simple web interface nature, Grafana can be modiﬁed by the detector experts of AMS, without requiring
programming experience. This substantially improves update speed since changes to a particular subdetector’s plots
can be tasked directly to the team that manages the monitoring of said subdetector. This improves usability as teams
can create dashboards to ﬁt their individual needs and allows for an overall collaborative, concurrent design of the
AMI. This prevents responsibility from falling on a single person, eliminating the risk of a single point of failure.

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 8 of 16

New Monitoring Interface for the AMS Experiment

Furthermore, unlike the legacy AMI, the new Grafana-based AMI allows for the export of data points into a CSV
text format using the web interface itself, directly from the panels. While such exporting is possible directly from the
InﬂuxDB backend, it often requires the use of its own InﬂuxDB command line interface. Furthermore, it requires direct
access to the backend, which is limited to only the database administrators, for security and stability reasons.

The ability to export data points allows a wider access to the data than ever before, with members no longer needing

proprietary AMS software or bulky AMS raw data to work with monitoring data.

6.2. Resource Beneﬁts

While the legacy AMI followed an outdated strategy of developing the visualizations on the server side and only

displaying them for the user, Grafana follows the more modern approach of letting clients handle the visualizations.

Grafana eﬃciently uses client resources to query data and create plots, which signiﬁcantly reduces the load on the
server as each page refresh no longer causes the server to parse a relational database to build new plots. Instead, the
server simply streams the time series data points while Grafana on the client side takes care of the plotting and visuals.
Since only data points are streamed, the bandwidth is small and allows for fast access and refreshes on the new AMI.
Additionally, due to Git-powered syncing, both Grafana instances are updated when one is modiﬁed, reducing

maintenance cost in terms of manpower.

6.3. Beneﬁts of Grafana over Alternatives

Grafana allows the building of complex plots and supports a variety of display devices, including mobile devices,
without requiring any additional programming. On the client side, since Grafana uses a web interface, clients do not
install anything and can simply point their web browser to the Grafana server URL to access the AMI. This allows for
easy access by all devices and operating systems, without the need for any setup.

Grafana’s simplicity, easy accessibility from multiple devices, and visually attractive plots made it the ideal choice
for the AMI’s time series data analytics software. Its ability to allow collaborative design by authorized users via the
web interface enables diﬀerent monitoring teams to customize their dashboard immediately themselves without the
need for the original programmers to modify the script.

Grafana supports a wide range of visualization tools such as various charts, heatmaps, histograms, pie charts,
gauges, tables, single value stats, node graphs, and more. An example of some of these types of plots can be seen in
Figure 7, which showcases a typical dashboard used in the new AMI (additional examples can be seen in Section 7).
In addition, Grafana has custom plug-in support so missing features can be programmed in. It supports a wide range of
client libraries making its API accessible via Python, JavaScript, Go, Arduino, and more, making it versatile. Finally,
it has support for custom alerts and alarms that can warn predeﬁned users if a value goes above or below a certain
threshold via email, SMS, or instant messengers [21].

The above outlined features allow shift takers to coalesce their diﬀerent monitoring software into a single Grafana
interface due to the wide range of supported visualizations, alerting methods, and support for custom plug-ins. This
streamlines the job of shift takers and improves eﬃciency and response time.

Additional beneﬁts of Grafana over competitors, such as Graphite’s own visualization tool and Kibana, are the
native support for InﬂuxDB and additional support for simultaneous querying of data from diﬀerent databases.
Grafana’s plug and play nature allows multiple database sources to be “plugged” in for easy querying. Replacing
one database with another is simple and accessible via the web interface.

Lastly, the user interface is fully featured and customizable. It allows for simple analytics to be done instantly on
the queried data without the need for the database itself to perform the analytics, features customizability for default
time ranges, refresh speed, and viewing modes, and automatically scales to ﬁt the screen it is being viewed on.

6.4. Security Beneﬁts

While InﬂuxDB is restricted to database administrators, reducing security risks, Grafana is designed to be open
to the AMS collaboration members across the globe. As such, Grafana’s innate security features are used to provide
secure access to members.

Firstly, IP addresses and hostnames can be limited, blacklisted, or whitelisted to prevent speciﬁc sources from
connecting if need be. Secondly, Open Authorization (OAuth) and Lightweight Directory Access Protocol (LDAP)
was set up to allow CERN’s Single Sign On login for the former and enable AMS’s own LDAP authentication for the
latter, each belonging to one of the two Grafana instances. Lastly, individual user permission can be limited to prevent
unnecessary or accidental tampering. Currently, most users are “viewers”, shift designers who can modify panels and

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 9 of 16

New Monitoring Interface for the AMS Experiment

Figure 7: An example of a dashboard commonly used at the POCC. It shows the usage of single value stats (the current
Beta Angle and the Last Update panel), tables (showing ISS rotation and solar array parameters), scatter plots (such as
beta angle), and a custom-made panel (direct downloading of a CSV for the data).

dashboards are “editors”, and admins of the AMI are appropriately titled “admins” with the ability to modify everything
including security setups, backend databases, and more.

The increased security options and the uniﬁcation of the previous monitoring systems into a single Grafana system
had the additional bonus of allowing shift takers to monitor AMS health remotely. This was previously not possible with
the legacy AMI as many monitoring software instances were only running on computers at the POCC. The universal
web interface along with the up-to-standard security protocols allowed shift takers to perform their duties while away
from the POCC, a feature that saw much use during the COVID-19 pandemic.

7. Implementation

Figure 8 shows the homepage of the new AMI, where plots are organized into dashboards, which are then further
organized into panels on the homepage. Each panel is dedicated to a particular shift. This homepage can also be
accessed remotely, facilitating remote shift taking. Grafana’s wide range of visualization tools were used to replicate
many of the original monitoring consoles at POCC, with several plugins being used to further extend the available
tools. This allowed the creation of status maps (Figure 9), histograms (Figure 10), scatter plots (Figure 11), time-series
status indicators (Figure 12), table status indicators (Figure 13), on/oﬀ indicators (Figure 14), and GPS location markers
(Figure 15).

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 10 of 16

New Monitoring Interface for the AMS Experiment

Figure 8: Homepage of the new AMI, showcasing the diﬀerent dashboards that shift takers at the POCC can quickly
access.

Figure 9: Example of a status map being used to display the occupancy of the TRD. The occupancy of each channel can
be seen for any given time. If a particular channel shows black, it indicates the channel has died. Conversely, if a channel
shows the same color for a long period of time, it can indicate a large amount of noise is being consistently detected.
Here, blue represents the lowest, nonzero, occupancy, while maroon shows the highest occupancy. Note: the status map
was added via the statusmap plugin [22].

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 11 of 16

New Monitoring Interface for the AMS Experiment

Figure 10: An example of histograms. The Silicon Tracker is calibrated twice per orbit, when ISS crosses the equator.
During this time, the pedestal and noise levels are measured for each channel, represented as histograms in this ﬁgure.

Figure 11: An example of a panel that uses a silicon tracker layer’s channels instead of time as the x-axis. This panel is a
scatter plot used to showcase the sigma value for each of the layer’s channel using the scatter plugin [23].

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 12 of 16

New Monitoring Interface for the AMS Experiment

Figure 12: An example of detector status monitoring, in this case the UTTPS, and basic math calculations. When the
UTTPS is turned on, the pump current is displayed while the pump status is set to 1. We can see that, when the pump
is shut oﬀ, the status and current become 0. The green points represent the weighted diﬀerence between the average and
the reference current, an example of how Grafana can perform math on the data points directly, without it being a part of
the database.

Figure 13: An example of the table format being used as a status indicator. Whenever the ToF detector experiences a bit
ﬂip or reboots, certain entries show as red. When everything is nominal again, the status returns to green, allowing an
at-a-glance look at the status of the detector.

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 13 of 16

New Monitoring Interface for the AMS Experiment

Figure 14: An example of an on/oﬀ time indicator. The UTTPS has several heaters, six of which are shown here with
their speciﬁc IDs on the left. At a glance, we can see that for the duration of 12 hours, RAP_A was consistently on and
FAC_B alternated between on and oﬀ for the ﬁrst 6 hours and then, like the other heaters, was oﬀ.

Figure 15: An example of a Grafana panel being used to display the the ISS’s GPS coordinates as a line on an interactive
map using the TrackMap plugin [24].

7.1. Scalability

With the more modern method of having the server manage the database data streaming and client CPU manage
the visualization aspects, the new AMI is conﬁgured for easy scalability. Previously, the legacy AMI managed both
the database and visualization aspects, limiting the number of users at any given time due to CPU overload via
visualization requests. With the visualizations oﬄoaded to client CPUs, the number of people that can access the
AMI with reasonably fast speeds concurrently has been signiﬁcantly increased.

In addition, using the dual redundant backend setup, a load balancer can be applied to divert client requests to
whichever backend server has the least load. While the current load levels do not require this, it can be enabled with
relative ease, future prooﬁng the new AMI for the growing number of users.

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 14 of 16

New Monitoring Interface for the AMS Experiment

7.2. Simpliﬁcation of Data Retrieval for Analysis

Since Grafana can be used to directly retrieve a CSV ﬁle of data points, data analysis for certain measurements has

been simpliﬁed, preventing the need for accessing AMS raw ﬁles directly for monitoring data.

A recent example of the new AMS Monitoring Interface being used in such a way was in the scheduling of the
photon trigger. Since photons are neutral, the photon trigger activates when pair production occurs and a photon is
converted into an electron and positron pair. However, their particle tracks are not isolated and often contaminated
with background noise and particles, causing the photon trigger to ﬁre too frequently and suppressing the activation
of other triggers for other particles. To alleviate this issue, a schedule was necessary to understand when the chance
of photons hitting the subdetectors was highest to enable the photon trigger only during those times. In order to do
this, longitude, latitude, Level 1 Trigger times, and AMS livetimes was taken directly from the the new AMI to be
processed. The result was a more sophisticated schedule of when to let the photon trigger be active, improving the
quality of data acquisition.

Another area of beneﬁt is the easy applicability of machine learning algorithms. Plans to implement robotic
monitoring systems that use machine learning to analyze the time series data and make predictions for the automatic
adjustments are currently underway. Long short-term memory [25] and Recurrent Neural Networks [26] specialize in
data points with a temporal order to detect anomalous behavior, which can be used to trigger more sophisticated alerts
automatically. The easy access to monitoring and science data via CSV ﬁles will help in creating training and testing
datasets for these machine learning models.

8. Conclusion

The new AMS Monitoring Interface uses modern tools and design philosophies to better equip experts with the
tools they need to monitor the AMS. It considers the AMS Raw Data points as time series data points, each with a
timestamp, a measurement, and tags to sort and organize various sensor information. It uses InﬂuxDB as the backend
database on a remote server to store, process, retrieve, and display all health data, and uses Grafana as the time series
data analytics software to display interactive, dynamic plots on the client’s computer using the client’s processing
power. This signiﬁcantly speeds up the displaying of plots, allows easy access to the AMI from across the globe, and
simpliﬁes the process of adding new measurements, future-prooﬁng the AMI.

In addition, the new AMI allows users to download data points directly from the database via Grafana, eliminating
the need for processing large AMS ﬁles. Grafana allows for the modiﬁcation and plot building to be done graphically,
eliminating the complexity of scripts and allowing diﬀerent subdetector teams to be in charge of their own plot designs.
Modern security features were implemented for secure access to AMS members around the globe. And lastly, because
both InﬂuxDB and Grafana have APIs that can be used by simple scripts to automate tasks, two AMS Monitoring
Interfaces were set up on AMS and CERN’s servers for dual redundancy and simple scripts were used to keep both
instances of the AMI synchronized.

With a dedicated team formed that is responsible for the general upkeep and software updating of the new AMI,
the new AMI was successfully deployed in January 2020 and has been in use since then at both the CERN and ASIA
POCC.

Acknowledgements

The authors would like to thank Professor Samuel C. C. Ting for his support, Dr. Michael Capell for his invaluable
comments and review of the paper, and the AMS collaboration as a whole for their input and feedback during the
design process of the new AMS Monitoring Interface. This research was supported by the Turkish Energy, Nuclear
and Mineral Research Agency (TENMAK) under Grant No. 2020TAEK(CERN)A5.H1.F5-26.

References
[1] S. Ting, The Alpha Magnetic Spectrometer on the International Space Station, Nuclear Physics B - Proceedings Supplements 243-244 (2013)

12–24.

[2] V. Choutko, A. Egorov, A. Eline, B. S. Shan, Computing Strategy of the AMS Experiment, Journal of Physics: Conference Series 664 (2015)

032029.

[3] A. A. M. Delil, A. A. Woering, B. Verlaat, Development of a mechanically pumped two-phase CO2 cooling loop for the AMS-2 tracker

experiment, Technical Report, NIKHEF, 2002.

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 15 of 16

New Monitoring Interface for the AMS Experiment

[4] M. Heil, K. Andeen, A. Bachlechner, A. Bartoloni, U. Becker, B. Beischer, B. Borgia, C. H. Chung, W. De Boer, H. Gast, I. Gebauer, T. Kirn,
A. Kounine, K. L ¨ Ubelsmeyer, N. Nikonov, A. Obermeier, A. Putze, S. Schael, A. Schulz Von Dratzig, G. Schwering, T. Siedenburg,
F. Spada, W. Sun, V. Vagelli, Z. Weng, S. Zeissler, V. Zhukov, N. Zimmermann, Operations and Alignment of the AMS-02 Transition
Radiation Detector,
in: 3rd International Cosmic Rays Conference, IUPAP, 2013. URL: https://inspirehep.net/record/1412832/
files/icrc2013-1232.pdf.

[5] J. Van Es, A. Pauw, G. Van Donk, T. Zwartbol, AMS02 Tracker Thermal Control System Overview and Spin-Oﬀ for Future Spacecraft Cooling
System Developments, in: 60th International Astronautical Congress 2009, 60th International Astronautical Congress 2009, IAC 2009, 2009.
URL: https://reports.nlr.nl/bitstream/handle/10921/269/TP-2009-699.pdf?sequence=1.

[6] L. Mussolin, B. Bertucci, A. Faba, G. Ambrosi, G. Scolieri, F. Tissi, M. Gaggiotti, G. Morelli, Z. Zhang, V. Koutsenko, C. Solano, A. Kulemzin,
Y. Yu, K. Bollweg, P. B. Mott, C. Clark, T. Urban, H. Ju, T. Siedenburg, C. Hoon Chung, F. De Angelis, Overview of the mechanical, thermal
vacuum and EMI/EMC tests performed for the AMS-02 UTTPS space qualiﬁcation campaign, 20th IEEE Mediterranean Electrotechnical
Conference, MELECON 2020 - Proceedings (2020) 130–135.

[7] G. Alberti, P. Zuccon, AMI: AMS monitoring interface, in: Journal of Physics: Conference Series, volume 331, Institute of Physics Publishing,
2011, p. 082008. URL: https://iopscience.iop.org/article/10.1088/1742-6596/331/8/082008https://iopscience.iop.
org/article/10.1088/1742-6596/331/8/082008/meta. doi:10.1088/1742-6596/331/8/082008.

[8] Tobias Oetiker, About RRDtool, 2017. URL: https://oss.oetiker.ch/rrdtool/.
[9] R. G. Aparicio, D. Gomez, I. C. Coz, D. Wojcik, DataBase on Demand, Journal of Physics: Conference Series 396 (2012) 052034.
[10] A. Lossent, A. Rodriguez Peon, A. Wagner, PaaS for web applications with OpenShift Origin, Journal of Physics: Conference Series 898

(2017) 082037.

[11] J. Lin, E. Keogh, S. Lonardi, P. Patel, Finding motifs in time series, in: Proc. of the 2nd Workshop on Temporal Data Mining, 2002, pp. 53–68.
[12] H. Abe, T. Yamaguchi, Implementing an integrated time-series data mining environment-a case study of medical kdd on chronic hepatitis, in:

1st international conference on complex medical engineering (CME2005), 2005.

[13] A. McGovern, D. Rosendahl, A. Kruger, M. Beaton, R. Brown, K. Droegemeier, Understanding the formation of tornadoes through data
in: 5th conference on artiﬁcial intelligence and its applications to environmental sciences at the American meteorological society,

mining,
2007.

[14] A. McGovern, D. H. Rosendahl, R. A. Brown, K. K. Droegemeier, Identifying predictive multi-dimensional time series motifs: an application

to severe weather prediction, Data Mining and Knowledge Discovery 2010 22:1 22 (2010) 232–258.

[15] B. Celly, V. Zordan, Animated people textures, in: Proc. of 17th International Conference on Computer Animation and Social Agents (CASA),

2004.

[16] T. Beermann, A. Alekseev, D. Baberis, S. Crépé-Renaudin, J. Elmsheuser, I. Glushkov, M. Svatos, A. Vartapetian, P. Vokac, H. Wolters,
Implementation of ATLAS Distributed Computing monitoring dashboards using InﬂuxDB and Grafana, EPJ Web of Conferences 245 (2020)
03031.

[17] M. Fadhel, E. Sekerinski, S. Yao, A Comparison of Time Series Databases for Storing Water Quality Data, Advances in Intelligent Systems

and Computing 909 (2018) 302–313.

[18] InﬂuxDB Inc., InﬂuxDB OSS 2.0 Documentation, 2021. URL: https://docs.influxdata.com/influxdb/v2.0/.
[19] P. Grzesik, D. Mrozek, Comparative Analysis of Time Series Databases in the Context of Edge Computing for Low Power Sensor Networks,

Computational Science – ICCS 2020 12141 (2020) 371.

[20] DB-Engines, DB-Engines Ranking - popularity ranking of time Series DBMS, 2021. URL: https://db-engines.com/en/ranking/

time+series+dbms.

[21] Grafana Labs, Grafana Features, 2021. URL: https://grafana.com/grafana/.
[22] Flant, Statusmap plugin for Grafana, 2022. URL: https://grafana.com/grafana/plugins/flant-statusmap-panel/.
[23] Michael Moore, Scatter, 2022. URL: https://grafana.com/grafana/plugins/michaeldmoore-scatter-panel/.
[24] Carey Metcalfe, TrackMap, 2022. URL: https://grafana.com/grafana/plugins/pr0ps-trackmap-panel/.
[25] P. Malhotra, L. Vig, G. Shroﬀ, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: Proceedings, volume 89,

2015, pp. 89–94.

[26] Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, D. Pei, Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent
Neural Network, in: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19,
Association for Computing Machinery, New York, NY, USA, 2019, p. 2828–2837. URL: https://doi.org/10.1145/3292500.3330672.
doi:10.1145/3292500.3330672.

R. K. Hashmani, M. Konyushikhin, B. S. Shan et al.

Page 16 of 16

