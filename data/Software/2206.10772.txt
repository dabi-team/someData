2
2
0
2

n
u
J

1
2

]
E
S
.
s
c
[

1
v
2
7
7
0
1
.
6
0
2
2
:
v
i
X
r
a

An Empirical Study On Correlation between Readme Content and Project
Popularity

AKHILA SRI MANASA VENIGALLA and SRIDHAR CHIMALAKONDA,
Indian Institute of Technology Tirupati, India

Readme in GitHub repositories serves as a preliminary source of information, and thus helps developers in understanding about the
projects, for reuse or extension. Different types of contextual and structural content, which we refer to as categories of the content
and features in the content respectively, are present in readme files, and could determine the extent of comprehension about project.
Consequently, the structural and contextual aspects of the content could impact the project popularity. Studying the correlation

between the content and project popularity could help in focusing on the aspects that could improve popularity, while designing the
readme files. However, existing studies explore the categories of content and types of features in readme files, and do not explore their
usefulness towards project popularity. Hence, we present an empirical study to understand correlation between readme file content

and project popularity. We perform the study on 1950 readme files of public GitHub projects, spanning across ten programming

languages, and observe that readme files in majority of the popular projects are well organised using lists and images, and comprise

links to external sources. Also, repositories with readme files containing contribution guidelines and references were observed to be

associated with higher popularity.

CCS Concepts: • Software and its engineering → Software libraries and repositories.

Additional Key Words and Phrases: Readme files, Good practices, Features, Categories, Popularity

1 INTRODUCTION

The number of open source projects available on platforms such as GitHub are increasing day-by-day. GitHub alone
hosts more than 200 million repositories, involving more than 83 million developers, as of May, 20221. Developers
across the world contribute to these projects for their development, identifying and resolving bugs, introducing new

features and so on [2, 32]. Parts of the projects, such as certain code snippets or the projects as a whole, are reused

for development of newer projects on GitHub [22]. Readme is one of the important artifacts of the repository, that is

interacted for contribution or reuse, while also supporting further analysis on the repositories [36, 43].

Readme files contain variety of information that includes instructions on using the project, the features of the

repository, contribution guidelines, release information and so on [14, 36], which is used both by developers and

researchers [25, 29, 35]. The information in readme file largely impacts the contribution or usage of a repository [35].

Studies have also revealed that unclear and insufficient readme files have lead to poor developer onboarding and reduced

contributions from developers [35]. Studies on developer discussions, such as gitter platform, have also indicated that

developers consult documentation in the form of readme files for issue resolution and better project comprehension

1https://github.com/about

Authors’ address: Akhila Sri Manasa Venigalla, cs19d504@iittp.ac.in; Sridhar Chimalakonda, ch@iittp.ac.in,
Indian Institute of Technology Tirupati, Tirupati, India.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2022 Association for Computing Machinery.
Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

Venigalla and Chimalakonda

[17]. This in turn impacts the popularity and progress of the repository as introducing new features, resolving bugs

and so on depends on contributors of the repository [40]. Prana et al. have categorised the content in readme files into

seven categories to facilitate better and easier discovery of information present and consequently update the file in

cases of missing categories [36]. The content in readme files has been analysed in a recent study to understand the

factors that impact popularity of academic AI repositories [20]. Patterns in readme files of java-based repositories on

GitHub were identified, and their association with project popularity has also been analysed [30]. While the consistent

changes in readme files were observed to be correlated to project popularity [1], these studies further indicate that

content in readme files could have an impact on the popularity of the repository, along with the frequency of changes

to readme files.

Analysing the content that differs across popular and non-popular readme files could provide insights on useful

and important content that should be present in readme files for better project popularity. This analysis could also

lead to understanding the common practices of logging readme files among the popular projects. However, there exist

very few empirical studies in the literature that study the correlation between content in readme files and properties of

project. While Fan et al. [20] have considered readme file content in assessing the popularity of projects, this study was

specific to academic AI repositories. Also, the study conducted by Liu et al. [30] analyzes the extent of conformance

of readme content to official GitHub guidelines, and compares the presence with number of stars associated with the

project. However, this study is specific to projects in java programming language. Hence, in this paper, we plan to
perform an empirical study on the correlation of readme content with project popularity, irrespective of the type
of repositories. We include fork count, watcher count and pullrequest count, along with star count in determining

popularity of the project, as indicated in the literature [1, 7, 12].

We perform an empirical study to identify

• RQ1 - Correlation between features (structural content) in the readme files and the project popularity.
• RQ2 - Correlation between categories (contextual content) of content present in readme files and the project

popularity

• RQ3 - Features and categories of readme content present in popular repositories, which could be important for

better comprehension of the repositories.

We determine popularity of the repositories by considering stars, forks, watchers and pull requests, which were observed

to be correlated with project popularity in the literature [1, 16, 20, 28]. The RQ1 and RQ2 could provide insights on the

relation between project popularity and each readme category and feature, while RQ3 could provide insights on the

important features and categories that are present/absent in popular versus non-popular repositories. RQ3 could thus

drive us towards a list of important readme categories and features prevalent in popular repositories.

To answer the above research questions, we study the content in readme files of around 2000 GitHub repositories

spanning across top 10 popular programming languages on GitHub, listed in Table 1. The programming languages

were considered based on their order of popularity on GitHub, as we carryout the analysis on GitHub repositories.

This order of popularity was observed to differ with the standard TIOBE Index (presented in Table 1). However, while

order of popularity differed, 7 out of the 10 programming languages considered had the TIOBE Index value less than

10, indicating that majority (70%) of the programming languages considered are widely popular, thus suggesting

generalisation of results of the study. We devise an approach to automatically calculate the feature count present in

readme files for each of the features. The content in readme files could be categorized using Latent Dirichlet Allocation

(LDA) topic modelling approach [4]. LDA approach can be assigned to classify the content into eight categories based

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

3

Language

C
C++
C#
Go
Java
Javascript
PHP
Python
Ruby
Typescript

GH Popularity
Rank
10
6
9
5
3
2
8
1
7
4

TIOBE Index

2
4
5
13
3
7
8
1
16
40

Table 1. The top 10 popular programming languages and their corresponding ranking in terms of popularity

on the seven categories mentioned in [36], and the eighth category corresponding to ‘Others’, which includes content

that does not belong to any of the seven categories. The keywords in each of the categories should be analysed and

labelled accordingly, with the seven categories. Sections in readme files can be identified based on the header notations,

and the content in each section can then be analysed for the presence of keywords corresponding to the eight categories

(seven + others). The content can be labelled with the category based on keywords, using machine learning classifiers

such as SVM [23], Random Forest [9] and so on. A similar approach is presented in the readme classifier provided

by Prana et al. in [36] to categorize the content present in readme files, and hence was used to identify categories in

readme files of the selected repositories. We then compare the distribution of these features and categories with project

popularity, towards answering the research questions.

We define features as the structural components involved in presenting the readme files, such as lists, im-

ages, external links, code components and so on.

We define categories of content based on the type of content present in the readme files, such as installation

instructions, contribution guidelines and so on.

The main contributions of the paper are:

• An empirical analysis of features and categories in 1950 readme files corresponding to 2000 GitHub repositories

across top 10 popular programming languages on GitHub.

• A list of features and categories in readme files, and their correlation with the popularity of repositories. The

results of the study are presented in Section 4.

• A ranked list of important features and categories that could potentially support better project comprehension,

assuming popular repositories to have better comprehension.

The rest of the paper is organised as follows - Section 2 discusses some of the works in the literature that analysed

the popularity of repositories based on various factors. Section 3 describes the methodology followed in the study

and Section 4 discusses the results of the study. The threats to validity in the study are presented in Section 5. The

implications of the study to researchers and practitioners are discussed in Section 6, followed by conclusion in Section 7.

Manuscript submitted to ACM

4

2 RELATED WORK

Venigalla and Chimalakonda

Studies in the literature have analysed project characteristics of open source projects, based on their correlation with

artifacts such as issues [26], pull requests [1] and forks [44] associated with the project, and based on the content in

readme files such as badges [39], keywords [34], and textual categories [20]. We group the most relevant studies in

literature that aim towards understanding project characteristics on the basis of artifacts analysed, into two categories -

(i) corresponding to correlation with artifact features and (ii) corresponding to readme files.

2.1 Studies corresponding to correlation of artifact features and project characteristics

There exists works in the literature that explore the relationship between various characteristics of the repositories and

the features of different artifacts in the repositories.

For example, the technical complexity of language in issue discussions has been analysed, which revealed that

discussions inline with the project-specific language contribute to faster issue resolution rates [26]. Other characteristics

such as determining pull request acceptance based on contributors’ profile [42], issue resolution based on type of

maintenance activity in the project and so on were explored in the literature [33].

Popularity is among the most commonly considered characteristics of the project. Borges et al. have analysed

correlation between different project characteristics such as fork count, commit count, repository age and so on, and

the project popularity of top starred 2500 repositories on GitHub, by considering star count as popularity metric [6]. A

considerable difference in the values of these characteristics was observed in relation to the star-count of the repositories

[6]. These features were further used in generating prediction models to predict the number of stars and ranking of

GitHub repositories [5]. Zhu et al. defined popularity of the projects based on the projects’ fork count and analysed the

variation of fork counts of projects based on the patterns of folder structure in projects [44]. The folder structure and

types were analysed for 140K GitHub projects, where the standard folders such as those corresponding to testing and

usage examples were observed to contribute to increased fork counts [44].

The updates made and their frequency to documentation related files in different types of projects were studied to

assess if they contribute to popularity of the projects, with number of stars, forks and pull requests being considered as

dimensions of popularity [1]. This study revealed that consistent updates to documentation related artifacts contribute to

increased popularity of the projects [1]. Twenty features corresponding to source code, replicability and documentation

of the projects have been studied to understand their impact on the star count of the academic AI repositories [20]. The

relation between different combinations of content in java readme files and star count of the corresponding repositories

has been analysed, to find the most frequently used patterns in readme files of popular java repositories [30].

2.2 Studies corresponding to readme files

Readme files are considered as a preliminary source to understand about a repository, both by researchers and developers.

Readme files have been used to assess and understand various properties of repositories in the literature [20, 30, 39].

Zhang et al. have analysed the content in readme files to detect similarities among the repositories [43]. The quality of

around 290K GitHub repositories from project maintainers’ perspective is assessed by obtaining a degree of correlation

of content corresponding to badges in readme files with project quality [39]. In an attempt to understand types of

content present in Readme files, Prana et al. have analysed the readme content of 393 GitHub repositories. The content

was then classified into seven categories based on the information being discussed in each section of the readme files

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

5

[36]. This categorisation was perceived to be useful to the developers and a classifier has been designed to automatically

detect such categories in the readme files, to ease the process of locating information in readme files [36].

The content in readme files of 77 million GitHub repositories presented by GHTorrent has been analysed for the

presence of links or keywords pointing to "donations", to identify donation platforms [34]. The quality of readme

content has been analysed to understand the quality of software documentation in projects [38]. Ten dimensions that

define quality of a document have been presented and their presence is checked against readme files of 159 GitHub

projects corresponding to R language manually [38]. Towards understanding the factors that influence adoption of CI

tools to Github repositories, the readme content is processed, to extract repository badges, that provide insights on the

nature of the projects, which could consequently help in identifying features of the projects [27].

The readme files, along with the source code of about 1.1K GitHub repositories in the context of AI have been

analysed to identify the features in the readme content that contribute to popularity of the repositories [20]. About

10 features corresponding to the documentation were presented and were analysed for contributions to popularity of

the repositories [20]. The content in readme files of 129 GitHub projects, corresponding to COVID-19, was analysed

to identify the category of projects, in an attempt to identify the relation between different types of bugs and the

project categories, in projects dealing with COVID-19 [37]. Readme files of 14.9K java repositories on GitHub have

been clustered based on the content present in them, to assess their conformance to GitHub official guidelines, to detect

common patterns, and to identify the relationship between star count and the presence of specific groups of content in

the repositories [30].

Features of various artifacts such as frequency of updates, number of contributors and so on were analysed for

their correlation with popularity. Though readme files are one of the important artifacts of the project, there exist

very few studies that assess contents in the readme files, especially towards their contribution to project popularity.

While Fan et al. and Liu et al. explore the impact of different features of readme files on the project popularity, these

studies focuses only on academic ai repositories and java-based repositories, and consider only star count as a metric

for project popularity [20, 30].

Hence, through this study, we wish to extend the scope, to include GitHub repositories irrespective of their domain

and programming language, to consider both structural and contextual characteristics of the readme content. We also

aim to identify if there exists a difference across programming languages with respect to the categories and features

present and their relation with project popularity, which could be determined based on the number of stars, forks,

watchers and pull requests.

3 STUDY DESIGN

The main aim of this paper is to assess the relation between project popularity and the features and the types of content

in readme files. We envision that understanding the features and categories that strongly relate to project popularity

could provide insights on specific type of content and features that could be considered by project developers and
contributors for better project popularity. Assessing the presence of correlation between the features or categories and the
project popularity could indicate the level of impact of these aspects on project popularity.

To this end, we devise our study based on the Goal Question Metric approach [3], which was used for conducting

empirical studies in the literature that compare existing approaches or artifacts [11], presented in Table 2, to explore

the following research questions:

RQ1: Does presence of a specific feature relate to the popularity of the repository?

Manuscript submitted to ACM

6

Goal
Purpose:
Issue:
Object:
Viewpoint:
Context:

Venigalla and Chimalakonda

Question
RQ1: Does presence of a specific feature
impact the popularity of the repository?

Understanding
the impact
of features & categories RQ2: Does presence of a specific category fisher’s exact test p-value
on project popularity
in GitHub projects

Metric
wilcoxon’s rank sum test
p-value, cliff’s delta

impact the popularity of the repository?
RQ3: Which features and categories
determine the popularity
of the repository?

gini importance score,
permutation-based
importance score

Table 2. Summary of Goal Question Metric

We process popular and non-popular repositories to extract the features present in the repository and then analyse

the results using statistical hypothesis tests that support numerical values, to understand if each of the features have

a significant relation with the popularity of the repository. It has been observed that popular projects tend to have

references with links to external sources and other GitHub repositories in their readme files, which were observed to

be minimal among the non-popular repositories. The popular repositories included images in the readme files, which

could provide more details about the projects for users and contributors.

RQ2: Does presence of a specific category relate to the popularity of the repository? The popular and non-
popular repositories were analysed for presence of categories in the readme files. We applied statistical significance

tests, relevant to non-numerical values, to understand the significance of each category in contributing to popularity of

the projects. Popular projects comprised more details in the readme files with respect to the contribution guidelines and

references, which were hardly present in the non-popular projects. However, details about how to use a repository

were present in readme files of majority of the repositories, irrespective of their popularity.

RQ3: Which features and categories vary among the popular and non-popular repositories? Presence of a
feature, or a category in the popular repositories and absence of the same in non-popular repositories, or viceversa, could

indicate that the specific feature or category is capable of determining the popularity of the project. We build a random

forest classifier to identify the important features and categories. To reduce the confusion and increase performance of

the random forest classifier, features are analysed for any possible correlations among them, and one of the correlated

features from each pair of correlated features was dropped while building the random forest classifier. This analysis of

important features and categories revealed that presence of external links, links to other GitHub repositories, references

and license information strongly differentiate the popular repositories from the non-popular ones.

We follow the four phase methodology presented in Figure 1 that deals with data collection, data processing and result

analysis at a higher level, similar to the empirical studies in literature that assess crowd sourced code and open-source

projects [10, 11, 13, 41], or the correlation between readme file content and popularity [20, 30]. For this empirical

study, the data collection process deals with extracting the readme files of GitHub repositories (phase 1), while the

data processing phase deals with feature extraction (phase 2) from, and categorization (phase 3) of, content in readme

files and the result analysis phase deals with analysing the correlation between project popularity and features and

categories using statistical approaches (phase 4). We elaborate each of the four phases below:

3.1 Phase 1: Data Collection

Studies in the literature with similar goals of assessing correlation of presence of certain artifacts have analysed open

source repositories ranging from about 100 to 77 million. The closest relevant works in the literature, that we consider

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

7

Fig. 1. Methodology of the Empirical Study

as a basis for features [20] and categories [36], consider 1.1K and 393 repositories respectively. Another work that

analyses project characteristics based on star count [6] considered 2.5K repositories. Similar to these existing studies,

we collected 2000 GitHub repositories, which could be a sufficient sample of data, to assess the correlation between

readme content and popularity of repositories. These repositories span across the top 10 most popular programming
languages on GitHub2. 200 repositories were collected from each of the 10 programming languages, and the metadata
corresponding to number of stars, forks, watchers and pull requests has been extracted.

As an initial filtering criteria to support selection of repositories, we considered top starred 100 repositories and

100 repositories randomly, with star-count in the range of 0 to 100, from each programming language, to create an

initial datadump of 2000 repositories. The bottom most repository of the top starred 100 repositories across all the
languages had the stargazer count as 51533, as of 10th December 2021. To ensure significant difference between the
popular and non-popular repositories, we make an initial assumption that repositories with star count less than 100

could be considered as non-popular when compared to the popular repositories with at least 5153 stars. This also

satisfies the assumptions in literature which considered repositories with at least 1000 stars to be popular [24]. Thus,

we consider star-count as an initial filter to identify popular and non-popular repositories, as star count is one of the

prominent features to determine popularity [20, 30]. However, as popularity metric could also include other metrics

of the project [1, 16], we later define the popularity metric to consider stars, forks, watchers and pull requests. We

use this metric to assign popularity ranks to the repositories, by considering all 200 repositories across each of the

programming languages. Thus, we consider star count as a prominent feature to determine popularity, but, we do not

limit the popularity rank assignment solely to star count.

For a well-distributed set of non-popular repositories, we extracted the metadata of 100 repositories randomly, with

at least one repository with stargazer count in the range of 1 to 100 from each of the programming languages. While

the popular repositories were considered based on the extreme values of stars (top 100 most starred repositories), the

2https://madnight.github.io/githut/#/pull_requests/2021/4
3https://github.com/rails-api/active_model_serializers

Manuscript submitted to ACM

8

Venigalla and Chimalakonda

Feature description
Number of lists
Number of images

Feature [f(x)]
List
Image
Animated Image Number of animated images
Video
Table
GitHub
Links
Code
Inline Code
Project

Number of video links
Number of tables
Number of github links
Number of external links
Number of code blocks
Number of inline code elements
Number of links to project page

Identification [x]
<ol>& <ul>
<img>
.gif
“video”, “youtu.be” in url sentence
<center>
“github.com” in url
<a>and “url”
Half the number of 3 backticks
<code>& </code>
"project" in "url"

Source format
html
html
html
html
html
html
html
markdown
html
html

Table 3. Features considered for analysis of readme files

extreme values of stars (top 100 least starred repositories) could not be considered for non-popular repositories. This is

because, majority of the least starred repositories had a star count of 0, thus not facilitating well-distributed nature of

the dataset. As a result, a list of 1000 popular and 1000 non-popular repositories across the ten programming languages

have been extracted. This extraction took about 3 working days, due to the range limitations of GitHub API.

We cloned all the 2000 repositories and processed these repositories to extract the readme files of interest across

the repositories. The files with names ‘readme’ and extensions - ‘.txt’, ‘.md’, ‘.rdoc’, ‘.html’, ‘.asciidoc’, ‘.adoc’, ‘.rst’ and

‘.markdown’ have been extracted. We observed that some of the repositories have multiple files with the name ‘readme’

in the sub-folders of the projects, while some repositories do not comprise any files with the name ‘readme’. We limit

the focus of this study to the readme files on the starting page of the GitHub repository, as they first appear while

visiting a repository, and hence consider only those readme files in the root folder of each repository. Thus, we ensure

that projects having readme content on the starting page of the GitHub repository are analysed. This resulted in a

set of 1950 readme files that were analysed. All the readme files were renamed after their parent repository for better

comprehension of the results.

3.2 Phase 2: Feature extraction

We explore different types of components present in the readme files. Fan et al. have presented 10 features in the

documentation dimension in their study on popularity of academic AI repositories [20]. Readme files were considered

for analysis in the study by Fan et al. to obtain insights on the documentation dimension. As we also aim to understand

the impact of components present in readme files on popularity of the repository, similar to the study by Fan et al.

[20], we considered 9 out of the 10 features presented in documentation dimension by Fan et al in this study. We did

not consider ‘has-license’ feature presented in [20], as this feature deals with presence of license information in the

repository, but does not evaluate if the ‘license’ component is present specifically in the readme file. We performed a

manual walkthrough of readme files of 10 repositories, corresponding to languages being considered, on GitHub, to

identify the presence of any other components, other than the 9 features being considered. We observed that readme

files might contain links to external sources which need not be other GitHub repositories or those specific to the project.

Thus, we add the tenth feature, ‘external links’ to the set of features, and extract the 10 features, listed in Table 3, from

each of the repositories.

Considering the ease of extracting features presented in Table 3, such as list, tables, and so on from html files, we
convert the readme files written in markdown format to html using the markdown parser provided by python, the

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

9

Category
What
Why
How
When
Who
References
Contribution Contribution guidelines

Content Examples
Functionalities of the project
Purpose of the project
Installation instructions
Release information and project status
Contributors and project team
API documentation & other useful information

Table 4. Categories considered for analysis of readme files

4 package. The features links, images, animated images, video links, tables, github links, project links, inline code
mistune
and external links were extracted from the html version of the files, while the feature code was extracted from the
markdown version of the files. Each feature, f(x) is extracted based on the presence of identification, x, and the resultant

value, count_feature_fx, for each feature is calculated as follows:

for each_line in html file, if “x” in each_line, count_feature_fx = count_feature_fx + 1

i.e.,

𝑐𝑜𝑢𝑛𝑡𝑥 = (cid:205)𝑙𝑖𝑛𝑒𝑠𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒𝑥

where, countx refers to the number of occurrences of a specific feature f(x) and instancex refers to the presence of the
corresponding identification, x, in the corresponding source files.

3.3 Phase 3: Content Categorization

In order to categorize content in the readme files, we consider seven categories, as presented in [36] by Prana et al.
and the GitHub guidelines5 to create readme files [30]. In the study [36], Prana et al., have proposed seven classes to
categorize the content present in readme files of GitHub repositories. Since, the current study also deals with readme

files in GitHub projects, we see that the categories presented by Prana et al. [36], listed in Table 4, are relevant to the
study. These categories were also observed to be inline with GitHub Recommended Sections for readme files, and were
integrated with the seven categories. The what category corresponds to project description section, how category to
installation and usage section, contribution category to contributing section, and the who category to credits section of
the recommended sections. Almost all the GitHub recommended guidelines for readme files were thus observed to be a

subset of the categories presented in [36], facilitating us to consider both the recommended guidelines as well as the

categories proposed by Prana et al., by identifying the seven categories listed in Table 4 in readme files.

The readme files extracted from each of the repositories are processed, and the section headers and corresponding

content in the files are extracted. This extracted data is processed to remove stop words and generate tokens. Each of the

tokens of a specific section in the readme file are analysed and mapped to one of the seven categories if they are related

to one of the categories. This mapping is done with the help of a pre-trained SVM-based classifier, trained on manually

annotated dataset as a part of READMEClassifier [36]. This classifier helps in identifying the types of categories present

in the readme files, and also supports labelling of a specific section in the readme file with the corresponding category.

4https://mistune.readthedocs.io/en/latest/
5https://guides.github.com/features/wikis/

Manuscript submitted to ACM

10

Venigalla and Chimalakonda

With the readme files as input, a list of categories present in each of the files, based on the tokens in the files were

obtained as output. Also, the categories of the corresponding sections for each readme file were presented as a result

of classification. This data is further processed to include the boolean values ‘0’ and ‘1’ to each of the categories, to

indicate presence or absence of a category in the readme file.

3.4 Phase 4: Result Analysis

The features and categories present in each of the repositories are compared to the popularity of the repositories to

identify possible differences in the correlation of features and categories with respect to popularity. To facilitate this

analysis, we consider popular and non-popular repositories to be two groups of data. While stars were considered as

the basis to extract repositories during data collection, works in the literature have explored varied approaches towards

determining the popularity of repositories. To ensure credibility of repositories in popular and non-popular groups, we

consider a two level popularity ranking mechanism as in [24], where the first level, in Phase 1, considers only stars and

the second level, in this phase (Phase 4), includes other dimensions. Inline with the existing literature, we use the four

dimensions - stars, forks, watchers and pull requests to assign popularity ranks to the repositories [1, 7, 12]. We define

the popularity metric as the sum of these four dimensions, as follows:

popularity = star_rank + fork_rank + watcher_rank + pullrequest_rank

star_rank : position of repository in ascending order of star_count

fork_rank : position of repository in ascending order of fork_count

watcher_rank : position of repository in ascending order of watcher_count

pullrequest_rank : position of repository in ascending order of pullrequest_count

The star_rank, fork_rank, watcher_rank and pullrequest_rank are calculated for all the repositories in each language

in descending order. The repository with largest value is ranked with a larger value than others, with respect to

each dimension. The ranks for all the four dimensions are added to generate popularity value. The popularity value

is then compared across all the repositories corresponding to a specific programming language, and the resultant

popularity_rank is calculated similar to the dimension ranks, where the repository with largest popularity value is

ranked with the highest value. Thus, the repository with larger popularity rank implies better popularity. For example,
6 is 198, with star_rank,
in the repository list of C programming language, the popularity rank of the repository netdata
fork_rank, watcher_rank and pullrequest_rank being 197, 184, 197 and 193 respectively. The top ranked 100 repositories

in terms of popularity are considered as popular repositories, while the rest were considered as non-popular repositories.

While the order of popularity ranks varied slightly when compared to the star count, we observed that the top 100

repositories with respect to the popularity rank belonged to the top starred 100 repositories in each programming

language. Thus, for each programming language, we arrive at a list of popular and non-popular repositories.

Feature Analysis. The count of presence of each feature for all the repositories has been extracted. We then apply
3.4.1
the Wilcoxon’s rank sum test to obtain insights on whether there exists a significant difference in this count among

popular and non-popular repository groups in each programming language.

We start the statistical analysis with a null hypothesis that both the groups of data follow same statistical distribution.

We then assign ranks to each repository in the groups, based on the feature value, considering one feature at once. The

means of ranks are calculated for each group and the difference in the mean values is processed to obtain the p-value.

The z-score, or the standard deviation of the groups is calculated, assuming a near-normal distribution among the

6https://github.com/netdata/netdata

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

11

Wilcoxon’s p-value
>0.05
<0.05
R, Py, Js, J,
C#, Go, C, C++
R, T, Py, P, Js, J,
C#, Go, C, C++

T, P

Effect Size based on Cliff’s delta

Negligible

Small

Medium

Large

T

R, T, Py, P, Js, J,
C#, Go, C, C++
R, T, Py, P, Js, J,
C#, Go, C, C++
R, T, Py, P, Js, J,
C#, Go, C, C++

R, T, P, Js,
J, C#, Go, C, C++
R, P, J, C#,
Go, C, C++
R, T, Py, P, Js, J,
C#, Go, C, C++

R, C#, C, C++

Py, Js, J, Go

P, J

R, Py, Js, C#,
Go, C, C++

P

T

Py

T,Py,Js

T, P

R, Py, Js, J,
C#, Go, C, C++
R, T, Py, P, Js,
J, C#, Go, C, C++

Feature

List

Image

Animation

Video

Table

Github

Links

Project

Inline

Code

R, T, Py, P, Js, J,
C#, Go, C, C++
R, T, Py, P, Js, J,
C#, Go, C, C++

Js, Go, C++

R, T, Py, P, Js, J,
C#, Go, C, C++

T, Py

R, T, Py, P,
J, C#, C

R, T, P, C

Py, J, Go, C++

Js, C#

P

R, T, Py, Js,
C#, Go, C, C++

R, P, Js, J,
C#, Go, C, C++

R, P, J,
C#, Go, C

Py, Js, C++

T

Table 5. Wilcoxon p-value and Cliff’s delta for features being considered, across each of the ten programming languages considered -
C, C++, C#,R: Ruby, T: Typescript, Py: Python, P: PHP, Js: Javascript, J: Java

samples. The probability (p-value) is then determined based on the z-score. Considering the confidence level to be 95%,

we compare the p-value with 𝛼 (0.05). According to Wilcoxon’s rank sum test, if the p-value is less than 𝛼, then the null

hypothesis is rejected, indicating that both the groups differ in their distribution. The wilcoxon’s rank sum test is thus

applied for all the ten features under consideration, for each of the programming languages. While the p-value ranges

have differed across different programming languages, the results of Wilcoxon’s rank sum test indicate that three of

the ten features considered had p-value greater than 0.05 for all programming languages and three other features had

p-value less than 0.05 for all the programming languages.

To further understand the extent of impact of each feature on the popularity of project, beyond the p-value, we apply

Cliff’s delta on both the data groups, as it is observed to well support the hypothesis testing method [31]. As a part

of applying Cliff’s delta on the popular and non-popular repository groups, the probabilities of a feature value of a

random repository in a specific group being greater than the feature value of a repository in another group, and vice

versa are calculated and their difference is obtained. The difference in probabilities is termed as cliff’s delta and the

value determines if the association between the feature and popularity is small, medium, large or negligible.

The result of applying wilcoxon’s rank sum test and cliff’s delta on the popular and non-popular groups of data, for

each feature is presented in Table 5.

3.4.2 Category Analysis. The categories present in readme file of each repository have been identified. The presence or
absence of a category is noted by a boolean value, where ‘0’ indicates absence of the specific category and ‘1’ indicates

the presence of the category. We apply statistical hypothesis tests to understand the impact of each category on the

popularity of the repositories in each programming language. While Wilcoxon’s rank sum test could identify the impact

Manuscript submitted to ACM

12

Venigalla and Chimalakonda

Category

What

Why

How

When

Who

Reference

Contribution

Fisher’s Exact test p-value

<0.05
C, C++, C#, Go,
J, Js, P, Py, R, T

>0.05

C#

C, C++, C#, Go,
J, Js, P, Py, R, T
C, C++, Go, J,
Js, P, Py, R, T
C, C++, C#, Go,
J, Js, P, Py, R, T

J

C, C++, C#, Go,
Js, P, Py, R, T
C, C++, C#, Go,
J, Js, P, Py, R, T
C, C++, C#, Go,
J, Js, P, Py, R, T

Table 6. Fishers exact test p-value for categories being considered

in case of numerical data, the current analysis supports categorical and non-numerical data. Hence, we apply Fisher’s

exact test on the repositories for each of the categories to understand if there exists a significant association between

presence of a category in the readme file and the popularity group of the repository. The Fisher’s exact test forms a

2X2 contingency matrix, with the count of popular and non-popular repositories comprising of a category, and not

comprising of the category respectively. This contingency table is used to calculate the hyper-geometric probability

value (p-value) of the category over two groups of data.

Considering the confidence level to be 95%, the categories with p-value less than 0.05 are considered to be significantly

associated with the popularity of the repositories. The p-value for three and two categories were observed to be less

than 0.05 and greater than 0.05 respectively for all programming languages. The results of the Fisher’s exact test are

presented in Table 6.

Identifying Important Features and Categories. To understand the important features and categories that could
3.4.3
be considered as strongly associated with popularity of a repository in a specific programming language, a random

forest classifier is built individually for set of features and categories. The features and categories that resulted in better

accuracy of the random forest classifier were then considered as important features and categories.

To avoid reduced importance of correlated features in the random forest classifier, we first check for the presence

of collinearity among the features using Spearman’s correlation factor. For every pair of features, the spearman’s

correlation factor 𝜌 is calculated and compared against the threshold. The threshold for 𝜌 has been set to 0.7, inline

with the threshold in the study by Fan et al [20].

While the spearman’s correlation factor varied for pairs of features across different programming languages, the
correlation factor for github links and external links, was observed to be greater than the threshold for all programming
languages. The list of correlated features across the ten programming languages considered is presented in Table 7. If

there is only one pair of correlating features, we randomly drop one of those features in building the random forest

classifier. If there are more than one pair of correlating features, we identify if there exists a common feature among

these pairs, retain this feature, and discard the rest of the features. In cases where there does not exist any common

feature among the pairs, one of the feature in the pairs is randomly discarded, and the random forest classifier is built

using the rest of the features.

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

13

Language Correlating features
C
C++
C#
Go
Java
Javascript
PHP
Python
Ruby
Typescript

[{Github, Links}]
[{Github, Links}, {Image, Links}]
[{Github, Links}, {Image, Links}]
[{Github, Links}, {Image, Links}, {List, Links}]
[{Github, Links}]
[{Github, Links}, {Image, Links}]
[{Github, Links}]
[{Github, Links}]
[{Github, Links}]
[{Github, Links}]

Table 7. Correlating features across the ten programming languages

The spearman’s correlation factor was also calculated for all the pairs of categories. However, no two categories were

observed to have strong correlation, with 𝜌 value less than 0.7 for all the pairs across all the programming languages.

The random forest classifier is built using the non-correlating features, and a train to test ration of 70:30. While the

train to test ratio remained constant, the test and train data was altered with replacement using the out-of-bag bootstrap

strategy, for 1000 iterations. As a result, the accuracy of random forest classifier in each iteration has been calculated

and the average accuracy of the model was obtained. We observed the average accuracy of random forest classifier built

to be at least 70% for nine of the ten programming languages, while that of PHP was observed to be 63%. We calculate

the importance of each feature using the gini importance [8, 9] that could be obtained from the random forest classifier

model. The gini importance ranks the features in terms of the extent to which a feature decreases impurity of the model.

The average of decrease in impurity corresponding to a feature provides insights on the importance of the feature.

Any possible incorrect interpretations are mitigated by considering the variance in accuracy of the model with random

shuffling of each feature or category using permutation-based importance [15]. A larger variance in accuracy of the

model implies more importance of the feature or category with respect to popularity. Both methods to assess importance

of a feature revealed similar levels of importance for majority of the features and categories. If there are any discrepancies

in levels of importance across two methods, the difference in impurity values of the respective features or categories

and the difference in accuracy variance of the respective features or categories is calculated and compared. If there is a

larger difference in the accuracy variance values, then the respective features are ranked based on permutation-based

importance, else, they are ranked based on gini importance. The ranks assigned to the features and categories in each

of the programming languages are presented in Tables 8 and 9.

For example, a discrepancy in the levels of importance was observed in case of the Go programming language where
Inline and Code features were ranked as the fourth and fifth most important by gini importance, and vice-versa by
permutation-based importance. However, the difference in the importance scores were considerably low and almost
zero in case of permutation-based importance, when rounded up to three decimals. Hence, Inline feature is ranked
higher than the Code feature.

The correlating features identified through spearman’s correlation analysis were assigned the same rank as that of
the corresponding correlating feature. For example, since Links and GitHub, and the Links and Images features were
observed to have stronger correlation in C++, the random forest classifier built for C++ considers only Links feature
and thus the results of gini-importance and permutation-based importance do not result in the values for Images and
Github. However, since these two features are strongly correlated to Links feature, the values of importance and the
ranks could be same as the Links feature. The results of ranking, in the descending order of importance of features is
presented in Table 8.

Manuscript submitted to ACM

14

Venigalla and Chimalakonda

Features with their corresponding ranks of importance

GitHub Links

Image List

Inline Code Video Project Table Animation

C
C++
C#
Go
Java
Javascript
PHP
Python
Ruby
Typescript

1
1
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
1
1

2
2
1
1
3
1
2
4
2
4

3
3
3
1
2
3
3
2
3
6

4
4
5
4
4
6
4
3
4
3

5
5
7
5
6
4
5
5
5
2

6
7
8
7
8
6
7
8
7
5

7
6
4
6
5
5
6
6
6
7

8
9
9
8
8
6
8
8
7
9

9
8
6
8
7
6
8
7
7
7

Table 8. Feature Importance and ranking based on gini-importance and permutation-based importance across the 10 programming
languages. Ranks of importance are presented in descending order, where feature with rank 1 has highest importance and that with
rank 9 has the least importance.

Categories with their corresponding ranks of importance
What Why How When Who Refernece Contribution

C
C++
C#
Go
Java
Javascript
PHP
Python
Ruby
Typescript

4
4
4
2
3
4
4
4
5
4

7
7
7
7
7
7
7
7
7
7

5
6
5
6
5
6
5
5
4
5

6
5
6
5
4
5
5
5
5
5

3
2
3
4
6
2
1
3
3
1

1
3
2
3
1
3
2
1
1
3

2
1
1
1
2
1
3
2
2
2

Table 9. Category Importance and ranking based on gini-importance and permutation-based importance across the 10 programming
languages. Ranks of importance are presented in descending order, where category with rank 1 has highest importance and that with
rank 7 has the least importance.

A similar approach, as for features, was followed to build the random forest classifier based on categories. The train

to test ratio was considered to be 70:30, with out-of-bag bootstrap strategy applied over 1000 iterations. The average

accuracy of the random forest classifier built based on categories was observed to be at least 72% for all the programming

languages, except for PHP and Ruby, for which, the accuracy was 64%. The gini-importance and permutation-based

importance was calculated for each of the categories, and the level of importance of categories was observed to be the

same across both the methods. The result of ranking the categories based on their importance is presented in Table 9.

4 RESULTS

In this section we elaborate the results of the analysis and answer the research questions based on the results.

RQ1: Does presence of a specific feature relate to the popularity of the repository?
The features listed in Table 3 have been extracted from each of the readme files of the 2000 repositories. The count

of these features is considered to be associated with project popularity. The distribution of feature counts for all the

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

15

Fig. 2. Plot displaying correlation of all 10 features across repositories in all the 10 programming languages

Fig. 3. Plot displaying correlation of four features
with popularity for C language repositories

Fig. 4. Plot displaying correlation of four features with popularity
of Typescript language repositories

repositories under consideration, irrespective of their programming languages, with respect to the popularity rank is

displayed in Fig. 2. The popularity rank, here is calculated across all the 200 repositories. The spike in the graph in Fig.

2 indicates the count of the corresponding feature. Higher and larger number of spikes for features with increasing

popularity rank indicate that the respective feature could have a positive correlation with the popularity rank, i.e., the
count of the feature increases with increase in popularity rank. The distribution of List, GitHub, Links and Image feature
counts with respect to popularity rank of the repositories in C and Typescript languages is presented in Fig. 3 and Fig. 4

Manuscript submitted to ACM

16

Venigalla and Chimalakonda

respectively. Fig. 3 depicts increasing number of spikes for GitHub and Links features, with popularity rank. The spikes
for Image and List features are increasing at a slower rate than GitHub and Links.

Also, the List feature has spikes increasing unevenly with popularity rank. This could indicate that GitHub and Links
features have more positive correlation with popularity rank, followed by Image and List features for repositories in C
language. Fig. 4 depicts increasing number of spikes for GitHub and Links features with popularity rank, while that
of Image and List features do not evenly increase unlike Links and GitHub features, with more unevenness in case of
List feature. This could indicate strong positive correlation of GitHub and Links features with popularity ranks, and
weaker correlation of Image, followed by List features. Also, the Fig. 3 and Fig. 4 indicate that List feature in C language
repositories has more positive correlation than the List feature in Typescript language repositories, as there are more
number of even spikes for List feature in Fig. 3, than in Fig. 4. The graphs presented here are intended to bring-out a
high level idea of correlation of some features with popularity rank. Exact values of all feature counts for repositories in
all the languages can be found here7. However, to gain a much clearer understanding statistically, if each of the features
have a correlation with the popularity of the repositories, we applied non-parametric statistical hypothesis tests that

determine the statistical significance between two groups of data. Such tests were observed to be applied in literature in

studies with similar aims of understanding impact of specific nature of projects on the projects’ characteristics and the

association between these characteristics and project nature [12, 18–21].

Wilcoxon’s rank-sum test was applied on each of the ten features to identify if there exists a statistically significant

difference in the feature values among the popular and non-popular groups of repositories. The p-value of wilcoxon’s

rank sum test determines the presence of significant statistical difference. Features with p-value less than 0.05 (𝛼) imply

that the respective features are associated with popularity of the repository and are observed to vary across popular

and non-popular repositories.

It has been observed that Image, Github, Links and Inline features are strongly associated with popularity of repositories
and Animated Image, Video and Table were observed not to have a strong relation with popularity of repositories across
all programming languages, based on the corresponding p-value ranges, as presented in Table 5. This indicates that the

features with strong association could be considered as differentiating factors, with respect to readme files, among the
popular and non-popular repositories. List feature was observed to be strongly associated with popularity across the
10 programming languages, except for PHP and Typescript. Also, the Code feature was observed not to have a strong
relation with popularity across the 10 programming languages, except for Python and Typescript.

The results were further supported by Cliff’s delta test, which also revealed the extent of effect of a feature on the
popularity of the repository. The Links feature was observed to have larger effect size, indicating strongest association
with popularity across all programming languages. Also, the Image and Github features had larger effect sizes for all the
programming languages, except for Java, PHP, Typescript and PHP, Typescript respectively. It was also observed that
the Animated Image, Video, and Table features had almost negligible association with the popularity of the repositories,
irrespective of the programming language corresponding to the repository. We observed that majority of the repositories,
in both popular and non-popular categories did not contain Animated Images or Tables.

For example, only 9 of the 200 repositories corresponding to C language contained at least one Animated Image, with
two being the maximum number of Animated Images present in any repository. Two C# repositories with popularity
ranks 124 and 39, belonging to popular and non-popular groups respectively contained four Animated Images each,
in their readme files, thus, indicating almost no relation of Animated Images with the popularity. Also, only one

7https://osf.io/feutq/?view_only=e5517387c69f4d959a852758a085ea25

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

17

Fig. 5. Plot displaying correlation of the seven categories
with popularity for C language repositories

Fig. 6. Plot displaying correlation of the seven categories with
popularity of Typescript language repositories

repository with popularity rank 190 among the 200 Typescript repositories contained tables in the readme files, thus
making it difficult to identify distinguishing patterns among popular and non-popular repositories with respect to Tables.

Inclusion of links to external sources to provide easy navigation to the references in readme files is observed as a
common practice in popular repositories across all programming languages. Also, including Code in the readme
files of was observed to be prevalent in the most popular Typescript projects.

RQ2: Does presence of a specific category relate to the popularity of the repository?
The content in the readme files has been processed to identify the presence of categories listed in Table 4. Every

repository has been appended with boolean values corresponding to each of the categories, where ‘0’ and ‘1’ indicate

the absence and presence of the categories in the readme file. The distribution of presence of the seven categories across

repositories in C and Typescript languages are presented in Fig. 5 and Fig. 6 respectively. The increasing density of dots
in the plots indicates increasing presence of the categories. The presence of Reference, Contribution and Who categories
is observed to be increasing with popularity rank, compared to other features, in Fig. 5, which could indicate strong
positive correlation of these features with popularity rank for C language repositories. Moreover, Reference category has
more density than Contribution, which has more density than Who category, towards increasing popularity rank, which
could indicate that correlation of these categories with popularity rank is in the order of Reference > Contribution > Who
for C language repositories. Similarly, in case of Typescript repositories, the densities of Reference, Contribution and Who
categories are increasing with popularity rank, but the Reference category also has higher densities at few instances, for
lower popularity ranks. The Who category has lower density for low popularity rank, and almost continuous density
as the popularity rank increases, when compared to Reference category. This could indicate that Who category has
stronger correlation with popularity rank, than that of Reference category for repositories in Typescript language. The
presence of categories for repositories in each programming language can be found here8.

To gain a clearer understanding of the correlation, similar to the case of features, we employ the statistical analysis
method, Fisher’s exact test. Fisher’s exact test supports analysis of null hypothesis independence in samples with non-
numeric categorical values, and hence, was applied to assess statistical significance of a category in contributing to

8https://osf.io/feutq/?view_only=e5517387c69f4d959a852758a085ea25

Manuscript submitted to ACM

18

Venigalla and Chimalakonda

the popularity. The p-value less than 0.05 (𝛼) implies that the category and the popularity of the repository are not

independent of each other and thus, the category has a strong association with the popularity of the repository. We

observe that p-values of three of the seven categories are less than 0.05, for all the 10 programming languages, while that
of the Why and When categories is greater than 0.05, for all the 10 programming languages considered, as presented in
Table 6. The Who category had p-value less than 0.05 for all programming languages except Java and the How category
had p-value greater than 0.05 for all programming languages except C#.

The How category was observed to be present in majority of the repositories, across all programming languages
considered, except for C#, irrespective of their popularity. 189 out of the 200 Java repositories contained How category
in the readme files. This could be the reason for weaker association of How with the popularity of the repositories.
However, we cannot conclude that the absence of How category could change the popularity of the repository.

Details corresponding to project functionalities, guidelines to contribute to project and reference sources in readme

files are positively and strongly associated with the popularity of repositories in majority of the programming

languages. The details of installation instructions or ways to use the repository are present more commonly in

popular C# repositories, when compared to other programming languages.

RQ3: Which features and categories vary among the popular and non-popular repositories?
The features and categories that have stronger association with popularity among others were identified through

analysis of increased impurity using gini importance. The higher the impurity in absence of a feature or category,

stronger the association of the respective feature or category on the popularity of the repository.

Based on the ranks assigned considering results of gini and permutation-based importance, as presented in Table 8,
we observe that the Links and Github features and the Contribution and Reference features have the strongest association
with popularity of the repositories in almost all the 10 programming languages considered. The Image, List, Inline, Code
features follow the Links in level of importance on the project popularity. The categories Who, What and When follow
the Contribution and Reference categories in the order of extent of association with the project popularity, as seen in
Table 9.

The projects with better popularity are observed to contain content referring to the project page, contribution

guidelines and are well-organised in the form of lists, images and inline code. Also, the presence of team information

was observed to be a distinguishing factor among the popular and non-popular repositories in PHP and Typescript

languages.

5 THREATS TO VALIDITY

In this section we specify the threats to validity with respect to the empirical study performed, the data considered and

the statistical methods used to perform the empirical study.

Construct Validity - The features and categories considered for analysis could be valid for all projects across GitHub,
irrespective of the programming language or type of the project. However, the results corresponding to association

of the features or categories with project popularity are restricted to the 2000 repositories considered for the study.

Replicating the study on a different set of repositories might yield different results. However, as the current set of

repositories span across 10 programming languages, with relatively large distribution in terms of the star counts, this

set could be considered as a representative sample of all the repositories on GitHub. Hence, a similar association of the

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

19

features and categories could be observed on other GitHub repositories, suggesting (but not proving) generalizability of

the results.

The popularity rank considered for the study is aimed to provide equal weightage to the star_count, fork_count,

watcher_count and pull request _count of the repositories. However, the popularity of the repositories might also

depend on other characteristics of repositories such as contributor count, user count and so on. Including other such

factors of the repositories and updating the formula to calculate popularity rank accordingly, could alter the repositories

in the current popular and non-popular groups. Some of the repositories currently in the popular group might migrate

to non-popular group with change in factors considered to calculate the popularity.

Internal Validity - We define features and categories as the structural and type of content present in the readme
files respectively. The notion of these definitions is specific to the study and could vary based on the researchers’

perceptions. The features and categories considered for the study are based on the existing literature corresponding

to readme files. The features proposed by Fan et al. in [20] and the categories proposed by Prana et al. in [36] have

been considered. Thus, there is a possibility that there might exist other features or categories in the readme content,

which can be explored in future extensions of the study. However, the categories considered were also observed to be a
superset of GitHub recommended guidelines, indicating low possibility of missing out on the categories.

The association of the features is identified using the p-value obtained by wilcoxon’s rank sum test and the delta

value obtained using the cliff’s delta statistical hypothesis tests. While these statistical tests to identify the statistical

significance of features were observed to be relevant to the numerical non-parametric data in the current study [19, 20],

using other statistical hypothesis tests could result in different results. Similarly, while the use of Fisher’s exact test on

the non-numerical categorical values is relevant to the current data with respect to the categories [20], other statistical

analysis methods could result in different results.

External Validity - The results obtained are confined to the version of repositories and their and corresponding

software artifacts as on 16 December 2021, as this empirical study is performed on locally downloaded repositories.

The accuracy of categorizing the extracted information into different categories is dependant on the efficiency of the

readme classifier provided by Prana et al. in [36]. Applying a different classifier for the same class labels might yield in

different distributions of the classes in the readme files of the repositories. The accuracy of identifying the feature_count

for each of the features is dependant on the automated approach presented in the study, based on the identification

marks discussed in [20]. Adding different identification marks for the features could result in varied feature_counts. For
example, for the Project feature, the current identification mark is the presence of project keyword in the urls in html
formats of the readme files. The Project feature could further be calculated by considering the presence of project name
in the urls in html format. However, such decisions depend on the perception and choices of the researchers. As we are
considering the features presented by Fan et al. in [20], we consider the choice of identification marks discussed in [20].

6 DISCUSSION

This empirical study performed on 2000 GitHub repositories across 10 different programming languages provides

insights on possible structural and contextual content that could be included in readme files, to ensure better popularity

of the repositories. The results of the study lead to a list of good practices in creating readme files, which are observed

to be commonly used in popular repositories and are not very common in the non-popular repositories. Following a

structured format using lists, including pictorial representations such as images, adding references and providing links

to relevant external sources and other GitHub projects, specifying the ways to contribute to projects, mentioning the

team details and functional properties of the repository are identified as some of the good practices in designing readme

Manuscript submitted to ACM

20

Venigalla and Chimalakonda

files. However, we can not claim that these factors correspond to an exhaustive set of good practices, and can be further

extended based on other factors of the repository such as the domain, type of usage and so on. Also, these factors

are specific to the repositories and popularity metrics considered. The popularity metric could be restricted to fewer

dimensions such as stars and forks, or could be further enhanced by adding more dimensions such as number of users

and contributors. Using a different set of repositories for popular and non-popular groups could yield different results.

However, since there is a large difference between the star count of the popular and non-popular repositories, the

current set of repositories could be considered as a representative sample, thus indicating generalisability of the results.

However, further studies with varied dimensions of popularity and varied set of repositories should be performed to

prove the generalisability of results.

Insights for Practitioners - The Github and Links features were observed to be the most impactful features across
all programming languages. These features were observed to be largely present in popular repositories and were hardly

present in non-popular repositories. These two features can be grouped as links to external sources, which could be
narrowed down to References category. Software practitioners could thus consider including references, with links to
external sources and relevant GitHub projects in their readme files. Software practitioners, especially those working
with C#, Go and Javascript projects, could follow the good practices of structuring their readme files using Lists and
Images, along with References, towards arriving at better readme file. They could further include the team details of the
project, specific steps to be followed to contribute to the project in the readme files to support new developers who

wish to contribute to projects, thus, facilitating developer onboarding.

Insights for Researchers - The links, images, lists, contribution guidelines, references, team details and functional
features of repositories were observed to be strongly, and positively associated with popularity of the repositories.

These features could be taken into account in designing approaches that predict popularity of repositories, and also in

approaches that improve or generate readme files. The above mentioned features and categories could be considered

as a preliminary set of good practices in designing readme files and thus could be potentially useful in arriving at a

taxonomy of good practices in designing the readme files. Researchers could further explore the impact of readme

content on other characteristics of the projects such as project progress, issue resolution rate, number of contributors

and so on, which could further contribute to taxonomy of good practices for readme files. The impact of content in

readme files on the quality of readme files and the quality of documentation on a broader level could also be explored.

7 CONCLUSION AND FUTURE WORK

We perform a study on 200 GitHub repositories in each of the top-10 most popular programming languages to understand

the relation of features and categories with the popularity of the repositories. The features and categories are identified

and their association with popularity has been assessed using statistical hypothesis tests. The importance order of

each of the features and categories has further been calculated by building a random forest classifier and applying gini

importance and permutation-based importance methods on the random forest classifier generated.

We observe that the presence of references, contribution guidelines, information about the project team and the

purpose of the project in readme files are strongly and positively associated with popularity of the repositories. Also,

structuring the content as lists, adding images, and links to external sources and other GitHub projects was observed to

positively and proportionally be related to the popularity of repositories. Some features and categories such as details

about how to use the project were not observed to be related to the popularity, the reason being that they are present

in majority of the readme files, irrespective of popularity, and thus such categories or features could be included as

frequent practices in creating a readme file.

Manuscript submitted to ACM

An Empirical Study On Correlation between Readme Content and Project Popularity

21

We plan to extend this study to identify the impact of these features and categories on other project attributes such

as project progress, project quality and so on. We also plan to perform further studies on multiple projects, to identify

new features and categories, and extend the list of features and categories being studied. Future research directions for

this study could also include analysing correlation of the features and categories across projects in specific domains

such as machine learning, deep learning, game engines, games and so on. This analysis could also help in bringing out

patterns of features and categories in readme files in projects of specific domains. This correlation analysis could help

in identifying good practices of readme files across multiple domains and various programming languages.

REFERENCES
[1] Karan Aggarwal, Abram Hindle, and Eleni Stroulia. 2014. Co-evolution of project documentation and popularity within GitHub. In Proceedings of

the 11th Working Conference on Mining Software Repositories. 360–363.

[2] Lingfeng Bao, Xin Xia, David Lo, and Gail C Murphy. 2019. A large scale study of long-time contributor prediction for github projects. IEEE

Transactions on Software Engineering (2019).

[3] Victor R Basili, Gianluigi Caldiera, and H Dieter Rombach. 1994. Goal, question metric paradigm. Encyclopedia of Software Engineering, vol. 1.
[4] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research 3, Jan (2003), 993–1022.
[5] Hudson Borges, Andre Hora, and Marco Tulio Valente. 2016. Predicting the popularity of GitHub repositories. In Proceedings of the The 12th

International Conference on Predictive Models and Data Analytics in Software Engineering. 1–10.

[6] Hudson Borges, Andre Hora, and Marco Tulio Valente. 2016. Understanding the factors that impact the popularity of GitHub repositories. In 2016

IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 334–344.

[7] Hudson Borges and Marco Tulio Valente. 2018. What’s in a github star? understanding repository starring practices in a social coding platform.

Journal of Systems and Software 146 (2018), 112–129.

[8] Leo Breiman. 1996. Some properties of splitting criteria. Machine learning 24, 1 (1996), 41–47.
[9] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5–32.
[10] Jeanderson Cândido, Jan Haesen, Maurício Aniche, and Arie van Deursen. 2021. An Exploratory Study of Log Placement Recommendation in an
Enterprise System. In 18th IEEE/ACM International Conference on Mining Software Repositories, MSR 2021, Madrid, Spain, May 17-19, 2021. IEEE,
143–154. https://doi.org/10.1109/MSR52588.2021.00027

[11] Haowen Chen, Xiao-Yuan Jing, Zhiqiang Li, Di Wu, Yi Peng, and Zhiguo Huang. 2020. An empirical study on heterogeneous defect prediction

approaches. IEEE Transactions on Software Engineering (2020).

[12] Shaiful Alam Chowdhury and Abram Hindle. 2016. Characterizing energy-aware software projects: Are they different?. In Proceedings of the 13th

International Conference on Mining Software Repositories. 508–511.

[13] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. 2021. An Empirical Study on the
Usage of BERT Models for Code Completion. In 18th IEEE/ACM International Conference on Mining Software Repositories, MSR 2021, Madrid, Spain,
May 17-19, 2021. IEEE, 108–119. https://doi.org/10.1109/MSR52588.2021.00024

[14] Filipe R Cogo, Gustavo A Oliva, Cor-Paul Bezemer, and Ahmed E Hassan. 2021. An empirical study of same-day releases of popular packages in the

npm ecosystem. Empirical Software Engineering 26, 5 (2021), 1–42.

[15] Adele Cutler, D Richard Cutler, and John R Stevens. 2012. Random forests. In Ensemble machine learning. Springer, 157–175.
[16] Laura Dabbish, Colleen Stuart, Jason Tsay, and Jim Herbsleb. 2012. Social coding in GitHub: transparency and collaboration in an open software

repository. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 1277–1286.

[17] Osama Ehsan, Safwat Hassan, Mariam El Mezouar, and Ying Zou. 2020. An empirical study of developer discussions in the gitter platform. ACM

Transactions on Software Engineering and Methodology (TOSEM) 30, 1 (2020), 1–39.

[18] Yuanrui Fan, Xin Xia, Daniel A COSTA, David Lo, Ahmed E Hassan, and Shanping Li. 2019. The impact of changes mislabeled by szz on just-in-time

defect prediction. IEEE transactions on software engineering (2019), 1.

[19] Yuanrui Fan, Xin Xia, David Lo, and Ahmed E Hassan. 2018. Chaff from the wheat: Characterizing and determining valid bug reports. IEEE

transactions on software engineering 46, 5 (2018), 495–525.

[20] Yuanrui Fan, Xin Xia, David Lo, Ahmed E Hassan, and Shanping Li. 2021. What makes a popular academic AI repository? Empirical Software

Engineering 26, 1 (2021), 1–35.

[21] Yuanrui Fan, Xin Xia, David Lo, and Shanping Li. 2018. Early prediction of merged code changes to prioritize reviewing tasks. Empirical Software

Engineering 23, 6 (2018), 3346–3393.

[22] Mohammad Gharehyazie, Baishakhi Ray, Mehdi Keshani, Masoumeh Soleimani Zavosht, Abbas Heydarnoori, and Vladimir Filkov. 2019. Cross-project

code clones in GitHub. Empirical Software Engineering 24, 3 (2019), 1538–1573.

[23] M.A. Hearst, S.T. Dumais, E. Osuna, J. Platt, and B. Scholkopf. 1998. Support vector machines. IEEE Intelligent Systems and their Applications 13, 4

(1998), 18–28. https://doi.org/10.1109/5254.708428

Manuscript submitted to ACM

22

Venigalla and Chimalakonda

[24] Hadhemi Jebnoun, Houssem Ben Braiek, Mohammad Masudur Rahman, and Foutse Khomh. 2020. The scent of deep learning code: An empirical

study. In Proceedings of the 17th International Conference on Mining Software Repositories. 420–430.

[25] Mitchell Joblin and Sven Apel. 2021. How Do Successful and Failed Projects Differ? A Socio-Technical Analysis. ACM Trans. Softw. Eng. Methodol.

(dec 2021). https://doi.org/10.1145/3504003

[26] David Kavaler, Sasha Sirovica, Vincent Hellendoorn, Raul Aranovich, and Vladimir Filkov. 2017. Perceived language complexity in GitHub issue
discussions and their effect on issue resolution. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE,
72–83.

[27] Hemank Lamba, Asher Trockman, Daniel Armanios, Christian Kästner, Heather Miller, and Bogdan Vasilescu. 2020. Heard it through the Gitvine:
an empirical study of tool diffusion across the npm ecosystem. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. 505–517.

[28] Michael J Lee, Bruce Ferwerda, Junghong Choi, Jungpil Hahn, Jae Yun Moon, and Jinwoo Kim. 2013. GitHub developers use rockstars to overcome

overflow of news. In CHI’13 Extended Abstracts on Human Factors in Computing Systems. 133–138.

[29] Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John Grundy, and Xiaohu Yang. 2021. On the Reproducibility and Replicability of Deep Learning in

Software Engineering. ACM Transactions on Software Engineering and Methodology (TOSEM) 31, 1 (2021), 1–46.

[30] Yuyang Liu, Ehsan Noei, and Kelly Lyons. 2022. How ReadMe files are structured in open source Java projects. Information and Software Technology

148 (2022), 106924.

[31] Guillermo Macbeth, Eugenia Razumiejczyk, and Rubén Daniel Ledesma. 2011. Cliff’s Delta Calculator: A non-parametric effect size program for two

groups of observations. Universitas Psychologica 10, 2 (2011), 545–555.

[32] Reed Milewicz, Gustavo Pinto, and Paige Rodeghero. 2019. Characterizing the roles of contributors in open-source scientific software projects. In

2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). IEEE, 421–432.

[33] Alessandro Murgia, Giulio Concas, Roberto Tonelli, Marco Ortu, Serge Demeyer, and Michele Marchesi. 2014. On the influence of maintenance

activity types on the issue resolution time. In Proceedings of the 10th international conference on predictive models in software engineering. 12–21.

[34] Cassandra Overney. 2020. Hanging by the thread: an empirical study of donations in open source. In Proceedings of the ACM/IEEE 42nd International

Conference on Software Engineering: Companion Proceedings. 131–133.

[35] Gede Artha Azriadi Prana, Denae Ford, Ayushi Rastogi, David Lo, Rahul Purandare, and Nachiappan Nagappan. 2021.

Including everyone,
everywhere: Understanding opportunities and challenges of geographic gender-inclusion in oss. IEEE Transactions on Software Engineering (2021).
[36] Gede Artha Azriadi Prana, Christoph Treude, Ferdian Thung, Thushari Atapattu, and David Lo. 2019. Categorizing the content of github readme

files. Empirical Software Engineering 24, 3 (2019), 1296–1327.

[37] Akond Rahman and Effat Farhana. 2021. An Empirical Study of Bugs in COVID19 Software Projects. Journal of Software Engineering 9 (2021), 3.
[38] Christoph Treude, Justin Middleton, and Thushari Atapattu. 2020. Beyond accuracy: Assessing software documentation quality. In Proceedings of the
28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1509–1512.
[39] Asher Trockman, Shurui Zhou, Christian Kästner, and Bogdan Vasilescu. 2018. Adding sparkle to social coding: an empirical study of repository

badges in the npm ecosystem. In Proceedings of the 40th International Conference on Software Engineering. 511–522.

[40] Jason Tsay, Laura Dabbish, and James Herbsleb. 2014. Let’s talk about it: evaluating contributions through discussion in GitHub. In Proceedings of

the 22nd ACM SIGSOFT international symposium on foundations of software engineering. 144–154.

[41] Morteza Verdi, Ashkan Sami, Jafar Akhondali, Foutse Khomh, Gias Uddin, and Alireza Karami Motlagh. 2020. An empirical study of c++ vulnerabilities

in crowd-sourced code examples. IEEE Transactions on Software Engineering (2020).

[42] Yue Yu, Huaimin Wang, Vladimir Filkov, Premkumar Devanbu, and Bogdan Vasilescu. 2015. Wait for it: Determinants of pull request evaluation

latency on github. In 2015 IEEE/ACM 12th working conference on mining software repositories. IEEE, 367–371.

[43] Yun Zhang, David Lo, Pavneet Singh Kochhar, Xin Xia, Quanlai Li, and Jianling Sun. 2017. Detecting similar repositories on GitHub. In 2017 IEEE

24th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 13–23.

[44] Jiaxin Zhu, Minghui Zhou, and Audris Mockus. 2014. Patterns of folder use and project popularity: A case study of GitHub repositories. In Proceedings

of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement. 1–4.

Manuscript submitted to ACM

