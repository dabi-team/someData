Abstract

Computational Law has begun taking the role in society which has been predicted for some time. Automated
decision-making and systems which assist users are now used in various jurisdictions, but with this maturity come
certain caveats. Computational Law exists on the platforms which enable it, in this case digital systems, which means
that it inherits the same ﬂaws. Cybersecurity is one framework which addresses these potential weaknesses, and in
this paper we go through known issues and discuss them in the various levels, from design to the physical realm.
We also look at machine-learning speciﬁc adversarial problems, which entail further weaknesses. Additionally, we
make certain considerations regarding computational law and existing and future legislation. Finally, we present three
recommendations which are necessary for computational law to function globally, and which follow ideas in safety and
security engineering. As indicated, we ﬁnd that computational law must seriously consider that not only does it face the
same risks as other types of software and computer systems, but that failures within it may cause ﬁnancial or physical
damage, as well as injustice. The consequences of Computational Legal systems failing are in this sense greater than
if they were merely software and hardware. And if the system employs machine-learning, it must take note of the very
speciﬁc dangers which this brings, of which data poisoning is the classic example. Computational law must also be
explicitly legislated for, which we show is not the case currently in the EU, and this is also true for the cybersecurity
aspects that will be relevant to it. But there is great hope in EU’s proposed AI Act, which makes an important attempt
at taking the speciﬁc problems which Computational Law bring into the legal sphere. Lastly, our recommendations for
Computational Law and Cybersecurity are: Accommodation of threats, adequate use, and that humans must remain in
the centre of their deployment. The latter is primarily for the abilities humans process and which allow them to handle
emergencies.

2
2
0
2

l
u
J

1

]

Y
C
.
s
c
[

1
v
5
9
2
0
0
.
7
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
The Dangers of Computational Law and Cybersecurity;
Perspectives from Engineering and the AI Act

Kaspar Rosager Ludvigsen1, Shishir Nagaraja2, and Angela Daly3

1Department of Computer and Information Sciences, University of Strathclyde,
kaspar.rosager-ludvigsen@strath.ac.uk
2Department of Computer and Information Sciences, University of Strathclyde,
shishir.nagaraja@strath.ac.uk
3Leverhulme Research Centre for Forensic Science and Dundee Law School,
adaly001@dundee.ac.uk

June 2022

1 Introduction

Despite law being established as an academic discipline
for a signiﬁcant period of time, it is still rather unde-
ﬁned and lacks the rigour that other disciplines possess.
There are ongoing debates and unresolved questions as
to when law is deductive or inductive1, so for computa-
tional law (CL) to even claim being future-proof misses
the mark. Like the existence of exotic matter in astron-
omy, you may deduce or assume their existence, but em-
pirical evidence will eventually prevail and show whether
it is worthwhile2. The same can be said for CL, as its
implementation should not be forced because of powers
outside the academic and legal sphere3, or for the sake

1And while these are vital, we cannot discuss them further in this
paper. For a contrary opinion, see footnote 4, page 98 in Ana Margarida
Sim˜oes Gaudˆencio, “Presumption(s) of Correctness(?): Comparing the
Methodological Relevance of Precedents in Civil Law and in Common
Law Systems” in Common Law - Civil Law The Great Divide? (Springer
2022) (cid:104)https://link.springer.com/chapter/10.1007/978- 3- 030- 87718-
7%7B%5C %7D8(cid:105).

2Past questions on this are still not fully answered, see Robert A
Malaney and William A Fowler, “The transformation of matter after the
big bang” (1988) 76(5) American Scientist 472.

3Wolfgang Hoffmann-Riem,

“Legal Technology/Computational
Law” (2021) 1(1) Journal of Cross-disciplinary Research in Computa-
tional Law 1, 10 - 11.

Increased use must be justiﬁed, regardless of
of proﬁt.
whether some types of technology are forced upon every-
one without any other reason than power4.

CL can be deﬁned by its usage, like many other ﬁelds
of law5. Law in general, and CL too, contain an important
modiﬁer:

They are affected by the technological systems they ex-
ist in6, and CL expresses this in a much more extreme
manner than any other ﬁeld. It would not even exist with-
out its extra-legal roots7. Computer science provides the

4See any work on facial recognition or contract tracing, e.g., Isadora
Neroni Rezende, “Facial recognition in police hands: Assessing the
‘Clearview case’ from a European perspective” (2020) 11(3) New Jour-
nal of European Criminal Law 375; Ilia Siatitsa, “Freedom of assembly
under attack: General and indiscriminate surveillance and interference
with internet communications” (2020) 102(913) International Review of
the Red Cross 181; Lucie White and Philippe Van Basshuysen, “Without
a trace: Why did corona apps fail?” (2021) 47(12) Journal of Medical
Ethics E83.

5See ongoing work on how this is understood, like Burkhard Schafer,

Legal Tech and Computational Legal Theory (2022).

6There are diverging opinions on this, see Bert-Jaap Koops, “Should
ICT Regulation Be Technology-Neutral?” in (2006) versus Paul Ohm,
“The argument against technology-neutral surveillance laws” [2010]
Texas Law Review.

7See, e.g., Louis O Kelso, “Does the Law Need a Technological Rev-
olution?” (1946) 18 Rocky Mountain Law Review 378; W Daniel Hillis,
“New computer architectures and their relationship to physics or why

basis and logic for CL, and in turn brings the same draw-
backs8.

A good parallel to describe this, would be the event of
IoT devices9. The increased adoption of these brings in
a new danger at every step, as all of them are (usually)
connected to a network and make use of software. Every
danger posed to computers and digital systems therefore
exist in these devices, which are often used close to hu-
mans or for critical infrastructure, increasing the amount
of possible accidents and the risk of damage10.

CL shares this, every logical or otherwise systemic pit-
fall can pose a danger. For example, circumvention and
adversarial law usually relies on a system that consists
of courts and subjects seeking to abuse it11. Doing the
same in CL only requires obfuscating the actions within
the logic of the code or otherwise abusing technical limi-
tations or faults of the system. The different types of par-
ties which can do this to these systems are many times
larger, than the sum of money and inﬂuence required to
do this within pre-existing legal systems.

This brings us to some unique points about what kind
of vulnerabilities CL will always have. Cybersecurity
(which now applies to CL) consists of certain assump-
tions, either expressed directly or seen but not necessar-
ily explicitly mentioned by those who practice it12. One

computer science is no good” (1982) 21(3-4) International Journal of
Theoretical Physics 255.

8While this is not a new or unknown phenomena, it is worth dis-
cussing and elaborating on, both to make it clearer for policymakers,
users and the public in general.

9IoT devices can be part of a CL system, e.g., the data could feed

into a system which decides on the basis of it.

10See any literature on this problem, Kaspar Rosager Ludvigsen and
Shishir Nagaraja, “Dissecting liabilities in adversarial surgical robot fail-
ures: A national (Danish) and EU law perspective” (2022) 44 Computer
Law and Security Review (cid:104)https://doi.org/10.1016/j.clsr.2022.105656(cid:105);
Kevin Fu and others, “Safety, Security, and Privacy Threats Posed by
Accelerating Trends in the Internet of Things” [2020] Computing Com-
munity Consortium (cid:104)http://arxiv.org/abs/2008.00017(cid:105); Syed Rizvi and
others, “Securing the Internet of Things (IoT): A Security Taxonomy for
IoT” [2018] Proceedings - 17th IEEE International Conference on Trust,
Security and Privacy in Computing and Communications and 12th IEEE
International Conference on Big Data Science and Engineering, Trust-
com/BigDataSE 2018 163.

11Extra-legal means to circumvent or abuse the legal system exist too,
very much analogously to how many different ways an adversary can
attack digital systems.

12We will not provide further argumentation for the assumptions, as
these are not stringently codiﬁed in cybersecurity at the time of writing,
but matter to how it is deployed in practice. For example, zero-day ex-

of these is the assumption that there will never be a per-
fect defence. We are never reaching an equilibrium of
defences and attacks, and the risk of incursions that can
succeed will always exist. A major consequence of CL ei-
ther becoming or already being implemented into national
legal systems, is that they can now suffer adversarial fail-
ures (from adversarial attacks). Therefore, ironically, CL
as a tool for further automation may be a target for auto-
mated adversarial attacks and loophole analysis itself.

Like in law, adversary merely refers to the subject be-
ing against the target, but the attacks and failures en-
tail weaknesses which only increase in sophistication and
consequences as the systems or technology used becomes
more complicated and increasingly used. To illustrate
this, imagine the difference between a court system which
only partially or does not rely on CL, and one which does.
The former cannot be brought to a halt or fully hijacked
by an adversary. But a court system which makes fully or
mostly use of CL deﬁnitely can, unless it has backups and
redundancies that enables it to revert to a non-CL state.

In this sense, CL inherently makes a legal system more
vulnerable, in turn potentially damaging rights, individu-
als, corporations and even the state itself. It is from here
its relationship with cybersecurity must be scrutinised,
understood and realised, which we can only discuss so
much here.

In this paper, we take a very narrow look at one case,
which is adversarial attacks on machine-learning (ML)
models and any system that makes use of them. Com-
putational legal systems that consist partially or fully of
these, in any shape or form, will be vulnerable to attacks
on the classiﬁers13, one of the fundamental parts of these
systems, which all the learning is used on and to form,
and attacks to extract the data14 which they are created
on, or the source code of the entire system. These are all
well known, but for CL, they create barriers which make
the practical implementation of these systems potentially
unsafe.

Safety here refers to accidents or losses, while cyberse-

ploits may be dangerous, but are something you must prepare for and no
one expects everything to be perfectly preventable.

13Matt Fredrikson, Somesh Jha, and Thomas Ristenpart, “Model in-
version attacks that exploit conﬁdence information and basic counter-
measures” (2015) 2015-Octob Proceedings of the ACM Conference on
Computer and Communications Security 1322.

14Florian Tram`er and others, “Stealing Machine Learning Models via

Prediction APIs” (2016) (cid:104)http://arxiv.org/abs/1609.02943(cid:105).

curity refers to lowering the risk of adversarial failures. It
is said that safety is increasingly covering cybersecurity15,
as seen with the IoT example above, and this implies that
CL must develop defences in its technical implementation
and logic. Limiting itself to ways where risks can be mit-
igated or hazards can be controlled is suitable, and follow
the tradition of anything that absorbs cybersecurity as part
of its being16

Section 2 deﬁnes what we see as CL and cybersecurity.
After this, we discuss the weaknesses which CL contains
in 5 levels. In Section 3, we dive further into machine-
learning speciﬁc issues with CL, and we also comment on
the problems which black boxes and authentication bring.
Section 4 contains a brief legal commentary on how cy-
bersecurity is regulated in the EU, with a focus on its ef-
fect on CL, and how the EU’s future AI Act17 may affect
CL as a ﬁeld. We then provide general recommendations
for the future, because of the obvious consequences which
CL and cybersecurity together bring in Section 5, Section
6 has some ideas for future work, and ﬁnally, Section 7
concludes the paper.

2 Computational Law and Cyberse-

curity

CL and cybersecurity naturally ﬁt together, because the
former interacts or exists in some digital space which re-
quires software and/or hardware. This section combines
the two and looks at weaknesses.

2.1 Deﬁnitions

CL can be deﬁned as:

15Acknowledged by the European Union, see page 10 of ‘MDCG
2019-16 Guidance on Cybersecurity for medical devices’, (cid:104)https://ec.
europa.eu/docsroom/documents/41863(cid:105), last accessed 30 June 2022.

16Ibid. But any type of product which gains network connectivity
could be a good example. For a general overview in the EU, see Cezary
Banasi´nski and Marcin Rojszczak, “Cybersecurity of consumer prod-
ucts against the background of the EU model of cyberspace protection”
(2021) 7(1) Journal of Cybersecurity 1.

17Proposal for a Regulation of the European Parliament and of the
Council laying Down Harmonised Rules on Artiﬁcial Intelligence (Ar-
tiﬁcial Intelligence Act) and Amending Certain Union Legislative Acts,
COM/2021/206.

“The techniques of computational logic, applied to the
semantic rules as well as the data, form the basis of a
computational law system.18”

CL is considered to be automation of legal reasoning.
But from the quote by Love, we can see that there are
essentially two broad deﬁnitions, either purely automation
or broadly applying computational logic to law. We will
consider both in this paper.

2.1.1 Forms of Computational Law

To better contextualise CL, we mention selected forms
here. This is not an exhaustive list, and some applications
will cover several areas.

• Automated assistance within legal systems. Turb-
oTax19 (while not developed by a State) is always
brought up, but the Danish state is another example
where income and other relevant information is au-
tomatically sought and ﬁlled in20 in tax returns and
similar forms, leading to automated experiences for
most subjects. On the back-end, this type of CL is
also used by the authorities.

• Contract related ﬁelds, like smart contracts21, as well
as other types or derivatives, are considered CL, but
are often found in their own sub genres in relation
to the technology which they exist in. These see pri-
marily use in private law22, and will not always be
able to execute their contents fully without assistance
from litigation or other means23. The latter comes

18Nathaniel Love and Michael Genesereth, “Computational Law”

[2005] ICAIL 205, 206.

19See Kacey Marr, “You’re Only as Good as Your Tax Software: The
Tax Court’s Wrongful Approval of the Turbotax Defense in Olsen v.
Commissioner” (2012) 81(2) University of Cincinnati Law Review 709,
for an example of function and issues.

20https://lifeindenmark.borger.dk/economy-and-tax/

the-danish-tax-system/a-general-introduction-to-the-danish-tax-system,
last accessed 30 June 2022.

21Abhishek Dixit and others, “Towards user-centered and legally
relevant smart-contract development: A systematic literature review”
(2022) 26(November 2021) Journal of Industrial Information Integra-
tion 100314 (cid:104)https://doi.org/10.1016/j.jii.2021.100314(cid:105).

22However, even public authorities and states will be private law sub-

jects to the companies they create contractual relations with.

23For an elaboration of these issues, see Pablo Sanz Bay´on, “Key Le-
gal Issues Surrounding Smart Contract Applications” (2019) 1 KLRI;
Monika Di Angelo, Alfred Soare, and Gernot Salzer, “Smart contracts

down to needing state or legal system assistance to
fulﬁll them, in case of disputes or disagreements.

• Automated legal decision-making. This can range
from public legal decisions on subsidies, pension,
automated payments24 or company registration, with
a much bigger potential in the future25. For future
applications, the research community and the indus-
try as such tends to focus on automated tools or AI
judges26 but this seems very far-fetched with the ca-
pabilities of current CL systems.

• Quantum CL, if the event of quantum computing oc-
curs, should be its own separate ﬁeld27. Primarily
due to the power imbalance, but also the new physi-
cal constraints of the system. The former refers to the
substantially increased amount of computing power
these will have over conventional hardware, the lat-
ter to the real tangible physical difference which
quantum computers contain, meaning that new threat
models and measures will have to be developed28.

What these all have in common, is that they exist in or
as software and make use of hardware, while containing

in view of the civil code” (2019) Part F1477 Proceedings of the ACM
Symposium on Applied Computing 392; Kevin Werbach and Nicolas
Cornell, “Contracts Ex Ma China” (2017) 67(2) Duke Law Journal.

24The wrongful texts on Estonian AI should have referred to the devel-
opment of automated systems for certain types of payment, see https://
www.just.ee/en/news/estonia-does-not-develop-ai-judge, last accessed
30 June 2022.

25For an overview in public legal systems, see Ulrik BU Roehl, “Un-
derstanding Automated Decision-Making in the Public Sector: A Clas-
siﬁcation of Automated, Administrative Decision-Making” in Service
Automation in the Public Sector (Springer 2022) (cid:104)https://link.springer.
com/10.1007/978-3-030-92644-1%7B%5C %7D3(cid:105).

26Some examples could be John Morison and Adam Harkens, “Re-
engineering justice? Robot judges, computerised courts and (semi) au-
tomated legal decision-making” (2019) 39(4) Legal Studies 618; Nu
Wang, “”Black Box Justice”: Robot Judges and AI-based Judgement
Processes in China’s Court System” [2020] International Symposium on
Technology and Society 58; Fabrice Muhlenbach, Long Nguyen Phuoc,
and Isabelle Sayn, “Predicting Court Decisions for Alimony: Avoiding
Extra-legal Factors in Decision made by Judges and Not Understandable
AI Models” (2020) (cid:104)http://arxiv.org/abs/2007.04824(cid:105).

27Jeffery Atik and Valentin Jeutner, “Quantum computing and compu-
tational law” (2021) 13(2) Law, Innovation and Technology 302 (cid:104)https:
//doi.org/10.1080/17579961.2021.1977216(cid:105), 305.

28There is a wealth of post-quantum research which is ongoing, see,
e.g., Jongmin Ahn and others, “Toward Quantum Secured Distributed
Energy Resources: Adoption of Post-Quantum Cryptography (PQC) and
Quantum Key Distribution (QKD)” (2022) 15(3) Energies.

a level of automation and operate within or as the legal
system. This entails that cybersecurity is an issue for all
of them.

2.1.2 Cybersecurity

Cybersecurity can be deﬁned as freedom from adversarial
failures29, but there exists other deﬁnitions outside of tra-
ditional security engineering. We chose this because it is
narrow and focused on mitigating or limiting the damage
of what failure to defend against attacks may cause.

As mentioned, this freedom is hard to attain, and it
is therefore more of a goal to strive for, than a goal to
reach30. Adversaries successfully attacking a system does
not always lead to safety failures, but with the few exam-
ples we have mentioned, there is a risk they will. Safety
is the general idea of freedom from accidents or losses31,
where both ﬁnancial as well as human losses are included.
Cybersecurity itself is very diverse, and includes ev-
erything from encryption, good practice for building
databases32 and naming conventions33, to physical ele-
ments like air gaps34, or access control35 to both the
servers or robots36.

What we focus on is the additional layer of complexity
and potential risk which cybersecurity adds to CL. Very
much like safety engineering with the event of the indus-
trial revolution, cybersecurity needs to be integrated and

29Nancy G Leveson, Safeware: System Safety and Computers (1.,

Addison-Wesley Publishing Company, Inc 1995).

30And can be expanded to include the process of attaining security,
see Tomasz Zdzikot, “Cyberspace and Cybersecurity” in Cybersecurity
in Poland (2022) 17 - 18.
31Leveson (n 29).
32Habib Ibrahim, Songul Karabatak, and Abdullahi Abba Abdullahi,
“A Study on Cybersecurity Challenges in E-learning and Database Man-
agement System” [2020] 8th International Symposium on Digital Foren-
sics and Security, ISDFS 2020.

33Ross Anderson, Security engineering: a guide to building depend-

able distributed systems (John Wiley & Sons 2020) 259-271.

34Physical gaps between the system and the internet or just public
space. Guri has written a range of papers on how to mitigate and un-
derstand air gaps, see e.g., Mordechai Guri, “Lantenna: Exﬁltrating data
from air-gapped networks via ethernet cables emission” [2021] Proceed-
ings - 2021 IEEE 45th Annual Computers, Software, and Applications
Conference, COMPSAC 2021 745.

35Adriano Valenzano, “Industrial cybersecurity: Improving security
through access control policy models” (2014) 8(2) IEEE Industrial Elec-
tronics Magazine 6.

36Each area could be analysed speciﬁcally with regards to CL, but this

is not the paper for that.

considered when designing and also deploying CL sys-
tems. Gone are the days where the application and idea
behind CL is purely discussed, as there are weaknesses
which certain types of logic or choices add, giving rise to
practical impacts.

2.2 Weaknesses

CL can exist in a theoretical manner without considering
cybersecurity, but when deployed in practice, defences are
needed in various ways. Stochastic or completely unpre-
dictable issues are not considered because of their extraor-
dinary nature. But the limitations to what can be consid-
ered special enough to warrant lack of liability depends
on the jurisdiction, to this, see our past work37, for an
analysis combining private law and cybersecurity within
a Danish context.

2.2.1 General Issues

With inspiration from existing literature, we sketch an
overarching conceptual understanding of how cybersecu-
rity matters to CL.

Weakness in CL can be described in the following lev-

els:

1. Logic or design.

2. Software.

3. Hardware.

4. System or network.

5. Physical.

will impose a risk on everyone in the system, making
level 4 exceptionally difﬁcult. This includes the very net-
work(s) that CL use.

Atttacks on the physical layer, level 5, is to usually en-
able an attack on a lower level, except for those unique to
it such as physical destruction, but shoulder-surﬁng38 or
physical side-channel attacks39 are examples of the ﬁrst.
Most levels are described or speciﬁed better in exist-
ing literature. Regardless of this, there are some unique
additional explanations needed for each.

To illustrate the division, we include an visualisation
which shows the interactions between the different levels.

2.2.2 Logic

Cybersecurity confer devastating and potentially perma-
nent weaknesses to CL systems through its choice of
logic, as it is what deﬁnes what should be understood and
done. This is explained by how it is constructed, e.g.,
through the use of machine-learning or expert decision-
making, or through choices of programming language or
other technical design decisions. These must be not only
uncovered and possibly mitigated, but some may warrant
the removal of the system entirely, if any of the core weak-
nesses compromise any basic cybersecurity attributes at
large. For example, machine-learning based systems may
contain weaknesses that allow both data and the code be-
hind it to be easily retrievable, and if defences against this
are not vigilantly updated and improved throughout the
life-cycle of the system, they should not even be used in
the ﬁrst place.

Another could be the logic behind what constitutes a
decision in the system40, becoming law, which may be
circumventable or possible to be hijacked, either through

If the logic or the design of the system does not con-
sider most common threats, and has not been thoroughly
tested, cybersecurity adds a layer of weakness to all lev-
els. Testing for hardware is the same as software, but level
4 is different.

Broadly, all the attacks and defences work on individ-
ual levels (each substation), but if these are not applied
uniformly, the weakness will consist of other systems (not
those related to the CL), because individual weaknesses

37Ludvigsen and Nagaraja, “Dissecting liabilities in adversarial surgi-
cal robot failures: A national (Danish) and EU law perspective” (n 10).

38Refers to snooping, observing literally over a shoulder or otherwise,
to deduce or directly observe passwords or other information. There
exists plenty of research on the matter, see, e.g., Mihai Bˆace and others,
“PrivacyScout : Assessing Vulnerability to Shoulder Surﬁng on Mobile
Devices” (2022) 21(1) Proceedings on Privacy Enhancing Technologies
1.

39Physical, unlike digital side-channel attacks, refers to readings
which then allow an adversary to manipulate or otherwise know some-
thing they should not. This could be electromagnetic and so on, see
following article for a great overview with a focus on neural networks,
Maria M´endez Real and Rub´en Salvador, “Physical side-channel attacks
on embedded neural networks: A survey” (2021) 11(15) Applied Sci-
ences (Switzerland).

40This applies to all types of platforms CL can use.

known techniques or some discovered by fuzzing41. Cir-
cumvention or hijacking can be seen as obfuscation or
abuse by known constraints of the system, which is known
in law as:

Loopholes42, deliberate non-compliance without en-
forcement43 or stalling for time44 through litigation or in
public law45. CL allows circumvention etc., that does not
necessarily require humans or much effort measured by
time, which increases the risk as to whether it will be
used46.

2.2.3 Software

CL is expressed in and usually executes within the soft-
ware level. Pandora’s box is therefore opened in regards
to weaknesses, anything that is possible with the very soft-
ware47 that is or the CL resides in, expresses the opportu-
nities of attack and failures. This could be in the form of

41Fuzzing is the practice of testing arbitrary or deliberate inputs or
actions, and seeing how the system reacts to it. Some that can cause
adversarial attacks may be discovered at the design stage, others later.
For an overview, see Richard Mcnally, Ken Yiu, and Duncan Grove,
Fuzzing : The State of the Art (techspace rep, DSTO Defence Science
and Technology Organisation 2012).

42There exists an ocean of research on the concept, good specialised
examples could be Matthew R Espinosa, “Small Business Cybersecu-
rity: A Loophole to Consumer Data” (2022) 24(2) The Scolar: St.
Mary’s Law Review on Race and Social Justice; Grant Butler, “The
Sky Reefer Loophole : How Modern Carriers Lessen Their Liability
Through Foreign Arbitration and Choice of Court Provisions-and Four
Countries Who Stopped It” (2022) 46(1) Tulane Maritime Law Journal.
43For an empirical study with interesting perspectives on this, see Aliu
Oladimeji Shodunke, “Enforcement of COVID-19 pandemic lockdown
orders in Nigeria: Evidence of public (Non)compliance and police ille-
galities” (2022) 77(May) International Journal of Disaster Risk Reduc-
tion 103082 (cid:104)https://doi.org/10.1016/j.ijdrr.2022.103082(cid:105).

44Joanna Mazur and Marcin Seraﬁn, “Stalling the State: How Dig-
ital Platforms Contribute to and Proﬁt From Delays in the Enforce-
ment and Adoption of Regulations” [2022] Comparative Political Stud-
ies 001041402210896 (cid:104)http : / / journals . sagepub . com / doi / 10 . 1177 /
00104140221089651(cid:105).

45This list in not exhaustive, but represents common areas.
46Cheap externalities usually leads to increased use, as we have seen
with the data sharing, see, e.g., Shota Ichihashi, “The economics of data
externalities” (2021) 196 Journal of Economic Theory 105316 (cid:104)https :
//doi.org/10.1016/j.jet.2021.105316(cid:105). Worth noting that even if elements
make higher proﬁts for shareholders, this does not mean decreased costs
for users or better rights, see Ronald J Deibert, “Subversion Inc: The
Age of Private Espionage” (2022) 33(2) Journal of Democracy 28. This
analogy persists with CL as well, as it will end up being extremely cheap
to attack and abuse the systems going forward.

47This could be the AI that decides, or one of several subsystems.

denial-of-service of various kinds, leading to loss of in-
tegrity or availability. The latter is especially important if
the CL is the only source of decisions on for example legal
subsidies or permits, and if such a system is taken out, and
there are no redundancies, systemic ﬁnancial loss is very
possible48. Software is usually also the target of attacks
from the other levels, and attacking the CL software is
very possible through other software in the system some-
where, which we focus on further below.

2.2.4 Hardware

Attacking software through hardware is classic, as
seen with the Spectre attack on CPUs49, and recently
Hertzbleed50, allow for various actions going from hard-
ware to software. In a CL context, this would allow at-
tackers everything from stealing data, to escalation attacks
that would give them control of the software, or simply a
way to destroy it through ransomware attacks with no way
to reverse the encryption of ﬁles.

Defences against this are often physical, but as attacks
like Spook.js51 show, you do not even need to have such
access to perform the attack. In this sense, hardware only
adds to the burden of defending CL systems, and the most
important detail of all here, is there is no way to prevent
all side or covert channels from being exploited, as they
often are caused by deliberate decisions in hardware ar-
chitecture. This is best seen with the Spectre attack and its
derivatives, as the weakness that allows it also increases
CPU performance immensely, and will therefore not (un-
less by law) be changed or removed. Analogies to this
will exist in many types of hardware.

2.2.5 System

As mentioned above, attacks from other types of soft-
ware towards the one which the CL system resides in is

48As is mental damage to those who are denied ﬁnances to live from,

as could be the case with pension or other types of social services.

49There exist many good papers on defences and solutions against it,
see e.g., Mohd Fadzil Abdul Kadir and others, “Retpoline technique for
mitigating spectre attack” [2019] Proceedings - 2019 6th International
Conference on Electrical and Electronics Engineering, ICEEE 2019 96.
50Yingchen Wang and others, “Hertzbleed : Turning Power Side-

Channel Attacks Into Remote Timing Attacks on x86” (2022).

51Ayush Agarwal and others, “Spook.js: Attacking Chrome Strict Site

Isolation via Speculative Execution” [2022] .

the biggest threat overall. Spook.js52 does this partially
too, as it enables attacks in the browser, which in a CL
context could result in an adversary stealing information
from singular users accessing CL decisions or data which
is being provided to such a a system. On the provider
end, operation systems, proprietary software or even ma-
licious antivirus53 or just plain malware, can all cause one
or several types of software or potentially hardware to be
compromised. In this context, the weakness is the entire
infrastructure where we rely on many types of software
and hardware at once.

Solutions like trusted or trustworthy54 hardware are not
enough, and attackers merely need to exploit or use well
known techniques to gain some type of access, then lie in
wait until they can use their position to potentially reach
the system which houses the CL. Even from the perspec-
tive of the user, weaknesses on their side may allow ad-
versaries to manipulate or violate the conﬁdentiality of CL
system, adding an additional attack venue. In this sense,
the system level weakness of CL is most likely the most
severe, as the potential battleﬁeld of different options and
tools which adversaries can use are frankly many times
greater than any of the others.

types will regrettably always exist due to their passive or
irreplaceable nature56, either due to the costs for prevent-
ing any leakage or disabling designs which are necessary
for the functioning of the hardware.

2.3 Strengths

On the contrary, CL does contain certain strengths over
non-computational systems. Distributed and massive
decision-making57, simpliﬁed and efﬁcient support and
search powers (to help users or citizens) and the other
classic beneﬁts of automation apply58. However, all of it
requires the right kind of humans in-the-loop and legisla-
tive framework. Assuming the latter is true in a system,
the strengths of CL are quite clear. Regardless of this,
the strengths may not outweigh the costs, which we will
comment on later.

3 Cases

To speciﬁcally illustrate the interaction between CL and
cybersecurity, we take a look at some examples.

2.2.6 Physical

3.1 Machine-Learning Speciﬁc Issues

Destroying servers physically, or abusing physical inter-
faces such as USB55 all constitute primary reasons to have
backups and redundancies for CL systems. Physical at-
tacks, unless aimed at destruction, will be means to es-
calate and gain access to hardware and software. It may
seem like the simplest area, but defending and perhaps ex-
pecting employees to abuse their knowledge of location of
servers and so on, is in its own way paramount in cyber-
security regarding CL systems.

As mentioned, physical side-channel attacks are an-
other matter where defences must be erected, but many

52Agarwal and others (n 51).
53Gopalakrishnan Prakash and Marimuthu Parameswari, “On review-
ing the implications of Rogue Antivirus” (2016) 25(2) Journal of Infor-
mation Ethics 128.

54Trustworthy must be made to never fail, whereas trusted merely is
the idea that it should not fail, but there is no assurance it never will,
Anderson (n 33) 13.

55Tyler Thomas and others, “Duck Hunt: Memory forensics of USB
attack platforms” (2021) 37 Forensic Science International: Digital In-
vestigation 301190 (cid:104)https://doi.org/10.1016/j.fsidi.2021.301190(cid:105).

Existing taxonomies and threat models for machine-
learning (ML) models and systems sufﬁciently describe
the problem 59. But in the context of CL, we can deliber-

56Electromagnetic emissions are close to impossible to prevent ef-
ﬁciently in all systems, which means this can usually always be de-
ployed. For insights into this, see Asanka P Sayakkara and Nhien An
Le-Khac, “Forensic insights from smartphones through electromagnetic
side-channel analysis” (2021) 9 IEEE Access 13237; Asanka Sayakkara,
Nhien An Le-Khac, and Mark Scanlon, “A survey of electromagnetic
side-channel attacks and discussion on their case-progressing potential
for digital forensics” (2019) 29 Digital Investigation 43 (cid:104)https://doi.org/
10.1016/j.diin.2019.03.002(cid:105). It could also be audio, anything that can
reveal the inner workings of hardware is possible to make use of.

57On the contrary, see Jennifer Cobbe, “Administrative law and the
machines of government: Judicial review of automated public-sector
decision-making” (2019) 39(4) Legal Studies 636.

58While this is contentious, and requires more empirical research to
conﬁrm, there are indications towards this direction, see Monika Zal-
nieriute, Lyria Bennett Moses, and George Williams, “The rule of law
and automation of government decision-making” (2019) 82(3) Modern
Law Review 425.

59See examples like Rajesh Gupta and others, “Machine Learning
Models for Secure Data Analytics: A taxonomy and threat model”

ately go past the CIA triad, conﬁdentiality, integrity and
availability60, and focus on:

1. Poisoning (modiﬁcation) or possession of the train-

ing data.

2. Attacks on the model or during the creation of it.

3. Hijacking of the communication from or to the

model or where it operates.

3.1.1 Poisoning or possession of data

For ML based CL systems, this is where the biggest risk
lies. Not only can all training data, personal or not, be
stolen and published or used to make a competitive model,
but these can also result in consequences which cannot
be seen until the system has made an unfair or outright
dangerous decision. Even if it did not matter whether
the SyRi system was ML based or not61, an attack with
similar or more severe consequences like in the Post Of-
ﬁce case62, could be caused by deliberately changing the
data to make a system commit to wrong decisions or ac-
tions. There are defences, but vigilance by every human
involved, combined with an understanding that data may
be poisoned in the ﬁrst place, and regular auditing, must
be prioritised to minimize the threat which this consti-
tutes.

3.1.2 Model Attacks

Not unlike other types of software, protecting the model at
the design stage is vital, as adversaries can inject or mod-
ify parts which may, like above, cause decisions with very

(2020) 153(February) Computer Communications 406 (cid:104)https : / / doi .
org / 10 . 1016 / j . comcom . 2020 . 02 . 008(cid:105); Nicolas Papernot and oth-
ers, “SoK: Security and Privacy in Machine Learning” [2018] Pro-
ceedings - 3rd IEEE European Symposium on Security and Privacy,
EURO S and P 2018 399. For a practical summary of the current
threat model landscape, see https://docs.microsoft.com/en-us/security/
engineering/threat-modeling-aiml, last accessed 30 June 2022.

60Failures in each may overlap during speciﬁc successful attacks. The
CIA triad is a core concept in cybersecurity at large, and is understood
in a literal sense.

61Adamantia Rachovitsa and Niclas Johann, “The Human Rights Im-
plications of the Use of AI in the Digital Welfare State: Lessons Learned
from the Dutch SyRI Case” (2022) 22(2) Human Rights Law Review 1.
62James Christie, “The post ofﬁce horizon it scandal and the presump-
tion of the dependability of computer evidence” (2020) 17(March) Digi-
tal Evidence and Electronic Signature Law Review 49. The loss of lives
were a later consequence, but are still signiﬁcant.

legal or physical consequences due to its CL nature63.
Unlike poisoning, model attacks may be used by journal-
ists and competitors to attempt to reveal the contents of
the model for various purposes. Activism in cybersecu-
rity is well known64, and may play a role here and else-
where, and with the position CL takes in society, this clash
may be further exacerbated. Like other types of activism,
which include activities that may be considered criminal,
this is typically divided into black65, white66 and grey hat-
ted67, but what matters the most, is the danger that each
possess. This must be considered before using CL with
ML, as the adversaries may cause undesirable outcomes
for the provider, user and citizen respectively, explicitly
by revealing the model illegally, ethically or unethically.

3.1.3 Communication Attacks

While less dangerous than the two others, and seen heav-
ily elsewhere in various ways, communication attacks, if
done in a CL system with multiple steps, could potentially
cause some of the same issues as above. Traditionally,
these are divided into several categories, such as replay,
hijacking or modiﬁcation. The central part is that what is
sent and received is attacked, not necessarily the software
itself, in this sense its actions68. Contents of the commu-

63Or simply steal the model and use it for themselves.
64There exists plenty of research on this, from many different types of
sciences, e.g., Jordana George and Dorothy E Leidner, “Digital activism:
A hierarchy of political commitment” (2018) 2018-Janua Proceedings of
the Annual Hawaii International Conference on System Sciences 2299.
65KHazel Kwon and Jana Shakarian, “Black-Hat Hackers’ Crisis In-
formation Processing in the Darknet: A Case Study of Cyber Un-
derground Market Shutdowns” in In Networks, Hacking, and Media
– CITA MS@30: Now and Then and Tomorrow.
(November 2018)
(cid:104)https : / / www. emerald . com / insight / content / doi / 10 . 1108 / S2050 -
206020180000017007/full/html(cid:105).

66Andrew R Schrock, “Civic hacking as data activism and advocacy:
A history from publicity to open government data” (2016) 18(4) New
Media and Society 581.

67Georg Thomas, Oliver Burmeister, and Gregory Low, “The Impor-
tance of Ethical Conduct by Penetration Testers in the Age of Breach
Disclosure Laws.” (2019) 23 Australasian Journal of Information Sys-
tems 1.

68Papers which give an overview on this are, e.g., Paul Syverson, “A
Taxonomy of Replay Attacks” [1994] Proceedings The Computer Secu-
rity Foundations Workshop VII 187 (cid:104)http://ieeexplore.ieee.org/lpdocs/
epic03/wrapper.htm?arnumber=315935(cid:105); Christos Xenofontos and oth-
ers, “Consumer, Commercial, and Industrial IoT (In)Security: Attack
Taxonomy and Case Studies” (2022) 9(1) IEEE Internet of Things Jour-
nal 199; Peter Huitsing and others, “Attack taxonomies for the Modbus

nication could lead to changes in decisions, or the system
could just stop functioning from wrong received input.

3.2 Black Boxes

Black boxes may exist deliberately, or for political or un-
known reasons, but from a cybersecurity perspective, they
are not wanted or useful. Trade-secrets or patents can
warrant this, but the general idea is strong security, not
secrecy69. CL suffers further from this through the loss of
legitimacy. Transparency and openness about what a sys-
tem can and should do will increase trust and conﬁdence
and vice versa. The exceptions for this would be surveil-
lance or military purposes, but even these should still be
as strong as possible70. ML based solutions may be Black
Boxes for the two reasons mentioned above, but another
could be the complexity of the neural network which sup-
ports it. The question then becomes whether CL systems
should make use of such technology, if it is not possible
to comprehend or in a transparent manner show what goes
on inside of it. Considering the weight which CL systems
in the future may have, in both a human and legal manner,
it may simply not be advisable to use them on this basis
alone. Outside the reasoning above, it could also be due
to the security worries which a system you do not know
or understand can have.

3.3 Authentication

solve its issues72, its practical constraints through circum-
vention73 or coercion, and lack of empirical research that
really considers the population at large and not just stu-
dents or paid individuals as the data source.

CL in authentication refers to the legal identiﬁcation of
the individual, either through a legal decision or as part of
a process, and includes access to the state, banks and other
companies which may constitute critical or important dig-
ital infrastructure in the lives of individuals. The conﬂict
occurs when the CL is faulty, either deliberately or not,
and leads to injustice or ﬁnancial losses. Not unlike law
in general, there is no quick solution, and CL must guar-
antee humans-in-the-loop to function in practice.

In terms of cybersecurity, authentication may be an en-
abler for fraud through impersonation or integrity loss.
Unlike past means of fraud, authentication in CL enables
these actions to be done remotely and at a massive scale,
which lowers the ﬁnancial and practical costs and in-
creases the risk of occurrence dramatically. This must be
countered on a design or software level, but is rarely the
focus of developers and users of these systems.

4 Cybersecurity Legislation

While there is little concrete and hard law which directly
regulates cybersecurity74, we do have certain EU legisla-
tion which speciﬁes elements of it.

When authentication71 is automated in a digital manner,
it will be considered as being part of CL, regardless of
whether it is incorporated or interpreted into the legal sys-
tem.

But, authentication has a known amount of problems,
such as proliferation of new technology, which does not

protocols” (2008) 1(C) International Journal of Critical Infrastructure
Protection 37 (cid:104)http://dx.doi.org/10.1016/j.ijcip.2008.08.003(cid:105).

69Auguste Kerckhoffs, “La cryptographie militaire” (1883) IX Jour-
nal des sciences militaires 5 (cid:104)http : / / www . petitcolas . net / fabien /
kerckhoffs/(cid:105).
70ibid.
71For good taxonomy based overviews, see Mohammed El-Hajj and
others, “Analysis of authentication techniques in Internet of Things
(IoT)” (2017) 2017-Janua 2017 1st Cyber Security in Networking Con-
ference, CSNet 2017 1; Sravani Challa and others, “Authentication Pro-
tocols for Implantable Medical Devices: Taxonomy, Analysis and Fu-
ture Directions” (2018) 7(1) IEEE Consumer Electronics Magazine 57.

4.1 The Cybersecurity Regulation and Fu-

ture Legislation

Currently, cybersecurity in the EU is regulated on a prod-
uct to product type basis via guidance75, somewhat dedi-

72Sander Joos and others, Adversarial Robustness is Not Enough:
Practical Limitations for Securing Facial Authentication (1, vol 1, As-
sociation for Computing Machinery 2022).

73Jim Blythe, Ross Koppel, and Sean W Smith, “Circumvention of
security: Good users do bad things” (2013) 11(5) IEEE Security and
Privacy 80.

74This statement can be contested, on the basis that many jurisdictions
will have overarching rules and legislation which may sound like it regu-
lates it. The problem with a majority of these is that they leave the tech-
nical questions to guidance or worse, standards, without enforcement by
professionals who actually understand these attributes. A further com-
parative legal analysis of this should be done elsewhere.

75If medical devices are used as an example, see our commentary
on the role which cybersecurity guidance has in: Ludvigsen and Na-

cated rules76, through the NIS directive77 in a national and
fragmented manner, strictly by the Cybersecurity Act78
which only applies to EU institutions, and coordinated in
a semi-volunteered manner by ENISA via the Cybersecu-
rity Act. On the sidelines, we do have standards and other
measures that may be enforced on a contractual or na-
tional basis, but they come with the usual caveats. For CL,
this means that until there is harder law specifying which
techniques should and should not be used, it will exist in a
grey area. Yet again, CL will be treated like other types of
software and systems, even if there should perhaps be spe-
cialised rules to accommodate the increased risks caused
by failure, not unlike the cybersecurity requirements that
exist for critical infrastructure like telecommunication79.

4.1.1 Cybersecurity Resilience Act

Earlier in 2022, the Commission called for evidence for a
future Cybersecurity Resilience Act80. We provided com-

garaja, “Dissecting liabilities in adversarial surgical robot failures: A
national (Danish) and EU law perspective” (n 10); Kaspar Ludvigsen,
Shishir Nagaraja, and Angela Daly, “When Is Software a Medical De-
vice? Understanding and Determining the “Intention” and Requirements
for Software as a Medical Device in European Union Law” [2021] Eu-
ropean Journal of Risk Regulation 1. For other perspectives, see Ba-
nasi´nski and Rojszczak (n 16); Pier Giorgio Chiara, “The IoT and the
new EU cybersecurity regulatory landscape” [2022] (May) International
Review of Law, Computers and Technology 1 (cid:104)https://doi.org/10.1080/
13600869.2022.2060468(cid:105).

76Each type has its own Regulations and Directives. For a good ex-
ample, see Regulation 2019/941 on risk-preparedness in the electricity
sector and repealing Directive 2005/89/EC regarding critical infrastruc-
ture, [2019], L 158/1. Note that this is still not centralised hard law, but
delegation. For general commentary, see Leandros A Maglaras and oth-
ers, “Cyber security of critical infrastructures” (2018) 4(1) ICT Express
42 (cid:104)https://doi.org/10.1016/j.icte.2018.02.001(cid:105).

77Directive 2016/1148, concerning measures for a high common level
of security of network and information systems across the Union, [2016]
L 194/1.

78Through Regulation 2019/81 on ENISA (the European Union
Agency for Cybersecurity) and on information and communications
technology cybersecurity certiﬁcation and repealing Regulation (EU)No
526/2013 (Cybersecurity Act), [2019] L 151/15.

79See Edyta Karolina Szczepaniuk and Hubert Szczepaniuk, “Analy-
sis of cybersecurity competencies: Recommendations for telecommuni-
cations policy” (2022) 46(3) Telecommunications Policy 102282 (cid:104)https:
//doi.org/10.1016/j.telpol.2021.102282(cid:105) for analysis on human factors
in it.

80See Kaspar Rosager Ludvigsen and Shishir Nagaraja, “The Oppor-
tunity to Regulate Cybersecurity in the EU (and the World): Recommen-
dations for the Cybersecurity Resilience Act” [2022] 1 (cid:104)http://arxiv.org/
abs/2205.13196(cid:105).

mentary for this, but there are some further considera-
tions when discussing CL. Resilience is usually deﬁned
as a system that enables detection, tolerance and recovery
from issues81. The problem with CL, is that there must be
adequate legislation and redundancies and mechanisms,
not just technical or engineering based solutions. Real re-
silience in CL is thus only attained when we have had to
time study the failures and issues of past systems, such as
the SyRi case82, and this is traditionally how we improve
both safety and security. Resilience in CL must there-
fore be a matter of increased redundancy, in the form of
subsidiary systems or humans which can take over roles
of the CL system in an emergency, strict logic and de-
sign which focuses heavily on preventing adversarial or
non-adversarial failures through standards or existing re-
search on issues and problems with the hardware or soft-
ware used, and recovery mechanisms which allow the CL
system to keep functioning either partially or fully after
disruption of any kind. The latter should be both recovery
from actions of the system, so that the resources (time, ﬁ-
nancial) are not wasted, but also in a classic sense, where
it can recover from attacks which bring down the or par-
tially hijacks the system. There is currently no interest in
creating these in the future Cybersecurity Resilience Act,
but there may be another approach to this problem.

4.2 Artiﬁcial Intelligence Act

CL will become part of the European product legislation
world via the proposed AI Act83. This means that both
its cybersecurity84 and its mechanisms as artiﬁcial intel-
ligence85 will be the subject of regulation. How much
and and of which kind remains to be seen, as the Act has
not been ﬁnalised yet. Irrespective of the progress of the
Act, we can draw some general considerations which CL
bring. The proposed AI Act allows the EU to regulate CL

81Deﬁned more narrowly and precisely in Anderson (n 33) 251 - 252.
82Rachovitsa and Johann (n 61).
83Proposal for a Regulation of the European Parliament and of the
Council laying Down Harmonised Rules on Artiﬁcial Intelligence (Ar-
tiﬁcial Intelligence Act) and Amending Certain Union Legislative Acts,
COM/2021/206.
84Art 15.
85Through what kind of risk it constitutes, which a lot of the time will
be high, see Art 6(2) and Annex III. CL can ﬁt many of the categories
speciﬁed in Annex III, which means it will mostly be considered High
Risk AI.

in a practical manner, and since the type depends on the
area which the CL system functions in, it can either draw
rules from there86, or perhaps be granted new guidance
which it must follow under the supervision of National
Competent Authorities (authorities). Sadly, if the same
type of loose enforcement seen in other types of product
legislation continues, there is a risk of easy circumven-
tion or mere loose slaps on the wrists for private or public
providers of CL, which may not be very beneﬁcial for the
subjects which will be affected by poorly secured CL sys-
tems.

4.2.1 Cybersecurity and Resilience

Let us ﬁrstly take a closer look at Article 15:

“1. High-risk AI systems shall be designed and devel-
oped in such a way that they achieve, in the light of their
intended purpose, an appropriate level of accuracy, ro-
bustness and cybersecurity, and perform consistently in
those respects throughout their lifecycle. ...”

The AI Act considers robustness and cybersecurity
here, and the wording is much clearer and closer to the
expectations of those that build these systems. The Arti-
cle continues with:

“... 3. High-risk AI systems shall be resilient as regards
errors, faults or inconsistencies that may occur within the
system or the environment in which the system operates,
in particular due to their interaction with natural persons
or other systems. The robustness of high-risk AI systems
may be achieved through technical redundancy solutions,
which may include backup or fail-safe plans. High-risk
AI systems that continue to learn after being placed on
the market or put into service shall be developed in such a
way to ensure that possibly biased outputs due to outputs
used as an input for future operations (‘feedback loops’)
are duly addressed with appropriate mitigation measures.
...”

As CL will mostly be considered High-risk, these rules
will apply directly. The deﬁnition of resilience here ex-
cludes what is considered robustness, but in a combined
understanding, the Article covers what resilience tradi-
tionally is. All in all, these are clear and well aligned with
best practice, and even include considerations which we

did earlier regarding machine-learning. The main prob-
lem then becomes enforcement, and whether those that
use or develop the systems will actually adhere to the
rules.

4.2.2 Human Oversight

Secondarily, the AI Act also considers human oversight
in Article 14, which from a literal reading does come
very close to the kind of rules needed to regulate CL ade-
quately:

“1. High-risk AI systems shall be designed and devel-
oped in such a way, including with appropriate human-
machine interface tools, that they can be effectively over-
seen by natural persons during the period in which the AI
system is in use.”

“2. Human oversight shall aim at preventing or min-
imising the risks to health, safety or fundamental rights
that may emerge when a high-risk AI system is used in
accordance with its intended purpose or under conditions
of reasonably foreseeable misuse, in particular when such
risks persist notwithstanding the application of other re-
quirements set out in this Chapter. ...”

Article 14(1) and 14(2) entail effective oversight, a
rather strong and non-negotiable position, which warrants
transparency and ease of use, perhaps an issue for CL sys-
tems which make use of machine-learning. Minimizing
risk via human oversight is very sound, especially regard-
ing misuse87. Continuing this, Article 14(3) requires built
and implemented or identiﬁed human oversight by the
provider. This is a logical extension of the ideas above.
Article 14(4) speciﬁes exactly what the human oversight
should be capable off, effectively professional or system
requirements, including full understanding, tendency and
bias awareness, correct interpretation, knowing when to
disregard the AI, and being able to intervene or stop the
system.

Overall, Article 14 has taken all of the best elements
of how humans in-the-loop should be implemented, and
expressed it in a fairly comprehensive manner. Regard-
ing CL, the complexity will exist in how the system han-
dles and understands the law, and the Article does not re-
solve this issue, but deﬁnitely leads the way. However,
like above, we are left without opportunities for sanction-

86Various product legislation will work alongside the AI Act, see Art

6(2) and Annex II.

87Which could be adversarial failures.

ing, and there are no concrete deﬁnitions to ﬁnd here or
in the Annexes as to what exact behaviour we are look-
ing for, as this will be fairly complicated when actually
deﬁned narrowly in internal rules or guidance. This is
unlike cybersecurity, where techniques and concepts like
requiring encryption and air gaps are fairly technical, but
very possible to require in hard law.

4.2.3 Enforcement

The enforcement structure is quite important to consider
when discussing CL, as there will be a certain overlap be-
tween it and AI. Ideally, the AI Act ends up in a form or
is accompanied by additional regulation which enables it
to handle the speciﬁc consequences of CL as AI rather
well88, but in its current form, this is not the case. How-
ever, the AI Act is equipped with obligations and require-
ments, and some means to enforce them, albeit not as
harshly or directly as many had wanted89. The obliga-
tions are found in Articles 16 - 24 and 61 - 62, specif-
ically for providers90. There are additional enforcement
measures in Articles 63 - 68, which may partially rely on
the obligations above. Like other product legislation, the
authority is (usually) not the body which will technically
test whether the product conforms with the rules. Instead,
this is done by Notiﬁed Bodies91, which will likely be
private organisations, again a parallel to existing systems
in, e.g., the medical device world in the EU. Secondar-
ily, you have the aforementioned National Competent Au-
thorities92 who also act as Notifying Authorities93. Most
of the duty of care or fulﬁllment of responsibilities, and
even reporting, are put on the shoulders of the providers.
In a CL context, this can be rather dangerous, as the 15

88If Annex I stays in its current shape, all CL will be considered AI

unless they provide assistance, but even some of these may pass.

89See critical analysis in Vera L´ucia Raposo, “Ex machina: prelimi-
nary critical assessment of the European Draft Act on artiﬁcial intelli-
gence” (2022) 30(1) International Journal of Law and Information Tech-
nology 88 for more.

90Additional obligations for importers and distributors exist as well,
and uniquely, some obligations for users, which is extremely important
for CL systems. This is because unintended use or unexpected conse-
quences, in litigation, may hinge on who caused it or additionally, prod-
uct liability rules.

91Art 33, but be aware of the role which subsidiaries will play, see Art

34 for this.
92Art 59.
93Art 30.

day notice in Article 62, by virtue of how long it is, can
cause massive damage to individuals wrongfully decided
on or assisted by the CL system. The enforcement mech-
anisms themselves rely on Article 64, which should give
the authorities access to pretty much everything regarding
the AI, but for them to independently step in and inves-
tigate, there must be a risk at a national level94. CL will
not fulﬁll the requirements to be considered a risk on this
stage, so Article 67 or 68 must be used instead. Using the
same evaluation as in Article 65, the authority can force
the provider of AI to withdraw and, if possible, repair and
prevent the risk which the AI poses95. Worth noting is the
deﬁnition of risk or breach of obligations in Article 67(1):

“...

it presents a risk to the health or safety of per-
sons, to the compliance with obligations under Union or
national law intended to protect fundamental rights or to
other aspects of public interest protection ...”

For CL, the latter part is intrinsically relevant, and if lit-
erally and loyally followed, could be the supporting stone
which CL needs in its proper deployment. The problem
then becomes the authorities themselves, whether they
will realise and act on this in time and other general en-
forcement issues. The ﬁrst is (uniquely) answered in Ar-
ticle 59(4), but this can be abused or deliberately not fol-
lowed, causing the authority to become inactive or at least
barely functioning. For the second part, the wording in
Article 65(2) is ambiguous, and can be interpreted in a va-
riety of ways. If we follow the logic and ideas from exist-
ing product legislation96, it must rely on self-reporting97
or information from other national authorities, or worse,
through disclosures from journalists or researchers in an
informal manner. Lastly, the AI Act delegates the means
which the authorities can withdraw or otherwise sanction
the providers to the Member States, which sadly gives lit-
tle certainty going forward. In a CL context, this means
that ongoing injustice or ﬁnancial damage could take a
long time be discovered and resolved, and in this sense,
an ex ante review mechanism of the AI with CL features
would be much more adequate.

94Art 65(2).
95Art 67(1).
96And since there is nothing explicitly stating when, where, and how

market surveillance could ideally be done on AI in the AI Act.

97Art 61 and 62.

5 Recommendations

If we consider the clash that CL and cybersecurity bring,
we can combine it with common sense arguments from
both safety and security engineering, and the ideas found
in the AI Act, forming three recommendations:

1. CL systems must accommodate the cybersecurity
threats which they can include, at a design, deploy-
ment and post-deployment level (life-cycle), consid-
ering all levels of weakness.

2. CL systems should only be used when adequate, as
it is not a silver bullet for every problem it is applied
to. Adequate analysis and predictions must be inde-
pendently made, with the caveats which this brings
through likelihood and assumptions.

3. Humans must still play a role in CL, as they rep-
resent plasticity, adaptability and arbitration, values
which no CL system can possess. This is the only
way which the systems can handle unexpected oc-
currences, systemic injustice and other difﬁculties.

5.1 Comments

While these are more general and will apply to a general
comprehension of CL and cybersecurity in general, de-
tails and the understanding of each must be commented
on brieﬂy.

First recommendation. This concerns itself with de-
fences and mitigation measures towards adversarial
threats. Many of these techniques or measures may fol-
low from existing best practice, rules or certiﬁcations,
but for CL, the consequences may be as severe as when
cybersecurity is breached in medical devices98 or other
areas which can damage individuals or human rights.
What follows from this, is taking it gravely seriously, not
only because of the consequences, but also the procedu-
ral risk that comes with it, as failures will warrant law-
suits of various kinds depending on the jurisdiction. On
the other hand, if deployed outside of liberal democratic

98We discuss the potential consequences and attacks on medical de-
vices like surgical robots in Ludvigsen and Nagaraja, “Dissecting liabil-
ities in adversarial surgical robot failures: A national (Danish) and EU
law perspective” (n 10).

states, these considerations can be removed and CL can
be used with impunity, as is seen in China99, even if this
is not advisable. Note that wording is soft, accommodate
only indicates what is reasonable to expect, and does not
mean that it cannot be merely resilient to the failures. As
we noted earlier, resilience includes recovery, making a
broader and inclusive wording more ﬁtting.

Second recommendation. There are times where
machine-learning100 or other types of CL may not be ad-
equate. Firstly, because manual decision-making or the
existing judicial or public legal system is enough to han-
dle to problem, or secondarily, because the proportion-
ality or the consequences on rights or ﬁnances on indi-
viduals may weigh greater than implementing the system.
Regardless of this, choosing to use CL is a political deci-
sion, but this recommendation then warrants ex post criti-
cism and eventual dismantlement. There is little appreci-
ation of the conservative aspect of non-choice or denial of
certain uses of technology in existing rules, but this is tra-
ditional and well known in safety engineering. There will
simply be times where safety risks outweigh the eventual
advantages a system may bring, which means it should
not be used, regardless of the ﬁnancial or personal inter-
ests in it. This should in particular apply to CL because of
its potentially systemic damage capabilities.

Third Recommendation. Mandating humans in sys-
tems is not in any way new, but we ask for it for a
more traditional reason: The qualities which only humans
have101. This is, again, a reference to the role which
humans have in safety engineering, where our ability to
adapt in emergencies make us invaluable102. Of course,
the caveat to this is where adversarial behaviour or sludg-

99Fan Liang and Yuchen Chen, “The making of “good” citizens:
China’s Social Credit Systems and infrastructures of social quantiﬁca-
tion” (2022) 14(1) Policy and Internet 114.

100Isabel Chien and others, “Multi-disciplinary fairness considerations
in machine learning for clinical trials” [2022] 906 (cid:104)http://arxiv.org/abs/
2205.08875(cid:105).

101But it may have negative consequences, if used in a manner to cir-
cumvent or otherwise abuse the good ideas behind human oversight, see
Ben Green, “The ﬂaws of policies requiring human oversight of govern-
ment algorithms” (2022) 45 Computer Law & Security Review 105681
(cid:104)https://doi.org/10.1016/j.clsr.2022.105681(cid:105).

102Leveson (n 29) 101 - 102.

ing103 causes us to not react appropriately. Still, humans
must act as both emergency measures and redundancies,
the latter speciﬁcally referring to humans stepping in and
overtaking the role which CL had so far. This could be
in decision-making or automated assistance, both ﬁelds
where humans used to sit, which means we still have
experience and infrastructure to provide it, albeit much
slower than what CL can provide. Article 14 in EU’s pro-
posed AI Act is a great example of how such a rule could
be designed.

6 Future Work

Continuous in-depth analysis of existing CL systems, and
acknowledging what constitutes CL to include as much
as possible is, in our view, warranted. This should include
interdisciplinary aspects, as understanding one side or an-
other is not sufﬁcient to actually comprehend the system
as a whole, consequences, constraints and all.

The role which economics play in both enforcement
or lobbying by powerful corporations or other states
should be considered regarding CL too. Essentially, eco-
nomic studies which critically consider the potential loop-
holes and adversarial actions that powerful actors can use
against CL systems, is wanted on the basis of this paper,
perhaps extending existing economics of security ideas.

Based on our very informal recommendations, ex-
panded analysis on what constitutes the right situation
to deploy CL, a condensed analysis of which threats for
which types of CL exist, and the necessary human roles
needed in CL would be adequate.

And ﬁnally, fusing CL research with existing but much
older digitalised law104 and legal informatics is suitable,
as both ﬁelds have common agendas and perceptions of
digital law, and would beneﬁt from the expertise and al-
ternative starting points which each bring.

103Much great literature exists on this, see Laura A Paul and others,
“Teaching and Educational Methods Nudge or Sludge ? An In-Class Ex-
perimental Auction Illustrating How Misunderstood Scientiﬁc Informa-
tion Can Change Consumer Behavior 1 Introduction and Background”
(2022) 4(1) Applied Economics Teaching Resources (AETR) 34, for a
recent overview and example.

104Such as the works by Jon Bing.

7 Conclusion

As it is clear, CL presents a situation where law gains both
the advantages and disadvantages of the other discipline
which inspired its existence105. In this paper, we ﬁrstly
made a conceptualisation of cybersecurity in CL speciﬁ-
cally, which consists of ﬁve levels. Now that CL absorbs
cybersecurity weaknesses which occur in each layer, we
show that there exists mitigation measures for the prob-
lems. But we know from safety and security engineering,
that an ongoing arms race is and will be needed going for-
ward, a sort of constant ongoing development of defences
and considerations, which attempts to mitigate or lessen
the damage that attacks can cause. Building these into CL
will be vital, just as it is with cybersecurity everywhere
else.

We ﬁnd that CL is vulnerable to the same issues which
both AI and neural networks face, if the systems make
use of the technology106. The logic and practical imple-
mentation in which CL is implemented, presents a danger
which very few other legal disciplines contain. There-
after, we show how fragmented and rather softly cyber-
security is regulated in the EU, which in relation to CL,
presents practical and real problems. But, we then anal-
yse the AI Act, and it provides sound measures known
from elsewhere regarding cybersecurity and human over-
sight. Regardless of issues with enforcement, this illus-
trates promise and understanding for a future where CL
will play a central role in the legal system and in public
consciousness in general. Clearly, the AI Act here com-
bines CL (as AI) and cybersecurity, and regulates them
together in a legal sense. Finally, we provide three recom-
mendations for CL related to cybersecurity, which sum-
marised are: Accommodation of threats, adequate use of
CL, maintenance of the human role.

These are general, but from a EU perspective, the AI
Act already does a good job in implementing them all,
giving us hope for a future where CL and cybersecurity

105Not unlike the issues which philosophy or political directions has
caused of systemic damage to law in the past. An example of this could
be “communist law”, see e.g., Jiˇr´ı Pˇrib´aˇn, “From ‘which rule of law?’ to
‘the rule of which law?’: Post-communist experiences of european legal
integration” (2009) 1(2) Hague Journal on the Rule of Law 337.

106This may seem rather redundant to mention, but these tend to be
overlooked in favour of what advantages the systems may bring. In this
sense, making sure that all aspects of new technology are discussed and
understood is a core role of researchers and the public alike.

can safely melt together.

Figure 1: Visualisation of the 5 weakness levels and their interactions.

Design/LogicSoftwareHardwareSystemPhysical