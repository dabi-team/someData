RiskLoc: Localization of Multi-dimensional Root Causes by
Weighted Risk

Marcus Kalander
Noahâ€™s Ark Lab, Huawei Technologies
marcus.kalander@huawei.com

2
2
0
2

y
a
M
0
2

]

G
L
.
s
c
[

1
v
4
0
0
0
1
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Failures and anomalies in large-scale software systems are unavoid-
able incidents. When an issue is detected, operators need to quickly
and correctly identify its location to facilitate a swift repair. In this
work, we consider the problem of identifying the root cause set that
best explains an anomaly in multi-dimensional time series with
categorical attributes. The huge search space is the main challenge,
even for a small number of attributes and small value sets, the
number of theoretical combinations is too large to brute force. Pre-
vious approaches have thus focused on reducing the search space,
but they all suffer from various issues, requiring extensive manual
parameter tuning, being too slow and thus impractical, or being
incapable of finding more complex root causes.

We propose RiskLoc to solve the problem of multidimensional
root cause localization. RiskLoc applies a 2-way partitioning scheme
and assigns element weights that linearly increase with the distance
from the partitioning point. A risk score is assigned to each element
that integrates two factors, 1) its weighted proportion within the
abnormal partition, and 2) the relative change in the deviation score
adjusted for the ripple effect property. Extensive experiments on
multiple datasets verify the effectiveness and efficiency of RiskLoc,
and for a comprehensive evaluation, we introduce three synthet-
ically generated datasets that complement existing datasets. We
demonstrate that RiskLoc consistently outperforms state-of-the-art
baselines, especially in more challenging root cause scenarios, with
gains in F1-score up to 57% over the second-best approach with
comparable running times.

KEYWORDS
Root cause localization; Anomaly localization; Multi-dimensional
time series

1 INTRODUCTION
In large software systems, various types of failures and anomalies
are common and unavoidable occurrences [9]. A failure can se-
verely degrade system performance and affect service availability
which may lead to economic losses [2]. Therefore, it is critical to
quickly identify and localize a failure before it can affect the user
experience. To this end, all important metrics, key performance in-
dicators (KPIs), are generally collected periodically and monitored
to identify performance or quality issues [10, 16, 17]. In practice,
anomaly detection models are applied to these collected time series,
and when an anomaly is identified, an operator is alerted or the
system initializes a self-diagnosis and repair procedure.

However, it is not always feasible or practical to run anomaly de-
tection models on all individual time series due to the overwhelming
quantity of KPIs and difficulties with low sample sizes, e.g., spo-
radic and short-lived user-specific KPIs. The anomaly detection is

Table 1: Example of a multi-dimensional measure with two at-
tributes. The abnormal rows are in bold and the best root cause set
is { (ğ‘‹ , âˆ—) }.

Data Center Device Type Actual

Forecast

X
X
Y
Y
Y

D1
D2
D1
D2
D3

Total

10
3
15
30
100

158

30
10
14
30
102

186

thus often run on aggregated KPIs, where the aggregation is done
over several shared attributes such as geographical region and data
center [4, 7]. Each attribute has a range of categorical values on
which to aggregate, and the data can thus be seen as multidimen-
sional time series data. We refer to the attribute combinations and
their attribute values at different levels of aggregation as elements.
When anomalous behavior is identified in an aggregated measure,
we need to quickly locate where the fault lies, that is, identify the
root cause. Specifically, we want to localize the set of elements that
best explains the anomaly.

To identify the root cause of an anomaly, the expected (fore-
casted) values for each element are compared to the actual values
in the abnormal period. Table 1 shows an example with two at-
tributes where the aggregated measure (total) is abnormal with
the root cause being {(ğ‘‹, âˆ—)}, where âˆ— indicates an aggregated at-
tribute. Due to the nature of the problem, the root cause is a set of
elements with different levels of aggregation. The main challenge
is the huge search space since we need to consider all possible
combinations of any number of attribute values. Even for tiny toy
examples such as the one in Table 1, the number of potential root
causes is 22+3+5 âˆ’ 1. For a measure with ğ‘‘ dimensions each with ğ‘›
values, the number of valid elements are (cid:205)ğ‘‘
(cid:1)ğ‘›ğ‘– = (ğ‘› + 1)ğ‘‘ âˆ’ 1
which gives 2(ğ‘›+1)ğ‘‘ âˆ’1 âˆ’ 1 number of possible combinations.

ğ‘–=1

(cid:0)ğ‘‘
ğ‘–

Evidently, it is not possible to examine the whole search space for
even reasonably small ğ‘‘ and ğ‘›, hence, previous works have focused
on reducing the search space. Adtributor [3] and iDice [8] focus
only on simpler cases. Adtributor can only identify root causes that
lie within a single attribute, while iDice assumes a low number
of root cause elements. R-Adtributor [11] is a recursive version
of Adtributor that incorporates the entire search space; however,
the stop condition for the recursion is hard to adjust correctly.
Apriori [1] suffers from the same tuning difficulty, and its running
time is an order of magnitude slower than the rest. HotSpot [13] can
be slow and has problems finding root causes with many elements
with low levels of aggregation. Squeeze [7] proposes a clustering
approach and top-down search for each cluster, however, it is highly

 
 
 
 
 
 
sensitive to the clustering outcome and has difficulty with very fine-
grained anomalies. AutoRoot [6] improves on Squeeze, however,
it cannot identify multiple elements in the same root cause and is
still highly dependent on clustering accuracy.

To resolve the above challenges, we propose RiskLoc, which par-
titions the data into two sets, normal and anomalous, and iteratively
identifies high-risk root cause elements by applying our proposed
risk score. The risk score consists of two components, ğ‘Ÿ1 and ğ‘Ÿ2,
where ğ‘Ÿ1 considers the weighted proportion of an element in the
normal and anomalous partitions while ğ‘Ÿ2 compares the impact
when adjusting for the ripple effect [13] (a property stating that
changes in an element will propagate to its descendant elements).
Iteration stops when the identified elements explain the abnormal
divergence to a satisfactory degree. Previous works have mainly
used proprietary data for method evaluation, and only [7] have
made various semi-synthetic datasets publicly available. Unfor-
tunately, there is a distinct lack of real-world datasets, and the
semi-synthetic ones are of relatively small-scale. To demonstrate
the effectiveness of our approach, we introduce multiple synthetic
datasets with different characteristics, including one that is multiple
magnitudes larger than previous datasets.
Our main contributions are as follows.

â€¢ We propose RiskLoc to accurately and efficiently locate root
causes in multi-dimensional time series without any sig-
nificant root cause assumptions. We exploit a partitioning
scheme and iteratively search for probable root cause ele-
ments within the abnormal partition. A heuristic risk score is
proposed to appraise elements by integrating two key com-
ponents: 1) the weighted deviation score distribution of an
element and 2) the impact of adjusting for the ripple effect
property.

â€¢ We introduce three fully synthetic benchmark datasets for
comprehensive algorithm evaluation that complement the
publicly available semi-synthetic datasets [7] since these
datasets do not cover the entire search space; there are no
root causes with multiple elements in the same cuboid and
the datasets are all relatively small-scale. Furthermore, the
code for the synthetic dataset generation is released, together
with code for RiskLoc and baselines, to help alleviate the
problem of data scarcity in the field.1

â€¢ We empirically demonstrate the advantages of RiskLoc with
extensive experiments on multiple datasets. RiskLoc consis-
tently outperforms state-of-the-art baselines. On the most
challenging semi-synthetic dataset, RiskLoc attains an im-
provement of 57% over the second-best approach, while the
gains on the synthetic datasets is up to 41%.

2 RELATED WORK
Root cause localization in multidimensional measures is becoming
increasingly relevant with increasing system size and higher de-
mand for service reliability. Several previous works have focused
on specific scenarios (e.g., advertising systems [3, 11]) and, more
recently, generic approaches [6, 7]. The pioneering work in the
area is Adtributor [3] which uses explanatory power and surprise
metrics to identify the set of most likely root causes; however, it is

1Source code is available at https://github.com/shaido987/riskloc.

limited to finding root causes in a single attribute. R-Adtributor [11]
extends Adtributor to handle root causes in multiple attributes by
recursively running Adtributor. However, the termination condi-
tion of when to stop the recursion is hard to specify, often returning
inexact and verbose root causes sets.

Apriori [1] was proposed as part of a system for detection and
localization of end-to-end performance degradations at cellular
service providers. It identifies leaf elements where the actual values
deviate from the predicted values and then applies association
rule mining techniques to localize the root cause. The method is
sensitive to the parameter selections for support and confidence,
requiring manual fine-tuning depending on the data. Furthermore,
the running time is often too long for real-time usage.

iDice [8] uses various pruning strategies to identify effective
attribute combinations (referred to as elements in this work) of
emerging issues within issue reports. However, it is created to
handle simpler cases where only a few of the elements are affected
by the anomaly and will thus perform poorly in more complex
situations.

Hotspot [13] adopts Monte Carlo tree search (MCTS) to manage
the large search space. To evaluate the element sets, they introduce
the ripple effect property and, building on this, a potential score for
ranking potential root causes. HotSpot has difficulty identifying
more complex root causes involving elements with many attributes
or multiple elements. Moreover, both HotSpot and iDice focus on
simple (fundamental) measures and are not designed to handle
nonadditive measures.

As established by [7], the above works may miss elements that
have insignificant anomaly magnitudes, and they propose Squeeze
to rectify this issue. To prune the search space, a density-based
clustering on the leaf elements relative residuals is used and each
cluster is searched for a root cause set by leveraging a generalized
potential score. The assumption is made that each cluster only con-
tains elements with the same root cause, which may not always be
true. The final result is highly dependent on the clustering accuracy,
which involves hard-to-determine parameters. In addition, Squeeze
relies on a heuristic method to sort the elements before scoring
the partitions, however, this ranking fails for the most fine-grained
elements and can result in numerous false positives.

More recently, AutoRoot [6] improves upon Squeeze by intro-
ducing parameter-free clustering, relative scoring, and simplified
filtering strategy. However, the issues of reliance on accurate clus-
tering and the assumption of a single root cause per cluster are still
present. AutoRoot makes further assumptions on the root cause
and removes the ability to return multiple elements from the same
cluster to speed up execution.

3 PRELIMINARIES
In this section, we begin by defining the problem, the related nota-
tion, and terminology. Then, we briefly introduce the ripple effect
property, a key assumption for the success of the algorithm. Finally,
we give a simplified example to clarify the problem.

3.1 Problem Formulation and Terminology
When an anomaly is identified in an aggregated measure, we want
to localize the combination of attributes and attribute values that

2

number of items, and response times are examples of fundamental
measures. On the other hand, derived measures are obtained by
applying a function on fundamental measures and are generally
nonadditive. Relevant examples are revenue per page view and
success rates, i.e., quotients of fundamental measures. Derived mea-
sures may require more complex handling depending on the applied
method [7].

Each element will have a actual value and a forecast value. We
denote these as ğ‘£ (Â·) and ğ‘“ (Â·), respectively. The actual value is mea-
sured in the monitored system while the forecast value is the result
of a forecasting algorithm, e.g., Prophet [14], ARMA [12], or simple
moving average. For fundamental measures, the values of the leaf
elements can be found directly while the values of the non-leaf
elements are simply the sum of its descendant leaf elements, i.e.,

ğ‘£ (ğ‘’) =

âˆ‘ï¸

ğ‘£ (ğ‘’ â€²),

ğ‘“ (ğ‘’) =

âˆ‘ï¸

ğ‘“ (ğ‘’ â€²).

(3)

ğ‘’â€² âˆˆğ¿ğ· (ğ‘’)

ğ‘’â€² âˆˆğ¿ğ· (ğ‘’)

For a derived measure ğ‘€ğ‘‘ , we denote the function to create the
derived measure as â„(Â·), where ğ‘€ğ‘‘ = â„(ğ‘€1, ..., ğ‘€ğ‘›) and {ğ‘€1, ..., ğ‘€ğ‘› }
are the fundamental measures used. For non-leaf elements, the ag-
gregated actual values are computed as ğ‘£ (ğ‘’) = â„(ğ‘£1 (ğ‘’), ..., ğ‘£ğ‘› (ğ‘’)),
where {ğ‘£1 (ğ‘’), ..., ğ‘£ğ‘› (ğ‘’)} are the actual values of the respective fun-
damental measure. The forecast values are computed analogously
as ğ‘“ (ğ‘’) = â„(ğ‘“1 (ğ‘’), ..., ğ‘“ğ‘› (ğ‘’)).

Another relevant term is the explanatory power [3] of an element
which is defined as the portion of change in the overall measure
that can be explained by the change in the given elementâ€™s value.
For an element ğ‘’ of a fundamental measure ğ‘€, we have:

ğ‘’ğ‘ (ğ‘’) :=

ğ‘£ (ğ‘’) âˆ’ ğ‘“ (ğ‘’)
ğ‘£ (ğ‘€) âˆ’ ğ‘“ (ğ‘€)

.

(4)

For derived measures, we refer to the derivation given in [3].

3.2 Ripple Effect and Deviation Score
The ripple effect was introduced by Sun et al. [13] and was further
generalized to derived measures and zero forecast values by Li et al.
[7]. The property characterizes how a change in an element is prop-
agated to its descendant leaf elements. Since the actual and forecast
values of an element ğ‘’ are the aggregation of its descendant leaf
element values, a change in ğ‘’ will undoubtedly affect the elements
in ğ¿ğ· (ğ‘’). The ripple effect property states that an element ğ‘’ğ‘Ÿ in the
root cause set ğ‘…ğ‘† will affect all elements ğ‘’ âˆˆ ğ¿ğ· (ğ‘’ğ‘Ÿ ) following the
proportions of their forecast values, that is:

=

ğ‘“ (ğ‘’) âˆ’ ğ‘£ (ğ‘’)
ğ‘“ (ğ‘’)

ğ‘“ (ğ‘’ğ‘Ÿ ) âˆ’ ğ‘£ (ğ‘’ğ‘Ÿ )
ğ‘“ (ğ‘’ğ‘Ÿ )
The extension in [7] to generalized ripple effect (GRE) replaces ğ‘“ (Â·)
with
to allow for zero forecast values. The enhanced prop-
erty is thus as follows:

, (ğ‘“ (ğ‘’ğ‘Ÿ ) â‰  0).

ğ‘“ ( Â·)+ğ‘£ ( Â·)
2

(5)

ğ‘“ (ğ‘’) âˆ’ ğ‘£ (ğ‘’)
ğ‘“ (ğ‘’) + ğ‘£ (ğ‘’)

=

ğ‘“ (ğ‘’ğ‘Ÿ ) âˆ’ ğ‘£ (ğ‘’ğ‘Ÿ )
ğ‘“ (ğ‘’ğ‘Ÿ ) + ğ‘£ (ğ‘’ğ‘Ÿ )

.

(6)

Following this property, the deviation score [7] of an element ğ‘’ is
defined as:

ğ‘‘ğ‘  (ğ‘’) := 2 Â·

,

(7)

ğ‘“ (ğ‘’) âˆ’ ğ‘£ (ğ‘’)
ğ‘“ (ğ‘’) + ğ‘£ (ğ‘’)

which adhering to GRE will theoretically give all leaf elements in
the same root cause similar scores.

3

Figure 1: Cuboid relation graph with ğ‘‘ = 4.

best explain the anomalous behaviour. We use time series data or
log data with timestamps as input and aggregate all values with the
same accompanying attributes within the anomaly interval; e.g., in
Table 1, the time information has been removed, and each attribute
combination has a single aggregated value.

More formally, we consider an aggregated measure ğ‘€ with a set
of attributes A = {ğ´1, ğ´2, ..., ğ´ğ‘‘ } where each attribute ğ´ğ‘– âˆˆ A con-
tains a set of ğ‘›ğ‘– distinct categorical values Vğ‘– = {ğ‘£1, ğ‘£2, ..., ğ‘£ğ‘›ğ‘– }. We
denote a distinct combination of attribute values {(ğ‘ 1, ..., ğ‘ ğ‘‘ )|ğ‘ 1 âˆˆ
{V1 âˆª âˆ—}, ..., ğ‘ ğ‘‘ âˆˆ {Vğ‘‘ âˆª âˆ—}} as an element, where a wildcard âˆ— de-
notes aggregation in an attribute following the notation in [13]. For
example, for a measure with ğ‘‘ = 4 attributes, we can describe the
fully aggregated measure as (âˆ—, âˆ—, âˆ—, âˆ—). Similarly, a set of elements
{(ğ‘ , âˆ—, âˆ—, âˆ—)|ğ‘  âˆˆ V1} have their second, third, and fourth attributes
fully aggregated. Our goal is to localize the root cause set ğ‘…ğ‘† that
contains the elements, at any level of aggregation, that together
offer the best explanation of the anomalous behaviour while being
as succinct as possible.

The most fine-grained elements are denoted as leaf elements.
These elements do not have any aggregation and the set of leaf
elements is thus denoted as E = {(ğ‘ 1, ğ‘ 2, ğ‘ 3, ğ‘ 4)|ğ‘ 1 âˆˆ V1, ğ‘ 2 âˆˆ
V2, ğ‘ 3 âˆˆ V3, ğ‘ 4 âˆˆ V4}. An element ğ‘’2 = (ğ‘¢1, .., ğ‘¢ğ‘‘ ) is considered a
proper descendant of ğ‘’1 = (ğ‘£1, ..., ğ‘£ğ‘‘ ) iff

(ğ‘’1 â‰  ğ‘’2) âˆ§

ğ‘‘
(cid:219)

ğ‘–=1

(ğ‘¢ğ‘– = ğ‘£ğ‘– âˆ¨ ğ‘£ğ‘– = âˆ—).

(1)

For convenience, we define the set of descendant leaf elements of
an element ğ‘’ as:

ğ¿ğ· (ğ‘’) = {ğ‘’ â€²|(ğ‘’ â€² descendant of ğ‘’) âˆ§ (ğ‘’ â€² is a leaf element)}.

(2)

Based on the different degrees of aggregation, we form sets of
elements called cuboids [5]. We denote the set of all cuboids as C
where a single cuboid ğ¶ğ‘˜ âˆˆ C (ğ‘˜ being an arbitrary combination
of 1, ..., ğ‘‘) consists of the elements whose aggregated attributes are
the same. The cuboid containing leaf elements is thus denoted as
ğ¶1234. Cuboids can be separated into different layers determined
by the number of fully aggregated attributes, as shown in Figure 1
where the lines represent the parent-child relationship between the
cuboids. We further denote ğ¸ğ‘† (ğ¶) to be the element set of cuboid
ğ¶, e.g., we have ğ¸ğ‘† (ğ¶1) = {(ğ‘ , âˆ—, âˆ—, âˆ—)|ğ‘  âˆˆ V1}.

Two types of measures are considered, fundamental and derived
measures [3]. Fundamental measures are additive and can easily be
divided into segments following their attribute values. Page views,

Figure 2: An overview of RiskLoc.

3.3 Problem Intuition
To clarify the problem of localizing the set of elements that best
explains the anomaly, we take the simple example in Table 1. We
denote the two cuboids in the first layer as ğ¶ğ‘ and ğ¶ğ‘¡ , represent-
ing the data center and device type, respectively. The second layer
has a single cuboid ğ¶ğ‘ğ‘¡ with leaf elements. Each cuboid contains
a set of elements with the relevant aggregations, i.e., ğ¸ (ğ¶ğ‘ ) =
{(ğ‘‹, âˆ—), (ğ‘Œ, âˆ—)}, ğ¸ (ğ¶ğ‘¡ ) = {(âˆ—, ğ·1), (âˆ—, ğ·2), (âˆ—, ğ·3)}, and ğ¸ (ğ¶ğ‘ğ‘¡ ) =
{(ğ‘‹, ğ·1), (ğ‘‹, ğ·2), (ğ‘Œ , ğ·1), (ğ‘Œ , ğ·2), (ğ‘Œ , ğ·3)}.

The anomaly detection is performed on the fully aggregated
measure; in Table 1 the aggregated forecast value is ğ‘“ (âˆ—, âˆ—) = 186,
while the actual value is ğ‘£ (âˆ—, âˆ—) = 158. The best set of each cuboid
are compared to determine the best overall element set. For cuboid
ğ¶ğ‘¡ , we can determine that (âˆ—, ğ·3) is likely not affected by the anom-
aly. For (âˆ—, ğ·1) and (âˆ—, ğ·2) the aggregated values are 44 and 40
while the actual values are 25 and 33, making this a possible root
cause. However, in ğ¶ğ‘ , (ğ‘‹, âˆ—) offers a better and more succinct ex-
planation of the root cause, as it contains only the two abnormal
leaf elements. Succinctness (i.e., Occamâ€™s razor principle) is also
the reason to prefer (ğ‘‹, âˆ—) in favor of the root cause set in ğ¶ğ‘ğ‘¡ :
{(ğ‘‹, ğ·1), (ğ‘‹, ğ·2)}. The set of elements that best explain the root
cause is therefore {(ğ‘‹, âˆ—)}.

4 METHODOLOGY
In this section, we introduce RiskLoc, our proposed solution to the
multi-dimensional root cause localization problem. An overview
of RiskLoc is shown in Figure 2. In essence, when an anomaly
is detected by some anomaly detection module on a monitored
measure, RiskLoc is initiated and the search for the root cause
location is started. The actual values of all leaf elements are acquired
and aggregated for the anomaly time interval and the forecast
values are computed from historical data. RiskLoc employs three
main components in its search for the root cause set, 1) leaf element
partitioning and weighting, 2) risk score, and 3) element search and
iteration. The details of each component and an optional element
pruning strategy are presented below.

4.1 Leaf Element Partitioning and Weighting
We compute the deviation score for each leaf element using Equa-
tion 7. Leaf elements with small deviation scores can be considered
normal, as their relative forecast residuals are small, while devia-
tion scores further from zero may be anomalous. Two histograms

Figure 3: Two examples of typical deviation score distributions.

Algorithm 1 Partitioning and Weighting
Input: leaf element set E
1: D = {ğ‘‘ğ‘  (ğ‘’) | ğ‘’ âˆˆ E}
2: Remove outliers in D
3: if |min D | < |max D | then
4:

ğ‘¡ = âˆ’ min D
ğ¸ğ‘› = {(ğ‘’, |ğ‘¡ âˆ’ ğ‘‘ğ‘  (ğ‘’)| , 0) | ğ‘’ âˆˆ E, ğ‘‘ğ‘  (ğ‘’) < ğ‘¡,

ğ‘“ (ğ‘’) â‰  0 âˆ¨ ğ‘£ (ğ‘’) â‰  0}

ğ¸ğ‘ = {(ğ‘’, |ğ‘‘ğ‘  (ğ‘’)| , 1) | ğ‘’ âˆˆ E, ğ‘‘ğ‘  (ğ‘’) â‰¥ ğ‘¡ }

ğ‘¡ = âˆ’ max D
ğ¸ğ‘› = {(ğ‘’, |ğ‘¡ âˆ’ ğ‘‘ğ‘  (ğ‘’)| , 0) | ğ‘’ âˆˆ E, ğ‘‘ğ‘  (ğ‘’) > ğ‘¡,

ğ‘“ (ğ‘’) â‰  0 âˆ¨ ğ‘£ (ğ‘’) â‰  0}

5:

6:

10:

11:

7:
8: else
9:

ğ¸ğ‘ = {(ğ‘’, |ğ‘‘ğ‘  (ğ‘’)| , 1) | ğ‘’ âˆˆ E, ğ‘‘ğ‘  (ğ‘’) â‰¤ ğ‘¡ }

12:
13: end if
14: ğ¸ğ‘§ = {(ğ‘’, 0, 0) | ğ‘’ âˆˆ E, ğ‘“ (ğ‘’) = 0, ğ‘£ (ğ‘’) = 0}
15: Eğ‘¤ = ğ¸ğ‘› âˆª ğ¸ğ‘ âˆª ğ¸ğ‘§
16: Eğ‘¤ = {(ğ‘’, min{ğ‘¤, 1}, ğ‘) | (ğ‘’, ğ‘¤, ğ‘) âˆˆ Eğ‘¤ }
Output: Weighed leaf element set Eğ‘¤

illustrating typical deviation score distributions can be seen in Fig-
ure 3. Note that the deviation scores of the anomalous leaf elements
share the same sign. This is generally the case as an anomaly was
detected in the aggregated measure, anomalies with conflicting sign
risk cancel each other when aggregating and are thus less likely to
be detected.

We use deviation scores to separate the leaf elements into a
normal and an abnormal set using a simple 2-way partitioning
scheme. The elements are given weights corresponding to their
distance from the partitioning point. The complete procedure is
detailed in Algorithm 1. A suitable partitioning point is determined
by relying on the simple assumption that normal elements are
relatively evenly distributed around zero, i.e., that the forecasts are
reasonably accurate, and that the anomalous leaf element share
the same sign. We capture the maximum absolute deviation score
(adjusting for a few outliers; we simply remove the top and bottom
5 unique values) in the opposite direction to the anomaly and flip
the sign. This estimates the maximum absolute deviation score of
the normal data. The anomaly direction is simultaneously inferred
from the data but could alternatively be obtained from the anomaly
detection procedure.

4

Algorithm 2 Element Search (ES)
Input: weighed leaf element set Eğ‘¤, risk threshold ğ‘¡ğ‘Ÿ , explanatory

Algorithm 3 RiskLoc
Input: leaf element set E, risk threshold ğ‘¡ğ‘Ÿ , proportional explana-

power threshold ğ‘¡ğ‘’ğ‘

1: for ğ‘™ = 1 to ğ‘‘ do
ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  = {}
2:
for ğ¶ âˆˆ C in layer ğ‘™ do

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

prune elements in ğ¸ğ‘† (ğ¶)
for ğ‘’ğ‘Ÿ âˆˆ ğ¸ğ‘† (ğ¶) do

ğ‘Ÿğ‘–ğ‘ ğ‘˜ = apply equation 12 on ğ‘’ğ‘Ÿ
if ğ‘Ÿğ‘–ğ‘ ğ‘˜ â‰¥ ğ‘¡ğ‘Ÿ and ğ‘’ğ‘ (ğ‘’ğ‘Ÿ ) â‰¥ ğ‘¡ğ‘’ğ‘ then
ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  = ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  âˆª {ğ‘’ğ‘Ÿ }

end if
end for

end for
if ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  is not empty then

return ğ‘’ğ‘Ÿ âˆˆ ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  with maximum ğ‘’ğ‘ (ğ‘’ğ‘Ÿ )

end if
14:
15: end for
16: return null

ğ‘’ğ‘ (Eğ‘¤) = âˆ’ğ‘’ğ‘ (Eğ‘¤)

tory power threshold ğ‘¡ğ‘ğ‘’ğ‘
1: Obtain Eğ‘¤ using Algorithm 1
2: ğ‘… = {ğ‘’ | (ğ‘’, ğ‘¤, ğ‘) âˆˆ Eğ‘¤, ğ‘ = 1}
3: if ğ‘’ğ‘ (ğ‘…) < 0 then
4:
5: end if
6: ğ‘¡ğ‘’ğ‘ = ğ‘¡ğ‘ğ‘’ğ‘ Â· ğ‘’ğ‘ (ğ‘…)
7: ğ‘…ğ‘† = {}
8: while ğ‘’ğ‘ (ğ‘…) â‰¥ ğ‘¡ğ‘’ğ‘ do
ğ‘’ğ‘Ÿ = ES(Eğ‘¤, ğ‘¡ğ‘Ÿ , ğ‘¡ğ‘’ğ‘ )
9:
if ğ‘’ğ‘Ÿ is null then

10:

âŠ² Negate ğ‘’ğ‘ for all elements

11:

12:

13:

14:

break

end if
Eğ‘¤ = {(ğ‘’, ğ‘¤, ğ‘) | (ğ‘’, ğ‘¤, ğ‘) âˆˆ Eğ‘¤, ğ‘’ âˆ‰ ğ¿ğ· (ğ‘’ğ‘Ÿ )}
ğ‘… = {ğ‘’ | (ğ‘’, ğ‘¤, ğ‘) âˆˆ Eğ‘¤, ğ‘ = 1}
ğ‘…ğ‘† = ğ‘…ğ‘† âˆª {ğ‘’ğ‘Ÿ }

15:
16: end while
Output: Set of root causes ğ‘…ğ‘†

Each leaf element is given a weight corresponding to the distance
from the partitioning point. For the anomalous elements, the weight
is the deviation score (i.e., a linear increase from the partitioning
point) with a maximum value of 1. For the normal leaf elements,
we similarly increase the weight linearly from the partitioning
point; however, we start the weights with 0. In this way, normal
leaf elements close to the partitioning point will have small weights.
The influence of these leaf elements is thus reduced since they are
more likely to be anomalous than points further away. Therefore,
the accuracy of the partitioning scheme will have a lower impact on
the final result, in contrast to the rigid clustering used by Squeeze
and AutoRoot. We further note that leaf elements with both ğ‘£ (ğ‘’) = 0
and ğ‘“ (ğ‘’) = 0 do not contain any information to determine whether
their parent elements are anomalous or not. The weights of these
leaf elements are thus set to 0.

4.2 Risk Score
We propose a heuristic risk score to identify potential root cause
elements in each cuboid, i.e., high-risk elements. The risk score
consists of two terms, ğ‘Ÿ1 and ğ‘Ÿ2, which reflect the two key aspects
of the root cause elements: descendant elements are mainly in the
anomalous partition, and the forecast residuals follow the property
of the ripple effect.

For an element ğ‘’ğ‘Ÿ , independent of the level of aggregation, we
obtain the weighted sum of its descendant leaf elements separated
by the partitioning scheme. Let Eğ‘¤ be the weighted leaf element
set obtained by Algorithm 1 and ğ‘† the descendant leaf elements of
ğ‘’ğ‘Ÿ , i.e., ğ‘† = {(ğ‘¤, ğ‘) | (ğ‘’, ğ‘¤, ğ‘) âˆˆ Eğ‘¤, ğ‘’ âˆˆ ğ¿ğ· (ğ‘’ğ‘Ÿ )}, then we have:

ğ‘¤ğ‘ =

âˆ‘ï¸

ğ‘¤,
(ğ‘¤,1) âˆˆğ‘†

ğ‘¤ğ‘› =

âˆ‘ï¸

ğ‘¤ .
(ğ‘¤,0) âˆˆğ‘†

We define ğ‘Ÿ1 as follows:

ğ‘Ÿ1 =

ğ‘¤ğ‘
ğ‘¤ğ‘› + ğ‘¤ğ‘ + 1

.

(8)

(9)

The purpose of ğ‘Ÿ1 is to verify whether ğ‘’ğ‘Ÿ is relevant to the anomaly.
A high ğ‘Ÿ1 score indicates that the majority of the descendant leaf

5

elements are within the anomalous partition. Note that the signifi-
cance of elements in the highest layer is limited by adding 1 to the
denominator.

According to the generalized ripple effect in Equation 6, the
expected value of descendant leaf elements ğ‘’ âˆˆ ğ¿ğ· (ğ‘’ğ‘Ÿ ) should be
ğ‘£ (ğ‘’ğ‘Ÿ )
ğ‘(ğ‘’) = ğ‘“ (ğ‘’)
ğ‘“ (ğ‘’ğ‘Ÿ ) . The difference between ğ‘£ (ğ‘’) and ğ‘(ğ‘’) quantifies
the degree to which ğ‘’ follows the ripple effect paradigm. Differ-
ent from GPS [7] and NPS [6], we propose a simplified score ğ‘Ÿ2
to be used in conjunction with ğ‘Ÿ1. Since ğ‘Ÿ1 already incorporates
the relative importance of ğ‘’ğ‘Ÿ , we do not need ğ‘Ÿ2 to consider leaf
elements ğ‘’ âˆ‰ ğ¿ğ· (ğ‘’ğ‘Ÿ ). Instead, we compare the deviation scores of
all leaf elements ğ‘’ âˆˆ ğ¿ğ· (ğ‘’ğ‘Ÿ ) before (ğ‘Ÿğ‘‘ ) and after (ğ‘Ÿğ‘›) adjusting for
the ripple effect:

ğ‘Ÿğ‘› =

âˆ‘ï¸

ğ‘’ âˆˆğ¿ğ· (ğ‘’ğ‘Ÿ )

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

ğ‘(ğ‘’) âˆ’ ğ‘£ (ğ‘’)
ğ‘(ğ‘’) + ğ‘£ (ğ‘’)

(cid:12)
(cid:12)
,
(cid:12)
(cid:12)

âˆ‘ï¸

ğ‘Ÿğ‘‘ =

|ğ‘‘ğ‘  (ğ‘’)|.

(10)

ğ‘’ âˆˆğ¿ğ· (ğ‘’ğ‘Ÿ )

ğ‘Ÿ2 then directly quantifies how much the average deviation score
has changed:

ğ‘Ÿ2 =

ğ‘Ÿğ‘›
ğ‘Ÿğ‘‘

.

(11)

If ğ‘Ÿ2 is close to zero then ğ‘’ğ‘Ÿ adhere to the ripple effect and is a
more likely root cause candidate, on the other hand, a large value
indicates that the relative forecast residuals of the descendant leaf
elements of ğ‘’ğ‘Ÿ has a high variance, i.e., ğ‘’ğ‘Ÿ is unlikely to belong to
the root cause set.

Finally, we combine ğ‘Ÿ1 and ğ‘Ÿ2 together into a single risk metric:

ğ‘Ÿğ‘–ğ‘ ğ‘˜ = ğ‘Ÿ1 âˆ’ ğ‘Ÿ2.

(12)

Following the above definitions, we have that 0 â‰¤ ğ‘Ÿ1 < 1 and
ğ‘Ÿ2 â‰¥ 0, hence, ğ‘Ÿğ‘–ğ‘ ğ‘˜ < 1. Note that for leaf elements, ğ‘Ÿ2 will always
return 0, so we will rely entirely on ğ‘Ÿ1 for these.

Dataset

n

d

#elements

Type

Residual

Anomaly

Maximum Max elements Mean significance

Table 2: Summary of the datasets.

A
Aâ˜…
B0
B1
B2
B3
B4
D

S
L
H

16,684
9,430
9 Â· 100
9 Â· 100
9 Â· 100
9 Â· 100
9 Â· 100
9 Â· 100

1,000
1,000
100

5
5
4
4
4
4
4
4

5
4
6

15,324
15,324
21,600
21,600
21,600
21,600
21,600
21,600

48,000
36,000
24,000,000

Fundamental
Fundamental
Fundamental
Fundamental
Fundamental
Fundamental
Fundamental
Derived

Fundamental
Fundamental
Fundamental

3.92%
3.92%
0.80%
3.19%
6.38%
9.55%
12.7%
3.99%

0.0% âˆ’ 20.2%
0.0% âˆ’ 8.02%
0.0% âˆ’ 19.9%

3
5
3
3
3
3
3
3

3
5
3

1
1
1
1
1
1
1
1

3
1
3

0.28
0.28
0.06
0.07
0.10
0.12
0.14
0.03

0.09
0.00
0.04

4.3 Element Search and Iteration
We do not make any assumption on the number of elements in the
root cause set or whether the elements are in the same cuboid or
not. Instead, we identify the single most likely element ğ‘’ğ‘Ÿ , remove
its leaf elements ğ¿ğ· (ğ‘’ğ‘Ÿ ) from Eğ‘¤, and then repeat the search for the
next candidate. The details of the element search are in Algorithm 2
and the iteration is carried out in Algorithm 3.

The element search is done from lower layer cuboids to higher
layer ones following the Occamâ€™s razor principle, allowing for an
early exit if a suitable candidate is found. For an element to be con-
sidered suitable, we require its risk score to be above a threshold ğ‘¡ğ‘Ÿ
and its explanatory power to be above a minimum threshold ğ‘¡ğ‘’ğ‘
(see Equation 4). When multiple candidates from the same layer
satisfy the conditions, we return the element with the highest ex-
planatory power. To terminate the search and determine a suitable
value for ğ‘¡ğ‘’ğ‘ , we consider the total explanatory power of the leaf
elements in the anomalous partition, see Algorithm 3. When the
identified root cause set explains a sufficiently large proportion, the
search is ended. This is determined by the parameter ğ‘¡ğ‘ğ‘’ğ‘ , where
0 < ğ‘¡ğ‘ğ‘’ğ‘ â‰¤ 1. We expect a proper root cause set to adequately
explain the disparity between the actual and forecast values, which
rationalizes our use of explanatory power, both as the termination
condition and for selecting relevant elements. In this way, we en-
sure that each identified element is significant and explains at least
ğ‘¡ğ‘ğ‘’ğ‘ of the total anomaly. Note that we only need to adequately
explain leaf elements in the anomalous partition, and anomalies
with insignificant magnitudes [7] can therefore still be identified.
Depending on the direction of the anomaly, it may be necessary
to adjust the sign of the elementsâ€™ explanatory power to ensure
that their values are positive. In Algorithm 3 (lines 3 to 5), we
consequently examine the total sum of explanatory power in the
anomalous partition; if it is negative, then we switch the sign for
all elements.

4.4 Element Pruning
An optional pruning of the elements can be done to increase the
efficiency of the algorithm and decrease the running time, without
having any impact on the returned result. See line 4 in Algorithm 2.
We identify any element ğ‘’ whose leaf elements set ğ¿ğ· (ğ‘’) has a
maximum potential explanatory power less than ğ‘¡ğ‘’ğ‘ and remove

6

these and all of their descendant elements from the search space. We
compute the maximum potential explanatory power of an element
ğ‘’ considering the optimal subset of descendant elements:

ğ‘’ğ‘+ (ğ‘’) =

âˆ‘ï¸

max{0, ğ‘’ğ‘ (ğ‘’ â€²)}.

(13)

ğ‘’â€² âˆˆğ¿ğ· (ğ‘’)

If ğ‘’ğ‘+ (ğ‘’) < ğ‘¡ğ‘’ğ‘ , then no subset of descendant elements of ğ‘’ will have
an explanatory power high enough to be selected and can safely be
pruned. The benefit of element pruning decreases at higher layers
since there are fewer descendant elements, and we can therefore
limit the element pruning to the first few layers.

5 EVALUATION
In this section we begin by introducing the experimental setup,
delineating the synthetic dataset generation, datasets, and evalu-
ation metrics. This is followed by a comparison between RiskLoc
and related work, a parameter sensitivity analysis, and an ablation
study.

5.1 Experimental Setup
Synthetic Dataset Generation. We generate synthetic data
5.1.1
that emulates real-world data and complements the available semi-
synthetic datasets [7]. To generate these synthetic data instances,
instead of generating time series, we directly construct the summed-
up values for each separate element to simplify the generation
process. As such, there is no concept of time involved and each leaf
element ğ‘’ only has a single actual value ğ‘£ (ğ‘’) and a single forecast
value ğ‘“ (ğ‘’). Actual values ğ‘£ (ğ‘’) are sampled from a one-parameter
Weibull distribution [15] with ğ›¼ âˆ¼ ğ‘ˆ [0.5, 1.0], where ğ‘ˆ is the
uniform distribution with a closed interval. This is a subexponential,
heavy-tailed distribution with a single tail, allowing the elements
to take a broad and diverse range of values, with the majority being
relatively closer to 0. For aesthetic purposes, we multiply all values
by 100. We then set the actual values of an element to 0 with a
probability ğ‘ âˆ¼ ğ‘ˆ [0, 0.25] to increase the difficulty in localizing
root causes and to mirror a characteristic commonly seen in real
world data.

For normal leaf elements (i.e., not in the root cause), we emulate
the forecast residuals by adding Gaussian noise to the actual values

with some value ğœ:

ğ‘“ (ğ‘’) = ğ‘£ (ğ‘’) Â· ğ‘ (1, ğœ)

(14)

For each generated instance, the number of anomalies and the
number of elements involved in each is randomized from pre-set
ranges. Then, each anomaly is randomly assigned a severity level
and a deviation, corresponding to its distance from the normal
samples and the variance of its leaf elements. Note that there is no
guarantee that the root causes are significant. The full details of
each generated data set can be found in Appendix A.

5.1.2 Datasets. We use the semi-synthetic datasets from Li et al.
[7] for evaluation and further introduce three entirely synthetic
datasets, S, L, and H , which are generated using the approach
described above. See Table 2 for a summary of all datasets.

A, Bğ‘– with ğ‘– âˆˆ [0, .., 4], and D are relatively simple; Each dataset
has 9 scenarios, with 100 instances for each scenario. The difference
between the scenarios is the number of elements in the root cause
and in which layer the root cause is inside, both with values between
1 and 3. D have a structure similar to BâŸ© but contain a derived
measure: a quotient of two fundamental measures with a normal
success rate following ğ‘ˆ [0.9, 1.0], see [7] for the full details. Aâ˜…
is a more challenging dataset, we divide A as obtained from Li
et al. [7] into A and Aâ˜…, where instances with root causes in layers
lower than 3 or with more than 3 elements are put into Aâ˜… and
the others in A. Note that our A corresponds to A as used for
evaluation in [7].

The root causes in the semi-synthetic datasets do not cover the
full search space, as only one element is allowed in each anomaly
(albeit with up to three simultaneous anomalies in a single instance,
but these must be from different cuboids), and all anomalies are
always in the same layer. Moreover, the maximum forecast residual
in the normal leaf elements is only around 13%. In our synthetic
datasets, we allow for higher forecast residuals, multiple root cause
elements from the same cuboid, and from different layers within
the same instance. Moreover, in contrast to Li et al. [7], the anomaly
direction is randomized (but the same for all anomalies in a single
instance). Overall, the synthetic datasets are intended to be more
complex and difficult.

The datasets S and H allow multiple elements in each anomaly
(i.e., within the same cuboid) and can have up to 9 separate ele-
ments in the root cause. On the other hand, L only has anomalies
in the highest layer where none of the root cause elements have
been aggregated in any dimension. These are typically insignificant
compared to the numerous normal elements and are thus difficult
to correctly identify. H is multiple orders of magnitudes larger
than the other datasets with 24 million leaf elements. In real-world
scenarios, dimensions and attribute combinations of this size is not
uncommon and H is created to verify the algorithmsâ€™ effectiveness
on these larger search spaces.

5.1.3 Evaluation Metrics. We assess the effectiveness of the meth-
ods using the F1-score. The F1 score is calculated at the element
level, where a correctly reported element is considered a true posi-
tive (TP), a missed element is a false negative (FN), and any addi-
tional identified elements are false positives (FP). The F1-score is
computed from the sum of all TP, FN, and FP as:

7

F1-score =

2 Â· ğ‘‡ ğ‘ƒ
2 Â· ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ + ğ¹ ğ‘

.

(15)

Another key metric is the efficiency of the algorithms. We conse-
quently evaluate the average running time of the algorithms in
different scenarios.

5.2 Experimental Results
We evaluate our proposed method against the following previous
works: Adtributor [3], R-Adtributor [11], HotSpot [13], Squeeze [7],
and AutoRoot [6]. Additional information regarding the running
environment and parameter settings can be found in Appendix B.

5.2.1 Effectiveness. The average F1-scores of each algorithm on the
relatively simple datasets are presented in Figure 4 together with
the standard deviation over each root cause setting. See Appendix C
for the full table. As can be observed in the figure, RiskLoc consis-
tently outperforms all baselines on all datasets with comparatively
small performance deviations between different root cause settings
within each dataset. We note a decline in the results for all base-
lines on the Bğ‘– datasets as the residual of the normal elements gets
higher, while RiskLoc has more stable performance. In the dataset
A, there is significant overlap between anomalies that appear to
severely affect AutoRoot at the lower layers, resulting in markedly
worse performance compared to RiskLoc and Squeeze. Notably, R-
Adtributor has worse overall results than Adtributor, despite being
able to handle deeper anomalies. This can be explained by Adtibutor
performing considerably better on first-layer anomalies, regardless
of the number of elements in the root cause.

The results on the more complex datasets are presented in Ta-
ble 3. RiskLoc achieves the best performance on all datasets with
significant improvements over the second-best algorithms. For the
most difficult semi-synthetic dataset, Aâ˜…, the relative increase in
F1-score is 57% and on the synthetic S dataset, the gain is 41%. The
results on L illustrate how Adtributor is unable to find fine-grained
root causes, while R-Adtributor and HotSpot are of limited use.
Squeeze, AutoRoot, and RiskLoc are better able to handle these fine-
grained root causes, with RiskLoc obtaining the highest F1-score
with a gain of 28% over Autoroot, the second best approach. Finally,
on the large-scale dataset H used to verify the effectiveness of the
algorithms in larger search spaces, RiskLocâ€™s gain is 40%.

5.2.2 Efficiency. The time consumption of each algorithm is pre-
sented in Figure 5. As shown in the figure, Adtributor consistently
consumes the least amount of time, while HotSpot and Squeeze are
the slowest. RiskLoc runs in less than 3 seconds on average on the
simpler datasets, while S and H takes longer. The running time
is clearly dependent on the search space size, while the root cause
complexity has a smaller significance. Although the running time
of Adtributor and R-Adtributor is significantly faster than that of
RiskLoc, the accuracy is inadequate for all but the simplest root
causes. We further note that for most algorithms, including RiskLoc,
there is a performance-speed trade-off, and in our parameter selec-
tion we have focused on the performance and the speed may thus
have been negatively impacted.

Figure 4: F1-score comparison for the simple datasets.

Table 3: F1-scores on the more complex datasets.

Algorithm

Dataset

Adtributor

R-Adtributor

HotSpot

Squeeze

AutoRoot

RiskLoc

Aâ˜…
S
L
H

0.0650 Â± 0.12
0.0589
0.0000
0.0235

0.00003 Â± 0.00
0.0066
0.0089
0.0000

0.0127 Â± 0.02
0.1740
0.1848
0.1109

0.2951 Â± 0.16
0.1283
0.3524
0.0511

0.1802 Â± 0.06
0.4478
0.5266
0.3492

0.4640 Â± 0.10
0.6350
0.6767
0.4906

Figure 5: Average running time (s) on different datasets.

Figure 6: Parameter sensitivity for risk threshold ğ‘¡ğ‘Ÿ and propor-
tional explanatory power threshold ğ‘¡ğ‘ğ‘’ğ‘ .

5.3 Parameter Sensitivity Analysis
We perform a parameter sensitivity analysis for RiskLoc. Two pa-
rameters can be tuned, the risk threshold ğ‘¡ğ‘Ÿ and the proportional
explanatory power threshold ğ‘¡ğ‘ğ‘’ğ‘ . We adjust them one by one
while keeping the other fixed at the default value (ğ‘¡ğ‘Ÿ = 0.5 and
ğ‘¡ğ‘ğ‘’ğ‘ = 0.02) using six datasets: Bğ‘–, ğ‘– âˆˆ [0, .., 4] and D. We report
the average and standard deviation of the F1-scores and running
times for each parameter setting and present the results in Figure 6.
The performance is relatively stable for risk thresholds between
0.4 and 0.6. For lower risk thresholds, elements with a descendant
element belonging to the root cause set may be returned erro-
neously. These parent elements incorporate the root cause, hence
their ğ‘Ÿ1 score may be high but they simultaneously contain leaf
elements that do not conform to the ripple effect. However, if ğ‘Ÿ1 is
sufficiently high and ğ‘Ÿ2 does not compensate enough, then a low ğ‘¡ğ‘Ÿ
may return these elements. On the other hand, root cause elements
may be missed if the risk threshold is too high. We also note that
the running time decreases with higher ğ‘¡ğ‘Ÿ as fewer elements are
considered and the search can be terminated earlier.

For the proportional explanatory power threshold ğ‘¡ğ‘ğ‘’ğ‘ , we ob-
serve that both the F1-score and running time increase with lower
values. With a lower threshold, the iterative search will terminate

8

later, allowing for more calls to Algorithm 2, thus increasing the
running time. Simultaneously, the performance will increase since
fewer root cause elements are missed. However, a low ğ‘¡ğ‘ğ‘’ğ‘ may
return extraneous elements if there are elements within the abnor-
mal partition that do not belong to the root cause set. Performance
stabilizes around 0.02, while running time begins to increase expo-
nentially for lower values.

5.4 Ablation Study
We conduct an ablation study to validate the effectiveness of the
main components of our proposed method. Specifically, we investi-
gate the performance impact of removing or altering the following
four integral components.

â€¢ No outlier removal: No outliers removed in Algroithm 1.
â€¢ No ğ‘Ÿ1: The ğ‘Ÿ1 component in the risk score is fixed to 1.
â€¢ No ğ‘Ÿ2: The ğ‘Ÿ2 component in the risk score is fixed to 0.
â€¢ No weights: The leaf element weights are removed (equiva-

lent to setting all weights to 1).

The results are reported in Table 4. For each variant, the best val-
ues for the parameters ğ‘¡ğ‘ğ‘’ğ‘ and ğ‘¡ğ‘Ÿ are identified from the search
space. We first make the wrap-up observation that all variants have

Table 4: Ablation study. The relative worst F1-score is in bold for
each variant.

Variant

RiskLoc

No outlier removal
No ğ‘Ÿ1
No ğ‘Ÿ2
No weights

Dataset

A
0.77

0.22
0.74
0.50
0.35

Bğ‘–
0.98

0.98
0.79
0.92
0.88

D
0.96

0.90
0.63
0.88
0.67

Aâ˜…
0.46

0.28
0.44
0.18
0.33

S
0.64

0.61
0.63
0.62
0.60

L
0.68

0.75
0.48
0.00
0.50

H
0.49

0.41
0.42
0.36
0.33

Table 5: Average running time (s) with different element pruning
layer restrictions.

Variant

No pruning

Layer 1
Reduction (%)

Layer 3
Reduction (%)

Dataset

A
1.70

1.20
29.4

1.52
10.6

Bğ‘–
2.27

1.78
21.6

1.94
14.5

D
2.27

1.76
22.5

2.08
8.4

Aâ˜…
4.05

2.86
29.4

3.66
9.6

S
15.0

12.0
20.0

12.5
16.7

L
3.37

2.64
21.7

3.00
11.0

H
9,308

6,838
26.5

5,959
36.0

lower or similar performance than RiskLoc on all datasets except
for where no outliers are removed on the L dataset. This dataset
has very few abnormal leaf elements; therefore, removing a small
number of outliers can confuse the direction of the anomaly. A
more complex outlier removal scheme could reduce the negative
impact or could be avoided by assuming that the anomaly direction
is given by the prior anomaly detection stage.

We further note that while each component is indispensable, the
impact is highly dependent on the dataset characteristics. RiskLoc
without outlier removal achieves markedly worse results on A and
Aâ˜… but has less import on the other datasets. On the other hand,
ğ‘Ÿ1 is a key component on the Bğ‘– and D datasets, while ğ‘Ÿ2 and leaf
element weights are essential on nearly all datasets.

Furthermore, we study the effects of the element pruning scheme.
The element pruning does not affect the F1-score but the running
time. The results of no pruning are compared to pruning restricted
to layer one and three are shown in Table 5. As can be observed,
pruning will always decrease the running time. However, for smaller
datasets, its advantageous to limit the pruning to the first layer,
while for the largest dataset H , allowing deeper element pruning
is faster. This is due to the incurred overhead cost.

6 CONCLUSION
In this paper, we propose RiskLoc, an efficient and accurate algo-
rithm to locate root causes in multi-dimensional time series without
significant root cause assumptions. Specifically, RiskLoc first divides
the leaf elements into normal and abnormal partitions and assigns
importance weights. All elements are evaluated using a risk score
which integrates the weighted distribution of descending leaf ele-
ments and the impact of adjusting for the ripple effect. A set of root
cause elements is identified by iteratively searching layer by layer
until elements with a sufficiently large risk score and explanatory
power are identified. We conduct extensive experiments on several

9

semi-synthetic and three complementary synthetic datasets to ver-
ify the effectiveness and efficiency of our approach. We demonstrate
that RiskLoc consistently outperforms state-of-the-art baselines,
especially in challenging root cause scenarios. In particular, on the
most difficult semi-synthetic dataset, we achieve a relative increase
in F1-score of 57% compared to the second-best approach, while
gains of up to 41% are obtained on the synthetic datasets. For future
work, we will focus on enhancing the model robustness in scenarios
with low quality or missing forecast values, and further explore
new partitioning schemes and innovative pruning strategies.

REFERENCES
[1] Faraz Ahmed, Jeffrey Erman, Zihui Ge, Alex X. Liu, Jia Wang, and He Yan. 2017.
Detecting and Localizing End-to-End Performance Degradation for Cellular Data
Services Based on TCP Loss Ratio and Round Trip Time. IEEE/ACM Transac-
tions on Networking 25, 6 (2017), 3709â€“3722. https://doi.org/10.1109/TNET.2017.
2761758

[2] Chetan Bansal, Sundararajan Renganathan, Ashima Asudani, Olivier Midy, and
Mathru Janakiraman. 2020. Decaf: Diagnosing and triaging performance issues
in large-scale cloud services. In Proceedings of the ACM/IEEE 42nd International
Conference on Software Engineering: Software Engineering in Practice. 201â€“210.
[3] Ranjita Bhagwan, Rahul Kumar, Ramachandran Ramjee, George Varghese, Sur-
jyakanta Mohapatra, Hemanth Manoharan, and Piyush Shah. 2014. Adtrib-
utor: Revenue Debugging in Advertising Systems. In Proceedings of the 11th
USENIX Conference on Networked Systems Design and Implementation (Seattle,
WA) (NSDIâ€™14). USENIX Association, USA, 43â€“55.

[4] Jiazhen Gu, Chuan Luo, Si Qin, Bo Qiao, Qingwei Lin, Hongyu Zhang, Ze Li,
Yingnong Dang, Shaowei Cai, Wei Wu, et al. 2020. Efficient incident identification
from multi-dimensional issue reports via meta-heuristic search. In Proceedings
of the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 292â€“303.

[5] Jiawei Han, Micheline Kamber, and Jian Pei. 2011. Data mining: concepts and
techniques third edition. In The Morgan Kaufmann Series in Data Management
Systems. 83â€“124.

[6] Pengkun Jing, Yanni Han, Jiyan Sun, Tao Lin, and Yanjie Hu. 2021. AutoRoot: A
Novel Fault Localization Schema of Multi-dimensional Root Causes. In 2021 IEEE
Wireless Communications and Networking Conference (WCNC). IEEE, 1â€“7.
[7] Zeyan Li, Chengyang Luo, Yiwei Zhao, Yongqian Sun, Kaixin Sui, Xiping Wang,
Dapeng Liu, Xing Jin, Qi Wang, and Dan Pei. 2019. Generic and Robust Lo-
calization of Multi-Dimensional Root Causes. In 2019 IEEE 30th International
Symposium on Software Reliability Engineering (ISSRE). IEEE, 47â€“57.

[8] Qingwei Lin, Jian-Guang Lou, Hongyu Zhang, and Dongmei Zhang. 2016. IDice:
Problem Identification for Emerging Issues. In Proceedings of the 38th International
Conference on Software Engineering (Austin, Texas) (ICSE â€™16). Association for
Computing Machinery, New York, NY, USA, 214â€“224. https://doi.org/10.1145/
2884781.2884795

[9] Michael R Lyu et al. 1996. Handbook of software reliability engineering. Vol. 222.

IEEE computer society press CA.

[10] Weibin Meng, Ying Liu, Yichen Zhu, Shenglin Zhang, Dan Pei, Yuqing Liu, Yihao
Chen, Ruizhi Zhang, Shimin Tao, Pei Sun, et al. 2019. LogAnomaly: Unsupervised
Detection of Sequential and Quantitative Anomalies in Unstructured Logs. In
IJCAI, Vol. 19. 4739â€“4745.

[11] Moa Persson and Linnea Rudenius. 2018. Anomaly detection and fault localization
an automated process for advertising systems. Masterâ€™s thesis. Chalmers University
of Technology.

[12] Brandon Pincombe. 2005. Anomaly detection in time series of graphs using arma

processes. Asor Bulletin 24, 4 (2005), 2.

[13] Y. Sun, Y. Zhao, Y. Su, D. Liu, X. Nie, Y. Meng, S. Cheng, D. Pei, S. Zhang, X.
Qu, and X. Guo. 2018. HotSpot: Anomaly Localization for Additive KPIs With
Multi-Dimensional Attributes. IEEE Access 6 (2018), 10909â€“10923. https://doi.
org/10.1109/ACCESS.2018.2804764

[14] Sean J Taylor and Benjamin Letham. 2018. Forecasting at scale. The American

Statistician 72, 1 (2018), 37â€“45.

[15] W. Weibull. 1951. A Statistical Distribution Function of Wide Applicability.

Journal of Applied Mechanics 18 (1951), 293â€“297.

[16] Shenglin Zhang, Ying Liu, Dan Pei, Yu Chen, Xianping Qu, Shimin Tao, and Zhi
Zang. 2015. Rapid and robust impact assessment of software changes in large
internet-based services. In Proceedings of the 11th ACM Conference on Emerging
Networking Experiments and Technologies. 1â€“13.

[17] Shenglin Zhang, Ying Liu, Dan Pei, Yu Chen, Xianping Qu, Shimin Tao, Zhi
Zang, Xiaowei Jing, and Mei Feng. 2016. Funnel: Assessing software changes in
web-based services. IEEE Transactions on Services Computing 11, 1 (2016), 34â€“48.

A SYNTHETIC DATASET DETAILS
As introduced in Section 5.1, we generate three synthetic datasets
S, L, and H to use in our method evaluation. Dataset S has 1,000
instances, each with 5 attributes with value sets of size 10, 12, 10, 8,
and 5, respectively, for a total of 48,000 leaf elements. For dataset
L, we generate 1000 instance with 4 dimensions of size 10, 24, 10,
and 15, i.e., a total of 36,000 leaf elements. Finally, H contains 100
instances, each with 6 dimensions of size 10, 5, 250, 20, 8, and 12,
for a total of 24,000,000 leaf elements.

For the forecast residuals in Equation 14, we use ğœ âˆ¼ ğ‘ˆ [0, 0.25]
for S and H , and ğœ âˆ¼ ğ‘ˆ [0, 0.10] for L. The actual and forecast
values are then randomly swapped for each element with a 50%
probability to ensure that the noise is equally distributed.

We introduce synthetic anomalies to the data where the number
of anomalies for each instance is randomly drawn from a range.
For S and H , we use [1, 3] while L use [1, 5], i.e., L can have
up to 5 anomalies in a single instance. The number of elements
in each anomaly is likewise uniformly random between [1, 3] for
S and H , while L is fixed to 1 since the anomalies in L are only
placed in the highest layer. We make sure that there are no over-
lapping anomalies, e.g., if one anomaly is in element (âˆ—, âˆ—, ğ‘5, âˆ—)
then an anomaly (âˆ—, âˆ—, ğ‘5, ğ‘‘3) is not allowed. We also ensure that
each cuboid contains only a single anomaly (although it can involve
multiple elements).

Each anomaly has a severity ğ‘  and a deviation ğ‘‘ corresponding
to its distance from the normal samples and the variance of its leaf
elements. We set ğ‘  âˆ¼ ğ‘ˆ [0.25, 1.0] and ğ‘‘ âˆ¼ ğ‘ˆ [0.0, 0.1] for S and
H , and ğ‘  âˆ¼ ğ‘ˆ [0.5, 1.0] and ğ‘‘ = 0.0 for L. L does not require any
deviation since each anomaly is a single leaf element. We allow
for the anomaly direction to change (i.e., if (cid:205) ğ‘“ (ğ‘’) > (cid:205) ğ‘£ (ğ‘’) or
vice versa) but for a single generated instance, the direction of the
anomalies are the same. All elements in a single anomaly are scaled
the same (i.e., following the ripple effect [13]) with:

ğ‘¥ = max(ğ‘¥ Â· (1 âˆ’ ğ‘ (ğ‘ , ğ‘‘)), 0)

(16)

where ğ‘¥ = ğ‘£ (ğ‘’) if (cid:205) ğ‘£ (ğ‘’) > (cid:205) ğ‘“ (ğ‘’), otherwise ğ‘¥ = ğ‘“ (ğ‘’).

B EXPERIMENTAL DETAILS
We run all experiments on a Linux server with 24 x Intel(R) Xeon(R)
CPU E5-2620 v3 @ 2.40GHz and 128G RAM memory. All algorithms
are implemented in Python and rely heavily upon the Pandas and
NumPy libraries. The experiments were all carried out under the
same conditions and parallelization is done over the instances (files)
themselves. Notably, by adjusting the code to run the cuboids (or
clusters for Squeeze and AutoRoot) in parallel, a significant reduc-
tion in running time could be obtained.

For each baseline algorithm, we try a set of values for each
tunable parameter and select the ones that achieve the highest
average F1-score on the Bğ‘–, ğ‘– âˆˆ [0, .., 4] datasets. We define ğ‘†1 =
{ğ‘–/10, ğ‘– âˆˆ [1, .., 9]} and ğ‘†2 = ğ‘†1 âˆª {0.01, 0.05, 0.15}.
Adtributor: We run the algorithm with ğ‘‡ğ¸ğ‘ƒ âˆˆ ğ‘†1 and ğ‘‡ğ¸ğ¸ğ‘ƒ âˆˆ ğ‘†2
with the condition ğ‘‡ğ¸ğ‘ƒ â‰¥ ğ‘‡ğ¸ğ¸ğ‘ƒ . The optimal parameters were de-
termined to be ğ‘‡ğ¸ğ‘ƒ = 0.1 and ğ‘‡ğ¸ğ¸ğ‘ƒ = 0.1 which are used in all
experiments. ğ‘˜ is set to 3 for all datasets since it is the highest
number of anomalies that can be found. We tried with ğ‘˜ = 5 for

Aâ˜… and S but did not get better results.

R-Adtributor: Similar to the standard Adtributor algorithm, we
use ğ‘‡ğ¸ğ¸ğ‘ƒ âˆˆ ğ‘†2 and select the best ğ‘‡ğ¸ğ¸ğ‘ƒ value. The value of ğ‘‡ğ¸ğ¸ğ‘ƒ is
set to 0.2 and ğ‘˜ is set to 3.

HotSpot: For HotSpot, we try using both the original score, ğ‘ğ‘  [13],
and the extended ğºğ‘ƒğ‘† [7] with the threshold ğ‘ƒğ‘‡ âˆˆ ğ‘†1. The best
result is achieved by using ğºğ‘ƒğ‘† with ğ‘ƒğ‘‡ = 0.8. We further set
ğ‘€ = 200 which is a trade-off between running time and perfor-
mance. In the original article, ğ‘€ is set to values between 5 and 15.
Here, we allow for larger values, which may give higher F1-scores
at the cost of longer running times. A higher value for ğ‘€ may give
better results, however, HotSpot with ğ‘€ = 200 is already requires
more than twice the running time as compared to the second slow-
est algorithm on most datasets.

Squeeze: The code for Squeeze has been open sourced.2 We use
their implementation with default values for all experiments with
the exception of setting the maximum returned element per cluster
for H to 3 (the optimal value) to improve the running speed. Note
that the code was revised following the "known issues" as indicated
in the README.md file to enhance the performance and uses the
amended formula for constant C3 as follows:

ğ‘”ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ =

ğ‘”ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ =

log(ğ‘›ğ‘¢ğ‘š_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ + 1)
ğ‘›ğ‘¢ğ‘š_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ
ğ‘›ğ‘¢ğ‘š_ğ‘ğ‘¡ğ‘¡ğ‘Ÿ
log(ğ‘›ğ‘¢ğ‘šğ‘ğ‘¡ğ‘¡ğ‘Ÿ + 1)

ğ‘”ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ = âˆ’ log(coverage of abnormal leaves)
ğ¶ = ğ‘”ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ Â· ğ‘”ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ Â· ğ‘”ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’

(17)

AutoRoot: AutoRoot has a single parameter ğ›¿. We tried a set of
values {0.5, 0.3, 0.25, 0.2, 0.1, 0.05, 0.01} and found that 0.25 per-
forms the best on the Bğ‘– datasets. However, we noticed that the
performance on the more challenging datasets is markedly worse
than for a lower ğ›¿. To get a fairer baseline we therefore set ğ›¿ = 0.1
on A, Aâ˜… and the synthetic datasets.

RiskLoc: We run RiskLoc with ğ‘¡ğ‘Ÿ = 0.5 and ğ‘¡ğ‘ğ‘’ğ‘ = 0.02. The details
of the parameter sensitivity are in Section 5.3. The reported results
for D is using the original calculation for explanatory power and
not the extended formula for derived measures [3], increasing the
F1-score by around 7% on average. In the running-time experiments,
we have restricted element pruning to the first layer for all datasets
except H which use up to the third layer.

C ADDITIONAL EXPERIMENTAL RESULTS
We present the full results of the algorithms on all root cause sce-
narios for the datasets A, Bğ‘– with ğ‘– âˆˆ [0, .., 4], and D in Table 6 and
dataset Aâ˜… in Table 7. In both tables, layer denotes the layer with
the ground-truth root causes while #elements signify the number
of elements within the root causes.

2https://github.com/NetManAIOps/Squeeze
3https://github.com/NetManAIOps/Squeeze/issues/6

10

Dataset Algorithm

(1,1)

(1,2)

(1,3)

Root cause (layer, #elements)
(2,2)

(2,3)

(2,1)

Table 6: F1-score comparison.

A

B0

B1

B2

B3

B4

D

Adtributor
R-Adtributor
HotSpot
Squeeze
AutoRoot
RiskLoc

Adtributor
R-Adtributor
HotSpot
Squeeze
AutoRoot
RiskLoc

Adtributor
R-Adtributor
HotSpot
Squeeze
AutoRoot
RiskLoc

Adtributor
R-Adtributor
HotSpot
Squeeze
AutoRoot
RiskLoc

Adtributor
R-Adtributor
HotSpot
Squeeze
AutoRoot
RiskLoc

Adtributor
R-Adtributor
HotSpot
Squeeze
AutoRoot
RiskLoc

Adtributor
R-Adtributor
HotSpot
Squeeze
AutoRoot
RiskLoc

0.5005
0.0404
0.2765
0.8882
0.2654
0.9997

0.6494
0.4930
0.9600
0.8811
1.0000
0.9600

0.6557
0.6019
0.9600
0.9333
0.9852
0.9950

0.6689
0.6207
0.9900
0.8558
1.0000
1.0000

0.6579
0.6634
1.0000
0.8952
1.0000
1.0000

0.6623
0.7488
0.9900
0.8531
0.9950
1.0000

0.5556
0.3247
0.9000
0.9500
0.6929
0.9500

0.4059
0.0052
0.1537
0.7719
0.1728
0.8097

0.5522
0.2213
0.6267
0.9448
0.9772
0.9700

0.6212
0.2458
0.6200
0.9610
0.9875
0.9850

0.6340
0.2318
0.6600
0.9746
0.9872
0.9899

0.6611
0.3361
0.6622
0.9347
0.9819
0.9924

0.6954
0.3730
0.6312
0.8732
0.9561
0.9826

0.5835
0.2824
0.6667
0.9850
0.7906
0.9825

0.3445
0.0015
0.0920
0.6476
0.1393
0.6838

0.4682
0.0647
0.5025
0.9640
0.9130
0.9775

0.4887
0.0712
0.4840
0.9866
0.9364
0.9898

0.4615
0.1100
0.4350
0.9420
0.9664
0.9774

0.4988
0.1852
0.4650
0.8897
0.9131
0.9686

0.5263
0.1143
0.3800
0.8430
0.8963
0.9607

0.4979
0.1262
0.5215
0.9760
0.7871
0.9666

0.0000
0.0086
0.0415
0.8102
0.4131
0.9023

0.0000
0.1739
0.8600
0.9565
0.9900
0.9286

0.0000
0.1778
0.8800
0.8732
0.9901
1.0000

0.0000
0.1654
0.8800
0.8889
0.9899
0.9612

0.0000
0.1139
0.9000
0.8349
0.9381
1.0000

0.0000
0.0735
0.9100
0.8224
0.9082
1.0000

0.0000
0.0194
0.8800
0.9500
0.6071
0.9548

0.0000
0.0042
0.0251
0.6470
0.2451
0.7771

0.0000
0.1382
0.5533
0.9875
0.9561
0.9723

0.0000
0.1538
0.5867
0.9455
0.9745
0.9925

0.0000
0.1511
0.6107
0.8244
0.8732
0.9924

0.0000
0.1871
0.5800
0.8173
0.8981
0.9975

0.0000
0.1135
0.6020
0.7196
0.8490
0.9975

0.0000
0.0672
0.6533
0.9500
0.7621
0.9698

0.0000
0.0010
0.0146
0.5275
0.2035
0.6701

0.0000
0.0824
0.4561
0.9352
0.8588
0.9592

0.0000
0.0712
0.4100
0.9231
0.9117
0.9651

0.0000
0.1081
0.4110
0.8174
0.8024
0.9882

0.0000
0.1101
0.4300
0.7692
0.7854
0.9883

0.0000
0.1073
0.4450
0.7097
0.7582
0.9698

0.0000
0.1114
0.5013
0.9449
0.7721
0.9454

Table 7: F1-score comparison on Aâ˜….

(3,1)

(3,2)

(3,3)

0.0000
0.0000
0.0016
0.4992
0.5136
0.7948

0.0000
0.0401
0.5100
0.9327
0.9900
0.9744

0.0000
0.0346
0.4000
0.8815
0.9515
0.9899

0.0000
0.0118
0.4000
0.8835
0.9255
0.9848

0.0000
0.0227
0.3200
0.7798
0.9101
0.9848

0.0000
0.0220
0.2400
0.7207
0.7708
0.9744

0.0000
0.0021
0.9000
0.9254
0.6873
0.9543

0.0000
0.0000
0.0004
0.4089
0.3610
0.6714

0.0000
0.0684
0.4667
0.9474
0.9344
0.9556

0.0000
0.0288
0.4000
0.9389
0.9330
0.9590

0.0000
0.0418
0.3467
0.8655
0.8433
0.9848

0.0000
0.0479
0.3267
0.7910
0.8840
0.9692

0.0000
0.0377
0.2533
0.7538
0.7608
0.9558

0.0000
0.0080
0.6667
0.8750
0.7723
0.9622

0.0000
0.0000
0.0000
0.3592
0.2452
0.6083

0.0000
0.0595
0.4000
0.9538
0.8079
0.9437

0.0000
0.0511
0.3300
0.9325
0.7992
0.9640

0.0000
0.0553
0.3300
0.8571
0.7230
0.9726

0.0000
0.0337
0.2957
0.7701
0.8311
0.9565

0.0000
0.0450
0.2807
0.7206
0.7107
0.9383

0.0000
0.0079
0.5000
0.8848
0.7129
0.9370

Algorithm

Adtributor
R-Adtributor
HotSpot
Squeeze
AutoRoot
RiskLoc

(1,4)

(1,5)

(2,4)

0.3062
0.0000
0.0618
0.5463
0.1284
0.5644

0.2790
0.0000
0.0384
0.4686
0.1753
0.4788

0.0000
0.0003
0.0097
0.4219
0.1820
0.5710

Root cause (layer, #elements)
(3,4)

(3,5)

(2,5)

(4,1)

(4,2)

(4,3)

0.0000
0.0000
0.0033
0.3566
0.2379
0.4487

0.0000
0.0000
0.0008
0.2213
0.2515
0.5179

0.0000
0.0000
0.0000
0.2158
0.2549
0.4754

0.0000
0.0000
0.0000
0.2286
0.0699
0.5188

0.0000
0.0000
0.0000
0.1278
0.1636
0.3472

0.0000
0.0000
0.0000
0.0693
0.1586
0.2632

11

