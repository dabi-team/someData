2
2
0
2

r
p
A
9
1

]

G
L
.
s
c
[

1
v
8
8
9
8
0
.
4
0
2
2
:
v
i
X
r
a

CPU- and GPU-based Distributed
Sampling in Dirichlet Process Mixtures for
Large-scale Analysis

Or Dinari*1
dinari@post.bgu.ac.il

Raz Zamir*1
razzam@post.bgu.ac.il

John W. Fisher III2
ﬁsher@csail.mit.edu

Oren Freifeld1
orenfr@cs.bgu.ac.il

1Computer Science, Ben-Gurion University, Beer-Sheva, Israel
2MIT CSAIL, Cambridge MA, USA

Abstract

In the realm of unsupervised learning, Bayesian nonparametric
mixture models, exempliﬁed by the Dirichlet Process Mixture Model
(DPMM), provide a principled approach for adapting the complex-
ity of the model to the data. Such models are particularly useful
in clustering tasks where the number of clusters is unknown. De-
spite their potential and mathematical elegance, however, DPMMs
have yet to become a mainstream tool widely adopted by practition-
ers. This is arguably due to a misconception that these models scale
poorly as well as the lack of high-performance (and user-friendly)
software tools that can handle large datasets efﬁciently. In this paper
we bridge this practical gap by proposing a new, easy-to-use, statisti-
cal software package for scalable DPMM inference. More concretely,
we provide efﬁcient and easily-modiﬁable implementations for high-
performance distributed sampling-based inference in DPMMs where
the user is free to choose between either a multiple-machine, multiple-
core, CPU implementation (written in Julia) and a multiple-stream
GPU implementation (written in CUDA/C++). Both the CPU and GPU
implementations come with a common (and optional) python wrap-
per, providing the user with a single point of entry with the same
interface. On the algorithmic side, our implementations leverage a

1

 
 
 
 
 
 
leading DPMM sampler from [1]. While Chang and Fisher III’s imple-
mentation (written in MATLAB/C++) used only CPU and was designed
for a single multi-core machine, the packages we proposed here dis-
tribute the computations efﬁciently across either multiple multi-core
machines or across mutiple GPU streams. This leads to speedups,
alleviates memory and storage limitations, and lets us ﬁt DPMMs to
signiﬁcantly larger datasets and of higher dimensionality than was
possible previously by either [1] or other DPMM methods. Our open-
source code (GPLv2 licensed) is publicly available on github.com.

1

Introduction

In unsupervised learning, Bayesian Nonparametric (BNP) mixture mod-
els, exempliﬁed by the Dirichlet-Process Mixture Model (DPMM), provide
a principled approach for Bayesian modeling while adapting the model
complexity to the data. This contrasts with ﬁnite mixture models whose
complexity is determined manually or via model-selection methods. To ﬁx
ideas, an important DPMM example is the Dirichlet-Process Gaussian Mix-
ture Model (DPGMM), a Bayesian ∞-dimensional extension of the classical
Gaussian Mixture Model (GMM). Despite their potential, however, and al-
though researchers have used them successfully in numerous applications
during the last two decades, DPMMs still do not enjoy wide popularity
among practitioners, largely due to computational bottlenecks that exist
in current algorithms and/or implementations. In particular, one of the
missing pieces is the availability of software tools that: 1) can efﬁciently
handle DPMM inference in large datasets; 2) are user-friendly and can also
be easily modiﬁed.

We argue that in order for DPMMs to become a practical choice for large-scale
data analysis, implementations of DPMM inference must leverage parallel- and
distributed-computing resources (in an analogy, consider how advances in
GPU computing and GPU software contributed to the success of deep
learning). This is because of not only potential speedups but also memory
and storage considerations. For example, this is especially true in dis-
tributed mobile robotic sensing applications where multiple autonomous
agents working together have limited computational and communication
resources. As another motivating example, consider unsupervised data-
analysis tasks in large and high-dimensional computer-vision datasets.

In other words, while DPMMs are theoretically ideal for handling unlabeled
datasets, current implementations of DPMM inference do not scale well

2

Package
CUDA/C++ GPU

Processor

Julia

CPU

Description
The fastest package for high N (number of
data points) and d (data dimensions) on
a single machine; supports multiple GPU
streams.
Supports both multiple cores and multiple
machines.

Python

Either CPU or GPU Wrapper to the CUDA/C++ and Julia pack-

ages

Table 1: Overview of the proposed packages

with the size of the dataset and/or the dimensionality.

This is partly since most existing implementations are serial and they do
not harness the power of distributed computing. This does not mean that
there do not exist distributed algorithms for DPMM inference. There is,
however, a large practical gap between designing such an algorithm and having
it implemented efﬁciently in a way that fully utilizes the available computing
resources. Thus, the very few publicly-available implementations of such
distributed algorithms are fairly limited in their capabilities (as well as their
expressiveness; e.g., some support only isotropic Gaussians [2], etc.). Our
work closes this gap by providing effective and scalable statistical software
for typical large-scale inference.

Concretely, we propose two solutions from which the users can choose
according to their needs, constraints, and available computing resources.
The ﬁrst proposed solution, implemented purely in CPU, is based on dis-
tributing computations across multiple cores as well as multiple machines.
The second proposed solution, based mostly on GPU, relies on distributing
computations across multiple GPU streams in a single machine. See Table 1
for more details.

More generally (than the topic of DPMM inference), distributed imple-
mentations, at least in traditional programming languages used for such
implementations, tend to be hard to debug, read, and modify. This clashes
with the usual workﬂow of algorithm development. As a remedy, in our
recent workshop paper [3], which constitutes a preliminary and partial
version of this paper, we proposed a Julia implementation for distributed
sampling-based DPMM inference. Since the publication of [3] we have
improved the performance of our Julia implementation, have added its
GPU counterpart in CUDA/C++, and added an optional Python wrapper for

3

both the CPU and GPU implementations.

To summarize, in this paper we explain how to use, via either Julia or
CUDA/C++, an efﬁcient distributed DPMM inference for large-scale analy-
sis. Particularly, based on a leading parallel Markov Chain Monte Carlo
(MCMC) inference algorithm [1] (to be discussed in section 2) – originally
implemented in C++ for a single multi-core CPU single machine in a highly-
specialized fashion using a shared-memory model – we provide novel,
more scalable, and easier-to-use-or-modify implementations that leverage
either the latest Nvidia’s asynch memory allocation API for GPU or Julia’s
capabilities for distributing CPU computations efﬁciently across multiple
multi-core machines using a distributed memory model. This leads to
speedups, alleviates memory and storage limitations, and lets us infer
DPMMs from signiﬁcantly larger and higher-dimensional datasets than
was previously possible by either [1] or other DPMM inference methods.
Our Julia and CUDA/C++ implementations are also accompanied by an
optional Python wrapper which hides the Julia & CUDA/C++ code from the
user and provides a single point of entry that lets the user employ, in the
same settings and with the same code, either one of our CPU and GPU
packages.

2 Models and the Inference Algorithm

2.1 Preliminaries: Finite Mixture Models and Clustering

Let d and K be two positive integers and let x be a generic point in Rd. A
K-component Finite Mixture Model (FMM), also known as a parametric
mixture model, is a probabilistic model whose associated d-dimensional
probability density function (pdf) is

p(x; θ) = ∑K

k=1 πk f (x; θk)

(1)

k=1, (πk)K

where θ = (πk, θk)K
1 and πk ≥ 0 ∀k), and f (x; θk) is a d-dimensional pdf parameterized by θk.
The ( f (·; θk))K
k=1
are called the mixture proportions (or weights).

k=1 functions are called the mixture components while (πk)K

k=1 form a convex combination (namely, ∑K

k=1 πk =

Example 1 In the (ﬁnite) Gaussian Mixture Model (GMM), θk = (µk, Σ
k) and
f (x; θk) = N (x; µk, Σ
k) where the latter is a multivariate Gaussian pdf evaluated
at x and parameterized by a mean vector, µk ∈ Rd, and a Symmetric Positive-

4

Deﬁnite (SPD) d × d covariance matrix, Σ

k. In other words, in this example

f (x; θk) = N (x; µk, Σ

k) (cid:44) (2π)−d/2(det Σ

k)−1/2 exp(− 1

2 (x − µk)TΣ−1

k (x − µk)) .

(2)

The case where x takes values in a discrete set is similar, except that each
component is a probability mass function (pmf), not a pdf. A popular exam-
ple is a mixture of categorical distributions or, more generally, multinomial
distributions.
Let X = (xi)N
i=1 stand for N data points and again let K > 0 be an integer.
Clustering is the task of partitioning X into K parts, called clusters and de-
noted by C = (Ck)K
k=1. Let zi denote the latent point-to-cluster assignment
of xi; i.e., cluster k is deﬁned as Ck = (xi)i:zi=k. In the standard formulation
of FMM-based clustering, the data points are assumed to be independent
and identically-distributed (i.i.d.) draws from an FMM and one typically
tries to maximize the corresponding likelihood function, p(X|θ), over θ.
The most popular approach for (locally-) maximizing that likelihood is
via an Expectation-Maximization (EM) algorithm [4]. As the time-honored
Bayesian formulation helps avoiding problems such as over-ﬁtting and
enables encoding prior knowledge, the FMM also has a Bayesian (but
still ﬁnite) variant, where θ is assumed to be random and drawn from a
suitable prior [5] (speciﬁcally, the prior over (πk)K
k=1 is usually a Dirichlet
distribution, not to be confused with a Dirichlet process). In which case, one
targets the posterior distribution, p(θ|X), rather than the likelihood; e.g.,
one can try to maximize p(θ|X) over θ, sample θ from it, compute posterior
expectations, etc.

2.2 The Dirichlet Process Mixture Model (DPMM)

Below we provide a brief and informal introduction to the DPMM, focusing
only on the essentials required for understanding this manuscript and make
it self-contained as possible. For a comprehensive and formal mathematical
treatment, see [6]. For an applied data-analysis perspective, see [7]. For a
gentle introduction with machine-learning and/or computer-vision readers
in mind, see the theses by [8] or [9].

The DPMM is a BNP extension of the FMM [10]. Loosely speaking, a DPMM
entertains the notion of a mixture of inﬁnitely-many components.
The weights, π = (πk)∞
k=1, are drawn from a Grifﬁths-Engen-McCloskey
(GEM) stick-breaking process with a concentration parameter α > 0 [11],

5

while the components, (θk)∞
FMM case.

k=1, are drawn from a prior as in the Bayesian

Example 2 The Dirichlet Process GMM (DPGMM) is a GMM with inﬁnitely-
many Gaussians.

Like the FMM, the DPMM is often used for clustering. With a DPMM,
however, the number of clusters, K, is not assumed to be known; rather, it is
viewed as a latent random variable whose value is inferred with the rest of
the model.

While in an FMM the number of clusters and the number of components
are typically equal, and are thus both denoted by a single symbol, K, with
a DPMM the situation is different: although there are inﬁnitely-many
components, the (latent and random) number of clusters, K, is ﬁnite (par-
ticularly, K ≤ N). The inferred value of K depends on α (the higher α is,
the more clusters are expected), on the complexity of the data, and the
(hyper-parameters of the) prior over the components (that said, when N is
large, the last two factors are usually more inﬂuential than α).

Example 3 Recall that the standard prior for Gaussian components is a Normal
Inverse-Wishart (NIW) distribution [5]. If the NIW’s hyper-parameters are set to
strongly favor small covariances (hence small clusters), then this will implicitly
favor a large K. Likewise, if the NIW prior strongly favors larger covariances
(hence large clusters), then K will tend to be small. In the lack of prior knowledge,
however, the NIW prior can be set to be very weak (i.e., high uncertainty), letting
the data speak for itself.

For simplicity, our text below implicitly assumes that all the random vectors
involved have either a pdf or a pmf. One known mathematical construction
of the DPMM uses the following distributions:

π|α ∼ GEM(π; α),
i.i.d.∼ fθ(θk; λ),
θk|λ
zi|π i.i.d.∼ Cat(zi; π)
xi|zi, θzi ∼ fx(xi; θzi ),

∀k ∈ {1, 2, . . .},

∀i ∈ {1, 2, . . . , N},

∀i ∈ {1, 2, . . . , N} .

(3)

(4)

(5)
(6)

Here, fθ(·; λ) is the pdf or pmf (associated with some base measure) param-
eterized by λ, the inﬁnite-length vector π = (πk)∞
k=1 is drawn from a GEM
stick-breaking process with a concentration parameter α > 0 (particularly,
πk > 0 for every k and ∑∞
k=1 πk = 1) while θk is drawn from fθ. Each of the

6

i=1 is generated by ﬁrst drawing a label, zi ∈ Z+,
N i.i.d. observations (xi)N
from π (i.e., Cat is the categorical distribution), and then xi is drawn from
(a pdf or a pmf) fx parameterized by θzi. Informally,

xi

i.i.d.∼ ∑∞

k=1 πk fx(xi; θk) .

(7)

Here too, each fx(·; θk) is called a component and we make no distinction
between a component, fx(·, θk), and its parameter, θk. The so-called labels
(zi)N
i=1 encode the observation-to-component assignments. A cluster is
a collection of points sharing a label; i.e., xi is in cluster k, denoted by
Ck, if and only if zi = k. Let (the random variable) K be the number of
unique labels: K = |{k : zi = k for some i ∈ {1, . . . , N}}|; i.e., K is also the
number of clusters and is bounded above by N. Typically, and as assumed
in this manuscript, fθ is chosen to be a conjugate prior [5] to fx. The latent
variables here are K, (θk)∞
k=1. For more details (and other
constructions), see [8].

k=1, π, and (zi)N

Example 4 In the case of Gaussian components, where fθ(·; λ) is an NIW pdf,
we have

θk
(cid:122) (cid:125)(cid:124) (cid:123)
µk, Σ
k;

fθ(

λ
(cid:125)(cid:124)

(cid:122)
κ, m, ν, Ψ) = NIW(µ, Σ; κ, m, ν, Ψ) (cid:44) N (µ; m, 1
κ

(cid:123)

Σ)W −1(Σ; ν, Ψ)
(8)

where W −1(Σ; ν, Ψ) is the Inverse-Wishart distribution (over d × d SPD matri-
ces), the hyperparamers are

λ = (m, Ψ, κ, ν)

(9)

where m ∈ Rd, the d × d matrix Ψ is SPD, and the two real numbers κ and ν
satisfy κ > 0 and ν > d − 1 (do not confuse κ (“kappa”) with k, the index of the
component).

2.3 Inference via Chang and Fisher III’s DPMM Sampler

We now brieﬂy review a DPMM sampler proposed by [1]. That sampler
consists of a restricted Gibbs sampler [12] and a split/merge framework [13]
which together form an ergodic Markov chain. The operations in each step
of that sampler are highly parallelizable. Importantly, the splits and merges
let the sampler make large moves along the (posterior) probability surface
as in such operations multiple labels change their label simultaneously to
the same different label. This is unlike what happens, e.g., in methods that

7

k=1, π, and (zi)N

must change each label separately from the others. We now describe the
essential details.
The augmented space. The latent variables, (θk)∞
k=1, are
augmented with auxiliary variables. For each component θk two subcompo-
nents (conceptually thought of as l=“left” and r=“right) are added, ¯θk,l, ¯θk,r,
with subcomponent weights ¯πk = ( ¯πk,l, ¯πk,r). Implicitly, this means that
every cluster Ck is augmented with two subclusters, ¯Ck,l and ¯Ck,r. For each
cluster label zi, an additional subcluster label, ¯zi ∈ {l, r}, is added; i.e., sub-
cluster ¯Ck,l ⊂ Ck consists of all the points in Ck whose subcluster label is l
(the other subcluster, ¯Ck,r, is deﬁned similarly).
The restricted Gibbs sampler. This restricted sampler is not allowed to
change (the current estimate of) K; rather, it can change only the parameters
of the existing clusters and subclusters, and when sampling the labels, it
can assign an observation only to an existing cluster. For each instantiated
component k, changing

θk, ¯θk,l, and ¯θk,r

is done using

p(θk|Ck; λ), p( ¯θk,l| ¯Ck,l; λ), and p( ¯θk,r| ¯Ck,r; λ) ,

(10)

(11)

respectively, where the latter three are the conditional distributions of the
cluster or subcluster parameters given the cluster or subclusters (and the
prior hyperparamers, λ) . In section 4, we will dive deeper into the details
of the restricted Gibbs sampler.

The split/merge framework. Splits and merges allow the sampler to
change K using the Metropolis-Hastings framework [14]. Particularly,
the auxiliary variables are used to propose splitting an existing cluster or
merging two exiting ones. When a split is accepted, each of the newly-born
clusters is augmented with two new subclusters. The Hastings ratio of a
split is [1]:

Hsplit =

αΓ(Nk,l) fx( ¯Ck,l; λ)Γ(Nk,r) fx( ¯Ck,2; λ)
Γ(Nk) fx(Ck; λ)

(12)

where Γ is the Gamma function, Nk, Nk,l and Nk,r are the numbers of points
in Ck, ¯Ck,l and ¯Ck,r, respectively, while

fx(Ck; λ), fx( ¯Ck,l; λ), and fx( ¯Ck,r; λ)

(13)

8

(a) Unlabeled data

(b) DPMMSubClusters Detection

Figure 1: 20 clusters detected by DPMMSubClusters

represent the marginal likelihood of Ck, ¯Ck,l and ¯Ck,r respectively. Concrete
expressions for the marginal likelihood, in the case of Gaussian or Multi-
nomial components (the component types considered in our experiments)
appear in [9].

Finally, a merge proposal is based on taking two existing clusters and
proposing merging them into one. The corresponding Hastings ratio is
Hmerge = 1/Hsplit where ¯Ck,l and ¯Ck,r are replaced with the two clusters,
and Ck is replaced with the result of the merge. For the derivation behind
Hsplit and Hmerge, see [1].

Importantly, and as any successful DPMM inference method, Chang and
Fisher III’s sampler can detect different numbers of clusters according to
the complexity of the dataset. For example, in Figure 1 we demonstrate
unlabeled data with 20 clusters while in Figure 2 we show data consisting
of 6 clusters. Using our implementation (the topic of the next section), the
sampler correctly detected the different numbers of clusters in each dataset,
while using the same code and the same hyperparamters.

3 Software

The proposed software supports three different software languages (Julia,
CUDA/C++, Python), two operating systems (Windows & Linux) as well as
multiple interface options. In terms of performance, the proposed software
is faster than other publicly-available packages that exist today and that
we were able to test. Particularly, as long as the product of N (number
of data points) times d (the dimension of the data) is not too low, then
our GPU version is consistently at least a few times faster than any other

9

(a) Unlabeled data

(b) DPMMSubClusters Detection

Figure 2: 6 clusters detected by DPMMSubClusters

implementation we are aware of.

3.1 Installation

Our software (licensed under GPLv2) is available on GitHub. The URLs
for our packages appear in Table 2. In order to compile the CUDA/C++
package the user will need C++14 (or higher) and CUDA 11.2 (or higher). For
visualization purposes (i.e., plotting points in 2D), the CUDA/C++ windows
version also requires OpenCV. However, this visualization is used only for
debugging purposes so the installation of OpenCV is not mandatory for
using our implementation. For the CPU package the user will need Julia
1.5 (or higher). For Python we used 3.8. The installation steps below were
tested successfully on Windows 10 (Visual Studio 2019), Ubuntu 18.04, and
Ubuntu 21.04.

(1) Install CUDA version 11.2 (or higher) from

https://developer.nvidia.com/CUDA-downloads

(2) git clone https://github.com/BGU-CS-VIL/DPMMSubClusters_

GPU

(3) Install Julia from: https://julialang.org/downloads/platform

(4) Add our DPMMSubCluster package from within a Julia terminal via

Julia package manager:
] add DPMMSubClusters

(5) Add our dpmmpython package in python:

pip install dpmmpython

10

(6) Add Environment Variables:

• On Linux:

(a) Add ”CUDA VERSION” with the value of the version of your

CUDA installation (e.g., 11.5).

(b) Add to the ”PATH” environment variable the path to the Julia

executable (e.g., in .bashrc add: export PATH =$PATH:$HOME/julia/julia-
1.6.0/bin).

(c) Make sure that CUDA PATH exist. If it is missing add it with
a path to CUDA (e.g., export CUDA PATH=/usr/local/cuda-
11.6/).

(d) Make sure that the relevant CUDA paths are included in

$PATH and $LD LIBRARY PATH (e.g., export PATH=/usr/local/cuda-
11.6/bin:$PATH, export LD LIBRARY PATH=/usr/local/cuda-
11.6/lib64:$LD LIBRARY PATH).

• On Windows:

(a) Add ”CUDA VERSION” with the value of the version of your

CUDA installation (e.g., 11.5).

(b) Add to the ”PATH” environment variable the path to the Julia

executable
(e.g., C:\Users\¡USER¿\AppData\Local\Programs\Julia\Julia-
1.6.0\bin).

(c) Make sure that CUDA PATH exists. If it is missing add it
with a path to CUDA (e.g., C:\Program Files\NVIDIA GPU
Computing Toolkit\CUDA\v11.6).

(7) Install cmake if necessary.

(8) Install PyJulia from within a Python terminal:
import julia;julia.install();

(9) For Windows only (optional, used on for debugging purposes): Install

OpenCV

(a) run Git Bash

(b) cd ¡YOUR PATH TO DPMMSubClusters GPU¿/DPMMSubClusters

(c) ./installOCV.sh

11

GitHub

Package
CUDA/C++ https://github.com/BGU-CS-VIL/DPMMSubClusters_GPU
Julia
Python

https://github.com/BGU-CS-VIL/DPMMSubClusters.jl
https://github.com/BGU-CS-VIL/dpmmpython

Table 2: GitHub URL for each package.

3.2 Building

For Windows for the CUDA/C++ package both of the build options below
are viable. For Linux use Option 2.
Option 1: DPMMSubClusters.sln - Solution ﬁle for Visual Studio 2019
Option 2: CMakeLists.txt

(1) Run in the terminal:

cd <YOUR_PATH_TO_DPMMSubClusters_GPU>/DPMMSubClusters
mkdir build
cd build
cmake -S ../

(2) Build:

Windows:

cmake --build . --config Release --target ALL_BUILD

Linux:

cmake --build . --config Release --target all

3.3 Post Build

Add Environment Variable:

• On Linux:

Add ”DPMM GPU FULL PATH TO PACKAGE IN LINUX” with
the value of the path to the binary of the package DPMMSubClusters -
GPU.
The path is: <YOUR PATH TO DPMMSubClusters GPU>/
DPMMSubClusters/DPMMSubClusters.

• On Windows:

Add ”DPMM GPU FULL PATH TO PACKAGE IN WINDOWS” with
the value of the path to the exe of the package DPMMSubClusters -
GPU.

12

The path is: <YOUR PATH TO DPMMSubClusters GPU>\DPMMSubClusters\
build\Release\DPMMSubClusters.exe.

3.4 Executing

Below are listed several options to run DPMM inference using our software.
Usually the best performance will be achieved by running the CUDA/C++
version which uses the GPU (when available).

3.4.1 From Julia code - CPU

The following sample code generates a synthetic GMM dataset with N =
105 points, dimension d = 2 and K = 10 clusters, and then ﬁts a DPMM to
the data (without knowing K or the other parameters of the GMM) using
the sampler.

using DPMMSubClusters

x,labels,clusters =

DPMMSubClusters.generate_gaussian_data(10ˆ5, 2, 10, 100.0)

hyper_params =

DPMMSubClusters.niw_hyperparams(Float32(1.0),

zeros(Float32,2),
Float32(5),
Matrix{Float32}(I, 2, 2)*1)
DPMMSubClusters.dp_parallel(x, hyper_params, Float32(100000.0),

100, 1, nothing, true, false, 15,
labels)

3.4.2 From a C++ code – GPU

dp parallel() in ‘dp parallel sampling class’ is the function that
should be called in order to run the program. The sample code below will
ﬁrst generate a synthetic random dataset and will then run the sampler to
ﬁt a DPMM to it.

srand(12345);
data_generators data_generators;
MatrixXd x;
std::shared_ptr<LabelsType> labels =

std::make_shared<LabelsType>();

double** tmean;

13

double** tcov;
int N = (int)pow(10, 5);
int D = 2;
int numClusters = 2;
int numIters = 100;

data_generators.generate_gaussian_data(N, D, numClusters, 100.0,

x, labels, tmean, tcov);

std::shared_ptr<hyperparams> hyper_params =

std::make_shared<niw_hyperparams>(1.0, VectorXd::Zero(D), 5,
MatrixXd::Identity(D, D));

dp_parallel_sampling_class dps(N, x, 0, prior_type::Gaussian);
ModelInfo dp = dps.dp_parallel(hyper_params, N, numIters, 1, true,

false, false, 15, labels);

3.4.3 From the command line – GPU

Running the CUDA/C++ program can be done from the command line (in
both Linux and Windows). There are a few parameters that can be used
to run the program. In order to set the hyperparams the params path
parameter can be used. The value of the parameter should be a path for a
Json ﬁle which includes the hyperparams (i.e. alpha or hyper params for
the prior) . In order to use this parameter follow this syntax:

--params_path=<PATH_TO_JSON_FILE_WITH_MODEL_PARAMS>

There are few more parameters like model path for the path to a npy
ﬁle which include the model and result path for the path of the output
results.

Our code has support for both Gaussian and Multinomial distributions. It
can be easily adapted to other component distributions, e.g., Poisson, as
long as they belong to an exponential family. The default distribution is
Gaussian. To specify a distribution other than a Gaussian, use the prior -
type parameter. For example:

--prior_type="Multinomial"

The Json ﬁle containing the model parameters can contain many parameters
that can be controlled by the user. A few examples are: alpha, prior, number
of iterations, burn out and kernel. The full list of parameters can be seen
in the function init() in ‘global params’. The result ﬁle is a Json ﬁle
which by default contains the predicted labels, the weights, the Normalized

14

Mutual Information (NMI) score and the running time per iteration. A
few other parameters can be added to the result ﬁle. Samples for these
additional parameters are commented out in the main.cpp ﬁle.

3.4.4 From Python code – CPU and GPU

The dpmmwrapper.py ﬁle contains an example for how to run either the
GPU or CPU packages from Python (look for the code in the function main).
In the following example we are generating a GMM synthetic dataset for
105 point, 2 dimensions and 10 clusters. We are running it on the GPU.

from julia.api import Julia
jl = Julia(compiled_modules=False)
from dpmmpython.dpmmwrapper import DPMMPython
from dpmmpython.priors import niw

data, gt = DPMMPython.generate_gaussian_data(sample_count=10000,

dim=2, k=10,
var=100.0)

prior = niw(kappa = 1, mu = np.zeros(2), nu = 3, psi = np.eye(2))
labels, clusters, results = DPMMPython.fit(data = data,

alpha = 100,
prior = prior, verbose = True,
gt = gt,
gpu = True)

Here, the variable “results” depends on the backend: for the CUDA/C++
backend it will be ‘null‘, while for the Julia backbone “results” will contain
other information (see the documentation of our Julia ‘ﬁt’ function). The
“gt” variable stands for the ground-truth labels.

3.4.5 From a Jupiter notebook – CPU and GPU

The notebook dpmmwrapper.ipynb can be executed to test different pack-
ages including our GPU and CPU packages on multiple datasets.

3.5 Test

Our CUDA/C++ package was built with the Test Development Driven (TDD)
methodology. The Windows version comes with 53 unit tests which cover
more than 60% of the code. Usually it takes less than 3 minutes to run all

15

tests. Those tests are for both Gaussian and Multinomial distributions, and
include the different CUDA kernels (for matrix multiplication) mentioned
in section 4 below. We used the GoogleTest framework to write those tests
and they were validated with a Visual Studio 2019 test engine.

4 The Proposed Implementation

4.1 Design and Implementation

In our implementation(s) after we initialize all the required objects, we start
running the iterations of the sampler. For each iteration in group step()
we execute the algorithm which is described below in detail. For concrete-
ness, below we will refer to the CUDA/C++ code during the algorithm’s
description and not to the Julia code. However, the structure of the code
in both packages is similar enough to be followed as long as the reader is
familiar with any of those languages.

We use the same notation as in [1] where N is the number of data points
and K is the (current estimate of the) number of clusters.

• For each iteration of the restricted Gibbs sampling:

(a) Sample cluster weights π1, π2, . . . , πK:

(π1, . . . , πK, ˜πK+1) ∼ Dir(N1, . . . , NK, α).

(14)

In our code we built a vector points count that holds the num-
ber of points for each cluster and apply a Dirichlet-distribution
sampling at once for all clusters:

dirichlet_distribution<std::mt19937> d(points_count);
std::vector<double> dirichlet = d(*globalParams->gen);

(b) Sample sub-cluster weights from a 2D Dirichlet distribution:

( ¯πkl, ¯πkr) ∼ Dir(Nkl + α/2, Nkr + α/2),

∀k ∈ {1, . . . , K}.

(15)

In order to propose meaningful splits that are likely to be ac-
cepted, the algorithm uses auxiliary variables such that each
cluster consists of 2 sub-clusters (conceptually thought of as
¯πk = { ¯πkl, ¯πkr} denote the weights of the
“left” and “right”).
sub-clusters of cluster k. The code is implemented in sample -
cluster params() in ‘shared actions’.

16

(c) Sample cluster parameters:

θk

∝
∼ fx(xIk; θk) fθ(θk; λ),

∀k :∈ {1, . . . , K}.

(16)

Here, fx(X; θk) is the likelihood of a set of data points under
the parameter θk, fθ(θ; λ) is the likelihood of the parameter θ
under the prior fθ(·; λ), ∝
∼ denotes sampling proportional to
the right-hand side of the equation, and Ik = {i : zi = k} is
the set of indices of that points labels as belonging to cluster
k. Each distribution (e.g., a Gaussian or a Multinomial) has its
own sample distribution() function which calculates the
cluster’s parameters. The prior classes for the Gaussian and
Multinomial distributions are ‘niw’ and the ‘multinomial -
prior’ respectively which inherit from the ‘prior’ class. The
likelihood is calculated in each class within the function log -
marginal likelihood(). In order to extend and support to
more distributions new classes which inherit from ‘prior’ may
be added.

(d) Sample sub-cluster parameters:

¯θkh

∝
∼ fx(xIkh; ¯θkh) fθ( ¯θkh; λ),

∀k :∈ {1, . . . , K} ,

h ∈ {l, r}.
(17)

¯θk = { ¯θkl, ¯θkr} denotes the parameters of the sub-clusters of clus-
ter k. In the code we used the same functions that we used for
the calculation the cluster’s parameters. We maintain the fol-
lowing objects in memory: a std::vector of local cluster
type where each item in that vector holds the cluster and sub-
cluster parameters (e.g., ¯θk, µk, Σ
k, ¯θkl, ¯θkr, ¯πkl and ¯πkr for the kth
Gaussian), the point counts (i.e., Nk, Nkl, Nkr) and the sufﬁcient
statistics as well as the cluster weights (i.e., π1, π2, . . . , πk).

(e) Sample cluster assignments for each point:

zi

∝
∼ ∑K

k=1 πk fx(xi; θk)1(zi = k),

∀i ∈ {1, . . . , N}.

(18)

To sample from a probability distribution efﬁciently we imple-
mented a GPU kernel in
sample by probability() based on a C algorithm [15]. To
summarize all k of a point we wrote the dcolwise dot all -
kernel() kernel. These kernels are working in parallel on all
components. Each component is working in a different GPU

17

stream while in each stream the points are aggregated to blocks
that are running in parallel. We set a number of 512 threads
per block. This parameter can be ﬁne-tuned to optimize the
performance based on the GPU hardware being used.

(f) Sample sub-cluster assignments for each point:

¯zi

∝
∼ ∑

h∈{l,r} πzih fx(xi; ¯θzih)1( ¯zi = h),

∀i ∈ {1, . . . , N}.

(19)

¯zi ∈ {l, r} variables are (conceptually thought of as “left” and “right”)
indicate which of the sub-cluster the ith point is assigned to. We
have a thin version of the cluster and sub-cluster parameters thin -
cluster params. For a single machine this structure is unneeded
since the data already exists in ‘local cluster’. However, we
maintain it not only for consistency with the Julia package but also be-
cause this option may be valuable in the future to scale our CUDA/C++
implementation to multiple machines.

For efﬁciency we are not copying the data; rather, we use std::shared -
ptr between the structures. We have also chunks of the data per GPU
in the device memory as part of the ‘gpuCapability’ structure.
The structure contains the following 3 properties: List of points xi
(d points), chunks of the cluster assignments zi (d labels) and
sub-cluster assignments ¯zi (d sub labels).
struct gpuCapability
{

int* d_labels;
int* d_sub_labels;
double* d_points;

};

class cudaKernel
{
protected:

std::map<int, gpuCapability> gpuCapabilities;

};

• Propose and Accept Splits:

By sampling the sub-clusters, one is able to propose and accept mean-
ingful splits that divide a cluster into its 2 sub-clusters.

(a) Propose to split cluster k into its 2 sub-clusters for all k ∈ {1, 2, . . . , K}.

18

(b) Calculate the Hastings ratio H and accept the split with proba-

bility min(1, H):

Hsplit =

αΓ(Nkl) fx(xIkl ; λ) · Γ(Nkr) fx(xIkr; λ)
Γ(Nk) fx(xIk; λ)

(20)

The proposal is done in the function should split local()
and the split itself is done in the kernel split cluster local -
worker().

• Propose and Accept Merges:

Merges are proposed by merging 2 sub-clusters into one with each
of the original clusters becoming a sub-cluster of the new merged
cluster.

(a) Propose to merge clusters k1, k2 for all pairs k1, k2 ∈ {1, 2, . . . , K}.

(b) Calculate the Hastings ratio Hmerge,

Hmerge =

Γ(Nk1
αΓ(Nk1

)
+ Nk2
)Γ(Nk2
)

p(x| ˆz)
p(x|z)

×

Γ(α)
Γ(α + Nk1

+ Nk2

×

)

Γ( α

2 + Nk1
Γ( α

)Γ( α
2 + Nk2
2 )Γ( α
2 )

)

,

and accept the merge with probability min(1, Hmerge). The pro-
posal is done in function should merge() and the merge itself
is done in the kernel merge clusters worker().

(21)

4.2 Optimizing Processing

Two kernels for optimization
In order to optimize the performance of the matrix multiplication which is
required in our algorithm we used two different CUDA kernels and decide
which one to use based on the size of the d × N matrix (where d is the
dimension of the data and N is the number of points). We added a built-in
capability to automatically select, at run-time, the best kernel automatically
based on d, N and the GPU capabilities. For more optimization in case that
the best kernel is known we provide an option to set the kernel which can
save some time at the beginning of the run (especially with a high N and a
high d). We measured the optimal kernel on an NVidia Quadro RTX 4000
card. On matrices whose size was below 640,000 size (d × N), Kernel #1 is
optimized and above this number Kernel #2 is optimized. Kernel #1 was
written in a native way for CUDA to multiply two matrices. Kernel #2 was
written with cublas API which is more optimal for big matrices.

19

Data Structure
To optimize the operations that needed to be done on the matrices and in
the device, all the data in the GPU and in Eigen’s structure are stored in
a column-major order. In general, in places which we needed to perform
operations on all data points, we iterate on the dimensions and run (in
parallel) each point calculation on a different GPU core.

4.3 Parallelism

The sampler from [1] was designed with massive parallelization in mind.
Thus, most of its operations are parallelizable. Particularly, sampling cluster
parameters is parallelizable over the clusters, sampling assignments can be
computed independently for each point, and cluster splits can be proposed
in parallel. Thus, a multi-core implementation is quite straightforward,
with the caveat that one needs to be careful with merges; i.e., to prevent
more than 2 clusters merge into the same single cluster; e.g., if clusters 1 and
2 are merged at the same time when clusters 2 and 3 are merged, this would
imply the three clusters (1,2, and 3) would be merged into a single one.
However, this would be inconsistent with the underlying model. Recall
that [1] released a multi-core single-machine C++ implementation (for CPU).
The nature of the algorithm, however, let us build an implementation that
goes beyond their original implementation, by allowing parallelization
across multiple machines, not just multiple CPU cores. In turn, this enables
us to not only leverage more computing power and gain speedups but also
provide practical beneﬁts in terms of memory and storage. For example,
our Julia implementation can be used within a distributed network of
weak agents (e.g., small robots collecting data). It also never transfers data
(which is expensive and slow); rather, we transfer only sufﬁcient statistics
and parameters. Thus, the proposed implementation is also well suited
for a network of low-bandwidth communication. Likewise, on a single
machine, we are able to use the GPU’s cores powers and Nvidia streams
and asynchronous calls.

4.3.1 Multiple GPU Streams

Most of the code was written as separated individual streams. Each stream
contains sequences of operations that execute in issue-order on the GPU.
CUDA operations in different streams may run concurrently. In many places
we tied the stream to a speciﬁc cluster. Thus, we could run close to O(1)
instead of O(K). We took advantage of CUDA’s asynchronous memory
allocation API which was exposed lately in version 11. We were able to

20

Figure 3: CUDA running in multi streams. Copy and Kernels are working
in parallel. Blue color indicates an active kernel. The horizontal axis stands
for time (in [sec]).

allocate and deallocate the parameters as part of the sequence of operations
and in this way to improve the concurrency of other kernels that were
running on other streams. In Figure 3 we show an example of memory
allocation operations that were performed on the GPU in parallel and at
the same time that the kernel operation was running.

4.3.2 Multiple GPU Cards

We tested the performance of our implementation by parallelizing the
processing across multiple GPUs. The logic that we adopt was to paral-
lelize the parts that we were running with multi-streams on multi-GPUs.
We implemented a container with all available GPUs: std::map<int,
gpuCapability> gpuCapabilities. In the places that we sample the
cluster and sub-cluster parameters and where the sufﬁcient statistics are
been calculated, instead of creating a stream on the same GPU we took
the next available GPU by calling to the function pick any device and
created a stream on the current selected device. Our hypothesis was that
in places which K is large there should be more impact. Our observation
when we tested 2 GPUs (of type Quadro RTX 4000) was that the perfor-
mance is not better. Consequently, we disabled that option by default in the
cudaKernel::init function using the following line: numGPU = 1. On
a different hardware and different GPU types, however, it is possible that
this option will yield better results. Thus, the user may want to experiment
with codenumGPU ¿ 1, depending on the available hardware.

4.4 Runtime Complexity

We now go throughout the CUDA/C++ runtime complexity step by step
based on the algorithm. The symbol G below denotes the number of GPU
cores.
For each iteration of a restricted Gibbs sampling: Sampling the cluster
and sub-cluster parameters (including weights) takes constant time for

21

In Julia it is parallelized over P processes on the master.
each cluster.
This takes O(K × d/P) time. In CUDA/C++ we observed that using ’omp
parallel for’ (for K) when sampling the cluster parameters is slower
than performing it sequentially. The root cause is the fact that Eigen (which
we rely on for matrix manipulation) already uses multiple CPU cores and
it also uses the GPU efﬁciently in each one of its threads. Thus, running
over the K clusters in parallel is slower than sequentially. Therefore the
complexity of CUDA/C++ is O(K × d). Sampling the cluster weights is
also done in constant time. Copying cluster and sub-cluster weights and
parameters from host to device is parallelized over CUDA streams over
K clusters. The process is parallelized over K clusters and over G GPU
cores so it takes O(K × d/G). Sampling the cluster assignments takes
O(N × K × T/G) time, where T depends on the observation model, e.g.
for multivariate Gaussian observation model with NIW prior T = d2,
wherein for a multinomial observation model with Dirichlet prior T = d.
Sampling the sub-cluster assignments takes O(N × T/G). Updating the
cluster and sub-cluster sufﬁcient statistics can be split up into 3 steps. The
ﬁrst step is for all streams to calculate the sufﬁcient statistics for the data
it is in charge of which is common to all kinds of distributions. This step
takes O(N/G) time. The second step is to calculate the sufﬁcient statistics
which is unique per the distribution. It is done in parallel to each sub-
cluster (l, r) and to the cluster. This step takes O(N × T/G) time. The
third step is to aggregate across all streams. This step takes O(K × d) time.
Overall, this takes O(N × K × T/G) + O(N × T/G) + O(K) time. So total:
O(N × K × T/G).

Splits: Proposing splits by looking at each cluster is O(K), noting that we
can drop the T here as we use previously calculated values. Processing all
the accepted splits requires updating the sufﬁcient statistics which could
take at the worst case O(N/G) + O(K) if all clusters are split.

Merges: Proposing merges by inspecting each cluster pair is O(K2), as in
the splits, we can drop the T, by using the previously calculated values. Pro-
cessing all the accepted merges also requires updating sufﬁcient statistics.
The worst case (i.e., if all clusters are merged) is thus O(N) + O(K).

To summarize, the total runtime complexity is O(K) + O(N × K × T/G) =
O(N × K × T/G)

22

4.5 Memory Complexity

We now look at the amount of memory used on the GPU. The data is
stored as one array, so we have O(d × N) on the GPU. The amount of
data for the labels and sub labels is O(N). Each stream also has a copy of
the cluster and sub-cluster parameters which takes O(K) space. We also
have to aggregate sufﬁcient statistics for each cluster after sampling the
assignments, taking O(K × T). Hence, we have a total memory usage of
O(d × N). Since usually N (cid:29) K, the memory overhead is insigniﬁcant in
comparison to the data itself.

5

Illustrations

5.1 Synthetic Data: Dirichlet Process Gaussian Mixture Model

(DPGMM)

We tested our implementation on small and large synthetic GMM datasets.
We wrote a class data generators to generate the synthetic datasets. We
ran 112 tests with different parameters: N (103,104,105,106), d (2,4,8,16,32,64,128)
and K (4,8,16,32). For each test we ran 100 iterations (sufﬁced for conver-
gence in all methods). We repeated each test 10 times with the same random
Gaussian synthetic data that we generated and then averaged the result-
ing NMI scores and running times. In each test we compared CUDA/C++,
Julia, and a Bayesian Gaussian Mixture from sklearn. The latter, like our
implementations, infers K with the rest of the model. However, unlike our
implementations, this sklearn’s model requires an upper bound on K.

During our testing we observed that, on a single machine, our CUDA/C++
implementation is faster than our Julia implementation in most of the cases.
The cases in which Julia was faster were when running with a low N (up to
10K) and a low d (up to 32). When the data is larger (e.g., when N = 106 or
d = 128) the CUDA/C++ solution is 2.5 times faster than Julia on average. In
general, when N × d × K is high the CUDA/C++ solution is faster than the
Julia solution. Both Julia and CUDA/C++ are faster on average than sklearn
solution: on average, Julia was 2.6 times faster and the CUDA/C++ was 5.3
times faster. Moreover, in a few cases the CUDA/C++ was 35 times faster
than sklearn.

In Figure 4 we demonstrated the time comparison for N = 106. In the left
side when d ≤ 4 we see that our solutions were faster than sklearn. Due to
the slowness of sklearn for the cases when d > 4 we set in the right side

23

]
c
e
s
[

)
e
l
a
c
s

c
i
m
h
t
i
r
a
g
o
l
(

e
m
T

i

Figure 4: DPGMM synthetic data, time, N = 106. In the right panel, due to
sklearn’s slowness, we had to give it the unfair advantage of using the true
K as the upper bound of the number of clusters.

of the ﬁgure an easier target for sklearn and set sklearn’s upper bound for
K to the true K (without doing it, sklearn’s running time for a high d was
impracitically slow, many orders of mangitude slower than our implementa-
tions). It is obvious from the ﬁgure than even when we gave a big advantage
to sklearn, our implementations still beat it. For accuracy we measured the
NMI for each case. NMI comparison is displayed in Figure 5. Again we split
the test into two parts. The left side of the ﬁgure describes a fair compari-
son where all the methods were given the same conditions. The right side
describes the case sklearn enjoyed the advantage of a reduced complexity
achieved by using the true K as the upper bound. Despite that advan-
tage, our solutions still yielded better results almost in all cases. All tests
can be reproduced by running the dpmmwrapper.ipynb Jupiter Notebook
from https://github.com/BGU-CS-VIL/dpmmpython. Because of
the slowness of sklearn, the notebook as is might take many days. You can
consider to reduce the number of permutations that we tested for N, d and
K or to reduce the number of repeats.

24

e
r
o
c
S
I

M
N

Figure 5: DPGMM synthetic data, NMI, N = 106. In the right panel, due
to sklearn’s slowness, we had to give it the unfair advantage of using the
true K as the upper bound of the number of clusters.

5.2 Synthetic Data: Dirichlet Process Multinomial Mixture

Model (DPMNMM)

We ran 72 tests with different parameters: N (103,104,105,106), d (4,8,16,32,64,128)
and K (4,8,16,32) while d ≥ K. For each test we ran 100 iterations (sufﬁced
for convergence). We repeated 10 times each test with the same random
multinomial synthetic data and then averaged the NMI results and running
times. In each test we compared our CUDA/C++ and Julia solutions to each
other (sklearn does not support multinomial components when the number
of components is unknown). Unlike in the case of Gaussian components,
here, with multinomials, we observed that our CUDA/C++ solution was
uniformly faster than our Julia solution. On average the former was 5 times
faster than the latter. The results (using N = 106) are displayed in Figure 6.
The corresponding NMI scores apear in Figure 7.

25

]
c
e
s
[

)
e
l
a
c
s

c
i
m
h
t
i
r
a
g
o
l
(

e
m
T

i

e
r
o
c
S
I

M
N

Figure 6: DPMNMM synthetic data, time, N = 106.

Figure 7: DPMNMM synthetic data, NMI, N = 106.

26

5.3 Real Data

We also tested our solution on real datasets, where, as pre-processing,
we used Principal Component Analysis (PCA) to reduce the dimension-
lity. For DPGMM we used the following datsets: mnist (N = 60000,d =
32,K = 10); fashion mnist (N = 60000, d = 32, K = 10) and ImageNet-100
(N = 125000, d = 64, K = 100). We compared our CUDA/C++ and Julia
implementations with sklearn BayesianGaussianMixture. For DPMNMM
we used 20newsgroups (N = 11314, d = 20000, K = 20), and compared
our CUDA/C++ and Julia implementations to each other (sklearn does not
support multinomial components when the number of components is
unknown). In the DPMNMM case, when the dimension was very high
(d = 20, 000) the CUDA/C++ package was 188 times faster than Julia. In Fig-
ure 8 we compared the running times on each dataset between the different
packages. It is clear from the ﬁgure that our CUDA/C++ package is much
faster from the two others, and that our Julia package is much faster than
sklearn. In Figure 9 we displayed the NMI comparison the different pack-
ages. On ImageNet-100, our CUDA/C++ and Julia packages are almost equal
sklearn (with a minor difference of 0.013). In all other cases, our solutions
were more accurate than sklearn. Morover, even though, on ImageNet-100,
our solutions did not score higher NMI values than sklearn, the K value
that we predicated was much more accurate. That is, while the true K was
100, sklearn predicated K = 500 (which was the value of the upper bound it
was given). In contrast we predicted on average K = 96.8 (with a standard
deviation of 17.78).

6 Summary and Discussion

We extended the DPMM inference method from [1] by efﬁcient and easily-
modiﬁable implementations for high-performance distributed sampling-
based inference. Our software can be adopted by practitioners in an easy
way where the user is free to choose between either a multiple-machine,
multiple-core, CPU implementation (written in Julia) and a multiple-stream
GPU implementation (written in CUDA/C++). We also provide an optional
Python wrapper which hides the CPU and GPU implementations as a
single package with one interface. The proposed implementation supports
both Gaussian and Multinomial components. However, it is also easy to
extent the code to support other exponential families. We also tested the
proposed implementation for learning models from real datasets as mnist,
fashion mnist, ImageNet-100 and 20newgroups. We compared our solution

27

]
c
e
s
[

)
e
l
a
c
s

c
i
m
h
t
i
r
a
g
o
l
(

e
m
T

i

Figure 8: DPMM on real data: running times. In the 20newsgroup dataset
(where the data dtype is discrete) the components are Multinomials. In the
other datasets the components are Gaussians.

to sklrean and showed that for large and high dimensions our solution
achieves the same, or better, accuracy while being much faster. Empirically,
we found that when using high-dimensional Gaussians on a single machine
our GPU package is faster than any other solution that is currently publicly
available. For Multinomial components, our GPU package showed even
better results which are by at least two order of magnitude faster than any
other existing solution. We provided a solution for cross operating systems
platforms (Windows & Linux). Our tests can be easily reproduced via a
Jupiter Notebook included with our Python package.

7 Computational details

We ran our tests on two different hardware. One with strong CPU, more
RAM and GPU from GTX family: i7-6800K CPU, 3.40GHz with 12 cores,
64GB RAM, GeForce GTX 1070. Second conﬁguration with slower CPU,
less RAM and GPU from RTX family: E5-2620 v3 CPU, 2.40GHz with 12
cores, 32GB RAM, Quadro RTX 4000. In both cases we used the same code.
We used CUDA driver version 11. We tested on Windows 10 and Linux

28

e
r
o
c
S
I

M
N

Figure 9: DPMM on real data: NMI scores. In the 20newsgroup dataset
(where the data dtype is discrete) the components are Multinomials. In the
other datasets the components are Gaussians.

Ubuntu 18.04 and 21.04. The Python version that we used for the wrapper
was 3.8. In the CUDA/C++ package for windows we used OpenCV version
4.5.2 to demonstrate visualization in 2D of the clustering process iteration
by iteration.

8 Open Sources

In Table 3 we specify all the open sources that we use in the CUDA/C++
package. In Table 4 we specify all the open sources that we use in the Julia
package.

29

Link
https://github.com/
rogersce/cnpy
https://github.com/
madler/zlib
https://github.com/
gcant/dirichlet-cpp
https://gist.
github.com/redpony/
fc8a0db6b20f7b1a3f23
https://github.com/
vcflib/vcflib

tuxfamily.org
https://www.kthohr.
com/statslib.html
https://github.com/
kthohr/gcem
https://github.com/
open-source-parsers/
jsoncpp
https://github.com/
Craigacp/MIToolbox
https://opencv.org/

Open Source
cnpy

zlib

Usage
Read and write models from
npy format
Required by cnpy

dirichlet distribu-
tion
logdet

Dirichlet distribution

Log determinant for Eigen us-
ing Cholesky

vcﬂib

eigen

stats

gcem

Log(gamma) and multinor-
mal sampling
Matrix and vector operations https://eigen.

Sampling from an inverse-
Wishart
Required by stats

jsoncpp

Read and write Json ﬁles

MIToolbox

Calculates NMI

opencv

Drawing (for debugging)

Table 3: C++ open-source packages used in the proposed implementation

30

Open Source
Clustering.jl

Usage
Evaluation metrics

DistributedArrays.jl Distribute Computations

Distributions.jl

Probability Distributions

JLD2.jl

NPZ.jl

Saving and restoring check-
points
Compatability with Numpy
data

SpecialFunctions.jl Log Gamma function

Link
https://github.com/
JuliaStats/Clustering.
jl
https://github.
com/JuliaParallel/
DistributedArrays.jl
https://github.
com/JuliaStats/
Distributions.jl
https://github.com/
JuliaIO/JLD2.jl
https://github.com/
fhs/NPZ.jl
https://github.
com/JuliaMath/
SpecialFunctions.jl

Table 4: Julia open-source packages used in the proposed implementation

31

Bibliography

[1] Chang, Jason and Fisher III, John W. Parallel sampling of DP mixture

models using sub-cluster splits. In NIPS, 2013.

[2] Wang, Ruohui and Lin, Dahua. Scalable estimation of dirichlet process
mixture models on distributed data. In Proceedings of the Twenty-Sixth
International Joint Conference on Artiﬁcial Intelligence, IJCAI-17, 2017.

[3] Dinari, Or, Yu, Angel, Freifeld, Oren, and Fisher III, John. Distributed
MCMC inference in Dirichlet process mixture models using Julia. In
IEEE CCGRID Workshop on High Performance Machine Learning, 2019.

[4] Dempster, Arthur P, Laird, Nan M, and Rubin, Donald B. Maximum
likelihood from incomplete data via the em algorithm. Journal of the
Royal Statistical Society: Series B (Methodological), 1977.

[5] Gelman, Andrew, Stern, Hal S, Carlin, John B, Dunson, David B,
Vehtari, Aki, and Rubin, Donald B. Bayesian data analysis. Chapman
and Hall/CRC, 2013.

[6] Ghosal, Subhashis and Van der Vaart, Aad. Fundamentals of nonpara-

metric Bayesian inference. Cambridge University Press, 2017.

[7] M ¨uller, Peter, Quintana, Fernando Andr´es, Jara, Alejandro, and Han-

son, Tim. Bayesian nonparametric data analysis. Springer, 2015.

[8] Sudderth, Erik Blaine. Graphical models for visual object recognition and
tracking. PhD thesis, Massachusetts Institute of Technology, 2006.

[9] Chang, Jason. Sampling in computer vision and Bayesian nonparametric
mixtures. PhD thesis, Massachusetts Institute of Technology, 2014.

[10] Antoniak, Charles E. Mixtures of Dirichlet processes with applications

to bayesian nonparametric problems. AoS, 1974.

[11] Pitman. Combinatorial stochastic processes. Technical report, Tech-
nical Report 621, Dept. Statistics, UC Berkeley, 2002. Lecture notes,
2002.

[12] Robert, Christian and Casella, George. Monte Carlo statistical methods.

Springer Science & Business Media, 2013.

32

[13] Jain, Sonia and Neal, Radford M. A split-merge markov chain monte
carlo procedure for the dirichlet process mixture model. Journal of
computational and Graphical Statistics, 13(1):158–182, 2004.

[14] Hastings, W Keith. Monte Carlo sampling methods using Markov

chains and their applications. 1970.

[15] Smith, Warren D. How to sample from a probability distribution. Tech-
nical report, Technical Report DocNumber17. NEC Research, 2002.

33

