Simple Techniques Work Surprisingly Well for Neural Network
Test Prioritization and Active Learning (Replicability Study)

Michael Weiss
michael.weiss@usi.ch
UniversitÃ  della Svizzera italiana
Lugano, Switzerland

Paolo Tonella
paolo.tonella@usi.ch
UniversitÃ  della Svizzera italiana
Lugano, Switzerland

2
2
0
2

y
a
M
4
2

]

G
L
.
s
c
[

2
v
4
6
6
0
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Test Input Prioritizers (TIP) for Deep Neural Networks (DNN) are an
important technique to handle the typically very large test datasets
efficiently, saving computation and labelling costs. This is particu-
larly true for large scale, deployed systems, where inputs observed
in production are recorded to serve as potential test or training data
for next versions of the system. Feng et. al. propose DeepGini, a very
fast and simple TIP and show that it outperforms more elaborate
techniques such as neuron- and surprise coverage. In a large-scale
study (4 case studies, 8 test datasets, 32â€™200 trained models) we
verify their findings. However, we also find that other comparable
or even simpler baselines from the field of uncertainty quantifica-
tion, such as the predicted softmax likelihood or the entropy of the
predicted softmax likelihoods perform equally well as DeepGini.

CCS CONCEPTS
â€¢ Software and its engineering â†’ Software testing and de-
bugging; â€¢ Computing methodologies â†’ Neural networks; â€¢
Theory of computation â†’ Active learning.

KEYWORDS
Test prioritization, neural networks, uncertainty quantification

ACM Reference Format:
Michael Weiss and Paolo Tonella. 2022. Simple Techniques Work Surpris-
ingly Well for Neural Network Test Prioritization and Active Learning
(Replicability Study). In Proceedings of the 31st ACM SIGSOFT International
Symposium on Software Testing and Analysis (ISSTA â€™22), July 18â€“22, 2022,
Virtual, South Korea. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3533767.3534375

1 INTRODUCTION
Deep Neural Networks (DNN) are typically trained and tested on
large data sets. Test input prioritizers (TIP) allow developers to order
or to identify a subset of the test inputs which â€“ similar to tradi-
tional software testing [5, 22, 35] â€“ detect faults at a reduced time
and energy cost. In addition, and arguably even more importantly,
TIP can be used with production ML (Machine Learning) based
systems, to facilitate iterative improvements of the deployed model:

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9379-9/22/07. . . $15.00
https://doi.org/10.1145/3533767.3534375

As nicely described through the Virtuous Circle of ML [27], depicted
in Figure 1, modern ML based systems profit from economies of
scale, as the more often a ML based system is used, the more real-
world production data it observes and collects, which in turn can
be used to further test and improve its ML components, leading to
better predictions and thus attracting even more users. However,
this iterative process becomes quickly infeasible as large scales are
reached: With a very large number of users, observed production
data become hard if not impossible to process for model improve-
ment. TIP can be used to identify the small share of production
inputs which expose model faults and should thus be integrated in
a test suite, or which are generally insufficiently represented in the
training data and should thus be used for active learning, i.e., should
be added to the training set of future training runs. This is in line
with the industrial practice of data-centric quality improvement,
described by Andrej Karpathy, Teslaâ€™s director for AI, in a recent
presentation [14].

In order to handle inputs of large scale systems, TIP must scale
very well as well. To this aim, Feng et. al. [7] proposed DeepGini,
an approach for classification DNNs which assigns priority scores
based on the softmax outputs of a DNN at negligible cost. The au-
thors showed that DeepGini reliably outperformed classical DNN
TIP from the two families of neuron coverage and surprise cov-
erage. In this paper, we replicate, extend and contextualize the
findings obtained in that paper. Specifically, we make the following
contributions:
â€¢ Replication We successfully replicate the key findings by Feng
et. al. [7], showing that DeepGini outperforms various types of
surprise and neuron coverage metrics on execution time, as well
as test prioritisation and active learning effectiveness.

â€¢ Comparison We extended the list of TIP approaches compared
against DeepGini with recent improved variants of surprise cov-
erage and uncertainty quantifiers. Our experiments show that

Figure 1: Virtuous Circle of Machine Learning

 
 
 
 
 
 
ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

Michael Weiss and Paolo Tonella

uncertainty quantifiers, including a simple baseline that uses just
the predicted class likelihood, perform comparably well.

higher priority to OOD data, making active learning a viable activity
in the virtuous circle of ML.

â€¢ Statistical Evaluation Previous work shows evidence that some
TIP performances are highly sensitive to random influences dur-
ing model training [41]. We thus perform our experiments on
100 individually trained models and discuss the statistical signifi-
cance of our results. Such statistical analysis was not carried out
in the original DeepGini paper [7].

â€¢ Versatile Artifacts Not only do we release all our code, as gen-
erally requested for replication studies [37], but we also publish
core components of the investigated techniques, which we think
could be useful for a wide range of possible follow-up studies.
Our implementations of neuron coverage TIP, surprise coverage
TIP and DeepGini will are easily installable through the pypi
package manager. Our generated datasets are made available as
standalone artifacts.

2 MOTIVATION
In the following, we motivate the use of DeepGini and similar TIP
techniques in the test prioritization and active learning scenarios.

2.1 Test Prioritization
Test suites for traditional (non ML-based) software systems can
become very large and running the full test suite might come at
a large cost, both regarding time (making the development pro-
cess inefficient) and energy consumption. Correspondingly, TIP for
traditional software has been investigated to give priority to tests
which are more likely to fail, providing fast feedback about the
discovered problems [5, 22, 35]. When testing ML based systems, in
addition to saving computation time and providing early feedback,
TIP is also important to mitigate the high cost associated with test
data labelling. For instance, test data gathered from the production
environment is typically unlabelled. Another example is automati-
cally generated inputs, e.g. produced by fuzzing techniques, where
clearly, the main cost does not come with input generation, but
from manual labelling of the relevant inputs.

For companies adopting the virtuous circle of ML (see Figure 1),
TIP is extremely useful, because a constant stream of potentially
useful test data is observed on deployed systems and collected for
the continuous improvement of the model. Here, not all inputs
which lead to a model failure (i.e., a wrong or inaccurate model
prediction) lead to an observable system failure, as the production
system may be able to compensate for occasional deficiencies of
the ML components. Hence, it is nontrivial to identify production
time inputs which should be collected and reported to the testing
team.

2.2 Active Learning
Active Learning builds on the idea that novel data, observed in
production, can be used when training the next generation of the
same model. Clearly, before production data can be used, it has to
be (usually manually) labelled. Again, it becomes very important
to select unlabelled raw data which has the potential to be most
useful for future model improvement. This is typically data which
is out-of-distribution (OOD) with respect to the previous training
dataset. TIP could drastically reduce the labelling cost by giving

Several recent papers target the problem of identifying the most
relevant among the available raw inputs [4, 7, 15, 24, 38] to im-
prove the behaviour of an ML based system by retraining its DNNs.
TIP is clearly a very suitable candidate for such a task [7]. Specifi-
cally, a TIP would be used as acquisition function [10] to prioritize
unlabelled data according to their potential to improve a DNNâ€™s per-
formance when such data is added to the training set. The selected,
highest prioritized data would then be labelled (typically manually,
by a human) and added to the training set for retraining. Here,
we note that the goal when using a TIP for active learning is now
slightly shifted with respect to test prioritization: We are not pri-
marily interested in whether an input leads to a model failure (e.g.,
a mis-classification), but whether an input is out-of-distribution,
i.e., insufficiently represented during training.

3 APPROACHES
In the following, we describe the different TIP techniques we eval-
uate in this paper. For all of them, we use the following common
notation: We let ð‘› denote the number of training samples, ð‘Ž the
number of neurons considered by the specific approach and ð‘˜ as a
further model specific parameter.

3.1 Neuron Coverage
Similar to code coverage, Neuron Coverage (NC) can be used to
prioritize a list of test inputs, either by descending amount of abso-
lute coverage achieved by each test or by the additional amount of
coverage that each test brings with respect to the overall coverage
reached by previously executed tests. NC creates a (boolean) cov-
erage profile for every test input, where, individually, coverage of
each DNN neuron is evaluated based on its activation. Depending
on the specific variant, the coverage profile may distinguish differ-
ent activation segments of each neuron and treat them as separate
coverage targets. We refer to Feng. et. al. [7] for a nice overview of
different NC technqiues, and only provide a very short description
of each approach in this section.

In Neuron Activation Coverage (NAC-ð‘˜) [31], a node is considered
covered, if its activation is higher than ð‘˜. To also take low activation
into account, ð‘˜-Multisection NC (KMNC-ð‘˜) [23] the range of acti-
vations observed on the training set for each neuron is uniformly
divided into ð‘˜ segments, which are considered covered if the DNN
neuron activations for the considered test inputs fall within each
segment. Taking the opposite direction, Neuron Boundary Coverage
(NBC-ð‘˜) [23] regards only two segments per node, representing ac-
tivations that fall below or above the activation range (boundaries)
observed during training. To move further away from these bound-
aries and detect novel, distant activations, the training boundaries
can be shifted apart by ð‘˜ðœŽ, where ðœŽ denotes the standard devia-
tions of the activations observed for the corresponding node on the
training set. As a special case of NBC-ð‘˜, Strong Neuron Activation
Coverage (SNAC-k) [23] considers only the upper bound, thus halv-
ing the size of the coverage profile. Instead, Top-ð‘˜ NC (TKNC-ð‘˜) [23]
considers a neuron covered if it belongs to the ð‘˜ neurons with the
highest activations within the same layer.

Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study)

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

CTM vs. CAM. Given a coverage profile, the coverage of a test
input can be calculated as the percentage of trueâ€™s in its coverage
profile. Prioritization of tests by decreasing coverage is denoted
as Coverage-Total Method (CTM). While CTM is fast and easy to
run, it has the disadvantage that it does not necessarily aim for
diversity in the prioritized tests, as different tests with high cov-
erage might cover the same neurons, while tests that cover previ-
ously uncovered neurons might have a low total coverage, which
would schedule them late in the execution. To address this issue,
Coverage-Additional Method (CAM) aims to reach an overall com-
bined coverage as quickly as possible, using a greedy approach.
By starting with the test with highest coverage, CAM then always
adds the test which covers the most previously uncovered targets
in the coverage profile. It thus automatically increases the diversity
in the prioritized test list. The main disadvantage of CAM is its
computational complexity, running in a time quadratic in the test
set size.

Our implementation. The primary computational challenge arou-
nd NC arises from memory requirements, which grow linear in
ð‘Ž for all NC and thus quickly become intractable when handling
very large DNNs. We mitigate this problem, to some extent, by
using online batch processing: Collecting activation traces (which
typically consist of 64 or 32 bit floats) and reducing them to the
typically much smaller boolean coverage profiles in batches allow
us to reduce memory consumption by up to (almost) 64 times,
depending on the size of the coverage profile of the used NC.

3.2 Surprise Adequacy and -Coverage
Surprise Adequacy (SA) denotes a collection of techniques to mea-
sure how surprising a DNN input is, i.e., how novel or out-of-
distribution the DNN activations for a given input are with respect
to the activations observed on the training set. While in theory SA
measurements could be applied on the full set of neurons, similar
to NC, in practice it is often limited to the activations of a single
or few hidden layers. If we denote such activation traces as ð´ð‘‡ð‘†ð´,
we thus have ð‘Ž = |ð´ð‘‡ð‘†ð´ |. Considering only a subset of the DNN
nodes when collecting ð´ð‘‡ð‘†ð´ brings two main advantages: First,
later layers are responsible for processing higher level features ex-
tracted from the raw input data, making them more appropriate to
measure surprise [15]. Second, the use of fewer activations allows
for a faster and more memory efficient execution of SA. The lat-
ter is particularly important, as the computational cost is a major
shortcoming of most SA variants [16, 41].

3.2.1 Likelihood-Based SA. Likelihood-Based SA (LSA) estimates
the negative log-likelihood of a test inputâ€™s ð´ð‘‡ð‘†ð´ using a Gaussian
Kernel Density Estimator (KDE) parameterized on the training set
ð´ð‘‡ð‘†ð´. Gaussian KDE is known to be slow to train, with quadratic or
even cubic runtimes in ð‘› [40]. Moreover, likelihood quantification
at prediction time runs in O (ð‘› Â· ð‘š), as inputs must be compared to
all training dataâ€™s activations [16]. Fast implementations such as the
one by scikit-learn1 mitigate this problem as long as ð‘š and ð‘› are
not too large [41]. To the best of our knowledge, all publicly avail-
able source code that accompanies papers using surprise adequacy
[15, 41] rely on the scikit-learn implementation, which comes at

a cost: The implementation is numerically unstable, and as we use
it with many features, it is likely that an imprecise representation
of the ð´ð‘‡ð‘†ð´ covariance matrix leads to a crash of the KDE2. Look-
ing at the above mentioned previous implementations of LSA, this
problem appears to have been mitigated by choosing a layer with
only a few neurons, i.e., a low ð‘Ž, and by ignoring neurons whose
activationâ€™s variance on the training set is below some threshold.

Our implementation of LSA. Also relying on the fast and well
tested scikit-learnâ€™s GaussianKDE implementation [30], our im-
plementation aims to reduce the risk of KDE crashes due to nu-
merical imprecision to a minimum: Besides providing a way to
specify a threshold for minimum required variance in a neuronâ€™s
activation trace, it also exposes an interface to specify a (relative
or absolute) number of neurons to consider, which are selected
by decreasing variance. In addition, we extended scikit-learnâ€™s
GaussianKDE to modify the covariance matrix of the training ð´ð‘‡ð‘†ð´
by adding a dynamically chosen, small ðœ– to the covariances diago-
nal, making it positive semidefinite, while keeping the falsification
of predictions small. Combined, these steps allow us to eliminate
most crashes due to numerical instability. In the very rare cases
when GaussianKDE still fails, our implementation fails gracefully,
by returning a surprise value of 0.

3.2.2 Distance-Based SA. Distance-Based Surprise Adequacy (DSA),
which is defined only for classification problems, quantifies surprise
as the ratio between a test inputâ€™s ð´ð‘‡ð‘†ð´ (Euclidean) distance to the
closest training ð´ð‘‡ð‘†ð´ of the same class, and the distance between
that point and its closest training ð´ð‘‡ð‘†ð´ belonging to any other class.
DSA has been shown to outperform LSA for test input prioritization,
active learning [15] and fault prediction [41]. The major disand-
vantage of DSA is clearly its computational cost [41]. For a single
test input, the distance to each ð´ð‘‡ð‘†ð´ has to be calculated, leading
to O (ð‘Ž Â· ð‘›) time and space complexities. To mitigate this problem,
only a subset of the training ð´ð‘‡ð‘†ð´ has been considered, leading
to acceptable DSA performance [41]. While this does not reduce
the asymptotic complexity, it provides both speed and memory
reductions proportionate to the selected sampling ratio. The same
authors [41] also proposed to calculate DSA for multiple test inputs
at the same time using multithreaded, vectorized batch calculations.
While this leads to considerable practical improvements in terms of
speed, it requires drastically more memory: With a batch size ð‘ and
ð‘¡ threads, peak memory load can increase to O (ð‘¡ Â· ð‘ Â· ð‘› Â· ð‘š), which
can become problematic even for moderate batch and thread sizes.
Let us consider for example a setting with ð‘Ž = 1600 and ð‘š = 60, 000
(as in our MNIST case study). Choosing ð‘¡ = 8 and ð‘ = 32, and a
float precision of 32 bits, would result in a worst-case peak memory
load of more than 98.3GB.

Our implementation of DSA. Based on the implementation by
Weiss et. al. [41], our implementation supports (uniform) training
set subsampling and vectorized, multithreaded batched computa-
tion. Besides minor changes, we have replaced their batching and
threading logic with one that reduces the variance in memory con-
sumption, which thus allows for a much better tuning of the batch
size ð‘ for optimal performance.

1https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html

2See e.g., https://stackoverflow.com/a/66902455/

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

Michael Weiss and Paolo Tonella

3.2.3 Mahalanobis-Distance Based SA. Mahalanobis-Distance Based
SA (MDSA), which â€“ despite its name â€“ can be understood as an
alternative to likelihood based SA, aims to provide a much more
efficient way to compute the likelihood of a testâ€™s ð´ð‘‡ð‘†ð´, given the
observed ð´ð‘‡ð‘†ð´ of the training set. By requiring only the covariance
matrix of the training set ð´ð‘‡ð‘†ð´, the Mahalanobis distance [25] can
calculate the likelihood for the given inputâ€™s ð´ð‘‡ð‘†ð´ efficiently, mak-
ing prediction runtimes constant in the training set size. Memory-
wise, storing the covariance matrix costs O (ð‘Ž2). To calculate the
covariance matrix, the full set of training ð´ð‘‡ð‘†ð´ has to be collected
(thus typically resulting in a setup runtime and memory require-
ment linear in ð‘›). In practice, similar to DSA and LSA, these setup
requirements could potentially be relaxed by choosing a subset of
training samples (similar to DSA) or only high-variance features
(similar to LSA). Even more so, the memory requirements could
be made constant in ð‘› by using an online covariance estimation
algorithm, specifically designed to scale well for large datasets by
estimating the covariance on a non-persistent stream of data [3, 36].
We note however, that, as for SA, ð‘Ž is typically much smaller than
the total number of neurons, and the size of ð‘› only matters at setup
time, so for many real-world examples the latter optimizations are
not really needed.

Our implementation of MDSA. We fit the covariance matrix using
the estimator provided in scikit-learn, and its integrated function
to calculate the Mahalanobis distance, which thus introduces only
minimal technical debt in our code and leverages a well-tested,
performance optimized implementation.
3.2.4 Multi-Modal SA. If the distribution of training set ð´ð‘‡ð‘†ð´ is
multi-modal, i.e., can be interpreted as a set of different, simpler
(e.g. multivariate gaussian) distributions, is is reasonable to perform
likelihood estimation taking advantage of multimodal distributions.
To do so, two modifications of SA have been proposed [17]: Multi-
modal LSA (MLSA), where instead of KDE, a Gaussian Mixture Model
(GMM) is used, and Multimodal MDSA (MMDSA), where training
set activation traces are clustered using the ð‘˜-means algorithm,
followed by the creation of a distinct instance of MDSA for each
cluster.

Our implementation of Multi-Modal SA. To implement MLSA,
similar to LSA we use the scikit-learn implementation of GMM,
which allows for a well-tested, fast and compact code. To implement
MMDSA, we introduce a general, abstract MultiModal-SA class ar-
chitecture, based on the composite design pattern [11]. While being
a SA class instance itself, MultiModal-SA consists of a collection
of sub-SA instances and a discriminator to decide which sub-SA
instance to use for a given input. Fitting of both discriminator and
sub-SA instances to the data is achieved by means of a transpar-
ent, single method call. Given this abstract class, our MMDSA is
then implemented by using the scikit-learn implementation of
ð‘˜-means as discriminator and our MDSA implementation for the
sub-SA instances. Provided that the different sub-SA instances are
independent of each other and thus parallelizable, our implemen-
tation of the MultiModal-SA class allows the specification of the
number of threads to be used when quantifying surprise.

3.2.5 Per-Class SA. Classification problems can be viewed as a
special case of the MultiModal SA distributions described above.

Indeed, samples for which the DNN predicts the same class label
form a cluster. Thus, already when LSA was first proposed, the
authors recommended to use a dedicated instance of LSA for each
class label [15], intuitively leading to a better likelihood estima-
tion. We denote this approach as Per-Class LSA (PC-LSA). Naturally,
we would expect a similar advantage also for MDSA, MLSA and
MMDSA, leading to PC-LSA, PC-MDSA, PC-MLSA and PC-MMDSA.
It is worthwhile that for SA variants for which quantification time
grows in ð‘›, such as LSA, per-class quantification furthermore re-
duces quantification time, as only a subset of the training set ð´ð‘‡ð‘†ð´
has to be considered.

Our implementation of Per-Class SA. Given our MultiModal-SA
implementation described above, implementing per-class variants
of SA is easily achieved by using the predicted class label as dis-
criminator.

Surprise Coverage. To use surprise adequacy as a coverage
3.2.6
criterion, the coverage profile is constructed as a one-dimensional
boolean array, representing equally large, adjacent buckets of sur-
prise. A bucket is covered by a test input if such input has a surprise
adequacy in the range represented by the given bucket. Thus, for
each considered DNN neuron the surprise coverage profile of an
input can have either one or zero covered buckets, where the latter
happens in case the surprise value falls outside of the range of
surprise values defined for the neuronâ€™s buckets. Once surprise
coverage profiles are available, test prioritization can be achieved
by applying the total or the additional method (CTM or CAM).

3.3 DeepGini
Having massive scalability in mind, DeepGini [7] provides a way
to calculate a test prioritization score by working only on the test
inputs activations of the DNNs softmax output layer (which also
limits the applicability of DeepGini to classification problems). Thus,
DeepGiniâ€™s runtime and memory requirements are only dependent
on the number of classes, which is typically small and which we
consider a constant, making the runtime and memory requirments
of DeepGini O (1). Given a classification problem with ð¶ classes,
the softmax values ð‘™ð‘ (ð‘–) for class ð‘ and input ð‘–, which are values
between 0 and 1, summing up to 1, DeepGini is defined as

DeepGini(ð‘–) = 1 âˆ’

ð¶
âˆ‘ï¸

ð‘=1

ð‘™ð‘ (ð‘–)2

The DeepGini score is minimum (zero) when the DNN predicts
one class with high certainty, assigning a softmax value of zero to
all classes except the predicted one. It then increases as softmax
values are distributed across an increasing number of classes that
compete with the predicted one as classification alternatives. When
used as a TIP, DeepGini orders the inputs by decreasing score, so
as to give higher priority to inputs with higher spread of softmax
values across classes, as these inputs are associated with a higher
classification uncertainty.

Besides its fast runtime, the design of DeepGini is primarily
motivated by the property of having a single maximum, occurring
when the predicted softmax value is the same for all classes, as
proved by DeepGiniâ€™s original authors [7].

Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study)

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

3.4 Uncertainty Quantifiers
As stated by its authors, â€œDeepGini is designed [. . . ] to quickly iden-
tify misclassified testsâ€[7], i.e., to estimate the uncertainty of the
DNN on the reported classification. With a similar objective, the lit-
erature provides a range of DNN uncertainty quantifiers [42]. In this
paper, we thus compare DeepGini also against such uncertainty met-
rics. Specifically, we consider (Softmax-)Entropy, Vanilla-Softmax,
Prediction-Confidence Score and Monte-Carlo Dropout.

Vanilla Softmax. The Vanilla-Softmax metric is simply the high-
est activation in the output softmax layer for a classification prob-
lem, subtracted from 1 to obtain a metric that correlates positively
with the misclassification probability, similar to DeepGini.

Vanilla Softmax(ð‘–) = 1 âˆ’

ð¶
max
ð‘=1

ð‘™ð‘ (ð‘–)

As the argmax of the softmax array is used as the DNNs pre-
dictions, Vanilla Softmax comes at virtually no computational or
theoretical complexity and is thus often used as a naive, very sim-
ple benchmark [4, 13, 29, 42]. Besides its simplicity, Vanilla Soft-
max also guarantees a single global extremum, as was aimed for
when proposing DeepGini: Vanilla-Softmax reaches its single global
maximum when all classes are predicted with the same softmax
likelihood. The proof sketch is simple: Clearly, the hypothesized
optimum (ð‘™ð‘ (ð‘–) = 1/ð¶, âˆ€ð‘ âˆˆ {1, ..., ð¶}) is a valid softmax array (i.e., all
values are between 0 and 1, sum up to 1 and the predicted class has
the weakly highest value). Any lower value of the predicted classâ€™s
softmax value, which is needed to further increase Vanilla-Softmax,
would make it no more the highest, hence the predicted, value.
Similarly, to show that this is the unique optimum we can observe
that if the winning class has ð‘™ð‘ (ð‘–) = 1/ð¶ , all other classes must have
the same value in a valid softmax array.

Prediction-Confidence Score (PCS) [45]. The Prediction-Confidence
Score is defined as the difference in softmax likelihood between
the predicted class and the second runner-up class. The main mo-
tivation behind PCS is that smaller values of PCS indicate that
a prediction is made close to the decision boundary, i.e., a small
change in the input might possibly change the predicted class. Same
as for Vanilla-Softmax and DeepGini, when referring to PCS, we ac-
tually subtract it from 1 to get a score which is expected to correlate
positively with misclassification probability [42].

Entropy. The authors of DeepGini also considered the entropy
in the softmax layer as an alternative to their approach [7]. No
empirical comparison between the two was made however, as the
authors motivated their preference for DeepGini claiming that it
is a simpler metric, better justified for DNN test prioritization and
requiring no information theory background and a â€˜non-statistical
viewâ€™ [7] to the problem.

MC-Dropout. Point-predictor based uncertainty metrics, such
as Vanilla-Softmax, PCS and Entropy, while showing very good
misclassification prediction results in practice [42], suffer from ma-
jor theoretical drawbacks when used as uncertainty quantifier [8].
A simple, yet theoretically much better founded way to extract
uncertainty is Monte-Carlo Dropout, where dropout layers used for
stochastic regularization during training are enabled at prediction
time, which allows the sampling of multiple stochastic predictions

for the same test input and correspondingly for the inference of
both an average prediction and the associated uncertainty (e.g.,
standard deviation) from the observed distribution of predicted
values [9].

Our implementation of uncertainty quantifiers. We base our im-
plementation of uncertainty metrics on uncertainty-wizard [43],
which provides configurable, tested and fast implementations of
various uncertainty metrics. It also exposes interfaces to add new
uncertainty metrics, which we used to implement DeepGini.

4 EMPIRICAL PROCEDURE
The goal of our experiments is to compare the previously described
TIP approaches in various configurations and along three dimen-
sions: (1) runtime, (2) test prioritization effectiveness, and (3) active
learning capabilities. The aim is to replicate the original DeepGini
results, but also to extend them to uncertainty quantifiers, not con-
sidered in the original DeepGini paper [7], and to more recent
variants of SA. We also make sure that the non determinism affect-
ing the training process is taken into account by statistical tests,
not carried out in the original DeepGini paper. Correspondingly,
we answer the following research questions:

RQ1: Replication Are our results consistent with those obtained
by the authors who proposed DeepGini, i.e., does DeepGini
outperform NC, LSC and DSC, along the three considered
dimensions?

RQ2: Comparison to Other Approaches How does DeepGini
compare to uncertainty quantifiers and newer SA variants?
RQ3: Statistical Analysis Are our findings sensitive to random

influences due to the DNN training process?

4.1 Experimental Design
As we are replicating an existing paper, the broad context of our
analysis is predetermined by the original paper. Within such con-
straints however, we made the following modifications to mitigate
some of the original threats to validity, leading to overall more
reliable results.
â€¢ Reimplementation: We implemented the scripts to run our ex-
periments without consulting or copying from the reproduction
package provided alongside the replicated paper [7]. This reduces
the risk of having the same bugs in our code and in the original
code, and follows the ACM guidelines for reproducibility [1].
â€¢ Repeated Runs: Random influences in the DNN training can have
major influences in the performance measured by adequacy crite-
ria [41]. We thus repeat all our experiments 100 times and discuss
statistical significance in RQ3.

â€¢ Additional Case Study and Reimplemented Model Architectures:
Replicability studies are also supposed to check that under reason-
able changes that extend the validity scope, the overall findings
remain unchanged [2]. We thus replaced one case study (Svhn)
with a different one (Imdb) and implemented our own versions
of the DNN model architectures for the case studies, to see if the
replicated findings indeed generalize to a new case study and to
different implementations of the original case studies.

â€¢ Original threats to validity: We identified issues (e.g., with ad-
versarial examples) in the original experimental design, which

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

Michael Weiss and Paolo Tonella

may give DeepGini an unfair advantage. We thus changed the
experimental design to fix them (see below).

4.1.1 Timing Experiments. Timing experiments were conducted
on the first 10 (of the 100) test prioritization experiments run for
the Mnist case study. While timing was taken, the machine was
otherwise not used, to ensure an as fair as possible comparison
between the different approaches. For the remaining experiments,
times were also tracked, and are available in our reproduction
package, but for reasons of efficiency, we ran multiple runs in
parallel, which makes these runtime measures not comparable.

4.1.2 Test Prioritization. Our experimental design follows the de-
sign described in the replicated paper: Test datasets are prioritized
by all compared approaches and are evaluated for their ability to
give high priority to examples that make the DNN fail (in the fol-
lowing, we equate a DNN failure with a DNN misclassification, as
DeepGini is applicable only to classifier DNNs). In testing terminol-
ogy, each misclassified input would be considered a (unique) fault,
and the approach which detects most faults as early as possible is
preferable. This is measured using a standard metric in test priori-
tization, the Average Percentage Faults Detected (APFD) [5, 22, 35].
Every approach is tested on a nominal test set, which contains just a
few misclassifications, and on an out-of-distribution (OOD) test set,
containing additional misclassifications. The OOD test set consists
of the nominal inputs, extended with additional, harder to classify
inputs. For the latter, the original paper uses adversarial examples,
while we use corrupted inputs, as motivated below.

Problem with Adversarial Examples. The replicated paper uses ad-
versarial examples to create OOD, misclassified data points. While
using adversarial examples has some advantages, amongst which
are a very easy way to create arbitrary input formats using tools
like foolbox [32, 33] and their guaranteed misclassification, we
argue that they come with a major threat to validity when com-
paring DeepGini against Neuron Coverage and Surprise Adequacy:
Most adversarial attacks perturbate the input in a minimal way,
which is still sufficient to produce the desired model output (typi-
cally a misclassification). DeepGini, but also Max-Softmax, Entropy
and PCS, compute their priority score based exactly on the out-
put that is deliberately changed by an adversarial attack. Since
adversarial attacks perform minimal changes necessary to achieve
a misclassification, typically the output softmax distribution under
an adversarial attack is reshaped until one of the wrong classes
becomes the winner, leaving the other classes with a relatively
high softmax value, which makes the input easily detectable by
DeepGini, Max-Softmax, Entropy and PCS, giving them an unfair
advantage over the other approaches. Moreover, the adversarial
input perturbations introduced by adversarial attacks cannot be
considered as representative of real world test inputs, as they are
synthesized artificially based on the internal DNN operations to
fool the final classification. Hence, with adversarial attacks as OOD
test set it would be unclear if the observed results generalize to
natural, more realistic OOD inputs.

Corrupted Data. To mitigate the threat to validity which comes
with adversarial data, we use corrupted inputs instead: Corrupted
inputs are test sets where nominal data points are manipulated
using a range of modifiers inspired by real-world input corruptions.

Figure 2: Active Learning Experimental Setup

They thus provide DNN-independent, realistic OOD data. While the
literature uses and provides a range of corrupted test sets [12, 26, 39],
for two of our case studies no corrupted datasets existed yet and we
thus propose novel corrupted datasets for them. These are described
in subsection 4.3.

4.1.3 Active Learning. As in the replicated paper, we split our test
sets into two parts, the active split, from which 20% of the nominal
dataset samples (or 10% of the OOD dataset samples, to obtain the
same absolute number of selected samples) are used for retraining,
and the eval split, which is used for final evaluation of the retrained
models. This process is illustrated in Figure 2. As opposed to the
original authors, we perform the active learning experiments not
only using OOD test sets, but also using nominal test sets. Moreover,
we also measure the cross-dataset improvements, i.e., how well a
nominal active split helps to improve performance on an OOD eval
split, and vice-versa. Lastly, our experiments further differ from the
ones conducted in the replicated in that we compare the TIP-based
training set selection against a random training set selection, to
measure the actual benefit gained by using a TIP.

4.2 Tested Approaches
We compare DeepGini against a total of 38 TIP, consisting of 24
different NC TIP, 10 surprise TIP and 4 uncertainty TIP. For what
concerns the NC TIP, we base our selection of ð‘˜ on the choices
made in the replicated paper, with the exception of KMNC: Here,
the authors used ð‘˜ = 1, 000 and ð‘˜ = 10, 000, but these values lead
to coverage profiles so large that the approaches did not terminate
in a reasonable amount of time in our setting, where multiple runs
are carried out to gain statistical significance. To be able to include
it, we used KMNC with ð‘˜ = 2, leading to a size of the coverage
profiles comparable to that obtained with the other NC metrics. We
test all our NC metrics using both CAM and CTM.

For what concerns SA, we chose the same number of segments
in the coverage profiles as the replicated paper (1,000), but we dy-
namically select the upper bound used to define these segments
based on the surprise adequacy values observed on the test dataset,
as we assume would be done in a practical setting. As thus every
test input has the exact same coverage (1/1000), using CTM would
be equivalent to random ordering. For SA, we thus consider CAM
on the surprise coverage profiles, but also the raw surprise adequa-
cies in decreasing order as a replacement for CTM, such that the
most surprising inputs would be selected first. As DeepGini is only

Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study)

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

applicable to classification problems, we use the per-class variants
of LSA, MLSA, MDSA and MMDSA.

For what concerns the uncertainty metrics, only MC-Dropout
has configurable parameters, and we use a very large number of
samples (200), aggregated using variation ratio, as recommended
in recent literature [42]. As opposed to the replicated paper, which
further narrowed down their selection of tested approaches after
collecting initial insights, we evaluate all TIP approaches for all
research questions.

4.3 Test Subjects

Mnist. A grey-scale image digit classification problem [19], repre-
senting the most commonly used dataset in the literature on testing
of machine learning based systems [34]. As corrupted data, we use
Mnist-c [26].

Fashion Mnist (Fmnist). A more challenging drop-in replacement
for Mnist, representing 10 different types of clothing [44]. To the
best of our knowledge, no corrupted version exists in the literature.
Thus, we created and plan to release (in case of paper acceptance)
Fashion-mnist-c, where images are corrupted using selected cor-
ruption methods from Mnist-c, as well as additional corruptions
targeting image orientation and rotations, since most nominal im-
ages are identically oriented. Examples of corruptions are: Various
types of noise (e.g. shot-noise), blurring (e.g. glass-blurring), trans-
formations (e.g. saturation, brightness), and orientation (e.g. flipping
right-left).

Cifar-10. A ten class color image classification problem [18]. As

corrupted data, we use Cifar10-c [12].

Imdb. Binary sentiment (positive / negative) classification of
textual Imdb reviews. For classification, we use a transformer model,
thus an architecture very different from the one used in the other
case studies. For the Imdb dataset to the best of our knowledge
no corrupted datasets are available from the literature. We thus
created and plan to release (in case of paper acceptance) a text
corruption approach, which corrupts text by mimicking wrong auto-
completions (replacing words with other words starting with the
same 3 letters), wrong auto-corrections (replacing words with other
words at small Levenshtein distance [20]), bad single-word level
translations (replacing words with a synonym based on Wordnet [6],
potentially ill-chosen given the context), and single-letter typos.
Our approach is configurable by severity level, to generate instances
of Imdb-c with corruptions of variable severity.

5 RESULTS
5.1 RQ1 Replication
5.1.1 Execution Time. The fact that DeepGini is much faster to
compute than both NC and DSC, LSC is obvious from the approach
descriptions in section 3, and our empirical results confirmed this
fact.

The results (time required to prioritize a full test set, averages
over the nominal and ood test sets) are shown in Table 1. Prioritizing
a mnist test set took less than 2 seconds on average for DeepGini
(which essentially reflects the time required for the DNN to make
the softmax predictions).

In the same setting, the NC-CTM methods required between
11 and 39 seconds, with an additional 2 to 12 seconds if CAM
was used. DSA and PC-LSA took much longer than even NC: For
example, for mnist, PC-LSA took 152s, DSA took more than 17
minutes (1060s). Clearly, PC-MDSA is the fastest SA metric, taking
only 60s to prioritize the same dataset. Given the simple nature of
the surprise coverage profiles (only 1,000 booleans, out of which
exactly one is true), the calculation of the surprise coverage CAM
variants took less than a second longer than their surprise adequacy
counterparts (a table with the detailed results is omitted for space
reasons, but is included in the replication package). Understandably,
our absolute measurements differ from the ones reported in the
replicated study [7], amongst other reasons as we use different
implementations, different hardware, different OOD test sets and
different models. However, for what concerns their primary finding,
i.e., DeepGini being the fastest approach, we come to the same
conclusion.

5.1.2 Test Prioritization. The results for the test prioritization ex-
periments are reported in Table 1. The approaches which were
originally compared against DeepGini, i.e., the NC variants, as well
as the DSC and PC-LSC variants of surprise (not reported in Ta-
ble 1 for lack of space, but just mildly inferior to DSA and PC-LSC,
respectively), perform worse than DeepGini on all case studies on
both nominal and OOD test datasets, confirming the primary find-
ing in the replicated paper [7]: First, with the expected APFD value
for random ordering being 50% (asymptotically, for a large number
of prioritized tests), we can see that DeepGini is clearly effective at
identifying misclassified DNN inputs, as itâ€™s lowest observed APFD
value is 68.9%. The NC metrics, in most cases, do not perform much
better, if better at all, than the random baseline in both their CTM
and CAM variants. Similarly, the CAM variants of the basic surpise
adequacies, PC-LSA-CAM and DSA-CAM, performed worse than
DeepGini, but better than most NC TIP, which is in line with the
findings in the replicated paper.

Here, it is worth mentioning that while the original experi-
ments [7] only measured the APFDs on the Mnist case study, their
primary finding that DeepGini outperforms all other approaches
w.r.t. APFD values generalizes to all our case studies, as well as to
the corruption based OOD datasets. For what concerns the latter,
as expected we find that adversarial examples used in the original
experiments [7] are much more favorable towards DeepGini than
the more realistic corruption based outliers that we used in our
study: The performance of DSA-CAM and (PC-)LSA-CAM on the
nominal Mnist test set is similar in both papers with respect to their
absolute APFD value (roughly 65%). However, DeepGini achieved
an APFD of >98% in the experiments with adversarial data [7], while
using the corruption based OOD data, the APFD performance of
DeepGini dropped by more than 10%.

5.1.3 Active Learning. The results for the active learning exper-
iments are reported in Table 2. Differently from Feng. et. al. [7],
we report the difference in accuracy to a random data selection
(RANDOM), to not only investigate which TIP performs best, but
also if and how much each TIP is effective in guiding active learning.
For comparison, the row "ORIGINAL" refers to the model accuracy
before active learning. For what concerns the dominance of Deep-
Gini to guide active learning, our results are less clear than the

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

Michael Weiss and Paolo Tonella

nominal

mnist
ood

time

nominal

fmnist
ood

time

nominal

cifar10
ood

imdb

time

nominal

ood

time

approach

e
g
a
r
e
v
o
c
n
o
r
u
e
n

e
s
i
r
p
r
u
s

NAC-0.75-CAM 43.83%
38.64%
NAC-0.75
56.89%
NBC-0-CAM
54.03%
NBC-0
51.61%
SNAC-0-CAM
51.30%
SNAC-0
49.75%
TKNC-1-CAM
40.96%
KMNC-2
92.48%
DSA
87.47%
PC-LSA
89.86%
PC-MDSA
89.59%
PC-MLSA
79.94%
PC-MMDSA
98.22%
98.22%
98.21%
98.20%
97.35%

t
n

y DeepGini
Vanilla SM
PCS
Entropy
MC-Dropout

i
a
t
r
e
c
n
u

17s
48.71%
11s
44.97%
53s
53.33%
43s
44.50%
43s
69.46%
37s
70.36%
42s
56.02%
29.76%
39s
69.73% 1060s
152s
65.13%
60s
68.64%
107s
69.16%
134s
66.28%
1s
87.80%
1s
88.09%
1s
88.61%
1s
87.27%
221s
90.59%

42.46%
41.25%
42.91%
38.69%
50.37%
49.71%
50.48%
49.52%
76.71%
60.99%
64.34%
64.06%
56.23%
86.35%
86.42%
86.28%
86.03%
83.71%

24s
45.80%
19s
45.35%
80s
48.79%
68s
45.48%
70s
59.45%
63s
59.63%
58s
53.74%
40.13%
64s
71.48% 1012s
148s
64.26%
59s
64.29%
93s
63.08%
121s
63.90%
2s
72.36%
2s
72.26%
2s
71.80%
2s
72.47%
392s
73.07%

48.88%
48.58%
49.59%
49.18%
50.36%
50.31%
50.13%
50.17%
61.17%
49.77%
56.13%
52.69%
51.39%
70.89%
70.81%
70.55%
70.97%
n.a.

72s
46.70%
62s
46.52%
200s
50.78%
183s
50.80%
179s
49.97%
174s
49.88%
109s
50.61%
50.34%
177s
57.93% 1143s
135s
47.28%
128s
52.98%
107s
50.73%
191s
48.90%
3s
68.90%
3s
68.79%
3s
68.47%
3s
69.03%
n.a.
n.a.

26.55%
26.53%
60.08%
60.09%
51.33%
51.32%
51.43%
51.41%
66.20%
55.83%
65.78%
55.95%
60.43%
73.52%
73.52%
73.52%
73.52%
60.38%

30.82%
20s
30.82%
19s
54.01%
34s
54.02%
34s
50.01%
34s
50.01%
34s
50.04%
20s
50.05%
33s
65.18%
43s
56.14%
29s
64.14%
3s
57.60%
5s
60.80%
12s
68.93%
3s
68.93%
3s
68.93%
3s
3s
68.93%
56.56% 364s

This table provides an overview of selected results. Find the full table, including 21 additional TIP, as CSV, in the reproduction package.
APFDs at most 0.05% worse than the best value, or times at most 1s higher than the fasted are highlighted gray.

Table 1: Test Prioritization APFDs (capability to detect misclassifications) and Runtimes

ones reported by Feng. et. al. [7]: For Mnist, Fmnist and Cifar10,
DeepGini outperforms all approaches considered by Feng. et. al. [7]
with only very view exceptions, but sometimes by a very narrow
margin. For Imdb, in particular when facing corrupted (OOD) in-
puts, we find that DeepGini is clearly outperformed by DSA and
PC-MDSA. This may be caused by the different nature of this case
study (text instead of image classification) or the correspondingly
different type of OOD data.

In all case studies, we found that NC techniques are sometimes
inferior to the random baseline. This showcases the importance to
compare each approach with a random baseline. Without it, all TIP
would have appeared successful at guiding active learning.

Clearly, our results are more ambiguous than the ones reported
in Feng. et. al. [7]. A primary root cause for this is our empirical
setup, representing a more realistic setting with less and diverse
outliers, which pose a much harder, but more realistic problem
for active learning. However, we still clearly observe DeepGiniâ€™s
capability to prioritize inputs for active learning, and doing so better
than neuron and surprise coverage in most cases, and we thus still
consider our results a successful confirmation of the superiority of
DeepGini reported in Feng. et. al. [7].

5.1.4 Confirmed findings from other papers. While not the pri-
mary objective of our experiments, our results confirm various
findings reported in recent research papers, in particular w.r.t. sur-
prise adequacy: First, we find that MDSA is indeed a valuable,
faster alternative to LSA, yielding comparable if not better results
in our experiments, which is in line with the results in the paper
proposing MDSA [16]. Second, the fact that DSA typically outper-
forms PC-LSA is consistent with the paper proposing these two

approaches [15]. Lastly, we also observed the previously reported
non-dominance between PCS and MC-Dropout [45].

Summary of RQ1 (Replication)

Our experimental results fully support the dominance of
DeepGini on execution times and test prioritization effec-
tiveness, and with only a few justified exceptions, active
learning capability. In addition, some of our results are
consistent with a range of observations made in related
papers [15, 16, 45].

5.2 RQ2 Comparison
Overall, we considered three additional types of TIP in our study,
which were not considered in the study by Feng. et. al. [7]: (1)
The use of raw surprise adequacy instead of surprise coverage,
(2) Recently proposed variants of surprise adequacy, specifically
PC-MDSA, PC-MLSA, PC-MMDSA, and (3) uncertainty quantifiers.
For what concerns surprise adequacy, our results (see Table 1 and
Table 2) show that using raw surprise adequacy instead of surprise
coverage is beneficial, which is expected: Test inputs with high
surprise adequacy are more novel and unexpected, and thus intu-
itively also more challenging to classify and more useful for active
learning. Furthermore, we find that as intended when proposed,
MDSA offers a faster replacement of LSA with similar prediction
performance, and that multimodal SA can indeed improve perfor-
mance over unimodal SA, as we can see when comparing MLSA
with LSA in Table 1 and Table 2. Despite all improvements, DSA
remains the best performing SA variant, while all SA variants are
in general inferior to DeepGini.

Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study)

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

mnist

fmnist

cifar10

imdb

nominal

ood

nominal

ood

nominal

ood

nominal

ood

approach

n

e ORIGINAL
RANDOM

i
l
e
s
a
b

99.15%
99.15%

93.68%
95.52%

90.15%
90.12%

72.17%
75.20%

69.16%
69.42%

64.74%
65.66%

83.72%
83.95%

80.28%
81.45%

Subsequent results are differences to the random baseline.

e
g
a
r
e
v
o
c
n
o
r
u
e
n

e
s
i
r
p
r
u
s

NAC-0.75-CAM
NAC-0.75
NBC-0-CAM
NBC-0
SNAC-0-CAM
SNAC-0
TKNC-1-CAM
KMNC-2
DSA
PC-LSA
PC-MDSA
PC-MLSA
PC-MMDSA

y DeepGini
Vanilla SM
PCS
Entropy
MC-Dropout

t
n
i
a
t
r
e
c
n
u

0.01%
-0.00%
0.01%
0.01%
0.02%
0.02%
0.02%
0.00%
0.04%
0.01%
0.03%
0.03%
0.02%
0.07%
0.04%
0.05%
0.05%
0.05%

0.03%
-0.86%
0.26%
-1.05%
0.09%
0.14%
0.32%
-1.87%
0.80%
0.11%
0.04%
0.17%
0.17%
0.89%
0.90%
1.02%
0.82%
1.22%

0.08%
0.05%
0.09%
0.03%
0.16%
0.11%
0.09%
0.08%
0.11%
0.07%
0.14%
0.13%
0.12%
0.12%
0.13%
0.14%
0.11%
0.15%

0.33%
-0.62%
0.71%
-0.93%
0.75%
0.58%
0.82%
-1.96%
1.41%
0.78%
0.64%
0.62%
0.74%
1.30%
1.44%
1.16%
1.37%
1.53%

0.14%
0.12%
-0.02%
-0.23%
-0.02%
0.09%
-0.10%
-0.01%
0.05%
-0.10%
-0.05%
-0.06%
0.05%
0.10%
0.04%
0.06%
-0.06%
n.a.

0.12%
-0.03%
0.09%
-0.15%
-0.01%
0.15%
0.08%
-0.02%
0.38%
0.23%
0.36%
0.12%
0.10%
0.57%
0.42%
0.40%
0.55%
n.a.

-0.39%
-0.26%
0.14%
0.15%
0.02%
-0.06%
-0.03%
0.04%
0.24%
0.23%
0.24%
0.21%
0.18%
0.17%
0.29%
0.22%
0.18%
0.20%

-0.64%
-0.78%
0.31%
0.28%
0.01%
0.02%
0.07%
-0.02%
0.48%
0.20%
0.47%
0.32%
0.37%
0.37%
0.38%
0.31%
0.38%
0.35%

This table provides an overview of selected results. Find the full table,
including 21 additional TIP, cross dataset and cross-split evaluations in the reproduction package.
Accuracies at most 0.01% worse than the best value are highlighted gray.

Table 2: Active Learning (capability to prioritize data for retraining)

On the other hand, we find that DeepGini is frequently, but not
always, outperformed by the other uncertainty metrics we added
to this study (see Table 1 and Table 2). While the fact that a so-
phisticated, theoretically well founded and very frequently used
MC-Dropout performs well is not so surprising, it is quite unex-
pected that even the most simple, naive baseline, Vanilla Softmax,
reaches similar performance as DeepGini. Indeed, as discussed in
RQ3, within the family of softmax-based uncertainty metrics, in-
cluding DeepGini, we do not find statistically significant differences
among the considered approaches.

Summary of RQ2 (Comparison to Other Approaches)

We found that raw surprise adequacy often outperforms
surprise coverage and that recently proposed innovations
such as Multi-Modal or Mahalanobis Distance based SA
are valuable alternatives to traditional LSA. Most interest-
ingly, however, we found that DeepGini does not in general
perform better than even the simplest baseline from uncer-
tainty quantification, Vanilla Softmax. Actually, it is often
outperformed by uncertainty quantification approaches.

5.3 RQ3 Statistical Analysis
We pairwise compare all 39 tested TIP to assess the significance
(using two-sided Wilcoxon signed-rank test) and effect size (using

the paired Vargha-Delaney method) in their difference. We applied
Bonferroni error correction to account for the multiple tests3.

Test Prioritization. Our statistical analysis regarding test prioriti-
zationâ€™s APFDs for the three best uncertainty quantification, sur-
prise adequacy and neuron coverage approaches is shown in Fig-
ure 3 (the values for all approaches will be available in the repli-
cation package). For what concerns the APFD values, our main
findings from RQ1 and RQ2 are confirmed: The softmax based
metrics DeepGini, Vanilla Softmax and Softmax Entropy outper-
form the other approaches with very low ð‘-value and very high
effect size. Moreover, the low effect size and high ð‘-value observed
within this family of approaches (lower right 3x3 corner in Figure 3)
strongly supports our finding that DeepGini is not in general ca-
pable of outperforming the simplest techniques from uncertainty
quantification.

Active Learning. When running a similar analysis for our active
learning experiments, we observe a similar heatmap, shown in
Figure 4 (note the different ranges in the two axes w.r.t. Figure 3).
Again, significance and effect size of the differences within the
uncertainty quantification family, which includes DeepGini, MC-
Dropout and Softmax Entropy, is low, but in this case we find that
this group extends to DSA, whose differences with respect to the
simple uncertainty quantifiers are also insignificant and of low
effect size.

3To allow easy comprehension of our results, we multiplied all p-values with the
Bonferroni correction factor: (cid:0)39
2

(cid:1) = 741.

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

Michael Weiss and Paolo Tonella

Figure 3: Statistical Analysis: Test Prioritization

Figure 4: Statistical Analysis: Active Learning

Active learning results have generally low effect sizes, indicating
that the performance of most approaches is very similar. This might
be due to the challenging, but more realistic, setting adopted in
our experimental design, where outliers are less frequent and more
realistic than the large amount of adversarial examples used in
the replicated paper. In our setting, active learning leads to just
marginal improvements and random influences during training
largely impact the final performance.

Summary of RQ3 (Statistical Analysis)

We can confirm the replicated paperâ€™s main finding, i.e.,
DeepGiniâ€™s dominance over SA and NC, with high signifi-
cance and large effect size. However, thereâ€™s no significant
difference and negligible effect size when comparing Deep-
Gini to simple baselines, such as Vanilla Softmax.

6 THREATS TO VALIDITY

Internal validity. When configuring the approaches being com-
pared, we made several choices on their parameters. Whenever
possible we remained consistent with the choices made in the pa-
per being replicated [7]. In other cases, we chose hyperparameters
compatible with the time and resources allocated to our empirical
comparison. E.g. KMNC-1000 and KMNC-10000 which were found
to be too computationally expensive by Feng. et. al. [7], were re-
placed with KMNC-2, in order to achieve similar coverage profile
size, and thus runtime, as most other NC approaches. In the other
cases, we document carefully and motivate our decisions in the
paper.

Conclusion validity. Differently from the paper being replicated [7],
we mitigate the conclusion validity threat associated with the non-
determinism of the training process by repeating our experiments
100 times, each time with a newly trained model. Thanks to such

repetitions, our findings are corroborated by a statistical analysis,
which discriminates differences due to chance from differences that
are statistically significant.

External validity. Our results are focused on the problem of test
input prioritization for early fault detection or active learning, so
they might not generalize to other problems that require test input
ordering or selection. In particular, while our experiments suggest
the use of simple softmax-based TIP, such as Vanilla Softmax, PCS,
Softmax-Entropy, but also DeepGini, they should not be blindly
trusted for arbitrary applications that deviate from the ones consid-
ered in this paper. In fact, when used as uncertainty metric, Vanilla
Softmax suffers from well known theoretical drawbacks [8, 13, 28].
Much of this critique revolves around Vanilla Softmaxâ€™s inability
to reliably quantify the absolute probability of a fault (values are
typically underestimated) and it being a point-predictor (i.e., non-
Bayesian) approach. It is also easily fooled by adversarial examples.
By replacing Svhn with Imdb as our fourth case study, we have
extended the external validity of the findings reported in the orig-
inal paper [7] to a quite different domain, natural language pro-
cessing for sentiment analysis. Still, the overall number of case
studies across both papers remains limited, which demands for
additional, similar work on different case studies which we support
by providing a comprehensive reproduction package.

7 ARTIFACTS
Alongside our paper, we release a set of artifacts in a way which
facilitates not only reproduction of our results, but also easy and reli-
able reuse of our code possibly for different purposes. To this extent,
we release four kinds of artifacts: (1) a full reproduction package,
(2) software libraries that we developed for core components of the
evaluated approaches, (3) datasets, and (4) all experimental results
as CSV tables. All our artifacts are released under a permissive MIT
license and archived on zenodo.org.

Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study)

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

Reproduction Package. All our code is available on a Github repos-
itory and through a pre-built Docker image. The latter exposes a
command line interface to easily re-run all or selected parts of our
experiments, without having to install all dependencies manually.
Alongside the full code, we release an archive of files containing
intermediate results of our experiments, such as trained models
and calculated priorities.
Image: docker pull ghcr.io/testingautomated-usi/simple-tip
Code & Docs: github.com/testingautomated-usi/simple-tip.

dnn-tip: DNN Test Prioritizer Software Library. We release core
components of our experiments, allowing to use and assess different
NC and SA metrics. These are likely to be useful also in other,
different studies (e.g., our implementations of the different NC
and SA approaches), being provided as an easy to use, standalone
Python library.

Install from pypi: pip install dnn-tip

Code & Docs: github.com/testingautomated-usi/dnn-tip.
We merged our implementation of DeepGini, which can be seen as
a variant of common uncertainty quantifiers, into uncertainty-
wizard, a library providing a wide range of uncertainty quanti-
fiers [42, 43].

Datasets. Corrupted datasets, such as Mnist-c [26], Cifar-10-
c [12] and Imagenet-C [12] have been shown to be useful in a wide
range of studies regarding robustness of DNNs, and we thus release
our Fmnist-c and Imdb-c datasets as standalone artifacts: Fmnist-c
can easily be installed through Huggingface datasets [21]:
Huggingface: mweiss/fashion_mnist_corrupted

Code & Docs: github.com/testingautomated-usi/fashion-mnist-c.
Instead of a precompiled dataset, Imdb-c is released as a python
library allowing to generate the dataset locally. This is due to copy-
right issues with the underlying Imdb dataset, but also to provide
additional features such as a specifiable level of corruption severity
or the support for arbitrary (non-Imdb) English text datasets.
Install from pypi: pip install corrupted-text

Code & Docs: github.com/testingautomated-usi/corrupted-text.

Experimental Results. For space reasons we were only able to
present a selection of aggregated results in this paper. The full set
of results, in machine readable formats, are released as part of our
reproduction package(see above).

8 CONCLUSION AND FUTURE WORK
In this paper, we studied the capabilities of a wide range of differ-
ent DNN test input prioritizers, along three main dimensions: (1)
their capability to give high priority to misclassified (faulty) inputs;
(2) their capability to identify inputs potentially useful for active
learning; (3) speed. As a positive result, we successfully replicated
the primary findings by Feng. et. al. [7], namely the very good
performance of DeepGini, using a a more extensive and thorough
experimental setup than the original paper. On the other hand, we
also found multiple alternative approaches from the uncertainty
quantification literature, including the simplest ones, to perform
comparably well as DeepGini. This finding not only strongly sug-
gests the use of uncertainty quantifiers as TIP for DNNs, it also
strongly emphasizes the need for future work to compare against

even the most naive baselines (e.g., Vanilla Softmax), and to show
the advantages of novel approaches both empirically and analyti-
cally over simple uncertainty quantifiers.

Our future research on DNN test input prioritization will focus
on the identification and quantification of secondary requirements
for TIP, beyond early fault detection and active learning, such as
diversity in the selected tests, or targeted prioritization aiming at
specific root causes for DNN faults.

ACKNOWLEDGMENTS
This work was partially supported by the H2020 project PRECRIME,
funded under the ERC Advanced Grant 2017 Program (ERC Grant
Agreement n. 787703).

REFERENCES
[1] [n. d.]. ACM Artifact Review and Badging â€“ Version 2.0. https://www.acm.org/

publications/policies/artifact-review-badging. Accessed: 2021-08-12.

[2] [n. d.]. ReScience C Whatâ€™s the difference between replication and reproduction?

http://rescience.github.io/faq/. Accessed: 2021-08-12.

[3] Janine Bennett, Ray Grout, Philippe PÃ©bay, Diana Roe, and David Thompson.
2009. Numerically stable, single-pass, parallel statistics algorithms. In 2009 IEEE
International Conference on Cluster Computing and Workshops. IEEE, 1â€“8.
[4] David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun
Zhao. 2020. Cats are not fish: Deep learning testing calls for out-of-distribution
awareness. In Proceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering. 1041â€“1052.

[5] Sebastian G. Elbaum, Alexey G. Malishevsky, and Gregg Rothermel. 2000. Pri-
oritizing test cases for regression testing. In Proceedings of the International
Symposium on Software Testing and Analysis, ISSTA. 102â€“112.

[6] Christiane Fellbaum. 2010. WordNet. In Theory and applications of ontology:

computer applications. Springer, 231â€“243.

[7] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu
Chen. 2020. Deepgini: prioritizing massive tests to enhance the robustness of
deep neural networks. In Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis. 177â€“188.

[8] Yarin Gal. 2016. Uncertainty in Deep Learning. Ph. D. Dissertation. University of

Cambridge.

[9] Yarin Gal and Zoubin Ghahramani. 2016. Dropout As a Bayesian Approximation:
Representing Model Uncertainty in Deep Learning. In Proceedings of the 33rd
International Conference on International Conference on Machine Learning - Volume
48 (New York, NY, USA) (ICMLâ€™16). JMLR.org, 1050â€“1059. http://dl.acm.org/
citation.cfm?id=3045390.3045502

[10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017. Deep bayesian active
learning with image data. In International Conference on Machine Learning. PMLR,
1183â€“1192.

[11] Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides, and Design Patterns.
1995. Elements of reusable object-oriented software. Vol. 99. Addison-Wesley
Reading, Massachusetts.

[12] Dan Hendrycks and Thomas Dietterich. 2018. Benchmarking Neural Network
Robustness to Common Corruptions and Perturbations. International Conference
on Learning Representations (2018). arXiv:1903.12261v1 [cs.LG]

[13] Dan Hendrycks and Kevin Gimpel. 2016. A Baseline for Detecting Mis-
(2016).

classified and Out-of-Distribution Examples in Neural Networks.
arXiv:1610.02136v3 [cs.NE]

[14] Andrej Karpathy. [n. d.]. CVPR Workshop on Autonomous Driving - Andrej Karpa-
thy details Teslaâ€™s Self Driving Car Strategy. Youtube Channel: Tesla Intelligence
UK. https://youtu.be/gZ2SeiLjaEc

[15] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In 2019 IEEE/ACM 41st International Conference
on Software Engineering (ICSE). IEEE, 1039â€“1049.

[16] Jinhan Kim, Jeongil Ju, Robert Feldt, and Shin Yoo. 2020. Reducing dnn labelling
cost using surprise adequacy: An industrial case study for autonomous driving.
In Proceedings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. 1466â€“
1476.

[17] S. Kim and S. Yoo. 2021. Multimodal Surprise Adequacy Analysis of Inputs
for Natural Language Processing DNN Models. In 2021 IEEE/ACM International
Conference on Automation of Software Test (AST). IEEE Computer Society, Los
Alamitos, CA, USA, 80â€“89. https://doi.org/10.1109/AST52587.2021.00017
[18] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.

Technical Report.

ISSTA â€™22, July 18â€“22, 2022, Virtual, South Korea

Michael Weiss and Paolo Tonella

[19] Yann LeCun, Corinna Cortes, and CJ Burges. 2010. MNIST handwritten digit
database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist 2 (2010).
[20] Vladimir I Levenshtein et al. 1966. Binary codes capable of correcting deletions,
insertions, and reversals. In Soviet physics doklady, Vol. 10. Soviet Union, 707â€“710.
[21] Quentin Lhoest, Albert Villanova del Moral, Patrick von Platen, Thomas Wolf,
Mario Å aÅ¡ko, Yacine Jernite, Abhishek Thakur, Lewis Tunstall, Suraj Patil,
Mariama Drame, Julien Chaumond, Julien Plu, Joe Davison, Simon Brandeis,
Victor Sanh, Teven Le Scao, Kevin Canwen Xu, Nicolas Patry, Steven Liu, An-
gelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Nathan Raw, Sylvain
Lesage, Anton Lozhkov, Matthew Carrigan, ThÃ©o MatussiÃ¨re, Leandro von
Werra, Lysandre Debut, Stas Bekman, and ClÃ©ment Delangue. 2021. Datasets:
A Community Library for Natural Language Processing. In Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations. Association for Computational Linguistics, 175â€“184. https:
//aclanthology.org/2021.emnlp-demo.21

[22] Jingjing Liang, Sebastian G. Elbaum, and Gregg Rothermel. 2018. Redefining
prioritization: continuous prioritization for continuous integration. In Proceedings
of the 40th International Conference on Software Engineering, ICSE. 688â€“698.
[23] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yang Chen, Ting Su, Li Li, Yang Liu, et al. 2018. Deepgauge: Multi-granularity
testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering. 120â€“131.

[24] Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, and Yves Le Traon.
2021. Test selection for deep learning systems. ACM Transactions on Software
Engineering and Methodology (TOSEM) 30, 2 (2021), 1â€“22.

[25] Prasanta Chandra Mahalanobis. 1936. On the generalized distance in statistics.

National Institute of Science of India.

[26] Norman Mu and Justin Gilmer. 2019. MNIST-C: A Robustness Benchmark for

Computer Vision. CoRR (2019). arXiv:1906.02337v1 [cs.CV]

[27] Andrew Ng. [n. d.]. Virtuous Circle of AI. Lecture Slides, CS299 - Deep Learning,
Stanford University. https://cs229.stanford.edu/materials/CS229-DeepLearning.
pdf. Accessed: 2021-13-12.

[28] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily
fooled: High confidence predictions for unrecognizable images. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 427â€“436.
[29] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin,
Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. Can you trust
your models uncertainty? Evaluating predictive uncertainty under dataset shift.
Advances in Neural Information Processing Systems (2019), 13991â€“14002.
[30] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825â€“2830.
[31] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-
tomated whitebox testing of deep learning systems. In proceedings of the 26th
Symposium on Operating Systems Principles. 1â€“18.

[32] Jonas Rauber, Wieland Brendel, and Matthias Bethge. 2017. Foolbox: A Python
toolbox to benchmark the robustness of machine learning models. In Reliable

Machine Learning in the Wild Workshop, 34th International Conference on Machine
Learning. http://arxiv.org/abs/1707.04131

[33] Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel.
2020. Foolbox Native: Fast adversarial attacks to benchmark the robustness of
machine learning models in PyTorch, TensorFlow, and JAX. Journal of Open
Source Software 5, 53 (2020), 2607. https://doi.org/10.21105/joss.02607

[34] Vincenzo Riccio, Gunel Jahangiroba, Andrea Stocco, Nargiz Humbatova, Michael
Weiss, and Paolo Tonella. 2020. Testing machine learning based systems: a
systematic mapping. Empirical Software Engineering (2020).

[35] Gregg Rothermel, Roland H. Untch, Chengyun Chu, and Mary Jean Harrold. 2001.
Prioritizing Test Cases For Regression Testing. IEEE Trans. Software Eng. 27, 10
(2001), 929â€“948.

[36] Erich Schubert and Michael Gertz. 2018. Numerically stable parallel computation
of (co-) variance. In Proceedings of the 30th International Conference on Scientific
and Statistical Database Management. 1â€“12.

[37] Yannis Smaragdakis. [n. d.]. ISSTA 2022 - Technical Track Call for Papers - Section
on Replicability Studies. https://conf.researchr.org/track/issta-2022/issta-2022-
technical-papers#Call-for-Papers. Accessed: 2021-15-12.

[38] Andrea Stocco and Paolo Tonella. 2020. Towards anomaly detectors that learn
continuously. In 2020 IEEE International Symposium on Software Reliability Engi-
neering Workshops (ISSREW). IEEE, 201â€“208.

[39] Andrea Stocco, Michael Weiss, Marco Calzana, and Paolo Tonella. 2020. Mis-
behaviour Prediction for Autonomous Driving Systems. In Proceedings of 42nd
International Conference on Software Engineering. ACM, 12 pages.

[40] Shitong Wang, Jun Wang, and Fu-lai Chung. 2014. Kernel Density Estimation,
IEEE Transactions on

Kernel Methods, and Fast Learning in Large Data Sets.
Cybernetics 44, 1 (2014), 1â€“20. https://doi.org/10.1109/TSMCB.2012.2236828
[41] Michael Weiss, Rwiddhi Chakraborty, and Paolo Tonella. 2021. A Review and
Refinement of Surprise Adequacy. In 2021 IEEE/ACM Third International Workshop
on Deep Learning for Testing and Testing for Deep Learning (DeepTest). IEEE
Computer Society, Los Alamitos, CA, USA, 17â€“24.
https://doi.org/10.1109/
DeepTest52559.2021.00009

[42] Michael Weiss and Paolo Tonella. 2021. Fail-safe execution of deep learning
based systems through uncertainty monitoring. In 2021 IEEE 14th International
Conference on Software Testing, Validation and Verification (ICST). IEEE. IEEE,
24â€“35.

[43] Michael Weiss and Paolo Tonella. 2021. Uncertainty-Wizard: Fast and User-
Friendly Neural Network Uncertainty Quantification. In 2021 14th IEEE Conference
on Software Testing, Verification and Validation (ICST). 436â€“441. https://doi.org/
10.1109/ICST49551.2021.00056

[44] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST: a Novel Image
Dataset for Benchmarking Machine Learning Algorithms. CoRR abs/1708.07747
(2017). arXiv:1708.07747 http://arxiv.org/abs/1708.07747

[45] Xiyue Zhang, Xiaofei Xie, Lei Ma, Xiaoning Du, Qiang Hu, Yang Liu, Jianjun
Zhao, and Meng Sun. 2020. Towards characterizing adversarial defects of deep
learning software from the lens of uncertainty. In Proceedings of 42nd International
Conference on Software Engineering. ACM.

