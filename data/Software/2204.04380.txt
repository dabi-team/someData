GigaScience, 2017, 1–7

doi: xx.xxxx/xxxx
Manuscript in Preparation
Paper

2
2
0
2

r
p
A
9

]

V
C
.
s
c
[

1
v
0
8
3
4
0
.
4
0
2
2
:
v
i
X
r
a

P A P E R

A dataset of ant colonies motion trajectories in indoor
and outdoor scenes for social cluster behavior study

Meihong Wu1,†, Xiaoyan Cao1,†, Xiaoyu Cao2 and Shihui Guo1,*

1School of Informatics, Xiamen University, Xiamen, 361000, China and 2Chemistry and Chemical Engineering,
Xiamen University, Xiamen, 361000, China

*guoshihui@xmu.edu.cn
†Contributed equally.

Abstract
Motion and interaction of social insects (such as ants) have been studied by many researchers to understand the clustering
mechanism. Most studies in the ﬁeld of ant behavior have only focused on indoor environments, while outdoor environments are
still underexplored. In this paper, we collect 10 videos of ant colonies from different indoor and outdoor scenes. And we develop an
image sequence marking software named VisualMarkData, which enables us to provide annotations of ants in the video. In all 5354
frames, the location information and the identiﬁcation number of each ant are recorded for a total of 712 ants and 114112
annotations. Moreover, we provide visual analysis tools to assess and validate the technical quality and reproducibility of our data.
It is hoped that this dataset will contribute to a deeper exploration on the behavior of the ant colony.

Key words: social insects; outdoor scenes; image sequence marking software

Background

Social insects often tend to cluster into a colony [1], which forms a
complex social network [2]. From time to time, the social network
springs up with self-organized clustering behaviors, including di-
vision of labor [3], task specialization [4], and distributed problem
solving [5]. Biologists analyze the evolution of social network to
understand the clustering behavior of insects [6], thus promoting
the development of relevant modern applications, such as wireless
communication [7] and cluster intelligent control [8]. The key re-
quirement of this research is the ability to track the motions and
interactions of each individual robustly and accurately.

Until the late 20th century, biologists still manually marked the
motion trajectories on the video to guarantee the quality. However,
they have to track each individual at one time, which might mean
watching the entire video 50 times or more in a crowded scene. [9]
Obviously, manually tracking is time-consuming and prone to hu-
man error. It becomes an inhibiting factor in obtaining the com-
plete and accurate dataset required to analyze the evolution of so-
cial networks. Therefore, in the past two decades, attempts have
been made to automate the tracking process for social insects uti-
lizing computer vision (CV) techniques [10, 11, 12, 13, 14].

Traditional CV techniques free researchers from manual
work through approaches such as foreground segmentation algo-
rithm [15], temporal difference method [10] and hungarian algo-
rithm [16]. Such approaches, however, have failed to address the
noise in the image [17], resulting in the limitation that a laboratory
environment with a clean background is needed. Nevertheless,
many scientiﬁcally valuable results are obtained in nature rather
than laboratory environment [18, 19, 20, 21].

Fortunately, with the emergence of deep learning, CV tech-
niques are already capable of addressing many complex tasks [22,
23, 24], which brings a piece of good news to automated insect
tracking in outdoor scenes. Several studies have explored auto-
mated multi-ant tracking in outdoor scenes using deep learning-
based models [25, 26]. Experimental results demonstrate that
these models could be scaled up into a cost-effective alternative
to traditional manual tracking methods which are typically costly
and/or labor intensive [25, 26]. A critical requirement for the devel-
opment of these models is access to the datasets containing motion
trajectories of insects in the video. To the best of our knowledge,
however, the current studies are all using only one outdoor-scene
dataset, which is lack of data diversity.

Compiled on: April 12, 2022.
Draft manuscript prepared by the author.

1

 
 
 
 
 
 
2 | GigaScience, 2017, Vol. 00, No. 0

Figure 1. The pipeline for marking motion trajectories of ants in an image sequence, taking an outdoor scene as an example. A total of 133 ants appear in this image sequence,
and we select one ant to be marked in each epoch. We use a square bounding box to point out the ant’s location and record the relevant parameters at the same time. After all
ants of the entire image sequence have been marked, we check the quality of the annotations frame by frame so that wrong annotations (red font) can be corrected (green
font). Then, we merge all the annotations of the image sequence into one ﬁle. Additionally, we provide three visualization tools to verify the data quality.

Sequence
Seq0001
Seq0002
Seq0003
Seq0004
Seq0005
Seq0006
Seq0007
Seq0008
Seq0009
Seq0010

FPS

Resolution

25

1920×1080

30

1280×720

Length
351 (00:14)
351 (00:14)
351 (00:14)
351 (00:14)
1001 (00:40)
600 (00:20)
677 (00:23)
577 (00:19)
526 (00:18)
569 (00:19)

Ants
10
10
10
10
10
73
162
133
193
101

Annotations
3510
3510
3510
3510
3510
11178
25158
10280
27902
22044

Table 1. Statistics of ant videos with annotations in indoor and outdoor scenes.

In this paper, we collect video recordings of 10 ant colonies from
different scenes (including indoor and outdoor scenes). Besides,
we develop an image sequence mark software named VisualMark-
Data, which is used to mark the pixel patches covered by ants in
each frame of the video. The total size of our dataset is 5354 frames,
712 ants, and 114112 labels. We believe that our dataset will beneﬁt
future research on social insect behavior analysis.

Data Description

We collect 10 videos that record activities of different ant colonies,
including colonies from both indoor and outdoor scenes. To help
us mark the motion trajectories, we develop an image sequence
marking software called VisualMarkData.

After spending a large quantity of time and effort, we obtain a
dataset with 5354 frames and 114112 annotations. Table 1 shows
the statistical information of our dataset.

Data acquisition

Indoor environment
We collect 50 workers of Japanese arched ants species, which
ranged from 7.4 to 13.8 mm in body length [27]. We construct a

laboratory environment for them, including a stable light source
and a transparent plastic container. We randomly divide them into
5 colonies of 10 ants each. Then, we load each colony into the
container in turns and ﬁlm their activities with a high-resolution
video camera.

Outdoor environment

We collect 5 videos of black ant colonies in different outdoor envi-
ronments, with the number of black ants ranging from 73 to 193
in the videos. The body length of black ants is between 8 to 10
mm [28]. These videos are provided by Depositphotos, an online
video site (https://cn.depositphotos.com/home.html), and access
is subject to a royalty-free license.

Data Records

The dataset consists of 10 image sequences from different scenes
in JPEG digital image format, which is publicly available on https:
//data.mendeley.com/datasets/9ws98g4npw/3. Alongside, we pro-
vide annotations marked by VisualMarkData for all image se-
quences in the form of text. In the dataset, the images and annota-
tions of each sequence are organized into three folders are named
’det’, ’gt’, and ’img’.

Meihong Wu et al.

| 3

Position
1
2
3
4
5
6

Name
Frame number
Identity number
Bounding box left
Bounding box top
Bounding box width
Bounding box height

7

Conﬁdence score

Description
Indicate at which frame the object is present
Each ant trajectory is identiﬁed by a unique ID (-1 for detections)
Coordinate of the top-left corner of the ant bounding box
Coordinate of the top-left corner of the ant bounding box
Width in pixels of the ant bounding box
Height in pixels of the ant bounding box
Indicates how conﬁdent the detector is that this instance is a ant.
For the ground truth and results, it acts as a ﬂag whether the entry is to be considered

Table 2. Data format for annotation ﬁles, both for ’det.txt’ and ’gt.txt’ ﬁles.

Figure 2. Visual analysis of the marking results on indoor ant videos. (a) Visualization of motion trajectories of the ants for each sequence of the indoor scene. (b) Speed
distributions in image space for 5 sequences of indoor scenes, respectively. (c) Ant speed histograms per indoor sequence.

Det folder

The ’det’ folder contains a ’det.txt’ ﬁle which is the ground truth
for detection, recording the location parameters of the ants in all
frames, which is similar to multi-object tracking challenge [29].
Each line represents one ant instance, and it contains 7 values as
shown in Table 2. The ﬁrst number indicates in which frame the
ant appears, while the second number identiﬁes that ant as belong-
ing to a trajectory by assigning a unique ID (set to -1 in a detection
ﬁle, as no ID is assigned yet). The next four numbers indicate the
location of the bounding box of the ant in 2D image coordinates.
The location is indicated by the top-left corner as well as the width
and height of the bounding box. This is followed by a single num-
ber, which denotes the conﬁdence score.

Gt folder

The ’gt’ folder contains a ’gt.txt’ ﬁle, which is the ground truth for
muti-object tracking. Similar to the ’det.txt’ ﬁle, it also contains
7 values (see details in Table 2). The difference compared to the
’det.txt’ ﬁle is that the second number in the ’gt.txt’ ﬁle records
the ID of an ant as belonging to a trajectory, which provides the key
information for implementing multi-ant tracking. Besides, each
ant can be assigned to only one trajectory.

Img folder

The ’img’ folder stores the original image sequence converted
from the video. All images are converted to JPEG and named se-
quentially to a 6-digit ﬁle name (e.g. 000001.jpg).

Analyses

Visually conﬁrm

The ground truth annotations for all image sequences in the
dataset are visually conﬁrmed by the data annotation staff. The
visual reviewing consists of two aspects, sequence-level (coarse-
grained) and image-level (ﬁne-grained).

Firstly, staffs perform a coarse-grained review of a single se-
quence. Speciﬁcally, we draw the annotations on the correspond-
ing images, and then convert the image sequence to video. By re-
playing the video (see movies in https://drive.google.com/file/
d/1ibXbBhbCN8Ald_8SGV-luRBYUizZf-Zl/view?usp=sharing), staff
can quickly conﬁrm which segments of the video are poor quality
and needed to be re-marked. For each scene, an example image
frame is shown in Figure 2 (a) and Figure 3 (a). After that, staff re-
views the quality of annotations frame-by-frame via VisualMark-
Data. For inaccurate annotations, staff modiﬁes manually by using
the "Check and modify" function of VisualMarkData (see details in
Methods).

Motion speed analysis

Further, to demonstrate the reliability of our dataset, we analyze
the distribution of the movement speed of the ants in our dataset.
First, for each ant, we use the 2D Euclidean distance [30] to cal-
culate its pixel distance between two adjacent frames. Therefore,
of the ant at frame t can be deﬁned by the
the pixel distance (cid:52)pst

4 | GigaScience, 2017, Vol. 00, No. 0

Figure 3. Visual analysis of the marking results on outdoor ant videos. (a) Visualization of motion trajectories of the ants for each sequence of the outdoor scene. (b) Speed
distributions in image space for 5 sequences of outdoor scenes, respectively. (c) Ant speed histograms per outdoor sequence.

following equation:

= (cid:112)(pxt

(cid:52)pst

– pxt–1)2 + (pyt

– pyt–1)2

(1)

where pxt

denotes the pixel position of the ant in the horizon-
tal direction at frame t. Similarly, pyt
denotes the pixel position in
the vertical direction. To convert the pixel distance to real-world
coordinates, we divide the ant’s body length L (unit: m) in the real
world by body length n (unit: pixel) in the image. Thus, the real-
world displacement of the ant at frame t, (cid:52)st
(unit: m) can be ex-
pressed as follows:

(cid:52)st

= (cid:52)pst × L/n

(2)

Since the FPS for a specifc video is a constant fc, the velocity vt

(unit: m · s

–1) at frame t can be formulated as:

vt

= (cid:52)st
1/fc

(3)

Where, the v0 is set to 0.

According to the aforementioned equations, combined with the
location information of ants in annotations, we can analyze the
motion speed of ants in the video, as shown in Figure 2 (b), (c)
and Figure 3 (b), (c). Speciﬁcally, the overall motion speed of ants
–1 and 1.98±1.84
in indoor and outdoor scenes are 2.16±1.49 cm · s
–1, respectively. These values are within a reasonable range
cm · s
–1 under bi-directional traf-
(ants average motion speed is 2.85 cm·s
ﬁc condition [31]). This demonstrates that the ant colony activity
dataset we collected and marked is real and reliable.

Discussion

The image sequence marking software VisualMarkData is a toolkit
with interactive visualization. The goal of the software is to pro-
vide a convenient tool for researchers marking movement trajec-
tories of social insects in videos, thus facilitating the study of the
behavioral mechanisms of social insects. Additionally, by using
the software, researchers will obtain standardized annotation data,
as details in the previous section. VisualMarkData is open source,
which enables researchers to mark their image sequence datasets
of any multi-object motion scenario. Alongside, we have provided
publicly available Python Scripts to illustrate the analysis of data as
well as usage of the data. To visualize and reproduce the results de-
scribed in the Technical Validation section, we develop two scripts

for the researchers. Also, we provide another script to calculate
metrics [29] of multi-object tracking that enables any deep learn-
ing algorithm to evaluate the tracking accuracy on our dataset. The
annotated trajectory data can be used for training and testing of su-
pervised learning models, thus providing a powerful tool for study-
ing a wider range of ant colony behaviors.

In the future, we will enrich the VisualMarkData with more fea-
tures to reduce the difﬁculty of marking and improve the efﬁciency
of marking. The software currently marks targets based on their
center points, and we are considering introducing stretchable an-
notation capabilities based on rectangles or ellipses. In addition,
the simultaneous annotation of multiple targets in one frame is
also a feature worth developing. Along with that, we can intro-
duce semi-automated annotation, i.e., embedding a neural net-
work model into the VisualMarkData, which will automatically
predict and annotate objects of the current frame based on the in-
formation in the previous frame. Thus, the annotators only need
to ﬁne-tune the annotation, which will signiﬁcantly improve the
efﬁciency of the annotation.

The dataset and VisualMarkData will boost researchers both in
biology and computer science to study on behavior of social insects
in different environments. We hope that this work will contribute
to the potential discovery of ant colony behavioral mechanisms
and facilitate the application of the image processing ﬁeld in biol-
ogy.

Potential implications

Swarm behavior is one of the most important features of social in-
sects, which has important signiﬁcance for the study of embod-
ied intelligence [8]. Speciﬁcally, social insects often tend to clus-
ter into a colony [1], which forms a complex dynamical system to-
gether with the surrounding environment [2]. So far, researchers
do not know enough about the mechanisms behind swarm behav-
iors of social insects. We believe our image sequence marking soft-
ware and dataset could facilitate the analysis of ant colony behavior
leading to the development of embodied intelligence.

Methods

Hardware devices for acquiring raw data

For indoor environments, we use a cylindrical container made of
transparent plastic providing a space for the ants to move around.

Meihong Wu et al.

| 5

Figure 4. The interactive interface of our image sequence marking software is named VisualMarkData. After selecting an image sequence, the user can acquire the annotation
by clicking on the center location of the ant’s body.

This container has a bottom diameter of 10 cm, a side height of
15 cm, and is not closed at the top. Ants, loaded in the container,
are ﬁlmed with a high-resolution video camera (Panasonic GX 85)
with 25 frames per second (FPS) in the format H.264 with a reso-
lution of 1920×1080 pixels. And the distance between the camera
and the top of the container is 30 cm so that the ﬁlming view of
the camera can cover the whole container. To ensure stable ﬁlm-
ing, we ﬁx the camera on a tripod, as well as hanging a light bulb
above the container. Besides, the anti-dusting powder is applied
to the inner wall of the container, preventing ants from escaping
from the container during the ﬁlming.

Description of the marking software VisualMarkData

We develop an image sequence marking software called Visual-
MarkData to provide the locations and identiﬁcation numbers of
objects in the sequence for motion analysis. The operation proce-
dure of VisualMarkData is as follows, and its interface is shown in
Figure 4.

• Choose Image Set. Before marking, the user should click
"Choose ImageSet" to select an image set. The ﬁlename of
the image set is deﬁned in the format of "SeqXObjectYIm-
ageZ", where X is the name of the sequence, Y is the num-
ber of objects in the ﬁrst frame and Z is the size of the bound-
ing box which represents the object. For example, the image
set, named "Seq0001Object10Image94", indicates that the se-
quence "0001" contains 10 objects in the ﬁrst frame, and each
object will be marked with a bounding box with the size of
94x94.

• Create Output Directory. The user needs to click "Output Di-
rectory" to select the storage path of annotations. Since Visual-
MarkData only focuses on one object per marking round (each
round goes through the whole image sequence), the output
folder is suggested to be named with the identiﬁcation number
of the object, e.g. "0001". As the identity number of the object
is user-deﬁned, the user can use any number for the object and

folder as long as it is unique.

• Select Start Frame. In the last step before starting marking,
you need to enter the start frame, the default value is 0. This
means that you are allowed to exit the software halfway and
continue the progress of the current marking task the next
time. Then, you can click the "Start" button.

• Marking. The user clicks on the center of the object in the cur-
rent frame, and the software will automatically save the digital
location of the center, as well as a bounding box centered on the
object. It should be emphasized that the user only marks the
same object until ﬁnishes the entire image sequence, and then
the user can focus on another object by repeating the same op-
eration for the previous one.

• Next Frame. The user clicks the "Next" button to show the
next frame on the window of the software. The marked location
on the previous frame will be displayed with a green-dotted,
which can help the user quickly locate the target object.

• Previous Frame. If the marked location of the previous frame is
incorrect, the user can click the "Previous" button to roll back
one frame.

• Check and Modify. After the user ﬁnishes marking the entire
image set, checking is needed to guarantee the quality. In this
case, the user can enter the speciﬁc frame to modify the anno-
tations by carrying out Select Start Frame step.

• Merge Annotations. After all objects in a sequence have been
marked and reviewed, the user needs to click the "Merge" but-
ton, thereby all annotations for each object will be sorted by
frames and then the ID of the object both in ascending order.

Availability of source code and requirements

Lists the following:

• Project name: ANTS_marking_and_analysis_tools
• Project home page: e.g. https://github.com/holmescao/ANTS_

marking_and_analysis_tools

• Operating system(s): Platform independent

6 | GigaScience, 2017, Vol. 00, No. 0

• Programming language: Python, MATLAB, Shell
• Other requirements: MATLAB R2021b (with Image Processing

Toolbox)

• License: MIT License

Availability of supporting data and materials

The data set supporting the results of this article is available in
the ANTS–ant detection and tracking repository at https://data.
mendeley.com/datasets/9ws98g4npw/3.

Declarations

List of abbreviations

CV: computer vision; ID: identity; FPS: frames per second

Consent for publication

Not applicable

Competing Interests

The authors declare that they have no competing interests.

Funding

This work was supported by the Natural Science Foundation of Fu-
jian Province of China (No. 2019J01002).

Author’s Contributions

M.W. and S.G conceived the experiment(s), X.C. conducted the ex-
periment(s), X.C. analyzed the results. All authors reviewed the
manuscript.

Acknowledgements

The authors thank the reviewers, for providing useful suggestions
for improvements and valuable feedback on the workﬂow and the
manuscript.

References

7. Motani M, Srinivasan V, Nuggehalli PS. Peoplenet: engineer-
ing a wireless virtual social network. In: Proceedings of the
11th annual international conference on Mobile computing and
networking; 2005. p. 243–257.

8. Tiacharoen S, Chatchanayuenyong T. Design and development
of an intelligent control by using bee colony optimization tech-
nique. American Journal of Applied Sciences 2012;9(9):1464.
9. Poff C, Nguyen H, Kang T, Shin MC. Efﬁcient tracking of ants in
long video with GPU and interaction. In: 2012 IEEE Workshop
on the Applications of Computer Vision (WACV) IEEE; 2012. p.
57–62.

10. Khan Z, Balch T, Dellaert F. MCMC-based particle ﬁltering
for tracking a variable number of interacting targets.
IEEE
transactions on pattern analysis and machine intelligence
2005;27(11):1805–1819.

11. Khan Z, Balch T, Dellaert F. MCMC data association and sparse
factorization updating for real time multitarget tracking with
merged and multiple measurements.
IEEE transactions on
pattern analysis and machine intelligence 2006;28(12):1960–
1972.

12. Oh SM, Rehg JM, Dellaert F. Parameterized duration mmod-
eling for switching linear dynamic systems.
In: 2006 IEEE
Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR’06), vol. 2 IEEE; 2006. p. 1694–1700.

Shape-and-
13. Veeraraghavan A, Chellappa R, Srinivasan M.
IEEE transactions
behavior encoded tracking of bee dances.
on pattern analysis and machine intelligence 2008;30(3):463–
476.

14. Fletcher M, Dornhaus A, Shin MC. Multiple ant tracking with
global foreground maximization and variable target proposal
distribution. In: 2011 IEEE Workshop on Applications of Com-
puter Vision (WACV) IEEE; 2011. p. 570–576.

15. Li M, Zhang Z, Huang K, Tan T. Estimating the number of peo-
ple in crowded scenes by mid based foreground segmentation
and head-shoulder detection. In: 2008 19th international con-
ference on pattern recognition IEEE; 2008. p. 1–4.

16. Li Y, Huang C, Nevatia R. Learning to associate: Hybridboosted
multi-target tracker for crowded scene. In: 2009 IEEE confer-
ence on computer vision and pattern recognition IEEE; 2009. p.
2953–2960.

17. Zhao M, Liu H, Wan Y. An improved Canny edge detection al-
gorithm based on DCT. In: 2015 IEEE International Conference
on Progress in Informatics and Computing (PIC) IEEE; 2015. p.
234–237.

18. Schmelzer E, Kastberger G.

‘Special agents’ trigger so-
cial waves in giant honeybees (Apis dorsata). Naturwis-
senschaften 2009;96(12):1431–1441.
19. Kastberger G, Weihmann F, Hoetzl T.

Social waves in gi-
ant honeybees (Apis dorsata) elicit nest vibrations. Naturwis-
senschaften 2013;100(7):595–609.

1. Vandermeer J, Perfecto I, Philpott SM. Clusters of ant colonies
and robust criticality in a tropical agroecosystem. Nature
2008;451(7177):457–459.

2. Balch T, Khan Z, Veloso M. Automatically tracking and analyz-
ing the behavior of live insect colonies. In: Proceedings of the
ﬁfth international conference on Autonomous agents; 2001. p.
521–528.

3. Hölldobler B, Wilson EO, et al. The ants. Harvard University

Press; 1990.

4. Whitehouse ME, Jaffe K. Ant wars: combat strategies, territory
and nest defence in the leaf-cutting antAtta laevigata. Animal
Behaviour 1996;51(6):1207–1217.

5. Vaughan RT, Støy K, Sukhatme GS, Matarić MJ. Whistling in
the dark: cooperative trail following in uncertain localization
space. In: Proceedings of the fourth international conference
on Autonomous agents; 2000. p. 187–194.

6. Fewell

JH.

Social

insect networks.

Science

2003;301(5641):1867–1870.

20. Tan K, Dong S, Li X, Liu X, Wang C, Li J, et al. Honey bee in-
hibitory signaling is tuned to threat severity and can act as a
colony alarm signal. PLoS biology 2016;14(3):e1002423.

21. Dong S, Wen P, Zhang Q, Wang Y, Cheng Y, Tan K, et al. Ol-
factory eavesdropping of predator alarm pheromone by sym-
patric but not allopatric prey. Animal Behaviour 2018;141:115–
125.

22. Schor N, Bechar A, Ignat T, Dombrovsky A, Elad Y, Berman
S. Robotic disease detection in greenhouses: Combined detec-
tion of powdery mildew and tomato spotted wilt virus. IEEE
Robotics and Automation Letters 2016;1(1):354–360.

23. Wang G, Li W, Zuluaga MA, Pratt R, Patel PA, Aertsen M, et al.
Interactive medical image segmentation using deep learning
with image-speciﬁc ﬁne tuning. IEEE transactions on medical
imaging 2018;37(7):1562–1573.

24. Wang C. Research and application of trafﬁc sign detection and
recognition based on deep learning.
In: 2018 International
Conference on Robots & Intelligent System (ICRIS) IEEE; 2018.

Meihong Wu et al.

| 7

p. 150–152.

25. Imirzian N, Zhang Y, Kurze C, Loreto RG, Chen DZ, Hughes DP.
Automated tracking and analysis of ant trajectories shows vari-
ation in forager exploration. Scientiﬁc reports 2019;9(1):1–10.
26. Cao X, Guo S, Lin J, Zhang W, Liao M. Online tracking of ants
based on deep association metrics: method, dataset and evalu-
ation. Pattern Recognition 2020;103:107233.

27. Terayama M, Ogata K. Two new species of the ant genus
Probolomyrmex (Hymenoptera, Formicidae) from Japan. Kon-
tyu 1988;56(3):590–594.

28. Ayieko MA, Kinyuru J, Ndong’a M, Kenji G. Nutritional value
and consumption of black ants (Carebara vidua Smith) from
the Lake Victoria region in Kenya. Advance Journal of Food Sci-
ence and Technology 2012;.

29. Leal-Taixé L, Milan A, Reid I, Roth S, Schindler K. Motchal-
lenge 2015: Towards a benchmark for multi-target tracking.
arXiv preprint arXiv:150401942 2015;.

30. Fabbri R, Costa LDF, Torelli JC, Bruno OM. 2D Euclidean dis-
tance transform algorithms: A comparative survey. ACM Com-
puting Surveys (CSUR) 2008;40(1):1–44.

31. Wang Q, Song W, Zhang J, Lo S. Bi-directional movement char-
acteristics of Camponotus japonicus ants during nest reloca-
tion. Journal of Experimental Biology 2018;221(18):jeb181669.

