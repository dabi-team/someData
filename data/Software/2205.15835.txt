Using Source Code Metrics for Predicting
Metamorphic Relations at Method Level

Alejandra Duque-Torres†, Dietmar Pfahl†, Claus Klammer‡, and Stefan Fischer‡
†Institute of Computer Science , University of Tartu, Tartu, Estonia
E-mail: {duquet, dietmar.pfahl}@ut.ee
‡Software Competence Center Hagenberg (SCCH) GmbH, Hagenberg, Austria
E-mail: {claus.klammer, stefan.ﬁscher}@scch.at

2
2
0
2

y
a
M
1
3

]
E
S
.
s
c
[

1
v
5
3
8
5
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—Metamorphic testing (TM) examines the relations
between inputs and outputs of test runs. These relations are
known as metamorphic relations (MR). Currently, MRs are
handpicked and require in-depth knowledge of the System Under
Test (SUT), as well as its problem domain. As a result, the
identiﬁcation and selection of high-quality MRs is a challenge.
Kanewala et al. suggested the Predicting Metamorphic Relations
(PMR) approach for automatic prediction of applicable MRs
picked from a predeﬁned list. PMR is based on a Support Vector
Machine (SVM) model using features derived from the Control
Flow Graphs (CFGs) of 100 Java methods. The original study
of Kanewala et al. showed encouraging results, but developing
classiﬁcation models from CFG-related features is costly. In this
paper, we aim at developing a PMR approach that is less costly
without losing performance. We complement the original PMR
approach by considering other than CFG-related features. We
deﬁne 21 features that can be directly extracted from source
code and build several classiﬁers, including SVM models. Our
results indicate that using the original CFG-based method-level
features,
in particular for a SVM with random walk kernel
(RWK), achieve better predictions in terms of AUC-ROC for
most of the candidate MRs than our models. However, for one
of the candidate MRs, using source code features achieved the
best AUC-ROC result (greater than 0.8).

Index Terms—Software testing, metamorphic testing, meta-

morphic relations, prediction modelling

I. INTRODUCTION

Metamorphic Testing (MT) is a software testing technique
that attempts to alleviate the test oracle problem [2]. A test
oracle is a mechanism for determining whether or not the
outcomes of a program are correct [3], [4]. The oracle problem
arises when the system under test (SUT) lacks an oracle or
when developing one to verify the computed results is practi-
cally impossible [4]. Instead of verifying the individual outputs
of the SUT, as traditional software testing techniques do, MT
examines the relations between the inputs and outputs of test
runs. These relations are known as Metamorphic Relations
(MRs). MRs deﬁne how outputs should vary in response to a
deﬁned change of inputs when executing the SUT [5], [6]. If
a particular MR is violated for at least one test input (and its
change), there is a high probability that the SUT has a fault.
On the other hand, if a particular MR is not violated it does not
guarantee that the SUT is free of ﬂaws. Thus, the effectiveness
of MT is greatly dependent on the appropriateness of the MRs
used [5].

Identifying and selecting appropriate MRs is not a triv-
ial task. It requires a deep understanding of the SUT and
its domain. Identifying and selecting high-quality MRs has
been noted as a signiﬁcant challenge. Several approaches to
determine how to choose “good” MRs have been proposed.
For instance, Liu et al. [7] introduced the Composition of
MRs (CMRs) technique for constructing new MRs by mixing
multiple existing ones. Zhang et al. [8] proposed a method
in which an algorithm searches for MRs expressed as linear
or quadratic equations. Chen et al. [9] developed METRIC,
a speciﬁcation-based technique and related tool for identi-
fying MRs based on the category-choice framework. They
also expanded METRIC into METRIC+ by integrating the
information acquired from the output domain [10]. Among
those approaches, Predicting Metamorphic Relations (PMR),
introduced by Kanewala et al. [1], [11], uses machine learning
(ML) techniques to automatically detect likely MR of program
methods using features extracted from a method’s control-ﬂow
graph (CFG).

The idea behind the PMR approach is to build a model
that predicts whether a method in a newly developed SUT
can be tested using a speciﬁc MR. The key part of PMR
is feature design. In the original PMR work [1], Kanewala
et al. used the CFG’s path- and node-based features of 48
Java methods and three predeﬁned MRs to train support vector
machines (SVM) and decision trees models. The authors show
encouraging results when using node- and path-based features
and SVM. Then, Kanewala et al. [11] extends their initial work
by examining 100 Java methods and a set of six predeﬁned
MRs. As in the initial study, Kanewala et al. use SVMs,
and features extracted from the methods’ CFGs. However,
instead of the node- and path-based features, the authors used
measures of similarity between graphs. In particular, random
walk kernel (RWK) and graphlet kernel (GK). Kanewala et al.
concluded that SVM models trained with RWK-based features
performed better than those trained with GK and with node-
and path-based features (using a default linear kernel).

While the original study by Kanewala et al. [1], [11] showed
encouraging results, the design of PMR features was limited
to features extracted from the methods’ CFGs only. First
generating CFGs from the source code and then extracting
features is relatively costly as compared to extracting features
directly from the source code. To see whether and how the

 
 
 
 
 
 
PMR approach could be improved by using features extracted
directly from source code, as well as using those features
with other classiﬁcation approaches than SVM, we decided to
conduct a study using the same set of methods and the same set
of MRs as in Kanewala et al. [11]. We complement the original
PMR approach by looking at 21 features directly extracted
from the source code. In addition, we experiment with ﬁve
different binary classiﬁcation models (including SVM).

In the context of our study, we answer the following

research questions:

• RQ1: What set of source code based features provides

the best PMR performance?

• RQ2: Does PMR performance improve when using
source code based features instead of CFG-based fea-
tures?

In our study, we focus on PMR feature engineering. In
particular, we are interested in understanding whether and how
it is possible to achieve a similar or better performance than
that obtained by Kanewala et al. [1], [11], but using features
extracted from source code rather than features extracted from
the CFG.

The rest of the paper is structured as follows. Section II
presents the related work. In Section III, we describe the
methodology. In Section IV, we present results and answers
to our research questions. We discuss some threats to validity
in Section V. Finally, we conclude the paper in Section VI.

II. RELATED WORK
Since MT was introduced in 1998 by Chen et al. [2], it has
been widely studied with increasing interest in recent years
[12]. Several studies have shown MT as a strong technique
for testing the “non-testable programs” where an oracle is
unavailable or too difﬁcult
to implement [12]–[15]. Also,
MT has been demonstrated to be an effective technique for
testing in a variety of application domains, e.g., autonomous
driving [16], [17], cloud and networking systems [18], [19],
bioinformatic software [20], [21], scientiﬁc software [22], [23].
However, the efﬁcacy of MT heavily relies on the speciﬁc MRs
employed.

Kanewala et al. [1], were the ﬁrst to show that, for previ-
ously unseen methods, applicable MRs can be predicted using
ML techniques. Their work showed that classiﬁcation models
created using a set of features extracted from CFGs and a set of
predeﬁned MRs are effective in predicting whether a method
in a newly developed SUT can be tested using a speciﬁc MR
taken from the pre-deﬁned set. Then, they extend their ﬁrst
work [1] by conducting a feature analysis to identify the most
effective CFG’s related features for predicting MRs [11]. Their
results showed that SVM models built with features based
on CFG similarity measurements, in particular using RWK,
perform better than SVM models using nodes- and paths-based
features with linear kernel.

Hardin et al. [24] extended the initial PMR study [1] using
semi-supervised learning techniques on a set of node-based
features and the CFG path tagged with six predeﬁned MRs.
Rahman et al. [25] applied PMR approach for predicting three

Fig. 1: PMR procedure

the RWK can effectively predict

high-level categories of MRs (i.e., Permutative, Additive, and
Multiplicative) for matrix-based programs. Their results show
these MRs. Nair et
that
al. [26] explored and compared equivalent and non-equivalent
mutants as data augmentation technique to broaden the training
set using PMR. Their augmentation approach was tested on the
PMR original study dataset [1]. The study demonstrated that
equivalent mutants are a valid data augmentation technique to
improve the PMR detection rate. Zhang et al. [27] presented
RBF-MLMR, a multi-label technique that predicts MRs using
radial basis function neural networks. Instead of using several
binary classiﬁers like in PMR, RBF-MLMR use a neural
network to predict all potential MRs for a given method.
The major difference between this technique and PMR is the
usage of multi-label and neural networks, but it follows the
same pipeline as PMR original study. Also, the RBF-MLMR’s
feature design is CFG’s node- and path-based.

III. METHODOLOGY

In this paper, we focus on automated MR identiﬁcation
following the PMR approach but using the method’s source
code metrics instead of its CFG information. Answers to the
research questions are obtained from the analysis of the results
of several implementations of the PMR procedure. Therefore,
in this section, we ﬁrst present the PMR procedure, which is
slightly different from the one proposed by Kanewala et al.
[1], [11] (Section III-A). Then, we present the labelled dataset
used in our study (Section III-B). Finally, we present the
performance measures used in this paper study (Section III-C)

A. PMR Procedure

Figure 1 shows the PMR procedure. The PMR procedure
consists of three phases. Phase I is responsible for extracting
metrics of the method source code. The output of this phase
is a csv ﬁle with 21 source code metrics related features per
method. Phase II is in charge of preparing the data according

Method_nVn,cVn,c...Vn,c1...11Ft2Ft1...FtcMR1...MR2MRcMethod_1V12V11...V1c0...11Method_2V22V21...V2c1...10...........................Labelled	datasetMethod_1Phase	I	-	Feature	extractionStep	2.1MR LabellingStep	2.2Phase	II	-	Data	preparationTesting dataStep	3.1Training dataData split Step	3.2ModelCreationStep	3.3PerformanceevaluationPhase	III	-	Training	and	testingML model setSCminerMethod_nVn,cVn,c...Vn,cFt2Ft1...FtcMethod_1V12V11...V1cMethod_2V22V21...V2c...............EncodingMethod_nVn,cVn,c...Vn,cFt2Ft1...FtcMethod_1V12V11...V1cMethod_2V22V21...V2c...............Method_nVn,cVn,c...Vn,c1...11Ft2Ft1...FtcMR1...MR2MRcMethod_1V12V11...V1c0...11Method_2V22V21...V2c1...10...........................Labelled	datasetto the requirements of the ML algorithms. For example,
encoding categorical features. Also, each method is labelled
with elements from the set of pre-deﬁned MRs. Phase III is
in charge of training and evaluating the binary classiﬁcation
models that predict whether a speciﬁc MR is applicable to
the unit testing of a speciﬁc method. Below we describe each
phase in detail.

1) Phase I - Feature extraction: in this phase, a list of
features from various source code metrics is retrieved for each
method. A source code metric is a quantitative measure of
a software system’s attribute. Measuring software complex-
ity [28], assessing software maintainability [29], measuring
software quality [30], and other applications rely on source
code metrics. Recent studies have demonstrated the usefulness
of employing source code metrics in a variety of research
areas, including defect prediction [31], and time cost reduction
in mutation testing [32]–[34]. In our implementation, a total
of 21 source code metrics are used, as shown in Table I.
We use SCminer1, which is an open-source tool for mining
source code metrics at method level that support three different
programming languages (Java, C++, and Python).

TABLE I: Software metrics

Description

Total number of lines of code
Total number of code lines without blank lines
Total number of code lines without comments or
libraries statements
Total number of code lines without blank lines,
comments or libraries statements
Total number of lines with code statements
Token is the word and operators, etc.
First code line
Last code line
It provides the full input parameters
including the variable name
Number of inputs arguments
Inputs data type, e.g., int array, int, ﬂoat, etc.
Number of arithmetical operators
Number of operands, e.g., variables,
numeric and string constants
Total number of variables declared
Number of loops
Cyclomatic Complexity Number, which is the number
of possible alternative paths through a piece of code
Number of external methods called
Tells if the method has a return value
Tells how many return statements the method has
Return data type, e.g., int array, int, ﬂoat, etc.
extention ﬁle (Java, C++, or Python)

Metric

tloc
sloc whbl

nloc

nloc whbl

sloc statements
token count
start line
end line

full parameters

numArg
dataArg
numOper

numOperands

total Var
numLoops

CCN

numMethCall
has return
totalReturn
returnDataType
ext

2) Phase II – Data preparation: This phase is made up of

two steps:

Step 2.1 – Encoding: This step is in charge of preparing the
data according to the requirements of the ML algorithms. For
instance, encoding categorical features. Also, a further feature
selection analysis can be done. Feature selection is an extra
activity that allows exploring the different performances of
ML models when different features are used.

Step 2.2 – Labelling MRs: Since PMR employs supervised
learning classiﬁcation techniques, which need the usage of a
labelled dataset to give instances for learning. After the feature
extraction phase and encoding step, the training dataset is
constructed by manually labelling each method with suitable
MRs. Depending on whether a certain MR does or does not
satisfy the method, the method is labelled with a 1 or a 0 for
such MR.

3) Phase III – Training and testing: This step entails
extracting information from data using one or more supervised
ML algorithms, or a combination of them. There are three
steps that must be taken.

Step 3.1 – Data split: This step is responsible for splitting
the dataset into two subsets: a training set and a test set. The
training set is used to create the prediction model, while the
test set is used to evaluate the performance of the created
prediction model.

Step 3.2 – Model creation refers to the process of building
prediction models. Choosing a good modelling technique is vi-
tal for the training and prediction stage in any ML application,
including the PMR approach. In this paper, a total of 5 popular
classiﬁcation algorithms are investigated when applying them
to PMR, including:

• Random Forest (RF)
• Support Vector Machine (SVM) with linear kernel
• Decision Trees (DT)
• Gaussian Naıve Bayes (GNB)
• Logistic Regression (LR)
Step 3.3 – Performance evaluation: This step measures
the performance of the created prediction models. To assess
classiﬁcation performance, we utilised stratiﬁed k–fold cross-
validation. The k–fold crossvalidation approach assesses how
well a prediction model performs on previously unknown data.
The data set is randomly partitioned into k subgroups in k–
fold cross-validation. Then, k–1 subsets are utilised to develop
the predictive model (training), and the remaining subset is
used to assess the predictive model’s performance (testing).
This procedure is performed k times, with each of the k
subsets being used to assess performance. In stratiﬁed k–fold
cross-validation, k–folds are partitioned so that they include
roughly the same percentage of positive (functions that display
a speciﬁc MR) and negative (functions that do not exhibit a
speciﬁc metamorphic relation) samples as the original data set.

B. Dataset and pre-deﬁned set of MRs

Kanewala et al. [11] provide a dataset with 100 Java
methods in their CFG representation2. Instead of using the
CFGs, we built a code corpus containing the source code
of the same 100 Java methods. The methods are from the
open-source libraries Colt Project [35], an open-source library
written for high-performance scientiﬁc and technical comput-
ing, Apache Mahou [36], a machine learning library, Apache
Commons Mathematics [37], a library of mathematics and
statistics components, and Java Collections [38], a framework

1https://github.com/aduquet/SCminer

2http://www.cs.colostate.edu/saxs/MRpred/functions

that provides an architecture to store and manipulate the group
of objects. All of these libraries are written in Java. To obtain
the source code of the methods presented in the form of CFGs
by Kanewala et al., we search in the different libraries for the
name of the method.

To be able to make a fair comparison, we use the same set
of six pre-deﬁned MRs as in the original study [11]. Table II
lists the MRs used, the changes in the inputs and the expected
outputs, and the total number of methods to which a speciﬁc
MR applies. Details of the set of methods, i.e., method name,
library to which it belongs, and the MRs that apply, have been
made available in github3.

TABLE II: MRs used and total number of methods that match
((cid:88)) a speciﬁc MR

MR

Change in the input

Output expected

Add a positive constant
Remove an element
Add a new element

ADD
EXC
INC
MUL Multiply by a positive constant
PER
INV
ADD: Additive, MUL: Multiplicative, PER: Permutative
INC: Inclusive, EXC: Exclusive, INV: Invertive

Permute the components
Take the inverse of each element

Increase or remain constant
Decrease or remain constant
Increase or remain constant
Increase or remain constant
Remain constant
Decrease or remain constant

(cid:88)

56
32
34
66
33
63

C. Performance Measures

We evaluate the performance of our PMR implementation
using prediction recall, accuracy, precision, F1-score, and
AUC-ROC for each subject using 10-fold cross validation and
then summarise (using the mean) those statistics to compare
the performance of different classiﬁcation algorithms.

In this paper, we denote a classiﬁcation output in which a
speciﬁc M Rn satisﬁes the method m as the positive class and
a classiﬁcation output in which speciﬁc M Rn does not satisfy
the method m as the negative class. Using this notation, each
standard performance measure is expressed as a function of
the counts of elements in the Confusion Matrix deﬁned as
follows (denote TP as true positive, TN as true negative, FP
as false positive, FN as false negative):

• recall = TP / (TP + FN)
• accuracy = (TP + TN) / (TP + FP + TN + FN)
• precision = TP / (TP + FP)
• F1-score is the harmonic mean of precision and recall
• AUC-ROC, area under the receiver operating character-

istic (ROC) curve

IV. RESULTS

The full set of data generated during our experiments as

well as all scripts can be found in our GitHub repo4.

A. RQ1: What set of source code based features provides the
best PMR performance?

As a ﬁrst step to answering RQ1, we explore whether using
a subset of the maximum set of 21 features helps improve the
performance of each MR classiﬁer. To do this, an RF classiﬁer

3Link to methods description
4VST22 PMR-SourceCodeMetrics-E536/

Fig. 2: Feature importance score per MR

is used to calculate the importance score of each feature. Note
that, taking randomness into account, we built RF classiﬁers
ten times and then averaged the importance score for each
feature. Figure 2 shows the feature importance score per MR
and Table III lists the absolute importance values per MR.
In both, Figure 2 and Table III, the features are ranked from
highest to lowest score.

From Figure 2 and Table III one can see that regardless of
the MR, the importance ranking of features follows a fairly
uniform pattern. It seems that from the third-ranked feature,
i.e., “C: tloc”, regardless of the MR, features are ranked in
the same order. Moreover, for the last three features, i.e., “S:
returnData Type”, “T: ext”, and “U: full-Parameters”, the score
is zero for all MRs. Also, it is important to highlight that from
the fourth feature for the MRs: MUL, PER, and INV, their
scores are quite close to each other for the same features. This
happens also for the MRs: ADD, EXC, and INC, but from the
eleventh feature, i.e., “L: numLoop”, onward. Regarding the
top-3 features with the best scores, the MRs: EXC, INC, MUL
and INV follow the same pattern concerning the order of the
feature importance score, this is “A: dataArg”, “B: CCN” and
“C: tloc”. In MRs: ADD and PER, the order of importance
differ because “B: CCN” is the highest score instead of “A:
dataArg”. It could be said that this is the only exception to
the general ranking pattern we found.

After ranking the features from highest to lowest according
to their importance score, we explored the gradual selection of
best-ranked features. Also, we attempted to discover the best
subset in the reduced subject set using 10-fold cross-validation
and RF classiﬁer. We started with a subset with the three best-
ranked features; then we augment the subset with the following
three features, and so on. In total, we evaluated seven subsets
for each MR. Each time, the next three highest-ranked features
were added until reaching the total set of 21 features.

Table IV shows the performance values obtained using
the RF classiﬁer in terms of AUC-ROC and Precision when
the classiﬁer is trained with a different feature subsets. In
this analysis, we want to know which subset of features is

ADDEXCINCMULPERINVMetamorphicrelation0.00.20.40.60.81.0FeatureimportanceA:dataArgB:CCNC:tlocD:sloc-whblE:sloc-statementsF:nloc-whblG:nlocH:token-countI:start-lineJ:end-lineK:numArgL:numLoopsM:totalVarN:numOperO:numMethCallP:hasReturnQ:totalReturnR:numOperandsS:returnDataTypeT:extU:full-ParametersTABLE III: Feature importance score absolute values

MR

A

ADD
EXC
INC
MUL
PER
INV

0.92
1.00
1.00
1.00
0.57
1.00

B

1.00
0.86
0.81
0.72
1.00
0.68

C

0.71
0.62
0.53
0.67
0.54
0.56

D

0.40
0.54
0.48
0.41
0.41
0.41

E

0.38
0.48
0.46
0.33
0.38
0.35

F

0.31
0.48
0.45
0.32
0.36
0.32

G

0.30
0.46
0.44
0.30
0.24
0.31

H

0.30
0.45
0.43
0.28
0.23
0.30

I

0.29
0.45
0.43
0.27
0.23
0.29

J

0.29
0.43
0.42
0.26
0.22
0.28

k

0.28
0.42
0.41
0.26
0.22
0.27

L

0.28
0.30
0.33
0.25
0.21
0.26

M

0.27
0.28
0.28
0.25
0.20
0.25

N

0.26
0.26
0.25
0.24
0.19
0.22

O

0.24
0.24
0.22
0.23
0.18
0.21

P

0.23
0.24
0.21
0.23
0.15
0.20

Q

0.22
0.22
0.20
0.21
0.14
0.19

R

0.21
0.21
0.16
0.15
0.09
0.18

S

0.00
0.00
0.00
0.00
0.00
0.00

T

0.00
0.00
0.00
0.00
0.00
0.00

U

0.00
0.00
0.00
0.00
0.00
0.00

0.85

0.91

AVG
0.00
AVG: Average, A: dataArg, B: CCN, C: tloc, D: sloc-whbl, E: sloc-statements, F: nloc-whbl, G: nloc, H: token-count, I: start-line, J: end-line, K: numArg
L: numLoops, M: totalVar, N: numOper, O: numMethCall, P: hasReturn, Q: totalReturn, R: numOperands, S: returnDataType, T: ext, U: full-Parameters

0.60

0.34

0.37

0.32

0.40

0.33

0.44

0.17

0.22

0.31

0.20

0.24

0.27

0.25

0.21

0.33

0.00

0.00

TABLE IV: AUC-ROC and precision absolute values per MR
using the top-ranked n features in a RF classiﬁer

Metric MR

AUC(cid:63)

ADD
EXC
INC
MUL
PER
INV

Feat3
0.620
0.629
0.720
0.649
0.763
0.595

Feat6
0.590
0.593
0.675
0.518
0.725
0.611

Top-ranked n features
Feat15
0.620
0.589
0.675
0.518
0.725
0.636

Feat12
0.886
0.833
0.717
0.762
0.747
0.625

Feat9
0.620
0.583
0.720
0.518
0.725
0.588

Prec±

ADD
EXC
INC
MUL
PER
INV

0.767
0.866
0.888
0.853
0.814
0.833
Feat: Feature, (cid:63)AUC: AUC-ROC, ±Prec: Precision

0.613
0.651
0.733
0.833
0.761
0.625

0.627
0.667
0.767
0.875
0.625
0.675

0.640
0.693
0.733
0.854
0.805
0.714

0.740
0.667
0.727
0.675
0.625
0.75

Feat18
0.590
0.623
0.675
0.491
0.641
0.545

0.729
0.688
0.761
0.630
0.625
0.600

Feat21
0.590
0.666
0.694
0.578
0.747
0.640

0.769
0.614
0.652
0.657
0.625
0.675

the best based on the AUC-ROC and Precision performance
measures. AUC-ROC indicates the extent to which the model
can distinguish between classes. The higher the AUC-ROC,
the better the model will predict the positive class as positive
and the negative class as negative. In our context, the positive
case is the one when an MR applies to a method. Since we
plan to use the PMR approach for generating initial test cases
for methods that are yet lacking tests, it is more important that
we avoid the occurrence false positives. A false positive would
result in generating tests based on a MR that actually is not
applicable. The performance measure Precision is commonly
used to assess the capability of a binary classiﬁer to predict
the positive case correctly. Therefore, we are particularly
interested in this measure when comparing the performance
of the various classiﬁers we build.

With regards to AUC-ROC, Table IV shows that three out
of six MRs, i.e., ADD, EXC, and MUL, have the best results
with the top-ranked 12 features, with a difference greater
than 0.2 to all other feature sets. For MRs INC and PER,
the highest values for AUC-ROC are achieved when the 3
top-ranked features are used. However, the difference to the
performance using 12 top-ranked features is less than about
0.01. For MR INV, the best AUC-ROC score is when the
complete set of features is used. However, as for INC and
PER, the difference with the 12 top-ranked features is rather
small (0.015). Therefore, based on the AUC-ROC measure,
it seems to be reasonable to simply use the 12 top-ranked
features across the board for all MRs.

With regards to Precision, Table IV shows that for four out
of six MRs, i.e., EXC, INC, PER and INV, the best perfor-

mance is achieved with the 12 top-ranked features. The highest
precision value for the MR ADD is obtained when all 21
features are used. However, the difference between 21 features
and 12 top-ranked features is only 0.002, approximately. This
indicates that performance-based on precision does not vary
signiﬁcantly if the top 12 features are used.

In the next step, we explore the PMR approach using four
different classiﬁcation models in addition to RF,
i.e., DT,
GNB, SVM and LG. Each classiﬁer is trained with the top-
ranked 3 and 12 features, and the full set of 21 features. We
include the sets of 3 and 21 features in the analysis to double-
check whether the choice of 12-features is also the best for
other classiﬁers than RF. We evaluate the performance of each
classiﬁer using 10-fold cross-validation. Speciﬁcally, 70% of
the instances’ datasets are used for training in each fold,
and the remaining 30% are used for testing. The prediction
statistics (accuracy, precision, recall, F1 score, and AUC-
ROC) are recorded for each fold. The average values of these
statistics for each classiﬁer and each MR are listed in Table V.
As expected, Table V conﬁrms that using the 12 top-ranked
features is almost always the best choice across the board
(best performance is printed in bold). In those cases where
12 features don’t yield the best performance, the difference to
the best performing feature set is alway less than 0.02.

To decide which classiﬁer is the best for each MR, we
relied on the values of AUC-ROC and Precision when using
classiﬁers based on 12 features.

For MR ADD,

the highest average of AUC-ROC and
Precision is achieved when using GNB (average of 0.875 and
0.914).

For MR EXC, the highest average of AUC-ROC and Preci-
sion is achieved when using RF (average of 0.833 and 0.866).
For MR INC, the highest average of AUC-ROC and Preci-
sion is achieved when using RF (average of 0.717 and 0.888).
the highest average of AUC-ROC and
Precision is achieved when using LG (average of 0.833 and
0.875).

For MR MUL,

For MR PER, the highest average of AUC-ROC and Preci-
sion is achieved when using DT (average of 0.857 and 0.914).
For MR INV, the highest average of AUC-ROC and Preci-
sion is achieved when using SVM with linear kernel (average
of 0.857 and 0.844).

In summary, the results show that there is not one best
classiﬁer. RF is best for two MRs and each of the other
classiﬁers is best for exactly one MR.

TABLE V: PMR performance metrics when using 3, 12, and 21 top-ranked features on RF, DT, GNB, SVM, and LG classiﬁers

MR

Clas(cid:63)

ADD

EXC

INC

MUL

PER

INV

RF
DT
GNB
SVM
LG

RF
DT
GNB
SVM
LG

RF
DT
GNB
SVM
LG

RF
DT
GNB
SVM
LG

RF
DT
GNB
SVM
LG

RF
DT
GNB
SVM
LG

(cid:63)Classiﬁer

Feat3

0.764
0.787
0.809
0.740
0.680

0.725
0.660
0.690
0.672
0.662

0.765
0.681
0.687
0.706
0.657

0.725
0.662
0.680
0.707
0.741

0.715
0.708
0.623
0.875
0.765

0.655
0.762
0.661
0.776
0.768

Accuracy
Feat12

Feat21

0.886
0.800
0.833
0.803
0.800

0.800
0.750
0.712
0.704
0.657

0.900
0.750
0.705
0.802
0.701

0.800
0.703
0.700
0.804
0.801

0.810
0.750
0.700
0.910
0.830

0.702
0.800
0.701
0.802
0.860

0.884
0.680
0.726
0.620
0.680

0.650
0.570
0.667
0.640
0.667

0.630
0.612
0.668
0.610
0.613

0.650
0.620
0.660
0.610
0.680

0.620
0.666
0.545
0.840
0.700

0.608
0.600
0.620
0.750
0.676

Precision
Feat12

Feat21

0.767
0.812
0.914
0.833
0.807

0.866
0.844
0.800
0.733
0.844

0.888
0.616
0.862
0.806
0.695

0.853
0.802
0.834
0.850
0.875

0.814
0.914
0.828
0.875
0.822

0.833
0.844
0.833
0.844
0.761

0.769
0.725
0.695
0.675
0.693

0.614
0.695
0.666
0.667
0.625

0.652
0.516
0.604
0.667
0.601

0.657
0.695
0.675
0.733
0.669

0.675
0.777
0.622
0.799
0.783

0.675
0.563
0.484
0.692
0.695

Feat3

0.627
0.769
0.805
0.754
0.750

0.667
0.770
0.733
0.700
0.735

0.767
0.566
0.733
0.737
0.648

0.875
0.749
0.755
0.792
0.772

0.625
0.846
0.725
0.837
0.803

0.675
0.703
0.659
0.768
0.728

Feat3

0.972
0.750
0.780
0.729
0.762

0.833
0.600
0.600
0.599
0.628

0.778
0.465
0.627
0.590
0.633

0.912
0.828
0.899
0.845
0.840

0.639
0.721
0.500
0.709
0.688

0.788
0.762
0.759
0.799
0.794

Recall
Feat12

Feat21

0.971
0.833
0.833
0.838
0.833

1.000
0.667
0.667
0.667
0.665

0.890
0.596
0.667
0.645
0.668

1.000
0.833
0.940
0.857
0.857

0.712
0.775
0.666
0.750
0.709

0.857
0.857
0.857
0.833
0.857

0.973
0.666
0.726
0.620
0.690

0.666
0.532
0.534
0.530
0.590

0.666
0.333
0.587
0.534
0.597

0.823
0.822
0.857
0.833
0.823

0.566
0.666
0.333
0.667
0.667

0.719
0.667
0.660
0.764
0.731

Feat3

0.827
0.692
0.796
0.747
0.762

0.733
0.551
0.564
0.710
0.621

0.808
0.513
0.604
0.558
0.452

0.812
0.776
0.803
0.742
0.817

0.726
0.717
0.589
0.698
0.720

0.776
0.691
0.787
0.813
0.767

F1 score
Feat12

Feat21

0.830
0.727
0.823
0.835
0.833

0.800
0.602
0.667
0.857
0.671

0.888
0.589
0.667
0.667
0.571

0.875
0.833
0.823
0.857
0.875

0.789
0.857
0.727
0.729
0.750

0.800
0.714
0.823
0.857
0.857

0.824
0.656
0.769
0.659
0.690

0.666
0.500
0.461
0.562
0.571

0.727
0.438
0.542
0.448
0.333

0.748
0.719
0.782
0.626
0.759

0.662
0.576
0.450
0.667
0.690

0.751
0.667
0.750
0.769
0.676

AUC-ROC
Feat12

Feat21

0.886
0.816
0.875
0.833
0.800

0.833
0.667
0.619
0.694
0.696

0.717
0.605
0.681
0.761
0.690

0.762
0.791
0.625
0.761
0.833

0.725
0.857
0.642
0.825
0.795

0.625
0.667
0.625
0.857
0.762

0.590
0.698
0.667
0.673
0.675

0.666
0.536
0.571
0.573
0.523

0.694
0.523
0.510
0.548
0.429

0.578
0.651
0.524
0.690
0.750

0.747
0.761
0.566
0.761
0.694

0.640
0.541
0.511
0.667
0.667

Feat3

0.620
0.757
0.771
0.753
0.738

0.629
0.602
0.595
0.634
0.610

0.720
0.564
0.596
0.655
0.560

0.649
0.721
0.575
0.726
0.792

0.763
0.809
0.604
0.793
0.745

0.595
0.604
0.568
0.762
0.714

With regards to RQ1, our results indicate that we
achieve almost always a performance greater than 0.8
in terms of AUC-ROC and Precision when predicting
MRs using source code based features (the one excep-
tion is the case of AUC-ROC for MR INC). The best
results are obtained with only 12 features out of 21.
However, there is not one single best classiﬁer for all
MRs. With regards to Precision, each classiﬁer is best
for one MR at least once. With regards to AUC-ROC,
with the exception of DT, each classiﬁer is best for
one MR at least once.

B. RQ2: Does PMR performance improve when using source
code based features instead of CFG-based features?

classiﬁers using 12 top-ranked source code based features. The
highest AUC-ROC values for each MR are printed in bold font.

TABLE VI: Comparison between AUC-ROC values per MR
obtained by Kanewala et al. [11] using SVM with CFG-
related features (NF-PF, GK and RWK), and AUC-ROC values
obtained when using RF, DT, GNB and LG, with the 12 top-
ranked source code based features.

12 top-ranked source code based feat.

MR

Kanewala et al. [11]
GK
SVM SVM SVM RF

NF-PF
SVM

RWK

0.81
0.78
0.84
0.73
0.93
0.84

ADD
0.89
0.83
EXC
INC
0.72
MUL
0.76
0.76
PER
INV
0.64
Feat: Feature, (cid:63)AUC: AUC-ROC, ±Prec: Precision

0.83
0.78
0.88
0.78
0.91
0.68

0.83
0.69
0.76
0.76
0.83
0.86

0.92
0.90
0.89
0.83
0.95
0.76

DT

0.82
0.67
0.61
0.79
0.86
0.67

GNB

0.88
0.62
0.68
0.63
0.64
0.63

LG

0.80
0.70
0.69
0.83
0.80
0.76

The left-hand side of Table VI shows the AUC-ROC values
obtained when SVM models are trained with three feature
extraction approaches, i.e., node and path features (NF-PF),
graphlet kernel (GK), and random walk kernel (RWK), as
reported by Kanewala et al. [11]. The features used in these
classiﬁers are CFG-related. Since precision has not been
reported by Kanewala et al., we must base our comparison
exclusively on the AUC-ROC measure. Also, Kanewala et al.
do not report results from other models but SVM. The right-
hand side of Table VI shows the AUC-ROC values for the ﬁve

The comparison between Kanewala et al. and our models
shows that in ﬁve out of six cases Kanewala et al.’s RWK-
SVM model is performing best. Only for MR INV our SVM
model using 12 top-ranked source code based features and
linear kernel is performing best. For MR MUL, our LG clas-
siﬁer achieves a tie. However, considering that the extraction of
features from CFGs is more expensive than building classiﬁers
directly from the source code (i.e., without ﬁrst having to
construct CFGs from the source code and then analyse them),
our classiﬁers might still be acceptable if their performance is

not much lower than that of Kanewala et al.’s best classiﬁer.
From Figure 3 (which plots the values presented in Table VI),
one can see that not only for MR INV one of our classiﬁers
performs best but also for all other MRs one of our classiﬁers
has a performance close to that of Kanewala et al.’s best
classiﬁer.

Fig. 3: Comparison of the AUC-ROC results obtained by
Kanewala et al. [11] using node- path-based features (NF-
PF), Graphlet Kernel (GK) and Random Walk Kernel (RWK)
in SVMs, and the AUC-ROC results obtained in this work
using the source code Metrics (SM) in SVM, RF, DT, GNB,
and LG

Regarding RQ2, our results indicate that classiﬁers us-
ing source code based features most of the time cannot
achieve better performance in terms of AUC-ROC.
SVM classiﬁers using CFG-based features and the
RWK are best for ﬁve out of six MRs. Only for INV,
the SVM classiﬁer using twelve source code based
features (and the default linear kernel) is better than
the SVM classiﬁer with RWK proposed by Kanewala
et al. [11]. However, the performance of classiﬁers
using source code based features is not dramatically
worse than that of the best classiﬁer using CFG-based
features. Since source code based features are much
cheaper to extract, this might outweigh the small loss
of performance.

V. DISCUSSION

We now discuss the four most relevant threats to validity of

our study.

A. Internal validity

Since Kanewala et al.’s original study only reported the
performance of their classiﬁers only in terms of AUC-ROC, we
had to base our comparison on that measure although, for our
intended application of the PMR approach, precision would be
the more appropriate measure. In addition, more studies are

needed to investigate how it affects the overall performance
of PMR when the same method is implemented differently, as
this would directly affect feature extraction in both the original
study and our modelling approach.

B. External validity

For the sake of fair comparison, our study uses the same set
of methods as the original study but uses the source code of the
method instead of its CFG representation. However, it would
have been preferable to use a corpus of source code consisting
of a greater number of methods. Consequently, both our study
and the original one cannot determine the true extent of the
efﬁcacy of the PMR approach.

C. Construct validity

In this paper, we used the SCmine framework to extract the
source code metrics (features) at method level. SCminer is an
open-source tool that uses third-party libraries. Usage of these
third-party libraries represents potential threats to construct
validity. To avoid this, we veriﬁed that the results produced by
SCminer are correct by manually inspecting randomly selected
outputs produced by the tool.

D. Conclusion validity

We used AUC-ROC and presicion value for evaluating the
performance of the classiﬁers. We considered AUC-ROC and
Presicion > 0.80 as a good classiﬁer. This is consistent with
most of the ML literature.

VI. CONCLUSION

In this paper, we evaluate the performance of PMR using
features related to the source code. We start by extracting
21 metrics related to the source code as features. Next, we
perform features importance analysis using RF classiﬁers.
After selecting the best set of features, we evaluate them in
ﬁve different classiﬁers to ﬁnd out which is the best in terms
of AUC-ROC and Precision. Finally, to see if source code-
related features improve PMR performance when using CFG-
related features; we compared the AUC-ROC results obtained
by Kanewala et al. [11] with our own results. In summary, a
total of 21 characteristics and 5 classiﬁcation algorithms are
evaluated in this study. All classiﬁers are carefully evaluated
using 10-time cross-validation; To evaluate the performance of
our PMT implementation, 5 performance metrics are recorded
(per fold) and then averaged. Our results show that PMR
can achieve results greater than 0.8 in terms of precision
when predicting MRs using features based on the source
code. The best results are obtained with only 12 features
out of 21. However, there is no single best classiﬁer for all
MRs. Also, classiﬁers that use source-based features most of
the time cannot perform better in terms of AUC-ROC. For
this particular performance metric, the use of CFG-related
functions in particular RWK is better than ours four out of
six times and ties once.

ADDEXCINCMULPERINVMetamorphicrelation0.600.650.700.750.800.850.900.95AUC-ROCSVM:NF-PF[11]SVM:GK[11]SVM:RWK[11]SVM:SM(Feat-12)RF:SM(Feat-12)DT:SM(Feat-12)GNB:SM(Feat-12)LG:SM(Feat-12)ACKNOWLEDGEMENT
This research was partly funded by the Estonian Center of
Excellence in ICT research (EXCITE), the European Regional
Development Fund,
the IT Academy Programme for ICT
Research Development, the Austrian ministries BMVIT and
BMDW, the Province of Upper Austria under the COMET
(Competence Centers for Excellent Technologies) program
managed by FFG, and grant PRG1226 of the Estonian Re-
search Council.

REFERENCES
[1] U. Kanewala and J. M. Bieman, “Using machine learning techniques
to detect metamorphic relations for programs without test oracles,”
in IEEE 24th International Symposium on Software Reliability Engi-
neering (ISSRE), 2013, pp. 1–10.
T. Y. Chen, S. C. Cheung, and S. M. Yiu, “Metamorphic testing: A
new approach for generating next test cases,” Department of Computer
Science, Hong Kong University of Science and Technology, Hong
Kong, Tech. Rep. HKUST-CS98-01, 1998.

[2]

[3] A. Duque-Torres, A. Shalygina, D. Pfahl, and R. Ramler, “Using
rule mining for automatic test oracle generation,” in 8th International
Workshop on Quantitative Approaches to Software Quality (QuASoQ),
ser. QuASoQ’20, 2020.
E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo, “The
oracle problem in software testing: A survey,” IEEE Transactions on
Software Engineering, vol. 41, no. 5, pp. 507–525, 2015.

[4]

[5] H. Liu, F.-C. Kuo, D. Towey, and T. Y. Chen, “How effectively does
metamorphic testing alleviate the oracle problem?” IEEE Transactions
on Software Engineering, vol. 40, no. 1, pp. 4–22, 2014.
Z. Q. Zhou, L. Sun, T. Y. Chen, and D. Towey, “Metamorphic
relations for enhancing system understanding and use,” IEEE Trans-
actions on Software Engineering, vol. 46, no. 10, pp. 1120–1154,
2020.

[6]

[8]

[7] H. Liu, X. Liu, and T. Y. Chen, “A new method for constructing
metamorphic relations,” in 12th International Conference on Quality
Software, 2012, pp. 59–68.
J. Zhang, J. Chen, D. Hao, Y. Xiong, B. Xie, L. Zhang, and H. Mei,
“Search-based inference of polynomial metamorphic relations,” in
29th ACM/IEEE International Conference on Automated Software
Engineering, ser. ASE’14, Vasteras, Sweden, 2014, pp. 701–712.
T. Y. Chen, P.-L. Poon, and X. Xie, “METRIC: METamorphic
Relation Identiﬁcation based on the Category-choice framework,”
Journal of Systems and Software, vol. 116, pp. 177–190, 2016.
[10] C.-A. Sun, A. Fu, P.-L. Poon, X. Xie, H. Liu, and T. Y. Chen,
“METRIC+: A metamorphic relation identiﬁcation technique based
on input plus output domains,” IEEE Transactions on Software
Engineering, vol. 47, no. 9, pp. 1764–1785, 2021.

[9]

[12]

[11] U. Kanewala, J. M. Bieman, and A. Ben-Hur, “Predicting meta-
morphic relations for testing scientiﬁc software: A machine learning
approach using graph kernels,” Software testing, veriﬁcation and
reliability, vol. 26, no. 3, pp. 245–269, 2016.
S. Segura, G. Fraser, A. B. Sanchez, and A. Ruiz-Cort´es, “A survey
on metamorphic testing,” IEEE Transactions on software engineering,
vol. 42, no. 9, pp. 805–824, 2016.
T. Y. Chen, F.-C. Kuo, H. Liu, P.-L. Poon, D. Towey, T. H. Tse,
and Z. Q. Zhou, “Metamorphic testing: A review of challenges and
opportunities,” ACM Computing Surveys, vol. 51, no. 1, Jan. 2018.

[13]

[14] C. Murphy, G. Kaiser, L. Hu, and L. Wu, “Properties of machine
learning applications for use in metamorphic testing,” in In Proceed-
ings of the 20th International Conference on Software Engineering &
Knowledge Engineering (SEKE), 2008.
S. Segura, D. Towey, Z. Q. Zhou, and T. Y. Chen, “Metamorphic
testing: Testing the untestable,” IEEE Software, vol. 37, no. 3, pp. 46–
53, 2020.

[15]

[16] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, “DeepRoad:
GAN-based metamorphic testing and input validation framework
for autonomous driving systems,” in 33rd IEEE/ACM International
Conference on Automated Software Engineering (ASE), IEEE, 2018,
pp. 132–142.

[17]

[18]

[19]

Z. Q. Zhou and L. Sun, “Metamorphic testing of driverless cars,”
Communications of the ACM, vol. 62, no. 3, pp. 61–67, Feb. 2019.
P. C. Canizares, A. N´unez, J. de Lara, and L. Llana, “MT-EA4Cloud:
A methodology for testing and optimising energy-aware cloud sys-
tems,” Journal of Systems and Software, vol. 163, p. 110 522, 2020.
Z. Zhang, D. Towey, Z. Ying, Y. Zhang, and Z. Q. Zhou, “MT4NS:
Metamorphic testing for network scanning,” in 6th IEEE/ACM In-
ternational Workshop on Metamorphic Testing (MET), ser. MET’21,
2021, pp. 17–23.

[20] M. Srinivasan, M. P. Shahri, I. Kahanda, and U. Kanewala, “Qual-
ity assurance of bioinformatics software: A case study of testing
a biomedical
text processing tool using metamorphic testing,” in
IEEE/ACM 3rd International Workshop on Metamorphic Testing
(MET), ser. MET’18, Gothenburg, Sweden: Association for Comput-
ing Machinery, 2018, pp. 26–33.

[21] M. P. Shahri, M. Srinivasan, G. Reynolds, D. Bimczok, I. Kahanda,
and U. Kanewala, “Metamorphic testing for quality assurance of
protein function prediction tools,” in IEEE International Conference
On Artiﬁcial Intelligence Testing (AITest), IEEE, 2019, pp. 140–148.
Z. Peng, U. Kanewala, and N. Niu, “Contextual understanding and
improvement of metamorphic testing in scientiﬁc software develop-
ment,” in 15th ACM/IEEE International Symposium on Empirical
Software Engineering and Measurement (ESEM), 2021, pp. 1–6.

[22]

[23] X. Lin, M. Simon, and N. Niu, “Exploratory metamorphic testing
for scientiﬁc software,” Computing in Science Engineering, vol. 22,
no. 2, pp. 78–87, 2020.

[24] B. Hardin and U. Kanewala, “Using semi-supervised learning for
predicting metamorphic relations,” in 3rd IEEE/ACM International
Workshop on Metamorphic Testing (MET), ser. MET’18, 2018,
pp. 14–17.

[25] K. Rahman and U. Kanewala, “Predicting metamorphic relations for
matrix calculation programs,” in 3rd IEEE/ACM International Work-
shop on Metamorphic Testing (MET), ser. MET’18, 2018, pp. 10–13.
[26] A. Nair, K. Meinke, and S. Eldh, “Leveraging mutants for auto-
matic prediction of metamorphic relations using machine learning,”
in Proceedings of the 3rd ACM SIGSOFT International Workshop
on Machine Learning Techniques for Software Quality Evaluation,
ser. MaLTeSQuE 2019, Tallinn, Estonia: Association for Computing
Machinery, 2019, pp. 1–6.
P. Zhang, X. Zhou, P. Pelliccione, and H. Leung, “Rbf-mlmr: A
multi-label metamorphic relation prediction approach using rbf neural
network,” IEEE Access, vol. 5, pp. 21 791–21 805, 2017.
T. Honglei, S. Wei, and Z. Yanan, “The research on software met-
rics and software complexity metrics,” in 2009 International Forum
on Computer Science-Technology and Applications, vol. 1, 2009,
pp. 131–136.

[27]

[28]

[30]

[29] D. Coleman, D. Ash, B. Lowther, and P. Oman, “Using metrics to
evaluate software system maintainability,” Computer, vol. 27, no. 8,
pp. 44–49, 1994.
S. H. Kan, Metrics and Models in Software Quality Engineering, 2nd.
USA: Addison-Wesley Longman Publishing Co., Inc., 2002.
[31] K. Gao, T. M. Khoshgoftaar, H. Wang, and N. Seliya, “Choosing
software metrics for defect prediction: An investigation on feature
selection techniques,” Software: Practice and Experience, vol. 41,
no. 5, pp. 579–606, 2011.

[32] A. Duque-Torres, N. Doliashvili, D. Pfahl, and R. Ramler, “Predicting
survived and killed mutants,” in IEEE International Conference on
Software Testing, Veriﬁcation and Validation Workshops (ICSTW),
2020, pp. 274–283.

[33] D. Mao, L. Chen, and L. Zhang, “An extensive study on cross-project
predictive mutation testing,” in 2019 12th IEEE Conference on Soft-
ware Testing, Validation and Veriﬁcation (ICST), 2019, pp. 160–171.
J. Zhang, L. Zhang, M. Harman, D. Hao, Y. Jia, and L. Zhang,
“Predictive mutation testing,” IEEE Transactions on Software Engi-
neering, vol. 45, no. 9, pp. 898–918, 2019.

[34]

[35] Colt project, http://acs.lbl.gov/software/colt/, Accessed: 2021-09-21.
Apache mahout, https://mahout.apache.org/, Accessed: 2021-09-21.
[36]
Apache commons mathematic, http : / / commons . apache . org / proper /
[37]
commons-math/, Accessed: 2021-09-21.
Java collections, https://docs.oracle.com/javase/8/docs/technotes/
guides/collections/overview.html, Accessed: 2021-09-21.

[38]

