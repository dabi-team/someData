2
2
0
2

n
u
J

9

]
E
S
.
s
c
[

2
v
9
8
1
2
0
.
4
0
2
2
:
v
i
X
r
a

Automating Staged Rollout with Reinforcement Learning

Shadow Pritchard
University of Tulsa
Tulsa, Oklahoma, USA
swp7196@utulsa.edu

Vidhyashree Nagaraju
University of Tulsa
Tulsa, Oklahoma, USA
vidhyashree-nagaraju@utulsa.edu

Lance Fiondella
University of Massachusetts
Dartmouth
Dartmouth, Massachusetts, USA
lfiondella@umassd.edu

ABSTRACT
Staged rollout is a strategy of incrementally releasing software
updates to portions of the user population in order to accelerate
defect discovery without incurring catastrophic outcomes such as
system wide outages. Some past studies have examined how to
quantify and automate staged rollout, but stop short of simultane-
ously considering multiple product or process metrics explicitly.
This paper demonstrates the potential to automate staged rollout
with multi-objective reinforcement learning in order to dynamically
balance stakeholder needs such as time to deliver new features and
downtime incurred by failures due to latent defects.

CCS CONCEPTS
• Software and its engineering → Software testing and de-
bugging.

KEYWORDS
DevOps, Staged Rollout, Reinforcement Learning, Software Relia-
bility

ACM Reference Format:
Shadow Pritchard, Vidhyashree Nagaraju, and Lance Fiondella. 2022. Au-
tomating Staged Rollout with Reinforcement Learning. In New Ideas and
Emerging Results (ICSE-NIER’22), May 21–29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 5 pages. https://doi.org/10.1145/3510455.3512782

1 INTRODUCTION
Researchers recognize the importance of software performance en-
gineering and application performance management [2] during the
development and operation phases, including interoperability be-
tween different tools and techniques. However, it is also important
to consider process and product quality metrics in an integrated
manner as well as any tradeoffs imposed. For example, staged roll-
out [17] seeks to accelerate deployment of new features while
preserving safety, yet this strategy of introducing an update to a
subset of users may pose tradeoffs such as the time to deliver a
new feature and the downtime experienced by users. Techniques
to automate the staged rollout process could reduce the level of
expertise required to balance these stakeholder needs.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9224-2/22/05. . . $15.00
https://doi.org/10.1145/3510455.3512782

Tarvo et al. [12] defined and collected metrics on software up-
dates, but required the user to make release decisions. Zhao et
al. [17] introduced a framework to release software to subsets
of users based on time, power, and risk-based scheduling, which
rely on predetermined thresholds or time windows. The thesis of
Velayutham [15] treated staged rollout as a time series problem
with autoregressive integrated moving average and long short-
term memory to automate decision making, but tradeoffs were not
explicitly considered.

Potential reinforcement learning approaches to model staged
rollout include non-stationary [4] methods, which encompass time-
varying rewards. Abdallah et al. [1] introduced repeated Q-learning
to update reward tables more frequently for seldom visited states
in order to improve performance under non-stationary conditions.
Multi-objective reinforcement learning [6] attempts to balance re-
wards associated with multiple objectives through a weighted, lin-
ear reward [5]. Additional methods to balance multiple objectives in-
clude geometric steering [13] or hyper volume action selection [14].
This paper presents a model of the staged rollout problem as a
non-stationary Markov decision process (MDP). Multi-objective
Q-learning with upper confidence bound exploration is applied
to solve the MDP. The approach is demonstrated on a data set
from the software reliability engineering literature, illustrating
reinforcement learning as a promising approach to make dynamic
decisions during staged rollout.

The remainder of the paper is organized as follows: Section 2
formulates a state model of the staged rollout process, discussing
tradeoffs and reward function modeling. Section 3 summarizes
Q-learning with upper confidence bound as well as naive policy
enumeration. Section 4 develops metrics to assess policies identified
by reinforcement learning. Section 5 presents preliminary results.
Section 6 describes plans for future research.

2 STAGED ROLLOUT MODELING AND

TRADEOFF ASSESSMENT

This section presents a state model of the staged rollout problem
that can be interpreted as a Markov decision process (MDP), where
a multi-objective reinforcement learning agent makes decisions to
balance two primary factors, including (i) downtime and (ii) deliv-
ery time according to stakeholder preference. For example, safety
critical software industries place greater emphasis on minimizing
failures, whereas many other industries prioritize minimizing de-
livery time to get a product to market quickly.

2.1 Staged Rollout Model
Staged rollout of software [17] has been recognized as a strategy
to field new functionality on an ongoing basis without incurring
failures that induce system outages, widespread unavailability of

 
 
 
 
 
 
ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA

Pritchard, et al.

services, economic losses, and user dissatisfaction. The rationale
for staged rollout is to publish an updated software possessing new
functionality for use by a subset of the user base to avoid the major
problems described above, but also to accelerate the discovery of
defects. The development team then attempts to correct the source
of the problem and begins the process of staged rollout anew.

Figure 1 shows a simple state diagram of a staged rollout process.

Figure 1: State diagram of staged rollout

State 𝐷𝑒𝑣 represents the development state, where software is tested
by an internal team. An elementary model of the traditional ap-
proach to software updates simply transitions to the 𝑂𝑝𝑠 state, once
the software is deemed satisfactory with respect to functional re-
quirements, reliability, and other desired attributes, where the 𝑂𝑝𝑠
state exposes the software to the entire base of 𝑛𝑂𝑝𝑠 users. Staged
rollout, instead, transitions from the 𝐷𝑒𝑣 state to state 𝑖1, which
represents the first stage of staged rollout, where the software is
published for use by 𝑝𝑖1 percent or 𝑛𝑖1 = 𝑝𝑖1 × 𝑛𝑂𝑝𝑠 of the user
base. Multiple stages of staged rollout between development and
full deployment are also possible. This more general case possesses
𝑚 intermediate staged rollout states, where it may be reasonable to
assume that each transition from 𝑖 𝑗 to 𝑖 𝑗+1 increases the fraction
of the user base exposed to the software such that 𝑝𝑖 𝑗 < 𝑝𝑖 𝑗 +1 and
𝑛𝑖 𝑗 < 𝑛𝑖 𝑗 +1 . Failure in any state transitions to the 𝐷𝑒𝑣 state, where
root cause analysis and defect removal are attempted.

The state model described in Figure 1 enables explicit considera-
tion of the tradeoffs between downtime and delivery time. Down-
time is determined by the state in which the failure occurs and is
proportional to the fraction of the user base 𝑛𝑖 𝑗 and mean time to
resolve (𝑀𝑇𝑇 𝑅). Thus, failure in the 𝑗𝑡ℎ state of staged rollout (𝑖 𝑗 )
contributes less to downtime than failure in the state 𝑖 𝑗+1. However,
the defect exposure rate in the 𝑗𝑡ℎ state of staged rollout is also
less than the defect exposure rate in the ( 𝑗 + 1)𝑡ℎ state, meaning
that the downtime experienced, and the time required to discover
and remove all defects are modeled as competing constraints. Thus,
minimizing downtime by remaining in the 𝐷𝑒𝑣 state until all defects
have been detected and removed will likely delay delivery time.
Similarly, unrestrained transition to the Ops state immediately after
each defect is resolved is likely to exacerbate downtime. Therefore,
it may not be possible to simultaneously minimize downtime and
delivery time, posing a multi-objective problem. Moreover, organi-
zations implementing staged rollout may express different levels
of tolerance for these undesirable outcomes. Subsequently, it is
unlikely that a single optimal policy or "one size fits all" approach
to staged rollout exists. Instead, it is necessary to select transition
times 𝑡 𝑗,𝑗+1 =
that balance downtime and delivery time in

1
𝜆 𝑗,𝑗 +1

a manner that is satisfactory to the customer. Intuitively, a high
defect discovery rate in the 𝐷𝑒𝑣 state is likely to indicate that ad-
ditional defects will be uncovered. Hence, staged rollout should
not be performed because it would risk greater downtime. There-
fore, the problem is to select numerical values of transition rates
𝜆𝑗,𝑗+1 = 1
that achieve the desired balance between downtime
𝑡 𝑗,𝑗 +1
and delivery time. In the case where these rates vary as a function
of time, the problem is said to be non-stationary.

Since the transitions 𝜆𝐷𝑒𝑣,𝑖1 and 𝜆𝑖1,𝑂𝑝𝑠 of Figure 1 are decisions,
the staged rollout state diagram is a Markov decision process to
which reinforcement learning can be applied in order to solve for a
range of policies to take actions (make transition decisions) from a
given state. Traditional software reliability models [3] assume that
the rate of defect discovery is nonhomogeneous. For example, if the
rate of defect discovery is high in the early stages of testing, then the
optimal action will be to stay in the 𝐷𝑒𝑣 state, where the penalty for
failure is lowest. However, later in testing when fewer undiscovered
defects remain, the optimal action will be to transition to state 𝑖1 or
𝑂𝑝𝑠 in order to reduce delivery time. Therefore, the model is non-
stationary because the rate at which defects are discovered changes
as testing progresses. Thus, this change in the defect detection rate
will also cause the reward to change and the optimal policy for a
given state will change over time.

2.2 Process Tradeoffs and Reward Modeling
This section defines quantitative metrics for delivery time and
downtime in terms of the state model shown in Figure 1 as well as
how these metrics are incorporated into a multi-objective reward
model suitable for application of reinforcement learning.

2.2.1 Delivery Time. To model the impact of staged rollout on
delivery time, we assume that one unit of time in the 𝐷𝑒𝑣 state ad-
vances the timeline by one unit, whereas time in the staged rollout
and 𝑂𝑝𝑠 state accelerate the rate at which time advances propor-
tional to the percentage of the user base. Therefore, staged rollout
may be regarded as a modern form of accelerated life testing [10]
for software. For example, if the complete user base is composed of
𝑛𝑂𝑝𝑠 = 10, 000 users and staged rollout exposes new functionality
to 𝑝𝑖1 = 0.1 or 10% of the user base, then 𝑛𝑖1 = 1000. Similarly, if
𝑛𝐷𝑒𝑣 = 50, then a simple method to compute the acceleration factor
in each state of staged rollout is the ratio between the number of
users in a state over the baseline in the 𝐷𝑒𝑣 state such that the ac-
𝑛𝑖1
celeration factor in the staged rollout and 𝑂𝑝𝑠 states are
𝑛𝐷𝑒𝑣 = 20
𝑛𝑂𝑝𝑠
𝑛𝐷𝑒𝑣 = 200 respectively. This simplifying assumption can be
and
improved, since testers are familiar with the functionality and inten-
tionally stress the program to expose defects. Modeling these ratios
is a research question that requires staged rollout data. Neverthe-
less, the simplifying assumptions made here enable a quantitative
framework upon which to improve.

The preliminary assumption of linear acceleration factors de-
scribed above provides a concrete starting point to measure the
cumulative time required to reach a release decision. Specifically,
delivery time may be defined as the time to discover and eliminate
defects associated with a test set plus the time to transition from the
𝐷𝑒𝑣 to 𝑂𝑝𝑠 state (𝑡𝐷𝑒𝑣,𝑖1 + 𝑡𝑡1,𝑂𝑝𝑠 ) because no additional defects are
discovered under the simplifying assumption that the final defect is

Automating Staged Rollout with Reinforcement Learning

ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA

resolved immediately. Modeling advances that explicitly consider
the time between defect discovery and resolution [7, 9] can further
enhance the realism of the staged rollout deployment model.

2.2.2 Downtime. To model the impact of staged rollout on down-
time, we assume that failure in the 𝐷𝑒𝑣 state does not incur down-
time, since only internal testing is performed at this stage. However,
downtime incurred in the staged rollout and 𝑂𝑝𝑠 states are propor-
tional to the fraction of the user base multiplied by the mean time
to resolve defects such that the accumulated downtime increases
by 𝑝𝑖1 ∗ 𝑀𝑇𝑇 𝑅 or 𝑝𝑂𝑝𝑠 × 𝑀𝑇𝑇 𝑅. This simplifying assumption may
be conservative, since not all users exposed to the functionality
will necessarily experience the failure. Similar to delivery time,
the downtime experienced must be modeled from staged rollout
data and has important implications for identifying an optimal
deployment policy for transition times, since conservative assump-
tions may unnecessarily increase delivery time. Thus, our prelim-
inary model expresses the total downtime as the weighted sum
𝑀𝑇𝑇 𝑅 × (cid:205)𝑛
𝑝𝑠 (𝑖) , where 𝑠 (𝑖) denotes the state in which the 𝑖𝑡ℎ
𝑖=1
failure occurs.

2.2.3 Reward Modeling. To model staged rollout as a reinforce-
ment learning problem, this section defines the reward function
according to techniques from multi-objective reinforcement learn-
ing [6]. Specifically, a linear combination of the delivery and down
time measures defined in Sections 2.2.1 and 2.2.2 at each time step.
To facilitate the selection of weights in an intuitive manner, our on-
going research seeks to specify constrained optimization problems
to natural forms such as (i) minimize delivery time while limiting
downtime to a specified constraint or (ii) minimizing downtime
while limiting delivery time to a specified constraint.

3 ALGORITHMS
This section describes the Q-learning [16] algorithm to estimate
the rewards of state-action pairs, the upper confidence bound ex-
ploration strategy, and a naive policy enumeration approach to
establish a baseline with which to compare the performance of
reinforcement learning.

3.1 Q-Learning
Reinforcement learning [11] employs a Markov decision process
to estimate the reward of each state-action pair denoted 𝐸 [𝑟 (𝑠, 𝑎)],
which determines a policy 𝜋 (𝑠) that informs the best action to
take in any given state. Q-learning [16] is a model free algorithm to
estimate the reward of a state-action pair according to the following
update equation

𝑄 (𝑠𝑡 , 𝑎) = 𝑄 (𝑠𝑡 , 𝑎) + 𝛼 (𝑟 + 𝛾 max
𝑎′ ∈A

𝑄 (𝑠𝑡 +1, 𝑎′) − 𝑄 (𝑠𝑡 , 𝑎))

(1)

where 𝑠𝑡 is the state at time step 𝑡, 𝑎 the action taken, 𝛼 ∈ (0, 1)
the learning rate, 𝑟 the reward, 𝛾 ∈ (0, 1) the time discount, and 𝑎′
the action in the next state (𝑡 + 1) chosen from all available actions
(A). A time discount close to one expresses a willingness to defer
rewards farther into the future, whereas small values of gamma
increase preference for immediate rewards. The learning rate 𝛼 is a
multiplier which determines how much emphasis to place on the
most recently observed reward when updating the reward estimate
of a state-action pair.

3.2 Upper Confidence Bound (UCB)
The upper confidence bound exploration strategy combines the
𝑄-value with an adaptation from statistical upper confidence limits
to choose the next action in a given state.

√︄

𝑎′ = max
𝑎 ∈A

𝑄 (𝑠, 𝑎) + 𝑐 ×

log

(cid:19)

(cid:18) 𝑡 + 1
𝑛(𝑠, 𝑎)

(2)

where 𝑐 > 0 is a scaling constant, 𝑡 is the present time step, and 𝑛
is the number of times that a state-action pair has been selected
previously. Thus, the UCB exploration strategy selects an action
according to the estimated reward as well as the uncertainty em-
bodied in the finite sample size associated with state-action pairs.
The upper confidence term is larger for smaller values of 𝑛, encour-
aging exploration of less frequently taken state-action pairs. Small
values of 𝑐 tend toward exploitation of the action with the highest
𝑄-value, whereas large values of 𝑐 encourage exploration of the
least frequently taken actions.

3.3 Naive Policy Enumeration
Naive policy enumeration does not use reinforcement learning, is
not capable of being used in an online fashion, and is therefore
not a competitive alternative. It is a deterministic approach based
on emulation with the full data to approximate the Pareto opti-
mal front of tradeoffs between delivery time and downtime from
a range of stationary policies in order to objectively compare the
performance of a reinforcement learning approach in terms of its
distance from optimality. Specifically, state transition policy vec-
tor 𝝀 = ⟨𝜆𝐷𝑒𝑣,𝑖1
, 𝜆𝑖1,𝑂𝑝𝑠 ⟩ for Figure 1 is applied with a range of
deterministic values such as the cross product of policies, where
𝜆𝐷𝑒𝑣,𝑖1 and 𝜆𝑖1,𝑂𝑝𝑠 respectively denote the time to spend in the
𝐷𝑒𝑣 and staged rollout state without failure before transitioning to
the staged rollout or 𝑂𝑝𝑠 state. Thus, naive enumeration applies a
range of deterministic policy vectors on the same data set and com-
putes a tuple including the downtime and delivery time for each
policy vector. A plot consisting of only the tuples that correspond
to non-dominated policies (lowest delivery time for a fixed down-
time or lowest downtime for a fixed delivery time) constitute the
Pareto optimal front for comparison with reinforcement learning
strategies.

4 METRICS
To impose greater rigor and promote objective comparison of rein-
forcement learning approaches, we define two quantitative metrics,
including range, which measures the flexibility of an approach, and
average suboptimality, which may be regarded as a performance-
oriented metric. Both of these metrics are computed relative to a
baseline Pareto optimal curve determined from naive policy enu-
meration. The discussion that follows introduces these metrics,
provides a mathematical formula, and explains what it quantifies
as well as how the metric supports objective comparison.

4.1 Range
The range of an approach (𝑎) with respect to an objective (𝑜) is

𝑟𝑎𝑛𝑔𝑒𝑎,𝑜 =

max 𝑎, 𝑜 − min 𝑎, 𝑜
max 𝑛𝑎𝑖𝑣𝑒, 𝑜 − min 𝑛𝑎𝑖𝑣𝑒, 𝑜

.

(3)

ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA

Pritchard, et al.

Thus, Equation (3) is the range of values an approach achieves with
respect to an objective divided by the range of the naive approach,
which is treated as a baseline for the sake of normalization. The
range of the naive approach is therefore 1.0. Approaches that attain
a range greater than 1.0 for an objective produce a wider Pareto
front than the naive approach, which directly impacts policy selec-
tion, since it will not be possible to identify a Pareto optimal policy
below the minimum or above the maximum of a specific objective
with an approach.

4.2 Suboptimality
The suboptimality of a specific policy obtained by an approach with
respect to an objective is

𝑠𝑢𝑏𝑜𝑝𝑡𝑝,𝑎,𝑜 =

𝑜𝑝,𝑎
𝑜𝑝,𝑛𝑎𝑖𝑣𝑒

(4)

where 𝑜𝑝,𝑎 is the value of the objective achieved by policy 𝑝 obtained
with approach 𝑎 and 𝑜𝑝,𝑛𝑎𝑖𝑣𝑒 is the value of the objective achieved
by a policy determined from the naive approach. Thus, Equation (4)
is simply the ratio of the value achieved by an approach for a
specified objective divided by an equivalent value for the naive
approach when all other objectives are held constant. In other
words, we divide the value associated with a point on the Pareto
curve of an approach by an equivalent point on the curve for the
naive approach. Often, there is no naive policy that possesses the
exact same value as the point for the policy under consideration.
We therefore approximate suboptimality by linearly interpolating
two points on the naive Pareto curve in order to obtain a precisely
matching value. The average suboptimality (𝐸 [𝑠𝑢𝑏𝑜𝑝𝑡𝑝,𝑎,𝑜 ]) is a
summary statistic, which computes the supoptimality for each
point (individual policy) of a specified approach and objective and
then calculates the average of these values.

5 RESULTS
To demonstrate the potential to automate staged rollout with rein-
forcement learning, the SYS1 data set [8], which documents 𝑛 = 136
times at which unique defects were detected during approximately
25 hours of testing was employed. The weight on delivery time (𝑤0)
was varied in the interval (0, 1), while the remaining weight was
placed on downtime such that 𝑤1 = 1 − 𝑤0. For each combination
of weights, Q-learning with the upper confidence bound explo-
ration strategy given in Equation (2) was applied with parameters
𝑐 = 0.15, learning rate 𝛼 = 0.15, and time discount 𝛾 = 0.999999.
It should be noted that Q-learning with the UCB strategy learned
as progress was made along the testing timeline of the SYS1 data
set. Therefore, this method did not require separate data sets for
training. Naive policy enumeration was performed with pairs of
values from the cross product of 𝑡𝐷𝑒𝑣,𝑖1 = {1, 100, 200, . . . , 10000}
and 𝑡𝑖1,𝑂𝑝𝑠 = {1, 100, 200, . . . , 10000}. For the sake of illustration,
parameters of the staged rollout model were set to 𝑀𝑇𝑇 𝑅 = 10,
𝑛𝐷𝑒𝑣 = 50, 𝑛𝑖 = 1000, and 𝑛𝑂𝑝𝑠 = 10000.

Figure 2 compares the range of policies (points) identified by UCB
as well as those determined by naive policy enumeration, which can
only serve as a baseline because it is deterministic, unlike reinforce-
ment learning, which makes decisions as data becomes available.
Figure 2 indicates that UCB policies with low delivery times and
downtime greater than 300 are competitive with naive enumeration.

However, there is a visible gap between the downtime achieved by
UCB compared to naive enumeration for downtimes less than 300.
One possible explanation is that UCB must perform some amount
of exploration. While parameter tuning could certainly reduce this
gap, a more important enhancement will be to incorporate software
engineering specific testing factors that reinforcement learning can
use to drive staged rollout decisions. Moreover, one may regard
the results obtained by naive policy enumeration as “lucky”, since
they simply specify fixed values for times to wait before transition-
ing between states when no failure is experienced. The average
suboptimality metric given in Equation (4) summarizes this gap as
a single number. For example, the average suboptimality of UCB
with respect to downtime and delivery time were 2.79 and 2.72,
indicating the potential margin for improvement.

Figure 2: Tradeoff between downtime and delivery time on
SYS1 data

Figure 2 also shows that the best policy to minimize downtime
that UCB could identify was approximately 70 (upper left most
grey dot on Pareto curve). Thus, naive enumeration was able to
achieve less downtime at the expense of higher delivery times. The
range metric given in Equation (3) summarizes the width of the
UCB policies relative to naive enumeration, which were 0.80 and
0.83 for downtime and delivery time respectively, indicating that
policies identified by UCB were approximately 80% as wide as the
corresponding naive policies.

6 FUTURE PLANS
Our preliminary results obtained for Q-learning combined with
the upper confidence bound approach suggest that reinforcement
learning is a viable approach to automate staged rollout. Our on-
going work will consider alternative state of the art reinforcement
learning methods to match or exceed naive policies and provide a
greater range of flexibility. Techniques to specify a desired balance
of objectives as a constrained optimization problem will provide
a layer of abstraction to automatically select and apply a suitable
policy on the Pareto front. Toward this end, simple and efficient
techniques such as binary search can be used to identify a policy
that best matches stakeholder needs.

Automating Staged Rollout with Reinforcement Learning

ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA

ACKNOWLEDGMENT
This material is based upon work supported by the National Science
Foundation under Grant Number 1749635 and the Office of Naval
Research under Award Number N00014-22-1-2012. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect
the views of the National Science Foundation or Office of Naval
Research.

REFERENCES
[1] Sherief Abdallah and Michael Kaisers. 2016. Addressing environment non-
stationarity by repeating Q-learning updates. The Journal of Machine Learning
Research 17, 1 (2016), 1582–1612.

[2] Andreas Brunnert, André van Hoorn, Felix Willnecker, Alexandru Danciu, Wil-
helm Hasselbring, Christoph Heger, Nikolas Herbst, Pooyan Jamshidi, Reiner
Jung, Joakim von Kistowski, et al. 2015. Performance-oriented DevOps: A re-
search agenda. arXiv preprint arXiv:1508.04752 (2015).

[3] W. Farr. 1996. Handbook Of Software Reliability Engineering. McGraw-Hill, New

York, NY, chapter Software Reliability Modeling Survey, 71–117.

[4] Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. 2020. Towards
continual reinforcement learning: A review and perspectives. arXiv preprint
arXiv:2012.13490 (2020).

[5] Dmitry Krass. 1990. Contributions to the theory and applications of Markov decision

processes. Ph.D. Dissertation. The Johns Hopkins University.

[6] Chunming Liu, Xin Xu, and Dewen Hu. 2014. Multiobjective reinforcement
learning: A comprehensive overview. IEEE Transactions on Systems, Man, and
Cybernetics: Systems 45, 3 (2014), 385–398.

[7] J. Lo and C. Huang. 2006. An integration of fault detection and correction
processes in software reliability analysis. Journal of Systems and Software 79, 9
(2006), 1312–1323.

[8] Michael Lyu. 1996. Handbook of software reliability engineering (2 ed.). IEEE

computer society press CA.

[9] Maskura Nafreen, Melanie Luperon, Lance Fiondella, Vidhyashree Nagaraju,
Ying Shi, and Thierry Wandji. 2020. Connecting Software Reliability Growth
Models to Software Defect Tracking. In International Symposium on Software
Reliability Engineering. IEEE, 138–147.

[10] Wayne Nelson. 2009. Accelerated testing: statistical models, test plans, and data

analysis. Vol. 344. John Wiley & Sons.

[11] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Intro-

duction. A Bradford Book, Cambridge, MA, USA.

[12] Alexander Tarvo, Peter F Sweeney, Nick Mitchell, VT Rajan, Matthew Arnold,
and Ioana Baldini. 2015. CanaryAdvisor: a statistical-based tool for canary testing.
In International Symposium on Software Testing and Analysis. 418–422.

[13] Peter Vamplew, Rustam Issabekov, Richard Dazeley, Cameron Foale, Adam Berry,
Tim Moore, and Douglas Creighton. 2017. Steering approaches to Pareto-optimal
multiobjective reinforcement learning. Neurocomputing 263 (2017), 26–38.
[14] Kristof Van Moffaert, Madalina M Drugan, and Ann Nowé. 2013. Hypervolume-
based multi-objective reinforcement learning. In International Conference on
Evolutionary Multi-Criterion Optimization. Springer, 352–366.

[15] Girish Velayutham. 2021. Artificial Intelligence assisted Canary Testing of Cloud

Native RAN in a mobile telecom system.

[16] Christopher John Cornish Hellaby Watkins. 1989. Learning from delayed rewards.

(1989).

[17] Zhenyu Zhao, Mandie Liu, and Anirban Deb. 2018. Safely and quickly deploying
new features with a staged rollout framework using sequential test and adaptive
experimental design. In International Conference on Computational Intelligence
and Applications. IEEE, 59–70.

