2
2
0
2

p
e
S
4
1

]
E
S
.
s
c
[

2
v
6
4
4
5
0
.
8
0
2
2
:
v
i
X
r
a

CoditT5: Pretraining for Source Code and
Natural Language Editing

Jiyang Zhang
The University of Texas at Austin
Austin, TX, USA
jiyang.zhang@utexas.edu

Sheena Panthaplackel
The University of Texas at Austin
Austin, TX, USA
spantha@cs.utexas.edu

Pengyu Nie
The University of Texas at Austin
Austin, TX, USA
pynie@utexas.edu

Junyi Jessy Li
The University of Texas at Austin
Austin, TX, USA
jessy@austin.utexas.edu

Milos Gligoric
The University of Texas at Austin
Austin, TX, USA
gligoric@utexas.edu

ABSTRACT
Pretrained language models have been shown to be eï¬€ective in
many software-related generation tasks; however, they are not well-
suited for editing tasks as they are not designed to reason about ed-
its. To address this, we propose a novel pretraining objective which
explicitly models edits and use it to build CoditT5, a large lan-
guage model for software-related editing tasks that is pretrained
on large amounts of source code and natural language comments.
We ï¬ne-tune it on various downstream editing tasks, including
comment updating, bug ï¬xing, and automated code review. By out-
performing standard generation-based models, we demonstrate the
generalizability of our approach and its suitability for editing tasks.
We also show how a standard generation model and our edit-based
model can complement one another through simple reranking strate-
gies, with which we achieve state-of-the-art performance for the
three downstream editing tasks.

CCS CONCEPTS
â€¢ Computing methodologies â†’ Machine learning; â€¢ Software
and its engineering â†’ Software evolution.

KEYWORDS
Pretrained language models, editing, bug ï¬xing, comment updat-
ing, automated code review

ACM Reference Format:
Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Mi-
los Gligoric. 2022. CoditT5: Pretraining for Source Code and Natural Lan-
guage Editing. In 37th IEEE/ACM International Conference on Automated
Software Engineering (ASE â€™22), October 10â€“14, 2022, Rochester, MI, USA. ACM,
New York, NY, USA, 12 pages. https://doi.org/10.1145/3551349.3556955

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full cita-
tion on the ï¬rst page. Copyrights for components of this work owned by others than
the author(s) must be honored. Abstracting with credit is permitted. To copy other-
wise, or republish, to post on servers or to redistribute to lists, requires prior speciï¬c
permission and/or a fee. Request permissions from permissions@acm.org.
ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9475-8/22/10. . . $15.00
https://doi.org/10.1145/3551349.3556955

1 INTRODUCTION
Large language models pretrained on massive amounts of data have
led to remarkable progress in recent years, with models like BART [26],
GPT [7, 43], and T5 [44] yielding huge improvements for a vast
number of text generation tasks. Inspired by this, a new research
initiative has emerged around building large models that are pre-
trained on source code and technical text to address software-related
tasks. This includes models like PLBART [1], CodeGPT-2 [32], and
CodeT5 [55]. While these models demonstrate impressive perfor-
mance on generation tasks like code summarization, code genera-
tion, and code translation, it is unclear if they are well-suited for
the editing nature of many software-related tasks. For instance,
bug ï¬xing [49] entails editing source code to resolve bugs, auto-
mated code review [51] requires editing source code to incorporate
feedback from review comments, and comment updating [16, 29,
31, 40] pertains to updating outdated natural language comments
to reï¬‚ect code changes.

In principle, such editing tasks can be framed as standard gener-
ation tasks in which an input sequence (e.g., buggy code snippet) is
completely re-written to form the output sequence (e.g., ï¬xed code
snippet). In this way, existing pretrained conditional generation
models can be ï¬ne-tuned to autoregressively generate a sequence
from scratch. However, this can be problematic in practice [40].
When applying large generation models like PLBART and CodeT5
to these tasks, we ï¬nd that they can generate output which merely
copies the input without performing any edits (up to 34.25%) or
even deviates substantially from the input, introducing irrelevant
changes. We provide an example of automated code review in Fig-
ure 1, where a reviewer prescribes edits that need to be made to a
given code snippet: â€œGenerally better to qualify than making static
importâ€. Using the code snippet and this comment, PLBART gen-
erates an output sequence which copies the original code, without
applying any edits. While the output is valid and a likely sequence
according to PLBARTâ€™s language model, it makes no edits based
on the reviewerâ€™s comments.

We attribute these weaknesses to the fact that such models rely
on pretraining objectives designed for generating code (or software-
related natural language) in sequence by exploiting patterns with
respect to preceding tokens. Therefore, a model has to learn to im-
plicitly perform edits by generating tokens one by one in accor-
dance with the underlying probability that it has learned for which

 
 
 
 
 
 
ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric

tokens belong alongside one another, rather than being aware of
where information should be retained or modiï¬ed.

Intuitively, edit-based generation requires a diï¬€erent approach
that more frequently refers back to the input sequence, and can
often be characterized by localized operations (e.g., insertion, dele-
tion, substitution). To guide a model in discerning edit locations
in the input sequence and reason about the necessary edit opera-
tions, we design a novel pretraining objective that explicitly mod-
els edits. Our approach is inspired by content planning in natural
language generation where a skeleton of key elements are ï¬rst gen-
erated and used to guide more accurate and precise generation of
full text [14, 33, 42, 45]. Speciï¬cally, during decoding, a model ï¬rst
generates an edit plan that explicitly details the edit operations.
Then, it proceeds to autoregressively generate the target edited se-
quence, during which it attends to the edit plan. Through this, we
eï¬€ectively encourage the model to learn to better reason about ed-
its and how they should be applied to form the target sequence. Us-
ing this objective, we develop CoditT5, a large language model for
software-related edit tasks that is pretrained on more than 5.9 mil-
lion open-source programming language code snippets and 1.6 mil-
lion natural language comments from the CodeSearchNet [22] train-
ing data.

For evaluation, we ï¬ne-tune CoditT5 on three downstream tasks:
comment updating, bug ï¬xing, and automated code review. For
each of these tasks, we show that CoditT5 outperforms state-of-
the-art models as well as large pretrained standard generation-based
models. Through this, we demonstrate that our model and the pro-
posed edit-based pretraining objective generalize across tasks and
are better suited for editing tasks in the software domain.

Furthermore, in our evaluation, we ï¬nd that our edit-based model,

CoditT5, can be further improved if combined with a standard
generation-based model. We ï¬nd that the edit-based and standard
generation-based models are complementary to one another. Namely,
while the edit-based model provides better explicit modeling of
concrete edits, a standard generation-based model provides certain
advantages in terms of the contextual coherence of the generated
target sequence. To exploit this complementary nature of these
models, we combine the two models through reranking strategies
which require no additional training. Our results show that the
combined approaches outperform the two models individually by
up to 19.35%.
We summarize our main contributions as follows:
â€¢ We formulate a novel pretraining objective that entails ï¬rst gen-
erating a plan consisting of edit operations to be applied to the
input sequence followed by the resulting target sequence.

â€¢ We build and release CoditT5, a large language model for software-
related editing tasks that is pretrained on large amounts of source
code and natural language with the new pretraining objective.
â€¢ Upon task-speciï¬c ï¬ne-tuning, we show that CoditT5 achieves
improved performance over existing models for three distinct
downstream editing tasks (comment updating, bug ï¬xing and au-
tomated code review), demonstrating its eï¬€ectiveness and gen-
eralizability.

Before Editing

default List<Pattern> getExcludedResponseHeaderPatterns() {

return emptyList();

}
Reviewerâ€™s Comment
Generally better to qualify than making static import
PLBART
default List<Pattern> getExcludedResponseHeaderPatterns() {

return emptyList();

}

Figure 1: An example in automated code review task where
PLBART merely copies the input which does not match re-
viewerâ€™s comment.

â€¢ We show that by combining our edit-based CoditT5 model with
a standard generation model through simple reranking strate-
gies, we can beat each of the individual models and achieve new
state-of-the-art in all three tasks, demonstrating the complemen-
tary nature of edit-based and standard generation models.

Our code and data is publicly available at
https://github.com/EngineeringSoftware/CoditT5.

2 BACKGROUND
We ï¬rst give a high-level overview of the building blocks that are
necessary to understand our approach.

2.1 Generation with Transformer-Based

Models

Conditional Sequence Generation. Conditional sequence genera-
tion entails generating an output sequence given an input sequence.
Many tasks are framed in this manner, including machine transla-
tion (e.g., translating a sentence from French to English) [2], text
summarization (e.g., generating a brief summary for a given news
article) [46], and code generation (e.g., generating a code snippet
for a given natural language speciï¬cation) [58].

Encoder-Decoder Framework. In recent years, conditional sequence

generation tasks are being addressed with encoder-decoder mod-
els. An encoder-decoder model consists of two neural components:
an encoder and a decoder. The input sequence is fed into the en-
coder, which produces learned vector representations of the tokens
in that sequence. These learned vector representations are then
passed into the decoder, which generates the output sequence one
token at a time. Speciï¬cally, the decoder predicts the next token
by reasoning over the input sequence and the tokens generated at
previous time steps.

Transformers. Transformers [52] are powerful neural models that
are commonly adopted as the encoder and decoder in the encoder-
decoder framework. These models rely on an attention mechanism
to learn representations for tokens by relating them to other to-
kens in the sequence. Namely, a transformer-based encoder will
learn representations for each token in the input sequence by â€œat-
tendingâ€ to other input tokens. For the decoder, when generating
a token at timestep ğ‘¡, it will â€œattendâ€ to the representations of the

CoditT5: Pretraining for Source Code and Natural Language Editing

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

@param users List of user objects

noising function

@param [MASK] List of objects

Encoder

Decoder

<ReplaceOld> [MASK] <ReplaceNew> users <ReplaceEnd> <Insert> user <InsertEnd>

1

<s>
@param users List of user objects

2

Figure 2: The corrupted text is encoded with a bidirectional encoder, and the decoder is pretrained to generate sequences of
edit actions to recover the original text followed by a separation token (<s>), and ï¬nally the target sequence

output tokens generated from timestep 1 to ğ‘¡ âˆ’ 1 as well as the rep-
resentations of tokens from the input sequence. Transformer mod-
els can become very large with huge numbers of attention heads,
encoder and decoder layers.

2.2 Large Pretrained Language Models
Large pretrained language models generally refer to the class of
large transformer-based models that are trained on large amounts
of unlabeled data (collected from webpages, news articles, etc.) with
unsupervised training objectives. This includes a vast number of
models like GPT [7, 43], BART [26], and T5 [44].

Denoising Autoencoder Pretraining. BART and T5 models are pre-
trained using denoising autoencoding unsupervised training objec-
tives. Namely, a noising function is ï¬rst applied to a given input se-
quence inp to form inpâ€². Common noising functions include Token
Masking: tokens in the input sequence are randomly masked; To-
ken Deletion: random tokens are deleted from the input sequence;
Token Inï¬lling: a span of tokens are sampled and replaced with a
mask token; Sentence Permutation: sentences in the document are
shuï¬„ed in a random order. Then, inpâ€² is fed into a modelâ€™s encoder,
and the encoderâ€™s learned representation is passed into the decoder,
which generates an output sequence, out, that is expected to resem-
ble the original input sequence (inp). In other words, the model is
trained to â€œdenoiseâ€ inpâ€², using a training objective that minimizes
the error between out and the original input, inp. Through this, the
model learns to extract meaning from the input sequence and also
generate ï¬‚uent and coherent output. Therefore, by pretraining on
massive amounts of data, the model develops an understanding of
how things in the world relate to one another as a strong language
modeling capability.

Fine-tuning for Downstream Tasks. Since large pretrained lan-
guage models are trained using unsupervised training objectives
on huge amounts of data, they cannot generally be directly ap-
plied to downstream tasks (e.g., translation, summarization). Fine-
tuning is a common technique to transfer the knowledge learned
during pretraining to target downstream tasks. Speciï¬cally, the
pretrained model is further trained for the downstream task on
some amount of supervised data.

2.3 Large Pretrained Language Models for

Software Engineering

Inspired by the success of large pretrained models in Natural Lan-
guage Processing (NLP), a number of machine learning models pre-
trained on source code and technical text have been proposed for
solving various software-related problems.

For instance, inspired by BART, Ahmad et al. [1] developed PLBART,

which is a large pretrained language model that can be ï¬ne-tuned
for a number of code understanding (e.g., code summarization)
and generation (e.g., code translation) tasks. Similarly, inspired by
T5, Wang et al. [55] built a larger model CodeT5, which is pre-
trained on six programming languages together with their natural
language comments collected from open-source repositories. Spe-
cially, it is pretrained to incorporate information from identiï¬ers
in the code. CodeT5 has shown promising results in code-related
generation tasks such as code summarization, code generation and
code-related understanding tasks such as clone detection and vul-
nerability identiï¬cation. However, aforementioned models are for
generation and they are only implicitly aware of edit operations if
at all.

3 CODITT5
CoditT5 is built upon the encoder-decoder framework with the
same architecture as CodeT5. As shown in Figure 2, the model is
pretrained with our proposed objective: generating the edit-based
output sequence given the corrupted input sequence. In this sec-
tion, we ï¬rst explain our proposed pretraining objective (Section 3.1).
We then discuss how we build CoditT5 by pretraining on this ob-
jective, including the data used for pretraining (Section 3.2), and
additional details of the pretraining setup (Section 3.3).

3.1 Pretraining Objective
We formulate a new pretraining objective that is designed to en-
courage a model to explicitly reason about edits. At a high-level,
this objective falls under the realm of denoising autoencoding in
which an input sequence is ï¬rst corrupted with noising functions
and the model is trained to denoise the corrupted sequence by gen-
erating an output sequence that matches the original input sequence.

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric

While existing models like PLBART and CodeT5 pretrained using
this setup perform very well on various generation tasks (e.g., code
summarization/generation), we ï¬nd that they do not generalize
well when ï¬ne-tuned on editing tasks. Namely, they are susceptible
to learning to copy the original input sequence instead of actually
performing edits, up to 34.25% of the time (Table 3).

We propose the following edit-based output sequence representa-
tion (shown in Figure 2): [Edit Plan] <s> [Target Sequence], where
the model is trained to generate an edit plan ( 1 ) consisting of
explicit edit operations that must be applied to the corrupted se-
quence to reconstruct the original input sequence, followed by a
separation token (<s>), and ï¬nally the target sequence ( 2 ) that
matches the original input sequence. This is inspired by the con-
cept of content planning, originating from natural language gener-
ation [45]. In content planning, a high-level plan is ï¬rst outlined,
specifying the discourse structure of the content to be generated,
and then lexical realization is performed to generate the text.

3.1.1 Edit Plan. The edit plan entails the speciï¬c edit operations
that are needed to recover the original input sequence. For exam-
ple, in Figure 2, the input sequence: â€œ@param users List of user ob-
jectsâ€ is corrupted by masking â€œusersâ€ and removing token â€œuserâ€:
â€œ@param [MASK] List of objectsâ€. With this, a model must ï¬rst rea-
son about the fact that [MASK] in the corrupted input sequence
needs to be replaced with â€œusersâ€ and â€œuserâ€ should be inserted
between â€œofâ€ and â€œobjectsâ€ when producing the target sequence.
To construct the sequence of edit operations, we closely follow the
format proposed by Panthaplackel et al. [40]:

<Operation> [span of tokens] <OperationEnd>

Here, <Operation> is either Insert or Delete. We also include the
Replace operation, with a slightly diï¬€erent structure (since both
the old content to be replaced as well as the new content to replace
it with must be speciï¬ed):

<ReplaceOld> [span of old tokens]
<ReplaceNew> [span of new tokens] <ReplaceEnd>

To determine the speciï¬c edit operations for a given example, we
use diï¬„ib1 to compute the optimal set of edits needed to transform
the corrupted input sequence into the original input sequence. Mul-
tiple edit operations are placed in the same order as the span of
tokens under editing appears in the input sequence (for example,
the edit plan in Figure 2 consists of two edit operations).

3.1.2 Target Sequence. One might ask whether we could simply
apply the sequence of edit operations in the generated edit plan
to the corrupted input sequence directly to recover the original in-
put sequence heuristically. For example, if we align â€œ<ReplaceOld>
[MASK] <ReplaceNew> user <ReplaceOld>â€ with a corrupted
input sequence â€œ@param [MASK] List of user objectsâ€, it is very
clear that all we need to do is replace [MASK] with â€œuserâ€and no
additional generation is needed. However, there are two main is-
sues with this. First, not all operations will be speciï¬ed in a deter-
ministic manner. For example, if the edit plan is â€œ<Insert> user
<InsertEnd>â€, it is not clear where the new token â€œuserâ€ should
be added to. Second, the generated edit plan does not correspond

1

https://docs.python.org/3/library/diï¬„ib.html

Table 1: Statistics collected from downstream tasks for cre-
ating pretraining dataset. Avg. No. of Tokens represents the
average number of tokens in each edited span; Avg. No. of
Spans represents the average number of edited spans in each
input sequence.

Probability of Delete edit
Probability of Insert edit
Probability of Replace edit
Avg. No. of Tokens
Avg. No. of Spans

PL NL

0.49
0.21
0.30
6.50
1.90

0.07
0.11
0.82
3.00
1.40

to contiguous output tokens since it consists of fragmented infor-
mation (edit operations and token spans) rather than a complete
sentence. As a result, neural language models may fail to generate
correct edit plans due to their lack of language properties such as
ï¬‚uency and coherency [40].

Therefore, we need an additional step for learning to apply edits
while simultaneously maintaining ï¬‚uency and coherency. For this
reason, once the edit plan is outlined as a sequence of edit opera-
tions, the target sequence (which is expected to recover the original
input sequence) must also be generated: â€œ@param users List of user
objectsâ€. The decoder generates tokens in a left-to-right manner,
meaning that when generating a token at a given timestep, it is
aware of all tokens generated in previous timesteps. So, when gen-
erating the target sequence, the decoder can exploit the sequence
of edits that was generated in the edit plan earlier. In this way, the
model can reason the edits and the generation simultaneously.

3.1.3 Noising Functions. To support learning across a diverse set
of edit actions during pretraining, we consider multiple noising
functions for corrupting the input sequence: 1) randomly mask-
ing spans with the special [MASK] token which requires the model
to replace it with the correct spans, 2) inserting [MASK] token at
random positions which requires the model to identify the useless
spans and delete them and 3) deleting spans of tokens in the input
sequence which requires the model pinpoint the position and add
back the missing pieces.
3.2 Pretraining Data
3.2.1 Data Collection. Following prior work, we pretrain CoditT5
on large amounts of source code and natural language comments
from the CodeSearchNet [22] dataset which consists of functions
of six programming languages (Java, Python, Ruby, Php, Go and
JavaScript) together with the natural language comments. Code-
SearchNet is widely used to pretrain large language models, such
as CodeT5 [55] and UniXcoder [18]. We use the training set of
the processed CodeSearchNet dataset provided by Guo et al. [18]
which contains 6.1 million programming languages code snippets
(functions/methods) and 1.9 million natural language comments.

3.2.2 Data Preparation. To enable CoditT5 to capture common
edit patterns, we want the pretraining dataset to reï¬‚ect the com-
mon activities conducted by software developers. Speciï¬cally, in
the pretraining dataset, the probability of each edit operations ap-
plied to the spans in the input sequence and the length (number of

CoditT5: Pretraining for Source Code and Natural Language Editing

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Table 2: Statistics of the datasets used to pretrain CoditT5.
First row: number of programming language and natural
language; second row: average number of tokens in cor-
rupted input sequences; third row: average number of to-
kens in the output sequence (edit plan + target sequence).

PL

NL

Examples
Avg. C-Tokens
Avg. O-Tokens

5,956,069
102.01
120.23

1,675,277
15.42
26.57

tokens) of the corrupted span should be consistent with the distri-
butions and sizes of real-world edits in downstream editing tasks.
To this end, we collect statistics for source code edits from the
training sets of the bug ï¬xing and automated code review down-
stream tasks and statistics for natural language edits from the com-
ment updatingâ€™s training set. As shown in Table 1, we collect the
probability of each edit operation (insert, delete and replace) to be
performed on a span; the average number of tokens in each span
that is edited; and the average number of spans that are edited in
each input sequence. For each example in the pretraining dataset,
we then uniformly sample the spans and the edit operations that
should be applied in accordance with the statistics collected from
the downstream datasets.

Similar to CodeT5 [55], we use the RoBERTa [30] tokenizer to
tokenize all sequences (input, edit plan, target). More concretely,
the tokenizer splits words in the sequence into tokens (subwords)
that are used by the model. Moreover, we remove input sequences
that are shorter than 3 tokens and longer than 512 tokens after to-
kenization which leave us with 5.9 million programming language
code snippets and 1.6 million natural language comments. This is
because too short inputs are usually incomplete and CodeT5 is de-
signed to only handle sequence of length 512. Table 2 presents the
statistics of the pretraining dataset.

3.3 Pretraining Setup

Model Architecture. CoditT5 consists of 12 encoder and decoder
layers, 12 attention heads, and a hidden dimension size of 768. The
total number of parameters is 223M. Model parameters are initial-
ized from the CodeT5-base model, and we further pretrain it on
the CodeSearchNet pretraining dataset (Section 3.2) using our pro-
posed objective (Section 3.1).

Training. We implement CoditT5 using PyTorch 1.9.0 and use
16 NVidia 1080-TI GPUs, Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz
for pretraining for 4 days. For ï¬ne-tuning, we run the experiments
on 4 NVidia 1080-TI GPUs, Intel(R) Xeon(R) CPU E5-2620 v4 @
2.10GHz with the same hyper-parameters as CodeT5.

4 EXPERIMENTAL DESIGN
To assess CoditT5 and our proposed pretraining objective, we ï¬ne-
tune the model on three software-related downstream tasks. Note
that during ï¬ne-tuning, the model is still trained to generate the
edit-based output sequence. However, at test time, we discard the
edit plan and take the generated target sequence as the ï¬nal model

Table 3: Percentage that model just copy the input.

Models

PLBART CodeT5 CoditT5

ğµ2ğ¹ğ‘ 
ğµ2ğ¹ğ‘š
Comment Updating (clean)
Comment Updating (full)
Automated Code Review

6.48
10.92
21.33
34.25
22.24

7.97
10.08
16.67
25.47
29.28

0.55
0.78
2.67
5.73
1.28

Table 4: Statistics for the datasets used for downstream
tasks.

Task

Comment Updating

Bug Fixing

Train Valid Test

16,494
16,494

46,628
52,324

1,878
1,878

5,828
6,542

150
1,971

5,831
6,538

clean
full

ğµ2ğ¹ğ‘ 
ğµ2ğ¹ğ‘š

Automated Code Review

13,753

1,719

1,718

output. Namely, we use the generated sequence after the separa-
tion token <s> as modelâ€™s prediction.

4.1 Downstream Tasks

Comment Updating. The task of comment updating entails auto-
matically updating a natural language comment to reï¬‚ect changes
in the corresponding body of code [40]. For instance, in Example 2
in Figure 5, the old @return comment needs to be revised based
on the changes in the method. Instead of directly returning the
yaw Euler angle measured in radians, the unit of the return value
is changed to degrees in the new version, with the method call
Math.toDegrees().

Bug Fixing. Given a buggy code snippet, the task of bug ï¬xing
entails generating a ï¬xed code snippet, which no longer contains
the bug [48].

Automated Code Review. Given a code snippet under review and
a brief natural language sentence prescribing code edits, automated
code review requires automatically generating the revised code
snippet, which captures the recommended changes [51]. For exam-
ple, in Figure 1, emptyList() should be changed to Collections.-
emptyList() because the reviewer suggests not using static im-
port.

4.2 Data for Downstream Tasks
We use datasets that have been established and previously used for
each of the three tasks. The statistics of the datasets is shown in Ta-
ble 4. Unlike pretraining where the goal is to recover the corrupted
input sequences, during ï¬ne-tuning, CoditT5 is trained to gener-
ate an edit plan for completing the downstream editing task, that
can be applied to a part of the input (e.g., old comment), followed
by the target sequence (e.g., new comment).

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric

Comment Updating. For this task, Panthaplackel et al. [39] has
released a corpus of Java method changes paired with changes
in the corresponding comments (spanning @return, @param, and
summary comments). This dataset also comes with a clean subset
of the test set which was manually curated. The input sequence
used for ï¬ne-tuning is formed by concatenating the old comment
and code edits. The code edits follow the representation described
in Section 3.1.1, except that an additional Keep operation is in-
cluded to denote spans that are left unchanged.

Bug Fixing. We consider the Java BugFixPairs-Small (ğµ2ğ¹ğ‘  ) and
BugFixPairs-Medium (ğµ2ğ¹ğ‘š) datasets, originally released by Tu-
fano et al. [48]. Chakraborty and Ray [8] supplemented these datasets
with additional context, namely natural language guidance from
the developer, and the method where the patch should be applied.
ğµ2ğ¹ğ‘  contains shorter methods with a maximum token length 50,
and ğµ2ğ¹ğ‘š contains longer methods with up to 100 tokens in length.
The input sequence used for ï¬ne-tuning is formed with the buggy
code, natural language guidance, and code context.

Automated Code Review. We use the automated code review dataset

released by Tufano et al. [51], which consists of Java methods (be-
fore and after the review) paired with pull request comments, de-
rived from pull request reviews on GitHub and Gerrit. To reduce
the vocabulary size, they further abstracted Java methods by re-
placing identiï¬ers and literals with special tokens. In this work,
we use the data with concrete tokens. The input sequence used for
ï¬ne-tuning is formed using the code snippet before review and the
pull request comment from reviewers.

4.3 Baselines
4.3.1 Generation Baselines. We consider two large standard gen-
eration language models trained with denoising autoencoding pre-
training objectives which are not edit-based: PLBART and CodeT5.
Both of these are ï¬ne-tuned to directly generate the target output
sequence. Furthermore, to better assess the value of actually pre-
training using the proposed objective instead of simply ï¬ne-tuning
a model to generate an edit-based output sequence, we also con-
sider ï¬ne-tuning CodeT5 to generate the specialized edit-based
output sequence representation. We refer to this as CodeT5 (w/
edit-based output). We ï¬ne-tune each of these models using the
same input context as CoditT5.

4.3.2 Task-Specific Baselines. We additionally compare against the
state-of-the-art models for each of the downstream tasks.

For comment updating, the state-of-the-art model is Panthap-
lackel et al. [40], which entails Recurrent Neural Network (RNN)
based encoders for representing the old comment and code ed-
its, and an RNN-based decoder for decoding edits. These edits are
parsed at test time and reranked based on similarity to the old com-
ment and likelihood based on a comment generation model.

For bug ï¬xing, the state-of-the-art model is essentially PLBART

ï¬ne-tuned on the ğµ2ğ¹ğ‘  and ğµ2ğ¹ğ‘š to generate the ï¬xed code [8].

For automated code review, no baselines are available for the
speciï¬c version of the dataset we used with concrete identiï¬ers
and literals (rather than the one with abstracted identiï¬ers and
literals). Therefore, we rely on those described in Section 4.3.1 and
establish new baselines for this version of the dataset.

Before Editing:

public HashConfigurationBuilder capacityFactor (float capacityFactor) {

if ( numSegments < 0 )

throw new IllegalArgumentException ("capacityFactor must be positive");

this.capacityFactor = capacityFactor ;
return this;

}
Reviewerâ€™s Comment:

typo: capacityFactor instead of numSegments

CodeT5:
public HashConfigurationBuilder capacityFactor(float capacityFactor) {

this.capacityFactor = capacityFactor;
return this;

}
CoditT5:
public HashConfigurationBuilder capacityFactor (float capacityFactor) {

if ( capacityFactor < 0 )

throw new IllegalArgumentException ("capacityFactor must be positive") ;

this.capacityFactor = capacityFactor;
return this;

}

Figure 3: Comparing the output of CodeT5 and CoditT5
for a automated code review example. CodeT5 generates in-
correct output that drastically deviates from the input code
while CoditT5 generates the correct output, performing
only relevant edits.

4.4 Evaluation Metrics
For comment updating, we report performance on the same met-
rics that have been used previously to benchmark models for this
task [40]. This includes: xMatch (whether the model prediction ex-
actly matches the ground truth), common metrics that measure lex-
ical overlap for evaluating text generation (BLEU-4 2 [41] and ME-
TEOR [3]), and common metrics for measuring text editing (GLEU [35]
and SARI [56]). For bug ï¬xing, we use xMatch, as done in prior
work [8]. For automated code review, we report performance on
xMatch and BLEU-4, which have been used previously to bench-
mark models for this task [51].

5 EVALUATION
We organize our evaluation around three main research questions:
RQ1: How does our edit-based model, CoditT5, compare to gen-
eration and task-speciï¬c baselines for edit-related tasks?
RQ2: Does our proposed pretraining objective help a model in bet-
ter reasoning about and performing edits?
RQ3: Can a standard generation model complement CoditT5 by
integrating the two models?

5.1 Comparing CoditT5 to Baselines
We present results in Tables 5-8. Note that the results shown in the
last two rows in each of the tables are explained later in Section 5.3.
We perform statistical signiï¬cance testing using bootstrap tests [4]
with conï¬dence level 95%.

RQ1: How does our edit-based model, CoditT5, compare to
generation and task-speciï¬c baselines for edit-related tasks?

2

We measure 1âˆ¼4-gram overlap and compute the average.

CoditT5: Pretraining for Source Code and Natural Language Editing

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Table 5: Results for comment updating on the clean test set. The results with the same preï¬xes (e.g., ğ›½) are NOT statistically
signiï¬cantly diï¬€erent.

Models

xMatch BLEU-4 METEOR GLEU

SARI

Panthaplackel et al. [40]
PLBART
CodeT5
CodeT5 (w/ edit-based output)
CoditT5

CoditT5 (reranked with CodeT5)
CodeT5 (reranked with CoditT5)

33.33
35.33
38.00
40.00
43.33ğœ’

45.33
44.00ğœ’

56.55
62.04ğ›¾
65.20ğ›¼
62.97ğ›¾
64.56

66.80
65.58ğ›¼

52.26
56.79
59.63
59.08
60.75

63.33
62.44

51.88
54.75
58.84ğ›½
58.72ğ›½
59.53

61.60
60.48

56.23
52.83
58.80
61.11ğœ– ğœ‚
61.41ğ›¿ ğœ–

61.48ğ›¿ ğœ‚
62.57

Table 6: Results for comment updating on the full test set. The results with the same preï¬xes (e.g., ğ›½) are NOT statistically
signiï¬cantly diï¬€erent.

Models

xMatch BLEU-4 METEOR GLEU SARI

Panthaplackel et al. [40]
PLBART
CodeT5
CodeT5 (w/ edit-based output)
CoditT5

CoditT5 (reranked with CodeT5)
CodeT5 (reranked with CoditT5)

24.81
22.98
28.56
29.83ğ›¿ğ›¾
29.38ğ›¿

30.14ğ›¾
27.80

48.89
55.42â„ğœ„
58.37ğ›¼
54.83
55.30ğ›½ ğœ„

58.72ğ›¼
55.54ğ›½ â„

44.58
49.12
53.13
50.71
51.14ğœ’

53.60
51.44ğœ’

45.69
47.83
51.90
50.67ğœ–
50.62ğœ–

52.81
50.02

47.93
43.40
49.23
52.01ğœ‚
51.39

50.47
52.24ğœ‚

Table 7: Results on bug ï¬xing dataset. The results with the
same preï¬xes (e.g., ğ›½) are NOT statistically signiï¬cantly dif-
ferent.

Table 8: Results for automated code review. The results with
the same preï¬xes (e.g., ğ›½) are NOT statistically signiï¬cantly
diï¬€erent.

xMatch

Models

xMatch BLEU-4

Models

PLBART
CodeT5
CodeT5 (w/ edit-based output)
CoditT5

ğµ2ğ¹ğ‘ 

31.09
34.81
36.37
37.52

CoditT5 (reranked with CodeT5)
CodeT5 (reranked with CoditT5)

40.22
39.56

ğµ2ğ¹ğ‘š

24.18
26.66
29.28ğ›¼
29.96ğ›¼

32.06ğ›½
32.24ğ›½

PLBART
CodeT5
CodeT5 (w/ edit-based output)
CoditT5

CoditT5 (reranked with CodeT5)
CodeT5 (reranked with CoditT5)

26.78
34.98
36.38ğ›¼
37.19ğ›¼

40.98
43.42

79.38
83.20
80.06ğ›½
80.50ğ›½

84.12ğœ’
83.92ğœ’

We ï¬nd that CoditT5 (and most of the pretrained models) dras-
tically outperforms Panthaplackel et al. [40] (a non-pretrained model)
across metrics for comment updating. This demonstrates the value
of large language model pretrained on vast amounts of data using
unsupervised pretraining objectives.

Next, across all three tasks, CoditT5 achieves higher perfor-
mance than the two standard generation-based pretrained models,
signiï¬cantly outperforming PLBART and CodeT5 for most of the
metrics, highlighting the beneï¬t of explicitly modeling edits for
these editing tasks. In fact, CodeT5 (w/ edit-based output), which
explicitly models edits only during ï¬ne-tuning rather than pre-
training, outperforms CodeT5 on edit-based metrics (xMatch, SARI).

This further underlines the utility of the edit-based output sequence
representation that we developed.

Nonetheless, across most metrics, CoditT5 still outperforms CodeT5

(w/ edit-based output), which is not pretrained using the pretrain-
ing objective but uses the same edit-based output sequence rep-
resentation during ï¬ne-tuning. This demonstrates the importance
of actually pretraining with this representation rather than relying
on ï¬ne-tuning alone.

5.2 Evaluating our Pretraining Objective
While we observe that CoditT5 tends to achieve slightly lower
performance than CodeT5 on generation-based metrics (BLEU-4,
METEOR) for two of the tasks, we ï¬nd that it signiï¬cantly out-
performs other metrics which capture whether the correct edits

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric

Example 1
Before Editing:

protected boolean isProcessed(ChronicleLogOffsetTracker tracker, long offset) {

long last = tracker.readLastCommittedOffset();
return ( last > 0 ) && ( last >= offset );

}
Reviewerâ€™s Comment:
No need for parentheses.
Edit plan

hDeletei ( hDelete_Endi hDeletei ) hDelete_Endi
Target sequence:

protected boolean isProcessed(ChronicleLogOffsetTracker tracker, long offset) {

long last = tracker.readLastCommittedOffset();
return last > 0 && last >= offset;

}

Example 2
Before Editing:

public Builder setDataSize(Estimate dataSize) {

this.dataSize = requireNonNull(dataSize, "dataSize can not be null");
return this;

}
Reviewerâ€™s Comment
you donâ€™t validate in other builders method (and you donâ€™t have to)
Edit plan

hDeletei equireNonNull(dataSize, "dataSize can not be null"); hDelete_Endi
Target sequence:

public Builder setDataSize(Estimate dataSize) {

this.dataSize = dataSize;
return this;

}

Figure 4: Examples for automated code review for which
CoditT5 generated ambiguous or erroneous edit plans but
still managed to generate the correct target sequences.

Table 9: Percentages of target sequence generated by
CoditT5 being consistent with the edit plan.

Datasets

Is Consistent (%)

ğµ2ğ¹ğ‘ 
ğµ2ğ¹ğ‘š
Comment Updating (clean)
Comment Updating (full)
Automated Code Review

92%
88%
87%
85%
74%

are generated, such as xMatch and GLEU and SARI for comment
updating. This suggests that CoditT5 is indeed better at editing.
By inspecting the outputs of the two models, we ï¬nd that CodeT5
tends to make drastic and unnecessary edits while CoditT5 ap-
pears to be better at making more ï¬ne-grained edits. For example,
in Figure 3, CodeT5 generates output that completely discards crit-
ical statements in the code, whereas CoditT5 is able to correctly
localize the part of the input code that needs to be changed and
make editions properly. We attribute this to the fact that CodeT5
is not designed to reason about edits while CoditT5 is. We fur-
ther evaluate the inï¬‚uence of our proposed pretraining objective
on this editing capability.

RQ2: Does our proposed pretraining objective help a model in
better reasoning about and performing edits?

First, we compare how often CoditT5 naively copies the input
content without actually performing any edits, to two pretrained
models which use generation-based pretraining objectives. We re-
port the percentages in Table 3. By copying substantially less of-
ten than the PLBART and CodeT5, we ï¬nd that CoditT5 learns to
more frequently perform edits with our proposed edit-based pre-
training objective which indicates it is suitable for editing tasks.

CoditT5â€™s decoder is encouraged to generate a target sequence
that follows the outlined edit plan; however, we do not constrain
the decoder in any way to do this.3 Nonetheless, we ï¬nd that in
the majority of cases (74%-92%), the target sequence is consistent
with the edit plan, as shown in Table 9. More concretely, the target
sequence generally resembles what would be produced if the edit
operations in the edit plan were applied to the original content.
This suggests that the pretraining objective does in fact guide the
model in reasoning about edits.

For cases in which there is ambiguity or errors in the edit plan,
we ï¬nd that CoditT5 still often manages to generate the correct
target sequence, by disregarding unreasonable edits or disambiguat-
ing ambiguous edits. We show two examples in automated code
review in Figure 4 with the Java method before review, the gen-
erated edit plan, and the generated target sequence. In Example
1, the edit plan is ambiguous since there are multiple instances of
â€œ(â€ and it does not specify which one(s) should be deleted. How-
ever, the generated target sequence is correct, as the model was
able to correctly reason about the most appropriate edit locations.
In Example 2, the edit plan is imprecise and blindly following this
plan would result in syntactically incorrect code, but the model still
managed to perform the correct edits and produced valid output
by ignoring the fallacious edit. Overall, we ï¬nd that both compo-
nents of the edit-based output sequence representation used in the
pretraining objective (edit plan and target sequence) are critical.

5.3 Integrating CoditT5 and CodeT5
CoditT5 is designed to complement a generation model by pro-
viding more explicit guidance for edits. However, a model that is
trained to generate edits can struggle with coherence and ï¬‚uency
since it is not actually trained to generate consecutive text [40]. By
including the generation of the target sequence in the pretraining
objective, we do mitigate this to some extent, even when there are
ambiguities or errors in the edit plan. However, there appears to
be a trade-oï¬€ between performing the correct edits while maintain-
ing performance with respect to generation metrics. More speciï¬-
cally, in Tables 5-8, CoditT5 outperforms CodeT5 with respect to
xMatch (and SARI for comment updating), but underperforms with
respect to BLEU-4. To exploit the slight superiority of CodeT5 in
this respect, we consider incorporating CodeT5 into our approach.

RQ3: Can a pure generation model complement CoditT5 by
integrating the two models?

3

We do not want potential errors in the edit plan to propagate to the target sequence.

CoditT5: Pretraining for Source Code and Natural Language Editing

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Example 1
Buggy Code

public List<TagVFilter> getFilters() {

if ((filters) == null ) {

filters = new ArrayList<TagVFilter>();

}
return filters;

}
CoditT5:
public List<TagVFilter> getFilters() {

if ((filters) == null ) {

filters = new ArrayList<TagVFilter>();

}
return new ArrayList(filters);

}
CoditT5 (reranked with CodeT5):
public List<TagVFilter> getFilters() {

if ((filters) == null ) {

filters = new ArrayList<TagVFilter>();

}
return new ArrayList<TagVFilter>(filters);

}

Example 2
/** @return double The yaw Euler angle. */
public double getRotY() {

return mOrientation.getRotationY();

}

/** @return ? */
public double getRotY() {

return Math.toDegrees(mOrientation.getRotationY());

}
CodeT5: @return double The yaw Euler angle.

Reranked CodeT5: @return double The yaw Euler angle in degrees.

Figure 5: Examples from comment updating and bug ï¬xing
which demonstrate the impact of reranking.

5.3.1 Experimental Setup. We combine the two models using sim-
ple likelihood-based reranking strategies at test time (with no ad-
ditional training). Namely, at test time, CoditT5 and CodeT5 each
generate 20 candidates using beam search. While we have been
only looking at the top one prediction for all previous experiments,
we will consider all 20 candidates for reranking. We compute a
reranking score for each of these to essentially re-score them. The
candidate which has the highest reranking score will be the ï¬nal
model prediction. We investigate two diï¬€erent reranking strate-
gies:

CoditT5 (reranked with CodeT5): To exploit the language-speciï¬c
norms learned by CodeT5, we rerank the candidates generated by
CoditT5 based on the probability score CodeT5â€™s language model
assigns to the corresponding target sequences (namely after <s>).
We compute the length-normalized conditional log probability
score of CodeT5 generating the target sequence, conditioned on
the same input:

ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ = ğ‘™ğ‘œğ‘”(ğ‘ƒ (ğ‘‡ |ğ¼ )

1
ğ‘ )

where ğ‘‡ is the target sequence, ğ¼ is the modelâ€™s input, ğ‘ is the
length of ğ‘‡ . We also length-normalize the log probability of the
candidate, as scored by CoditT5, and then add the two probability
scores together to obtain the reranking score.

CodeT5 (reranked with CoditT5): Conversely, we also rerank the
output of CodeT5 based on the likelihood of CoditT5, such that
the generated sequence can be assessed in terms of explicit edits.
We ï¬rst parse the output of CodeT5 into the edit-based output se-
quence representation (as described in Section 3.1.1) and then con-
catenate it with the modelâ€™s output using <s>. Then we compute
the likelihood of CoditT5 generating this sequence, conditioned
on the same input. We then add the length-normalized log probabil-
ity score of CoditT5 with the score originally assigned by CodeT5
(after length-normalizing and applying log).

5.3.2 Results. We provide results in the bottom two rows of Ta-
bles 5-8. By reranking the output of CoditT5 using CodeT5, we
are able to achieve improved performance on all the metrics in-
cluding BLEU-4 across tasks (and the other generation-based met-
ric, METEOR, for comment updating). To illustrate this, consider
Example 1 in Figure 5, with a buggy code snippet and outputs corre-
sponding to CoditT5 before and after reranking. We observe that
CoditT5 correctly localizes the bug and correctly identiï¬es that
the edit entails initializing an ArrayList in the return statement.
However, the generated target sequence is a defective code snippet
which does not properly initialize an ArrayList with the correct
type TagVFilter. By leveraging CodeT5â€™s likelihood score, we are
able to eï¬€ectively ï¬lter out the defective prediction and obtain the
correct output.

By reranking the output of CodeT5 using CoditT5, we see sig-
niï¬cant improvements with respect to CodeT5 on metrics that more
directly evaluate whether the correct edits were performed, includ-
ing xMatch as well as GLEU and SARI for comment updating. This
suggests that the edit-based and generation-based models are in-
deed complementary to one another. As a case study, consider Ex-
ample 2 in Figure 5. CodeT5 produces a sequence which simply
copies the old comment, without capturing the code changes. While
this may be a likely comment sequence, according to CodeT5â€™s lan-
guage model, copying without applying any edits is not a likely
edit plan to be generated for CoditT5.

By combining CoditT5 and CodeT5 through reranking, we can
further boost performance substantially across most metrics for
all three tasks, outperforming the two models individually, and
achieving new state-of-the-art.

6 LIMITATIONS

Other Programming Languages. The downstream editing tasks
we studied in this work are using Java. Since CoditT5â€™s pretrain-
ing is on the dataset consisting of six programming languages, we
expect it to also perform well on editing tasks in other program-
ming languages, but we leave empirically verifying this as future
work.
Data Contamination. CoditT5 is pretrained on data collected
from open-source projects. It is possible that similar examples in
pretraining data exist in downstream tasksâ€™ test set. While prior
work [7] has shown that data contamination may have little im-
pact on the performance of pretrained models in natural language
processing tasks, future work can investigate this problem for pre-
trained models for software engineering.

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric

7 RELATED WORK
In this section, we consider the most closely related work on learn-
ing edits, large pretrained models for code, pretrained models for
code edits and combining complementary models.
Learning Edits. Prior work has studied learning edits in both nat-
ural language and programming language. We followed the ap-
proach of explicitly representing edits as sequences with edit ac-
tions. Our edit representation is inspired by Panthaplackel et al.
[39, 40], who studied learning comment edits based on code edits.
Brody et al. [6], Chen et al. [10], Tarlow et al. [47], Yao et al. [57]
represented code as ASTs (abstract syntax trees) and the code ed-
its as edit actions over the AST nodes rather than tokens. We do
not focus on editing structured data (AST) as it can not be gener-
alized to natural language, and it can not be easily combined with
large pretrained models which are primarily based on sequence of
tokens.

Alternatively, edits can be encoded into vector representations
(or embeddings). Guu et al. [20] studied learning edit embeddings
for natural language generation in a prototype-then-edit style. Yin
et al. [59] studied learning code edits as embeddings and then ap-
plying them to natural language insertion and code bug ï¬xing.
Hashimoto et al. [21] developed a retrieve-and-edit framework for
text-to-code generation, where the edits are learned as parameters
of a seq2seq model. Similarly, Li et al. [27] proposed a retrieve-
and-edit framework for code summarization task where the model
ï¬rst learns an edit vector and then generate the revised summary
conditioned on it. Although learning edits as embeddings can be
eï¬€ective for individual tasks, it is not suitable to be used in the
pretraining ï¬ne-tuning paradigm, because there is a large domain
gap between the edit embeddings learned on diï¬€erent tasks. More
over, edit embeddings are less explainable compared to the explicit
edit representations we use.

Another line of work that carries out the idea of learning edits is
copying mechanism, including copying individual tokens [17, 53]
and spans [38, 60], which helps the model to â€œkeepâ€ unchanged to-
kens and focus on generating the edited part. Iv et al. [23] built a
T5-based model to update the existing articles based on the given
new evidence. The model is trained to output a copy token instead
of the copied sentence and a special reference token before the
updated text which identiï¬es the evidence to support the update.
Ding et al. [12] trained the model to emit pointers that indicate the
positions for editions and new tokens to be inserted at the same
time. Similarly, Chen et al. [10], Tarlow et al. [47] augmented the
transformer-based decoder with pointers to the input graph rep-
resentation of the code which specify the input locations to edit.
Although related, it is orthogonal to our work of learning edits
with pretraining.
Large Pretrained Models for Code. Motivated by the success
of large pretrained models for many NLP tasks, domain-speciï¬c
models that are pretrained on source code and technical text have
emerged, including CodeBERT [15], GraphCodeBERT [19], CodeGPT-
2 [32], CodeT5 [55], PLBART [1], PyMT5 [11], SynCoBERT [54],
SPT-Code [37], Codex [9] and UniXcoder [18]. Similar to our ap-
proach, GraphCodeBERT, CodeT5, SynCoBERT, SPT-Code and UniX-
coder also designed specialized pretraining objectives driven by
their targeted tasks. As we showed in this work, the combination

of an edit-based language model and a standard language model
can achieve better performance than using the standard language
model alone.
Pretrained Models for Code Edits. Prior work already explored
applying pretrained models, despite not well-suited, on editing tasks.
Chakraborty and Ray [8] used PLBART for code bug ï¬xing, which
we compared to in our work. Similarly, Drain et al. [13] further pre-
trained BART model on 67K Java repositories mined from GitHub
and ï¬ne-tuned speciï¬cally on the bug ï¬xing dataset [49]. Mas-
tropaolo et al. [34], Wang et al. [55] both pretrained T5 model
on CodeSerchNet and used it for bug ï¬xing, which we included
as a baseline (CodeT5). Codex [9] showed promising performance
on editing tasks by specifying the existing code as a prompt and
providing an edit instruction to the model. Tufano et al. [50] and
Li et al. [28] both proposed a transformer-based encoder-decoder
model pretrained on large code reviewer speciï¬c data for code re-
view related tasks including code change quality estimation, re-
view comment generation and code reï¬nement. While they demon-
strate impressive performance on various tasks, none of them are
fundamentally well-suited for edit tasks. In this work, we develop
CoditT5 with a novel pretraining objective for generating edit
sequences, which can complement the generation model such as
CodeT5 for edit tasks.
Combining Complementary Models. We used reranking [24,
36] to combine complementary models in this work. Ensembling [25]
is another approach for combining complementary models for gen-
eration tasks, but requires additional training. Co-training [5] and
tri-training [61] approaches, although shown to be very eï¬€ective
in combining complementary models, are designed for classiï¬ca-
tion models rather than generation models.

8 CONCLUSION
In this paper, we present a novel edit-driven pretraining objec-
tive and use it to develop CoditT5, a pretrained language model
for software-related editing tasks. CoditT5 is pretrained on large
amounts of source code and natural language comments to per-
form edits, and we evaluate this model by ï¬ne-tuning it on three
distinct downstream tasks: comment updating, bug ï¬xing and auto-
mated code review. By outperforming task-speciï¬c baselines and
pure generation baselines across tasks, we demonstrate the suit-
ability of CoditT5 (and our pretraining objective) for editing tasks
and its generalizability. We additionally ï¬nd that a pure generation-
based model and CoditT5 can complement one another through
simple reranking strategies, which outperform each of the models
individually and also achieve new state-of-the-art performance for
the three downstream editing tasks that we consider.

ACKNOWLEDGMENTS
We thank Nader Al Awar, Yu Liu, Raymond J. Mooney, Aditya
Thimmaiah, Zhiqiang Zang, and the anonymous reviewers for their
comments and feedback. We acknowledge the Texas Advanced Com-
puting Center (TACC) at The University of Texas at Austin for pro-
viding HPC resources that have contributed to the research results
reported within this paper. This work is partially supported by the
US National Science Foundation under Grant Nos. CCF-1652517,
CCF-2107291, IIS-2107524 and IIS-2145479.

CoditT5: Pretraining for Source Code and Natural Language Editing

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

REFERENCES
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Uni-
ï¬ed Pre-training for Program Understanding and Generation. In Conference of
the North American Chapter of the Association for Computational Linguistics: Hu-
man Language Technologies. 2655â€“2668.

[2] Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural machine
translation by jointly learning to align and translate. In International Conference
on Learning Representations.

[3] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT
evaluation with improved correlation with human judgments. In ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or
Summarization. 65â€“72.

[4] Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An Empirical In-
vestigation of Statistical Signiï¬cance in NLP. In Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language
Learning. 995â€“1005.

[5] Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data

with co-training. In Computational Learning Theory. 92â€“100.

[6] Shaked Brody, Uri Alon, and Eran Yahav. 2020. A structural model for contex-
International Conference on Object-Oriented Programming,

tual code changes.
Systems, Languages, and Applications 4 (2020), 1â€“28.

[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. In Advances in Neural
Information Processing Systems. 1877â€“1901.

[8] Saikat Chakraborty and Baishakhi Ray. 2021. On Multi-Modal Learning of Edit-

ing Source Code. In Automated Software Engineering. 443â€“455.

[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large language models trained on code.
arXiv preprint arXiv:2107.03374 (2021).

[10] Zimin Chen, Vincent J Hellendoorn, Pascal Lamblin, Petros Maniatis, Pierre-
Antoine Manzagol, Daniel Tarlow, and Subhodeep Moitra. 2021. PLUR: A uni-
fying, graph-based view of program learning, understanding, and repair. In Ad-
vances in Neural Information Processing Systems. 23089â€“23101.

[11] Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel
Sundaresan. 2020. PyMT5: multi-mode translation of natural language and
Python code with transformers. In Empirical Methods in Natural Language Pro-
cessing. 9052â€“9065.

[12] Yangruibo Ding, Baishakhi Ray, Premkumar Devanbu, and Vincent J Hellen-
doorn. 2020. Patching as Translation: the Data and the Metaphor. In Automated
Software Engineering. 275â€“286.

[13] Dawn Drain, Chen Wu, Alexey Svyatkovskiy, and Neel Sundaresan. 2021. Gen-
erating bug-ï¬xes using pretrained transformers. In International Symposium on
Machine Programming. 1â€“8.

[14] Angela Fan, Mike Lewis, and Yann Dauphin. 2019. Strategies for Structuring
Story Generation. In Annual Meeting of the Association for Computational Lin-
guistics. 2650â€“2660.

[15] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages. In Empirical Methods
in Natural Language Processing: Findings. 1536â€“1547.

[16] Zhipeng Gao, Xin Xia, David Lo, John Grundy, and Thomas Zimmermann. 2021.
Automating the removal of obsolete TODO comments. In Joint European Soft-
ware Engineering Conference and Symposium on the Foundations of Software En-
gineering. 218â€“229.

[17] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating Copy-
ing Mechanism in Sequence-to-Sequence Learning. In Annual Meeting of the As-
sociation for Computational Linguistics. 1631â€“1640.

[18] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022.
UniXcoder: Uniï¬ed Cross-Modal Pre-training for Code Representation. In An-
nual Meeting of the Association for Computational Linguistics. 7212â€“7225.
[19] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. GraphCode-
BERT: Pre-training Code Representations with Data Flow. In International Con-
ference on Learning Representations.

[20] Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. 2018. Gen-
erating sentences by editing prototypes. Transactions of the Association for Com-
putational Linguistics 6 (2018), 437â€“450.

[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. 2018. A
retrieve-and-edit framework for predicting structured outputs. In Advances in
Neural Information Processing Systems.

[22] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of seman-
tic code search. arXiv preprint arXiv:1909.09436 (2019).

[23] Robert Iv, Alexandre Passos, Sameer Singh, and Ming-Wei Chang. 2022. FRUIT:
Faithfully Reï¬‚ecting Updated Information in Text. In Conference of the North

American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies. 3670â€“3686.

[24] Reno Kriz, JoÃ£o Sedoc, Marianna Apidianaki, Carolina Zheng, Gaurav Kumar,
Eleni Miltsakaki, and Chris Callison-Burch. 2019. Complexity-Weighted Loss
and Diverse Reranking for Sentence Simpliï¬cation. In Conference of the North
American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies. 3137â€“3147.

[25] Alexander LeClair, Aakash Bansal, and Collin McMillan. 2021. Ensemble Models
for Neural Source Code Summarization of Subroutines. In International Confer-
ence on Software Maintenance and Evolution. 286â€“297.

[26] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. In Annual Meeting of the Association for Com-
putational Linguistics. 7871â€“7880.

[27] Jia Li, Yongmin Li, Ge Li, Xing Hu, Xin Xia, and Zhi Jin. 2021. Editsum: A retrieve-
and-edit framework for source code summarization. In Automated Software En-
gineering. 155â€“166.

[28] Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep
Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, et al. 2022. CodeRe-
viewer: Pre-Training for Automating Code Review Activities. arXiv preprint
arXiv:2203.09095 (2022).

[29] Bo Lin, Shangwen Wang, Kui Liu, Xiaoguang Mao, and TegawendÃ© F BissyandÃ©.
2021. Automated Comment Update: How Far are We?. In International Confer-
ence on Program Comprehension. 36â€“46.

[30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta:
A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[31] Zhongxin Liu, Xin Xia, David Lo, Meng Yan, and Shanping Li. 2021.

Just-In-
Time Obsolete Comment Detection and Update. IEEE Transactions on Software
Engineering (2021).

[32] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-
sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. arXiv preprint arXiv:2102.04664 (2021).

[33] Lara Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock, Shruti
Singh, Brent Harrison, and Mark Riedl. 2018. Event representations for auto-
mated story generation with deep neural nets. In AAAI Conference on Artiï¬cial
Intelligence. 868â€“875.

[34] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
usage of text-to-text transfer transformer to support code-related tasks. In Inter-
national Conference on Software Engineering. 336â€“347.

[35] Courtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault. 2015.
Ground truth for grammatical error correction metrics. In Annual Meeting of
the Association for Computational Linguistics and International Joint Conference
on Natural Language Processing. 588â€“593.

[36] Graham Neubig, Makoto Morishita, and Satoshi Nakamura. 2015. Neural Rerank-
ing Improves Subjective Quality of Machine Translation: NAIST at WAT2015. In
Workshop on Asian Translation. 35â€“41.

[37] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo.
2022. SPT-Code: Sequence-to-Sequence Pre-Training for Learning the Repre-
sentation of Source Code. In International Conference on Software Engineering.
2006â€“2018.

[38] Sheena Panthaplackel, Miltiadis Allamanis, and Marc Brockschmidt. 2021. Copy
that! editing sequences by copying spans. In AAAI Conference on Artiï¬cial Intel-
ligence. 13622â€“13630.

[39] Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric, and Raymond J Mooney.
2021. Deep just-in-time inconsistency detection between comments and source
code. In AAAI Conference on Artiï¬cial Intelligence. 427â€“435.

[40] Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li, and Raymond
Mooney. 2020. Learning to Update Natural Language Comments Based on Code
Changes. In Annual Meeting of the Association for Computational Linguistics.
1853â€“1868.

[41] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In Annual Meeting of
the Association for Computational Linguistics. 311â€“318.

[42] Karl Pichotta and Raymond Mooney. 2016. Learning statistical scripts with
LSTM recurrent neural networks. In AAAI Conference on Artiï¬cial Intelligence.
2800â€“2806.

[43] Alec Radford, Jeï¬€rey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al. 2019. Language models are unsupervised multitask learners.
OpenAI blog 1 (2019), 9.

[44] Colin Raï¬€el, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Lim-
its of Transfer Learning with a Uniï¬ed Text-to-Text Transformer. Journal of
Machine Learning Research 21 (2020), 1â€“67.

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric

[45] Ehud Reiter and Robert Dale. 1997. Building Applied Natural Language Gener-

[53] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. Ad-

ation Systems. Natural Language Engineering 3 (1997), 57â€“87.

vances in Neural Information Processing Systems.

[46] Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention
Model for Abstractive Sentence Summarization. In Empirical Methods in Natural
Language Processing. 379â€“389.

[47] Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine
Manzagol, Charles Sutton, and Edward Aftandilian. 2020. Learning to ï¬x build
errors with graph2diï¬€ neural networks. In International Conference on Software
Engineering Workshops. 19â€“20.

[48] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and
Denys Poshyvanyk. 2019. On learning meaningful code changes via neural ma-
chine translation. In International Conference on Software Engineering. 25â€“36.

[49] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. 2019. An empirical study on learning bug-ï¬xing
patches in the wild via neural machine translation. Transactions on Software
Engineering 28 (2019), 1â€“29.

[50] Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys
Poshyvanyk, and Gabriele Bavota. 2022. Using Pre-Trained Models to Boost
Code Review Automation. In International Conference on Software Engineering.
2291â€“2302.

[51] Rosalia Tufano, Luca Pascarella, Michele Tufanoy, Denys Poshyvanykz, and
Gabriele Bavota. 2021. Towards Automating Code Review Activities. In Inter-
national Conference on Software Engineering. 163â€“174.

[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. In Advances in Neural Information Processing Systems. 5998â€“6008.

[54] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao
Wu, Jin Liu, and Xin Jiang. 2021. SynCoBERT: Syntax-Guided Multi-Modal Con-
trastive Pre-Training for Code Representation. arXiv preprint arXiv:2108.04556
(2021).

[55] Yue Wang, Weishi Wang, Shaï¬q Joty, and Steven C.H. Hoi. 2021. CodeT5:
Identiï¬er-aware Uniï¬ed Pre-trained Encoder-Decoder Models for Code Under-
standing and Generation. In Empirical Methods in Natural Language Processing.
8696â€“8708.

[56] Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-
Burch. 2016. Optimizing statistical machine translation for text simpliï¬cation.
Transactions of the Association for Computational Linguistics 4 (2016), 401â€“415.

[57] Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, and Graham Neubig. 2021.
Learning Structural Edits via Incremental Tree Transformations. In International
Conference on Learning Representations.

[58] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for General-
Purpose Code Generation. In Annual Meeting of the Association for Computa-
tional Linguistics. 440â€“450.

[59] Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt, and
Alexander L Gaunt. 2018. Learning to Represent Edits. In International Confer-
ence on Learning Representations.

[60] Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou. 2018. Sequential copying

networks. In AAAI Conference on Artiï¬cial Intelligence. 4987â€“4994.

[61] Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data us-
ing three classiï¬ers. Transactions on knowledge and Data Engineering 17 (2005),
1529â€“1541.

