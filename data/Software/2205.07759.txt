2
2
0
2

y
a
M
5
2

]

R
C
.
s
c
[

2
v
9
5
7
7
0
.
5
0
2
2
:
v
i
X
r
a

Software Updates Strategies: a Quantitative
Evaluation against Advanced Persistent Threats

Giorgio Di Tizio, Michele Armellini, Fabio Massacci

1

Abstract—Software updates reduce the opportunity for exploitation. However, since updates can also introduce breaking changes,
enterprises face the problem of balancing the need to secure software with updates with the need to support operations. We propose a
methodology to quantitatively investigate the effectiveness of software updates strategies against attacks of Advanced Persistent
Threats (APTs). We consider strategies where the vendor updates are the only limiting factors to cases in which enterprises delay
updates from 1 to 7 months based on SANS data.
Our manually curated dataset of APT attacks covers 86 APTs and 350 campaigns from 2008 to 2020. It includes information about
attack vectors, exploited vulnerabilities (e.g. 0-days vs public vulnerabilities), and affected software and versions. Contrary to common
belief, most APT campaigns employed publicly known vulnerabilities.
If an enterprise could theoretically update as soon as an update is released, it would face lower odds of being compromised than those
waiting one (4.9x) or three (9.1x) months. However, if attacked, it could still be compromised from 14% to 33% of the times.
As in practice enterprises must do regression testing before applying an update, our major ﬁnding is that one could perform 12% of all
possible updates restricting oneself only to versions ﬁxing publicly known vulnerabilities without signiﬁcant changes to the odds of
being compromised compared to a company that updates for all versions.

Index Terms—Advanced Persistent Threats, software vulnerabilities, software updates

✦

1 INTRODUCTION

A RECENT study [1] shows that it takes more than 200

days for an enterprise to align 90% of their machines
with the latest (not known to be vulnerable) software ver-
sion given the need to perform regression testing [2].

Such behavior is rational because not all vulnerabilities
are always exploited in the wild [3], and several authors
have determined that the actual risk of slow updates against
‘mass attackers’ is limited [4], [5] and often due to spe-
ciﬁc types of vulnerabilities such as those traded in Black
Markets [6] or with other predictable characteristics [7],
[8]. Hence, risk analysis might be an effective approach
when considering ‘mass attackers’ which might well be
‘work averse’ and stick to old exploits until they are no
longer proﬁtable [9]. However, many companies also face
Advanced Persistent Threats (APTs). APTs are highly spe-
cialized professionals [10] that use a variety of customized
strategies [11], often leveraging on spearphishing [12] and
0-days [13], [14] to maintain a stealthy proﬁle [13]. In this
scenario, slow updates do not seem appropriate.

Yet, not all the APTs are really sophisticated [15]. Some
reports challenged some of these ‘allegations’ and observed
that APTs often reuse tools, malware, and vulnerabili-
ties [12], [16], [17]. These reports are based on threat in-
telligence data with few overlaps [18] thus capturing only
partial information of the APT attacks [19], [15].

These conﬂicting claims may be due to the lack of a sys-
tematic study. Indeed, previous works on APTs analysis [20],
[13], [10], [21] mostly reported a qualitative analysis of a
handful of APTs. However, relying on qualitative estima-
tions is known to produce risk miscategorization and wrong
prioritization [22], [23] due to several factors like judgmental
biases [23], agenda-setting, and framing [24]. Framing of
individual reports can produce a distorted perception of
the risk. We lack a broad view of the APT landscape that
allows companies to correctly assess the advantages and
disadvantages of current approaches to software updates.
Data acquisition of APT campaigns, i.e. speciﬁc attacks con-
ducted by APT groups, is currently a challenging task. Semi-
automated approaches based on report parsing [25] proved
to be too riddled with false positives because the associa-
tions between APTs and software vulnerabilities (identiﬁed
by a CVE) are based on the presence of keywords and not on
the semantics of the document. Here our research questions
are as follows:

RQ1. What are the APTs characteristics that quantitatively de-
scribe the landscape of APT campaigns as observable from
public reports?

RQ2. Given a quantitative description of both APT campaigns and
software updates, how effective are different update strategies
to protect against APT campaigns?

We thus make the following contributions:

• G. Di Tizio (corresponding author) is with University of Trento, Italy.

E-mail: giorgio.ditizio@unitn.it

• M. Armellini is with University of Trento, Italy.
•

F. Massacci is with University of Trento, Italy and Vrije Universiteit
Amsterdam, The Netherlands.

The ﬁnal version of this paper appears in: IEEE Transactions on Software
Engineering, 2022. DOI 10.1109/TSE.2022.3176674

• We build a structured, manually veriﬁed database in
Neo4j of 86 APTs and more than 350 campaigns based
on an exhaustive search of over 500 technical reports
and blogs and up to 22 different resources for each APT.
The database [26] is available on Zenodo.

• We present a methodology to quantify and compare the

 
 
 
 
 
 
effectiveness and cost of software update strategies on
historical data about campaigns.

• We quantitatively evaluated the effectiveness and cost
of different software updates strategies, in terms of the
conditional probability of being compromised and the
number of updates required for 5 widely used software
products (Ofﬁce, Acrobat Reader, Air, JRE, and Flash
Player) for the Windows O.S.

Scope of the work: We provide a quantitative anal-
ysis of the risk against APT to allow companies to make
rational decisions on software updates. We do not propose
new mechanisms to detect and mitigate APTs attacks.

2 THE SOFTWARE UPDATE PROBLEM

If a company could only update for new functionalities, the
choice would be obvious: why ﬁxing what is not broken?
Yet, companies must update for security reasons too. How-
ever, it is not uncommon that vulnerability ﬁxes are merged
with features changes in a single update. Every time a new
version of a software is published, one can

• update immediately;
• wait some time (e.g. for regression testing) and update;
• skip the update.

This choice may be inﬂuenced by asynchronous events re-
lated to the reservation, disclosure, and exploitation of software
vulnerabilities in the current release.

Unfortunately, a company cannot fully decide in advance
the conﬁguration they will have when hit (or most fre-
quently not hit) by an attacker as it depends on the at-
tacker’s choice. A company can only decide on the software
updates strategy. To capture what can happen we introduce
some terminology.

2.1 Terminology

For each vulnerability we identiﬁed ﬁve instants of time:

• Vulnerability Reserved time (tVr ): when the CVE entry for

the vulnerability is reserved by MITRE;

• Vulnerability Published time (tVp ): when the CVE for the

vulnerability is published in NVD;

• Vulnerability Exploited time (tVe ): when the vulnerability

is observed to be exploited in the wild;

• Update release time (tUr ): when an update that addresses

the vulnerability is released.

• Update deployed time (tUd ): when an update that ad-

dresses the vulnerability is deployed.
Tab. 1 shows how we can classify attack scenarios based
on the instant of time tVe and its relative position with the
other events: tVr , tVp , and tUr Fig. 1 summarizes the possible
combinations of the different events.

2.2 The Software Update Strategies

To answer RQ2, we describe the update strategies, sum-
marized in Tab. 2, for an enterprise based on what was
discussed previously: update, wait and then update, or skip. It
is important to underline that disabling automated updates
is not uncommon in enterprise networks [27], [28]. This is
mainly due to compatibility issues between the updated
software and internal projects [2], [29] that can produce

TABLE 1: Classiﬁcation of Attack Scenarios

2

Scenario
Unknown-
Unknown/
Unpreventable
(UU/U)
Unknown-
Unknown/
Preventable
(UU/P)
Known-Unknown/
Unpreventable
(KU/U)

Known-Unknown/
Preventable
(KU/P)
Known-Known/
Unpreventable
(KK/U)
Known-Known/
Preventable (KK/P)

Description
The vulnerability is exploited before a CVE
was reserved, before its public disclosure, and
before an update for the vulnerability was
released.
The vulnerability is exploited before a CVE
was reserved, before its public disclosure, but
after an update for the vulnerability was re-
leased.
The vulnerability is exploited after a CVE
was reserved, before its public disclosure, and
before an update for the vulnerability was
released.
The vulnerability is exploited after a CVE was
reserved, before its public disclosure, and after
an update for the vulnerability was released.
The vulnerability is exploited after a CVE was
reserved, after its public disclosure, and before
an update for the vulnerability was released.
The vulnerability is exploited after a CVE was
reserved, after its public disclosure, and after
an update for the vulnerability was released.

TABLE 2: Update strategies

Each strategy represents an approach for updating the software. The Im-
mediate strategy represents the upper bound achievable by an enterprise.
The Planned, Reactive, and Informed Reactive are evaluated with different
update intervals that represent different level of responsiveness.

Strategy

Immediate

Planned

Reactive

Update
Interval
/

1,
3,
months
1,
3,
months

Informed
Reactive

1,
3,
months

Description

7

7

7

Update to each newest version as soon
as it is available without any delay
Update to each newest version but wait
a delay before the deployment
Update to the ﬁrst new (non vulnera-
ble) version only after the publication
in NVD of a CVE, wait a delay before
the deployment
Update to the ﬁrst new (non vulnera-
ble) version only after the reservation
by MITRE of a CVE entry, wait a delay
before the deployment

disruption of the enterprise work. In this case delays are
introduced to perform regression testing.

We considered the application of a software update with
a certain delay, starting from the date on which the strategy
bases its decision. We considered different update intervals
to determine how the probabilities change if a more respon-
sive approach is employed. We leverage update intervals
data from SANS [30] based on a variety of enterprises (Gov-
ernment, Financial Services, Healthcare, and Consulting)
and from Kotzias et al. [1] based on 28k enterprises. Tab. 3
shows the update intervals from SANS and maps them to
our update strategies.
Immediate strategy: The enterprise updates its software as
soon as a new version is available (tUr ) and without delay.
If multiple updates are released in the same time interval,
the update takes the most recent one. The update is applied
even if a vulnerability for the previous version is not present
yet. This is the theoretical limit for the enterprise because it
is bounded only by the release speed of the vendor. How-
ever, this approach is likely impractical because updates

3

At the time when a software update is available, we have 4 cases: Case 1) there is no reservation and publication of vulnerabilities before and after
the release of an update for the current version. In this case, there is no exploitation of the vulnerability. Case 2) after a software update is released,
a vulnerability is reserved and disclosed for the current version. Case 3) before the release of a software update a vulnerability is reserved for the
current version, but the disclosure happens after the update release. Case 4) the reservation and disclosure of a vulnerability for the current version
happen before the release of an update. Different update strategies can be applied but are all constrained by the presence of a new release. The
exploitation events (vertical lines) can happen at any instant of time asynchronously from the reservation-disclosure process and the release of
updates. They are classiﬁed following Tab. 1

Fig. 1: Combinations of vulnerability reservation, disclosure, and exploitation events with the presence of new updates.

TABLE 3: Update Intervals from SANS [30]

TABLE 4: State of the Art on APTs - Main Research Topics

Percentage of enterprises that update weekly, monthly, quarterly, or
with other delays. We evaluated the strategies with these update
intervals. Enterprises that update weekly are comparable to the
Immediate strategy. We associated 7 months for the update interval
of ’Other’ from [1].

Research Category
APTs data sources
Metrics for TI sources
Attackers characteristics
Detection of attacks

Update
interval
Weekly
Monthly

Quarterly

Other

% Enterprises Update Strategy Correspondence

24.9
57.5

7.7

10.0

Immediate
Planned/Reactive/Informed Reactive
within 1 month delay
Planned/Reactive/Informed Reactive
within 3 months delay
Planned/Reactive/Informed Reactive
within 7 months delay

require some time to be deployed in an enterprise to not
break other functionalities.

Planned strategy: The company updates its software to each
new version with a delay from the release date (tUr ). If
multiple updates are released in the same time interval,
the company takes the most recent one. This delay factors
the time for regression testing and update deployment. The
delays are taken from Tab. 3. As in the Immediate strategy, the
update is not triggered by the knowledge of vulnerabilities
but only on the availability of a new update.

Reactive strategy The enterprise updates the software only
after the publication of a vulnerability by NVD (tVp ) with
a delay taken from Tab. 3. The new version installed is the
ﬁrst non-vulnerable update available at that time.

Informed Reactive strategy The enterprise updates the soft-
ware only after the reservation of a vulnerability by MITRE
(tVr ). The new version installed is the ﬁrst non-vulnerable
update available at that time. This strategy describes an
enterprise that pays an annual subscription fee to get in-
formation about the non publicly disclosed vulnerabilities
from companies that provide 0-days data information (e.g.
Exodus Intelligence, Zerodium). The strategy presents an
update interval as the Reactive and Planned strategies.

State of the Art
[11]
[19], [18]
[13]*, [20]*,[12]
[31],
[35],
[33],
[36], [14], [37], [38], [39], [40],
[41],[42], [43], [44], [45],[46]
[47], [48], [40]
[36], [48]

[32],

[34],

This paper
X

X

X
X

Game Theory
Exploitation likelihood
Analysis of update releases -

* Performed high level analysis on few APTs campaigns.

3 RELATED WORKS

Tab. 4 shows the research categories addressed by the state-
of-the-art on APTs. The majority of the research activity
focused on the detection of APTs campaigns while few
papers tried to characterize their behavior, estimate the risk,
and evaluate update strategies from real data.

3.1 APT and Metrics for Threat Intelligence sources

Lemay et al. [11] presented a description of different re-
sources about the activities of more than 40 APTs. Li et
al. [19] utilized a set of metrics (Volume, Differential con-
tribution, Exclusive contribution, Latency, Accuracy, and
Coverage) to compare different public and private Threat
Intelligence (TI) data feeds. They observed that in the ma-
jority of the data feeds there is no overlapping of Indicator
of Compromise (IOC) and a high number of false positives.
Similarly, Bouwman et al. [18] analyzed two paid TI and
observed very few overlaps in the indicators for 22 APTs.
The distinction is conﬁrmed with a comparison with open
TI data. Furthermore, they observed that TI data is em-
ployed in the decision process of companies, but there is
a lack of metrics to determine the quality of these data.
Several works [49], [25], [50] proposed a (semi-)automated
approach based on report parsing to generate a database of
IOC. However, merely relying on the results of the auto-
mated approach generates many false positives. For exam-
ple in [25], we observed that CVEs are wrongly associated
to the admin@338 group in a report about the Poison Ivy

malware, where several campaigns from different actors are
described. We provide a manually curated database from
which we can quantitatively evaluate the impact and cost of
software update strategies.
Several studies evaluated the overlap among threat data feeds,
showing poor accuracy. Mechanisms to semi-automatically ex-
tract information from reports are prone to false-positive.

3.2 Analysis of attackers characteristics

Ussath et al. [20] analyzed 22 reports about APT campaigns
and mapped them into the three phases of an attack (initial
compromise, lateral movements, and Command&Control).
They found that most of them employ social engineering
techniques and living-off-the-land techniques. Furthermore,
they noted that 0-day vulnerabilities are not exploited fre-
quently by APTs. Chen et al. [13] studied 4 APT campaigns
to analyze the phases of these attacks and determine pos-
sible countermeasures. Urban et al. [12] analyzed 93 APT
reports (66 different APTs) and determined that spearphish-
ing is the main attack vector. They then collected OSINT
data like domain names and social media information of 30
companies to determine how much information is available
to the adversaries. Additional works on APTs analysis fo-
cused on describing the phases of the attacks and possible
countermeasures [10], the analysis of the malware employed
in a few well-known campaigns [21], or the prevalence of
living-off-the-land techniques in certain samples [51].

To the best of our knowledge, we are the ﬁrst to analyze
more than 350 campaigns exploiting 118 different CVEs
from the inspection of more than 500 reports. This massive
analysis makes it possible to draw signiﬁcant conclusions
on the efﬁcacy of update strategies.
Although several works provided insights into the APT ecosys-
tem, the analysis focused on a handful of campaigns that make
it hard to draw signiﬁcant conclusions on the characteristics of
APTs.

3.3 Detection of attacks

An orthogonal problem is to detect live APTs attacks once
they get into the network. Different research proposed to
employ machine learning [39], [34], [46], information ﬂow
tracking [37], [42], [44], [45], statistical correlation [52], and
big data analysis [36], [14], [33], [35].

Shu et al. [41] employed a temporal computational graph
to perform threat hunting activities via graph patterns
matching and analyzed a case study on a DARPA threat
detection competition. Pei et al. [38] developed a framework
to generate a multi-dimensional weighted graph based on
log entries and identify attacks by the presence of dense
connections among logs using unsupervised learning tech-
niques. They evaluated it over 15 APTs campaigns.
The state-of-the-art focused mainly on the detection and response
against APT attacks, while there is a lack of investigation on the
orthogonal problem of prevention.

3.4 Game Theory

Hu et al. [47] presented a two-layer attack/defense game
to study APT attackers that make use of insiders and
compute the best strategies for the attacker and defender.

4

Sahabandu et al. [40] formulated a game-theoretic model
to determine the optimal defender strategy in terms of
tracking of information ﬂow (Dynamic Information Flow
Tracking). Yang et al. [48] proposed a Nash game to model
the response strategy and minimize the loss of an enterprise
against lateral movements in the network of APTs in the
network. We instead focus on the initial access phase of
APTs campaigns and we evaluated the efﬁcacy of software
update strategies based on real data of attacks.
Game theory is extensively applied to ﬁnd an optimal strategy
against targeted attacks. However, these studies employ artiﬁcial
data and networks.

3.5 Analysis of exploitation likelihood

Many works employed ML and statistical methods to ana-
lyze vulnerabilities and predict the exploitation likelihood
by joining data from resources like NVD, Exploit DB [3],
historical data on attacks [4], [7], Dark Web forums [53], and
Twitter [54], [8]. An extensive discussion of the academic
literature on empirical cyber risk can be found in [55].

Other works investigated actual compromises using
logs. Marchetti et al. proposed a framework to prioritize
the internal clients of an organization that are most likely
to be compromised by an APT using internal (network logs
and ﬂow records) and external (social media) data [36] and
to detect data exﬁltration using a set of host-based features
and ﬂow records analysis [14]. Similarly, Bilge et al. [5] and
Liu et al. [56] employed supervised learning algorithms to
determine machines at risk of infection from internal logs
on binary ﬁle appearance, external data of misconﬁgured
services (e.g. DNS or BGP), and malicious behaviors (e.g.
spam or phishing).

We extend this line of research by proposing a method-
ology to evaluate the probability of being compromised by
APTs and the cost associated with the update strategy.
Analysis of historical data about vulnerability and attacks as well
as live information provided by logs and social platforms allows
one to evaluate the exploitation likelihood.

3.6 Analysis of update releases

From the client-side, Nappa et al. [27] proposed a systematic
analysis of the update process and update delay on client
applications, and performed a survival analysis of vulnera-
bilities based on data from Symantec. Similarly, Kotzias et
al. [1] presented a longitudinal study of the update behavior
for 12 client software and 112 server applications based
on data from 28k enterprises. Sarabi et al. [57] employed
Symantec dataset to model users’ update delay as a geo-
metric distribution and study 4 different products (Chrome,
Firefox, Thunderbird, and Flash Player).

From the vendor-side, Arora et al. [58] analyzed vendors’
patch behavior as a function of several factors like disclosure
time, characteristics of the vendor, and severity of the vul-
nerability. Clark et al. [59] studied if agile methods produce
a higher number of vulnerabilities in Firefox. They observed
that rapid software releases do not increase the number
of vulnerabilities in the code. Ozment and Schechter [60]
analyzed the impact of legacy code on the number of
vulnerabilities observed OpenBSD versions.

Similar to our work, Beres et al. [61] employed a discrete-
event simulator to determine the exposure reduction pro-
duced by different security policies by varying update speed
and mitigations. However, they modeled events like exploits
and updates availability assuming ﬁxed exponential func-
tions looking at global trends observed by a security ﬁrm.

We present a quantitative evaluation of the effectiveness
and cost of realistic update strategies by using historical
data about APT campaigns.
Several works analyzed the update behavior of clients and ven-
dors. However, there are only theoretical works on the efﬁcacy of
updates against targeted attacks for an enterprise.

4 METHODOLOGY
In this section, we present a methodology to evaluate the
effectiveness and cost of update strategies.

The deﬁnition of probabilistic risk assessment [62] is:

Risk = Pr (Compr |Attack ) · P (Attack ) · Impact

(1)

How to determine P (Attack ) is still an unsolved problem
in cyber-security [63] while the Impact of cyber-attacks has
received extensive discussion [64], [65]. In this paper, we
focus on P (Compr |Attack ) i.e. the conditional probability
of being compromised given an attack (or campaign as
used in this paper). We propose a methodology to com-
pute the conditional probability of being compromised in
Eq. 1 by employing historical data about releases available,
vulnerabilities, and their exploitation in campaigns. Tab. 5
overviews our methodology.

Step 1: Extract APT and software data

To collect data we analyzed both unstructured (technical
reports, blogs about APT campaigns, and vendor’s reposito-
ries) and structured (MITRE Att&ck and NVD repositories)
public sources.
Unstructured sources: Similar to Urban et al. [12], we man-
ually collected data about APT campaigns from more than
500 technical reports and blogs. We started from the MITRE
Att&ck APT groups list and one researcher:

• collected the reports associated with each APT group
from the Threat Actor Encyclopedia [66], that relies on
sources like Malpedia, MISP, AlienVault, and MITRE;
• extended this set of resources by searching on the
Internet for reports using as keywords the APT name
as stated in MITRE (e.g. Stealth Falcon) and the term
”CVE” until data saturation was reached, i.e. new re-
ports do not add new information to the APT cam-
paigns. The reports are obtained from cyber-security
companies like Kaspersky, FireEye, Palo Alto Net-
works, Google Project Zero as well as from technical
forums and blogs.

Extracted Information

Two researchers independently analyzed the content of each
report manually to identify the following information for a
campaign:

• the date when the campaign is ﬁrst observed;
• the CVE(s) exploited;
• the attack vector(s) employed.

5

We uniquely identify a campaign using the date in which
it is ﬁrst observed. If a campaign employs different at-
tack vectors and/or different CVEs, we create multi-
ple entries in the form <APT name,attack vector,date> or
<APT name,CVE,date>. Each entry is linked to one or more
reports containing this information.

We do not perform open coding because the information
in the reports is deterministic and already based on the
MITRE industry standards on CVEs 1 and Initial Access
Tactic2. The association of CVEs and attack vectors to a
certain APT is based on the explicit attribution in the con-
sulted resources. Let us consider the following snippet from
a Mandiant report referring to APT123:

In June 2014, the [Arbor Networks] blog highlighted that
the backdoor was utilized in campaigns from March 2011
till May 2014. Following the release of the article, Fire-
Eye observed a distinct change in RIPTIDE’s protocols
and strings. . . . FireEye dubbed this new malware family
HIGHTIDE.
On Sunday August 24, 2014 we observed a spear phish
email sent to a Taiwanese government ministry. Attached
to this email was a malicious Microsoft Word document
(MD5: f6fafb7c30b1114befc93f39d0698560) that exploited
CVE-2012-0158. It is worth noting. . .

we extracted the following information: date=08/2014;
CVE=CVE-2012-0158; attack vector=spearphishing attach-
ment.

The entries were then reviewed by a third researcher, not
involved in the initial manual analysis, to resolve inconsis-
tencies. Cohen’s kappa values are 1, 0.976, and 0.863 for the
CVE, date, and attack vector respectively (42 disagreements
over 652 entries) which show a good agreement among the
raters. Total agreement on CVEs is unsurprising as CVEs
are unique strings and reported by copying and pasting
the string into the data collection form. Such agreement
would not happen between a manual rater and an automatic
procedure as we already noted for DAPTSET [25] which is
so riddled with false positives to be unusable. Simply, an au-
tomatic procedure will collect all CVEs including those that
a human rater will see as clearly irrelevant (past campaign,
related examples, etc.). Most disagreements are on the attack
vector as the mapping of the natural description into the cor-
responding MITRE Att&ck category is sometimes amenable
to interpretation (27 out of the 42 disagreements).

To resolve uncertainty among resources, we made the

following conservative assumptions:

• if report A says CVE-1 is exploited by an APT campaign
and report B says CVE-2 is exploited we mark both
CVE-1 and CVE-2 as exploited by the APT in question.
• if report A says an APT campaign started on month X
and report B says an APT campaign started on month
Y we mark them as two distinct campaigns.

It is not uncommon that different security companies have
non-overlapping information about APT campaigns [19],

1. https://www.cve.org/About/History
2. https://attack.mitre.org/tactics/TA0001/
3. https://www.mandiant.com/resources/darwins-favorite-apt-group-2

Step 1: Extract APT and software data

TABLE 5: Methodology overview

6

INPUT APT groups from MITRE Att&ck

OUTPUT A set of campaigns in the form <APT name,CVE,date>, <APT name,attack vector,date> and a set of software updates in the

PROCEDURE

form <sw,update,release date>
Identify campaigns information and software releases:

• Collect resources describing campaigns for each APT based on Threat Actor Encyclopedia [66] and Internet searches

using the MITRE APT name and ”CVE” as keywords;

• Manually extract from resources the key information: date when campaign is observed, CVE(s) exploited, attack vector

employed;

• For each CVE, automatically extract software and versions affected from NVD;
• Manually extract from software vendors website the update number and date of release.

Step 2: Instantiate update strategy

INPUT A set of software updates (<sw,update,release date>), update strategy (Immediate, Planned, Reactive, Informed Reactive), CVEs

exploited in APT campaigns

OUTPUT A matrix that describes the application of updates for the software in the period [2008-2020]

PROCEDURE

Create a matrix with rows identifying software versions and columns identifying months in [2008-2020] that determines the
installed software version at a given time:

• Select the entry corresponding to the ﬁrst vulnerable version available on 01/2008 (same for all strategies);
• Select another entry corresponding to a new version depending on the update strategy: on the release date of an update
for the software (Immediate) or with a delay (Planned), on the publication (Reactive) or reservation date (Informed Reactive)
of a CVE for the software with a delay;

• Consider availability of non-vulnerable updates at the time of publication of a CVE when computing delay for Reactive

and Informed Reactive.

Step 3: Instantiate APT campaigns events

INPUT

Set of events for different campaigns (<APT name,CVE,date>)

OUTPUT A set of matrices of campaigns. Each matrix describes the software versions targeted by a certain campaign in the period

PROCEDURE

[2008-2020]
For each campaign, create a matrix with rows identifying software versions and columns identifying months in the [2008-2020]
that determines targeted software version at a given time:
• Extract the affected software versions from the CVEs;
• Select the entry of the affected software versions from the date of the campaign up to 2020.

Step 4: Generate pessimistic scenarios

INPUT A matrix that describes the application of updates for the software in the period [2008-2020]

OUTPUT A matrix that describes the application of updates for the software in the period [2008-2020] and maintains both versions

during the transition month

PROCEDURE Update matrix to maintain the previous version in the month in which a new update is installed:

• For each month in which the software version is updated to a new version, keep the entry corresponding to the previous

version installed for that month only.

Step 5: Compute conditional probability of being compromised

INPUT A set of matrices of update strategies and a set of matrices of campaigns events

OUTPUT

PROCEDURE

The conditional probability of being compromised given a set of campaigns are targeting you (P (Compr |Attack )) based on
the update strategy, # of updates performed
Compute successful campaigns targeting installed software in the period [2008-2020]. For each matrix of update strategy:

• Select a matrix of campaign events and compute the element-wise product of the matrix with the update strategy matrix

to identify the intersection of installed and targeted software versions;

• Sum rows of the resulting matrix to determine the months when a campaign is successful, save the campaign if successful;
• Continue with another matrix of campaign events until no more campaigns.
• Compute conditional probability as the number of successful campaigns divided by the number of matrix campaigns

considered;

• Compute the number of updates counting non-empty rows in the matrix of update strategy.

Step 6: Compare strategies effectiveness

INPUT
OUTPUT
PROCEDURE

The successful campaigns for the different update strategies
Conﬁdence Intervals (CI) for update strategies
Compare the CI intervals of different update strategies:

• Compute the Agresti-Coull 95% CI for the proportion of successful campaigns by update strategy;
• Compare intervals, if they overlap update strategies are similar;
• Compute pair-wise agreement of successful campaigns for pair of update strategies and Agresti-Coull 95% CI for the
resulting proportion of agreement. The interval identiﬁes the expected range of proportion of campaigns that succeed
against both update strategies.

[18]. We discuss the implications of this choice in §7. Fig. 2
in §5 summarizes the number of reports per APT.

For the software, we retrieved versions for a subset of the
targeted products (discussed in §5.4) with their release date.
This procedure was manual as it is not trivial to obtain past
versions release date [27] because vendors’ repositories are
unstructured and not intended for past versions indexing.
Structured sources: For each CVE obtained from the un-
structured sources, we automatically extracted the list of
products and versions affected from NVD. This informa-
tion is integrated with the Common Platform Enumeration
(CPE) Dictionary. From the CPE, we extracted the list of
vulnerable versions based on the CPE Match Strings.4

Step 2: Instantiate update strategies

We employed a matrix representation to compare update
strategies and APT campaigns.

Each strategy is represented as a matrix in which the
rows represent the versions of the different products (e.g.
Acrobat Reader 9.2, Flash Player 11.0.1.152) and the column a
speciﬁc date with a month-base granularity (e.g. 12/2009).
A matrix cell is 1 if, on that date, that version of the product
is installed and otherwise 0. For a ﬁrst approximation,
we avoid considering the presence of multiple versions
installed for the same product5.

All strategies start from the same version, that is the
oldest vulnerable version of a campaign that is available at
the beginning of 2008. A strategy updates its version based
on the release date of a new version, the publication date,
and the reservation date of a CVE for the Immediate and
Planned, Reactive, and Informed Reactive strategy respectively.
The ﬁrst two strategies (Planned and Immediate) update
when a new release for the software is available (w/ and
w/o an update interval respectively). If multiple software
versions are released on the same date, they will update to
the newest consistent version.6

For the latter two strategies (Reactive and Informed Re-
active) the next version installed, if available, is the ﬁrst
most recent version that is not affected by the CVE. We also
considered the availability of updates based on the attack
scenario in Step 4.

Identiﬁcation of the outcome of attack scenarios

Depending on the availability of an update at the time of the
publication of the CVE we have to discern two scenarios:

• The release of an update is available before the pub-
lication of the CVE (tUr ≤ tVp ). The time when a
company may decide to update because it is aware of
the vulnerability is correctly computed from the time
when a new vulnerability is published. This is the
(implicit) assumption in [1], [27].

4. For example, CVE-2016-4113 affects all the versions of Flash Player
up to 21.0.0.213. The associated JSON NVD ﬁle does not provide the en-
tire list of affected versions (including the updates) in the CVE descrip-
tion but a CPE URI of the form cpe:2.3:a:adobe:ﬂash player:*:*:*:*:*:*:*:*”,
”versionEndIncluding”:”21.0.0.213”, we thus matched the CPE in the
CPE dictionary to get the list of all prior versions affected.

5. We assume an update is applied on enterprise’s machines at once.
6. For example, if the current JRE version installed is 6u6 and a new
update for JRE 5u13 is released after that, the update is ignored because
it represents a downgrade of a major update.

7

• The release of an update is available after the publica-
tion of the CVE (tVp < tUr ). In this case, computing the
time when a company may decide to update from the
time of publication of the vulnerability will include an
interval of time where a vulnerability for the version of
a product is known but a non-vulnerable version has
not been released yet (tUr − tVp ). The time available to
the company must be computed from the time when
the release is available.

Step 3: Instantiate APT campaigns events

We created a matrix for each APT campaign with the same
rows and columns of the update strategy matrix in Step
2. An entry is set to 1 if the version is affected by a
CVE exploited by the campaign from the date when the
campaign starts until 2020.

Step 4: Generate pessimistic scenarios

The updates and attacks have a month-based granularity
because most of the resources do not contain information
about the exact day in the month in which an update is
published or a campaign is performed. We further discuss
the limitation of these data in §7.

To balance possible interleaves between updates and
campaigns within the same month, we performed two
analyses: a pessimistic APT-ﬁrst scenario and an optimistic
Update-ﬁrst scenario, that assume the campaign is executed
before or after the update respectively.

To simulate the APT-ﬁrst scenario, we create a new ma-
trix from the update strategy matrix where we maintained
the previous version also in the month in which the new
update is installed. In other words, the two versions coexist
in the month. Thus, we simulate the application of the
update later in the month while allowing the APT to exploit
the vulnerability. This is done by keeping selected the entry
corresponding to the previous version also in the column in
which we move to another version for each update strategy
matrix generated in the Step 2.

Step 5: Compute conditional probability of being com-
promised

We evaluate at each instant of time, with a month-base granu-
larity, the sequence of versions installed on a set of software
products for each strategy and compare them with the
software exploited by the APTs to determine the potentially
successful campaigns.7 We use the term potentially successful
because the success of exploiting a vulnerability depends on
the characteristics of the execution environment [67]. A cam-
paign is considered successful if it exploits at least one of the
software products considered. From the matrix of updates
obtained from Step 2,5 and the matrix of campaigns obtained
from Step 3, we compute the conditional probability of being
compromised given one is targeted by the campaigns at a
given instant of time ti. The probability is computed as the

7. For example,

in 12/2009 the CVE-2009-4324, affecting Acrobat
Reader up to version 9.2, is exploited in the wild. If at any time from
12/2009 an update strategy updates to one of these versions, then the
campaign is potentially successful.

number of potentially successful campaigns at time ti over
the total number of campaigns active at that instant of time.

P (Compr|C, t = ti) =

|potentially successful campaignsti |
|active campaignsti |

(2)

where:

• |potentially successful campaignsti | is the number of
active campaigns at time ti that exploit at least one
version of a product currently installed at that time.
• |active campaignsti | is the total number of active cam-

paigns at time ti.

We computed the |potentially successful campaignsti | by
performing an element-wise product of the matrix of update
strategy with each matrix describing an APT campaign.
The resulting matrix identiﬁes the versions that were in-
stalled and exploited by the campaign in a given month.
With the sum of the rows of the resulting matrix, one
obtains a vector of values ≥ 0 for each ti .8 If an en-
is > 0, then the campaign is included in
try at time ti
|potentially successful campaignsti |.

The overall percentage of potentially successful cam-
paigns over the total number of campaigns in the entire
interval of time is computed as:

P (Compr|C) =

|potentially successful campaigns|
|campaigns|

=

|{C |∃ti : C ∈ potentially successful campaignsti }|
|campaigns|

(3)
In other words, the total number of potentially successful
campaigns is obtained from the set of campaigns that could
be successful in at least one instant of time ti. If a campaign
can succeed in several instants of time, it is counted only
once in the period of interest.

The number of software updates is obtained from the
matrix representing the strategy, by counting the number of
rows that contain at least one non-zero entry in the columns.

Step 6: Compare strategies effectiveness

For each update strategy, we obtain from Eq. 3 a probability
of being compromised based on the sample of campaigns
considered. To predict the range in which the probability of
being compromised for the entire population of campaigns
resides we compute a conﬁdence interval (CI). In case of
binary outcomes (success, failure), we compute the Agresti-
Coull conﬁdence interval [68] that is recommended when
the sample size is ≥ 40 [69]. From the CIs of different update
strategies, we can then compare their performance. Two
strategies are similar if their CIs signiﬁcantly overlap.

We then determine the percentage of campaigns for
which the two strategies behave in the same way by com-
puting the proportion of campaigns that either succeeded
or failed against both strategies. By computing the Agresti-
Coull interval for the resulting proportion we obtain the
range of similarity of the two strategies in terms of the
percentage of campaigns that both succeed or failed against
two update strategies.

8. Values can be > 1 if campaigns can exploit different products.

8

Only for 11 APTs ( 13%) we were not able to ﬁnd more than one resource
for their campaigns. This is typically due APTs that are not particularly
active or that are tracked by a single cyber-security company.

Fig. 2: Number of collected reports per APT.

5 DATASET
We considered only APT groups that launched at least one
campaign from 2008 to 01/2020 and for which a precise date
for the campaign is present in at least one report.

The ﬁnal database contains information about 86 APT
groups. For the excluded APTs, we either did not ﬁnd infor-
mation for their campaigns, or the date of their campaign
was not known. For example, the Kaspersky article [70]
provides a list of CVEs but does not provide information
about the campaign when they were exploited. Fig. 2 shows
the distribution of reports per APT.
For more than half of the APT campaigns saturation is reached
with at most 5 distinct resources, while for some APTs we
collected more than 15 and up to 22 different resources. Only
for 11 APTs, we collected a single resource, which typically is
a white paper containing detailed information about the APT’s
activity over an extended period of time.

We now answer RQ1 with a quantitative analysis of the
attack vectors employed, the vulnerabilities exploited, and
the software products targeted.

5.1 Attack Vectors

We analyzed the attack vectors exploited in the different
campaigns with the presence and absence of software vul-
nerability. Tab. 6 shows the different attack vectors and the
number of campaigns in which are observed. We underline
that a campaign can employ one or more attack vectors.9

We can observe that spear phishing is the main attack
vector [12], present in 130 campaigns that do not exploit
any vulnerability and 122 campaigns that exploit at least
one vulnerability. Interestingly drive-by compromise is not
only employed when a vulnerability is present but also used
to facilitate campaigns that employ social engineering to
trigger users to download malware.

We have 47 campaigns for which we do not know
the attack vector. For 9 of them, the report identiﬁed the
vulnerability exploited but not the attack vector.10 If this

9. For example, it is not uncommon to have campaigns that exploit

both spearphishing and drive-by compromise.

10. For example, some vulnerabilities (e.g. CVE-2012-0158) can be

exploited via spearphishing techniques and drive-by compromise.

TABLE 6: Attack vector campaigns and software vulns

Attack vector
Spear phishing
Drive-by Compromise
Supply Chain Compromise
Valid Accounts
External Remote Services
Exploit Public-Facing Appl.
Replic. via Remov. Media
Undetermined
Total

# of Campaigns

w/o vuln w/ at least one vuln
122*
34*
0
1
0
7
1
9*
174 (162 unique)

130*
15*
5*
3*
3
3*
0
38*
197 (190 unique)

* Contains duplicates due to multiple attack vectors.

TABLE 7: Top 10 client-side and Top 10 server-side/O.S.
products exploited

The products are obtained from the CVEs exploited in a campaign. If a
CVE affects multiple products, all the software are considered. Products
are distinguished in client-side, server-side application, and O.S.

Product

Flash Player (EOL)

Vendor
Microsoft Ofﬁce
Microsoft Windows 2008 Server
Microsoft Windows 7
Microsoft Windows Vista
Microsoft Windows 2012 Server
Adobe
Microsoft Windows 8.1
Microsoft
Microsoft
Microsoft
Microsoft
Microsoft
Microsoft Windows 10
Microsoft Windows 8
Microsoft
Adobe
Microsoft
Adobe
Oracle
Oracle

IE (EOL)
Acrobat Reader
.NET framework
Air
JRE
JDK

Commerce Server
SQL server
Visual Basic
Visual FoxPro
BizTalk Server

Software
Client
O.S.
O.S.
O.S.
O.S.
Client
O.S.
Server
Server
Client
Client
Server
O.S.
O.S.
Client
Client
Client
Client
Client
Client

# Campaigns (%)
68 (41.9%)
49 (30.2%)
43 (26.5%)
41 (25.3%)
39 (24.0%)
35 (21.6%)
29 (17.9%)
19 (11.7%)
19 (11.7%)
19 (11.7%)
19 (11.7%)
18 (11.1%)
18 (11.1%)
14 (8.6%)
13 (8.0%)
11 (6.8%)
5 (3.1%)
5 (3.1%)
4 (2.5%)
4 (2.5%)

information is not present in the report, we avoided making
assumptions. For the remaining campaigns, the information
about the attack vector was vague or missing.11

5.2 Popular Products and CVEs

We observed 118 unique vulnerabilities exploited by the
APTs in at least one campaign between 2008 and 2020. Some
CVEs are exploited in several campaigns by different APTs.
Tab. 7 shows the ten most targeted client-side applica-
tions and the ten most targeted server/O.S. products based
on the exploited CVEs. A campaign is counted over different
products if the CVE employed is applicable to different soft-
ware products. For example, CVE-2012-0158 affects Ofﬁce,
SQL server, Visual Fox Pro, and Commerce Server.12 Ofﬁce
is by far the major target of campaigns followed by Windows
O.S. and Flash Player. This is coherent with the attack vectors
previously observed as they are commonly exploited via
spearphishing with malicious attachments.

APTs tend to ”share” vulnerabilities during their
campaigns. Only 8 APTs (Stealth Falcon, APT17,
Equation, Dragonfly, Elderwood, FIN8, DarkHydrus,

9

The number of unique vulnerabilities employed in Unknown-Unknown
(UU) and Known-Unknown (KU) attack scenarios grows signiﬁcantly in
recent years, compared to the ﬁrst years of observation. On average
around 5 distinct vulnerabilities per year are exploited by APTs.

Fig. 3: Number of distinct vulnerabilities exploited over the
years by different attack scenarios.

and Rancor) exploit CVEs that are not used by anyone
else.13 We are aware of vulnerabilities (e.g CVE-2017-0144)
that are associated with Equation and used by other APTs,
but we did not ﬁnd enough information about the date
when the vulnerabilities were employed. Roughly 35% of
the APTs exploit CVEs observed in campaigns of other
groups. 17 APTs share 4 or more vulnerabilities, while many
APTs sharing a single vulnerability have only exploited that
vulnerability during their campaigns (14 out of 20).

5.3 Evolution in exploiting vulnerabilities

Fig. 3 shows the evolution of the number of unique vulner-
abilities exploited in the *-unknown attack scenarios14 in our
database. It represents a lower bound of the vulnerability
exploited in the wild. Project Zero [72] collects information
about 0-days in the wild by including also unattributed at-
tacks. The mean number of distinct vulnerabilities exploited
per year is roughly 5. We can observe how the numbers
grew signiﬁcantly in recent years. However, it can be inﬂu-
enced by the limited number of reports for campaigns in the
early period (2008-2011), where it was less likely to report
information about cyber-attacks. The drop for 2019/2020 is
due to the natural delay of publicly reporting campaigns
caused by the proximity of the period of data collection
with the date of the campaigns themself. Thus, we expect
the values to be higher if recomputed in the future.

Looking at the occurrence of a CVE in an APT campaign,
the majority of the APTs prefer to exploit CVE already
published, with few APTs as exceptions.15

5.4 Software for Analysis of Update Strategies

As discussed in §4, the collection of update releases from
vendors’ websites is a manual procedure. Here, for a ﬁrst
approximations, we focus on collecting updates for a subset
of all software targeted by APTs.

13. Only three APTs have exploited more than one vulnerability

11. For example, the Sony hack campaign in 2014 [71].
12. We do not have information about the exact software targeted.

For example, they could all have exploited Ofﬁce.

during all their campaigns.

14. Either already reserved (KU) or not reserved (UU).
15. Stealth Falcon, PLATINUM, APT17

Tab. 7 shows the most targeted products by vendor. We
decided to cover the most exploited client-side product for
each vendor because (1) from Tab. 6 most of the campaigns
exploit attack vectors directed to client-side software and (2)
it is not uncommon to have products from these vendors in
an enterprise computer. For Adobe, the Flash Player product
is end of life (EOL) thus we decided to include the other two
software products Reader and Air. Even if Flash Player is EOL
in 2020 we still think it is interesting to see how different
update strategies would affect the security of enterprises
because it has been frequently exploited in the last years.
Also, vendors’ EOL of products, unfortunately, does not
coincide with the disappearance from the ﬁeld and end of
exploitation as we are observing with Internet Explorer [73].
For a ﬁrst approximation, we limited the analysis to
the Ofﬁce 2016 release only, as different releases (Ofﬁce
2013, Ofﬁce 365) can be seen as different products as they
require buying a different license each. We considered the
Knowledge Base (KB) updates from the Microsoft Update
Catalog as the versions of the software. We assumed that KB
updates for Ofﬁce are cumulative, i.e. the package contains
all previously released ﬁxes.

In summary, we collected releases of updates for 5 different
software products from 3 different vendors: Ofﬁce, Flash Player,
Acrobat Reader, Air, and JRE. We considered only releases for
Microsoft Windows O.S. as it covers at least half of the enterprise
computers [74]. With this set of software products, we cover 44%
of the campaigns (that exploit software vulnerabilities), 62% of
the APT groups, and 33% of the CVEs.

6 QUANTITATIVE ANALYSIS OF UPDATES
We now present an analysis of the speed of exploitation of
individual vulnerabilities and the prevalence of *-Unknown
and Known-Known attacks in APT campaigns. We then quan-
titatively evaluate the effectiveness and cost of the different
update strategies against the APT campaigns.

6.1 Survival Analysis

We performed preliminary survival analysis on the vul-
nerabilities to compute the interval in months that passed
from the publication of the CVE and the ﬁrst campaign that
exploited the CVE (exploit age). Fig. 4 shows the Kaplan-
Meier plot for all the products in our database and for
the set of products discussed in §5 (Ofﬁce, Flash Player,
Reader, Air, and JRE). We can see that roughly 40% of
the vulnerabilities are exploited for the ﬁrst time before the
publication. This is coherent with what was observed by
Chen et al. [8], where 49% of the CVEs are exploited before
the NVD score is published. Furthermore, roughly 27%
of the vulnerabilities are exploited the ﬁrst time16 within a
month from the publication from NVD showing that APTs
are fast to exploit new CVE [75]. Another interesting fact is
that a signiﬁcant number of vulnerabilities are exploited a
few months before the NVD publication. This phenomenon
can be partially explained because the observation of attacks
in the wild brings software vendors to know about the vul-
nerability and thus the publication of a CVE. It is important
to underline that this value does not mean that ≈40% of the

16. Among all the APTs.

10

All products
Office,Flash,Reader,Air,JRE

1

0.8

0.6

0.4

0.2

0
-50 -40 -30 -20 -10

0

10

20

30

40

50

60

Months from publication [months]

n
o
i
t
a
t
i
o
p
x
e

l

r
o
f

t
s
e
r
e
t
n

i

.
b
o
r
p

l

e
v
i
t
a
u
m
u
C

The survival is based on the ﬁrst time the CVE is exploited in a campaign.
More than half of the vulnerabilities are exploited for the ﬁrst time within
one month from the publication. However, there is high survivability of
a small set of CVEs (roughly 10%) that are exploited after more than 1
year from the publication. If we consider only Ofﬁce, Flash Player, Reader,
Air, and JRE the behavior is similar.

Fig. 4: Proportion of survival of CVE from publication
(NVD) for all products and a subset (Ofﬁce, Flash Player,
Reader, Air, and JRE).

campaigns are unpreventable because 1) *-Unknown attacks
can exploit several vulnerabilities17 and 2) many of these
CVEs are exploited multiple times from different APTs after
months from their ﬁrst exploitation.

If we only consider the vulnerabilities exploited the ﬁrst
time in KK attacks (as in [76]), we observe that roughly 47%
of them are exploited within 30 days from their publica-
tions18. In contrast with previous results [77], we observed
a long tail for part of the vulnerabilities, one out of 10 CVE
is exploited after one year from its publication, and 1 out of
20 after more than two years.

6.2 Classiﬁcation of APT campaigns

Each APT campaign exploiting at least one vulnerability ﬁts
into one of these (possibly overlapping) groups:

• Campaigns with at least one Known-Known (KK) attack
. In other words, the campaign exploited at least one
vulnerability (either preventable or unpreventable) that
was already present in the NVD database.

• Campaigns with at least one Known-Unknown (KU)
attack. In other words, the campaign exploited at least
one vulnerability (either preventable or unpreventable)
that was not present in the NVD database but an entry
was already reserved by MITRE.19

• Campaigns with at least one Unknown-Unknown (UU)
attack. In other words, the campaign exploited at least
one vulnerability (either preventable or unpreventable)
that was not even reserved by MITRE.
Out of 352 campaigns,

them
least one vulnerability (Tab. 6). Figure 5
162

resulting Venn diagram for

than half of

employ at
the
shows

less

the

17. A famous example is Stuxnet.
18. Bilge et al. [76] observed a similar value of roughly 42%
19. Thus, a small number of people known already some information

about the vulnerability. E.g. vulnerability researchers.

 
 
 
 
TABLE 8: Optimistic (Update ﬁrst) and pessimistic (APT
ﬁrst) overall conditional probability of being compromised
for different update strategies and update interval with the
associated # of updates for the period [01/2008-01/2020]

11

Update

Strategy

#Updates

Prob.

Odds

Interval

(Update ﬁrst — APT ﬁrst)

/

Immediate

Planned

1 Month

Reactive

Informed Reactive

360

357

44

44

22.2-58.3%

1x-4.9x

58.3-63.9%

4.9x-6.2x

61.1-66.7%

5.5x-7.0x

58.3-66.7%

4.9x-7.0x

Planned

350

72.2-75.0%

9.1x-10.5x

3 Months

Reactive

Informed Reactive

44

44

73.6-76.4%

9.8x-11.3x

73.6-76.4%

9.8x-11.3x

Planned

337

86.1-87.5% 21.7x-24.5x

7 Months

Reactive

Informed Reactive

44

44

84.7-86.1% 19.4x-21.7x

84.7-86.1% 19.4x-21.7x

Comparing the Reactive and Informed Reactive strategies,
there is a small advantage in knowing about not publicly
known vulnerabilities only if the update interval is small.
Once the enterprise waits 3 to 7 months, the vulnerability is
now publicly known and actively exploited by the APTs.

We reported in Fig. 6 the Agresti-Coull Interval for
each update strategy for the different update intervals. The
Planned, Reactive, and Informed Reactive strategies are almost
identical as we see a signiﬁcant overlap of the CI among
these three strategies. The probability of being compromised
lies within [52%-74%] for the Planned and Informed Reactive
and [55%-77%] for the Reactive in the pessimistic scenario.
In the case of an optimistic Update-ﬁrst scenario, we observe
that there is a clear difference between the Immediate and
the Planned strategies, while this advantage is lost in the
case of the pessimistic APT-ﬁrst scenario. To evaluate the
similarity we computed, for each pair of strategies, the
proportion of campaigns that either succeeded or failed
against both strategies. We estimate the Agresti-Coull CI for
the resulting proportions. The results show that the Planned
and Reactive behave in the same way for at least 90% up to
99% of the campaigns for a 1 month update interval, for
88% and up to 98% for a 3 months update interval, and for
92% up to 99% for a 7 months interval. While, the Reactive
and Informed Reactive behave in the same way for 90% and
up to 99% of the campaigns for a 1 months update interval,
and for 94% up to 100% for a 3 and 7 months interval.
Since hackers focus on new versions, a strategy that always
updates to the new version but with a delay gives time to the
APT to target and exploit a vulnerability. In contrast, a reactive
approach that updates rarely might present to attackers an older
version that does not include the new vulnerable code [78]. In
other words, either you update always and immediately to the
new versions or just updating lately has the same risk proﬁle but
cost you a lot more than updating rarely [79].

7 LIMITATIONS
The dataset obtained is based on publicly available reports.
While this is just a small part of existing campaigns, this

The majority of campaigns exploited at least one vulnerability in
a KK attack (after publication by NVD and after reservation by
MITRE). Only a few launched UU attacks (both before reserva-
tion by MITRE and before publication by NVD).

Fig. 5: Classiﬁcation of APT Campaigns.

interest.

119 out of

campaigns
campaigns of
employed only vulnerabilities in Known-Known attacks.
APTs heavily exploit known CVE to compromise their target.
The prioritization of updates is thus a key factor that can
signiﬁcantly reduce the impact of APTs campaigns.

162

6.3 Evaluation of software updates strategies

We now answer RQ2 by applying our methodology (§4)
to compute the overall probability of being compromised
(Eq. 3) in the interval of time [Jan 2008-Jan 2020] with the
updates strategies and update interval presented in §2 for
the software discussed in § 5.4. Tab. 8 summarizes the results
in terms of the number of updates required, the conditional
probability and the odds ratio for the optimistic (Update ﬁrst)
and pessimistic (APT ﬁrst) scenarios.

Updating the software as soon as a new release is
available (Immediate strategy) provides the optimal lower-
bound probability of being compromised. Even in this case,
roughly 1 out of 4 campaigns can compromise the target.
Although an immediate update can be applied in some
critical situations, if we consider a more realistic approach in
which the software is updated with some delay in the month
(Immediate with APT ﬁrst), the odds of being compromised
increases by a factor of 5.

The Planned strategy provides a similar, although slightly
better, probability of being compromised compared to a
strategy that waits for the presence of public vulnerabilities
(Reactive strategy). However, waiting to update when a CVE
is published presents 8x times fewer updates. Thus, if an
enterprise cannot keep up with the updates and need to wait
before deploying them, can consider being simply reactive.
For the Planned strategy the number of updates decreases
with bigger intervals because the updates are shifted outside
of the period of observation. If a longer update interval is
used, the probability of being compromised increases by
a factor of 9 and 20 for 3 months and 7 months update
intervals respectively. Interestingly, for the 7 months delay,
we have that the Reactive and Informed Reactive perform
slightly better than the Planned strategy.

95% CI 1 Month Update Interval

95% CI 3 Months Update Interval

95% CI 7 Months Update Interval

12

Informed Reactive

Reactive

Planned

Immediate

0

10 20 30 40 50 60 70 80 90 100
Prob. of being compromised (%)

0

10 20 30 40 50 60 70 80 90 100
Prob. of being compromised (%)

0

10 20 30 40 50 60 70 80 90 100
Prob. of being compromised (%)

(a) Update-ﬁrst (optimist) scenario. There is a difference between the Immediate and the other strategies. However, Planned, Reactive,
and Informed Reactive behave similarly thus updating to each new version with some delay or relying on reserved CVE does not
worth it.

95% CI 1 Month Update Interval

95% CI 3 Months Update Interval

95% CI 7 Months Update Interval

Informed Reactive

Reactive

Planned

Immediate

0

10 20 30 40 50 60 70 80 90 100
Prob. of being compromised (%)

0

10 20 30 40 50 60 70 80 90 100
Prob. of being compromised (%)

0

10 20 30 40 50 60 70 80 90 100
Prob. of being compromised (%)

(b) APT-ﬁrst (pessimist) scenario. The Immediate and Planned present a similar behavior for the 1 month update interval but differ
with bigger intervals. In the pessimistic scenario the Planned, Reactive, and Informed Reactive behave similarly thus updating to each
new version with some delay or relying on reserved CVE does not worth it.

Fig. 6: Agresti-Coull Interval (CI) for the update strategies with different update intervals

paper is the ﬁrst that tries to aggregate a manually validated
dataset of APTs campaigns, CVE, and vulnerable products
and it is a ﬁrst step in the direction of an open and extensive
dataset on APT campaigns.

The process to obtain information about campaigns was
semi- automated but required manual effort to analyze and
to extract the key information about campaign dates, CVE,
and attribution. We assume that this type of information
reported by reputable security companies is not deliber-
ately wrong, and our methodology strives to ﬁnd multiple
sources reporting the same campaign to control for possible
errors. Since keyword-based automated searches (e.g. [25])
present limitations in the number of false associations that
they generate, we decided that a manual approach would
provide a more precise description of the APT ecosystem.
Although the manual extraction of information from re-
ports does not present difﬁculties, it can include erroneous
matching of APT campaigns. To limit that, the manual
analysis was performed by two researchers independently
and inconsistencies were resolved by a third researcher.

We decided to ignore reports about campaigns where
not enough information about the start and attribution was
available. Thus, it is possible that certain vulnerabilities
discussed in the reports are not included in the dataset.

We applied a conservative approach in extracting infor-
mation from different reports reporting mutually disjoint
CVEs exploited on the same date. Thus, potentially assum-
ing fewer campaigns with a higher number of CVEs each.
The probability of being compromised must be seen as an
upper bound of what APT can achieve. However, the odds
ratio between update strategies remains the same.

We relied on the NVD data as the industry standard but
it is known to contain errors in the list of product names,
CVE publication date [80] and vulnerable versions [81],
[82]. We leave for future work the application of these
approaches to ﬁnd inconsistencies. We relied on the data

of observation of the campaigns as reported in the reports
we consulted. This information could be wrong and detect
only a more recent campaign. We tried, when possible, to
ﬁnd multiple resources about the campaign. The collection
of release dates for the software discussed in §5.4 is collected
manually given that vendors’ repositories are not intended
for past versions. Thus, the releases collected and employed
in the evaluation might have errors and this could affect the
Immediate and Planned strategies.

We used a month-based date granularity for the pub-
lication of the CVE, the release of new versions, and the
date of the campaigns because the exact day in a month
in which the campaign started is not known. This decision
has a potential impact on the results. If a campaign for a
CVE published on 29/01/2017 started on 01/02/2017 then
in our case the exploit age is one month, even if the CVE
is exploited a few days after the publication. However,
the results we observed (e.g. exploit age of vulnerabilities)
are coherent with previous observations of attacks in the
wild [76], thus we think that the number of these cases is
minimal and do not affect the results.

The same considerations apply to the results in Tab. 8: if
a release is performed on 15/02/2019 and a campaign ex-
ploiting the software is executed on 03/02/2019, the month
granularity would traduce both actions as performed on
02/2019. We thus considered two complementary scenarios:
an optimistic scenario (Update ﬁrst) and a pessimistic scenario
(APT ﬁrst). In the Update ﬁrst the example above will traduce
in the defender be able to update before the execution of the
campaign. While in the APT ﬁrst we assumed the opposite.
Finally, those companies that have an update interval
that is less than a month will present a probability of being
compromised that stays between the Immediate Update-ﬁrst
and the Immediate APT-ﬁrst scenarios.

We assumed that a campaign will be carried on from
the date when the campaign started up to the end of

the observation (i.e. 2020). This causes an inﬂation of the
number of campaigns that are active at a given instant of
time in Eq. 2. However, we follow a conservative approach
and assumed that if an APT has access to a vulnerability
it will always be able to employ it given that one is under
attack. We discuss extensions in the §8.

8 CONCLUSIONS AND FUTURE WORK
In this work, we proposed a methodology to quantitatively
investigate the effectiveness and cost of software updates strategies
against APT Campaign. We applied the methodology to build
a database of APT campaigns and presented an analysis of
the attack vectors, vulnerabilities, and software exploited by
86 different APTs in more than 350 campaigns over 12 years.
The database is publicly available on Zenodo [26].

In contrast to expectation, we showed that preventive
mechanisms like updates can inﬂuence the probability of
being compromised by APT. However, software updates
based on wrong measures of risk can be counterproductive.
Our analysis shows that a purely Reactive update strategy
(wait until a vulnerability gets out) presents results very
similar to a Planned strategy (always update to the newest
version), but with only 12% of the updates. Furthermore,
the Informed Reactive strategy, where updates are applied
based on reserved information about not publicly known
vulnerabilities (e.g. by paying for information on 0-days),
does not produce signiﬁcant advantages compared to
the Reactive strategy and it is useless if the enterprise
has several months of delay before applying the update.
In summary, for the broadly used products we analyzed, if you
cannot keep updating always and immediately (e.g. because you
must do regression testing before deploying an update), then
being purely reactive on the publicly known vulnerable releases
has the same risk proﬁle than updating with a delay but costs
signiﬁcantly less.

Future work can extend the analysis to a more complete
set of software products and evaluate a subset of campaigns
by targeted enterprises, attacker preferences, or network ex-
posure based on IDS alerts [63]. To achieve that, one would
require to have company-speciﬁc information to move from
a conditional probability to an absolute probability.

We also plan to extend the evaluation by considering
campaigns as active only for a limited period. Further data
about the lifetime of campaigns in the wild is required.

ACKNOWLEDGMENTS
We thank the student Veronica Chierzi for helping in the
data collection of update releases. This work was partly
funded by the European Union under the H2020 Pro-
gramme under grant n. 830929 (CyberSec4Europe) and
n.952647 (AssureMOSS).

REFERENCES

[1] P. Kotzias et al., “Mind your own business: A longitudinal study
of threats and vulnerabilities in enterprises,” in Proc. of NDSS-19,
2019.
I. Pashchenko et al., “A qualitative study of dependency manage-
ment and its security implications,” in Proc. of ACM-CCS-20, 2020.
[3] M. Bozorgi et al., “Beyond heuristics: learning to classify vulnera-

[2]

bilities and predict exploits,” in Proc. of SIGKDD-10, 2010.

13

[4] L. Allodi and F. Massacci, “Comparing vulnerability severity and

exploits using case-control studies,” TISSEC-14, 2014.

[5] L. Bilge et al., “Riskteller: Predicting the risk of cyber incidents,”

in Proc. of ACM-CCS-17, 2017.

[6] L. Allodi, “Economic factors of vulnerability trade and exploita-

[7]

tion,” in Proc. of ACM-CCS-17, 2017.
J. Jacobs et al., “Improving vulnerability remediation through
better exploit prediction,” in Proc. of WEIS-19, 2019.

[8] H. Chen et al., “Using twitter to predict when vulnerabilities will

be exploited,” in Proc. of SIGKDD-19, 2019.

[9] L. Allodi et al., “The work-averse cyberattacker model: Theory and
evidence from two million attack signatures,” Risk Analysis, 2021.
[10] A. Rot and B. Olszewski, “Advanced persistent threats attacks
in cyberspace. threats, vulnerabilities, methods of protection,” in
Proc. of FedCSIS-17, 2017.

[11] A. Lemay et al., “Survey of publicly available reports on advanced

persistent threat actors,” Computers & Security, 2018.

[12] T. Urban et al., “Plenty of phish in the sea: Analyzing potential

pre-attack surfaces,” in Proc. of ESORICS-20, 2020.

[13] P. Chen et al., “A study on advanced persistent threats,” in Proc. of

IFIP-14, 2014.

[14] M. Marchetti et al., “Analysis of high volumes of network trafﬁc for

advanced persistent threat detection,” Computer Networks, 2016.

[15] T. Steffens, Attribution of Advanced Persistent Threats - How to

Identify the Actors Behind Cyber-Espionage. Springer, 2020.

[16] Kaskersky, 2016, https://securelist.com/cve-2015-2545-overview-

of-current-threats/74828/. Accessed: 2020-11-01.

[17] Crowdstrike, 2014, https://www.crowdstrike.com/blog/french-
connection-french-aerospace-focused-cve-2014-0322-attack-
shares-similarities-2012/. Accessed: 2020-11-01.

[18] X. Bouwman et al., “A different cup of ti? the added value of
commercial threat intelligence,” in Proc. of USENIX-20, 2020.
[19] V. G. Li et al., “Reading the tea leaves: A comparative analysis of

threat intelligence,” in Proc. of USENIX-19, 2019.

[20] M. Ussath et al., “Advanced persistent threats: Behind the scenes,”

in Proc. of CISS-16, 2016.

[21] N. Virvilis and D. Gritzalis, “The big four - what we did wrong in

advanced persistent threat detection?” in Proc. of ARES-13, 2013.

[22] L. Anthony (Tony) Cox Jr, “What’s wrong with risk matrices?” Risk

Analysis: An International Journal, vol. 28, 2008.

[23] H. Kunreuther, “Risk analysis and risk management in an uncer-
tain world 1,” Risk Analysis: An International Journal, vol. 22, 2002.
[24] D. A. Scheufele and D. Tewksbury, “Framing, agenda setting, and
priming: The evolution of three media effects models,” Journal of
communication, vol. 57, 2007.

[25] G. Laurenza and R. Lazzeretti, “daptaset: A comprehensive map-

[26] G. D.

ping of apt-related data,” in Proc. of FINSEC-19, 2019.
al.,

“Advanced
2022.
(APTs)
https://doi.org/10.5281/zenodo.6514817

et
campaigns

database,”

Tizio

Persistent
Threats
[Online]. Available:

[27] A. Nappa et al., “The attack of the clones: A study of the impact of

shared code on vulnerability patching,” in Proc. of SSP-15, 2015.

[28] C. Xiao et al., “From patching delays to infection symptoms: Using
risk proﬁles for an early discovery of vulnerabilities exploited in
the wild,” in Proc. of USENIX-18, 2018.

[29] C. Bogart et al., “How to break an API: cost negotiation and com-
munity values in three software ecosystems,” in Proc. of FSE’16,
2016.

“Sans

[30] SANS,
2019,
vey
room/whitepapers/analyst/membership/38900.
2020-12-01.

sur-
management
https://www.sans.org/reading-
Accessed:

vulnerability

2019,”

[31] P. Giura and W. Wang, “A context-based detection framework for

advanced persistent threats,” in Proc. of ICCS-12, 2012.

[32] W. Zhao et al., “Extended petri net-based advanced persistent
threat analysis model,” in Computer Engineering and Networking,
2014.

[33] P. Bhatt et al., “Towards a framework to detect multi-stage ad-
vanced persistent threats attacks,” in Proc. of SOSE-14, 2014.
[34] S. Chandran et al., “An efﬁcient classiﬁcation model for detecting

advanced persistent threat,” in Proc. of ICACCI-15, 2015.

[35] I. Friedberg et al., “Combating advanced persistent threats: From
network event correlation to incident detection,” Computers &
Security, 2015.

[36] M. Marchetti et al., “Countering advanced persistent

threats
through security intelligence and big data analytics,” in Proc. of
CyCon-16, 2016.

[68] A. Agresti and B. A. Coull, “Approximate is better than “exact”
for interval estimation of binomial proportions,” The American
Statistician, vol. 52, 1998.

[69] L. D. Brown et al., “Interval Estimation for a Binomial Proportion,”

14

Statistical Science, vol. 16, 2001.
“Equation

[70] Kaspersky,
and
https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2018/03/08064459/Equation group questions and answers.pdf

questions
Available:

group:
[Online].

answers,”

2015.

[71] Novetta,
the
https://www.operationblockbuster.com/wp-content/uploads/2016/02/Operation-Blockbuster-Report.pdf.
Accessed: 2020-10-01.

“Operation
thread

unraveling
2016,

-
attack,”

blockbuster

sony

long

the

of

[72] ProjectZero,

2021,
https://googleprojectzero.blogspot.com/p/0day.html. Accessed:
2021-05-15.

wild”,”

“0day

the

”in

[73] Kaspersky, 2020, https://securelist.com/ie-and-windows-zero-

day-operation-powerfall/97976/. Accessed: 2020-12-01.
“Operating
market
still

[74] I.
–
top!”
https://kommandotech.com/statistics/operating-system-
market-share/. Accessed: 2021-01-04.

Stevanovic,
gates

system
at

alone

bill

the

is

share
2020,

[75] FireEye,

fast:

Time

“Think
and

between
exploitation
part

disclosure,

patch
intelligence
2020,

vulnerability

vulnerability

release
for
https://www.ﬁreeye.com/blog/threat-research/2020/04/time-between-disclosure-patch-release-and-vulnerability-exploitation.html.
Accessed: 2020-10-01.

management,

two,”

-

[76] L. Bilge and T. Dumitras, “Before we knew it: an empirical study
of zero-day attacks in the real world,” in Proc. of ACM-CCS-12,
2012.

[77] K. Nayak et al., “Some vulnerabilities are different than others -
studying vulnerabilities and attack surfaces in the wild,” in Proc.
of RAID-14, 2014.

[78] S. Dashevskyi et al., “A screening test for disclosed vulnerabilities
in FOSS components,” IEEE Trans. Software Eng., vol. 45, 2019.
[79] F. Massacci et al., “Solarwinds and the challenges of patching: Can
we ever stop dancing with the devil?” IEEE Secur. Priv., vol. 19,
2021.

[80] A. Anwar et al., “Cleaning the nvd: Comprehensive quality assess-

ment, improvements, and analyses,” IEEE TDSC, 2021.

[81] Y. Dong et al., “Towards the detection of inconsistencies in public
security vulnerability reports,” in Proc. of USENIX-19, 2019.
[82] V. H. Nguyen et al., “An automatic method for assessing the ver-
sions affected by a vulnerability,” Empirical Software Engineering,
2016.

[37] G. Brogi and V. V. T. Tong, “Terminaptor: Highlighting advanced
persistent threats through information ﬂow tracking,” in Proc. of
NTMS-16, 2016.

[38] K. Pei et al., “HERCULE: attack story reconstruction via commu-
nity discovery on correlated log graph,” in Proc. of ACSAC-16,
2016.

[39] I. Ghaﬁr et al., “Detection of advanced persistent threat using
machine-learning correlation analysis,” Future Generation Comp.
Syst., 2018.

[40] D. Sahabandu et al., “DIFT games: Dynamic information ﬂow
tracking games for advanced persistent threats,” in Proc. of IEEE
CDC-18, 2018.

[41] X. Shu et al., “Threat intelligence computing,” in Proc. of ACM-

CCS-18, 2018.

[42] S. M. Milajerdi et al., “HOLMES: real-time APT detection through
correlation of suspicious information ﬂows,” in Proc. of SSP-19,
2019.

[43] Y. Shen and G. Stringhini, “ATTACK2VEC: leveraging temporal
word embeddings to understand the evolution of cyberattacks,”
in Proc. of USENIX-19, 2019.

[44] W. U. Hassan et al., “Tactical provenance analysis for endpoint

detection and response systems,” in Proc. of SSP-20, 2020.

[45] X. Han et al., “Unicorn: Runtime provenance-based detector for

advanced persistent threats,” in Proc. of NDSS-20, 2020.

[46] A. Alsaheel et al., “Atlas: A sequence-based learning approach for

attack investigation,” in Proc. of USENIX-21, 2021.

[47] P. Hu et al., “Dynamic defense strategy against advanced persis-

tent threat with insiders,” in Proc. of INFOCOM-15, 2015.

[48] L. Yang et al., “A risk management approach to defending against
the advanced persistent threat,” IEEE Transactions on Dependable
and Secure Computing, 2018.

[49] X. Liao et al., “Acing the IOC game: Toward automatic discovery
and analysis of open-source cyber threat intelligence,” in Proc. of
ACM-CCS-16, 2016.

[50] K. Satvat et al., “EXTRACTOR: extracting attack behavior from

threat reports,” in Proc. of EuroSP’21, 2021.

[51] F. Barr-Smith et al., “Survivalism: Systematic analysis of windows

malware living-off-the-land,” in Proc. of SSP-21, 2021.

[52] J. Sexton et al., “Attack chain detection,” Statistical Analysis and

Data Mining, 2015.

[53] M. Almukaynizi et al., “Proactive identiﬁcation of exploits in the
wild through vulnerability mentions online,” in Proc. of CyCon-17,
2017.

[54] C. Sabottke et al., “Vulnerability disclosure in the age of social
media: Exploiting twitter for predicting real-world exploits,” in
Proc. of USENIX-15, 2015.

[55] D. W. Woods and R. B ¨ohme, “Systematization of knowledge:

Quantifying cyber risk,” in Proc. of S&P-21, 2021.

[56] Y. Liu et al., “Cloudy with a chance of breach: Forecasting cyber

security incidents,” in Proc. of USENIX-15, 2015.

[57] A. Sarabi et al., “Patch me if you can: A study on the effects of
individual user behavior on the end-host vulnerability state,” in
Proc. of PAM-17, 2017.

[58] A. Arora et al., “An empirical analysis of software vendors’ patch
release behavior: Impact of vulnerability disclosure,” Inf. Syst. Res.,
2010.

[59] S. Clark et al., “Moving targets: Security and rapid-release in

ﬁrefox,” in Proc. of ACM-CCS-14, 2014.

[60] A. Ozment and S. E. Schechter, “Milk or wine: Does software

security improve with age?” in Proc. of USENIX-06, 2006.

[61] Y. Beres et al., “Analysing the performance of security solutions
to reduce vulnerability exposure window,” in Proc. of ACSAC-08,
2008.

[62] B. C. Ezell et al., “Probabilistic risk analysis and terrorism risk,”

Risk Analysis, vol. 30, 2010.

[63] L. Allodi and F. Massacci, “Security events and vulnerability data
for cybersecurity risk estimation,” Risk Analysis, vol. 37, 2017.
[64] R. Anderson et al., “Measuring the changing cost of cybercrime,”

in In Proc. of WEIS-19, 2019.

[65] S. Dambra et al., “Sok: Cyber insurance - technical challenges and

a system security roadmap,” in Proc. of SSP-20, 2020.

[66] ThaiCERT, 2019, https://www.thaicert.or.th/downloads/ﬁles/A Threat Actor Encyclopedia.pdf.

Accessed: 2020-06-01.

[67] S. Dashevskyi et al., “TESTREX: a testbed for repeatable exploits,”

in Proc. of CSET’14, 2014.

Dataset and Replication Guide - Software
Updates Strategies: a Quantitative Evaluation
against Advanced Persistent Threats

Giorgio Di Tizio, Michele Armellini, Fabio Massacci

1

2
2
0
2

y
a
M
5
2

]

R
C
.
s
c
[

2
v
9
5
7
7
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—This is the replication guide for the paper Software Updates Strategies: a Quantitative Evaluation against Advanced
Persistent Threats

✦

1 INTRODUCTION

T HE supplementary materials in this paper provides

additional information about the terminology used (§2),
the data collection procedure (§3), the structure of the
database (§4), the state-of-the-art on APTs (§5), the dataset
(§6 and §7), and the methodology applied to evaluate efﬁ-
cacy of update strategies (§8).

vectors/CVEs are employed in a campaign, we added
one entry for each distinct attack vector/CVE.

3) we searched for additional resources by web searches
using as keywords: ”APT name CVE”. We collected
new resources until saturation was reached. For each
new resource we reapplied 2).

2 LISTS OF TERMS

Tab. 1, 2, and 3 contain the terminology that is used in the
paper. Some of these deﬁnitions are taken from the STIX
speciﬁcation1.

3 PROCEDURE FOR DATA COLLECTION

We describe the manual procedure employed to collect data
about APT campaigns and software updates from unstruc-
tured resources like technical reports and vendors’ websites.

3.1 Collection of APT campaigns

We started from the list of APT groups obtained from
MITRE [1]. For each group:

1) we searched in the ThaiCERT Threat Group Cards
v1.01 [2] the resources describing campaigns associated
with the group2.

2) we manually read each resource to identify the charac-
teristics of the campaign in terms of date of execution,
attack vector(s), and CVE(s) exploited. If a resource
contains a date3, we created an entry that identify the
campaign by its date of execution with additional in-
formation about the CVE exploited and the attack vec-
tor (technique): <APT name,CVE exploited,date start>,
<APT name,technique,date start>.
If multiple attack

• G. Di Tizio (corresponding author) is with University of Trento, Italy.

E-mail: giorgio.ditizio@unitn.it

• M. Armellini is with University of Trento, Italy.
•

F. Massacci is with University of Trento, Italy and Vrije Universiteit
Amsterdam, The Netherlands.

3.2 Collection of Software updates

We collected the information about the date and version of
updates for 5 client-side applications: Ofﬁce 2016, Acrobat
Reader, Flash Player, Air, and JRE. We visited the ofﬁcial
websites of the vendors (Microsoft, Adobe, and Oracle) and
searched for the web pages containing information about
the releases and security updates for the products of interest.
As vendors’ repositories are highly unstructured and not
intended for past version indexing we manually collected
the date of release and version. In the case of old versions or
EOL products, we relied on the Internet Archive4 or external
resources like JPCERT5 and Wikipedia.

4 NEO4J DATABASE

We present the structure of the Neo4j database describing
the nodes, their properties, and their relationships.

We extended our dataset with the malware, tools, and
techniques associated with each APT using the pyattck li-
brary [3]. We then associated a list of aliases to each APT
using the MISP ThreatActor galaxy [4].

4.1 Node Labels

APT

This node label follows the lines of the STIX Threat Actor
type. The information has been manually extracted from
MITRE Att&ck. The node contains the following data:

• labels: describes the type of APT. For example, possible
values are activist, criminal, crime-syndicate, nation-
state;

• name: is the name of the threat group as in MITRE

1. https://oasis-open.github.io/cti-documentation/resources#stix-20-speciﬁcation.
2. At the time of writing a newer version (v2.0) is available at the

Att&ck;

ThaiCERT website.

3. With at least a month-base granularity.

4. https://archive.org/
5. https://www.jpcert.or.jp/english/about/

 
 
 
 
 
 
TABLE 1: Nodes Terminology

APT
Vulnerability
Campaign
Technique
Product
Version
Country

A sophisticated group involved in malicious cyber activities.
A software ﬂaw that can result in a security breach or a violation of the system’s security policies.
Time-bounded set of activity, carried out by an APT, that uses particular techniques against a set of targets.
A method employed to achieve a speciﬁc goal like initial access, privilege escalation, etc.
A software product that is vulnerable to at least one vulnerability exploited by an APT.
A speciﬁc release of a software product that is vulnerable to at least one vulnerability exploited by an APT.
Allegedly country of origin of an APT.

TABLE 2: Measures Terminology

Reserved time tVr
Published time tVp
Exploited time tVe
Update Release time tUr
Observed time to
Updated time tpatched
Exploit Age

Time when a CVE entry for a vulnerability is reserved by MITRE.
Time when the CVE for the vulnerability is published in NVD.
Time when a campaign is being observed to start.
Time when the version of a software product is released.
Time when a product with a given version is observed to be vulnerable to a vulnerability.
Time when a software product is updated to a non vulnerable version.
Interval of time between the publication of a CVE and the ﬁrst observation of the vulnerability in a campaign.

TABLE 3: Common terms

0-day vulnerability

Unknown vulnerabil-
ity
Unknown attack vector

CVE

A vulnerability exploited w/o an update
available.
A vulnerability exploited when it was not
publicly known yet.
The attack vector was either not speciﬁed
in the report or was not known.
An unique identiﬁer associated to a vul-
nerability by MITRE.

• description: is a short description of the group;
• goals: are the goals of the APT. For example, ﬁnancial

gain, espionage, and sabotage;

Country

This node label contains the following data:

• name: is the name of the Country. It represents the
Country in which an APT is allegedly to have ori-
gin/location according to the resources consulted. We
did not make any assumption on the country origin of
an APT because attribution is a well-known problem in
the threat intelligence ﬁeld. [5]

Alias

This node label contains the following data:

• name: is another name with which the APT is called.
For example, APT 186, is called Dynamite Panda by
Crowdstrike, Scandium by Microsoft, Wekby by Palo
Alto, etc.;

Identity

This node label represents a sector that is targeted by a
certain APT. The following parameters are used:

• name: is the name of the sector. For example energy,

defense, telecommunications, etc.;

Vulnerability

This node label describes a vulnerability exploited by a
certain APT. The following properties are present:

• name: is the CVE associated with the vulnerability. For

example CVE-2016-4113;

6. It is the name used by MITRE Att&ck.

• baseScore: is the CVSS Severity and Metrics Base Score

of the CVE (v2 or v3);

• reservedDate: is the date when the CVE has been re-

served by MITRE (in the format MM-YYYY);

• publishedDate: is the date when the CVE has been
published in NVD (in the format MM-YYYY). The date
when NIST publishes a CVE could differ from the date
when the CVE is reserved by MITRE [6];

Campaign
This node label describes a campaign carried by a certain
APT. It contains:

• date start: is the date when the campaign was ﬁrst

observed;

We ignore the date of the end of the campaign due to the
fact that this information is not reliable, if not absent at all,
in the reports analyzed.

Malware
This node label describes the malware used by certain APTs.
It contains:

• name: is the name associated with the malware;
• platform: is the list of platforms that are vulnerable to
the malicious software. For example Windows, Linux,
macOS;

Tool
This node label describes a legitimate tool available that is
exploited by a threat actor during their campaigns. Each
node contains:

• name: is the name of the tool. For example, Winexe;

Technique
This node label describes a technique used by an APT to
compromise the target. It contains:

• name: is the name of the technique as deﬁne in MITRE

Att&ck. For example, File and Directory Discovery;

• tactic: is the goal for which this technique is used. For

example, initial access, command-and-control, etc.;

• platforms: is the platform affected. For example, Linux,

Windows, macOS;

• permissions: are the permissions required to implement
this technique. For example, user, administrator, etc.;

2

Product

7 CAMPAIGNS ATTACK VECTORS AND CVES

certain APT;

• APT
APT;
• APT

This node label describes a software product that is vulner-
able to at least a CVE exploited by an APT. It contains the
following property:

• name: is the name of the product. For example, Internet

Explorer;

Version

This node label describes the version related to a speciﬁc
software product for which a CVE has been published. It
contains the following properties:

• name: is the version of the product;
• product: is the name of the product;
• update: is the name of the update, if any. For example

sp1 for Windows XP;

• os: operating system(s) where the product can run;

4.2 The Relationships

The graph database presents different relationships that link
nodes:

• APT

uses−−−→Malware: deﬁnes which malware is used by a

uses−−−→Tool: deﬁnes which tool is used by a certain

uses−−−→Technique: deﬁnes which technique is used by

a certain APT;

• APT

origin−−−−→Country: deﬁnes the allegedly country of

origin of an APT;
attributed to
←−−−−−−−−Campaign: describes a campaign al-

• APT

legedly to be attributed to a certain APT;

• Campaign

targets
−−−−−→Vulnerability: describes which CVE is

exploited in a speciﬁc campaign;

• APT

targets
−−−−−→Identity: deﬁnes which sector is targeted by

a certain APT;

• APT

alias←−−−Alias: deﬁnes a relation between the names

associated by different companies to a certain APT;

• Product

has−−→Version: deﬁnes which version are associ-

ated to a product;

• Version

vulnerable to
−−−−−−−−−→Vulnerability: deﬁnes which CVE

• Campaign

affect a version of a product.
employs
−−−−−→Technique: deﬁnes which Technique
is used to perform the initial access. This is based on the
MITRE Att&ck Enterprise Initial Access section.

5 STATE-OF-THE-ART RESEARCH QUESTIONS

Tab. 4 shows the research questions of the SoA and the
related results concerning APTs and TI in the different
categories.

Tab. 6 and Tab. 7 shows the matrix by pair of attack vectors
for the campaigns with and without vulnerabilities. We did
not observe more than two attack vectors exploited in a sin-
gle campaign in our dataset. The numbers on the diagonal
line identify the number of campaigns that exploited only
a speciﬁc attack vector, while on the remaining entries we
have the number of campaigns that employed two different
vectors. The matrices are symmetrical with respect to the
diagonal line. The number of unique campaigns can be
obtained from the sum of the elements on the diagonal line
with either the elements above or below the diagonal.

Tab. 8 shows the 12 CVEs observed in the highest num-
ber of campaigns. These results are conﬁrmed by the CISA
and FBI report [32].

8 MATRIX OF STRATEGIES AND CAMPAIGNS

Tables 9 and 10 present a fragment of the matrix for the
Immediate strategy and the timeline of campaigns of Acro-
bat Reader. The algorithm Alg. 1 shows the pseudo-code
utilized to compute the conditional probability of being
compromised in an instant of time ti. For each of the update
strategy matrices (e.g. Tab. 9), we create a matrix for each
campaign and perform an element-wise multiplication be-
tween them. The result is a matrix with the same dimension
that has 1 in the entry in which the update strategy has
a product version installed when the campaign exploited
it. We then sum all the rows and normalize to 1 (because
a campaign can target multiple products). The result is a
vector in which each entry is 1 or 0 depending if at a speciﬁc
ti the campaign had success or not.

Algorithm 1: Computation of Conditional Probabil-
ity

Data: matrix updates,list campaigns
Result: A vector containing the number of potentially

successfull campaigns for all products for each instant
of time ti, the number of active campaigns for all
products for each instant of time ti

potentially successful campaigns = zeroes(2008:2020); /*
|CPz ,Verx |ti */;
active campaigns = zeros(2008:2020) /* |CPz |ti */;
for campaign in list campaigns do

matrix campaign = create matrix(campaign);
active campaigns +=

normalize(sum rows(matrix campaigns));
result = matrix updates.*matrix campaign;
tmp successful campaigns = normalize(sum rows(result));
potentially successful campaigns +=

tmp successful campaigns;

end

6 APT LIST

Tab. 5 reports the list of APTs classiﬁed by their major goal.
The APTs in bold target at least one of the products of
interest (Ofﬁce, Flash Player, Reader, Air, JRE) and thus are
considered in the evaluation.

We thank the student Veronica Chierzi for helping in the
data collection of update releases. This work was partly
funded by the European Union under the H2020 Pro-
gramme under grant n. 830929 (CyberSec4Europe) and
n.952647 (AssureMOSS).

ACKNOWLEDGMENT

3

TABLE 4: Research Questions and Answers from the State of the Art about APTs over the years

Paper Year Category
[7]

2012 Detection of
attacks

Research Questions
1)How can we model APT attacks?

[8]

[9]

[10]

[11]

[12]

[13]

2014 Detection of
attacks

2014 Analysis
attackers
characteristics

of

2014 Detection of
attacks
2015 Detection of
attacks
2015 Game Theory

2015 Detection of
attacks

[14]

2016 Analysis

of

exploitation
likelihood

2016 Detection of
attacks

2016 Detection of
attacks

of

2016 Analysis
attackers
characteristics
2016 Detection of
attacks

[15]

[16]

[17]

[18]

[19]

2)How can we detect APTs based on this
model?
1)How can we model APT attacks?

1)What are the characteristics of APT at-
tacks?

characterize

2)What are the phases of an APT attack?
1)How can we associate logs to phases of
an APT attack?
1)How can we detect APT malware on a
system?
1)How can we
de-
fense/attacker strategies with the presence
of intruders?
1)How can we detect phases of APT at-
tacks from logs?
1)How to determine the hosts with the
highest risk of being targeted by APT at-
tacks?
1)How can we identify suspicious hosts
employed to exﬁltrate data based on their
network trafﬁc?
1)How can we detect phases of APT at-
tacks from IDS data?
2)How can we link different alerts to a
single attack?
1)What are the characteristics of APT at-
tacks?

1)How can we detect phases of APT at-
tacks from logs?

2018 Detection of
attacks

1)How can we detect in real-time APT
attacks based on IDS data?

[20]

2018 APTs

data

sources

[21]

2018 Analysis

of

exploitation
likelihood,
Game Theory
2018 Detection of
attacks, Game
Theory
2018 Detection of
Attacks

2019 Detection of
attacks
2019 Detection of
attacks

[22]

[23]

[24]

[25]

[26]

2019 Metrics for TI
sources

[27]

2020 Metrics for TI
sources

[28]

[29]

[30]

[31]

2020 Analysis
attackers
characteristics

of

2020 Detection of
attacks
2020 Detection of
attacks
2021 Detection of
attacks

1)What are the open-source resources
about different APTs?
1)Does exist a Nash equilibrium in the allo-
cation of response resources against APTs
to limit the loss?

1)How can we model Dynamic Informa-
tion Flow Tracking to determine optimal
strategies for the defender?
1)How can we efﬁciently perform threat
hunting from data logs and alerts?

1)How can we detect in real-time APT
campaigns from low-level event traces?
1)How can we detect phases of attacks
from IPS?
2)How can we detect changes in strategies
by the attackers?
1)What are the metrics to evaluate TI data
feeds?
2)What are the major limitations of TI
sources?
1) What does paid TI consist of?

2) How paid TI compares to open TI
sources?
3) How do customers use TI?

1)What are the most used tactics employed
by APTs?

2)Are companies leaking information ex-
ploitable for the campaigns?
1)How to detect APT attacks at run-time?

1)How to detect APT attacks at run-time?

1)How to reconstruct APT steps from logs?

4

Answers
An attack pyramid model with the goal on the top and the planes
representing the environments. Events are placed in the pyramid.
Correlated events describe an attack. Detection is based on signatures,
proﬁling, and policy rules.
An extended Petri net is developed to describe attack goals, the net-
work, and the attacker’s actions.
APTs attack speciﬁc targets, are well-resourced, and stealthy.

Recon, initial intrusion, C&C, lateral movement, and exﬁltration.
A framework with an embedded intrusion kill chain receives logs from
sensors and correlated events to determine phases of the attack
Using random forest to classify malware based on features like CPU
and memory usage.
A two-layer differential game between defender and attacker and
multiple insiders. Nash equilibrium exists for both layers.

A rule-based model that correlates events from host security logs to
extract anomalies from normal behaviors.
A framework that analyzes both internal (network logs) and external
(blacklist, social media) data to determine uncommon behaviors and
exposure indicators.
A framework extracts features from network ﬂow records to determine
anomalous variations from normal behaviors.

A framework determine phases of attacks based on the information
ﬂow between events detected by IDS.
A tag is associated to an event from the IDS and its propagation
underlines the complete chain of the attack.
APTs employ different phases (initial comprom., lateral mov., C&C, and
exﬁlt.). They rely on spearphishing and known vulnerabilities.

A weighted graph from system logs determines dense connections
among events triggered by the attackers and sparse connections with
benign events.
A ML system composed of three modules in sequence: generation of
alerts, clustering of alerts, and prediction of APT attacks based on the
correlated alerts obtained in the previous module.
The majority of the resources come from industry reports. A list of
resources for different APTs is provided.
Assuming attack and response strategy constant in time, a greedy
algorithm is developed to determine the Nash equilibrium against
lateral movements.

A game-theoretic model of DIFT and adversarial information ﬂow is
formulated to determine the equilibrium of both defender and attacker.

Monitored events (process, ﬁles, network sockets, etc.) are described in
a temporal computational graph. A graph database is generated and
queried to extract attacks.
Information ﬂows between ﬁle, process, etc. in a provenance graph are
correlated and mapped to a corresponding TTPs description.
IPS alerts are converted in short sentences and their context is analyzed
using word embedding technique.
Cosine similarity is used to quantify embedding changes over time.

Volume, differential contribution, exclusive contribution, latency, cov-
erage, and accuracy.
Coverage (different feeds have few overlaps) and false positive indica-
tors.
Indicators, reports, requests for information, portal for previous data.

There is almost no overlap between paid TI and open TI and also
between different paid TI.
Network detection, situational awareness, SOC prioritization, and in-
forming busines decisions.
80% of the attacks employ spearphising.

90% of the companies leak data exploitable for phishing campaigns.

Extended EDR to include provenance graph to reduce false alarms and
log retention
A provenance graph and clustering is used to detect anomalies.

Causal graph extracted from logs are analyzed via NLP and ML
techniques.

TABLE 5: List of APTs in the dataset grouped by goal. APTs in bold are considered in the evaluation

Goal
espionage

APTs
admin@338, APT12, APT16, APT17, APT18, APT19, APT28, APT29, APT3, APT30, APT32, APT33, APT37, APT39, BlackOasis,
BRONZE BUTLER, Charming Kitten, CopyKittens, Dark Caracal, Darkhotel, DarkHydrus, Deep Panda, DragonOK, Equation,
Gallmaker, Gamaredon Group, Gorgon Group, Group5, Ke3chang, Lazarus Group, Leafminer, Leviathan, Lotus Blossom,
Magic Hound, menuPass, Molerats, MuddyWater, Naikon, NEODYMIUM, OilRig, Patchwork, PLATINUM, Poseidon Group,
PROMETHIUM, Putter Panda, Rancor, Scarlet Mimic, Sowbug, Stealth Falcon, Stolen Pencil, Strider, Suckﬂy, TA459, Threat
Group-3390, Thrip, Tropic Trooper, Turla, Winnti Group, APT1, APT41, Dust Storm, Elderwood, Honeybee, Kimsuky, Night
Dragon, PittyTiger, Taidoor, The White Company, WIRTE.
APT33, Dragonﬂy, Dragonﬂy 2.0, Equation, Lazarus Group, Sandworm Team, TEMP.Veles.

sabotage
ﬁnancial gain APT38, Carbanak, Cobalt Group, Deep Panda, FIN10, FIN5, FIN6, FIN7, FIN8, GCMAN, Gorgon Group, Lazarus Group, RTM,

SilverTerrier, FIN4, Silence.

TABLE 6: Attack vectors campaigns w/ software vulns

TABLE 9: Fragment of Immediate strategy matrix

g
n
i
h
s
i
h
p
r
a
e
p
S
111
11

e
s
i

m
o
r
p
m
o
C
y
b
-
e
v
i
r
D
11
22

1

n
i
a
h
C
y
l
p
p
u
S

e
s
i

m
o
r
p
m
o
C

e
t
o
m
e
R

l
a
n
r
e
t
x
E

s
e
c
i
v
r
e
S

s
t
n
u
o
c
c
A
d

i
l
a
V

1

g
n
i
c
a
F
-
c
i
l
b
u
P
t
i
o
l
p
x
E

n
o
i
t
a
c
i
l

p
p
A

7

h
g
u
o
r
h
T
n
o
i
t
a
c
i
l

p
e
R

a
i
d
e
M
e
l
b
a
v
o
m
e
R

d
e
n
i
m
r
e
t
e
d
n
U

1

1

8

Spear phishing
Drive-by Compromise
Supply Chain Compromise
Valid Accounts
External Remote Services
Exploit Public-Facing Application
Replication Through Removable Media
Undetermined

TABLE 7: Attack vectors campaigns w/o software vulns

g
n
i
h
s
i
h
p
r
a
e
p
S
125
3
1

1

e
s
i

m
o
r
p
m
o
C
y
b
-
e
v
i
r
D
3
11

1

n
i
a
h
C
y
l
p
p
u
S

e
s
i

m
o
r
p
m
o
C
1

3

s
t
n
u
o
c
c
A
d

i
l
a
V

1
3

e
t
o
m
e
R

l
a
n
r
e
t
x
E

s
e
c
i
v
r
e
S

3

g
n
i
c
a
F
-
c
i
l
b
u
P

t
i
o
l
p
x
E

n
o
i
t
a
c
i
l

p
p
A
1

2

h
g
u
o
r
h
T
n
o
i
t
a
c
i
l

p
e
R

a
i
d
e
M
e
l
b
a
v
o
m
e
R

d
e
n
i
m
r
e
t
e
d
n
U

1

0

37

Spear phishing
Drive-by Compromise
Supply Chain Compromise
Valid Accounts
External Remote Services
Exploit Public-Facing Application
Replication Through Removable Media
Undetermined

The boxed version might be zero or one depending on whether we
consider the update ﬁrst or APT ﬁrst criteria for updates happening
within the month.
05/12

10/12

06/12

07/12

08/12

09/12

11/12

12/12

reader-9.3.3
reader-9.3.4
reader-9.4
reader-10
reader-10.0.1
reader-10.0.2
reader-10.0.3
reader-10.1
reader-10.1.1
reader-10.1.2
reader-10.1.3

reader-10.1.4
reader-11

1

1

1

0/1

1

1

0/1
1

1

1

TABLE 10: Fragment of the campaign matrix timeline

05/2012
6
6
5
5
5
4

06/2012
7
7
6
6
6
5

07/2012
7
7
6
6
6
5

08/2012
7
7
6
6
6
5

09/2012
7
7
6
6
6
5

10/2012
7
7
6
6
6
5

11/2012
7
7
6
6
6
5

12/2012
7
7
6
6
6
5

reader-9.3.3
reader-9.3.4
reader-9.4
reader-10
reader-10.0.1
reader-10.0.2
reader-10.0.3
reader-10.1
reader-10.1.1
reader-10.1.2
reader-10.1.3
reader-10.1.4
reader-11

TABLE 8: Top CVE with highest number of campaigns.

[4] MISP,

https://github.com/MISP/misp-

Campaigns across different CVEs can overlap. For example, APT30 em-
ployed both CVE-2010-3333 and CVE-2012-0158 in a single campaign.

[5]

Affected Product(s)

# Campaigns

Ofﬁce, Visual Basic, SQL server,...
Ofﬁce
Ofﬁce, Windows Server
Ofﬁce
Ofﬁce, Sharepoint, Word
Flash player
Ofﬁce
Flash player
Ofﬁce, Sharepoint, Word
.NET
Flash player, Air, Reader,...
Windows Vista, 7, Server 2008,...

18
13
11
8
7
7
6
6
5
5
5
5

CVE

CVE-2012-0158
CVE-2017-11882
CVE-2017-0199
CVE-2010-3333
CVE-2015-1641
CVE-2016-4117
CVE-2015-2545
CVE-2015-5119
CVE-2014-1761
CVE-2017-8759
CVE-2011-0611
CVE-2014-4114

REFERENCES

galaxy/blob/master/clusters/. Accessed: 2020-09-01.
J. A. Guerrero-Saade and C. Raiu, “Walking in your enemy’s
shadow: When fourth-party collection becomes attribution hell,”
in Proc. of Virus Bulletin-17, 2017.

[6] H. Chen et al., “Using twitter to predict when vulnerabilities will

be exploited,” in Proc. of SIGKDD-19, 2019.

[7] P. Giura and W. Wang, “A context-based detection framework for

advanced persistent threats,” in Proc. of ICCS-12, 2012.

[8] W. Zhao et al., “Extended petri net-based advanced persistent
threat analysis model,” in Computer Engineering and Networking,
2014.

[9] P. Chen et al., “A study on advanced persistent threats,” in Proc. of

IFIP-14, 2014.

[10] P. Bhatt et al., “Towards a framework to detect multi-stage ad-
vanced persistent threats attacks,” in Proc. of SOSE-14, 2014.
[11] S. Chandran et al., “An efﬁcient classiﬁcation model for detecting

advanced persistent threat,” in Proc. of ICACCI-15, 2015.

[12] P. Hu et al., “Dynamic defense strategy against advanced persis-

tent threat with insiders,” in Proc. of INFOCOM-15, 2015.

[13] I. Friedberg et al., “Combating advanced persistent threats: From
network event correlation to incident detection,” Computers &
Security, 2015.

[14] M. Marchetti et al., “Countering advanced persistent

threats
through security intelligence and big data analytics,” in Proc. of
CyCon-16, 2016.

[15] ——, “Analysis of high volumes of network trafﬁc for advanced

[1] MITRE, https://attack.mitre.org/groups/. Accessed: 2020-08-01.
[2] ThaiCERT, 2019, https://www.thaicert.or.th/downloads/ﬁles/A Threat Actor Encyclopedia.pdf.

persistent threat detection,” Computer Networks, 2016.

Accessed: 2020-06-01.

[3] Pyattack, https://pypi.org/project/pyattck/. Accessed: 2020-09-

01.

[16] G. Brogi and V. V. T. Tong, “Terminaptor: Highlighting advanced
persistent threats through information ﬂow tracking,” in Proc. of
NTMS-16, 2016.

5

[17] M. Ussath et al., “Advanced persistent threats: Behind the scenes,”

in Proc. of CISS-16, 2016.

[18] K. Pei et al., “HERCULE: attack story reconstruction via commu-
nity discovery on correlated log graph,” in Proc. of ACSAC-16,
2016.

[19] I. Ghaﬁr et al., “Detection of advanced persistent threat using
machine-learning correlation analysis,” Future Generation Comp.
Syst., 2018.

[20] A. Lemay et al., “Survey of publicly available reports on advanced

persistent threat actors,” Computers & Security, 2018.

[21] L. Yang et al., “A risk management approach to defending against
the advanced persistent threat,” IEEE Transactions on Dependable
and Secure Computing, 2018.

[22] D. Sahabandu et al., “DIFT games: Dynamic information ﬂow
tracking games for advanced persistent threats,” in Proc. of IEEE
CDC-18, 2018.

[23] X. Shu et al., “Threat intelligence computing,” in Proc. of ACM-

CCS-18, 2018.

[24] S. M. Milajerdi et al., “HOLMES: real-time APT detection through
correlation of suspicious information ﬂows,” in Proc. of SSP-19,
2019.

[25] Y. Shen and G. Stringhini, “ATTACK2VEC: leveraging temporal
word embeddings to understand the evolution of cyberattacks,”
in Proc. of USENIX-19, 2019.

[26] V. G. Li et al., “Reading the tea leaves: A comparative analysis of

threat intelligence,” in Proc. of USENIX-19, 2019.

[27] X. Bouwman et al., “A different cup of ti? the added value of
commercial threat intelligence,” in Proc. of USENIX-20, 2020.
[28] T. Urban et al., “Plenty of phish in the sea: Analyzing potential

pre-attack surfaces,” in Proc. of ESORICS-20, 2020.

[29] W. U. Hassan et al., “Tactical provenance analysis for endpoint

detection and response systems,” in Proc. of SSP-20, 2020.

[30] X. Han et al., “Unicorn: Runtime provenance-based detector for

advanced persistent threats,” in Proc. of NDSS-20, 2020.

[31] A. Alsaheel et al., “Atlas: A sequence-based learning approach for

attack investigation,” in Proc. of USENIX-21, 2021.

[32] CISA, “Top 10 routinely exploited vulnerabilities,”

2020,
https://www.us-cert.gov/ncas/alerts/aa20-133a. Accessed: 2020-
09-01.

6

