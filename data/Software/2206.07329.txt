2
2
0
2

n
u
J

5
1

]

R
C
.
s
c
[

1
v
9
2
3
7
0
.
6
0
2
2
:
v
i
X
r
a

Application-Oriented Selection of Privacy
Enhancing Technologies⋆

Immanuel Kunz and Andreas Binder

Fraunhofer AISEC
{firstname.lastname}@aisec.fraunhofer.de

Abstract. To create privacy-friendly software designs, architects need
comprehensive knowledge of existing privacy-enhancing technologies (PETs)
and their properties. Existing works that systemize PETs, however, are
outdated or focus on comparison criteria rather than providing guidance
for their practical selection. In this short paper we present an enhanced
classiﬁcation of PETs that is more application-oriented than previous
proposals. It integrates existing criteria like the privacy protection goal,
and also considers practical criteria like the functional context, a technol-
ogy’s maturity, and its impact on various non-functional requirements.
We expect that our classiﬁcation simplieﬁes the selection of PETs for
experts and non-experts.

Keywords: Privacy Engineering · Privacy Enhancing Technologies ·
Privacy By Design.

1 Introduction

A decisive activity in privacy engineering is the selection of appropriate Privacy
Enhancing Technologies (PETs), for example to fulﬁll requirements or mitigate
risks in goal-based or risk-based engineering methods respectively. While this
step is highly application-speciﬁc, it can be approached systematically, since
common decision criteria for PET-selection exist. For example, one criterion
that can guide engineers in their design decisions is the privacy goal that is
targeted by a PET, such as anonymity or undetectability.

Existing works have proposed diﬀerent systematizations of PETs in the past.
The LINDDUN methodology [11], for instance, categorizes PETs using their pri-
vacy protection goal, and diﬀerentiates between PETs that target transactional
and contextual data [44]. Heurix et al. [22] categorize PETs, e.g., regarding the
trust scenario they target and their involvement of a trusted third party. Yet,
these systematizations do not suﬃciently take into account the practical context
in which PETs are selected: they often omit the PET’s functional context it can
be applied in, as well as other practical criteria. Also, they are partly outdated.

In this short paper, we develop a new PET classiﬁcation that is more application-

oriented. Our classiﬁcation builds upon previous proposals, integrating some of

⋆

The
available
publication
(https://doi.org/10.1007/978-3-031-07315-1_5)

ﬁnal

is

at

link.springer.com

 
 
 
 
 
 
2

I. Kunz and A. Binder

their criteria [1, 11, 22] like the technology’s targeted privacy protection goal
and its impact on other non-functional requirements. It furthermore includes
the PET’s functional context, as well as prioritization criteria, like the maturity
of the technology. We present a classiﬁcation of 30 PETs that we have done
according to the proposed criteria.

To demonstrate the eﬀectiveness of our classiﬁcation, we compare it to the
one proposed by LINDDUN based on a use case. We expect that our work can
support engineers in selecting appropriate PETs, and motivate PET develop-
ers to evaluate their technologies according to our criteria, making them more
comparable and usable for architects and engineers.

2 Classiﬁcation

Various approaches to selecting appropriate PETs have been proposed in the
past, but the adequate application of PETs can be even more challenging than
their selection, for example due to the development eﬀort involved in applying a
PET in a certain environment. Our goal is therefore to develop an application-
centered classiﬁcation of PETs that anticipates the challenges that a PET can
cause when it is applied. In the following, we describe and motivate the criteria
we have included in our classiﬁcation.

2.1 Motivating Example

To motivate the choice of criteria, we lay out the following scenario of an engineer
developing an architecture for a generic privacy-friendly cloud service, assum-
ing that it is representative for the decisions architects and developers have to
make when implementing privacy requirements. This generic service allows users
to register, authenticate, provide personal data to the service (e.g. names, ad-
dresses), interact with other users of the service (e.g. sending messages), and to
retrieve data that is stored in the service’s storages.

An engineer creates a threat model for an initial design of the service which
reveals a detectability threat for the service’s users: since their messages to other
users are observable by the service and possibly external attackers, their rela-
tionships can be identiﬁed. A requirement is therefore elicited which states that
users’ messages should be undetectable. A PET that mitigates this threat has
to match several criteria:

First, it has to ﬁt the functional context of the service. This includes the
targeted privacy protection goal, such as anonymity or undetectability. To ensure
suitability for the scenario, however, the targeted privacy protection goal is not
suﬃcient; also, the PET has to ﬁt the functional requirements of the application.
In a messaging scenario, for example, diﬀerent PETs are applicable than in an
information retrieval or computing scenario. Ideally, the PET should furthermore
not only target the correct privacy goal, but should also be measurable in the
achieved privacy gain. This way, a more comparable and reproducible selection
is facilitated.

Application-Oriented Selection of Privacy Enhancing Technologies

3

Second, various non-functional properties can play a role. For example, the
PET should be mature enough to be applied and maintained by the development
team: applying a certain technology can otherwise imply large eﬀorts for conﬁg-
uration and further development of the technology. To be able to weigh diﬀerent
usable PETs against each other, also further properties are of interest: PETs can
have an impact on the overall architecture, on the utility of transactional data,
and the performance of the respective interaction.

In the following we describe the criteria we use to cover these considerations,

and classify a list of PETs accordingly.

2.2 Criteria

Privacy Protection Goal One of the most meaningful criteria for selection is
the targeted privacy protection goal. To that end we use the goals proposed by
the LINDDUN methodology [11], most of which have originally been proposed
by Pﬁtzmann and Hansen [32]: Unlinkability, anonymity, plausible deniability,
undetectability, conﬁdentiality, awareness, and policy compliance.

Every PET targets one or more of these goals. Often, it is the case that several
goals are targeted by one PET, since they partly overlap or imply each other.
For example, it is hardly possible to achieve undetectability without anonymity.
In our classiﬁcation, however, we usually only present one privacy protection
goal which we consider to be primarily targeted.

Privacy Metric A technology’s suitability and eﬀectiveness need to be mea-
surable to evaluate its added privacy gain and monitor the system over time.
Note that the broader problem of creating a metric suite that comprehensively
covers the notion of privacy is a research problem out of scope for this paper
(see for example Wagner and Yevseyeva [43]). A number of privacy metrics are
reviewed in [42].

Functional Scenario The functional context the PET shall be applied in is
highly application-speciﬁc. Still, some categories of functional scenarios can be
identiﬁed and can be used as a selection ﬁlter. We identify the following generic
interactions:Computation, Messaging, Retrieval, Release, Authentication, as well
as Authorization. We use the motivating scenario above to clarify these values:
a computation means that data is processed, e.g. a virtual machine processes
user data to create recommendations for users; Release is the release of data to
another party, e.g. a user sends location data to the social network; Messaging, is
a point-to-point interaction between two users via n other parties, e.g. two users
of the same social network communicate with each other via the service; Authen-
tication is the process of determining the identity of a user, while Authorization
is the process of determining the rights of an identiﬁed user.

Maturity When selecting a PET, cost factors play an important role, e.g. in the
set-up of the PET and in its continuous maintenance. In this paper we use the

4

I. Kunz and A. Binder

technology’s maturity as an indicator for set-up and maintenance costs, since a
technology that is less mature will likely have more defects and will likely imply
a more laborious set-up.

For this criterion we loosely base the possible values on the Technology Readi-
ness Level (TRL) which describes a technology’s maturity on a scale from 1 to
9 [31]. We generalize this scale to 3 levels as follows: level 1 is a level often
achieved in scientiﬁc work, which describes a concept and may already prove
feasibility in a proof of concept; level 2 can be seen as the development and
testing stage, i.e. adopting such a technology still would require considerable de-
velopment eﬀort if it is applied to a speciﬁc use case; ﬁnally, level 3 means that
the technology is readily available and ﬁeld-tested, but may still require some
set-up cost for the adaption.

Performance Impact The performance of processes and interactions can be
impacted by the use of PETs. Evaluating the performance of a certain PET,
however, is not trivial, especially in comparison to other technologies. We there-
fore evaluate a PET’s performance in a simpliﬁed manner as follows. We ﬁrst
generically describe the performance requirements in a certain functional sce-
nario, and then assess if the use of a PET is expected to signiﬁcantly impact the
performance requirements or not.

We consider Computation and Retrieval scenarios generally to have high
performance requirements: In these scenarios the user waits for the result of
the interaction and will probably notice also small delays. In contrast, Authen-
tication, Authorization, and Release scenarios generally have low performance
requirements since they are usually one-time actions for which performance im-
pacts are more acceptable. Also, we consider Messaging to be a scenario of
asynchronous communication where small increases in latency are not noticed
by the users.

Architectural Impact An impact on the architecture is given if the PET re-
quires a dedicated architectural component or modiﬁcations to the architecture,
e.g. setting up a mix net requires a separate mix server. This is an important
selection criterion, since the selection of a PET with this property needs to be
considered early on in the design process.

Utility A utility impact is given if a PET reduces the quality of transactional
data, e.g. by distorting or ﬁltering it—and thus decreasing the data’s utility.

2.3 Classifying PETs

Table 1 shows our classiﬁcation proposal according to the criteria deﬁned above.
Note that our classiﬁcation only includes the so-called hard privacy goals [11], i.e.
Anonymity, Unlinkability, Plausible Deniability, and Undetectability. The soft

Application-Oriented Selection of Privacy Enhancing Technologies

5

Table 1. Classiﬁcation of Privacy Enhancing Technologies. A black square indicates
that the PET addresses the respective goal, while a triangle indicates a negative impact
on the respective criterion.

n
o
i
t
a
i
d
u
p
e
R
-
n
o
N

y
t
i
l
i
b
a
ﬁ
i
t
n
e
d
I

e
c
n
a
i
l
p
m
o
C
-
n
o
N

y
t
i
l
i
b
a
t
c
e
t
e
D

e
r
u
s
o
l
c
s
i
D

s
s
e
n
e
r
a
w
a
n
U

y
t
i
l
i
b
a
k
n
i
L

(cid:4)

(cid:4) (cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4) (cid:4)

(cid:4)

e
m
a
N

k-anonymity, l-diversity,
t-closeness
Suppression
Recoding
Aggregation
Swapping
Noise masking
PRAM
Synthetic data
Mix Network

Group Signatures

(cid:4) (cid:4) (cid:4)

s
c
i
r
t
e

M

l
a
n
o
i
t
c
n
u
F

o
i
r
a
n
e
c
S

Data similarity

Release

y
t
i
r
u
t
a
M

3 [33]

3 [33]
3 [33]
3 [33]
1 [23]
3 [30]
2 [40]

e
c
n
a
m
r
o
f
r
e
P

e
r
u
t
c
e
t
i
h
c
r
A

y
t
i
l
i
t

U

◭

◭
◭
◭ ◭

◭
◭
◭

3 [38] ◭ ◭

Release
Release
Release
Release
Release
Release
Release
Messaging

Data similarity
Data similarity
Data similarity
Data similarity
Data similarity
Data similarity
Data similarity

Cryptographic
Games
Cryptographic
Games
Cryptographic
Games

Entropy

Cryptographic
Games
Cryptographic
Games
Cryptographic
Games
Cryptographic
Games
Cryptographic
Games
Cryptographic
Games
Cryptographic
Games
Cryptographic
Games
Cryptographic
Games
Data similarity
Entropy
Cryptographic
Games

Release

2 [24]

◭

AuthN

2 [8]

AuthN, AuthZ 2 [5] ◭

AuthN,
Release

3 [14]

Messaging

3 [17]

Messaging

3 [17]

Retrieval

3 [6]

Retrieval

2 [28] ◭ ◭

Retrieval

2 [7] ◭ ◭

Messaging

2 [16] ◭ ◭

Computation

2 [18] ◭

Computation 3 [3, 25]

Messaging,
Release
Messaging
Messaging

3

2 [9]

Computation

3 [4] ◭

(cid:4)

Indistinguishability

Release

2 [41]

Indistinguishability

Release

2 [41]

(cid:4)

(cid:4)

Cryptographic
Games
Attacker Success
Probability

AuthN, AuthZ 2 [39]

Release

2 [37]

◭

◭

◭

Anonymous Credentials (cid:4) (cid:4)

Zero Knowledge Proofs (cid:4) (cid:4)

Pseudonymization

(cid:4)

Deniable Authentication

Deniable encryption

Searchable Encryption

Private Information
Retrieval

Oblivious Transfer

Proxy Re-Encryption

Homomorphic
Encryption
Trusted Execution
Environment
(A)Symmetric
Encryption
Dummy traﬃc
Steganography

MPC

Local Diﬀerential
Privacy
Global Diﬀerential
Privacy

(cid:4)

Attribute-based encr.

Federated Learning

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)
(cid:4) (cid:4)

(cid:4)

6

I. Kunz and A. Binder

privacy goals Awareness and Policy Compliance1 are usually targeted by more
generic design patterns (see e.g. [1]). There can, however, be overlaps between
technologies and patterns: For instance, onion routing can be seen both as a
design pattern and a PET.

Note that regarding the targeted privacy protection goal, we always assign
the goal that is targeted primarily. For example, Zero Knowledge Proofs (ZKP)
primarily address the threats linkability and identiﬁability. While they could
theoretically also be used to secretly release information, an engineer would not
choose ZKP to achieve conﬁdentiality.

Note also that throughout the paper, we use k-anonymity [36] as a placeholder

also for other related ones like l-diversity [29], t-closeness [27], etc.

3 Use Case and Discussion

3.1 Use Case

In this section, we compare our classiﬁcation to the LINDDUN classiﬁcation
which was ﬁrst proposed by Deng et al. [11] and has since been updated on the
LINDDUN website [44]2. Note that the LINDDUN classiﬁcation also includes
a selection methodology. This methodology is based on the LINDDUN threat
types which are connected to general mitigation strategies. These strategies in
turn are mapped to applicable PETs. For instance, a linkability threat concerning
a data ﬂow may be mapped to the mitigation strategy protect transactional data
which in turn yields the PETs multi-party computation, encryption, and others.
We use again the motivating example introduced earlier, and extend and de-
tail it as follows: our example cloud service is a social network that allows to
add friends, exchange private messages with each other, as well as make public
posts. Furthermore, the service oﬀers a location-based feature where users can
provide their location data and are then oﬀered possible contacts in their prox-
imity they can message. Note that the example used in the original LINDDUN
approach is a social network application as well, making it a well-suited basis
for a comparison.

We use three example threats from diﬀerent LINDDUN categories, i.e. a
linkability, an identiﬁability, and a disclosure threat, to demonstrate the eﬀec-
tiveness of our classiﬁcation in comparison to the LINDDUN classiﬁcation. Note
that these threats have also been identiﬁed (on a more high level) in an example
analysis conducted by the LINDDUN authors for their social network running
example, see [10]. The threats and results from the PET-selection of both ap-
proaches are described in the following. Table 2 summarizes the results.

1 As soft privacy goals, some works also use the goals Intervenability and Trans-

2

parency [21].
Note that we do not compare our approach to Heurix et al. [22], since they partly
use diﬀerent privacy protection goals and generally provide few selection criteria.

Application-Oriented Selection of Privacy Enhancing Technologies

7

Identiﬁability This threat describes the possibility that the server can identify
the user via the transmitted transactional data, e.g. due to identiﬁers like name
or address.

– Applying LINDDUN in this scenario we obtain 7 PETs. With our classi-
ﬁcation we obtain: Suppression, Recoding, Aggregation, Swapping, Noise
Masking, PRAM, Synthetic Data, Mix Network, Group Signatures, Global
Diﬀerential Privacy.

– This is the only case in which our classiﬁcation outputs more PETs than the

LINDDUN methodology.

Linkability This threat concerns potential linkability of diﬀerent types of trans-
actional data which is released by the user to the server. Applying the LINDDUN
method results in a set of 10 PETs as summarized in Table 2.

In comparison we can see that our classiﬁcation correctly omits several PETs
since they are not usable to mitigate a linkability threat, i.e. Multi-Party Compu-
tation, (A)Symmetric Encryption, and Homomorphic Encryption. Furthermore,
it does provide more applicable ones, e.g. Aggregation and Noise Masking which
can obfuscate the relation between data.

Disclosure This threat concerns the disclosure of transmitted data due to inse-
cure connections. Applying the LINDDUN method results in a set of 12 PETs,
see Table 2. Our approach results in 4 PETs: Proxy Re-Encryption, Deniable
Encryption, (A)symmetric Encryption, and Steganography.

– On the basis of this comparison, we expect that our classiﬁcation better
supports engineers in the selection of PETs than existing classiﬁcations and
selection approaches.

– This is achieved mainly by ﬁltering through the functional scenario, but also

the more targeted mapping of PETs to protection goals.

– Consider also that using our classiﬁcation, a user can further prioritize the
results using the maturity, utility, and architecture impact criteria. For ex-
ample, the (a)symmetric encryption may be prioritized in the last example
because it has high maturity and no impact on architecture or utility.

3.2 Discussion

Limitations One limitation of our approach is its coverage: while it is an ex-
tensive collection of 32 PETs, it is not complete and should be extended and
maintained in the future. Especially the evaluation of the maturity criterion may
become outdated soon, e.g. if current research proposals are developed further.
Furthermore, the criteria we propose are deduced from a use case. Therefore,
their eﬀectiveness still has to be validated in real-world studies. Some relevant
criteria, e.g. regarding other non-functional requirements, could also be missing.

8

I. Kunz and A. Binder

Table 2. Comparison of the results of applying the LINDDUN PET Selection method
and our classiﬁcation. Those PETs that are included in both classiﬁcations are written
in bold. For example, for the linkability threat LINDDUN suggests Multi-Party Com-
putation as a possible mitigation. This PET is also included in our classiﬁcation, which,
however, has not suggested it for this threat. Veriﬁable Encryption is also suggested
by LINDDUN, but it has not been considered in our classiﬁcation at all.

Threat

Linkability

LINDDUN Result
k-anonymity
Multi-Party Computation
(A)symmetric encryption
Homomorphic Encryption
Deniable Encryption
Anonymous Buyerseller Watermarking Prot.
Veriﬁable Encryption

k-Anonymity
(A)symmetric Encryption
Homomorphic Encryption
Private Information Retrieval
Oblivious Transfer
Searchable Encryption
Deniable Encryption
Veriﬁable Encryption
Context-Based Access Control
Privacy-Aware Access Control
Privacy-Preserving Data Mining
Private Search
Homomorphic Encryption
k-Anonymity
(A)symmetric Encryption
Private Information Retrieval
Oblivious Transfer
Searchable Encryption
Deniable Encryption
Veriﬁable Encryption
Context-Based Access Control
Privacy-Aware Access Control
Privacy-Preserving Data Mining
Private Search
Multi-Party Computation
(A)symmetric encryption
Homomorphic encryption
Deniable Encryption
Anonymous Buyerseller Watermarking Prot.
Veriﬁable Encryption

Disclosure
(Release)

Disclosure
(Computa-
tion)

Identiﬁability
(Release)

Our Classiﬁcation
k-anonymity and l-diversity
Suppression
Recoding
Aggregation
Swapping
Noise Masking
PRAM
Synthetic Data
Group Signatures
Global Diﬀerential privacy
(A)Symmetric Encryption
Federated Learning

Homomorphic encryption
Multi-Party Computation
Trusted Execution Environment

k-anonymity and l-diversity
Suppression
Pseudonymization
Group Signatures
Global Diﬀerential Privacy

Application-Oriented Selection of Privacy Enhancing Technologies

9

Still, we expect our classiﬁcation to improve the systematic selection of PETs,
and the evaluation of software architectures. For instance, design decisions in
software architectures can be linked to our classiﬁcations and systematically
evaluated.

Our classiﬁcation could be biased since the use case and the threats the
LINDDUN analysis identiﬁes were known to the authors before the classiﬁcation
was ﬁnished. We assume, however, that the bias is low since it was developed
in discussion with multiple domain experts who did not know the LINDDUN
analysis. Also, we would argue that it is evident from that both the general
criteria as well as the classiﬁcation itself are independently applicable from the
social network example.

Criteria Evidently, it is not guaranteed that the criteria we propose are com-
prehensive and that they capture what engineers require as selection criteria in
practice. On the basis of the case example above, however, we expect that it
works better than existing approaches also in other applications.

In comparison to Al-Momani et al. [1], we do not include criteria that indicate
an impact on security and complexity, because we would argue that they are
redundant. Complexity is always increased by a PET to some degree, while
the actual degree of complexity is too diﬃcult to measure. With regards to the
security impact, there is one privacy protection goal that directly contradicts
a security goal, i.e. plausible deniability contradicts non-repudiation. Thus, any
PET that targets plausible deniability also counteracts said security goal which
can therefore directly be derived from our classiﬁcation.

In the following we also compare our criteria with Heurix et al. [22]:

– The Aim dimension is similar to our privacy protection goal.
– The Scenario dimension is not in scope for us, since we focus on client-server

interactions where the server is untrusted.

– The Aspect dimension is similar to the mitigation strategies in the LIND-
DUN method [44]. We do not consider these because they are implied in
the functional scenario: for instance, a PET that targets the authentication
scenario addresses protection of ID.

– The Foundation and Data dimensions not relevant for the selection in prac-

tice.

– The Trusted Third Party dimension is covered by our criterion of architec-

tural impact.

– The Reversibility dimension is largely the same criterion as our utility crite-
rion, since the distortion or deletion of a value is usually non-reversible.

Furthermore, previous selection methodologies do not provide means for pri-
oritizing PETs [11,26]. In a set of potentially applicable PETs, however, we would
argue it is important to have prioritization factors, such as their maturity, as we
propose in this paper.

10

I. Kunz and A. Binder

4 Related Work

4.1 Privacy By Design

Generally, our classiﬁcation can be seen as a tool that supports privacy by de-
sign. As such, it is complementary to other privacy engineering methods which
often assume a PET-selection without further detailing this step [19,20,35]. One
such approach is proposed by Alshammari and Simpson [2] who develop an engi-
neering process that devises architectural strategies, i.e. combinations of tactics,
patterns, and PETs, to fulﬁll privacy goals. In their approach, the set of usable
PETs is determined by the chosen design pattern. The concrete selection of a
PET, however, is not speciﬁed in their work. As such, our classiﬁcation could be
integrated into their methodology.

4.2 Systematization of PETs

Also, further works have investigated the selection and systematization of PETs.
Al-Momani et al. [1] follow a similar approach as we do but focus on privacy
patterns rather than concrete technologies. As explained above, patterns rather
target soft privacy goals. They use the following criteria to classify patterns:
applicability scope , privacy objective, qualities (e.g. performance impact), data
focus, and LINDDUN GO hotspot. In this paper, we have partly used similar
criteria; many criteria, however, are diﬀerent since the selection of concrete tech-
nologies requires other seleciton criteria than patterns, e.g. maturity. Note also,
that our classiﬁcation of the targeted privacy objective (called privacy goal in
this paper) diﬀers in some cases from theirs. Kunz et al. [26] also propose a
selection method for PETs but their approach is limited to PETs that manipu-
late transactional data, e.g. generalization or ﬁlter. A systematization of PETs
is proposed by Heurix et al. [22]. They identify the dimensions Scenario, Aspect,
Aim, Foundation, Data, trusted third party, and Reversibility. We would argue,
however, that these dimensions are not directly helpful for engineers who have to
select a concrete PET. Rubio et al. [34] review 10 PETs regarding their eﬃciency
for smart grids. Since their analysis is focused on smart grids, they also use re-
spective classiﬁcation criteria, like suitability for billing or monitoring purposes.
Their work is thus complementary to ours since we do not include smart grids
as a functional scenario.

ENISA has previously promoted a prototype of a PET maturity reposi-
tory [13]; to the best of the authors’ knowledge, however, ENISA has not con-
tinued this repository.

There is furthermore an ENISA publication about Privacy and Data Protec-
tion by Design which classiﬁes PETs into several categories which, however, are
not intended as selection criteria [12].

Another recent ENISA publication [15] proposes a categorization of PETs
regarding the categories truth-preserving, intelligibility-preserving, and operable
technology.

Application-Oriented Selection of Privacy Enhancing Technologies

11

5 Conclusions

The selection of privacy-enhancing technologies is a task that is diﬃcult to ad-
dress systematically. In this paper we have proposed application-oriented criteria
that allow such a systematic selection, and have classiﬁed a number of PETs ac-
cording to these criteria, e.g. their functional scenario and applicable metrics.

One open issue is the performance evaluation of PETs, since their perfor-
mances are usually not easily comparable. In future work, we therefore plan to
propose an evaluation framework for the measurement of the performance of
PETs. We also want to extend our classiﬁcation with more PETs, and connect
them with other concepts, such as design patterns. Unifying these in, e.g., a com-
prehensive ontological description of privacy concepts may represent a valuable
support tool for engineers. Furthermore, existing threat modeling tools can be
extended with suggestions for mitigation based on our classiﬁcation. Future work
also needs to show the eﬀectiveness of the proposed classiﬁcation in real-world
applications.

References

1. Al-Momani, A., Wuyts, K., Sion, L., Kargl, F., Joosen, W., Erb, B., Bösch, C.: Land
of the lost: privacy patterns’ forgotten properties: enhancing selection-support for
privacy patterns. In: Proceedings of the 36th Annual ACM Symposium on Applied
Computing. pp. 1217–1225 (2021)

2. Alshammari, M., Simpson, A.: Privacy architectural strategies: an approach for
achieving various levels of privacy protection. In: Proceedings of the 2018 Workshop
on Privacy in the Electronic Society. pp. 143–154 (2018)

3. AMD: Amd sev. https://developer.amd.com/sev/
4. Bab, K., Flockhart, P., Qin, L., Micheal, W.M., Issa, R., Lapets, A., Manzelli, R.,

Howe, W., Jansen, F.: Jiﬀ. https://github.com/multiparty/jiff (2021)

5. Bloemen, R., Vienhage, P.: Openzkp. https://github.com/0xProject/OpenZKP

(2020)

6. Bost,

R.:

symmetric
Open
https://github.com/OpenSSE (2021)
S.:

Diehl,

A.,

7. Centelles,

searchable

encryption

(opensse).

1-out-of-2

oblivious

transfer.

https://github.com/adjoint-io/oblivious-transfer (2020)

8. Claßen, Philip and Grabowski, Kamil and Modras, Krzysztof: Anonymous creden-

tials. https://github.com/whotracksme/anonymous-credentials (2020)

10. Deng, M., Wuyts, K.,
running
Linddun:

9. David, R., Sison, J., Vickery, J., Bundoo, K.A., Ahmed, S., Sybil-E: Lsb-
steganography. https://github.com/RobinDavid/LSB-Steganography (2020)
Scandariato,
-
example

Joosen,
W.:
-.
https://www.linddun.org/downloads, Social network 2.0 – running example
https://www.linddun.org/downloads,Socialnetwork2.0\T1\textendashrunningexample,
accessed on 14.02.2022

Preneel,

network

social

R.,

2.0

B.,

11. Deng, M., Wuyts, K., Scandariato, R., Preneel, B., Joosen, W.: A privacy threat
analysis framework: supporting the elicitation and fulﬁllment of privacy require-
ments. Requirements Engineering 16(1), 3–32 (2011)

12

I. Kunz and A. Binder

12. European
Privacy
https://www.enisa.europa.eu/publications/privacy-and-data-protection-by-design

Cybersecurity
by

for
protection

(ENISA):
(2015),

Agency

design

Union

data

and

13. European Union Agency for Cybersecurity (ENISA): Pets maturity assessment

repository (2019), https://www.enisa.europa.eu/publications/enisa2019s-pets-maturity-assessment-repository

14. European Union Agency

for Cybersecurity

tion techniques and best practices—recommendations
nology
privacy
https://www.enisa.europa.eu/publications/pseudonymisation-techniques-and-best-practices

protection

according

data

and

to

(ENISA): Pseudonymisa-
tech-
on shaping
(2019),
provisions

15. European Union Agency for Cybersecurity (ENISA)): Data protection engineering

(2022), https://www.enisa.europa.eu/publications/data-protection-engineering

16. Frederickson, C.: recrypt. https://github.com/IronCoreLabs/recrypt-rs (2022)
17. Goldberg, I.: Oﬀ-the-record messaging. https://otr.cypherpunks.ca/
18. Google:

homomorphic

encryption

(fhe).

Fully

https://github.com/google/fully-homomorphic-encryption

19. Gürses, S., Troncoso, C., Diaz, C.: Engineering privacy by design. Computers,

Privacy & Data Protection 14(3), 25 (2011)

20. Gürses, S., Troncoso, C., Diaz, C.: Engineering privacy by design reloaded. In:

Amsterdam Privacy Conference. pp. 1–21 (2015)

21. Hansen, M., Jensen, M., Rost, M.: Protection goals for privacy engineering. In:

2015 IEEE Security and Privacy Workshops. pp. 159–166. IEEE (2015)

22. Heurix, J., Zimmermann, P., Neubauer, T., Fenz, S.: A taxonomy for privacy en-

hancing technologies. Computers & Security 53, 1–17 (2015)

23. Hundepool, A., Domingo-Ferrer, J., Franconi, L., Giessing, S., Nordholt, E.S.,
Spicer, K., De Wolf, P.P.: Statistical disclosure control, vol. 2. Wiley New York
(2012)

24. IBM: libgroupsig. https://github.com/IBM/libgroupsig (2021)
25. Intel: Intel sgx. https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html
26. Kunz, I., Banse, C., Stephanow, P.: Selecting privacy enhancing technologies for
iot-based services. In: International Conference on Security and Privacy in Com-
munication Systems. pp. 455–474. Springer (2020)

27. Li, N., Li, T., Venkatasubramanian, S.: t-closeness: Privacy beyond k-anonymity
and l-diversity. In: 2007 IEEE 23rd international conference on data engineering.
pp. 106–115. IEEE (2007)

28. Liones, E., Langille, D.: Muchpir demo. https://github.com/ReverseControl/MuchPIR

(2021)

29. Machanavajjhala, A., Kifer, D., Gehrke, J., Venkitasubramaniam, M.: l-diversity:
Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery from
Data (TKDD) 1(1), 3–es (2007)

30. Mivule, K.: Utilizing noise addition for data privacy, an overview. arXiv preprint

arXiv:1309.3958 (2013)

31. NASA:

Technology

readiness

level

deﬁnitions.

https://www.nasa.gov/directorates/heo/scan/engineering/technology/technology_readiness_level

32. Pﬁtzmann, A., Hansen, M.: A terminology for

talking about privacy by
data minimization: Anonymity, unlinkability, undetectability, unobservability,
pseudonymity, and identity management (2010)

33. Prasser, F., Kohlmayer, F., Babioch, K., Vujosevic, I., Bild, R.: Arx data

anonymization tool. https://arx.deidentifier.org/

34. Rubio, J.E., Alcaraz, C., Lopez, J.: Selecting privacy solutions to prioritise control
in smart metering systems. In: International Conference on Critical Information
Infrastructures Security. pp. 176–188. Springer (2016)

Application-Oriented Selection of Privacy Enhancing Technologies

13

35. Spiekermann, S., Cranor, L.F.: Engineering privacy. IEEE Transactions on software

engineering 35(1), 67–82 (2008)

36. Sweeney, L.: k-anonymity: A model for protecting privacy. International Journal
of Uncertainty, Fuzziness and Knowledge-Based Systems 10(05), 557–570 (2002)
2018),

37. The TensorFlow Federated Authors: TensorFlow Federated (12

https://github.com/tensorflow/federated

38. The TOR Project: Tor browser. https://www.torproject.org/
39. Unknown authors: Openabe. https://github.com/zeutro/openabe (2021)
40. Unknown Authors: Python implementation of post-randomisation method for dis-

closure control. https://github.com/JiscDACT/pram (2021)

41. Unknown authors: Diﬀerential privacy. https://github.com/google/differential-privacy

(2022)

42. Wagner, I., Eckhoﬀ, D.: Technical privacy metrics: a systematic survey. ACM Com-

puting Surveys (CSUR) 51(3), 1–38 (2018)

43. Wagner, I., Yevseyeva, I.: Designing strong privacy metrics suites using evolution-
ary optimization. ACM Transactions on Privacy and Security (TOPS) 24(2), 1–35
(2021)
44. Wuyts,
J.:
https://www.linddun.org/mitigation-strategies-and-solutions,
2021-07-30

Wouter,
solutions.
accessed:

K.,
Linddun:

Mitigation

strategies

Landuyt,

Sions,

Van

and

D.,

L.,

