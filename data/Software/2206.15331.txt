GitHub Copilot AI pair programmer: Asset or Liability?

Arghavan Moradi Dakhel∗, Vahid Majdinasab∗, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais

Polytechnique Montreal, Montreal, Canada

Zhen Ming (Jack) Jiang

York University, Toronto, Canada

2
2
0
2

n
u
J

0
3

]
E
S
.
s
c
[

1
v
1
3
3
5
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL)
based solution, called Copilot, has been proposed by Open AI and Microsoft as an industrial product. Although some
studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to
understand how developers can beneﬁt from it eﬀectively. In this paper, we study the capabilities of Copilot in two
diﬀerent programming tasks: (i) generating (and reproducing) correct and eﬃcient solutions for fundamental algorithmic
problems, and (ii) comparing Copilot’s proposed solutions with those of human programmers on a set of programming
tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in
computer science, like sorting and implementing basic data structures. In the latter, a dataset of programming problems
with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all
fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has
some diﬃculties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show
that the correct ratio of human solutions is greater than Copilot’s correct ratio, while the buggy solutions generated by
Copilot require less eﬀort to be repaired. While Copilot shows limitations as an assistant for developers especially in
advanced programming tasks, as highlighted in this study and previous ones, it can generate preliminary solutions for
basic programming tasks.

Keywords: Code Completion, Language Model, GitHub Copilot, Testing.

1. Introduction

Recent breakthroughs in Deep Learning (DL), in par-
ticular the Transformer architecture, have revived the Soft-
ware Engineering (SE) decades-long dream of automating
code generation that can speed up programming activities.
The aim of program generation is to deliver a program that
meets a user’s intentions in the form of input-output ex-
amples, natural language descriptions, or partial programs
[2, 22, 17].

Program synthesis is useful for diﬀerent purposes such
as teaching, programmer assistance or the discovery of
new algorithmic solutions for a problem [17]. One ﬁnds
diﬀerent approaches to automatic code generation in the
literature, from natural language programming [23] and
formal models [10, 19] to Evolutionary Algorithms [33] and
machine-learned translation [28].

∗Corresponding authors. Both authors contributed equally to this

research.

Email addresses: {arghavan.moradi-dakhel,
vahid.majdinasab, amin.nikanjam, foutse.khomh,
michel.desmarais}@polymtl.ca (Amin Nikanjam, Foutse Khomh,
Michel C. Desmarais), zmjiang@cse.yorku.ca (Zhen Ming (Jack)
Jiang)

Novel large language models with the transformer archi-
tecture recently achieved good performance in automatic
program synthesis [4, 5, 6, 14]. One such model is Codex [5];
a GPT-3 [4] language model with up to 12 billion parame-
ters which has been ﬁne-tuned with 159 GB of code samples
from 54 million GitHub repositories. Codex shows a good
performance in solving a set of hand-written programming
problems (i.e., not in the training dataset) using Python,
named HumanEval dataset [5]. This dataset includes sim-
ple programming problems with test cases to assess the
functional correctness of codes. A production version of
Codex is available as an extension on Visual Studio Code
development environment, named GitHub Copilot1. Copi-
lot, as an industrial “AI pair programmer”, can generate
code in diﬀerent programming languages when provided
with some context (called prompt), such as comments,
methods names, and surrounding code. There are already
studies that evaluated the correctness of Copilot’s solutions
on diﬀerent tasks [11, 34]. However, they did not inves-
tigate the reproducibility or complexity of the generated
solutions. Nguyen and Nadi [25] compared the complexity

1https://copilot.github.com/

Preprint submitted to Journal of Software and System

July 1, 2022

 
 
 
 
 
 
of Copilot’s solutions in diﬀerent programming languages,
besides their correctness. However, they did not analyze
the eﬃciency of the solutions proposed by Copilot. They
also did not examine what it would take to repair the
buggy solutions. Authors in [35] conducted a user study to
understand how Copilot can help programmers complete a
task. They studied how much time participants needed to
complete a task using Copilot. They neither examined the
quality or complexity of the codes produced by Copilot nor
did they compare them with the codes generated by their
participants (without using any code completion tool).

Therefore, despite all these previous studies, we still
do not know if–how Copilot, as an industrial component,
can be leveraged by developers eﬃciently. We need to go
beyond evaluating the correctness of Copilot’s suggestions
for fundamental programming problems and examine how
despite its limitations, it can be used as an eﬀective pair
programming tool. In this paper, we conduct an empirical
evaluation of Copilot’s strengths and weaknesses with two
diﬀerent strategies and formulate guidelines for its eﬀective
adoption as well as recommendations for its improvement.
First, we assess Copilot in solving fundamental algorithmic
problems (i.e., searching and sorting) in programming. We
study the correctness and reproducibility of Copilot’s solu-
tions to these problems. Secondly, we compare Copilot’s
solutions with human solutions in solving programming
tasks, to assess the extent to which it can mimic the work
of a human pair programmer. We use a dataset of diﬀerent
programming tasks containing up to 4000 human-provided
solutions (correct and buggy). The results of our study
show that Copilot is capable of providing eﬃcient solutions
for the majority of fundamental problems, however some
solutions are buggy or non-reproducible. We also observed
that Copilot has some diﬃculties in combining multiple
methods to generate a solution. Compared to human pro-
grammers, Copilot’s solutions to programming tasks have
lower correct ratio and diversity. While the buggy codes
generated by Copilot can be repaired easily, the results
highlight the limitation of Copilot in understanding some
details in the context of the problems, which are easily
understandable by humans.

To summarize, this paper makes the following contri-

butions:

• We present an empirical study on the performance
and functionality of Copilot’s suggestions for funda-
mental algorithmic problems.

• We empirically compare Copilot’s solutions with hu-
man solutions on a recent dataset of programming
problems.

• We indicate and discuss reproducibility of the results

when experimenting with Copilot.

• We make the dataset used and the detailed results
obtained in this study publicly available online [13]
for other researchers and–or practitioners to replicate
our results or build on our work.

2

The rest of this paper is organized as follows.
We brieﬂy review the related works in Section 2. Section 3
presents the design of our study to evaluate Copilot as
an assistant to developers. We report our experiments to
assess Copilot’s suggestions for fundamental algorithmic
problems and compare generated suggestions with what
programmers really do on speciﬁc programming tasks in
Section 4. We discuss our results and potential limitations
in Section 5. Threats to validity are reviewed in Section 6.
Finally, we conclude the paper in Section 7.

2. Related Works

A few studies empirically investigate diﬀerent capabili-
ties of Copilot. Sobania et al. [32] compared Copilot with
a genetic programming (GP)-based approach that achieved
good performance in program synthesis. Their ﬁndings
show that GP-based approaches need more time to gen-
erate a solution. Moreover, training GP-based models is
expensive due to the high cost of data labeling. Also, these
approaches are not suitable to support developers in prac-
tice as GP usually generates codes that are bloated and
diﬃcult to understand by humans [32]. Vaithilingam et al.
[35] conducted a human study involving 24 participant to
understand how Copilot can help programmers to complete
a task. They focused on 3 Python programming tasks: “1.
edit csv, 2. web scraping” and “3. graph plotting”. These
tasks involve less problem solving eﬀort compared to the
typical programming tasks in our study. They are mostly
related to using programming language libraries. Also,
they did not compare the Copilot’s suggestions with their
participants’ suggestions when working without the help
of Copilot. Their ﬁnding shows that while Copilot did not
necessarily improve the task completion time and success
rate, programmers prefer to use Copilot for their daily
tasks because it suggests good starting points to address
the task. In Drori and Verma [11], authors studied the
capability of Copilot in solving linear algebra problems for
the MIT linear algebra course. In the same line of work,
Tang et al. examined the capability of Copilot in solving
university level probability and statistical problems [34].
These two studies only focused on the correctness ratio of
Copilot’s solutions and did not examine its performance on
programming tasks. Nguyen and Nadi [25] evaluated Copi-
lot on 33 LeetCode questions in 4 diﬀerent programming
languages. They used the LeetCode platform to test the
correctness of Copilot’s solutions. The questions in their
study included diﬀerent levels of diﬃculty. Although they
evaluated the correctness of Copilot’s solutions and com-
pared their understandability, they did not assess whether
Copilot successfully found the optimal solution for each
task.

Another group of studies focuses on vulnerability is-
sues to evaluate Copilot solutions. As mentioned before,
Copilot is trained on a large volume of publicly available
code repositories on GitHub which may contain bug or

vulnerability problems. Pearce et al. [27] conducted dif-
ferent scenarios on high-risk cybersecurity problems and
investigated if Copilot learns from buggy code to generate
insecure code. Another study investigates how Copilot can
reproduce vulnerabilities in human programs [3]. To do
so, they ﬁrst used a dataset of vulnerabilities generated
by humans, then they rebuilt the whole code before the
bug and asked Copilot to complete the code. The com-
pleted section was manually inspected by three coders to
determine if Copilot reproduced the bug or ﬁxed it. Moroz
et al. [24] examined the challenges and the potential of
Copilot to improve the productivity of developers. They
highlighted the copyright problems and the safety issues of
its solutions. They discussed about the non-deterministic
nature of such models and the harmful content that could
be generated by them. Authors in [37] surveyed 2631 de-
velopers about the impact of Copilot on their productivity
and highlighted diﬀerent metrics of users’ interaction with
Copilot that help predict their productivity. They relied on
the SPACE [15] framework to generate 11 Likert-style ques-
tions in their survey. Also, they analyzed the usage data of
Copilot of the participants who responded to this survey.
They extracted diﬀerent metrics from this data such as
acceptance rate of solutions, persistence rate, unchanged
and mostly unchanged accepted solutions, etc. They found
that the acceptance rate of solutions by developers is the
most relevant metric that shows the impact of Copilot on
the productivity of developers.

To the best of our knowledge, none of these studies
compared Copilot with humans for solving programming
tasks. The majority of these studies focused on assessing
the correctness of Copilot’s solutions and highlighted its
issues; e.g., the presence of vulnerabilities in generated so-
lutions. In this study, we focus on fundamental algorithmic
problems and compare Copilot with humans in solving real
world programming tasks.

3. Study Design

In this section, we present our methodology to assess
Copilot as an AI-based assistant for developers and detail
the experimental design to achieve our goals. To assess
Copilot as an “AI pair programmer”, we need to see how
it can eﬀectively assist developers during the development
process. To solve a programming task, a developer usually
builds on the top of fundamental data structures (e.g.,
queues, trees, graphs) and algorithms (e.g., search, sorting)
in computer science. Moreover, the developer needs to
come up with creative ideas to achieve the goal(s) of the
programming task eﬃciently. As the development is an
iterative process and a developer may examine her code
several times for reﬁnement/debugging, the bug-proneness
and creativity of automatically generated solutions should
be assessed. Therefore, an ideal automatic code generator
should be able to recommend proper structure/algorithms
and innovative clues to the developer, so that the developer
can build on them. The recommended code is expected

to be competitive with what humans may generate during
the development.

As none of the previous studies examined the eﬀective-
ness of Copilot as a programming assistant, we evaluate
Copilot on: 1) the adequacy of recommended code for fun-
damental algorithmic problems, and 2) how the recommen-
dation compares to human-provided solutions. Speciﬁcally,
we address the following research questions (RQs):

RQ1: Can Copilot suggest correct and eﬃcient solutions

for fundamental algorithmic problems?

– Are Copilot’s suggestions for these problems

reproducible?

RQ2: Are Copilot’s solutions competitive with human solu-

tions for solving programming problems?

– How diﬃcult is it to repair Copilot’s buggy sug-

gestion compared to human solutions?

In the rest of this section, we describe the methodology
we followed to answer each of our RQs as illustrated in
Figure 1.

3.1. RQ1: Copilot on Algorithm Design

Our goal in RQ1 is to observe if Copilot can gener-
ate solutions for basic algorithmic problems given clear
descriptions of the problem (the same way developers are
assessed in their coding ability). The algorithms we have
chosen are fundamental in software development. There-
fore, it is imperative to study Copilot’s ability in generating
code for them since it claims to take the position of a pair
programmer.

3.1.1. Data Collection

We selected the problems and their descriptions from
[8]. We choose this resource because it is a highly cited
and prestigious textbook that is widely used for teaching
algorithmic design fundamentals to computer science stu-
dents [9]. In this book, the authors explain the principal
algorithms that computer engineers must be knowledge-
able about by breaking them into categories. Since our
problems were selected from this book, we followed its cat-
egorization, such that our tests on Copilot were conducted
on 4 categories:

• Sorting Algorithms: Sorting algorithms are among
the ﬁrst algorithmic concepts that are taught to com-
puter science students. These algorithms introduce
the concept of time complexity and how ineﬃcient
code can make diﬀerences in more complex programs.
Sorting algorithms are used in databases to segment
large amounts of data that cannot be loaded entirely
into memory or in numerical operations to determine
which numbers should undergo operations ﬁrst in
order for the results to be produced as quickly as
possible. From this section, we selected some well-
known sorting algorithms which students are asked

3

Figure 1: The Workﬂow of proposed methods. The study includes two diﬀerent methods to test Copilot in recommending codes to solve
programming problems. The ﬁrst pipeline focuses on algorithmic problems collected from a well-known algorithm design book [8]. The second
pipeline focuses on the assignments of a Python programming course [20]. It compares Copilot with students in solving programming problems
in diﬀerent aspects.

to learn and then implement. These algorithms are
methods for sorting an array of numbers (integers
or ﬂoats) in a descending or ascending manner. In
these problems, time complexity, a measure of an
algorithm’s run-time as a function of input length, is
an important factor to be considered.

From the algorithms described in the book, we se-
lected bubble sort, bucket sort, heap sort, insertion
sort, merge sort, quick sort, radix sort, and selection
sort. We selected these algorithms based on their
implementation complexity, from easy to hard, based
on [8]’s descriptions. Copilot’s ability to understand
and produce code for these algorithms, will allow the
programmer to use the generated code in their code
bases and instead focus on more complex parts of the
program.

• Data Structures. From this section, we selected
the Binary Search Tree (BST). BST is a basic data
structure which is taught in algorithm design. Each
node of the tree contains a value (called key) that
is greater than all the values in the left sub-tree
and smaller than all the values in its right sub-tree.
The implementation of BST involves multiple steps,
namely:

– Finding the minimum and maximum values in

the tree before inserting any new value.

– In-order tree walk to extract all the values in

the tree in a sorted manner.

– Finding the successor node. Given a node x, the
successor of x is the node that has the smallest
value which is greater than x.

4

• Graph Algorithms. From this section, we selected
the Elementary Graph Algorithms. These algorithms
are used to perform some elementary operations on
a graph. Since graphs store information about how
each node is connected to others, they can be used in
implementing applications such as maps and social
media user connections. We tested Copilot on the
following graph algorithms problems:

– Generating code for a simple graph data struc-

ture.

– Breadth First Search (BFS) on a graph.

– Depth First Search (DFS) on a graph.

– Implementing Directed Acyclic Graphs (DAG).
DAGs require a more complex implementation
logic compared to simple graphs, since during ini-
tialization, based on the directions of the edges,
we need to check if a cycle exists in the graph
or not.

– Finding reachable vertices. A pair of vertices
are deﬁned as reachable, if both vertices can be
reached from each other in a directed graph.

• Advanced Design and Analysis Techniques. We
selected the greedy algorithms from this section. Un-
like the algorithms described above, greedy algo-
rithms are methods for solving optimization problems
by breaking them down into multiple smaller prob-
lems and selecting the best solution at the given time.
We selected the “activity selection”, an introductory
problem to greedy algorithms from [8].

Solution FoundOptimize SolutionFoundReproduced CorrectsolutionSame DayAfter 30 Days Students'SubmissionsEvaluationRepairing Cost ofBuggy SolutionsDiversity of CorrectSolutionsQuality of SolutionsRatio of CorrectSolutionsStudentsGitHubCopilotAssignmentsPythonProgrammingCourseRecommened SolutionEvaluationRepairing  ToolPromptEngineeringAlgorithmicProblemsdef search(x, seq):    for i in range(len(seq)):        if x <= seq[i]:            return i    return len(seq)Correct SolutionFound3.1.2. Prompt Engineering

Alongside code completion, Copilot can generate code
from natural language descriptions in the form of comments.
However, as noted by [21], if the description becomes too
long or detailed, Copilot’s performance degrades. Authors,
in [8], assumed that the reader has no experience in cod-
ing or algorithm design. For this reason, each problem is
described by building upon concepts that were explained
in the previous chapters. As a result, some problem de-
scriptions span multiple paragraphs and sometimes, pages.
Therefore, our prompt engineering was done in two ways:

1. Describing the problem: We needed to summa-
rize each problem’s description to feed them to Copi-
lot, while staying as faithful as possible to the books.
To make sure that our descriptions were understand-
able and did contain enough information about the
algorithm being described, we cross-checked each of
them with those on popular coding websites such
as W3SCHOOLS [36] and GEEKSFORGEEKS [16]
as well. For cross-checking, the second author sum-
marized [8]’s algorithm descriptions while keeping
Copilot’s limits in mind. If there were diﬀerences
in the descriptions (i.e., the description was missing
some key elements in explaining the problem), the
descriptions were revised. This approach was taken
for sorting algorithms, binary search trees, and the
“activity selection” problem.

2. Asking Copilot to generate a solution by men-
tioning Algorithm/Problem’s name: For ele-
mentary graph algorithms, instead of describing the
problem as if the developer has no knowledge about
the algorithm, we asked Copilot to generate codes
by directly asking what we were looking for. For
example, instead of describing what a graph is, we
asked ”create a class that implements a graph data
structure”. We did this for two reasons:

• The graph data structure is well known even to
novice programmers and we are trying to assess
Copilot’s ability in generating code in the same
level of novice and intermediate programmers.
• Given Copilot’s position as a pair programmer,
it would be counter productive to explain fun-
damental algorithms during software design and
production. Hence, we wanted to see if Copilot
is able to recognize well-known data structures
without having to describe what they are in-
depth.

The second author created the input descriptions as
explained above. Then, these descriptions where checked
with the ﬁrst author to make sure that they were correct,
understandable, and contained enough information about
the problem being described. The ﬁrst two authors both
have more than 5 years of experience in coding and pro-
gram design. To assess the inter-rater agreement, we have

5

calculated Cohen’s Kappa score [7]. While the score was
0.95 implying an almost perfect agreement, for cases
where there were conﬂicts about the descriptions, we met
and discussed the conﬂicts to resolve them. At the end,
the descriptions were checked with the third author who
has more than 10 years of experience in teaching algorithm
design. Therefore, the ﬁnal input descriptions were what
all three authors agreed on.

Excluding sorting algorithms, other problems require
code to be generated using a previous code as it is very
common practice in both functional and object oriented
programming For these problems, we followed exactly the
book’s example by asking Copilot to generate code for the
underlying concept and then for the succeeding problems,
we asked it to implement the solution using the code it had
generated before. We have recorded all our descriptions
and Copilot’s responses in our replication package [13].

3.1.3. Query Execution and Evaluation Metrics

Below, we brieﬂy explain the 4 diﬀerent markers which
we have used to evaluate Copilot and explain them in detail
in the rest of this section.

1. Successful code generation (Response Received).
Whether Copilot was able to understand the problem
and output code for solving it.

2. Code correctness. Whether the code which was

suggested has syntax errors or bugs.

3. Code optimality. Whether Copilot was able to
generate solutions with optimal time complexity.

4. Code reproducibility. Whether the code gener-
ated by Copilot on another independent run has the
same structure as another run’s given the same de-
scription.

Testing Successful Code Generation:. For testing suc-
cessful code generation, we gave the description of the prob-
lem in the form of comments. Then, Copilot was asked to
generate code for the input description. Given that Copilot
tries to understand the general purpose of the code from
the script’s ﬁlename, to make sure that solutions were gen-
erated from our descriptions, we gave the scripts unrelated
names. For example, if we were testing on bubble sort, we
named the corresponding script as ”script 1”.

Testing Code Correctness and Optimality:. For test-
ing code correctness and optimality, we took 2 steps:

• In order to analyze generated codes’ time complexity,
the codes were inspected manually by the ﬁrst two
authors.

• In order to make sure that the generated codes were
correct and functioned as intended, we wrote unit
tests for each script and ran them to make sure that
the generated solution runs without any syntax er-
rors and/or bugs. All scripts are accessible in our
replication package.

Testing Code Reproducibility:. For testing code repro-
ducibility, ﬁrst we need to ensure that our results are as
independent from each other as possible. So, we terminated
our internet connection and closed the editor after each
experiment. After which, we re-connected to the internet,
opened the editor, and asked Copilot to generate solutions
by giving it the exact same description. Another step in
testing reproducibility was repeating the steps above, 6
times for each problem. In order to make sure our results
stay consistent over time, we performed 2 trials within a
30 day time window. Each trial, consists of 3 experiments
for each algorithm and each experiment contains up to 10
suggestions provided by Copilot. The experiments done
during the ﬁrst trial are named tests 1 through 3 in our
dataset. The results of the second trial which was con-
ducted 30 days later are named tests 4 through 6 in our
dataset. For quantifying the reproducibility between trials,
we have compared the Abstract Syntax Trees (AST) of each
of Copilot’s suggestions between each test. To do this, we
have used the code from [30]. We measure reproducibility
in 2 ways:

1. Inter-set reproducibility: The inter-set reproducibil-
ity is calculated as the average of AST similarity
between the suggestions in each trial. Each trial con-
sists of 3 separate experiments where we ask Copilot
to generate a solution for the given problem. The
inter-set reproducibility, measures how similar these
suggestions (in a trial) are to each other.

2. Intra-set reproducibility: The intra-set reproducibil-
ity is calculated as the average of AST similarity
between the suggestions of diﬀerent trials. We have
conducted 2 diﬀerent sets of trials within a 30 day
time window. The intra-set reproducibility measures
how similar the suggestions between 2 trials, are to
each other after this time window.

To measure AST similarity, the codes that are to be
compared to each other need to have no syntax errors.
Even though some of Copilot’s suggestions are similar to
each other, since they had syntax errors, we could not
calculate AST similarity between them without editing
the generated code. Doing so, meant ﬁxing these syntax
errors by hand which would have been against our goal
to test Copilot solely on code generation. As a result, for
measuring AST similarity, we did not include suggestions
which had errors and could not be compiled.

The second author, executed all the steps above in
order to collect experiment results. Then, the ﬁrst author
examined and evaluated the results obtained by the second
author in order to validate them on the markers described
earlier in this section. This, resulted in a Cohen’s Kappa
coeﬃcient of 0.61 which indicates a moderate agreement
between the ﬁrst and second authors. For resolving conﬂicts
of opinion, the results were discussed with the third author
in a meeting to reach a consensus. After doing so, for
each algorithm category, a Cohen’s Kappa coeﬃcient to

asses the inter-rater agreements between the authors were
calculated as follows:

• Sorting Algorithms: 0.89

• Binary Search Trees: 1

• Elementary Graph Algorithms: 0.83

• Greedy Algorithms: 1

Which results in an average Kappa coeﬃcient of 0.93 which
indicates an almost perfect agreement on the obtained
results between the authors.

3.2. RQ2: Copilot vs. Human

In this subsection, we aim to describe our methodology
for RQ2, on how to compare Copilot codes with human
written codes. First we illustrate the dataset of program-
ming tasks that we used in our experiments and explain
why select this dataset. Then, we explain how we employ
Copilot to generate solutions for each task in this dataset.
After that, we present how we selected students’ solutions
for this comparison. Finally we discuss the criteria to com-
pare Copilot with students in solving Python programming
tasks from diﬀerent aspects.

3.2.1. Dataset of Programming Tasks

To conduct this part of the study, we require a dataset
of some programming problems plus human-provided solu-
tions. To address this requirement, we use a dataset of a
Python repairing bug benchmark2 that includes students’
submissions for 5 Python programming assignments in a
Python course. Since Copilot is still in initial steps and
shows good performance on simple programming tasks, we
choose this dataset of simple programming assignments
with the novice programmer level codes (submissions) for
a Python course. These tasks are not included in the pro-
gramming tasks in HumanEval [5] evaluation set. Also,
this dataset includes diﬀerent test cases for each task and a
tool that gives us the possibility to automatically check the
functional correctness of Copilot’s solutions against test
cases. In addition, the description of problems are human-
written. It reduces the chance for Copilot to memorize the
solutions from its trainset.

This dataset includes 2442 “Correct” and 1783 “Buggy”
student submissions for 5 Python programming assignments
in a Python course. Another study also used this dataset for
characterizing the beneﬁt of adaptive feedback for errors
generated by novice developers [1]. Table 1 shows the
description of each programming task. Each task includes a
description of the problem, one or more reference solutions,
a diﬀerent number of submissions by students that includes
“Correct” and “Buggy” solutions, and diﬀerent unit tests
for each task, with an average of 9 tests per problem, to
evaluate the functional correctness of solutions.

2https://github.com/githubhuyang/refactory

6

This dataset also contains a tool named “Refactory”
to automatically repair the buggy submissions of students
if applicable [20].
In our study, we use this tool to re-
pair buggy solutions generated by Copilot and students
to evaluate the complexity of ﬁxing bugs in codes gener-
ated by Copilot compared to those of novice programmers.
This tool matches each buggy program with the closest
correct solution based on its AST structure. Then, it mod-
iﬁes diﬀerent blocks of the incorrect program to repair its
bug(s) and convert it to a correct solution if possible. This
tool shows better performance than other state-of-the-art
methods in repairing buggy programs such as Clara [18].
Despite others that need a large and diverse range of cor-
rect solutions, this tool can repair buggy codes even with
one or two references (i.e., correct solutions).

3.2.2. Solving Programming Problems with Copilot
To generate solutions with Copilot, we feed the descrip-
tion of each programming task in Table 1, called prompt, to
Copilot. At each attempt, Copilot only returns the Top-10
solutions for a prompt. Thus, we do not have access to
the rest of the potential suggestions. To inquire about the
Copilot’s consistency in generating solutions, similar to the
previous experiments, we repeat the process 5 times and
each time collect its top 10 suggested solutions. Expressly,
we ask Copilot to solve each programming problem in 5
diﬀerent attempts and collect the top 10 suggested solu-
tions in each one. Thus in total, we collect 50 solutions by
Copilot for each problem.

As we already explained in subsection 3.2.1, there are
diﬀerent test cases per task. To evaluate the functional
correctness of Copilot’s solutions, a solution is considered
“’Correct” if it passes all the unit tests related to its problem.
Otherwise, it is considered as “Buggy”.

3.2.3. Downsampling Student Solutions

The average number of student submissions for these
tasks is 689.8. In each attempt on Copilot, we only have
access to its top 10 suggestions. One solution to have more
suggestions could be to increase the number of attempts on
Copilot. But, increasing the number of attempts to more
than 5 will increase the number of duplicate answers in
Copilot’s solutions. We discuss the duplicate solutions in
Section 3.2.4 with more details. Thus, instead of increasing
the number of attempts on Copilot, we downsample the
students’ submissions to the same size of Copilot solutions
(50 in total) to have a fair comparison between students
and Copilot in the number of solutions.

3.2.4. Evaluation Criteria

For this part of our study, we consider diﬀerent criteria
to compare solutions suggested by Copilot and students
to solve these programming tasks. We investigate the
solutions on the following markers.
In the rest of this
section, we explain each metric in more details.

1. The correct ratio of solutions (pass@Topk),

2. The repairing costs of buggy solutions,

3. The diversity of solutions,

4. The cyclomatic complexity of codes.

(1) The correct ratio of solutions (pass@Topk)

A very common metric to evaluate programming lan-
guage models is pass@k metric [21, 5]. For example, to
calculate pass@100, we need to generate n ≥ 100 sample
solutions, count the number of solutions that passed all
test cases, and then calculate the fraction of 100 out of all
correct solutions. However, since Copilot returns only the
Top10 solutions in each attempt, we cannot accurately use
this metric in our study.

In this study, what attracts our interest is the pass@Topk
of all the attempts. It means that if we call Copilot n times
for the same problem (the same prompt), n equals the
number of attempts, and collect the Topk solutions of each
attempt, then pass@Topk equals the fraction of these so-
lutions that passed all the test units. As an example for
pass@Top2, we collect all the Top2 suggested solutions
for a problem in n = 5 diﬀerent attempts (#solutions =
k ∗ n = 2 ∗ 5 = 10). Then pass@Top2 reports the fraction
of these Top2 solutions that passed all test units. However,
we cannot calculate the pass@TopK for students.

Another evaluation that comes to our attention is the
Correct Ratio (CR) of solutions. Here by CR, we mean the
fraction of correct solutions out of all solutions suggested
by Copilot or human for each problem. We calculate this
fraction for each problem while collecting Topk suggestions
of Copilot in diﬀerent attempts. For students, we calcu-
late the fraction of correct submissions out of all students’
submissions for each problem. Also, we calculate the distri-
bution of the CR and its average in independent attempts
on Copilot. We like to study how increasing the number of
attempts (giving diﬀerent chances to Copilot to solve the
same problem) impacts the CR.

(2) The Repairing Costs of Buggy Solutions

After computing the CR for Copilot and students, we
aim to compare Copilot’s buggy solutions with students’
buggy submissions. Our observation shows that several
buggy solutions generated by Copilot can be easily con-
verted into a correct solution by applying small changes. We
discuss this observation in detail in Section 4.2.2. This ob-
servation brings us to the idea of repairing buggy solutions
generated by Copilot and students and then comparing
them in repairing costs. We use the repairing tool that we
explained in Section 3.2.1, and report three diﬀerent met-
rics to evaluate the repairing cost of buggy solutions [20]
including:

• Repair Rate: This metric shows the fraction of
buggy codes that passed all test cases after the repair
process.

• Avg. Repair Time: This metric shows the average
time taken to repair a buggy program in seconds.

7

Table 1: A summary of the dataset used to compare Copilot with the human in solving simple programming tasks. The
Dataset includes the assignments and submissions of a Python programming course. It includes students’ submissions for 5 Python programming
tasks in two categories of “Correct” and “Buggy” solutions [20].

Task

Description

Sequential
Search

Unique Dates
Months

Duplicate
Elimination

Sorting
Tuples

Top k
Elements

Takes in a value “x” and a sorted sequence “seq”,
and returns the position that “x” should go to such
that the sequence remains sorted. Otherwise, return
the length of the sequence.
Given a month and a list of possible birthdays, re-
turns True if there is only one possible birthday
with that month and unique day, and False other-
wise. Implement 3 diﬀerent functions: unique day,
unique month and contains unique day.
Write a function remove extras(lst) that takes in a
list and returns a new list with all repeated occur-
rences of any element removed.
We represent a person using a tuple (gender, age).
Given a list of people, write a function sort age that
sorts the people and returns a list in an order such
that the older people are at the front of the list.
You may assume that no two members in the list of
people are of the same age.
Write a function top k that accepts a list of integers
as the input and returns the greatest k number of
values as a list, with its elements sorted in descending
order. You may use any sorting algorithm you wish,
but you are not allowed to use sort and sorted.

q1

q2

q3

q4

q5

Total

#Correct
Solution

#Buggy
Solution

768

575

291

435

546

308

419

357

418

108

2442

1783

• Relative Patch Size (RPS): This metric deﬁnes
as the Tree-Edit-Distance (TED) between the AST
of a buggy code and the AST of its repaired code,
normalized by the AST size of the buggy code.

(3) The Diversity of Solutions

It is already shown in language models such as Codex
that increasing the number of sample solutions for a pro-
gramming task can increase the number of correct solutions
that pass all test units [5, 21]. However, they did not study
if this increment is due to the increasing the diversity of so-
lutions or if the new correct solutions are just a duplication
of previous ones.

Copilot claims that it removes duplicate solutions among
the Top10 suggested solutions in a single attempt. How-
ever, our observations show the appearance of duplicate
solutions in Top10 suggestions of a single attempt. Figure 2
shows three diﬀerent solutions generated by Copilot for the
task “Duplicate Elimination” at a single attempt. As we
can see, the structure of all three codes is the same. The
only diﬀerence between Figure 2a and Figure 2b is in the
variable name, “item” and “i”. Also, the solution in Figure
2c is the same as the solution in Figure 2a alongside com-
ments. Since Copilot compares the sequence of characters
to eliminate duplicates, it considers these three solutions

as three unique suggestions in Top10 solutions of a single
attempt.

In this study, we use a new method to remove the dupli-
cate solutions in each attempt. We investigate if increasing
the number of attempts and consequently increasing the to-
tal number of solutions will increase the number of unique
solutions. Also, we compare the diversity of solutions (cor-
rect and buggy) provided by Copilot and students. This
metric compares Copilot’s novelty in generating solutions
to that of students in solving a programming task.

To remark on the duplicate solutions, we compare the
AST of two codes, similar to what we did in Section 2. We
eliminate the last level leaves in AST which are variable
or function names. Also, we ignore comments or any nat-
ural language text in each solution. Then, we calculate a
similarity between the AST of every two solutions for a
problem by method introduced in [30]. If the similarity
between two ASTs is equal to 1, then they are assumed to
be duplicate. We keep just one of the solutions. Any value
less than 1 represents a diﬀerence between the functionality
of two solutions.

(4) The Cyclomatic Complexity of Codes

Finally, we compare the complexity of codes generated
by Copilot and students for each task. Cyclomatic Com-

8

(a) Sample code 1

(b) Sample code 2

(c) Sample code 3

Figure 2: Three diﬀerent solutions were generated by Copilot for “Duplicate Eliminatation” Task in one attempt. There is
no diﬀerence between the approach of these 3 solutions in solving the task. The only diﬀerence between (a) and (b) is in variable names, “i”
and “item”. The diﬀerence between (c) and (b) is the additional comment in (c). The diﬀerences between (c) and (a) are in variable names
and comments.

plexity (C.C.) (McCabe’s Cyclomatic Complexity) shows
the number of independent paths. Speciﬁcally, the number
of decisions that can be made in a source code [12, 31].
When comparing solutions for a problem, the lower a code’s
C.C. value is, the more optimized that code is considered
to be. We use a Python package, RADON 3, to calculate it.
C.C. close or above 10 is interpreted as not an optimized
code.

4. Empirical results

In this section, we present the results we obtained to

answer our RQs, one by one.

4.1. RQ1: Copilot on Algorithm Design

In this section, we assess the capability of Copilot to
solve algorithmic problems. First, we describe our method-
ology to answer RQ1. Then we present our empirical
results.

Tables 2 – 9 show our results on testing Copilot’s ability
for code generation. In the following sections, we discuss
our results in detail. To highlight the diﬀerence between
our 2 trials which have been conducted 30 days apart from
each other, for each marker, we have indicated the results
of the trials separately from each other. To showcase
our results from each round of testing in each trial, we
count the number of successful tests. For example, if the
generated code passes 2 of the 3 tests for a given marker,
we display it as “2/3”. We have also used “0” and “-” to
diﬀerentiate between cases where we obtained no results
and not applicable cases. For example, if we did not receive
a response for a solution, we display it as “0/3” and since
we had no responses, ﬁnding a correct solution was not
applicable, hence, we use “-” to indicate this circumstance.

4.1.1. Sorting Algorithms (Kappa Score: 0.89)

Our results show that Copilot is relatively capable at
understanding the problem and generating reproducible
code which executes at optimal time complexity. However,
if the algorithm requires helper functions (i.e., functions
which require to be coded outside of the main function in

3https://radon.readthedocs.io/en/latest

9

order for the algorithm to be implemented) it will struggle
to generate solutions. For the Bubble, Radix, and Heap
sort algorithms, Copilot generated diﬀerent codes during
our second trial.

• Bubble Sort. This sorting algorithm is one of the
easiest sorting algorithms and Copilot was able to
generate correct, optimal code for it. As shown in
Table 2, for the ﬁrst trial, out of all 3 experiments,
Copilot generated code that was related to bubble sort
and all 3 contained correct solutions. However, during
our second trial, Copilot was starting to generate code
for insertion and selection sort (instead of bubble
sort), as shown in Table 2 as “0/3”, when we gave it
the same description as before.

• Radix Sort. This algorithm is more diﬃcult to
implement. Copilot struggled to understand what
the problem was in our ﬁrst trial (“1/3” in Table
2). However, it was able to generate correct, optimal
code for it.
In the second trial however, Copilot
showed improvement in understanding the problem
as it generated codes for solving the problem (“3/3”
in Table 2) but none of the generated codes were
correct (“0/3” in Table 2).

• Heap Sort. Since implementing heap sort requires
implementing a max heap, and then writing a sorting
function, this algorithm is one of the harder algo-
rithms to implement. During our ﬁrst set of tests,
Copilot was unable to generate a solution for this
problem until we explicitly asked it to “generate a
function that implements the heap sort algorithm”
and even then the generated code was faulty. How-
ever, during our second trial, Copilot generated the
correct solution from our indirect description of the
problem where we explained the algorithm in detail
without mentioning its name.

Tables 2 and 3 show our results on Copilot’s code gen-
eration ability and the suggested codes’ reproducibility,
respectively. As our results show, there is no clear pattern
between algorithm complexity and Copilot’s output. For
example, out of the sorting algorithms we tested, Bubble
sort is the easiest to implement and Copilot was able to

Table 2: Results of Copilot’s code generation ability on sorting algorithms.

Algorithm

Bubble Sort

Bucket Sort

Heap Sort

Insertion Sort

Merge Sort

Quick Sort

Radix Sort

Selection Sort

Response Received

Correct Solution

Reproduced

Optimal

First Trial Second Trial First Trial Second Trial First Trial Second Trial First Trial Second Trial

3/3

3/3

1/3

3/3

3/3

3/3

1/3

3/3

0/3

3/3

3/3

3/3

2/3

3/3

3/3

3/3

3/3

3/3

0/3

3/3

2/3

1/3

1/3

3/3

-

1/3

2/3

3/3

0/3

1/3

0/3

2/3

3/3

3/3

-

3/3

0/3

2/3

-

2/3

-

3/3

2/3

3/3

-

2/3

2/3

-

3/3

3/3

-

3/3

2/3

1/3

1/3

3/3

-

1/3

2/3

3/3

-

1/3

-

2/3

Table 3: Similarity ratios of the AST of Copilot’s suggestions on
sorting algorithms.

Algorithm

Bubble Sort
Bucket Sort
Heap Sort
Insertion Sort
Merge Sort
Quick Sort
Radix Sort
Selection Sort

Inter-set First Trial

Inter-set Second Trial

Intra-set

0.93
0.76
-
0.93
0.007
0.54
-
0.29

-
0.38
0.39
1
-
0.4
0.43
-

-
0.49
-
0.96
-
0.008
-
-

output optimal code for it during the ﬁrst set of experi-
ments. However, during our second trial, given the same
description, Copilot was unable to generate a single code
snippet for this problem and instead generated solutions
for other sorting algorithms. On the other hand, for the
Heap sort algorithm which is much harder, during the ﬁrst
trial Copilot did not generate any correct solutions but did
so during the second trial.

The authors disagreed on the result of generated codes
for selection sort. Our source for describing the problems
was [8] and therefore, the input prompt was summarized
from the description in the book. The given prompt for
this algorithm was “create a function that accepts a list as
input. the function should create two lists named sorted
and unsorted. the function sorts an array by repeatedly
ﬁnding the minimum element (considering ascending order)
from unsorted list and putting it at the beginning of the
sorted list. ﬁnally it should return the sorted array”. Given
this description, the second author only accepted solutions
that followed this exact description, mainly those which
created the two empty sorted and unsorted lists. Upon
review however, the ﬁrst and third authors mentioned that
some solutions followed the selection sort algorithm, which
can be found on the web, without following the exact
steps mentioned in the description. After discussions, these
solutions were considered as correct as well.

4.1.2. Binary Search Trees (Kappa Score: 1)

For this problem, we ﬁrst asked Copilot to generate
the BST data structure which should comprise of a class
with parent, right, and left nodes alongside the node’s
value. After that, we asked Copilot to generate a method
which handles insertion per BST insertion algorithm for

the class. Then, we asked Copilot to create a method for
deleting a node. These operations require the BST to be
re-built in order to conform to the BST property. We also
asked Copilot to implement a search method for ﬁnding if
a value is present in the BST. These 3 methods, comprise
the base BST data structure. In the next steps, we asked
Copilot to generate functions for ﬁnding the maximum and
minimum value in a tree, performing an in-order tree walk,
and ﬁnding the successor node of a child node. Tables 4
and 5 show our results.

As our results show, Copilot is adept at producing al-
gorithms for BSTs. It should be noted that Copilot was
able to generate code which conformed to the optimal
time complexities that are required for an eﬃcient BST. In
addition, Copilot was able to generate multiple diﬀerent
versions (with iterative and recursive programming tech-
niques) for “Finding maximum and minimum values in the
tree”, “In-order tree walk”, and “Finding successor nodes”
problems. As Table 4 shows, unlike sorting algorithms,
reproducibility was not an issue as Copilot generated the
same code at least once throughout diﬀerent experiments
during diﬀerent trials. However, as Table 5 shows, there
are diﬀerences between the codes that are suggested during
each experiment which means that Copilot was not simply
repeating itself.

For the “In-order Tree Walk” problem, Copilot gen-
erated functions inside the main function responsible for
executing the walk. These functions were duplicate func-
tions of those generated for ﬁnding minimum and maximum
values in the tree. This is bad programming practice as it
over-complicates the code. However, since these functions
were not used by the original function at all, the generated
code was still optimal.

4.1.3. Elementary Graph Algorithms (Kappa Score:

0.83)

As our algorithms are becoming more complex, it is
required for Copilot to generate code that uses the previous
codes that it has generated. For example, checking if a
graph is cyclic, requires using a BFS or DFS approach. If
Copilot does not use the codes that it has generated for
BFS and DFS during checking if a graph is cyclic, we will
be left with code pieces that repeat the same operation over

10

Table 4: Results of Copilot’s code generation ability on BSTs.

Algorithm

Response Received

Correct Solution

Reproduced

Optimal

First Trial Second Trial First Trial Second Trial First Trial Second Trial First Trial Second Trial

Binary Search Tree
Data Structure

Finding Minimum and
Maximum Values in Tree

In-order Tree Walk

Finding The Successor Node

3/3

3/3

3/3

3/3

1/3

3/3

3/3

3/3

3/3

1/3

3/3

2/3

1/3

2/3

3/3

3/3

3/3

3/3

3/3

3/3

-

3/3

3/3

3/3

3/3

1/3

3/3

2/3

1/3

2/3

3/3

3/3

Table 5: Similarity ratios of the AST of Copilot’s suggestions on
BSTs.

number of activities that can be performed as long as they
do not overlap. Overlapping is deﬁned as:

• An activity’s start time must be after a previous

activity’s end time.

• An activity should not happen during another activ-

ity.

Our results are outlined in Tables 8 and 9.
Copilot was able to implement an activity class given
the description “implement a class called activity. Each
instance of this class has two attributes: start-time and
end-time. Both should be integer numbers between 0 and
24”. However, it failed to check for the class’s input types
to be integers and place a limit on the range of input
numbers.

Next, we asked Copilot to implement a method for
comparing activities. We gave it the following description:
“implement a function for comparing two activities. the
function should return True if the ﬁrst activity ends before
the second activity starts. if the inputs have overlapping
start-times, return False”. Here, Copilot implemented
the description correctly. However, since this method is
dependent on its inputs being instances of the activity
class, this code will fail if the input is anything else. Type
checking is important and a basic operation to do which
Copilot fails to do here.

For adding activities to a set of activities, Copilot was
asked to create a method which accepts a set of activities
alongside a start time and end time of an activity. The
method should ﬁrst create a new activity instance with the
given start and end time and then check if this new activity
does not overlap with the activities in the set. Copilot was
unable to generate the necessary code for this no matter
how detailed the description was.

Algorithm

Inter-set First Trial

Inter-set Second Trial

Intra-set

Binary Search Tree
Data Structure
Finding Minimum and
Maximum Values in Tree
In-order Tree Walk
Finding The Successor Node

0.17

1
0.72
-

-

0.96
0
0.1

-

0.91
0.49
-

and over which is a bad practice in programming. Tables
6 and 7 show our results.

Our results show that like BSTs, Copilot is adept at
generating code for elementary graph algorithms. As men-
tioned, up until now, we described the problem to Copilot
in detail without mentioning the name of what we wanted
explicitly. However, now that we are asking Copilot the
name of the algorithm, the generated codes have little to
no diﬀerences with each other even through diﬀerent sets
of trials. We observed that some of the generated code is
using an advanced Python feature called “operator over-
loading” in which a native Python function is re-written
by the programmer to behave diﬀerently depending on the
arguments that it receives as input. During our testing for
BFS and DFS, Copilot generated code for both algorithms
regardless of us asking it to do so only for one of them.

Even though Copilot was able to recognize and generate
code for our description, some of the generated codes had
one ﬂaw and since successor methods use the previous
methods, this bug was present in every piece of generated
code. This snow-balling eﬀect, has aﬀected our Kappa
score as well. This bug, was a result of Copilot considering
the nodes being named by integer numbers. As a result,
if a node is created with a name that is not integer (e.g.
”A” or ”Node1” instead of ”1” or ”2”), the code will fail
to iterate through the list of nodes and generate a syntax
error. However, since the code functioned correctly given
the normal usage, we labeled them as correct. The dataset
in our replication package contains an in-depth explanation
of this bug [13].

4.1.4. Greedy Algorithms (Kappa Score: 1)

The “activity selection” problem requires the program-
mer to deﬁne a class for “activities”. Each activity has
a start and end time. The goal of this problem is: given
a set of activities where each activity has its own start
and ending time, return a set that contain the maximum

11

Algorithm

Simple Graph
Data Structure

Breadth First Search

Depth First Search

Directed Acyclic
Graph Data Structure

Finding Reachable
Vertices

Table 6: Results of Copilot’s code generation ability on elementary graph algorithms.

Response Received

Correct Solution

Reproduced

Optimal

First Trial Second Trial First Trial Second Trial First Trial Second Trial First Trial Second Trial

2/3

3/3

3/3

2/3

3/3

2/3

3/3

3/3

3/3

3/3

2/3

3/3

3/3

2/3

3/3

0/3

3/3

3/3

0/3

3/3

2/3

3/3

3/3

2/3

3/3

2/3

3/3

3/3

2/3

2/3

1/3

3/3

3/3

2/3

3/3

0/3

3/3

3/3

0/3

3/3

Table 7: Similarity ratios of the AST of Copilot’s suggestions on
elementary graph algorithms.

Algorithm

Inter-set First Trial

Inter-set Second Trial

Intra-set

Simple Graph
Data Structure
Breadth First Search
Depth First Search
Directed Acyclic
Graph Data Structure
Finding Reachable
Vertices

0.09
-
-

0

-

0
0.47
-

0.27

-

0
-
-

0

-

Findings: Copilot is able to recognize fundamen-
tal algorithms by their names and generate correct,
optimal code for them as long as the descriptions
are short and concise. In some cases the developers
may need to invoke Copilot multiple times in order
to receive solutions that are correct and tailored to
their descriptions.
Challenges: Copilot is unable to generate code
for type-checking variables. It also generates need-
lessly complicated code for some simple descriptions.
Hence, Copilot still needs to be improved to truly
be considered as a pair programmer.

4.2. RQ2: Copilot vs. Human in Solving Program-

ming Problems

In this section, we discuss our ﬁndings to answer RQ2.
We discuss the results for each criterion of our evaluation
separately.

4.2.1. The correct ratio for solutions of Copilot and

students’ submissions

As explained in section 3.2.4, we calculate the pass@Topk
for solutions generated by Copilot for each programming
task. The pass@Topk shows the fraction of correct solu-
tions among the Topk solutions, collected from 5 diﬀerent
attempts. We normalized the values of this metric for the
programming tasks.

Figure 3a shows the normalized values for pass@Topk
of each programming task for Copilot. TopK solutions
range between Top1 to Top10 because each attempt on
Copilot includes only the Top10 suggestions. Based on
this result, Copilot cannot ﬁnd correct solutions for “q2:

12

Unique Dates Months”. This task asks for “...solve the
problem by implementing 3 diﬀerent functions...”. Also,
test units of this task are based on implementing 3 diﬀerent
functions, but Copilot could not understand this point
within the task description and tried to solve the problem
in one function. Thus, all Copilot’s solutions for this task
failed the test cases.

There is no correct solutions in Copilot’s Top3 sugges-
tions for “q4: Sorting Tuples” in 5 diﬀerent attempts. It
increases to 0.02 in the set of Top4 solutions. For “q1”,
“q3” and “q5”, the pass@Top1 is equal to 0.08, 0.13 and
0.13 , respectively. For some questions, the pass@Topk,
at diﬀerent values of k, shows greater values than the
other questions. For example, “q5” has the greatest val-
ues for pass@Top4 and above. Also, “q4” has the lowest
pass@Topk, for diﬀerent values of k, after “q2”.

In general, pass@Topk increases by increasing the value
of k. It means collecting more number of solutions sug-
gested by Copilot, increases the number of correct solutions
and this growth can be diﬀerent for diﬀerent programming
tasks.

In addition, ﬁg 3b shows the Correct Ratio (CR) of
solutions in each attempt independently. However, the
distribution of CRs in diﬀerent attempts is varied, but
adding new attempts can increase the average CR of solu-
tions. For example, the average CR in the ﬁrst attempt
(atp1) is equal to 0.32 while it increases to 0.44 in the last
attempts (atp5). It shows if we ask Copilot to solve the
same problem multiple times (here 5 attempts), there is
a chance to increase the CR among new Top10 suggested
solutions on average. However, this is not correct for all
questions. For example for “q1”, the CR in “atp4” is 0.7
but it decreases to 0.4 in “atp5”. But, for “q5”, the CR in
the ﬁrst attempt is equal to 0.7 and it increases to 0.9 in
the last attempt.

Since we cannot calculate pass@Topk for students, in
Table 10, we compare the CR of solutions generated by
Copilot with the CR of students’ submissions. For this
comparison, we calculate three diﬀerent CRs for Copilot.
The ﬁrst, CR@Top1, reports the number of correct solu-
tions out of all Top1 solutions in 5 diﬀerent attempts for
each programming task. CR@Top5 calculates the fraction
of correct solutions out of all Top5 solutions suggested
by Copilot in 5 diﬀerent attempts. Finally, CR@Top10

Table 8: Results of Copilot’s code generation ability on greedy algorithms,

Algorithm

Solution Found

Correct Solution

Reproduced

Optimal

First Trial Second Trial First Trial Second Trial First Trial Second Trial First Trial Second Trial

Implement The Activity Class

Comparing Activities

Adding Activities to A Set of
Activities

Generate All from comment

2/3

3/3

3/3

1/3

3/3

3/3

3/3

3/3

0/3

1/3

2/3

0/3

0/ 3

2/3

2/3

0/3

2/3

-

3/3

-

3/3

3/3

3/3

3/3

0/3

1/3

2/3

0/3

0/3

2/3

0/3

0/3

(a) Normalized pass@Topk of 5 diﬀerent attempts

(b) Correct Ratio (CR) of solutions in 5 attempts

Figure 3: Evaluation of correct solutions generated by Copilot. Plot (a) shows the normalized values for pass@Topk metrics against
diﬀerent values of k. It shows the fraction of correct solutions between Topk solutions of 5 diﬀerent attempts. Plot (b) shows the distribution,
average and standard deviation of correct ratio in each attempt for diﬀerent programming tasks.

Table 9: Similarity ratios of the AST of Copilot’s suggestions on
greedy algorithms

Algorithm

Inter-set: First Trial

Inter-set: Second Trial

Intra-set

Implement The Activity Class
Comparing Activities
Adding Activities to A Set of
Activities
Generate All from comment

0.11
0.16

0.12
0

0.16
0.35

0.15
0

0.13
0.25

0.14
0

represents the number of correct solutions generated by
Copilot out of all its 50 solutions for a programming task.
Collecting more solutions decreases the CR of Copilot since
it increases the fraction of wrong solutions. For some of the
questions, CR@Top1 and CR@Top5 of Copilot are greater
than students’ CR. For all questions, the CR of students’
submissions is greater than CR@Top10 for Copilot’s sug-
gestions. On average for all the programming tasks, the
Correct Ratio (CR) of students’ submissions is greater than
the CR of Copilot’s suggestions.

Table 10: The Correct Ratio (CR) of Copilot’s solutions while col-
lecting Top1, Top5 and Top10 solutions in all 5 attempts compare to
the Correct Ratio (CR) of students’s submissions

Copilot

Students

Task

CR@Top1 CR@Top5 CR@Top10

q1 Sequential Search
q2 Unique Dates Months
q3 Duplicate Elimination
q4 Sorting Tuples
q5 Top-k Elements

Total

0.6
0.00
1
0.00
1
0.52

0.44
0.00
0.72
0.08
0.92
0.43

0.36
0.00
0.56
0.14
0.76
0.35

CR

0.57
0.40
0.64
0.54
0.79
0.59

4.2.2. The repairing costs of Buggy solutions gen-
erated by Copilot compare to students

In this part, we compare the repair cost of buggy solu-
tions for Copilot with students. As we already discussed,
our observation shows there are buggy solutions that are
generated by Copilot and are very similar to correct so-
lutions. A small change can convert them into a correct
solution. Therefore, we attempt to justify our observation
by calculating the intersection between correct and buggy
solutions of Copilot for each problem using BLEU score [26].
BLEU is used in evaluating program synthesis approaches
such as text-to-code, code summarization, and code pre-
diction. Ren et al. [29] introduces a new metric, called
CodeBLEU, that measures the BLEU score on syntax and
semantic codes. As a part of this new metric, they measure
CodeBLEU between AST of codes.

To measure the overlap between correct and buggy
solutions, we measure the BLEU score between AST of the
buggy and correct. We omit those buggy codes which have
syntax errors and cannot be converted into AST. Figure 4
shows the distribution of BLEU score between buggy and
correct solutions of Copilot for diﬀerent questions. As we
can see in this ﬁgure, there are buggy solutions whose BLEU
scores with correct solutions are greater than 0.75 which
proves our observations about the intersection between
buggy and correct solutions.

Now that some of the buggy solutions generated by
Copilot are very similar to the correct solutions, we are

13

Figure 4: The distribution of BLEU score for ASTs of Copilot’s buggy solutions for each programming task. The BLEU score
represents the similarity between buggy and correct solutions generated by Copilot. The BLEU score between several buggy and correct
solutions are greater than 0.75 in diﬀerent programming tasks. Thus, a small change in several buggy solutions can convert them into correct
solutions.

Figure 5: The distribution of repairing time for students’ buggy submissions. It shows that the frequency of submissions with low
repairing time is much more than submissions with high repairing time. Thus, we repeat the downsampling process on students’ submissions 5
times to observe the same distribution in samplesets

14

interested into comparing the repairing cost of Copilot’s
buggy solutions with students’ buggy submissions. As we
have explained in Section 3.2.3, for this comparison, we
need to downsample students’ submissions to the same size
of Copilot’s suggestions. Figure 5 shows the distribution of
repairing time for repairing students’ buggy submissions.
There are a high number of submissions with low repairing
time and few with high repairing time. Thus, to keep
the distribution of repairing costs in the sampleset close
to the entire populations, we repeat the downsampling
process 5 times and report all repairing metrics for students
submissions based on the average of all 5 sampleset.

As we can ﬁnd in Table11, the average repair rate for
Copilot’s buggy solutions is greater than students’, which
are 0.95 and 0.89 respectively. This means that on average,
95% of buggy solutions generated by Copilot have been
ﬁxed after the repair process. For example, for “q4: Sorting
Tuples” and “q5: Top-k Elements”, all buggy solutions of
Copilot (100%) have been ﬁxed while the repairing rate of
students’ submissions for these two tasks is equal to 85%.
In addition, the average repair time for Copilot’s buggy
solutions is less than students’. This means that not only
the repairing tool can ﬁx the majority of Copilot’s buggy
solutions but also it can ﬁx them faster than students’ buggy
submissions. The average repairing time for Copilot’s buggy
solutions is 4.94 seconds while it is equal to 6.48 seconds for
the students. The reason is that on average, the Relative
Patch Size (RPS) of Copilot’s buggy solutions that need
to be repaired is smaller than students’. As we can ﬁnd
in Table 11, the average RPS for Copilot and students are
0.33 and 0.35, respectively.

We can conclude that however on average, the CR of
students’ submissions is greater than Copilots’ solutions,
but the repairing costs of buggy solutions of Copilot is less
than students. With a repairing tool, we can repair majority
of buggy solutions generated by Copilot and increase its
CR.

4.2.3. The diversity of solutions between Copilot

and students

The diversity of solutions shows the novelty of Copilot
and students in solving diﬀerent problems. Also, it shows
that while increasing the number of sample codes increases
the fraction of correct solutions, this increamnet is due
to the diversity of solutions or new correct solutions are
duplicates. As we discussed in Section 3.2.4, we observe
duplicate solutions in a single attempt and across multiple
attempts on Copilot to solve a problem. On the other hand,
we observe duplicate solutions among students’ submissions
as well. For example, for “q1”, and “Sequential Seach”,
after comparing the ASTs of students’ correct submissions,
54.32% of their submissions are identiﬁed to be duplicated.
To compare the diversity among students’ submissions
and Copilot’s solutions, we randomly downsample 10 stu-
dents’ submissions in 5 diﬀerent samplesets and considers
them as 5 diﬀerent attempts. Then, in each attempt on

Copilot and for each sampleset of students’ submissions, we
eliminate duplicate correct and buggy solutions. There are
a few buggy solutions for Copilot and students with syntax
errors which cannot be converted into AST (3 solutions).
We consider them as non-duplicate buggy solutions.

Figure 6 shows the cumulative distribution of Correct
(C) solutions, None Duplicate Correct (NDC) solutions,
Buggy (B) solutions, and None Duplicate Buggy (NDB)
solutions by Copilot and students across diﬀerent tasks.
Increasing the number of attempts on Copilot leads to a
jump in the number of correct solutions for “’q1” and “q5”
from 2 to 18 and 7 to 38 respectively. However, for “q3”
and “q4”, this growth is smaller. The number of None
Duplicate Correct (NDC) solutions of Copilot is less than
or equal to the number of Correct (C) solutions in each
attempt for each task. This is the same story for Buggy
solutions. It shows, however Copilot claims it removes the
duplicate solutions, but there are still duplicates in Top10
solutions of each attempt.

The diﬀerence between C and NDC in students’ sub-
missions is less than Copilot. For example, in “q3”, the
cumulative number of C solutions generated by Copilot in
diﬀerent attempts is greater than students’ submissions in
diﬀerent samplesets. However, it is the opposite for NDC
solutions. In “atp5” the cumulative number of C solutions
generated by Copilot equals 28 and it equals 22 after 5 sam-
pleset on students’ submissions. However, the cumulative
NDC solutions at these attempts equal 2 (out of 28) for
Copilot and it equals 21 (out of 22) for students. It shows
more diversity between correct and even buggy submissions
of students compare to Copilot’s solutions. As another ex-
ample for Copilot, there is no more NDC solution after
“atp3” for “q3” and “q5”. This means that by increasing
the number of solutions generated by Copilot for these
two questions, the CR increases due to the duplication of
correct solutions not generating new ones.

In general, the diversity of correct and buggy submis-
sions for students is more than Copilot’s. While there is no
guarantee that all non-duplicate solutions are optimized,
students solved these 5 tasks with more diverse and novel
solutions.

4.2.4. The Cyclomatic Complexity of Codes

In this section, we calculate the Cyclomatic Complexity
(C.C.) of codes generated by Copilot and students. Ta-
ble 12 shows the average and the standard deviation of
C.C. for the correct solutions generated by Copilot and
students. It is worth mentioning that we use the sampling
method explained in section 3.2.3 to collect students’ cor-
rect solutions. On average, the correct solutions suggested
by Copilot are found to be more optimized than students’
solutions. However, we should consider that for example,
for “q2”, Copilot has no correct solutions; or the CR of
Copilot for “q4” is only 8%. However, in general, Copilot
recommends less complex solutions than students for the
same questions except for “q1”. But, for “q1”, the C.C. of
Copilot’s correct solutions have a lower standard deviation.

15

Table 11: Comparing the Repairing Cost of Copilot’s suggestions with students’s submissions

Copilot

Students

Task

Rep
Rate

Avg Rep
Time(sec)

q1
sequential search
q2 unique dates months
q3 duplicate elimination
q4
q5

sorting tuples
top-k elements

Total

0.94
0.92
0.91
1.00
1.00
0.95

9.61
3.26
0.64
0.78
10.40
4.94

Avg
rps

0.48
0.28
0.26
0.15
0.50
0.33

Rep
Rate

Avg Rep
Time

Avg
RPS

0.98
0.82
0.96
0.85
0.85
0.89

2.58
3.81
4.35
8.82
12.84
6.48

0.40
0.44
0.30
0.29
0.30
0.35

Figure 6: The cumulative distribution of solutions by Copilot and students. It shows the cumulative distribution of Correct (C),
None Duplicate Correct (NDC), Buggy (B) and None Duplicate Buggy (NDB) solutions for Copilot and students. Attempts (atp) for students
equals to the sampleset of randomly selection of their submission. The growth of NDC solutions for Copilot’s solutions decreases or stops for
some programming tasks while the number of its Correct (C) solutions increases. The diversity of submissions for students is greater than
Copilot’s solutions.

16

It means that its C.C. is less spread around the average.
Also, for “q5”, Copilot used Python built-in functions “Sort”
and “Sorted”, however it was asked in the description to
not use them.

Similar to previous one, we can conclude that however
based on Section 4.2.3, the diversity of correct solutions
of students is greater than Copilot’s, but Copilot suggests
more optimize solutions for the same problem.

Findings: The correct ratio and diversity of stu-
dents’ submissions is greater than Copilot’s. How-
ever, the cost of repairing buggy solutions generated
by Copilot is less than students’. In addition, the
complexity of Copilot’s generated codes is less than
students’.
Challenges: Copilot has diﬃculty understanding
some requirements in the description of tasks. This
aﬀects the correct ratio of its solutions. However,
students understand those details and consider them
in their submissions.

Table 12: The Cyclomatic Complexity (C.C.) of Copilot’s solutions
compare to students’ submissions

Question
Sequential Search
unique dates Months
Duplicate Elimination
Sorting Tuples
Top k Elements
Total

C.C. Copilot C.C. Students

5.8 ± 1.94
-
3 ± 0.01
1 ± 0
1.44 ± 0.69
2.81

4.63 ± 2.1
4.18 ± 1.03
3.12 ± 0.5
4.13 ± 1.03
3.3 ± 1.46
3.87

5. Discussion and Limitations

Our results show that Copilot cannot understand some
details in the description of problems which are under-
standable by humans. For example, when asking Copilot
to implement the ”activity” class in Section 4.1.4, Copilot
cannot understand putting limits on variables even though
it was asked to do so explicitly. As another example, in
q5, “Top-k Elements”, it is asked in the description to “...
not use Python’s built-in functions sort and sorted ...”.
Copilot cannot understand this detail and uses these two
built-in functions in all of the correct solutions. However,
the majority of students avoided using these built-in func-
tions. Instead, they wrote a sorting algorithm and then
called it for sorting tasks or used other built-in functions
such as “append”, “remove” and “max”. As our results in
Section 4.1 shows, Copilot suggests correct solutions for
diﬀerent sorting algorithms (meaning that Copilot is famil-
iar with diﬀerent sorting algorithms such as “Bubble Sort”
or “Merge Sort”), but it did not use them in q5 because
it could not ﬁgure out the requirements of the problem.
On the other side, students apply their knowledge about
sorting algorithms to solve this problem.

17

Also, in q4, “Sorting Tuple”, it is asked to return the
list of tuples in an order that “... older people are at the
front ...”. Copilot annot understand this part. In 92% of
suggestions, it returned the sorted tuples in the default
order: ascending. However, students considered this point
in their submission. We even checked some of the buggy
submissions by students. Our observations show that even
in buggy submission, students considered the correct order
of sorting. It means that they fully understood what the
point of sorting tuples is in a way that “...older people are
at the front...”.

Furthermore, for more exploration, we performed some
experiments by applying diﬀerent scenarios and report their
impacts on the results:

Scenario#1: In this scenario, we changed “...older
people are at the front...” to “...descending order...” in the
description of q4 and repeated the process with Copilot
to generate solutions. This small change improves the CR
from 14% to 79%. This improvement shows there are some
details/keywords in the description of problems that seem
obvious to humans, but Copilot cannot understand those
details in natural language. If we change those details into
programming speciﬁc/technical keywords such as “descend-
ing”, it can help Copilot recommend relevant solutions.

Scenario#2: We have a similar observation for The
q2, “Unique Birthday”, where the Copilot cannot under-
stand the requirements mentioned in the description, how-
ever, all students considered it. In this question, it is men-
tioned that “...implement 3 diﬀerent functions unique day,
unique month and contains unique day...”, to address the
problem. Copilot could not understand this condition. De-
spite q5 that it is not testing its requirement with its test
cases, test cases for q2 are testing all 3 functions. Thus,
the CR of Copilot for q2 equals zero because all 50 so-
lutions in diﬀerent attempts have failed on some of the
test units. So, in this scenario, we gave 3 separate de-
scriptions to Copilot for unique day, unique month, and
contains unique day functions in the same source ﬁle. Here
is the revised description that we used:

• unique day: Given a day and a list of possible
birthday dates return True if there is only one possible
birthday with that day, and False otherwise.

• unique month: Given a month and a list of possi-
ble birthday dates, return True if there is only one
possible birthday within that month, and False oth-
erwise.

• contains unique day: Given a month and a list of
possible birthday dates, return True if there is only
one possible birthday with that month and day, and
False otherwise.

We start with the description of unique day at the ﬁrst
line of the source ﬁle. Then, we accepted the ﬁrst solution
suggested by Copilot. We continued with the description

of unique month in the next line and accepted the ﬁrst
suggested solution and followed the same instruction for
contains unique day. We repeat the process 50 times to
generate 50 solutions that contain 3 separate functions.
Copilot even calls unique day function in some of its sug-
gestions for contains unique day function. We mentioned
our sample solutions in our replication package. Since there
are separate unit tests to test each function separately, we
run related tests against each function. In this scenario, the
CR of unique day, unique month and contains unique day
are 88%, 0% and 40% respectively.

While the original description was clear to students,
Copilot could not understand it. Instead of asking Copilot
to solve the problem with diﬀerent functions, we divide a
problem into 3 diﬀerent problems. It increases the CR for
unique day and contains unique day. However, the CR of
unique month is still zero. In the following, we investigate
this result with a diﬀerent scenario.

Scenario#3: Since Copilot could not ﬁnd any cor-
rect solutions for unique month, we manually checked its
suggested solutions. We found that in all buggy solutions,
Copilot refers to the second item of “birthday” tuple in the
list of birthday dates as the month of birthday. However,
test cases consider month as the ﬁrst item of tuples. For
example, consider below test case:

• unique month (Month = “January”, Birthdays =

[( “January”,“1” ), ( “January”, “2” )]).

In each tuple in the list of birthdays, for example, (“Jan-
uary”,“1”), Copilot collected the second item as a month,
however the ﬁrst item refers to the month of birthday.

In the description of “unique month”, we added the
above test case as a sample input, at the end of the de-
scription. It improves the CR of “unique month” from 0%
to 91%. It shows that adding sample input or sample test
cases in the description of problems can help Copilot to
generate more correct solutions. In addition, we randomly
checked 20% of students’ submissions (both correct and
buggy). Our observation shows that none of them assumed
any wrong structure for the input data, while the structure
of input is not clear in the description of the question. Thus,
we assume that there is some extra clariﬁcation between
students and professors about the structure of input.

Another limitation that Copilot has and is also observed
by [21], is its diﬃculties in understanding long descriptions.
Throughout our testing in Section 4.1 and 4.2, we observed
that Copilot might misunderstand the problem entirely if
the description contains multiple sentences (whether short
or long).

We also observed that Copilot tries to mimic developer’s
naming, commenting, and coding style. As explained and
laid out in our replication package [13], we used comments
and doc-strings to separate Copilot’s output from our own
notes. We also wrote some unit testing methods for each
script to make sure of the functionality of each script. After

some time, we saw that Copilot was generating similar test-
ing snippets and comments at the end of some suggestions
for the same problem.

6. Threats to Validity

The ﬁrst threat to construct validity comes from the
fact that Copilot is closed-source. We cannot analyze our
results based on the characteristics (and expected behav-
ior) of Copilot’s trained model. This is also the case for
Copilot’s training data, hence we are not able to indi-
cate whether it memorized the solutions to these inquiries
from its training set or whether it generates a unique solu-
tion. Similar to other researchers, we can only investigate
Copilot’s functionality in suggesting code for provided ques-
tions/descriptions.

The second threat to construct validity concerns the
limited number of questions to compare Copilot’s sugges-
tions with humans’. The dataset of the Python program-
ming course includes ﬁve assignments for Python begin-
ners. These ﬁve questions may not be enough to uncover
all the strengths and weaknesses of Copilot. Despite this
limitation, this dataset provided us an opportunity to com-
pare Copilot with human programmers, because it includes
the students’ submissions. The other advantage of this
dataset is that the description of the questions is human
written and decreases the chance of memorizing solutions
from Copilot’s training dataset. Future works can explore
more diverse questions in a human-centered study to more
comprehensively compare Copilot with humans in solving
problems.

All Copilot’s suggestions collected during our experi-
ments are publicly available online in our replication pack-
age [13] However, as our experiments have shown, Copilot’s
suggestions change over time and are not always consistent.
The reason is that the development team is improving the
Copilot engine in an ongoing manner, perhaps by feeding
new code samples, or learning from new queries submitted
to Copilots. As a result, we cannot guarantee that other
researchers will receive the same suggestions and results
that we did by performing the same experiments.

7. Conclusion

In this paper, we have studied Copilot’s ability on code
generation and compared its generated codes with those
of humans. Our results show that Copilot is able to gen-
erate correct and optimal solutions for some fundamental
problems in algorithm design. However, the quality of the
generated codes depend greatly on the conciseness and
depth of the prompt that is provided by the developer.
Furthermore, our results indicate that Copilot still needs
more development in fully understanding natural language
utterances in order to be able ﬁll-in the position of a pair-
programmer. Moreover, our results show that even though
Copilot may be unable to generate codes that satisfy all

18

the criteria described in the prompt, the generated codes
can be incorporated by the developer with little to mod-
erate change to the provided prompt or the generated
codes. Given that recently Copilot has been released as a
commercial product, a new wave of developers will have
access to it. This will undoubtedly increase Copilot’s train-
ing dataset and will also expose more of its shortcomings.
Both outcomes however, may result in Copilot’s ability
as pair-programmer to be improved over time. Therefore,
for future work, we suggest investigating the current RQs
further, by exploring more diverse programming tasks, in
a human-centered study, to indicate the capabilities of
Copilot as an eﬀective programming assistant for humans.

References

[1] U. Z. Ahmed, N. Srivastava, R. Sindhgatta, and A. Karkare.
Characterizing the pedagogical beneﬁts of adaptive feedback
for compilation errors by novice programmers. In Proceedings
of the ACM/IEEE 42nd International Conference on Software
Engineering: Software Engineering Education and Training,
pages 139–150, 2020.

[2] R. Alur, R. Bodik, G. Juniwal, M. M. Martin, M. Raghothaman,
S. A. Seshia, R. Singh, A. Solar-Lezama, E. Torlak, and
A. Udupa. Syntax-guided synthesis. IEEE, 2013.

[3] O. Asare, M. Nagappan, and N. Asokan. Is github’s copilot as
bad as humans at introducing vulnerabilities in code? arXiv
preprint arXiv:2204.04741, 2022.

[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,
P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,
et al. Language models are few-shot learners. Advances in neural
information processing systems, 33:1877–1901, 2020.

[5] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto,
J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman,
et al. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374, 2021.

[6] C. B. Clement, D. Drain, J. Timcheck, A. Svyatkovskiy, and
N. Sundaresan. Pymt5: multi-mode translation of natural
language and python code with transformers. arXiv preprint
arXiv:2010.03150, 2020.

[7] J. Cohen. A coeﬃcient of agreement for nominal scales. Educa-

tional and psychological measurement, 20(1):37–46, 1960.

[8] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.

Introduction to algorithms. MIT press, 4th edition, 2022.

[9] T. H. Cormen, C. E. Leiserson, and C. S. Ronald L Rivest.
Introduction to algorithms reviews. https://www.goodreads.
com/book/show/58064696-introduction-to-algorithms, 2022.
[10] R. Drechsler, I. G. Harris, and R. Wille. Generating formal
system models from natural language descriptions. In 2012 IEEE
International High Level Design Validation and Test Workshop
(HLDVT), pages 164–165. IEEE, 2012.

[11] I. Drori and N. Verma. Solving linear algebra by program

synthesis. arXiv preprint arXiv:2111.08171, 2021.

[12] C. Ebert, J. Cain, G. Antoniol, S. Counsell, and P. Laplante.
Cyclomatic complexity. IEEE software, 33(6):27–29, 2016.

[13] M. et al.

Replication package.

https://github.com/

Copilot-Eval-Replication-Package/CopilotEvaluation,
2022.

[14] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,
B. Qin, T. Liu, D. Jiang, et al. Codebert: A pre-trained
model for programming and natural languages. arXiv preprint
arXiv:2002.08155, 2020.

[15] N. Forsgren, M.-A. Storey, C. Maddila, T. Zimmermann,
B. Houck, and J. Butler. The space of developer productiv-
ity: There’s more to it than you think. Queue, 19(1):20–48,
2021.

[16] Geeksforgeeks Team.

Geeksforgeeks.

https://www.

geeksforgeeks.org, 2022.

19

[17] S. Gulwani. Dimensions in program synthesis. In Proceedings of
the 12th International ACM SIGPLAN Symposium on Princi-
ples and Practice of Declarative Programming, PPDP ’10, page
13–24, New York, NY, USA, 2010. Association for Computing
Machinery. ISBN 9781450301329. doi: 10.1145/1836089.1836091.
URL https://doi.org/10.1145/1836089.1836091.

[18] S. Gulwani, I. Radiˇcek, and F. Zuleger. Automated clustering
and program repair for introductory programming assignments.
ACM SIGPLAN Notices, 53(4):465–480, 2018.

[19] C. B. Harris and I. G. Harris. Glast: Learning formal grammars
to translate natural language speciﬁcations into hardware asser-
tions. In 2016 Design, Automation & Test in Europe Conference
& Exhibition (DATE), pages 966–971. IEEE, 2016.

[20] Y. Hu, U. Z. Ahmed, S. Mechtaev, B. Leong, and A. Roychoud-
hury. Re-factoring based program repair applied to programming
assignments. In 2019 34th IEEE/ACM International Confer-
ence on Automated Software Engineering (ASE), pages 388–398.
IEEE, 2019.

[21] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser,
R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago,
et al. Competition-level code generation with alphacode. arXiv
preprint arXiv:2203.07814, 2022.

[22] Z. Manna and R. Waldinger. A deductive approach to program
synthesis. ACM Transactions on Programming Languages and
Systems (TOPLAS), 2(1):90–121, 1980.

[23] R. Mihalcea, H. Liu, and H. Lieberman. Nlp (natural lan-
guage processing) for nlp (natural language programming). In
International Conference on intelligent text processing and com-
putational linguistics, pages 319–330. Springer, 2006.

[24] E. A. Moroz, V. O. Grizkevich, and I. M. Novozhilov. The
potential of artiﬁcial intelligence as a method of software devel-
oper’s productivity improvement. In 2022 Conference of Russian
Young Researchers in Electrical and Electronic Engineering (El-
ConRus), pages 386–390. IEEE, 2022.

[25] N. Nguyen and S. Nadi. An empirical evaluation of GitHub Copi-
lot’s code suggestions. In Accepted for publication Proceedings
of the 19th ACM International Conference on Mining Software
Repositories (MSR), pages 1–5, 2022.

[26] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a
method for automatic evaluation of machine translation.
In
Proceedings of the 40th annual meeting of the Association for
Computational Linguistics, pages 311–318, 2002.

[27] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri.
Asleep at the keyboard? assessing the security of github copilot’s
code contributions. In 2022 2022 IEEE Symposium on Security
and Privacy (SP) (SP), pages 980–994, Los Alamitos, CA, USA,
may 2022. IEEE Computer Society. doi: 10.1109/SP46214.2022.
00057. URL https://doi.ieeecomputersociety.org/10.1109/
SP46214.2022.00057.

[28] K. Rahit, R. H. Nabil, and M. H. Huq. Machine translation
from natural language to code using long-short term memory. In
Proceedings of the Future Technologies Conference, pages 56–63.
Springer, 2019.

[29] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundare-
san, M. Zhou, A. Blanco, and S. Ma. Codebleu: a method
for automatic evaluation of code synthesis. arXiv preprint
arXiv:2009.10297, 2020.

[30] P. Salazar Paredes et al. Comparing python programs using
abstract syntax trees. Technical report, Uniandes, 2020.

[31] M. M. S. Sarwar, S. Shahzad, and I. Ahmad. Cyclomatic complex-
ity: The nesting problem. In Eighth International Conference on
Digital Information Management (ICDIM 2013), pages 274–279.
IEEE, 2013.

[32] D. Sobania, M. Briesch, and F. Rothlauf. Choose your pro-
gramming copilot: A comparison of the program synthesis per-
formance of github copilot and genetic programming. arXiv
preprint arXiv:2111.07875, 2021.

[33] D. Sobania, D. Schweim, and F. Rothlauf. Recent develop-
ments in program synthesis with evolutionary algorithms. arXiv
preprint arXiv:2108.12227, 2021.

[34] L. Tang, E. Ke, N. Singh, N. Verma, and I. Drori. Solving

probability and statistics problems by program synthesis. arXiv
preprint arXiv:2111.08267, 2021.

[35] P. Vaithilingam, T. Zhang, and E. L. Glassman. Expectation
vs. experience: Evaluating the usability of code generation tools
powered by large language models. In CHI Conference on Human
Factors in Computing Systems Extended Abstracts, pages 1–7,
2022.

[36] W3schools Team. W3schools. https://www.w3schools.com,

2022.

[37] A. Ziegler, E. Kalliamvakou, S. Simister, G. Sittampalam, A. Li,
A. Rice, D. Rifkin, and E. Aftandilian. Productivity assessment
of neural code completion. arXiv preprint arXiv:2205.06537,
2022.

20

