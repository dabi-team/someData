Noname manuscript No.
(will be inserted by the editor)

A Comparative Study of Application-level Caching
Recommendations at the Method Level

Rˆomulo Meloca · Ingrid Nunes

2
2
0
2

l
u
J

0
3

]
E
S
.
s
c
[

1
v
2
3
2
0
0
.
8
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Performance and scalability requirements have a fundamental role in
most large-scale software applications. To satisfy such requirements, caching is
often used at various levels and infrastructure layers. Application-level caching—
or memoization—is an increasingly used form of caching within the application
boundaries, which consists of storing the results of computations in memory to
avoid re-computing them. This is typically manually done by developers, who iden-
tify caching opportunities in the code and write additional code to manage the
cache content. The task of identifying caching opportunities is a challenge because
it requires the analysis of workloads and code locations where it is feasible and
beneﬁcial to cache objects. To aid developers in this task, there are approaches that
automatically identify cacheable methods. Although such approaches have been
individually evaluated, their eﬀectiveness has not been compared. We thus in this
paper present an empirical evaluation to compare the method recommendations
made by the two existing application-level caching approaches at the method level,
namely APLCache and MemoizeIt, using seven open-source web applications. We
analyse the recommendations made by each approach as well as the hits, misses
and throughput achieved with their valid caching recommendations. Our results
show that the eﬀectiveness of both approaches largely depends on the speciﬁc ap-
plication, the presence of invalid recommendations and additional conﬁgurations,
such as the time-to-live. By inspecting the obtained results, we observed in which
cases the recommendations of each approach fail and succeed, which allowed us to
derive a set of seven lessons learned that give directions for future approaches to
support developers in the adoption of this type of caching.

Keywords Software Performance · Caching · Application-level Caching ·
Memoisation · Empirical Study · Web Applications

R. Meloca and I. Nunes
Instituto de Inform´atica, Universidade federal do Rio Grande do Sul
Porto Alegre, Brazil
E-mail: {rmmeloca,ingridnunes}@inf.ufrgs.br

 
 
 
 
 
 
2

1 Introduction

Rˆomulo Meloca, Ingrid Nunes

Many software systems that rely on the Internet—such as web applications and
those in the context of Internet of Things and cloud computing—commonly have
to meet increasing non-functional requirements. These include reduced response
time and the ability to deal with a high number of requests. To support a growing
number of simultaneous requests, developers must ﬁnd a means of speeding up
computations or improve the underlying hardware. Even though the scaling up
of hardware is frequently an eﬀective way to address performance and scalabil-
ity requirements, it incurs in ﬁnancial costs that can grow exponentially (Abbott
and Fisher, 2009). Consequently, developers must ﬁnd alternatives to satisfy such
requirements. For this purpose, caching is typically used at various levels and
application infrastructure to reduce the response time. Although some forms of
caching are transparent to application developers, such as by using a proxy server,
a recent form of caching, namely application-level caching, has been adopted in a
complementary way to other forms of caching by exploiting speciﬁcities of indi-
vidual applications (Mertz and Nunes, 2017b).

Application-level caching, or memoization, is the use of caching within the
boundaries of a software application. It requires developers to identify opportu-
nities where the result of an already computed value can be reused and to add
additional code to manage cached objects. This type of caching can be employed,
for instance, at the database layer (Scully et al., 2017), client-side (Huang et al.,
2010), or within the business logic (Della Toﬀola et al., 2015; Mertz and Nunes,
2018). Wherever such caching opportunities reside, it is not a trivial task to ﬁnd
them because, not only spots where computed values that are eligible to be reused
must be found, but also the application workload must be inspected to identify val-
ues that are frequently accessed or take a long time to compute. Moreover, as the
workload evolves, caching opportunities must be constantly revised to avoid perfor-
mance decay. Consequently, this is a time-consuming and error-prone task (Mertz
and Nunes, 2017a), and developers beneﬁt from supporting solutions.

Diﬀerent approaches have been proposed to help developers employ caching
at the application level. Some of them focus on providing recommendations of
cacheable spots (Xu, 2012, 2013; Nguyen and Xu, 2013; Maplesden et al., 2015b;
Della Toﬀola et al., 2015; Chen et al., 2018), while others aim to automate the
identiﬁcation and management of cached content at this level (Mertz and Nunes,
2018; Wang et al., 2014). In many cases, these approaches are complementary be-
cause they target diﬀerent granularity levels—complex objects have been examined
by four approaches (Xu, 2012, 2013; Maplesden et al., 2015b; Chen et al., 2018),
while three works focus on identifying methods to be cached (Nguyen and Xu, 2013;
Della Toﬀola et al., 2015; Mertz and Nunes, 2018).

Our work focuses in particular on approaches that identify cacheable methods
because by knowing which methods to cache, developers can improve the applica-
tion performance without major changes in the source code. Nguyen and Xu (2013)
were the ﬁrst to explore this performance opportunity. However, they examined
only recurrent method outputs, without assessing neither whether the method is
feasible to be cached (i.e. produce the same output for the same input) nor if it
would provide beneﬁts if cached. There are only two approaches that aim to do
this, namely APLCache (Mertz and Nunes, 2018) and MemoizeIt (Della Toﬀola

A Comparative Study of Application-level Caching

3

et al., 2015). Although they have been individually evaluated, there is a lack of
studies that compare them.

We thus in this paper fulﬁl this gap and present an empirical study that
compares APLCache and MemoizeIt. Our goal is to compare their suggestions
of cacheable spots and evaluate their eﬀectiveness in terms of hits, misses and
throughput, using seven open-source projects of web applications written in Java.
All selected applications include caching manually implemented by developers. Our
study has a two-phase procedure. First, we collect execution traces of each appli-
cation based on a “learning” workload. These traces are given as input for both
application-level caching approaches. As result, we obtain the recommendations
made by APLCache and MemorizeIt. Each recommendation is then inspected for
validity. Second, we execute “testing” workloads with four versions of each appli-
cation: (i) application with the caching made by developers; (ii) application with
valid recommendations made by APLCache; (iii) application with valid recom-
mendations made by MemoizeIt; and (iv) application with no caching.

Our results show that both approaches are able to discover opportunities diﬀer-
ent from those made by the developers. However, suggested opportunities must be
carefully inspected because they include methods that will lead to bugs if cached.
Considering the valid recommendations, both approaches are able to identify op-
portunities that generate hits. With respect to hits, misses and throughput, we
observed diverging results across the diﬀerent applications as well as number of
simultaneous users. By qualitatively analysing the collected data, we identiﬁed
the strengths and weaknesses of each approach. This knowledge is the basis for
further improvements to be made in both APLCache and MemoizeIt, or even for
developing future approaches.

Our key contributions are the following: (i) a study protocol for evaluating and
comparing application-level caching approaches at the method level; (ii) an empir-
ical comparison between APLCache and MemoizeIt; and (iii) seven implications of
our study that give directions for improving existing approaches and key aspects
that remain unaddressed.

We next provide a background on application-level caching as well as overview
work on this topic, providing details of the compared approaches. We describe in
Section 3 our study design and details of the target web applications. In Section 4,
we present and analyse the results of our study. Lessons learned and threats to
validity are discussed in Section 5. Finally, conclusions are presented in Section 6.

2 Background and Related Work

Much work has been done to support developers to improve the performance
of software applications. There are approaches that focus on code optimisation.
They propose, for example, rearranging instructions in the source code to avoid
the repeated instantiation of objects inside loops (Xu et al., 2012) or ineﬃcient
recurrent JavaScript instructions (Selakovic and Pradel, 2016). Other approaches
aim to prioritise performance optimisation opportunities of a method (Chen et al.,
2018) or improve the way that proﬁlers show provided results by better ranking
and aggregating them (Maplesden et al., 2015b,a). Our work focuses on the use
of caching at the application level to improve the application performance. In this
section, we introduce background on this topic and discuss supporting approaches

4

1
2
3
4
5
6
7
8
9
10
11
12
13

class AccountsManager {

Rˆomulo Meloca, Ingrid Nunes

Map<Account, Double> cache
= new HashMap<>();

AccountsDAO accountsDAO

= new AccountsDAO();

Double getBalance(Account account) {

return cache.

computeIfAbsent(account, key

-> accountsDAO.retrieveBalance());

}

}

Listing 1 Example of a caching at the application level.

(Section 2.1), and then provide details of the two approaches compared in our
study (Section 2.2).

2.1 Application-level Caching

Application-level caching (Mertz and Nunes, 2017b) consists of the use of caching
for storing the results of computations (such as of a method or a set of instructions)
to be reused in the future. This is often manually implemented in the source code,
such as in the example presented in Listing 1. In this example, a Map is used to cache
the result of the getBalance() method, which retrieves data from the database only
if it is not cached. Alternatively to a local map, cache components can be used,
such as Redis or Caﬀeine. In addition to deciding what to cache (Mertz and Nunes,
2018), developers must also deal with, e.g., the invalidation of cached content in
case of changes (Ghandeharizadeh et al., 2012) and a deﬁnition of a time-to-live
(TTL) (Alici et al., 2012). Caching opportunities in the code are computations
that result in the same output for the same input at least for a period of time,
and do not involve statements that cannot be skipped (e.g. a writing operation).
Caching these computations saves computation time, thus providing performance
gains and helping increase scalability, if they have high cost to compute and/or are
frequently executed. Manually ﬁnding and managing these caching opportunities
is a challenge and, therefore, approaches have been proposed to support this.

Because application-level caching is a cross-cutting concern, imposing main-
tainability challenges, there are approaches that help managing the cache compo-
nent. In this direction, there is work that (1) searches for the best way to opti-
mise memory allocation (Radhakrishnan, 2004; Ghandeharizadeh et al., 2015); (2)
improves admission or replacement algorithms (Qin et al., 2014; Venketesh and
Venkatesan, 2009; Ali Ahmed and Shamsuddin, 2011; Yang and Zhang, 2003; Sae-
mundsson et al., 2014; Santhanakrishnan et al., 2006; Ali et al., 2012); (3) provides
adaptive caching policies (Alici et al., 2012; Subramanian et al., 2006; Megiddo and
Modha, 2004); or (4) automatically manages the caching at the server side (Ports
et al., 2010). In addition, there are approaches (Xu et al., 2014; Zaidenberg et al.,
2015; Hwang and Wood, 2013) that focus on addressing issues of a speciﬁc cache
component, namely Memcached. Given that caching opportunities are often lo-
cated at the application boundaries, there are approaches that focus on speciﬁc
locations to cache. Most of them deal with various issues associated with caching
at the data layer (Ghandeharizadeh et al., 2012; Sun et al., 2017; Leszczy´nski and
Stencel, 2010; Scully et al., 2017; Wang et al., 2014; Gupta et al., 2011; Chen et al.,

A Comparative Study of Application-level Caching

5

2016; Larson et al., 2004). However, there are approaches that focus on caching
content at the presentation layer (Huang et al., 2010; Candan et al., 2001) or deal
speciﬁcally with ﬁles (Guo and Engler, 2011).

With respect to the identiﬁcation of caching opportunities, there is work that
targets data structures and methods. Approaches that focus on the former search
for reusable data structures (Xu, 2012; Nguyen and Xu, 2013) or disposed ob-
jects (Xu, 2013), helping reduce the memory consumption. The work by Nguyen
and Xu (Nguyen and Xu, 2013) also has a component that target methods, but
it is limited to the identiﬁcation of methods that produce recurrent results (re-
gardless of the input) and does not assess the beneﬁt of caching. We refer to work
in this direction as method-level approaches (Della Toﬀola et al., 2015; Mertz and
Nunes, 2018), which aim at identifying methods that result in outputs that are
possible to cache (i.e. produce the same output for the same input for a certain
time frame) and would provide performance improvements if cached. Our study
focuses on comparing these method-level approaches, and they are further detailed
in next section.

2.2 Compared Method-level Approaches

From the existing application-level caching approaches introduced above, only two
explore methods inputs and outputs to identify those that are possible to be cached
and/or provide beneﬁts if cached. These approaches are APLCache (Mertz and
Nunes, 2018) and MemoizeIt (Della Toﬀola et al., 2015). Both of them rely on the
runtime analysis of the application execution considering a particular workload.
They involve algorithms, metrics, and heuristics to suggest cacheable methods.
In the following sections, we provide an overview of how each approach works,
describing the rationale for their suggestion in a high-level way. For a complete
understanding of the details of each approach, we refer the reader to their original
publications.

2.2.1 APLCache

Mertz and Nunes (2018) proposed an approach to automate the use of application-
level caching. It is implemented as a framework, named APLCache, to be seam-
lessly integrated to web applications. The framework monitors the application at
runtime collecting execution traces, identiﬁes cacheable spots, and manages con-
tent to be cached. The decision of what to cache is based on the Cacheability
Pattern, which was derived from a qualitative study (Mertz and Nunes, 2017a) of
application-level caching in web applications. The implementation of the pattern
consists of the deﬁnition of ﬁve metrics—staticity, changeability, frequency, share-
ability, and expensiveness—that are used to select content to be cached. These
metrics are used to select methods that, in addition to producing the same out-
puts for the same inputs for certain periods of time, have some of the following
properties: (i) are frequent; (ii) their results are shared among multiple users; and
(iii) are expensive to compute. Generally, these metrics are assessed for all meth-
ods, and the mean and standard deviation are used as the basis to make decisions.
A particularity of this approach is that it does not recommend to cache all pairs of
inputs and output of a selected method, but only those that satisfy the properties

6

Rˆomulo Meloca, Ingrid Nunes

introduced above. To compare and store inputs and outputs, this approach seri-
alises objects. APLCache has been evaluated using three web applications, using
as baseline the original application (which includes the caching made by develop-
ers) and the application with no caching. The module of APLCache that chooses
cacheable methods can be used as a recommender, which is considered in our
study.

2.2.2 MemoizeIt

A recommender of cacheable methods, named MemoizeIt, has been proposed by
Della Toﬀola et al. (2015). Their approach consists of four steps: time and fre-
quency proﬁling, input-output proﬁling, clustering and ranking, and suggest cache
implementation. This ﬁrst step discards methods that are called only once or are
not expensive to compute (according to a given threshold). Then, the remaining
candidates are reﬁned, discarding every method that has any diﬀerent output for
the same input. To reduce the computation time of this step, it is possible to ex-
ecute it either in the exhaustive or the iterative mode. The former compares the
whole object reference tree, while the latter partially compares inputs and outputs,
truncating them at some point. The result of this step is a set of methods that are
feasible to be cached, which are ranked according to their potential saved com-
putation time. The third step aggregates methods that are called in a sequence,
based on the analysis of the call graph of the application. Finally, the last step
suggests how to implement the caching of objects. The implementation can vary
in size, storing a single or multiple instances of the results of a method, and in
scope, which can be an instance or global (static) map. MemoizeIt was evaluated
with eleven single-threaded programs.

APLCache and MemoizeIt have two key diﬀerences. First, when MemoizeIt
points out a method to be cached, any pair of inputs-output is cached if invoked.
In contrast, APLCache selects speciﬁc pairs to be cached. Second, APLCache ad-
mits caching the result of a computation based on a set of inputs even if the
results vary in a certain time frame. In certain applications, stale data are al-
lowed for a short period of time in these cases, or the cached object is invalidated.
MemoizeIt, in turn, only recommends methods that have invariant results for a
set of inputs. Note that both approaches consider only the method inputs and
output, not analysing if there are instructions within the method that cannot be
skipped. MemoizeIt is used as a recommender and thus suggestions must be man-
ually inspected. APLCache, in turn, acknowledges this problem and suggests that
developers annotate uncacheable methods that fall in this situation. However, this
requires manually analysing all application methods, which is a time-consuming
and error-prone activity.

3 Study Settings

Given that we introduced the two approaches that are compared in our work, we
now proceed to the description of our study settings.

A Comparative Study of Application-level Caching

7

3.1 Research Questions

The goal of our work is to evaluate and compare method recommendations made
by two application-level caching approaches. In order to achieve this goal, we focus
on answering two research questions, detailed as follows.

– RQ1: What are the diﬀerences between the recommendations made by APLCache

and MemoizeIt?

– RQ2: What are the performance improvements provided by the valid caching

recommendations of each approach?

With RQ1, we aim to analyse the recommendations made by each approach
and compare them with each other and the caching decisions made by developers.
With RQ2, we focus on assessing the improvements provided by the caching recom-
mendations in term of throughput, hits and misses, which are the typical metrics
used to evaluate caching. We use as baseline applications with no caching and the
caching implemented by developers. Hereafter, we refer to APLCache, MemoizeIt,
and the developers’ implementation as APL, MEM, and DEV, respectively.

3.2 Procedure

To answer our research questions, we speciﬁed a two-phase procedure, which is
overviewed in Figure 1. In phase 1, we ﬁrst generate a synthetic workload in a
systematic way and execute a set of web applications with no caching and collect
their execution traces. Not only is a real workload typically not freely available,
but also it would not provide beneﬁts to our study. The goal is to assess the ability
of application-level caching approaches to learn usage patterns. Consequently, they
should be able to learn any typical usage pattern. The collected execution traces are
then provided as input for APL and MEM to generate caching recommendations.1
As they can be invalid, we inspect them discarding those that cannot be cached.
Next, in phase 2, we implement three versions of the application, caching methods
based on APL, MEM, and DEV. These versions as well as the application with
no caching are executed using another synthetic workload, but following the same
set of probabilities of navigation of the previous workload so that there is a usage
pattern. Finally, we collect metrics to be analysed.2 We next provide details of
each step of our study.

3.2.1 Phase 1

Application Adaptation and Workload Generation. We selected a set of tar-
get applications (described in next section) that contain on its implementation
cached methods. These applications were modiﬁed by removing all caching snip-
pets, creating their uncached version, which we refer to as NOCACHE. To be

1 Note that if new application-level caching approaches are proposed to recommend methods
to cache, the same procedure can be used. The only requirement is to use the recommendations
made by the new approach. We provide details on how to reproduce the study in Appendix A
for those who would like to use our infrastructure to conduct similar studies.

2 The source code of our study is available for reproducibility at http://inf.ufrgs.br/

prosoft/resources/2021/emse-apl-caching-comparison.

8

Rˆomulo Meloca, Ingrid Nunes

Fig. 1 Overview of the study procedure.

able to execute the applications, we automatically generated data for those that
do not have a database dump available. Moreover, we manually inspected each
application and generated a navigation structure stored in a JSON ﬁle. This ﬁle
contains the information of two graphs GN = (V, EN ), which gives the possible
HTTP requests that can be done after another, and GR = (V, ER), which gives
the HTTP requests that are required to be executed before another. Each vertex
v ∈ V represents a HTTP request. Each (v, u) ∈ EN indicates that the HTTP
request u is allowed to be subsequently invoked after the HTTP request v accord-
ing to the business rules of the application. Each (v, u) ∈ ER indicates that the
HTTP request v must have been executed before the HTTP request u is invoked.
For example, a product can be updated only after being visualised. The execution
of v guarantees that, e.g., required attributes are stored in the local session of
the application. A synthetic workload is then generated by selecting a sequence
of requests that are in accordance with the navigation structure in the JSON ﬁle.
A possible next request is randomly selected based on the previous requests. This
selection also considers the proportion of 80% of read-only requests (GET) and
20% of write requests (POST, PUT and DELETE), which is in accordance with
the TPCW3 benchmark and used in previous work (Mertz and Nunes, 2018). The
generation of each request contains the required headers and form data.

Collection of Execution Traces. To have a set of execution traces, we use the
default Java implementation for basic HTTP requests and Apache implementation
to multipart requests in our simulator. We sent requests for ﬁve minutes for each
application. We used a dedicated machine to simulate the requests (Intel Core i5
processor and 8 Gb of main memory) and another to host the applications (Intel
Core i7 processor and 32 Gb of main memory). Using AspectJ4, we intercepted
every method call, recording it in a ﬁle with JSON objects. Each trace contains the
time spent to compute (start and end times), as well as the HTTP user session, the
method signature, input parameters and the produced result. In order to serialise
parameters and the return of methods to JSON, we adapted JSON-java5 to break
cycles and to truncate packages not related to the application. We serialised each
ﬁeld of objects, recursively, instead of its getters. To decide whether a ﬁeld should
be truncated during the serialisation we consulted the POM ﬁle searching for
a dependency that matches the package name of the ﬁeld. Fields belonging to

3 http://www.tpc.org/tpcw
4 https://www.eclipse.org/aspectj
5 https://github.com/stleary/JSON-java

APLCache1#m(0,{ ... })2#m(0,{v:0})MemoizeItAnalyseRecommendationsPhase 2CacheMetricsPhase 1AppsexecAppsexec{}{}{}TracesUncachedA Comparative Study of Application-level Caching

9

internal Java packages are converted to a string (String.valueOf()) instead of
being recursively explored.

Recommendation Generation and Analysis. APL and MEM had to be
adapted to generate caching recommendations using our traces. For APL, we
had to extract from its framework the component that decides what to cache.
For MEM, we implemented it using the original implementation as a basis, which
could not be used because it consists of an instrumentation in the source code to
collect information at runtime. We, instead, provide as input a previously gener-
ated set of traces that had to be processed. Our implementation includes both
the iterative and exhaustive kernels. Moreover, MEM requires a callgraph of the
application, and we use java-callgraph6 for this purpose. For deserialising traces,
we use Gson7. APL and MEM include a set of thresholds, for which we use the
same values adopted by their authors. These thresholds are not required to be
tuned for speciﬁc applications, They are general setups to give semantics for cer-
tain concepts of the approaches. For example, APL classiﬁes methods as frequent
when they are k standard deviations above the average. MEM indicates with a
threshold when a method executes so fast that it should not even be considered
for caching. We thus used the threshold conﬁgurations proposed by the authors.
By running the compared approaches, we collected their respective sets of recom-
mendations. Each recommendation was manually analysed—taking into account
the method and its context—to verify if it does not contain any command that
must always be executed when the method is invoked. That is, if the method is
cached, the application would result in a failure. Common circumstances in which
a recommendation is invalid are when the recommended method: (a) performs a
writing operation in the database; (b) makes a request or sends a message to an
external service; (c) writes into a ﬁle; (d) changes data stored in static ﬁelds; or
(e) directly or indirectly manipulates the parameters given in its invocation. This
veriﬁcation was made by a careful manual analysis of the applications, comple-
mented by the use of available unit tests. Recommendations that break unit tests
are also considered invalid.

3.2.2 Phase 2

Caching Implementation. Using Caﬀeine8 as a cache component, we created
three versions of the application in addition to its uncached version. We imple-
mented an APL and a MEM versions, using the valid recommendations of each
approach. While for MEM every call to a cached method is searched in the cache
component or stored in it, for APL only calls with particular inputs are cached.
Therefore, before storing an entry in the cache component, we must verify if the
corresponding input is an APL recommendation. We also replaced the original
caching done by developers in the DEV version by using a standardised cache
component and to be able to collect metrics that serve as baseline. Despite this
change, the developers’ caching decisions remained the same. Finally, for all cached
versions, we conﬁgured the cache component with no size limit, in order to avoid
inﬂuencing the results by the choice of a cache replacement policy that would au-

6 https://github.com/gousiosg/java-callgraph
7 https://github.com/google/gson
8 https://github.com/ben-manes/caffeine

10

Rˆomulo Meloca, Ingrid Nunes

tomatically evict objects. Having a particular size limit could cause misses that
are not due to choice of cached methods, favouring one of the approaches. Cached
objects leave the cache when their TTL expires. We adopted the TTL values used
by developers.

Execution and Metrics Collection. In order to measure the performance of
each version of the application (NOCACHE, APL, MEM, and DEV) under the
same conditions, we generated the workloads for each application based on their
NOCACHE version and we ran it ten times on every version. In the generation the
allowed random values and the chances of performing read or write operations—as
discussed in Phase 1—requests are chosen and logged to be exactly the same for the
next ten executions of each application version and number of simulated users.We
simulated 1, 5, and 25 simultaneous users—similarly to previous work (Mertz and
Nunes, 2018)—ﬁring requests during 10 minutes. In order to validate the number
of users that should be simulated and duration of the simulation, we ran previous
tests and observed no large diﬀerences in the behaviour of the applications and
the cache component when simulating 5, 10, or 50 users. Moreover, we also found
in previous testings that there is no need to collect data from 10 to 25 minutes of
execution because there is also no behavioural change in such a period. This occurs
because the workload follows a particular overall usage pattern (as it is expected in
web applications that aim at exploiting application-level caching) and the caching
recommendations are generated for that particular pattern. These workloads were
then used to execute each version of each application 10 times to measure the
throughput (requests handled per second). An additional execution was performed
to assess cache performance metrics. The cache performance metrics are mainly
hits and misses, from which hit-ratio can be derived. Moreover, as APL does not
necessarily put an object in the cache each time that there is miss (because it
caches only particular inputs), we also collected the number of cache additions and
discarded inputs.

Based on the caching recommendations and the collected metrics, we compare
APL and MEM. Note that, although DEV is used as a baseline, the caching
decisions made by developers cannot be directly compared with APL and MEM
because developers made decisions based on their production environment. This is
not an issue to compare APL and MEM because they generate recommendations
for a given workload, regardless if it is synthetic or collected from a production
environment. Consequently, DEV is used in our study as a reference and the
developers’ decisions are not in-depth discussed.

3.3 Target Applications

For selecting our target web applications, we retrieved from GitHub the 1,000 most
popular Java repositories according to stars. From these, we discarded those that
do not contain a POM ﬁle that indicate at least one dependency to one of the
common caching providers, namely Spring9, Redis10, Ehcache11, Memcached12

9 https://spring.io
10 https://redis.io
11 https://www.ehcache.org
12 https://memcached.org

A Comparative Study of Application-level Caching

11

Table 1 Target applications.

Application

Domain

LOC #Files GitHub Stars

Azkaban 3.70.1
Cloudstore 2.0.0

Keycloak 5.0.0

Killbill 0.20.9

Petclinic 1.5.0

Shopizer 2.2.0

A workﬂow manager
An e-commerce application
based on TPC-W bench-
mark
A manager for identity and
access
A subscription billing and
payment platform
A sample Spring-based ap-
plication
A customisable e-commerce
application

Thingsboard 2.3.0 An IoT platform

77,484
11,228

473
98

395,388

3,257

134,279

1,018

1,566

95,363

74,180

25

797

848

2,125
16

2,997

1,788

2,704

2,105

2,182

and Caﬀeine13. We manually analysed the description of the resulting 492 reposi-
tories, from which 415 were excluded because they consist of example applications,
are not web applications, or do not use English in the application source code. The
remaining 77 repositories were manually inspected to select those that (i) include
application-level caching already implemented by their developers through map-
pings or one of the mentioned tools, (ii) handle HTTP POST, and (iii) are feasible
to adapt and create the versions needed for our study. We checked if: (1) the
application does not involve mixed programming languages because we need to
intercept the code using aspect orientation; (2) the caching was not implemented
in an interleaved way with the business logic because changing such a code could
lead to bugs while preparing the setup of our study; and (3) the applications pro-
vides a REST interface allowing us to make GET and POST HTTP requests. This
resulted in nine repositories, from which we discarded two due to performance and
non-reproducibility issues.

The seven web applications that are used in our study are presented in Table 1,
along with a description of their domain, numbers of lines of code (LOC) and
number of ﬁles. As can be seen, they are from various domains and sizes. In
Table 2, we present the number of cached opportunities manually implemented by
the developers with dedicated code (as shown in Listing 1) in each application as
well as the amount of data generated to populate their databases. Azkaban have
demo workﬂows available, while Thingsboard includes accounts with built-in data,
which were used. Data entities presented in Table 2 also have many relationships
with other entities not listed in table.

4 Results and Analysis

By following the described procedure, we obtained the results presented in this
section. Additional information about the data collected in the study together
with complementary charts is available online.14

13 https://github.com/ben-manes/caffeine
14 http://inf.ufrgs.br/prosoft/resources/2021/emse-apl-caching-comparison

12

Rˆomulo Meloca, Ingrid Nunes

Table 2 Target applications: caching and database data.

Application DEV Cached Methods

Input Data

Synthetic Data

Azkaban
Cloudstore
Keycloak
Killbill
Petclinic
Shopizer
Thingsboard

4
4
12
30
1
16
13

2 demo workﬂows No
Yes
10,000 items
Yes
100 clients
Yes
100 accounts
Yes
1,000 vets
Yes
10,000 products
No
3 demo clients

Table 3 Analysis of caching recommendations.

Recommendations

n
o
i
t
a
c
i
l
p
p
A

Azkaban

Cloudstore

Keycloak

Killbill

Petclinic

Shopizer

Thingsboard

h
c
a
o
r
p
p
A

APL
MEM

APL
MEM

APL
MEM

6
2

3
2

2
7

APL
17
MEM 25

APL
MEM

3
1

APL
10
MEM 24

APL
MEM

2
2

g
n
i
t
s
i
x
E

l
e
v
o
N

d
i
l
a
v
n
I

l
u
f
e
s
U

0
0

0
0

0
0

3
3

0
0

2
2

0
0

0
0

3
5

0
5

13
40

2
1

4
10

6
1

6
2

3
2

2
5

19
24

3
1

12
22

1
2

Average

6.14
APL
MEM 9.00

0.71
0.71

4.00
8.86

6.57
8.29

e
t
a
r

s
s
e
n
l
u
f
e
s
U

1.00
1.00

1.00
1.00

1.00
0.71

0.95
0.86

1.00
1.00

1.00
0.85

0.50
1.00

0.92
0.92

l
a
t
o
T

6
2

6
7

2
12

33
68

5
2

16
36

8
3

10.86
18.57

4.1 RQ1: Caching Decisions

The number of method recommendations made by APL and MEM for each appli-
cation are shown in Table 3. The recommendations can be: (i) novel, if the method
was not cached by developers; (ii) existing, if the method was cached by develop-
ers; or (iii) invalid, if the recommendation corresponds to an uncacheable method.
We also present how many of the valid recommendations are useful, i.e. cached
methods that led to hits, and the usefulness rate, which is the percentage of valid
recommendations that were useful.

As can be seen, for most of the applications, both APL and MEM provide
various invalid recommendations. Their main criteria for considering a method

A Comparative Study of Application-level Caching

13

cacheable is the analysis of method inputs and the corresponding output, and the
content of the method is not taken into account. This is a crucial issue to provide
better recommendations. For Killbill, for example, the number of MEM invalid
recommendations is even higher than the valid recommendations. By analysing
these invalid MEM suggestions, we observed that they mostly consist of request-
mappings Evidence 1). These methods provide as output a constant but cannot be
cached because of internal operations that cannot be skipped. Other recommended
uncacheable methods are those that return an iterator instead of a list. To illustrate
an invalid MEM recommendation, we show as follows a method of Petclinic, which
would cause a failure while listing veterinarians to users if cached.

@GetMapping("/vets.html")
public String showVetList(Map<String, Object> model) {

Vets vets = new Vets();
vets.getVetList().addAll(findAll());
model.put("vets", vets);
return "vets/vetList";

}

Invalid recommendations of APL, in contrast, consists mainly of methods that
return the result of a computation but also include writing operations that must
be carried out. Moreover, the APL invalid recommendations also include methods
that are not intended to always result in the same output given the same input.
For instance, APL recommended to cache the following method of Cloudstore,
which based on the APL calculations was not considered changeable enough to be
discarded.

public List<IItem> getPromotional() {

Random rand = new Random();
int randomId = rand.nextInt(1000) + 1;
String hql = "SELECT * FROM item where item.id = " + randomId;
. . .
return items;

}

This shows the importance of indicating to APL uncacheable methods because,
otherwise, it would introduce bugs in the target applications if used as a self-
adaptive approach, as its authors proposed. In addition, identifying uncacheable
methods might be actually helpful for APL because, if these are discarded, the
mean and standard deviations of the metrics used to make caching decisions would
change, leading to another set of recommendations (Evidence 2).

In some situations, the number of APL and MEM recommendations is simi-
lar. However, by inspecting the recommended methods, there is little intersection
between them. This suggests that these two application-level caching approaches
are complementary. Not only are they diﬀerent from each other, but they also
recommend methods that were not cached by the developers and, as shown by the
usefulness rate, these methods generate hits, potentially providing performance
improvements.

A main factor that justiﬁes the diﬀerences between the two approaches is that
APL admits some degree of changeability in the cached methods. Although this
can cause stale data in the application, this is accepted by developers when caching

14

Rˆomulo Meloca, Ingrid Nunes

other methods in the applications, relying only on the TTL to invalidate cached
content. Developers must thus be careful to verify if undesired stale data is caused
by caching a recommended method.15 Moreover, MEM cannot guarantee that the
recommended methods never change because it relies solely on the observed exe-
cutions. Nevertheless, it is less likely to recommend methods that cause stale data.
Consequently, when analysing suggested recommendations, developers should con-
sider false positives for APL (recommended methods that could cause undesired
stale data) and false negatives for MEM (not recommended methods that, although
changeable, can be cached and provide beneﬁts if so). This explanation justiﬁes
methods recommended by APL but not by MEM. Recommendations made only
by MEM occur because APL assumes that methods that are not frequent, expen-
sive, or shared by multiple users would not provide beneﬁts if cached, even if they
are not changeable (Evidence 3). Therefore, it does not recommend methods in
this case, while MEM does.

As mentioned in our study procedure, DEV is used as a reference in our study.
However, we observed a main general diﬀerence between both APL and MEM and
the developers’ choices is that developers, in some occasions, cached only lower-
level methods, while the compared recommenders suggested higher-level methods
(Evidence 4). For example, in the Petclinic application, the developers cached
the method Collection<Vet> findAll(), while APL and MEM recommended to
cache a method that calls the above method, shown below.

Vets showResourcesVetList() {

return new Vets().getVetList().addAll(

findAll()

);

}

As can be seen, the developers cached only the database access. However, APL and
MEM also prevented the re-instantiation of the Vets data structure. Of course, for
this to be possible, the code must be inspected to know if this data structure can
indeed be reused (which is the case).

Now, we look into the particularities of each approach. APL recommends to
cache particular inputs of a method, not all of them, assuming that only a subset
of the inputs would promote beneﬁts if cached. In Table 4, we provide data as-
sociated with such inputs. By comparing the Cached column of this table (which
shows the number of distinguished cached inputs) and the Novel and Existing
columns of Table 3, it is possible to see that only a few inputs per method are
cached (Evidence 5). For example, Keycloak has two Novel or Existing cached
methods as well as two cached inputs. Consequently, one input was cached from
each method. Petclinic, in turn, had three Novel or Existing cached methods, but
12 cached inputs. This means that at least one method had more than one input
cached. In the second phase of our study, we measured—from cached methods—
how many calls occurred with uncached inputs and, from these, how many are
distinct. These data are shown in Table 3, for 1, 5 and 25 users. We observed that
many inputs are recurrent and therefore could be cached. Nevertheless, APL can

15 Note that deciding the validity of the recommendation is not possible in these cases without
discussing the requirements of each application with involved stakeholders. We considered these
valid recommendations.

A Comparative Study of Application-level Caching

15

Table 4 Inputs discarded by APLCache.

Application Cached

Uncached Inputs

Distinct
5

25

1

Occurrences
5

25

1

Azkaban
Cloudstore
Keycloak
Killbill
Petclinic
Shopizer
Thingsboard

8
2
2
25
12
12
3

2,769
600
1
15
68
12
5

8,836
600
1
15
84
12
5

10,730
600
1
15
85
12
5

645,950
3,962
4,735
2,742
1,772
50,233
30

2,177,048
22,494
24,748
17,084
6,589
301,519
30

2,579,975
56,293
36,657
25,235
6,468
1,243,368
30

only recommend inputs that were observed in traces used to produce recommen-
dations. Consequently, methods that have recurrent inputs that vary over time are
not adequately cached. The complete APL solution focuses on constantly monitor-
ing the application and updating the cached inputs and methods. Although this
mechanism addresses this input issue, the solution must be used with caution to
not create bugs by automatically caching invalid recommendations.

Diﬀerently from APL, MEM not only recommends cacheable methods but
also how to implement them, as explained in Section 2. We detail the suggested
implementation alternatives in Table 5. For all valid recommendations, MEM rec-
ommended the use of a global cache, shared among the various instances of a
class. With respect to the size of the cached used for a method, most of the rec-
ommendations consist of single-instance caches. This means that for a particular
method, there is only one entry in the cache and, if an input that is not in the
cache is provided as parameter of a method call, the method is executed and its
output is cached replacing the previous cached entry (Evidence 6). In only six
cases (of the Killbill application), the cache can hold multiple entries in the cache
for a method. On the one hand, single-instance caching prevents overpopulating
the cache, as each method is limited to a single entry in the cache. On the other
hand, if the input of a cached method with single-instance caching begins to vary,
the method is always executed and its output replaces the cached object, leading
to a performance worse than if the method is not cached. This type of imple-
mentation should, therefore, be used with a solid understanding of the method
behaviour. MEM does not analyse the range of possible inputs (based on traces)
of methods, but evaluates considering the observed traces which implementation
leads to better performance results. Lastly, there are few cached getter methods,
which are those that have no input. In these cases, the result is constant and the
method is only re-executed if the TTL of the cached entry expires.

MEM has another particularity, which is the choice for two alternative kernels
for generating recommendations. The exhaustive kernel provides the complete re-
sults (which are those reported in the paper), while the iterative kernel truncates
the analysis to gain performance. This caused one method of Shopizer to be erro-
neously recommended as cacheable by the iterative kernel. In addition, this kernel
also discards two methods of Killbill that could be cached. This occurred because
two inputs are considered equal (based on analysis of part of the their object trees)
when they are not (based on their complete object trees) but the outputs are dif-
ferent. Therefore, the iterative kernel erroneously discarded the method because

16

Rˆomulo Meloca, Ingrid Nunes

Table 5 Types of valid MEM recommendations.

Application Instance Global Getter Single-instance Multi-instance

Azkaban
Cloudstore
Keycloak
Killbill
Petclinic
Shopizer
Thingsboard

0
0
0
0
0
0
0

2
2
7
27
1
30
2

0
0
3
0
1
2
0

2
2
4
21
0
28
2

0
0
0
6
0
0
0

the supposed same input generated diﬀerent outputs. Although the exhaustive ker-
nel recommends them, they are invalid recommendations due to internal method
operations.

4.2 RQ2: Performance Measurements

Having analysed the recommendations made by APL and MEM, we now examine
the impact that caching these recommendations have on the performance of the
applications.16 The results are shown in Table 6, which shows how much each set
of cached methods (DEV, APL, and MEM) increases or decreases the throughput
(requests per second) of the applications in relation to the NOCACHE version.
We present the median of the set of our ten executions—the higher the median,
the better. Additionally, Table 6 also presents the diﬀerence between the ﬁrst and
third quartiles, which indicates the variance among the executions. The lower the
diﬀerence, the lower the variance.

Figure 2 complements this table by presenting a bar chart of the normalised
throughputs (with error) labelled with the actual throughput. The normalisation
has the purpose of easing the comparison across the diﬀerent applications. For two
applications (Cloudstore and Keycloak) one or two executions are outliers because
some requests resulted in crashes (which also occur in the original applications),
leading to higher throughputs. Given that we consider the median value, the results
are not disturbed by these outliers.

In addition to the throughput, we examine performance metrics of the cache,
measuring hits and misses, as shown in Figure 3 and Table 7. As above, hits
and misses are normalised in the chart with the actual numbers informed in the
labels. We also present the addition of entries in the cache. This is helpful to
understand the APL results because this approach caches only particular method
inputs. Consequently, when there is a cache miss, the calculated result is not
necessarily cached. For the MEM version, in turn, the number of additions is
equal to the number of misses. Divergences between additions and misses in the
DEV version are due to the manual update of cache entries that had their data
source changed (additions that are not a consequence of a miss).

The results of the compared approach present diverging results across the vari-
ous applications as well as the diﬀerent number of users within a same application.
APL has a higher variance in the relative throughput (Evidence 7). It ranges from

16 We remind the reader that additional charts to further inspect the results are available at
http://inf.ufrgs.br/prosoft/resources/2021/emse-apl-caching-comparison.

A Comparative Study of Application-level Caching

17

Table 6 Throughput compared to the NOCACHE version and Quartile Increase.

Application Approach

1 User

5 Users

25 Users

Relative Throughput and Quartile Increase

Median Q. Inc. Median Q. Inc. Median Q. Inc.

Azkaban

Cloudstore

Keycloak

Killbill

Petclinic

Shopizer

Thingsboard

Average

DEV

APL

MEM

DEV

APL

MEM

DEV

APL

MEM

DEV

APL

MEM

DEV

APL

MEM

DEV

APL

MEM

DEV

APL

MEM

DEV

APL

MEM

9.94%

-13.35%

2.40%

6.39%

-10.59%

-7.29%

9.56%

7.32%

0.75%

11.24%

16.09%

2.92%

12.49%

4.48%

3.48%

10.88%

7.76%

25.65%

0.26%

0.10%

0.10%

8.68%

1.68%

4.00%

0.18%

0.40%

0.30%

0.68%

0.99%

1.51%

3.06%

1.36%

-1.08%

0.51%

5.43%

-2.83%

-1.07%

0.13%

0.46%

0.13%

1.41%

1.18%

0.58%

0.67%

0.42%

0.08%

10.52%

-0.33%

-1.31%

27.21% 171.80% 103.40%

2.65% 111.34%

2.75% 131.82%

1.05%

0.56%

0.53%

0.65%

1.97%

3.61%

1.41%

0.55%

1.54%

1.43%

0.12%

1.58%

0.46%

0.45%

0.87%

0.57%

-0.33%

13.08%

19.84%

4.89%

3.07%

1.80%

1.55%

14.06%

5.93%

19.56%

-0.05%

-0.55%

-0.71%

0.91%

1.23%

0.75%

0.61%

3.29%

1.45%

2.34%

1.70%

2.23%

1.85%

0.42%

2.27%

0.96%

9.17%

13.38%

19.21%

3.49%

1.18%

0.48%

7.04%

16.68%

26.45%

3.73%

-0.16%

-1.49%

-4.51%

0.95%

-16.82%

19.90%

-0.53%

1.65%

-1.51%

18.79%

20.24%

3.35%

0.17%

0.13%

0.12%

0.50%

0.66%

0.70%

41.33%

29.60%

29.12%

0.96%

2.71%

1.72%

3.02%

4.75%

4.82%

0.78%

1.01%

3.01%

4.25%

5.25%

3.09%

9.10%

8.25%

4.99%

-16.82% (Shopizer, 25 users) to 131.82% (Keycloak, 25 users), while the relative
throughput obtained with MEM varies from -7.29% (Cloudstore, 1 user) to 25.65%
(Shopizer, 1 user). Consequently, it is not possible to claim that any of the ap-
proaches (APL, MEM or DEV) leads to a higher performance or higher hit-ratio.
To understand why this happened, we next investigate in depth the issues that
occurred with the diﬀerent approaches that led to these results.

We begin by looking at the results obtained with APL. As discussed, this
approach selects particular inputs of cacheable methods to be cached, and its
recommendations often included a single method input to be cached. Caching
a pair of method inputs and output that is not frequent uses space in the cache
without providing any gain. However, the choice for particular inputs to be cached
causes performance decays due to two reasons. (1) As shown in Table 4, there are
many recurrent inputs in cached methods. This means that inputs, possibly not
observed during the period to generate recommendations, could provide beneﬁts
if cached. This situation can be seen in the Shopizer application, which has a high
number of misses but a low number of additions. It also has a low number of
distinct inputs that are not cached. (2) Even in the cases where the not cached

18

Rˆomulo Meloca, Ingrid Nunes

Table 7 Additions, Hits and Misses by Application.

Additions

Hits

Misses

t
c
e
j
o
r
P

n
a
b
a
k
z
A

e
r
o
t
s
d
u
o
l
C

k
a
o
l
c
y
e
K

l
l
i

b

l
l
i

K

c
i
n

i
l
c
t
e
P

r
e
z
i
p
o
h
S

d
r
a
o
b
s
g
n
h
T

i

s
r
e
s
U

1

5

25

1

5

25

1

5

25

1

5

25

1

5

25

1

5

25

1

5

25

h
c
a
o
r
p
p
A

DEV
APL
MEM

DEV
APL
MEM

DEV
APL
MEM

t
u
p
h
g
u
o
r
h
T

9.94%
-13.35%
2.40%

1.36%
-1.08%
0.51%

0.67%
0.42%
0.08%

6.39%
DEV
APL
-10.59%
MEM -7.29%

1,304.00
665.00
926.00

4,231.00
604.00
2,597.00

5,058.00
594.00
3,015.00

110.00
1.00
3,680.00

550.00
DEV
APL
1.00
MEM -1.07% 19,779.50

5.43%
-2.83%

1,335.00
DEV
APL
5.00
MEM -1.31% 48,652.50

10.52%
-0.33%

DEV
APL
MEM

9.56%
7.32%
0.75%

27.21%
DEV
APL
111.34%
MEM -0.33%

DEV 103.40%
131.82%
APL
7.04%
MEM

6.00
5.00
1.00

5.00
11.00
1.00

3.00
30.00
1.00

Median

Mean

St. Dev. Median

Mean

St. Dev. Median

Mean

St. Dev.

869.67
664.00
926.00

2,821.00
645.60
2,597.00

3,372.33
667.20
3,015.00

82.00
4.33
3,680.00

379.67
4.33
19,779.50

910.33
7.00
48,652.50

223.86
5.00
1,894.60

2,517.29
11.00
9,899.80

2,083.14
30.00
7,714.60

752.29
27.96
552.96

2,442.19
63.92
2,798.73

2,919.66
100.95
3,245.62

26,253.00
12,512.00
2,590.00

89,954.00
44,148.00
9,398.50

105,566.00
52,385.00
11,108.00

52.00
5.77
93.34

2,310.00
72,927.50
5,814.00

309.87
5.77
468.81

12,547.00
390,261.00
16,198.00

26,253.00
76,308.00
2,590.00

89,954.00
261,052.33
9,398.50

105,566.00
307,667.33
11,108.00

2,896.00
72,927.50
5,814.00

15,598.67
390,261.00
16,198.00

0.00
117,029.39
3,587.86

0.00
398,027.51
13,069.45

0.00
468,775.34
15,375.33

1,114.28
92,511.49
0.00

5,463.26
494,035.71
22,900.36

767.81
3.46
1,181.58

571.12
0.00
2,592.92

6,638.07
0.00
13,554.49

5,494.73
0.00
10,562.28

34,280.24
12.04
34,451.40

32,045.00
992,560.00
41,681.00

53,938.00
4,730.00
49,999.00

40,150.33
992,560.00
41,681.00

14,143.77
1,256,818.66
58,923.21

289,847.14
4,730.00
149,999.33

418,788.59
0.00
173,205.66

182,420.00
24,737.00
49,999.00

1,027,833.00
24,737.00
149,999.33

1,429,579.42
0.00
173,205.66

164,842.00
36,627.00
49,999.00

1,001,538.86
36,627.00
149,999.33

1,371,929.40
0.00
173,205.66

94,800.50
18,103.00
58,590.71

194,375.53
37,541.39
77,149.21

1,367.00
665.00
926.00

4,360.50
635.50
2,597.00

5,384.50
677.00
3,015.00

110.00
1,916.00
3,680.00

550.00
10,914.00
19,779.50

1,335.00
27,308.00
48,652.50

6.00
2,370.00
1.00

5.00
12,379.50
1.00

3.00
18,343.50
1.00

25.50
23.00
46,036.00

1,367.00
108,211.67
926.00

4,360.50
363,379.33
2,597.00

1,931.82
261,269.87
552.96

6,165.26
881,140.06
2,798.73

5,384.50
430,551.83
3,015.00

7,613.42
1,044,222.15
3,245.62

82.00
1,325.00
3,680.00

379.67
7,502.33
19,779.50

910.33
18,771.33
48,652.50

223.86
2,370.00
1,894.60

2,517.29
12,379.50
9,899.80

2,083.14
18,343.50
7,714.60

48.88
178.74
37,356.58

52.00
1,139.87
93.34

309.87
6,496.28
468.81

767.81
16,268.81
1,181.58

571.12
3,344.62
2,592.92

6,638.07
17,491.70
13,554.49

5,494.73
25,899.20
10,562.28

51.66
458.68
34,445.19

4,239.50
250.00
5,488.00

14,689.50
46.00
482.00

21,123.00
7.00
2,671.50

1,924.00
420.00
420.00

6,790.00
1,504.00
1,504.00

6,572.00
1,429.00
1,427.00

320.50
3,496.00
786.50

1,831.00
20,859.00
1,660.00

7,646.00
85,835.00
6,741.00

1.00
1.00
0.00

1.00
1.00
0.00

1.00
1.00
0.00

501,843.40
62,787.00
100,269.15

1,033,550.66
163,814.67
262,508.42

76.00
24.00
240,052.50

91.00
958.89
211,452.50

791,946.40
68,377.92
145,821.60

1,633,101.93
217,297.47
409,644.16

124.00
29.00
379,123.00

127.38
1,414.05
380,233.21

78.52
3,123.92
210,186.83

116.28
4,608.72
413,039.81

1,924.00
468.00
420.00

6,790.00
1,769.33
1,504.00

6,572.00
1,725.67
1,427.00

320.50
3,496.00
2,038.17

1,831.00
20,859.00
11,203.69

7,646.00
85,835.00
37,899.56

1.00
1.00
0.00

1.00
1.00
0.00

1.00
1.00
0.00

0.00
414.09
0.00

0.00
1,671.87
0.00

0.00
1,648.15
0.00

280.72
0.00
2,161.66

1,673.01
0.00
12,975.28

7,085.21
0.00
51,136.50

0.00
0.00
0.00

0.00
0.00
0.00

0.00
0.00
0.00

9.00
10.00
10.00

10.00
10.00
10.00

10.00
8.00
10.00

1.00
4,672.00
1.00

1.00
28,123.00
5.00

9.00
625.67
10.00

10.00
2,239.00
10.00

10.00
2,189.00
10.00

1.00
4,186.17
317.59

1.00
25,127.00
1,877.68

0.00
1,067.23
0.00

0.00
3,860.74
0.00

0.00
3,778.47
0.00

0.00
1,359.45
687.42

0.00
8,225.69
3,946.12

3.00
115,821.00
8.00

3.00
103,614.67
7,262.86

0.00
33,797.44
14,563.39

9.00
31.00
3.00

9.00
31.00
3.00

9.00
31.00
3.00

9.00
31.00
3.00

9.00
31.00
3.00

9.00
31.00
3.00

0.00
0.00
0.00

0.00
0.00
0.00

0.00
0.00
0.00

9.00
10.00
10.00

10.00
10.00
10.00

10.00
8.00
10.00

1.00
1.00
1.00

1.00
5.00
5.00

3.00
8.00
8.00

9.00
1.00
3.00

9.00
1.00
3.00

9.00
1.00
3.00

9.00
35.00
10.00

10.00
42.67
10.00

10.00
33.00
10.00

0.00
44.17
0.00

0.00
56.58
0.00

0.00
44.17
0.00

1.00
1.00
317.59

1.00
5.00
1,877.68

3.00
8.00
7,262.86

0.00
0.00
687.42

0.00
0.00
3,946.12

0.00
0.00
14,563.39

9.00
1.00
3.00

9.00
1.00
3.00

9.00
1.00
3.00

0.00
0.00
0.00

0.00
0.00
0.00

0.00
0.00
0.00

11.23%
16.08%

50.50
22.00
2.92% 46,036.00

15,903.50
26.84
37,350.58

13.08%
19.84%

149.00
23.00
4.89% 240,052.50

84,014.60
26.47
211,426.17

181,845.49
16.11
210,213.97

16.68%
26.45%

204.00
26.00
3.73% 379,123.00

132,146.50
36.00
380,193.71

286,228.24
21.87
413,077.32

DEV
APL
MEM

DEV
APL
MEM

DEV
APL
MEM

DEV
APL
MEM

DEV
APL
MEM

12.49%
4.48%
3.48%

3.07%
1.80%
1.55%

-0.16%
DEV
APL
-1.49%
MEM -4.51%

10.88%
DEV
APL
7.76%
MEM 25.65%

14.06%
DEV
APL
5.93%
MEM 19.56%

0.95%
DEV
-16.82%
APL
MEM 19.90%

DEV
APL
MEM

0.26%
0.10%
0.10%

-0.05%
DEV
APL
-0.55%
MEM -0.71%

-0.53%
DEV
APL
1.65%
MEM -1.51%

A Comparative Study of Application-level Caching

19

Fig. 2 Normalised throughput (requests per second) by approach for each application.

inputs are not frequent, if there is a wide range of inputs, the cost of managing
the cache within a method might not compensate the gains provided by caching
a particular frequent input. This is the case of the Azkaban application, which
also has a high number of misses and a low number of additions, but the number
of distinct inputs that are not cached is higher. In both cases, APL achieved the
worst results in most of the user conﬁgurations. Therefore, it is important to not
only search for inputs that would provide gains if cached (because of its frequency
or time to compute), but also analyse the range of possible inputs (Evidence 8).

287.17364.38339.38331.421115.451142.911133.291127.601321.601324.911317.211316.1112.6713.6412.5512.1340.2440.7540.1539.5340.2340.7738.9940.8469.5382.7372.0977.77396.70430.43403.88408.241030.791143.021020.691034.218.258.499.627.6648.4752.1954.7145.76158.32192.15228.21190.3328.2128.8026.4926.29290.14174.63136.83137.281189.721043.86549.32513.2059.5859.6859.5859.52294.40295.88293.91296.02519.17508.01503.03510.7310.8110.369.589.3152.7849.8046.1944.0481.2574.9866.6664.26PetclinicShopizerThingsboardAzkabanCloudstoreKeycloakKillbill1525152515251525152515251525Number of UsersApproachDEVAPLMEMNOCACHE20

Rˆomulo Meloca, Ingrid Nunes

Fig. 3 Normalised additions, hits and misses by approach for each application.

3320260918523228846351943336101176030228924262535180783157899541879792300210556622216649270273418522180276872151942583311107696030132467360131139395592127319730514585586885814780522467963239619851201204518336239752467360225071139395595631427319730551567947311176214949930145823857347302028930449998247377194831449998366277010772449998474015679473247591762149499366871458238573510159035896414503840146507422868413214659124649108618948005410135565083501843420053838889137919464291643233963918965581821972850748602686710199125597105910128101099101014041924420530867901504517765721427187791067171010656710101269875241309861597833496641244582085936621456488583515292606393502342698730152424130912433766159783196196196110110110319631963196AzkabanCloudstoreKeycloakKillbillPetclinicShopizerThingsboardadditionhitmiss1525152515251525152515251525Number of UsersApproachDEVAPLMEMA Comparative Study of Application-level Caching

21

Furthermore, it is important to distinguish when a cacheable method receives a
particular frequent input and when a method tends to receive frequent inputs,
which change over time.

While APL focuses not only on identifying methods that are feasible to be
cached but also a subset that would provide gains if cached, MEM aims to iden-
tify all methods that can be cached (discarding those that are cheap to compute
based on a ﬁxed threshold). This justiﬁes why MEM not always provides perfor-
mance improvements—some of the recommendations, although cacheable (i.e. the
same inputs produce the same output), do not provide gains if cached because it
is not too frequent or expensive to compute. MEM assumes that developers de-
cide which recommendations to cache. To ease this analysis, the approach ranks
the recommendations by calculating the time saved by caching a method based
on the potential hits and its computation time. We observed that highly ranked
recommendations in fact are those that lead to more hits during the execution
(Evidence 9). A reason for MEM to achieve low throughput in some cases, such
as for Cloudstore, is that most of the recommendations indicated that methods
should be cached using a single-instance cache. This caused the cache entry asso-
ciated with a method to be constantly replaced with a new pair of inputs-output,
without reusing the previous one (i.e. cache thrashing) (Evidence 10). This led
to a high number of misses with a low number of hits.

We used as much as possible of the original implementations of APL and MEM
and, in the cases when it was not possible, we used the original implementation as
a basis. When analysing our results, we observed that implementation details also
largely contribute to the obtained results. MEM compares if objects are equal with
the equals() method, while APL serialises objects. This is because APL keeps a
table at runtime with the speciﬁc inputs of a method that should be cached. On
the one hand, serialising and deserialising objects at runtime have a cost, resulting
in a penalty in the gained performance. On the other hand, some of the compared
objects had properties that are not included in the serialisation. These, if taken
into account, could wrongly lead to the conclusion that two objects are not equal
when they are. Consequently, APL could reuse methods that MEM considered not
cacheable by mistakenly identifying that two objects are diﬀerent.

Finally, we emphasise that the results obtained with the developers’ cached
methods are presented as a baseline. As the workload used by the developers to
make caching decisions is not available and is diﬀerent from ours, their decisions
can be not optimal in our scenario. Nevertheless, the version of the application
that included methods cached by developers (DEV) still obtained the best results
for the majority of the applications. This provides evidence of the adequacy of the
workload used for this study.

5 Discussion

Based on the presented results, we now discuss implications from our empirical
evaluation, which give directions for future application-level approaches. We also
examine the threats to the validity of our study.

22

Rˆomulo Meloca, Ingrid Nunes

5.1 Causes for Missed Opportunities

APL and MEM provided caching recommendations that result in performance
gains. However, by analysing the caching opportunities that they missed, we ob-
served that both APL and MEM do not take into account other factors that are
fundamental to provide better results in terms of throughput and hits. These fac-
tors are listed as follows.

– Time-to-live (TTL). When putting elements in the cache, we used the TTL
used by developers in the target application. However, the TTL plays a key
role in the cache management. If a cache entry expires before it is reused, it
provide no gains. None of the approaches analysed the distribution of repeated
calls and the selection of TTL values. They only take into account the complete
set of traces regardless of how close or spread the repeated calls are.

– Changeability. A cacheable method is feasible to be cached even if the same
inputs can produce a diﬀerent output. In this case, cache entries can be evicted
or stale data are allowed for short periods of time. If this is taken into consid-
eration, more methods that could provide performance gains could be cached.
MEM discards them. APL, in turn, considers this but, because it does not
recommend a TTL to reduce the chances of stale data, it could led to bugs if
it is used to automatically manage the application-level caching.

– Shareability. APL achieved better results for many users in most of the cases.
This is mainly because, to select methods to cache, it considers if a certain
computation is reused for diﬀerent users (better results were not achieved with
Shopizer due to the issue of caching only a particular input of a method). This
is an important aspect that is not considered in MEM.

– Prediction of Hits. Both approaches analyse a given set of traces to suggest
recommendations. They assume that estimated hits in the future are equal to
the hits that would occur with the given set of traces. This occurs mainly in
APL because it assumes that a method with frequent inputs in the analysed
traces would receive calls with the same input in the future. Consequently, it is
important for the approaches to verify if they can predict hits and performance
gains, not only assume that the given set of traces is completely representative
of future workloads.

5.2 Recommendations for Future Approaches

The results do not indicate that there is a best approach to recommend meth-
ods to cache. However, they revealed many circumstances in which each approach
are unable to provide adequate recommendations, which are the basis for devel-
oping future application-level caching supporting tools. Key circumstances are
highlighted throughout the previous section as pieces of evidence. These evidences
are related to adjustments that must be done in the evaluated approaches. Con-
sequently, based on our experiment, we derived various lessons learned, which are
categorised into seven groups, discussed as follows.

1. Explore method changeability and time-to-live (TTL) values (Evidence
7). MEM considers methods as cacheable when a method always produce a
same output for a given input. This signiﬁcantly limits the choice for methods

A Comparative Study of Application-level Caching

23

to cache. APL, in contrast, admits methods that have low changeability, that
is, methods that in general have this behaviour but not always. Developers also
make choices as APL. However, they control data freshness by choosing ade-
quate TTL values or doing manual cache content invalidation. Consequently,
it is crucial for approaches to further explore this issue and explore how to
choose adequate TTLs for particular methods. Moreover, the choice for TTL
also allows methods that have many calls in short periods of time (but less
than other methods considering a larger time frame) to be cached making an
eﬀective use of the cache by being cached with short TTL values.

2. Inspect the domain of method parameters (Evidence 5 and 11). APL
concerns frequent and/or expensive method inputs. Although some method
inputs promote beneﬁts if cached, the costs of managing the cache to cache
one or a few inputs when a method is called a much higher number of times with
other inputs can result in a performance decrease. Therefore, it is fundamental
not only to search for frequent/expensive inputs, but also to look at the range
of values provided as parameters. MEM take a step towards this direction by
recommending the size of the cache, but the simplistic idea of having a single
instance cache (i.e. one entry only) can result in cache thrashing.

3. Consider the method body to detect invalid recommendations. (Evidence
1 and 2). Both investigated approaches recommended many invalid recommen-
dations, because they inspect solely the values of method inputs and outputs.
This prevents the complete automation of application-level caching because,
for various methods, there are internal operations that cannot be skipped. As
a consequence, it is important for approaches to examine internal method op-
erations to discard such methods. This would largely reduce the developers’
eﬀort, who currently manually perform this task.

4. Investigate the trade-oﬀ between caching coarse- or ﬁne-grained meth-
ods (Evidence 4). Our study showed that in some situations developers cached
ﬁne-grained methods (a callee method) while APL and MEM recommended
caching coarse-grained methods (a caller method). Caching the callee promotes
a higher number of hits (because it may be called by multiple methods), while
caching the caller method saves more computation time (because it reuses more
operations). The trade-oﬀ between choosing a callee or caller method should
be further investigated, as in diﬀerent situations a diﬀerent choice is optimal.
5. Consider the identiﬁcation of methods feasible to cache and, from those,
the selection of methods that improve performance as separate tasks
(Evidence 1–3 and 9). APL not only aims to identify methods that are feasible
to be cached, but also to select a subset of methods that promotes more beneﬁts
if cached. Diﬀerently, MEM focuses on identifying methods that are feasible to
be cached (using a more strict deﬁnition) and ranks these methods. However,
both approaches still recommend many invalid recommendations. Therefore,
the automation of application-level caching could be split into two separate
tasks: (i) the recommendation of methods that are feasible to be cached, that
is, those do not cause bugs if cached; and (ii) the recommendation of a subset
of methods feasible to be cached that would promote performance gains if
cached. By guaranteeing that the recommendations of (ii) do not cause bugs if
cached, it is possible to automate it using an adaptive approach. Because the
application workload varies at runtime, caching decisions must be constantly
revised. This automation relieves developers from a time-consuming task.

24

Rˆomulo Meloca, Ingrid Nunes

As discussed by the authors of MemoizeIt, detecting whether a recommenda-
tion is completely safe to cache is a costly operation. Detecting this requires
recording and comparing the complete previous state of the application before
and after caching, including what is outside the boundaries of the application
(such as in a database). In order to reduce such a cost, future approaches
could perform an assisted static analysis (oﬄine or at compilation time) to
mark methods as uncacheable. Marking such methods would reduce the costs
of the analysis by excluding all methods that contain any write operation in
static variables, inputs received in the method, or even in the outside envi-
ronment. With developers in the loop, next generation approaches could make
safer and more reliable recommendations.

6. Provision of Explanations (Evidence 5 and 8). There are many aspects that
should be considered to decide methods to cache (input-output behaviour, the
domain of input values, the invocation frequency, the computation time, and
so on). Given that the complete automation of application-level caching is still
not possible due to invalid recommendations and the choice for methods that
lead to performance decays, recommender approaches could provide comple-
mentary information to the recommendation so that developers could make
more informed decisions of what methods to cache.

7. Consider implementation aspects (Evidence 6 and 10). Our study has
shown that how application-level caching is implemented has a high impact
on the application performance. More speciﬁcally, although the serialisation
of objects takes time to be performed, it led to diﬀerent results when objects
are compared. Thus, these implementation issues must be seriously taken into
account because they impact on the performance in many ways.

5.3 Threats to Validity

We next report how we addressed the identiﬁed threats to validity in our study.

5.3.1 Construction Validity

Our study required the implementation and adaptation of existing approaches as
well as the target applications. To prevent the introduction of bugs in APL and
MEM, we validated our implementation carefully analysing the behaviour and
outputs of each developed module using a small-scale set of inputs. Moreover, we
logged every critical operation in order to verify if the implementation followed
the expected ﬂow. We also used an existing cache component, which increases the
reliability of our implementation, with respect to this aspect. Regarding the imple-
mentation of diﬀerent versions of the applications, they were created by adding or
removing localised cache snippets. This task was performed with a careful analysis
and the execution of unit tests (if present).

The compared approaches use thresholds. Although we used the values pro-
vided by the authors of these approaches, we explored a range of values for the
thresholds using testing data. The best results were those obtained with the de-
fault thresholds. We also had to choose other parameters, such as the amount of
time to execute the applications and number of simultaneous users. Varying the

A Comparative Study of Application-level Caching

25

amount of time both to generate traces and to collect measurements produced
consistent results. The same occurred with other numbers of simultaneous users.
To avoid the impact of external factors on the measured throughput, such as
operating system tasks, we executed using docker images the same set of requests
ten times and analysed the variance of results. Moreover, no other tasks were being
carried out in the used machine. Even the simulation of requests was performed
in a diﬀerent machine.

5.3.2 External Validity

A threat to the external validity of our study is our selected applications, which
one can argue that are not representative. In order to address this, we followed a
systematic procedure to select GitHub repositories, which include applications of
diﬀerent domains and sizes. Another threat is the generated workloads and data
stored in the databases, which could be not representative of the applications in
real settings. Note that both APL and MEM generate recommendations for a
speciﬁc workload and data. Consequently, our generated synthetic workloads and
data aﬀected APL and MEM equally. Moreover, we extracted a navigation graph
according to the organisation of pages within each web application to generate
realistic sequences of requests, avoiding making particular methods more frequent
than they would be in a real scenario. We also added a probability of a user closing
the browser session while accessing the web application to make the interaction
even more realistic.

An issue that may be seen as a threat is the marginal improvements in the
throughput values provided by caching of both compared approaches as well as
the developers’ version. With respect to the diﬀerences of APL and MEM in com-
parison with the applications with no caching, as discussed, both approaches make
some recommendations that provide performance improvements (adequate recom-
mendations) and some that not only do not improve performance but also degrades
it (inadequate recommendations). Consequently, the inadequate recommendations,
instead of improving the performance, increase the computation time by access-
ing and adding cached content. This justiﬁes the variances in the results. With
respect to the developers’ version, we highlight that it was added as a reference.
However, the workloads and data used for the applications are not the same that
the developers used to make decisions of what to cache. Therefore, their decisions
might not be optimal considering our simulated scenarios.

6 Conclusion

The current demands of high performance and scalability in software applications
lead developers to explore a wide range of techniques to meet these requirements.
Application-level caching is an approach that has been increasingly being adopted.
Although it addresses performance issues by reusing frequent and/or expensive
computations, it comes with an eﬀort to identify methods to cache and revise
caching decisions periodically. Approaches have been proposed to support devel-
opers in this time-consuming task.

From the existing application-level caching approaches, two are able to identify
and recommend cacheable methods. They have been individually evaluated with

26

Rˆomulo Meloca, Ingrid Nunes

a small subset of applications. In this paper, we presented the results of a study
in which we evaluated these approaches, namely APLCache and MemoizeIt, using
a set of seven open-source web applications. As an evaluation in this direction is
a challenge because involves many design decisions and parameters, we proposed
a protocol to evaluate these approaches, with the aim of reducing possible bias in
the evaluation. We adapted the original web applications, creating four versions
of each: an uncached version, and three versions with cached methods selected by
developers, APLCache and MemoizeIt. The results were analysed from two per-
spectives, which are the caching choices and performance (throughput, hits and
misses). The obtained results revealed that both approaches are able to identify
cacheable methods that improve performance if cached. However, their recom-
mendations include many invalid recommendations as well as methods that do
not promote expected beneﬁts. We made an in-depth analysis of good and bad
recommendations, which allowed us to derive seven lessons learned that pave the
way for developing the next generation of application-level caching supporting
approaches.

As future work, we aim to explore the raised issues unaddressed by APLCache
and MemoizeIt. Our ongoing work is to exploit TTL values to identify better
caching opportunities as well as to make an eﬀective use of the cache, thus reducing
the need for large cache components.

Acknowledgements The authors would like to thank for CNPq grants ref. 131271/2018-0,
ref. 313357/2018-8, and ref. 428157/2018-1. This study was ﬁnanced in part by the Coor-
dena¸c˜ao de Aperfei¸coamento de Pessoal de N´ıvel Superior - Brasil (CAPES) - Finance Code
001.

A Reproducibility

To reproduce and/or change the parameters of our experiment, it is required to have two
computers with suﬃcient resources, both running Linux-based systems and having Java 11+,
Maven, Docker CE and docker-compose installed. In our repository, we provide a conﬁgure ﬁle
that helps to install such dependencies. Also, this conﬁguration script automates the cloning of
all the repositories of each application version in the proper folder, as well as the tools that we
implemented and rely on. It is also possible to run our experiment in standalone mode (with
only one computer). However, it may not generate reliable results, due to the interference of the
requester application that would compete on resources with the running application. Finally,
compiling and installing our tools through Maven is also required, which is automatically made
by a script named compile.sh.

To the ﬁrst phase of the experiment, we automatically detect all the applications (including
new ones) within the folder that holds the NOCACHE version of applications through the script
trace.sh. The hostname for the server machine must be informed. We then check whether the
traces of the application were already collected in the outputs folder. If not yet executed,
we send a signal to the server machine (which could also be the same machine) to bring the
application up via a Docker container. It is expected to the server machine to be listening
for requests with our Java tool called RemoteExecutor, as well as having all the mentioned
conﬁgurations done. New applications are expected to have a Dockerﬁle—as we did for our
target applications—including its required steps to compile and to run, as well as a docker-
compose describing its dependencies, such as a database and its initialisation SQL. We also look
for the ﬁles whitelist, blacklist and ignored in order to conﬁgure which Java packages should be
serialised by our JSONSerialiser tool for the tracings. Also, including the dependency of our
ApplicationTracer in the pom or gradle ﬁle is required, so we can automatically inject code in
every method of new applications. We then automatically wait for the application to start and
once started we start ﬁring requests. For new applications, describing the JSON ﬁle containing

A Comparative Study of Application-level Caching

27

the workload graph as we did for our applications in the workloads folder is required for this
step. When ﬁnished with the tracing, we automatically tear the application down and clean
its created ﬁles and data.

To the recommendation of methods based on the collected traces, it is just needed to
execute the approaches using as input the trace ﬁle generated in the ﬁrst phase. Manual
analysis of the recommendations, as well as the manual implementation of the caching with
our Cache component, is required. Particularly for APL, the generated ﬁles containing the
recommended inputs (which is already included in our repository) are required to be in the
outputs folder. It is also important, for APL, that the same serialisation parameters adopted
in the ﬁrst phase are used in the second phase, so the inputs can match. For new applications,
the Maven/Gradle dependency of our Cache component must be added. For new approaches,
it is required that they are able to read and process the tracing ﬁle generated in the ﬁrst phase
so they can give their recommendations.

To the second phase of the experiment, we automatically detect all the applications and
start ﬁring the requests through our script named run.sh.The hostname for the server machine
must be informed. If not yet generated the workloads, we automatically do it. We then start
sampling all the ten executions for all the groups of simulated users collecting our metrics in
the disk. Particular executions that already succeeded are skipped. It is expected in this phase
that each application version is in the proper folder and includes the ﬁles, conﬁgurations and
caching as mentioned before.

To aggregate the results produced in the outputs folder, the script reduce.sh helps on
initialising the CSV ﬁles and executing our tools that calculate our metrics. The hostname for
the server machine must be informed. In order to generate visualisations for the aggregated
CSV ﬁles, the script plot.sh under the analysis folder may be invoked.

References

Abbott ML, Fisher MT (2009) The art of scalability: Scalable web architecture, processes, and

organizations for the modern enterprise. Pearson Education

Ali W, Shamsuddin SM, Ismail AS (2012) Intelligent Web proxy caching approaches based on
machine learning techniques. Decision Support Systems 53(3):565–579, DOI 10.1016/j.dss.
2012.04.011

Ali Ahmed W, Shamsuddin SM (2011) Neuro-fuzzy system in partitioned client-side Web
cache. Expert Systems with Applications 38(12):14715–14725, DOI 10.1016/j.eswa.2011.05.
009

Alici S, Altingovde IS, Ozcan R, Barla Cambazoglu B, Ulusoy ¨O (2012) Adaptive time-to-live
strategies for query result caching in web search engines. In: Lecture Notes in Computer
Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in
Bioinformatics), vol 7224 LNCS, pp 401–412, DOI 10.1007/978-3-642-28997-2 34

Candan KS, Li WS, Luo Q, Hsiung WP, Agrawal D (2001) Enabling dynamic content caching
for database-driven web sites. ACM SIGMOD Record 30(2):532–543, DOI 10.1145/376284.
375736, URL http://dl.acm.org/citation.cfm?id=376284.375736

Chen TH, Shang W, Hassan AE, Nasser M, Flora P (2016) CacheOptimizer: Helping Develop-
ers Conﬁgure Caching Frameworks for Hibernate-based Database-centric Web Applications.
In: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations
of Software Engineering - FSE 2016, ACM Press, New York, New York, USA, pp 666–677,
DOI 10.1145/2950290.2950303, URL http://dx.doi.org/10.1145/2950290.2950303

Chen Z, Chen B, Xiao L, Wang X, Chen L, Liu Y, Xu B (2018) Speedoo: prioritizing perfor-
mance optimization opportunities. In: Proceedings of the 40th International Conference on
Software Engineering, ACM, pp 811–821

Della Toﬀola L, Pradel M, Gross TR (2015) Performance problems you can ﬁx: a dynamic
analysis of memoization opportunities. In: Proceedings of the 2015 ACM SIGPLAN In-
ternational Conference on Object-Oriented Programming, Systems, Languages, and Ap-
plications - OOPSLA 2015, ACM Press, New York, New York, USA, pp 607–622, DOI
10.1145/2814270.2814290, URL http://dx.doi.org/10.1145/2814270.2814290

Ghandeharizadeh S, Yap J, Barahmand S (2012) Cosar-cqn: an application transparent ap-
proach to cache consistency. In: Twenty First International Conference On Software Engi-
neering and Data Engineering

28

Rˆomulo Meloca, Ingrid Nunes

Ghandeharizadeh S, Irani S, Lam J (2015) Cache Replacement with Memory Allocation. Pro-
ceedings p 9, DOI 10.1137/1.9781611973754.1, URL http://epubs.siam.org/doi/abs/10.
1137/1.9781611973754.1

Guo PJ, Engler D (2011) Using Automatic Persistent Memoization to Facilitate Data Anal-
ysis Scripting. In: Proceedings of the 2011 International Symposium on Software Testing
and Analysis, ACM, New York, NY, USA, ISSTA ’11, pp 287–297, DOI 10.1145/2001420.
2001455, URL http://doi.acm.org/10.1145/2001420.2001455

Gupta P, Zeldovich N, Madden S (2011) A trigger-based middleware cache for ORMs.
In: Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial
Intelligence and Lecture Notes in Bioinformatics), Springer Berlin Heidelberg, Lisbon,
Portugal, vol 7049 LNCS, pp 329–349, DOI 10.1007/978-3-642-25821-3 17, URL https:
//doi.org/10.1007/978-3-642-25821-3_17

Huang J, Liu X, Zhao Q, Ma J, Huang G (2010) A browser-based framework for data cache
in web-delivered service composition. In: Proceedings - 2010 IEEE International Conference
on Service-Oriented Computing and Applications, SOCA 2010, DOI 10.1109/SOCA.2010.
5707138

Hwang J, Wood T (2013) Adaptive performance-aware distributed memory caching. In:
10th International Conference on Autonomic Computing (ICAC 13), USENIX Asso-
ciation, San Jose, CA, pp 33–43, URL https://www.usenix.org/conference/icac13/
technical-sessions/presentation/hwang

Larson P˚A, Goldstein J, Zhou J (2004) MTCache: Transparent mid-tier database caching in
SQL server. In: Proceedings - International Conference on Data Engineering, vol 20, pp
177–188, DOI 10.1109/ICDE.2004.1319994, arXiv:1011.1669v3

Leszczy´nski P, Stencel K (2010) Consistent caching of data objects in database driven web-
sites. In: Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁ-
cial Intelligence and Lecture Notes in Bioinformatics), vol 6295 LNCS, pp 363–377, DOI
10.1007/978-3-642-15576-5 28

Maplesden D, von Randow K, Tempero E, Hosking J, Grundy J (2015a) Performance analysis
using subsuming methods: An industrial case study. In: Software Engineering (ICSE), 2015
IEEE/ACM 37th IEEE International Conference on, IEEE, vol 2, pp 149–158

Maplesden D, Tempero E, Hosking J, Grundy JC (2015b) Subsuming methods: Finding
new optimisation opportunities in object-oriented software. In: Proceedings of the 6th
ACM/SPEC International Conference on Performance Engineering, ACM, pp 175–186
Megiddo N, Modha DS (2004) Outperforming LRU with an adaptive replacement cache algo-

rithm. Computer 37(4):58–65, DOI 10.1109/MC.2004.1297303

Mertz J, Nunes I (2017a) A Qualitative Study of Application-Level Caching. IEEE Transac-
tions on Software Engineering 43(9):798–816, DOI 10.1109/TSE.2016.2633992, URL https:
//doi.org/10.1109/TSE.2016.2633992http://ieeexplore.ieee.org/document/7762909/
Mertz J, Nunes I (2017b) Understanding application-level caching inweb applications: A com-
prehensive introduction and survey of state-of-the-art approaches. ACM Computing Surveys
50(6), DOI 10.1145/3145813

Mertz J, Nunes I (2018) Automation of Application-level Caching in a Seamless Way. Software

- Practice and Experience DOI 10.1002/spe.2571

Nguyen K, Xu G (2013) Cachetor: Detecting Cacheable Data to Remove Bloat. In: Proceedings
of the 2013 9th Joint Meeting on Foundations of Software Engineering, ACM, New York,
NY, USA, ESEC/FSE 2013, pp 268–278, DOI 10.1145/2491411.2491416, URL http://doi.
acm.org/10.1145/2491411.2491416

Ports DRK, Clements AT, Zhang I, Madden S, Liskov B (2010) Transactional Consistency and
Automatic Management in an Application Data Cache. In: Proceedings of the 9th USENIX
Symposium on Operating Systems Design and Implementation, USENIX Association, CA,
USA, pp 279–292

Qin X, Wang W, Zhang W, Wei J, Zhao X, Zhong H, Huang T (2014) PRESC2: Eﬃcient self-
reconﬁguration of cache strategies for elastic caching platforms. Computing 96(5):415–451,
DOI 10.1007/s00607-013-0365-6

Radhakrishnan G (2004) Adaptive application caching. Bell Labs Technical Journal 9(1):165–

175, DOI 10.1002/bltj.20011, URL http://dx.doi.org/10.1002/bltj.20011

Saemundsson T, Bjornsson H, Chockler G, Vigfusson Y (2014) Dynamic Performance Proﬁling
of Cloud Caches. In: Proceedings of the ACM Symposium on Cloud Computing - SOCC
’14, pp 1–14, DOI 10.1145/2670979.2671007, URL http://dl.acm.org/citation.cfm?doid=
2670979.2671007

A Comparative Study of Application-level Caching

29

Santhanakrishnan G, Amer A, Chrysanthis PK (2006) Self-tuning caching: The univer-
sal caching algorithm. Software - Practice and Experience 36(11-12):1179–1188, DOI
10.1002/spe.755

Scully Z, Chlipala A, Scully Z, Chlipala A (2017) A program optimization for automatic
database result caching. In: POPL 2017:Proceedings of the 44th ACM SIGPLAN Sympo-
sium on Principles of Programming Languages, pp 271–284, DOI 10.1145/3009837.3009891,
URL http://dl.acm.org/citation.cfm?doid=3009837.3009891

Selakovic M, Pradel M (2016) Performance issues and optimizations in JavaScript. In: Pro-
ceedings of the 38th International Conference on Software Engineering - ICSE ’16, ACM
Press, New York, New York, USA, pp 61–72, DOI 10.1145/2884781.2884829, URL http:
//dl.acm.org/citation.cfm?doid=2884781.2884829

Subramanian R, Smaragdakis Y, Loh GH (2006) Adaptive caches: Eﬀective shaping of cache
behavior to workloads. In: Proceedings of the Annual International Symposium on Microar-
chitecture, MICRO, pp 385–396, DOI 10.1109/MICRO.2006.7

Sun H, Xiao B, Wang X, Liu X (2017) Adaptive trade-oﬀ between consistency and performance
in data replication. Software - Practice and Experience 47(6):891–906, DOI 10.1002/spe.2462
Venketesh P, Venkatesan R (2009) A Survey on Applications of Neural Networks and Evo-
lutionary Techniques in Web Caching. IETE Technical Review 26(3):171, DOI 10.4103/
0256-4602.50701, URL http://tr.ietejournals.org/text.asp?2009/26/3/171/50701
Wang W, Liu Z, Jiang Y, Yuan X, Wei J (2014) EasyCache: a transparent in-memory data
caching approach for internetware. In: Proceedings of the 6th Asia-Paciﬁc Symposium on
Internetware on Internetware, ACM Press, New York, New York, USA, pp 35–44, DOI
10.1145/2677832.2677837, URL http://dx.doi.org/10.1145/2677832.2677837

Xu G (2012) Finding reusable data structures. Proceedings of the ACM international con-
ference on Object oriented programming systems languages and applications - OOPSLA
’12 p 1017, DOI 10.1145/2384616.2384690, URL http://dl.acm.org/citation.cfm?doid=
2384616.2384690

Xu G (2013) Resurrector: A tunable object lifetime proﬁling technique for optimizing real-
world programs. In: Proceedings of the 2013 ACM SIGPLAN International Conference on
Object Oriented Programming Systems Languages &#38; Applications, ACM, New York,
NY, USA, OOPSLA ’13, pp 111–130, DOI 10.1145/2509136.2509512, URL http://doi.
acm.org/10.1145/2509136.2509512

Xu G, Yan D, Rountev A (2012) Static detection of loop-invariant data structures. In: Noble
J (ed) ECOOP 2012 – Object-Oriented Programming, Springer Berlin Heidelberg, Berlin,
Heidelberg, pp 738–763

Xu Y, Frachtenberg E, Jiang S, Paleczny M (2014) Characterizing Facebook’s Memcached
Workload. In: IEEE Internet Computing, vol 18, pp 41–49, DOI 10.1109/MIC.2013.80
Yang Q, Zhang HH (2003) Web-log mining for predictive web caching. IEEE Transactions on
Knowledge and Data Engineering 15(4):1050–1053, DOI 10.1109/TKDE.2003.1209022
Zaidenberg N, Gavish L, Meir Y (2015) New caching algorithms performance evaluation. In:
Proceedings of the 2015 International Symposium on Performance Evaluation of Computer
and Telecommunication Systems, SPECTS 2015 - Part of SummerSim 2015 Multiconference,
DOI 10.1109/SPECTS.2015.7285291

