2
2
0
2

g
u
A
0
3

]
E
S
.
s
c
[

1
v
0
9
2
4
1
.
8
0
2
2
:
v
i
X
r
a

Model-based Fault Classiﬁcation
for Automotive Software

Mike Becker1, Roland Meyer1, Tobias Runge1,2,
Ina Schaefer1,2, Sören van der Wall1, and Sebastian Wolﬀ3

1 TU Braunschweig, Germany
{mike.becker,roland.meyer,s.van-der-wall}@tu-bs.de
2 KIT Karlsruhe, Germany
{tobias.runge,ina.schaefer}@kit.edu
3 New York University, USA
sebastian.wolff@nyu.edu

Abstract Intensive testing using model-based approaches is the stan-
dard way of demonstrating the correctness of automotive software. Un-
fortunately, state-of-the-art techniques leave a crucial and labor intensive
task to the test engineer: identifying bugs in failing tests. Our contribu-
tion is a model-based classiﬁcation algorithm for failing tests that assists
the engineer when identifying bugs. It consists of three steps. (i) Fault
localization replays the test on the model to identify the moment when
the two diverge. (ii) Fault explanation then computes the reason for the
divergence. The reason is a subset of actions from the test that is suﬃ-
cient for divergence. (iii) Fault classiﬁcation groups together tests that
fail for similar reasons. Our approach relies on machinery from formal
methods: (i) symbolic execution, (ii) Hoare logic and a new relationship
between the intermediary assertions constructed for a test, and (iii) a
new relationship among Hoare proofs. A crucial aspect in automotive
software is timing requirements, for which we develop appropriate Hoare
logic theory. We also brieﬂy report on our prototype implementation for
the CAN bus Uniﬁed Diagnostic Services in an industrial project.

1

Introduction

Intensive testing is the de-facto standard way of demonstrating the correctness
of automotive software, and the more tests the higher the conﬁdence we have in
a system [41]. Model-based approaches have been instrumental in pushing the
number of tests that can be evaluated, by increasing the degree of automation for
the testing process. Indeed, all of the following steps are fully automated today:
determining the test cases including the expected outcome, running them on
the system, and comparing the outcome to the expectation [44]. Yet, there is a
manual processing step left that, so far, has resisted automation. If the outcome
of the test and the expectation do not match, the bug has to be identiﬁed. This
is the moment the test engineer comes into play, and also the moment when
automation strikes back. The bug will not only show up in one, but rather in

 
 
 
 
 
 
2

M. Becker et al.

Test-1

A

Test-2

A

Test-3

B

[
[

req CTR set 5
5ms]
2ms] ○ res CTR ack 5

[○ 14ms] ○ req CTR get
[○ 4ms] (cid:13) res CTR ret 0

req CTR set 5
0ms]
5ms] ○ res CTR ack 5

[
[
[○ 12ms]
[○ 11ms]
[○ 1ms] ○ req CTR get
[○ 3ms] (cid:13) res CTR ret 0

req CTR log <data>
res CTR done

req CTR set 5
res CTR ack 5
req CTR log <data>
res CTR done

4ms]
[
3ms]
[
[
2ms]
[ 12ms]
[○ 56ms] ○ req CTR get
[○ 4ms] (cid:13) res CTR ret 5

Figure 1. Traces of an ECU CTR with operations set, get, and log. Faults are marked
with (cid:13), relevant events with ○. Labels A and B indicate distinct causes for the faults.

a large number of test cases, and the engineer has to go through all of them to
make sure not to miss a mistake. This is the problem we address: assist the test
engineer when searching for the bugs among a large number of failing tests.

Though our ideas may apply more broadly, we develop them in the context of
hardware-in-the-loop testing for embedded controllers (ECUs) in the automotive
industry [4]. The ﬁnal ECU with its software is given to the test engineer as a
black box. During testing, the ECU interacts with a (partly simulated) physi-
cal environment. This interaction is driven by a test suite derived from a test
model. There are several characteristics that make hardware-in-the-loop testing
substantially diﬀerent from the earlier steps in the continuous integration and
testing process (model/software/processor-in-the-loop testing). The ﬁrst is the
importance of timing requirements [2]. Second, the ECU with its software is a
black-box. Indeed, in our setting, it is provided by a supplier and the testing unit
does not have access to the development model. Third, there is a test model cap-
turing the product requirements document (PRD). It is a complex artifact that
speciﬁes the intended system behavior at a ﬁne level of detail, including logical
states, transitions, timing requirements, and message payloads. Indeed, “testing
automotive systems often requires test scenarios with a very precise sequence of
time-sensitive actions” [4]. As is good practice [4,44,41], the test model is diﬀer-
ent from the development model (it is even developed by a diﬀerent company).
Lastly, there are hundreds to thousands of tests, which is not surprising as it is
known that real-time requirements “are notoriously hard to test” [44].

Example 1. Figure 1 illustrates the task at hand (ignore the ○ marks for now).
The ﬁgure shows three traces derived from the Uniﬁed Diagnostic Services [24].
A trace is a recording of the requests and responses resulting from executing a
test case (pre-deﬁned request sequence) on the ECU under test. Each line of the
trace contains one message, carrying: (i) a time stamp indicating the time since
the last message resp. the start, (ii) the type of message, req for requests and res
for responses, (iii) an ECU identiﬁer, the recipient for requests and the sender
for responses, (iv) the name of an operation, e.g., set, and (v) optional payload.
In the ﬁrst trace, the ECU with identiﬁer CTR is requested to perform the set
operation with value 5. The ECU acknowledges that the operation was executed
successfully, repeating value 5. Subsequently, CTR receives a get request to which

Model-based Fault Classiﬁcation for Automotive Software

3

it responds with (returns) value 0. The second trace additionally requests a log
operation between set and get. In the third trace, get returns 5 instead of 0.

The get responses in all traces are marked with (cid:13) because they are faulty.
Our example PRD requires get to return the value of the latest set, unless more
than 50ms have passed since the latest (response to) set, in which case 0 has to
be returned. Assume the PRD does not specify any inﬂuence of log on set/get,
and vice versa. The ﬁrst two traces expose the same fault, indicated by A :
the set appears to have been ignored. The last trace exposes a diﬀerent fault,
(cid:117)(cid:116)
indicated by B : CTR appears to have ignored that 50ms have passed.

Our contribution is an algorithm that classiﬁes failing test cases according to
their causes. The algorithm expects as input the same information that is avail-
able to the test engineer: the test model and the traces of the failing tests. It con-
sists of three steps: fault localization, fault explanation, and fault classiﬁcation.
The fault localization can be understood as replaying a trace on the model to
identify the moment when the two diverge. In Example 1, this yields the (cid:13) marks.
The fault explanation then computes the reason for the divergence. The reason
can be understood as a small set of messages in the trace that is suﬃcient for the
divergence. In the example, this set is marked with ○. Even when removing the
remaining messages, we would still have a bug. The fault classiﬁcation groups to-
gether traces that are faulty for similar reasons. In the example, labels A and B .
Our approach relies on machinery from formal methods, following the slogan
in [4]: “more formal semantics are needed for test automation”. Behind the fault
localization is a symbolic execution [28,13]. The challenge here is to summa-
rize loops in which time passes but no visible events are issued. We solve the
problem with a widening approach from abstract interpretation [6]. Our fault
explanation [51,18,3,39,17,19,27] is based on Hoare logic [5,43]. The challenge
is to identify messages as irrelevant (for making the test fail), if they only let
time pass but their eﬀect is dominated by earlier parts of the test. We achieve
this using a new relationship between the assertions in the Hoare proof that is
constructed for the test at hand. The fault classiﬁcation [50,49] equates Hoare
proofs [37]. The challenge is again related to timing: the precise moments in
which messages arrive will be diﬀerent from test to test. We propose a notion
of proof template that allows us to equate Hoare proofs only based on timing
constraints satisﬁed by the underlying tests. The precise timing does not matter.
We implemented the classiﬁcation in a project with the automotive industry.
Our implementation targets the CAN bus Uniﬁed Diagnostic Services. The test
model has all the features mentioned above: real time, messages, and numerical
payloads. It is generated from a high-level language (not discussed here) and has
12k states and 70k transitions. Our approach is practical: we process test suits
of up to 1000 tests of average length 40 with outliers of up to 2500 in 24 minutes.
One may wonder why we classify tests at all. Since they are derived from a
test model, why not group them by the functionality they test or coverage they
achieve? The point is that functionality and coverage are only means of exposing
faults [49]. The faults are what matters for the test engineer, and the same fault
will show up in tests for diﬀerent functions. Our experiments conﬁrm this: we

4

M. Becker et al.

discover previously undetected faults in tests that targeted functions diﬀerent
from the failing one. We are particularly successful with faults involving timing,
which are largely function independent and therefore admit a high degree of
non-determinism. Taking a step back, tests are designed by functionality or
coverage, because it is hard to anticipate or even formulate possible faults in
advance [50,49,44,46]. Our explanation step makes the notion of a fault precise,
and allows us to obtain the classiﬁcation the engineer needs to write a test report.
Another question is whether we approach the problem from the wrong side.
There is a large body of work on test suite minimization [49,35]. So why classify
tests a posteriori when we could have executed less tests in the ﬁrst place? The
answer is that test suite minimization techniques are known to reduce the fault
detection eﬀectiveness, as demonstrated in the famous WHLM [47], WHMP [48],
and Siemens [40] studies. This is inacceptable in the automotive sector.

2 Formal Model

We introduce a class of automata enriched by memory and clocks to model PRDs.
A so-called PRD automaton is a tuple A = (Q, →, S , E , V , C ) with a ﬁnite set
of states Q, a ﬁnite transition relation → among states, initial states S ⊆ Q,
a ﬁnite set of events E , a ﬁnite set of memory variables V , and a ﬁnite set of
clocks C . Variables and clocks are disjoint, V ∩C = ∅. Transitions take the form
p−−−−→e, g, up q with states p, q ∈ Q, event e ∈ E , guard g, and update up. Addition-
ally, there are transitions p−−−−−→∆, g, up q that react on time progression, denoted by
the special symbol ∆ /∈ E . Guards are Boolean formulas over (in)equalities of
memory variables, clocks, and constants. We assume a strict typing and forbid
(in)equalities among memory variables and clocks. Updates are partial functions
that may give new values to variables v , up(v ) ∈ Z, or reset clocks c, up(c) = 0.
Lifting variable updates from values to terms (over variables) is straightforward.
The runtime behavior of PRD automata is deﬁned in terms of labeled transi-
tions between conﬁgurations. A conﬁguration of A is a tuple cf = (p, ϕ) consist-
ing of a state p ∈ Q and a total valuation ϕ : V → Z ∪ C → R≥0 of variables
and clocks. The conﬁguration is initial if p ∈ S is initial (no conditions on ϕ).

Valuations ϕ are aﬀected by the progression of time t and updates up. Pro-
gressing ϕ by t yields a new valuation ϕ + t, coinciding with ϕ on all variables
v and advancing all clocks c by t, (ϕ + t)(c) = ϕ(c) + t. To apply up to ϕ, we
(ϕ) = ϕ(cid:48) such that
introduce the transformer
(cid:75)

. It yields a new valuation

up

up

(cid:74)

(cid:75)

(cid:74)

ϕ(cid:48)(v ) = up(v ) (cid:54)= ⊥ ? up(v ) : ϕ(v )

and

ϕ(cid:48)(c) = up(c) (cid:54)= ⊥ ? 0 : ϕ(c) .

PRD automata A process ﬁnite traces w = s1 . . . sn of events and time pro-
gressions, si ∈ E ∪ R≥0. Events are instantaneous and time progressions make
explicit the passing of time. A basic run (p1, ϕ1)−−→s1
· · · −−→sn (pn+1, ϕn+1) of A on
w is a sequence of steps where (p1, ϕ1) is initial. Steps (p, ϕ)−→e (q, ϕ(cid:48)) for events
e ∈ E are due to transitions in A, so they satisfy the following two conditions:
(i) There is a transition p−−−−→e, g, up q such that g is enabled. Enabledness means

that ϕ is a model of g, written ϕ |= g.

Model-based Fault Classiﬁcation for Automotive Software

5

AE

p0

∆

req CTR log <data>, true, ∅

req CTR get, true, ∅

req CTR set <val>, true, ∅

p2

∆

p1

∆

res CTR done, true, ∅

p0

res CTR ret <val>, ctx = <val>, ∅

p3

∆

res CTR ack <val>, true, {ctx (cid:55)→ <val>}

A∆

E (cid:48)

p4

∆
clk < 55, ∅

∆, 50 ≤ clk < 55, {ctx (cid:55)→ 0}

res CTR ack <val>, true, {clk (cid:55)→ 0}

res CTR fail <val>, true, ∅

∆, true, ∅

p4

p5

∆

Figure 2. Model AE × A∆ for the ECU CTR from Example 1. Automaton AE speciﬁes
operations log, get, and set. Automaton A∆ speciﬁes how variable ctx is reset. We
omit the guards true and updates ∅ on ∆-loops. We use E (cid:48) (cid:44) E \{res CTR ack <val>}.

(ii) The valuation ϕ(cid:48) is induced by the transformer for up, ϕ(cid:48) =
Similarly, steps (p, ϕ)−→t (q, ϕ(cid:48)) taking time t ∈ R≥0 require:

(ϕ).

up
(cid:74)

(cid:75)

(i) There is a ∆-transition p−−−−−→∆, g, up q enabled after waiting t time, ϕ + t |= g.
(ii) Valuation ϕ(cid:48) is induced by clock progression plus up, ϕ(cid:48) =
(ϕ + t).
Finally, there are stuttering steps (p, ϕ)−→0 (p, ϕ) which have no requirements.

up
(cid:74)

(cid:75)

Next, we lift basic runs to allow for multiple ∆-transitions during a single time
progression t in w . This is needed to support complex behavior while waiting,
as seen in Example 1. We rewrite w by splitting and merging time progressions.
More precisely, we rewrite w into w (cid:48) along these equivalences:

w1.w2 ≡ w1.0.w2

and

w1.t.w2 ≡ w1.t1.t2.w2 if

t = t1 + t2 .

(TEQ)

Then, we say that A has a run on w if there is w (cid:48) with w (cid:48) ≡ w so that A has a
basic run on w (cid:48). The speciﬁcation L(A) induced by A is the set of all traces w on
which A has a run. Readers familiar with hybrid systems will observe that our
rewriting produces ﬁnite decompositions only, thus excludes zeno behavior [1].
In practice, models have many transitions between two states in order to cap-
ture state changes that ignore parts of the event or accept a large number of pos-
sible values. To avoid PRD automata growing unnecessarily large, we use regular
expressions instead of single events as transition labels. The automaton model
presented so far naturally extends to such a lift. Our implementation integrates
this optimization, see §7. For simplicity, we stick to vanilla automata hereafter.

Example 2. Automata AE and A∆ from Figure 2 specify CTR from Example 1.
Automaton AE addresses get, log, and set. The set request takes an arbitrary
value <val> as parameter. As discussed above, we use <val> as shorthand which
can be translated on-the-ﬂy into vanilla automata. The set request is always
enabled and does not lead to updates. It may be followed by an ack, indicating
success, or a fail response. If successful, variable ctx is updated to <val>. The
reset of ctx after 50ms is implemented by A∆. Operations get and log are similar.

6

M. Becker et al.

Automaton AE does not specify any timing behavior, all its states have an
always-enabled ∆-self-loop without updates. The timing behavior is speciﬁed by
automaton A∆. It uses ack responses as a trigger to reset the timer clk and then
waits until clk holds a value of at least 50. Once the threshold is reached, the
∆-transition from p4 to p5 setting ctx to 0 becomes enabled. Here, A∆ allows
for slack: the reset must happen within 5ms once 50ms have passed. Within
these 5ms, A∆ may choose to cycle in p4 without resetting or move to p5 while
resetting ctx . In practice, this kind of slack is common to account for the inability
of hardware to execute after exactly 50ms, as a guard like clk ≤ 50 would require.
The overall speciﬁcation of our example is the composition AE × A∆. The
cross-product is standard: a step can be taken only if both AE and A∆ can take
(cid:117)(cid:116)
the step. We do not go into the details of operations over automata.

3 Fault Localization

We propose a method for localizing faults in traces w . Intuitively, we do so by
letting A run on w . If for some preﬁx w (cid:48).s of w there is no step to continue the
run, i.e., w (cid:48) ∈ L(A) but w (cid:48).s /∈ L(A), then s is a fault and w (cid:48).s its witness.

Technically, identifying faults s in w is more involved. Establishing w (cid:48) ∈ L(A)
requires us to ﬁnd w (cid:48)(cid:48) ≡ w (cid:48) and a basic run of A on w (cid:48)(cid:48). Establishing w (cid:48).s /∈ L(A),
however, requires us to show that there exists no basic run of A on w (cid:48).s at all. It
is not suﬃcient to show that the single basic run witnessing w (cid:48) ∈ L(A) cannot
be extended to w (cid:48).s. We have to reason over all ˜w ≡ w (cid:48).s and over all basic runs
on them. To cope with this, we encode symbolically all such basic runs of A as
a Hoare proof. The Hoare proof can be thought of as a certiﬁcate for the fault.
Interestingly, our techniques for fault localization (§3), explanation (§4), and
classiﬁcation (§5) do not rely on the exact form of Hoare proofs or how they
are obtained—any valid proof will do. Hence, we prefer to stay on the semantic
level. We discuss how to eﬃciently generate the necessary proofs in §6. Note that
the timing aspect of our model requires us to develop novel Hoare theory in §6.

Symbolic Encoding We introduce a symbolic encoding to capture inﬁnitely
many conﬁgurations in a ﬁnite and concise manner.

A symbolic conﬁguration is a pair cf (cid:93) = (p, F ) where p is a state and F is a
ﬁrst-order formula. We use F to encode potentially inﬁnitely many variable/clock
valuations ϕ. We say F denotes ϕ if ϕ is a model for F , written ϕ |= F .

A condition P is a ﬁnite set of symbolic conﬁgurations. We write (p, ϕ) |= P
if there is (p, F ) ∈ P with ϕ |= F . We also write P (cid:118) R if cf |= P implies cf |= R
for all cf . If P (cid:118) R and R (cid:118) P , we simply write P = R. The initial condition is
Init (cid:44) { (p, true) | p ∈ S } and the empty condition is false = ∅. For simplicity,
we assume that conditions contain exactly one symbolic conﬁguration per state,
as justiﬁed by the next lemma.

Lemma 1. P ∪ {(p, F ), (p, G)} = P ∪ {(p, F ∨ G)} and P ∪ {(p, false)} = P .

Model-based Fault Classiﬁcation for Automotive Software

7

Test-1

{true} [ 5ms] {true} req CTR set 5 {true} [ 2ms] {true} res CTR ack 5
{(p0, p4 : ctx (cid:54)= 0 ∧ clk < 32)} [14ms] {(p0, p4 : ctx (cid:54)= 0 ∧ clk < 46)} req CTR get
{(p2, p4 : ctx (cid:54)= 0 ∧ clk < 46)} [ 4ms] {(p2, p4 : ctx (cid:54)= 0)} res CTR ret 0 {false}

Test-2

{true} [ 0ms] {true} req CTR set 5 {true} [ 5ms] {true} res CTR ack 5
{(p0, p4 : ctx (cid:54)= 0 ∧ clk < 23)} [12ms] {(p0, p4 : ctx (cid:54)= 0 ∧ clk < 35)} req CTR log <data>
{(p3, p4 : ctx (cid:54)= 0 ∧ clk < 35)} [11ms] {(p3, p4 : ctx (cid:54)= 0 ∧ clk < 46)} res CTR done
{(p0, p4 : ctx (cid:54)= 0 ∧ clk < 46)} [ 1ms] {(p0, p4 : ctx (cid:54)= 0 ∧ clk < 47)} req CTR get
{(p2, p4 : ctx (cid:54)= 0 ∧ clk < 47)} [ 3ms] {(p2, p4 : ctx (cid:54)= 0)} res CTR ret 0 {false}

A

A

Figure 3. Hoare proofs for Test-1 and Test-2.

Later, we will use conditions P below quantiﬁers ∃x.P and in the standard
Boolean connectives G ⊕ P with formulas G. We lift those operations to condi-
tions by pushing them into the symbolic conﬁgurations of P as follows:

∃ x. P (cid:44) {(p, ∃ x. F ) | (p, F ) ∈ P } and G ⊕ P (cid:44) {(p, G ⊕ F ) | (p, F ) ∈ P } .

Finding Faults We localize faults in traces w = s1 . . . sn. This means we check
whether or not A has a run on w . To do so, we rely on a Hoare proof for w
which takes the form

{ P0 } s1 · · · { Pi−1 } si { Pi } · · · sn { Pn }

where every triple { Pi } si { Pi+1 } is a Hoare triple. Intuitively, the Hoare triple
means: every step for si starting in a conﬁguration from Pi leads to a conﬁgura-
tion in Pi+1. Hoare triples are deﬁned to be insensitive to trace equivalence:
|= { P } s { R } : ⇐⇒ ∀cf , cf (cid:48), w (cid:48). cf |= P ∧ s ≡ w (cid:48) ∧ cf −−→w (cid:48) cf (cid:48) =⇒ cf (cid:48) |= R .

If the condition is satisﬁed, we call the Hoare triple valid. For brevity, we write
{ P } w (cid:48).s { S } if there is R so that { P } w (cid:48) { R } and { R } s { S } are both valid.
Strengthening resp. weakening the precondition P resp. postcondition R pre-
serves validity: P (cid:48) (cid:118) P and |= { P } s { R } and R (cid:118) R(cid:48) implies |= { P (cid:48) } s { R(cid:48) }.
Now, ﬁnding faults boils down to checking the validity of Hoare triples/proofs.
It is easy to see that A has no run on w if and only if |= { Init } w { ∅ }, where
Init = { cf
| cf initial } is the set of initial conﬁgurations in A. Hence, s is a fault
and the preﬁx w (cid:48).s of w its witness if there is P (cid:54)= ∅ so that |= { Init } w (cid:48) { P }
and |= { P } s { ∅ } are both valid Hoare triples/proofs.
Lemma 2. If |= { Init } w { P } s { ∅ } and P (cid:54)= ∅, then w .s witnesses fault s.

Example 3. Figure 3 gives proofs that perform fault localization in Test-1 and
Test-2 from Figure 1. The beginning of both traces is irrelevant for the fault, so
true is used as intermediate condition. Then, the conditions track the amount
of time that passes in the form of an upper bound on clock clk . Since clk stays
below 50ms, variable ctx is never reset by A∆. Hence, get must not return 0.
But because get does return 0 in the trace, we arrive at false—it is a fault. (cid:117)(cid:116)

8

M. Becker et al.

The Hoare proof certifying witness w .s is input to the fault explanation and
classiﬁcation in the next sections. As stated earlier, we defer the generation of
Hoare proofs (by means of strongest postconditions and weakest preconditions)
to §6, as it is orthogonal to fault explanation and classiﬁcation.

4 Fault Explanation

We analyze the Hoare proof certifying the fault of a witness generated in §3. Our
goal is to extract the events that contribute to the fault and dispose of those that
are irrelevant. The result will be another valid Hoare proof that concisely explains
the fault. To determine the concise proof, assume the Hoare proof certifying the
fault can be partitioned into { Init } w1 { P } w2 { R } w3 { Pk }. If P denotes less
conﬁgurations than R, P (cid:118) R, we say that w2 is irrelevant (the events therein).
To see this, consider some conﬁguration cf
leads
to some cf (cid:48) |= R which in turn leads to the fault by executing w3. However,
cf |= R already holds. So, we can just execute w3 from cf to exhibit the fault—
w2 is irrelevant indeed.

|= P . Executing w2 from cf

When timing plays a role in the fault, one might not be able to establish
the simple inclusion P (cid:118) R. Removing w2 altogether also removes the time that
passes in it. However, it might be this passing of time, rather than the events,
that leads to the fault. Therefore, we also check if the events (and the events only)
in w2 are irrelevant. This is the case if waiting has the same eﬀect as performing
full w2. Technically, we check the validity of the triple { P } w2|R≥0 { R }. The
projection w2|R≥0 removes all events E from w2: e|R≥0 = (cid:15) and t|R≥0 = t. The
validity of the triple captures our intuition: any conﬁguration cf |= P can simply
wait (taking ∆-transitions) for the same amount as w2 and arrive in cf (cid:48) |= R from
which w3 and the fault are executable—the removed events w2|E are irrelevant.
We apply the above reasoning—both P (cid:118) R as well as |= { P } w2|R≥0 { R }—
to all partitionings of the given proof to identify the irrelevant sequences. The
remaining events and time progression all contribute to the fault. The result is
the most concise explanation of the fault.

Unfortunately, our pruning rules are not conﬂuent, meaning that diﬀerent
sequences of irrelevance checks may lead to diﬀerent explanations. A witness may
have more than one explanation if two irrelevant sequences partially overlap. To
see this, consider the following (special case) partitioning of the witness’ proof

{ Init } w1 { P } w2 { R } w3 { P } w4 { R } w5 { false } .

Here, we deem irrelevant w2.w3 and w3.w4. However, we cannot remove w2.w3.w4
entirely because the resulting proof might not be valid, which requires P (cid:118) R.
Even removing the intersection w3 of the irrelevant sequences may not produce
a valid proof as R (cid:118) P might not hold either. The same problems arise if
only (w2.w3)|E and/or (w3.w4)|E is irrelevant. We argue that this is desired:
the witness is, in fact, a witness for two diﬀerent faults, explained by w1.w4.w5
resp. w1.w2.w5. Overall, we compute all combinations of explanations in case

Model-based Fault Classiﬁcation for Automotive Software

9

there are overlapping irrelevant sequences. While this gives exponentially many
combinations in theory, we rarely ﬁnd overlaps in practice.

Example 4. We give the fault explanation for the proof of Test-2 in Figure 3. As
expected, both events req CTR log <data> and res CTR done are irrelevant. The
condition P = { (p0, p4 : ctx (cid:54)= 0 ∧ clk < 23) } before the log request reaches R =
{ (p0, p4 : ctx (cid:54)= 0 ∧ clk < 47) } after the log response. This remains true even
when removing both events. Indeed, { P } [12ms][11ms][ 1ms] { R } is a valid
(cid:117)(cid:116)
Hoare triple and thus justiﬁes removing the events.

5 Fault Classiﬁcation

We propose a classiﬁcation technique that groups together witnesses exhibiting
the same or a similar fault. The input to our classiﬁcation is a set W of witness
explanations as constructed in §4. The result of the classiﬁcation is a partition
of W into disjoint classes W = W1 (cid:93) · · · (cid:93) Wm. The partitioning is obtained by
factorizing W along an equivalence ∼ relating witness explanations with similar
faults. If ∼ is eﬀectively computable, so is the factorization. We focus on ∼.

Intuitively, two explanations are similar and thus related by ∼, if comprised
of the same sequence of Hoare triples, that is, the same sequence of events and
intermediary assertions. This strict equality, however, does not work well when
timing is involved. Repeatedly executing the same sequence of events is expected
to observe a diﬀerence in timing due to ﬂuctuations in the underlying hardware.
Moreover, explanations have already been stripped by irrelevant sequences the
events and duration of which might diﬀer across explanations.

To make up for these discrepancies, we relate explanations that are equal
up to similar clocks. Consider an (in)equality F over clocks C . We can think
of F , more concretely its solutions, as a polytope M ⊆ R|C |. Then, two clock
assignments ϕ, ϕ(cid:48) ∈ R|C | are similar if they agree on the membership in M . That
is, ϕ and ϕ(cid:48) are similar if ϕ, ϕ(cid:48) ∈ M or ϕ, ϕ(cid:48) /∈ M . The polytope M we consider
will stem from the transition guards in A. Similarity thus means that A cannot
distinguish the two clock assignments—they fail for the same reason.

Clock similarity naturally extends to sets of polytopes. The set of polytopes
along which we diﬀerentiate clock assignments is taken from a proof template. A
proof template for a trace is a unique Hoare proof where placeholders are used
instead of actual time progressions. Hence, the explanations under consideration
are instances of the template, i.e., can be obtained by replacing the placeholders
with the appropriate time progressions. More importantly, the template gives
rise to a set of atomic constraints from which all polytopes appearing in the
explanations can be constructed (using Boolean connectives). Overall, this means
that two explanations are similar if the clocks they allow for are similar wrt. the
polytopes of the associated proof template, meaning that A cannot distinguish
them and thus fails for the same reason.

A proof template for events e1 . . . ek is a valid Hoare proof of the form

{ Init } u0 · · · { P2i−1 } ei { P2i } ui { P2i+1 } · · · uk { false }

10

M. Becker et al.

This proof is a template because u = u0, . . . , uk are symbolic time progressions,
i.e, they can be thought of as variables rather than actual values from R≥0. An
instance of the template is a valid Hoare proof

{ Init } t0 · · · { R2i−1 } ei { R2i } ti { R2i+1 } · · · tk { false }

with actual time progressions t = t0, . . . , tk such that the Pi subsume the Ri for
the given choice of symbolic time progression, Ri (cid:118) Pi[u (cid:55)→ t].

For the classiﬁcation to work, we require the following properties of templates:

(C1) the template is uniquely deﬁned by the sequence u0.e1 . . . ek.uk, and
(C2) the symbolic conﬁgurations appearing in the Pi are quantiﬁer-free.

The former property associates a unique template to every trace. This is neces-
sary for a meaningful classiﬁcation via templates. The latter property ensures
that the atomic constraints we extract from the template (see below) will contain
only clocks from C . This is necessary for equisatisﬁability to be meaningful. In
§6 we show that weakest preconditions generate appropriate templates.

An atomic clock constraint is an (in)equality over symbolic time progressions
and ordinary clocks (from C ). We write acc(P ) for all such constraints syntac-
tically occurring in P . For Pi from the above proof template, acc(Pi) is a set
of building blocks from which the Ri of all instantiations can be constructed.
Moreover, A cannot distinguish time progression beyond acc(Pi), making them
ideal candidates for judging similarity.

We turn to the deﬁnition of the equivalence relation ∼. To that end, consider

two explanations α, β of the following form

α:

β:

{ Init } · · · { R2i−1 } ei { R2i } ti { R2i+1 } · · · { false }
{ Init } · · · { R(cid:48)
2i+1 } · · · { false } .

2i−1 } ei { R(cid:48)

2i } t (cid:48)

i { R(cid:48)

The events e1, . . . , ek match in both explanations, but the time progressions t
and t (cid:48) may diﬀer. (Explanations with distinct event sequences are never related
by ∼.) Both explanations are instances of the same proof template σ,

σ:

{ Init } · · · { P2i−1 } ei { P2i } ui { P2i+1 } · · · { false } .

Now, for α and β to be similar, α ∼ β, we require the Ri and R(cid:48)
i to satisfy the
exact same atomic clock constraints appearing in Pi relative to the appropriate
instantiation of the symbolic clock values. It is worth stressing that we require
satisﬁability, not logical equivalence, because we want the clocks to be similar,
not equal. We write SAT(F ) if F is satisﬁable, that is, if there is an assignment ϕ
to the free variables in F such that ϕ |= F . Formally then, we have:

α ∼ β

iﬀ

∀i ∀F ∈ acc(Pi). SAT(F [u (cid:55)→ t]) ⇐⇒ SAT(F [u (cid:55)→ t (cid:48)]) .

It is readily checked that ∼ is an equivalence relation, that is, is reﬂexive, sym-
metric, and transitive, as alluded to in the beginning. Transitivity, in particular,
is desirable in our use case. First, it means that all explanations from a class Wi
of W are pairwise similar, that is, exhibit the same fault. Second, the partitions

Model-based Fault Classiﬁcation for Automotive Software

11

are guaranteed to be disjoint. Finally, it allows for the partitioning of W to be
computed eﬃciently (by tabulating the result of the SAT queries), provided the
SAT queries are eﬃcient for the type of (in)equalities used.

Lemma 3. Relation ∼ is an equivalence relation.

Example 5. We classify the explanations of
Test-1 and Test-2. Recall from Example 4
that the explanations correspond to the
proofs from Figure 3 with the log events
removed. Both explanations have the same
sequence of events. Figure 4 gives their
common template. The atomic clock con-
straints are u1 + u2 < 50, clk + u1 < 50,
and clk + u1 + u2 < 50. Test-1 and Test-2
are similar because each clock constraint is
satisﬁable after instantiating the time pro-
gressions with the values in the respective
trace. Hence, our classiﬁcation groups the
explanations together, Test-1 ∼ Test-2. (cid:117)(cid:116)

6 Hoare Proofs with Timing

Template<Test-1, Test-2>

A

{(p1, p4 : u1 + u2 < 50)}
res CTR ack 5
{(p0, p4 : ctx (cid:54)= 0 ∧ clk + u1 + u2 < 50)}
[ u2ms]
{(p0, p4 : ctx (cid:54)= 0 ∧ clk + u1 < 50)}
req CTR get
{(p2, p4 : ctx (cid:54)= 0 ∧ clk + u1 < 50)}
[ u1ms]
{(p2, p4 : ctx (cid:54)= 0)}
res CTR ret 0
{false}

Figure 4. Proof template for the ex-
planations of Test-1 and Test-2.

For the techniques presented so far to be useful, it remains to construct Hoare
proofs for traces w . Strongest postconditions and weakest preconditions are the
standard way of doing so. The former yields eﬃcient fault localization (§3).
The latter satisﬁes the requirements for templates (§5). Moreover, interpolation
between the two produces concise proofs beneﬁcial for fault explanations (§4).

It is worth pointing out that the aforementioned concepts are well-understood
for programs and ordinary automata. However, they have not been generalized
to a setting like ours where timing plays a role. Indeed, works like [20,42,23,22]
involve timing, but do not develop the Hoare theory required here.

Strongest Postconditions We compute the post image, that is, make precise
how A takes steps from symbolic conﬁgurations. A step from a symbolic conﬁg-
uration (p, F ) due to transition p−−−−−→∆, g, up q on time progression t can be taken
if the guard is enabled after waiting for t time. After waiting, all clocks c are
c(cid:48) = c + t. This means before waiting we have c = c(cid:48) − t. However, clocks are
always non-negative, c(cid:48) − t ≥ 0. Overall, we replace in F all clocks by their old
versions and enforce non-negativity, F (cid:48) = F [C (cid:55)→ C − t] ∧ C ≥ t. It remains to
check guard g and apply update up. It is easy to see that the set of valuations in
F (cid:48) satisfying g is precisely G = F (cid:48) ∧ g. To perform a singleton update { x (cid:55)→ y },
we capture the new valuation of x by the equality x = y. To avoid an inﬂuence
of the update of x on other variables/clocks, we have to rewrite G to not contain
x. This is needed as G might use x to correlate other variables/clocks—we want

12

M. Becker et al.

to preserve these correlations without aﬀecting them. We use an existential ab-
straction that results in G (cid:48) = ∃z. G[x (cid:55)→ z] ∧ x = y. Then, the post image is
(q, G (cid:48)). For stuttering steps, we add the original conﬁguration (p, F ) to the post
image. Steps due to events from E are similar.

We deﬁne a symbolic transformer that implements the above update of the

symbolic encoding F to G (cid:48) in the general case:

g|{x (cid:55)→ y}
(cid:75)
(cid:74)

(cid:93)
t (F ) (cid:44) ∃z. (F [C (cid:55)→ C − t ] ∧ C ≥ t ∧ g)[x (cid:55)→ z] ∧ x = y

where x is short for a sequence x1, . . . , xm of variables/clocks. We arrive at:

post (cid:93)
post (cid:93)

t (P ) (cid:44) { (q,
e (P ) (cid:44) { (q,

g|up

g|up

(cid:74)

(cid:74)

(cid:75)

(cid:75)

(cid:93)

t (F )) | (p, F ) ∈ P ∧ p−−−−−→∆, g, up q } ∪ (cid:0)t = 0 ? P : ∅(cid:1)
(cid:93)
0(F )) | (p, F ) ∈ P ∧ p−−−−→e, g, up q } .

The post image is sound and precise in the sense that it captures accurately the
steps the conﬁgurations denoted by P can take. The lemma makes this precise.

Lemma 4. cf (cid:48) |= post (cid:93)

s (P ) iﬀ there is cf |= P with cf −→s cf (cid:48).

Example 6. We apply post (cid:93) to P = {(p4, 49 ≤ clk ≤ 52)} for A∆ from Figure 2.
Recall that A∆ resets variable ctx within a 5ms after clk has reached the 50ms
mark. Indeed, post (cid:93)
1(P ) for 1ms contains both the resetting and the non-resetting
case: (p5, 50 ≤ clk ≤ 53 ∧ ctx = 0) and (p4, 50 ≤ clk ≤ 53).

The post image is still lacks a way to commute with the (TEQ) congruences.
1(P )) witnesses the reset via condition 55 ≤ clk ≤ 58 ∧ ctx = 0
6(P ), which is false since all
(cid:117)(cid:116)

While post (cid:93)
for both p4 and p5, it is not equivalent to post (cid:93)
transitions in p4 are disabled for a full 6ms wait.

5(post (cid:93)

While the post image captures the individual steps of basic runs on traces w ,
we have to consider the basic runs of all traces w (cid:48) ≡ w to generate a Hoare proof
for w . Basically, the (TEQ) equivalences state that the time progressions between
events can be split/merged arbitrarily. To that end, we deﬁne the strongest
postcondition sp which inspects all basic runs simultaneously, intuitively, by
rewriting according to the (TEQ) equivalences on-the-ﬂy. Wlog., we avoid the
merging case of (TEQ) by assuming that w is normalized in the sense that
every event is preceded and succeeded by exactly one time progression. Then,
for events e the strongest postcondition merely applies the post image to e. For
time progressions t, the strongest postcondition considers all decompositions of t
into fragments t1, . . . , tk that add up to t and applies the post image iteratively
to all the ti. This includes stuttering where 0 is rewritten to 0. . . 0. If there
are loops in A, the strongest postcondition might need to consider inﬁnitely
many decompositions. We address this problem by enumerating decompositions
of increasing length and applying to each decomposition a widening ∇ with the
following properties: (i) the result of the widening is (cid:118)-weaker than its input,
Pi (cid:118) ∇(P1, · · ·, Pk) for all i, and (ii) the widening stabilizes after ﬁnitely many
iterations for (cid:118)-increasing sequences, P1 (cid:118) P2 (cid:118) · · · implies that there is k so

Model-based Fault Classiﬁcation for Automotive Software

13

that ∇(P1, · · ·, Pi) = ∇(P1, · · ·, Pi+1) for all i ≥ k. We write ∇(Pi)i∈N and mean
the stabilized ∇(P1, · · ·, Pk). Given a widening, the strongest postcondition is:

(cid:16)

spt (P ) (cid:44) ∇
spe (P ) (cid:44) post (cid:93)

∃t1, · · ·, ti. t = t1 + · · · + ti ∧ post (cid:93)
sps.w (P ) (cid:44) spw ◦ sps (P )

e (P )

ti ◦ · · · ◦ post (cid:93)

(cid:17)
t1 (P )
sp (P , w ) (cid:44) spw (P )

i∈N

where the t1, . . . , ti are fresh. Observe that the sequence of post images in spt is
(cid:118)-increasing: one can always extend the decomposition by additionally waiting
for 0 time, post (cid:93)
ti(P ). The strongest postcondition considers
all basic runs and ∇ overapproximates the reachable conﬁgurations. It is sound.

ti(P ) (cid:118) post (cid:93)

0 ◦ post (cid:93)

Lemma 5. If sp (P , w ) (cid:118) R, then |= { P } w { R }.

For the lemma to be useful, an appropriate widening ∇ is required. In gen-
eral, ﬁnding such a widening is challenging—after all, it resembles ﬁnding loop
invariants—and for doing so we refer to existing works, like [11,12,7], to name a
few. In practice, a widening may be obtained more easily. In case A is free from
∆-cycles, stabilization is guaranteed after k iterations, where k is the length of
the longest simple ∆-path. If there are ∆-cycles, stabilization is still guaranteed
after k iterations if all ∆-cycles are idempotent. A ∆-cycle is idempotent if re-
peated executions of the cycle produce only conﬁgurations that already a single
execution of the cycle produces. Interestingly, idempotency can be checked while
computing the widening: if the (k + 1)st iteration produces new conﬁgurations,
idempotency does not hold. In our setting, idempotency was always satisﬁed.
For the remainder of this paper, we assume an appropriate widening is given.

Weakest Preconditions We also compute weakest preconditions, the time-
reversed dual of strongest postconditions. Our deﬁnition will satisfy the template
requirements (C1) and (C2) from §5.

The pre image is the set of symbolic conﬁgurations that reach a conﬁguration
in A. Consider some (q, G) and p−−−−−→∆, g, up q. The pre image ﬁrst rewinds updates
up = { x (cid:55)→ y } by replacing x with y. Then, it adds a disjunct H = G[x (cid:55)→ y]∨¬g.
Adding the disjunct makes the pre image weaker; it does not aﬀect soundness
in Lemma 6 which ignores the stuck conﬁgurations denoted by (p, ¬g). Finally,
we rewind the clock progression t by replacing all clocks c in H with c + t. We
arrive at the pre image F = H [C (cid:55)→ C +t]. Transitions due to events are similar.
We deﬁne a symbolic transformer to apply the above process:

g|{x (cid:55)→ y}
(cid:74)

(cid:93)

t (G) (cid:44) (G[x (cid:55)→ y] ∨ ¬g)[C (cid:55)→ C + t] .
(cid:75)

To account for other transitions leaving p that are enabled in H , we compute
the meet (cid:117) of the per-transition pre images. Intuitively, this intersects symbolic
conﬁgurations on a per-state basis, ensuring that any conﬁguration from the pre
image either gets stuck or steps to one of the conﬁgurations we computed the
pre image for. Technically, the meet (cid:117) for sets M of symbolic conﬁgurations is:

(cid:108) M (cid:44) { (p, (cid:86)

(p,F )∈M F ) | p ∈ Q } .

14

M. Becker et al.

Notably, when considering the meet of M , we cannot understand M as a condi-
tion. This is because conditions treat symbolic conﬁgurations disjunctively and
can be normalized by Lemma 1. However, the meet is not preserved under these
transformations. We write M1 (cid:117) M2 to mean (cid:100)(M1 ∪ M2).

The discussion yields the following deﬁnition of the pre image:

pre (cid:93)

t (P ) (cid:44) (cid:108){(p,

pre (cid:93)

e (P ) (cid:44) (cid:108){(p,

g|up

g|up

(cid:74)

(cid:74)

(cid:75)

(cid:75)

(cid:93)

t (G)) | (q, G) ∈ P ∧ p−−−−−→∆, g, up q} (cid:117) (cid:0)t = 0 ? P : ∅(cid:1)
0(G)) | (q, G) ∈ P ∧ p−−−−→e, g, up q}

(cid:93)

which captures precisely forced reachability in A, as stated by the next lemma.

Lemma 6. cf |= pre (cid:93)

s (P ) iﬀ for all cf (cid:48), cf −→s cf (cid:48) implies cf (cid:48) |= P .

Example 7. We apply pre (cid:93) to P = {(p4, 49 ≤ clk ≤ 52)} for A∆ from Figure 2.
Computing pre (cid:93)
1(P ) highlights the need for the meet. The ∆-loop on p4 does not
give (p4, 48 ≤ clk ≤ 51) as precondition. Instead, it is (p4, 48 ≤ clk < 49) which
is the result of {(p4, 48 ≤ clk ≤ 51)} (cid:117) {(p4, clk ≥ 54 ∨ clk < 49)}. Indeed, A∆
reaches a non-P conﬁguration via the resetting transition to p5 if clk = 49. (cid:117)(cid:116)

The weakest precondition wp (s, R) denotes all conﬁgurations that either
step to R under s or have no step at all. Technically, the weakest precondition
repeatedly applies the pre image for all decompositions of time progressions. For
termination, we again rely on the widening ∇. Since the pre image sequence
is (cid:118)-decreasing, we turn it into an increasing sequence by taking complements.
More precisely, we use the widening ∇(P1, · · · , Pm) (cid:44) ¬∇(¬P1, · · ·, ¬Pm). The
weakest precondition is deﬁned by:

(cid:16)

wpt (P ) (cid:44) ∇
wpe (P ) (cid:44) pre (cid:93)

e (P )

∀t1, · · ·, ti. t = t1 + · · · + ti =⇒ pre (cid:93)

wpw .s (P ) (cid:44) wpw ◦ wps (P )

t1 ◦ · · · ◦ pre (cid:93)

(cid:17)
ti(P )
wp (w , P ) (cid:44) wpw (P )

i∈N

Note that wpt applies to ordinary time progressions t as well as symbolic time
progressions u appearing in proof templates (cf. §5). The weakest precondition
is sound in that it produces valid Hoare triples.

Lemma 7. If P (cid:118) wp (w , R), then |= { P } w { R }.

Concise Hoare Proofs The developed theory allows for an eﬃcient way to
produce concise Hoare proofs. We ﬁrst apply strongest postconditions to generate
an initial proof. Then, starting from the back, apply weakest preconditions and
interpolation [8]. We make this precise to simplify the initial proof.

Combining Lemmas 2 and 5 gives an eﬀective way of ﬁnding faults in traces
w = s1 . . . sn and extracting a witness: iteratively compute the strongest postcon-
dition for increasing preﬁxes of w and check if the result is unsatisﬁable. That is,
compute P = sp (s1. · · · .sk, Init) and check if P = false. If so, then (cid:98)w = s1 . . . sk
is a witness for fault sk. Otherwise, continue with the preﬁx s1 . . . sk.sk+1 which

Model-based Fault Classiﬁcation for Automotive Software

15

can reuse the previously computed P : sp (s1 . . . sk.sk+1, Init) = sp (sk+1, P ). As
per Lemma 5, the approach gives rise to the valid Hoare proof

{ Init } s1 · · · { Pi−1 } si { Pi } · · · sk { false } with Pi+1 = sp (Pi, si+1) .

It is well-known that strongest postconditions produce unnecessarily complex
proofs [33]. To alleviate this weakness, we use interpolation [8]. For two formulas
F and G with F =⇒ G, an interpolant is a formula I with F =⇒ I and I =⇒ G.
The interpolant for conditions P and R with P (cid:118) R, denoted I (P , R), results
from interpolating the symbolic conﬁgurations in P with the corresponding ones
in R. Interpolants exist in ﬁrst-order predicate logic [8,31].

From the above sp - generated proof we construct an interpolated proof

{ Init } s1 · · · { I (Pi−1, Ri−1) } si { I (Pi, Ri) } · · · sk { false }

using wp as follows. Assume we already constructed, starting from the back, the
interplolants I (Pk, Rk) through I (Pi+1, Ri+1). Now, the goal is to obtain an in-
terpolant I so that { I } si+1 { I (Pi+1, Ri+1) } is valid. The weakest precondition
for the latest interpolant yields Ri = wp (si, I (Pi+1, Ri+1)). This gives a valid
Hoare triple |= { Ri } si+1 { I (Pi+1, Ri+1) }. Our goal is to interpolate Pi and Ri.
If Pi (cid:118) Ri, we can interpolate Pi and Ri to obtain I = I (Pi, Ri).4 Otherwise,
we simply choose I = Ri. By Lemma 7 together with I (cid:118) Ri, we know that
|= { I } si+1 { I (Pi+1, Ri+1) } is valid. Overall, this constructs a valid proof.

7 Application in Automotive Software

We implemented and tested our approach on benchmarks provided by a project
partner from the automotive industry. The implementation parses, classiﬁes, and
annotates traces of ECUs running the Uniﬁed Diagnostic Services (UDS). We
turned a PRD with 350 pages of natural language specifying 23 services into a
PRD automaton of 12.5k states and 70k transitions. We evaluate our tool on
1000 traces which are processed within 24 minutes. Our tool is implemented in
C# and processes traces in the three stages explained below. It naturally supports
multi-threading for the localization, explanation, and classiﬁcation since they are
agnostic to the (set of) other traces being analyzed.

Preprocessing Stage The ﬁrst stage parses trace ﬁles and brings them into
a shape similar to Figure 1. UDS specify a request-response protocol for ECUs
communicating over a CAN bus. The traces are a recording of all messages seen
on the bus during a test run. We found the preprocessing more diﬃcult than
expected, because the trace ﬁles have a non-standard format. These problems
stem from the fact that our industrial partner creates tests partly manually and
inserts natural language annotations. A useful type of annotation that we could
extract are the positions deemed erroneous by the test environment.

4 One can show that the inclusion Pi (cid:118) Ri is always satisﬁed in our setting where
∆-cycles are idempotent and the widenings ∇ and ∇ simply enumerate all necessary
decompositions of time progressions. Appendix A.3 gives a more general property.

16

M. Becker et al.

Modeling Stage The second stage creates the test model, a PRD automaton
as deﬁned in §2. Modeling a natural language PRD is a non-trivial and time-
consuming process. To translate the PRD into an automaton, we developed
an API capable of describing services and their communication requirements.
The API supports a declarative formulation of the communication requirements
which it compiles down into an automaton. The compilation is controlled by a set
of parameters, as the PRD prescribes diﬀerent behavior depending on the ECU
version (and related static parameters). There are further high-level modeling
constructs such as regular expressions, as alluded to in §2 and seen in Figure 2.
Unfortunately, not all requirements from the PRD are restricted to the trace:
there are events internal to the ECU that are not contained in the trace ﬁles.
While our API and PRD automata are capable of expressing these requirements,
the test environment is unable to detect them. To circumvent the problem of
missing information, we over-approximated our model using non-determinism.
That is, we simply allow our model to do any of the speciﬁed behaviors for
unobservable internal events. The only downside of this is that errors dependent
on these events cannot be found during fault localization.

Analysis Stage The last stage performs fault localization (§3), explanation (§4),
and classiﬁcation (§5). We carefully inspected the results of 86 traces curated
by our industrial partner. The tests targeted one of the 23 services, yet they
contain requests and responses to a multitude of services responsible for setting
up the ECU conﬁguration. The annotations of the test environment marked 100
faults, 95 of which are also found by our fault localization. Our tool ﬁnds and
explains another 10 undetected faults, which totals to 105 explained faults. The
ﬁve faults missed by our localization are actually faulty annotations by the test
environment, which we will explain in a moment.

The classiﬁcation divides the faults into six partitions. We found that each
partition belongs to one of the following three error types: (i) ECU responds too
late (1+8); (ii) ECU fails to reset a variable upon restart (2); (iii) ECU responds
when it should not (2+1+91). Here, 1+8 means we have two partitions, one with
a single witness explanation, one with eight equivalent witness explanations.
The 91 witness explanations all refer to the function under test.

There are two partitions with error type (i). This is because the late response
is by another service and thus leads to diﬀerent control ﬂow in the automaton.
Indeed, there might be distinct root causes: diﬀerent services are likely controlled
by diﬀerent pieces of code. A similar reason produces three partitions of error
type (iii). Interestingly, the singleton partition for (i) is completely missed by the
test environment (no fault was marked). This supports our claim that the test
environment only detects faults targeted by the tests and ignores other faults.
The other partition of (i) was detected by the test environment by incident: in
some traces, the ECU response is so late that the test environment incorrectly
marks the response missing. These are the faulty marks missed by our tool.
Instead, our tool actually detects the late responses and marks them correctly.
Our tool provides a partitioning ﬁle with direct links to the trace ﬁles. It
also modiﬁes the trace ﬁles to highlight the events related to the fault (cf. §4)

Model-based Fault Classiﬁcation for Automotive Software

17

and provide an intuitive explanation of the fault. As for the latter, the user is
informed about the diﬀerence between the observed and the automaton-expected
behavior. Our manual inspection showed no incorrect classiﬁcation. That is, our
tool has never grouped together traces which test engineers would deem caused
by distinct faults. This is promising feedback because incorrect classiﬁcation is
dreaded: a single missed ﬂaw of an ECU can cause large costs. Overall, we reduce
the workload of manually inspecting 86 traces with 100 fault marks to inspecting
six representative faults that expose more misbehavior than marked by the test
environment.

8 Related Work

Fault Explanation Our work on fault explanation is related to minimizing unit
tests in [29]: tests are pruned by removing the commands that are not contained
in a backward slice from a failing instruction. With timing constraints, slicing
does not work (every command is relevant), which is why we have developed
our approach based on Hoare logic. The assertions provided by a Hoare proof
have the additional advantage of being able to prune even dependent commands
inside a slice (based on the relationship between intermediary assertions), which
leads to higher reduction rates. Similar to our approach is the fault localization
and explanation from [5,43]. That work also makes use of interpolation [32] and
is able to strip inﬁxes from a trace despite dependencies. Our fault localization
can be understood as a generalization to a timed setting where every command
contributes to the progression of time and therefore is delicate to remove.

A popular fault explanation approach that can be found in several variants in
the literature [51,18,3,39,17,19,27] is Delta debugging: starting from a failing test,
produce a similar but passing test, and take the diﬀerence in commands as an
explanation of the fault. In [51,18,39,17,19], the passing test is found by repeat-
edly testing the concrete system [18], which is impossible in our in-vitro setting.
In [3,17,27], a model checker resp. a solver is queried for a passing test resp. a
satisﬁable subset of clauses. Our Hoare proof can be understood as building up
an alternative and valid execution. Diﬀerent from a mere execution, however,
intermediary assertions provide valuable information about the program state
that we rely on when classifying tests.

The explanation from [25] divides a computation into fated and free segments,
the former being deterministic reactions to inputs and the latter being inputs
that, if controlled, avoid the fault and hence should be considered responsible.
The segments are computed via rather heavy game-theoretic techniques, which
would be diﬃcult to generalize to timed systems. A more practical variant can be
found in [45,52]. These works modify tests in a way that changes the evaluation
of conditionals. Neither can we re-run tests in an in-vitro setting, nor would we
be able to inﬂuence the timing behavior.

There is a body of literatur on statistical approaches to ﬁnding program
points that are particularly prone to errors, see the surveys [49,46]. We need to
pinpoint the precise as possible cause of a bug, instead.

18

M. Becker et al.

Fault Classiﬁcation Previous works on test case classiﬁcation follow the same
underlying principle [14,26,16,10,15,9,38,30,36]: devise a distance metric on test
cases that is used to group them. The metrics are based on properties like the
commonality/frequency of words in comments and variables in the code [10]
or the correlation of tests failing/passing in previous test runs [14]. Symbolic
execution has been used to derive more semantic properties based on the source
code location of faults [30] and the longest preﬁx a failing trace shares with some
passing trace [36]. The problem is that the suggested metrics are at best vague
surrogates for the underlying faults. Using a model-based approach, we compare
traces not against each other but against a ground truth (the PRD automaton).
Another related line of work is test case prioritization, test case selection,
and test suite minimization [49]. Although formulated diﬀerently, these prob-
lems share the task of choosing tests from a predeﬁned pool. Experiments have
shown that manually chosen test suites outperform automatically selected ones
in their ability to expose bugs [50]. To increase the number of tests that can be
evaluated manually by an expert, the literature has proposed the use of cluster-
ing algorithms to group together tests with similar characteristics (so that the
expert only has to evaluate clusters). The clustering is computed from syntactic
information (a bitwise comparison of test executions). As argued before, we use
semantic information and compute the classiﬁcation wrt. a ground truth.

On the automatic side, [37] suggests the use of Hoare proofs to classify error
traces. Our approach follows this idea and goes beyond it with the proposal of
proof templates. Proof templates seem to be precisely the information needed
to classify tests that are subject to real-time constraints. Harder et al. suggest
to minimize test suites based on likely program invariants inferred from sample
values obtained in test runs [21]. Hoare triples are more precise than invariants,
even more so as we work with ground truth rather than sample values.

Acknowledgements The results were obtained in the projects “Virtual Test
Analyzer I – III”, conducted in collaboration with IAV GmbH. The last author
is supported by a Junior Fellowship from the Simons Foundation (855328, SW).

References

1. Abadi, M., Lamport, L.: An old-fashioned recipe for real time. In: REX Workshop.

LNCS, vol. 600, pp. 1–27. Springer (1991)

2. Alur, R., Dill, D.L.: A theory of timed automata. TCS 126(2), 183–235 (1994)
3. Ball, T., Naik, M., Rajamani, S.K.: From symptom to cause: localizing errors in

counterexample traces. In: POPL. pp. 97–105. ACM (2003)

4. Bringmann, E., Krämer, A.: Model-based testing of automotive systems. In: ICST.

pp. 485–493. IEEE (2008)

5. Christ, J., Ermis, E., Schäf, M., Wies, T.: Flow-sensitive fault localization. In:

VMCAI. LNCS, vol. 7737, pp. 189–208. Springer (2013)

6. Cousot, P., Cousot, R.: Abstract interpretation: A uniﬁed lattice model for static
analysis of programs by construction or approximation of ﬁxpoints. In: POPL. pp.
238–252. ACM (1977)

Model-based Fault Classiﬁcation for Automotive Software

19

7. Cousot, P., Halbwachs, N.: Automatic discovery of linear restraints among variables

of a program. In: POPL. pp. 84–96. ACM Press (1978)

8. Craig, W.: Linear reasoning. A new form of the herbrand-gentzen theorem. J.

Symb. Log. 22(3), 250–268 (1957)

9. Dickinson, W., Leon, D., Podgurski, A.: Finding failures by cluster analysis of

execution proﬁles. In: ICSE. pp. 339–348. IEEE (2001)

10. DiGiuseppe, N., Jones, J.A.: Concept-based failure clustering. In: SIGSOFT FSE.

p. 29. ACM (2012)

11. Dillig, I., Dillig, T., Li, B., McMillan, K.L.: Inductive invariant generation via

abductive inference. In: OOPSLA. pp. 443–456. ACM (2013)

12. Flanagan, C., Leino, K.R.M.: Houdini, an annotation assistant for esc/java. In:

FME. LNCS, vol. 2021, pp. 500–517. Springer (2001)

13. Godefroid, P., Klarlund, N., Sen, K.: DART: directed automated random testing.

In: PLDI. pp. 213–223. ACM (2005)

14. Golagha, M., Lehnhoﬀ, C., Pretschner, A., Ilmberger, H.: Failure clustering without

coverage. In: ISSTA. pp. 134–145. ACM (2019)

15. Golagha, M., Pretschner, A., Fisch, D., Nagy, R.: Reducing failure analysis time:

An industrial evaluation. In: ICSE-SEIP. pp. 293–302. IEEE (2017)

16. Golagha, M., Raisuddin, A.M., Mittag, L., Hellhake, D., Pretschner, A.: Aletheia: a
failure diagnosis toolchain. In: ICSE (Companion Volume). pp. 13–16. ACM (2018)
17. Groce, A.: Error explanation with distance metrics. In: TACAS. LNCS, vol. 2988,

pp. 108–122. Springer (2004)

18. Groce, A., Visser, W.: What went wrong: Explaining counterexamples. In: SPIN.

LNCS, vol. 2648, pp. 121–135. Springer (2003)

19. Guo, L., Roychoudhury, A., Wang, T.: Accurately choosing execution runs for
software fault localization. In: CC. LNCS, vol. 3923, pp. 80–95. Springer (2006)
20. Haase, V.: Real-time behavior of programs. IEEE Transactions on Software Engi-

neering SE-7(5), 494–501 (1981)

21. Harder, M., Mellen, J., Ernst, M.D.: Improving test suites via operational abstrac-

tion. In: ICSE. pp. 60–73. IEEE (2003)

22. Haslbeck, M.P.L., Nipkow, T.: Hoare logics for time bounds - A study in meta

theory. In: TACAS (1). LNCS, vol. 10805, pp. 155–171. Springer (2018)

23. Hooman, J.: Extending hoare logic to real-time. Formal Aspects Comput. 6(6A),

801–826 (1994)

24. ISO: ISO 14229-1:2020 Road vehicles — Uniﬁed diagnostic services (UDS) — Part
1: Application layer. Standard ISO 14229-1:2020, International Organization for
Standardization, Geneva, CH (2020)

25. Jin, H., Ravi, K., Somenzi, F.: Fate and free will in error traces. In: TACAS. LNCS,

vol. 2280, pp. 445–459. Springer (2002)

26. Jordan, C.V., Hauer, F., Foth, P., Pretschner, A.: Time-series-based clustering
for failure analysis in hardware-in-the-loop setups: An automotive case study. In:
ISSRE Workshops. pp. 67–72. IEEE (2020)

27. Jose, M., Majumdar, R.: Cause clue clauses: error localization using maximum

satisﬁability. In: PLDI. pp. 437–446. ACM (2011)

28. King, J.C.: Symbolic execution and program testing. CACM 19(7), 385–394 (1976)
29. Leitner, A., Oriol, M., Zeller, A., Ciupa, I., Meyer, B.: Eﬃcient unit test case

minimization. In: ASE. pp. 417–420. ACM (2007)

30. Liu, C., Han, J.: Failure proximity: a fault localization-based approach. In: SIG-

SOFT FSE. pp. 46–56. ACM (2006)

31. Lyndon, R.: An interpolation theorem in the predicate calculus. Paciﬁc Journal of

Mathematics 9, 129–142 (1959)

20

M. Becker et al.

32. McMillan, K.L.: Interpolation and sat-based model checking. In: CAV. LNCS,

vol. 2725, pp. 1–13. Springer (2003)

33. McMillan, K.L.: Interpolation and model checking. In: Handbook of Model Check-

ing, pp. 421–446. Springer (2018)

34. de Moura, L.M., Bjørner, N.: Z3: an eﬃcient SMT solver. In: TACAS. LNCS,

vol. 4963, pp. 337–340. Springer (2008)

35. Pan, R., Bagherzadeh, M., Ghaleb, T.A., Briand, L.C.: Test case selection and
prioritization using machine learning: a systematic literature review. Empir. Softw.
Eng. 27(2), 29 (2022)

36. Pham, V., Khurana, S., Roy, S., Roychoudhury, A.: Bucketing failing tests via

symbolic analysis. In: FASE. LNCS, vol. 10202, pp. 43–59. Springer (2017)
37. Podelski, A., Schäf, M., Wies, T.: Classifying bugs with interpolants.

In:

TAP@STAF. LNCS, vol. 9762, pp. 151–168. Springer (2016)

38. Podgurski, A., Leon, D., Francis, P., Masri, W., Minch, M., Sun, J., Wang, B.:
Automated support for classifying software failure reports. In: ICSE. pp. 465–477.
IEEE (2003)

39. Renieris, M., Reiss, S.P.: Fault localization with nearest neighbor queries. In: ASE.

pp. 30–39. IEEE (2003)

40. Rothermel, G., Harrold, M.J., Ostrin, J., Hong, C.: An empirical study of the
eﬀects of minimization on the fault detection capabilities of test suites. In: ICSM.
pp. 34–43. IEEE (1998)

41. Schäuﬀele, J., Zurawka, T.: Automotive Software Engineering - Grundlagen,

Prozesse, Methoden und Werkzeuge eﬃzient einsetzen (6. Auﬂ.). Vieweg (2016)

42. Schneider, F.B., Bloom, B., Marzullo, K.: Putting time into proof outlines. In:

REX Workshop. LNCS, vol. 600, pp. 618–639. Springer (1991)

43. Schwartz-Narbonne, D., Oh, C., Schäf, M., Wies, T.: VERMEER: A tool for tracing

and explaining faulty C programs. In: ICSE (2). pp. 737–740. IEEE (2015)

44. Utting, M., Pretschner, A., Legeard, B.: A taxonomy of model-based testing ap-

proaches. Softw. Test. Veriﬁcation Reliab. 22(5), 297–312 (2012)

45. Wang, T., Roychoudhury, A.: Automated path generation for software fault local-

ization. In: ASE. pp. 347–351. ACM (2005)

46. Wong, W.E., Gao, R., Li, Y., Abreu, R., Wotawa, F.: A survey on software fault

localization. IEEE Trans. Software Eng. 42(8), 707–740 (2016)

47. Wong, W.E., Horgan, J.R., London, S., Mathur, A.P.: Eﬀect of test set minimiza-
tion on fault detection eﬀectiveness. Softw. Pract. Exp. 28(4), 347–369 (1998)
48. Wong, W.E., Horgan, J.R., Mathur, A.P., Pasquini, A.: Test set size minimization
and fault detection eﬀectiveness: A case study in a space application. J. Syst. Softw.
48(2), 79–89 (1999)

49. Yoo, S., Harman, M.: Regression testing minimization, selection and prioritization:

a survey. Softw. Test. Veriﬁcation Reliab. 22(2), 67–120 (2012)

50. Yoo, S., Harman, M., Tonella, P., Susi, A.: Clustering test cases to achieve eﬀective
and scalable prioritisation incorporating expert knowledge. In: ISSTA. pp. 201–212.
ACM (2009)

51. Zeller, A.: Isolating cause-eﬀect chains from computer programs. In: SIGSOFT

FSE. pp. 1–10. ACM (2002)

52. Zhang, X., Gupta, N., Gupta, R.: Locating faults through automated predicate

switching. In: ICSE. pp. 272–281. ACM (2006)

Model-based Fault Classiﬁcation for Automotive Software

21

A Additional Material

We provide details missing in the main part of the paper.

A.1 Missing Deﬁnitions

We lift quantiﬁcation and the standard Boolean connectives to conditions P ,
where G are formulas, as follows:

∃ x. P (cid:44) { (p, ∃ x. F ) | (p, F ) ∈ P }
G ⇒ P (cid:44) { (p, G ⇒ F ) | (p, F ) ∈ P }

G ∧ P (cid:44) { (p, G ∧ F ) | (p, F ) ∈ P }
¬P (cid:44) { (p, ¬F ) | (p, F ) ∈ P }

For x = x1, . . . , xm and y = y1, . . . , ym and C = {c1, . . . , cm} we deﬁne the

following abbreviations:

x = y (cid:44)

m
(cid:94)

xi = yi

C ≥ t (cid:44) (cid:94)

i=1
c ≥ t

c∈C

G[x (cid:55)→ z] (cid:44) G[x1 (cid:55)→ z1, . . . , xm (cid:55)→ zm]

F [C (cid:55)→ C − t ] (cid:44) F [c1 (cid:55)→ c1 − t, . . . , cn (cid:55)→ cn − t]

A.2 Checking Inclusion

We devise an eﬀective check for inclusions of the form P (cid:118) R. Towards such
a check, consider an abstract conﬁguration (p, F ) ∈ P . Note that we cannot
simply test membership of (p, F ) in R since the conﬁgurations in R might be
strictly weaker or just syntactically diﬀerent. Instead, we test whether there is
some (p, G) ∈ R such that F =⇒ G is valid. The implication ensures that the
valuations denoted by F are also denoted by G. More formally, ϕ |= F implies
ϕ |= G and thus also (p, ϕ) |= P implies (p, ϕ) |= R, for any valuation ϕ. The
implications can then be discharged by oﬀ-the-shelf SMT solvers like Z3 [34],
which is what we use.

The inclusion check P (cid:118) R among conditions proposed above allows for an
elegant and powerful tuning mechanism of the resulting explanation’s precision.
In our experience, it might be helpful to ignore certain parts of the model A, for
instance, the timing behavior of speciﬁc clocks. To reﬁne the explanation in this
manner, one can choose a subset x ⊆ V ∪ C of variables and clocks that will
be ignored during the construction of the explanation. To ignore x, we simply
adapt the above check to integrate an existential abstraction as follows:

(p, F ) (cid:118)x (p, G) : ⇐⇒ (∃x. F ) =⇒ (∃x. G) .

Then, P is subsumed by R wrt. to the hidden set x, denoted by P (cid:118)x R, if for
all (p, F ) ∈ P there is (p, G) ∈ R such that (p, F ) (cid:118)x (p, G). This way, changes
to the valuation of the variables/clocks in x are ignored. Hence, the explanation
also becomes incognizant of x.

22

M. Becker et al.

A.3 Precision of sp and wp

The strongest postcondition avoids false-positives in the sense that it will not
produce false for any w if A has a run on w .

Lemma 8. If A has a run on w , then sp (Init, w ) (cid:54)= false.

Lemmas 5 and 7 state that the strongest postcondition and the weakest
precondition are sound in that they generate valid Hoare triples. The converse is
also ture, provided the widenings ∇ and ∇ are suﬃciently precise. Formally, the
widening ∇ (narrowing ∇) is precise, if ∇M = (cid:83) M (∇M = (cid:100) M ) for all M .
Observe that if ∇ is precise, so is ∇.

Lemma 9. If ∇ is precise, then |= { P } w { R } implies sp (P , w ) (cid:118) R.

Lemma 10. If ∇ is precise, then |= { P } w { R } implies P (cid:118) wp (w , R).

As a consequence, interpolation between strongest postconditions and weak-

est preconditions is possible whenever ∇ and ∇ are precise.

Corollary 1. If ∇ is precise, then sp (P , w ) (cid:118) R iﬀ P (cid:118) wp (w , R).

B Missing Proofs

Proof (Lemmas 1 to 3). The claims follow immediately from the deﬁnition. (cid:117)(cid:116)

(cid:75)

(cid:74)

up

Proof (Lemma 4). Consider the interesting case cf = (p, ϕ)−→t (q, ϕ(cid:48)) = cf (cid:48) of
a non-stuttering step that is due to a transition of the form p−−−−−→∆, g, up q with
up = {x (cid:55)→ y} and ϕ |= F for some (p, F ) ∈ P . Then, ϕ + t |= G ∧ g where G
corresponds to F after waiting for t time, G = F [C (cid:55)→ C −t]∧C ≥ t. This means
ϕ+t yields values for x which can be chosen to satisfy ϕ+t |= ∃z.(G ∧g)[x (cid:55)→ z].
The successor valuation ϕ(cid:48) =
(ϕ + t) is constructed in such a way that it
equals ϕ + t on all variables except x. On variables x from x, it equals the
assigned value z from z, ϕ(cid:48)(x) = z. Similarly, for clock resets c in x we have
ϕ(cid:48)(c) = 0. We arrive at ϕ(cid:48) |=
Assume (q, ϕ(cid:48)) |= post (cid:93)

t (P ). In case of t = 0 and (q, ϕ(cid:48)) |= P , we are already
done by a stuttering transition. Otherwise, there is a transition p−−−−−→∆, g, up q such
(cid:93)
t (P ) with ϕ(cid:48) |=
t (F ). The existential quantiﬁer
that (q,
guarantees the existence of a valuation ϕ(cid:48)(cid:48) with ϕ(cid:48)(cid:48) |= G ∧ g and ϕ(cid:48) =
(ϕ(cid:48)(cid:48)),
where we use G = F [C (cid:55)→ C − t] ∧ C ≥ t as before. Valuation ϕ(cid:48) evaluates
variables and clocks in x by the satisfying choices for the quantiﬁer ∃z. Since
G requires each clock value to be greater than t, there is ϕ with ϕ + t = ϕ(cid:48)(cid:48).
Altogether, we conclude (p, ϕ)−→t (q, ϕ(cid:48)).

t (F ) and thus (q, ϕ(cid:48)) |= post (cid:93)

t (F )) ∈ post (cid:93)

up
(cid:74)

t (P ).

g|up

g|up

g|up

(cid:75)

(cid:74)

(cid:75)

(cid:75)

(cid:74)

(cid:75)

(cid:74)

(cid:93)

(cid:93)

The arguments are similar for transitions due to an event e.

(cid:117)(cid:116)

Proof (Lemma 5). Assume sp (P , w ) (cid:118) P . We proceed by induction on w . In
the base case, we have w = t. Consider some w (cid:48) = t1. . . . .tn ≡ t and cf −−→w (cid:48) cf (cid:48)
with cf |= P . Using Lemma 4 repeatedly, we get cf (cid:48) |= sp (P , w ). Thus, cf (cid:48) |= R.

Model-based Fault Classiﬁcation for Automotive Software

23

In the induction step, we have w ≡ w (cid:48) = t1 . . . tn.e.w (cid:48)(cid:48). Consider a run of the
form cf −−−−→t1...tn cf (cid:48)−→e cf (cid:48)(cid:48)−−→w (cid:48) cf (cid:48)(cid:48)(cid:48). Similar to the base case, cf (cid:48) |= spt (P ) = P (cid:48),
where t = t1 + . . . + tn. Lemma 4 now yields cf (cid:48)(cid:48) |= spe (P (cid:48)) = P (cid:48)(cid:48). By induction,
we obtain the desired cf (cid:48)(cid:48)(cid:48) |= sp (w (cid:48)(cid:48), P (cid:48)(cid:48)) (cid:118) R.
(cid:117)(cid:116)

Proof (Lemma 6). Assume cf = (p, ϕ). Further, assume that for all cf (cid:48) we have
cf −→s cf (cid:48) implies cf (cid:48) |= P . To the contrary, assume cf = (p, ϕ) (cid:54)|= pre (cid:93)
s (P ) = R.
So there is

g|up

(p,

(cid:74)

(cid:75)

(cid:93)
s, g, {x(cid:55)→y} q
t (G)) ∈ R and (q, G) ∈ P and p−−−−−−−→

such that ϕ + t (cid:54)|= G[x (cid:55)→ y] and ϕ + t |= g. We consider the case of a time
progression s = ∆, the remaining case of an event e is analogous. Since ϕ+t |= g,
there is cf (cid:48) = (q, ϕ(cid:48)) with cf −→t cf (cid:48), namely ϕ(cid:48) =
(ϕ + t). By assumption,
cf (cid:48) |= P and ϕ(cid:48) |= G (since we require only one symbolic conﬁguration per state).
But then ϕ + t |= G[x (cid:55)→ y]: if ϕ(cid:48) satisﬁes G then it also satisﬁes G[x (cid:55)→ y] since
the valuation of x is unimportant. And since ϕ(cid:48) coincides with ϕ + t on all
variables and clocks outside x, also ϕ + t |= G[x (cid:55)→ y].

up

(cid:74)

(cid:75)

Assume cf |= pre (cid:93)

s (P ) with cf = (p, ϕ) and consider a step cf −→t cf (cid:48) with
cf (cid:48) = (q, ϕ(cid:48)) due to transition p−−−−−→∆, g, up q. Then, ϕ(cid:48) =
(ϕ + t). We have
(cid:93)
ϕ |=
t (G) by assumption. The condition ¬g cannot be satisﬁed by ϕ + t,
due to the transition being enabled. But when ϕ + t |= G[x (cid:55)→ y], then changing
the valuation of x to y satisﬁes G. So, ϕ(cid:48) |= G and cf (cid:48) |= P .
(cid:117)(cid:116)

g|up

up

(cid:75)

(cid:74)

(cid:75)

(cid:74)

Proof (Lemma 7). Assume P (cid:118) wp (w , R). We proceed by induction. In the base
case w = t ≡ w (cid:48) = t1 . . . tn. Consider cf |= wp (w , R) and cf −−→w (cid:48) cf (cid:48). Then, by
repeatedly applying Lemma 6 and the deﬁnition of ∇, we obtain cf (cid:48) |= R.

In the induction step, we have that w is equivalent to w (cid:48).e.t1 . . . tn. Consider
a run of the form cf −−→w (cid:48) cf (cid:48)−→e cf (cid:48)(cid:48)−−−−−−→
t1ldotstn cf (cid:48)(cid:48)(cid:48). By induction, cf |= wp (w , R(cid:48))
implies cf (cid:48) |= R(cid:48) = wpe (R(cid:48)(cid:48)). Using Lemma 7 yields cf (cid:48)(cid:48) |= R(cid:48)(cid:48) = wpt (R), where
t = t1 + . . . + tn. Similar to the base case, we obtain cf (cid:48) |= R.
(cid:117)(cid:116)

Proof (Lemma 8). We show the following, slightly stronger proposition: for all
P and w , if there is a sequence of steps on w (cid:48) ≡ w in A starting in some cf |= P
(not necessarily initial) and ending in cf (cid:48), then cf (cid:48) |= sp (P , w ) (cid:54)= false. We
proceed by induction.

In the base case, consider w = t. Let w (cid:48) be a decomposition of w , w ≡ w (cid:48). By
deﬁnition, w (cid:48) is of the form w (cid:48) = t1 . . . tn with t = t1 +· · ·+tn. A run on w (cid:48) takes
the form cf = cf 0−−→t1 cf 1−−→t2
t (P ).
Repeatedly applying Lemma 4 thus gives cf n |= post (cid:93)
t1(P ). By
deﬁnition, this means cf (cid:48) |= sp (P , w ).

· · · −−→tn cf n = cf (cid:48). Lemma 4 yields cf 1 |= post (cid:93)

tn ◦ · · · ◦ post (cid:93)

For the induction step, consider w = w (cid:48).e.t. Assume there is a step sequence
|= P on w .e.t. By induction, it visits cf (cid:48) |= sp (P , w ) = R after w .
from cf
Lemma 4 yields that the next conﬁguration is cf (cid:48)(cid:48) |= post (cid:93)
e (R). Finally, with same
line of argument used in the base case we conclude that the sequence terminates
in some conﬁguration cf (cid:48)(cid:48)(cid:48) |= sp (post (cid:93)
(cid:117)(cid:116)

e (R), t). Hence, sp (P , w .e.t) (cid:54)= false.

24

M. Becker et al.

Proof (Lemma 9). Assume |= { P } t { R } with t ∈ R≥0, the remaining cases are
similar. Consider cf (cid:48) |= sp (P , t). We get:

cf (cid:48)

|= ∇(cid:0)∃t1 · · · , tn. t = t1 + · · · + tn ∧ post (cid:93)
(cid:8)∃t1 · · · , tn. t = t1 + · · · + tn ∧ post (cid:93)

(cid:91)

=

tn ◦ · · · ◦ post (cid:93)
tn ◦ · · · ◦ post (cid:93)

t1(P )(cid:1)
n∈N
t1(P )(cid:9)

n∈N

where the equality is due to the assumption of ∇ being precise. This means there
is a decomposition t = t1 + · · · + tk with cf (cid:48) |= t = t1 + · · · + tk ∧ post (cid:93)
◦ · · · ◦
tk
post (cid:93)
t1(P ). Lemma 4
now yields some cf |= P with cf −−−−→t1...tk cf (cid:48). Hence, cf (cid:48) |= R must hold due to
(cid:117)(cid:116)
the validity of { P } t { R }.

t1(P ). It is easy to see that this means cf (cid:48) |= post (cid:93)
tk

◦· · ·◦post (cid:93)

Proof (Lemma 10). Analogous to the proof of Lemma 9.

(cid:117)(cid:116)

