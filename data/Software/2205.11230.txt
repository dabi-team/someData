A Deep Learning Ensemble Framework for
Off-Nadir Geocentric Pose Prediction

Christopher Sun
Monta Vista High School
Cupertino, CA, United States

Jai Sharma
Monta Vista High School
Cupertino, CA, United States

Milind Maiti
Monta Vista High School
Cupertino, CA, United States

2
2
0
2

g
u
A
6

]

V
C
.
s
c
[

3
v
0
3
2
1
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—Computational methods to accelerate natural dis-
aster response include change detection, map alignment, and
vision-aided navigation. Current software functions optimally
only on near-nadir images, though off-nadir images are often
the ﬁrst sources of information following a natural disaster. The
use of off-nadir images for the aforementioned tasks requires
the computation of geocentric pose, which is an aerial vehicle’s
spatial orientation with respect to gravity. This study proposes
a deep learning ensemble framework to predict geocentric pose
using 5,923 near-nadir and off-nadir RGB satellite images of
cities worldwide. First, a U-Net Fully Convolutional Neural
Network predicts the pixel-wise above-ground elevation mask of
the RGB images. Then, the elevation masks are concatenated
with the RGB images to form four-channel inputs fed into a
second convolutional model, which predicts orientation angle
and magniﬁcation scale. A performance accuracy of R2 = 0.917
signiﬁcantly outperforms previous methodologies. In addition,
outlier removal is performed through supervised interpolation,
and a sensitivity analysis of elevation masks is conducted to
gauge the usefulness of data features, motivating future avenues
of feature engineering. The high-accuracy software built in this
study contributes to mapping and navigation procedures for
effective disaster response to save lives.

Keywords—geocentric pose, convolutional neural network, en-

semble learning

known as geocentric pose, is unavailable when images are
initially captured [4].

Geocentric pose consists of

three components: above-
ground elevation, angle of orientation with respect to gravity,
and scale of magniﬁcation between the actual and apparent
sizes of ground-level objects. The following two ﬁgures,
adapted from the website of the Overhead Geopose Challenge
hosted by the National Geospatial-Intelligence Agency, illus-
trate the concepts of scale and angle [5].

Fig. 1: The scale in geocentric pose represents the magniﬁca-
tion between building heights as represented on ground versus
in a 2-D image plane.

I. INTRODUCTION

Using the deﬁnitions in Fig. 1,

scale =

||Q2 − Q1||
z2 − z1

.

An increasing frequency of natural disasters paralleled by
a rapid increase in the availability of large satellite imagery
data sets has introduced the challenge of big data analysis for
practical disaster relief applications [1] [2]. For an unmanned
aerial vehicle (UAV) to effectively respond to natural disasters,
versatile software must be able to perform a wide array of
functionalities, such as detecting objects, monitoring changes
on land, and mapping surroundings to allow for communica-
tion between the UAV and humans. While computer vision
researchers have implemented algorithms to assist in search
and rescue operations, there has been less progress in the tasks
of vision-aided navigation and map alignment for disaster
relief [3]. These object detection algorithms perform most
optimally on near-nadir images, where object parallax and
occlusion are minimized. Oblique, off-nadir images can be
repurposed for these algorithms through a semantic, three-
dimensional knowledge of surroundings, but this information,

Fig. 2: The angle in geocentric pose represents the degree
of rotation between the y-axis and the ﬂow vector ﬁeld as
determined by the direction of steepest vertical descent. The
angle describes an object’s orientation with respect to gravity.

Geocentric pose is harder to extract from off-nadir images
compared to their near-nadir counterparts; yet, off-nadir im-
ages are the most immediate sources of information following
natural disasters [6]. This limitation creates the need for an

 
 
 
 
 
 
efﬁcient and accurate computational methodology for off-nadir
geocentric pose prediction.

There has been limited research on deep-learning-aided
geocentric pose prediction from satellite imagery. One of the
few works that used this approach is that of Christie et al.,
who created an encoding for geocentric pose using monocular
satellite imagery, attempting to rectify object parallax of
monocular images and thereby improving the accuracy of
object localization for Earth observation tasks [7] [4]. The
main shortcoming of this work was the accuracy of geocentric
pose prediction.

This paper outlines a novel ensemble methodology to
predict the geocentric pose of near-nadir and oblique RGB
satellite images. First, a fully convolutional U-Net model is
used to predict the pixel-wise elevations of RGB satellite
images. Then, a convolutional model with subsequent fully
connected layers is used to predict the scale of magniﬁcation
and angle of orientation, given the RGB image as well as the
predicted pixel-wise elevations.

Fig. 3: The ﬂowchart above depicts the ensemble framework
to predict geocentric pose.

TABLE I: Geographical Sources of Data

Location

No. of Images

San Fernando, AR
Atlanta, USA
Jacksonville, USA
Omaha, USA

2325
704
1098
1796

Elevation was given in centimeters, scale was given in
pixels per centimeter2, and angle was given in radians. Due
to computational constraints, image dimensions were resized
from 2048 × 2048 × 3 to 256 × 256 × 3 using interpolation
through resampling by pixel area relation.

B. Outlier Removal

The data set contained some elevation masks with im-
probably large pixel values, higher than the tallest building
in the city the data was acquired from. The solution to
this problem was location-based thresholding, which involved
ﬁnding pixel value thresholds for each city in Table 1. Images
from Jacksonville and Omaha did not contain any outliers.
For each image from Atlanta, the number of pixels N that
exceeded 4,000 centimeters was counted. If N exceeded 100,
we determined that the image indeed contained a building
taller than 4,000 centimeters. If N was less than 100, all
said pixels were interpolated with the median elevation in the
image. For images from San Fernando, the outlier threshold
was a hard cap of 3,000 centimeters, because the landscape
was fairly even and did not contain tall buildings.

One limitation of location-based thresholding was that
thresholds were experimentally determined, since there was
no objective way to best deal with outliers. In particular,
the challenge was discriminating between large pixel values
that were indeed invalid and those that were valid due of the
presence of a tall building.

II. MATERIALS AND METHODS

A. Data Used For Experimentation

C. Autoencoder Model

The data used for this research is part of the Urban Semantic
3D Data Set, publicly available from the IEEE DataPort [8].
The data set contains 5,923 RGB images, their pixel-wise
above-ground elevations1, and their measurements of scale and
angle. The images were captured from four general locations
in North and South America.

Before embarking on the ensemble framework, an autoen-
coder was trained on the RGB images to gauge the practi-
cality of a deep learning approach. Our reasoning was that
if meaningful latent space representations could be produced
from the RGB images, the other convolutional models would
likewise have the capability to extract useful features to predict
geocentric pose.

1Some elevation masks contain “Not a Number” (NaN) pixel values. The

2The unit for scale was converted to pixels per decameter when reporting

solution to this problem is discussed in Section III-B.

metrics.

elevation mask of each RGB image. The model was based
on a U-Net architecture designed for a wildﬁre detection task,
with modiﬁcations such as the addition of extra convolutional
blocks and skip connections between downsampling and up-
sampling layers [9]. The ﬁlter size was 5 × 5 throughout the
model. Starting with 32 ﬁlters, the number of ﬁlters was dou-
bled for each layer during downsampling and halved for each
layer during upsampling. Batch Normalization was employed
between convolution layers for a regularization effect, and
the ReLU activation function was used throughout the model.
The model was conﬁgured with an 85-15% train-validation
split and trained until convergence using Adam optimization,
a constant learning rate of 0.001, and a mini-batch size of 32
images [10]. Images with “Not a Number” (NaN) values were
withheld from the training data set for data cleaning purposes
described in Section III-B.

The described model will henceforth be referred to as the

U-Net Elevation Model.

E. Scale-Angle Model

Fig. 6: Shown above is the architecture of the Scale-Angle
Model trained with 7,076,370 parameters. RGB images and
their elevation masks were stacked depthwise and fed through
the model, which predicted the remaining two components of
geocentric pose.

The second model in the ensemble framework, referred to
as the Scale-Angle Model, was a convolutional neural network
with a series of ﬁve double-convolution layers and maximum
pooling in between each series. The resulting 7 × 7 × 256
shape was ﬂattened and fed through six fully connected layers3
to generate the scale and angle predictions. The model was
conﬁgured with an 80-20% train-validation split and trained
until convergence using the same speciﬁcations as the U-Net
Elevation Model.

After elevation mask predictions were obtained from the
U-Net Elevation Model,
they were concatenated with the
original RGB images along the channels axis to produce four-
channel images. These images were then fed as inputs to
the Scale-Angle Model for the prediction of scale and angle.
Together, the U-Net Elevation Model and the Scale-Angle
Model predicted all three components of geocentric pose.

Fig. 4: Shown above is the architecture of the Autoencoder
Model trained with 16,823,107 parameters, consisting of an
encoder (left) and a decoder (right).

The encoder used convolutional operations to condense the
input images to intermediate matrices of shape 32 × 32 ×
16. These matrices were then ﬂattened to a vector of length
16384, after which a dense layer was used to arrive at the latent
space representation of length 512. Multidimensional Scaling
(MDS) was used to visualize these high-dimensional latent
space representations on a two-dimensional plane. An inverse
architecture of the encoder was used to decode the latent space
representations back into images of shape 256 × 256 × 3. The
autoencoder, a result of combining the encoder and decoder
into one model, was trained with the goal of retrieving decoded
images as similar to the original input images as possible.
The accomplishment of this task would conﬁrm the feasibility
of the extraction of high-level features using computer vision
techniques.

D. U-Net Elevation Model

Fig. 5: Shown above is the architecture of the U-Net Elevation
Model trained with 23,966,337 parameters. Skip connections
between downsampling and upsampling blocks allow for the
concatenation of past feature information during later stages
of processing.

A fully convolutional neural network, the ﬁrst model in the
ensemble framework, was trained to predict the pixel-wise

3Fig. 6 only displays ﬁve fully connected layers. The sixth layer contained

2 hidden nodes, corresponding to scale and angle, respectively.

III. RESULTS

Interpolation of Invalid Data

A. Autoencoder: Latent Space Representations4

Fig. 7: MDS was employed to visualize the latent spaces
representations of the encoded images. In the ﬁgures above,
each point represents an encoded image, which is colored by
the magnitude of its scale and angle, respectively.

a) Visualization for Scale: No clear association was
observed from the visualization of scale encodings. This
may have been caused by the decrease in variance due to
dimensionality reduction.

b) Visualization for Angle: Contrary to the results of the
scale encodings, the visualizations for angle encodings showed
a clearer association. Namely, images with higher angles were
aggregated spatially to the left while images with lower angles
were aggregated spatially to the right. The signiﬁcance of
this result was twofold, demonstrating not only that there
is a relationship between an RGB image and its angle, but
also that
this said relationship can be learned through a
convolutional approach. Hence, we considered it feasible to
use deep learning techniques to learn this relationship and
predict the geocentric pose of RGB images.

Evaluation Metrics

In addition to the mean absolute error (MAE) and median
absolute deviation (MAD) metrics, the models in the ensemble
framework were evaluated by the R2 metric, deﬁned as

R2 = 1 −

(cid:80)n
(cid:80)n

i=1(yi − ˆyi)2
i=1(yi − ¯y)2 ,

where n is the number of values, yi is the ith true value, ˆyi is
the ith predicted value, and ¯y is the mean of all true values.

B. U-Net Elevation Model Performance

The U-Net Elevation Model achieved an R2 metric of
0.926 on training data and 0.865 on validation data. The
model was prone to minimal overﬁtting, with a high degree of
generalizability to unseen data, which was both numerically
and visually conﬁrmed.

4It must be noted that these visualizations do not capture all nuances of

the high-dimensional data, so they are used as proof of concept.

Recall that images containing NaN values were excluded
from training. This was to allow NaN pixels to be replaced
with the predictions of the U-Net Elevation Model after
convergence on valid data. Interpolation was performed on
all NaN values, serving as a data cleaning and quality control
mechanism that allowed all 5,923 images to be used for the
development of the Scale-Angle Model.

Fig. 8: The elevation masks predicted by the model are
compared to the true elevation masks (left). Notice the sim-
ilarity between the true and predicted elevation masks. The
model interpolations are displayed next to their counterparts
containing large amounts of invalid pixel values (right). Notice
how the third row contains a small yellow patch of NaN
values near the top left of the elevation mask. Images like
this required interpolation for only a small number of pixels.
On the other hand, for images containing mostly NaN values,
such as the fourth and ﬁfth rows, most of the elevation mask
was reconstructed using model predictions.

C. Scale-Angle Model Performance

IV. DISCUSSION

A. Insights from Deep Learning Models

a) Autoencoder Model: The latent space representations
of the RGB images were twelve times smaller in storage com-
pared to the original images, yet preserved important features
within the data. The ability of the autoencoder to reproduce
the RGB images motivates future avenue of exploration that
involve using autoencoders as feature extractors that can feed
high-level characteristics of RGB images into the ensemble
framework.

b) Feature Importance for Ensemble Framework: We
sought to understand the usefulness of the features extracted by
the U-Net Elevation Model by testing the degree of importance
of the elevation masks in regards to geocentric pose prediction.
This was accomplished through a sensitivity analysis involving
three different versions of the Scale-Angle Model, as follows.

1) RGB-Only Model: The RGB-Only Model received only
the RGB images as inputs, acting as a negative control.
2) RGB-Elevation Model The RGB-Elevation Model was

the standard Scale-Angle Model.

3) Elevation-Only Model The Elevation-Only Model re-
ceived only the elevation masks as inputs, acting as a
baseline performance standard.

These three models were trained with the same hyperparameter
speciﬁcations as described in Section II-E, with the exception
of the input shape, which differed along the channels axis.
The R2 of each of the three deﬁned models and the P-values
between adjacent R2 metrics, calculated as a difference in
proportions, is listed below, with the models numbered in the
order they were introduced.

TABLE III: Scale-Angle Model Sensitivity Analysis

1) R2

P-value

2) R2

P-value

3) R2

Scale
Angle

0.591
0.733

8.55E-80
7.30E-55

0.924
0.963

2.24E-4
5.16E-15

0.881
0.963

The R2 of the RGB-Elevation Model exceeded that of the
RGB-Only Model for both scale and angle with a statistically
signiﬁcant P-value. Similarly, the R2 of the RGB-Elevation
Model exceeded that of the Elevation-Only Model for scale
with a statistically signiﬁcant P-value. The Elevation-Only
Model surprisingly outperformed the RGB-Elevation Model
in angle predictions, but by an extremely small margin.

The results of the sensitivity analysis show that the presence
of elevation masks as inputs is integral for the Scale-Angle

5The Elevation-Only Model outperformed the RGB-Elevation Model in
angle predictions, an opposite manifestation of the alternative hypothesis,
hence the P-value > 0.5.

Fig. 9: The above plots show the resemblance between true
and predicted values as well as the distribution of error of the
Scale-Angle Model.

The Scale-Angle Model achieved an R2 metric of 0.991 on
training data and 0.943 on validation data, weighed equally be-
tween scale and angle. The following conclusions are derived
from Fig. 9.

a) True vs. Predicted Value: The predicted scale and
angle measurements for all images are plotted against the
true scale and angle measurement, with linear regression
coefﬁcients of 0.995 and 0.994, respectively.

b) Performance Distributions: The distributions of the
true and predicted values have similar shape and spread
for both scale and angle, showing the model’s ability to
capture trends between the RGB-and-Elevation inputs and the
corresponding geocentric pose.

c) Distribution of Absolute Error Loss: The distributions
of absolute error loss for both scale and angle are heavily
skewed right, which shows that
the model rarely outputs
predictions that are faulty to a large degree. The distribution
for angle is more skewed than that for scale.

d) Sorted Absolute Error: The graphs of sorted absolute
error for both scale and angle take a skewed shape. On average,
the model was able to predict the scale to within 0.386 pixels
per decameter, and the angle to within 0.079 radians (≈ 4.53◦).

The ﬁnal metrics for the geocentric pose ensemble frame-

work are summarized below.

TABLE II: Geocentric Pose Ensemble Framework Metrics

U-Net Model

Elevation (m)
Val
Train
0.335
0.263
1.049
0.847
0.865
0.926

Scale-Angle Model

Scale (px/dam)
Val
Train
0.386
0.192
0.582
0.243
0.924
0.990

Angle (rad)
Val
0.079
0.160
0.963

Train
0.063
0.091
0.991

Metric
MAD
MAE
R2

V. CONCLUSION

The ensemble framework proposed in this paper will im-
prove the versatility of natural disaster response software
through the prediction of geocentric pose for both near-nadir
and off-nadir aerial vehicles. Validated by an R2 metric supe-
rior to previous research investigations, our results will allow
for the repurposing of off-nadir images for disaster response
tasks such as vision-aided navigation, change detection, and
map alignment tasks for effective disaster response.

REFERENCES

[1] Van Aalst, M. K. (2006). The impacts of climate change on the risk of

natural disasters. Disasters, 30(1), 5-18.

[2] Muruganandham, S. (2016). Semantic Segmentation of Satellite Images
using Deep Learning (thesis). Faculty of Electrical Engineering Depart-
ment of Cybernetics.

[3] Mishra, B., Garg, D., Narang, P., & Mishra, V. (2020). Drone-surveillance
for search and rescue in natural disaster. Computer Communications, 156,
1-10.

[4] Christie, G., Foster, K., Hagstrom, S., Hager, G. D., & Brown, M. Z.
(2021). Single view geocentric pose in the wild. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp.
1162-1171).

[5] Overhead

Geopose

Challenge.

DrivenData.

www.drivendata.org/competitions/78/overhead-geopose-challenge/

[6] Weir, N., Lindenbaum, D., Bastidas, A., Etten, A. V., McPherson, S.,
Shermeyer, J.,
... & Tang, H. (2019). Spacenet mvoi: A multi-view
overhead imagery dataset. In Proceedings of the ieee/cvf international
conference on computer vision (pp. 992-1001).

[7] Christie, G., Abujder, R. R. R. M., Foster, K., Hagstrom, S., Hager,
G. D., & Brown, M. Z. (2020). Learning geocentric object pose in
oblique monocular images. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (pp. 14512-14520).

[8] Foster, K., Christie, G., & Brown, M. (2020). Urban Semantic 3D Dataset.

IEEE Dataport. dx.doi.org/10.21227/9frn-7208.

[9] Sun, C. (2022). Analyzing Multispectral Satellite Imagery of South Amer-
ican Wildﬁres Using Deep Learning. In 2022 International Conference on
Applied Artiﬁcial Intelligence (ICAPAI) (pp. 1-6). IEEE.

[10] Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980.

[11] Sharma, J., Sun, C., & Maiti, M.

(2022). Jaisharmz/geocentric-
pose: Github repository for the research paper “A deep learning en-
semble framework for off-nadir geocentric pose prediction.” GitHub.
github.com/jaisharmz/geocentric-pose

ADDITIONAL INFORMATION

The authors have made the code for the research in this

paper open source and available on GitHub (click) [11].

Model to achieve high performance metrics, meaning above-
ground elevation is a crucial feature for the prediction of the
scale and the angle. Elevation masks could speciﬁcally be
beneﬁcial in predicting the angle of orientation, as knowledge
of relative building heights can assist in determining their
orientation. On the other hand, processing RGB images in
conjunction with elevation masks could provide useful infor-
mation about the scale of magniﬁcation, which relies on a
measure of the conversion factor between pixels in the image
and distances in the real world.

B. Limitations and Future Improvements

When analyzing speciﬁc predictions of low accuracy, an
unexpected result was that the Scale-Angle Model struggled
to predict the angle of some near-nadir images, likely because
the height of buildings are difﬁcult to perceive at these angles,
rendering it difﬁcult to distinguish between angles in this
range.

As stated previously, computational constraints necessitated
images, which lowered images’
the downsizing the input
resolutions. Instead, random cropping can be employed to
accomplish the same task while preserving the original res-
olution. Performance can further be enhanced through image
augmentation, particularly for the cities of Atlanta and Jack-
sonville, to prevent the model from overﬁtting to cities with
the most labeled data. Additionally, the ensemble framework
may beneﬁt from the usage of the Ground Sample Distance
(GSD) feature included in the meta-data.

Future research should explore optimal outlier removal
methods for this data set. Further experimentation is also
necessary to investigate how the model architectures described
in this paper could be improved. For example, changing the
size of the bottleneck layer in the U-Net Elevation Model
or using two separate, simpler models to predict scale and
angle might result in better performance. In addition, labeled
geocentric pose data from contrasting landscapes is required to
understand the ensemble’s degree of generalization, since the
ensemble was trained mainly on suburban and urban images.

C. Comparison to Prior Research

While Christie et al. achieved a cumulative R2 metric
of 0.799,
the models described in this study achieved a
cumulative R2 metric of 0.917, albeit on different testing
examples [4]. Nevertheless, this demonstrates a major increase
in accuracy and reliability of geocentric pose predictions. We
attribute this higher performance to the ensemble methodology
as well as the data preprocessing and supervised interpolation
that served as data quality control.

