SmartSAGE: Training Large-scale Graph Neural Networks
using In-Storage Processing Architectures

Yunjae Lee

Jinha Chung
School of Electrical Engineering
KAIST
{yunjae408, jinha.chung, mrhu}@kaist.ac.kr

Minsoo Rhu

2
2
0
2

y
a
M
0
1

]

R
A
.
s
c
[

1
v
1
1
7
4
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—Graph neural networks (GNNs) can extract features
by learning both the representation of each objects (i.e., graph
nodes) and the relationship across different objects (i.e., the edges
that connect nodes), achieving state-of-the-art performance in
various graph-based tasks. Despite its strengths, utilizing these
algorithms in a production environment faces several challenges
as the number of graph nodes and edges amount to several
billions to hundreds of billions scale, requiring substantial storage
space for training. Unfortunately, state-of-the-art ML frame-
works employ an in-memory processing model which signiﬁcantly
hampers the productivity of ML practitioners as it mandates the
overall working set to ﬁt within DRAM capacity. In this work,
we ﬁrst conduct a detailed characterization on a state-of-the-
art, large-scale GNN training algorithm, GraphSAGE. Based on
the characterization, we then explore the feasibility of utiliz-
ing capacity-optimized NVMe SSDs for storing memory-hungry
GNN data, which enables large-scale GNN training beyond the
limits of main memory size. Given the large performance gap
between DRAM and SSD, however, blindly utilizing SSDs as
a direct substitute for DRAM leads to signiﬁcant performance
loss. We therefore develop SmartSAGE, our software/hardware
co-design based on an in-storage processing (ISP) architecture.
Our work demonstrates that an ISP based large-scale GNN
training system can achieve both high capacity storage and high
performance, opening up opportunities for ML practitioners to
train large GNN datasets without being hampered by the physical
limitations of main memory size.

I. INTRODUCTION

Deep neural network (DNN) based machine learning (ML)
algorithms are providing super-human performance in areas
of image classiﬁcation, natural language processing, speech
recognition, and others. However, such DNN based ML design
paradigms primarily targeted Euclidean data (e.g., image, text,
and audio), having limited adoption in domains where non-
Euclidean data structures such as graphs are utilized. Recently,
graph neural networks (GNNs) have emerged as a powerful
tool in application domains that target arbitrarily structured
graph inputs, where feature vectors are associated with graph
nodes and edges. GNNs have found signiﬁcant success in the
areas of e-commerce and advertisement where the graph nodes
and edges represent objects and their relationships, providing
unparalleled performance in traditional graph analytics work-
loads. For instance, Pinterest’s PinSAGE [85] or Alibaba’s

This is the author preprint version of the work. The authoritative version
will appear in the Proceedings of the 49th IEEE/ACM International Sympo-
sium on Computer Architecture (ISCA-49), 2022.

AliGraph [90] leverages GNNs to analyze and extract high
quality features from graphs with billions of user/item feature
embeddings, achieving state-of-the-art performance.

[78] and PyTorch Geometric (PyG)

With the proliferation of GNNs, we are witnessing a large
number of ML frameworks tailored for graph learning being
developed, examples of which include Deep Graph Library
(DGL)
[21]. Unlike
conventional DNNs (e.g., convolutional, recurrent, or fully-
connected layers) which exhibit a highly regular and dense
dataﬂow, GNN training inherently contains a hybrid mix of
both sparse and dense dataﬂows (Figure 1). More concretely,
the frontend stages of GNN training which conduct “input
data preparation” (e.g., graph neighbor sampling, feature table
lookup) follow the typical graph analytics’ sparse and irregular
memory access characteristics. In contrast, the backend stages
which conduct “graph learning” (e.g., graph convolutions) em-
ploy well-structured, dense DNN algorithms using multi-layer
perceptron (MLP) layers. Therefore, DGL and PyG provides
dedicated user-level APIs tailored to the sparse dataﬂow of
input data preparation stages (i.e., ﬁne-grained feature gather-
scatter for graph neighbor sampling and feature aggregation)
which eases the programming of irregular and sparse frontend
stages of GNN training.

While DGL and PyG help improve the programmability of
GNNs, both frameworks employ the in-memory processing
model [21], [78] (i.e., the target graph nodes/edges and its
feature embedding tables must be stored inside main memory)
which limits ML practitioners from scaling “up” the graph
dataset, signiﬁcantly hampering user productivity. General
trend in the GNN research space has been to increase the
number of graph nodes as well as the number of edges (i.e.,
larger and more complex graphs) while employing more so-
phisticated DNNs for feature extraction (i.e., convolutions [41]
to attentions [75]). An important reason why such scaled-up
GNNs became prevalent is because it helps improve GNN’s
algorithmic performance, similar to how ML algorithms tar-
geting Euclidean data show higher performance with larger
and deeper DNNs. Unfortunately, current ML frameworks’
in-memory processing model forces ML practitioners to tune
the GNN training algorithm to ﬁt within the several tens to
hundreds of GBs of CPU memory, preventing developers from
scaling up the graph network structure as well as its feature
vector size. To this end, this paper explores the feasibility of
utilizing NAND ﬂash-based non-volatile memory (NVM) so-

1

 
 
 
 
 
 
lutions to address the memory “capacity” bottlenecks of large-
scale GNN training. Our proposal encompasses innovations at
both the software and hardware stack, as detailed below.

Software. Given the wide performance gap between DRAM
and NVMe SSDs, the central research challenge lies in how
system architects should go about harmoniously architecting
the memory-storage hierarchy that is most appropriate for
GNN training’s algorithmic properties. Consequently, we start
by conducting a chacterization on a state-of-the-art
large-
scale GNN training algorithm, GraphSAGE [26]. Our char-
acterization reveals that GNN training’s frontend input data
preparation stage (Figure 1) is the most memory capacity
intensive and becomes a prime candidate to be ofﬂoaded to
SSDs. We therefore establish a baseline SSD-centric training
system which stores the memory-hungry data structures (i.e.,
graph nodes/edges) inside SSDs. These data structures are
mapped to user-space memory address via memory-mapped
(mmap) ﬁle I/O, which allows the most recently accessed
pages to be buffered inside the OS managed page cache (i.e.,
stored in DRAM), potentially narrowing the large performance
gap between DRAM and SSDs.

Unfortunately, our baseline SSD-centric training system is
shown to beneﬁt little from the locality-optimized OS page
incurring high performance loss when ofﬂoaded to
cache,
SSDs with an average 9.8× slowdown vs. an oracular, in-
memory processing based system (i.e., all graph datasets can
be stored in DRAM). Careful examination of the bottlenecks
caused by the SSD-ofﬂoaded data preparation reveals the
following key observation: because data preparation exhibits
ﬁne-grained irregular parallelism, it becomes challenging for
the page cache to reap locality beneﬁts, only to add several
tens of microseconds of latency in traversing through the
system software stack to maintain the page cache, rendering
the frontend data preparation stage to be highly latency limited.
As such, our proposal designs the software architecture to be
optimized for latency, rather than locality. More concretely,
we restructure ML framework’s software runtime system as
well as the host driver stack to bypass the OS system software
layers and directly access the SSD without page caching. Such
design point obviates the latency overheads in maintaining
the page cache to buffer recently accessed data, signiﬁcantly
reducing the time taken to fetch graph datasets from the
SSD. We demonstrate that optimizing the DRAM↔SSD data
movements for latency leads to signiﬁcant speedup on end-
to-end GNN training time with an average 2.5× vs. the base-
line mmap-based SSD, closing the performance gap between
DRAM vs. SSDs to “only” an average 3.8×.

Hardware. To further close DRAM-vs-SSD’s remaining
performance gap, we also innovate at the hardware architecture
level driven by both recent technological trends and GNN
algorithm awareness. Recent trends point to the emergence of
SSD storage devices that natively support in-storage process-
ing (ISP) capabilities (e.g., Samsung-Xilinx’s SmartSSD [66],
NGD system’s Newport
[71], Eideticom’s NoLoad
[15],
CSP [19]) that offer fast communication between the ﬂash
devices and the ISP units. The key objective of our proposal

Fig. 1: High-level overview of a large-scale GNN training pipeline.

is to synergistically combine ISP capabilities of these emerg-
ing computational storage devices (CSDs) with our latency-
optimized software runtime system and host driver stack to
achieve “DRAM-level” effective throughput, as provided with
the in-memory processing baseline ML frameworks.

To this end, we develop SmartSAGE, an ISP based GNN
training system that intelligently ofﬂoads the data intensive
stages to the ISP unit, closely coupled inside the SSD. The
key research problem naturally lies in identifying which part
of the GNN training algorithm is most appropriate to be
handled by our ISP architecture. Our characterization reveals
that the nature of GNN training’s data preparation is effectively
a reduction operation as it seeks to extract out multiple
“subgraphs” from a much larger input graph. Rather than
having large, coarse-grained chunks of the input graph be
transferred from SSD to DRAM for subgraph generation by
the host CPU, SmartSAGE ofﬂoads the data intensive steps
of data preparation (more speciﬁcally, the neighbor sampling
step) to the ISP units. This allows SmartSAGE to only transfer
the subgraphs from SSD to DRAM, preprocessed by the
ISP unit, signiﬁcantly reducing the data movements between
SSD→DRAM by an average 20×. Putting everything together,
SmartSAGE holistically combines our latency-optimized soft-
ware system with an ISP accelerated hardware architecture,
achieving substantial speedup on end-to-end GNN training
time with an average 3.5× (max 5.0×) vs. the baseline SSD-
centric system. To summarize our key contributions:

• We conduct a detailed characterization on the data-
intensive frontend data preparation stage of large-scale
GNN training, root-causing several limitations of con-
ventional in-memory processing GNN training systems.
• Driven by our characterization, we motivate and explore
the viability of exploiting NVMe SSDs as a direct substi-
tute for capacity limited DRAM based training systems.
• To bridge the wide performance gap between DRAM
vs. SSD, we co-design the software/hardware of large-
scale GNN training, presenting our latency-optimized
software coupled with an in-store processing architec-
ture, achieving superior performance than baseline SSD-
centric systems.

II. BACKGROUND

A. Graph Neural Networks

GNNs are a variant of DNNs that operate over graph data
structures. A unique property of GNNs is that they try to
extract features by learning both the representation of each
objects (i.e., graph nodes) as well as the relationship across
different objects (i.e., the edges that connect nodes). Consider
the task of recommending a video/movie clip in an online
video streaming service (e.g., YouTube, Netﬂix). A purely

2

Data PreparationNeighbor SamplingFeature Table LookupAggregationTarget Node SelectionCombinationInputGraphGNN Training12345FinalResultAlgorithm 1 Neighbor sampling for “subgraph” generation
1: Set of target nodes M ; neighborhood N ; sampling size s; Sampled set

of nodes S

for i ← 0 to s do

2:
3: /* Sampling neighborhood nodes of target nodes */
4: S ← ∅
5: for u ∈ M do
6:
7:
8:
9:
10:
11: end for

/* Randomly select its neighborhood nodes */
v ← RandomSelect(N (u))
S ← S ∪ v

end for

B. “Sample and Aggregate” for Large-scale GNN Training

While GNNs have established a new standard in a variety of
applications within the academic community, they have only
recently started being deployed in practical, real-world prob-
lems [85], [90]. A key challenge that production environments
face is that the number of graph nodes and edges amount
to several billions to hundreds of billions scale, requiring
substantial compute and memory for training and deployment.
Training early GNN models [41] required operating on the
full graph Laplacian during training, so the entire graph data
and intermediate states of all graph nodes must be stored in
memory. Such high compute and memory requirements can
only be met when the target graph data structure is at a small
scale.

To address the practical needs of scaling GNN training and
deployment to massive, “web-scale” graph data, the seminal
work on GraphSAGE (short for graph sampling and aggre-
gation [26], [85]) proposed a highly scalable GNN training
framework enabling large-scale graph learning (Figure 1).
Rather than targeting the entire graph nodes and all the accom-
panying neighbor nodes for training, GraphSAGE ﬁrst chooses
a ﬁxed number of M target nodes (M being equivalent to
the training mini-batch size, typically in the range of several
thousands), which is much smaller than the number of entire
nodes, e.g., M =1 in step 1 of Figure 2. For each target node,
GraphSAGE then “samples” s neighborhood nodes around
each target node (Algorithm 1, step 2 of Figure 2), and only
those sampled nodes near the target nodes will later be targeted
for GNN training (i.e.,
the aggregation stage followed by
combination in Figure 1). In effect, GraphSAGE dynamically
constructs a subgraph (generated over a mini-batch of M
target nodes, each target node containing s sampled nodes
among its neighbors) and iteratively conducts convolutions
only around the subgraph (step 3 and 4 in Figure 2). This
allows the total number of target nodes for mini-batch training
as well as the number of neighboring nodes for a given target
node all be hyperparameters of the training algorithm, which
helps drastically reduce the active compute and memory re-
quirements of GNN training. As a result, GNNs are gradually
seeing real-world adoption in consumer facing products, a
well-known example being the usage at Pinterest (through a
GNN called PinSAGE [85]) to generate embedding feature
vectors for images/etc. Given their wide adoption and general

Fig. 2: Illustration of the GraphSAGE sample and aggregate operator gener-
ating a single target node’s embedding vector through a depth-2 convolution
(considering upto two-hop (k=2) neighboring nodes). Training GNNs with a
mini-batch of M involves selecting M target nodes (i.e., M =1 in this ﬁgure),
each going through the sample and aggregation process. Example assumes two
neighboring nodes are sampled per each target node (i.e., s=2). For instance,
among the three neighbor nodes of node B (nodes C/F /E in step 1), we
only include nodes C and E (step 2) as part of the subgraph. In this work,
we focus on addressing the bottlenecks incurred in step 1 and 2.

content based approach (e.g., recurrent neural networks [6])
would represent each object under consideration (i.e.,
the
movie) based on the features derived from the target object
(e.g., the genre of the movie). However, there can be valuable
information existent across two (and potentially many) distinct
objects, as there could be common properties that these objects
share. By modeling such relationship as graph edges and each
individual objects as graph nodes, a GNN can learn to extract
meaningful feature representations not only from the object
itself but also from the edges that are associated with it.

Consequently, GNN-based ML applications are achieving
state-of-the-art performance on a wide range of graph-based
algorithms, which include node/edge prediction [17], [22],
[41], graph clustering [86], recommendation models [14],
and others. Graph convolutional neural networks for instance
aggregate target nodes’ features and context information from
their k-hop neighborhood nodes, similar to how conventional
convolutional layers of a DNN application extracts features
from a given pixel’s neighborhood pixels. By stacking multiple
of such convolution operations in sequence (e.g., two convolu-
tion layers consider up to two-hop (k=2) neighborhoods), the
coverage of feature learning could exponentially propagate far
reaches of the graph.

3

[Step 4] Depth-k(=2)convolution that computes the embedding vector for target node(Aggregatefeature vector only from sampled nodes A, B, C, E, G, H, I and convolve)A(1)(1)pBG(1)(1)hGhBhAhN(A)(2)hATarget nodeIHECCONVOLVE()CONVOLVE()CONVOLVE()(n)hX: n-thlayer embedding vector of node X-: neighborhood nodes of X-N(X): multi-layer perceptron-: pooling function-p[Step 3] Feature aggregation(Read feature vectors for sampled nodes)JCDFTarget nodeBAEGHe1e2en…b1b2bn…g1g2gn…h1h2hn…a1a2an…i1i2in…c1c2cn…I[Step 2] Neighbor sampling(Sample two neighbor nodes (s=2)from 2-hop neighbors)CDFTarget nodeBAEGHJI[Step 1] Select Mtarget nodes(M: batch size, example assumes M=1)CDTarget nodeBAEGHFJI(a)

(b)

Fig. 3: (a) The hybrid CPU-GPU based GNN training system assuming an
in-memory processing model. (b) The baseline SSD-centric training system
utilizes NVMe SSDs to store the memory-hungry neighbor edge list array
which forces the neighbor sampling stage (step (cid:182)) to be conducted over the
SSD, rather than DRAM, causing performance overheads. Note that step (cid:182)
to (cid:186) in this ﬁgure each matches the corresponding steps (cid:182)−(cid:186) in Figure 1.

applicability in enabling large-scale GNN training,

C. System Architecture for GNN Training

A GNN training algorithm’s graph dataset consists of sev-
eral key data structures including the graph neighbor edge
list array that encapsulates the structure of the graph using
its adjacency matrix, and the feature table which abstracts
each node’s unique property as a feature vector. In real-world
graphs, the number of graph edges outweighs the number of
graph nodes, so the overall memory consumption is generally
dominated by the neighbor edge list array (discussed further
in Section IV-B/Figure 10) rather than the feature table, the
aggregate size of which can amount to several hundreds to
thousands of GBs for large-scale graphs. Such constraint
poses several key challenges in designing the overall system
architecture for large-scale GNN training.

Consequently, state-of-the-art GNN training systems typi-
cally employ a hybrid CPU-GPU design where the frontend
data preparation is undertaken by the CPU while the backend
GNN training is handled by the GPU (Figure 3(a)). Be-
cause GPUs employ bandwidth-optimized but capacity-limited
HBM, they are unable to locally store the memory capacity
limited neighbor edge list array. Therefore, capacity-optimized
CPU DIMMs are utilized for storing the memory-hungry graph
data and the CPU goes through the neighbor sampling phase
using the neighbor edge list array (step (cid:182) in Figure 1 and
Figure 3(a)). Once the sampled subgraph is generated, the
feature table is looked up to aggregate the corresponding
feature vectors of each sampled node (step (cid:183)). The aggregated
feature vectors are then copied over to the GPU over PCIe
(step (cid:184)) for GNN training (step (cid:185) and (cid:186)).

III. MOTIVATION AND CHARACTERIZATION

A. Motivation

A critical limitation with current graph learning frameworks
(DGL [78] and PyG [21]) is that their in-memory processing
model (i.e., the key data structures of large-scale GNN training
must all be stored in DRAM) prevents developers from
scaling up the graph structure and its feature vector size. One
promising alternative is to employ NVMe SSDs for storing the

(a)

(b)

Fig. 4: Execution timeline of hybrid CPU-GPU under (a) the baseline system
assuming in-memory processing and (b) when the SSD is used to store the
large-scale graph data. A single subgraph is generated by a CPU-side producer
worker process, multiples of which are stored into the GPU work queue to
be consumed by the GPU worker process sequentially for GNN training.

memory-hungry data structures of GNN training (Figure 3(b)),
utilizing main memory as a fast cache for high locality data. A
potential challenge with SSDs, however, is that they operate as
block devices where data is transferred in coarse 4 KB chunks
at a much lower throughput than DRAM. In the remainder
of this section, we conduct a detailed characterization on
GraphSAGE based large-scale GNN training and identify key
challenges of utilizing SSDs to address the memory capacity
limitations of current in-memory processing ML frameworks.

B. Data Preparation in In-Memory Training

A typical GNN training pipeline employs the producer-
consumer model as illustrated in Figure 4. The data prepara-
tion stage is implemented using multiple CPU-side producer
workers operating in parallel, each of which conducts neighbor
sampling to generate the mini-batch inputs, i.e., the subgraphs.
These subgraphs are then stored into a work queue which the
GPU-side consumer process utilizes to initiate GNN training.
The implication of storing the memory capacity limited
graph datasets in an SSD is that the neighbor sampling stage
of data preparation (step (cid:182) in Figure 1, Figure 3(b)) must
now be conducted over the slow and low bandwidth I/O block
interface. Understanding the algorithmic property of sampling
and its memory access behavior is therefore crucial in gauging
the feasibility of SSDs for large-scale GNN training.

To this end, in Figure 5, we start by ﬁrst characterizing the
on-chip caching and off-chip DRAM bandwidth utilization of
the neighbor sampling algorithm under the baseline in-memory
processing setting (i.e., all input graph dataset is assumed
to be entirely stored in DRAM). Results show that neighbor
sampling exhibits low caching efﬁciency with an average 62%
last-level cache (LLC) miss rate. As discussed in Algorithm 1,
neighbor sampling involves randomly selecting a ﬁxed number
of nearby nodes for each of the target nodes as it traverses
through the k-hop neighborhoods. Because the target nodes

4

andNICSSDSystem Interconnect (PCIe)CPUGPUMemory321and54NICSSDCPUGPUMemory32and5System Interconnect (PCIe)41GNN Training(GPU)TimeData Preparation(CPU)Batch 0Batch 0Execution timeline(In-memory processing model)Batch 1Batch 2Batch 1Batch 2Batch 3Batch 3GNN Training(GPU)TimeData Preparation(CPU)Batch 0Batch 0Execution timeline(Large-scale processing model)Batch 1Batch 2Batch 1Batch 2Wasted GPU timeWasted GPU timeFig. 5: The LLC miss rate (left) and DRAM bandwidth utilization (right)
during the neighbor sampling stage with baseline in-memory processing
training using PyG. We utilize Linux perf (caching) and Intel RDT utility
(bandwidth) for our evaluation.

Fig. 7: Percentage of training time where the GPU is idle due to the lack of
input mini-batches to process.

centric system incurs an average 9.8× (maximum 19.6×)
slowdown vs. in-memory processing. As discussed in Sec-
tion III-B, neighbor sampling exhibits ﬁne-grained irregular
parallelism with low locality. Therefore, the mmap-based SSD
design point experiences signiﬁcant misses in the OS page
cache during neighbor sampling, so the merits of utilizing the
page cache to reap locality beneﬁts are outweighed by the
high latency overheads of maintaining the OS managed page
cache itself (e.g., handling the page-faults incurred during the
memory-mapped neighbor edge list array accesses, bringing
in the faulted pages into the page cache, frequent user-kernel
space context switches, etc). Consequently, the baseline SSD-
centric system suffers from a throughput mismatch between
the producer-consumer, leaving the GPU idle whenever the
work queue runs out of subgraphs to use as inputs for
GNN training. In Figure 7, we quantify the magnitude of
such producer-consumer throughput mismatch by showing the
fraction of training time where the GPU is left idle, waiting
for the subgraphs to be generated by the CPU-side producer
workers. The baseline in-memory processing model shows
high GPU utilization because the data preparation stage is
capable of generating input subgraphs at high throughput
(Figure 4(a)). With the baseline mmap-based SSD-centric
system, however, the producer workers fall short in sufﬁciently
providing large enough number of subgraphs for the GPU to
consume, exhibiting large periods of GPU idle time, causing
a signiﬁcant slowdown (Figure 4(b)).

Driven by our characterization, this paper explores an ISP
based SSD-centric architecture for large-scale GNN training.
As we detail in the next section, our proposition holistically
addresses the dual challenges of memory capacity limited
large-scale graph learning and the wide performance gap
between DRAM vs. SSD.

IV. SMARTSAGE ARCHITECTURE

A. Architecture Overview

Our proposed SmartSAGE architecture employs an ISP
accelerator tailored for subgraph generation, co-designed with
our latency-optimized software runtime system and host driver.
This section takes a bottom-up approach in presenting Smart-
SAGE, describing our hardware level innovations ﬁrst (Sec-
tion IV-B) followed by a discussion of the software architec-
ture that interfaces our proposed ISP architecture to the ML
frameworks (Section IV-C).

Fig. 6: (Left-axis) Breakdown of GNN training time into key steps of
data preparation (black-gray) and GNN training (blue). (Right-axis) End-to-
end training time normalized to the baseline in-memory processing system
(DRAM). Experiments are conducted using PyG.

that constitute a training mini-batch are typically scattered
across the input graph, the execution of neighbor sampling
is dominated by (random) memory lookup operations with
little compute intensity, exhibiting a highly sparse and irregular
dataﬂow. Interestingly, the off-chip memory bandwidth uti-
lization is generally low despite such high memory intensity,
consuming only an average 21% of 125 GB/sec maximum
memory throughput. This is because each sampling operation
only amounts to a ﬁne-grained 8 byte read transaction, leading
to severe underutilization of DRAM read throughput. Such
characterization result
the neighbor sampling
implies that
algorithm is severely memory latency limited, rather than
throughput limited, providing guidelines on optimizing our
SSD based training system. We now explore the implication
of conducting neighbor sampling over an SSD.

C. Data Preparation in SSD-centric Training

We establish our baseline SSD-centric training system to
store the memory capacity limited graph dataset (esp. the
neighbor edge list array) inside the SSD for the neighbor
sampling operation (Figure 3(b)). These data structures are
accessed using memory-mapped ﬁle I/O (mmap) which maps
the contents of the graph dataset ﬁle within the user-space
memory address. Because the most recently accessed pages are
buffered inside main memory’s OS page cache, it is possible
for our baseline SSD-centric system to signiﬁcantly reduce
the data fetch latency for high locality accesses, narrowing
the performance gap between DRAM vs. SSD.

Figure 6 provides a breakdown of end-to-end GNN training
time (left-axis) as well as normalized latency (right-axis) when
comparing the baseline in-memory processing vs. mmap-based
SSD-centric training system. As depicted, the baseline SSD-

5

020406080100020406080100RedditMovielensAmazonOGBN-100MProtein-PIDRAM BW utilization(%)LLC miss rate (%)LLC miss rateDRAM BW utilization05101520250%20%40%60%80%100%DRAMSSD (mmap)DRAMSSD (mmap)DRAMSSD (mmap)DRAMSSD (mmap)DRAMSSD (mmap)RedditMovielensAmazonOGBN-100MProtein-PILatency (normalized)Latency breakdownNeighbor samplingFeature lookupCPU-to-GPU transferGNN TrainingElseLatency020406080100DRAMSSD (mmap)DRAMSSD (mmap)DRAMSSD (mmap)DRAMSSD (mmap)DRAMSSD (mmap)RedditMovielensAmazonOGBN-100MProtein-PIGPU idle time (%)Fig. 8: A ﬁrmware-based CSD architecture. The embedded processor cores
execute routine SSD ﬁrmware tasks (e.g., FTL) to handle the host-side I/O
block requests and generate ﬂash page read/write requests from/to the ﬂash
devices, buffering them inside an on-device DRAM buffer (referred to as
SSD’s DRAM page buffer) to send it back to the host CPU. SmartSAGE
utilizes these embedded processor cores to run neighbor sampling directly off
of SSD’s DRAM page buffer, seeking to reduce latency.

Hardware. The SmartSAGE ISP is implemented as part of
the ﬁrmware within the SSD (Figure 8). Such design decision
allows SmartSAGE to maintain compatibility with existing
hardware (SSD hardware) and software (the OS and the NVMe
protocol) stack. By initiating neighbor sampling near SSD,
however, SmartSAGE is able to increase effective throughput
for subgraph generation by utilizing the internal SSD band-
width. Because the sampled, subgraphs are densely packed
within the SSD→DRAM returned logical blocks, SmartSAGE
can signiﬁcantly reduce unused data transferred over PCIe,
achieving high neighbor sampling throughput.

Software. SmartSAGE is designed to reduce the command
and control overheads in the OS and host driver stack by
optimizing the software runtime for latency rather than lo-
cality. Instead of needlessly incurring several
tens of mi-
croseconds of latency to maintain the opportunistic OS page
cache, SmartSAGE allocates a user-space scratchpad buffer
to manually orchestrate high locality data movements using
Linux direct I/O. This allows SmartSAGE software runtime
system to bypass the OS page cache thereby signiﬁcantly re-
ducing the latency overheads of traversing through the system
software stack. Additionally, SmartSAGE host driver employs
an ISP instruction that coalesces multiple I/O commands under
a single NVMe command, further reducing latency. Such
lightweight communication path is utilized by our SmartSAGE
ISP architecture to streamline the subgraph generation process
with high performance.

B. Hardware Acceleration using In-Storage Processing Archi-
tectures

Why choose ﬁrmware-based (and not FPGA-based)
CSDs for ISP? SmartSAGE ISP unit accelerates the subgraph
generation process by ofﬂoading the neighbor sampling oper-
ator as part of SSD’s ﬁrmware execution. There are currently
two prominent approaches in designing a computational stor-
age device (CSD) with ISP capabilities. One popular approach
in designing CSDs is to utilize the embedded cores within the
SSD device for in-storage processing at the SSD’s ﬁrmware
level (e.g., OpenSSD [43], NGD system’s Newport [15], [71],

Fig. 9: Key steps undertaken when conducting neighbor sampling over
an FPGA-based CSD (e.g., SmartSSD [66]). Because the SSD and FPGA
communicates over a PCIe switch integrated within the CSD device, neighbor
sampling over an FPGA-based CSD incurs a two-step P2P transfer (i.e.,
SSD→FPGA and then FPGA→CPU), unlike a ﬁrmware-based CSD where
a single SSD→CPU data transfer is invoked.

Biscuit [25]). An alternative CSD design point is to utilize
FPGA circuitry integrated near the SSD device and conduct
ISP at the hardware level (e.g., Samsung’s SmartSSD [66],
Eideticom’s NoLoad CSP [19]). In the remainder of this paper,
we refer to each of these CSD types as ﬁrmware-based CSD
and FPGA-based CSD, respectively.

We observe that ofﬂoading neighbor sampling to an FPGA-
based CSD is not cost-effective because of the following two
factors. First, as our characterization study revealed in Sec-
tion III, the neighbor sampling operator is mostly dominated
by random data lookups with very low compute intensity.
Therefore, synthesizing hardwired (FPGA) circuitry to merely
conduct data gathers and scatters can underutilize the FPGA
reconﬁgurable logic (although this problem can potentially
be alleviated by spatially sharing the FPGA among different
processes). But more crucially, neighbor sampling ofﬂoaded
to an FPGA-based CSD incurs a two-step P2P data transfer
which adds signiﬁcant performance overhead: 1) SSD→FPGA
to conduct the neighbor sampling using FPGA and generate
the subgraph (step 1/2 in Figure 9), and 2) FPGA→CPU
to transfer the subgraph to host CPU memory (step 3 in
Figure 9). We observe that the latency overheads of such
two-step neighbor sampling outweighs the beneﬁts of ISP
acceleration, providing little beneﬁts over the baseline mmap-
based SSD system.

To this end, SmartSAGE employs a ﬁrmware-based CSD
substrate to implement our ISP architecture (Figure 8). Be-
cause neighbor sampling is mostly composed of irregular
data lookups with low compute intensity, it suits well to the
wimpy embedded cores (relative to the host-side x86 CPUs)
equipped within ﬁrmware-based CSDs. Later in Section VI-D,
we quantitatively evaluate both CSD design points for the
completeness of our study.

Key observations and proposed approach. Figure 10
illustrates the key intuition behind SmartSAGE ISP neighbor
sampling operator. Here, the neighbor edge list array stores all
the neighborhood nodes’ IDs around a given graph node in a
sequential manner, for all the nodes within the graph dataset.
Under the baseline mmap-based SSD, the CPU initiates a
memory-mapped block I/O read requests to this data structure

6

NVMe/PCIe controllerDRAM(Page buffer)NAND flashFlash controllerDDR controllerHigh-speed interconnectHost systemNVMe/PCIeinterfaceEmbedded processor coresMemoryFPGASSDCPUPCIeSwitchSystem Interconnect (PCIe)132SmartSSD(a)

(b)

Fig. 10: (a) Subgraph generation under the baseline SSD-centric system
and (b) our SmartSAGE ISP based neighbor sampling operation, amplifying
effective throughput in generating the subgraph. The example illustrates the
process of extracting two subgraphs over two separate mini-batches (blue:
mini-batch #0, orange: mini-batch #1), each generating a subgraph from two
target nodes (target node ID of (blue: 0/65) and (orange: 30/90) and its
neighborhood nodes (sampling rate: 2 neighbors).

as means to fetch all the target node’s neighborhood ID list
into main memory. Because the target node IDs within a
mini-batch are rarely aligned consecutively, there exists little
spatial locality that can be reaped out by the CPU during
neighbor sampling. Consequently, the baseline SSD-centric
system ends up generating a large number of I/O block
fetch requests, proportional to the number of target nodes
subject for neighbor sampling (e.g., one edge list “chunk” is
fetched per each target node in Figure 10(a)). This results in
a signiﬁcant overfetching of useless data from SSD→CPU,
leading to severe underutilization of precious I/O bandwidth.
SmartSAGE ISP architecture, on the other hand, performs ﬁne-
grained gather operations from the edge list array directly
from the SSD’s DRAM page buffer, constructing a dense
ID list of “sampled” neighborhood node IDs (Figure 10(b)).
Because this dense, sampled node ID list (i.e., the subgraph
itself) is subject for SSD→CPU transfer, SmartSAGE is able
to signiﬁcantly amplify the effective throughput of neighbor
sampling.

Hardware/software interaction in ISP neighbor sam-
pling. We now detail the key components of SmartSAGE ISP
design as well as the associated hardware/software interactions
involved during in-storage neighbor sampling (Figure 11).

7

Fig. 11: Major components of SmartSAGE ISP architecture and the key steps
undertaken during the lifetime of in-storage neighbor sampling.

SmartSAGE adds two major components within the SSD
ﬁrmware: 1) The ISP control unit handles the CPU→SSD of-
ﬂoaded subgraph generation request, generating the necessary
set of commands to send to the subgraph generator. 2) The
subgraph generator is in charge of extracting out the subgraph
from the large-scale input graph, sending series of ﬂash page
read requests to the low-level ﬂash devices. Once the requested
ﬂash pages are returned and subsequently cached into SSD’s
DRAM page buffer, the subgraph generator initiates in-storage
neighbor sampling to gather the sampled nodes of the subgraph
under construction. Below we summarize the major steps
undertaken during SmartSAGE’s subgraph generation.

1) Our custom designed SmartSAGE driver (detailed in
Section IV-C) sends a subgraph generation request to the
SSD ﬁrmware intitially as an NVMe write command,
which includes a pointer to the neighbor sampling
conﬁguration data (NSconf ig) stored inside CPU mem-
ory. NSconf ig contains key parameters of the sampling
operation, e.g., number of target nodes as well as their
logical block address, neighborhood node IDs to sample,
and other metadata (step (cid:182) in Figure 11).

2) When the SSD ﬁrmware receives the subgraph genera-
tion request, it triggers a DMA access to CPU memory
to copy the NSconf ig data from CPU→SSD using SSD’s
NVMe host controller (step (cid:183)). Once the SSD receives
the NSconf ig data,
it goes through several stages to
prepare for subgraph generation, one major step being
the address translation process (from the logical address
to ﬂash’s physical page address) to determine where
within the ﬂash devices should the subgraph generator
send ﬂash page read requests to (step (cid:184)). A given
target node’s neighbor nodes’ ID list can potentially
require multiple ﬂash page read requests depending on
the number of edges connected to the target node.
3) The ﬂash page read requests are sent to the pending ﬂash
page request queue to prepare for in-storage neighbor
sampling, which the low-level ﬂash controller utilizes
to kick off ﬂash page read operations (step (cid:185)).

4) Once a ﬂash page read request is serviced, the corre-
sponding neighbor edge list will be cached inside SSD’s

0653090NeighborEdge List Array.........573105160357191206780124154097Node #0Node #30Node #65Node #90NeighborNodeIDs7310540352080124159767NeighborNodeIDs516071911. SSD→CPU transfer of neighborhood node IDs in block granularity chunks51607191976712415SampledNeighbor IDsSampledNeighbor IDs2. CPU samples from CPU memoryTarget Node ID(Subgraph #0)Target Node ID(Subgraph #1)0653090NeighborEdge List Array.........573105160357191206780124154097Node #0Node #30Node #65Node #901. Conduct in-storage neighbor sampling directly from the SSD’s DRAM page buffer.51607191976712415SampledNeighbor IDsSampledNeighbor IDs2. SSD→CPU transfer of sampled node IDsTarget Node ID(Subgraph #0)Target Node ID(Subgraph #1)51607191976712415SampledNeighbor IDsSampledNeighbor IDsCPUSmartSAGEdriverNSconfigDRAMSampledsubgraphNSconfigSubgraph bufferSSDDRAM page buffer1234567Flash page requestsEmbedded coreISP control unitSubgraph generatorDRAMHost coreSmartSAGEDriverFlashNAND flashLow-level schedulerDRAM page buffer (step (cid:186)). SmartSAGE utilizes the
SSD’s embedded cores to conduct ﬁne-grained neighbor
sampling over the neighbor edge list, stored inside the
SSD’s DRAM page buffer. The sampled nodes are
collected into ISP’s pending subgraph buffer (step (cid:187)).
5) Once all the target nodes’ neighbor IDs are sampled, the
sampled subgraph is ready to be transferred back to the
CPU. At the high level of the SSD ﬁrmware polling
loop, our scheduler checks for completed subgraph
generation requests. If the sampled subgraph is ready
and the NVMe host controller is available, we initiate
a SSD→CPU DMA write operation to copy back the
subgraph information into CPU DRAM (step (cid:188)).

C. Latency-optimized Runtime and Host Driver Stack

Although our ISP design helps signiﬁcantly reduce the
latency of neighbor sampling, there are still signiﬁcant per-
formance improvement opportunities to be reaped out. As
discussed in Section III, the baseline mmap-based SSD ex-
periences signiﬁcant slowdown as the locality-optimized OS
page cache is rarely useful in reducing I/O access time. Addi-
tionally, recall from Figure 10 that the baseline SSD invokes
a large number of I/O block fetch requests per each mini-
batch training as the data accesses to the neighbor edge list
exhibit low spatial locality (i.e., only a single edge list “chunk”
can be requested over a single I/O block fetch request), which
adds another layer of latency penalty. This is because servicing
any given I/O block fetch request involves context switching
through the user↔kernel space when traversing through the
host driver stack.

We design our software runtime and the host driver stack to
be optimized for latency ﬁrst and locality second, as detailed
below.

Direct I/O. Linux comes with a direct I/O feature where
the ﬁle read/write operations go directly from the user-space
application to the storage device, bypassing the OS page cache
completely (Figure 12). Given the locality limited nature of
neighbor sampling, our runtime system utilizes such feature
(O_DIRECT ﬂag) to allocate a user-space buffer to manually
orchestrate high locality data movements between SSD↔CPU,
without relying on the opportunistic OS page cache. As we
quantitatively analyze in Section VI-B, our direct I/O based
software runtime alone, even without the ISP feature, helps
reduce neighbor sampling latency by 2.9× compared to the
baseline mmap-based SSD system.

I/O command coalescing. As detailed in Section IV-B, the
in-storage neighbor sampling operation is invoked by sending
a subgraph generation request in the form of an NVMe write
command to the SSD. Unlike the baseline SSD-centric system
which spawns off multiple high latency I/O fetch requests
for a single subgraph generation, SmartSAGE’s host driver
encapsulates the entire gather/scatter operation of neighbor
sampling under a single NVMe transaction. More concretely,
all the key parameters of subgraph generation (e.g., logical
block address of all
target nodes, neighborhood node ID
lists to sample, etc) are stored as part of the NSconf ig data

Fig. 12: SmartSAGE’s latency-optimized direct I/O based software runtime
(left) and host driver stack employing I/O command coalescing (right).

TABLE I: Graph dataset information.

In-memory

Large-scale

Dataset

Nodes

Edges

Size (GB)

Nodes

Edges

Size (GB)

Features

233.0K
Reddit
5.5M
Movielens
Amazon
42.5M
OGBN-100M 89.6M
907.0K
Protein-PI

114.6M
6.0B
1.3B
3.2B
317.5M

0.8
45
9.7
26
2.4

37.3M 53.9B
22.2M 59.2B
265.9M 9.5B
179.1M 5.0B
8.8B
9.1M

402
442
75
41
66

602
1K
32
32
512

(Figure 11), which the SSD ﬁrmware retrieves through a one-
time CPU→SSD DMA transaction. This allows SmartSAGE
to signiﬁcantly reduce the number of I/O commands invoked
to generate a single subgraph, reducing command and control
overheads of the host driver stack and further reducing latency.
NVMe compatibility and system integration. Our custom
I/O interface is implemented using the ioctl() system
call, which maintains complete compatibility with current
NVMe protocols. The subgraph generation request is indicated
using a single unused command bit, which the SSD ﬁrmware
utilizes to invoke in-storage neighbor sampling. Aside from
this special-purpose bit, the hardware/software interface uti-
lizes the exact same command structure of conventional I/O
read/write commands and the CPU→SSD copy of NSconf ig
data as well as SSD→CPU copy of the ﬁnal, neighbor sampled
subgraph is orchestrated using existing DMA engines.

Because SmartSAGE’s runtime and host driver stack main-
tains compatibility with current NVMe protocol and the ISP
operator is implemented as part of the SSD ﬁrmware, Smart-
SAGE is fully compatible with existing CSD architectures.

V. METHODOLOGY

Hardware/software platform. We use PyTorch Geometric
to implement a GraphSAGE based GNN training pipeline.
When evaluating end-to-end training performance, we employ
a CPU-GPU system containing an Intel Xeon Gold 6242 CPU
with 192 GB of DRAM and NVIDIA’s Tesla T4 GPU.

CSD platform. We implement our neighbor sampling ISP
operator using the open-source Cosmos+ OpenSSD [43].
OpenSSD contains a fully functional NVMe ﬂash SSD with 2
TB of storage and a customizable SSD ﬁrmware executed by
a dual core ARMv7 Cortex-A9 processor. The host interface
controller of OpenSSD communicates with the CPU over an
8-lane PCIe (gen2) channel. We design SmartSAGE’s ISP
operator within OpenSSD’s ﬁrmware and measure wall clock
time for evaluating performance.

8

ApplicationOS page cachemmap-basedApplicationBufferDirect I/O-basedNVMedevice drivermini-batch0req1Direct I/O(as-is)mini-batch0req2mini-batch1req0mini-batch1req1coalesced req for mini-batch0coalesced reqfor mini-batch1SmartSAGESmartSAGEDriver(a) Reddit

Fig. 14: SmartSAGE’s speedup for neighbor sampling vs. baseline mmap-
based SSD system (single worker).

(b) Protein-PI

Fig. 13: Degree distribution of the in-memory (left) vs. large-scale (right)
graph datasets for a subset of our datasets. As depicted, despite Kronecker
fractal expansion enabling the number of nodes as well as edges to increase
dramatically (represented by the increase in the number of nodes with degree-
K in the y-axis), the overall power-law distribution of each dataset, before/after
fractal expansion is applied remains similar.

Comparison to alternative training systems. While we
demonstrate SmartSAGE’s merits using OpenSSD, for the
completeness of our study, we also evaluate SmartSAGE’s ISP
neighbor sampling operator on top of an FPGA-based CSD
using Samsung-Xilinx’s SmartSSD [8] (Section VI-D). We
also compare SmartSAGE against a training system utilizing
Intel’s Optane DC Persistent Memory Module (PMEM) that is
installed on the memory bus (NVDIMM), providing 768 GB
of capacity to store the graph datasets (Section VI-C).

Dataset. Existing graph datasets utilized for GNN studies
are generally at small-scale that comfortably ﬁt within main
memory, defeating the main purpose of our study. Prior
work on Kronecker Graph Theory [7] suggests graph fractal
expansion methods to expand the scale of the input graph
dataset while properly maintaining the graph’s distinctive
characteristics (e.g., power-law degree distribution, community
structure, etc). We therefore employ Kronecker fractal expan-
sion method [7] to synthetically generate large-scale graph
datasets, the key features of which are summarized in Table I
(the “Large-scale” column). Prior work [53] observed that
real-world graphs that become larger with more nodes and
edges generally exhibit higher average degrees (i.e., the rate at
which the number of edges increase is faster than the increase
in number of nodes), a property known as the densiﬁcation
power law. The synthetically generated large-scale datasets are
designed to properly reﬂect such behavior, represented by the
higher average degree of large-scale datasets vs. in-memory
datasets (Figure 13). The graph datasets are all compressed
in CSR (compressed sparse row) format and we employ the
default conﬁguration of GraphSAGE when training our GNN
algorithm with a mini-batch size of 1024. In Section VI-F, we
discuss the sensitivity of SmartSAGE when deviating from
these default conﬁgurations.

VI. EVALUATION

Fig. 15: Effect of reducing the I/O command coalescing granularity on
SmartSAGE(HW/SW) speedup.

system (SSD(mmap)), and two design points employing our
hardware/software optimizations, namely 2) direct I/O based
SSD training system without ISP (SmartSAGE(SW)) and 3)
our proposed architecture with all the proposed optimizations
in place (SmartSAGE(HW/SW)). Later in Section VI-C, we
compare SmartSAGE against an upper bound, oracular in-
memory processing design point which assumes the CPU
contains inﬁnite DRAM capacity so that all the graph datasets
can be stored locally in DRAM. SmartSAGE is also compared
against Intel’s Optane DC PMEM in Section VI-C and an ISP
architecture based on an FPGA-based CSD in Section VI-D.

A. “Single” Worker’s Neighbor Sampling Performance

As discussed in Figure 4, a GNN training pipeline employs
a producer-consumer model where multiple CPU-side workers
independently conduct neighbor sampling for subgraph gener-
ation. To precisely understand the efﬁcacy of our proposal on
improving a CPU-side worker’s neighbor sampling operation,
we ﬁrst instantiate just a single worker for subgraph gen-
eration and evaluate its performance (Figure 14). Compared
to the baseline mmap-based SSD system, our software-only
SmartSAGE with direct I/O runtime (SmartSAGE(SW)) alone
provides an average 1.5× speedup in neighbor sampling. Such
result demonstrates the advantage of using direct I/O to bypass
the OS page cache, which helps optimize the data preparation
stage for latency, rather than locality.

is

ISP

The

sampling

performance

SmartSAGE’s

neighbor
further with

enhanced
even
architecture
(SmartSAGE(HW/SW)), providing an additional average
speedup of 6.6× (maximum 7.8×) over the software-only
SmartSAGE(SW). Overall, SmartSAGE(HW/SW) achieves
an average 10.1× (maximum 12.6×) speedup vs. the mmap-
based SSD system, successfully resolving the bottlenecks of
the baseline architecture.

This section ﬁrst focuses on exploring the following three
design points: 1) the baseline mmap-based SSD training

It is worth mentioning that the signiﬁcant speedup Smart-
just a result of the hardware-level

SAGE achieves is not

9

02468101214SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)RedditMovielensAmazonOGBN-100MProtein-PISpeedup00.20.40.60.811.2102451225664161102451225664161102451225664161102451225664161102451225664161RedditMovielensAmazonOGBN-100MProtein-PIPerformanceFig. 16: SmartSAGE’s neighbor sampling speedup vs. baseline SSD(mmap)
(multiple workers). Results assume 12 concurrent workers as performance is
at its highest with 12 workers, for both baseline and SmartSAGE.

Fig. 18: Latency breakdown of end-to-end GNN training time.

level of speedup achieved with in-storage neighbor sampling.
Such phenomenon is shown in Figure 17, where Smart-
SAGE(HW/SW)’s speedup vs. SmartSAGE(SW) gets gradu-
ally reduced as we add more CPU-side workers. Nonethe-
less, we emphasize that our evaluation results are highly
conservative as state-of-the-art CSD architectures provide far
more performance than our OpenSSD platform. For instance,
recently announced CSDs like NGD system’s Newport [15],
[71] provides much faster compute and communication per-
formance for ISP usage. Unlike our OpenSSD platform which
utilizes a dual core ARMv7 Cortex-A9 for ﬁrmware execution,
Newport contains four ARMv7 M7 cores for ﬁrmware execu-
tion with an additional quad core ARMv8 Cortex-A53 solely
dedicated for ISP purposes. Due to limited public availability
of Newport CSDs, we employed the OpenSSD system as a
proof of concept to our proposal, demonstrating substantial
improvements in throughput vs. baseline SSD.

In general, we conclude that the SmartSAGE’s key insights
are directly applicable to any ﬁrmware-based CSD because
SmartSAGE maintains full compatibility with current NVMe
protocol and its ISP is programmed at the software level as
part of the SSD ﬁrmware.

C. End-to-End GNN Training Time

Figure 18 summarizes the effectiveness of SmartSAGE on
reducing end-to-end GNN training time. To thoroughly cover
the evaluation space, we explore two in-memory processing
based systems: 1) Intel’s Optane DC Persistent Memory Mod-
ule (PMEM) which can store the entire graph dataset within
its NVDIMMs and 2) an oracular DRAM-only design which
assumes the main memory is inﬁnitely sized to enable large-
scale GNN training at DRAM speed. We also establish a
hypothetical SmartSAGE design point which assumes that the
CSD architecture contains dedicated, ISP-purposed embedded
cores (like Newport CSD [15], [71]) such that the oppor-
tunties inherent with our in-storage neighbor sampling can
be fully unlocked (SmartSAGE(oracle)). SmartSAGE(oracle)’s
performance is estimated by assuming the neighbor sampling
speedup achieved under single worker execution (Figure 14)
can equally be achieved under multiple workers.

Compared to the baseline mmap-based SSD system, Smart-
SAGE (HW/SW) provides an average 3.5× (maximum 5.0×)
improvement in training throughput. While such improvements
are impressive, SmartSAGE(HW/SW) still incurs an average
60% performance loss vs.
the DRAM-only design point.

Fig. 17: SmartSAGE(HW/SW)’s speedup vs. SmartSAGE(SW) when scaling
up the number of CPU-side workers, from 1 to 12.

acceleration of neighbor sampling or direct I/O based software
runtime; also equally important is our co-designed host driver
stack that minimizes the I/O command and control overheads.
In Figure 15, we show the effect of reducing the I/O command
coalescing granularity on SmartSAGE(HW/SW)’s speedup.
The default conﬁguration of SmartSAGE(HW/SW) coalesces
all the target nodes’ neighbor sampling within a given mini-
batch (i.e., 1024 target nodes, the leftmost design point) under
a single NVMe command, which is encapsulated inside a
single NSconf ig (neighbor sampling conﬁguration data, see
Figure 11). As the coalescing granularity becomes smaller
(from left to right in the x-axis), the latency overheads of
sending the I/O commands to the SSD start outweighing
the beneﬁts provided with ISP, experiencing a signiﬁcant
performance hit.

Overall, the evaluation in this section highlights the effec-
tiveness of SmartSAGE’s software/hardware co-design; 1) di-
rect I/O, 2) I/O command coalescing, and 3) ISP acceleration.

B. “Multiple” Workers’ Neighbor Sampling Performance

We now explore the effectiveness of SmartSAGE when
multiple workers are concurrently accessing the storage system
for subgraph generation (Figure 16). SmartSAGE(HW/SW)
provides an average 4.4× (maximum 5.5×) speedup in neigh-
bor sampling compared to the baseline SSD(mmap). Com-
pared to the single worker execution scenario (Figure 14),
however, notice that the additional speedup provided with
SmartSAGE(HW/SW) vs. SmartSAGE(SW) is reduced. Our
analysis showed that, when multiple workers simultaneously
access the SSD for in-storage neighbor sampling, OpenSSD’s
relatively wimpy embedded cores get overwhelmed in deliver-
ing sufﬁcient levels of ISP compute power for high throughput
neighbor sampling. More concretely, since our OpenSSD
based neighbor sampling operator time-shares the embedded
cores with the ﬂash management ﬁrmware, the interference our
neighbor sampling causes to the SSD ﬁrmware degrades the

10

0123456SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)RedditMovielensAmazonOGBN-100MProtein-PISpeedup02468124812124812124812124812124812RedditMovielensAmazonOGBN-100MProtein-PISpeedup00.20.40.60.811.2SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SmartSAGE (oracle)PMEMDRAMSSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SmartSAGE (oracle)PMEMDRAMSSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SmartSAGE (oracle)PMEMDRAMSSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SmartSAGE (oracle)PMEMDRAMSSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SmartSAGE (oracle)PMEMDRAMRedditMovielensAmazonOGBN-100MProtein-PILatency breakdown (normalized)Neighbor samplingFeature lookupCPU-to-GPU transferGNN TrainingElseFig. 19: FPGA-based CSD vs. SSD(mmap) and SmartSAGE(SW).

Nonetheless, such DRAM-only architecture is an unbuildable,
upper bound design point and existing in-memory processing
based systems are not able to train the large-scale GNNs
that our SSD-based system will enable. Intel PMEM does
much better than the baseline SSD(mmap), “only” incurring an
average 1.2× slowdown vs. the oracular DRAM-only design.
However,
the performance advantage of PMEM comes at
the cost of lower storage density and lower GB/$ vs. SSD.
Compared to these high cost in-memory processing based
systems, SmartSAGE(oracle) performs very competitively,
achieving an average 70% and 90% of the performance of the
DRAM-only and PMEM based systems, respectively. These
results demonstrate that, with newer/future CSD architectures
provisioned with more ISP compute power and faster ﬂash
devices, an NVMe SSD based system can become a viable
option for large-scale GNN training while not compromising
on performance.

D. Comparison to FPGA-based CSD Designs

In Section IV-B, we argued that a ﬁrmware-based CSD
is much more appropriate for in-storage neighbor sampling
compared to an FPGA-based CSD. Figure 19 quantitatively
demonstrates our rationale behind such design decision. We
utilize Samsung-Xilinx’s SmartSSD [66]
to implement a
neighbor sampling operator within SmartSSD’s FPGA logic
(denoted “FPGA-CSD”). Such an FPGA-CSD based design
conducts in-storage neighbor sampling by: 1) copying the nec-
essary chunks of neighbor edge list array from SSD→FPGA
using P2P transfer (gray), 2) utilizing FPGA’s hardwired
neighbor node gather unit to conduct sampling over FPGA’s
local DRAM (yellow), and 3) transferring the sampled sub-
graph from FPGA→CPU (blue). As depicted, the performance
of FPGA-CSD is bottlenecked on the latency to move the
neighbor edge list array chunks from SSD→FPGA, failing to
achieve any performance advantage even over our software-
only SmartSAGE(SW).

E. Power and Energy Consumption

The CPU-GPU based GNN training system consumes sev-
eral thousands of watts of system-wide power (i.e., the CPU,
GPU, DRAM, SSD). Due to the following factors, the added
power overheads of SmartSAGE is expected to be negligible,
allowing the signiﬁcant reduction in training time to propor-
tionally improve system-level energy-efﬁciency. First, Smart-
SAGE(HW/SW) is a purely ﬁrmware-based CSD that utilizes
existing embedded cores within the SSD. Consequently, the

Fig. 20: Sensitivity of SmartSAGE’s speedup to different sampling algorithm,
i.e., GraphSAINT.

Fig. 21: Sensitivity of SmartSAGE’s end-to-end speedup to sampling rate.

added power overheads of SmartSAGE(HW/SW), if any, is
amortized by the signiﬁcant reduction in end-to-end training
time. For the more future-looking SmartSAGE(oracle) design
that integrates additional, dedicated ISP-purposed embedded
cores (assuming quad core ARMv8 Cortex-A53 like NGD
Newport), the CSD will add around 2 − 6 watts of TDP of
additional power consumption, which is a reasonable overhead
(vs. CPU/GPU/DRAM/SSD’s system-level power consump-
tion) given the signiﬁcant reduction in training time.

F. Sensitivity

This subsection evaluates SmartSAGE’s robustness to vari-

ous conﬁguration parameters of GNN training.

Alternative sampling algorithm. To demonstrate the ro-
bustness of our proposal, we implement another state-of-
the-art graph sampling algorithm, GraphSAINT [88]. Unlike
GraphSAGE, GraphSAINT employs a regular random walk
based method to sample a neighbor per node across multiple
target nodes. As depicted in Figure 20, SmartSAGE achieves
an average of 8.2× end-to-end speedup with GraphSAINT,
demonstrating our proposal’s robustness and ﬂexilibity in
accommodating various graph sampling algorithms.

Sampling rate. Figure 21 shows SmartSAGE’s neigh-
bor sampling speedup when we sweep the sampling rate
by 0.5× and 2× of the default setting (default: 25 and
10 neighbor nodes sampling per each target node in the
ﬁrst and second GNN layer,
respectively). As depicted,
SmartSAGE(HW/SW)’s speedup is gradually decreased (in-
creased) as sampling rate gets larger (smaller). Under Smart-
SAGE(HW/SW), as we increase the sampling rate, the sub-
graph being transferred over SSD→CPU gets larger and even-

11

00.20.40.60.811.2SSD (mmap)SmartSAGE (SW)FPGA-CSDSSD (mmap)SmartSAGE (SW)FPGA-CSDSSD (mmap)SmartSAGE (SW)FPGA-CSDSSD (mmap)SmartSAGE (SW)FPGA-CSDSSD (mmap)SmartSAGE (SW)FPGA-CSDRedditMovielensAmazonOGBN-100MProtein-PILatency breakdownSSD->CPUSSD ->FPGAFPGA->CPUSampling (FPGA)Sampling (host CPU)Else02468101214SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)RedditMovielensAmazonOGBN-100MProtein-PISpeedup012345678SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)SSD (mmap)SmartSAGE (SW)SmartSAGE (HW/SW)0.5x1.0x2.0x0.5x1.0x2.0x0.5x1.0x2.0x0.5x1.0x2.0x0.5x1.0x2.0xRedditMovielensAmazonOGBN-100MProtein-PISpeeduptually approaches the data transfer size of SmartSAGE(SW).
Such phenomenon renders SmartSAGE(HW/SW)’s speedup
to become smaller than when evaluated under the default
sampling rate.

Training batch size. We also study SmartSAGE’s sensi-
tivity to larger/smaller mini-batch sizes. Results showed that
the chosen mini-batch size have little effect on SmartSAGE’s
achieved speedup. In general, we conﬁrmed SmartSAGE’s
robustness under different design points but omit the results
due to space constraints.

VII. RELATED WORK

The focus of our work is on accelerating large-scale GNN
training with an ISP architecture. There is a large body of
prior literature exploring in-storage/near-data processing [1],
[5], [10], [12], [13], [16], [25], [27], [31]–[33], [35], [38],
[38], [39], [42], [49], [51], [54], [58], [65], [67], [72]–[74],
[76], [81], [81]–[83] or in-memory processing [2]–[4], [9],
[18], [20], [23], [29], [34], [36], [37], [44], [45], [50], [60],
[68], [79] architectures for data-intensive workloads as well
as ASIC/FPGA/GPU based acceleration for graph neural net-
works [24], [40], [52], [54]–[56], [59], [69], [80], [84], [87].
There is also prior work exploring heterogeneous memory
systems for training large-scale ML algorithms [11], [28],
[30], [46]–[48], [61]–[64], [77]. Due to space limitations, we
summarize a subset of these related work below.

ISP designs for data-intensive workloads. BlueDBM [31]
investigated an ISP architecture targeting big data analytics,
where a dedicated FPGA fabric is utilized for accelerating
nearest neighbor search, string search, and graph traversal.
Biscuit [25] similarly explores ISP acceleration for big data
the ISP substrate utilized is more close to
workloads but
a ﬁrmware-based CSD rather than BlueDBM’s FPGA-based
CSD design. Active Flash [72], Catalina [73], ActiveSort [51],
Morpheus [74], GraFBoost [32], and work by Do et al. [16]
are also ISP architectures that target data analytics (e.g., pat-
tern matching, k-mean clustering, etc), database management,
MapReduce frameworks, object (de)serialization, graph ana-
lytics, and Microsoft SQL, respectively. Aside from these prior
art focusing on domain-speciﬁc acceleration of a particular
application area, there is also a set of previous work that seeks
to enhance the programmability of ISP designs. Willow [67]
explores architectural support to enhance the programmability
and ﬂexibility of using programmable SSDs for ISP. Summa-
rizer [42] proposes a set of ﬂexible programming APIs for
conducting ISP-based data ﬁltering and data summarization
operations. GraphSSD [58] proposes a graph semantic aware
ISP architecture that provides a simple programming interface
for enhanced programmability. The adoption of CSDs for
domain-speciﬁc acceleration is a trait SmartSAGE resembles
to these prior literature, especially with Biscuit, Summarizer,
and GraphSSD. Unlike SmartSAGE, however, Biscuit employs
a dedicated pattern matching IP for ISP, which cannot be
utilized for GNN’s graph sampling. Similarly, GraphSSD is
speciﬁcally optimized for efﬁcient graph data layout mapping
within the physical pages, focusing on in-storage graph update

operators for bandwidth ampliﬁcation, which cannot be readily
employed for graph sampling. Summarizer bears similarity
to SmartSAGE in that it supports in-storage reduction op-
erations for generic data-intensive workloads like TPC-H.
Because Summarizer does not target GNN training, however,
it lacks several of SmartSAGE’s software-level optimizations
like direct I/O or mini-batch level I/O command coalescing
(Figure 12). More crucially, our work targets a new, emerging
application domain (i.e., machine learning based graph neural
network training) and uncovers a new system-level bottleneck
driven by different intuitions, rendering our key contributions
unique.

Accelerating GNN inference. There is also a rich set
of previous work exploring hardware/software acceleration
techniques for machine learning inference [24], [38], [40],
[55], [56], [69], [80], [81], [84]. While not speciﬁcally tar-
geting GNNs, RecSSD [81] and work by Kim et al. [38] are
recent ISP designs utilizing ﬁrmware-based CSDs to overcome
the memory capacity bottlenecks of the inference stage of
DNN-based recommendation models. HyGCN [84] is one of
the ﬁrst ASIC based GCN accelerators providing substantial
energy-efﬁciency improvement for the backend GNN layers of
graph learning. GLIST [54] is an FPGA-based CSD for GNN
inference and GNNAdvisor [80] proposes an efﬁcient software
runtime for GPU-based GNN execution. Unlike these prior
work focusing on accelerating the inference of GNNs, Smart-
SAGE focuses on uncovering the system-level bottlenecks of
training, more speciﬁcally the frontend data preparation stage.
Overall, the key contribution of SmartSAGE is orthogonal to
these prior studies.

Accelerating GNN training. GraphACT [87] proposes a
CPU-FPGA based acceleration platform for GNN training.
Marius [59] is a software framework that aims to provide high-
performance large-scale graph learning within a single ma-
chine. Unlike SmartSAGE however, Marius targets a different
graph learning algorithm so the model parameters trained with
Marius are the graph embedding vectors, not the GNN layers
(as is the case with SmartSAGE). More importantly, Marius
proposes a software-level pipelining solution for fast training,
unlike the ISP based SmartSAGE. Similar to SmartSAGE,
prior work on AliGraph/PaGraph/DistDGL [57], [89], [90]
employ off-the-shelf graph partitioning algorithms for large-
scale GNN training, partitioning the graph sampling and
learning process across multiple workers for parallel GNN
training. As pointed out by Su et al. [70], distributed GNN
training can suffer from load imbalance issues (e.g., number
of k-hop sampled nodes can differ across graph partitions),
intermittent model parameter synchronization overheads, and
etc. SmartSAGE explores a different research space for GNN
training systems, enabling large-scale GNN training within
a single machine using NVMe SSD while still achieving
DRAM-level performance. Overall, the contribution of Smart-
SAGE is orthogonal to these studies.

12

VIII. CONCLUSION

In this work, we investigate the viability of utilizing NVMe
SSDs to overcome the memory capacity limitations of current,
in-memory processing GNN training systems. We propose
an in-storage processing based GNN training system called
SmartSAGE which synergistically combines the ISP capabil-
ities of emerging CSDs with a latency-optimized software
runtime and host driver stack. By intelligently ofﬂoading
the data intensive frontend data preparation stage of GNN
training, SmartSAGE signiﬁcantly resolves the bottlenecks of
the baseline SSD-centric training system, achieving signiﬁcant
performance improvements.

ACKNOWLEDGEMENT

This research is partly supported by the National Re-
search Foundation of Korea (NRF) grant funded by the Ko-
rea government(MSIT) (NRF-2021R1A2C2091753), the Su-
per Computer Development Leading Program of the NRF
funded by the Korea government MSIT under grant NRF-
2020M3H6A1085498, and by Samsung Electronics Co., Ltd
(IO201210-07974-01). We also appreciate the support from
Samsung Advanced Institute of Technology (SAIT) and the
EDA tools supported by the IC Design Education Center
(IDEC), Korea. Minsoo Rhu is the corresponding author.

REFERENCES

[1] A. Acharya, M. Uysal, and J. Saltz, “Active Disks: Programming Model,
Algorithms and Evaluation,” ACM SIGOPS Operating Systems Review,
1998.

[2] S. Aga, S. Jeloka, A. Subramaniyan, S. Narayanasamy, D. Blaauw,
and R. Das, “Compute Caches,” in Proceedings of the International
Symposium on High-Performance Computer Architecture (HPCA), 2017.
[3] M. Alian, S. W. Min, H. Asgharimoghaddam, A. Dhar, D. K. Wang,
T. Roewer, A. McPadden, O. O’Halloran, D. Chen, J. Xiong, D. Kim,
W.-m. Hwu, and N. S. Kim, “Application-Transparent Near-Memory
Processing Architecture with Memory Channel Network,” in Proceed-
ings of the International Symposium on Microarchitecture (MICRO),
2018.

[4] H. Asghari-Moghaddam, Y. H. Son, J. H. Ahn, and N. S. Kim,
“Chameleon: Versatile and Practical Near-DRAM Acceleration Archi-
tecture for Large Memory Systems,” in Proceedings of the International
Symposium on Microarchitecture (MICRO), 2016.

[5] D.-H. Bae, J.-H. Kim, S.-W. Kim, H. Oh, and C. Park, “Intelligent SSD:
A Turbo for Big Data Mining,” in Proceedings of the ACM International
Conference on Information & Knowledge Management, 2013.

[6] T. Bansal, D. Belanger, and A. McCallum, “Ask the GRU: Multi-task
Learning for Deep Text Recommendations,” in Proceedings of the ACM
Conference on Recommender Systems (RECSYS), 2016.

[7] F. Belletti, K. Lakshmanan, W. Krichene, Y.-F. Chen, and J. Anderson,
“Scalable Realistic Recommendation Datasets through Fractal Expan-
sions,” arXiv preprint arXiv:1901.08910, 2019.

[8] W. Cheong, C. Yoon, S. Woo, K. Han, D. Kim, C. Lee, Y. Choi, S. Kim,
D. Kang, G. Yu, J. Kim, J. Park, K.-W. Song, K.-T. Park, S. Cho, H. Oh,
D. D. Lee, J.-H. Choi, and J. Jeong, “A Flash Memory Controller for
15us Ultra-Low-Latency SSD Using High-Speed 3D NAND Flash with
3us Read Time,” in Proceedings of the International Solid State Circuits
Conference (ISSCC), 2018.

[9] P. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y. Liu, Y. Wang, and Y. Xie,
“PRIME: A Novel Processing-in-Memory Architecture for Neural Net-
work Computation in ReRAM-Based Main Memory,” in Proceedings of
the International Symposium on Computer Architecture (ISCA), 2016.

[10] B. Y. Cho, W. S. Jeong, D. Oh, and W. W. Ro, “XSD: Accelerating
MapReduce by Harnessing the GPU Inside an SSD,” in Proceedings of
the 1st Workshop on Near-Data Processing, 2013.

[11] M. Cho, T. Le, U. Finkler, H. Imai, Y. Negishi, T. Sekiyama, S. Vinod,
V. Zolotov, K. Kawachiya, D. Kung, and H. Hunter, “Large Model
Support for Deep Learning in Caffe and Chainer,” in SysML, February
2018.

[12] S. Cho, C. Park, H. Oh, S. Kim, Y. Yi, and G. R. Ganger, “Active Disk
Meets Flash: A Case For Intelligent SSDs,” in Proceedings of the ACM
International Conference on Supercomputing (ICS), 2013.

[13] I. S. Choi and Y.-S. Kee, “Energy Efﬁcient Scale-In Clusters with
In-Storage Processing for Big-Data Analytics,” in Proceedings of the
International Symposium on Memory Systems (MEMSYS), 2015.
[14] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, “Learning Steady-
States of Iterative Algorithms over Graphs,” in ICML, 2018, pp. 1114–
1122.

[15] J. Do, V. C. Ferreira, H. Bobarshad, M. Torabzadehkashi, S. Rezaei,
A. Heydarigorji, D. Souza, B. F. Goldstein, L. Santiago, M. S. Kim,
P. M. V. Lima, F. M. G. Franca, and V. Alves, “Cost-Effective,
Energy-Efﬁcient, and Scalable Storage Computing for Large-Scale AI
Applications,” ACM Transactions on Storage, 2020.

[16] J. Do, Y.-S. Kee, J. M. Patel, C. Park, K. Park, and D. J. DeWitt,
“Query Processing on Smart SSDs: Opportunities and Challenges,”
the ACM SIGMOD International Conference on
in Proceedings of
Management of Data (MOD), 2013.

[17] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gomez-
Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams, “Convo-
lutional Networks on Graphs for Learning Molecular Fingerprints,” in
Proceedings of the International Conference on Neural Information
Processing Systems (NIPS), 2015.

[18] C. Eckert, X. Wang, J. Wang, A. Subramaniyan, R. Iyer, D. Sylvester,
D. Blaaauw, and R. Das, “Neural Cache: Bit-Serial In-Cache Acceler-
ation of Deep Neural Networks,” in Proceedings of the International
Symposium on Computer Architecture (ISCA), 2018.

[19] “NoLoad CSP,” 2021. [Online]. Available: https://www.eideticom.com/

products.html

[20] A. Farmahini-Farahani, J. H. Ahn, K. Morrow, and N. S. Kim, “NDA:
Near-DRAM Acceleration Architecture Leveraging Commodity DRAM
Devices and Standard Memory Modules,” in Proceedings of the Interna-
tional Symposium on High-Performance Computer Architecture (HPCA),
2015.

[21] M. Fey and J. E. Lenssen, “Fast Graph Representation Learning with
PyTorch Geometric,” in Proceedings of the International Conference on
Learning Representations (ICLR), 2019.

[22] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur, “Protein Interface
Prediction Using Graph Convolutional Networks,” in Proceedings of
the International Conference on Neural Information Processing Systems
(NIPS), 2017.

[23] M. Gao, J. Pu, X. Yang, M. Horowitz, and C. Kozyrakis, “TETRIS:
Scalable and Efﬁcient Neural Network Acceleration with 3D Memory,”
in Proceedings of the International Conference on Architectural Support
for Programming Languages and Operating Systems (ASPLOS), 2017.
[24] T. Geng, A. Li, R. Shi, C. Wu, T. Wang, Y. Li, P. Haghi, A. Tumeo,
S. Che, S. Reinhardt, and M. Herbordt, “AWB-GCN: A Graph Con-
volutional Network Accelerator with Runtime Workload Rebalancing,”
in Proceedings of the International Symposium on Microarchitecture
(MICRO), 2020.

[25] B. Gu, A. S. Yoon, D.-H. Bae, I. Jo, J. Lee, J. Yoon, J.-U. Kang,
M. Kwon, C. Yoon, S. Cho, J. Jeong, and D. Chang, “Biscuit: A Frame-
work for Near-Data Processing of Big Data Workloads,” in Proceedings
of the International Symposium on Computer Architecture (ISCA), 2016.
[26] W. L. Hamilton, R. Ying, and J. Leskovec, “Inductive Representation
Learning on Large Graphs,” in Proceedings of the International Confer-
ence on Neural Information Processing Systems (NIPS), 2017.

[27] Y.-C. Hu, M. T. Lokhandwala, T. I, and H.-W. Tseng, “Dynamic
Multi-Resolution Data Storage,” in Proceedings of the International
Symposium on Microarchitecture (MICRO), 2019.

[28] C.-C. Huang, G. Jin, and J. Li, “SwapAdvisor: Pushing Deep Learning
Beyond the GPU Memory Limit via Smart Swapping,” in Proceedings of
the International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS), 2020.

[29] R. Hwang, T. Kim, Y. Kwon, and M. Rhu, “Centaur: A Chiplet-based,
Hybrid Sparse-Dense Accelerator for Personalized Recommendations,”
in Proceedings of the International Symposium on Computer Architec-
ture (ISCA), 2020.

[30] H. Jin, B. Liu, W. Jiang, Y. Ma, X. Shi, B. He, and S. Zhao, “Layer-
Centric Memory Reuse and Data Migration for Extreme-Scale Deep

13

Learning on Many-Core Architectures,” ACM Transactions on Architec-
ture and Code Optimization, 2018.

[31] S.-W. Jun, M. Liu, S. Lee, J. Hicks, J. Ankcorn, M. King, S. Xu,
and Arvind, “BlueDBM: An Appliance for Big Data Analytics,” in
Proceedings of the International Symposium on Computer Architecture
(ISCA), 2015.

[32] S.-W. Jun, A. Wright, S. Zhang, S. Xu, and Arvind, “GraFboost: Using
Accelerated Flash Storage for External Graph Analytics,” in Proceedings
of the International Symposium on Computer Architecture (ISCA), 2018.
[33] Y. Kang, Y.-s. Kee, E. L. Miller, and C. Park, “Enabling Cost-Effective
Data Processing With Smart SSD,” in Proceedings of the IEEE Sympo-
sium on Mass Storage Systems and Technologies (MSST), 2013.
[34] L. Ke, U. Gupta, B. Y. Cho, D. Brooks, V. Chandra, U. Diril,
A. Firoozshahian, K. Hazelwood, B. Jia, H.-H. S. Lee, M. Li, B. Maher,
D. Mudigere, M. Naumov, M. Schatz, M. Smelyanskiy, X. Wang,
B. Reagen, C.-J. Wu, M. Hempstead, and X. Zhang, “RecNMP: Acceler-
ating Personalized Recommendation with Near-Memory Processing,” in
Proceedings of the International Symposium on Computer Architecture
(ISCA), 2020.

[35] K. Keeton, D. A. Patterson, and J. M. Hellerstein, “A Case for Intelligent

Disks (IDISKs),” Acm Sigmod Record, 1998.

[36] B. Kim, J. Park, E. Lee, M. Rhu, and J. H. Ahn, “TRiM: Tensor
Reduction in Memory,” in IEEE Computer Architecture Letters, 2020.
[37] D. Kim, J. Kung, S. Chai, S. Yalamanchili, and S. Mukhopadhyay, “Neu-
rocube: A Programmable Digital Neuromorphic Architecture with High-
Density 3D Memory,” in Proceedings of the International Symposium
on Computer Architecture (ISCA), 2016.

[38] M. Kim and S. Lee, “Reducing Tail Latency of DNN-based Recom-
mender Systems using In-Storage Processing,” in Proceedings of the
ACM SIGOPS Asia-Paciﬁc Workshop on Systems (APSys), 2020.
[39] S. Kim, H. Oh, C. Park, S. Cho, S.-W. Lee, and B. Moon, “In-Storage
Processing of Database Scans and Joins,” Information Sciences, 2016.
[40] K. Kiningham, C. Re, and P. Levis, “GRIP: A Graph Neural Network
Accelerator Architecture,” arXiv preprint arXiv:2007.13828, 2020.
[41] T. N. Kipf and M. Welling, “Semi-Supervised Classiﬁcation with Graph
Convolutional Networks,” in Proceedings of the International Confer-
ence on Learning Representations (ICLR), 2017.

[42] G. Koo, K. K. Matam, T. I., H. K. G. Narra, J. Li, H.-W. Tseng,
S. Swanson, and M. Annavaram, “Summarizer: Trading Communication
with Computing Near Storage,” in Proceedings of the International
Symposium on Microarchitecture (MICRO), 2017.

[43] J. Kwak, S. Lee, K. Park, J. Jeong, and Y. H. Song, “Cosmos+ OpenSSD:
Rapid Prototype for Flash Storage Systems,” ACM Transactions on
Storage, 2020.

[44] Y. Kwon, Y. Lee, and M. Rhu, “TensorDIMM: A Practical Near-
Memory Processing Architecture for Embeddings and Tensor Operations
in Deep Learning,” in Proceedings of the International Symposium on
Microarchitecture (MICRO), 2019.

[45] Y. Kwon, Y. Lee, and M. Rhu, “Tensor Casting: Co-Designing
Algorithm-Architecture for Personalized Recommendation Training,”
in Proceedings of the International Symposium on High-Performance
Computer Architecture (HPCA), 2021.

[46] Y. Kwon and M. Rhu, “A Case for Memory-Centric HPC System
Architecture for Training Deep Neural Networks,” in IEEE Computer
Architecture Letters, 2018.

[47] Y. Kwon and M. Rhu, “Beyond the Memory Wall: A Case for Memory-
Centric HPC System for Deep Learning,” in Proceedings of the Inter-
national Symposium on Microarchitecture (MICRO), 2018.

[48] Y. Kwon and M. Rhu, “A Disaggregated Memory System for Deep

Learning,” in IEEE Micro, 2019.

[49] J. Lee, H. Kim, S. Yoo, K. Choi, H. P. Hofstee, G.-J. Nam, M. R. Nutter,
and D. Jamsek, “Extrav: Boosting Graph Processing Near Storage with a
Coherent Accelerator,” Proceedings of the VLDB Endowment (PVLDB),
2017.

[50] S. Lee, S.-h. Kang, J. Lee, H. Kim, E. Lee, S. Seo, H. Yoon, S. Lee,
K. Lim, H. Shin, J. Kim, O. Seongil, A. Iyer, D. Wang, K. Sohn, and
N. S. Kim, “Hardware Architecture and Software Stack for PIM Based
on Commercial DRAM Technology,” in Proceedings of the International
Symposium on Computer Architecture (ISCA), 2021.

[51] Y.-S. Lee, L. C. Quero, S.-H. Kim, J.-S. Kim, and S. Maeng, “ActiveSort:
Efﬁcient External Sorting Using Active SSDs in the MapReduce Frame-
work,” Future Generation Computer Systems, 2016.

[52] Y. Lee, Y. Kwon, and M. Rhu, “Understanding the Implication of Non-
Volatile Memory for Large-Scale Graph Neural Network Training,” in
IEEE Computer Architecture Letters, 2021.

[53] J. Leskovec, J. Kleinberg, and C. Faloutsos, “Graphs over Time:
Densiﬁcation Laws, Shrinking Diameters and Possible Explanations,”
in Proceedings of
the ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), 2005.

[54] C. Li, Y. Wang, C. Liu, S. Liang, H. Li, and X. Li, “GLIST: Towards
In-Storage Graph Learning,” in Proceedings of USENIX Conference on
Annual Technical Conference (ATC), 2021.

[55] J. Li, A. Louri, A. Karanth, and R. Bunescu, “GCNAX: A Flexi-
ble and Energy-Efﬁcient Accelerator for Graph Convolutional Neural
Networks,” in Proceedings of the International Symposium on High-
Performance Computer Architecture (HPCA), 2021.

[56] S. Liang, Y. Wang, C. Liu, L. He, H. Li, D. Xu, and X.-W. Li, “EnGN:
A High-Throughput and Energy-Efﬁcient Accelerator for Large Graph
Neural Networks,” IEEE Transactions on Computers, 2020.

[57] Z. Lin, C. Li, Y. Miao, Y. Liu, and Y. Xu, “PaGraph: Scaling GNN Train-
ing on Large Graphs via Computation-Aware Caching,” in Proceedings
of the ACM Symposium on Cloud Computing (SoCC), 2020.

[58] K. K. Matam, G. Koo, H. Zha, H.-W. Tseng, and M. Annavaram,
“GraphSSD: Graph Semantics Aware SSD,” in Proceedings of
the
International Symposium on Computer Architecture (ISCA), 2019.
[59] J. Mohoney, R. Waleffe, H. Xu, T. Rekatsinas, and S. Venkataraman,
“Marius: Learning Massive Graph Embeddings on a Single Machine,”
in Proceedings of USENIX Symposium on Operating Systems Design
and Implementation (OSDI), 2021.

[60] J. Park, B. Kim, S. Yun, E. Lee, M. Rhu, and J. H. Ahn, “TRiM:
Enhancing Processor-Memory Interfaces with Scalable Tensor Reduc-
tion in Memory,” in Proceedings of the International Symposium on
Microarchitecture (MICRO), 2021.

[61] X. Peng, X. Shi, H. Dai, H. Jin, W. Ma, Q. Xiong, F. Yang, and X. Qian,
“Capuchin: Tensor-based GPU Memory Management for Deep Learn-
ing,” in Proceedings of the International Conference on Architectural
Support for Programming Languages and Operating Systems (ASPLOS),
2020.

[62] J. Ren, J. Luo, K. Wu, M. Zhang, H. Jeon, and D. Li, “Sentinel: Efﬁcient
Tensor Migration and Allocation on Heterogeneous Memory Systems
for Deep Learning,” in Proceedings of the International Symposium on
High-Performance Computer Architecture (HPCA), 2021.

[63] M. Rhu, N. Gimelshein, J. Clemons, A. Zulﬁqar, and S. W. Keckler,
“vDNN: Virtualized Deep Neural Networks for Scalable, Memory-
Efﬁcient Neural Network Design,” in Proceedings of the International
Symposium on Microarchitecture (MICRO), October 2016.

[64] M. Rhu, M. O’Connor, N. Chatterjee, J. Pool, Y. Kwon, and S. W.
Keckler, “Compressing DMA Engine: Leveraging Activation Sparsity
for Training Deep Neural Networks,” in Proceedings of the Interna-
tional Symposium on High-Performance Computer Architecture (HPCA),
February 2018.

[65] E. Riedel, G. Gibson, and C. Faloutsos, “Active Storage for Large-
Scale Data Mining and Multimedia Applications,” in Proceedings of
Conference on Very Large Databases (VLDB), 1998.

[66] “SmartSSD,”

2021.

[Online]. Available:

https://www.xilinx.com/

applications/data-center/computational-storage/smartssd.html

[67] S. Seshadri, M. Gahagan, S. Bhaskaran, T. Bunker, A. De, Y. Jin,
Y. Liu, and S. Swanson, “Willow: A User-Programmable SSD,” in
Proceedings of USENIX Symposium on Operating Systems Design and
Implementation (OSDI), 2014.

[68] A. Shaﬁee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P.
Strachan, M. Hu, R. S. Williams, and V. Srikumar, “ISAAC: A Convo-
lutional Neural Network Accelerator with In-Situ Analog Arithmetic in
Crossbars,” in Proceedings of the International Symposium on Computer
Architecture (ISCA), 2016.

[69] X. Song, T. Zhi, Z. Fan, Z. Zhang, X. Zeng, W. Li, X. Hu, Z. Du,
Q. Guo, and Y. Chen, “Cambricon-G: A Polyvalent Energy-Efﬁcient
Accelerator for Dynamic Graph Neural Networks,” IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems, 2021.

[70] Q. Su, M. Wang, D. Zheng, and Z. Zhang, “Adaptive Load Balancing for
Parallel GNN Training,” in Proceedings of MLSys Workshop on Graph
Neural Networks and Systems (GNNSys), 2021.

[71] N. Systems, “Newport Platform,” 2021. [Online]. Available: https:

//www.ngdsystems.com/solutions#NewportSection

[72] D. Tiwari, S. Boboila, S. Vazhkudai, Y. Kim, X. Ma, P. Desnoyers,
and Y. Solihin, “Active Flash: Towards Energy-Efﬁcient, In-Situ Data

14

Analytics on Extreme-Scale Machines,” in Proceedings of USENIX
Conference on File and Storage Technologies (FAST), 2013.

[73] M. Torabzadehkashi, S. Rezaei, A. Heydarigorji, H. Bobarshad,
V. Alves, and N. Bagherzadeh, “Catalina: In-Storage Processing Acceler-
ation for Scalable Big Data Analytics,” in Proceedings of the Euromicro
International Conference on Parallel, Distributed and Network-Based
Processing (PDP), 2019.

[74] H.-W. Tseng, Q. Zhao, Y. Zhou, M. Gahagan, and S. Swanson, “Mor-
pheus: Creating Application Objects Efﬁciently for Heterogeneous Com-
puting,” in Proceedings of the International Symposium on Computer
Architecture (ISCA), 2016.

[75] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Ben-
gio, “Graph Attention Networks,” in Proceedings of the International
Conference on Learning Representations (ICLR), 2018.

[76] J. Wang, D. Park, Y.-S. Kee, Y. Papakonstantinou, and S. Swanson,
“SSD In-Storage Computing for List Intersection,” in Proceedings of
the International Workshop on Data Management on New Hardware
(DaMoN), 2016.

[77] L. Wang, J. Ye, Y. Zhao, W. Wu, A. Li, S. L. Song, Z. Xu, and T. Kraska,
“Superneurons: Dynamic GPU Memory Management for Training Deep
Neural Networks,” in Proceedings of the Symposium on Principles and
Practice of Parallel Programming (PPOPP), 2018.

[78] M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma,
L. Yu, Y. Gai, T. Xiao, T. He, G. Karypis, J. Li, and Z. Zhang, “Deep
Graph Library: A Graph-Centric, Highly-Performant Package for Graph
Neural Networks,” arXiv preprint arXiv:1909.01315, 2019.

[79] X. Wang, J. Yu, C. Augustine, R. Iyer, and R. Das, “Bit Prudent In-Cache
Acceleration of Deep Convolutional Neural Networks,” in Proceedings
of the International Symposium on High-Performance Computer Archi-
tecture (HPCA), 2019.

[80] Y. Wang, B. Feng, G. Li, S. Li, L. Deng, Y. Xie, and Y. Ding,
“GNNAdvisor: An Adaptive and Efﬁcient Runtime System for GNN
Acceleration on GPUs,” in Proceedings of USENIX Symposium on
Operating Systems Design and Implementation (OSDI), 2021.

[81] M. Wilkening, U. Gupta, S. Hsia, C. Trippel, C.-J. Wu, D. Brooks,
and G.-Y. Wei, “RecSSD: Near Data Processing for Solid State Drive
Based Recommendation Inference,” in Proceedings of the International
Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS), 2021.

[82] L. Woods, Z. Istv´an, and G. Alonso, “Ibex: An Intelligent Storage Engine
with Support for Advanced SQL Ofﬂoading,” Proceedings of the VLDB
Endowment (PVLDB), 2014.

[83] S. Xu, T. Bourgeat, T. Huang, H. Kim, S. Lee, and A. Arvind,
“AQUOMAN: An Analytic-Query Ofﬂoading Machine,” in Proceedings
of the International Symposium on Microarchitecture (MICRO), 2020.
[84] M. Yan, L. Deng, X. Hu, L. Liang, Y. Feng, X. Ye, Z. Zhang, D. Fan,
and Y. Xie, “HyGCN: A GCN Accelerator with Hybrid Architecture,”
in Proceedings of the International Symposium on High-Performance
Computer Architecture (HPCA), 2020.

[85] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and
J. Leskovec, “Graph Convolutional Neural Networks for Web-Scale Rec-
ommender Systems,” in Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD), 2018.

[86] R. Ying, J. You, C. Morris, X. Ren, W. L. Hamilton, and J. Leskovec,
“Hierarchical Graph Representation Learning with Differentiable Pool-
ing,” in Proceedings of the International Conference on Neural Infor-
mation Processing Systems (NIPS), 2018.

[87] H. Zeng and V. Prasanna, “GraphACT: Accelerating GCN Training
on CPU-FPGA Heterogeneous Platforms,” in Proceedings of the ACM
International Symposium on Field-Programmable Gate Arrays (FPGA),
2020.

[88] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna, “Graph-
SAINT: Graph Sampling Based Inductive Learning Method,” in Pro-
ceedings of the International Conference on Learning Representations
(ICLR), 2020.

[89] D. Zheng, C. Ma, M. Wang, J. Zhou, Q. Su, X. Song, Q. Gan, Z. Zhang,
and G. Karypis, “DistDGL: Distributed Graph Neural Network Training
for Billion-Scale Graphs,” arXiv preprint arXiv:2010.05337, 2021.
[90] R. Zhu, K. Zhao, H. Yang, W. Lin, C. Zhou, B. Ai, Y. Li, and
J. Zhou, “AliGraph: A Comprehensive Graph Neural Network Platform,”
Proceedings of the VLDB Endowment (PVLDB), 2019.

15

