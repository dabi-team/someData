Towards Optimizing OCR for Accessibility

Peya Mowar

Tanuja Ganu
Microsoft Research, India
{t-peyamowar, tanuja.ganu, saikat}@microsoft.com

Saikat Guha

2
2
0
2

n
u
J

4
2

]

V
C
.
s
c
[

2
v
4
5
2
0
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

Visual cues such as structure, emphasis, and icons
play an important role in efﬁcient information foraging by
sighted individuals and make for a pleasurable reading ex-
perience. Blind, low-vision and other print-disabled indi-
viduals miss out on these cues since current OCR and text-
to-speech software ignore them, resulting in a tedious read-
ing experience. We identify four semantic goals for an en-
joyable listening experience, and identify syntactic visual
cues that help make progress towards these goals. Empiri-
cally, we ﬁnd that preserving even one or two visual cues in
aural form signiﬁcantly enhances the experience for listen-
ing to print content.

1. Introduction

Worldwide, there are over 250 million blind and low-
vision (BLV), or otherwise print-disabled individuals [11].
Accessible long-form content, such as textbooks, maga-
zines, newspapers, and literature, is a key enabler for their
intellectual enrichment and high quality of life. Braille and
audio books are scarce or expensive, while human-mediated
solutions like having friends and family read out content is
an imposition reserved only for the most critically needed
content.

Computer vision based apps [8] that combine optical
character recognition (OCR) and text-to-speech (TTS) tend
to be used for micro-content, such as labels on medicines
and packaged goods, not for long-form content. Figure 1
illustrates why. On left is an article from a daily newspa-
per, and on the right is the OCR output of the same from a
popular OCR app. The OCR output lacks all the visual cues
(e.g., structure, emphasis, icons) that sighted users depend
on to get a quick overview of the content before selectively
drilling down. The lack of visual cues combined with the
linear sequential nature of audio ruins the enjoyability of
automatically converted long-form content.

OCR was never optimized for the accessibility use case.
Driven primarily by the information retrieval use-case,
whether searching in books [4] or recognizing handwrit-

Figure 1. A news article and it’s OCR output.

ten postal codes [10], OCR entirely ignores visual cues as
evidenced by the output of popular OCR libraries and ser-
vices [5, 7, 9]. At the same time, recognizing all possible
visual cues across all languages is a signiﬁcant undertaking
with questionable utility for the accessibility use case given
the constraints placed by the linearity and limited aural cues
(e.g. voice, tempo) of the eventual audio output.

This work argues for more research in preserving visual
cues in aural form and takes the ﬁrst steps in that direction.
Speciﬁcally, we i) deﬁne the OCR for accessibility problem
and propose metrics for glanceability, readability, listen-
ability, and satisﬁability of long-form content, ii) identify
the simplest visual cues that, when preserved as aural cues,
signiﬁcantly enhance the accessibility of long-form content,
and iii) apply our approach to daily newspaper content and
provide initial results. Overall, we believe a concerted effort
into optimizing OCR and TTS for accessibility of long-form
content is possible and holds signiﬁcant promise in enrich-
ing the lives of print-disabled individuals.

2. Background

2.1. Accessibility Research

Accessibility research has majorly concentrated on in-
creasing digital access for BLV users [18]. This includes
improving the narrative experience for digital mediums
such as IDEs [21,24], collaborative documents [16] and sci-

1

 
 
 
 
 
 
entiﬁc reports [28]. The focal point of the research commu-
nity has been generating non-visual representations for vi-
sual content. This is done either in the form of rich and de-
scriptive alt-text [19] or using auditory cues such as earcons
and their derivatives (spearcons, lyricons, etc.) [12].

These non-visual representations are generated to substi-
tute “visual information scents” that help users easily navi-
gate to information of their interest. This process is referred
to as information foraging and has been studied extensively
for web access to BLV users [26].

2.2. Text Recognition and OCRs

Optimal character recognition (popularly known as
OCR) systems and more broadly text recognition systems
extract text from images. They are mostly utilized as a form
of data entry for digitizing print content such as bank state-
ments, legal records, healthcare reports, and invoices. They
are also employed in applications such as machine transla-
tion [6] and cognitive computing [1].

Table 1 summarizes the output functionalities of some
state-of-the-art OCR APIs. The research in this domain
contributes in improving these OCRs in one of three ways:
i) scene-text, handwriting, cursive text recognition (in the
wild) [14, 22, 23, 27, 29, 31] ii) developing models for Indic
languages [13] and iii) extracting text in the correct reading
order [15, 25, 30].

2.3. Research Gaps

Prior accessibility research has not focused on print-
impaired users (that include BLV, illiterate, and users with
learning and physical disabilities), and more speciﬁcally on
print accessibility. More work has been done for digital ac-
cess, where text metadata is more commonly available in
the form of HTML tags and alternate texts. Non-proﬁts such
as the DAISY Forum [3] in India have addressed this need,
however, they rely on manual intervention due to the lack
of important metadata in the OCR text outputs. This leads
to a huge restraint in the speed and costs associated with
digitization for the print-impaired.

The required metadata is nothing else but the proxy for
the visual information scents present in printed content.
These visual cues or “syntaxes” include but are not lim-
ited to bold, italics, underline, highlighting, font style, size,
color, ordered and unordered lists, shapes such as boxes and
lines, spacing, indentation, alignment, shadow, background
color. Since current OCR systems lack in providing this
information, a print-disabled reader is unable to leverage
these cues resulting in a higher time cost to gain value from
the same content.

Thus, there is a need for text recognition systems that
can preserve these visual cues for narration. However, not
all syntaxes may be equally relevant to optimize the users’
information gain, and thus, it is vital to understand the de-

gree of abstraction necessary to alleviate the accessibility
challenges presented by their underlying semantics [17].

3. Methodology

Our research goal is to identify the visual cues, which
when presented to the users, result in an optimal rate of
information gain. To do this, we consider the underlying
semantics of these cues, the lack of which pose certain ac-
cessibility challenges. We classify these challenges in long-
form text content into four broad categories:

Figure 2. Visual cues in a page from a magazine.

1. Glanceability: In order to emphasise key content for
the users, long-form texts such as newspapers, books and
magazines contain content with varying grades of emphasis.
For example, breaking news is often centered on the ﬁrst
page, occupying a signiﬁcant amount of area to grab the
reader’s attention; and similarly, headings and subheadings
in books or magazines often have bolden text and a larger
font size than the remaining text as shown in Figure 2. This
relies on the users’ ability to glance at the page, view the
emphasized content and get the gist of the whole content,
and is a challenge for print-impaired users who do not get
access to the emphasized content from an OCR.

2. Readability: This is the ease with which the users can
comprehend the content. Sighted users can understand the
organization and structure of the content due to the text for-
matting such as a breakout box depicted using a colored
shape in the background or list elements marked by bullet
points or enlarged ﬁrst alphabets as shown in Figure 2. On
the other hand, print-impaired users listen to the same con-
tent with no context being provided about the structure of
the content, which is innately visual in nature and thus lose
out on the comprehensibility of the content.

3. Listenability: The listening experience very often dif-
fers from the reading experience of content due to the lack
of spatial information which can be randomly accessed. Re-
vealing too many visual cues in the narration may become
overwhelming for the listener. Thus, along with readabil-
ity, it is also ﬁtting to take listenability of the content into
consideration. This is done by considering the ability of the
users to listen, understand and enjoy the narrative version

2

OCR APIs

Output Functionalities

PyTesseract
Azure Cognitive Services Read API Recognised characters, words, sentences, box boundaries, conﬁdences, orientation, script, whether handwriting
Google Cloud OCR

Recognised characters, box boundaries, conﬁdences, orientation, script

Recognised words, phrases, polygonal boundaries, conﬁdences, script

Table 1. Output functionalities of state-of-the-art OCR APIs

of a long-form text content.

4. Satisﬁability: Often in print media, a plethora of col-
ors, icons, shapes, and font styles are used, as shown in
Figure 2, in order to attract and retain the readers, improve
their overall reading experience and most importantly, make
it fun and create appropriate psychological impact. Thus,
when we digitize the same print content for print-impaired
users, it becomes imperative to take these fun visual ele-
ments into consideration and make the narration a reward-
ing and enjoyable experience for them. This is what we
refer to as satisﬁability - when the content meets the user
expectations.

4. Experiments and Learnings

We conducted two sets of experiments - an interview
study and a survey to ﬁnd the least degree of abstractive for-
matting required that can address these challenges deﬁned
in Section 3. We perform our initial set of experiments on
8 articles extracted from the front page of The Hindu, a
popular English daily newspaper in India. The articles con-
tain 250-500 words and a diverse set of visual cues such as
breakout boxes, list elements and ﬁgures.

4.1. Interview Study

The objective of this experiment was to observe patterns
in the reading behaviour of sighted users and derive insights
on glanceability, readability and satisﬁability.

4.1.1 Experiment Setup

We conducted task-based, semi-structured user interviews.
We recruited 8 participants for reading the content. All par-
ticipants were above 18 years of age, sighted and proﬁcient
in the English language. The interviews were conducted
over video conferencing and were 30 minutes long in dura-
tion.

For each article, we extracted the text using the Azure
Read API [7] and generated four stages of visual format-
ting, as explained in Table 2. We created four decks - each
containing the 8 articles, 2 articles per a format style. The
participants were randomly allotted one deck to perform
their tasks on.

a given text content. In the interview, we requested the par-
ticipants to ﬁnd and locate all the subheadings present in the
news article. We then noted the the accuracy and the time
that the participants took in ﬁnding these subheadings.

2. Readability: We measured the time taken by the users
in identifying different structural elements to evaluate read-
ability of the content.
In our interviews, the participants
were asked to identify if the news article contains a break-
out box (which typically contains a summarization, time-
line, quote or statistics) in the article. We noted the time
and accuracy with which the participants responded.

3. Satisﬁability: The metric we used to evaluate satisﬁ-
ability was the users’ self-assessment of their reading ex-
perience based on the User Engagement Scale [20]. We
asked the users to rate their responses to the following two
statements on a Likert Scale ranging from Strongly Dis-
agree to Strongly Agree (1-5): i) The reading experience
was rewarding. ii) The reading experience was enjoyable.

4.1.3 Quantitative Results

1. Glanceability: Accuracy - Without any visual cues (F0),
the participants were able to correctly identify the subhead-
ings only 18.75% of the time. Simply by boldening the im-
portant text (F1), the accuracy raised to 68.75%. Time Cost
- On an average, the participants spent 43 seconds to iden-
tify the headline in unformatted text (F0). In fact, 43.75%
of the time, the users spent less than 30 seconds and gave
up. Moreover, of the users who gave accurate responses for
the task, the median time spent to ﬁnd the subheading in
unformatted text was 80 seconds, signiﬁcantly higher than
any of the other formats, as is evident from Figure 3a.

2. Readability: Accuracy - The accuracy of responses
positively correlated with the presence of visual cues, rising
from 25% when no cues were given (F0) to 81.25% when
text was emphasised and structured (F2). Time Cost - The
median time spent by the participants on unformatted text
was 91 seconds, and this time dropped signiﬁcantly (to 20
seconds) with a single grade increase in formatting (F1).
This is illustrated in Figure 3b.

3. Satisﬁability: On an average, we found that partici-
pants found the content to be both more rewarding and en-
joyable with more visual cues. This is illustrated in Fig-
ure 3c.

4.1.2 Evaluation

4.2. Survey

1. Glanceability: We evaluated glanceability by noting the
time taken by the users in identifying the key content from

The objective of this experiment was to garner insights
on the relative listenability of two audios and validate if in-

3

Visual Formats Description

F0
F1
F2
F3

Plain text extracted from the article using Azure Cognitive Services OCR in ‘natural reading order’
To F0, added emphasis by boldening the headline, subheading, byline, breakout box as well as metadata
To F1, changing font colors for all the text present in bounding boxes to provide a sense of structuring
Original article image with all formatting intact

Auditory format Description

A0
A1

F0 narrated as is by the voice ‘Neerja – English Indian’
F1 narrated where the bolden text narrated by the voice ‘Prabhat – English Indian’ and the plain article text narrated by ‘Neerja
– English Indian’

Table 2. Considered visual and auditory formats

(a) Glanceability.

(b) Readability.

(c) Satisﬁability.

(d) Listenability.

Figure 3. Visualisations of the quantitative results.

serting the simplest aural cues in the narration improves the
overall listening experience of the users.

4.2.1 Experiment Setup

We distributed the survey to our friends and family and
ended up receiving 12 responses. All participants were
above 18 years of age, sighted and proﬁcient in the English
language. The survey took an average time of 40 minutes
for completion.

We leveraged the Azure Speech Studio [2] for convert-
ing two of the previously mentioned formats to audio nar-
rations. The audio formats are explained further in detail in
Table 2.

(A1), the narration became 31% better (than A0) for the
listeners. Figure 3d illustrates the results of the survey.

4.3. Qualitative Results

1. Heavy reliance on bolden text: We observed that some
participants completely relied on bolden text to ﬁnd the sub-
headings. P2, when presented with unformatted text (F0),
simply gave up the search: ”This article doesn’t have any
subheadings. There’s nothing in bold”. Moreover, some
participants declared that everything bolden in F1 (head-
line, meta-data, byline, etc.) was the subheading, without
actually going through the content.

2. Cases and Punctuation as substitutes: During the in-
terviews, participants heavily depended on cases and punc-
tuation (visual cues picked up by the OCR) for ﬁnding el-
ements. Text in quotes were declared as subheadings, and
a continuous stream of uppercase letters as breakout boxes.
This had a negative effect. For instance, an article had a
quote inside a breakout box. No participant was able to cor-
rectly identify that breakout box.

3. Emphasis provided a break in monotony: The survey
participants pointed out that usage of two different voices
helped break the monotonous ﬂow of the audio, provided a
better outline of the article and improved the overall listen-
ing experience - ”The variation ( the use of a different voice
to speak out, what mostly appeared to be heading), helping
alleviating the monotonicity of the spoken content.”.

4. Lack of visual appeal affected reading experience:
Some participants were severely underjoyed to see unfor-
matted text (F0) and rated the reading experience without
actually going through it in entirety. This points to how ap-
peal plays a major role in attracting a reader and making it a
rewarding experience. When in the deck, the original article
(F3) showed up, the reaction was of pure bliss and comfort.

4.2.2 Evaluation and Quantitative Results

5. Conclusion

1. Metrics: We evaluated listenability by asking the users
to compare between two audio clips of different formats
and answer whether one was ‘much better’, ‘better’, ‘same’,
‘worse’ or ‘much worse’ than the other. Moreover, we also
took the listener’s qualitative inputs on their listening expe-
rience into account.

2. Results: We discovered that simply by emphasizing

In this paper, we articulated the need for preserving vi-
sual cues in aural form in order to enhance the accessibil-
ity of printed content for blind, low-vision, and other print-
disabled individuals. We found that existing OCR services
and current research in OCR largely ignore visual cues that
are necessary for information foraging. We proposed met-
rics for glanceability, readability, listenability and overall

4

satisﬁability, and characterised how adding even one or two
visual cues in aural form signiﬁcantly improve the narration
experience for print-disabled individuals.

References

[1] Azure

cognitive

https : / / azure .
microsoft.com/en- in/services/cognitive-
services/. 2

services.

[2] Azure speech studio. https://speech.microsoft.

com/portal. 4

[3] Daisy forum of india. https://daisyindia.org/. 2
[4] Google books. https://books.google.co.in/. 1
[5] Google cloud vision api. https://cloud.google.

com/vision/docs/ocr. 1

[6] Google lens. https://lens.google/. 2
[7] Microsoft

api.
https : / / docs . microsoft . com / en -
us / azure / cognitive - services / computer -
vision/overview-ocr#read-api. 1, 3

cognitive

services

azure

read

[8] Microsoft

lens.

https : / / play . google . com /
store / apps / details ? id = com . microsoft .
office.officelens&gl=US. 1

[9] Pytesseract.

https : / / pypi . org / project /

pytesseract/. 1

[10] The mnist database of handwritten digits. http://yann.

lecun.com/exdb/mnist/. 1

[11] World blindness and visual impairment: despite many suc-
cesses, the problem is growing. https://www.ncbi.
nlm . nih . gov / pmc / articles / PMC5820628 / # :
˜:text=In%202015%2C%20there%20were%20an%
20estimated%20253%20million,whom%200.49%
25%20are%20blind%20and%202.95%25%20have%
20MSVI. 1

[12] Stephen A. Brewster, Peter C. Wright, and Alistair D. N. Ed-
wards. An evaluation of earcons for use in auditory human-
computer interfaces. In Proceedings of the INTERACT ’93
and CHI ’93 Conference on Human Factors in Computing
Systems, CHI ’93, page 222–227, New York, NY, USA,
1993. Association for Computing Machinery. 2

[13] Agam Dwivedi, Rohit Saluja, and Ravi Kiran Sarvadevab-
hatla. An ocr for classical indic documents containing arbi-
trarily long words. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR)
Workshops, June 2020. 2

[14] Sharon Fogel, Hadar Averbuch-Elor, Sarel Cohen, Shai Ma-
zor, and Roee Litman. Scrabblegan: Semi-supervised vary-
ing length handwritten text generation. In 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 4323–4332, 2020. 2

[15] Chen-Yu Lee, Chun-Liang Li, Chu Wang, Renshen Wang,
Yasuhisa Fujii, Siyang Qin, Ashok Popat, and Tomas Pﬁs-
ter. ROPE: Reading order equivariant positional encoding for
graph-based document information extraction. In Proceed-
ings of the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International Joint Con-
ference on Natural Language Processing (Volume 2: Short

5

Papers), pages 314–321, Online, Aug. 2021. Association for
Computational Linguistics. 2

[16] Cheuk Yin Phipson Lee, Zhuohao Zhang, Jaylin Herskovitz,
JooYoung Seo, and Anhong Guo. Collabally: Accessible
collaboration awareness in document editing. In CHI Con-
ference on Human Factors in Computing Systems, CHI ’22,
New York, NY, USA, 2022. Association for Computing Ma-
chinery. 1

[17] Anastassia Loukina, Van Rynald T. Liceralde, and Beata
Beigman Klebanov. Towards understanding text factors in
oral reading. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume
1 (Long Papers), pages 2143–2154, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics. 2
[18] Kelly Mack, Emma McDonnell, Dhruv Jain, Lucy Lu Wang,
Jon E. Froehlich, and Leah Findlater. What do we mean by
“accessibility research”? a literature survey of accessibility
papers in chi and assets from 1994 to 2019. In Proceedings of
the 2021 CHI Conference on Human Factors in Computing
Systems, CHI ’21, New York, NY, USA, 2021. Association
for Computing Machinery. 1

[19] Meredith Ringel Morris, Jazette Johnson, Cynthia L. Ben-
nett, and Edward Cutrell. Rich representations of visual con-
tent for screen reader users. In Proceedings of the 2018 CHI
Conference on Human Factors in Computing Systems, CHI
’18, page 1–11, New York, NY, USA, 2018. Association for
Computing Machinery. 2

[20] Heather L. O’Brien, Paul Cairns, and Mark Hall. A practi-
cal approach to measuring user engagement with the reﬁned
user engagement scale (ues) and new ues short form. Inter-
national Journal of Human-Computer Studies, 112:28–39,
2018. 3

[21] Venkatesh Potluri, Priyan Vaithilingam, Suresh Iyengar,
Y. Vidya, Manohar Swaminathan, and Gopal Srinivasa.
Codetalk: Improving programming environment accessibil-
ity for visually impaired developers. In Proceedings of the
2018 CHI Conference on Human Factors in Computing Sys-
tems, CHI ’18, page 1–11, New York, NY, USA, 2018. As-
sociation for Computing Machinery. 1

[22] Liang Qiao, Sanli Tang, Zhanzhan Cheng, Yunlu Xu, Yi Niu,
Shiliang Pu, and Fei Wu. Text perceptron: Towards end-to-
end arbitrary-shaped text spotting. Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, 34(07):11899–11907,
Apr. 2020. 2

[23] Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, and
Umapada Pal. Stefann: Scene text editor using font adaptive
In 2020 IEEE/CVF Conference on Com-
neural network.
puter Vision and Pattern Recognition (CVPR), pages 13225–
13234, 2020. 2

[24] Harini Sampath, Alice Merrick, and Andrew Macvean. Ac-
cessibility of command line interfaces. In Proceedings of the
2021 CHI Conference on Human Factors in Computing Sys-
tems, CHI ’21, New York, NY, USA, 2021. Association for
Computing Machinery. 1

[25] Hongyin Tang, Xingwu Sun, Beihong Jin, and Fuzheng
Zhang. A bidirectional multi-paragraph reading model for

zero-shot entity linking. Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, 35(15):13889–13897, May
2021. 2

[26] Pooja Upadhyay. Comparing non-visual and visual informa-
tion foraging on the web. In Extended Abstracts of the 2020
CHI Conference on Human Factors in Computing Systems,
CHI EA ’20, page 1–8, New York, NY, USA, 2020. Associ-
ation for Computing Machinery. 2

[27] Zhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai, and
Cong Yao. Textscanner: Reading characters in order for ro-
bust scene text recognition. Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, 34(07):12120–12127, Apr.
2020. 2

[28] Lucy Lu Wang, Isabel Cachola, Jonathan Bragg, Evie Yu-
Yen Cheng, Chelsea Haupt, Matt Latzke, Bailey Kuehl,
Madeleine van Zuylen, Linda Wagner, and Daniel S. Weld.
Improving the accessibility of scientiﬁc documents: Current
state, user needs, and a system solution to enhance scientiﬁc
pdf accessibility for blind and low vision users, 2021. 2
[29] Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu,
Gang Yu, and Shuai Shao. Shape robust text detection with
In 2019 IEEE/CVF
progressive scale expansion network.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 9328–9337, 2019. 2

[30] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu
Wei. Layoutreader: Pre-training of text and layout for read-
ing order detection. In EMNLP 2021, November 2021. 2
[31] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang
Zhou, Weiran He, and Jiajun Liang. East: An efﬁcient and
In 2017 IEEE Conference on
accurate scene text detector.
Computer Vision and Pattern Recognition (CVPR), pages
2642–2651, 2017. 2

6

