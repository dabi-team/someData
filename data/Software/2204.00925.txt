A UCB-BASED TREE SEARCH APPROACH TO JOINT
VERIFICATION-CORRECTION STRATEGY FOR LARGE SCALE
SYSTEMS

A PREPRINT

Peng Xu
Grado Department of Industrial and Systems Engineering
Virginia Tech
Blacskburg, VA 24060
xupeng@vt.edu

Xinwei Deng
Department of Statistics
Virginia Tech
Blacskburg, VA 24060
xdeng@vt.edu

Alejandro Salado
Department of Systems and Industrial Engineering
The University of Arizona
Tucson, AZ 85721
alejandrosalado@arizona.edu

April 5, 2022

ABSTRACT

Veriﬁcation planning is a sequential decision-making problem that speciﬁes a set of veriﬁcation
activities (VA) and correction activities (CA) at different phases of system development. While VAs
are used to identify errors and defects, CAs also play important roles in system veriﬁcation as they
correct the identiﬁed errors and defects. However, current planning methods only consider VAs as
decision choices. Because VAs and CAs have different activity spaces, planning a joint veriﬁcation-
correction strategy (JVCS) is still challenging, especially for large-size systems. Here we introduce
a UCB-based tree search approach to search for near-optimal JVCSs. First, veriﬁcation planning is
simpliﬁed as repeatable bandit problems and an upper conﬁdence bound rule for repeatable bandits
(UCBRB) is presented with the optimal regret bound. Next, a tree search algorithm is proposed
to search for feasible JVCSs. A tree-based ensemble learning model is also used to extend the
tree search algorithm to handle local optimality issues. The proposed approach is evaluated on the
notional case of a communication system.

Keywords veriﬁcation planning · Bayesian network · multi-armed bandit problem · correction activity · tree search ·
random forest

This work has been submitted to the IEEE for possible publication. Copyright may be
transferred without notice, after which this version may no longer be accessible.

2
2
0
2

r
p
A
2

]
E
S
.
s
c
[

1
v
5
2
9
0
0
.
4
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
1 Introduction

A PREPRINT

System veriﬁcation is deﬁned as the process that evaluates whether a system or its components fulﬁll their require-
ments [Engel, 2010]. System veriﬁcation is planned and implemented as a veriﬁcation strategy (VS) that speciﬁes how
to implement activities at different developmental phases and on different system conﬁgurations [Salado and Kannan,
2018a]. A VS consists of veriﬁcation activities (VA), each of which is used to identify errors and defects, and correc-
tion activities (CA) that correct the identiﬁed errors and defects. While VAs includes inspection, analysis, analogy,
demonstration, test, and sampling, CAs usually take the form of rework, repair, or redesign, depending on whether
the activity encompasses substitution of a faulty part, modiﬁcation of the product, or modiﬁcation of the product’s de-
sign [Engel, 2010, for Standardization, 2015]. A VS is deﬁned aiming at three objectives: “maximizing conﬁdence on
veriﬁcation coverage, which facilitates convincing a customer that contractual obligations have been met; minimizing
risk of undetected problems, which is important for a manufacturer’s reputation and to ensure customer satisfaction
once the system is operational; and minimizing invested effort, which is related to manufacturer’s proﬁt” [Salado,
2015].

strategy planning methods have been proposed to design VSs,

Several
including decomposition ap-
proach [Barad and Engel, 2006], set-based design [Xu and Salado, 2019], parallel tempering method [Xu et al., 2021],
and reinforcement learning method [Xu et al., 2020]. All these methods treat only VAs as dedicated decisions that
are planned for a system conﬁguration at a development phase (i.e., system state), and CAs are simpliﬁed as default
actions or even ignored reactively [Xu and Salado, Under Review]. This simpliﬁcation may undermine the value of
resulting VSs because of the potential suboptimality of CAs. Thus, an extended paradigm of veriﬁcation planning is
presented in our previous paper to include both VAs and CAs as the result of independent decisions. All dependency
relationships of VAs and CAs are summarized in that extended paradigm. An order-based dynamic programming
(ODP) method is also proposed to ﬁnd the optimal joint veriﬁcation-correction strategies (JVCS) that speciﬁes VAs
and CAs along a veriﬁcation process [Xu and Salado, Under Review]. However, as there are more possible activities,
the number of the resulting possible system states increases exponentially [Xu and Salado, Under Review]. So, it be-
comes computationally a very difﬁcult task to update the values of all system states. Thus, the ODP method fails to
obtain optimal strategies when the system size is large, and an alternative planning method is lacking.

As it is computationally difﬁcult to ﬁnd the optimal strategy for large systems, we proposed a UCB-based tree search
approach for large scale systems in this paper. The proposed method can generate near-optimal JVCSs that ﬁt well with
the engineering requirements within certain computational resources. The contributions of this paper are as follows:

First, we presented a search rule based on the upper conﬁdence bound for repeatable bandits (UCBRB). To simplify the
veriﬁcation planning problem, a repeatable bandit model is presented as an extension of the traditional multi-armed
bandit problem. Then the UCBRB rule is proposed with the regret theory. The UCBRB rule can be used to ﬁnd
near-optimal strategies of those decision-making problems whose strategies are repeatable.

Second, we designed a UCBRB1 tree search method to apply the UCBRB rule to veriﬁcation planning. This method
originates from traditional AO* algorithms [Nilsson, 1982] that require accurate admissible heuristic functions, which
is hard to deﬁne in veriﬁcation planning. So, we approximate the heuristic functions with the proposed UCBRB rule.
The characteristics of system veriﬁcation, including conﬁdence information, two types of activities, and a value-based
model, are also considered to design this method for the search of JVCSs.

Third, we leveraged a tree-based ensemble learning model to handle local optimality issue of the UCBRB1 tree search
method. As veriﬁcation planning is a sequential decision-making problem with dependency, the distributions of state
values are not stationary during tree search processes (i.e., concept drift). These concept drifts usually result in local
optimality of JVCSs. We trained random forest regression (RFR) with collected samples of system states during the
tree search process and used a lower conﬁdence bound of RFR outputs to predict state values as prior information.
These prediction values are used to narrow the gap of concept drifts between different system states and help jump out
of the local optimum spaces.

The rest of this paper is organized as followed. Section 2 reviews the literature about bandit-based methods, tree
search methods for sequential decision-making, and tree-based ensemble learning models. Section 3 introduces the
basic underlying models to model veriﬁcation planning problems to underpin the proposed work. Section 4 presents
the proposed UCB-based tree search approach. Section 5 uses a demonstrative case to illustrate the performance of
the approach. Our conclusions and future directions are summarized in Section 6.

2

A PREPRINT

2 Literature Review

2.1 Bandit-based Methods

The multi-armed bandit problem is a sequential decision model
in which an agent needs to decide which
arm of K different slot machines to maximize their reward while improving their information at the same
time [Bergemann and Välimäki, 2006]. This problem provides a paradigm of the tradeoff between exploration (trying
out each arm to ﬁnd the best one) and exploitation (playing the arm believed to give the best payoff). The reward of
each arm is a random variable xk with an unknown distribution and the distributions of all arms are independent from
each other. This bandit problem aims at ﬁnding a policy that determines which bandit to play based on previous trials.
The performance of a policy is commonly measured by the agent’s regret [Bubeck and Cesa-Bianchi, 2012], which is
the expected loss due to not playing the best bandit.

In a seminal paper, Lai and Robbins [Lai et al., 1985] made a regret analysis to ﬁnd an asymptotic lower bound on the
growth rate of total regret for a large class of reward distributions. The regret bound is O(ln n), where n is the overall
number of plays. Since then, various online policies have been proposed, among which the UCB1 policy developed by
Auer et al. [Auer et al., 2002] is considered the optimal [Kuleshov and Precup, 2014, Huo and Fu, 2017]. The UCB1
policy is to play a machine k that maximizes ¯xk +
, where ¯x is the average reward obtained from machine k,
and nk is the number of times machine k has been played so far. That is, this policy can balance the exploitation of the
bandit currently believed to be optimal with the exploration of other bandits that currently appear suboptimal but may
turn out to be superior in the long run [Browne et al., 2012]. This UCB1 policy achieves logarithmic regret uniformly
over time (not asymptotically) without any prior knowledge regarding the reward distributions. However, the UCB1
is limited by its assumption that the optimal machine is determined by comparing the expected average rewards of all
machines. That is, the UCB1 policy would lose its rationality if the expected average reward is not the measure of a
machine’s performance.

2 ln n
nk

q

To solve the tree search problem, Kocsis and Szepesvári [Kocsis and Szepesvári, 2006, Kocsis et al., 2006] proposed
the use of UCB1 as tree search policy, which is called the upper conﬁdence bound for trees (UCT). Its formula is

U CB = ¯xk + D1

2 ln n
nk

,

r

(1)

where D1 is constant. While the UCT follows the assumption of expected average rewards to determine upper con-
ﬁdence bounds, some variant policies have been proposed as extensions of the UCT to adapt to their domains where
the assumption is invalid. We ﬁnd two domains that are related to system veriﬁcation in this paper. First, Schadd et
al. [Schadd et al., 2008] extended bandit problems to single-player games with perfect information where the expected
maximum value rather than the expected average value is used to choose moves. They proposed a single-player MCTS
policy (SP-MCTS) that add a third term to the UCT rule:

U CB = ¯x + D2

2 ln n
nk

+

s

x2 − nk · ¯x2 + D3
nk

,

r

(2)

where D3 is a constant that artiﬁcially inﬂates the standard deviation for infrequently visited nodes. However, as the
averaged values are used as the ﬁrst item to represent the performance, the phenomenon that a good move is hidden by
previous records of bad moves may still occur [Bjornsson and Finnsson, 2009]. Second, Galichet et al [Galichet et al.,
2013] studied a risk-aware bandit problem that measures machine’s performance with the conditional value at risk
α (CVaR), which is the average of the lowest quantiles of the reward distribution with level α. As the goal is to
ﬁnd the machine with maximal CVaR, machines are selected with the best lower conﬁdence bound on their CVaRs,
which is called MARAB policy \CV aRk − D4
ln(⌈nα⌉)
⌈nkα⌉ , where D4 > 0 is a constant that controls the exploration vs
exploitation tradeoff. As α decreases, the risk-aware bandit problem boils down to a standard max-min optimization
problem. However, the calculation of the lowest quantiles results in the storage issue when the bandit problem is of
large size, because all previous trials must be recorded to determine the quantiles.

q

2.2 Tree Search Methods for Sequential Decision-making

Sequential decision-making is a procedural approach to decision-making that aims at ﬁnding a policy that maxi-
mizes the expected return for all possible initial system states, where earlier decisions inﬂuences the later available
choices [Barto et al., 1989]. We refer by sequential decision-making under uncertainty to those problems where the re-
sults of a decision follow a random distribution, such as bandit problems, path planning under uncertainty, and system
veriﬁcation. We distinguish between two types of sequential decision-making problems under uncertainty according to

3

A PREPRINT

whether all following states of a decision should be further planned. If only one following state should be planned after
a decision, ﬁnding the policy requires determining the set of optimal actions that are connected as a path. In contrast, if
it is necessary to plan all following states, the optimal policy can be represented as an AND/OR tree where all actions
of a decision have OR relationship and all results of an action have AND relationship. While the ﬁrst type is widely
studied in the ﬁelds related to reinforcement learning [Kaelbling et al., 1996], the second type has been explored in a
variety of research domains, such as system veriﬁcation [Salado and Kannan, 2018a], disease diagnosis [Mitra et al.,
2005], and troubleshooting [Khanafer et al., 2008].

The key difference between the methods of these two types lies in the calculation of rewards. That is, the reward of
a state depends on all following states in the second type of decision-making problems. Until now, two approaches
have been proposed to ﬁnd the policy of the second type. The ﬁrst approach is heuristic search methods that orig-
inate from AO* algorithms for AND/OR trees. Nilsson [Nilsson, 1968, 1982] ﬁrst described a version of AO* for
searching AND/OR trees to ﬁnd a solution in the form of a tree. Martelli and Montanari [Martelli and Montanari,
1978] generalized this algorithm for searching AND/OR graphs to ﬁnd a solution in the form of an acyclic graph.
The graph-search version is more efﬁcient than the tree-search version when the same state can be reached along
different paths because it avoids performing duplicate searches [Hansen and Zilberstein, 2001]. Hansen and Zilber-
sterin [Hansen and Zilberstein, 2001] proposed a generalization of AO*, called LAO*, to ﬁnd solution graphs with
loops. As the solution of veriﬁcation planning can be presented as directed acyclic graphs, we only focus on the AO* al-
gorithm for system veriﬁcation. All these AO* algorithms use admissible heuristic functions to ﬁnd exact solutions and
have been applied to troubleshooting [Warnquist et al., 2009] and diagnosis of autonomous system [Chanthery et al.,
2010]. However, when the admissible heuristic functions are not available, the AO* algorithm cannot be applied.

The second approach is dynamic programming methods (DP) that solve the problem by breaking it down into simpler
sub-problems in a recursive manner. DP methods can be classiﬁed into exact DP methods and approximate DP meth-
ods according to whether the type of solution policies are exact or approximate. Exact DP methods, such as backward
induction method, value iteration, and policy iteration, are used to solve exact planning problems [Sniedovich, 2010].
They have been applied to solve various ﬁelds, such as veriﬁcation planning [Kulkarni et al., 2020, 2021], circuit de-
sign [Shachter and Bhattacharjya, 2010], and fault location [Velimirovic et al., 2019]. However, when the state space
is large, exact DP methods fail in ﬁnding exact solutions due to the curse of dimensions [Powell, 2007]. Instead,
approximate DP methods can be used to approximate the decision-making process. These approximate methods can
be broken down into four classes according to the types of approximate policies: myopic policies, lookahead policies,
policy function approximations, and value function approximations [Powell, 2007]. While myopic and lookahead
policies minimize costs for one or several ﬁnite time periods, ignoring the effect of a decision now on the later time
periods, they are limited if the long-term effects are important. Policy function approximations are applied when the
structure of a policy is fairly obvious [Powell, 2016]. Because veriﬁcation strategies have VAs and CAs in this pa-
per, the structure of a policy suffers the dependency relationship between VAs and CAs. So, the fourth type of value
function approximations is mostly related to veriﬁcation planning. Many different approximation models of value
functions have been proposed, such as look-up tables of Q learning [Watkins and Dayan, 1992], support vector regres-
sion [Bethke et al., 2008], and neural network [Bertsekas and Tsitsiklis, 1995]. However, as far as we know, there are
no value function approximation methods for the second type of sequential decision-making under uncertainty.

2.3 Tree-based Ensemble Learning Models

Ensemble learning is an effective technique that has increasingly been adopted to combine multiple learning models
to improve overall prediction accuracy [Dietterich, 2000a]. These ensemble techniques have the advantage to alleviate
the small sample size problem by incorporating over multiple learning models to reduce the potential for overﬁtting
the training data [Dietterich, 2000b, Yang et al., 2010]. Decision trees are commonly used in ensemble learning
model because decision trees are sensitive to small changes on the training set [Dietterich, 2000a]. So, the scope
of ensemble learning models is narrowed down to tree-based models. These ensemble learning models have been
applied to many ﬁelds, such as bioinformatics [Yang et al., 2010], defect prediction [Matloob et al., 2021], and remote
sensing [Saini and Ghosh, 2017]. However, as far as we know, tree-based ensemble learning models have not been
used in veriﬁcation planning.

Some common types of ensemble learning include bagging, boosting, and stocking, which are realized as some basic
models, such as random forest (RF) [Ho, 1995], XGBoost [Chen and Guestrin, 2016], LightGBM [Ke et al., 2017], and
CatBoost [Dorogush et al., 2018]. Many studies have been conducted to test the performance of these models and each
of them has its own merits. For example, Bentéjac and Csörg˝o [Bentéjac et al., 2021] found that CatBoost obtained
the best results in generalization accuracy and AUC while LightGBM had the fastest training speed in the studied
datasets. Ibrahim et al. [Hancock and Khoshgoftaar, 2020] recommended CatBoost algorithm for better prediction
In another study about highly imbalanced Big Data [Jhaveri et al., 2019],
of loan approvals and staff promotion.

4

A PREPRINT

XGBoost was found to be better than CatBoost because of its shorter training time. In addition, the applications of
tree-based ensemble learning models are not limited to these basic models. For example, Jhaveri et al. [Ibrahim et al.,
2020] proposed a weighted random forest along with AdaBoost to predict the success rate of Kickstarter campaigns.
Zhang et al. [Zhang et al., 2018] combined RFs with XGBoost to establish the data-driven wind turbine fault detection
framework. Zeinulla et al. [Zeinulla et al., 2020] proposed a fuzzy random forest model to diagnose heart disease with
incomplete and dirty datasets. So, the selection of ensemble learning models depends on the characteristics of the
research task and datasets.

It is noticeable that ensemble learning has also been used in nonstationary environment, where the underlying data dis-
tribution changes over time (i.e., concept drift). Elwell and Polikar [Elwell and Polikar, 2011] proposed an ensemble
of classiﬁers-based approach for incremental learning of concept drift. The proposed algorithm collects consecutive
batches of data, trains one new classiﬁer for each batch of data it receives, and combines these classiﬁers using a
dynamically weighted majority voting. Later, Yin et al. [Yin et al., 2013, 2015] introduce a comprehensive hierarchi-
cal approach called dynamic ensemble of ensembles. It includes two stages. First, component classiﬁers and interim
ensembles are dynamically trained. Second, the ﬁnal ensemble is then learned by exponentially weighted averaging
with available experts. However, all these methods use the weighted averaging method to fuse the information of all
components. Thus, they cannot be directly applied in the veriﬁcation planning that searches for optimal strategies.

3 Veriﬁcation Planning Framework

3.1 System Veriﬁcation with Bayesian Networks

We consider that a given system can be decomposed into a set of system elements and assume that the objective
of system veriﬁcation is to verify relevant requirements for these elements. We conceive system veriﬁcation as a
set of tuples of system parameters θ1, · · · , θI about these requirements and the VAs that provide information about
such system parameters, denoting the resulting veriﬁcation evidence of a VA by µj with j = 1, · · · , J. Using the
modeling framework presented in [Salado and Kannan, 2019], we build a basic system veriﬁcation model as a BN
Ω = (Θ ∪ M, E), where Θ is a set of nodes, each of which represents one system parameter, M is a set of nodes that
represent VAs, and E is a set of directed edges that connects the different nodes. There are three types of directed
edges: Θ ⇒ Θ, Θ ⇒ M , and M ⇒ M , which capture the information dependencies between system parameters,
between system parameters and VAs, and between VAs, respectively. In the resulting BN, nodes representing VAs
will be treated as observable nodes (those whose node states can be observed directly) and nodes representing system
parameters will be treated as hidden nodes (those whose value states cannot be observed directly but are inferred from
the values of the observable nodes). For example, consider a computer system that has two parameters, processor
speed (denoted by θ1) and computer speed (denoted by θ2), and each parameter has its own VA (denoted by µ1 and
µ2, respectively). The BN can be built accordingly, as shown in Fig. 1 (a).
Because the interpretation of the information provided by VAs is subjective [Salado and Kannan, 2018b], we capture
the information about system parameters as beliefs. Without loss of generality, all nodes are assumed to be binary
(i.e., two node states, such as pass/fail or compliant/non-compliant). The nature of Bayesian analysis, and of BNs by
extension, allows for easy removal of this restriction and use of any number of discrete values and even continuous
belief distributions [Berger, 2013]. The speciﬁc beliefs of a network node are presented as a conditional probability
table (CPT) in this paper. Each CPT summarizes the dependency relationships between a node and all its parent nodes.
After all CPTs are elicited as prior distributions of a BN, the impact of a VA on the beliefs is modeled as follows: (1) A
veriﬁcation result A(µi) is collected after executing µi (i.e., an observable node µi is observed); and (2) the posterior
distributions of the network nodes are updated by the Bayesian rule.

CAs are deﬁned as those that correct errors or defects that are found during system development [Xu and Salado,
2021]. CAs impact the conﬁdence of system parameters because they affect the system conﬁguration. In our previous
study [Xu and Salado, 2021], uncertain evidence is leveraged to model the effects of CAs on the BN. Three basic
types of CAs are modeled with their uncertain evidence: rework, repair, and redesign. For example, when a repair
activity is executed to modify a faulty element with parts, processes, or materials that were initially unplanned for
that element, it is assumed that repairing the element has impact on the beliefs of the corresponding parameter in the
veriﬁed system. We can apply virtual evidence to represent the impact of repair on the beliefs. For example, a repair
activity is conducted to improve the overall computer speed. As the repair is applied to the system. This piece of
evidence is captured as virtual evidence applied on θ2 directly. The virtual evidence is shown as a virtual node ϕ1 on
θ2 in Fig. 1 (b). When the uncertain evidence of a CA is collected, the beliefs of a BN are updated in the same way as
that of VAs [Xu and Salado, 2021]. That is, when the uncertain evidence of ϕk is added to the BN, the conﬁdence of
other network nodes can be updated with the Bayesian rule.

5

A PREPRINT

(cid:537)(cid:20)(cid:3)

(cid:537)(cid:20)(cid:3)

(cid:537)(cid:21)(cid:3)

(cid:537)(cid:21)(cid:3)

(cid:537)(cid:20)(cid:3)

(cid:537)(cid:20)(cid:3)

(cid:537)(cid:21)(cid:3)

(cid:537)(cid:21)(cid:3)

(cid:307)(cid:20)

(cid:307)(cid:20)

(cid:541)(cid:20)

(cid:541)(cid:20)

(cid:541)(cid:21)

(cid:541)(cid:21)

(cid:541)(cid:20)

(cid:541)(cid:20)

(cid:541)(cid:21)

(cid:541)(cid:21)

(a) An Exemplar Network

(b) Repair on the BN

Figure 1: Illustration of the Bayesian Network Models.

3.2 Veriﬁcation Planning Problems

We consider a veriﬁcation process with time events t = 1, · · · , T . For simplicity, we assume that only one VA and
one CA is conducted at each time event and the VA is followed by the CA. This assumption can be relaxed when
conducting more than one activity in parallel, which is reserved for future work.
In this paper, the paradigm of
veriﬁcation planning is deﬁned as a sequential process of repeating VAs and CAs at T time events, as shown in Fig. 2.
The solution of veriﬁcation planning is the assignment of VAs and CAs along a veriﬁcation process. Because each VA
has multiple possible results (e.g., P ass/F ail), different combinations of activities exist along the same process and
all of these possible combinations can be presented as an activity tree. One example of an activity tree is shown in
Fig. 3. Each path from the root node to a terminal node is called a veriﬁcation path in this study. In the example in
Fig 3, there are 5 veriﬁcation paths, and all veriﬁcation paths share the same initial system state S1. System states are
generated along with the collection of activity results at each veriﬁcation path. At the end of each veriﬁcation path, the
veriﬁcation process terminates with a certain system state, which is called terminal state (denoted by ‘Stop’, as shown
in Fig. 3).

(cid:55)(cid:76)(cid:80)(cid:72)(cid:3)(cid:40)(cid:89)(cid:72)(cid:81)(cid:87)(cid:3)(cid:87)

(cid:55)(cid:76)(cid:80)(cid:72)(cid:3)(cid:40)(cid:89)(cid:72)(cid:81)(cid:87)(cid:3)(cid:87)(cid:14)(cid:20)

(cid:57)(cid:36)

(cid:38)(cid:36)

(cid:57)(cid:36)

(cid:38)(cid:36)

Figure 2: Illustration of Two Time Events of a Veriﬁcation Process

(cid:51)

(cid:57)(cid:36)

(cid:41)

(cid:38)(cid:36)

(cid:38)(cid:36)

(cid:51)

(cid:57)(cid:36)

(cid:41)

(cid:51)

(cid:57)(cid:36)

(cid:41)

(cid:38)(cid:36)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:51)

(cid:57)(cid:36)

(cid:41)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:38)(cid:36)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:38)(cid:36)

(cid:54)(cid:87)(cid:82)(cid:83)

Figure 3: One JVCS example (P: pass; F: fail)

At each time event, veriﬁcation planning consists of assigning a VA and a CA from their own activity spaces, each of
which is a set of all eligible activity actions, including the action ‘NA’ (i.e., No Activity). It is notable that there are
two constraints about the activity spaces of VAs and CAs. First, it is unnecessary to repeat any activity again if the
previous result of such an activity remains valid. That is, if an activity has been executed and its result can still be
used to update beliefs, such an activity is not included in the activity space. Second, implementing a CA can make
the existing results of a VA invalid if the result of the VA depends on the corrected parameter. The reason for this is
that once a CA changes a system parameter, all existing veriﬁcation results that depend on such a parameter lose their
credibility in deducing accurate posterior beliefs of the system. According to this constraint, it can be inferred that
each VA may be executed multiple times if some CAs inﬂuence the relevant parameters of such a VA. Therefore, the
activity spaces of VAs and CAs depend on the system state at the time the decision is made, and the speciﬁc set of
activity actions always change along the entire veriﬁcation process.

To measure the performance of a JVCS, three value factors are considered to calculate the value function. The ﬁrst
value factor is activity costs, which is a ﬁxed amount of ﬁnancial resources necessary to conduct either a VA or a CA.
It is denoted as C(µj) for µj and C(ϕk) for ϕk. For example, if a rework activity is executed to replace a faulty
element, corresponding activity costs could include the purchasing cost of new elements and labor fees to replace the
elements. The second value factor is failure costs, C(A(µj ) = F ail) , which is incurred when the result of a VA is
found to be F ail. The third value factor is system revenue, B(θi), which is obtained only when a veriﬁcation process
terminates (i.e., reaches a terminal state) and the system is deployed. B(θi) depends on the evolution in conﬁdence

6

A PREPRINT

Symbol
t
θi
µj
ϕk
A or A(·)
B(θi)
C(·)
Sm
Hi
Zw
Ψ
U (·)

Xk,s
nk
n
uk
u∗
Di

Table 1: Summary of Notations

Description
Time event, t = 1, · · · , T .
System parameter, i = 1, · · · , I.
Veriﬁcation activity, j = 1, · · · , J.
Correction activity, such as repair, rework, redesign.
Activity results. A(µj) is the result of a VA (e.g., A(µj) = P ass or F ail).
Revenue that depends on system parameter θi.
Cost, including C(µj ), C(ϕk), and C(A(µj ) = F ail).
System state, m = 1, · · · , M .
Threshold value for B(θi).
Veriﬁcation path, w = 1, · · · , W .
JVCS, Ψ = {Zw : w = 1, · · · , W } = {Sm : 1, · · · , M }.
Value function. U (Sm) is the state value of Sm. U (Zk) is the overall value of Zk.
U (Ψ|S1) is the overall value of Ψ for a given initial system state Sm and U (Ψ|Sm) =
U (Sm).
Reward of the sth play of the kth machine, k = 1, · · · , K, which has a distribution Fk.
Number of plays of the kth machine.
Total number of plays of a bandit problem, i.e., n = n1 + n2 + · · · + nk.
Reward supremum of the kth machine.
Maximum reward supremum of all machines.
Constants, including D0,D1,D2,D3,D4,D6,D7,D8,D9,D10.

U CB(Sm) Upper conﬁdence bound function of system state Sm.
Lookup table that stores state values and visit counts.

L

that the system is operating correctly as VAs are performed. We consider the system to be deployed only when the
conﬁdence level P (θi = P ass) of the target parameter θi at a system state Sm reach or surpass certain thresholds, Hi.
For simplicity, all of these value factors are summarized at each terminal state. Each veriﬁcation path could be stopped
in two situations. First, the conﬁdence levels of all target parameters reach their thresholds {Hi}. Second, the action
‘NA’ is selected when assigning a VA. Consider a JVCS Ψ that starts from a system state S1 and has w veriﬁcation
paths Z1, · · · , ZW and each veriﬁcation path has a set of system states {Sm}w. For a given veriﬁcation path Zw, the
overall value is calculated as:

U (Zw) =

B(θi)P (θi = P ass|Zw)δ(P (θi = P ass|Zw) > Hi)

i
X

−

C(µj ) −

C(A(µj ) = F ail) −

C(ϕk),

(3)

where δ(·) is an indicator function whose value is 1 if the statement is true and 0 otherwise. Because a JVCS Ψ
consists of a set of veriﬁcation paths, the performance of this JVCS U (Ψ|S1) is calculated as the expected value of a
veriﬁcation process E(U (Zw)), which is the weighted sum of the overall values of all veriﬁcation paths:

j
X

j
X

Xk

where P (Zw) is the probability of a veriﬁcation path Zw that is calculated by multiplying state transition probabilities
(i.e., the probabilities of an activity result) along a veriﬁcation path:

w
X

U (Ψ|S1) = E(U (Zw)) =

P (Zw)U (Zw),

(4)

The veriﬁcation planning problem is to solve for the optimal JVCS for a given initial system state S1:

m
Y

P (Zw) =

P (A|Sm),

A summary of the notations used in this paper is shown in Table 1.

Ψopt = arg max

Ψ

U (Ψ|S1),

(5)

(6)

4 Proposed UCB-based Tree Search Approach

4.1 Upper Conﬁdence Bound for Repeatable Bandits (UCBRB)

As each veriﬁcation path has a set of its own system states, all system states of a JVCS can be represented as an
AND/OR tree. At any system state whose following system states are not fully explored, veriﬁcation planning shares

7

A PREPRINT

the uncertain characteristic with bandit problems. That is, when an activity is selected, the expected reward of this
activity follows an unknown distribution. However, veriﬁcation planning differs from a sequential play of bandit
problems. As long as the prior knowledge about the target system is determined, implementing a given JVCS always
generates the same expected reward by enumerating all possible veriﬁcation paths, which means a JVCS is repeatable.
So, a repeatable bandit problem (RBP) is investigated in this section ﬁrst as a simpliﬁcation of veriﬁcation planning.

Consider a K-armed bandit problem where K machines are played sequentially and only one machine is played each
time. The reward of a level pull of each machine is represented by a random variable Xk,s for 1 ≤ k ≤ K, where k
is the index of a machine. Successive plays of machine k yield rewards Xk,1, Xk,2, · · · , which are independent and
identically distributed (i.i.d.) according to an unknown distribution Fk. The support of Fk is [ak, bk]. The distributions
of all machines are independent from each other; i.e., Xk,s and Xk′,s′ are independent (and usually not identically
distributed) for each 1 ≤ k < k′ ≤ K and each s, s′ ≥ 1. Assume that whenever a reward is collected from machine
k, the player can remember the tricks about reproducing such a reward through controlling the pulling factors, such
reaction time point, pull speed, and pull length. So, when the player has collected some pull results of all machines,
they can select the machine of the optimal reward and repeat some previous level pull to obtain the same reward. This
bandit problem is deﬁned as RBP in this paper. The objective of a RBP is to maximize the sum of rewards earned
through a sequence of pulls.

The main difference between traditional bandit problems and the RBP is the expected reward of each machine. In
traditional bandit problems, the reward of a level pull is a random observation of machine k. So, the expected reward
of machine k is the expectation of Fk. However, in RBPs, only the maximum reward of machine k will be considered
for further repetition. So, the expected reward of machine k can be represented by the maximum reward after nk
plays of machine k, where n1 + n2 + · · · + nk = n and n is the total plays of all machines. Other results with lower
rewards are not considered for future repetition any more. As each machine of a RBP has its own distribution with
a supremum uk ≤ bk, the maximum among the supremum rewards of all bandits is deﬁned as the overall supremum
u∗ = max

(uk). We follow the previous study [Lai et al., 1985] to deﬁne the regret of a RBP as:

k

K

u∗ · n − uk

E(nk),

(7)

That is, the regret of a RBP is the expected loss due to the fact that the player does not always repeat the optimal play.

Xk=1

For bandit problems, a policy is a strategy that chooses the next machine to play based on the sequence of past
plays and obtained rewards [Auer et al., 2002]. Previous policies, such as the UCB1, work by providing an upper
conﬁdence bound for each machine to optimize the accumulated regret. However, as these policies use the expectation
of a machine as the target, they are not appropriate choices for RBPs. Thus, we proposed a modiﬁed policy called
upper conﬁdence bound for repeatable bandits:

• Initialization: play each machine once;

• Loop: play machine k that maximizes xmax

= max(xk,1, xk,2, · · · , xk,nk ) is the
maximum reward obtained from machine k, nk is the number of times machine k has been played so far, D0
is a constant that is determined by the distributions of all machines, and n is the overall number of plays.

, where xmax

k + 4ln(n)
D0·nk

k

It is noticeable that D0 is the minimum of D0,k and each D0,k is the bound constant of the distribution of each
machine, as shown in Lemma 1. As the distribution of each machine is unknown, the constant D0,k can be estimated
according to presumptive distributions and collected samples. For example, assume the distribution of any machine
Fk is a uniform distribution [ak, bk]. The range (bk − ak) can be estimated as n+1
). So,the estimate of
D0 is min

k − xmin

n−1 (xmax

k

n−1
k −xmin
(n+1)(xmax

k

) .

k

The regret bound of this policy is summarized as the theorem below. The proof of this theorem is provided in Appendix.

Theorem 1 For all K > 1, if the UCBRB rule is run on K machines having arbitrary reward distributions with
support in [0, 1], then its expected regret after any number n of plays is at most O(ln(n)).

4.2 UCBRB1 Tree Search Method

As veriﬁcation planning can be viewed as a sequential play of RBPs, we use the proposed UCBRB rule to calculate
the UCB of the expected reward of a nonterminal system state Sm:

U CB(Sm) = xmax

k + D6

ln(n)
nk

, D6 ≥ 0

(8)

8

A PREPRINT

k

is the maximum expected reward of nk JVCSs that starts from Sm (i.e., max

where xmax
({U (Ψ|Sm))}), nk is the
visit count of Sm, and n is the overall visit count of its preceding state Sm−1. D6 is a constant that depends on the
unknown distribution of the state value, which may be obtained through sensitivity analysis in practice. If the range of
xmax
/D7, and D7 is a discount constant to normalize the range.
k
With this UCBRB rule, system states are evaluated with the balance between exploration and exploitation. In addition,
if the conﬁdence of target parameters reaches the threshold Hi or a ‘NA’ is selected as a VA, the corresponding system
state is a terminal state and the reward of this state is deterministic no matter how many more times it is visited. So,
the UCBs of these terminal states are simply their maximum reward value and the second item in Eq. 8 is 0.

is larger than 1, the ﬁrst item may be replaced by xmax

Ψ

k

As a veriﬁcation process consists of multiple time events, the UCBRB rule is insufﬁcient because it only solves the
comparison between the activities at a given system state. Therefore, we proposed a UCBRB1 tree search method to
make sequential decisions along a veriﬁcation process. The method consists of a set of loops, with each loop having
two stages. First, an AND/OR tree is generated in a forward way where each nonterminal state is expanded as a tip
node and an activity is selected to generate its following states. Then, the set of nonterminal states are updated by
removing the expanded state and adding nonterminal following states. This expansion step is repeated until the set of
nonterminal states is empty. In particular, all feasible activities are determined ﬁrst according to the activity constraints
in Section 3.2. Then each activity is evaluated with the UCB values of all its following states. The activity with the
largest value is chosen, as shown in Eq. 9:

µ∗ = arg max

µj

ϕ∗ = arg max
ϕk

(−C(µj) − C(A(µj ) = F ail) +

P (A(µj) = a)U CB(Sm+1)|a), or

(−C(ϕk) + U CB(Sm+1|ϕk)).

a
X

(9)

where Sm+1 means the next system state after conducting the activity and a represents P ass or F ail. Second, the
node information of all tree nodes, including expected reward and visit counts are updated backwardly from terminal
nodes to the root node. That is, the expected value of each node is updated according to expected values of their child
nodes in this tree:

U (Sm) = −C(µj ) − C(A(µj ) = F ail) +

P (A(µj) = a)U (Sm+1|µ∗)), or

U (Sm) = −C(ϕk) + U (Sm+1|ϕ∗)).

a
X

(10)

Then the expected rewards are compared with the previous record of this state and saved in the look-up table. If its
value is larger than that in the look-up table, the record is updated with the larger one. The visit counts of all tree nodes
are added by 1 in the look-up table.

The UCBRB1 tree search method is designed by extending the previous work [Vomlelová and Vomlel, 2003] from
three aspects to improve the efﬁciency of tree search. First, as all following states of an AND branch must be consid-
ered in an AND/OR tree, all nonterminal states of an AND branch are expanded simultaneously in the method. Second,
we update the information of all nodes only when the whole AND/OR tree is expanded completely rather than when-
ever a node is expanded. Third, we use the UCBs rather than an admissible function to represent the heuristic value of
a system state.

In addition, two adjustments are made about Eq. 8 in practice. First, while Eq. 8 is meaningless if either n or nk is 0,
the visit counts of all system states are 0 at the beginning. So, we assume that all system states have been visited once
before the tree search and, for simplicity, the expected value is set as N one. Then, Eq. 8 is equivalent to:

U CB(Sm) = xmax

k + D6

ln(n + 1)
nk + 1

,

(11)

Second, because all following system states are expanded, the number of tree nodes increases exponentially along with
the tree depth, which makes the strategy space very complex. To simplify the strategy space, we add a penalty item in
Eq. 8 to prevent the over-expansion of trees:

U CB(Sm) = xmax

k + D6

ln(n + 1)
nk + 1

− f (m|Ψ),

(12)

For simplicity, the penalty item f (m|Ψ) is assumed to be a function of system index m when it is expanded. For
example, f (m|Ψ) = D8 ∗ ⌊m/D9⌋, D8 = 1, and D9 = 50. That is, the UCB of a nonterminal state is reduced by 1
every 50 system states. Finally, the UCBRB1 tree search method is summarized in Algorithm 1.

9

Algorithm 1 UCBRB1 Tree Search Method

1: Inputs:

A PREPRINT

Expand all tip nodes of Ψ according to the UCBs of all following states calculated by Eq. 12.
Add expanded states to Ψ.
Denote all nonterminal expanded states as tip nodes.

L = {}: lookup table; Ψ: sample tree.
2: Initialize Ψ = {S1}, where S1 is the initial state.
3: while Ψ has some tip nodes do
4:
5:
6:
7: end while
8: Update state values and visit counts of all nodes in Ψ according to Eq. 10.
9: Update L with the updated state values and visit counts.
10: if the expected value U (Ψ) converges then
11:
12: else
13:
14: end if

Output Ψopt as the solution.

Go to 2.

4.3 Function Approximation of State Values

Even though the proposed UCBRB1 tree search method can solve for a near-optimal JVCS, the tree search process has
local optimality issues. That is, the activities of a strategy become ﬁxed at a system state after certain rounds of tree
search and it is impossible to explore other possible activities. To handle this issue, we use a function approximation
model to approximate state values by generalizing the values of collected system states. Here this model serves two
purposes. First, when a new nonterminal state is expanded, the selection of activities suffers from a lack of state
information because all following states have not been visited before. However, if a prior state value is added to
the calculation of UCBs, some promising action will be selected and the efﬁciency of tree search will be improved.
Second, because the tree search process collect state information incrementally, concept drifts occurs in terms of the
distribution of U (Sm). This concept drift can result in the immovability of activity selection, as will be shown in the
experiment section. So, the function approximation model is expected to provide some heuristic information to narrow
the gap of distributions between different system states. Thus, the tree search process can jump out of local optimum
spaces.

To approximate state values, it is necessary to identify all factors that contribute to the distribution of a state value.
During a veriﬁcation process, there are four types of independent information, including prior conﬁdence of system
parameters and activities, collected evidence of activities, value factors of a veriﬁcation process, and policy rules (e.g.,
the UCBRB rule). We use three types of variables as the input of the function approximation model to predict state
values, as shown in Fig. 4. First, the posterior conﬁdence values of all system parameters are used because they directly
determine whether one system can be deployed. Second, the statuses of all activities, including whether all parameters
are corrected and whether valid veriﬁcation results are collected, are denoted as a list of categorical variables. Third,
the counts of all executed VAs and CAs are calculated as two count variables to distinguish node information. These
count variables are redundant because they are the sum of all status values.

(cid:40)(cid:79)(cid:76)(cid:70)(cid:76)(cid:87)(cid:72)(cid:71)(cid:3)(cid:51)(cid:85)(cid:76)(cid:82)(cid:85)(cid:3)
(cid:38)(cid:82)(cid:81)(cid:73)(cid:76)(cid:71)(cid:72)(cid:81)(cid:70)(cid:72)

(cid:38)(cid:82)(cid:79)(cid:79)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:40)(cid:89)(cid:76)(cid:71)(cid:72)(cid:81)(cid:70)(cid:72)

(cid:57)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3)(cid:41)(cid:68)(cid:70)(cid:87)(cid:82)(cid:85)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)
(cid:57)(cid:72)(cid:85)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:54)(cid:87)(cid:85)(cid:68)(cid:87)(cid:72)(cid:74)(cid:92)

(cid:55)(cid:85)(cid:72)(cid:72)(cid:3)(cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)
(cid:51)(cid:82)(cid:79)(cid:76)(cid:70)(cid:92)(cid:3)(cid:53)(cid:88)(cid:79)(cid:72)

(cid:51)(cid:82)(cid:86)(cid:87)(cid:72)(cid:85)(cid:76)(cid:82)(cid:85)(cid:3)(cid:38)(cid:82)(cid:81)(cid:73)(cid:76)(cid:71)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)
(cid:82)(cid:73)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:3)(cid:51)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)

(cid:54)(cid:87)(cid:68)(cid:87)(cid:88)(cid:86)(cid:3)(cid:57)(cid:72)(cid:70)(cid:87)(cid:82)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)
(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)

(cid:40)(cid:91)(cid:83)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:57)(cid:68)(cid:79)(cid:88)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)
(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:3)(cid:54)(cid:87)(cid:68)(cid:87)(cid:72)(cid:86)

(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:3)(cid:57)(cid:68)(cid:85)(cid:76)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)
(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)

Figure 4: Dependency between Veriﬁcation Information and State Values

10

A PREPRINT

With the analysis above, we propose to use random forest regression (RFR) to approximate the dependency relation-
ship. The rationale of using RFR for such a function approximation is as follows. First, the input variables are a
mixture of continuous, categorical, and count ones. RFR can accept such mixed variables as inputs. Second, all input
variables are correlated because they depend on collected evidence. RFR are robust to such redundant inputs. Third,
RFR can generate prediction intervals by using the outputs of all decision trees, which can provide a robust approx-
imation of state values. This is important because if an improper activity is selected, all following activities may be
vain attempts.

While sample trees are generated continuously along the tree search process, we train RFR models periodically along
the tree search process. That is, we divide tree search processes into basic sampling periods and collect tree nodes of
sample trees as training datasets at each sampling period. Then a RFR model is built for each sampling period and the
output of the latest model are used as the predicted state values. This is explained from two aspects. First, there is a
dependency among sample trees because the UCBs are calculated based on all previous state values and visit counts.
That is, all training samples are not i.i.d. samples. When the number of sample trees increases, the relationship between
training samples becomes increasingly complex. So, models are trained at each period to simplify the dependency
between sample trees. Second, because of the concept drift of state value distributions, those built approximation
models may lose their generalization accuracy gradually. So, we only use the latest RFR model to predict state values.

We add RFR approximation to extend the UCBRB1 tree search method in two places. First, whenever a sampling
period is ﬁnished, we train a RFR model based on the lookup table and the sample trees in the sampling period.
Because of the randomness of tree search process, the dependency relationship of collected sample trees is random
and always change during the tree search process. So, we use RFR models to interpolate the datasets and each decision
tree is built with a zero mean squared error. That is, the information of all visited system states are saved accurately
in the RFR models. Second, when calculating the UCBs of state values, we use the latest RFR model to predict state
values as prior values. In particular, the k-th percentile of the outputs of all decision trees is used as the prior state
value ˆxk. Each prior state value is compared with the state value record in the lookup table. Thus, the UCB formula
in Eq. 12 is extended as:

U CB(Sm) = max(ˆxk, xmax

k

) + D6

ln(n + 1)
nk + 1

− f (m|Ψ),

(13)

where the ﬁrst item is the maximum between the prior value and the maximum record value.

We also make another two adjustments about data processing to improve the quality of training datasets. First, when
each sample tree is generated, all terminal nodes of a sample tree are not included as training samples. The reason is
that if ‘NA’ is selected as a VA, this type of terminal nodes suffer larger variance of state values than nonterminal nodes
because their following states have not been visited. If parameter conﬁdence reaches the threshold, it is unnecessary
to predict state values. Second, besides the tree nodes in the latest sampling period, another set of system states is
uniformly sampled from the lookup table with replacement and added to the training datasets. This adjustment is made
to reduce the bias caused by the dependency of sample trees. For simplicity, the number of sampled system states is
the same as that of sample tree nodes. The extended algorithm is called UCBRB2 tree search method in this paper.

5 Experimental Design

5.1 Problem Description

In this section, we implement the proposed framework to design a JVCS for an optical instrument in a satel-
lite [Salado and Kannan, 2019]. The notional instrument has been used to support prior research in veriﬁca-
tion [Salado and Kannan, 2019]. The system parameters of this optical instrument and their possible VAs are modeled
as the BN shown in Fig. 5. System parameters are represented as circle nodes and candidate VAs square nodes. The
deﬁnitions of the nodes are given in [Salado and Kannan, 2019], hence not presented here. Each node is characterized
with its own conditional probability tables (CPT). Their speciﬁc values are synthetic and have been generated using
the generalized Noisy-OR and Noisy-AND model [Pearl, 2014], which takes into account the physical meaning of
the different modes when estimating their mutual effects for reasonability of the data. While this instrument is ver-
iﬁed through a set of these VAs, each parameter θi has CAs to correct potential errors and defects. Without loss of
generality, this experiment takes only repair activities ϕk as an example of CAs.
In this experiment, we assume that system revenue is driven by system parameter θ1. Hence, θ1 is set as the single target
parameter. The threshold for the system deployment rule, H1, is set as 0.90. Cost data have also been synthetically
generated in thousand dollar units ($1, 000). The revenue B(θ1) has been set to 20, 000 so that it provides a balance
when making a selection tradeoff between different VAs. In the BN, there is a dependency between θ1 and all other
network nodes. That is, once an activity is executed on any node of these 32 nodes and an activity result is collected,

11

Table 2: Cost Table (Unit: $1,000)

Activity
Activity Cost
Failure Cost
Activity
Activity Cost
Failure Cost
Activity
Activity Cost
Failure Cost

ϕ1(θ1) ϕ2(θ2) ϕ3(θ3) ϕ4(θ4) ϕ5(θ5) ϕ6(θ6) ϕ7(θ7) ϕ8(θ8) ϕ9(θ9) ϕ10(θ10)
8500
-
µ13
3300
0
µ25
3500
12000

6000
-
µ20
3300
12000
µ32
400
0

5200
-
µ14
3400
0
µ26
2300
5000

1000
-
µ17
300
0
µ29
2300
0

4200
-
µ19
100
0
µ31
300
0

8500
-
µ18
500
0
µ30
400
0

2000
-
µ15
300
0
µ27
2400
0

2300
-
µ16
300
0
µ28
3300
0

2300
-
µ21
3400
8000

2000
-
µ22
3400
2000

A PREPRINT

µ11
3300
15000
µ23
300
0

µ12
500
8000
µ24
2400
10000

the conﬁdence P (θ1 = P ass) will change. The activity costs of the different activities, as well as the failure costs
of VAs, are provided in Table 2. Speciﬁc values have been generated according to the type of activities deﬁned
in [Salado and Kannan, 2019].

(cid:541)(cid:21)(cid:19)

(cid:541)(cid:21)(cid:20)

(cid:541)(cid:20)(cid:20)

(cid:541)(cid:21)(cid:23)

(cid:541)(cid:21)(cid:24)

(cid:541)(cid:21)(cid:26)

(cid:537)(cid:25)

(cid:541)(cid:20)(cid:21)

(cid:541)(cid:21)(cid:25)

(cid:537)(cid:26)

(cid:541)(cid:21)(cid:21)

(cid:541)(cid:21)(cid:22)

(cid:541)(cid:20)(cid:28)

(cid:537)(cid:20)

(cid:537)(cid:24)

(cid:537)(cid:21)

(cid:537)(cid:23)

(cid:541)(cid:21)(cid:27)

(cid:541)(cid:20)(cid:22)

(cid:541)(cid:20)(cid:23)

(cid:541)(cid:20)(cid:26)

(cid:541)(cid:20)(cid:27)

(cid:537)(cid:20)(cid:19)

(cid:541)(cid:22)(cid:21)

(cid:537)(cid:27)

(cid:537)(cid:28)

(cid:541)(cid:21)(cid:28)

(cid:537)(cid:22)

(cid:541)(cid:22)(cid:19)

(cid:541)(cid:22)(cid:20)

(cid:541)(cid:20)(cid:24)

(cid:541)(cid:20)(cid:25)

Figure 5: BN of the Optical Instrument (The smaller network outlined by the dash line is used in Scenario 1 and the
whole network is used in Scenario 2.)

5.2 Experimental Method

With the provided problem and generated data, the whole experiment is realized with Python 3.6 and Bayes Net
Toolbox for Matlab [Murphy et al., 2001]. Two scenarios are conducted to study the performance of the proposed
approach. In Scenario 1, the target network is the smaller one outlined by the dash line in Fig. 5. With 5 system
parameters and 9 VAs in this BN, there are 2 · 25 · 29 = 1259712 total system states. The costs items of all activities
are shown in bold in Table 2. To get some intuition about this scenario, we apply the order-based backward induction
method ﬁrst to solve for the exact JVCS. When the backward induction is conducted to calculate the expected values
of all states, the optimal activities of all system states are identiﬁed to constitute the exact JVCS, as shown in Fig. 6.
With 24 veriﬁcation paths (i.e., the number of terminal states ‘Stop’), the exact JVCS has 93 tree nodes (excluding
terminal states ‘Stop’). The depth of this JVCS is 13 nodes (i.e., 7 time events). The expected value of this JVCS is
7, 788 while the total running time is 97, 925 sec.

In Scenario 2, the target network is the whole network in Fig. 5. With 10 system parameters and 22 VAs in this BN,
there are 2 · 210 · 222 = 6.43 ∗ 1013 total system states. As the number of total system states is too large, the order-based
backward induction method is infeasible for this whole network and the exact solution is unknown. However, because
the small network is a subset of the whole one, the exact JVCS in Scenario 1 can be used as a reference. So, there is a
question whether a better JVCS can be found in the large network, which is discussed later.

In each scenario, the proposed approach is compared with two types of benchmark methods. First, we compare
the UCBRB rule with other UCB rules, including the UCT rule (i.e., Eq. 1) and the SP-MCTS rule (i.e., Eq. 2).
As veriﬁcation planning is such a special problem that we cannot ﬁnd an accurate admissible heuristic function of all
states, the AO* algorithm cannot be applied to solve for AND/OR trees. Algorithm 1 in Section 4.2 is the only feasible

12

(cid:51)(cid:18)(cid:19)(cid:17)(cid:25)(cid:27)(cid:27)

(cid:541)(cid:20)(cid:25)

(cid:41)(cid:18)(cid:19)(cid:17)(cid:24)(cid:27)(cid:23)

(cid:49)(cid:36)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:26)(cid:23)(cid:22)

(cid:541)(cid:20)(cid:28)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:24)(cid:26)(cid:19)

(cid:19)(cid:17)(cid:27)(cid:19)(cid:27)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:27)(cid:22)(cid:25)

(cid:307)(cid:20)

(cid:541)(cid:20)(cid:28)

(cid:41)(cid:18)(cid:19)(cid:17)(cid:26)(cid:22)(cid:26)

(cid:49)(cid:36)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:26)(cid:27)(cid:27)

(cid:541)(cid:20)(cid:26)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:25)(cid:25)(cid:19)

(cid:19)(cid:17)(cid:26)(cid:23)(cid:22)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:26)(cid:24)(cid:20)

(cid:307)(cid:24)

(cid:541)(cid:20)(cid:28)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:26)(cid:21)(cid:19)

(cid:49)(cid:36)

(cid:19)(cid:17)(cid:27)(cid:24)(cid:22)

(cid:307)(cid:20)

(cid:49)(cid:36)

(cid:19)(cid:17)(cid:27)(cid:27)(cid:24)

(cid:307)(cid:20)

(cid:49)(cid:36)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:27)(cid:28)(cid:26)

(cid:541)(cid:20)(cid:24)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:26)(cid:27)(cid:28)

(cid:49)(cid:36)

(cid:19)(cid:17)(cid:27)(cid:28)(cid:26)

(cid:307)(cid:22)

(cid:19)(cid:17)(cid:27)(cid:22)(cid:25)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:27)(cid:28)(cid:26)

(cid:49)(cid:36)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:20)(cid:27)

(cid:541)(cid:20)(cid:21)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:25)(cid:24)(cid:19)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:20)(cid:27)

(cid:541)(cid:20)(cid:27)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:19)(cid:21)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:19)

(cid:541)(cid:20)(cid:21)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:25)(cid:19)(cid:20)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:25)

(cid:541)(cid:20)(cid:26)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:23)(cid:21)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:20)(cid:27)

(cid:541)(cid:20)(cid:26)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:24)(cid:23)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:22)

(cid:541)(cid:20)(cid:25)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:27)(cid:22)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:20)

(cid:541)(cid:20)(cid:28)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:27)(cid:25)

(cid:49)(cid:36)

(cid:307)(cid:20)

(cid:49)(cid:36)

(cid:307)(cid:23)

(cid:49)(cid:36)

(cid:307)(cid:20)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:307)(cid:24)

(cid:541)(cid:20)(cid:24)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:26)(cid:27)(cid:28)

(cid:19)(cid:17)(cid:27)(cid:28)(cid:26)

(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:19)

(cid:49)(cid:36)

(cid:307)(cid:22)

(cid:541)(cid:20)(cid:28)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:27)(cid:24)

(cid:49)(cid:36)

A PREPRINT

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:19)(cid:17)(cid:28)(cid:20)(cid:27)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:19)(cid:17)(cid:28)(cid:20)(cid:27)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:19)(cid:17)(cid:28)(cid:19)(cid:19)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:54)(cid:87)(cid:82)(cid:83)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:26)
(cid:541)(cid:20)(cid:27)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:26)(cid:28)(cid:19)

(cid:54)(cid:87)(cid:82)(cid:83)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:20)(cid:28)

(cid:541)(cid:20)(cid:27)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:19)(cid:22)
(cid:54)(cid:87)(cid:82)(cid:83)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:23)
(cid:541)(cid:20)(cid:26)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:23)(cid:19)

(cid:54)(cid:87)(cid:82)(cid:83)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:26)
(cid:541)(cid:20)(cid:26)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:23)(cid:21)
(cid:54)(cid:87)(cid:82)(cid:83)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:25)

(cid:541)(cid:20)(cid:26)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:27)(cid:23)(cid:21)

(cid:49)(cid:36)

(cid:307)(cid:23)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:19)(cid:17)(cid:28)(cid:19)(cid:25)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:49)(cid:36)

(cid:307)(cid:23)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:49)(cid:36)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:19)(cid:17)(cid:28)(cid:20)(cid:27)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:54)(cid:87)(cid:82)(cid:83)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:24)

(cid:541)(cid:20)(cid:27)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:26)(cid:27)(cid:27)

(cid:54)(cid:87)(cid:82)(cid:83)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:26)

(cid:541)(cid:20)(cid:27)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:26)(cid:28)(cid:20)

(cid:54)(cid:87)(cid:82)(cid:83)
(cid:51)(cid:18)(cid:19)(cid:17)(cid:28)(cid:19)(cid:26)

(cid:541)(cid:20)(cid:27)
(cid:41)(cid:18)(cid:19)(cid:17)(cid:26)(cid:28)(cid:19)

(cid:49)(cid:36)

(cid:307)(cid:23)

(cid:49)(cid:36)

(cid:307)(cid:23)

(cid:49)(cid:36)

(cid:307)(cid:23)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:19)(cid:17)(cid:28)(cid:19)(cid:23)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:19)(cid:17)(cid:28)(cid:19)(cid:26)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:54)(cid:87)(cid:82)(cid:83)

(cid:19)(cid:17)(cid:28)(cid:19)(cid:25)

(cid:54)(cid:87)(cid:82)(cid:83)

Figure 6: The exact JVCS solution of Scenario 1

Table 3: Expected value and runtime of all possible D6 constants

D6
Expected Value
Runtime

0.1
7608.14
979.50

0.25
7777.40
4510.50

0.5
7780.78
7452.68

1.0
7767.16
15095.41

1.5
7670.18
19899.54

2
7088.53
22555.04

method to ﬁnd a JVCS as far as we known. So, we combine Algorithm 1 with all these UCB rules to compare their
effects. For each rule, their constant values are selected through their own sensitivity analyses. We set D7 = 20000,
D8 = 1, and D9 = 50. The tree search process is conducted by generating 5000 sample trees. We record the optimal
state value every 50 sample trees at the initial state to compare the performance of difference UCB rules.

Second, we compare the proposed methods with a Monte Carlo method. The UCBRB2 tree search method is also
tested as an extension of the UCBRB1 one. A RFR model is trained every 3000 tree nodes and another 3000 system
states are sampled from the lookup table. Each RFR model consists of 100 decision trees and the 5th percentile of the
100 outputs are used as predicted state values. The hyperparameter ‘Bootstrap’ is set as F alse and the hyperparameter
‘minimum number of samples required to be at a leaf node’ is set as 1 to interpolate state samples while other hyperpa-
rameters are set as their default values. Finally, a Monte Carlo method is designed to search for a JVCS in a random
fashion. As the number of tree nodes can be large if all activities are chosen randomly. A constraint about the total
number of tree nodes is added in this Monte Carlo method. That is, the total node number is less than D10 = 50.

5.3 Experimental Results

5.3.1 Scenario 1

In the UCBRB rule (given by Eq. 8), the constant D6 depends on the unknown distributions of the speciﬁc problem.
So, it is necessary to determine the constant D6 ﬁrst for the proposed approach. For simplicity, a set of six possible D6
constants [0.1, 0.25, 0.5, 1, 1.5, 2] is used to ﬁnd the optimal one. For each constant value, the tree search algorithm in
Algorithm 1 is conducted to test the performance and their trends are shown in Fig. 7. It is found that when D6 is 0.5,
the expected value reaches the maximum 7780.78 among the 5000 sample trees. The expected values and runtimes of
all possible constants are listed in Table 3.

13

A PREPRINT

8000

6000

l

e
u
a
V
d
e
t
c
e
p
x
E

4000

2000

0

D6 = 2
D6 = 1.5
D6 = 1
D6 = 0.5
D6 = 0.25
D6 = 0.1

0

1000

2000

3000

4000

5000

Tree Number

Figure 7: Constant selection for the UCBRB rule

From Fig. 7 and Table 3, it can be observed that when D6 decreases, the expected value converges faster. This is
because more weights are allocated to the exploitation of existing strategies (i.e., the ﬁrst item in Eq. 8) when D6
decreases. The runtime also decreases become similar trees are exploited. However, if D6 is too small, the tree search
process may get stuck in local optimal spaces. For example, when D6 = 0.1, the expected value becomes stable after
the 1950th sample tree. Thus, D6 should be large enough given ﬁxed computation resources. In this case, as the limit
of tree number is set as 5000, we set D6 as 0.50 for the UCBRB rule in this experiment.
Next, three kinds of UCB rules are compared to solve this small network problem. With a similar approach for the
D6 value of the UCBRB, the optimal constants of UCT and SP-MCTS is found to be D1 = 0.50, D2 = 0.50, and
D3 = 10000. The performance of all UCB rules are shown in Fig. 8 (a). It is found that even though both UCT and
SP-MCTS rules can ﬁnd their near-optimal strategies, the UCBRB rule outperforms them slightly after the 4000th
sample tree. This is explained by the usage of maximum value as the ﬁrst item in Eq. 8, which make the UCBRB more
sensitive to the optimal activities. It is also found that SPUCB converges faster than UCT. We attribute it to the third
item in Eq. 2 as it adds more weights to those nodes that are visited less frequently. Because there is no signiﬁcant
difference between the optimal expected values of all UCB rules, all UCB rules can be considered for the activity
selection in this scenario.

Finally, we also conduct the Monte Carlo method and the UCBRB2 tree search method to search this small strategy
space. The Monte Carlo method is conducted to generate 5000 sample trees. Then the UCBRB2 tree search method
shares the same D6 value with the UCBRB1 tree search method. Their performances are shown in Fig. 8 (b) and
summarized in Table 4. It is found that the Monte Carlo method cannot compete with the proposed two methods in
terms of the expected values of JVCS, even though it costs the least runtime among all methods. The UCBRB1 tree
search method can ﬁnd the optimal strategy with the highest expected value and its runtime is close to that of the
Monte Carlo method.

Even though the UCBRB2 tree search method costs the more runtime and its expected value is not the highest, it can
solve the local optimality issue that exists in the UCBRB1 tree search method. For simplicity, we use the distribution
of all activities at the initial state S1 to study the local optimality issue. If the UCBRB1 tree search method is used, the
ﬁrst activity is always ﬁxed as µ19 after the 500th sample tree, as shown in Fig. 9 (a) (i.e., activity immovability). This
activity immovability problem is attributed to the concept drift that enlarges the value gap of the ﬁrst item in Eq. 8.
So, the UCB of other activities cannot surpass that of µ19, as shown in Fig. 10 (a). The UCBRB2 tree search method
solved this local optimality issue by narrowing the value gap with the prior information, as shown in Fig 10 (b). So,
more sample trees are allocated to other activities and it is possible to jump out of the local optimum strategy space
(i.e., Fig. 9 (b)).

5.3.2 Scenario 2

In Scenario 2, there are 10 CAs and 22 VAs. As only some extra network nodes are added to the BN, all methods in
Scenario 1 can be applied directly. For simplicity, all constants are assigned with the same values as those in Scenario
1. We compare all UCB rules ﬁrst in this scenario and show their results in Fig. 8 (c) and Table 4. It is found that the
JVCS generated by the UCBRB rule has the maximum expected value 8478.43, while the UCT and SP-MCTS only
found their strategies with 7011.08 and 6114.59. In particular, the expected values of the UCT rule do not surpass
6000 until the 3350th sample tree. The reason provided by our analysis is that this large network has more large-cost

14

 
A PREPRINT

UCBRB
UCBRB+RFR
Monte Carlo

l

e
u
a
V
d
e
t
c
e
p
x
E

8000

7000

6000

5000

4000

3000

2000

1000

0

UCBRB
UCT
SP-MCTS

0

1000

2000

3000

4000

5000

0

1000

2000

3000

4000

5000

Tree Number

(a) Scenario 1 - UCB rules

Tree Number

(b) Scenario 1 - Benchmark methods

UCBRB
UCT
SP-MCTS

l

e
u
a
V
d
e
t
c
e
p

E

7500

5000

2500

0

−2500

−5000

−7500

UCBRB
UCBRB+RFR
Monte Carlo

0

1000

2000

3000

4000

5000

0

1000

2000

3000

4000

5000

Tree Number

(c) Scenario 2 - UCB rules

Tree Number

(d) Scenario 2 - Benchmark methods

Figure 8: Plots of All UCB Rules and Benchmark Methods

8000

7000

6000

5000

4000

3000

l

e
u
a
V
d
e
t
c
e
p
x
E

l

e
u
a
V
d
e
t
c
e
p
x
E

8000

7000

6000

5000

4000

3000

2000

Table 4: Performance comparison of all UCB rules and benchmark methods (including UCT, SP-MCTS, Monte Carlo,
and ODP)

Scenario 1

Scenario 2

Expected Value
7780.78
7746.82
7526.81
7780.52
801.45
7788

Runtime
7452.68
8915.13
10183.85
12120.16
6797.46
97925

Expected Value
8478.43
7011.08
6114.59
8487.27
0
-

Runtime
102109.83
281919.13
227776.50
186804.14
73234.74
-

UCBRB
UCT
SP-MCTS
UCBRB+RFR
Monte Carlo
Exact Solution

15

 
 
 
 
 
A PREPRINT

activities than the small network. So, the mean value is a biased estimate of a state value that makes the estimated
UCB less accurate. However, the mean value still has some effect in the long term as it can be found that the expected
value gradually reaches 7000. The SP-MCTS rule can ﬁnd a JVCS with the value 6114.59 at the 300th sample tree.
However, once this rule ﬁnds the JVCS with the 6114.59, it fails to provide a better JVCS as the value gradually
decreases after the 300th sample tree. This is explained by the third item in Eq. 2 as it is not sensitive to the maximum
value when the number of samples is large. Therefore, the UCBRB rule can ﬁnd a better strategy than other ones in
Scenario 2 because it uses the maximum function as a more accurate estimate of state value.

The performances of all three benchmark methods are shown in Fig. 8 (d) and also summarized in Table 4. It is found
that the proposed two methods have found near-optimal JVCSs while the Monte Carlo method fails in providing a
JVCS with a positive expected value. The reason is attributed to the large network that has more high-cost activities.
So, it is harder to search for a strategy with positive expected values randomly. Instead, estimating the expected value
with UCBs can avoid repeating the over-exploration of high-cost activities and yield better strategies with limited
tree samples. However, these two proposed methods also cost much more time than the Monte Carlo method. It
is attributed to the UCB calculation and the network size. The UCBRB2 tree search method converges faster than
the UCBRB1 one in the ﬁrst 2000 sample trees, even though there is no signiﬁcant difference between their optimal
expected values.

The activity immovability problem still occurs when the UCBRB1 tree search method is used. The optimal activity at
the initial state is ﬁxed as µ19, as shown in Fig. 9 (c). It is caused by the same reason as in Scenario 1 that the value
gap of the ﬁrst item in Eq. 8 is too large. So, the UCBs of other activities can hardly surpass that of µ19. However,
the UCBRB2 tree search method can improve the UCBs signiﬁcantly so that other activities are allocated with more
exploration times. Thus, the tree search process can jump out of local optimum spaces and ﬁnd a better JVCS. This
explains why the JVCS of the UCBRB2 method has the largest expected value 8487.27.

5.4 Discussion

The two scenarios in this experiment are designed by expanding the activity set of the same system network. With the
proposed UCBRB1 and UCBRB2 tree search methods, a better JVCS is found in Scenario 2, which is attributed to
those extra 5 CAs and 13 VAs (i.e., ϕ6 to ϕ10, and µ20 to µ32) in Scenario 2. However, this phenomenon does not
always occur. We have veriﬁed the expected value of the exact JVCS (ref. Fig. 6) in the context of the large network
and found its expected value is 76 because the conﬁdence of the target node drops below 0.9 at most terminal states
of veriﬁcation paths in the large network. Thus, if the costs of all extra activities are very large, it is highly likely that
the expected values of near-optimal JVCSs in Scenario 2 are lower than 7,788. Due to some low-cost activities, such
as ϕ10 and µ23, there are still some opportunities to improve the strategy performance in Scenario 2. Therefore, while
ﬁnding JVCSs is the task of this research, the effects of all activity costs on JVCSs are not fully explored, which is left
as a future work.

During the tree search process, we also found that activity immovability always happens in this veriﬁcation planning
problem, no matter which UCB rule is used. This problem cannot be solved by adjusting the constant D6, neither. It
is attributed to the concept drift of value distributions. In addition, we assume there are two causes of concept drift.
The ﬁrst one is the lacking of the completeness in tree search processes. If it requires a large number of ‘Pass’ activity
results to reach the deployment threshold of the target parameter, the tree of a near-optimal JVCS has a large depth
and it cannot be found easily. Instead, only some partial information is collected, which results in concept drift. The
second cause is the second item of UCB rules. When the parent visit count is 1 (i.e., n = 0 in Eq. 8), the ﬁrst item
is N one and the second item is 0. Then, the optimal activity is ‘NA’ because it has the minimum activity cost 0. So,
the optimal activity of an unvisited state is always ‘NA’, which makes the expected values of its parent tree nodes
inaccurate. However, it is noticeable that activity immovability also has a positive effect on the search of JVCS. After
the tree search processes ﬁnd a simple JVCS with a positive expected value, this set of activities is ﬁxed as a core set
of activities and its value can be improved by exploitation. Thus, it is unnecessary to eliminate this effect. Instead, it
is suggested to improve the UCBs of other promising activities.

While the ensemble learning model is applied to extend the UCBRB1 tree search method, the optimal expected value
is not improved signiﬁcantly. It is mainly attributed to CAs in our view. Because CAs can eliminate errors and defects
and reset the statuses of VAs, they can improve the conﬁdence of system parameters fundamentally and reduce the
impact of negative activity results. So, if a JVCS can eliminate system errors, the selection of activities at early time
events does not matter too much on expected values in this experiment. From the aspect of practice, the UCBRB1
tree search method should be considered ﬁrst as it can provide a feasible JVCS with less runtime and storage space.
However, if it is important to explore other candidate activities, the UCBRB2 method is a better choice because it can
handle local optimality issues and solve the exploration-exploitation dilemma much better than the UCBRB1 one.

16

A PREPRINT

y
c
n
e
u
q
e
r
F

y
c
n
e
u
q
e
r
F

50

40

30

20

10

0

50

40

30

20

10

0

50

40

30

y
c
n
e
u
q
e
r
F

20

10

0

50

40

30

y
c
n
e
u
q
e
r
F

20

10

0

01234567891

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

2

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

5

5

5

5

5

5

5

5

5

5

6

6

6

6

6

6

6

6

6

6

7

7

7

7

7

7

7

7

7

7

8

8

8

8

8

8

8

8

8

8

9

9

9

9

9

9

9

9

9

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

(a) Scenario 1 - UCBRB1 tree search method

Loop Number

01234567891

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

2

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

5

5

5

5

5

5

5

5

5

5

6

6

6

6

6

6

6

6

6

6

7

7

7

7

7

7

7

7

7

7

8

8

8

8

8

8

8

8

8

8

9

9

9

9

9

9

9

9

9

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

(b) Scenario 1 - UCBRB2 tree search method

Loop Number

01234567891

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

2

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

5

5

5

5

5

5

5

5

5

5

6

6

6

6

6

6

6

6

6

6

7

7

7

7

7

7

7

7

7

7

8

8

8

8

8

8

8

8

8

8

9

9

9

9

9

9

9

9

9

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

Loop Number

(c) Scenario 2 - UCBRB1 tree search method

01234567891

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

2

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

5

5

5

5

5

5

5

5

5

5

6

6

6

6

6

6

6

6

6

6

7

7

7

7

7

7

7

7

7

7

8

8

8

8

8

8

8

8

8

8

9

9

9

9

9

9

9

9

9

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

Loop Number

(d) Scenario 2 - UCBRB2 tree search methods

Figure 9: Distribution of All Activities at the Initial State S1

17

μ19
μ18
μ17
μ16
μ15
μ14
μ13
μ12
μ11

NA

μ19
μ18
μ17
μ16
μ15
μ14
μ13
μ12
μ11

NA

μ32
μ31
μ30
μ29
μ28
μ27
μ26
μ25
μ24
μ23
μ22
μ21
μ20
μ19
μ18
μ17
μ16
μ15
μ14
μ13
μ12
μ11

NA

μ32
μ31
μ30
μ29
μ28
μ27
μ26
μ25
μ24
μ23
μ22
μ21
μ20
μ19
μ18
μ17
μ16
μ15
μ14
μ13
μ12
μ11

NA

A PREPRINT

μ19
μ18
μ17
μ16
μ15
μ14
μ13
μ12
μ11

NA

μ19
μ18
μ17
μ16
μ15
μ14
μ13
μ12
μ11

NA

μ32
μ31
μ30
μ29
μ28
μ27
μ26
μ25
μ24
μ23
μ22
μ21
μ20
μ19
μ18
μ17
μ16
μ15
μ14
μ13
μ12
μ11

NA

μ32
μ31
μ30
μ29
μ28
μ27
μ26
μ25
μ24
μ23
μ22
μ21
μ20
μ19
μ18
μ17
μ16
μ15
μ14
μ13
μ12
μ11

NA

0.4

0.3

l

e
u
a
V
B
C
U

0.2

0.1

0.0

0.4

0.3

0.2

l

e
u
a
V
B
C
U

0.1

0.0

l

e
u
a
V
B
C
U

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0.6

0.5

0.4

l

e
u
a
V
B
C
U

0.3

0.2

0.1

0.0

0

1000

2000

3000

4000

5000

Sample Tree

(a) Scenario 1 - UCBRB1 tree search method

0

1000

2000

3000

4000

5000

Sample Tree

(b) Scenario 1 - UCBRB2 tree search method

0

1000

2000

3000

4000

5000

Sample Tree

(c) Scenario 2 - UCBRB1 tree search method

0

1000

2000

3000

4000

5000

Sample Tree

(d) Scenario 2 - UCBRB2 tree search methods

Figure 10: UCBs of All Activities at the Initial State S1

18

 
 
 
 
A PREPRINT

6 Conclusion

This paper has presented a UCB-based tree search approach to solve the veriﬁcation planning problem. First, we
simplify the veriﬁcation planning problem as a repeatable bandit problem and propose a UCBRB rule. The upper
regret bound of this UCBRB rule is also found and proved. Then we propose a UCBRB1 tree search method to apply
the UCBRB rule to search for JVCSs. A tree-based ensemble learning model is also used to extend the UCBRB1 tree
search method by using RFR models to predict state values. It is found that the proposed UCBRB rule can outperform
other UCB rules in the experiment. This advantage is more obvious when the size of the system network increases.
The UCBRB2 tree search method can also effectively solve the local optimality issue and handle the exploration-
exploitation dilemma better than the UCBRB1 one.

We would like to remark there are three limitations in the proposed methodology. First, the constants of all UCB rules
are selected from several possible values in the experiment. The selected value is used as a deterministic value at all
system states in the two scenarios. Second, some other parameters and constants of tree search algorithms are not
optimized, including discount constant, penalty item, total sample tree number. Third, when training RFR models to
predict state values, we set most hyper-parameters with their default values and make some ad hoc adjustments, such
as removing terminal states as samples and adding the same number of system states from lookup tables, which are
not fully optimized to improve the tree search performance.

In addition, we suggest that this work opens three main future research directions. First, the horizon of veriﬁcation
processes is not fully explored in this study as a penalty item is simply added to restrict the horizon. More mechanisms
may be developed to explore the effect of horizon on JVCSs. Second, this work focuses on the tree search given a
ﬁxed system network. The selection of possible sub-networks could be explored to simplify tree search processes. The
sub-networks may be generated by evaluating the impacts of activities on JVCSs. Third, only RFR models are studied
in this work. They are also trained with some intuitive features to approximate system values. Other machine learning
methods need to be explored as benchmarks in the future.

Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant No. CMMI-1762883.

Appendix

Lemma 1 [Galichet et al., 2013, Lemma 4.1] Let v be a bounded distribution with support in [0, 1], with u its supre-
mum and let us assume that v is lower bounded in the neighborhood of a:

∃D > 0, ∀ε > 0, P (X ≥ a − ε) > Dε, with X ∼ v.

Let x1, . . . xt be t-sample independently drawn from X. The maximum value over xi, i = 1, · · · , t goes exponentially
fast to u:

P ( max
1≤i≤t

(xi) ≤ u − ε) > exp(−tDε).

Proof of Theorem 1:

Proof 1 For each machine k, the upper bound of nk is determined ﬁrst as Tk(n). Let h be some positive integer,
It = k is an indicator function, and X max
is

be the maximum value over xk,i, i = 1, · · · , t. The supremum of X max

k

k

19

A PREPRINT

uk and u∗ = max

(uk).

k

n

Tk(n) = 1 +

{It = k}

Xt=K+1
n

≤ h +

{It = k, Tk(t − 1) ≥ h}

Xt=K+1
n

≤ h +

{X max

∗ +

Xt=K+1
n

≤ h +

≤ h +

Xt=K+1
t−1
∞

{ min
0<s<t

X max

∗,s +

t−1

{X max

∗,s +

t=1
X

s=1
X

Xsk=h

4ln(t − 1)
D · T∗(t − 1)

≤ X max

k +

4ln(t − 1)
D · Tk(t − 1)

, Tk(t − 1) ≥ h},

4ln(t − 1)
D · s

≤ max
h<sk<t

X max

k,sk +

4ln(t − 1)
D · sk

}

4ln(t)
D · s

≤ X max

k,sk +

4ln(t)
D · sk

}

The inequation X max

∗,s + 4ln(t)

D·s ≤ X max

k,sk + 4ln(t)

D·sk

implies at least one of the following inequations must hold:

X max

∗,s ≤ u∗ −

X max

k,sk ≥ uk +

4ln(t)
D · s

,

4ln(t)
D · sk

,

4ln(t)
D · sk
is always less than uk, Inequation 15 cannot be true.

u∗ < uk + 2

,

Because X max
k,sk

When sk ≥ 8ln(t)

D·(u∗−uk) , Inequation 16 is false because

u∗ − uk − 2

4ln(t)
D · sk

≥ u∗ − uk − (u∗ − uk) = 0.

With Lemma 1, we have

P (X max

∗,s ≤ u∗ −

4ln(t)
D · s

) ≤ exp(−sD

4ln(t)
Ds

) = exp(−4ln(t)) = t−4

(14)

(15)

(16)

So, we have

Thus,

E(Tk(n)) ≤

≤

≤

∞

t−1

t−1

t=1
X
∞

s=1
X
t

sk=⌈8ln(t)/D(u∗−uk)⌉
X

t

P (X max

∗,s ≤ u∗ −

4ln(t)
D · s

)

8ln(t)
D(u∗ − uk)

8ln(t)
D(u∗ − uk)

+

+

(cid:25)

(cid:25)

(cid:24)

(cid:24)

8ln(t)
D(u∗ − uk)

+ 1 +

t=1
X
π2
6

t−4

sk=1
X

s=1
X

K

K

K

u∗ · n − uk

E(nk) =

(u∗ − uk)E(nk) ≤

((u∗ − uk)(

Xk=1

Xk=1

Xk=1

20

8ln(t)
D(u∗ − uk)

+ 1 +

π2
6

)) = O(ln(t))

A PREPRINT

References

Avner Engel. Veriﬁcation, Validation, and Testing of Engineered Systems. John Wiley & Sons, 2010.

Alejandro Salado and Hanumanthrao Kannan. A mathematical model of veriﬁcation strategies. Systems Engineering,

21(6):593–608, 2018a.

International Organization for Standardization. Quality Management Systems-Fundamentals and Vocabulary (ISO

9000: 2015). ISO Copyright ofﬁce, 2015.

Alejandro Salado. Deﬁning better test strategies with tradespace exploration techniques and pareto fronts: Application

in an industrial project. Systems Engineering, 18(6):639–658, 2015.

M Barad and A Engel. Optimizing VVT strategies: a decomposition approach. Journal of the Operational Research

Society, 57(8):965–974, 2006.

Peng Xu and Alejandro Salado. A concept for set-based design of veriﬁcation strategies. In INCOSE International

Symposium, volume 29, pages 356–370. Wiley Online Library, 2019.

Peng Xu, Alejandro Salado, and Xinwei Deng. A parallel tempering approach for efﬁcient exploration of the veriﬁca-

tion tradespace in engineered systems. arXiv preprint arXiv:2109.11704, 2021.

Peng Xu, Alejandro Salado, and Guangrui Xie. A reinforcement learning approach to design veriﬁcation strategies
of engineered systems. In 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages
3543–3550. IEEE, 2020.

Peng Xu and Alejandro Salado. A mathematical approach to design veriﬁcation strategies that incorporates correction

activities as dedicated decisions. Systems Engineering, Under Review.

Nils J Nilsson. Principles of artiﬁcial intelligence. Springer Science & Business Media, 1982.

Dirk Bergemann and Juuso Välimäki. Bandit problems. Technical report, Cowles Foundation for Research in Eco-

nomics, Yale University, 2006.

Sébastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit prob-

lems. arXiv preprint arXiv:1204.5721, 2012.

Tze Leung Lai, Herbert Robbins, et al. Asymptotically efﬁcient adaptive allocation rules. Advances in applied

mathematics, 6(1):4–22, 1985.

Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine

learning, 47(2):235–256, 2002.

Volodymyr Kuleshov and Doina Precup.

Algorithms for multi-armed bandit problems.

arXiv preprint

arXiv:1402.6028, 2014.

Xiaoguang Huo and Feng Fu. Risk-aware multi-armed bandit problem with application to portfolio selection. Royal

Society open science, 4(11):171377, 2017.

Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen,
Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree search
methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1–43, 2012.

Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning.

In European conference on machine

learning, pages 282–293. Springer, 2006.

Levente Kocsis, Csaba Szepesvári, and Jan Willemson. Improved monte-carlo search. Univ. Tartu, Estonia, Tech. Rep,

1, 2006.

Maarten PD Schadd, Mark HM Winands, HJVD Herik, Guillaume MJ-B Chaslot, and Jos WHM Uiterwijk. Single-
player monte-carlo tree search. In International Conference on Computers and Games, pages 1–12. Springer, 2008.

Yngvi Bjornsson and Hilmar Finnsson. Cadiaplayer: A simulation-based general game player. IEEE Transactions on

Computational Intelligence and AI in Games, 1(1):4–15, 2009.

Nicolas Galichet, Michele Sebag, and Olivier Teytaud. Exploration vs exploitation vs safety: Risk-aware multi-armed

bandits. In Asian Conference on Machine Learning, pages 245–260. PMLR, 2013.

Andrew Gehret Barto, Richard S Sutton, and CJCH Watkins. Learning and sequential decision making. University of

Massachusetts Amherst, MA, 1989.

Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of

artiﬁcial intelligence research, 4:237–285, 1996.

21

A PREPRINT

Suman K Mitra, Te-Won Lee, and Michael Goldbaum. A bayesian network based sequential inference for diagnosis

of diseases from retinal images. Pattern recognition letters, 26(4):459–470, 2005.

Rana M Khanafer, Beatriz Solana, Jordi Triola, Raquel Barco, Lars Moltsen, Zwi Altman, and Pedro Lazaro. Auto-
mated diagnosis for umts networks using bayesian network approach. IEEE Transactions on vehicular technology,
57(4):2451–2461, 2008.

Nils J Nilsson. Searching problem-solving and game-playing trees for minimal cost solutions. In Proceedings IFIP

Congress, 1968, volume 2, pages 1556–1562, 1968.

Alberto Martelli and Ugo Montanari. Optimizing decision trees through heuristically guided search. Communications

of the ACM, 21(12):1025–1039, 1978.

Eric A Hansen and Shlomo Zilberstein. LAO*: A heuristic search algorithm that ﬁnds solutions with loops. Artiﬁcial

Intelligence, 129(1-2):35–62, 2001.

Håkan Warnquist, Jonas Kvarnström, and Patrick Doherty. Planning as heuristic search for incremental fault diag-
nosis and repair. In Scheduling and Planning Applications Workshop (SPARK) at the International Conference on
Automated Planning and Scheduling (ICAPS), 2009.

Elodie Chanthery, Yannick Pencolé, and Nicolas Bussac. An AO*-like algorithm implementation for active diagnosis.
In 10th International Symposium on Artiﬁcial Intelligence, Robotics and Automation in Space, i-SAIRAS, pages
75–76, 2010.

Moshe Sniedovich. Dynamic programming: foundations and principles. CRC press, 2010.

Aditya U Kulkarni, Alejandro Salado, Christian Wernz, and Peng Xu. Is verifying frequently an optimal strategy?
a belief-based model of veriﬁcation. In International Design Engineering Technical Conferences and Computers
and Information in Engineering Conference, volume 83983, page V009T09A061. American Society of Mechanical
Engineers, 2020.

Aditya U Kulkarni, Alejandro Salado, Peng Xu, and Christian Wernz. An evaluation of the optimality of frequent

veriﬁcation for vertically integrated systems. Systems Engineering, 24(1):17–33, 2021.

Ross D Shachter and Debarun Bhattacharjya. Dynamic programming in inﬂuence diagrams with decision circuits. In
Twenty-Sixth Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 509–516. Citeseer, 2010.

Lazar Z Velimirovic, Aleksandar Janjic, and Jelena D Velimirovic. Fault location and isolation in power distribution
network using markov decision process. In 2019 14th International Conference on Advanced Technologies, Systems
and Services in Telecommunications (TELSIKS), pages 408–411. IEEE, 2019.

Warren B Powell. Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703. John Wiley

& Sons, 2007.

Warren B Powell. Perspectives of approximate dynamic programming. Annals of Operations Research, 241(1):319–

356, 2016.

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3):279–292, 1992.

Brett Bethke, Jonathan P How, and Asuman Ozdaglar. Approximate dynamic programming using support vector

regression. In 2008 47th IEEE Conference on Decision and Control, pages 3811–3816. IEEE, 2008.

Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Proceedings of 1995 34th

IEEE conference on decision and control, volume 1, pages 560–564. IEEE, 1995.

Thomas G Dietterich. Ensemble methods in machine learning.

In International workshop on multiple classiﬁer

systems, pages 1–15. Springer, 2000a.

Thomas G Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees:

Bagging, boosting, and randomization. Machine learning, 40(2):139–157, 2000b.

Pengyi Yang, Yee Hwa Yang, Bing B Zhou, and Albert Y Zomaya. A review of ensemble methods in bioinformatics.

Current Bioinformatics, 5(4):296–308, 2010.

Faseeha Matloob, Taher M Ghazal, Nasser Taleb, Shabib Aftab, Munir Ahmad, Muhammad Adnan Khan, Sagheer
Abbas, and Tariq Rahim Soomro. Software defect prediction using ensemble learning: A systematic literature
review. IEEE Access, 2021.

Rashmi Saini and SK Ghosh. Ensemble classiﬁers in remote sensing: A review. In 2017 International Conference on

Computing, Communication and Automation (ICCCA), pages 1148–1152. IEEE, 2017.

Tin Kam Ho. Random decision forests. In Proceedings of 3rd international conference on document analysis and

recognition, volume 1, pages 278–282. IEEE, 1995.

22

A PREPRINT

Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd

international conference on knowledge discovery and data mining, pages 785–794, 2016.

Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM:
A highly efﬁcient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017.
Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. CatBoost: gradient boosting with categorical features

support. arXiv preprint arXiv:1810.11363, 2018.

Candice Bentéjac, Anna Csörg˝o, and Gonzalo Martínez-Muñoz. A comparative analysis of gradient boosting algo-

rithms. Artiﬁcial Intelligence Review, 54(3):1937–1967, 2021.

John Hancock and Taghi M Khoshgoftaar. Performance of CatBoost and XGBoost in medicare fraud detection. In
2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 572–579. IEEE,
2020.

Siddharth Jhaveri, Ishan Khedkar, Yash Kantharia, and Shree Jaswal. Success prediction using random forest, Cat-
Boost, XGBoost and AdaBoost for Kickstarter campaigns. In 2019 3rd International Conference on Computing
Methodologies and Communication (ICCMC), pages 1170–1173. IEEE, 2019.

Abdullahi A Ibrahim, Raheem L Ridwan, MM Muhamme, et al. Comparison of the CatBoost classiﬁer with other
machine learning methods. International Journal of Advanced Computer Science and Applications, 11(11):738–
748, 2020.

Dahai Zhang, Liyang Qian, Baijin Mao, Can Huang, Bin Huang, and Yulin Si. A data-driven design for fault detection

of wind turbines using random forests and XGboost. Ieee Access, 6:21020–21031, 2018.

Elzhan Zeinulla, Karina Bekbayeva, and Adnan Yazici. Effective diagnosis of heart disease imposed by incomplete
data based on fuzzy random forest. In 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), pages
1–9. IEEE, 2020.

Ryan Elwell and Robi Polikar. Incremental learning of concept drift in nonstationary environments. IEEE Transactions

on Neural Networks, 22(10):1517–1531, 2011.

Xu-Cheng Yin, Kaizhu Huang, and Hong-Wei Hao. Dynamic ensemble of ensembles in nonstationary environments.

In International Conference on Neural Information Processing, pages 76–83. Springer, 2013.

Xu-Cheng Yin, Kaizhu Huang, and Hong-Wei Hao. DE2: Dynamic ensemble of ensembles for learning nonstationary

data. Neurocomputing, 165:14–22, 2015.

Alejandro Salado and Hanumanthrao Kannan. Elemental patterns of veriﬁcation strategies. Systems Engineering, 22

(5):370–388, 2019.

Alejandro Salado and Hanumanthrao Kannan. Properties of the utility of veriﬁcation. In 2018 IEEE International

Systems Engineering Symposium (ISSE), pages 1–8. IEEE, 2018b.

James O Berger. Statistical decision theory and Bayesian analysis. Springer Science & Business Media, 2013.
Peng Xu and Alejandro Salado. Modeling correction activities in the context of veriﬁcation strategies. Systems

Engineering, 2021.

Marta Vomlelová and Jirí Vomlel. Troubleshooting: NP-hardness and solution methods. Soft Computing, 7(5):357–

368, 2003.

Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Elsevier, 2014.
Kevin Murphy et al. The bayes net toolbox for matlab. Computing science and statistics, 33(2):1024–1034, 2001.

23

