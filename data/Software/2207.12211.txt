2
2
0
2

n
u
J

9
2

]

A
N
.
h
t
a
m

[

1
v
1
1
2
2
1
.
7
0
2
2
:
v
i
X
r
a

hp3D User Manual

Stefan Henneking

Leszek Demkowicz

Oden Institute for Computational Engineering and Sciences

The University of Texas at Austin

Austin, TX. 2022.

 
 
 
 
 
 
Acknowledgements

The authors would like to thank the developers and collaborators who have contributed to the
development of the hp3D software. In addition to the authors, the main contributors to the current
version of the hp3D ﬁnite element code are (in alphabetical order):

• Jacob Badger

• Federico Fuentes

• Paolo Gatto

• Brendan Keith

• Kyungjoo Kim

• Jaime D. Mora

• Sriram Nagaraj

• Socratis Petrides

Additionally, the authors would like to thank each one of the reviewers who have helped with

writing and improving this user manual.

The development of the hp3D software and documentation, including this user manual, was

partially funded by NSF award #2103524.

ii

Contents

Chapter 1.

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 2.

Installing the hp3D Code . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1 Compiling the Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.1 Dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.2 Conﬁgure ﬁle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Compiling an Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 3. Basic Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1

Important Global Variables

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.1

System-generated variables

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.2 User-deﬁned variables

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Application-Speciﬁc Input Files and Routines . . . . . . . . . . . . . . . . . . . . . .

3.2.1

Input ﬁles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.2 Driver and required routines . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3 Mesh Data Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.1 Element data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.2 Looping through active mesh elements . . . . . . . . . . . . . . . . . . . . . .

3.3.3 Uniform mesh reﬁnements . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 Fundamental Finite Element Algorithms . . . . . . . . . . . . . . . . . . . . . . . . .

3.4.1 Assembly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4.2 Linear solver

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

3

4

4

4

5

6

6

6

7

8

9

9

10

10

11

12

12

12

13

Chapter 4. Advanced Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

4.1 Custom Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1.1 Neumann boundary condition . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1.2

Impedance boundary condition . . . . . . . . . . . . . . . . . . . . . . . . . .

14

14

14

iii

4.2 Adaptivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2.1 Adaptive solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2.2 Adaptive reﬁnements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Trace Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4 Coupled Variables

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

15

15

17

18

Chapter 5.

Parallel Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

5.1 Modules and Variables for Parallel Computation . . . . . . . . . . . . . . . . . . . .

5.2 Leveraging Parallelism in Applications . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2.1 Generic element loops

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2.2 Adaptive solution with a distributed mesh . . . . . . . . . . . . . . . . . . . .

5.2.3 Master–worker paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3 Load Balancing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

20

23

23

25

27

30

Appendix A. Model Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

A.1 Poisson Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.1.1 Galerkin implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.1.2 DPG primal implementation . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.2 Linear Elasticity Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.3 Maxwell Problems

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.3.1 Galerkin implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.3.2 DPG ultraweak implementation . . . . . . . . . . . . . . . . . . . . . . . . . .

32

32

37

42

42

43

43

Appendix B. Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

B.1 Optical Fiber Ampliﬁer

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2 Adaptive Solution of High-Frequency Acoustic and Electromagnetic Scattering . . . .

B.3 Linear Elasticity with Two Materials . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.4 Nonlinear Elasticity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.5 Time-Harmonic Applications in Linear Viscoelasticity . . . . . . . . . . . . . . . . .

B.6 Acoustics of the Human Head . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.7 Electromagnetic Radiation and Induced Heat Transfer in the Human Body . . . . . .

44

44

45

45

46

46

47

Appendix C.

Implementation of the DPG Method . . . . . . . . . . . . . . . . . . . 48

iv

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

v

Chapter 1

Introduction

The hp3D ﬁnite element (FE) software has been developed by Prof. Leszek Demkowicz and his
students, postdocs, and collaborators at The University of Texas at Austin over the course of many
years. The current version of the software is available at https://github.com/Oden-EAG/hp3d.
This user manual was written to provide guidance to current and prospective users of hp3D. We
welcome any feedback about the user manual and the hp3D code in general; please contact us at:

• Prof. Leszek Demkowicz: leszek@oden.utexas.edu

• Dr. Stefan Henneking: stefan@oden.utexas.edu

What and who is the hp3D code written for? The hp3D code is an academic software.
For this reason, hp3D is diﬀerent from other publicly available ﬁnite element codes (e.g., FEniCS,
MFEM, deal.II, ...) in several important ways:

• Many of these modern codes are written with the aim to hide as much of the ﬁnite element
machinery from the user as possible, leaving the user to only specify weak formulations,
including boundary conditions, and other features of their application in abstract form. While
this software design has several advantages, e.g. fast prototyping of a new application, and
may be seen as beginner-friendly, it does not at all expose the user to important FE concepts
such as shape functions, Piola maps, and element-level integration. hp3D distinguishes itself
by exposing the user to these fundamental concepts and, for this reason, is conceptually aimed
toward two diﬀerent kinds of audiences:

1. the novel FE user who is interested in learning 3D ﬁnite element coding technology; and

2. the advanced FE user who is interested in having lower-level access to the software making

it simpler to add custom, application-speciﬁc features to the code.

In hp3D, there are only minimal abstraction layers between the lower-level data structures and
the upper-level user application code. On the one hand, this direct access to data structures
makes it straightforward to customize FE implementations; on the other hand, we advise that

1

only advanced users attempt modiﬁcations of data structures that are part of the library code,
because there are few safeguards protecting the user from introducing bugs. When questions
about library features arise, our best advice is to contact the developers before attempting
modiﬁcations.

• hp3D includes a host of advanced ﬁnite element features such as isotropic and anisotropic
hp-adaptivity for hybrid meshes of all element shapes, supporting conforming discretizations
of the entire H 1–H(curl)–H(div)–L2 exact-sequence spaces. These features were built based
on decades of experience in FE coding which went into developing optimized data structures
for hp-adaptive ﬁnite element computation. The hp3D data structures come with unique algo-
rithms, including routines for constrained approximation and projection-based interpolation
based on rigorous mathematical FE theory. For more information about the FE software
design of the hp3D library, we refer to [8, 21]. More recent additions include the support
for trace variables needed for discretizations with the Discontinuous Petrov–Galerkin (DPG)
method, as well as support for hybrid MPI/OpenMP-parallel computation, also detailed in
[21].

• Unlike some other FE libraries, hp3D does not have a full-time developer team that is working
to maintain or develop features for the software. Rather, the hp3D development has depended
on a small team of collaborators writing and testing newly developed features in the code. For
this reason, there is no large support or developer team that can swiftly add features upon
user request.

Is there a 2D version of the hp3D code? Yes, the two-dimensional hp2D FE software is
conceptually equivalent to the 3D code with the exception that it does not support MPI-distributed
parallel computation. However, we have so far not made it publicly available but we may do so
in the future if there is an interest by the community. A former version of the 2D FE code was
documented in:

• L. Demkowicz. Computing with hp Finite Elements. I. One and Two Dimensional Elliptic

and Maxwell Problems. Chapman & Hall/CRC Press, Taylor and Francis, 2006

Remark 1.1. This is a preliminary version of the user manual for the hp3D ﬁnite element software.
The user manual is still being developed and updated. In addition to this user manual, the hp3D
code is documented in the following references and references therein:

• L. Demkowicz et al. Computing with hp Finite Elements. II. Frontiers: Three Dimensional

Elliptic and Maxwell Problems with Applications. Chapman & Hall/CRC, 2007

• S. Henneking and L. Demkowicz. Computing with hp Finite Elements. III. Parallel hp3D

Code. In preparation, 2022

• F. Fuentes et al. “Orientation embedded high order shape functions for the exact sequence

elements of all shapes”. In: Comput. Math. Appl. 70.4 (2015), pp. 353–458

2

Chapter 2

Installing the hp3D Code

An application written for the hp3D software is compiled in two steps. First, the hp3D library
itself is compiled; second, the particular application, which must be linked to the hp3D library,
is compiled. Changes to the application take eﬀect by recompiling only the application, whereas
changes to the underlying library source code take eﬀect only if both the library and the application
are recompiled.

Under most circumstances, the user will not need to recompile the hp3D library since changes
in the application do not aﬀect the library source code. Therefore, most users will only compile the
hp3D library once, and from then on exclusively modify and compile the application code.

In some instances, the user may wish to change the dependencies of the library, e.g., linking
to a new version of a third-party package, which will require recompilation of the library. Another
situation for which the library must be recompiled is if the user chooses to toggle one of the library-
wide preprocessor variables (e.g., DEBUG) or modify the compiler arguments.
It is recommended
that ‘make clean’ is always executed before recompiling the hp3D library.
The user can download the hp3D GitHub repository via https or ssh:

• via https: git clone https://github.com/Oden-EAG/hp3d.git

• via ssh: git clone git@github.com:Oden-EAG/hp3d.git

The main directory is then accessed by ‘cd hp3d/trunk’.

Remark 2.1. The hp3D software has been deployed on a variety of UNIX-based systems. The code
runs eﬃciently on personal laptops, including MacBooks, small workstations and clusters, all the
way to large supercomputers. Currently, we are working on providing a new and more robust build
system for the hp3D library. We do not have any experience installing hp3D on Windows-based
systems and currently have no plans to add support for such systems.

3

2.1

Compiling the Library

This section provides some basic instructions for installing the code. The user should be familiar
with the system in order to provide ﬁle paths to the required third-party packages/dependencies.

The hp3D library is written exclusively in Fortran. The Fortran compiler must be compatible
with the Fortran90 standard and the compiler must support the Message Passing Interface (MPI)
(e.g. OpenMPI, MPICH). The support for OpenMP threading is optional.

2.1.1 Dependencies

The conﬁgure ﬁle m_options (see next section) must link to valid paths for external libraries. The
following external libraries are used:

• Intel MKL [optional]

• X11

• PETSc (all following packages can be installed with PETSc)

• HDF5/pHDF5

• MUMPS

• Metis/ParMetis

• Scotch/PT-Scotch

• PORD

• Zoltan

2.1.2 Conﬁgure ﬁle

The user must create a conﬁgure ﬁle trunk/m_options, which provides information needed by the
makefile. We recommend that the user copies an existing conﬁgure ﬁle and then modiﬁes it as
needed:

1. Copy one of the existing m_options ﬁles from the example ﬁles in hp3d/trunk/m_options_files/

into hp3d/trunk/. For example:
‘cp m_options_files/m_options_TACC_intel19 ./m_options’.

2. Modify the m_options ﬁle to set the correct path to the main directory:

Set the HP3D_BASE_PATH to the absolute path of the hp3d/trunk/.

3. To compile the library, type ‘make’ in hp3d/trunk/. Before compiling, link to the external
libraries by setting the correct paths in the m_options ﬁle—e.g. PETSC_LIB, PETSC_INC—as
well as setting compiler options by modifying the m_options ﬁle as described below.

4

The library compilation is governed by the user-deﬁned preprocessing ﬂags COMPLEX and

DEBUG:

• COMPLEX = 0:

Stiﬀness matrix, load vector(s) and solution DOFs are real-valued.
Code blocks within: #if C_MODE ...

#endif, are disabled.

• COMPLEX = 1:

Stiﬀness matrix, load vector(s) and solution DOFs are complex-valued.
Code blocks within: #if C_MODE ...

#endif, are enabled.

• DEBUG = 0:

Compiler uses optimization ﬂags and the library performs only minimal checks during the
computation.
Code blocks within: #if DEBUG_MODE ...

#endif, are disabled.

• DEBUG = 1:

Compiler uses debug ﬂags, and the library performs additional checks during the computation.
Code blocks within: #if DEBUG_MODE ...

#endif, are enabled.

All computations in hp3D are executed in double-precision arithmetic; that is, depending on the
COMPLEX ﬂag, variables are declared as real(8) or complex(8), respectively. hp3D provides a generic
variable declaration VTYPE, which may be employed by the user in the application, defaulting to
either real- or complex-type depending on whether the library was compiled in real or complex
mode. The user should keep in mind some external library paths may also diﬀer depending on real-
and complex-valued computation. When compiling the hp3D library, the library path is created
under either hp3d/complex/ or hp3d/real/, depending on the choice of preprocessing ﬂag COMPLEX.
Support for OpenMP threading is optional and can be enabled/disabled via preprocessing

ﬂag OPENMP. Additional preprocessing ﬂags for enabling/disabling third-party libraries:

• HP3D_USE_INTEL_MKL = 0:

Dependency on Intel MKL package is disabled.

• HP3D_USE_INTEL_MKL = 1:

Dependency on Intel MKL package is enabled, providing additional solver options to the user
(e.g. Intel MKL PARDISO).

2.2

Compiling an Application

Applications are implemented in hp3d/trunk/problems. A few applications and model problems
are provided in the public hp3D repository and can serve as examples. For example, the directory
problems/POISSON/GALERKIN contains a Galerkin FE implementation for the classical variational
Poisson problem. To compile and run the problem, type ‘make’ in the application folder, i.e.,

5

‘cd problems/POISSON/GALERKIN; make; ./run.sh’.
The implementation of this model problem is described in Appendix A.1.1. Other model problems
and applications are described in Appendices A–B.

Chapter 3

Basic Features

This chapter discusses some of the basic concepts in hp3D which the user should be familiar with,
as well as how to set up the application-speciﬁc ﬁles and routines that need to be provided by the
user.

3.1

Important Global Variables

The hp3D code deﬁnes several important global variables, some of which are system-generated and
may be used within the application code, and others which are user-deﬁned and instruct the library
code.

3.1.1 System-generated variables

Several system modules (e.g. physics) provide useful variables to the library and the user. We show
some examples of these modules and their variables and encourage the user to take a look at the
modules to learn about additional variables that may be of interest. Unless otherwise mentioned,
the modules are located in trunk/src/modules/.

• module physics:

– NR_PHYSA:

This global integer variable deﬁnes the total number of physics variables (or physics
attributes) that have been requested by the user and are stored in the data structure.
Note that each physics attribute may have multiple components. Moreover, a physics
attribute may only be partially supported (i.e. deﬁned over a part of the mesh).

– NR_COMP(i), i=1,...,NR_PHYSA:

This global integer array deﬁnes, for each physics variable, how many components have

6

been requested by the user. Depending on the energy space, a component may be scalar-
valued (H 1, L2) or vector-valued (H(curl), H(div)).

– NRINDEX:

This global integer deﬁnes the total number of components that have been requested by
the user. That is, it is the sum of the entries in the global array NR_COMP(:).

• module assembly:

– ALOC(i,j), BLOC(i), i,j = 1,...,NR_PHYSA:

These two variables are provided to the user for computing local element matrices, i.e. the
user must store the element stiﬀness matrix in ALOC and the element load vector in BLOC.
Both ALOC and BLOC are “super-arrays” with submatrices (blocks) deﬁned for interactions
of each physics attribute. For example, ALOC(i,j)%nrow deﬁnes the number of rows of the
(i, j)-th block of the stiﬀness matrix and ALOC(i,j)%array(:,:) has the corresponding
real-valued or complex-valued entries. The concept will become clear when looking at
model problem implementations (elem routine) with multiple physics variables. See also
Section 4.4 on computing with coupled variables.

• module data_structure3D:

– NRELIS, NRELES:

The data_structure3D module stores variables and arrays related to the mesh data
structure (see Section 3.3). NRELIS is the number of initial mesh elements (does not
change during computation), and NRELES is the number of active mesh elements (increases
with each mesh reﬁnement).

3.1.2 User-deﬁned variables

These global variables, set directly by the user, aﬀect the library behavior. For example, the
static condensation module stc, which implements static condensation routines for local element
matrices and is used by the system routine celem_systemI during assembly, lets the user deﬁne
three variables.

• module stc:

– ISTC_FLAG ∈ {.true.,.false.}:

Enables/disables static condensation of bubble DOFs in element matrices.

– STORE_STC ∈ {.true.,.false.}:

Enables/disables storing Schur complement factors (recomputed otherwise).

– HERM_STC ∈ {.true.,.false.}:

Enables/disables optimized linear algebra routines for Hermitian matrices.

• module parameters:

7

– MAXP ∈ {1,2, ..., 9}:

Deﬁnes the maximum polynomial order of approximation allowed to be used anywhere in
the mesh. This parameter is used internally for preallocating data structures and must be
changed before compiling the hp3D library. The maximum order currently supported is
MAXP=9. The user is advised to compile the library with a lower value of MAXP, depending
on the maximum anticipated order used in computation, for more eﬃcient low-order
computation.

– NRCOMS ∈ {1,2, ...}:

Number of copies of each variable stored in the data structure, e.g. multiple copies may be
needed when computing time-stepping solutions. This parameter may be set at runtime
before mesh initialization by calling the set_parameters routine.

• module paraview:

– VLEVEL ∈ {0,1,2,3,4}:

Determines how reﬁned the mesh output is when exporting geometry and solution data
If VLEVEL=0, the mesh is outputted “as is” with the geometry and
to ParaView/vtk.
solution linearly interpolated for the ParaView plot. Each “upscaling” of VLEVEL increases
the resolution by one reﬁnement level.

• module physics:

– PHYSAd(i) ∈ {.true.,.false.}, i=1,...,NR_PHYSA:

Used for indicating a homogeneous Dirichlet BC for any components of the variable
deﬁned as a Dirichlet component. For such components, the computation is then more
eﬃcient because no Dirichlet data needs to be interpolated.

– PHYSAi(i) ∈ {.true.,.false.}, i=1,...,NR_PHYSA:
Used for computing with traces (see Section 4.3).

– PHYSAm(i) ∈ {.true.,.false.}, i=1,...,NR_PHYSA:

Used for enabling/disabling physics attributes (see Section 4.4).

3.2

Application-Speciﬁc Input Files and Routines

hp3D’s library is set up to read certain user-deﬁned inputs when initializing the program (e.g. initial
geometry ﬁle) and call certain user-deﬁned routines (e.g. providing element-local matrices). This
section brieﬂy reviews the input ﬁles and routines that the user must provide.

In general, when beginning a new application implementation, it is advisable to start oﬀ by
copying an existing model implementation and then modify the required routines. Applications
should be coded within the subdirectory trunk/problems/. Examples of speciﬁc model problem
implementations are given in Appendix A.

8

3.2.1 Input ﬁles

Each of the three required input ﬁles—control, physics, and geometry—must be provided with a
certain formatting and deﬁne a number of required parameters. For additional details and examples
of the input formatting, we refer to the model problems.

• control

Sets global variables in module control. The user must provide the (relative) path to this ﬁle
to the FILE_CONTROL variable in module environment before mesh initialization.

• physics

Sets global variables in module physics. The user must provide the (relative) path to this ﬁle
to the FILE_PHYS variable in module environment before mesh initialization.

• geometry

Deﬁnes the initial geometry mesh. The user must provide the (relative) path to this ﬁle to
the FILE_GEOM variable in module environment before mesh initialization.

3.2.2 Driver and required routines

• program main:

The main program is the “driver” of the application. It should set environment variables, ini-
tialize the mesh data structure, and interact with the user (or execute a pre-deﬁned job). The
initialization of an application includes calling various library routines, such as read_control,
read_input, read_geometry, and hp3gen.

• subroutine set_initial_mesh(Nelem_order):

The initialization routine hp3gen sets up the mesh data structures (see next section). Dur-
ing this initialization, a user-provided routine set_initial_mesh is called which deﬁnes the
supported physics variables and initial order of approximation for each element (stored in
Nelem_order(:)), as well as the boundary condition ﬂags for physics components on element
faces.

• subroutine dirichlet:

If the user has speciﬁed non-homogeneous Dirichlet ﬂags for any component on the boundary,
then the user-provided dirichlet routine is required. The routine is called from the system
routine update_Ddof which computes the Dirichlet data via projection-based interpolation.
dirichlet takes a physical coordinate input x and must return the value and ﬁrst derivatives
of the expected H 1, H(curl), and/or H(div) boundary data at coordinate x.

• subroutine elem:

The elem routine is at the heart of the application code—it implements the variational formu-
lation on the element level. This routine is called by system routines during the ﬁnite element

9

assembly and provides the element-local stiﬀness matrix and load vector for each element in
the active mesh.

3.3 Mesh Data Structure

Due to the support for hybrid meshes and adaptive reﬁnements, the hp3D mesh data structure is
dynamically build as reﬁnements are executed. Before the main data structure arrays (ELEMS and
NODES) are discussed, we review how element data is stored and accessed for elements of diﬀerent
shapes.

3.3.1 Element data

Module element_data. The hp3D code supports four types of elements: hexahedra, tetrahedra,
prisms, and pyramids. Any element computations are executed in an object-oriented programming
fashion using general element utilities provided in src/modules/element_data. For instance, a
simple loop through element vertices is executed by

do iv=1,Nvert(elem_type)

...

enddo

Listing 3.1: Loop over element vertices.

where elem_type is a member variable of an element specifying the element type—‘bric’, ‘tetr’,
‘pris’, ‘pyra’—and Nvert(elem_type) is a function returning the number of vertices for each
element type. In this way, we avoid writing four separate versions of the code for the four types
of elements but cover all four cases with just one piece of code. The user may want to review the
module element_data to learn about the existing element utilities and the underlying logic.

The element_data module starts by listing master element coordinates for the four element
vertices. This deﬁnes the geometry of master elements and establishes enumeration of master
element vertices. The element edges are listed next by providing numbers of the corresponding
endpoint vertices. This again serves a double purpose: we enumerate the element edges and provide
the corresponding local edge orientations. Finally, we list element faces by providing face-to-vertex
node connectivities. This again implies the local face orientations. Finally, following the speciﬁed
orientations, we provide local parametrizations for element edges and faces. All these utilities
are necessary for computing element matrices. The elem routines provided in the model problem
implementations (Appendix A) illustrate how to use the utilities provided in the module.

Initial mesh generation. Data structure arrays ELEMS and NODES. Following the input of
geometry for the Geometry Modeling Package (GMP), an initial mesh is generated. The initial
mesh generator represents an interface between the GMP package and the hp3D code, and it does

10

essentially two things: it generates initial mesh vertex, edge, face and element middle nodes in the
data structure array NODES, and it generates initial mesh elements in data structure array ELEMS,
including element-to-nodes connectivities. Both ELEMS and NODES are arrays of objects provided by
the data_structure3D module. Array ELEMS is static; its dimension equals the number of initial
mesh elements equal to the number of GMP blocks. Array NODES is dynamic; it grows during the
mesh reﬁnements as new nodes are generated. The elements arising from mesh reﬁnements are
logically identiﬁed with their middle nodes. Middle node numbers are thus unique identiﬁers for
an element, and the number of a node always coincides with the location in the NODES array. For
the initial mesh, the number of an initial mesh element in ELEMS coincides with the corresponding
middle node number in NODES.

3.3.2 Looping through active mesh elements

The NODES array stores all (abstract) element nodes—vertices, edges, faces, and middle nodes—
whether they are part of the current mesh or not. In order to access active mesh elements (associated
with active middle nodes), the hp3D code provides a functionality to loop through the active mesh
elements in their “natural” order deﬁned by the tree structure of the NODES array.

Natural order of elements. A typical loop through the active elements in the mesh is executed
by using routine datstrs/nelcon:

mdle = 0

do iel=1,NRELES

call nelcon(mdle, mdle)

...

enddo

Listing 3.2: Loop over active mesh elements.

where NRELES is a global variable storing the number of active mesh elements. The nelcon
routine is based on nodal trees restricted to element middle nodes only. See [3, 8] for additional
information.

Remark 3.1. For MPI/OpenMP parallel execution of loops over active mesh elements, the hp3D
data_structure3D module builds two arrays, ELEM_ORDER and ELEM_SUBD, based on nelcon. ELEM_ORDER
is an array of all active mesh elements, and ELEM_SUBD is an array of active mesh elements within
a subdomain of a distributed mesh. Each one is built (resp. updated) by using nelcon after each
mesh reﬁnement or mesh repartitioning. See Section 5.1 for additional details.

11

3.3.3 Uniform mesh reﬁnements

An isotropic uniform h-reﬁnement can be executed via routine global_href. Anisotropic uniform
h-reﬁnement routines are also provided for some cases (e.g. global_href_aniso_bric) but should
be used carefully since particular anisotropic reﬁnements may not be compatible with a hybrid mesh
of diﬀerent element types (see reﬁnement ﬂags in Section 4.2.2).

Analogously, isotropic uniform p-reﬁnements are easily executed via routine global_pref. For
the case of p-reﬁnements, the user may also execute global unreﬁnements (i.e. uniformly lowering
the polynomial order of approximation) via routine global_punref.
Adaptive hp-reﬁnements are discussed in Section 4.2.2.

3.4

Fundamental Finite Element Algorithms

The code supports two fundamental FE algorithms: solution of a boundary-value problem and
adaptive solution of a boundary-value problem.
In the second case, the user must provide an
additional routine providing an a-posteriori error estimate. This section outlines the assembly and
linear solve of a ﬁnite element system in hp3D. Adaptive solutions are discussed in Section 4.2
(adaptive solver and reﬁnements) and Section 5.2.2 (parallel computation of an adaptive solution).

3.4.1 Assembly

The ﬁnite element assembly process takes element-local stiﬀness matrices and load vectors (provided
by the user routine elem) and assembles the local blocks into a sparse global matrix. This process
also includes much of the sophisticated hp3D machinery for constrained approximation, as well
as modiﬁcations due to accounting for Dirichlet data and static condensation of element-interior
degrees of freedom. The assembly routines are hidden from the user and do not usually need to be
interacted with directly. Instead, the user calls one of the provided linear solver interfaces which
perform the assembly process for the user application.

The global stiﬀness matrix and load vector are assembled automatically by the solver interface,
i.e. the assembly routines loop through all active elements in the mesh and compute the correspond-
ing element matrices. This implementation of the element stiﬀness matrix and load vector are in the
user-provided element routine elem. For each active element mdle, the code calls elem from routine
src/constrs/celem_system which returns the information about the modiﬁed element correspond-
ing to mdle and the modiﬁed element matrices. For a detailed discussion of celem_system, see [3,
8]. In summary, the celem_system routine compiles the information about the modiﬁed element
nodes, the corresponding number of DOFs, performs partial assembly to account for the constrained
nodes, and enforces the Dirichlet BCs by computing the modiﬁed load vector and eliminating the
Dirichlet DOFs from the system of equations. The solution DOFs returned by the solver are then
stored in the data structure arrays.

12

3.4.2 Linear solver

The code provides interfaces to several linear solvers:

• MUMPS:

Diﬀerent MUMPS solver options are available to the user via the following solver interfaces:

– OpenMP MUMPS: mumps_sc

Sequential MUMPS solver with OpenMP support.

– MPI/OpenMP MUMPS: par_mumps_sc

Distributed MUMPS solver with OpenMP support.

– MPI/OpenMP Nested Dissection: par_nested

Statically condenses subdomain interior DOFs onto subdomain interfaces and subse-
quently solves the coupled interface problem via the distributed MUMPS solver.

• MKL_PARDISO: pardiso_sc

Intel MKL’s PARDISO solver (OpenMP support only) is available via solver interface pardiso_sc,
provided the library code was compiled with ﬂag HP3D_USE_INTEL_MKL = 1 (see Section 2.1.2).

• Frontal Solver: solve1

Homegrown sequential solver (no MPI or OpenMP support) used mainly for debugging pur-
poses. The solver interface is implemented in the routine solve1.

• PETSc: petsc_solve

The PETSc solver interface supports a variety of external solver options, depending upon the
PETSc library hp3D was linked to during compilation (cf. Section 2.1.1).

Some of the solver interfaces take a single-character input argument ‘H’ or ‘G’, indicating whether
the linear system is symmetric/Hermitian or not, in order to perform optimized linear algebra
computations. In some cases, the user may also specify ‘P’ to indicate positive-deﬁniteness of the
system.

The auxiliary data structures for assembly and linear solve of the ﬁnite element system are
deallocated before the solver interface returns. On return, the ﬁnite element solution (i.e. solution
degrees of freedom) has been stored in the data structure and is available to the user for post-
processing.

13

Chapter 4

Advanced Topics

This is a preliminary version of the user manual. This chapter on the advanced FE concepts provided
by the hp3D code is currently under development and will be further expanded and completed in a
future version of the user manual.

4.1

Custom Boundary Conditions

In the preceding chapter, it was shown how to set up basic Dirichlet boundary conditions for a
variable. In this section, some custom options for setting up various types of boundary conditions
are discussed. Note that the hp3D code allows the user to customize boundary conditions in an
almost arbitrary way. A few common boundary condition types are presented with the goal of
introducing the general idea of customizing boundary conditions for an application.

In the user-deﬁned set_initial_mesh routine, boundary conditions are set per component:

• ELEMS(iel)%bcond(1:6,1:NRINDEX) ∈ { 0,1, ..., 9 }:

user-deﬁned value indicating for faces of each initial mesh element iel whether a component
is a free component (0), a Dirichlet component (1), or has a custom BC (2, ..., 9).

• NODES(nod)%bcond(1:NRINDEX) ∈ { 0,1 }:

system-generated ﬂag indicating for each node nod whether a component is a Dirichlet com-
ponent (1) or not (0).

4.1.1 Neumann boundary condition

Discussion of Neumann BC will be included in a future version of the user manual.

4.1.2 Impedance boundary condition

Discussion of impedance BC will be included in a future version of the user manual.

4.2

Adaptivity

The sophisticated hp-adaptive capabilities are one of the main selling points of the hp3D ﬁnite
element code. This section describes the basics of an adaptive solution procedure and how the user

14

can execute hp-adaptive mesh reﬁnements in the application code.

4.2.1 Adaptive solver

The simplest adaptive algorithm executes the standard logical sequence:

Solve −→ Estimate −→ Mark −→ Reﬁne

until a prescribed global error tolerance is met. Error estimation involves a loop through elements
and computation of element error indicators plus possible additional information aiming at selection
of an optimal element reﬁnement. Once the element error indicators are known for all active
elements, selected elements are marked for pre-selected hp-reﬁnements. Two marking strategies are
most popular: the greedy strategy and the Dörﬂer strategy.

In the greedy strategy, a maximum element error indicator error_max is ﬁrst determined.
All elements which exceed a certain percentage of error_max are then marked for reﬁnement. The
reﬁnement criterion for a single element is thus

error > perc * error_max.

In Dörﬂer’s strategy [10], the elements are ﬁrst organized in the order of descending element
error indicators and the total error error_glob is computed by summing up the element error
indicators. The ﬁrst n elements whose cumulative sum exceeds a certain percentage of the total
error are then marked for reﬁnement, i.e.:

n
(cid:88)

j=1

elem_errorj > perc ∗ error_glob.

Marked elements are placed on a separate list along with the requested reﬁnement ﬂags.

4.2.2 Adaptive reﬁnements

This section gives an overview of the hp-adaptive capabilities of the hp3D code. Note that the section
does not discuss the implementation of an error indicator function for guiding mesh adaptivity, but
rather introduces the mechanism of executing the adaptive reﬁnements. It is assumed that some
error indicator function, usually problem-dependent, is provided by the user.

The starting point for adaptive reﬁements is an array elem_ref(:) of size nr_elem_ref
storing a list of the middle node numbers (mdle) corresponding to elements marked for reﬁnement.

In the ﬁrst example, we show how h-adaptive reﬁnements are executed. In essence,
h-adaptivity.
each middle node is reﬁned one-by-one by calling the refine routine with a corresponding reﬁnement

15

ﬂag kref encoding the type of h-reﬁnement. For middle nodes that can be anisotropically reﬁned,
kref encodes how to reﬁne the corresponding element. Encoding of reﬁnement ﬂags depends upon
the element type; for instance,

• a hexahedral element may be reﬁned in 7 diﬀerent ways; for example:

– kref = 111 : reﬁne in x, y, z
– kref = 110 : reﬁne in x, y
– kref = 101 : reﬁne in x, z

• a prismatic element may be reﬁned in 3 diﬀerent ways:

– kref = 11 : reﬁne in xy, z
– kref = 10 : reﬁne in xy
– kref = 01 : reﬁne in z

For hybrid meshes, isotropic reﬁnements can be executed by obtaining isotropic reﬁnement
ﬂags for any element type from the routine get_isoref for the respective middle node. After exe-
cuting h-adaptive reﬁnements, it is essential that the user calls the close_mesh routine to preserve
1-irregularity of the mesh.

!..iterate over elements marked for refinement

do iel=1,nr_elem_ref

mdle = elem_ref(iel)

! ...get isotropic h-refinement flag for element type

call get_isoref(mdle, kref)

! ...h-refine element

call refine(mdle,kref)

enddo

!..enforce 1-irregular mesh

call close_mesh

!..update geometry and Dirichlet DOFs

call update_gdof

call update_Ddof

Listing 4.1: Isotropic h-adaptive reﬁnements.

In the second example, we show how p-adaptive reﬁnements are executed. Isotropic
p-adaptivity.
p-adaptive reﬁnements are carried out easily by the routine execute_pref which must only be
provided with the list of elements to be reﬁned in the form of the corresponding middle nodes.
After executing adaptive p-reﬁnements, the user may want to call one of the system routines
enforce_min_rule or enforce_max_rule to apply the minimum or maximum rule, respectively,
to handle neighboring elements of diﬀerent approximation order in a particular way (i.e. either
lowering or raising the order of approximation on the corresponding element interfaces).

16

!..execute isotropic p-refinements for a list of elements

call execute_pref(elem_ref,nr_elem_ref)

!..enforce minimum rule

call enforce_min_rule

!..update geometry and Dirichlet DOFs

call update_gdof

call update_Ddof

Listing 4.2: Isotropic p-adaptive reﬁnements.

Similar to the h-adaptive case where the reﬁnement ﬂag encoded possibly anisotropic re-
ﬁnements, the polynomial order p of element nodes encodes the possibly anisotropic order of ap-
proximation. In order to execute anisotropic p-adaptive reﬁnements, the user may instead call the
routine perform_pref, which additionally must be provided with a list of p-reﬁnement ﬂags for
middle nodes corresponding to the list of elements to be reﬁned. For each middle node on this list,
the p-reﬁnement ﬂag encodes the desired order of approximation for the middle node. For example,
p = 222 encodes isotropic quadratic order for the middle node of a hexahedral element, whereas
p = 34 encodes anisotropic mixed order for the middle node of a prism. The routine will then
execute corresponding p-reﬁnements of edge and face nodes. After ﬁnishing p-adaptive reﬁnements,
the user may again want to enforce the minimum or maximum rule.

4.3

Trace Variables

This section discusses how trace variables can be discretized in hp3D. This feature is especially
important for discretizations with the DPG method [7]. The hp3D code oﬀers conforming dis-
cretizations for traces of the exact-sequence energy spaces, i.e. functions belonging to the H 1/2-,
H −1/2(curl)-, and H −1/2-energy spaces.
In hp3D, these trace unknowns—which are deﬁned on
element interfaces—are discretized by using restrictions of H 1-, H(curl)-, and H(div)-conforming
elements to the element boundary. The discretized trace unknowns are thus continuous, tangentially
continuous, and normally continuous, respectively. For further reading, we refer to [4, 5].

To realize trace variables as restrictions of exact-sequence-conforming elements, the user spec-

iﬁes for each physics attribute whether it is a trace variable via a global ﬂag:

PHYSAi(i) ∈ {.true.,.false.}, i=1,...,NR_PHYSA.

If enabled, this implies that interactions of bubble (interior) DOFs of an element are not stored in
the element matrices for the corresponding traces. In order to compute with trace unknowns, the
user must enable hp3D’s static condensation module stc (see Section 3.1). By default, all physics
variables are deﬁned as standard variables unless otherwise instructed by the user. The user may
not for obvious reasons request a trace of an L2-conforming (discontinuous) variable.

17

4.4

Coupled Variables

In hp3D, problems with coupled variables may be deﬁned in a way where the coupling happens over
the entire domain, over part of the domain, or at an interface between diﬀerent parts of the domain.
This section primarily focuses on problems with variables supported in the entire mesh.

Computing with multiple physics variables. Solving multiphysics problems in hp3D requires
an understanding of how hp3D internally stores and computes interactions between multiple vari-
ables. First, recall the following essential information from previous sections:

• Global variables NR_PHYSA, NR_COMP(:), and NRINDEX store the total number of physics at-
tributes, the number of components per physics attribute, and the total number of components,
respectively.

• In the set_initial_mesh routine, boundary conditions are set separately for each component.

• In the elem routine, element-local matrices are assembled in blocks corresponding to (sup-

ported) physics attributes:
ALOC(1:NR_PHYSA,1:NR_PHYSA)%array(:,:) (element stiﬀness); and
BLOC(1:NR_PHYSA)%array(:,:) (element load).
Note that the load “vector” may have multiple columns if solving a problem for multiple
right-hand sides.

Physics variables are always deﬁned and stored (in physics) in the order of the exact sequence.
For each approximation space, the user may deﬁne multiple physics attributes (in the physics input
ﬁle), and each attribute may have one or multiple components (also deﬁned in the physics input
ﬁle).

The elem routine that assembles each of the stiﬀness and load blocks must provide the
system assembly routines with information about which blocks have been locally computed. This
is necessary for two reasons:

1. An element may not support all physics variables thus will only assemble a subset of the blocks

corresponding to the supported attributes.

2. A linear system for only a subset of the supported variables is assembled and solved.

In order to provide this information to the system routines, elem returns a ﬂag for each of the blocks
indicating whether it was computed by the element. The following arguments are deﬁned in the
elem routine:

• integer, intent(in) ::

Mdle

Mdle is the unique middle node number associated with the element.

18

• integer, intent(out) ::

Itest(NR_PHYSA)

For the i-th test function (corresponding to the i-th physics attribute), Itest(i) = 1 indicates
that the i-th block (row) of ALOC and BLOC corresponds to an active and supported variable.

• integer, intent(out) ::

Itrial(NR_PHYSA)

For the i-th trial function (corresponding to the j-th physics attribute), Itrial(j) = 1 indi-
cates that the j-th block (column) of ALOC corresponds to an active and supported variable.

The (i, j)-th block of ALOC is provided by the elem routine if and only if Itest(i) = 1 and
Itrial(j) = 1, and the i-th block of BLOC is provided if and only if Itest(i) = 1. Usually, when
the (i, j)-th stiﬀness block is computed, then the (j, i)-th block is also provided.

In a problem where the user would like to
Solving for a subset of the physics variables.
compute a subset of the variables at a time (e.g. staggered solve in simple ﬁxed-point iteration),
this can easily be done by toggling the global ﬂags PHYSAm(:) of the physics module. For the
i-th physics attribute (i = 1,...,NR_PHYSA), PHYSAm(i)=.false. deactivates the corresponding
physics attribute. If all physics variables are otherwise supported on the entire mesh, then the elem
routine can very easily set the assembly ﬂags Itest and Itrial as follows:

! in:

Mdle

- element middle node number

! out: Itest

- index for assembly

!

Itrial

- index for assembly

subroutine elem(Mdle, Itest,Itrial)

integer, intent(in) :: Mdle

integer, intent(out) :: Itest(NR_PHYSA)

integer, intent(out) :: Itrial(NR_PHYSA)

! .......

Itest(1:NR_PHYSA) = 0; Itrial(1:NR_PHYSA) = 0

!

!..all variables are supported on all elements,

! thus assemble for all of the currently enabled variables

do i = 1,NR_PHYSA

if (PHYSAm(i)) then

Itest(i) = 1; Itrial(i) = 1

endif

enddo

! ...... compute the corresponding blocks of ALOC and BLOC

Listing 4.3: Solving for a subset of variables in elem.

Remark 4.1. More information about partial support of variables (deﬁned in parts of the domain)
will be included in a future version of the user manual.

19

Chapter 5

Parallel Computation

This chapter gives a brief introduction to parallel computation with MPI and OpenMP in hp3D.
Further details about parallel algorithms and data structures in hp3D are given in [21].

5.1 Modules and Variables for Parallel Computation

This section brieﬂy reviews essential functionality and variables provided in some of hp3D’s paral-
lelization modules that are used internally and can also be leveraged by the user in the problem
implementation.

mpi_wrapper module. The mpi_wrapper module provides two essential routines: mpi_w_init
and mpi_w_finalize. In every problem implementation, these two routines should be the very ﬁrst
and the very last call, respectively, of the program:

program main

use mpi_wrapper

call mpi_w_init

! ... program execution ...

call mpi_w_finalize

end program main

Listing 5.1: Initiating and ﬁnalizing hp3D’s MPI environment.

These calls are required for initializing and closing the MPI environment, including hybrid MPI/OpenMP
threading, as well as initializing and closing the Zoltan environment used for load balancing (see
Section 5.3). The calls to mpi_w_init and mpi_w_finalize should be made even if the program is
executed by a single MPI process.

The user is also encouraged to use the mpi_w_handle_err(Ierr, Str) routine of the mpi_wrapper

module which prints the error code and a user-deﬁned error string Str if the return code Ierr of
an MPI function is not equal to MPI_SUCCESS. For example:

20

call MPI_BARRIER (MPI_COMM_WORLD, ierr)

call mpi_w_handle_err (ierr, "MPI_BARRIER returned with error code.")

Listing 5.2: Checking and printing MPI error codes.

mpi_param module. This module stores global parameters and variables related to MPI paral-
lelism. The user may want to make use of the following parameters that are initialized with the
MPI environment:

• integer, save :: NUM_PROCS

Total number of MPI processes. Return value of MPI_COMM_SIZE for the MPI_COMM_WORLD
communicator.

• integer, save :: RANK

MPI rank of each MPI process. RANK ∈ {0, 1, . . . , NUM_PROCS-1}. Return value of MPI_COMM_RANK
for the MPI_COMM_WORLD communicator.

• integer, parameter :: ROOT = 0

hp3D deﬁnes the MPI process with RANK 0 as the root process (or equivalently, host process).
Frequently, the root process takes a diﬀerent execution path from the remaining processes
(e.g. in user-interactive computation—see Section 5.2.3, or in I/O operations).

par_mesh module. Most of the essential mesh (re-)partitioning functionality in hp3D is provided
by the module src/modules/par_mesh.
It should be emphasized again that the initial mesh is
not distributed. In other words, initially, the degrees of freedom are present on all MPI processes,
i.e. every MPI process has its own copy of the initial FE mesh. The par_mesh module provides a
distr_mesh routine that can be called by the user to distribute the mesh at any point, either right
after initialization or after some mesh reﬁnements. The distr_mesh routine also serves as a routine
for repartitioning (resp. load balancing)—see Section 5.3.

The par_mesh module deﬁnes two important global variables that keep track of the current

state of the mesh in the parallel environment:

• logical, save :: DISTRIBUTED

– .false. if the current mesh is not distributed (all DOFs present on all MPI processes);
.true. if the current mesh is distributed (DOFs are distributed across MPI processes).
– Once distributed, a mesh cannot be “undistributed.” In other words, even if the partition-
ing is unbalanced (e.g., all DOFs are on one MPI process), the mesh is still distributed
in the sense that each mesh element is associated with one particular subdomain (resp.
MPI process).

21

– Initially, DISTRIBUTED = .false.

After calling distr_mesh, DISTRIBUTED = .true.

– If using sequential computation (single MPI process), the value of DISTRIBUTED is irrel-

evant.

• logical, save :: HOST_MESH

– .true. if and only if the ROOT process has the entire mesh (all DOFs are present on

ROOT).

– Initially, HOST_MESH = .true. since all DOFs present on all processes including the ROOT

process. After calling distr_mesh, HOST_MESH = .false. unless NUM_PROCS = 1.

– The DOFs of a distributed mesh can be collected on the host process by calling the
collect_dofs routine in src/mpi/par_aux.F90. Collecting DOFs on host enables utili-
ties written only for sequential computing (e.g., interactive visualization) and can serve
as a debugging tool. After calling collect_dofs, HOST_MESH = .true.

– If using sequential computation (single MPI process), the value of HOST_MESH is always

.true.

As will be discussed in more detail in Section 5.2, the user should be familiar with the
meaning of these two variables to leverage parallel computation in hp3D applications. In particular,
using these mesh state variables greatly simpliﬁes writing and executing an hp3D application that
supports both the sequential and distributed-memory setting.

ELEM_ORDER and ELEM_SUBD. The data_structure3D module provides two arrays that are fre-
quently used in the parallel environment. Recall from Section 3.3 that iterating over the active
mesh elements is commonly done by using the nelcon routine that provides a natural ordering
of elements (see Listing 3.2).
In the parallel setting, this sequential looping is replaced with a
loop over one of two active mesh element arrays: ELEM_ORDER and ELEM_SUBD. The ELEM_ORDER
array stores all active mesh elements in their natural order; the ELEM_SUBD array stores all active
mesh elements within a particular subdomain of a distributed mesh. After each mesh reﬁnement or
mesh repartitioning, the two arrays ELEM_ORDER and ELEM_SUBD are automatically updated by the
update_ELEM_ORDER routine of the data_structure3D module:

mdle = 0; NRELES_SUBD = 0

do iel=1,NRELES

call nelcon(mdle, mdle)

ELEM_ORDER(iel) = mdle

if (NODES(mdle)%subd .eq. RANK) then

NRELES_SUBD = NRELES_SUBD + 1

ELEM_SUBD(NRELES_SUBD) = mdle

endif

enddo

22

Listing 5.3: Updating the data structure arrays ELEM_ORDER and ELEM_SUBD.

The total number of active mesh elements (size of ELEM_ORDER) is denoted by NRELES, and
the number of elements within the subdomain (size of ELEM_SUBD) is denoted by NRELES_SUBD. The
ELEM_ORDER and ELEM_SUBD arrays are used internally for parallelizing the element loops, and they
can be leveraged by the user for the same purpose as demonstrated in Section 5.2.1.

5.2

Leveraging Parallelism in Applications

This section discusses how MPI and OpenMP parallelism can be leveraged in the user application.
As previously discussed in this chapter, most of the functionality that enables parallel computation
in hp3D is hidden from the user. Nonetheless, the user should be to some extent familiar with the
parallel environment to be able to implement or modify customizable parallel algorithms within the
application.

Before reading this section, it is highly recommended to review the essential modules and

data structures enabling parallelism introduced in the preceding section (Section 5.1).

5.2.1 Generic element loops

Looping over mesh elements is one of the most common operations in a ﬁnite element code. Usually,
these parallelized loops are hidden from the user in hp3D; for example, the assembly process only
requires the user to provide local element matrices which are then modiﬁed and assembled to
a global linear system by hp3D’s parallelized system routines. However, for any meaningful FE
computation, it is essential for the user to understand how a parallel element loop works. For
instance, post-processing solution data commonly requires volumetric or boundary integrals over
each element. In this type of computation, most of the work is element-local thus can be eﬀectively
parallelized. Usually, the parallelized element-local computation is followed by a communication
step (e.g., a reduction operator) to calculate aggregate values of interest (e.g., a global residual).

Subdomain element loops.
memory computation—is implied by the mesh partitioning. Section 5.1 introduced the data struc-
tures and concepts needed to write a parallel loop over mesh elements within subdomains of the
distributed mesh. A typical subdomain element loop can be implemented as follows:

In the distributed setting, the ﬁrst level of parallelism—MPI distributed-

integer :: iel, mdle

real(8) :: elem_val, subd_val, total_val

!..if computing sequentially (single MPI process), subdomain is the entire mesh

if (.not. DISTRIBUTED) ELEM_SUBD(1:NRELES) = ELEM_ORDER(1:NRELES)

!..iterate over elements within the subdomain

23

subd_val = 0.d0

do iel=1,NRELES_SUBD

mdle = ELEM_SUBD(iel)

! ...element-local computation

elem_val = ...

! ...aggregate element values over subdomain

subd_val = subd_val + elem_val

enddo

!..aggregate subdomain values globally

call MPI_REDUCE(subd_val,total_val,1,MPI_REAL8, &

MPI_SUM,ROOT,MPI_COMM_WORLD, ierr)

Listing 5.4: Looping over active mesh elements in a subdomain.

Parallelizing subdomain element loops. With MPI and OpenMP enabled, the user will typ-
ically write a loop over elements within each subdomain using OpenMP threading for parallel
element computation within the subdomain. This second level of parallelism—OpenMP shared-
memory computation—must be initiated explicitly by opening an OpenMP parallel environment.
In this case, element-local variables must be declared as PRIVATE variables by the user (i.e., each
thread has its own copy of the variable), and subdomain aggregate values are computed by an
OpenMP REDUCTION clause. In other words, with two levels of parallelization, aggregate values are
computed by a two-level reduction operation: ﬁrst, a reduction across OpenMP threads within the
subdomain computation; and second, a reduction across MPI processes for the global aggregate
value.

!$OMP PARALLEL DO PRIVATE(mdle,elem_val) &

!$OMP REDUCTION(+:subd_val)

do iel=1,NRELES_SUBD

mdle = ELEM_SUBD(iel)

! ...element-local computation

elem_val = ...

! ...aggregate element values over subdomain

subd_val = subd_val + elem_val

enddo

!$OMP END PARALLEL DO

Listing 5.5: Accelerating subdomain element loops by OpenMP threading.

OpenMP thread scheduling. By default, an OpenMP-parallel loop uses static thread schedul-
ing, i.e. each loop iteration (resp. element workload) is statically assigned a-priori to a particular
OpenMP thread. In context of hp-adaptively reﬁned meshes with elements of diﬀerent shape, this

24

default strategy can lead to a severely unbalanced workload between OpenMP threads due to highly
variable element workload. In this case, it is preferable to use dynamic thread scheduling: by passing
the OpenMP clause SCHEDULE(DYNAMIC), each thread is assigned one loop iteration (resp. element
workload) at a time. Load balancing is thus also done on two levels: the distributed-memory level
(a-priori load balancing by mesh repartitioning) and the shared-memory level (dynamic load bal-
ancing by OpenMP thread scheduling). While dynamic thread scheduling at runtime eliminates the
need for a-priori load balancing work, it comes with a scheduling overhead at loop execution. This
overhead can be somewhat reduced by assigning a few elements at a time (i.e., using an OpenMP
chunk size larger than one in the dynamic schedule) or by using a guided schedule with variable
chunk size. These options reduce scheduling overhead by somewhat compromising on load balance.
The optimal choice of OpenMP schedule will depend on a variety of problem-dependent factors.
Generally speaking: if the workload is similar for all elements, use either a static schedule or dy-
namic schedule with large chunk sizes; if the element workload varies much, use a dynamic schedule
with small chunk sizes.

Remark 5.1. As previously mentioned, element loops for matrix assembly and other tasks are part
of the system routines and thus hidden from the user application. In hp3D, these system routines
by default employ a dynamic OpenMP thread schedule in all element loops to account for varying
element workload in adaptive solutions.

5.2.2 Adaptive solution with a distributed mesh

A typical use-case of performing element-loops is a-posteriori error estimation. An element-local
error indicator can be computed independently and in parallel for each element. For instance, in
the DPG method the element-local residual serves as a built-in error indicator [7]. The following
steps illustrate how parallel MPI/OpenMP computation is used for computing adaptive reﬁnements
based on Dörﬂer’s marking strategy (see Section 4.2.1) using element residuals on a distributed
mesh in hp3D:

1. Compute element residuals within the subdomain:

subd_res = 0.d0; elem_res(1:NRELES) = 0.d0

!$OMP PARALLEL DO PRIVATE(mdle,subd) &

!$OMP SCHEDULE(DYNAMIC) REDUCTION(+:subd_res)

do iel=1,NRELES

mdle = ELEM_ORDER(iel)

call get_subd(mdle, subd)

if (RANK .ne. subd) cycle

call elem_residual(mdle, elem_res(iel))

subd_res = subd_res + elem_res(iel)

enddo

!$OMP END PARALLEL DO

25

Listing 5.6: Adaptive reﬁnements based on element residuals: (a) subdomain computation.

2. Communicate residual values by MPI reduction:

call MPI_ALLREDUCE(subd_res,total_res,1,MPI_REAL8, &

MPI_SUM,MPI_COMM_WORLD, ierr)

call MPI_ALLREDUCE(MPI_IN_PLACE,elem_res,NRELES,MPI_REAL8, &

MPI_SUM,MPI_COMM_WORLD, ierr)

Listing 5.7: Adaptive reﬁnements based on element residuals: (b) global communication.

3. Mark elements for adaptive reﬁnement (Dörﬂer’s strategy):

!..sort elements by residual values (in descending order)

mdle_ref(1:NRELES) = ELEM_ORDER(1:NRELES)

call qsort_duplet(mdle_ref,elem_res,NRELES)

!..mark elements for refinement based on marking strategy

alpha = 0.5d0 ! (marking coefficient)

nr_elem_ref = 0; sum_res = 0.d0

do iel=1,NRELES

nr_elem_ref = nr_elem_ref + 1

sum_res = sum_res + elem_res(iel)

if (sum_res > alpha*total_res) exit

enddo

Listing 5.8: Adaptive reﬁnements based on element residuals: (c) element marking.

4. Reﬁne marked elements:

!..iterate over elements marked for refinement

do iel=1,nr_elem_ref

mdle = mdle_ref(iel)

! ...get isotropic h-refinement flag for element type

call get_isoref(mdle, kref)

! ...h-refine element

call refine(mdle,kref)

enddo

!..enforce 1-irregular mesh

call close_mesh

!..update geometry and Dirichlet DOFs

call update_gdof

call update_Ddof

!..repartition the mesh for load balancing

26

call distr_mesh

Listing 5.9: Adaptive reﬁnements based on element residuals: (d) element reﬁnement.

5.2.3 Master–worker paradigm

There are numerous of ways of realizing a parallel driver for hp3D applications. The user is, of
course, free to design the driver of a parallel implementation as needed for the particular application
of interest. This section introduces one concept that has been employed successfully within many
hp3D problem implementations. Generally, we distinguish between two diﬀerent modes of operation
(resp. program execution):

• Interactive mode:

A user-interactive job passes only the required arguments for initiating the program (primarily
the geometry and physics input ﬁles) during the program call. Mesh reﬁnements, adaptive
solution process, visualization and I/O are all controlled interactively by the user input. This
mode is suitable for sequential (single MPI process) computation or parallel computation
with a moderate number of MPI processes executed on a single compute node. Usually, the
interactive mode is the primary mode of execution during code development and debugging.

• Production mode:

A production job passes all of the program arguments needed for execution during the program
call. Pre-processing, mesh reﬁnements, adaptive solutions, and post-processing operations are
all driven by a pre-deﬁned job script. This mode is suitable for any number of MPI processes
and MPI/OpenMP conﬁguration, including large-scale execution on many compute nodes.

As an example, we refer to the driver ﬁle main.F90 of the Galerkin Poisson implementation
in problems/POISSON/GALERKIN. In this driver implementation, a program argument “-job JOB”
controls whether the program is executed in the interactive mode (JOB.eq.0) or production mode
(JOB.ne.0).
In production mode, the value of JOB may then deﬁne which one of a number of
pre-deﬁned job scripts should be executed.

The remainder of this section is concerned with the implementation of the so-called master–
worker paradigm for the interactive mode of hp3D applications in the parallel distributed-memory
setting.

Interactive mode in the sequential setting. First, we introduce a typical interactive I/O
screen employed by hp3D’s subroutines (e.g., see problems/POISSON/GALERKIN/main.F90):

!..user interface displaying menu in a loop

idec = 1

do while(idec /= 0)

27

! ...print user options

call print_menu

! ...read user input

read(*,*) idec

! ...act on user input

select case(idec)

case(0) ; exit

!

...call a routine handling the user input

call default ; exec_case(idec)

end select

enddo

Listing 5.10: Interactive mode in shared-memory execution.

For context, a basic user menu could include the following user options:

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

QUIT....................................0

---- Visualization ----

HP3D graphics (graphb)..................1

HP3D graphics (graphg)..................2

Paraview................................3

---- DataStructure ----

Display arrays (interactive)...........10

Display DataStructure info.............11

---- Refinements ----

Single uniform h-refinement............20

Single uniform p-refinement............21

Refine a single element................22

---- Solvers ----

MUMPS..................................30

Pardiso................................31

Frontal................................32

PETSc..................................33

---- Error / Residual ----

Compute exact error....................40

Compute residual.......................41

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

28

Listing 5.11: A basic interactive-mode user menu.

Similar I/O screens are utilized in various system debugging routines the user has access to
(e.g., displaying nodal information via the interactive routine src/datstrs/result). The mecha-
nism for executing these routines in the distributed-memory setting is similar to the one described
in the context of the application driver main discussed here.

Interactive mode in the parallel setting. As a general rule, the user always interacts with
one distinct MPI process at a time. This MPI process—called the master —is the ROOT process.
All other MPI ranks are referred to as worker processes.
In this master–worker paradigm, user
inputs are handled by the master process who communicates the input to the worker processes. For
that reason, the parallel driver that handles the user interaction is split into two driver routines:
master_main and worker_main.

if (JOB .ne. 0) then

! ...execute production mode

call exec_job

else

! ...execute interactive mode

select case(RANK)

case(ROOT) ; call master_main

case default ; call worker_main

end select

endif

Listing 5.12: Splitting master and worker execution paths.

Similar to the sequential case, the master_main—executed only by the master process—
displays an interactive user menu and waits for the user input. Once the user input is read, the
master broadcasts the user input to the workers. Consequently, the worker_main routine—executed
by all worker processes—primarily consists of a receiving broadcast (from ROOT) followed by exe-
cuting the user command:

idec=1

do while(idec /= 0)

! ...waiting for the master to broadcast user input

call MPI_BCAST(idec,1,MPI_INTEGER,ROOT,MPI_COMM_WORLD, ierr)

! ...act on user input

select case(idec)

case(0) ; exit

!

...call a routine handling the user input

call default ; exec_case(idec)

29

end select

enddo

Listing 5.13: Interactive mode in distributed-memory execution.

In essence, this summarizes how interactive user inputs are typically handled in parallel hp3D
applications. The master–worker paradigm can, in principle, be executed for an arbitrary number
of processes. Primarily, though, the interactive mode serves the user to check intermediate results
of the computation interactively during code development and debugging.

Remark 5.2. In some circumstances, the user may want to interact directly with one of the worker
processes. For instance, when the user wants to display information that the master process does
not have access to, such as geometry degrees of freedom outside of its subdomain. In this case, the
master can prompt a user input for choosing a particular worker rank to interact with rather than
relaying all user commands to the worker by MPI communication.

!..ask the user which MPI process should display information

if (RANK .eq. ROOT)

write(*,*) 'Select processor RANK: '; read(*,*) r

endif

call MPI_BCAST(r,1,MPI_INTEGER,ROOT,MPI_COMM_WORLD, ierr)

!..open interactive data structure routine from worker process with rank r

if (r .eq. RANK) call result

!..wait for the interactive worker routine to finish

call MPI_BARRIER(MPI_COMM_WORLD, ierr)

Listing 5.14: Initiating an interactive worker routine by a master broadcast.

5.3

Load Balancing

The basic algorithm for an adaptive solution with dynamic load balancing is as follows:

1. Initialize mesh (not distributed)

2. (Re-)distribute the mesh (distr_mesh routine from par_mesh module)

3. Solve

4. Estimate the error (end if solution is suﬃciently accurate)

5. Mark elements for reﬁnement

6. Reﬁne the mesh (hp)

7. Optionally, repartition the mesh for load balancing purposes (continue with Step 2); otherwise,

continue with Step 3.

30

In Step 6 (reﬁnement), hp3D implements subdomain inheritance when breaking nodes, i.e. the
subdomain value subd of a newly created node (h-reﬁnement) is inherited from the father node.
This may lead to load imbalance. The hp3D interface to the load balancing library Zoltan is pro-
vided in the module zoltan_wrapper. This module includes several of the above mentioned func-
tionalities to the user: for example, zoltan_w_eval evaluates the quality of the current mesh, and
zoltan_w_set_lb(LB) sets an internal parameter (ZOLTAN_LB) determining the load balancing strat-
egy. The mesh repartitioning begins with a call to the routine zoltan_w_partition(Mdle_subd)
that returns, for each active element (i.e., active middle node) in the mesh, the newly assigned
subdomain for the repartitioned mesh. Having determined a new partition for the mesh, the data
migration step follows. This functionality is implemented in the distr_mesh routine of the module
par_mesh.

Routines provided by the zoltan_wrapper module:

• zoltan_w_eval

Evaluates the quality of the current mesh partitioning.

• zoltan_w_set_lb(LB)

Sets global variable ZOLTAN_LB to deﬁne partitioning algorithm.
ZOLTAN_LB ∈ {BLOCK, RANDOM, RCB, RIB, HSFC, GRAPH}.

• zoltan_w_partition(Mdle_subd(1:NRELES))

Returns new partitioning (subdomain number for each middle node) for the active mesh.

Each one of the partitioning algorithms is executed through the Zoltan library, however relies
on partitioners from other external libraries (e.g. ParMETIS); the list includes both geometry-based
and graph-based algorithms. For more information, we refer to Zoltan’s documentation [9].

If during repartitioning element ownership has changed to a new subdomain,
Data migration.
then the old owner and new owner of the element may exchange DOF data for each element node
if requested by the user setting the global variable EXCHANGE_DOF = .true. in module par_mesh:

• For each element node, the old owner

1. packs nodal data, i.e., H 1-, H(curl)-, H(div)-, and L2-DOFs, into a buﬀer array;
2. sends nodal data to the new owner (point-to-point communication).

• For each element node, the new owner

1. allocates nodal DOFs;
2. receives nodal data from the old owner (point-to-point communication);
3. unpacks nodal data from buﬀer array into H 1-, H(curl)-, H(div)-, and L2-DOFs.

31

Exchanging DOFs in this way is communication-intensive and should be deactivated by the

user if not needed for the application. By default, EXCHANGE_DOF = .true..

Chapter A

Model Problems

A.1

Poisson Problems

For the ﬁrst model problem implementation, we consider the Poisson problem with inhomogeneous
Dirichlet BC:

−∇ · ∇u = f

in Ω ,

u = u0

on Γ .

Classical variational formulation:






u ∈ H 1(Ω) : u = u0 on Γ ,

(∇u, ∇v) = (f, v) ,

v ∈ H 1(Ω) : v = 0 on Γ .

A.1.1 Galerkin implementation

The Galerkin FE implementation of the variational problem is located in the application directory
problems/POISSON/GALERKIN. In the remainder of this section, ﬁle paths may be given as relative
paths within the application directory.

Input ﬁles:

• control/control: sets global control variables, e.g.

– NEXACT ∈ { 0,1 }: indicates whether the exact solution is known.
– EXGEOM ∈ { 0,1 }: indicates whether isoparametric or exact-geometry elements are used.

• input/physics: sets initially allocated nodes and physics variables.

32

100000

1

MAXNODS, nodes anticipated

NR_PHYSA, physics attributes

field

contin 1

H1 variable

Listing A.1: POISSON/GALERKIN/input/physics input ﬁle.

– The value of MAXNODS does not have to be precise; if more nodes are needed, the code
allocates them on-the-ﬂy. However, it is recommended for eﬃciency that the code does
not reallocate, as well as not selecting MAXNODS much larger than needed.

– NR_PHYSA=1 speciﬁes that one physics variable is declared.
– ‘field contin 1’ speciﬁes “nickname, approximation space, number of components” of
a variable. The approximation spaces are: H 1 – contin, H(curl) – tangen, H(div) –
normal, L2 – discon.

– For this Galerkin FE formulation, one H 1 variable is needed.

• geometries/hexa_orient0: deﬁnes the initial geometry mesh (a cube).

Next, we take a look at the required routines that must be provided by the user:

• set_initial_mesh:

for each initial mesh element, this routine sets

– the supported physics variables;
– the initial polynomial order of approximation;
– the boundary condition ﬂags for element faces on the boundary.

!..loop over initial mesh elements

do iel=1,NRELIS

! ...1. set physics

ELEMS(iel)%nrphysics = 1

allocate(ELEMS(iel)%physics(1))

ELEMS(iel)%physics(1) ='field'

! ...2. set initial order of approximation

select case(ELEMS(iel)%type)

case('tetr'); Nelem_order(iel) = 1*IP

case('pyra'); Nelem_order(iel) = 1*IP

case('pris'); Nelem_order(iel) = 11*IP

case('bric'); Nelem_order(iel) = 111*IP

end select

33

! ...3. set BC flags: 0 - no BC ; 1 - Dirichlet; 2-9 Custom BCs

ibc(1:6,1) = 0

do ifc=1,nface(ELEMS(iel)%type) ! loop through element faces

neig = ELEMS(iel)%neig(ifc)

select case(neig)

case(0); ibc(ifc,1) = 1

end select

enddo

! ...allocate BC flags (one per component),

!

and encode face BCs into a single BC flag

allocate(ELEMS(iel)%bcond(1))

call encodg(ibc(1:6,1),10,6, ELEMS(iel)%bcond(1))

enddo

• dirichlet:

Listing A.2: POISSON/GALERKIN/set_initial_mesh routine.

– User-provided routine required by the system routine update_Ddof which computes the
Dirichlet DOFs for element nodes (vertices, edges, faces) with a non-homogeneous Dirich-
let BCs.

– update_Ddof interpolates H 1, H(curl), H(div) Dirichlet data using projection-based in-

terpolation.1

– Required only if non-homogeneous Dirichlet BCs were set by the user in set_initial_mesh.

! routine dirichlet: returns Dirichlet data at a point

!

!

!

!

!

!

in:

Mdle

- middle node number

X

Icase

- a point in physical space

- node case (specifies supported variables)

out: ValH, DvalH - value of the H1 solution, 1st derivatives

ValE, DvalE - value of the H(curl) solution, 1st derivatives

ValV, DvalV - value of the H(div) solution, 1st derivatives

subroutine dirichlet(Mdle,X,Icase, ValH,DvalH,ValE,DvalE,ValV,DvalV)

Listing A.3: POISSON/GALERKIN/common/dirichlet routine.

• elem:

– User-provided routine that computes the element-local stiﬀness matrix and load vector.

1L. Demkowicz.

“Polynomial exact sequences and projection-based interpolation with application to Maxwell

equations”. In: Mixed Finite Elements, Compatibility Conditions, and Applications. Springer, 2008, pp. 101–158

34

– System module assembly provides global arrays for this purpose:

∗ ALOC(:,:)%array: Element-local stiﬀness matrix.
∗ BLOC(:)%array: Element-local load vector.
∗ These arrays are declared omp threadprivate for shared-memory parallel assembly

of diﬀerent element matrices with OpenMP threading.

elem is called during assembly for each middle node Mdle in the active mesh.

Remark A.1. Constrained approximation, modiﬁcation for Dirichlet nodes, and static con-
densation of element-interior (bubble) DOFs are automatically done afterwards by the system
routine celem_system which provides the modiﬁed element matrices to the assembly procedure.

!..determine element type; number of vertices, edges, and faces

etype = NODES(Mdle)%type

nrv = nvert(etype); nre = nedge(etype); nrf = nface(etype)

!..determine order of approximation

call find_order(Mdle, norder)

!..determine edge and face orientations

call find_orient(Mdle, norient_edge,norient_face)

!..determine nodes coordinates

call nodcor(Mdle, xnod)

!..set quadrature points and weights

call set_3D_int(etype,norder,norient_face, nrint,xiloc,waloc)

! ....... element integrals:

!..loop over integration points

do l=1,nrint

! ...coordinates and weight of this integration point

xi(1:3)=xiloc(1:3,l); wa=waloc(l)

! ...H1 shape functions (for geometry)

call shape3DH(etype,xi,norder,norient_edge,norient_face, nrdofH,shapH,gradH)

! ...geometry map

call geom3D(Mdle,xi,xnod,shapH,gradH,nrdofH, x,dxdxi,dxidx,rjac,iflag)

! ...integration weight

35

weight = rjac*wa

! ...get the RHS

call getf(Mdle,x, fval)

! ...loop through H1 test functions

do k1=1,nrdofH

!

...Piola transformation: q → ˆq and ∇q → J −T ˆ∇ˆq

q = shapH(k1)

dq(1:3) = gradH(1,k1)*dxidx(1,1:3) + &

gradH(2,k1)*dxidx(2,1:3) + &

gradH(3,k1)*dxidx(3,1:3)

!

!

!

...accumulate for the load vector: (f, q)
b_loc(k1) = b_loc(k1) + q*fval*weight

...loop through H1 trial functions

do k2=1,nrdofH

...Piola transformation: p → ˆp and ∇p → J −T ˆ∇ˆp

p = shapH(k2)

dp(1:3) = gradH(1,k2)*dxidx(1,1:3) + &

gradH(2,k2)*dxidx(2,1:3) + &

gradH(3,k2)*dxidx(3,1:3)

!

...accumulate for the stiffness matrix: (∇p, ∇q)

a_loc(k1,k2) = a_loc(k1,k2) + weight*(dq(1)*dp(1)+dq(2)*dp(2)+dq(3)*dp(3))

enddo; enddo; enddo

Listing A.4: POISSON/GALERKIN/elem routine

This concludes the list of necessary input ﬁles and routines required for deﬁning the appli-
cation code from the library-perspective. However, the user is encouraged to take a look at the
remaining ﬁles within the POISSON/GALERKIN directory which include the driver main and a vari-
ety of auxiliary ﬁles. In a future version of the user manual, we will include a discussion of these
auxiliary ﬁles as well.

36

A.1.2 DPG primal implementation

Broken primal DPG formulation:






(u, ˆσn) ∈ H 1(Ω) × H −1/2(Γh) : u = u0 on Γ ,

(∇u, ∇v) − (cid:104)ˆσn, v(cid:105)Γh = (f, v) ,

v ∈ H 1(Ωh) .

Compared to the Galerkin FE implementation, the primal DPG implementation mostly diﬀers
in the elem routine. In practice, the DPG method is implemented in its mixed form [7] but the
extra unknown—the Riesz representation of the residual—is statically condensed on the element
level (see Appendix C).

The implementation is provided in problems/POISSON/PRIMAL_DPG.
Compared to the input ﬁles for the Galerkin implementation, the only change is in the physics

ﬁle:

100000

2

MAXNODS, nodes anticipated

NR_PHYSA, physics attributes

field

contin 1

H1 variable

trace

normal 1

H(div) variable

Listing A.5: POISSON/PRIMAL_DPG/input/physics input ﬁle.

We now have speciﬁed two physics unknowns—u and ˆσn, i.e. NR_PHYSA=2. The additional trace
unknown ˆσn is declared as an H(div) variable in the physics ﬁle; the normal trace ˆσn must later
be speciﬁed as such by setting PHYSAi(2)=.true. (e.g. in the main driver).

The elem routine for DPG formulations can be structured into three distinct steps:

1. Element integration
• Stiﬀness: B
• Load: l
• Gram matrix: G

2. Boundary integration
• Stiﬀness: ˆB

3. Constructing DPG linear system
• Dense linear algebra
• Statically condensed system

stored in ALOC, BLOC

Element-local system.

37

(H,V)(V,H)(V,V)ALOC(H,H) (V)BLOC (H)Recall the statically condensed system

Auxiliary local variables:

(cf. Appendix C):






B∗G−1B B∗G−1 ˆB

ˆB∗G−1B ˆB∗G−1 ˆB











uh

ˆuh






 =




B∗G−1l

ˆB∗G−1l






• stiff_HH ← B
• stiff_HV ← ˆB
• bload_H ← l
• stiff_ALL ←

(cid:104)
B | ˆB | l

(cid:105)

In the elem routine, these steps are implemented as follows:

1. Preliminary set up:

!..allocate auxiliary matrices

allocate(gramP(NrTest*(NrTest+1)/2))

allocate(stiff_HH(NrTest,NrdofH))

allocate(stiff_HV(NrTest,NrdofVi))

allocate(bload_H(NrTest))

!..determine element type; number of vertices, edges, and faces

etype = NODES(Mdle)%type

nrv = nvert(etype); nre = nedge(etype); nrf = nface(etype)

!..determine order of approximation (element integrals)

call find_order(Mdle, norder)

!..determine enriched order of approximation (hexa)

nordP = NODES(Mdle)%order+NORD_ADD*111

!..determine edge and face orientations

call find_orient(Mdle, norient_edge,norient_face)

!..determine nodes coordinates

call nodcor(Mdle, xnod)

! ....... element integrals

Listing A.6: POISSON/PRIMAL_DPG/elem: preliminary set up

2. Element integration:

!..use the enriched order to set the quadrature

INTEGRATION = NORD_ADD ! ∆p ∈ {1, 2, . . .}
call set_3D_int_DPG(etype,norder,norient_face, nrint,xiloc,waloc)

!..loop over integration points

do l=1,nrint

! ...coordinates and weight of this integration point

38

xi(1:3)=xiloc(1:3,l); wa=waloc(l)

! ...H1 shape functions (for geometry)

call shape3DH(etype,xi,norder,norient_edge,norient_face, nrdofH,shapH,gradH)

! ...discontinuous H1 shape functions

call shape3HH(etype,xi,nordP, nrdof,shapHH,gradHH)

! ...geometry map

call geom3D(Mdle,xi,xnod,shapH,gradH,nrdofH, x,dxdxi,dxidx,rjac,iflag)

! ...integration weight

weight = rjac*wa

! ...get the RHS

call getf(Mdle,x, fval)

! ...1st loop through enriched H1 test functions

do k1=1,nrdofHH

!

...Piola transformation

v = shapHH(k1)

dv(1:3) = gradHH(1,k1)*dxidx(1,1:3) + &

gradHH(2,k1)*dxidx(2,1:3) + &

gradHH(3,k1)*dxidx(3,1:3)

!

!

!

!

!

!

!

!

!

...accumulate load: (f, v)

bload_H(k1) = bload_H(k1) + fval*v*weight

...loop through H1 trial functions

do k2=1,nrdofH

...Piola transformation

dp(1:3) = gradH(1,k2)*dxidx(1,1:3) + &

gradH(2,k2)*dxidx(2,1:3) + &

gradH(3,k2)*dxidx(3,1:3)

...accumulate stiffness: (∇u, ∇hv)

stiff_HH(k1,k2) = stiff_HH(k1,k2) + weight*(dv(1)*dp(1)+dv(2)*dp(2)+dv(3)*dp(3))

enddo

...2nd loop through enriched H1 test functions for Gram matrix

do k2=k1,nrdofHH

...Piola transformation

q = shapHH(k2)

dq(1:3) = gradHH(1,k2)*dxidx(1,1:3) + &

gradHH(2,k2)*dxidx(2,1:3) + &

39

!

!

!

gradHH(3,k2)*dxidx(3,1:3)

...determine index in triangular packed format

k = (k2-1)*k2/2+k1

...accumulate Gram with test inner product: (v, v)V := (v, v) + (∇hv, ∇hv)

aux = q*v + (dq(1)*dv(1) + dq(2)*dv(2) + dq(3)*dv(3))

gramP(k) = gramP(k) + aux*weight

enddo; enddo; enddo

Listing A.7: POISSON/PRIMAL_DPG/elem: element integration

3. Boundary integration:

!..determine order of approximation (boundary integrals)

norderi(1:nre+nrf) = norder(1:nre+nrf)

norderi(nre+nrf+1) = 111

!..loop through element faces

do ifc=1,nrf

! ...sign factor to determine the outward normal unit vector

nsign = nsign_param(etype,ifc)

! ...face type ('tria','quad')

ftype = face_type(etype,ifc)

! ...face order of approximation

call face_order(etype,ifc,norder, norderf)

! ...set 2D quadrature

INTEGRATION = NORD_ADD ! ∆p
call set_2D_int_DPG(ftype,norderf,norient_face(ifc), nrint,tloc,wtloc)

! ...loop through integration points

do l=1,nrint

!

!

!

...face coordinates

t(1:2) = tloc(1:2,l)

...face parametrization

call face_param(etype,ifc,t, xi,dxidt)

...determine discontinuous H1 shape functions

40

!

!

!

!

!

!

!

!

!

!

call shape3HH(etype,xi,nordP, nrdof,shapHH,gradHH)

...determine element H(div) shape functions (for fluxes), interfaces only (no bubbles)

call shape3DV(etype,xi,norderi,norient_face, nrdof,shapV,divV)

...determine element H1 shape functions (for geometry)

call shape3DH(etype,xi,norder,norient_edge,norient_face, nrdof,shapH,gradH)

...geometry map

call bgeom3D(Mdle,xi,xnod,shapH,gradH,nrdofH,dxidt,nsign, &

x,dxdxi,dxidx,rjac,dxdt,rn,bjac)

...integration weight

weight = bjac*wtloc(l)

...loop through enriched H1 test functions

do k1=1,nrdofHH

v = shapHH(k1)

...loop through H(div) trial functions

do k2=1,nrdofVi

...Piola transformation

s(1:3) = (dxdxi(1:3,1)*shapV(1,k2) + &

dxdxi(1:3,2)*shapV(2,k2) + &

dxdxi(1:3,3)*shapV(3,k2)) / rjac

...normal component

sn = s(1)*rn(1)+s(2)*rn(2)+s(3)*rn(3)

...accumulate stiffness: −(cid:104)σ · n, v(cid:105)Γh

stiff_HV(k1,k2) = stiff_HV(k1,k2) - sn*v*weight

enddo; enddo ! end loop through trial / test functions

enddo; enddo; ! end loop through integration points / faces

Listing A.8: POISSON/PRIMAL_DPG/elem: boundary integration.

4. Construction of DPG linear system:

!---------------------------------------------------------------------

! Construction of statically condensed DPG linear system

!---------------------------------------------------------------------

!..create auxiliary matrix for dense linear algebra

allocate(stiff_ALL(NrTest,NrTrial+1))

!..Total test/trial DOFs of the element

41

i = NrTest ; j1 = NrdofH ; j2 = NrdofVi

!..Copy stiffness and load into one matrix: stiff_ALL ← [B | ˆB | l ]

stiff_ALL(1:i,1:j1)

= stiff_HH(1:i,1:j1)

stiff_ALL(1:i,j1+1:j1+j2) = stiff_HV(1:i,1:j2)

stiff_ALL(1:i,j1+j2+1)

= bload_H(1:i)

deallocate(stiff_HH,stiff_HV)

!..A. Compute Cholesky factorization of Gram Matrix, G = UTU (= LLT)

call DPPTRF('U',NrTest,gramP,info)

!

!..B. Solve triangular system to obtain ˜B, i.e. solve (L ˜B =) UT ˜B = [B | l]
call DTPTRS('U','T','N',NrTest,NrTrial+1,gramP,stiff_ALL,NrTest,info)

allocate(raloc(NrTrial+1,NrTrial+1)); raloc = ZERO

!..C. Matrix multiply: BTG−1B (= ˜BT ˜B)

call DSYRK('U','T',NrTrial+1,NrTest,ZONE,stiff_ALL,NrTest,ZERO,raloc,NrTrial+1)

!..D. Fill lower triangular part of Hermitian matrix ˜BT ˜B

do i=1,NrTrial

raloc(i+1:NrTrial+1,i) = raloc(i,i+1:NrTrial+1)

enddo

!..raloc has now all blocks of the stiffness and load:

! raloc = ALOC(1,1) ALOC(1,2)
ALOC(2,2)
ALOC(2,1)
!

BLOC(1)

BLOC(2)

Listing A.9: POISSON/PRIMAL_DPG/elem: constructing DPG linear system.

A.2

Linear Elasticity Problems

Linear elasticity will be added in a future version of the user manual.

A.3 Maxwell Problems

For time-harmonic Maxwell problems, the solution is complex-valued; the hp3D library must there-
fore be compiled with preprocessing ﬂag COMPLEX=1.

42

• Linear time-harmonic Maxwell equations:

∇ × E + iωµH = 0

∇ × H − (iωε + σ)E = J imp

in Ω ,

in Ω ,

n × E = n × E0

on Γ .test

• Curl–curl formulation:

∇ × (µ−1∇ × E) − (ω2ε − iωσ)E = −iωJ imp

in Ω ,

n × E = n × E0

on Γ .test

• Classical variational formulation:






E ∈ H(curl, Ω) : n × E = n × E0 on Γ ,

(µ−1∇ × E, ∇ × F )Ω − ((ω2ε − iωσ)E, F )Ω = −iω(J imp, F )Ω ,

F ∈ H(curl, Ω) : n × F = 0 on Γ .

The formulation involves just one unknown E ∈ H(curl, Ω), deﬁned on the whole domain.

A.3.1 Galerkin implementation

The implementation is provided in problems/MAXWELL/GALERKIN.

A.3.2 DPG ultraweak implementation

Ultraweak Maxwell will be added in a future version of the user manual.

43

Chapter B

Applications

To provide further examples and give the reader an idea of the scope of computations hp3D is
suitable for, this chapter brieﬂy summarizes applications that have been implemented within the
current version of the hp3D ﬁnite element code.
If an implementation is available in the public
repository, we provide a ﬁle path for the interested user. References for further reading are provided
in all cases.

B.1

Optical Fiber Ampliﬁer

Fiber ampliﬁers are optical waveguides made of silica glass designed for power-scaling highly-
coherent laser light. At high optical intensities, undesired nonlinear eﬀects may negatively aﬀect
the beam quality. hp3D has been used to study such nonlinear eﬀects (e.g. the interplay between
the propagating electromagnetic ﬁelds and thermal eﬀects) for active gain ﬁber ampliﬁers based on
a ﬁnite element model of the time-harmonic Maxwell equations coupled with the heat equation [22,
28]. An implementation of this application is available in the problems/LASER/ directory.

In addition to the coupled multiphysics formulation, the application employs a high-order
discretization and anisotropic adaptive reﬁnements [20] for a hybrid mesh consisting of both pris-
matic and hexahedral elements with curvilinear geometry. The application also served as the ﬁrst
real testbed problem for large-scale computation with the MPI/OpenMP parallel hp3D code, suc-
cessfully scaling up to thousands of wavelengths and ∼1B degrees of freedom [19, 18]. The DPG FE
implementation of the Maxwell equations also features fast integration routines via sum factorization
for both hexahedral and prismatic elements [26, 1].

B.2

Adaptive Solution of High-Frequency Acoustic and Electromagnetic Scattering

Accurate and eﬃcient numerical simulations of wave propagation phenomena are very crucial in
numerous engineering and physics applications, such as non-destructive testing, plasma fusion,
modeling of meta-materials and biomedical and radar imaging. With traditional discretization
methods, these simulations are extremely challenging due to two major issues:
instability and
indeﬁniteness of the linear systems. Consequently, common cutting-edge elliptic solvers simply
break down. The DPG method overcomes both issues. As a non-standard minimum residual
method, it promises high accuracy, unconditional stability and deﬁnite linear systems.

This work implemented a novel multigrid solver within hp3D for linear systems arising from

44

DPG discretizations with a special focus in acoustic and electromagnetic wave propagation problems.
The construction is heavily based on the attractive properties of the DPG method, but also on well-
established theory of Schwarz domain decomposition and multigrid methods. As it is showcased
in [29, 31, 30], the method is stable and reliable, and it is suitable for adaptive hp-meshes. The
solver works hand-in-glove with the built-in DPG error indicator to drive adaptive mesh reﬁnements.
Integrating the iterative solver with the adaptive reﬁnement procedure enables eﬃcient and accurate
solutions to challenging problems in the high-frequency regime. A distributed parallel version of
the DPG multigrid solver is currently under development and will be discussed in a future version
of the user manual; the technology is also described detail in [21].

B.3

Linear Elasticity with Two Materials

Linear elasticity is a continuum model of the deformation and stresses of solids. The ﬁrst ﬁnite
element methods were developed to predict the eﬀects of loads on structures modeled with linear
elasticity and the study of linearly elastic materials has continued to be an important application
of ﬁnite element analysis to this day. A ﬁnite element method for linear elasticity is available in the
problems/HOSE directory.

The implementation employs distinct DPG methods (primal and ultraweak [23]) for two
separate subdomains in curvilinear body. The primal DPG method is used in a subdomain with
compressible material (steel) and the ultraweak DPG method is used in a subdomain with incom-
pressible material (rubber) [13]. The coupling between the two methods is naturally incorporated
through the common trace and ﬂux dofs on the element boundary. This application demonstrates
support in hp3D for subdomain-dependent local element assembly, which can be used to support
the solution of multiphysics problems.

B.4

Nonlinear Elasticity

Beyond linear elasticity problems, it is common in computational mechanics to model and simulate
nonlinear elasticity cases, e.g. to compute large deformations. In such cases, one needs to account
for a non-convex stored energy functional that must be minimized. A practical approach for solving
this minimization problem is to apply loads in incremental steps to ﬁnd successive local minima via
Newton–Raphson methods. Each iteration of the Newton–Raphson method applied to the system
of nonlinear equations solves a linear system that is derived from a variational formulation of the
linearized boundary value problem (see [25], Chapters 6 and 7).

The implementation of the nonlinear elasticity problem in hp3D assumes an isotropic hypere-
lastic material with dead load undergoing quasi-static deformation. Various variational formulations
of the linearized equations can be employed for this scenario and numerically solved in hp3D with
user-deﬁned convergence tolerances. The application directory, problems/NONLINEAR_ELASTICITY,
includes several implementations: the Bubnov–Galerkin discretization of the classical variational for-

45

mulation in GALERKIN; the DPG discretization of the broken primal formulation in DPG_PRIMAL; and
the DPG discretization of the broken ultraweak formulation in DPG_UW. The mathematical theory
and material constitutive models are detailed in [25]. A more thorough description of the imple-
mentation for this problem is presented in [21]. Results of multiple nonlinear elasticity examples
using hp3D are presented in [21, 27].

B.5

Time-Harmonic Applications in Linear Viscoelasticity

The linearized equations describing the dynamics of viscoelastic material behavior in the time-
harmonic regime are very similar to those of time-harmonic linear elasticity. The diﬀerence is that
the material properties themselves (as well as the relevant unknowns) are complex-valued. The
reason is that the (linearized) constitutive model describing the relationship between stress and
strain depends on the strain history, and is given by a Volterra-type convolution, which becomes a
product in the time-harmonic regime. This allows to model real-life experiments in dynamic me-
chanical analysis (DMA) as well as phenomena that involve low-amplitude vibrations of viscoelastic
material at diﬀerent frequencies.

An example where hp3D was used includes the faithful reproduction of a DMA experiment
involving a thin beam. Here support for complex variables is needed, as well as anisotropic p and
local anisotropic reﬁnements in h when the domain is discretized with hexahedra. The reason is that
adaptivity is localized in certain regions where most of the deformation gradient is concentrated
due to nontrivial clamping eﬀects [12]. Moreover, in an application involving form-wound stator
coils found in electric machinery, it is necessary to introduce a nontrivial “surface” forcing coming
from high-frequency Lorentz forces. This type of forcing is not typically supported. However, with
the proper variational formulation and boundary conditions, it is possible to use hp3D to solve this
problem, which also includes some of the features mentioned previously [11].

B.6

Acoustics of the Human Head

The problem was formulated and solved in the course of the dissertation of Paolo Gatto devoted to
the analysis of transmission of sound in the human head [16], see also [2, 15]. More speciﬁcally, it
studied the mechanism of transmitting exterior acoustic energy into the cochlea with the ear canal
blocked. The domain of interest included the human skull modeled with a spherical shell with an
inner structure representing the ear canal, a cochlea with the tympanic membrane, and a simpliﬁed
model of ossicles consisting of a single elastic rod connecting the ear drum with the oval window in
the cochlea. The skull was surrounded with an additional layer representing air, terminated with a
PML. The skull, ear canal, cochlea, ossicles and the three membranes were modeled with elasticity;
whereas the air, exterior and interior ear, the brain and the ﬂuid inside of the cochlea were modeled
with acoustics, a total of 16 subdomains whose geometry was represented within the GMP package.
The initial tetrahedral linear geometry/mesh was obtained using NETGEN and imported into GMP

46

where two important modiﬁcations were made. The skull and all membranes, represented initially
with zero thickness interfaces, were extruded into thin-walled structures, and the triangular mesh
representing the skull was extruded into two additional layers for air and PML. The extrusions
brought in both prismatic and pyramidal elements which motivated adding these elements to the
hp3D code.

In the elastic subdomains, the code supported the elastic displacement ﬁeld—a single H 1
In the acoustical subdomains, the code supported the pressure—a
ﬁeld with three components.
single-component H 1 ﬁeld. Note that the nodes located on acoustic/elastic interfaces supported
both variables. More precisely, in the external acoustic domain the acoustic variable represented
the scattered pressure, and in the internal acoustical domains the total pressure. The forcing came
from a plane wave impinging on the skull, resulting with an additional term on the exterior acous-
tics/elasticity interface. A standard variational formulation (see e.g. [8]) was used with couplings
between the elasticity and acoustic problems enforced weakly by incorporating interface integrals
in the variational formulation. The problem was discretized with the standard Galerkin method.
In the initial mesh, the three membranes were discretized with quintic elements to avoid locking,
with the rest of the domain covered with quadratic elements except for the PML subdomain where
higher order anisotropic elements were used. Original shape functions for H 1-conforming elements
of all shapes were developed in [17]. Finally, the problem was solved using h-adaptivity driven by
a problem-dependent, implicit a-posteriori error estimate developed in the course of the project.

B.7

Electromagnetic Radiation and Induced Heat Transfer in the Human Body

The project focused on estimating heating eﬀects in the human head from the absorption of elec-
tromagnetic (EM) waves emanating from a cell phone; it constituted (partially) the dissertation of
Kyungjoo Kim [24].

The transient heat equation was coupled with time-harmonic Maxwell equations; the standard
Galerkin method was used for both heat and Maxwell problem. The heat equation was discretized
with the implicit Euler method. At each time step, the new temperature ﬁeld was used to compute
new (temperature-dependent) material constants for the Maxwell equations. Due to disparate time
scales, the time-harmonic version of the Maxwell equations was employed. The resulting new electric
ﬁeld was then used to update the source in the heat equation. The iterations were continued until
a steady-state was reached.

The head was modeled with both simpliﬁed multilayer spherical geometry as well as a more
precise (homogeneous) head model imported from COMSOL. The application was the second prob-
lem driving the original development of the current version of the hp3D code. The Maxwell problem
was discretized with H(curl)-conforming tetrahedral and prismatic elements.

47

Chapter C

Implementation of the DPG Method

The hp3D library has been the main tool for the numerical studies with the Discontinuous Petrov–
Galerkin (DPG) Method with Optimal Test Functions [7]. Many of the recent applications imple-
mented in hp3D (see Appendix B) were discretized with the DPG method. DPG implementations
are straightforward to implement within the hp3D framework, and we brieﬂy review (in abstract
form) how the DPG linear system is constructed.

Examples of model problem implementations with DPG are also provided (e.g. Appendix A.1.2).

The DPG mixed formulation. Consider an abstract variational problem and its Petrov–Galerkin
(PG) discretization:






u ∈ U

b(u, v) = l(v)

v ∈ V

−→






uh ∈ Uh ⊂ U

b(uh, vh) = l(vh)

vh ∈ Vh ⊂ V

The Petrov–Galerkin discretization of the problem requires dim Uh = dim Vh. For U = V, the
choice of Uh = Vh leads to the Bubnov–Galerkin (BG) method. In the Petrov–Galerkin Method with
Optimal Test Functions, we embed the original problem into an equivalent mixed problem:






ψ ∈ V, u ∈ U

(ψ, v)V + b(u, v) = l(v) v ∈ V

−→

b(w, ψ)

= 0

w ∈ U






ψh ∈ Vh, uh ∈ Uh

(ψh, vh)V + b(uh, vh) = l(vh) vh ∈ Vh

b(wh, ψh)

= 0

wh ∈ Uh

The mixed formulation involves an additional unknown—ψ ∈ V—the Riesz representation of the
residual. On the continuous level ψ = 0 but its discretization ψh ∈ Vh is diﬀerent from zero. The test
inner product (·, ·)V enters directly the problem and aﬀects the corresponding FE solution uh. The
main motivation for solving the more expensive mixed problem comes from stability considerations:
the discrete spaces Uh, Vh need not longer be of the same dimension, and it is easier to satisfy

48

the discrete inf–sup condition by simply employing a larger discrete test space. We are solving
eﬀectively an overdetermined discrete system.

An alternative is to test with a larger space of broken test functions V (Ωh) ⊃ V where Ωh
denotes the decomposition of the domain into ﬁnite elements. Testing from a larger space of broken
test functions necessitates the introduction of an additional unknown—the trace ˆu—a Lagrange
multiplier. The modiﬁed problem looks as follows:





u ∈ U, ˆu ∈ ˆU

b(u, v) + (cid:104)ˆu, v(cid:105)Γh
(cid:125)
(cid:123)(cid:122)
(cid:124)
bmod((u,ˆu),v)

= l(v)

v ∈ V(Ωh) .

The new group variable includes the original unknown and the trace unknown. The corresponding
PG scheme with optimal test functions is known as the DPG method. The word discontinuous in
the name refers to the use of broken (discontinuous) test spaces.






u ∈ U, ˆu ∈ ˆU, ψ ∈ V(Ωh)

(ψ, v)V(Ωh) + b(u, v) + (cid:104)ˆu, v(cid:105)Γh = l(v)

v ∈ V(Ωh)

b(w, ψ)

(cid:104) ˆw, v(cid:105)Γh

= 0

= 0

w ∈ U

ˆw ∈ ˆU .

The main advantage of using the broken test space in context of the PG method with optimal
test functions is that the Gram matrix resulting from the discretization of the test inner product
(ψ, v)V becomes block-diagonal, and the additional variable ψ ∈ V (Ωh) can be statically condensed
on the element level. The whole “DPG magic” happens only in the element routine, the remaining
part of the code remains the same as for the standard Bubnov–Galerkin method. The mixed
problem,









G B ˆB

B∗

ˆB∗

0

0

0

0

















ψ

u

ˆu













=







l

0

0







,

becomes a standard Bubnov–Galerkin problem:






B∗G−1B B∗G−1 ˆB

ˆB∗G−1B∗

ˆB∗G−1 ˆB∗













u

ˆu


 =






B∗G−1l

ˆB∗G−1l




 .

Recall that the trace unknown ˆu is discretized in hp3D by using element-wise restrictions of

49

H 1-, H(curl)-, and H(div)-conforming elements. This is accomplished by ﬁrst deﬁning a standard
H 1, H(curl), or H(div) physics variable in the physics input ﬁle and then declaring it a trace via
the global ﬂag array PHYSAi(:). For more information on the traces, see Section 4.3.

Constructing the DPG linear system.
linear system can be constructed in the following way:

In practice, the local element matrices of the DPG

First, reduce the broken mixed formulation to a matrix equation:

• Let Uh = {ui}N

i=1, ˆUh = {ˆui} ˆN

i=1

, and Vr = {vi}M
i=1

(where M > N + ˆN ) denote bases for the

discrete trial space Uh × ˆUh and the enriched test space V r, respectively.

• Deﬁne the stiﬀness matrix for the modiﬁed bilinear form, the Gram matrix, and the load

vector:

Bij = b(uj, vi),

ˆBij = (cid:104)ˆuj, vi(cid:105), Gij = (vj, vi)V ,

li = l(vi).

• In matrix form, the problem can now be formulated in the following way:

Find the set of coeﬃcients (over ﬁeld F = R (or C))

w = [wi]N

i=1 ∈ FN ,

ˆw ∈ [ ˆwi]

i=1 ∈ F ˆN , and q = [qi]M
ˆN

i=1 ∈ FM

such that

satisfy

uh =

N
(cid:88)

i=1

wiui,

ˆuh =

ˆN
(cid:88)

i=1

ˆwiˆui, and Ψ =

M
(cid:88)

i=1

qivi









G B ˆB

B∗

ˆB∗

0

0

0

0

























Ψ

uh

ˆuh









.









l

0

0

=

Let B := [B ˆB]. Then,

B∗G−1B = B∗(LL∗)−1B = (L−1B)∗

˜B
(cid:122) (cid:125)(cid:124) (cid:123)
(L−1B) = ˜B∗ ˜B ,

B∗G−1l = (L−1B)∗(L−1l) = ˜B∗(L−1l) .

That is, the statically condensed DPG linear system is computed as follows:

1. Cholesky factorization: G = LL∗

2. Solve triangular system:

3. Matrix multiply:

[ ˜B |˜l ] = L−1[B | l ]
˜B∗[ ˜B |˜l ]

50

The elem routine concludes by storing this statically condensed system into the corresponding blocks
of the assembly arrays ALOC and BLOC (cf. 4.4).

Bibliography

[1] J. Badger, S. Henneking, and L. Demkowicz. “Sum factorization for fast integration of DPG

matrices on prismatic elements”. In: Finite Elem. Anal. Des. 172 (2020), p. 103385.

[2] L. Demkowicz et al. “Modeling of Bone Conduction of Sound in the Human Head Using hp

Finite Elements. I. Code Design and Veriﬁcation”. In: Comput. Methods Appl. Mech. Engrg.

200.21-22 (2011), pp. 1757–1773.

[3] L. Demkowicz. Computing with hp Finite Elements. I. One and Two Dimensional Elliptic and

Maxwell Problems. Chapman & Hall/CRC Press, Taylor and Francis, 2006.

[4] L. Demkowicz. Energy Spaces. Lecture notes; Oden Institute. 2018.

[5] L. Demkowicz. Mathematical theory of ﬁnite elements. Lecture notes; The University of Texas

at Austin. 2020.

[6] L. Demkowicz. “Polynomial exact sequences and projection-based interpolation with applica-

tion to Maxwell equations”. In: Mixed Finite Elements, Compatibility Conditions, and Appli-

cations. Springer, 2008, pp. 101–158.

[7] L. Demkowicz and J. Gopalakrishnan. “Discontinuous Petrov–Galerkin (DPG) method”. In:

Encyclopedia of Computational Mechanics Second Edition (2017), pp. 1–15.

[8] L. Demkowicz et al. Computing with hp Finite Elements. II. Frontiers: Three Dimensional

Elliptic and Maxwell Problems with Applications. Chapman & Hall/CRC, 2007.

[9] K. Devine et al. “Zoltan data management services for parallel dynamic applications”. In:

Comput. Sci. Eng. 4.2 (2002), pp. 90–97.

[10] W. Dörﬂer. “A convergent adaptive algorithm for Poisson’s equation”. In: SIAM J. Numer.

Anal. 33.3 (1996), pp. 1106–1124.

51

[11] F. Fuentes. “Various applications of discontinuous Petrov–Galerkin (DPG) ﬁnite element

methods”. PhD thesis. The University of Texas at Austin, 2018.

[12] F. Fuentes, L. Demkowicz, and A. Wilder. “Using a DPG method to validate DMA experi-

mental calibration of viscoelastic materials”. In: Comput. Math. Appl. 325 (2017), pp. 748–

765.

[13] F. Fuentes et al. “Coupled variational formulations of linear elasticity and the DPG method-

ology”. In: J. Comput. Phys. 348 (2017), pp. 715–731.

[14] F. Fuentes et al. “Orientation embedded high order shape functions for the exact sequence

elements of all shapes”. In: Comput. Math. Appl. 70.4 (2015), pp. 353–458.

[15] P. Gatto and L. Demkowicz. “Modeling of bone conduction of sound in the human head:

simulation results.” In: J. Comput. Acoust. 21.4 (2013), pp. 1–30.

[16] P. Gatto. “Modeling bone conduction of sound in the human head using hp ﬁnite elements”.

PhD thesis. The University of Texas at Austin, 2012.

[17] P. Gatto and L. Demkowicz. “Construction of H 1-Conforming Hierarchical Shape Functions

for Elements of All Shapes and Transﬁnite Interpolation”. In: Finite Elem. Anal. Des. 46

(2010), pp. 474–486.

[18] S. Henneking, J. Grosek, and L. Demkowicz. Parallel simulations of high-power optical ﬁber

ampliﬁers. Submitted to Lect. Notes Comput. Sci. Eng. (accepted). 2022.

[19] S. Henneking. “A scalable hp-adaptive ﬁnite element software with applications in ﬁber optics”.

PhD thesis. The University of Texas at Austin, 2021.

[20] S. Henneking and L. Demkowicz. “A numerical study of the pollution error and DPG adap-

tivity for long waveguide simulations”. In: Comput. Math. Appl. 95 (2021), pp. 85–100.

[21] S. Henneking and L. Demkowicz. Computing with hp Finite Elements. III. Parallel hp3D

Code. In preparation, 2022.

[22] S. Henneking, J. Grosek, and L. Demkowicz. “Model and computational advancements to full

vectorial Maxwell model for studying ﬁber ampliﬁers”. In: Comput. Math. Appl. 85 (2021),

pp. 30–41.

52

[23] B. Keith, F. Fuentes, and L. Demkowicz. “The DPG methodology applied to diﬀerent varia-

tional formulations of linear elasticity”. In: Comput. Methods Appl. Mech. Engrg. 309 (2016),

pp. 579–609.

[24] K. Kim. “Finite element modeling of electromagnetic radiation and induced heat transfer in

the human body”. PhD thesis. The University of Texas at Austin, 2013.

[25] J. D. Mora. “PolyDPG: A discontinuous Petrov–Galerkin methodology for polytopal meshes

with applications to elasticity”. PhD thesis. The University of Texas at Austin, 2020.

[26] J. D. Mora and L. Demkowicz. “Fast integration of DPG matrices based on sum factorization

for all the energy spaces”. In: Comput. Methods Appl. Math. 19.3 (2019), pp. 523–555.

[27] J. D. Mora and L. Demkowicz. “Polyhedral Discontinuous Petrov–Galerkin Finite Element

Method for Hyperelasticity”. In: In preparation (2022).

[28] S. Nagaraj et al. “A 3D DPG Maxwell approach to nonlinear Raman gain in ﬁber laser

ampliﬁers”. In: J. Comput. Phys. 2 (2019), p. 100002.

[29] S. Petrides. “Adaptive multilevel solvers for the discontinuous Petrov–Galerkin method with

an emphasis on high-frequency wave propagation problems”. PhD thesis. The University of

Texas at Austin, 2019.

[30] S. Petrides and L. Demkowicz. “An adaptive DPG method for high frequency time-harmonic

wave propagation problems”. In: Comput. Math. Appl. 74.8 (2017), pp. 1999–2017.

[31] S. Petrides and L. Demkowicz. “An adaptive multigrid solver for DPG methods with appli-

cations in linear acoustics and electromagnetics”. In: Comput. Math. Appl. 87 (2021), pp. 12–

26.

53

