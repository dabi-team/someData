De-biasing “bias" measurement

KRISTIAN LUM, Twitter Inc., USA
YUNFENG ZHANG, Twitter Inc., USA
AMANDA BOWER, Twitter Inc., USA

2
2
0
2

n
u
J

9
2

]
E
M

.
t
a
t
s
[

2
v
0
7
7
5
0
.
5
0
2
2
:
v
i
X
r
a

When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many

such groups–it is often called "biased." While much of the work in algorithmic fairness over the last several years has focused on

developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such

“bias," much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures

of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted

decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to

measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they

purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities

along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the

“double-corrected" variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model

performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical

optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased

estimators of group-wise model performance disparities indicate statistically significant differences, when accounting for statistical

bias in the estimator, the estimated between-group disparities are no longer statistically significant.

CCS Concepts: • Computing methodologies → Machine learning; Artificial intelligence.

ACM Reference Format:
Kristian Lum, Yunfeng Zhang, and Amanda Bower. 2022. De-biasing “bias" measurement. In 2022 ACM Conference on Fairness,
Accountability, and Transparency (FAccT ’22), June 21–24, 2022, Seoul, Republic of Korea. ACM, New York, NY, USA, 17 pages. https:
//doi.org/10.1145/3531146.3533105

1 INTRODUCTION

In the interest of developing models that behave equitably for different swaths of the population, identifying and

mitigating group-wise disparities in model performance–typically measured in terms of accuracy, false positive rates,

and so on–have become a central theme of creating “fair" models. Models that exhibit group-wise performance
disparities are often termed “biased"1–an overloaded term, which in this case, simply means that that model performance
is substantively different for some groups than others. Canonical examples of models expressing between-group

performance variability include the identification of racial disparities in false positive rates of recidivism prediction

models [5], race-gender disparities in accuracy rates in gender classification models [12], and racial disparities in police

allocation in predictive policing [37]. In each of these cases, model performance disparities were readily apparent simply

1To be clear, we will refer to this type of “bias” as “group-wise model performance disparities” or “performance disparities” for short throughout our
paper.

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the owner/author(s).

© 2022 Copyright held by the owner/author(s).
Manuscript submitted to ACM

1

 
 
 
 
 
 
FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Lum, et al.

Fig. 1. Each bar represents the accuracy of one of three hypothetical classification models for determining if text is spam or not
broken down by age of the author of the text (left) or language of the text (right). It is relatively difficult to determine the degree of
group-wise disparities for each model by only a visual examination.

by inspecting the model performance for each group separately and judging that the magnitude of difference between

the groups was unacceptably large. When there are a small number of groups, each composed of many observations–as

was the case in all of the above examples–this method of measuring and evaluating group-wise model performance

disparities is possible.

When the number of groups across which performance disparities is being evaluated is large, it is difficult for a human

to make judgments about the degree of disparities present simply by inspecting the model performance for all groups.

Human-understandable measurement of model performance disparities and uncertainty quantification thereof are

important if decision-makers are to engage in a well-informed consideration of the trade-offs between the relative merits

of a variety of candidate models, determine whether a particular model should be mitigated for performance disparities,

and decide whether any model should be deployed at all. To illustrate, suppose a machine learning practitioner needs to

select from among three candidate binary classification models that use text as an input. Figure 1 shows model accuracy
broken down by the language of the text and age of the text’s author for three hypothetical models.2 The height of each
bar represents each model’s accuracy within the corresponding group. Which model exhibits the lowest group-wise

performance disparities by age? Do any of them exhibit acceptable levels of model performance disparities with respect

to age? Given a model, are there more disparities with respect to age or language? How does the group-wise performance

disparities between models compare? While it is difficult to answer these questions just with three candidate models, in

real-world settings, the situation can be drastically more difficult. Machine learning practitioners may be considering

hundreds or thousands of models given the plethora of hyperparameter choices when training neural networks and

other models. Answering these questions requires well-designed, summary metrics that transform high-dimensional

outputs of model performance on each group into lower-dimensional summaries while retaining the salient information.

We call these “meta-metrics” since they are a metric on the vector whose coordinates contain model performance

metrics on each group. We use this term to differentiate them from the “base metric", which is the performance metric

applied to each group separately .

Meta-metrics have received little attention in the algorithmic fairness literature despite their ubiquity and importance.

Specifically, there have been no investigations of whether the metrics that are used to measure group-wise model

performance disparities are themselves biased in the statistical estimation sense of the word. Uninterpretable, statistically

biased meta-metrics can lead to costly outcomes. For example, suppose we obtain measurements that incorrectly suggest

2Code to produce this figure, all simulations, and analysis in this paper is available at https://github.com/twitter-research/double-corrected-variance-
estimator

2

De-biasing “bias" measurement

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

a model has a large degree of group-wise model performance disparities and thus requires mitigation. If we retrain the

model with additional “fairness” constraints, we will likely end up lowering the utility of the model for every group

involved unnecessarily. On the other hand, if the measurement is not easily understood, e.g. on a small scale that does not

lend itself to distinguishing different scenarios, performance disparities may go undetected. Unfortunately, as we show

in Section 4, many of the meta-metrics currently in use are statistically biased representations of the underlying quantity

they purport to represent. The statistical bias arises because they fail to account for statistical uncertainty/sampling

variance in the base metrics across which they are summarizing information. In particular, they are statistically biased

upwards, exaggerating the extent of performance disparities, potentially leading to mitigation in cases where model

performance is negligible or even identical across groups. The statistical bias is particularly large when some sub-groups

have large sampling variance, e.g. in situations where some groups consist of few individuals. This situation is common:

when evaluating model performance disparities across the intersection of many group variables, as in [32], even large

data sets can have intersectional groups that consist of very few observations. For example, in the Adult income
dataset3– a commonly used benchmark dataset for evaluating model performance disparities– some intersectional
groups defined by race and gender contain fewer than 50 individuals. Many intersectional groups defined by race and

10 year age bins contain only one individual per group.

Furthermore, typical approaches to quantifying statistical uncertainty of existing meta-metrics, e.g. bootstrapping,

can fail to cover the true degree of between-group disparities at a much higher than expected rate because they capture

uncertainty about a quantity that is itself a statistically biased estimator of the true underlying value. This failure is

more pronounced when the statistical bias is large. Therefore, using existing tooling on problems with many groups,

some of which are small, results in either measurements that are so statistically biased as to paint a misleading picture of

between-group disparities, or using heuristics to remove small-sized groups, thereby potentially further marginalizing

already marginalized groups by excluding them from group-wise model performance analysis.

In this paper we make several contributions. First, we demonstrate that commonly used meta-metrics depict a

statistically biased representation of the true degree of model performance disparities. As a remedy, we introduce a

corrected estimator of between-group performance disparities. Second, we illustrate the dangers of a naïve approach to

uncertainty quantification via bootstrapping for meta-metrics. In short, we show that bootstrapping induces additional

sampling variability that is unaccounted for by a simple statistical bias-corrected estimator. This leads to a situation

where even the original corrected estimator (when applied to bootstrap samples) is statistically upward biased and

bootstrap intervals have very poor coverage. Third, using the case of binary classification as an example– the most

common paradigm in algorithmic fairness– we derive a novel, conceptually simple, and easily implementable double-

corrected estimator. This estimator accounts not only for the sampling variance inherent to the original base metric but

also the sampling variance induced by the bootstrap procedure itself. We demonstrate that this estimator has coverage

near the nominal level in simulated examples. Finally, we show using a real data example that using naïve methods for

quantifying model performance disparities can indicate substantial disparities in model performance whereas using our

estimator that appropriately accounts for statistical uncertainty in the base metrics leads to the opposite conclusion.

Taken in whole, this work illustrates the delicate and difficult nature of rigorously measuring and assessing uncertainty

about the between-group model performance disparities that have come to characterize the field of algorithmic fairness.

The paper proceeds as follows. In Section 2, we survey related work on measuring model performance disparities. In

Section 3, we introduce notation and in Section 4 we show mathematically and through simulation that all meta-metrics

3Downloaded from the UCI Machine Learning Repository at https://archive.ics.uci.edu/ml/datasets/adult.

3

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Lum, et al.

we have identified suffer from statistical bias. In Section 5, we develop a statistically unbiased estimator for summarizing

group performance disparities with associated uncertainty quantification, along the way illustrating the pitfalls with

naïve application of meta-metrics and bootstrapping. In Section 6, we present our example using the Adult Income

dataset. In Section 7, we close with recommendations and pointers to corners of the statistics literature that address

very similar problems that will likely prove useful to this task.

2 RELATED WORK

Many group fairness papers assume that there are only two demographic groups of interest–a privileged group and a

non-privileged group–in order to make theory feasible or presentation digestible [6, 15, 20, 29, 36, 49, 51]. Although

assessing binary group disparities in this case is straight forward, e.g. using the absolute value of the difference in false

positive rates between two groups as a measure of unfairness [8], this assumption is not appropriate in many real-world

settings. While there are some papers that introduce theory or methods that can handle arbitrarily many demographic

groups, many of these papers do not synthesize group disparities, and some even turn to binary demographic groups

when validating their approaches [18, 24, 26, 38, 53]. Furthermore, empirical work typically shows bar graphs or tables to

illustrate model performance disparities over multiple demographic groups. Although this granularity of information is

undoubtedly useful and important on its own, these works do not provide a way to quickly synthesize group disparities

of model performance across many potential models [1, 12, 30, 52].

Outside of the typical group fairness literature, there have been several other threads that address model performance

disparities. While individual fairness approaches [17] provide a way to measure disparities in a group-free way–

seemingly alleviating the problem we set out to solve–oftentimes to practically validate these individual fairness

approaches, group-wise disparities are measured [11, 50] leading us back to our initial problem. Second, statistical

dependence between demographic groups and model predictions is a common way to measure group-wise model

performance disparities [33, 39, 45], e.g. correlation. However, the degree of dependence does not tell us the degree of

group disparities in model performance, so these approaches in general are inappropriate for our purposes. In addition,

we consider categorical demographic groups, not continuous demographic groups, so many of these approaches are not

even applicable for the setting we study. Third, economic inequality metrics have been proposed for summarizing model

disparities at the individual level without demographic information [7, 35, 40, 41, 44]. While undoubtedly important,

these demographic-free measures of inequality in the allocation of benefits from a model address a different problem.

Demographic disparities are in and of themselves an important component of understanding the differential impacts of

a model.

There are a small number of papers that propose approaches for practically measuring disparities when there are

many demographic groups [2, 21, 22, 31, 32]. All of these papers synthesize group disparities by either reporting the

the maximal deviation from average performance (sometimes normalized by subgroup size [32]), the mean absolute

deviation from average performance [31], or some measure of the disparity between the maximum and minimum of the

base metrics.

Furthermore, there is a growing collection of open source software tools for measuring and mitigating group-wise

model performance disparities of machine learning models such as IBM’s AI Fairness 360 [9], Microsoft’s FairLearn

[10], Google’s TensorFlow Fairness Indicators [23], LinkedIn’s LiFT [46], FairTest [45], Aequitas [42], FAT Forensics

[43], FairVis [13], FairSight [3], Themis [4], Silva [49], and Responsibly [28]. Most of these tools can only handle

binary demographic groups. For those that can handle arbitrarily many groups, they tend to use simple meta-metrics

without consideration of statistical bias or cannot aggregate group disparities. IBM’s AI Fairness 360 tool [9] synthesizes

4

De-biasing “bias" measurement

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Meta-Metric Name
max-min difference
max-min ratio
max absolute difference
mean absolute deviation
variance
generalized entropy index (𝛼 ≠ 0, 1)

Formula
𝑀mm-diff (𝑌 ) = max𝑘 𝑌𝑘 − min𝑘 𝑌𝑘
𝑀mm-ratio (𝑌 ) = max𝑘 𝑌𝑘
min𝑘 𝑌𝑘
𝑀max-abs-diff (𝑌 ) = max𝑘 |𝑌𝑘 − ¯𝑌 |
𝑀mad (𝑌 ) = 1
𝐾
𝑀var (𝑌 ) = 1
𝐾−1
(cid:205)𝐾

(cid:205)𝑘 |𝑌𝑘 − ¯𝑌 |
(cid:205)𝑘 (𝑌𝑘 − ¯𝑌 )2
(cid:17)𝛼
(cid:105)
(cid:104)(cid:16) 𝑌𝑘
𝑌

1
𝐾𝛼 (𝛼−1)

− 1

𝑘=1

Used by

Type
Extremum [10]
Extremum [10, 22]
Extremum [2, 32]
Variability
Variability

[31]

Variability

[9, 44, 46]

Table 1. Meta-metrics for summarizing between-group variability in model performance.

between-group disparities via the generalized entropy index, although the majority of their group fairness metrics can

only handle two demographic groups. Although Google TensorFlow Fairness Indicators tool can handle arbitrarily many

demographic groups, users are required to determine the disparities through bar charts or tabular data that contains the

measurements on each of the groups. Their approach is very similar to the hypothetical example depicted in Figure 1.

Microsoft’s FairLearn tool [10] can handle arbitrarily many groups where group disparities are measured as a function

of the maximum and minimum of model performance over all the groups. LinkedIn’s LiFT tool [46] measures variability

in group disparities through economic inequality metrics like the generalized entropy index. Neither FairLearn nor

LiFT quantifies the uncertainty of the point estimates of these group disparity measures.

Table 1 summarizes the meta-metrics we have identified in both the academic literature and in open-source tools for

group-wise model performance disparities measurement and mitigation. We include one additional meta-metric that

we have not found used to summarize disparities in the literature or open source tools: the variance. Our reason for

including the variance will quickly become apparent.

3 NOTATION

Suppose we have 𝑛 individuals, where associated with each individual is a covariate vector 𝑥 and an outcome variable
𝑤. We use model 𝑓 (𝑥) to predict 𝑤. These individuals fall into 𝐾 different groups, which we index by 𝑘 = 1, ..., 𝐾, each
consisting of 𝑛𝑘 individuals. Because we will not be dealing further with observations at the individual level, we define
𝑤𝑘 and 𝑥𝑘 to be the set of outcomes and covariate vectors, respectively, for all individuals who fall in group 𝑘, and
overload 𝑓 (𝑥𝑘 ) to mean 𝑓 applied to each covariate vector in 𝑥𝑘 individually. In the fairness literature, the groups usually
denote different demographic groups or other salient social characteristics across which one might desire comparable

model performance. Common choices include race, gender, location, and intersections thereof. To each group, we

apply a base model performance metric, 𝑚, that summarizes how well the model performs for individuals within
that group. We define 𝑌𝑘 := 𝑚(𝑤𝑘, 𝑓 (𝑥𝑘 )) to be the observed model performance for group 𝑘. Unlike in conventional
modeling settings where 𝑌 typically represents an observation for one individual, 𝑌𝑘 represents a summary statistic of
model performance across all 𝑛𝑘 individuals who fall in group 𝑘. 𝑌𝑘 is itself a statistic with “true" model performance
is the sampling variance of the estimator 𝑌𝑘 , not the variance of the
𝜇𝑘 = E[𝑌𝑘 ] and standard error 𝜎𝑘 . To be clear, 𝜎 2
𝑘
data itself. Finally, let 𝑌 := [𝑌1, . . . , 𝑌𝑘 ]𝑇 and 𝜇 = [𝜇1, . . . , 𝜇𝑘 ]𝑇 be 𝑘-dimensional vectors, with ¯𝑌 and ¯𝜇 the mean of the
vectors, respectively.

As in much of the algorithmic fairness literature, our examples focus on binary classification. In this setting, common

choices of the base metric, 𝑚, are the accuracy, false positive rate, positive predictive value, etc. In all examples presented,

we simulate from the following model

5

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

𝑍𝑘 ∼ Binomial(𝑛𝑘, 𝜇𝑘 )

𝑌𝑘 =

𝑍𝑘
𝑛𝑘

Lum, et al.

(1)

This simulation framework is flexible enough to accommodate many of the base metrics that typically would be used
for binary classification, though the interpretation of 𝑛𝑘 and 𝑍𝑘 is metric-dependent. For example, if 𝑚 is accuracy,
then 𝑍𝑘 represents the number of correct classifications and 𝑛𝑘 represents the total number of individuals in the group.
If 𝑚 is the false positive rate, then 𝑍𝑘 is the number of negative classifications among those observations that were
truly positive and 𝑛𝑘 is the number of positives in group 𝑘.

4 STATISTICAL BIAS IN GROUP-WISE MODEL PERFORMANCE DISPARITIES METRICS

Whereas the metric 𝑌𝑘 = 𝑚(𝑤𝑘, 𝑓 (𝑥𝑘 )) is applied to the data and predictions within a group to calculate model
performance for that group specifically, meta-metrics summarize the degree to which the model’s performance varies
across groups. These summary measures, 𝑀 (𝑌 ), take in the vector of observed model performance, 𝑌 = [𝑌1, ..., 𝑌𝐾 ], and
output a number that measures the degree of group-wise model performance disparities, i.e. a distance from the ideal

state in which the model performs equally well for all groups. Here we consider statistical bias (the expected difference

between the estimator and the truth) in several metrics that have been used in the algorithmic fairness literature, have

been implemented in open source software for measuring and remediating disparities, and/or are common statistical

measures of variability. Table 1 defines several such metrics.

Suppose we are interested in measuring the “true" degree to which model performance varies across groups, 𝑀 (𝜇).
We don’t observe 𝜇, but we do observe 𝑌 – a vector of statistically unbiased estimates of 𝜇. What happens if we use
𝑀 (𝑌 ) as a measurement of 𝑀 (𝜇)? For several of the considered metrics, it is easy to see mathematically that 𝑀 (𝑌 ) is
a statistically biased estimate of 𝑀 (𝜇), though an expression for the expected upward-deviation is not available. For
example, for the mean absolute deviation,

E[𝑀mad (𝑌 )] = E (cid:104) 1
= 1
𝐾
(cid:205)𝐾

𝐾
(cid:205)𝐾

(cid:105)

(cid:205)𝐾

|𝑌𝑘 − ¯𝑌 |
𝑘=1
(cid:12)𝑌𝑘 − ¯𝑌 (cid:12)
E (cid:12)
(cid:12)
𝑘=1
(cid:12)E (cid:2)𝑌𝑘 − ¯𝑌 (cid:3)(cid:12)
(cid:12)
(cid:12)
|𝜇𝑘 − ¯𝜇|

𝑘=1
(cid:205)𝐾

> 1
𝐾
= 1
𝐾
𝑘=1
= 𝑀mad (𝜇),

where the inequality comes by applying Jensen’s inequality to each term in the sum since the absolute value is a convex
function. Similar arguments relying on Jensen’s inequality, can be made to show that 𝑀max-abs-diff (𝑌 ) is also a positively
statistically biased estimator of 𝑀max-abs-diff (𝜇).

For the variance, we can get an analytical expression for the expectation of 𝑀var (𝑌 ) in terms of 𝑀var (𝜇) and the 𝜎𝑘 s:

E[𝑀var (𝑌 )] = E

(cid:34)

1
𝐾 − 1

(cid:35)

∑︁

(𝑌𝑘 − ¯𝑌 )2

𝑘
(cid:32)

(cid:33)

𝜎 2
𝑘

,

1
𝐾

∑︁

𝑘

= 𝑀var (𝜇) +

6

De-biasing “bias" measurement

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Fig. 2. Empirical illustration that naïve estimates of meta-metrics in Table 1 are statistically biased upwards. 𝐾 on the 𝑥-axis indicates
the number of groups. The sum of the number of samples over all the groups is constant regardless of 𝐾 and the number of samples
per group is equal for any given 𝐾.

This is derivable using the fact that E[𝑋 2] = E[𝑋 ]2 + Var(𝑋 ) and (cid:205)𝑘 (𝑥𝑘 − ¯𝑥)2 = (cid:205)𝐾
given in the appendix. Because 𝜎 2
𝑀var (𝜇), the “true" between-group model performance variance.

𝑘 − 𝐾 ¯𝑥 2. The derivation is
𝑥 2
𝑘 ≥ 0, this shows that 𝑀var (𝑌 ) is also an upwardly statistically biased estimator of

𝑘=1

We have shown mathematically that several of the meta-metrics are statistically biased. Others are not so mathe-

matically tractable. We investigate all meta-metrics in Table 1 empirically through simulation. In our simulation, five
(cid:17). In
thousand individuals are evenly divided into 𝐾 groups for 𝐾 ranging from five to 150. That is, 𝑛𝑘 = round (cid:16) 5,000
cases where 𝐾 does not evenly divide 𝑛, the total population size deviates slightly from 5,000 due to rounding. We set 𝜇
to be the corresponding vector of 𝐾 “true" model performance values equally spaced on [𝑙, .9] (i.e. 𝜇𝑘 = 𝑙 + 𝑘−1
𝐾−1 (.9 − 𝑙))
and consider several different values of 𝑙, the lower bound. Given 𝜇𝑘 and 𝑛𝑘 , we simulate from the model described in
(1). Figure 2 shows a Monte Carlo estimate of the statistical bias, 𝑀 (𝑌 ) − 𝑀 (𝜇), averaged over 1,000 simulations for
several values of 𝐾 and 𝑙, for each of the considered meta-metrics. A different meta-metric is given in each panel of the

𝐾

figure.

For all considered meta-metrics, 𝑀 (𝑌 ) over-estimates 𝑀 (𝜇). In practice, this is problematic for two reasons. In an
absolute sense, this can paint a misleading picture of the extent to which a model performs differently across groups.

For decision-makers who need this information to determine how and whether a model should be used or modified,

exaggerating the degree to which the model is “unfair" may cause unnecessary model adjustments. In the context

of the well-known fairness-accuracy trade-off [16], having misleading measures of group-wise model performance

disparities may cause model builders to trade off more accuracy than necessary in the name of eliminating group-wise

7

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Lum, et al.

model performance disparities, resulting in a less performant model for all individuals. In a relative sense, this can

lead to misleading conclusions about how group-wise model performance disparities compares along different axes.

For example, suppose we wanted to evaluate the degree of disparities for two “sensitive variables": age and language,
as in the example in Figure 1. These variables have different numbers of groups, with 𝐾 = 15 for age and 𝐾 = 40 for
language. In the case where individuals are evenly distributed across groups as in this simulation, we can see that all

of our metrics would, on expectation, return a higher estimate of group-wise model performance disparities for the
variable with 𝐾 = 100 than the variable with 𝐾 = 10, all else held constant. That is, we would erroneously conclude
that the model had more group-wise model performance disparities with respect to language than age, even when the

true between-group variability in model performance was identical for both variables. This remains true even when the

model is “fair" for both axes (i.e. when the lower bound is 0.9 and all groups have equal true performance).

In this simulated example, the statistical bias increases as a function of 𝐾 for all metrics. This is an artifact of the fact

that the observations are equally distributed across the groups. Comparing the statistical bias of a “sensitive variable"

with individuals evenly distributed across many groups and another “sensitive variable" with individuals unevenly

distributed across fewer groups may not exhibit the same pattern of a larger statistical bias for the many-group variable

than the few-group variable.

5 CORRECTING FOR STATISTICAL BIAS IN GROUP-WISE MODEL PERFORMANCE DISPARITIES

MEASUREMENT

Given the statistical bias in all of the existing meta-metrics, what is to be done? The expression for the statistical bias in
the variance offers one possibility. Because we know the statistical bias in 𝑀var (𝑌 ) as a function of the 𝜎𝑘 s, we can
correct for it and use the estimator

ˆ𝑀var (𝜇) = 𝑀var (𝑌 ) −

1
𝐾

∑︁

ˆ𝜎 2
𝑘 ,

𝑘

(2)

where ˆ𝜎𝑘 is the standard error of the estimate of model performance for group 𝑘. As it turns out, (2) was first introduced
by Cochran in the random effects ANOVA context in 1954 [14] and was popularized for meta-analysis of between-study

variability by Hedges and Olkin in 1985 [27]. Often, in these other contexts, this estimator is truncated to preclude the
possibility of a negative variance estimate, i.e. ˆ𝑀var (𝜇) = max(0, 𝑀var (𝑌 ) − 1
𝑘 ). We also adopt this convention.
𝐾
The estimator given in Equation (2) offers a conceptually clean and easily calculable estimate of between-group model

(cid:205)𝑘 ˆ𝜎 2

performance variance. The untruncated version is statistically unbiased when statistically unbiased estimates of the

standard errors are also available [48]. We find this estimator appealing because it is easy to explain and easy to calculate

without access to statistics-specific software packages. Going forward, due to the mathematical tractability of the

variance as a meta-metric, we focus our investigation on measuring and quantifying uncertainty about the variance of

model performance across groups.

5.1 Simulation example

We extend the simulation framework to focus in on four scenarios. These scenarios are summarized in Table 2. In all
scenarios presented, we consider a training set of size of 𝑛 = 5000 with 𝐾 = 100. In the Equal Group-size condition, all
groups have 𝑛
𝐾 = 50 observations. In the Unequal Group-size condition, we create group sizes by linearly interpolating
between 10 and 90 and rounding. The minimum group size is 10 and the maximum 90. Every integer between 10 and

90 has at least one group of that size; occasionally due to rounding there are two groups with the same number of

8

De-biasing “bias" measurement

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Equal Performance

Unequal Performance

Equal Group-size
𝜇1 = 𝜇2 = ... = 𝜇𝑘 = 0.8
𝑛𝑘 = 𝑛
𝐾
𝜇𝑘 = .1 + 0.8 𝑘−1
𝐾−1
𝑛𝑘 = 𝑛
𝐾

Unequal Group-size
𝜇1 = 𝜇2 = ... = 𝜇𝑘 = 0.8
𝑛𝑘 = round(10 + 80 𝑘−1
𝐾−1 )
𝜇𝑘 = .1 + 0.8 𝑘−1
𝐾−1
𝑛𝑘 = round(10 + 80 𝑘−1
𝐾−1 )

Table 2. The four scenarios used for simulations. We consider the case when there are no group-wise model performance disparities
(equal performance) or not (unequal performance) and also whether the number of samples in each group is equal or not.

observations. In the Equal Performance condition, 𝜇1 = 𝜇2 = ... = 𝜇𝑘 = 0.8. In these scenarios, the true variance across
groups is zero. In the Unequal Performance condition, group performance is equally spaced between 0.1 and 0.9, as in

the simulation in Figure 2. Here, the true variance in performance across groups is 0.055. For each scenario, we again

use the simulation model given in (1).

For each scenario, we simulate data and calculate the variance of 𝑌 and the corrected variance of 𝑌 as in (2). To

estimate ˆ𝜎 2, we use the plug-in estimator of the sampling variance of a binomial proportion, ˆ𝜎 2 =
this 1000 times. Figure 3 shows the distribution of these estimates. While we have already seen (both mathematically

. We repeat

𝑌 (1−𝑌 )
𝑛𝑘

and through simulation) that the uncorrected variance (red) is upwardly statistically biased, this shows that correcting

for the statistical bias by substracting off an estimate of the average standard error of the groups successfully results in

between-group variance estimates that are centered at the truth (vertical black line), i.e. statistically unbiased. Next we

explore what happens if we bootstrap this corrected variance estimator to obtain bootstrap intervals for uncertainty

quantification.

Fig. 3. Each panel shows a different simulation scenario depending on whether the group sizes are equal or unequal and whether
there are truly disparities or not in model performance among the groups. The black vertical lines indicate the true variance of
model performance among the groups. We show the histogram of uncorrected (red) and corrected (blue) estimates of between-group
variability in model performance across 1000 simulated replicates. The uncorrected estimate is statistically biased upwards.

5.2 Bootstrapping the (statistical) bias-corrected variance estimate

The corrected variance estimator defines an easy and explainable way to obtain a statistically unbiased point estimate

of the true between-group performance variance. Bootstrapping offers a similarly conceptually simple and easily

implementable method for calculating uncertainty intervals. To create non-parametric bootstrap intervals, one samples

9

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Lum, et al.

Fig. 4. Each panel shows a different simulation scenario depending on whether the group sizes are equal or unequal and whether
there are truly disparities or not in model performance among the groups. We show the histogram of bootstrap samples of the
corrected (blue) and uncorrected (red) variance for one draw from the generative model. Intervals represent 95% bootstrap intervals.
Neither estimator covers the true variance of model performance among the groups shown by the vertical black lines.

with replacement from the observed data. If we consider the number of individuals in each group to be fixed (as we do
in the following examples), then for each group 𝑘, we sample 𝑛𝑘 individuals with replacement from the collection of
all individuals in group 𝑘. This makes one bootstrap sample, 𝑌 ∗ = [𝑌 ∗
𝐾 ]. For each bootstrap sample, a statistic–
1
such as the corrected variance– is calculated. This process is repeated 𝐵 times, resulting in 𝐵 samples of the target

, ..., 𝑌 ∗

statistic. Uncertainty intervals for the statistic are calculated as the empirical quantiles of the 𝐵 estimates. For example,

a standard 95% interval would be calculated by setting the lower and upper bounds of the interval to be the 0.025

and 0.975 quantiles of the samples of the statistic, respectively. There are many techniques for bootstrapping, such

as versions that use the bootstrap samples only to calculate standard errors around point estimate obtained from

the original sample [19]. Here, we use the “percentile" bootstrap. Future work should consider other techniques for

bootstrapping in this context, such as using single-corrected estimator as a point estimate and the double-corrected

estimator to obtain intervals about that point estimate.

Figure 4 shows the distribution of bootstrap samples of the corrected and uncorrected variance for one draw from

the generative model. The horizontal bars show 95% bootstrap intervals for the corrected variance. It is important to

differentiate that whereas in Figure 3, the histogram is made up of single estimates of the variance each corresponding

to independent draws from the generative model, in Figure 4 the histogram is generated by bootstrap re-sampling
from one draw from the generative model to obtain uncertainty estimates for the variance estimate associated with
that single draw. Here, we see that the naïve application of the (corrected) variance estimator to each bootstrap sample

results in distributions of estimates that are systematically shifted upwards with intervals (horizontal lines) that do not

even come close to covering the true value (shown by the vertical black line). When bootstrapping, even our statistically

unbiased estimator of the variance is statistically biased.

5.3 What happened?

Let’s go back to our original model given in (1). Under this model, the variance of our estimator 𝑌𝑘 is given by
, which is what we’ve used as our estimate of the sampling variance of 𝑌𝑘 when applying the correction.
ˆ𝜎 2
𝑘 =

𝑌𝑘 (1−𝑌𝑘 )
𝑛𝑘

10

De-biasing “bias" measurement

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

However, when we generate bootstrap samples (by re-sampling the observations within groups), our generative model

for each bootstrap sample becomes the following:

𝑋𝑘 ∼ Binomial(𝑛𝑘, 𝜇𝑘 )

𝑌𝑘 =

𝑋 ∗
𝑘 ∼ Binomial(𝑛𝑘, 𝑌𝑘 )

𝑌 ∗
𝑘 =

𝑋𝑘
𝑛𝑘
𝑋 ∗
𝑘
𝑛𝑘

Applying iterated expectations, we find that the sampling variance of each bootstrap sample is given by

Var(𝑌 ∗

𝑘 ) = E(Var(𝑌 ∗

𝑘 | 𝑌𝑘 )) + Var(E(𝑌 ∗

𝐾 | 𝑌𝐾 ))

= E

(cid:19)

(cid:18) 𝑌𝑘 (1 − 𝑌𝑘 )
𝑛𝑘

= E

(cid:32) 𝑌𝑘
𝑛𝑘

𝜇𝑘
𝑛𝑘

−

(cid:33)

+

−

𝑌 2
𝑘
𝑛𝑘
(cid:18) Var(𝑌𝑘 )
𝑛𝑘

−

𝜇𝑘
𝜇𝑘 (1 − 𝜇𝑘 )
𝑛𝑘
𝑛2
𝑘
2𝜇𝑘 (1 − 𝜇𝑘 )
𝑛𝑘

−

=

=

=

+ Var(𝑌𝑘 )

𝜇𝑘 (1 − 𝜇𝑘 )
𝑛𝑘

+

E(𝑌𝑘 )2
𝑛𝑘
𝜇2
𝑘
𝑛𝑘
𝜇𝑘 (1 − 𝜇𝑘 )
𝑛2
𝑘

−

+

(cid:19)

+

𝜇𝑘 (1 − 𝜇𝑘 )
𝑛𝑘

𝜇𝑘 (1 − 𝜇𝑘 )
𝑛𝑘

Again, using a plug-in estimate of the sampling variance of 𝑌 ∗
𝑘

, we get ˆ𝜎 2∗

𝑘 =

2𝑌 ∗

𝑘 (1−𝑌 ∗
𝑘 )
𝑛𝑘

−

that for each bootstrap sample, a “double-corrected" estimate of the variance of 𝜇 is given by

𝑘 (1−𝑌 ∗
𝑌 ∗
𝑘 )
𝑛2
𝑘

. This implies

ˆ𝑀var (𝜇) = 𝑀var (𝑌 ∗) −

1
𝐾

∑︁

ˆ𝜎 2∗
𝑘 .

𝑘

In summary, to obtain bootstrap intervals for the variance of 𝜇, we must account not only for the variance in the

generative model, but also the additional variance that occurs due to the bootstrapping procedure itself. Figure 5 shows

bootstrap intervals and a histogram of bootstrap samples for the uncorrected, corrected, and double-corrected estimates

of the between-group variance for one draw from the generative. Here we see that– at least for this replicate– by

explicitly accounting for the additional variance induced by the bootstrap sampling process itself, we have once again

successfully created an estimator that accurately captures between-group variance.

Extending beyond a single replicate, Table 3 shows the empirical coverage of 95% bootstrap intervals across 1000

replicates. We see that applying the double-corrected variance estimator to create bootstrap intervals of the between-

group variance has coverage similar to the nominal 95% level.

6 REAL DATA EXAMPLE

We compare the variance and double-corrected variance estimators of group-wise model performance disparities in
a model built from the Adult Income dataset4, which is frequently used in algorithmic fairness research. It contains

4Downloaded from the UCI Machine Learning Repository at https://archive.ics.uci.edu/ml/datasets/adult.

11

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Lum, et al.

Fig. 5. Each panel shows a different simulation scenario depending on whether the group sizes are equal or unequal and whether
there are truly disparities or not in model performance among the groups. We show the histogram of 500 bootstrap samples and 95%
bootstrap intervals for the uncorrected (red), corrected (green), and double-corrected (blue) estimators of the between-group variance
for one draw from the generative model. The 95% bootstrap interval for the double corrected variance estimate is the only estimator
that covers the true variance (indicated by the vertical black lines) of model performance among the groups.

Equal Size; Equal Perf
Unequal Size; Equal Perf
Equal Size; Unequal Perf
Unequal Size; Unequal Perf

uncorrected_var
0.0
0.0
15.4
10.4

corrected_var
0.0
0.0
67.6
60.4

double_corrected_var
99.7
99.3
94.9
93.0

Table 3. Empirical coverage of the 95% bootstrap intervals over 1000 replicates.

records of demographic and income data for 48,842 individuals from the 1994 census database. There are a total of 14
features, a weight column (fnlwgt), and a label column that indicates whether each person makes over $50K a year. We
split the data randomly into train and test sets by 70%-30%, and trained histogram-based gradient boosting classification
trees5 using all 14 features of the dataset. The weight column is dropped as per prior research [50, 53]. The resulting
classifier has a 87.3% accuracy on the test set.

We are interested in how the model performs with respect to two demographic variables race and age. We divide
age into 8 groups from 15 to 95, each spanning 10 years. The distribution of these two features are highly skewed. In
the test set, across race, white accounts for 86% of the data, while American Indian and Eskimo account for only 0.9%.
Across the 8 age groups, the smallest group, (85, 95], accounts for just 0.1%, while the largest group, (25, 35], accounts

for 27%. We expect the presence of these small groups leads to increased statistical bias in estimated variance.

Figure 6 shows 95% bootstrap intervals for the uncorrected and double corrected variance estimates. We calculated

the intervals over three base model performance metrics, selection rate (SR), false positive rate (FPR), and true positive

rate (TPR), since they are the base metrics that popular fairness criteria such as demographic parity and equalized

odds depend on. The figure shows that the discrepancy between the uncorrected and double-corrected intervals is

particularly large for TPR: the double-corrected intervals cover zero, while the uncorrected intervals do not. In other

words, had we just used the uncorrected intervals, we would have concluded with high statistical certainty that there

was a large disparity in model’s TPR over both racial and age groups. However, after accounting for uncertainty about

5We used the scikit-learn implementation https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html.

12

De-biasing “bias" measurement

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Fig. 6. 95% bootstrap intervals for uncorrected and double-corrected estimates of variance over three performance metrics (FPR, SR,
TPR) and two demographic features of the Adult income dataset. There are 8 age groups, 5 racial groups, and 40 intersectional groups.
We see that we can come to different conclusions about whether there is between-group variability in model performance depending
on which estimator we use. Notably, there are cases when the double corrected estimate says there could be no between-group
variability when the uncorrected estimate says there is.

TPR within each group, we conclude that the disparities are much lower, and we cannot statistically rule out zero

disparities in false positives as a possibility.

7 CONCLUSION

In this paper, we have demonstrated both theoretically and empirically that meta-metrics commonly used in the

algorithmic fairness literature and in open-source tools designed for measuring group-wise model performance

disparities are themselves statistically biased measurements. This occurs because existing meta-metrics fail to account for

statistical uncertainty in the underlying base metrics, thus exaggerating between-group model performance disparities.

In practice, this can cause misleading inferences about the extent of model performance disparities and erroneous

conclusions about the relative degree of disparities between different sensitive variables. After identifying this problem,

we propose a new estimator for evaluating between-group model performance disparities. This estimator builds upon

the work of Cochran [14] and Hedges and Olkin [27] by accounting for variability induced by bootstrap procedures.

Our resulting double-corrected estimator offers a simple, conceptually clean, and easily implementable way to measure

model performance disparities and associated uncertainty.

The method we have developed offers one approach to quantifying group-wise model performance disparities in

the presence of many groups. To our knowledge, this is the first time this question has been studied in the context

of algorithmic fairness. However, between-group variance estimation is well-studied in the statistics literature more

generally. Common approaches include restricted maximum likelihood estimation of the variance parameter in random

effects models [25]. The literature on meta-analysis also contains many analogous methods. In that domain, the goal is

to estimate variability in effect size across studies. There, 𝑌 represents the effect size of a particular study, and 𝜎 the

standard error of that estimate. (See [47] for an excellent overview of methods available to estimate between-study

effect size variance and its associated uncertainty in the context of meta-analysis and [34] for a recent simulation study

13

FPRSRTPRage_grouprace0.000.010.020.030.040.050.000.010.020.030.040.050.000.010.020.030.040.05doublecorrecteduncorrecteddoublecorrecteduncorrectedmethodFAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Lum, et al.

comparing methods.) Many of these approaches also offer solutions for quantifying model performance disparities

across many groups. Future work should perform simulation studies comparing different methods for measuring

between-group variability with parameters tailored to the specifics of typical problems encountered when evaluating

model “fairness."

Finally, model performance disparities as measured by meta-metrics are not dispositive of the presence or absence of

algorithmic harms. While large disparities typically indicate that an issue needs further investigation, small measured

disparities do not guarantee that the system is fair or free from adverse impacts. Just as it would be foolish to claim a

computer system is completely secure or a data set is completely private, it is always possible that there are undiscovered

vulnerabilities that our measurements have not uncovered. While we have presented an approach to more accurately

measure model performance disparities, these measurements cannot tell us whether the observed disparities are

acceptable or whether we have calculated disparities with respect to appropriate grouping variables. These judgments

are subjective and require understanding of the context in which a model will be used. Like all metrics, meta-metrics

simply cannot capture the entirety of the impact of machine learning systems on people.

REFERENCES
[1] Robert Adragna, Elliot Creager, David Madras, and Richard Zemel. 2020. Fairness and robustness in invariant learning: A case study in toxicity

classification. Algorithmic Fairness through the Lens of Causality and Interpretability Workshop at NeurIPS (2020).

[2] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. 2018. A reductions approach to fair classification. In

International Conference on Machine Learning. PMLR, 60–69.

[3] Yongsu Ahn and Yu-Ru Lin. 2019. Fairsight: Visual analytics for fairness in decision making. IEEE transactions on visualization and computer graphics

26, 1 (2019), 1086–1095.

[4] Rico Angell, Brittany Johnson, Yuriy Brun, and Alexandra Meliou. 2018. Themis: Automatically testing software for discrimination. In Proceedings of
the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 871–875.

[5] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine Bias. ProPublica (2016).
[6] Chloé Bakalar, Renata Barreto, Stevie Bergman, Miranda Bogen, Bobbie Chern, Sam Corbett-Davies, Melissa Hall, Isabel Kloumann, Michelle Lam,
Joaquin Quiñonero Candela, et al. 2021. Fairness On The Ground: Applying Algorithmic Fairness Approaches to Production Systems. arXiv preprint
arXiv:2103.06172 (2021).

[7] Jack Bandy and Nicholas Diakopoulos. 2021. More Accounts, Fewer Links: How Algorithmic Curation Impacts Media Exposure in Twitter Timelines.

Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–28.

[8] Yahav Bechavod and Katrina Ligett. 2017. Penalizing unfairness in binary classification. Fairness, Accountability and Transparency in Machine

Learning Workshop at KDD (2017).

[9] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep
Mehta, Aleksandra Mojsilovic, et al. 2018. AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic
bias. arXiv preprint arXiv:1810.01943 (2018).

[10] Sarah Bird, Miro Dudík, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker. 2020.

Fairlearn: A toolkit for assessing and improving fairness in AI. Microsoft, Tech. Rep. MSR-TR-2020-32 (2020).

[11] Amanda Bower, Hamid Eftekhari, Mikhail Yurochkin, and Yuekai Sun. 2021. Individually Fair Ranking. International Conference on Learning

Representations (2021).

[12] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on

fairness, accountability and transparency. PMLR, 77–91.

[13] Ángel Alexander Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng, Jamie Morgenstern, and Duen Horng Chau. 2019. FairVis: Visual analytics

for discovering intersectional bias in machine learning. In 2019 IEEE Conference on Visual Analytics Science and Technology (VAST). IEEE, 46–56.

[14] William G Cochran. 1954. The combination of estimates from different experiments. Biometrics 10, 1 (1954), 101–129.
[15] Cyrus DiCiccio, Sriram Vasudevan, Kinjal Basu, Krishnaram Kenthapadi, and Deepak Agarwal. 2020. Evaluating fairness using permutation tests. In

Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1467–1477.

[16] Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, and Kush Varshney. 2020. Is there a trade-off between fairness and accuracy?

a perspective using mismatched hypothesis testing. In International Conference on Machine Learning. PMLR, 2803–2813.

[17] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd

innovations in theoretical computer science conference. 214–226.

14

De-biasing “bias" measurement

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

[18] Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Max Leiserson. 2018. Decoupled classifiers for group-fair and efficient machine learning.

In Conference on fairness, accountability and transparency. PMLR, 119–133.

[19] Bradley Efron and Robert J Tibshirani. 1994. An introduction to the bootstrap. CRC press.
[20] Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P Hamilton, and Derek Roth. 2019. A comparative

study of fairness-enhancing interventions in machine learning. In Fairness, Accountability, and Transparency (FAT*). 329–338.

[21] Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. 2019. Fairness-aware ranking in search & recommendation systems with application
to linkedin talent search. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2221–2231.
[22] Avijit Ghosh, Lea Genuit, and Mary Reagan. 2021. Characterizing Intersectional Group Fairness with Worst-Case Comparisons. In Proceedings of
2nd Workshop on Diversity in Artificial Intelligence (AIDBEI) (Proceedings of Machine Learning Research, Vol. 142), Deepti Lamba and William H. Hsu
(Eds.). PMLR, 22–34. https://proceedings.mlr.press/v142/ghosh21a.html

[23] Google. [n.d.]. TensorFlow Fairness Indicators. https://www.tensorflow.org/tfx/guide/fairness_indicators [Online; accessed 1-20-22].
[24] Sruthi Gorantla, Amit Deshpande, and Anand Louis. 2021. On the Problem of Underranking in Group-Fair Ranking. In International Conference on

Machine Learning. PMLR, 3777–3787.

[25] David A Harville. 1977. Maximum likelihood approaches to variance component estimation and to related problems. Journal of the American

statistical association 72, 358 (1977), 320–338.

[26] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. 2018. Fairness without demographics in repeated loss minimization.

In International Conference on Machine Learning. PMLR, 1929–1938.

[27] Larry V Hedges and Ingram Olkin. 1985. Statistical methods for meta-analysis. Academic press.
[28] Shlomi Hod. 2018–. Responsibly: Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems. http://docs.responsibly.ai/

[Online; accessed 1-20-22].

[29] Lily Hu, Nicole Immorlica, and Jennifer Wortman Vaughan. 2019. The disparate effects of strategic manipulation. In Proceedings of the Conference on

Fairness, Accountability, and Transparency. 259–268.

[30] Xiaolei Huang, Linzi Xing, Franck Dernoncourt, and Michael J. Paul. 2020. Multilingual Twitter Corpus and Baselines for Evaluating Demographic
Bias in Hate Speech Recognition. In Proceedings of the 12th Language Resources and Evaluation Conference. European Language Resources Association,
Marseille, France, 1440–1448. https://aclanthology.org/2020.lrec-1.180

[31] James E Johndrow and Kristian Lum. 2019. An algorithm for removing sensitive information: application to race-independent recidivism prediction.

The Annals of Applied Statistics 13, 1 (2019), 189–220.

[32] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2018. Preventing fairness gerrymandering: Auditing and learning for subgroup

fairness. In International Conference on Machine Learning. PMLR, 2564–2572.

[33] Junpei Komiyama, Akiko Takeda, Junya Honda, and Hajime Shimao. 2018. Nonconvex optimization for regression with fairness constraints. In

International conference on machine learning. PMLR, 2737–2746.

[34] Dean Langan, Julian PT Higgins, Dan Jackson, Jack Bowden, Areti Angeliki Veroniki, Evangelos Kontopantelis, Wolfgang Viechtbauer, and Mark
Simmonds. 2019. A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses. Research synthesis methods 10, 1
(2019), 83–98.

[35] Tomo Lazovich, Luca Belli, Aaron Gonzales, Amanda Bower, Uthaipon Tantipongpipat, Kristian Lum, Ferenc Huszar, and Rumman Chowdhury.

2021. Measuring Disparate Outcomes of Content Recommendation Algorithms with Distributional Inequality Metrics. (2021).

[36] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. 2018. Delayed impact of fair machine learning. In International Conference

on Machine Learning. PMLR, 3150–3158.

[37] Kristian Lum and William Isaac. 2016. To predict and serve? Significance 13, 5 (2016), 14–19.
[38] David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. 2018. Learning adversarially fair and transferable representations. In International

Conference on Machine Learning. PMLR, 3384–3393.

[39] Jérémie Mary, Clément Calauzenes, and Noureddine El Karoui. 2019. Fairness-aware learning for continuous attributes and treatments. In International

Conference on Machine Learning. PMLR, 4382–4391.

[40] Kevin S McCurley. 2008. Income inequality in the attention economy. http://static.googleusercontent.com/media/research.google.com/en//pubs/

archive/33367.pdf. (2008).

[41] Guillaume Saint-Jacques, Amir Sepehri, Nicole Li, and Igor Perisic. 2020. Fairness through Experimentation: Inequality in A/B testing as an approach

to responsible design. arXiv preprint arXiv:2002.05819 (2020).

[42] Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby Stevens, Ari Anisfeld, Kit T Rodolfa, and Rayid Ghani. 2018. Aequitas: A bias

and fairness audit toolkit. arXiv preprint arXiv:1811.05577 (2018).

[43] Kacper Sokol, Raul Santos-Rodriguez, and Peter Flach. 2019. FAT Forensics: A Python toolbox for algorithmic fairness, accountability and

transparency. Journal of Open Source Software (2019).

[44] Till Speicher, Hoda Heidari, Nina Grgic-Hlaca, Krishna P Gummadi, Adish Singla, Adrian Weller, and Muhammad Bilal Zafar. 2018. A unified
approach to quantifying algorithmic unfairness: Measuring individual &group unfairness via inequality indices. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. 2239–2248.

[45] Florian Tramer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Jean-Pierre Hubaux, Mathias Humbert, Ari Juels, and Huang Lin. 2017. Fairtest:
Discovering unwarranted associations in data-driven applications. In 2017 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE,

15

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

Lum, et al.

401–416.

[46] Sriram Vasudevan and Krishnaram Kenthapadi. 2020. LiFT: A Scalable Framework for Measuring Fairness in ML Applications. In Proceedings of the

29th ACM International Conference on Information & Knowledge Management. 2773–2780.

[47] Areti Angeliki Veroniki, Dan Jackson, Wolfgang Viechtbauer, Ralf Bender, Jack Bowden, Guido Knapp, Oliver Kuss, Julian PT Higgins, Dean Langan,
and Georgia Salanti. 2016. Methods to estimate the between-study variance and its uncertainty in meta-analysis. Research synthesis methods 7, 1
(2016), 55–79.

[48] Wolfgang Viechtbauer. 2005. Bias and efficiency of meta-analytic variance estimators in the random-effects model. Journal of Educational and

Behavioral Statistics 30, 3 (2005), 261–293.

[49] Jing Nathan Yan, Ziwei Gu, Hubert Lin, and Jeffrey M Rzeszotarski. 2020. Silva: Interactively Assessing Machine Learning Fairness Using Causality.

In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–13.

[50] Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. 2019. Training individually fair ML models with sensitive subspace robustness. International

Conference for Learning Representations (2019).

[51] Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo Baeza-Yates. 2017. Fa* ir: A fair top-k ranking

algorithm. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. 1569–1578.

[52] Meike Zehlike, Tom Sühr, Ricardo Baeza-Yates, Francesco Bonchi, Carlos Castillo, and Sara Hajian. 2022. Fair Top-k Ranking with multiple protected

groups. Information Processing & Management 59, 1 (2022), 102707.

[53] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018

AAAI/ACM Conference on AI, Ethics, and Society. 335–340.

A DERIVATIONS

Derivation of the single-corrected variance

E[𝑀var (𝑌 )] = E

(cid:34)

(cid:35)

∑︁

(𝑌𝑘 − ¯𝑌 )2

𝑘 − 𝐾 ¯𝑌 2]
𝑌 2

1
𝐾 − 1

1
𝐾 − 1

E[

𝑘
∑︁

𝑘

=

=

=

=

1
𝐾 − 1

1
𝐾 − 1

1
𝐾 − 1

(cid:32)

(cid:32)

(cid:32)

∑︁

𝑘

∑︁

𝑘

∑︁

𝑘

(cid:33)

𝑘 − 𝐾E[ ¯𝑌 2]
𝜎 2

𝜇2
𝑘 +

∑︁

𝑘

(𝜇𝑘 − ¯𝜇 + ¯𝜇)2 +

(cid:33)

𝑘 − 𝐾E[ ¯𝑌 2]
𝜎 2

∑︁

𝑘

(𝜇𝑘 − ¯𝜇)2 + 𝐾 ¯𝜇2 +

(cid:33)

𝜎 2
𝑘 − 𝐾E[ ¯𝑌 2]

∑︁

𝑘

𝐾 ¯𝜇2 +

𝐾 ¯𝜇2 +

∑︁

𝑘

∑︁

𝑘

(cid:33)

𝜎 2
𝑘 − 𝐾E[ ¯𝑌 2]

𝑘 − 𝐾 ( ¯𝜇2 +
𝜎 2

1
𝐾 2

(cid:33)

∑︁

𝜎 2)

𝑘

(cid:33)

∑︁

𝜎 2
𝑘 )

𝑘

(cid:33)

∑︁

𝜎 2
𝑘 −

1
𝐾

𝑘
(cid:32) 𝐾 − 1
𝐾

𝜎 2
𝑘 )

∑︁

𝑘

(cid:32)

(cid:32)

(cid:32)

= 𝑚𝑣𝑎𝑟 (𝜇) +

= 𝑚𝑣𝑎𝑟 (𝜇) +

= 𝑚𝑣𝑎𝑟 (𝜇) +

= 𝑚𝑣𝑎𝑟 (𝜇) +

1
𝐾 − 1

1
𝐾 − 1

1
𝐾 − 1

1
𝐾 − 1

= 𝑀var (𝜇) +

(cid:32)

1
𝐾

∑︁

(cid:33)

𝜎 2
𝑘

,

𝑘

16

De-biasing “bias" measurement

FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea

17

