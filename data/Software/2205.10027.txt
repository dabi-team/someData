PISTA: PRECONDITIONED ITERATIVE SOFT THRESHOLDING
ALGORITHM FOR GRAPHICAL LASSO

GAL SHALOM∗, ERAN TREISTER† , AND IRAD YAVNEH∗

Abstract. We propose a novel quasi-Newton method for solving the sparse inverse covariance
estimation problem also known as the graphical least absolute shrinkage and selection operator
(GLASSO). This problem is often solved using a second order quadratic approximation. However, in
such algorithms the Hessian term is complex and computationally expensive to handle. Therefore, our
method uses the inverse of the Hessian as a preconditioner to simplify and approximate the quadratic
element at the cost of a more complex (cid:96)1 element. The variables of the resulting preconditioned
problem are coupled only by the (cid:96)1 sub-derivative of each other, which can be guessed with minimal
cost using the gradient itself, allowing the algorithm to be parallelized and implemented eﬃciently
on GPU hardware accelerators. Numerical results on synthetic and real data demonstrate that our
method is competitive with other state-of-the-art approaches.

Key words. Graphical LASSO, Sparse Precision Matrix Estimation, Proximal Methods, Pre-

conditioning.

AMS subject classiﬁcations. 90C25, 65D18, 65K10, 65F08

1. Introduction. Inverse covariance estimation is a fundamental problem in
modern statistics. Speciﬁcally, the inverse covariance matrices of multivariate normal
distributions are used in numerous applications. One of the most common uses of
the inverse covariance, also known as the Precision Matrix, is to describe statistical
models. That is, the graph structure of Gaussian graphical models can be inferred
from the Precision Matrix [19]. This inferred graph can be used for analyzing gene
networks [8], ﬁnancial assets and stocks dependencies [10,30], social networks [13] and
other inter-dependent data [5, 18].

The most straightforward approach to estimating the Precision Matrix would
be to use the inverse of the empirical covariance matrix. Evidently, inverting the
empirical covariance matrix requires a number of samples that equals or exceeds the
dimension of the matrix. However, many problems of interest are high dimensional
[8, 17], to the extent that they are signiﬁcantly larger than the number of available
In such cases, some
samples, so the empirical covariance matrix is rank-deﬁcient.
kind of regularization is essential. One of the most common ways to estimate the
Precision Matrix is to solve a (cid:96)1-regularized maximum likelihood estimation problem,
known as the graphical least absolute shrinkage and selection operator (Graphical
LASSO, or GLASSO) problem. The use of (cid:96)1-regularization aims to achieve a sparse
estimation while keeping the problem convex. Sparsity of the Precision Matrix may be
interpreted as simplicity—a sparse Precision Matrix implies a simple inferred graph
structure [15]. Thus, when a genuine empirical Precision Matrix cannot be computed,
a sparse valid estimation is often the preferred choice [7, 15, 31]. We refer the reader
to the ﬁrst chapter of [15] and to [7] for additional motivation.

As the problem is well studied, many algorithms and methods for its solution have
been presented over the years [1, 2, 6, 9, 12, 14, 16, 17, 21, 22, 24, 26, 27]. Some of these

2
2
0
2

y
a
M
0
2

]

A
N
.
h
t
a
m

[

1
v
7
2
0
0
1
.
5
0
2
2
:
v
i
X
r
a

∗Faculty of Computer Science, Technion — Israel

Israel (gal-
shalom@cs.technion.ac.il, irad@cs.technion.ac.il). Supported in part by the Israel Science Foun-
dation, grant No. 1639/19.

Institute of Technology,

†Department of Computer Science, Ben-Gurion University of

(er-
ant@cs.bgu.ac.il). This work was supported in part by the Israeli Council for Higher Education
(CHE) via the Data Science Research Center, Ben-Gurion University of the Negev, Israel.

the Negev,

Israel

1

 
 
 
 
 
 
2

G. SHALOM, E. TREISTER, AND I. YAVNEH

methods are computationally intensive or diﬃcult to parallelize, rendering parallel
computation accelerators such as GPUs ineﬀective. As far as we know, none of the
algorithms in the literature were designed speciﬁcally with an eﬃcient GPU deploy-
ment in mind. Nevertheless, various existing algorithms are able to take advantage of
GPU processing power with a simple implementation, e.g., G-ISTA [14], VSM [21],
PSM [9], ALM [24], Newton-Lasso [22] and Orthant-Based Newton [22]. Each of these
has its strengths and weaknesses. Other algorithms, such as BCD-IC [26], can be de-
ployed on a GPU as well, however, the method and the authors’ implementation are
complex, and the extensive usage of scalar operations may hinder its performance.

In this work we introduce a preconditioned Iterative Soft Thresholding Algorithm
(pISTA) for solving the graphical LASSO problem. pISTA is a quasi-Newton algo-
rithm designed to be highly parallel and suitable for using GPU capabilities to its
beneﬁt by exploiting the problem structure eﬀectively. A traditional second order
quadratic approximation includes a complex Hessian term which is computationally
expensive to handle. Moreover, all the elements of the Hessian term are coupled, mak-
ing eﬃcient GPU deployment challenging. To this end, pISTA uses the inverse of the
Hessian as a preconditioner to simplify and approximate the quadratic element, at the
cost of a more complex (cid:96)1 element. This is highly beneﬁcial since the Hessian inverse
is easily obtained for the GLASSO problem. The variables of the resulting precondi-
tioned problem are coupled only by the (cid:96)1 sub-derivative of each other, which can be
guessed with minimal cost using the gradient itself. The resulting pISTA algorithm
uses only matrix operations, making it easier for exploiting the GPU eﬃciently.

This paper is organized as follows. In section 2 we refer to related work, and in
section 3 we formulate the problem. The pISTA algorithm is introduced in section 4,
and its convergence is proved in section 5. Experimental results are presented in
section 6, and conclusions follow in section 7.

2. Related Work. Many numerical algorithms have been developed, which are
speciﬁcally designed for solving the sparse inverse covariance estimation problem.
G-ISTA [14] uses a proximal gradient method which estimates the inverse covari-
ance iteratively. GLASSO [12] splits the problem into smaller LASSO problems and
updates the estimation by solving them separately. QUIC [16] uses a proximal New-
ton method on the objective function. BCD-IC [26] uses a block coordinate descent
method. ALM [24] uses an alternating linearization technique by splitting the ob-
jective function into two linear approximations and minimizing them alternatingly.
PSM [9] employs a projected gradient method. VSM [21] uses Nesterov’s smooth opti-
mization technique. Newton-Lasso [22] and Orthant-Based Newton [22] use FISTA [3]
and Conjugate Gradient methods, respectively, to solve reduced second order approx-
imations. Although some algorithms are designed to be run in parallel using the
multi-core model of the CPU, none of them take into speciﬁc consideration the par-
allelism model of the GPU and the usage of GPUs as a computation accelerator.

Additionally, some algorithm were developed speciﬁcally for large scale matrices
where the matrix can only reside in memory in sparse format. Big&Quic and SQUIC
[4, 17] extend QUIC [16] to large scales. Big&Quic uses on-demand computation of
the Hessian’s columns and a special procedure of the linesearch conditions using Schur
complements. SQUIC utilizes sparse Cholesky factors of the iterations matrix. ML-
BCD [27] takes BCD-IC [26], which is already suitable to large scale problems, and
accelerates it using a generic multilevel framework, which was originally suggested
in [28] for LASSO. This acceleration can in principle be used with our proposed
method as well. Large scale algorithms may beneﬁt from the GPU processing power,

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

3

however memory constraints and usage of sparse format make this task non-trivial.

Graphical LASSO can also be used in a mixture model setup when multiple sparse
inverse covariances are utilized [11]. This allows a richer statistical model, but requires
the estimation of multiple sparse Precision Matrices. An eﬃcient GPU-based solver
is highly beneﬁcial is such scenarios as well.

3. Background.

3.1. Sparse Inverse Covariance Estimation. Estimating the parameters of
multivariate Gaussian distributions is a fundamental problem in statistics. Given m
i=1 ∈ Rn, where yi ∼ N (µ, Σ), one would like to estimate
independent samples {yi}m
the mean µ ∈ Rn and either the covariance Σ, or its inverse Σ−1, which is also called
the Precision Matrix. Both the mean µ and the covariance Σ are often estimated by
the maximum likelihood estimator (MLE). The MLE is given by the parameters µ
and Σ that maximize the probability to sample the observed data {yi}m

i=1:

(3.1)

ˆµ, ˆΣ = arg max

µ,Σ

= arg max

µ,Σ

m
(cid:89)

i=1
m
(cid:89)

i=1

P(yi|Σ, µ)

1
(cid:112)(2π)mdet(Σ)

(cid:18)

exp

−

1
2

(yi − µ)T Σ−1(yi − µ)

(cid:19)

.

MLE has an analytical solution:

(3.2)

ˆµ =

1
m

m
(cid:88)

i=1

yi,

ˆΣ = S ∆=

1
m

m
(cid:88)

(yi − ˆµ)(yi − ˆµ)T ,

i=1

where ˆµ and ˆΣ are called the empirical mean and empirical covariance, respectively.
Usually, one considers the log-likelihood objective (negative log of (3.1)), mini-

mized over the inverse covariance matrix, yielding the inverse covariance MLE:

(3.3)

ˆΣ−1 = arg min

f (A) ∆= − log(det(A)) + T r(SA) .

A(cid:31)0

The solution is indeed the inverse of (3.2). However, in cases where the number
of available samples is smaller than the dimension of yi (m < n), the matrix S is
rank-deﬁcient and thus non-invertible, whereas the true Σ is assumed to have full
rank and is positive deﬁnite. In other words, we cannot estimate Σ−1 by inverting
S, and further assumptions should be considered. A common choice in this case is
to assume that Σ−1 is sparse. Σ−1 can be interpreted as a conditional dependence
matrix, that is, its oﬀ-diagonal entries indicate the dependence between the row and
column variables, given all the remaining variables [23].

A common approach is to regularize the log-likelihood objective (3.3) with a

sparsity promoting (cid:96)1-penalty [1, 2, 6]:

(3.4)

ˆΣ−1 = arg min

F (A) ∆= arg min

A(cid:31)0

A(cid:31)0

f (A) + α||A||1 ,

where α > 0 is a scalar and ||A||1 = (cid:80)
Graphical Lasso problem.

i,j |Ai,j|. Equation (3.4) is known as the

As seen from (3.4), the objective function is convex and is composed of two
parts—a smooth convex part f (A) and a non-smooth convex part α||A||1. Although
the objective is convex, the non-smooth term makes traditional algorithms ineﬀective,
and more specialized solvers are needed.

4

G. SHALOM, E. TREISTER, AND I. YAVNEH

3.2. Proximal Methods for Sparse Inverse Covariance Estimation. A
common approach to solving convex problems comprised of smooth and non-smooth
parts is known as proximal methods, where we approximate the objective function at
each iteration as follows: the smooth part is approximated by a quadratic function
while the non-smooth part is kept unchanged. This approach is especially attractive
when the non-smooth part is a separable function (e.g., point-wise).

Speciﬁcally, applying proximal methods to the objective function F (A) of (3.4),
the smooth term f (A) is approximated by a quadratic function while the term α||A||1
remains unchanged. This approximation yields a linear LASSO [25] problem ((cid:96)1-
regularized quadratic objective). Once deﬁned, we minimize it approximately using
a LASSO solver. Denoting the descent direction at the k-th iteration by D(k), then
at each iteration we solve:

(3.5)

D(k) = arg min

˜F (A(k) + D)

D

= arg min

f (A(k)) +

D

(cid:68)

g(k), D

(cid:69)

+

1
2

(cid:68)

D, H (k)D

(cid:69)

+ α||A(k) + D||1 ,

where g(k) = ∇f (A(k)) is the gradient at the kth iteration, and H (k) depends on the
particular method. For H (k) = I we get the “Proximal Gradient” method, which is
called G-ISTA [14]. At the other extreme, for H (k) = ∇2f (A(k)), the Hessian at the
kth iteration, we get the “Proximal Newton” method known as QUIC [16].

The gradient and the Hessian are given by:

(3.6)

∇f (A) = S − A−1

, ∇2f (A) = A−1 ⊗ A−1 ,

where ⊗ is the Kronecker product. We note that once the gradient has been computed,
we can compute Hessian-vector products at relatively low cost by using Kronecker
product properties. However, solving (3.5) with H (k) = ∇2f (A(k)) is highly complex
and it is typically done by coordinate descent iterations [16]. This makes it hard to
use computation accelerators (such as GPUs) eﬃciently.

4. The preconditioned Iterative Soft Thresholding Algorithm (pISTA).
In this section we develop the preconditioned Iterative Soft Thresholding Algorithm
(pISTA). In [16], the Proximal Newton method for our problem, each coordinate
computation depends on the values of all the other coordinates due to the Hessian in
the quadratic element. In pISTA, we aim for a simpler and easier to solve quadratic
element at the cost of a more complex non-smooth (cid:96)1 part.

First, in subsection 4.1, we restrict our problem to a smaller set of variables
as done in other algorithms [16, 22, 27].
In subsection 4.2, we simplify and relax
the quadratic part using a preconditioner, at the cost of making the non-smooth
part α||A||1 more complex. Lastly, in subsection 4.3, we solve the resulting problem,
completing the development of pISTA. In contrast to second order methods like QUIC,
for example, in pISTA each coordinate depends on the other variables only through
the sub-gradient of the non-smooth part ((cid:96)1 norm), which is more complex. However,
the sub-gradient of the (cid:96)1 term can be approximated easily, and relatively well, by the
sign of the elements of the current iterate (if it is nonzero) or of its gradient (where
the elements of the iterate vanish), allowing our quasi-Newton method to be eﬃcient.

4.1. Restricting the Updates to the Free-set. As introduced in [16] and
used in [22, 27], we restrict the descent direction D to the free-set of A(k). That is,
an element in D which is not in the free-set of A(k) is set to zero. Denote by S (k)
A the

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

5

free-set at iteration k, deﬁned as follows:

(4.1)

(cid:26)

S (k)
A =

(i, j)(cid:12)

(cid:12) A(k)

i,j (cid:54)= 0

(cid:27)

(cid:26)

∪

(i, j)(cid:12)

(cid:12) |∇f (A(k)|i,j > α

(cid:27)

.

According to Lemma 7 of [16], solving (3.5) restricted to the variables that are not
in the free-set of A(k) will result in a zero value in every element in the direction
D. Thus, restricting D to the free-set is equivalent to solving (3.5) in an alternating
two-block manner: the ﬁrst step restricted to the variables not in the free-set (which
makes no change in our approximation), and the second step restricted to the free-set.
The free set is recomputed before the next iteration.
Deﬁne the restriction mask at the k-th iteration:

(4.2)

[M(k)

A ]i,j =

(cid:26) 1 (i, j) ∈ S (k)
A
(i, j) /∈ S (k)
A

0

.

Then, the restricted problem we solve at the kth iteration can be written as:

(4.3)

D(k) = arg min

D

˜F (A(k) + (M(k)

A (cid:12) D)) ,

where (cid:12) is the Hadamard product. Note that, in the solution of (4.3), the elements
of D(k) that are not in the free-set of A(k) may have nonzero values. However, we
restrict the updates to the indices in the free-set of A(k) only, therefore, our descent
direction is M(k)
A (cid:12) D(k), which is equivalent to setting the values of D(k) that are
not in the free-set to zero.

4.2. Preconditioning of the Descent Direction. Note the following property

of the Hessian in (3.6):

(4.4)

(cid:0)∇2f (A)(cid:1)−1

= A ⊗ A ,

where ⊗ is the Kronecker product. This means that the Hessian inverse can be
obtained without any computational overhead, using the typically sparse A instead
of the typically dense A−1. Here we are interested in using this appealing property
to accelerate the solution of (3.5), where H (k) = ∇2f (A(k)). To this end, we use it as
a preconditioner to D:

(4.5)

vec(D) = (cid:0)∇2f (A(k))(cid:1)−1

vec(∆) ⇒ D = A(k)∆A(k) ,

where vec denotes the column-stacking of a matrix into a vector.

Denote W (k) = (A(k))−1. Using the preconditioned descent direction (4.5) in
the restricted problem (4.3), with H (k) = ∇2f (A(k)) = W (k) ⊗ W (k), results in the
following equation:

∆(k) = arg min

˜F

(cid:16)

A(k) + M(k)

A (cid:12) (A(k)∆A(k))

(cid:17)

∆

= arg min

(4.6)

∆

+

1
2

+ α

(cid:69)

(cid:68)

A (cid:12) (A(k)∆A(k))

f (A(k)) +

(cid:68)
g(k), M(k)
A (cid:12) (A(k)∆A(k)), W (k) (cid:16)
M(k)
(cid:12)
(cid:12)
A (cid:12) (A(k)∆A(k))
(cid:12)
(cid:12)
(cid:12)1
(cid:12)

(cid:12)
(cid:12)A(k) + M(k)
(cid:12)

M(k)

.

(cid:12)
(cid:12)
(cid:12)

(cid:17)
A (cid:12) (A(k)∆A(k))

W (k)(cid:69)

6

G. SHALOM, E. TREISTER, AND I. YAVNEH

After applying the mask restriction M and the preconditioning to (3.5), the
quadratic part (the second element in (4.6)) is still complex. We relax it by removing
the mask restriction M and replacing it with a multiplicative scalar:

(4.7)

1
2

(cid:68)
M(k)

A (cid:12) (A(k)∆A(k)), W (k) (cid:16)

M(k)
A (cid:12) (A(k)∆A(k))
A(k)∆A(k), W (k)(A(k)∆A(k))W (k)(cid:69)
(cid:68)

≈

1
2t

(cid:17)

W (k)(cid:69)
(cid:68)
1
2t

=

A(k)∆A(k), ∆

(cid:69)

,

where t is a scalar that is computed using linesearch. Since we do not change the ﬁrst
order terms (the gradient and (cid:96)1), this can still result in a monotonically convergent
method. Denote the resulting quasi-Newton approximation by P (∆; t), then the k-th
iterate is given by:

∆(k) = arg min

P (∆; t)

∆

(4.8)

= arg min

f (A(k)) +

∆

+ α

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)A(k) + M(k)
(cid:12)

(cid:68)
g(k), M(k)

A (cid:12) (A(k)∆A(k))
(cid:12)
(cid:12)
A (cid:12) (A(k)∆A(k))
(cid:12)
(cid:12)
(cid:12)1
(cid:12)

.

(cid:69)

+

1
2t

(cid:68)

A(k)∆A(k), ∆

(cid:69)

4.3. pISTA. Our pISTA algorithm solves the problem deﬁned by (4.8) in each
iteration, with an appropriately chosen t. First, we ﬁnd a solution ∆(k) to (4.8).
To obtain this, we need to formulate the sub-diﬀerential of the non-smooth (cid:96)1 term
||A + D||1. First, denote the sub-diﬀerential of ||A + D||1 by:

(4.9)

G(k)(D) ∆=

=

(cid:40)

∂||A(k) + D||1
∂D
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G(k)

G(k)
i,j = [sign(A(k) + D)]i,j
G(k)
i,j ∈ [−1, 1]

[A(k) + D]i,j (cid:54)= 0
[A(k) + D]i,j = 0

(cid:41)

.

Next, using the chain rule and some known derivatives with subsection 4.3, we for-
mulate the sub-diﬀerential of the quasi-Newton approximation:

(4.10)

∂P (∆; t)
∂∆

=

(cid:40)

1
t

A(k)∆A(k) + A(k) (cid:16)

g(k) (cid:12) M(k)
A

(cid:17)

A(k) + αA(k) (cid:16)

G(k) (cid:12) M(k)
A

(cid:17)

A(k)

G(k) ∈ G(k) (cid:16)

M(k)

A (cid:12) (A(k)∆A(k))

(cid:41)

(cid:17)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where G(k) represents a sub-gradient of the (cid:96)1 term.

The desired ∆(k) that will be used to compute the descent direction is one which

includes the matrix 0 in its sub-gradients:

(4.11)

∆(k) :

0 ∈

∂P (∆(k); t)
∂∆

.

Notice that after ﬁnding ∆(k), it will be used to compute the descent direc-
tion M(k)
A (cid:12) (A(k)∆(k)A(k)). Thus, we can equivalently ﬁnd D(k) =
A(k)∆(k)A(k) instead of ∆(k). In other words, we shall ﬁnd D(k) which has 0 as one

A (cid:12) D(k) = M(k)

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

7

of its sub-gradients:

(4.12) D(k) : 0 ∈

(cid:40)

1
t

D(k) + A(k) (cid:16)

g(k) (cid:12) M(k)
A

(cid:17)

A(k) + αA(k) (cid:16)

G(k) (cid:12) M(k)
A

(cid:17)

A(k)

G(k) ∈ G(k) (cid:16)

M(k)

A (cid:12) D(k)(cid:17)

(cid:41)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Note that the change of variables has made the ﬁrst (Hessian) term trivial. To ﬁnd
D(k), we split the sub-gradient to n2 equations where the (i, j)-th equation is composed
by the (i, j) element in the sub-gradient. For each equation (i, j), we deﬁne the
equation variable as D(k)
i,j . The approximate solution (developed in Appendix A) is:

(4.13)

where

(4.14)

D(k)

i,j = −A(k)

i,j + SoftThreshold

(cid:16)

A(k) − t · B(k)

i,j , t · C (k)

i,j

(cid:17)

,

C (k)

i,j =






(cid:16)

(cid:16)

α ·

α ·

(cid:17)

A(k)
A(k)

i,i · A(k)
i,i · A(k)

,
j,j + A(k)

j,j

i,j · A(k)

j,i

i = j

i (cid:54)= j

,

(cid:17)

,

and
(4.15)
B(k) = A(k) (cid:16)

g(k) (cid:12) M(k)
A

(cid:17)

A(k) + αA(k) (cid:16)

G(k) (cid:12) M(k)
A

(cid:17)

A(k) − C (k) (cid:12)

(cid:16)
G(k) (cid:12) M(k)
A

(cid:17)

.

So far, we developed a second order approximation for the GLASSO using the
Hessian inverse and a quadratic relaxation. We found a closed form direction D(k)
for each (i, j) entry, assuming that the rest of the entries in G(k) are given. In the
following subsection we show how we approximate those entries.

Algorithm 4.1 describes the pISTA method using (4.13). One part that is still
missing from the description so far is the computation of G(k) and the linesearch pro-
cedure for ﬁnding an appropriate t, which we describe next. It is important to note
that all the steps and operations in Algorithm 4.1 can be computed using matrix oper-
ations. This property allows us to easily beneﬁt from GPU computation acceleration
and easy GPU deployment.

Algorithm 4.1 pISTA(S, α, A(0))

1: Result: A
2: Initialization: k = 0;
3: while stop criteria not met do

4:

5:

Compute S (k)
A according to (4.1);
Compute M(k)
A according to (4.2);
Compute G(k) according to (4.16);
Find an appropriate t and compute D(K) according to (4.13);

6:
7:
8: A(k+1) = A(K) + M(k)
9: end while

A (cid:12) D(K);

8

G. SHALOM, E. TREISTER, AND I. YAVNEH

i,j + D(k)

4.3.1. The Approximation of G. The algorithm depends on a good approxi-
mation of G(k), especially at the beginning of the solution process. Recall that G(k)
i,j is
the sub-derivative of |A(k)
i,j |, and although the sub-derivative cannot be com-
i,j + D(k)
puted, it can be approximated by sign(A(k)
i,j ). Eventually, at the late iterations
G(k) (cid:12) M(k)
A will converge to sign(A(k)), as all zero elements in A(k) will be out of the
A . Therefore, their elements in G(k) (cid:12) M(k)
free-set S (k)
A will be zero, and all non-zero
elements in A(k) will have a constant G(k), which is their sign. Moreover, at those
later iterations, where sign(A(k)) is not expected to change at all, the above formula
(4.13) solves (4.12) completely as all the terms except D(k) are constant. On the other
hand, at the initial iterations G(k) (cid:12) M(k)
A cannot be easily computed because some
elements in the free set may change sign or be zero in the ﬁnal solution. To this end,
we approximate G(k) by predicting the “next” sign of the elements. That is, we take
the sign of the non-zero entries, and if an entry is zero, then we take the opposite sign
of its gradient. Thus, a good approximation of G(k) is:

(4.16)

(cid:40)

G(k)
i,j =

sign(A(k)
A(k)
i,j )
i,j ) A(k)
−sign(g(k)

i,j (cid:54)= 0
i,j = 0

,

where g(k) = ∇f (A(k)) as deﬁned before. This means that we choose the next signs
according to a small step of proximal gradient descent. The above G(k) together with
the free-set restriction mask M(k) deﬁne the minimum sub-gradient of F (A(k)) with
respect to (cid:96)2-norm.

4.3.2. Performing Linesearch to ﬁnd t. At each iteration we perform line-

search to obtain a parameter t satisfying:

(4.17)

(4.18)

A(k) + M(k)

A (cid:12) D(k) (cid:31) 0 ,
A (cid:12) D(k)) < F (A(k)) .

F (A(k) + M(k)

5. Convergence Analysis. Throughout this section, unless stated otherwise,
all the matrices are assumed to be symmetric and real. The deﬁnitions of f (A), F (A),
P (∆; t), g, M, G and D are as above. We will denote D∗(k)
as the direction D(k)
which satisﬁes equation (4.12) for a given t:

t

(5.1) 0 ∈

(cid:40)

1
t

t + A(k) (cid:16)
D∗(k)

g(k) (cid:12) M(k)
A

(cid:17)

A(k) + αA(k) (cid:16)

G(k) (cid:12) M(k)
A

(cid:17)

A(k)

G(k) ∈ G(k) (cid:16)

M(k)

A (cid:12) D∗(k)

t

(cid:41)

(cid:17)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Also, denote by G∗(k)

t

the G(k) for which D∗(k)

t

attains 0:

(5.2)

0 =

1
t

t + A(k) (cid:16)
D∗(k)

g(k) (cid:12) M(k)
A

(cid:17)

A(k) + αA(k) (cid:16)

t (cid:12) M(k)
G∗(k)

A

(cid:17)

A(k) .

In the proofs only, we will omit the iteration symbol when the meaning is eveident.

This section is organized as follows. In subsection 5.1 we prove that if our pISTA
iteration does not update A(k), then A(k) = arg minA(cid:31)0 F (A). In subsection 5.2 we
prove that for every iteration there exists a t ≥ tpd > 0 which satisﬁes (4.17) and

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

9

keeps the matrix positive deﬁnite. Lastly, in subsection 5.3 we prove that for every
iteration there exists such a t ≥ tdc > 0 that also satisﬁes (4.18) and decreases our
function value. Combining all the theorems and lemmas we get the following:

Corollary 5.1. By Lemma 5.2, Theorem 5.4, Corollary 5.5, Lemma 5.10,

Lemma 5.12 and Corollary 5.14 - pISTA converges to arg minA(cid:31)0 F (A).

5.1. Fixed point iteration at minimum. The following Lemma 5.2 states
that if a pISTA iteration ends with no update to A(k), then A(k) = arg minA(cid:31)0 F (A).

Lemma 5.2. D∗(k)

t = 0 ⇐⇒ A(k) = arg minA(cid:31)0 F (A).

Proof. First, we will prove that if A is the minimum of F (·), then pISTA will result
in no update. Assume A(k) = arg minA(cid:31)0 F (A), therefore 0 is one of the sub-gradients
of F (A) at A(k):
(5.3)

0 ∈

∂F (A(k))
∂A

(cid:40)

=

g + αG

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Gi,j = sign(Ai,j) Ai,j (cid:54)= 0
Ai,j = 0
Gi,j ∈ [−1, 1]

(cid:41)

(cid:40)

(cid:41)

=

g + αG

G ∈ G(0)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Let G∗ be the sub-gradient G for which g + αG∗ = 0. Thus:

g + αG∗ = 0

⇒(g + αG∗) (cid:12) MA = 0
⇒A ((g + αG∗) (cid:12) MA) A = 0
⇒A (g (cid:12) MA) A + αA (G∗ (cid:12) MA) A = 0 .

If we write this in a sub-diﬀerential form, we get:

(cid:40)

0 ∈

A (g (cid:12) MA) A + αA (G (cid:12) MA) A

(cid:41)

G ∈ G(0)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

which, according to (5.1), means that D∗

t = 0 has a 0 in its sub-gradients.

Now, we will prove that if a pISTA iteration ends with no update, then A is the
t = 0, then, according to (5.2), G∗

F (·) minimum. Assume D∗

t satisﬁes:

A (g (cid:12) MA) A + αA (G∗

t (cid:12) MA) A = 0

⇒A ((g + αG∗
⇒(g + αG∗

t ) (cid:12) MA = 0 .

t ) (cid:12) MA) A = 0 / · A−1

Let us now deﬁne ˆG so that g + α ˆG ∈ ∂F
∂A :

(cid:40)

ˆGi,j =

[G∗
t ]i,j
[g/α]i,j

[MA]i,j (cid:54)= 0
[MA]i,j = 0

.

It is clear that g + α ˆG = 0, because if [MA]i,j = 0, then by deﬁnition Ai,j = 0 and
|g| ≤ α. This implies that A(k) = arg minA(cid:31)0 F (A).

In the proof of Lemma 5.2, no assumptions on t were made, therefore we have

the following:

Corollary 5.3. ∃t > 0 : D∗(k)

t = 0 ⇐⇒ ∀t > 0 : D∗(k)

t = 0.

10

G. SHALOM, E. TREISTER, AND I. YAVNEH

5.2. Satisfying positive deﬁniteness. The following Theorem 5.4 states that
if A(k) (cid:54)= arg minA(cid:31)0 F (A) then there exists a t0 such that every step-size t0 > t > 0
keeps the matrix positive deﬁnite. Thus, as stated in Corollary 5.5, the linesearch
parameter can be bounded away from zero.

Theorem 5.4. For every iteration k, there exists a t0 such that for every t0 >

t > 0: A(k) + M(k)

A (cid:12) D∗(k)

t (cid:31) 0.

Proof. We ﬁrst deﬁne a new constant η:

(5.4)

η = max

Gi,j ∈[−1,1]

||A (g (cid:12) MA) A + αA (G (cid:12) MA) A||2

F .

Using η we can upper-bound the Frobenius norm of the descent step MA (cid:12) D∗
t :

||MA (cid:12) D∗

t ||2

F ≤ ||D∗

t ||2
F

= t2 ||A (g (cid:12) MA) A + αA (G∗

t (cid:12) MA) A||2

F ≤ t2η .

Let t0 = λmin(A)

√

η

, then, for 0 < t < t0:

|λmin(MA (cid:12) D∗

t )| ≤ ||MA (cid:12) D∗

t ||F ≤ t

√

η < λmin(A) ,

and by Weyl’s inequality:

λmin(A + MA (cid:12) D∗

t ) ≥ λmin(A) + λmin(MA (cid:12) D∗
t (cid:31) 0 .

⇒ A + MA (cid:12) D∗

t ) > 0

Corollary 5.5. The linesearch parameter t can be chosen to be bounded away

from zero while keeping the matrix positive deﬁnite, i.e., t ≥ t0

2 > 0.

5.3. Decrease in function objective. In this section we show that under
suitable conditions the sequence {F (A(k))} converges to minA(cid:31)0 F (A) which means
that the sequence {A(k)} converges to arg minA(cid:31)0 F (A). Then, we prove that our
pISTA iteration satisﬁes those conditions, which completes the convergence proof. In
this section we denote the space of symmetric positive deﬁnite matrices by S++.

Denote:

T (∆; t) = P (M(k)

A (cid:12) ∆; t) = f (A(k))

+

+

(cid:68)
g(k), M(k)

A (cid:12)

(cid:16)

A(k)(M(k)

A (cid:12) ∆)A(k)(cid:17)(cid:69)
(cid:69)

(cid:68)
A(k)(M(k)

1
2t
(cid:12)
(cid:12)
(cid:12)A(k) + M(k)
(cid:12)
(cid:12)
(cid:12)

A (cid:12) ∆)A(k), (M(k)
A(k)(M(k)

A (cid:12) ∆)
A (cid:12) ∆)A(k)(cid:17)(cid:12)
(cid:12)
(cid:12)

A (cid:12)

(cid:16)

+ α

(cid:12)
(cid:12)
(cid:12)1

.

We state a relation between arg min∆ T (∆; t) and the descent direction M(k)

A (cid:12) D∗(k)

t

:

Lemma 5.6. Denote ∆∗(k)

t

∆= arg min∆ T (∆; t), then

A(k) (cid:16)

M(k)

A (cid:12) ∆∗(k)

t

(cid:17)

A(k) = M(k)

A (cid:12) D∗(k)

t

.

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

11

Proof. The sub-diﬀerential of T (∆; t) is deﬁned as:

∂T (∆; t)
∂∆

(cid:40)

=

MA (cid:12)

(cid:18) 1
t

A (MA (cid:12) ∆) A + A (g (cid:12) MA) A + αA (G (cid:12) MA) A

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G ∈ G (cid:0)MA (cid:12) (cid:0)A(MA (cid:12) ∆)A(cid:1)(cid:1)

Denote Z = A(MA (cid:12) ∆)A, then we get:

∂T (∆; t)
∂∆

(cid:40)

=

MA (cid:12)

(cid:18) 1
t

Z + A (g (cid:12) MA) A + αA (G (cid:12) MA) A

(cid:19)

(cid:41)

.

(cid:41)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G ∈ G (MA (cid:12) Z)

.

We notice that, for Z ∗

t = MA (cid:12) D∗

t , 0 is in the sub-diﬀerential:

(cid:40)

0 ∈

MA (cid:12)

(cid:18) 1
t

Z ∗

t + A (g (cid:12) MA) A + αA (G (cid:12) MA) A

(cid:19)

(cid:16)

G ∈ G

MA (cid:12) Z ∗(k)

t

(cid:41)

(cid:17)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Thus, if ∆∗

t = arg min∆ T (∆; t), then A (MA (cid:12) ∆∗

t ) A = MA (cid:12) D∗
t .

We next prove that f (A) has a Lipschitz continuous gradient under suitable con-

ditions and state an important property for this kind of functions.

Lemma 5.7. For any square matrices A (cid:31) 0 and B: ||AB||F ≤ ||A||2||B||F .
Proof. Denote by bi the i-th column of matrix B, then:

||AB||2

F =

(cid:88)

i

||Abi||2

2 ≤ ||A||2
2

(cid:88)

i

||bi||2

2 = ||A||2

2||B||2

F .

Lemma 5.8. Let β be a positive constant, then the function f (A) has a Lipschitz
(cid:12)X ∈ S++} with a Lipschitz

continuous gradient over the domain B = {βI (cid:22) X (cid:22) γI(cid:12)
constant of 1

β2 .

Proof. To prove that the function f (A) has a Lipschitz continuous gradient with
a Lipschitz constant of 1
β2 ||X −Y ||F
for every X, Y ∈ B. The gradient is given by ∇f (A) = S − A−1. Thus, for every
X, Y ∈ B:

β2 , we need to prove that ||∇f (X)−∇f (Y )||F ≤ 1

||∇f (X) − ∇f (Y )||F = ||Y −1 − X −1||F

= ||Y −1(X − Y )X −1||F
≤ ||Y −1||2||X − Y ||F ||X −1||2

=

≤

1
λmin(X)

1
λmin(Y )
1
β2 ||X − Y ||F .

||X − Y ||F

12

G. SHALOM, E. TREISTER, AND I. YAVNEH

Corollary 5.9. [Properties of a function with Lipschitz continuous gradient] Let
(cid:12)X ∈ S++}, then for every Y, X ∈ B:

β be a positive constant and B = {βI (cid:22) X (cid:22) γI(cid:12)

f (Y ) ≤ f (X) + (cid:104)∇f (X), (Y − X)(cid:105) +

1
2β2 (cid:104)Y − X, Y − X(cid:105) .

The rest of the proofs are inspired by [27], [16] and [29], following the proofs

of [27] with slight modiﬁcations.

Lemma 5.10 (corresponds to lemma 4.1 in [27]). Let β be a positive constant
(cid:12)X ∈ S++}. Assume that for every iteration k, A(k) ∈ B,

and B = {βI (cid:22) X (cid:22) γI(cid:12)
then:

(5.5)

F (A(k)) − F (A(k+1)) ≥ L · ||A(k) − A(k+1)||2

F .

Where L > 0. Furthermore, the linesearch parameter t can be chosen to be bounded
away from zero, i.e., t ≥ tmin > 0.

Proof. According to pISTA iteration, A(k+1) = A(k) + M(k)

A (cid:12) D∗(k)

t

. Therefore,

F (A(k)) − F (A(k+1)) = F (A(k)) − F (A(k) + M(k)

A (cid:12) D∗(k)

t

) ,

and from now on, we shall omit the iteration symbol k:

F (A(k)) − F (A(k+1)) = F (A) − F (A + MA (cid:12) D∗
t )

= f (A) + α||A||1 − f (A + MA (cid:12) D∗

t ) − α||A + MA (cid:12) D∗

t ||1 .

Using Corollary 5.9 on f (A + MA (cid:12) D∗

t ):

F (A(k)) − F (A(k+1)) ≥ α||A||1 −

(cid:16)

(cid:104)g, MA (cid:12) D∗
t (cid:105)

1
2β2 (cid:104)MA (cid:12) D∗
By the deﬁnition of T (∆; t) and Lemma 5.6:

+

t , MA (cid:12) D∗

t (cid:105) + α||A + MA (cid:12) D∗

t ||1

(cid:17)

.

(cid:104)g, MA (cid:12) D∗

t (cid:105) = T (∆∗

t ; t) −

1
2t

therefore,

(cid:10)MA (cid:12) D∗

t , A−1 (MA (cid:12) D∗

t ) A−1(cid:11) − f (A) ,

F (A(k)) − F (A(k+1)) ≥ f (A) + α||A||1 −

t , MA (cid:12) D∗
t (cid:105)

1
2t
t = arg min∆ T (∆; t), we know that T (∆∗

(cid:10)MA (cid:12) D∗

+

Because ∆∗

F (A(k)) − F (A(k+1)) ≥ f (A) + α||A||1 −

1
2β2 (cid:104)MA (cid:12) D∗
t , A−1 (MA (cid:12) D∗

t ) A−1(cid:11) − T (∆∗

t ; t) .

t ; t) ≤ T (0; t) = f (A) + α||A||1:

1
2β2 (cid:104)MA (cid:12) D∗
t , A−1 (MA (cid:12) D∗

t , MA (cid:12) D∗
t (cid:105)

t ) A−1(cid:11) − T (0; t)

≥ −

+

+

(cid:10)MA (cid:12) D∗

1
2t
1
2β2 (cid:104)MA (cid:12) D∗
1
(cid:10)MA (cid:12) D∗
2t
1
1
2β2 ) (cid:104)MA (cid:12) D∗
2tγ2 −

≥ (

t , MA (cid:12) D∗
t (cid:105)

t , A−1 (MA (cid:12) D∗

t ) A−1(cid:11)

t , MA (cid:12) D∗

t (cid:105) .

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

13

Denote L = 1
chosen to be bounded away from zero, i.e., t ≥ tmin = 1

2β2 , then L is positive as long as t < ( β
2 · ( β

2tγ2 − 1

γ )2. Furthermore, t can be
γ )2 > 0.

Lemma 5.11 (corresponds to lemma 4.2 on [27]). Let β be a positive constant and
B = {βI (cid:22) X (cid:22) γI(cid:12)
(cid:12)X ∈ S++}, and assume that for every iteration k, A(k) ∈ B. Let
{A(kj )} be any inﬁnite and converging sub-series of {A(k)} and let ¯A denote its limit.
Then ¯A = arg minA(cid:31)0 F (A).

Proof. Since the sub-series {A(kj )} convergences to ¯A, then {F (A(kj ))} conver-
gences to F ( ¯A). According to Lemma 5.10, the full series {F (A(k))} is monotone
and hence convergent because {F (A(kj ))} is convergent. Therefore, {F (A(kj +1)) −
F (A(kj ))} → 0, which implies following (5.5) that ||A(kj ) − A(kj +1)||2
F → 0 and
limj→∞ A(kj +1) = ¯A. According to pISTA, A(k+1) = A(k) + M(k)
and be-
t
cause M(k)
||2
F → 0. We know that
D∗(k)
satisﬁes (5.1), by taking the limit as j → ∞ and using Lemma 5.2 we get that
t
¯A = arg minA(cid:31)0 F (A).

A (cid:54)= 0, ||A(kj ) − A(kj +1)||2

F → 0 implies ||D∗(k)

A (cid:12) D∗(k)

t

Lemma 5.12. [corresponds to lemma 4.3 on [27]] Let β be a positive constant and
(cid:12)X ∈ S++}. Assume that for every iteration k, A(k) ∈ B. Then

B = {βI (cid:22) X (cid:22) γI(cid:12)
the series {A(k)} has a limit and it is given by arg minA(cid:31)0 F (A).

Proof. The domain B is compact, the rest of the proof follows the proof of lemma

4.3 in [27] exactly.

We have shown that pISTA converges under the condition of bounded eigenvalues. It
remains to show that pISTA satisﬁes it.

Lemma 5.13 (corresponds to lemma 2 on [16]). Let U = {A|F (A) ≤
(cid:12)X ∈ S++} where

F (A0) and A ∈ S++}. Then, U ⊆ B = {βI (cid:22) X (cid:22) γI(cid:12)
β > 0.

Proof. See lemma 2 in [16].
Corollary 5.14. The series {A(k)} created by the pISTA algorithm satisﬁes

(4.18), hence, A is contained in U and in B.

6. Numerical Results. We compare the performance of our pISTA algorithm
to the performance of GISTA [14], Orthant-Based Newton (OBN) [22] and ALM [24]
(without skipping step1), all of which can be implemented eﬃciently and easily on
GPU based on matrix operations. We note that Newton-Lasso [22], VSM [21] and
PSM [9] can be implemented in the same manner on GPU too, but all of them have
worse performance than OBN [22] and ALM [24] according to the authors’ measure-
ments. Other algorithms, such as BIG&QUIC [17] and BCD-IC [26], can be imple-
mented on GPU as well. However, their implementations might be highly complex
due to the diﬀerences between the CPU parallelism model and the GPU parallelism
model, or their implementations might have worse results than the CPU counterparts
due to the numerous scalar operations. The algorithms are all implemented in Python
as recommended by their authors using 32bit ﬂoating point precision. Each algorithm
is implemented using the GPU as eﬃciently as possible with the Cupy library.

We initialize all methods using GISTA [14] initialization where:

(6.1)

A(0)

i,j =

(cid:26) (Si,i + α)−1

0

i = j
i (cid:54)= j

.

1As implemented by the authors: https://www.math.ucdavis.edu/∼sqma/ALM-SICS.html

14

G. SHALOM, E. TREISTER, AND I. YAVNEH

For ALM only, we consider any number which is less than or equal to 10 · (cid:15)machine
as zero and set those numbers to zero at end of each iteration. For OBN we use 10
inner iterations, and for ALM we use the hyperparamters used by [24] except that we
set µ0 = 1
α if α < 0.5. For our pISTA algorithm, we limit our linesearch over t by
stopping if t is less than 10−4. In that case, we use a step of t ≤
which
keeps the matrix positive deﬁnite. As proved in subsection 5.3, we can use a t < ( β
γ )2
where β and γ are lower and upper bounds of the eigenvalues, respectively. However,

0.9
cond(A(k))

(cid:17)2

(cid:16)

we cannot compute those bounds, so we assume that
should satisfy the linesearch criteria.

(cid:16)

0.9
cond(A(k))

(cid:17)2

is suﬃcient and it

As a stopping criterion for all the method, we follow [17, 26, 27] and use
is the minimum sub-

minz ||∂F (A(k)||1 < 10−2||A(k)||1, where minz ||∂F (A(k)||
gradient norm.

All experiments were run on a machine with Intel(R) Xeon(R) Gold 6230
2.10GHz processor with 4 cores, 20GB RAM, GeForce RTX 2080 Ti GPU and
Ubuntu 18.04.5 operating system. We also use Python3.9.2, CUDA11.0, Numpy1.20.1
and Cupy9.0. Our full python framework and code can be found at

https://github.com/GalSha/GLASSO_Framework.

6.1. Synthetic Experiments. First, we evaluate the algorithms on synthetic

data. We use three diﬀerent types of matrices as our ground truth:

• Chain graphs: as described in [16]. The ground truth matrix Σ−1 is set to

be Σ−1

i,i = 1 and Σ−1

i,i+1 = Σ−1

i,i−1 = −0.5.

• Graphs with random sparsity structures: as described in [20]. We generate a
sparse matrix U with non-zero elements set to be +1 or −1. We deﬁne the
ground truth as Σ−1 = U T U where all oﬀ diagonal elements are in [−1, 1].
We control the number of non-zeros in U and tune it such that Σ−1 has
approximately 0.5% non-zeros.

• Planar graphs: as described in [27]. First we create our graph G(V, E). We
generate n random points on the unit square to be our vertices V . We use
Delaunay triangulation to generate our edges E. Given the graph connectiv-
ity, we deﬁne Σ−1 as its graph Laplacian, i.e., Σ−1
i,j = −1 if (i, j) ∈ E and
Σ−1

i,i = di where di is the degree of vertex i.

To ensure that the matrices are positive deﬁnite, we add a predeﬁned diagonal term
of max{−1.2λmin, 10−1} · I.

We do two sets of experiments, with n = 1, 000 and with n = 10, 000. In each
set we draw 3% · n of samples and run the algorithms with two diﬀerent values of α.
We repeat each experiment ﬁve times and show the average results in Table 1 and
Table 2 for the ﬁrst set and second set, respectively. Also, we compare the results of
a CPU only implementation of our pISTA algorithm for both sets of experiments.

In the tables, we show the average time and iterations it took for each algorithm
to reach the stopping criterion. We also show the number of non-zeros in the output
matrix, and the minimum sub-gradient Frobenius norm.

From Table 1, we see that pISTA outperforms every other algorithm with respect
to running time on all the low-dimension matrices except one. We notice that pISTA
requires less iterations than GISTA and more iterations than OBN. This is expected
as GISTA is a ﬁrst order method, OBN is a second order method, and the pISTA al-
gorithm is a quasi-Newton method. Thus, pISTA convergence rate should be between

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

15

Problem Parameters

pISTA (GPU)

n
m/n
α

1,000
3%
0.6
1,000
3%
0.4
1,000
3%
0.6
1,000
3%
0.4
1,000
3%
0.6
1,000
3%
0.4

Σ−1 type
|Σ−1|0

Chain
2998

Chain
2998

Random
5936

Random
5936

P lanar
6958

P lanar
6958

0.04s (2.0)
0.0017
2959.2
0.10s (6.6)
0.0142
25307.2
0.04s (2.2)
0.0265
2184.0
0.10s (6.4)
0.0666
26335.2
0.03s (2.0)
0.0008
2995.6
0.24s (15.4)
0.0244
28495.2

pISTA (CPU)

GISTA
time (iter)
minz ||∂F ( ˜Σ−1)||F
|| ˜Σ−1||0
0.05s (3.0)
0.0275
2970.4
0.15s (10.6)
0.0336
25302.0
0.06s (3.6)
0.0462
2191.2
0.21s (15.2)
0.0286
26400.4
0.04s (2.4)
0.0269
3011.6
0.28s (20.2)
0.0241
28623.2

0.35s (2.0)
0.0017
2959.2
1.40s (6.6)
0.0142
25307.2
0.32s (2.2)
0.0265
2184.0
1.05s (6.4)
0.0666
26335.2
0.31s (2.0)
0.0008
2995.6
3.05s (15.6)
0.0244
28471.6

OBN

ALM

0.05s (2.0)
0.0020
2959.2
0.11s (4.0)
0.0003
25246.0
0.06s (2.2)
0.0824
2185.2
0.13s (4.6)
0.0005
26246.4
0.05s (2.0)
0.0028
2997.6
0.15s (5.6)
0.0130
28290.4

0.55s (11.0)
0.0312
2966.0
2.47s (53.8)
0.5966
25246.8
0.61s (14.0)
0.8561
2209.0
2.41s (53.4)
2.5377
26256.8
0.47s (10.2)
0.3156
3039.6
3.13s (69.6)
1.8171
28312.8

Table 1: Results for 1, 000 × 1, 000 Precision Matrix
Bold text marks the best results

Problem Parameters

pISTA (GPU)

n
m/n
α

10,000
3%
0.4
10,000
3%
0.2
10,000
3%
0.4
10,000
3%
0.2
10,000
3%
0.4
10,000
3%
0.2

Σ−1 type
|Σ−1|0

Chain
29,998

Chain
29,998

Random
508,787

Random
508,787

P lanar
69,949

P lanar
69,949

5.69s (3.0)
0.0101
34685.2
12.89s (7.0)
0.0951
97765.6
1.56s (0.6)
2.1558
10823.6
5.67s (3.0)
4.3416
140699.6
3.92s (2.0)
1.2172
63249.6
20.91s (10.2)
1.4050
194261.6

pISTA (CPU)

GISTA
time (iter)
minz ||∂F ( ˜Σ−1)||F
|| ˜Σ−1||0
10.80s (6.0)
0.1478
35104.0
27.53s (16.6)
0.8418
98432.8
1.85s (0.6)
3.0901
10823.6
15.95s (9.0)
0.8136
140869.6
8.84s (5.0)
0.0477
63446.8
63.17s (35.8)
0.5368
195714.0

OBN

ALM

13.85s (3.0)
0.0060
34839.2
20.02s (4.0)
0.0129
97881.2
1.60s (0.6)
2.1558
10823.6
13.97s (3.0)
1.0047
140715.6
13.73s (3.0)
0.1113
63324.0
41.54s (7.6)
1.3284
192740.0

103.44s (28.6)
28.5176
34879.6
168.46s (48.0)
24.0052
98339.2
767.79s (237.4)
44.2140
11072.0
153.16s (44.2)
21.1711
141009.8
100.34s (28.2)
6.6973
63572.8
269.97s (79.4)
12.4953
193075.0

249.18s (3.0)
0.0101
34685.2
578.37s (7.0)
0.0951
97765.6
68.74s (0.6)
2.1558
10823.6
263.86s (3.0)
4.3416
140699.6
174.70s (2.0)
1.2172
63249.6
943.43s (10.2)
1.4049
194261.2

Table 2: Results for 10, 000 × 10, 000 Precision Matrix
Bold text marks the best results

GISTA and OBN. For small α, OBN achieves a better minimum sub-gradient and
better sparsity, however, the diﬀerence is negligible. Also, the GPU implementation
of pISTA is up to ×14 times faster than its CPU counterpart, proving the desirability
of GPU centric algorithms.

16

G. SHALOM, E. TREISTER, AND I. YAVNEH

In Table 2, we see similar results. pISTA outperforms every other algorithm
with respect to running time on all matrices. We notice that pISTA also achieves
better sparsity patterns than the other algorithms. As expected, pISTA requires less
iterations than GISTA and more iterations than OBN. For high-dimension matrices,
the speedup of the GPU over CPU is up to ×44, making it extremely valuable in high
dimensions.

)
A
(
F
n
i
m
−
)
1
−
˜Σ
(
F

104

100

10−4

pISTA
GISTA
OBN
ALM

0

20

40

Iteration

)
A
(
F
n
i
m
−
)
1
−
˜Σ
(
F

102

10−1

10−4

0

pISTA
GISTA
OBN
ALM

20
Iteration

40

(a) Chain Precision Matrix

(b) Random Precision Matrix

)
A
(
F
n
i
m
−
)
1
−
˜Σ
(
F

104

100

10−4

0

pISTA
GISTA
OBN
ALM

20

60

40
Iteration

80

(c) Planar Precision Matrix

Fig. 1: Semilog plot of F ( ˜Σ−1) − minF (A) as function of
iterationa for 10, 000 × 10, 000 Precision Matrix with
α = 0.2

aThe iterations are not equal in complexity or time between the algorithms

In Figure 1, we show a semi-log plot of F ( ˜Σ−1) − minF (A) as a function of the
iteration, however, it is important to note that the iterations are not equal in com-
plexity or required time. We only present the plots of the 10, 000 × 10, 000 Precision
Matrices for α = 0.2 because of space consideration. We deﬁne minF (A) as the
minimum value achieved for F among all the algorithms and iterations in that exper-
iment. The plots show what we expect, pISTA, as a quasi-Newton method, achieves
a convergence rate between linear (GISTA) and quadratic (OBN). Moreover, we see
that the convergence rate is quadratic in the ﬁrst few iterations.

6.2. Real World Data Experiments. For real world data we use gene expres-
sion data sets that are available at the Gene Expression Omnibus http://www.ncbi.
nlm.nih.gov/geo/. We preprocess the data to have zero mean and unit variance for
each variable, i.e., diag(S) = I. Table 3 shows the results for data sets of various sizes
including the name codes of the data sets used.

In Table 3, we see that there is no silver bullet. There are some problems where

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

17

Problem Parameters

pISTA

GISTA

OBN

ALM

Data set

GSE-3016

GSE-3016

GSE-3016

GSE-26242

GSE-26242

GSE-26242

GSE-7039

GSE-7039

GSE-7039

GSE-52076

GSE-52076

GSE-52076

n
m/n
α

1322
4.77% (63)
0.85
1322
4.77% (63)
0.75
1322
4.77% (63)
0.65
1536
6.25% (96)
0.85
1536
6.25% (96)
0.75
1536
6.25% (96)
0.65
6264
3.99% (250)
0.9
6264
3.99% (250)
0.8
6264
3.99% (250)
0.7
11064
5.39% (596)
0.9
11064
5.39% (596)
0.8
11064
5.39% (596)
0.7

time (iter)
minz ||∂F ( ˜Σ−1)||F
|| ˜Σ−1||0

0.09s (3)
0.1915
3478
0.14s (5)
0.0092
10796
1.02s (33)
0.0134
20682
0.08s (2)
0.0118
4876
0.45s (14)
0.0003
13896
1.81s (32)
0.0096
28494
2.31s (4)
0.0397
34286
10.42s (14)
0.0566
49200
41.87s (41)
0.0206
70700
9.78s (4)
0.0027
32840
10.05s (4)
0.1162
42852
17.34s (7)
0.3950
49612

0.11s (3)
0.0378
3560
0.35s (14)
0.0049
10890
1.55s (66)
0.0158
20660
0.15s (4)
0.0186
4962
0.70s (24)
0.0056
13900
2.43s (83)
0.0148
28942
8.52s (15)
0.0798
34364
39.91s (69)
0.1218
50020
101.95s (166)
0.1302
70820
30.65s (13)
0.1908
32850
55.54s (23)
0.2827
43086
167.93s (65)
0.3866
49644

0.16s (3)
0.2185
3490
0.32s (6)
0.0025
10806
0.66s (11)
0.0016
20386
0.14s (2)
0.0103
4862
0.47s (7)
0.0001
13798
1.01s (13)
0.0087
28148
5.92s (4)
0.0014
34250
9.39s (6)
0.0449
49054
16.39s (10)
0.0012
70572

Out of GPU memory

Out of GPU memory

Out of GPU memory

1.01s (18)
0.2921
3596
2.06s (39)
8.5051
10809
3.17s (59)
0.0011
20440
1.67s (26)
0.0081
4912
2.39s (40)
3.5156
13810
4.28s (66)
6.7166
28182
74.19s (77)
33.9381
34266
*s (>1000)
*
*
*s (>1000)
*
*
103.10s (23)
23.4233
32990
168.27s (39)
71.0796
43471
317.99s (75)
58.4818
49840

Table 3: Results for real world data set
Bold text marks the best results

pISTA will outshine and there are some problems where OBN will outshine. How-
ever, OBN requires much more memory than pISTA, making it hard to use in high
dimension on GPUs.

7. Conclusions. In this work we presented a quasi-Newton method for solving
sparse inverse covariance estimation problem. Our method creates simpliﬁed approx-
imate second order optimization where the diﬀerent variables dependency can be
guessed in a simple manner. Moreover, the method is designed to be implemented us-
ing matrix operations which can be done on a GPU, allowing us to solve the problem
eﬃciently. We showed the desirability of GPU implementations and that our pISTA
method has better results for various problems structures.

18

G. SHALOM, E. TREISTER, AND I. YAVNEH

Appendix A.

A.1. General solution for sign derivative equation. Deﬁne T (x):

x (cid:54)= 0 :

x = 0 :

T (x) = sign(x)

T (x) ∈ [−1, 1] .

Lemma A.1. The solution for:

(A.1)

is given by:

(A.2)

x =

x + b + cT (x + a) = 0,

c > 0






c < b − a
−b + c
−a
c > |b − a|
−b − c −c > b − a

= −a + SoftThreshold(a − b, c) .

Proof. First, consider the case where x > −a ⇒ T (x + a) = 1:
(cid:26) x + b + c = 0
x > −a

(cid:26) x = −b − c
x > −a

(cid:26) x = −b − c

−b − c > −a

⇒

⇒

⇒

(cid:26) x = −b − c
−c > b − a

.

In a similar way, for x < −a ⇒ T (x + a) = −1 and we get:

(cid:26) x = −b + c
c < b − a

.

Lastly, consider the case that x = −a, then since c > 0 we get:

0 ∈ −a + b + c · t, t ∈ [−1, 1]

which leads to

a − b ∈ [−c, c] ⇒ c ≤ |a − b| ⇒ c ≤ |b − a| .

A.2. Approximate solution for the pISTA sub-gradient equation. Con-

sider the following problem:

(A.3) D :

0 ∈

(cid:40)

1
t

D + A (g (cid:12) MA) A + αA (G (cid:12) MA) A

Gi,j = [sign(A + MA (cid:12) D)]i,j
Gi,j ∈ [−1, 1]

[A + MA (cid:12) D]i,j (cid:54)= 0
[A + MA (cid:12) D]i,j = 0

(cid:41)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Let us split the equation into n2 scalar equations where the (i, j)-th equation is com-
posed of the elements with indices (i, j). Denote the equation variable Di,j = x. The
(i, j) equation is:

(A.4)

1
t

x + [A (g (cid:12) MA) A]i,j + α [A (G (cid:12) MA) A]i,j = 0 ,

where Gi,j = T (Ai,j + [MA]i,j · x). As we consider equations where [MA]i,j (cid:54)= 0 (the
rest are zeros), we can use Gi,j = T (Ai,j + x). Deﬁne:

(A.5)

G−i,−j
k,l

=






0
0
Gk,l

k = i, l = j
k = j, l = i
otherwise

.

PRECONDITIONED ISTA FOR GRAPHICAL LASSO

19

We can write (A.4) as:

1
t

(A.6)

where

(A.7)

x + [A (g (cid:12) MA) A]i,j + α (cid:2)A (cid:0)G−i,−j (cid:12) MA

(cid:1) A(cid:3)

i,j + Ci,jT (Ai,j + x) = 0 ,

Ci,j =

(cid:26) α · (Ai,i · Aj,j) ,

α · (Ai,i · Aj,j + Ai,j · Aj,i) ,

i = j
i (cid:54)= j

is the diagonal entry of the Kronecker matrix α · A ⊗ A corresponding to the entry
(i, j). Note that Ci,j > 0 since α > 0 and A(k) is symmetric positive deﬁnite, and its
diagonal is strictly positive. According to Lemma A.1, the solution to (A.6) is:

x = −Ai,j + SoftThreshold

(cid:16)

Ai,j−t ·

(cid:16)

[A (g (cid:12) MA) A]i,j

+α (cid:2)A (cid:0)G−i,−j (cid:12) MA

(cid:17)

(cid:1) A(cid:3)

i,j

, t · Ci,j

(cid:17)

.

Notice that:

(A.8)

α (cid:2)A (cid:0)G−i,−j (cid:12) MA

(cid:1) A(cid:3)

i,j = α [A (G (cid:12) MA) A]i,j − Ci,j · Gi,j · [MA]i,j
= [αA (G (cid:12) MA) A − C (cid:12) (G (cid:12) MA)]i,j ,

where (cid:12) is the Hadmard product. To write (A.2) more compactly, ﬁrst deﬁne:

B = A (g (cid:12) MA) A + αA (G (cid:12) MA) A − C (cid:12) (G (cid:12) MA) ,

then we get

x = −Ai,j + SoftThreshold(Ai,j − t · Bi,j, t · Ci,j)
⇒ D = −A + SoftThreshold(A − t · B, t · C) .

REFERENCES

[1] O. Banerjee, L. El Ghaoui, and A. d’Aspremont, Model selection through sparse maximum
likelihood estimation for multivariate gaussian or binary data, J. Mach. Learn. Res., (2008).
[2] O. Banerjee, L. E. Ghaoui, A. d’Aspremont, and G. Natsoulis, Convex optimization tech-
niques for ﬁtting sparse gaussian graphical models, in Proceedings of the 23rd International
Conference on Machine Learning, 2006, p. 89–96.

[3] A. Beck and M. Teboulle, A fast iterative shrinkage-thresholding algorithm for linear inverse

problems, SIAM Journal on Imaging Sciences, (2009), pp. 183–202.

[4] M. Bollhofer, A. Eftekhari, S. Scheidegger, and O. Schenk, Large-scale sparse inverse
covariance matrix estimation, SIAM Journal on Scientiﬁc Computing, (2019), pp. A380–
A401.

[5] X. Chen, Y. Liu, H. Liu, and J. Carbonell, Learning spatial-temporal varying graphs with
applications to climate data analysis, AAAI Conference on Artiﬁcial Intelligence, (2010).
[6] A. d’Aspremont, O. Banerjee, and L. Ghaoui, First-order methods for sparse covariance

selection, SIAM Journal on Matrix Analysis and Applications, (2006).

[7] A. P. Dempster, Covariance selection, Biometrics, (1972), pp. 157–175.
[8] A. Dobra, C. Hans, M. Jones, J. Nevins, G. Yao, and M. West, Sparse graphical models
for exploring gene expression data, Journal of Multivariate Analysis, (2004), pp. 196–212.
[9] J. Duchi, S. Gould, and D. Koller, Projected subgradient methods for learning sparse gaus-

sians, in Uncertainty in Artiﬁcial Intelligence, 2008, p. 153–160.

[10] J. Fan, J. Zhang, and K. Yu, Vast portfolio selection with gross-exposure constraints, Journal

of the American Statistical Association, (2012), pp. 592–606.

[11] S. E. Finder, E. Treister, and O. Freifeld, Eﬀective learning of a GMRF mixture model,

IEEE Access, (2022).

20

G. SHALOM, E. TREISTER, AND I. YAVNEH

[12] J. Friedman, T. Hastie, and R. Tibshirani, Sparse inverse covariance estimation with the

graphical lasso, Biostatistics, (2007), pp. 432–441.

[13] A. Goldenberg and A. W. Moore, Bayes net graphs to understand co-authorship networks?,
in Proceedings of the 3rd International Workshop on Link Discovery, 2005, p. 1–8.
[14] D. Guillot, B. Rajaratnam, B. Rolfs, A. Maleki, and I. Wong, Iterative thresholding
algorithm for sparse inverse covariance estimation, Advances in Neural Information Pro-
cessing Systems, (2012).

[15] T. Hastie, R. Tibshirani, and M. Wainwright, Statistical Learning with Sparsity: The Lasso

and Generalizations, Chapman & Hall/CRC, 2015.

[16] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar, Quic: Quadratic approximation
for sparse inverse covariance estimation, Journal of Machine Learning Research, (2014).
[17] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, P. K. Ravikumar, and R. Poldrack, Big &
quic: Sparse inverse covariance estimation for a million variables, in Neural Information
Processing Systems, 2013, pp. 3165–3173.

[18] T. Id´e, A. C. Lozano, N. Abe, and Y. Liu, Proximity-Based Anomaly Detection using Sparse

Structure Learning, 2009, pp. 97–108.

[19] S. Lauritzen, Graphical models, Oxford Statistical Science Series, Clarendon Press, 1996.
[20] L. Li and K.-C. Toh, An inexact interior point method for l1-regularized sparse covariance

selection, Mathematical Programming Computation, (2010), p. 291–315.

[21] Z. Lu, Smooth optimization approach for sparse covariance selection, SIAM Journal on Opti-

mization, (2009).

[22] P. A. Olsen, F. Oztoprak, J. Nocedal, and S. J. Rennie, Newton-like methods for sparse
inverse covariance estimation, in Neural Information Processing Systems, 2012, p. 755–763.
[23] H. Rue and L. Held, Gaussian Markov random ﬁelds: theory and applications, CRC press,

2005.

[24] K. Scheinberg, S. Ma, and D. Goldfarb, Sparse inverse covariance selection via alternating

linearization methods, in Neural Information Processing Systems, 2010, p. 2101–2109.

[25] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Statistical

Society. Series B (Methodological), (1996), pp. 267–288.

[26] E. Treister and J. S. Turek, A block-coordinate descent approach for large-scale sparse

inverse covariance estimation, in Neural Information Processing Systems (NIPS), 2014.

[27] E. Treister, J. S. Turek, and I. Yavneh, A multilevel framework for sparse optimization
with application to inverse covariance estimation and logistic regression, SIAM Journal on
Scientiﬁc Computing, 38 (2016), pp. S566–S592.

[28] E. Treister and I. Yavneh, A multilevel iterated-shrinkage approach to l {1} penalized least-
squares minimization, IEEE transactions on signal processing, 60 (2012), pp. 6319–6329.
[29] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, Sparse reconstruction by separable
approximation, IEEE Transactions on Signal Processing, (2009), pp. 2479–2493.
[30] X. Xuan and K. Murphy, Modeling changing dependency structure in multivariate time
series, in Proceedings of the 24th International Conference on Machine Learning, 2007,
p. 1055–1062.

[31] M. Yuan and Y. Lin, Model selection and estimation in the gaussian graphical model,

Biometrika, (2007), pp. 19–35.

