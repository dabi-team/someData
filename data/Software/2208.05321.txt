2
2
0
2

g
u
A
8

]

R

I
.
s
c
[

1
v
1
2
3
5
0
.
8
0
2
2
:
v
i
X
r
a

A Frequency-aware Software Cache for Large Recommendation
System Embeddings

Jiarui Fang
HPC-AI Technology Inc.
fangjr@hpcaitech.com

Geng Zhang∗
School of Computer Science, Wuhan
University
zhangg@whu.edu.cn

Jiatong Han
HPC-AI Technology Inc.
jiatong.han@hpcaitech.com

Shenggui Li, Zhengda Bian,
Yongbin Li
HPC-AI Technology Inc.
{lisg,bian.zhengda,ybl}@hpcaitech.com

Jin Liu
School of Computer Science, Wuhan
University
jinliu@whu.edu.cn

Yang You†
National University of Singapore
(NUS)
youy@comp.nus.edu.sg

ABSTRACT
Deep learning recommendation models (DLRMs) have been widely
applied in Internet companies. The embedding tables of DLRMs are
too large to fit on GPU memory entirely. We propose a GPU-based
software cache approaches to dynamically manage the embedding
table in the CPU and GPU memory space by leveraging the id’s fre-
quency statistics of the target dataset. Our proposed software cache
is efficient in training entire DLRMs on GPU in a synchronized
update manner. It is also scaled to multiple GPUs in combination
with the widely used hybrid parallel training approaches. Evaluat-
ing our prototype system shows that we can keep only 1.5% of the
embedding parameters in the GPU to obtain a decent end-to-end
training speed.

1 INTRODUCTION
The DLRMs are important machine learning applications that offer
a personalized user experience in industry [5, 9, 10]. DLRMs are
typically based on both dense and sparse features. The embedding
table is used to handle sparse features.

Embedding tables are growing larger nowadays because an in-
crease in embedding capacity usually leads to an improvement in
recommendation quality. Currently, large embeddings can easily
scale up to Trillions of parameters [6]. The embedding tables are
too large to fit on GPU memory entirely. Existing solutions resort
to heterogeneous training typically involving CPU computation
of the embedding part and GPU computation of the dense part.
However, the random embedding lookups on the CPU can be an
order of magnitude slower than on that the GPU. To hide the CPU
computational overhead, They often introduce asynchronous up-
date strategies, which are notorious for their uncertain convergence
rate.

This work focuses on using homogeneous training of the en-
tire model solely on GPU in a synchronous updating manner. To
meet huge memory requirements, we apply a software cache to
dynamically move active embedding rows between CPU and GPU.
Different from the existing software cache approaches, like the
Unified Virtual Memory (UVM) adopted in TorchRec, our cache
leverages the dataset id frequency statistics to improve the cache

hit ratio and applies a set of optimization to both reduce cache
indexing overhead and improve PCI-e bandwidth utilization. Our
contributions are listed as follows.

• We proposed a software cache approach to manage embed-
ding parameters in hybrid memory space including CPU and
GPU. Our cache-related operations are parallel executed on
GPU. Therefore has a limited impact on the overall perfor-
mance. The cache utilizes the id frequency statistic of the
target dataset to improve the cache hit ratio.

• We scale the embedding using the cache to multiple GPUs

using hybrid parallel training approaches.

• Our code is publicly available at url 1.

2 RELATED WORK
2.1 Deep Learning Recommendation Model
Deep Learning Recommendation Model (DLRM)[9] is a widely-
adopted recommendation model that is able to take in both sparse
and dense features that are essentially useful for predicting user-
item matches. An embedding layer is used for sparse feature embed-
ding look-ups, and our work centers around adapting embedding
layer such that model training can be supported even with ex-
tremely large embedding sizes and relatively small GPU memory.
There are also dense layers in DLRM that process dense inputs
that are usually continuous, and an over-arch layer that processes
combined results from dense layers and embedding layer. They will
not be our focus in this work.

2.2 Large-Scale Embedding Training
2.2.1 Embedding Table Sharding. There are multiple approaches
to supporting giant embedding layer training in DLRM. One tradi-
tional and universally adopted approach is to use Tensor Parallelism
(TP) that shards embedding tables in a specific manner (such as
table-wise, or column-wise) onto multiple GPUs, and combines the
look-up results using an all-reduce operation before proceeding
with subsequent model steps. TorchRec2, as a recently released
implementation of work [8], has such functionality of embedding
table sharding and TP.

∗The work is done when Geng is at HPC-AI Technology Inc.
†corresponding author. Yang You is a faculty member at NUS; this work was done at
HPC-AI Technology Inc.

1https://github.com/zxgx/FreqCacheEmbedding
2https://github.com/pytorch/torchrec

 
 
 
 
 
 
Jiarui Fang, Geng Zhang, Jiatong Han, Shenggui Li, Zhengda Bian, Yongbin Li, Jin Liu, and Yang You

2.2.2 Hybrid CPU-GPU Embedding Storage. Orthogonal to the
above method, there are also works in distributing embedding ta-
bles onto GPU and CPU main memory, as GPU main memory is
often expensive and restricted in size compared to CPU main mem-
ory. This implementation is often termed cached embedding or
parameter server embedding by recent papers. Persia [6] designs a
parameter server that resides in CPU memory and communicates
and manages the embedding weights updates informed by model
training happening in GPU. It is shown to be able to support tril-
lions of parameters training. There have also been works [1, 4, 7]
that observe an empirical long-tail distribution of look-up queries,
designing frequency-aware cache strategies that share a similar
underlying motivation to ours. Hotline [2] makes effort to acceler-
ate CPU-GPU communication by designing a specialized hardware
scheduler that prioritizes more frequently queried embeddings and
delays less frequent ones.

Specific to DLRM, there are both embedding layers and dense
layers, which are I/O-intensive and compute-intensive respectively.
Persia [6] put all embedding querying duty on CPU while putting
dense layers computation duty entirely on GPU, which in this paper
is termed as heterogeneous training. Compared to homogeneous
training that uses GPU or CPU only, heterogeneous training can
better leverage the CPU and GPU resources. We further this effort
and put frequently queried embedding cache in GPU and use GPU
parallelized operations to speed up embedding look-ups.

Synchronous Embedding Updates. During actual model train-
2.2.3
ing, embedding weights are updated with each training epoch end-
ing synchronously. One simple idea to speed up training is to update
embedding weights asynchronously and overlap the step with the
next training step as implemented in HET [7]. However, we take
a synchronous approach finally to avoid uncertainty convergence
and possible model accuracy loss.

3 MOTIVATIONS
Heterogeneous training is the most popular way to meet exces-
sive memory requirements for the training of embeddings. Because
embedding tables are data-intensive operations that featured large
memory requirements and small computation requirements, pre-
vious work [2, 6, 7] proposed to execute the embedding tables on
CPU, and execute the remainder dense DNN part on GPU. In this
way, communication between CPU and GPU via PCI-e is the main
performance bottleneck. Work [6, 7] heavily relies on asynchronous
update strategies to overlap the computation and data transfer. The
asynchronous updates introduce uncertainty to the convergence
rate of the training, which is unfriendly to the system users.

This work focuses on applying the homogeneous training ap-
proaches to training embedding tables entirely on GPU. We observe
that running embedding on GPU has significant gains compared
to CPU. Due to the irregular memory access pattern of embedding
lookup, the hardware cache on the CPU benefits little from the
training embedding tables. Therefore, the memory access speed of
embedding is largely limited by the bandwidth of the main memory
bus, and the bandwidth of High Bandwidth Memory (HBM) on
GPU is orders of magnitude higher than the bandwidth of DDR4
or DDR5 memory on CPU. Figure 1 shows around 50x speedup on
GPU to CPU executing PyTorch EmbeddingBag. In the benchmark,

the CPU is AMD EPYC 7543 32-Core processor and the GPU is
NVIDIA A100. From the above benchmark data, it is implied that
even if there are 10% cache misses during the training process, there
will be a 500% increase in computing latency even if we still do not
consider the data transmission.

Figure 1: Speedup of GPU to CPU on EmbeddingBags execu-
tion using PyTorch.

Software cache is necessary considering the large embedding
table sizes (usually tens of GB, at most tens of TB) that can hardly
fit into GPU memory. With such an approach, CPU memory serves
as the main embedding storage that holds all of the embedding
data, while the GPU only stores a small faction of the embedding
data to be involved in computation in the near future. With similar
motivation, TorchRec adopts a Unified Virtual Memory (UVM) 3 as
the software cache. It provides a single, unified memory address
space for CPU and GPU and automatically replaces and evicts
unused pages. However, this approach brings extra overhead from
the data transmission process which operates at the granularity of
embedding rows, leaving the bandwidth of PCI-e not sufficiently
utilized. With the extra overhead, the software cache approach
UVM is not ideal in its cost efficacy, and thus insufficiently good for
DLRM. And as a result, some work resorts to hardware solutions
to implement a fast caching approach [2, 8].

The question is: Can we implement an efficient software
cache on embeddings to implement homogeneous GPU com-
pute for DLRMs without extra hardware accelerators? We
solve the performance issue of the software cache from two view-
points. First, increase the cache hit ratio. It has been reported that
there exist skewed popularity distributions of embeddings [7]. As
shown in the Figure 2, a very small number of popular ids occur
very often. For example, the top 0.14% and top 0.012% popular em-
bedding rows in the Criteo dataset and Avazu account for 90% total
number of updates. The observation motivates us to reduce the
communication by caching frequently updated embeddings rows
into the GPU memory. Second, reduce caching overhead, includ-
ing cache key indexing and data transmission cost. To reduce key
indexing overhead, we design a parallel caching method that gets
executed on multiple GPUs. To increase the bandwidth utilization
of data transmission, we increase the message size by converting
transmission from row-wise to block-wise into the cache.

3https://developer.nvidia.com/blog/unified-memory-cuda-beginners/

GPU/CPU speedup020406080(batch size, num emebdding, dim)(16K, 30K, 64)(16K, 300K, 64)(16K, 600K, 64)(16K, 30K, 128)(16K, 300K, 128)(16K, 600K, 128)(32K, 30K, 64)(32K, 300K, 64)(32K, 600K, 64)(32K, 30K, 128)(32K, 300K, 128)(32K, 600K, 128)A Frequency-aware Software Cache for Large Recommendation System Embeddings

Figure 2: The id frequency distribution in Criteo Kaggle and Avazu dataset. The left two figures show the frequency distribution
for top-10K ids. The number in the right figure show the percentage of ids accounting for the top-10% access.

4 THE SOFTWARE CACHE
We design a frequency-aware software cache on GPU leveraging
id frequency in the dataset as shown in Figure 3. It is a two-level
cache with uses the hybrid memory space composed of CPU and
GPU memory. The embedding tables are resident in CPU memory
as CPU weight, while some copies of embedding rows are placed in
GPU memory as CUDA Cached Weight. During DLRMs training, the
embedding rows activated by the input ids of this iteration are dy-
namically transmitted from CPU Weight to CUDA Cached Weight.
If necessary, it uses the Least Frequently Used (LFU) algorithm to
evict unused rows.

The software cache is innovative in the following aspects. First, it
uses the frequency statistics of the dataset id to improve the hit rate.
These statistics are prepared into the cache statically before training.
Secondly, it uses the buffer mechanism to improve the bandwidth
utilization of data in PCI-e transmission, and also strictly limits the
memory used. Finally, cache-related operations are parallelized on
GPU and introduces very little overhead.

4.1 Terminology
Before introducing the cache algorithm, let’s define some terms.

• ids: The ids from dataset.
• cpu_row_idx: The index of the embedding row in the CPU

Weight.

• gpu_row_idx: The index of the embedding row correspond-

ing in CUDA Cached Weight.

• idx_map: a map from id to cpu_row_idx.
• cache_idx_map: a map from gpu_row_idx to cpu_row_idx.

4.2 Static Module
The static module of the system is prebuilt before training that does
not change afterward. The static module contains the CPU Weight,
which contains the entire original embedding weight values but
is reordered in rows from high to low according to its occurrence
frequency in the target dataset. In the original weight, the input id
is the same as the row number of the weight. This does not hold for
reorder CPU weight. Therefore, it uses idx_map, implemented with
a 1D array, to convert the input id to the reordered row number.

To get the id frequency distribution, we can simply scan the
dataset before training. If the dataset is too large, we can use the
sampling method proposed in work [1].

Figure 3: The Framework of the Frequency-aware Software
Cache.

4.3 Dynamic Module
The dynamic module of the system keeps updating during the
training according to input ids. It transmits embedding rows be-
tween CPU and GPU efficiently to place the embedding data re-
quired by each iteration in the CUDA memory before computa-
tion. The dynamic module consists of the CUDA cached weight, a
cache_idx_map, and a data transmitter.

The CUDA Cached Weight contains a fraction of the CPU weight.
The cached_idx_map, also a 1D array, converts the cpu_row_idx of
the CPU Weight to the gpu_row_idx of the CUDA Cached Weight.
The ratio of the size CUDA Cached Weight to the size of CPU
Weight is named as cache ratio, which can be 1.5% by default.

The data transmitter is responsible for the bidirectional trans-
mission of data between the CUDA Cached Weight and the CPU
Weight. It uses a buffering approach to solve the problem of insuf-
ficient utilization of PCI-e bandwidth for fine-grained row-wise
transmission. The embedding rows scattered in the memory are

CPU WeightidxmapmergeCPU bufferCUDA bufferCUDA Cached Weightscattercached_idx_mapupdatesStatic ModuleDynamic Module5050201515203525558, 25, 55172648data transmitterevicthighfreqlowfreqJiarui Fang, Geng Zhang, Jiatong Han, Shenggui Li, Zhengda Bian, Yongbin Li, Jin Liu, and Yang You

concentrated as continuous data blocks in the local memory of the
source device. Then the blocks are transferred between the CPU
and GPU and scattered to the corresponding position of the target
memory. Moving data in blocks can improve the PCI-e bandwidth
utilization. The overhead of concentration and scattering is also
very small. Because the data transfer bandwidth on the CPU and
GPU is several orders of magnitude higher than that of PCI-e.

The memory footprint of the data transmitter is strictly limited.
In the worst case, all input ids are cache missed. We need to transfer
𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒 × 𝑓 𝑒𝑎𝑡𝑢𝑟𝑒_𝑛𝑢𝑚 × 𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔_𝑠𝑖𝑧𝑒 elements. In order to
prevent the buffer from occupying too much memory, we strictly
limit the buffer size. If the transferred data is larger than the buffer,
we complete the transfer multiple times.

All cache-related operations are executed on the GPU. In Fig-
ure 3, all the green boxes are stored in CUDA memory and we use
the GPU parallel operations to implement cache-related operations.
The workflow of the dynamic module during the training process
is shown in the Algorithm 1. First, we identify the cpu_row_idx
in the CPU Weight activated by the current batch of input ids of
this iteration. Then, we find out the CPU embedding rows that are
not in CUDA Cached Weight. We need to move these rows from
the CPU to the GPU. To ensure that these rows have enough space
to be placed in the CUDA Cached Weight, we may need to evict
some rows from the CPU Cached Weight back to the CPU Weight
through the data transmitter. Then, the data transmitter moves
the embedding rows to the CUDA Cached Weight and updates the
corresponding entries in the cached_idx. The cache indexing oper-
ations, involving unique, isin, nonzero, index_copy, index_fill
and argsort, are executed on GPU in parallel using highly opti-
mized APIs provided by PyTorch. After cache-related operations,
Deep Learning frameworks, PyTorch in our case, uses the CUDA
Cached Weight to finish the training process of this iteration on
GPU.

We design an LFU eviction strategy to evict the current not-in-
use rows to the CPU weight. Different from the traditional LFU
method, we use the id frequency statistics of the dataset collected in
advance. Suppose we need to evict 𝑛 rows. We only need to select
the 𝑛 largest cpu_row_idx, and then evict out the corresponding
rows. The evicted rows are also transmitted via the data transmitter.
Cache warm-up can be placed before training. We fill in the CUDA
Cached Weight using the highest frequency ids.

4.4 Scaling to multiple GPUs
Our embedding tables implemented with our frequency-aware soft-
ware cache are compatible with the parallel training approaches [3].
In this work, we apply the widely used hybrid parallel method to
scale to multiple GPUs as shown in Figure 4. Tensor Parallel is used
for the Embedding layer, and data parallel is used for the dense
layers. Switching the parallel mode requires additional communi-
cation operations. We need to perform all2all operations on the
output activations.

ALGORITHM 1: Cache-related Operations

1 Function PrepareCache(ids)
2

/*Convert ids from dataset to gpu_row_idxs in Cached CUDA

3

4

5

6

7

8

Weight*/
Require: idx_map, cached_idx_map;
cpu_row_idxs = unique(index_select(idx_map, dim=0,
index=ids));
comm_cpu_row_idxs = idx_map[isin(cpu_row_idxs,
cached_idx_map, invert=True)];

/*transmitting embeddings row between CPU and GPU.*/
PrepareCPURowIdxOnGPU(comm_cpu_row_idxs,
cpu_row_idxs);
gpu_row_idxs= index_select(cached_idx, dim=0,
cpu_row_idxs);
Return gpu_row_idxs;

9
10 end
11 Function PrepareCPURowIdxOnGPU(cpu_row_idxs, backlist)
12

Require: cache_capacity;
Require: cache_idx;
Require: data_transmitter;
evict_num = numel(cpu_row_idxs) - cache_capacity;
if evict_num > 0: then

/*make sure cpu_row_idx in backlist stay in GPU and

won’t be evicted*/
mask_cpu_row_idx = isin(cache_idx_map, backlist);
/*set ids w.r.t the mask_idx in cache_idx to -2 (less than

any other ids)*/

backup_idxs =

cache_idx_map[mask_cpu_row_idx].clone();
invalid_idx = nonzero(backup_gpu_row_idx).squeeze(1);
cache_idx_map.index_fill_(dim = 0, index=invalid_idx,
value = -2);

/*select the max evict_num ids to be evicted from GPU*/
evict_gpu_row_idxs = argsort(cache_idx_map,
descending=True)[:evict_num];

/*revert value of cache_idx_map back to their origin value*/
cached_idx_map.index_copy_(0, index=invalid_idx, value
= backup_idxs);

target_cpu_row_idx = cached_idx_map[evict_idx];
/*move rows of evict_idx from Cached CUDA Weight to

CPU weight*/
data_transmitter.move_to_cpu(evict_gpu_row_idxs,
target_cpu_row_idx);

end
/*-1 in cache_idx indicates id not in GPU*/
target_gpu_row_idxs = nonzero(cache_idx_map ==
-1).squeeze(1)[:numel(ids)]
data_transmitter.move_to_gpu(cpu_row_idx,
target_gpu_row_idxs);

cache_idx[target_gpu_row_idxs] = idcpu_row_idxs;

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33
34 end

5 EXPERIMENTS
5.1 Dataset & Software & TestBed
Table 2 reports the hardware of our testbed. The GPUs are connected
via a PCI-e 3.0 switch. For Criteo Kaggle dataset, we directly use the

13 continuous features as the dense features for each sample and
construct the embedding table for each of the 26 sparse features.
Due to the seriously imbalanced labels, we found that it requires
non-trivial efforts to achieve convergence for DLRM trained on
Avazu dataset. To solve this issue, we adopt the preprocessing

A Frequency-aware Software Cache for Large Recommendation System Embeddings

Figure 4: Hybrid Parallel Approach to Scale Freq-aware Em-
bedding on 4 GPUs.

steps 4 commonly suggested by the Kaggle community. It is worth
mentioning that we focus on the effectiveness of the cache-based
training scheme. Therefore, we focus on the consistency of the
convergence of the baseline and our improved system, rather than
obtaining an optimal model accuracy. The training/validation/test
sets take 90%/5%/5% of the whole dataset respectively. The statistics
of the preprocessed datasets are summarized in Table 1.

The embedding dimensions for all the embedding tables of DLRM
are set as 128. The intermediate hidden sizes of bottom MLP for
dense features are set to 512,256,128, and the top MLPs are of hidden
sizes 1024,1024,512,256,1. The learning rates are set as constants, 1
and 5e-2 for Criteo Kaggle and Avazu respectively.

We prototype our system using PyTorch v1.11. Datasets include
Criteo Kaggle and Avazu. Software-wise we use TorchRec DLRM
as the baseline. The baseline uses a planner (an instance of Em-
beddingShardingPlanner in TorchRec 5) to automatically search
the parallel strategies for embedding tables based on the hardware
network topology. According to our observation, the planner is able
to search Embedding tables applied with UVM. In our test cases,
the table-wise parallel strategy is always the parallel strategy of
the planners. It means that each process holds different embedding
tables, and may result in a potential memory load imbalance. In
contrast, our prototype system concatenates all embedding tables
into a large embedding table and uses tensor parallelism to evenly
partition the matrix along the embedding dimension, a.k.a column-
wise 1D Tensor Parallel. More parallel strategies for embedding
will be investigated in our future work.

5.2 Performance on one GPU
5.2.1 Accuracy. As shown in Figure 5 and Figure 6, we compare
the AUROC achieved after 1 epoch training on validation dataset
and test dataset, respectively. The software cache using various
cache ratio settings has minimal effect on the accuracy of the model.
The differences between our system and the baseline in AUROC
metrics are less than 0.01 on the two datasets.

4https://www.kaggle.com/code/leejunseok97/deepfm-deepctr-torch
5https://github.com/pytorch/torchrec/blob/main/examples/sharding/uvm.ipynb

Figure 5: Accuracy of DLRM on Criteo Kaggle vs. Cache Ra-
tio using batch size as 16K on 1 GPU.

Figure 6: Accuracy of DLRM on Avazu vs. Cache Ratio using
batch size as 64K on 1 GPU.

5.2.2 Memory Usage. Figure 7 and Figure 8 present the CUDA
memory usage of our system and TorchRec. When setting the cache
ratio to 1.5%, our software cache can save around 80% CUDA mem-
ory consumption.

Figure 7: Memory Usage of DLRM on Criteo Kaggle using
batch size as 16K on 1 GPU.

5.2.3 Throughput. Figure 9 and Figure 10 present how the cache
ratio affects the DLRM performance on 1 GPU.

As can be seen from the figure, when the cache is 1.5%, a nearly-
optimal throughput can be obtained. This is because when the
cache ratio is higher than 1.5%, more unused records are being
loaded onto GPU, thus it does not help much to improve the hit
rate. Instead, using a small cached CUDA weight can speed up the
cache-related operations on GPU.

all2allCUDA Cached EmbeddingDenseDNNactivationsactivationsinput idsdiskours valours testtorchrec valtorchrec testAUROC (iter/sec)0.770.780.78cache ratio (%)0.30.61.533264Accuracy vs. Cache Ratio on Criteo Kaggleours valours testtorchrec valtorchrec testAUROC (iter/sec)0.820.830.84cache ratio (%)0.30.61.533264Accuracy vs. Cache Ratio on Avazuours allocatedtorchrec allocatedours reservedtorchrec reservedCUDA Mem. (GB)05101520Cache Ratio (%)0.30.61.533264CUDA Memory vs. Cache Ratio on Criteo KaggleJiarui Fang, Geng Zhang, Jiatong Han, Shenggui Li, Zhengda Bian, Yongbin Li, Jin Liu, and Yang You

Table 1: Statistics of datasets

Dataset
Criteo Kaggle
Avazu

#Sparse feature
26
13

#Dense feature
13
8

#Embedding item
33,762,577
9,445,823

#Train
39,291,954
36,386,071

#Val
3,274,331
2,021,448

#Test
3,274,332
2,021,448

Table 2: Hardware Configuration for Experiments

Device
8xGPU
CPU

Architect
NVIDIA A100
AMD EPYC 7543 32-Core

Memory
80GB
512GB

Figure 8: Memory Usage of DLRM on Avazu using batch size
as 64K on 1 GPU.

In the context of synchronous parameter updating, using soft-
ware cache decrease by about 50% of the overall performance com-
pared to TorchRec. It is implied that in order to make training with
larger embeddings possible, some performance degradation was
acceptable.

Figure 10: Throughput of DLRM on Avazu vs. Cache Ratio
using global batch size as 64K on 1 GPU.

GPU. By increasing the degree of parallelism, TorchRec can reduce
memory usage per GPU, but still requires more than ours. Since our
software caching method has already used very little memory on a
single GPU, no obvious memory savings are observed and expected
when extended to multiple GPUs.

Figure 11: Memory Usage of DLRM on Avazu when scaling
to 8 GPUs.

Figure 9: Throughput of DLRM on Criteo Kaggle using
global batch size as 16K on 1 GPU.

5.3 Performance on multiple GPUs
5.3.1 Memory Usage. Figure 12 and Figure 11 present the memory
usage when we scale model training from 1 GPU to 8 GPUs. Our
GPU memory usage is still significantly less than TorchRec on
multiple GPUs. Also, the splitting of the Embeddingbag by TorchRec
leads to unbalanced GPU memory usage. We make the maximum
memory usage when experimenting with TorchRec on a single

Figure 12: Memory Usage of DLRM on Criteo Kaggle when
scaling to 8 GPUs.

ours allocatedtorchrec allocatedours reservedtorchrec reservedCUDA Mem. (GB)0246Cache Ratio (%)0.30.61.533264CUDA Memory vs. Cache Ratio on AvazuoursTorchRecThroughput (iter/sec)406080Cache Ratio (%)0.30.61.533264Throughput vs. Cache Ratio on Criteo KaggleoursTorchRecThroughput (iter/sec)6080100Cache Ratio (%)0.30.61.533264Throughput vs. Cache Ratio on Avazuours allocatedtorchrec allocatedours reservedtorchrec reservedCUDA Mem. (GB)0246#GPU1.01.52.02.53.03.54.0CUDA Memory vs. Cache Ratio on Avazuours allocatedtorchrec allocatedours reservedtorchrec reservedCUDA Mem. (GB)05101520#GPU1.01.52.02.53.03.54.0CUDA Memory vs. Cache Ratio on Criteo Kagglethat we can obtain a decent end-to-end training speed by keeping
only 1.5% of the embedding parameters on GPU. This study paves
a way for future studies on building extremely large GPU DLRM
systems.

REFERENCES
[1] Muhammad Adnan, Yassaman Ebrahimzadeh Maboud, Divya Mahajan, and
Prashant J Nair. 2021. Accelerating recommendation system training by leverag-
ing popular choices. arXiv preprint arXiv:2103.00686 (2021).

[2] Muhammad Adnan, Yassaman Ebrahimzadeh Maboud, Divya Mahajan, and
Prashant J Nair. 2022. Heterogeneous Acceleration Pipeline for Recommendation
System Training. arXiv preprint arXiv:2204.05436 (2022).

[3] Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen Huang, Yongbin Li, Chuan-
rui Wang, Fan Cui, and Yang You. 2021. Colossal-AI: A Unified Deep Learning
System For Large-Scale Parallel Training. arXiv preprint arXiv:2110.14883 (2021).
[4] Antonio A Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James
Zou. 2021. Mixed dimension embeddings with application to memory-efficient
recommendation systems. In 2021 IEEE International Symposium on Information
Theory (ISIT). IEEE, 2786–2791.

[5] Carlos A Gomez-Uribe and Neil Hunt. 2015. The netflix recommender system:
Algorithms, business value, and innovation. ACM Transactions on Management
Information Systems (TMIS) 6, 4 (2015), 1–19.

[6] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan
Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, et al. 2021. Persia: a hybrid
system scaling deep learning based recommenders up to 100 trillion parameters.
arXiv preprint arXiv:2111.05897 (2021).

[7] Xupeng Miao, Hailin Zhang, Yining Shi, Xiaonan Nie, Zhi Yang, Yangyu Tao, and
Bin Cui. 2021. Het: Scaling out huge embedding model training via cache-enabled
distributed framework. arXiv preprint arXiv:2112.07221 (2021).

[8] Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch,
Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, et al. 2022.
Software-hardware co-design for fast and scalable training of deep learning rec-
ommendation models. In Proceedings of the 49th Annual International Symposium
on Computer Architecture. 993–1011.

[9] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-
Jean Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model
for personalization and recommendation systems. arXiv preprint arXiv:1906.00091
(2019).

[10] Brent Smith and Greg Linden. 2017. Two decades of recommender systems at

Amazon. com. Ieee internet computing 21, 3 (2017), 12–18.

A Frequency-aware Software Cache for Large Recommendation System Embeddings

5.3.2 Throughput. Figure 13 and Figure 14 present the throughput
when we scale the training from 1 GPU to 8 GPUs. In our testbed,
GPUs are connected by PCI-e and the communication bandwidth
between GPUs is limited. It damages the scalabilities of both sys-
tems. The gap between the throughput of our system and TorchRec
is still existing in multiple GPUs. However, it gradually shrinks as
the number of GPUs increases, because the proportion of inter-GPU
communication becomes larger.

Figure 13: Throughput of DLRM on Criteo Kaggle vs. Cache
Ratio using batch size as 16K on 1-8 GPUs.

Figure 14: Throughput of DLRM on Avazu using global batch
size as 64K on 1-8 GPUs.

6 FUTURE WORK
In future work, we will evaluate our method on even larger datasets,
such as Criteo 1TB dataset6. Also, We will adopt an input-id-prefetch
method that looks ahead to more input ids to improve the cache
eviction efficacy. We will also explore the combination of embed-
ding compression algorithm [4] with the caching approach, and
manage the memory space NVMe in our caching method.

7 CONCLUSION
This paper proposes a software cache design method to solve the
problem that the large-scale embedding tables training cannot be
executed in GPU. Based on the target dataset id frequency statistics,
our software cache achieves high-speed indexing and data trans-
mission. Preliminary experiments on our prototype system show

6https://ailab.criteo.com/criteo-1tb-click-logs-dataset/

78114461643603617torchrecoursThroughput Per GPU (iter/sec)020406080100120#GPU1248Throughput of DLRM on Criteo Kaggle using global batch size as 64K110140743863904139torchrecoursThroughput Per GPU (iter/sec)020406080100120140#GPU1248Throughput of DLRM on Avazu using global batch size as 64K