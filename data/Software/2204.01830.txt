This is the author‚Äôs draft of an article to be subitted to
an IEEE publication. Copyright may be transferred without
notice, after which this version may no longer be accessible.
A copyright notice will be added here upon submission, and
additional information in the case of an acceptance/publica-
tion.

1

2
2
0
2

r
p
A
6

]
I

N
.
s
c
[

2
v
0
3
8
1
0
.
4
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
WiFiEye ‚Äì Seeing over WiFi Made Accessible
Philipp H. Kindt ‚àó, Cristian Turetta‚Ä†, Florenc Demrozi‚Ä†,
Alejandro Masrur‚àó, Graziano Pravadelli‚Ä†, Samarjit Chakraborty‚Ä°
‚àóTU Chemnitz, Germany,
‚Ä†University of Verona, Italy,
‚Ä°University of North Carolina at Chapel Hill, USA

2

Abstract‚ÄîWhile commonly used for communication purposes,
an increasing number of recent studies consider WiFi for sensing.
In particular, wireless signals are altered (e.g., reÔ¨Çected and
attenuated) by the human body and objects in the environment.
This can be perceived by an observer to infer information on
human activities or changes in the environment and, hence, to
‚Äúsee‚Äù over WiFi. Until now, works on WiFi-based sensing have
resulted in a set of custom software tools ‚Äì each designed for
a speciÔ¨Åc purpose. Moreover, given how scattered the literature
is, it is difÔ¨Åcult to even identify all steps/functions necessary to
build a basic system for WiFi-based sensing. This has led to
a high entry barrier, hindering further research in this area.
There has been no effort to integrate these tools or to build
a general software framework that can serve as the basis for
further research, e.g., on using machine learning to interpret
the altered WiFi signals. To address this issue, in this paper, we
propose WiFiEye ‚Äì a generic software framework that makes all
necessary steps/functions available ‚Äúout of the box‚Äù. This way,
WiFiEye allows researchers to easily bootstrap new WiFi-based
sensing applications, thereby, focusing on research rather than
on implementation aspects. To illustrate WiFiEye‚Äôs workÔ¨Çow, we
present a case study on WiFi-based human activity recognition.

Fig. 1: A typical scenario for WiFi-based sensing, consisting
of at least one AP and an observer that collects CSI data from
the environment. This data is then analyzed with the help of
ML methods.

I. INTRODUCTION

‚ÄúSeeing‚Äù over WiFi: Most of us rely on WiFi for communica-
tion purposes, e.g., for connecting our smartphones and laptops
with each other and to the Internet. This has catapulted WiFi to
ubiquity over the last years and, hence, WiFi signals are almost
everywhere. While propagating from a sender to a receiver,
WiFi signals are altered by the human body and/or objects
in the environment. An external observer can perceive these
changes in the received signals and, e.g., detect the presence of
humans and/or their activities. Hence, beyond communication,
WiFi networks can actually be used to sense or ‚Äúsee‚Äù [1].
Recently, the IEEE 802.11bf Task Group was created with the
goal of standardizing WiFi-based sensing in the future, which
underpins the growing importance of this area [2].

Applications of WiFi-based sensing are as varied as inter-
esting. For example, the number of subjects in a room can
be estimated [3], or their activities can be classiÔ¨Åed, detecting
whether somebody walks, lies/rests, or falls onto the Ô¨Çoor [4].
Compared to other technologies, WiFi-based sensing has
a number of decisive advantages. First, no sensors need to
be worn on the body. Second, WiFi networks are anyway
deployed for communication purposes. As a result, WiFi-
based sensing is more economical than most other wireless
approaches, e.g., Ultra-Wideband (UWB) or radar-based ones.
Third, WiFi-based sensing is intrinsically privacy-preserving,

since it reveals signiÔ¨Åcantly less personal data than, for exam-
ple, camera-based approaches.

Figure 1 shows a typical scenario involving WiFi-based
sensing. It consists of one or more access points (APs), a
passive observer, and one or more human subjects performing
different activities. The passive observer extracts the channel
state information (CSI) from the captured packets, which
we describe in more detail in Section II. The CSI data is
then analyzed to infer information on the activities of these
subjects. Typically, this is done using machine learning (ML)
methods, e.g., support vector machines (SVMs), decision
trees (DTs), or convolutional neural networks (CNNs). For
example, a human subject carries out multiple different
gestures in a WiFi environment. For each gesture, CSI data is
recorded and fed into a ML model for training. Once trained,
such a model can reliably classify unknown CSI data, thereby
achieving an accuracy of more than 90 % [1].

Setting up a base system: Advances in CSI data analysis
are hindered by the high effort involved in creating a working
base system. In particular, a workÔ¨Çow for WiFi-based sensing
consists of data acquisition and recording, format conversion,
data preprocessing, visualization, and classiÔ¨Åcation. All of
these steps need to be realized before any research on the
actual classiÔ¨Åcation, e.g., using machine learning methods, can
be carried out.

In more detail, Ô¨Årst, it is necessary to ‚Äúunlock‚Äù CSI data
by Ô¨Çashing a custom Ô¨Årmware onto the WiFi radio. This
is mandatory because no off-the-shelf radio Ô¨Årmware grants
access to the CSI data to an external program running on the
host computer.

Second, CSI data cannot be fed directly into a ML frame-
work, such as TensorÔ¨Çow or PyTorch. Usually, multiple pa-
rameterizable, application-speciÔ¨Åc preprocessing, Ô¨Åltering and
data format conversion steps need to be applied before the
data becomes usable.

Third, determining the positions of the senders (i.e., APs)
and the observer that best suit the intended application requires
experience and intuition. Other aspects to be optimized are the
set of APs to be considered, the parameter values of differ-
ent preprocessing steps, and the trafÔ¨Åc characteristics of the
network. Typically, this optimization is carried out iteratively
by repeatedly modifying the setup, recording a sequence of
CSI data and analyzing the impact of the modiÔ¨Åcations on
the captured CSI data, e.g., using multiple plots. Multiple
iterations are necessary until a suitable conÔ¨Åguration is found,
which is a time-consuming and cumbersome procedure. The
currently used iterative ofÔ¨Çine-workÔ¨Çow is hence not very
if the preprocessed CSI data could be
effective. Instead,
visualized in real-time, the impact of every change could be
observed immediately, leading to better results with a reduced
effort.

Unfortunately, existing software tools

for WiFi-based
sensing only provide a subset of the functions needed in
this workÔ¨Çow. As a result, before developing and optimizing
any ML techniques, signiÔ¨Åcant effort has to be invested
into obtaining a working base system.
to
though being of tremendous
the best of our knowledge,
use, none of the existing software tools support real-time
visualization and an online-adjustment of all parameter values.

In addition,

Contributions: This work aims to substantially reduce the
initial effort for developing a WiFi-based sensing system.
To this end, we introduce a software tool called WiFiEye
and make it available to the public under an open-source
license1. It provides a ready-to-use base workÔ¨Çow, such that
downstream WiFi-based sensing applications can focus on the
machine learning aspects. WiFiEye, for the Ô¨Årst time, pro-
vides a graphical, intuitive, and interactive real-time workÔ¨Çow,
which greatly facilitates the development and analysis of WiFi-
based sensing systems. By presenting WiFiEye, we make the
following contributions:

1) We compile the knowledge from the related literature into

ready-to-use software.

2) We provide an adjustable, working base-system ‚Äúout of
the box‚Äù, thereby minimizing the effort when developing
new WiFi-based sensing applications.

3) We propose a system to process and visualize all data in
real-time for supporting the optimization of the setup and
its parameter values interactively, and to support running
a classiÔ¨Åer in real-time.

1Available at https://github.com/pkindt/WiFiEye

3

WiFiEye works together with a Raspberry Pi using a
Nexmon-based Ô¨Årmware [5], [6], as well as literally any
ML framework, e.g., TensorÔ¨Çow, Keras or PyTorch. Using
WiFiEye, all required steps including data recording, format
conversion, preprocessing, and visualization, are already
implemented and ready-to-use. The entire workÔ¨Çow can be
parameterized and optimized at runtime, while visualizing the
effects of every adjustment in real-time. Most processing steps
are carried out via plugins, which can be activated, conÔ¨Ågured
and deactivated independently from each other. Additional
plugins can be developed by everyone with negligible effort.
Thereby, the functionality of WiFiEye can be extended or
modiÔ¨Åed easily for custom projects. Researchers can use
WiFiEye as a working and adjustable base system and focus
on the actual data classiÔ¨Åcation tasks. By publishing WiFiEye,
we intend to foster further innovations and new applications
in this area, making this topic more accessible for both the
networking as well as the ML community.

II. CURRENT STATE OF WIFI-BASED SENSING PRACTICE

In this section, we Ô¨Årst provide the necessary background
on WiFi-based sensing and then describe a commonly used
workÔ¨Çow and its shortcomings.

A. Background: Channel State Information (CSI)

When a wireless signal ùëã is sent, the signal ùëå received by

another device is given by

Y = H ‚ó¶ X + N,

(1)

where ùëÅ is related to noise, whereas ùêª represents the
CSI. Here, ‚ó¶ represents an element-wise multiplication. The
802.11ac protocol supports channels of 20 MHz to 160 MHz
bandwidth. Using Orthogonal Frequency Division Multiplex-
ing (OFDM), each channel is subdivided into ùëÅ = 64 (for
20 MHz channels) to ùëÅ = 512 (for 160 MHz channels) sub-
carriers, and data is transmitted simultaneously on all of them,
except for a few guard subcarriers. Due to the small bandwidth,
the channel for each subcarrier is assumed to be Ô¨Çat, i.e.,
all frequencies of the subcarrier undergo approximately the
same perturbations. Hence, H = [‚Ñé1, ‚Ñé2, ..., ‚ÑéùëÅ ] for a received
WiFi Frame is a vector of complex values ‚Ñéùëñ, where the index
ùëñ identiÔ¨Åes the subcarrier. The magnitude of every complex
number ‚Ñéùëñ represents the attenuation the signal has undergone,
whereas the phase expresses the phase change induced, e.g.,
by human motion [3].

Using Automatic Gain Control (AGC), every WiFi receiver
continuously adjusts the CSI magnitude on average over all
subcarriers to lie within a certain range. Hence, the measured
CSI relates to momentary changes in attenuation, while persis-
tent changes are cancelled out by AGC. In contrast, another
signal called the Received Signal Strength Indicator (RSSI)
accounts for the average received power over all subcarriers
after antenna and cable loss, but before AGC takes effect. The
RSSI is typically an integer that indicates the received power
in ùëëùêµùëö. We next describe the steps involved in a commonly
used workÔ¨Çow for WiFi-based sensing and its shortcomings.

B. WorkÔ¨Çow for WiFi-Based Sensing

As of

today, most workÔ¨Çows for WiFi-based sensing

research function as follows.

Gathering CSI: The Ô¨Årmware of off-the-shelf WiFi radios
does not expose the CSI data to any external software. But
for a few selected WiFi SoCs, e.g., some instances of the
recent Broadcom BCM43xx device family, as well as the
older Intel IWL5300 radio and some Atheros SoCs, custom
Ô¨Årmware is available. For example, research projects such as
Nexmon [7], [6] provide dedicated Ô¨Årmware patches for the
recent BCM4355c0 SoC used in a Raspberry Pi, which enables
reading the CSI and RSSI.

The patched radio Ô¨Årmware provides the CSI data in the
form of a local UDP broadcast on the Raspberry Pi. These
UDP packets are usually written into a Ô¨Åle on the Raspberry
Pi using standard command line tools, e.g., tcpdump. Such
Ô¨Åles are then manually copied to a PC or laptop for further
analysis. The format of the CSI data extracted from the
PCAP Ô¨Åles depends on the actual chip of the BMC43xx
family. A combination of dedicated libraries (e.g., libpcap
and CSIKit [8]) are required to extract the data. Preprocessing
and (ofÔ¨Çine-)visualization is either carried out by custom
developments in environments like MATLAB, or are partially
provided by dedicated libraries such as CSIKit [8].

Preprocessing: Next, multiple preprocessing steps on the
data are required. First, the raw data from the UDP packets
needs to be split up into amplitude and phase. Moreover,
parts of the data correspond to guard subcarriers and hence
do not carry any information on the environment. They need
to be removed from the data stream. Additionally, the phase
data contains jumps of 2 √ó ùúã, which need to be removed.
Furthermore, the attenuation of the wireless signal caused
by the environment is available in the form of the RSSI and
the CSI, as described earlier. It is useful to merge CSI and
RSSI into a frequency-dependent aggregated attenuation [9].
In addition, CSI and RSSI data is often Ô¨Åltered, e.g., by
Lowpass, Hampel or Wavelet Ô¨Ålters [10]. The actual set
of required preprocessing steps depends on the individual
application. While some steps can be reused from standard
software or dedicated CSI tools, most of them are left over
to be implemented by every developer on their own.

Setup Optimization: After an initial system has been de-
veloped, the setup needs to be optimized. For example, how
sensitive the CSI data is to a certain event of interest depends
on the positions of the APs and the receiver, which need to be
optimized empirically. Other parameters that can potentially
be tuned are the set of considered APs, the wireless channels
to be used, and the trafÔ¨Åc characteristics of the network. Fur-
thermore, the Ô¨Ålters used for data preprocessing have multiple
tunable parameters that impact the results, e.g., coefÔ¨Åcients
for low-pass Ô¨Åltering the RSSI signal. These parameters also
need to be optimized empirically for a particular scenario. For
this purpose, CSI data is Ô¨Årst recorded and then visualized
for analysis. Based on the results, the setup is then altered

4

and another optimization iteration begins by recording another
sequence of CSI data.

CSI data cannot be visualized well using standard tools.
First, it has 3 dimensions (i.e. time, subcarrier and actual
value). Furthermore, every CSI value is a complex number,
and therefore, expands to an amplitude and phase. The
amplitude typically exhibits a large dynamic range, and the
visualization needs to focus on those ranges of values that
represent the events of interest. Since which range of values
contains information related to the events of interest is not
known a-priori, the visualization itself needs to be adjustable.
Hence, also for visualization,
is necessary to develop
custom solutions or at least to combine and adjust different
available tools. In addition, repeatedly recording CSI data
into Ô¨Åles, copying them on a PC or laptop, preprocessing and
visualizing them and adjusting the setup after each iteration
is time-consuming and cumbersome.

it

Model Generation: After the setup has been optimized and
parameterized, a dataset is usually recorded for the purpose
of training a ML model. Furthermore, the data needs to be
converted to a format supported by the ML framework, e.g.,
a comma-separated value (.csv) format.

The purpose of the model resulting from the training proce-
dure is to classify previously unseen data in real-time. Since a
real-time workÔ¨Çow is unavailable in most setups, the recorded
dataset is commonly split into testing and training data. The
model is trained using the training data and evaluated based
on the testing data to mimic an actual online classiÔ¨Åcation.

C. Shortcomings

The workÔ¨Çow described above has multiple signiÔ¨Åcant
shortcomings. First, the entire chain from data acquisition,
interpretation and format conversion, preprocessing, model
generation to data classiÔ¨Åcation involves a large number of dif-
ferent steps. Some of them are provided by different available
tools, whereas others need to be written by every developer
on their own. This requires understanding and combining
bits and pieces from the literature, from standard software
and from dedicated WiFi-based sensing tools, while miss-
ing parts require time-consuming custom implementations.
Clearly, despite the existence of a dedicated radio Ô¨Årmware
that makes CSI data available, signiÔ¨Åcant effort and specialized
knowledge is required for developing a working base system.
In addition, all currently available CSI processing tools,
to the best of our knowledge, do not support any real-time
operation and visualization. As a workaround, the data is
recorded on the Raspberry Pi, manually copied to a PC or
laptop and analyzed ofÔ¨Çine. After adjusting the setup, e.g.,
by repositioning the passive receiver, another iteration of
recording and analysis begins. Repeatedly carrying out this
analysis and optimization based on this ofÔ¨Çine workÔ¨Çow is
impractical, since a large number of iterations are needed.
This is cumbersome and time-consuming, while the limited
number of iterations often does not fully utilize the potential
for optimizations of the setup. In contrast, carrying out all
processing steps in real-time, adjusting all parameter values

5

Fig. 2: WiFiEye‚Äôs workÔ¨Çow for WiFi-based sensing.

online, and studying the impact of any change of the setup in
real-time could signiÔ¨Åcantly facilitate the optimization proce-
dure. It could also lead to better results.

Furthermore, since no tool to deliver preprocessed data to a
classiÔ¨Åcation model in real-time is available, the vast majority
of works, e.g. [3], [11], [12], [13], elude the effort in creating
such an online workÔ¨Çow. They instead subdivide previously
recorded CSI data into testing and training data. Typically,
the testing data is composed of multiple data windows dis-
tributed over the entire recorded data. The resulting model
is evaluated based on this testing data to mimic an actual
online classiÔ¨Åcation. The accuracy of the classiÔ¨Åer obtained in
this way can differ from what would be achieved in an actual
online classiÔ¨Åcation, since the environment might change after
training the model. A realistic evaluation would again require
a workÔ¨Çow that conveys data to a classiÔ¨Åer in real-time.

Creating a real-time workÔ¨Çow would require streaming the
data from the Reaspberry Pi onto a PC or laptop, performing
all preprocessing in real-time, visualizing the CSI data also
in real-time, and Ô¨Ånally streaming this data into a classiÔ¨Åer.
Implementing such a real-time workÔ¨Çow further increases the
effort that needs to be spent until any research on the machine-
learning model can be carried out.

WiFiEye addresses these shortcomings and drastically re-
duces the development effort by providing a real-time work-
Ô¨Çow out-of-the-box. In particular, it provides a complete, pre-
conÔ¨Ågured, parameterizable default workÔ¨Çow, which can be
modiÔ¨Åed and extended easily through a plugin architecture. All
processing is done in real-time, and a powerful visualization
is provided to also show the preprocessed data in real-time.
Hence, the setup can be optimized using immediate feedback
rather than relying on multiple ofÔ¨Çine iterations. This helps in

selecting the set of APs considered, optimizing the positions
of the receiver and the APs, determining the parameter values
of the preoprocessing steps, and other optimizations. Finally,
WiFiEye supports streaming data to a classiÔ¨Åer in real-time,
while importing back its classiÔ¨Åcation results, which can be
annotated in the real-time visualization.

III. THE WIFIEYE FRAMEWORK

In this section, we Ô¨Årst provide an overview on WiFiEye

and then describe its components in more detail.

A. Overview

Figure 2 shows a WiFi-based sensing setup involving Wi-
FiEye. The green block on the left represents multiple WiFi
access points (APs) that emit WiFi signals. For example,
every AP continually broadcasts beacon frames to advertise its
presence with a frequency of around 9 Hz. The yellow block
in the middle represents a CSI observer, e.g., a Raspberry Pi,
which captures WiFi signals and extracts CSI and RSSI data.
The grey block on the right represents a laptop or PC, which
performs signal processing, classiÔ¨Åcation, and visualization.
The data recorded into Ô¨Åles is used to train a pattern recog-
nition model. This model can be queried in real-time using
WiFiEye‚Äôs live export feature.

Figure 3 summarizes the steps that need to be carried out to
implement an arbitrary, CSI-based pattern recognition scenario
using WiFiEye based on a previously prepared Raspberry
Pi. After downloading (1st block), compiling, and running
WiFiEye (2nd block), one can collect data by using WiFiEye
(3rd block).

To facilitate the setup of an initial ML model, WiFiEye
provides two preconÔ¨Ågured python scripts making use of the

EthernetSniffing WiFiSoCCSI ServerTimestampMACsAmplitudePhaseRSSIRaspberry PICSI amplitudeClassifier outputAPs1inEthernetWiFiEyeStudio running on PC/LaptopPattern recognition model generationReal-time ClassificationCSV FilesData ManagementCSIPreprocessingModelLive exportCSI VisualizationMandatoryOptionalLegendLearning dataClassification resultspopular TensorFlow [14] ML framework. A generic, CNN-
based pattern recognition model
is generated through the
train model.py script (4th block), which contains multiple
conÔ¨Åguration options. It generates a standard CNN architec-
ture, which can be trained for different purposes. The model
is composed of two convolutional layers, which carry out
feature extraction based on a time window of CSI data. Next,
a max pooling layer reduces the dimension of the feature map.
After that, two dense fully connected layers map the Ô¨Çattened
features to the output classes and also compute a conÔ¨Ådence
value for each of them. The classify.py script (5th block) can
be executed from within WiFiEye Studio to query the model
in real-time. These two scripts serve as a starting point for
developing own models, which are not limited to the described
architecture or the TensorÔ¨Çow library.

B. Data Flow

The radio Ô¨Årmware extracts CSI data from the frame pream-
bles it captures, which carry synchronization information that
remains unencrypted even in encrypted networks. WiFiEye
provides a program called the CSI Server for the Raspberry Pi.
It reads the CSI data from the patched WiFi SoC, timestamps
it and transfers it to a PC or laptop in real-time, where it is
processed further.

C. WiFiEye‚Äôs Components

WiFiEye consists of two software packages. The CSI Server
is deployed on the Raspberry Pi, whereas an intuitive graphical
tool called WiFiEye Studio is usually run on a PC or laptop
(cf. Figure 2). After receiving the data form the CSI Server,
WiFiEye Studio mainly carries out the following 3 tasks:

1) Data management: The CSI data is needed at different
sinks, and each of them requires different data formats
and preprocessing steps. WiFiEye provides all function-
alities to control and convert this dataÔ¨Çow.

2) Preprocessing: Different preprocessing steps are needed
to bring the data into a suitable form for analysis, e.g.,
gain compensation and guard carrier removal/nulling. Wi-
FiEye provides an extensible and adjustable preprocess-
ing architecture. A common workÔ¨Çow is preconÔ¨Ågured to
work out-of-the-box, which is described later.

3) Visualization: WiFiEye provides a real-time visualization
of the CSI amplitude, phase and RSSI data. WiFiEye can
additionally annotate the output of a classiÔ¨Åer in real-
time. This is helpful for optimizing the setup, tuning
the parameters of the preprocessing steps, and analyzing
whether a particular setup is sensitive to a certain event
of interest.

We next describe these functionalities in more detail.

1) Data Management: The CSI data can be recorded into
data Ô¨Åles, exported to a classiÔ¨Åer and visualized in real-
time. WiFiEye provides different preprocessing steps and
data formats for each of these possibilities. For example, it
might be beneÔ¨Åcial to only visualize data belonging to certain
transmitters of interest, while at the same time exporting the
data belonging to all transmitters into a .csv Ô¨Åle. WiFiEye

6

internally converts the data into the required format and allows
Ô¨Åltering for one or multiple transmitter MAC addresses, which
can be chosen during runtime. WiFiEye supports recording
Ô¨Åles in three different formats,
i.e., one comma-separated
value (.csv) format optimized for simplicity, one .csv format
optimized for size, and a binary format for minimalistic
memory requirements.

In addition to exporting data to a Ô¨Åle, WiFiEye can launch
an arbitrary executable from the graphical user interface. The
CSI data is then streamed to the executable by writing to its
standard input. ClassiÔ¨Åcation results are imported into WiFi-
Eye by reading from the standard output of the executable.

2) CSI Preprocessing: The data received from the patched
WiFi SoC needs to be preprocessed before recording and
real-time export. WiFiEye provides a plugin-based, extensible
architecture for this purpose. In particular, each preprocessing
step is realized by a plugin. Plugins are linked dynamically
during runtime, can be compiled individually and are easy to
implement.

As shown in Figure 4, WiFiEye Studio allows the user to
select the set of active plugins and to determine their execution
order by assigning individual priorities.

Furthermore, every plugin can expose multiple adjustable

parameters to the user.

WiFiEye comes with a comprehensive set of processing
plugins. It thereby supports a generic and useful preprocessing
workÔ¨Çow, which is described next.

MAC Filtering: When capturing WiFi frames, data from all
APs in range is received, of which only a few are relevant.
WiFiEye allows selecting suitable APs interactively during
runtime.
Amplitude/Phase Extraction: From the complex values ‚Ñéùëñ,
ùëñ = 1...ùëõ, WiFiEye extracts the amplitudes ùëéùëñ and phases Œ¶ùëñ,
which can then be processed separately.
Bandwidth Narrowing: WiFi uses channels with different
bandwidths, and the BCM43455c0 WiFi radio on the Rasp-
berry Pi can monitor channels from 20 MHz to 80 MHz
bandwidth. When a transmitter only uses e.g., 20 MHz,
whereas the radio captures using 80 MHz, most of the data
does not contain any usable information. WiFiEye supports
narrowing the bandwidth of its input data, thereby supporting a
mode of operation in which the conÔ¨Åguration on the Raspberry
Pi never needs to be changed.
Subcarrier Removal: WiFiEye can set the values of subcar-
riers that do not carry any valid information to zero to avoid
disturbing the ML model.
Subcarrier Re-Ordering: The CSI values on the different
subcarriers received from the WiFi SoC are sometimes not
ordered in a linear manner.WiFiEye reorders them if needed.
AGC Compensation: WiFi SoCs use automatic gain control
(AGC) to bring the signal amplitude and hence the CSI into a
desired range. WiFiEye can cancel the impact of AGC using
the method described in [9].
Denoising the RSSI: In our experiments, we found that
than
the RSSI is prone to noise to a much larger extent
the CSI. WiFiEye hence provides an exponential smoothing

7

Fig. 3: Steps required to create a new CSI-based recognition system using WiFiEye.

certain event or activity.

For visualizing the output of a classiÔ¨Åer, a bar is painted (see
Figure 2). The height of each bar corresponds to the class
number to which the considered time window of CSI data
has been assigned, and the bar color depicts the classiÔ¨Åcation
conÔ¨Ådence. Jointly examining CSI data and classiÔ¨Åcation
results allows studying for which kind of input the classiÔ¨Åer
creates which output.

IV. CASE STUDY
This section describes a real-world experiment using Wi-
FiEye to collect data, train a pattern recognition model and
perform real-time classiÔ¨Åcation.

A. Experimental Setup

In a kitchen, we placed a wireless router as a data source
to the left-hand side of a sink, while a Raspberry Pi 4B as a
WiFi observer was placed on its right-hand side, as shown in
Figure 6. The goal was to recognize different human activities,
i.e., washing, drying and soaping hands, as well as the sink
being unused.

We recorded 5 minutes of CSI data for each of the four
different activities and generated a ML model with negligible
effort using the train model.py script provided along with
WiFiEye. To study the effect of systematically optimizing the
setup, we have further repeated the experiment for 2 different
distances of the Raspberry Pi from the sink, i.e., 150 cm and
500 cm. Figure 5 depicts the CSI amplitude for both distances
when washing hands. The pattern at a distance of 1.5 m is
more distinct and Ô¨Åne-grained as for a distance of 5 m. Our
results will show that this is also reÔ¨Çected in the performance
of the classiÔ¨Åer.

B. Evaluation

We used 75% of the recorded data for training our model
and 25% for evaluating the accuracy of the resulting classiÔ¨Å-
cation, which is based on 1-second time windows. For each
window, we compared the results with the actually performed
activity. We evaluated the performance in terms of the F-score,
which is a standard metric that combines precision and recall.
For a distance of the receiver from the sink of 5 m, the
classiÔ¨Åer could distinguish between these activities with a F-
score of 0.93. More precisely, we could correctly classify the
sink being idle in 78 % of all corresponding time windows,
soaping hands in 99 %. washing hands in 100 %, and drying
hands in 92 % of them. For a distance of 1.5 m, the F-score
improved to 0.97. Here, the sink being idle could be correctly
classiÔ¨Åed in 97 %, soaping hands with 97 %, washing hands
with 99 %, and drying hands with 96 %.

Fig. 4: ConÔ¨Åguring Ô¨Ålter plugins through the graphical user
interface.

method to denoise the RSSI, thereby also improving the AGC
compensation.
Phase Unwrapping and Cleansing: The resulting phase in-
formation from the WiFi SoC contains multiple discontinuities
caused by the phase ‚Äúwrapping around‚Äù for values greater
than 2 ¬∑ ùúã or lower than 0. WiFiEye linearizes the phase data
accordingly.

3) CSI Visualization: Visualizing CSI data in real-time is
crucial for optimizing the setup. WiFiEye can simultaneously
visualize the CSI amplitude, CSI phase and RSSI in real-time,
as shown in Figure 5.

CSI amplitude and phase are displayed using two inde-
pendent, synchronized, 3-dimensional plots. One dimension
is given by the reception time of the frame. The second
dimension is the subcarrier index, and the third dimension
is formed by the CSI amplitude or phase. In WiFiEye, time
forms the X-axis, the subcarrier index the Y-axis and the CSI
amplitude or phase is represented by different colors. Low
amplitudes are drawn in blue, high ones in red, while blending
from blue to red for intermediate values. The depicted time
window is updated in real-time by scrolling from the right to
the left.

On the right side of the plot (cf. Figure 5), there are two
sliders, which can be used to select two thresholds that deÔ¨Åne a
certain range of CSI amplitudes or phases to be visualized. All
values that lie outside of these ranges are discarded, whereas
the remaining ones are re-scaled to cover the entire range of
possible colors. Hence, the sliders can be used to selectively
visualize the ranges of values that are most sensitive to a

Download‚û¢git clone repo‚û¢cd WiFiEyeStudio‚û¢cd CSIGUICompile &Run WiFiEye‚û¢qmake‚û¢make install‚û¢./WiFiEyeCollect data with WiFiEyeoselect APoselect data formatostart recordostop recordPattern recognition model generationoconfig. modelorun train_model.pyosave modelReal-Time classification with WiFiEye‚û¢./WiFiEyeoset path to classify.pyostart classificationostop classification8

Fig. 5: Real-Time visualization of the CSI amplitude. As can be seen, a setup in which the receiver had a distance of 1.5 m
from a pair of hands being washed showed a more distinct and Ô¨Åne-grained pattern in the CSI data when somebody made a
hand-washing gesture than at a distance of 5 m.

[3] F. Demrozi, C. Turetta, F. Chiarani, P. H. Kindt, and G. Pravadelli,
‚ÄúEstimating indoor occupancy through low-cost BLE devices,‚Äù IEEE
Sensors Journal, no. to appear, available via IEEE early access, 2021.
[4] W. Wang, A. X. Liu, M. Shahzad, K. Ling, and S. Lu, ‚ÄúUnderstanding
and modeling of WiFi signal based human activity recognition,‚Äù in Inter-
national Conference on Mobile Computing and Networking (Mobicom),
2015, pp. 65‚Äì76.

[5] F. Gringoli, M. Schulz, J. Link, and M. Hollick, ‚ÄúFree your CSI: A chan-
nel state information extraction platform for modern Wi-Fi chipsets,‚Äù
in International Workshop on Wireless Network Testbeds, Experimental
Evaluation& Characterization (WiNTECH), 2019, p. 21‚Äì28.

[6] M. Schulz, D. Wegemer, and M. Hollick. (2017) Nexmon: The C-based

Ô¨Årmware patching framework. [Online]. Available: https://nexmon.org

[7] D. Halperin, W. Hu, A. Sheth, and D. Wetherall, ‚ÄúTool release: Gathering
802.11 n traces with channel state information,‚Äù ACM SIGCOMM
Computer Communication Review, vol. 41, no. 1, pp. 53‚Äì53, 2011.
[8] G. Forbes, S. Massie, and S. Craw, ‚ÄúWiFi-based human activity recog-
nition using Raspberry Pi,‚Äù in IEEE International Conference on Tools
with ArtiÔ¨Åcial Intelligence (ICTAI).

IEEE, 2020, pp. 722‚Äì730.

[9] Z. Gao, Y. Gao, S. Wang, D. Li, and Y. Xu, ‚ÄúCrisloc: Reconstructable
CSI Ô¨Ångerprinting for indoor smartphone localization,‚Äù IEEE Internet of
Things Journal, vol. 8, no. 5, pp. 3422‚Äì3437, 2021.

[10] J. Sch¬®afer, B. R. Barrsiwal, M. Kokhkharova, H. Adil, and J. Liebe-
henschel, ‚ÄúHuman activity recognition using CSI information with
Nexmon,‚Äù Applied Sciences, vol. 11, no. 19, 2021.

[11] M. Won, S. Sahu, and K. Park, ‚ÄúDeepWiTrafÔ¨Åc: Low cost WiFi-
based trafÔ¨Åc monitoring system using deep learning,‚Äù CoRR, vol.
abs/1812.08208, 2018.

[12] X. Zhang, R. Ruby, J. Long, L. Wang, Z. Ming, and K. Wu, ‚ÄúWiHu-
midity: A novel CSI-based humidity measurement system,‚Äù in Smart
Computing and Communication.
Springer International Publishing,
2017, pp. 537‚Äì547.

[13] C. Turetta, F. Demrozi, P. H. Kindt, A. Masrur, and G. Pravadelli,
‚ÄúPractical identity recognition using WiFi‚Äôs channel state information,‚Äù
in Design, Automation and Test in Europe Conference (DATE), 2022, to
appear.

[14] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin et al., ‚ÄúTensorFlow: Large-scale
machine learning on heterogeneous systems,‚Äù 2015, software available
from tensorÔ¨Çow.org. [Online]. Available: https://www.tensorÔ¨Çow.org/

Fig. 6: Experimental setup.

V. CONCLUDING REMARKS

WiFi-based sensing is being actively studied by the com-
munity and is expected to further grow in importance in the
near future. However, the progress is slowed down by the
considerable effort it takes to develop a custom base system.
We aim to signiÔ¨Åcantly reduce this effort by introducing WiFi-
Eye, thereby making WiFi-based sensing more accessible by
providing the entire workÔ¨Çow ‚Äúout of the box‚Äù. In the future,
we plan to add support for additional hardware platforms by
providing customized versions of WiFiEye‚Äôs CSI Server for
them.

REFERENCES

[1] Y. Ma, G. Zhou, and S. Wang, ‚ÄúWiFi sensing with channel state
information: A survey,‚Äù ACM Computing Surveys (CSUR), vol. 52, no. 3,
pp. 1‚Äì36, 2019.

[2] F. Restuccia, ‚ÄúIEEE 802.11 bf: Toward ubiquitous Wi-Fi sensing,‚Äù arXiv

preprint arXiv:2103.14918, 2021.

