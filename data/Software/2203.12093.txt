2
2
0
2

r
a

M
2
2

]
E
S
.
s
c
[

1
v
3
9
0
2
1
.
3
0
2
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

1

Enhancing Mobile App Bug Reporting via
Real-time Understanding of Reproduction Steps

Mattia Fazzini, Member, IEEE, Kevin Moran, Member, IEEE, Carlos Bernal-Cardenas, Student
Member, IEEE, Tyler Wendland, Student Member, IEEE, Alessandro Orso, Fellow, IEEE, and Denys
Poshyvanyk, Member, IEEE

Abstract—One of the primary mechanisms by which developers receive feedback about in-ﬁeld failures of software from users is
through bug reports. Unfortunately, the quality of manually written bug reports can vary widely due to the effort required to include
essential pieces of information, such as detailed reproduction steps (S2Rs). Despite the difﬁculty faced by reporters, few existing bug
reporting systems attempt to offer automated assistance to users in crafting easily readable, and conveniently reproducible bug
reports. To address the need for proactive bug reporting systems that actively aid the user in capturing crucial information, we
introduce a novel bug reporting approach called EBUG. EBUG assists reporters in writing S2Rs for mobile applications by analyzing
natural language information entered by reporters in real-time, and linking this data to information extracted via a combination of static
and dynamic program analyses. As reporters write S2Rs, EBUG is capable of automatically suggesting potential future steps using
predictive models trained on realistic app usages. To evaluate EBUG, we performed two user studies based on 20 failures from 11
real-world apps. The empirical studies involved ten participants that submitted ten bug reports each and ten developers that
reproduced the submitted bug reports. In the studies, we found that reporters were able to construct bug reports 31% faster with EBUG
as compared to the state-of-the-art bug reporting system used as a baseline. EBUG’s reports were also more reproducible with respect
to the ones generated with the baseline. Furthermore, we compared EBUG’s prediction models to other predictive modeling
approaches and found that, overall, the predictive models of our approach outperformed the baseline approaches. Our results are
promising and demonstrate the feasibility and potential beneﬁts provided by proactively assistive bug reporting systems.

Index Terms—Bug Reporting, Mobile Apps, Natural Language Processing, Language Modeling

(cid:70)

1 INTRODUCTION
Developers rely on bug reports to identify and ﬁx prob-
lems in software. These bug resolution activities have been
shown to constitute a major part of the software main-
tenance process, which in turn, typically accounts for a
majority of the development effort [1]. There are generally
two major types of reports that developers must manage:
those produced by automated systems and those manually
constructed by users. For in-ﬁeld software failures that can
be automatically recognized and captured, i.e., those that
are revealed by a known oracle (e.g., a crash or a program
assertion failure), researchers have developed automated
techniques that capture ﬁne-grained failure information
and devised approaches to reproduce the failures based
on the captured information. However, prior research that
empirically analyzed issue trackers in open source projects
found that software problems identiﬁed by known oracles

• M. Fazzini and T.Wendland are with the Department of Computer Science
& Engineering, University of Minnesota, Minneapolis, MN, 55455.
E-mail: [mfazzini,wendl155]@umn.edu

• K. Moran is with the Department of Computer Science, George Mason

University, Fairfax, VA, 22030.
E-mail: kpmoran@gmu.edu

• C. Bernal Cardenas, and D. Poshyvanyk are with the Department of

Computer Science, William & Mary, Williamsburg, VA, 23185.
E-mail: [cebernal,denys]@cs.wm.edu

• A. Orso is with the School of Computer Science, College of Computing,

Georgia Institute of Technology, Atlanta, GA, 30332.
E-mail: orso@cc.gatech.edu

Manuscript received March, 2021

accounted for less than 50% of the studied issues [2]. This
ﬁnding illustrates that a majority of software problems in
the studied open source systems stemmed from functional
issues that must be reported manually.

Despite the prevalence and importance of manually re-
ported bugs, most existing issue tracking systems largely
rely upon loosely structured, free-from text entry to capture
information from reporters. The lack of structure gener-
ally results in reports whose quality is largely dependent
upon the experience and thoroughness of the reporter. Past
studies have illustrated that the most important informa-
tion for developers in bug reports is reproduction steps,
which is also the most difﬁcult information for reporters
to provide, resulting in reports that developers are unable
to reproduce [3]. The difﬁculties often faced in manual bug
reporting largely stem from the cognitive and lexical gap that
exists between reporters and developers [4]. That is, there
is typically a rift between the knowledge of the reporter
of a bug and that of a developer, who has a wealth of
domain speciﬁc knowledge about a given software system.
In the current landscape of issue tracking systems, the
task of bridging this gap falls almost entirely upon the
stakeholders: either a reporter must spend extensive effort
in crafting a report, or a developer must strive to “translate”
the low-quality information provided in a poorly written
report to map it to the code and ultimately reproduce and
ﬁx the issue.

To mitigate this problem, we investigate the potential of
ofﬂoading part of the cognitive burden of bug reporting on

 
 
 
 
 
 
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

2

the reporting system itself. Speciﬁcally, we aim to proactively
assist reporters in deﬁning high-quality bug reproduction
steps (S2Rs), so as to facilitate the reporting process and
provide developers with more useful information, thus re-
ducing the cognitive load on all the stakeholders. As past
work showed, designing such a bug reporting system is a
tenuous balancing act. On the one hand, in order to elicit
more detailed information from reporters, a bug reporting
system must impose some structure on the information
being collected. On the other hand, doing so can make a
system more difﬁcult to use for reporters, which would
ultimately limit its practical applicability and usefulness.

Prior work on the FUSION bug reporting interface has
clearly illustrated this tension [4]. FUSION leveraged infor-
mation gleaned from static and dynamic analysis to popu-
late a series of dropdown menus having the objective to help
reporters in providing S2Rs. That work demonstrated that
an enhanced reporting system with populated, structured
ﬁelds can increase the quality of reports as measured by
their reproducibility. However, the higher quality of FU-
SION’s reports came at a cost, as it generally took reporters
longer to write reports than with traditional free-form, text-
based reporting systems. Longer report creation times may
indicate a higher cognitive load for reporters, which is likely
to hurt adoption in practice. In this paper, we further inves-
tigate these trade-offs in intelligent bug reporting systems,
with the aim of designing a system that ﬁnds a sweet spot
between ease of use and effectiveness in capturing high
quality bug reports.

To this end, we introduce the EBUG reporting technique.
At the core of EBUG there are (1) a real-time engine for
understanding S2Rs written in natural language and (2) a
predictive model capable of suggesting likely S2Rs, trained
on real application usages. Supporting this core function-
ality are automated static and dynamic analyses capable
of extracting a detailed GUI model from a given mobile
app, including screenshots and GUI-related metadata. The
primary means of interaction with EBUG is via a “smart”
unstructured text ﬁeld, wherein reporters are tasked with
writing S2Rs in natural language. When a user begins writ-
ing, EBUG automatically recognizes different components
of a given S2R (i.e., action, target GUI-component) and at-
tempts to “auto-complete” missing information in a manner
akin to Google’s smart compose feature in Gmail [5]. EBUG’s
natural language understanding and predictive model are
capable of recognizing and suggesting both entire S2Rs
as well as individual components thereof. We developed
EBUG’s predictive modeling using a novel application of
n-gram language modeling to sequences of actions from
natural app usages collected by end-users. We provide a
video demonstration of EBUG in its online appendix [6].

The main assumption for EBUG to work effectively
is that reported S2Rs describe GUI actions in the app.
The description of an S2R needs to be provided through
text but does not need to follow a pre-imposed structure.
Forthermore, EBUG does not make assumptions on the type
of failure reported by the user as it focuses on the S2Rs
associated with the failure.

We evaluated EBUG in two user studies that involved
both bug reporting and bug reproduction tasks, based on
20 failures from 11 real-world apps. Speciﬁcally, the studies

included ten participants submitting ten bug reports each
and ten developers reproducing the submitted bug reports.
The ten participants that submitted the bug reports were
not developers but four of the participants have industry
exposure through internships as software engineers. The
participants that reproduced bug reports were all develop-
ers. These developers have experience in app development,
but were not the developers of the apps considered in
the study. Comparing bug creation and bug reproduction
against the state-of-the-art FUSION approach, we found that
participants were able to create high quality reports quicker,
as compared to FUSION, and that the resulting reports were
more reproducible that those created with FUSION.
This paper makes the following contributions:

• EBUG, a bug reporting system for mobile apps that
leverages automated natural language understanding,
static and dynamic analyses, and predictive modeling
to facilitate the reporting process.

• An implementation of EBUG for Android apps that is
publicly available, together with the artifacts and the
infrastructure we used in our evaluation [6].

• An evaluation that provides initial evidence of the

efﬁciency, effectiveness, and usefulness of EBUG.

2 TERMINOLOGY & MOTIVATING EXAMPLE
This section introduces some relevant terminology and
presents an example that we use to motivate our approach.

2.1 Terminology

Given a bug report that describes a failure for an app, we
informally use the terms relevant failure and relevant app to
indicate the failure and the app. We use the term steps to
reproduce (S2R) to indicate the textual description, in the
bug report, of an operation that should be performed on the
relevant app to reproduce the relevant failure (e.g., Enter
“Transaction” in the “Description” text box). We use the
terms GUI action (or simply action) and GUI interaction (or
simply interaction) interchangeably to indicate the operation
performed on relevant app’s GUI. A GUI action is composed
by its type (e.g., typing something in the GUI), the GUI
element (or simply element) affected by the action (e.g., the
text box in the GUI having the label “Description”), and,
if present, the action’s parameters (e.g., the “Transaction”
text). We use the term target GUI element (or simply target)
to indicate the GUI element affected by the action.

2.2 Motivating Example

Our motivating example, shown in Fig. 1, is a real bug
report [7] for GNUCASH [8], a widely used real-world app to
track expenses that has been installed over 100K times [9].
In the ﬁgure, the section labeled Bug Report contains the
bug report submitted by user 21nds, which contains the
same information as the one in GNUCASH’s issue tracking
system [7]. The report describes a bug that manifested itself
when the user changed the type of an account’s transaction
from Withdrawal to Deposit, and the app failed to store
this change. The bug report is followed by a discussion
(section labeled Discussion in Fig. 1), which was necessary
for developers to understand and reproduce the bug. In the

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

3

1 Bug Report

Title: Deposit / Withdrawal change existing entry
Author: 21nds
Text: An existing entry is created as a “Withdrawal”. Try to
change the existing entry from a “withdrawal” to “Deposit”
button next to the amount, it does change the color of the amount
to green and changes the button to say Deposit, but when you
click the checkmark to save the entry it does not save the entry
as a deposit. (also vice versa deposit ->Withdrawal) Also when
entering it will auto-ﬁll the amount from a previous transaction,
and automatically mark it as withdrawal or deposit but will not
save if you change it to the other.

2 Discussion

Message: #1
Author: codinguser
Text: Thanks for the feedback. Could you please provide
some more information? What version of the app are you
using? What Android version? Are you using double-entry
accounting? How many splits were in the transaction? Or was
it a simple transfer from one account to another.

...

Message: #6
Author: 21nds
Text: I have done a little more investigation. I have Deleted
ALL accounts and created one account “Checking”. I was
using it for a simple checkbook register, so single accounting.
It was working good about 2 releases ago in this fashion, but
one of the updates in the last couple updates made it stop
working for this. Thank You!

...

Message: #10
Author: fefe982
Text: Bug is conﬁrmed.

Fig. 1. Bug report and reproduction discussion from GNUCASH app.

ﬁrst message (message #1), an app developer (codinguser)
asked the user to provide additional information on the bug.
After a few messages among developers (message #6), the
user reported that she deleted all accounts, created a single
accounting account, and the transaction that she created in
this account was the one leading to the bug described in
the original report. After the user provided these additional
details, an app developer (fefe982) could conﬁrm the bug
(message #10). The bug report took six days and ten discus-
sion messages to be reproduced and conﬁrmed.

In the bug report, the user provided a description con-
taining some of the S2Rs but forgot to report some essen-
tial S2Rs that are necessary to reproduce the bug. These
circumstances caused additional and unnecessary work for
developers. Our bug reporting approach, EBUG, aim to
decrease this extra work by guiding users in thoroughly and
efﬁciently reporting S2Rs so that developers can more easily
reproduce reported bugs.

3 THE EBUG APPROACH
In this section, we present EBUG, an approach for enhancing
mobile app bug reporting. The basic idea behind EBUG is
to help users write high-quality bug reports by processing

natural language S2Rs in real-time (i.e., as S2Rs are typed in
the report) and guiding its users in adding S2Rs (or portions
thereof) that make the bug reports complete and accurate.

Fig. 2 provides an overview of EBUG’s workﬂow and its

two main phases: models generation and bug reporting.

The models generation phase operates ofﬂine and gener-
ates three app-related models (depicted with solid, double
borders in Fig. 2). First, in its GUI model generation step,
this phase takes as input the relevant app and a device and
combines static and dynamic analysis to compute the GUI
model of the app. Then, in its prediction models generation
step, this phase processes execution traces generated by the
app users and creates two statistical models: the GUI action
prediction model and the GUI element prediction model.

When a user is reporting a failure, the bug reporting
phase processes the S2Rs in real-time using natural language
processing, maps the content of the S2Rs to the GUI model
of the relevant app, and uses the prediction models to
suggest S2Rs (or portions thereof). The ﬁnal output of this
phase is a bug report containing S2Rs aimed at reproducing
the failure experienced by the user. The rest of this section
describes the two phases of EBUG in detail.

3.1 Models Generation Phase

3.1.1 GUI Model Generation

The GUI model (GM ) captures which GUI actions are pos-
sible in the different screens of the relevant app and encodes
GUI properties associated with actions. EBUG generates
GM ofﬂine, as it might not be always practical to analyze
S2Rs while the user is typing them in the bug report. For ex-
ample, a solution that dynamically analyzes the GUI of the
app while the user is typing the S2Rs might need to perform
time-consuming operations for backtracking the exploration
(e.g., restarting the app [10]) when the user edits/deletes
previously typed S2Rs, which might signiﬁcantly affect the
performance of the approach.

EBUG builds the GM of the relevant app by (1) com-
puting a static GUI model (GMs ) through static analysis, (2)
extracting a dynamic GUI model (GMd ) through a dynamic
analysis, and (3) combining GMs and GMd into GM . The
use of both static and dynamic analysis allows EBUG to
achieve high-coverage in modeling the GUI of the app and
its corresponding GUI actions.

The GUI models computed by EBUG (GMs , GMd , and
GM ) are directed graphs G = (N, E), where N is the set of
nodes in G and E ⊆ N × N . EBUG creates two types of
nodes. A node s ∈ S corresponds to a screen in the app,
whereas a node v ∈ V corresponds to a GUI element. The
set of nodes N contains the sets S and V , therefore having
N = S ∪ V . Both types of nodes have properties that are
stored in the nodes as tuples. The properties of a screen s
consist of its name and screenshot ((cid:104)name, screenshot(cid:105)). The
properties of an element v consist of its text, identiﬁer, type,
and screenshot ((cid:104)text, id , type, screenshot(cid:105)). (If one or more
of these properties is missing or cannot be computed for a
given element, EBUG stores an empty value for it.) EBUG
uses the text and the identiﬁer of an element to map S2Rs to
GUI actions in the app during the bug reporting phase. We
selected these properties as they are particularly suitable for

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

4

Fig. 2. High-level overview of the EBUG approach.

characterizing and identifying GUI elements from textual
information in bug reports [11].

EBUG computes two types of edges E = C ∪ T . A
containment edge c ∈ C ⊆ S × V captures the containment
relationship between a screen and a GUI element (s c−→ v),
whereas a transition edge t ∈ T ⊆ V × S connects a
GUI element to a screen (v t−→ s) and models the tran-
sition to s after exercising v. Transition edges have two
properties t ((cid:104)a-type, t-type(cid:105)) that capture the (a-type) of
action performed on v to trigger the transition to s, and
the (t-type) of analysis—static or dynamic—used to extract
the information from the app. Finally, EBUG models clicks,
long clicks, scrolls, text typing as actions in the model. It
also represents screen rotations in the model by associating
to each screen s a screen rotation operation. Speciﬁcally,
screen rotations are represented with a dummy GUI element
v contained in s whose transition t reaches s. The rest of this
section describes how EBUG computes GM .

To build the static GUI model GMs , EBUG leverages
a technique presented in related work [12] that captures
(1) the creation and propagation of GUI elements in the
code, (2) the relationship between screens and elements, (3)
what actions can be performed on the elements, and (4) the
effects of source code operations on the elements. To capture
this information, the static analysis used by EBUG [12]
builds and processes a constraint graph. The nodes in the
constraint graph model statements creating and affecting
the behavior of screens and GUI elements. Edges represent
constraints on the ﬂow of values between statements. The
analysis identiﬁes properties and relationships associated
with screens and GUI elements by propagating values in
the graph representing screens and GUI element objects.
To propagate the values, the analysis uses a ﬁxed-point
algorithm. After running the ﬁxed-point algorithm, the anal-
ysis extracts properties and relationships of screens and
GUI elements by analyzing the values ﬂowing to speciﬁc
statements in the graph. Based on the results of the static
analysis, EBUG ﬁrst adds all the screens Ss to GMs ; it then
adds all the element nodes Vs, while creating a containment
edge cs between an element’s screen and the element; it
ﬁnally adds transition edges Ts between nodes in Vs and Ss
(setting t-type = STATIC) based on the identiﬁed transitions.
EBUG builds the dynamic GUI model GMd by dynami-
cally exploring the relevant app through a depth-ﬁrst traver-
sal (DFT) that starts from the initial/main screen of the app
and navigates through the app by interacting with the GUI

elements on each screen. Speciﬁcally, the traversal clicks on
all the clickable components and types pre-deﬁned text (i.e.,
“Test”) on all editable elements that are reachable through
the traversal. (The pre-deﬁned value of text inputs could
be extended in future work to use semantically relevant
inputs as done in related work [13].) Before each step of the
traversal is executed, the technique extracts relevant infor-
mation about the current screen and its GUI elements. The
technique then executes the action associated with each GUI
component in a depth-ﬁrst manner. During the exploration,
the technique also captures the transitions between different
screens by tracking the visited screens. In the traversal, if a
GUI element is clicked and that operation would bring the
traversal to a screen not belonging to the app (e.g., clicking
a web link that would launch a browser app), the technique
would execute a back command in order to continue the
exploration in the relevant app. If the traversal exits the app
and reaches the home screen of the device/emulator for any
reason, the technique re-launches the app and continues
the traversal using the depth-ﬁrst strategy. During GUI
exploration, there may be cases in which there are no GUI
actions are available on the screen and EBUG must backtrack
to make progress. When this occurs, our approach restarts
the app (while resetting its data) and navigates to the screen
containing the GUI element that should be exercised next.
This step is possible because EBUG keeps track of visited
screens and exercised GUI elements.

d, ..., vn

When EBUG visits a new screen sd (i.e., sd /∈ Sd), it adds
sd to GMd . It then adds all the elements {v1
d } in sd to
d from sd to vi
GMd . Finally, it creates a containment edge ci
d
for each element vi
d (with i ∈ {1, ..., n}) just added to GMd .
EBUG determines that two screens are the same if their
GUI trees (composed of their GUI elements) are the same.
In computing this information, EBUG does not consider
the elements contained in “container elements” (e.g., list
containers), as the number of contained elements can change
dynamically. However, if a new element is present in such
containers, this element is added to GMd . When EBUG
identiﬁes that a screen sd is different from the previously
visited screen sp
d to sd
where vp
d exercised by the approach
(setting t-type = DYNAMIC). While adding a node or an edge
to GMd EBUG also collects the node’s or edge’s properties.
For example, when EBUG adds a GUI element to the graph,
the approach also saves the element’s screenshot. Finally,
for elements that do not have a text property, EBUG tries to
extract this information from the screenshot of the element

d, it adds a transition edge from vp

d is is the element from sp

GUI Model GenerationStaticGUI AnalysisDynamicGUI AnalysisGUIModelingPrediction Models GenerationTraceExtractionTraceAnalysisReal-TimeBug ReportAnalysisApp UserAppRelevantAppDeviceAppRelevantAppDeviceUserReportingA BugTracesGUI ElementPredictionModelGUI ModelStaticGUI ModelDynamicGUI ModelBugReportUser InputsBug ReportTextModels Generation PhaseBug Reporting PhaseGUI ActionPredictionModelIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

5

Fig. 3. Portion of the GUI model graph (GM) that includes parts of
the AccountsActivity and TransactionsActivity screens.

Fig. 4. AccountsActivity screen
in the GNUCASH app.

Fig. 5. TransactionsActivity
screen in the GNUCASH app.

using optical character recognition (based on the algorithm
implemented in [14]).

EBUG creates GM by computing the union [15] of GMs
and GMd that is, GM = GMs ∪ GMd = (Ns ∪ Nd, Es ∪ Ed).
While computing the union, EBUG identiﬁes matching
nodes in the two graphs. In this process, the approach does
not consider the screenshot property. However, EBUG uses
the screenshot values from the nodes in GMd when the
approach adds the nodes in GM . After taking the union
of the two graphs, if a node v ∈ V does not have any
transition to any node in S, EBUG creates a “dummy”
s ∈ S ∧ s c−→ v ∧ c ∈ C and t = (cid:104)DUMMY, DUMMY(cid:105) to denote
that after performing an action on v the screen does not
change. During the bug reporting phase, EBUG uses GM to
identify the GUI actions described by the S2Rs.

Fig. 3 provides a graphical representation for a portion
of the GM computed from GNUCASH, which we presented
in the motivating example (Sec. 2). This portion of the
graph models the screens depicted in Fig. 4 and 5. Nodes
with double borders are screens, while the remaining nodes
are the elements of the screens. Edges with solid lines are
containment edges, while the other edges are transitions.

3.1.2 Prediction Models Generation

In this step, EBUG builds two statistical models to cap-
ture how users interact with the relevant app and predict
likely interactions given a certain context (i.e., a sequence
of interactions). The two models are the GUI action and
the GUI element prediction models (GAPM and GEPM
in short, respectively). The bug reporting phase uses the
GAPM to provide recommendations for not-yet-typed S2Rs.
EBUG leverages the GEPM to, instead, help users complete
partially written S2Rs.

EBUG computes the two models from user-generated
execution traces. EBUG can collect these traces when de-
velopers distribute the relevant app for alpha or beta test-
ing, the app runs in the ﬁeld, or during in-house user
testing. EBUG produces a trace by intercepting the actions
performed on the app’s GUI and logging them in the
trace. The approach identiﬁes clicks, long clicks, scrolling
actions, text typing actions, and screen rotations. For each
user execution, EBUG stores actions as a sequence of tu-
ples in the trace. For each action, EBUG stores the tu-

ple: (cid:104)s-name, a-type, e-type, e-id (cid:105). The tuple contains
the name of the screen (s-name) where the action was
performed, the type (a-type) of the action, and the type
(e-type) and the identiﬁer (e-id ) of the element exercised
by the action. EBUG collects this information to uniquely
identify an action during the app’s execution. The approach
uses the traces to also reﬁne the GUI model. Speciﬁcally,
EBUG translates the information contained in an action
(containing screen, action type, and affected element) into
corresponding nodes and edges in the GUI model.

Fig. 6 provides an example of a user trace. The ﬁgure
reports a portion of a trace collected from the GNUCASH
app. In this portion, the user created an account and added
a transaction to the account. In Fig. 6, the fourth tuple
represents a click on the “plus” button highlighted with a
dashed red line in Fig. 4.

EBUG computes the two prediction models using two
different sets of traces. EBUG derives these two sets of traces
from the user traces. We call the two sets the GUI action
traces (GATs) and the GUI element traces (GETs). EBUG uses
the former set to create the GAPM and the latter set to
generate the GEPM. EBUG creates a GAT and a GET for each
user trace. The GAT and the GET are composed of a space-
separated sequence of tokens. EBUG computes the tokens
from the tuples in the corresponding user trace. Fig. 7 and 8
portray the GAT and the GET, respectively, derived from the
user trace depicted in Fig. 6. (In both traces, the symbol (cid:44)→
indicates that the sequence included in the trace continues.)
EBUG uses n-gram-based language modeling [16] to
build the GAPM and the GEPM from the GATs and the
GETs, respectively. At a high level, language models assign
probabilities to sequences of words and can be used to
estimate the probability of a word given its preceding words
(also known as the history of a word) [16]. EBUG uses this
characteristic of language models during its bug reporting
phase to predict the occurrence of an S2R (or part thereof)
given a sequence of previously occurring S2Rs. Because the
GUI action and element traces do not contain sequences
of S2Rs but contain sequences of GUI-action-based tokens,
in this step, EBUG creates token-based prediction models.
EBUG maps tokens to S2Rs during its bug reporting phase.
We chose to leverage n-gram based models over other more
complex sequence based prediction models (RNNs and

name = “AccountsActivity”screenshot = …text=“Add a newtransaction toan account”id =“btn_new_transaction”type = “ImageButton”screenshot = …name = “TransactionsActivity”screenshot = …text = “Accounts” id = “action_bar_title”type = “TextView” screenshot = …text = “Search” id = “menu_search”type = “TextView”screenshot = …text = “Description”id = “input_transaction_name”type = “”EditText screenshot = …text = “Save”id = “menu_save”type = “TextView”screenshot = …a_type = “CLICK”t_type = “DYNAMIC” a_type = “CLICK”t_type = “DYNAMIC” a_type = “DUMMY”t_type = “DUMMY” a_type = “TYPE”t_type = “STATIC” a_type = “TYPE”t_type = “DYNAMIC” a_type = “CLICK”t_type = “DYNAMIC” a_type = “CLICK”t_type = “DYNAMIC” 13524IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

6

...
(cid:104)AccountsActivity,CLICK,TextView,menu_add_account(cid:105)
(cid:104)AccountsActivity,TYPE,EditText,edit_text_account_name(cid:105)
(cid:104)AccountsActivity,CLICK,TextView,menu_save(cid:105)
(cid:104)AccountsActivity,CLICK,ImageButton,btn_new_transaction(cid:105)
(cid:104)TransactionsActivity,TYPE,EditText,input_transaction_name(cid:105)
(cid:104)TransactionsActivity,TYPE,EditText,input_transaction_amount(cid:105)
(cid:104)TransactionsActivity,CLICK,TextView,menu_save(cid:105)
...
Fig. 6. Portion of a user trace collected from GNUCASH app.

... AccountsActivity.CLICK.menu_add_account.TextView
(cid:44)→

AccountsActivity.TYPE.EditText.edit_text_account_name
AccountsActivity.CLICK.TextView.menu_save
AccountsActivity.CLICK.ImageButton.btn_new_transaction
TransactionsActivity.TYPE.EditText.input_transaction_name
TransactionsActivity.TYPE.EditText.input_transaction_amount
TransactionsActivity.CLICK.TextView.menu_save ...

(cid:44)→

(cid:44)→

(cid:44)→

(cid:44)→

(cid:44)→

Fig. 7. Portion of the GUI action trace computed from the user trace in Fig. 6.

... AccountsActivity.CLICK AccountsActivity.menu_add_account.TextView AccountsActivity.TYPE AccountsActivity.EditText.edit_text_account_name
AccountsActivity.CLICK AccountsActivity.TextView.menu_save AccountsActivity.CLICK AccountsActivity.ImageButton.btn_new_transaction
(cid:44)→
TransactionsActivity.TYPE TransactionsActivity.EditText.input_transaction_name TransactionsActivity.TYPE
TransactionsActivity.EditText.input_transaction_amount TransactionsActivity.CLICK TransactionsActivity.TextView.menu_save ...

(cid:44)→

(cid:44)→

Fig. 8. Portion of the GUI element trace computed from the user trace in Fig. 6.

other Neural Language models) as such models typically
require large amounts of training data to be effective.

EBUG’s n-gram models are able to provide suggestions
for sequences that are shorter than the order of the n-gram
models or for sequences that appear with a slightly different
context (i.e., arrangement) by leveraging the Kneser-Ney
smoothing method [17]. This method is one of the most
commonly used and best performing n-gram smoothing
methods [16]. Using the Kneser-Ney smoothing method,
lower-order n-gram sequences are encoded in the models.
Lower-order n-gram sequences are then used to make pre-
dictions for sequences shorter than the order of the models
and to provide suggestions for sequences appearing with a
slightly different context [16].

To compute EBUG’s prediction models, EBUG treats
trace tokens as words and uses the GATs and GETs as
the training data to build the the n-gram-based models
characterizing the GAPM and the GEPM. EBUG’s n-gram
models use closed vocabularies that are computed using the
tokens of the training traces. Based on this characteristic,
when EBUG translates the text of an S2R into a prediction
model token in the bug reporting phase, and the token is not
part of the model vocabulary, the technique does not make
any suggestions to the reporter. Because EBUG considers n-
gram based models and the length of the token sequence
used to make the prediction has a ﬁxed length (i.e., n-1), a
token that is not part of the vocabulary would impact (i.e.,
prevent) only n-1 predictions. This characteristic does not
affect the ability of the models to identify suggestions for
sequences that were not observed in the training traces.

To instantiate the language modeling framework de-
scribed so far, EBUG needs to determine the order of the
n-grams used by the models. EBUG determines the order
using the concept of wasted effort. We deﬁne wasted effort
(we) as the number of S2R-related suggestions that a user
needs to process before ﬁnding the suggestion that com-
pletes the sequence of already entered S2Rs. (The concept
of wasted effort is similarly used in the fault localization
literature [18] to evaluate fault localization techniques.) To
give an example, if a user needs to process two “unrelated”
suggestions before ﬁnding the “useful” suggestion, then
we = 2. When considering multiple prediction tasks, we
determine the performance of a prediction model using the
wasted effort score metric:

wes =

k
(cid:88)

i=1

(cid:30) k
(cid:88)

ci

wei

i=1

(1)

In Equation 1, wei is the wasted effort in task i and ci

is equal to 1 if the model provided the desired suggestion
for task i, 0 otherwise. To give an example, in a model that
provides three suggestions for every prediction task i, ci
will be equal to zero if none of the three suggestions is
the desired suggestion, and the wasted effort wei would be
equal to three. If the model provided the desired suggestion
in the third position of the suggestion list, ci will be equal
to one and wei will be equal to two.

Because wes is affected by the length of the suggestions
provided to the users, we search the best performing model
across two dimensions: the n-gram order and the length of
the suggestions. EBUG limits the search space by consider
n-grams up to length 10 and suggestions up to length 10.
(Our experiments detailed in Sec. 5 show that higher values
for the order and the length is highly unlikely to lead to
better results). EBUG searches the prediction model that
minimizes wes and uses that model in the bug reporting
phase. (The approach performs the search for each of the
two prediction models used by EBUG.) EBUG leverages
the GATs and the GETs to compute the wes of different
models. If the relevant app has a database of traces extracted
from existing bug reports, EBUG also uses these traces to
compute the wes of different models. While computing wes,
EBUG maps the concept of wasted effort to trace tokens
and uses the token sequences from the traces to check if a
token suggestion is a useful suggestion. A token suggestion
is useful if the suggested token is the same as the next
token in the sequence considered. EBUG computes wes for a
model using leave-one-out (sequence) cross-validation. For
each leave-one-out test sequence, the approach considers the
ﬁrst token in the sequence and uses the model to provide
suggestions for the next token. (For the GEPM, EBUG makes
predictions only for the element tokens.) EBUG repeats this
process until it reaches the last token in the sequence. EBUG
divides the sum of the wasted effort by number of “useful”
suggestions across all folds to compute wes for the model.
Finally, EBUG selects the model with the minimum wes
to ﬁnd the model that minimizes the effort needed by the
user to ﬁnd a “useful” suggestion. In computing wes, we
consider that the beneﬁt of ﬁnding a “useful” suggestion
has the same weight as the cost of processing an “unrelated”
suggestion. We leave as future work the idea of investigating
different weights in the computation of wes.

In the process of comparing prediction models, we do
not need to use an intrinsic evaluation metric such as per-
plexity [16] as our extrinsic evaluation metric wes is fairly
inexpensive to compute. Because EBUG can determine the
n-gram order of the prediction models automatically, the

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

7

Algorithm 1: S2Rs analysis in the bug reporting phase.
Input

: GM: GUI model for the relevant app
GAPM: GUI action prediction model
GEPM: GUI element prediction model
Output: bugReport: bug report describing the failure

1 begin
2

3

currS2REs = []
currScreen = Get-Current-Screen(GM,currS2REs)
Display-Screen(currScreen)

4
5 while TRUE do
6

upon event S2RS_TEXT_CHANGE do

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

currS2RsText = Get-S2Rs-Text()
textEdit = Get-Text-Edit()
Check-Updated(currS2RsText,textEdit,currS2REs)
prevS2REs = currS2REs
textEditOp = textEdit.Get-Text-Operation()
if textEditOp = INSERT then

newText = textEdit.Get-New-Text()
if textEdit.Get-New-Text() == ST then

currS2REs = Compute-S2REs(GM,currS2RsText,prevS2REs)
Display-Validated-S2Rs(currS2REs)
Display-Screen(GM,currS2REs)
S2RSuggestions = Get-GA-Suggestions(GAPM,currS2REs)
Display-S2Rs-Suggestions(S2RSuggestions)

else if newText == SPACE then

currS2RText = Get-Current-S2R-Text(currS2RsText,textEdit,

prevS2REs)

suggestionType = Analyze-Partial-S2R-Text(currS2RText)
if suggestionType == TARGET then

currS2RAction = Get-Current-S2R-Action(currS2RText)
precS2REs = Get-Prec-S2REs(Get-Prec-Text(currS2RText,

currS2RsText),prevS2REs)

GESuggestions = GEPM.Get-GE-Suggestions(precS2REs,

currS2RAction)

Display-GE-Suggestions(GESuggestions)

else if suggestionType == PARAM then

PSuggestion = Get-P-Suggestion(currS2RText)
Display-P-Suggestion(PSuggestion)
else if suggestionType == STRUCTURE then

SSuggestion = Get-S-Suggestion(currS2RText)
Display-S-Suggestion(SSuggestion)

else if textEditOp = DELETE then

currS2REs = Compute-S2REs(GM,currS2RsText,prevS2REs)
Display-Validated-S2Rs(currS2REs)
Display-Screen(GM,currS2REs)
upon event SUBMIT_BUG_REPORT do
currS2RsText = Get-S2Rs-Text()
prevS2REs = currS2REs
currS2REs = Compute-S2REs(GM,currS2RsText,prevS2REs)
bugReport = Get-Bug-Report()
bugReport.Set-PS2Rs(currS2REs)
return bugReport

(s2r -text), the abstract GUI action (AGA) representing the
S2R (a-action), the GUI action corresponding to the AGA
(action), the GUI screen on which the action is performed
(b-screen), and the screen displayed by the relevant app
after performing the action (a-screen).

An AGA is composed of the elements of a sentence that
describe a GUI action. More precisely, an AGA is character-
ized by the type of the action (a-type), the description of the
element exercised by the action (e-desc), and the description
of the parameters associated with the action (p-desc). All
of the AGAs contain the type of the action, but not all of
them contain the description of the element or the action,
as this information might not apply. We call the action
“abstract” because the action does not contain executable
information. To provide an example, the AGA for the sen-
tence Enter “Transaction” in the “Description” text box (that
describes an operation in the app of the motivating example)
is (cid:104)TYPE, the “Description” text box, “Transaction”(cid:105). We
detail how EBUG computes AGAs in Sec. 3.2.1.

A GUI action contains the type of the action (a-type),
the element affected by the action (element), and the
parameters of the action (params). An element is composed
of the GUI screen of the element (e-screen), the type
the element
of

the identiﬁer of

the element

(e-type),

Fig. 9. EBUG’s bug reporting interface.

approach can update the models as new training traces or
bug reports are collected from the users. Although EBUG
provides a mechanism to automatically determine the n-
gram order of the prediction models, developers can also
specify this value as well as the desired length for the
suggestions manually.

When EBUG reaches the end of this step, the GAPM and

the GEPM are ready for use in the bug reporting phase.

3.2 Bug Reporting Phase

In this phase, EBUG guides its users in the process of re-
porting failures so that app developers can easily reproduce
submitted bug reports. Speciﬁcally, when a user is reporting
a failure, the approach analyzes the S2Rs contained in the
report and provides S2R-related suggestions so that the user
can include high-quality (i.e., no missing or ambiguous)
S2Rs in the report. Additionally, EBUG also provides S2R-
related suggestions to accelerate the bug reporting process.
Users can report bugs using the interface depicted in
Fig. 9, where S2Rs are reported in the text box labeled
with 7 . Algorithm 1 describes how EBUG guides users
in the process of reporting S2Rs. The algorithm takes as
inputs the GUI model (GM), the GUI action prediction model
(GAPM), and the GUI element prediction model (GEPM). The
algorithm’s output is a bug report (bugReport) containing
the S2Rs reported by the user. This output is also the ﬁnal
output of the approach. At a high level, the algorithm uses
natural language processing to map the S2Rs to GUI ac-
tions in the application, analyzes the sequence of actions to
provide suggestions for not-yet-typed S2Rs, and processes
partial sentences to provide suggestions on how to complete
partially written S2Rs.

Throughout the bug reporting phase, Algorithm 1 pro-
cesses provided S2Rs and encodes them into a list of S2R
entities (currS2REs in Algorithm 1). EBUG creates an entity
for each S2R identifying a GUI action in the relevant app.
An S2R entity is a tuple that contains: the text of the S2R

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

8

TABLE 1
Some of EBUG’s rules to identify a GUI action and its details from a clause’s dependency tree.

ID

Grammar Rules

A1 (n1 ∈ C )

(n1 ∈ C )

A2

e1 =dobj
−−−−−→ (n2 )[tree(n2 )]e-desc
e1 =advmod
−−−−−−−→ (n2 =long)
e2 =dobj
−−−−−→ (n3 )[tree(n3 )]e-desc

A3 (n1 ∈ R)

A4

A5

(n1 ∈ S )

e1 =prt
−−−−→ (n2 ∈ D)[n2]p-desc
e2 =prep
−−−−−→ (n3 )

e3 =pobj
−−−−−→ (n4 )[tree(n4 )]e-desc

(n2 )[tree(n2 ) \ tree(n2 , prep)]p-desc

e1 =dobj
←−−−−− (n1 ∈ T )

e2 =prep
−−−−−→ (n3 )

e3 =pobj
−−−−−→ (n4 )[tree(n4 )]e-desc

Clause Example

Abstract GUI Action

Click the “Transaction” element.

(cid:104)CLICK, the “Transaction” element, (cid:105)

Long click the “Transaction” element.

(cid:104)LONG_CLICK, the “Transaction” element, (cid:105)

Rotate the screen.

(cid:104)ROTATE, , (cid:105)

Scroll up on the “Transactions” list.

(cid:104)SCROLL, the “Transaction” list, UP(cid:105)

Enter “Transaction” in the “Description” text box. (cid:104)TYPE, the “Description” text box, “Transaction”(cid:105)

TABLE 2
Some of EBUG’s rules to identify suggestions for partial clauses.

ID

Grammar Rules

Clause Example Suggestion

S1 (n1 ∈ C ) (cid:57)
S2 (n1 ∈ T ) (cid:57)
e1 =dobj
←−−−−− (n1 ∈ T )
e3 =pobj
−−−−−→ (n4 ∈ DET ) (cid:57)

e2 =prep
−−−−−→ (n3 )

(n2 )

S3

Click

Type

PARTICLE

PARAM

Type "Transaction"
in the

TARGET

(e-id ), and the text displayed by the element (e-text).
To provide an example, the GUI action for the sentence
Enter “Transaction” in the “Description” text box is (cid:104)TYPE,
(cid:104)TransactionsActivity, EditText, input_transaction_name,
“Description”(cid:105), [“Transaction”](cid:105). We describe how EBUG
identiﬁes GUI actions from AGAs in Sec. 3.2.2.

Algorithm 1 starts by initializing the current list of S2R
entities to be empty (line 2). In this initialization phase, the
algorithm also identiﬁes the app’s initial screen (line 3) and
displays this information to the user while preemptively
retrieving the screen information from GM. Although EBUG
displays this information, the user can also report S2Rs
starting from a different screen of the apps. After these
initialization steps, the algorithm executes its main loop
(lines 5-44), where EBUG helps the user in reporting S2Rs
(lines 6-37). The loop terminates when the user decides to
submit the bug report (lines 38-44). In this last step, the
algorithm stores the content of the bug report together with
the S2R entities associated with the S2R description.

We now detail EBUG’s NLP analysis (Sec. 3.2.1) and then
describe how EBUG leverages the analysis to helps users in
reporting S2Rs (Sec. 3.2.2).

3.2.1 Natural Language Processing of S2Rs

EBUG processes each sentence in the text of the S2Rs using
a two-step approach. First, EBUG preprocesses a sentence in
the report to simplify its analysis. Speciﬁcally, the approach
performs noise removal, lexicon normalization, and object
standardization [19] on the sentence as described in related
work [11]. For noise removal, EBUG discards content within
parenthesis, which, in our experience, is generally unneces-
sary for mapping an S2R to the corresponding GUI action.
For lexicon normalization, EBUG normalizes non-standard
words to their canonical form (e.g., EBUG replaces the
word “Tap” with “Click”). (We use the word replacements
identiﬁed in related work [11] to perform this task.) For
object standardization, the approach simpliﬁes the text of a
sentence by replacing sequences of words referring to a GUI
element in the relevant app with a freshly-created textual
identiﬁer. EBUG performs this operation using the textual

Fig. 10. Dependency tree for a sentence.

properties saved in the GUI model (GM) and leveraging the
fact that the text displayed by an app usually follows a title
or sentence-case convention [20]. This last operation helps
in simplifying the analysis of the text of the S2Rs.

After its preprocessing step, EBUG analyzes the sen-
tence. More precisely, the approach analyzes each clause
that appears in a sentence, as each of the clauses could
described a different action in the relevant app. To identify
the clauses of a sentence, the approach leverages related
work [21], which parses a dependency parse tree recursively
and, at each step, predicts whether an edge should yield
an independent clause. EBUG analyzes clauses using their
dependency tree representation [16], [22]. A dependency
tree is a directed graph that captures the syntactic structure
of a clause and provides a representation of the grammatical
relations between words in the clause. Words are nodes, and
relations are edges in the tree. Fig. 10 provides an example
of a dependency tree (extracted from the clause enter “Trans-
action” in the “Description” text box). (The dependency tree
in the ﬁgure does not report relations between words and
punctuation for readability.)

To identify whether a clause describes a GUI action,
EBUG analyzes a clause’s dependency tree using a rule-
based approach as done in related work [11], [23]. Specif-
ically, EBUG uses grammar rules to identify whether a tree
represents a GUI action and to and extract the relevant
information characterizing the action into an AGA. Table 1
reports some of EBUG’s grammar rules to extract AGAs
(For the complete list of grammar rules, please check our
online appendix [6].) Column Grammar Rule reports the rule
information, column Clause Example provides an example of
a clause matched by the corresponding rule, and column
Abstract GUI Action details the AGA extracted from the
e1 =dobj
corresponding clause. For example, the rule (n1 ∈ C )
−−−−−→
(n2 )[tree(n2 )]e-desc matches the clause Click the “Transaction”
element., and (cid:104)CLICK, the “Transaction” element, (cid:105) is the AGA
extracted from clause.

EBUG uses grammar rules also to analyze partial clauses.
Table 2 reports some of the grammar rule used for this task.
Given a partial clause, EBUG uses the grammar rules to

Enter “Transaction” in the “Description” text box.pobjdobjprepdetnnnnIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

9

Fig. 11. Example of the information used by EBUG to analyze new text in the S2Rs description.

identify which suggestion to make. The approach makes
three types of suggestions: PARTICLE, PARAM, and TARGET.
Suggestions of type PARTICLE (e.g., “the”) have the objective
to help users write well-formed clauses, which help EBUG’s
analysis. Suggestions of type PARAM remind the users that
they also need to specify a parameter for certain types of
action (e.g., when users describe that they typed some text
in the GUI, they should also specify the text they typed).
Suggestions of type TARGET provide users with a list of likely
targets for the partial S2R, and this list is computed using
the GUI element prediction model. For example, given the
partial clause Type “Transaction” in the, EBUG uses the rule
S3 to identify that this clause requires an action target to
deﬁne a complete S2R.

3.2.2 S2Rs Reporting

This part of EBUG is summarized in Algorithm 1 at lines 5-
44. EBUG’s analysis performs different operations based on
whether the user enters (lines 12-33) or removes (lines 34-
37) text to/from the S2Rs description. At a high level, when
the user enters new text (Case 1), the algorithm helps the
user by validating the user-provided S2Rs (i.e., informs the
user whether the S2Rs can be mapped to GUI actions in
the app) and by providing S2R-related suggestions on how
to complete the report. When the user removes text from
the S2Rs description, EBUG re-validates user-provided S2Rs
(Case 2). We now describe Case 1 and Case 2 in detail.

Case 1: Handling Additions to the S2Rs Description

EBUG processes new text in the S2Rs description only
when the user either enters a sentence terminator (ST) or
a space character (SPACE). When the user enters a sentence
terminator (Case 1a), EBUG assumes that the user ﬁnished
describing a certain S2R and provides suggestions for possi-
bly following S2Rs. When the users enters a space character
(Case 1b), the algorithm considers the description of the cer-
tain S2R to be partial, and helps the user in completing the
S2R by providing suggestions on its content and structure.

Case 1a: Processing Complete S2Rs
When the user enters a sentence terminator (lines 14-
19), the algorithm (i) maps the S2Rs to GUI actions in

the app while encoding the GUI actions into S2R en-
tities (Compute-S2REs), (ii) informs the user about vali-
if necessary, updates EBUG’s interface
dated S2Rs and,
to display the app’s screen associated with the sequence
of GUI actions extracted from the user-provided S2Rs
(Display-Screen), (iii) computes suggestions for not-yet-
typed S2Rs (Get-GA-Suggestions), and (iv) provides the
suggestions to the user (Display-S2Rs-Suggestions). Fig. 11
provides an example of some of the information computed
by EBUG in this process. We now describe the four steps of
EBUG that perform these operations.

Step 1: Mapping S2Rs to S2R entities. After the user com-
pletes a speciﬁc S2R , EBUG uses the NLP analysis described
in Sec. 3.2.1 to extract a list of abstract GUI actions (AGAs)
from the S2Rs description. Given a list of AGAs (e.g., the list
of tuples labeled with 1b in Fig. 11), EBUG uses the GUI
model (GM in Algorithm 1) to map the AGAs into a sequence
of GUI actions and encode the actions into S2R entities.
At a high level, EBUG identiﬁes GUI actions based on the
description contained in the AGAs. The mapping process
operates as follows. EBUG uses the list of S2R entities from
the previous mapping (prevS2REs) to identify the oldest
S2R that the approach did not process yet. To do so, EBUG
processes the newly computed AGAs and ﬁnds the longest
sequence matching the AGAs in prevS2REs. The approach
starts processing AGAs from the ﬁrst one that does not have
a match, and also processes all subsequent AGAs. EBUG
looks at past matches to avoid processing AGAs that did
not change. If prevS2REs is empty, EBUG starts the mapping
task from the ﬁrst AGA in the list.

(i)

(iii)

(ii) GM,

information:

For each AGA, EBUG identiﬁes the corresponding
the AGA
action using four pieces of
(aga),
the screen (rs) “reached” by the
GUI action matched with the previously considered
the screen (lrs) “reached” by the last
AGA, and (iv)
GUI action that was successfully matched to an AGA.
For example, based on the GUI model represented in
if action (cid:104)CLICK, (cid:104)AccountsActivity, ImageButton,
Fig. 3,
btn_new_transaction, “Add transaction to an account”(cid:105),
[](cid:105) was the action matched to the previous AGA, then rs
would correspond to TransactionsActivity, and lrs would
correspond to the same screen. For the ﬁrst AGA in the

[…] ⟨CLICK,the “Save” element,⟩;⟨CLICK, the “new transaction” button,⟩[…] Click on the “Save” element. Click the “new transaction” button.[…] ⟨CLICK,⟨AccountsActivity,TextView,menu_save⟩,⟩;⟨CLICK,⟨AccountsActivity,ImageButton,btn_new_transaction,“Add transaction to an account”⟩,⟩S2Rs DescriptionAbstract GUI ActionsGUI ActionsGUI Action Prediction Model Suggestion[…] AccountsActivity.CLICK.TextView.menu_save AccountsActivity.CLICK.ImageButton.btn_new_transaction TransactionsActivity.TYPE.EditText.input_transaction_nameS2Rs Suggestion[…] Click on the “Save” element. Click the “new transaction” button. Type “value” in the “Description” text boxGUI Action for Prediction⟨TYPE,⟨TransactionsActivity,EditText,input_transaction_name,“Description”⟩,[]⟩EBug’s S2Rs Interfafce1a1b1c2a3a3b3cS2R Entities1d[…] ⟨Click on the “Save” element.,⟨CLICK,the “Save” element,⟩,⟨CLICK,⟨AccountsActivity,TextView,menu_save⟩,⟩, AccountsActivity, AccountsActivity⟩;    ⟨Click the “new transaction” button.,CLICK, the “new transaction” button,⟩, ⟨CLICK,⟨AccountsActivity,ImageButton,btn_new_transaction,“Add     transaction to an account”⟩,⟩, AccountsActivity, TransactionsActivity⟩IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

10

R(lrs, e-desc, v)=

(cid:40)

α ∗ Max (S (e-desc, v.e-text), S (e-desc, v.e-id)) + (1 − α) ∗

0

1
1+D(lrs,v)

if Max (S (e-desc, v.e-text), S (e-desc, v.e-id)) > β
otherwise

Fig. 12. Equation to rank relevant GUI elements when mapping abstract GUI actions into GUI actions.

list, EBUG uses the initial screen of the relevant app as the
value for rs and lrs. When the approach is not able to ﬁnd
a matching GUI action while analyzing the preceding AGA,
EBUG assigns the special value (∗) to rs, which indicates
that the currently reached screen could be any screen in GM.
To ﬁnd the GUI action corresponding to an AGA, the
approach performs four operations. First, EBUG identiﬁes
all the relevant elements v in rs. (If rs=∗, EBUG selects
all the elements in GM. ) Second, EBUG reﬁnes the list of
elements by selecting only the ones that have an outgoing
t−→) such that the action type of the edge
transition edge (
is equal to the action type of the AGA. Third EBUG
ranks the elements based on how relevant they are to
the element description (e-desc) contained in the AGA
and how close they are to lrs. The approach uses the
equation in Fig. 12 to determine how relevant (function
R(lrs, e-desc, v) or R in short) an element is. Finally, if
the top-ranked element has R > 0, the approach selects
the element as the target of the GUI action and creates
the corresponding S2R entity. For example, considering
the AGA (cid:104)CLICK, the “new transaction“ button, (cid:105),
and
the GUI model
represented
rs=lrs=AccountsActivity, EBUG would ﬁnd that
the
(cid:104)AccountsActivity, ImageButton,
top-ranked element
btn_new_transaction, “Add transaction to an account”(cid:105).
When the top-ranked element has R = 0, EBUG creates
an S2R entity without a GUI action, indicating that the
approach could not ﬁnd a matching GUI action for the S2R.
We now describe the details of the equation in Fig. 12.

Fig.

in

3,

is

the meaning of every word in the vector representation. For
the element identiﬁer, EBUG also performs an additional
preprocessing step in which the approach splits the value
of the property into words. Speciﬁcally, EBUG replaces un-
derscores with spaces and splits apart composite words that
follow a camel case convention, both of which are common
occurrences in apps [31]. To determine the similarity of two
properties (e.g., e-desc and e-text), the approach computes
the cosine similarity between the corresponding vectors.
The cosine similarity ranges between [0, 1], where 1 corre-
sponds to the highest similarity. If both S (e-desc, v.e-text)
and S (e-desc, v.e-id ) are below a threshold value β, EBUG
does not consider the element as being relevant to the AGA
and R returns 0. The approach uses 0.5 as the default value
for β. EBUG uses this value to disregard unrelated matches
and the value is based on an empirical analysis done in
related work [11].

1

EBUG uses the second component ((1 − α) ∗

1+D(lrs,v) ) to
prioritize matches with elements belonging to screens that
are in the proximity of the screen containing the previously
matched element. Speciﬁcally, D(lrs, v) is the distance be-
tween lrs and the screen vs containing v. The approach
measures the distance by ﬁnding the shortest path between
lrs and vs in GM and counting the number of transition edges
in the path. If EBUG does not ﬁnd a path connecting lrs and
vs, the approach sets D(lrs, v)=0.
1+D(lrs,v) ranges between
[0, 1], where 1 corresponds to the shortest distance (i.e., the
screens are the same). When lrs=rs and rs (cid:54)= ∗, the value of
1+D(lrs,v) is always 1 as EBUG selects elements from rs.

1

1

1

The equation is based on three parameters (lrs,e-desc,
and v). The ﬁrst part of the equation has two components,
α ∗ Max (S (e-desc, v.e-text), S (e-desc, v.e-id )) and (1 − α) ∗
1+D(lrs,v) . EBUG uses the ﬁrst component to compute the
similarity between e-desc and v. The approach computes
the similarity by taking the maximum (function Max ) of
two values. The ﬁrst value corresponds to the semantic
similarity between e-desc and the text of the GUI element
(e-text). The second value represents the semantic similar-
ity between e-desc and the identiﬁer of the GUI element
(e-id ). EBUG uses semantic similarity values, and not string-
distance-based metrics (e.g., the Levenshtein distance [24]),
to account for the fact that users might use, in their bug
reports, words that have a different representation from the
ones in the developer-deﬁned labels, but these words could
be either synonyms or similar in meaning (e.g., “create”
and “add”). EBUG computes semantic similarity values
using vectors based on word embeddings extracted from
a FastText model [25]. Our approach uses FastText as it is
has been shown to outperform related work [26], [27], [28],
[29], [30] in different contexts [25].

The approach represents each of the three textual prop-
erties (e-desc, e-text, and e-id ) in Fig. 12 as a single vector
computed by removing stopwords (as they introduce unnec-
essary noise) and by averaging the vectors of the remaining
words. By taking the average, EBUG is able to incorporate

Finally, the equation in Fig. 12 uses the constant α to
deﬁne the weight of the two equation’s components. EBUG
uses 0.5 as the default value for α to assign the same weight
to the two components.

Step 2: Displaying Validated S2Rs and Current Screen. In
this step, EBUG provides feedback to the user based on
whether the approach was able to map the S2Rs into GUI
actions. The goal of this step is to inform the user that
certain S2Rs might require further reﬁnement. Speciﬁcally,
for the S2R entities that EBUG could map to GM (i.e., entities
containing the corresponding GUI action), the approach
displays and highlights (in green) the text of the S2Rs in
its graphical interface. For S2R entities not mapped to GM,
the approach reports their S2Rs in its graphical interface but
does not highlight them, as we wanted to avoid reporting
false negatives (i.e., valid actions not mapped to GM due
to the possibly partial information encoded in the model).
EBUG displays the results of the validation process in the
section of Fig. 9 labeled with Steps validation. During this
step, the approach also updates its interface to display the
reached screen rs associated with the last reported S2R
entity. If rs=∗, EBUG displays a screen informing the user
that the last S2R was not matched. The screen is displayed
in the part of Fig. 9 labeled with Current Screen.

Step 3: Computing S2R Suggestions. EBUG uses the GUI
action prediction model (GAPM in Algorithm 1) to provide

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

11

suggestions for not-yet-typed S2Rs. To identify relevant
suggestions, the approach ﬁrst encodes the S2R entities into
a sequence of tokens and then uses GAPM to identify the next
token (S2R) for the sequence. Speciﬁcally, EBUG translates
the last n − 1 S2R entities into tokens. (The tokens are in the
same order as they appear in the S2Rs description.) EBUG
uses the GUI actions associated with the S2R entities to
compute the sequence tokens. The structure and the content
of the tokens processed by GAPM are detailed in Sec. 3.1.2. In
Fig. 11, the parts labeled with 1d and 2a show how EBUG
translates S2R entities into tokens suitable for GAPM. Once the
approach retrieves the token suggestions from GAPM, EBUG
uses them to display suggestions to the user.

Step 4: Providing S2R Suggestions to the User. Given a
list of suggestions, EBUG displays them to the user in the
same order as provided by GAPM. First, for each token, the
approach retrieves additional information associated with
the token (e.g., its text) from GM Then, the approach displays
the text of the S2R corresponding to the token, and, when
available, a screenshot of the GUI element affected by the
action associated with the token. (EBUG does not always
have a screenshot for a GUI element, as the approach
uses static analysis to identify some of the elements in the
GUI model). To translate a token into an S2R, EBUG uses
textual templates. EBUG has templates for each GUI action
supported by the approach. The approach ﬁlls the template
with the information associated with the action represented
by a token. In Fig. 11, the parts labeled with 2a , 3a , 3b ,
and 3c show how EBUG displays suggestions to the user.

Case 1b: Processing Partial S2Rs
When the user enters a space character (lines 20-33), EBUG
(i) analyzes the text of the S2R to identify whether the S2R
needs further reﬁnement (Analyze-Partial-S2R-Text), (ii)
computes suggestions on how to complete the S2R, and (iii)
provides the suggestions to the user.

Step 1: Analyzing Partial S2Rs. EBUG uses the NLP anal-
ysis described in Sec. 3.2.1 to analyze the current S2R and
identify how to help the user in completing the S2R. EBUG
can (i) suggest likely targets (i.e., GUI elements) for the
type of GUI action described in the S2R (lines 23-27), (ii)
inform the user that the S2R should also specify a parameter
(i.e., the text to be typed in a text box) for the GUI action
described in the S2R (lines 28-30), and (iii) add grammatical
particles to the S2R so that the S2R is easier to analyze
(lines 31-33).

Step 2: Computing Partial S2R Suggestions. The approach
leverages the GUI element prediction model (GEPM in Algo-
rithm 1) to suggest likely targets for a certain S2R. To iden-
tify relevant suggestions, the approach extracts the action
type associated with the S2R, encodes the action type into a
model token, transforms preceding S2R entities (precS2REs)
into model tokens, and uses the sequence of tokens to
identify predictions based on GEPM. Similarly to what EBUG
does in the case of a complete S2R suggestion, if an S2R
entity does not have a corresponding GUI action, EBUG
does not compute partial S2R suggestions for sequences that
include the entity. Once the approach retrieves the token
suggestions from GEPM, EBUG stores them in the same order
as they are provided by the model and uses them to display
suggestions to the user. Additionally, the technique is also

able to provide suggestions based on partially typed GUI
elements. In this case, the suggestions are not based on
the GEPM but are based on text matching. When EBUG
identiﬁes that the S2R needs a parameter or a suggestion for
improving the structure of the S2R, the approach provides
the corresponding textual suggestion.

Step 3: Providing Partial S2R Suggestions to the User. EBUG
offers suggestions for GUI elements similarly to how it
provides suggestions for complete S2Rs. Suggestions for pa-
rameters and the structure of certain S2Rs are automatically
added to the bug report and the user can accept them by
pressing the tab character.

Case 2: Handling Removals from the S2Rs Description

When the user removes text from the S2Rs description
(lines 34-37), the algorithm analyzes whether the sequence
of GUI actions associated with the S2Rs has changed, and
updates EBUG’s interface to display the app’s screen as-
sociated with the action sequence. Speciﬁcally, EBUG (i)
maps S2Rs into S2R entities, (ii) validates the S2Rs, and (iii)
displays the screen reached by the last S2R entity in the
sequence. The approach performs these steps following the
methodology described in Case 1a-Step 1 and Case 1a-Step 2.

4 IMPLEMENTATION

We implemented EBUG in a system that supports bug
reporting for Android apps. The system consists of three
main modules. The GUI models generation module is written
in Java and leverages Gator [12] to perform EBUG’s static
analysis, builds on UiAutomator [32] to complete the ap-
proach’s dynamic analysis, and stores the GUI model using
the Neo4J [33] graph database. The prediction models genera-
tion module leverages the Getevent [34] and UIAutomator
utilities to collect user traces and builds on the MITLM
toolkit [35] and the MonkeyLab infrastructure [36] to gen-
erate the n-gram based language models characterizing the
GUI-action and GUI-element prediction models. Finally, the
bug reporting module is implemented as a web application.
This web application leverages the Google Cloud Natural
Language infrastructure [37] to perform EBUG’s natural
language processing analysis and builds on the Quill li-
brary [38] to implement the interactive S2R reporting ap-
proach. The module uses the FastText model trained on the
Common Crawl dataset [39] to identify the semantic similar-
ity between S2R descriptions and GUI element properties.
The web application also provides access to bug reports
saved in a MySQL database.

5 EMPIRICAL EVALUATION

To determine the effectiveness and efﬁciency of EBUG, we
used the implementation of our approach and performed
two user studies. The studies’ main focus is to assess the
extent to which EBUG is able to facilitate the creation of
highly reproducible bug reports. In the studies, we also
compared the efﬁciency and effectiveness of EBUG with
those of FUSION [4]. We selected FUSION as a baseline
because the technique also aims to improve bug reporting,
handles Android apps, and has been shown to create bug
reports that are easier to reproduce compared to the ones

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

12

In the evaluation, we targeted the following research

A07 MILEAGE

Finance 3.0.8

generated using traditional issue tracking systems [4]. The
effectiveness of this reporting system [4] makes it a strong
baseline against which to compare EBUG.

In the evaluation, we also compare the effectiveness
of EBUG’s GUI-action and GUI-element prediction models
with those of prediction models based on AKOM [40] and
CPT+ [41]. We selected AKOM as a baseline because the
technique generates a model to perform sequence predic-
tions, the technique functions effectively even without a
large amount of training data, and the approach has been
used to perform sequence predictions in numerous applica-
tions and various domains [40]. We selected CPT+ as it has
similar characteristics as AKOM (e.g., the technique func-
tions effectively even without a large amount of training
data) and, in some domains, the technique was shown to
provide better accuracy than AKOM [41].

questions (RQs):

• RQ1: Do developers using EBUG’s reports reproduce more

failures compared to when using FUSION’s reports?

• RQ2: Is bug reporting with EBUG more efﬁcient than

FUSION?

• RQ3: Do developers using EBUG’s reports reproduce failures
more efﬁciently compared to when using FUSION’s reports?
• RQ4: Is bug reporting with EBUG leading to a better user

experience than the one with FUSION?

• RQ5: Do developers using EBUG’s reports have a better user

experience compared to FUSION?

• RQ6: Do EBUG’s GUI-action and GUI-element prediction
models provide a better wasted effort score than prediction
models based on AKOM and CPT+?

5.1 Experimental Context

In this section, we present the benchmarks used in the
evaluation, describe the process used to collect user traces,
and detail the user studies’ characteristics.

5.1.1 Benchmarks

Our empirical evaluation is based on 20 unique, real-world
failures from 11 apps. We identiﬁed the failures by randomly
selecting bug reports from a collection of bug reports used
in related work. These bug reports were used to evaluate
different aspects of mobile app bug reporting [4], [42].
Before including a bug report in our benchmark dataset,
we ﬁrst conﬁrmed that the failure described in the report
was still reproducible. To reproduce the failure, we ran the
app on a Nexus 7 emulator. The versions of the app and the
Android system running on the emulator were the same as
those used in related work. (We used a Nexus 7 emulator
as FUSION’s empirical evaluation also used this emulator.)
The ﬁrst 20 bug reports that we randomly selected were
reproducible. Nineteen failures were reproducible on the
emulator running Android 4.4. One failure was reproducible
on the emulator running Android 5.0. Nine bug reports
were used in FUSION’s empirical evaluation [4], whereas
the remaining eleven bug reports were used in recent work
by Chaparro et al. [42]. All the 20 bug reports belong to
free-text-based issue tracking systems. Among the failures
described in the 20 bug reports, four failures manifest as
crashes and sixteen failures provide an incorrect output to

TABLE 3
Apps, bug reports, and failures used in the empirical evaluation. OIB =
observable incorrect behavior of the reports (O = output, C = crash).

App
ID

App Name

Category Version

Report
ID

Failure
ID

Min GUI
Actions

OIB

A01 AARD
A02 ACV
A03 CAR REPORT Auto
Prod.
A04 DOC VIEWER

Books
1.6.10
Comics 1.4.1.4

3.4.1
2.2

A05 DROIDWEIGHT Heath

1.5.4

A06a GNUCASH

Finance 1.4.3

A06b GNUCASH

Finance 2.1.1

A08 NOTEPAD
A09 OLAM
A10 SCHEDULE

Tools
Books
Books

1.06
1
1.32.2

A11 TIME TRACKER Prod.

0.17

104
11
43
48
38
25
247
256
615
620
663
49
53
64
23
2
154
24
25
46

F01
F02
F03
F04
F05
F06
F07
F08
F09
F10
F11
F12
F13
F14
F15
F16
F17
F18
F19
F20

4
7
10
9
5
11
16
20
9
11
10
15
12
11
6
2
7
11
10
8

O
C
O
O
O
O
O
O
O
O
C
O
O
O
C
C
O
O
O
O

the user. Examples of output errors include displaying an
erroneously computed value, incorrectly displaying a GUI
element, and bringing the user to the wrong screen.

Comparing our dataset with the ones we used to create
it, we can report the following. The dataset from the baseline
technique we considered [4] includes 15 failures from 14
apps. That dataset has three failures that manifest as crashes
and 12 failures that manifest as output failures. The second
dataset [42] contains 24 failures from six apps. Four failures
manifest as crashes and 12 manifest as output failures. We
believe that the size and diversity between our dataset and
these two other datasets are comparable.

Table 3 details some of the characteristics of the apps,
bug reports, and failures. Speciﬁcally, column APP ID pro-
vides a unique identiﬁer for the app, App Name reports
the name of the app, Category provides the category of the
app on the Google Play store [43], Version details the app’s
version used to reproduce the failure, Report ID reports the
identiﬁer of the bug report describing the failure, Failure ID
provides a unique identiﬁer for the failure, Min GUI Actions
details the minimum number of GUI actions necessary to
reproduce the failure, and OIB labels the type of observable
incorrect behavior (i.e., crash or output failure). For the
GNUCASH app, we use two different identiﬁers (A06a and
A06b) because we reproduced the failures on two different
versions of the app. The average and the median number of
GUI actions necessary to reproduce the failures are 9.7 and
10, respectively. These numbers highlight that the failures
we considered require multiple steps to reproduce the issue.

5.1.2 GUI Models and User Traces

EBUG uses pre-computed GUI models in its bug reporting
phase. In the evaluation, we computed one GUI model for
each app version considered. The generation of each GUI
model is based on static and dynamic app analysis. The
static analysis took, on average, 16 seconds to complete.
For the dynamic analysis, we did not perform the analysis

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

13

exhaustively but, instead, used a timeout of 60 minutes, as
this timeout was shown to be effective in related work [4].

In our experiments, we used user traces to generate the
prediction models and gathered the traces by designing
a data collection activity. In this activity, we invited nine
participants to generate the traces. The participants’ demo-
graphics include three participants without any computer
science (CS) background, three undergraduate students in
CS, and three graduate students in CS. In the activity, we
assigned four apps to each participant, asked the partici-
pants to ﬁrst get familiar with each app’s functionality for
ﬁve minutes, and then invited them to collect at least three
traces for each app while exercising the app’s functionality
as they would do in a normal use of the app. In total, the
participants collected 152 traces. The overall number of GUI
actions in the traces is 3, 580. We asked participants to collect
traces for both A06a and A06b as the apps’ GUI differed
signiﬁcantly. Finally, the data collection activity took about
two weeks to complete.

Based on this set of traces, we generated the predic-
tion models following the methodology described in Sec-
tion 3.1.2. Table 7 reports the n-gram order (column Order
under EBUG headers) and the number of suggestions pro-
vided by each prediction (column SN under EBUG headers)
for both the GUI-action and GUI-element prediction models.

5.1.3 Bug Reporting Study

To determine the extent to which EBUG is able to facilitate
the creation of highly reproducible bug reports and how
efﬁciently users can do so, we ran a user study involving ten
participants. The participants’ demographics include two
undergraduate and eight graduate students in CS, and none
of the participants took part in the data collection activity.
Additionally, the participants had been studying CS for
an average of 7.65 years (min=2, max=14), had 7.8 years
(min=3, max=14) of programming experience, and had an
average of self-determined experience level in mobile app
programming of 4.7 (min=2, max=8) based on 1-10 assess-
ment scale. None of the participants in this study is a profes-
sional developer, but four of the participants have industry
exposure through internships as software engineers.

In the study, we asked the participants to report the
failures from Table 3, using both FUSION and EBUG. Specif-
ically, each participant reported ﬁve failures using FUSION
and ﬁve failures using EBUG. We assigned failures to par-
ticipants making sure that participants would not report the
same failure with both tools and would not report two fail-
ures belonging to the same version of the app while using
the same tool. We took these two measures to minimize
bias in the reporting task. In the study, the subjects used
anonymized variants of the tools, and we did not inform
them about which tool was ours and which one was the
baseline. It should be noted that we ensured that the order
in which each participant was exposed to a tool was set such
that half of the participants used EBUG ﬁrst and half used
FUSION ﬁrst to mitigate potential effects of learning bias.

Before starting the study, we asked participants to watch
two tutorial videos (about ﬁve-minutes long) explaining
how to use FUSION and EBUG. (The participant could skip
watching the tutorial videos if they chose to do so.) At the
beginning of the study, we provided each participant with

a document containing the failures the participant should
report and the tool the participant needed to use to report a
certain failure. We provided information about each failure
in the form of a video. The video contained the sequence
of GUI actions necessary to reproduce the failure and how
the failure manifested itself. We recorded these videos by
performing the GUI actions reported in Table 3 on a Nexus
7 emulator. In the document, we asked participants to report
failures based on what they saw in the videos. We chose to
expose bugs through video because we are interested in par-
ticipants experience reporting bugs assuming that an end-user
has a good understanding of the failure that they are reporting.
As such, while exposing users to annotated videos of bugs
may not represent a real-world scenario, it provided us with
an avenue to clearly illustrate app failures in a manner in
which we could be reasonably certain that the participant
understood the bug. The document listed failures so that
half of the participants would start reporting failures using
FUSION, and the remaining half would start with EBUG. We
randomized the order of the remaining failure-tool pairs in
each document. The document also instructed participants
to report the failures in the order they appeared in the
document. During the study, we asked the participants to
record the time it took them to report each failure and record
their screen as they were reporting the failures. (We used the
screen recordings to check whether the information entered
by participants was the same as the one saved in the two
tools’ database. After watching the screen recordings, we
could conﬁrm that the information was the same.) Finally,
during the study, the participants did not have access to the
original bug reports of the failures.

At the end of the study, we asked the participants
to complete a survey. In the survey, participants entered
demographics information, recorded the time it took them
to report each failure, and answered ten user experience
(UX) questions for both FUSION and EBUG. Among the ten
questions, six were usability questions, and four were user
preference questions. Table 4 reports the usability (R-UN
questions) and user-preference questions (R-PN questions)
we asked to the participants. We formulated the usability
questions based on the system usability scale (SUS) by John
Brooke [44], as this scale is often used to compare the us-
ability between systems [44]. The answers to these usability
questions are structured using a ﬁve-level Likert scale that
ranges from “Strongly disagree” to “Strongly agree”. We
formulated the user preference questions based on the user
experience honeycomb developed by Peter Morville [45]
and asked the participants to answer the questions using
free-form text ﬁelds. Finally, the user experience questions
were also used in the original evaluation of FUSION [4].

We ran the study over a three-week time frame and
obtained a dataset of 80 bug reports. The dataset includes 40
bug reports generated by each tool and has two bug reports
for each failure-tool pair.

5.1.4 Bug Reproduction Study

To determine whether the bug reporting study participants
created reproducible bug reports, we ran a user study
involving ten professional developers with experience in
mobile app development from ten different companies. The
developers had different levels of experience in mobile app

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

14

TABLE 4
User experience questions asked to participants in the bug reporting
study. When asking questions about FUSION, we replaced <sys> with
System A. For EBUG, we replaced <sys> with System B.

Question

QID
R-U1 I think that I would like to use <sys> frequently.
R-U2 I found <sys> very cumbersome to use.
R-U3 I found the various functions in <sys> were well integrated.
R-U4 I thought <sys> was easy to use.
R-U5 I found <sys> unnecessarily complex.
R-U6 I thought that <sys> was very useful for reporting a bug.
R-P1 What functionality did you ﬁnd useful when reporting bugs with <sys>?
R-P2 What information (if any) were you not able to report with <sys>?
R-P3 What elements do you like the most from <sys>
R-P4 What elements do you like the least from <sys>

TABLE 5
User experience questions asked in the bug reproduction study. QID =
an identiﬁer for the question, Question = user experience question.

QID

Question

D-U1 I think that I would like to use this type of bug report frequently.
D-U2 I found this type of bug report unnecessarily complex.
D-U3 I thought this bug report was easy to read/understand.
D-U4 I found this type of bug report very cumbersome to read.
D-U5 I thought that this type of report was very useful for reproducing the bug.

D-P1 What feature(s) did you ﬁnd useful when reproducing the bugs with

using this type of bug report?

D-P2 What additional information (if any) would you like to see in this type of

bug report?

D-P3 What elements do you like the most from this type of bug report?
D-P4 What elements do you like the least from this type of bug report?

programming and bug reporting management. Speciﬁcally,
the developers had an average programming experience
of 9.4 years (min 2, max=12), had an average of self-
determined experience level in mobile app programming
of 5.4 (min=2, max=9) based on 1-10 assessment scale,
and had an average of self-determined experience level
in bug reporting and management of 6.8 (min=1, max=9)
based on 1-10 assessment scale. Although the professional
developers involved in the study have experience in app
development, these developers were not the developers of
the apps considered in the study. We opted not to involve
the developers of the benchmark apps as we wanted to
avoid including developers that could have been already
aware of the reported bugs, which could have impacted the
reproduction part of the study. In the study, we asked the
developers to reproduce the bug reports gathered in the bug
reporting study. In this study, we also asked the developers
to reproduce the original bug reports (i.e., the bug reports
listed and linked in Table 3). We assigned ten bug reports
to each developer. Four bug reports were generated using
FUSION, four were created using EBUG, and two were from
the set of original bug reports. We assigned bug reports to
developers making sure that they would not try to repro-
duce the same report. Similar to the bug reporting study, it
should be noted that we ensured that the order in which
each participant was exposed to each type of report was set
such that one-third of the participants used EBUG ﬁrst and
one-third used FUSION ﬁrst, and one-third used the original
reports ﬁrst to mitigate potential effects of learning bias.

During the study, the developers could access the bug re-
ports using the anonymized variants of the tools. Like in the
bug reporting study, we did not inform the subjects about
which tool was ours and which one was the baseline. For the
original bug reports, developers had access to a screenshot
of the original bug reports. We took this measure so that
the developers could not access any follow-up discussion
in the bug tracking system of the original bug report. In
the study instructions, we asked developers to reproduce
the bug reports using a set of Nexus 7 emulators. These
emulators contained the apps associated with the reports
that the developers needed to reproduce. We also informed
the developers that they needed to record the time it took
them to reproduce each of the bug reports. We asked the
developers to collect evidence (in the form of a screenshot
or ﬁles stored in the emulator) that they reproduced a bug
report (without including the time it took to perform this
operation in the time they logged to reproduce a bug report).

We used the evidence to verify that the subjects succefully
reproduced the bug reports. Finally, we informed the devel-
opers that they should stop trying to reproduce a report if
the report took more than 10 minutes to reproduce. To select
the 10 minutes value, we relied on our experience from
related work [4] and used the same value as in that work.
We used the same value to provide a consistent evaluation
of the techniques, and in related work, we also observed
that the value had mitigated participant fatigue. Although
setting the limit could provide a partial view of the total
number of bug reports that could be reproduced, developers
generally need to decide how much time they would like to
invest in reproducing a report, and our results could also be
interpreted as that time being set to 10 minutes.

Like in the bug reporting study, we asked participants
to complete a survey at the end of the study. In the survey,
developers entered demographics information, recorded the
time it took them to reproduce each bug report, and an-
swered nine user experience (UX) questions about FUSION,
EBUG, and the original bug reports. Among the nine ques-
tions, ﬁve were usability questions, and four were user
preference questions. Table 5 reports the usability (D-UN
questions) and user-preference questions (D-PN questions).
We formulated the questions following the same principles
adopted in the bug reporting study.

5.2 Results

5.2.1 RQ1: Effectiveness
To answer RQ1, we identiﬁed how many failures developers
could reproduce using the reports generated in the bug re-
porting study. Table 6 provides this information. Speciﬁcally,
column Repro under the EBUG header reports which failures
developers could reproduce using the reports generated
with EBUG. Column Repo under the FUSION header reports
the results for FUSION. Table 6 also reports the identiﬁer
of the developer that tried to reproduce a certain report
(columns Dev ID).

Overall, developers using EBUG’s reports could success-
fully reproduce the report’s failure in 37 out of 40 cases
(92.5% success rate). Developers using FUSION’s reports
could do so in 32 out of 40 cases (80% success rate). After
inspecting the bug reports, we explain this result with the
fact that bug report participants created more complete bug
reports while using EBUG. This characteristic can be seen
in Table 6 by comparing the S2Rs columns in the EBUG
and FUSION sections of the table. Speciﬁcally, participants
reported a total of 364 S2Rs using EBUG and 339 using

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

15

TABLE 6
Bug reporting and bug reproduction studies results. App ID = identiﬁer of the benchmark app, Fail. ID = identiﬁer for the failure in the benchmark
app, Rep ID = participant identiﬁer in the bug reporting study, S2Rs = number of steps to reproduce in the bug report submitted by the participant,
Treport = time taken by the participant to submit the bug report, Dev ID = developer identiﬁer in the bug reproduction study, Trepro = time taken by
the participant to submit the bug report, Repro = (cid:51) if a developer reproduced the bug report ((cid:55) otherwise), Val S2Rs = number of S2Rs validated
by EBUG, GUI Action Sugg. = GUI action suggestions (GAS), GAS P = number of GUI Action suggestions provided to the participants of the bug
reporting study, GAS A = number of GUI Action suggestions accepted by the participants of the bug reporting study, GAS A E = number of GUI
Action suggestions accepted and then edited by the participants of the bug reporting study, GAS A D = number of GUI Action suggestions
accepted and then deleted by the participants of the bug reporting study, GUI Element Sugg. = GUI element suggestions (GES), GES P = number
of GUI Element suggestions provided to the participants of the bug reporting study, GES A = number of GUI Element suggestions accepted by the
participants of the bug reporting study, GES A E = number of GUI Element suggestions accepted and then edited by the participants of the bug
reporting study, GES A D = number of GUI Element suggestions accepted and then deleted by the participants of the bug reporting study, PS A =
number of particle suggestions accepted by the participants of the bug reporting study.

FUSION

Reporting

Reproducing

EBUG

Reporting

Reproducing

S2Rs Treport

App
ID

Fail.
ID

A01 F01

A02 F02

A03 F03

A04 F04

A05

A06a

F05

F06

F07

F08

F09

A06b

F10

F11

F12

A07

F13

F14

A08 F15

A09 F16

A10 F17

F18

A11

F19

F20

Rep
ID
4
R03
4
R08
4
R04
6
R08
8
R05
R10 10
7
R05
4
R06
5
R03
3
R08
7
R09
5
R02
R03 13
R07 12
R05 13
R10 19
9
R03
9
R09
8
R02
R06 10
6
R04
R07 15
R08 13
R05 11
R02 16
6
R06
R04 11
R09 16
6
R10
7
R01
3
R07
2
R01
7
R06
7
R01
4
R01
9
R07
8
R02
R10 11
R04 10
R09 11

S2Rs

Trepro

o Rep
Dev
r
p
e
ID
ID
R
1m56s (cid:51) R02
5
9m10s D10
4m00s (cid:51) R10
4
9m14s D03
4m30s (cid:51) R02
6
7m18s D09
2m37s (cid:51) R09
6
6m20s D04
4m10s (cid:51) R02 10
7m48s D09
9m55s (cid:51) R09 16
5m40s D01
6m34s D07 10m00s (cid:55) R07
8
3m33s (cid:51) R01 10
11m57s D02
1m40s (cid:51) R07
5
7m36s D07
4m36s D01 10m00s (cid:55) R01
5
4m18s (cid:51) R06
5
10m38s D02
2m50s D06 10m00s (cid:55) R04
5
3m30s (cid:51) R02 12
16m51s D09
3m10s (cid:51) R09
21m29s D04
9
19m01s D07 10m00s (cid:55) R04 15
6m19s (cid:51) R08 13
17m20s D02
10m33s D06 10m00s (cid:55) R10
9
3m30s (cid:51) R01
7
14m12s D03
8m10s (cid:51) R05
9
12m29s D08
3m15s (cid:51) R09
21m15s D01
7
1m08s (cid:51) R08 10
11m52s D10
2m33s (cid:51) R03 11
12m36s D05
10m03s D01 10m00s (cid:55) R07 22
8m50s (cid:51) R01 21
4m50s D09
1m58s (cid:51) R03 17
7m55s D10
3m01s (cid:51) R08 10
8m32s D05
3m26s (cid:51) R05 12
5m37s D08
7m00s (cid:51) R10 10
10m08s D03
1m14s (cid:51) R04
5
4m03s D05
1m01s (cid:51) R06
6
5m00s D08
3m27s (cid:51) R03
2
2m40s D05
16s (cid:51) R06
2
1m49s D06
9m11s D04 10m00s (cid:55) R07
7
2m11s (cid:51) R05
6m19s D06
7
5m51s (cid:51) R10 10
2m57s D07
7m41s D03 10m00s (cid:55) R04
7
2m00s (cid:51) R05
13m45s D08
7
7m22s (cid:51) R06
8
5m55s D04
1m40s (cid:51) R08 12
4m27s D10
3m42s (cid:51) R03 12
7m02s D02

Val
S2Rs
3
3
4
4
10
16
4
7
5
4
5
5
11
9
11
12
8
6
9
7
9
7
20
12
16
9
10
10
5
6
2
2
7
7
9
6
5
6
12
12

GUI Action Sugg.
P
5
3
1
7
9
14
0
6
1
2
2
0
15
31
14
5
12
7
2
12
3
9
5
2
15
0
1
8
3
7
3
1
0
3
0
0
0
0
0
1

A A E A D P
1
1
1
3
0
0
5
0
0
15
1
5
7
0
0
6
3
0
12
0
0
4
1
0
5
0
0
3
0
0
5
0
0
3
0
0
6
5
1
5
6
11
20
2
4
13
0
0
6
4
2
4
2
1
13
0
0
12
1
3
9
0
0
2
4
2
16
0
0
3
0
0
2
7
0
7
0
0
12
0
0
5
2
1
3
1
0
3
4
0
1
1
0
2
0
0
7
0
0
10
0
0
9
0
0
2
0
0
11
0
0
10
0
0
9
0
0
0
7
1
31 278 185
364 315 209 46

GUI Element Sugg.
A A E A D
0
1
0
1
1
4
0
3
0
7
0
5
1
6
0
2
0
4
0
2
0
5
0
3
0
4
0
1
0
6
0
10
0
4
0
1
0
8
0
4
0
8
0
1
0
14
0
3
0
2
0
7
0
11
0
4
0
3
0
1
0
1
0
2
0
5
0
7
0
8
0
0
0
5
0
6
0
9
0
7
2

1
1
0
0
3
5
0
0
0
0
0
0
2
3
0
2
0
0
0
0
0
0
0
0
6
0
0
4
0
1
0
0
0
0
0
0
0
0
0
0
28

0
0
0
11
0
0
3
2
1
0
0
0
0
1
10
0
0
3
3
5
1
0
0
0
0
0
1
0
0
1
0
0
0
1
1
2
0
0
0
0
46

PSA

Trepro

Treport

o
Dev
r
p
e
ID
R
2m36s (cid:51)
4m58s D02
1m13s (cid:51)
2m30s D07
1m45s (cid:51)
5m17s D01
59s (cid:51)
9m42s D06
1m04s (cid:51)
4m55s D05
1m47s (cid:51)
6m42s D07
5m40s (cid:51)
7m29s D09
2m30s (cid:51)
6m27s D01
45s (cid:51)
3m42s D10
1m13s (cid:51)
4m07s D05
8m02s D08 10m00s (cid:55)
1m20s (cid:51)
3m15s D03
2m26s (cid:51)
10m30s D02
1m07s (cid:51)
20m11s D07
7m36s (cid:51)
18m40s D04
8m11s D09 10m00s (cid:55)
3m18s (cid:51)
3m35s D08
3m00s (cid:51)
4m07s D01
3m35s (cid:51)
8m14s D05
2m55s (cid:51)
10m04s D10
5m10s (cid:51)
5m03s D06
3m30s (cid:51)
9m49s D03
3m00s (cid:51)
7m32s D06
4m00s (cid:51)
6m23s D03
2m18s (cid:51)
6m22s D02
1m22s (cid:51)
5m01s D07
2m50s (cid:51)
5m13s D04
3m15s (cid:51)
3m33s D09
1m22s (cid:51)
2m48s D02
25s (cid:51)
4m58s D10
1m00s (cid:51)
2m03s D04
29s (cid:51)
2m47s D10
35s (cid:51)
5m38s D08
5m00s (cid:51)
3m04s D03
4m10s (cid:51)
2m41s D09
5m28s D01 10m00s (cid:55)
2m34s (cid:51)
5m45s D05
48s (cid:51)
7m40s D06
45s (cid:51)
3m31s D08
2m58s (cid:51)
5m50s D04
120m20s 37

0
0
0
0
2
0
1
0
1
0
1
1
0
0
4
0
0
0
0
0
0
0
7
0
0
0
1
0
1
0
0
1
0
0
0
0
0
1
0
0
21 251m47s

-

-

339 365m13s

-

201m13s 32

-

FUSION. Additionally, the result can also be related to the
fact that 315 out of the 364 S2Rs reported using EBUG, were
also validated by the technique. As another explanation of
the result, the clarity of the the S2Rs provided by EBUG
also appeared as a comment in the survey answers of seven
out the 10 developers. Finally, developers using EBUG’s bug
reports could reproduce all of the 20 failures we selected for
the empirical evaluation.

We manually analyzed the three reports submitted using
EBUG that could not be reproduced by developers and

identiﬁed that all three reports had missing S2Rs. The ﬁrst
report (F11 submitted by R04) had two missing S2Rs, the
second report (F06 submitted by R06) had six missing S2Rs,
and the third report (F08 submitted by R08) had seven
missing S2Rs. Because of the missing S2Rs, two of the
reports (F06 submitted by R06 and F08 submitted by R08)
had some S2Rs that could not be validated by EBUG. In
the third report (F11 submitted by R04), all S2Rs could be
validated by EBUG as the missing S2Rs corresponded to

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

16

GUI actions in the screens as already validated S2Rs, and
therefore EBUG did not identify issues with the sequence of
S2Rs. Although the developers did not provide the reasons
for which they could not reproduce these reports in their
exit surveys, we believe that these missing S2Rs might have
affected the bug reproduction outcome. We believe that
automated bug reproduction techniques could be combined
with EBUG to provide feedback on whether the reports
could be reproduced upon submission, and this feedback
might incentivize users to reﬁne their reports.

In the bug reproduction study, we also asked developers
to reproduce the failures using the original bug reports. De-
velopers were able to do so in 15 out of 20 cases (75% success
rate). These results show that EBUG is more effective than
FUSION and the original bug tracking systems in providing
reproducible bug reports to developers.

In summary, we believe that the results show that EBUG
can be effective in providing reproducible bug reports and
that EBUG is more effective than FUSION.

5.2.2 RQ2: Bug Reporting Efﬁciency

To answer RQ2, we compare the amount of time the par-
ticipants took to submit a bug report using FUSION and
EBUG. For each bug reporting task, Table 6 reports the time
(columns Treport ) participants took to report a speciﬁc failure.
In total, when using EBUG, the participants submitted
the 40 bug reports in 251m47s (about 6m18s per report).
When using FUSION instead, the participants took 365m13s
to report the same failures (about 9m8s per report). These
results show a decrease of 31.06% in the bug reporting
time when using EBUG. If we only considered successfully
reproduced bug reports, the average bug reporting time for
EBUG is 6m14s and for FUSION is 9m13s. These numbers
indicate that the decrease in bug reporting time not only
applies to all bug reports but, in particular, also applies to
successfully reproduced ones.

The bug reporting time of EBUG is characterized by a
number of suggestions provided by EBUG and accepted
by the participants of the study. For both GUI action and
GUI element suggestions (GUI Action Sugg. and GUI Ele-
ment Sugg. headers), Table 6 reports provided (P), accepted
(A), accepted and then edited (A E), and accepted and
then deleted (A D) suggestions. In total, EBUG provided
209 GUI Action suggestions, and participants accepted 46
suggestions (without any modiﬁcation). The participants
also accepted 28 additional suggestions and then edited the
suggestions. In the modiﬁcations, the participants reﬁned
the content of the suggestion by providing additional infor-
mation in the S2R. The participants also accepted 31 sug-
gestions that they then removed from the bug report. One
participant (R09) rewrote part of two bug reports multiple
times before submitting the results, and this contributed to
12 suggestions being deleted. Although deleted suggestion
could represent suggestions that were not ultimetly helpful
for participants, deleted suggestions might be leveraged by
automated bug reproduction techniques [11], [23] to guide
the exploration performed by the techniques to reproduce
bug reports. The number of GUI element suggestions is
278. The participants accepted 185 suggestions, accepted
and then edited two suggestions, and accepted and then

deleted 46 suggestions. The GUI element suggestions re-
ported in Table 6 only include suggestions provided by
the GUI element prediction model. As a side note, the par-
ticipants also accepted 42 suggestions created for partially
written GUI elements. (These suggestions are based on text
matching and not the GUI element prediction models). The
participants also accepted 21 particle suggestions (PSA). The
small number of accepted particle suggestions is due to
the signiﬁcant number of accepted GUI action and element
suggestions. The usefulness of the suggestions provided by
EBUG appeared as a comment in the survey answers of nine
out the ten participants and the comments highlighted that
the suggestions made reporting faster.

Analyzing failures individually, we identiﬁed two cases
(F02 and F16) in which the average time to report the failure
(the study had two submitted bug reports per failure and
tool) was lower in FUSION as compared to EBUG. The ﬁrst
failure (F02) is an output failure and the second one (F16)
is a crash. In the ﬁrst case, the average reporting time with
EBUG was higher because one of the two users (R09) had
a signiﬁcantly higher reporting time. Although we do not
know the exact reason behind this higher reporting time, we
observed that the user rewrote four S2Rs three times because
the subsequent S2R could not be validated by EBUG. This
result shows that improving the accuracy of EBUG’s GUI
model, which is used to validate S2Rs, can further improve
EBUG’s efﬁciency. The second failure (F16) is the failure
caused by the shortest number of GUI actions (two). In
this case, the average reporting time with EBUG (2m25s) is
slightly slower than FUSION (2m15s). We believe that EBUG
does not perform better than FUSION in this case because
users cannot take full advantage of EBUG’s suggestions.

Give the results of our bug reporting study, we can
conclude that EBUG allows users to submit bug reports
more efﬁciently than FUSION.

5.2.3 RQ3: Bug Reproduction Efﬁciency

To answer RQ3, we compare the time developers took to
reproduce FUSION’s and EBUG’s bug reports. Table 6 reports
the time (columns Trepro) developers took to reproduce the
failure described in the bug reports.

Overall, developers took 120m20s to reproduce EBUG’s
reports and 201m13s to reproduce FUSION’s reports. These
numbers identify a 40.2% decrease in the time to reproduce
bug reports when EBUG’s reports. (Note that this number is
affected by the 10m time limit set for the bug reproduction
task.) As in RQ1, we explain this improvement due to
the fact that EBUG provides more complete bug reports
with respect to FUSION. If we only consider bug reports
successfully reproduced by developers, developers could
reproduce EBUG’s bug reports in 2m26s and FUSION’s bug
reports in 3m46s on average. This result highlights that
developers can reproduce EBUG’s bug reports faster even
without considering the cases where the bug reproduction
task is not successful. Finally, developers could also re-
produce EBUG’s bug reports faster than the original bug
reports, which took 3m5s on average in the successful cases.
Considering failures for which developers could repro-
duce both participant-submitted bug reports (the study had
two submitted bug reports per failure and tool), developers
using bug reports submitted with EBUG were slower in only

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

17

Fig. 13. Percentage of bug reports reproduced by developers over time.

one case (F11). To report this failure, users had to provide a
value from a dropdown list that contained a large number of
values (225). Because EBUG also allows users to report this
information just by using text instead of speciﬁcally select-
ing a value from a suggestion, reporters using EBUG pro-
vided this information using a textual description without
going through the technique’s suggestions. The description
provided by both users was not complete, and we believe
that this aspect might have caused the higher reproduction
time with EBUG, as developers had to identify the right
value among a large number of values. This result highlights
a tradeoff between ﬂexibility in the bug reporting process
and efﬁciency in bug reproduction time. We believe that
this tradeoff is worth investigating in future work through
additional user studies.

Fig. 13 reports the percentage of bug reports reproduced
within n minutes (where n ranges from 0 to 10). The
graph conﬁrms that developers could reproduce more bug
reports and faster using EBUG’s reports as compared to
FUSION’s reports and the original reports. The only case
where developers were not the fastest using EBUG’s reports
is within the ﬁrst minute, where 20% of the original reports
were reproduced as opposed to 17.5% of EBUG’s reports.
We believe that this last result can be explained by the
fact that some of the original reports were slightly quicker
to process as they contained less information, but in the
following minutes, when relevant S2Rs were necessary to
reproduce the failures, EBUG outperformed both FUSION
and the original reports.

Because reports for the same failure were submitted
by different participants and reproduced by different de-
velopers, it is not possible for us to compare results in-
dividually and identify exactly why it took longer for a
certain failure to be reproduced using reports from EBUG.
However, analyzing the content of submitted reports, we
believe that missing S2Rs and the developers’ experience
could be factors that affected the failure reproduction time.

In summary, we can conclude that EBUG allows devel-
opers to reproduce bug reports more efﬁciently than FUSION
and the issue tracking systems of the original bug reports.

Fig. 14. Usability answers in the survey of the bug reporting study.

5.2.4 RQ4: Bug Reporting User Experience

To answer RQ4, we collected and analyzed the participants’
answers to the exit survey questions of the bug reporting
study. We report the answer to the usability questions
(R-UN questions in Table 4) in Figure 14 using centered
stacked bar charts. For questions R-U1, R-U3, R-U4, and R-
U6 agreement is better, while for questions R-U2 and R-U5
disagreement is better.

Overall, the participants provided very positive answers
to the usability questions related to EBUG. Nine of the ten
participants either agreed (6) or strongly agreed (3) with
the statement “I think I would like to use EBUG frequently”
(R-U1). Participants also provided positive answers (5 agree
and 3 strongly agree) in relation to the statement “I thought
that EBUG was very useful for reporting a bug” (R-U6). Among
all answers provided by all of the participants, only one
was negative. Speciﬁcally, one participant agreed with the
statement “I found EBUG very cumbersome to use” (R-U2).
The same participant had a neutral answer in relation to
the statement “I thought EBUG was easy to use” (R-U4). We
believe that this answer was related to the fact that the
participant experienced networking issues during the study,
and our system relies on a network connection to operate.

In relation to user experience with EBUG, we also want
to report a few excerpts from the free-text answers we
received from the participants when they answered the
user preference questions (R-PN questions in Table 4). Some
answers indicate that the suggestions provided by EBUG
make the bug reporting process more efﬁcient: “[...] Auto-
complete really speeds up the process [...]” (participant R02),
“[...] The context-aware list of components helps ﬁnding the one
I’m thinking of [...] Templates of step description reduce a lot of
typing” (R03), and “[...] I like the action and GUI components’
suggestions. That functionality is very useful for saving time [...]”
(R6). Finally, some answers indicate that EBUG is useful
and a natural way to submit bug reports: “I liked that I
was basically typing a paragraph of instructions that happened

012345678910Minutes020406080100Percentage (%)EBugFusionOriginalIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

18

to automatically link with images.” (R01), “It feels natural while
reporting bugs with [EBUG ] [...]” (R05), and “I felt that this
tool was very useful and would prove very easy to use, even if you
had no coding experience [...]” (R07).

The free-text answers also provided useful feedback for
improving EBUG and bug reporting through future work.
A comment (from participant R03) mentioned that “If the
app contains same-looking components on multiple screens, the
components list shows me multiple options, when there should
be only one valid option if the tool considers the current state
of the app”. This situation can appear when EBUG is not
able to validate previously typed S2R. In this case, EBUG
provides a suggestion that contains all available GUI actions
in the app (ordered by their distance in the GUI model with
respect to the last validated step). This feedback highlights
the opportunity to improve EBUG in two ways. First, future
research could deﬁne techniques to create an even more
accurate GUI model. Second, future research could focus on
user studies in which users receive a limited number of op-
tions in the suggestions based on a pre-deﬁned GUI model
distance value (and not include all available GUI actions
as the options). A second comment (from R05) highlighted
that “[...] external controls such as power button, or emulator
controls for changing orientation were not accessible.. Power
button support is currently not provided by EBUG, but it
could be added by representing the operation in the GUI
model. For both types of actions, a user might not receive
such suggestions also because the prediction model did
not contain such actions. Having a more complete predic-
tion model could be achieved by collecting additional user
traces, but there is always the possibility of not having some
actions in the model. Considering the results obtained while
assessing EBUG’s bug reporting efﬁciency, we believe that
the technique has good results even if some actions are not
present in the prediction model. Another comment (from
R06) reported that “The selected target screen is unnecessary.
At least in my case, I never used it at all.”. The selected target
screen is the one in the lower-right part of Fig. 9. Future user
studies related to EBUG’s usage could focus on assessing the
usefulness of each of the technique’s interface components.
In summary, these preliminary results demonstrate that
the participants had a positive user experience when sub-
mitting reports with EBUG, and this experience was better
than the one with FUSION.

5.2.5 RQ5: Bug Reproduction User Experience

We answer RQ5 by analyzing the developers’ answers to
the exit survey of the bug reproduction study. We report
the answer to the usability questions (D-UN questions in
Table 5) in Figure 15 using centered stacked bar charts. For
questions D-U1, D-U3, and D-U5 agreement is better, while
for questions D-U2 and D-U4 disagreement is better.

Also for RQ5, the subjects provided very positive an-
swers to the usability questions related to EBUG. All de-
velopers either agreed (four) or strongly agreed (six) with
the statement “I think that I would like to use this type of bug
report frequently” (D-U1). All developers also agreed (three)
or strongly agreed (seven) with the statement “I thought this
bug report was easy to read/understand”’ (D-U3). Additionally,
developers provided positive answers (two agree and seven
strongly agree) in relation to the statement “I thought that

Fig. 15. Usability answers in the survey of the bug reproduction study.
OBTS is the original bug tracking system.
this type of report was very useful for reproducing the bug”
(D-U5). If we consider the participants’ answers to all the
questions, we found that all the answers were either neutral
(two answers) or positive (48 answers). Developers did not
provide as positive answers for FUSION’s usability as the
ones provided for EBUG. In fact, some participants dis-
agreed with statement D-U1 (four disagree and one strongly
disagree) and D-U5 (two disagree). Figure 15 also shows that
the answers about the original bug tracking systems are not
as positives as the ones provided for EBUG.

The developers’ answers to the user preference questions
(D-PN questions in Table 5) suggest that EBUG’s reports
are actionable and easy to understand: “Simple step details,
ordered and simple” (developer D03), “list of operation is very
easy to understand” (D04), “I found both the summary of the
bug on the left side, with expected vs observed behavior to be
useful to provide context. [...] I think the list of steps with
screenshot on the right is what makes this type of bug report
system particularly effective ” (D06), “I liked that it was clear
and with the right amount of information to reproduce the steps.
The list of actions/screenshots is the most useful.” (D07), and
“The screenshots were very useful in ﬁnding the bug [...] (D10).
The free-text answers from the bug reproduction study
also provided useful insights for improving EBUG and bug
reproduction in future work. A comment (from developer
D01) suggested that an “[i]mage of the ﬁnal (buggy) state of the
app” would be a good addition to EBUG. Currently, EBUG
does not focus on improving the bug reporting process of
the observed behavior. However, we believe that automated,
real-time, and iterative bug reproduction techniques could
help users report a screenshot of the observed behavior and
believe that this is an interesting direction for future work.
A second comment (from D02) mentioned that “I think the
description ﬁeld doesn’t have relevant information if it’s a copy of
the steps, but could be useful to hold additional information”.
The description ﬁeld mentioned by the developer is the
description typed by the reporter. The developer found
this description redundant as compared to the list of S2Rs

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

19

TABLE 7
Prediction models comparison. App ID = app identiﬁer, Traces = number of traces , Predictions = number of predictions, Order = order of AKOM’s
and EBUG’s models, SN = number of suggestions provided by the models, wes = wasted effort score, PL = sequence preﬁx length for CPT+.

App ID Traces Predictions

AKOM

GUI Action Models
CPT+

EBUG

AKOM

GUI Element Models
CPT+

EBUG

A01
A02
A03
A04
A05
A06a
A06b
A07
A08
A09
A10
A11

12
15
14
15
12
15
14
13
12
9
11
10

285
283
458
284
211
464
398
285
250
126
265
271

Order SN wes PL SN wes Order SN wes Order SN wes PL SN wes Order SN wes
1 0.727
1 0.675
1 0.703
1 1.513
1 1.269
1 1.275
1 0.961
1 0.566
1 0.429
1 0.355
1 1.284
1 0.726

1 1.159 4
1
0.85 3
1 0.877 2
1 2.595 2
1 1.542 3
1 1.549 2
1 1.355 2
1 0.717 5
1 0.799 2
1 0.636 2
1 2.193 2
1 1.221 2

1 0.727 3
1 0.626 3
0.69 3
1
1 1.427 2
1 1.542 3
1 1.468 3
1
0.98 3
1 0.647 3
1 0.563 3
1 0.313 3
1 1.325 2
1 0.936 2

1 1.021
1 0.952
1 0.885
1 3.239
1 1.638
1 1.275
1 1.151
1 0.686
1 0.603
1 0.575
1 2.118
1 1.117

1 1.568
1
1.55
1 1.018
1 3.239
1 1.931
1 1.698
1 1.745
1 0.759
1
0.88
1 0.909
1 2.581
1 1.398

1 2.314
1 2.291
1 3.362
1 8.793
1 4.024
1 8.098
1 4.169
1 2.132
1 0.908
1
1 4.096
4.42
1

4
7
4
2
1
2
1
2
3
2
2
3

5
9
8
5
4
9
9
5
8
5
4
5

3
5
4
3
3
7
6
3
4
2
2
5

4
6
6
4
2
4
4
4
8
4
3
6

1.1

validated by EBUG. The approach displays the information
to the developer in case S2Rs were not suitably validated,
and, in this way, the developer could always refer to the
description originally provided by the user. We believe
that additional user studies could further improve EBUG’s
interface, and these studies could possibly display the user
description on an on-demand basis. Finally, another com-
ment (from D04) highlighted that images associated with
S2Rs are not always present, and this increases the difﬁculty
in understanding the reported bug: “[n]ot always image[s]
are present, this increase[s] [the] difﬁcult[y] to understand”. This
situation can appear when EBUG is not able to validate an
S2R. Devising techniques to improve EBUG’s GUI model
would mitigate this type of situation.

The answers provided in the study offer some prelimi-
nary results on the user experience with EBUG and reveal
that participants had a positive user experience when re-
producing bug reports with EBUG. Additionally, the results
provide initial evidence that this experience was better than
the one with FUSION’s reports or the original bug reports.

5.2.6 RQ6: Prediction Models’ Effectiveness

To answer RQ6, we leveraged the user traces collected
for EBUG’s evaluation and compared EBUG’s prediction
models with prediction models generated using AKOM [40]
and CPT+ [41]. Speciﬁcally, we compared the prediction
models of EBUG used in the bug reporting study with the
best performing models generated using AKOM and CPT+.
To compute AKOM and CPT+ prediction models, we follow
a similar methodology as the one described in Section 3.1.2.
For AKOM models, we identiﬁed the best performing model
for each app by varying the model order and doing leave-
one-out (sequence) cross-validation. We varied the sequence
preﬁx length to identify the best performed CPT+ models.
We use the wasted effort score (wes) metric to ﬁnd the best
performing models for a speciﬁc technique and compare
models across approaches. The wasted effort score is an ex-
trinsic evaluation metric and we used this metric as extrinsic
evaluations are the most suitable way to evaluate models in
sequence prediction tasks [16].

Table 7 reports the performance of the models. For
each app, the table reports the best performing models for
predicting GUI actions and GUI elements using AKOM,
CPT+, and EBUG. Table 7 is divided into three sections. The

ﬁrst section reports the identiﬁer (column App ID) of the
benchmark apps considered in the evaluation, the number
of traces (Traces) to evaluate the models, and the number
of predictions used to evaluate the models (Predictions). The
GUI action and GUI element models were evaluated using
the same number of predictions because when we evaluated
GUI element models, we only assessed predictions on the
tokens representing GUI elements (see Section 3.2 for more
details). The second section reports the best performing
models for making GUI Action suggestions and the third
section for making GUI element suggestions. Columns Or-
der provide the order of the models, columns SN provide
the number of suggestions provided by the models for each
prediction task, columns wes provide the wasted effort score
for the models, and columns PL provide the sequence preﬁx
length for the CPT+ models. Table 7 reports in bold the
wasted effort score of the best performing model across all
models for a certain app.

Overall, EBUG provides the best performing models in
16 out of the 24 cases considered (one of the cases is a
tie with a model based on AKOM). AKOM offers the best
performing model in nine out of the 24 (including the
tie). CPT+ never provides the best performing model. If
we considered the wasted effort score of the GUI action
prediction models across all apps, EBUG’s wes is 1.127 and
AKOM’s wes is 1.198. For GUI element prediction models,
EBUG’s wes is 0.846 and AKOM’s wes is 0.902.

Although the overall difference between AKOM models
and EBUG’s n-gram models is not big, the n-gram models
performed better than the AKOM models both in terms of
wasted effort and correctly made predictions. Across all
the GUI action prediction tasks, EBUG’s n-gram models
produced 1,683 correct predictions and a wasted effort of
1,897. AKOM models led to 1,629 correct predictions and
a wasted effort of 1,951. For GUI element prediction tasks,
EBUG’s n-gram models produced 1,939 correct predictions
and a wasted effort of 1,641. AKOM models led to 1,882
correct predictions and a wasted effort of 1,698. Even if these
differences are not big, when considering that the same
failure might be reported by multiple users, the differences
can become more signiﬁcant. Considering these results,
EBUG’s n-gram models can help reporters by providing
more useful predictions while lowering the wasted effort.
However, additional studies are also needed to conﬁrm and

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

20

extend these results and we suggest those as possible future
work.

Based on these results, we can conclude that EBUG pro-
vides the best performing prediction models in the majority
of the cases, and when we consider all models of all apps,
EBUG has the best wasted effort score as compared with
baseline approaches.

5.3 Threats to Validity

As it is the case for most empirical evaluations, there are
both external and construct threats to validity associated
with our results. In the bug reporting study, the participants
were CS students. These participants might have a different
background compared to other bug reporters. To mitigate
this threat to validity, the study included participants from
three institutions and at a different level of their studies.
Additionally, past work has illustrated that performance
of CS students and professional developers do not differ
dramatically in some software engineering tasks [46]. The
results of the bug reporting study might also be affected by
the user traces collected in the data collection activity. To
mitigate this threat, we invited participants with different
backgrounds (some of which did not have a CS background)
and asked them to use the benchmark apps as they would
do in a normal use of the apps. The results of the bug
reproduction study might also not generalize to other devel-
opers. To mitigate this threat to validity, the study included
developers from ten different companies at different career
stages. In the bug reproduction study, we used 10 minutes
as a time limit for developers to reproduce bug reports.
This value could provide a partial view of the total number
of reproduced bug reports as more bug reports could be
reproduced after the time limit. We used this value to be
consistent with related work [4]. We also decide to use the
value, as, in related work, we also observed that the value
had mitigated participant fatigue.

Another threat to external validity is that our results
might not generalize to other failures or apps. In particular,
we only considered 20 failures in our empirical evaluation.
This limitation is an artifact of the complexity of our eval-
uation. To mitigate this threat, we used randomly selected
real-world failures of varying types and complexity. These
failures belong to real-world apps from different categories
and functions. These apps include apps such as GNUCASH,
MILEAGE, and DOC VIEWER, which have complex function-
ality, hundreds of widgets, and hundreds of thousands of
users. We believe that, given the complexity of the failures
and apps considered in the studies, EBUG should also apply
to other types of failures and apps.

Finally, in terms of construct validity, there might be
errors in the implementation of our approach or our experi-
mental infrastructure. To mitigate this threat, we extensively
inspected the results of the evaluation manually.

6 DISCUSSION

Through our empirical evaluation, we found that EBUG
is more efﬁcient and effective than the baseline. Based on
the evaluation results and the feedback received from the
participants of the bug reporting study, we believe that the

results can be attributed to the S2R suggestions and the
validation operations performed by EBUG. For one of the
failures considered, bug reporting with EBUG was not more
efﬁcient. In this case, one of the bug reporters rewrote four
S2Rs three times. We believe that this situation appeared
because EBUG could not validate a subsequent S2R in the
report. This result reveals that improving the accuracy of the
GUI models generated by EBUG, which are used to validate
the S2Rs, can further improve EBUG’s efﬁciency. Developers
using EBUG’s bug reports were generally more efﬁcient in
reproducing bug reports as compared to developers using
the baseline’s bug reports. Developers using bug reports
submitted with EBUG were slower only in the case of one
speciﬁc failure. To report this failure, the reporters had to
provide a value from a dropdown list that contained a
large number of values, and the reporters decided to use
free text instead of inspecting a list of options provided
by EBUG. The description provided by both users was not
complete, and we believe that this aspect might have caused
the higher reproduction time with EBUG. This result shows
a tradeoff between ﬂexibility in the bug reporting process
and efﬁciency in bug reproduction time. We believe that this
tradeoff is an interesting aspect related to bug reporting and
should be the target of future user studies.

During our evaluation, we focused on examining how
useful EBUG is for reporting bugs, however,
it is also
important to comment on the general scalability and ap-
plicability of our technique. Given that a majority of the
“heavy” processing required by EBUG is performed a-priori,
that is before the reporting process commences, and the
“online” portions of EBUG generally scale well, we expect
EBUG to scale reasonably well for a variety of complex
applications. The one potential limitation with scaling may
be with EBUG’s GUI-component matching procedure. That
is, if there is a very large number of components present
within an app, searching for the corresponding component
may introduce a short delay in the reporting interface.
However, while we cannot conﬁrm EBUG’s performance
outside the context of our study, we speculate that, based on
the authors’ experience of the general size of most Android
applications, EBUG would generally scale to a majority of
apps available today.

Another important point of investigation to understand
the reasons for EBUG general performance in our empirical
evaluation is the extent to which static and dynamic GUI models
contribute to the accuracy of the suggestion engine. To identify
the contributions of the static and dynamic GUI models
(which together form the GUI model used by EBUG), we
analyzed the S2Rs that were entered by the participants
of the bug reporting study and that were also validated
by EBUG. As reported in Table 6, the technique validated
315 S2Rs across all apps. One of those S2Rs had the corre-
sponding GUI action that was present only in the static GUI
model, 140 S2Rs had corresponding GUI actions appearing
in both the static and the dynamic GUI model, and 174 S2Rs
had corresponding GUI actions present only in the dynamic
GUI model. This result highlights that the dynamic GUI is
of primary importance for EBUG. If we look at all the GUI
actions (1,255) encoded in the GUI models of all the apps
considered, 155 GUI actions are present only in the static
GUI models, 274 appear in both the static and the dynamic

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

21

GUI models, and 826 are present only in the dynamic GUI
models. Although a large number of GUI actions are part
of the dynamic GUI models, the number of GUI actions
provided by the static GUI model is not negligible, making
them potentially useful for validating S2Rs.

The predictions accepted during bug reporting are an-
other important aspect of EBUG. In our study we found that
participants accepted a large number of both GUI action and
GUI element suggestions. The wasted effort score during
bug reporting was higher for GUI action predictions and
lower for GUI element predictions as compared to when
models were built (i.e., when evaluating the models on
user traces). Although the wasted effort scores associated
with GUI action prediction models are higher during bug
reporting, the number of accepted suggestions, the study
participant’s feedback, and the overall lower bug reporting
time of EBUG suggests that the predictions can be helpful
during bug reporting. We believe that performing additional
studies on wasted effort during bug reporting is an interest-
ing venue for future work.

Another important aspect of EBUG’s practical applicabil-
ity is concerned with the collection of the app usage models.
One of the key facets of EBUG is the utilization of “real-
world” usage traces of a given application to help prioritize
potential predicted S2Rs. There are several ways in which
such traces could be collected. For example, as developers
release beta versions of an application to crowd-testers,
traces could be collected from crowd-testers [47]. Addition-
ally, traces could be collected from developer written test
cases, or even usages collected by developers themselves
during the testing process of each version of their app.
EBUG uses traces to gather an understanding of commonly
performed GUI action sequences and provides suggestions
when those GUI action sequences are part of a sequence that
reveals a bug. For this reason, EBUG does not need to collect
traces that expose bugs to provide suggestions.

There are many potential future directions of work for
improving EBUG. For instance, currently our execution
traces are tied to a speciﬁc application, meaning that data
collection must be done on a per-app basis. However, recent
work from the Ubicomp community has illustrated the
beneﬁts of modeling user behaviors across applications [48].
This suggests that future work could look to learn a gener-
alized abstract usage model across many applications that
is supplemented with a smaller amount of app-speciﬁc data
from a diverse set of users. This may help to improve the
need of collecting usage data for EBUG. We also believe
future work could examine different types of reporting
interfaces, such as on-device reporting or chatbot-based
systems, given the success of prior programming tools
that use these interfaces [49]. Finally, given the additional
information that is captured via EBUG, we believe future
work could examine potential beneﬁts to downstream bug
reporting tasks such as bug triaging or localization.

7 LIMITATIONS

EBUG computes a GUI model of the relevant app in its
models generation phase. GUI model generation is based
on static and dynamic analysis. In the evaluation, we used
a timeout of 60 minutes for the dynamic analysis. Although

the dynamic analysis might take longer if the analysis is run
exhaustively (or a higher timeout is used), our evaluation
indicates that even with a timeout of 60 minutes EBUG can
achieve good results. Furthermore, the analysis needs to be
run only once for every app release. For this reason, we be-
lieve that the cost associated with this part of EBUG should
have a limited impact on EBUG. The analyses performed
by EBUG might not fully capture all the transitions and
screens in the relevant app. EBUG’s evaluation results give
us conﬁdence that generated models are effective, but this
aspect of the technique could be improved in future work
by leveraging ongoing research on GUI model abstraction
and representation (e.g., [50]).

Our current prototype tool, in its static analysis part of
the GUI model generation phase, does not currently create
different screens for the different fragments in an activity
of a certain app. This characteristic can lead the prototype
tool to create an overapproximated GUI model of the app.
The GUI model is overapproximated as it might contain
transitions that do not actually exist in the app due to over-
approximations (e.g., context-insensitive analysis) made in
the static analysis used by EBUG. Although this aspect
might lead to false positives in the S2Rs validation process,
we did not encounter signiﬁcant challenges related to this
aspect in the studies performed to evaluate EBUG.

The main assumption for EBUG to work effectively is
that reported S2Rs describe GUI actions in the app. The
description of a S2R needs to be provided through text
but does not need to follow an imposed structure. EBUG is
able to process S2R descriptions having different structures
by leveraging the dependency trees associated with the
descriptions [16], [22]. To take full advantage of EBUG,
the text of a S2R should describe the components of the
corresponding GUI action, that is, the type of action, the
GUI element affected by the action, and the properties of
the action (when applicable). EBUG is also able to operate
even if one or more S2R descriptions do not provide the
desired information. Although the technique will not be
able to map those GUI descriptions into GUI actions, it
can map subsequent descriptions leveraging the GUI model
and feedback from the user (i.e., the user can correct the
provided mapping ). This part of EBUG also allows the tech-
nique to support incomplete or missing S2R descriptions.

Our approach does not currently offer support for ac-
tions based on multi-touch gestures (e.g., pinch in). If an app
uses that type of action, EBUG would not be able to validate
or provide suggestions for that type of action but would still
be able to support other actions in the app. We are currently
investigating ways to support that type of action, and, based
on our experience, that type is not the predominant one in
a large number of apps.

Our approach binds app interactions to GUI elements.
Certain Android apps, however, rely on bitmapped (rather
than GUI) elements. Hence, the approach cannot currently
handle such apps. Luckily, the vast majority of these apps
are games, whereas other types of apps tend to rely on
standard GUI elements. We also believe that those apps
could be supported by identifying interactable elements us-
ing approaches based on computer vision techniques (e.g.,
edge detection).

Finally, our predictive models require training data.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

22

Although it might not be possible to collect such data
from users in some cases, we believe that developers can
mitigate this problem by performing this activity in-house
as described also in Sec. 6.

8 RELATED WORK

execution information [67], and adopted a learning to rank
approach [68]. We view EBUG as complimentary to the tech-
niques in this line of research, as its main goal is to improve
the quality of bug reports before they are submitted, which
can ultimately beneﬁt all of the downstream approaches by
providing reports of higher quality.

8.1 Automation for Bug Reporting Systems

8.2 Bug Reporting Studies

FUSION is a bug reporting system developed by Moran et
al. [4] that has the goal of facilitating the reporting process
by providing pre-populated structured ﬁelds that reporters
can use to compose the reproduction steps of their reports.
While this approach led to the construction of bug reports
that were more reproducible than a baseline issue tracking
system, it came at the cost of longer reporting times. Our
work in developing the EBUG approach differs in two
important aspects. First, we are the ﬁrst to explore how
predictive modeling can enhance the bug reporting process
by proactively providing suggestions to users about yet-
to-be-completed information. Second, our interface blends
a predictive free-form text ﬁeld in conjunction with visual
information (e.g., screenshots), whereas FUSION used more
structured menus. We compared EBUG against FUSION in
our comprehensive evaluation and found that reporters
could create bug reports faster with EBUG and these reports
were more reproducible in far less time, illustrating the
advantages of EBUG.

Other approaches from related work focus on the analy-
sis of bug reports after they are created. Speciﬁcally these
techniques analyze completed bug reports and (i) detect
missing information [51], (ii) assess the quality of the re-
production steps [42], and (iii) automatically reproduce bug
reports [11], [23], [52]. While such research is related to
ours, we view our proposed approach as complimentary, as
it aims to increase bug report quality at the time of reporting
which in turn, can help to improve the performance of
the techniques in this area of research by providing higher
quality bug information to analyze. Addtionally, automated
reproduction techniques of bug reports [11], [23] and app
reviews [52] could be integrated with bug reporting systems
like EBUG so that the bug reporting process would have an
iterative feedback loop in which the results of automated
bug reproduction are provided to the users as they are
reporting bugs to help them reﬁne the reports.

Finally, a wealth of work has been conducted on au-
tomating downstream SE tasks that are tied to bug re-
porting, such as (i) triaging, (ii) fault localization, and (iii)
duplicate detectio. We now provide a summary of the work
in this area of research but refer the reader to surveys on
this line of research for a more thorough discussion [18],
[53], [54]. Related work on triaging improved assignment of
bug reports to developers [55], [56], [57], [58], reduced bug
report tossing [59], and prioritized bug reports [60]. Work
on bug reports and fault localization used information re-
trieval to identify ﬁles that likely need ﬁxing [61], leveraged
multi-step recommendation models based on information
retrieval [62], used version control history information to
identify relevant buggy ﬁles [63], utilized stack traces to
identify ﬁles leading to crashing faults [64], [65]. Related
work on duplicate bug report detection used information
retrieval [66], leveraged natural language processing and

A large body of work focused on understanding the var-
ious aspects of the bug reporting processes, and we draw
inspiration from several of the ﬁndings of this research area.
Bettenburg et al. [3] performed a study with developers of
open source software systems and found that (i) steps to
reproduce, (ii) stack traces, and (iii) test cases were the most
important pieces of information to be included within a bug
report. However, these pieces of information were also the
most difﬁcult for reporters to provide. This helps to illus-
trate the importance of EBUG’s real-time understanding and
auto-completion of S2Rs as these features can help reporters
in quickly writing effective S2Rs. Bettenburg et al. [69] also
examined developer’s perceptions of duplicate bug reports,
and found that many developers did not necessarily view
them as harmful, but rather, as providing important infor-
mation. While EBUG does not focus on duplicate reports,
we hope that the accurate reporting mechanism provided by
our approach could aid future automated analysis tools that
aim to ﬁnd related or duplicate bug reports for developers.
The same authors also proposed a tool that is capable of
extracting structural information from bug reports [70]. Sim-
ilarly, Song & Chaparro studied and proposed techniques
for automatically identifying different types of lexical infor-
mation from bug reports [51], [71]. EBUG is complementary
to this line of research as the approach introduces a new
form of real-time analysis of S2Rs that can identify various
components in the S2Rs.

8.3 Field Failure Reproduction

The research area of ﬁeld failure reproduction shares similar
goals with our work, and with bug reporting systems in
general, in that they attempt to capture and relay fault
information to developers. Related work on ﬁeld failure
reproduction proposed techniques to reproduce failures
using genetic programming [72], [73], crowdsourcing [74],
symbolic execution [75], [76], [77], [78], static analysis com-
bined with symbolic execution [79], model checking [80],
dynamic exploration [81], sequential pattern minin on user
traces [82], and test case mutaiton [83].

However, we view EBUG, and other user-facing bug
reporting systems as largely complimentary to these ﬁeld
failure reproduction approaches. This is mainly due to the
fact that in-ﬁeld techniques require a known oracle, such as
a crash, to identify that a failure has occurred. Conversely,
user-facing bug reporting systems can be used for failures
discovered by end-users or other developers. These bugs
have been shown to account for more than half of the
reported bugs in open source systems [2].

8.4 User Support Systems and Interactive Debugging

Previous work in the SE research community has fo-
cused on creating tools that emulate a chatbot-like inter-
face for debugging, question answering, and user support.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

23

For instance, the work by Ko et al. focused on helping
programmers ask questions about program behavior [49].
Parmit et al. introduced the LemonAid system that provided
contextual assistance on websites through crowdsourcing
techniques. There are also a number of commercial tools
such as BugClipper [84] or BugSee [85] provide features for
screen recording, and bug report annotation. In contrast to
these tools, EBUG offers proactive suggestions to help speed
up and improve the quality of information included in the
bug reporting process. While EBUG does not make use of a
chatbot interface, we believe this is an interesting direction
of future work to explore for the domain of bug reporting.
8.5 Automated GUI Exploration for Mobile Apps

The modeling techniques employed by EBUG share certain
similarities with automated GUI exploration techniques for
mobile apps. Automated input generation approaches for
mobile apps perform random-based input generation [86],
[87], [88], [89], do systematic input generation [90], [91], [92],
[93], [94], employ model-based input generation [10], [36],
[92], [95], [96], [97], [98], use a search-based approach [99],
[100], and perform symbolic input generation [101], [102]. Re-
lated work, also investigated how to integrate user feedback
to improve mobile app testing [103], [104].

The two most highly related techniques to those pro-
posed in EBUG are the MonkeyLab approach proposed
by Linares-Vasquez et al. [36], and the Polariz approach
proposed by Mao et al. [105]. Both of these approaches
attempt to incorporate app usages gathered from crowd
workers in order to generate a more diverse set of actions to
guide automated input generation. EBUG’s input generation
approach and prediction modeling function in a similar
manner, but with a markedly different end goal: aid users
in predicting reproduction steps during the bug reporting
process. As such, EBUG’s approach uses an n-gram model
to predict likely reproduction steps and target GUI elements
for reporters, rather than using this information to guide an
input generation technique.

9 CONCLUSION
When users experience a software failure, they have the
option of submitting a bug report to provide information
about the failure. Developers rely on submitted bug reports
to identify and remedy the faults in their software. Unfortu-
nately, the quality of manually constructed bug reports can
vary widely due to the effort required to include essential
information, such as a detailed description of the S2Rs. This
issue affects and complicates the developers’ task of repro-
ducing the failures experienced by the users. To help users in
writing bug reports that are easier to reproduce, we propose
the EBUG bug reporting approach. EBUG understands S2Rs
written in natural language and, as the user reports them,
uses a novel predictive model to suggest additional S2Rs for
helping the user complete the bug report. We implemented
EBUG to support bug reporting in the context of Android
apps and empirically evaluated it in two user studies.
The studies involved ten participants who submitted ten
bug reports each, and ten developers who reproduced the
submitted bug reports. Our results show that reporters were
able to write bug reports 31% faster with EBUG as compared
to a state-of-the-art bug reporting system used as a baseline.

The results also show that developers could reproduce more
failures using EBUG’s reports than using those generated
using the baseline.

We foresee a number of venues for future work. First,
we will investigate ways to help users in deﬁning observed
and expected behavior. Second, we will study how to auto-
matically extract more domain knowledge from the relevant
app so that EBUG can understand and provide suggestions
for macro-S2Rs (i.e., S2Rs that should be interpreted as
sequences of multiple actions on the GUI). Third, we will
evaluate the application of the approach in the context of
web and GUI-based desktop software. Fourth, we also think
that EBUG could be extended by evaluating it in additional
studies. Speciﬁcally, EBUG could be evaluated in studies
where developers use EBUG to report bugs and studies
where EBUG is used by the developers of the relevant apps
upon receiving newly identiﬁed bugs. Fifth, future studies
could also analyze the tradeoffs between closed and open
vocabularies in EBUG’s prediction models. Finally, and more
on the engineering side, we will extend EBUG to model
fragments as independent screens in the relevant app and
add support for multi-touch gestures, which will allow us
to handle reports involving these kinds of interactions.

ACKNOWLEDGMENT
W&M and GMU co-authors have been supported in part
by the NSF CCF-1955853 and CCF-2007246 grants. Any
opinions, ﬁndings, and conclusions expressed herein are the
authors’ and do not necessarily reﬂect those of the sponsors.

REFERENCES

[1]

[2]

G. Tassey, “The economic impacts of inadequate infrastructure
for software testing,” National Institute of Standards and Tech-
nology, Tech. Rep., 2002.
L. Tan, C. Liu, Z. Li, X. Wang, Y. Zhou, and C. Zhai, “Bug charac-
teristics in open source software,” Empirical Software Engineering,
vol. 19, no. 6, pp. 1665–1705, 2014.

[4]

[3] N. Bettenburg, S. Just, A. Schröter, C. Weiss, R. Premraj, and
T. Zimmermann, “What makes a good bug report?” in Proceedings
of the 16th ACM SIGSOFT International Symposium on Foundations
of Software Engineering. New York, NY, USA: ACM, 2008.
K. Moran, M. Linares-Vásquez, C. Bernal-Cárdenas, and
D. Poshyvanyk, “Auto-completing bug reports for android ap-
plications,” in Proceedings of the 2015 10th Joint Meeting on Foun-
dations of Software Engineering. New York, NY, USA: ACM, 2015.
[On-
smart
(2021)
line]. Available: https://www.blog.google/products/gmail/
subject-write-emails-faster-smart-compose-gmail

compose.

Google

[5]

[7]

(2021) EBUG’s

and D. Poshyvanyk.

[6] M. Fazzini, K. Moran, C. Bernal-Cardenas, T. Wendland,
A. Orso,
online
appendix. [Online]. Available: https://www-users.cs.umn.edu/
~mfazzini/ebug.html
(2021) Deposit/withdrawal change existing entry.
Available:
issues/247
(2021) Gnucash github. [Online]. Available: https://github.com/
codinguser/gnucash-android
(2021) Gnucash. [Online]. Available: https://play.google.com/
store/apps/details?id=org.gnucash.android

[Online].
https://github.com/codinguser/gnucash-android/

[9]

[8]

[10] W. Choi, G. Necula, and K. Sen, “Guided gui testing of android
apps with minimal restart and approximate learning,” in Proceed-
ings of the 2013 ACM SIGPLAN International Conference on Object
Oriented Programming Systems Languages & Applications. New
York, NY, USA: ACM, 2013, pp. 623–640.

[11] M. Fazzini, M. Prammer, M. d’Amorim, and A. Orso, “Automat-
ically translating bug reports into test cases for mobile apps,” in
Proceedings of the 27th ACM SIGSOFT International Symposium on
Software Testing and Analysis. New York, NY, USA: ACM, 2018,
pp. 141–152.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

24

[12]

S. Yang, H. Zhang, H. Wu, Y. Wang, D. Yan, and A. Rountev,
“Static window transition graphs for android (t),” in Proceedings
of the 2015 30th IEEE/ACM International Conference on Automated
Software Engineering. Piscataway, NJ, USA: IEEE Press, 2015.

[13] T. Wanwarang, N. P. Borges, L. Bettscheider, and A. Zeller, “Test-
ing apps with real-world inputs,” in Proceedings of the IEEE/ACM
1st International Conference on Automation of Software Test, ser. AST
’20. New York, NY, USA: Association for Computing Machinery,
2020, p. 1–10.
(2021) Tesseract ocr. [Online]. Available: https://github.com/
tesseract-ocr/tesseract

[14]

[15] F. Harary, Graph Theory. USA: Avalon Publishing, 1969.
[16] D. Jurafsky and J. H. Martin, Speech and Language Processing (2nd

[17]

Edition). USA: Prentice-Hall, Inc., 2009.
S. F. Chen and J. Goodman, “An empirical study of smoothing
techniques for language modeling,” Computer Science Group,
Harvard University, Tech. Rep. TR-10-98, 1998.

[18] W. E. Wong, R. Gao, Y. Li, R. Abreu, and F. Wotawa, “A survey
on software fault localization,” IEEE Transactions on Software
Engineering, vol. 42, no. 8, pp. 707–740, 2016.

[19] G. Miner, J. Elder, T. Hill, R. Nisbet, D. Delen, and A. Fast,
Practical Text Mining and Statistical Analysis for Non-structured Text
Data Applications. Orlando, FL, USA: Academic Press, 2012.
J.
Jul.) Making
case.
making-a-case-for-letter-case-19d09f653c98

letter
https://medium.com/@jsaito/

[Online]. Available:

(2016,

Saito.

case

[20]

for

a

[21] G. Angeli, M. J. J. Premkumar, and C. D. Manning, “Leveraging
linguistic structure for open domain information extraction,”
in Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing.
Stroudsburg, PA, USA: The
Association for Computer Linguistics, 2015, pp. 344–354.
[22] M. de Marneffe, B. MacCartney, and C. D. Manning, “Gener-
ating typed dependency parses from phrase structure parses,”
in Proceedings of the Fifth International Conference on Language
Resources and Evaluation. Bern, Switzerland: European Language
Resources Association (ELRA), 2006, pp. 449–454.

[23] Y. Zhao, T. Yu, T. Su, Y. Liu, W. Zheng, J. Zhang, and W. G. J. Hal-
fond, “Recdroid: Automatically reproducing android application
crashes from bug reports,” in Proceedings of the 41st International
Conference on Software Engineering.
Piscataway, NJ, USA: IEEE
Press, 2019, p. 128–139.

[24] V. I. Levenshtein, “Binary codes capable of correcting deletions,

insertions, and reversals,” in Soviet physics doklady, 1966.

[25] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching
word vectors with subword information,” Transactions of the
Association for Computational Linguistics, pp. 135–146, 2017.
[26] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estima-

tion of word representations in vector space,” 2013.

[27] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean,
“Distributed representations of words and phrases and their
compositionality,” in Proceedings of the 26th International Confer-
ence on Neural Information Processing Systems.
Red Hook, NY,
USA: Curran Associates Inc., 2013, p. 3111–3119.

[28] M.-T. Luong, R. Socher, and C. D. Manning, “Better word rep-
resentations with recursive neural networks for morphology,” in
Proceedings of the Seventeenth Conference on Computational Natural
Language Learning, 2013, pp. 104–113.
S. Qiu, Q. Cui, J. Bian, B. Gao, and T.-Y. Liu, “Co-learning of word
representations and morpheme representations,” in Proceedings
of the 25th International Conference on Computational Linguistics:
Technical Papers, 2014, pp. 141–150.

[29]

[31]

[30] R. Soricut and F. J. Och, “Unsupervised morphology induction
using word embeddings,” in Proceedings of the 2015 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, 2015, pp. 1627–1637.
J. Ostrander, Android UI Fundamentals: Develop and Design. Berke-
ley, CA, USA: Peachpit Press, 2012.
(2021) UI automator.
android.com/training/testing/ui-automator
(2021) Neo4j. [Online]. Available: https://neo4j.com
(2021) Getevent.
com/devices/input/getevent

[Online]. Available: https://source.android.

[Online]. Available: https://developer.

[33]
[34]

[32]

[35] B.-J. Hsu and J. Glass, “Iterative language model estimation:
efﬁcient data structure & algorithms,” in Ninth Annual Conference
of the International Speech Communication Association, 2008.

[37]

[36] M. Linares-Vásquez, M. White, C. Bernal-Cárdenas, K. Moran,
and D. Poshyvanyk, “Mining android app usages for generating
actionable gui-based execution scenarios,” in Proceedings of the
12th Working Conference on Mining Software Repositories. Piscat-
away, NJ, USA: IEEE Press, 2015, p. 111–122.
(2021) Google natural
//cloud.google.com/natural-language
(2021) Quill. [Online]. Available: https://quilljs.com
(2021) English word vectors.
//fasttext.cc/docs/en/english-vectors.html
J. Pitkow and P. Pirolli, “Mining longest repeating subsequences
to predict world wide web surﬁng,” in Proceedings of the 2nd
Conference on USENIX Symposium on Internet Technologies and
Systems. USA: USENIX Association, 1999, p. 13.

[Online]. Available: https:

[Online]. Available: https:

language.

[38]
[39]

[40]

[41] T. Gueniche, P. Fournier-Viger, R. Raman, and V. S. Tseng, “Cpt+:
Decreasing the time/space complexity of the compact prediction
tree,” in Proceedings of the 19th Paciﬁc-Asia Conference on Knowledge
Discovery and Data Mining. Berlin, Germany: Springer, 2015.

[42] O. Chaparro, C. Bernal-Cárdenas, J. Lu, K. Moran, A. Marcus,
M. Di Penta, D. Poshyvanyk, and V. Ng, “Assessing the quality
of the steps to reproduce in bug reports,” in Proceedings of the
2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineer-
ing. New York, NY, USA: ACM, 2019, p. 86–96.
(2021) Gnucash. [Online]. Available: https://play.google.com
J. Brooke, “SUS: A quick and dirty usability scale,” in Usability
evaluation in industry. London: Taylor and Francis, 1996.
[45] P. Morville. (2021) User experience design. [Online]. Available:

[43]
[44]

[46]

http://semanticstudios.com/user_experience_design
I. Salman, A. T. Misirli, and N. Juristo, “Are students represen-
tatives of professionals in software engineering experiments?” in
Proceedings of the 37th International Conference on Software Engineer-
ing - Volume 1, ser. ICSE ’15.

IEEE Press, 2015, p. 666–676.

[47] K. Mao, M. Harman, and Y. Jia, “Crowd intelligence enhances
automated mobile testing,” in Proceedings of the 32nd IEEE/ACM
International Conference on Automated Software Engineering, ser.
ASE 2017.

IEEE Press, 2017, p. 16–26.

[48] X. Chen, Y. Wang,

J. He, S. Pan, Y. Li, and P. Zhang,
“Cap: Context-aware app usage prediction with heterogeneous
graph embedding,” Proc. ACM Interact. Mob. Wearable Ubiquitous
Technol., vol.
[Online]. Available:
1, mar
https://doi.org/10.1145/3314391

3, no.

2019.

[49] A. J. Ko and B. A. Myers, “Debugging reinvented: Asking and
answering why and why not questions about program behav-
ior,” in Proceedings of the 30th International Conference on Software
Engineering (ICSE’08), 2008, p. 301–310.

[52]

[50] D. Lai and J. Rubin, “Goal-driven exploration for android ap-
plications,” in 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE), 2019, pp. 115–127.
[51] O. Chaparro, J. Lu, F. Zampetti, L. Moreno, M. Di Penta, A. Mar-
cus, G. Bavota, and V. Ng, “Detecting missing information in
bug descriptions,” in Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering. New York, NY, USA: ACM,
2017, p. 396–407.
S. Li, J. Guo, M. Fan, J.-G. Lou, Q. Zheng, and T. Liu, “Automated
bug reproduction from user reviews for android applications,”
in 2020 IEEE/ACM 42nd International Conference on Software Engi-
neering: Software Engineering in Practice (ICSE-SEIP). USA: IEEE,
2020, pp. 51–60.
J. Zhang, X. Wang, D. Hao, B. Xie, L. Zhang, and H. Mei, “A
survey on bug-report analysis,” Science China Information Sciences,
vol. 58, no. 2, pp. 1–24, 2015.
J. Uddin, R. Ghazali, M. M. Deris, R. Naseem, and H. Shah, “A
survey on bug prioritization,” Artiﬁcial Intelligence Review, vol. 47,
no. 2, pp. 145–180, 2017.

[53]

[54]

[55] R. Shokripour, J. Anvik, Z. M. Kasirun, and S. Zamani, “Why so
complicated? simple term ﬁltering and weighting for location-
based bug report assignment recommendation,” in Proceedings
of the 10th Working Conference on Mining Software Repositories.
Piscataway, NJ, USA: IEEE Press, 2013, pp. 2–11.

[56] H. Naguib, N. Narayan, B. Brügge, and D. Helal, “Bug report
assignee recommendation using activity proﬁles,” in Proceedings
of the 10th Working Conference on Mining Software Repositories.
Piscataway, NJ, USA: IEEE Press, 2013, pp. 22–30.
J. woo Park, M.-W. Lee, J. Kim, S. won Hwang, and S. Kim, “Cos-
triage: A cost-aware triage algorithm for bug reporting systems,”
2011.

[57]

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

25

[58] M. Linares-Vasquez, K. Hossen, H. Dang, H. Kagdi, M. Gethers,
and D. Poshyvanyk, “Triaging incoming change requests: Bug or
commit history, or code authorship?” in Proceedings of the 28th
IEEE International Conference on Software Maintenance.
Piscat-
away, NJ, USA: IEEE Press, 2012, pp. 451–460.

[59] G. Jeong, S. Kim, and T. Zimmermann, “Improving bug triage
with bug tossing graphs,” in Proceedings of the the 7th Joint Meet-
ing of the European Software Engineering Conference and the ACM
SIGSOFT Symposium on The Foundations of Software Engineering.
New York, NY, USA: ACM, 2009, pp. 111–120.

[61]

[60] H. Shen, J. Fang, and J. Zhao, “Eﬁndbugs: Effective error ranking
for ﬁndbugs,” in Proceedings of the 2011 IEEE Fourth International
Conference on Software Testing, Veriﬁcation and Validation. Piscat-
away, NJ, USA: IEEE Press, 2011, pp. 299–308.
J. Zhou, H. Zhang, and D. Lo, “Where should the bugs be ﬁxed? -
more accurate information retrieval-based bug localization based
on bug reports,” in Proceedings of the 34th International Conference
on Software Engineering. Piscataway, NJ, USA: IEEE Press, 2012.
[62] D. Kim, Y. Tao, S. Kim, and A. Zeller, “Where should we ﬁx this
bug? a two-phase recommendation model,” IEEE Transactions on
Software Engineering, vol. 39, no. 11, pp. 1597–1610, 2013.
S. Wang and D. Lo, “Version history, similar report, and structure:
Putting them together for improved bug localization,” in Proceed-
ings of the 22Nd International Conference on Program Comprehension.
New York, NY, USA: ACM, 2014, pp. 53–63.

[63]

[64] R. Wu, H. Zhang, S.-C. Cheung, and S. Kim, “Crashlocator:
Locating crashing faults based on crash stacks,” in Proceedings of
the 2014 International Symposium on Software Testing and Analysis.
New York, NY, USA: ACM, 2014, pp. 204–214.

[65] L. Moreno, J. J. Treadway, A. Marcus, and W. Shen, “On the use
of stack traces to improve text retrieval-based bug localization,”
in 2014 IEEE International Conference on Software Maintenance and
Evolution. USA: IEEE, 2014, pp. 151–160.

[66] A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, D. Lo, and C. Sun,
“Duplicate bug report detection with a combination of infor-
mation retrieval and topic modeling,” in Proceedings of the 27th
IEEE/ACM International Conference on Automated Software Engi-
neering. New York, NY, USA: ACM, pp. 70–79.

[67] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun, “An approach
to detecting duplicate bug reports using natural language and
execution information,” in Proceedings of the 30th International
Conference on Software Engineering. New York, NY, USA: ACM,
2008, pp. 461–470.
J. Zhou and H. Zhang, “Learning to rank duplicate bug reports,”
in Proceedings of the 21st ACM International Conference on Infor-
mation and Knowledge Management. New York, NY, USA: ACM,
2012, pp. 852–861.

[68]

[69] N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim, “Du-
plicate bug reports considered harmful... really?” in Proceedings
of the 2008 IEEE International Conference on Software Maintenance.
Piscataway, NJ, USA: IEEE Press, 2008, pp. 337–345.

[70] ——, “Extracting structural information from bug reports,” in
Proceedings of the 2008 International Working Conference on Mining
Software Repositories. New York, NY, USA: ACM, 2008, pp. 27–30.
[71] Y. Song and O. Chaparro, BEE: A Tool for Structuring and Analyzing

Bug Reports. New York, NY, USA: ACM, 2020, p. 1551–1555.

[72] M. Soltani, A. Panichella, and A. van Deursen, “A guided genetic
algorithm for automated crash reproduction,” in Proceedings of the
39th International Conference on Software Engineering. Piscataway,
NJ, USA: IEEE Press, 2017, pp. 209–220.

[73] F. M. Kifetew, W. Jin, R. Tiella, A. Orso, and P. Tonella, “Repro-
ducing ﬁeld failures for programs with complex grammar-based
input,” in Proceedings of the 2014 IEEE Seventh International Con-
ference on Software Testing, Veriﬁcation and Validation. Piscataway,
NJ, USA: IEEE Press, 2014, pp. 163–172.

[74] M. Gómez, R. Rouvoy, B. Adams, and L. Seinturier, “Reproducing
context-sensitive crashes of mobile apps using crowdsourced
monitoring,” in Proceedings of the International Conference on Mobile
Software Engineering and Systems. New York, NY, USA: ACM,
2016, pp. 88–99.

[75] N. Chen and S. Kim, “Star: Stack trace based automatic crash re-
production via symbolic execution,” IEEE Transactions on Software
Engineering, vol. 41, no. 2, pp. 198–220, 2015.

[76] W. Jin and A. Orso, “Automated support for reproducing and de-
bugging ﬁeld failures,” ACM Trans. Softw. Eng. Methodol., vol. 24,
no. 4, 2015.

[77] ——, “Bugredux: Reproducing ﬁeld failures for in-house debug-
ging,” in Proceedings of the 34th International Conference on Software
Engineering. Piscataway, NJ, USA: IEEE Press, 2012, pp. 474–484.
[78] Y. Cao, H. Zhang, and S. Ding, “Symcrash: Selective recording
for reproducing crashes,” in Proceedings of the 29th ACM/IEEE
International Conference on Automated Software Engineering. New
York, NY, USA: ACM, 2014, pp. 791–802.

[79] C. Zamﬁr and G. Candea, “Execution synthesis: A technique for
automated software debugging,” in Proceedings of the 5th European
Conference on Computer Systems. New York, NY, USA: ACM, 2010,
pp. 321–334.

[80] M. Nayrolles, A. Hamou-Lhadj, S. Tahar, and A. Larsson, “A bug
reproduction approach based on directed model checking and
crash traces,” Journal of Software: Evolution and Process, vol. 29,
no. 3, 2017.

[81] M. White, M. Linares-Vásquez, P. Johnson, C. Bernal-Cárdenas,
and D. Poshyvanyk, “Generating reproducible and replayable
bug reports from android application crashes,” in Proceedings of
the 2015 IEEE 23rd International Conference on Program Comprehen-
sion. Piscataway, NJ, USA: IEEE Press, 2015, p. 48–59.

[83]

[82] T. Roehm, S. Nosovic, and B. Bruegge, “Automated extraction of
failure reproduction steps from user interaction traces,” in Pro-
ceedings of the 2015 IEEE 22nd International Conference on Software
Analysis, Evolution, and Reengineering. Piscataway, NJ, USA: IEEE
Press, 2015, pp. 121–130.
J. Xuan, X. Xie, and M. Monperrus, “Crash reproduction via test
case mutation: Let existing test cases help,” in Proceedings of the
2015 10th Joint Meeting on Foundations of Software Engineering.
New York, NY, USA: ACM, 2015, pp. 910–913.
“Bugclipper http://bugclipper.com,” 2021.
“https://www.bugsee.com,” 2021.
(2021) Android ui/application exerciser monkey. [Online]. Avail-
able: http://developer.android.com/tools/help/monkey.html

[84]
[85]
[86]

[87] A. Machiry, R. Tahiliani, and M. Naik, “Dynodroid: An input
generation system for android apps,” in Proceedings of the 2013
9th Joint Meeting on Foundations of Software Engineering. New
York, NY, USA: ACM, 2013, pp. 224–234.

[88] R. Sasnauskas and J. Regehr, “Intent fuzzer: Crafting intents of
death,” in Proceedings of the 2014 Joint International Workshop on
Dynamic Analysis and Software and System Performance Testing,
Debugging, and Analytics. New York, NY, USA: ACM, 2014.
[89] L. Ravindranath, S. Nath, J. Padhye, and H. Balakrishnan, “Au-
tomatic and scalable fault detection for mobile applications,” in
Proceedings of the 12th Annual International Conference on Mobile
Systems, Applications, and Services. New York, NY, USA: ACM,
2014, pp. 190–203.

[90] D. Amalﬁtano, A. R. Fasolino, P. Tramontana, S. De Carmine,
and A. M. Memon, “Using gui ripping for automated testing
of android applications,” in Proceedings of the 27th IEEE/ACM
International Conference on Automated Software Engineering. New
York, NY, USA: ACM, 2012, pp. 258–261.
S. Anand, M. Naik, M. J. Harrold, and H. Yang, “Automated
concolic testing of smartphone apps,” in Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Soft-
ware Engineering. New York, NY, USA: ACM, 2012, pp. 59–69.

[91]

[92] T. Azim and I. Neamtiu, “Targeted and depth-ﬁrst exploration
for systematic testing of android apps,” in Proceedings of the
2013 ACM SIGPLAN International Conference on Object Oriented
Programming Systems Languages & Applications. New York, NY,
USA: ACM, 2013, pp. 641–660.

[93] K. Moran, M. Linares-Vásquez, C. Bernal-Cárdenas, C. Vendome,
and D. Poshyvanyk, “Automatically discovering, reporting and
reproducing android application crashes,” in Proceedings of the
IEEE International Conference on Software Testing, Veriﬁcation and
Validation. Piscataway, NJ, USA: IEEE Press, 2016, pp. 33–44.
(2021) Google ﬁrebase test lab robo test. [Online]. Available:
https://ﬁrebase.google.com/docs/test-lab/robo-ux-test
[95] D. Amalﬁtano, A. R. Fasolino, P. Tramontana, B. D. Ta, and A. M.
Memon, “Mobiguitar: Automated model-based testing of mobile
apps,” IEEE Software, vol. 32, no. 5, pp. 53–59, 2015.

[94]

[96] R. N. Zaeem, M. R. Prasad, and S. Khurshid, “Automated gen-
eration of oracles for testing user-interaction features of mobile
apps,” in Proceedings of the 2014 IEEE International Conference on
Software Testing, Veriﬁcation, and Validation. Washington, DC,
USA: IEEE Computer Society, 2014, pp. 183–192.

[97] W. Yang, M. R. Prasad, and T. Xie, “A grey-box approach
for automated gui-model generation of mobile applications,”

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. #, NO. #, 2021

26

in Proceedings of the 16th International Conference on Fundamental
Approaches to Software Engineering. Berlin, Heidelberg: Springer-
Verlag, 2013, pp. 250–265.

[98] H. Zhang and A. Rountev, “Analysis and testing of notiﬁca-
tions in android wear applications,” in Proceedings of the 2017
IEEE/ACM 39th International Conference on Software Engineering.
Piscataway, NJ, USA: IEEE Press, 2017, pp. 347–357.

[99] R. Mahmood, N. Mirzaei, and S. Malek, “Evodroid: Segmented
evolutionary testing of android apps,” in Proceedings of the 22Nd
ACM SIGSOFT International Symposium on Foundations of Software
Engineering. New York, NY, USA: ACM, 2014, pp. 599–609.
[100] K. Mao, M. Harman, and Y. Jia, “Sapienz: Multi-objective auto-
mated testing for android applications,” in Proceedings of the 25th
International Symposium on Software Testing and Analysis. New
York, NY, USA: ACM, 2016, pp. 94–105.

[101] C. S. Jensen, M. R. Prasad, and A. Møller, “Automated testing
with targeted event sequence generation,” in Proceedings of the
2013 International Symposium on Software Testing and Analysis.
New York, NY, USA: ACM, 2013, p. 67–77.

[102] N. Mirzaei, H. Bagheri, R. Mahmood, and S. Malek, “Sig-droid:
Automated system input generation for android applications,”
in Proceedings of the 2015 IEEE 26th International Symposium on
Software Reliability Engineering. USA: IEEE Computer Society,
2015, p. 461–471.

[103] G. Grano, A. Ciurumelea, S. Panichella, F. Palomba, and H. C.
Gall, “Exploring the integration of user feedback in automated
testing of android applications,” in 2018 IEEE 25Th international
conference on software analysis, evolution and reengineering (SANER).
USA: IEEE, 2018, pp. 72–83.

[104] L. Pelloni, G. Grano, A. Ciurumelea, S. Panichella, F. Palomba,
and H. C. Gall, “Becloma: Augmenting stack traces with user
review information,” in 2018 IEEE 25th International Conference on
Software Analysis, Evolution and Reengineering (SANER). USA:
IEEE, 2018, pp. 522–526.

[105] K. Mao, M. Harman, and Y. Jia, “Crowd intelligence enhances
automated mobile testing,” in Proceedings of the 32nd IEEE/ACM
International Conference on Automated Software Engineering.
Pis-
cataway, NJ, USA: IEEE Press, 2017, p. 16–26.

Mattia Fazzini is an Assistant Professor in the
Department of Computer Science & Engineering
at the University of Minnesota. He completed
his Ph.D. in Computer Science from the Georgia
Institute of Technology in 2019. His research
interests lie primarily in the area of software
engineering, with emphasis on techniques for
improving software quality. The central theme of
his research is the development of approaches
for testing and maintenance of mobile apps.
He has published in several top peer-reviewed
software engineering venues including: ASE, ICSE, ICST, and ISSTA.
He received the Facebook Testing and Veriﬁcation award in 2019 for
his research on mobile app testing. More information is available at
https://www-users.cs.umn.edu/~mfazzini/.

Kevin Moran is currently an Assistant Professor
at George Mason University where he directs
the SAGE research group. He received a Ph.D.
degree from William & Mary in August 2018.
His main research interest involves facilitating
the processes of software engineering, main-
tenance, and evolution with a focus on mobile
platforms. He has published in several top peer-
reviewed software engineering venues including:
ICSE, ESEC/FSE, TSE, USENIX, ICST, ICSME,
and MSR. His research has been recognized
with ACM SIGSOFT distinguished paper awards at ESEC/FSE 2019
and ICSE 2020. He is a member of the ACM and IEEE. More information
is available at http://www.kpmoran.com

Carlos Bernal-Cárdenas is currently Ph.D. can-
didate in Computer Science at the College of
William & Mary as a member of the SEMERU re-
search group advised by Dr Denys Poshyvanyk.
His research interests include software engi-
neering, software evolution and maintenance,
information retrieval, software reuse, mining soft-
ware repositories, mobile applications develop-
ment, and user experience. He has published in
several top peer-reviewed software engineering
venues including: ICSE, ESEC/FSE, ICST, and
MSR. He has also received the ACM SigSoft Distinguished paper award
at ESEC/FSE’15 & ’19 and ICSE’20. More information is available at
http://www.cs.wm.edu/~cebernal/.

Tyler Wendland is a graduate student at the
University of Minnesota. He graduated with a
B.S. in Computer Science from the University
of Minnesota in 2020. His research focuses on
devising automated software engineering tech-
niques to improve bug reporting processes.

Alessandro Orso is a Professor in the School
of Computer Science and an Associate Dean in
the College of Computing at the Georgia Insti-
tute of Technology. He received his M.S. degree
in Electrical Engineering (1995) and his Ph.D.
in Computer Science (1999) from Politecnico di
Milano, Italy. From March 2000, he has been at
Georgia Tech. His area of research is software
engineering, with emphasis on software testing
and program analysis. His interests include the
development of techniques and tools for improv-
ing software reliability, security, and trustworthiness, and the validation
of such techniques on real-world systems. Dr. Orso has received funding
for his research from both government agencies, such as DARPA, ONR,
and NSF, and industry, such as Facebook, Fujitsu Labs, Google, IBM,
and Microsoft. He served on the editorial boards of ACM TOSEM and
on the Advisory Board of Reﬂective Corp, and he is serving on the
editorial boards of IEEE TSE. Ha also served as program chair for
ACM SIGSOFT ISSTA 2010, program co-chair for IEEE ICST 2013,
ACM-SIGSOFT FSE 2014, and ACM-SIGSOFT/IEEE ICSE 2017, and
as a technical consultant to DARPA. Dr. Orso has received multiple
Best Paper and ACM Distinguished Paper Awards and three Impact
and Most Inﬂuential Paper Awards. Dr. Orso is a Distinguished Mem-
ber of the ACM and an IEEE Fellow. More information is available at
http://www.cc.gatech.edu/~orso/.

Denys Poshyvanyk is a Professor of Computer
Science at William and Mary. He received the
MS and MA degrees in Computer Science from
the National University of Kyiv-Mohyla Academy,
Ukraine, and Wayne State University in 2003
and 2006, respectively. He received the PhD
degree in Computer Science from Wayne State
University in 2008. He served as a program
co-chair for ASE’21, MobileSoft’19, ICSME’16,
ICPC’13, WCRE’12 and WCRE’11. He currently
serves on the editorial board of IEEE Transac-
tions on Software Engineering (TSE), Empirical Software Engineering
Journal (EMSE, Springer), Journal of Software: Evolution and Process
(JSEP, Wiley) and Science of Computer Programming. His research in-
terests include software engineering, software maintenance and evolu-
tion, program comprehension, reverse engineering and software repos-
itory mining. His research papers received several Best Paper Awards
at ICPC’06, ICPC’07, ICSM’10, SCAM’10, ICSM’13, CODAPSY’19 and
ACM SIGSOFT Distinguished Paper Awards at ASE’13, ICSE’15, ES-
EC/FSE’15, ICPC’16, ASE’17, ESEC/FSE’19 and ICSE’20. He also
received the Most Inﬂuential Paper Awards at ICSME’16, ICPC’17 and
ICPC’20. He is a recipient of the NSF CAREER award (2013). He
is a member of the IEEE and ACM. More information is available at:
http://www.cs.wm.edu/~denys/

