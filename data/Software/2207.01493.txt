2
2
0
2

n
u
J

0
3

]

Y
C
.
s
c
[

1
v
3
9
4
1
0
.
7
0
2
2
:
v
i
X
r
a

AI Ethics: Software Practitioners’ and
Lawmakers’ Points of View

Arif Ali Khan1, Muhammad Azeem Akbar2, Muhammad
Waseem*3, Mahdi Fahmideh4, Aakash Ahmad5, Peng Liang3,
Mahmood Niazi6 and Pekka Abrahamsson7

1University of Oulu, Finland
2Lappeenranta-Lahti University of Technology, Finland
3Wuhan University, China
4University of Southern Queensland, Australia
5University of Ha’il, Saudi Arabia
6King Fahd University of Petroleum and Minerals, Saudi Arabia
7University of Jyvaskyla, Finland

Abstract

Despite their commonly accepted usefulness, Artiﬁcial Intelligence
(AI) technologies are concerned with ethical unreliability. Various
guidelines, principles, and regulatory frameworks are designed to en-
sure that AI technologies bring ethical well-being. However, the impli-
cations of AI ethics principles and guidelines are still being debated.
To further explore the signiﬁcance of AI ethics principles and relevant
challenges, we conducted an empirical survey of 99 AI practitioners
and lawmakers from twenty countries across ﬁve continents. Study
ﬁndings conﬁrm that transparency, accountability, and privacy are the
most critical AI ethics principles. On the other hand, lack of ethi-
cal knowledge, no legal frameworks, and lacking monitoring bodies are
found the most common AI ethics challenges. The impact analysis
of the challenges across AI ethics principles reveals that conﬂict in
practice is a highly severe challenge. Our ﬁndings stimulate further

1

 
 
 
 
 
 
research, epically empowering existing capability maturity models to
support the quality assessment of ethics-aware AI systems.

1 Introduction

Artiﬁcial Intelligence (AI) becomes necessary across a vast array of industries
including health, manufacturing, agriculture, and banking [1]. AI technolo-
gies have the potential to substantially transform society and oﬀer various
societal beneﬁts, which are expected to happen from high-level productiv-
ity and eﬃciency. In line with this, the ethical guidelines presented by the
European commission expert group on AI highlights that (AI HLEG) [2]:

(cid:15) “AI is not an end in itself, but rather a promising means to increase
human ﬂourishing, thereby enhancing individual and societal well-being and
the common good, as well as bringing progress and innovation.”

However, the promising beneﬁts of AI technologies have been considered
with worries that the complex and opaque systems might bring more social
harm than good [1]. People start thinking beyond the operational capabili-
ties of AI technologies and investigate the ethical aspects of developing strong
and potentially life consequential technologies. US government and private
companies do not consider the virtual implications of decision-making sys-
tems in health, criminal justice, employment, and creditworthiness without
ensuring that these systems are not coded intentionally or unintentionally
with structural biases [1].

Concomitant with advances in AI systems, we witness the ethical failure
of those systems. For example, a high rate of unsuccessful job applications
that was decided by the Amazon recruitment system was later found biased
against women applicants and triggered interest in AI ethics [3]. The need
for developing policies and principles that address the ethical aspects of AI
systems is timely and critical. The ethical harm in AI systems might may
jeopardize human control and these potential threats set a stage for AI ethics
discussion. AI systems are not only concerned with technical eﬀorts, but also
need to consider the social, political, legal, and intellectual aspects. However,
AI’s current state of ethics is broadly unknown to the public, practitioners,
policy, and lawmakers [4].

Extensively, the ethically aligned AI system should meet the following
three components through the entire life cycle [2]: 1) the proposed AI system
should comply with all the applicable laws and regulations, 2) adhere to

2

ethical principles and values, and 3) should be technically and socially robust.
To the best of our knowledge, no empirical study is conducted that cover
and explore the above three core components based on the data collected
from industrial practitioners and lawmakers. For instance, Vakkuri et al. [4]
conducted a survey study to determine industrial perceptions based only on
four AI ethics principles, Lu et al.[5] conducted interviews with researchers
and practitioners to understand the AI ethics principles implications and the
motivation for rooting these principles in the design practices. Similarly,
Leikas et al. [6] mainly focused on AI ethics guidelines.

In this study, insights are provided by encapsulating the views and opin-
ions of practitioners and lawmakers regarding the AI ethics principles and
challenges by collecting survey data from 99 respondents across 20 diﬀerent
countries.

2 Background

Generally, the AI ethics is classiﬁed under the umbrella of applied ethics,
which mainly consider the ethical issues that happen because of developing,
deploying, and using the AI systems. It focuses on linking how an AI system
could raise worries related to human autonomy, freedom in a democratic
society, and quality of life. Ethical reﬂection across AI technologies could
serve in achieving multiple societal purposes [2]. For instance, it can stimulate
focusing on innovations that aim to foster ethical values and bring collective
well-being. Ethically aligned or trustworthy AI technologies can ﬂourish
sustainable well-being in society by bringing prosperity, wealth maximization,
and value creation [2].

It is vital to understand the development, deployment, and use of AI
technologies to ensure that everyone can build a better future and live a
thriving life in the AI-based world. However, the increasing popularity of AI
systems has raised concerns such as reliability and impartiality of decision-
making scenarios [2]. We need to make sure that decision-making support of
AI technologies must have an accountable process to ensure that their actions
are ethically aligned with human values that should not be compromised [2].
In this regard, diﬀerent organizations and technology giants developed
committees to draft the AI ethics guidelines. Google and SAP presented the
guidelines and policies to develop ethically aligned AI systems [7]. Similarly,
the Association of Computing Machinery (ACM), Access Now, and Amnesty

3

International jointly proposed the principles and guidelines to develop an
ethically mature AI system [7]. In Europe, the independent high-level expert
group on artiﬁcial intelligence (AI HLEG) developed the guidelines for pro-
moting trustworthy AI [2]. The Ethically Aligned Design (EAD) guidelines
are presented by IEEE, consisting of a set of principles and recommendations
that focus on the technical and ethical values of AI systems [8]. In addition,
the joint ISO/IEC international standard committee proposed the ISO/IEC
JTC 1/SC 42 standard which covers the entire AI ecosystem, including trust-
worthiness, computational approach, governance, standardization, and social
concerns [9].

However, various researchers claim that the extant AI ethics guidelines
and principles are not eﬀectively adopted in industrial settings. McNamara
et al.
[10] conducted an empirical study to understand the inﬂuence of the
ACM code of ethics in the software engineering decision-making process. Sur-
prisingly, the study ﬁndings reveal that no evidence has been found that the
ACM code of ethics regulate decision-making activities. Vakkuri et al. [11]
conducted multiple interviews to know the status of ethical practices in the
domain of the AI industry. The study ﬁndings uncover the fact that various
guidelines are available; however, their deployment in industrial domains are
far from being mature. The gap between AI ethics research and practice re-
mains an ongoing challenge. To bridge this gap, we conducted an empirical
study to know the signiﬁcance of AI ethics principles, challenges, and their
impact by encapsulating the views of AI practitioners and lawmakers.

3 Setting the Stage

Previously, we conducted a systematic literature review (SLR) to provide
an in-depth overview of AI ethics challenges and principles [12]. This study
aims to empirically validate the SLR ﬁndings based on the insights provided
by the AI practitioners and lawmakers. AI practitioners have higher ethi-
cal responsibilities compared to others. Practitioners often make the design
decisions of complex autonomous systems with less ethical knowledge. The
magnitude of risks in AI systems makes practitioners responsible for under-
standing ethical attributes. To achieve reliable outcomes, it is essential to
know the practitioners understanding of AI ethics principles and challenges.
Law resolves everyday conﬂicts and sustains order in social life. People con-
sider law an information source as it impacts social norms and values [13].

4

The aim of considering this type of population (lawmakers) is to understand
the application of the law to AI ethics. The data collected from legislation
personnel will uncover the question, of whether standing AI ethics principles
are suﬃcient, or is there a need for innovative standards [13]?

The primary goal of this study is to understand the signiﬁcance of the
AI ethics principles, challenges, and the severity of the challenges across the
principles. We used industrial collaboration contacts to search the AI prac-
titioners and send a formal invitation to participate in this study. Moreover,
various law forums across the world were contacted and requested to partic-
ipate in this survey. The targeted populations were approached using social
media networks including LinkedIn, WeChat, ResearchGate, Facebook, and
personal email addresses. The survey instrument consists of four core sec-
tions: 1) demographics 2) AI ethics principles 3) challenges 4) challenges
impact on principles. The survey questionnaire also includes open-ended
questions to know the novel principles and challenges that were not identi-
ﬁed during the SLR [12]. The Likert scale is used to evaluate the signiﬁcance
of each principle and challenge and assess the severity level of the challenging
factors. The survey instrument is structured both in English and Chinese.
The software industry in China is ﬂourishing like never before, where AI is
taking the front seat and is home to some of the leading technology giants in
the world, such as Huawei, Alibaba, Baidu, Tencent, and Xiaomi. However,
it would be challenging to collect the data from the Chinese industry because
of the language barriers. Mandarin is the national and oﬃcial language in
China, unlike India, where English is commonly used for oﬃcial purposes.
Therefore, the Chinese version of the survey instrument is developed to cover
the major portion of the targeted population. Both English and Chinese ver-
sions of the survey instrument are available online for replication [14].

The piloting of the questionnaire is performed by inviting three exter-
nal qualitative software engineering research experts. The expert’s sugges-
tions were mainly related to the overall design, and understandability of the
survey questions. The suggested changes were incorporated, and the sur-
vey instrument was online deployed using Google forms (English version)
and Tencent questionnaire (Chinese version). The data were collected from
September 2021 to April 2022 and we received 107 responses. The manual
review revealed that eight responses were incomplete and we only considered
99 responses for the ﬁnal data analysis.

5

4 Survey Results

Frequency analysis is performed to organize the descriptive data and it is
more suitable for analyzing a group of variables both for numeric and ordinal
data. We noticed that 99 respondents from 20 countries across 5 continents
with 9 roles and 10 diﬀerent backgrounds participated in the survey study
(see Figure 1(a-c)). The organizational size (number of employees) of survey
participants mostly ranges from 50 to 249, which is 28% of the total responses
(see Figure 1(d)). Of all the responses, majority (48%) have 3-5 years of
experience working with AI focused projects as practitioners or lawmakers
(see Figure 1(e)).

Participants were asked to explain their opinions about the perceived
importance of AI systems in their organization. The majority of the partic-
ipants positively agreed. For instance, 77% mentioned that their respective
organizations consider ethical aspects in AI processes or develop policies for
AI projects, 12% answered negatively, and 10% were not sure about it (see
Figure 1(f)). We mapped the respondents’ roles across nine diﬀerent cate-
gories using thematic mapping (see Figure 1(b)). The ﬁnal results show that
the most of the respondents (29%) are classiﬁed across the law practitioner
category. Similarly, the domains of the participants’ organizations are con-
ceptually framed in 10 core categories and the results revealed that most
(19%) of the organizations are working on smart systems (see Figure 1(c)).
The survey responses are classiﬁed as average agree, neutral and average
disagree (see Figure 2(a-b)). We observed that average ≥ 60% of the respon-
dents positively conﬁrmed the AI ethics principles and challenges identiﬁed
in the SLR study [12]. For instance, one survey participant mentioned that:
(cid:213) “The listed AI ethics principles are comprehensive and extensive to

cover various aspects of ethics in AI.”

Moreover, we selected the most frequently reported seven challenging fac-
tors and six principles discussed in our SLR study [12]. The aim is to inves-
tigate the severity impact of the seven challenges (i.e., lack of ethical knowl-
edge, vague principles, highly general principles, conﬂict in practice, interpret
principles diﬀerently, lack of technical understanding, and extra constraints)
across the six AI ethics principles (i.e. transparency, privacy, accountability,
fairness, autonomy, and explainability). The survey participants were asked
to rate the severity impact using the Likert scale: short-term (insigniﬁcant,
minor, moderate) and long-term (major, and catastrophic) (see Figure 2(c)).
The results reveal that most challenges have long-term impacts on the princi-

6

ples (major, and catastrophic). For example, interpret principles diﬀerently
challenge is likely to have a long-term impact (i.e., 50% major, and 27%
catastrophic) on the transparency principle.

Further, the non-parametric statistical analysis is performed to under-
stand the signiﬁcant diﬀerences between both types of populations (AI prac-
titioners and lawmakers). Because of the allowed word limits, the statistical
discussion and dataset are provided online [14].

5 Data Interpretation

The results illustrate that the majority of the survey participants positively
agreed to consider the identiﬁed list of AI ethics principles (see Figure 2(a)).
We noticed that 77.8% of survey respondents thought transparency as the
most signiﬁcant AI ethics principle. This is an interesting observation as
transparency is equally conﬁrmed as one of the core seven essential require-
ments by AI HLEG [2] for realizing the ‘trustworthy AI’. Transparency pro-
vides detailed explanations of logical AI models and decision-making struc-
tures understandable to the system stakeholders. Moreover, it deals with the
public perceptions and understanding of how AI systems work. Broadly, it
is a societal and normative ideal of “openness”.

The second most signiﬁcant principle to the survey participants was ac-
countability (71.7%). It refers to the expectations or requirements that the
organizations or individuals need to ensure throughout the lifecycle of an AI
system. They should be accountable according to their roles and applicable
regulatory frameworks for the system design, development, deployment, and
operation by providing documentation on the decision-making process or con-
ducting regular auditing with proper justiﬁcation. Privacy is the third most
frequently occurred principle, supported by 69.7% of the survey participants.
It refers to preventing harm, a fundamental right speciﬁcally aﬀected by the
decision-making system. Privacy compels data governance throughout the
system lifecycle, covering data quality, integrity, application domain, access
protocols, and capability to process the data in a way that safeguards pri-
vacy. It must be ensured that the data collected and manipulated by the AI
system shall not be used unlawfully or unfairly discriminate against human
beings. For example, one of the respondents mentioned that

(cid:213) “The privacy of hosted data used in AI applications and the risk of

data breaches must be considered.”

7

Figure 1: Demographic details

8

Brazil262a) Geo Distribution of Survey Participantsc) Types of Software/Systemsb) Professional RolesCanadaChinaCubaDenmarkSaudi ArabiaFinlandFranceGermanyIndiaItalyNew ZealandNorwayPakistanRussiaSpainSwedenTurkeyUnited ArabEmiratesUnitedKingdom1241131326311181113251Asia (77, 77.8%)Europe (18, 18.2%)North America (1, 1.0%)South America (3, 3.0%)Australia (1, 1.0%)0102030Healthcare Green TechnologyData PrivacyAutonomous Systems & RoboticsSmart SystemsGames DevelopmentLaw and Regulatory SystemInformation SystemsBanking and Taxation0102030911114Other(s)12151953100-2Years3-5Years6-10Years> 10 Years2136GenericAI focused4148271010501020304050e) Years of Experience13 %27 %28 %20 %10 %1 %1-910-4950-249others250-499> 500d) Size of Organisation (number of employees)f) AI EthicsYesNoNotSure12%10%77%Software DeveloperChief Executive OfficerProject ManagerSoftware TesterRequirements EngineerSoftware Designer and ArchitectAI EngineerOther(s)01020304317325105293Law PractitionerIn general, the survey ﬁndings of AI ethics principles are in line with
the widely adopted accountability, responsibility, and transparency (ART)
framework [15] and the ﬁndings of an industrial empirical study conducted
by Ville et al. [2]. Both studies [2] [15] jointly considered transparency and
accountability as the core AI ethics principles, which is consistent with the
ﬁndings in this survey. However, we noticed that privacy has been ignored
in both mentioned studies [2] [15], but is placed as the third most signiﬁcant
principle in this survey. The reason might be that, as more and more AI
systems have been placed online, the signiﬁcance of privacy and data pro-
tection is increasingly recognized. Presently, various countries embarked on
legislation to ensure the protection of data and privacy.

Further, the results reveal that the majority of the survey respondents
(>60%) conﬁrmed the identiﬁed challenging factors [12] (see Figure 2(b)).
Lack of ethical knowledge is considered as the most signiﬁcant challenge by
(81.8%) the survey participants. It exhibits that knowledge of ethical aspects
across AI systems is largely ignored in industrial settings. There is a signiﬁ-
cant gap between research and practice in AI ethics. Various guidelines and
policies devised by researchers and regulatory bodies discussed diﬀerent ethi-
cal goals for AI systems. However, these goals have not been widely adopted
in the industrial domain because of limited knowledge of scaling them in
practice. The ﬁndings are in agreement with the industrial study conducted
by Ville et al.
[11], which summarises the overall ﬁndings by stating that
ethical aspects of AI systems are not particularly considered, and it mainly
happened because of a lack of knowledge, awareness, and personal commit-
ment. We noticed that no legal frameworks (69.7%) is ranked as the second
most common challenge for considering ethics in the AI domain. The pro-
liferation of AI technologies in high-risk areas starts mounting the pressure
of designing ethical and legal standards and frameworks to govern them. It
highlights the nuances of the debate on AI law and lays the groundwork for
a more inclusive AI governance framework. The framework shall focus on
most pertinent ethical issues raised by the AI systems, the use of AI across
industry and government organisations, and economic displacement (i.e. the
ethical reply to the loss of jobs as a result of AI-based automation). The
third most common challenging factor is lacking monitoring bodies, and it
was highlighted by (68.7%) of the survey participants. Lacking monitoring
bodies refers to the lack of regulatory oversight to assess ethics in AI sys-
tems. It raises the issue of public bodies’ empowerment to monitor, audit,
and oversee the enforcement of ethical concerns in AI technologies by the do-

9

main (e.g., health, transport, education). One survey respondent mentioned
that

(cid:213) “I believe it shall be mandatory for the industry to get standard approval
from monitoring bodies to consider ethics in the development process of AI
systems.”

Monitoring bodies extensively promote and observe the ethical values in
society and evaluate technology development associated with ethical aspects
of AI [2]. They would be tasked to advocate and deﬁne responsibilities and
develop rules, regulations, and practices in a situation where the system takes
a decision autonomously. The monitoring group should ensure “ethics by, in
and for design” as mentioned in AI HLEG [2] guidelines.

Additionally, the survey participants elaborated on new challenging fac-

tors. For instance, one of the participants mentioned that

(cid:213) “Implicit biases in AI algorithms such as data discrimination and cog-

nitive biases could impact system transparency.”
Similarly, the other respondent reported that
(cid:213) “Biases in the AI system’s design might bring distress to a group of

people or individuals.”

Moreover, a survey respondent explicitly considered the lack of tools for
ethical transparency and AI biases as signiﬁcant challenges to AI ethics. We
noticed that AI biases is reported as the most common additional challenge.
It will be interesting to further explore (i) the type of biases that might be
embedded with the AI algorithms, (ii) the causes of these biases, and (iii)
corresponding countermeasures to minimize the negative impact on AI ethics.
Finally, the severity impact of the seven challenging factors is measured
with respect to the six AI ethics principles (see Figure 2 (c)). For the trans-
parency principle, we noticed that the challenging factor interpret principles
diﬀerently has signiﬁcant long-term impacts, and 77% (i.e., 50% major, and
27% catastrophic) of the survey participants agreed to it. The interpretation
of ethical concepts can change for a group of people and individuals. For
instance, the practitioners might perceive transparency diﬀerently (more fo-
cused on technical aspects) than law and policymakers, who have broad social
concerns. Furthermore, lack of ethical knowledge has a short-term impact on
the transparency principle, and it is evident from the survey ﬁndings sup-
ported by 52% (7% insigniﬁcant, 25% minor, and 20% moderate) responses.
Lack of knowledge could be instantly covered by attaining knowledge, un-
derstanding, and awareness of transparency concepts.

Conﬂict in practice is deemed the most signiﬁcant challenge to the pri-

10

vacy principle. Hence, 74% (i.e., 53% major, and 21% catastrophic) survey
respondents considered it a long-term severe challenge. Various groups, or-
ganizations, and individuals might have opinion conﬂicts associated with
privacy in AI ethics. It is critical to interpret and understand privacy con-
ﬂicts in practice. We noticed that (82%) of survey participants considered
the challenging factor extra constraints as the most severe (long-term) chal-
lenge for both accountability and fairness principles. Situational constraints,
including organizational politics, lack of information, and management in-
terruption, could possibly interfere with the accountability and fairness mea-
sures.
It could negatively impact the employee’s motivation and interest
to explicitly consider ethical aspects across the AI activities. Interestingly,
(82%) of the survey respondents considered conﬂict in practice as the most
common (long-term) challenge for autonomy and explainability principles.

Overall, we could interpret that conﬂict in practice is the most severe
challenge, and its average occurrence is >77% for all the principles. It gives
a general understanding to propose speciﬁc solutions that focus on tackling
the opinion conﬂict regarding the real-world implication of AI ethics prin-
ciples. The results further reveal lack of ethical knowledge has an average
(38%) short-term impact across selected AI ethics principles. The lack of
knowledge gap could be covered by conducting training sessions, workshops,
certiﬁcation, and encouraging social awareness of AI ethics. Knowledge in-
creases the possibility of AI ethics success and acceptance in the best practice
of the domain.

11

Figure 2: AI Ethics Principles and Challenges

12

StronglyAgreeModeratelyAgreeSlightlyAgree204060803. Accountability4. Fairness5. Autonomy6. Explainability7. Justice8. Non-maleficence9. Human Dignity10. Beneficence11. Responsability12. Safety13. Data Security14. Sustainability15. Freedom16. Solidarity17. Prosperity18. Effectiveness19. Accuracy20. Predictability21. InterpretabilityNeutralStronglyAgreeModeratelyDisagreeSlightlyDisagreeAverage AgreeAverage Disagree 193121772103050709071.721.224251316862.61428191011561.6192713139359.61271228221699363.624.2121. Lack of ethical Knowledge2. Vague principles3. Highly general principles4. Conflict in practice5.  Interpret principles differently6. Lack of technical understanding7. Extra constraints8. Lacking monitoring bodies9. No legal frameworks10. Business interest11. Pluralism of ethical methods 12. Cases of ethical dilemmas13. Machine distortion 14. Lack of guidance and adoption 15. Lack of cross-cultural cooperation  163317105766.723.211a) AI Ethics Principlesb) AI Ethics ChallangesTransparencyPrivacyAccountabilityFairnessAutonomyExplainabilityMinorCatastrophicLong-term7917c) Impacts of AI Ethics Challanges on AI Ethics PrinciplesBA: Lack of ethical knowledgeB: Vague PrinciplesC: Highly Generic PrinciplesD: Conflict in PracticeE: Interpret principles differentlyF: Lack of technical understandingG: Extra Constraints21.228.31525.228121015208666.724.268.729211871031124.224251591410264.626.32922141582965.725.3331616101013165.724.2191811139165.723.260.663.660.664.661.628.2291418261625153114891010112124181010143171210611172221581341226202516152182914851018271751531361531.361.623.221.228.326.262.662.624.2622.22891. Transparency2. Privacy 77.812.110183425113512212618.269.722481.812.161545212040608010305070901002612361318121720.261.614272391421026.364.6218321351631.363.6114202719117266.722.213162921611266.727.314192425811668.723.2622272088669.722.28202515813160.631.3172118231112162.626.3131127261012264.625.311162325713664.628.391829201212367.720.25192824106761.628.315Short-termInsignificant45ModerateMajorB37308519A253420204021918224131617CDE50122764F201619404G214121115A153919B10234417C416223918D59115321E3173723F814233816G1182135245BA6184712313214022C27303921D112174425E410164821F118214118G15115329E317243916F514184319G26294220313234713B415231641CD417214314A3595131E510174027F613244115214G203924B16105131C317204415D315214020A411233724E515184219F2122739191G1920441524114834B410184720C412244019DA31720431613166 Lessons Learned

The study ﬁndings result in the following lessons learned:

1. Emerging Roles: Besides practitioners, the role of policy and law-
makers is also important in deﬁning the ethical solutions for AI-based
systems. Based on our knowledge, this study is the ﬁrst eﬀort made to
encapsulate the views and opinions of both types of populations.

2. Conﬁrmatory Findings: This study empirically conﬁrms the AI
ethics principles and challenging factors discussed in our published SLR
study [12].

3. Adherence to AI principles: The most common principles (e.g.,
transparency, privacy, accountability) and challenges (e.g., lack of eth-
ical knowledge, no legal frameworks, lacking monitoring bodies) must
be carefully realized in AI ethics.

4. Risk-aware AI ethics principle design: The challenging factors
have mainly long-term severity impacts across the AI ethics principles.
It opens a new research call to propose solutions for minimizing or
mitigating the impacts of the challenging factors.

The given catalogue (see Figure 2) of principles and challenging factors
could be used as a guideline for deﬁning ethics in the AI domain. Moreover,
the catalogue is a starting point for further research on AI ethics.
It is
essential to mention that the identiﬁed principles and challenging factors
only reﬂect the perceptions of 99 practitioners and lawmakers in 20 countries.
More deep and comprehensive empirical investigation with wider groups of
practitioners would be useful to generalize the study ﬁndings globally. This,
together with proposing a robust solution for integrating ethical aspects in
AI design and process ﬂow, will be part of our future work.

References

[1] C.

Pazzanese,

“Ethical

concerns mount
role

in

as

AI
takes
industries.”

decision-making

bigger
https://news.harvard.edu/gazette/story/2020/10/
ethical-concerns-mount-as-ai-takes-bigger-decision-making-role/.
accessed on 2022-04-05.

more

13

[2] High-Level Expert Group on Artiﬁcial Intelligence, “Ethics guidelines
for trustworthy AI.” https://www.aepd.es/sites/default/files/
2019-12/ai-ethics-guidelines.pdf. accessed on 2022-04-06.

[3] J.

Dastin,

“Worldwide

2017–2021.”

date,
-amazon-com-jobs-automation-insight-idUSKCN1MK08G.
on 2022-04-05.

software

DevOps
up-
https://www.reuters.com/article/
accessed

forecast

[4] V. Vakkuri, K.-K. Kemell, J. Kultanen, and P. Abrahamsson, “The cur-
rent state of industrial practice in artiﬁcial intelligence ethics,” IEEE
Software, vol. 37, no. 4, pp. 50–57, 2020.

[5] Q. Lu, L. Zhu, X. Xu, J. Whittle, D. Douglas, and C. Sanderson,
“Software engineering for responsible AI: An empirical study and op-
erationalised patterns,” in Proc. of the 44th IEEE/ACM International
Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP), pp. 241–242, IEEE, 2022.

[6] J. Leikas, R. Koivisto, and N. Gotcheva, “Ethical framework for de-
signing autonomous intelligent systems,” Journal of Open Innovation:
Technology, Market, and Complexity, vol. 5, no. 1, p. 18, 2019.

[7] A. Jobin, M. Ienca, and E. Vayena, “The global landscape of AI ethics
guidelines,” Nature Machine Intelligence, vol. 1, no. 9, pp. 389–399,
2019.

[8] IEEE Global Initiative on Ethics of Autonomous and Intelligent Sys-
tems, “Ethically aligned design: A vision for prioritizing human well-
being with autonomous and intelligent systems.” http://standards.
ieee.org/develop/indconn/ec/autonomous_systems.html, Version
2. IEEE, 2017. accessed on 2022-04-06.

[9] ISO, “ISO/IEC JTC 1/SC 42 artiﬁcial intelligence.” https://www.iso.

org/committee/6794475.html, 2017. accessed on 2022-02-26.

[10] A. McNamara, J. Smith, and E. Murphy-Hill, “Does ACM’s code of
ethics change ethical decision making in software development?,” in
Proc. of the 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE), pp. 729–733, ACM, 2018.

14

[11] V. Vakkuri, K.-K. Kemell, M. Jantunen, and P. Abrahamsson, “This is
just a prototype: How ethics are ignored in software startup-like environ-
ments,” in Proc. of the 21st International Conference on Agile Software
Development (XP), pp. 195–210, Springer, 2020.

[12] A. A. Khan, S. Badshah, P. Liang, M. Waseem, B. Khan, A. Ahmad,
M. Fahmideh, M. Niazi, and M. A. Akbar, “Ethics of AI: A system-
atic literature review of principles and challenges,” in Proc. of the 26th
International Conference on Evaluation and Assessment in Software En-
gineering (EASE), pp. 383–392, ACM, 2022.

[13] A. Wilk,

“Teaching AI, ethics,

law and policy,” arXiv preprint

arXiv:1904.12470, 2019.

[14] A. A. Khan, M. Azeem Akbar, M. Waseem, M. Fahmideh, P. Liang,
“Replication pack-
A. Aakash, M. Niazi, and P. Abrahamsson,
age for the paper: Ai ethics:
Software practitioners’ and law-
makers’ points of view.” https://drive.google.com/drive/folders/
accessed on
1f9NX6hCRyUa0l-GL0yHqoNPN9Yl68pXF?usp=sharing.
2022-06-26.

[15] V. Dignum, “Responsible autonomy,” arXiv preprint arXiv:1706.02513,

2017.

15

