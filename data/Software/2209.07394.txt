ESAVE: Estimating Server and Virtual Machine Energy

Priyavanshi Pathania†, Rohit Mehra†, Vibhu Saujanya Sharma†, Vikrant Kaulgud†, Sanjay Podder‡,
Adam P. Burden*
†Accenture Labs, India ‡Accenture, India *Accenture, USA
{priyavanshi.pathania,rohit.a.mehra,vibhu.sharma,vikrant.kaulgud,sanjay.podder,adam.p.burden}@accenture.com

2
2
0
2

p
e
S
5
1

]
E
S
.
s
c
[

1
v
4
9
3
7
0
.
9
0
2
2
:
v
i
X
r
a

ABSTRACT
Sustainable software engineering has received a lot of attention in
recent times, as we witness an ever-growing slice of energy use, for
example, at data centers, as software systems utilize the underlying
infrastructure. Characterizing servers for their energy use accu-
rately without being intrusive, is therefore important to make sus-
tainable software deployment choices. In this paper, we introduce
ESAVE which is a machine learning-based approach that leverages
a small set of hardware attributes to characterize a server or virtual
machine’s energy usage across different levels of utilization. This
is based upon an extensive exploration of multiple ML approaches,
with a focus on a minimal set of required attributes, while show-
casing good accuracy. Early validations show that ESAVE has only
around 12% average prediction error, despite being non-intrusive.

KEYWORDS
Sustainable Software Engineering, Green IT, Energy Estimation,
Carbon Emissions, Machine Learning, Energy Modeling

ACM Reference Format:
Priyavanshi Pathania†, Rohit Mehra†, Vibhu Saujanya Sharma†, Vikrant
Kaulgud†, Sanjay Podder‡, Adam P. Burden*. 2022. ESAVE: Estimating
Server and Virtual Machine Energy. In 37th IEEE/ACM International Con-
ference on Automated Software Engineering (ASE ’22), October 10–14, 2022,
Rochester, MI, USA. ACM, New York, NY, USA, 3 pages. https://doi.org/10.
1145/3551349.3561170

1 INTRODUCTION
While technology has been a great enabler for sustainability in
various fields, it has a carbon footprint of its own. Multiple studies
have estimated that the internet and communications technology
industry (ICT) currently accounts for 2-7% of the global greenhouse
gas emissions (GHG), and is expected to increase to 14% by 2040 [3].
A major share of these emissions can be attributed to the design,
development, deployment, and distribution of software systems
and the corresponding hardware infrastructure required to support
these activities [9]. Moreover, data centers (DC) required to run
these software systems, currently account for 1-2% of the global
electricity demand (or about 0.3% of the global GHG emissions)
[1, 13]. If left unaddressed, these rapidly growing emissions can
have a disastrous impact on the sustainability of our environment.
One of the primary reasons behind these high carbon emissions
is the non-optimization of software systems from a sustainabil-
ity perspective (specifically green/energy perspective) [15]. While
there are multiple reasons like lack of awareness, intrusive ap-
proaches, hidden impact, etc. attributing to this non-optimization
[12], the dearth of tools and techniques to quantify the environmen-
tal impact of a software system w.r.t the energy consumed by the
underlying hardware, is one of the foremost [10, 15]. This acts as a

DOI: https://doi.org/10.1145/3551349.3561170

major challenge for an environmentally-inclined practitioner (de-
veloper, project manager, etc.) who wants to adopt energy-efficient
practices in her software, but does not have the required tooling
support to measure and quantify the realized benefits. For exam-
ple, quantifying the energy benefits of replacing an energy-hungry
design pattern with an energy-efficient design pattern [14].

A majority of the industrial software systems are deployed on
DC, including both on-premise and cloud DC. Also, in most cases,
even the applications deployed on end-user devices (smartphone,
laptop, desktop, etc.) usually have a backend component deployed
on DC. Due to this sheer volume of compute happening on these
DC, they generate almost double the emissions (45% of ICT in 2020),
in comparison to the end-user devices (24% of ICT in 2020) [2].
Hence, in order to have a bigger impact on the overall sustainability
of our environment, optimizing software systems that reside on
these DC, is the primary focus of this research.

In the recent past, there have been some tools and approaches
to estimate the energy consumption of the underlying hardware
(as a proxy for the energy consumption of the software system
running on top), but they are either highly approximate, intrusive
in nature, require access to a copious amount of static/run-time
telemetry, etc. Although some of these approaches, along with their
inherent limitations, might still serve the purpose of academic re-
search, they are impractical from an industry usage standpoint. For
example, the Cloud Carbon Footprint tool needs access to the cloud
service provider’s billing report, in order to estimate the energy
consumption and carbon emissions of the utilized cloud resources
[5]. Granting access to such confidential information might not be
possible for an organization due to adherence to multiple security,
privacy, and compliance-related protocols. The problem further am-
plifies for a software services organization where project artifacts
are predominantly owned by the clients. Keeping in mind these
limitations, we believe that an ideal energy estimation approach
should (i) be non-intrusive (ii) leverage easily accessible telemetry
(iii) support diverse architectures and manufacturers (iv) avoid con-
stant retraining (v) estimate the energy of the entire hardware and
not just components, while demonstrating (vi) high accuracy.

In this paper, we introduce ESAVE (Estimating Server And Virtual
machine Energy), a novel approach that enables energy estimation
for software systems running on top of bare-metal servers or virtual
machines (VM), for both on-premise and cloud DC. The approach is
non-intrusive and leverages a user-friendly set of hardware config-
uration attributes and basic runtime telemetry to estimate the en-
ergy consumption using a custom-trained machine learning model,
which has been trained on an industry-standard benchmark called
SPECpower_ssj2008. Despite supporting non-intrusive estimation,
along with other characteristics of an ideal energy estimation ap-
proach, ESAVE has been validated to exhibit less than 12% average
prediction error (across both cross and external validation).

 
 
 
 
 
 
ASE ’22, October 10–14, 2022, Rochester, MI, USA

Priyavanshi Pathania†, Rohit Mehra†, Vibhu Saujanya Sharma†, Vikrant Kaulgud†, Sanjay Podder‡, Adam P. Burden*

2 FORMULATING ESAVE
For building a non-intrusive energy estimation approach, we lever-
aged the SPECpower_ssj2008 dataset for training our ML models
[7, 18]. It is the first industry-standard benchmark that measures
the power of server class computers at graduated load levels of 0%
(idle) to 100%, in 10% increments. At the time of this research, the
dataset had 755 observations corresponding to servers produced
by noted original equipment manufacturers like IBM, HP, Dell, etc.
and spread across 90 features describing the server configuration
and its power characteristics. Please note that the dataset uses the
terminology as load levels instead of CPU utilization levels, but for
this paper, we have used them interchangeably [8, 19].

Post initial data cleanup, feature engineering, and extensive ex-
ploratory data analysis, we selected 15 features from the dataset
to train our model on. These features describe the server charac-
teristics pertaining to its CPU, memory, storage, and server nodes.
Selection of these features was based upon a careful balance be-
tween statistical considerations (for e.g, the correlation between
these features and server power characteristics) and ease-of-access
considerations (for e.g, while power supply rating exhibits a decent
correlation with server power characteristics, it is difficult to access,
especially on the cloud, and hence not selected). Moreover, based
upon our internal brainstorming sessions with key stakeholders
in our organization, we devised another set of 8 reduced features,
by giving higher importance to ease-of-access considerations, and
selecting the features that describe the server characteristics per-
taining only to its CPU. Table 1 lists both sets of selected features
(ref. Models - All Features) and reduced features (ref. Models - Re-
duced Features). Finally, we selected 11 dependent variables (from
power at 0% (idle) CPU util. to power at 100% CPU util., in incre-
ments of 10%) to then train a set of 11 individual ML models for
estimating each one of those dependent variables.

Since our cleaned-up dataset comprised mainly of continuous
and dichotomous features, we leveraged regression-based algo-
rithms to train our estimation models. Overall, we tested with 13
different regression algorithms (for example, linear regression, sup-
port vector regression, neural network regression, etc.).

To evaluate the goodness of fit for the trained models, we lever-
aged the Mean Absolute Percentage Error (MAPE) metric [6]. Out
of the 13 regression algorithms that we tested, XGBoost (an opti-
mized gradient boosting algorithm) demonstrated the least MAPE,
consistently across all 11 trained models [4]. Table 1 shows a sum-
mary of the 10-fold cross-validation results for the XGBoost models.
The average MAPE across all 11 models is reported to be 9.78% for
Models - All Features and 12.27% for Models - Reduced Features.

The results indicate that the trained models can be used to esti-
mate the power consumption curve of bare-metal servers by hav-
ing access to the aforementioned set of hardware features, while
demonstrating high accuracy. Moreover, the estimated power curve
can thereafter be leveraged to estimate the energy consumption
of a software system running on top of such a bare-metal server,
by getting access to the average/temporal CPU utilization values
recorded during a specific software run (using existing performance
monitoring tools or simple batch/shell commands).

While these trained models currently only work for bare-metal
servers, we are working on extending this approach to support VMs

Table 1: 10-fold cross-validation results for the XGBoost
models, across all 11 trained models. (P@CPU = Power at
specific CPU Utilization, SE = Standard Error)

Models - All Features
#Chips, #CoresPerChip, #ThreadsPerCore, CPUFrequency
L1CacheIValue, L1CacheDValue, L2Cache, L3Cache
Memory, isSSD, #StorageDrives, StorageCapacity
isSingleNode, isMultiNode, #Nodes

Models - Reduced Features
#Chips, #CoresPerChip, #ThreadsPerCore, CPUFrequency
L1CacheIValue, L1CacheDValue, L2Cache, L3Cache

MAPE (%)

SE (%)

MAPE (%)

SE (%)

16.35
10.50
10.31
9.39
9.13
8.96
8.74
8.68
8.39
8.39
8.71

1.15
0.74
1.05
0.61
0.53
0.55
0.49
0.52
0.49
0.48
0.56

20.69
13.35
12.55
11.89
11.78
11.64
11.37
10.73
10.68
10.48
9.90

3.33
2.11
2.09
2.06
2.02
1.89
1.81
1.67
1.74
1.67
1.5

P@CPU

P@idle
P@10
P@20
P@30
P@40
P@50
P@60
P@70
P@80
P@90
P@100

Table 2: Early results of our validation experiments on com-
paring ESAVE and Turbostat.

Turbostat

ESAVE

Training Dataset

Run Time (sec)

Avg. Energy (J)

Avg. CPU Util. (%)

Avg. Energy (J)

Diff. (%)

Kaggle Cats vs Dogs
Kaggle Natural Scenes
MNIST

599
470
149

127,587
61,100
22,052

63.82
30.26
33.70

113,810
63,450
20,264

11.41
03.78
08.45

as well, by leveraging the concept of host energy apportioning. This
refers to apportioning of the underlying host’s energy consump-
tion to the respective VM, depending upon the portion of the host
resources dedicated to the VM [16]. Further explorations in this
direction represent a major portion of our planned future work.

3 EARLY VALIDATION
To validate the effectiveness/accuracy of our approach, we con-
ducted a set of early experiments that compared our energy esti-
mation results to that of turbostat [11]. Since turbostat only works
on bare-metal instances (VMs are not supported), we selected the
Amazon AWS instance g4dn.metal for our experiments. To exercise
the instance, we trained three neural network-based ML models
as workload. During the training process, the instance’s energy
was monitored using turbostat and per second CPU utilization was
logged using psutil [17]. Finally, an offline energy estimation was
performed using ESAVE by leveraging the hardware configuration
of the bare-metal server and previously logged CPU utilization
values (using the aforementioned Models - Reduced Features). Table
2 highlights the comparison between both the estimations. Despite
ESAVE being non-intrusive, the average deviation between both
the estimations was recorded to be 7.89%, which is very promising.

4 CONCLUSION AND FUTURE DIRECTIONS
In this paper, we introduced ESAVE, a novel approach for estimating
the energy consumed by a software system, running on a server, by
employing custom-trained ML models. Initial results indicate that
our approach performs very well in terms of both cross and external
validation. In the near future, we intend to extend ESAVE to VMs,
as well as conduct in-depth experiments to extensively validate
its applicability/accuracy. ESAVE is currently being piloted across
multiple software sustainability use cases at our organization, with
very encouraging initial feedback from key stakeholders.

ACKNOWLEDGEMENT
The authors would like to thank Samarth Sikand and Raghotham
M. Rao for their invaluable support throughout the ESAVE journey.

ESAVE: Estimating Server and Virtual Machine Energy

ASE ’22, October 10–14, 2022, Rochester, MI, USA

REFERENCES
[1] International Energy Agency. Accessed - 20/07/2022. Data Centres and Data
Transmission Networks. https://www.iea.org/reports/data-centres-and-data-
transmission-networks.

[2] Lotfi Belkhir and Ahmed Elmeligi. 2018. Assessing ICT global emissions footprint:
Trends to 2040 and recommendations. Journal of Cleaner Production 177 (2018),
448–463. https://doi.org/10.1016/j.jclepro.2017.12.239

[3] United Nations Environment Programme: Copenhagen Climate Centre. Accessed
- 20/07/2022. Greenhouse gas emissions in the ICT sector: trends and methodolo-
gies. https://c2e2.unepccc.org/wp-content/uploads/sites/3/2020/03/greenhouse-
gas-emissions-in-the-ict-sector.pdf.

[4] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting
System. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD
’16). Association for Computing Machinery, New York, NY, USA, 785–794. https:
//doi.org/10.1145/2939672.2939785

[5] cloudcarbonfootprint.org. Accessed - 20/07/2022. Cloud Carbon Footrpint. https:

//www.cloudcarbonfootprint.org.

[6] Arnaud de Myttenaere, Boris Golden, Bénédicte Le Grand, and Fabrice Rossi.
2016. Mean Absolute Percentage Error for regression models. Neurocomputing
192 (2016), 38–48. https://doi.org/10.1016/j.neucom.2015.12.114 Advances in
artificial neural networks, machine learning and computational intelligence.
[7] Larry D. Gray, Anil Kumar, and Harry H. Li. 2008. Workload Characterization of
the SPECpower_ssj2008 Benchmark. In Performance Evaluation: Metrics, Models
and Benchmarks, Samuel Kounev, Ian Gorton, and Kai Sachs (Eds.). Springer
Berlin Heidelberg, Berlin, Heidelberg, 262–282.

[8] Chung-Hsing Hsu and Stephen W. Poole. 2011. Power signature analysis of the
SPECpower_ssj2008 benchmark. In (IEEE ISPASS) IEEE International Symposium
on Performance Analysis of Systems and Software. 227–236. https://doi.org/10.
1109/ISPASS.2011.5762739

[9] Eva Kern, Markus Dick, Stefan Naumann, and Tim Hiller. 2015.

Impacts of
software and its engineering on the carbon footprint of ICT. Environmental Impact
Assessment Review 52 (2015), 53–61. https://doi.org/10.1016/j.eiar.2014.07.003

Information technology and renewable energy - Modelling, simulation, decision
support and environmental assessment.

[10] Patricia Lago. 2015. Challenges and Opportunities for Sustainable Software. In
2015 IEEE/ACM 5th International Workshop on Product Line Approaches in Software
Engineering. 1–2. https://doi.org/10.1109/PLEASE.2015.8

[11] linux.org. Accessed - 20/07/2022. TURBOSTAT. https://www.linux.org/docs/

man8/turbostat.html.

[12] Rohit Mehra, Vibhu Saujanya Sharma, Vikrant Kaulgud, Sanjay Podder, and
Adam P. Burden. 2022. Towards a Green Quotient for Software Projects. In
To appear in the Proceedings of the 40th International Conference on Software
Engineering: Software Engineering in Practice (Pittsburgh, Pennsylvania, USA)
(ICSE ’22). ACM. https://doi.org/10.48550/arXiv.2204.12998

[13] nature.com. Accessed - 20/07/2022. How to stop data centres from gobbling up

the world’s electricity. https://www.nature.com/articles/d41586-018-06610-y.

[14] Adel Noureddine and Ajitha Rajan. 2015. Optimising Energy Consumption of
Design Patterns. In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, Vol. 2. 623–626. https://doi.org/10.1109/ICSE.2015.208

[15] Gustavo Pinto and Fernando Castor. 2017. Energy Efficiency: A New Concern
for Application Software Developers. Commun. ACM 60, 12 (nov 2017), 68–75.
https://doi.org/10.1145/3154384

[16] GHG Protocol. 2017. ICT Sector Guidance built on the GHG Protocol Product

Life Cycle Accounting and Reporting Standard. Global: GHG Protocol (2017).

[17] pypi.org. Accessed - 20/07/2022. psutil. https://pypi.org/project/psutil/.
[18] Jóakim von Kistowski, Klaus-Dieter Lange, Jeremy A. Arnold, Sanjay Sharma,
Johann Pais, and Hansfried Block. 2018. Measuring and Benchmarking Power
Consumption and Energy Efficiency. In Companion of the 2018 ACM/SPEC In-
ternational Conference on Performance Engineering (Berlin, Germany) (ICPE ’18).
Association for Computing Machinery, New York, NY, USA, 57–65.
https:
//doi.org/10.1145/3185768.3185775

[19] Xiao Zhang, Jianjun Lu, and Xiao Qin. 2013. BFEPM: Best Fit Energy Prediction
Modeling Based on CPU Utilization. In 2013 IEEE Eighth International Conference
on Networking, Architecture and Storage. 41–49. https://doi.org/10.1109/NAS.
2013.12

