2
2
0
2

l
u
J

3
1

]
E
S
.
s
c
[

2
v
0
6
5
5
0
.
7
0
2
2
:
v
i
X
r
a

1

1+1>2: Programming Know-What and
Know-How Knowledge Fusion, Semantic
Enrichment and Coherent Application

Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Zhengkang Zuo, Changjing Wang, Xin Xia

Abstract—Software programming requires both API reference (know-what) knowledge and programming task (know-how) knowledge.
Lots of programming know-what and know-how knowledge is documented in text, for example, API reference documentation and
programming tutorials. To improve knowledge accessibility and usage, several recent studies use Natural Language Processing (NLP)
methods to construct API know-what knowledge graph (API-KG) and programming task know-how knowledge graph (Task-KG) from
software documentation. Although being promising, current API-KG and Task-KG are independent of each other, and thus are void of
inherent connections between the two types of knowledge. Our empirical study on Stack Overﬂow questions conﬁrms that only 36% of
the API usage problems can be answered by the know-how or the know-what knowledge alone, while the rest questions requires a
fusion of both. Inspired by this observation, we make the ﬁrst attempt to fuse API-KG and Task-KG by API entity linking. This fusion
creates nine categories of API semantic relations and two types of task semantic relations which are not present in the stand-alone
API-KG or Task-KG. According to the deﬁnitions of these new API and task semantic relations, our approach dives deeper than
surface-level API linking of API-KG and Task-KG, and infer nine categories of API semantic relations from task descriptions and two
types of task semantic relations with the assistance of API-KG, which enrich the declaration or syntactic relations in the current API-KG
and Task-KG. Our fused and semantically-enriched API-Task KG supports coherent API/Task-centric knowledge search by text or code
queries. We have implemented our approach on Java programming documentation and built a web tool to search and explore API and
programming task knowledge. Our evaluation conﬁrms the high-accuracy of our knowledge extraction, fusion and enrichment methods,
and the effectiveness and usefulness of our API-Task KG for answering Stack Overﬂow questions.

Index Terms—Software Programming, Knowledge Graph, Knowledge Fusion, Semantic Enrichment, Knowledge Search.

(cid:70)

1 INTRODUCTION

To use certain APIs in programming tasks, developers
resort to software documentation, such as API reference and
programming tutorials. API reference documentation ex-
plains the functionalities and usage directives of individual
APIs, while programming tutorials describe different tasks
and scenarios in which some APIs are (or are not) applicable
and how to use them properly or avoid misuses. In the
literature, the former is referred to as know-what, while the
latter is know-how. Recent studies construct API knowledge
graph (API-KG) [1], [2], [3] and programming task knowl-
edge graph (Task-KG) [4], [5] from software documentation.
They show that lifting programming knowledge from semi-
or unstructured text into structured knowledge enables
API- or task-centric knowledge recommendation and can
proactively reduces API misuses.

Although these studies show the promising of knowl-
edge graph methods for software engineering, the two
types of knowledge are currently independently analyzed,
extracted and used. However, through the observation of
common issues developers encounter (Section 2), we argue
that programming know-what and know-how knowledge
should be fused and used as a whole and the whole will

be more than the sum of the two parts. We show three
Stack Overﬂow questions in Figure 1 to illustrate this need.
Figure 1(a) shows a highly up-voted and frequently-viewed
question “how to avoiding ConcurrentModiﬁcationExcep-
tion1”. This common programming issue is caused by in-
voking collection.remove() to remove elements when iterat-
ing a Collection in a loop. The accepted answer posts an
explanation with reference to iterator.remove() in Java API
speciﬁcation (i.e., the knowledge source for API-KG), and
an exemplary code snippet from a Java Tutorial (i.e., the
knowledge source for Task-KG). This example suggests that
developers need both know-what and know-know knowl-
edge to use APIs correctly in programming tasks. API-
KG and Task-KG can be fused together by API entities
mentioned in task descriptions or code examples.

However, knowledge graph fusion should go way be-
yond this surface-level API linking. In fact, both of API-
KG and Task-KG contain inferrable knowledge for enriching
each other. Figure 1(b) shows an example of this need and
possibility. The question asks “how to rewrite the content of
ﬁle2”. The accepted answer summarizes two ways about ﬁle
rewriting. These two ways come from the two tasks in Java
Tutorial for Java FileOutputStream3 and Java FileWrite4,

• Q. Huang, Z. Yuan, Z. Zuo, C. Wang are with School of Computer

Information Engineering, Jiangxi Normal University, China.

• Z. Zuo is the corresponding author(zuo803@jxnu.edu.cn)
• Z. Xing is with Australian National University Australia.
• X. Xin is with HuaWei.

1. https://stackoverﬂow.com/questions/223918
2. https://stackoverﬂow.com/questions/1016278
3. http://tutorials.jenkov.com/java-io/ﬁleoutputstream.html
4. http://tutorials.jenkov.com/java-io/ﬁlewriter.html

 
 
 
 
 
 
Fig. 1: The Need for Programming Know-What and Know-How Knowledge Fusion and Semantic Enrichment

respectively. This implies the two tasks can achieve the same
goal (we refer to this type of task relation as Task-Align in
our empirical study). However, this Task-Align relation does
not appear in Java Tutorial, but can be inferred from the
similar task phrases (“overwrite the ﬁle with FileOutput-
Stream” and “overwrite an existing ﬁle with FileWriter”)
and the API-Extend relation between java.io.Filewrite and
java.io.FileOutputStream mentioned in the task phrases. This
example illustrates that API entities and relations can help
to infer new task semantic relations to enrich Task-KG.

Figure 1(c) shows another frequently-viewed question
“cannot issue data manipulation statements with execute-
Query5”. The accepted answer suggests to substitute exe-
cuteUpdate() for executeQuery(), which implies a what we
call Function-Replace relation between executeUpdate() and
executeQuery(). Unfortunately, this API Function-Replace re-
lation does not appear in the Java API speciﬁcation, nor
is covered by the API syntactic relations in API-KG. But it
can be inferred from the tutorial (Update the Database6).
Because tutorials often discuss and contrast related APIs
from the task perspective, rich API semantic relations may
be inferred to enrich API-KG.

To develop an in-depth understanding of the comple-
mentary nature of know-what and know-how knowledge
and explore the opportunity to incorporate them, we con-
duct an empirical study on 100 randomly-selected API
usage questions on Stack Overﬂow. In this study, we ﬁnd
that only 36% of the API usage questions can be answered
by the know-how or know-what knowledge alone, while the
rest requires a fusion of both types of knowledge. From this
study, we identify nine categories of API semantic relations
(e.g., function-replace, behavior difference, function collab-
oration) and two types of task semantic relations (task-align
and task-overlap) for this fusion.

5. https://stackoverﬂow.com/questions/1905607
6. http://tutorials.jenkov.com/jdbc/update.html

Inspired by this empirical study, we propose a novel
approach to construct a fused and semantically-enriched
API-Task KG on the basis of independent API-KG and Task-
KG. Our approach consists of four steps: KG construction,
KG fusion, KG enrichment and KG application. For KG con-
struction, we integrate the methods in [1], [2] to construct
the API-KG from API reference documentation and use the
method in [4] to construct the Task-KG from programming
tutorials. API-KG represents API entities, API function and
directive sentences and API declaration relations, and Task-
KG represents task entities (as <action, object> tuples),
task attributes (e.g., code examples, code summary) and
task dependencies. For KG fusion, we design an API-entity
linking method to fuse API-KG and Task-KG, which is the
combination at the entity level rather than the text level. For
KG enrichment, according to the deﬁnition of the API/task
semantic relations in our empirical study, we enrich API-KG
with nine types of API semantic relations inferred from task
descriptions in Task-KG, and enrich Task-KG with two types
of task semantic relations inferred from the APIs used in the
tasks and API relations in API-KG. For KG application, we
develop an API- and task-centric search engine and a web
tool for searching and exploring programming know-what
and know-how knowledge by text or code queries.

Our evaluation focuses on two aspects: the intrinsic
quality of the key steps of our approach, and the effec-
tiveness and usefulness of our API-Task KG. The key steps
of our approach include function sentence identiﬁcation,
directive sentence identiﬁcation, task phrase extraction and
API-packet extraction, inference of API semantic relations,
and inference of task semantic relations. Considering the
sheer amount of data instances in KG, we use a statistical
sampling method [6] to estimate the accuracy of these steps.
Our analysis shows: 94.30% and 92.70% accuracy for func-
tion and directive sentence identiﬁcation; 89.84% F1-score
for task phrase extraction and 89.72% accuracy for API-

2

ExecuteQuery()ExecuteUpdate()Function-ReplaceJava Tutorial (Task-KG)solution-2(a)(a)(b)(c)solution-1Java Tutorial (Task-KG)Java Doc (API-KG)packet extraction; 86.61% accuracy for KG fusion; 97.27%
and 90.55% average accuracy for API semantic relation
inference and task semantic relation inference, respectively.
To evaluate the effectiveness and usefulness of our API-
Task KG, we sample 16 frequently-viewed Stack Overﬂow
(SO) questions from 100 questions in our empirical study,
and collect the accepted answers of these questions. We
use either the question title or the code snippet in the
question to search our API-Task KG in our prototype tool,
and compare the recommended API and/or task knowledge
with the accepted answers. For 11 out of 16 questions, the
recommended knowledge includes the accepted answers.
In addition, the API-Task linkages and the inferred API
semantic relations and task semantic relations allow our
tool to suggest rich extended knowledge for the question,
which are not covered in the accepted answer. To verify
whether the extended suggestion is helpful or results in
unnecessary information overload, we conduct a user study
in which two groups of graduate students (6 each) use either
our API-Task KG tool or independent API-KG and Task-
KG tools to ﬁnd the solutions to the 16 SO questions. Our
study suggests that our API-Task KG helps the students ﬁnd
correct solutions faster (27.7% less completion time) and
more accurately (17.7% higher solution accuracy), and the
extended knowledge inspires the student to explore new
ideas or even come up with better solutions.

The idea of our work can be regarded as an instance
of the general phenomenon “people always distill or infer
the richer knowledge by fusing the independent knowledge,
and leverage this new knowledge to solve the problems
that the independent knowledge does not cover.” [7]. In this
paper, we make the following contributions:

• We conduct an empirical study to understand the
need of know-what and know-how knowledge fu-
sion and summarize ﬁne-grained semantic relations
for knowledge fusion, including nine categories of
API semantic relations and two types of task seman-
tic relations.

• To our best knowledge, we are the ﬁrst to investigate
the fusion and cross-enrichment of programming
know-what and know-how knowledge, and the ap-
plication of fused knowledge in API search.

• We propose a systematic approach for KG construc-
tion, fusion and semantic enrichment, and build a
prototype tool to search programming know-what
and know-how knowledge coherently.

• We apply our approach to Java API reference and
programming tutorials. Our evaluation conﬁrms the
high quality of the resulting API-Task KG and pro-
vide the evidence of its effectiveness and usefulness.
Our data package can be found here7

2 EMPIRICAL STUDY ON KNOWLEDGE FU-
SION

API-KG and Task-KG contain an abundance of structural
knowledge for solving programming issues. To understand

7. know-what-and-know-how-data

3

the role of know-what knowledge in API-KG and know-
how knowledge in Task-KG in addressing API usage ques-
tions, we conduct an empirical study on the questions
selected from Stack Overﬂow, and answer the following
research questions: RQ1 - If an API usage question can be
solved using API-KG or Task-KG alone, we regard it as Q1;
otherwise we regard it as Q2. What are the percentages of Q1
and Q2, respectively? RQ2 - Do the questions of Q1 imply the
semantic relations that are not in API-KG and Task-KG? RQ3 -
Do the questions of Q2 imply the semantic relations that are not
in API-KG and Task-KG?
2.1 Study Design

2.1.1 Background of API-KG and Task-KG

API-KG [1], [2], and Task-KG [4], [5], have been proposed
to improve the know-what and know-how knowledge ac-
cessibility by lifting programming knowledge from semi-
or unstructured text into structured knowledge. This struc-
tured knowledge exists in the form of relations of KG. Since
these relations are extracted from the document structure,
we refer to them as declaration (i.e., syntactic) relations.
The API-KG [1], [2] contains eight declaration relations, i.e.,
contain, extend, implement, throw, hasMethod, hasParameter,
hasField and hasConstructor. The Task-KG [4], [5] contains
three declaration relations, i.e., parent-child, sibling and tem-
poral. However, the know-what knowledge and know-how
knowledge reﬂected in these declarations are independent,
and we argue that this is not enough to solve programming
issues. Therefore, we conduct an empirical study to under-
stand the complementary nature of know-what and know-
how knowledge and explore the opportunity to combine
them for facilitating the discovery of the more ﬁne-grained
semantic relations.

2.1.2 Data Preparation

We select API usage questions tagged with ”java“ tag in the
SO data dump [8], and sort them in the descending order
by view count. Then we select 100 top-viewed questions
based on the two criteria: 1) the question title contains the
API name or the exception information; 2) the question has
the accepted answer with sufﬁcient details rather than just
referencing materials. The manual selection was performed
independently by two Master students with more than
four years of Java development experience, with a Cohen’s
Kappa agreement [9] of 0.842, i.e., almost perfect agreement.
As developers tend to answer the questions on Stack
Overﬂow with API descriptions (e.g., API function and
directive sentences [2]) and code examples, we consider
them as answer points following the treatment mentioned
in [10]. Note that an accepted answer may contain more
than one answer point, but it has only one main answer
point (the most direct answer to the question), and all of the
rest are secondary answer points.

Given 100 top-viewed questions, we ask those two Mas-
ter students to extract answer points from the accepted
answer of each question independently. Since two students
might extract different answer points to the same question,
we assign a PhD student (with more than six years of
Java development experience) to check the different answer
points and solve their conﬂicts. The Cohen’s Kappa coef-
ﬁcient between two Master students is 0.880 (i.e., almost
perfect agreement). Finally, we obtain 206 answer points,

including 100 main answer points. As a result, we collect
100 top-viewed questions, each of which has a main answer
point and optionally some secondary answer points.

2.1.3 Protocol

To answer RQ1, we give these two students a 10-minute
training session on how to distinguish Q1 from Q2 in the
100 top-viewed questions. If the main answer point to each
question could be retrieved in API-KG or Task-KG sepa-
rately, the question belongs to Q1; otherwise it belongs to
Q2. However, considering that there are differences between
the descriptions in the answer point and the ones in API-
KG or Task-KG, those two Master students would consider
the semantic similarity between the two descriptions, rather
than literal similarity, when retrieving answer points. For
example, the answer point to the question “Difference be-
tween StringBuilder and StringBuffer” is “StringBuffer is
synchronized, StringBuilder is not.”, while the description
in API-KG is “If synchronization is required then it is rec-
ommended that StringBuffer be used”. These two descrip-
tions are literally-different but semantically-similar. For the
answer point annotated differently, the PhD student serve
as an arbiter to make the ﬁnal decision.

To answer RQ2 and RQ3, we give those two Master
students a 20-minute training session on telling them the
existing declaration relations (i.e., syntactic relations) in
API-KG and Task-KG, so that they determine whether or
not semantic relations that are not in API-KG and Task-KG
exist in the questions of Q1 or Q2 type.

2.2 Result and Analysis

2.2.1 Answer to RQ1

Given each question, two students search for its main an-
swer point in API-KG [2] and Task-KG [4] separately, and
annotate the question as Q1 or Q2 depending on whether
the answer is retrieved. Statistically, Q1 accounts for 36%
while Q2 for 64%. Finally, we calculate the agreement be-
tween the two students for annotating, and the Cohen’s
Kappa coefﬁcient is 0.864 (i.e., almost perfect agreement).

2.2.2 Answer to RQ2

After analyzing whether or not the semantic relations that
are not in API-KG [2] and Task-KG [4] exist in the questions
of Q1, we ﬁnd that 78.3% of the questions of Q1 potentially
imply four types of new API semantic relations as follows.
Function Similarity relation is deﬁned that two API
entities have similar usage. We ﬁnd that 23.5% of the
questions in Q1 reveal this API relation. For the question
“How to store a large (10 digits) integer?”, the answer
point “If at any point you need bigger numbers, you can
try java.math.BigInteger, or java.math.BigDecimal” reveals
that both java.math.BigInteger and java.math.BigDecimal can
be used to operate bigger numbers.

Function Opposite relation is deﬁned that two API
entities have opposite usage. We ﬁnd that 9.8% of the
questions in Q1 reveal this API relation. For the question
“How to store IP Address range vs location”, the answer
point “The higherEntry(K key) does the opposite of the
lowerEntry(K key), meaning higherEntry(K key) returns a
key-value mapping associated with the least key strictly
greater than the given key, or null if there is no such key.”

reveals that higherEntry(K key) and lowerEntry(K key) have
the opposite function when operating the element.

Behavior Difference relation is deﬁned that two similar
API entities behave differently when completing the same
task. We ﬁnd that 21.6% of the questions in Q1 reveal
this API relation. For the question “What is the difference
between the add and offer methods in a Queue in Java”, the
answer point “when element can not be added to collection
the add method throws an exception and offer doesn’t.”
reveals that add() and offer() behave differently when adding
elements to a collection fails.

Function Replace relation is deﬁned that one API entity
should be replaced by another API entity in some speciﬁc
condition. We ﬁnd that 45.1% of the questions in Q1 reveal
this API relation. For the question “Iterator has.next() - is
there a way to get the previous element instead of the next
one?”, the answer point “You can use ListIterator instead of
Iterator. ” reveals that developers should use ListIterator and
Iterator when you need to get the previous element. This is
because ListIterator has previous() and hasPrevious() methods.
In this process, we calculate the agreement between the
two students for their annotations, and the Cohen’s Kappa
coefﬁcient is 0.836 (i.e., substantial agreement).
2.2.3 Answer to RQ3
By analyzing whether or not the semantic relations that are
not in API-KG and Task-KG exist in the questions of Q2, we
ﬁnd that 79.3% of the questions of Q2 potentially imply ﬁve
types of new API semantic relations and two kinds of task
semantic relations as follows.

Function Collaboration relation is deﬁned that two API
entities should be used together when accomplishing a task.
We ﬁnd that 14.3% of the questions in Q2 reveal this API
relation. For the question “how to start a function after
stop typing in a JTextField in java”, the answer point “Use
a Swing Timer and a DocumentListener, each time the
Document is updated, reset the Time” reveals that developer
should use Time and DocumentListener together when you
need to start a function after stop typing in a JTextField.

Type Conversion relation is deﬁned that that two API
entities can be converted to each other. We ﬁnd that 12.5%
of the questions in Q2 reveal this API relation. For the
question “Convert java.util.Date to String”, the answer point
“Convert a Date to a String using format() method” reveals
that Date can be converted to String under some conditions.
Implementation Constraint relation is deﬁned that the
implementation of one API relies on the other API. We ﬁnd
that 10.7% of the questions in Q2 reveal this API relation.
For the question “Remove equal item from java list”, the
answer point “If you properly override the equals method,
you can then just use the remove method” reveals that the
implementation of remove() relies on equal().

Logic Constraint relation is deﬁned that one API should
be called before or after using another API. We ﬁnd that
7.1% of the questions in Q2 reveal this API relation. For the
question “How to pause all running threads? and then re-
sume?”, the answer point ”wait() suspend the current thread
until another thread calls notify() to wake up.” reveals that
developers need to use notify() to wake up a thread after
suspending it with wait().

Efﬁciency Comparison relation is deﬁned that efﬁciency
comparison of two APIs in some speciﬁc conditions. We ﬁnd

4

that 5.3% of the questions in Q2 reveal this API relation.
For the question “Performance of TreeMap, HashMap and
LinkedHashMap?”, the answer point “Use a HashMap un-
less you have some need for ordering. HashMap is faster.”
reveals that HashMap is more efﬁcient than TreeMap and
LinkedHashMap when sorting is not required.

Task Align relation is deﬁned that different tasks achieve
the same goal. Actually, the same questions are often solved
in different ways. We ﬁnd that 33.9% of the questions in Q2
reveal this API relation. For the question “How to create
a Java string from the contents of a ﬁle?”, as shown in
the accepted answers, the developers offer six ways to
answer this question, and each contains a different code
example (i.e., different tasks). For example, the tasks can
be completed with File.readString() and Files.readAllLines().

Task Overlap relation is deﬁned that a snippet of code
for one task may overlap with a portion of code for another
task in terms of API usage. We ﬁnd that 16.1% of the
questions in Q2 reveal this API relation. For the question
“Create ArrayList from array” and the question “Initial-
ization of an ArrayList in one line”, these two different
questions are linked to each other by URLs on Stack Over-
ﬂow. This is because some of the code in their accepted
answers overlaps, i.e., “ArrayList<String> places = new
ArrayList<>(Arrays.asList());“.

In this process, we calculate the agreement between
the two students for annotation, and the Cohen’s Kappa
coefﬁcient is 0.857 (i.e., almost perfect agreement).
Fusion of know-what and know-how knowledge is frequently
needed to solve programming issues. This fusion requires
nine categories of API semantic relations and two types of
task semantic relations, which are not present in the existing
independent API-KG and Task-KG.

3 APPROACH

Our empirical study suggests the necessity of the fusion and
mutual enrichment of know-what and know-how knowl-
edge. As illustrated in Figure 2, our approach ﬁrst constructs
an API-KG and a Task-KG separately (Section 3.1) and links
the two KGs by API entities mentioned in task entities
(Section 3.2). Next, our approach infers new semantic re-
lations from one KG to enrich the other KG (Section 3.3 and
Section 3.4). Finally, we develop an API/Task-centric search
engine and a web interface that recommends programming
know-what and know-how knowledge as a uniﬁed whole
for text or code queries (Section 3.5). Moreover, the ﬁgure
in the middle shows an overview of API-Task KG, and the
bottom ﬁgure shows an application screenshot.

3.1 KG Construction

We construct API-KG and Task-KG from API reference
documentation and programming tutorials respectively.

3.1.1 API-KG Construction

We adopt the methods proposed in [1], [2] to construct API-
KG. These methods assume that API reference documenta-
tion follow a consistent semi-structured format, from which
we extract eight types of API entities (Package, Class, Inter-
face, Exception, Method, Parameter, Field) and eight types
of API declaration relations (contain, extend, implement,

Note that the ﬁgure in the middle, the red and purple dotted line
represents the enriched semantic relation, and the blue dotted line
represents the entity link between API-KG and Task-KG.
Fig. 2: KG Construction, Enrichment and Application
throw, hasMethod, hasParameter, hasField, hasConstructor).
These API entities and declaration relations constitute a so-
called API skeleton graph [1], [2]. Then we extract Function
or Directive Sentence [2] from API description as API at-
tributes. The former deﬁnes the functionality of the API,
the latter tells how to correct use the APIs. Speciﬁcally,
we resolve the pronouns in API descriptions with Neural-
Coref [11], and split the descriptions into sentences with
Spacy [12]. As API descriptions contain many API tokens,
general English tokenizer (e.g, Jieba [13]) would break an
API token into several tokens, which negatively affects
sentence splitting. For example, the method “add(index, E)”
will be broken into “add(index” and “E)” with the general
tokenizer. Therefore, we use software-speciﬁc tokenizer as
in the previous work [1], [2]. Then we do part-of-speech
(POS) tagging for the sentences, and identify the function
sentences based on two criteria: (1) API name is the subject
in the sentence; or (2) the sentence starts with a verb phrase,
for example, “returns the numbers of the elements in this
collection” for Collection.size(). Meanwhile we identify the
directive sentences according to 86 keywords (e.g., must,
only) deﬁned in an empirical study of API directives in API
documentation [14]. For example, according to the keyword
“must”, we extract the directive sentence “All methods on
the Array interface must be fully implemented if the JDBC
driver supports the data type”. We use the linking methods
in [1] to link the identiﬁed function and directive sentences
to the corresponding APIs. Following the practice in [1], [2],
if there are multiple function sentences, we merge as one
function-sentence attribute. We keep each directive sentence
as a separate API attribute.

3.1.2 Task-KG Construction
We follow the method in [4] to construct Task-KG. In par-
ticular, we extract task entities, attributes and relations from

5

 API  Relation SentenceTwo Task   Semantic  Relations API Reference DocumentationKG Construction, Fusion and EnrichmentProgramming  TutorialsAPI RelationSentenceExtractionTask KGConstructionConstruction Nine API  Semantic  Relations API Entities,  API RelationsAPI KGAPI Semantic  Relation  InferenceTask Semantic  Relation  InferenceAPI EntityLinkingFused and Semantically-Enriched API-Task KGthe semi-structured programming tutorials.

a) Task Entity Extraction. We use the same set of NLP
tools as described in Section 3.1.1 to process task texts,
split them into sentences and obtain the POS tags for the
sentences. Different from [4] that represent task entities at
sentence level, we extract verb phrases as task entities which
is more ﬁne-grained and can be more accurately predicted
(see Section 4.1.2). We extract verb phrases from the sen-
tences with Spacy, and annotate them independently by two
authors if a verb phrase implies a clear task intention and
should be considered as a task entity. For example,“Convert
Set to List” implies a task, while “pass the set as parameter
to addAll()” does not. If the two annotators disagree with
each other, the third author serves as an arbiter and make
the ﬁnal decision. We obtain a dataset including 4,688 task
phrases and 6,075 non-task phrases for training a CNN-
based task phrase classiﬁer [15].

Given a task phrase, we extract Action and Object
based on POS tag patterns. If POS tags match the pattern
Verb+Noun(*), we extract Verb as Action and Noun(*) as
Object, e.g., “Reads a single character” is chunked into
Action reads and Object a single character. If POS tags match
the pattern Verb+Noun(*)+ADP+Noun(*) and “ADP” means
adposition, we extract Verb+Noun(*) as Action and Noun(*)
as Object, e.g., “Remove element from Collection” is split
into Action remove element and Object Collection.

b) Task Attribute Extraction. Four task attributes will
be extracted from the description of a task: (1) important
Notes for the task; (2) Code Snippet illustrating how the task
works; (3) Code Summary; and (4) API Packet which is a
triplet <API name, package (or interface, class) name, the
number of parameters>. For example, list.add(’element’) will
be packeted into <add(), java.util.List, 1>. Here, we use the
number of parameters instead of parameter types for two
reasons: 1) matching APIs by the number of parameters is
faster than by parameter types (e.g., int, long, ﬂoat, <T>,
<E>); and 2) the parameter types identiﬁed from text are
often not accurate due to lack of type context. If multiple
notes (or code summaries, code snippets) are extracted for
a task, we merge them into one attribute entry. But we
keep each extracted API packet separately as an individual
attribute to perform individual API entity linking.

Note that not all task entities have all the four task
attributes. We extract important notes and code summary
based on a set of commonly seen keywords for such in-
formation in tutorial descriptions, for example, “if, note,
must, remember, instead, only, except, notice” for important
notes, and “ﬁrst, second, third, then, ﬁnally, this example, in
this case for code summary. Code snippet in programming
tutorials are usually enclosed with special HTML tag (e.g,
“<codebox>”), which can be easily identiﬁed and extracted.
For API packet, we extract the API name mentioned in
the sentence with the special HTML tag (e.g, <code>), for
example, remove() in “<code>remove()</code> element
from collection”. If there is no special tag for the API name,
we use the orthographic feature-based regular expressions
[16] (e.g., camel-case style, end with “()”) to extract API
names. Then we match the API name in the code snippet
(if any) to get the corresponding package (or interface,
class) name and the number of parameters, which forms
API packet. For the matching, we transform the source

code into a typed Abstract Syntax Tree involving APIs by
using Spoon [17], a tool for partial code parsing and type
resolution. For example, if put() in the task text matches
sortedMap.put(”a”, ”one”) in the code, we would create an
API packet <put(), java.util.sortedMap, 2> for the task. If
the task does not have code snippet or the API name does
not match any token in the code snippet, the API packet will
have only API name and the other two slots are null.

c) Task Relation Extraction. Three types of task relations
are extracted: (1) Hierarchical Relation between a parent task
and child tasks; (2) Sibling Relation between the same-level
child tasks; and (3) Temporal Relation for task execution order.
Hierarchical and sibling relations are extracted based on
document structure, for example, section and subsection
(e.g, HTML tags <h1>,<h2>), list bullets (e.g., HTML tags
<ul> and<li>). Temporal relations are extracted based on
commonly used temporal words (e.g, before, after, once) from
the task description between the adjacent child tasks.

3.2 KG Fusion

KG fusion is to link the task entities in Task-KG with
the API entities in API-KG. This is based on API entity
linking rather than the surface-level keyword matching. For
example, based on the keyword matching, add() mentioned
in a task would be confused to be linked to List.add(E),
List.add(int,E) or Collection.add(E).

3.2.1 API Representation for Entity Linking

To achieve high-quality entity linking, we represent an
API mention in the task description as a T<AP,Sent> tuple
where AP is the API packet attribute derived for this API
mention and Sent is the sentence where this API is men-
tioned. We also represent each API entity in API-KG as a
AP I<AP,Sent> tuple where AP and Sent are the API packet
and the function sentence attribute of this API entity. API
packet represents API syntactic structure while the sentence
embodies API usage semantics. Both types of information
are needed as several APIs may have the same syntactic
structure but their usage semantics would generally be dif-
ferent. For example, List.remove(int) and List.remove(Object)
have the same syntactic structure (i.e., <remove(), List, 1>),
but the former is to remove an element at the given index,
while the latter is to remove the given object.

3.2.2 API Entity Linking

Entity linking aims to link an entity mention in text to the
entity in a knowledge graph. The entity of our concern
is API, and they can be mentioned in task descriptions
or user queries. Without losing the generality, we refer to
Q<AP,Sent> as a query tuple (e.g., the tuple for an API
mention of a task entity) and {AP I<AP,Sent>} as the set
of candidate tuples for the API entities in the API-KG.
Note that the API entity linking method introduced here for
linking task entities and API entities will also be used for
API semantic relation inference (see Section 3.3), task-align
relation inference (see Section 3.4), and API/Task-centric
knowledge graph search (see Section 3.5).

First, we use Q<AP > as a query API packet to ﬁnd a
set of candidate API entities in API-KG by matching API
syntactic structure (i.e., AP IAP and QAP ). To check whether
two API packets are the same or not, we proposed an API

6

packet matching method. Given a query AP1<x,y,z> and
a candidate AP2<x’,y’,z’> where AP<x,y,z> represents an
API packet<API name, package (or interface, class) name,
parameter number>, if the expression (x! = null && x ==
x(cid:48))&&(y == null (cid:107) y == y(cid:48))&&(z == null (cid:107) z == z(cid:48))
is true, AP1<x,y,z> and AP2<x’,y’,z’> are matched, oth-
erwise the two API packets do not match. That is, two
matched API packets must always have the same non-null
API name. For the package (or interface, class) name (or
the parameter number), the matching is relaxed as these
two slots of QAP could be null. In such cases, the second
(or third) slot is regard as matched. If the second (or third)
slot is not null, then the corresponding slots of the two API
packets must be the same to match the two API packets.

If API packet matching ﬁnds only one API entity, that
API entity is linked to the task. If there are multiple candi-
date APIs, we ﬁlter the candidate API entities by comparing
API usage semantics (i.e., AP ISent and QSent). To check if
two sentences are semantically similar, we use the sentence-
embedding based matching method [3], [18].

For each sentence, we embed tokens into word embed-
dings, and average all token vectors into a sentence vector.
We calculate the cosine similarity between the two sentence
vectors, and select the API entity whose AP ISent has the
highest cosine similarity with QSent. In our current tool,
we train the word2vec model [19] with the 128-dimensional
word vectors on all the sentences in the JDK 1.8 API speciﬁ-
cation and the Java tutorial text.

3.3 Enrichment of API Semantic Relations

So far the API-KG contains only API declaration (i.e., syntac-
tic) relations extracted from document structure. However,
according to our empirical study, APIs often have semantic
relations with other APIs. For example, Deque.add(E) and
Deque.offer(E) support similar function, but they are also
different in that “if cannot insert the element into Queue,
offer() returns false while add() throws exception.”. Al-
though some API semantic relations can be inferred from
API reference documentation [10], and more types and more
ﬁne-grained API semantic relations can be inferred from
programming tutorials (i.e., the source of Task-KG). This
is because API reference documentation focuses mainly on
describing individual APIs while programming tutorials
often explain related APIs from the task perspective. For
example, from Java API speciﬁcation, existing method [10]
infers only a function-similar relation between Deque.add(E)
and Deque.offer(E) based on the similarity of their function
sentences. However, from the tutorial about the task “add an
element to the end (tail) of a Deque8”, we can infer the subtle
behavior-difference (deﬁned in the empirical study 2.2.3)
relation between the two APIs. According to the deﬁnitions
of nine categories of API semantic relations in our empirical
study 2.2.3, we develop a pattern-based method to infer
these API semantic relations from the task attributes, which
greatly enrich API relational knowledge in API-KG.

3.3.1 Inference of API Semantic Relations

To extract API sentences that might contain certain API
semantic relations, we summarize 18 sentence patterns from

the task description. Specially, we parse the text description
and split the text into sentences with the same software text
processing method described in Section 3.1.1. If a sentence
starts with a conjunction (e.g, but, and, etc.), it will not be
separated from the previous sentence. This keeps the rele-
vant sentences as a whole sentence to clarify the context of
API semantic relations. For example, in “In order to update
the database you need to use a Statement. But, instead of
calling the executeQuery() method, you call the executeUp-
date() method.9”, “in order to update ...” before “But” illus-
trates when you should use Statement.executeUpdate(String)
instead of Statement.executeQuery(String). Then we detect
API mentions in the sentences with the API mention extrac-
tion method [16]. If the number of detected API mentions is
greater than two, the sentence will be identiﬁed as an API
relation sentence. Finally, we perform POS tagging for the
API sentences with Spacy[12].

Three authors collaboratively analyze 10,129 API relation
sentences and summarize 18 syntactic patterns for extract-
ing nine types of API semantic relations. Due to space
limitation, we only show one of the sentence patterns for
each API semantic relation in table 1. All of 18 sentence pat-
terns can be found in our data package. If an API sentence
matches a pattern, the corresponding API relation will be ex-
tracted from the sentence. The matching is case-insensitive
and the word stem is used for matching different word
variants (e.g., differ* for differs, different, difference, differences).
Our approach may infer more than one relation for the
two APIs from one API sentence, because one API sentence
may match several patterns at the same time. For exam-
ple, based on the pattern ”AE1 [like/same/similar], AE2
[except/but]“ or the pattern “AE1 [fast/better/easier/easy]
than AE2”, we can extract the sentence “The Concur-
rentHashMap is very similar to the java.util.HashTable
class, except that ConcurrentHashMap offers better concur-
rency than HashTable does.”. At this time, we can extract
a behavior-difference and a efﬁciency-comparison relation for
java.util.ConcurrentHashMap and java.util.HashTable.

The API mentions in an extracted API relation sentence
will be linked to the corresponding API entities in API-KG
by the API entity linking method described in Section 3.2,
and the semantic relation between the two API entities is
added to API-KG. This API relation sentence is also attached
to the API entities as an inferred API relation attribute.

3.4 Enrichment of Task Semantic Relations

The Task-KG now contains three types of task relations
(parent-child, sibling and temporal) extracted from tutorial
document structure. However, according to our empirical
study 2.2.3, implicit task semantic relations, such as task-
align relation and the task-overlap relation, are often not
present in tutorial document structure, For the former, for
example, the tasks ”add elements to List“ and “add a
collection of objects to a Collection” can accomplish the
same goal. For the latter, for example, the code for the task
“iterate a list with an iterator” overlap with the one for
another task “remove elements during iteration”. In the Java
tutorial, these tasks have no explicit relations because they
are organized in different sections. However, with the help

8. http://tutorials.jenkov.com/java-collections/deque.html

9. http://tutorials.jenkov.com/jdbc/update.html

7

TABLE 1: Sentence Patterns for Extracting API Relation Sentences from Task Descriptions

Relation Category
Function Similarity
Function Opposite

Sentence Patterns
AE1 [like/similar/same] AE2
AE1 opposite (ADP) AE2

Behavior Difference

AE1 and AE2 differ in

Function Replace

VB ((ADP) NP) AE1 [instead
of/rather than/not] AE2

Example
Deque.pop() is similar to how the removeFirst() works
The ﬂoor() does the opposite of the ceiling(), . . .
The add() and offer() methods differ in how the behave
if the Queue is full so no more elements can be added

if some of the operations in the transaction fail,
you would call the rollback() instead of commit().

Function Collaboration AE1 (be) VB(VBN) [with/to] AE2 DataInputStream is often used together with DataOut-

Type Conversion
Implement Constraint
Logic Constraint

Efﬁciency Comparison

Convert AE1 to AE2
AE1 (will) use AE2
VB AE1 [after/followed
by/then/until] AE2
AE1 [fast/better/easier/easy]
than AE2

putStream
Convert List to Set
remove() method will use equals() to decide. . .
run() is executed by the thread after call start()

HashMap is typically faster than TreeMap. . .

Note: AE (API entity), VB (verb), ADP (adposition), NP (Noun phrase), ADV (adverb), VBN (past participle).

of API entities and API relations in API-KG, we can infer
task-align and task-overlap relations to enrich Task-KG.

3.4.1 Inference of Task Align Relations
Recall that a task entity is represented as a <action, object>
tuple. We infer a task-align relation between the two tasks
if the task action phrases are similar and the APIs involved
in the object have some relation in API-KG. For example,
as “add elements” is similar to “add a collection of an
object” and “java.util.list” extends “java.util.Collection”, we
will infer a task-align relation between “add elements to List”
and “add a collection of an object to Collection”.

Given two task entities (T1 and T2), we calculate a task
similarity score for aligning the tasks by Simtask(T 1, T 2) =
Actscore(T 1, T 2) + Objscore(T 1, T 2) (equation 1). If the
similarity score is greater than a user-deﬁned threshold,
we infer a task-align relation between T1 and T2. In our
current tool, we empirically set the threshold at 1.5 through
the experiments on a small validation set of tasks. In this
equation, Actscore(T 1, T 2) is the cosine similarity between
the phrase vectors of the action phrases of T1 and T2. For
the Objscore(T 1, T 2), we match the object phrase with the
API name in the API packets of the task entity to determine
if the object mentions an API. If the Objects of both task
entities contain APIs, we transform API mentions into two
tuples <AP, Sent> where AP is the matched API packet
and Sent is the task phrase. We link these two tuples
to the API entities in API-KG by the API entity linking
method described in Section 3.2. If the two API entities
are linked and there is a certain API relation between the
two API entities, Objscore(T 1, T 2) returns 1; otherwise 0. If
either one of Objects does not contain an API, we calculate
Objscore(T 1, T 2) in the same way as Actscore(T 1, T 2) .

3.4.2 Inference of Task Overlap Relations

We infer task-overlap relations by comparing the code snip-
pets of the two tasks and determining if the code of one task
overlaps the partial code of the other in terms of API usage.
Given the code snippet of a task entity, we extract
the APIs and transform them into API packets with the
API packet extraction method in Section 3.1.2. We retain

programming language keywords (e.g, “while”, “if ”) in the
code. Then, we represent the extracted APIs into query
tuples <AP,Sent> where AP is the API packet attribute of
this task for an extracted API and Sent is the combination of
the code summary and note attributes of the task. Next we
ﬁnd the corresponding API entities in API-KG through the
API entity linking method in Section 3.2. Finally, we obtain
a set of unique API entities used in the task code snippet.
That is, we do not consider the times an API is used.

Given two code snippets (C1 and C2) of the two tasks,
we denote two sets of API entities involved in C1 and C2
as AP IC1 and AP IC2, respectively. We calculate the code
similarity between the two tasks by Simcode(C1, C2) =
(cid:84) AP IC2/|AP IC1| (equation 2), which computes to
AP IC1
what extent C1 overlaps C2 in terms of API usage. Note that
Simcode(C1, C2) could be different from Simcode(C2, C1).
So we average Simcode(C1, C2) and Simcode(C2, C1) as the
code similarity of the two tasks. If the average score is
greater than a given threshold, we deduce a task-overlap
relation between the two tasks. In our current tool, we
empirically set the threshold at 0.6 through the experiments
on a small validation set of tasks.
3.5 Knowledge Graph Application

Based on our fused and semantically-enriched API-Task KG,
we developed a knowledge search tool for recommending
programming know-what and know-how knowledge coher-
ently. This tool consists of the back-end knowledge graph
and the front-end web interface. The back-end is a Neo4j
graph database which stores our API-Task KG. The front-
end supports text/code search. Instead of simple keyword
based search, our search engine uses KG methods to extract
task, API and relevant attributes from the query, and per-
form API/Task-centric search over our API-Task KG.

For example, given the text query “how to insert an item
in List with add()”, the knowledge search results by our tool
is shown in Figure 2. Our search engine extracts a task entity
<insert an item, List> from the query and uses the task-
align method described in Section 3.4.1 to match the task
entities in API-Task KG. Our task-align method can match
the relevant tasks even they lexical gap by keywords, for
example, “insert an element at speciﬁc index into a list”,

8

4.1 KG Quality Evaluation (RQ1)

This RQ examines all our KG steps from API-KG and Task-
KG construction, to KG fusion, and enrichment of API and
Task semantic relations. Consider the large amount of data
instances in KGs, we adopt a statistical sampling method
[6] that examines the minimal number of data instances so
that we can estimate the accuracy of the samples with the
error margin 0.05 at 95% conﬁdence level. The sampling size
is 384. We recruit two graduate students (not involved in
this work) with two years of Java development experience
to label the sampled data instances independently. We use
Cohen’s kappa [9] to evaluate the inter-rater agreement. For
each sample, if the two students make different labels, a
third student is assigned to make an additional label to
resolve the conﬂict by the majority-win strategy. Based on
the ﬁnal labels, we calculate the data accuracy.

4.1.1 API-KG construction

The construction of API skeleton graph is mechanic, relying
on only correctly parsing document format and structure.
In our KG, we obtain 44,174 function sentences and 52,685
directive sentences. The accuracy of these extracted function
and directive sentences need to be examined as they rely
on a set of keywords observed by ourselves and previ-
ous work [14]. We sample 384 function sentences and 384
directive sentences, and ask the annotators to determine
whether they are function (or directive) sentences according
to the deﬁnition in [2]. The Cohen’s kappa between the two
students is 0.941 and 0.876 for function sentences and direc-
tive sentences, respectively, which indicated almost perfect
agreement. Based on the ﬁnal labels, our keyword-based
methods achieve 94.30% and 92.70% for the identiﬁcation of
function and directive sentences, respectively. The errors are
caused by two main reasons. First, POS tagging errors, e.g.,
“peek” from the sentence “peek in interface Queue<E>” is
tagged as a Verb, while it is actually a method name. Sec-
ond, extract incomplete sentences due to incorrect sentence
splitting, e.g., “returns the lowest index i such that (o==null
?”. These errors could be mitigated by software-speciﬁc POS
tagging and sentence splitting methods. [20]

4.1.2 Task-KG Construction

For the task phrase identiﬁcation, we trained a sentence clas-
siﬁer (TextCNN) [15]. We compare two training methods:
use 10,763 labeled whole sentences (as baseline) or 10,763
labeled verb phrases in these sentences (our method). The
baseline was used in [4]. Both methods use the same conﬁg-
uration and were evaluated with 10-fold cross-validation.
The Precision, Recall, F1-Score and Accuracy of our method
are 89.79%, 89.90%, 89.84% and 86.56%, while those of
the baseline are 51.36%, 77.80%, 61.88% and 61.36%. Our
method performs much better because task phrases are
much less noisy than the whole sentences.

For the API-packet attribute extraction, we sampled 384
API packets from 325 task entities, and ask the annotators to
determine if all the three slots of the API packets are correct.
Only if all slots are correct, the API packet is regarded
as correct. The two annotators have 0.931 Cohen’s kappa
(almost perfect agreement). Based on the ﬁnal labels, the
accuracy of the API-packet attributes is 89.72%. The errors

Fig. 3: Screenshot of API-centric knowledge search

and “insert an item in List”. In addition to the Best Matched
Task, our search engine lists other related tasks through the
syntactic and semantic task relations, for example, “Add
element to Set” which has a task-align relation with the
matched task. Furthermore, our search engine also extracts
an API mention add() from the query, and links this API
mention to the API entity List.add(int,E) in API-KG. As
such, it complements the recommended task know-how
knowledge with the relevant API know-what knowledge.
Even more, our tool visualizes the KG fragments involving
the recommended tasks and APIs from which the user
may learn more extended knowledge. For example, through
another Task-align relation, the user will learn how to “Add
Collection of Objects to Collection”.

In a similar vein, given a code query (one or multiple
code lines) for an “execute SQL queries” task, our search
engine extracts the used APIs and constructs the API pack-
ets as described in Section 3.1.2. Then, it uses the API packet
matching method to ﬁnd candidate API entities in the API-
KG, and lists these API entities as the main API knowledge
(see Figure 3). Also, through API-task links, our search en-
gine ﬁnds a list of tasks that use the APIs in the code query,
and uses the task-overlap method to rank these tasks by
their API usage similarities to the code query. For example,
it will recommend related tasks such as “create a ResultSet
by execute queries” for the code query. From the knowledge
graph fragment visualized on the right panel, the user
can learn other extended knowledge, e.g., function-replace
relation, the user will learn to substitute executeUpdate() for
executeQuery() when updating the database, and the task
“execute SQL queries” which can achieve the same goal
as the recommended task “create a ResultSet by executing
queries” (the two tasks have task-overlap relations).

4 EVALUATION

As a proof of concept, we applied our approach to JDK 1.8
API speciﬁcation10 and the Java Tutorial11 on the jenkov
website. The resulting API-Task KG consists of 8,672 API
entities and 7,806 task entities, among which 2,382 API
entities and 4,716 task entities are cross linked. Our API-
Task KG contains 916 API semantic relations and 7,496 task
semantic relations which are not available in the existing
API-KG [1], [2] and Task-KG [4]. We conduct a series of
experiments to investigate two general research questions:
RQ1 - what is the accuracy of the key steps of our KG methods and
what is the quality of the constructed API-Task KG? RQ2 - Can
our API-Task-KG-empowered coherent knowledge search answer
the frequently-encountered programming questions in practice?

10. https://docs.oracle.com/javase/8/docs/api/
11. http://tutorials.jenkov.com/java/index.html

9

come mainly from the inaccurate package (or interface,
class) name. For example, the class name of stream() should
be java.util.Set, but it is mistaken as java.util.Map. This is be-
cause, given Map.keySet().stream(), we don’t know the return
type of Map.keySet() and thus mistake java.util.Map for the
class name of stream(). If this is case “Set s = Map.keySet();
s.stream()”, we can identify java.util.Set from the variables.

4.1.3 KG Fusion

For the KG fusion, we sampled 384 API-Task links, and
ask the annotators to determine whether the task entity is
linked to the correct API entity in API-KG, based on the task
entity attributes. The two annotators have 0.897 Cohen’s
kappa (almost perfect agreement). Based on the ﬁnal labels,
the entity linking method achieves 86.61% accuracy for KG
fusion. Through the error analysis, the errors are mainly
caused by inaccurate upper stream API packets, function
sentences, code summary, etc. For example, the task “set a
generic type for a Collection” is incorrectly linked to the
“java.lang.String”, because we inaccurately extract “Java
String‘’ as the API packet <String, lang, 0> in the task
attribute. In addition, we also sample 2,400 API entities from
API-Task KG and transform these API entities into API-
packets. 86.46% of API packets are unique. This indicates
that using the API syntactic structure can distinguish most
API entities and thus provide a solid foundation for the high
quality fusion of API KG and Task KG.

4.1.4 Inference of API Semantic Relations

We examine 916 API semantic relations inferred by our
approach. Table 2 shows the accuracy of each type of API se-
mantic relations and the Cohen’s Kappa agreement between
the two annotators. All agreement rates are above 0.76 (at
least substantial agreement). All accuracies are above 0.95.
The accuracy is 1 for Function Opposite, Type Conversion and
Efﬁciency Comparison, as these three relations generally use
clear and speciﬁc keywords (e.g, opposite, convert, fast)
to match with the syntactic patterns. The remaining six
API semantic relations are also highly accurate, but two
shortcomings: 1) our syntactic patterns cannot distinguish
the APIs from the self-deﬁned methods. For example, stop()
and dostop() are extracted as a function-replace relation,
but dostop() is not an API but a self-deﬁned method. 2)
our approach may infer incorrect API relations from API
sentences without considering the overall logic of the sen-
tence. For example, we infer an API relation <java.util.list,
Logical-Constraint, java.util.Stack<E>> from the API sen-
tence “each element is removed from the List then pushed
onto the Stack” based on the syntactic pattern VB AE1 then
AE2. However, this sentence expresses only the processing
order but not the logical constraint between the two APIs.

4.1.5 Inference of Task Semantic Relations

We sample 384 task-align relations and 384 task-overlap
relations in our API-Task KG, and ask the annotators to
determine their validity. The accuracy for task-align and
task-overlap relations is 88.63% and 92.47%, respectively.
The annotators have 0.796 and 0.843 Cohen’s Kappa re-
spectively (almost perfect agreement). Through the error
analysis, we identify two main causes of errors. First, the

TABLE 2: Accuracy of API Semantic Relation Inference

Relation Category
Function Opposite
Function Similarity
Behavior Difference
Function Replace
Function Collabora-
tion
Implement
Constraint
Logical Constraint
Type Conversion
Efﬁciency Compari-
son

Number Accuracy Agreement
8
116
119
95

1.000
0.893
0.836
0.821

1.000
0.965
0.975
0.958

412

0.951

0.793

91

43
23

9

0.953

0.952
1.000

1.000

0.768

0.780
1.000

1.000

wrong task-align relations are caused by the incorrect task
phrases, for example, “reverse List using Stack” is task-
aligned with “copy all elements of a List”, but the two tasks
have no relation. For the correct identiﬁed task phrases, the
accuracy of inferring task-align relation is actually 97.86%.
Second, the wrong task-overlap relations are caused by the
incorrect API names extracted from the code. For example,
our tool may split API names incorrectly because of im-
proper processing of HTML tag (e.g., <br>). As an example,
“stream.<br>forEach()” is spilt into two names “stream”
and “forEach()” rather than as a whole. Such errors affect
the subsequent API entity linking and API usage matching.
Our KG construction, fusion and enrichment steps are accu-
rate and produce a high-quality API-Task KG.

4.2 Effectiveness and Usefulness of KG (RQ2)

We conduct a pilot study to evaluate if our KG-empowered
knowledge search could answer the questions on SO.

4.2.1 Dataset

To simulate real-world programming issues, we sort the
SO questions from empirical study in the descending order
by view count, and select top-16 questions, as listed in the
table 3. They had been viewed in total 13,238k times and re-
ceived 13,191 votes (as of 20th August 2021), which indicates
that developers frequently encounter similar problems. For
each question, we collect the question title and description
and the accepted answer. The descriptions of the last ﬁve
questions (Q12-Q16) contain code snippets with problems.

4.2.2 Effectiveness Evaluation

Our tool supports both text and code queries. For text
search, we input the questions title from Q1 to Q11. For
code-search, we input the code snippets from Q12 to Q16.
Given a query, our tool outputs the answer including the
best matched API or task entity and the extended knowl-
edge of related APIs and/or tasks. For each question, we
use the same graduate students settings as KG quality
evaluation to determine if the answer by our tool includes
the accepted answer. If our answer includes the accepted
answer, we regard our tool can positively answer the ques-
tion (labeled as P), otherwise labeled as negative (N). We
evaluate the inter-rater agreement by Cohen’s kappa. We
invite a third student to provide an additional label to
resolve the disagreements by majority-win.

10

TABLE 3: 16 SO Questions & Effectiveness Labels

Questions
1.Remove equal
item from java list.
2.How to Insert an
item in List with
add()?

3.How to check if a
ﬁle exists?

4.Get the ﬁrst ele-
ment of the List or
Set?
5.How to efﬁciently
iterate entry in
Map?
6.How to rewrite
the content of a
ﬁle?
7.Convert Set to List
without creating
new List.
8.Sort ArrayList of
custom Objects by
property.

Label Questions

N

P

P

N

P

P

P

N

9.How to split a
string in Java?

10.How to properly
stop the Thread?

11.Avoid exception
when removing
objects in a loop.
12.Difference
between add() and
offer().

13.Performance of
TreeMap, HasMap?

14.Cannot issue
data statement with
executeQuery().
15.add objects to
List throws an
exception.

16.What does cipher
.update do in java?

Label

N

P

P

P

P

P

N

P

Table 3 presents the analysis results. For 11 (68.75%) of
the 16 questions (labeled with P), the annotators believe
the answers by our tool include the accepted answers of
these questions and thus should be able to solve these 11
questions. Consider Q11 “Iterating through a Collection,
avoiding ConcurrentModiﬁcationException when removing
objects in a loop?”. Given this question title, the text-based
search would retrieve the tasks like “Remove Element From
Collection”, “Iterate a Collection” and “Remove Elements
During Iteration” based on the literal text of the task
phrases. However, our tool recommends the third task as
the best match because our tool also considers Concurrent-
ModiﬁcationException which is implicit in the description of
the third task “Calling remove() does not cause a Concur-
rentModiﬁcationException to be thrown.”.

In addition, the inferred API or task semantic relations
play an important role in solving the questions. Take Q16
”What does cipher.update do in java?” as an example.
The user knows how to use cipher.doﬁnal(), but he doesn’t
know what cipher.update() does, which often appears with
cipher.doﬁnal(). Therefore, given a code snippet that contains
cipher.doﬁnal(), he hopes search Engine to recommend ci-
pher.update(). Our tool parses cipher.doFinal() from the code.
Then, based on the inferred Function-Collaboration relation,
the tool recommends cipher.update() as an extended API
which covers the accepted answer. This makes the user
understand that cipher.update() is used for slicing the big-
scale data into small-scale data, and then cipher.doﬁnal() is
used for storing the small-scale data.

For the N-labeled questions, our tool only recommends
some related knowledge which do not directly cover the
accepted answer. Take Q4 ”Get the ﬁrst element of the
List or Set?” as an example. Given this question, our tool
recommends “Get Elements From a Java List” as the best

match task. However, this match is only for List but not Set.
Then our tool recommends “Iterate Set Using Iterator” and
“Iterate List Using Iterator” as the extended tasks based on
the inferred task-align relation. However, the two extended
tasks explain the cursor traversal rather than the index
traversal, which do not directly solve Q4.

4.2.3 User Study

we conduct a user study to further evaluate how our knowl-
edge recommendation may help novice developers solve the
16 SO questions.

Baseline. To verify the effectiveness of our fused API-
Task recommendation, and whether the extended sugges-
tions based on knowledge fusion are helpful for developers,
we use API-KG [2] and Task-KG [4] as our baseline. API-KG
is to declare what APIs are, i.e., tell know-what knowledge
of API. Task-KG is to illustrate how to use the API, i.e., tell
know-how knowledge of API. See the details of API-KG and
Task-KG in Section 2.1.1.

Methodology. We recruit 12 undergraduate students
(not involved in this work) who have novice-level Java
development experience. Through a pre-study survey, we
ensure that none of these students had encountered the
experimental questions before. We assign them into two
groups (GA and GB) randomly (6 students in each group).
The GA participants use our tool to answer the questions
while those in GB use the independent API-KG and Task-
KG tools. GB participants can use either or both API-KG
and Task-KG tools, but the two tools are separate.

For the GA participants, we give a 20-minutes training
session to help them learn how to use our tool. They are
asked to perform text search for Q1-Q11 and code search
for Q12-Q16. The GA participants are not allowed to view
the information outside our tool. For the GB participants,
we also give them a 20-minute demonstration on how to
use API-KG or Task-KG tool, and ask them to search with
the single KG tool until the ﬁnal result is determined. It is
recommended that, after the participants input the query
statement and obtain the query results, they should prefer
to the query results from SO, and rewrite the original query
with the API information or the exception information of
the partial code in these results for the next round of the
search. All participants can issue any queries they like,
not limited to just question title or description information.
Each question is given eight minutes. For each question,
the participants are asked to submit an answer in text with
the detailed information (not just the reference). We also
record the process of question answering with the screen
recording tool, so that we observe the search behavior of
the participants. The two authors collaboratively compare
the correctness of the participants’ answers and collect the
completion time of each question per participant.

Results and Analysis. Table 4 shows the average ques-
tion completion time (AveQCT) and the average answer
correctness (AveAC) for GA and GB. GA complete the
questions faster (290.90 versus 402.57 seconds) and more
accurately (80.21% versus 61.46%) than GB. We use Welch’s
T-test [21] for verifying the statistical signiﬁcance of the
differences. The difference in time is statistically signiﬁcant
(p(cid:28)0.05), while the one in accuracy is insigniﬁcant (p=0.01).

11

TABLE 4: Performance Comparison

Groups

GroupA

GroupB

Participants AveQCT (S) AveAC(%)
306.5
302.7
268.7
293.8
287.9
285.8
290.90±8.29
417.3
378.9
403.6
395.4
423.8
396.4
402.57±14.82

P1
P2
P3
P4
P5
P6
Ave±stddev
P7
P8
P9
P10
P11
P12
Ave±stddev

93.75
75.00
68.75
81.25
75.00
87.50
80.21±8.40
62.50
56.25
62.50
68.75
62.50
62.50
62.50±3.61

Note: Ave QT: Average Question-Completion-Time

AveAC: Average Answer Correctness

By reviewing the task completion videos, we observed
that GB often switched back and forth between the two
separate API-KG tool and the Task-KG tool when ﬁnding
the answer to these questions. This prolonged their comple-
tion time. In contrast, our KG already distills and organizes
the knowledge as explicit API/task entities and their rich
relations. Our tool can search and present the relevant
knowledge in a well-structured and coherent way. This
helps the GA ﬁnd the answers faster and more accurately.

Note that the average accuracy of GA is 80.21%, which
is higher than the 68.75% effectiveness ratio in our effective
evaluation. This is because ﬁve GA participants inferred the
correct answers to three N-labeled questions (Q4/Q8/Q9)
based on the extended knowledge even though it does
not directly cover the accepted answers. For example, by
reviewing the question answering videos, we observed that
P1 inferred the answer to Q4 by the extended knowledge,
such as the “Iterate the element in List”, and “Iterate the
element in Set”, and learnt to get the ﬁrst element of List
or Set with the cursor traversal. It reveals that the extended
knowledge is potentially useful for knowledge acquisition.
GA have higher accuracy on Q6/Q13/Q14/Q16, com-
pared with GB. For Q6, ﬁve GA participants answered
the two ways to rewrite the ﬁle while only two in GB
answered the two ways. For example, through the question
answering videos, we noticed that P3 and P9 used the same
query for retrieval(i.e., “How to rewrite the content of a
ﬁle”). The difference is that P3 directly acquired the correct
answer through the Task-align relation between the two tasks
given by our tool, while P9 got only one way to rewrite
ﬁles from Task-KG tool that did not satisfy the question
answer. For Q13/Q14/Q16, six GA participants answered
correctly while none of them in GB were answered correctly.
For example, through the question answering videos, in
order to solve the Q14, we observed that P11 ﬁrst inputted
the misused method (i.e., java.sql.Statement.executeQuery())
as the query to search with the API-KG tool, and got the
java.sql.Statement.executeQuery() deﬁnition from the search-
ing results but he failed to ﬁx the programming issues.
And then P11 reformulated the query with the excep-
tion information thrown by the program (i.e., can not is-
sue data manipulation statements with executeQuery()), and

12

inputted it into the Task-KG tool. However, he only got
the task “how to use executeQuery() execute the state-
ment in SQL.”, and fail to ﬁx the programming issues
again. And the P6 inputted the issue code statement into
our tools, and acquired the answer through the Function-
Replace relation between Statement.executeQuery(String) and
Statement.executeUpdate(String) given by our tool and he
succeeded in ﬁxing the programming issue.

Furthermore, GA spent 27%, 32%, 34% and 37%
less time on Q6/Q13/Q14/Q16, respectively. Both GA
and GB had good accuracy (74.24% average accu-
racy for GA versus 72.73% for GB) on ten ques-
tions (Q2/Q3/Q4/Q7/Q8/Q9/Q10/Q11/Q12/Q15), but
GA spent on average 32.41% less time than GB, especially
on Q11 and Q12, GA were 51.09% faster. For example,
through the task completion videos, we observed that P7
in GB learned the answer to Q12 by reading the deﬁnition
of Deque.add() and Deque.offer() in different searching results
from the API-KG tool, while P1 in GA directly acquired the
answer through the function-similar and behavior-difference
relations between the two APIs given by our tool. It reveals
that the semantic knowledge generated by our fused KG can
help developers to solve the different questions that cannot
be solved by API-KG or Task-KG independently.

Both GA and GB had low accuracy on Q1 and Q5
(41.67% for GA and 33.33% for GB). For Q1, we observed
that P7 in GB got the task “how to use remove() in the
list” from the Task-KG tool, but he did not get a cue
for overriding equals(). After some investigation, P3 in GA
noticed the logical-constraint between remove() and equals()
recommended by our tool, but she run out of time to
submit the answer. For Q5, P5 in GA learned three ways
to iterate entry in Map with our tool, i.e., by using Iterator,
by using foreach, and by using Lambda Expression. At this
time, because our tool currently does not extract efﬁciency-
comparison between the three ways to accomplish the task.
P5 randomly copied one of the ways as the answer without
carefully examining the differences between the three ways,
which was an incorrect answer to Q5. P7 in GB only got one
way from the Task-KG tool, which was incorrect.

Our evaluation provides the initial evidence of the effectiveness
and usefulness of our KG methods for answering programming
questions that developers frequently encounter.

5 DISCUSSION

The major threat to internal validity is the subjective judg-
ment of the sampled data instances and the answer cor-
rectness. To mitigate this issue, we invited the annotators
who were not involved in our work. They annotated the
data independently and had high inter-rater agreements. We
make our experimental data available for replication, which
can be further validated the community. One major threat to
external validity is our approach was evaluated on only Java
documentation. Another external threat is the limitation of
the NLP-based tools. For example, when performing POS
tag, Spacy may mistake the API name as “VERB” or other
parts of speech. For another example, when Spacy splits
a line text into multiple sentences, the sentence split error
may occur due to the unexpected HTML tag(e.g., <br>).
Moreover, an external threat is our user study involves only

16 programming questions. These questions are common
programming issues, covering four different aspects, task-
oriented (e.g, Q2/Q3), API comparison (e.g, Q12/Q13),
code optimization (e.g, Q1/Q16) and code debugging (e.g,
Q14/Q15). The generalizability of our approach has to be
further validate for other programming languages and dif-
ferent programming knowledge needs in the future.

Multiple applications can beneﬁt from the resulting API-
Task KG, the signiﬁcance is as follows: 1) It extends the code
search in a better space. The API entities of our KG can be
used to match the under-development code to provide more
code examples for the developers; 2) Our fused KG provides
the interoperability for the code debugging. For the misused
API involved in the exception information, it tells how to ﬁx
this error. 3) Our tool is also useful for code optimization.
Through analyzing the Efﬁciency Comparison relation, we
can replace the current API with the faster API.

6 RELATED WORKS

Software documentation is a common way to communicate
programming knowledge [14]. Several studies identify the
challenges in accessing and using the knowledge in doc-
umentation [14] Recently, knowledge graph methods have
been proposed to improve the knowledge accessibility by
lifting the knowledge from document text to explicit APIs,
tasks, and their relations and constraints [1], [2], [10], [3],
[4], [5]. For example, Li et al. [1] and Liu et al. [2] construct
API knowledge graph with API caveats or directives. Such
API-KGs support API-centric caveat search or knowledge
summary. Sun et al. [4] construct a task-oriented knowledge
graph and support task-centric search. They further enhance
the Task-KG with more actions and ﬁne-grained code snip-
pets, and integrate the Task-KG into the IDE to support task-
aware API recommendation[5].

All these works focus on either API (know-what) knowl-
edge or task (know-how) knowledge. Liu et al. [10], [2] fuse
API knowledge and general computing knowledge from
Wikipedia, but our work is the ﬁrst attempt to fuse the
two types of programming knowledge. In addition, our
approach enriches the fused knowledge graph with ﬁne-
grained API and task semantic relations. Liu et al. [10] infer
three types of API comparison relations (function, constraint
and efﬁciency), which are similar to our API semantic
relations but much coarse grained. Ren et al. [3] converts
API caveat sentences into explicit API constraint relations
which supports API misuse detection. Their constraints cor-
respond to only one type of our API semantic relations (i.e.,
logical constraint). Some studies [10], [22], [23], [24] infer
logical constraints from text, but they express the constraints
as logic formulas rather than knowledge graph.

In addition to software documentation, much program-
ming knowledge exists in heterogeneous formats, such
as code, forum discussions, programming screencasts. Re-
covering the traceability across these heterogeneous for-
mats receives much attention. Srinivas et al. [25] construct
Graph4Code by connecting source code with different us-
age documentation to enrich program code semantic. Re-
searchers also link source code examples to API documen-
tation [26], [27]. The complementary nature of different in-
formation sources has also been explored to provide a more
complete picture of relevant knowledge [28], [29], [30], [3],

[31]. All existing works analyzes and links heterogeneous
information at the text level. In contrast, our knowledge
fusion is at the entity level. With API and task as explicit
entities in a graph, our approach supports coherent API- and
task-centric knowledge search and presents programming
know-what and know-how knowledge as a uniﬁed whole.

7 CONCLUSION

In this paper, we conduct an empirical study on the API
usage questions on Stack Overﬂow, and identify the ne-
cessity and mutual enrichment of know-what and know-
how knowledge for programming. Based on the identiﬁed
new API and task semantic relations, we present a novel
approach for constructing a fused and semantically en-
riched knowledge graph of APIs and tasks from software
documentation. Our graph fusion is based on API entity
linking which considers both API syntactic structure and
API usage semantics. Our semantic enrichment derives API
semantic relations and task semantic relations which cannot
be extracted from document structure or be derived from a
single source. Based on the fused and semantically-enriched
API and task knowledge graph, our knowledge search en-
gine achieves the effect of “1+1>2” through coherent API-
and task-centric matching and recommendation. As a proof
of concept, we build a large-scale, high-quality API-Task
knowledge graph for Java programming, and demonstrates
its usefulness for ﬁnding relevant API and task knowledge
needed to answer programming questions that developers
frequently encounter. In the future, we will apply our ap-
proach to more programming languages and more domains
(e.g., blockchain) and integrate our knowledge graph into
IDE for supporting practical application.

8 ACKNOWLEDGEMENTS

The work is partly supported by the National Nature Sci-
ence Foundation of China under Grant (Nos. 61902162,
61862033, 61762049), the National Nature Science Founda-
tion of Jiangxi Province (20202BAB202015), the Science and
technology Key project of Education Department of Jiangxi
Province (GJJ210307).

REFERENCES

[1] H. Li, S. Li, J. Sun, Z. Xing, X. Peng, M. Liu, and X. Zhao, “Im-
proving api caveats accessibility by mining api caveats knowledge
graph,” 2018 IEEE International Conference on Software Maintenance
and Evolution (ICSME), pp. 183–193, 2018.

[2] M. Liu, X. Peng, A. Marcus, Z. Xing, W. Xie, S. Xing, and Y. Liu,
“Generating query-speciﬁc class api summaries,” Proceedings of
the 2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering,
2019.

[4]

[3] X. Ren, X. Ye, Z. Xing, X. Xia, X. Xu, L. Zhu, and J. Sun, “Api-
misuse detection driven by ﬁne-grained api-constraint knowledge
graph,” 2020 35th IEEE/ACM International Conference on Automated
Software Engineering (ASE), pp. 461–472, 2020.
J. Sun, Z. Xing, R. Chu, H. Bai, J. Wang, and X. Peng, “Know-
how in programming tasks: From textual tutorials to task-oriented
knowledge graph,” 2019 IEEE International Conference on Software
Maintenance and Evolution (ICSME), pp. 257–268, 2019.
J. Sun, Z. Xing, X. Peng, X. Xu, and L. Zhu, “Task-oriented
api usage examples prompting powered by programming task
knowledge graph,” ArXiv, vol. abs/2006.07058, 2020.

[5]

[6] R. Singh and N. Mangat, “Elements of survey sampling,” 1996.

13

[7] P. Bagherzadeh and S. Bergler, “Competing independent modules
for knowledge integration and optimization,” in Findings of the
Association for Computational Linguistics: EMNLP 2021, 2021, pp.
4416–4425.
S. O. data dump, https://archive.org/download/stackexchange/,
2019.
J. R. Landis and G. Koch, “An application of hierarchical kappa-
type statistics in the assessment of majority agreement among
multiple observers.” Biometrics, vol. 33 2, pp. 363–74, 1977.

[9]

[8]

[10] Y. Liu, M. Liu, X. Peng, C. Treude, Z. Xing, and X. Zhang, “Gen-
erating concept based api element comparison using a knowledge
graph,” 2020 35th IEEE/ACM International Conference on Automated
Software Engineering (ASE), pp. 834–845, 2020.

[11] NeuralCoref, https://github.com/huggingface/neuralcoref.
[12] Spacy, https://spacy.io.
[13] Jieba, https://github.com/fxsjy/jieba.
[14] M. Martin, M. Eichberg, E. Tekes, and M. Mezini, “What should
developers be aware of? an empirical study on the directives of
api documentation,” Empirical Software Engineering, vol. 17, pp.
703–737, 2011.

[15] Y. Kim, “Convolutional neural networks for sentence classiﬁca-

tion,” in EMNLP, 2014.

[16] D. Ye, Z. Xing, C. Y. Foo, J. Li, and N. Kapre, “Learning to extract
api mentions from informal natural language discussions,” 2016
IEEE International Conference on Software Maintenance and Evolution
(ICSME), pp. 389–399, 2016.

[17] R. Pawlak, M. Martin, N. Petitprez, C. Noguera, and L. Seinturier,
“Spoon: A library for implementing analyses and transformations
of java source code,” Software: Practice and Experience, vol. 46, pp.
1155 – 1179, 2016.

[18] Q. Huang, X. Xia, Z. Xing, D. Lo, and X. Wang, “Api method
recommendation without worrying about the task-api knowledge
gap,” 2018 33rd IEEE/ACM International Conference on Automated
Software Engineering (ASE), pp. 293–304, 2018.

[19] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Dis-
tributed representations of words and phrases and their composi-
tionality,” in NIPS, 2013.

[20] S. Yitagesu, X. Zhang, Z. Feng, X. Li, and Z. Xing, “Automatic
part-of-speech tagging for security vulnerability descriptions,”
2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp. 29–40, 2021.

[21] W. Bl, “The generalization of ‘student’s’ problem when several
different population varlances are involved,” Biometrika, vol. 34,
pp. 28–35, 1947.

[22] H. Zhong, L. Zhang, T. Xie, and H. Mei, “Inferring resource
speciﬁcations from natural language api documentation,” 2009
IEEE/ACM International Conference on Automated Software Engineer-
ing, pp. 307–318, 2009.

[23] L. Tan, D. Yuan, G. Krishna, and Y. Zhou, “/*icomment: bugs or

bad comments?*/,” in SOSP, 2007.

[24] Y. Zhou, R. Gu, T. Chen, Z. Huang, S. Panichella, and H. Gall,
“Analyzing apis documentation and code to detect directive de-
fects,” 2017 IEEE/ACM 39th International Conference on Software
Engineering (ICSE), pp. 27–37, 2017.

[25] K. Srinivas,

I. Abdelaziz,

J. T. Dolby, and J. McCusker,
“Graph4code: A machine interpretable knowledge graph for
code,” ArXiv, vol. abs/2002.09440, 2020.

[26] L. Moreno, G. Bavota, M. D. Penta, R. Oliveto, and A. Marcus,
“How can i use this method?” 2015 IEEE/ACM 37th IEEE Interna-
tional Conference on Software Engineering, vol. 1, pp. 880–890, 2015.
[27] S. Subramanian, L. Inozemtseva, and R. Holmes, “Live api doc-
umentation,” Proceedings of the 36th International Conference on
Software Engineering, 2014.

[28] G. Petrosyan, M. Robillard, and R. Mori, “Discovering information
explaining api types using text classiﬁcation,” 2015 IEEE/ACM
37th IEEE International Conference on Software Engineering, vol. 1,
pp. 869–879, 2015.

[29] C. Treude and M. Robillard, “Augmenting api documentation
with insights from stack overﬂow,” 2016 IEEE/ACM 38th Interna-
tional Conference on Software Engineering (ICSE), pp. 392–403, 2016.
[30] R. Abdalkareem, E. Shihab, and J. Rilling, “What do developers
use the crowd for? a study using stack overﬂow,” IEEE Software,
vol. 34, pp. 53–60, 2017.

[31] L. Ponzanelli, G. Bavota, A. Mocci, M. D. Penta, R. Oliveto,
B. Russo, S. Haiduc, and M. Lanza, “Codetube: Extracting rele-
vant fragments from software development video tutorials,” 2016

14

IEEE/ACM 38th International Conference on Software Engineering
Companion (ICSE-C), pp. 645–648, 2016.

QING HUANG received the M.S degree
in computer application and technology
from Nanchang University, in 2009, and
the PH.D. degree in computer software
and theory from Wuhan University, in
2018. He is currently an Assistant Pro-
fessor with the School of Computer and
Information Engineering, Jiangxi Normal University, China.
His research interests include information security, software
engineering and knowledge graph.

Zhiqiang Yuan is a second-year gradu-
ate student at the School of Computer
Jiangxi
and Information Engineering,
Normal University, China. His research
interests include software engineering
and knowledge graph.

Zhenchang Xing is an Associate Profes-
sor in the Research School of Computer
Science, Australian National University.
Previously, he was an Assistant Profes-
sor in the School of Computer Science
and Engineering, Nanyang Technolog-
ical University, Singapore, from 2012-
2016. His main research areas are software engineering,
applied data analytics, and human-computer interaction.

ZHENGKANG ZUO received the Ph.D.
degree in computer science and tech-
nology from the Chinese Academy of
Sciences (CAS), Beijing, China. He is cur-
rently a professor and deputy director
of the Computer Science and Technol-
ogy Department of Jiangxi Normal University, Nanchang,
China. His main research interests include software formal
methods, generic programming, etc.

Changjing Wang received the bachelor’s
and master’s degrees from Jiangxi Nor-
mal University, China, in 1999 and 2004,
respectively. He received the PhD de-
gree from Institute of Software, Chinese
Academy of Science, China, in 2012. He
is currently a professor with the College
of Computer Information and Engineering, Jiangxi Normal
University, China. His research interests include Web service
and formal method.

Xin Xia is the director of
the Soft-
ware Engineering Application Technol-
ogy Lab at Huawei, China. He received
the ACM SIGSOFT Early Career Re-
searcher Award in 2022. His current re-
search focuses on data science for soft-
ware engineering, i.e., mining and ana-
lyzing rich data in software repositories to uncover interest-
ing and actionable information.

