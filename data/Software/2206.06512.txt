2
2
0
2

n
u
J

3
1

]

A
N
.
h
t
a
m

[

1
v
2
1
5
6
0
.
6
0
2
2
:
v
i
X
r
a

Algorithms for Parallel Generic hp-adaptive Finite Element
Softwareâˆ—

MARC FEHLING, Colorado State University, USA
WOLFGANG BANGERTH, Colorado State University, USA

The hp-adaptive finite element method (FEM) â€“ where one independently chooses the mesh size (h) and
polynomial degree (p) to be used on each cell â€“ has long been known to have better theoretical convergence
properties than either h- or p-adaptive methods alone. However, it is not widely used, owing at least in parts to
the difficulty of the underlying algorithms and the lack of widely usable implementations. This is particularly
true when used with continuous finite elements.

Herein, we discuss algorithms that are necessary for a comprehensive and generic implementation of
hp-adaptive finite element methods on distributed-memory, parallel machines. In particular, we will present
a multi-stage algorithm for the unique enumeration of degrees of freedom (DoFs) suitable for continuous
finite element spaces, describe considerations for weighted load balancing, and discuss the transfer of variable
size data between processes. We illustrate the performance of our algorithms with numerical examples, and
demonstrate that they scale reasonably up to at least 16 384 Message Passing Interface (MPI) processes.
We provide a reference implementation of our algorithms as part of the open-source library deal.II.
CCS Concepts: â€¢ Mathematics of computing â†’ Computations in finite fields; Mathematical software.
Additional Key Words and Phrases: Parallel algorithms, hp-adaptivity, finite element methods, high perfor-
mance computing

1 INTRODUCTION
In the hp-adaptive variation of the finite element method (FEM) for the solution of partial differential
equations, one adaptively refines the mesh (h-adaptivity) and independently also chooses the
polynomial degree of the approximation on every cell (p-adaptivity). This method is by now 40
years old [BabuÅ¡ka and Dorr 1981] and, at least from a theoretical perspective, well understood
[BabuÅ¡ka and Guo 1996; Guo and BabuÅ¡ka 1986a,b]. In particular, it is known that hp-adaptivity
provides better accuracy per degree of freedom (DoF) than either the h- or p-adaptive methods
alone; more specifically, it exhibits a convergence rate where the approximation error in many
cases decreases exponentially with the number of unknowns ğ‘ â€“ i.e., the error satisfies ğ‘’ = O (ğ‘ âˆ’ğ‘ )
for some ğ‘  > 1 that may depend on the solution â€“, rather than an algebraic rate ğ‘’ = O (ğ‘ âˆ’ğ›¾ ) for
some ğ›¾ > 0. In other words, hp-adaptivity is asymptotically superior to h- or p-adaptivity alone.

Yet, hp-adaptive methods are not widely used. The reasons for this lack of use are probably
debatable but surely include (i) that the literature provides many criteria by which to choose
whether h- or p-refinement should be selected if the error on a cell is large, but that there is no
consensus on which one is best; and (ii) a lack of widely usable implementations. For the first of
these points, we refer to the comprehensive comparison in [Mitchell and McClain 2014]. Instead, in
this contribution, we address the second point: the lack of widely available implementations.

A survey of the finite element landscape shows that there are few options for those who are
interested in experimenting with hp-methods. Most of the open-source distributed-memory parallel
implementations of hp-adaptive methods available that we are aware of â€“ specifically the ones in
the libraries PHAML [Mitchell 2002], PHG [Zhang 2019], and MoFEM [Kaczmarczyk et al. 2020] â€“ have

âˆ—Dedicated to the memory of William F. Mitchell.

Authorsâ€™ addresses: Marc Fehling, Department of Mathematics, Colorado State University, 1874 Campus Delivery, Fort
Collins, CO, 80523-1874, USA, marc.fehling@colostate.edu; Wolfgang Bangerth, Department of Mathematics and Department
of Geosciences, Colorado State University, 1874 Campus Delivery, Fort Collins, CO, 80523-1874, USA, bangerth@colostate.
edu.

 
 
 
 
 
 
2

M. Fehling and W. Bangerth

not found wide use in the community and are not backed by large user and developer communities.
To the best of our knowledge, other popular libraries like FEniCS/FEniCSx [AlnÃ¦s et al. 2015],
GetFEM [Renard and Poulios 2020], and FreeFEM++ [Hecht 2012] do not offer hp-adaptive methods
at all or have only experimental support as is the case with libMesh [Kirk et al. 2006].

In other cases, such as the ones discussed in [Bey et al. 1996; Chalmers et al. 2019; Jomo et al. 2017;
PaszyÅ„ski and Demkowicz 2006; PaszyÅ„ski and Pardo 2011], the implementation of hp-methods is
restricted to discontinuous Galerkin (DG) methods; the same limitation also applies to the libraries
MFEM [Anderson et al. 2021; Pazner and Kolev 2022] and DUNE [Bastian et al. 2021; Gersbacher 2016].
This case is relatively easy to implement because the construction of finite element spaces is purely
local, on every cell independent of its neighbors. At the same time, DG methods are expensive â€“
especially in three dimensions â€“ because DoFs are duplicated between neighboring cells, and the
resulting large linear systems and corresponding memory consumption have hampered adoption of
DG schemes in most applications outside the simulation of hyperbolic systems. As a consequence,
while the use of DG methods for hp-adaptivity is a legitimate approach, there are many important
use cases where continuous finite element spaces remain the method of choice.

Finally, let us mention publications [Jomo et al. 2017; PaszyÅ„ski and Pardo 2011] that also
demonstrate the use of hp-adaptive methods, but only discuss implementations for shared-memory
machines. The Hermes library [Å olÃ­n et al. 2008] also falls into that category. [Laszloffy et al. 2000]
does present distributed-memory algorithms, but only shows scaling to 16 processors, whereas we
are here interested in much larger levels of parallelism. We are not aware of any commercial tools
capable of using hp-methods, either for sequential or parallel computations.

As a consequence of our search for available implementations, and to the best of our knowledge,
only the deal.II library [Arndt et al. 2021a,b] appears to have generic support for hp-adaptive
methods for a wide variety of finite elements, discontinuous or continuous, as previously discussed
in detail in [Bangerth and Kayser-Herold 2009]. Still, deal.II has only recently begun to support
hp-adaptive methods for parallel computations [Fehling 2020]. It is this specific gap that we wish
to address in this contribution, by considering what algorithms are necessary to implement hp-
methods on large parallel machines using a distributed-memory model based on the Message Passing
Interface (MPI). The target for our work is the solution of two- and three-dimensional scalar- or
vector-valued partial differential equations, using an arbitrary combination of finite elements, and
scaling up to tens of thousands of processes and billions of unknowns.

More specifically, our goals for this work are:

â€¢ The development of a scalable algorithm to uniquely enumerate DoFs on meshes on which
finite element spaces of different polynomial degrees may be associated with each cell. Simply
enumerating all DoFs on a mesh turns out to be non-trivial already in distributed-memory
implementations of h-adapted, unstructured meshes (as discussed in [Bangerth et al. 2012])
as well as for sequential implementations of the hp-method (see [Bangerth et al. 2007]), and
it is no surprise that the combination of the two leads to additional complications.

â€¢ An efficient distribution of workload among all processes with weighted load balancing, since
the workload per cell depends on its local number of DoFs and thus varies from cell to cell
with hp-adaptive methods. We will present strategies on how to determine weights on each
cell for this purpose.

â€¢ The ability to transfer data of variable size between hp-adapted meshes. In the hp-context,
the amount of data stored per cell is proportional to the number of local DoFs and, again,
varies between cells.

â€¢ An assessment of the parallel efficiency of the algorithms mentioned above.

Algorithms for Parallel Generic hp-adaptive Finite Element Software

3

In this paper, we will first address the task of enumerating all DoFs in a distributed-memory
setting in Section 2. We will then present strategies for weighted load balancing in Section 3 and
continue with ways to transfer data of variable size in Section 4. In Section 5, we then illustrate
the performance and scalability of our methods using numerical results obtained on the Expanse
supercomputer [Strande et al. 2021; Towns et al. 2014], using up to 16 384 cores. We present
conclusions in Section 6.

Code availability. The algorithms we will discuss in the remainder of this paper are mostly
implemented and available in the open-source library deal.II, version 9.3 [Arndt et al. 2021a,b];
some enhancements are only in the development branch and will be released with the next release
in the summer of 2022. All functionality is available under the LGPL 2.1 license. That said, our
discussions are not specific to deal.II and are generally applicable to any other finite element
software. In particular, even though we will only show examples of quadrilateral or hexahedral
meshes, our algorithms are readily applicable also to simplex or mixed meshes.

The two programs that implement the test cases of Section 5.1 and for which we show results in

Sections 5.2 and 5.3 are available as part of the tool hpbox [Fehling 2022].

2 ENUMERATION OF DEGREES OF FREEDOM
In the abstract, the finite element method defines a finite-dimensional space ğ‘‰â„ within which one
seeks the discrete solution of a (partial) differential equation. In practice, one needs to construct a
basis {ğœ‘ğ‘– }ğ‘ âˆ’1
for this space so that numerical solutions ğ‘¢â„ âˆˆ ğ‘‰â„ can be expressed as expansions of
ğ‘–=0
the form ğ‘¢â„ (x) = (cid:205)ğ‘– ğ‘ˆğ‘–ğœ‘ğ‘– (x) where the ğ‘ˆğ‘– are the nodal coefficients of the expansion.

The basis functions of ğ‘‰â„ are mathematically defined via nodal functionals [Brenner and Scott
2008], but for the purposes of this section, it is only important to know that each basis function
is associated with either a vertex, an edge, a face, or the interior of a cell of a mesh. In order to
enumerate the DoFs on an unstructured mesh, one therefore simply walks over all cells, faces,
edges, and vertices and, in a first step, allocates as much memory as is necessary to store the indices
of DoFs associated with each of these entities, setting the index to an invalid value. In a second
step, one then repeats the loop and assigns consecutive indices to each degree encountered that
has not yet received a valid number. It is clear, however, that this two-stage algorithm needs to be
modified for parallel, distributed-memory computations.

In the remainder of this section, our goal is to describe an algorithm that achieves this enumeration
in parallel for the hp-adaptive case. For context, let us first briefly outline how this is done for
distributed, unstructured meshes when only one type of finite element is used (Section 2.1), followed
by a description of the algorithm used for hp-adaptive methods on a single process (Section 2.2). In
Section 2.3, we then present our new algorithm for parallel hp-adaptive methods, which can be
seen as a combination and enhancement of the former two.

We do not cover details on handling hanging nodes and constraints in this manuscript. It turns
out that for the new algorithm, their handling does not require any change from the methods
described in [Bangerth et al. 2012; Bangerth and Kayser-Herold 2009].

2.1 Enumerating degrees of freedom on distributed, unstructured meshes

In a parallel program where the mesh data structure is stored in distributed memory, the situation
is complicated by the fact that each process only knows a subset of cells â€“ namely, those cells that
are â€œlocally ownedâ€ along with a layer of â€œghost cellsâ€. At the same time, we need to assign globally
unique indices to all entities of the distributed mesh: at the end of the algorithm, each process must
know the global indices of those DoFs that are located on this processâ€™s locally owned and ghost
cells.

4

M. Fehling and W. Bangerth

For the relatively simple case where the finite element is the same on each cell (no p-adaptivity),
the index assignment is typically achieved by identifying a tie-breaking process that defines which
process â€œownsâ€ a mesh entity on the interface between the sub-domains of cells owned by individual
processes (i.e., which of the adjacent processes owns a vertex, an edge, or a face on this interface).
This process is then also the owner of the DoFs located on these entities. A possible tie-breaker is
that the process with the smallest MPI rank is chosen as the owner of an entity on a subdomain
interface.

Enumeration of DoFs then proceeds by each process enumerating the DoFs it owns, starting at
zero. All of these indices are then shifted so that we obtain globally unique indices across processes.
Next, each process sends the indices associated with locally owned cells to those processes that have
these cells as ghost cells. Because processes may not yet know all DoF indices on the boundaries
of locally owned cells at the time of this communication step, the exchange has to be repeated a
second time to ensure that each process knows the full set of indices on both the locally owned
cells as well as ghost cells, and all of the vertices, edges, and faces bounding these cells.

A formal description of this algorithm â€“ which consists of five stages â€“ has been given in
[Bangerth et al. 2012] and forms the basis of the discussions for the parallel hp-case below in
Section 2.3.

2.2 Enumerating degrees of freedom in the sequential hp-context
In the hp-context, each cell ğ¾ âˆˆ T of a triangulation or mesh T may use a different finite element.
To make the notation that we use below concrete, let us assume that we want the global function
space ğ‘‰â„ be constructed so that the solution functions ğ‘¢â„ âˆˆ ğ‘‰â„ satisfy ğ‘¢â„ |ğ¾ âˆˆ ğ‘‰â„ (ğ¾) where ğ‘‰â„ (ğ¾) is
the finite element space associated with cell ğ¾. Furthermore, let us assume that ğ‘‰â„ (ğ¾) can only
(cid:9)ğ¼
defined on the reference cell Ë†ğ¾ that are then
be one from within a collection of spaces (cid:8) Ë†ğ‘‰ (ğ‘–)
ğ‘–=0
Ë†ğ‘‰â„ where Mğ¾ is the operator that maps the
mapped to cell ğ¾ in the usual way, i.e., ğ‘‰â„ (ğ¾) = Mğ¾
finite element space from the reference cell to ğ¾; the details of this mapping are not of importance
. Each of
to us here. We denote the â€œactive FE indexâ€ on cell ğ¾ by ğ‘(ğ¾), i.e., ğ‘‰â„ (ğ¾) = Mğ¾
the spaces Ë†ğ‘‰ (ğ‘–)

has a number of DoFs associated with each vertex, edge, face, and cell interior.

Ë†ğ‘‰ (ğ‘ (ğ¾))
â„

â„

A trivial implementation of enumerating all DoFs would simply loop over all cells ğ¾ âˆˆ T and
enumerate all DoFs on both the cell ğ¾ and its vertices, edges, and faces independently of the
enumeration on neighboring cells. To do so requires storing multiple sets of indices of DoFs on
vertices, edges, and faces, each set corresponding to one of the adjacent cells. This strategy would
result in a global finite element space that is discontinuous between neighboring cells, but continuity
can be restored by adding constraints that relate DoFs on neighboring cells.

â„

8ğ‘ˆ3 + 3

8ğ‘ˆ1 âˆ’ 1

The left panel of Fig. 1 illustrates this approach. Here, each cellâ€™s DoFs are independently
enumerated. Continuity of the solution is then restored by introducing identity constraints of the
form ğ‘ˆ9 = ğ‘ˆ1, ğ‘ˆ11 = ğ‘ˆ3, ğ‘ˆ14 = ğ‘ˆ5, in addition to the more traditional â€œhanging node constraintsâ€
ğ‘ˆ13 = 3

4ğ‘ˆ5, ğ‘ˆ15 = âˆ’ 1

While conceptually simple, this approach is wasteful as it introduces many more DoFs than
necessary, along with a large number of constraints. In the extreme case of using ğ‘„1 (tri-linear)
Lagrange elements on all cells of a uniformly refined 3d mesh, one ends up with approximately
eight times as many DoFs, 7/8 of which are constrained. In actual test cases using hp-adaptivity,
[Bangerth and Kayser-Herold 2009, Sec. 4.2] report that these â€œunnecessaryâ€ DoFs can be up to
15 % of the total number of DoFs in 3d.

8ğ‘ˆ3 + 3

8ğ‘ˆ1 + 3

4ğ‘ˆ5.

To avoid this wastefulness, the algorithms described in [Bangerth and Kayser-Herold 2009]
â€œunifyâ€ DoFs where possible during the enumeration phase. For example, in the case shown in
Fig. 1, the DoFs on shared vertices can be unified for the particular choice of elements adjacent

Algorithms for Parallel Generic hp-adaptive Finite Element Software

5

2

4

0

7

8

6

3

11
15

5

14

13
9

1

22

31

28

25
19

23

32

29

26
20

24

33

30

27
21

12
18

17

16
10

2

4

0

7

8

6

3

3
12

5

5

11
1

1

19

28

25

22
16

20

29

26

23
17

21

30

27

24
18

10
15

14

13
9

Fig. 1. Enumeration of DoF indices on a mesh with two cells on which the left cell uses a ğ‘„2 (bi-quadratic)
Lagrange element and the right cell uses a ğ‘„4 (bi-quartic) element. We distinguish between support points on
vertices (â€¢), lines (â–¡) and quadrilaterals (â—¦). Left: Naive enumeration of DoFs. Continuity is ensured through
constraints. Right: A better way in which we â€œunifyâ€ some DoFs.

8ğ‘ˆ3 + 3

8ğ‘ˆ1 + 3

8ğ‘ˆ3 + 3

8ğ‘ˆ1 âˆ’ 1

to these vertices, as can be the DoF located on the common edgeâ€™s midpoint. This leads to the
enumeration shown on the right side of the figure, for which we then only need to add constraints
ğ‘ˆ11 = 3

4ğ‘ˆ5, ğ‘ˆ12 = âˆ’ 1

At the same time, it is clear that this â€œunificationâ€ step requires knowing about the global indices
of DoFs on neighboring cells during enumeration, and this presents issues that need to be addressed
in the parallel context if one of the cells adjacent to a vertex, edge, or face is a ghost cell. Furthermore,
each process must know the active FE index not only for its locally owned cells, but also for ghost
cells, before the enumeration can begin. We have to take into account all of these considerations in
the extension of the algorithms of [Bangerth and Kayser-Herold 2009] to the parallel context in the
next section.

4ğ‘ˆ5.

2.3 The parallel hp-case
Having discussed the fundamental algorithms necessary to globally enumerate DoFs in the context
of both parallel unstructured meshes, and for the sequential hp-case, let us now turn to an algorithm
that combines both of these features. As we will see, this algorithm turns out to be non-trivial.

2.3.1 Goals for the parallel algorithm. In developing such an enumeration algorithm, we are
guided by the desire to come up with an enumeration that leads to a total number of DoFs that is
independent of the number of processes. In other words, we do not want to treat vertices, edges,
or faces that happen to lie on subdomain boundaries any different than if they were within the
interior of a subdomain. We consider this an important feature to achieve scalable and predictable
algorithms, and because it makes debugging problems easier. Furthermore, we would like to develop
an algorithm that includes the â€œunification stepâ€ mentioned above to avoid generating too many
trivial constraints.

At the end of the algorithm, each parallel process needs to know the globally unique indices of
all DoFs located on the locally owned cells as well as on ghost cells, including the outer vertices,
edges, and faces of ghost cells beyond which the current process has no knowledge of whether and
how the mesh continues.

Finally, we want this algorithm to have linear complexity in the number of cells or the number
of DoFs. We achieve this by stating it as a fixed-length series of loops over all cells owned by each
process and, if necessary, over all ghost cells on this process. Because each process only loops over
its own cells, and because the number of DoFs per process is balanced (see also Section 3), we
obtain an algorithm that we expect to scale optimally both strongly and weakly.

The algorithm that achieves all of this â€“ see the discussion below â€“ can be broken down into
seven distinct stages. In addition to their description, we illustrate each stage in an example for

6

M. Fehling and W. Bangerth

which we consider a two-dimensional mesh of four neighboring cells meeting at a central vertex.
On this mesh, we use bi-quadratic (ğ‘„2) Lagrange elements with 9 unknowns on the bottom left
and top right cell, and bi-quartic (ğ‘„4) elements with 25 unknowns on the remaining two cells.
Furthermore, we assume that the partitioning algorithm has divided the mesh into two subdomains:
subdomain zero contains the bottom two cells, subdomain one the top two. This setup is shown in
Fig. 2, where we illustrate the progress of the enumeration algorithm. Each figure shows the view
from process zero on the left, and from process one on the right.

2.3.2 Algorithm inputs. The algorithm we describe in the following needs the following pieces of
information as inputs:

â€¢ A set of cells ğ¾ that constitute the locally owned and ghost cells, and information how
neighboring cells are connected. The algorithm does not need to know where these cells
are geometrically located in an ambient space â€“ although this is of course important for the
downstream application of the finite element method â€“ but only the topological connection
of vertices, edges, and faces to the cells of which they are part of.

â€¢ A process must know to which process each of its ghost cells belongs. Since we identify
subdomain ids with process ranks in a MPI universe, that means that we need to store the
owning processâ€™s subdomain, or short owner, of all ghost cells.

â€¢ Each cell on every process has an associated global identifier. This identifier is the same on
all processes that store this cell, whether as part of their locally owned cells or as a ghost cell.
â€¢ For each locally owned or ghost cell, every process must know the active FE index â€“ that is,
which element Ë†ğ‘‰ ğ‘ (ğ¾)
is in use on each cell ğ¾. Because the active FE index is typically computed
only on each processâ€™s locally owned cells, this information needs to be exchanged between
processes before the start of the algorithm; as with any ghost exchange of information, this
can efficiently be done through point-to-point communication.

â„

â€¢ For each element in the collection (cid:8) Ë†ğ‘‰ (ğ‘–)
â„

, the algorithm needs to know how many DoFs
this element has per vertex, edge, face, or cell interior. For example, for the ğ‘„4 Lagrange
element in 2d that we use in our illustrative example, there is one DoF per vertex, three per
edge, and nine per cell interior.

(cid:9)ğ¼
ğ‘–=1

â€¢ For each pair of elements, the corresponding finite element implementations need to be able
to identify whether two DoFs located on the same entity (vertex, edge, or face) can be unified.
For example, our algorithm needs to be able to ask the combination of the ğ‘„2 and ğ‘„4 elements
in the example shown in Fig. 1 whether the single DoF each wants to store on a shared vertex
can receive a single index, and whether the one ğ‘„2 DoF on the shared edge can be unified
with one of the three DoFs the ğ‘„4 element wants to allocate on the common edge. The details
of how an answer to such a query can be implemented are not relevant to our description
here, but are discussed at length in [Bangerth and Kayser-Herold 2009].

2.3.3 Description of the algorithm. The algorithm we show consists of seven stages (plus an initial
memory allocation and initialization stage), as detailed below. To make understanding it easier, we
illustrate each step in Fig. 2 for our model test case; note that the figure is continued over several
pages. In our description, we follow the same nomenclature as [Bangerth et al. 2012]. Specifically,
we generally use an index ğ‘ or ğ‘ for subdomains (identified with process ranks in a MPI universe),
and we denote the set of all locally owned cells on process ğ‘ by Tğ‘
, the set of all ghost cells by
loc
Tğ‘
. In the description below, the
ghost
process index ğ‘ is generically used to identify the â€œcurrentâ€ process, i.e., the rank of the process
that is executing the algorithm.

, and the set of all locally relevant cells by Tğ‘

loc âˆª Tğ‘

rel = Tğ‘

ghost

Our algorithm then proceeds in the following steps:

Algorithms for Parallel Generic hp-adaptive Finite Element Software

7

i

i

i

i
i

i

i

i

i
i

i
i

i

i
i
2

4

0

i
i

i

i
i
2

4

0

i

i

i

i
i
7

8

6

i

i

i

i
i
7

8

6

i

i

i

i
i

i

i

i

i
i

i
i

i

i

3

i

i

11
15

5

14

1

i
i

i

i

3

13
9

i

i

11
15

5

14

13
9

1

22

31

28

25
19

22

31

28

25
19

i

i

i
23

32

29

26
20

i

i

i
23

32

29

26
20

i

i

i
12
18

17

16
10

24

33

30

27
21

13

22

19

16
10

2
6

5

4
0
i

i

i

(Stage 1) Local enumeration.

i

i

i
12
18

17

16
10

24

33

30

27
21

13

22

19

16
10

2
6

5

4
0
i

i

i

14

23

20

17
11
i

i

i

14

23

20

17
11
i

i

i

15

24

21

18
12

15

24

21

18
12

3
9

8

7
1

i

i

3
9

8

7
i

i

i

27

29

25

i

i

i
i

27

29

i

i

i

i
i

i

i

i

i
i

i

i

i

i
i

32

33

31
i

i

i

i
i

32

33

31
i

i

i

i
i

28

30

26
i
i

i

i
i

28

30

26
i
i

i

i
i

i

i

i

i
i

i

i

i

i
i

(Stage 2) Tie-break.

Fig. 2. Exemplary application of our enumeration algorithm for DoFs. Changes made at each step are highlighted.
The left diagram of each subfigure depicts the situation for process 0, whereas the right side shows the domain
from the perspective of process 1. The top half of each subfigure constitutes subdomain 1, while the bottom cells
are assigned to subdomain 0.

(0) Initialization (without illustration). Loop over all locally relevant cells ğ¾ âˆˆ Tğ‘

, and on each
of its vertices, edges, faces, and ğ¾ itself allocate enough space to store as many DoF indices
as are necessary for the element identified by the active FE index ğ‘(ğ¾). If a neighboring
element has already allocated space for the same active FE index, then no additional space is
necessary. In other words, for each entity within Tğ‘
, we need to allocate space for a map
rel
from the active FE indices of adjacent cells to an array of indices of DoFs indices.
Once space is allocated, all DoF indices are set to an invalid value that we denote by ğ‘– in the
following (for example ğ‘– (cid:66) âˆ’1).

rel

8

M. Fehling and W. Bangerth

i

i

i

i
i

i

i

i

i
i

i
i

i

i
i
2

4

0

i
i

i

i
i
2

4

0

i

i

i

i
i
7

8

6

i

i

i

i
i
7

8

6

i

i

i

i
i

i

i

i

i
i

i
i

i

i

3

i

i

3
15

5

5

1

i
i

i

i

3

13
1

i

i

3
11

5

5

10
1

1

22

31

28

25
19

18

26

23

20
15

i

i

i
i

32

29

26
20

i

i

i
i

27

24

21
16

i

i

i
i
18

17

16
10

24

33

30

27
21

13

22

19

16
10

2
6

5

4
i
i

i

i

(Stage 3) Unification.

i

i

i
i
14

13

12
9

19

28

25

22
17

37

46

43

40
35

29
32

31

30
i
i

i

i

14

23

20

17
i
i

i

i

38

47

44

41
i
i

i

i

15

24

21

18
12

39

48

45

42
36

27

32

27
9

29

29

7
i

i

i

i

i

i

i
i

i

i

i

i
i

33

31
i

i

i

i
i

50

55

50
34

52

52

33
i

i

i

i

i

i

i
i

i

i

i

i
i

56

54
i

i

i

i
i

28

30

26
i
i

i

i
i

51

53

49
i
i

i

i
i

i

i

i

i
i

i

i

i

i
i

(Stage 4) Global re-enumeration.

Fig. 2. (continued) Exemplary application of our enumeration algorithm for DoFs.

loc

(1) Local enumeration. Iterate over all locally owned cells ğ¾ âˆˆ Tğ‘

. For each of the vertices, edges,
faces, and the cell interior, assign valid DoF indices in ascending order, starting from zero, if
indices have not already been assigned for an entity and the current ğ‘(ğ¾).

(2) Tie-break. Iterate over all locally owned cells ğ¾ âˆˆ Tğ‘

. If a vertex, edge, or face that is part of
ğ¾ is also part of an adjacent ghost cell ğ¾ â€²
ghost belongs
ghost), and if ğ¾ â€²
to a subdomain of lower rank ğ‘ < ğ‘, then invalidate all DoFs on this mesh entity by setting
their index to the invalid value ğ‘–.

loc
ghost so that ğ‘(ğ¾) = ğ‘(ğ¾ â€²

(3) Unification. Iterate over all locally owned cells ğ¾ âˆˆ Tğ‘

. For all shared DoFs on vertices, edges,
and faces to neighboring cells ğ¾ â€² (locally owned or ghost), ask the elements corresponding to
active FE indices ğ‘(ğ¾) and ğ‘(ğ¾ â€²) whether some of the DoFs can be unified between the two
elements. If ğ¾ â€² is also a locally owned cell, perform the unification by replacing one index (or

loc

Algorithms for Parallel Generic hp-adaptive Finite Element Software

9

39

48

45

42
36

39

48

45

42
36

37

46

43

40
35

37

46

43

40
35

29
32

31

30
i
2

4

0

29
32

31

30
2
2

4

0

38

47

44

41
i
7

8

6

38

47

44

41
7
7

8

6

50

55

50
34

52

52

33

3

3
11

5

5

10
1

1

18

26

23

20
15

56

54
i

27

24

21
16

51

53

49
i
14

13

12
9

19

28

25

22
17

37

46

43

40
35

29
32

31

30
i
2

4

0

(Stage 5) Ghost exchange.

50

55

50
34

52

52

33

3

3
11

5

5

10
1

1

18

26

23

20
15

56

54
54

27

24

21
16

51

53

49
49
14

13

12
9

19

28

25

22
17

37

46

43

40
35

29
32

31

30
2
2

4

0

38

47

44

41
i
7

8

6

38

47

44

41
7
7

8

6

39

48

45

42
36

39

48

45

42
36

50

55

50
34

52

52

33
3

3

11

5

5

10
1

1

18

26

23

20
15

56

54
i

27

24

21
16

50

55

50
34

52

52

33
3

3

11

5

5

10
1

1

18

26

23

20
15

56

54
54

27

24

21
16

51

53

49
i
14

13

12
9

51

53

49
49
14

13

12
9

19

28

25

22
17

19

28

25

22
17

(Stage 6) Merge on interfaces.

Fig. 2. (continued) Exemplary application of our enumeration algorithm for DoFs.

a set of indices) by the corresponding index of the other DoF to which it is unified. If ğ¾ â€² is a
ghost cell, and if the DoF on ğ¾ needs to be unified with the corresponding one on ğ¾ â€² (rather
than the other way around), then set the index of the DoF on ğ¾ to the invalid value ğ‘–.

At this point in the algorithm, each process knows which DoFs are owned by this process â€“
namely, the ones on locally owned cells that are enumerated as anything other than ğ‘– â€“ although
the final indices of these DoFs are not yet known.

(4) Global re-enumeration. Iterate over all locally owned cells ğ¾ âˆˆ Tğ‘

and re-enumerate those
DoF indices in ascending order that have a valid value assigned, ignoring all invalid indices.
Store the total number of all valid DoF indices on this subdomain as ğ‘›ğ‘ . In a next step, shift
all indices by the number of DoFs that are owned by all processes of lower rank ğ‘ < ğ‘, or in

loc

10

M. Fehling and W. Bangerth

other words, by (cid:205)ğ‘âˆ’1
and can be obtained via MPI_Exscan [Message Passing Interface Forum 2021].

ğ‘=0 ğ‘›ğ‘. Computing this shift corresponds to a prefix sum or exclusive scan,

At this stage, each process has (consecutively) enumerated a certain subset of DoFs, and we call
these the â€œlocally owned DoFsâ€. In later use, each process then owns the corresponding rows of
matrices and entries in vectors, but the concept of locally owned DoFs is otherwise of no importance
to the remainder of the algorithm. Importantly, however, we still need to ensure that each process
learns of the remaining DoFs that are located on locally owned or ghost cells and whose indices
are not currently known.

(5) Ghost exchange. In this step, we need to send sets of indices from those locally owned cells
that are ghost cells on other processes, to those processes on which they are ghost

ğ¾ âˆˆ Tğ‘
loc
cells. We do this in the following steps:
a. For each process ğ‘ â‰  ğ‘ that is adjacent to ğ‘, allocate a map with keys corresponding to
global cell identifiers and values equal to a list of indices of those DoFs defined on this cell.
. If ğ¾ is a ghost cell on process ğ‘, then add the global identifier of ğ¾

b. Iterate over all ğ¾ âˆˆ Tğ‘
loc

and the list of DoFs on ğ¾ to the map for process ğ‘.

c. Send all of the maps to their designed process ğ‘ via nonblocking point-to-point communi-

cation (e.g., using MPI_Isend [Message Passing Interface Forum 2021]).

d. Receive data containers from processes of adjacent subdomains ğ‘ via nonblocking point-
to-point communication (e.g., using MPI_Irecv [Message Passing Interface Forum 2021]).
The data so received corresponds to the DoF indices on all ghost cells of this subdomain ğ‘.
On each of these cells, set the received DoF indices accordingly.

All communication in this step is symmetric, which means that a process only receives
data from another process when it also sends data to it. Thus, there is no need to negotiate
communication.

After this ghost exchange, each DoF on an interface between a locally owned and a ghost cell has
exactly one valid index assigned.

(6) Merge on interfaces. Iterate over all locally relevant cells ğ¾ âˆˆ Tğ‘

. On interfaces between
locally owned and ghost cells, set all remaining invalid DoF indices to the corresponding
valid one.

rel

At this stage, all processes know the correct indices for all DoFs located on locally owned cells.
However, during the ghost exchange in stage (5) above, some processes may have sent index sets
for some cells that may still contain the invalid index ğ‘– and not all of these can be resolved through
unification with locally known indices in stage (6). This is not illustrated in the figures but would
require a larger example mesh; the source of these ğ‘– markers are if a ghost cell owned by process ğ‘
does not only border a cell owned by process ğ‘, but also a cell owned by yet another process ğ‘â€²
that is not a neighbor of ğ‘, and process ğ‘ will only learn about indices on this cell as part of the
ghost exchange with ğ‘â€² itself. As a consequence, we have to repeat stage (5) one more time:

(7) Ghost exchange (without illustration). Repeat the steps of stage (5). However this time, only
data from those cells have to be communicated which had invalid DoF indices prior to stage
(5d).

At the end of this algorithm, all global DoF indices have been set correctly, and every process
knows the indices of DoFs located on locally owned and ghost cells. These are the DoFs that in
[Bangerth et al. 2012] are called the â€œlocally relevant DoFsâ€. Interestingly, while the algorithm is
substantially more complicated than the one without p-adaptivity discussed in [Bangerth et al.
2012], no additional communication steps are necessary.

Algorithms for Parallel Generic hp-adaptive Finite Element Software

11

Remark 1. In three-dimensional scenarios, [Bangerth and Kayser-Herold 2009, Sec. 4.6] points out
possible complications with circular constraints during DoF unification whenever three or more different
finite elements share a common edge. We have not found other satisfactory solutions for this problem
in the intervening 13 years, and consequently continue to implement the suggestion in [Bangerth and
Kayser-Herold 2009]: all DoFs on such edges are excluded from the unification step and will be treated
separately via constraints. Since the decision to use or not use the unification algorithm on these edges
is independent of whether the adjacent cells are on the same or different processes, this decision has
no bearing on our overall goals of ensuring that the number of used indices be independent of the
partitioning of the mesh. In the examples presented in Section 5, the fraction of identity constraints
stays below 3 %.

Remark 2. During stage (3) of our example in Fig. 2, we follow the DoF unification procedure as
described in [Bangerth and Kayser-Herold 2009]: if different finite elements meet on a subdomain
interface, all shared DoFs will be assigned to the finite element representing the common function
space (that is, when using elements within the same family, the one with the lower polynomial degree).
Of course, different decisions are possible, which might have an impact on parallel performance. For
example, [Bangerth et al. 2012, Remark 2] pointed out that on a face, all DoFs should belong to the same
subdomain to speed up parallel matrix-vector multiplications. We implemented such an enumeration
algorithm as an alternative to the one presented here. For the Laplace example used for the weighted
load balancing experiments described in Section 5.2, we found that both implementations take the
same run time (< 1 % deviation).

3 LOAD BALANCING

In order to enable our algorithms to scale well, we need to ensure that each process does roughly
the same amount of work. In contrast to h-adaptively refined meshes, a major difficulty here is
that the workload per cell does not remain the same: different parts of the overall hp-adaptive
algorithm scale differently with the number ğ‘›DoFs of unknowns per cell â€“ for example, the cost of
enumerating DoFs on a cell is proportional to ğ‘›DoFs, whereas assembling cell-local contributions to
the global system costs O (ğ‘›3
DoFs) from coupling of DoFs and quadrature, unless one uses specific
features of the finite element basis functions. More importantly, how the cost of a linear solver or
algebraic multigrid implementation â€“ together the largest contribution to a programâ€™s run time â€“
scales with the polynomial degree or number of unknowns on a cell is quite difficult to estimate a
priori. As a consequence, when using different polynomial degrees on different cells, it is not easy
to derive theoretically what the computational cost of a cell is going to be, and consequently how
to weigh each cell.

[Oden et al. 1994; Patra and Oden 1995] investigates different decomposition and load balancing
strategies with various types of weights, which are closely tied to their hp-adaptive algorithm.
These studies use the number of DoFs as a natural choice for the weight of a cell, but we believe
that this does not reflect the computational effort accurately for the reasons pointed out above.

Herein, we use an empirical approach in which we assume that the relative cost ğ‘¤ of a cell ğ¾

can be expressed as

ğ‘¤ (ğ¾) = ğ‘›DoFs(ğ¾)ğ‘,

(1)

with some, a priori unknown, exponent ğ‘. During load balancing, we then weigh each cell with this
factor and seek to partition meshes so that the sums of weights of the cells in each partition are
roughly equal, using the algorithms provided by p4est [Burstedde et al. 2011, Sec. 3.3].

We experimentally determine the value for the exponent ğ‘ for which the overall run time of our
program is minimized, and show results to this end in Section 5.2. From the considerations above,

12

M. Fehling and W. Bangerth

one would expect that the minimum should be in the range 1 â‰¤ ğ‘ â‰¤ 3, and this indeed turns out to
be the case.

It is worth mentioning that the approach only minimizes the overall run time, but likely leaves
each individual operation sub-optimally load balanced. This imbalance is a common problem
when a program executes algorithms whose cell-local costs are not proportional (see, for example,
[GassmÃ¶ller et al. 2018]) and can only be solved by re-partitioning data structures between the
different phases of a program â€“ say, between matrix assembly and the actual solver phase. Exploring
this issue is beyond the scope of our study.

4 PACKING, UNPACKING, AND TRANSFERRING DATA

A frequent operation in finite element codes is the serialization of all information associated with
a cell into an array, and moving this data. Examples for where this operation is relevant are re-
partitioning a mesh among processes after refinement, and the generation of checkpoints for later
restart. In such cases, it is often convenient to write all information associated with the cells of one
process into contiguous buffers.

For h-adaptive meshes, this presents few challenges since the size of the data associated with
every cell is the same and, consequently, can be packed into buffers of fixed size per cell. On the
other hand, for hp-methods, different cells require different buffer sizes for efficiency, and creating
contiguous storage schemes for all data on each process requires a bit more thought. Thought is
also necessary when devising mechanisms to subsequently transfer this data to other processes.
In practice, we implement such schemes using a two-stage process: in a first stage, we assess
how much memory the data on each cell requires, and allocate a contiguous array that can hold
information from all cells. In this phase, we also build a second array that holds the offsets into the
first array at which the data from each cell starts. The storage scheme therefore resembles the way
sparse matrices are commonly stored in compressed row storage (CSR). In a second stage, we copy
the actual data from each cell into the respective part of the array.

For serialization, one can then write the two arrays in their entirety to disk. For re-partitioning,
parts of the arrays have to be sent to different processes based on which process will own a cell.
For this step, it is useful to sort the order in which cells are represented in the two arrays in such a
way that data destined for one target is stored as one contiguous part of the arrays. In this way, all
information to be sent to one process can be transferred with a single non-blocking point-to-point
send operation for each of the two arrays, without the need for further copy operations.

In the work we describe here, parallel mesh management is provided by the p4est library for

which the transfer of data of non-uniform sizes is described in [Burstedde 2020, Sec. 5.2].

5 NUMERICAL RESULTS

Ultimately, the algorithms we have presented in Sections 2, 3, and 4 are only useful if they can
be efficiently implemented. In this section, we assess our approaches using two test cases: a
two-dimensional Laplace problem, and a three-dimensional Stokes problem. We discuss these in
Section 5.1 below.

Based on these test problems, we first assess how one needs to choose load balancing weights
for each cell based on the polynomial degree of the finite element applied (Section 5.2). Using
the resulting load balancing strategy, we then discuss how our algorithms scale in Section 5.3; an
important question to discuss in this context will be how one would actually define and measure
â€œscalabilityâ€ in the context of hp-adaptive methods.

All of the results shown in this section have been obtained using codes that are variations of
tutorial programs of the deal.II library. All features discussed in this paper are implemented

Algorithms for Parallel Generic hp-adaptive Finite Element Software

13

in deal.II, see also [Arndt et al. 2021a,b]. All data were generated with the tool hpbox [Fehling
2022].

5.1 Test cases

We evaluate the performance of our algorithms using two test cases discussed below: a two-
dimensional Laplace equation posed on the L-shaped domain, and a three-dimensional Stokes
problem posed on a domain that resembles a forked (â€œYâ€-shaped) pipe. Both of these cases are
chosen because the domain induces corner singularities in the solution, resulting in parts of the
domain where either large cells with high-order elements or small cells with low-order elements
are best suited to approximate the exact solution. In other words, these cases mimic practical
situations that are well suited to hp-adaptive methods. Furthermore, being able to demonstrate
our algorithms on both a relatively simple, scalar two-dimensional problem and a much more
complex three-dimensional, coupled vector-valued problem illustrates the range and limitations of
our algorithms.

In each test case, we start from a coarse discretisation of the problem, solve it, and refine it
in multiple iterations to end up with a mesh tailored to the problem. For this purpose, we need
mechanisms to decide which cells we want to refine and how. We use an error estimator based on
[Kelly et al. 1983] to mark cells for general refinement. Further, we use a smoothness estimator
based on the decay of Legendre coefficients as described by [Eibner and Melenk 2007; Houston
and SÃ¼li 2005; Mavriplis 1994] to decide how we want to refine each cell. We employ fixed number
refinement for both h- and p-refinement, which means that the fraction of cells we are going to
refine is always the same. We state our choice of fractions in the descriptions below. The mesh is
repartitioned after each refinement iteration.

5.1.1 Test case 1: A Laplace problem on the L-shaped domain. Our first test case concerns the
solution of the Laplace problem with Dirichlet boundary conditions:

âˆ’ğ›¥ğ‘¢ (ğ‘¥) = 0 on ğ›º ,

ğ‘¢ (ğ‘¥) = ğ‘¢sol(ğ‘¥)

on ğœ•ğ›º,

(2)

where we choose ğ›º âŠ‚ R2 as the L-shaped domain, ğ›º = (âˆ’1, 1)2\[0, 1] Ã— [âˆ’1, 0]. It is well understood
that on such domains, the Laplace equation admits a singular solution; indeed, in polar coordinates
ğ‘Ÿ = âˆšï¸ğ‘¥ 2 + ğ‘¦2 > 0 and ğœƒ = arctan(ğ‘¦/ğ‘¥), the function

ğ‘¢sol(ğ‘¥) = ğ‘Ÿ ğ›¼ sin(ğ›¼ ğœƒ )
is a solution for an opening angle at the reentrant corner of ğœ‹/ğ›¼ with ğ›¼ âˆˆ (1/2, 1). For the L-shaped
domain, we have ğ›¼ = 2/3 and the corresponding solution is shown in Fig. 3. We impose ğ‘¢ = ğ‘¢sol as
the boundary condition on ğœ•ğ›º, and the resulting (exact) solution of the Laplace equation that we
seek to compute is then ğ‘¢ = ğ‘¢sol everywhere in ğ›º.

(3)

This solution is singular at the origin: with unit vectors ğ‘’ğ‘Ÿ = cos(ğœƒ )ğ‘’ğ‘¥ âˆ’ sin(ğœƒ )ğ‘’ğ‘¦ and ğ‘’ğœƒ =

sin(ğœƒ )ğ‘’ğ‘¥ + cos(ğœƒ )ğ‘’ğ‘¦, we find that

âˆ‡ğ‘¢sol(ğ‘¥) = ğ›¼ğ‘Ÿ ğ›¼âˆ’1 [sin(ğ›¼ ğœƒ )ğ‘’ğ‘Ÿ + cos(ğ›¼ ğœƒ )ğ‘’ğœƒ ] ,

(4)

and consequently lim
ğ‘Ÿ â†’0

âˆ¥âˆ‡ğ‘¢sol(ğ‘¥) âˆ¥2 = âˆ for our choice of ğ›¼.

The numerical solution of the Laplace equation on the L-shaped domain is a classical test case.
For example, [Mitchell and McClain 2014] presents several benchmarks for hp-adaptation for this
situation. A similar scenario is also used in the step-75 tutorial of the deal.II library [Fehling et al.
2021].

In our study, we choose Lagrange elements ğ‘„ğ‘˜ with polynomial degrees ğ‘˜ = 2, . . . , 7. We mark
30 % of cells for refinement and 3 % for coarsening, from which we pick 90 % to be p-adapted and

14

M. Fehling and W. Bangerth

)
ğ‘¦

,

ğ‘¥
(
l
o
s
ğ‘¢

1

0
âˆ’1

âˆ’0.5

0

ğ‘¥

0.5

1 âˆ’1

1

0

ğ‘¦

1.2

1

0.8

0.6

l
o
s
ğ‘¢

0.4

0.2

0

Fig. 3. The solution (3) of the Laplace problem (2) on the L-shaped domain.

10 % to be h-adapted. We choose to favor p-refinement since the only non-smooth part of the
solution is around the point singularity at the origin.

Fig. 4 shows a typical hp-mesh and its partitioning from a sequence of adaptive refinements. It
illustrates that the corner singularity requires h-adaptation resulting in small cells, whereas further
away from the corner, the solution is smoother and can be resolved on relatively coarse meshes
using high polynomial degrees. Far away from the origin, the estimated errors are low so that
large cells and low polynomial degrees are sufficient. The lobe pattern results from the anisotropic
resolution property of polynomials on quadrilaterals.

The numerical scheme we choose to solve this problem is based on Trilinos [Heroux et al.
2005] for parallel linear algebra, and uses the ML package [Gee et al. 2007] as an algebraic multigrid
(AMG) preconditioner inside a conjugate gradient iteration.

5.1.2 Test case 2: Flow through a Y-pipe. As a second test case, we consider the solution of the
Stokes equation describing slow flow,

âˆ’ğ›¥ğ‘¢ + âˆ‡ğ‘ = 0,
âˆ’âˆ‡ Â· ğ‘¢ = 0.

(5a)

(5b)

As domain, we choose a forked, â€œYâ€-shaped pipe, see Fig. 5. We impose no-slip boundary conditions
on the lateral surfaces (ğ‘¢ = 0), and model the inflow at one opening as a Poisseuille flow via Dirichlet
boundary conditions. The other two ends are modeled via zero-traction boundary conditions.
Velocity and pressure solutions are also shown in Fig. 5.

The â€œwelding seamsâ€ at which the three pipes meet are non-convex parts of the boundary,
again resulting in singular solutions where we expect that the gradient of the velocity ğ‘¢ becomes
infinite; the pressure is also singular at these locations. We chose this as the second test case
because it enables us to verify that enumerating DoFs, along with all of the other ingredients of
our hp-adaptive solution approach, are efficient and scale well also for three-dimensional problems
with the much more complex choice of finite element and solver techniques necessary to solve the
Stokes problem.

In particular, we use â€œTaylor-Hoodâ€ type elements ğ‘„ğ‘„ğ‘„ğ‘˜ /ğ‘„ğ‘˜âˆ’1 [Taylor and Hood 1973], where the
three components of the velocity solution use elements of polynomial degree ğ‘˜ and the single

Algorithms for Parallel Generic hp-adaptive Finite Element Software

15

1

0.5

ğ‘¦

0

âˆ’0.5

âˆ’1

âˆ’1

âˆ’0.5

0
ğ‘¥

0.5

1

1

0.5

0

âˆ’0.5

âˆ’1

âˆ’1

âˆ’0.5

0
ğ‘¥

0.5

1

2

4

3
6
polynomial degree ğ‘˜

5

7

0 1 2 3 4 5 6 7 8 9 10 11
subdomain id

Fig. 4. Numerical approximation of the Laplace problem (2) after six adaptation cycles and five initial global
refinements. Left: The mesh and polynomial degrees used on each cell. Right: Partitioning of the mesh onto 12 MPI
processes with a load balancing weighting exponent of ğ‘ = 1.9.

component of the pressure uses an element of polynomial degree (ğ‘˜ âˆ’ 1). In our study, we choose a
collection of elements with ğ‘˜ = 3, . . . , 6.

Both refinement and hp-decision indicators are based on the scalar-valued pressure solution. We
mark 10 % of cells for refinement and 1 % for coarsening, which we divide equally into being h- and
p-adapted.

We solve the linear saddle point system that results from discretization using flexible GMRES
[Saad 1993] and a Silvester-Wathen-type preconditioner [Silvester and Wathen 1994] in which we
treat the elliptic block with the ML AMG preconditioner [Gee et al. 2007] of Trilinos [Heroux et al.
2005]. This combination of solver and preconditioner is known to scale to very large problems, at
least for elements of fixed order, see [Bangerth et al. 2012; Kronbichler et al. 2012].

Remark 3. In our experiments, we have found that the AMG solver used in both of the test cases
struggles with increasing fragmentation of polynomial degrees in the mesh. In order to address this, we
limit the difference of polynomial degrees on neighboring cells to one, in a scheme not dissimilar to
the commonly used approach of only allowing neighboring cells to differ by at most one level in mesh
refinement. In our experiments, this â€œsmoothingâ€ of polynomial degrees reduces the number of solver
iterations by up to 70 %; this translates equally to the wallclock time spent on solving the linear system.

5.2 Load balancing
As mentioned in Section 3, it is not clear a priori how to weigh the contribution of each cell of a
mesh to the overall cost of a program. As a consequence â€“ and unlike the h-adaptive case â€“ it is
not clear what the optimal load balancing strategy is.

Using the weighting proposed in Section 3, we have therefore run numerical experiments that
vary the relative weighting of cells based on the number of DoFs on each cell. We carry out

16

M. Fehling and W. Bangerth

1.23

1

0.8

0.6

0.4

0.2

0

e
r
u
s
s
e
r
p

1

0.8

0.6

0.4

y
t
i
c
o
l
e
v

0.2

0

6

5

4

3

1
âˆ’
ğ‘˜
ğ‘„
/
ğ‘˜
ğ‘„ğ‘„ğ‘„

f
o

ğ‘˜
e
e
r
g
e
d
l
a
i
m
o
n
y
l
o
p

Fig. 5. Stokes flow through the Y-pipe as described by equation (5) after four adaptation cycles and three initial
refinements. Top: The lower half shows the domain and the pressure, while the upper half depicts the mesh and a
vector plot describing the velocity field. Bottom left: A cut-away showing those cells with low polynomial degrees,
located generally where either the estimated errors are low or near the non-convex parts of the domain. Bottom
right: Cut-away showing those cells with a high polynomial degree.

investigations on a mesh with a wide variety of polynomial degrees and a substantial number of
hanging nodes that we obtain after a number of mesh refinement cycles. We keep this particular
mesh, but partition it differently onto the available MPI processes for varying values of the weighting
exponent ğ‘ in (1), and run a complete refinement cycle involving enumeration of DoFs, assembly of
the linear system, and solution of the linear system on the so-partitioned mesh.

The results are shown in Fig. 6 for the two test cases defined in Section 5.1. For both cases, the
largest contribution to the overall cost is the linear solver; the noise in the corresponding curves
results from slightly different numbers of linear solver iterations, likely a consequence of decisions
made in how the AMG algorithm builds its hierarchy in response to which rows of the overall
matrix are stored on which process. The data shown in Fig. 6 suggest that the overall run time is
minimized with an exponent of ğ‘ â‰ˆ 1.9 for the Laplace test case, and ğ‘ â‰ˆ 2.4 for the Stokes test
case. We use these values for the weighting exponent for all other experiments shown below.

Algorithms for Parallel Generic hp-adaptive Finite Element Software

17

linear solver

assemble linear system

full cycle

quadratic fit

Laplace

Stokes

220

200

180
20

n
o
i
t
u
c
e
x
E
/

]
s
[

e
m

i
t
k
c
o
l
c
l
l
a

W

1,000

800

600

200

0

1

2

3

4

0

1

2

3

4

Weighting exponent ğ‘

Fig. 6. Wall clock times for several operations of a complete adaptation cycle, when partitioning the mesh using
different weighting exponents ğ‘, see (1). Left: For one cycle of the two-dimensional Laplace problem of Section 5.1.1.
The problem has about 51 million DoFs and is solved on 96 MPI processes. Right: For one cycle of the three-
dimensional Stokes problem of Section 5.1.2. The problem has about 15 million DoFs and is solved on 96 MPI
processes.

The data shown in the figure makes it clear that the optimal exponent depends on the problem
solved, and needs to be assessed for each problem individually. However, in general the dependency
of the run time on the specific choice of exponent is relatively weak.

5.3 Efficiency and scalability of algorithms

In the following, we assess whether the algorithms we proposed in Section 2 are efficient and scale
to large problem sizes. To answer this question, we first discuss what â€œscalabilityâ€ means in the
context of hp-adaptive methods, before turning to results obtained on the test cases defined in
Section 5.1.

All results shown in this subsection were obtained on the Expanse supercomputer [Strande et al.

2021; Towns et al. 2014].

5.3.1 How to define scalability? One typically measures the efficiency of a parallel algorithm
running on ğ‘ƒ processes operating in parallel on ğ‘ work items through either â€œstrong scalingâ€
(where the problem size ğ‘ is fixed and we vary the number of processes ğ‘ƒ) or â€œweak scalingâ€
(where one increases the problem size ğ‘ along with the number of processes ğ‘ƒ, keeping ğ‘ /ğ‘ƒ
constant). In both cases, one measures the time it takes the algorithm to complete work.

For h-adaptive algorithms, it is relatively straightforward to define what ğ‘ is supposed to be:
it could be (i) the number of cells in the mesh, (ii) the number of unknowns in a finite element
discretization on that mesh (which equals the size of the linear systems that result), or (iii) the

18

n
o
i
t
u
c
e
x
E
/

]
s
[

e
m

i
t
k
c
o
l
c
l
l
a

W

102

101

100

10âˆ’1

10âˆ’2

linear solver
enumerate DoFs
105 DoFs per process

assemble linear system
estimate and mark
O (ğ‘DoFs)

M. Fehling and W. Bangerth

setup data structures
coarsen and refine

1024 MPI processes

4096 MPI processes

102

101

100

10âˆ’1

10âˆ’2

10âˆ’3

107

108

109

107

Number of DoFs

108

109

Fig. 7. Laplace problem: Scaling of wallclock time as a function of the number of unknowns ğ‘DoFs on a sequence
of consecutively refined meshes, for 1024 (left) and 4096 MPI processes. Each MPI process owns more than 105
DoFs only to the right of the indicated vertical line; to the left of this line, processes do not have enough work to
offset the cost of communication, and parallel efficiency should not be expected. The solid black trend lines for
O (ğ‘DoFs) are offset downward by a factor of four on the right, to illustrate optimal strong scaling when increasing
the number of MPI processes by a factor of four.

number of nonzero entries in the matrix (which determines the cost of a matrix-vector product, but
is also an important consideration in the cost of algorithms such as AMG). The choice of which of
these we want to call ğ‘ is unimportant because they are all proportional to each other. Indeed, if one
uses an optimal solver such as multigrid, one could also (iv) define ğ‘ to be the number of floating
point operations required to solve the linear system for a given problem â€“ it is again proportional
to the other measures.

But things are not this easy for hp-adaptive methods: when using different polynomial degrees
on cells, the four quantities mentioned above are no longer proportional to each other when
considering an hp-fragmented mesh. This disproportionality is of no importance when considering
strong scalability, because the problem size ğ‘ is fixed. But it is not obvious how to define weak
scalability because a sequence of problems that keeps ğ‘ /ğ‘ƒ constant for one definition of ğ‘ may
not imply that ğ‘ /ğ‘ƒ is constant for any of the other definitions of ğ‘ . Similarly, we show results
below where we increase ğ‘ for fixed ğ‘ƒ, observing how time scales with ğ‘ â€“ for which, again, the
observed scaling depends on what definition of ğ‘ we choose.

As a consequence, we describe results below where we either use ğ‘ = ğ‘DoFs (the number of
global DoFs in the problem), or ğ‘ = ğ‘nonzeros (the global number of nonzero entries in the matrix
which needs to be solved with on a given mesh). As expected, we will see that operations such as
the assembly of a linear system and its solution do not scale as O (ğ‘DoFs), but they instead scale
close to O (ğ‘nonzeros).

Algorithms for Parallel Generic hp-adaptive Finite Element Software

19

5.3.2 Results for the Laplace test case of Section 5.1.1. With these considerations in mind, let us
now turn to concrete timing data. Below, we show results for how much time our implementation
of the Laplace test case of Section 5.1.1 spends in each of the following categories of operations
(ordered roughly in their relevance to the overall run time to the program):

â€¢ Linear solver: This category includes setting up the AMG preconditioner, and then solving

the linear system.

â€¢ Assemble linear system: Compute cell-local matrix and right-hand side vector contributions to
the linear system, and insertion into the global objects. This step also includes communicating
these contributions to the process owning a matrix or vector row if necessary.

â€¢ Setup data structures: This step includes a number of setup steps that happen after generating
a mesh and before the assembly of the linear system. Specifically, we include the enumeration
of DoFs; exchanging between processes which non-locally owned matrix entries they will
write into; setting up a sparsity pattern for the global matrix; allocation of memory for the
system matrix and vectors; and determining constraints that result from hanging nodes and
boundary conditions.

â€¢ Enumerate DoFs: This category, a subset of the previous one, measures the time to enumerate

all DoFs based on the algorithm discussed in Section 2.3.

â€¢ Estimate and mark: Once the linear system has been solved, this step computes error and
smoothness estimates for each locally owned cell. It then computes global thresholds for
hp-adaptation, and flags cells for either h- or p-adaptation.

â€¢ Coarsen and refine: This final step performs the actual h-adaptation on marked cells while
enforcing a 2:1 cell size relationship across faces. It also updates the associated finite element
on cells (p-adaptation) while limiting the difference of polynomial degrees across cell inter-
faces. This category also measures the transfer data between old and new mesh, as well as
the cost of re-partitioning the mesh between processes.

Fig. 7 shows timing information for a situation where we repeatedly solve the problem while
adaptively refining the hp-mesh, on both 1024 and 4096 MPI processes. In this setup, with a fixed
number ğ‘ƒ of processes, one would hope that the run time increases linearly with the problem size
ğ‘ . Our results demonstrate that this linearity holds when ğ‘ is the ğ‘DoFs on each of the meshes â€“
at least once the problem is large enough. Importantly for the current paper, operations such as
estimating hp-indicators and refining the mesh accordingly, and in particular the enumeration of
DoFs using the algorithm of Section 2.3 are only minor contributions to the overall run time, which
is dominated by the assembly and in particular solution of linear systems.

On the other hand, Fig. 7 also shows that both the assembly and the solution of the linear
system do not scale like O (ğ‘DoFs). This result may not be surprising in view of the discussions of
Section 5.3.1: as we move from left to right, we do not only increase the number of unknowns, but
also increase polynomial degrees on cells, resulting in denser and denser linear systems that are
more costly to assemble and solve. As a consequence, Fig. 8 shows the same data as a function of
the nonzero entries ğ‘nonzeros. This figure illustrates that using this definition, both assembly and
the solution of linear systems scale nearly perfectly as O (ğ‘nonzeros).

A comparison of the left and right panels of Figs. 7 and 8 â€“ and in particular how the various
curves approach the trend lines O (ğ‘ ) that are offset in the panels by the ratio of the number of MPI
processes used â€“ shows that for sufficiently large problems, we also have good strong scaling. We
expand on this in Fig. 9 where we show scaling for a fixed problem with the number of processes.
The figure shows that most operations may not scale perfectly as O (1/ğ‘ƒ), but scalability is at
least adequate as long as the problem size per process remains sufficiently large (to the left of the

20

n
o
i
t
u
c
e
x
E
/

]
s
[

e
m

i
t
k
c
o
l
c
l
l
a

W

102

101

100

10âˆ’1

10âˆ’2

linear solver
enumerate DoFs
106 nonzeros per process

assemble linear system
estimate and mark
O (ğ‘nonzeros)

M. Fehling and W. Bangerth

setup data structures
coarsen and refine

1024 MPI processes

4096 MPI processes

102

101

100

10âˆ’1

10âˆ’2

109

1010
Number of nonzero matrix elements

109

1010

1011

Fig. 8. Laplace problem: The same scaling data as shown in Fig. 7, except shown as a function of the nonzero
entries of the matrix.

dashed line). The exception is the performance of the linear solver; this is a known problem with
implementations of algebraic multigrid methods, but also beyond the scope of the current paper.

5.3.3 Results for the Stokes test case of Section 5.1.2. We repeat many of these timing studies, using
the same timing categories, for the Stokes test case to assess whether our results also hold for a
more complex, three-dimensional, and vector-valued problem.

The left panel of Fig. 10 illustrates how run time scales with the size of the problem (here
measured by the number of global DoFs ğ‘DoFs) and again shows that most operations scale as one
would expect given the results of the previous section.

The right panel of Fig. 10 presents strong scaling data. As before, we get good strong scalability
as long as the problem size per process is sufficiently large (to the left of the dashed line). At the
same time, the figure also illustrates the limitations imposed by the linear solver we use and that
have prevented us from considering larger problems: much larger problems would have taken
many hours to solve even with large numbers of processes. We did not think that the associated
expense in CPU cycles would have provided further insight that is not already clear from the results
of the previous section and the figure â€“ namely, that with the exception of the linear solver and
possibly assembly, all hp-related operations scale reasonably well to large problem sizes for both
simple (2d Laplace) and complex (3d Stokes) problems.

6 CONCLUSIONS

In this manuscript, we have presented algorithms combining our previous work on parallel and
hp-adaptive finite element methods, and that allows us to solve problems with hp-adaptive methods
on large, parallel machines with distributed memory. In particular, we have presented algorithms
for the enumeration of DoFs, a heuristic approach to weighted load balancing, and on how to
transfer data of variable size between processes.

Algorithms for Parallel Generic hp-adaptive Finite Element Software

21

linear solver
enumerate DoFs
105 DoFs per process

assemble linear system
estimate and mark
O (1/ğ‘ƒ)

setup data structures

52 million DoFs

1.05 billion DoFs

n
o
i
t
u
c
e
x
E
/

]
s
[

e
m

i
t
k
c
o
l
c
l
l
a

W

102

100

10âˆ’2

102

101

100

10âˆ’1

8

16 32 64 128 256 51210242056

1024

2056

4096

8192

16384

Number of MPI processes

Fig. 9. Laplace problem: Strong scaling for one advanced adaptation cycle at different problem sizes. Each MPI
process owns more than 105
DoFs only to the left of the indicated vertical line; to the right of this line, processes do
not have enough work to offset the cost of communication, and parallel efficiency should not be expected. Left:
Fixed problem size of roughly 52 million DoFs. Right: Fixed problem size of roughly 1.05 billion DoFs. The trend
lines for O (1/ğ‘ƒ) are offset between the two panels by the ratio of the size of the problem to allow for assessing
weak scalability of the algorithms.

The results we have shown in Section 5 illustrate that our algorithms all scale reasonably well
both to large problems and large MPI process counts, and in particular â€“ as one might have expected
â€“ that (i) the linear solver is the bottleneck in solving partial differential equations that result from
hp-discretizations, and that (ii) the enumeration algorithm of Section 2 contributes to the overall
run time in an essentially negligible way.

Our data therefore also clearly points to future work necessary to make hp-methods viable for
more widespread use: we need more scalable iterative solvers and preconditioners, specifically
ones that are better than the AMG ones we have used here. Such work would, for example, build
on the geometric multigrid (GMG) ideas in [Mitchell 2010], or hybrid approaches like in [Brown
et al. 2022; Fehn et al. 2020]. Furthermore, the literature suggests that the matrix-free approaches of
[Brown et al. 2022; Kronbichler and Kormann 2012; Munch et al. 2022] should be able to overcome
many of these solver limitations.

Acknowledgments

This paper is dedicated to the memory of William (Bill) F. Mitchell (1955â€“2019), who for many
years moved the hp-finite element method along by providing high-quality implementations of
the method through his PHAML software [Mitchell 2002] when there were few other packages that
one could play with. Equally importantly, in a monumental effort, he collected and compared the
many different ways proposed in the literature in which one can drive hp-adaptivity in practice.
This work â€“ an extension of his work in the late 1980s comparing h-adaptive refinement criteria

22

n
o
i
t
u
c
e
x
E
/

]
s
[

e
m

i
t
k
c
o
l
c
l
l
a

W

linear solver
enumerate DoFs
105 DoFs per process

assemble linear system
estimate and mark
O (ğ‘DoFs) or O (1/ğ‘ƒ)

M. Fehling and W. Bangerth

setup data structures
coarsen and refine

128 MPI processes

15 million DoFs

104

102

100

10âˆ’2

106.5

107
Number of DoFs

102

100

10âˆ’2

107.5

4

16

64

256

1024

Number of MPI processes

Fig. 10. Stokes problem. Left: Consecutive adaptation cycles with 128 MPI processes. The dashed line again indicates
105
DoFs per process; processes have more than this number only to the right of the line. Right: Strong scaling with
a fixed problem of 15 million DoFs. Computations exceed 105
DoFs per process only to the left of the dashed line.

[Mitchell 1989] â€“ resulted in a comprehensive 2014 paper that in the end stood at 39 pages [Mitchell
and McClain 2014], but for which the original 2011 NIST report had a full 215 pages [Mitchell and
McClain 2011].

Computational methods only gain broad acceptance when the literature contains incontrovertible
evidence in the form of comparison between methods. Papers that do such comparisons are tedious
to write and often not as highly regarded as ones that propose new methods, but crucial for our
community to finally see which methods work and which donâ€™t. Bill excelled at writing such
papers, and his contributions to hp-finite element methods will continue to be highly regarded. His
impartial and objective approach to declaring winners and losers will be missed!

An obituary for Bill Mitchell can be found at [Boisvert 2019].

Funding

This work used compute resources provided by the Extreme Science and Engineering Discovery
Environment (XSEDE) [Towns et al. 2014], which is supported by National Science Foundation
grant number ACI-1548562.

MFâ€™s work was supported by the National Science Foundation under award OAC-1835673 as

part of the Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program.

WBâ€™s work was partially supported by the National Science Foundation under award OAC-

1835673; by award DMS-1821210; and by award EAR-1925595.

REFERENCES

Martin AlnÃ¦s, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris Richardson, Johannes
Ring, Marie E. Rognes, and Garth N. Wells. 2015. The FEniCS Project Version 1.5. Archive of Numerical Software 3, 100
(Dec. 2015), 9â€“23. https://doi.org/10.11588/ans.2015.100.20553

Algorithms for Parallel Generic hp-adaptive Finite Element Software

23

Robert Anderson, Julian Andrej, Andrew Barker, Jamie Bramwell, Jean-Sylvain Camier, Jakub Cerveny, Veselin Dobrev,
Yohann Dudouit, Aaron Fisher, Tzanio Kolev, Will Pazner, Mark Stowell, Vladimir Tomov, Ido Akkerman, Johann Dahm,
David Medina, and Stefano Zampini. 2021. MFEM: A Modular Finite Element Methods Library. Computers & Mathematics
with Applications 81 (Jan. 2021), 42â€“74. https://doi.org/10.1016/j.camwa.2020.06.009

Daniel Arndt, Wolfgang Bangerth, Bruno Blais, Marc Fehling, Rene GassmÃ¶ller, Timo Heister, Luca Heltai, Uwe KÃ¶cher,
Martin Kronbichler, Matthias Maier, Peter Munch, Jean-Paul Pelteret, Sebastian Proell, Konrad Simon, Bruno Turcksin,
David Wells, and Jiaqi Zhang. 2021a. The deal.II library, Version 9.3. Journal of Numerical Mathematics 29, 3 (Sept. 2021),
171â€“186. https://doi.org/10.1515/jnma-2021-0081

Daniel Arndt, Wolfgang Bangerth, Denis Davydov, Timo Heister, Luca Heltai, Martin Kronbichler, Matthias Maier, Jean-Paul
Pelteret, Bruno Turcksin, and David Wells. 2021b. The deal.II finite element library: design, features, and insights.
Computers & Mathematics with Applications 81 (2021), 407â€“422. https://doi.org/10.1016/j.camwa.2020.02.022

Ivo BabuÅ¡ka and Milo R. Dorr. 1981. Error estimates for the combined h and p versions of the finite element method. Numer.

Math. 37, 2 (June 1981), 257â€“277. https://doi.org/10.1007/bf01398256

Ivo BabuÅ¡ka and Benqi Guo. 1996. Approximation Properties of the h-p Version of the Finite Element Method. Computer
Methods in Applied Mechanics and Engineering 133, 3-4 (July 1996), 319â€“346. https://doi.org/10.1016/0045-7825(95)00946-9
Wolfgang Bangerth, Carsten Burstedde, Timo Heister, and Martin Kronbichler. 2012. Algorithms and Data Structures
for Massively Parallel Generic Adaptive Finite Element Codes. ACM Trans. Math. Software 38, 2 (Jan. 2012), 14/1â€“28.
https://doi.org/10.1145/2049673.2049678

Wolfgang Bangerth, Ralf Hartmann, and Guido Kanschat. 2007. deal.II - A General-Purpose Object-Oriented Finite Element

Library. ACM Trans. Math. Software 33, 4 (Aug. 2007), 24/1â€“27. https://doi.org/10.1145/1268776.1268779

Wolfgang Bangerth and Oliver Kayser-Herold. 2009. Data Structures and Requirements for hp Finite Element Software.

ACM Trans. Math. Software 36, 1 (March 2009), 4/1â€“31. https://doi.org/10.1145/1486525.1486529

Peter Bastian, Markus Blatt, Andreas Dedner, Nils-Arne Dreier, Christian Engwer, RenÃ© Fritze, Carsten GrÃ¤ser, Christoph
GrÃ¼ninger, Dominic Kempf, Robert KlÃ¶fkorn, Mario Ohlberger, and Oliver Sander. 2021. The DUNE Framework:
Basic Concepts and Recent Developments. Computers & Mathematics with Applications 81 (Jan. 2021), 75â€“112. https:
//doi.org/10.1016/j.camwa.2020.06.007

Kim S. Bey, Abani Patra, and John Tinsley Oden. 1996. hp-Version Discontinuous Galerkin Methods for Hyperbolic
Conservation Laws. Computer Methods in Applied Mechanics and Engineering 133, 3 (July 1996), 259â€“286. https:
//doi.org/10.1016/0045-7825(95)00944-2

Ronald F. Boisvert. 2019. A Tribute to William F. Mitchell. https://sinews.siam.org/Details-Page/a-tribute-to-william-f-

mitchell

Susanne Brenner and Ridgway Scott. 2008. The Mathematical Theory of Finite Element Methods (third ed.). Springer, New

York. https://doi.org/10.1007/978-0-387-75934-0

Jed Brown, Valeria Barra, Natalie Beams, Leila Ghaffari, Matthew Knepley, William Moses, Rezgar Shakeri, Karen Stengel,
Jeremy L. Thompson, and Junchao Zhang. 2022. Performance Portable Solid Mechanics via Matrix-Free p-Multigrid.
https://doi.org/10.48550/arXiv.2204.01722 arXiv:2204.01722 [cs, math]

Carsten Burstedde. 2020. Parallel Tree Algorithms for AMR and Non-Standard Data Access. ACM Trans. Math. Software 46,

4 (Nov. 2020), 32:1â€“32:31. https://doi.org/10.1145/3401990

Carsten Burstedde, Lucas C. Wilcox, and Omar Ghattas. 2011. p4est: Scalable Algorithms for Parallel Adaptive Mesh
Refinement on Forests of Octrees. SIAM Journal on Scientific Computing 33, 3 (2011), 1103â€“1133. https://doi.org/10.1137/
100791634

Noel Chalmers, Gbemeho Agbaglah, Marcin Chrust, and Catherine Mavriplis. 2019. A Parallel hp-Adaptive High Order
Discontinuous Galerkin Method for the Incompressible Navier-Stokes Equations. Journal of Computational Physics: X 2
(March 2019), 100023/1â€“22. https://doi.org/10.1016/j.jcpx.2019.100023

Tino Eibner and Jens Markus Melenk. 2007. An Adaptive Strategy for hp-FEM Based on Testing for Analyticity. Computational

Mechanics 39, 5 (April 2007), 575â€“595. https://doi.org/10.1007/s00466-006-0107-0

Marc Fehling. 2020. Algorithms for massively parallel generic hp-adaptive finite element methods. Ph. D. Dissertation.

Bergische UniversitÃ¤t Wuppertal, Forschungszentrum JÃ¼lich GmbH. http://hdl.handle.net/2128/25427

Marc Fehling. 2022. hpbox: Sandbox for hp-adaptive methods. Zenodo. https://doi.org/10.5281/zenodo.6425947
Marc Fehling, Peter Munch, and Wolfgang Bangerth. 2021. The deal.II tutorial step-75: parallel hp-adaptive multigrid

methods for the Laplace equation. Zenodo. https://doi.org/10.5281/zenodo.6423570

Niklas Fehn, Peter Munch, Wolfgang A. Wall, and Martin Kronbichler. 2020. Hybrid Multigrid Methods for High-Order
Discontinuous Galerkin Discretizations. J. Comput. Phys. 415 (Aug. 2020), 109538. https://doi.org/10.1016/j.jcp.2020.
109538

Rene GassmÃ¶ller, Harsha Lokavarapu, Eric Heien, Elbridge Gerry Puckett, and Wolfgang Bangerth. 2018. Flexible and
Scalable Particle-in-Cell Methods With Adaptive Mesh Refinement for Geodynamic Computations. Geochemistry,
Geophysics, Geosystems 19, 9 (2018), 3596â€“3604. https://doi.org/10.1029/2018GC007508

24

M. Fehling and W. Bangerth

Michael W. Gee, Christopher M. Siefert, Jonathan J. Hu, Ray S. Tuminaro, and Marzio G. Sala. 2007. ML 5.0 Smoothed

Aggregation Userâ€™s Guide. Technical Report SAND2006-2649. Sandia National Laboratories.

Christoph Gersbacher. 2016.

Implementation of hp-Adaptive Discontinuous Finite Element Methods in DUNE-FEM.

https://doi.org/10.48550/arXiv.1604.07242 arXiv:1604.07242 [cs]

Benqi Guo and Ivo BabuÅ¡ka. 1986a. The h-p Version of the Finite Element Method, Part 1: The Basic Approximation Results.

Computational Mechanics 1, 1 (March 1986), 21â€“41. https://doi.org/10.1007/BF00298636

Benqi Guo and Ivo BabuÅ¡ka. 1986b. The h-p version of the finite element method, Part 2: General Results and Applications.

Computational Mechanics 1, 3 (Sept. 1986), 203â€“220. https://doi.org/10.1007/bf00272624

FrÃ©dÃ©ric Hecht. 2012. New Development in Freefem++. Journal of Numerical Mathematics 20, 3-4 (Dec. 2012), 251â€“266.

https://doi.org/10.1515/jnum-2012-0013

Michael A. Heroux, Roscoe A. Bartlett, Vicki E. Howle, Robert J. Hoekstra, Jonathan J. Hu, Tamara G. Kolda, Richard B.
Lehoucq, Kevin R. Long, Roger P. Pawlowski, Eric T. Phipps, Andrew G. Salinger, Heidi K. Thornquist, Ray S. Tuminaro,
James M. Willenbring, Alan Williams, and Kendall S. Stanley. 2005. An Overview of the Trilinos Project. ACM Trans.
Math. Software 31, 3 (Sept. 2005), 397â€“423. https://doi.org/10.1145/1089014.1089021

Paul Houston and Endre SÃ¼li. 2005. A Note on the Design of hp-Adaptive Finite Element Methods for Elliptic Partial
Differential Equations. Computer Methods in Applied Mechanics and Engineering 194, 2-5 (Feb. 2005), 229â€“243. https:
//doi.org/10.1016/j.cma.2004.04.009

John N. Jomo, Nils Zander, Mohamed Elhaddad, Ali Ã–zcan, Stefan Kollmannsberger, Ralf-Peter Mundani, and Ernst Rank.
2017. Parallelization of the Multi-Level hp-Adaptive Finite Cell Method. Computers & Mathematics with Applications 74,
1 (July 2017), 126â€“142. https://doi.org/10.1016/j.camwa.2017.01.004

Åukasz Kaczmarczyk, Zahur Ullah, Karol Lewandowski, Xuan Meng, Xiao-Yi Zhou, Ignatios Athanasiadis, Hoang Nguyen,
Christophe-Alexandre Chalons-Mouriesse, Euan J. Richardson, Euan Miur, Andrei G. Shvarts, Mebratu Wakeni, and
Chris J. Pearce. 2020. MoFEM: An Open Source, Parallel Finite Element Library. Journal of Open Source Software 5, 45
(Jan. 2020), 1441/1â€“8. https://doi.org/10.21105/joss.01441

D. W. Kelly, J. P. De S. R. Gago, O. C. Zienkiewicz, and I. Babuska. 1983. A posteriori Error Analysis and Adaptive Processes
Internat. J. Numer. Methods Engrg. 19, 11 (1983), 1593â€“1619.

in the Finite Element Method: Part I - Error Analysis.
https://doi.org/10.1002/nme.1620191103

Benjamin S. Kirk, John W. Peterson, Roy H. Stogner, and Graham F. Carey. 2006. libMesh: A C++ Library for Parallel
Adaptive Mesh Refinement/Coarsening Simulations. Engineering with Computers 22, 3 (Dec. 2006), 237â€“254. https:
//doi.org/10.1007/s00366-006-0049-3

Martin Kronbichler, Timo Heister, and Wolfgang Bangerth. 2012. High Accuracy Mantle Convection Simulation through
Modern Numerical Methods. Geophysical Journal International 191, 1 (Oct. 2012), 12â€“29. https://doi.org/10.1111/j.1365-
246X.2012.05609.x

Martin Kronbichler and Katharina Kormann. 2012. A Generic Interface for Parallel Cell-Based Finite Element Operator

Application. Computers & Fluids 63 (June 2012), 135â€“147. https://doi.org/10.1016/j.compfluid.2012.04.012

Andras Laszloffy, Jingping Long, and Abani K Patra. 2000. Simple Data Management, Scheduling and Solution Strategies
for Managing the Irregularities in Parallel Adaptive hp Finite Element Simulations. Parallel Comput. 26, 13 (Dec. 2000),
1765â€“1788. https://doi.org/10.1016/S0167-8191(00)00054-5

Catherine Mavriplis. 1994. Adaptive Mesh Strategies for the Spectral Element Method. Computer Methods in Applied

Mechanics and Engineering 116, 1-4 (1994), 77â€“86. https://doi.org/10.1016/S0045-7825(94)80010-3

Message Passing Interface Forum. 2021. MPI: A Message-Passing Interface Standard (Version 4.0). Technical Report. University

of Tennessee, Knoxville, Tennessee. https://www.mpi-forum.org/

William F. Mitchell. 1989. A comparison of adaptive refinement techniques for elliptic problems. ACM Trans. Math. Software

15, 4 (Dec. 1989), 326â€“347. https://doi.org/10.1145/76909.76912

William F. Mitchell. 2002. The Design of a Parallel Adaptive Multi-Level Code in Fortran 90. In Computational Science -
ICCS 2002 (Lecture Notes in Computer Science 2331, Vol. 3), Peter M. A. Sloot, Alfons G. Hoekstra, C. J. Kenneth Tan, and
Jack J. Dongarra (Eds.). Springer, Berlin, Heidelberg, 672â€“680. https://doi.org/10.1007/3-540-47789-6_70

William F. Mitchell. 2010. The hp-multigrid Method Applied to hp-adaptive Refinement of Triangular Grids. Numerical

Linear Algebra with Applications 17, 2-3 (April 2010), 211â€“228. https://doi.org/10.1002/nla.700

William F. Mitchell and Marjorie A. McClain. 2011. A Comparison of hp-Adaptive Strategies for Elliptic Partial Differential
Equations (Long Version). Technical Report 7824. National Institute of Standards and Technology. https://doi.org/10.
6028/NIST.IR.7824

William F. Mitchell and Marjorie A. McClain. 2014. A Comparison of hp-Adaptive Strategies for Elliptic Partial Differential

Equations. ACM Trans. Math. Software 41, 1 (Oct. 2014), 2/1â€“39. https://doi.org/10.1145/2629459

Peter Munch, Timo Heister, Laura Prieto Saavedra, and Martin Kronbichler. 2022. Efficient Distributed Matrix-Free
https://doi.org/10.48550/arXiv.2203.12292

Multigrid Methods on Locally Refined Meshes for FEM Computations.
arXiv:2203.12292 [cs, math]

Algorithms for Parallel Generic hp-adaptive Finite Element Software

25

John Tinsley Oden, Abani Patra, and Yusheng Feng. 1994. Domain Decomposition for Adaptive hp Finite Element Methods.
In Domain Decomposition Methods in Scientific and Engineering Computing (Contemporary Mathematics, Vol. 180), David E.
Keyes and Jinchao Xu (Eds.). American Mathematical Society, Providence, Rhode Island, 295â€“301. https://doi.org/10.
1090/conm/180

Maciej PaszyÅ„ski and Leszek Demkowicz. 2006. Parallel, Fully Automatic hp-Adaptive 3D Finite Element Package. Engineering

with Computers 22, 3 (Dec. 2006), 255â€“276. https://doi.org/10.1007/s00366-006-0036-8

Maciej PaszyÅ„ski and David Pardo. 2011. Parallel Self-Adaptive hp Finite Element Method with Shared Data Structure.

Computer Methods in Material Science 11, 2 (2011), 399â€“405.

Abani Patra and John Tinsley Oden. 1995. Problem Decomposition for Adaptive hp Finite Element Methods. Computing

Systems in Engineering 6, 2 (April 1995), 97â€“109. https://doi.org/10.1016/0956-0521(95)00008-N

Will Pazner and Tzanio Kolev. 2022. Uniform Subspace Correction Preconditioners for Discontinuous Galerkin Methods
with hp-Refinement. Communications on Applied Mathematics and Computation 4, 2 (June 2022), 697â€“727. https:
//doi.org/10.1007/s42967-021-00136-3

Yves Renard and Konstantinos Poulios. 2020. GetFEM: Automated FE Modeling of Multiphysics Problems Based on a

Generic Weak Form Language. (April 2020).

Youcef Saad. 1993. A Flexible Inner-Outer Preconditioned GMRES Algorithm. SIAM Journal on Scientific Computing 14, 2

(March 1993), 461â€“469. https://doi.org/10.1137/0914028

David Silvester and Andrew Wathen. 1994. Fast Iterative Solution of Stabilised Stokes Systems Part II: Using General Block

Preconditioners. SIAM J. Numer. Anal. 31, 5 (Oct. 1994), 1352â€“1367. https://doi.org/10.1137/0731070

Pavel Å olÃ­n, Jakub ÄŒervenÃ½, and Ivo DoleÅ¾el. 2008. Arbitrary-Level Hanging Nodes and Automatic Adaptivity in the hp-FEM.
Mathematics and Computers in Simulation 77, 1 (Feb. 2008), 117â€“132. https://doi.org/10.1016/j.matcom.2007.02.011
Shawn Strande, Haisong Cai, Mahidhar Tatineni, Wayne Pfeiffer, Christopher Irving, Amit Majumdar, Dmitry Mishin,
Robert Sinkovits, Mike Norman, Nicole Wolter, Trevor Cooper, Ilkay Altintas, Marty Kandes, Ismael Perez, Manu
Shantharam, Mary Thomas, Subhashini Sivagnanam, and Thomas Hutton. 2021. Expanse: Computing without Boundaries:
Architecture, Deployment, and Early Operations Experiences of a Supercomputer Designed for the Rapid Evolution
in Science and Engineering. In Practice and Experience in Advanced Research Computing. ACM, Boston MA USA, 1â€“4.
https://doi.org/10.1145/3437359.3465588

C. Taylor and P. Hood. 1973. A Numerical Solution of the Navier-Stokes Equations Using the Finite Element Technique.

Computers & Fluids 1, 1 (Jan. 1973), 73â€“100. https://doi.org/10.1016/0045-7930(73)90027-3

John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop,
Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, and Nancy Wilkins-Diehr. 2014. XSEDE: Accelerating
Scientific Discovery. Computing in Science & Engineering 16, 5 (Sept. 2014), 62â€“74. https://doi.org/10.1109/MCSE.2014.80
Lin-bo Zhang. 2019. A Tutorial on PHG. Technical Report. Academy of Mathematics and Systems Science, Chinese Academy

of Sciences. http://lsec.cc.ac.cn/phg

