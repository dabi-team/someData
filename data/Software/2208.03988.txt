2
2
0
2

g
u
A
8

]
E
S
.
s
c
[

1
v
8
8
9
3
0
.
8
0
2
2
:
v
i
X
r
a

Fuzzing Microservices In Industry: Experience of Applying
EvoMaster at Meituan

Man Zhang1, Andrea Arcuri1, Yonggang Li2, Kaiming Xue2
Zhao Wang2, Jian Huo2, Weiwei Huang2
1 Kristiania University College, Norway
2 Meituan, Beijing

Abstract

With several microservice architectures comprising of thousands of web services in total, used
to serve 630 million customers, companies like Meituan face several challenges in the verification
and validation of their software. The use of automated techniques, especially advanced AI-based
ones, could bring significant benefits here. EvoMaster is an open-source test case generation tool
for web services, that exploits the latest advances in the field of Search-Based Software Testing
research. This paper reports on our experience of integrating the EvoMaster tool in the testing
processes at Meituan. Experiments were carried out to evaluate its performance in detail on two
industrial web services which are parts of a large e-commerce microservices. A questionnaire and
interviews were carried out with the engineers and managers at Meituan, to better understand the
applicability and usability of fuzzing tools like EvoMaster in real industrial settings. On the one
hand, the results of these analyses clearly show that existing tools like EvoMaster are already of
benefits for practitioners in industry, e.g., EvoMaster detected 21 real faults and achieved an
average of 71.3% coverage for code defining endpoints and 51.7% coverage for code implementing
business logic, on two industrial APIs. On the other hand, there are still many critical challenges
that the research community has to investigate.

Keywords: Empirical industrial study, automated test generation, SBST, fuzzing, REST

1

Introduction

In microservice architectures [41], large enterprise systems are split in hundreds/thousands of connected
web services. This software architecture is widely applied in industry, to enable rapid and frequent
delivery of the services in production. Considering the large number of services and their complex
interactions, there are several challenges in the verification and validation of such systems. Automated
testing techniques (such as EvoMaster [10, 17]) could be one possible manner to address such
challenges.

When experimenting with test case generation techniques, one challenge is that large enterprise
systems are rarely present in open-source repositories. Currently, the performance of EvoMaster has
been mainly assessed on open-source APIs (such as APIs collected in the GitHub repository EMB [2]).
With recent studies on REST API fuzzers [32, 52], EvoMaster has been evaluated as the most
performing tool on such open-source APIs. But, most of these open-source APIs are web services
that work in isolation, and not part of a microservice architecture [2]. As part of industry-driven
research [27], EvoMaster has been evaluated on industrial APIs as well (e.g., in [15]). But those
were part of small systems.

For a test generation tool, it is not only a matter of finding faults. How actual practitioners use
those tools, and how they integrate them in their development processes, is of paramount importance.
This is particularly the case for large enterprise systems, where several non-technical challenges are
present as well (e.g., communication issues when there are hundreds of engineers, and one service
depends on another service developed by a different team). Therefore, our objectives in this study are:

• first, to assess the effectiveness of EvoMaster at solving test generation problems in a real,

large industrial setting;

1

 
 
 
 
 
 
• then, to investigate essential features for enabling its further adoption into industrial practice;

• further, to understand existing testing challenges in industry which could be addressed by the

research community.

To achieve such objectives, we employed EvoMaster on two real industrial services, part of one system
(called Meituan Select) with hundreds of microservices, provided by Meituan1 (a large e-commerce
company). We performed a detailed review of the results achieved by EvoMaster (e.g., its generated
test cases), and conducted a questionnaire and interviews with eight industrial practitioners at Meituan.
Those consisted of a total of 15 questions regarding applicability, effectiveness and integration of
EvoMaster and existing problems and challenges in industry.

Based on results on the two chosen APIs, compared to existing tests written manually (if the API
has any), EvoMaster achieved competitive line coverage in key implementations of the services, i.e.,
at average 71.3% of code for defining endpoints and 51.7% of code for implementing business logic.
With the generated tests by EvoMaster, it was possible for the industrial partner to identify real
faults in their systems. In the two case studies, a total of 21 unique potentially critical faults where
identified, which the engineers at Meituan have fixed. Regarding feedback on the usability of the tool,
for the engineers at Meituan, there did not seem to exist any major difficulty in applying the tool on
their industrial applications. In addition, as discussed with our industrial partner, manual configuration
of EvoMaster (such as writing drivers to handle the SUTs) could be automated. This is because
industrial APIs developed by the same company are typically built with the same pattern (e.g., same
framework, like Spring). Thus, the handling of the SUT (such as starting, stopping and resetting) can
be templatized, greatly simplifying when tool is run on thousands of different servicers. Regarding how
to integrate EvoMaster in their development processes, adopting it to their Continuous Integration
(CI) environment would be highly beneficial. As discussed with our partner, considering that the
manual configuration could be automated and EvoMaster can be started from the command-line,
enabling it on their CI systems was a simple task. Depending on time cost of EvoMaster, a main
problem to investigate is how to apply the fuzzer on CI, such as in which action (e.g., after each pull
request) to enable test generation with the fuzzer. Regarding feedback on outputs, the interviews
pointed out that the generated test cases should be improved in terms of their readability, and these
generated tests should improve the combination of testing various endpoints to better cover the business
logic of their systems. With this study, also based on the results of the questionnaire and interviews,
we discuss lessons learned in terms of testing setup, testing criteria, locating faults, tool integration
and assertion generation, and then summarize important common challenges we face in the fuzzing of
industrial microservices.

The main contribution of this paper is an empirical study of the application of an academic fuzzer
in a real industrial setting, targeting the domain of large scale microservice architectures, including
how software engineers would integrate such fuzzers in their development processes. Furthermore,
the questions in our interviews/questionnaire are a superset of an existing study in the unit testing
domain, which enable us to do some forms of meta-analysis. The evaluation of scientific research
in actual industrial settings, albeit in a single company, is an essential step toward addressing the
significant gap between research and practice. On the one hand, research work only evaluated in the lab
might rely on assumptions that might not hold true in practice. On the other hand, industry-relevant
research with collaborations with industry partners take a massive amount of time, and it is rare
that this kind of work involve more than 1 or 2 industrial case studies at a time, which can lead to
less generalizable results. Both kinds of studies are important, but, considering the current trends in
Software Engineering research, ‘‘Generalizability is overrated’’ [23].

The paper is organized as follows. We discuss related work in Section 2 and introduce EvoMaster,
a fuzzer we employ in this empirical study, in Section 3. Section 4 presents our design of the empirical
study followed by experimental results discussion in Section 5. Lessons learned and common challenges
are summarized in Section 6 and Section 7, respectively. We clarify threats to validity in Section 8
and conclude the paper in Section 9.

1https://about.meituan.com/en

2

2 Related Work

Most of recent approaches for automated web service testing are developed for REST APIs in the
context of black-box testing [47, 19, 28, 31, 36, 40, 49]. For REST APIs, OpenAPI [5] provides a
well-structured and machine-readable schema language to specify how to access these web services.
Such schemas provide a base to enable automated testing techniques to access the system under test
(SUT). For example, RESTler proposed by Atlidakis et al. [19] can automatically produce a sequence
of requests to test REST APIs. The sequence is decided based on either a statistical analysis on the
schema with request types, or a dynamic analyses on feedback received from previously executed
requests. In addition, RESTler is now integrated with different testing techniques for regression
testing [29], security aspect testing [20], and test data generation [28]. Viglianisi et al. [47] developed
RESTTESTGEN that employs a dependency analysis on the schema to better generate test orders
and inputs. The approach comprises two components, i.e., nominal tester and error tester, for finding
defects by testing the SUT against nominal scenarios and error scenarios. Laranjeiro et al. [36] proposed
bBOXRT for robustness testing in the context of black-box testing by testing the REST APIs with
various invalid inputs. The invalid inputs are generated based on a set of mutation rules on data type
with the schema. RESTest [40] developed by Martin-Lopez et al. is a black-box testing framework
which has been integrated with different techniques such as fuzzing test inputs, adaptive random
testing and constraint-based testing to generate tests for REST APIs. The constraint-based testing
are performed based on inter-parameter dependencies analysis on the schema. RestCT [49] proposed
by Wu et al. is a black-box combinatorial testing approach for REST APIs. The approach consists of
two phases for generating orders and inputs of requests with OpenAPI specifications. Hatfield-Dodds
and Dygalo developed Schemathesis [30] using property-based testing techniques. The tool derives
structure and semantics of APIs for enabling REST API testing.

Considering that a black-box testing approach is independent from the programming language and
source code, there are available more industrial case studies to be used for evaluating it, compared with
a white-box testing approach. For example, live services on the internet can be used for these kinds
of experiments. Many industrial services (such as Microsoft Azure, Office365 cloud services, Google
Drive, Spotify), and hundreds of real REST APIs listed on public API aggregators, have been used to
conduct experiments when evaluating the aforementioned black-box testing techniques. Several real
faults were found, e.g., based on the returned HTTP status codes, in a process that is also known as
fuzzing.

Regarding industrial evaluations, there exist some recent work to conduct empirical studies on
automated unit testing in industrial settings related to financial applications [7] and embedded
systems [50]. Regarding system-level testing, the tool evaluations have been mainly investigated for
user interface (UI) testing on ERP applications [24] and mobile applications [8, 48]. Other testing
techniques, such as mutation testing [22, 43], fault debugging [56] and flaky tests [35] have also been
evaluated in industry.

With recent studies [32, 52] conducted for studying existing fuzzers (including the fuzzers described
above and EvoMaster), white-box EvoMaster achieved the best performance on open-source APIs.
In addition, to the best of our knowledge, there does not exist in the literature any industrial evaluation
for white-box system testing of microservices. This paper fills this important gap in the research
literature.

3 EvoMaster

EvoMaster is an open-source testing tool designed for addressing system test case generation for
enterprise web services (currently support REST and GraphQL APIs) [17].
It is a search-based
tool, using evolutionary algorithms to generate effective test cases. The tool enables both white-
box [9, 11, 12, 54] (for APIs running on the JVM and NodeJS) and black-box testing [13].

EvoMaster is composed of two parts: the Driver (only used when performing white-box testing)
implements bytecode instrumentation, which enables the collection at runtime of code coverage and
advanced white-box heuristics, and provides interfaces to start/stop/reset the SUTs; and the Core

3

which provides a set of search algorithms (such as MIO [11] and MOSA [42]) to generate tests, defines
fitness functions to guide the search, and extracts info on how to access the SUT (e.g., OpenAPI [5]
for REST APIs).

3.1 The MIO Algorithm

The Many Independent Objective (MIO) [11] algorithm is specialized for system-level test case
generation in the context of white-box testing.
It is the default search algorithm for white-box
EvoMaster. Let us briefly summarize it here in this section. The algorithm is inspired by (1+1)
EA [37], which only employs sampling and mutation operators. Pseudo-code of the algorithm is shown
in Algorithm 1.

In the MIO algorithm, we defined fitness function δ with testing targets, which comprise all lines,
branches, status codes and fault finding. Each of them is a target to be optimized (i.e., covered) during
the evolutionary search. Each target has a population for it with a maximum size (denoted as n).
The MIO algorithm starts with an empty population (denoted as S). At each iteration, the algorithm
would either sample a new individual or mutate an existing individual in the evolving populations,
controlled by a probability Pr. If the individual reaches a new target, the MIO algorithm would create
a new population for it (e.g., a new population Tk for the newly covered target k). If a target is covered
with the individual, the size of its corresponding population would be reduced to 1, and the target
would not be considered further in the search. In the context of web service testing, the individual is a
test which is a sequence of HTTP calls. After a certain percentage of search budget is used (denoted
as F ), the MIO algorithm uses a focused search, where it increases the probability to mutate the
individuals which have recent improvement, instead of exploring other areas of the search landscape.
At the end of the search, the MIO algorithm outputs a test suite for the SUT, comprising of the best
evolved test cases for each testing target (denoted as A).

Algorithm 1: Pseudo-code of the MIO Algorithm [11]

Input

: Stopping condition C, Fitness function δ, Population size n, Probability for random sampling
Pr, Start of focused search F

Output : Archive of optimized individuals A

1 S ← SetOf EmptyP opulations( )
2 A ← {}
3 while ¬C do
4

if Pr > rand( ) then

p ← RandomIndividual( )

else

p ← SampleIndividual(S)
p ← M utate(p)

end
foreach element k ∈ ReachedTargets(p) do

if NewTarget(k) then

S ← S ∪ Tk

end
Tk ← Tk ∪ {p}
if IsT argetCovered(k) then
U pdateArchive(A, p)
S ← S \ Tk

else if |Tk| > n then

RemoveW orst(Tk, δ)

end

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

end
U pdateP arameters(F , Pr, n)

22
23 end

4

3.2

Integrated Novel Techniques

Throughout the years, since its inception in 2016, EvoMaster has been extended with various novel
techniques [14, 16, 55, 51, 21] to serve a more comprehensive testing in the context of white-box testing
for enterprise systems.

Enterprise web systems typically interact with databases and employs SQL to specify operations on
it. To test such systems, states of data in the interacted databases would possibly have a direct impact
on code coverage. EvoMaster was enhanced with a SQL handling technique [14], which defines SQL
query heuristics, and enables SQL execution monitoring and direct data insertion into the database
with SQL commands from the tests. With such technique, EvoMaster is capable of producing more
effective tests with generated SQL commands based on the SQL heuristics to directly manipulate states
of the SUT. In this context, a test (also referred as an individual) with additional SQL commands
could result in a long and complex chromosome to evolve. To have an effective mutation for handling
such individuals, an adaptive weight-based hypermutation [51] was developed that comprises a set of
novel strategies to adaptively manipulate the number of genes to mutate, select genes to mutate, and
guide how to mutate the values in these genes.

In the context of white-box testing, the flag problem is a common issue, i.e., lack of guidance to the
search due to boolean expressions. To address this issue, testability transformation [16] techniques have
been developed in EvoMaster that enable to transform the source code of the SUT (with replacement
methods), for providing better gradient values, e.g., for branch distance computations [34]. In addition,
test inputs are tracked in the transformed methods that further provides additional info (referred as
taint analysis) to better generate test data. Moreover, EvoMaster uses specific transformations
particularly for handling web service testing, e.g., to handle cases in which the OpenAPI schemas are
underspecified.

REST is one of the most applied techniques for building web services in industry. To enable
an automated testing for it, EvoMaster has been integrated with a set of techniques based on
REST domain. REST test generation problem was reformulated as a search problem [12] by defining
various types of genes corresponding to REST schema, i.e., OpenAPI. In addition, the fitness function
for MIO was enhanced with additional considerations on HTTP status codes and fault detection
(i.e., 500 status code). Moreover, to better sample tests for REST, a smart sampling technique was
developed with pre-defined structures. Furthermore, by further exploiting REST domain knowledge,
a resource-dependency based MIO [55] was proposed that is composed of resource-based sampling,
mutation and dependency heuristics for producing tests with effective resource handling. Besides REST
APIs, EvoMaster also enables the fuzzing of GraphQL APIs [21].

4 Experiment Design

4.1 Research Questions

To investigate EvoMaster in industrial settings and understand industrial challenges in the testing of
web services at Meituan, we conducted an empirical study to answer the following research questions:

RQ1: How difficult is it to set up a tool like EvoMaster in real industrial settings?

RQ1.1: How difficult do industrial practitioners think is to apply EvoMaster?
RQ1.2: What challenges do exist in setting up EvoMaster in industrial settings?

RQ2: How does EvoMaster perform in real industrial settings?

RQ2.1: How does EvoMaster perform with different time budgets on industrial case studies?
RQ2.2: How effective does EvoMaster perform on industrial case studies?
RQ2.3: How effective do industrial practitioners consider the automatically generated tests?

RQ3: What major barriers do industrial practitioners face when integrating EvoMaster into their

development process?

RQ4: What are the most important testing challenges that practitioners meet in their testing context?

5

Figure 1: An example of the SUT with its connected services

4.2 Subject of Study

Meituan Select is a large-scale e-commerce platform for community group bulk buying (e.g., fresh
vegetables, meat) that is a part of Meituan, and its products have covered 2 600 cities and counties in
China. Products can be bought directly from farmers or distributors, and then be delivered to the
community. With this e-commerce platform, its daily order volume is more than 30 millions. More than
630 million users are registered and use this e-commerce platform. The platform is constructed with
hundreds of web services (referred as microservices) for, e.g., ordering, allocation, packing, delivery,
and payments.

For the experiments in this paper, we conducted our study with two of those services, denoted as
CS1 and CS2 , chosen by the team at Meituan we are in contact with (as they were currently working
on these two web services). These services are part of this large microservice architecture, interacting
with many other services and databases (as SUT shown in Figure 1). Both services are RPC-based
APIs (i.e., in particular, a modified version of Apache Thrift [6]) implemented with Java. Table 1
presents detailed descriptive statistics of the two case studies, i.e., the number of Java class files, lines
of codes and the number of exposed endpoints (i.e., methods that can be invoked via RPC). We also
report #Services which is the number of other services that the SUT directly interacts with (as B
and D in Figure 1). Among them, #U is the number of its direct upstream services which the SUT
depends on (e.g., B), and #D is the number of its direct downstream services, which call and use the
SUT (e.g., D). Note that here we only report the number of services with direct communications. For
instance, the the upstream services of CS1 have further upstream services, e.g., A for B in Figure 1.
Regarding the databases, in industrial settings where there is the need to scale to hundreds of
millions of customers, the applied database is often distributed. At Meituan, this is achieved by sets
of databases with an ad-hoc Rational Database Service (RDS), developed internally by Meituan to
meet their scale needs. Connections between the SUT and databases are managed by sets of database
middlewares. For instance, as shown in Figure 1, a connection between SUT and databases might be
further distributed to multiple data sources (e.g., read, write) to different databases. In addition, the
RDS audits and monitors all SQL commands executed on the databases. For example, TRUNCATE and
DROP commands are forbidden from the web services, as the databases and tables are allowed to be
created only manually through an audit process. In Table 1, we also report related statistics of the
database used in the two chosen web services.

The two web services chosen as case studies in this paper are services whose previous versions
are actively used in production at Meituan. Before the running of the experiments for this paper, no
known bugs and faults were present (as all discovered bugs are promptly fixed).

4.3

Industrial Participants, Questionnaire and Interview

One of the main tasks in an industrial evaluation is to collect feedback from the industry practitioners.
In software engineering research, typical methods to collect such feedback are based on questionnaires
and interviews conducted with employees at these companies [38, 46]. In this study, to empirically
assess the fuzzer EvoMaster and evaluate its further potential integration in industrial settings, we

6

SUTDBA…DB MiddlewareDBDBRational Database ServiceCHTTP or RPCDB ConnectionDB MiddlewareDBManual Operation on DBuserTable 1: Descriptive statistics of industrial case studies

SUT #Scripts LOCs #Endpoints #Services (U, D) #Tables #RowsOfData

CS1
CS2

Total

245
98

32393
12152

343

44545

33
13

46

18 (14, 4)
11 ( 7, 4)

29 (21, 8)

142
17

159

256024
1840

257864

-Services(U,D) represents a number of services that the SUT directly interacts with, #U is a number of upstream services
and #D is a number of downstream services; #RowsOfData is a number of rows of existing data in related tables.

Table 2: Description of Industrial Participants

Position

#Participants #Years #Group

Abb.

Director of Software Quality
Principal Software Engineer
QA (Quality Assurance) Manager
QA Engineer

1
1
3
3

15+
11--12
8--10
0--10

200+ DSQ
30--50
PSE
10--25 QAM
QAE

#Years represents years of working in testing in industry, and #Group is a number of members in the group the
participant leads.

conducted our study with eight employees at Meituan (testers and managers). Table 2 represents info
of the eight industrial participants, i.e., positions (see Position), years of working in testing in industry
(see #Years), and the size of group the participant leads (see #Group). Note that terms in Position
are based on the job title terminology used internally at Meituan. The eight participants vary over
four different positions, which are responsible for various testing related tasks. This could provide
diverse viewpoints to this industrial evaluation. More specifically, the DSQ participant is in charge of
all testing departments at Meituan Select. He is involved in the interview study mainly to provide
viewpoints to challenges relating to their business scope. The PSE participant is the department
manger of internal testing tool development at Meituan Select, which can share opinions from the
standpoint of applicability and further integration of the fuzzer and challenges they meet in order
to enable testing of industrial services. The three QAM participants manage testing tasks and are
responsible for various specific services, such as services for inventory management at warehouse and
payment. They could give us feedback on preference of applying EvoMaster to their testing tasks,
and share their difficulties/experience in managing testing tasks for various kinds of industrial services.
The three QAE participants were involved based on their familiarity with the selected case studies.
They mainly handle testing tasks for specific services, such as write test cases and perform manual
testing. Thus, they could give feedback on the generated tests and share their experience in daily tasks
which fuzzers like EvoMaster aim to automate.

To collect the feedback from these participants, we designed a set of questions for the questionnaire
and interviews from four perspectives: applicability, effectiveness, integration, and existing problem-
s/challenges. Questions applied in questionnaire/interviews are shown in Table 3. Questionnaire
was performed with Google Form. Questions for assessing EvoMaster were designed with 5-Point
likert scale (QA1-4 and QB1--2 ) and Yes/No (QB5 ). Other questions for inquiring improvements
and existing problems are open-text. Interviews were conducted with online Zoom meetings. Each
interview was setup with one researcher as interviewer (the first author), one Meituan employee as
interviewee, and one Meituan employee to take minutes (the fourth author).

4.4 Experiment Procedure

To achieve our goals, the experiment design is presented in Table 4, with detailed involvements of
each participant for each task and each RQ. The overall procedure was conducted as:

T 1. First, a QAM manager from Meituan tried to apply EvoMaster on one of their applications.

Then, a questionnaire (QAs) was performed for collecting feedback on this experience.

T 2. To better understand the industrial web services developed at Meituan, we made a preliminary

7

Table 3: Questions in Questionnaire and Interview

Applicability (only Questionnaire)
QAs How difficult was it for you to set up EvoMaster (QA1), resolve dependencies (QA2), write driver

(QA3), run EvoMaster, and execute tests(QA4)?

Effectiveness (only Questionnaire)

QB1 How would you like to rate the readability of the generated tests?
QB2 How would you like to rate the quality of the generated tests?
QB3 How can the generated tests be improved?
QB4 Describe what you like better about manually written tests than generated tests?
QB5 Would you keep the generated system-level tests?
if yes, how would you like to keep and use them?
QB6

Integration (Questionnaire and Interview)
QC1 What are the major barriers from your point of view in adopting the EvoMaster tool?
QC2 Given your current infrastructure setup, how would you like to have automated system-level test

generation framework integrated?

Existing Problems and Challenges (Questionnaire and Interview)

QD1 What is one of your current major issues/time consuming activity with manual testing that you

would like to have automation for?

QD2 What kinds of faults are harder to detect in the system?
QD3 What are the most important challenges that you meet in testing?

study with EvoMaster on the two case studies selected by the industrial partner (i.e., Meituan).

T 3. After this preliminary study, we conducted an experiment of EvoMaster on the industrial
case studies to study its effectiveness. Regarding time budget settings, as discussed with the
industrial partner, testing performed on their CI pipeline typically takes a couple of minutes.
Since developers might wait for implementing other tasks, the time cost for the fuzzer is preferably
less than 1 hour. However, as a search-based method, the specified search budget in EvoMaster
would have a major impact on its performance. With a consideration on time constraints in
industry, therefore, we set three stopping criteria (i.e., 30 minutes, 1 hour and 10 hours) in this
experiment. Note: this is rather different from the typical amount of 24 hours used in fuzzing
literature. Furthermore, as an industrial setting with live services and databases involved (running
on Meituan’s testing environment), the testing could unfortunately be performed only once. As
EvoMaster uses randomized algorithms, this is a threat to validity.

T 4. To further investigate the effectiveness of EvoMaster with practitioners’ point of view, we
performed a manual analysis with a manager at Meituan on the results (such as covered code and
identified faults) achieved by EvoMaster in the two case studies.

T 5. Once the tests were generated, we conducted a questionnaire (QBs) on them with one manager

Table 4: Experiment tasks and participants for each RQ

RQs
RQ1.1
2
RQ2.1
2
3

RQ3
RQ4

Tasks Researchers

T1
T2
T3
T4
T5

T6
T7

×
(cid:68)
(cid:68)
(cid:68)
×

×
×

Industrial Practitioners
QAs(CS1 , CS2 ): 1 QAM
×
×
Manual Analysis: 1 QAM
QBs(CS1 ): 1 QAM, 2 QAE;
QBs(CS2 ): 1 QAM, 1 QAE
QCs, QDs: 1 QAM, 3 QAE
Interview: 1 DSQ, 1 PSE, 2 QAM

8

Set up

Resolve Dependencies

Write Driver

Run

Execute Tests

0

1.2 1.4 1.6 1.8
0: Very Difficult, 1: Difficult, 2: Moderate, 3: Easy, 4: Very Easy
Figure 2: Answers provided by the industrial partner about the difficulties on applying EvoMaster
(QAs)

3.2 3.4 3.6 3.8

0.2 0.4 0.6 0.8

2.2 2.4 2.6 2.8

1

3

4

2

and three engineers, for assessing the quality of these tests generated by EvoMaster, in views of
these industrial practitioners.

T 6. In addition, we also collected their answers with QCs and QDs regarding the potential integration

and existing problems they meet.

T 7. Last, to better understand and gain insight on industrial problems and challenges, we had
interviews with a director of software quality, a principal software engineer and two quality
assurance managers.

5 Experiment Results

5.1 Results for RQ1: Applicability

The goal of this RQ1 is to evaluate the applicability of EvoMaster in industrial settings. The
applicability is firstly assessed in the view of industrial participants, i.e., how difficult is to use the tool.
To use EvoMaster, besides a runnable jar file, there are also installers for Windows/OSX/Linux
since version 1.2.0, and the tool can be started with command line. In order to apply EvoMaster
for white-box testing, it requires a manual work for writing a driver to configure how to start, stop
and reset the SUT. However, documentation and examples are provided about how to get started
with EvoMaster. Given such documentation, we asked the manager from Meituan to try the fuzzer
with their services, then collect his answers regarding QAs. As the answers shown in Figure 2, in
general, he did not meet any difficulty in applying the tool and checking the outputs. Rates for
setup, writing driver, running the tool and executing tests were evaluated as Easy. For resolving
dependencies, however, he met some dependency conflicts when writing an EvoMaster driver for the
SUT. This required a manual fix, and therefore he rated this task as Moderate. In addition, based on
the experience of applying EvoMaster, our industrial partner considers that the manual configuration
of EvoMaster for writing the driver could be automated. Because their industrial APIs are built
with the same pattern, code for handling the APIs (such as starting, stopping and resetting) can
be templatized. This would promote EvoMaster integration into industrial settings, especially in
microservice architectures with hundreds of services.

To further investigate the applicability, we made a preliminary study on the industrial case studies.
First, we found that, since EvoMaster mainly addresses REST APIs and does not directly support
any RPC mechanism (e.g., such as Apache Thrift) yet, in order to employ EvoMaster on their
services, our industrial partner needed to implement an additional REST layer to link to the Thrift
functions. As they mentioned, building such links was rather straightforward as each REST endpoint

9

could map to one RPC function. However, the manual work with the additional layer might bring new
problems, and without a native RPC support, the effectiveness of EvoMaster might be limited.

Regarding execution environment, at Meituan, they had deployed a specific testing environment
where all services are up and running (which is a typical practice in industry). Within this environment,
a service under test is able to interact with all its required services (e.g., external services and databases
shown in Figure 1). As the fuzzer, in order to generate independent tests, EvoMaster requires a
proper reset of the SUT, e.g., clean modified data in databases, reset/manipulate states of related
services. But, in an industrial setting involving a large microservice architecture, it is not trivial to reset
the state of the SUT. First, the interacted external services are not controllable from the test cases,
which means that we cannot manipulate them to specific states before executing a new test. Regarding
the databases, as external services, they are also connected among the services, and a database might
be shared by multiple services. As discussed with our industrial partner, it is time-consuming for
them to completely isolate the required databases for a specific SUT. In addition, with their testing
environment, currently there is no solution that could enable a reset of databases programmatically.
With EvoMaster, a DbCleaner utility is provided to clean data in directly connected SQL databases,
but, due to SQL operation constraints managed by the RDS (e.g., TRUNCATE is disabled), the utility is
not applicable to these industrial case studies.

Regarding default setting of the fuzzer, EvoMaster integrates SQL handling to insert data
into the databases with defined SQL heuristics for test generations [14]. Considering such restricted
manipulation on databases, with inserted data that cannot be easily cleaned, then the states of the
SUT would be changed significantly over time during the test generation process. Also, since the
interacted database is not completely isolated, such insertions might lead to side-effect on their testing
environment (which is used as well by other development teams at Meituan). As discussed with the
industrial partner, we decided to disable the SQL handling of EvoMaster [14] for this study, as it
would take time to find out of to configure the RDS for this kind of testing process (as the RDS is
developed and maintained by a different team).

Furthermore, we found that, besides authentication restrictions, there also exist some restrictions
based on intra-service business logic. For instance, an endpoint in a SUT X might only accept requests
which contain specific information, and the specific information could be put by an endpoint in another
service Y. Thus, in order to access the endpoint in the SUT X, the endpoint in Y must be invoked
first, or the request must be sent by the endpoint in Y. Such info among services are handled in an
internal Tracer service developed by Meituan. Thus, in order to access the endpoints in testing, we
defined pre-actions to resolve such constraints in the case studies, and the invocation of the pre-actions
would be handled with a probability as authentication in EvoMaster, i.e., a probability to control
whether the request contains such required info.
RQ1: Applying EvoMaster was evaluated as an overall easy task. But, due to limited access to the
database and closely connected external services, there exist difficulties in resetting the state of the
SUT. In addition, EvoMaster lacks a native support for RPC used in the industrial case studies.

5.2 Results for RQ2 : Effectiveness

5.2.1 Results for RQ2.1

Figure 3 plots covered targets at every 5% budget used by the search for the two applied case
studies, with search budgets of 30 minutes, 1 hour and 10 hours. With the figures, for the two case
studies, results achieved by 10h clearly outperform results with 30 minutes and 1 hour. Comparing
results between 30 minutes and 1 hour, their performances are close, and in CS1 , the budget with
30 minutes achieves even better results than the 1 hour budget. Considering the random nature of
search algorithms, it could happen by chance, but it also reveals that probably there is a lack of
exploration of the search landscape with the small budget. Recall that, due to the resource constraints,
we could run the experiments only once. However, results are in line with previous studies where
EvoMaster was applied on open-source software.

Applying search for many objectives such as test generation, it is essential to explore the search
landscape at early stages for providing diversification of individuals. Then, such individuals could

10

(a) CS1

(b) CS2

Figure 3: Average covered targets (y-axis) with 30m (green line), 1h (red line) and 10h (blue line)
throughout the search (RQ1) for 2 industrial applications, reported at 5% intervals of the used budget
allocated for the search (x-axis)

enable a further possibility to cover new targets, e.g., code coverage and fault finding in our context,
in the later exploitation stage. Therefore, with automated testing approach such as EvoMaster, a
longer budget would be required, i.e., more than 1 hour, to achieve better results. However, a too long
search budget (e.g., 24 hours) might be not viable, if engineers are not willing to wait so long to get
results. But this also strongly depends on whether tools like EvoMaster would be typically run on a
developer machine, or on a remote dedicated CI server. Also, it depends on whether the fuzzing is
done during a regular development day, or before a major software release (this latter would like entail
spending a longer time for testing).

To study the performance of EvoMaster, we applied the generated JUnit tests with 10 hours
and executed them on the two case studies with IntelliJ IDEA [4], to collect the code coverage metrics.
Table 5 reports overall code coverage (denoted as %Lines), and also coverage for three layers used
in the industrial applications, i.e., Client, Data Access Object (DAO), and Server. Client mainly
defines exposed endpoints with Data Transfer Objects (DTO), Server implements main services and
business logic, and DAO manages the communications with the databases. Based on the coverage
shown in Table 5a, we found that our approach achieves competitive results (compared to current
testing practices employed at Meituan) on Client and Server (i.e., 62.8% and 46.3% for Client and
Server in CS1 , and 79.8% and 57.1% for Client and Server in CS2 ), but the performance on DAO is
limited in both case studies.

We also compared the performance on code coverage with existing manual written tests, when
existing. Note that testing is mainly performed manually starting from the user side involving many
services for processing the user request (see Figure 1), e.g., perform user requests as real business
scenarios. Thus, it is not required to have manually written tests (e.g., JUnit tests) for each service.
In this study, there are 11 manually written tests for CS2 which achieve 16.8% line coverage. Our
generated tests show clear better results than the existing manually written tests.

5.2.2 Results for RQ2.2

To have a deeper understanding about the results on the industrial case studies, we performed a
manual review on the generated tests and coverage reports with the industrial partner, together. By
reviewing the tests, the industrial partner found that, firstly, the tests outputted by EvoMaster are
quite useful for them from three perspectives, and the results are reported in Table 5b:

11

6000700080009000515304560759010h1h30m28502900295030003050515304560759010h1h30mTable 5: Results of tests generated by EvoMaster with 10h for each SUT

(a) number of generated tests and code coverage

SUT #Tests %Lines (Client, DAO, Server)

CS1
CS2

231
64

33.5% (62.8%, 10.9%, 46.3%)
32.6% (79.8%, 15.3%, 57.1%)

(b) fault detection

SUT #InputValidations #Assertions #Exceptional #RealUniqueFaults (#C)

CS1
CS2

357
151

34
6

17
16

34 (10)
18 (11)

- #InputValidations is the number of test actions which perform an input validations on the SUT; #Assertions is the
number of successful requests with useful assertions in the generated tests; #Exceptional is the number of tests which
cover critical exceptional branches; #RealUniqueFaults is the number of unique faults revealed by us and industrial
partner, where #C is the number of the faults which are potential critical identified by industrial partner.

1. Input validation branch coverage: the generated tests are capable of covering various input
validations of the services, and their combinations, which are rarely covered with manual testing
and manually written tests as their testing mainly addresses business scenarios;

2. Successful requests and their assertions: EvoMaster is able to generate successful requests to
the SUT, and have complete assertions with respects to the responses that could be not possibly
achieved (way too time consuming) by manually written tests when the elements in the response
are many (e.g., large and complex JSON objects);

3. Exceptional branch coverage: the tests also enable covering different critical exceptional scenarios
such as invalid operations on resources at a given state, authentication, or time related constraints
on operations (e.g., exceptions regarding too frequent operations on the endpoints).

In addition, our tests are helpful for finding real faults by checking tests with 5xx status code, and
returned responses based on given inputs. With 5xx status codes, we found real faults related to
NullPointerException (which have been fixed now by the developers at Meituan). Some other faults
are related to external services that they would need a further investigation, e.g., there might be an
improper handling on updated external services, and the external services are handled by different
teams. Moreover, there exist some tests which return successful responses (e.g., HTTP status code 200)
but the given inputs are actually not correct (i.e., a status code in the 4xx family should had rather been
returned, or at least an error message). By further investigating the problem with the QA manager at
Meituan, this is due to lack of validation and verification on input parameters achieved by the SUT
that needs to be improved However, this points out an important aspect of automated test generation:
the generated tests should be easy to read and understand, because not all faults lead to program
crashes, and reviewing the generated tests can lead to spot this kind of faults. Furthermore, there are
some improper messages in the responses, which are not in accord with their response definitions.

In Table 5b, we report the number of unique faults revealed by the manual review with the
industrial partner (#RealUniqueFaults), where #C is the number of potentially critical faults for
the industrial partner that they will fix or at least investigate in details, i.e., excluding the problems
related to improper responses.

During the review, we also found some limitations of the current version of EvoMaster. From
the point of view of the industrial partner, the tests for each single endpoint are useful, but there lacks
of meaningful method sequence combinations for covering their business logic. Such limitations are
due to two main reasons. First, we do not have a native support to Apache Thrift applications. The
additional proxy layer using REST only bridges a REST endpoint to a Thrift endpoint, and so does not
have a further handling on the HTTP responses. Thus, the status code of HTTP responses are typically
either 2xx or 5xx (when there is a crash), as properly handling 4xx would be time consuming (i.e.,
manually validating each input parameter). Such setting would limit EvoMaster’s fitness function

12

%
e
g
a
t
n
e
c
r
e
P

Reability Quality

80

60

40

20

100

0

0

0

0

0

0

VeryLow

Low

Moderate

High

VeryHigh

(a) QB1 and QB2

0

No

(b) QB5

Yes

Figure 4: Answers provided industrial partner about tests generated by EvoMaster

regarding status code coverage. In addition, EvoMaster identifies 2xx status code as successful
requests, then it could possibly combine a failing (but with status 200 instead of 4xx) POST creation
with a GET query with the sampling strategies for REST. Moreover, the fitness function on targets
to maximize by EvoMaster are currently based on code coverage, status code coverage and fault
detection. But in order to cover various business scenarios, such targets would not be sufficient, i.e.,
more code coverage might not result in covering more business scenarios. In future, we plan to define
further business scenario related metrics into the fitness function to tackle this limitation.

We made an investigation on uncovered code, and have a further discussion with the industrial
partner. First, as seen from Table 5, achieved coverage on DAO is low. One possible reason for the low
coverage in DAO is that we disabled SQL heuristics in these experiments, as discussed in Section 5.1.
Thus, without any heuristic on SQL, it might result in such underperformance in DAO. In addition, we
also found that there exist some unreached classes in DAO which define SQL operations (e.g., SELECT
ALL, DELETE BY WHERE), and those classes seem never been used (i.e., dead-code). As checked with
the developers at Meituan, those classes are actually automatically generated by their development
framework. In addition, they also pointed out that, in their services, there possibly exist some other
unused code which is not deleted due to their frequent updated services. With our current competitive
coverage on Client and Server, as an extra benefit of using a tool like EvoMaster, they could further
find such old, no-longer used code and remove it. Moreover, some uncovered code is related to the
managing of RPC request queues. Currently, EvoMaster does not properly handle asynchronous
and concurrent aspects, which could limit its effectiveness on such type of code. Furthermore, code
branches related to the interaction with external services are difficult to be covered.

5.2.3 Results for RQ2.3

Besides the manual review, we also conducted a questionnaire (QBs) about the generated tests, given
to four industrial participants whom are familiar with the applied case studies. As shown in Table 4
(see T5), we collected in total 5 answers, i.e., 3 for CS1 and 2 for CS2 . For QB1 and QB2, results
with five rating scale (i.e., Very Low, Low, Moderate, High, Very High) on readability and overall
quality are represented in Figure 4a. Regarding the readability, we received 3 rates (60%) on Low and
2 rates (40%) on High. All Low rates are about CS1 , and all High rates are about CS2 . For CS1 , 231
tests (see Table 5) are hard for the developers/testers to read and recognize meaningful tests in them.
This could be a main reason for such negative feedback on readability, as feedback on 54 tests for CS2
are positive. In terms of overall quality, we received 4 rates (80%) on Moderate and 1 rates (20%) on
VeryHigh.

13

QB3: How can the generated tests be improved? Answers about further improvements covers two
aspects: (1) readability: currently, tests are mainly named with a number or HTTP status code. It
would be better to have some endpoint info on the names, which could help them to read the tests; in
addition, there exist some quite large body payload on the requests and assertions on responses. To
improve readability, it might be better to put such heavy info into a separated file, e.g., JSON. (2)
assertion with respects to database: the current assertions in the tests are based on the responses in
the HTTP requests. However, it would be better to have some assertions directly on the content of
the databases. For instance, an action is performed to create a resource, then it would be better to
check whether data corresponding to the new resource is inserted into the database. Using an HTTP
GET to fetch such data from the SUT would not be enough, as such data could had been just cached,
and not necessarily saved into the database.

QB4: Describe what you like better about manually written tests than generated tests? One
advantage of manual tests is easy-to-read. Furthermore, the manual tests are defined to address their
business logic, and that has more meaningful combinations of endpoints than automatically generated
tests.

QB5: Would you keep the generated system-level tests? and QB6: if yes, how would you like to
keep and use them? Regarding QB5, 100% of the respondents (see Figure 4b) show positive feedback
in keeping the automatically generated tests. They would like to further use them for testing input
validations and endpoints used to query info from the other services. In addition, they also want to
apply them for regression testing (i.e., when the services are updated).

RQ2: As a search-based approach, i.e., EvoMaster, a relatively higher budget (e.g., 10h) is
required to tackle industrial test generation. Regarding its effectiveness, with 10 hours’ time budget,
EvoMaster achieves competitive code coverage on key implementations in industrial applications,
outputs useful tests covering scenarios that are not easy to be handled by manually written tests and
manual testing, and is capable of identifying real faults. With answers of questionnaires from four
industrial practitioners on two industrial case studies, quality of generated tests are rated as either
Moderate (80%) or VeryHigh (20%), and 100% participates would like to keep the generated tests
and further use them. But, there also exist some limitations, i.e., business logic coverage, test
readability and assertions on database state.

5.3 Results for RQ3: Integration

QC1: What are the major barriers from your point of view in adopting the EvoMaster tool? First,
considering the useful results achieved by EvoMaster, developers/testers at Meituan are willing
to integrate our tool in their development and testing processes. But, since EvoMaster does not
have a native support for Apache Thrift, this requires a manual configuration between Thrift and
REST. This would be a major adoption barrier from their point of view. In addition, considering
that they do not have a viable solution to reset their databases yet, in order to generate independent
tests with EvoMaster, they would like to develop such database reset operation from their side
(as they use their own customized RDS). Moreover, from the view of one of the principal software
engineers/managers, there might exist a learning curve for their testers to adapt a new technique and
a need to extend EvoMaster test case writer to adopt their testing framework. But he does not
consider them as major barriers.

QC2: Given your current infrastructure setup, how would you like to have automated system-level
test generation framework integrated? Considering their development environment, the preferred
option for the industrial partner is to adopt EvoMaster into their CI pipeline, e.g., when a Pull
Request (PR) is started. But it also depends on the required time cost of EvoMaster. If EvoMaster
requires more than 1 hour, they would firstly integrate it in an offline manner, e.g., set up to run
EvoMaster nightly locally.
RQ3: The main barrier to adopt EvoMaster into the industrial development process is its current
lack of a native support to RPC. The preferred option to have EvoMaster integrated is with CI,
but how to apply it would depend on time cost that needs to investigate.

14

5.4 Results for RQ4: Existing problems and challenges

The research task for RQ4 aims at understanding existing testing difficulties in industry, and discuss
potential solutions that tools like EvoMaster could help to tackle.

QD1: What is one of your current major issues/time consuming activity with manual testing
that you would like to have automation for? Currently, their testing tasks are mainly performed
manually, e.g., manually written tests, a black-box testing by performing users’ behaviors from client
sides with UI, or analyze the SUT by replaying its historical execution. All these manual tasks are
time consuming. Based on the structure of their services and their business logic, they are willing to
promote automation testing by considering at the level of APIs, business logic, and UI. Besides, test
preparation such as database and external services setup is very time-consuming for them. Hundreds
of the connected services are running on one platform, and the developers in a team typically do not
know the details of services which they are not responsible for. Thus, when they perform testing on
one SUT, they possibly misconfigure some external services or lack the needed related data prepared in
the database. They would like to have some kind of automated recommendations for possibly related
services and database state for the SUT, which could help their testers to setup their testing scenarios.
QD2: What kinds of faults are harder to detect in the system? Based on their experience, the hard

to detect faults are related to:

1. data in the database: the services might perform some analysis on the existing data in the
database, then results of the analysis could be used by other services. In this case, if there exist
some dirty/invalid data in the database, it could crash the other processes.

2. associated services: in order to test the system regarding a business logic, there could be multiple
services under test. An error might occur in one service (e.g., wrong data saved in database) but
the fault might manifest only in other services (e.g., when reading such data, and expecting it to
be correct). Considering the complexity of the whole system (hundreds of services), locating this
kind of faults is hard.

3. dependent frameworks: to build their platform, they also used software frameworks, and these

frameworks might have bugs which could result in errors of their services.

4. non-determinate faults: since some faults occur non-deterministically, it is hard for them to
reproduce them and fix them. This is a common issue in the testing of complex distributed
systems.

5. interaction with external environment: Meituan’s business has close interactions with humans
(who are responsible for diverse tasks, e.g., quality inspector, storekeeper) and psychical devices
referred as System Actors. The testing is performed typically with an assumption that the
System Actors operate properly. However, when it is not true, then some faults would only be
detected once the services are run in production.

QD3: What are the most important challenges that you meet in testing? Based on the business
they deal with, starting from a user request, there could go through lots of services that could result
in very complex combinations of various inputs and different states of the system. Considering such
complexity, first, it is hard to define the testing scope regarding the services. In addition, some
combinations could be reached only under certain states of the system. To generate and explore such
states is also challenging. Currently, as mentioned by a principal software engineer who is responsible
for developing the testing tools of Meituan, their team would need to prepare data into their testing
environment based on real data from their services in production. Such data preparation in the
database usually takes at least four hours each week. Moreover, their platform has a high demand
for time-efficiency and security (e.g., payments) that would bring further challenges in testing. For
instance, their database services are accessed by many services, and there could be slow SQL queries in
one service that would negatively affect the database services, and so result in failures (e.g., timeouts)
in other services. Furthermore, how to better test scenarios which is related to external environment
is challenging for them, as discussed also in QD2. Last but not least, due to the rapid increase of
business requirements, often there is limited time for performing manual testing.

15

RQ4: Most of testing tasks are performed manually for testing the business logic which shows an
urgent need for effective automation support. Due to complexity of microservices and business
features, there exist various challenges on, e.g., locating faults, defining test scope, handling external
services and databases, preparing test data, and testing under uncertain environment (such as
humans).

6 Lessons Learned

6.1 Testing setup

In this study, the first challenge we faced was to set up our tool in the industrial environment (as
discussed in Section 5.1). Unlike the open-source case studies (like the ones collected in EMB [2]), in
industry when dealing with microservice architectures, relationships among services are more complex,
accesses to databases are more restricted, and data in the database is more sensitive. Thus, it is hard
to handle all dependent resources and enable a proper reset of automated test generation in such
industrial setting.

For external services, there could be a possibility to handle them with mocking techniques, and
further manipulate responses with search from mocked services to maximize the coverage of testing
targets (e.g., code coverage and fault detection). However, considering the complexity of these
dependencies, it would be challenging to setup them properly in an automated way. Furthermore,
this would further enlarge the search space for test case generation. Note that these issues are not
specific to just EvoMaster, but they would likely apply to any fuzzer used in this testing domain
(i.e., microservices).

Regarding the reset of databases, as discussed with the industrial partner, they could help us to
isolate complete databases for testing with EvoMaster. As the first setup, they could provide empty
databases with an interface to clean all data. Thus, before executing every individual, we could reset
the SUT to a certain state for generating independent tests at the end. However, based on the answers
collected during the survey and interviews, it might not be sufficient to generate tests starting with
empty databases. Considering the existing data in the database, they are intentionally added for
covering their business scenarios in real practice. Table 1 shows the number of rows of existing data in
the databases the SUTs directly interact with in their testing environment. For CS1 , there exist 256
024 rows. If the testing is involving many services, it is impractical to clean all data and re-add them
before every single test. To better adopt EvoMaster into this kind of industrial setting, there would
be a challenge about how to utilize such existing data in the fuzzer (e.g., during test sampling) that
could further result in a good coverage on real scenarios. This would be related to the data preparation
challenge discussed in Section 5.4, and have a proper reset of SUTs.

6.2 Testing criteria

With this study, we found that it might not be sufficient to define testing targets only with code
coverage, HTTP response status coverage and fault detection. For solving industrial testing problems,
besides code coverage and fault detection, there would be a need to enhance the fitness function from
more dimensions, e.g., business logic, time constraints, database performance and system security, and
how to measure such dimensions and cope with them together is very challenging.

When analyzing the industrial services for the experiments in this paper, we found that, in order to
understand and monitor their system behaviors, the services are built with various monitoring features.
For instance, regarding the business logic, there exist a tracing infrastructure (named Tracer) inspired
by Google Dapper [45] that enables tracking a complete path taken through every services from a
user request, as the example in Figure 1. With the code instrumentation done in EvoMaster, such
tracked paths could provide additional info for measuring coverage related to the business logic. As a
timely concern on database performance from our industry partners, it would be important to further
enhance the fitness function with respects to SQL execution time for exploring slow SQLs in the SUT.

16

6.3 Fault locating

In microservices architecture, a request from the client could go through a long and complex list of
services. The service where a fault is observed might not be the root of the fault, which could be in any
interacted service with respect to this request. Thus, locating the root of the fault is often challenging.
In the context of white-box testing, EvoMaster manages to refer a last executed statement for an
identified bug, i.e., 500 status code, when testing a single web application. However, to better locate
faults in microservice architectures, more info would be required.

In addition, currently EvoMaster only identifies status code 500 as a bug (as well as discrepancies
in the OpenAPI schema). However, there could possibly exist other bugs which result in non-500
status code, e.g., 200 in this study. To help locating faults, it is required to have a more comprehensive
study on faults in microservices with various aspects (e.g., security) and define more intelligent rules
to identify the faults. Moreover, non-deterministic faults often appear in microservice architectures,
due to the nature of distributed systems. Characterizing the causes of such non-deterministic faults
(such as [35]) is required to be investigated in the context of microservice architectures.

6.4

Integration

RPC is widely used in industry, especially for intra-communications among services in microservice
architectures. However, to the best of our knowledge, no technique or tool in the literature handles
the fuzzing of RPC services. A native support to RPC would be required for enabling further
EvoMaster integration into industrial settings. In addition, RPC-based APIs could be built with
various frameworks, such as gRPC [3], Thrift [6], and Dubbo [1]. A solution generalizing to various RPC
frameworks will bring lots benefits to its industrial application. However, designing such generalized
solution is also challenging.

To use EvoMaster, testers and developers would prefer an adaption of EvoMaster to their
development environment, i.e., CI. To better apply research outputs to industry, building such type of
tooling would be helpful [25]. But, in industry, time cost would be an important metric for automated
testing tools. Considering time restriction in practice, current required time cost by EvoMaster
might limit its application on the CI (e.g., when dealing with Pull Requests). One possible solution is
to reuse generated tests. For instance, in industry, code is updated frequently, then tests generated
by latest commit could be possibly re-used afterward for a following update. Instead of starting from
scratch, we could possible utilize previously generated tests for enabling a better starting point for the
search [44]. This could reduce the time cost of the search.

Another possible solution could be to reduce or prioritize the testing targets. For instance, instead
of maximizing code coverage of the whole project, we might focus on the code coverage with respect to
modifications and track generated tests across commits (such as differential regression testing [29]).
Moreover, by reviewing the generated tests, we found that, for different services, there might exist
different rules to define successful/failed requests in responses when dealing with RPC-based services.
In order to adapt our response coverage to such services, we might further provide an interface in the
Driver for specifying rules in responses to define successful/failed requests.

6.5 Assertion and readability

Our current strategy to produce assertions in the generated tests is based on HTTP responses. However,
as testing of an industrial application, test assertions would be required to be more comprehensive. As
pointed out by our industry partner, we plan to firstly extend our assertions with respect to databases.
In addition, with this study, we found that some content in responses are non-deterministic. A proper
handling is required for producing assertion to deal with such non-deterministic content, to avoid
generating flaky tests.

One major concern from testers/developers about automatically generated tests is readability. A
more readable output would help them to identify problems and locate faults. EvoMaster integrates
a test clustering technique for splitting tests into different files with respect to faults in the context of
REST APIs [39]. First, a further handling for RPC would be needed. In addition, considering hundreds
of tests as CS1 , more clustering strategies would be required, e.g., splitting test suites grouped by

17

endpoints. Moreover, we could also simplify the generated tests by moving over-informative inputs
and assertions into a separate file, as suggested by some of the testers at Meituan. But whether such
move would have side-effects on debugging will need to be investigated. Furthermore, test readability
can be one of the objectives to optimize for [26].

6.6 Unit Testing Tool

In this study, questionnaire we designed (with QAs, QBs and QCs in Table 3) covers all questions
from an industrial study on unit test generation [7]. This was done to enable meta-analyses.

Compared with the industrial feedback on the unit testing tools, both test automation tools (such
as EvoSuite and EvoMaster) obtain positive feedback on usability (QAs), i.e., easy task to use it.
Regarding the generated tests (QB1 --QB5 ), the unit testing tool received more negative answers on
inputs and assertions than EvoMaster. This could happen because, in the context of system testing,
combination of inputs for validation and responses dealt with are more complex than the unit testing.
Having more sufficient testing on input validation is also important for web services, and manually
creating such combinations would be impractical. Regarding assertions, currently EvoMaster utilizes
the HTTP responses to create assertions. The responses in web services are typically defined with
certain purposes, e.g., status code and info messages, while such meaningful responses are often not
applied in unit testing. In addition, both tools were evaluated as needing to have an improvement on
readability of the generated tests, which is also related to the size of the generated test suite files.

Regarding QB5: Would you keep the generated system-level tests?, most of the answers for unit
testing is No, but we received 100% Yes answer. This result could be related to negative feedback
on the generated tests by the unit testing tool. In addition, compared with system testing, for unit
testing it would be easier and less time-consuming to create unit tests manually, and that could result
in the preference of developers/testers on manually written tests. Regarding the integration (QC ),
both surveys collected the same preferred option with CI, and the major barriers are both related
to currently applied frameworks. However, based on the answers collected with our questionnaire
and interviews, our industry partner showed positive attitudes to integrate EvoMaster into their
development process (even if it is not integrated with CI yet), and help us to bridge interfaces (e.g.,
reset database) for enabling EvoMaster with their current testing environment.

Due to the different complexity of the addressed problem compared to manual testing, the system
testing automation tool (such as EvoMaster) received a more positive feedback and shows a more
promising integration than unit testing generation. However, this is based on only two studies in
industry, and more would be needed to be able to generalize this claim.

7 Common Challenges

EvoMaster is an open-source SBST fuzzer for automated testing of web services, such as REST and
GraphQL APIs. We are authors of this fuzzer. In the context of REST API testing, EvoMaster has
been studied by different groups [33, 53] to compare it with various existing open-source fuzzers on
open-source REST APIs. Results of these studies demonstrate that EvoMaster can be considered
as the most performant tool in fuzzing these selected open-source APIs. However, to the best of our
knowledge, there is no study performed to evaluate white-box fuzzers in industrial settings for Web
APIs. Therefore, our goal in this study is to bridge such gap by investigating challenges that academic
researchers might meet in applying fuzzers (such as EvoMaster) in industrial settings and existing
challenges industry meet to be able discuss potential solutions the research community could tackle.
In addition, regarding the subject of study, it is a large-scale e-commerce system which consists of
hundreds of microservices, covering various e-commerce business scenarios, such as ordering, warehouse
management, transportation, delivery and payments, developed by Meituan Select. As the first attempt
to apply EvoMaster on their industrial settings, our industrial partner selected two services which
are parts of their microservice architecture, closely interacting with other services and databases (recall
Table 1), and chose eight employees to be part of this study. With such real industrial settings,
additionally considering the performant results achieved by EvoMaster, lessons learned in this study
are not only limited to EvoMaster, but also might apply to other fuzzers for their further industrial

18

applications. Therefore, in this section, we summarize important common challenges (denoted as C-#)
which Web API fuzzers might face in industrial settings, along with potential solutions and our future
work.

C-1 To fuzz industrial Web APIs, besides generating inputs and requests to the SUT, the fuzzer
requires to be capable of manipulating responses of the directly interacted services and databases.

Solution: Mocking techniques are a suitable solution to handle external services. However, how
to enable it as part of the fuzzer is a challenge that the research community needs to
address.

C-2 To enable an automated Web API testing, it requires to reset states of the SUT. However, enabling
such reset is very challenging in industrial settings. For instance, databases in industry are very
complex and large, e.g., 200k rows for the test data in CS1 . It is impractical to reset such database
at every test execution.

Solution: As discussed with our industrial partner, they plan to implement a solution to flashback
states of the databases based on timestamps, as they think such solution is also useful
for them for their other testing tasks. However, the time efficiency of the flashback is
of paramount importance, e.g., 1 second could result in at most 3600 requests within 1
hour, which would significantly limit the effectiveness of the fuzzer.

C-3 In industry, testing data in the databases is typically maintained based on data collected in real
production for testing their APIs with various business scenarios. Besides resetting databases,
considering the amount of data in these databases, how to make full use of them is another
challenge.

Solution: EvoMaster is equipped with sql handling to analyze existing data of SQL databases
with JDBC connection. As a white-box SBST fuzzer, it is possible to enable heuristics
for optimizing selection and manipulation of existing data which requests relate to.
However, considering the large amount of data, how to optimize it in a time effective
way would be a potential challenge we might face.

C-4 An industrial Web API could connect multiple databases using database sharding techniques,
which is a common solution for distributed databases in industry. Thus, fuzzers are required to
be able to handle multiple connections in their database handling solutions.

Solution: EvoMaster enables SQL database handling, but such handling is currently built
based on one JDBC connection. EvoMaster could be extended to support multiple
JDBC connections, but how to cope with the multiple connections when inserting data
(e.g., determinate a database to perform the insertion) will be another challenge we
need to address.

C-5 Common testing criteria might not be sufficient to tackle industrial problems. As discussed with
our industrial partner, they concern more on metrics relating to their business logic, efficiency,
performance and security. Such dimensions are likely also common to other industrial APIs. How
to measure these dimensions as testing criteria and deal with them during fuzzing APIs would be
an important challenge that research community should address.

Solution: Currently EvoMaster does not consider performance testing criteria, such as response
time, which could be considered as future work to extend EvoMaster for performance
testing. However, such info has been collected during search. As requested by our
industrial partner, EvoMaster now can trace and output logs of all SQL commands
executed during testing. Then, with this info, our industrial partner can conduct
further performance analysis on their databases based on these logs.

19

Solution: (specific to industrial microservices equipped with monitoring system) As a white-box
fuzzer, it is possible to track paths of accessing APIs at runtime by instrumenting
services for monitoring microservices, if the SUT has any such connections. Then, the
tracked info might enable identifying business logic and further employ them as parts
of fitness function of EvoMaster.

C-6 Due to close and complex interactions among Web APIs and databases, locating faults in

microservices is a challenge industry currently meets.

Solution: To better locate faults, it is the first step for researchers to understand and characterize

faults in microservices, which is going to be an important future work.

C-7 Industry needs an automated solution of fuzzing RPC-based APIs. However, it has not been
addressed yet by research community. We are now in the process of enabling such native support
of fuzzing RPC-based APIs.

Solution: Besides the Thrift framework our industrial partner employs, supporting other RPC

frameworks (e.g., gRPC) is a primary step to address this challenge.

C-8 Time is essential in industry. Besides improving time efficiency of the fuzzer, it is also important
to investigate how to better apply the fuzzer during the industrial development processes (e.g.,
different time budgets for different development actions).

Solution: To tackle this challenge, there is a need to conduct an empirical study in real industrial
development processes with various development actions (such as after a pull request
or before publishing a new feature), that is going to be our future work.

C-9 Industrial Web APIs often employ sources of non-determinism in their implementation (such as
calling Random or accessing current CPU clock) that can result in flaky tests. To avoid producing
flaky tests, it is important that fuzzers can properly handle such sources of non-determinism in
the generated tests.

Solution: As a white-box fuzzer, EvoMaster could take advantage of directly handling non-
determinism in the source code (e.g., instrumenting a fixed seed for Random). However,
it requires to first identify all possible non-deterministic sources in the source code.

C-10 Hundreds of tests could be generated when fuzzing industrial APIs, such as 231 tests for CS1 . Note:
a fuzzer could evaluate millions of tests during its search, but the output is usually minimized,
e.g., generate minimal test suites with the highest coverage achieved during the search. To better
help tester use these generated tests (e.g., for debugging and regression testing), readability of the
tests is important in industrial practice.

Solution: Classification is one potential solution to improve readability of the tests, such as
group tests based on success or failure responses into different files. The tests with
failure in responses could be further classified into different files based on types (such
as user error or system error). To enable such classification, a study for understanding
faults in microservices is required to conduct first. EvoMaster has been equipped
with a clustering strategy for tests of REST APIs. However, a strategy specific to the
domain of RPC-based APIs is needed.

8 Threats to Validity

Conclusion Validity. In this study, we employed questionnaire and interview methods (as shown in
Table 3) to collect feedback from industrial professionals in software testing, with different roles (e.g., a
director, QA mangers and engineers), from in total eight participants. Thus, we could collect feedback
and answers in terms of different viewpoints as discussed in Section 4.3. As shown in Table 4, as the
authors of EvoMaster, we independently performed two out of the seven tasks in this industrial

20

evaluation, i.e., T2 : a preliminary study of EvoMaster on industrial case studies (such as database
and authentication handling) and T3 : coverage analysis based on generated tests. To reduce bias
in the analysis of the results, in T4, we together with our industrial partner performed a manual
analysis in the generated tests for assessing the effectiveness of EvoMaster. For the other four tasks
conducted with questionnaire and interviews, all answers are from the eight employees at the Meituan
(Table 4). In addition, for each question, except QAs, we received at least four answers from at least
two profiles in industry (Table 4), which reduces bias in the results. Regarding QAs, it required to take
time to perform some specific tasks. However, considering the tight schedule of engineers in industry, it
is difficult to involve more human resources for performing this kind of tasks for research experiments,
like in this study. In addition, based on the experience of using EvoMaster, our industrial partner
considers that the manual configuration of EvoMaster (such as writing drivers to handle the SUTs)
could be automated. This is because their industrial APIs are built with the same pattern. Handling
of the APIs (such as starting, stopping and resetting) can be easily templatized. Thus, for them, there
is no need to evaluate manual efforts to apply EvoMaster.

As the industrial services need to interact with live services and databases and their required test
budgets, the testing on such services could unfortunately be performed only once. Considering the
randomness nature of the search algorithms employed by EvoMaster, this might be a threat to
validity. However, in industry, such testing would be only deployed with one run, and the study on
outputs by EvoMaster was conducted closely with our industrial partner, e.g., a manual review of
the coverage report and of each generated test.

Internal Validity. There is no guarantee that the implementation of EvoMaster is bug-free.
However, EvoMaster has been used to perform many experiments using open-source case studies [12,
14, 16, 51], and it is also carefully tested, i.e., currently with 652 test suites (including unit tests
and end-to-end tests), which covers 60.65% lines of its code base. In addition, EvoMaster is an
open-source project which can be reviewed and examined by anyone who is interested in it.

External Validity. The study was conducted with two industrial APIs from only one company,
which might be a threat to generalization of our results to other companies. For researchers, this
is a typical problem, as it is hard to have an opportunity to transfer academic outputs to industry,
and find more companies to conduct such type of empirical study. Furthermore, this kind of analyses
involving human subjects are time consuming, much more than for example evaluating a fuzzer only
‘‘in the lab’’ on a set of open-source APIs. Also, there is a major difference between empirical studies
with students and empirical studies with practitioners in industry, as their cost is much higher, and
so usually their sample size is smaller. However, this kind of studies are essential to address the gap
between research and practice. Many studies done only ‘‘in the lab’’ by researchers might rely on
assumptions that do not hold in practice, making the use of novel scientific techniques not fit for
practice. A scientific result with no practical use might not be fitting for an engineering discipline like
software engineering. Reporting experience studies in different companies can help building a large
enough body of knowledge throughout time, from which meta-analyses can be use to infer general
results. To this aim, we used the same type of questions from an existing work on the application of a
unit test generation tool in industry (Section 6.6). However, two studies alone are not enough yet to
draw general conclusions. More studies of this kind are in dire need in the literature.

9 Conclusion

It is very challenging to test enterprise applications using a microservice architecture, as they are
often composed of a large amount of web services, such as the ones developed at Meituan. Automated
techniques such evolutionary search have demonstrated their effectiveness on solving many testing
problems. However, there is a lack of empirical evaluation in industrial microservices.

EvoMaster is an open-source test case generation tool that exploits the latest advances in the
field of Search-Based Software Testing for web services. In this paper, we conducted an empirical study
on integrating EvoMaster into real industrial settings. The study was firstly performed by evaluating
the effectiveness of EvoMaster on two services from Meituan Select, in terms of code coverage
and fault detection. Besides, a questionnaire and interviews were carried out with the engineers

21

and managers at Meituan, to further assess the applicability, usability and potential integration of
EvoMaster in industrial settings and understand the current testing challenges practitioners are
facing.

Results show that EvoMaster clearly demonstrates its benefits for practitioners in industry, e.g.,
unused code identification, test generation, code coverage, and real fault detection. But, there are
still many critical challenges posed in this study that are required to be investigated by the research
community, like how to deal with RPC services, interactions with external services and large distributed
databases.

EvoMaster is in active state of development. Since this pilot study at Meituan was carried out
in 2021, we have been already in the process of supporting RPC directly. Once the major challenges
pointed out in this study will be addressed, it will be important to repeat these surveys/questionnaires
with industrial practitioners, to see what next needs to be addressed before this kind of fuzzing
techniques can become commonly used in industrial practice.

Many studies done only ‘‘in the lab’’ might rely on assumptions that do not hold in practice, making
them unusable for industry. This paper gives the important scientific contribution of empirically
evaluating a Web API fuzzer in industry, confirming its applicability and usability in this domain.
Most of the challenges identified in this study are not specific to EvoMaster, and would likely apply
to any other fuzzer for Web APIs.

EvoMaster is open-source, with each release automatically stored on Zenodo for long term storage
(e.g., current version 1.5.0 [18]). To learn more about EvoMaster, visit our website www.evomster.org

References

[1] [n. d.]. Dubbo. https://dubbo.apache.org/en/.

[2] [n. d.]. EvoMaster Benchmark (EMB). https://github.com/EMResearch/EMB.

[3] [n. d.]. gRPC. https://grpc.io/.

[4] [n. d.]. Intellij IDEA Code coverage. https://www.jetbrains.com/help/idea/code-coverage.html.

[5] [n. d.]. OpenAPI/Swagger. https://swagger.io/.

[6] [n. d.]. thrift. https://thrift.apache.org/.

[7] M Moein Almasi, Hadi Hemmati, Gordon Fraser, Andrea Arcuri, and Janis Benefelds. 2017. An
industrial evaluation of unit test generation: Finding real faults in a financial application. In
International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP).
IEEE, 263--272.

[8] Nadia Alshahwan, Xinbo Gao, Mark Harman, Yue Jia, Ke Mao, Alexander Mols, Taijin Tei, and
Ilya Zorin. 2018. Deploying Search Based Software Engineering with Sapienz at Facebook. In
International Symposium on Search Based Software Engineering (SSBSE). Springer, 3--45.

[9] Andrea Arcuri. 2017. RESTful API Automated Test Case Generation. In IEEE International

Conference on Software Quality, Reliability and Security (QRS). IEEE, 9--20.

[10] Andrea Arcuri. 2018. EvoMaster: Evolutionary Multi-context Automated System Test Generation.
In IEEE International Conference on Software Testing, Verification and Validation (ICST).
IEEE.

[11] Andrea Arcuri. 2018. Test suite generation with the Many Independent Objective (MIO) algorithm.

Information and Software Technology 104 (2018), 195--206.

[12] Andrea Arcuri. 2019. RESTful API Automated Test Case Generation with EvoMaster. ACM

Transactions on Software Engineering and Methodology (TOSEM) 28, 1 (2019), 3.

22

[13] Andrea Arcuri. 2020. Automated Black-and White-Box Testing of RESTful APIs With EvoMaster.

IEEE Software 38, 3 (2020), 72--78.

[14] Andrea Arcuri and Juan P. Galeotti. 2020. Handling SQL Databases in Automated System Test
Generation. ACM Transactions on Software Engineering and Methodology (TOSEM) 29, 4 (2020),
1--31.

[15] Andrea Arcuri and Juan P Galeotti. 2021. Enhancing Search-based Testing with Testability
Transformations for Existing APIs. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 1 (2021), 1--34.

[16] Andrea Arcuri and Juan P Galeotti. 2021. Enhancing Search-based Testing with Testability
Transformations for Existing APIs. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 1 (2021), 1--34.

[17] Andrea Arcuri, Juan Pablo Galeotti, Bogdan Marculescu, and Man Zhang. 2021. EvoMaster: A
Search-Based System Test Generation Tool. Journal of Open Source Software 6, 57 (2021), 2153.

[18] Andrea Arcuri, ZhangMan, asmab89, Bogdan, Amid Gol, Juan Pablo Galeotti, Seran, Al-
berto Martín López, Agustina Aldasoro, Annibale Panichella, and Kyle Niemeyer. 2022. EMRe-
search/EvoMaster:. https://doi.org/10.5281/zenodo.6651631

[19] Vaggelis Atlidakis, Patrice Godefroid, and Marina Polishchuk. 2019. RESTler: Stateful REST API
Fuzzing. In ACM/IEEE International Conference on Software Engineering (ICSE) (Montreal,
Quebec, Canada) (ICSE). IEEE, 748–758.

[20] Vaggelis Atlidakis, Patrice Godefroid, and Marina Polishchuk. 2020. Checking security properties
of cloud service rest apis. In IEEE International Conference on Software Testing, Verification
and Validation (ICST). IEEE, 387--397.

[21] Asma Belhadi, Man Zhang, and Andrea Arcuri. 2022. Evolutionary-based Automated Testing for

GraphQL APIs. In Genetic and Evolutionary Computation Conference (GECCO).

[22] Moritz Beller, Chu-Pan Wong, Johannes Bader, Andrew Scott, Mateusz Machalica, Satish
Chandra, and Erik Meijer. 2021. What It Would Take to Use Mutation Testing in Industry—A
Study at Facebook. In International Conference on Software Engineering: Software Engineering
in Practice (ICSE-SEIP). IEEE, 268--277.

[23] Lionel Briand, Domenico Bianculli, Shiva Nejati, Fabrizio Pastore, and Mehrdad Sabetzadeh.
2017. The case for context-driven software engineering research: generalizability is overrated.
IEEE Software 34, 5 (2017), 72--75.

[24] Matteo Brunetto, Giovanni Denaro, Leonardo Mariani, and Mauro Pezzè. 2021. On introducing
automatic test case generation in practice: A success story and lessons learned. Journal of Systems
and Software 176 (2021), 110933.

[25] José Campos, Andrea Arcuri, Gordon Fraser, and Rui Abreu. 2014. Continuous test generation:
enhancing continuous integration with automated test generation. In IEEE/ACM Int. Conference
on Automated Software Engineering (ASE). ACM, 55--66.

[26] Ermira Daka, José Campos, Gordon Fraser, Jonathan Dorn, and Westley Weimer. 2015. Modeling
readability to improve unit tests. In Proceedings of the 2015 10th Joint Meeting on Foundations
of Software Engineering. 107--118.

[27] Vahid Garousi, Dietmar Pfahl, João M Fernandes, Michael Felderer, Mika V Mäntylä, David
Shepherd, Andrea Arcuri, Ahmet Coşkunçay, and Bedir Tekinerdogan. 2019. Characterizing
industry-academia collaborations in software engineering: evidence from 101 projects. Empirical
Software Engineering 24, 4 (2019), 2540--2602.

23

[28] Patrice Godefroid, Bo-Yuan Huang, and Marina Polishchuk. 2020. Intelligent REST API Data
Fuzzing. In ACM Symposium on the Foundations of Software Engineering (FSE) (Virtual Event,
USA) (ESEC/FSE 2020). ACM, 725–736.

[29] Patrice Godefroid, Daniel Lehmann, and Marina Polishchuk. 2020. Differential regression testing
for REST APIs. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software
Testing and Analysis. 312--323.

[30] Zac Hatfield-Dodds and Dmitry Dygalo. 2022. Deriving Semantics-Aware Fuzzers from Web
API Schemas. In 2022 IEEE/ACM 44th International Conference on Software Engineering:
Companion Proceedings (ICSE-Companion). IEEE, 345--346.

[31] Stefan Karlsson, Adnan Causevic, and Daniel Sundmark. 2020. QuickREST: Property-based Test
Generation of OpenAPI Described RESTful APIs. In IEEE International Conference on Software
Testing, Verification and Validation (ICST). IEEE.

[32] Myeongsoo Kim, Qi Xin, Saurabh Sinha, and Alessandro Orso. 2022. Automated Test Generation
for REST APIs: No Time to Rest Yet. In Proceedings of the 31st ACM SIGSOFT International
Symposium on Software Testing and Analysis (Virtual, South Korea) (ISSTA 2022). Association
for Computing Machinery, New York, NY, USA, 289–301. https://doi.org/10.1145/3533767.
3534401

[33] Myeongsoo Kim, Qi Xin, Saurabh Sinha, and Alessandro Orso. 2022. Automated Test Generation
for REST APIs: No Time to Rest Yet. https://doi.org/10.48550/ARXIV.2204.08348

[34] Bogdan Korel. 1990. Automated software test data generation. IEEE Transactions on software

engineering 16, 8 (1990), 870--879.

[35] Wing Lam, Patrice Godefroid, Suman Nath, Anirudh Santhiar, and Suresh Thummalapenta.
2019. Root Causing Flaky Tests in a Large-Scale Industrial Setting. In ACM Int. Symposium on
Software Testing and Analysis (ISSTA) (Beijing, China) (ISSTA 2019). 101–111.

[36] Nuno Laranjeiro, João Agnelo, and Jorge Bernardino. 2021. A black box tool for robustness

testing of REST services. IEEE Access 9 (2021), 24738--24754.

[37] P. K. Lehre and X. Yao. 2007. Runtime Analysis of (1+1) EA on Computing Unique Input

Output Sequences. In IEEE Congress on Evolutionary Computation (CEC). 1882--1889.

[38] Timothy C Lethbridge, Susan Elliott Sim, and Janice Singer. 2005. Studying software engineers:
Data collection techniques for software field studies. Empirical Software Engineering (EMSE) 10,
3 (2005), 311--341.

[39] Bogdan Marculescu, Man Zhang, and Andrea Arcuri. 2022. On the Faults Found in REST APIs
by Automated Test Generation. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 3 (2022), 1--43.

[40] Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cortés. 2021. RESTest: Automated
Black-Box Testing of RESTful Web APIs. In ACM Int. Symposium on Software Testing and
Analysis (ISSTA). ACM.

[41] Sam Newman. 2015. Building Microservices. " O’Reilly Media, Inc.".

[42] Annibale Panichella, Fitsum Kifetew, and Paolo Tonella. 2018. Automated Test Case Generation
IEEE

as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets.
Transactions on Software Engineering (TSE) 44, 2 (2018), 122--158.

[43] Goran Petrović and Marko Ivanković. 2018. State of mutation testing at google. In International
Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). 163--171.

24

[44] José Miguel Rojas, Gordon Fraser, and Andrea Arcuri. 2016. Seeding strategies in search-based
unit test generation. Software Testing, Verification and Reliability 26, 5 (2016), 366--401.

[45] Benjamin H Sigelman, Luiz Andre Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald
Beaver, Saul Jaspan, and Chandan Shanbhag. 2010. Dapper, a large-scale distributed systems
tracing infrastructure. (2010).

[46] Klaas-Jan Stol and Brian Fitzgerald. 2018. The ABC of Software Engineering Research. ACM
Transactions on Software Engineering and Methodology (TOSEM) 27, 3, Article 11 (sep 2018),
51 pages. https://doi.org/10.1145/3241743

[47] Emanuele Viglianisi, Michael Dallago, and Mariano Ceccato. 2020. RESTTESTGEN: Automated
Black-Box Testing of RESTful APIs. In IEEE International Conference on Software Testing,
Verification and Validation (ICST). IEEE.

[48] Wenyu Wang, Dengfeng Li, Wei Yang, Yurui Cao, Zhenwen Zhang, Yuetang Deng, and Tao Xie.
2018. An empirical study of android test generation tools in industrial cases. In IEEE/ACM Int.
Conference on Automated Software Engineering (ASE). IEEE, 738--748.

[49] Huayao Wu, Lixin Xu, Xintao Niu, and Changhai Nie. 2022. Combinatorial Testing of RESTful

APIs. In ACM/IEEE International Conference on Software Engineering (ICSE).

[50] Chengyu Zhang, Yichen Yan, Hanru Zhou, Yinbo Yao, Ke Wu, Ting Su, Weikai Miao, and
Geguang Pu. 2018. Smartunit: Empirical evaluations for automated unit testing of embedded
software in industry. In International Conference on Software Engineering: Software Engineering
in Practice (ICSE-SEIP). IEEE, 296--305.

[51] Man Zhang and Andrea Arcuri. 2021. Adaptive Hypermutation for Search-Based System Test
Generation: A Study on REST APIs with EvoMaster. ACM Transactions on Software Engineering
and Methodology (TOSEM) 31, 1 (2021).

[52] Man Zhang and Andrea Arcuri. 2022. Open Problems in Fuzzing RESTful APIs: A Comparison

of Tools. arXiv preprint arXiv:2205.05325 (2022).

[53] Man Zhang and Andrea Arcuri. 2022. Open Problems in Fuzzing RESTful APIs: A Comparison

of Tools. https://doi.org/10.48550/ARXIV.2205.05325

[54] Man Zhang, Asma Belhadi, and Andrea Arcuri. 2022. JavaScript Instrumentation for Search-Based
Software Testing: A Study with RESTful APIs. In IEEE International Conference on Software
Testing, Verification and Validation (ICST). IEEE.

[55] Man Zhang, Bogdan Marculescu, and Andrea Arcuri. 2021. Resource and dependency based test
case generation for RESTful Web services. Empirical Software Engineering 26, 4 (2021), 1--61.

[56] Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Wenhai Li, and Dan Ding. 2018. Fault analysis
and debugging of microservice systems: Industrial survey, benchmark system, and empirical study.
IEEE Transactions on Software Engineering (TSE) (2018).

25

