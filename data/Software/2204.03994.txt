Labeling-Free Comparison Testing of Deep Learning Models

Yuejun Guo
SnT, University of Luxembourg
Luxembourg

Qiang Hu
SnT, University of Luxembourg
Luxembourg

Maxime Cordy
SnT, University of Luxembourg
Luxembourg

Xiaofei Xie
Singapore Management University
Singapore

Mike Papadakis
SnT, University of Luxembourg
Luxembourg

Yves Le Traon
SnT, University of Luxembourg
Luxembourg

2
2
0
2

r
p
A
8

]

G
L
.
s
c
[

1
v
4
9
9
3
0
.
4
0
2
2
:
v
i
X
r
a

ABSTRACT
Various deep neural networks (DNNs) are developed and reported
for their tremendous success in multiple domains. Given a specific
task, developers can collect massive DNNs from public sources for
efficient reusing and avoid redundant work from scratch. However,
testing the performance (e.g., accuracy and robustness) of multiple
DNNs and giving a reasonable recommendation that which model
should be used is challenging regarding the scarcity of labeled data
and demand of domain expertise. Existing testing approaches are
mainly selection-based where after sampling, a few of the test data
are labeled to discriminate DNNs. Therefore, due to the random-
ness of sampling, the performance ranking is not deterministic.
In this paper, we propose a labeling-free comparison testing ap-
proach to overcome the limitations of labeling effort and sampling
randomness. The main idea is to learn a Bayesian model to infer
the modelsâ€™ specialty only based on predicted labels. To evaluate
the effectiveness of our approach, we undertook exhaustive experi-
ments on 9 benchmark datasets spanning in the domains of image,
text, and source code, and 165 DNNs. In addition to accuracy, we
consider the robustness against synthetic and natural distribution
shifts. The experimental results demonstrate that the performance
of existing approaches degrades under distribution shifts. Our ap-
proach outperforms the baseline methods by up to 0.74 and 0.53 on
Spearmanâ€™s correlation and Kendallâ€™s ğœ, respectively, regardless of
the dataset and distribution shift. Additionally, we investigated the
impact of model quality (accuracy and robustness) and diversity
(standard deviation of the quality) on the testing effectiveness and
observe that there is a higher chance of a good result when the
quality is over 50% and the diversity is larger than 18%.

KEYWORDS
deep neural network, comparison testing, labeling-free, Bayesian
model

ACM Reference Format:
Yuejun Guo, Qiang Hu, Maxime Cordy, Xiaofei Xie, Mike Papadakis, and Yves
Le Traon. 2022. Labeling-Free Comparison Testing of Deep Learning Models.
In Proceedings of . , 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
, ,
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Deep learning (DL) has achieved great success in various domains,
such as computer vision [29], natural language processing [26],
code understanding [30], and autonomous driving [18]. Due to
the outstanding performance of deep neural networks (DNNs),
researchers from the software engineering (SE) community have
attempted to apply DNNs for diverse SE tasks, such as source code
processing [7, 30], automatic software testing [41, 42], and GUI
designs [40]. Generally, for developers, compared with designing
a new DNN architecture that requires tremendous DL knowledge,
reusing publicly available models is an efficient and popular strategy
to avoid preparing everything from scratch. Moreover, it is practical
in real-world applications concerning the training cost and difficulty
due to the increasing size and complexity of DNNs.

With the fast-growing of DNNs, it is easy to collect massive
DNNs by either pre-trained model files (e.g., .h5 and .pth) or train-
ing scripts from public sources like GitHub [4]. Thus, there is a high
demand in testing the performance of collected models for practical
use. However, â¶ as the contributors and developing constraints
are diverse, their performance is rarely given or is not reliable by
unknown testing conditions. For example, only 21 of the 165 DNNs
we collected and evaluated in this paper have a performance report.
â· Testing the DNNs requires test data especially out-of-distribution
(OOD) data [9, 13, 17]. In practice, when deploying DNNs in real
scenarios, the distribution shift is inevitable. Namely, the test data
can be OOD compared to the initial test data which by default
come from the same distribution as the training data. As a result,
these models may exhibit remarkably different behaviors, which
raises the concern of quality and reliability [6]. For instance, for the
same dataset iWildCam (please refer to Section 4.3 for more details),
two DNNs exhibit 75.74% and 76.60% accuracy on initial test data,
while with distribution shift, their performance turns to 76.82%
and 65.30%, respectively, on new test data. â¸ test data can be from
the real world and unlabeled [35]. For example, AOJ [1] receives
submissions in different programming languages all the time. How-
ever, manual labeling is expensive and tedious, especially when
domain knowledge is in demand. For example, a Java developer can
easily annotate the source code in Java but may have difficulties
with other programming languages as C++. How to precisely and
efficiently test and rank the performance of DNNs when only given
the predictions is our key point of problem definition.

Some approaches have been proposed for comparison testing
of multiple DNNs. For example, a recent approach named sample
discrimination based selection (SDS) [25] achieves positive model
ranking results on three benchmark datasets. SDS selects a set of

 
 
 
 
 
 
, ,

Guo and Hu, et al.

data to label based on the majority voting [31] and item discrimina-
tion [14]. These data are considered as the most discriminative in
terms of distinguishing the accuracy between DNNs. Finally, DNNs
are ranked based on their accuracy on these selected and labeled
data. However, there are three main limitations. First, it is sample-
selection-based which requires the labeling effort. Second, the rank
is only based on the accuracy of in-distribution (ID) data that fol-
low the same distribution as the training data. However, for model
deployment, the performance (robustness) on out-of-distribution
(OOD) remains uncertain and should be considered. Third, the eval-
uation is limited to the image domain, its effectiveness for software
tasks is unclear.

Concerning the above three issues, in this paper, we propose
a labeling-free and reliable approach to rank multiple DNNs. To
summarize, the main contributions of our work are:

(1) We proposed a novel approach for comparison testing and
ranking multiple DNN models to facilitate the reuse of DNNs
from public sources.

(2) In addition to the ranking by accuracy based on ID data,
our proposed approach is flexible and stable to deal with
the ranking concerning robustness based on OOD data with
both synthetic (15 types of corruptions Ã— 5 levels of severity)
and natural distribution shifts.

(3) Our approach is labeling-free. To overcome the challenge of
labeling effort, our approach infers the specialties of DNNs by
building a Bayesian model using only the observed predicted
labels. The Bayesian model estimates the most likely model
specialty for the predictions of DNNs. In addition, compared
to existing approaches, such as SDS, our approach does not
require random sampling in the procedure, which avoids the
sampling randomness.

(4) We experiment on 9 benchmark datasets for comparison test-
ing of DNNs. These datasets span different domains includ-
ing image, text, and source code with different programming
languages (Java and C++). To the best of our knowledge,
this is the first DNN comparison testing work containing
datasets other than image.

2 BACKGROUND AND RELATED WORK
We review the work on deep learning testing and especially com-
parison testing.

2.1 Deep Learning Testing
Deep learning (DL) testing refers to evaluating the quality of devel-
oped deep neural networks (DNNs) for further deployment [39]. A
simple and local testing strategy is to split a dataset into training,
validation, and test sets. The training and validation sets contribute
to the training process to tune parameters. The test set is untouched
by the training process to provide an unbiased evaluation of the
accuracy. Typically, this testing is built on the assumption that the
training and test sets are independent and identically distributed.
However, in practical scenarios, the testing always suffers from
distribution shifts where the quality (robustness) of models can
degrade significantly [20]. For instance, adding a minor Gaussian
noise to an image can mislead a DNN to the wrong function [16].

Another example is about the customer comments. With new col-
lected comments, since new customers may have appeared [28], the
model needs to be rechecked for its quality and reliability. Given
these concerns, the robustness of DL systems has recently received
considerable attention from researchers [9, 15, 20].

Roughly speaking, there are two types of distribution shifts,
synthetic and natural. Synthetic distribution shift mainly comes
from adding artificial perturbations (corruptions) into raw data.
Dan and Thomas [16] proposed to add 15 types of algorithmically
generated corruptions with 5 levels of severity to image data to
mimic realistic situations, such as noise, blur, snow, and zoom. Based
on these corruptions, different benchmark datasets, such as CIFAR-
10-C [16] and MNIST-C [27], have been developed for testing the
robustness of DNN models. On the other hand, natural distribution
shift is usually induced by the change of environment or population
and exists in raw data, such as the change of camera traps [8] and
new customers [28]. A recent benchmark [20] provides in-the-wild
distribution shifts covering diverse data domains and applications.

2.2 Comparison testing
In conventional software engineering, comparison testing [19, 32,
33] aims at figuring out the strength and weaknesses of a newly
developed software product compared with existing products. The
end goal is to facilitate the deployment of a product with high
functionality and reliability.

Regarding a DNN as a software product, we can take the same
testing by testing different DNNs with the same test. However, the
obstacle is that usually, the test data are numerous but unlabeled.
Thus, there is no prior information to reveal the underlying perfor-
mance. Recently, Meng et al. [25] proposed to compare the perfor-
mance of multiple DNNs based on a few labeled data. Concretely,
the problem turns into how to find out the most discriminative
data that can amply distinguish the difference. In their proposed
sample discrimination based selection method, the majority voting
[31] is first applied to produce pseudo labels based on which DNNs
are classified to top, middle, and bottom groups following the item
discrimination [14]. Via the prediction difference by the top and
bottom DNNs, each data has a unique discrimination score and the
high ones are selected for the final ranking.

A close topic to comparison testing is test selection [24, 34, 36]
where a few data are selected and annotated to approximate the
performance of a DNN. For instance, Li et al. [23] proposed the cross
Entropy-based sampling to identify the most representative data of
a test set. Similarly, Chen et al. [10] developed the practical accuracy
estimation. The difference is that in test selection, the objective is a
single DNN, while in comparison testing, the objective is multiple
DNNs. Undoubtedly, one can first approximate the performance of
each DNN by selecting its corresponding representative set then
undertake the comparison. However, this will largely increase the
effort in labeling and is less practical than selecting once.

3 METHODOLOGY
3.1 Problem Formulation
In this paper, we are interested in the classification task. Given a
ğ¶-class task over a sample space Z = X Ã— Y â†’ R, where ğ‘¥ âˆˆ X is
an input data and ğ‘¦ âˆˆ Y is its class label. Let ğ‘“ : ğ‘¥ â†’ ğ‘¦ be a deep

Labeling-Free Comparison Testing of Deep Learning Models

, ,

neural network (dnn) that maps ğ‘¥ to the problem domain. Given
ğ‘› models, ğ‘“1, ğ‘“2, . . . , ğ‘“ğ‘›, extracted from public sources and a set of
unlabeled test data ğ‘‡ , the problem we study is to estimate the rank
of models regarding their performance on ğ‘‡ğ‘‡ğ‘‡ = {ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘š }.
Figure 1 illustrates the workflow.

We assume that the manual labeling is expensive especially when
domain knowledge is required. To this end, we propose to tackle
the ranking problem by only querying the predictions, which is
highly applicable in practical scenarios. Remarkably, in this paper,
we consider the performance of both accuracy and robustness. The
accuracy and robustness are the correctness ratio of prediction on
ID and OOD data, respectively.

3.2 Our Approach
Figure 2 gives an example of our approach. In this simple 3-class
example, there are 3 DNN models (ğ‘“1, ğ‘“2, ğ‘“3) given 6 unlabeled sam-
ples (ğ‘¥1, ğ‘¥2, . . . , ğ‘¥6). The goal is to rank the 3 models concerning
their accuracy on these samples in the absence of true labels. First,
we compute the predicted label of each sample by each model and
remove ğ‘¥6 where all the models have the same prediction. Sec-
ond, we initialize the two parameters, ğ›¼ğ›¼ğ›¼ and ğ›½ğ›½ğ›½, contained in our
approach. ğ›¼ğ›¼ğ›¼ refers to how difficult a sample is for all models to
predict the correct label. ğ›½ğ›½ğ›½ indicates how good a model is to out-
put the correct labels of all samples. Initially, we use the simplest
and most commonly used majority voting heuristic [31] to give a
pseudo label to each sample. For instance, the pseudo label of ğ‘¥1 is
0 because 2 (ğ‘“1, ğ‘“2) of 3 models predict the label as 0. ğ›¼ğ›¼ğ›¼ is defined as
the ratio of mismatched models that output a different label instead
of the pseudo one. ğ›½ğ›½ğ›½ is calculated as the ratio of correctly predicted
samples over the entire set. Third, since the pseudo labels are not
the true labels, ğ›¼ğ›¼ğ›¼ and ğ›½ğ›½ğ›½ cannot truly reflect the data difficulty and
model ability. We optimize these two parameters by a likelihood
estimation method in the presence of true labels. Finally, based on
the optimized ğ›½ğ›½ğ›½ ( 4
, 1
, 3
5 ), we obtain the ranking 1, 3, and 2 for ğ‘“1, ğ‘“2,
5
5
and ğ‘“3, respectively.

Given that no label is available in the test data, the main idea of
our approach is to infer the specialties of DNNs by approximately
maximizing the likelihood between the predictions and true labels
via the expectation-maximization (EM) algorithm [12]. Let (cid:101)ğ‘Œğ‘Œğ‘Œ =
ğ‘¦ğ‘– ğ‘— (cid:9)
(cid:8)
1â‰¤ğ‘– â‰¤ğ‘š,1â‰¤ ğ‘— â‰¤ğ‘› be the predicted labels of ğ‘‡ğ‘‡ğ‘‡ and ğ‘Œğ‘Œğ‘Œ = {ğ‘¦ğ‘– }1â‰¤ğ‘– â‰¤ğ‘š
(cid:101)
ğ‘¦ğ‘– ğ‘— refers to ğ‘¥ğ‘– and model ğ‘“ğ‘— . Given the ob-
be the true labels. Here, (cid:101)
served (cid:101)ğ‘Œğ‘Œğ‘Œ and latent ğ‘Œğ‘Œğ‘Œ governed by unknown parameters ğœƒğœƒğœƒ , the like-
(cid:16)
(cid:17)
(cid:16)
ğœƒğœƒğœƒ ; (cid:101)ğ‘Œğ‘Œğ‘Œ
(cid:101)ğ‘Œğ‘Œğ‘Œ, ğ‘¦ğ‘– | ğœƒğœƒğœƒ
lihood function is defined as ğ¿
.
The goal is to search the best ğœƒğœƒğœƒ that maximizes the likelihood, in
other words, the probability of observing (cid:101)ğ‘Œğ‘Œğ‘Œ . As for ğœƒğœƒğœƒ , inspired by
[37], we consider two factors, data difficulty ğ›¼ğ›¼ğ›¼ = {ğ›¼ğ‘– }1â‰¤ğ‘– â‰¤ğ‘š and
model specialty ğ›½ğ›½ğ›½ = (cid:8)ğ›½ ğ‘— (cid:9)
1â‰¤ ğ‘— â‰¤ğ‘š, that influence the performance of
DNNs. Namely, ğœƒğœƒğœƒ = (ğœƒğœƒğœƒ, ğ›½ğ›½ğ›½). Algorithm 1 presents the pseudo code
of our approach.

(cid:16)
(cid:101)ğ‘Œğ‘Œğ‘Œ | ğœƒğœƒğœƒ

ğ‘š
(cid:205)
ğ‘–=1

= ğ‘

=

ğ‘

(cid:17)

(cid:17)

Step 1: Pruning. Inevitably, some data will receive the same
predictions by all models, which is useless for discriminating the
performance and causes computational cost. For this reason, we
filter these data without losing any information for ranking and
obtain a smaller set ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² (Lines 1-6 in Algorithm 1).

Step 2: Initializing. First, for each data ğ‘¥ğ‘– , a pseudo label is
(cid:17)

voted by the majority of DNNs, namely, ğ‘¦ â€²
1â‰¤ ğ‘— â‰¤ğ‘›
(Lines 7-9). Next, ğ›¼ğ‘– is the number of DNNs that gives a different
label from the pseudo label and ğ›½ ğ‘— is the accuracy based on pseudo
labels (Lines 10-15). Formally, the definitions are:
ğ‘– , 1 â‰¤ ğ‘— â‰¤ ğ‘›(cid:9)
ğ‘¦ğ‘– ğ‘— â‰  ğ‘¦ â€²
ğ‘›

ğ‘– , 1 â‰¤ ğ‘– â‰¤ (cid:12)
ğ‘¦ğ‘– ğ‘— = ğ‘¦ â€²
(cid:12)ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€²(cid:12)
(cid:12)
(cid:12)

(cid:8)
ğ‘¦ğ‘– ğ‘— | (cid:101)
(cid:101)

(cid:8)
ğ‘¦ğ‘– ğ‘— | (cid:101)
(cid:101)

ğ‘– = mode

ğ‘¦ğ‘– ğ‘— (cid:9)
(cid:101)

, ğ›½ ğ‘— =

(cid:12)ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€²(cid:12)
(cid:12)

ğ›¼ğ‘– =

(cid:16)(cid:8)

(cid:9)

(1)
Step 3: Optimizing. The EM algorithm solves the optimization
problem by iteratively performing an expectation (E) step and a
maximization (M) step (Lines 16-26). In the E-step, it estimates the
expected value of the log likelihood:
(cid:104)

(cid:17)(cid:105)

ğ‘„ (ğœƒğœƒğœƒ,ğœƒğ‘™ğ‘ğ‘ ğ‘¡
ğœƒğ‘™ğ‘ğ‘ ğ‘¡
ğœƒğ‘™ğ‘ğ‘ ğ‘¡ ) = E

log ğ¿

(cid:16)
ğœƒğœƒğœƒ ; (cid:101)ğ‘Œğ‘Œğ‘Œ,ğ‘Œğ‘Œğ‘Œ

=

|ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² |
âˆ‘ï¸

ğ‘–=1

E [log (ğ‘ (ğ‘¦ğ‘– ))] +

|ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² |
âˆ‘ï¸

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘—=1

ğ‘¦ğ‘– ğ‘— | ğ‘¦ğ‘–, ğ›¼ğ‘–, ğ›½ ğ‘— (cid:1) (cid:3)
E (cid:2)ğ‘ (cid:0)
(cid:101)

(2)

1

where ğœƒğœƒğœƒ = (ğ›¼ğ›¼ğ›¼, ğ›½ğ›½ğ›½) and ğœƒğ‘™ğ‘ğ‘ ğ‘¡
ğœƒğ‘™ğ‘ğ‘ ğ‘¡
ğœƒğ‘™ğ‘ğ‘ ğ‘¡ is from the last E-step. For the computa-
tion, we use the definition from [37] where ğ‘ (cid:0)
ğ‘¦ğ‘– ğ‘— = ğ‘¦ ğ‘— | ğ›¼ğ‘–, ğ›½ ğ‘— (cid:1) =
(cid:101)
ğ‘¦ğ‘– ğ‘— and ğ›½ğ›½ğ›½ are independent given ğ›¼ğ›¼ğ›¼, ğ‘ (ğ‘¦ğ‘– ) =
1+ğ‘’âˆ’ğ›¼ğ‘– ğ›½ ğ‘— . Besides, as (cid:101)
ğ‘ (cid:0)ğ‘¦ğ‘– | ğ›¼ğ›¼ğ›¼, ğ›½ ğ‘— (cid:1). Remarkably, ğ‘¦ğ‘– represents the true label of a sample.
In the ranking problem, ğ‘¦ğ‘– is absent but the probability of taking it
as a true label is can be inferred by ğ‘ (cid:0)ğ‘¦ğ‘– | ğ›¼ğ›¼ğ›¼, ğ›½ ğ‘— (cid:1).

In the M-step, the gradient ascent is applied to search for ğ›¼ğ›¼ğ›¼ and

ğ›½ğ›½ğ›½ that maximize ğ‘„.

Step 4: Ranking. Finally, as ğ›½ğ›½ğ›½ well estimate the abilities of
each DNN given the observed labels, we use this vector to rank
DNNs (Line 27). In other words, a high specialty indicates a good
performance on the data.

4 EXPERIMENTAL SETUP
4.1 Implementation
All experiments were conducted on a high-performance computer
cluster and each cluster node runs a 2.6 GHz Intel Xeon Gold 6132
CPU with an NVIDIA Tesla V100 16G SXM2 GPU. We implement
the proposed approach and baseline methods based on the state-of-
the-art frameworks, Tensorflow 2.3.0 and PyTorch 1.6.0. To allow
for reproducibility, our full implementation and evaluation subjects
are available on GitHub 1. For synthetic data distribution shift, we
consider two benchmark datasets where each includes 15 types of
natural corruption with 5 levels of severity. In other words, we test
on 150 datasets with the synthetic distribution shift. Due to the
space limitation, we only report the average results on corrupted
data for baseline methods. The remaining results corroborate our
findings and are freely available on our companion project website
[3].

4.2 Research Questions
In this study, we focus on the following three research questions:
â€¢ RQ1 (effectiveness given ID test): How does our proposed

approach ranking multiple DNNs given ID test data?

1We will make the implementation publicly available upon acceptance.

, ,

Guo and Hu, et al.

Figure 1: Illustration of studied problem.

datasets, MNIST-C [27] and CIFAR-10-C [16], for MNIST and CIFAR-
10, respectively. Each benchmark includes 75 datasets with 15 types
of natural corruptions, such as Gaussian noise, shot noise, impulse
noise, defocus blur, frosted glass blur, motion blur, zoom blur, snow,
frost, fog, brightness, contrast, elastic, pixelate, and jpeg. Besides,
each type of corruption has 5 levels of severity. For the natural
distribution shift, we use two datasets for iWildCam and Amazon
from a recent-published benchmark, WILDS [20]. The distribution
shift comes from new came traps in iWildCam and new users in
Amazon. Table 1 lists the details of datasets.

DNNs. From Github, we collect, in total, 165 models, 30 for
MNIST, 25 for Fashion-MNIST, 30 for CIFAR-10, 20 for iWildCam,
20 for Amazon, 20 for Java250, and 20 for C++1000. In concrete, the
models of MNIST and CIFAR-10 are extracted using the github links
in [25]. The 25 models of Fashion-MNIST are extracted from [2]
provided by Meng et al. [25]. For iWildCam and Amazon, we train
models using the implementation in the benchmark WILDS [20].
For Java250 and C++1000, we train models using the implementa-
tion in the benchmark Project CodeNet [30]. Table 1 presents the
accuracy and robustness of DNNs on ID test data and OOD test data
with natural distribution shift, respectively. Table 2 summarizes
the robustness on MNIST-C and CIFAR-10-C with the synthetic
distribution shift.

4.4 Baseline Methods
In our study, we compare our approach to 3 baseline methods,
random sampling, SDS, and CES. All baseline methods are sample-
selection-based. Following [25], the labeling budget of the baseline
methods ranges from the number of DNNs (i.e., 30 DNNs in MNIST)
to 180 at intervals of 5. Let ğ‘› be the number of DNNs, ğ‘š be the
number of unlabeled test data, and ğ‘ be the number of data selection
(e.g., 31 in MNIST).

Random sampling is a basic and model-independent method
for data selection where each data has an equal probability to be
considered. A subset of data is randomly selected and annotated to
rank DNNs.

Figure 2: An example of our four-step approach. It ranks 3
DNNs (ğ‘“1, ğ‘“2, ğ‘“3) given 6 unlabeled data (ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥5, ğ‘¥6) in a
3-class classification task. Numbers (0, 1, 2) highlighted in
colors are predicted labels.

â€¢ RQ2 (effectiveness under distribution shift): How does
our approach ranking multiple DNNs given OOD test data
(including synthetic and natural distribution shifts)?

â€¢ RQ3 (impact factors of our approach): What is the impact

of model quality and diversity on ranking DNNs?

4.3 Datasets and DNNs
Datasets. We choose 7 datasets, MNIST [22], Fashion-MNIST [38],
CIFAR-10 [21], iWildCam [8], Amazon [28], Java250, and C++1000
[30] that are widely studied in previous work. These datasets cover
the image (first 4), text (Amazon), and source code (Java250 and
C++1000) domains. The test data that follow the same distribution
as the training set are the so-called in-distribution (ID) data. The test
data with data distribution shift are out-of-distribution (OOD). In
our work, we consider two types of distribution shifts: synthetic and
natural. For the synthetic distribution shift, we use two benchmark

TaskDNNsBayesian modelRanking solutionUnlabeled test setPerformance criterionsearchXYtestpredict213queryXYannotateselectData difficultyModel specialitySelection-based approachOur approachinitialize1!1"1#1$1%1&23!001221/3"012121/3#200211/4/////1!1"1#1$1%23!0012213"012125353#20021535413132313131!1"1#1$1%23!001225453"012125153#2002153541312131312DNN3!3"3#Ranking132(1) Pruning(2) Initializing (majority voting)(3) Optimizing (EM)(4) RankingLabeling-Free Comparison Testing of Deep Learning Models

, ,

// Data difficulty

ğœŒ =

Algorithm 1: Labeling-free comparison testing

Input

: {ğ‘“1, ğ‘“2, . . . , ğ‘“ğ‘› }: DNNs for comparison
ğ‘‡ğ‘‡ğ‘‡ = {ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘š } : test set
Î“: performance criterion

Output : {ğ‘Ÿ â€² (ğ‘“1) , ğ‘Ÿ â€² (ğ‘“2) , . . . , ğ‘Ÿ â€² (ğ‘“ğ‘›) }: Rank of DNNs
/* Step1: Pruning

1 ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² = { }
2 for ğ‘– = 1 â†’ ğ‘š do
(cid:12)
(cid:110)(cid:8)
(cid:12)
(cid:12)

ğ‘¦ğ‘– ğ‘— (cid:9)
(cid:101)
ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² â† ğ‘¥ğ‘– ;

if

3

4

1â‰¤ ğ‘— â‰¤ğ‘›

(cid:111)(cid:12)
(cid:12)
(cid:12)

> 1 then

ğ‘¦ğ‘– ğ‘— is the predicted label by ğ‘“ğ‘—

// (cid:101)

*/

*/

end

5
6 end

/* Step 2: Initializing
// Majority voting
7 for ğ‘– = 1 â†’ |ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² | do
ğ‘¦â€²
ğ‘– = mode

(cid:16)(cid:8)

ğ‘¦ğ‘– ğ‘— (cid:9)
(cid:101)

8

1â‰¤ ğ‘— â‰¤ğ‘›

(cid:8)

ğ›¼ğ‘– =

ğ‘¦ğ‘– ğ‘— | (cid:101)
(cid:101)

9 end
10 for ğ‘– = 1 â†’ |ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² | do
ğ‘¦ğ‘– ğ‘— â‰ ğ‘¦â€²
ğ‘›

11
12 end
13 for ğ‘— = 1 â†’ ğ‘› do
ğ›½ ğ‘— =

ğ‘¦ğ‘– ğ‘— | (cid:101)
(cid:101)

ğ‘¦ğ‘– ğ‘— =ğ‘¦â€²

14

(cid:8)

ğ‘– ,1â‰¤ ğ‘— â‰¤ğ‘›(cid:9)

ğ‘– ,1â‰¤ğ‘– â‰¤ |ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² |(cid:9)
|ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² |

(cid:17)

;

;

// Model specialty

15 end

/* Step 3: Optimizing
16 ğ›¼ğ›¼ğ›¼ğ‘™ğ‘ğ‘ ğ‘¡ = {ğ›¼ğ‘– }1â‰¤ğ‘– â‰¤ |ğ‘‡ â€²ğ‘‡ â€²ğ‘‡ â€² |
17 ğ›½ğ›½ğ›½ğ‘™ğ‘ğ‘ ğ‘¡ = (cid:8)ğ›½ ğ‘— (cid:9)
1â‰¤ ğ‘— â‰¤ğ‘›
18 ğ‘„ğ‘™ğ‘ğ‘ ğ‘¡ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’ğ‘„ (ğ›¼ğ›¼ğ›¼ğ‘™ğ‘ğ‘ ğ‘¡ , ğ›½ğ›½ğ›½ğ‘™ğ‘ğ‘ ğ‘¡ ) ;

*/

// ğ¶ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’ğ‘„ estimates the

log likelihood based on Equation (2)

19 ğ›¼ğ›¼ğ›¼, ğ›½ğ›½ğ›½ = ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ğ´ğ‘ ğ‘ğ‘’ğ‘›ğ‘¡ (ğ›¼ğ›¼ğ›¼ğ‘™ğ‘ğ‘ ğ‘¡ , ğ›½ğ›½ğ›½ğ‘™ğ‘ğ‘ ğ‘¡ , ğ‘„ğ‘™ğ‘ğ‘ ğ‘¡ ) ; // ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ğ´ğ‘ ğ‘ğ‘’ğ‘›ğ‘¡

updates ğ›¼ğ›¼ğ›¼, ğ›½ğ›½ğ›½

20 ğ‘„ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’ğ‘„ (ğ›¼ğ›¼ğ›¼, ğ›½ğ›½ğ›½)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

> 1ğ¸ âˆ’ 5 do

ğ‘„âˆ’ğ‘„ğ‘™ğ‘ğ‘ ğ‘¡
ğ‘„ğ‘™ğ‘ğ‘ ğ‘¡
ğ‘„ğ‘™ğ‘ğ‘ ğ‘¡ = ğ‘„
ğ›¼ğ›¼ğ›¼ğ‘™ğ‘ğ‘ ğ‘¡ , ğ›½ğ›½ğ›½ğ‘™ğ‘ğ‘ ğ‘¡ = ğ›¼ğ›¼ğ›¼, ğ›½ğ›½ğ›½
ğ›¼ğ›¼ğ›¼, ğ›½ğ›½ğ›½ = ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ğ´ğ‘ ğ‘ğ‘’ğ‘›ğ‘¡ (ğ›¼ğ›¼ğ›¼ğ‘™ğ‘ğ‘ ğ‘¡ , ğ›½ğ›½ğ›½ğ‘™ğ‘ğ‘ ğ‘¡ , ğ‘„ğ‘™ğ‘ğ‘ ğ‘¡ )
ğ‘„ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’ğ‘„ (ğ›¼ğ›¼ğ›¼, ğ›½ğ›½ğ›½)

21 while

22

23

24

25
26 end

/* Step 4: Ranking

27 ğ‘Ÿ â€² (ğ‘“1) , ğ‘Ÿ â€² (ğ‘“2) , . . . , ğ‘Ÿ â€² (ğ‘“ğ‘›) = ğ‘†ğ‘œğ‘Ÿğ‘¡ (ğ›½ğ›½ğ›½)
28 return {ğ‘Ÿ â€² (ğ‘“1) , ğ‘Ÿ â€² (ğ‘“2) , . . . , ğ‘Ÿ â€² (ğ‘“ğ‘›) }

*/

Sample discrimination based selection (SDS) [25] is the state-
of-the-art approach in ranking multiple DNNs with respect to the
accuracy. Following [25], among data in the top 25% with high
discrimination scores, we randomly select a given budget of data
to label and annotate to perform the ranking task.

Cross Entropy-based Sampling (CES) [23] is designed to se-
lect a set of representative data to approximate the actual perfor-
mance given a single DNN. We follow the same procedure as [25]
to adapt CES for multi-DNN comparison. First, for each model, we
utilize CES to select ğ‘ subsets of data with respect to the labeling
budgets. Then, we rank all the models using each selected subset
and produce ğ‘ ranking results. Next, Spearmanâ€™s rank-order cor-
relation is applied to each ranking (see Section 4.5), and therefore,
we obtain ğ‘ correlation coefficients. After that, for each model, we

calculate the average of the ğ‘ correlation coefficients. Finally, the
model with the largest average correlation is regarded as the best,
and the ranking performance based on this model is the baseline.
Due to the random manner in the sampling methodology, each

experiment of the baseline methods is repeated 50 times.

4.5 Evaluation Measures
To evaluate the effectiveness of each method, we employ three
statistical analysis, Kendallâ€™s ğœ rank correlation [11], Spearmanâ€™s
rank-order correlation [11], and Jaccard similarity [25]. The first
two evaluate the general ranking on all models, while the last one
specifically estimates the ranking on top-ğ‘˜ DNNs.

Given ğ‘› DNNs, ğ‘“1, ğ‘“2, . . . , ğ‘“ğ‘›, let ğ‘Ÿ (ğ‘“1) , ğ‘Ÿ (ğ‘“2) , . . . , ğ‘Ÿ (ğ‘“ğ‘›) be the
ground truth ranking and ğ‘Ÿ â€² (ğ‘“1) , ğ‘Ÿ â€² (ğ‘“2) , . . . , ğ‘Ÿ â€² (ğ‘“ğ‘›) be the esti-
mated ranking. The Spearmanâ€™s rank-order correlation coefficient
is computed as

ğ‘›

ğ‘›
(cid:205)
ğ‘–=1

ğ‘Ÿ (ğ‘“ğ‘– ) ğ‘Ÿ â€² (ğ‘“ğ‘– ) âˆ’

ğ‘Ÿ (ğ‘“ğ‘– )

(cid:19)

ğ‘Ÿ â€² (ğ‘“ğ‘– )

(cid:19) (cid:18) ğ‘›
(cid:205)
ğ‘–=1

(cid:18) ğ‘›
(cid:205)
ğ‘–=1
(cid:19)2(cid:35) (cid:34)
ğ‘›

(cid:118)(cid:116)(cid:34)
ğ‘›

ğ‘›
(cid:205)
ğ‘–=1

ğ‘Ÿ (ğ‘“ğ‘– )2 âˆ’

(cid:18) ğ‘›
(cid:205)
ğ‘–=1

ğ‘Ÿ (ğ‘“ğ‘– )

ğ‘›
(cid:205)
ğ‘–=1

ğ‘Ÿ â€² (ğ‘“ğ‘– )2 âˆ’

(cid:19)2(cid:35)

ğ‘Ÿ â€² (ğ‘“ğ‘– )

(cid:18) ğ‘›
(cid:205)
ğ‘–=1

(3)
A large ğœŒ indicates that the correlation between the ground truth
and estimation is strong.

Kendallâ€™s ğœ is

ğœ =

ğ‘ƒ âˆ’ ğ‘„
âˆšï¸(ğ‘ƒ + ğ‘„ + ğ‘‡ ) (ğ‘ƒ + ğ‘„ + ğ‘ˆ )

(4)

where ğ‘ƒ and ğ‘„ are the numbers of ordered and disordered pairs
in {ğ‘Ÿ (ğ‘“ğ‘– ) , ğ‘Ÿ â€² (ğ‘“ğ‘– )}, respectively. ğ‘‡ and ğ‘ˆ are the numbers of ties
in {ğ‘Ÿ (ğ‘“ğ‘– )} and {ğ‘Ÿ â€² (ğ‘“ğ‘– )}, respectively. A large ğœ indicates a strong
agreement between the ground truth and estimation.

Meng et al. proposed to apply the Jaccard similarity for mea-
suring the similarity between the top-ğ‘˜ models. The similarity
coefficient is defined as:

ğ½ğ‘˜ =

| {ğ‘“ğ‘– | ğ‘Ÿ (ğ‘“ğ‘– ) <= ğ‘˜ } âˆ© {ğ‘“ğ‘– | ğ‘Ÿ â€² (ğ‘“ğ‘– ) <= ğ‘˜ } |
| {ğ‘“ğ‘– | ğ‘Ÿ (ğ‘“ğ‘– ) <= ğ‘˜ } âˆª {ğ‘“ğ‘– | ğ‘Ÿ â€² (ğ‘“ğ‘– ) <= ğ‘˜ } |

, 1 â‰¤ ğ‘– â‰¤ ğ‘›

(5)

A large ğ½ğ‘˜ implies a high success in identifying the top-ğ‘˜ models.

5 RESULTS AND DISCUSSION
5.1 RQ1: Effectiveness Given ID Test Data
First, we compare the effectiveness of four methods in ranking
multiple DNNs based on the accuracy of ID data. Figure 3 shows
the result measured by Spearmanâ€™s rank-order correlation. The
first conclusion we can draw is that, over seven datasets, all meth-
ods succeed in outputting positively correlated rankings. By com-
parison, our proposed approach continuously outperforms (by up
to 0.74) the baseline methods regardless of the labeling budget.
Namely, the ranking by our approach is strongly correlated with
the ground truth. In general, for the three sample-selection-based
baseline methods, the correlation between the estimated rank and
the ground truth increases when more data are labeled. However,
for some datasets, the performance is still far from our approach.
For example, in Amazon, our approach obtains a correlation co-
efficient of 0.80, while the best baseline, SDS, only achieves 0.48
using the maximum labeling budget of 180. Besides, due to the

, ,

Guo and Hu, et al.

Table 1: Summary of datasets. â€œ#DNNâ€ is the number of DNNs collected for each dataset. â€œ#IDâ€ is the number of in-distribution
test data. â€œ#OODâ€ is the number of out-of-distribution test data with synthetic or natural distribution shifts. MNIST-C and
CIFAR-10-C are two benchmark datasets and the robustness is summarized in Table 2.

Dataset
MNIST
Fashion-MNIST
CIFAR-10
iWildCam
Amazon
Java250
C++1000

Data Type
Image of handwritten digits
Image of fashion products
Image of animals and vehicles
Image of wildlife
Text of comments
Source code in Java
Source code in C++

#Classes
10
10
10
182
5
250
1000

#DNN
30
25
30
20
20
20
20

#ID Accuracy (%)
85.27-99.54
10k
90.09-93.38
10k
69.90-95.92
10k
75.72-77.26
8.154k
73.60-74.84
46.95k
64.73-87.39
15k
71.39-92.10
99.997k

#OOD Distribution Shift Robustness (%)
MNIST-C
Synthetic
-
-
CIFAR-10-C
Synthetic
65.30-76.82
Natural
71.35-72.35
Natural
-
-
-
-

750k
-
750k
42.791k
100.05k
-
-

Table 2: Summary of MNIST-C and CIFAR-10-C with the
synthetic distribution shift and robustness. Each dataset in-
cludes 15 types of natural corruptions (e.g., Gaussian Noise)
with 5 levels of severity (1-5). The number in each cell
presents the minimum and maximum robustness of multi-
ple DNNs given the corruption type and severity.

Corruption Type

Severity=1

Severity=2

Severity=3

Severity=4

Severity=5

Gaussian Noise
Shot Noise
Impulse Noise
Defocus Blur
Frosted Glass Blur
Motion Blur
Zoom Blur
Snow
Frost
Fog
Brightness
Contrast
Elastic
Pixelate
JPEG

Gaussian Noise
Shot Noise
Impulse Noise
Defocus Blur
Frosted Glass Blur
Motion Blur
Zoom Blur
Snow
Frost
Fog
Brightness
Contrast
Elastic
Pixelate
JPEG

84.85-99.49
84.89-99.53
84.29-99.20
58.63-95.69
65.06-96.52
56.71-97.29
82.68-99.54
51.43-99.06
17.25-98.99
9.74-97.07
66.18-99.53
37.47-99.22
49.09-99.07
80.40-98.64
84.90-99.53

59.02-82.46
66.72-88.90
63.82-88.70
69.62-96.13
35.53-74.17
67.30-94.31
64.45-93.94
67.65-93.63
66.73-92.93
68.71-95.87
69.55-95.76
67.12-95.77
62.71-93.92
69.62-94.77
69.29-87.71

MNIST-C

80.49-99.25
84.73-99.49
71.33-98.91
31.76-84.91
54.02-94.33
36.28-90.04
81.50-99.45
17.78-98.38
10.60-97.91
9.61-95.32
21.94-99.22
20.38-99.17
12.80-17.58
84.37-99.11
84.93-99.56

55.89-99.05
84.87-99.38
56.50-98.62
9.73-43.40
19.06-75.43
25.23-78.19
80.42-99.35
19.27-96.03
9.52-96.72
9.75-89.51
12.55-98.99
10.29-99.04
79.89-97.75
77.65-98.47
85.10-99.54

CIFAR-10-C

45.70-75.57
57.29-81.71
52.58-83.19
68.32-96.19
37.23-73.88
59.71-91.27
57.73-94.29
54.93-87.77
62.62-89.90
61.93-95.16
68.88-95.65
49.64-93.63
63.91-94.03
67.38-91.97
64.43-83.60

34.70-73.70
42.12-75.33
42.74-76.43
66.24-95.85
40.17-73.61
49.23-86.62
49.07-92.93
59.97-88.74
54.64-84.75
55.08-93.99
67.51-94.98
38.69-91.56
63.78-93.46
60.46-89.24
61.23-81.53

29.86-98.77
84.32-99.00
27.77-96.39
3.73-20.46
15.63-70.29
20.29-65.58
78.30-99.22
16.53-93.73
9.18-96.63
9.73-85.28
9.96-98.60
9.74-98.08
44.52-94.33
62.11-92.34
84.78-99.44

30.07-72.54
37.34-74.12
24.28-66.53
52.11-91.50
30.33-68.81
48.20-86.27
42.70-91.18
58.91-86.40
53.15-83.61
45.97-91.18
65.73-94.44
30.37-87.35
55.80-86.46
42.55-78.97
59.08-80.11

16.92-96.02
83.12-98.68
16.80-88.67
1.96-18.30
11.12-54.46
18.59-61.07
74.93-98.7
11.39-95.73
9.05-95.36
9.69-73.37
9.10-97.06
8.02-92.11
12.32-71.43
54.96-89.33
84.85-99.36

25.08-71.14
31.77-71.97
15.32-61.54
31.98-86.14
32.24-68.36
40.05-81.53
35.24-87.35
54.09-82.56
44.76-76.56
32.22-75.58
60.59-92.64
15.53-64.79
53.64-80.07
28.45-75.55
54.93-77.30

sampling randomness, each baseline method obtains different rank-
ing results over 50 experiments, which is indicated by the large
standard deviation (up to 0.36, shaded area in the figure) at each
labeling budget. As a result, the rank by one experiment is not
reliable by occasionally being good and poor. In particular, the stan-
dard deviation becomes smaller when more data are labeled, which
means the ranking method highly relies on the labeling budget. By
contrast, since our approach is labeling-free, there is no sampling
randomness, in other words, the rank is deterministic.

Additionally, Figure 4 presents the effectiveness of all ranking
methods based on Kendallâ€™s ğœ rank correlation. By comparison, the
result confirms the conclusion drawn from the analysis based on

Spearmanâ€™s rank-order correlation. Namely, our approach stands
out concerning the effectiveness without sampling randomness.

Besides, to demonstrate the significance of the two statistical
analyses, we calculate the corresponding ğ‘-value of all methods. A
ğ‘-value lower than the common significance level 0.05 indicates that
the ranking is strongly correlated with the ground truth. Except for
the iWildCam dataset, the ranking results by our approach are all
strongly correlated. However, due to the effectiveness and sampling
randomness, the baseline methods always achieve insignificant
rankings. For the iWildCam dataset, we believe the reason is that
the difference between multiple DNNs is too slight given 182 classes.
For instance, the accuracy difference between the best and worst is
only 1.54% (Table 1). The impact of the accuracy/robustness on the
ranking is investigated in Section 5.3.

On the other hand, we evaluate different methods concerning
identifying the top-ğ‘˜ DNNs (ğ‘˜ = 1, 3, 5, 10). Table 3 lists the result of
Jaccard similarity. On average, our approach achieves the best result
regardless of the datasets. It is better than the worst performance
by up to 0.33, 0.32, 0.33, and 0.27 in the top 1, 3, 5, 10 rankings,
respectively. Concretely, in the top-1 ranking, for datasets MNIST,
Fashion-MNIST, and iWildCam, all methods (Random, SDS, and
ours) are not effective (under 0.08). Remark that CES takes the best
results of all models for each labeling budget when knowing the
ground truth. In other words, actually, it takes ğ‘› (number of DNNs)
times of labeling budget. Therefore, it sometimes outperforms the
others but is not applicable in practice.

Table 3: Jaccard similarity of ranking the top-ğ‘˜ DNNs based
on the clean accuracy. For Random, SDS, and CES, we report
the average results over all labeling budgets. The best per-
formance is highlighted in gray. The higher the better.

k=1

k=3

Jaccard Method MNIST Fashion-MNIST CIFAR-10
0.19
0.17
0.21
1.00
0.36
0.37
0.39
1.00
0.50
0.60
0.55
1.00
0.80
0.85
0.82
1.00

Random
SDS
CES
Our
Random
SDS
CES
Our
Random
SDS
CES
Our
Random
SDS
CES
Our

0.02
0.07
0.23
0.00
0.12
0.24
0.28
0.20
0.19
0.33
0.36
0.43
0.35
0.55
0.46
0.54

0.01
0.03
0.65
0.00
0.07
0.11
0.69
0.20
0.11
0.17
0.89
0.25
0.23
0.41
0.73
0.67

k=10

k=5

iWildCam Amazon Java250 C++1000 Average
0.10
0.20
0.15
0.36
0.45
0.85
0.43
0.00
0.19
0.27
0.27
0.40
0.46
0.72
0.51
0.50
0.28
0.38
0.39
0.50
0.53
0.72
0.61
0.67
0.49
0.63
0.62
0.80
0.61
0.68
0.76
1.00

0.12
0.20
0.15
1.00
0.17
0.31
0.21
1.00
0.25
0.39
0.29
1.00
0.43
0.49
0.46
0.67

0.09
0.20
0.94
1.00
0.19
0.29
0.80
0.50
0.30
0.44
0.68
0.67
0.58
0.79
0.68
1.00

0.07
0.02
0.10
0.00
0.14
0.20
0.15
0.20
0.22
0.33
0.24
0.25
0.43
0.46
0.44
0.43

Labeling-Free Comparison Testing of Deep Learning Models

, ,

(a) MNIST

(b) Fashion-MNIST

(c) CIFAR-10

(d) iWildCam-ID

(e) Amazon-ID

(f) Java250

(g) C++1000

Figure 3: Spearmanâ€™s rank-order correlation coefficient of ranking results based on ID test data. The higher the better. The
shaded area represents the standard deviation. â€œBudgetâ€ is the number of labeled data (only apply to Random, SDS, and CES).

(a) MNIST

(b) Fashion-MNIST

(c) CIFAR-10

(d) iWildCam-ID

(e) Amazon-ID

(f) Java250

(g) C++1000

Figure 4: Kendallâ€™s ğœ of ranking results based on ID test data. The higher the better. The shaded area represents the standard
deviation. â€œBudgetâ€ is the number of labeled data (only apply to Random, SDS, and CES).

Answer to RQ1: Based on the accuracy of ID test data, our ap-
proach outperforms all the 3 selection-based baseline methods
in outputting strongly correlated ranking. In addition, statistical
analysis demonstrates that the outperforming is significant.

5.2 RQ2: Effectiveness Under Distribution Shift
For the synthetic distribution shift, Tables 4 and 5 summarize the
results of Spearmanâ€™s rank-order correlation and Kendallâ€™s ğœ on

MNIST-C and CIFAR-10-C, respectively. We observe that our ap-
proach achieves the best performance in most cases, for instance,
291 of 300 cases in MNIST-C and 289 of 300 cases in CIFAR-10-C
concerning Spearmanâ€™s correlation, and 287 of 300 and 288 of 300,
respectively, in two benchmarks concerning Kendallâ€™s ğœ. Further-
more, as shown in RQ1, SDS performs the second-best among four
ranking approaches. However, in these two tables, compared to
random and CES, SDS tend to lose its performance (highlight in
yellow). For example, in MNIST-C with Defocus Blur, severity-2,

3045607590105120135150165180Budget0.600.650.700.750.800.850.900.95Spearman coefficientRandomSDSCESOur2540557085100115130145160175Budget0.00.20.40.60.8Spearman coefficientRandomSDSCESOur3045607590105120135150165180Budget0.840.860.880.900.920.940.960.981.00Spearman coefficientRandomSDSCESOur203550658095110125140155170Budget0.10.00.10.20.30.40.50.6Spearman coefficientRandomSDSCESOur203550658095110125140155170Budget0.20.00.20.40.60.8Spearman coefficientRandomSDSCESOur203550658095110125140155170Budget0.40.50.60.70.80.91.0Spearman coefficientRandomSDSCESOur203550658095110125140155170Budget0.30.40.50.60.70.80.9Spearman coefficientRandomSDSCESOur3045607590105120135150165180Budget0.450.500.550.600.650.700.750.800.85Kendall tauRandomSDSCESOur2540557085100115130145160175Budget0.10.00.10.20.30.40.50.60.7Kendall tauRandomSDSCESOur3045607590105120135150165180Budget0.700.750.800.850.900.95Kendall tauRandomSDSCESOur203550658095110125140155170Budget0.10.00.10.20.30.4Kendall tauRandomSDSCESOur203550658095110125140155170Budget0.20.00.20.40.6Kendall tauRandomSDSCESOur203550658095110125140155170Budget0.30.40.50.60.70.80.9Kendall tauRandomSDSCESOur203550658095110125140155170Budget0.20.30.40.50.60.70.80.9Kendall tauRandomSDSCESOur, ,

Guo and Hu, et al.

SDS ranks the models wrongly with a correlation of -0.03 (Table
4), while both random and CES can achieve comparable ranking
performance as our approach. In short, this existing state-of-the-art
approach is sensitive to synthetic distribution shifts, which calls
for the testing under distribution shifts of existing approaches.

Figure 5 shows the performance measured by Jaccard similarity
of MNIST-C and CIFAR-10-C, respectively. In the 75 corruptions of
MNIST-C (Figure 5(a)), both our approach and CES outperform the
random sampling and SDS to precisely identify the top DNNs. On
the other hand, in CIFAR-10-C (Figure 5(b)), our approach achieves
the best performance (similarity of 1) in most cases (173 of 300).

For the natural distribution shift, the results on iWildCam and
Amazon are shown in Figure 6. Our approach can better distinguish
the performance of DNNs than the baseline methods. In particular,
in Amazon, our approach is significantly better by up to 0.70 and
0.53 based on Spearnmanâ€™s correlation and Kendallâ€™s ğœ, respectively.
In addition, concerning the Jaccard similarity in Table 6, our ap-
proach is consistently the best in identifying the top DNNs. Note
that in ranking the top 10 DNNs, our approach is not the best but
is close to the best with a difference of 0.01 or 0.02 in iWildCam
and Amazon, respectively.

In addition, compared to the effectiveness given ID test data, the
ranking by all methods is different since the performance of DNNs
changes given OOD test data. However, we notice the opposite
phenomenon in iWildCam and Amazon. Given ID test data, our
approach achieves 0.39 and 0.80 concerning Spearmanâ€™s rank-order
correlation coefficient for iWildCam and Amazon, respectively.
While given OOD test data, the results are 0.91 and 0.71, respectively.
In other words, the effectiveness improves on the OOD test data
in iWildCam but degrades in Amazon. To make clear the reason
behind this, we analyze the accuracy and robustness of multiple
DNNs on ID and OOD test data (Table 1), respectively. In iWildCam,
the performance difference of its 20 DNNs becomes larger on OOD
test data, from 1.54% to 11.52%. In Amazon, the performance of all
20 DNNs degrades, from 74.84% to 72.35%. Besides, the performance
difference in Amazon becomes smaller. Therefore, we believe that
the modelâ€™s ability and the performance difference among DNNs
have an impact on the ranking effectiveness, which leads to the
investigation in RQ3.

Answer to RQ2: Under different distribution shifts, our approach
still outstands in all ranking methods. Due to the distribution
shift, the performance of DNNs declines, which degrades the
ranking effectiveness. Particularly, the state-of-the-art SDS is
sensitive to synthetic distribution shifts and fail to defeat the
random sampling and CES in many cases.

5.3 RQ3: Impact factors
As mentioned in RQ2, by comparing the ranking effectiveness given
ID and OOD test data, we raise the demand of investigating the
two impact factors, the quality and diversity of multiple DNNs. The
quality refers to the modelâ€™s performance given the test data and is
calculated as the average accuracy or robustness over all DNNs on
each dataset. For instance, in MNIST without distribution shift, the
quality is the average accuracy of 30 DNNs on the ID test data, and
in MNIST-C with Gaussian Noise (severity=1), the quality is the
average robustness of 30 DNNs on the corresponding OOD data.

The diversity indicates the performance difference among DNNs
and is the standard deviation of accuracy or robustness over all
DNNs on each dataset.

Figure 7 plots the distribution of ranking performance (Spear-
manâ€™s correlation) concerning the quality and diversity. In Figure
7(a), most good rankings happen with a high model quality (greater
than 50%). The reason is that in our scenario, we only have the ac-
cess to the predictions of test data of multiple DNNs, which setups
the initial inference of data difficulty and model specialty. There-
fore, the learned Bayesian model can be more precise when the
qualities of DNNs are high. Furthermore, this also explains why
our approach outperforms the selection-based methods. For exam-
ple, the state-of-the-art SDS selects a few discriminative data to
annotate to rank DNNs and the selection of data highly relies on
the predicted labels. As a result, since the low qualities of DNNs
always give a wrong estimation of the discrimination ability of data,
the ranking performance is poorer. For instance, in Java250 and
C++1000, SDS only reaches 0.82 and 0.79 on Spearmanâ€™s correlation
with 20 labeled data, respectively. However, our approach achieves
0.96 and 0.95 in two datasets, respectively, with no labeling effort.
On the other hand, Figure 7(b) reveals that there is a high chance
of a good ranking when DNNs are diverse (larger than 18%). Addi-
tionally, a poor ranking mostly happens when DNNs are too close
to each other, which confirms the result of iWildCam with ID data
(Figure 3(d) and Figure 4(d)) that all ranking methods obtain poor
ranking.

Answer to RQ3: By investigating the impact factors, model qual-
ity and model diversity, of ranking performance, we observe that
when the multiple DNNs have high quality (e.g., the average ac-
curacy/ robustness is over 50%), the performance of DNNs can be
discriminated better. On the other hand, there is a higher chance
of a good ranking when DNNs are more diverse (larger than 18%).

6 THREATS TO VALIDITY
The internal threat is mainly due to the implementation of the base-
line methods, our proposed approach, and the evaluation metrics.
For SDS, we use the original implementation on GitHub provided
by Meng et al. [25]. For random sampling and CES, we implement
it based on the description in [25] and carefully check the result to
be consistent with that in [25]. Regarding the evaluation metrics,
we adopt popular libraries, such as SciPy [5].

The external threat comes from the evaluated tasks, datasets,
DNNs, and baseline methods. Regarding the classification tasks,
we consider three different ones, image, text, and source code. For
the datasets, we select the publicly available datasets. In particu-
lar, for datasets with the synthetic distribution shift (15 types of
natural corruptions) and natural distribution shift, we employ four
public benchmarks. Concerning the DNNs, we collect them (either
the off-the-shelf models or train with the provided scripts) from
different repositories on GitHub. These models are with different
architectures and parameters. For the comparison, we consider
three sample-selection-based baseline methods and apply different
numbers of labeling budgets to imply their performance.

The construct threat mainly lies in the sampling randomness in
the baseline methods and the evaluation measures. To reduce the
impact of randomness, for each baseline method, we repeat each

Labeling-Free Comparison Testing of Deep Learning Models

, ,

Table 4: Spearmanâ€™s rank-order correlation coefficient of ranking results based on MNIST-C and CIFAR-10-C. For Random,
SD, and CES, we compute the average and standard deviation over all labeling budgets and 50-time experiments. The best
performance is highlighted in gray. The values highlighted in yellow are where CES or random outperform SDS. The higher
the better.

Corruption Type

Gaussian Noise
Shot Noise
Impulse Noise
Defocus Blur
Frosted Glass Blur
Motion Blur
Zoom Blur
Snow
Frost
Fog
Brightness
Contrast
Elastic
Pixelate
JPEG

Gaussian Noise
Shot Noise
Impulse Noise
Defocus Blur
Frosted Glass Blur
Motion Blur
Zoom Blur
Snow
Frost
Fog
Brightness
Contrast
Elastic
Pixelate
JPEG

Random

0.80Â±0.08
0.79Â±0.09
0.69Â±0.09
0.92Â±0.03
0.31Â±0.12
0.73Â±0.08
0.78Â±0.08
0.55Â±0.09
0.44Â±0.10
0.43Â±0.10
0.87Â±0.05
0.91Â±0.04
0.94Â±0.03
0.77Â±0.08
0.78Â±0.09

0.83Â±0.07
0.90Â±0.04
0.88Â±0.05
0.94Â±0.02
0.83Â±0.05
0.91Â±0.03
0.91Â±0.03
0.92Â±0.03
0.93Â±0.03
0.94Â±0.02
0.94Â±0.02
0.94Â±0.02
0.93Â±0.02
0.93Â±0.03
0.88Â±0.05

Severity=1
SDS

CES

Our Random

Severity=2
SDS

CES

Our

Random

Severity=3
SDS

CES

Our

Random

Severity=4
SDS

CES

Our

Random

Severity=5
SDS

CES

Our

0.89Â±0.04
0.86Â±0.06
0.75Â±0.07
0.90Â±0.18
0.34Â±0.10
0.78Â±0.05
0.84Â±0.06
0.57Â±0.08
0.43Â±0.09
0.43Â±0.09
0.58Â±0.08
0.52Â±0.08
0.83Â±0.06
0.83Â±0.05
0.86Â±0.06

0.88Â±0.03
0.94Â±0.01
0.92Â±0.02
0.97Â±0.01
0.89Â±0.03
0.95Â±0.01
0.91Â±0.02
0.94Â±0.01
0.96Â±0.01
0.97Â±0.01
0.97Â±0.01
0.97Â±0.01
0.97Â±0.01
0.97Â±0.01
0.93Â±0.02

0.83Â±0.06
0.83Â±0.07
0.89Â±0.05
0.93Â±0.03
0.92Â±0.03
0.93Â±0.03
0.86Â±0.06
0.90Â±0.04
0.96Â±0.02
0.99
0.89Â±0.04
0.93Â±0.03
0.95Â±0.02
0.82Â±0.06
0.83Â±0.07

0.86Â±0.05
0.91Â±0.03
0.90Â±0.04
0.95Â±0.02
0.85Â±0.05
0.92Â±0.03
0.92Â±0.02
0.93Â±0.02
0.94Â±0.02
0.95Â±0.02
0.95Â±0.02
0.95Â±0.02
0.94Â±0.02
0.94Â±0.02
0.90Â±0.03

0.94
0.89
0.95
0.98
0.98
0.94
0.92
0.95
0.99
0.99
0.96
0.98
0.83
0.86
0.91

0.98
0.99
0.99
1
0.97
0.95
1
0.99
0.99
1
0.99
0.99
0.99
0.99
0.97

0.86Â±0.06
0.78Â±0.09
0.56Â±0.10
0.94Â±0.02
0.18Â±0.12
0.70Â±0.08
0.77Â±0.08
0.48Â±0.10
0.44Â±0.10
0.43Â±0.11
0.92Â±0.03
0.94Â±0.03
0.30Â±0.20
0.76Â±0.08
0.78Â±0.08

0.88Â±0.06
0.81Â±0.08
0.84Â±0.06
0.93Â±0.02
0.83Â±0.06
0.91Â±0.03
0.92Â±0.02
0.91Â±0.03
0.92Â±0.03
0.95Â±0.02
0.94Â±0.02
0.95Â±0.02
0.93Â±0.03
0.91Â±0.03
0.83Â±0.08

0.93Â±0.03
0.86Â±0.06
0.63Â±0.08
-0.03Â±0.10
0.22Â±0.10
0.73Â±0.06
0.84Â±0.05
0.48Â±0.09
0.42Â±0.09
0.42Â±0.09
0.45Â±0.09
0.48Â±0.09
0.40Â±0.08
0.82Â±0.06
0.85Â±0.06

0.95Â±0.02
0.86Â±0.04
0.90Â±0.04
0.97Â±0.01
0.93Â±0.02
0.91Â±0.02
0.93Â±0.02
0.93Â±0.02
0.95Â±0.01
0.97Â±0.01
0.97Â±0.01
0.97Â±0.01
0.96Â±0.01
0.92Â±0.02
0.88Â±0.03

0.88Â±0.05
0.83Â±0.07
0.92Â±0.03
0.95Â±0.02
0.92Â±0.03
0.96Â±0.01
0.87Â±0.05
0.96Â±0.02
0.98Â±0.01
0.99Â±0.01
0.93Â±0.03
0.95Â±0.02
0.38Â±0.19
0.84Â±0.06
0.80Â±0.06

0.91Â±0.04
0.84Â±0.06
0.86Â±0.05
0.94Â±0.02
0.85Â±0.05
0.92Â±0.02
0.93Â±0.02
0.92Â±0.03
0.93Â±0.02
0.95Â±0.02
0.95Â±0.02
0.95Â±0.02
0.94Â±0.02
0.92Â±0.02
0.85Â±0.05

0.96
0.89
0.96
0.97
0.98
0.90
0.91
0.99
0.99
0.99
0.98
0.99
-0.01
0.91
0.89

0.91
0.97
0.97
0.98
0.98
0.99
0.99
0.99
0.98
1
0.99
0.99
0.98
0.99
0.99

MNIST-C

0.91Â±0.04
0.78Â±0.09
0.43Â±0.10
0.90Â±0.05
-0.48Â±0.10
0.56Â±0.08
0.77Â±0.08
0.50Â±0.10
0.44Â±0.11
0.39Â±0.10
0.96Â±0.02
0.98Â±0.01
0.88Â±0.05
0.57Â±0.10
0.78Â±0.08

0.93Â±0.03
0.86Â±0.06
0.49Â±0.09
-0.23Â±0.07
-0.49Â±0.08
0.59Â±0.07
0.83Â±0.06
0.50Â±0.08
0.42Â±0.08
0.38Â±0.08
0.44Â±0.09
0.39Â±0.10
0.52Â±0.08
0.58Â±0.08
0.85Â±0.05

CIFAR-10-C

0.94Â±0.02
0.91Â±0.05
0.86Â±0.06
0.92Â±0.03
0.80Â±0.07
0.92Â±0.03
0.93Â±0.02
0.90Â±0.03
0.91Â±0.03
0.95Â±0.02
0.94Â±0.02
0.95Â±0.02
0.92Â±0.03
0.90Â±0.04
0.79Â±0.10

0.94Â±0.01
0.94Â±0.01
0.89Â±0.03
0.93Â±0.02
0.91Â±0.03
0.94Â±0.02
0.94Â±0.02
0.91Â±0.02
0.90Â±0.02
0.97Â±0.01
0.97Â±0.01
0.96Â±0.01
0.92Â±0.02
0.92Â±0.02
0.90Â±0.03

0.92Â±0.03
0.83Â±0.07
0.94Â±0.02
0.92Â±0.03
0.95Â±0.02
0.96Â±0.01
0.87Â±0.05
0.96Â±0.02
0.98Â±0.01
0.98Â±0.01
0.97Â±0.01
0.98Â±0.01
0.89Â±0.04
0.89Â±0.04
0.82Â±0.07

0.95Â±0.02
0.93Â±0.03
0.88Â±0.04
0.93Â±0.02
0.83Â±0.06
0.94Â±0.02
0.94Â±0.02
0.91Â±0.03
0.93Â±0.03
0.96Â±0.01
0.95Â±0.02
0.95Â±0.02
0.93Â±0.02
0.92Â±0.03
0.82Â±0.07

0.97
0.86
0.98
-0.02
0.93
0.87
0.91
0.99
0.99
0.95
0.99
1
0.97
0.97
0.90

0.95
0.93
0.92
0.99
0.98
0.99
0.99
0.99
0.98
0.99
0.99
0.99
0.99
0.98
0.98

0.94Â±0.02
0.75Â±0.09
0.20Â±0.11
0.75Â±0.10
-0.50Â±0.09
0.46Â±0.09
0.77Â±0.08
0.51Â±0.10
0.43Â±0.11
0.4Â±0.11
0.98Â±0.01
0.96Â±0.02
0.94Â±0.03
0.13Â±0.13
0.77Â±0.08

0.94Â±0.02
0.93Â±0.03
0.92Â±0.03
0.93Â±0.02
0.89Â±0.06
0.93Â±0.02
0.93Â±0.02
0.90Â±0.04
0.90Â±0.05
0.96Â±0.02
0.94Â±0.02
0.94Â±0.02
0.91Â±0.03
0.79Â±0.07
0.75Â±0.10

0.96Â±0.02
0.82Â±0.06
0.25Â±0.10
-0.17Â±0.09
-0.50Â±0.07
0.49Â±0.08
0.82Â±0.06
0.51Â±0.08
0.42Â±0.09
0.39Â±0.08
0.43Â±0.09
0.24Â±0.09
-0.29Â±0.08
0.09Â±0.11
0.84Â±0.05

0.93Â±0.01
0.93Â±0.01
0.86Â±0.02
0.94Â±0.02
0.96Â±0.01
0.94Â±0.02
0.95Â±0.02
0.93Â±0.02
0.90Â±0.03
0.97Â±0.01
0.97Â±0.01
0.96Â±0.01
0.92Â±0.02
0.79Â±0.05
0.88Â±0.03

0.95Â±0.02
0.81Â±0.07
0.97Â±0.01
0.80Â±0.08
0.95Â±0.02
0.95Â±0.02
0.89Â±0.04
0.96Â±0.02
0.99Â±0.01
0.98Â±0.01
0.98Â±0.01
0.97Â±0.01
0.94Â±0.02
0.87Â±0.05
0.82Â±0.06

0.95Â±0.01
0.94Â±0.02
0.93Â±0.02
0.94Â±0.02
0.92Â±0.03
0.94Â±0.02
0.94Â±0.02
0.91Â±0.03
0.92Â±0.03
0.96Â±0.01
0.95Â±0.02
0.95Â±0.02
0.92Â±0.03
0.82Â±0.06
0.79Â±0.08

0.98
0.89
0.98
-0.07
0.87
0.50
0.90
0.98
0.99
0.91
1
0.96
0.97
0.98
0.90

0.94
0.96
0.85
0.99
0.94
0.98
0.99
0.99
0.99
0.99
0.99
0.91
0.99
0.87
0.99

0.98Â±0.01
0.69Â±0.09
0.01Â±0.11
0.69Â±0.14
-0.31Â±0.10
0.45Â±0.09
0.75Â±0.08
0.48Â±0.10
0.44Â±0.11
0.33Â±0.11
0.98Â±0.01
0.89Â±0.06
0.96Â±0.01
-0.31Â±0.13
0.77Â±0.08

0.95Â±0.02
0.94Â±0.02
0.93Â±0.03
0.94Â±0.02
0.86Â±0.07
0.93Â±0.02
0.93Â±0.02
0.90Â±0.04
0.86Â±0.05
0.95Â±0.02
0.94Â±0.02
0.88Â±0.05
0.82Â±0.09
0.93Â±0.04
0.70Â±0.12

0.98Â±0.01
0.75Â±0.06
0.06Â±0.10
0.03Â±0.09
-0.34Â±0.08
0.49Â±0.08
0.81Â±0.06
0.47Â±0.09
0.42Â±0.08
0.32Â±0.09
0.44Â±0.08
0.13Â±0.08
-0.65Â±0.06
-0.34Â±0.10
0.83Â±0.06

0.93Â±0.01
0.92Â±0.02
0.92Â±0.02
0.96Â±0.01
0.96Â±0.02
0.95Â±0.02
0.96Â±0.01
0.89Â±0.03
0.87Â±0.03
0.96Â±0.01
0.97Â±0.01
0.90Â±0.02
0.92Â±0.02
0.88Â±0.05
0.86Â±0.04

0.98Â±0.01
0.84Â±0.06
0.98Â±0.01
0.76Â±0.10
0.92Â±0.03
0.94Â±0.03
0.89Â±0.04
0.97Â±0.01
0.98Â±0.01
0.97Â±0.01
0.99Â±0.01
0.91Â±0.05
0.97Â±0.01
0.90Â±0.04
0.81Â±0.06

0.95Â±0.01
0.95Â±0.02
0.95Â±0.02
0.95Â±0.02
0.90Â±0.04
0.94Â±0.02
0.94Â±0.02
0.91Â±0.03
0.89Â±0.04
0.96Â±0.01
0.95Â±0.02
0.90Â±0.04
0.85Â±0.06
0.94Â±0.02
0.74Â±0.09

0.99
0.97
0.98
0.03
0.09
-0.18
0.91
0.99
0.99
-0.14
0.98
0.23
-0.45
0.88
0.89

0.93
0.91
0.85
0.98
0.93
0.99
0.99
0.99
0.94
0.96
0.99
0.86
0.97
0.72
0.98

Table 5: Kendallâ€™s ğœ of ranking results based on MNIST-C and CIFAR-10-C. For Random, SD, and CES, we compute the average
and standard deviation over all labeling budgets and 50-time experiments. The best performance is highlighted in gray. The
values highlighted in yellow are where CES or random outperform SDS. The higher the better.

Corruption Type

Gaussian Noise
Shot Noise
Impulse Noise
Defocus Blur
Frosted Glass Blur
Motion Blur
Zoom Blur
Snow
Frost
Fog
Brightness
Contrast
Elastic
Pixelate
JPEG

Gaussian Noise
Shot Noise
Impulse Noise
Defocus Blur
Frosted Glass Blur
Motion Blur
Zoom Blur
Snow
Frost
Fog
Brightness
Contrast
Elastic
Pixelate
JPEG

Random

0.68Â±0.08
0.68Â±0.09
0.58Â±0.08
0.80Â±0.05
0.27Â±0.09
0.6Â±0.08
0.67Â±0.08
0.47Â±0.08
0.38Â±0.08
0.35Â±0.08
0.77Â±0.06
0.82Â±0.05
0.84Â±0.04
0.65Â±0.08
0.67Â±0.09

0.68Â±0.08
0.75Â±0.06
0.74Â±0.06
0.82Â±0.04
0.69Â±0.06
0.77Â±0.05
0.78Â±0.04
0.79Â±0.05
0.79Â±0.05
0.82Â±0.04
0.82Â±0.04
0.82Â±0.04
0.80Â±0.04
0.80Â±0.04
0.73Â±0.06

Severity=1
SDS

CES

Our Random

Severity=2
SDS

CES

Our Random

Severity=3
SDS

CES

Our Random

Severity=4
SDS

CES

Our Random

Severity=5
SDS

CES

Our

0.79Â±0.05
0.75Â±0.07
0.64Â±0.06
0.79Â±0.16
0.29Â±0.06
0.65Â±0.06
0.73Â±0.06
0.51Â±0.06
0.38Â±0.06
0.36Â±0.07
0.53Â±0.06
0.47Â±0.06
0.70Â±0.07
0.72Â±0.06
0.75Â±0.07

0.72Â±0.04
0.80Â±0.03
0.78Â±0.03
0.87Â±0.02
0.76Â±0.04
0.84Â±0.03
0.76Â±0.03
0.81Â±0.03
0.86Â±0.02
0.88Â±0.02
0.87Â±0.03
0.88Â±0.03
0.86Â±0.03
0.86Â±0.02
0.80Â±0.03

0.71Â±0.07
0.72Â±0.07
0.77Â±0.06
0.82Â±0.04
0.80Â±0.05
0.81Â±0.04
0.75Â±0.06
0.79Â±0.05
0.88Â±0.03
0.93Â±0.02
0.79Â±0.05
0.84Â±0.04
0.85Â±0.04
0.70Â±0.07
0.71Â±0.07

0.71Â±0.06
0.77Â±0.05
0.76Â±0.05
0.83Â±0.04
0.71Â±0.06
0.79Â±0.05
0.79Â±0.04
0.80Â±0.04
0.81Â±0.04
0.83Â±0.04
0.83Â±0.03
0.83Â±0.04
0.82Â±0.04
0.82Â±0.04
0.75Â±0.05

0.82
0.77
0.86
0.92
0.89
0.83
0.81
0.84
0.96
0.94
0.85
0.92
0.70
0.73
0.81

0.92
0.94
0.93
0.97
0.88
0.89
0.96
0.94
0.93
0.97
0.96
0.96
0.95
0.94
0.89

0.74Â±0.07
0.67Â±0.09
0.48Â±0.08
0.83Â±0.04
0.17Â±0.08
0.58Â±0.08
0.66Â±0.09
0.41Â±0.08
0.37Â±0.09
0.35Â±0.09
0.82Â±0.04
0.86Â±0.04
0.22Â±0.15
0.64Â±0.08
0.67Â±0.09

0.74Â±0.07
0.65Â±0.08
0.69Â±0.07
0.81Â±0.04
0.68Â±0.06
0.78Â±0.05
0.79Â±0.04
0.78Â±0.05
0.79Â±0.05
0.84Â±0.04
0.82Â±0.04
0.84Â±0.04
0.80Â±0.04
0.78Â±0.05
0.66Â±0.08

0.82Â±0.04
0.75Â±0.07
0.52Â±0.06
-0.02Â±0.06
0.19Â±0.07
0.61Â±0.06
0.73Â±0.06
0.42Â±0.06
0.36Â±0.07
0.34Â±0.07
0.40Â±0.06
0.43Â±0.06
0.31Â±0.06
0.70Â±0.06
0.74Â±0.06

0.82Â±0.03
0.69Â±0.05
0.76Â±0.06
0.87Â±0.03
0.81Â±0.04
0.78Â±0.03
0.80Â±0.03
0.80Â±0.04
0.85Â±0.03
0.88Â±0.02
0.87Â±0.03
0.89Â±0.02
0.84Â±0.03
0.78Â±0.03
0.72Â±0.04

0.77Â±0.06
0.71Â±0.07
0.80Â±0.05
0.84Â±0.03
0.81Â±0.04
0.86Â±0.03
0.76Â±0.06
0.87Â±0.03
0.92Â±0.02
0.93Â±0.02
0.84Â±0.04
0.87Â±0.03
0.28Â±0.15
0.72Â±0.06
0.69Â±0.07

0.77Â±0.05
0.69Â±0.07
0.71Â±0.06
0.82Â±0.04
0.71Â±0.06
0.79Â±0.04
0.81Â±0.04
0.80Â±0.04
0.80Â±0.04
0.85Â±0.03
0.83Â±0.04
0.85Â±0.03
0.81Â±0.04
0.79Â±0.04
0.69Â±0.06

0.86
0.78
0.85
0.88
0.89
0.76
0.80
0.95
0.94
0.92
0.91
0.93
-0.03
0.79
0.76

0.78
0.87
0.88
0.93
0.88
0.94
0.97
0.95
0.93
0.97
0.94
0.95
0.92
0.95
0.94

MNIST-C

0.80Â±0.05
0.67Â±0.10
0.35Â±0.08
0.77Â±0.06
-0.37Â±0.08
0.47Â±0.07
0.66Â±0.08
0.43Â±0.09
0.36Â±0.09
0.32Â±0.08
0.89Â±0.03
0.91Â±0.02
0.74Â±0.06
0.46Â±0.09
0.66Â±0.08

0.83Â±0.04
0.75Â±0.07
0.39Â±0.06
-0.18Â±0.06
-0.36Â±0.07
0.49Â±0.06
0.71Â±0.06
0.43Â±0.07
0.35Â±0.07
0.30Â±0.06
0.39Â±0.06
0.34Â±0.07
0.42Â±0.07
0.47Â±0.07
0.74Â±0.06

CIFAR-10-C

0.81Â±0.04
0.77Â±0.06
0.72Â±0.07
0.79Â±0.04
0.65Â±0.07
0.79Â±0.04
0.80Â±0.04
0.76Â±0.05
0.78Â±0.05
0.84Â±0.04
0.82Â±0.04
0.84Â±0.04
0.78Â±0.05
0.76Â±0.05
0.63Â±0.10

0.82Â±0.03
0.82Â±0.03
0.72Â±0.04
0.81Â±0.03
0.77Â±0.05
0.81Â±0.04
0.81Â±0.03
0.78Â±0.04
0.77Â±0.03
0.89Â±0.03
0.87Â±0.03
0.87Â±0.03
0.79Â±0.03
0.79Â±0.04
0.74Â±0.04

0.82Â±0.04
0.72Â±0.07
0.83Â±0.04
0.79Â±0.05
0.85Â±0.04
0.85Â±0.03
0.76Â±0.06
0.88Â±0.03
0.92Â±0.02
0.91Â±0.02
0.90Â±0.02
0.92Â±0.02
0.76Â±0.06
0.76Â±0.05
0.71Â±0.07

0.83Â±0.03
0.80Â±0.04
0.74Â±0.05
0.80Â±0.04
0.68Â±0.06
0.81Â±0.04
0.82Â±0.04
0.78Â±0.04
0.80Â±0.04
0.85Â±0.03
0.83Â±0.03
0.85Â±0.03
0.80Â±0.04
0.78Â±0.04
0.65Â±0.07

0.87
0.74
0.89
0.02
0.81
0.73
0.82
0.93
0.93
0.83
0.95
0.97
0.87
0.87
0.78

0.84
0.80
0.78
0.93
0.89
0.92
0.96
0.93
0.91
0.96
0.94
0.93
0.94
0.91
0.91

0.84Â±0.04
0.63Â±0.09
0.16Â±0.08
0.61Â±0.10
-0.39Â±0.08
0.38Â±0.07
0.65Â±0.08
0.43Â±0.08
0.36Â±0.09
0.32Â±0.09
0.91Â±0.02
0.88Â±0.04
0.82Â±0.05
0.11Â±0.10
0.66Â±0.08

0.82Â±0.04
0.80Â±0.04
0.79Â±0.05
0.80Â±0.04
0.75Â±0.07
0.80Â±0.04
0.81Â±0.04
0.76Â±0.05
0.77Â±0.06
0.85Â±0.03
0.82Â±0.04
0.83Â±0.04
0.76Â±0.05
0.64Â±0.07
0.59Â±0.09

0.86Â±0.03
0.70Â±0.06
0.20Â±0.07
-0.13Â±0.06
-0.39Â±0.06
0.40Â±0.06
0.70Â±0.06
0.44Â±0.06
0.35Â±0.07
0.31Â±0.06
0.36Â±0.07
0.21Â±0.06
-0.18Â±0.06
0.08Â±0.07
0.73Â±0.06

0.80Â±0.03
0.79Â±0.03
0.69Â±0.03
0.82Â±0.04
0.84Â±0.03
0.81Â±0.04
0.83Â±0.03
0.81Â±0.04
0.76Â±0.03
0.89Â±0.02
0.88Â±0.02
0.86Â±0.03
0.78Â±0.03
0.62Â±0.05
0.71Â±0.05

0.86Â±0.03
0.69Â±0.07
0.89Â±0.03
0.65Â±0.08
0.84Â±0.04
0.83Â±0.04
0.78Â±0.05
0.88Â±0.03
0.93Â±0.02
0.90Â±0.02
0.92Â±0.02
0.89Â±0.03
0.83Â±0.04
0.74Â±0.06
0.71Â±0.07

0.84Â±0.03
0.82Â±0.04
0.80Â±0.04
0.82Â±0.04
0.79Â±0.05
0.82Â±0.04
0.82Â±0.04
0.78Â±0.04
0.79Â±0.05
0.87Â±0.03
0.84Â±0.04
0.85Â±0.03
0.78Â±0.04
0.66Â±0.07
0.62Â±0.07

0.91
0.76
0.89
-0.07
0.71
0.36
0.81
0.91
0.94
0.77
0.97
0.84
0.87
0.89
0.79

0.80
0.83
0.66
0.93
0.81
0.92
0.95
0.94
0.94
0.96
0.94
0.78
0.92
0.72
0.92

0.91Â±0.02
0.57Â±0.09
0.03Â±0.08
0.56Â±0.13
-0.24Â±0.08
0.36Â±0.07
0.63Â±0.08
0.41Â±0.09
0.36Â±0.09
0.27Â±0.08
0.91Â±0.03
0.76Â±0.08
0.87Â±0.03
-0.22Â±0.09
0.65Â±0.08

0.83Â±0.04
0.81Â±0.04
0.81Â±0.04
0.82Â±0.04
0.72Â±0.08
0.81Â±0.04
0.81Â±0.04
0.76Â±0.05
0.72Â±0.06
0.84Â±0.04
0.82Â±0.04
0.74Â±0.06
0.66Â±0.09
0.81Â±0.05
0.54Â±0.10

0.91Â±0.02
0.61Â±0.06
0.07Â±0.07
0.04Â±0.06
-0.25Â±0.06
0.39Â±0.06
0.68Â±0.06
0.41Â±0.07
0.34Â±0.06
0.26Â±0.06
0.35Â±0.06
0.10Â±0.05
-0.48Â±0.06
-0.24Â±0.07
0.72Â±0.06

0.80Â±0.03
0.76Â±0.03
0.77Â±0.03
0.86Â±0.03
0.86Â±0.03
0.84Â±0.03
0.85Â±0.03
0.75Â±0.04
0.72Â±0.04
0.86Â±0.02
0.88Â±0.02
0.73Â±0.03
0.78Â±0.04
0.76Â±0.06
0.70Â±0.06

0.92Â±0.02
0.71Â±0.07
0.90Â±0.02
0.61Â±0.10
0.80Â±0.04
0.81Â±0.04
0.78Â±0.06
0.90Â±0.03
0.92Â±0.02
0.88Â±0.03
0.92Â±0.02
0.78Â±0.07
0.89Â±0.03
0.77Â±0.05
0.69Â±0.07

0.84Â±0.03
0.83Â±0.04
0.83Â±0.03
0.83Â±0.03
0.77Â±0.05
0.82Â±0.04
0.82Â±0.04
0.78Â±0.05
0.75Â±0.05
0.86Â±0.03
0.84Â±0.04
0.78Â±0.05
0.69Â±0.07
0.83Â±0.04
0.58Â±0.09

0.93
0.87
0.92
0
0.05
-0.13
0.84
0.94
0.93
-0.07
0.89
0.12
-0.31
0.72
0.78

0.78
0.74
0.67
0.90
0.80
0.94
0.92
0.94
0.83
0.86
0.95
0.67
0.90
0.61
0.91

experiment concerning the labeling budgets, datasets 50 times and
report the results of both average and standard deviation. Since our
proposed approach does not rely on sampling data to annotate, there
is no sampling randomness. Considering the randomness (gradient
ascent search) in the EM algorithm, we repeat our approach 50
times and found that the randomness was negligible (less than
1.84E-03). Regarding the evaluation measures, we consider three
popular statistical analyses. The Kendallâ€™s ğœ rank correlation and

Spearmanâ€™s rank-order correlation can infer the effectiveness of
the methods concerning the general ranking, while the Jaccard
similarity can specifically check the performance concerning the
top-ğ‘˜ ranking. Besides, for the statistical analyses, we report the
ğ‘-value to demonstrate the significance.

, ,

Guo and Hu, et al.

(a) MNIST-C

(b) CIFAR-10-C

Figure 5: Jaccard similarity of ranking the top-5 DNNs. For Random, SDS, and CES, we report the average results over all
labeling budgets. At each corruption, we report the performance of 5 levels of severity (e.g., Gaussian Noise-3). The higher the
better.

Table 6: Jaccard similarity of ranking the top-ğ‘˜ DNNs based
on the OOD test data of iWildCam and Amazon with natu-
ral distribution shift. For Random, SDS, and CES, we report
the average results over all labeling budgets. The best per-
formance is highlighted in gray. The higher the better.

(a) iWildCam-OOD

(b) Amazon-OOD

Jaccard Dataset

ğ‘˜=1

ğ‘˜=3

ğ‘˜=5

ğ‘˜=10

iWildCam
Amazon
iWildCam
Amazon
iWildCam
Amazon
iWildCam
Amazon

Random SDS CES Our
1
0.96
1
0.22
1
0.63
1
0.28
0.67
0.61
1
0.39
0.82
0.83
0.43
0.44

0.68
0.15
0.41
0.21
0.47
0.29
0.65
0.45

0.66
0.1
0.4
0.16
0.44
0.23
0.62
0.41

(c) iWildCam-OOD

(d) Amazon-OOD

Figure 6: Spearmanâ€™s rank-order correlation coefficient (first
row) and Kendallâ€™s ğœ (second row) of ranking results based
on OOD test data. The higher the better. The shaded area
represents the standard deviation. â€œBudgetâ€ is the number
of labeled data (only apply to Random, SDS, and CES).

7 CONCLUSION
Observing the limitations (labeling effort, sampling randomness,
and performance degradation on out-of-distribution data) of exist-
ing selection-based methods, we proposed a labeling-free approach
to undertake the task of ranking multiple deep neural networks
(DNNs) without the need of domain expertise. The main idea is to
build a Bayesian model given the predicted labels of data, which
allows for free labeling and non-sampling randomness. The ex-
perimental results on various domains (image, text, and source

(a) Quality Vs correlation

(b) Diversity Vs correlation

Figure 7: The impact of model quality/diversity on the rank-
ing performance of our approach. Each blue point indicates
the Spearmanâ€™s correlation coefficient of a specific dataset
and the corresponding DNNs. All 159 datasets are included.
For each figure, we annotate the point by the (a) diversity
and (b) quality where the correlation is lower than 0.7.

code) and different performance criteria (accuracy and robustness
against synthetic and natural distribution shifts) demonstrate that

203550658095110125140155170Budget0.00.20.40.60.8Spearman coefficientRandomSDSCESOur203550658095110125140155170Budget0.20.00.20.40.6Spearman coefficientRandomSDSCESOur203550658095110125140155170Budget0.00.20.40.60.8Kendall tauRandomSDSCESOur203550658095110125140155170Budget0.20.10.00.10.20.30.40.5Kendall tauRandomSDSCESOur0.20.40.60.81.0Model quality0.40.20.00.20.40.60.81.0Spearman correlation0.090.040.030.110.180.160.010.180.120.110.000.000.050.100.150.200.25Model diversity0.40.20.00.20.40.60.81.0Spearman correlation0.270.130.100.350.320.210.150.410.450.400.76Labeling-Free Comparison Testing of Deep Learning Models

, ,

our approach significantly outperforms the three baseline meth-
ods concerning both Spearmanâ€™s correlation and Kendallâ€™s ğœ. In
addition, the results of the Jaccard similarity show the efficiency
of our approach in identifying the top-ğ‘˜ (ğ‘˜ = 1, 3, 5, 10) DNNs. To
promote replication of our study, we make the source code and its
documentations available2.

Observing the ranking difference on ID and out-of-distribution
(OOD) test data, our approach might be useful to detect the existence
of distribution shifts. We will consider this in the future work.

REFERENCES
[1] 2018. AIZU online judge.

accessed 10 January 2021.

https://judge.u-aizu.ac.jp/onlinejudge/ Online;

[2] 2021. DNN models for Fashion-MNIST. github.com/Testing-Multiple-DL-Models/

SDS/tree/main/models. Online; accessed 2 November 2021.

[3] 2021. Project website of ranking multiple DNNs. https://sites.google.com/view/

ranking-of-multiple-dnns

[4] 2022. GitHub. https://github.com/. Online; accessed 25 January 2022.
[5] 2022. SciPy. https://scipy.org/. Online; accessed 27 January 2022.
[6] Rob Alexander, Rob Ashmore, Andrew Banks, Ben Bradshaw, John Bragg, John
Clegg, Christopher Harper, Catherine Menon, Roger Rivett, Philippa Ryan, Nick
Tudor, Stuart Tushingham, John Birch, Lavinia Burski, Timothy Coley, Neil
Lewis, Ken Neal, Ashley Price, Stuart Reid, and Rod Steel. 2020. Safety assurance
objectives for autonomous systems.

[7] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1â€“29. https://doi.org/10.1145/3290353

[8] Sara Beery, Elijah Cole, and Arvi Gjoka. 2020. The iWildCam 2020 Competition

Dataset. arXiv preprint arXiv:2004.10340 (2020).

[9] David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun
Zhao. 2020. Cats are not fish: deep learning testing calls for out-of-distribution
awareness. Association for Computing Machinery, New York, NY, USA, 1041â€“1052.
https://doi.org/10.1145/3324884.3416609

[10] Junjie Chen, Zhuo Wu, Zan Wang, Hanmo You, Lingming Zhang, and Ming Yan.
2020. Practical accuracy estimation for efficient deep neural network testing.
ACM Transactions on Software Engineering and Methodology 29, 4, Article 30
(October 2020), 35 pages. https://doi.org/10.1145/3394112

[11] W. Wayne Daniel. 1990. Applied nonparametric statistics. PWS-KENT Pub.

https://books.google.lu/books?id=0hPvAAAAMAAJ

[12] A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from
incomplete data via the em algorithm. Journal of the Royal Statistical Society.
Series B (Methodological) 39, 1 (1977), 1â€“38. http://www.jstor.org/stable/2984875
[13] Swaroopa Dola, Matthew B Dwyer, and Mary Lou Soffa. 2021. Distribution-
aware testing of neural networks using generative models. In 2021 IEEE/ACM
43rd International Conference on Software Engineering (ICSE). IEEE, 226â€“237.
https://doi.org/10.1109/ICSE43902.2021.00032

[14] Robert L. Ebel. 1954. Procedures for the analysis of classroom tests. Educational
and Psychological Measurement 14, 2 (1954), 352â€“364. https://doi.org/10.1177/
001316445401400215

[15] Xiang Gao, Ripon K Saha, Mukul R Prasad, and Abhik Roychoudhury. 2020. Fuzz
testing based data augmentation to improve robustness of deep neural networks.
In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
(Seoul, South Korea) (ICSE â€™20). IEEE, New York, NY, USA, 1147â€“1158. https:
//doi.org/10.1145/3377811.3380415

[16] Dan Hendrycks and Thomas Dietterich. 2019. Benchmarking neural network
robustness to common corruptions and perturbations. Proceedings of the Inter-
national Conference on Learning Representations (2019). https://openreview.net/
forum?id=HJz6tiCqYm

[17] Rui Hu, Jitao Sang, Jinqiang Wang, and Chaoquan Jiang. 2021. Understanding
and testing generalization of deep networks on out-of-distribution data. arXiv
preprint arXiv:2111.09190 (2021).

[18] Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao, Dingfu Zhou, Peng
Wang, Yuanqing Lin, and Ruigang Yang. 2018. The apolloscape dataset for
autonomous driving. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops. 954â€“960. https://doi.org/10.1109/CVPRW.
2018.00141

[19] S Kavitha and D Jeevitha. 2014. Software testing methods and techniques. In
International Conference on Information and Image Processing (ICIIP-2014).
[20] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang,
Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips,
Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw,
Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson,

2We will make the implementation publicly available upon acceptance.

Sergey Levine, Chelsea Finn, and Percy Liang. 2021. WILDS: A Benchmark of
in-the-Wild Distribution Shifts. In International Conference on Machine Learning
(ICML).

[21] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.

Technical Report. University of Toronto, Toronto.

[22] Yann Lecun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (November
1998), 2278 â€“ 2324.

[23] Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and Jian LÃ¼. 2019.
Boosting operational dnn testing efficiency through conditioning. In Proceedings
of the 2019 27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (Tallinn, Estonia)
(ESEC/FSE 2019). Association for Computing Machinery, New York, NY, USA,
499â€“509. https://doi.org/10.1145/3338906.3338930

[24] Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, and Yves Le Traon.
2021. Test Selection for Deep Learning Systems. ACM Trans. Softw. Eng. Methodol.
30, 2 (2021), 13:1â€“13:22. https://doi.org/10.1145/3417330

[25] Linghan Meng, Yanhui Li, Lin Chen, Zhi Wang, Di Wu, Yuming Zhou, and Baowen
Xu. 2021. Measuring discrimination to boost comparative testing for multiple
deep learning models. In IEEE/ACM 43rd International Conference on Software
Engineering (ICSE). IEEE Computer Society, Los Alamitos, CA, USA, 385â€“396.
https://doi.org/10.1109/ICSE43902.2021.00045

[26] Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam
Chenaghlu, and Jianfeng Gao. 2021. Deep learningâ€“based text classification:
a comprehensive review. Comput. Surveys 54, 3 (April 2021). https://doi.org/10.
1145/3439726

[27] Norman Mu and Justin Gilmer. 2019. MNIST-C: A Robustness Benchmark for

Computer Vision. CoRR abs/1906.02337 (2019). arXiv:1906.02337

[28] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations
using distantly-labeled reviews and fine-grained aspects. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
[29] Niall Oâ€™Mahony, Sean Campbell, Anderson Carvalho, Suman Harapanahalli,
Gustavo Velasco Hernandez, Lenka Krpalkova, Daniel Riordan, and Joseph Walsh.
2020. Deep learning vs. traditional computer vision. In Advances in Computer
Vision, Kohei Arai and Supriya Kapoor (Eds.). Springer International Publishing,
Cham, 128â€“144. https://doi.org/10.1007/978-3-030-17795-9_10

[30] Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi,
Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker,
Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan
Malaika, and Frederick Reiss. 2021. CodeNet: a large-scale AI for code dataset
for learning a diversity of coding tasks. arXiv:2105.12655 [cs.SE]

[31] Omer Sagi and Lior Rokach. 2018. Ensemble learning: a survey. WIREs Data
Mining and Knowledge Discovery 8, 4 (2018). https://doi.org/10.1002/widm.1249
[32] Abhijit Sawant, Pranit Bari, and Pramila Chawan. 2012. Software testing tech-
niques and strategies. International Journal of Engineering Research and Applica-
tions(IJERA) 2 (06 2012), 980â€“986.

[33] PB Selvapriya. 2013. Different software testing strategies and techniques. Inter-

national Journal of Science and Modern Engineering 2, 1 (2013).

[34] Weijun Shen, Yanhui Li, Lin Chen, Yuanlei Han, Yuming Zhou, and Baowen
Xu. 2020. Multiple-boundary clustering and prioritization to promote neural
network retraining. In IEEE/ACM International Conference on Automated Software
Engineering (Melbourne, VIC, Australia). Association for Computing Machinery,
New York, United States, 410â€“422.

[35] Yan Sun, Celia Chen, Qing Wang, and Barry Boehm. 2017. Improving missing
issue-commit link recovery using positive and unlabeled data. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
147â€“152. https://doi.org/10.1109/ASE.2017.8115627

[36] Jingyi Wang, Jialuo Chen, Youcheng Sun, Xingjun Ma, Dongxia Wang, Jun Sun,
and Peng Cheng. 2021. RobOT: robustness-oriented testing for deep learning
systems. 300â€“311. https://doi.org/10.1109/ICSE43902.2021.00038

[37] Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan.
2009. Whose vote should count more: optimal integration of labels from labelers of
unknown expertise. In Proceedings of the 22nd International Conference on Neural
Information Processing Systems (Vancouver, British Columbia, Canada) (NIPSâ€™09).
Curran Associates Inc., Red Hook, NY, USA, 2035â€“2043. https://proceedings.
neurips.cc/paper/2009/file/f899139df5e1059396431415e770c6dd-Paper.pdf

[38] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017.

Fashion-MNIST:
for benchmarking machine learning algorithms.

a novel
arXiv:cs.LG/1708.07747 [cs.LG]

image dataset

[39] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Machine Learn-
ing Testing: Survey, Landscapes and Horizons. CoRR abs/1906.10742 (2019).
arXiv:1906.10742 http://arxiv.org/abs/1906.10742

[40] Tianming Zhao, Chunyang Chen, Yuanning Liu, and Xiaodong Zhu. 2021.
GUIGAN: learning to generate GUI designs using generative adversarial net-
works. In 2021 IEEE/ACM 43rd International Conference on Software Engineering
(ICSE). IEEE, 748â€“760. https://doi.org/10.1109/ICSE43902.2021.00074

, ,

Guo and Hu, et al.

[41] Yan Zheng, Yi Liu, Xiaofei Xie, Yepang Liu, Lei Ma, Jianye Hao, and Yang Liu.
2021. Automatic web testing using curiosity-driven reinforcement learning. In
2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE).
423â€“435. https://doi.org/10.1109/ICSE43902.2021.00048

[42] Yan Zheng, Xiaofei Xie, Ting Su, Lei Ma, Jianye Hao, Zhaopeng Meng, Yang Liu,
Ruimin Shen, Yingfeng Chen, and Changjie Fan. 2019. Wuji: automatic online
combat game testing using evolutionary deep reinforcement learning. In 2019
34th IEEE/ACM International Conference on Automated Software Engineering (ASE).
772â€“784. https://doi.org/10.1109/ASE.2019.00077

