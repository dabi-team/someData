Constructing Large-Scale Real-World
Benchmark Datasets for AIOps

Zeyan Li1, Nengwen Zhao1, Shenglin Zhang2, Yongqian Sun2
Pengfei Chen3, Xidao Wen1, Minghua Ma4, Dan Pei1
1Tsinghua University, 2Nankai University, 3Sun Yat-sen University, 4Microsoft Reasearch

2
2
0
2

g
u
A
8

]
E
S
.
s
c
[

1
v
8
3
9
3
0
.
8
0
2
2
:
v
i
X
r
a

ABSTRACT
Recently, AIOps (Artificial Intelligence for IT Operations) has been
well studied in academia and industry to enable automated and effec-
tive software service management. Plenty of efforts have been dedi-
cated to AIOps, including anomaly detection, root cause localization,
incident management, etc. However, most existing works are eval-
uated on private datasets, so their generality and real performance
cannot be guaranteed. The lack of public large-scale real-world
datasets has prevented researchers and engineers from enhancing
the development of AIOps. To tackle this dilemma, in this work, we
introduce three public real-world large-scale datasets about AIOps,
mainly aiming at KPI anomaly detection, root cause localization on
multi-dimensional data, and failure discovery and diagnosis. More
importantly, we held three competitions in 2018/2019/2020 based
on these datasets, attracting thousands of teams to participate. In
the future, we will continue to publish more datasets and hold
competitions to promote the development of AIOps further.

1 INTRODUCTION
In recent years, traditional manual operations and maintenance
have gradually become time-consuming and error-prone due to the
increasing complexity of modern IT service architectures. To tackle
this dilemma, Artificial Intelligence for IT Operations (AIOps) has
been proposed to relieve some manual intervention required. Gart-
ner [35] defines AIOps as the application of machine learning (ML)
and data mining to IT operations. AIOps combine big data and ML
functionality to enhance and partially replace traditional manual ef-
forts, including performance monitoring, anomaly detection, event
correlation and analysis, incident management, etc.

In the literature, tremendous efforts have been devoted to the
research of AIOps, including anomaly detection [7, 29, 34, 43], root
cause localization [5, 6, 18, 19, 24, 25, 28, 33, 37, 40, 41, 47], failure
prediction [11, 27, 39, 44, 46, 49], incident management [9, 10, 12,
31], resource management [14, 15] and so on. However, due to
the lack of public large-scale real-world datasets, it is difficult to
compare these approaches fairly. Although these works achieve
promising performance on their respective private datasets, these
approaches could perform unstably in the real world. It is also
difficult to transfer an approach to real generic application due to the
various nature of different target systems for AIOps. Thus, AIOps
is at the crossing point between Artificial Intelligence research and
operation engineers, which usually requires the cooperation of
academic researchers and industry to obtain real data and scenarios
and carry out research on this basis. However, it is challenging for
every researcher to find industrial partners since the monitoring
data used for AIOps are highly confidential. Therefore, the lack of
public large-scale real-world datasets is one significant limitation
of AIOps research and application.

Although there exist several public datasets, these datasets suffer
from several limitations. First, the majority of existing datasets
covers only one AIOps scenario, e.g., Yahoo Benchmark [21] for
KPI anomaly detection and [51] for faulty microservice localization.
However, AIOps involves diverse scenarios, for example, KPI anom-
aly detection, log anomaly detection, capacity planning, root cause
localization, etc. It is unsatisfactory to evaluate comprehensive
AIOps approaches separately on such scenario-specific datasets.
Second, the scale of the dataset is limited. For example, Yahoo Bench-
mark for KPI anomaly detection only includes hundreds of data
points. In general, real-world service systems tend to support nu-
merous users and perform complex architecture. Thus, the dataset
should be large and diverse to be close to the real world. Third,
some datasets are synthetic, not from the real world. Such datasets
cannot reflect the real system behavior, which could incur risk
for real applications. As a result, rich, large-scale, and real-world
datasets to assist the research of AIOps are highly desired.

To promote the development of AIOps, we have published three
large-scale and real-world datasets, which are based on production
systems of our industrial partner (e.g., Sougo, eBay, Tencent, Suning,
and China Mobile Zhejiang). These datasets are for KPI anomaly
detection, root cause localization for multi-dimension data, and
failure discovery and diagnosis, which is a comprehensive scenario,
respectively. There have been many research works [8, 13, 19, 23, 24,
38, 45, 50] using our datasets. Furthermore, based on these datasets,
we have held an AIOps algorithm competition in 2018/2019/2020,
attracting 125/141/141 teams and hundreds of participants1. The
solutions of top teams are also shared publicly [4]. We believe
such datasets would benefit the AIOps research and practice, as
ImageNet [16] did to the fields of computer vision and deep learning.
We would continue constructing and publishing new datasets and
holding AIOps algorithm competitions once a year.

The main contributions of our work are summarized as follows.
‚Ä¢ Up to now, we have published three large-scale real-world datasets
[1‚Äì3], including KPI anomaly detection, root cause localization
for multi-dimensional data, and failure discovery and diagnosis.
These datasets have been used by many research works.

‚Ä¢ We hold the yearly AIOps algorithm challenge, which involves
thousands of researchers and engineers and significantly pro-
motes this community‚Äôs development.

2 OUR WORK
Following existing work [36], we categorize the research of AIOps
into two macro-areas, failure management and resource manage-
ment. Failure management aims to reduce the negative impact
incurred by failures so as to ensure service availability and user

1https://competition.aiops-challenge.com/home/competition

 
 
 
 
 
 
ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Z. Li, N. Zhao, S. Zhang, Y. Sun, P. Chen, X. Wen, M. Ma, D. Pei

Figure 1: Pipeline and key measurements in failure manage-
ment

experience. Resource management aims to allocate and save vari-
ous resourcees (e.g., power, VMs) in IT service management. We
currently mainly focus on failure management, and resource man-
agement could be our future work.

As presented in Fig. 1, the goals of failure discovery, failure
diagnosis, and failure mitigation are to reduce the mean time to
detect (MTTD), to reduce the mean time to recover (MTTR), and to
increase the mean time between failures (MTBF), respectively. In
our work, to tackle the bottleneck of AIOps research, motivated by
ImageNet [16], we aim to construct large-scale real-world bench-
mark datasets for AIOps. We first focus on failure management
and have published three related datasets, including KPI anomaly
detection (Section 3), root-cause localization for multi-dimensional
data (Section 4), and failure discovery and diagnosis (Section 5). We
will introduce each dataset and related background in detail in the
following three sections.

3 KPI ANOMALY DETECTION
In this section, we first introduce the background of KPI anomaly
detection. Then we present the details of our published dataset to
enhance the research of KPI anomaly detection.

3.1 Background
To closely monitor service running status and identify system fail-
ures, engineers tend to continuously collect many key performance
indicators (KPIs) and identify anomalies from KPIs. Accurate KPI
anomaly detection can trigger prompt troubleshooting, help to
avoid economic loss, and maintain user experience [29, 38, 43, 48].
In recent years, tremendous efforts have been dedicated to KPI
anomaly detection. Existing works could be classified into three cat-
egories, i.e., supervised methods (e.g., Opprentice [29] and EDAGS [21]),
unsupervised methods (e.g., Donut [43]), and semi-supervised meth-
ods (e.g., ADS [7]). However, there exist several significant chal-
lenges, restricting the research of KPI anomaly detection [38].

‚Ä¢ Various KPI patterns. Based on our experience and observation,
KPI patterns could be classified into three categories: seasonal
(e.g., transaction volume), stable (e.g., success rate), and unstable
(e.g., CPU utilization). Therefore, a successful approach should
be generalized enough for different patterns.

‚Ä¢ Diverse KPI anomaly patterns. The definition of KPI anomalies
is the behavior violating normal patterns, like spikes, dips, and
jitters. Besides, different scenarios and components have different
levels of tolerance to anomalies. Thus, the designed approaches
should have the ability to detect diverse anomaly patterns.
‚Ä¢ Lack of sufficient labeled data. On the one hand, the number
of KPIs is huge in large-scale systems. On the other hand, la-
beling anomalies requires experienced engineers with domain

Table 1: Overview of KPI anomaly detection dataset ùê¥

KPI ID
02e99bd4f6cfb33f
046ec29ddf80d62e
07927a9a18fa19ae
09513ae3e75778a3
18fbb1d5a5dc099d
1c35dbf57f55f5e4
40e25005ff8992bd
54e8a140f6237526
71595dd7171f4540
769894baefea4e9e
76f4550c43334374
7c189dd36f048a6c
88cf3a776ba00e7c
8a20c229e9860d0c
8bef9af9a922e0b3
8c892e5525f3e491
9bd90500bfd11edb
9ee5879409dccef9
a40b1df87e3f1c87
a5bf5d65261d859a
affb01ca2b4f0b45
b3b2e6d1a791d63a
c58bfcbacb2822d1
cff6d3c01e6a6bfa
da403e4e3f87c9e0
e0770391decc44ce

#Points
241189
17568
21920
239975
240299
240969
209155
16497
295337
17568
17568
295379
130872
17568
258907
294019
238798
130899
275850
237426
295361
16495
241453
295258
241148
294048

Anomaly ratio
4.37%
0.45%
0.58%
0.09%
3.27%
3.97%
0.31%
0.02%
0.37%
0.05%
0.49%
0.14%
2.37%
0.02%
0.20%
1.04%
0.05%
2.24%
0.13%
0.01%
0.19%
0.07%
0.05%
0.36%
3.17%
1.04%

Time span (#days)
183
60
86
183
183
183
146
61
207
60
60
207
90
60
182
207
183
90
194
183
207
61
183
207
183
207

knowledge [50]. Thus, it is challenging and time-consuming to
obtain sufficient labeled data to construct a supervised model
and evaluate performance.

Although there exist rich KPI anomaly detection works, these
approaches are mainly evaluated on their private datasets. Besides,
there are no large-scale real-world KPI anomaly detection datasets
currently for model training and evaluation. Thus, despite the
promising performance of their papers, the performance of real
deployment in the industry is far from satisfying. Although there
exist several public datasets about KPI anomaly detection like Ya-
hoo Benchmark [21] and Numenta Anomaly Benchmark [22], they
only contain KPIs with limited data points and synthetic anomalies.
In comparison, the number and type of KPIs are large and diverse.
Consequently, a large-scale and diverse KPI anomaly dataset is
highly desired in the community of KPI anomaly detection.

3.2 Dataset
To tackle the above challenges, we constructed a large-scale real-
world KPI anomaly detection dataset (named ùê¥) with sufficient
labels, covering various KPI patterns and anomaly patterns. This
dataset is collected from five large Internet companies (Sougo, eBay,
Baidu, Tencent, and Ali). Specifically, dataset ùê¥ includes 27 KPIs
in total. For each KPI, experienced engineers in these companies
manually labeled anomalies carefully. Table 1 shows some statistical
information about this dataset, including the total number of data
points, anomaly ratio and time span. We could observe that the time
span of each KPI is from two months to seven months, which is
large-scale compared with existing datasets like Yahoo [21]. Figure 2
presents five KPIs within a time span of one week. Obviously, the
KPI patterns in this dataset are indeed various.

Based on this dataset, we held the first AIOps algorithm challenge
in 20182, attracting 125 teams and over 300 participants from both
academia and industry. Besides, motivated by the novel evaluation

2https://competition.aiops-challenge.com/home/competition/1484452272200032281

FailureAMitigationDiscoveryFailureBMeantimetorestore(MTTR)Meantimebetweenfailures(MTBF)Meantimetodetect(MTTD)Constructing Large-Scale Real-World Benchmark Datasets for AIOps

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

It indicates a failure occurring in the system when the overall
value of a measure becomes abnormal (e.g., the total dollar amount
decreases compared with the expected normal value). However, in
practice, a failure usually only causes the measure values under
specific attribute combinations [18] abnormal [24]. An attribute
combination refers to a slice of the whole multi-dimensional data
by selecting records with specified values of some attributes, and it
has been formally defined in existing literature [18]. For example,
in Table 3, only the measure values under {Province=Beijing} are
significantly abnormal. Such attribute combinations show the scope
of the failure, and thus, are substantial clues for failure diagnosis. In
Table 3, for example, by localizing {Province=Beijing}, engineers
get to know that the failure is related to the network or servers
for users in Beijing, which directs further diagnosis. Therefore, we
call such attribute combinations as root causes of multi-dimensional
data.

Table 2: Example structured logs

Order ID

Timestamp

Dollar Amount

Province

ISP

T001
T002

2021.11.11 10:00:01
2021.11.11 10:00:05

$16
$21

Beijing
Beijing

China Mobile
China Unicom

Table 3: Example multi-dimensional data

Province

ISP

Dollar Amount

Expected Dollar Amount

Beijing
Beijing
Shanghai
Guangdong
Zhejiang
Guangdong
Shanxi
Jiangsu
Tianjin

China Mobile
China Unicom
China Unicom
China Mobile
China Unicom
China Unicom
China Unicom
China Unicom
China Mobile

Total

5
10
30
10
2
200
20
200
41
518

10
20
31
9.8
2
210
22
203
43
550.8

4.2 Dataset
Most existing works [5, 6, 18, 19, 25, 28, 37, 40, 41, 47] on root cause
localization for multi-dimensional data use their private datasets
for evaluation. Therefore, to enhance the research of root cause
localization for multi-dimensional data, we construct and publish a
dataset [2], named ùêµ, based on the real-world large online shopping
platform of Suning. Ltd.

In dataset ùêµ, there are five attributes, which are named as i, e, c,
p and l for confidential reasons. The measure of ùêµ is the average
number of order per minute. In total, the dataset ùêµ contains 400
synthetic failures with ground-truth root cause labels, spanning
eight weeks. We use synthetic failures rather than real-world fail-
ures because it is hard to collect hundreds of production failures
with ground-truth root causes. The synthesis process of failures
contains six steps.

(1) We first select a time point for anomaly injection where the

overall measure values are smooth.

(2) We randomly choose ùëö ‚àà {1, 2, 3, 4} different attributes out of

the five attributes.

(3) Then we randomly choose ùëõ ‚àà {1, 2, 3, 4} different attribute com-
binations corresponding to the ùëö selected attributes. These ùëõ
selected attribute combinations are the ground-truth root cause
where we are going to inject anomalies.

Figure 2: Some KPI examples of dataset ùê¥

strategy of KPI anomaly detection proposed in [43], we import
alarm delay restriction and design a novel evaluation method used
in the competition3. This dataset and evaluation method have been
widely used in many research works [13, 38, 50]. The best F1-score
achieved in this competition is 0.8216. The solutions proposed by
the top teams are shared in public [4].

4 ROOT CAUSE LOCALIZATION FOR

MULTI-DIMENSIONAL DATA

In this section, we first introduce the background of root cause
localization for multi-dimensional data. Then we introduce our
published large-scale dataset for this problem.

4.1 Background
To closely monitor the events occurring in a system, various types
of structured logs are generated, carrying critical information for
failure diagnosis. The structured logs record events with many
fields describing the events. For example, an online shopping plat-
form could generate structured logs recording orders. As shown
in Table 2, the field Timestamp describes when the order occurs,
Dollar Amount describes the dollar amount of the order, Province
describes the location of the corresponding customer, and ISP de-
scribes the network that the customer used. Engineers would like
to collect the overall value of some fields, such as the total dollar
amount, to monitor the status of the whole system. We call such
fields measures [6, 24]. Other fields characterize the events and we
call them attributes [6, 18, 24, 28]. By grouping original structured
logs by some attributes and aggregating values of some fields, multi-
dimensional data [24] are generated. For example, to generate the
multi-dimensional data in Table 3, we group structured logs like
those in Table 2 by Timestamp, Province, and ISP, and aggregate
values of Dollar Amount. Note that in Table 3, we only present
a snapshot of the whole multi-dimensional data, i.e., the multi-
dimensional data at a specific timestamp. Multi-dimensional data is
a more compact representation of the original structured logs, and
thus, we focus on multi-dimensional data for failure diagnosis.

3https://github.com/iopsai/iops/tree/master/evaluation

MonTueWenTurFriSatSunMonTueWenTurFriSatSunMonTueWenTurFriSatSunMonTueWenTurFriSatSunMonTueWenTurFriSatSunESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Z. Li, N. Zhao, S. Zhang, Y. Sun, P. Chen, X. Wen, M. Ma, D. Pei

(4) For each selected attribute combination, we modify the measure
values of it, i.e., the measure values of those rows at the select
time point satisfying the attribute combination. The modified
values are calculated by generalized ripple effect (GRE) [24] with
a random magnitude.

(5) We add extra noises, which are generated by Gaussian distri-
bution, on both modified rows and other rows to simulate the
cases where GRE does not work perfectly and there are forecast
residuals, respectively.

(6) We check the synthetic failure with the following conditions.
When anyone of them are triggered, we go back to step 2 to
regenerate a failure at the time point.
‚Ä¢ There is other attribute combinations sharing almost the same
rows with the selected ground-truth attribute combinations.
‚Ä¢ The injected noises is so large that the overall measure value

of unmodified records (in step 3) is also abnormal.

Overall, among the 400 synthetic faults, 144/131/89/36 faults con-
tains 1/2/3/4 root-cause attribute combinations, and 96/136/108/60
faults involve 1/2/3/4 attributes.

Based on the dataset ùêµ, we held the second AIOps challenge in
20194. There are 141 teams and 547 participants from both academia
(39%) and industries (61%) in this competition. The best performance
achieved by them in this competition is 0.9593 with respect to F1-
score. The solutions proposed by the top teams are published in
[4].

There are several existing research works [19, 24] utilizing the
data provided by AIOps Challenge 2019. Note that the datasets B0 ‚àº
B4 used and published by Squeeze [24] are based on exactly the
same original data as the dataset ùêµ described above and generated
by almost the same synthesis process.

5 FAILURE DISCOVERY AND DIAGNOSIS
In Section 3 and Section 4, we introduce our datasets on KPI anom-
aly detection and root cause localization for multi-dimensional data.
These datasets are useful to benchmark failure discovery and fail-
ure diagnosis algorithms respectively. However, on the one hand,
engineers require automatic failure discovery and diagnosis in an
end-to-end manner. On the other hand, root cause localization for
multi-dimensional data provides only clues to the exact root causes,
and it still requires further diagnosis to obtain fine-grained root
causes given such clues. Thus, we construct a dataset for bench-
marking both failure discovery and fined-grained failure diagnosis
approaches. In this section, we first introduce the background of
the problem. Then we introduce the details of our dataset.

5.1 Background
A distributed system is composed of many cooperating compo-
nents, such as load balancers, web servers, application servers, and
databases. For example, in Fig. 3, we present the architecture of a
production distributed system (named S) at a major Internet ser-
vice provider, China Mobile Zhejiang. In Fig. 3, each vertex is a
component. These components are of five classes, including OSB
(Oracle Service Bus), service, DB, Docker, and OS. To monitor the
status of the components, engineers collect various KPIs of these

Figure 3: The components and their relationships, includ-
ing call dependencies (solid) and deployment dependencies
(dashed), in the distributed system S.

components, such as queries per minute, success rate, and average
response time.

By KPI anomaly detection on these KPIs, we can discover failures
in the distributed systems and figure out which components are
abnormal. However, due to the complex dependencies, a failure
occurring at a root-cause component can propagate to other com-
ponents and cause their KPIs abnormal as well [23]. Therefore, to
diagnose the failure, it is necessary to distinguish the root-cause
components from other abnormal components. Furthermore, to
localize the fine-grained root causes of a failure, it is required to
identify root-cause metrics of the root-cause components further.
For each component, there are plenty of metrics (e.g., CPU utiliza-
tion and network throughput) collected. When we identified the
root cause metrics, for example, CPU usage, it is clear that the fail-
ure is caused by CPU exhaustion on the component. In summary,
we aim to discover failures by KPI anomaly detection and then
identify the root-cause components and the root-cause metrics.

As introduced in Section 3, KPI anomaly detection has been
extensively studied. The existing approaches of identifying root-
cause components [17, 20, 23, 26, 32, 45, 51] can be classified into
two categories, i.e., trace-based [17, 23, 26, 45, 51] and dependency
graph-based [20, 32]. There are also some works [30, 42] further
identifying root-cause metrics. MicroDiag [42] models the depen-
dencies among metrics by causality discovery and ranks metrics
by PageRank. FluxRank [30] ranks metrics by learning-to-rank.

5.2 Dataset
Most existing works on failure diagnosis use private datasets for
evaluation. Some existing works [23, 51] published their datasets,
but these datasets contain only traces and thus do not support
fine-grained failure diagnosis. Furthermore, though there are also
some public datasets (e.g., Azure5 and Alibaba6) including fine-
grained metrics, there are no ground-truth failure time points and
root causes. Therefore, to provide a comprehensive benchmark
for failure discovery and fine-grained diagnosis, we construct and
publish a dataset [3], named ùê∂. Dataset ùê∂ has already been used by
existing research works [8, 23, 45].

The dataset ùê∂ is based on ùë†ùë¶ùë†ùê¥, whose architecture is shown in
Fig. 3. There are three types of monitoring data in the dataset ùê∂,
i.e., traces, KPIs, and metrics. User requests are sent to one of the
OSB components and then invoke other components, including
Service and DB. The whole execution process of a user request is
called a trace. We record each trace by its spans on each component,
including the attributes of each span and the causal relationships
among spans. A span is a unit of work done in a component of a

4https://competition.aiops-challenge.com/home/competition/1484446614851493956

5https://github.com/Azure/AzurePublicDataset
6https://github.com/alibaba/clusterdata

OSB1OSB2Docker1Docker2Docker3Docker4Docker5Docker6Docker7Docker8DB1DB2DB3OS1OS2OS3OS5OS6OS4Constructing Large-Scale Real-World Benchmark Datasets for AIOps

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Table 4: A example trace in the dataset ùê∂

callType

startTime

elapsedTime

success

traceId

id

pid

cmdbId

serviceName

OSB
CSF
RemoteProcess
CSF
RemoteProcess
LOCAL
JDBC

1590249600016
1590249600025
1590249600026
1590249600035
1590249600037
1590249600054
1590249600054

274.0
257.0
254.0
33.0
30.0
7.0
3.0

True
True
True
True
True
True
True

6a171e24568385015da7
6a171e24568385015da7
6a171e24568385015da7
6a171e24568385015da7
6a171e24568385015da7
6a171e24568385015da7
6a171e24568385015da7

6a171919979385015da7
6a1714eba31485915da7
6a17182dad5063a15da7
6a171a703a9063325da7
6a1719334a3114525da7
6a171648b90214635da7
6a171721a43214635da7

None
6a171919979385015da7
6a1714eba31485915da7
6a17182dad5063a15da7
6a171a703a9063325da7
6a1719334a3114525da7
6a171648b90214635da7

os_021
os_021
docker_003
docker_003
docker_005
docker_005,
docker_005

osb_001
csf_001
csf_001
csf_002
csf_002
local_method_011
db_003

Category Metrics list

Table 5: The metrics collected in the dataset ùê∂

Docker

container_{thread_total,fgct,thread_idle,thread_used_pct,session_used,thread_running,fgc,cpu_used,mem_used, fail_percent}

Linux

Oracle

Agent_ping,Buffers_used,CPU_{idle_pct,user_time,system_time,iowait_time,util_pct},Cache_used,Disk_{rd_ios,io_util,await,avgqu_sz,svctm,wr_
kbs,rd_kbs,wr_ios},FS_{used_space,used_pct,total_space,max_util,max_avail},ICMP_ping,Incoming_network_traffic,Memory_{used_
pct,used,available,free,available_pct,total},Num_{of_processes,of_running_processes},Outgoing_network_traffic,Page_{po,pi},Processor_{load_
5_min,load_1_min,load_15_min},Received_{packets,errors_packets,queue},Recv_total,Send_total,Sent_{errors_packets,queue,packets},Shared_
memory,Swap_used_pct,System_{wait_queue_length,block_queue_length},Zombie_Process,ss_total

ACS,AIOS,AWS,Asm_Free_Tb,CPU_{free_pct,Used_Pct},Call_Per_Sec,DFParaWrite_Per_Sec,DbFile_Used_Pct,DbTime,Exec_Per_Sec,Hang,LFParaWrite_
Per_Sec,LFSync_Per_Sec,Logic_Read_Per_Sec,Login_Per_Sec,MEM_{real_util,Used,Total,Used_Pct},New_{Tbs_Free_Gb,Tbs_Used_Pct},On_Off_
State,PGA_{Used_Pct,used_total},Physical_Read_Per_Sec,Proc_{User_Used_Pct,Used_Pct},Redo_Per_Sec,Row_Lock,SEQ_Used_Pct,SctRead_Per_
Sec,SeqRead_Per_Sec,Sess_{Used_Undo,Used_Temp,Connect,Active},Session_pct,TPS_Per_Sec,Tbs_{Used_Pct,Free_Gb},TempTbs_Pct,Total_Tbs_
Size,UndoTbs_Pct,Used_Tbs_Size,User_Commit,tnsping_result_time

Redis

Redis_key_count,blocked_clients,connected_clients,evicted_keys,expired_keys,instantaneous_{output_kbps,input_kbps,ops_per_sec},keyspace_
{hits,misses},maxmemory,mem_fragmentation_ratio,redis_{ping,load},rejected_connections,total_{connections_received,commands_processed},used_
{memory_peak,cpu_sys,memory_rss,cpu_user,memory}

distributed system7. The attributes of the spans on each component
can be aggregated into KPIs to reflect the overall status of each
component. In Table 4, we preset an example trace, where each row
is a span. Each span has a unique ID (the field id), the ID of its parent
span (the span that invokes this span) (pid), and the unique ID of
the whole trace (traceId). With these three fields we can identify
the spans of a trace and the causal relationships among the spans.
The other fields characterize a span. For example, the span of the
first row occurs at 1590249600016 (startTime, Unix timestamp in
milliseconds), costs 274 milliseconds, and has a successful response.
Its call type is OSB and occurs at the component os_021. The KPIs
(e.g., success rate, average response time) of the component are
available by grouping the spans by cmdbId and aggregating some
fields (e.g., success, elapsedTime). Note that the overall KPIs (i.e.,
the KPIs on the OSB components) are directly given in the published
dataset. Besides the traces, we also collect the fine-grained metrics
of each component. There are four categories of metrics, which are
summarized in Table 5.

In the dataset ùê∂, we provide 169 injected failures with ground-
truth time points and root causes, spanning one month. We use
injected failures rather than real-world failures because it is hard to
collect so many real-world failures with ground-truth root causes.
On ùë†ùë¶ùë†ùê¥, we injected 7 types of failures in total, which are summa-
rized in Table 6.

Table 6: Failure injection in the dataset ùê∂
Description

Injection type

Component

Close the listening port of the target instances
Decrease the session limit of the target instance
Stress the CPU of the target container

close
session limit
CPU stress *
network delay $ Delay packets randomly on the target container
network loss $ Drop packets randomly on the target container

Database
Database
Container
Container
Container
Physical node network delay $ Delay packets randomly on the target node
Physical node network loss $ Drop packets randomly on the target node
Injection tool: stress-ng (*) and tc ($)

Based on the dataset ùê∂, we held the third AIOps Challenge
in 20208. It attracted 141 teams and 517 participants from both
academia and industries. In this competition, the participants are
asked to detect and diagnose these failures in an online manner. For
each failure, each team is allowed to submit at most two potential
root-cause metrics. For network failures on containers, including
both loss and delay, as we did not collect network metrics on con-
tainers, the participants are asked to localize the root-cause con-
tainers only. If the result submitted by a team of a failure achieves
a precision score greater than or equal to 0.5, then the result is con-
sidered valid. For each failure, the valid results from different teams
are ranked by time to diagnose/F-0.5 score, then each of them gets
max(10 ‚àí ùëñ + 1, 0) points (ùëñ is the rank). The best score achieved by
the participants is 755, given 129 failures in total.

6 CONCLUSION
AIOps has attracted a great deal of attention from academics and
industry. A significant limitation of the research of AIOps is the
lack of public real-world and large-scale datasets. To tackle this
problem, we have published three datasets, including KPI anomaly
detection, root-cause localization for multi-dimensional data, failure
discovery, and diagnosis. More importantly, we held an algorithm
competition once a year based on the public datasets, attracting
hundreds of teams to take part. Our work is helpful for practitioners
and researchers to apply AIOps to enhance service reliability. In
the future, we will continuously publish more datasets involving
various AIOps scenarios and hold the competition. Welcome to pay
attention to and actively participate in the competition.

7We follow OpenTracing specification: https://opentracing.io/docs/overview/spans/

8https://competition.aiops-challenge.com/home/competition/1484441527290765368

ESEC/FSE 2022, 14 - 18 November, 2022, Singapore

Z. Li, N. Zhao, S. Zhang, Y. Sun, P. Chen, X. Wen, M. Ma, D. Pei

REFERENCES
[1] 2018. NetManAIOps/KPI-Anomaly-Detection: 2018AIOps: The 1st Match for

AIOps. https://github.com/netmanaiops/kpi-anomaly-detection

[2] 2019. NetManAIOps/MultiDimension-Localization: 2019AIOps: The 2nd Match
for AIOps. https://github.com/NetManAIOps/MultiDimension-Localization

[3] 2020. AIOps-Challenge-2020-Data. NetManAIOps.

https://github.com/

NetManAIOps/AIOps-Challenge-2020-Data

[4] 2022. AIOps Workshop. https://workshop.aiops.org.
[5] Faraz Ahmed, Jeffrey Erman, Zihui Ge, Alex X. Liu, Jia Wang, and He Yan. 2017.
Detecting and Localizing End-to-End Performance Degradation for Cellular Data
Services Based on TCP Loss Ratio and Round Trip Time. IEEE/ACM Transactions
on Networking 25, 6 (Dec. 2017), 3709‚Äì3722.

[6] Ranjita Bhagwan, Rahul Kumar, Ramachandran Ramjee, George Varghese, Sur-
jyakanta Mohapatra, Hemanth Manoharan, and Piyush Shah. 2014. Adtributor:
Revenue Debugging in Advertising Systems. In NSDI 2014.

[7] Jiahao Bu, Ying Liu, Shenglin Zhang, Weibin Meng, Qitong Liu, Xiaotian Zhu,
and Dan Pei. [n.d.]. Rapid deployment of anomaly detection models for large
number of emerging kpi streams. In IPCCC 2018.

[8] Yang Cai, Biao Han, Jie Li, Na Zhao, and Jinshu Su. 2021. ModelCoder: A Fault
Model Based Automatic Root Cause Localization Framework for Microservice
Systems. In IWQoS 2021.

[9] Junjie Chen, Xiaoting He, Qingwei Lin, Yong Xu, Hongyu Zhang, Dan Hao, Feng
Gao, Zhangwei Xu, Yingnong Dang, and Dongmei Zhang. 2019. An empirical
investigation of incident triage for online service systems. In ICSE-SEIP 2019.

[10] Junjie Chen, Xiaoting He, Qingwei Lin, Hongyu Zhang, Dan Hao, Feng Gao,
Zhangwei Xu, Yingnong Dang, and Dongmei Zhang. 2019. Continuous incident
triage for large-scale online service systems. In ASE 2019.

[11] Yujun Chen, Xian Yang, Qingwei Lin, Hongyu Zhang, Feng Gao, Zhangwei Xu,
Yingnong Dang, Dongmei Zhang, Hang Dong, Yong Xu, et al. 2019. Outage
Prediction and Diagnosis for Cloud Service Systems. In WWW 2019.

[12] Zhuangbin Chen, Yu Kang, Liqun Li, Xu Zhang, Hongyu Zhang, Hui Xu, Yangfan
Zhou, Li Yang, Jeffrey Sun, Zhangwei Xu, et al. 2020. Towards intelligent incident
management: why we need it and how we make it. In FSE 2020.

[13] Zhuangbin Chen, Jinyang Liu, Yuxin Su, Hongyu Zhang, Xiao Ling, Yongqiang
Yang, and Michael R. Lyu. 2022. Adaptive Performance Anomaly Detection for
Online Service Systems via Pattern Sketching. In ICSE 2022. arXiv:2201.02944

[14] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich, Marcus Fontoura,
and Ricardo Bianchini. 2017. Resource central: Understanding and predicting
workloads for improved resource management in large cloud platforms. In SOSP
2017.

[15] Christina Delimitrou and Christos Kozyrakis. 2014. Quasar: Resource-efficient
and qos-aware cluster management. ACM SIGPLAN Notices 49, 4 (2014), 127‚Äì144.
[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. [n.d.]. Ima-

genet: A large-scale hierarchical image database. In CVPR 2009.

[17] Yu Gan, Mingyu Liang, Sundar Dev, David Lo, and Christina Delimitrou. 2021.
Sage: Practical and Scalable ML-Driven Performance Debugging in Microservices.
In ASPLOS 2021.

[18] Jiazhen Gu, Chuan Luo, Si Qin, Bo Qiao, Qingwei Lin, Hongyu Zhang, Ze Li,
Yingnong Dang, Shaowei Cai, Wei Wu, Yangfan Zhou, Murali Chintalapati, and
Dongmei Zhang. 2020. Efficient Incident Identification from Multi-Dimensional
Issue Reports via Meta-Heuristic Search. In FSE 2020.

[19] Pengkun Jing, Yanni Han, Jiyan Sun, Tao Lin, and Yanjie Hu. 2021. AutoRoot: A
Novel Fault Localization Schema of Multi-Dimensional Root Causes. In WCNC
2021.

[20] Myunghwan Kim, Roshan Roshan, and Sam Shah. 2013. Root Cause Detection
in a Service-Oriented Architecture. ACM SIGMETRICS Performance Evaluation
Review (June 2013). https://doi.org/10.1145/2494232.2465753

[21] Nikolay Laptev, Saeed Amizadeh, and Ian Flint. 2015. Generic and scalable

framework for automated time-series anomaly detection. In KDD 2015.

[22] Alexander Lavin and Subutai Ahmad. 2015. Evaluating real-time anomaly detec-
tion algorithms‚Äìthe Numenta anomaly benchmark. In 2015 IEEE 14th International
Conference on Machine Learning and Applications (ICMLA).

[23] Zeyan Li, Junjie Chen, Rui Jiao, Nengwen Zhao, Zhijun Wang, Shuwei Zhang,
Yanjun Wu, Long Jiang, Leiqin Yan, Zikai Wang, Zhekang Chen, Wenchi Zhang,
Xiaohui Nie, Kaixin Sui, and Dan Pei. 2021. Practical Root Cause Localization for
Microservice Systems via Trace Analysis. In IWQoS 2021.

[24] Zeyan Li, Chengyang Luo, Yiwei Zhao, Yongqian Sun, Kaixin Sui, Xiping Wang,
Dapeng Liu, Xing Jin, Qi Wang, and Dan Pei. 2019. Generic and Robust Localiza-
tion of Multi-Dimensional Root Causes. In ISSRE 2019.

[25] Fred Lin, Keyur Muzumdar, Nikolay Pavlovich Laptev, Mihai-Valentin Curelea,
Seunghak Lee, and Sriram Sankar. 2020. Fast Dimensional Analysis for Root
Cause Investigation in a Large-Scale Service Environment. Proceedings of the
ACM on Measurement and Analysis of Computing Systems (June 2020).

[26] Jinjin Lin, Pengfei Chen, and Zibin Zheng. 2018. Microscope: Pinpoint Perfor-
mance Issues with Causal Graphs in Micro-Service Environments. In ICSOC 2018,
Claus Pahl, Maja Vukovic, Jianwei Yin, and Qi Yu (Eds.).

[27] Qingwei Lin, Ken Hsieh, Yingnong Dang, Hongyu Zhang, Kaixin Sui, Yong
Xu, Jian-Guang Lou, Chenggang Li, Youjiang Wu, Randolph Yao, et al. 2018.
Predicting Node failure in cloud service systems. In FSE 2018.

[28] Qingwei Lin, Jian-Guang Lou, Hongyu Zhang, and Dongmei Zhang. 2016. iDice:

Problem Identification for Emerging Issues. In ICSE 2016.

[29] Dapeng Liu, Youjian Zhao, Haowen Xu, Yongqian Sun, Dan Pei, Jiao Luo, Xiaowei
Jing, and Mei Feng. 2015. Opprentice: Towards practical and automatic anomaly
detection through machine learning. In IMC 2015.

[30] Ping Liu, Yu Chen, Xiaohui Nie, Jing Zhu, Shenglin Zhang, Kaixin Sui, Ming
Zhang, and Dan Pei. 2019. FluxRank: A Widely-Deployable Framework to Auto-
matically Localizing Root Cause Machines for Software Service Failure Mitigation.
In ISSRE 2019.

[31] Jian-Guang Lou, Qingwei Lin, Rui Ding, Qiang Fu, Dongmei Zhang, and Tao Xie.
2017. Experience report on applying software analytics in incident management
of online service. Automated Software Engineering 24, 4 (2017), 905‚Äì941.
[32] Meng Ma, Jingmin Xu, Yuan Wang, Pengfei Chen, Zonghua Zhang, and Ping
Wang. 2020. AutoMAP: Diagnose Your Microservice-Based Web Applications
Automatically. In WWW 2020.

[33] Minghua Ma, Zheng Yin, Shenglin Zhang, Sheng Wang, Christopher Zheng, Xin-
hao Jiang, Hanwen Hu, Cheng Luo, Yilin Li, Nengjun Qiu, et al. 2020. Diagnosing
root causes of intermittent slow queries in cloud databases. Proceedings of the
VLDB Endowment 13, 8 (2020).

[34] Minghua Ma, Shenglin Zhang, Junjie Chen, Jim Xu, Haozhe Li, Yongliang Lin,
Xiaohui Nie, Bo Zhou, Yong Wang, and Dan Pei. 2021. {Jump-Starting} Mul-
tivariate Time Series Anomaly Detection for Online Service Systems. In ATC
2021.

[35] Susan Moore. 2019. Gartner: How to Get Started With AIOps. https://www.

gartner.com/smarterwithgartner/how-to-get-started-with-aiops

[36] Paolo Notaro, Jorge Cardoso, and Michael Gerndt. 2021. A Survey of AIOps
Methods for Failure Management. ACM Transactions on Intelligent Systems and
Technology (TIST) 12, 6 (2021), 1‚Äì45.

[37] M. A. Qureshi, L. Qiu, A. Mahimkar, J. He, and G. Baig. 2020. Multi-Dimensional
Impact Detection and Diagnosis in Cellular Networks. In 2020 16th International
Conference on Mobility, Sensing and Networking (MSN).

[38] Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang, Xiaoyu
Kou, Tony Xing, Mao Yang, Jie Tong, and Qi Zhang. 2019. Time-series anomaly
detection service at microsoft. In KDD 2019.

[39] Mohammed Shatnawi and Mohamed Hefeeda. 2015. Real-time failure prediction

in online services. In INFOCOM 2015.

[40] Yongqian Sun, Youjian Zhao, Ya Su, Dapeng Liu, Xiaohui Nie, Yuan Meng, Shiwen
Cheng, Dan Pei, Shenglin Zhang, Xianping Qu, and Xuanyou Guo. 2018. HotSpot:
Anomaly Localization for Additive KPIs With Multi-Dimensional Attributes. IEEE
Access 6 (2018), 10909‚Äì10923.

[41] H. Wang, G. Rong, Y. Xu, and Y. You. 2020. ImpAPTr: A Tool For Identifying The

Clues To Online Service Anomalies. In ASE 2020.

[42] Li Wu, Johan Tordsson, Jasmin Bogatinovski, Erik Elmroth, and Odej Kao. 2021.
MicroDiag: Fine-Grained Performance Diagnosis for Microservice Systems. In
ICSE21 Workshop on Cloud Intelligence. 7.

[43] Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying
Liu, Youjian Zhao, Dan Pei, Yang Feng, Jie Chen, Zhaogang Wang, and Honglin
Qiao. 2018. Unsupervised Anomaly Detection via Variational Auto-Encoder for
Seasonal KPIs in Web Applications. In WWW 2018.

[44] Yong Xu, Kaixin Sui, Randolph Yao, Hongyu Zhang, Qingwei Lin, Yingnong Dang,
Peng Li, Keceng Jiang, Wenchi Zhang, Jian-Guang Lou, et al. 2018. Improving
service availability of cloud systems by predicting disk error. In ATC 2018.
[45] Guangba Yu, Pengfei Chen, Hongyang Chen, Zijie Guan, Zicheng Huang, Linxiao
Jing, Tianjun Weng, Xinmeng Sun, and Xiaoyun Li. 2021. MicroRank: End-to-End
Latency Issue Localization with Extended Spectrum Analysis in Microservice
Environments. In WWW 2021.

[46] Ke Zhang, Jianwu Xu, Martin Renqiang Min, Guofei Jiang, Konstantinos Pelechri-
nis, and Hui Zhang. 2016. Automated IT system failure prediction: A deep
learning approach. In Big Data 2022.

[47] Xu Zhang, Chao Du, Yifan Li, Yong Xu, Hongyu Zhang, Si Qin, Ze Li, Qingwei
Lin, Yingnong Dang, Andrew Zhou, Saravanakumar Rajmohan, and Dongmei
Zhang. 2021. HALO: Hierarchy-Aware Fault Localization for Cloud Systems. In
KDD 2021.

[48] Xu Zhang, Junghyun Kim, Qingwei Lin, Keunhak Lim, Shobhit O Kanaujia, Yong
Xu, Kyle Jamieson, Aws Albarghouthi, Si Qin, Michael J Freedman, et al. 2019.
Cross-dataset time series anomaly detection for cloud systems. In ATC 2019.
[49] Nengwen Zhao, Junjie Chen, Zhou Wang, Xiao Peng, Gang Wang, Yong Wu, Fang
Zhou, Zhen Feng, Xiaohui Nie, Wenchi Zhang, et al. 2020. Real-time incident
prediction for online service systems. In FSE 2020.

[50] Nengwen Zhao, Jing Zhu, Rong Liu, Dapeng Liu, Ming Zhang, and Dan Pei. 2019.
Label-Less: A Semi-Automatic Labelling Tool for KPI Anomalies. In INFOCOM
2019.

[51] Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Dewei Liu, Qilin Xiang, and
Chuan He. 2019. Latent Error Prediction and Fault Localization for Microservice
Applications by Learning from System Trace Logs. In FSE 2019.

