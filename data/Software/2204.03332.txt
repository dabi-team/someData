2
2
0
2

r
p
A
7

]
E
S
.
s
c
[

1
v
2
3
3
3
0
.
4
0
2
2
:
v
i
X
r
a

PREDICTING PERFORMANCE OF
HETEROGENEOUS AI SYSTEMS WITH
DISCRETE-EVENT SIMULATIONS

Vyacheslav Zhdanovskiy
Institute for Information
Transmission Problems, RAS

Lev Teplyakov
Institute for Information
Transmission Problems, RAS

Bolshoy Karetny per. 19, Moscow, 127051, Russia; Bolshoy Karetny per. 19, Moscow, 127051, Russia

Moscow Institute of Physics and Technology
(National Research University)
Institutskiy per. 9, Dolgoprudny, 141701, Russia
E-mail: zhdanovskiy.vd@phystech.edu

E-mail: teplyakov@visillect.com

Anton Grigoryev
Institute for Information Transmission Problems, RAS
Bolshoy Karetny per. 19, Moscow, 127051, Russia
E-mail: me@ansgri.com

KEYWORDS

Heterogeneous computing; Discrete-event simula-
tions; Artiﬁcial intelligence; Video analytics; Software
architecture

ABSTRACT

In recent years, artiﬁcial intelligence (AI) technolo-
gies have found industrial applications in various ﬁelds.
AI systems typically possess complex software and het-
erogeneous CPU/GPU hardware architecture, making
it diﬃcult to answer basic questions considering perfor-
mance evaluation and software optimization. Where is
the bottleneck impeding the system? How does the per-
formance scale with the workload? How the speed-up
of a speciﬁc module would contribute to the whole sys-
tem? Finding the answers to these questions through
experiments on the real system could require a lot of
computational, human, ﬁnancial, and time resources.
A solution to cut these costs is to use a fast and ac-
curate simulation model preparatory to implementing
anything in the real system.
In this paper, we pro-
pose a discrete-event simulation model of a high-load
heterogeneous AI system in the context of video ana-
lytics. Using the proposed model, we estimate: 1) the
performance scalability with the increasing number of
cameras; 2) the performance impact of integrating a
new module; 3) the performance gain from optimizing
a single module. We show that the performance esti-
mation accuracy of the proposed model is higher than
90%. We also demonstrate, that the considered sys-
tem possesses a counter-intuitive relationship between
workload and performance, which nevertheless is cor-
rectly inferred by the proposed simulation model.

INTRODUCTION

In recent years, there has been a signiﬁcant growth
of interest in machine learning and artiﬁcial intelli-

gence (AI) technologies. Analysts expect the AI mar-
ket to grow to more than USD 660 billion by 2028
(GrandViewResearch.com 2021). Nowadays, AI tech-
nologies are used in various ﬁelds, such as urban ser-
vices (Wang and Sng 2015), retail (Weber and Sch¨utte
2019), medicine (Chen et al. 2021), etc.

One of the key technologies that allowed for the
In particular, deep neural
progress is deep learning.
networks made it possible to achieve almost human-
like object recognition quality in problems like image
classiﬁcation (Rawat and Wang 2017), object detection
(Arnold et al. 2019; Liu et al. 2020) and image segmen-
tation (Guo et al. 2018). In order to achieve this ac-
curacy and still provide reasonable recognition speed,
deep neural networks have to exploit special hardware
providing fast matrix multiplication, most commonly —
GPUs. This poses a problem for software developers,
because they have to design the architecture of their
AI-based applications with heterogeneous CPU/GPU
computations in mind.

In detail, software developers have to utilize the par-
allelism provided by modern multi-core CPUs along
with the GPU acceleration. Meanwhile the GPU is
used for the neural network inference, the CPU is used
to prepare the neural networks’s input and postprocess
its output, run various computer vision algorithms like
tracking, localization, keypoint detection. The CPU
also handles all the additional modules delivering AI
results to the end user — API calls, business logic,
database management, etc. In order to deliver on all
these tasks on advanced heterogeneous hardware with
the given time, memory and energy constraints, soft-
ware developers have to design rather complex archi-
tectures (Sutter et al. 2005).

The complex software architecture, in turn, compli-
cates performance evaluation and software optimiza-
tion of such systems (Voss et al. 2019c). In particular,

Communications of the ECMS, Volume 36, Issue 1, Proceedings,
©ECMS Ibrahim A. Hameed, Agus Hasan, Saleh Abdel-Afou Alaliyat (Editors)
ISBN: 978-3-937436-77-7/978-3-937436-76-0(CD) ISSN 2522-2414

 
 
 
 
 
 
it is hard to locate the bottleneck module and to pre-
dict, how its optimization will aﬀect the performance
of the entire system. It may lead to lots of human and
ﬁnancial resources as well as time resources being spent
on optimization without any signiﬁcant increase in per-
formance of the entire system. For a new module to be
designed and implemented, it might be hard to map
the system’s constraints to the constraints of a speciﬁc
module. It may turn out - after the resources are spent
on implementing a new module and its integration into
the system! - that the module results in unaﬀordable
slowdown of the system.

An elegant way to avoid the aforementioned prob-
lems is to design a simulation model of the system and
infer the feasibility of optimizations on the model prior
to implementing them in the code. Discrete-event simu-
lations of software have been used before in other areas,
for example, planning, evaluation and optimization of
Hadoop clusters (Bian et al. 2014; Wang et al. 2014;
Liu et al. 2016; Chen et al. 2016). However, to the best
of our knowledge, no one yet has tried to apply this ap-
proach to applications with heterogeneous CPU/GPU
computing, such as AI systems. Moreover, such a sim-
ulation model can be used for other tasks like capacity
planning. This can be achieved by evaluating the model
on a collected set of suitable hardware conﬁgurations
(Korobov et al. 2020).

In terms of performance modelling, diﬀerent AI ap-
plications have their own unique specialties. The key
diﬀerence is which hardware components to consider.
For example, video analytics (Wang and Sng 2015)
and self-driving cars (Badue et al. 2021) are mostly
CPU/GPU intensive, meanwhile AI on pure-cloud so-
lutions also rely heavily on the network performance,
which requires to consider the I/O subsystem in the
model.

In this work, we consider a video analytics system as
a typical example of AI application utilizing CPU/GPU
computing. The possibility of applying our research to
other AI applications is discussed in Section III. While
the GPU is used to infer the neural network respon-
sible for complex object detection tasks such as hu-
man detection, the CPU is used for preprocessing the
video frames, postprocessing the neural network output
and running classic computer vision algorithms such as
ORB keypoint detector and descriptor (Rublee et al.
2011). The considered system has the following func-
tionality:
• processing video feed from multiple video cameras;
• person detection using YOLOv4 (Bochkovskiy et al.
2020);
• person 3D localization;
• single- and multi-camera person tracking.

We design a discrete-event simulation model which
is low-cost both in terms of its development and simu-
lation speed and can easily be adopted by the software
developers. We use it to estimate:
1. the performance scalability with the increasing num-
ber of video cameras;
2. the performance gain from optimizing a single sys-

tem module;
3. the performance impact caused by integrating of a
new module in the system.

The rest of the paper is structured as follows: Sec-
tion I describes the software architecture of the video
analytics system, Section II provides a description of
the proposed simulation model, Section III contains nu-
merical results. Section IV concludes the paper.

SYSTEM DESCRIPTION

Flow graph paradigm

A common design pattern to eﬃciently implement
parallelism in heterogeneous and parallel systems is to
use the ﬂow graph paradigm (Grigoryev et al. 2015;
Badue et al. 2021; Huang et al. 2021). With it, the al-
gorithm is represented as a data ﬂow graph (Voss et al.
2019b), see Fig. 1. Each graph node (vertex) receives a

f1

f2

f3

f4

f5

Fig. 1: Example of a ﬂow graph. Nodes f2 and f3 can
process messages broadcasted from f1 in parallel. f 4
can process messages from f2 and f3 independently or
wait until messages from both nodes are present — it
depends on the desired behaviour.

message from its predecessors, processes it and broad-
casts the output to the successors. Input and output
nodes are the exception:

• input nodes either produce an input message by itself
or receive it somewhere from outside the graph;
• output nodes do not broadcast the result, but instead
store it or deliver it outside the graph.

The considered video analytics system has multiple
places where parallelism can be utilized. In particular:

• frames from diﬀerent video streams can be processed
in parallel;
• a single frame can be processed in parallel by multiple
data-independent detectors, for example, by the neural
network and the keypoint detector.

Flow graphs allow to eﬃciently utilize these types of
parallelism by executing diﬀerent nodes of the graph in
parallel at the same time.

There are multiple frameworks implementing this
paradigm. Notable examples include oneTBB1 and
Taskﬂow2 (Huang et al. 2021).
In this work, we use
oneTBB.

Video
stream #1

Node A

Node E

GPU Node

Node B

Node C

Node D

Output

Video
stream #2

Node A

Node B

Node C

Node D

Output

Node E

(a)

CPU core #1

Node A

Node E

Node B

Node B

Node C

Node D

CPU core #2

Node A

Node E

Node C

Node D

GPU

GPU Node

GPU Node

...

...

...

time

(b)
Fig. 2: Software architecture of the studied video analytics system represented as a ﬂow graph (a). Sample timing
diagram of the system execution (b). Note than Node E is processed in parallel with the GPU Node, meanwhile
Node B has to wait until GPU Node ﬁnishes processing its task. Node E is the CPU-intensive module we consider in
Section III. It is turned oﬀ for all experiments except “Integration of a new module in the system”.

Software architecture of the video analytics sys-
tem

In this section, we provide a brief description of the
software architecture used in the considered video an-
alytics system.

Overall, the whole system is represented by a ﬂow
graph, see Fig. 2a. Each video stream from a single
camera is represented by a separate graph component.
We shall notice that we deliberately show a less detailed
version of the real graph for the sake of simpliﬁcation.
All graph components are executed in a single thread
pool.

Meanwhile many ﬂow graph nodes can be executed in
parallel, some of them can not because their functions
are not thread safe.
In particular, this is true with
the neural network nodes. Because neural networks
are usually implemented in a way (NVIDIA 2022) that
each neural network instance can be executed on GPU
by only one context at a particular moment, multiple
video streams have to put their tasks for the neural net-
work inference in a shared queue and then wait for their
completion, see Fig. 2b. We implement this function-

1Formerly TBB; more details at: https://www.intel.

com/content/www/us/en/developer/tools/oneapi/
onetbb.html

2More details at: https://taskflow.github.io/

ality with oneTBB’s async nodes (Voss et al. 2019a).

SIMULATION MODEL

In this section, we describe the details of the pro-
posed simulation model. Overall, the system ﬂow graph
has a near-direct representation in the model. We im-
plemented the model in Python using the SimPy3 li-
brary.

Algorithm 1 Flow graph node simulation for a basic
node

Q — input queue (from predecessors)
S — output queue (to successors)
C — pool of free CPU cores
P — distribution of the node’s running time
while not all frames are processed do

m ← Q.pop() (blocks if Q is empty)
c ← C.pop() (blocks if C is empty)
t ∼ P
wait(t)
S.push(m)
C.push(c)

end while

3More details at: https://simpy.readthedocs.io/en/

latest/index.html

Algorithm 2 Flow graph node simulation for a GPU
async node

Q — input queue (from predecessors)
S — output queue (to successors)
C — pool of free CPU cores
G — GPU
P — distribution of the node’s running time
while not all frames are processed do

m ← Q.pop() (blocks if Q is empty)
c ← C.pop() (blocks if C is empty)
G.lock() (blocks if G is busy)
C.push(c)
t ∼ P
wait(t)
G.release()
c ← C.pop() (blocks if C is empty)
S.push(m)
C.push(c)

end while

The source code of the implementation is available
at GitHub4. It contains only ≈ 400 LoC of Python (in-
cluding the proﬁling trace parsers), meanwhile the real
system contains ≈ 15000 LoC of C++ (not including
the dependencies).

We consider a CPU with N logical cores in the model.
Each ﬂow graph node waits until the input message is
present, see Algorithm 1. Then it waits until there is
a free CPU core, then the node occupies the CPU core
for the execution time. The execution time is sampled
from independent empirical distributions measured by
proﬁling the real system. Input messages that have not
yet been processed are stored in a queue. Eﬀectively,
this represents the work of oneTBB’s thread pool.

The GPU neural network node is modelled in a
slightly diﬀerent way, see Algorithm 2. Like a basic ﬂow
graph node, it waits for an input message and a CPU
core. Then it waits until the GPU is free, then locks it
for the execution time, while yielding the CPU core for
some another ﬂow graph node. When the GPU is done
with processing the message, the node waits again for
a free CPU core in order to broadcast its output to the
successors. Eﬀectively, this represents the mechanism
shown in Fig. 2b.

The ﬂow graph nodes’ execution times are sampled
from empirical distributions measured by proﬁling the
real system. Fig. 3 contains an example of such distri-
bution for the neural network inferencer. The oneTBB
ﬂow graph nodes are proﬁled using Intel Flow Graph
Tracer and Flow Graph Analyzer (Voss et al. 2019c).
The non-oneTBB activities like the video decoding and
the neural network inference are sampled by our in-
house tracing library.

We deliberately use ﬂow graphs in the model instead
of some well-known formalisms like Petri Nets (Peter-
son 1977) and Queuing Networks (Chandy et al. 1975).
It makes designing the simulation model easier because

4More details at: https://github.com/iitpvisionlab/

heterogeneous-ai-system-simulator

Fig. 3: Empirical distribution of neural network infer-
ence time.

we can explicitly use the same graph structure and syn-
chronization primitives used as the real system. This
approach also has potential for the model to be au-
tomatically generated by parsing the proﬁling traces.
Moreover, our approach is easier to be adopted by the
software developers who are unlikely to be experts in
modelling and simulation.

We shall notice that we do not consider the over-
head caused by communications between the nodes, as
the execution time of each node signiﬁcantly exceeds
the average communication time in our case. However,
our simulation model can easily be upgraded by intro-
ducing additional timings between consecutive graph
nodes. This will allow to model other AI applications
like AI on pure-cloud solutions, where there is signif-
icant communication overhead cause by the network.
The communication overhead can also be important in
self-driving cars, where the TCP/IP stack is often used
for communication even within a single machine, e.g.
the ROS framework(Quigley et al. 2009).

EVALUATION

In this section we evaluate the proposed simulation
model. In order to do it, we compared the performance
metrics predicted by the simulator with those measured
on the real video analytics system.
In our case, the
performance metrics is the average frames per second
(FPS) per each video stream.

The hardware speciﬁcations are listed in Table 1.

TABLE 1: Testbed hardware speciﬁcations

Parameter Value
CPU

Intel Core i7-7800X, 3.5 GHz,
6 Cores, 12 Threads
NVIDIA GeForce GTX 1080 Ti,
11 GB VRAM
64 GB

GPU

RAM

15161718Neural network inference time, ms0.00.20.40.60.81.01.21.4Probability densityFig. 4: FPS per video stream depending on the number
of video streams.

Fig. 5: Performance impact caused by integrating a
new CPU-intensive module

Estimating the model accuracy and the system
scalability

Estimating the feasibility of software optimiza-
tions

In this experiment, we varied the number of video
streams in order to estimate the accuracy of the simu-
lation model. The results presented in Fig. 4 demon-
strate, that the performance prediction error rate of the
simulation model is less than 10%, which is accurate
enough to rely on the model’s prediction in planning
the scalability of the system. It is also lower than the
error rate threshold of 20% used in a related work (Bian
et al. 2014).

On small number of video streams (up to three) the
system experiences almost no performance drop due
to eﬃcient parallelism. However, when the number of
video streams increases, the performance drops signif-
icantly, because the neural network becomes the bot-
tleneck that hinders the parallelism: threads stand idle
waiting for a task to execute. When the number of
video streams exceeds the number of CPU logical cores,
the performance drops even more because there is not
enough free threads to execute appearing tasks.

Integration of a new module in the system

To study an impact of a new module on the system,
we conducted the following experiment. We added a
CPU-intensive module (Node E in Fig 2a) in the sys-
tem and measured the overall slowdown on both the
real model and the simulator. The results, see Fig 5,
show that the module produces approximately 2.5X
slowdown on 1 video stream, meanwhile producing no
noticeable slowdown on 12 video streams. The result is
counter-intuitive: the increase in workload makes the
overhead of the module unnoticeable instead of it grow-
ing linearly with the workload. This occurs because
the neural network, while being comparably fast on
one stream, becomes the bottleneck on high number
of video streams.

This experiments demonstrates, that for systems
with high degree of parallelism the impact of adding
a new module or changing the existing one on system’s
performance could be nontrivial.

In this section we study the feasibility of optimiza-
tions in the considered system with the help of the pro-
posed simulation model by the example of the following
optimization.

The quality of service (QoS) of the considered video
analytics system highly depends on tracking algo-
rithms. Tracking accuracy, in turn, depends on many
parameters of the algorithm. In order to improve QoS,
it is needed to ﬁnd the optimal values of the parame-
ters.

The simplest way to ﬁnd the optimal values is the
grid search. However, for the high-dimensional search
space of parameters it is computationally unfeasible.
More sophisticated optimization methods like Bayesian
optimization allow to mitigate but not fully alleviate
this problem, still requiring many runs of the system.
Fortunately, all the required runs use the same input
data with diﬀerent values of parameters. Therefore, in
theory, it is possible to save a lot of time by caching
neural network predictions and reusing them on each
run instead of inferring the real network.

However, the cache may not provide the desired per-
formance gain, because some other module can become
the bottleneck. Therefore, prior to implementing and
testing the optimization (which takes about one week
for a software engineer), we study the feasibility of the
optimization on the simulation model (which takes a
couple of hours).

First, we used the developed simulation model to es-
timate the performance gain of implementing the neu-
ral network cache without considering the overhead of
the cache itself. Eﬀectively, we set the execution time
of the GPU node to zero. The speed-up appeared to
be 13.8x (Table 2, “ideal” cache).

Then we implemented a LevelDB-based5 cache mod-
ule detached from the system, benchmarked its perfor-
mance and used the data in simulation model to ac-

5More details at: https://github.com/google/leveldb

123456Number of video streams05101520Average FPS per video streamSimulatorReal system1612Number of video streams05101520FPS per streamSimulator (before)Real system (before)Simulator (after)Real system (after)TABLE 2: Overall system speedup on 6 video streams
from implementing the cache

Experiment
Real system
Simulator, “ideal” cache
Simulator, “real” cache

Overall system speedup
11.3x
13.8x
12.0x

count for the overhead. The speed-up appeared to be
12.0x (Table 2, “real” cache).

Finally, we integrated the developed cache module
into the system. The real achieved speed-up was equal
to 11.3x (Table 2, real system), close to the predicted
value of 12.0x.

The experiment demonstrates, that with a negligible
overhead in time spent on simulating each step of the
implementation, it allows to correctly estimate the limit
of optimization’s impact. Moreover, it could save a lot
of time, if it had emerged that a speciﬁc optimization
step is not worth implementing.

CONCLUSIONS

In this work, we proposed a discrete-simulation
model to predict the performance of a heterogeneous
CPU/GPU video analytics system. The proposed
model can easily be adopted by the software develop-
ers who are not experts in simulation and modelling.
We showed that the accuracy of performance estima-
tion using the proposed system is higher than 90% in
each experiment.

We used the simulation model to predict perfor-
mance scale with workload; to estimate the impact of a
new module on the whole system, which demonstrated
counter-intuitive results yet correctly predicted by the
simulator; to predict the feasibility of an optimization.
We believe such a simulation model should become
a workplace tool for software designers and it could
save lots of resources by easily inferring the feasibility
of optimizations and modiﬁcations prior to doing some
costly work on a real system.

Possible future research includes taking into consid-
eration the hardware conﬁguration to predict, for ex-
ample, an optimal hardware price — quality of service
trade-oﬀ of a system.

REFERENCES

Arnold, Eduardo; Omar Y Al-Jarrah; Mehrdad Dianati;
Saber Fallah; David Oxtoby; and Alex Mouzakitis.
2019. “A survey on 3D object detection methods for au-
tonomous driving applications.” IEEE Transactions on
Intelligent Transportation Systems, 20(10):3782–3795.
Badue, Claudine; Rˆanik Guidolini; Raphael Vivacqua
Carneiro; Pedro Azevedo; Vinicius B Cardoso; Avelino
Forechi; Luan Jesus; Rodrigo Berriel; Thiago M
Paixao; Filipe Mutz; et al. 2021. “Self-driving cars: A
survey.” Expert Systems with Applications, 165:113816.
Bian, Zhaojuan; Kebing Wang; Zhihong Wang; Gene
Munce; Illia Cremer; Wei Zhou; Qian Chen; and Gen
Xu. 2014.
“Simulating Big Data clusters for system
planning, evaluation, and optimization.” In 2014 43rd
International Conference on Parallel Processing, 391–
400.

Bochkovskiy, Alexey; Chien-Yao Wang;

Yuan Mark Liao. 2020.
and accuracy of object detection.”
arXiv:2004.10934.

and Hong-
“YOLOv4: Optimal speed
arXiv preprint

Chandy, K. Mani; Ulrich Herzog; and Lin Woo. 1975. “Para-
metric analysis of queuing networks.” IBM Journal of
Research and Development, 19(1):36–42.

Chen, Jianguo; Kenli Li; Zhaolei Zhang; Keqin Li; and
Philip S Yu. 2021. “A survey on applications of artiﬁ-
cial intelligence in ﬁghting against COVID-19.” ACM
Computing Surveys (CSUR), 54(8):1–32.

Chen, Qian; Kebing Wang; Zhaojuan Bian; Illia Cremer;
Gen Xu; and Yejun Guo. 2016. “Cluster performance
simulation for Spark deployment planning, evaluation
and optimization.”
In International Conference on
Simulation and Modeling Methodologies, Technologies
and Applications, 34–51.

GrandViewResearch.com. 2021. “Artiﬁcial intelligence mar-
ket size, share and trends analysis report by solution,
by technology (deep learning, machine learning, natu-
ral language processing, machine vision), by end use,
by region, and segment forecasts, 2021 - 2028.” https:
//www.grandviewresearch.com/industry-analysis/
artificial-intelligence-ai-market.
Accessed:
18.01.2022.

Grigoryev, Anton; Timur Khanipov; Ivan Koptelov; Dmitry
Bocharov; Vassily Postnikov; and Dmitry Nikolaev.
2015.
“Building a robust vehicle detection and clas-
siﬁcation module.” In ICMV 2015.

Guo, Yanming; Yu Liu; Theodoros Georgiou; and Michael S
Lew. 2018. “A review of semantic segmentation using
deep neural networks.” International journal of multi-
media information retrieval, 7(2):87–93.

Huang, Tsung-Wei; Dian-Lun Lin; Chun-Xun Lin; and Yibo
Lin. 2021. “Taskﬂow: A lightweight parallel and hetero-
geneous task graph computing system.” IEEE Transac-
tions on Parallel and Distributed Systems, 33(6):1303–
1320.

Korobov, Nikita; Oleg Shipitko; Ivan Konovalenko; An-
ton Grigoryev; and Marina Chukalina. 2020. “SWaP-
C based comparison of onboard computers for un-
manned vehicles.” In ER(ZR)-2019, volume 154, 573–
583 (Springer, Singapore, 2020).

Liu, Jun; Bianny Bian; and Samantika Subramaniam Sury.
2016. “Planning your SQL-on-Hadoop deployment us-
ing a low-cost simulation-based approach.”
In 2016
28th International Symposium on Computer Architec-
ture and High Performance Computing (SBAC-PAD),
182–189.

Liu, Li; Wanli Ouyang; Xiaogang Wang; Paul Fieguth;
Jie Chen; Xinwang Liu; and Matti Pietik¨ainen. 2020.
“Deep learning for generic object detection: A survey.”
International journal of computer vision, 128(2):261–
318.

NVIDIA. 2022.

“NVIDIA TensorRT Documentation.”
https://docs.nvidia.com/deeplearning/tensorrt/
developer-guide/index.html#threading. Accessed:
28.01.2022.

Peterson, James L. 1977.

“Petri nets.” ACM Computing

Surveys (CSUR), 9(3):223–252.

Quigley, Morgan; Ken Conley; Brian Gerkey; Josh Faust;
Tully Foote; Jeremy Leibs; Rob Wheeler; Andrew Y
Ng; et al. 2009. “Ros: an open-source robot operating
system.” In ICRA workshop on open source software,
volume 3, page 5.

Rawat, Waseem and Zenghui Wang. 2017. “Deep convolu-
tional neural networks for image classiﬁcation: A com-
prehensive review.” Neural computation, 29(9):2352–
2449.

Rublee, Ethan; Vincent Rabaud; Kurt Konolige; and Gary
Bradski. 2011. “ORB: An eﬃcient alternative to SIFT

or SURF.”
puter vision, 2564–2571.

In 2011 International conference on com-

Sutter, Herb et al. 2005.

“The free lunch is over: A fun-
damental turn toward concurrency in software.” Dr.
Dobb’s journal, 30(3):202–210.

Voss, Michael; Rafael Asenjo; and James Reinders, Beef
Up Flow Graphs with Async Nodes, 513–534 (Apress,
Berkeley, CA, 2019a).
ISBN 978-1-4842-4398-5. doi:
10.1007/978-1-4842-4398-5_18.

Voss, Michael; Rafael Asenjo; and James Reinders, Flow
Graphs, 79–107 (Apress, Berkeley, CA, 2019b). ISBN
978-1-4842-4398-5. doi:10.1007/978-1-4842-4398-5_3.
Voss, Michael; Rafael Asenjo; and James Reinders, Flow
Graphs: Beyond the Basics, 451–511 (Apress, Berke-
ley, CA, 2019c). ISBN 978-1-4842-4398-5. doi:10.1007/
978-1-4842-4398-5_17.

Wang, Kebing; Zhaojuan Bian; Qian Chen; Ren Wang; and
Gen Xu. 2014.
“Simulating Hive cluster for deploy-
ment planning, evaluation and optimization.” In 2014
IEEE 6th International Conference on Cloud Comput-
ing Technology and Science, 475–482.

Wang, Li and Dennis Sng. 2015. “Deep learning algorithms
with applications to video analytics for a smart city: A
survey.” arXiv preprint arXiv:1512.03131.

Weber, Felix Dominik and Reinhard Sch¨utte. 2019. “State-
of-the-art and adoption of artiﬁcial intelligence in re-
tailing.” Digital Policy, Regulation and Governance.

AUTHOR BIOGRAPHIES

VYACHESLAV ZHDANOVSKIY
was born in Verkhnyaya Pyshma, Rus-
sia. He obtained his B.Sc.
in Applied
Physics and Mathematics in 2020 from
Moscow Institute of Physics and Tech-
nology (MIPT). Currently he is a M.Sc.
student in Computer Science and Engi-
neering at MIPT. Since 2020, he works
at the Vision Systems Lab at the Institute for Informa-
tion Transmission Problems. His research interests in-
clude heterogeneous and parallel computing, computer
vision and distributed systems. His e-mail address is
zhdanovskiy.vd@phystech.edu.

and M.Sc.

LEV TEPLYAKOV was born in
Arkhangelsk, Russia. He obtained
in Ap-
his B.Sc.
plied Physics and Mathematics from
Moscow Institute of Physics and
Technology (MIPT) in 2017 and 2019
correspondingly. Since 2016, he has
been developing industrial computer
vision systems with the Vision Sys-
tems Lab at the Institute for Information Transmission
Problems. His research interests include heterogeneous
and parallel computing, object detection and tracking.
His e-mail address is teplyakov@visillect.com.

ANTON GRIGORYEV was born
in Petropavlovsk-Kamchatskiy, Rus-
sia. Having graduated from Moscow
Institute of Physics and Technology,
he has been developing industrial
computer vision systems with the Vi-
sion Systems Lab at the Institute for
Information Transmission Problems
since 2010. His research interests
are image processing and enhancement methods, au-
tonomous robotics and software architecture. His e-
mail address is me@ansgri.com.

