Failure Identiï¬cation from Unstable Log Data using
Deep Learning

Jasmin Bogatinovskiâˆ—, Sasho Nedelkoskiâˆ—, Li Wuâˆ—, Jorge Cardosoâ€ , Odej Kaoâˆ—
âˆ—Distributed and Operating Systems, Technical University Berlin, Germany
â€ Huawei Munich Research, Munich, Germany
{jasmin.bogatinovski, odej.kao}@tu-berlin.de

2
2
0
2

r
p
A
6

]
E
S
.
s
c
[

1
v
6
3
6
2
0
.
4
0
2
2
:
v
i
X
r
a

Abstractâ€”The reliability of cloud platforms is of signiï¬cant
relevance because society increasingly relies on complex software
systems running on the cloud. To improve it, cloud providers are
automating various maintenance tasks, with failure identiï¬cation
frequently being considered. The precondition for automation is
the availability of observability tools, with system logs commonly
being used. The focus of this paper is log-based failure identi-
ï¬cation. This problem is challenging because of the instability
of the log data and the incompleteness of the explicit logging
failure coverage within the code. To address the two challenges,
we present CLog as a method for failure identiï¬cation. The
key idea presented herein based is on our observation that by
representing the log data as sequences of subprocesses instead
of sequences of log events, the effect of the unstable log data is
reduced. CLog introduces a novel subprocess extraction method
that uses context-aware neural network and clustering methods
to extract meaningful subprocesses. The direct modeling of log
event contexts allows the identiï¬cation of failures with respect
to the abrupt context changes, addressing the challenge of
insufï¬cient logging failure coverage. Our experimental results
demonstrate that the learned subprocesses representations reduce
the instability in the input, allowing CLog to outperform the
baselines on the failure identiï¬cation subproblems â€“ 1) failure
detection by 9-24% on F1 score and 2) failure type identiï¬cation
by 7% on the macro averaged F1 score. Further analysis shows
the existent negative correlation between the instability in the
input event sequences and the detection performance in a model-
agnostic manner.

Index Termsâ€”failure identiï¬cation; system reliability;

log

data; cloud computing; deep learning;

I. INTRODUCTION

Cloud systems are a mixture of complex multi-layered
software and hardware. They enable applications of ever-
increasing heterogeneity and complexity powering different
technologies such as the Internet of Things, distributed pro-
cessing frameworks, databases, virtual reality, among others.
The emergence of complexity within the cloud relates to
diverse maintenance challenges, with an important challenge
of being prone to failures [1]. The failures have a signiï¬cant
impact on the performance affecting user experience and
leading to economic losses [2]. Therefore, accurate and timely
failure identiï¬cation is crucial for enhancing the reliability of
the cloud and its services.

Cloud providers are considering many approaches to address
the problem of failure identiï¬cation, commonly by adopting
various data-driven methods [3]. Their fundament resides in
Â©2022 IEEE. Permission from IEEE must be obtained for all uses, in any
current or future media, including reprinting/republishing this material for
advertising or promotional purposes. This paper is accepted at IEEE
CCGrid 2022. For citations use references from the conference proceedings.

the available monitoring data, with log data (logs for short)
commonly being utilized. Other monitoring data,
like key
performance indicator metrics (KPI, e.g., memory utilization,
I/O bytes), provide clues for detecting failures, however, they
do not provide a verbose description of the type of the
failure [4] making the failure identiï¬cation incomplete. For
example, a sharp increase in the curve of memory utilization
only indicates that the memory utilization increases, but it
cannot tell why it happens in isolation. In comparison, logs are
textual data recording events with different granularity, provid-
ing human-understandable clues for the failure and its type.
For example, from several repetitions of the two consecutive
log lines, â€œl1: Interface changed state to up.â€ and â€l2: Interface
changed state to down.â€, operators can detect a failure in the
system, assign its type â€œInterface Flappingâ€, conclude that the
interface is ï¬‚apping and obtain clues for potential root causes
(bad cable connection a common suspect in this example). A
single log is composed of a static event template describing the
event (e.g., â€Interface changed state to (cid:104)âˆ—(cid:105).â€) and parameters
(e.g., up) giving variable event information.

The focus of this study is the problem of log-based failure
identiï¬cation. Traditionally, it is addressed by manual analysis,
like keyword search of failure words (e.g., â€failâ€) or log levels
with great severity (e.g., â€errorâ€) [5]. Owning to the unprece-
dented development of the cloud systems, logs are consistently
generated in large volumes (several TB per day [3]), making
the task of manual log-based diagnosis cumbersome. Thereby,
automatic approaches for log-based failure identiï¬cation are
increasingly researched and adopted [6]â€“[10].

in form of

Current approaches 1) identify failures from single log
lines [10] or 2) exploit groups/contexts of log events (i.e.,
log se-
co-occurring event
templates/events)
quences [11] (i.e., series of event
templates with external
identiï¬er) and count vectors [7]. Depending on the assumed
input, different challenges emerge. When considering groups
of log events, the challenge of unstable log sequences oc-
curs [6]. Unstable log sequences are sequences from the same
type of workload execution having slightly different sequential
structures. Fig. 1 depicts examples of unstable sequences
caused by different reasons. In the sequences of events denoted
with â€Event Duplicationâ€ and â€Missing Eventâ€, the original
sequence (E2, E5, E1, E4, E6) is modiï¬ed by repeating a
single event â€E4â€ or dropping the event â€E1â€, accordingly.
The two sequences still represent normal system behaviour but

 
 
 
 
 
 
have slightly different structures. Such problems are common
in cloud systems where the log data is analyzed at a central
place. The network errors, limited throughput, or storage issues
are referenced causes for events repeating or dropping. There
are other sources of instabilities (e.g., the preprocessing of
raw logs), altering the normal
log sequences in a similar
way. Notably, the instability causes similar properties of the
unstable normal and failure sequences (e.g., shortened lengths
or contexts differ in a single event), making it harder to
distinguish them from one another. For example,
the two
labeled â€Misidentifying Eventâ€ and
sequences in Fig. 1,
â€Failureâ€, differ just within one event on the fourth position
(â€E7â€ and â€E3â€). The ï¬rst arises due to an error in the log
event preprocessing, while the second is because the template
describes a failure event. Therefore, the instability inï¬‚icts a
modeling challenge and increases the entropy in the data. From
modeling perspective, this requires accounting for the unequal
importance of the log events within the contexts, impairing
the detection performance otherwise [6].

Fig. 1. Examples of unstable and failure log event sequences.

The methods using single log lines do not suffer from the
problem of unstable logs, predominantly due to incorporating
information about the semantics of the log events [8]. These
approaches demonstrate strong performance [10] however,
they cannot detect failures that are not explicitly logged.
For example, in the aforenamed failure with type â€Interface
Flappingâ€ (with logs l1, l2), none of the two logs has a log
level with greater severity (i.e., â€errorâ€ or â€criticalâ€), nor do
they explicitly describe a failure. The failure can be detected
just within the context of several repetitions of the speciï¬c
pair of logs. Furthermore, these types of contextual failures
occur often. For example, for the release Pike (version 3.12.1)
of a popular cloud resource managing system OpenStack,
there are more than 20% of failures not explicitly logged
within a single log line [12]. Acknowledging that developers
have an insufï¬cient understating of the complexities of the
running system environment during development results in
insufï¬cient failure logging coverage [3]. Conclusively,
the
failures that do not manifest in individual lines make the failure
identiï¬cation possible only within the context of other logs
(e.g., presence/absence of frequently co-occurring logs).

Contributions. 1) To overcome the two challenges, in this
paper as the main contribution, we introduce CLog â€“ a method
for log-based failure identiï¬cation. The key idea of CLog is
reducing the instability of the input log event sequences by
representing them as sequences of subprocesses (i.e., groups of
similar contexts). Since subprocesses represent contexts (co-

occurring log event templates), their number is signiï¬cantly
smaller than the event number used to represent the sequences.
The two key beneï¬ts of the change in the representation are
that â€“ a) by representing the log event sequences by a smaller
number of subprocesses, we directly reduce the entropy in
the input representation, reducing the effect of the unstable
log sequences; b) the modeling of contexts allows detect-
ing failures in terms of abrupt context changes, addressing
the challenge of insufï¬cient logging failure coverage. The
challenge that arises is the extraction of subprocesses. 2) To
address it, we contribute a novel method for unsupervised
subprocesses extraction based on context-aware deep learning
and clustering methods. 3) Our experimental results on two
datasets from OpenStack (with 172 failures) demonstrate that
CLog outperforms the baselines on the two failure identiï¬ca-
tion subproblems: failure detection (by 9-24% on F1 score)
and failure type identiï¬cation (by 7% on macro average F1
score). By injecting unstable event sequences, we show CLogâ€™s
robust performance dropping by just 6% under a severe ratio
of unstable sequences. 4) Finally, we contribute by open-
sourcing the datasets and method for reproducibility purposes
and fostering the research on this practically relevant problem.
The remaining of the paper is structured as follows. Sec-
tion II gives the key observation for the approach, alongside
the problem deï¬nition. Section III describes the proposed
methodology. Section IV discusses the experimental results
in response to four research questions. Section V discusses
the related work for the two sub-problems of failure detection
and failure type identiï¬cation. Section VI concludes the paper
and gives directions for future work.

II. PRELIMINARY

A. Problem Deï¬nition

In this paper, we address the problem of log-based fail-
ure identiï¬cation [5]. We decompose the problem into two
subproblems, i.e., (1) failure detection and (2) failure type
identiï¬cation, deï¬ned in the following.

Failure Detection (FD). Let L = {l1, l2 . . . li . . . ln} be a
set of n time-ordered logs from cloud services, and there exist
an index set J âˆˆ N capturing dependency relation between the
logs, i.e., sj = (lji âˆˆ L|j âˆˆ J), where lji denotes individual
log of the sequence sj. Further, we assume that there exist
a function p+ denoting the normality score of the sequence
p+(Ï†(sj)) : Rd (cid:55)â†’ R, where Ï† : S (cid:55)â†’ Rd is the representation
function of sequence sj into d-dimensional numerical vector
space, and S is the available sequence set. The task of failure
detection is deï¬ned as ï¬nding the set A = {sj âˆˆ S|a1 <
p+(sj)||p+(sj) > a2, j âˆˆ J}, where a1, a2 are constants such
that a1 < a2. Although the individual logs li in the sequence
sj can describe normal events, the overall sequence can denote
a failure. The index set J in the context of logs can represent
task ID, process ID, or workload ID. It can be given apriori (as
considered here) or reconstructed by an additional procedure.
We assume that the majority of the log messages li and the
sequences sj describe normal system behaviour.

E2E5E1E4E6Event DuplicationMisidentified EventMissing EventFailure Sequenceâ€œInstance Failureâ€Normal SequenceE2E5E1E4E6E4E2E5E1E7E6E2E5E4E6E2E5E1E3E6Failure Type Identiï¬cation (FTI). Given a set of detected
failure sequences A and the set of failure types identiï¬ers
T = {t1, t2 . . . tw}, where w denotes the number of unique
failure type identiï¬ers, the task of failure type identiï¬cation is
ï¬nding a function f (Ï†(si)) : A (cid:55)â†’ T .

Failure Identiï¬cation (FI). Given the sets L, T, S, and
J,
failure identiï¬cation is ï¬nding the set
the task of
ËœA = {(s1, t1), . . . (si, ti) . . . (s|A|, t|A|)}, where the failure
sequence si is detected by estimating the normality score func-
tion Ëœp+( ËœÏ†(si)) and the thresholds Ëœa1 and Ëœa2, while its type ti is
identiï¬ed by the estimate of Ëœf ( ËœÏ†(si)). The estimates of ËœÏ†(si),
Ëœp+( ËœÏ†(si)) and Ëœf ( ËœÏ†(si)) further are used for representing, and
failure identiï¬cation on novel sequences. CLog addresses the
problem by ï¬nding suitable representation for the sequences
ËœÏ†(si) (Section III-B), which are used to ï¬nd estimates for Ëœp+
(Section III-C1) and Ëœf (Section III-C2).

Fig. 2. Entropy (measuring instability) increases with more unique symbols.

B. Key Observation

The key observation CLog relies on is reducing the entropy
of the input representation of log sequences by representing
them as sequences of subprocesses instead of log events. For
example, a sequence of log events si = (E1, E2, E5, E3, E1)
with a task ID i, where each of {E1, E2, E3, E5} denotes
log event template, can equivalently be represented as se-
quence of two subprocesses/symbols si = (Sa, Sb) such that
Sa = (E1, E2, E5), and Sb = (E3, E1), while Sa and Sb
are referred to as subprocesses. Fig. 2 depicts the impact of
changing the log event sequence representation into sequences
of subprocesses on data from OpenStack. It is seen that with
increasing the grouping window size, the number of log events
in the subprocesses increases, and the average entropy over
all the sequences in the data also increases (the triangles).
The entropy is the highest when the whole sequence is
represented with individual log events (the diamond). Notably,
when the sequences are represented with subprocesses, the
number of symbols used to represent the sequences is smaller
(the circles), and the average entropy is reduced. It implies
a reduction of the sensitiveness over the individual
logs,
effectively reducing the effect of the instability in the log event
sequences. An important goal of CLog is to learn subprocesses
by preserving their characteristics, i.e., by learning context-
aware event sequence groups.

III. CLOG: METHOD FOR LOG-BASED FAILURE
IDENTIFICATION

To address the problem of failure identiï¬cation, we propose
CLog. Fig. 3 gives an overview of the method. It has three
parts 1) log parsing, 2) context-aware subprocesses extraction,
and 3) failure identiï¬cation. The log parsing, as a general
preprocessing procedure in log analysis [13], extracts the event
templates from the incoming raw log event sequences, trans-
forming the raw log sequences into sequences of log events.
The event sequences are processed by the context-aware
subprocesses extraction part, converting them into sequences
of subprocesses. This part leverages our observation that by
representing the log sequences on a level of subprocesses,
the entropy of the representation sequence is smaller. This
increases the stability of the input and reduces the impact
of the unstable logs. Finally, the processed log sequences of
subprocesses are given as input into the failure identiï¬cation
part. The latter is composed of two modules (a) failure detector
and (b) failure type identiï¬cation (FTI). The goal of the failure
detector is to detect the failure sequences of subprocesses.
The FTI module further identiï¬es the failure types based
on operators experience. CLog has two modes of operation:
ofï¬‚ine and online. During the ofï¬‚ine phase, the parameters
of the log parser, context-aware subprocesses part, and failure
identiï¬cation parts are learned, and the models are induced
and stored. In the online phase, the stored models are loaded
and used to identify failures. In the following, we describe the
internal mechanisms of the three parts of CLog in detail.

Fig. 3. CLog overview.

A. Log Parsing

The generated raw system logs are unstructured. Since we
are interested in modeling the sequences of events, as the
ï¬rst step, we extract the event templates from the raw logs
by applying automatic log parsing. Log parsing decouples the
log templates from log parameters (the variable part in a log),
directly extracting input in useable modeling format. We pref-
ered automatic log parsing because the alternatives (e.g., reg-
ular expression and â€grockâ€ patterns), although successful for
parsing the templates, are system-speciï¬c, requiring frequent
updates, which makes them challenging for maintenance [13].
While there are many log parsers available, CLog utilizes a
tree-based parser Drain [14]. Zhu et al. [13] identiï¬ed Drain
among the most efï¬cient in comparison to 12 other parsers on
ten benchmark datasets from diverse software systems. The
three main properties making it popular and widely adopted
are its correctness, efï¬ciency and intuitive meaning of the

60s120s180s240s300sorgi.seq.len.window size123456average entropysubprocesses/symbols1050518 raw events518 raw event orig.seq.Context-aware Subprocess ExtractionFailureIdentificationE2E5E3E2E6E1E1E9E5window size (e.g., 60s)time axist1t2t3Task ID 1: S1S2S4Sequence of SubprocessesOUTPUT: Task ID 1 has  Failed. The type of failure is, e.g., Failure Instance.INPUT: t1,2,3â€¦time intervalsRaw System Log MessagesSequence of EventsLog Parsinghyperparameters (making the tuning process undemanding).
Once parsed, the events are organized in sequences by CLogâ€™s
hyperparameter window size, corresponding to the arriving
time interval and task ID of the events, and are proceeded as
output towards the context-aware subprocess extraction part.

B. Context-aware Subprocess Extraction

The context-aware subprocess extraction is the central part
of the method. Its goal is the extraction of subprocesses from
the parsed log event sequences. By representing an execution
workload (with a task ID) on a higher-level granularity, i.e., by
subprocesses, we reduce the entropy in the input, addressing
the problem of unstable log event sequences. This part com-
bines context-aware neural network and clustering methods to
learn explicit relationships between the events within the event
sequences preserving their local properties. Fig. 4 depicts the
overall design of the context-aware subprocess extraction part
with a running example. Conceptually, it is composed of three
submodules â€“ (1) preprocessing submodule that transforms
the input sequences into a format suitable for learning, (2)
neural network learning module which is combined with a
batched kmeans method to learn subprocesses in an unsuper-
vised manner, (3) subprocess extraction module that assigns
a unique subprocess identiï¬er to the input event sequence. In
the following, we describe the submodules.

Fig. 4. The internal design of context-aware subprocess extraction part (with
a detailed explanation of a running example).

1) Preprocessing Submodule: The goal of the preprocess-
ing submodule is to preprocess the input log event sequences
in a uniï¬ed format suitable for the neural network. It has two
components: padding and masking.

Padding. The padding component receives the sequences
of log events as input, with each event represented by a
unique identiï¬er (e.g., integer). We refer to it as a token.
The sequences of events in a given time interval are different
in length. However, the neural network requires a ï¬xed-size
representation of the input. The padding component speciï¬es a
hyperparameter max length and appends each of the shorter
log sequences with a dedicated token [P D] up to max length
to enforce ï¬xed-size representation. The longer sequences
(having more than max length events) are truncated. Notably,
we add a dedicated token [LSE] (Log Sequence Embedding)
at the beginning of each sequence. During learning, we enforce

the sequence token representations to propagate through the
upper layers in the network via the [LSE] token. Thereby,
[LSE] attends over all the tokens from the sequence and
summarizes the relevant context during learning. The [LSE]
token serves as a sequence vector representation later used to
group contexts and identify subprocesses. The output of the
padding module is the prepended and padded event sequence.
Masking. To learn context-aware groups, we consider a
general self-supervised learning task from natural language
processing (NLP) research called Masked Language Modeling
(MLM) [15]. To apply the MLM task, the masking component
is processing the prepended and padded log event sequences
in a suitable format. More speciï¬cally, as input, it receives the
prepended and padded log event sequences and outputs a set
of pairs of masked log event sequences and original masked
events. Masked log event sequences are sequences of log
events created by replacing all of the events from an original
log sequence with a special [M ] (masked) token. For example,
for the input sequence (E2, E5, E3), one masked sequence is
(E2, [M ], E3), with E5 being the original masked event. There
are three masked event sequences for this example. [LSE]
and [P D] tokens are not affected by the masking procedure.
During training, a masked sequence is given as input to the
neural network, while the original masked token is used as
the prediction target. By predicting the mask from the co-
occurring tokens, the method learns the most important events
from the surrounding context, extracting context-aware repre-
sentations. Note that by this procedure single input sequence is
multiplied several times. We keep track of the origin (the input
event sequence) of each masked sequence and use it to extract
its corresponding subprocess identiï¬er (see Section III-B3).

2) Neural Network Submodule: The neural network sub-
module learns context-aware groups of masked log sequences.
It implements a neural network following the design of a
self-attention encoder of the Transformer [15] architecture.
The advantage given by this architectural choice resides in
its capability to learn the contextual information between the
input events. When learning the parameters of the network,
guided by a carefully designed cost function, the model learns
local relationships between the events based on their co-
occurrence extracting useful contextual features. The neural
network submodule has four components: vectorizer, encoder
block, output layers and masked subprocess id assignment.
The vectorizer transforms the masked input event sequences
of tokens into numerical vector sequences. These vectors are
called event embeddings and are part of the training procedure.
At the beginning of the training procedure, the event vectors
are randomly initialized and are updated during training. This
way, they learn contextual information about the events.

The encoder block is composed of a self-attention encoder
layer. The self-attention extracts co-occurring information by
weighting the input vector embeddings by their similarity to all
the other embeddings in the given context. Combining the self-
attention with the MLM learning task modiï¬es the parameters
of the network to learn the context of the original masked
event and extract sequential properties. The hyperparameters

Paddingâ€¦INUPT:Neural NetworkSubmodule:ğ‘ ğ‘–:Maskingğ‘ ğ‘–1ğ‘š:ğ‘ ğ‘–2ğ‘š:ğ‘ ğ‘–3ğ‘š:Sequence of events[LSE]E2E5E3[PD][PD]Masked Subprocess ID assignment; Output Layer  ğœ½â€²VectorizerEncoder Block (neural network with self-attention)ğœ½E2E5ğ‘ ğ‘–1ğ‘š:ğ‘ ğ‘–2ğ‘š:ğ‘ ğ‘–3ğ‘š:ğ‹ğ’”ğ’ğ’,ğ’“ğ’;Î¸E5: (0.42, 0.12 â€¦ 0.44) Preprocessing:OUTPUT:-parametersâˆ…S1:S2:vector of size dSubprocess ExtractionSk:ğ‘ ğ‘–2ğ‘šğ‘ ğ‘–1ğ‘š,ğ‘ ğ‘–3ğ‘šâ€¦The event sequence: is assigned with subprocess ID S1. Explanation: 2/3 masked subprocess sequences             have an ID S1.ğ‘ ğ‘–1ğ‘š,ğ‘ ğ‘–2ğ‘šNNtargets:[LSE] vectorâ€¦â€¦â€¦[LSE][M]E5E3[PD][PD][LSE]E2E3[PD][PD]E2E5E3Subprocesses:E2E5E3[LSE]E2E5[PD][PD]E3[M][M]of the encoder are the model size (denoted by d), the number
of encoder layers, the number of heads, and the dropout ratio
(used to prevent overï¬tting). We reference the reader to Devlin
et al. [15] for the speciï¬c details on self-attention neural net-
works. Particularly interesting is the embedding of the [LSE]
token. Since [LSE] serves as an embedding of the sequence,
it learns contextual properties of the masked input sequences.
The output from the encoder is the vector embedding of the
[LSE] token for each of the masked sequences, proceeded
towards the output layer.

The output layer is composed of two layers with nonlinear
activation (RELU is used). The purpose of it
is to map
the masked sequence embedding vector [LSE] of size d, to
a vector with a size corresponding to the total number of
events/tokens C. The output of this layer is used to calculate
the loss. As an optimization loss function, we use categorical
cross-entropy. Notably, during the execution of a workload,
some events occur only once (e.g., â€notiï¬cation of successful
creation of a VMâ€) while others in greater frequency (e.g.,
HTTP or RPC calls). When using the original loss formulation
on the MLM task, the less frequent events will be averaged
out, resulting in missing important information. To account
for the imbalances of the distribution of the events, we use
weighted categorical cross-entropy given in Eq. 1 as follows:

Jm(Ïˆ(sm

(cid:48)

), ym

n,c; w)

n,c; Î¸, Î¸
C
(cid:88)

c=1

=

1
|C|

âˆ’wcym

n,clog

(cid:48)

n,c; Î¸, Î¸

exp(Ïˆ(sm
i=1 exp(Ïˆ(sm

))
n,i; Î¸, Î¸(cid:48)))

(cid:80)C

(1)

where Ïˆ denotes the function modeled by the neural network,
Î¸ and Î¸(cid:48) are the parameters of the encoder, and the output layer
accordingly, sm
n,c is a masked sequence obtained from the n-th
input sequence sn, ym
n,c is the original masked event/token, C
denotes the total token numbers and wc represents the weight
of an individual token. The weights (w â€“ a weights vector) are
assigned such that the less frequent events have weight values
closer to 1, as opposed to the frequent ones that have values
closer to 0. Therefore, we optimize for preserving the correct
predictions on the infrequent events, addressing the challenge
of the imbalance of the event frequency distribution.

to group the embeddings of

Masked Subprocess ID assignment. The masked sub-
process ID assignment receives the vector embedding of the
[LSE] token as input. It applies the mini batched kmeans
algorithm [16]
the masked
event sequences into a predetermined number of k subpro-
cesses/centroids identiï¬ers. The mini batched kmeans algo-
rithm is a commonly used method for identifying similar
instance groups in an unsupervised way. While the goal of
the encoder block is to learn context-aware representations,
the mini batched kmeans complements it by extracting similar
context groups, enabling the extraction of subprocesses. We
used mini batch kmeans because it allows per batch update
) and clustering parameters (M)
of the network (Î¸ and Î¸
as opposed to the classical kmeans method. To group the
contexts, kmeans optimizes the loss given in Eq. 2 by altering
between two steps: 1) updating a centroid mk as the average of

(cid:48)

the embeddings currently assigned to it, and 2) reassignment
of the embeddings to the nearest newly calculated centroid.

Jk(Ï†(sm

n , rn; Î¸), M) = ||Ï†(sm

n ; Î¸) âˆ’ rnM||2

(2)

where M âˆˆ Rkxd represent the matrix of subprocess context-
group (interchangeably referred to as centroids), while rn is
an indicator vector of discrete values (0â€™s and 1â€™s) with just
one element set to one, corresponding to the membership of
the masked sequence sm
n to a certain centroid mk. The number
of subprocesses identiï¬ers k is a hyperparameter.

Finally, we add the two optimization losses as J = Jm+Î»Jk
to obtain the ï¬nal loss subject to optimization. By combined
optimization of the two losses, the parameters of the context-
aware subprocess extraction learn local contexts and local-
context groups based on their similarity. The role of the
hyperparameter Î» is to ensure learning of correct contexts and
correct context-embedding groups by trading off the impact of
the two losses. We further discuss the optimization procedure.
Optimization. The optimization is done in two phases: 1)
pretraining and 2) joint training. We ï¬rst describe the pretrain-
ing phase. Since at the beginning everything is initialized at
random, we pre-train the neural network parameters (Î¸ and
Î¸
) by the weighted cross-entropy loss (Eq. 1). That way, the
model learns good initial parameters for the encoder while
extracting context-aware features for the masked sequences.
The pretraining is terminated after observing a lack of im-
provement in the loss on ï¬ve consecutive epochs. At the end
of the pretraining, the [LSE] vectors are valid representations
of the masked input sequences. Afterwards, the subprocesses
prototypes (M) are initialized by kmeans using [LSE] masked
sequence embeddings of the training data.

(cid:48)

Joint training (phase 2). The joint optimization function has
a discrete variable (rn), making the parameter updates non-
trivial. To address this issue, we calculate the gradients by
alternating stochastic gradient descent (ASGD) [16]. ASGD
alters the updates of the network parameters and centroids
such that, when the network parameters are updated,
the
centroids are ï¬xed and vice versa. Therefore, the optimization
problem does not depend on the discrete variable, enabling the
parameter updates. The training of the network parameters and
the centroids is done in batches. Eq. 3 is used for centroids
update. At each batch,
the centroids with newly assigned
embeddings are slightly updated based on their distance to
the newly calculated centroids. Additionally, some of the
centroids are updated more frequently than others making
the loss convergence slower. Inspired by Yang et al. [16],
we resolve this issue by penalizing the updates with the
term 1
. Ck counts the number of times a cluster is assigned an
ck
embedding during an epoch. The larger the number of assigned
embeddings, the smaller is the centroid updated and vice versa.
It normalizes the intensity of the centroid update as a learning
rate, different for each cluster.

mk â† mk âˆ’

1
ck

(Ï†(sm

n ; Î¸) âˆ’ mk)rn

(3)

where V is the set of validation normal sequences of sub-
processes. The normality score estimate Ëœp+(si) is a symmetric
positive function, given as the spread of the probability of
the sequence si under the HMM (t(si)) from the mean score
estimates of the validation data. The parameters of the HMM
are learned on the normal training data, thereby, the failure
detector models the normal system state. We assume that the
normal data is always obtainable from the periods of system
operation when there are no issue reports or log events with
â€errorâ€ or â€criticalâ€ log levels. Any sequence with signiï¬cantly
different values for the normality score estimate is detected
as a failure. Using the symmetrical property of the normality
function, we estimate the thresholds as Ëœa1/2 = Âµ Â± 3Ïƒ,
where Âµ and Ïƒ are the mean value and standard deviation of
the validation score estimates calculated by standard formulas.
Thereby, the failure detector is fully unsupervised. The number
of hidden states is one hyperparameter of HMM.

2) Failure Type Identiï¬cation: Once the failure is detected,
the failure sequence proceeds towards the failure type identiï¬-
cation module. This module leverages the redundancy property
of failures in cloud systems [5]. This property emerges for
various reasons,
including temporary failure ï¬xes by de-
velopers without addressing the root cause, environmental
issues (e.g., machine failure or network disconnections), or
running the same system in different environments. Notably,
the redundancy implies repetitive patterns in logs, allowing
usage of operational information to identify the failure type.
The failure type identiï¬cation subpart has two components
(1) feature extraction and (2) failure type identiï¬cation model.
The feature extraction processes the sequences of subpro-
cesses in a format suitable for the FTI learning method. Each
sequence is represented by a count vector that counts the num-
ber of occurrences of a subprocess within the sequence. For ex-
ample, for the sequence of subprocesses s = (S1, S3, S1) and
total of four subprocesses (S1, S2, S3, S4), the count vector is
given as CV (s) = (2, 0, 1, 0). The absence/presence of certain
subprocesses from the sequence (e.g., lack of the subprocess
with the event â€Failure to spawn an instance.â€) are distinctive
features that discriminate among failure types. Therefore, the
count vector is a suitable sequence representation. We also
considered the normality score estimates from the failure
detector as an additional feature (pHMM).

The extracted features are used to ï¬t the FTI model given
by Ëœf (si). FTI learns a multiclass classiï¬cation model that
classiï¬es the input sequences into several predeï¬ned types
of failures. As an adequate methods we considered several
popular multiclass classiï¬cation methods, i.e., Random Forest
(RF) [18], Decision Tree (DT) [19], Logistic Regression
(LR) [20] and AdaBoost [21]. They show good performance
and do not require extensive hyperparameter optimization [18].
The ï¬nal output of CLog is given as ËœA = {(sj, ti)|sj âˆˆ S, ti âˆˆ
T, Ëœp+(sj) < Ëœa1||Ëœp+(sj) > Ëœa2, ti = Ëœf (sj), j âˆˆ J}.

Fig. 5.

Internal architectural design of failure identiï¬cation part.

3) Subbprocess Exctraction: The extraction of a subpro-
cess identiï¬er (ID) is done as follows. Given an original
input event sequence and the subprocess ID assignments of
its masked subsequences, we count the number of occurrences
of the subprocesses IDs and divide the counts by the length
of the original input event sequence. The subprocess ID with
the highest score value is assigned as a subprocess ID for
the input event sequence. Intuitively, if the majority of the
masked subsequences are assigned with a single subprocess
ID, the subprocess ID with the maximal score value is the most
relevant for the input event sequence. Fig. 4 depicts an example
of extracting the subprocess S1 for the sequence (E2, E5, E3).

C. Failure Identiï¬cation

The failure identiï¬cation part is given sequences of sub-
processes with the same task ID as input. Fig. 5 depicts the
internal design. It is composed of two subparts 1) failure
detector and 2) failure type identiï¬er. The failure detector
detects if the input sequence of subprocesses represents failure.
When failure is detected, the sequence proceeds towards the
failure type identiï¬cation part, which identiï¬es the type of
failure based on prior historical information. We describe the
details in the following.

1) Failure Detection: As a modeling choice for the failure
detector, we considered Hidden Markov Model (HMM) [17].
HMM, models the sequences of subprocesses by assuming
that the appearance of the next subprocess within the se-
quence depends only on the current subprocess. The main
advantages of HMM are that it directly handles sequential
data, does not require further preprocessing of the input, and
is fast for both learning and inference (with a reasonably
high number of hidden states). To produce normality score
estimates for a sequence Ëœp+(si), we used HMM probability
scores (t(s)), calculated by marginalizing the probabilities over
all the subprocesses of the sequence and the hidden states
of the ï¬tted HMM t(s) = âˆ’ log (cid:80)
h q(h)q(s|h), where h
denotes the hidden states, and q(s|h) denotes the likelihood
of the subprocess given the hidden state. The normality score
estimates for a single sequence si is given in Eq. 4, as follows:

Ëœp+(si) = (

1
|V|

|V|
(cid:88)

sj

t(sj) âˆ’ t(si))2

(4)

IV. EXPERIMENTAL EVALUATION

In this section, we describe the experimental evaluation. We
give details about the experimental design, present and discuss

Feature Extraction1.Count Vector of Supbrocesses(CV)2.Probability under HMM (pHMM)3.Combination (CV+pHMM)Failure Detectionà·ªğ‘+(ğ‘ ğ‘–)thresholdsà·¦ğ‘1,à·¦ğ‘2Failure Sequencesà·¦ğ‘1à·¦ğ‘2FTI Model(RF, DT, LR, AdaBoost)Failure Type IdentificationFitted FTI Modeláˆšğ‘“ğ‘ ğ‘–Offline PhaseOnline PhaseOUTPUT: à·©ğ‘¨INPUT:Sequences ofSubprocessest1: (S1, S2, S1, S4)t2: (S3, S1, S1, S2)t3: (S1, S3, S4, S4)â€¦Failure Sequence Labels (training)hs1hs2hs3HMMÎ¼ÏƒThreshold EstimationFailure DetectorDataset name

Number of Tasks

No Failure

Assertion Failures

Number of Log Messages

Number of Unique Events

Number of Failure Events

OpenStack
Syntetic

878
500

706
421-476

172
24-79

217534
167215

518
474

167
123

Average number of events
fault-free task
1323
1309

TABLE I
DATASET STATISTICS

the experimental results in response to four research questions.

A. Experimental Design

1) OpenStack dataset: To evaluate CLog, we considered
a large scale study of failures in OpenStack, introduced in
Cotroneo et al. [12]. To the best of our knowledge, it is the
most comprehensive publicly available dataset of log failure
data from a cloud system. Its strength is the wide range of
covered failures following the most common problem reports
in the OpenStack bug repository. The faults are generated by
software fault-injection procedure, i.e., modifying the source
code of OpenStack and running a predeï¬ned workload under
fault-injected and fault-free (normal) conditions.

The considered fault types are grouped into four groups as
of following: 1) throw exception (method raises an exception
in accordance to a predeï¬ned API list), 2) wrong return value
(method returns an incorrect value, e.g., return null reference),
3) wrong parameter value (calling a method with an incorrect
value for a parameter), and 4) delay (method returns the result
after a long delay, e.g., caused by hardware failure â€“ leading
to triggering timeout mechanisms or stall). As a running
workload with a unique task ID,
the authors considered
the creation of a new instance deployment. This workload
conï¬gures a new virtual infrastructure from scratch â€“ it creates
VM instances, volumes, key pairs, and security groups, virtual
network, assigns instance ï¬‚oating IPs, reboots the instances,
attaches the instances to volumes and deletes all resources.
Importantly, this comprehensive workload invokes the three
key services of OpenStack Nova, Cinder, and Neutron, causing
diverse manifestations of the faults as failures.

To generate ground truth labels for the failure state, assertion
and API checks are performed at the end of the workload runs.
There are three failure types: 1) failure instance, 2) failure SSH
and 3) failure attaching volume. While the authors provide
information on a granularity of a workload with a task ID,
we further labeled the individual logs. More speciï¬cally, two
human annotators labeled more than 200000 logs to ï¬nd the
ones related to the logged failure. The agreement between the
annotators is 0.67 Cohenâ€™s Kappa score. TABLE I gives the
detailed statistics of the used data.

Syntetic dataset: To evaluate the robustness of our method
in dealing with unstable log data, we have created a synthetic
dataset. The data is created similarly as in [6]. We start with
the normal OpenStack dataset and apply the following three
operations to extract failure sequences, i.e., 1) random removal
of log events, 2) repetition of a randomly selected log event in
the sampled log sequence, and 3) random shufï¬‚ing the order
of several events. To inject unstable log event sequences, we
randomly sample 500 log sequences (normal and failed), and

in b-percentage in the sampled data, we inject the aforenamed
operations in random order.

2) Baselines: We compare the failure detection method
three unsupervised baselines (two sequential-based
against
DeepLog [11], HMM [17], and one count-based PCA [22]),
and two methods commonly used in practice by developers [5].
Those are â€Log Levelâ€, which uses the severity level of
the log (i.e., failure exist if the log level is one of â€errorâ€,
â€fatalâ€, or â€criticalâ€) and â€Semanticâ€ based on the semantics
of a log (i.e., a human identiï¬es the failure as logged in
a single log line) [23]. Recent study [8] identiï¬es DeepLog
as having a state-of-the-art performance among unsupervised
methods. Additionally, we considered a supervised automatic
failure identiï¬cation method LogRobust [6],
that requires
labels for the severity level of the sequences. For failure type
identiï¬cation, we compare against LogClass [4], which trains
a multiclass model on individual logs to identify failures types.
We used task ID failure type alongside the annotations of the
single logs to construct a target label and apply this method.

the model size d was set

3) Experimental Setup: We conducted the experiments as
follows. The hyperparameters of the log parser Drain, i.e.,
the similarity threshold and depth, were set to 0.45 and 5 as
commonly used values for OpenStack logs [13]. For phase
1 the training was performed for a maximal of 200 epochs,
and phase 2 training for a maximal of 20 epochs. As an
optimizer, we used SGD with a learning rate set to 0.0001.
For the encoder,
to 128, with
two encoder layers and four heads. To prevent overï¬tting,
we set
the dropout rate to 0.01. Experimentally, we ï¬nd
that Î» with value 0.1 leads to robust results. The optimized
hyperparameters of CLog (performed on a separate validation
set) are the number of extracted subprocesses, the window
size, and the number of hidden states of the HMM. They were
selected from the range values of the sets {10, 20, 30, 40, 50},
{60s, 120s, 180s, 240s, 300s} and {2, 4, 8, 16} accordingly.
The max length was set
to 32. The hyperparameters of
the considered FTI methods set are to their implementation
defaults from the sckit-learn library. The baselines for failure
detection were trained following a survey [2] of log-based
failure detection from software systems. LogClass was trained
as in the original paper [4]. The failure detection performance
was evaluated on F1, precision and recall as common evalua-
tion metrics, with the failure being a positive label. The same
performance scores were used for FTI, with macro averaging
over the three failure types. The experiments were conducted
on a Linux server with Intel Xeon(R) 2.40GHz CPU and
RTX 2080 GPU running with Python 3.6 and PyTorch 1.5.0.

TABLE II
COMPARISON OF CLOG AGAINST BASELINES ON FAILURE DETECTION.

Scores/Category
Methods:
F1
Precision
Recall

Unsupervised

CLog
0.94Â±0.02
0.97Â±0.03
0.91Â±0.03

HMM
0.82Â±0.09
0.8Â±0.11
0.84Â±0.10

PCA
0.77Â±0.05
0.82Â±0.07
0.73Â±0.06

DeepLog
0.85Â±0.03
0.78Â±0.02
0.93Â±0.03

Developer Practicies

Semantic
0.81Â±0.0
1.0Â±0.0
0.66Â±0.0

Log Level
0.70Â±0.02
0.74Â±0.02
0.65Â±0.02

Supervised
LogRobust
0.96Â±0.01
0.94Â±0.02
0.98Â±0.02

B. Research Questions

1) RQ1: How does CLog compare against baselines on the
task of failure detection?: We evaluate CLog detection per-
formance against three unsupervised methods, two commonly
used developer practices and one supervised method. The
training is done on 60% randomly sampled normal sequences,
while the thresholds (and other hyperparameters) are selected
on a random sample of 20% normal sequences. The rest of the
sequences are used to report the performance scores. The best
results for CLog are obtained for a total of 10 subprocesses
(centroids), two hidden states in the HMM and a window size
of 180 seconds1. The experiments are repeated ten times to
reduce the assessment bias of the results. We report the mean
and standard deviation of the results.

Since CLog is an unsupervised method, we ï¬rst discuss
the results between CLog and the unsupervised baselines.
TABLE II shows the results. CLog outperforms the unsu-
pervised baselines by margins between 9-17% on the F1
score. Importantly, CLog and HMM both use HMM to model
the sequences, but they differ in the granularity of the in-
put representation. Marginalizing over the learning method
suggest
that changing the input representation of the log
event sequences with sequences of subprocesses is beneï¬cial.
Combining these results with our observation (see Fig. 2)
demonstrates that reducing the entropy by changing the input
representation improves the detection performance. CLog pre-
dominantly improves the precision over the sequence-based
methods (DeepLog and HMM), while having strong perfor-
mance on recall. The input of the sequential-based baselines
has larger entropy which challenges the discrimination against
normal sequences, leading to many of them being detected as
failures, i.e., increasing the false positives. Comparing CLog
against the quantitative-based method (PCA) leads to good
performance in precision but reduced recall. The count vector
representation and the limited modeling power of PCA (as a
linear model) are potential causes for the incorrect detection
of the true failures. Notably, from an economic perspective,
the observed improvements are signiï¬cant because improving
failure detection by even 0.1% in F1 score can save hundreds
of thousands of dollars [9].

The improved performance of CLog against the two com-
monly used developer practices, i.e., log level (by 24% on
F1) and single line semantic-based approach, is mainly due
to the problem of insufï¬cient logging failure coverage. The
semantic-based baseline is constructed based on the expert

1The code and the data are given in the GitHub repository of the project

https://github.com/context-aware-Failure-Identiï¬cation/CLog.git

inspection of the logs for failures, i.e., it detects all of the
single-line logged failures. However, as we observed when
performing the manual log analysis, and as shown in Cotroneto
et al. [12], around 20% of the failures in the dataset are not
explicitly logged. In comparison, CLog can detect non-logged
failures because it models different contexts, i.e., its correlates
events co-occurring together. The violations of these contexts
(e.g., an expected log event is missing from the context) are
informative in implicitly detecting non-logged failures. The
log level-based approach experiences the lowest performance.
Despite the problem of insufï¬cient failure coverage, it further
suffers from the problem of wrong log level assignment [23].
A log may be assigned a log level â€ERRORâ€ but still describe
a normal event. Therefore, relying on the log level leads to
reporting more failures than there are, affecting the precision.
Finally, comparing CLog against the supervised baseline
LogRobust, suggests that CLog has a drop in performance by
2% on the F1 score while for others, it exceeds 11%. However,
LogRobust requires labeled log sequences to build a model.
Due to a large number of logs constantly being generated,
the labeling is often infeasible in practice, and it is the most
common referenced critique of the supervised methods [2].
Therefore, CLog has better practical properties because of the
high detection performance and unsupervised design.

TABLE III
COMPARISON OF CLOG AGAINST BASELINES ON FTI.

Scores

F1

Precision

Recall

Multiclass
method
RF
DT
LR
AdaBoost
RF
DT
LR
AdaBoost
RF
DT
LR
AdaBoost

CLog
(pHMM rep.)

0.74Â±0.0
0.74Â±0.0
0.72Â±0.0
0.69Â±0.0
0.74Â±0.0
0.74Â±0.0
0.71Â±0.0
0.71Â±0.0
0.75Â±0.0
0.75Â±0.0
0.73Â±0.0
0.73Â±0.0

CLog
(CV rep.)

0.86Â±0.01
0.78Â±0.04
0.86Â±0.0
0.86Â±0.02
0.86Â±0.01
0.78Â±0.04
0.85Â±0.0
0.86Â±0.02
0.87Â±0.01
0.81Â±0.04
0.88Â±0.0
0.87Â±0.02

CLog
(CV+pHMM
combined)
0.86Â±0.01
0.78Â±0.03
0.87Â±0.0
0.87Â±0.02
0.86Â±0.01
0.78Â±0.03
0.87Â±0.0
0.86Â±0.02
0.87Â±0.01
0.81Â±0.03
0.89Â±0.0
0.88Â±0.02

LogClass
(TFILF)

0.84Â±0.05
0.84Â±0.08
0.8Â±0.1
0.62Â±0.12
0.83Â±0.05
0.82Â±0.07
0.77Â±0.08
0.59Â±0.13
0.86Â±0.06
0.85Â±0.07
0.88Â±0.11
0.74Â±0.14

2) RQ2: How effective is CLog for the problem of failure
type identiï¬cation?: This RQ evaluates the capability of the
FTI module of CLog to reuse the historical information from
the operator in detecting different types of failures. Specif-
ically, we evaluate three representations of the subprocess
sequences for CLog (1. probability score from the HMM
(pHMM), 2. count vectors (CV), and 3. combination of both)
against LogClass [4] as a baseline. LogClass uses single logs
as input. We randomly sample 60% of the labeled failure
sequences/logs from the original dataset to train the multiclass

using the synthetic dataset generation procedure previously
described. TABLE IV shows the results. CLogâ€™s detection
method preservers high detection performance even under a
high ratio of unstable sequences. The results demonstrate that
the extracted subprocesses are sufï¬ciently sensitive to the local
changes within the log sequences, making the performance
robust.

TABLE IV
CLOG FAILURE DETECTION EVALUATION ON SYNTHETIC DATA

injection ratio
5%
10%
15%
20%

F1
0.94
0.92
0.90
0.88

Precision
0.97
0.95
0.95
0.94

Recall
0.91
0.89
0.86
0.83

V. RELATED WORK

Failure Detection. There are plenty of works considering
the problem of log-based failure detection. Considering the
assumption of whether log labels are available or not, the
methods are categorized into: unsupervised and supervised.
We ï¬rst discuss the unsupervised methods. They are consid-
ered practically useful because they do not require labels [2].
these are one-class methods modeling the
Predominantly,
normal system state and reporting failures when signiï¬cant de-
viations occur. Yamanishi et al. [17] introduce an unsupervised
sequential method for failure detection that uses HMM on log
event sequences to model the normal state. The probability
under the HMM is used as a normality score. DeepLog [11]
trains a neural network â€“ LSTM on sequences of log events
on the auxiliary task of next event prediction. If the output
prediction of the auxiliary task is wrong, the method reports
failure. LogAnomaly [24] is similar to DeepLog, but further
augments the input of DeepLog with semantic and count-
based features. Another popular method is PCA [22]. It is
a reconstruction-based method that constructs subspace from
the count vectors of the normal log event sequences and uses
the reconstruction error to detect failures. Compared to other
methods that do not directly address the problem of unstable
sequences, CLog addresses it by representing the sequences
of log events with sequences of subprocesses. The semantic-
based methods use the semantics of the single logs to identify
failures. Despite the traditionally used approaches in industry,
like keyword search (e.g., search for â€errorâ€) or log level
search [5], another line of works learns properties of failure
logs. One such example is Logsy [10]. It combines labeled
data from other software systems and a hyperspherical loss
when learning the discriminative properties of the failures. The
semantic baseline we considered assumes that methods like
Logsy perform ideally, thereby, we do not directly compare
with it. These methods are unable to detect failures that not
explicitly are logged. By modeling sequences, CLog detects
contextual failures when abrupt changes in the normality
contexts of the co-occurring events occur.

Supervised methods assume the availability of labels for
logs from the target system. LogRobust [8] uses a sequential

Fig. 6.

Impact of the window size over the detection performance.

model, while the remaining 40% are used for evaluation.
To reduce the bias due to the sampling, we repeated the
experiments 30 times. We report the average performance
scores and their standard deviation over the different methods,
evaluating the representations independent of the methods.

TABLE III enlists the results of the three different repre-
sentations of CLog and the baseline on the FTI subproblem.
The analysis of the three representations by CLog suggests
that the combination (CV+pHMM) achieves the best F1 score.
Predominantly, the improvement originates from the count
vectors, seen by the better individual results in comparison
with pHMM. Finally, the combination (CV+pHMM) of CLog
outperforms the baseline LogClass. LogClass uses single logs
to identify the type of failures. Therefore, if the failure is
not explicitly logged, LogClass cannot identify its type. On
the contrary, CLog considers the occurrence of the individual
subprocesses and can represent discriminative patterns among
the types of failures improving the performance.

3) RQ3: How does the entropy of the sequences inï¬‚uence
the failure detection performance independent of the model?:
With this question, we verify the impact of the unstable log
event sequences over the failure detection performance in a
model agnostic manner. Following our key observation given
in Section II-B, we grouped the input events into time-intervals
of increasing window size and evaluated several models of
CLog, DeepLog, and PCA similarly as in RQ1. The best
average performance of CLog averaged over the window sizes
(not individual as in RQ1) is obtained for 30 subprocesses.

Fig. 6 shows the results. It can be observed that as the
window size increases, the detection performance decreases.
Paring the average entropy over the sequences for different
window sizes (see Fig. 2) with the detection results reveals
a negative correlation between the increased entropy and
failure detection performance. Owning to the greater modeling
power CLog, and DeepLog show a relatively lower drop in
performance in the range 4-5%. In comparison, PCA has a
performance drop between 8-50%. Importantly, CLog outper-
forms DeepLog because of the smaller instability in the input.
4) RQ4: How robust is CLog on the problem of failure
detection?: We conduct experiments on the synthetic dataset.
The failure detector was trained on the original data as in
RQ1. We randomly inject bâˆ’percentages unstable sequences

60s120s180s240s300swindow size0.20.40.60.81.0F1PCADeepLogCLog60s120s180s240s300swindow size0.20.40.60.81.0Precision60s120s180s240s300swindow size0.20.40.60.81.0Recallrepresentation of the input to train a deep learning neural
network â€“ LSTM augmented with an attention mechanism to
learn failure sequences. Other methods, such as SVM, decision
trees, logistic regression and nearest neighbours, are also being
considered [25]. Due to the evolution of logs and their large
volumes, the expensiveness of labeling is referenced critique
to supervised methods, questioning their usability [2].

Failure Type Identiï¬cation. In one of the earliest works,
Oliner et al. [26] use keyword search of common words
(e.g., â€interface failureâ€, â€errorâ€, and similar) to identify
different types of failures within single logs of four different
supercomputer systems. Similarly, Meng et al. [27] use single
log lines represented as bag-of-words, alongside the Random
Forest method to classify different types of system logs. Log-
Class [4] introduces TF-ILF as a novel representation method
logs and applies commonly used multiclass
for individual
classiï¬cation methods to categorize the type of failure. A
common drawback of these approaches is the assumption
of full failure coverage in single logs. However, due to the
problem of insufï¬cient logging coverage within the source
code [3], this may not always be the case. Different from
others, we pair count vectors from the subprocesses of a
given sequence with a multiclass classiï¬er to use the past
information about similar failure types.

VI. CONCLUSION

This paper addresses the problem of the automation of log-
based failure identiï¬cation, which is a crucial maintenance
task to enhance the reliability in cloud systems. It introduces
a novel method CLog, which decouples the problem of failure
identiï¬cation into two subproblems 1) failure detection and
2) failure type identiï¬cation. We observe that by representing
the input log data as sequences of subprocesses instead of
sequences of individual events, the entropy in the input, caused
by the unstable logs, is reduced. CLog uses this observation
and introduces a novel subprocess extraction method, which
jointly trains context-aware deep learning and clustering meth-
ods to extract subprocesses. Our experiments demonstrate that
the extracted sequences of subprocesses are beneï¬cial for im-
proving the performance of the two subproblems of 1) failure
detection (by 9-24% over the baselines) and 2) failure type
identiï¬cation (by 7% over the baseline). Further, we show that
CLog has robust performance, under high ratios of injected
unstable sequences, experiencing just a 6% performance drop.
The key observation presented herein opens new possibilities
for how to most efï¬ciently extract meaningful subprocess with
minimal information about the sequences (e.g., discarding the
sequence identiï¬ers), which we aim to explore next. These
achievements can ultimately bridge the gap between automatic
log-based failure detection and root-cause analysis, further
enhancing the reliability of cloud systems.

REFERENCES

[1] P. Garraghan, R. Yang, Z. Wen, A. Romanovsky, J. Xu, R. Buyya, and
R. Ranjan, â€œEmergent failures: Rethinking cloud reliability at scale,â€
IEEE Cloud Computing, vol. 5, pp. 12â€“21, 2018.

[2] S. He, J. Zhu, P. He, and M. R. Lyu, â€œExperience report: System log
analysis for anomaly detection,â€ in Proc. of the 27th IEEE International
Symposium on Software Reliability Engineering, 2016, pp. 207â€“218.
[3] S. He, P. He, Z. Chen, T. Yang, Y. Su, and M. R. Lyu, â€œA survey on
automated log analysis for reliability engineering,â€ ACM Comput. Surv.,
vol. 54, 2021.

[4] W. Meng, Y. Liu, S. Zhang, F. Zaiter, Y. Zhang, Y. Huang, Z. Yu,
Y. Zhang, L. Song, M. Zhang, and D. Pei, â€œLogclass: Anomalous log
identiï¬cation and classiï¬cation with partial labels,â€ IEEE Trans. Netw.,
vol. 18, pp. 1870â€“1884, 2021.

[5] Q. Lin, H. Zhang, J.-G. Lou, Y. Zhang, and X. Chen, â€œLog clustering
based problem identiï¬cation for online service systems,â€ in Proc. of
the 38th International Conference on Software Engineering Companion,
2016, p. 102â€“111.

[6] X. Zhang et al., â€œRobust log-based anomaly detection on unstable log
data,â€ in Proc. of the 27th ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE), 2019, p. 807â€“817.

[7] S. He, Q. Lin, J.-G. Lou, H. Zhang, M. R. Lyu, and D. Zhang,
â€œIdentifying impactful service system problems via log analysis,â€ in
Proc. of the 26th Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, 2018, p. 60â€“70.
[8] Z. Chen, J. Liu, W. Gu, Y. Su, and M. R. Lyu, â€œExperience report: Deep
learning-based system log analysis for anomaly detection,â€ CoRR, vol.
2107.05908, 2021.

[9] Y. Zhu, W. Meng, Y. Liu, S. Zhang, T. Han, S. Tao, and D. Pei, â€œUnilog:
Deploy one model and specialize it for all log analysis tasks,â€ 2021.

[10] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, â€œSelf-
attentive classiï¬cation-based anomaly detection in unstructured logs,â€
CoRR, vol. abs/2008.09340, 2020.

[11] M. Du, F. Li, G. Zheng, and V. Srikumar, â€œDeeplog: Anomaly detection
and diagnosis from system logs through deep learning,â€ in Proc. of the
ACM SIGSAC Conference on Computer and Communications Security
(CCS), 2017, p. 1285â€“1298.

[12] D. Cotroneo, L. De Simone, P. Liguori, R. Natella, and N. Bidokhti,
â€œHow bad can a bug get? an empirical analysis of software failures in the
openstack cloud computing platform,â€ in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. New York,
NY, USA: Association for Computing Machinery, 2019, pp. 200â€“211.
[13] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu, â€œTools and
benchmarks for automated log parsing,â€ in Proc. of the 41st International
Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP), 2019, p. 121â€“130.

[14] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, â€œDrain: An online log parsing
approach with ï¬xed depth tree,â€ in 2017 IEEE International Conference
on Web Services. NY, USA: Curran Associates, 2017, pp. 33â€“40.
[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBERT: Pre-
training of deep bidirectional transformers for language understanding,â€
in Proc. of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics, 2019, pp. 4171â€“4186.
[16] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong, â€œTowards k-means-
friendly spaces: Simultaneous deep learning and clustering,â€ in Proc. of
the 34th International Conference on Machine Learning, 2017.
[17] K. Yamanishi and Y. Maruyama, â€œDynamic syslog mining for network
failure monitoring,â€ in Proc. of the 11nd SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, 2005, p. 499â€“508.
[18] L. Breiman, â€œRandom forests,â€ Mach. Learn., vol. 45, pp. 5â€“32, 2001.
[19] J. R. Quinlan, â€œInduction of decision trees,â€ Mach. Learn., vol. 1, pp.

81â€“106, 1986.

[20] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical
Learning. New York, NY, USA: Springer New York Inc., 2001.
[21] Y. Freund and R. E. Schapire, â€œA short introduction to boosting,â€ in
Proc. of the 16 International Joint Conference on Artiï¬cial Intelligence,
1999, pp. 1401â€“1406.

[22] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, â€œDetecting
large-scale system problems by mining console logs,â€ in Proc. of the
22nd Symposium on Operating Systems Principles, 2009, p. 117â€“132.
[23] H. Li, W. Shang, and A. E. Hassan, â€œWhich log level should developers
choose for a new logging statement?â€ Empir. Softw. Eng., vol. 22, p.
1684â€“1716, 2017.

[24] W. Meng et al., â€œLoganomaly: Unsupervised detection of sequential and
quantitative anomalies in unstructured logs,â€ in Proc. of the International
Joint Conferences on Artiï¬cial Intelligence, 2019, pp. 4739â€“4745.

[25] J. Breier and J. BraniË‡sovÂ´a, â€œAnomaly detection from log ï¬les using data
mining techniques,â€ in Information Science and Applications. Berlin,
Heidelberg: Springer Berlin Heidelberg, 2015, pp. 449â€“457.

[26] A. Oliner and J. Stearley, â€œWhat supercomputers say: A study of ï¬ve
system logs,â€ in Proc. of the 37th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks, 2007, pp. 575â€“584.
[27] W. Meng, Y. Liu, S. Zhang, D. Pei, H. Dong, L. Song, and X. Luo,
â€œDevice-agnostic log anomaly classiï¬cation with partial labels,â€ in Proc
of 26th International Symposium on Quality of Service, 2018, pp. 1â€“6.

