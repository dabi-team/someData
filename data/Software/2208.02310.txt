DESIGN OF SECURE AND ROBUST COGNITIVE
SYSTEM FOR MALWARE DETECTION

2
2
0
2

g
u
A
3

]

R
C
.
s
c
[

1
v
0
1
3
2
0
.
8
0
2
2
:
v
i
X
r
a

Sanket Shukla
Electrical and Computer Engineering
George Mason University
Fairfax, USA.
sshukla4@gmu.edu

August 5, 2022

ABSTRACT

The computer systems for decades have been threatened by various types of hardware and software
attacks of which Malware have been one of the pivotal issues. This malware has the ability to steal,
destroy, contaminate, gain unintended access, or even disrupt the entire system. There have been
techniques to detect malware by performing static and dynamic analysis of malware ﬁles, but, stealthy
malware has circumvented the static analysis method and for dynamic analysis, there have been
previous works that propose different methods to detect malware. However, these techniques do
not perform well on stealthy malware. Moreover, the rising trend and advancements in machine
learning has resulted into its numerous applications in the ﬁeld of computer vision, pattern recognition
to providing security to hardware devices. Machine learning based malware detection techniques
rely on grayscale images of malware and tends to classify malware based on the distribution of
textures in graycale images. Albeit the advancement and promising results shown by machine
learning techniques, attackers can exploit the vulnerabilities by generating adversarial samples.
Adversarial samples are generated by intelligently crafting and adding perturbations to the input
samples. There exists majority of the software based adversarial attacks and defenses. To defend
against the adversaries, the existing malware detection based on machine learning and grayscale
images needs a preprocessing for the adversarial data. This can cause an additional overhead and can
prolong the real-time malware detection. So, as an alternative to this, we explore RRAM (Resistive
Random Access Memory) based defense against adversaries. Therefore, the aim of this thesis is to
address the above mentioned critical system security issues. The above mentioned challenges are
addressed by demonstrating proposed techniques to design a secure and robust cognitive system. First,
a novel technique to detect stealthy malware is proposed. The technique uses malware binary images
and then extract different features from the same and then employ different ML-classiﬁers on the
dataset thus obtained. Results demonstrate that this technique is successful in differentiating classes
of malware based on the features extracted. Secondly, I demonstrate the effects of adversarial attacks
on a reconﬁgurable RRAM-neuromorphic architecture with different learning algorithms and device
characteristics. I also propose an integrated solution for mitigating the effects of the adversarial attack
using the reconﬁgurable RRAM architecture.

1

Introduction to Malware Threats

The hardware security discipline in recent years experienced a plethora of threats like the Malware attacks [1–7],
Side-Channel Attacks [8–11], Hardware Trojan attacks [12], reverse engineering threats [13–26] and so on. I focus on
the malware detection technique here along with some state-of-the-art works. Malicious Software, generally known as
‘malware’ is a software program developed by an attacker to gain unintended access of a computer system for performing
illegit and malicious activities like stealing data, sensitive information ( like passwords, SSN’s), contaminating and
manipulating data without users consent. According to 2018 threat report by McAfee labs, about 73 million malicious
ﬁles, 66 million malicious IP addresses and and 63 million malicious URL’s were detected and declared as risky. Similar

 
 
 
 
 
 
threat report by McAfee reported 57.6 million malicious ﬁles in 2017. This rapidly increasing trend of generation
of malware is a serious threat and global concern for the community. It results in need to develop a promising
and comprehensive malware detection methods with robustness. Traditional and primitive software based solutions
for malware detection methods such as signature-based and semantics-based anomaly detection techniques induce
remarkable computational overheads to the system [27] [28] [29] [30]. Traditional approaches towards analyzing
malware involve extraction of binary signatures from malware, comprising their ﬁngerprint. There is an exponential
increase in the number of new signatures released every year, due to the rapid escalation of malware.

1.1 Existing Machine Learning and Deep Learning Defense Against Malware

Machine learning is an emerging technique and is extensively used in various ﬁelds like computer-vision, pattern-
recognition, natural language processing, computer security etc. where the massive volume of data is generated regularly.
Among several machine learning classiﬁers, the neural network class, especially deep neural networks (DNNs) and
convolutional neural networks (CNNs), have tremendously transformed the capabilities and computational power of
the computer systems. Some of the machine learning applications in the aforementioned ﬁelds includes self-driving
cars [31], deep space exploration, weather prediction, object recognition, and so on. Advancements and progress in
the ﬁeld of computer-vision has anticipated the development of self-driving cars without any human intervention [32].
Similarly, machine learning has shown promising results to secure computer systems against malware and stealthy
malware using image recognition [33] and pattern recognition [7, 34] techniques. Despite the beneﬁts and results
showcased by advancements in machine learning [35], the existing vulnerabilities tend to exploit by impacting the
performance of the machine learning classiﬁer.

1.2

Introduction to Adversarial Training Data

Although the machine learning techniques tend to be robust to the noise, the exposed vulnerabilties has shown that
the output of machine learning classiﬁer can be easily manipulated by crafting perturbations to the input data [36–38].
The data generated by crafting perturbations is generally known as Adversarial samples. These adversarial samples
are constructed by perturbing the input data in one or multiple cycles iteratively under certain constraints in order to
escalate the classiﬁcation error rate.

Figure 1: (a) Original MNIST digit “7"; (b) BIM generated adversarial image; (c) DF generated adversarial image; (d)
FGSM generated adversarial image and (e) MIM generated adversarial image

Figure 1 illustrates a simple adversarial sample generated from the MNIST digit dataset [39] for digit ‘7’. The Figure
1(a) is the orgiginal image which is classiﬁed as 7 by the neural network classiﬁer. The images in Figure 1(b), 1(c), 1(d),
1(e) are generated by the basic iterative method (BIM), Deepfool attack (DF), fast gradient sign method (FGSM), and
momentum iterative method (MIM) respectively. It can observed from the Figure 1(a), 1(c) and 1(e) that the normal and
adversarial samples look similar for human observation. It needs to be noted that the noise in Figure 1(c) and 1(e) can
be increased or reduced by tuning the parameters of the attack. With the change in attack parameters, the classiﬁer
output and it’s conﬁdence will be modiﬁed. More details on generating the adversarial attacks are presented in Section
4.

2

(b) BIM(c) DF(a) Original digit(d) FGSM(e) MIMIn this work, the intent is to design a secure cognitive system by proposing two defense mechanisms to mitigate the
malware and adversarial attacks. The proposed defenses are organized in two distinct chapters and discussed in detail.

2 Malware Detection by Extracting Gray-scale Image Features

As seen in the introduction, static code analysis and dynamic code analysis are some of the approaches used for
analyzing malware. Static analysis looks for malicious patterns by disassembling the code and exploring the control
ﬂow of the executable. Whereas, in dynamic analysis malicious code is executed in a virtual environment and based on
the execution trace a behavioral report characterizing the executable is generated. These techniques have their pros
and cons. Although, static analysis offers the most complete coverage but it suffers from code obfuscation. Prior to
analysis, the executable has to be unpacked and decrypted, and even then, the analysis can be thwarted by problems
of intractable complexity. Dynamic analysis does not need the malware executable to be unpacked or decrypted and
is also more efﬁcient but it is time intensive and consumes resources, which results in scalability issues. Moreover,
sometimes environment does not satisfy the triggering conditions, leaving some malicious behaviors unobserved.

This work focuses on a completely different and a novel approach to characterize and analyze malware. Approach tends
to represent a malware executable as a binary string of zeros and ones. Furthermore, this vector of binaries can be
reshaped and converted into a matrix which can be later viewed as an image. Malware belonging to same family showed
signiﬁcant visual similarities in image texture. We discuss representing malware binaries as images. We consider
malware classiﬁcation problem as one of image classiﬁcation problem. Existing classiﬁcation techniques require either
disassembly or execution whereas our method does not require either but still shows signiﬁcant improvement in terms
of performance. Moreover, our method is also resilient to popular obfuscation techniques such as section encryption.
This automatic classiﬁcation technique should be very valuable for anti-virus companies and security researchers who
report thousands of malware everyday.

2.1 Literature Review and Motivation

Several tools such as text editors and binary editors can both visualize and manipulate binary data. There have been
several GUI-based tools which facilitate comparison of ﬁles. However, there has been limited research in visualizing
malware. In [40] Yoo used self organizing maps to detect and visualize malicious code inside an executable. In [41]
Quist and Liebrock develop a visualization framework for reverse engineering. They identify functional areas and
de-obfuscate through a node-link visualization where nodes represent the address and links represent state transitions
between addresses. In [42] Trinius et al. display the distributions of operations using treemaps and the sequence of
operations using thread graphs. In [43] Goodall et al. develop a visual analysis environment that can aid software
developers to understand the code better. They also show how vulnerabilities within software can be visualized in their
environment.

While there hasn’t been much work on viewing malware as digital images, Conti et al. [44] visualized raw binary data of
primitive binary fragments such as text, C++ data structure, image data, audio data as images. In [45] Conti et al. show
that they can automatically classify the different binary fragments using statistical features. However, their analysis is
only concerned with identifying primitive binary fragments and not malware. This work presents a similar approach by
representing malware as grayscale images.

Several techniques have been proposed for clustering and classiﬁcation of malware. These include both static analysis
[46–52] as well as dynamic analysis [53–56]. We will review papers that speciﬁcally deal with classiﬁcation of malware.
In [56] Rieck et al. used features based on behavioral analysis of malware to classify them according to their families.
They used a labeled dataset of 10,072 malware samples labeled by an anti-virus software and divide the dataset into 14
malware families. Then they monitored the behavior of all the malware in a sandbox environment which generated a
behavioral report. From the report, they generate a feature vector for every malware based on the frequency of some
speciﬁc strings in the report. A Support Vector Machine is used for training and testing the feature on the 14 families
and they report an average classiﬁcation accuracy of 88%. In contrast to [56], Tian et al [49] use a very simple feature,
the length of a program, to classify 7 different types of Trojans and obtain an average accuracy of 88%. However, their
analysis was only done on 721 ﬁles. In [50,51] the same authors improve their above technique by using printable string
information from the malware. They evaluated their method on 1521 malware consisting of 13 families and reported a
classiﬁcation accuracy of 98.8%. In [53], Park et al. classify malware based on detecting the maximal common sub
graph in a behavioral graph. They demonstrate their results on a set of 300 malware in 6 families.

With respect to related works [57–63], our classiﬁcation method does not require any disassembly or execution of the
actual malware code. Moreover, the image textures used for classiﬁcation provide more resilient features in terms
of obfuscation techniques, and in particular for encryption. Finally, we evaluated our approach on a larger dataset

3

consisting in 25 families within a malware corpus of 9,458 malware. The evaluation results show that our method offers
similar precision at a lower computational cost.

2.2 EDA Analysis of the Malign Dataset

To mitigate the issue of classiﬁcation of malware our ﬁrst step was to perform exploratory data analysis (EDA) on
the dataset. We are using malimg dataset for the analysis and the dataset distribution is as shown in Figure 4. Prior
to perform Exploratory data analysis we extracted some important features to create training and testing dataset. We
extracted the following features for grayscale images in dataset: “energy, entropy, contrast, dissimilarity, homogeneity
and correlation". “Entropy" deﬁnes statistical measure of randomness used to characterize the texture of the input
image. “Energy" deﬁnes sum of squared elements in the gray level co-occurence matrix. “Contrast" deﬁnes intensity
contrast between pixel and neighbor. “Dissimilarity" degree of dissimilarity between images. “Homogeneity" measures
the closeness of the distribution of elements in the gray level co-occurence matrix to gray level co-occurence matrix
diagonal. “Correlation" deﬁnes correlation between a pixel and its neighbor over entire image.

Figure 2 shows 2-D scatter plot of gabor-entropy v/s LBP-energy. Here we can easily cluster datapoints of 3 different
classes, however there are many overlapping datapoints which makes it difﬁcult to rely on these two features for
malware classiﬁcation. Similar when we select other features as shown in Figure 3 gabor-entropy v/s correlation. In this
2-D scatter plot we can cluster datapoints of 4 different classes but still some datapoints are overlapped. So, neither of
the 2-D scatter plots could give us signiﬁcant classiﬁcation outcomes. The graphs in Figure 3 and Figure 2 exhibits
collinearity and to overcome this we extracted new features by performing feature engineering on current features by
applying some mathematical functions like log, square, cube, etc. To classify the datapoints in the overlapping reqion
was a big challenge. At the same time we had to consider time complexity, power and performance factors as well. This
calls for more robust malware detection mechanism.

Figure 2: Gabor-entropy v/s LBP-energy

• Here explain all the extracted features in detail - like one small paragraph each feature and how you extracted

them

• Mention how you converted the binaries to the images.

• Any challenges you faced. Then how you extracted the features.

4

Figure 3: Gabor-entropy v/s Correlation

2.3 Experimental Setup and Data Collection

All the data collection was done using Python scripts for different classes of malware images and the extracted features.
The experiments were run on a Windows 10 OS with Intel i7 Coffeelake, 32GB RAM, NVidia GTX 1080 Ti 8GB
Graphics Card. The scripts were run in Jupyter notebook and the machine learning models were trained on the same
machine with Weka tool [64].

2.4 Proposed Technique for Malware Detection

Figure 5 depicts the entire process that we have performed to detect malwares and categorize them in different
classes.The ﬁrst three blocks of the Figure 5 namely Malware Images, Feature Extraction, constructing CSVs and EDA
visualization have been discussed in detail. Here we describe the other blocks of the diagram. So far we have learned
all the background required for detecting malware using features corresponding to the malware images which were
extracted from malware binaries. The conclusion of our EDA motivated that further analysis needed to be done to
make fruitful use of the dataset created and process it in such a way that would give better and desired results after
classiﬁcation. The whole purpose is to prepare the data such that we do not need to tweak the classiﬁcation models to a
great extent and hence save a lot of time. Until now we are done with:

• Obtaining the dataset

• Converting binaries to grayscale images

• Developing Python scripts to extract different features related to images from the dataset

• Generating CSV dataset ﬁles for the 6-main classes and 25-sub classes of malware

• Exploratory data analysis for dataset visualization of the 6-main and 25-sub classes to conclude what direction

to proceed in

Depending upon the EDA results, we were suggested to go for a feature selection technique that basically reduces our
features to avoid irrelevant features being taken into consideration.

5

Figure 4: Dataset distribution with Classes and Sub-Classes

2.4.1 Feature Reduction and Selection using Principle Component Analysis

There are many feature reduction and selection algorithms available but we decided to go with the most common PCA
(Principle Component Analysis) algorithm. As the name suggests, PCA tries to minimize dataset to relevant features.
There are many reasons why PCA component analysis is crucial before training classiﬁers:

• Reducing Computation Time and Complexity: More the features, more time required to train the ML-model.

Hence less complexity of the implemented model.

• Remove Irrelevant Features: There might be many features in the dataset that do not signiﬁcantly contribute to
’relevant information’ needed for best classiﬁcation accuracy results, hence, removing or reducing the dataset
to the most relevant ones will provide better accuracy under less time.

PCA is an algorithm that constructs totally new features known as Principle Components (PCs). These newly constructed
features basically cover all the variance and information of the dataset thus reducing the complexity of models without
compromising the information in the dataset. We had set the conﬁguration in the Weka tool to include 95% of the
information of the dataset (both 6-class and 25-subclasses) with at the most 5 features while building the PC equations.
The results of our PCA analysis is discussed in the Section 2.5.

2.4.2 Feature Engineering - Scaling and Normalization

Usually the dataset that is built using the extracted features is not ‘clean’ and needs a lot of preprocessing to ensure
optimal detection accuracy of the classiﬁers and its correct functioning as well. In our case, since we built the feature
extraction and CSV scripts such that it was preprocessed even before the data was saved to CSV ﬁles hence eliminating
the need of ‘cleaning’ the dataset. The only thing that we needed to do was data normalization which is a prerequisite
of the PCA analysis. We normalized the dataset with the help of a ﬁlter available in the Weka tool named “normalize".
This scales and normalizes and entire columns in the dataset and also does the same across the columns. This process
was much needed to ensure our classiﬁcation stage does not take any performance hit. Figures 6 and 7 show the
visualization of the main and the sub-classes of malware dataset for correlation and contrast features. It shows how
the data is distributed across various classes of malware and we have chosen only those graphs that have distribution
spread across the x-axis, whereas all the other feature graphs were not so much spread and the columns overlapped each

6

 Figure 5: Dataset distribution with Classes and Sub-Classes

other - this was resolved when we trained the ML classiﬁers with the dataset because the problem is solved in higher
dimensional space as against 2D visualization.

2.4.3 Bag of Classiﬁers

After the visualization was done on the dataset, we have trained our classiﬁer(s) with the dataset. We have named this
section as ’bag of ML classiﬁers’ because we have trained multiple classiﬁers -each of a different type- to observe and
draw conclusions as to which one performs the best. I our case, we have used the supervised type of classiﬁers which in
our case have shown promising results as against unsupervised methods that rendered less than 30% detection accuracy
and needed further improvisations and hence we did not include those results here. We used the Weka tool [64] to
perform all the ML training and testing. We have used the following types of classiﬁers which are unique in themselves:

7

Figure 6: Visualization for 6-main classes; (a) All malware classes, (b) Correlation feature distribution and (c) Contrast
feature distribution

2.4.4 Naive Bayes

Naive Bayes is a simple, yet effective and commonly-used, machine learning classiﬁer. It is a probabilistic classiﬁer that
makes classiﬁcations using the Maximum A Posteriori decision rule in a Bayesian setting. It can also be represented
using a very simple Bayesian network. Naive Bayes classiﬁers have been especially popular for text classiﬁcation, and
are a traditional solution for problems such as spam detection [65].

2.4.5 Random Forest

Random Forest is a ﬂexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning,
a great result most of the time. It is also one of the most used algorithms, because it’s simplicity and the fact that it can
be used for both classiﬁcation and regression tasks.Random Forest is a supervised learning algorithm. Random forest
builds multiple decision trees and merges them together to get a more accurate and stable prediction [66].

2.4.6 Logistic Regression

Logistic regression is a classiﬁcation algorithm used to assign observations to a discrete set of classes. Unlike linear
regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid
function to return a probability value which can then be mapped to two or more discrete classes. LR could help use
predict whether the class under consideration is a rootkit or a worm. Logistic regression predictions are discrete - only
speciﬁc values or classes are allowed [67]. All the results of the classiﬁers are presented in Section 2.5.

2.4.7 Ontograph with Protege Tool

Figure 8 illustrates the ontograph for malimg dataset. We obtained this ontograph by using protege tool. Protege
tool is a free, open source ontology editor and a knowledge management system. Protege provides a graphic user
interface to deﬁne ontologies. It also includes deductive classiﬁers to validate that models are consistent and to infer
new information based on the analysis of an ontology [68].

2.5 Experimental Results

We here discuss and present all the results that we gathered for various sections of our project discussed so far. The
results are presented in the same order as we have mentioned in the earlier sections.

2.5.1 PCA Analysis

The results of the 6-main class and 25-sub class PCA analysis is as shown in Table 1 respectively. The tables clearly
indicate the features that were used to build the new principle components (PCs) and how much information each
of them carries is mentioned in the ‘attribute’ column of the tables. These PCs can be used to feed and train the

8

 Figure 7: Visualization for 25-sub classes; (a) All malware sub-classes, (b) Correlation feature distribution and (c)
Contrast feature distribution

Figure 8: Ontograph for malimg-dataset

9

 Figure 9: Results for 6-main classes (a) Classiﬁcation results, (b) Validation Results

ML-classiﬁer exploiting the beneﬁts mentioned previously. It is to be noted that we did not use the results of the
PCA for training our classiﬁers because with the existing dataset we could get better accuracy and performance of the
ML-models with limited time complexity and hence it was not needed to use PCA results but we still did include the
results just to show that PCA is also a good option for achieving better accuracy with less time complexity.

Table 1: PCA Results for 25-sub class of malware

Ranked Attributes
0.7445
0.6981
0.6591
0.6234

0.342Energy-0.341Homogeneity
2 -0.567Malware_class=Allaple.A-0.357Malware_class=Lolyda.AA3
3 -0.776Malware_class=Allaple.L+0.524Malware_class=Allaple.A
4 -0.681Malware_class=Yuner.A-0.253Contrast

2.5.2 Classiﬁcation Results

The results of all the classiﬁers that we have used are presented here with the conclusions that we can draw from them.
Figure 9(a),(b) show classiﬁcation accuracy and other performance metrics. Figure 9 shows the results of 6-main classes
of malware after training and testing with 4-different machine learning models. We have used 80-20% train/test while
training the models. Figure 10 shows the bar graph plots for the same data discussed earlier. The bar graphs show
results with different performance metrics and we can conclude that all the classiﬁers used satisfy the given dataset and
the problem of linear EDA graphs where all the plots were kind of linear were solved after classiﬁcation results as the
classiﬁcation is done in higher dimensions. We did 2 fold validation on each classiﬁer and did not go with higher fold
validations as it took a lot of time to validate. The same results have been plotted for 25-sub classes of malware as
shown in Figure 12 and Figure 13. These graphs show that with 25 class dataset, the results are better although some of
the classes overlap which might be because two or more classes show similar characteristic features. Figure 11 shows
the confusion matrix for

10

       (a)                                                                           (b)  Figure 10: Results for 6-main classes (a) Classiﬁcation bar graphs, (b) Validation bar graphs

Figure 11: Confusion matrices for all the Classiﬁers for six main malware class

11

         (a)                                                                                (b)   0.8340.8750.8340.7990.9450.9950.9950.9950.99510.970.970.970.970.9940.9860.9860.9860.9860.99200.20.40.60.811.2TPPrecisionRecallF-measureROCClassification ResultsNaiveBayesRandom ForestLogistic RegressionJrip0.8220.8730.8220.7870.9430.9940.9940.9940.99410.9690.9680.9690.9690.9950.9810.9820.9810.9810.9900.20.40.60.811.2TPPrecisionRecallF-measureROCValidation ResultsNaiveBayesRandom ForestLogistic RegressionJripNaive Bayes - Confusion Matrix - classification\validation  Random Forest - Confusion Matrix - classification\validation a b c d e f      a b c d e f     31\189 64\379 27\162 127\682 0 3\9 a Trojan  247\1402 1\4 1\3 0\6 0 3\6 a Trojan 0 1082\5289 0 9\59 0 0 b Worm  2\14 1089\5323 0 0 0 0 b Worm 0 0 18\110 38\156 0 4\8 c backdoor  1\4 0 59\266 0 0 0\4 c backdoor 0\3 0 0 150\727 0 0 d rootkit  0\7 0 0 150\723 0 0 d rootkit 0 0 0 0 21\106 0 e virus  0 0 0 0 21\106 0 e virus 3\13 0 0\4 0\1 0 79\390 f worm  1\3 0 0\2 0 0 81\403 f worm                  Logistic Regression - Confusion Matrix - classification\validation  JRip - Confusion Matrix - classification\validation a b c d e f      a b c d e f     222\1262 28\142 2 1\11 1 1\3 a Trojan  240\1372 5\33 4\6 2\5 0\0 1\5 a Trojan 16\88 1074\5251 0 1\1 0 0 b Worm  8\54 1083\5286 0 0 0 0 b Worm 1\2 0 59\271 0 0 1 c backdoor  2\11 0 58\258 0 0 0\5 c backdoor 1 0 0 150\729 0 0 d rootkit  1\24 0 0 149\706 0 0 d rootkit 0 0 0 0 21\106 0 e virus  0 0 0 0 21\106 0 e virus 1 0 1 1 1\3 81\402 f worm  1\11 0 0\2 0 0 81\395 f worm  Figure 12: Results for 25-sub classes (a) Classiﬁcation results, (b) Validation Results

Figure 13: Results for 25-sub classes (a) Classiﬁcation bar graphs, (b) Validation bar graphs

12

       (a)                                                             (b)                                                 (a)                         (b)   0.8180.8610.8180.8170.9810.9670.9670.9670.9670.9980.8790.8790.8790.8760.9810.9460.9470.9460.9460.98300.20.40.60.811.2TPPrecisionRecallF-measureROCClassification Results -25 ClassNaiveBayesRandom ForestMulticlassJrip0.8070.8550.8070.8050.980.970.9690.970.9690.9990.8840.8820.8840.8820.980.9390.940.9390.9380.98100.20.40.60.811.2TPPrecisionRecallF-measureROCValidation Results -25 ClassNaiveBayesRandom ForestMulticlassJrip2.6 Related Works

This section discusses some of the previous works that have been published in the past that have proposed methods to
detect malwares. Some of the techniques discussed here are software-based while others are hardware-based. Works
in [69, 70] extensively described how hardware performance counters (HPCs) can be used to detect anomalies in
applications and classify malware as against benign employing ML-classiﬁers. The authors have used HPCs to feed
to different set of classiﬁers and presented their results. They have also proposed malware detection for resource
constrained systems where performance counters are limited and where systems resources have to used sparingly.
Hence, they propose the use of Ensemble learning methods to boost the performance of general ML-classiﬁers. [71]
has discussed about the feasibility of using HPCs for malware detection. They have also used ML-models to classify
applications and supported their claim. In [6, 7] authors detect stealthy malwares by converting malware binaries
into grayscale images and then extracting patterns by performing raster scanning. The grayscale images are further
represented as sequence of patterns which are further used for sequence classiﬁcation using RNN-LSTM’s. Work
in [5] introduces a hybrid approach which utilizes the microarchitectural traces obtained through on-chip embedded
hardware performance counters (HPCs) and the application binary for malware detection. The obtained HPCs are fed
to multi-stage machine learning (ML) classiﬁer for detecting and classifying the malware. Authors in [72] presents a
collaborative machine learning (ML)-based malware detection framework. It introduces a) performance-aware precision-
scaled federated learning (FL) to minimize the communication overheads with minimal device-level computations; and
(2) a Robust and Active Protection with Intelligent Defense strategy against malicious activity (RAPID) at the device
and network-level due to malware and other cyber-attacks. [73] has proposed how kernel-level rootkits can be detected
using HPCs on hardware level. They have described the process of training ML-models with the acquired HPCs and
then presented results in support of their claim. They have tested their proposed mechanism by feeding the detector
with both rootkit and clean traces. Authors in [74] interestingly used SVD (Singular-value decomposition) matrix in
collaboration with HPCs to train ML-models to detect malwares. This is kind of partial software and partial hardware
based approach in detecting anomalies. [75] has proposed a software based approach to detect malicious piece of code.

3 Need for RRAM-Neuromorphic Architecture based Defense Against Adversarial

Attacks

As, described in the introduction, the adversarial attacks can be broadly classiﬁed into two categories: (a) poisoning
attacks and (b) evasion attacks. Poisoning attacks are attacks on the ML classiﬁer during the training phase [76–80], and
the evasive attacks are targeted for inference stage of machine learning techniques. Poisoning attacks are appropriate
for online environments because they focus on attacking the classiﬁers during the training phase. Therefore, in this
work we focus on the evasive attacks, as many of the existing machine learning works are primarily ofﬂine learning
based and are constrained by resources and the computational time requirements.

Rise in the types of adversarial attacks led to development of adversarial defense techniques. Some of the prominent
software based adversarial defenses includes adversarial training [36, 37], defensive distillation [81] and MagNet [82].
Even though these adversarial defense show some robustness against adversarial samples, they also have major
drawbacks and weaknesses. More details over the adversarial defenses is presented in Section 4.

As the aforementioned defense is developed based on software, researchers have started to shift the focus from software
to hardware [83–88]. There have been a new research trending which comprises of leveraging neuromorphic computing
to provide a robust defense against the adversaries. In [89] authors demonstrate the advantage conferred by the
non-idealities present in analog crossbars in terms of adversarial robustness. Authors in the paper [90] propose a
neuromorphic approach based on sparse coding. These neuromorphic based techniques are designed speciﬁcally for a
targeted attack such as one-pixel attack and FGSM attack. The interoperability of these defense against other pool of
adversarial attacks still remains a concern.

In this work, we ﬁrst provide an overview of evasive attacks on the ML classiﬁers. Further, we present different
existing defense techniques for the adversarial attacks. As FGSM is one of the fastest evasive attacks, an in-depth
discussion regarding the FGSM adversarial attack is provided. In this work, we look at initially introduced defense
against adversarial samples, Adversarial training is one of the defense techniques introduced for adversarial attacks.
Further, in this paper we explore the potency of adversarial attacks on a reconﬁgurable RRAM-based Neuromorphic
Architecture. We also propose a method for mitigating adversarial attacks in deployed IoT devices combining precise
software training algorithms and the reconﬁgurable RRAM-based Neuromorphic architecture.

13

4 Background

Adversarial samples are generated by introducing crafted perturbations into the normal input data generated. This
makes the adversarial data look similar to the normal input data, but still the machine learning model mispredicts the
class with a high probability. These adversarial samples can be considered as an optical illusion for the ML classiﬁers.
In this section, we present different techniques widely used for generating the adversarial samples, and review some of
the popular defense techniques deployed.

4.1 Adversarial Attacks

Here we present an overview of some of the adversarial attacks that are effective against machine learning classiﬁers.

4.1.1 Fast Gradient Sign Method (FGSM)

The most common technique to perform adversarial attack is to perturb the image with gradient of the loss with respect
to the image or input. Then gradually increase the magnitude of the perturbation until the image is misclassiﬁed.

Fast Gradient Sign method (FGSM) [37] is one of the ﬁrst known adversarial attacks. The complexity to generate
FGSM attack is lower compared to other adversarial attacks, even against deep learning models. This technique features
low complexity and fast implementation. Consider a ML classiﬁer model with θ as the parameter, x being the input to
the model, and y is the output for a given input x, and L(θ, x,y) be the cost function used to train the neural network.
Then the perturbation with FGSM is computed as the sign of the model’s cost function gradient. The adversarial
perturbation generated with FGSM [37] is mathematically given as

xadv = x + (cid:15)sign(∇xL(θ, x, y))

(1)

where (cid:15) is a scaling constant ranging between 0.0 to 1.0 is set to be very small such that the variation in x (δx) is
undetectable. One can observe that in FGSM the input x is perturbed along each dimension in the direction of gradient
by a perturbation magnitude of (cid:15). While, a small (cid:15) leads to well-disguised adversarial samples, a large (cid:15), is likely to
introduce large perturbations.

4.1.2 Basic Iterative Method (BIM)

From previous discussion it can be observed that, FGSM adds perturbation in each of the dimension, however, no
optimization on perturbations are performed. In [91] Kurakin proposed an iterative version of FGSM, called as Basic
iterative method (BIM). BIM is an extension of FGSM technique, where instead of applying the adversarial perturbation
once with (cid:15), the perturbation is applied multiple times iteratively with small (cid:15). This produces a recursive noise on the
input and optimized application of noise. It can be mathematically represented as follows:

N +1 = Clipx,(cid:15)(xadv
xadv

xadv
0 = x
N + (cid:15)sign(∇xL(θ, xadv
N , y)))

(2)

In the above mathematical expression,Clipx,(cid:15) represents the clipping of the adversarial input magnitudes such that they
are within the neighborhood of the original sample x. This technique allows more freedom for the attack compared
to the FSGM method because the perturbation can be controlled and the distance of the adversarial sample from the
classiﬁcation boundary can be carefully ﬁne-tuned. The experimental results presented in [91] have shown that BIM
can cause higher misclassiﬁcations compared to the FGSM attack on the Imagenet samples.

4.1.3 Momentum Iterative Method (MIM)

The momentum method is an accelerated gradient descent technique that accumulates the velocity vector in the direction
of the gradient of the loss function across multiple iterations. In this technique, the previous gradients are stored, which
aids in navigating through narrow valleys of the model, and alleviate problems of getting stuck at local minima or
maxima. This momentum method also shows its effectiveness in stochastic gradient descent (SGD) to stabilize the
updates. This MIM principle is applied in [92] to generate adversarial samples. MIM has shown a better transferability
and shown to be effective compared to FGSM attack.

14

4.1.4 DeepFool Attack

DeepFool (DF) is an untargeted adversarial attack optimized for L2 norm, introduced in [93]. DF is efﬁcient and
produces adversarial samples which are more similar to the original inputs as compared to the discussed adversarial
samples generated by FGSM and BIM attacks. The principle of the Deepfool attack is to assume neural networks as
completely linear with a hyper-plane separating each class from another. Based on this assumption, an optimal solution
to this simpliﬁed problem is derived to construct adversarial samples. As the neural networks are non-linear in reality,
the same process is repeated considering the non-linearity into the model. This process is repeated multiple times for
creating the adversaries. This process is terminated when an adversarial sample is found i.e., misclassiﬁcation happens.
Considering the brevity and focus of the current work, we limit the details in this draft. However, the interested readers
can refer to the work in [93] for exact formulation of DF.

4.2 Adversarial Defenses

So far, the different adversarial attack techniques are discussed. Here, we discuss some of the prominent existing
defenses against the above discussed attacks.

4.2.1 Adversarial Training

Adversarial training is one of the preliminary solutions for making the ML classiﬁers robust against the adversarial
examples, proposed in [94]. The idea is to train the ML classiﬁer with the adversarial examples so that the ML classiﬁer
can have adversarial information [36, 37] and adapt its model based on the learned adversarial data. However, one of the
major drawbacks of this technique is to anticipate the type of attack and train the classiﬁer based on those attacks and
determining the criticality of the adversarial component.

4.2.2 Defensive Distillation

Defensive distillation is another defense technique proposed in [81]. The idea is to train the classiﬁer using the
distillation training techniques and hides the gradient between softmax layer and the presoftmax layer. This makes it
complex to generate adversarial examples directly on the network [95], as the knowledge is imparted from a bigger
network during the training process. However, [96] shows that such a defense can be bypassed with one of the following
three strategies: (1) choosing a more proper loss function; (2) calculation of gradients from pre-softmax layer rather
than softmax layer; or (3) attack an easy-to-attack dummy network ﬁrst and then transfer to the distilled network,
similar to the distillation defense. The generation of adversaries can be simpler if the attacker knows the parameters and
architecture of the defense network i.e., whitebox attack.

4.2.3 MagNet

MagNet is proposed in [82], where a two-level strategy with detector and reformer is proposed. In the detector phase(s),
the system learns to differentiate between normal and adversarial examples by approximating the manifold of the
normal examples. This is performed with the aid of auto-encoders. Further, in the reformer, the adversarial samples are
moved close the manifold of normal samples with small perturbations. Further using the diversity metric, the MagNet
can differentiate the normal and adversarial samples. MagNet is evaluated against different adversarial attacks presented
previously and has shown to be robust in [82].

4.2.4 Detecting Adversaries

Another defense technique is to detect adversarial examples with the aid of statistical features [97] or separate
classiﬁcation networks [98]. In [98], for each adversarial technique, a DNN classiﬁer is built to classify whether the
input is a normal sample or an adversary. The detector was directly trained on both normal and adversarial examples.
The detector shows good performance when the training and testing attack examples were generated from the same
process and the perturbation is large enough. However, it does not generalize well across different attack parameters
and attack generation processes.

5 Proposed RRAM-based Neuromorphic Architecture

The Internet of Things (IoT) continues to expand and there is increasing interest in computing at the edge of the
network especially in machine learning applications. Many current implementations of machine learning, and more
speciﬁcally Deep Neural Networks (DNNs), have large power and computational resource requirements. The high

15

memory bandwidth and power requirement prevent implementation in low-power real-time applications. Neuromorphic
architectures have been explored for near-memory and in-memory computing to for low-power high memory bandwidth
implementation of neural networks. Particularly, two-terminal RRAM devices in crossbar arrays have been extensively
investigated to store weight matrix and perform in-memory computing. The RRAM-based weight matrix crossbar of
neural networks can be visualized in Figure 15. The input vector gets multiplied by the weight matrix to produce the
composition of the neurons at the output. A winner-take-all or softmax activation function can then be used to determine
the winning neuron and thus the classiﬁcation. In spite of tremendous progress in this area two-terminal RRAM devices
suffers from issues such as high write current, convoluted read and write strategies, and need for a selector diode to
mitigate sneak currents. Recently reported gated-RRAM devices offers to solve these issues and provide opportunities
for simultaneous read and write which will be very beneﬁcial for adjusting the weights as needed.

Figure 14: Feed-forward neural network crossbar performing MAC operation on input vector and weight matrix

Gated-RRAM have been investigated as memristive synaptic devices for in-memory computing of the multiply-
accumulate (MAC) behavior of neurons. Additionally, integrating multi-state gated-RRAM allows for a more dense
memory crossbar since each gated-RRAM device can store multiple bits. The weight matrix crossbar of neural networks
can be visualized in Figure 14. The input vector gets multiplied by the weight matrix to produce the composition of
the neurons at the output. A winner-take-all or softmax activation function can then be used to determine the winning
neuron and thus the classiﬁcation. Similarly in a neuromorphic gated-RRAM crossbar, the product of the input voltage
and gated-RRAM conductance produces a current at each synaptic branch of a neuron. The summation ampliﬁer at the
output then computes the multiply-accumulate function of the input vector, applied as voltages and the weight vector of
gated-RRAM conductances of the neuron as shown in Figure 15.

16

Figure 15: Gated-RRAM neuromorphic crossbar performing MAC operation on voltage-encoded input vector and
conductance-encoded weight matrix

The reconﬁgurability of gated-RRAM allows for synaptic weights to be programmed into the neuromorphic crossbar.
The gated-RRAM device can be potentiated or depressed by applying a positive or negative bias at the gate respectively.
We were able to train weights using the keras package in Python 3.9 on the MNIST dataset and then write them to the
neuromorphic crossbar. However, the conductance curve of the gated-RRAM devices inﬂuences how the off-chip trained
weights are mapped to the RRAM conductance space. This may effect the behavior of the neuromorphic-implemented
model depending on the non-linearity of the conductance curve of the RRAM devices. Additionally, on-chip training
directly on the RRAM crossbar has been explored and has demonstrated competitive training accuracy [99]. In Bailey
et al. [99], a spike-timing dependent plasticity-based (STDP) algorithm is used to tune the weights of neuromorphic
architecture. This training algorithm results in different weight map, thus a different trained model, than the keras-trained
weights as shown in Figure 17.

Gated-resistive random-access memory (gated-RRAM) has been recently report and investigated as a gated-synaptic
device for neuromorphic architectures [101]. Gated-RRAM is a memristive device consisting of a top and bottom
electrode connected to an oxide channel as shown in Figure 18. A gate terminal is coupled to the channel oxide through
an insulating layer allowing for a bias to be applied across the channel oxide while simultaneously applying a bias across
the top and bottom electrode. This behavior allows for simultaneous reading and writing the gated-RRAM device and
allows minimal programming circuitry. The gated-RRAM device can be potentiated by applying a positive bias to the
gate to cause the oxygen defects to drift toward the top and bottom electrode increasing the conductance of the channel.
The device can then be depressed by applying a negative bias to the gate to cause the oxygen defects to diffuse back

17

Figure 16: Weight map of keras-trained weights on MNIST

Figure 17: Weight map of STDP-based training on MNIST

Figure 18: Gated-RRAM device model with top electrode (TE), bottom electrode (BE), gate, and channel oxide

18

Figure 19: Conductance curves of gated-RRAM tuned through gated-synaptic model in [100]

towards the channel oxide lowering the conductance of the channel. The conductance states or curve a device follows
as it is potentiated and depressed is material dependent. The conductance of these devices has been shown to follow
sigmoidal, linear, or inverse exponential trends as shown in Figure 19 [100]. These conductance curves can determine
the distribution of the conductance states of the gated-RRAM device and are controlled by gc in the gated-synaptic
device model [100]. Similar to the two-terminal RRAM crossbar arrays, in a neuromorphic gated-RRAM crossbar, the
product of the input voltage and gated-RRAM conductance produces a current at each synaptic branch of a neuron.
The summation ampliﬁer at the output then computes the multiplyaccumulate function of the input vector, applied as
voltages and the weight vector of gated-RRAM conductances of the neuron as shown in Figure 15.

6 Results

In this section, we evaluate and present the performance on the MNIST digit dataset [39]. The adversarial attacks are
generated using Cleverhans library [102].

Table 2 reports the performance of the employed neural network on MNIST Digits dataset. Normal Classiﬁcation
Accuracy: In the absence of adversarial samples, the classiﬁer achieves an accuracy of 98.25%, loss of 0.088, precision
of 0.98, and recall of 0.98. We also report the accuracy of neural network in the presence of adversarial attacks in Table
2. The number of adversarial samples are 10,000 in each case, and one can observe that in the presence of adversaries
the classiﬁcation accuracy falls to as low as 1.13%. With the increase in (cid:15), the accuracy decreases in case of FGSM,
MIM and BIM.

We present the effect of adversarial training by training neural network with adversarial data generated using different
adversarial attacks and present the accuracy results in Table 3. The results show that after performing adversarial
training, the accuracy of the neural network to classify data improves as reported in the Table 3. However, the accuracy
can further degrade if the attack parameters ((cid:15) in this case) are modiﬁed.

We then loaded the keras-trained weights to the RRAM neuromorphic architecture and tested it on the MNIST dataset
and the various adversarial attacks. Additionally, we also tested the performance of the on-chip STDP-trained weights
on the same datasets. Table 4 report the accuracy of both the keras-trained and STDP-trained weights with RRAM
devices with completely linear conductance curve and no write variability. In the absence of adversarial samples, the

19

Table 2: Accuracy of neural network before and after adversarial attack

Adversarial attack
FGSM
BIM
MIM
DF

Parameter No Attack With Attack
98.25 %
98.25 %
98.25 %
98.25 %

(cid:15) = 0.3
(cid:15) = 0.3
(cid:15) = 0.3
MI = 50

8.54
1.34
1.28
1.13

Table 3: Accuracy (%) of MNIST Classiﬁcation under Different Adversarial Attacks on Different Adversarial Trained
Networks

Attack
Parameter
FGSM
DF
MIM
BIM

BIM
(cid:15) = 0.3
79.1
53.40
78.2

MIM
(cid:15) = 0.3
78.2
54.25

78.1

FGSM
(cid:15) = 0.3 MI=50

DF

41.04
73.7
69.3

77.1

77.1
78.2

off-chip keras-trained network outperforms the on-chip STDP-trained weights but both training methods yield above
70% accuracy. However, with the introduction of adversarial samples, the accuracy is suddenly reduced to below 10%
for keras-trained weights and below 20% for STDP-trained weights. Additionally, we tested the two networks with
varying conductance curves, as shown in Table 5, and varying device write variability, as shown in Table 6. In all cases,
the accuracy of the RRAM architecture was reduced to below 20% once adversarial samples were introduced.

7 Proposed Method of Mitigation and Future Work

In this work, we demonstrated that the gated-RRAM neuromorphic architecture serves as a versatile implentation
of a hardware neural network. The weights can be trained off-chip using software models and programmed during
run-time. Additionally, more biologically inspired algorithms can be applied to train the synaptic devices on-chip. The
gated-RRAM architecture is able to tune is learned model through different training algorithms and through different
device characteristic such as conductance distribution and noise from device write variance. However, we observed that
this versatile model is still susceptible to an unknown adversarial attack.

We would like to investigate an integrated system as shown in Figure 20. Our results in 3 demonstrate the ability of
software networks to train on adversarial attacks for future mitigation of adversarial attacks. We have also demonstrated
the ability to load (write) pre-trained weights to the RRAM neuromorphic architecture model. Therefore, a software
network trained on adversarial models can be implemented using the RRAM neurormorphic architectures allowing it to
be deployed in low-power real-time applications while mitigating attacks from adversaries. Additionally, as shown in
20, we would like to explore methods for the chip to detect unknown adversarial attacks so that it may later train on
them and identify adversarial inputs or sources. Our goal is to develop a reconﬁgurable architecture that is robust to
adversarial attacks and with the capabilities to be integrated in low-power IoT devices or applications.

20

Table 4: Accuracy (%) of MNIST classiﬁcation of RRAM neuromorphic architecture on MNIST dataset and different
adversarial attacks. The conductance curve of the gated-RRAM devices is linear and there is no write variance present.

Adversarial Attack

Training No Attack

Keras
STDP

90.44%
71.22%

MIM
FGSM
8.91%
8.74%
13.09% 11.98% 11.69% 12.15%

BIM
8.95%

DF
9.60%

Table 5: Accuracy (%) of MNIST classiﬁcation of RRAM neuromorphic architecture, with varying conductance
distributions, on MNIST dataset and different adversarial attacks. Additionally, there is no write variance present.

Adversarial Attack

Training RRAM gc No Attack
85.53%
0.0
90.44%
0.5
65.21%
1.0
72.29%
0.0
71.22%
0.5
72.00%
1.0

Keras
Keras
Keras
STDP
STDP
STDP

FGSM
BIM
MIM
DF
9.54%
9.63% 10.43% 9.59%
8.74%
8.91%
9.60%
8.95%
4.96%
4.97%
5.77%
5.12%
9.15%
9.67%
7.62%
9.08%
13.09% 11.98% 11.69% 12.15%
11.36% 10.44% 9.64% 10.50%

8 Conclusion

We have ﬁrst described the process of converting malware binaries to images and then explained the process of feature
extraction. With the help of our results we have supported our claim that malware binary images can be used to detect
classes of malware from each other with high accuracy and with a variety of classiﬁers. We then have presented the
confusion matrices to show the classiﬁcation efﬁciency of the employed models. Protege tool has been used to obtain
ontograph for the dataset based on the main and sub classes. We hope to develop this method and take it further
ahead in classifying malwares from benign and also improvise our time complexity by implementing this on hardware
accelerated GPU. We also plan to improve our Ontology by adding ‘experiences’ to malware instances.

Moreover, we demonstrated that the gated-RRAM neuromorphic architecture serves as a versatile implentation of
a hardware neural network. The weights can be trained off-chip using software models and programmed during
run-time. Additionally, more biologically inspired algorithms can be applied to train the synaptic devices on-chip. The
gated-RRAM architecture is able to tune is learned model through different training algorithms and through different
device characteristic such as conductance distribution and noise from device write variance. However, we observed that
this versatile model is still susceptible to an unknown adversarial attack. We have also demonstrated the ability to load
(write) pre-trained weights to the RRAM neuromorphic architecture model. Therefore, a software network trained on
adversarial models can be implemented using the RRAM neurormorphic architectures allowing it to be deployed in
low-power real-time applications while mitigating attacks from adversaries. Additionally, we would like to explore
methods for the chip to detect unknown adversarial attacks so that it may later train on them and identify adversarial
inputs or sources. Our goal is to develop a reconﬁgurable architecture that is robust to adversarial attacks and with the
capabilities to be integrated in low-power IoT devices or applications.

21

Table 6: Accuracy (%) of MNIST classiﬁcation of RRAM neuromorphic architecture, with varying device variance,
on MNIST dataset and different adversarial attacks. Additionally, the RRAM devices have a linear conductance
distribution.

Adversarial Attack

Training RRAM σ2 No Attack
90.44%
0
Keras
10−6
90.42%
Keras
10−5
90.38%
Keras
10−4
90.41%
Keras
71.22%
0
STDP
10−6
71.2%
STDP
10−5
71.19%
STDP
10−4
70.74%
STDP

DF
9.60%
9.60%
9.64%
9.70%

BIM
8.95%
8.96%
9.00%
9.09%

MIM
FGSM
8.91%
8.74%
8.92%
8.75%
8.99%
8.84%
8.80%
9.01%
13.09% 11.98% 11.69% 12.15%
13.08% 11.98% 11.68% 12.16%
13.10% 12.00% 11.70% 12.20%
13.26% 12.09% 11.75% 12.27%

9 Future Work

We aim to expand this work in the future and expand the scope of these techniques. Firstly, we will augment the malware
data and then try to explore the performance of deep learning based techniques for malware detection. Secondly, we
also look forward to generate stealthy malware data by adding complexity in the malware generated. The idea will be
to evaluate the performance of deep learning and pattern recognition techniques over the stealthy malware data. We
also plan to work on expanding the ontology of the malware classes and sub-classes based on the features extracted,
to interact with the computers to make an efﬁcient decisions. Moreover, our research will further investigate transfer
learning of adversarial attacks for making an efﬁcient RRAM based adversarial attack mitigation. Thus, our intent will
be to make the RRAM-based neuromorphic architecture more robust against adversarial attacks.

References

[1] A. Dhavlle, S. Shukla, S. Rafatirad, H. Homayoun, and S. M. P. Dinakarrao, “Hmd-hardener: Adversarially
robust and efﬁcient hardware-assisted runtime malware detection,” in Design, Automation and Test in Europe
(DATE), 2021.

[2] A. Dhavlle, R. Hassan, M. Mittapalli, and S. M. P. D, “Design of hardware trojans and its impact on cps systems:

A comprehensive survey,” in International Symposium on Circuits and Systems (ISCAS), 2021.

[3] M. M. Ahmed, A. Dhavlle, N. Mansoor, S. M. P. D, K. Basu, and A. Ganguly, “What can a remote access
hardware trojan do to a network-on-chip?” in International Symposium on Circuits and Systems (ISCAS), 2021.
[4] S. M. P. Dinakarrao, S. Amberkar, S. Bhat, A. Dhavlle, H. Sayadi, A. Sasan, H. Homayoun, and S. Rafatirad,
“Adversarial attack on microarchitectural events based malware detectors,” in Design Automation Conference
(DAC), 2019.

[5] S. Shukla, G. Kolhe, S. M. P. D, and S. Rafatirad, “Microarchitectural events and image processing-based hybrid
approach for robust malware detection: Work-in-progress,” in Proceedings of the International Conference on
Compliers, Architectures and Synthesis for Embedded Systems Companion, 2019.

[6] S. Shukla, G. Kolhe, S. M. PD, and S. Rafatirad, “Rnn-based classiﬁer to detect stealthy malware using localized
features and complex symbolic sequence,” in 2019 18th IEEE International Conference On Machine Learning
And Applications (ICMLA), 2019, pp. 406–409.

[7] S. Shukla, G. Kolhe, S. M. P D, and S. Rafatirad, “Stealthy malware detection using rnn-based automated
localized feature extraction and classiﬁer,” in 2019 IEEE 31st International Conference on Tools with Artiﬁcial
Intelligence (ICTAI), 2019, pp. 590–597.

[8] Y. Yarom and K. Falkner, “Flush+reload: A high resolution, low noise, l3 cache side-channel attack,” in USENIX

Conference on Security, 2014.

22

Figure 20: Integrated architecture of neuromorphic architecture with adversarial attack detection and mitigation

[9] D. Gruss, C. Maurice, K. Wagner, and S. Mangard, “Flush+ﬂush: A fast and stealthy cache attack,” in Int. Conf.

on Detection of Intrusions and Malware, and Vulnerability Assessment, 2016.

[10] A. Dhavlle, R. Mehta, S. Rafatirad, H. Homayoun, and S. M. P. Dinakarrao, “Entropy-shield: Side-channel
entropy maximization for timing-based side-channel attacks,” in International Symposium on Quality Electronic
Design (ISQED), 2020.

[11] A. Dhavlle, S. Rafatirad, K. Khasawneh, H. Homayoun, and S. M. P. Dinakarrao, “Imitating functional operations
for mitigating side-channel leakage,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems, pp. 1–1, 2021.

[12] M. Meraj Ahmed, A. Dhavlle, N. Mansoor, P. Sutradhar, S. M. Pudukotai Dinakarrao, K. Basu, and A. Ganguly,
“Defense against on-chip trojans enabling trafﬁc analysis attacks,” in 2020 Asian Hardware Oriented Security
and Trust Symposium (AsianHOST), 2020, pp. 1–6.

[13] G. Kolhe, S. M. PD, S. Rafatirad, H. Mahmoodi, A. Sasan, and H. Homayoun, “On custom lut-based obfuscation,”

in Proceedings of the 2019 on Great Lakes Symposium on VLSI, 2019.

[14] G. Kolhe, H. M. Kamali, M. Naicker, T. D. Sheaves, H. Mahmoodi, P. D. Sai Manoj, H. Homayoun, S. Rafatirad,
and A. Sasan, “Security and complexity analysis of lut-based obfuscation: From blueprint to reality,” in 2019
IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 2019.

[15] R. Hassan, G. Kolhe, S. Rafatirad, H. Homayoun, and S. M. Dinakarrao, “Satconda: Sat to sat-hard clause

translator,” in International Symposium on Quality Electronic Design (ISQED), 2020.

[16] G. Kolhe, S. Salehi, T. D. Sheaves, H. Homayoun, S. Rafatirad, M. P. D. Sai, and A. Sasan, “Securing hardware
via dynamic obfuscation utilizing reconﬁgurable interconnect and logic blocks,” in 2021 58th ACM/IEEE Design
Automation Conference (DAC), 2021, pp. 229–234.

23

[17] G. Kolhe, T. D. Sheaves, K. I. Gubbi, T. Kadale, S. Rafatirad, S. M. P. Dinakarrao, A. Sasan, H. Mahmoodi,
and H. Homayoun, “Silicon validation of LUT-based logic-locked IP cores,” in 2022 59th ACM/IEEE Design
Automation Conference (DAC), 2022.

[18] G. Kolhe, S. Salehi, T. D. Sheaves, H. Homayoun, S. Rafatirad, S. M. P. Dinakarrao, and A. Sasan, “LOCK
& ROLL: Deep-Learning Power Side-Channel Attack Mitigation using Emerging Reconﬁgurable Devices and
Logic Locking,” in 2022 59th ACM/IEEE Design Automation Conference (DAC), 2022.

[19] G. Kolhe, T. D. Sheaves, S. M. P. D, H. Mahmoodi, S. Rafatirad, A. Sasan, and H. Homayoun, “Breaking the
design and security trade-off of look-up table-based obfuscation,” ACM Trans. Des. Autom. Electron. Syst., jan
2022. [Online]. Available: https://doi.org/10.1145/3510421

[20] Z. Chen, L. Zhang, G. Kolhe, H. M. Kamali, S. Rafatirad, S. M. Pudukotai Dinakarrao, H. Homayoun,
C.-T. Lu, and L. Zhao, “Deep graph learning for circuit deobfuscation,” May 2021. [Online]. Available:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8184091/

[21] Z. Chen, G. Kolhe, S. Rafatirad, C.-T. Lu, S. Manoj P.D., H. Homayoun, and L. Zhao, “Estimating the circuit
de-obfuscation runtime based on graph deep learning,” in 2020 Design, Automation Test in Europe Conference
Exhibition (DATE), 2020, pp. 358–363.

[22] R. Hassan, G. Kolhe, S. Rafatirad, H. Homayoun, and S. M. P. Dinakarrao, “A neural network-based cognitive
obfuscation towards enhanced logic locking,” IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems, pp. 1–1, 2021.

[23] R. Hassan, G. Kolhe, S. Rafatirad, H. Homayoun, and S. M. Dinakarrao, “SATConda: SAT to SAT-Hard Clause
Translator,” in 2020 21st International Symposium on Quality Electronic Design (ISQED), 2020, pp. 155–160.
[24] V. V. Menon, G. Kolhe, A. Schmidt, J. Monson, M. French, Y. Hu, P. A. Beerel, and P. Nuzzo, “System-
level framework for logic obfuscation with quantiﬁed metrics for evaluation,” in 2019 IEEE Cybersecurity
Development (SecDev), 2019, pp. 89–100.

[25] V. Menon, G. Kolhe, A. Schmidt, J. Monson, M. French, Y. Hu, P. A. Beerel, and P. Nuzzo, “Quantifying security

and overheads for obfuscation of integrated circuits,” in GOMACTech Conference, 2019.

[26] V. Menon, G. Kolhe, J. Fifty, A. Schmidt, J. Monson, M. French, Y. Hu, P. A. Beerel, and P. Nuzzo, “Logic

obfuscation: Modeling attack resiliency,” in GOMACTech Conference, 2020.

[27] G. Jacob, H. Debar, and E. Filiol, “Behavioral detection of malware: From a survey towards an established

taxonomy,” Journal in Computer Virology, vol. 4, pp. 251–266, 08 2008.

[28] N. Patel, A. Sasan, and H. Homayoun, “Analyzing hardware based malware detectors,” in 2017 54th

ACM/EDAC/IEEE Design Automation Conference (DAC), 2017, pp. 1–6.

[29] Q. Chen and R. A. Bridges, “Automated behavioral analysis of malware: A case study of wannacry ransomware,”
in 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), 2017, pp.
454–460.

[30] A. Nazari, N. Sehatbakhsh, M. Alam, A. Zajic, and M. Prvulovic, “Eddie: Em-based detection of deviations
in program execution,” in 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture
(ISCA), 2017, pp. 333–346.

[31] J. Stilgoe, “Machine learning, social learning and the governance of self-driving cars,” Social Studies of Science,

11 2017.

[32] S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu, “A survey of deep learning techniques for autonomous

driving,” Journal of Field Robotics, 11 2019.

[33] S. Shukla and et al., “Work-in-progress: Microarchitectural events and image processing-based hybrid approach

for robust malware detection,” in CASES, 2019.

[34] S. Shukla, P. D. Sai Manoj, G. Kolhe, and S. Rafatirad, “On-device malware detection using performance-aware
and robust collaborative learning,” in 2021 58th ACM/IEEE Design Automation Conference (DAC), 2021.
[35] S. Kasarapu, S. Shukla, R. Hassan, A. Sasan, H. Homayoun, and S. M. PD, “Cad-fsl: Code-aware data generation
based few-shot learning for efﬁcient malware detection,” in Proceedings of the Great Lakes Symposium on VLSI
2022, ser. GLSVLSI ’22. New York, NY, USA: Association for Computing Machinery, 2022.

[36] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, “Intriguing properties

of neural networks,” arXiv preprint arXiv:1312.6199, 2013.

[37] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” arXiv preprint

arXiv:1412.6572, 2014.

24

[38] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, “The limitations of deep learning in
IEEE, 2016, pp.

adversarial settings,” in 2016 IEEE European symposium on security and privacy (EuroS&P).
372–387.

[39] Y. LeCun and C. Cortes, “MNIST handwritten digit database,” 2010.

[Online]. Available:

http:

//yann.lecun.com/exdb/mnist/

[40] I. Yoo, “Visualizing windows executable viruses using self-organizing maps,” in Proceedings of the 2004 ACM

Workshop on Visualization and Data Mining for Computer Security, ser. VizSEC/DMSEC ’04, 2004.

[41] D. A. Quist and L. M. Liebrock, “Visualizing compiled executables for malware analysis,” in 2009 6th Interna-

tional Workshop on Visualization for Cyber Security, 2009.

[42] P. Trinius, T. Holz, J. Göbel, and F. C. Freiling, “Visual analysis of malware behavior using treemaps and thread

graphs,” in 2009 6th International Workshop on Visualization for Cyber Security, 2009.

[43] J. R. Goodall, H. Radwan, and L. Halseth, “Visual analysis of code security,” in Proceedings of the Seventh

International Symposium on Visualization for Cyber Security, ser. VizSec ’10, 2010.

[44] G. Conti, S. Bratus, A. Shubina, A. Lichtenberg, B. Sangster, and M. Supan, “A visual study of primitive binary

fragment types,” 2010.

[45] G. Conti, S. Bratus, A. Shubina, B. Sangster, R. Ragsdale, M. Supan, A. Lichtenberg, and R. Perez-Alemany,
“Automated mapping of large binary objects using primitive fragment type classiﬁcation,” Digit. Investig., vol. 7,
Aug. 2010.

[46] M. E. Karim, A. Walenstein, A. Lakhotia, and L. Parida, “Malware phylogeny generation using permutations of

code,” JOURNAL IN COMPUTER VIROLOGY, vol. 1, pp. 13–23, 2005.

[47] J. Z. Kolter and M. A. Maloof, “Learning to detect and classify malicious executables in the wild,” J. Mach.

Learn. Res., vol. 7, Dec. 2006.

[48] D. Gao, M. K. Reiter, and D. Song, “Binhunt: Automatically ﬁnding semantic differences in binary programs,”
in Proceedings of the 10th International Conference on Information and Communications Security, ser. ICICS
’08, 2008.

[49] R. Tian, L. M. Batten, and S. C. Versteeg, “Function length as a tool for malware classiﬁcation,” in 2008 3rd

International Conference on Malicious and Unwanted Software (MALWARE), Oct 2008.

[50] R. Tian, L. Batten, R. Islam, and S. Versteeg, “An automated classiﬁcation system based on the strings of trojan
and virus families,” in 2009 4th International Conference on Malicious and Unwanted Software (MALWARE),
2009.

[51] R. Islam, R. Tian, L. Batten, and S. Versteeg, “Classiﬁcation of malware based on string and function feature

selection,” in 2010 Second Cybercrime and Trustworthy Computing Workshop, 2010.

[52] M. Gheorghescu, “An automated virus classiﬁcation system,” 2006.

[53] Y. Park, D. Reeves, V. Mulukutla, and B. Sundaravel, “Fast malware classiﬁcation by automated behavioral
graph matching,” in Proceedings of the Sixth Annual Workshop on Cyber Security and Information Intelligence
Research, ser. CSIIRW ’10, 2010.

[54] M. Bailey, J. Oberheide, J. Andersen, Z. M. Mao, F. Jahanian, and J. Nazario, “Automated classiﬁcation and
analysis of internet malware,” in Proceedings of the 10th International Conference on Recent Advances in
Intrusion Detection, ser. RAID’07, 2007.

[55] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda, “Scalable, behavior-based malware

clustering.”

[56] K. Rieck, T. Holz, C. Willems, P. Düssel, and P. Laskov, “Learning and classiﬁcation of malware behavior,” in
Proceedings of the 5th International Conference on Detection of Intrusions and Malware, and Vulnerability
Assessment, ser. DIMVA ’08, 2008.

[57] S. M. P. Dinakarrao, S. Amberkar, S. Bhat, A. Dhavlle, H. Sayadi, A. Sasan, H. Homayoun, and S. Rafatirad,
“Adversarial attack on microarchitectural events based malware detectors,” in Proceedings of the 56th Annual
Design Automation Conference 2019, 2019.

[58] F. Brasser, L. Davi, A. Dhavlle, T. Frassetto, S. M. P. Dinakarrao, S. Rafatirad, A.-R. Sadeghi, A. Sasan,
H. Sayadi, S. Zeitouni, and H. Homayoun, “Advances and throwbacks in hardware-assisted security: Special
session,” in Proceedings of the International Conference on Compilers, Architecture and Synthesis for Embedded
Systems, 2018.

25

[59] A. Dhavlle, R. Mehta, S. Rafatirad, H. Homayoun, and S. M. Pudukotai Dinakarrao, “Entropy-shield:side-channel
entropy maximization for timing-based side-channel attacks,” in 2020 21st International Symposium on Quality
Electronic Design (ISQED), 2020, pp. 161–166.

[60] A. Dhavlle, R. Hassan, M. Mittapalli, and S. M. P. Dinakarrao, “Design of hardware trojans and its impact on cps
systems: A comprehensive survey,” in International Symposium on Circuits and Systems (ISCAS), 2021, pp. 1–5.

[61] A. Dhavlle, S. Rafatirad, K. Khasawneh, H. Homayoun, and S. M. P. Dinakarrao, “Imitating functional operations
for mitigating side-channel leakage,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems, vol. 41, no. 4, pp. 868–881, 2022.

[62] A. Dhavlle, S. Shukla, S. Rafatirad, H. Homayoun, and S. M. Pudukotai Dinakarrao, “Hmd-hardener: Adversari-
ally robust and efﬁcient hardware-assisted runtime malware detection,” in Design, Automation & Test in Europe
Conference & Exhibition (DATE), 2021, pp. 1769–1774.

[63] M. Meraj Ahmed, A. Dhavlle, N. Mansoor, P. Sutradhar, S. M. Pudukotai Dinakarrao, K. Basu, and A. Ganguly,
“Defense against on-chip trojans enabling trafﬁc analysis attacks,” in Asian Hardware Oriented Security and
Trust Symposium (AsianHOST), 2020, pp. 1–6.

[64] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten, “The weka data mining
software: An update,” SIGKDD Explor. Newsl., vol. 11, no. 1, pp. 10–18, Nov. 2009. [Online]. Available:
http://doi.acm.org/10.1145/1656274.1656278

[65] https://towardsdatascience.com/introduction-to-naive-bayes-classiﬁcation-4cffabb1ae54, last accessed: 14th

May 2019.

[66] https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd, last accessed: 14th May 2019.
[67] https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html, last accessed: 14th May 2019.

[68] M. A. Musen, “The protÉgÉ project: A look back and a look forward,” AI Matters.

[69] H. Sayadi, S. M. P. D, A. Houmansadr, S. Rafatirad, and H. Homayoun, “Comprehensive assessment of run-time
hardware-supported malware detection using general and ensemble learning,” in Proceedings of the 15th ACM
International Conference on Computing Frontiers, 2018.

[70] N. Patel, A. Sasan, and H. Homayoun, “Analyzing hardware based malware detectors,” in ACM/EDAA/IEEE

Design Automation Conference, 2017.

[71] J. Demme, M. Maycock, J. Schmitz, A. Tang, A. Waksman, S. Sethumadhavan, and S. Stolfo, “On the feasibility
of online malware detection with performance counters,” SIGARCH Comput. Archit. News, vol. 41, no. 3, pp.
559–570, Jun 2013.

[72] S. Shukla, G. Kolhe, S. M. P. D, and S. Rafatirad, “On-device malware detection using performance-aware and

robust collaborative learning,” in Design Automation Conference (DAC), 2021.

[73] S. Baljit and et al., “On the detection of kernel-level rootkits using hardware performance counters,” in ACM

AsiaCCS’17, 2017.

[74] M. B. Bahador, M. Abadi, and A. Tajoddin, “HPCMalHunter: Behavioral malware detection using hardware
performance counters and singular value decomposition,” in Int. Conf. on Computer and Knowledge Engineering,
2014.

[75] G. Jacob, H. Debar, and E. Filiol, “Behavioral detection of malware: from a survey towards an established

taxonomy,” Journal in Computer Virology, vol. 4, no. 3, pp. 251–266, Aug 2008.

[76] B. Nelson, M. Barreno, F. J. Chi, A. Joseph, B. I. P. Rubinstein, U. Saini, C. Sutton, J. Tygar, and K. Xia,

“Exploiting machine learning to subvert your spam ﬁlter,” in LEET, 2008.

[77] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. D. Tygar, “Antidote:
understanding and defending against poisoning of anomaly detectors,” in Proceedings of the 9th ACM SIGCOMM
Conference on Internet Measurement, 2009, pp. 1–14.

[78] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector machines,” arXiv preprint

arXiv:1206.6389, 2012.

[79] L. Muñoz-González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee, E. C. Lupu, and F. Roli, “Towards
poisoning of deep learning algorithms with back-gradient optimization,” in Proceedings of the 10th ACM
Workshop on Artiﬁcial Intelligence and Security, 2017, pp. 27–38.

[80] S. Barve, S. Shukla, S. M. P. Dinakarrao, and R. Jha, Adversarial Attack Mitigation Approaches Using RRAM-

Neuromorphic Architectures. New York, NY, USA: Association for Computing Machinery, 2021.

26

[81] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation as a defense to adversarial perturbations
against deep neural networks,” 2016 IEEE Symposium on Security and Privacy (SP), pp. 582–597, 2016.
[82] D. Meng and H. Chen, “Magnet: a two-pronged defense against adversarial examples,” in Proceedings of the

2017 ACM SIGSAC conference on computer and communications security, 2017, pp. 135–147.

[83] A. Dhavlle and S. M. Pudukotai Dinakarrao, “A comprehensive review of ml-based time-series and signal
processing techniques and their hardware implementations,” in International Green and Sustainable Computing
Workshops (IGSC), 2020, pp. 1–8.

[84] S. Bavikadi, A. Dhavlle, A. Ganguly, A. Haridass, H. Hendy, C. Merkel, V. J. Reddi, P. R. Sutradhar, A. Joseph,
and S. M. Pudukotai Dinakarrao, “A survey on machine learning accelerators and evolutionary hardware
platforms,” IEEE Design & Test, vol. 39, no. 3, pp. 91–116, 2022.

[85] A. Dhavlle, S. Rafatirad, H. Homayoun, and S. M. P. Dinakarrao, “Cr-spectre: Defense-aware rop injected
code-reuse based dynamic spectre,” in Design, Automation & Test in Europe Conference & Exhibition (DATE),
2022, pp. 508–513.

[86] M. M. Ahmed, A. Dhavlle, N. Mansoor, S. M. P. Dinakarrao, K. Basu, and A. Ganguly, “What can a remote
access hardware trojan do to a network-on-chip?” in International Symposium on Circuits and Systems (ISCAS),
2021, pp. 1–5.

[87] A. Dhavlle and S. Shukla, “A novel malware detection mechanism based on features extracted from converted

malware binary images,” CoRR, vol. abs/2104.06652, 2021.

[88] “Table of contents,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 41,

no. 4, pp. C1–C4, 2022.

[89] A. Bhattacharjee and P. Panda, “Rethinking non-idealities in memristive crossbars for adversarial robustness in

neural networks,” arXiv preprint arXiv:2008.11298, 2020.

[90] E. Kim, J. Yarnall, P. Shah, and G. T. Kenyon, “A neuromorphic sparse coding defense to adversarial images,” in

Proceedings of the International Conference on Neuromorphic Systems, 2019, pp. 1–8.

[91] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the physical world,” ArXiv, vol.

abs/1607.02533, 2017.

[92] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, “Boosting adversarial attacks with momentum,” 2018

IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9185–9193, 2018.

[93] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: A simple and accurate method to fool deep
neural networks,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2574–2582,
2016.

[94] U. Shaham, Y. Yamada, and S. Negahban, “Understanding adversarial training: Increasing local stability of

neural nets through robust optimization,” arXiv preprint arXiv:1511.05432, 2015.

[95] N. Carlini and D. A. Wagner, “Towards evaluating the robustness of neural networks,” 2017 IEEE Symposium on

Security and Privacy (SP), pp. 39–57, 2017.

[96] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” arXiv preprint

arXiv:1503.02531, 2015.

[97] K. Grosse, P. Manoharan, N. Papernot, M. Backes, and P. McDaniel, “On the (statistical) detection of adversarial

examples,” ArXiv, vol. abs/1702.06280, 2017.

[98] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff, “On detecting adversarial perturbations,” ArXiv, vol.

abs/1702.04267, 2017.

[99] T. Bailey, A. Ford, S. Barve, J. Wells, and R. Jha, “Development of a short-term to long-term supervised
spiking neural network processor,” IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. doi:
10.1109/TVLSI.2020.3013810.

[100] A. Jones and R. Jha, “A compact gated-synapse model for neuromorphic circuits,” IEEE Transactions on

Computer-Aided Design of Integrated Circuits and Systems, vol. 10.1109/TCAD.2020.3028534.

[101] E. Herrmann, A. Rush, T. Bailey, and R. Jha, “Gate controlled three-terminal metal oxide memristor,” IEEE

Electron Device Letters, vol. vol. 39, no. 4, pp. 500-503, April 2018.

[102] N. Papernot, F. Faghri, N. Carlini, I. Goodfellow, R. Feinman, A. Kurakin, C. Xie, Y. Sharma, T. Brown, A. Roy,
A. Matyasko, V. Behzadan, K. Hambardzumyan, Z. Zhang, Y.-L. Juang, Z. Li, R. Sheatsley, A. Garg, J. Uesato,
W. Gierke, Y. Dong, D. Berthelot, P. Hendricks, J. Rauber, R. Long, and P. McDaniel, “Technical report on the
cleverhans v2.1.0 adversarial examples library,” arXiv: Learning, 2016.

27

