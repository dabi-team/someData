2
2
0
2

r
p
A
8

]
E
C
.
s
c
[

1
v
7
1
1
5
0
.
4
0
2
2
:
v
i
X
r
a

RESERVOIRCOMPUTING.JL: AN EFFICIENT AND MODULAR
LIBRARY FOR RESERVOIR COMPUTING MODELS

A PREPRINT

Francesco Martinuzzi

Remote Sensing Centre for Earth System Research
Center for Scalable Data Analytics and Artiﬁcial Intelligence
Julia Computing Inc.
francesco.martinuzzi@uni-leipzig.de

Chris Rackauckas
Massachusetts Institute of Technology
Julia Computing Inc.
crackauc@mit.edu

Anas Abdelrehim
Julia Computing Inc.
anasabdel.rehim@juliacomputing.com

Miguel D. Mahecha
Remote Sensing Centre for Earth System Research
miguel.mahecha@uni-leipzig.de

Karin Mora
Remote Sensing Centre for Earth System Research
German Centre for Integrative Biodiversity Research (iDiv)
karin.mora@uni-leipzig.de

ABSTRACT

We introduce ReservoirComputing.jl, an open source Julia library for reservoir computing models.
The software offers a great number of algorithms presented in the literature, and allows to expand on
them with both internal and external tools in a simple way. The implementation is highly modular,
fast and comes with a comprehensive documentation, which includes reproduced experiments from
literature. The code and documentation are hosted on Github under an MIT license https://
github.com/SciML/ReservoirComputing.jl.

Keywords reservoir computing, echo state networks, julia, machine learning

1

Introduction

Time series modeling is a very common technique throughout many areas of machine learning. However, many standard
recurrent models are known to be susceptible to problems such as the vanishing gradient [Pascanu et al., 2013] or the
extreme sensitivity of chaotic systems to their parameterization [Wiggins et al., 2003]. To counter these issues reservoir
computing (RC) techniques were introduced as recurrent models which can be trained without requiring gradient-based
approaches [Lukoševiˇcius and Jaeger, 2009]. Independently proposed as echo state networks (ESNs) [Jaeger, 2001]
and liquid state machines (LSMs) [Maass et al., 2002], these architectures are based on the expansion of the input data
using a ﬁxed random internal layer, known as the reservoir, and the subsequent mapping of the reservoir to match an
output. This output mapping can normally be written in a simple closed form, such as an L2 minimization computed
via a single QR-factorization. This allows the RC approach to achieve faster computational times with less parameter
tuning compared to deep learning models which rely on local optimization. Numerous alterations have been made
since their inception, such as deep architectures [Gallicchio et al., 2018] and different training approaches Chatzis
and Demiris [2011], Shi and Han [2007]. Applications have conﬁrmed that these improved architectures and training
techniques can be impactful in many ﬁelds of study: from surrogates for stiff systems [Anantharaman et al., 2020] to
reconstruction of chaotic attractors [Pathak et al., 2018a] and prediction of climate dynamics [Nadiga, 2021]. This
expanding landscape necessitates the development of production-quality software to empower scientists with all of
the latest techniques without having to implement from scratch every novel variation on their own. While a number

 
 
 
 
 
 
ReservoirComputing.jl

A PREPRINT

Figure 1: ReservoirComputing.jl architecture. SV stands for support vector.

of libraries for ESNs and RC are available [Trouvain et al., 2020, Steiner et al., 2021, Pontes-Filho et al., 2020], they
lack the hackability necessary to alter the provided architectures and achieve the top performances from the literature.
To provide a truly modular library for RC models we present ReservoirComputing.jl, an efﬁcient and ﬂexible library
written using the Julia language [Bezanson et al., 2017]. This software provides a user oriented package with intuitive
high level application programming interfaces (APIs), while maintaining a great level of customization. This ﬂexibility
is obtained from both the design choices of ReservoirComputing.jl and by leveraging the multiple dispatch architecture
of Julia. This gives users the freedom to mix any Julia code with the reservoir computers in a seamless way to promote
the latest performance and research.

2 Theoretical Background

The setup of a RC model [Konkoli, 2018] is based on leveraging the complex dynamics of a nonlinear system, the
reservoir R. The role of this layer is to expand the input data u(t) at time t into a higher dimension. This operation
can be aided by an input layer χ, which maps the data onto the reservoir. The input data u(t) drives the reservoir R
dynamics to a certain conﬁguration. The resulting state x(t) represents the expansion of the input data and is used for
training. Once the states have been obtained for all the training length T the RC model can be trained. The states x(t)
can be manipulated at this stage: through concatenation with the input data u(t) such as z(t) = [x(t), u(t)] [Jaeger,
2007], using padding with some constant value c usually set c = 1.0 such as z(t) = [c, u(t)] [Lukoševiˇcius, 2012], or
through nonlinear transformations [Chattopadhyay et al., 2020]. The training is performed in one step in order to ﬁnd
the best ﬁt between the states x(t) and the desired output y(t)g. The most common way to perform the training is linear
regression, creating the readout layer ψ. This layer is ﬁnally used in the prediction phase to obtain the desired output
v(t) = ψ(x(t)).

3 Library Overview

ReservoirComputing.jl features the implementation of different RC models, training methods, and state variations. The
architecture with the main components are detailed below and illustrated in Figure 1. Given the modular nature of the
software more algorithms can be easily implemented on top of the existing ones. Any of the many libraries of the Julia
community or custom codes can be used as part of the functions within the reservoir computer. Importantly, as Julia is a
just-in-time (JIT) compiled language, these extensions do not sacriﬁce performance.

Building models. ReservoirComputing.jl provides a simple interface for model building that closely follows the
workﬂow presented in the corresponding literature. It includes a standard implementation of ESNs [Lukoševiˇcius, 2012]
as well as a hybrid variation [Pathak et al., 2018b], gated recurrent unit ESN [Wang et al., 2020] and double activation
function ESN [Lun et al., 2015]. Multiple input layers χ and reservoirs R are also provided, ranging from weighted
input layers [Lu et al., 2017] to minimally complex input layers and reservoirs [Rodan and Tino, 2010, Rodan and
Tiˇno, 2012], including a reservoir obtained through pseudo single value decomposition [Yang et al., 2018]. Reservoir
computing with cellular automata (ReCA) [Yilmaz, 2014, Nichele and Molund, 2017] is another family of models
available in the library leveraging the package CellularAutomata.jl [Martinuzzi, 2022].

2

ReservoirComputing.jl

A PREPRINT

Figure 2: Speed comparison of RC libraries.

Training. The training algorithms are implemented in a low level fashion, supporting all RC models of the library.
Multiple training methods to obtain the output layer ψ can be obtained from open source libraries such as MLJLinear-
Models.jl [Blaom and Vollmer, 2020], GaussianProcesses.jl [Fairbrother et al., 2021] and LIBSVM.jl [Kornblith and
Pastell, 2021], a Julia porting of LIBSVM [Chang and Lin, 2011].

Prediction. ReservoirComputing.jl leverages diverse prediction techniques. The generative approach allows the
model to run autonomously: the predicted output v(t) is fed back into the model to obtain the prediction for the next
time step v(t + 1). The predictive approach instead uses the standard feature-label setup.

States modiﬁers. Other lower level implementations are included, such as the possibility to modify the state vectors.
All the approaches detailed in §2 are included in the library.

4 Code Quality

The library is made available on Github, where it is continuously tested through Github actions. The package is part
of the scientiﬁc machine learning (SciML) community [sci], a NumFOCUS sponsored project since 2020 [num]. An
extensive online documentation is also provided with the library, where models are documented at length with multiple
examples for different applications.

A comparison of features and performance of ReservoirComputing.jl with similar libraries is provided in Table 1 and in
Figure 2, respectively. The feature comparison in Table 1 showcases the large offering of models available. The plot in
Figure 2 illustrates the better computational speed of the library. The performance test task is a next step prediction
of the Mackey-Glass system [Glass and Mackey, 2010] with time delay τ = 17. The dense reservoir matrix and the
dense input matrix are generated with uniform distribution sampled from [−1, 1]. The spectral radius of the reservoir
matrix is scaled by 1.25. The ridge regression parameter is set to 10−8. Training and prediction lengths are both equal
to 4999. The time reported is the sum of training and prediction. For central processing unit (CPU) computations the
precision is set to ﬂoat64, for the graphics processing unit (GPU) computations it is ﬂoat32. EchoTorch is not present in
the comparison as the version available on Github was not running on the examples provided. The versions tested are as
follows: PyRCN v0.0.16, ReservoirPy v0.3.2, ReservoirComputing.jl v0.8 release candidate and pytorch-esn retrieved
from Github on March 2022. All the simulations were run on a Dell XPS 9510 ﬁtted with an Intel i9-11900H CPU, a
Nvidia GeForce RTX 3050 Ti GPU and 16 GB of RAM.

5 Conclusion and Outlook

ReservoirComputing.jl is a comprehensive, modular and fast library for RC models with a strong focus on ESNs. In
the future we plan to expand the model library by including LSMs and extreme learning machines (ELMs), while
maintaining the intuitiveness of the current implementation.

3

ReservoirComputing.jl

A PREPRINT

Code quality

Models: ESN

Training

Output Miscellanea

e
g
a
u
g
n
a
L

n
o
i
t
a
t
n
e
m
u
c
o
D

s
l
a
i
r
o
t
u
T
/
I
P
A

s
t
s
e
T

n
o
i
t
a
r
e
l
e
c
c
a
U
P
G

s
n
o
r
u
e
n

y
k
a
e
L

ReservoirComputing.jl
ReservoirPy
EchoTorch
pytorch-esn
PyRCN

(cid:51) (cid:51)/(cid:51) (cid:51) (cid:51) (cid:51)
Julia
Python (cid:51) (cid:51)/(cid:51) (cid:51) (cid:55) (cid:51)
(cid:55) (cid:51)/(cid:55) (cid:51) (cid:51) (cid:51)
Python
(cid:55) (cid:51) (cid:51)
(cid:55) (cid:51)/(cid:55)
Python
Python (cid:51) (cid:51)/(cid:51) (cid:51) (cid:55) (cid:51)

]
0
2
0
2

,
.
l
a

t
e

i
l
r
a
S
i

s
t
i
n
u

d
e
t
a
G

D

[

(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)

N
S
E
d
i
r
b
y
H

]
b
8
1
0
2

,
.
l
a

t
e

k
a
h
t
a
P
[

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

s
r
i
o
v
r
e
s
e
r

e
l
p
i
t
l
u
M

(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:51)

]
8
1
0
2

,
.
l
a

t
e

o
i
h
c
c
i
l
l
a
G

e
r
u
t
c
e
t
i
h
c
r
a
p
e
e
D

s
l
e
d
o
m
N
S
E
-
n
o
N

n
o
i
s
s
e
r
g
e
r

r
a
e
n
i
L

n
o
i
s
s
e
r
g
e
r

n
a
i
s
s
u
a
G

n
o
i
s
s
e
r
g
e
r

V
S

g
n
i
n
i
a
r
t

e
n
i
l
n
O

e
v
i
t
a
r
e
n
e
G

e
v
i
t
c
i
d
e
r
P

s
n
o
i
t
a
c
ﬁ
i
d
o
m
s
e
t
a
t
S

s
t
e
s

a
t
a
D

n
o
i
t
a
z
i
m

i
t
p
O

[

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:51) (cid:51) (cid:51) (cid:55)
(cid:51) (cid:51) (cid:55)
(cid:55) (cid:51) (cid:55)
(cid:55) (cid:51) (cid:55)
(cid:51) (cid:51) (cid:55)

(cid:55)
(cid:55) (cid:51) (cid:51) (cid:51) (cid:55) (cid:51) (cid:51)
(cid:55) (cid:51) (cid:55) (cid:51) (cid:51)
(cid:55)
(cid:55) (cid:51) (cid:55) (cid:51) (cid:55)
(cid:55)
(cid:55) (cid:51) (cid:55) (cid:51) (cid:55) (cid:51) (cid:51)

(cid:55)

(cid:55)

Table 1: Comparison of RC libraries. The code quality section focuses on high level, quality of life library features. The
subsections: Documentation indicates whether the library includes general examples or how-to guides; API/Tutorials
indicates whether API documentation and step by step tutorials are provided. In the ESN section we list common
models included in the libraries. Subsection Multiple reservoirs indicates whether native multiple reservoir matrix
constructions are provided. In the training section, SV is short hand for support vector regression. In the Miscellanea
section Data sets indicates whether ready to use data sets are provided in the library; Optimization stands for native
hyperparameter optimization.

Acknowledgements

This work was partially supported by the German Federal Ministry of Education and Research (BMBF, 1IS18026B) by
funding the competence center for Big Data and AI “ScaDS.AI” Dresden/Leipzig. K.M. is funded by the iDiv Flexpool
program. M.D.M. and K.M. thank the European Space Agency for funding the "DeepExtremes" project (AI4Science
ITT).

References

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural networks. In

International conference on machine learning, pages 1310–1318. PMLR, 2013.

Stephen Wiggins, Stephen Wiggins, and Martin Golubitsky. Introduction to applied nonlinear dynamical systems and

chaos, volume 2. Springer, 2003.

Mantas Lukoševiˇcius and Herbert Jaeger. Reservoir computing approaches to recurrent neural network training.

Computer Science Review, 3(3):127–149, 2009.

Herbert Jaeger. The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.
Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148(34):13,
2001.

Wolfgang Maass, Thomas Natschläger, and Henry Markram. Real-time computing without stable states: A new

framework for neural computation based on perturbations. Neural computation, 14(11):2531–2560, 2002.

Claudio Gallicchio, Alessio Micheli, and Luca Pedrelli. Design of deep echo state networks. Neural Networks, 108:

33–47, 2018.

Sotirios P Chatzis and Yiannis Demiris. Echo state gaussian process. IEEE Transactions on Neural Networks, 22(9):

1435–1445, 2011.

Zhiwei Shi and Min Han. Support vector echo-state machine for chaotic time-series prediction. IEEE transactions on

neural networks, 18(2):359–372, 2007.

Ranjan Anantharaman, Yingbo Ma, Shashi Gowda, Chris Laughman, Viral Shah, Alan Edelman, and Chris Rackauckas.
Accelerating simulation of stiff nonlinear systems using continuous-time echo state networks. arXiv preprint
arXiv:2010.04004, 2020.

Jaideep Pathak, Brian Hunt, Michelle Girvan, Zhixin Lu, and Edward Ott. Model-free prediction of large spatiotem-
porally chaotic systems from data: A reservoir computing approach. Physical review letters, 120(2):024102,
2018a.

4

ReservoirComputing.jl

A PREPRINT

Balasubramanya T Nadiga. Reservoir computing as a tool for climate predictability studies. Journal of Advances in

Modeling Earth Systems, 13(4):e2020MS002290, 2021.

Nathan Trouvain, Luca Pedrelli, Thanh Trung Dinh, and Xavier Hinaut. Reservoirpy: an efﬁcient and user-friendly
library to design echo state networks. In International Conference on Artiﬁcial Neural Networks, pages 494–505.
Springer, 2020.

Peter Steiner, Azarakhsh Jalalvand, Simon Stone, and Peter Birkholz. Pyrcn: Exploration and application of esns. arXiv

preprint arXiv:2103.04807, 2021.

Sidney Pontes-Filho, Pedro Lind, Anis Yazidi, Jianhua Zhang, Hugo Hammer, Gustavo BM Mello, Ioanna Sandvig,
Gunnar Tufte, and Stefano Nichele. A neuro-inspired general framework for the evolution of stochastic dynamical
systems: Cellular automata, random boolean networks and echo state networks towards criticality. Cognitive
Neurodynamics, pages 1–18, 2020.

Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing.

SIAM review, 59(1):65–98, 2017.

Zoran Konkoli. Reservoir Computing, pages 619–629. Springer US, New York, NY, 2018. ISBN 978-1-4939-6883-1.

doi:10.1007/978-1-4939-6883-1_683. URL https://doi.org/10.1007/978-1-4939-6883-1_683.

H. Jaeger. Echo state network. Scholarpedia, 2(9):2330, 2007. doi:10.4249/scholarpedia.2330. revision #196567.
Mantas Lukoševiˇcius. A practical guide to applying echo state networks. In Neural networks: Tricks of the trade, pages

659–686. Springer, 2012.

Ashesh Chattopadhyay, Pedram Hassanzadeh, and Devika Subramanian. Data-driven predictions of a multiscale
lorenz 96 chaotic system using machine-learning methods: reservoir computing, artiﬁcial neural network, and long
short-term memory network. Nonlinear Processes in Geophysics, 27(3):373–389, 2020.

Jaideep Pathak, Alexander Wikner, Rebeckah Fussell, Sarthak Chandra, Brian R Hunt, Michelle Girvan, and Edward
Ott. Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model.
Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(4):041101, 2018b.

Xinjie Wang, Yaochu Jin, and Kuangrong Hao. A gated recurrent unit based echo state network. In 2020 International

Joint Conference on Neural Networks (IJCNN), pages 1–7. IEEE, 2020.

Shu-Xian Lun, Xian-Shuang Yao, Hong-Yun Qi, and Hai-Feng Hu. A novel model of leaky integrator echo state

network for time-series prediction. Neurocomputing, 159:58–66, 2015.

Zhixin Lu, Jaideep Pathak, Brian Hunt, Michelle Girvan, Roger Brockett, and Edward Ott. Reservoir observers:
Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear
Science, 27(4):041102, 2017.

Ali Rodan and Peter Tino. Minimum complexity echo state network. IEEE transactions on neural networks, 22(1):

131–144, 2010.

Ali Rodan and Peter Tiˇno. Simple deterministically constructed cycle reservoirs with regular jumps. Neural computation,

24(7):1822–1852, 2012.

Cuili Yang, Junfei Qiao, Honggui Han, and Lei Wang. Design of polynomial echo state networks for time series

prediction. Neurocomputing, 290:148–160, 2018.

Ozgur Yilmaz. Reservoir computing using cellular automata. arXiv preprint arXiv:1410.0162, 2014.

Stefano Nichele and Andreas Molund. Deep reservoir computing using cellular automata.

arXiv preprint

arXiv:1703.02806, 2017.

Francesco Martinuzzi. Martinuzzifrancesco/cellularautomata.jl: v0.0.2, January 2022. URL https://doi.org/10.

5281/zenodo.5879385.

Anthony D. Blaom and Sebastian J. Vollmer. Flexible model composition in machine learning and its implementation

in MLJ, 2020.

Jamie Fairbrother, Christopher Nemeth, Maxime Rischard, Johanni Brea, and Thomas Pinder. Gaussianprocesses. jl: A

nonparametric bayes package for the julia language. Journal of Statistical Software (to appear), 2021.

Simon Kornblith and Matti Pastell. https://github.com/juliaml/libsvm.jl, November 2021. URL https://github.

com/JuliaML/LIBSVM.jl.

Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.edu.tw/
~cjlin/libsvm.

5

ReservoirComputing.jl

A PREPRINT

Sciml website. URL https://sciml.ai/.
Sciml at numfocus. URL https://numfocus.org/project/sciml.
L. Glass and M. Mackey. Mackey-Glass equation. Scholarpedia, 5(3):6908, 2010. doi:10.4249/scholarpedia.6908.

revision #186443.

Daniele Di Sarli, Claudio Gallicchio, and Alessio Micheli. Gated echo state networks: a preliminary study. In 2020
International Conference on INnovations in Intelligent SysTems and Applications (INISTA), pages 1–5. IEEE, 2020.

6

