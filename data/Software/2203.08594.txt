TOWARDS A ROADMAP ON SOFTWARE ENGINEERING FOR
RESPONSIBLE AI

2
2
0
2

r
a

M
9

]
E
S
.
s
c
[

1
v
4
9
5
8
0
.
3
0
2
2
:
v
i
X
r
a

Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing
Data61, CSIRO, Australia
ﬁrstname.lastname@data61.csiro.au

March 17, 2022

ABSTRACT

Although AI is transforming the world, there are serious concerns about its ability to behave and
make decisions responsibly. Many ethical regulations, principles, and frameworks for responsible
AI have been issued recently. However, they are high level and difﬁcult to put into practice. On the
other hand, most AI researchers focus on algorithmic solutions, while the responsible AI challenges
actually crosscut the entire engineering lifecycle and components of AI systems. To close the gap
in operationalizing responsible AI, this paper aims to develop a roadmap on software engineering
for responsible AI. The roadmap focuses on (i) establishing multi-level governance for responsible
AI systems, (ii) setting up the development processes incorporating process-oriented practices for
responsible AI systems, and (iii) building responsible-AI-by-design into AI systems through system-
level architectural style, patterns and techniques.

Key words: AI, machine learning, responsible AI, ethics, software engineering, software architecture, MLOps, DevOps,
requirement engineering

1

Introduction

Artiﬁcial intelligence (AI) is considered one of the major driving forces to transform society and industry and has been
successfully adopted in data-rich domains. Although AI is solving real-world challenges and improving our quality of
life, there are serious concerns about its ability to behave and make decisions in a responsible manner. Responsible AI
has become one of the greatest scientiﬁc challenges of our time. Both legal and ethical aspects need to be considered to
achieve responsible AI. Since the law establishes the minimum standards of behaviour while ethics sets the maximum
standards, in this paper, we use the terms responsible AI, ethical AI and ethics to cover the broader set of requirements.

A large number of ethical principle frameworks have been recently issued by governments, research institutions, and
enterprises [1], which responsible AI technologies and systems are supposed to adhere to. A degree of consensus
around the principles has been achieved [2]. However, these principles are high-level and do not provide operationalized
guidance and software engineering methods on how to develop responsible AI systems. This leaves unanswered
questions such as how can these principles be designed for, implemented and tracked in developing and operating an
AI system. On the other hand, signiﬁcant research has gone into ethical algorithms where the formulation of some
ethical principles is amenable to mathematical deﬁnitions, analysis and theoretical guarantees. These algorithm-level
mechanisms mainly focus on a small subset of ethical principles (such as privacy [3] and fairness [4]) relying on
theoretical heuristics. There is a lack of linkage to the software development processes, especially requirements
engineering, system design methods, or operations.

Therefore, this paper presents a research roadmap on software engineering for operationalizing responsible AI. Rather
than staying at the ethical principle level or going straight down to the AI algorithm level, this paper focuses on the
software engineering approach to operationalizing responsible AI. We perform a systematic literature review (SLR) on
software engineering for responsible AI to summarize the current state and identify the critical research challenges. As
shown in Fig. 1, the proposed roadmap focuses on (i) establishing multi-level governance for responsible AI systems,

 
 
 
 
 
 
A PREPRINT - MARCH 17, 2022

Figure 1: Overview of the roadmap.

(ii) setting up the development processes incorporating process-oriented best practices for responsible AI systems,
and (iii) building responsible-AI-by-design into the AI systems through system-level architectural style, patterns and
techniques.

The remainder of the paper is organized as follows. Sec. 2 discusses the methodology. The rest of the paper is divided
into three parts: governance perspective (Sec. 3), process perspective (Sec. 4), and system perspective (Sec. 5). For
each perspective, we present the current state and the challenges being faced by the community.

2 Methodology

To develop a roadmap, we performed an SLR following the guideline in [5]. Fig. 2 illustrates the methodology. The
two research questions are deﬁned for the SLR: 1) What responsible AI principles are addressed by the study; 2) What
solutions for responsible AI can be identiﬁed. The data sources include ACM Digital Library, IEEE Xplore, Science
Direct, Springer Link, and Google Scholar. The study only includes papers presenting concrete solutions for responsible
AI and exclude papers discussing high-level frameworks. A set of 159 primary studies was identiﬁed. The complete
SLR protocol is available as online material 1. We use the ethical principles listed in Harvard University’s mapping
study [2]. Fig. 3 lists an adapted summary of the principles (responsibility is merged into accountability given the
overlapping deﬁnitions).

Figure 2: Methodology.

2

Software engineering best practicesArchitectural style and patternsSystem-level  techniquesProcessperspectiveSystemperspectiveTrust vs. TrustworthinessIndustry-levelgovernanceGovernanceperspectiveOrganization-levelgovernanceTeam-levelgovernanceACM, IEEE, Science Direct,Springer, Google ScholarRetrieved papers (1904)Tentative primarystudies (246)Identification of the need for SLRResearch questionfinalisationProtocol  developmentQuality assessmentRoadmap developmentKeyword searchScreening based ontitle, abstract, orextended readingForward & backwardsnowballingData extractionSelection of  primary studiesData synthesis &analysis SLR protocolPilot studyRoadmapSeed set papers(32)Primary studies (159)A PREPRINT - MARCH 17, 2022

3 Governance Perspective

3.1 Current State

The governance for responsible AI systems can be deﬁned as the structures and processes that are designed to
ensure the development and use of AI systems are compliant to ethical regulations and responsibilities. As shown in
Fig. 4, the governance can be built into three levels based on Shneiderman’s governance structure [6]: industry-level,
organization-level, and team-level.

3.1.1 Industry-level governance

The industry-level governance requires the governments and AI industry to act collectively via regulation, policy,
standards to make the AI systems acceptable by society. One form of regulation is analogous to building codes to which
developers can adhere when developing AI systems [7]. Incentives/penalties may be applied for ethical/unethical
software development [6]. Given the laws and public policies usually take a long time to enact, governments can consider
adopting agile regulatory sandbox on a time-limited basis for the emerging AI technology. For example, the Singapore
government and EU have applied a regulatory sandbox to allow autonomous vehicles to be legally on the roads without
changing the national laws [8]. As there may be various sector/domain-speciﬁc risk concerns (e.g. military or health),
there is a need to extend and adapt generic regulations to sector/domain-speciﬁc regulations [9]. Professional and
non-governmental organizations and research institutions have been making signiﬁcant efforts on developing standards,
guidelines, and open-source tools and platforms[6]. Regulators could incentivize organizations for responsible
AI innovation. AI project funding bodies could require applicants to include responsible AI statements in their
funding applications and implement ethical checklist driven monitoring and management of grant funding.

Independent oversight is essential to the accountability of AI systems. External audits could be conducted by
independent third parties (such as AI Safety Commission) during the development or post-hoc. The inspection of the AI
systems’ behaviours and decision-making is required either by reviewing interpretable AI models or having access to
the artifacts of AI systems, such as datasets, source code, and documents. For example, Z-inspection process [10] is a
generic inspection processes to assess the trustworthiness of AI systems. When an accident happens, causal analysis
can be performed using the why-because method for accident investigation. Insurance companies can play a role of a
guarantor for responsible AI and compensate for the failures of AI systems [6].

AI capability maturity model is being introduced to examine organizations’ AI capability [11, 12]. Like the
conventional software engineering capability maturity model, the AI capability maturity model has different levels
of maturity based on development processes and desired responsible AI metrics. The results assessed by the AI
capability maturity model could be used for ethical certiﬁcation [13] to certify an organization’s ability to achieve
ethical principles. In addition to organization-level assessment and certiﬁcation, both the AI capability maturity model
and ethical certiﬁcation could be extended to cover a broader view to provide veriﬁable evidence for improving human
trust in AI systems, e.g., assessing and certifying for organizations, development processes, developers, operators, AI
systems, components, models.

1https://drive.google.com/file/d/16fawGwzuuMwFpCAl4mH-MtNcWQRpFNtv/view?usp=sharing

1. Privacy. AI systems should preserve the data privacy.

2. Accountability. Those responsible for the various phases of the AI system lifecycle should be identiﬁable and accountable

for the outcomes of the system.

3. Safety & Security. AI systems should safely and securely operate in accordance with their intended purpose throughout

their lifecycle.

4. Transparency & Explainability. There should be responsible disclosure to ensure people know when they are being

engaged/impacted by an AI system. The behaviors and decisions should be explainable.

5. Fairness and Non-discrimination. AI systems should be inclusive and should not involve or result in unfair discrimination

against individuals, communities or groups.

6. Human Control of Technology. When an AI system impacts a person, community, or environment, there should be a

timely process to allow people to challenge the use or output of the system.

7. Promotion of Human Values. AI systems should respect human rights, diversity, and the autonomy of individuals, and

beneﬁt individuals, society and the environment.

Figure 3: An adapted summary of AI Ethics Principles [2].

3

A PREPRINT - MARCH 17, 2022

Figure 4: Multi-level governance for responsible AI.

3.1.2 Organization-level governance

Achieving responsible AI in an organization requires the establishment of AI governance that includes setting up
founding principles, an ethics committee, a governance structure, governance metrics, external committees,
organization-wide training, and promotion of diversity and dissent [14]. The ethics committee should oversee the
overall AI-driven decision-making processes (not only the AI systems). Code of conduct (such as speciﬁc code of
ethics) should be developed for the employees (e.g. developers or operators) to follow [6].

Leadership commitment is essential to organization-level governance. Responsible AI statements should be de-
scribed explicitly in an organization’s values, vision, and mission [6]. The establishment of responsible AI governance
could be incorporated into CEO’s contracts and performance reviews [8]. The management can enforce the organi-
zational culture on responsible AI and use ceremonies to celebrate responsible AI successes (e.g. certiﬁcate granted).
AI transformation workshops can be organized to assess the impact, e.g. using human-centered AI canvas2.

Internal audit review is required to cover the complete lifecycle of AI systems and include continuous monitoring
instruments (e.g. checklist for retrospective meetings and data/code reviews). Extensive reporting of failures and
near misses should be produced with why-because analysis. Ethical risk assessment checklists [15] need to be
co-designed with practitioners [16] (e.g. for ethics committees, team, or prospective purchasers) for each of the
ethical principles (e.g. fairness risk identiﬁcation questionnaire [17], reproducibility checklists, fairness checklists [15]),
taking into account the application category and automation level for risk and timeliness.

Role-level accountability [18] is established in the organization through formal contractual mechanisms (e.g., legal
agreements between system users, data contributors, and the project team) to hold each other accountable [19]. Diverse
types of ethical quality constraints are enforced as a contract, such as service contract, model contract, and data contract.
The provenance of data, model, and code allows examining role-level accountability.

AI ethics training programs should be introduced within the organizations, including technical and non-technical
ethics and human rights training for different roles (such as the management, developers, data scientists, op-
erators) and organizational awareness. The content of an AI ethics training course could include governance for
responsible AI, ethical operations of AI systems, trustworthy development processes, and responsible-AI-by-
design with case studies (e.g. using the design of an ethical/unethical agent [20], autonomous vehicles [21]).

2https://medium.com/@albmllt/introducing-the-human-centered-ai-canvas-a4c9d2fc127e

4

Industry-levelAgile regulatory sandboxStandards, guidelines, open-source toolsBuilding codesSector-specific regulationsIncentives for innovationIndependent oversightCausal analysisAI capability maturity modelEthical certificationFailure compensationOrganization-levelFounding principlesEthics committeeGovernance structureTrainingPromotion of diversity and dissentCode of conductLeadership commitmentExplicit statement in values, vision, and missionOrganizational cultureInternal audit reviewEthical risk assessment checklistsRole-level accountabilityContractsMetricsExternal committeeTeam-levelConformance to standard processesCustomized agile processes Tight coupling of AI and non-AI development sprints and standupsAutomated process by using toolsModel-based process assessmentCustomized IDEOpen processesArtifact-specific lifecycle management processDiverse teamsDecentralized infrastructure for verifiable credentialsContinuous documentingCeremoniesStakeholder engagementA PREPRINT - MARCH 17, 2022

3.1.3 Team-level governance

Team-level governance is mainly for managing the AI projects and overall development processes. The development
processes should be conformance to standard processes. Agile development processes can be adapted and cus-
tomised by incorporating responsible AI principles [22]. The AI and non-AI development sprints and standups
need to be closely coupled [23]. One effective way to ensure fairness, human-centred values, and HSE wellbeing
in AI systems is to make the development teams diverse (e.g., gender, age, race, culture) and engage stakehold-
ers throughout the lifecycle of AI systems. Culture needs to be explicitly considered in the design when there is
culture-sensitive data or context involved. For example, culture could determine whether it is ethical or unethical for a
conversational AI system to tell a lie to humans (e.g. when negotiating a price) [24]. For indigenous projects, indigenous
people need to be involved in the development process to help incorporate culture concerns into development and
decision-making (e.g., following CARE3). Ethicists can be included into the development team to promote a more
ethical development of AI systems [25]. Conﬂict/trade-off resolution process is needed to achieve consensus during the
development.

The day-to-day workﬂow of software engineers (e.g. training and deployment pipeline [23]) is expected to be uniﬁed
and automated by using the standardized tools (e.g., Jupiter notebooks, python, Github) [26] to improve productivity
and integrate AI ethics tools. Model-based process assessment checks the process compliance by linking workﬂow
models with assessment criteria [27]. Customized visual IDE tools may help developers with varying levels of
experience [23]. The project team can consider publishing the ﬁndings through scientiﬁc publication following the
scientiﬁc norms of research integrity and knowledge sharing to provide public transparency [19]. Open process across
the development lifecycle (e.g. full access to artifacts from stakeholders or authorized third-parties) and Artifact
speciﬁc lifecycle management process are helpful to improve transparency and accountability [28]. Decentralized
computational infrastructure is built for veriﬁable ethical credentials which can be used as proofs of responsible
AI compliance [29]. Developers are suggested to prepare standardized documentation (e.g. ISO AI standards4). There
are various types of documentation for managing the development of responsible AI systems, such as model cards,
data statements, datasheets for datasets, AI service factsheets [15], requirements speciﬁcation, design document,
implementation diary, testing report, maintenance plan [28].

3.2 Research Challenges

Establishing multi-level governance for responsible AI. Despite nearly one hundred frameworks for ethical principles
being issued [1], these principles are too high-level and hard to operationalize. A systematic and holistic governance
framework connected with software development lifecycles is highly desirable to translate ethical principles into
practices throughout the entire lifecycle of AI systems. The governance structure and processes need to be designed
and organized into multiple levels, including industry-level (which can be further divided into international-level and
country-level), organization-level, and team-level. So the governments, organizations and development teams can
follow the framework to governance AI systems.

Governance mechanisms linking with stakeholders and lifecycle processes. The governance mechanisms need to
be designed in a way that links with stakeholders and process stages. This makes the practitioners easy to adopt the
governance mechanisms in practice. Also, to identify and track accountability, we need to understand the stakeholders
of AI systems and their roles in the governance throughout the entire lifecycle of AI systems.

The communication between the SE people and the rest (i.e. people who are not SE-people). Responsible AI
challenges are broader than software engineering and need to be addressed by a multidisciplinary team with expertise in
software engineering, machine learning, social science, human-machine interaction, and user experience. However, as
the ﬁnal deliverable to the society is a responsible AI system, we believe the software engineering people are the key
force to drive the research and collaboration.

Education for students. As there is a serious concern about the social impact of AI systems, education on responsible
AI is urgently required for students across all across all levels (such as K-12 and university education).

3https://www.gida-global.org/care
4https://www.iso.org/committee/6794475/x/catalogue/

5

A PREPRINT - MARCH 17, 2022

Figure 5: Development process practices.

4 Process Perspective

4.1 Current State

Fig. 5 summarises the methods and best practices that can be incorporated into development processes, so the developers
could consider to apply them during the development.

4.1.1 Requirement engineering

Responsible AI requirements are either omitted or mostly stated as high-level project objectives in practice. The existing
methodologiesshould be extended and adopted for AI systems to ensure that the requirements captured are as accurate
and complete as possible while recognizing the special characteristics of AI systems such as autonomy, continuous
learning and the value-alignment problem.

Requirements Elicitation: Elicitation is the process of collecting requirements from stakeholders and other
sources, including goals, domain knowledge, business rules, operational environment, organizational environment [30].
Ethical principles are an essential source for identifying ethical requirements types and relevant and inclusive stakehold-
ers. Some of the important stakeholders for collecting ethical requirements include not only business owners, system
users, ethical/legal experts, regulators but also data providers,impacted data subjects, operators, advocacy groups and
even concerned public. Culture safety is a critical ethical requirement for AI systems that involve culture sensitive
data. There are a variety of requirements elicitation techniques that can be adapted to gather ethical requirements
(e.g. training/validating data fairness requirements and secondary data usage requirements), such as interviews [31],
scenarios, requirement workshops, interactive demos/prototypes [31], and user stories. Ethical user stories (e.g.
utilizing ECCOLA cards [32]) is an effective way to transform the abstract ethical requirements into tangible outcomes.

Requirements Analysis: One of the most important activities in requirement analysis is requirements classiﬁca-
tion [30] and resolution of requirements conﬂicts. We group the ethical principles (in Fig. 3) into two requirements
categories based on their nature and characteristics. The ﬁrst group includes privacy, safety & security, fairness, and
human values, which are similar to software qualities [33] and can be considered as non-functional requirements.
Some principles, such as safety & security, are the quality attributes well studied in the dependability community [34]
and can be identiﬁed and considered early in the development lifecycle. Recurring requirement problems and associated
reusable design fragments, like patterns/tactics, could be applied to meet those quality attributes [35]. Privacy is not a
standard software quality attribute [33], but has been treated as an increasingly important non-functional requirement
to realize regulatory requirements, like GDPR5. Recurring requirements and reusable patterns/practices have been
summarized in for privacy [36]. Fairness is another requirement that the developers should collect and design for
from early development life cycles. Technical mechanisms/practices to remove bias at different stages of the pipeline
have been designed [4]. There has been recently emerging research on casting human values into requirements in

5General Data Protection Regulation (GDPR), https://gdpr-info.eu/.

6

RequirementsengineeringDesignImplementationVerification and validationOperationElicitation: ethical userstories; workshops;interviews; demos;prototypes. Analysis: classifyingethical principles intonon-functional qualityrequirements andfunctional meta-governancerequirements. Specification: verifiable ethical requirements;data requirementsthroughout lifecycle;negative requirements;examples. V&V: traceable bothbackword and forward;continuous validation.Architecture:architectural style andpatterns. Ethical risk: frequencyof occurrence;consequencesize/response. Value sensitive design:participatoryworkshops; card-basedtoolkits Modelling: SysML;formal models;ontologies; ethicalknowledge bases. Simulation: ethicalscenario simulation;digital twin. XAI UX: wizard of oz;human realism.Ethical standards andchecklist for coding Continuousdocumentation Recording the author ofeach line and whomade changes Manual/automatic codereview for ethics:ethical metrics. Ethical compliancechecking for APIs:knowledge graph basedon regulations. Construction withreuse: market place forAI assets; credential;guidelines and toolingsupport for modelmigration; glue code.Test: ethicalacceptance test;integration test forinteraction between AIcomponents and non-AIcomponents; usabilitytest; data test;infrastructure test;differential test;metamorphic test. Tracking testing history Ethical assessment fortest cases Formal verification FMEA & FTA Assurance cases Ethical score: AIquotient; humannessfactors.Deployment: phaseddeployment;homogeneousredundancy. Continuous monitoringand validation:outcomes; dynamic,adaptive, andextensible ethical riskassessment; version-based feedback;incentives. Version control: co-versioning of data,model, code, config.;co-versioning of AI andnon-AI components. Accountability: bill ofmaterials; audit trail;accountabilityknowledge graphs andontologies.A PREPRINT - MARCH 17, 2022

software engineering and their operationalization [37, 38], including the extension of value-based design methods [39],
extension of human factor research on productivity and usability into human values consideration. But these efforts are
still limited to a small subset of human values [40]. The reason to group these principles is that they can be handled
and validated using how non-functional quality attributes are handled in software system design. Some principles
can be quantitatively validated, whereas others can be qualitatively handled by the widely used design patterns and
process-oriented practices.

The second group includes transparency & explainability, human control of technology, and accountability, which are
meta-level governance issues and can be classiﬁed as functional requirements for improving users’ conﬁdence in
the AI system. Transparency & explainability can be fulﬁlled by designing functions for receiving an explanation
of a prediction or decision and having access to the artifacts of AI systems. Human control of technology requires a
function that allows the users to challenge the output or use of the AI system. To meet accountability, the AI system
should include a function for tracing and identifying those who are responsible for the various phases of the AI system
lifecycle and the outcomes of the system.

Requirements Speciﬁcation: AI systems are complex software systems that involve hardware components. Thus, both
software requirements and system requirements (e.g., requiring certiﬁed hardware components) are needed. It’s worth
noting that AI systems try to solve the problems autonomously with a level of independence and agency and cannot be
fully speciﬁed. It is important to judiciously make ethical requirements quantiﬁable or measurable, and avoid vague
and unveriﬁable requirements [30]. Scope of responsibility need to be clearly deﬁned in the requirements [41]. Both
team diversity and the choice of ethical requirements veriﬁcation/validation techniques can be speciﬁed as process
requirements. Data requirements [42] need to be listed explicitly and speciﬁed throughout the data lifecycle (i.e.,
collection, management, survey, consumption, termination) taking into account all the involved roles (i.e., data provider,
manager, analyser, consumer) and ethical concerns, e.g. training/validation data fairness requirements and secondary
data usage requirements. Functional requirements should be stated with performance measures [42] and their
explanations in the context of the domain. Speciﬁc examples of the desired outcomes can be listed for the given
inputs [6]. Negative requirements with misuse, abuse and confuse cases can be described in the speciﬁcation [43].

Requirements Veriﬁcation and Validation: The ethical requirements should be speciﬁed traceable both backward
to the stakeholders and forward into the design modules, code pieces, and test cases. Requirements validation becomes
an activity that needs to be performed continuously during operation, which includes both monitoring and analysis of
production data from the ethical perspective [42]. Awareness of potential mismatches between training data and
real-world data is necessary to prevent the trained model from being unsuitable for its intended purpose. Model update
or recalibration on new data is important for the trustworthiness of AI systems. The conditions of retraining need to be
considered during the speciﬁcation phase, such as time, frequency, data characteristics, user feedback. The intended
operating environment should be understood as complete as possible. For example, the knowledge about the input
data fed into the decision-making component of the AI system and the data sources used to obtain the input data
should also be learnt [44].

4.1.2 Design

Signiﬁcant efforts have been put on ethical algorithms where ethical properties have been enabled via mathematical
deﬁnitions, analysis and theoretical guarantees, such as fairness [4] and privacy [3]. However, these algorithm focused
mechanisms have limited theoretical heuristic and are conﬁned to a small subset of ethical principles that can be easily
translated to quantiﬁable ethical properties. Most of the time, these ethical-aware algorithms are too complicated to
explain to less numeracy-equipped stakeholders [45] and hardly connected to the development processes.

There are different ways in the design process to reduce the ethical risk by managing: 1) Frequency of occurrence: The
frequency of automatic decision-making by AI systems can be reduced. Instead, AI systems can make suggestions to
human and ask for human’s approval on the ﬁnal decision; 2) Consequence size: The consequence size can be managed
through deployment strategies, e.g. only deploying the new model version to a group of users; 3) Consequence
response: This can be done through worst case analysis (e.g., FMEA), the resilient design to recover the state, or
punishing through the incentive mechanism, or overriding the recommended decisions, or undoing the actions.

There has been emerging research on the design process that considers ethical principles. Value sensitive design[39] is
a design approach that incorporates human value into the whole design process. To drive the value sensitive design for
responsible AI systems, participatory co-design workshops can be organized for developers and stakeholders [46]
using different types of toolkits and methods, such as card-based toolkits [46, 47, 48], ethical matrix [49], ethical
hackathon [50], user journeys, sketches, low-ﬁdelity paper prototypes, high-ﬁdelity clickable wireframes [51],
role play scenarios, table-top poll, low-ﬁdelity storyboards [52], value sensitive algorithm design [53].

7

A PREPRINT - MARCH 17, 2022

There are a variety of card-based toolkits that can be used in the design workshops. Envisioning cards are one of the
most adopted card-based toolkits in practice, which help designers envision of the long term effect their systems will
have on stakeholders. [46, 47]. In addition to envisioning cards, there are other card-based toolkits that can be used
in the value sensitive design workshops[46, 48], including model cards describing models with the inherent design
trade-offs, data cards analyzing possible data sources, people/persona cards discussing potential stakeholders and
their values and interests in the system, criteria/checklist cards considering different social and technical criteria,
ethics cards reﬂecting on ethical implications of implementing an AI system, situation cards describing problematic
situations, inspiration cards supporting participants to generate innovative and design concepts with new technologies.
Some card-based toolkits are designed speciﬁcally for a type of applications, such as Tiles for designing IoT applications
and PlutoAR for designing augmented reality (AR) applications [46].

Modelling has been adapted to reﬂect ethical concerns and relevant design decisions, including using SysML to
represent the architecture and describe the ethical aspects of AI systems [54], designing formal models incorporating
human values [55], using ontologies to represent the artifacts of AI systems to make them accountable [56], building
up ethical knowledge bases recommending design paths considering ethical concerns [57], using inductive logic
programming to codifying ethical principles [58].

Before deploying AI systems in real-world, it is important to perform system-level simulation to understand the
behavior of AI systems and evaluate ethical quality attributes in a cost-effective way. ethical scenario simulation [59]
is an effective simulation-driven design method. A digital twin simulates what is happening to an AI system running in
the real world, which can help ﬁnd unethical issues at run-time and improve the design.

Trust factors should be analyzed and considered during the design, such as capability of systems, availability of
interface, personality of agent (e.g., voice embodiment, visual virtual ﬁgure or physical representation) [60]. The four
major factors for trustworthiness-by-design include data, algorithm, architecture, software [61].

Many efforts for explainable AI (XAI) user interface design have been spent on checklists [62, 63, 64]. The question-
driven checklists are an effective way to understand the user needs, choices of XAI techniques, and XAI design
factors [63]. The main factors for the XAI design include information included, information delivery, and interaction
included [64]. The questions can be classiﬁed into the following groups: input, output, performance (can be extended
to ethical performance), how, why, why not, what if, how to be that, how to still be this [62]. The conversational
interface design can be experimented via a wizard of oz in which users interact with a system that the users believe to
be autonomous but is actually being operated by an unseen human [65]. There are ways to increase the level of human
trust in AI systems through XAI user interface: i) integrating human realism (i.e. anthropomorphism), including
appearance and other behavioural characteristics, into the interface design [66]; ii) proactively informing users and
the public data use information; iii) providing measurable beneﬁts to users; iv) informing users capabilities, scope of
use, and limitations of AI systems; v) informing users credentials of AI systems and operators; vi) explanations of
data, algorithm, models, system decisions and behaviours; vii) considering explanation audiences’ explainability needs
across the AI project lifecycle.

4.1.3 Implementation

Ethical implementation standards are collections of implementation rules from the ethical perspective, including
communication methods (i.e., documentation), programming languages, coding, interface, and tooling [30]. Developers
need to follow the communication standards to continuously maintain high-quality, up-to-date code documentation
that covers both AI components and non-AI component. The author of each line and who made changes need to
be recorded and maintained in a repository to enable accountability and traceability. Code review can be conducted
following the predeﬁned ethical standards. Ethical principles and metrics need to be deﬁned and added into the
developments tools to automate ethical quality checks. Initial attempts have been made on responsible AI tooling
support, mainly on algorithm-level rather than system-level, such as Google6 and Microsoft7. In particular, there are
many industry fairness toolkits, such as IBM’s AI Fairness 360, Google’s Fairness Indicators, Microsoft’s Fairlearn,
and UChicago’s Aequitas.

There may be ethical quality issues with APIs (e.g., data privacy breaches or fairness issues). Thus, ethical compliance
checking for APIs is needed to detect if any ethics violation exists. Ethical knowledge graphs can be built based on
the ethical principles and guidelines (e.g. privacy knowledge graph based on GDPR [67]) to automatically examine
whether APIs are compliant with regulations for AI ethics. Call graph might also be needed for code analysis as there
might be interactions between different APIs.

6https://cloud.google.com/responsible-ai
7https://www.microsoft.com/en-us/ai/responsible-ai-resources

8

A PREPRINT - MARCH 17, 2022

Construction with reuse means to develop responsible AI systems with the use of existing AI assets, e.g. from an
organizational repository or an open-source platform. A marketplace can be built up to trade the reusable AI assets,
including component code, models, and datasets. Blockchain can be adopted to design an immutable and transparent
marketplace enabling the auction-based trading for AI assets and material assets (e.g., cloud resources) [68]. To
ensure the ethical quality, credentials can be bound with the AI assets or developers, which can also be supported by
blockchain platforms. If different frameworks are used in model migration, guidelines and tooling support (such as
pytorch2keras8) are needed to automatically migrate a model from one framework to another. Glue code may be
used to integrate AI components with non-AI components to eliminate incompatibility since there are various inputs
and outputs for the AI component 9.

4.1.4 Veriﬁcation and Validation

Veriﬁcation and validation are used together for checking whether an AI system meets the requirements described in the
speciﬁcation and fulﬁlls its intended purpose in a responsible way. Ethical acceptance testing (e.g. Bias testing) can
be designed for detecting the ethics-related design ﬂaws [69] and verifying the ethical requirements (e.g. whether the
data pipeline has appropriate privacy control, fairness testing for training/validation data). The behaviour of the AI
system should be quantiﬁed by the acceptance testing and the acceptance criteria for each of the ethical principles
should be deﬁned in a testable way. A testing leader may be appointed to lead the testing for each ethics principle.
For example, When bias detected at runtime, the monitoring reports are returned to the bias testing leader [6]. An
ethical scoring system using a set of actionable tests can be used to measure how ready for production a given AI
system is from the ethics perspective. When certifying ethical aspects of an AI system/component, benchmark testing
may be needed. Formal veriﬁcation can be used to prove that a system matches its ethical requirements through a
comprehensive mathematical analysis [70, 71].

A collection of test cases with expected results should be maintained to detect possible ethical failures in a wide variety
of extreme situations. New test cases need to be added when there is a new requirement added or the operation context
changes [6]. All the test cases for veriﬁcation and validation should pass the ethics assessment. The history of testing
should be recorded and tracked, such as how and by whom the ethical issues were ﬁxed.

The traditional testing techniques can be adapted for testing AI systems. Unit testing can be performed for both AI
components and non-AI components according to the speciﬁcation (including model-level speciﬁcation). Interactions
between AI system components, particularly in between AI components (i.e. for AI pipeline) and between AI com-
ponents and non-AI components, need to be veriﬁed through incremental integration testing along the development
process [43]. Infrastructure testing is also part of the integration testing. Sanity testing is performed after the
software build to ensure that the code changes introduced are working aligned with ethical principles. Usability
testing [6] measures stakeholder performance and satisfaction and is essential to ensure the systems is easy to use and
does what users and indirect stakeholders expect in terms of responsible AI. Data tests need to be performed to check
whether the serving data is the data we expect in the operating environment, e.g., check the inputs for each variable
match what the model expects. Skew tests checks how representative the training data is of the serving data, e.g.,
through the percentage of missing data in the serving data compared to the training data. AI4SE4AI can be applied to
automate the testing through AI testing and/or cognitive testing [43].

Both failure mode and effects analysis (FMEA) and fault tree analysis (FTA) are tools to understand ethical failures
and risk of AI systems [43]. Assurance cases [72] (e.g. safety case [73]) provide evidence and arguments that support
claims about ethical properties or behaviors of AI systems. There are four levels of evaluation assurance [70], including
basic disclosure, tested claims, active testing, and formal veriﬁcation.

Scoring tools allow the team to measure the level of trustworthiness [74] or trust (e.g., trustworthiness/trust score) by
assessing the outcomes of AI systems based on the contextual data or capturing users’ experiences with AI features.
Artiﬁcial intelligent quotient [75] tracks a conversational AI system’s level of competence and capabilities over time
(e.g. knowledge, language, reasoning, creative and critical thinking, working memory). The agents’ humanness factors
(e.g. speaking and listening) can help improve the trust in conversational AI systems [76].

4.1.5 Operations

Given the continual learning of AI systems based on new data and the higher degree of uncertainty and risks associated
with the autonomy of the AI systems, there is a strong desire for deployment strategies and continuous validation of

8https://github.com/gmalivenko/pytorch2keras
9https://insights.sei.cmu.edu/blog/software-engineering-for-machine-learning-characterizing-and-detecting-mismatch-in-machine-learning-systems/

9

A PREPRINT - MARCH 17, 2022

responsible AI requirements. The deployment strategies include phased deployment (i.e., deploying AI systems for a
subset group of users initially to reduce ethical risk), homogeneous redundancy, etc.

The existing work on the continuous monitoring and validation mainly focuses on the AI system outputs (e.g.,
performance metrics - accuracy, precision, and recall ) rather than the outcomes (i.e. whether the AI system behaves
and make decisions responsibly) [77]. With different context data (e.g., users, trafﬁc, weather) in operation, AI systems
are continuously evolving to address ethical risks. The current practice for ethical risk assessment is often one-off
type of risk assessment (e.g. Five Safes Framework10). Ethical risk assessment is expected to be dynamic, adaptive,
and extensible for different context e.g., culture. . Version-based feedback (e.g., ethical violation) should be reported
to the development team and other stakeholders continuously. Incentives can be applied to encourage the ethical
outcomes of AI systems in terms of both decisions and behaviors. The time and frequency of validation and conditions
of necessary retraining should be predeﬁned [42].

An AI system usually involves co-evolution of data, model, code, and conﬁgurations. Data/model/code/conﬁguration
co-versioning traces exactly what datasets and conﬁguration parameters were used to train and evaluate the model.
Co-evolution may also happen to AI components and non-AI components, thus co-versioning is required to facilitate
the maintenance and communication between the AI component development team and non-AI component team.

Bill of materials (BOM) [78, 79] enables the transparency and accountability by keeping a formal record of the supply
chain details of the components used in building an AI system, such as component name, version, supplier, dependency
relationship, author and timestamp. Ethical audit trail records every step of AI systems from the ethics perspective.
Developers need to consider trade-offs between accountability and performance when making design decisions on what
date is placed on blockchain/cloud. Accountability knowledge graphs support capturing accountability information
throughout the lifecycle of AI systems and auditing them programmatically. To build such knowledge graphs, ontology
can be used to describe and model accountability information [56, 80].

4.2 Research Challenges

Integrating the software development process with the AI model development pipeline for ethical principles.
There is a methodological gap between the AI model development pipeline that produces the AI model and software
development process that develops the non-AI components and AI components embedding AI model [81]. Given the AI
model pipeline is more experimental with still limited methodological support, integrating the AI model pipeline into
the agile software development process needs better understanding of artifacts, activities and roles involved. Although
initial tooling attempts have been made on the integration (such as Microsoft Team Data Science Process11 and IBM
Watson Studio12), the integrated AI system development process still need to be standardized and supported by software
tools taking into account responsible AI principles.

Capturing ethical principles by requirement engineering. Compared with traditional software and general responsi-
ble software, AI systems also need to consider requirements about models, training data, system autonomy oversight
and may emphasize certain ethical requirements more due to AI-based autonomous and potentially opaque behaviour
and decision making. In particular, some of the ethical principles are hard to deﬁne and quantify. To make the ethical
principles be captured by requirements engineering for AI systems, the community should put further efforts on
requirements engineering methods and provide a concrete guidance on requirement engineering for responsible AI
systems.

Designing AI systems for improving both trustworthiness and trust. Trustworthiness is the ability of an AI system
to meet ethical principles, while trust is users’ subjective estimates of the trustworthiness[18]. Trust in AI systems
involves trust duality that includes trust in providers and trust in technologies that can be further classiﬁed into trust in
an AI technology and a base technology (e.g. vehicle) [82]. Both trust in a provider and trust in a technology need
to be considered in parallel. A user’s trust in an AI system may have a signiﬁcant gap compared to the AI system’s
inherent trustworthiness, i.e., a user underestimates or overestimates a system’s trustworthiness and has inadequate or
excessive trust into the system. More efforts need to be made on the design to improve both trustworthiness and trust in
AI systems. Process and product (i.e. system perspective) mechanisms can be leveraged to achieve trustworthiness for
different ethical principles, whereas process and product evidence need to be offered for different types of trusters in a
proper communication way to drive trust. These will help close the gap between their subjective estimation and the
system’s more objective trustworthiness. Instead of focusing on veriﬁable product trustworthiness via mathematical
algorithm-level guarantees, researchers need to systematically explore a broader variety of mechanisms in system-level
product design and development processes to improve both trustworthiness and trust.

10https://www.aihw.gov.au/about-our-data/data-governance/the-five-safes-framework
11https://docs.microsoft.com/en-us/azure/architecture/data-science-process/overview
12https://www.ibm.com/cloud/watson-studio

10

A PREPRINT - MARCH 17, 2022

Figure 6: Architectural patterns for responsible-AI-by-design.

Continuously monitoring and validating the outcomes of AI systems against responsible AI requirements. There
are two forms of AI system development deﬁned in [83]: requirement-driven development and outcome-driven
development. The latter is about the real-world operation and outcome of AI systems. Seamless integration of
requirement-driven development and outcome-driven development requires understanding the unique characteristics of
AI systems. The development of AI systems is a continual and iteration process. Validation of outcomes (i.e. whether
the system provides the intended beneﬁts and behave appropriately given the situation) is required for AI systems rather
than just outputs (e.g. precision, accuracy and recall). Also, since all principles need to be instantiated, it is necessary to
make risk assessment and management extensible , adaptive, and dynamic for different context, with guided extension
points. For example, some principle might be automatically instantiable for different culture context and extended
following guided extension points. Further work is needed on MLOps for continuously monitoring and validating the
outcomes of AI systems against the responsible AI requirements.

5 System Perspective

5.1 Current State

5.1.1 Architectural style and patterns

Responsible-AI-by-design architectural style can be deﬁned by a set of design principles and patterns. Some design
principles include co-architecting of AI components and non-AI components, minimum complexity, design with
reuse. The architecture of AI systems includes AI components that produce and embed the AI models and non-AI
components that uses the outputs of AI components for overall system functionalities. As the AI components are often
iteratively experimented by data scientists/engineers who are not familiar with software engineering, co-architecting of
the AI components and the non-AI components can ensure the seamless integration of the two types of components
and consideration of both system-level and model-level (ethical) requirements when making design decisions. Scenario-
based evaluation methods can be adapted to evaluate the architecture of AI systems. Developers should follow the
principle of minimum complexity that includes both the design of AI systems (e.g. software/system architecture,
whether adopting AI or not, selection of AI techniques) and the future operating environment [44]. One principle
is design with reuse. AI system components, particularly the AI components for producing AI models, should be

11

Pattern nameAI mode switcherEthical sandboxMulti-modelpredictorFederatedlearnerEthical black boxCross-systemauditorCo-versioningregistryContinuousvalidatorObjectiveTrustworthinessTrustworthinessTrustworthinessTrustworthinessTrustTrustTrustTrustworthinessTarget usersArchitectsArchtiectsArchitectsArchitectsOperatorsOperatorsData scientistsOperatorsImpactedstakeholdersEmployees,management,consumers,regulatorsEmployees,management,consumers,regulatorsEmployees,management,consumers,regulatorsEmployees,management,consumers,regulatorsEmployees,management,consumers,regulatorsEmployees,management,consumers,regulatorsEmployees,management,consumers,regulatorsEmployees,management,consumers,regulatorsLifecycle stagesDesignDesignDesignDesignOperationOperationOperationOperationPrivacyN/AYesN/AYesYesYesN/AYesAccountabilityN/AN/AN/AN/AYesYesYesYesSafety&securityYesYesYesYesYesYesN/AYesTrans. & expla.N/AN/AN/AN/AYesYesYesYesFairnessN/AYesYesN/AYesYesN/AYesHuman controlYesN/AN/AN/AYesYesN/AYesHuman valuesYesYesN/AN/AYesYesN/AYesMechanismInvocation, dismissal,kill switch, override,fallbackEthical margin,watchdogN/AN/AImmutable logImmutable logN/ARebuild alertApplicabilityHighHighHighMediumHighLowHighHighBenefitsIncreased safety,contestability andautonomyIsolatedenvironment,increased securityand safetyEnabled cross-validation andfault-toleranceIncreased privacyand reliabilityEnabledaccountability andregulatoryconformanceEnabled global-view accountabilityEnabled differentlevels of co-versioningEnabled run-time alert andexplicitmitigationDrawbacksLimited by quality ofthe non-AIcomponent, limitedin real-timeDifficult to use forhard-to-quantifyethical risks,performance penaltyIncreaseddevelopmenteffort, requirespecialised skillsIncreaseddevelopmenteffort, potentialsampling bias,performancepenaltyPerformancepenalty, Increaseddevelopment effort.Potential privacybreach risk,performancepenaltyIncreased storagecostDifficult to usefor hard-to-quantify ethicalrisksKnown usesTesla autopilot [98],Waymo [105], Baiduautonomous mini-bus [7]Safe-visor [65],Fastcase AI Sandbox[44], AI Sandbox [87]TFUtils [81],Kubeflow [62],AWS SageMaker[5]TensorFlowFederated [97],LEAF [14], FATE[45]Idependent auditsfor safety [43],ethical black box[107], distributedblakbox [42]Global-viewaccountability [76],blackbox for XAI [1]DVC [23], MlflowModel Registry [21],Replicate.ai [86]Neptune [80],AWS SageMakerModel Monitor[6], Qualdo [88]A PREPRINT - MARCH 17, 2022

evaluated against responsible AI requirements and reused as much as possible to improve productivity [44]. Ethical
certiﬁcates may be needed for reusing the AI components developed by the third party.

Fig. 6 lists a set of architectural patterns for responsible-AI-by-design, which could be embedded as the product features
of AI products. Adopting AI or not can be considered as a major architectural design decision when designing a
software system. For example, AI mode switcher offers users efﬁcient invocation and dismissal mechanisms for
activating or deactivating the AI component when needed [84, 85, 86, 87]. Kill switch is a special type of invocation
mechanism which immediately shuts down the AI component and its negative effects. The decisions made by the AI
component can be executed automatically or reviewed by a human expert in critical situations. The human expert
serves to approve or override the decisions. Human intervention can also happen after acting the AI decision through
the fallback mechanism that reverses the system back to the state before executing the AI decision. A built-in guard
ensures that the AI component is only activated within the predeﬁned conditions (such as domain of use, boundaries of
competence). Users can ask questions or report complaints/failures/near misses through a recourse channel.

Ethical sandbox can be applied to run the AI component separately (e.g. from untrusted third parties) [88, 89].
Maximal tolerable probability of violating the responsible AI requirements should be deﬁned as ethical margin for the
sandbox [90]. A watch dog can be used to limit the execution time of the AI component to reduce the ethical risk [91].
Multi-model predictor employs two or more models to perform the same task [92, 93, 94]. This pattern can improve
the reliability by deploying different models under different context (e.g., different regions) and enabling fault-tolerance
by cross-validating ethical requirements for a single decision (e.g., only accepts the same results from the employed
models) [95, 96]. Federated learner preserves the data privacy by training models locally on the client devices and
formulating a global model on a central server based on the local model updates [97, 98, 99]. Decentralized learning
is an alternative to federated learning, which uses blockchain to remove the single point of failure and coordinate the
learning process in a fully decentralized way.

In the event of negative outcomes, the responsible humans can be identiﬁed by an ethical black box for accountabil-
ity [100]. The ethical black box continuously records sensor data, internal status data, decisions, behaviors (both system
and operator) and effects [101]. All of these data need to be kept as evidence with the timestamp and location data
using an immutable log (e.g. using blockchain)[102]. Cross-system auditor provides global-view accountability by
ﬁnding discrepancies among the data collected from a set of AI systems and identifying liability when negative events
occur [103, 104]. Co-versioning registry can be applied to ensure accountability in two cases: 1) co-versioning of
AI components and non-AI components; 2) co-versioning the artifacts within the AI components, i.e., data, model,
code, and conﬁguration [105, 106, 107]. Continuous validator supports continuous ethical risk assessment by mon-
itoring and validating the predeﬁned ethical metrics [108, 109, 110]. The time and frequency of validation should
be conﬁgured within the continuous validator. Version-based feedback and rebuild alert should be sent when the
predeﬁned conditions are met. Incentive mechanisms can be designed to reward/punish the ethical/unethical behavior
or decisions of AI systems.

5.1.2 System-Level Techniques

There are many system-level techniques, which could be embedded as components:

• Fairness: demographic parity, data augmentation, weighted data sampling, re-sampling, re-weighting, swapping

labels, removing dependencies, equalized odds checking;

• Privacy: data sanitizing, federated learning, decentralized learning, differential privacy, secure multiparty

computation, homographic learning, fog computing;

• Safety and security: sandboxing, trusted execution environments; safety margin; data representativeness

checking, approximate computing;

• Explainability: global explanations, local explanations, feature-based explanations (contrastive, counterfactual,
rule-based explanations), exemplar-based explanations, post hoc explanations for black box models, prospective
explanations, surrogate models (LIME, SHAP, PyExplainer [111]), what-if, conﬁdence scores.

5.2 Research Challenges

Deﬁning architectural style for responsible-AI-by-design. An AI system consists of AI components and non-AI
components that are interconnected and work together to achieve the system’s objective [112]. An AI model needs to
be trained and integrated into the inference component of the system to perform the required functions. Both the AI
pipeline components and the inference component can be considered as AI components. Combining AI and non-AI
components may create new emergent behavior and dynamics. Therefore, ethics need to be considered at system-level,
including AI components, non-AI components and their connections. For example, the new data gathered by the data

12

A PREPRINT - MARCH 17, 2022

collector need to be fed into the model trainer, whereas the effect of actions decided by the AI model may change
the behavior of non-AI components and could be collected through the feedback component built into the AI system.
One way to achieve responsible-AI-by-design is to deﬁne an architectural style through a set of architectural design
principles taking into account ethical principles. Additional principles need to be explored in addition to the initial
attempt.

Dealing with ethical requirement trade-offs using design patterns. There are trade-offs between functional require-
ments and ethical principles or in between some of the ethics principles. Privacy and utility are often conﬂicting. For
example, to fulﬁll the privacy requirements, the datasets can be de-identiﬁed and aggregated so that individuals cannot
be uniquely identiﬁed [113], which may lead to worse distributional properties and affect the reliability. Requirement
inconsistency also happens to accountability and privacy. For example, when particular activities are not compliant to
some regulatory standards, we need to ﬁnd out where this happened and who to blame. In this case, there might be
an issue about data privacy. The current practice dealing with conﬂicting ethics principles is usually the developers
following one principle while overriding the other rather than building balanced trade-offs with stakeholders making the
ultimate value and risk call. Patterns can be used to deal with the system-level trade-offs among conﬂicting responsible
AI principles and other requirements in an inclusive manner. A pattern catalogue is needed to provide concrete guidance
on how to design responsible AI systems.

6 Conclusion

To operationalize responsible AI, we present a research roadmap on software engineering for responsible AI. The ﬁndings
could be organised as operationalized guidelines for the stakeholders of AI systems (e.g. regulators, management,
developers). Some ﬁndings could be framed as tools (e.g. ethical risk assessment) or embedded as product features of
AI systems to reduce the ethical risk and unlock the market where there is currently little trust (e.g. ethical black box).
Although the major industry solutions (such as model cards) have been identiﬁed in our study through Google scholar
and the snowballing process, we plan to do an industry landscape that will cover the complete state of the practice.

References

[1] A. Jobin, M. Ienca, and E. Vayena, “The global landscape of ai ethics guidelines,” Nature Machine Intelligence,

vol. 1, no. 9, pp. 389–399, 2019.

[2] J. Fjeld et. al., “Principled artiﬁcial intelligence: Mapping consensus in ethical and rights-based approaches to

principles for ai,” Berkman Klein Center Research Publication, no. 2020-1, 2020.

[3] Z. Ji, Z. C. Lipton, and C. Elkan, “Differential privacy and machine learning: a survey and review,” 2014.

[4] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A survey on bias and fairness in machine

learning,” 2019.

[5] B. A. Kitchenham and S. Charters, “Guidelines for performing systematic literature reviews in software engi-

neering,” Tech. Rep., 2007.

[6] B. Shneiderman, “Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy

human-centered ai systems,” ACM Trans. Interact. Intell. Syst., vol. 10, no. 4, 2020.

[7] C. Landwehr, “We need a building code for building code,” Commun. ACM, vol. 58, no. 2, p. 24–26, jan 2015.

[8] J. S. Borg, “Four investment areas for ethical ai: Transdisciplinary opportunities to close the publication-to-

practice gap,” Big Data & Society, vol. 8, no. 2, 2021.

[9] J. C. Ib´a˜nez and M. V. Olmeda, “Operationalising ai ethics: how are companies bridging the gap between practice

and principles? an exploratory study,” AI & SOCIETY, 2021.

[10] R. V. Zicari et. al., “Z-inspection®: A process to assess trustworthy ai,” IEEE Transactions on Technology and

Society, vol. 2, no. 2, pp. 83–97, 2021.

[11] S. A. A. et. al., “Towards an artiﬁcial intelligence maturity model: From science ﬁction to business facts,” in

PACIS, 2019.

[12] P. F. et. al., “Developing an artiﬁcial intelligence maturity model for auditing,” in ECIS, 2021.

[13] P. Cihon, M. J. Kleinaltenkamp, J. Schuett, and S. D. Baum, “Ai certiﬁcation: Advancing ethical practice by
reducing information asymmetries,” IEEE Transactions on Technology and Society, vol. 2, no. 4, p. 200–209,
Dec 2021.

[14] R. Eitel-Porter, “Beyond the promise: implementing ethical ai,” AI Ethics 1, vol. 1, p. 273–80, 2021.

13

A PREPRINT - MARCH 17, 2022

[15] A. Jacovi, A. Marasovi´c, T. Miller, and Y. Goldberg, “Formalizing trust in artiﬁcial intelligence: Prerequisites,

causes and goals of human trust in ai,” in FAccT ’21, 2021, p. 624–635.

[16] M. A. Madaio, L. Stark, J. Wortman Vaughan, and H. Wallach, “Co-designing checklists to understand organiza-

tional challenges and opportunities around fairness in ai,” in CHI ’20, 2020, p. 1–14.

[17] M. S. A. Lee and J. Singh, “Risk identiﬁcation questionnaire for detecting unintended bias in the machine

learning development lifecycle,” in AIES’21, 2021, p. 704–714.

[18] L. Z. et. al., “Ai and ethics - operationalising responsible ai,” Humanity Driven AI: Productivity, Wellbeing,

Sustainability and Partnership, 2021.

[19] B. R. J. et. al., “The ethics of artiﬁcial intelligence in pathology and laboratory medicine: Principles and practice,”

Academic Pathology, vol. 8, 2021.

[20] A. Weiss et. al., “Using the design of adversarial chatbots as a means to expose computer science students to the
importance of ethics and responsible design of ai technologies,” in Human-Computer Interaction – INTERACT
2021, 2021, pp. 331–339.

[21] H. Furey and F. Martin, “Ai education matters: A modular approach to ai ethics education,” AI Matters, vol. 4,

no. 4, p. 13–15, jan 2019.

[22] W. H. et. al., “How can human values be addressed in agile methods? a case study on safe,” 2021.
[23] S. Amershi et. al., “Software engineering for machine learning: A case study,” in ICSE-SEIP’19, 2019, pp.

291–300.

[24] T. W. K. et. al., “When is it permissible for artiﬁcial intelligence to lie? a trust-based approach,” 2021.

[25] S. McLennan, A. Fiske, L. A. Celi, R. M¨uller, J. Harder, K. Ritt, S. Haddadin, and A. Buyx, “An embedded

ethics approach for ai development,” Nature Machine Intelligence, vol. 2, no. 9, pp. 488–490, 2020.

[26] E. Papagiannidis et. al., “Deploying ai governance practices: A revelatory case study,” in Responsible AI and

Analytics for an Ethical and Inclusive Digitized Society, 2021, pp. 208–219.

[27] R. Woitsch et. al., “Collaborative model-based process assessment for trustworthy ai in robotic platforms,” in

Society 5.0, 2021, pp. 163–174.

[28] B. Hutchinson et. al., “Towards accountability for machine learning datasets: Practices from software engineering

and infrastructure,” in FAccT ’21, 2021, p. 560–575.

[29] W. Chu, “A decentralized approach towards responsible ai in social ecosystems,” 2021.

[30] I. Society, P. Bourque, and R. Fairley, “Guide to the software engineering body of knowledge (swebok (r)),”

2014.

[31] E. M. JingWen et. al., “Improving of user trust in machine learning recommender-based business applications
through ui design: A case study,” in HCI in Business, Government, and Organizations, 2018, pp. 715–729.
[32] E. Halme et. al., “How to write ethical user stories? impacts of the eccola method,” in Agile Processes in Software

Engineering and Extreme Programming, 2021, pp. 36–52.

[33] ISO, “Iso/iec25010:2011 systems and software engineering–systems and software quality requirements and
evaluation (square)–system and software quality models,” International Organization for Standardization, vol. 34,
p. 2910, 2011.

[34] A. Avizienis, J.-C. Laprie, B. Randell, and C. Landwehr, “Basic concepts and taxonomy of dependable and

secure computing,” IEEE Trans. Dependable Secur. Comput., vol. 1, no. 1, p. 11–33, Jan. 2004.

[35] L. Bass, P. Clements, and R. Kazman, Software architecture in practice. Addison-Wesley Professional, 2003.
[36] S. Y. Chia, X. Xu, H.-Y. Paik, and L. Zhu, “Analysing and extending privacy patterns with architectural context,”

in Proceedings of the 36th Annual ACM Symposium on Applied Computing, 2021.

[37] W. H. et. al., “Human values in software engineering: Contrasting case studies of practice,” IEEE Transactions

on Software Engineering, pp. 1–1, 2020.

[38] B. H. et. al., “Towards accountability for machine learning datasets: Practices from software engineering and

infrastructure,” 2020.

[39] J. van den Hoven, P. E. Vermaas, and I. van de Poel, “Design for values: An introduction,” in Handbook of Ethics,

Values, and Technological Design: Sources, Theory, Values and Application Domains, 2015, pp. 1–7.

[40] H. Perera et. al., “A study on the prevalence of human values in software engineering publications, 2015 – 2018,”

in ICSE’20, 2020, p. 409–420.

14

[41] G. Lima, N. Grgi´c-Hlaˇca, and M. Cha, “Human perceptions on moral responsibility of ai,” in CHI ’21, 2021.
[42] A. Vogelsang and M. Borg, “Requirements engineering for machine learning: Perspectives from data scientists,”

in REW’19, 2019, pp. 245–251.

A PREPRINT - MARCH 17, 2022

[43] C. Ebert and M. Weyrich, “Validation of autonomous systems,” IEEE Software, vol. 36, no. 5, pp. 15–23, 2019.
[44] A. Steimers and T. B¨omer, “Sources of risk and design principles of trustworthy artiﬁcial intelligence,” in Digital
Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management. AI, Product and Service,
2021, pp. 239–251.
“Review

https://www.gov.uk/

[45] CDEI,

algorithmic

making,”

decision

bias

into

in

government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/
main-report-cdei-review-into-bias-in-algorithmic-decision-making, 2020,
2020.

accessed on 16 December

[46] K.-E. K. e. a. Bilstrup, Staging Reﬂections on Ethical Dilemmas in Machine Learning: A Card-Based Design

Workshop for High School Students, 2020, p. 1211–1222.

[47] S. Umbrello, “The role of engineers in harmonising human values for ai systems design,” 2021.
[48] H. Shen et. al., “Value cards: An educational toolkit for teaching social impacts of machine learning through

deliberation,” in FAccT ’21, 2021, p. 850–861.

[49] F. Muhlenbach, “A methodology for ethics-by-design ai systems: Dealing with human value conﬂicts,” 2020.
[50] M. P. et. al., “Harnessing interdisciplinarity to promote the ethical design of ai systems,” 2019.
[51] S. S. et. al., “Design methods for artiﬁcial intelligence fairness and transparency,” in IUI Workshops, 2021.
[52] Z. Skinner, S. Brown, and G. Walsh, “Children of color’s perceptions of fairness in ai: An exploration of equitable

and inclusive co-design,” in CHI EA ’20, 2020, p. 1–8.

[53] H. Zhu, B. Yu, A. Halfaker, and L. Terveen, “Value-sensitive algorithm design: Method, case study, and lessons,”

Proc. ACM Hum.-Comput. Interact., vol. 2, no. CSCW, 2018.

[54] M. T. et. al., “Accountable system design architecture for embodied ai: a focus on physical human support robots,”

Advanced Robotics, vol. 33, pp. 1248 – 1263, 2019.

[55] B. Fish and L. Stark, “Reﬂexive design for fairness and other human values in formal models,” Proceedings of

the 2021 AAAI/ACM Conference on AI, Ethics, and Society, Jul 2021.

[56] I. Naja, M. Markovic, P. Edwards, and C. Cottrill, “A semantic framework to support ai system accountability

and audit,” in The Semantic Web, 2021, pp. 160–176.

[57] K. Sekiguchi and K. Hori, “Organic and dynamic tool for use with knowledge base of ai ethics for promoting

engineers’ practice of ethical ai design,” AI & SOCIETY, vol. 35, 2020.

[58] M. Anderson and S. L. Anderson, “Geneth: a general ethical dilemma analyzer,” Paladyn, Journal of Behavioral

Robotics, vol. 9, pp. 337 – 357, 2014.

[59] S. et. al., “Simulation driven design and test for safety of ai based autonomous vehicles,” in CVPRW’21, 2021,

pp. 122–128.

[60] D. Wu and Y. Huang, “Why do you trust siri?: The factors affecting trustworthiness of intelligent personal
assistant,” Proceedings of the Association for Information Science and Technology, vol. 58, no. 1, pp. 366–379,
2021.

[61] X. M. Liu and D. Murphy, “A multi-faceted approach for trustworthy ai in cybersecurity,” Journal of Strategic

Innovation and Sustainability, vol. 15, no. 6, Dec. 2020.

[62] Q. V. Liao, D. Gruen, and S. Miller, “Questioning the ai: Informing design practices for explainable ai user

experiences,” Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, Apr 2020.

[63] Q. V. L. et. al., “Question-driven design process for explainable ai user experiences,” 2021.
[64] R. Larasati, A. De Liddo, and E. Motta, “Ai healthcare system interface: Explanation design for non-expert user

trust,” in ACMIUI-WS 2021: Joint Proceedings of the ACM IUI 2021 Workshops, vol. 2903, 2021.

[65] S. F. Jentzsch et. al., “Conversational interfaces for explainable ai: a human-centred approach,” in International
Workshop on Explainable, Transparent Autonomous Agents and Multi-Agent Systems, 2019, pp. 77–92.
[66] D. D. Luxton, “Recommendations for the ethical use and design of artiﬁcial intelligent care providers,” Artiﬁcial

Intelligence in Medicine, vol. 62, no. 1, pp. 1–10, 2014.

[67] Intersoft Consulting, “Gdpr,” Sep 2019. [Online]. Available: https://gdpr-info.eu/

15

A PREPRINT - MARCH 17, 2022

[68] N. Six, A. Perrichon-Chr´etien, and N. Herbaut, “Saiaas: A blockchain-based solution for secure artiﬁcial

intelligence as-a-service,” in Deep-BDB’21, 2022, pp. 67–74.

[69] A. Chattopadhyay, A. Ali, and D. Thaxton, “Assessing the alignment of social robots with trustworthy ai design

guidelines: A preliminary research study,” in CODASPY’21, 2021, p. 325–327.

[70] R. H. C. Yap, “Towards certifying trustworthy machine learning systems,” in Trustworthy AI - Integrating

Learning, Optimization and Reasoning, 2021, pp. 77–82.

[71] L. Dennis and M. Fisher, Veriﬁable Autonomy and Responsible Robotics, 2021, pp. 189–217.
[72] Y. Yanagisawa and Y. Yokote, “A new approach to better consensus building and agreement implementation for
trustworthy ai systems,” in Computer Safety, Reliability, and Security. SAFECOMP 2021 Workshops, 2021, pp.
311–322.

[73] L. Gauerhof, R. Hawkins, C. Picardi, C. Paterson, Y. Hagiwara, and I. Habli, “Assuring the safety of machine
learning for pedestrian detection at crossings,” in Computer Safety, Reliability, and Security, 2020, pp. 197–212.

[74] S. R. J. et. al., “Creating a tool to reproducibly estimate the ethical impact of artiﬁcial intelligence,” 2019.

[75] M. Tschopp and M. Ruef, “On trust in ai—a systemic approach,” 2018.

[76] P. Hu, Y. Lu, and Y. Y. Gong, “Dual humanness and trust in conversational ai: A person-centered approach,”

Computers in Human Behavior, vol. 119, p. 106727, 2021.
[77] H. Barmer et al., “National ai engineering initiative,” 2021.

[78] The United States Department of Commerce, “The minimum elements for a software bill of materials (sbom),”
2021. [Online]. Available: https://www.ntia.doc.gov/ﬁles/ntia/publications/sbom minimum elements report.pdf

[79] I. Barclay, A. Preece, I. Taylor, and D. Verma, “Towards traceability in data ecosystems using a bill of materials

model,” arXiv preprint arXiv:1904.04253, 2019.

[80] M. Markovic, I. Naja, P. Edwards, and W. Pang, “The accountability fabric: A suite of semantic tools for

managing ai system accountability and audit,” in CEUR Workshop Proceedings, 2021.

[81] S. Mart´ınez-Fern´andez, X. Franch, A. Jedlitschka, M. Oriol, and A. Trendowicz, “Developing and operating
artiﬁcial intelligence models in trustworthy autonomous systems,” in International Conference on Research
Challenges in Information Science, 2021, pp. 221–229.

[82] M. Renner et. al., “Achieving trustworthy artiﬁcial intelligence: Multi-source trust transfer in artiﬁcial intelligence-

capable technology,” in ICIS’21, 2021.

[83] J. Bosch, “From efﬁciency to effectiveness: Delivering business value through software,” in Software Business,

2019, pp. 3–10.

[84] P. Vassilakopoulou, “Sociotechnical approach for accountability by design in ai systems.” in ECIS, 2020.
[85] Tesla, “Tesla autopilot,” 2015. [Online]. Available: https://www.tesla.com/autopilot

[86] Waymo, “Waymo,” 2009. [Online]. Available: https://waymo.com/

[87] Baidu, “Baidu apollo minibus,” 2018. [Online]. Available: https://apollo.auto/minibus/index.html

[88] Fastcase, “Fastcase ai sandbox,” 2017. [Online]. Available: https://www.fastcase.com/sandbox/

[89] A. Sandbox, “Ai sandbox,” 2019. [Online]. Available: https://aisandbox.dev/

[90] A. Lavaei, B. Zhong, M. Caccamo, and M. Zamani, “Towards trustworthy ai: safe-visor architecture for
uncertiﬁed controllers in stochastic cyber-physical systems,” in Proceedings of the Workshop on Computation-
Aware Algorithmic Design for Cyber-Physical Systems, 2021, pp. 7–8.

[91] A. C. Serban, “Designing safety critical software systems to manage inherent uncertainty,” in ICSA-C’19, 2019,

pp. 246–249.

[92] NeuroAILab, “Tfutils multi-model training for tensorﬂow,” 2018. [Online]. Available: http://neuroailab.stanford.

edu/tfutils/fundamentals/multimodel.html

[93] Kubeﬂow, “Kubeﬂow,” 2018. [Online]. Available: https://www.kubeﬂow.org/

[94] AWS, “Amazon sagemaker,” 2017. [Online]. Available: https://aws.amazon.com/sagemaker/
[95] J. Dai et. al., “More reliable ai solution: Breast ultrasound diagnosis using multi-ai combination,” arXiv preprint

arXiv:2101.02639, 2021.

[96] M. Nafreen, S. Bhattacharya, and L. Fiondella, “Architecture-based software reliability incorporating fault

tolerant machine learning,” in RAMS’20, 2020, pp. 1–6.

16

A PREPRINT - MARCH 17, 2022

[97] TensorFlow, “Tensorﬂow federated,” 2019. [Online]. Available: https://www.tensorﬂow.org/federated
[98] S. Caldas et al., “Leaf: A benchmark for federated settings,” 2019. [Online]. Available: https://leaf.cmu.edu/
[99] FATE Project, “Fate,” 2019. [Online]. Available: https://github.com/FederatedAI/FATE
[100] G. Falco et al., “Governing ai safety through independent audits,” Nature Machine Intelligence, vol. 3, no. 7, pp.

566–571, 2021.

[101] A. F. Winﬁeld and M. Jirotka, “The case for an ethical black box,” in Annual Conference Towards Autonomous

Robotic Systems, 2017, pp. 262–273.

[102] G. Falco and J. E. Siegel, “A distributed ‘black box’audit trail design speciﬁcation for connected and automated

vehicle data and software assurance,” arXiv preprint arXiv:2002.02780, 2020.

[103] B. S. Miguel, A. Naseer, and H. Inakoshi, “Putting accountability of ai systems into practice,” in IJCAI’21, 2021,

pp. 5276–5278.

[104] A. Adadi and M. Berrada, “Peeking inside the black-box: A survey on explainable artiﬁcial intelligence (xai),”

IEEE Access, vol. 6, 2018.

[105] DVC, “Dvc,” 2017. [Online]. Available: https://dvc.org/
[106] Databricks, “Mlﬂow model registry on databricks,” 2021. [Online]. Available: https://docs.databricks.com/

applications/mlﬂow/model-registry.html

[107] Replicate, “Replicate,” 2020. [Online]. Available: https://replicate.com/
[108] Neptune, “neptune.ai,” 2016. [Online]. Available: https://neptune.ai/
[109] AWS, “Amazon sagemaker model monitor,” 2019. [Online]. Available: https://aws.amazon.com/sagemaker/

model-monitor/

[110] Saturam, “Qualdo,” 2020. [Online]. Available: https://www.qualdo.ai/
[111] C. Pornprasit, C. Tantithamthavorn, J. Jiarpakdee, M. Fu, and P. Thongtanunam, “Pyexplainer: Explaining the
predictions of just-in-time defect models,” in 2021 36th IEEE/ACM International Conference on Automated
Software Engineering (ASE), 2021, pp. 407–418.

[112] Q. L. et. al., “Software engineering for responsible ai: An empirical study and operationalised patterns,” 2021.
[113] M. K. A. et. al., “Opening the software engineering toolbox for the assessment of trustworthy ai,” in 1st

International Workshop on New Foundations for Human-Centered AI, 2020.

17

