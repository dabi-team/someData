Performance portable ice-sheet modeling with MALI

Jerry Watkins1,*, Max Carlson1, Kyle Shan2,**, Irina Tezaur1, Mauro Perego3, Luca
Bertagna3, Carolyn Kao4,**, Matthew J. Hoﬀman5, and Stephen F. Price5

1Sandia National Laboratories, Quantitative Modeling & Analysis Department, Livermore, CA, USA.
2Micron Technology, Boise, ID, USA.
3Sandia National Laboratories, Center for Computing Research, Albuquerque, NM, USA.
4TSMC, Hsinchu, Taiwan.
5Los Alamos National Laboratory, Fluid Dynamics and Solid Mechanics Group, Los Alamos, NM, USA.
*Corresponding author; email: jwatkin@sandia.gov.
**All work completed as a student at Stanford University.

April 12, 2022

Abstract

High resolution simulations of polar ice-sheets play a crucial role in the ongoing eﬀort to
develop more accurate and reliable Earth-system models for probabilistic sea-level projections.
These simulations often require a massive amount of memory and computation from large super-
computing clusters to provide suﬃcient accuracy and resolution. The latest exascale machines
poised to come online contain a diverse set of computing architectures. In an eﬀort to avoid
architecture speciﬁc programming and maintain productivity across platforms, the ice-sheet
modeling code known as MALI uses high level abstractions to integrate Trilinos libraries and
the Kokkos programming model for performance portable code across a variety of diﬀerent archi-
tectures. In this paper, we analyze the performance portable features of MALI via a performance
analysis on current CPU-based and GPU-based supercomputers. The analysis highlights per-
formance portable improvements made in ﬁnite element assembly and multigrid preconditioning
within MALI with speedups between 1.26–1.82x across CPU and GPU architectures but also
identiﬁes the need to further improve performance in software coupling and preconditioning on
GPUs. We also perform a weak scalability study and show that simulations on GPU-based
machines perform 1.24–1.92x faster when utilizing the GPUs. The best performance is found in
ﬁnite element assembly which achieved a speedup of up to 8.65x and a weak scaling eﬃciency
of 82.9% with GPUs. We additionally describe an automated performance testing framework
developed for this code base using a changepoint detection method. The framework is used to
make actionable decisions about performance within MALI. We provide several concrete exam-
ples of scenarios in which the framework has identiﬁed performance regressions, improvements,
and algorithm diﬀerences over the course of two years of development.

2
2
0
2

r
p
A
8

]
E
C
.
s
c
[

1
v
1
2
3
4
0
.
4
0
2
2
:
v
i
X
r
a

Preprint

 
 
 
 
 
 
1

Introduction

1.1 Motivation

The Greenland and Antarctic ice-sheets contain the largest reserves of fresh water on Earth and
have the greatest potential to cause changes in future sea level. The Special Report on the Ocean
and Cryosphere in a Changing Climate (SROCC) [42] pointed to ice-sheets as one of the dominant
contributors to rising and accelerating global mean sea level and stated that sea level will continue
to rise for centuries due to mass loss from ice-sheets. Projections for future sea-level rise are
dependent on the ability of ice-sheet models (ISMs) to simulate ice sheet mass loss, via a wide
range of processes and instabilities, but major uncertainties in ice-sheet dynamics currently exist
[42, 44].

In their 2007 Fourth Assessment Report, the Intergovernmental Panel on Climate Change
(IPCC) deﬁned clear deﬁciencies with ISMs ability to accurately capture dynamic processes and
generally did not include these models when estimating future sea level rise [57]. Ice-sheet mod-
eling improved dramatically with progress from many community supported ice-sheet models
[13, 24, 35, 63, 75] and the IPCC’s Fifth Assessment Report noted increased use of ISMs in climate
models; however, major uncertainties remained [22]. Since then, there have been many studies
which use multiple computational models to narrow the uncertainty but increasing computational
cost remains a limiting factor [20, 66, 36, 25, 47]. The SROCC noted that high resolution simula-
tions without model simpliﬁcations are ultimately needed to obtain accurate projections of future
global mean sea level [42].

The increasing ﬁdelity and resolution of ISMs pose signiﬁcant computational challenges and
demand their adoption of modern software techniques. This paper focuses on performance portable
methods and optimizations that can be used to improve and maintain scalable performance in the
presence of changing models, software and hardware.

1.2 Performance portability

High resolution simulations of ice-sheet dynamics require a massive amount of memory and compu-
tation from large High Performance Computing (HPC) clusters, which coincidentally have under-
gone a dramatic change over the past decade. The current list of fastest supercomputers [55] shows
a diverse set of computing architectures, which typically include processors and accelerators from a
variety of vendors. Software portability across these architectures is important for productivity, as
the life cycle of a code base is typically much longer than the life cycle of individual supercomputers.
Heterogeneous compute nodes are also prevalent in new systems and will continue to be a chal-
lenge for software developers as the HPC community moves towards exascale [29]. The current
strategy for HPC compute nodes utilizes two architectures, CPUs and GPUs, with fundamentally
diﬀerent design philosophies, to achieve high performance. CPUs are designed to minimize latency
and are best utilized for sequential code performance while GPUs are designed to maximize memory
and computational throughput in the presence of a suﬃciently large and parallelizable work load.
The latest CPUs also include vector processing units which can further improve computational
In scientiﬁc computing, there are often many design choices that must be consid-
throughput.
ered while developing software, and heterogeneous computing adds another design variable when
choosing the best algorithms. Though it may be tempting to construct a highly optimized imple-
mentation for current HPC systems, this type of software development will become increasingly
harder to maintain as future models, software and hardware become increasingly more complex.
This motivates the need for fundamental abstractions to be present at the application level during

Preprint

1

code development. In response to these ongoing challenges in HPC, performance portability has
grown to become crucial for simulating physical phenomena at high resolutions.

Even with the urgency of the challenge, there is still no consensus on a clear deﬁnition for
the term “performance portability” [40, 50]. In general, performance portability for an applica-
tion means that a reasonable level of performance is achieved across a wide variety of computing
architectures with the same source code. Here, “performance” and “variety” are also admittedly
In [51, 52], performance portability is quantiﬁed through eﬃciencies based on both
subjective.
In [76], this is extended
application and architecture performance for a given set of platforms.
to include the “Rooﬂine” model which captures a more realistic set of empirically determined
performance bounds. Since this paper focuses more on application level improvements, herein per-
formance portability will be characterized by application execution time and scalability eﬃciencies
for multicore/manycore processors and GPUs.

There have been a number of approaches to performance portability for application developers
including directives such as OpenMP and OpenACC, and frameworks such as Kokkos [19, 72],
RAJA [32] and OCCA [39]. Performance portability for ﬁnite element analysis has also been
executed on a variety of diﬀerent software packages including Hiﬂow3 [2] and Firedrake/FEniC-
S/PyOP2 [60, 38, 61].

As ISM codes evolve to be more robust, accurate, performant and portable on the latest HPC
systems, a heavier burden is placed on software developers to support and improve functionality.
Maintaining developer productivity is crucial for delivering on scientiﬁc discovery. Unfortunately,
productivity is diﬃcult to quantify, with a wide range of possible metrics [23]. For scientiﬁc software
development, version control and automated testing has been challenging to integrate [34, 49], but
have shown to be eﬀective methods for improving productivity.

Automated testing becomes even more important as performance portable libraries and frame-
works improve and expand their capabilities. As discussed in [50], these libraries and frameworks
strive to improve developer productivity by reducing programming complexity and the need for
platform-speciﬁc tuning while maintaining and improving performance. Staying up-to-date with
these libraries and frameworks becomes crucial for maintaining an active code base which utilizes
the latest HPC machines; however, maintaining performance portability in the presence of active
development can be a diﬃcult task. Small changes within a compiler, library, architecture, or code
can cause dramatic changes to performance and performance deﬁciencies can be diﬃcult to identify
retroactively. Automated performance testing oﬀers a means to improve productivity by reducing
the time it takes to identify performance regressions and improvements to performance portability.

1.3 Previous related work

Traditionally for HPC, ISM codes relied solely on Message Passing Interface (MPI) libraries to
achieve performance on supercomputers. MPI focuses on distributed memory parallelism, where
memory may need to be communicated across multiple compute nodes.
In [24], a 60% weak
scalability eﬃciency is computed on a set of Greenland ice-sheet meshes for the full Stokes solver
in Elmer/Ice using 168 to 1092 cores. The computational component of the hybrid “SSA+SIA”
model in PISM is found to scale well on up to 1024 cores in [17] but the I/O component is found
to scale poorly. In [21], low-overhead performance instrumentation is developed for the “Blatter-
Pattyn” model in ISSM and good scaling is found on up to 3072 cores. The study ﬁnds that matrix
assembly and I/O begins to scale poorly and highlights the importance of continuous performance
monitoring.

A code with MPI-only is not able to take advantage of the computational throughput available
on shared memory architectures including compute nodes with dedicated GPUs. GPUs can pro-

Preprint

2

vide substantial performance improvements to existing ISMs if properly utilized. In [8], a CUDA
implementation of the “iSOSIA” approximation is used to show that higher-order ice ﬂow models
can be signiﬁcantly accelerated with NVIDIA GPUs. FastICE is introduced in [58] as a parallel,
GPU-accelerated full Stokes solver developed in CUDA, which utilizes a matrix-free method with
pseudo-transient continuation. This is extended to a portable framework written in Julia in [59]
and a parallel eﬃciency over 96% on 2197 GPUs is achieved.

In this work, the velocity solver in MALI, which uses the “Blatter-Pattyn” model formulation,
is extended to be performance portable using a multigrid preconditioned, Newton-Krvylov method
where extensive improvements have been made to matrix assembly performance. The performance
and scalability of MALI and its velocity solver is analyzed on multiple architectures including
Intel Knights Landing (KNL) and NVIDIA V100 GPUs. The testing framework in MALI is also
extended to include performance monitoring with automated detection of performance regressions
and improvements using a unique changepoint detection method.

MALI (MPAS-Albany Land Ice, [31]) is an ice-sheet model built on top of two main libraries: the
MPAS (Model for Prediction Across Scales) library [62], written in Fortran and used for developing
variable-resolution Earth system model components, and Albany, a C++ ﬁnite element code for
solving partial diﬀerential equations [64]. The performance of MALI is dominated by the solution
of the ﬁrst-order approximation to the Stokes equations (hereafter simply ﬁrst-order velocity or
ﬁrst-order, see Section 2 below); hence, the performance portability eﬀorts described in this work
have been mainly targeting the C++ implementation of these equations in Albany. We note that
MALI can model several additional physical processes including the ice temperature evolution,
subglacial hydrology and iceberg calving [31].

Albany uses high level abstractions to integrate Trilinos libraries [28] and the Kokkos program-
ming model [19, 72] for performance portable code across a variety of diﬀerent architectures. Albany
follows an “MPI+X” programming model, where MPI is used for distributed memory parallelism
and the Kokkos library is used for shared memory parallelism. Kokkos provides abstractions for
parallel execution and data management of shared memory computations in order to obtain optimal
data layouts and hardware features, reducing the complexity of the application code. The perfor-
mance portable implementation in Albany is described in detail in [16] where the authors highlight
ﬁnite element assembly performance for Aeras, the atmospheric dynamical core implemented in
Albany.

Albany Land Ice (ALI) is ﬁrst introduced in [68] under the name Albany/FELIX. In [69], the
scalability of the multigrid preconditioned, velocity solver is analyzed on up to 1024 cores. An
initial study on the performance portability of the ﬁnite element assembly showed deﬁciencies in
distributed memory assembly on GPU architectures in [74] but performance and scalability was
reasonable among diﬀerent architectures.

In this paper, we highlight recent improvements to ﬁnite element assembly that eliminate pre-
vious deﬁciencies. We also begin analyzing a new, performance portable velocity solver in ALI and
expand our performance analysis to MALI.

1.4 Main contributions

The MALI code was developed in response to the growing challenges in developing a more accurate
and eﬃcient ISM [31]. In this paper, the performance portable features of MALI are introduced
and analyzed on the two supercomputing clusters: NERSC Cori and OLCF Summit. A change-
point detection method is also introduced and tested for automated performance testing on next
generation architectures. The main contributions of this work are summarized as follows:

Preprint

3

• Insights into the development of a performance portable, ﬁnite element code base using high-

level abstractions from Trilinos libraries and the Kokkos programming model.

• A description of new, performance-enhancing features introduced in MALI and an analysis
demonstrating the expected improvements on diﬀerent HPC architectures, including Intel
Knights Landing (KNL) and NVIDIA V100 GPUs.

• A weak scalability study and a demonstration of speedup over CPU-only simulations.

• Insights into the development of a changepoint detection method for automated performance
testing and demonstrations of tracking performance regressions, improvements, and diﬀer-
ences between algorithms.

The methods introduced focus on improving performance portable modeling in MALI, but are
extensible to other applications targeting HPC.

The remainder of this paper is organized as follows. Section 2 introduces the ice-sheet equations
relevant to our analysis. Section 3 gives a detailed overview of how these equations are implemented,
solved and veriﬁed in MALI. In Section 4, the methods used to achieve, improve and maintain per-
formance portability in MALI are introduced. Lastly, Section 5 provides three numerical examples
that demonstrate the expected performance of MALI on HPC systems and the utility of automated
performance testing.

2 The governing ice-sheet equations

In this section, the main equations governing the ice-sheet dynamics are brieﬂy discussed. The
section begins with a description of the ﬁrst-order velocity equations and is followed by a description
of the mass continuity equations. More information can be found in [31, 68]. In this work, we will
always assume a “topologically extruded” ice-sheet geometry, meaning that the ice-sheet geometry
can be obtained by vertically extruding the two-dimensional (2D) basal area, according to the
local ice thickness. A consequence of this assumption is that the margin of the ice-sheet is always
vertical, though the ice thickness is typically small at grounded glacier termini.

At the ice-sheet scale, ice behaves as a highly viscous, shear-thinning, incompressible ﬂuid and
can be modeled by nonlinear Stokes equations. In this paper, a ﬁrst-order approximation [18, 65]
of the Stokes equations is considered, often referred to as the “Blatter-Pattyn” model [6, 43] or the
“ﬁrst-order” model. The model is quasi-static with static velocity (momentum balance) equations
coupled to a dynamic thickness (mass) equation. In conservative form, the three-dimensional (3D),
ﬁrst-order velocity equations are written as,

−∇ · (2µe ˙(cid:15)1) + ρg

−∇ · (2µe ˙(cid:15)2) + ρg

∂s
∂x
∂s
∂y

= 0,

= 0,

(1)

where x, y and z are spatial coordinates bounded by the ice domain Ω, ρ is the ice density, g is
gravitational acceleration and s ≡ s (x, y) is the upper surface of the domain. The strain rates in
Equation (1) are deﬁned as the vectors,

˙(cid:15)1 = [2 ˙(cid:15)xx + ˙(cid:15)yy, ˙(cid:15)xy, ˙(cid:15)xz]T ,
˙(cid:15)2 = [ ˙(cid:15)xy, ˙(cid:15)xx + 2 ˙(cid:15)yy, ˙(cid:15)yz]T ,

(2)

Preprint

4

where the components of the approximate strain rate tensor can be written as,

˙(cid:15)xx =

∂u
∂x

,

˙(cid:15)yy =

∂v
∂y

,

˙(cid:15)xy =

(cid:18) ∂u
∂y

1
2

+

(cid:19)

,

∂v
∂x

˙(cid:15)xz =

1
2

∂u
∂z

,

˙(cid:15)yz =

1
2

∂v
∂z

,

(3)

and u and v are the ice velocity components in the direction of x and y, respectively.

The eﬀective viscosity, µe, in Equation (1) is derived from Glen’s ﬂow law [14, 41] and is written

as,

µe =

1
n −1
A− 1
n ˙(cid:15)
e

,

1
2

(4)

where n is Glen’s power law exponent and ˙(cid:15)e is the eﬀective strain rate, given by

yy + ˙(cid:15)xx ˙(cid:15)yy + ˙(cid:15)2
The ﬂow law rate factor in Equation (4) is strongly temperature dependent and is determined
through an Arrhenius relation,

e = ˙(cid:15)2
˙(cid:15)2

xx + ˙(cid:15)2

xy + ˙(cid:15)2

xz + ˙(cid:15)2

yz.

(5)

A = A0 exp

−

,

(6)

(cid:18)

(cid:19)

Q
RT ∗

where A0 is a constant of proportionality, Q is the activation energy for ice creep, T ∗ is the ice
temperature corrected for the pressure melting point and R is the universal gas constant.

The boundary conditions are best described by partitioning the surface of the 3D ice-sheet

domain into upper, lower and lateral surfaces,

Γ = Γs ∪ Γβ ∪ Γl,

(7)

where Γs is the upper surface, Γβ is the lower surface and Γl is the lateral surface. The boundary
conditions can then be deﬁned as:

1. a homogeneous boundary condition on the upper surface (atmosphere pressure is neglected),

˙(cid:15)1 · n = 0,

˙(cid:15)2 · n = 0,

on Γs,

(8)

where n is the outwards facing normal vector;

2. a Robin boundary condition on the lower surface, representing a linear sliding law at the bed,

2µe ˙(cid:15)1 · n + βu,

2µe ˙(cid:15)2 · n + βv,

on Γβ,

(9)

where the basal sliding coeﬃcient β ≡ β (x, y) is non-negative where the ice is grounded and
zero where the ice is ﬂoating;

3. a dynamic Neumann boundary condition at the ice margin accounting for the back pressure
from the ocean where the ice is submerged (note that by convention z = 0 represents the sea
level),

2µe ˙(cid:15)1 · n − ρg (s − z) n = ρwg max (z, 0) n,
2µe ˙(cid:15)2 · n − ρg (s − z) n = ρwg max (z, 0) n,

on Γl,

(10)

where ρw is the density of water and z is the elevation above sea level.

The steady velocity equations described above are coupled to a dynamic equation for the con-
servation of mass. Speciﬁcally, as the ice-sheet evolves in time, mass continuity is enforced through
the following equation,

∂H
∂t

+ ∇ · (H ¯u) = ˙a + ˙b,

(11)

where H is ice thickness, t is time, ¯u is depth-averaged velocity vector, ˙a is surface mass balance
and ˙b is basal mass balance. The thickness equation is then used to evolve the geometry in time.
The ice temperature is held constant in time.

Preprint

5

3

Implementation in MALI

In this section we describe how the governing equations introduced in Section 2 are discretized and
implemented in MALI, focusing in particular on the C++ velocity solver model in ALI. We ﬁrst
give a high-level overview of the implementation and then provide a detailed description for the
two more computationally expensive components relevant to the paper: the ﬁnite element assembly
and the preconditioned linear solver. A brief description of the MALI testing framework follows.

3.1 Overview

The ice thickness H in Equation (11) is discretized in MPAS with a upwind ﬁnite volume method
on an unstructured, two-dimensional, Voronoi grid [31]. At every time step, the ﬁrst-order velocity
equations (Section 2) are solved in ALI [68]. Brieﬂy, the equations are discretized using low-order
nodal prismatic ﬁnite elements on a 3D mesh extruded from a triangulation dual to the MPAS
Voronoi mesh. The discrete version of the velocity equations can be written in the compact form

F(U ; {φi}, {∇φi}, H, β, . . .) = 0.

(12)

Here U is the solution vector, containing the values of ice velocity at the mesh nodes. {φi} and
{∇φi} are the sets of the basis functions and their gradients. F is a vector function of the solution
U . F also depends on the basis functions and on ﬁelds like ice thickness H and basal friction β.
We refer to F(U ; ·) as the residual.

A damped Newton’s method is used to solve the nonlinear discrete system (12):

Here J k

F :=

∂F
∂U

(cid:12)
(cid:12)
(cid:12)
(cid:12)U =U k

F δk+1
J k

U = −F(U k),

U k+1 = U k + αk δk+1
U .

(13)

is the Jacobian matrix and αk is the damping factor. At each nonlinear

iteration k, the linear system (13) is solved with the GMRES method using the “matrix-dependent
semicoarsening-algebraic multigrid” (MDSC-AMG) [73] preconditioner.

Figure 1 shows a ﬂow chart of ice-sheet dynamics in MALI, focusing on high-level components
in the velocity solver in ALI. Each node of the ﬂow chart is described below with references to
relevant Trilinos packages:

• Import: Imports the ice velocity solution, U , from a nonoverlapping data structure where
each MPI rank owns a unique part of the solution to an overlapping data structure where
some data exists on multiple ranks. This gives each rank access to relevant solution data
without any further communication. This is performed by Trilinos/Tpetra [3].

• Gather: Gathers solution values from an overlapping data structure to an element local
data structure where data is indexed according to element and local node. This process also
includes the gathering of geometry and ﬁeld data from MPAS. This is constructed using
Trilinos/Phalanx [45, 46, 64] and Trilinos/Kokkos [19, 72].

• Interpolate: Interpolates the solution and solution gradient from nodal points to quadra-
ture points. Other ﬁeld variables also require interpolation. This is constructed using Trili-
nos/Phalanx and Trilinos/Kokkos.

• Evaluate: Evaluates the residual, Jacobian and source terms of the ﬁrst-order equations.
These operators are templated in order to take advantage of automatic diﬀerentiation for
analytical Jacobians using Trilinos/Sacado [54]. This also uses Trilinos/Phalanx and Trili-
nos/Kokkos.

Preprint

6

Figure 1: A ﬂow chart depicting the workﬂow in MALI, focusing on high-level components in
the velocity solver in ALI. The shapes of the nodes are constructed to show performance relevant
characteristics of the code while the colors are used to diﬀerentiate high-level abstractions. In this
case, ellipses represent portions of code with MPI communication and rounded rectangles represent
portions of code which do not require MPI communication. The conditionals, LC (Linear solver
converged) and NC (nonlinear solver converged), are represented with diamonds. The red-orange
node represents the computation required for explicit time stepping of ice thickness, H, in MPAS,
the purple node represents preconditioner construction (PC), and the pink nodes represent the
linear solver required to solve (13). Finite element assembly begins and ends with distributed
memory assembly colored in yellow but also performs shared memory processes colored in blue.

Preprint

7

LC?Prec*xOp*xPCExportScatterEvaluateInterpolateGatherImportNC?MPASNoYesNoYesUUHResidualJacobianAlbanyLandIceMPAS-AlbanyLandIce• Scatter: Scatters residual and Jacobian values from an element local data structure to an
overlapping data structure. This is constructed using Trilinos/Phalanx and Trilinos/Kokkos.

• Export: Exports the residual and Jacobian from an overlapping data structure to a nonover-
lapping data structure where information is updated across MPI ranks. This global structure
allows for eﬃcient use of linear solvers and is performed by Trilinos/Tpetra.

• PC (Preconditioner Construction): Constructs the MDSC-AMG preconditioner from

the Jacobian matrix and is performed by Trilinos/MueLu [5] and Trilinos/Ifpack2 [56].

• Prec*x: Applies the preconditioner to the solution vector of the linear system and is per-

formed by Trilinos/Belos [4], Trilinos/MueLu, Trilinos/Ifpack2.

• Op*x: Applies the Jacobian matrix to the solution vector of the linear system and is per-

formed by Trilinos/Belos.

• LC? (Linear Solver Converged?): The linear solver loop is converged when a ﬁxed linear
tolerance or a maximum number of iterations is reached. This is managed by Trilinos/Belos.

• NC? (Nonlinear Solver Converged?): The nonlinear solver loop is converged when a

ﬁxed nonlinear tolerance is reached. This is managed by Trilinos/NOX [70].

• MPAS: Once the ice velocity, U , has fully converged, it is interpolated to MPAS cell edges
and used to update H using forward Euler. The new ice-sheet geometry is passed back into
Albany Land Ice to re-compute U for the next time step. This process is performed until a
ﬁnal time step is reached. A more detailed description of thickness evolution in MPAS can
be found in [31].

By constructing high-level abstractions for solving nonlinear partial diﬀerential equations and utiliz-
ing Trilinos packages as components, application developers are able to apply existing performance
portable algorithms which are actively supported and improved by experts. Developers are also able
to utilize the same algorithms on multiple applications, allowing for greater impact and increased
sustainability.

3.2 Finite element assembly

The ﬁnite element approach implemented in Albany is designed to easily incorporate multiple
physics models with graph-based evaluation using the Trilinos/Phalanx package [45, 46, 64]. The
assembly is decomposed into a set of nodes called “evaluators”. Evaluators have a speciﬁed set of
inputs and outputs and are organized in a directed acyclic graph (DAG) based on dependencies.
Figure 2 shows a simpliﬁed example of a DAG for the for ﬁnite element assembly in Albany. The
inherent advantage of using a DAG is the increased ﬂexibility, extensibility and usability of using
modular evaluators when performing ﬁnite element assembly. The DAG also provides potential for
task parallelism. The disadvantage of using a DAG is that there is a potential for performance
loss through code fragmentation and a static graph can also lead to repetition of unneeded data
movement and computation. This is discussed in more detail in Section 4.1.

Albany utilizes automatic diﬀerentiation to compute the Jacobian matrix and enable Newton-
like methods for the solution of nonlinear partial diﬀerential equations. More in general, the ﬁnite
element assembly in Albany is designed for embedded analysis using template-based generic pro-
gramming [45, 46, 64]. Embedded analysis, such as derivative-based optimization and sensitivity

Preprint

8

Figure 2: Albany uses a directed acyclic graph (DAG) for ﬁnite element assembly. In this example,
a global residual is constructed by interpolating the solution and a parameter onto quadrature
points. Basis functions are also needed to complete the interpolation which depend on physical
coordinates. In the case of the ﬁrst-order equations, the solution is the ice velocity. One example
of a parameter could be the surface height.

Preprint

9

UGatherSolutionInterpolateSolutionResidualScatterGatherCoordinatesBasisFunctionsxInterpolateParameterGatherParameterpanalysis, for partial diﬀerential equations requires construction of mathematical objects such as Ja-
cobians, Hessian-vector products and derivatives with respect to parameters. Albany utilizes C++
templates and operator overloading to perform automatic diﬀerentiation using the Trilinos/Sacado
package [54]. Sacado provides multiple data types for storing the derivative components, each with
their own relative advantages and disadvantages. The DFad options sets the number of deriva-
tive components at runtime, and is hence the most ﬂexible but least eﬃcient option. The SLFad
options sets the maximum number of derivative components at compile time, making this option
both ﬂexible and relatively eﬃcient. The most eﬃcient but least ﬂexible option is SFad. For this
option, the number of derivative components is set at compile time. In MALI, we generally select
the SFad type whenever possible, so as to achieve the best possible performance. The diﬀerence in
performance with the various options is most profound in a GPU run, due to the substantial cost
of performing dynamic allocation on the GPU.

Finite element evaluators also contain Kokkos parallel execution kernels for performance porta-
bility on shared memory architectures [16, 74]. Kokkos utilizes memory and execution spaces to
determine where memory is stored and where code is executed. Phalanx evaluators utilize MDField
with Kokkos View for memory management and evaluators can be used as a Kokkos functor to per-
form parallel operations. Sacado operators have also been designed to work with Kokkos. Figure 3
shows a simpliﬁed example of an evaluator with Kokkos.

Figure 3: The ﬁrst-order local residual computation is performed in a Phalanx evaluator that uses
Kokkos for shared memory parallelism and Sacado for automatic diﬀerentiation. This is a simpliﬁed
example of the computation of a single term in the residual. When called with a double EvalT
type, the routine returns the residual; when called with a Fad EvalT type, automatic diﬀerentiation
is applied and a Jacobian is returned.

The Phalanx evaluation type is passed via the template parameter EvalT and dictates whether
a residual with a double type or a Jacobian with a Fad type is computed. A Kokkos RangePolicy
is used to parallelize over cells over an execution space, ExeSpace. In this paper, only the Serial
and Cuda executions spaces are used to diﬀerentiate between CPU and GPU execution but other

Preprint

10

execution spaces are also available. The properties of each case are described in more detail in
[16, 74]

3.3 Preconditioner for linear solve

A primary challenge in simulating ice-sheets at scale is solving the linear system associated with a
thin, high-aspect ratio mesh. It has been shown [10, 33, 73] that multigrid methods can be used to
address the challenges arising due to the anisotropic nature of the problem, although alternative
methods have been proposed [12, 27]. The performance of the linear solver is thus primarily dictated
by the eﬃcacy of the multigrid preconditioner. The MDSC-AMG preconditioner introduced in [73]
is speciﬁcally designed for ice-sheet meshes where a mesh is ﬁrst constructed from 2D topological
data and extruded in the vertical direction to construct a 3D mesh. The primary strategy for the
multigrid hierarchy is to coarsen the ﬁne mesh in the vertical direction until a single layer is reached
and apply smoothed aggregation algebraic multigrid (SA-AMG) on the plane. Here, we are able
to take advantage of the performance portable implementations of SA-AMG and point smoothers
implemented in Trilinos/MueLu and Trilinos/Ifpack2.

3.4 Testing

Software quality tools are a central part of the Albany code base and are crucial for developer
productivity [64]. Rather than using a ﬁxed release of Trilinos, ALI is designed to stay up-to-date
with Trilinos’ version of the day, to ensure that the code inherits the most up-to-date additions
and improvements to Trilinos. This requires a close collaboration between Albany and Trilinos
developers and ensures rapid response to issues that might arise. The current nightly test harness
includes unit, regression and performance tests on Intel and IBM multicore CPUs and NVIDIA
GPUs and is monitored on a dedicated dashboard.

4 Methods for improving and maintaining performance portabil-

ity

In this section, we discuss the major enhancements made to MALI to both improve and maintain
performance portability. We provide three examples of ﬁnite element assembly optimizations, which
improved performance on both CPU and GPU systems. Memoization is utilized to avoid unneces-
sary data movement and computation from the MALI workﬂow. Optimizations in matrix assembly
and boundary condition computation led to signiﬁcant speedups on both CPU and GPU and a large
reduction in memory usage. We also provide a brief description of a new, performance portable
MDSC-AMG preconditioner implemented in Trilinos/MueLu and tuned for ice-sheet modeling.
Lastly, we provide a description of an automated performance testing framework for identifying
regressions, improvements and performance diﬀerences between algorithms.

4.1 Memoization

A static DAG similar to the one shown in Figure 2 is executed when a new global residual or
Jacobian is needed within the nonlinear solver. This leads to a repetition of unnecessary data
movement and computation when input quantities do not change between calls of the DAG. A
performance gain can be achieved by storing the results of expensive nodes in the DAG and returning
the stored results when input quantities do not change. This process is known as memoization.

Preprint

11

In Albany, memoization is implemented by constructing a new DAG which only follows changes
caused by the solution. Figure 4 shows an example of the new DAG created by performing memoiza-
tion in Figure 2. The ﬁrst call executes the original DAG while storing all intermediate quantities.

Figure 4: Albany uses memoization to create a new DAG which only depends on changes to
the solution. This avoids unnecessary data movement and computations when parameters and
coordinates do not change. For examples, in the case of ﬁrst-order equations, the surface height
does not change at each nonlinear iteration.

Then, the new DAG is called by default while the original DAG is only called when there is a
change to a parameter or the coordinates. An initial speedup of roughly 1.4 on CPUs and 1.5
on GPUs was found when analyzing ﬁnite element assembly performance relative to the assembly
without memoization.

4.2 Jacobian matrix

As discussed in Section 3, the ﬁnite element assembly of residual and Jacobian is performed on
an “overlapped” distribution of the DOFs, while linear systems require a “unique” distribution of
DOFs.

Until recently, Albany was using two separate Tpetra CrsMatrix objects for the Jacobian: an
overlapped version for ﬁnite element assembly, and a non-overlapped version for linear solvers. An

Preprint

12

UGatherSolutionInterpolateSolutionResidualScatterStoredFieldStoredFieldExport operation (involving MPI communication) was used to copy data between the overlapped
and the unique matrices, migrating oﬀ-processor rows to their owner.

We improved this portion of the library by switching to the new Tpetra FeCrsMatrix objects,
which can store overlapped and non-overlapped matrix in a single object, by storing the “owned”
rows ﬁrst, followed by the oﬀ-processor ones. This arrangement allows to build the non-overlapped
matrix as a “subview” of the overlapped one. The beneﬁt is twofold: the memory footprint for
the Jacobian is roughly halved, plus no copy is needed to transfer data for the local rows from the
overlapped Jacobian to the non-overlapped Jacobian. This translated to a speedup of roughly 1.1
on CPUs and 2.1 on GPUs when analyzing ﬁnite element assembly performance relative to the old
implementation.

4.3 Boundary conditions

In order to achieve high performance on GPUs, ﬁelds corresponding to boundary data needed to be
aligned for coalesced access on the device. The process is described in detail in [11] for a diﬀerent
problem and is summarized in Figure 5. Originally, boundary data were stored using the same

Figure 5: Boundary data were originally stored as a volume ﬁeld combined with a mapping data
structure for accessing appropriate boundary cells. It is now aligned to match one-to-one with the
side set map and can be read in a coalesced fashion on the device.

layout as a volume ﬁeld with a data structure that contained a list of indices corresponding to
cells that belong to the boundary. This eﬀectively meant that each thread in a device block was
loading data from non-consecutive locations in memory which is a highly ineﬃcient access pattern
for GPUs. By aligning boundary data to match the layout of the side set mapping data structure,
all boundary ﬁelds are now read eﬃciently within a device kernel. Modifying the boundary data
layouts had the additional beneﬁt of signiﬁcantly reducing memory usage for both CPU and GPU.
A speedup of roughly 1.2 and 8.7 was achieved on CPUs and GPUs, respectively, when analyzing
ﬁnite element assembly performance relative to the old implementation.

Preprint

13

012345678012345678012345678012340123401234Old Boundary FieldSide Set MapVolume FieldNew Boundary FieldSide Set MapVolume Field4.4 Matrix dependent semicoarsening algebraic multigrid

Performance portable SA-AMG is provided by Trilinos/MueLu and is activated by using the Kokkos
version of each component. This was extended to also include performance portable matrix depen-
dent grid transfers for semicoarsening to complete the performance portable MDSC-AMG precon-
ditioner introduced in Section 3.3.

The Kokkos version of MDSC (MDSC-Kokkos) uses Kokkos View as temporary data structures
while assembling the prolongation matrix. Kokkos parallel_for is used to ﬁll the contribution
from each vertical line in parallel. A block tridiagonal system is assembled for each coarse layer in
a vertical line on the ﬂy. The system is then solved inline within the kernel using KokkosBatched
SerialLU from Kokkos Kernels [71] where each thread performs an LU factorization without
pivoting. The performance portable implementation is designed speciﬁcally as a batched direct
solver for many small matrices and, in this case, additional optimizations are not needed. Once the
solution is obtained, it is placed directly in the prolongation matrix.

Three performance portable point smoothers are provided by Trilinos/Ifpack2: MT Gauss-
Seidel, Two-stage Gauss-Seidel, and Chebyshev. An autotuning framework using random search
via Scikit-learn [48] is developed and used to determine the best smoother parameters for a small
ALI test problem. We found that the performance portable smoothers did not outperform the serial
line and point Gauss-Seidel smoothers in CPU simulations but Chebyshev smoothers performed
the best in GPU simulations. Thus, the best CPU simulations continue to use the original line and
point Gauss-Seidel smoothers while GPU simulations utilize the Chebyshev smoothers.

4.5 Automated performance testing

Performance tests are constructed as an extension of nightly regression tests. For example, a
regression test might compute the steady-state solution of Equation (1) on a coarse Greenland
mesh and compare computed surface velocities with known surface velocities. A performance
test would perform the same calculation but also compare the end-to-end wall-clock time of the
simulation to a speciﬁed value. Unfortunately, HPC clusters regularly exhibit large variations in
performance causing performance tests to fail without any changes to the software. Thus, this
method of performance testing is rarely used and changes in performance can go unnoticed for
weeks or months.

A fundamental problem in maintaining performance tests is the ability to assess variations
in performance on HPC systems. This requires a statistical approach to determine performance
regressions and improvements. This is exempliﬁed in [30] where the authors provide methods of
measuring and reporting performance on HPC systems. Performance regressions, or performance
degradation in software execution, can occur through various mechanism, including changes in
compilers, third party libraries, hardware, or software. As the number of developers of a scientiﬁc
software stack grows, the likelihood of performance regressions increase. This is addressed by
developing a framework that automatically collects performance metrics and applies a changepoint
detection method to the data to detect changes in performance during nightly testing.

Changepoint detection is well researched in many ﬁelds [1, 9, 67, 15] and is the process of ﬁnding
abrupt variations in time series data. A changepoint detection method performs hypothesis testing
between the null hypothesis, H0, where no change occurs and the alternative hypothesis, HA, where
a change occurs. Given the performance metric time series,

X = {x1, x2, . . . , xn} ,

(14)

where n is the number of historical samples collected while testing for a performance metric, x, a

Preprint

14

subset of X can be deﬁned as,

X j

i = {xi, xi+1, . . . , xj} ,

(15)

where i and j are the lower and upper limits of the time series, X j
formulated as,

i . Two families of hypotheses are

H0 : f ν−1
HA : f ν−1

1 = f n
ν ,
(cid:54)= f n
ν ,

1

∀ν ∈ K,

ν ∈ K,

(16)

is the probability density function ∀x ∈ X j

where f j
i and K = {2, 3, . . . , n}. This can be viewed
i
as a generalized likelihood ratio test where H0 states that all x ∈ X belong to a single probability
distribution while HA states that there exists some ν such that all x ∈ X ν−1
ν belong
to two separate probability distributions, respectively [26].

and x ∈ X n

and X n

A two-sample t-test of X ν−1

ν is performed to determine whether ν is a potential change-
point. In order to perform multiple hypothesis tests, the Bonferroni correction [7] is used to adjust
the signiﬁcance level by α/k where α is the desired signiﬁcance level and k = n − 1 is the number
of tests. This correction is known to be overly conservative for large numbers of tests so only
the largest changes in the time series are chosen. The pseudocode for this method is shown in
Algorithm 1.

1

1

Algorithm 1 The single changepoint detection algorithm performs k hypothesis tests on the time
series X n
1 using the signiﬁcance level α. The time series is assumed to be roughly normal. The
result is a set of changepoints, C, and t-statistics, S. ArgSortDesc() performs an indirect sort from
largest to smallest values and returns an array of indices that would sort the array. This is used to
ﬁnd the k largest jumps.
1: function Changepoint(X n
2:

1 , α, k)

C = []
S = {}
t∗ = t

1−α/(2k)
n−2

4:
(cid:12)
5: K = 1 + ArgSortDesc(
(cid:12)X n
6:

for ν ∈ Kk

2 − X n−1

1

(cid:12)
(cid:12))

1 do
t = T-Test(X ν−1
1
if |t| > t∗ then
C ← [C, ν]
S ← S ∪ {ν → t}

, X n
ν )

return C, S

3:

7:

8:

9:

10:

A performance metric time series is likely to contain multiple changepoints. A sequential method
is used to diﬀerentiate the time series based on previously identiﬁed changepoints. Once a change-
point is detected, the method disregards any data prior to the changepoint. This ensures that
changepoints are not retroactively changed as new data are introduced.

Due to the large variation in HPC systems, the time series data may also contain outliers which
can be identiﬁed as changepoints. Multiple methods are used to ensure that the changepoints are
accurate in the presence of outliers:

1. In any single t-test, outliers are identiﬁed on both distributions using the median absolute
deviation [37] with a threshold comparable to three standard deviations. We remove at most
10% of the total data.

2. A minimum number of consecutive detections, m, of the same changepoint are needed before

conﬁrming a changepoint. This helps dilute the inﬂuence of an outlier.

Preprint

15

3. As the time series is traversed sequentially, the sample size or “lookback window” for each
test is limited by w observations. This helps avoid hypersensitivity where the smallest change
in the time series becomes signiﬁcant when the sample size is too large.

Algorithm 1 along with its modiﬁcations for multiple changepoint detection is implemented
in python and executed during nightly performance testing. The log-transformed values are used
because a log-normal distribution seems to ﬁt the data slightly better than a normal distribution.
A signiﬁcance level of α = 0.005 is chosen to ensure conﬁdence and only the k = 10 largest changes
are considered. m = 3 consecutive detections are needed to conﬁrm a changepoint and a lookback
window of w = 30 is chosen. This typically means that a minimum of three days are needed to detect
a changepoint but this depends on data variability. Daily results are reported on an automated
Jupyter notebook and posted online (see https://sandialabs.github.io/ali-perf-data/), and
performance regressions are reported through an automated email report.

Performance regressions and improvements are quantiﬁed by utilizing changepoints to deﬁne
subsets within the time series. Given a changepoint and two subsets, a 99% conﬁdence interval
(CI) for the diﬀerence in mean on log-transformed values is computed using a t-distribution. When
transformed back, a relative performance ratio (speedup or slowdown) is given for the regression
or improvement. Equation (17) shows an example of how the relative performance is computed,

log (X n
ν )
log (cid:0)X ν−1

1

(cid:1)

(cid:16)

= exp

log (X n

ν ) − log (cid:0)X ν−1

1

(cid:1)(cid:17)

,

(17)

where the overline represents an arithmetic mean. A similar technique is also used in Sections 5.1
and 5.2 to compute speedups, proportions and eﬃciencies with 99% conﬁdence intervals.

For performance comparisons, the diﬀerence in mean on log-transformed data between two
performance tests needs to be established. A paired t-test is performed by taking the diﬀerence
between the log-transformed data for the two tests where the dates intersect. The changepoint
detection method is used on this data to identify subsets and a 99% conﬁdence interval (CI) for
the diﬀerence in mean on log-transformed data is computed on each subset. This is also computed
as a relative performance ratio when transformed back.

5 Numerical results

In this section, the performance of MALI and standalone ALI is analyzed on two variable reso-
lution Greenland ice-sheet meshes and a series of increasing higher resolution Antarctic ice-sheet
meshes. In the ﬁrst Greenland case, MALI is compared with and without the features described
in Sections 4.1 and 4.4, and performance improvements are shown across all HPC architectures.
In the second case, a weak scalability study of Antarctica shows that simulations perform best
when utilizing the GPUs on modern HPC systems. In the last Greenland case, several examples
are given on how the changepoint detection method described in Section 4.5 is used to identify
performance regressions, improvements and diﬀerences in algorithm performance. What follows is
a brief description of the experimental setup.

The MALI code base consists of three open-source software projects that are continuously
updated through github repositories and tested nightly for performance and correctness on HPC
machines. Table 1 shows where the three projects currently exist and the commit ids used for
the performance experiments to follow. The code is compiled with the Kokkos Serial execution
space for CPU-only simulations and Cuda for simulations utilizing GPUs. CPU-only simulations
are executed with MPI ranks mapped to cores while GPU simulations are executed with MPI ranks

Preprint

16

Table 1: MALI software repositories

Software

Repository

Git Branch

Commit Id

MPAS
Albany
Trilinos

https://github.com/MALI-Dev/E3SM
https://github.com/sandialabs/Albany
https://github.com/trilinos/Trilinos

develop
master
develop

d6309858d9
9d292d8f5
155e45e86c2

mapped to GPUs. In all experiments, CUDA-Aware MPI is turned oﬀ and CUDA_LAUNCH_BLOCKING
is turned on.

The simulations are executed on the four architectures provided on the Cori and Summit su-
percomputers. A summary of each testing environment is provided in Table 2. Wall-clock time

Table 2: MALI simulations are executed on the three platforms or four architectures given below.
A limited number of cores are utilized on some systems in order to keep a core idle for system
operations. On Summit, an MPI-only simulation using only the CPU is tested along with an
MPI+GPU simulation.

Cori (HSW)

Cori (KNL)

Summit (PWR9,V100)

Name

CPU

Number of Cores
GPU
Node Arch.
Memory per Node

Intel Xeon E5-2698 v3
Haswell
16
None
2 CPUs
125 GiB

Intel Xeon Phi 7250
Knights Landing
68
None
1 CPU
94 GiB

CPU Compiler
GPU Compiler
MPI
Node Conﬁg.

Intel 19.0.3.199
None
cray-mpich 7.7.10
32 MPI

Intel 19.0.3.199
None
cray-mpich 7.7.10
64 MPI

IBM POWER9

22
NVIDIA Tesla V100
2 CPUs + 6 GPUs
604 GiB +
15.7 GiB/GPU
gcc 9.1.0
nvcc 11.0.3
spectrum-mpi 10.4.0.3
PWR9: 42 MPI
V100: 6 MPI

is captured by using MPAS and Trilinos/Teuchos timers to obtain an average time across MPI
processes. The relevant timers and their descriptions are shown in Table 3.

5.1 MALI Greenland ice-sheet 1-to-10 kilometer variable resolution case

Here we consider a variable-resolution grid of the Greenland ice-sheet, which is ﬁner in regions
with a more complex ﬂow structure, i.e. close to the margin and in regions where the observed
surface velocity is higher. The 2D grid with cell spacing ranging from 1km to 10km is extruded
in the vertical direction using 10 layers of variable thickness (thinner at the bed). The basal
sliding condition and temperature are pre-computed using an initialization approach that matches
the surface velocity observation while satisfying the ﬁrst-order velocity equations coupled to a
In this case, MALI is used to perform an initial state
steady state enthalpy equation [53, 27].
calculation and a single time step, leading to two nonlinear solves using ALI. The temperature is held
ﬁxed during the time step. The nonlinear and linear solver tolerances are 1 × 10−5 and 1 × 10−8,
respectively. Simulations are compared with and without the features described in Sections 4.1

Preprint

17

Table 3: The MALI timers described below are used to collect the average wall-clock time across
MPI processes.

Timer

Total Time
Total Solve
Total Fill

Preconditioner
Construction
Linear Solve

Description

Total simulation time reported by MALI
Total nonlinear solve time reported by ALI
Total ﬁnite element assembly time starting from the ice velocity import
and ending with the export of the residual and Jacobian
Total time constructing the MDSC-AMG preconditioner

Total time in the linear solver including the application of the precon-
ditioner

and 4.4. The cases are given in Table 4. Multiple samples are collected for each case using the

Table 4: The MALI Greenland ice-sheet 1-to-10 km variable resolution simulation is executed with
and without the speciﬁc features described below.

Case Name

Description

Baseline

Improvement

Finite element assembly without memoization, serial MDSC, default
smoother settings
Finite element assembly with memoization, MDSC-Kokkos, optimal
smoothers found through autotuning

same allocation and a mean error bar is computed using the method described in Section 4.5.
A two-sample t-test of the mean diﬀerence of the log is also performed to ensure diﬀerences are
statistically signiﬁcant. This is then used to compute a conﬁdence interval for the speedup of the
improvement relative to the baseline. The results for each timer are shown in Figure 6.

The Total Fill timer in Figure 6b is used to measure the improvement from memoization. The
variation from this timer is small and the performance improvement is clear across all architec-
tures. Larger improvements are seen on Cori. More variation is seen from the Preconditioner
Construction timer in Figure 6c which is used to measure the improvement from MDSC-Kokkos.
In this case, the speedup on Cori is not statistically signiﬁcant but the speedup on POWER9 shows
that there may be some beneﬁt on CPU architectures. The speedup on V100 GPUs is larger and
more signiﬁcant. Lastly, the Linear Solve timer in Figure 6d is used to measure the improvement
from tuning the GPU preconditioner. There is some variation and performance loss seen in the
linear solve on Haswell CPUs but it is not very signiﬁcant and the slowdown is not seen on the
other CPU architectures. Again, there is a statistically signiﬁcant speedup on V100 GPUs.

The performance of the linear solver is highlighted in Table 5.
Figure 6a shows the overall performance improvement in MALI from the addition of memo-
ization, MDSC-Kokkos and GPU preconditioner tuning. There is a statistically signiﬁcant perfor-
mance improvement across all architectures despite the large variation on Cori. Lastly, Figure 7
shows the proportions of total wall-clock for each architecture, case and timer. The plot shows that
on CPU platforms, Total Fill remains a signiﬁcant portion of total runtime. On GPU platforms,
the ﬁnite element assembly is much less signiﬁcant when compared to the linear solver and a large
portion of runtime is in other portions of the code. This includes the initial setup of the data

Preprint

18

(a) Total Time

(b) Total Fill

(c) Preconditioner Construction

(d) Linear Solve

Figure 6: The MALI Greenland ice-sheet 1-to-10 km variable resolution simulation is executed
multiple times on four architectures (Cori: HSW, KNL; Summit: PWR9, V100) and two cases in
order to capture improvements across four timers. Architectures, timers and cases are deﬁned in
Table 2, 3 and 4, respectively. The lower/upper quartiles are shown along with the median in a
box plot while a dashed line is used to show the full data range. The sample size is trimmed using
the methods discussed in Section 4.5 and outliers are shown as circles. The trimmed sample sizes
for the baseline and improvement are given in the table as a pair and a mean error bar is plotted.
The speedup from the improvement relative to the baseline is also given along with a conﬁdence
interval (CI). CIs are reported as (LL, UL) where LL is the lower limit and UL is the upper limit.

Preprint

19

Table 5: The MALI Greenland ice-sheet 1-to-10 km variable resolution simulation is executed
multiple times on four architectures (Cori: HSW, KNL; Summit: PWR9, V100) and two cases in
order to capture improvements in the linear solve. Architectures and cases are deﬁned in Table 2
and 4, respectively. The table below shows the average number of linear iterations and the total
linear solve time across 26 nonlinear iterations (2 nonlinear solves) for all cases. A 99% conﬁdence
interval is reported (when statistically signiﬁcant) as (LL, UL) where LL is the lower limit and UL
is the upper limit.

Case

Avg. Lin. Its.

Linear Solve Time (s)

HSW

KNL

PWR9

V100

Baseline
Improvement

Baseline
Improvement

Baseline
Improvement

Baseline
Improvement

12.0
12.0

12.0
12.0

11.7
11.7

43.9 (43.5, 44.2)
48.3 (48.2, 48.5)

23.6 (23.4, 23.8)
24.8 (23.5, 26.1)

61.4 (61.1, 61.6)
61.4 (61.1, 61.7)

20.9 (20.8, 21.0)
20.9

37.3 (36.9, 37.7)
18.1 (18.1, 18.2)

Figure 7: The MALI Greenland ice-sheet 1-to-10 km variable resolution simulation is executed
multiple times on four architectures (Cori: HSW, KNL; Summit: PWR9, V100) and two cases.
Architectures, timers and cases are deﬁned in Table 2, 3 and 4, respectively. This plot shows the
ratio of each timer compared to Total Time.

Preprint

20

structures which only executes once but is signiﬁcantly more expensive when compared to CPU
platforms.

5.2 ALI Antarctica ice-sheet weak scalability study

The second case focuses on solving the ﬁrst-order velocity equations in a weak scalability of ALI
on a series of structured Antarctica ice-sheet meshes. This case has been used in a number of other
papers including [69, 73, 27] where more detailed descriptions are given. In this case, the focus will
be on how well the CPU+GPU solver performs over the CPU-only version. The ﬁve meshes vary in
resolution from 1 to 16 kilometers and quadrilateral element counts vary from 51,087 to 13,413,740.
The mesh is extruded by 20 layers during the setup phase, the equation is solved using the methods
described in Section 3, and the mean value of the ﬁnal solution is compared to a previously tested
value using a relative tolerance of 1.0 × 10−5 to ensure the results remain consistent across runs
and architectures. The basal sliding coeﬃcient is predetermined using deterministic inversion from
observed surface velocities [53] and a realistic temperature ﬁeld is provided. Table 6 shows the
number of compute nodes allocated and the total degrees of freedom for each case.

Table 6: A series of increasingly higher resolution Antarctic ice-sheet simulations are executed in a
weak scalability study on four architectures. The table below shows the computing resources and
degrees of freedom (DOF) associated with each case.

Resolution

Nodes

DOF

16km
8km
4km
2km
1km

1
4
16
64
256

2.20 × 106
8.83 × 106
3.53 × 107
1.41 × 108
5.66 × 108

In this case, standalone ALI is used to perform a single nonlinear solve where the tolerances
for the nonlinear and linear solvers are set to 1 × 10−5 and 1 × 10−6, respectively. Simulations
are executed with all improvements described in Section 4. Similar to the Greenland case in
Section 5.1, multiple samples are collected for each case using the same allocation and a mean error
bar is computed. Two-sample t-tests of the mean diﬀerence of the log between the PWR9 and V100
cases are also performed and a 99% conﬁdence interval for the speedup of the GPU relative to the
CPU-only simulation is given in Figure 8. The ﬁrst notable result is that the Total Fill is around
8 times faster when utilizing the V100 GPUs. The same cannot be said about the Preconditioner
Construction and Linear Solve where the performance is worse. Despite this performance loss,
Total Solve is faster when utilizing the GPU. It is important to note that the PWR9, CPU-only
Linear Solve performed exceptionally well compared to the other architectures.

Weak scaling is used to determine how well a code is able to maintain the same wall-clock time
when simulating larger problems with a proportionally larger amount of resources. In this case,
the problem size is not exactly proportional to the resource size so the following formula is used to
compute the weak scaling eﬃciency in terms of percentages,

η =

(t1/N1)/(tn/Nn)
n

× 100%,

0 < η < 100%,

(18)

where t is the wall-clock time, n is the number of compute nodes, N is the number of degrees of
freedom, and larger values are better. Conﬁdence intervals for the eﬃciency are computed by using

Preprint

21

(a) Total Solve

(b) Total Fill

(c) Preconditioner Construction

(d) Linear Solve

Figure 8: A series of increasingly higher resolution Antarctic ice-sheet simulations are executed in
a weak scalability study on four architectures (Cori: HSW, KNL; Summit: PWR9, V100). Four
timers are captured. These are deﬁned in Table 3. The mean error bar, median and lower/upper
quartiles of each case are given along with outliers shown as circles. The speedup from the V100
CPU-GPU simulation relative to the POWER9 CPU-only simulation is also shown along with a
conﬁdence interval (CI). CIs are reported as (LL, UL) where LL is the lower limit and UL is the
upper limit.

Preprint

22

the same mean diﬀerence of the log between the single compute node case and the 256 node case.
The results are shown in Table 7. In this study, the Haswell CPU performed the best when looking

Table 7: A series of increasingly higher resolution Antarctic ice-sheet simulations are executed in
a weak scalability study on up to 256 compute nodes on Cori and Summit. Four architectures
(Cori: HSW, KNL; Summit: PWR9, V100) are tested and four timers are captured as deﬁned in
Table 3. A weak scalability eﬃciency is computed for each case where one compute node is used as
the reference. Larger values are better. A 99% conﬁdence interval is reported as (LL, UL) where
LL is the lower limit and UL is the upper limit.

Total Solve

Total Fill

Preconditioner
Construction

Linear Solve

HSW
KNL
PWR9
V100

68.9% (67.0, 70.9)
63.5% (62.3, 64.6)
65.1% (63.3, 66.9)
42.2% (42.0, 42.4)

82.2% (81.5, 82.9)
85.3% (84.5, 86.0)
73.1% (70.0, 76.4)
82.9% (80.5, 85.4)

41.2% (38.2, 44.5)
33.0% (30.8, 35.5)
39.5% (39.0, 40.0)
55.2% (54.7, 55.8)

67.5% (66.2, 68.8)
61.1% (60.6, 61.6)
63.0% (62.9, 63.1)
31.9% (31.6, 32.2)

at the Total Solve while the CPU+GPU case performed the worst. The Total Fill performed well
across all architectures and there’s a noticeable improvement on the GPU compared to previous
studies [74]. In Preconditioner Construction, the CPU+GPU case performed best. The main
cause for poor scaling on GPU platforms is visible in the Linear Solve. This can be explained by
looking at the performance of the linear solver in Table 8. The average number of linear iterations
from the GPU linear solve is much larger (reaching the maximum iteration constraint) at 256
compute nodes which contributed to worse scaling. The PWR9, CPU-only case also has much
smaller linear solve times compared to the other architectures.

Figure 9 shows the proportions of total wall-clock for each architecture, resolution and timer.
On CPU platforms, Total Fill is the dominant contributor to Total Solve performance across all
resolutions. At lower resolutions, Preconditioner Construction becomes a larger contributor.
On GPU platforms, it’s clear that Linear Solve is the largest contributor across all resolutions
with Total Fill falling to less than 10% at the lowest resolution.

5.3 ALI Greenland ice-sheet 1-to-7 kilometer variable resolution performance

test

The last case focuses on solving the ﬁrst-order velocity equations for a Greenland ice-sheet, 1-
to-7 kilometer variable resolution mesh in a nightly performance testing framework for ALI. The
numerical test is used to identify performance regressions and improvements within ALI. In this
test, a two-dimensional, unstructured, Greenland ice-sheet mesh with 479,930 triangle elements is
used. The test ﬁrst extrudes the mesh by 10 layers using 3 tetrahedra per layer to create a mesh
with 14,397,900 elements and 5,520,460 degrees of freedom. Then, equation (1) is solved using the
methods described in Section 3, and the mean value of the ﬁnal solution is compared to previously
tested values using a relative tolerance of 1.0 × 10−5. The basal sliding coeﬃcient is estimated
using deterministic inversion from observed surface velocities [53] and a realistic temperature ﬁeld
is provided. The test is currently running on two small clusters with diﬀerent HPC architectures as
shown in Table 9. The simulations executed on Blake and Weaver utilize 8 and 2 nodes, respectively.
The historical Total Time for the two cases is shown in Figure 10. The plots show variability
associated with the code base and the system. Statistically signiﬁcant changepoints are detected

Preprint

23

Table 8: A series of increasingly higher resolution Antarctic ice-sheet simulations are executed in
a weak scalability study on four architectures (Cori: HSW, KNL; Summit: PWR9, V100). The
table below shows the total number of nonlinear iterations (1 nonlinear solve), the average number
of linear iterations per nonlinear iteration for all cases and the total linear solve time. A 99%
conﬁdence interval is reported (when statistically signiﬁcant) as (LL, UL) where LL is the lower
limit and UL is the upper limit.

Resolution

Nodes

Nlin. Its.

Avg. Lin. Its.

HSW

KNL

PWR9

V100

16km
8km
4km
2km
1km

16km
8km
4km
2km
1km

16km
8km
4km
2km
1km

16km
8km
4km
2km
1km

1
4
16
64
256

1
4
16
64
256

1
4
16
64
256

1
4
16
64
256

8
8
9
9
9

8
8
9
9
9

8
8
9
9
9

8
8
9
9
9

16.5
15.5
15.2
15.1
17.9

16.2
15.6
15.2
14.6
18.2

16.1
13.1
14.8
24.0
17.7

Linear Solve Time
(s)

9.5s (9.4, 9.5)
9.6s (9.4, 9.8)
10.7s (10.6, 10.8)
11.3s (11.2, 11.4)
14.1s (13.8, 14.4)

20.4s (20.3, 20.5)
23.1s (22.8, 23.5)
26.1s (25.8, 26.4)
26.1s (25.7, 26.4)
33.5s (33.3, 33.7)

5.5s
4.7s
6.4s (6.3, 6.4)
11.8s
8.7s

88.7 (88.6, 88.8)
87.2 (87.1, 87.3)
89.6 (89.6, 89.7)
131.4 (131.2, 131.6)
194.2 (193.6, 194.8)

8.8s (8.7, 8.9)
9.1s (9.1, 9.2)
11.1s (11.0, 11.1)
17.1s (17.0, 17.2)
27.7s (27.5, 27.8)

Table 9: ALI nightly performance tests are executed nightly on the two small clusters given below.

Name

CPU

Number of Cores
GPU
Node Arch.
Memory per Node
CPU Compiler
GPU Compiler
MPI
Node Conﬁg.

Blake

Intel Xeon Platinum 8160
Skylake
24
None
2 CPUs
188 GiB
Intel 18.1.163
None
openmpi 2.1.2
48 MPI

Weaver

IBM POWER9

20
NVIDIA Tesla V100
2 CPUs + 4 GPUs
319 GiB + 15.7 GiB/GPU
gcc 7.2.0
nvcc 10.1.105
openmpi 4.0.1
4 MPI

Preprint

24

Figure 9: A series of increasingly higher resolution Antarctic ice-sheet simulations are executed in
a weak scalability study on four architectures (Cori: HSW, KNL; Summit: PWR9, V100). Timers
are deﬁned in Table 3. This plot shows the ratio of each timer compared to Total Solve.

using the methods described in Section 4.5 in order to identify regressions and improvements. The
simulations in both time series utilize memoization so the CPU performance in Figure 10a has not
changed much.
In contrast, many of the other changes described in section 4 have been added
over the course of the time series causing dramatic improvements to GPU performance as shown
in Figure 10b.

Figure 10 also shows many performance regressions and improvements over the course of the
time series. One recent example is the transition to Kokkos 3.5.0 which caused a regression to CPU
performance. The regression along with the improvement from the ﬁx is shown in Figure 11. In
this particular case, the usage of Kokkos atomic_add was changed for the Serial execution space.
This caused a 15% (99% CI: 14%, 16%) slowdown to Total Fill. This was then ﬁxed manually in
Albany by avoiding the use of atomic_add when running with a Serial execution space. After the
change, Figure 11b shows that the performance improved by 15% (99% CI: 13%, 16%), reverting
the performance loss due to the original regression. Since then, the issue was reported and a ﬁx
has been introduced into Kokkos.

Algorithm performance comparisons can also be historically tracked within the performance
testing framework. One example is the performance comparison of the ﬁnite element assembly
with and without memoization. In this case, two performance tests which store the wall-clock time
of 100 residual and Jacobian evaluations are tracked. Figure 12 shows the observations between
the two cases paired by simulation date on Blake and Weaver. Both plots show that the test with
memoization has performed faster than the test without memoization throughout the entire time
series. On the CPU platform shown in Figure 12a, the relative performance has not changed much
but on the GPU platform in Figure 12b the relative performance has increased over time. The
key diﬀerence in this case was the boundary condition improvements which signiﬁcantly reduced

Preprint

25

(a) Total Time on 8 Blake nodes (384 Skylake cores)

(b) Total Time on 2 Weaver nodes (8 V100 GPUs)

Figure 10: The ALI Greenland ice-sheet 1-to-7 km variable resolution simulation is executed nightly
on two platforms in order to detect regressions and improvements. Blue markers are recorded wall-
clock time. Means are computed between changepoints and are indicated by solid red lines. The
dotted red lines are ± two standard deviations.

Preprint

26

(a) Total Fill regression

(b) Total Fill improvement

Figure 11: The ALI Greenland ice-sheet 1-to-7 km variable resolution simulation is executed nightly
on Blake in order to detect regressions and improvements. Blue markers are recorded wall-clock
time. Means are computed between changepoints and are indicated by solid red lines. The mean
values are also given in the blue box along with a 99% conﬁdence interval (CI). CIs are reported
as (LL, UL) where LL is the lower limit and UL is the upper limit. The dotted red lines are ±
two standard deviations. Lastly, the blue boxes also show ratios (speedup, slowdown) between sets
with a 99% CI. This particular case highlights a performance regression and its later improvement
from the ﬁx.

Preprint

27

(a) Total Fill on 8 Blake nodes (384 Skylake cores)

(b) Total Fill on 2 Weaver nodes (8 V100 GPUs)

Figure 12: The ALI Greenland ice-sheet 1-to-7 km variable resolution ﬁnite element assembly tests
with and without memoization are executed nightly on two platforms in order to detect regressions,
improvements and analyze comparisons. Observations from the two cases are joined by date by
taking the diﬀerence between the log of the timer data and plotting the relative performance
(speedup, slowdown) with markers. Solid lines indicate means between changepoints and dotted
lines represent a 99% conﬁdence interval for the mean.

Preprint

28

the Total Fill time and caused evaluators without memoization to take up a larger portion of the
Total Fill time.

The time series since the most recently detected changepoint can be used to determine whether
the latest relative performance for memoization is statistically signiﬁcant. A paired t-test is used
to test the mean diﬀerence and the data is summarized as shown in Figure 13. The results show

(a) Total Fill on 384 Skylake cores

(b) Total Fill on 8 V100 GPUs

Figure 13: The ALI Greenland ice-sheet 1-to-7 km variable resolution ﬁnite element assembly tests
with and without memoization are executed nightly on two platforms in order to detect regressions,
improvements and analyze comparisons. Observations from the two cases are joined by date by
taking the diﬀerence between the log of the timer data and performing a paired t-test. The ﬁgures
show sample sizes since the last changepoint for each test as well as mean wall-clock times (s)
and standard deviation. The paired relative performance (speedup) is also given along with the
standard error, p-value and a 99% conﬁdence interval (CI). CIs are reported as (LL, UL) where LL
is the lower limit and UL is the upper limit.

that the current estimated speedup from memoization for this case is 2.22 (99% CI: 2.21, 2.23) on
CPU platforms and 7.28 (99% CI: 7.21, 7.34) on GPU platforms.

6 Conclusions

In this paper, the performance portable features of MALI are introduced and analyzed on the two
supercomputing clusters: NERSC Cori and OLCF Summit. First, the ﬁrst-order velocity model
and the mass continuity equation are introduced along with their implementations within Albany
Land Ice and MPAS, respectively. This is used to further describe improvements that have been
made to the ﬁnite element assembly process and linear solve within MALI. The new features focus
on improving performance portability in MALI but are extensible to other applications targeting
HPC systems.

Two numerical experiments are provided to analyze the expected performance on diﬀerent HPC
architectures. The ﬁrst case utilized MALI to simulate an initial state calculation and single time

Preprint

29

step for a Greenland ice sheet 1-to-10 kilometer resolution mesh and compared baseline simulations
without speciﬁc features with improved simulations with the features described in the paper. The
results show that ﬁnite element assembly with memoization, MDSC-Kokkos and tuned smoothers
are performant across all architectures with an expected speedup of 1.60 (99% CI: 1.32, 1.93) on
Cori-Haswell, 1.82 (99% CI: 1.78, 1.86) on Cori-KNL, 1.26 on Summit-POWER9 and 1.30 (99%
CI: 1.29, 1.32) on Summit-V100. The study also highlights speciﬁc regions in need of improvement
in the model. In particular, the need to improve the performance of the coupling between MPAS
and Albany, the ﬁnite element assembly process on CPUs, and the preconditioner on GPUs.

The second numerical experiment utilized ALI to perform a steady-state simulation of the
Antarctic ice sheet on a series of structured meshes in a weak scalability study. The results show
that simulations on Summit-V100 perform the best with a 1.92 (99% CI: 1.91, 1.92) speedup over
Summit-POWER9 in the low resolution case and 1.24 (99% CI: 1.21, 1.28) speedup over Summit-
POWER9 in the high resolution case. The best results on Summit are shown during ﬁnite element
assembly where the speedup over Summit-POWER9 is 8.65 (99% CI: 8.22, 9.10). The results also
show good weak scaling in ﬁnite element assembly for CPU/GPU but poor weak scaling in the
preconditioner on CPU/GPU and in the linear solve on GPU architectures. Further analysis shows
that the average number of linear iterations per nonlinear iteration increases dramatically as the
resolution increases, highlighting the need for a more scalable preconditioner for this particular
problem.

This paper also introduces a changepoint detection method for automated performance testing.
A detailed description of the method is given along with examples of how the method can be used
to detect performance regressions, improvements and diﬀerences in algorithm performance over
In this case, an automated performance testing framework is used with ALI to simulate
time.
the Greenland ice sheet using a 1-to-7 kilometer variable resolution mesh. The results show the
method being exercised on two years of data and an example of a successful detection of performance
regression and improvement. The results also show an example of a nightly performance comparison
where two tests are used to compare ALI with and without memoization. This case was used to
show how the method can be used to detect regressions and improvements in algorithm performance
over time as the utility of memoization has improved to up to 7.28 (99% CI: 7.21, 7.34) speedup
over simulations without memoization on GPU platforms over the course of two years.

Data availability

The performance testing framework and data is available in https://github.com/sandialabs/
ali-perf-tests and https://github.com/sandialabs/ali-perf-data. The results are accessi-
ble via a browser here: https://sandialabs.github.io/ali-perf-data. Performance data, pre-
processing scripts and post-processing scripts are available in https://github.com/sandialabs/
ali-perf-data under the directory paper-data/watkins2022performance.

Acknowledgments

Support for this work was provided through the SciDAC projects FASTMath and ProSPect, funded
by the U.S. Department of Energy (DOE) Oﬃce of Science, Advanced Scientiﬁc Computing Re-
search and Biological and Environmental Research programs. This research used resources of the
National Energy Research Scientiﬁc Computing Center (NERSC), a U.S. Department of Energy
Oﬃce of Science User Facility operated under Contract No. DE-AC02-05CH11231. This research
used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Labora-

Preprint

30

tory, which is supported by the Oﬃce of Science of the U.S. Department of Energy under Contract
No. DE-AC05-00OR22725.

The authors thank Trevor Hillebrand from Los Alamos National Laboratory for help with setting
up the ice-sheet grids, datasets and for fruitful discussions. The authors also thank Luc Berger,
Christian Glusa, Mark Hoemmen, Jonathan Hu, Brian Kelley, Jennifer Loe, Roger Pawlowski, Siva
Rajamanickam, Chris Siefert, Raymond Tuminaro and Ichitaro Yamazaki from Sandia National
Laboratories for their help with Trilinos components and Si Hammond for troubleshooting on
Sandia HPC systems.

Disclaimer

Sandia National Laboratories is a multimission laboratory managed and operated by National
Technology and Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell
International, Inc., for the U.S. Department of Energy’s National Nuclear Security Administration
under contract DE-NA-0003525.

This paper describes objective technical results and analysis. Any subjective views or opinions
that might be expressed in the paper do not necessarily represent the views of the U.S. Department
of Energy or the United States Government.

References

[1] Samaneh Aminikhanghahi and Diane J Cook. A survey of methods for time series change

point detection. Knowledge and information systems, 51(2):339–367, 2017.

[2] Hartwig Anzt, Werner Augustin, Martin Baumann, Hendryk Bockelmann, Thomas Gengen-
bach, Tobias Hahn, Vincent Heuveline, Eva Ketelaer, Dimitar Lukarski, Andrea Otzen, et al.
HiFlow3–a ﬂexible and hardware-aware parallel ﬁnite element package. Preprint Series of the
Engineering Mathematics and Computing Lab, (06), 2010.

[3] Christopher G Baker and Michael A Heroux. Tpetra, and the use of generic programming in

scientiﬁc computing. Scientiﬁc Programming, 20(2):115–128, 2012.

[4] Eric Bavier, Mark Hoemmen, Sivasankaran Rajamanickam, and Heidi Thornquist. Amesos2
and Belos: Direct and iterative solvers for large sparse linear systems. Scientiﬁc Programming,
20(3):241–255, 2012.

[5] Luc Berger-Vergiat, Christian Alexander Glusa, Jonathan J Hu, Christopher Siefert, Ray-
mond S Tuminaro, Mayr Matthias, Prokopenko Andrey, and Wiesner Tobias. MueLu User’s
Guide. Technical Report SAND2019-0537, Sandia National Laboratories, 2019.

[6] Heinz Blatter. Velocity and stress ﬁelds in grounded glaciers: a simple algorithm for including

deviatoric stress gradients. Journal of Glaciology, 41(138):333–344, 1995.

[7] Carlo Bonferroni. Teoria statistica delle classi e calcolo delle probabilita. Pubblicazioni del R

Istituto Superiore di Scienze Economiche e Commericiali di Firenze, 8:3–62, 1936.

[8] Christian Fredborg Brædstrup, Anders Damsgaard, and David Lundbek Egholm. Ice-sheet

modelling accelerated by graphics cards. Computers & Geosciences, 72:210–220, 2014.

[9] Boris Brodsky. Change-point analysis in nonstationary stochastic models. CRC Press, 2016.

Preprint

31

[10] Jed Brown, Barry Smith, and Aron Ahmadia. Achieving textbook multigrid eﬃciency for

hydrostatic ice sheet ﬂow. SIAM Journal on Scientiﬁc Computing, 35(2):B359–B375, 2013.

[11] Max Carlson, Jerry Watkins, and Irina Tezaur. Improvements to the performance portability
of boundary conditions in Albany Land Ice. CSRI Summer Proceedings, pages 177–187, 2020.

[12] Chao Chen, Leopold Cambier, Erik G Boman, Sivasankaran Rajamanickam, Raymond S Tumi-
naro, and Eric Darve. A robust hierarchical solver for ill-conditioned systems with applications
to ice sheet modeling. Journal of Computational Physics, 396:819–836, 2019.

[13] Stephen L Cornford, Daniel F Martin, Daniel T Graves, Douglas F Ranken, Anne M Le Brocq,
Rupert M Gladstone, Antony J Payne, Esmond G Ng, and William H Lipscomb. Adap-
tive mesh, ﬁnite volume modeling of marine ice sheets. Journal of Computational Physics,
232(1):529–549, 2013.

[14] Kurt M Cuﬀey and William Stanley Bryce Paterson. The physics of glaciers. Academic Press,

2010.

[15] David Daly, William Brown, Henrik Ingo, Jim O’Leary, and David Bradford. The use of change
point detection to identify software performance regressions in a continuous integration sys-
tem. In Proceedings of the ACM/SPEC International Conference on Performance Engineering,
pages 67–75, 2020.

[16] Irina Demeshko, Jerry Watkins, Irina K Tezaur, Oksana Guba, William F Spotz, Andrew G
Salinger, Roger P Pawlowski, and Michael A Heroux. Toward performance portability of the
Albany ﬁnite element analysis code using the Kokkos library. The International Journal of
High Performance Computing Applications, 2018.

[17] Phillip Dickens. A performance and scalability analysis of the MPI based tools utilized in a
large ice sheet model executing in a multicore environment. In International Conference on
Algorithms and Architectures for Parallel Processing, pages 131–147. Springer, 2015.

[18] John K Dukowicz, Stephen F Price, and William H Lipscomb. Consistent approximations
and boundary conditions for ice-sheet dynamics from a principle of least action. Journal of
Glaciology, 56(197):480–496, 2010.

[19] H Carter Edwards, Christian R Trott, and Daniel Sunderland. Kokkos: Enabling manycore
performance portability through polymorphic memory access patterns. Journal of Parallel and
Distributed Computing, 74(12):3202–3216, 2014.

[20] Tamsin L Edwards, Sophie Nowicki, Ben Marzeion, Regine Hock, Heiko Goelzer, H´el`ene
Seroussi, Nicolas C Jourdain, Donald A Slater, Fiona E Turner, Christopher J Smith, et al.
Projected land ice contributions to twenty-ﬁrst-century sea level rise. Nature, 593(7857):74–82,
2021.

[21] Yannic Fischler, Martin R¨uckamp, Christian Bischof, Vadym Aizinger, Mathieu Morlighem,
and Angelika Humbert. A scalability study of the ice-sheet and sea-level system model (ISSM,
version 4.18). Geoscientiﬁc Model Development Discussions, pages 1–33, 2021.

[22] Gregory Flato, Jochem Marotzke, Babatunde Abiodun, Pascale Braconnot, Sin Chan Chou,
William Collins, Peter Cox, Fatima Driouech, Seita Emori, Veronika Eyring, et al. Evaluation
of climate models. In Climate change 2013: the physical science basis. Contribution of Working

Preprint

32

Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change,
pages 741–866. Cambridge University Press, 2014.

[23] Nicole Forsgren, Margaret-Anne Storey, Chandra Maddila, Thomas Zimmermann, Brian
Houck, and Jenna Butler. The SPACE of developer productivity: There’s more to it than
you think. Queue, 19(1):20–48, 2021.

[24] O Gagliardini, T Zwinger, F Gillet-Chaulet, G Durand, L Favier, B De Fleurian, R Greve,
M Malinen, C Mart´ın, P R˚aback, et al. Capabilities and performance of Elmer/Ice, a new-
generation ice sheet model. Geoscientiﬁc Model Development, 6(4):1299–1318, 2013.

[25] Heiko Goelzer, Sophie Nowicki, Anthony Payne, Eric Larour, Helene Seroussi, William H. Lip-
scomb, Jonathan Gregory, Ayako Abe-Ouchi, Andrew Shepherd, Erika Simon, C´ecile Agosta,
Patrick Alexander, Andy Aschwanden, Alice Barthel, Reinhard Calov, Christopher Chambers,
Youngmin Choi, Joshua Cuzzone, Christophe Dumas, Tamsin Edwards, Denis Felikson, Xavier
Fettweis, Nicholas R. Golledge, Ralf Greve, Angelika Humbert, Philippe Huybrechts, Sebastien
Le Clec’H, Victoria Lee, Gunter Leguy, Chris Little, Daniel Lowry, Mathieu Morlighem, Isabel
Nias, Aurelien Quiquet, Martin R¨uckamp, Nicole Jeanne Schlegel, Donald A. Slater, Robin
Smith, Fiamma Straneo, Lev Tarasov, Roderik Van De Wal, and Michiel Van Den Broeke.
The future sea-level contribution of the Greenland ice sheet: A multi-model ensemble study
of ISMIP6. Cryosphere, 14(9):3071–3096, 2020.

[26] Douglas M Hawkins, Peihua Qiu, and Chang Wook Kang. The changepoint model for statistical

process control. Journal of quality technology, 35(4):355–366, 2003.

[27] Alexander Heinlein, Mauro Perego, and Sivasankaran Rajamanickam. FROSch preconditioners
for land ice simulations of Greenland and Antarctica. SIAM Journal on Scientiﬁc Computing,
44(2):B339–B367, 2022.

[28] Michael A Heroux, Roscoe A Bartlett, Vicki E Howle, Robert J Hoekstra, Jonathan J Hu,
Tamara G Kolda, Richard B Lehoucq, Kevin R Long, Roger P Pawlowski, Eric T Phipps, et al.
An overview of the Trilinos project. ACM Transactions on Mathematical Software (TOMS),
31(3):397–423, 2005.

[29] Michael A Heroux, Rajeev Thakur, Lois McInnes, Jeﬀrey S Vetter, Xiaoye Sherry Li, James
Aherns, Todd Munson, and Kathryn Mohror. ECP software technology capability assessment
report. Technical report, Oak Ridge National Lab.(ORNL), Oak Ridge, TN (United States),
2020.

[30] Torsten Hoeﬂer and Roberto Belli. Scientiﬁc benchmarking of parallel computing systems:
In Proceedings of the
twelve ways to tell the masses when reporting performance results.
international conference for high performance computing, networking, storage and analysis,
pages 1–12, 2015.

[31] Matthew J Hoﬀman, Mauro Perego, Stephen F Price, William H Lipscomb, Tong Zhang,
Douglas Jacobsen, Irina Tezaur, Andrew G Salinger, Raymond Tuminaro, and Luca Bertagna.
MPAS-Albany Land Ice (MALI): a variable-resolution ice sheet model for Earth system mod-
eling using Voronoi grids. Geoscientiﬁc Model Development, 11(9):3747–3780, 2018.

[32] Richard D Hornung and Jeﬀrey A Keasler. The RAJA portability layer: overview and status.
Technical report, Lawrence Livermore National Lab.(LLNL), Livermore, CA (United States),
2014.

Preprint

33

[33] Tobin Isaac, Georg Stadler, and Omar Ghattas. Solution of nonlinear stokes equations dis-
cretized by high-order ﬁnite elements on nonconforming and anisotropic meshes, with applica-
tion to ice sheet dynamics. SIAM Journal on Scientiﬁc Computing, 37(6):B804–B833, 2015.

[34] Upulee Kanewala and James M Bieman. Testing scientiﬁc software: A systematic literature

review. Information and software technology, 56(10):1219–1232, 2014.

[35] E Larour, H Seroussi, M Morlighem, and E Rignot. Continental scale, high order, high spatial
resolution, ice sheet modeling using the Ice Sheet System Model (ISSM). Journal of Geophysical
Research: Earth Surface, 117(F1), 2012.

[36] Anders Levermann, Ricarda Winkelmann, Torsten Albrecht, Heiko Goelzer, Nicholas R.
Golledge, Ralf Greve, Philippe Huybrechts, Jim Jordan, Gunter Leguy, Daniel Martin, Math-
ieu Morlighem, Frank Pattyn, David Pollard, Aurelien Quiquet, Christian Rodehacke, Helene
Seroussi, Johannes Sutter, Tong Zhang, Jonas Van Breedam, Reinhard Calov, Robert De-
Conto, Christophe Dumas, Julius Garbe, G. Hilmar Gudmundsson, Matthew J. Hoﬀman,
Angelika Humbert, Thomas Kleiner, William H. Lipscomb, Malte Meinshausen, Esmond Ng,
Sophie M. J. Nowicki, Mauro Perego, Stephen F. Price, Fuyuki Saito, Nicole-Jeanne Schlegel,
Sainan Sun, and Roderik S. W. van de Wal. Projecting Antarctica’s contribution to future
sea level rise from basal ice shelf melt using linear response functions of 16 ice sheet models
(LARMIP-2). Earth System Dynamics, 11(1):35–76, feb 2020.

[37] Christophe Leys, Christophe Ley, Olivier Klein, Philippe Bernard, and Laurent Licata. Detect-
ing outliers: Do not use standard deviation around the mean, use absolute deviation around
the median. Journal of experimental social psychology, 49(4):764–766, 2013.

[38] GR Markall, A Slemmer, DA Ham, PHJ Kelly, CD Cantwell, and SJ Sherwin. Finite ele-
ment assembly strategies on multi-core and many-core architectures. International Journal
for Numerical Methods in Fluids, 71(1):80–97, 2013.

[39] David S Medina, Amik St-Cyr, and Tim Warburton. OCCA: A uniﬁed approach to multi-

threading languages. arXiv preprint arXiv:1403.0968, 2014.

[40] JR Neely. DOE Centers of Excellence performance portability meeting. Technical report,

Lawrence Livermore National Lab. (LLNL), Livermore, CA (United States), 2016.

[41] JF Nye. The distribution of stress and velocity in glaciers and ice-sheets. Proc. R. Soc. Lond.

A, 239(1216):113–133, 1957.

[42] Michael Oppenheimer, Bruce Glavovic, Jochen Hinkel, Roderik van de Wal, Alexandre K
Magnan, Amro Abd-Elgawad, Rongshuo Cai, Miguel Cifuentes-Jara, Robert M Deconto, Tuhin
Ghosh, et al. Sea level rise and implications for low lying islands, coasts and communities.
2019.

[43] Frank Pattyn. A new three-dimensional higher-order thermomechanical ice sheet model: Basic
sensitivity, ice stream development, and ice ﬂow across subglacial lakes. Journal of Geophysical
Research: Solid Earth, 108(B8), 2003.

[44] Frank Pattyn, Lionel Favier, Sainan Sun, and Ga¨el Durand. Progress in Numerical Modeling
of Antarctic Ice-Sheet Dynamics. Current Climate Change Reports, pages 1–11, jul 2017.

Preprint

34

[45] Roger P Pawlowski, Eric T Phipps, and Andrew G Salinger. Automating embedded analysis
capabilities and managing software complexity in multiphysics simulation, Part I: Template-
based generic programming. Scientiﬁc Programming, 20(2):197–219, 2012.

[46] Roger P Pawlowski, Eric T Phipps, Andrew G Salinger, Steven J Owen, Christopher M Siefert,
and Matthew L Staten. Automating embedded analysis capabilities and managing software
complexity in multiphysics simulation, Part II: Application to partial diﬀerential equations.
Scientiﬁc Programming, 20(3):327–345, 2012.

[47] Antony J. Payne, Sophie Nowicki, Ayako Abe-Ouchi, C´ecile Agosta, Patrick Alexander,
Torsten Albrecht, Xylar Asay-Davis, Andy Aschwanden, Alice Barthel, Thomas J. Bracegirdle,
Reinhard Calov, Christopher Chambers, Youngmin Choi, Richard Cullather, Joshua Cuzzone,
Christophe Dumas, Tamsin L. Edwards, Denis Felikson, Xavier Fettweis, Benjamin K. Galton-
Fenzi, Heiko Goelzer, Rupert Gladstone, Nicholas R. Golledge, Jonathan M. Gregory, Ralf
Greve, Tore Hattermann, Matthew J. Hoﬀman, Angelika Humbert, Philippe Huybrechts, Nico-
las C. Jourdain, Thomas Kleiner, Peter Kuipers Munneke, Eric Larour, Sebastien Le clec’h,
Victoria Lee, Gunter Leguy, William H. Lipscomb, Christopher M. Little, Daniel P. Lowry,
Mathieu Morlighem, Isabel Nias, Frank Pattyn, Tyler Pelle, Stephen F. Price, Aur´elien Qui-
quet, Ronja Reese, Martin R¨uckamp, Nicole-Jeanne Schlegel, H´el`ene Seroussi, Andrew Shep-
herd, Erika Simon, Donald Slater, Robin S. Smith, Fiammetta Straneo, Sainan Sun, Lev
Tarasov, Luke D. Trusel, Jonas Van Breedam, Roderik van de Wal, Michiel van den Broeke,
Ricarda Winkelmann, Chen Zhao, Tong Zhang, and Thomas Zwinger. Future sea level change
under CMIP5 and CMIP6 scenarios from the Greenland and Antarctic ice sheets. Geophysical
Research Letters, pages 1–8, 2021.

[48] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

[49] Zedong Peng, Xuanyi Lin, Michelle Simon, and Nan Niu. Unit and regression tests of scientiﬁc

software: A study on SWMM. Journal of Computational Science, 53:101347, 2021.

[50] S John Pennycook, Jason D Sewall, Douglas W Jacobsen, Tom Deakin, and Simon McIntosh-
Smith. Navigating performance, portability, and productivity. Computing in Science & Engi-
neering, 23(5):28–38, 2021.

[51] Simon J Pennycook, JD Sewall, and VW Lee. A metric for performance portability. arXiv

preprint arXiv:1611.07409, 2016.

[52] SJ Pennycook, JD Sewall, and VW Lee. Implications of a metric for performance portability.

Future Generation Computer Systems, 2017.

[53] Mauro Perego, Stephen Price, and Georg Stadler. Optimal initial conditions for coupling
ice sheet models to Earth system models. Journal of Geophysical Research: Earth Surface,
119(9):1894–1917, 2014.

[54] Eric Phipps and Roger Pawlowski. Eﬃcient expression templates for operator overloading-
In Recent Advances in Algorithmic Diﬀerentiation, pages

based automatic diﬀerentiation.
309–319. Springer, 2012.

Preprint

35

[55] TOP500 Project. June 2021 TOP500 list. https://www.top500.org/lists/top500/2021/

06/. [Online; accessed 25-October-2021].

[56] Andrey Prokopenko, Christopher Siefert, Jonathan J Hu, Mark Frederick Hoemmen, and
Alicia Marie Klinvex. Ifpack2 user’s guide 1.0. Technical report, Sandia National Laboratories,
2016.

[57] David A Randall, Richard A Wood, Sandrine Bony, Robert Colman, Thierry Fichefet, John
Fyfe, Vladimir Kattsov, Andrew Pitman, Jagadish Shukla, Jayaraman Srinivasan, et al. Cli-
mate models and their evaluation. In Climate change 2007: The physical science basis. Con-
tribution of Working Group I to the Fourth Assessment Report of the IPCC (FAR), pages
589–662. Cambridge University Press, 2007.

[58] Ludovic R¨ass, Aleksandar Licul, Fr´ed´eric Herman, Yury Y Podladchikov, and Jenny Suckale.
Modelling thermomechanical ice deformation using an implicit pseudo-transient method (Fas-
tICE v1. 0) based on graphical processing units (GPUs). Geoscientiﬁc Model Development,
13(3):955–976, 2020.

[59] Ludovic R¨ass, Ivan Utkin, Thibault Duretz, Samuel Omlin, and Yuri Y Podladchikov. Assess-
ing the robustness and scalability of the accelerated pseudo-transient method towards exascale
computing. Geoscientiﬁc Model Development Discussions, 2022:1–46, 2022.

[60] Florian Rathgeber, David A Ham, Lawrence Mitchell, Michael Lange, Fabio Luporini, An-
drew TT McRae, Gheorghe-Teodor Bercea, Graham R Markall, and Paul HJ Kelly. Firedrake:
automating the ﬁnite element method by composing abstractions. ACM Transactions on
Mathematical Software (TOMS), 43(3):24, 2017.

[61] Florian Rathgeber, Graham R Markall, Lawrence Mitchell, Nicolas Loriant, David A Ham,
Carlo Bertolli, and Paul HJ Kelly. PyOP2: A high-level framework for performance-portable
simulations on unstructured meshes. In High Performance Computing, Networking, Storage
and Analysis (SCC), 2012 SC Companion:, pages 1116–1123. IEEE, 2012.

[62] Todd Ringler, Mark Petersen, Robert L. Higdon, Doug Jacobsen, Philip W. Jones, and Mathew
Maltrud. A multi-resolution approach to global ocean modeling. Ocean Modelling, 69:211–232,
2013.

[63] Ian C Rutt, M Hagdorn, NRJ Hulton, and AJ Payne. The Glimmer community ice sheet

model. Journal of Geophysical Research: Earth Surface, 114(F2), 2009.

[64] Andrew G Salinger, Roscoe A Bartlett, Andrew M Bradley, Qiushi Chen, Irina P Demeshko,
Xujiao Gao, Glen A Hansen, Alejandro Mota, Richard P Muller, Erik Nielsen, Jakob Ostien,
Roger Pawlowski, Mauro Perego, Eric Phipps, WaiChing Sun, and Irina Tezaur. Albany: Using
component-based design to develop a ﬂexible, generic multiphysics analysis code. International
Journal for Multiscale Computational Engineering, 14(4), 2016.

[65] Christian Schoof and Richard CA Hindmarsh. Thin-ﬁlm ﬂows with wall slip: an asymptotic
analysis of higher order glacier ﬂow models. The Quarterly Journal of Mechanics & Applied
Mathematics, 63(1):73–114, 2010.

[66] H´el`ene Seroussi, Sophie Nowicki, Antony J. Payne, Heiko Goelzer, William H. Lipscomb,
Ayako Abe-Ouchi, C´ecile Agosta, Torsten Albrecht, Xylar Asay-Davis, Alice Barthel, Rein-
hard Calov, Richard Cullather, Christophe Dumas, Benjamin K. Galton-Fenzi, Rupert Glad-
stone, Nicholas R. Golledge, Jonathan M. Gregory, Ralf Greve, Tore Hattermann, Matthew J.

Preprint

36

Hoﬀman, Angelika Humbert, Philippe Huybrechts, Nicolas C. Jourdain, Thomas Kleiner, Eric
Larour, Gunter R. Leguy, Daniel P. Lowry, Chistopher M. Little, Mathieu Morlighem, Frank
Pattyn, Tyler Pelle, Stephen F. Price, Aur´elien Quiquet, Ronja Reese, Nicole-Jeanne Schlegel,
Andrew Shepherd, Erika Simon, Robin S. Smith, Fiammetta Straneo, Sainan Sun, Luke D.
Trusel, Jonas Van Breedam, Roderik S. W. van de Wal, Ricarda Winkelmann, Chen Zhao, Tong
Zhang, and Thomas Zwinger. ISMIP6 Antarctica: a multi-model ensemble of the Antarctic
ice sheet evolution over the 21st century. The Cryosphere, 14(9):3033–3070, sep 2020.

[67] Alexander Tartakovsky, Igor Nikiforov, and Michele Basseville. Sequential analysis: Hypothesis

testing and changepoint detection. CRC Press, 2014.

[68] Irina K Tezaur, Mauro Perego, Andrew G Salinger, Raymond S Tuminaro, and Stephen F
Price. Albany/FELIX: a parallel, scalable and robust, ﬁnite element, ﬁrst-order Stokes ap-
proximation ice sheet solver built for advanced analysis. Geoscientiﬁc Model Development,
8(4):1197, 2015.

[69] Irina K Tezaur, Raymond S Tuminaro, Mauro Perego, Andrew G Salinger, and Stephen F
Price. On the scalability of the Albany/FELIX ﬁrst-order Stokes approximation ice sheet
solver for large-scale simulations of the Greenland and Antarctic ice sheets. Procedia Computer
Science, 51:2026–2035, 2015.

[70] The NOX and LOCA Project Team. The NOX and LOCA Project Website. https://

trilinos.github.io/nox_and_loca.html. [Online; accessed 4-April-2022].

[71] Christian Trott, Luc Berger-Vergiat, David Poliakoﬀ, Sivasankaran Rajamanickam, Damien
Lebrun-Grandie, Jonathan Madsen, Nader Al Awar, Milos Gligoric, Galen Shipman, and Ge-
oﬀ Womeldorﬀ. The Kokkos EcoSystem: Comprehensive performance portability for high
performance computing. Computing in Science Engineering, 23(5):10–18, 2021.

[72] Christian R. Trott, Damien Lebrun-Grandi´e, Daniel Arndt, Jan Ciesko, Vinh Dang, Nathan
Ellingwood, Rahulkumar Gayatri, Evan Harvey, Daisy S. Hollman, Dan Ibanez, Nevin Liber,
Jonathan Madsen, Jeﬀ Miles, David Poliakoﬀ, Amy Powell, Sivasankaran Rajamanickam,
Mikael Simberg, Dan Sunderland, Bruno Turcksin, and Jeremiah Wilke. Kokkos 3: Program-
ming model extensions for the exascale era. IEEE Transactions on Parallel and Distributed
Systems, 33(4):805–817, 2022.

[73] R Tuminaro, Mauro Perego, I Tezaur, A Salinger, and Stephen Price. A matrix dependen-
t/algebraic multigrid approach for extruded meshes with applications to ice sheet modeling.
SIAM Journal on Scientiﬁc Computing, 38(5):C504–C532, 2016.

[74] Jerry Watkins, Irina Tezaur, and Irina Demeshko. A study on the performance portability of
the ﬁnite element assembly process within the Albany Land Ice solver. In Numerical Methods
for Flows, pages 177–188. Springer, Cham, 2020.

[75] R Winkelmann, Maria A Martin, Monika Haseloﬀ, Torsten Albrecht, E Bueler, C Khroulev,
and Anders Levermann. The Potsdam parallel ice sheet model (PISM-PIK)-Part 1: Model
description. The Cryosphere, 5(3):715, 2011.

[76] Charlene Yang, Rahulkumar Gayatri, Thorsten Kurth, Protonu Basu, Zahra Ronaghi, Ade-
doyin Adetokunbo, Brian Friesen, Brandon Cook, Douglas Doerﬂer, Leonid Oliker, et al. An
empirical rooﬂine methodology for quantitatively assessing performance portability. In 2018

Preprint

37

IEEE/ACM International Workshop on Performance, Portability and Productivity in HPC
(P3HPC), pages 14–23. IEEE, 2018.

Preprint

38

