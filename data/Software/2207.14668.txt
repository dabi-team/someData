lifex: a ﬂexible, high performance library for the
numerical solution of complex ﬁnite element problems

P. C. Africa

MOX, Department of Mathematics, Politecnico di Milano
Piazza Leonardo da Vinci, 32, 20133, Milano (Italy)

2
2
0
2

t
c
O
8
1

]
S
M

.
s
c
[

2
v
8
6
6
4
1
.
7
0
2
2
:
v
i
X
r
a

Abstract

Numerical simulations are ubiquitous in mathematics and computational science. Several industrial and
clinical applications entail modeling complex multiphysics systems that evolve over a variety of spatial and
temporal scales. This study introduces the design and capabilities of lifex, an open source C++ library for
high performance ﬁnite element simulations of multiphysics, multiscale, and multidomain problems. lifex
meets the emerging need for versatile, eﬃcient computational tools that are easily accessed by users and
developers. We showcase its ﬂexibility and eﬀectiveness on a number of illustrative examples and advanced
applications of use and demonstrate its parallel performance up to thousands of cores.

Keywords: high performance computing, ﬁnite elements, numerical simulations, multiphysics problems
PACS: 02.30.Jr, 02.60.Cb, 02.70.-c, 02.70.Dh
2020 MSC: 35-04, 65-04, 65Y05, 65Y20, 68-04, 68N30

Nr. Code metadata description
C1
C2

Current code version
Permanent link to code/repository used for
this code version

C3
C4
C5
C6

C7

C8
C9

Code Ocean compute capsule
Legal Code License
Code versioning system used
Software code languages, tools, and ser-
vices used
Compilation requirements, operating envi-
ronments, and dependencies
Link to developer documentation/manual
Support email for questions

Please ﬁll in this column
v1.5.0
Homepage: https://lifex.gitlab.io/,
Code repository: https://gitlab.com/
lifex/lifex
N/A
LGPLv3
git
C++ (standard ≥ 17), MPI, CMake ≥ 3.12.0

deal.II ≥ 9.3.0, VTK ≥ 9.0.0, Boost ≥
1.76.0
https://lifex.gitlab.io/lifex/
pasqualeclaudio.africa@polimi.it

Table 1: lifex metadata.

1. Motivation and signiﬁcance

A broad range of applications in biology, medicine, physics, engineering, astronomy, energy, environmen-
tal, and material sciences can be described by multiple physical processes interacting at diﬀerent spatial and

Email address: pasqualeclaudio.africa@polimi.it (P. C. Africa)

Preprint submitted to SoftwareX

October 19, 2022

 
 
 
 
 
 
Figure 1: lifex oﬃcial logo. This image is licensed under a Creative Commons Attribution-ShareAlike 4.0 International
License.

temporal scales [1]. From the mathematical modeling perspective, such systems can be viewed as agglom-
erations of well-deﬁned physics referred to as core models. This explains the emerging need to develop new
universal computational frameworks for the numerical simulation of multiphysics, multiscale, and multido-
main problems. Such tools should easily enable the realization of in silico experiments and provide a stable
and intuitive development environment without compromising computational accuracy and eﬃciency.

The development of a tool of this kind plays a central role in decoupling the software development phase
from the time-consuming process of performing diﬀerent analyses – from forward simulations to sensitivity
analysis, optimization, and uncertainty quantiﬁcation – and in enabling the simulation of each core model
in both standalone and various coupled conﬁgurations [2].

We introduce lifex (pronounced /­laIf"Eks/, oﬃcial logo shown in Fig. 1), an open source library for the
numerical solution of partial diﬀerential equations (PDEs) and related coupled problems, released under the
LGPLv3 license terms. It is written in C++ using modern programming techniques available in the C++17
standard and builds on the deal.II [3] ﬁnite element (FE) core. lifex aims at providing a ﬂexible and
intuitive but robust and high performance tool simplifying the deﬁnition of complex physical models and
their parameters, coupling schemes, and post-processing.

lifex enables its users to shift the focus from technical numerics and implementation details toward
the plain discrete mathematical formulation of the problems of interest. The library comes with extensive
documentation and several examples and test cases that cover a wide range of applications and numerical
strategies.

While serving similar purposes as existing multiphysics libraries in the open source community, such as
FEniCS [4, 5], MFEM [6], MOOSE [7], and preCICE [8], lifex oﬀers several distinctive features, including the
following:

• an intuitive user programming interface with extreme ease of use;

• modern programming paradigms by design, leveraging the C++17 standard, and up-to-date versions of

third-party dependencies;

• parallel scalability up to thousands of cores;

• interoperability; that is, the possibility of importing and exporting data and meshes with common ﬁle

formats, with particular reference to VTK;

• support for arbitrary FEs, among those available in the deal.II backend [9];

• the possibility to import meshes with either hexahedral or tetrahedral elements [10, 3];

• a clean and meticulously documented code base.

Each of these features is outlined below.

2. Software description

lifex was conceived in 2019 as an academic research library within the framework of the iHEART
project (see Acknowledgements) at the Politecnico di Milano, with a primary focus on mathematical models
and numerical schemes for integrated simulations of cardiac function.

2

Since its initial design, many modules for the simulation of diﬀerent core models have been added to
the code base. The development of lifex was founded on strict coding conventions and practices [11]. The
rapid increase in the number of developers and users testiﬁes to the shallow learning curve of its kernel;
it is fast and general enough to be used for diverse applications and merits being released as a standalone
library.

Third-party dependencies of lifex include the following: deal.II (conﬁgured with support to PETSc
[12] and Trilinos [13]), VTK, and Boost. lifex can be conﬁgured to use, by default, linear algebra data
structures and algorithms from PETSc, Trilinos (either through the interfaces exposed by deal.II or
directly) or deal.II itself; where needed, a speciﬁc datatype or solver provided by one of the three backends
may also be hard-coded, disregarding the default type with which lifex was conﬁgured. All the code is
natively parallel through the message passing interface (MPI); following a distributed memory paradigm, the
global mesh is partitioned so that each MPI process owns and stores only a subset of cells.

This library aspires to maximum portability, having being deployed successfully on Linux, Windows, and
macOS operating systems. This has motivated the use of advanced deployment technology, more speciﬁcally
mk [14] (a set of portable, pre-compiled scientiﬁc packages for x86-64 Linux systems), lifex-env [15] (a set
of build-from-source shell scripts explicitly inspired by candi), and Spack. Pre-built Docker images with
all dependencies installed are also ready for download and use. More details can be found on the lifex
documentation.

2.1. Software architecture

Structurally, the key features of lifex can be grouped into three main components:

1. An abstraction layer built on top of the deal.II FE library, exposing abstract numerical helpers
as essential building blocks that foster the development of advanced data structures and numeri-
cal schemes for time integration, linearization, solving and preconditioning linear systems, imposing
boundary conditions, and mesh handling.

2. A framework for multiphysics coupling, with functionalities enabling the transfer of solution ﬁelds
and data from one core model to the other, either in the same domain or across multiple domains.

3. A seamless user interface through several advanced input/output (I/O) capabilities, with a focus on
importing data coming from the post-processing of experimental results, imaging techniques, or other
numerical simulations, such as with the help of the VTK library.

The main code components falling into these three categories, their classes, and their interactions are

schematized in Fig. 2.

2.2. Software functionalities

All lifex executables are built on the functionalities described below and are classiﬁed as follows:

apps:

Generic applications that are not model-speciﬁc, such as tools for printing mesh statistics or converting
between compatible ﬁle formats.

examples:

Problems and solvers that deﬁne speciﬁc model or geometric parameters, such as boundary conditions,
initial conditions, domain, and so on.

tests:

Executables used for automatic testing (run via CTest), automatically run on continuous integration
(CI) services at each git push on GitLab remote. Tests also include a number of tutorials, which can
be used as prototypes for building new applications. All tests and tutorials are used to determine the
overall code coverage; that is, a metric that determines the number of lines of code that are successfully
validated by the testing procedure.

3

Figure 2: Overview of main lifex components. The main classes and their interactions are shown, grouped into three categories:
abstract numerical helpers (blue), multiphysics coupling (red), and user interface (yellow).

A lifex executable is typically associated with a set of common attributes, such as user-speciﬁed com-
mand line ﬂags, the name of a parameter ﬁle (that is, a ﬁle containing all conﬁgurations, parameters, and
settings used to run the executable, organized in a tree-like subsection structure), an execution mode ﬂag
that speciﬁes whether to generate a new parameter ﬁle or actually execute the app, an output directory
containing all output ﬁles, MPI rank and size used for parallel computations. Upon running, such attributes
are shared among instances of all classes. Moreover, all main classes are designed so as to expose their own
speciﬁc parameters, such as geometry, physical parameters, discretization schemes, numerical settings, and
I/O options, from the parameter ﬁle, each within its own subsection path.

The following three main classes deﬁne the minimal kernel interface common to all lifex modules and

executables:

Core:

A class implemented following the singleton design pattern [16] that stores attributes that are global and
common to all other classes, such as those listed directly above.

CoreModel:

An abstract class that inherits Core and extends it with pure virtual methods that deﬁne the interface
exposed by each core model or numerical solver throughout lifex. Classes in the CoreModel hierarchy
expose a set of parameters that conﬁgure their behavior. Such parameters are exposed to the user
through the parameter ﬁle (see Sec. 2.2.3). A sample code snippet is provided and discussed in Sec. 2.3.

lifex init:

A lifespan handler that takes care of properly initializing all attributes and dependencies needed by each
run, such as the instance of the singleton Core and MPI; an instance of this class is typically constructed
at the very beginning of the main() function and destroyed at the program’s end.

More speciﬁc high-level data structures are introduced below.

4

2.2.1. Abstract numerical helpers

An enormous part of lifex consists of abstract wrappers and helpers: most of these classes explicitly
invoke or refer to deal.II design and features [9], with the goal of exposing a higher-level interface to them
and facilitating the implementation of advanced numerical schemes for a given problem. The main classes
are described below.

MeshHandler:

A wrapper around deal.II distributed meshes. The user can select whether to import a mesh with hex-
ahedral or tetrahedral elements; depending on that choice, this class owns an instance of a distributed
or a fullydistributed triangulation from deal.II; the latter is a recent introduction that adds support
to tetrahedral meshes [3], whose functionalities at the time of writing are still to be consolidated. The
MeshHandler class interacts closely with MeshInfo, which parses information from the input mesh like
volume and surface tags to be used, for example, to impose diﬀerent boundary conditions on diﬀerent
parts of the boundary or to diﬀerentiate material properties in diﬀerent sub-regions. Helper functions
implemented in the geometry/mesh info and geometry/finders modules allow the computation of
(sub)domain volumes and boundary surfaces or to locate, for example, the closest degree of freedom
(DoF), mesh vertex, or boundary face to a given input point.

BCHandler:

A helper class to impose diﬀerent types of boundary conditions. Dirichlet boundary conditions can be
either applied directly to an FE vector or imposed as linear constraints to the linear system arising from
an FE discretization. For vector problems, normal or tangential ﬂuxes can also be imposed. A helper
method to assemble Neumann and Robin-like contributions to a local system’s right-hand side is also
provided.

LinearSolverHandler:

For a sparse, distributed linear system, this class provides a simple interface that enables the user to select
at run time which linear solver to use and all of its options (for instance, maximum number of iterations,
tolerances, stopping criteria, and history log), as parsed from the parameter ﬁle. Many common solvers
are included, such as CG, GMRES, BiCGStab, MinRes, FGMRES, but in principle any solver exposed by
deal.II (including those from PETSc and Trilinos) is supported. Furthermore, the complete suite of
solvers from PETSc remains accessible via the -options file command line ﬂag, forwarded from lifex
to PETSc.

PreconditionerHandler:

Analogously to LinearSolverHandler, this class exposes parameters that are used for the precondition-
ing of linear systems. It supports many preconditioner types, such as algebraic multi-grid (AMG), block
Jacobi, additive Schwarz (SOR, SSOR, block SOR, block SSOR, ILU, ILUT), and can easily be extended
to support more.

BDFHandler:

For time-dependent problems, semi-implicit backward diﬀerence formula (BDF) time discretization
schemes [17] are implemented in this class, which deals with storing the information to advance the
problem from one time step to the next. This class stores and exposes the BDF solution and its extrap-
olation and can easily be extended to diﬀerent time-advancing schemes.

NonLinearSolverHandler:

For solving non-linear problems, a family of Newton methods is provided. An abstract implementation
requires the user to specify an assemble function, which assembles the Jacobian matrix and the residual
vector, and a solve function that assembles the preconditioner and solves the linear system associated
with each non-linear iteration; the two functions must return the norms of residual, solution, and Newton
increment to be used as possible stopping criteria. The frozen Jacobian (or Jacobian lagging) approach
[18], which consists of reassembling the Jacobian only once every n time steps, can be toggled to in-
crease computational eﬃciency. Two specializations for the quasi-Newton method with the Jacobian

5

Figure 3: Possible solution schemes for a geometrically coupled problem: monolithic (left) vs. partitioned (right) solution
scheme. Reprinted from [24]. The original image is licensed under a CC BY 3.0 License.

matrix approximated via ﬁnite diﬀerences [19] and for the inexact Newton method [20] are also supplied.
Moreover, each non-linear solution scheme can be equipped with proper acceleration strategies (static
relaxation, Aitken extrapolation [21], and Anderson acceleration [22]) to accelerate convergence. In addi-
tion to the non-linear solver handler, the user can beneﬁt from the use of automatic diﬀerentiation (with
support for the Sacado and ADOL-C interfaces exposed by deal.II), demonstrated on Tutorial04 AD
and Tutorial07 AD, which enables implementing the computation of exact derivatives (up to machine
precision) of complicated functions very easily.

2.2.2. Multiphysics coupling

The complexity of multiphysics, multiscale, and multidomain problem can be relieved with the help of
three hierarchies of classes that all serve the purpose of transferring solution ﬁelds and data either from one
core model to another or across internal interfaces.

In order to keep the code as general as possible, we assume that diﬀerent core models can be solved
using arbitrarily independent discretization schemes, such as diﬀerent FE degrees or mesh resolutions. This
improves the capturing of all physical phenomena involved, even though their dynamics can be characterized
by vastly diﬀerent spatial and temporal scales.

We note that problems involving more than one physical model (possibly on multiple domains sharing
a common interface, such as in the case of ﬂuid-structure interaction) can be generally solved using either
monolithic or partitioned algorithms [23], as schematized in Fig. 3.
In the former case, a global system
involving all unknowns from all problems is assembled and solved at each time step; in the latter, each
sub-problem is solved independently, and coupling conditions are imposed, for example by using explicit
schemes or sub-iterating with a ﬁxed-point scheme until a convergence of coupling conditions is reached.
Both choices are possible in lifex and illustrated by a number of examples and tests.

QuadratureEvaluation:

This class provides a high-level interface for the evaluation of arbitrary analytic functions or more complex
data structures at a given quadrature point. User-deﬁned classes deriving from QuadratureEvaluation
can easily be implemented for scalar, vector, or tensor ﬁelds. Furthermore, the QuadratureEvaluationFEM
hierarchy of classes is implemented to enable the coupling of multiple FE models solved in the same do-
main. Diﬀerent problems can be discretized using diﬀerent FE degrees, and the integrals arising from
the weak formulation can be approximated using quadrature formulas of diﬀerent types and degrees of
accuracy. The QuadratureEvaluationFEM classes provide an interface similar to that of FEValues from
deal.II: such objects are constructed using the DoFHandler associated with the FE ﬁeld to be evaluated
and the quadrature rule used for the target problem. By re-initializing such objects on each mesh cell,
the input ﬁeld can be evaluated at the corresponding quadrature points. lifex provides specializations
to automatically evaluate the FE solutions, gradients, and divergence of a given solution vector.

ProjectionL2:

Instead of the exact numerical evaluation allowed by QuadratureEvaluation classes, a smoothed L2
projection can be considered. Given a function f (x), this class computes a FE solution fh(x) that
satisﬁes (ε∇fh, ∇ϕi)Ω + (fh, ϕi)Ω = (f, ϕi)Ω for each basis function ϕi in the chosen FE space. The
numerical solution to this problem clearly involves a mass matrix: its lumping can be toggled, and the

6

Figure 4: Example of handling two domains Ω1 (left) and Ω2 (right) sharing a common interface Σ with conforming mesh
discretizations. The InterfaceHandler is able to deal properly with non-conforming parallel partitioning.

regularization parameter ε can be tuned to prevent numerical oscillations, for example in the case of
coarse meshes [25]. The solution fh obtained can thus easily be evaluated at the quadrature nodes
associated with the target problem.

InterfaceHandler:

Consider two subdomains Ω1 and Ω2 sharing a common interface Σ with conforming discretizations,
and let u1 and u2 be FE functions deﬁned on the two subdomains, typically representing solutions to
diﬀerential problems deﬁned on the two subdomains. Suppose that the problem deﬁned on Ω1 (Ω2)
involves conditions on Σ that depend on u2 (u1) [26, 23]. InterfaceHandler builds the interface maps;
that is, two mappings of DoFs between the local interface Σ and the global domains Ω1 and Ω2. This
is of critical importance in parallel simulations, where the parallel partitioning on both domains can be
diﬀerent, as in the example in Fig. 4. Finally, for each subdomain, this class manages the extraction
of interface data on Σ from the other subdomain and its application as a boundary condition on Σ.
This class deals only with conforming meshes; extensions to non-conforming discretizations, such as the
INTERNODES technique [27], are still under development.

The last case to be considered is transferring solutions between multiple core models solved using the
same FE discretization but with diﬀerent mesh resolutions. For nested hexahedral grids, the VectorTools
namespace of deal.II already provides functions that perform precisely the interpolation needed. This
procedure is hardly generalizable as it depends heavily on how the diﬀerent meshes have been generated and
on the mesh element type. For instance, transfer operators built on radial basis function (RBF) interpolators
could be used in the case of non-conforming discretizations [28, 29] but have yet to be implemented.

Clearly, the two approaches can be combined to couple diﬀerent models solved with both diﬀerent FE

approximations and mesh resolutions.

2.2.3. User interface
CommandLineParser:

lifex makes use of the lightweight parser clipp for parsing command line arguments. All executables
expose a set of command line options that can be printed using the -h (or --help) ﬂag:

7

./ e xecut able_name -h

ParamHandler:

Each lifex executable deﬁnes a set of parameters that are required in order to be run. They involve
problem-speciﬁc parameters (such as coeﬃcients, geometry, time interval, and boundary conditions),
numerical parameters (such as types of linear/non-linear solvers, tolerances, and maximum number of
iterations), I/O options, and so on. In the event an application has sub-dependencies such as a linear
solver, the related parameters are also included, typically in a proper subsection path. Parameters are
organized in a tree-like structure following the functionalities exposed by the ParameterHandler class
from deal.II. The ﬁrst step before running any executable is to generate the default parameter ﬁle(s)
via the -g (or --generate-params) ﬂag:

./ e xecut able_name -g -f filename . ext

At the user’s option, in order to guarantee a ﬂexible interface with external ﬁle processing tools, the
parameter ﬁle extension ext can be chosen among three diﬀerent interchangeable ﬁle formats prm, json
or xml, sorted from the most human-readable to the most machine-readable.

An excerpt of a prm ﬁle follows:

subsection Problem

subsection Mesh and space discretiz ation

# Parameter description goes here .
set Element type = Hex
# ...

end
# ...
subsection Linear solver

set Type = GMRES

subsection GMRES

set Max . number of temporary vectors = 100
# ...

end

end
# ...
subsection Preconditioner

set Type = AMG

subsection AMG

set W - cycle = true
# ...

end

end

Listing 1: Example of parameter ﬁle in prm format. The tree-like subsection structure is emphasized.

A parameter ﬁle can easily be set up using any text editor, without need to recompile the source code.
Finally, omitting the -g ﬂag in the command above, an existing parameter ﬁle is read and the simulation
subsequently run.
The ParamHandler class of lifex extends the deal.II class by two main functionalities:

verbosity control:

By default, only parameters declared to have a standard verbosity are printed. In order to customize
the user experience, the verbosity of each parameter can be decreased (minimal ) or increased (full )
from the source code. A parameter ﬁle containing a minimal (full) set of parameters can be generated
by passing the optional ﬂag minimal (full) to the -g ﬂag:

8

./ e xecutable_name -g [ minimal , full ] \

-f filename . ext

If the -g is provided without any further speciﬁcation, the intermediate level of verbosity is assumed.

multiple default values:

In principle, each application could be run to simulate diﬀerent scenarios or simply with diﬀerent
predeﬁned sets of parameters; lifex oﬀers the possibility of providing multiple default parameter ﬁles
out of the box. The ParamHandler class can read user-provided ﬁles in json format by specifying a
list of parameter names and their (new) default values, which will be appended to the complete set
of parameters and written to a ready-to-use ﬁle (see, for example, the time interpolation test).

Utilities for parsing lists of values are also provided in the param handler helpers module for conve-
nience of use.

(De-)serialization:

lifex includes a checkpointing system that allows for all aspects of a simulation to be serialized to ﬁle.
This allows recovering a simulation state after an unexpected failure, restarting after maximum computa-
tional wall time has been reached, or simply initializing a simulation with custom input data. Convenient
tools for (de-)serializing (distributed) meshes and solution vectors are provided in the io/serialization
module, with an interface to deal.II-compatible binary ﬁles, and their use is demonstrated in the
serialization test.

CSV readers and writers:

The simplicity of use of comma-separated value (CSV) ﬁles makes it a widely chosen option to process
data organized into ﬁelds. Many utility functions and classes are present in lifex to read and write
CSV ﬁles by converting number and text values into STL containers or deal.II data structures (vectors,
matrices, and so on). This enables easily post-processing simulation results, for example by exporting
point-wise variables at each time step.

TimeInterpolation:

Many applications require resampling discrete sets of data at arbitrary points, such as time-dependent
variables that need to be interpolated in correspondence with the time steps performed by the numerical
simulation. The TimeInterpolation class provides methods based on linear interpolation, cubic splines,
smoothing cubic splines, trigonometric interpolation (discrete Fourier transform), and linear and spline
interpolation of the derivative of the input data.

VTKFunction and VTKPreprocess:

Many physical problems are characterized by coeﬃcients derived from experimental data or imaging
techniques, such as segmented geometries of organs from magnetic resonance imaging (MRI) or computer
tomography (CT) scans [30, 31], or from post-processing of other numerical simulation steps [32]. The
VTK toolkit deﬁnes some of the most common data formats to deal with data deﬁned over volumes
(vtkUnstructuredGrids) or surfaces (vtkPolyData). Moreover, it is also used in sophisticated pipelines
for surface processing and mesh generation [33]. lifex provides a class named VTKFunction, inherited
from dealii::Function, that imports a VTK ﬁle containing a cell or point data ﬁeld and evaluates it at
an arbitrary point, possibly associated with a computational mesh. Three possible evaluation methods
are available: closest point, linear projection, and signed distance. Finally, the VTKPreprocess class
exploits VTKFunction to interpolate input VTK data onto FE vectors, which are serialized to ﬁle for later
importing and reuse in numerical simulations.

2.3. Sample code snippet

The following code illustrates a sample code snippet with comments, containing the minimal interface
exposed by the vast majority of all lifex classes; that is, those inherited from CoreModel. In particular, the
declare_parameters and parse_parameters methods are pure virtual and must be overridden, whereas

9

the run method is virtual and has an empty deﬁnition by default. An example of how to locally adjust the
verbosity of some parameters is also shown. Finally, this sample class makes use of a LinearSolverHandler,
for which we also declare and parse related parameters.

namespace lifex
{

class Problem : public CoreModel
{
public :

// Specify the subsection path where to
// declare current parameters .
Problem ( const std :: string & subsection_p a th )
: CoreModel ( subsection_path )

// Specify a " relative " subsection .
// Subpaths are separated by a "/".
, linear_solver (

p r m _ s ub s e c t i o n _ pa t h + " / Linear solver " ,
/* ... */ )

{}

virtual void
d ec l a r e_ p ar am e te r s ( ParamHandler & params ) const override
{

// Navigate subsections and declare parameters .
params . e n t e r _ s u b s e c t i o n _ p a t h ( p r m _ s u b s e c t i o n _ p at h ) ;
{

// Problem - dependent parameters .
// ...

params . set_verbosity ( VerbosityParam :: Full ) ;
{

// If -g full is * not * specified ,
// the parameters declared here will
// be hidden from the parameter file .
// ...

}
params . reset_verbosity () ;

}
params . l e a v e _ s u b s e c t i o n _ p a t h () ;

linear_solver . d ec la r e_ p ar am e te r s ( params ) ;

}

virtual void
pa rs e_ parameters ( ParamHandler & params ) override
{

// Actually parse parameter file .
params . parse () ;

// Analogously to declare_parameters ,
// navigate subsections , read parameters ,
// and possibly store them into class members .
// ...

linear_solver . parse_parameters ( params ) ;

}

virtual void
run () override
{

// Create mesh .
// Setup system .
// Assemble system .
// Solve system .

10

// Output solution .

}

private :

L i n e a r S o l v e r H a nd l e r linear_solver ;
// ...

};

}

3. Illustrative examples

lifex is capable of solving complex multiphysics problems. The functionalities described in the previous
section are pointed out in a series of tutorials that are found in the source code as tests. The tutorials
are sorted by increasing complexity and involve diﬀerent kinds of scalar or vector equations and coupled
problems, solved either monolithically or partitioned. Here, we provide a summary of the tutorials available
and the corresponding PDEs solved.

Tutorial01:

Linear elliptic equation:

Tutorial02:

Linear parabolic equation:

Tutorial03:

Non-linear elliptic equation:

Tutorial04:

Non-linear parabolic equation:

∂u
∂t

∂u
∂t

−∆u = f,

in (−1, 1)3.

− ∆u + u = f,

in (−1, 1)3 × (0, T ].

−∆u + u2 = f,

in (−1, 1)3.

− ∆u + u2 = f,

in (−1, 1)3 × (0, T ].

Tutorial04 AD:

The same as Tutorial04, with the Jacobian matrix assembled via automatic diﬀerentiation.

Tutorial05

Parabolic system of equations, solved monolithically:






∂u
∂t
∂v
∂t

− ∆u + u2 = f,

in (−1, 1)3 × (0, T ],

− ∆v + uv = g,

in (−1, 1)3 × (0, T ].

Tutorial06:

The same as Tutorial05, solved using an explicit partitioned scheme and exploiting the QuadratureEvaluationFEM
capabilities.

Tutorial07:

Cahn-Hilliard equation:






µ −

df
dc

∂c
∂t

− ∆µ = 0,

in (0, 1)3 × (0, T ],

(c) + λ∆c = 0,

in (0, 1)3 × (0, T ].

11

For further details about the mathematical and numerical formulations of all these problems, such as

boundary and initial conditions, please refer to the lifex documentation.

We present below three examples that showcase the main features of lifex. All the results shown are
new, original contributions. First, we prove that the abstract helpers for the advanced numerical schemes
described in Sec. 2 do not aﬀect parallel performance, as the speedup is almost approximately linear up
to thousands of cores; then, we present a multidomain problem where two Stokes problems are solved on
two cubes sharing a common face with proper interface conditions, proving that monolithic and partitioned
schemes for domain decomposition problems can easily be implemented with a negligible computational
overhead due to the parallel transfer of solutions across the interface; ﬁnally, an advanced, fully implicit
numerical solver for the Cahn-Hilliard equation demonstrates the ease of implementation and the enormous
ﬂexibility available to users when dealing with complex multiphysics problems.

3.1. Scalability study

We perform a strong scaling test on Tutorial06, where the following equations are solved:

∂u
∂t
∂v
∂t






− ∆u + u2 = f,

in Ω × (0, T ] = (−1, 1)3 × (0, T ],

− ∆v + uv = g,

in Ω × (0, T ],

u = uex,
v = vex,
u = u0,
v = v0,

on ∂Ω × (0, T ],

on ∂Ω × (0, T ],

in Ω × {0},

in Ω × {0},

where f, g, u0, and v0 are chosen such that the exact solution is

(cid:40)uex(x, t) = t cos(πx0) cos(πx1) cos(πx2),

vex(x, t) = et(cid:107)x(cid:107)2.

The two equations are discretized in time using the BDFHandler of order 1 for u and 3 for v, decoupled
using an explicit partitioned scheme, and linearized using the NonLinearSolverHandler class. Finally, the
FE space discretization consists of linear (quadratic) elements for u (v). The solution u appearing in the
second equation is evaluated using the capabilities of QuadratureEvaluationFEM. The mesh size consists of
2,097,152 cells (average cell diameter: h ≈ 0.027) and 19,121,282 DoFs (2,146,689 for u, 16,974,593 for v),
the time step chosen is equal to ∆t = 0.1, and the simulation is run until T = 1.

The scalability test was run on the GALILEO100 supercomputer available at CINECA (Intel CascadeLake
8260, 2.40GHz). We recorded the total simulation time and partial times spent in the assembly and linear-
solving phases; the speedup for the three quantities shown in Fig. 5 conﬁrms that on such a benchmark
problem, the main lifex data structures scale approximately linearly up to 4, 096 cores. The linear solver
performances slowly degrades beginning at about 512 cores, which is likely due to the limited problem size.
Table 2 reports the summary of the computational costs for the diﬀerent phases of a run of Tutorial06 on
1024 cores.

Some interesting conclusion can be drawn. First, the evaluation of u at quadrature nodes of the FE
space used for the discretization of the equation for v is invoked ≈ 443 million times, which makes a
substantial contribution to the assembly phase (about 27% of the total time): nevertheless, as Fig. 5
shows, the assembly phase still scales almost perfectly linearly, which proves that the implementation of the
QuadratureEvaluation hierarchy of classes introduces a computational overhead that scales almost ideally
in parallel. Moreover, the additional overhead from applying the BDFHandler, NonLinearSolverHandler,
LinearSolverHandler, and PreconditionerHandler wrappers is negligible and does not aﬀect the solver’s
overall parallel performance. This shows that lifex can reach an ideal parallel speedup while the abstract
numerical helpers and multiphysics coupling interface enable a signiﬁcant reduction in the total number of

12

Section
Solver for u: solve time step
Solver for u: non-linear solver
Solver for u: preconditioner assembly + linear solver
Solver for u: system assembly
Solver for u: linear solver
Solver for v: solve time step
Solver for v: system assembly
Solver for v: QuadratureEvaluationFEM initialization
Solver for v: QuadratureEvaluationFEM re-initialization
Solver for v: QuadratureEvaluationFEM evaluation
Solver for v: preconditioner assembly + linear solver
Solver for v: linear solver
Total wallclock time

11
11
33
44
33
11
11
11
22,528
443,418,624
11
11

No. calls Wall time % of total
3.05%
2.85%
2.51%
0.33%
0.17%
95.79%
92.67%
0.000%
0.000%
27.32%
2.29%
1.35%
100%

31.515s
29.460s
25.907s
3.365s
1.799s
988.561s
956.339s
0.000s
0.020s
278.760s
23.601s
13.897s
1031.991s

Table 2: Summary of computational costs of a run of Tutorial06 on 1024 cores.

lines of code compared to a naive implementation based only on deal.II, thus letting the user focus on the
plain discrete formulation of the problems of interest rather than on technical numerics and implementation
details.

3.2. Multidomain problems

The multidomain stokes example was run to demonstrate the parallel performance of the multidomain

capabilities of lifex, with particular reference to the InterfaceHandler class.

The problem being solved is the Stokes model:






−µ∆u + ∇p = 0,

∇ · u = 0,

in Ω,

in Ω,

u = [1, 0, 0]T , on Γin,
u = [0, 0, 0]T , on Γsides,
on Γout,

µ∇u · ν − pn = 0,

where

Ω = (−0.5, 1.5) × (−0.5, 0.5) × (−0.5, 0.5),

Γin = {x = −0.5} ,
Γout = {x = 1.5} ,
Γsides = (∂Ω\(Γin ∪ Γout))o,

and ν denotes the outward unit normal. Ω is split into the two subdomains Ω0 and Ω1 across the interface
Σ, which is deﬁned as

Ω0 = (−0.5, 0.5) × (−0.5, 0.5) × (−0.5, 0.5),
Ω1 = (0.5, 1.5) × (−0.5, 0.5) × (−0.5, 0.5),
Σ = {x = 0.5} .

Denoting the solution on the subdomain Ωi for i = 0, 1 by ui, pi, the multidomain formulation of the

13

Figure 5: Parallel speedup of lifex, demonstrated on Tutorial06. The speedup was computed on total time (red), time
spent in assembling the linear system (including the evaluation of the QuadratureEvaluationFEM ﬁeld) at each time step
(green), and time spent in solving the linear system at each time step through the LinearSolverHandler, together with the
PreconditionerHandler wrappers (blue).

problem is as follows:






−µ∆ui + ∇pi = 0,
∇ · ui = 0,

u0 = [1, 0, 0]T ,
ui = [0, 0, 0]T ,

∇µu1ν − pν = 0,

u0 = u1,

in Ωi,
in Ωi,

on Γin,

on Γsides ∩ ∂Ωi,
on Γout,
on Σ,

−µ∇u0ν0 − p0ν0, = µ∇u1ν1 + p1ν1

on Σ,

where the two conditions on Σ denote the continuity of velocity and stresses across the interface.

This problem can be solved by using either a ﬁxed-point or a monolithic scheme. The goal of this
section is to demonstrate the performance of the InterfaceHandler class. We thus ran a simulation using
6,714,692 DoFs on each subdomain Ωi for i = 0, 1 (6,440,067 for the velocity and 274,625 for the pressure
block), resulting in a total number of DoFs at the interface Σ equal to 49,923. The simulation requires 40
ﬁxed-point iterations to meet the prescribed tolerance of 10−6 on the increment norm of the interface data
u1.

Table 3 reports the summary of the computational costs for the diﬀerent phases of a run of the
multidomain stokes example on 1024 cores. As the number of interface DoFs is much smaller than the
total number of volume DoFs, the time spent in setting up interface maps and transferring the solutions
between the two subdomains Ω0 and Ω1 is negligible with respect to the phases of assembling and solving
the associated linear systems. This demonstrates that the overall computational cost associated with a
multidomain simulation, such as in a ﬂuid-structure interaction (FSI) framework, is largely dominated by
the time spent in assembling and solving the associated linear system(s), whereas the overhead introduced
by the InterfaceHandler class is negligible.

14

Section
InterfaceHandler: build interface maps
InterfaceHandler: extract interface values
InterfaceHandler: apply Dirichlet interface conditions
Problem in Ω0: preconditioner assembly + linear solver
Problem in Ω0: system assembly
Problem in Ω1: preconditioner assembly + linear solver
Problem in Ω1: system assembly
Total wallclock time

No. calls Wall time % of total
0.17%
0%
0%
48%
0.3%
51%
0.3%
100%

24.1s
2.38s
0.875s
6.73e+03s
42.2s
7.05e+03s
41.2s
1.39e+04s

1
80
80
40
40
40
40

Table 3: Summary of computational costs of a run of the example multidomain stokes on 1024 cores.

As a consequence, the speedup results shown in Sec. 3.1 still hold for multidomain problem, solved either

monolithically or partitioned, as long as the parallel partitioning of Ω0 and Ω1 is fairly load-balanced.

3.3. User interface ﬂexibility

Finally, to demonstrate the ease of implementing new solvers for complex multiphysics problems exploit-
ing the capabilities of lifex, we present the development of a spinodal decomposition model of a binary
ﬂuid undergoing shear ﬂow using the advective Cahn-Hilliard equation, a stiﬀ, non-linear, parabolic equation
characterized by the presence of fourth-order spatial derivatives [34]. Spinodal decomposition consists of the
separation of a mixture of two or more components to the bulk regions of both, which occurs, for example,
when a high-temperature mixture of two or more alloys is rapidly cooled.

The equation was discretized by using FEs in mixed form, thus splitting it into a system of two parabolic-

elliptic equations:

µ −






∂c
∂t

− ∆µ = 0,

(c) + λ∆c = 0,

df
dc

∇c · ν = 0,

∇µ · ν = 0,

in Ω × (0, T ] = (0, 1)3 × (0, T ],

in Ω × (0, T ],

on ∂Ω × (0, T ],

on ∂Ω × (0, T ],

c = 0.63 + 0.01 sin (2000πxyz) ,

in Ω × {0},

where f (c) = 100c2(1 − c)2.

The problem is discretized in time using a fully implicit scheme through the BDFHandler and linearized
using the NonLinearSolverHandler class; at each time step, the Jacobian matrix is computed exploiting
automatic diﬀerentiation and the associated linear system is solved using GMRES with an AMG precondi-
tioner. Fig. 6 shows the steady state solution over the computational domain.

Despite the complexity of the FE formulation of the above problem and the sophisticated numerical
schemes adopted for its discretization, the eﬀort involved in implementing such a new solver from scratch
in lifex requires writing approximately 350 lines of C++ code (excluding comments and blank lines). This
is a strikingly small number, especially given the vast ﬂexibility the user has in selecting many modeling
and numerical features, such as choosing the FE degree, setting up the computational domain and physical
parameters, changing the time discretization parameters (including the BDF order), selecting the type of
linear solver and preconditioner and related parameters, and so on.

4. Impact

The impact and wide applicability of lifex are demonstrated by the high number of journal articles,
preprints, conference abstracts, and PhD theses that have already cited it. Computational studies carried

15

Figure 6: Solution of the Cahn-Hilliard equation implemented in Tutorial07. Isosurfaces corresponding to values of the solution
c equal to 0.35 (red), 0.5 (green), and 0.65 (blue) are shown.

out with lifex have appeared in a variety of ﬁelds, mostly originating from but not limited to cardiovas-
cular modeling: cardiac electrophysiology [35, 36, 37, 38], cardiac mechanics, electromechanics, and blood
circulation [39, 32, 40, 41, 42, 43], ﬂuid dynamics [44, 45, 46, 31], FSI [23], poromechanics coupled with
blood perfusion [47, 48], and hemodynamics in patients aﬀected by COVID-19 [49].

Other notable computational models relying on lifex involve a comprehensive and biophysically detailed
electromechanics of the entire human heart [50] (see Fig. 7) and an integrated description of electrophysiology,
mechanics, and ﬂuid dynamics in the human left heart [51, 52]. Other studies oriented toward numerical
methods have addressed the development of a high-order, matrix-free solver for cardiac electrophysiology
[53] and reduced order methods for real-time simulations [54, 55, 56].

lifex provides a fast and stable environment with a gentle learning curve and enables obtaining un-
matched results in terms of model reliability, numerical accuracy, and computational eﬃciency. A compre-
hensive and up-to-date list of publications making use of lifex can be found at https://lifex.gitlab.
io/lifex/publications.html.

5. Conclusions

lifex is a parallel C++ library for simulations of multiphysics, multiscale, and multidomain problems
based on the deal.II FE core. lifex oﬀers several distinctive features, such as abstract helpers for advanced
numerical schemes, a convenient framework to deal with several kinds of coupled problems, and a friendly
interface that allows for the implementation of new complex FE solvers or sophisticated numerical schemes
with all parameters selectable at run time with an economical number of lines of code.

The abstraction layer and the software functionalities presented in Sec. 2 show either high parallel
speedup or negligible computational overheads. Overall, lifex shows a very high computational eﬃciency
and seamless parallel performance; it is thus an invaluable tool that can be run on diverse architectures,
ranging from laptop computers to HPC facilities and cloud platforms.

On the one hand, lifex provides a robust and friendly interface enabling easily accessible and repro-
ducible in silico experiments, without any compromise in terms of computational eﬃciency and numerical
accuracy. On the other, because it is conceived as a research library, lifex can be exploited by scientiﬁc

16

Figure 7: Snapshot of a cardiac electromechanics simulation on a whole-heart geometry during ventricular systole. The color
map shows the intracellular concentration of calcium ions in the myocardium. The computational mesh was generated from
the Zygote Solid 3D Heart Model [57].

computing experts to address new modeling and numerical challenges in an easily approachable development
framework.

Future directions for lifex will include expanding its developer and user bases, maintaining an active and
friendly community that welcomes new contributions, and making new advanced features openly available
to the wider public (see, for example, [58, 59]). In support of these goals, there are a number of technical
areas that will open the way to many upcoming developments. First, when moving towards larger numbers
of cores or DoFs, new bottlenecks are typically encountered, which will be addressed by carefully proﬁling all
parts of the code base and exploiting more eﬃcient algorithms, such as those based on matrix-free methods
[53]. Similarly, large computer clusters will likely beneﬁt from GPUs and similar devices in the near future,
so targeting new architectures and porting algorithms to diﬀerent programming models will be of utmost
importance [9]. Other possible areas of improvement will target more advanced numerical techniques, such
as higher-order or adaptive time advancing schemes, support of multidomain problems discretized over non-
conforming meshes (exploiting, for instance, the INTERNODES technique or RBF interpolators [28, 27, 29]),
and hp-adaptive discretizations to further increase numerical accuracy with a lower computational footprint
[9].

We expect lifex to attract a sizable community of users and developers. Any contribution is highly

appreciated, from code commits to bug reports and suggestions for improvement.

As we approach the exascale era that will be dominated by high-end supercomputers, numerical simu-
lations are expected to be one of the main computational workloads [60]; against this background, lifex
oﬀers transparency, accessibility, reproducibility, and reusability of in silico experiments, within a ﬂexible,
high performance software tool.

6. Conﬂict of Interest

We wish to conﬁrm that there are no known conﬂicts of interest associated with this publication and

that there has been no signiﬁcant ﬁnancial support for this work that could have inﬂuenced its outcome.

17

Acknowledgements

This project has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation program (grant agreement No 740132, iHEART - An Inte-
grated Heart Model for the simulation of the cardiac function; P.I. Prof. A. Quarteroni). We acknowledge
the CINECA award MathBeat under the ISCRA initiative, for the availability of high performance computing
resources and support. The lifex logo was designed by S. Pozzi.

lifex would not have been possible without a large and loyal team working on software development and
review, contributing code and ﬁxes or reporting bugs and suggestions: N. Barnaﬁ, L. Bennati, M. Bucelli,
L. Cicci, S. Di Gregorio, M. Fedele, I. Fumagalli, S. Pagani, R. Piersanti, F. Regazzoni, M. Salvador, S.
Stella, E. Zappon, and A. Zingaro, among many others.

Special thanks go to Profs. A. Quarteroni, L. Dede’, L. Formaggia, P. Gervasio, A. Manzoni, C. Vergara,
and P. Zunino for many stimulating and inspiring discussions and to L. Paglieri for endless patience and
support.

lifex shares the enthusiasm, passion, experience, and dedication to scientiﬁc computing brought by
several people who contributed to the LifeV library [61]. The name itself was inspired by LiFE (Library of
Finite Elements), conceived by Prof. Fausto Saleri.

References

[1] D. Groen, S. J. Zasada, P. V. Coveney, Survey of multiscale and multiphysics applications and communities, Computing

in Science Engineering 16 (2) (2014) 34–43. doi:10.1109/MCSE.2013.47.

[2] D. E. Keyes, L. C. McInnes, C. Woodward, W. Gropp, E. Myra, M. Pernice, J. Bell, J. Brown, A. Clo, J. Connors,
E. Constantinescu, D. Estep, K. Evans, C. Farhat, A. Hakim, G. Hammond, G. Hansen, J. Hill, T. Isaac, X. Jiao,
K. Jordan, D. Kaushik, E. Kaxiras, A. Koniges, K. Lee, A. Lott, Q. Lu, J. Magerlein, R. Maxwell, M. McCourt, M. Mehl,
R. Pawlowski, A. P. Randles, D. Reynolds, B. Rivi`ere, U. R¨ude, T. Scheibe, J. Shadid, B. Sheehan, M. Shephard, A. Siegel,
B. Smith, X. Tang, C. Wilson, B. Wohlmuth, Multiphysics simulations: Challenges and opportunities, The International
Journal of High Performance Computing Applications 27 (1) (2013) 4–83. doi:10.1177/1094342012468181.

[3] D. Arndt, W. Bangerth, B. Blais, M. Fehling, R. Gassm¨oller., T. Heister., L. Heltai, U. K¨ocher, M. Kronbichler, M. Maier,
P. Munch, J. P. Pelteret, S. Proell, S. Konrad, B. Turcksin, D. Wells, J. Zhang, The deal.II library, version 9.3, Journal
of Numerical Mathematics 29 (3) (2021) 171–186. doi:10.1515/jnma-2021-0081.
URL https://www.dealii.org/

[4] M. S. Alnaes, J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg, C. Richardson, J. Ring, M. E. Rognes, G. N. Wells,

The FEniCS project version 1.5, Archive of Numerical Software 3 (2015). doi:10.11588/ans.2015.100.20553.

[5] M. W. Scroggs, I. A. Baratta, C. N. Richardson, G. N. Wells, Basix: a runtime ﬁnite element basis evaluation library,

Journal of Open Source Software 7 (73) (2022) 3982. doi:10.21105/joss.03982.

[6] R. Anderson, J. Andrej, A. Barker, J. Bramwell, J.-S. Camier, J. Cerveny, V. Dobrev, Y. Dudouit, A. Fisher, T. Kolev,
W. Pazner, M. Stowell, V. Tomov, I. Akkerman, J. Dahm, D. Medina, S. Zampini, MFEM: A modular ﬁnite element
methods library, Computers & Mathematics with Applications 81 (2021) 42–74, development and Application of Open-
source Software for Problems with Numerical PDEs. doi:10.1016/j.camwa.2020.06.009.

[7] C. J. Permann, D. R. Gaston, D. Andrˇs, R. W. Carlsen, F. Kong, A. D. Lindsay, J. M. Miller, J. W. Peterson, A. E.
Slaughter, R. H. Stogner, R. C. Martineau, MOOSE: Enabling massively parallel multiphysics simulation, SoftwareX 11
(2020) 100430. doi:10.1016/j.softx.2020.100430.

[8] H.-J. Bungartz, F. Lindner, B. Gatzhammer, M. Mehl, K. Scheufele, A. Shukaev, B. Uekermann, preCICE – a fully
parallel library for multi-physics surface coupling, Computers & Fluids 141 (2016) 250–258, advances in Fluid-Structure
Interaction. doi:10.1016/j.compfluid.2016.04.003.

[9] D. Arndt, W. Bangerth, D. Davydov, T. Heister, L. Heltai, M. Kronbichler, M. Maier, J.-P. Pelteret, B. Turcksin,
D. Wells, The deal.II ﬁnite element library: Design, features, and insights, Computers & Mathematics with Applications
81 (2021) 407–422, development and Application of Open-source Software for Problems with Numerical PDEs. doi:
10.1016/j.camwa.2020.02.022.

[10] A. Quarteroni, A. Valli, Numerical approximation of partial diﬀerential equations, Vol. 23, Springer Science & Business

Media, 2008. doi:10.1007/978-3-540-85268-1.

[11] G. Wilson, D. A. Aruliah, C. T. Brown, N. P. C. Hong, M. Davis, R. T. Guy, S. H. Haddock, K. D. Huﬀ, I. M. Mitchell,
M. D. Plumbley, et al., Best practices for scientiﬁc computing, PLOS Biology 12 (1) (2014) 1–7. doi:10.1371/journal.
pbio.1001745.

[12] S. Balay, S. Abhyankar, M. F. Adams, S. Benson, J. Brown, P. Brune, K. Buschelman, E. M. Constantinescu, L. Dalcin,
A. Dener, V. Eijkhout, W. D. Gropp, V. Hapla, T. Isaac, P. Jolivet, D. Karpeev, D. Kaushik, M. G. Knepley, F. Kong,
S. Kruger, D. A. May, L. C. McInnes, R. T. Mills, L. Mitchell, T. Munson, J. E. Roman, K. Rupp, P. Sanan, J. Sarich,
B. F. Smith, S. Zampini, H. Zhang, H. Zhang, J. Zhang, PETSc Web page (2022).
URL https://petsc.org/

18

[13] M. A. Heroux, R. A. Bartlett, V. E. Howle, R. J. Hoekstra, J. J. Hu, T. G. Kolda, R. B. Lehoucq, K. R. Long, R. P.
Pawlowski, E. T. Phipps, A. G. Salinger, H. K. Thornquist, R. S. Tuminaro, J. M. Willenbring, A. Williams, K. S. Stanley,
An overview of the Trilinos project, ACM Trans. Math. Softw. 31 (3) (2005) 397–423. doi:10.1145/1089014.1089021.

[14] P. C. Africa, mk: environment modules for scientiﬁc computing software, Zenodo (08 2022). doi:10.5281/zenodo.6947700.
[15] P. C. Africa, lifex-env (08 2022). doi:10.5281/zenodo.6947962.
[16] E. Gamma, R. Helm, R. Johnson, J. Vlissides, Design Patterns: Elements of Reusable Object-Oriented Software, Addison-

Wesley Longman Publishing Co., Inc., USA, 1995.

[17] D. Forti, L. Ded`e, Semi-implicit BDF time discretization of the Navier–Stokes equations with VMS-LES modeling in a high

performance computing framework, Computers & Fluids 117 (2015) 168–182. doi:10.1016/j.compfluid.2015.05.011.

[18] J. Brown, P. Brune, Low-rank quasi-Newton updates for robust Jacobian lagging in newton methods, in: Proceedings of the
2013 International Conference on Mathematics and Computational Methods Applied to Nuclear Science and Engineering,
2013, pp. 2554–2565.
URL https://jedbrown.org/files/BrownBrune-LowRankQuasiNewtonRobustJacobianLagging-2013.pdf

[19] P. E. Gill, W. Murray, Quasi-Newton methods for unconstrained optimization, IMA Journal of Applied Mathematics 9 (1)

(1972) 91–108. doi:10.1007/BF01585529.

[20] S. C. Eisenstat, H. F. Walker, Choosing the forcing terms in an inexact Newton method, SIAM Journal of Scientiﬁc

Computing 17 (1) (1996) 16–32. doi:10.1137/0917003.

[21] U. K¨uttler, W. A. Wall, Fixed-point ﬂuid–structure interaction solvers with dynamic relaxation, Computational Mechanics

43 (1) (2008) 61–72. doi:10.1007/s00466-008-0255-5.

[22] H. F. Walker, P. Ni, Anderson acceleration for ﬁxed-point iterations, SIAM Journal on Numerical Analysis 49 (4) (2011)

1715–1735. doi:10.1137/10078356X.

[23] M. Bucelli, L. Dede’, A. Quarteroni, C. Vergara, Partitioned and monolithic algorithms for the numerical solution of

cardiac ﬂuid-structure interaction (2021).
URL https://www.mate.polimi.it/biblioteca/add/qmox/78-2021.pdf

[24] J. Borgdorﬀ, M. Mamonski, B. Bosak, K. Kurowski, M. Ben Belgacem, B. Chopard, D. Groen, P. Coveney, A. Hoek-
stra, Distributed multiscale computing with MUSCLE 2, the Multiscale Coupling Library and Environment, Journal of
Computational Science 5 (5) (2014) 719–731. doi:10.1016/j.jocs.2014.04.004.

[25] A. Quarteroni, R. Sacco, F. Saleri, Numerical mathematics, Vol. 37, Springer Science & Business Media, 2010. doi:

10.1007/b98885.

[26] A. Quarteroni, A. Valli, Domain decomposition methods for partial diﬀerential equations, Oxford University Press, 1999.

URL http://infoscience.epfl.ch/record/140704

[27] S. Deparis, D. Forti, P. Gervasio, A. Quarteroni, INTERNODES: an accurate interpolation-based method for coupling the
Galerkin solutions of PDEs on subdomains featuring non-conforming interfaces, Computers & Fluids 141 (2016) 22–41,
advances in Fluid-Structure Interaction. doi:10.1016/j.compfluid.2016.03.033.

[28] S. Deparis, D. Forti, A. Quarteroni, A rescaled localized Radial Basis Function interpolation on non-Cartesian and non-

conforming grids, SIAM Journal on Scientiﬁc Computing 36 (6) (2014) A2745–A2762. doi:10.1137/130947179.

[29] M. Salvador, L. Dede’, A. Quarteroni, An intergrid transfer operator using radial basis functions with application to

cardiac electromechanics, Computational Mechanics 66 (2) (2020) 491–511. doi:10.1007/s00466-020-01861-x.

[30] N. Paliwal, R. L. Ali, M. Salvador, R. O’Hara, R. Yu, U. A. Daimee, T. Akhtar, P. Pandey, D. D. Spragg, H. Calkins,
N. A. Trayanova, Presence of left atrial ﬁbrosis may contribute to aberrant hemodynamics and increased risk of stroke in
atrial ﬁbrillation patients, Frontiers in Physiology 12 (2021). doi:10.3389/fphys.2021.657452.

[31] I. Fumagalli, M. Fedele, C. Vergara, L. Dede’, S. Ippolito, F. Nicol`o, C. Antona, R. Scrofani, A. Quarteroni, An image-
based computational hemodynamics study of the systolic anterior motion of the mitral valve, Computers in Biology and
Medicine 123 (2020) 103922. doi:10.1016/j.compbiomed.2020.103922.

[32] F. Regazzoni, M. Salvador, P. Africa, M. Fedele, L. Dede’, A. Quarteroni, A cardiac electromechanical model coupled
with a lumped-parameter model for closed-loop blood circulation, Journal of Computational Physics 457 (2022) 111083.
doi:10.1016/j.jcp.2022.111083.

[33] M. Fedele, A. Quarteroni, Polygonal surface processing and mesh generation tools for the numerical simulation of the
cardiac function, International Journal for Numerical Methods in Biomedical Engineering 37 (4) (2021) e3435. doi:
10.1002/cnm.3435.

[34] J. Liu, L. Dede’, J. A. Evans, M. J. Borden, T. J. Hughes, Isogeometric analysis of the advective Cahn–Hilliard equation:
Spinodal decomposition under shear ﬂow, Journal of Computational Physics 242 (2013) 321–350. doi:10.1016/j.jcp.
2013.02.008.

[35] C. Vergara, S. Stella, M. Maines, P. C. Africa, D. Catanzariti, C. Dematt`e, M. Centonze, F. Nobile, A. Quarteroni,
M. Del Greco, Computational electrophysiology of the coronary sinus branches based on electro-anatomical mapping
for the prediction of the latest activated region, Medical & Biological Engineering & Computing (2022). doi:10.1007/
s11517-022-02610-3.

[36] R. Piersanti, P. C. Africa, M. Fedele, C. Vergara, L. Dede’, A. F. Corno, A. Quarteroni, Modeling cardiac muscle ﬁbers in
ventricular and atrial electrophysiology simulations, Computer Methods in Applied Mechanics and Engineering 373 (2021)
113468. doi:10.1016/j.cma.2020.113468.

[37] S. Pagani, L. Dede’, A. Frontera, M. Salvador, L. R. Limite, A. Manzoni, F. Lipartiti, G. Tsitsinakis, A. Hadjis,
P. Della Bella, A. Quarteroni, A computational study of the electrophysiological substrate in patients suﬀering from
atrial ﬁbrillation, Frontiers in Physiology 12 (2021). doi:10.3389/fphys.2021.673612.

[38] S. Stella, C. Vergara, M. Maines, D. Catanzariti, P. C. Africa, C. Dematt´e, M. Centonze, F. Nobile, M. Del Greco,
A. Quarteroni, Integration of activation maps of epicardial veins in computational cardiac electrophysiology, Computers

19

in Biology and Medicine 127 (2020) 104047. doi:10.1016/j.compbiomed.2020.104047.

[39] R. Piersanti, F. Regazzoni, M. Salvador, A. Corno, C. Vergara, A. Quarteroni, et al., 3D–0D closed-loop model for the
simulation of cardiac biventricular electromechanics, Computer Methods in Applied Mechanics and Engineering 391 (2022)
114607. doi:10.1016/j.cma.2022.114607.

[40] M. Salvador, F. Regazzoni, S. Pagani, L. Dede, N. Trayanova, A. Quarteroni, The role of mechano-electric feedbacks and
hemodynamic coupling in scar-related ventricular tachycardia, Computers in Biology and Medicine 142 (2022) 105203.
doi:10.1016/j.compbiomed.2021.105203.

[41] M. Salvador, M. Fedele, P. C. Africa, E. Sung, L. Dede’, A. Prakosa, N. Trayanova, J. Chrispin, A. Quarteroni, Electrome-
chanical modeling of human ventricles with ischemic cardiomyopathy: numerical simulations in sinus rhythm and under
arrhythmia, Computers in Biology and Medicine 136 (2021) 104674. doi:10.1016/j.compbiomed.2021.104674.

[42] L. Ded`e, A. Quarteroni, F. Regazzoni, Mathematical and numerical models for the cardiac electromechanical function, Atti
della Accademia Nazionale dei Lincei, Classe di Scienze Fisiche, Matematiche e Naturali. Rendiconti Lincei - Matematica
e Applicazioni 32 (2) (2021) 233–272. doi:10.4171/rlm/935.

[43] A. Quarteroni, L. Ded`e, F. Regazzoni, Modeling the cardiac electromechanical function: A mathematical journey, Bulletin

of the American Mathematical Society 59 (3) (2022) 371–403. doi:10.1090/bull/1738.

[44] A. Zingaro, M. Bucelli, I. Fumagalli, L. Dede’, A. Quarteroni, Modeling isovolumetric phases in cardiac ﬂows by an

augmented resistive immersed implicit surface method (2022). doi:10.48550/ARXIV.2208.09435.

[45] A. Zingaro, I. Fumagalli, M. Fedele, P. C. Africa, L. Dede’, A. Quarteroni, A. F. Corno, A geometric multiscale model for
the numerical simulation of blood ﬂow in the human left heart, Discrete and Continuous Dynamical Systems - S 15 (8)
(2022) 2391–2427. doi:10.3934/dcdss.2022052.

[46] I. Fumagalli, P. Vitullo, C. Vergara, M. Fedele, A. Corno, S. Ippolito, R. Scrofani, A. Quarteroni, Image-based compu-
tational hemodynamics analysis of systolic obstruction in hypertrophic cardiomyopathy, Frontiers in Physiology (2022)
2437doi:10.3389/fphys.2021.787082.

[47] N. A. Barnaﬁ Wittwer, S. D. Gregorio, L. Dede’, P. Zunino, C. Vergara, A. Quarteroni, A multiscale poromechanics model
integrating myocardial perfusion and the epicardial coronary vessels, SIAM Journal on Applied Mathematics 82 (4) (2022)
1167–1193. doi:10.1137/21M1424482.

[48] S. Di Gregorio, M. Fedele, G. Pontone, A. F. Corno, P. Zunino, C. Vergara, A. Quarteroni, A computational model applied
to myocardial perfusion in the human heart: From large coronaries to microvasculature, Journal of Computational Physics
424 (2021) 109836. doi:https://doi.org/10.1016/j.jcp.2020.109836.

[49] L. Ded`e, F. Regazzoni, C. Vergara, P. Zunino, M. Guglielmo, R. Scrofani, L. Fusini, C. Cogliati, G. Pontone, A. Quarteroni,
Modeling the cardiac response to hemodynamic changes associated with COVID-19: a computational study, Mathematical
Biosciences and Engineering 18 (4) (2021) 3364–3383. doi:10.3934/mbe.2021168.

[50] M. Fedele, R. Piersanti, F. Regazzoni, M. Salvador, P. C. Africa, M. Bucelli, A. Zingaro, L. Dede’, A. Quarteroni,
A comprehensive and biophysically detailed computational model of the whole human heart electromechanics (2022).
doi:10.48550/ARXIV.2207.12460.

[51] M. Bucelli, A. Zingaro, P. C. Africa, I. Fumagalli, L. Dede’, A. Quarteroni, A mathematical model that integrates cardiac
electrophysiology, mechanics and ﬂuid dynamics: application to the human left heart (2022). doi:10.48550/ARXIV.2208.
05551.

[52] M. Bucelli, M. G. Gabriel, G. Gigante, A. Quarteroni, C. Vergara, A stable loosely-coupled scheme for cardiac electro-

ﬂuid-structure interaction (2022). doi:10.48550/ARXIV.2210.00917.

[53] P. C. Africa, M. Salvador, P. Gervasio, L. Dede’, A. Quarteroni, A matrix-free high-order solver for the numerical solution

of cardiac electrophysiology (2022). doi:10.48550/ARXIV.2205.05136.

[54] L. Cicci, S. Fresca, S. Pagani, A. Manzoni, A. Quarteroni, Projection-based reduced order models for parameterized
nonlinear time-dependent problems arising in cardiac mechanics, Mathematics in Engineering 5 (2) (2023) 1–38. doi:
10.3934/mine.2023026.

[55] F. Regazzoni, M. Salvador, L. Ded`e, A. Quarteroni, A machine learning method for real-time numerical simulations of
cardiac electromechanics, Computer Methods in Applied Mechanics and Engineering 393 (2022) 114825. doi:10.1016/j.
cma.2022.114825.

[56] F. Regazzoni, A. Quarteroni, Accelerating the convergence to a limit cycle in 3D cardiac electromechanical simulations
through a data-driven 0D emulator, Computers in Biology and Medicine 135 (2021) 104641. doi:10.1016/j.compbiomed.
2021.104641.

[57] Z. M. G. Inc., Zygote solid 3d heart generation ii, Development Report (2014).
[58] P. C. Africa, R. Piersanti, M. Fedele, L. Dede’, A. Quarteroni, An open tool based on lifex for myoﬁbers generation in

cardiac computational models (software), Zenodo (01 2022). doi:10.5281/zenodo.5810268.

[59] P. C. Africa, R. Piersanti, M. Fedele, L. Dede’, A. Quarteroni, An open tool based on lifex for myoﬁbers generation in

cardiac computational models (2022). doi:10.48550/ARXIV.2201.03303.

[60] S. Alowayyed, D. Groen, P. V. Coveney, A. G. Hoekstra, Multiscale computing in the exascale era, Journal of Computa-

tional Science 22 (2017) 15–25. doi:https://doi.org/10.1016/j.jocs.2017.07.004.

[61] L. Bertagna, S. Deparis, L. Formaggia, D. Forti, A. Veneziani, The LifeV library: engineering mathematics beyond the

proof of concept (2017). doi:10.48550/ARXIV.1710.06596.

20

