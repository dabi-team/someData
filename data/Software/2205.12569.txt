2
2
0
2

t
c
O
6

]

R
C
.
s
c
[

2
v
9
6
5
2
1
.
5
0
2
2
:
v
i
X
r
a

1

Towards a Fair Comparison and Realistic
Evaluation Framework of Android Malware
Detectors based on Static Analysis and Machine
Learning

Borja Molina-Coronado, Usue Mori, Alexander Mendiburu,and Jose Miguel-Alonso

Abstract—z As in other cybersecurity areas, machine learning (ML) techniques have emerged as a promising solution to detect Android
malware. In this sense, many proposals employing a variety of algorithms and feature sets have been presented to date, often reporting
impresive detection performances. However, the lack of reproducibility and the absence of a standard evaluation framework make these
proposals difﬁcult to compare. In this paper, we perform an analysis of 10 inﬂuential research works on Android malware detection using
a common evaluation framework. We have identiﬁed ﬁve factors that, if not taken into account when creating datasets and designing
detectors, signiﬁcantly affect the trained ML models and their performances. In particular, we analyze the effect of (1) the presence of
duplicated samples, (2) label (goodware/greyware/malware) attribution, (3) class imbalance, (4) the presence of apps that use evasion
techniques and, (5) the evolution of apps. Based on this extensive experimentation, we conclude that the studied ML-based detectors
have been evaluated optimistically, which justiﬁes the good published results. Our ﬁndings also highlight that it is imperative to generate
realistic experimental scenarios, taking into account the aforementioned factors, to foster the rise of better ML-based Android malware
detection solutions.

Index Terms—Android malware detection, machine learning, mobile security, experimental analysis, static analysis

(cid:70)

1 INTRODUCTION

Over the past decade we have witnessed impressive
advances in mobile devices. Along with the hardware,
operating systems (OS) designed for the mobile market have
experienced pairwise functionality improvements. With a
market share near 72% as of the last quarter of 2021 [1],
the Android platform is the leading mobile OS. It has an
open source nature and is available for multiple processor
architectures. These facts, along with the availability of a
well documented development framework that enables a
rich set of services (voice and image recognition, contactless
payments, etc.), have contributed to the adoption of Android
beyond smartphones [2–4].

Coupled with this growing popularity, the increasing
attention paid by malware writers to this OS has highlighted
the risks to which users are exposed [5]. Data stored on
mobile devices is of vital importance, sensitive for users,
and has become a valuable target for attackers. According
to Kaspersky, adware and banking malware targeting these
devices were two of the main security threats in 2020, even
being detected in trusted app markets such as Google Play
[6].

Aware of these problems, researchers have seen machine
learning (ML) techniques as a promising solution for the

• Borja Molina Coronado, Alexander Mendiburu and Jose Miguel-Alonso
are with the Dept. of Computer Architecture and Technology, University of
the Basque Country UPV/EHU, Donostia, Spain.
E-mail:{borja.molina, alexander.mendiburu, j.miguel}@ehu.es

• Usue Mori is with the Dept. of Computer Science and Artiﬁcial Intelligence,

University of the Basque Country UPV/EHU, Donostia, Spain.
E-mail: usue.mori@ehu.es

implementation of Android malware detectors [7]. ML
methods leverage on app data to identify signals that are
useful for detecting malware. To this end, malware detection
can follow one of these strategies (or both in combination):
(1) anomaly-based detection focuses on building proﬁles from
goodware so that deviations from those proﬁles are ﬂagged
as dangerous; (2) misuse-based detection, instead, focuses on
learning the characteristics of both malware and goodware
in order to identify their presence in new apps [8]. Most of
the research works on Android malware detection belong to
the second group. More speciﬁcally, relying on supervised
ML algorithms for their detection mechanisms [9, 10].

To perform the detection of malware using ML algorithms
(either for misuse or anomaly-based systems), apps need
to be preprocessed in order to extract the set of features
that best describe their behavior. This task is performed
using dynamic or static software analysis techniques. In
dynamic analysis, the behavior of an app is monitored in a
controlled environment (sandbox), where user and system
interactions are simulated. Static analysis is based on the
inspection of the ﬁles contained in the application package
(APK) without needing to run the code. These techniques
have their own advantages and drawbacks. On the one hand,
through dynamic analysis, it is possible to access the code
that is loaded and executed in runtime. However, the success
of this analysis, in terms of coverage of code, greatly depends
on the simulation mechanism and the absence of sandbox
evasion instruments in the apps [11]. On the other hand,
static analysis is able to evaluate all the information present
in the APKs, but its success lies in the absence of evasion

 
 
 
 
 
 
techniques such as obfuscation or dynamic loading of code
[12]. Both analysis approaches are complementary and can
be combined for Android malware detection [13, 14].

One of the main difﬁculties faced by researchers when
proposing, developing and testing Android malware detec-
tors is the absence of a common and realistic evaluation
framework. This framework should include appropriate and
labeled datasets, essential for training and/or testing ML
algorithms. In particular, and as we illustrate in this paper,
most publicly available datasets are obsolete, only contain
malware, repeated samples, or comprise an insufﬁcient
number of samples. In view of this, authors opt for building
ad-hoc, custom datasets by downloading app samples from
different sources and labeling them using tools such as
VirusTotal1. This process is not only expensive, but also
complicates the reproducibility and comparison of ML-based
malware detection proposals.

The issue of reproducibility is aggravated by the unavail-
ability of the code that implements the proposed methods, or
by the omission in their respective publications of important
details that allow their implementation. This is specially
manifested for methods using ML algorithms that require
tuning a large number of parameters in order to perform
properly [15], as this information is not often provided. The
same is true for the evaluation procedures. In many cases,
they are not clearly described or are designed assuming very
optimistic scenarios [15, 16].

The main objective of this study is to perform a fair
comparison of Android malware detection proposals already
published in the literature, shedding light about their actual
effectiveness. Given the vast amount of proposals presented
over the years, as well as the absence of common and realistic
evaluation criteria, performing a fair comparison of methods
is not a straightforward task. We have chosen 10 popular
detectors based on static analysis that use different features
and ML methods, and compared them under a common
evaluation framework. In many cases, a re-implementation of
the algorithms used in the detectors has been required due to
the lack of the original authors’ implementations. The result
of this extensive implementation and experimental work
is, to the best of our knowledge, the most comprehensive
comparative study on Android malware detection methods
presented to date. The scientiﬁc contributions of this work
are summarized as follows:

1) We present a number of factors that negatively
affect the accuracy of Android malware detectors.
In particular, we consider ﬁve conditions that are
present in real life but are often ignored when
proposing malware detectors: (1) datasets contain
a large amount of apps that are almost identical to
others; (2) there is not always agreement on what
is goodware and what is malware, and some apps
lack enough consensus to be considered malicious
or benign; (3) there is more goodware than malware;
(4) malware authors may resort to evasion attempts
using obfuscation techniques; and, (5) malware and
goodware evolve over time. Then, we argue that it
is imperative to consider all these factors when de-

1. https://www.virustotal.com

2

signing and evaluating Android malware detectors
in order to provide realistic performance values.
2) We analyze the performance of state-of-the-art ML-
based Android malware detection approaches when
the above factors are taken into account. For this
purpose, we selected 10 highly inﬂuential detectors
that make use of static analysis techniques. We
show that the outstanding performances provided
by the authors of these approaches are unrealistically
optimistic due to design and evaluation ﬂaws.
3) We highlight the lack of reproducibility of published
work in this area. In this sense, we make the code
and datasets used in this comparative work publicly
available.

4) We discuss future research lines in Android malware
detection aiming to solve the identiﬁed design and
evaluation ﬂaws.

The structure of this paper is organized as follows. Sec-
tion 2 reviews the most important works related to the topic
of this paper. Section 3 includes basic concepts about Android
apps, static analysis and the type of data that can be obtained
using this technique. The state-of-the-art detectors based on
supervised ML classiﬁers that are considered in this work
are presented in Section 4. Section 5 describes how datasets
for malware detection are typically built, their limitations
(factors that should be considered when constructing them
but are often ignored) and the reproducibility problems that
the use of custom datasets entails. In Section 6, we present
our experimental setup. Section 7 discusses the limitations of
the selected Android malware detectors based on the analysis
of the factors identiﬁed in Section 5. Section 8 discusses the
characteristics that a realistic evaluation framework should
take into account, as well as the challenges and future
research lines in Android malware detection. We conclude
this paper in Section 9.

2 RELATED WORK
Android malware detection is a well-studied area in the
information security literature. Despite this, only a few
experimental studies have focused on analyzing what factors
impact the performance of malware detectors, which is the
main theme of this paper. Most of them focus their analysis
on a small group of detectors with similar characteristics.
To date, the most comprehensive study is [15]. This article
analyzed two different sources of bias in the evaluations
of three detection algorithms. The ﬁrst, known as spatial
bias, comes from the differences between the proportion of
samples from each class in the dataset. The second type,
temporal bias, is related to the inclusion of future knowledge
during model training. Experiments to test for spatial bias
concluded that the ratio between classes is determinant in the
results reported by the authors. Experiments on the impact of
temporal bias demonstrated that models tend to misclassify
malware as time passes, whereas the accuracy of goodware
remains stable over time. The conclusions drawn from this
work are similar to those obtained in two other previous
studies, each of which considers only one detector in their
experiments [16, 17].

The use of obfuscation techniques with malware apps
to evade detection was studied in [18]. The work presented

an analysis for ten Android commercial anti-virus products,
testing them by applying different obfuscation techniques
to malware. The results proved that all the analyzed tools
worsened their effectiveness to detect malware with at least
one type of obfuscation. This work served to highlight
the weaknesses of these commercial solutions. However,
given that the details of the detectors are not public, speciﬁc
conclusions about how obfuscation may affect ML detectors
for Android cannot be drawn from this study. Another work
assessed the ability of a ML malware detector for Windows
to identify packed2 malware [19]. An extensive set of experi-
ments was performed to conﬁrm whether packed samples
were identiﬁed due to the presence of traces left by packers
or by the behavior of the sample. The authors concluded
that ML detectors relying on static analysis features tend to
focus on signs of obfuscation. Thus, putting in question the
feasibility of these approaches against Windows malware
due to the amount of false positives. Nonetheless, further
studies need to be conducted to evaluate if these ﬁndings
also apply to Android malware detectors.

The presence of duplicates in datasets when designing
and evaluating detectors is partially discussed in [20]. Prelim-
inary experiments are carried out for two ML detectors, one
based on the usage of API calls and one using permissions.
The authors of that work postulate towards the existence
of a relation between duplicates and the obtention of over-
estimated performances for models. However, additional
analysis would be required to conﬁrm this hypothesis, as
the implicit reduction of the size of the dataset for extreme
duplicate removal conﬁgurations (using ample similarity
thresholds) may eventually originate similar results.

In summary, previous works have focused on the anal-
ysis of some speciﬁc evaluation ﬂaws affecting detectors,
including: spatial bias [15, 17, 21], temporal bias [15, 21], the
impact of obfuscation [18, 22] or the inﬂuence of duplicates
in the data [20]. However, these analyses were conducted
over a small number of methods [16, 17, 20], in some cases,
making use of similar feature sets [15]. Some studies were
not deep enough [20] or were performed exclusively for
commercial black-box detectors, so that the details about
their detection mechanisms, i.e., their features or whether
they are based on signatures or ML algorithms, are not
disclosed and results cannot be extrapolated [18, 22]. None
of these published works has taken into consideration the
bias caused due to the removal from datasets of apps that
are neither clearly goodware of malware, or has analyzed
the effect of using different thresholds to label an app
as malware on many existing detectors. Thus, we believe
that a more comprehensive analysis, that includes all the
identiﬁed factors and a higher number of detectors using
different features and ML methods, becomes mandatory in
the Android malware detection area. Contrary to published
literature, this manuscript not only provides a comparative
framework that evidences the lack of realistic proposals and
illustrates many extended design and evaluation biases, but
also, gives recommendations and identiﬁes future research
lines towards the proposal of realistic malware detectors.

2. Developers of malware apps use packers to evade detection and
analysis. A packer tool compresses and encrypts together data, resources
and the code of executable ﬁles. These elements are unpacked and
executed at run time.

3

3 BACKGROUND
This section slightly introduces some background necessary
to understand the rest of the manuscript. It includes descrip-
tions about the structure of Android apps and the type of
information that can be inspected through static analysis of
apps to feed ML algorithms.

3.1 Android Apps

Android apps are distributed in APKs (Android Application
Packages). An APK is a ZIP compressed ﬁle that contains the
resources that are essential for the execution of the app on
the system: the manifest data and the compiled code.

3.1.1 Manifest Data

The manifest ﬁle of an Android app deﬁnes a set of properties
and components that the app requires from the platform in
order to work. This ﬁle is in XML format and it is divided
into three main blocks or sections:

• The Application Components block deﬁnes the elements
of the app that interact with the OS while the app is
running or when a speciﬁc action is requested to the
OS by the user. These components implement back-
ground functionality (services), manage user screens
and app interactions (activities), enable app interac-
tions with other OS components or apps (broadcast
receivers and intent ﬁlters) and the interfaces to share
data with other processes (content providers).

• The Hardware and Software Features block deﬁnes the
OS properties and functionalities that the app requires
to function. This includes software features such as
backup support, user account management, input
methods, etc. Hardware features include elements
such as the camera, bluetooth transmission, ﬁnger-
print sensor, etc. Declaring requested features is use-
ful, for example, to prevent an app from running on a
phone that does not fulﬁll the required speciﬁcations.
• The Permissions block indicates the features that are
required by an app but are protected by the OS.
Access to these functionalities must be explicitly
granted by the user. By default, Android apps do not
have permissions to perform actions that could com-
promise the OS, user information, or other apps. Thus,
permissions are needed to access the microphone,
camera, contact list, Internet connection, location, etc.

3.1.2 Application Code

Android apps are generally developed in Java or Kotlin
(Google’s preferred language for Android development) and
transformed into Dalvik bytecode format during the com-
pilation process. The Dalvik bytecode runs on the Android
virtual machine, which serves as a platform-independent
environment. Interactions between the hardware components
of mobile devices, directly managed by the operating system,
and apps, managed by the virtual machine, are performed
through API libraries. These APIs provide a common way
to access to the hardware capabilities required by the apps.
Thus, abstracting the programmer from the particularities of
devices.

The Dalvik bytecode of apps is located inside the APK
in the classes.dex ﬁle. It includes all user-deﬁned classes

and functions, as well as constants and variable deﬁnitions.
External libraries, such as the Android framework, are not
part of the content of this ﬁle.

3.2 Static Analsis of Android Apps

Static analysis is a software technique that inspects apps to
extract their characteristics without the need to execute their
code and monitor their behavior at runtime [23]. By means of
static analysis, all execution paths present in the code and all
the information in the ﬁles of an app can be inspected. This
is done by using tools with code interpretation mechanisms
that extract understandable structures describing the internal
functions of apps, e.g., call graphs, data ﬂows, statistical
measures about code structure, etc. Such information is then
converted into a common set of explanatory features that
will later be processed by ML algorithms.

To perform the static analysis of the content of APK ﬁles,
tools such as AXMLprinter3 can be used to make ﬁelds
and components declared inside the manifest ﬁle accessible.
Instead, the classes.dex ﬁle can be converted to higher level
format using decompilers such as bakSmali4. After that,
different features can be obtained, including information
about instructions, methods, classes, strings and the usage
of API calls [9]. It is also possible to build different graph
structures representing the code. These include Call Graphs,
built following the call instructions (invoke) present in
the code; and Control Flow Graphs, which are created
considering also the jumps in the code caused by conditional
and loop statements (if, switch, for, while...). In
both types of graphs, a node represents a method or a block
of instructions that can only be executed sequentially, i.e.,
a basic block; and the edges represent the execution ﬂow
between nodes [23].

After performing static analysis, the data obtained from
APKs is mapped into feature vectors that represent the
apps in a structured way, suitable for processing by ML
algorithms. Figure 1 depicts the binary mapping as applied
to represent three apps by means of their strings, API calls
and permissions. It uses the values 1 or 0 to denote whether
a feature is present in an app’s code or not. Other mappings
are also possible, for example, frequency encoding accounts
for the number of times a feature is present in the app code.

4 ANDROID MALWARE DETECTORS BASED ON
SUPERVISED CLASSIFIERS
Supervised classiﬁcation is a popular ML task in which the
objective is to learn a mapping or classiﬁer ˆH : X → C,
where X is a space of features that describes the samples
(the input), and C is the space of class labels (the output).
To do so, the ML algorithm is fed with a set of labeled
samples D = {(x1, c1), ..., (xn, cn)} called the training set.
In the context of Android malware detection using ML
classiﬁers, the training set consists of a set of labeled apps.
Each app or sample is described as a vector of t features
xi = (x1, x2, ..., xt) which, in the context of this work, are
computed through the static analysis of its APK ﬁle. The
binary class label ci takes a value of 0 for goodware apps

4

and a value of 1 for malware apps. Once the classiﬁer is
trained, given the feature vector of a new app xk, it will
return its predicted class label ˆck (0 for goodware and 1 for
malware).

As mentioned, for the purpose of this work we selected
10 malware detection methods for Android. Eight of them
are selected because they have been published in in top-tier
journals and represent the most important papers in this area
of study, in terms of relevance, according to IEEExplore, Sco-
pus and Google Scholar. We have also added two additional
proposals, which have been considered in other experimental
comparative works, namely the Drebin [25] and BasicBlocks
[16]. All of these are misuse-based detectors, using different
supervised ML classiﬁcation algorithms and with different
sets of features extracted from the apps. In addition, given
the large number of features they obtain from APKs, some
approaches apply dimensionality reduction algorithms [8].
Table 1 outlines the key aspects of these detectors. As can
be seen, they were published between 2014 and 2019. Due to
the lack of standard datasets, an aspect which is studied in
depth in the next section, they all used custom collections of
apps to perform their experiments. In many cases, for these
proposals, sample selection, class ratios or labeling criteria
not only vary, but are not properly documented also. This
makes it difﬁcult to reproduce the experiments, compare
proposals or measure their contribution level. Furthermore,
none of their authors, excepting those of HMMDetector
and MaMaDroid, have made their code publicly available,
hindering the possibility to re-run the detectors, and compli-
cating their use for comparison purposes. Because of these,
a direct comparison of the performances reported in the
corresponding articles does not provide useful information.
Given the unavailability of the code of most of the
detectors, we have had to re-implement them in order to
perform the experimental analysis contained in this work.
We tried to reimplement every proposal in the best possible
way, so that we can offer fair and accurate comparisons.
However, this was a complex task, mainly due to the lack of
details concerning crucial aspects such as parameter values,
the feature extraction and training processes of the classiﬁer,
etc. The most problematic approaches in this sense have been
MultimodalDL and PermPair. Unfortunately, we were unable
to implement MultiModalDL [31], due to its complexity and
the omission of information regarding feature computations
(number of centroids, thresholds for similarity computation).
As for PermPair [32], we implemented the detector as
indicated in the original publication. However, the results
obtained by us are far from those originally reported by
the authors. For these reasons, we omit these works from
subsequent analysis.

We have implemented the remaining eight malware
detectors listed in Table 1 in Python language5. Note that
we have carried out the necessary static analysis tasks on
APKs to obtain the feature sets speciﬁc to each detector,
as explained in Section 3. For this purpose, we used the
Androguard framework [33], a widely-used static analysis
and reverse engineering tool for Android APK ﬁles. For

3. https://github.com/tracer0tong/axmlprinter
4. https://github.com/JesusFreke/smali

5. In the case of DroidDetector [27], we were able to implement the
detector assuming that feature selection was used to identify the set of
most relevant API calls used by the algorithm.

1
P
P
A

SUBSCRIBE

getSmsDetails()

getDeviceId()

getLine1Number()

getDeviceId()

READ_SMS

SEND_SMS

2
P
P
A

http://m4lw4.re

0x3d93cb

openConnection()

exec()

getConnectionInfo()

getInstalledApps()

INTERNET

3
P
P
A

0x3d93cb

wallpaper_dev

getDeviceId()

setWallpaper()

INTERNET

SET_WALLPAPER

5

wallpaper_dev
0
0
1

http://m4lw4.re
0
1
0

0x3d93cb
0
1
1

SUBSCRIBE
1
0
0

getInstalledApps
0
1
0

openConnection
0
1
0

getSmsDetails
1
0
0

getDeviceId
1
0
0

setWallpaper
1
0
1

getLine1Number
0
0
1

getConnectionInfo
1
0
0

exec
0
1
0

APP1
APP2
APP3

SET_WALLPAPER INTERNET

SEND_SMS READ_SMS

0
0
1

0
1
1

1
0
0

1
0
0

Fig. 1: Binary encoding of the strings (in orange), API calls (in black) and permissions (in blue) for three apps

dimensionality reduction, ML algorithms and the assess-
ment of the performance of detectors, we employed well-
established libraries such as scikit-learn [34] and numpy [35].
Our goal is not only to perform a comparative analysis
between Android malware detectors, but also to contribute
to the reproducibility and progress in the area by releasing
our programs and the instructions for their use in our GitLab
repository6.

5 DATASETS FOR ANDROID MALWARE DETEC-
TION. DRAWBACKS AND REPRODUCIBILITY ISSUES

When building a supervised classiﬁer, the dataset and the
procedures used for training and evaluation are key factors
in order to develop robust and well-performing models
[36]. Consequently, for ML-based malware detection, the
collection of apps and the process for their generation are
particularly important, both in terms of applicability to
real-world scenarios and reproducibility. In this section, we
identify and discuss a set of factors that must be considered
when creating datasets. We then describe the datasets most
commonly used by the Android malware detection research
community, pointing out their main drawbacks from the
point of view of the identiﬁed factors.

5.1 Factors Under Analysis

We have identiﬁed ﬁve factors that have a major impact on
the performance of ML based malware detectors for Android,
and that should be considered when creating the datasets
used to train and evaluate them. It should be noted that,
in our opinion, these factors are very important, but this
does not imply that they are the only inﬂuential aspects in
the design and evaluation of malware detectors based on
supervised classiﬁcation.

5.1.1 REDUNDANCY

The purpose of Android malware detectors is to identify
every type of malware regardless of their level of incidence.
This includes a wide range of samples pertaining to different
malware families and subfamilies. Typically, malware sam-
ples within a family or subfamily exhibit analogous code and
data structures to perform the same malicious activities [37].
This characteristic of malware results in datasets with groups
of samples that are very similar from the point of view of
detectors. This means that, within these groups, samples

6. To be published after acceptance

tend to be represented using identical feature sets, resulting
in redundancies in the data fed to ML models. On the one
hand, redundant samples in the training set cause bias in
ML algorithms because decisions towards groups with many
representatives have a great impact in the accuracy of the
model [36]. This makes models to ignore non-redundant
samples, since predictions for these instances yield little
improvement in accuracy. Resulting, thus, on detectors with
a limited ability to identify uncommon (less represented)
forms of malware. On the other hand, the presence of
duplicates in the evaluation set leads to inﬂated or poor
performances, depending on whether or not these large
groups of similar apps are correctly classiﬁed. Even if the
presence of duplicates or very similar apps has an important
effect on the results obtained by a detector, to the best of
our knowledge, only preliminary work has considered it
as an issue [20]. As a matter of fact, none of the detectors
considered in our comparative analysis were assessed taking
into account redundancies in the data as an major source of
bias.

5.1.2 LABELING-GREYWARE
One of the main problems when building datasets for An-
droid malware detectors based on supervised classiﬁcation
lies in the need to have labeled samples. The true nature of an
app is not known in many cases. This implies that apps need
to be analyzed in order to assign them a label concerning
their maliciousness. Ideally, labeling should be carried out
manually, by experts, to guarantee the highest number
of error-free labels. However, using human annotators to
perform this task is costly, both in terms of time and
resources [38] and it is not error-free either. Accordingly,
researchers usually rely upon automatic procedures to label
their customized datasets.

In the simplest labeling approach, apps are labeled
according to the source they were obtained from [32], e.g.,
those downloaded from trusted repositories (such as Google
Play) are goodware, whereas those from repositories such
as VirusShare are labeled as malware. This procedure relies
on the analysis, either automatic or manual, performed by
the managers of these repositories. In other cases, apps are
scanned using a collection of antivirus programs and labeled
depending on the number of positive (malware) alerts [39].
In this context, VirusTotal is a tool that allows users to upload
ﬁles, including APKs, and scan them using a collection of
antivirus engines. VirusTotal results, based on the number
of positive alerts raised by a ﬁle (we refer to this number
as VTD, from VirusTotal Detections), are widely used as the

TABLE 1: Android malware detection methods included in this analysis. They are the most relevant works according to the
literature

Method

Pub.
Year

AndroDialisys [24]

2017

BasicBlocks [16]

2016

Dataset
(#samples)
Google Play1(1 846)
Drebin (5 560)
Google Play1(52 000)
Android Genome (1 260)

APK Features

Encoding

Dim.
Reduction

Permissions
Intent Filters

Binary

-

Basic Blocks

Binary

Mutual Info

ML
Algorithm
Bayesian
Network
Random
Forest

Reported
Performance
TPR: 0.955
FPR: 0.044
TPR: 0.91
Precision: 0.94

6

Drebin [25]

2014

Benign1(123 453)
Drebin (5 560)

DroidDet [26]

2018

DroidDetector [27]

2016

HMMDetector [28]

2016

ICCDetector [29]

2016

MaMaDroid [30]

2019

MultimodalDL [31]

2018

PermPair [32]

2019

Google Play1(1 065)
VirusShare1(1 065)
Google Play1(20 000)
Android Genome (1 260)
Contagio1(500)
Google Play1(5 560)
Drebin (5 560)
Google Play1(12 026)
Drebin1(5 264)
Google Play1(8 447)
Drebin (5 560)
VirusShare1(29 933)

Google Play1(20 000)
Android Genome (1 260)
VirusShare1(20 000)

Benign1(6 993)
Android Genome (1 264)
Drebin1(2 764)
Contagio1(250)
Koodous1(2 975)
PwnZen1(300)

Permissions
App. Components
Hw. Components
Intents
Strings
API calls
Permissions
Intents
API calls

Permissions
API calls

Binary

-

SVM

TPR: 0.939
FPR: 0.01

Binary

tf-idf rank

Rotation
Forest

TPR: 0.884
Precision: 0.886

Binary

Mutual Info2

DBN

Opcodes

Sequence

HMM

Random
Forest

Intents
App. Components

Binary and
frequencies

Mutual Info

SVM

TPR: 0.981
FPR: 0.201

TPR: 0.968
Precision: 0.96
TPR: 0.931
FPR: 0.006

API call graph

Frequencies

App. Components
Intents
Hw. Components
Permissions
Opcodes
Strings
API calls

Binary

-

-

Random
Forest

TPR: 0.97
Precision: 0.95

DL

TPR: 0.99
Precision: 0.98

Permissions

Frequencies

Sequential
removal

Nearest
Neighbors

TPR: 0.951
FPR: 0.042

1 Unspeciﬁed set or labeling criteria from this source.
2 In the original paper, authors use a ﬁltered set of API calls without stating the criterion used to extract this set. As this information is

unavailable, we decided to use Mutual Information to select the most important API calls based on the training data.

criterion to label samples. To this end, a common procedure
makes use of thresholds to establish the level of consensus
required to label an APK as malware or goodware [40].

Leveraging on the VTD leads authors to decide what
threshold is adequate for malware and goodware, so it is
common to ﬁnd disparities in the literature. For example,
[17] set a threshold of VTD≥10 to ﬂag an app as malware,
while in [15], authors set this threshold at a much lower
value of 4. In both cases, the condition for labeling an app
as goodware was set to VTD=0. The choice of thresholds
not only means that the malware or goodware deﬁnitions
vary among articles, but also inﬂuences the amount of apps
that fall between these categories. These apps, which we
refer to as greyware, lack enough consensus by antivirus
programs to be considered as goodware or malware with
guarantees. Because of that, many researchers discard these
apps when training their detectors. However, greyware
is an important part of the Android ecosystem [41] and
will appear whenever the detector is deployed in a real
environment. Thus, discarding them at training time will
hinder the effectiveness of a detector once deployed. Indeed,
even if common thresholds were applied to label the data,

the VTD value provided by VirusTotal changes over time
[42], for example, due to engine updates aimed at improving
detection capability, or as engines are added or removed from
the platform. Consequently, disparities may occur between
models validated with exactly the same collection of apps
but labeled at different times [40].

5.1.3 IMBALANCE

The third factor we consider is related to the ratio of malware
and goodware in the dataset, especially in the data used for
training. In real life, most apps are innocuous in the security
aspect, with the actual proportion of malware being about
10% of the total number of Android apps [15], but with
this ratio being highly dependent on the particular market
from which apps are downloaded [43, 44]. As can be seen
in Table 1, researchers have trained their detectors following
their own criterion and assuming different class proportions.
However, the choice of the class ratio is important when
training ML models, since classical ML classiﬁers tend to
be biased towards the majority class in highly unbalanced
scenarios. Therefore, ignoring class imbalance can lead to

a false perception about the true performance of detectors
under real working conditions [15].

5.1.4 EVASION

Some malware authors are aware that their apps will be
examined by malware detectors, so they try to bypass
detection by using different evasion techniques. As tech-
nology evolves, different threats may appear and authors
should be aware that their systems will be the target of
attacks. Thus, a proper evaluation of the robustness of
malware detectors against such threats should be taken into
consideration. Unfortunately, none of the proposals included
in this comparative work have considered the effects of
evasion techniques, such as obfuscation, in their evaluations.

5.1.5 EVOLUTION

The last factor we highlight is related to the evolution of
malware and goodware over time. This implies that the
behavior of apps rarely remains static for a long time. Thus,
the characteristics of newer or mutated apps may differ
from those obtained from apps observed earlier, during the
training of a detector. Despite that, most authors design
and/or evaluate their ML detectors on the assumption that
future malware and goodware will remain similar to that
used at design time [15, 17].

5.2 Available Android Datasets and their Drawbacks

Android datasets that are used for building supervised
classiﬁers consist of collections of apps and their associ-
ated labels, which indicate whether an app is malware or
goodware. We have searched in the literature for datasets
of Android APKs designed for research on misuse-based
Android malware detectors, and have found ﬁve popular
ones. Their characteristics have been summarized in Table 2,
taking into account all the factors identiﬁed in the previous
section.

In two of the reported datasets (Drebin and CICAnd-
Mal2017), the labels of the samples were obtained by setting
some threshold over the VTD. For example, in Drebin, an app
is tagged as malware when its VTD≥2, for a subset of eight
antivirus engines from VirusTotal that are selected as reliable
by the authors. Only the Android Genome dataset was built
based on manual labeling. A combination of both labeling
approaches was used in the AMD collection: automatic
labeling was ﬁrst carried out using VirusTotal to ﬁlter and
cluster apps into malware families, and then a small subset
from each family was manually veriﬁed. Finally, note that
AndroZoo does not provide labels, supplying VTD values
instead –so it is up to the user how to use this information
for labeling.

To properly train detectors based on ML classiﬁers,
both malware and goodware samples are needed. Ideally,
greyware should also be included. Nonetheless, Drebin,
Android Genome and AMD comprise exclusively malware
samples and only AndroZoo allows samples to be labeled
as greyware. Another drawback of these datasets is related
to obfuscated malware. In this sense, the authors neither
identify, or explicitly include, obfuscated versions of mal-
ware, which makes it very difﬁcult to analyze the possible
effects of evasion attempts on the performance of detectors.

7

Something similar occurs for redundant samples, as none of
the papers describing the datasets provide information about
their presence.

Finally, as can be seen in Table 2, most of these datasets
were created more than ﬁve years ago and may contain
old-fashioned malware. The most recent dataset is CICAn-
dMal2017, which comprises a small set of apps released
between 2014 and 2017. This small number of instances may
not be enough for training and evaluating anti-malware
methods based on some ML algorithms [48]. In addition, in
most cases the samples included in these datasets cover only
a small period of time or their release date is not available.
These facts complicate the elaboration of proper analysis
about the evolution of the characteristics of apps.

The limitations of these datasets have led researchers
to build custom datasets, in a similar way as AndroZoo
was created, i.e., by combining APKs downloaded from
various sources or repositories. AndroZoo constitutes the
most important public source of Android apps for researchers
[47]. Sources of AndroZoo include app marketplaces (such as
Google Play), malware datasets, torrents and different mal-
ware repositories7. Since 2011, these sources are continuously
tracked to keep the collection up to date. Additionally to
the APKs, AndroZoo provides a ﬁle with information about
the apps contained in the dataset. The contents of this ﬁle,
which is updated daily, can be used to ﬁlter the samples to
be downloaded according to different criteria, such as the
market from which an APK was obtained, the SHA and the
date of the APK, and the VTD value.

From the above discussion, it is clear that the decisions
made by the authors during the construction of custom
datasets make them unique, and that particularities of these
datasets inﬂuence the performance of detectors. Moreover,
performance is not only dependent on the data used, but
also on the design of the experiments using these data. In
our opinion, and as mentioned in the previous section, the
use of different datasets and the consideration of different
guidelines in the design of the experiments, makes the
comparison of the metrics reported for these proposals
meaningless, entails an obstacle to the reproducibility and
seriously impairs progress on this relevant topic.

6 EXPERIMENTAL SETUP

In this section we present the experimental setup devised
for this work. We describe the datasets used, the process to
build and evaluate the Android malware detectors and the
evaluation metrics considered to measure their performance.

6.1 Master Dataset

Our starting point is a “master” dataset that allows us
to derive datasets useful to evaluate on the selected de-
tectors every factor presented in Section 5.1, i.e., REDUN-
DANCY, LABELING-GREYWARE, IMBALANCE, EVASION
and EVOLUTION.

To build our master dataset we selected apps from
AndroZoo, taking into account their origin, the reported

7. For example: ContagioDump (https://contagiodump.blogspot.
com/), Koodous (https://koodous.com), VirusShare (https://virusshare.
com/).

TABLE 2: Characteristics of popular Android Malware Datasets. The “?” symbol means that no information about this
characteristic is reported

8

Dataset

Time period

Android Genome [45]
Drebin [25]
AMD [37]

2010-2011
2010-2012
2010-2016

Labeling method
(Type of labels)
Manual (Binary)
VirusTotal (Binary)
Hybrid (Binary)

CICAndMal2017 [46]

2014-2017

VirusTotal (Binary)

AndroZoo [47]1

2011-

VirusTotal (VTD)

#samples

1 260 malware
5 560 malware
24 553 malware
426 malware
5 065 goodware
13 045 285 mixed

Obfuscated
samples
?
?
?

Redundant
samples
?
?
?

Timestamps
(cid:55)
(cid:55)
(cid:55)

?

?

?

?

(cid:55)

(cid:51)

1 This dataset is continuously growing

VTD and the date. We use apps from Google Play, AppChina
and VirusShare. The date of the apps was used to select
100 monthly samples of each class (malware, goodware and
greyware) during the period starting from January 2012 to
December 2019. As we mentioned earlier, there is a lack of
ground truth in the area, and automated labeling methods
based on VirusTotal have become commonplace. Given the
lack of standard criteria in the literature to interpret the
results obtained from VirusTotal [40], in this paper we resort
to a simpliﬁed but operational automated labeling method:
the use of VTD values and the application of a set of pre-
deﬁned thresholds. With this method we have a practical
deﬁnition of what is goodware, what is greyware and what
is malware that allows us to fairly compare detectors and
to show different sources of experimental bias. In particular,
we used the median of the thresholds used in two previous
works [15, 17] to label the malware, i.e., a VTD≥7. Goodware
samples were considered as those with VTD=0, whereas
samples with a 1≤VTD≤6 rating were labeled as greyware.
Moreover, we restricted our analysis to samples discovered
at most in 2019 to ensure that malware signatures for
samples in our dataset are well stabilized. Such criteria are
in concordance with what is recommended in [42] to obtain
reliable labels from VirusTotal.

In total, our master dataset consists of 28,800 samples.
The complete list of APKs and the instructions to download
this dataset, along with our code, are available in our
GitLab repository. In contrast to the datasets introduced
in Section 5.2, this dataset is more comprehensive as: (1)
it considers greyware; (2) it provides a sufﬁciently large
number of goodware, malware and greyware samples to
properly train and test ML detectors; (3) it considers a larger
time period and the organization of samples by months
allows us to split our dataset according to the age of the apps.
Additionally, compared to the datasets used for training the
detectors included in this analysis (see Table 1), the procedure
followed to build our master dataset and its derived datasets
is more systematic, rigorous and transparent.

6.2 Model Training and Assessment Process

For model construction and evaluation, the dataset is divided
into training and test partitions. In order to obtain unbiased
results, the test partition is always kept as a completely
separate set and is never used for training nor for feature
engineering processes (extraction, preprocessing or dimen-
sionality reduction). The model parameters are selected
using standard k-fold cross validation (with k = 5) within
the training set and following a grid search approach. In

TABLE 3: Evaluation metrics used in this paper

T P R = T P
P

F P R = F P
N

P recision = T P

T P +F P

F 1 = 2 ∗ T P R∗P recision
T P R+P recision

Amean = T P R+1−F P R

2

kappa = Po−Pc
1−Pc

Po = T P +T N
Pc = (cid:80)

P +N
k pk ∗ ˆpk

scenarios where EVOLUTION is considered because the
distributions of the data are assumed to change over time,
time-aware k-folds are used [49], i.e., all the apps of the
evaluation sets are more recent than those for training. In
all cases, folds are created maintaining the original ratio
between the classes in the given experimental scenario. This
is a common process in the ML literature for estimating the
generalization error of the ﬁnal model [36].

6.3 Evaluation Metrics

For this work, we considered a set of evaluation metrics
(see Table 3) which are common in the ML and computer
security literature [8]. Note that some of these metrics, such
as the TPR, the FPR or precision, offer complementary
information and should be used together to fully understand
the performance of a system. These metrics make use of: true
positives (TP), which indicate the number of correct positive
predictions; false positives (FP), that account for the number
of incorrectly predicted negative elements; and P and N,
which make reference to the number of positive and negative
elements in the data. Conversely, summary metrics such as
Amean, F1 score or kappa statistic, quantify the performance
of a system using one measure, but lack explainability and
do not provide insights regarding the causes of the good or
bad performance. F1 represents the harmonic mean between
precision and TPR, Amean is the arithmetic mean of the
accuracies for the positive and negative classes, and the
kappa statistic quantiﬁes the level of correlation between
the predictions made by a classiﬁer and the actual labels
in the data (Po being the accuracy of the detector, Pc being
the weighted sum of the predictions, pk being the actual
proportion of samples of the class k in the data and ˆpk being
the proportion of samples predicted as pertaining to class k).
Unlike the F1 score (as it does not consider True Negatives),
the kappa and Amean metrics are especially useful under
unbalanced problems.

7 COMPARATIVE ANALYSIS

In this section we run a complete set of experiments,
designing speciﬁc scenarios to analyze the effect of the factors
presented in Section 5.1. First, a basic scenario is used as the
departing point for subsequent experiments. In all scenarios,
the selected detectors are compared in equal conditions.

Departing from the master dataset, two different datasets
have been extracted, namely balanced and unbalanced (see
Table 4). For the balanced case, it is assumed that malware
and goodware are equally probable. Thus, from the period
2012-2015, ∼70% of all the goodware and ∼70% of all
the malware are uniformly sampled on a monthly basis
and used for training, keeping the remaining 30% of the
samples for testing purposes. On the contrary, under the
unbalanced conﬁguration, malware is assumed to be less
frequent than goodware and, consequently, the malware
is downsampled to become ∼10% of the goodware. This
process is performed by randomly sampling malware on
a monthly basis according to a Gaussian distribution with
parameters N (0.1, 0.02).

7.1 Baseline Scenario

Before analyzing the scenarios related to each factor, we test
detectors under the most basic and unrealistic assumptions,
i.e., discarding greyware, considering a balanced ratio be-
tween the classes, omitting evasion attempts and ignoring
the evolution of the apps. The aim of this scenario is to mimic
the (favorable) conditions that are commonly assumed in the
literature and try to reproduce the results obtained in the
original papers.

As shown in Table 5, half of the detectors showed
promising detection performances in this scenario, achieving
detection rates (TPR) over 0.8 and moderately low false
positives of about 0.1. The best detector under this scenario
in terms of summary metrics (0.91 for kappa), despite being
one of the ﬁrst works in the area, was Drebin, with TPR and
FPR values of 0.95 and 0.04, respectively. Another two well-
performing detectors were DroidDet and MaMaDroid, with
similar kappa ﬁgures of 0.84, despite using different features
and ML algorithms. The good results reported for some
methods contrast with those obtained for DroidDetector,
HMMDetector and AndroDialysis. Given the high number
of false positives of these detectors (above 20%), one could
conclude that these models are inappropriate even under
optimistic working conditions.

7.2 Redundancy Scenario

This scenario aims to study the impact of the presence of very
similar samples in Android datasets. To this end, we ﬁlter
out malware and goodware apps from our initial balanced
dataset. This is done by computing intra-class similarities
between samples and then applying the algorithm proposed
in [20]. The algorithm works by randomly selecting one
sample at a time and removing the samples lying in its
neighborhood, according to an (cid:15) radius threshold.

For our experiments, we use call frequencies of APIs
as the representation of apps and the Euclidean distance
to compute the degree of similarity between samples. The
intuition behind the consideration of this representation lies

9

Fig. 2: Number of malware groups per group size. Note the
logarithmic scale in the Y axis.

in that API call frequencies make reference to both the set of
API calls that describe the actions carried out by apps, and
the prevalence of these calls in the code. Thus, we assume
that when the Euclidean distance for two slightly different
apps is below or equal to (cid:15), both apps perform identical
actions, e.g., they are variations of the same app.

We perform the ﬁltering process using a redundancy
tolerance value (cid:15) equal to 0, which means that we consider
as duplicates two apps with exactly the same API call
frequencies. As a result of this process, a dataset without
exact duplicates is obtained. A summary of the characteristics
of this dataset are presented in Table 6. As can be seen, the
size of the malware subset is reduced substantially (almost
half of the malware was ﬁltered out), whereas for goodware
the number of removed duplicates is considerably smaller.
Details about the groups found for malware during the
deduplication process are shown in Figure 2. It depicts the
number of groups found int the malware dataset arranged
by group size. As can be seen, our ﬁltered dataset is
heterogeneous in terms of unique malware behaviors. More
speciﬁcally, 2285 samples only contain a duplicate, which
represent 88% of the ﬁltered malware. Groups with a small
number of duplicates represent the majority of the data, with
70% of the original malware containing less than 10 identical
samples. Large groups are also present. In this regard, about
2% of the resultant malware groups have more than 10
duplicates.

In order to conﬁrm the inﬂuence of the presence of very
similar apps in the generalization ability of ML models, the
results for detectors trained with ﬁltered data8 and evaluated
with the unﬁltered test set used for the baseline scenario
are shown in Table 7. As can be seen, almost all approaches
improve their performances, with similar TPR and lower
FPR values, compared to the results for detectors trained
with the unﬁltered training set (see Table 5). Among the most
beneﬁted methods in terms of performance are ICCDetector
and BasicBlocks, with improvements of 5% and 16% in
their kappa values, respectively. The results observed for

8. As in the baseline scenario, a balanced ratio between goodware
and malware is used for training. This is achieved by randomly
downsampling the majority class (goodware).

TABLE 4: Composition of Balanced and Unbalanced datasets derived from the 2012-2015 period of the Master Dataset

10

%malware
(#samples)
Balanced
70% (3360)
Unbalanced ∼7% (∼336)

Training

%goodware
(#samples)
70% (3360)
70% (3360)

ratio

1:1
∼1:10

%malware
(#samples)
30% (1440)
∼3% (∼144)

Testing
%goodware
(#samples)
30% (1440)
30% (1440)

ratio

1:1
∼1:10

TABLE 5: Performance of detectors for the baseline scenario

Method
AndroDialysis
BasicBlocks
Drebin
DroidDet
DroidDetector
HMMDetector
ICCDetector
MaMaDroid

TPR
0.848
0.823
0.953
0.936
0.472
0.824
0.800
0.930

FPR
0.291
0.097
0.043
0.088
0.335
0.560
0.093
0.085

Precision
0.744
0.894
0.956
0.913
0.585
0.595
0.895
0.915

F1
0.792
0.857
0.955
0.925
0.523
0.691
0.845
0.923

Amean
0.778
0.863
0.955
0.924
0.568
0.631
0.853
0.922

Kappa
0.556
0.726
0.910
0.848
0.137
0.263
0.706
0.845

TABLE 6: Composition of the full dataset used for the ﬁltered
scenario (data from 2012 to 2015)

Dataset
Original
Filtered

#malware
4800
2588

#goodware
4800
4291

this scenario support the hypothesis that duplicate samples
in the training set result in biased models, since accurate
predictions for these large groups at training time result
in higher accuracy, and the models are less prone to pay
attention to minority groups. Thus, removing duplicates from
training is translated into models that are able to generalize
better to different types of instances.

The previous experiment was carried out with an
unﬁltered test set. However, to evidence the impact that
the presence of duplicates in the testing set has on the
reported performances, we perform evaluations using
a ﬁltered test set. Table 8, depicts the performance for
detectors both trained and evaluated using data without
duplicates. Among all the methods, the best scoring are
Drebin and DroidDet with kappa values of 0.86 and 0.85,
respectively. However, as expected, most detectors report
lower detection performances in this scenario than in
the previous experiments (see Table 7 and Table 5). With
duplicates in the test set, correctly identifying bigger groups
has a signiﬁcant positive effect on the measured performance,
whereas erroneously classifying these groups penalizes it.
This effect disappears when duplicates are removed and
only one representative per group is left. Therefore, the
measured performance is a better indicator of generalization
ability of detectors.

7.3 Labeling-Greyware Scenarios

As previously mentioned, manual, exhaustive and error-free
labeling is not affordable. For this reason, the community
has agreed to use tools such as VirusTotal to automatically
label apps. In this sense, using thresholds on the VTD value
is the most common class separation criteria. However,
different interpretations and thresholds have been used,
which impacts on the obtained datasets. Two interesting
analyses arise here: (1) how the selection of the threshold

TABLE 7: Performance of detectors trained using the ﬁltered
dataset and evaluated with the baseline test set

Method
AndroDialysis
BasicBlocks
Drebin
DroidDet
DroidDetector
HMMDetector
ICCDetector
MaMaDroid

TPR
0.904
0.893
0.933
0.933
0.654
0.796
0.800
0.914

FPR
0.365
0.044
0.031
0.056
0.403
0.488
0.053
0.046

Precision
0.712
0.952
0.966
0.942
0.618
0.619
0.937
0.951

F1
0.796
0.922
0.949
0.937
0.635
0.697
0.863
0.932

Amean
0.769
0.924
0.950
0.938
0.625
0.653
0.873
0.934

Kappa
0.538
0.849
0.901
0.876
0.250
0.307
0.747
0.868

TABLE 8: Performance of detectors trained and evaluated
with ﬁltered sets

Method
AndroDialysis
BasicBlocks
Drebin
DroidDet
DroidDetector
HMMDetector
ICCDetector
MaMaDroid

TPR
0.913
0.880
0.925
0.926
0.728
0.774
0.797
0.900

FPR
0.362
0.065
0.056
0.073
0.395
0.545
0.070
0.072

Precision
0.716
0.930
0.942
0.926
0.647
0.586
0.918
0.925

F1
0.802
0.904
0.933
0.926
0.685
0.667
0.853
0.913

Amean
0.775
0.907
0.934
0.926
0.666
0.614
0.863
0.914

Kappa
0.551
0.814
0.868
0.853
0.332
0.229
0.726
0.828

affects performance and, (2) what is the behavior of detectors
when they examine apps that were discarded during labeling.
First, we evaluate how the selection of different criteria
for labeling datasets affects detectors and thus, the reported
performance. Speciﬁcally, we want to analyze whether the
uncertainty and difﬁculty of the problem is greatly reduced
when higher VTD values are used for labeling malware. To
do so, models are trained similarly to the baseline scenario,
i.e., using a balanced ratio between both classes with samples
selected from 2012 to 2015, but varying the threshold used
for labeling.

Results for this experiment are shown in Table 9. As can
be seen, the TPR of models is directly correlated with the
VTD used for labeling the dataset i.e., the higher the VTD,
the higher the TPR of the detector. The contrary is true for the
FPR, since it follows a reverse trend with respect to the VTD.
This conﬁrms that the malware detection problem becomes
easier whenever the separation between both classes, in
terms of the VTD, is enlarged. Figure 3 exhibits this effect for
the values of Amean obtained for the different detectors.

The second scenario is devoted to show how detectors
behave when facing greyware. Speciﬁcally, we want to
analyze whether the uncertainty and difﬁculty of the problem
increases when adding greyware and, thus, the omission of
these apps results on the oversimpliﬁcation of the problem
and leads to unrealistic and unfair evaluations. To do so,
models are trained using the balanced dataset and tested
exclusively on greyware samples from the period between
2012 and 2015.

We have compiled the responses of the detectors when
classifying greyware samples in Table 10. G indicates how
many input samples are identiﬁed as goodware, whereas

M refers to how many samples are classiﬁed as malware.
The total results are in the right column of the table. The
other columns show the partial results for the input samples
grouped by their VTD scores. As can be seen, on average 35%
of the total samples are considered goodware by detectors.
As expected, the results show certain correlation between the
VTD and the decisions made by the detectors: given an app,
the higher its VTD is, the more likely the detectors are to
classify it as malware. We can observe a high uncertainty for
samples with lower VTD values, explaining why authors opt
to discard greyware from their experiments. As a result, the
problem solved by detectors is simpliﬁed. Thus, providing
artiﬁcially boosted performance results and hiding an effect
that will appear in real-working conditions.

In addition, we also analyze how the VTD value and
thus, the labels, change over time. For the labeling of our
dataset we used the VTD scores provided by AndroZoo.
We completely reanalyzed the apps in our dataset with
VirusTotal and computed the confusion matrix to represent
the label swaps between the two analyses. As can be seen
in Table 11, most swaps occur for the greyware class. In this
sense, more than half of the apps change their label from
greyware to goodware (27.2%) or malware (28.1%). Also,
9.9% of the goodware and 6.9% of the malware apps fall
into the category of greyware after being reanalyzed. These
changes illustrate the downsides and limitations of using
the VTD score for labeling. On the one hand, the need of
providing the labels of samples when releasing datasets to
avoid class swaps caused by the reanalysis of apps and to
guarantee reproducibility. On the other hand, the need to
reconsider greyware as part of the datasets, as 44% of the
apps remain in this category even after being reanalyzed.

Fig. 3: Amean values for different VTD labeling thresholds

7.4 Imbalance Scenario

According to previous literature, around 10% of apps are
actually malware [15]. Similarly, in this scenario we assume
that there will be a strong imbalance between malware and
goodware. In order to analyze the behavior of the different
detectors in this context, the unbalanced dataset is used.
It is worth noting that making the test set balanced (or
unbalanced) does not harm the capabilities of the model, but
only has an effect in some performance metrics. In this sense,

11

the problem of imbalance in the test set is easily solved by
using suitable metrics, such as the Kappa or Amean, that do
not conceal the errors for the minority class [8].

Table 12 shows the results obtained for detectors under
this scenario. The use of unbalanced data for training results
in a reduction in the proportion of correctly classiﬁed
malware (TPR), with respect to the results obtained using
the baseline conﬁguration (see Table 5). Imbalance in the
training data is translated into less malware information
provided to algorithms, making it difﬁcult for detectors to
learn the characteristics of this class of samples. On average
the performance of most methods decreased about 12% as
measured by their Amean values. Among the best scoring
methods in the baseline scenario, MaMaDroid is one of the
methods that suffered a more signiﬁcant decrease in this
scenario, with a reduction of 18% in its Amean value.

7.5 Evasion Scenario

This scenario is devoted to test the robustness of ML-based
malware detectors under conditions where attackers attempt
to bypass detection using obfuscation. Our aim with this
section is to illustrate the need of carrying out the security
analysis of detectors. There are many evasion techniques
targetting the different phases of the malware detection
process that have not been included in this study to keep
the size and scope manageable, but deserve further analysis,
e.g., sandbox detection [50] or adversarial attacks against
ML models [51]. We focus our evaluation in obfuscation9
because it is a classical evasion technique that is commonly
overlooked by the authors of proposals [53]. We selected
a set of obfuscation strategies that are commonly used
in the wild to hide Android malware behaviors [54]. For
each obfuscation strategy considered, as well as for the
combination of all of them, a new obfuscated dataset is
obtained by: (1) randomly sampling 10% of the apps from
2012 to 2015 in our master dataset and, (2) applying the
required transformations to such apps using the ObfusAPK
[55] and the AAMO [56] tools. These transformations are:

• Renaming. The original name and identiﬁers of user-
deﬁned classes, ﬁelds and methods are changed by
meaningless strings.

• Changes in the structure of the code. This form of
obfuscation includes: (1) call indirections to add an
intermediate function that calls the function originally
present in the code, (2) insertion of goto instructions,
(3) inversion of conditionals to modify the execution
ﬂow of the app, (4) insertion of junk code, and (5)
reﬂection10 to hide function calls to internal code and
to the Android framework (APIs).

• Encryption. This technique involves: (1) the genera-
tion of a encryption/decryption random key, (2) the
encryption of native libraries, strings and assets, and

9. Obfuscation involves code modiﬁcations to hinder the static analysis
of apps without affecting their functionality. From the point of view of
malware creators, obfuscation hampers the extraction of the features
that may be indicative of malicious behaviors in order to fool detectors
[52].

10. Reﬂection is a feature of some programming languages that allows
an executing program to examine or “introspect” upon itself, and
manipulate internal properties of the program.

TABLE 9: Performance of detectors trained with data labeled using different thresholds over the VTD for the malware. TPR
stands for the True Positive Ratio and FPR is the False Positive Ratio

12

VTD>=1

VTD>=2

VTD>=3

VTD>=4

VTD>=5

VTD>=6

Method
AndroDyalisis
BasicBlocks
Drebin
DroidDet
DroidDetector
HMMDetector
ICCDetector
MaMaDroid

TPR
0.547
0.731
0.731
0.752
0.526
0.759
0.612
0.756

FPR
0.258
0.184
0.227
0.236
0.384
0.600
0.180
0.199

TPR
0.622
0.869
0.860
0.873
0.826
0.775
0.732
0.879

FPR
0.275
0.112
0.113
0.153
0.625
0.588
0.120
0.136

TPR
0.549
0.885
0.901
0.901
0.937
0.804
0.780
0.923

FPR
0.183
0.107
0.107
0.152
0.813
0.589
0.096
0.159

TPR
0.692
0.904
0.914
0.904
0.865
0.794
0.797
0.923

FPR
0.305
0.134
0.131
0.174
0.757
0.536
0.156
0.187

TPR
0.798
0.909
0.935
0.935
0.751
0.733
0.832
0.901

FPR
0.369
0.090
0.081
0.154
0.467
0.506
0.141
0.145

TPR
0.861
0.916
0.923
0.930
0.763
0.784
0.833
0.895

FPR
0.298
0.083
0.062
0.083
0.506
0.562
0.055
0.076

TABLE 10: Decisions made by baseline detectors when dealing with greyware samples from the period 2012-2015, for
different VTD values. G stands for the ratio of identiﬁcations as goodware, while M is the ratio of identiﬁcations as malware

VTD=1

VTD=2

VTD=3

VTD=4

VTD=5

VTD=6

Method
AndroDyalisis
BasicBlocks
Drebin
DroidDet
DroidDetector
HMMDetector
ICCDetector
MaMaDroid

G
0.504
0.675
0.696
0.700
0.629
0.415
0.683
0.677

M
0.496
0.325
0.304
0.300
0.371
0.585
0.317
0.323

G
0.356
0.452
0.460
0.437
0.691
0.296
0.460
0.423

M
0.644
0.548
0.540
0.563
0.309
0.704
0.540
0.577

G
0.272
0.394
0.379
0.336
0.607
0.311
0.386
0.366

M
0.728
0.606
0.621
0.664
0.393
0.689
0.614
0.634

G
0.307
0.333
0.323
0.297
0.659
0.259
0.381
0.313

M
0.693
0.667
0.677
0.703
0.341
0.741
0.619
0.687

G
0.275
0.248
0.251
0.221
0.607
0.255
0.295
0.217

M
0.725
0.752
0.749
0.779
0.393
0.745
0.705
0.783

G
0.200
0.248
0.148
0.159
0.536
0.163
0.240
0.137

M
0.800
0.752
0.852
0.841
0.464
0.837
0.760
0.863

1<=VTD<=6
M
0.681
0.608
0.624
0.642
0.379
0.717
0.593
0.645

G
0.319
0.392
0.376
0.358
0.622
0.283
0.408
0.356

TABLE 11: Label swaps for goodware (G), greyware (X) and
malware (M); between annotations of samples in the master
dataset using information contained in AndroZoo (rows) and
VirusTotal reanalysis reports (columns)

obfuscated samples according to their Amean values when
using the three obfuscation techniques in combination.

VirusTotal
reanalysis

AndroZoo
analysis

G
G 8 563
2 613
X
17
M

X
954
4 284
663

M
83
2 703
8 920

TABLE 12: Performance of detectors trained with 1:10 mal-
ware/goodware ratio

Method
AndroDialysis
BasicBlocks
Drebin
DroidDet
DroidDetector
HMMDetector
ICCDetector
MaMaDroid

TPR
0.373
0.460
0.740
0.686
0.266
0.106
0.520
0.513

FPR
0.032
0.017
0.017
0.013
0.162
0.010
0.013
0.008

Precision
0.543
0.734
0.816
0.837
0.145
0.516
0.795
0.865

F1
0.442
0.565
0.776
0.754
0.188
0.176
0.629
0.644

Amean
0.670
0.721
0.861
0.836
0.552
0.548
0.753
0.752

Kappa
0.396
0.531
0.754
0.731
0.076
0.149
0.599
0.617

(3) the insertion of a decryption code which is called
from every part of the app where these resources are
requested.

The results are summarized in Table 13. On average,
when considered separately, the most successful strategies
for evading detection are changes in code structure and
encryption. However, generally speaking, individual obfus-
cation strategies are not as effective as a combination of all
of them. Most detectors are prone to misclassify obfuscated
malware as goodware, as shown by the decrease in their
TPR values for the combined scenario. In this regard, the
least affected model is Drebin, being able to identify 79% of
the obfuscated malware, with 13% false positives. Others,
such as DroidDet and MaMaDroid obtained more moderate
performance ﬁgures, both identifying around 70% of the

7.6 Evolution Scenario

Both malware and goodware evolve over time, i.e., do not
follow a stationary distribution. Therefore, it is logical to
think that static detectors trained with apps for a certain
period of time will not necessarily work well with more
recent apps. To prove this assumption, the models trained
using the balanced conﬁguration (with data from the period
2012-2015) are tested with goodware and malware obtained
between 2016 and 2019.

The overall performance of most of the methods under
this scenario is characterized by an increment of both
false positives and false negatives (see Table 14). Figure 4a
provides a more detailed view of the evolution of the TPR,
FPR and Amean values throughout the period between 2016
and 2019. As can be seen, the performance of detectors
changes in a notable manner from one period to another.
Our hypothesis is that the popularity of some malware
families decreases with time, and, at some point, they are
replaced with newer families for which a detector may not
have been trained. Also, at some point, an old behavior may
become popular again, resulting in a sudden increase in
the performance of that detector. This is why the lines in
Figure 4c do not follow clear, decreasing trends. In addition,
contrary to what is stated in [15], the incremental trend
in the number of false alarms of detectors (see Figure 4b)
indicates that goodware also changes its behavior over time.
At any rate, our observations demonstrate the non-stationary
nature of malware and goodware, and show how classic,
batch-trained ML algorithms are not an appropriate solution
for Android malware detection given the instability of their
decisions.

TABLE 13: Performance of detectors with obfuscated apps for the evasion scenario

13

Renaming (Rn)

Method
AndroDialysis
BasicBlocks
Drebin
DroidDet
DroidDetector
HMMDetector
ICCDetector
MaMaDroid

TPR
0.793
0.624
0.95
0.931
0.154
0.895
0.246
0.733

FPR
0.288
0.163
0.012
0.028
0.216
0.489
0.033
0.034

Amean
0.752
0.73
0.968
0.951
0.469
0.702
0.606
0.848

Changes in code
structure (Co)
FPR
0.072
0.631
0.025
0.024
0.102
0.786
0.023
0.042

Amean
0.652
0.29
0.971
0.767
0.473
0.553
0.822
0.845

TPR
0.376
0.211
0.968
0.560
0.049
0.892
0.667
0.737

Encryption
(Enc)
FPR
0.074
0.137
0.138
0.021
0.126
0.684
0.023
0.037

Amean
0.678
0.64
0.799
0.956
0.503
0.558
0.822
0.941

TPR
0.431
0.418
0.738
0.934
0.133
0.802
0.668
0.92

Rn+Co+Enc

TPR
0.459
0.172
0.796
0.586
0.157
0.911
0.279
0.533

FPR
0.086
0.095
0.136
0.029
0.211
0.779
0.024
0.048

Amean
0.686
0.538
0.829
0.778
0.473
0.565
0.627
0.743

TABLE 14: Performance of baseline detectors using evalua-
tion data between 2016 and 2019

Method
AndroDialysis
BasicBlocks
Drebin
DroidDet
DroidDetector
HMMDetector
ICCDetector
MaMaDroid

TPR
0.656
0.390
0.787
0.752
0.685
0.795
0.440
0.721

FPR
0.268
0.771
0.171
0.108
0.175
0.488
0.153
0.086

Precision
0.709
0.336
0.820
0.873
0.796
0.619
0.741
0.893

F1
0.682
0.361
0.804
0.808
0.736
0.696
0.552
0.798

Amean
0.694
0.309
0.808
0.821
0.755
0.653
0.643
0.817

Kappa
0.388
-0.381
0.616
0.643
0.510
0.306
0.286
0.635

8 DISCUSSION: TOWARDS A REALISTIC FRAME-
WORK FOR MALWARE DETECTION
Based on the factors analyzed in the previous sections, this
section presents a collection of ideas or recommendations to
consider when designing and evaluating a realistic proposal
for Android malware detection.

Undoubtedly, one of the main problems encountered
when researching malware detectors for Android is the lack
of reproducibility of many proposals in the literature. The
datasets and code used in the experimental processes are
rarely public, and the details provided in the papers are
often not enough for a correct re-implementation of the
methods. Therefore, the most important recommendation
is that authors of future works ensure that their work and
experimental processes are fully reproducible. In addition
to this general aspect, the experiments conducted in the
previous section have highlighted the importance of using
adequate datasets when training models and assessing
their performance. Indeed, as we have seen, depending on
the datasets used, the same model can show near-perfect
performances or be almost irrelevant. Thus, we can conclude
that the datasets and experimental scenarios considered in
the literature are unrealistic and should be revised.

Related to the previous statements, our analysis has
shown that the presence of highly similar apps in the datasets
inﬂuences the performance of models. This also opens the
possibility to perform evasion attacks against classiﬁers, for
example, by developing malware apps with snippets of code
from minor malware variants. During training, models tend
to focus on large groups of duplicates, instead of trying to
generalize the variety of apps in the data. During model as-
sessment, duplicates are the cause of misleading performance
indicators that tend to overestimate the detection ability
of models. We can conclude that training and evaluation
efforts should leverage on datasets without duplicates to
improve and demonstrate the generalization capacity of
models [36]. Training should be carried out after balancing

the representatives within each of the classes, i.e., between
groups of similar samples; to avoid biases and exploit the
generalization capacity of ML models. Also, contextual
evaluations that take into account the prevalence of different
malware families or groups are desirable to describe the
actual reasons behind the performance of detectors. The
removal of duplicates has an additional advantage, as the
implicit reduction in the dimensionality of the data is useful
to speed up the labeling, training and evaluation processes.
In relation to the labeling of the apps, we have seen
that VirusTotal is widely used, but relying only on the
VTD reported by this tool entails some risks [42]. To begin
with, there is a lack of consensus on the labeling of the
apps and on the inclusion of greyware in the training and
evaluation of models. We should bear in mind that the line
between malware and goodware is not clearly deﬁned. As
we demonstrated, the choice of the labeling criteria highly in-
ﬂuences the generated model and the detection performance,
simplifying the problem as the VTD is increased. This lack
of consensus also results in the omission of a large amount
of greyware which lies in between. However, such apps are
present in the Android app ecosystem [41, 57], so, realistic
proposals should not ignore these type of apps but include
them in their training and evaluation processes. The nature
of such apps may be uncertain. In this regard, additional
information such as the details included in the VirusTotal
reports can also be used to support more sophisticated
labeling techniques, identifying particular functionalities like
advertising, bundling, etc. which could provide valuable
information to the user about a given (greyware) app. This
would help to create a taxonomy of the type of apps present
in the Android ecosystem, and to reach consensus on what
is goodware or malware. It would also help to determine
the usefulness of a third category of “potentially unwanted
apps” [58] and its consideration as a third class in the data. In
addition, to avoid the use of ﬁxed thresholds over the VTD
for labeling, and as an alternative to binary or three-class
classiﬁers, new detection proposals could explore regression
models that provide a risk/maliciousness metric, multi-step
learning approaches [59], unsupervised detectors that return
a degree of dissimilarity with respect to the benign class [60],
or semi-supervised methods that do not require fully labeled
datasets [61].

Also regarding labeling, a more general problem is that
not all antivirus engines in VirusTotal are equally reliable,
with some of them being correlated or specialized in speciﬁc
types of malware [62]. In addition, two engines from the
same vendor but speciﬁc for different platforms may differ

14

labels from malware, since anti-virus ﬁrms do not share a
common nomenclature. In this regard, despite that some
useful tools are available for agreeing on family labels from
VirusTotal reports [65], we believe that there is still a lot of
work to be done. For example, proposing a standard tax-
onomy of malicious behaviors that would allow a common
speciﬁcation of malware.

As a last problem with labeling, we have seen in the exper-
imentation that the VTD, and thus, the labeling, changes over
time [42]. Weakly labeled data and changes in the labeling
can hinder the generation of robust classiﬁers, leading to
detection errors. As a solution, the use of the VTD for
labeling, on its own, should be abandoned and replaced by
more sophisticated methods, for example, that incorporate
informative features provided in the analysis reports of
VirusTotal [63]. Active learning may also be a useful tool
in this sense. This technique allows the best representatives
of each class to be selected, reducing the effort of manual
labeling [66]. Nonetheless, using sophisticated methods for
labeling might become cumbersome and costly, and of course,
their reliability should be validated.

Another important aspect is that malware and goodware
apps are found in different proportions in the wild, depend-
ing on the source of the apps, but malware being the minority
class [15]. Ideally, a detector should perform well when
analyzing apps regardless of the source of the samples. Thus,
proposals should be trained and tested assuming unbalanced
datasets. Additionally, the adoption of suitable performance
metrics for unbalanced scenarios, such as the kappa and A-
mean metrics, should be contemplated as opposed to using
others, such as the accuracy, that do not reﬂect the real
performance of detectors in these contexts. We have seen
that classical ML supervised classiﬁcation algorithms used
for malware detection do not properly manage imbalance,
as they expect data to be equally balanced in order to build
a robust model [67]. As such, the use and design of speciﬁc
ML classiﬁers for unbalanced scenarios is an open research
line, which could be promising in this area of study. Some
examples include cost-based classiﬁcation methods, and
subsampling and oversampling methods [68]. In addition,
depending on the particular scenario in which the model
will be applied, and on the interest of the practitioner
(for example, reducing the number of FPs), different loss
functions could be used in the training phase. As alternative
approaches, anomaly detection algorithms may also be
considered [8].

Next, detectors have to work in a hostile scenario where
attackers will try to trick them to infect a system. These are
one of the ﬁrst lines of defence, so security of malware
detectors is important. Despite that our evaluation was
limited to simple evasion attacks based on the use of obfus-
cation techniques, we evidenced that most detectors are not
designed with security in mind and are, in fact, vulnerable
to attacks. Appropriate assessments of the robustness of
detectors should consider all the steps involved in the
malware detection process. Novel evaluation methodologies
that take into account the latest technology and attacks are
needed and represent a promising future line of research.
These should consider the evaluation of approaches to
attacks targeting the analysis phase of apps, such as sandbox
detection [69]; or, to attacks that target ML algorithms, such

(a) Amean values

(b) FPR values

(c) TPR values

Fig. 4: Evolution of the performance of baseline detectors for
the period 2016-2019

[63]. Although reliable labels can be derived from the VTD
[42], setting simple thresholds to the VTD assumes that all
antivirus engines are equally reliable in all situations and
makes no distinctions between them. In order to overcome
these aspects, techniques such as crowd learning [64], which
measure the relevance of the different antivirus engines
present in VirusTotal, could be used. Differences among
vendors are even more noticeable when obtaining family

as adversarial learning [70]. In this regard, the proposal
of robust ML algorithms against novel attacks is a very
interesting research line [71]. However, we should not forget
that cutting-edge detectors will also need to counteract
classical evasion attacks. Thus, the selection and design of
feature sets that are most useful to detect obfuscated malware
behaviors [72], as well as, the improvement of app analysis
tools, are still promising research areas. Therefore, obfuscated
samples are also essential in testing datasets. As for evasion
techniques and tools, in this paper we have considered only
a few; however, a broader set of techniques and tools that
implement them need to be thoroughly analyzed in future
work.

Finally, the malware detection problem is non-stationary,
i.e., classes evolve over time and rarely show constant charac-
teristics. A realistic detector should be able to cope with the
non-stationary nature of Android apps. We demonstrated
that the analyzed detectors, which are based on classical
ML classiﬁers, are not able to manage such changes. A
simple solution to this limitation could be to apply periodic
retraining procedures to obtain up-to-date detectors [8].
However, depending on the complexity of the algorithms,
this process could increment the cost of building and
maintaining detectors. Also, the cost of obtaining a sufﬁcient
number of labeled samples at a time should not be obviated.
Some authors have focused on slowing-down the aging of
models and reduce update efforts by ﬁnding similar API
functions [73]. As a more promising ML alternative for
dynamic scenarios, online or stream learning algorithms
have been proposed [74], some of which weight the instances
based on their prevalence or age. These algorithms can
automatically manage the changes in the distribution of
the data so that they are able to adapt the models without
intervention whenever these changes occur [75]. Accordingly,
the suitability of these methods for malware detection needs
to be explored further. Another option would be to consider
anomaly detectors or hybrid misuse-anomaly detectors [8]
as they may be more capable of dealing with the evolution
in the data and zero-day attacks.

9 CONCLUSIONS

One of the main conclusions of this work is that authors
of ML-based Android malware detectors tend to be very
optimistic when designing and evaluating their systems,
ignoring factors such as the presence of duplicates in the
datasets, the lack of robust labeling methods, the presence of
greyware, the imbalance between malware and goodware,
the existence of apps trying to evade detection and the
evolution of apps. Our evaluation work has shown how
these factors substantially affect the performance that can be
achieved with the tested detectors. We have seen, therefore,
that malware detectors are not ready for deployment in real
environments. Another important problem we have studied
is the lack of a common design and evaluation framework,
stemming, among other things, from the unavailability of
codes and standardized and appropriate datasets. This fact
greatly complicates the reproducibility of experiments. Our
contribution in this regard has been to release our data and
codes so that they can be used by other researchers. Finally,
we have included in this paper a number of ideas that can

contribute to the design of malware detectors which are able
to operate in realistic deployment scenarios.

15

CREDIT AUTHORSHIP CONTRIBUTION STATEMENT

Borja Molina-Coronado: Conceptualization, Methodology,
Software, Formal analysis, Investigation, Writing - Original
Draft, Writing - Review & Editing Usue Mori: Conceptual-
ization, Methodology, Writing - Review & Editing Alexan-
der Mendiburu: Conceptualization, Methodology, Writing -
Review & Editing Jose Miguel-Alonso: Conceptualization,
Methodology, Writing - Review & Editing

ACKNOWLEDGMENTS

This work has received support from the following pro-
grams: PID2019-104966GB-I00AEI (Spanish Ministry of Sci-
ence and Innovation), IT-1504-22 (Basque Government),
KK-2021/00095 and KK-2021/00065 (Elkartek projects
SIGZE and ALUSMART supported by the Basque Govern-
ment). Borja Molina-Coronado holds a predoctoral grant
(ref. PRE_2020_2_0167) by the Basque Government.

REFERENCES
[1] Statista. Mobile operating systems’ market share world-
wide from january 2012 to january 2022, 2022. [Online]
Available: https://www.statista.com/statistics/272698/
global-market-share-held-by-mobile-operating-systems-since-2009/.

[2] Mussab Alaa, Aws Alaa Zaidan, Bilal Bahaa Zaidan, Mo-
hammed Talal, and Miss Laiha Mat Kiah. A review of smart
home applications based on internet of things. Journal of
Network and Computer Applications, 97:48–65, 2017.

[3] Gianpaolo Macario, Marco Torchiano, and Massimo Vi-
olante. An in-vehicle infotainment software architecture
In 2009 IEEE International
based on google android.
Symposium on Industrial Embedded Systems, pages 257–260.
IEEE, 2009.

[4] Quang Do, Ben Martini, and Kim-Kwang Raymond Choo.
Is the data on your wearable device secure? an android
wear smartwatch case study. Software: Practice and Experi-
ence, 47(3):391–403, 2017.

[5] Meng Xu, Chengyu Song, Yang Ji, Ming-Wei Shih, Kangjie
Lu, Cong Zheng, Ruian Duan, Yeongjin Jang, Byoungyoung
Lee, Chenxiong Qian, et al. Toward engineering a secure
android ecosystem: A survey of existing techniques. ACM
Computing Surveys (CSUR), 49(2):1–47, 2016.

[6] Kaspersky Labs. Mobile malware evolution 2020,
[Online] Available: https://securelist.com/

Mar 2021.
mobile-malware-evolution-2020/101029/.

[7] Darell JJ Tan, Tong-Wei Chua, and Vrizlynn LL Thing.
Securing android: a survey, taxonomy, and challenges.
ACM Computing Surveys (CSUR), 47(4):1–45, 2015.

[8] Borja Molina-Coronado, Usue Mori, Alexander Mendiburu,
and Jose Miguel-Alonso. Survey of network intrusion
detection methods from the perspective of the knowledge
IEEE Transactions on
discovery in databases process.
Network and Service Management, 17(4):2451–2479, 2020.
[9] Parvez Faruki, Ammar Bharmal, Vijay Laxmi, Vijay Gan-
moor, Manoj Singh Gaur, Mauro Conti, and Muttukrishnan
Rajarajan. Android security: a survey of issues, malware
penetration, and defenses. IEEE communications surveys &
tutorials, 17(2):998–1022, 2014.

[10] Kimberly Tam, Ali Feizollah, Nor Badrul Anuar, Rosli
Salleh, and Lorenzo Cavallaro. The evolution of android
malware and android analysis techniques. ACM Computing
Surveys (CSUR), 49(4):1–41, 2017.

[11] Amir Aﬁanian, Salman Niksefat, Babak Sadeghiyan, and
David Baptiste. Malware dynamic analysis evasion tech-
niques: A survey. ACM Computing Surveys (CSUR), 52(6):
1–28, 2019.

[12] Andreas Moser, Christopher Kruegel, and Engin Kirda.
Limits of static analysis for malware detection. In Twenty-
Third Annual Computer Security Applications Conference
(ACSAC 2007), pages 421–430. IEEE, 2007.

[13] Michael Spreitzenbarth, Thomas Schreck, Florian Echtler,
Daniel Arp, and Johannes Hoffmann. Mobile-sandbox:
combining static and dynamic analysis with machine-
International Journal of Information
learning techniques.
Security, 14(2):141–153, 2015.

[14] Saba Arshad, Munam A Shah, Abdul Wahid, Amjad
Mehmood, Houbing Song, and Hongnian Yu. Samadroid: a
novel 3-level hybrid malware detection model for android
operating system. IEEE Access, 6:4321–4339, 2018.

[15] Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney,
Johannes Kinder, and Lorenzo Cavallaro. {TESSERACT}:
Eliminating experimental bias in malware classiﬁcation
across space and time. In 28th {USENIX} Security Sympo-
sium ({USENIX} Security 19), pages 729–746, 2019.

[16] Kevin Allix, Tegawendé F Bissyandé, Quentin Jérome,
Jacques Klein, Yves Le Traon, et al. Empirical assessment
of machine learning-based malware detectors for android.
Empirical Software Engineering, 21(1):183–211, 2016.

[17] Sankardas Roy, Jordan DeLoach, Yuping Li, Nic Herndon,
Doina Caragea, Xinming Ou, Venkatesh Prasad Ranganath,
Hongmin Li, and Nicolais Guevara. Experimental study
with real-world data for android app security analysis
using machine learning. In Proceedings of the 31st Annual
Computer Security Applications Conference, pages 81–90, 2015.
[18] Vaibhav Rastogi, Yan Chen, and Xuxian Jiang. Catch
me if you can: Evaluating android anti-malware against
IEEE Transactions on Information
transformation attacks.
Forensics and Security, 9(1):99–108, Jan 2014.

[19] Hojjat Aghakhani, Fabio Gritti, Francesco Mecca, Martina
Lindorfer, Stefano Ortolani, Davide Balzarotti, Giovanni
Vigna, and Christopher Kruegel. When malware is
packin’heat; limits of machine learning classiﬁers based on
static analysis features. In Network and Distributed Systems
Security (NDSS) Symposium 2020, 2020.

[20] Roopak Surendran. On impact of semantically similar apps
in android malware datasets. ArXiv, abs/2112.02606, 2021.
[21] Kevin Allix, Tegawendé F Bissyandé, Jacques Klein, and
Yves Le Traon. Are your training datasets yet relevant? In
International Symposium on Engineering Secure Software and
Systems, pages 51–67. Springer, 2015.

[22] Davide Maiorca, Davide Ariu, Igino Corona, Marco Aresu,
and Giorgio Giacinto. Stealth attacks: An extended insight
into the obfuscation effects on android malware. Computers
& Security, 51:16–31, 2015.

[23] Li Li, Tegawendé F Bissyandé, Mike Papadakis, Siegfried
Rasthofer, Alexandre Bartel, Damien Octeau, Jacques Klein,
and Le Traon. Static analysis of android apps: A systematic
literature review. Information and Software Technology, 88:
67–95, 2017.

[24] Ali Feizollah, Nor Badrul Anuar, Rosli Salleh, Guillermo
Suarez-Tangil, and Steven Furnell. Androdialysis: Anal-
ysis of android intent effectiveness in malware detection.
computers & security, 65:121–134, 2017.

[25] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo
Gascon, Konrad Rieck, and CERT Siemens. Drebin: Effec-
tive and explainable detection of android malware in your
pocket. In Ndss, volume 14, pages 23–26, 2014.

[26] Hui-Juan Zhu, Zhu-Hong You, Ze-Xuan Zhu, Wei-Lei Shi,
Xing Chen, and Li Cheng. Droiddet: effective and robust
detection of android malware using static analysis along
with rotation forest model. Neurocomputing, 272:638–646,
2018.

16

[27] Zhenlong Yuan, Yongqiang Lu, and Yibo Xue. Droiddetec-
tor: android malware characterization and detection using
deep learning. Tsinghua Science and Technology, 21(1):114–
123, 2016.

[28] Gerardo Canfora, Francesco Mercaldo, and Corrado Aaron
Visaggio. An hmm and structural entropy based detector
for android malware: An empirical study. Computers &
Security, 61:1–18, 2016.

[29] Ke Xu, Yingjiu Li, and Robert H Deng. Iccdetector: Icc-
based malware detection on android. IEEE Transactions on
Information Forensics and Security, 11(6):1252–1264, 2016.
[30] Lucky Onwuzurike, Enrico Mariconti, Panagiotis Andri-
otis, Emiliano De Cristofaro, Gordon Ross, and Gianluca
Stringhini. Mamadroid: Detecting android malware by
building markov chains of behavioral models (extended
version). ACM Transactions on Privacy and Security (TOPS),
22(2):1–34, 2019.

[31] TaeGuen Kim, BooJoong Kang, Mina Rho, Sakir Sezer, and
Eul Gyu Im. A multimodal deep learning method for
android malware detection using various features. IEEE
Transactions on Information Forensics and Security, 14(3):773–
788, 2018.

[32] Anshul Arora, Sateesh K Peddoju, and Mauro Conti.
Permpair: Android malware detection using permission
pairs. IEEE Transactions on Information Forensics and Security,
15:1968–1982, 2019.

[33] Anthony Desnos, Geoffroy Gueguen, and Sebastian Bach-
[Online] Available: https:

mann. Androguard, 2018.
//androguard.readthedocs.io/en/latest/.

[34] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal of Machine Learning
Research, 12:2825–2830, 2011.

[35] Charles R Harris, K Jarrod Millman, Stéfan J van der Walt,
Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric
Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith,
et al. Array programming with numpy. Nature, 585(7825):
357–362, 2020.

[36] Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
The elements of statistical learning: data mining, inference, and
prediction. Springer Science & Business Media, 2009.
[37] Fengguo Wei, Yuping Li, Sankardas Roy, Xinming Ou, and
Wu Zhou. Deep ground truth analysis of current android
malware. In International Conference on Detection of Intrusions
and Malware, and Vulnerability Assessment (DIMVA’17), pages
252–276, Bonn, Germany, 2017. Springer.

[38] Yuping Li, Jiyong Jang, Xin Hu, and Xinming Ou. Android
malware clustering through malicious payload mining. In
International symposium on research in attacks, intrusions, and
defenses, pages 192–214. Springer, 2017.

[39] Alireza Souri and Rahil Hosseini. A state-of-the-art survey
of malware detection approaches using data mining tech-
niques. Human-centric Computing and Information Sciences, 8
(1):3, 2018.

[40] Aleieldin Salem, Sebastian Banescu, and Alexander
Pretschner. Don’t pick the cherry: An evaluation methodol-
ogy for android malware detection methods. arXiv preprint
arXiv:1903.10560, 2019.

[41] Google. Android security & privacy. 2018 year in review,
https://source.android.com/security/

March 2019.
reports/Google_Android_Security_2018_Report_Final.
pdf.

[42] Shuofei Zhu, Jianjun Shi, Limin Yang, Boqin Qin, Ziyi
Zhang, Linhai Song, and Gang Wang. Measuring and mod-
eling the label dynamics of online anti-malware engines.
In 29th USENIX Security Symposium (USENIX Security 20),
pages 2361–2378. USENIX Association, August 2020. ISBN
978-1-939133-17-5.

[43] Google. Android security 2017 year in review, March
https://source.android.com/security/reports/

2019.
Google_Android_Security_2017_Report_Final.pdf.

[44] Martina Lindorfer, Stamatis Volanis, Alessandro Sisto,
Matthias Neugschwandtner, Elias Athanasopoulos, Fed-
erico Maggi, Christian Platzer, Stefano Zanero, and Sotiris
Ioannidis. Andradar: fast discovery of android applications
in alternative markets. In International Conference on Detec-
tion of Intrusions and Malware, and Vulnerability Assessment,
pages 51–71. Springer, 2014.

[45] Yajin Zhou and Xuxian Jiang. Dissecting android malware:
Characterization and evolution. In 2012 IEEE symposium on
security and privacy, pages 95–109. IEEE, 2012.

[46] Arash Habibi Lashkari, Andi Fitriah A Kadir, Laya Taheri,
and Ali A Ghorbani. Toward developing a systematic
approach to generate benchmark android malware datasets
and classiﬁcation. In 2018 International Carnahan Conference
on Security Technology (ICCST), pages 1–7. IEEE, 2018.
[47] Kevin Allix, Tegawendé F Bissyandé, Jacques Klein, and
Yves Le Traon. Androzoo: Collecting millions of android
apps for the research community. In 2016 IEEE/ACM 13th
Working Conference on Mining Software Repositories (MSR),
pages 468–471. IEEE, 2016.

[48] Sarunas J Raudys, Anil K Jain, et al. Small sample size
effects in statistical pattern recognition: Recommendations
for practitioners. IEEE Transactions on pattern analysis and
machine intelligence, 13(3):252–264, 1991.

[49] Sylvain Arlot and Alain Celisse. A survey of cross-
validation procedures for model selection. Statistics surveys,
4:40–79, 2010.

[50] Timothy Vidas and Nicolas Christin. Evading android
In Proceedings
runtime analysis via sandbox detection.
of the 9th ACM symposium on Information, computer and
communications security, pages 447–458, 2014.

[51] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow,
Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In
Proceedings of the 2017 ACM on Asia conference on computer
and communications security, pages 506–519, 2017.

[52] Guillermo Suarez-Tangil and Gianluca Stringhini. Eight
years of rider measurement in the android malware
ecosystem: evolution and lessons learned. arXiv preprint
arXiv:1801.08115, 2018.

[53] Mahmoud Hammad, Joshua Garcia, and Sam Malek. A
large-scale empirical study on the effects of code obfus-
cations on android apps and anti-malware products. In
Proceedings of the 40th International Conference on Software
Engineering, pages 421–431, 2018.

[54] Shuaike Dong, Menghao Li, Wenrui Diao, Xiangyu Liu,
Jian Liu, Zhou Li, Fenghao Xu, Kai Chen, Xiaofeng Wang,
and Kehuan Zhang. Understanding android obfuscation
techniques: A large-scale investigation in the wild. In Inter-
national conference on security and privacy in communication
systems, pages 172–192. Springer, 2018.

[55] Simone Aonzo, Gabriel Claudiu Georgiu, Luca Verderame,
and Alessio Merlo. Obfuscapk: An open-source black-box
obfuscation tool for android apps. SoftwareX, 11:100403,
2020. ISSN 2352-7110. doi: https://doi.org/10.1016/j.softx.
2020.100403.

[56] Mila Dalla Preda and Federico Maggi. Testing android mal-
ware detectors against code obfuscation: a systematization
of knowledge and uniﬁed methodology. Journal of Computer
Virology and Hacking Techniques, 13(3):209–232, 2017.

[57] Daniel Arp, Erwin Quiring, Feargus Pendlebury, Alexan-
der Warnecke, Fabio Pierazzi, Christian Wressnegger,
Lorenzo Cavallaro, and Konrad Rieck. Dos and don’ts
of machine learning in computer security. arXiv preprint
arXiv:2010.09470, 2020.

[58] Mark Gorrie. What is a pua (potentially unwanted
application) or pup (potentially unwanted program)?,

[Online] Available: https://us.norton.com/

2022.
internetsecurity-malware-what-are-puas-potentially-unwanted-applications.
html.

17

[59] Nadia Daoudi, Kevin Allix, Tegawendé F Bissyandé, and
Jacques Klein. A two-steps approach to improve the
performance of android malware detectors. arXiv preprint
arXiv:2205.08265, 2022.

[60] Arvind Mahindru and AL Sangal. Semidroid: a behavioral
malware detector based on unsupervised machine learning
techniques using feature selection approaches. International
Journal of Machine Learning and Cybernetics, 12(5):1369–1411,
2021.

[61] Jesper E Van Engelen and Holger H Hoos. A survey on
semi-supervised learning. Machine Learning, 109(2):373–440,
2020.

[62] Brad Miller, Alex Kantchelian, Michael Carl Tschantz,
Sadia Afroz, Rekha Bachwani, Riyaz Faizullabhoy, Ling
Huang, Vaishaal Shankar, Tony Wu, George Yiu, et al.
Reviewer integration and performance measurement for
malware detection. In International Conference on Detection of
Intrusions and Malware, and Vulnerability Assessment, pages
122–141. Springer, 2016.

[63] Aleieldin Salem, Sebastian Banescu, and Alexander
Pretschner. Maat: Automatically analyzing virustotal for
accurate labeling and effective malware detection. ACM
Transactions on Privacy and Security (TOPS), 24(4):1–35, 2021.
[64] Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier Movel-
lan, and Paul Ruvolo. Whose vote should count more:
Optimal integration of labels from labelers of unknown
expertise. Advances in neural information processing systems,
22:2035–2043, 2009.

[65] Silvia Sebastián and Juan Caballero. Avclass2: Massive
malware tag extraction from av labels. In Annual Computer
Security Applications Conference, pages 42–53, 2020.

[66] Sheng-Jun Huang, Rong Jin, and Zhi-Hua Zhou. Active
learning by querying informative and representative exam-
ples. Advances in neural information processing systems, 23:
892–900, 2010.

[67] Haibo He and Edwardo A Garcia. Learning from im-
IEEE Transactions on knowledge and data

balanced data.
engineering, 21(9):1263–1284, 2009.

[68] Alberto Fernández, Salvador García, Mikel Galar,
Ronaldo C Prati, Bartosz Krawczyk, and Francisco Herrera.
Learning from imbalanced data sets, volume 10. Springer, 2018.
[69] Akira Yokoyama, Kou Ishii, Rui Tanabe, Yinmin Papa, Kat-
sunari Yoshioka, Tsutomu Matsumoto, Takahiro Kasama,
Daisuke Inoue, Michael Brengel, Michael Backes, et al.
Sandprint: Fingerprinting malware sandboxes to provide
intelligence for sandbox evasion. In International Symposium
on Research in Attacks, Intrusions, and Defenses, pages 165–
187. Springer, 2016.

[70] Jiliang Zhang and Chen Li. Adversarial examples: Oppor-
tunities and challenges. IEEE transactions on neural networks
and learning systems, 31(7):2578–2593, 2019.

[71] Ambra Demontis, Marco Melis, Battista Biggio, Davide
Maiorca, Daniel Arp, Konrad Rieck, Igino Corona, Giorgio
Giacinto, and Fabio Roli. Yes, machine learning can be
more secure! a case study on android malware detection.
IEEE Transactions on Dependable and Secure Computing, 16(4):
711–724, 2017.

[72] Joshua Garcia, Mahmoud Hammad, and Sam Malek.
Lightweight, obfuscation-resilient detection and family
identiﬁcation of android malware. ACM Transactions on
Software Engineering and Methodology (TOSEM), 26(3):1–29,
2018.

[73] Xiaohan Zhang, Yuan Zhang, Ming Zhong, Daizong Ding,
Yinzhi Cao, Yukun Zhang, Mi Zhang, and Min Yang.
Enhancing state-of-the-art classiﬁers with api semantics to
detect evolved android malware. In Proceedings of the 2020
ACM SIGSAC conference on computer and communications

security, pages 757–770, 2020.

[74] Annamalai Narayanan, Mahinthan Chandramohan, Lihui
Chen, and Yang Liu. Context-aware, adaptive, and scalable
android malware detection through online learning. IEEE
Transactions on Emerging Topics in Computational Intelligence,
1(3):157–175, 2017.

[75] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and
Guangquan Zhang. Learning under concept drift: A review.
IEEE Transactions on Knowledge and Data Engineering, 31(12):
2346–2363, 2018.

18

