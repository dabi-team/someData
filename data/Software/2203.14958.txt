2
2
0
2

r
a

M
9
2

]
I

A
.
s
c
[

1
v
8
5
9
4
1
.
3
0
2
2
:
v
i
X
r
a

Requirements Elicitation in Cognitive Service for
Conversational Recommendation

Bolin Zhang, Zhiying Tu, Member, IEEE, Yunzhe Xu, Dianhui Chu, Member, IEEE,
and Xiaofei Xu, Member, IEEE,

1

Abstract—Nowadays, cognitive service provides more interactive way to understand users’ requirements via human-machine
conversation. In other words, it has to capture users’ requirements from their utterance and respond them with the relevant and suitable
service resources. To this end, two phases must be applied: I.Sequence planning and Real-time detection of user requirement,
II.Service resource selection and Response generation. The existing works ignore the potential connection between these two phases.
To model their connection, Two-Phase Requirement Elicitation Method is proposed. For the phase I, this paper proposes a user
requirement elicitation framework (URef) to plan a potential requirement sequence grounded on user proﬁle and personal knowledge
base before the conversation. In addition, it can also predict user’s true requirement and judge whether the requirement is completed
based on the user’s utterance during the conversation. For the phase II, this paper proposes a response generation model based on
attention, SaRSNet. It can select the appropriate resource (i.e. knowledge triple) in line with the requirement predicted by URef, and
then generates a suitable response for recommendation. The experimental results on the open dataset DuRecDial have been
signiﬁcantly improved compared to the baseline, which proves the effectiveness of the proposed methods.

Index Terms—Requirement Capture, Cognitive Service, Text Classiﬁcation, Response Generation.

✦

1 INTRODUCTION

C OGNITIVE conversation draws growing attention in

both academia and industry. Since natural languages
act as a powerful information carrier, conversation is a nat-
ural solution to the long-standing information asymmetry
problem in service matching [2]. Therefore, cognitive con-
versation can be viewed as a smart service deliverer in many
service seeking topics such as searching, recommendation,
and question answering. As seen through widely adopted
examples such as Google Assistant, Apple Siri, and Amazon
Alexa, these new-generation smart service delivery bots,
in the era of artiﬁcial intelligence, are expected to make it
much easier and more direct for users to obtain services via
cognitive conversation. By sensing intentions and demands
of the user, [3] deem those kind of bots are able to deliver
appropriate services to the user smartly. However, how to
elicit users’ requirements naturally and proactively need to
be coped with. Given the user requirement, it’s hard to select
a corresponding service resource and generate the response
to recommend this item to user.

To this end, the Requirements Elicitation in Cognitive

Service subjects to the following challenges:

- Sequential requirements: Usually, user’s require-
ments are sequentially correlated. It means that cer-
tain requirements often appear together in a speciﬁc
order. The existing works focus on single require-
ments detection without considering generate the

potential requirement sequence1. Sometimes, user’s
requirements may be fuzzy and need to be lead
into explicit requirements in a potential sequence
of requirements. Moreover, without the sequence ,
bots can not crawl the required service resources in
advance, which to meet user requirements.

- Dynamic requirements: During the conversation,
user’s requirement will change at any time and need
be satisﬁed in multi-turn. Therefore, whether the
requirement of the last turn are met or not will
affect the creation of a new requirement. The exist-
ing works [4] only focus on detecting the current
requirement, but lacked judgment on the completion
of the requirement. Moreover, after the requirement
changes, the potential sequence should also change
dynamically in real time be re-planned.

- Appropriate reply: Either in requirement elicitation
or response generation, a piece of reply text should
be output by generative models. However, the re-
sponses generated by this kind of models are usually
safe, anomaly, and hollow 2, so it is difﬁcult to deploy
for recommendation [5]. Thus, the generative models
should has the ability to select the relevant service
resource and generate an appropriate reply to the
user.

To address these challenges, two phases need to be
applied in succession. In Phase I, the mission of User Re-
quirement Elicitation is divided into two parts: Sequence

• All authors are with the School of Computer Science and Technology,

Harbin Institute of Technology, Weihai 264209 , China.
E-mail: {brolin, tzy hit, 181310122, chudh, xiaofei}@hit.edu.cn

This Manuscript based partly on work [1] presented at conference (ICWS
2021).

1. A path containing multiple requirements, the nodes have a contin-

uation relationship, and the length is moderate between 3-6.

2. Safe means the bot often responds with universal answers such
as ”thank you”; anomaly means the bot responds with opposing, false
and insulting remarks; hollow means the bot responds with vague and
unspeciﬁc content due to lack of knowledge of the user.

 
 
 
 
 
 
URef-(a)

Sequence
Ranking

Perfomance
Scoring

Knowledge
Scoring

User Requirement
2: chitchat about celebrities
3:  play music
3' : recommend movie

Potential Requirement Sequence

1

2

3

4

5

2

User Profile
name,age,gender
movie:Initial D
music:[Try,M ojito]
star:Jay Chou
food:null
...

Phase I

Phase II

Personal KB
(Jay Chou, Sing, Try)
(Jay Chou, starring in, 
Initial D)
...

URef-(b)

finish

update

3'

1

2

3

Joint Model

...

utterance

 last requirement

Bot: It seems that you like Jay Chou very much!
User: Yeah, Jay Chou's movie is also very good

service 
 resources

utterance

SaRSNet

Encoder

Attention
Network

Decoder

current 
requirement

The Initial D is a movie starring Jay Chou, which 
has been well received and recommended to you

generate

Fig. 1. The Overview of the proposed Two-phase Requirement Elicitation Method. Above the green line, Phase I is applied: The Requirement
Sequence planning is shown in the blue-dotted rectangle and Real-time detection of user requirement is shown in the red-dotted rectangle. Below
the green line, Phase II is applied: Given the current requirement, user’s utterance and service resources, SaRSNet generates an appropriate
response to meet user’s requirement.

Planning and Real-time Detection. In Phase II, the Recom-
mend Response Generation task is also divided into two
parts: Service Resource Selection and Response Generation.
Requirement Elicitation: ﬁrstly, to plan a potential re-
quirement sequence for multi-turn dialogue, and then to
detect the user’s requirement from their utterance and judge
whether requirement be met at the same time. Grounded
on user’s preference and personal Knowledge Base (KB),
Sequence Planning prepares user’s potential requirement
chain before the conversation. Based on users’ utterances,
Real-time Detection predicts user’s current-turn require-
ment during the conversation. By this way, the new re-
quirement is regarded as the starting vertex then re-plans
the potential chain when users change their mind. During
the conversation, the timing to lead the next potential re-
quirement depends on whether the current one has been
fully met. Thus, Real-time Detection is divided into two
potentially related sub-tasks: Requirement Completion Es-
timation and Current Requirement Prediction. The former
one is binary-class classiﬁcation task for predicting whether
the current requirement is fully ﬁnished. The latter one
is a multi-class classiﬁcation task for predicting the user’s
current requirement. These two sub-tasks can make full use
of the information learned by each other and improve their
own performance.

To this aim, the proposed user requirement elicitation
framework (URef) with two parts, URef-(a) and URef-(b),
will be applied in Phase I (as shown in Fig. 1). URef-(a) will
use the domain knowledge, such as music, celebrity, movie,
and etc., as the constraints of the requirement sequence
planning, due to the lack of domain-related knowledge
in the personal KB. For instance, if a user has the most

preferred entities in music domain, then URef-(a) will set the
sequence to ”daily greetings” → ”chitchat about celebrities”
→ ”play music”→... ”goodbye”]. URef-(b) is reponsable to
predict whether the current task is end, and decide whether
to follow the pre-deﬁne planning. For example, during the
conversation, URef-(b) realizes that it is the time to kick-
off the task node ”chitchat about celebrities”, Meanwhile,
it ﬁnds that the user’s intent (the current requirement)
has become ”recommend movie, according to the user’s
utterance. Thus, it asks URef-(a) to re-plan the sequence
with ”recommend movie” as a new starting point.

Response Generation: generating an appropriate reply
text based on the user’s utterance to elicit his/her explicit
requirement or satisfy his/her requirement directly. In the
personal KB, each SPO (Subject-Predicate-Object) triplet is
labeled with a requirement domain, and a triple can be
regarded as a service resource. The domains of the service
resources is consistent with the domains of the user proﬁle.
After elicit user’s requirement by URef, Service Resource
Selection aims to pick out one of the service resources that
belongs to the same domain as user requirements from the
personal KB. This allows the bot to meet the user’s current
requirement. Given users’ requirements, user’s utterance
and the selected service resource, Response Generation aims
to output a smooth, appropriate and informative text to
response the user.

For this purpose, the proposed recommend response
generation model (SaRSNet) will be applied in Phase II. As
shown in Fig. 1, SaRSNet is an encoder-decoder network
that selects the appropriate resource (i.e. SPO Triple) in
line with requirement of utterance and then generates a
suitable response to user. For example, the requirement

node (”recommend movie”) predicted by URef, then the
user’s utterance and the service resources will all be taken
as input by SaRSNet. Afterwards, one of the triples about
the movie will be selected based on attention network and
a text containing the service resource (Jay Chou, starring in,
Initial D) will be generated as output to response the user.
The main contributions are summarized as follow:

1) This paper propose a novel user requirement elici-
tation framework, URef. Based on user’s preference
and personal KB, URef-(a) prepares user’s potential
requirement sequence. Based on user’s utterance,
URef(b) judges whether the current requirement
have been met and then detect the true requirement.
2) This paper propose a response generation model
after user requirement elicitation, SaRSNet. Given
the user’s requirement, SaRSNet Choose the most
relevant service resources in personal KB and then
generation an appropriate response to user.

3) Both URef and SaRSNet conducted experiments on
the public dataset DuRecDial and the results have
been signiﬁcantly improved compared to the base-
line.

The rest of this paper is organized as follows. Section 2
summarizes the related works. URef is described in Section
3 and SaRSNet is described in Section 4. Section 5 shows the
experimental setup and Section 6 analyzes the experimental
result. In the end, 7 concludes this paper.

2 RELATED WORKS
The related works are in line with three major research
topics: Requirement Elicitation, Text Classiﬁcation and Re-
sponse Generation.

2.1 Requirement Elicitation

Basically, Requirement Elicitation (RE) is the practice of
researching and discovering the requirements of a system
from users, customers, and other stakeholders [6] in soft-
ware requirements engineering. In cognitive service, RE
refers to the process of detecting and understanding user
requirements from the original user requirement expression
(i.e. utterance). The existing works [7]–[9] focus on interpret-
ing user’s requirement by extracting user’s intention from
their one-off questions. However, only through multi-turn
dialogue, the explicit requirement of user can be elicited and
satisﬁed. Due to user’s goal-oriented behaviour in a conver-
sation, goal (i.e user requirement) planning has attracted lots
of research interests [10]. According to a certain requirement
of the user, the bot will have a goal in line with it. Endowing
it with the ability of proactively leading the dialogue with
an explicit goal, Requirement Planning takes a radical step
towards building type of human-like conversational agents
[11]. Although some research works [12]–[14] provide more
controllability to RE by planning goal, these models can
just output a single requirement, instead of a requirement
sequence. The work [15] considers the problem of goal
planning grounded on knowledge graph (KG). However,
this work regards the goal as an entity in KG, actually solves
the problem of topic selection in chitchat. In contrast, this
paper consider more diverse requirements (e.g. order news,

play music, recommend movie), not only limited to chitchat.
Furthermore, the existing works only predict the current
requirement based on user utterances while this paper plan
a potential and reasonable requirement sequence before the
conversation.

3

2.2 Text Classiﬁcation

Text classiﬁcation (TC) is the task of assigning a sentence
or document (e.g. articles, comments, utterances) to an
appropriate category [16]. With the development of deep
learning and pre-training models, TC has not only made
great progress in traditional tasks such as sentiment analysis
[17], [18] and news classiﬁcation [19], but also achieved
good results in multi-task learning (e.g. Intent Detection
and Slot Filling [20], [21], Entity and Relation Extraction
[22], [23], Sentiment and Act Classiﬁcation [24], [25]). In
this paper, the detection of user requirement can also be
regarded as a TC task. The traditional TC task only considers
the semantic information of the text, while the joint model
is used in multi-task learning framework to realize the
transfer and sharing of information between different sub-
tasks. Each sub-task can make full use of the information
learned by the other’s task to improve its own effect [26].
However, these works do not integrate features outside of
the semantics and the data labels are static, which cannot
effectively model the dependence3 of each requirement in
the time series during the conversation. The data label of
requirement detection has an order-dependent relationship.
By introducing the sub-task of Requirement Completion
Estimation, the requirement of the last time step is encoded
to deal with the classiﬁcation task in dynamic scenarios.

2.3 Response Generation

Given the user’s utterance, Response Generation (RG) is
aiming to generate a consistent and engaging response
text [27]. RG is an key component of the dialogue system
that affects the naturalness of conversation and the user
experience [28]. Inspired by the ﬁeld of machine translation,
the current mainstream approach to response generation is
to treat the task as a “translation” of user expressions to
machine responses. Since no external knowledge (service
resource) is introduced, the responses generated by the
end-to-end models proposed in the existing works ( [29]–
[31]) are relatively hollow and contain less information.
Although taking knowledge as additional input can alle-
viate the problem of generic responses in dialogue systems,
choosing right knowledge facts for the current scenarios is
still a challenge. To cope with it, the industry has proposed
several transformer-based pre-training generative models:
Plato2 [32] from Baidu with 1.6 billion parameters, Meenea
[33] from Google with 2.6 billion parameters, Blender [34]
from Facebook with 9.4 billion parameters, etc. Using a a
large-scale corpus as training data, these models will learn
some human response patterns under different dialogue
objectives, and the various knowledge contained in the
corpus will be implicitly retained by the large number

3. The choice of the next requirement depends to a certain extent
on the current requirement. For example, if the current requirement is
“recommend music”, the next requirement is likely to be “play music”.

of parameters. But this will take up a lot of computing
resources and training time, which is unbearable in some
cases. In this way, the user’s ﬁne-grained requirements such
as ordering restaurants and checking the weather cannot be
dealt with in task-oriented dialogues. In cognitive service,
when a user puts forward a requirement, the bot needs to
inform the user of the service resources (knowledge) which
satisfy the requirement [35]. Therefore, user’s requirement
(dialogue goal), service resources should be uniﬁed as ex-
ternal information as the input of the generative model.

3 USER REQUIREMENT ELICITATION

3.1 Overview of The Framework

The whole framework is divided into two parts: URef-(a)
and URef-(b). It respectively plans the potential requirement
sequence before the conversation and detects the require-
ments during the conversation. URef-(a) takes User Proﬁle
and Personal KB as input, and the output is a requirement
sequence that conforms to the user’s preference. URef-(a)
includes three steps:

- Performance Scoring: Calculate the user’s prefer-

ence for a single domain

- Knowledge Scoring: Calculate the knowledge rich-

ness of triplets in a single domain

- Sequence Ranking: According to the calculation of
the above modules and the heuristic information
in the historical data, the candidate sequence set is
screened to ﬁnd the optimal one.

news order

recommend news

ask news type

QA

ask the date

daily greetings

ask m

usic na

m

e

chitchat about celebrities

r

e

c

o

m

m

e

n

d

m

music order

u

sic

pla

y 

m

u

sic

v i e

d   m o

n

o m m e

c

r e

goodbye

ask movie name

ask the weather

recommend 
food

ask starring role

m en d p oi

reco m

ask time

weather information push

Fig. 2. Requirement Transition Backbone Graph. Each node on the
graph represents a user requirement. The edges between nodes rep-
resent the requirement transition relationship, which is mined from the
requirement sequence of users in the dataset. When there is a new kind
of requirement appears or no training set, this graph can be manually
designed and has a certain scalability.

The heuristic information, as shown in Fig. 2, is a user re-
quirement transition backbone graph constructed according
to the sequence of different users in training set. The can-
didate sequence set is obtained by traversing the backbone
graph.

During the conversation, URef-(b) uses completion sub-
net and prediction subnet to cope with Requirement Com-
pletion Estimation and Current Requirement Prediction
respectively, and enhances the performance of these two

4

networks by joint learning. The embedding vector of the last
utterance’s requirement node in the backbone graph and the
semantic vector of the current utterance are used as input for
URef-(b), which takes the result of requirement completion
(binary labels) and requirement prediction (multiple labels)
as output.

Algorithm 1 Requirement Transition Graph Building Algo-
rithm
Input: Historical requirement sequences set P ,the k-th
user’s requirement sequence is represented by pk, pk ∈
P ; M , a mapping dictionary for each requirement in pk
Output: A[n][n], n = |V |, the adjacency matrix of Transition

Graph G = (V, E)

len = length(pk)
for l = 0; l < len; l + + do

1: initialize matrix A with zero elements
2: for pk in sequences set P do
3:
4:
5:
6:
7:
8: end for
9: return A[n][n]

i, j = M (pk[l]), M (pk[l + 1])
A[i][j] + +

end for

3.2 Sequence Planning

The potential requirement sequence can be represented by
a graph structure, where each vertex represent a kind of
requirement and edge represent two different requirements
appear one after another. Before the sequence planning,
URef-(a) ﬁrstly construct Backbone Graph G = (V, E) by
Algorithm 1.

V is a set of nodes, and vi ∈ V denotes a kind of
requirement. E is the set of edges, and e = (vi, vj) ∈ E
denotes the transition relationship from vi to vj . The output
of Algorithm1 is an adjacency matrix of G, which is visual-
ized as the network as shown in Fig. 2.

Then, this paper designs two pipeline strategies to ﬁlter
the candidate paths in G. Strategy 1 ﬁrst uses the preference
satisfaction to ﬁlter out topk paths, and ﬁnally uses knowledge
abundance of user’s preferred domain to ﬁlter out the only
path as requirement sequence. Strategy 2 uses knowledge
abundance ﬁrst and then preference satisfaction for selection.
The inspiration of the proposed strategies comes from:

- A user’s preference for different domains is differ-
ent. The requirements that are relevant to the user’s
favorite domain (i.e. the domain contains the most
interest entities in proﬁle) should be prioritized.
- Under the condition of knowledge limited, the do-
main with the greatest knowledge richness should
be prioritized.
Strategy 1 and 2 respectively represent two different
dialogue modes preference-driven and knowledge-
driven.

-

{D} denotes a set of types of preferred domains appear-
ing in user proﬁle, the amount of domains is |D|. {enti
j}
denotes a list of interest entities of the i-th user in the j-th
domain, and |enti
j| denotes the amount of entities in it. The
set {R} contains all user requirements, and {Drk } $ {D}

 
Output probabilities 
of  the second goal
 in the sequence

batchsize*21

Softmax

Linear Layer

classifier 2

Output probabilities 
of  the first goal
 in the sequence

batchsize*21

Softmax

Linear Layer

classifier 1

Linear Layer

Relu

Relu

Linear Layer

Linear Layer

Relu

original 
features

additional 
features

5

classifier 3

Linear Layer

Output 
probabilities 
of  the  sixth goal
 in the sequence

Softmax

batchsize*21

Output probabilities 
of  the third goal
 in the sequence

batchsize*21

classifier 4

Linear Layer

Softmax

Linear Layer

classifier 6

Softmax

batchsize*21

Output probabilities 
of  the  forth goal
 in the sequence

classifier 5

Softmax

batchsize*21

Linear Layer

Output 
probabilities 
of  the  fifth goal
 in the sequence

Fig. 3. The architecture of SP-MLP. SP-MLP contains 6 classiﬁers, each of them predict the requirement sequentially.

denotes a set of preferred domains involved in a single
requirement rk, rk ∈ {R}. satrk denotes the i-th user’s
preference satisfaction of rk, formulated as:

satrk = P
P

|Drk |
m=1 |entm|i
|D|
n=1 |entn|i

∈ [0, 1]

(1)

{kgi} represents the i-th user’s personal KB linked with
his proﬁle. tj represents the j-th SPO triplet, and tj ∈ {kgi},
1 < j < |kgi|. {T dl
gk } represents a set of triplets of re-
quirement rk in l-th preferred domains, and k ∈ [1, |R|],
l ∈ [1, |D|]. abdrk denotes knowledge abundance of rk, formu-
lated as:

Algorithm 2 Requirement Sequence Planning Procedure
Input: Backbone graph G; i-th user’s proﬁle upi;

, i-th
user’s personal KB kgi; strategy identiﬁer s, picked out
from 1 and 2; topk, threshold of path numbers in the ﬁrst
ﬁlter;

Output: path, a sensible sequence of requirement nodes
1: initialize satisfaction and abundance score list S, A
2: initialize candidate path set C
3: P = T raverse(G, node = N one) {if node speciﬁed, tra-
verse all the paths starting from the given requirement
in G }

4: for p in path set P do
5:

initialize satisfaction and abundance score of p, sp, ap

abdrk = P

|D|
m=1 |T dm
rk |
|kgi|

∈ [0, 1]

(2)

The procedure of sequence planning is shown in Al-
gorithm 2. Firstly, URef traverse backbone graph built by
Algorithm 1 to get the candidate set of sequences, ans path.
Then Performance and Knowledge Scoring modules will be
used to calculate satrk and abdrk , respectively. Finally, all
the sequences in ans path are scored according to the given
strategy, and the one with the highest score is the output.

To compare with Algorithm 2, this paper designs a
simple Multi Layer Perceptrons Model (SP-MLP) for Se-
quence Planning, as shown in Fig. 3. It contains six 21-
class classiﬁers, each of them consists of 3 linear layers
and uses ReLU as the activation function to sequentially
predict the requirements. Each classiﬁer is connected by
a fully connected layer, and use sof tmax to calculate the
probability distribution of the 21 labels (20 requirement-
labels and one none-label) at its position. The cross entropy
is adopted as the loss function of SP-MLP. Apart from the
features already given in the dataset, SP-MLP takes user
preferences and knowledge abundance as the additional
input.

for r in path p do
sp = satr + +
ap = abdr + +

6:
7:
8:
end for
9:
S.append(sp)
10:
11: A.append(ap)
12: end for
13: if s==1 then
14:
15: else
16:
17: end if
18: path = max(C, 1)
19: return path

C = max(A, topk)

C = max(S, topk) {ﬁnd the largest topk paths in S}

3.3 Real-time Detection

The architecture of the joint model in URef-(b) is shown
in Fig. 4, contains three modules: Encoder, Completion
Network and Completion Network. The model takes the
current utterance and the last utterance’s requirement as
the input. ut represents the utterance of one user at time
t, and utterance’s requirement at the last time step is rut−1 .

prediction network

recommend movie

6

completion network

QA

ask the date

recommend 
movie

undone

ask music name

Pre-trained Model ERNIE Encoder

Node2Vec Encoder

chitchat about 
celebrities

w s

e

d   n

n

m e

m

o

c

r e

User Text: Yeah, Jay Chou's movie is also very good. Last Requipment: chitchat about celebrities

Fig. 4. The Architecture of Joint Model in URef-(b), including two encoders and two novel networks

The probability of requirement completion is estimated by
the formula (4) and the current requirement is predicted by
maximizing the following probability:

Then, completion vector vc which is utilized in the

binary-class classiﬁcation task, which is deﬁned as:

yc = softmax (W1vc + b1)

(7)

arg max
rut

P (cid:0)rut | ut, rut−1 (cid:1)

Prut (cid:0)l = 1 | ut, rut−1 (cid:1)

(3)

(4)

3.3.1 Encoder
URef-(b) contains two encoders: ERNIE and Node2Vec.
ERNIE is a large-scale pre-training model to extract seman-
tic features in utterances. Transformer is used to encode text
by ERNIE, which is a Seq2Seq model with the ability to
encode variable-length sentences into ﬁxed-length vectors
based on Self-Attention. After entering the user text expres-
sion into ERNIE, the vector vs which represents the semantic
information of utterance is obtained.

Node2Vec is a Graph Embedding model, based on
random walk algorithm to extract requirement transition
features. Node2Vec learns a mapping of nodes to a low-
dimensional space of features by maximizing the likelihood
of preserving network neighborhoods of nodes. Thus, the
vector vn, representation of each node in the backbone
graph, can be obtained.

3.3.2 Completion Network
Compared with the requirement information vn, the com-
pletion network prefer to use the utterance information
vs when estimating whether the requirement is complete.
Therefore, this network applies forget gate mechanism of
LSTM in the calculation of a weaken factor fc which can
control how many requirement information will be dropped
out. This weaken factor fc is deﬁned as:

fc = sigmoid (Wcvs + Ucvn + bc)

(5)

In the formula (5), Wc and Uc are the weighted matrices
and bc is the bias. In particular, node vector vn has the same
dimension as sentence vector vs.

After the requirement transition information forgotten
mechanism, completion vector vc that participates in the
estimation of requirement completion, which is deﬁned as:

vc = fc × vs, fc ∈ [0, 1]

(6)

3.3.3 Prediction Network

In the requirement prediction, both the utterance informa-
tion vs and the requirement information vn are applied.
Firstly, the prediction network calculates a reinforce factor
fr which can control how many requirement transition in-
formation will be fused. This reinforce factor fr is calculated
in the same way as fc:

fr = sigmoid (Wrvs + Urvn + br) ∈ [0, 1]

(8)

Then, the weakened vector vc output from the comple-

tion network is merged with enhanced sentence vector as:

fp = tanh (Wpvs + Up(frvc) + bp) ∈ [−1, 1]

(9)

The reset gate mechanism (similar to GRU) applies in
the calculation of the fusion factor fp, which can not only
drop out the information in vn that has nothing to do
with the requirement prediction, but also can reinforce the
information related to the prediction in vs.

Finally, the fusion vector vp is deﬁned in the formula
(10), which is utilized to predict the requirement of the
current utterance as the formula (11).

vp = fpvn + vs) + bp

yp = softmax (W2vp + b2)

(10)

(11)

4 RECOMMEND RESPONSE GENERATION
4.1 Overview of The Model

As shown in Fig. 5, SaRSNet is basically an encoder-decoder
model to deal with text generation task. The requirement
node, utterance and service resources will be taken as input
by the same Encoder. The requirement node is a phase pre-
dicted by URef, which represents a kind of user’s require-
ment. The utterance is current user’s expression text. The

7

copy 
mechanism

Context
Vector

n
o
i
t
u
b
i
r
t
s
i

n
o
i
t
n
e
t
t

A

Hidden

generate 
mechanism

Vocabulary (cid:1)istribution

...

...

Linear Layer

Selection Network

 Encoder

(cid:0)ecoder

Fig. 5. The architecture of SaRSNet, which is basically an encoder-decoder model.

service resources is a set of SPO triples in Personal KB, and
their domain is consistent with the domain of requirement
node after ﬁltering by Algorithm 3. The service resource
most relevant to the utterance is selected based on Selection
Network. The structure of Decoder in SaRSNet is the same
as the Encoder. In order to enable SaRSNet to replicate
the selected service resource during decoding, there is a
λ to control the switching between copy mechanism and
generation mechanism. Speciﬁcally, λ is used as a soft switch
to choose between generating a word from the vocabulary
by sampling from the Vocabulary Distribution, or copying
a word from the input sequence (contains service resource)
by sampling from Attention Distribution.

4.2 Resource Selection

During the encoding, Selection Network choose the best
service resource according to the user’s utterance. The de-
tails of Selection Network is shown in Fig. 6. The tokens
of each input text are fed one-by-one into the Encoder
(a single-layer bidirectional LSTM), producing a sequence
of encoder hidden states hi. Input the requirement, the
Encoder’s outputs hr, which is the last hidden state. Input
the utterance, the Encoder outputs hu, which is composed of
all hidden states (h1
u ). T represents the numbers
of tokens in the utterance. The service resources are spliced
into a sentence as the input of the Encoder, and <#>is added
at the end of each triplet. Then, the Encoder outputs hk,
which is composed of all hidden states (h1
k ) of
<#>. M represents the numbers of service resources. At the
time step t, the score of ht
u on the i − th service resource hi
k
is calculated as:

k, ..., hM

u, ..., hT

u, h2

k, h2

sti
k =

exp (cid:0)ht
uhi
k(cid:1)
M
uhi
i=1 exp (cid:0)ht

k(cid:1)

P

, i = 1, 2, . . . , M

(12)

The ﬁnal score of ht

u on all service resources is deﬁned

as:

k = max (cid:8)sti
st
u is scored by st

k , i = 1, 2, . . . , M (cid:9)
k and Selection Network outputs

(13)

Then, ht

ht
s:

s = ht
ht

ust
k

(14)

Finally, hu, hs and hr are compressed into ﬁnal hidden

states h by formula (15), where h = (h1, h2, ..., hT )

h = W hu + V hs + δhr

(15)

4.3 Response Generation

After the resource selection during encoding, the Decoder
of SaRSNet takes the ﬁnal hidden states h as input. In order
to copy a word from the selected resource at time step i,
the Attention Distribution ait is calculated as formula (16),
where t represents the time step of the Encoder and ei
represents the hidden state of the Decoder at time step i. The
time steps of the Decoder and Encoder are not consistent.

ait =

exp (ei−1ht)
M
k=1 exp (ei−1hk)

P

, t = 1, 2, . . . , M

(16)

Then, the context vector ci at time step i is calculated as:

ci =

T

X
t=1

exp (aitht)

(17)

ci can be seen as a ﬁxed-size representation of what has
been read from the source for this step. The vocabulary
distribution of generated words at time step i is calculated
as formula (18), where V ′, V , b′ and b are all learnable
parameters.

Pvocab = softmax (V ′ (V [ci, ei] + b) + b′)

(18)

D
SaRSNet utilize λ as soft switch to choose between copy
mechanism and generation mechanism at time step i, which
is calculated as formula (19).

λ = sigmoid (Wλ [ci, ei] + bλ)

(19)

Thus, the distribution of Decoder’s output words at the

time step i is calculated as formula (19).

yi = λPvocab + (1 − λ)

T

X
t=1

ait

(20)

Encoder

...

...

...

...

...

...

...

...

...

Encoder

Encoder

Fig. 6. The details of Seclection Network in SaRSNet.

Algorithm 3 Resource Classiﬁcation Process
Input: The set of domains, {D}; predicate in a SPO triple,

p; similarity threshold θ, set as 0.7

Output: The domain to which the current triple belongs, d,

d ∈ {D};

1: for di in {D} do
2:

s = sim(t, di) {compute semantic similarity between
t and di by SimNet}
if s ≥ θ then

d = di

3:
4:
end if
5:
6: end for
7: return d

5 EXPERIMENTS
5.1 Datasets

The experiments involve an open dataset named DuRecDial
(about 10k dialogues, 156k utterances), which contains user
proﬁle for personalized recommendation, relevant Personal
KB (service resource) and clear requirements to meet during
each conversation. DuRecDial is split into train/dev/test
data by randomly sampling 70%/10%/20% data. In DuRec-
Dial, there are 20 requirements and seven domains of user

proﬁle in total. The mapping relation between these do-
mains and user requirements is shown in Table. 1. However,
the service resources (triplets of the Personal KB) in DuRec-
Dial are not annotated with domains of user proﬁle. The
process of resource classiﬁcation is shown in Algorithm 3.

8

TABLE 1
Domains of Requirements in User Proﬁle & Personal KB

Domain
Star
Movie

Music

Food
POI
News

Weather

Requirments

chitchat about celebrities

recommend movie, ask movie name, ask starring role

recommend music, play music, music order,
ask music name

recommend food
recommend poi

recommend news, news order, ask news type
ask the weather, ask time, ask the date,
weather information push

∗

daily greetings, goodbye
∗ denotes requirements that have nothing to do with user proﬁle and personal
KB.

5.2 Comparison Models

For Sequence Planning, this paper compares the machine
learning model SP-MLP with the traditional strategy in
Algorithm 2. To show the effectiveness of the strategy pro-
posed in URef-(a), this paper compares two different inputs
of SP-MLP: original feature (OF) and additional feature
(AF). OF is a one-hot encoded feature of user proﬁle and
personal KB given in the dataset. After concatenating two
1 × 20 vectors, OF becomes AF. These two vectors encode
the preference satisfaction and knowledge abundance features
of user requirement, These two features are proposed in
Algorithm 2.

To show the advantages of URef-(b) for Real-time Detec-
tion, this paper compares it with the following text classiﬁ-
cation models:

- TextCNN: a typical deep learning algorithm for sen-
tence classiﬁcation tasks based on convolutional neu-
ral network [36].

- LSTM: a special kind of RNN, capable of learning

long-term dependencies in sentence [37].

- BiLSTM: bi-directional LSTM, can better capture the

two-way semantic dependence in sentence.

- BERT: a pre-trained model can be ﬁne-tuned with
just one additional output layer to obtain state-of-
the-art results on a wide range of natural language
processing tasks [38].

- ERNIE: a novel pre-trained language representation
model enhanced by knowledge, achieving new state-
of-the-art results on Chinese natural language pro-
cessing tasks [39].

- PaddlePALM: a fast, ﬂexible, extensible and easy-
to-use NLP large-scale pre-training and multi-task
learning framework based on ERNIE [40].

To show the advantages of SaRSNet for Recommend
Response Generation, this paper compares it with the fol-
lowing text generation models:

-

S2S-Attn: a model
introduces the attention
mechanism into S2S [41], where S2S is a vanilla
seq2seq model based on LSTM [42].

that

- Pointer-Generator: a hybrid pointer-generator net-
work that can copy words from the source text via
pointing [43].

- MGCG G: a multi-goal driven conversation genera-
tion framework, consists of ﬁve components: a Utter-
ance Encoder, a Knowledge Encoder, a Goal Encoder,
a Knowledge Selector, and a Decoder [44].

5.3 Evaluation Metrics

Totally, six evaluation metrics were used in the experiments:
Exact Match (EM), BLEU (BLEU-2) [45], Averaged Goal Re-
call (AGR) [1], Accuracy, Precision, Recall perplexity (PPL)
[46] and DISTINCT (DIST-2) [47].

For the task of sequence planning, EM and BLEU-2 are
utilized to indicate the general performance of URef-(a) on
sequence-level. EM indicates the percentage of predicted
requirement sequence that match the ground truth exactly,
which is often used to evaluate Constituency Parsing task.
BLEU-2 measures the similarity between predicted sequence
and real sequence, which is often used to evaluate Machine
Translation task. Following the setting in previous work (
[1]), this paper also measures the node-level performance of
URef-(a) using AGR4.

For the task of Requirement Completion Estimation and
Current Requirement Prediction, this paper both uses Accu-
racy to measure the performance of URef-(b), which is con-
sistent with the baseline. Moreover, F1 is used to measure
the overall performance of the model additionally, which is
the harmonic mean between the Recall and Precision.

For the task of Resource Selection, precision, recall and
F1 scores are evaluated the performance of SaRSNet. When
calculating the knowledge precision/recall/F1, this paper
compares the generated results with the correct knowledge.
For the task of Response Generation , BLEU-2, PPL and
DIST-2 are used to measure the performance of SaRSNet.
These three common metrics quantify the relevance, ﬂuency,
and diversity of generated responses.

5.4 Parameter Settings

In the experiments, the Adam Optimizer [48] is used to train
SP-MLP and the initial learning rate is set as 1e-3.

In URef-(b), max seq len is set as 128. Adam Optimizer
is also used and the initial learning rate is set to 2e-5.
Optional parameter warmup proportion is set as 0.1 and
weight decay is set as 0.01. For requirement transition
graph embedding learning, the window size h of random
walk is set as 3, the return value is set as 1, the inout value
is set as 1 and the dimension of graph-based requirement
node embedding is set as 100.

In SaRSNet, max seq len is set as 256, hidden size
of Encoder is set as 512 and dropout is set as 0.2. Adam
Optimizer is utilized and the initial learning rate is set to
2e − 4. warmup proportion is set to 0.2 and weight decay
is set as 0.01. The beam search is utilized during decoding
and the beam size is set as 10.

4. In this paper, AGR calculates the average recall of all requirement

nodes instead of the dialogue goal.

9

6 RESULTS
6.1 Sequence Planning

The proposed strategy is divided into two ﬁlters. The ﬁrst
ﬁlter selects topk sequences from the entire candidate se-
quences, and the second ﬁlter selects one of these topk
sequences as the ﬁnal result. The performance of the two
strategies proposed in URef-(a) is shown in Fig. 7. The size
of topk represents the ﬁrst ﬁlter’s hardness, which means
the diversity of the sequence set after screening. The larger
the topk, the easier the sequence in candidate set is to be
selected. When topk exceeds a threshold, the ﬁrst screening
becomes meaningless. In order to choose a suitable topk,
this paper sets topk from 1 to 8 and conduct experiments
respectively. In Fig. 7, EM1 and AGR1 represent Exact Match
and Averaged Goal Recall of Strategy 1 respectively.

The results show that with the increase of topk, the
accuracy and BLEU-2 of Strategy 1 gradually increase, and
both reach the maximum when topk = 5. Strategy 1 out-
performs Strategy 2 overall. Due to the different precision
of indicators, BLEU-2, AGR and EM decrease in order on
Strategy1&2. In the real world, the smarter strategy for
each one in dialogue is to “prescribe the right medicine”,
that is, ﬁrst decide which topics (requirements) to talk
about according to the other’s preferred domain, and then
screen the topics according to their own domain knowledge
(service resource). The experimental results are in line with
this reality.

However, with the increase of topk, the performance
of Strategy 2 gradually decreases. The explanation for this
result is that in dialogue, based solely on the knowledge
you have mastered, there will be a phenomenon of “self-
talking”. If this strategy is adopted, the requirement nodes
set by bot will deviate from user’s interests. The larger the
topk, the more beneﬁcial it is to oneself rather than the
other party, and it will deepen the degree of divergence.
preference satisfaction and knowledge abundance are also used
for sequence planning, respectively, with unpleasant ex-
perimental results, as shown in Table. 2. This proves the
effectiveness of the proposed strategy.

TABLE 2
Requirement Sequence Planning Performance

Method
Strategy1
Strategy2

Performance-only
Knowledge-only
SP-MLP-OF
SP-MLP-AF

EM
88.82
85.14
78.65
75.48
64.84
96.55

AGR
92.44
88.96
81.43
78.91
67.63
98.16

BLEU-2
94.03
92.55
83.24
81.68
78.83
98.57

Besides, this paper compares two different inputs of SP-
MLP. The experimental results are shown in Fig. 8. With the
increase of the epochs, the performance of OF and AF grad-
ually increases, and it drops when epoch =6 or 7. Although
OF’s performance is much lower than Strategy1&2, AF’s
performance has been greatly improved compared with OF.
EM of AF is the highest when epoch = 5, reaching 96.55%.
This further proves the effectiveness of the strategy.

The best experimental results of all the models and
strategies are shown in Table. 2. The performance of SP-

95%

93%

91%

89%

87%

85%

83%

(cid:2)(cid:3).(cid:4)2(cid:5)

(cid:6)(cid:7).(cid:8)2(cid:9)

(cid:10)(cid:11).(cid:12)4(cid:13)

(cid:14)(cid:15).(cid:16)(cid:17)(cid:18)

(cid:19)(cid:20).(cid:21)(cid:22)(cid:23)

(cid:24)(cid:25).(cid:26)(cid:27)(cid:28)

(cid:29)(cid:30).(cid:31) !

"5.#$&

’(.1)*

81%

80.26%

79%

77%

75%

79.2+,

-../92

39.:9;

<=.>9?

@A.B9C

EF.G9H

95%

93%

91%

89%

87%

85%

83%

81%

79%

77%

75%

92.44W

92.44X

92.44Y

92.44Z

9L.M9N

9O.P9Q

9R.TUV

[\.9]^

_‘.2ab

92.(cid:149)(cid:150)(cid:151)

92.99(cid:130)

92.(cid:131)(cid:132)(cid:133)

94.(cid:134)(cid:135)(cid:136)

9(cid:137).(cid:138)(cid:139)(cid:140)

9(cid:141).(cid:142)(cid:143)(cid:144)

9(cid:145).(cid:146)(cid:147)(cid:148)

9(cid:152).(cid:153)(cid:154)(cid:155)

}~.(cid:127)(cid:128)(cid:129)

(cid:156)9.9(cid:157)(cid:158)

9(cid:159).(cid:160)¡¢

£9.⁄¥ƒ

§9.¤4'

“9.«‹›

ﬁ9.ﬂ(cid:176)–

95%

93%

91%

89%

87%

10

IJ.49K

ij.4kl

mn.4op

qr.4st

uv.4wx

c2.de

f2.gh

85%

y4.z{|

83%

81%

79%

77%

75%

1

2

3

4

5

6

7

8

1

2

3

4

5

6

7

8

1

2

3

4

5

6

7

8

Strategy 1

Strategy 2

(a) Exact Match

Strategy 1

Strategy 2

(b) Averaged Goal Recall

Strategy 1

Strategy 2

(c) BLEU-2

Fig. 7. The performance of Algorithm 2 on different strategies

9(cid:10).04(cid:11)

9(cid:12).(cid:13)4(cid:14)

9(cid:15).(cid:16)(cid:17)(cid:18)

9(cid:6).(cid:7)(cid:8)(cid:9)

9(cid:22).(cid:23)(cid:24)(cid:25)

9(cid:26).(cid:27)4(cid:28)

9(cid:19).9(cid:20)(cid:21)

9J.KLM

9N.OPQ

9R.STU

9V.WXY

9Z.[\]

9^._‘a

9b.42c

9d.efg

100%

95%

90%

85%

80%

75%

70%

65%

60%

55%

50%

92.(cid:211)(cid:212)(cid:213)

92.(cid:214)9(cid:215)

9—.(cid:209)2(cid:210)

9(cid:216).(cid:217)(cid:218)

9(cid:219).(cid:220)(cid:221)(cid:222)

9(cid:223).(cid:224)Æ(cid:226)

94.ª(cid:228)(cid:229)

9(cid:230).2(cid:231)Ł

‰4.2(cid:190)¿

(cid:192)4.`4´

ˆ˜.¯˘˙

”2.»9…

(cid:204)2.˝˛ˇ

•‚.42„

¨(cid:201).˚¸

†‡.·(cid:181)¶

100%

95%

90%

85%

80%

75%

70%

65%

60%

55%

50%

91.28(cid:5)

ØŒ.º(cid:236)(cid:237)

110%

105%

100%

95%

90%

85%

80%

62.97(cid:0)

(cid:254)(cid:255).39%

(cid:1)5.(cid:2)(cid:3)(cid:4)

75%

(cid:29)(cid:30).(cid:31) !

70%

65%

60%

æ(cid:242).99(cid:243)

(cid:238)4.2(cid:239)(cid:240)

(cid:244)ı.(cid:246)(cid:247)ł

øœ.ß(cid:252)(cid:253)

"#.$&’

().*+,

-..4/2

49.4:;

<=.>?@

AB.C2D

EF.GHI

1

2

3

4

5

6

7

8

1

2

3

4

original feature

additional feature

original feature

5

6
additional feature

(a) Exact Match

(b) Averaged Goal Recall

7

8

1

2

3

4

5

6

7

8

original feature

additional feature

(c) BLEU-2

Fig. 8. The performance of SP-MLP on different features

MLP-OF is the worst, followed by the performance of using
knowledge abundance and preference satisfaction separately. SP-
MLP-AF has the best performance, which is 31.71%, 30.53%
and 19.74% higher than OF in EM, AGR and BLEU-2 respec-
tively. Compared with the best Strategy 1, AF has improved
EM, AGR and BLEU-2 by 7.73%, 5.72% and 4.54% respec-
tively. The results show that there is a certain gap between
the strategy learned by neural network based entirely on
the historical data and the operating rules in real world. The
features of knowledge abundance and preference satisfaction are
input into SP-MLP, which reﬂects the strategy of human-
machine dialogue and helps SP-MLP ﬁnd the learning direc-
tion. Compared with the strategy algorithm, neural network
has higher generalization ability and fault tolerance, so SP-
MLP with additional feature performs better. However, the
length of the sequence generated by SP-MLP is constant,
less than or equal to 6. SP-MLP is not enough to deal with
complex real scenes, because it’s a supervised model.

6.2 Real-time Detection

Real-time Detection is divided into Requirement Comple-
tion Estimation and Current Requirement Prediction. As
shown in Table. 3, the former has fewer labels than the
latter, so the accuracy is higher. To evaluate the contribution
of the Encoder (ERNIE) in URef-(b), this paper conducts
a comparative experiment between traditional text classiﬁ-
cation models (TextCNN,LSTM, BiLSTM) and pre-trained
models (BERT, ERNIE). The results show that the pre-
training model is better, and the accuracy of ERNIE is 0.38%
and 0.24% higher than that of BERT in Requirement comple-
tion and Requirement prediction, respectively. PaddlePALM

uses ERNIE to extract semantic features in utterance, and
jointly learns these two sub-tasks in the way of hard param-
eter sharing. Compared with ERNIE, the accuracy of Pad-
dlePALM has increased by 0.52% and 0.21% in Requirement
Completion and Requirement Prediction, respectively. This
proves that there is a potential correlation between these
two sub-tasks, and higher performance of each sub-task can
be obtained through joint learning. The joint model, URef-
(b), can achieve the accuracy scores of 96.67%, 94.13% for
Requirement Completion and Requirement Prediction, an
increase of 1.99%, 2.91% over PaddlePALM respectively, and
an increase of 2.54%, 2.91% over the baseline respectively. It
proves the effectiveness of the sub-nets in URef-(b) after the
fusion of requirements transition feature.

TABLE 3
Real-time Detection Performance

Method

baseline
TextCNN
LSTM
BiLSTM
BERT
ERNIE
PaddlePALM
URef-(b)

Requirement Completion
Acc/F1
94.13/93.28
91.26/90.12
90.27/89.92
92.31/91.34
93.78/91.94
94.16/93.37
94.68/93.39
96.67/95.16

Requirement Prediction
Acc/F1
91.22/90.66
91.17/90.08
91.89/90.15
91.65/90.26
90.77/90.10
91.01/90.71
91.22/90.67
94.13/93.62

TABLE 4
The generated responses for different inputs

11

Truth Response

Generated Response

Current
Requirement

Service Resource
(Subject,Predicate,Object)

User utterance:Yes, idols are excellent. Last Requirement: chitchat about celebrities. Completion: Finished

Would you like to
watch the movie
Kung Fu Panda 3?
This is a Hollywood
cartoon with a
Chinese ﬂavor.

The movie Kung Fu Panda 3 is very good. I recommend it to you.

Since you like him so much, do you want to watch his starring in
Tian Court?

Jay Chou starred in Kung Fu Panda 3.

Yes, he’s also a movie actor

User utterance:Hello, any news about Jay Chou? Last Requirement: -. Completion: -.

Hi, Jay Chou just
released a new song
Waiting For You,
which evoked
the memories of
youth in netizens

Then let me tell you a piece of news about Jay Chou,
he just released a new song Waiting For You.
Would you like to hear about Jay Chou’s news?

Jay Chou released Waiting For You, go and listen.

Hello, there are.

recommend
movie
recommend
movie

-

-

news order

news order

-

-

(Jay Zhou, starring,
Kung Fu Panda 3)

-

(Jay Zhou, starring,
Kung Fu Panda 3)
-

(Jay Zhou, releases,
Waiting For You)
-
(Jay Zhou, releases,
Waiting For You)
-

All sentences are in Chinese and they are translated into English in this table. Each pink row shows the user’s utterance, the last user’s requirement and the
completion of the last requirement. Given one utterance, there are different responses generated with four types of input : “+(-)r” and “+(-)k”. Below the pink row,
the ﬁrst line shows the output of SaRSNet when the current requirement and the service resource are taken as input. The remaining three lines show the output
of MGCG G with different types of input respectively. “-” represents the value of the item is none. The truth response is the real-valued reply to the utterance.
Moreover, the current requirement was predicted by URef and the service resource was selected by Selection Network in SaRSNet, both of which are taken as
input.

6.3 Response Generation

The performance of models on Resource Selection and Re-
sponse Generation are shown in Table. 5. All the models
do not perform well with only user utterance as input on
both two tasks, while improving when the requirement
node is added to the input. This indicates the necessity
of user requirements in the Recommend Response Gen-
eration. Moreover, ”S2S-Attn +r.-k.” performs worse than
”S2S-Attn -r.+k.”, because S2S-Attn does not have the ability
to select service resource. Compared to S2S-Attn, Pointer-
Generator is able to select resources based on copy mech-
anism, which is also introduced into SaRSNet. However,
MGCG G performs better than Pointer-Generator due to its
three encoders. This indicates the effectiveness of a strategy
for encoding the three inputs (utterance, requirement, re-
sources) separately, which is also introduced into SaRSNet.
SaRSNet overall outperforms MGCG G on six metrics of
two tasks, which shows that the modeling approach on
resource selection and text generation is reasonable.

6.4 Case Study

To reveal the impact of URef on SaRSNet in the proposed
Two-phase Requirement Elicitation Method, the generated
responses for different inputs are shown in Table. 4. Each
pink row shows the user’s utterance, the last user’s re-
quirement and the completion of the last requirement. The
current requirement was predicted by URef and the service
resource was selected by Selection Network of SaRSNet.
Given one utterance, there are different responses generated
with four types of input : “+r+k”, “+r-k”, “-r+k”, and “-r-k”.
“+r+k” means take both current requirement and service
resource as input.

• Utterance 1: Yse, idols are excellent? (是啊，偶像就

是优秀啊)
According to the predictions of URef-(a), the last

TABLE 5
Response Generation Performance

Method

S2S-Attn -r.-k.
S2S-Attn +r.-k.
S2S-Attn -r.+k.
S2S-Attn +r.+k.

Pointer-Generator -r.-k.
Pointer-Generator +r.-k.
Pointer-Generator -r.+k.
Pointer-Generator +r.+k.

MGCG G +r.+k.
SaRSNet +r.+k.

Resource Selection
Precision/Recall/F1
0.253/0.205/0.226
0.265/0.219/0.24
0.258/0.212/0.232
0.274/0.235/0.253
0.256/0.211/0.231
0.268/0.225/0.244
0.352/0.321/0.335
0.358/0.329/0.343
0.401/0.377/0.383
0.408/0.388/0.398

Response Generation
BLEU-2/DIST-2/PPL
0.152/0.018/25.82
0.171/0.024/23.08
0.173/0.021/22.85
0.195/0.026/20.13
0.169/0.035/18.57
0.177/0.034/18.11
0.181/0.044/17.82
0.195/0.059/17.24
0.219/0.052/17.69
0.225/0.083/16.12

All models take user’s utterance as original input. “+(-)r.” represents add (not
add) the requirement node to the original input. “+(-)k.” represents add (not
add) the service resource to the original input. For both “S2S-Attn +r.+k.”
and “Pointer-Generator +r.+k.”, this paper concatenates the utterance, the
requirement node, and the related service resources into a piece of text as
input.

user requirement is chitchat about celebrities. Based on
the utterance, URef-(b) supposes it has been ﬁnished
and predicts the user’s current potential requirement
is recommend movie. Thus, the bot should generate
a response grounded on the current requirement
and service resource to initiate a new conversation.
Without the requirement and resource, the generated
response (Yes, he’s also a movie actor. 对的，他还
是 个 演 员) is security and hollow when only the
utterance is taken as input. With the requirement,
the anomaly generated response (Since you like him
so much, do you want to watch his starring in Tian
Court? 既 然 你 这 么 喜 欢 他 ， 那 他 主 演 的 天 庭 外 传
你 要 不 要 看 一 下 ？) is more relevant to the topic.
But the recommended item (Kung Fu Panda 3) is
inconsistent with the truth response (Tian Court)

when inputs is missing the service resource. In fact,
Tian Court have nothing to do with Jay Chou. Lack
of resources leads to factual errors in generated re-
sponse. Although, the recommended item appears
in the generated response (Jay Chou starred in Kung
Fu Panda 3. 周杰伦主演了功夫熊猫3) with the service
resource, the response is only a statement of fact and
lacks the tone of recommendation cause of the miss-
ing of the current requirement. With the requirement
and the resource, SaRSNet generates an appropriate
response (The movie Kung Fu Panda 3 is very good.
I recommend it to you.功夫熊猫3这部电影很叫座，
我推荐给你看看) based on the utterance, which is
closer to the truth response.

• Utterance 2: Hello, any news about Jay Chou? (哈喽,

有关于周杰伦的新闻吗?)
Different from Utterance 1, Utterance 2 is the ﬁrst
sentence initiated by the user in a conversation.
Therefore, the last requirement does not exist. URef-
(b) predicts the current requirement is news order and
the completion is unﬁnished. The generated response
needs to answer the user’s question. As in the case of
Utterance 1, lack of the requirement and the resource
will result in a security, hollow or anomaly response.
Compared with others, the response (Then let me
tell you a piece of news about Jay Chou, he just
released a new song Waiting For You. 那我给你说一
个关于周杰伦的新闻吧，他刚刚发布新歌等你下课。)
generated by SaRSNet based on the requirement and
the resource meets the requirement of the user well.
This further demonstrates that both user requirement
predicted by URef and service resources selected by
SaRSNet are important.

7 CONCLUSION
This paper proposed a novel user requirement elicitation
framework (URef), which is divided into two parts: URef-
(a) and URef-(b). Grounded on user’s preference and Per-
sonal KB, URef-(a) prepares the user’s potential requirement
chain before the conversation. Based on the user’s utterance,
URef-(b) judges whether the last requirement is met and
then predicts user’s requirement in the current-turn. When
the requirement is changed, URef-(a) will re-plan a sequence
with the new starting point.

To deal with Recommend Response Generation task,
a novel Seq2Seq Model (SaRSNet) is proposed. SaRSNet
selects the right service resource based on scoring mecha-
nism, then generates an appropriate response text to user
grounded on user’s utterance, selected service resource and
user’s requirement node.

However, user proﬁle is known beforehand in this work,
not obtained by bots through questioning. Thus, it is inter-
esting to explore how to obtain the user proﬁle during the
conversation. This work will be left as the future work.

ACKNOWLEDGMENT
This work is partially supported by the National
Key
Program of
China(2018YFB1402500), the National Science Foundation
of China(61802089, 61772155, 61832004, 61832014).

Development

Research

and

12

REFERENCES

[1] B. Zhang, Z. Tu, Y. Jiang, S. He, G. Chao, D. Chu, and X. Xu,
“DGPF: A dialogue goal planning framework for cognitive service
conversational bot,” in 2021 IEEE International Conference on Web
Services, ICWS 2021, Chicago, IL, USA, September 5-10, 2021, 2021,
pp. 335–340.

[2] M. Baldauf, R. B ¨osch, C. Frei, F. Hautle, and M. Jenny, “Exploring
requirements and opportunities of conversational user interfaces
for the cognitively impaired,” in Proceedings of the 20th International
Conference on Human-Computer Interaction with Mobile Devices and
Services Adjunct, MobileHCI, 2018, pp. 119–126.

[3] X. Xu, Z. Wang, Z. Tu, D. Chu, and Y. Ye, “E-sbot: A soft service
robot for user-centric smart service delivery,” in 2019 IEEE World
Congress on Services (SERVICES), 2019, pp. 354–355.
S. Louvan and B. Magnini, “Recent neural methods on slot ﬁll-
ing and intent classiﬁcation for task-oriented dialogue systems:
A survey,” in Proceedings of the 28th International Conference on
Computational Linguistics, COLING, 2020, pp. 480–496.

[4]

[5] H. Chen, X. Liu, D. Yin, and J. Tang, “A survey on dialogue
systems: Recent advances and new frontiers,” SIGKDD Explor.,
vol. 19, no. 2, pp. 25–35, 2017.

[6] N. Bahurmuz, R. Alnajim, R. Al-Mutairi, Z. Al-Shingiti, F. Saleem,
and B. Fakieh, “Requirements elicitation techniques in mobile
applications: A systematic literature review,” Int. J. Inf. Technol.
Proj. Manag., vol. 12, no. 3, pp. 1–18, 2021.
S. Xue and F. Ren, “Intent-enhanced attentive bert capsule network
for zero-shot intention detection,” Neurocomputing, vol. 458, pp. 1–
13, 2021.

[7]

[8] Z. Liu, Y. Yan, K. He, S. Liu, H. Xu, and W. Xu, “Self-training
with masked supervised contrastive loss for unknown intents
detection,” in International Joint Conference on Neural Networks,
IJCNN, 2021, pp. 1–7.

[9] P. G. Shivakumar, M. Yang, and P. G. Georgiou, “Spoken language
intent detection using confusion2vec,” in Interspeech 2019, 20th An-
nual Conference of the International Speech Communication Association,
2019, pp. 819–823.

[10] C. Muise, T. Chakraborti, S. Agarwal, O. Bajgar, A. Chaud-
hary, L. A. Lastras-Monta ˜no,
J. Ondrej, M. Vodol´an, and
C. Wiecha, “Planning for goal-oriented dialogue systems,” CoRR,
vol. abs/1910.08137, 2019.

[11] Z. Jiang, J. Ma, J. Lu, G. Yu, Y. Yu, and S. Li, “A general planning-
based framework for goal-driven conversation assistant,” in The
Thirty-Third Conference on Artiﬁcial Intelligence, AAAI, 2019, pp.
9857–9858.

[12] J. Tang, T. Zhao, C. Xiong, X. Liang, E. P. Xing, and Z. Hu,
“Target-guided open-domain conversation,” in Proceedings of the
57th Conference of the Association for Computational Linguistics, ACL,
2019, pp. 5624–5634.

[13] W. Wu, Z. Guo, X. Zhou, H. Wu, X. Zhang, R. Lian, and H. Wang,
“Proactive human-machine conversation with explicit conversa-
tion goal,” in Proceedings of the 57th Conference of the Association for
Computational Linguistics, ACL, 2019, pp. 3794–3804.

[14] Y. Jang, J. Lee, and K. Kim, “Bayes-adaptive monte-carlo planning
and learning for goal-oriented dialogues,” in The Thirty-Fourth
Conference on Artiﬁcial Intelligence, AAAI, 2020, pp. 7994–8001.
[15] J. Xu, H. Wang, Z. Niu, H. Wu, and W. Che, “Knowledge graph
grounded goal planning for open-domain conversation genera-
tion,” in The Thirty-Fourth Conference on Artiﬁcial Intelligence, AAAI,
2020, pp. 9338–9345.

[16] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad,
M. Chenaghlu,
“Deep learning-based text
classiﬁcation: A comprehensive review,” ACM Comput. Surv.,
vol. 54, no. 3, pp. 62:1–62:40, 2021.

and J. Gao,

[17] O. Habimana, Y. Li, R. Li, X. Gu, and G. Yu, “Sentiment analysis
using deep learning approaches: an overview,” Sci. China Inf. Sci.,
vol. 63, no. 1, p. 111102, 2020.

[18] Y. Feng and Y. Cheng, “Short text sentiment analysis based on
multi-channel CNN with multi-head attention mechanism,” IEEE
Access, vol. 9, pp. 19 854–19 863, 2021.

[19] M. P. Akhter, J. Zheng, I. R. Naqvi, M. Abdelmajeed, A. Mehmood,
and M. T. Sadiq, “Document-level text classiﬁcation using single-
layer multisize ﬁlters convolutional neural network,” IEEE Access,
vol. 8, pp. 42 689–42 707, 2020.

[20] D. Wu, L. Ding, F. Lu, and J. Xie, “Slotreﬁne: A fast non-
autoregressive model for joint intent detection and slot ﬁlling,”
in Proceedings of the 2020 Conference on Empirical Methods in Natural

Language Processing, EMNLP 2020, Online, November 16-20, 2020,
2020, pp. 1932–1937.

of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT, 2019, pp. 4171–4186.

13

[21] J. Wang, K. Wei, M. Radfar, W. Zhang, and C. Chung, “Encoding
syntactic knowledge in transformer encoder for intent detection
and slot ﬁlling,” in Thirty-Fifth AAAI Conference on Artiﬁcial Intelli-
gence, 2021, 2021, pp. 13 943–13 951.

[22] K. Xue, Y. Zhou, Z. Ma, T. Ruan, H. Zhang, and P. He, “Fine-tuning
BERT for joint entity and relation extraction in chinese medical
text,” in 2019 IEEE International Conference on Bioinformatics and
Biomedicine, BIBM, 2019, pp. 892–897.

[23] M. Liu, Y. Zhang, W. Li, and D. Ji, “Joint model of entity recogni-
tion and relation extraction with self-attention mechanism,” ACM
Trans. Asian Low Resour. Lang. Inf. Process., vol. 19, no. 4, pp. 59:1–
59:19, 2020.

[24] L. Qin, W. Che, Y. Li, M. Ni, and T. Liu, “Dcr-net: A deep co-
interactive relation network for joint dialog act recognition and
sentiment classiﬁcation,” Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, vol. 34, no. 5, pp. 8665–8672, 2020.

[25] L. Qin, Z. Li, W. Che, M. Ni, and T. Liu, “Co-gat: A co-interactive
graph attention network for joint dialog act recognition and sen-
timent classiﬁcation,” in Thirty-Fifth AAAI Conference on Artiﬁcial
Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Appli-
cations of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium
on Educational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual
Event, February 2-9, 2021, 2021, pp. 13 709–13 717.

[26] M. Crawshaw, “Multi-task learning with deep neural networks: A

survey,” CoRR, vol. abs/2009.09796, 2020.

[27] L. Shang, Z. Lu, and H. Li, “Neural responding machine for
short-text conversation,” in Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Processing of the Asian
Federation of Natural Language Processing, ACL 2015, July 26-31,
2015, Beijing, China, Volume 1: Long Papers.
The Association for
Computer Linguistics, 2015, pp. 1577–1586.

[28] S. Santhanam and S. Shaikh, “A survey of natural

language
generation techniques with a focus on dialogue systems - past,
present and future directions,” ArXiv, vol. abs/1906.00500, 2019.

[29] T. Wen, M. Gasic, N. Mrksic, P. Su, D. Vandyke, and S. J. Young,
“Semantically conditioned lstm-based natural language genera-
tion for spoken dialogue systems,” in Proceedings of the 2015
Conference on Empirical Methods in Natural Language Processing,
EMNLP, 2015, pp. 1711–1721.

[30] T. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P. Su,
D. Vandyke, and S. J. Young, “Multi-domain neural network lan-
guage generation for spoken dialogue systems,” in The Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, 2016, pp. 120–129.
[31] S. Su, K. Lo, Y. T. Yeh, and Y. Chen, “Natural language generation
by hierarchical decoding with linguistic patterns,” in Proceedings
of the 2018 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies,
NAACL-HLT, 2018, pp. 61–66.

[32] S. Bao, H. He, F. Wang, H. Wu, H. Wang, W. Wu, Z. Guo, Z. Liu,
and X. Xu, “PLATO-2: towards building an open-domain chatbot
via curriculum learning,” in Findings of the Association for Compu-
tational Linguistics: ACL/IJCNLP, vol. ACL/IJCNLP 2021, 2021, pp.
2513–2525.

[33] D. Adiwardana, M. Luong, D. R. So, J. Hall, N. Fiedel, R. Thop-
pilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, and Q. V.
Le, “Towards a human-like open-domain chatbot,” CoRR, vol.
abs/2001.09977, 2020.

[34] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y. Liu,
J. Xu, M. Ott, E. M. Smith, Y. Boureau, and J. Weston, “Recipes
for building an open-domain chatbot,” in Proceedings of the 16th
Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, EACL, 2021, pp. 300–325.

[35] S. Montagna, S. Mariani, E. Gamberini, A. Ricci, and F. Zambonelli,
“Complementing agents with cognitive services: A case study in
healthcare,” J. Medical Syst., vol. 44, no. 10, p. 188, 2020.

[36] Y. Kim, “Convolutional neural networks for sentence classiﬁca-
tion,” in Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing, EMNLP, 2014, pp. 1746–1751.

[37] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”

Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997.

[38] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training
of deep bidirectional transformers for language understanding,”
in Proceedings of the 2019 Conference of the North American Chapter

[39] Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu,
H. Tian, and H. Wu, “ERNIE: enhanced representation through
knowledge integration,” CoRR, vol. abs/1904.09223, 2019.

[40] H.

Li,

“D-net

in

emnlp2019

mrqa

track,”

https://github.com/PaddlePaddle/Research/tree/master/NLP/MRQA2019-D-NET,
accessed Feb 20, 2020.

[41] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation
by jointly learning to align and translate,” in 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA,
USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[42] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence
learning with neural networks,” in Advances in Neural Information
Processing Systems 27: Annual Conference on Neural Information Pro-
cessing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada,
2014, pp. 3104–3112.

[43] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summariza-
tion with pointer-generator networks,” in Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers,
2017, pp. 1073–1083.

[44] Z. Liu, H. Wang, Z. Niu, H. Wu, W. Che, and T. Liu, “Towards con-
versational recommendation over multi-type dialogs,” in Proceed-
ings of the 58th Annual Meeting of the Association for Computational
Linguistics, ACL, 2020, pp. 1036–1049.

[45] K. Papineni, S. Roukos, T. Ward, and W. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of the
40th Annual Meeting of the Association for Computational Linguistics,
July 6-12, 2002, Philadelphia, PA, USA, 2002, pp. 311–318.

[46] M. Popel and D. Marecek, “Perplexity of n-gram and dependency
language models,” in Text, Speech and Dialogue, 13th International
Conference, TSD 2010, Brno, Czech Republic, September 6-10, 2010.
Proceedings, ser. Lecture Notes in Computer Science, vol. 6231,
2010, pp. 173–180.

[47] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, “A diversity-
promoting objective function for neural conversation models,” in
NAACL HLT 2016, The 2016 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, San Diego California, USA, June 12-17, 2016, 2016, pp.
110–119.

[48] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” in 3rd International Conference on Learning Representations,
ICLR, 2015.

Bolin Zhang received the MS degree in Com-
puter Science from the Harbin Institute of Tech-
nology (HIT), China. He is currently an Ph.D
at the ICES Lab of HIT. His research interests
include cognitive service, software service bot,
and dialogue system.

Zhiying Tu received the MS degree in Computer
Science from the Harbin Institute of Technol-
ogy (HIT), China. He received a Ph.D degree
in Computer Integrated Manufacturing (Produc-
tique) from the University of Bordeaux, France.
Since 2013, He began to work at HIT. His re-
search interests include Service Computing, En-
terprise Interoperability, and Cognitive Comput-
ing. He has 20 publications as edited books
and proceedings, refereed book chapters,and
refereed technical papers in journals and con-

ferences. He is the member of IEEE Computer Society.

14

Yunzhe Xu received the BCs degree in Com-
puter Science from the Harbin Institute of Tech-
nology (HIT), China. His research interests in-
clude Service Computing, text generation and
dialogue system.

Dianhui Chu received the MS and PhD degrees
in Computer Science from the Harbin Institute
of Technology (HIT), China. He is a professor
and dean at School of Computer Science and
Technology in HIT. His research interests include
Service Computing.

Xiaofei Xu received the MS and PhD degrees
in Computer Science from the Harbin Institute
of Technology (HIT), China. He is a professor
at School of Computer Science and Technology,
and vice president of HIT. His research interests
include enterprise intelligent computing, ser-
vices computing, Internet of services, and data
mining. He is the associate chair of IFIP TC5
WG5.8, chair of INTEROP-VLab China Pole, fel-
low of China Computer Federation (CCF), and
the vice director of the technical committee of
service computing of CCF. He is the author of more than 300 publica-
tions. He is member of the IEEE and ACM.

