Universally Adaptive Cross-Platform Reinforcement
Learning Testing via GUI Image Understanding

Shengcheng Yu, Chunrong Fang, Yulei Liu, Ziqian Zhang, Yexiao Yun, Xin Li, Zhenyu Chen
State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
∗corresponding author: fangchunrong@nju.edu.cn

2
2
0
2

g
u
A
9
1

]
E
S
.
s
c
[

1
v
6
1
1
9
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—With the rapid development of the Internet, more
and more applications (app) are playing an important role in
various aspects of the world. Among all apps, mobile apps and
web apps are dominant in people’s daily life and all industries.
In order to tackle the challenges in ensuring the app quality,
many approaches have been adopted to improve app GUI testing,
including random technologies, model-based technologies, etc.
However, existing approaches are still insufﬁcient in reaching
high code coverage, constructing high quality models, and
achieving generalizability. Besides, current approaches is heavily
dependent on the execution platforms (i.e., Android, Web). Apps
of distinct platforms share commonalities in GUI design, which
inspires us to propose a platform-independent approach with the
development of computer vision algorithms.

In this paper, we propose UNIRLTEST. It is a reinforcement
learning based approach utilizing a universal framework with
computer vision algorithms to conduct automated testing on
apps from different platforms. UNIRLTEST extracts the GUI
widgets from GUI pages and characterizes the GUI correspond-
ing layouts, embedding the GUI pages as states. UNIRLTEST
explores apps with the guidance of a novelly designed curiosity-
driven strategy, which uses a Q-network to estimate the values
of speciﬁc states and actions to encourage more exploration
in uncovered pages without platform dependency. The state
embedding similarity is used to calculate the rewards of each
exploration step. We conduct an empirical study on 20 mobile
apps and 5 web apps, and the results show that UNIRLTEST can
perform better than the baselines, especially in the exploration
of new states.

Index Terms—Software Testing, Platform-Independent Testing,

Reinforcement Learning, GUI Image Understanding

I. INTRODUCTION

Internet has been signiﬁcantly developing. As a conse-
i.e., applications (app)1, are becoming
quence, softwares,
dominant in people’s daily life and all kind of industries.
Among the apps of different platforms, mobile apps and
web apps are the representatives due to the popularization
of internet and mobile devices [1]. Testing is an efﬁcacious
method to ensure the software quality. Current approaches
utilize different strategies to automatically test the apps [2],
but such approaches still leave quite a room for improvement.
Manually written test scripts based on speciﬁc frameworks
[3] [4] are commonly used in app testing [5] [6] [7] [8]. Testers
are required to identify the target widgets and assign the test
events. However, the manual work is quite a waste of time and
human resources, causing great burden on app developers. The

1“Software” and “application” are used interchangeably in this paper.

involvement of manual efforts of unprofessional testers makes
the test scripts’ quality ranges widely based on the expertise.
With the aim of improving the testing efﬁciency, one most
widely used automated testing strategy is the random-based
exploration [9] [10]. Such a strategy can generate pseudo-
random test events to fuzz the applications. Due to the random
feature, these approaches have high efﬁciency in generating
test events. However, most of the generated test events are
invalid, which may not trigger interactive response nor lead
to increment
in code coverage. Among the random-based
approaches, Monkey [9] is a representative tool with high
efﬁciency. However, Monkey may generate test events like
clicking on a blank area, inputing texts into an ImageView
widget, etc. In addition, the explored space will fall into local
optimum and some speciﬁc space might be neglected, reducing
the test effectiveness.

To achieve higher exploring effectiveness, another group
of automated approaches adopt the model-based strategy [11]
[12] [13] [14] [15]. The model-based strategy constructs a
model for the app under test with dynamic or static app
analysis technologies. There are two main factors to determine
the effectiveness of the model-based approach, the constructed
model itself and the exploration strategy. Due to the limitations
of existing app analysis technologies, the generated model
cannot reach high code coverage, thus imposing restrictions to
the model-based strategy. Besides, speciﬁc app states require
speciﬁc domain knowledge to generate valid inputs, which is
tough for existing exploration strategies of automated testing.
Learning-based technologies have increasing adaptive appli-
cations in different ﬁelds of assisting automated testing [16]
[17] [18] [19]. Reinforcement learning (RL) [20] is an effective
algorithm to help exploring the app states. Speciﬁcally, Q-
learning [21] is a widely used reinforcement learning algo-
rithm, which beneﬁts from both random-based and model-
based strategy. RL-based automated testing approaches are
effective approaches for app testing.

However, it is still a tricky problem to effectively ensure
the app quality due to its large exploration space. Current
RL-based approaches are still faced with many problems.
Current approaches rely heavily on the platform features or
interfaces, which shackles these approaches from universally
usage across different platforms. For example, Q-testing [17]
utilizes the layout ﬁles of Android apps as app states; We-
bExplor [19] utilizes the front page HTML code as the app
states. However, according to our investigation and existing

 
 
 
 
 
 
work [22] [8], we ﬁnd that apps of different platforms adopt
quite similar widget styles2 (e.g., Button, TextField,
Switch, ImageView, CheckBox, etc.) and layouts (e.g.,
ListView, RecycleView, etc.). Therefore, such a situation
inspires us that
the GUI images can be utilized as app
states to guide the RL-based strategy to make the exploration
universally adaptive to apps of different platforms.

Another problem of current automated testing approaches
is the widget identiﬁcation. Current approaches rely on the
layout ﬁles to identify the target widgets [17]. However, most
apps adopt a hybrid version and embed Canvas elements or
customized elements into the app pages. Such GUI widgets
cannot be identiﬁed with existing approaches. As is often the
case, a group of widgets will be considered as only one single
widget, which greatly affects the exploration effectiveness.

In order to dispose the aforementioned challenges, we
propose a novel approach, named UNIRLTEST,
to real-
ize Universal platform-independent Reinforcement Learning
Testing via image understanding. We adopt a deep Q-network
(DQN) [23] to guide UNIRLTEST to explore more app states.
In this paper, we propose a novel algorithm to characterize
the app state from the perspective of GUI to identify the
similarity of different pages. Speciﬁcally, we combine the
widget feature and the structure feature. Widget feature starts
from a microscopic perspective and the structure feature views
the app page from the macroscopic perspective. For the widget
feature, we extract all the widget images and their coordinates
with the computer vision algorithms. The embedded widget
feature are composed of three parts. First, the widget image
is embedded to a vector with a convolutional neural network
(CNN); second, according to the widget coordinate, we change
the corresponding pixels of the app page to black and other
pixels to white, which is then fed into a CNN to generate a vec-
tor; the third part of the widget feature is a 14-length3 one-hot
vector. Then, the three vectors are concatenated to represent a
widget. For all the widgets in a page, we get the average vector.
For the structure feature, we use a tree structure to represent
the layout relationships of all the widgets, and the concrete
widget images are neglected. The trees are transformed to
strings with curly braces to keep the relative relationships.
The strings are fed into a recurrent neural network (RNN) to
obtain a vector. The widget feature and the structure feature
are concatenated to accomplish the page embedding.

During the exploring process, we adopt the curiosity-driven
[24] strategy in the DQN algorithm. Speciﬁcally, UNIRLTEST
designs the Q-network to obtain the state-action value func-
tion, which assigns a value to each given state-action pair, such
a value would identify the optimal state-action pair, and assign
a highest probability to the Boltzmann strategy [25]. The ﬁnal
determination of which action to be executed is based on
the probability. UNIRLTEST maintains a memory that records
all the explored states. The design of the reward function is
based on the exploration rate of one speciﬁc app page and the

2Customized rendering is not considered.
314 most common widget types are considered in this paper.

page transition counter. The exploration rate refers to what
percentage of actions are explored in one speciﬁc app page,
and the page transition counter refers to refers to how many
times the exploration is transited to this page. Among the
reward calculation, the “page” is important. The similarity of
between pages signiﬁcantly inﬂuence the reward calculation.

We declare the following noteworthy contributions:
• We introduce a universal platform-independent reinforce-
ment learning testing approach via image understanding.
• We propose a novel algorithm to represent the app pages
with the widget feature and the structure feature, which
helps characterize the States in the RL model.

• We implement a tool and conduct an empirical evaluation
to the effectiveness of UNIRLTEST on both mobile apps
and web apps, which shows the outstanding performance
of UNIRLTEST over the representative baselines.

More information and the reproduction package is

available on https://sites.google.com/view/unirltest.

II. BACKGROUND & MOTIVATION

In this section, we present the background and the motiva-
tion of this work, including the preliminaries of reinforcement
learning, and the limitations of current approaches. We present
the commonalities of mobile apps and web apps as the
intuition to complete this paper.

A. Deep Reinforcement Learning

Fig. 1. Markov Decision Process (MDP)

Reinforcement

learning is a group of machine learning
algorithms that make decisions and change the behaviors based
on environmental feedback to maximize expected beneﬁts
[20]. Reinforcement learning can be modeled as a Markov
decision process [26] (MDP) as shown in Fig. 1. The MDP
can be formalized as a four-tuple (cid:104)S, A, R, P(cid:105), where S refers
to the set of all the states, A refers to the set of all the
actions, R refers to the reward function: S × A → R, and
P refers to the transition probability function: P (s, a, s(cid:48)) =
P (st+1 = s(cid:48) | st = s, at = a). Generally speaking, at each
timestamp t, the agent senses the state st from the environ-
ment, and then will select an action at based on a speciﬁc

EnvironmentAnalysisAgentExecutionAgentStateTransitionStateActionRewardpolicy π (a|s). As the at is executed, a state transition to a
new state st+1 will happen, and a reward r should be assigned.
The object of RL is to train such an agent to maximize the
expected discounted cumulative reward.

When traditional tabular RL algorithms are faced with the
high-dimension or unbounded state and action spaces, it is
impractical and unstable to learn an efﬁcient policy merely
based on such tabular algorithms. Therefore, a group of more
robust RL algorithms combining the deep neural networks,
named deep RL, is proposed to overcome the shortcomings.
Deep Q-network (DQN) [23] is one model-free representative.
In DQN, a neural network is used to approximate the state-
action value function Qπ. The neural network is trained to
estimate the expected discounted cumulative reward under the
optimal policy π∗. With the combination of reinforcement
learning and deep learning, i.e., the DQN, it will become more
effective to explore the environment.

B. Commonalities of Mobile App & Web App

In order to apply the UNIRLTEST on different platforms,
including web platform and mobile platform, one signiﬁcant
prerequisite is the commonalities in app GUI of mobile apps
and web apps. The intuition of UNIRLTEST is to extract all
the widgets from the app pages for the exploration of the
RL algorithm (details in Section III-A). The app pages are
composed of the widgets. Besides, the widgets used in mobile
apps4 and in web apps5 can be mapped. We list several widely
used widgets and their mapping relationships. As is depicted
in TABLE I, the Android widget classes correspond to speciﬁc
HTML tags. From the visual aspect, the widget pairs also share
universal appearances.

TABLE I
COMMONALITIES OF MOBILE APP & WEB APP

Android Class

Web (HTML) Tag

Button
TextView
EditText
CheckBox.
ImageButton
ImageView
RadioButton
Switch
SeekBar
ProgressBar

<button>
<p>
<input>
<input type=“checkbox”>
<input type=“image”>
<img>
<input type=“radio”>
<input type=“checkbox”>
<input type=“range”>
<progress>

Moreover, corresponding widgets share similar applicable
actions (details in Section III-B). For example, for a clickable
Button in a mobile app or a web app, the possible actions
are click, right-click, etc.; for an editable TextField (or
<input> tag), the possible actions are input, long-click, etc.
Therefore, considering the above commonalities between
widgets of mobile apps and web apps, and starting both from
the visual aspect and the functionality correspondence, we can
develop an effective approach that utilizes the GUI information

with the help of CV technologies to guide the exploration of
the reinforcement learning algorithm.

C. Limitations of Current Approaches

Fig. 2. An example of the limitations of current approaches (mobile app)

Fig. 3. An example of the limitations of current approaches (web app)

For current approaches that apply RL algorithms in app
testing, like [17] [18] [19] [27], almost all of them depend
on speciﬁc tools to capture the GUI structures. For example,
UIAutomator6 is a tool that can extract the mobile app page
elements and the structure. WebExplor obtains the HTML
elements from the browser. However, one prominent problem
is that more and more pages adopt a hybrid version or have a
highly customized widget due to the deeper customization of
app GUIs. Under such circumstances, these widgets are hardly
possible to be obtained. Fig. 2 shows an example of a mobile
app. In this Rule app, the rule is a customized widget. The
blue-edge widget can be dragged, and the gear-like button can
be clicked. However, owing that this widget is a customized
one, the rule is recognized as a whole widget in the class
android.view.View. Also, the blue-edge widget and the
gear-like button cannot be obtained, which may negatively
affects the app exploration effectiveness.

Fig. 3 gives an example from the web version of Google
trigger

Maps. The red-circled texts are clickable and will

4https://developer.android.com/reference/android/view/View
5https://www.w3.org/

6https://developer.android.com/training/testing/other-components/

ui-automator

NodeDetailNAFtrueindex0textresource-idcom.rule.csw:id/surfaceView_mainclassandroid.view.Viewpackagecom.ruler.cswcontent-desccheckablefalsecheckedfalseclickabletrueenabledtruefocusedtruescrollablefalselong-clickablefalsebounds[75,0][2265,1080](0) View [75,0][2265,1080] Fig. 4. UNIRLTEST Framework

an information page, showing the details about the location.
However, texts of location names are not accessible from the
HTML layout structure ﬁles given that the whole page is a
simple Canvas widget from the perspective of the HTML ﬁles.

III. APPROACH

In this section, we present

the detailed design of the
UNIRLTEST. The app GUI screenshots are processed with
CV algorithms to embed the app page into a vector so as
to represent the states (Fig. 4 (cid:202)). Based on the extracted
widgets during the app GUI processing, candidate actions are
generated and assigned to the corresponding widgets (Fig. 4
(cid:203)). Then, UNIRLTEST calculates the state-action values with
the Q-network and assigns different probabilities to the actions
according to the values (Fig. 4 (cid:204)). The Q-network is trained
with the data from explored states, which are stored in
the memory (Fig. 4 (cid:205)) as the curiosity-driven strategy will
encourage UNIRLTEST to explore different states based on
the reward calculation (Fig. 4 (cid:206)). In the following sections,
we will give a detailed depiction to the UNIRLTEST design.

A. App GUI Processing

1) GUI Widget Extraction & Layout Characterization:
The ﬁrst step of GUI processing is to extract GUI widgets
from the app screenshots. We adopt the classic computer vision

algorithm, the Canny [28] edge detection, which is widely used
in widget recognition in GUI analysis [22], to our approach.
Targeting at one speciﬁc app screenshot of being turned
grayscale, the Canny algorithm identiﬁes the edges of widgets.
Then the edges will be extracted to form an image of binary
thresholding, where edges are represented with white pixels,
and non-edges with black pixels. Afterwards, the edges of
widgets are obtained. However, there are still some noise data
to be processed, e.g., the mis-recognized subtle areas. Such
noise data will not be assigned with interactable attributes, so
we have to eliminate such areas to avoid misleading during
the RL process. The accordance is the size ratio of the widget
to the whole screenshot, and we set a threshold to eliminate
the subtle mis-recognized areas.

After obtaining the widgets from the app screenshots,
the layout of the app screenshots should be characterized,
which illustrates the overall structure of the screenshot and
the relative positions among all widgets. Beyond concrete
widgets, layout characterization effectively reﬂects the whole
panorama of the GUI structure, which is also important in
the state determination. During the GUI design, the designers
tend to arrange the widgets with close semantics together,
and arrange widgets with different ones apart. Therefore, we
characterize the screenshot layout with a three-level hierarchy,

WidgetExtractionWidgetCoordinateTypeWidget EmbeddingLayout EmbeddingStateEmbeddingScreenshotCaptureAction Set GenerationActionEmbeddingAppSimilarityCalculationRewardExplorationMemoryQ Network(State-Action Value Network)Action DecisionAction ExecutionState Transition❶❷❺❹❸the Group, the Line, and the Column [8]. Group and
Line are horizontal operations, and Column is the vertical
operation. Group is a rough characterization and the design
intuition is that some widgets should be relatively close to each
other. Widgets within the same group may occupy different
Lines due to their sizes. We further divide the widgets within
each Group into different Lines and different Columns
according to the coordinates. Obtaining Columns in different
Lines are conducted respectively, so different Lines may
include different amounts of Columns7. Then, the app GUI
is structured to a widget tree.

2) Widget Embedding: All single widgets view the app
GUI from a microscopic perspective. We totally consider three
attributes of a widget image for each widget in the app GUI:
the widget image, the widget location, and the widget type.

Widget Image refers to the widget extracted from the GUI
screenshot. For each widget, we feed the widget image into
a convolutional neural network (CNN) to embed it into a
4096-dimension vector. Speciﬁcally, the CNN we use here is
a VGG-16 model [29]. The model is a pre-trained model in
[1] with the purpose of classifying widget types. The model
is trained with a dataset containing over 36k widget images
manually collected, labeled and veriﬁed by the authors. In
order to obtain the embedding vector, we delete the last
layer of the model, which outputs the probabilities indicating
the types. The last but one layer is a FullyConnected
layer that contains 4096 neurons, and it will output a 4096-
dimension vector that represents the widget image.

Widget Location indicates the location of the widget within
the page. Traditionally, in order to represent the coordinate of
a widget, common practices use four integers: the coordinate
of the left-upper corner and the width and height of the
widget. However, if we follow this practice, the weight of
the embedded widget is much lower than the widget image,
which contradicts to the actual situations. Therefore, we use
a black-and-white GUI image to represent the widget location
information. We turn the widget part of a GUI image into
all-white and other part all-black. Then, the transformed GUI
image are fed in to the VGG-16 model mentioned above to
obtain a 4096-dimension vector.

Widget Type is identiﬁed by a convolutional neural network
(CNN). Widget type has a close relationship to the applicable
actions. Therefore, the type attribute will affect the exploration.
Thus, we consider embedding the widget type. As mentioned
in the Widget Image part, the VGG-16 model is trained with
the purpose of identifying the widget type. The model covers
14 widely used widgets [1] which are applicable for different
platforms. Therefore, we use a 14-dimension one-hot vector
to represent the widget type. Compared with the widget image
and the widget location, the importance of widget type is
weaker, so we do not take actions to expand the dimensions.
After obtaining the three kind of the widget embedding
results, we concatenate the three vectors to form the wid-
get representation vector, with a length of 8206 dimensions

7The operation ﬁgure is presented in our online package due to space limit.

(2 × 4096 + 14). The 8206-dimension vector is the embedding
representation of one single widget on the app GUI.

3) Layout Embedding: The GUI layout views the app
GUI from a macroscopic perspective. As described in Sec-
tion III-A1, we characterize the layout with a hierarchy
structure. The hierarchy structure can be easily transformed
into a tree structure as shown in Fig. 5. The left part of Fig. 5
shows a simpliﬁed example of layout characterization, where
red lines identify the Group, blue lines identify the Line, and
the green lines identify the Column. In the right part of Fig. 5,
the ﬁrst level represents the whole app screenshot, the second
level represents the Groups, the third level represents the
Lines, and the fourth level represents the Columns, which
are actually the real widgets in the app screenshot. Due to the
special design of the layout characterization, the transformed
tree structure will deﬁnitely have four levels.

Fig. 5. Layout Characterization of App GUI

To further make use of the layout tree, we further transform
it into a string. One common practice is to use the curly
braces to indicate the level relationship. The elements within
the curly braces refer to the children nodes of the element
before the curly brace. Strings with curly braces can be one-
to-one mapped to the speciﬁc GUI trees.

Then, we embed the GUI layout with the LSTM model [30].
In order to train the LSTM model, we randomly generate 100
different GUI layout strings, and the dataset size is 10000
(100 × 100) pairs. The dataset is divided into training set,
validation set and test set at the ratio of 7:1:2, following the
common practice. The purpose of the LSTM is to ﬁt the tree
edit distance [31] [32]. The output of the model is a 512-
dimension vector after a FullyConnected layer, and the
generated vector represents the embedded GUI layout.

4) State Representation: The widget embedding results
and the layout embedding results are merged to obtain the
ﬁnal state (app page) representation. For each widget in the
app screenshot, we obtain a 8206-dimension vector. However,
the widget amount of different GUI screenshots are different,
so if we directly concatenate the vectors, we may get variable-
length vectors as widget numbers are different, which do not
ﬁt the requirements of the input of the RL framework [18].
Therefore, we conduct an average calculation to all the widget
embedding vectors, and obtain one single 8206-dimension
vector that represents all the widgets. Then, we concatenate the
widget embedding vector with the layout embedding vector to
obtain the ﬁnal app page vector, and view it as a corresponding

W1W2W3W5W6W4rootABCDEFW1W2W3W4.1W5W6W4.2root{A{C{W1 W2} D{W3}} B{E{W4.1 W5} F{W4.2 W6}}}state of the page in the RL framework. The length of the ﬁnal
vector representing the app GUI is 8718 (2×4096+14+512).

B. Action Generation

Actions are another signiﬁcant part of the whole reinforce-
ment learning framework. In the automated software testing
scenario, the action is limited. As shown in TABLE II, we list
all the actions8 that are considered in UNIRLTEST.

In software testing, the actions can be divided into three
main categories: the widget actions, the page actions, and the
system actions. Different actions may have different targets
and parameters. In this paper, we consider in total 17 different
actions. Some actions apply both mobile and web platforms,
and some are unique on a speciﬁc platform.

TABLE II
ACTIONS FOR UNIRLTEST

Platform

Widget
Actions

Page
Actions

System
Actions

Both

click,
input,
drag,
double click

Mobile

long click

Web

mid click,
right click

return,
back switch,
access grant,
access deny
network switch

phone interrupt

swipe,
split screen

orientation
switch

window size

——

Widget actions are actions that are directly applied to
speciﬁc widgets. We consider the widget image, the widget
location and the widget type presented in Section III-A2 as
the target widget embedding feature. Whether a widget action
is applicable on the current app page is determined by the
widget type (inferred by the CNN model in Section III-A2).
One widget may be applicable for different widget actions.

Page actions are those actions that verb the current app
GUI page. The page actions mostly involve the page size
or orientation changing. For example, the orientation switch
action will change the app orientation, the window size action
refers to the window size change. Besides, in order to simplify
the action type, we ﬁx some actions. For the swipe action, we
only consider the up and down swipe directions. Therefore, we
set a parameter of page actions as a number that identifying
the size change ratio, or the orientations.

System actions refer to the actions that applied to the operat-
ing system. Different from the widget and page actions, system
actions are not directly related to the app GUI. Therefore, no
extra information is required to invoke the system actions.

Additionally, the actions are embedded to ﬁt in the Q-
network. Based on the analysis to the three categories of
actions, action embedding is composed of three parts, the
action itself, the target widget, and the parameter information.
For the action itself, we use a 17-dimension one-hot vector
to represent the action type. For the target widget, we follow

8“Event” and “action” are equivalent in this paper.

the processing in Section III-A2, and use a 8206-dimension
vector to represent the target widget. For the window size
parameter, we take the number parameter as a one-dimension
vector to represent it. The three part vectors are concatenated
to obtain the action embedding vector. For some actions,
the target widget and the parameter might be not applicable,
and in such cases, we will pad the vector with “0”s in
the corresponding places. The action embedding vector is in
accordance with the RL framework of UNIRLTEST to make
the action selection decisions.

C. Exploration Policy Execution

be granted to the embedded current state and all the appli-
cable actions, UNIRLTEST’s exploration policy calculates the
values of such actions under such a state, and then guides the
appropriate action to execute, with the purpose of exploring as
many un-explored states as possible. The exploration policy is
a function that gives the execution probability distribution of
all possible actions according to the current state.

For the automated software testing, the content changes of
every accessible app page or widget may cause the app state
changes, so the state space to explore is huge. Moreover,
the executable action set of different states are different,
and the action space of the whole app to explore is huge.
Therefore, traditional RL algorithms using the Q-Table will
have difﬁculty in processing such a situation. Considering
the robust capability of deep learning models, we adopt a
deep neural network in UNIRLTEST as the key strategy for
exploring the app under test.

For a single step in the exploration (viewed as a test event),
UNIRLTEST ﬁrst merges the embedded state and all
the
embedded actions to calculate the Q-value of each action with
the Q-network. We add a weight to the Q-values all the actions
in order to reduce the probability of system actions, we set
the weight of widget and page actions as 1, and the weight of
system actions as 0.5. With the weighed Q-values obtained,
we use the SoftMax function to calculate the execution
probability of each action, and determine an action to execute.
1) Q-Network: The Q-network is a fully connected neural
network with four hidden layers, and each layer has 512
dimensions. The reason why using the fully connected neural
network instead of a CNN model or RNN model
is that
the ﬁtting speed is much faster. During the exploration of
the RL framework, the Q-network needs to be keep trained
with constantly refreshed training data, so the ﬁtting speed is
important to the efﬁciency. The fully connected neural network
is capable of achieving a balance between the high ﬁtting
speed and the ﬁtting accuracy.

The input of the Q-network is the (cid:104)state, action(cid:105) pair,
which is the concatenated result of the embedded state and
the embedded action. The output of the Q-network is a real
number, indicating the Q-value of the given (cid:104)state, action(cid:105)
pair. The larger the Q-value is, the higher the probability
of exploring new states will be. The loss function of the
model is deﬁned as the mean squared error of the output
Q-value and the expected Q-value (calculated as Equation 2

in Section III-D). When exploring of the app under test,
UNIRLTEST constantly re-trains the Q-network to make the
ﬁtting more accurate in evaluating the values of executing one
speciﬁc action. The training process of the Q-network involves
the state memory, and is illustrated in Section III-D.

2) Randomization Strategy: We will get

the execution
probability of all the actions under the current state with the
values of the (cid:104)state, action(cid:105) pairs. Generally speaking, it is
optimal to execute the action with the highest Q-value, but
if the optimal actions are always executed, the framework
keeps exploring with existing knowledge, making it hard to
explore unknown states. This is the so-called “exploration-
exploitation dilemma” [33] in RL. For UNIRLTEST, we use
the Boltzmann strategy [25] to alleviate the problem. Generally
speaking, the Boltzmann strategy assigns a probability to each
action according to the Q-value with the Equation 1. Then, the
probabilities will directly affect which action is to be executed.

P (x|X) =

ex
Σa∈X ea

(1)

D. Exploration Memory

State memory records every state transition and the corre-
sponding reward, which is the data source for the Q-network
training. The rewards, together with the state memory, will
have effect on the exploration policy, and then guide the
refreshing of the exploration policy.

In the memory,

the following data of timestamp t is
recorded: the state transition (cid:104)st, at, st+1(cid:105), the reward rt, and
the action set At+1 of state st+1. st, at, and rt represent
the state embedding, action embedding, and the reward at the
timestamp t, respectively. At+1 is the executable action set at
the timestamp t + 1. Qtarget is the expected Q-value, and is
calculated with the Equation 2.

deﬁne the exploration rate of state st+1 is et+1 = nt+1
, and
mt+1
based on the exploration rate, the reward is deﬁned as follows:

rt =

1 − et+1√
Nt

=

1 − nt+1
mt+1√
Nt

(3)

2) Page Similarity: Page similarity determines the transi-
tion times and helps determine the exploration rate. According
to our introduction in Section III-A, we calculate the page sim-
ilarity with the widget extraction and layout characterization.
With regard to the widget similarity, for all the widgets
in page A (pA) and page B (pB), we obtain the one-to-one
matched widget pairs if the widgets’ Interaction-over-Union
(IoU) value [22] is over the speciﬁc threshold (some widgets
may do not have matching widgets, and such widgets are
neglected). For each matched widget pair, we calculate the
distance with the embedded widget vector (Section III-A2)
through the Euclidean distance. We deﬁne the average distance
of all widget pairs as d(pA, pB), and the widget similarity is
simw(pA, pB) = 1 − d(pA, pB), ranging between [0, 1].

The layout of a GUI page is represented by a string
composed of widget nodes and curly braces (Fig. 5), so we use
the tree edit distance [31] [32] to calculate the layout similarity
between page A pA and page B pB:

siml(pA, pB) = 1 −

d(A, B)
max(nA, nB)

(4)

where d(A, B) refers to the tree edit distance, and nA,
nB refer to the node numbers of pa and pb. Speciﬁcally,
max(nA, nB) actually indicates the theoretical maximum
value of d(A, B). siml(pA, pB) ranges between [0, 1].

The page similarity is the weighed sum of widget and layout
similarities. We initially set the 0.5-0.5 weight. We also use a
threshold to determine whether two pages are the same page.

Qtarget
t

= rt + γmaxa∈At+1Q (st+1, a)

(2)

IV. EXPERIMENT

γ is the decay rate, and a larger γ means that a reward
of a more recent reward is more important. With the further
exploration of UNIRLTEST, more and more data are recorded.
Considering the efﬁciency problem, we cannot take all the
data into the training. We have to select part of the data
as the training data. In order to instruct the Q-network to
tendentiously learning from recent state transitions, when
selecting n data for training, we ﬁrst use the most recent n
2
data, and randomly select another n
2 data from the rest data.

E. Reward Calculation

For

each

Function:

1) Reward

transition
(cid:104)st, at, st+1(cid:105), UNIRLTEST assigns an appropriate reward.
A higher reward means a more signiﬁcant transition, thus
the RL framework learns the state and action values. A
well-designed reward function guides UNIRLTEST to explore
more state space, and avoid being stuck in local optimum.

state

At the timestamp t, the state transition is (cid:104)st, at, st+1(cid:105), the
transition times of st is Nt, the executable action number in
st+1 is mt+1, and the executed action number is nt+1. We

A. Experiment Setting

1) Research Question: We set three research questions to
evaluate the UNIRLTEST, and the ﬁrst two questions focus
on the code coverage and the cross code coverage analysis on
mobile apps and web apps, respectively. The third question
has a in-depth insight to the advantages of UNIRLTEST.

• RQ1 (Mobile App Effectiveness): How effective can

UNIRLTEST explore the mobile apps?

• RQ2 (Web App Effectiveness): How effective can

UNIRLTEST explore the web apps?

• RQ3 (Advantage Analysis): How can UNIRLTEST out-

perform the baselines?

2) Experiment Subjects: For mobile apps, we collect the
apps from existing work, including [13] [15] [17]. For web
apps, we use the apps in existing work like [19]. However, due
to the maintenance problem of the apps, some apps cannot be
compiled or instrumented, so we have to select the usable ones.
We respectively instrument the mobile apps with Jacoco9 and

9https://www.jacoco.org/jacoco/

TABLE III
CODE COVERAGE RESULTS

App

1.5 hour coverage

500 action coverage

Line Coverage

Branch Coverage

Line Coverage

Branch Coverage

M

Q/W

U

M

Q/W

U

M

Q/W

U

M

Q/W

U

M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
M11
M12
M13
M14
M15
M16
M17
M18
M19
M20

26.29%
28.64%
44.02%
10.67%
57.39%
37.79%
2.78%
33.98%
20.14%
21.82%
29.37%
31.68%
46.86%
20.56%
13.98%
75.44%
28.10%
49.54%
6.10%
29.53%

8.67%
27.96%
13.59%
11.81%
57.39%
36.45%
1.23%
15.08%
15.90%
26.54%
38.94%
26.16%
22.63%
14.89%
10.17%
52.63%
27.60%
29.88%
13.43%
61.60%

23.30%
29.05%
41.39%
13.77%
57.39%
40.84%
10.01%
37.54%
15.14%
30.59%
69.13%
36.26%
52.03%
19.51%
18.87%
71.93%
29.10%
41.21%
29.15%
27.74%

17.58%
21.94%
22.78%
4.74%
19.62%
18.97%
1.15%
17.21%
7.07%
14.23%
14.36%
26.71%
37.45%
13.75%
6.94%
56.45%
14.35%
39.13%
2.86%
14.35%

5.05%
21.79%
5.62%
5.53%
17.70%
18.97%
0.45%
5.67%
4.31%
19.51%
25.59%
18.54%
11.96%
9.40%
4.93%
29.03%
13.20%
18.98%
7.12%
36.09%

12.87%
23.63%
21.15%
5.56%
17.70%
20.69%
4.47%
19.00%
4.53%
23.03%
44.91%
24.94%
40.00%
12.97%
9.24%
41.94%
15.04%
30.21%
12.68%
10.43%

6.74%
19.25%
23.41%
8.83%
57.39%
37.79%
2.59%
32.13%
14.79%
19.72%
17.55%
28.11%
20.62%
12.00%
13.92%
71.93%
23.30%
34.35%
5.98%
29.25%

21.78%
13.40%
26.37%
11.47%
57.39%
33.59%
2.79%
29.75%
17.58%
19.06%
10.23%
25.84%
16.81%
10.92%
12.75%
52.63%
28.17%
29.34%
24.24%
28.96%

15.75%
15.49%
38.48%
8.82%
57.39%
36.64%
10.01%
26.84%
10.52%
30.27%
68.14%
25.37%
51.53%
14.44%
18.87%
71.93%
27.59%
34.07%
26.87%
27.74%

3.93%
13.79%
9.91%
3.82%
17.70%
18.97%
1.06%
16.21%
4.27%
12.60%
8.09%
22.08%
13.73%
7.60%
6.88%
38.71%
10.16%
22.06%
2.73%
13.91%

13.91%
9.98%
11.69%
5.18%
17.70%
21.55%
1.56%
11.98%
5.80%
13.24%
4.70%
16.56%
8.63%
7.25%
6.41%
29.03%
13.04%
18.62%
9.29%
13.48%

9.04%
11.23%
18.64%
3.82%
17.70%
18.10%
4.47%
11.14%
2.73%
22.88%
44.13%
17.44%
39.02%
9.11%
9.24%
41.94%
12.85%
22.73%
11.29%
10.43%

p
p
A
e
l
i
b
o
M

Average

30.73% 25.63% 34.70% 18.58% 13.97% 19.75% 23.98% 23.65% 30.84% 12.41% 11.98% 16.90%

p
p
A
b
e
W

W1
W2
W3
W4
W5

29.77%
41.02%
36.25%
22.76%
11.83%

19.61%
41.02%
26.89%
26.46%
13.30%

24.62%
41.02%
37.50%
26.31%
17.57%

16.92%
0.00%
15.09%
16.59%
14.50%

9.79%
0.00%
7.75%
20.07%
16.38%

13.51%
0.00%
9.88%
19.85%
23.00%

24.50%
51.02%
35.00%
26.44%
15.93%

6.47%
41.02%
26.89%
23.41%
9.88%

23.99%
51.02%
36.25%
26.30%
17.62%

13.42%
25.00%
14.46%
20.01%
20.39%

1.78%
25.00%
7.75%
17.55%
11.40%

13.03%
25.00%
15.09%
19.85%
22.93%

Average

28.33% 25.46% 29.40% 12.62% 10.80% 13.25% 30.58% 21.53% 31.04% 18.66% 12.70% 19.18%

web apps with NCY10. More details of our experiment subject
apps are available on our online package.

3) Baseline Approaches: We set two groups of baseline
tools, based on the random strategy and the learning-based
strategy. We use the Monkey [9] as a representative of ran-
dom strategy. Monkey is originally used for mobile apps,
so we realize the Monkey (M) for web apps ourselves. For
the learning-based strategy, we use two state-of-the-art ap-
proaches, the Q-testing (Q) [17] and the WebExplor (W) [19],
which are cutting-edge technologies for automated mobile and
web apps. We believe such tools, which prove their capability
compared with other tools, are representative.

4) Evaluation Metrics: Basically, we adopt

the most
widely accepted metric, coverage, as the main metric of this
paper. Speciﬁcally, we utilize the coverage of two granulari-
ties, the line coverage and the branch coverage. Besides, we
use the results of running 1.5 hours and generate 500 test
events. Moreover, we analyze the cross coverage situation,
which means how many code lines / branches are covered by
both tools, or how many code lines / branches are additionally
covered by one tool
than that of another one. The cross
coverage is calculated as follows:

coverageA over B =

coverageA − coverageB
coverageA ∪ coverageB

(5)

10https://istanbul.js.org/

B. RQ1: Mobile App Effectiveness

As shown in the TABLE III, we can ﬁnd that overall,
UNIRLTEST performs better than the baselines. UNIRLTEST
achieves better code coverage by 12.90% over Monkey and
35.39% over Q-testing in line coverage, and by 6.28% over
Monkey and 41.35% over Q-testing in branch coverage after
running 1.5 hours. UNIRLTEST outperforms Monkey by pro-
ducing less non-effective actions, and Q-testing misses many
valid GUI widgets that do not appear in the layout XML
ﬁles. Both Q-testing and UNIRLTEST have extra overhead in
processing the GUI pages and explore with the RL framework,
so there is no tools that will have better efﬁciency than Monkey
in generating actions, and we compare the code coverage when
generating the same number of actions. It is observed that
the advantage of UNIRLTEST is more highlighted, and the
UNIRLTEST achieves better code coverage by 28.59% over
Monkey and 30.37% over Q-testing in line coverage, and by
36.15% over Monkey and 41.04% over Q-testing in branch
coverage, which shows the superior performance in the action
generation effectiveness.

We also investigate the cross coverage among the three
approaches. For the 1.5 hour coverage, on average, UNIRL-
is not covered by
TEST has 13.69% more coverage that
the Monkey, and 24.24% more than Q-testing. For the 500
action coverage, on average, UNIRLTEST has 20.05% more
coverage that is not covered by the Monkey, and 19.57%

more than Q-testing. According to the data, we ﬁnd that
when generating the same number of actions, the advantage of
UNIRLTEST over Q-testing is reduced because UNIRLTEST
and Q-testing has similar action generation efﬁciency, while
UNIRLTEST captures more widgets. However, Monkey and
Q-testing generate actions beyond UNIRLTEST’s scope, but
the number is only 5.37% for Q-testing and 7.69% for Monkey
after 1.5 hour running, and 5.30% for Q-testing and 7.26% for
Monkey in 500 actions.

C. RQ2: Web App Effectiveness

UNIRLTEST also performs well in web apps (TABLE III)
without any modiﬁcation to the RL framework. On average
in 5 web apps after running 1.5 hours, UNIRLTEST achieves
3.81% more line coverages and 4.98% more branch coverage
than Monkey, and 15.51% more line coverages and 22.69%
more branch coverage than WebExplor. In generating 500
actions, UNIRLTEST achieves 1.50% more line coverages and
44.13% more branch coverage than Monkey, and 2.81% more
line coverages and 51.07% more branch coverage than WebEx-
plor. The results show that UNIRLTEST performs well in web
platforms without any modiﬁcation or parameter adjusting.
UNIRLTEST is a universal platform-independent approach.

With regard to the cross coverage, we compare UNIRLTEST
with Monkey and WebExplor. UNIRLTEST has 16.57% more
coverage that are not covered by the Monkey after running
1.5 hours, and the advantage expands to 37.46% when com-
paring the coverage of 500 actions. Conversely, the data of
Monkey over UNIRLTEST is only 0.54% and 0.03%. When
compared with WebExplor, the advantage data is 11.84% and
3.89%, respectively. Conversely, the data of WebExplor over
UNIRLTEST is only 5.06% and 1.58%.

The data above shows that UNIRLTEST achieves excellent
performance on both mobile and web platforms. Speciﬁcally,
the cross coverage analysis proves that UNIRLTEST is capable
of generating more events that are hard to be explored by
the baseline approaches. We have a in-depth analysis in the
following section.

D. RQ3: Advantage Analysis

We have an in-depth investigation to the results of UNIRL-
TEST over the baselines, and we list three of the signiﬁcant
reasons that help improve the effectiveness of UNIRLTEST.

Widget Granularity Exploration. The widget-based RL
exploration is one of the most signiﬁcant novel contribution
of this paper. Instead of retrieving the action targets from the
layout XML ﬁles like some existing approaches, UNIRLTEST
directly starts from the GUI page, and obtains more effective
widgets to alleviate the problems presented in Section II-C.
This is one of the signiﬁcant reason for the success of
UNIRLTEST. However, the widget accuracy [22] is one of
the obstacles that hinders UNIRLTEST achieving a better
performance.

Q-network with State and Action Embedding. After
getting the effective embedding to the states and the actions,
the Q-value of one speciﬁc state-action pair will be refreshed,

so the GUI pages with similar appearances that carry similar
functions will be recognized to avoid repeatedly covering the
same code snippets. The design of the embedding and the Q-
network gives UNIRLTEST the capability of processing the
huge exploration state and action space. However, there exist
a few situations where the assumption that similar GUI layout
are similar pages may not work, so the Q-network may under-
estimate the values of such pages.

Long Semantic Action Sequence. This point is the most
signiﬁcant one in our opinion, which is that UNIRLTEST
shows the potential of generate a long action sequence within
the context. During the generation of long semantic action
sequences, it may be interrupted if some irrelevant actions are
generated. Given that the “future” reward being considered
in the value estimation of current actions, a longer action se-
quence should be preferred by the RL exploration mechanism,
and such a long action sequence tend to have the context
semantics from the perspective of human testers.

E. Threats to Validity

The main thread to the experiment may be the usage of
the apps. In order to reduce this threat, we use the app under
experiment from the existing researches, including [13] [15]
[17] [19]. We also use some widely-used open-source apps
from a popular list [34] because some apps from the above
papers are not accessible due to the maintenance problems.
Besides, the used apps cover many different categories, which
expand the scenarios of GUI layout styles, ensuring the
generalizability of UNIRLTEST.

Another threat may be the parameter or hyperparameter
settings, like the thresholds in widget recognition algorithm,
or the deep learning model training. In order to alleviate the
bias, we set the parameters according to domain knowledge
or following the common practice from existing work. For
some critical parameters, we also conduct pilot studies to ﬁnd
suitable settings before the evaluation. For the baselines, we
use the default settings or the open-source package to avoid
possible biases.

UNIRLTEST contains several deep learning models, and the
performance of the proposed deep learning networks may be
a potential threat. In order to mitigate this threat, we try our
best to collect more data to train the neural networks. Besides,
we use networks with simple structures to avoid performance
problems that may affect the effectiveness of UNIRLTEST.

F. Discussion

In this paper, we implement the UNIRLTEST and con-
duct empirical evaluation on Android and web platforms.
However, we hold that UNIRLTEST is a universal approach,
and will be effective in all GUI-based platforms, including
iOS, harmonyOS, desktop operating systems, or even some
embedded systems. The only accordance of UNIRLTEST is
the analysis on the widget extraction and the layout char-
acterization. As long as the platform has a GUI based user
interface, UNIRLTEST will be capable of exploring the state
and action space. Therefore, we believe UNIRLTEST has

a strong generalizability on different platforms and behave
universally.

Another point is about the efﬁciency. Honestly speaking,
UNIRLTEST has a poor efﬁciency compared with the random
based approaches, like Monkey. However, this is the common
problem of the model-based approaches (based on dynamic
analysis) or learning based approaches. As far as we know,
none of the existing work has a comparison on the action
generation efﬁciency. Therefore, we hold that in the future,
UNIRLTEST will be a complement with the random-based
approaches, and will cover more code that cannot be covered
by the existing approaches.

V. RELATED WORK

A. Automated Software Testing

The most basic strategy of automated software testing is
the random-based strategy. Among them, Monkey is the most
widely used tool [9], and performs quite well on speciﬁc
benchmark apps [2]. Monkey is capable of generating large
amount of test events with high efﬁciency. However, the short-
comings are obvious. Due to the lack of effective guidance,
Monkey generates a large percentage of noneffective or redun-
dant test events, posing a threat to the testing effectiveness.
[35] alleviates such a problem by restricting the GUI states,
but the exploration process stills lacks a guidance.

Approaches adopting model-based strategy [36] [37] build
speciﬁc models for the testing exploration with static or
dynamic (or combination) app analysis [38]. AndroidRipper,
[39] proposed by Amalﬁtano et al., adopts a user-interface
driven ripper to explore the app GUI to exercise the app in
a structured way. Baek et al. [40] present the GUICC, which
provides the selection of multiple abstraction levels for GUI
model generation with a set of multi-level GUI comparison
criteria. Aimdroid [41] introduced by Gu et al. is a tool that
aims to manage the exploration of activities and to minimize
unnecessary transitions between them through an activity-
insulated multi-level strategy during the testing. Biagiola et al.
propose SubWeb [42], which takes advantage of the navigation
structure implicitly speciﬁed by developers for web testing.
DIG [12] pre-selects the most promising candidate test cases
based on their diversity from previous tests. GoalExplorer [43]
ﬁrstly statically models the app GUI and transitions, and then
guides the dynamic exploration of the app to the particular
target of interest. Sapienz [13], proposed by Mao et al., is
a typical tool that uses multi-objective search-based testing
to automatically explore and optimize test sequences. Stout
[15] is a representative and representative model-based tool,
which uses a stochastic Finite State Machine model to describe
the behavior of AUT. Due to the limitation of the modeling
algorithms, the model-based approach can neither precisely
nor completely characterize the apps under test.

Beneﬁting from the development of deep learning and
machine learning models, learning-based automated testing
technologies also emerges. Reinforcement
learning is one
group of suitable algorithms for app exploration. Wuji [27]
is a typical approach that utilizes RL algorithm to explore the

video game apps. It balances between winning the game and
exploring the space of the game to uncover more bugs. Q-
testing is an effective tool for Android testing using the RL
algorithm, it trains a neural network to divide different states
at the granularity of functional scenarios. WebExplor [19] is
an advanced tool for web app testing with the RL algorithm.
It adopts a curiosity-driven model to generate high-quality test
events containing temporal logical relations. Romdhana et al.
[18] compare different RL algorithms in mobile app testing.
Among the above automated testing approaches, many
progresses are made to improve the automated software
testing effectiveness and efﬁciency. However, as claimed in
Section II-C, current approaches are still faced with critical
problems, which hinder the approaches showing better perfor-
mance. Besides, as far as we know, none of current approaches
conduct universal apps testing on different platforms, which
build a high bar for app developers in such an era when the
number of running platforms are growing sharply.

B. GUI Understanding in Software Engineering

Due to the fragmentation problem of apps [44], more and
more work starts to apply GUI understanding technologies
to assist the software engineering tasks [8]. One group of
researches use GUI understanding technologies to conduct
reverse engineering and generate code snippets from GUI im-
ages [45] [46] [47] [48]. Such tools adopt traditional computer
vision technologies or deep learning / machine learning models
to analyze the GUI images.

Another important direction of using GUI understanding
in software testing is the test report analysis and optimiza-
tion. [49] use image understanding technologies to generate
reports for app screenshots indicating bugs. [1] prioritize
crowdsourced test reports with deep image and text semantic
understanding. Besides, [50] [51] [52] are all representative
work using GUI image understanding in crowdsourced test
report optimization. [53] [54] help reproduce the video-based
test reports by analyzing the GUI information in the video test
reports. [55] is capable of detecting duplication in video-based
test reports.

Also, GUI understanding is widely used in record and replay
test scripts. [56] introduces AppTestMigrator to migrate test
cases between apps using the similarity among GUI widgets.
[57] presents a tool that uses CV algorithms to recognize GUI
widgets and then controls robots to complete the automated
testing tasks. [8] realizes the cross-platform record and replay
of test scripts with the image and layout characterization.

One signiﬁcant basis of applying GUI understanding tech-
nologies in software testing is the widget identiﬁcation. Chen
et al. discuss the widget recognition algorithms in [22]. Liu
et al. propose a tool to help detect UI display issues with
deep GUI understanding [58]. Chen et al. [59] propose a
deep learning model to help predict the labels of speciﬁc GUI
widgets. Chen et al. [60] introduce a GUI design search engine
to assist GUI designers.

The above work inspires us to start the automated soft-
ware testing from the GUI perspective with computer vision

technologies. Moreover, considering the nature that some
signiﬁcant widgets, like embedded Canvas elements or other
customized elements, or the embedded H5 pages, which
cannot be retrieved from the GUI layout ﬁles, with traditional
widget identiﬁcation approaches, the analysis on the app GUI
will signiﬁcantly improve the exploration effectiveness of the
automated software testing.

VI. CONCLUSION

In order to tackle the challenges of automated software
testing, this paper proposes a universal reinforcement learning
framework with image understanding technologies to conduct
automated testing for apps of different platforms. UNIRLTEST
integrates a novel approach to embed the app GUI information
for the state comparison in the RL algorithm, and use a deep
neural network to construct the Q-network for the state-action
value determination. UNIRLTEST is the ﬁrst approach that
realizes the platform-independent automated testing. Accord-
ing to the experiment results on mobile and web apps, we
hold that UNIRLTEST is the ﬁrst approach universally test
apps of different platforms with excellent effectiveness, and
UNIRLTEST performs overall much better than the baselines
in code coverage. We also provide some insights into the
advantages of UNIRLTEST.

REFERENCES

[1] S. Yu, C. Fang, Z. Cao, X. Wang, T. Li, and Z. Chen, “Prioritize crowd-
sourced test reports via deep screenshot understanding,” in Proceedings
of
the 2021 IEEE/ACM 43rd International Conference on Software
Engineering.

IEEE, 2021, pp. 946–956.

[2] S. R. Choudhary, A. Gorla, and A. Orso, “Automated test input gen-
eration for android: Are we there yet? (e),” in Proceedings of
the
2015 30th IEEE/ACM International Conference on Automated Software
Engineering.

IEEE, 2015, pp. 429–440.

[3] Appium, “http://appium.io/,” 2022.
[4] Selenium, “https://www.selenium.dev/,” 2022.
[5] L. Gomez, I. Neamtiu, T. Azim, and T. Millstein, “Reran: Timing-and
touch-sensitive record and replay for android,” in Proceedings of the
2013 35th International Conference on Software Engineering.
IEEE,
2013, pp. 72–81.

[6] J. Guo, S. Li, J.-G. Lou, Z. Yang, and T. Liu, “Sara: self-replay aug-
mented record and replay for android in industrial cases,” in Proceedings
of the 28th acm sigsoft international symposium on software testing and
analysis, 2019, pp. 90–100.

[7] M. Halpern, Y. Zhu, R. Peri, and V. J. Reddi, “Mosaic: cross-platform
user-interaction record and replay for the fragmented android ecosys-
tem,” in Proceedings of the 2015 IEEE International Symposium on
Performance Analysis of Systems and Software.
IEEE, 2015, pp. 215–
224.

[8] S. Yu, C. Fang, Y. Yun, and Y. Feng, “Layout and image recognition
driving cross-platform automated mobile testing,” in Proceedings of the
2021 IEEE/ACM 43rd International Conference on Software Engineer-
ing.
[9] Google,

“https://developer.android.com/studio/test/other-testing-

IEEE, 2021, pp. 1561–1571.

tools/monkey,” 2022.

[10] A. Machiry, R. Tahiliani, and M. Naik, “Dynodroid: An input generation
system for android apps,” in Proceedings of the 2013 9th Joint Meeting
on Foundations of Software Engineering, 2013, pp. 224–234.

[11] S. Athaiya and R. Komondoor, “Testing and analysis of web applica-
tions using page models,” in Proceedings of the 26th ACM SIGSOFT
International Symposium on Software Testing and Analysis, 2017, pp.
181–191.

[12] M. Biagiola, A. Stocco, F. Ricca, and P. Tonella, “Diversity-based web
test generation,” in Proceedings of the 2019 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2019, pp. 142–153.

[13] K. Mao, M. Harman, and Y. Jia, “Sapienz: Multi-objective automated
testing for android applications,” in Proceedings of the 25th International
Symposium on Software Testing and Analysis, 2016, pp. 94–105.
[14] A. Mesbah, A. Van Deursen, and S. Lenselink, “Crawling ajax-based
web applications through dynamic analysis of user interface state
changes,” ACM Transactions on the Web, vol. 6, no. 1, pp. 1–30, 2012.
[15] T. Su, G. Meng, Y. Chen, K. Wu, W. Yang, Y. Yao, G. Pu, Y. Liu, and
Z. Su, “Guided, stochastic model-based gui testing of android apps,” in
Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering, 2017, pp. 245–256.

[16] D. Adamo, M. K. Khan, S. Koppula, and R. Bryce, “Reinforcement
the 9th ACM
learning for android gui
SIGSOFT International Workshop on Automating TEST Case Design,
Selection, and Evaluation, 2018, pp. 2–8.

testing,” in Proceedings of

[17] M. Pan, A. Huang, G. Wang, T. Zhang, and X. Li, “Reinforcement
learning based curiosity-driven testing of android applications,” in
Proceedings of the 29th ACM SIGSOFT International Symposium on
Software Testing and Analysis, 2020, pp. 153–164.

[18] A. Romdhana, A. Merlo, M. Ceccato, and P. Tonella, “Deep
reinforcement learning for black-box testing of android apps,” ACM
Transactions on Software Engineering and Methodology, nov 2021,
just Accepted. [Online]. Available: https://doi.org/10.1145/3502868
[19] Y. Zheng, Y. Liu, X. Xie, Y. Liu, L. Ma, J. Hao, and Y. Liu, “Automatic
web testing using curiosity-driven reinforcement learning,” in Proceed-
ings of the 2021 IEEE/ACM 43rd International Conference on Software
Engineering.

IEEE, 2021, pp. 423–435.

[20] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement
learning: A survey,” Journal of artiﬁcial intelligence research, vol. 4,
pp. 237–285, 1996.

[21] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,

no. 3, pp. 279–292, 1992.

[22] J. Chen, M. Xie, Z. Xing, C. Chen, X. Xu, L. Zhu, and G. Li, “Object
detection for graphical user interface: old fashioned or deep learning
or a combination?” in Proceedings of the 28th ACM joint meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2020, pp. 1202–1214.

[23] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” arXiv preprint arXiv:1312.5602, 2013.

[24] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A.
Efros, “Large-scale study of curiosity-driven learning,” Proceedings of
the 7th International Conference on Learning Representations, 2019.

[25] N. Cesa-Bianchi, C. Gentile, G. Lugosi, and G. Neu, “Boltzmann
information processing

exploration done right,” Advances in neural
systems, vol. 30, 2017.

[26] C. C. White III and D. J. White, “Markov decision processes,” European
Journal of Operational Research, vol. 39, no. 1, pp. 1–16, 1989.
[27] Y. Zheng, X. Xie, T. Su, L. Ma, J. Hao, Z. Meng, Y. Liu, R. Shen,
Y. Chen, and C. Fan, “Wuji: Automatic online combat game testing
using evolutionary deep reinforcement learning,” in Proceedings of the
2019 34th IEEE/ACM International Conference on Automated Software
Engineering.

IEEE, 2019, pp. 772–784.

[28] J. Canny, “A computational approach to edge detection,” IEEE Transac-
tions on pattern analysis and machine intelligence, no. 6, pp. 679–698,
1986.

[29] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” in Proceedings of the 3rd International
Conference on Learning Representations, 2015, San Diego, CA, USA,
May 7-9, 2015, Conference Track Proceedings, 2015.

[30] A. Graves, “Long short-term memory,” Supervised sequence labelling

with recurrent neural networks, pp. 37–45, 2012.

[31] M. Pawlik and N. Augsten, “Efﬁcient computation of the tree edit
distance,” ACM Transactions on Database Systems (TODS), vol. 40,
no. 1, pp. 1–40, 2015.

[32] ——, “Tree edit distance: Robust and memory-efﬁcient,” Information

Systems, vol. 56, pp. 157–173, 2016.

[33] H. Robbins, “Some aspects of the sequential design of experiments,”
Bulletin of the American Mathematical Society, vol. 58, no. 5, pp. 527–
535, 1952.

[34] P. Liu, L. Li, Y. Zhao, X. Sun, and J. Grundy, “Androzooopen: Collecting
large-scale open source android apps for the research community,” in
Proceedings of the 17th International Conference on Mining Software
Repositories, 2020, pp. 548–552.

[35] F. Y. B. Daragh and S. Malek, “Deep gui: Black-box gui input generation
the 2021 36th IEEE/ACM
IEEE,

with deep learning,” in Proceedings of
International Conference on Automated Software Engineering.
2021, pp. 905–916.

[36] D. Amalﬁtano, A. R. Fasolino, P. Tramontana, B. D. Ta, and A. M.
Memon, “Mobiguitar: Automated model-based testing of mobile apps,”
IEEE Software, vol. 32, no. 5, pp. 53–59, 2014.

[37] B. Yu, L. Ma, and C. Zhang, “Incremental web application testing using
page object,” in Proceedings of the 2015 Third IEEE Workshop on Hot
Topics in Web Systems and Technologies.

IEEE, 2015, pp. 1–6.

[38] A. Mesbah, A. Van Deursen, and D. Roest, “Invariant-based automatic
testing of modern web applications,” IEEE Transactions on Software
Engineering, vol. 38, no. 1, pp. 35–53, 2011.

[39] D. Amalﬁtano, A. R. Fasolino, P. Tramontana, S. De Carmine, and A. M.
Memon, “Using gui ripping for automated testing of android applica-
tions,” in Proceedings of the 2012 Proceedings of the 27th IEEE/ACM
International Conference on Automated Software Engineering.
IEEE,
2012, pp. 258–261.

[40] Y.-M. Baek and D.-H. Bae, “Automated model-based android gui
testing using multi-level gui comparison criteria,” in Proceedings of
the 31st IEEE/ACM International Conference on Automated Software
Engineering, 2016, pp. 238–249.

[41] T. Gu, C. Cao, T. Liu, C. Sun, J. Deng, X. Ma, and J. L¨u, “Aimdroid:
Activity-insulated multi-level automated testing for android applica-
tions,” in Proceedings of the 2017 IEEE International Conference on
Software Maintenance and Evolution.

IEEE, 2017, pp. 103–114.

[42] M. Biagiola, F. Ricca, and P. Tonella, “Search based path and input
data generation for web application testing,” in Proceedings of
the
2017 International Symposium on Search Based Software Engineering.
Springer, 2017, pp. 18–32.

[43] D. Lai and J. Rubin, “Goal-driven exploration for android applications,”
in Proceedings of the 2019 34th IEEE/ACM International Conference
on Automated Software Engineering.

IEEE, 2019, pp. 115–127.

[44] L. Wei, Y. Liu, and S.-C. Cheung, “Taming android fragmentation:
Characterizing and detecting compatibility issues for android apps,”
in Proceedings of the 31st IEEE/ACM International Conference on
Automated Software Engineering, 2016, pp. 226–237.

[45] C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu, “From ui design image
to gui skeleton: a neural machine translator to bootstrap mobile gui
implementation,” in Proceedings of the 40th International Conference
on Software Engineering, 2018, pp. 665–676.

[46] K. Moran, C. Bernal-C´ardenas, M. Curcio, R. Bonett, and D. Poshy-
vanyk, “Machine learning-based prototyping of graphical user interfaces
for mobile apps,” IEEE Transactions on Software Engineering, vol. 46,
no. 2, pp. 196–221, 2018.

[47] T. A. Nguyen and C. Csallner, “Reverse engineering mobile application
user interfaces with remaui (t),” in Proceedings of
the 2015 30th
IEEE/ACM International Conference on Automated Software Engineer-
ing.

IEEE, 2015, pp. 248–259.

[48] T. Zhao, C. Chen, Y. Liu, and X. Zhu, “Guigan: Learning to generate
gui designs using generative adversarial networks,” in Proceedings
of
the 2021 IEEE/ACM 43rd International Conference on Software
Engineering.

IEEE, 2021, pp. 748–760.

[49] S. Yu, “Crowdsourced report generation via bug screenshot under-
standing,” in Proceedings of the 2019 34th IEEE/ACM International
Conference on Automated Software Engineering.
IEEE, 2019, pp.
1277–1279.

[50] Y. Feng, Z. Chen, J. A. Jones, C. Fang, and B. Xu, “Test report
prioritization to assist crowdsourced testing,” in Proceedings of the 2015
10th Joint Meeting on Foundations of Software Engineering, 2015, pp.
225–236.

[51] Y. Feng, J. A. Jones, Z. Chen, and C. Fang, “Multi-objective test
report prioritization using image understanding,” in Proceedings of the
2016 31st IEEE/ACM International Conference on Automated Software
Engineering.

IEEE, 2016, pp. 202–213.

[52] J. Wang, M. Li, S. Wang, T. Menzies, and Q. Wang, “Images don’t lie:
Duplicate crowdtesting reports detection with screenshot information,”
Information and Software Technology, vol. 110, pp. 139–155, 2019.
[53] C. Bernal-C´ardenas, N. Cooper, K. Moran, O. Chaparro, A. Marcus,
and D. Poshyvanyk, “Translating video recordings of mobile app usages
into replayable scenarios,” in Proceedings of
the ACM/IEEE 42nd
International Conference on Software Engineering, 2020, pp. 309–321.
[54] S. Feng and C. Chen, “Gifdroid: Automated replay of visual bug
reports for android apps,” in Proceedings of the 2022 IEEE/ACM 43rd
International Conference on Software Engineering.

IEEE, 2022.

[55] N. Cooper, C. Bernal-C´ardenas, O. Chaparro, K. Moran, and D. Poshy-
vanyk, “It takes two to tango: Combining visual and textual information
for detecting duplicate video-based bug reports,” in Proceedings of the
2021 IEEE/ACM 43rd International Conference on Software Engineer-
ing.

IEEE, 2021, pp. 957–969.

[56] F. Behrang and A. Orso, “Test migration for efﬁcient

large-scale
assessment of mobile app coding assignments,” in Proceedings of the
27th ACM SIGSOFT International Symposium on Software Testing and
Analysis, 2018, pp. 164–175.

[57] J. Qian, Z. Shang, S. Yan, Y. Wang, and L. Chen, “Roscript: a visual
script driven truly non-intrusive robotic testing system for touch screen
applications,” in Proceedings of
the ACM/IEEE 42nd International
Conference on Software Engineering, 2020, pp. 297–308.

[58] Z. Liu, C. Chen, J. Wang, Y. Huang, J. Hu, and Q. Wang, “Owl eyes:
Spotting ui display issues via visual understanding,” in Proceedings
of the 2020 35th IEEE/ACM International Conference on Automated
Software Engineering.

IEEE, 2020, pp. 398–409.

[59] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhut, G. Li, and J. Wang,
“Unblind your apps: Predicting natural-language labels for mobile gui
components by deep learning,” in Proceedings of the 2020 IEEE/ACM
42nd International Conference on Software Engineering.
IEEE, 2020,
pp. 322–334.

[60] J. Chen, C. Chen, Z. Xing, X. Xia, L. Zhu, J. Grundy, and J. Wang,
“Wireframe-based ui design search through image autoencoder,” ACM
Transactions on Software Engineering and Methodology, vol. 29, no. 3,
pp. 1–31, 2020.

