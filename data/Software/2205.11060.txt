Wasserstein Generative Adversarial Networks for Online Test
Generation for Cyber Physical Systems

Jarkko Peltomäki
Åbo Akademi University
Turku, Finland
jarkko.peltomaki@abo.fi

Frankie Spencer
Åbo Akademi University
Turku, Finland
frankie.spencer@abo.fi

Ivan Porres
Åbo Akademi University
Turku, Finland
ivan.porres@abo.fi

2
2
0
2

y
a
M
3
2

]

G
L
.
s
c
[

1
v
0
6
0
1
1
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
We propose a novel online test generation algorithm WOGAN based
on Wasserstein Generative Adversarial Networks. WOGAN is a
general-purpose black-box test generator applicable to any system
under test having a fitness function for determining failing tests. As
a proof of concept, we evaluate WOGAN by generating roads such
that a lane assistance system of a car fails to stay on the designated
lane. We find that our algorithm has a competitive performance
respect to previously published algorithms.

ACM Reference Format:
Jarkko Peltomäki, Frankie Spencer, and Ivan Porres. 2022. Wasserstein
Generative Adversarial Networks for Online Test Generation for Cyber
Physical Systems. In The 15th Search-Based Software Testing Workshop
(SBST’22), May 9, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA,
5 pages. https://doi.org/10.1145/3526072.3527522

1 INTRODUCTION
Safety validation is the process of establishing the correctness and
safety of a system operating in an environment [2]. Validation
is necessary to ensure that a system works as expected before it
is taken into production. This is especially important with safety
properties of cyber-physical systems (CPS), where a fault can lead
to severe damage to property or even death.

System validation is a challenging and open problem. While there
is a large body of knowledge on the subject, the size, complexity,
and requirements for software-intensive systems have grown over
time. There is a need for cost and time efficient approaches that
support system-level verification and validation of complex systems
including program code, machine learning models, and hardware
components.

In this article, we present a novel test generation algorithm for
the CPS testing competition organized in 2022 within the 14th In-
ternational Search-Based Software Testing Workshop (SBST 2022).
The system under test (SUT) is the lane assist feature of a car. This
is a feature that aims to maintain a car safely within the boundaries
of its driving lane without human assistance. If working properly,
it can help to avoid an accident due to a distracted or tired driver.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SBST’22, May 9, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9318-8/22/05. . . $15.00
https://doi.org/10.1145/3526072.3527522

Figure 1: Two visually similar tests having fitnesses 0.26 (left) and
0.99 (right).

On the other hand, a faulty implementation may fail to avoid an
accident or even actively cause one.

The main safety requirement for the lane assist feature is de-
scribed as an upper bound 𝑏 on the percentage of the body of the car
that is out of the boundaries of its lane (BOLP) at any given moment.
We can formalize this in signal temporal logic [3] as □BOLP ≤ 𝑏.
The goal of the test generator is to falsify this requirement.

We do not have access to any kind of specification, design doc-
ument, or source code of the SUT. Instead, we are provided an
implementation that can be executed in a simulated environment
and a specification of the inputs and outputs of this simulator. The
input of the simulator is a driving scenario defined as a road with the
car as the only traffic. The output of a test 𝑡 is the maximum BOLP
observed during the simulation, which we denote by 𝑓 (𝑡). Based on
this premise, our approach is in essence a system-level, black-box
online test generator. It produces roads as inputs, observes the sys-
tem outputs, and uses these observation to decide what should be
the next test to execute. The goal is to find a failing test, i.e., a test
𝑡 whose fitness 𝑓 (𝑡) satisfies 𝑓 (𝑡) > 𝑏. In order to appreciate how
difficult the task is, we included in Figure 1 two roads defined by 6
points which are visually similar but have very different fitnesses.
This also shows that humans have hard time estimating the lane
assist performance: a human driver would perform equally well on
both roads.

There are two main quality criteria for our test generator. First,
our generator should aim to generate as many failing tests as pos-
sible within a given time budget. Executing a simulation is a time
consuming process and the testing process is limited by a budget
given as a time limit. The second goal is to generate failing tests
that are as diverse as possible. The hypothesis is that a set of diverse
failing tests can improve the process of determining the root cause
of the observed errors. These are two conflicting criteria since a

60657075808590951001050510152025303540456065707580859095100105051015202530354045 
 
 
 
 
 
SBST’22, May 9, 2022, Pittsburgh, PA, USA

Jarkko Peltomäki, Frankie Spencer, and Ivan Porres

test generator may generate many failing tests by exploiting previ-
ously found positives instead of exploring the input space for more
diverse tests.

The test generation algorithm presented in this article is based
on a novel approach to the falsification of CPS. While most existing
approaches use a known metaheuristic such as a genetic algorithm
to search for tests with high fitness, our algorithm uses a Wasser-
stein generative adversarial network (WGAN) [1] as a generator for
such tests. Our algorithm starts tabula rasa and interacts with the
system to learn how to generate high-fitness tests. The algorithm
has no domain knowledge about the SUT except for the representa-
tion of the input space to facilitate random search and learning by
a neural network. Minimal usage of domain knowledge makes our
algorithm general-purpose, i.e., it is easily adjusted for other SUTs.
Our experimental results indicate that our algorithm has competi-
tive performance when compared with the previously published
algorithms [8].

We remark that our algorithm shares ideas with the algorithm
of the paper [10] which is inspired by GANs. The algorithm of
[10] does not train a GAN in the sense of the original GAN paper
[6] as it trains a generator neural network whose outputs achieve
a large value when fed through a discriminator neural network.
While this approach can achieve good results [10], it is a viable
strategy for the generator to always generate a single good test. In
such a case, the resulting test generator would not fulfill the second
quality criterion. In order to work around this potential problem,
our algorithm trains a proper GAN. In fact, we train a WGAN, and
WGANs are known to be able to produce varied samples from their
target distributions [1].

We present our test generation algorithm in detail in Section 2
while its performance in test generation tasks similar to the SBST
2021 CPS testing competition is presented in Section 3. Finally, we
present our conclusions and a discussion of future work in Section 4.

2 A NOVEL ALGORITHM
2.1 Feature Representation
The input to the simulator is a sequence of points in the plane which
the provided simulator interface interpolates to a road. See Figure 1
for roads defined by 6 points. Not all sequences are valid: intersect-
ing roads and roads with steep turns are disallowed. Moreover, a
road must fit in a map of 200 × 200 units. An efficient black-box
validator for candidate roads is provided by the SBST 2021 CPS
competition [8].

Generating valid tests by randomly choosing sequences of plane
points is difficult, so we opted to use the feature representation
described in [4]. For us, a test is a vector in R𝑑 whose components
are curvature values in the range [−0.07, 0.07]. Given a test, we
fix the initial point to be the bottom midpoint of the map and the
initial direction to be directly up. Then we numerically integrate
the curvature values with a fixed step length 15 to obtain 𝑑 more
points. Fixing the initial point and direction is justified by noting
that the test fitness (maximum BOLP) should be translation and
rotation invariant.

This feature representation makes random search of valid roads
more feasible. In our experiments, we set 𝑑 = 5 and we estimated
that then the probability of a test being valid equals 0.48. On the

other hand, we estimated that a sequence of 6 plane points with
components chosen uniformly randomly in [0, 200] corresponds
to a valid road with probability 0.0036. For both estimates, we
generated 5000 random roads.

2.2 Overview of Models and Their Training
Let T be the set of failing tests in the test space [−0.07, 0.07]𝑑 . Our
aim is to learn online a mapping 𝐺 from a latent space [−1, 1]𝑑′
to the test space such that uniform sampling of the latent space
yields samples of T via the map 𝐺. To achieve this, we train this
generator 𝐺 as a WGAN [1]. In this setting, 𝐺 is a neural network
and it is trained to minimize the Wasserstein distance between the
distribution of 𝐺 and the real data distribution T . This is accom-
plished with the help of a critic 𝐶 (also a neural network) which,
informally speaking, is trained to distinguish between tests in T
and outside of it.

Training the WGAN traditionally requires having a training data
sample from T in advance. Since we do not have such data, an
alternative approach is needed. We propose to begin from a set R of
random valid tests and update it using an analyzer 𝐴 (also trained
online). We execute the tests of R on the SUT to know 𝑓 (𝑡) for all
𝑡 ∈ R. We use this data to train 𝐴 (a neural network) to approximate
the mapping 𝑓 . We may now use the generator 𝐺 to produce valid
tests and estimate their fitness by 𝐴 without consulting the SUT
(recall that a validator is available). When we have found a valid
test which is estimated by 𝐴 to have high fitness, we execute it on
the SUT and add the test and its outcome to R.

Training the WGAN directly on R treats all tests equally which
is counterproductive as R can contain many low-fitness tests. We
propose to create a training batch from R which is biased towards
high-fitness tests and train the WGAN on it. Ideally 𝐺 becomes
more able to generate high-fitness tests and 𝐴 gets more accurate as
more data is available. The analyzer 𝐴 does not need to be perfect;
it just needs to drive the training towards high-fitness tests.

It has been observed that WGANs, like GANs, can produce novel
variations of their training data [1] so, whenever the fitness function
𝑓 is well-behaved on the test space (e.g., it is locally continuous) and
the random search is representative enough, the generator should
eventually produce high-fitness tests, some of which belong to T .
Failing this could be taken as an indication that faults do not exist.
Our experiments confirm that Algorithm 1 can expose many
faults of the SUT of the SBST 2021 CPS testing competition showing
empirically the validity of the above ideas. This achieves the first
quality criterion of a test generator. WGANs have been shown
to avoid the so-called GAN mode collapse in which a generator
produces the same output over and over [1]. We observe this in our
experiments as we typically find clusters of failing tests that are
largely different from each other. This achieves the second quality
criterion.

2.3 The Algorithm
Let us explain the pseudocode of our WOGAN algorithm Algo-
rithm 1. Its main structure is similar to the OGAN algorithm of
[10]. Initially a population of 𝑁 random tests is created and exe-
cuted on the SUT. On each round of the outer loop, the analyzer
𝐴 is trained on the current tests 𝑇 and their fitnesses 𝐹 , and the

Wasserstein Generative Adversarial Networks for Online Test Generation for Cyber Physical Systems

SBST’22, May 9, 2022, Pittsburgh, PA, USA

Algorithm 1 WOGAN online test generator
Require: Execution time budget time_budget, number of initial

random tests 𝑁 , multiplier target_reducer.

1: 𝐴 ← initialize_analyzer( )
2: 𝐺, 𝐶 ← initialize_wgan( )
3: 𝛼 ← initialize_batch_parameter( )
4: 𝑇 , 𝐹 ← sample_and_execute_random_tests(𝑁 )
5: while clock < time_budget do
6:

train_analyzer(𝐴;𝑇 , 𝐹 )
𝑋 ← sample_batch(𝑇 , 𝐹 ; 𝛼)
train_wgan(𝐺, 𝐶; 𝑋 )
target ← 1
repeat

test ← generate(𝐺)
if ¬ valid(test) then

continue

target ← target · target_reducer

until predict(𝐴; test) ≥ target
test_fitness ← execute(test)
𝑇 ← 𝑇 ∪ {test}
𝐹 ← 𝐹 ∪ {test_fitness}
𝛼 ← update_batch_parameter(clock, 𝛼)

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

WGAN consisting of the generator 𝐺 and the critic 𝐶 is trained on
a biased batch of samples from 𝑇 (see the next paragraph). Then
candidate tests are produced by 𝐺 in the inner loop until a valid test
whose fitness 𝐴 estimates to be high enough has been produced.
The acceptance threshold is lowered on each iteration in order to
find suitable candidates reasonably fast. The accepted test is added
to the test suite along with its fitness. Finally a parameter 𝛼 con-
trolling the biased batching is updated. The algorithm terminates
when the time budget is exhausted.

Let us then describe how the biased batches are formed. Parti-
tion the interval [0, 1] (the range of 𝑓 ) into 𝐵 contiguous bins of
length 1/𝐵. Put each test in 𝑇 to a bin according to its fitness. We
weight the bins and sample 𝑀 bins (𝑀 is the batch size) according
to this weighting. The biased batch is formed by choosing a test
uniformly randomly (without replacement) from each sampled bin.
The weights are chosen according to a shifted sigmoid function
1/(1 + exp(−(𝑥 + 𝛼))). The parameter 𝛼 increases linearly from 0
to 3 during the execution. Thus initially low-fitness tests have a
chance of being selected, encouraging exploration, whereas later
only high-fitness tests are likely to be sampled.

3 EXPERIMENTAL RESULTS
In this section we compare our WOGAN algorithm to two other
algorithms: random search (to establish a baseline) and the Frenetic
algorithm [4], a genetic algorithm whose performance was deemed
to be among the best of the SBST 2021 CPS testing competition
entries [8].

test is considered failed if its fitness (maximum BOLP) exceeds
0.95. No speed limit is imposed on the simulator AI. We report the
results of 20 repeated experiments for each algorithm.

We use the version 0.24.0.1 of the BeamNG.tech simulator as
in the current 2022 iteration of the competition. The lane assist
AI used is the one provided by BeamNG.tech. The data was col-
lected on a desktop PC running Windows 10 Education with Intel
i9-10900X CPU, Nvidia GeForce RTX 3080 GPU, and 64 GB of RAM.

WOGAN. We produce roads with 6 points, i.e., we set 𝑑 = 5. This
is an arbitrary decision driven by the fact that it is challenging to
train neural networks with limited training data of high dimen-
sionality. We set 𝑑 ′ = 10, so the latent space has dimension 10. In
Algorithm 1, we set 𝑁 = 60 (approximately 20% of the roads that
can be tested in the given time budget) and target_reducer = 0.95.
We use 𝐵 = 10 (10 bins) and 𝑀 = 32 (batch size 32) in the WOGAN
batch sampling. Our random search produces tests (𝑐1, . . . , 𝑐5), as
described in Subsection 2.1, such that 𝑐𝑖+1 ∈ 𝑐𝑖 ± [−0.05, 0.05] for
all 𝑖 ≥ 1. Frenetic uses the same procedure.

We train the WGAN using gradient penalty as in [7]. The critic
and generator both use a dense neural network of two hidden lay-
ers with 128 neurons and ReLU activations. The analyzer has two
hidden layers with 32 neurons and ReLU activations. The WGAN
is trained with the default settings of [7] and learning rate 0.00005
as suggested in [1]. The analyzer is trained with the Adam op-
timizer with learning rate 0.001 and beta parameters 0 and 0.9.
These settings yielded good performance, but we did not attempt
to determine the best parameters empirically.

Frenetic. Frenetic [4] is a genetic algorithm that uses, in our opin-
ion, domain-specific mutators to generate new solutions. A remark-
able feature in Frenetic is the use of two sets of mutators depending
on if an individual represents a passed or a failed test. Mutators
for passed tests are used to explore the solution space while the
mutators for failed tests exploit the failure to generate new high-
fitness roads. The mutators for failed test mutators are: reversing
the cartesian representation of a road, reversing its curvature values,
splitting the road in two halves and swapping them, and mirror-
ing the road. These mutators create new roads that are diverse by
construction under many similarity measures and may lead to tests
with high-fitness.

We updated the Frenetic algorithm to use the latest version
of the simulator. We also set the number of roads for the initial
random search to 60 which is the same as in WOGAN. Due to the
changes, we consider that the version of Frenetic used in this article
is not equivalent to the one presented in [4] and its results are not
comparable.

Notice that Frenetic produces roads defined by variable num-
ber of points. Its random search builds roads with 20 ± 5 points
and the genetic algorithm rules can change this number. As stated
above, WOGAN considers only roads defined by 6 points which
is a much lower number. We opted not to change Frenetic in or-
der to keep our modified Frenetic as close as possible to the original.

3.1 Experiment Setup
The goal of the test generation is to generate a test suite with as
many failing tests as possible within a time budget of 2 hours. A

Random. This algorithm simply generates uniformly randomly
tests (𝑐1, . . . , 𝑐5) as described in Subsection 2.1 (that is, we generate
roads of 6 points).

SBST’22, May 9, 2022, Pittsburgh, PA, USA

Jarkko Peltomäki, Frankie Spencer, and Ivan Porres

Figure 2: Box plots for number of failing tests over 20 experiments.

Figure 3: Box plots for test suite diversities over 20 experiments.

Table 1: Table for main statistics of the experiments.

mean executed tests
SD executed tests
mean failing tests
SD failing tests
mean fitness final 80%
SD fitness final 80%
mean fitness final 20%
SD fitness final 20%
mean diversity
SD diversity

Random WOGAN Frenetic
201.25
253.8
11.56
14.54
21.50
31.30
6.74
11.61
0.65
0.50
0.03
0.04
0.68
0.59
0.04
0.03
16.85
11.51
2.05
4.59

337.7
2.28
8.70
3.81
0.25
0.01
0.26
0.03
16.41
4.66

3.2 Results
Number of Failing Tests. See Figure 2 for box plots for the num-
ber of failing tests in each of the 20 experiments per algorithm. See
also Table 1 for key statistics. Evidently both WOGAN and Frenetic
beat the random search which can only find 8.7 failing tests on
average out of an average of 338 tests executed per experiment.
WOGAN is statistically different from Frenetic (at significance level
0.05): the Wilcoxon signed-rank test reports a 𝑝-value of 0.02 under
the null hypothesis that the median of differences is 0. In Table 1,
we also include the means of average fitnesses of the final 80% and
final 20% of the test suites. Thus WOGAN and Frenetic manage
not only to find failing tests but high-fitness tests in general.

The results indicate that our approach can achieve performance
comparable to an algorithm using more domain-specific knowledge.
Thus we have experimentally validated that our WOGAN algorithm
fulfills the first quality criterion of finding many failing tests.

Generation Time. We define the generation time of a test being
the time elapsed between two executions of a valid test. Average
generation times are 0.5 s (SD 0.2 s), 5.1 s (SD 5.2 s), and 3.3 s (SD
11.2 s) for Random, WOGAN, and Frenetic respectively. However,
generation time is mostly spent on testing if a candidate test is

valid. The remaining time is negligible for Random and on aver-
age 146.7 ms (SD 14.8 ms) and 1.4 ms (SD 1.5 ms) for WOGAN and
Frenetic respectively. WOGAN uses this time mainly for model
training. All times reported are measured in real time. Observe that
WOGAN manages to execute more tests than Frenetic (see Table 1)
even though it uses more time for generation. This is explained by
noting that Frenetic produces longer roads which require longer
simulation time.

Failing Test Diversity. It is an open problem how to measure if the
failing tests are “diverse”. The SBST 2021 CPS testing competition
[8] used a certain notion of sparseness to measure this, but here
we opt for the following simpler measure. For each test, we find its
interpolation to a road (this is what is actually fed to the simulator)
and evenly reduce it to obtain a sequence of plane points of length
75 (the minimum length we observed). These points are rotated in
such a way that the initial direction is always directly up. Then we
transform the point sequence to angles between two consecutive
points to obtain a vector in R74. For a complete test suite, we define
its diversity to be the median of the pairwise Euclidean distances of
these vectors for failing tests. Small diversity value indicates that
the failing tests are not diverse.

The diversities of the test suites are reported in Figure 3 and
additional statistics are found in Table 1. Intuitively the set of fail-
ing tests for random search should be diverse. Therefore Figure 3
indicates that WOGAN and Frenetic both succeed at producing
reasonably diverse failing tests according to the selected diversity
measure. The diversity values for WOGAN and Frenetic are statisti-
cally different (at significance level 0.05): the Wilcoxon signed-rank
test reports a 𝑝-value of 0.0001 under the null hypothesis that the
median of differences is 0. We remark that the roads generated
by Frenetic are based on a higher number of plane points, so they
are by nature more varied. We conclude that we have shown ev-
idence that WOGAN fulfills the second quality criterion of being
able to generate diverse failing tests. It is true that the data suggests

RandomWOGANFrenetic1020304050Number of failed testsRandomWOGANFrenetic510152025DiversityWasserstein Generative Adversarial Networks for Online Test Generation for Cyber Physical Systems

SBST’22, May 9, 2022, Pittsburgh, PA, USA

that WOGAN removes diversity, but accurately assessing the sit-
uation would require a deeper study of the tradeoffs between our
conflicting quality criteria.

4 CONCLUSIONS
We have presented a novel online black-box test generation algo-
rithm based on ideas from generative adversarial networks. The
algorithm uses no explicit domain knowledge except in the choice of
problem feature representation. Additionally, the algorithm learns
a generator which can be thought of as a model for high-fitness
tests for the SUT. Further examination of this model could prove
useful in studying the SUT in more detail.

We have shown experimentally that our WOGAN algorithm
can achieve a performance comparable to previous competitive
algorithms. Not only is WOGAN able to find faults of the SUT, but
it can produce a varied set of failing tests. However, we remark that
our evaluation is based on a single experiment and that the merits
of the WOGAN algorithm should be examined independently in
the context of the SBST 2022 CPS testing competition.1

In the future, we aim to write a more complete subsequent work

providing full details, additional experiments, and actual code.

ACKNOWLEDGMENTS
This research work has received funding from the ECSEL Joint
Undertaking (JU) under grant agreement No 101007350. The JU
receives support from the European Union’s Horizon 2020 research
and innovation programme and Sweden, Austria, Czech Republic,
Finland, France, Italy, Spain.

REFERENCES
[1] M. Arjovsky, S. Chintala, and L. Bottou. 2017. Wasserstein generative adversarial
networks. In Proceedings of the 34th International Conference on Machine Learning
(Proceedings of Machine Learning Research, Vol. 70). PMLR, 214–223.

[2] A. Corso, R. J. Moss, M. Koren, R. Lee, and M. J. Kochenderfer. 2021. A survey
of algorithms for black-box safety validation of cyber-physical systems. J. Artif.
Intell. Res. 72 (2021), 377–428. https://doi.org/10.1613/jair.1.12716

[3] A. Donzé. 2013. On signal temporal logic. In Runtime Verification - 4th Interna-
tional Conference, RV 2013 (Lecture Notes in Computer Science, Vol. 8174). Springer,
382–383. https://doi.org/10.1007/978-3-642-40787-1_27

[4] E. Castellano et al. 2021. Frenetic at the SBST 2021 Tool Competition. In 2021
IEEE/ACM 14th International Workshop on Search-Based Software Testing (SBST).
36–37. https://doi.org/10.1109/SBST52555.2021.00016

[5] A. Gambi, G. Jahangirova, V. Riccio, and F. Zampetti. 2022. SBST tool competition
2022. In 15th IEEE/ACM International Workshop on Search-Based Software Testing,
SBST 2022.

[6] I. J. Goodfellow, J. P.-A., M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. 2014. Generative adversarial networks. CoRR abs/1406.2661 (2014).
arXiv:1406.2661

[7] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. 2017. Im-
proved training of Wasserstein GANs. In Proceedings of the 31st International
Conference on Neural Information Processing Systems (NIPS’17). Curran Associates
Inc., 5769–5779.

[8] S. Panichella, A. Gambi, F. Zampetti, and V. Riccio. 2021. SBST Tool Competition
2021. In 14th IEEE/ACM International Workshop on Search-Based Software Testing,
SBST 2021. IEEE, 20–27. https://doi.org/10.1109/SBST52555.2021.00011

[9] J. Peltomäki, F. Spencer, and I. Porres. 2022. WOGAN at the SBST 2022 CPS tool
competition. In 15th IEEE/ACM International Workshop on Search-Based Software
Testing, SBST 2022. https://doi.org/10.1145/3526072.3527535

[10] I. Porres, H. Rexha, and S. Lafond. 2021. Online GANs for automatic performance
testing. In IEEE International Conference on Software Testing, Verification and
Validation Workshops (ICSTW 2021). 95–100. https://doi.org/10.1109/ICSTW52544.
2021.00027

1

Note added in proof: The competition report is available at [5]. See also [9] for a
brief note on WOGAN’s performance in the competition.

