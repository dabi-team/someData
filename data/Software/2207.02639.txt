JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

1

Adversarial Robustness of Visual Dialog

Lu Yu, Verena Rieser

2
2
0
2

l
u
J

6

]

V
C
.
s
c
[

1
v
9
3
6
2
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Adversarial robustness evaluates the worst-case per-
formance scenario of a machine learning model to ensure its
safety and reliability. This study is the ﬁrst to investigate the
robustness of visually grounded dialog models towards textual
attacks. These attacks represent a worst-case scenario where the
input question contains a synonym which causes the previously
correct model to return a wrong answer. Using this scenario,
we ﬁrst aim to understand how multimodal input components
contribute to model robustness. Our results show that models
which encode dialog history are more robust, and when launching
an attack on history, model prediction becomes more uncertain.
This is in contrast to prior work which ﬁnds that dialog history is
negligible for model performance on this task. We also evaluate
how to generate adversarial test examples which successfully fool
the model but remain undetected by the user/software designer.
We ﬁnd that the textual, as well as the visual context are
important to generate plausible worst-case scenarios.

Index Terms—Adversarial attacks, visual dialog, model robust.

I. INTRODUCTION

N EURAL networks have been shown to be vulnerable to

adversarial attacks, e.g. [1], [2], [3]. These attacks repre-
sent a worst-case scenario where applying a small perturbation
on the original input causes the model to predict an incorrect
output with high conﬁdence. While some adversarial attacks
are targeted and malicious, some also occur naturally, e.g.
when the user input contains an adversarial word, as in this
paper, or a natural visual phenomena, such as fog [4]. Testing
for adversarial robustness is thus crucial to ensure safety and
reliability of a system.

In this paper, we evaluate the adversarial robustness of
state-of-the-art Visual Dialog (VisDial) models with the aim
to understand how different input components contribute to
robustness. It has previously been established that multiple
input modalities increase robustness of pre-neural conver-
sational
to know
interfaces, e.g. [5], [6]. Here, we want
which modalities can mitigate attacks on neural visual dialog
systems, and to what extent. We also aim to understand how to
best generate adversarial examples which successfully attack
the model while at the same time remain unnoticed by the
user/ software developer. This is important, since plausible
attacks are attacks which can also occur naturally during user
interaction.

To the best of our knowledge, we are the ﬁrst to explore
adversarial attacks on VisDial, which was introduced as a
shared task by [7]. A visual dialog system consists of three
components: an image (with a caption), a question and the
dialog history, i.e. previous user and system turns. The latter

Lu Yu is with School of Computer Science and Engineering, Tianjin
University of Technology, China, 300384,; Verena Rieser is with School of
Mathematical and Computer Sciences, Heriot Watt University, Edinburgh, UK,
EH14 4AS, e-mail:{luyu@email.tjut.edu.cn; v.t.rieser@hw.ac.uk.}

Fig. 1. A VisDial agent aims to answer a question related to an image by
ranking a list of candidate answers, given the dialog history. The attacker
attacks the text (question or history) via replacing a word with the synonym
so that the ranking of the predicted answers changes. ‘Relevance’ refers to a
human-annotated similarity score between answer candidates and the ground
truth for computing NDCG.

distinguishes VisDial from other tasks such as Visual Question
Answering (VQA) [8]. In order to answer the question accu-
rately, the AI agent has to ground the question in the image
and infer the context from history, see Fig. 1. VisDial has
attracted considerable interest over the past years, e.g.
[9],
[10], [11], [12], [13], [14], [15], [16], [17]. Most existing
research has focused on improving the modelling performance
on this task, whereas our aim is to evaluate model robustness
via adversarial attacks.

In addition, we use these attacks to improve our understand-
ing of how the model works (i.e. interpretability). Previous
work, such as [18] uses random perturbations to investigate
whether text-based neural dialog systems make use of dialog
history. In a similar vein, we use adversarial attacks on impor-
tant words (rather than random perturbations) on multi-modal
systems to estimate the impact of various input modalities on
model robustness, including history.

Our main contributions are:

• We show that dialog history contributes to model robust-
ness: We attack ten VisDial models which represent a
snapshot of current methods, including different encoding
and attention mechanisms, as well as recent graphical
networks and knowledge transfer using pretraining. We
measure the performance change before and after attack
and show that encoding history helps to increase the

Caption  QuestionHistoryAre any of them stores?We are looking out a window onto  a   street scene.Q1: Is the picture in color?A1: Yes.Q2: What room is this?A2: It is outside.......Prediction AnswersRelevanceYes. (GT answer)No.I think so.I can not tell....A fork resting on the plate.0.800.20.2...0ImageQuestion after AttackAre any of them retailers?Prediction Answers after attackRelevanceNo.Yes. I can not tell.Not that I can see....A fork resting on the plate.00.80.20.2...0Before Attack  After  Attack   Inputs 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

2

robustness against adversarial questions. We also show
that models become more uncertain when the history is
attacked.

(grammaticality); (3) stay unnoticed by humans, i.e. the human
still assigns the correct
label, while the model prediction
changes (label consistency).

• We evaluate adversarial text-generation within VisDial:
We leverage recent Synonym Substitution methods for
adversarial black-box attack [19], [20] and show that
BERT-based models are able to generate more contextu-
ally coherent perturbations. We also conduct an ablation
study to study the trade-off between the effectiveness of
the attack versus the overall text quality.

• We conduct a detailed human evaluation: We investi-
gate the trade-off between successful attacks and their
ability to remain unnoticed by humans. In particular, we
evaluate semantic similarity, ﬂuency/grammaticality and
label consistency. We ﬁnd that human evaluators are able
to identify an attack from the textual and multimodal
context.

II. RELATED WORK

A. Adversarial Attack for Text

Adversarial attacks have been widely investigated within
uni-modal applications, foremost for computer vision [21],
[22], [23]. Adversarial attacks on text are more challenging
due to its discrete nature, which makes it harder to stay
undetected. Textual attacks have been studied for tasks such
as sentiment analysis [19], natural language inference [20],
dialogue systems [24], [25] etc.

Adversarial textual attack methods can be divided into three
levels of granularity [26], [27]: character-level, word-level and
sentence-level attacks. Character-level attack [28], [29] can
often be detected by a spell checker. Sentence-level attack [30],
[31], [32], [33] permutes longer phrases or paraphrases the
whole sentence, which makes it challenging to maintain the
original semantics. Recent word-level attack methods [34],
[19], [20], [35], on the other hand, are more subtle and
harder to detect: they are targeted towards ‘vulnerable’ words,
which are substituted via their synonyms in order to preserve
semantic meaning. In our paper, we explore word-level attack
methods on VisDial.

B. Adversarial Attack for Multi-modal Systems

There is less research on adversarial attacks for multi-modal
tasks. For example, Optical Character Recognition [36], Scene
Text Recognition [37], Image Captioning [38] and VQA [39],
[40]. Most of these works utilise white box attack, where
the parameters, gradient and architecture of the model are
available, e.g. by attacking attention [39], [41]. Whereas we
follow a more realistic black-box setting which assumes that
the attacker only has access to the model’s prediction on test
data.

[40] is the closest related to our work:

they generate
adversarial textual attacks for the VQA task using contrastive
examples and thus don’t pay attention to semantic similar-
ity. In contrast, we are interested in generating adversarial
attacks which follow three desiderata, as outlined by [42]: An
adversarial text should (1) keep the same semantic meaning
(semantic similarity); (2) guarantee ﬂuency and grammar

A. Problem Formulation

III. METHOD

VisDial

formulated as

a discriminative
is given an image I,

learning
is
task, where the model
the di-
alog history (including the image caption C) H =
), the question Qt, and N =
( C
(cid:124)(cid:123)(cid:122)(cid:125)
H0

(cid:124)
100 candidate answers At = (A1
) to rank,
including the ground truth (GT), which is labelled Yt, where
t indicates the round ID.

, ..., (Qt−1, At−1)
, (Q1, A1)
(cid:125)
(cid:125)
(cid:123)(cid:122)
(cid:124)
H1

t , ..., A100

(cid:123)(cid:122)
Ht−1

t , A2

t

In the following, we focus on generating textual adversarial
examples for the question and history (including the caption).
That is, for a sentence X ∈ {Q, H}, and F (X) = Y , a
successful adversarial attack sentence Xadv should result in
F (Xadv) (cid:54)= Y , while meeting the following requirements:

• Semantic Similarity: Sim(X, Xadv) ≥ ε, where Sim(·)
is a semantic and syntactic similarity function. The se-
mantic similarity between the original sentence X and
the adversarial attack sentence Xadv should above a
similarity threshold ε; Following [19], we use Universal
Sentence Encoder [43] to encode the two sentences into
high dimensional vectors and use their cosine similarity
score as an approximation of semantic similarity.

• Grammaticality: The adversarial attack sentence Xadv

should be ﬂuent and grammatical.

• Label Consistency: Human annotators still assigns the
correct GT label Y after the original sentence X changes
to Xadv.

B. Visual Dialog Models

We adopt ten state-of-the-art VisDial models from [17],
[13], [15], [44] as the target models to attack – represent-
ing a snapshot of current techniques popular for VisDial.1
[17] experiment with several multi-modal encodings based
on Modular Co-Attention (MCA) networks [45]: MCA-
I encodes the image and question representation using late
fusion; MCA-H only encodes the textual history with late
fusion; MCA-I-H encodes image and history with late fusion;
MCA-I-HGQ encodes all three input modalities using early
fusion between question and history; MCA-I-VGH is another
early fusion variant which ﬁrst grounds the image and history.
We also consider Recursive Visual Attention (RvA) [13]
as an alternative to MCA, encoding history and image infor-
mation.

In addition, we test two variants of causal graphs from [15]
by adding to causal principles P1/P2: P1 removes the history
input to the model to avoid a harmful shortcut bias; P2 adds
one new (unobserved) node U and three new links to history,
question and answer respectively.

Finally, we test a Knowledge Transfer (KT) method based
on a Sparse Graph Learning (SGL) [44] framework using
pre-training model P1/P2.

1Details on model architecture can be found in the original papers.

JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

3

C. Synonym-based Methods

For generating attacks, we explore two state-of-the-art
synonym-based methods, which ﬁrst ﬁnd the vulnerable words
of the sentence, and then replace them with a semantically
similar word.2 These two methods differ in the way they
generate the synonyms:

• TextFooler [19] ﬁnds the synonym by using specialised
word embeddings from [46]. Candidates are selected
according to the cosine similarity between the word and
every other word.

• BertAttack [20] generates the synonym via BERT’s
masked language model using contextually embedded
perturbations.

In following these previous works, we ﬁrst detect vulnera-
ble words by calculating prediction change before and after
deleting a word. We then impose additional constraints to
improve the quality (and in particular the grammaticality)
of our attacks, which we will further analyse in an ablation
study: We apply a stop word list before synonym substitution,
extending the list by [19], [20] for our domain. We also apply
additional quality checks for selecting synonym candidates:
We ﬁlter by part-of-speech (POS)3 to maintain the grammar
of the sentence. We then experiment with a semantic simi-
larity threshold ε to choose the top k synonyms. Finally, we
iteratively select the word with the highest similarity until the
attack is successful.

D. Adversarial Attack on Visual Dialog Models

1) Question Attack: Attacking the question in VisDial
differs from other common textual attacks, such as sentiment
classiﬁcation, image captioning or news classiﬁcation, in the
following ways:

Question: The question in VisDial is generally much shorter
than a typical declarative sentence in the above tasks. The
average length of the question in the VisDial dataset is 6.2
words, which makes it harder to ﬁnd a word to attack. For
instance, “Is it sunny?”, “What color?”, “How many?”, there
is only one word left to attack after ﬁltering out the stop words,
i.e. {is, it, what, how}.

Answer: For the VisDial task, the model ranks N possible
candidate answers according to its log-likelihood scores. The
attack is considered successful once the top ranked answer
differs from the GT. However, there can be several candidate
answers which are semantically similar or equivalent, such as
“yes/yep/yeah”. This is different from other labelling tasks,
such as “positive/neutral/negative” sentiment. We account for
this fact by considering several common retrieval metrics
before and after the attack, including R@k (k=1,5,10), Mean
Reciprocal Rank (MRR), and Normalized Discounted Cumula-
tive Gain (NDCG) – a measure of ranking quality according to
manually annotated semantic relevance scores in a 2k subset
of VisDial.

2Note that previous work refers to these methods as “synonym-based”, e.g.
[42], but not all of the substitutions are synonyms. They can also include
different lemmatas of the same lexeme, such as singular and plural, as well
as different spellings, etc. Also see Table VIII.
3Using SpaCy https://spacy.io/api/tagger.

Model: In contrast to other common textual attacks appli-
cations, our model has several input modalities, which it can
leverage to answer the question. These input modalities can
be combined in different ways as explained above. One of
the goals of this paper is to understand how multiple input
encodings can contribute to model robustness.

2) History Attack: We also attack the textual history using
the same procedure. The use of history is the main distin-
guishing feature between the VisDial and the VQA task, and
thus of central interest in this work. History is mainly used
for contextual question understanding, including co-reference
resolution, e.g. “What color are they?”, and ellipsis, e.g. “Any
others?” [47], [48].

Our preliminary results indicate that attacking history is
hardly ever successful, i.e. does not result in label change.
This is in line with previous work, which suggests that history
only plays a negligible role for improving model performance
on the VisDial task, e.g. [49], [17]. However, there is also
some evidence that history helps, but to a smaller extent.
For example, [14] show that accuracy can be improved when
forcing the model to pay attention to history. Similarly, [17]
show that history matters for a sub-section of the data.

In a similar vein, we investigate how history contributes
to the model’s robustness and,
in particular, can increase
the model’s certainty in making a prediction. We adopt the
perplexity metric, following [18],
to measure the change
of prediction distribution after (unsuccessfully) attacking the
history, i.e. after adding the perturbation to the history while
the top-1 prediction is unchanged. The difference between the
perplexity before and after the attack reﬂects the uncertainty
change of the model. The perplexity with the original history
input is calculated with the following equation:

P P L(F (X), Y ) = −

(cid:88)

X

F (X)log2Y

(1)

And the perplexity after attack is:

P P L(F (Xadv), Y ) = −

(cid:88)

Xadv

F (Xadv)log2Y

(2)

IV. EXPERIMENTAL SETUP

A. Dataset

We use the VisDial v1.0 dataset, which contains 123,287
dialogs for training and 2,064 dialogs for validation. The ten
target models are trained on the training set and the adversarial
attacks are generated for validation set (as the test set is only
available to challenge participants).

B. Automatic Evaluation Metrics

In order to assess the impact of an attack, we use the
automatic evaluation metrics from [19]: The accuracy of
the model tested on the original validation data is indicated
as original accuracy and after accuracy on the adversarial
samples – the larger gap between these two accuracy means
the more successful of our attack (cf. relative performance
drop [∆]). The perturbed word percentage is the ratio of
the perturbed words and the length of the text. The semantic

JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

4

Inputs

Methods

Orig.R@1

Aft.R@1 [∆]

Orig.R@5

Aft.R@5 [∆]

Orig.NDCG

Aft.NDCG [∆]

Orig.MRR

Aft.MRR [∆]

Pert.

S.S.

Quer.

Question Attack

I-only
MCA-I
H-only MCA-H

I+H

MCA-I-HGQ
MCA-I-VGH
MCA-I-H

I+H

RvA

I-only
I+H

P1
P1+P2

I+H

SLG
SLG+KT

I-only
MCA-I
H-only MCA-H

I+H

MCA-I-HGQ
MCA-I-VGH
MCA-I-H

I+H

RvA

I-only
I+H

P1
P1+P2

46.6
45.9
50.8
48.6
50.0

49.9

48.8
41.9

49.1
48.7

46.6
45.9
50.8
48.6
50.0

49.9

48.8
41.9

38.2 [-18.0]
40.0 [-12.9]
45.6 [-10.2]
43.3 [-10.9]
45.2 [-9.6]

43.9 [-12.0]

43.5 [-10.9]
37.1 [-11.5]

43.9 [-10.6]
42.6 [-12.5]

36.1 [-22.5]
39.1 [-14.8]
44.2 [-13.0]
41.5 [-14.6]
43.1 [-13.8]

43.6 [-12.6]

42.6 [-12.7]
35.8 [-14.6]

76.3
76.8
81.7
78.7
81.4

82.2

80.2
66.9

81.1
71.3

76.3
76.8
81.7
78.7
81.4

82.2

80.2
66.9

BertAttack

62.7 [-17.8]
67.3 [-12.4]
71.4 [-12.6]
68.0 [-13.6]
69.5 [-14.6]

72.2 [-12.2]

69.2 [-13.7]
57.8 [-13.6]

72.1 [-11.1]
60.8 [-14.7]

61.5
52.2
60.0
62.6
59.6

56.3

60.0
73.4

63.4
74.5

TextFooler

63.9 [-16.3]
68.5 [-10.8]
71.6 [-12.4]
68.2 [-13.3]
71.2 [-12.5]

73.2 [-10.9]

71.1 [-11.3]
56.9 [-14.9]

61.5
52.2
60.0
62.6
59.6

56.3

60.0
73.4

54.9 [-10.7]
48.4 [-7.3]
55.2 [-8.0]
57.3 [-8.5]
54.6 [-8.4]

50.9 [-9.6]

54.2 [-9.7]
67.9 [-7.5]

58.4 [-7.9]
68.2 [-8.5]

53.9 [-12.4]
48.0 [-8.0]
54.4 [-9.3]
56.5 [-9.7]
53.7 [-9.9]

50.2 [-10.8]

53.5 [-10.8]
66.9 [-8.9]

60.0
60.0
64.3
62.2
63.8

64.2

62.9
54.0

63.4
59.9

60.0
60.0
64.3
62.2
63.8

64.2

62.9
54.0

47.7 [-20.5]
51.1 [-14.8]
55.6 [-13.5]
53.3 [-14.3]
54.6 [-14.4]

54.5 [-15.1]

54.1 [-14.0]
46.2 [-14.4]

55.0 [-13.2]
50.3 [-16.0]

47.1 [-20.5]
51.1 [-14.8]
54.8 [-14.8]
52.3 [-15.9]
54.0 [-15.4]

55.3 [-13.9]

54.4 [-13.5]
45.1 [-16.5]

16.7
16.7
17.1
16.7
16.7

17.0

17.4
17.0

17.5
17.3

16.8
17.1
17.0
16.5
16.9

16.9

17.3
17.1

74.4
75.4
74.1
74.3
74.8

74.4

74.2
73.7

73.4
74.6

74.4
74.6
74.4
74.4
74.7

74.9

74.3
73.7

5.2
5.2
5.2
5.2
5.2

5.2

5.2
5.2

5.2
5.2

19.7
19.7
19.9
19.8
19.8

19.9

20.1
19.8

I+H

49.1
48.7

SLG
SLG+KT

63.4
74.5
TABLE I
VISDIAL MODEL PERFORMANCE BEFORE ATTACKING QUESTION (ORIG.) AND AFTER (AFT.). IN ADDITION TO STANDARD METRICS, WE MEASURE THE
PERTURBED WORD PERCENTAGE (PERT.), SEMANTIC SIMILARITY (S.S) AND THE NUMBER OF QUERIES (QUER.) TO ASSESS BERTATTACK VS.
TEXTFOOLER. THE relative PERFORMANCE DROP IS LISTED AS [∆]. HIGHLIGHTS INDICATE THE LEAST ROBUST AND MOST ROBUST MODEL.

55.3 [-12.8]
49.8 [-16.9]

43.1 [-12.2]
41.6 [-14.6]

73.4 [-9.5]
59.7 [-16.3]

57.8 [-8.8]
67.6 [-9.3]

81.1
71.3

63.4
59.9

19.9
19.9

74.2
74.6

17.3
17.1

similarity measures the similarity between the original text and
the adversarial text by cosine similarity score. The number of
queries shows the efﬁciency of the attack (lower better). In
addition, we use retrieval based metrics to account for the
fact that VisDial is a ranking task: original/after R@{5, 10}
measures the performance of top 5/10 results before and after
attack (where R@1 corresponds to accuracy); we also report
original/after mean reciprocal rank (MRR) and original/after
Normalized Discounted Cumulative Gain (NDCG) which mea-
sure the quality of the ranking.

C. Implementation Details

All models are implemented with Pytorch. We embedded
BertAttack and TextFooler to our VisDial system4. We initially
set the semantic similarity threshold 0.5 for attacking both
question and history (but see ablation study of different
threshold in Table VI). Detailed results with R@k (k=10) are
shown in Appendix A and B due to space limitations. All our
code will be made available.

A. Question Attack

V. RESULTS

Table I summarises the results. We ﬁrst compare the results
of input encodings and fusion mechanisms. We ﬁnd that
MCA-I (with image input only) is the least robust model
with a relative performance drop of over 22% on R@1
using TextFooler. MCA-H (with no image input) is vulnerable
with respect to R@1, but does well on NDCG, suggesting
that history helps to produce a semantically similar response

4BertAttack code from https://github.com/LinyangLee/BERT-Attack and

TextFooler code from https://github.com/jind11/TextFooler.

despite the attack and lack of input
image. One possible
explanation of these results is given by previous research
claiming that VisDial models mainly pay attention to text,
e.g. [49]. However, in contrast to claims by [49], we ﬁnd
that history is important for robustness: In general, models
encoding history are more robust with the MCA-I-H model
being the least vulnerable model. Note that this is also the
best performing model in [17]. Recursive visual Attention
(RvA) in general shows lower robustness than MCA-based
methods. Causal encodings using graphs lead to comparable
robustness results for P1. Adding P2 results in a slight drop in
robustness. This is interesting, because P2 adds an unobserved
node to represent history while avoiding spurious correlations
from training data. This drop thus might suggest that previous
robustness is due to the very same bias. Additionally, we
observe that knowledge transfer (KT) via pre-training for
the SLG method helps to boost the performance of NDCG,
however not the robustness.

We further perform an example based analysis of the top-
1 predicted answer changes after a successful question attack,
see Fig. 2. We observe answer changes to the opposite meaning
(e.g. from “no” to “yes”), which can be considered as a
maximum successful attack. Some answers change to a similar
meaning in context (e.g. from “No pets or people” to “No”),
which is reﬂected in fewer NDCG changes. In some cases,
the answer changes from certain / deﬁnite to uncertain /
noncommittal and the other way round (e.g. from “white”
to “Not sure”).

Next, we compare the two attack methods. We ﬁnd
that TextFooler is more effective: It achieves up to 4.5%
higher drop than BertAttack. However, BertAttack is more
efﬁcient: It reduces the number of queries (Quer.) about

JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

5

History Attack

Orig.PPL

Aft.PPL [∆]

Fig. 2. Examples of answer change after question attack on MCA-I-H model
with BertAttack.

MCA-I
MCA-H
MCA-I-HGQ
MCA-I-VGH
MCA-I-H
RvA
P1
P1+P2
SLG
SLG+KT

-
53.2
49.4
52.3
49.5
53.4
-
77.0
52.7
65.0
TABLE II
COMPARISON OF PERPLEXITY INCREASE [∆] WHEN ATTACKING THE
HISTORY OF DIFFERENT VISDIAL MODELS WITH BERTATTACK.

-
60.0 [+6.8]
52.2 [+2.8]
52.3 [0]
51.9 [+2.4]
56.4 [+3.0]
-
77.0 [0]
53.4 [+0.7]
65.3 [+0.3]

Caption

User (question)

System (answer)

44.9%

Attack

30.8%
TABLE III
COMPARING WHICH PART OF HISTORY WAS CHOSEN FOR AN ATTACK ON
MCA-I-H MODEL WITH BERTATTACK.

24.3%

Fig. 3.
BertAttack and TextFooler.

Example attacks on the MCA-I-H target model generated by

four times compared to TextFooler. Efﬁciency is important in
attack settings, as attackers always run into danger of being
discovered. Furthermore, the perturbed word percentage (Pert.)
for both methods is around 17%, which means the average
perturbation is about one word for each question (since the
average length of the question is 6.2). Similarly, the semantic
similarity (S.S.) is over 70% which is about the same across
all models.

We further compare TextFooler and BertAttack using an
example-based analysis, see Fig. 3. We ﬁnd that TextFooler
is not able to distinguish words with multiple meanings
(homonyms), whereas BertAttack is able to use BERT context-
embeddings to disambiguate. Consider the examples where
TextFooler replaces “ﬂat” (adverb) with “loft” (noun) and
“faces” (noun) with “confront” (verb), which POS tagger
failed to catch. Based on the above results, we use BertAttack
to attack the MCA-I-H model in the following experiments.

B. History Attack

We followed the same procedure to attack the history,
which includes the caption, as well as the user questions
and the system answers. As explained in Section III-D2, we
consider an attack ‘successful’ once the probability of the
corresponding GT decreases and we use perplexity to measure
the uncertainty of the prediction. The results in Table II show
that attacking history increases the uncertainty of almost all
the models, especially when the history is the unique input
component (MCA-H model).5 This conﬁrms our previous
results that encoding history increases robustness.

When analysing which part of history was attacked the most
(see Table III), we ﬁnd that 44.9% of the time the image

5Attacking the history of MCA-I-VGH model doesn’t change the prediction
distribution because its encoder only uses a single round of history following
[17].

caption was attacked, followed by system answer 30.8% and
user question 24.3%. We thus conclude that the image caption
is the most vulnerable part (and ergo the most informative)
compared to the rest of history.

VI. ABLATION STUDY

We perform several ablation studies to analyze the impact
of the quality constraints. We are interested in the trade-off
between using these constraints to produce high quality text
(which increases the chance of the attack to remain unnoticed
by humans) versus an effective attack (which increases the
chance of the model changing its prediction).

1) Effect of Selecting Vulnerable Words: First, we compare
the results of choosing a random word in text to attack and
our vulnerable word attack. The results in Table IV show that
attacking the vulnerable word achieves a 2.0% higher relative
drop for R@1, NDCG and MRR.

2) Effect of Stop Words Set: Next, we compare the results
with/without stop words. The results in Table V show that
attacking all words leads to more successful attack in terms
of R@1 and NDCG, while attacking with stopwords leads
more successful attacks for MRR. We use stop words list for
all the experiments since attacking question words, preposition
or pronouns result in highly ungrammatical sentences.

3) Effect of Semantic Similarity: The semantic similarity
threshold between the original text and adversarial text is
used to guarantee the similar meaning of the attack. In the
previous experiments, we set 0.5 as default threshold. Table VI
shows results with different semantic similarity thresholds
(0.1, 0.3, 0.5 and 0.7) respectively. The results show that when
increasing the threshold ε from 0.1 to 0.7, the number of
successful attack decreases 4.1%, while R@1 and NDCG drop
around 3% after attack, which means there are more successful
attacks if we loosen the semantic similarity constraint. In
addition, the examples in Fig. 4 illustrate that a lower semantic
similarity threshold comes at the cost of lower ﬂuency and

R@1 AnswerQuestionOrig.: Is the mannequin a woman?Aft.:   Is the mannequin a girl?Orig.: No.Aft.:   Yes.Orig.: Are there any pets in the photo?Aft.:   Are there any animals in the photo?Orig.: No pets or people.Aft.:   No.Orig.: What color is the plane?Aft.:   What colour is the plane?Orig.: White.Aft.:   Not sure.TextFoolerBertAttackN/AOrig.: Is it a flat screen?Aft.:   Is it a loft screen?Orig.: Is it a close up of their faces or their bodies?Aft.: Is it a close up of their confront or their bodies?Orig.: Is it a close up of their faces or their bodies?Aft.: Is it a close up of their face or their bodies?Orig.: What color is the house?Aft.:   What color is the home?Orig.: What color is the house?Aft.:   What color is the residence?Orig.: Are there trees no the mountain?Aft.:   Are there woods on the mountain?Orig.: Are there trees no the mountain?Aft.:   Are there sapling on the mountain?JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

6

∆R@1 ∆NDCG ∆MRR

Random -7.6
-9.6
Ours

-6.0
-8.4
TABLE IV
EFFECT OF VULNERABLE WORD ATTACK ON MCA-I-H MODEL WITH
BERTATTACK.

-12.4
-14.4

∆R@1 ∆NDCG ∆MRR

All
Ours

-9.2
-8.4
TABLE V
EFFECT OF STOP WORDS SET ON MCA-I-H MODEL WITH BERTATTACK.

-12.6
-9.6

-10.3
-14.4

grammaticality, i.e. at the price of being more easily detectable
by humans. We will explore this in more detail in human study.
We analyze the combined effect of adding POS, semantic
similarity constraint and grammar check modules (We used
the same grammar tool as by [42].). From Table VII, we can
see that in general it results in less successful attack when
the number of constraints increases. The success from raw
attack to ‘disguised’ attack decreases 2.4% on R@1, 3.7%
on NDCG, but there is little effect on MRR. In addition, the
examples in Fig. 5 show that adding constraints improves the
textual quality of the adversarial attack and its likelihood to
be undetected by humans, which we investigate further in the
following evaluation study.

VII. HUMAN EVALUATION STUDY

We evaluate the quality of our generated adversarial ques-
tion attack by asking human judges on Amazon Mechan-
if the generated
ical Turk (AMT) to rate three aspects:

ε

0.1
0.3
0.5
0.7

Num./(%)

∆R@1 ∆NDCG ∆MRR

219 (10.6%)
215 (10.4%)
198 (9.6%)
135 (6.5%)

-10.8
-10.8
-9.6
-6.0

-9.6
-9.2
-8.4
-6.7

-14.1
-14.1
-14.4
-15.2

TABLE VI
COMPARISON OF NUMBER OF SUCCESSFUL ATTACKS (total val set
n=2064) WITH DIFFERENT SEMANTIC SIMILARITY THRESHOLDS ε ON
MCA-I-H MODEL WITH BERTATTACK.

Num./(%)

∆R@1 ∆NDCG ∆MRR

Raw Attack
+POS
+POS+ε(0.5)
+POS+ε(0.5)+Gram.

224 (10.9%)
221 (10.7%)
198 (9.6%)
190 (9.2%)

-11.6
-11.0
-9.6
-9.2

-9.9
-9.7
-8.4
-6.2

-13.9
-14.1
-14.4
-13.6

TABLE VII
EFFECT OF DIFFERENT QUALITY CONSTRAINTS ON MCA-I-H MODEL
WITH BERTATTACK.

question preserve the semantic similarity (semantic similarity
with/without given image); if the generated question is natural
and grammatical (grammaticality); if the human’s prediction is
unchanged for the generated question (label consistency). We
evaluate a total of 198 generated attacks, randomly sampled
from the development set, where three users are asked to rate
each instance.

1) Evaluation of Semantics: We ﬁrst ask crowd workers
to evaluate whether the original and the adversarial question
still have the same meaning on a scale from 1 to 4, where
1 is “One text means something completely different” and 4
is “They have exactly the same meaning”. Fig. 6 shows the
crowdsourcing interface and instructions. We elicited ratings
with and without showing the image in order to measure
the effects of multimodal grounding. Our results show that
the semantic similarity is rated slightly lower when shown
together with the original image (average score 3.518 / 4)
than without image (average score 3.564 / 4). The example

Fig. 4. Attack examples with different semantic similarity thresholds ε on
MCA-I-H model with BertAttack.

Fig. 5. Generated adversarial examples under different quality constraints on
MCA-I-H model with BertAttack.

ExamplesConstraints   (0.7)Orig.: Is it a large church?Aft.:   Is it a big church?Orig.: Is her hair pulled back?Aft.:   Is her wig pulled back?+Orig.: Is the fireplace lit?Aft.:   Is the furnace lit?+Orig.: What color is the wine?Aft.:   What colour is the wine?+Orig.: What is the adult doing?Aft.:   What is the adult done?Orig.: Is there buildings?Aft.:   Is there houses?+Orig.: Is the picture outside?Aft.:   Is the picture beyond?+Orig.: Are they titled?Aft.:   Are they untitled?+   (0.5)   (0.3)   (0.1)ExamplesConstraintsRawRaw + POSOrig.: Is it a large church?Aft.:   Is it a big church?Orig.: What color is the tennis court?Aft.:   What colour is the tennis court?+Orig.: Does the snow appear fresh?Aft.:   Does the snow appears fresh?+Orig.: Can you see the sun?Aft.:   Can you see the sunlight?+Orig.: Are they indoors?Aft.:   Are they outdoors?Orig.: Is this inside?Aft.:   Is this interior?+Orig.: Is it red?Aft.:   Is it reds?+Orig.: How tall is the man?Aft.:   How big is the man?+Raw + POS +    (0.5)Raw + POS +    (0.5) + GramJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

7

Fig. 6. AMT task description and interface to evaluate semantic consistency before and after the attack w/o image.

Attack Types

Percentage Gram. Score

synonyms for V+L tasks.

British vs. American English
Synonyms/near synonyms
Singular vs. Plural
Comparatives and Superlatives
Others

34.9%
34.3%
19.7%
4.0%
7.1%

4.923
4.417
3.974
4.208
3.452

TABLE VIII
PERCENTAGE AND GRAMMATICALITY SCORE OF DIFFERENT TYPES OF
ATTACK ON MCA-I-H MODEL WITH BERTATTACK.

Fig. 7. The visual context changes the perceived similarity rating by humans:
‘furnace’ becomes more dissimilar to ‘ﬁreplace’ in a living room context.

in Fig. 7 demonstrates how the visual context can change
the semantic similarity ratings. Therefore, one future avenue
is to use visually grounded word embeddings for generating

2) Evaluation of Grammaticality: We evaluated whether
the utterance is ﬂuent and grammatical (as deﬁned in Fig. 8)
on a scale from 1-5, where 1 is “Not understandable” and
5 is “Everything is perfect; could have been produced by
a native speaker”. Overall, our attacks are rated as highly
grammatical (average score 4.429 / 5). We furthermore in-
vestigate the effect of different attacks. In particular we
manually identify ﬁve common types of successful attacks.
Table VIII lists their frequencies and average grammaticality
rating. Synonyms/near synonyms is the main type of attack,
closely followed by British vs. American English (e.g. “color”
vs. “colour”, “bathroom” vs. “restroom”), others include
Singular vs. Plural, Comparatives and Superlatives (e.g.
“great/greater/greatest”) and Others mainly include grammar
operations like uncaught POS change (e.g. “sunny” vs. “sun”)
and tense change (e.g. “eat” vs. “ate”). Looking at
the
grammar ratings, we conclude that substituting British vs.
American English has the least
impact on grammaticality,
whereas grammatical operations, such as replacing singular
with plural, as well as changes classiﬁed under Others have
the worst impact.

3) Evaluation of Label Consistency: Finally, we evaluate
label consistency by asking users to judge whether the answer
remains unchanged for the adversarial question by selecting
among “1 - Yes, answer is correct”, “2 - No, answer is
incorrect” and “3 - Unsure” as shown in Fig. 9. We ask three

InstructionsWe give some examples for the different options.A - One text means something completely differente.g. "Can you see big ben"/ Can you see huge ben? (Entity changes - independent of picture)e.g. "Are the planes close to each other" / "Are the planes close to any other?" (Question scope changes)B - One text means something differente.g. "Is the dog/ dogs a Cocker Spaniel?" depends on whether there is more than 1 dog.C - The meaning is somehow similar but one of texts means something slightly different.e.g. "Are any of them stores?"/"Are any of them retailers?" (Similar meaning)D - They have exactly the same meaninge.g. "Does it have color?"/ "Does it have colour?" (Pretty much only applies to BE/ AE spelling?) How similar is the meaning of these two pieces of text ? QuestionText  (and image)Orig.: Is the fireplace lit ? Aft.:  Is the furnace lit ? Rate w/o image:2.33Rate w/ image:1.67JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

8

Fig. 8.

Interface of ’Evaluation of Grammaticality’ for AMT task.

judges to rate each instance and describe results by averaging
and by (a more conservative) majority vote to assign a gold
label. The results show that most (82.0% by averaging and
86.4% by majority vote) crowdworkers think the answer is
unchanged, few (9.6% and 8.1%) think the answer changes,
and the rest (8.4% and 5.5%) are not sure about the change.
We conclude that synonym-based attacks are successful in
remaining undetected by humans.

VIII. CONCLUSIONS

We evaluate the robustness of ten visual dialog models
by attacking question and history with two state-of-the-art
synonym based textural adversarial attack methods. We ﬁnd
that dialog history substantially contributes to model robust-
ness, despite previous results which suggest that history has
negligible effect on model performance, e.g. [49], [17]. We
also show limitations of current synonym-based textual attack
models, and stress the importance of context (both textual
as well as multi-modal) to generate semantically coherent
and grammatically ﬂuent adversarial attacks, which are likely
remain undetected by the user/ software developer. This is
important, since plausible attacks are attacks which can also
occur naturally during the interaction with a user, e.g. when
the user utterance contains an adversarial synonym. While the
observed effects of visually-grounded interpretations in our
human evaluation were relatively small, we do believe that
it is an important future direction. For example, we expect
improved results by using synonym substitution methods based
on visually-grounded word embeddings, e.g. using Visual-
Word2Vec [50]. We also believe that a more focused evaluation
on this issue would show stronger results, e.g. using targeted
contrast sets [51].

REFERENCES

[1] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.

[2] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine learning

at scale,” arXiv preprint arXiv:1611.01236, 2016.

[3] A. Kurakin, I. Goodfellow, S. Bengio et al., “Adversarial examples in

the physical world,” 2016.

[4] A. Ramanathan, L. Pullum, Z. Husein, S. Raj, N. Torosdagli, S. Pat-
tanaik, and S. K. Jha, “Adversarial attacks on computer vision algorithms
using natural perturbations,” in 2017 Tenth International Conference on
Contemporary Computing (IC3).

IEEE, 2017, pp. 1–6.

[5] S. Oviatt, “Breaking the robustness barrier: Recent progress on the
design of robust multimodal systems,” Advances in computers, vol. 56,
pp. 305–341, 2002.

[6] S. Bangalore and M. Johnston, “Robust understanding in multimodal
interfaces,” Computational Linguistics, vol. 35, no. 3, pp. 345–397,
2009.

[7] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh,

and D. Batra, “Visual dialog,” in CVPR, 2017, pp. 326–335.

[8] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and

D. Parikh, “VQA: Visual Question Answering,” in ICCV, 2015.

[9] A. Das, S. Kottur, J. M. Moura, S. Lee, and D. Batra, “Learning
cooperative visual dialog agents with deep reinforcement learning,” in
ICCV, 2017, pp. 2951–2960.

[10] S. Kottur, J. M. Moura, D. Parikh, D. Batra, and M. Rohrbach, “Visual
coreference resolution in visual dialog using neural module networks,” in
Proceedings of the European Conference on Computer Vision (ECCV),
2018, pp. 153–169.

[11] U. Jain, S. Lazebnik, and A. G. Schwing, “Two can play this game:
visual dialog with discriminative question generation and answering,”
in CVPR, 2018, pp. 5754–5763.

[12] Z. Zheng, W. Wang, S. Qi, and S.-C. Zhu, “Reasoning visual di-
alogs with structural and partial observations,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 6669–6678.

[13] Y. Niu, H. Zhang, M. Zhang, J. Zhang, Z. Lu, and J.-R. Wen, “Recursive
visual attention in visual dialog,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
6679–6688.

[14] T. Yang, Z.-J. Zha, and H. Zhang, “Making history matter: History-
advantage sequence training for visual dialog,” in ICCV, 2019, pp. 2561–
2569.

Instructions'Fluent - could this have been produced by a native speaker?''Grammatical - are there any grammar errors, such as verb agreement?'How fluent/grammatical is the text?QuestionTextIs the blanket cleaned ?JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

9

Fig. 9.

Interface of ’Evaluation of Label Consistency’ for AMT task.

[15] J. Qi, Y. Niu, J. Huang, and H. Zhang, “Two causal principles for
improving visual dialog,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2020, pp. 10 860–10 869.
[16] V. Murahari, D. Batra, D. Parikh, and A. Das, “Large-scale pretraining
for visual dialog: A simple state-of-the-art baseline,” in European
Conference on Computer Vision. Springer, 2020, pp. 336–352.
[17] S. Agarwal, T. Bui, J.-Y. Lee, I. Konstas, and V. Rieser, “History for
visual dialog: Do we really need it?” in Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, 2020, pp.
8182–8197.

the Association for Computational Linguistics.

[18] C. Sankar, S. Subramanian, C. Pal, S. Chandar, and Y. Bengio,
“Do neural dialog systems use the conversation history effectively?
the 57th Annual Meeting
an empirical study,” in Proceedings of
of
Florence, Italy:
Association for Computational Linguistics, Jul. 2019, pp. 32–37.
[Online]. Available: https://www.aclweb.org/anthology/P19-1004
[19] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is bert really robust?
a strong baseline for natural
language attack on text classiﬁcation
and entailment,” in Proceedings of the AAAI conference on artiﬁcial
intelligence, vol. 34, no. 05, 2020, pp. 8018–8025.

[20] L. Li, R. Ma, Q. Guo, X. Xue, and X. Qiu, “Bert-attack: Adversarial
attack against bert using bert,” arXiv preprint arXiv:2004.09984, 2020.
[21] N. Narodytska and S. P. Kasiviswanathan, “Simple black-box adversarial
perturbations for deep networks,” arXiv preprint arXiv:1612.06299,
2016.

[22] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, “Boosting

adversarial attacks with momentum,” in CVPR, 2018, pp. 9185–9193.

[23] C. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. L. Yuille,
“Improving transferability of adversarial examples with input diversity,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2019, pp. 2730–2739.

[24] T. Niu and M. Bansal, “Adversarial over-sensitivity and over-stability
strategies for dialogue models,” in Proceedings of the 22nd Conference
on Computational Natural Language Learning, 2018, pp. 486–496.
[25] E. Dinan, S. Humeau, B. Chintagunta, and J. Weston, “Build it break it
ﬁx it for dialogue safety: Robustness from adversarial human attack,” in
Proceedings of the 2019 Conference on Empirical Methods in Natural

Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 4529–4538.
[26] W. E. Zhang, Q. Z. Sheng, A. Alhazmi, and C. Li, “Adversarial attacks
on deep-learning models in natural language processing: A survey,” ACM
Transactions on Intelligent Systems and Technology (TIST), vol. 11,
no. 3, pp. 1–41, 2020.

[27] W. Wang, L. Wang, R. Wang, Z. Wang, and A. Ye, “Towards
a robust deep neural network in texts: A survey,” arXiv preprint
arXiv:1902.07285, 2019.

[28] S. Eger, G. G. S¸ ahin, A. R¨uckl´e, J.-U. Lee, C. Schulz, M. Mesgar,
K. Swarnkar, E. Simpson, and I. Gurevych, “Text processing like humans
do: Visually attacking and shielding nlp systems,” in Proceedings of the
2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), 2019, pp. 1634–1647.

[29] J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi, “Black-box generation of
adversarial text sequences to evade deep learning classiﬁers,” in 2018
IEEE Security and Privacy Workshops (SPW).
IEEE, 2018, pp. 50–56.
[30] M. T. Ribeiro, S. Singh, and C. Guestrin, “Semantically equivalent
adversarial rules for debugging nlp models,” in Proceedings of
the
56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), 2018, pp. 856–865.

[31] M. Iyyer, J. Wieting, K. Gimpel, and L. Zettlemoyer, “Adversarial
example generation with syntactically controlled paraphrase networks,”
in Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), 2018, pp. 1875–1885.

[32] Z. Zhao, D. Dua, and S. Singh, “Generating natural adversarial exam-
ples,” in International Conference on Learning Representations, 2018.
[33] W. C. Gan and H. T. Ng, “Improving the robustness of question
answering systems to question paraphrasing,” in Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics, 2019,
pp. 6065–6075.

[34] Y. Zang, F. Qi, C. Yang, Z. Liu, M. Zhang, Q. Liu, and M. Sun,
“Word-level textual adversarial attacking as combinatorial optimization,”
in Proceedings of
the Association for
the 58th Annual Meeting of
Computational Linguistics, 2020, pp. 6066–6080.

[35] S. Ren, Y. Deng, K. He, and W. Che, “Generating natural language

InstructionsIs it a correct/resonable answer for the question giventhe image?QuestionText  (and image)We give some examples for 'unsure' option."Unsure - the question doesn't make sense given the picture." (e.g. question askingabout "a man" when there is only a child in the picture.)"Unsure - I can't verify the answer given the picture." (e.g. question asking whethersomeone smiles, but it's hard to see.)"Unsure - the question is difficult to understand because it's ungrammatical" (e.g. thequestion is highly ungrammatical and disfluent)"Unsure - the question is ambiguous given the picture." (e.g. the question has morethan one answer)JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

10

A. Full Table of Question Attack

APPENDIX

We show the full table of question attack results including

R@10 in Table IX as supplement of Table I.

B. Detailed Results for Ablation Study

We list the full tables of ablation study in Table X, Table XI,
Table XII and Table XIII, as supplement Table IV, Table V,
Table VI, Table VII respectively.

adversarial examples through probability weighted word saliency,” in
Proceedings of the 57th annual meeting of the association for compu-
tational linguistics, 2019, pp. 1085–1097.

[36] C. Song and V. Shmatikov, “Fooling ocr systems with adversarial text

images,” arXiv preprint arXiv:1802.05385, 2018.

[37] X. Yuan, P. He, X. Lit, and D. Wu, “Adaptive adversarial attack on
scene text recognition,” in IEEE INFOCOM 2020-IEEE Conference on
Computer Communications Workshops (INFOCOM WKSHPS).
IEEE,
2020, pp. 358–363.

[38] H. Chen, H. Zhang, P.-Y. Chen, J. Yi, and C.-J. Hsieh, “Attacking visual
language grounding with adversarial examples: A case study on neural
image captioning,” arXiv preprint arXiv:1712.02051, 2017.

[39] X. Xu, X. Chen, C. Liu, A. Rohrbach, T. Darrell, and D. Song,
“Fooling vision and language models despite localization and attention
mechanism,” in CVPR, 2018, pp. 4951–4961.

[40] H. Shi, J. Mao, T. Xiao, Y. Jiang, and J. Sun, “Learning visually-
grounded semantics from contrastive adversarial samples,” in Proceed-
ings of the 27th International Conference on Computational Linguistics,
2018, pp. 3715–3727.

[41] V. Sharma, A. Kalra, S. C. Vaibhav, L. Patel, and L.-P. Morency, “Attend
and attack: Attention guided adversarial attacks on visual question
answering models,” in Proc. Conf. Neural Inf. Process. Syst. Workshop
Secur. Mach. Learn, vol. 4323, 2018.

[42] J. Morris, E. Liﬂand, J. Lanchantin, Y. Ji, and Y. Qi, “Reevaluating
adversarial examples in natural language,” in Findings of the Association
for Computational Linguistics: EMNLP 2020. Online: Association
for Computational Linguistics, Nov. 2020, pp. 3829–3839. [Online].
Available: https://www.aclweb.org/anthology/2020.ﬁndings-emnlp.341

[43] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John,
N. Constant, M. Guajardo-C´espedes, S. Yuan, C. Tar et al., “Universal
sentence encoder,” arXiv preprint arXiv:1803.11175, 2018.

[44] G.-C. Kang, J. Park, H. Lee, B.-T. Zhang, and J.-H. Kim, “Reasoning
visual dialog with sparse graph learning and knowledge transfer,” in
Findings of the Association for Computational Linguistics: EMNLP
2021, 2021, pp. 327–339.

[45] Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian, “Deep modular co-
attention networks for visual question answering,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 6281–6290.

[46] N. Mrkˇsic, D. OS´eaghdha, B. Thomson, M. Gaˇsic, L. Rojas-Barahona,
P.-H. Su, D. Vandyke, T.-H. Wen, and S. Young, “Counter-ﬁtting word
vectors to linguistic constraints,” in Proceedings of NAACL-HLT, 2016,
pp. 142–148.

[47] X. Yu, H. Zhang, Y. Song, Y. Song, and C. Zhang, “What you see is
what you get: Visual pronoun coreference resolution in dialogues,” in
Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China:
Association for Computational Linguistics, Nov. 2019, pp. 5123–5132.
[Online]. Available: https://www.aclweb.org/anthology/D19-1516
[48] M. Li and M.-F. Moens, “Modeling coreference relations in visual
the European
the 16th Conference of
dialog,” in Proceedings of
Chapter of
the Association for Computational Linguistics: Main
Volume. Online: Association for Computational Linguistics, Apr.
2021, pp. 3306–3318.
[Online]. Available: https://www.aclweb.org/
anthology/2021.eacl-main.290

[49] D. Massiceti, P. Dokania, N. Siddharth, and P. Torr, “Visual dialogue
without vision or dialogue,” in Proceedings of the NeurIPS Workshop
on Critiquing and Correcting Trends in Machine Learning, 2018.
[50] S. Kottur, R. Vedantam, J. M. Moura, and D. Parikh, “Visual word2vec
(vis-w2v): Learning visually grounded word embeddings using abstract
scenes,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 4985–4994.

[51] M. Gardner, Y. Artzi, V. Basmov, J. Berant, B. Bogin, S. Chen,
P. Dasigi, D. Dua, Y. Elazar, A. Gottumukkala, N. Gupta, H. Hajishirzi,
G. Ilharco, D. Khashabi, K. Lin, J. Liu, N. F. Liu, P. Mulcaire,
Q. Ning, S. Singh, N. A. Smith, S. Subramanian, R. Tsarfaty,
E. Wallace, A. Zhang, and B. Zhou, “Evaluating models’
local
decision boundaries via contrast sets,” in Findings of the Association
for Computational Linguistics: EMNLP 2020. Online: Association
for Computational Linguistics, Nov. 2020, pp. 1307–1323. [Online].
Available: https://www.aclweb.org/anthology/2020.ﬁndings-emnlp.117

JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, JULY 2022

11

Orig.R@1

Aft.R@1 [∆]

Orig.R@5

Aft.R@5 [∆]

Orig.R@10

Aft.R@10 [∆]

Orig.NDCG

Aft.NDCG [∆]

Orig.MRR [∆]

Aft.MRR

Pert.

S.S.

Quer.

Question Attack

MCA-I
MCA-H
MCA-I-HGQ
MCA-I-VGH
MCA-I-H

RvA

P1
P1+P2

SLG
SLG+KT

MCA-I
MCA-H
MCA-I-HGQ
MCA-I-VGH
MCA-I-H

RvA

P1
P1+P2

46.6
45.9
50.8
48.6
50.0

49.9

48.8
41.9

49.1
48.7

46.6
45.9
50.8
48.6
50.0

49.9

48.8
41.9

38.2 [-18.0]
40.0 [-12.9]
45.6 [-10.2]
43.3 [-10.9]
45.2 [-9.6]

43.9 [-12.0]

43.5 [-10.9]
37.1 [-11.5]

43.9 [-10.6]
42.6 [-12.5]

36.1 [-22.5]
39.1 [-14.8]
44.2 [-13.0]
41.5 [-14.6]
43.1 [-13.8]

43.6 [-12.6]

42.6 [-12.7]
35.8 [-14.6]

76.3
76.8
81.7
78.7
81.4

82.2

80.2
66.9

81.1
71.3

76.3
76.8
81.7
78.7
81.4

82.2

80.2
66.9

62.7 [-17.8]
67.3 [-12.4]
71.4 [-12.6]
68.0 [-13.6]
69.5 [-14.6]

72.2 [-12.2]

69.2 [-13.7]
57.8 [-13.6]

72.1 [-11.1]
60.8 [-14.7]

63.9 [-16.3]
68.5 [-10.8]
71.6 [-12.4]
68.2 [-13.3]
71.2 [-12.5]

73.2 [-10.9]

71.1 [-11.3]
56.9 [-14.9]

86.6
86.8
90.2
88.6
90.8

91.1

89.7
80.2

90.4
83.4

86.6
86.8
90.2
88.6
90.8

91.1

89.7
80.2

BertAttack

74.1 [-14.4]
76.6 [-11.8]
80.3 [-11.0]
78.4 [-11.5]
80.0 [-11.9]

82.6 [-9.3]

80.7 [-10.0]
71.1 [-11.3]

81.2 [-10.2]
74.4 [-10.8]

TextFooler

74.9 [-13.5]
78.3 [-9.8]
81.2 [-10.0]
78.9 [-10.9]
81.3 [-10.5]

84.2 [-7.6]

82.2 [-8.4]
71.8 [-10.5]

61.5
52.2
60.0
62.6
59.6

56.3

60.0
73.4

63.4
74.5

61.5
52.2
60.0
62.6
59.6

56.3

60.0
73.4

54.9 [-10.7]
48.4 [-7.3]
55.2 [-8.0]
57.3 [-8.5]
54.6 [-8.4]

50.9 [-9.6]

54.2 [-9.7]
67.9 [-7.5]

58.4 [-7.9]
68.2 [-8.5]

53.9 [-12.4]
48.0 [-8.0]
54.4 [-9.3]
56.5 [-9.7]
53.7 [-9.9]

50.2 [-10.8]

53.5 [-10.8]
66.9 [-8.9]

60.0
60.0
64.3
62.2
63.8

64.2

62.9
54.0

63.4
59.9

60.0
60.0
64.3
62.2
63.8

64.2

62.9
54.0

47.7 [-20.5]
51.1 [-14.8]
55.6 [-13.5]
53.3 [-14.3]
54.6 [-14.4]

54.5 [-15.1]

54.1 [-14.0]
46.2 [-14.4]

55.0 [-13.2]
50.3 [-16.0]

47.1 [-20.5]
51.1 [-14.8]
54.8 [-14.8]
52.3 [-15.9]
54.0 [-15.4]

55.3 [-13.9]

54.4 [-13.5]
45.1 [-16.5]

16.7
16.7
17.1
16.7
16.7

17.0

17.4
17.0

17.5
17.3

16.8
17.1
17.0
16.5
16.9

16.9

17.3
17.1

74.4
75.4
74.1
74.3
74.8

74.4

74.2
73.7

73.4
74.6

74.4
74.6
74.4
74.4
74.7

74.9

74.3
73.7

5.2
5.2
5.2
5.2
5.2

5.2

5.2
5.2

5.2
5.2

19.7
19.7
19.9
19.8
19.8

19.9

20.1
19.8

81.1
71.3

49.1
48.7

SLG
SLG+KT

43.1 [-12.2]
41.6 [-14.6]

82.7 [-8.5]
74.9 [-10.2]
TABLE IX
COMPARISON OF PERFORMANCE BEFORE ATTACKING QUESTION (ORIG.) AND AFTER (AFT.) ON DIFFERENT VISDIAL MODELS. IN ADDITION TO
STANDARD METRICS, WE MEASURE THE PERTURBED WORD PERCENTAGE (PERT.), SEMANTIC SIMILARITY (S.S) AND THE NUMBER OF QUERIES (QUER.)
TO ASSESS BERTATTACK VS. TEXTFOOLER. THE relative PERFORMANCE DROP IS LISTED AS [∆]. HIGHLIGHTS INDICATE THE LEAST ROBUST AND MOST
ROBUST MODEL, SUPPLEMENT OF TABLE I.

55.3 [-12.8]
49.8 [-16.9]

73.4 [-9.5]
59.7 [-16.3]

57.8 [-8.8]
67.6 [-9.3]

17.3
17.1

19.9
19.9

74.2
74.6

90.4
83.4

63.4
59.9

63.4
74.5

Orig.R@1

Aft.R@1

Orig.R@5

Aft.R@5

Orig.R@10

Aft.R@10

Orig.NDCG

Aft.NDCG

Orig.MRR

Aft.MRR

Pert.

S.S.

Quer.

Random
Ours

50.0

46.2
45.2

81.4
80.0
TABLE X
EFFECT OF VULNERABLE WORD ATTACK (FULL TABLE) ON MCA-I-H MODEL WITH BERTATTACK, SUPPLEMENT OF TABLE IV.

55.9
54.6

17.0
16.7

56.0
54.6

71.7
69.5

90.8

63.8

59.6

81.4

73.4
74.8

5.2
5.2

Orig.R@1

Aft.R@1

Orig.R@5

Aft.R@5

Orig.R@10

Aft.R@10

Orig.NDCG

Aft.NDCG

Orig.MRR

Aft.MRR

Pert.

S.S.

Quer.

All
Ours

50.0

43.7
45.2

81.4

73.3
69.5

90.8

84.3
80.0

59.6

54.1
54.6

63.8

57.2
54.6

16.7
16.7

74.4
74.8

6.1
5.2

TABLE XI
EFFECT OF STOP WORDS SET (FULL TABLE) ON MCA-I-H MODEL WITH BERTATTACK, SUPPLEMENT OF TABLE V.

ε

0.7
0.5
0.3
0.1

Orig.R@1

Aft.R@1

Orig.R@5

Aft.R@5

Orig.R@10

Aft.R@10

Orig.NDCG

Aft.NDCG

Orig.MRR

Aft.MRR

Pert.

50.0

47.0
45.2
44.6
44.6

81.4

69.2
69.5
69.5
69.5

90.8

79.4
80.0
79.9
80.0

59.6

55.6
54.6
54.1
53.9

63.8

54.1
54.6
54.8
54.8

16.1
16.7
16.9
17.1

S.S.

82.0
74.8
71.8
70.9

Quer.

5.8
5.2
5.1
5.1

TABLE XII
EFFECT OF SEMANTIC SIMILARITY THRESHOLD ε (FULL TABLE) ON MCA-I-H MODEL WITH BERTATTACK, SUPPLEMENT OF TABLE VI.

Orig.R@1

Aft.R@1

Orig.R@5

Aft.R@5

Orig.R@10

Aft.R@10

Orig.NDCG

Aft.NDCG

Orig.MRR

Aft.MRR

Pert.

Raw Attack
+POS
+POS+S.S.(0.5)
+POS+S.S.(0.5)+Gram.

50.0

44.2
44.5
45.2
45.4

81.4

69.8
69.5
69.5
70.9

90.8

80.2
80.0
80.0
81.2

59.6

53.7
53.8
54.6
55.9

63.8

54.9
54.8
54.6
55.1

17.4
17.1
16.7
13.0

S.S.

70.3
70.3
74.8
71.4

Quer.

4.9
5.1
5.2
5.2

TABLE XIII
EFFECT OF DIFFERENT CONSTRAINTS FOR ADVERSARIAL ATTACK (FULL TABLE) ON MCA-I-H MODEL WITH BERTATTACK, SUPPLEMENT OF TABLE VII.

