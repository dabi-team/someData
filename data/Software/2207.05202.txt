2
2
0
2

l
u
J

1
1

]

O
C
.
h
p
-
o
r
t
s
a
[

1
v
2
0
2
5
0
.
7
0
2
2
:
v
i
X
r
a

The Cosmic Graph: Optimal Information Extraction from Large-Scale Structure
using Catalogues

T. Lucas Makinen‚àó
Imperial Centre for Inference and Cosmology (ICIC) & Astrophysics Group, Imperial College London,
Blackett Laboratory, Prince Consort Road, London SW7 2AZ, United Kingdom and
Harvard & Smithsonian Center for Astrophysics, Observatory Building E,
60 Garden St, Cambridge, MA 02138, United States

Tom Charnock‚Ä†
Freelance consultant in statistical modelling

Pablo Lemos‚Ä°
Department of Physics and Astronomy, University of Sussex, Brighton, BN1 9QH, UK and
University College London, Gower St, London, UK

Natalia Porqueres¬ß and Alan Heavens¬∂
Imperial Centre for Inference and Cosmology (ICIC) & Astrophysics Group, Imperial College London,
Blackett Laboratory, Prince Consort Road, London SW7 2AZ, United Kingdom

Benjamin D. Wandelt‚àó‚àó
Sorbonne Universit¬¥e, CNRS, UMR 7095, Institut d‚ÄôAstrophysique de Paris,
98 bis boulevard Arago, 75014 Paris, France and
Center for Computational Astrophysics, Flatiron Institute, 162 5th Avenue, New York, NY 10010, USA
(Dated: July 13, 2022)

We present an implicit likelihood approach to quantifying cosmological information over discrete
catalogue data, assembled as graphs. To do so, we explore cosmological inference using mock
dark matter halo catalogues. We employ Information Maximising Neural Networks (IMNNs) to
quantify Fisher information extraction as a function of graph representation. We a) demonstrate
the high sensitivity of modular graph structure to the underlying cosmology in the noise-free limit, b)
show that networks automatically combine mass and clustering information through comparisons to
traditional statistics, c) demonstrate that graph neural networks can still extract information when
catalogues are subject to noisy survey cuts, and d) illustrate how nonlinear IMNN summaries can be
used as asymptotically optimal compressed statistics for Bayesian implicit likelihood inference. We
reduce the area of joint ‚Ñ¶m, œÉ8 parameter constraints with small (‚àº100 object) halo catalogues by a
factor of 42 over the two-point correlation function, and demonstrate that the networks automatically
combine mass and clustering information. This work utilizes a new IMNN implementation over
graph data in Jax, which can take advantage of either numerical or auto-diÔ¨Äerentiability. We also
show that graph IMNNs successfully compress simulations far from the Ô¨Åducial model at which the
network is Ô¨Åtted, indicating a promising alternative to n-point statistics in catalogue-based analyses.

Keywords: cosmology, large-scale structure, statistical methods, machine learning, graph networks, galaxy
surveys

I.

INTRODUCTION

Modern cosmological analyses typically focus on ob-
taining theory and parameter constraints from com-
pressed summary statistics obtained from Ô¨Åeld data such
as the Cosmic Microwave Background or weak lensing
mass-maps (Alsing & Wandelt 2018, JeÔ¨Ärey et al. 2020,
Tegmark et al. 1997). Recently, Ô¨Åeld-level analyses like

‚àó l.makinen21@imperial.ac.uk
‚Ä† tom@charnock.fr
‚Ä° p.lemos@sussex.ac.uk
¬ß n.porqueres@imperial.ac.uk
¬∂ a.heavens@imperial.ac.uk
‚àó‚àó bwandelt@iap.fr

Leclercq & Heavens (2021), Porqueres et al. (2021), al-
though computationally expensive, have made it possible
to sample the full Ô¨Åeld at the pixel level to ensure all sur-
vey information is accounted for in posterior construction
for cosmological parameters.

However, the data collected by telescopes are often in-
stantly compressed into discrete catalogues of sources,
like galaxies and their underlying dark mater halos, or
cosmic voids (Kreisch et al. 2021, Sutter et al. 2012).
The typical approach taken to analyse galaxy cluster
data is to ‚Äúpaint‚Äù identiÔ¨Åed sources onto a grid and per-
form luminosity peak counts in high-density regions as
a tracer for underlying dark matter. Analyses of these
catalogues usually focus on 2-point information, either
in real or Fourier space. However, these statistics are
only suÔ¨Écient when the underlying Ô¨Åeld is Gaussian,

 
 
 
 
 
 
which is not the case for late-time cosmic web struc-
tures. Finding a statistic with which to capture more
of this information is an active area of research. Exist-
ing methods include the three-point correlation function
(the bispectrum in Fourier space, e.g. Philcox & Ivanov
(2022)), Minkowski functionals (Petri et al. 2013), the
1D probability distribution function (Uhlemann et al.
2020), marked power spectra (Massara et al. 2022), min-
imum spanning trees (Barrow et al. 1985, Naidoo et al.
2022, 2019), and Ô¨Åeld-level sampling (Jasche et al. 2015,
Jasche & Wandelt 2013, Leclercq 2015, Leclercq & Heav-
ens 2021, Porqueres et al. 2021, Ramanah et al. 2019).
However, truncating analyses to power or bispectra al-
most certainly discards information, especially for highly
non-Gaussian Ô¨Åelds, while Ô¨Åeld-level methods quickly be-
come computationally expensive with increasing survey
volume. Likewise, void cosmology constructs correla-
tion functions from void positions and redshifts (Hamaus
et al. 2015) to capture under-dense regions in structure
formation. This sort of analysis usually discards morpho-
logical features of voids, such as void ellipticity, resulting
in a loss of information that could be relevant to the un-
derlying cosmological model (Biswas et al. 2010, Lavaux
& Wandelt 2010, Xu et al. 2019).

Graphs provide a natural way to describe the nonlinear
aspects of large-scale structure (LSS). Dark matter ha-
los and their galaxy clusters can be attributed to nodes
(vertices), while Ô¨Ålaments are traced by smaller halos and
edges connecting neighbouring edges. In this represen-
tation, clustering under gravity can be translated into
higher connectivity or number of edges. Higher order n-
point functions can be computed eÔ¨Éciently for clusters,
while avoiding the cost of computing extraneous connec-
tions across voids. Graph representation of LSS promises
a more modular approach to information quantiÔ¨Åcation,
and compliments the existing body of literature. Mini-
mum spanning trees (MSTs) have been used in cosmolog-
ical analyses since Barrow et al. (1985), and subsequent
studies have investigated using binned halo graph fea-
tures from simulations as cosmological probes (Adami &
Mazure 1999, Alpaslan et al. 2014, Beuret et al. 2017,
Bhavsar & Ling 1988, Colberg 2007, Coles et al. 1998,
Krzewina & Saslaw 1996, Libeskind et al. 2018, Ueda &
Itoh 1997, van de Weygaert et al. 1992). More recently,
Naidoo et al. (2022, 2019) use the minimum spanning
tree (MST) computed from the Quijote simulations to
compute the cosmological information by binning branch
and shape features of the MST computed over the sim-
ulation suite. Yang & Yu (2022) illustrate graph-based
approaches for modelling small-scale halo clustering in
cosmological simulations.

The advent of deep learning in cosmology has made
massive data generation and analysis more tractable.
Many studies have investigated neural techniques for
point estimate cosmological parameter extraction from
cosmological Ô¨Åelds via regression networks trained on
simulation-parameter pairs (Fluri et al. 2019, 2018, Gillet
et al. 2019, Kwon et al. 2020, Matilla et al. 2020, Pan

2

et al. 2020, Prelogovi¬¥c et al. 2021, Ravanbakhsh et al.
2017, Ribli et al. 2018), Ô¨Åeld reconstruction (Dai & Sel-
jak 2022, Jamieson et al. 2022), foreground removal em-
ulation (JeÔ¨Ärey et al. 2022, Makinen et al. 2020), or cos-
mological parameters from graphs (Villanueva-Domingo
& Villaescusa-Navarro 2022) with squared loss. As re-
viewed in Villaescusa-Navarro et al. (2020a), these tech-
niques can estimate the posterior mean of parameters
(see also JeÔ¨Ärey & Wandelt (2020)). This implies they
require simulations drawn from a prior, speciÔ¨Åed at the
time of training, not just near the parameters favored by
the data. This adds to the variability that needs to be
Ô¨Åt by the network.

We take a diÔ¨Äerent approach: we consider halo cata-
logue graphs as our dataset and use Information Max-
imising Neural Networks (IMNNs) to measure the Fisher
information contained in these graphs. IMNNs are neu-
ral networks that compress data to informative non-
linear summaries, trained on simulations to maximise
the Fisher information (Charnock et al. 2018, Makinen
et al. 2021). Training these networks automatically gives
Gaussian approximation uncertainties from the summary
Fisher information, and can make use of all available data
simultaneously, even saturating known Ô¨Åeld-level likeli-
hoods (Makinen et al. 2021). This approach enables us
to use asymptotically optimal nonlinear statistics (Als-
ing & Wandelt 2018, Charnock et al. 2018) to compute
summaries and estimate maximum likelihood parameters
and eÔ¨Écient implicit likelihood inference.

We combine this framework with a graph neural net-
work (GNN) architecture. GNNs are well-suited to dis-
crete and variable-length problems such as molecular
classiÔ¨Åcation, weather forecasting, and even physics (re)-
discovery with symbolic regression (Cranmer et al. 2020,
Lemos et al. 2022), (see Battaglia et al. (2018) for a com-
plete review).

Recent studies have made use of IMNNs for cosmology
(Fluri et al. 2022, Fluri et al. 2021, Makinen et al. 2021),
and highly non-Gaussian problems, such as galaxy type
identiÔ¨Åcation from multiband images (Livet et al. 2021).
However, previous implementations relied on computing
Fisher statistics for data with a Ô¨Åxed input size. Here,
using GNNs, we extend the framework to a much more
general class of problems. We will refer to graph IMNNs
as gIMNNs.

We show how gIMNN summaries from catalogue
graphs compares to traditional cosmological techniques
with respect to information extraction using the Quijote
halo catalogues (Villaescusa-Navarro et al. 2020b). We
illustrate that by encoding physical symmetries and more
descriptive graph attributes in the IMNN framework, we
can extract more information from limited catalogues
than traditional 2-point statistics. After reviewing the
IMNN framework in the context of variable-length graph
data, we present our halo catalogue graph representation
and GNN architecture.

We investigate cosmological information extraction us-
ing increasingly ornate graph representations. We Ô¨Årst

compare information as a function of increasing GNN
depth and graph connectivity on both invariant and non-
invariant graphs, and show that gIMNNs consistently
extract more information than the 2-pt function. We
next show that decorating graph nodes with mass fur-
ther increases information extraction. Third, we explore
the information stored in graph cardinality (the number
of nodes or objects and edges connecting them) in the
context of the halo mass function. Next, we proceed to
a more realistic case in which catalogue construction is
subject to various levels of uncertainty in the halo mass
determination. We conclude by showing how trained
gIMNN summaries can be used as optimal compressors
in simulation-based inference density estimation. We in-
clude supplementary descriptions of graph assembly and
network generalization in Appendix B.

II. LARGE-SCALE STRUCTURE AS A GRAPH

FIG. 1. Dark matter halo graph representation of large-scale
structure, constructed from a single Quijote simulation. The
largest halos (grey) with a mass Mi > 1.5√ó1015 M(cid:12) trace the
largest physical scales, here shown coloured by the log of the
halo‚Äôs mass, and connected to all neighbours within a radius of
rconnect = 200 Mpc. Smaller halo masses Mi > 1.1 √ó 1015 M(cid:12)
(light grey) trace smaller scale clustering. The box encases a
cosmological comoving volume of (1Gpc)3.

Graphs provide a natural language with which to de-
scribe the cosmic web. Dark matter halos are attributed
to nodes (vertices), while Ô¨Ålaments are traced by smaller
halos and edges, illustrated in Figure 1.
In this repre-
sentation, clustering under gravitational interactions can
be translated into higher edge cardinality (number of
edges). Higher order n-point functions can be computed
eÔ¨Éciently for clusters, while avoiding the cost of comput-
ing extraneous connections across voids. Void catalogues

3

(where edges would correspond to the walls separating
the voids) can likewise be assembled into the dual of a
halo graph. Graph construction also allows extra infor-
mation, such as the halo masses, to be added in the form
of node and edge labels, unlike correlation functions.

A. Graph Notation

We deÔ¨Åne a graph explicitly as a tuple G = (u, V, E),
following the notation in Battaglia et al. (2018). The
u is a global attribute of the graph,
a label or
i.e.
global parameter value.
V = {vi}i=1:N v is the set
of graph nodes, with cardinality N v. The edge set
E = {(ek, rk, sk)}rk=i,k=1:N e, indexed by k = 1 : N e,
is comprised of vectors ek of cardinality N e, which may
be directed, connected via receiving and sending indices
between nodes, rk and sk. Senders and receivers can be
equivalently parameterized by an adjacency matrix Aij
in which i and j index sender and receiver nodes, respec-
tively. Each node, indexed by i = 1 : N v, has a set of
edges, E(cid:48)
k, rk, sk)}rk=i,k=1:N e , connected to it via
a subset of senders and receivers. The full set of nodes is
deÔ¨Åned as V = {vi}i=1:N v , where each node vi is a vector
of features. In a physical system of particles, one might
represent V as a set of individual particles‚Äô attributes,
like mass, position, and velocity, with edges expressing
interactions, such as forces, between particles. A global
attribute of a graph might be a classiÔ¨Åcation label, such
as in molecule or cluster classiÔ¨Åcation (Kipf & Welling
2016, Satorras et al. 2021). Careful data representation
on graphs can vastly simplify physical problems via in-
ductive biases and symmetry capture (see e.g. Battaglia
et al. 2018, Cranmer et al. 2020, Lemos et al. 2022).

i = {(e(cid:48)

B. Halo Graphs

We deÔ¨Åne a dark matter halo graph G =
(u, V halo, Ehalo), constructed from a catalogue for a sin-
gle realisation of the universe. We can equivalently deÔ¨Åne
its dual, H = (u, V void, Evoid), from a void catalogue.
Note that if we assign global cosmological parameters to
u, G and H share this property. Hereafter we will focus
on graphs from halo catalogues.

The graph framework allows the cardinality of a cos-
mological graph‚Äôs nodes and edges to vary as a function
of cosmological or survey parameters, reÔ¨Çecting the often
strong dependence of the abundance of clusters on cos-
mological parameters. When assembling a graph from a
halo catalogue, we choose to vary two physical param-
eters: a mass cut, Mcut, and a linking radius, rconnect.
A halo i with a mass above Mcut is connected to a halo
j if the absolute distance between halos i and j is less
|dij| < rconnect. We display the same
than rconnect, i.e.
catalogue at two mass cuts in Figure 1. A conservative
Mcut = 1.5 √ó 1015M(cid:12) (dark points) contains the heaviest
halos and traces the largest scales, while smaller masses

(Mcut = 1.1 √ó 1015M(cid:12), light points) trace smaller scales.
Each graph is connected by rconnect = 200 Mpc.

Graphs can be assembled from halo catalogues in one of
two ways: as non-invariant or as invariant graphs. Non-
invariant graphs have positions, p, as node labels, set-
ting vi = pi, with edges labelled as the relative distances
between halos, dij. This graph is not invariant under
translations and rotations, as the node values are pinned
to the underlying simulation grid. Invariant graphs have
only relative positional information, stored in the edges.
The cosmological models that we wish to constrain are
invariant to rigid Euclidean group rotations and transla-
tions of the large-scale structure. In this work we include
both representations for completeness.

1. Node Features

In the invariant representation, graph nodes are ‚Äòdec-
orated‚Äô with either an indicator vi = vi = 1 in the un-
decorated case or the halo‚Äôs scalar mass, vi = vi = Mi.
In the non-invariant case, nodes are also decorated with
position vi = (Mi, pi). We describe graph construction
and padding details in Appendix B.

To construct invariant graphs, we impose translational
symmetry by attributing functions of relative positions
between halos on the edges. We compute the vector
separations dij = pi ‚àí pj between all halos and do not
link halos directly if |dij| > rconnect. For rotational in-
variance, we adopt Villanueva-Domingo & Villaescusa-
Navarro (2022)‚Äôs notation and Ô¨Årst compute the unit vec-
tors sij = dij/|dij| and ni = (pi ‚àí ¬Øp)/|pi ‚àí ¬Øp| where ¬Øp is
the centroid (or reference halo position). We then com-
pute the direction cosines aij = ni ¬∑ nj and bij = ni ¬∑ sij.
The normalized edge features for invariant halo graphs
are then

eij = [|dij|/rconnect, aij, bij] ,

(1)

whilst for non-invariant graphs, eij = |dij|/rconnect.

3. Global Features

A halo graph‚Äôs global features can be any quantity that
describes the global properties of the system, in this case
conÔ¨Åguration or cosmology parameters. In a regression
case, one might wish to label each halo graph simula-
tion with a set of cosmological or hydrodynamical pa-
rameters, as done in Villanueva-Domingo & Villaescusa-
Navarro (2022), and Ô¨Åt a neural network to minimise
some distance measure between the network output and
these parameters. Here, global properties will be arbi-
trary nonlinear summaries of cosmology, learned in an

4

unsupervised manner as a function of the graph‚Äôs at-
tributes using information maximising neural networks.

III.

INFORMATION MAXIMISING NEURAL

NETWORKS

The graph framework allows for a modular study of the
cosmological information embedded in large-scale struc-
ture. We next review IMNNs as a tool for information
extraction, as well as optimal compression for graphs as-
sembled from cosmological surveys. The IMNN frame-
work is presented in full in Charnock et al. (2018)with de-
velopmental updates discussed in Makinen et al. (2021),
but we review the formalism here for completeness and
introduce new aspects to the technique. The sharper
the peak of an informative likelihood function L(d|Œ∏) for
some Ô¨Åxed data d with nd data points and nŒ∏ parame-
ters at a given value of Œ∏, the more informative Œ∏ is about
the data. The Fisher information matrix describes how
much information d contains about the parameters, and
is given as the second moment of the score of the likeli-
hood

(cid:90)

FŒ±Œ≤ =

dd L(d|Œ∏)

‚àÇ ln L(d|Œ∏)
‚àÇŒ∏Œ±

‚àÇ ln L(d|Œ∏)
‚àÇŒ∏Œ≤

,

FŒ±Œ≤ = ‚àí

(cid:28) ‚àÇ2 ln L
‚àÇŒ∏Œ±‚àÇŒ∏Œ≤

(cid:29) (cid:12)
(cid:12)
(cid:12)Œ∏=Œ∏fid

,

evaluated at some Ô¨Åducial parameters. A large Fisher
information for a set of data indicates that the data is
very informative about the model parameters attributed
to it. Fisher forecasting for a given model is made pos-
sible by the information inequality and the Cram¬¥er-Rao
bound (Cram¬¥er 1946, Rao 1945), which states that the
minimum variance of the value of an estimator Œ∏ is given
by

(cid:104)(Œ∏Œ± ‚àí (cid:104)Œ∏Œ±(cid:105))(Œ∏Œ≤ ‚àí (cid:104)Œ∏Œ≤(cid:105))(cid:105) ‚â• F‚àí1
Œ±Œ≤ .

(4)

We will write the compression as a function f : d ‚Üí x.
For large datasets, data compression is essential for infer-
ence to avoid the curse of dimensionality. The MOPED
formalism (Heavens et al. 2000) gives optimal score com-
pression for cases where the likelihood and sampling dis-
tributions are exactly Gaussian.

IMNNs are neural networks that perform data com-
pression and compute the Fisher information of a data
set. Such compression is possible even if the data like-
lihood is unknown or intractable, simply based on hav-
ing simulations of the data at a given Ô¨Åducial parameter
point and local information about how the parameters
change the data distribution. It can be shown (Wandelt,
2022, in preparation) that the optimality of the IMNN
summaries holds for any unknown or intractable data
likelihood even though the IMNN maximizes Fisher infor-
mation assuming the parameter-independent covariance

(2)

(3)

2. Edge Features

and can be written as

5

FIG. 2. Cartoon of the graph-based information maximising neural network scheme. For each i = 1, . . . ns dark matter Ô¨Åeld
realization, halo catalogues are computed from a density Ô¨Åeld and assembled into a connected graph. A GNN block then
computes edge and node updates (green) outlined in Section III A, pooling graph attributes to compute global summaries,
x = u. This process is repeated for simulations at the Ô¨Åducial parameter values, as well as at Œ∏¬± for numerical derivative
calculation via equation (10). The output of the IMNN is the Fisher information matrix, computed via equation (8). The
network is trained via gradient back-propagation, with the det F and Cf contributing to the scalar loss function.

form of the Gaussian likelihood for the IMNN summaries

‚àí 2 ln L(x|d) = (x ‚àí ¬µf (Œ∏))T C‚àí1

f (x ‚àí ¬µf (Œ∏))

(5)

where

¬µf (Œ∏) =

1
ns

ns(cid:88)

i=1

xs
i

(6)

i , with {xs

is the mean of the compressed summaries xs
i |i ‚àà
[1, ns]}‚Äô and we assume a parameter-independent covari-
ance matrix. Here i indexes the random initialisation
of ns simulations, and the superscript s denotes quanti-
ties derived from simulations, unlike quantities without
the superscript s which are derived from actual observa-
tions. The summaries are obtained via simulation of data
i (Œ∏, i) via the compression scheme f : ds
i = ds
ds
i ‚Üí xs
i .
The covariance of the summaries is computed from the
data as well:

(Cf )Œ±Œ≤ =

1
ns ‚àí 1

ns(cid:88)

(xs

i ‚àí ¬µf )Œ±(xs

i ‚àí ¬µf )Œ≤.

(7)

i=1

Note that this covariance is assumed to be independent
of the parameters, which, whilst not strictly true, is en-
forced by regularisation during the Ô¨Åtting of the IMNN.
A Fisher matrix can then be computed from the likeli-
hood in equation (5):

FŒ±Œ≤ = tr[¬µT

f,Œ±C‚àí1

f ¬µf,Œ≤],

(8)

where we introduce the notation y,Œ± ‚â° ‚àÇy/‚àÇŒ∏Œ± for par-
tial derivatives with respect to parameters. If the com-
pression function f is a neural network parameterized
by layer weights w(cid:96) and biases b(cid:96) (with (cid:96) the layer in-
dex), the summaries (and respective mean and covari-
ance) then become functions of these new parameters

x(Œ∏) ‚Üí x(Œ∏, w(cid:96), b(cid:96)). To evaluate equation (8) for a neu-
ral compression, we must compute
ns(cid:88)

¬µf,Œ± =

‚àÇ
‚àÇŒ∏Œ±

1
ns

i=1

xs Ô¨Åd
i

.

(9)

One way of computing the derivatives of the summary
means with respect to the parameters is to deÔ¨Åne a Ô¨Ånite
diÔ¨Äerence gradient dataset by altering simulation Ô¨Åducial
values by a small amount, yielding

(cid:19)s Ô¨Åd

(cid:18) ‚àÇ ÀÜ¬µi
‚àÇŒ∏Œ±

‚âà

1
ns

ns(cid:88)

i=1

xs Ô¨Åd+
i
‚àÜŒ∏+

‚àí xs Ô¨Åd‚àí
i

Œ± ‚àí ‚àÜŒ∏‚àí
Œ±

.

(10)

To prevent extra information being extracted from ac-
cidental correlation in limited sized data sets, reported
statistics need to be computed on a validation set of sim-
ulations, which is unlikely to share the same accidental
correlations as the Ô¨Åxed training set. An alternative ex-
plored in Makinen et al. (2021) is to calculate the adjoint
gradient of the simulations as well as the derivatives of
the network parameters with respect to the simulations:

¬µf,Œ± =

1
ns

ns(cid:88)

i=1

(cid:18) ‚àÇx
‚àÇŒ∏Œ±

(cid:19)s Ô¨Åd

i

=

1
ns

ns(cid:88)

nd(cid:88)

i=1

k=1

‚àÇxs Ô¨Åd
i
‚àÇdk

‚àÇds Ô¨Åd
i
‚àÇŒ∏Œ±

.

(11)
If the gradient of the simulations can be computed ef-
Ô¨Åciently, this technique for computing the compression
Fisher information eliminates the need for hyperparam-
eter tuning of the Ô¨Ånite diÔ¨Äerence derivative size, ‚àÜŒ∏Œ±.

The network is trained to maximise the logarithm of
the determinant of the Fisher information, computed via
equation (8). As described in Charnock et al. and Livet
et al., the Fisher information is invariant to nonsingu-
lar linear transformations of the summaries. To remove

F!"edge	updatenode	updateuglobalsC#ùëô=1,‚Ä¶ùëõ$ùëë%$	‚ÄôùúÉ(={ùúÉ)*+,ùúÉ¬±}ùùÅ#,!density	0ield	simulatorgraph	assemblyGNN+	noise	survey	cutinteraction	network	blocksaggregation√ó	ùëÅ!"#x$this ambiguity, a term driving covariance to the identity
matrix is added

ŒõC =

1
2

(cid:18)

||(Cf ‚àí 1)||2

F +

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)(C‚àí1
(cid:12)

(cid:12)
(cid:12)
f ‚àí 1)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:19)

2

F

,

(12)

‚àö

where ||A||F ‚â°
This yields the loss function

tr AAT denotes the Frobenius norm.

Œõ = ‚àí| det F| + rŒõC ŒõC,

with regularization parameter

rŒõC =

ŒªŒõC
ŒõC + exp(‚àíŒ±ŒõC)

,

(13)

(14)

where Œª and Œ± are user-deÔ¨Åned parameters. When the
covariance is far from identity, the rŒõC function is large
and the optimization focuses on bringing the covariance
and its inverse back to identity. The network is trained
until the Fisher information stops increasing for a pre-
determined number of iterations. We stress that the
value of F reported as an information metric, however, is
the one computed via Eq. 8, computed over a validation
set of simulations in the case of a Ô¨Ånite set of data.

A. Graph Neural Networks

A graph neural network (GNN) block typically consists
of three update functions, œÜ = (œÜu, œÜv, œÜe), and three ag-
gregation functions, œÅ = (œÅu, œÅv, œÅe), applied sequentially
to a graph tuple G = (u, V, E). A single graph block (cid:96) is
comprised of several update steps to its elements:

1. Edge update: Each edge is parameterized by a func-
tion œÜ(cid:96)+1
e which takes as inputs its connected nodes,
previous value, and graph global properties and
yields another edge:

6

The order of operations of these updates is Ô¨Çexible, but
usually applied in the order displayed above, and in the
GNN block in Fig. 2. This framework allows œÜ functions
to be arbitrarily parameterized as neural networks with
nonlinear activation functions. Aggregation functions œÅ
must be allowed to take a variable number of arguments,
so are usually chosen to be permutation-invariant opera-
tors such as the mean, summation, or maximum (Bron-
stein et al. 2021). Stacking (cid:96) = 1 : N int GNN blocks
allows node information to be propagated to and from
neighbours N int degrees away, where int refers to inter-
actions. In this work all GNN blocks operate over the
entire graph. However, one could also devise surrogate
GNN blocks that operate on small scales and then pass
information up to larger scales via an aggregation func-
tion œÅsmall‚Üílarge, such that one GNN network is not re-
sponsible for operating on nodes of all scales in a densely-
populated graph. We detail our speciÔ¨Åc implementation
and architecture in Section IV A 2.

The GNN framework is readily incorporated into the
IMNN formalism, since the details of the neural network
architecture only serve to better capture how the data
changes with the parameters.
Instead of predicting an
output graph or class label, as in Battaglia et al. (2018),
our Ô¨Ånal global update œÜu outputs IMNN summaries,
x = u(cid:96)=N int
. This new aspect to the IMNN formalism
is the ability to operate over variable-length data inputs,
rendering the cardinality of input graphs, ndata = N v
and N e informative features of the data. A stochastic
system might yield a diÔ¨Äerent number of discrete parti-
cles for diÔ¨Äerent parameters, meaning the number of data
becomes a descriptor of the statistical model. This allows
for a study of information I = 1
2 ln det F as a function of
N v and enables much more Ô¨Çexible data modelling.

e(cid:96)+1
ij = œÜ(cid:96)+1

e

(v(cid:96)

i , v(cid:96)

j, e(cid:96)

iju(cid:96)),

(15)

IV. COSMOLOGICAL PARAMETER
INFERENCE WITH HALO CATALOGUES

where v(cid:96)
i and v(cid:96)
dexed by (sk, rk).

j are sender and receiver nodes in-

2. Node update: Each node is then parameterized by

a function œÜ(cid:96)+1

v
v(cid:96)+1
i = œÜ(cid:96)+1

v

and outputs a new node:
i , u(cid:96)(cid:1) ,

(cid:0)œÅe‚Üív(E(cid:96)+1

), v(cid:96)

i

(16)

Here a permutation-invariant aggregation opera-
tion œÅe‚Üív(E(cid:96)+1
) pools the neighbourhood of edges
E(cid:96)+1
connected to node i into a Ô¨Åxed-sized vector
i
to feed into the update function.

i

3. Global update: The global features of the graph are

then updated with a function œÜ(cid:96)+1
u :
u(cid:96)+1 = œÜu (cid:0)œÅe‚Üív(E(cid:96)+1), œÅv‚Üíu(V (cid:96)+1), u(cid:96)(cid:1) ,

(17)

where the graph‚Äôs edge (E(cid:96)+1) and node (V (cid:96)+1)
sets are pooled into Ô¨Åxed-sized vectors for the
global update.

FIG. 3. Information as a function of large-scale structure data
representation. Cosmological parameter information extrac-
tion eÔ¨Éciency increases as information is added and propa-
gated throughout the halo graph with increased connections
and message-passing steps. The second and third graph show
message propagation from the top left node after N int = 2
update steps.

connected	graph2pt	function+	interaction	steps+	node	decorationùëÅpt	function4ield‚àílevelimproved	information	extractionHere we consider applications of our graph IMNNs on
realistic cosmological problems. Even future astronomi-
cal studies will not be able to image complete dark mat-
ter overdensity Ô¨Åelds of large-scale structure. However,
discrete galaxy and void catalogues can be assembled
as tracers of structure. DiÔ¨Äerent LSS realizations from
stochastic initial conditions will have diÔ¨Äerent numbers
of halos and voids, posing a problem for usual Ô¨Åxed-size
neural networks, (which are themselves problematic for
inference unless treated correctly). We explore informa-
tion extraction as a function of graph connectivity in the
context of two-parameter inference for the matter density
parameter, ‚Ñ¶m, as well as œÉ8, the r.m.s. Ô¨Çuctuation of
density perturbations at the 8 h‚àí1 Mpc scale. Both ‚Ñ¶m
and œÉ8 parameterize the distribution of matter in cos-
mological simulations, so the graph topology should be
sensitive to changes in parameters.

The more descriptive a graph is, the more information
one intuitively expects to extract. We demonstrate this
trend by Ô¨Årst considering undecorated graphs, annotated
with just positions of and relative distances between ha-
los. We show that information extraction eÔ¨Éciency in-
creases as graph connectivity increases. We then show
that information increases further when halo masses are
included as node features.

The closest existing statistics to this representation
are n-point correlation and mass functions, and we show
how information increases beyond the 2-point correlation
function (2PCF) with the same catalogue as graph con-
nectivity is increased, as illustrated in the cartoon in 3.

A. Halo catalogues

Here we describe the simulated catalogues that are
used for training and validation.The Quijote Halo cat-
alogues are assembled from 3D overdensity Ô¨Åelds at the
present day (z = 0) using the Friends of Friends (FoF)
algorithm (Davis et al. 1985). Attributes computed by
the Ô¨Ånder are halo masses Mi, positions pi, and velocities
vi. Each full simulation yields a catalogue of ‚àº400,000
halos on average.

1. Graph Inputs Assembly

We initially connect graphs of a manageable size by
varying two hyperparameters. We Ô¨Årst make a minimum
mass cut Mcut to be considered in the catalogue. Nodes
are then connected to one another within a Euclidean
distance rconnect. We initially explore the noise-free limit
with known masses and Ô¨Åx Mcut = 1.5√ó1015M(cid:12) to assess
the pure information limit of the catalogues. We add
noise to the analysis in Section VI. This cut yields N v ‚àà
[70, 140] halos per catalogue. We visualize two graphs in
Appendix B, Ô¨Ågure 2.

We then assemble the truncated catalogues into non-
invariant and invariant graphs, as outlined in Section

7

II B. We initialize each graph‚Äôs global property with a
tuple u(cid:96)=0 = (arcsinh N v, arcsinh N e) summarising the
cardinality of the graphs. As described in Battaglia
et al. (2018), Lemos et al. (2022), Villanueva-Domingo &
Villaescusa-Navarro (2022), imposing symmetries in data
representation can improve GNN training, since the net-
work can focus on learning relevant correlations to the
problem, as opposed to re-learning symmetry. We test
this notion in the context of information extraction.

2. Graph Neural Network Architecture

We choose to parameterize our GNN functions with
simple fully-connected networks. Each œÜ function is
a dense network with two layers of 50 hidden neurons
and gelu activations (Hendrycks & Gimpel 2016). We
built a custom aggregation function akin to that found
in Villanueva-Domingo & Villaescusa-Navarro (2022), in
which mean, max, sum, and variance are computed over
node and edge attributes and then concatenated, since it
is not known a priori which function is most useful for in-
formation extraction. To aggregate e.g. the set of edges
Ei in a neighbourhood around node i we compute:

Ô£Æ

(cid:77)

j‚ààEi

eij =

Ô£∞max
j‚ààEi

eij,

(cid:88)

j‚ààEi

eij,

eij

Ô£π

Ô£ª

(18)

(cid:80)

j‚ààEi
(cid:80)

j‚ààEi

We additionally modify these operators with a trainable
arcsinh layer e.g. for edge-to-node aggregation:

Ô£´

Ô£∂

œÅ(cid:96)+1
e‚Üív(E(cid:96)

i ) = a arcsinh

Ô£≠b

e(cid:96)
ij + c

Ô£∏ + d,

(19)

(cid:77)

j‚ààE(cid:96)
i

where (a, b, c, d) are scalar learnable parameters initial-
ized as (1, 1, 0, 0) to ensure numerical stability for gradi-
ent calculation. All networks are trained with an Adam
optimizer with a learning rate set to 0.0001 and coupling
parameters Œª = 10 and Œ± = 0.95. We construct our
graphs and GNNs using the jraph (Godwin et al. 2020)
and Flax (Heek et al. 2020) libraries, which are both
Jax-compatible.

We train our gIMNNs by splitting the Quijote sim-
ulations into equally-sized training and validation sets.
Gradient descent is performed on training data, while
reported compression statistics (Fisher information) are
computed for the validation set using equation (8). Both
training and validation sets comprise of ns = 500 Ô¨Ådu-
cial simulations at Œ∏Ô¨Åd = (‚Ñ¶m, œÉ8) = (0.3175, 0.834) and
nd = 250 seed-matched derivative simulations perturbed
by Œ¥Œ∏ = (0.01, 0.015), yielding nd √ó 2 √ó 2 = 1000 simula-
tions (see Villaescusa-Navarro et al. (2020b) for details).
Training on the loss deÔ¨Åned in Eq 13 is performed until
the validation Fisher information stops increasing signif-
icantly for 1000 epochs.

B. Undecorated Graphs vs. n-point statistics

We Ô¨Årst consider an undecorated graph representa-
tion of halo catalogues without descriptive node features.
Drawing more edges between nodes increases the con-
nectivity of the graph, allowing information from a sin-
gle node to reach more distant neighbours. Undecorated
graphs of increasing connectivity are analogous to tradi-
tional n-point statistics computed for galaxy catalogues.
3-point statistics for example consider triangular group-
ings of galaxies, and generally oÔ¨Äer tighter constraints
from large-scale structure data than the 2PCF, as shown
in (Hahn et al. 2020).

1. Comparison to 2-point Correlation information

As a benchmark for our analysis, we also compare the
information content obtained from the 2-point correla-
tion function (2PCF), Œæ(r), of our small halo catalogues,
the real-space equivalent to the Quijote power spectrum
computed in Villaescusa-Navarro et al. (2020b). For a
statistic Q = Œæ(r), the Fisher information is given by
(Tegmark et al. 1997):

Fij =

1
2

(cid:40)

(cid:34)(cid:32)

tr

C‚àí1

‚àÇQ
‚àÇŒ∏i

‚àÇQ
‚àÇŒ∏j

T (cid:33)

(cid:32)

+

‚àÇQ
‚àÇŒ∏i

T ‚àÇQ
‚àÇŒ∏j

(cid:33)(cid:35)(cid:41)

,

(20)
where C is estimated from simulations at the Ô¨Åducial and
the derivatives are approximated numerically via

‚àÇQ
‚àÇŒ∏i

‚âà

i ) ‚àí Q(Œ∏‚àí
Q(Œ∏+
i )
i ‚àí Œ∏‚àí
Œ∏+

i

(21)

We use the full suite of 500 derivative and 1000 Ô¨Åducial
halo catalogue simulations to compute Eq 20, and cru-
cially make the same conservative mass cut. We bin dis-
‚àö
3 Gpc3, yielding
tances into 10 Ô¨Åxed bins between 0 and 3
a covariance matrix of size 1002. For the 2PCF we obtain
a Fisher information of 2.28 √ó 106, or Shannon entropy
of 7.319 nats.

2.

Increasing graph connectivity

Here we construct graphs of varying edge cardinality
by varying a physical connection parameter, rconnect ‚àà
{100, 200, 300} Mpc, yielding graphs with average edge
number (cid:104)N e(cid:105) ‚àà {27, 150, 437} respectively. We also com-
pare information as a function of increasing GNN inter-
action blocks, N int, for all rconnect, for both non-invariant
and invariantly-structured halo graphs. Network archi-
tecture is identical for each rconnect value, and initialized
by the same random seed.
Results. We display information extraction as a func-
tion of graph connectivity in Figure 4, computed for the
validation simulation set. We also display the catalogue‚Äôs

8

2PCF information (dashed line) as a benchmark.
In-
variant graphs (top row) train more smoothly than non-
invariant graphs (bottom row) since the network does
not have to learn relationships from position values on
the nodes. A single GNN block struggles to extract in-
formation with rconnect = 100 Mpc in the non-invariant
representation (lower left), but plateaus at ‚âà 2.8√ó107 for
all other conÔ¨Ågurations. This behavior is likely because in
most cases the network is both descriptive enough and is
able to capture patterns at much larger scales by attend-
ing to halos higher degrees away. The common saturation
value across multiple network and connectivity combina-
tions indicates that undecorated graphs typically contain
det FIMNN/ det F2PCF ‚âà 10 ‚àí 15 times more information
than the 2PCF, regardless of connectedness, provided a
descriptive enough network can extract it. The graph
representation improves marginal constraints in ‚Ñ¶m by a
factor of ‚âà 2 and in œÉ8 by a factor of ‚âà 11, displayed in
Figure 5.

We initially hypothesized that increasing network com-
plexity would increase the information extraction. How-
ever, we found that we obtain essentially equivalent in-
formation for any combination of rconnect and N int. It is
clear that for this small number of halos the information
is easily saturated at any level, but with more halos in
the catalogue, as discussed in Section VIII, hierarchical
clustering at the graph or network level might pull out
more information from e.g.
smaller mass scales. This
exploration is reserved for a future work.

Since all suÔ¨Éciently-connected representations obtain
the same information, we proceed in our experiments
with invariant graphs connected with rconnect = 200 Mpc
and N int = 2, since this combination resulted in the
smoothest and fastest training (4 minutes) on a single
NVIDIA-v100 GPU.

C. Decorated Graphs: Incorporating Halo Mass

We next decorate each halo node with the correspond-
ing (noise-free) halo mass. We widen the network‚Äôs hid-
den dimension to 64 and train both decorated and undec-
orated graphs with the same patience settings. Training
was restarted for each three times after plateau to ensure
saturation. The same network architecture is able to ex-
tract 2.3 times more information when decorated with
masses, corresponding to 42 times more information than
the 2PCF. We display the corresponding Fisher ellipses
in Figure 5, along with isomass lines of the halo mass
function, described in Section V A.

V. MASS CUT INFORMATION

We next investigate how much information is contained
in the mass cut, Mcut which determines halo number
N v. We compare two graph assemblies: one with a
Ô¨Åxed number of the most massive halos N v
Ô¨Åxed = 105,

9

FIG. 4. Information extraction comparison for undecorated invariant (top row ) and non-invariant (bottom row ) halo graphs
over validation simulations. Columns indicate how many graph network update steps are performed, and colours indicate
graphs assembled with diÔ¨Äerent physical connection radii. The information obtained from the 2PCF computed from the same
catalogues with the same mass cut is shown as a dashed line. For suÔ¨Éciently descriptive networks (N int > 1), invariantly-
assembled graphs train faster, but all graphs connected for r > 100 Mpc saturate at det F ‚âà 2.8 √ó 107, or ‚âà 15 times higher
than the 2PCF. Jagged dips in validation curves indicate points of restarting network training after patience criteria were met.

i.e.
the average number of halos across variable sized
halo catalogues with Ô¨Åxed mass cut (Mcut = 1015M(cid:12),
and another where the cardinality is allowed to vary
with Mcut = 1.5 √ó 1015M(cid:12). For each case we com-
pare decorated and undecorated graphs, and the epis-
temic and aleatoric errors associated with each represen-
tation. For epistemic uncertainty we train Ô¨Åve gIMNNs
with N int = 2 with diÔ¨Äerent initialization of network pa-
rameters, whilst Ô¨Åxing the training and validation sets,
displaying the best Fisher obtained as well as the mean
and standard deviation of ln det F over the Ô¨Åve runs. For
aleatoric uncertainty we Ô¨Åx the gIMNN weight values on
initialization and train on Ô¨Åve diÔ¨Äerent randomised train-
validation equal-sized splits of the available simulations.
For both aleatoric and epistemic cases, the same Ô¨Åve ran-
dom seeds and network architecture is used across all
data conÔ¨Ågurations.
Results. We display results in Table I. Fixing catalogue
size eliminates halo number as a useful feature to the net-
work, evidenced by much lower information yields. With-
out mass decoration there is less information in the graph
data so the data can be Ô¨Åt by more possible functions by
the network, so the aleatoric uncertainty (variability of
the data) is increased over the decorated case. However,
the network has to Ô¨Åt a simpler compression since there
are fewer relevant features without mass, so epistemic
error (variability in possible network weights) decreases,
compared to the decorated case). Fixed-length graphs do

not exceed the 2PCF information until annotated with
mass information.

When catalogues are allowed to vary with a physical
mass cut, much more information can be extracted from
both decorated and undecorated graphs. Including mass
information on the nodes again increases the variability
incurred across diÔ¨Äerent network initializations, but de-
creases the aleatoric uncertainty since we better describe
the likelihood with more information.

catalogue N v

Ô¨Åxed

variable

graph assembly
without mass
with mass
2PCF
without mass
with mass
2PCF

ln det F

epistemic

aleatoric
5.03 ¬± 0.47 5.98 ¬± 1.06
12.43 ¬± 1.44 12.39 ¬± 0.22

9.74

14.19

17.89 ¬± 0.33 17.66 ¬± 0.27
17.40 ¬± 0.57 17.85 ¬± 0.12

TABLE I. Comparison of extracted information from Ô¨Åxed-
and variable-length catalogues.
Information was extracted
with rconnect = 200 Mpc, and networks with N int = 2 across 5
identical initialisations. We display each conÔ¨Åguration‚Äôs best
Fisher information and epistemic and aleatoric means and
standard deviations over 5 network initializations, and 5 dif-
ferent train/validation set splits, respectively.

10

ical parameters, such as in Press & Schechter (1974), or
approximated using simulations (Reed et al. 2006).

We compare gIMNN Fisher constraints to the Press-
Schechter HMF, fPS(œÉ; ‚Ñ¶m, œÉ8), integrated from a Ô¨Åxed
Mcut as a function of cosmological parameters in Figure
5. We use the HMF since this quantity incorporates both
halo number and mass information. See Appendix A for
a detailed comparison of dn/dM and f (œÉ) functions. We
utilize hmf calc (Murray 2014, Murray et al. 2013) for
the calculation. The HMF and the corresponding halo
number density at Ô¨Åxed M = Mcut determines a rela-
tively narrow locus in the (‚Ñ¶m, œÉ8) plane. The clustering
information, traced by the 2PCF Fisher, (which is ac-
cessible by the network) serves to lift this degeneracy.
The network Fishers are nearly parallel to the HMF iso-
mass contours, but are not degenerate in the direction
of the 2PCF Fisher‚Äôs major axes. Decorating nodes with
masses also induces a slight rotation towards the isomass
contours, since the network has more detailed mass in-
formation to work with. This result indicates that the
network has automatically learned to extract information
from both halo clustering and mass information.

VI. WORKING WITH NOISY CATALOGUES

We next add observational noise and catalogue cuts
on-the-Ô¨Çy during gIMNN training to mimic survey as-
sembly with imperfect observations. Before a graph is
constructed from a halo catalogue and fed to the network
in training, halo masses are subjected to white noise with
Ô¨Åxed variance,

ÀÜmi = mi + N (0, œÉ2

noise),

(24)

where œÉnoise = AnoiseMcut. Observed halos that fall be-
low Mcut are then trimmed from the graph to mimic
real catalogue cuts in the presence of noisy mass esti-
mates. This noise model reÔ¨Çects uncertainty in the halo
Ô¨Ånder or galaxy catalogue builder. Smaller masses close
to Mcut are more likely to be cut due to mass under-
estimation, similar to low-brightness clusters in sky sur-
veys. We choose Mcut = 1.5 √ó 1015M(cid:12) and train identi-
cal networks with diÔ¨Äerent amplitudes of on-the-Ô¨Çy noise;
Anoise ‚àà {0.05, 0.1, 0.2}.
Results. We display validation curves over training
epoch in Figure 6. Increasing catalogue noise results in
higher variance per epoch in the computed Fisher statis-
tics, as well as a slightly lower information plateau. This
can be interpreted as higher noise obscuring small mass
scales in the information extraction. This eÔ¨Äect is illus-
trated via inÔ¨Çated Fisher constraints in Figure 7.

As the noise level increases the low-end masses have
more variance when drawn on-the-Ô¨Çy so more halos are
projected out of the catalogue because they fall below
Mcut, and so this information cannot be encapsulated in
the compressed summaries. Increasing the noise ampli-
tude to 20% of Mcut inÔ¨Çates constraints in ‚Ñ¶m. In the
high-noise limit, halo positions dominate det F , indicated

FIG. 5. Fisher matrix comparison for decorated (black)
and undecorated (green) graphs and 2PCF (dashed) com-
puted for the same catalogue, plotted over lines of the Press-
Schechter halo mass function fPS(œÉ; ‚Ñ¶m, œÉ8) integrated from
Mcut (gray). The HMF contours indicate the information
provided by a Ô¨Åxed physical mass cut, orthogonal to the posi-
tional information in the 2PFC. The IMNN Fisher ellipses fol-
low these contours, but are not degenerate in the direction of
the 2PCF, indicating that the IMNN has learned to use both
mass and clustering information. Graphs annotated with halo
mass tighten constraints by a factor of 2.3 over undecorated
graphs, and by a factor of 42 over 2PCF information.

A. Comparison to the Halo Mass Function

The results of Section V indicate that catalogue in-
formation extraction is extremely sensitive to a physical
mass cut. This behaviour is akin to constraints obtained
using halo mass cumulative distribution functions (Ar-
tis et al. 2021, Reed et al. 2006, Uhlemann et al. 2020).
The halo number density function is dn/dM , deÔ¨Åned as
the number of halos of mass M per unit volume per
unit interval in M , equivalently parameterized using the
linear overdensity of the density Ô¨Åeld,
smoothed r.m.s.
œÉ(M ), via the halo mass function (HMF), f (œÉ). The frac-
tion of mass in collapsed halos per unit interval ln œÉ‚àí1
obeys

(cid:90) ‚àû

‚àí‚àû

f (œÉ)d ln œÉ‚àí1 = 1,

(22)

and is related to the halo number density function via

dn
dM

=

œÅo
M

d ln œÉ‚àí1
dM

f (œÉ),

(23)

where œÅo is the mean mass density of the universe. The
form of f (œÉ; Œ∏) can be related analytically to cosmolog-

11

FIG. 6. Validation curves for noisy masses. Smaller noise
variance (darker curves) results in smaller per-epoch variance
in det F and slightly more information extraction. Informa-
tion leakage occurs with higher noise variance since smaller
scales are poorly resolved and trimmed from the catalogue.

by the relatively unchanged, position-dependent œÉ8 con-
straints. As noise decreases and masses are better known,
the Fisher exhibits the same rotation seen in Section V A
along the isomass HMF lines. This eÔ¨Äect is discussed in
detail in Appendix A.

Despite inÔ¨Çating constraints, showing the network
large numbers of on-the-Ô¨Çy noise realizations during
training can harden the network to the negative eÔ¨Äects
of limited training data and therefore provide smoother
training whilst still bein g able to extract information
at a similar level to noise-free catalogues. However, the
model of these noisy masses must be accurate to the noise
model expected for the real data otherwise the on-the-Ô¨Çy
simulations do not provide hardening of the summaries
in the correct way and may even project out informative
data correlations.

VII. APPLICATION TO IMPLICIT
LIKELIHOOD INFERENCE

The IMNN framework is both an information quan-
tiÔ¨Åcation scheme as well as an asymptotically optimal
compression mechanism for implicit likelihood inference.
The global network summaries used to compute Fisher
statistics can also be used as proxies for the cosmologi-
cal parameters via a score estimate (Alsing & Wandelt
2018, Charnock et al. 2018) using the IMNN Fisher and
covariance:

ÀÜŒ∏Œ± = F‚àí1
Œ±Œ≤

‚àÇ¬µi
‚àÇŒ∏Œ≤

C‚àí1

ij (xj(w, d) ‚àí ¬µj).)

(25)

These summaries are not explicit predictions for cosmo-
logical parameters, although they are pseudo-maximum
likelihood estimates for the parameters in the region
asymptotically close to the Ô¨Åducial cosmological param-
eter values. Instead, we suggest using these values as in-

FIG. 7. Fisher constraints for diÔ¨Äerent noise models, plot-
ted over lines of the Press-Schechter halo mass function inte-
grated from Mcut (gray). Higher œÉnoise (lighter curves) cause
low-mass halos to drop out, inÔ¨Çating constraints in ‚Ñ¶m, but
constraints in œÉ8 remain relatively unchanged since this pa-
rameter is largely position-dependent.

formative summaries in Approximate Bayesian Compu-
tation (ABC) or density estimation schemes, as demon-
strated in Makinen et al. (2021) and Charnock et al.
(2018). Figure 8 shows a spread of these summaries
over 200 test (non-seed matched with cosmic variance)
Œ∏¬± Quijote datasets. The network is able to distinguish
halo graphs simulated at diÔ¨Äerent parameter values (on
the level of the parameter degeneracy), rendering these
gIMNN summaries usable in accept-reject simulation-
based inference (SBI) schemes. We additionally discuss
network generalization to other data in Appendix B.

VIII. DISCUSSION & CONCLUSION

In this study, we explored cosmological information ex-
traction from halo catalogues assembled as graphs. We
Ô¨Årst introduced graphs as a general language for de-
scribing large-scale structure formation. We showed that
nonlinear summaries from suÔ¨Éciently expressive graph
neural networks far exceed the information contained in
traditional 2-point statistics, using just the ‚âà100 most
massive halos. We illustrated that decorating graphs
with mass information increases the information yield,
as well as decreases training-validation set sampling vari-
ance (aleatoric error). Finally, we showed that sum-
maries produced by the network are readily usable for
simulation-based inference.

12

scale structure. The graph framework presented here en-
ables further modular study of nonlinear statistics that
combine attributes of mass functions and correlation
functions, at a fraction of the computational cost of bis-
pectrum or trispectrum calculation. Cosmological pa-
rameter constraints from void catalogues, here the du-
als to halo graphs, might elucidate complementary con-
straints to those obtained here (Kreisch et al. 2021).

Coulton et al. (2022) and Jung et al. (2022) recently in-
vestigated bispectrum statistics from n-body primordial
nongaussianity (pNG) simulations in the Quijote suite to
investigate propagation of pNG to late-time cosmologi-
cal structure. A complementary study using gIMNNs
might pick up signatures not isolated in the bispectrum
framework. The gIMNN formalism could also be used
to summarise and understand better the halo merger
and clustering statistics as a function of scale or graph
feature, such as clique number. Hierarchical aggrega-
tion schemes, as described in Section III A, could make
the gIMNN scheme tractable for aggregating catalogued
galaxies into subhalo graphs, and further into a global
graph over the largest scales. Doing so would scale this
analysis to full-sized galaxy catalogues.

Follow-up study is warranted on much larger cata-
logues and with more parameters, with a view to ready-
ing this framework for applications to inference from
detailed physical simulations and, ultimately, realistic
galaxy surveys.

IX. CODE AVAILABILITY

The code used for this analysis is available at https://
github.com/tlmakinen/cosmicGraphs. Full documen-
tation for the IMNN software is available at https://
www.aquila-consortium.org/doc/imnn/index.html.

ACKNOWLEDGMENTS

T.L.M acknowledges the Imperial College London
President‚Äôs Scholarship fund for support of this study,
as well as the great discussions with Stephon Alexan-
der, David Spergel, and Doug Finkbeiner that inspired
this work. B.D.W. acknowledges support by the ANR
BIG4 project, grant ANR-16-CE23-0002 of the French
Agence Nationale de la Recherche; and the Labex ILP
(reference ANR-10-LABX-63) part of the Idex SUPER,
and received Ô¨Ånancial state aid managed by the Agence
Nationale de la Recherche, as part of the programme
Investissements d‚Äôavenir under the reference ANR-11-
IDEX-0004-02. This work was done within the Aquila
Consortium and the Learning the Universe Collabora-
tion. The Flatiron Institute is supported by the Simons
Foundation.

FIG. 8. gIMNN score summaries for test derivative datasets
(Œ∏¬± with cosmic variance. A trained network can distin-
guish between simulations generated at diÔ¨Äerent parameter
values, indicated by the diÔ¨Äerent distributions in summary
space. These score summaries are not explicit predictions for
cosmological parameters; they should be considered as infor-
mative summaries of the cosmological parameters that can be
used for ABC or density estimation for building cosmological
posteriors. In general and on average, simulations generated
at lower ‚Ñ¶m and œÉ8 values have lower pseudo-maximum like-
lihood estimated values for these parameters and vice versa,
however, they should not be taken as predictions of the cos-
mological parameters.

We also explored cosmological information as a func-
tion of graph construction. We showed that a signiÔ¨Åcant
amount of information is contained in the variable car-
dinality of the graph, N v, i.e. the number of halos in a
catalogue, and related this feature to the halo mass func-
tion formalism. This test demonstrated a distinct advan-
tage in graph representation: allowing data vector size to
vary with cosmology, combining both positional and mass
information automatically into just two statistics.

Next we demonstrated that gIMNN training can be
made robust to noise with little information leakage in
a more realistic setting where halo masses are estimates
with measurement error.

We also explored epistemic and aleatoric error in graph
representation. We showed that a combined compression
of masses and positional information decreased aleatoric
variability, meaning training and validation graphs be-
come more descriptive of their underlying likelihood with
decoration, even in a Ô¨Åxed-length scenario.

The results of this work hold several implications for
cosmological parameter estimation and study of large-

Adami, C., & Mazure, A. 1999, Astron. Astrophys. Suppl.
Ser., 134, 393, doi: 10.1051/aas:1999145
Alpaslan, M., Robotham, A. S. G., Driver, S., et al. 2014,
Monthly Notices of the RAS, 438, 177, doi: 10.1093/mnras/
stt2136
Alsing, J., & Wandelt, B. 2018, MNRAS, 476, L60, doi: 10.
1093/mnrasl/sly029
Artis, E., Melin, J.-B., Bartlett, J. G., & Murray, C. 2021, As-
tronomy & Astrophysics, 649, A47, doi: 10.1051/0004-6361/
202140293
Barrow, J. D., Bhavsar, S. P., & Sonoda, D. H. 1985, Monthly
Notices of the Royal Astronomical Society, 216, 17, doi: 10.
1093/mnras/216.1.17
Battaglia, P. W., Hamrick, J. B., Bapst, V., et al. 2018, Re-
lational inductive biases, deep learning, and graph networks.
https://arxiv.org/abs/1806.01261
Beuret, M., Billot, N., Cambr¬¥esy, L., et al. 2017, Astron-
omy and Astrophysics, 597, A114, doi: 10.1051/0004-6361/
201629199
Bhavsar, S. P., & Ling, E. N. 1988, Publications of the ASP,
100, 1314, doi: 10.1086/132325
Biswas, R., Alizadeh, E., & Wandelt, B. D. 2010, Phys. Rev.
D, 82, 023002, doi: 10.1103/PhysRevD.82.023002
Bronstein, M. M., Bruna, J., Cohen, T., & VeliÀáckovi¬¥c, P. 2021,
Geometric Deep Learning: Grids, Groups, Graphs, Geodesics,
and Gauges, arXiv, doi: 10.48550/ARXIV.2104.13478
Charnock, T., Lavaux, G., & Wandelt, B. D. 2018, Physical
Review D, 97, doi: 10.1103/physrevd.97.083004
Colberg, J. M. 2007, Monthly Notices of the Royal Astro-
nomical Society, 375, 337, doi: 10.1111/j.1365-2966.2006.
11312.x
Coles, P., Pearson, R. C., Borgani, S., Plionis, M., & Moscar-
dini, L. 1998, Monthly Notices of the RAS, 294, 245, doi: 10.
1046/j.1365-8711.1998.01147.x
Coulton, W. R., Villaescusa-Navarro, F., Jamieson, D.,
et al. 2022, Quijote-PNG: Simulations of primordial non-
Gaussianity and the information content of the matter Ô¨Åeld
power spectrum and bispectrum, arXiv, doi: 10.48550/
ARXIV.2206.01619
Cram¬¥er, H. 1946, Mathematical methods of statistics, by Har-
ald Cramer, .. (The University Press)
Cranmer, M., Sanchez-Gonzalez, A., Battaglia, P., et al. 2020,
Discovering Symbolic Models from Deep Learning with Induc-
tive Biases. https://arxiv.org/abs/2006.11287
Dai, B., & Seljak, U. 2022, arXiv e-prints, arXiv:2202.05282.
https://arxiv.org/abs/2202.05282
Davis, M., Efstathiou, G., Frenk, C. S., & White, S. D. M.
1985, Astrophys. J., 292, 371, doi: 10.1086/163168
Fluri, J., Kacprzak, T., Lucchi, A., et al. 2019, Physical Re-
view D, 100, doi: 10.1103/physrevd.100.063514
Fluri, J., Kacprzak, T., Lucchi, A., et al. 2022, arXiv e-prints,
arXiv:2201.07771. https://arxiv.org/abs/2201.07771
Fluri, J., Kacprzak, T., Refregier, A., et al. 2018, Physical
Review D, 98, doi: 10.1103/physrevd.98.123518
Fluri, J., Kacprzak, T., Refregier, A., Lucchi, A., & Hofmann,
T. 2021, Physical Review D, 104, doi: 10.1103/physrevd.
104.123526
Gillet, N., Mesinger, A., Greig, B., Liu, A., & Ucci, G. 2019,
Monthly Notices of the Royal Astronomical Society, doi: 10.
1093/mnras/stz010

13

Godwin, J., Keck, T., Battaglia, P., et al. 2020, Jraph: A
library for graph neural networks in jax., 0.0.1.dev. http:
//github.com/deepmind/jraph
Hahn, C., Villaescusa-Navarro, F., Castorina, E., & Scoc-
cimarro, R. 2020, Journal of Cosmology and Astroparticle
Physics, 2020, 040, doi: 10.1088/1475-7516/2020/03/040
Hamaus, N., Sutter, P., Lavaux, G., & Wandelt, B. D.
2015, Journal of Cosmology and Astroparticle Physics, 2015,
036‚Äì036, doi: 10.1088/1475-7516/2015/11/036
Heavens, A. F., Jimenez, R., & Lahav, O. 2000, Monthly
Notices of the Royal Astronomical Society, 317, 965‚Äì972,
doi: 10.1046/j.1365-8711.2000.03692.x
Heek, J., Levskaya, A., Oliver, A., et al. 2020, Flax: A neu-
ral network library and ecosystem for JAX, 0.5.2. http:
//github.com/google/flax
Hendrycks, D., & Gimpel, K. 2016, Gaussian Error Linear
Units (GELUs), arXiv, doi: 10.48550/ARXIV.1606.08415
Jamieson, D., Li, Y., de Oliveira, R. A., et al. 2022, Field
Level Neural Network Emulator for Cosmological N-body
Simulations, arXiv, doi: 10.48550/ARXIV.2206.04594
Jasche, J., Leclercq, F., & Wandelt, B. 2015, Journal of Cos-
mology and Astroparticle Physics, 2015, 036, doi: 10.1088/
1475-7516/2015/01/036
Jasche, J., & Wandelt, B. D. 2013, Monthly Notices of the
RAS, 432, 894, doi: 10.1093/mnras/stt449
JeÔ¨Ärey, N., Alsing, J., & Lanusse, F. 2020, Monthly Notices
of the Royal Astronomical Society, 501, 954, doi: 10.1093/
mnras/staa3594
JeÔ¨Ärey, N., Boulanger, F., Wandelt, B. D., et al. 2022,
Monthly Notices of the RAS, 510, L1, doi: 10.1093/mnrasl/
slab120
JeÔ¨Ärey, N., & Wandelt, B. D. 2020, Solving high-dimensional
parameter inference: marginal posterior densities & Moment
Networks. https://arxiv.org/abs/2011.05991
Jung, G., Karagiannis, D., Liguori, M., et al. 2022, Quijote-
PNG: Quasi-maximum likelihood estimation of Primordial
Non-Gaussianity in the non-linear dark matter density Ô¨Åeld,
arXiv, doi: 10.48550/ARXIV.2206.01624
Kipf, T. N., & Welling, M. 2016, Semi-Supervised ClassiÔ¨Å-
cation with Graph Convolutional Networks, arXiv, doi: 10.
48550/ARXIV.1609.02907
Kreisch, C. D., Pisani, A., Villaescusa-Navarro, F., et al. 2021,
The GIGANTES dataset: precision cosmology from voids in
the machine learning era, arXiv, doi: 10.48550/ARXIV.2107.
02304
Krzewina, L. G., & Saslaw, W. C. 1996, Monthly Notices of
the RAS, 278, 869, doi: 10.1093/mnras/278.3.869
Kwon, Y., Hong, S. E., & Park, I. 2020, Journal of the Korean
Physical Society, 77, 49‚Äì59, doi: 10.3938/jkps.77.49
Lavaux, G., & Wandelt, B. D. 2010, Monthly Notices of the
RAS, 403, 1392, doi: 10.1111/j.1365-2966.2010.16197.x
Leclercq, F. 2015, Bayesian large-scale structure inference and
cosmic web analysis, arXiv, doi: 10.48550/ARXIV.1512.04985
Leclercq, F., & Heavens, A. 2021, On the accuracy and pre-
cision of correlation functions and Ô¨Åeld-level inference in cos-
mology. https://arxiv.org/abs/2103.04158
Lemos, P., JeÔ¨Ärey, N., Cranmer, M., Ho, S., & Battaglia, P.
2022, Rediscovering orbital mechanics with machine learning.
https://arxiv.org/abs/2202.02306
Libeskind, N. I., van de Weygaert, R., Cautun, M., et al.
2018, Monthly Notices of the RAS, 473, 1195, doi: 10.1093/

Sutter, P. M., Lavaux, G., Wandelt, B. D., & Weinberg, D. H.
2012, Astrophys. J., 761, 44, doi: 10.1088/0004-637X/761/
1/44
Tegmark, M., Taylor, A. N., & Heavens, A. F. 1997, The
Astrophysical Journal, 480, 22‚Äì35, doi: 10.1086/303939
Ueda, H., & Itoh, M. 1997, Publications of the Astronomical
Society of Japan, 49, 131, doi: 10.1093/pasj/49.2.131
Uhlemann, C., Friedrich, O., Villaescusa-Navarro, F., Baner-
jee, A., & Codis, S. 2020, Monthly Notices of the RAS, 495,
4006, doi: 10.1093/mnras/staa1155
Uhlemann, C., Friedrich, O., Villaescusa-Navarro, F., Baner-
jee, A., & Codis, S. 2020, Monthly Notices of the Royal As-
tronomical Society, 495, 4006, doi: 10.1093/mnras/staa1155
van de Weygaert, R., Jones, B. J., & Mart¬¥ƒ±nez, V. J. 1992,
Physics Letters A, 169, 145, doi: https://doi.org/10.1016/
0375-9601(92)90584-9
Villaescusa-Navarro, F., Wandelt, B. D., Angl¬¥es-Alc¬¥azar,
D., et al. 2020a, Neural networks as optimal estimators to
marginalize over baryonic eÔ¨Äects. https://arxiv.org/abs/
2011.05992
Villaescusa-Navarro, F., Hahn, C., Massara, E., et al. 2020b,
The Astrophysical Journal Supplement Series, 250, 2, doi: 10.
3847/1538-4365/ab9d82
Villanueva-Domingo, P., & Villaescusa-Navarro, F. 2022,
Learning cosmology and clustering with cosmic graphs, arXiv,
doi: 10.48550/ARXIV.2204.13713
Xu, X., Cisewski-Kehe, J., Green, S., & Nagai, D. 2019, As-
tronomy and Computing, 27, 34‚Äì52, doi: 10.1016/j.ascom.
2019.02.003
Yang, D., & Yu, H.-B. 2022, A graph model for the cluster-
ing of dark matter halos, arXiv, doi: 10.48550/ARXIV.2206.
05578

mnras/stx1976
Livet, F., Charnock, T., Borgne, D. L., & de Lapparent, V.
2021, Catalog-free modeling of galaxy types in deep images:
Massive dimensional reduction with neural networks. https:
//arxiv.org/abs/2102.01086
Makinen, T. L., Charnock, T., Alsing, J., & Wandelt, B. D.
2021, Journal of Cosmology and Astroparticle Physics, 2021,
049, doi: 10.1088/1475-7516/2021/11/049
Makinen, T. L., Lancaster, L., Villaescusa-Navarro, F., et al.
2020, deep21: a Deep Learning Method for 21cm Foreground
Removal. https://arxiv.org/abs/2010.15843
Massara, E., Villaescusa-Navarro, F., Hahn, C., et al. 2022,
Cosmological Information in the Marked Power Spectrum of
the Galaxy Field, arXiv, doi: 10.48550/ARXIV.2206.01709
Matilla, J. M. Z., Sharma, M., Hsu, D., & Haiman, Z. 2020,
Physical Review D, 102, doi: 10.1103/physrevd.102.123506
Murray, S. 2014, HMF: Halo Mass Function calculator, As-
trophysics Source Code Library, record ascl:1412.006. http:
//ascl.net/1412.006
Murray, S. G., Power, C., & Robotham, A. S. G. 2013, As-
tronomy and Computing, 3, 23, doi: 10.1016/j.ascom.2013.
11.001
Naidoo, K., Massara, E., & Lahav, O. 2022, Monthly Notices
of the Royal Astronomical Society, 513, 3596, doi: 10.1093/
mnras/stac1138
Naidoo, K., Whiteway, L., Massara, E., et al. 2019, Monthly
Notices of the Royal Astronomical Society, 491, 1709, doi: 10.
1093/mnras/stz3075
Pan, S., Liu, M., Forero-Romero, J., et al. 2020, Cosmological
parameter estimation from large-scale structure deep learn-
ing. https://arxiv.org/abs/1908.10590
Petri, A., Haiman, Z., Hui, L., May, M., & Kratochvil, J. M.
2013, Phys. Rev. D, 88, 123002, doi: 10.1103/PhysRevD.88.
123002
Philcox, O. H., & Ivanov, M. M. 2022, Physical Review D,
105, doi: 10.1103/physrevd.105.043517
Porqueres, N., Heavens, A., Mortlock, D., & Lavaux, G.
2021, Monthly Notices of the Royal Astronomical Society,
509, 3194‚Äì3202, doi: 10.1093/mnras/stab3234
Prelogovi¬¥c, D., Mesinger, A., Murray, S., Fiameni, G., &
Gillet, N. 2021, Machine learning galaxy properties from 21
cm lightcones:
impact of network architectures and signal
contamination. https://arxiv.org/abs/2107.00018
Press, W. H., & Schechter, P. 1974, Astrophys. J., 187, 425,
doi: 10.1086/152650
Ramanah, D. K., Lavaux, G., Jasche, J., & Wandelt, B. D.
2019, Astronomy and Astrophysics, 621, A69, doi: 10.1051/
0004-6361/201834117
Rao, C. R. 1945, Bulletin of the Calcutta Mathematical So-
ciety, 37, 81‚Äì89
Ravanbakhsh, S., Oliva, J., Fromenteau, S., et al. 2017, Esti-
mating Cosmological Parameters from the Dark Matter Dis-
tribution. https://arxiv.org/abs/1711.02033
Reed, D. S., Bower, R., Frenk, C. S., Jenkins, A., & Theuns,
T. 2006, Monthly Notices of the Royal Astronomical Society,
374, 2, doi: 10.1111/j.1365-2966.2006.11204.x
Ribli, D., ¬¥Armin Pataki, B., & Csabai, I. 2018, An improved
cosmological parameter inference scheme motivated by deep
learning. https://arxiv.org/abs/1806.05995
Satorras, V. G., Hoogeboom, E., Fuchs, F. B., Posner, I., &
Welling, M. 2021, E(n) Equivariant Normalizing Flows, arXiv,
doi: 10.48550/ARXIV.2105.09016

14

We compare these two integrated quantities to undec-
orated and decorated gIMNN Fisher constraints in Fig-
ure 9. In the undecorated case (green ellipse), the net-
work has explicit access to halo number and clustering
information, resulting in contours more closely aligned
with integrated dn/dM (dashed green lines). By con-
trast, when the graph nodes are annotated with mass
labels (black ellipse), the network has explicit access to
a combination of halo number, mass, and clustering in-
formation. This results in a slight rotation towards the
integrated HMF f (œÉ) contours (dark solid lines), since
this quantity reÔ¨Çects the addition of mass fraction infor-
mation. This eÔ¨Äect is also illustrated in Figure 7. As
discussed in Section VI, as noise level decreases, the net-
work has access to sharper mass information, inducing a
rotation towards the integrated HMF line.

Appendix B: Details of Graph Assembly in Jax

Here we detail Jax-compatible graph assembly. Jax is
a pseudo-compiled language, meaning arrays must have a
pre-determined Ô¨Åxed length before sent to a GPU device
for operations like gradient descent. We navigate this
constraint by padding graph features by pre-determined
Ô¨Åxed values, and masking features to assess information
content in a modular fashion.

FIG. 10. Mass distribution after added noise, œÉnoise =
0.2Mcut (black) and simulated halo Ô¨Ånder cuts (teal) for a sin-
gle Ô¨Åducial simulation for masses larger than 1.1 √ó 1015 M(cid:12).
The orange dashed lines indicates the minimum mass consid-
ered by the ‚Äúsurvey‚Äù cutoÔ¨Ä, Mcut = 1.5 √ó 1015 M(cid:12).

The Quijote catalogues are assembled into graphs by
Ô¨Årst making a mass cut on a larger selection of halos.
Masses are then padded with a pre-determined padding
value, generally chosen to be max(N v) + 10. These
dummy halos are assigned a very large mass value and
a position outside of the 1Gpc3 box. A distance matrix
is then computed for both halo and dummy halo nodes,
along with sender-receiver indexes, (sk, rk).
In the in-
variant graph case we also compute relative angles be-
tween all halos, outlined in Section II B 2. Connections

FIG. 9. Comparison of undecorated graph (green ellipse) and
decorated graph (dark ellipse) gIMNN Fishers, plotted over
integrated halo number density (dashed green lines) and mass
fraction (dark solid lines) functions from Ô¨Åxed Mcut. Deco-
rating graphs with mass information induces a slight rota-
tion away from dn/dM and towards f (œÉ) contours, since this
quantity incorporates both mass and halo number informa-
tion.

Appendix A: Comparing Halo Mass and Number
Density Functions

Here we discuss the diÔ¨Äerence between the halo number
density function and halo mass function in the context of
gIMNN information extraction. The halo number density
function,

dn
dM

=

œÅo
M

d ln œÉ‚àí1(M )
dM

f (œÉ),

and the halo mass function,

f (œÉ) =

M
œÅo

dn(M )
d ln(œÉ‚àí1(M ))

,

(A1)

(A2)

within the Press-Schechter formalism (Press & Schechter
1974). Integrating the halo number density from a Ô¨Åxed
mass Mcut yields the number of halos with a mass above
this threshold:

N (Mi > Mcut) =

(cid:90) ‚àû

Mcut

dn
dM

dM,

(A3)

which in our case is the node cardinality of a halo graph,
N v. By contrast, integrating the halo mass function from
Mcut yields the fraction of total mass residing in col-
lapsed halos of mass above Mcut:

F (Mi > Mcut) =

(cid:90) ‚àû

Mcut

f (œÉ)dM,

(A4)

which incorporates both halo number N v and mass infor-
mation above Mcut.

|dij| > rconnect and dij = 0 (self-edges) are then removed
and replaced by a large padding value larger than rconnect.
We compute N e by the number of distances that Ô¨Åt these
criteria. Distance values and sender-receiver arrays are
then sorted smallest to largest and slotted into padded
arrays of length max(N e) + 10, where padded edge val-
ues are then replaced with zeros. We encountered gra-
dient stability issues when padding was made too large
for e.g.
smaller edge sets, since more padding means
the network is asked to operate on extra non-informative
features. This sorting arrangement is advantageous since
small distances (local connections) are always included
in the halo graph‚Äôs edges, even if the padding container
is chosen to be too small for all edges for a given rconnect
value. What this means is that networks trained on a
small rconnect can generalize reasonably well to datasets
constructed with larger connection criteria.
Adding Noise. When constructing noisy catalogues
on-the-Ô¨Çy, we Ô¨Årst truncate catalogues with a smaller
Mcut = 1.1 √ó 1015M(cid:12) to include more halos in the true
catalog. Every training epoch, a new realisation of obser-
vational noise is then added to the masses according to
Section VI, after which halos below Mcut = 1.5√ó1015M(cid:12)
are discarded, illustrated in Figure 10. The graph edge
attributes are then computed for the remaining halos as

15

described above. For the noise scheme chosen in this
work, this results in approximately equal numbers of ha-
los being discarded above and below the mass cut line, yet
increases the uncertainty in the informative HMF num-
ber count, which inÔ¨Çates constraints in ‚Ñ¶m.

Appendix C: Details of the GNN Architecture

1. Masking Graph Features

To conduct information extraction tests with and with-
out node decoration, the GNN architecture must remain
Ô¨Åxed between decorated and undecorated cases. To mask
a graph node or edge feature, an indicator 1 is assigned
to the array in place of numerical value. These unin-
formative features are fed through the network but do
not contribute to the information extraction since the
same operation is performed across Ô¨Åducial and deriva-
tive datasets, yet ensure a fair comparison of information
as a function of catalogue data features since network ar-
chitectures (hidden and output size dimensions) could be
kept Ô¨Åxed with the same initialized weights.

