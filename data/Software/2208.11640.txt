Repair Is Nearly Generation: Multilingual Program Repair with LLMs

Harshit Joshi,1 Jos´e Cambronero,2* Sumit Gulwani,2* Vu Le,2* Ivan Radicek,3* Gust Verbruggen4*
1 Microsoft, India
2 Microsoft, USA
3 Microsoft, Croatia
4 Microsoft, Belgium
{t-hjoshi, jcambronero, sumitg, levu, ivradice, gverbruggen}@microsoft.com

2
2
0
2

p
e
S
8
2

]
E
S
.
s
c
[

2
v
0
4
6
1
1
.
8
0
2
2
:
v
i
X
r
a

Abstract

Most programmers make mistakes when writing code. Some
of these mistakes are small and require few edits to the original
program – a class of errors recently termed last mile mistakes.
These errors break the ﬂow for experienced developers and
can stump novice programmers. Existing automated repair
techniques targeting this class of errors are domain-speciﬁc
and do not easily carry over to new domains. Transferring sym-
bolic approaches requires substantial engineering and neural
approaches require data and retraining. We introduce RING, a
multilingual repair engine powered by a large language model
trained on code (LLMC) such as Codex. Such a multilingual
engine enables a ﬂipped model for programming assistance,
one where the programmer writes code and the AI assistance
suggests ﬁxes, compared to traditional code suggestion tech-
nology. Taking inspiration from the way programmers man-
ually ﬁx bugs, we show that a prompt-based strategy that
conceptualizes repair as localization, transformation, and can-
didate ranking, can successfully repair programs in multiple
domains with minimal effort. We present the ﬁrst results for
such a multilingual repair engine by evaluating on 6 differ-
ent domains and comparing performance to domain-speciﬁc
repair engines. We show that RING can outperform domain-
speciﬁc repair engines in 3 of these domains. We also identify
directions for future research using LLMCs for multilingual
repair.

Introduction
The population of people writing code across different lan-
guages has steadily grown (Bureau of Labor Statistics 2021)
and ranges from novices to experts. Regardless of their expe-
rience level, programmers can make mistakes when writing
code. Program errors can range in a spectrum from those
that are easy to spot and ﬁx to those that require substantial
application knowledge and may be very subtle logical bugs.
However, even simple mistakes, such as syntax errors, which
may require a relatively small edit and may be apparent to a
programming expert, can be frustrating for novice program-
mers and can slow down the workﬂow of more experienced
programmers (Wexelblat 1976; Murphy et al. 2008; Altadmri
and Brown 2015; Drosos, Guo, and Parnin 2017).

One way to help programmers encountering these small
mistakes is by using automated program repair systems

*Listed in alphabetical order

(APRs). These tools take a faulty program and a speciﬁ-
cation of correctness as input, and produce a ﬁxed version
of the program that conforms to the speciﬁcation. In the con-
text of simple mistakes, this speciﬁcation may amount to the
judgment returned by the associated domain parser and type
checker. Recent work (Bavishi et al. 2022) has introduced the
term last-mile repairs to broadly describe this class of repairs,
where the original program is a small edit distance away from
the correct program. In this deﬁnition, program correctness
can be checked without substantial additional context (e.g.,
syntax, basic type errors). A quick search on most program-
ming help forums reveals a large number of questions for
such errors. For example, as of August 2022 a search in Stack-
overﬂow posts tagged with Python and SyntaxError returns
over 15,000 posts.

Existing work has explored performing these kind of
repairs automatically. Symbolic systems, such as Grm-
tools (Diekmann and Tratt 2020), typically build on error-
recovery mechanisms in parsers to enumerate local edits
that can resolve errors raised during processing of the input
program. However, such symbolic systems are limited as
they typically restrict their search space (to avoid state explo-
sions), cannot easily encode properties such as the likelihood
of particular repair candidates, and may require substantial
engineering to extend their application to a new domain or to
increase their repair coverage.

More recently, neural approaches have been successfully
applied to syntax and diagnostics-guided repair. For example,
Dr. Repair (Yasunaga and Liang 2020), BIFI (Yasunaga and
Liang 2021), and TFix (Berabi et al. 2021) use transformers
to produce repairs for C compilation errors, Python syntax
errors, and Javascript linter-raised diagnostics, respectively.
However, these require training on substantial amounts of
data to perform competitively. Some systems, such as LaMi-
rage (Bavishi et al. 2022), have also combined symbolic and
neural approaches to yield a blended system that can success-
fully repair broken programs in low-code languages such as
Excel and PowerFx.

Unfortunately, all these systems share a key drawback:
they require either substantial engineering (symbolic) or ad-
ditional data and training (neural) to adapt to new language
domains. In this paper, we propose a single repair engine, that
leverages a large language model trained on code (LLMC) –
OpenAI’s Codex – to perform multilingual repair.

 
 
 
 
 
 
Our system, RING (Repair Is Nearly Generation) exploits
Codex’s few-shot learning capabilities (Bareiß et al. 2022;
Drori et al. 2022) to perform multilingual program repair. To
do this effectively, we breakdown program repair into tradi-
tional phases, as conceptualized in most symbolic automated
program repair systems (Goues, Pradel, and Roychoudhury
2019; Liu et al. 2021; Bavishi et al. 2022): fault localization,
code transformation, and candidate ranking. We show how
each stage can be addressed with minimal effort by emu-
lating what a human developer would do and incorporating
this intuition into a prompt-design strategy for an LLMC.
This human-inspired prompt design is a key ingredient for
RING’s effectiveness.

We evaluate RING on six different language domains:
Excel, PowerFx, Python, Javascript, C, and Powershell. Our
results show that RING can successfully repair signiﬁcantly
more programs than a domain-speciﬁc repair engine in three
domains and shows competitive results for another two do-
mains. We identify possible directions for improvement based
on our results, including building domain-speciﬁc rankers,
and performing iterative querying with Codex. We also carry
out analyses to determine the effectiveness of each of our
conceptual stages. We analyze the extent to which we can
localize an error, even if we cannot ﬁx it. We show the im-
pact of smart few-shot selection for performing appropriate
code transformations. And we explore the extent to which
our ranking strategy correlates with success.

Jointly, these results provide the ﬁrst evidence that an
LLMC can enable multilingual repair (with a single shared
model) and can push the state-of-the-art-forward. In contrast
to other AI-assisted code features such as code completion,
this advance opens up the possibility of a ﬂipped interac-
tion model, where the user writes code and the AI assistant
performs the ﬁxing.

We make the following contributions:

• We present an LLMC-based approach to multilingual re-
pair that enables a ﬂipped model for AI-assisted program-
ming, one in which the user writes code and the assistant
suggests ﬁxes for last-mile mistakes (Bavishi et al. 2022).
Our LLMC approach is built on the intuition that ﬁxing
such mistakes can be conceptualized into three tasks: fault
localization, code transformation, and candidate ranking.
We devise a prompt-based approach to addressing these
tasks by emulating the type of information used by devel-
opers when manually ﬁxing these mistakes.

• We implement our approach using Codex in a system,
called RING, which employs compiler (or diagnostic)
messages, smart few-shot selection, and probability-based
ranking of repair candidates to perform repair across vary-
ing language domains. We perform an extensive evalua-
tion across six different domains, ranging from low-code
formula languages to popular scripting languages, show-
ing that multi-lingual repair with LLMCs is viable and
can compete with (or outperform) domain-speciﬁc repair
engines.

• As part of our evaluation, we introduce a novel domain for
last-mile repair: Powershell programs. We collect a bench-
mark set of 200 Powershell programs from Stackoverﬂow,

which we also release for future research.

Related Work
Automated Program Repair Finding and ﬁxing bugs is
challenging and tedious, even for language experts (Zhong
and Su 2015). The software engineering community has built
Automated Program Repair (APR) tools (Arcuri 2008) to
reduce the time and costs associated with debugging. The
premise of APR has since grown into a substantial research
domain across different languages, classes of bugs, and use
cases (Gazzola, Micucci, and Mariani 2019).

Early approaches for APR were symbolic and attempted
to ﬁx programs automatically by enumeration of repair can-
didates from templates (Debroy and Wong 2010), crafting
heuristics (Qi et al. 2014), and using program synthesis
(Nguyen et al. 2013). Although these systems can provide
strong guarantees on generated code, they are strongly tied
to their domain language. Moreover, symbolic systems are
restrictive in their scope, failing to repair programs that throw
compilers off the rails. On the other hand, building on the re-
cent advances in natural language processing, neural methods
have shown promise in learning program repairs. Researchers
have studied automatically correcting programs in introduc-
tory programming assignments (Pu et al. 2016; Parihar et al.
2017; Ahmed et al. 2018). DeepFix (Gupta et al. 2017) and
SampleFix (Hajipour, Bhattacharyya, and Fritz 2020) use
sequential deep learning models to ﬁx broken code in C.
However, these neural models are not as powerful as LLMCs
like Codex.

SynFix (Ahmed, Ledesma, and Devanbu 2021), Dr. Re-
pair (Yasunaga and Liang 2020), and TFix (Berabi et al. 2021)
leverage compiler diagnostics for Java, C, and Javascript, re-
spectively, but require a substantial amount of training and
data, failing to generalize across languages. Additionally,
neural models can generate plausible but incorrect code, mak-
ing spurious edits resulting in unparsable code (Guo et al.
2021). Although (Bavishi et al. 2022) tries to bridge the gap
between neural and symbolic approaches, it requires domain
specialization (symbolic parser) and large-scale data (neural
localizer and ranker). In contrast, RING leverages a power-
ful LLMC, Codex, capable of generating multilingual code
while guiding repair through readily available prompt-design
strategies.

Large Language Models The advent of Large Language
Models (LLM) trained on code and natural language shows
promise for code understanding and generation results. Au-
toregressive models (Shannon 1948; Radford et al. 2018),
such as Codex (Chen et al. 2021), are trained to predict the
next token, given the past token context over enormous cor-
pora. However, training LLMs is technically challenging and
expensive. They require a large dataset for each ﬁne-tuning
task and parallel training on multiple GPUs (Bommasani et al.
2021) to scale. An exciting aspect of LLMs is the zero-shot
and few-shot learning paradigm for adapting to tasks on-
the-ﬂy (Brown et al. 2020; Chowdhury, Zhuang, and Wang
2022). Prenner and Robbes (2021) evaluate Codex’s ability to
ﬁx 80 bugs in Python and Java. They manually provide buggy
lines for each program and use ﬁxed few shot examples in the

Figure 1: RING, powered by a Large Language Model trained on Code (LLMC), performs multi-lingual program repair. RING
obtains fault localization information from error messages and leverages LLMC’s few shot capabilities for code transformation
through example selection, forming the prompt. Finally, a simple, yet effective, technique is used for ranking repair candidates.

prompt. In contrast, our paper discusses various strategies to
build the prompt, and performs a more extensive study with
larger datasets over more languages.

Scope and Approach
Problem Deﬁnition and Scope Programming mistakes
typically occur on a spectrum: ranging from simple, easy
to ﬁnd and ﬁx mistakes like syntax errors to more complex,
hard to ﬁnd, subtle bugs in program logic or advanced fea-
tures such as multiprocessing. Recently, a wide array of sys-
tems (Bavishi et al. 2022; Yasunaga and Liang 2020, 2021;
Berabi et al. 2021) have been proposed to target the simple
part of this spectrum, resolving the kind of errors that stump
a novice programmer and slow down the workﬂow of an
experienced programmer. All these systems share a vision:
they repair relatively simple diagnostic-surfaced errors in
programs – a class of errors recently termed last mile mis-
takes (Bavishi et al. 2022). Unfortunately, all these systems
are domain-speciﬁc or require substantial effort to adapt to
a new domain. This section discusses our approach to cre-
ating a repair engine that can adapt to new domains (i.e., a
multilingual repair engine) by relying on an LLMC.

Approach We illustrate our approach using a running ex-
ample drawn from the BIFI (Yasunaga and Liang 2021)
dataset in Figure 2. The user has incorrectly used tuple
notation in the function signature (highlighted in pink),
writing (orig image, sigma, spacing). The cor-
rect Python 3 code would simply inline these arguments as
graph, orig image, sigma, spacing. Although
Python 2 supported this type of syntax for signature-level
tuple parameter unpacking, Python 3 removed it and raises a
syntax error1 with little detail on the underlying issue. This
example highlights that errors can also be introduced into
codebases as languages evolve. RING tries to ﬁx this mistake
without additional user intervention.

Figure 1 shows the architecture of RING, our approach
to automated program repair with Codex. We conceptualize

1https://peps.python.org/pep-3113/

1 def boundary_difference_power(graph,

(orig image, sigma, spacing) ):

orig_image = scipy.asarray(orig_image)
def boundary_term_division(i):

i = 1. /(i + 1)
i = scipy.power(i, sigma)
i[i <= 0] = sys.float_info.min
return i

__skeleton_difference(graph,

orig_image,
boundary_term_division)

2
3
4
5
6
7
8
9
10

Figure 2: A real Python 3 syntax error drawn from the BIFI
dataset. The portion highlighted makes use of signature-level
tuple parameter unpacking, which was previously valid in
Python 2, but was removed from Python 3. All listings are
simpliﬁed and adjusted for presentation clarity and brevity.

the task of ﬁxing bugs into three pillars: fault localization,
program transformation, and candidate ranking. This staged
approach to program repair is popular in the APR literature.
Each stage in our approach leverages the intuition underlying
how developers might approach such a stage manually. We
now show how to address each stage using an LLMC.

Fault Localization through Language Tooling

As a ﬁrst step towards debugging, a programmer typically
locates the cause of the bug. For most modern languages,
the task of Fault Localization (FL) of syntactic mistakes and
some semantic errors, such as type errors, is aided by tools
like the language’s compiler, static analyzers, or ad-hoc lin-
ters. Following this intuition, we include a preprocessed error
message produced by the domain’s compiler (or other static
analyzers). We preprocess this message to enforce consis-
tency across domains. Figure 3 shows this prompt variant for
our running example, where the highlighting corresponds to
our prepared syntax error message.

For domains where the error messaging may not be precise,
particularly with regards to the error location reported, we

function Driver(opts) {   Driver.__super__.constructor.apply( ...  this.messages = []; }Buggy Javascript CodeRING (LLMC Repair)Processed Compiler ErrorMessage: Unused variable.Error in line: 1, span starts 47.ExampleSelection#### Fix ES Lint Errors in Javascript  ### Buggy Javascript function(req, res, next) {       ...    }Error: (1) Unused variable. Error in line: 1, span starts 34.### Fixed Javascript function(req, res) {       ...    }### Buggy Javascript<Buggy Javascript Code>Error: Unused variable. Error in line: 1, span starts 47### Fixed JavascriptGenerated Promptfunction Driver(opts) {  Driver.__super__.constructor.apply(... this.messages = []; }Fixed Javascript Completion Pass@1LLMC (OpenAI Codex)Log Prob RankingPrompt GenerationFault LocalizationCode TransformationCandidate RankingThree Pillars of APRDeleted Code1 ### Buggy Python
2 def boundary_difference_power(graph,
3
4 ...
5

Error: (1) invalid syntax. Error in

(orig_image, sigma, spacing)):

6

line: 2 span starts 4 and ends 32.

Figure 3: To aid fault localization, we include a detailed
compiler error message with line/column span information.
We prepare uniform messages across domains by extracting
details from the corresponding domain compiler/analyzer.

found that a simple abstraction that removes the reported
error location but preserves the error text worked well.

Code Transformation through Few-Shot Learning
Once a developer has identiﬁed their desired edit location,
they must now apply an appropriate transformation—a se-
quence of edits—over the original source code at this location.
Most developers over time accumulate experience in the type
of transformation needed to resolve particular errors. But
before they have that experience, or if they encounter a mis-
take they are unfamiliar with, developers often search for
examples of similar buggy/correct code pairs that can inform
their own transformation. In the context of LLMs, few-shot
learning – the ability to learn from a few examples of the
intended task – has been shown to be feasible by adding re-
lated examples of the task to the prompt (Brown et al. 2020;
Poesia et al. 2022). We employ few-shot learning in RING
to address the code transformation stage.

We implement two variants of this smart few-shot selec-
tion, reﬂecting the potential differences in language tooling
across domains. In the ﬁrst variant, error vector selection,
we vectorize an error message by counting the instances of
reported error categories. In the resulting vector, each entry
corresponds to the count of compiler-reported occurrences
of that error category in the buggy program. We can retrieve
related buggy/correct code pairs from a bank of examples by
minimizing the L2 distance between the corresponding error
vectors. The intuition behind this approach is that similar
error messages may have similar ﬁxes.

Figure 4 shows a simpliﬁed prompt using error vector
selection for our running example. The retrieved example ex-
hibits the same error (and required ﬁx) as our buggy program.
With this prompt, RING produces the correct repair as the
top candidate.

While error vector selection is intuitive, unfortunately it
relies on having a detailed error reporting mechanism in the
underlying domain’s tooling. For example, the Excel parser
returns a detailed report including many different diagnostic
counters. In contrast, Python’s parser often returns the same
error type (e.g., SyntaxError) for different mistakes and in-
stead exposes additional information through the associated
natural language error message. To overcome this challenge,
we embed the compiler messages from our example bank
using pretrained CodeBert (Feng et al. 2020). We retrieve the
k most similar shots based on cosine similarity over embed-
dings and refer this retrieval as message embedding selection.

1

2

### Buggy Python

def initial solution(self, start,

(max shares, desired weight) ):

3
4
5 Error: (1) invalid syntax. Error in line

...

: 3, span starts 35 and ends: 36.

### Fixed Python

def initial solution(self, start,

max shares, desired weight ):

...

6

7

8
9

Figure 4: Our smart selection of few-shots consists of rele-
vant buggy-ﬁx examples retrieved from a bank of examples
based on a similarity metric over error messages. The ﬁrst
shot (pink background) displays the same invalid signature-
level tuple parameter unpacking (dark red background, bold)
as our target program. The ﬁxed portion of the shot (green
background, bold) removes the parentheses.

Candidate Ranking
Like many other related search problems (Polozov and Gul-
wani 2015), the ﬁnal step in automated program repair entails
ranking the candidates enumerated based on the prior two
stages (localization and transformation). We employ a rel-
atively simple (but effective) ranking strategy to order the
candidate programs. In particular, we average the token-level
log probabilities generated during the decoding process and
sort the candidates in descending order of their averages.

Domain-Speciﬁc Datasets
We evaluate RING’s effectiveness in six different domains,
ranging from low-code formula languages to popular script-
ing languages. We describe the dataset, domain-speciﬁc base-
line, and evaluation metric used in each domain.

Excel We use a recently released dataset of 200 Excel re-
pair tasks collected from Excel help forums (Bavishi et al.
2022). Each task consists of an Excel formula with syntax
errors (and some semantic errors, such as wrong function call
arity) and a ground truth repair. We also collect a set of 73
tasks where the Excel formula contains at least one type error
and annotated each such formula with a ground truth repair.
The ﬁnal collection consists of 273 Excel repair tasks.

A successful repair exactly matches the ground truth af-
ter normalizing tokens like whitespace (check Appendix for
more details). We compare RING to the neurosymbolic re-
pair engine LaMirage (Bavishi et al. 2022).

PowerFx Like Excel, we use the recently released 200
PowerFx repair tasks accompanying LaMirage. These Pow-
erFx formulas have syntactic and basic semantic errors col-
lected from help forums and anonymized product telemetry.
We use the same evaluation criteria as in Excel and com-

pare to the neurosymbolic repair engine LaMirage.

Python We evaluate RING on a random sample of 200
syntactically invalid Python code snippets drawn from the

dataset used by the SOTA syntax repair tool for Python:
BIFI (Yasunaga and Liang 2021). These code snippets were
collected from real GitHub repositories.

These snippets do not have a groundtruth repair, hence,
we employ the same evaluation metric described in the BIFI
paper. A repair is successful if the program produced satisﬁes
the Python 3 parser and the repaired program is less than 5
token edits away from the original buggy program. The edit
distance is computed using standard Levenshtein edit dis-
tance (Levenshtein et al. 1966) over program tokens. We also
include a comparison without token-edit distance threshold
in the Appendix.

We compare our performance to BIFI, a transformer-based
repair system that is iteratively trained by employing a code
breaker that learns to generate realistic errors and a code ﬁxer
that repairs such errors.

JavaScript We evaluate RING on a random sample of
200 JS code snippets drawn from the dataset released
with TFix (Berabi et al. 2021). Each snippet has at least
one error or warning reported by the popular linter ES-
Lint (T´omasd´ottir, Aniche, and Van Deursen 2018). In addi-
tion to syntax errors, ESLint also reports stylistic issues.

The dataset released by TFix contains a ground truth repair
code snippet for each buggy snippet. Both buggy and ground
truth code snippets were mined by the TFix authors from
GitHub commits. The originally released dataset contains
only the part of each code snippet relevant to the error and
repair. However, these parts are an arbitrary window around
the original fault location. We found that providing these
arbitrary windows to Codex resulted in spurious edits as the
windows had syntax errors that were just an artifact of the
windowing. To mitigate this, we extracted the whole function
(or whole ﬁle, if not in a function) that encompassed the
originally buggy and the repaired code snippets. We refer to
these as extended code snippets.

We compare our performance to TFix, a ﬁne-tuned T5 (Raf-
fel et al. 2020) model for JS repair. A repair is successful
if it matches the ground truth corresponding to the buggy
program. We run TFix on the code snippets as originally
presented and on our extended code snippets.

C We evaluate RING on a random sample of 200 C
code snippets drawn from the dataset released with DeepFix
(Gupta et al. 2017). These programs correspond to real user
programs written by students in an introductory programming
class and raise at least one compilation error.

We compare to Dr. Repair, a neural repair system that uses
graph attention to combine information from the buggy code
snippet and the associated compiler message (Yasunaga and
Liang 2020). We use the same success criteria introduced
in their paper: a repair must not raise any error messages
when compiled using gcc -w -std=c99 -pedantic.
Following BIFI, a successful repair must be less than 5 to-
ken edits away from the original buggy program. We also
include a comparison without edit-distance threshold in the
Appendix.

Powershell We are the ﬁrst paper to introduce the task of
repairing syntax errors in PowerShell commands. To create

a benchmark for this domain, we searched StackOverﬂow
(StackOverﬂow) for the word “error” in threads tagged with
powershell. This resulted in 14,954 threads. We extracted
code blocks with at least one space from the question and
the accepted answer. We then took the Cartesian product of
the question and answer code blocks in each thread, keeping
pairs where the question and answer code was syntactically
invalid and valid, respectively. We judged validity using the
PowerShell command Get-Command -syntax.

Finally, we manually annotated these candidate tasks by
navigating back to the associated StackOverﬂow post, con-
ﬁrming each pair was reﬂective of the original issue and did
not have extra changes. We kept a ﬁnal set of 208 task pairs.
There is no existing domain-speciﬁc engine with which
to compare as we introduce this task. We say a repair is
successful if it exactly matches the associated answer code
block from our manual curation.

Results and Analysis
We ﬁrst (RQ1) ask: how viable is RING’s Codex-powered
approach for repair across multiple languages? Next, we
investigate the extent to which Codex can address each of
our conceptual stages. For localization, (RQ2) to what extent
can RING perform error localization across languages? For
code transformation, (RQ3) to what extent does our smart
selection of few-shots improve performance? And ﬁnally, for
candidate ranking, (RQ4) to what extent can RING rely on
Codex’s log probabilities to rank candidates?

RQ1. Viability of multilingual repair
Table 1 shows RING’s performance, as well as domain-
speciﬁc repair engines, across each of our domains. We
present the best performing conﬁguration for each language
using pass@k performance metrics (Inala et al. 2022; Poe-
sia et al. 2022; Bavishi et al. 2022). For these experiments,
smart selection of few-shots is carried out in a leave-one-out
approach: for each benchmark task we consider remaining
benchmark tasks to be our example bank for drawing shots.
For C (Gupta et al. 2017) and Python (Yasunaga and Liang
2021) that do not have ground truth pair, we sample addi-
tional 400 programs from their corresponding datasets. We
run the best RING conﬁguration without smart example se-
lection on these 400 programs and pick those which do not
raise any diagnostics error. These pairs of buggy program
and corrected program constitute the example bank for smart
selection in C and Python.

For Excel, Python, and C, RING outperforms state-of-
the-art domain-speciﬁc repair engines at pass@1. For Pow-
erFx, we ﬁnd that RING’s pass@3 rate is comparable to the
pass@1 rate for LaMirage. Furthermore, there is a substantial
improvement in RING’s pass@3 compared to pass@1.

In Javascript, we ﬁnd that TFix applied to the original
code snippets obtains a pass@1 rate of 0.59 (approximately
7 points higher than that of RING). However, applying TFix
to the extended code snippets, deﬁned as the inner-most func-
tion (or full ﬁle, if none) containing the original fault, results
in a much lower pass@1 rate of 0.09. This performance degra-
dation can be attributed to the substantially longer sequences

Table 1: Comparison of RING with domain speciﬁc approaches. Bold denotes best performance for each language. All RING
experiments are at 0.7 temperature. RING can outperform domain-speciﬁc repair engines in Excel, Python, and C. In Javascript,
RING is capable of generating the right repair but ranking needs to improve. In the Powershell, with no existing baseline, RING
performs substantially worse – likely reﬂective of the lack of Powershell code in Codex’s training data.

Approach
RING

Localization
Abstracted Message

Transformation
Smart Selection

LaMirage (Bavishi et al. 2022)

Pass@1
0.82
0.71

Pass@3
0.89
0.76

Pass@50
0.92
-

Metric

Avg. Tokens

Exact Match

26 ±14

Language

Excel

PowerFx

Javascript

Python

C

Powershell

RING

Compiler Message

Smart Examples

LaMirage (Bavishi et al. 2022)

RING

Compiler Message

Smart Selection
TFix - extended code snippets (Berabi et al. 2021)
TFix - original dataset (Berabi et al. 2021)

RING

Compiler Message

Smart Selection

BIFI (Yasunaga and Liang 2021)

RING

Compiler Message

Smart Selection

Dr Repair (Yasunaga and Liang 2020)

RING

Compiler Message

Smart Selection

No Baseline Available

(an average of 208 T5 tokens) of the extended code snippets
compared to the original code snippets (an average of 74 T5
tokens).

In PowerShell (PS), we do not provide a domain-speciﬁc
comparison as we are the ﬁrst to introduce syntax repair in
this language. We observe that RING’s performance for PS is
substantially lower than those of other domains. We hypothe-
size that this may be a reﬂection of the (presumed) relative
scarcity of PS code in Codex’s training data. Manual inspec-
tion of PS failures also revealed that RING performs fewer
edits than required, as shown in Figure 5, which compares
the edit distance between the buggy and predicted repaired
program and the buggy and groundtruth correct program.

Given this evidence, we conclude that RING’s Codex-
powered approach can perform multilingual repair, and con-
trast this to the substantial effort required to build a domain-
speciﬁc repair engine. For example, TFix, BIFI, and Dr. Re-
pair were trained on 108k, 3M and 1.5M Javascript, Python,
and C code snippets, respectively, and LaMirage (the state-of-
the-art repair engine in Excel and PowerFx) trained pointer-
network-based error localizers and rankers, as well as imple-
mented multiple domain-speciﬁc rules.

Programs that were not ﬁxed by either RING or the
domain-speciﬁc engines shared some properties. In partic-
ular, some of these could be addressed with a combination
of iterative Codex querying and explicit light-weight check-
s/constraints that enforce domain-speciﬁc knowledge. For
example, we found a Python program that has two issues:
an invalid use of the reserved keyword async and a missing
parenthesis. For the keyword issue, we could query Codex
with the buggy program up to the invalid keyword usage, and
validate that the following token predicted is not a reserved
keyword, and then query Codex again, with the modiﬁed
buggy code fragment. This is akin to constrained decoding
used in Synchromesh (Poesia et al. 2022). Another possi-

0.71
0.85

0.46
0.09
0.59

0.94
0.92

0.63
0.55

0.10
-

0.85
0.88

0.59
-
-

0.97
0.95

0.69
-

0.19
-

e
c
n
a
t
s
i
D

t
i
d
E
h
t
u
r
T
d
n
u
o
r
G
o
t

y
g
g
u
B

8

6

4

2

0

0

0.87
-

0.64
-
-

0.97
0.96

0.70
-

0.27
-

Exact Match

29 ±19

Exact Match

163 ±106

Passes Parser
Edit Distance < 5

Passes Parser
Edit Distance < 5

104 ±150

223 ±72

Exact Match

24 ±30

2

4
Buggy to Repair Edit Distance

6

8

Figure 5: For incorrect Pass@1 repair attempt for Power-
shell, we observe that Codex makes fewer edits than required,
based on the edit-distance differences between predicted and
ground-truth programs when compared to the original buggy
program.

bility for further improving the performance of RING, is
to combine the candidate repairs it suggests with those pro-
duced by domain-speciﬁc repair engines – a method akin
to ensembling (Sagi and Rokach 2018). We found that at
pass@1, the domain-speciﬁc repair engines for PowerFx and
Javascript can repair 34 programs (17%) and 52 programs
(26%), respectively, that RING does not repair. The possi-
bilities for complementary repairs are fewer in Excel (15
formulas), Python (6 programs), and C (4 programs).

0.3

0.2

0.1

.
c
a
r
f

n
o
i
t
a
z
i
l
a
c
o
L

t
c
e
r
r
o
C

Excel
Javascript

PowerFx
Powershell

0

±1
±3
±2
Location Range (±k tokens)

±4

Figure 6: Fraction of incorrect pass@1 programs where
RING makes an edit within k tokens of the groundtruth
edit location.

n
o
i
t
c
a
r
F
e
v
i
t
a
l
u
m
u
C

1

0.8

0.6

0.4

0.2

0

n
o
i
t
c
a
r
F
e
v
i
t
a
l
u
m
u
C

1

0.8

0.6

0.4

0.2

0

Exact Matches@1

True
False

Passes Critic@1

True
False

0

200

400

Number of tokens

(a) Javascript

0

1,000

2,000

Number of tokens

(b) Python

Figure 7: Cumulative fraction of programs by number of
tokens in the original buggy program, grouped by whether
RING can repair at pass@1. Successful repairs tend to be
associated with shorter buggy programs.

RQ2. Error Localization
Even if RING cannot ﬁx a program at pass@1, localizing
the error can help users. For the four domains where we have
the groundtruth repaired program, We consider programs that
are not repaired at pass@1. For each such program, we take
the top candidate produced by RING and compare the edit
locations to the groundtruth edit locations. If the candidate
edit locations are all within a range of ±k tokens of the
groundtruth locations, we mark this as a correct localization.
Figure 6 summarizes our results. Localization success in
these programs varies by language – but can reach as high as
over a quarter of unrepaired programs (for PowerFx, given
a tolerance of one token). For such programs, where Codex
can localize the error but does not perform the correct edit,
drawing shots from a larger example bank may help.

Next, we explored a key contributing factor to overall
repair success (and localization in particular): program length.
We found that for most domains, the buggy programs that
RING can repair tend to be shorter than the buggy programs
it fails to repair. Figure 7 shows the cumulative fraction of
buggy programs by program length (in terms of tokens in the
buggy program), grouped based on their outcome (pass@1).
In both Javascript and Python, the programs successfully
repaired by RING tend to be shorter than those where it
fails. Interestingly for Excel, this relationship does not seem
to hold as strongly. We attribute this behaviour to shorter

y
t
i
s
n
e
D

l
e
n
r
e
K

y
t
i
s
n
e
D

l
e
n
r
e
K

20
15
10
5
0

20
15
10
5
0

True
False

True
False

y
t
i
s
n
e
D

l
e
n
r
e
K

60

40

20

0

−0.4 −0.2
avg. log probs

0

-0.06 -0.03

0
avg. log probs

(a) Excel

True
False

−0.2 −0.1

0
avg. log probs

y
t
i
s
n
e
D

l
e
n
r
e
K

(b) C

True
False

15

10

5

0
−0.3 −0.2 −0.1

0
avg. log probs

(c) PowerFx

(d) Powershell

Figure 8: (Gaussian) Kernel density plots for average token
log probabilities across domains, based on their pass@1 suc-
cess status. Clearer separation of distributions tends to be
associated with better performance (e.g., Excel, C). In Pow-
ershell, where RING struggles, this relationship is inverted.

programs lengths and the restrictive Excel grammar.

RQ3. Code Transformation
Table 2 shows the pass@1 rate with our smart selection of
few-shots for the prompt, compared to a strategy that uses
pre-deﬁned ﬁxed examples. The pre-deﬁned strategy allows
us to curate high-quality repair examples for common er-
rors, but these may not be relevant for all programs. Prior
work (Prenner and Robbes 2021) explored the use of ﬁxed
few-shot examples for APR with Codex.

Our results show that smart selection improves perfor-
mance in all domains. This performance improvement comes
from examples in the prompt that reﬂect similar errors (and
expected edits) to the target program. We use error vector
selection for Excel and Javascript, which have better and
more granular error categorization, and message embedding
selection for other domains. We observe that PowerFx shows
the smallest performance improvement. Manual inspection
revealed that PowerFx compiler messages tend to be impre-
cise, and using them to select examples can introduce some
noise into the prompt.

RQ4. Candidate Ranking
RING ranks candidate repairs based on the average of per-
token log probabilities. The effectiveness of this strategy de-
pends on the extent to which Codex is well-calibrated (Bella
et al. 2010; Nixon et al. 2019; Dormann 2020) for program
repair. Figure 8 compares log probabilities in Excel, PowerFx,
Powershell, and C. We show (Gaussian) kernel density plots
across domains based on the pass@1 outcome. For domains

Domain

Fixed Shots

Smart Shots

Fractional Change

Excel
PowerFx
Javascript
Python
C
Powershell

0.76
0.70
0.43
0.91
0.5
0.09

0.82
0.71
0.46
0.94
0.58
0.1

0.08
0.01
0.07
0.03
0.16
0.01

Table 2: Pass@1 for few-shots selected using our smart selec-
tion strategy, compared to pre-deﬁned ﬁxed examples. Smart
selection improves performance for all domains. For Pow-
erFx, we see the smallest improvement, which we attribute
to imprecise compiler diagnostics.

like Excel, where RING outperforms a domain-speciﬁc re-
pair engines, there is a clearer difference in distributions. In
PowerFx, where RING can repair programs but does not
outperform the domain-speciﬁc engine, this distribution dif-
ference is less clear. In Powershell, where RING fails to
repair a substantial fraction of programs, this relationship is
inverted.

Powershell
Javascript
C

Excel
Python
PowerFx

y
t
i
s
n
e
D

l
e
n
r
e
K

60

40

20

0

-0.15

-0.1

-0.05

0

Token average log probability

Figure 9: (Gaussian) kernel density plots of per-domain suc-
cessful pass@1 program scores (i.e., average token log prob-
abilities). We ﬁnd that less popular domains, like Powershell,
Excel, and PowerFx, have lower average program scores –
likely reﬂective of their relatively small fraction of Codex’s
training data.

Additionally, we ﬁnd that even for programs with a pass@1
success outcome, there are cross-domain differences in av-
erage token log probability, as shown in Figure 9. For less
popular domains (e.g., Powershell or Excel), the distribu-
tion means are further left than more popular domains (e.g.,
Javascript). This likely reﬂects the underlying language dis-
tribution in Codex’s training data.

Based on our observation of the gap between pass@1 and
pass@5, paired with these calibration insights, we believe
that a dedicated ranking model (Inala et al. 2022) may provide
a substantial payoff in multilingual repair.

Conclusion
We present RING, a multilingual repair engine powered by
Codex. We show various prompt-based strategies designed
to convey developer-like information to address the three
pillars of automate program repair: error localization, code

transformation, and candidate ranking. We evaluate RING
on 6 domains, including a benchmark for the novel task
of repairing Powershell programs, and compare to domain-
speciﬁc repair engines. We show RING can perform well
in multiple domains, even outperforming domain-speciﬁc
engines in some, with little engineering effort.

Acknowledgements
We thank Peter Lee for the CoPilot-ﬂipped-model analogy.
We thank Julia Liuson for inspiring and facilitating use of
Codex-as-a-component in our neuro-symbolic workﬂows.
We also thank the authors of baseline systems used in our
research – their sharing of models and data made this work
possible. We would also like to thank Abishai Ebenezer for
helping us curate the PowerShell evaluation benchmarks.

References
Ahmed, T.; Ledesma, N. R.; and Devanbu, P. 2021. SYN-
FIX: Automatically Fixing Syntax Errors using Compiler
Diagnostics. arXiv preprint arXiv:2104.14671.
Ahmed, U. Z.; Kumar, P.; Karkare, A.; Kar, P.; and Gulwani,
S. 2018. Compilation error repair: for the student programs,
from the student programs. In Proceedings of the 40th In-
ternational Conference on Software Engineering: Software
Engineering Education and Training, 78–87.
Altadmri, A.; and Brown, N. C. 2015. 37 million compila-
tions: Investigating novice programming mistakes in large-
scale student data. In Proceedings of the 46th ACM technical
symposium on computer science education, 522–527.
Arcuri, A. 2008. On the automation of ﬁxing software bugs.
In Companion of the 30th international conference on Soft-
ware engineering, 1003–1006.
Bareiß, P.; Souza, B.; d’Amorim, M.; and Pradel, M. 2022.
Code Generation Tools (Almost) for Free? A Study of Few-
Shot, Pre-Trained Language Models on Code. arXiv preprint
arXiv:2206.01335.
Bavishi, R.; Joshi, H.; S´anchez, J. P. C.; Fariha, A.; Gulwani,
S.; Le, V.; Radicek, I.; and Tiwari, A. 2022. Neurosymbolic
Repair for Low-Code Formula Languages. arXiv preprint
arXiv:2207.11765.
Bella, A.; Ferri, C.; Hern´andez-Orallo, J.; and Ram´ırez-
Quintana, M. J. 2010. Calibration of machine learning mod-
els. In Handbook of Research on Machine Learning Appli-
cations and Trends: Algorithms, Methods, and Techniques,
128–146. IGI Global.
Berabi, B.; He, J.; Raychev, V.; and Vechev, M. 2021. Tﬁx:
Learning to ﬁx coding errors with a text-to-text transformer.
In International Conference on Machine Learning, 780–791.
PMLR.
Bommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora,
S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.;
Brunskill, E.; et al. 2021. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners.

Advances in neural information processing systems, 33: 1877–
1901.
Bureau of Labor Statistics, U. 2021. Software developers,
Quality Assurance Analysts, and testers : Occupational out-
look handbook.
Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Ponde, H.; Kaplan,
J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; Ray,
A.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf, H.; Sastry, G.;
Mishkin, P.; Chan, B.; Gray, S.; Ryder, N.; Pavlov, M.; Power,
A.; Kaiser, L.; Bavarian, M.; Winter, C.; Tillet, P.; Such, F. P.;
Cummings, D. W.; Plappert, M.; Chantzis, F.; Barnes, E.;
Herbert-Voss, A.; Guss, W. H.; Nichol, A.; Babuschkin, I.;
Balaji, S. A.; Jain, S.; Carr, A.; Leike, J.; Achiam, J.; Misra,
V.; Morikawa, E.; Radford, A.; Knight, M. M.; Brundage, M.;
Murati, M.; Mayer, K.; Welinder, P.; McGrew, B.; Amodei,
D.; McCandlish, S.; Sutskever, I.; and Zaremba, W. 2021.
Evaluating Large Language Models Trained on Code. ArXiv,
abs/2107.03374.
Chowdhury, J. R.; Zhuang, Y.; and Wang, S. 2022. Novelty
Controlled Paraphrase Generation with Retrieval Augmented
Conditional Prompt Tuning. Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, 36(10): 10535–10544.
Debroy, V.; and Wong, W. E. 2010. Using Mutation to Au-
tomatically Suggest Fixes for Faulty Programs. 2010 Third
International Conference on Software Testing, Veriﬁcation
and Validation, 65–74.
Diekmann, L.; and Tratt, L. 2020. Don’t Panic! Better, Fewer,
Syntax Errors for LR Parsers. In 34th European Conference
on Object-Oriented Programming, ECOOP 2020, volume
166 of LIPIcs, 6:1–6:32.
Dormann, C. F. 2020. Calibration of probability predictions
from machine-learning and statistical models. Global ecology
and biogeography, 29(4): 760–765.
Drori, I.; Zhang, S.; Shuttleworth, R.; Tang, L.; Lu, A.; Ke,
E.; Liu, K.; Chen, L.; Tran, S.; Cheng, N.; et al. 2022. A
neural network solves, explains, and generates university
math problems by program synthesis and few-shot learning
at human level. Proceedings of the National Academy of
Sciences, 119(32): e2123433119.
Drosos, I.; Guo, P. J.; and Parnin, C. 2017. HappyFace:
Identifying and predicting frustrating obstacles for learning
programming at scale. In 2017 IEEE Symposium on Visual
Languages and Human-Centric Computing (VL/HCC), 171–
179. IEEE.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.;
Shou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. Codebert:
A pre-trained model for programming and natural languages.
arXiv preprint arXiv:2002.08155.
Gazzola, L.; Micucci, D.; and Mariani, L. 2019. Automatic
Software Repair: A Survey. IEEE Transactions on Software
Engineering, 45: 34–67.
Goues, C. L.; Pradel, M.; and Roychoudhury, A. 2019. Auto-
mated program repair. Communications of the ACM, 62(12):
56–65.
Guo, D.; Svyatkovskiy, A.; Yin, J.; Duan, N.; Brockschmidt,
M.; and Allamanis, M. 2021. Learning to complete code

In International Conference on Learning

with sketches.
Representations.
Gupta, R.; Pal, S.; Kanade, A.; and Shevade, S. K. 2017.
DeepFix: Fixing Common C Language Errors by Deep Learn-
ing. In AAAI.
Hajipour, H.; Bhattacharyya, A.; and Fritz, M. 2020. Sam-
pleFix: Learning to Correct Programs by Efﬁcient Sampling
of Diverse Fixes. In NeurIPS 2020 Workshop on Computer-
Assisted Programming.
Inala, J. P.; Wang, C.; Yang, M.; Codas, A.; Encarnaci´on, M.;
Lahiri, S. K.; Musuvathi, M.; and Gao, J. 2022. Fault-Aware
Neural Code Rankers. arXiv preprint arXiv:2206.03865.
Levenshtein, V. I.; et al. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet physics
doklady, volume 10, 707–710. Soviet Union.
Liu, K.; Li, L.; Koyuncu, A.; Kim, D.; Liu, Z.; Klein, J.; and
Bissyand´e, T. F. 2021. A critical review on the evaluation of
automated program repair systems. Journal of Systems and
Software, 171: 110817.
Murphy, L.; Lewandowski, G.; McCauley, R.; Simon, B.;
Thomas, L.; and Zander, C. 2008. Debugging: the good,
the bad, and the quirky–a qualitative analysis of novices’
strategies. ACM SIGCSE Bulletin, 40(1): 163–167.
Nguyen, H. D. T.; Qi, D.; Roychoudhury, A.; and Chandra,
S. 2013. SemFix: Program repair via semantic analysis.
International Conference on Software Engineering, 772–781.
Nixon, J.; Dusenberry, M. W.; Zhang, L.; Jerfel, G.; and Tran,
D. 2019. Measuring Calibration in Deep Learning. In CVPR
Workshops, volume 2.
Parihar, S.; Dadachanji, Z.; Singh, P. K.; Das, R.; Karkare, A.;
and Bhattacharya, A. 2017. Automatic grading and feedback
using program repair for introductory programming courses.
In Proceedings of the 2017 ACM Conference on Innovation
and Technology in Computer Science Education, 92–97.
Poesia, G.; Polozov, A.; Le, V.; Tiwari, A.; Soares, G.; Meek,
C.; and Gulwani, S. 2022. Synchromesh: Reliable Code Gen-
eration from Pre-trained Language Models. In International
Conference on Learning Representations.
Polozov, O.; and Gulwani, S. 2015. Flashmeta: A framework
for inductive program synthesis. In Proceedings of the 2015
ACM SIGPLAN International Conference on Object-Oriented
Programming, Systems, Languages, and Applications, 107–
126.
Prenner, J. A.; and Robbes, R. 2021. Automatic Program
Repair with OpenAI’s Codex: Evaluating QuixBugs. arXiv
preprint arXiv:2111.03922.
Pu, Y.; Narasimhan, K.; Solar-Lezama, A.; and Barzilay, R.
2016. sk p: a neural program corrector for MOOCs.
In
Companion Proceedings of the 2016 ACM SIGPLAN Inter-
national Conference on Systems, Programming, Languages
and Applications: Software for Humanity, 39–40.
Qi, Y.; Mao, X.; Lei, Y.; Dai, Z.; and Wang, C. 2014. The
strength of random search on automated program repair. In
Proceedings of the 36th International Conference on Software
Engineering, 254–265.

Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;
et al. 2018. Improving language understanding by generative
pre-training.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Explor-
ing the Limits of Transfer Learning with a Uniﬁed Text-to-
Text Transformer. Journal of Machine Learning Research,
21(140): 1–67.
Sagi, O.; and Rokach, L. 2018. Ensemble learning: A survey.
Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery, 8(4): e1249.
Shannon, C. E. 1948. A mathematical theory of communica-
tion. The Bell system technical journal, 27(3): 379–423.
StackOverﬂow. 2022. StackOverﬂow. https://stackoverﬂow.
com/.
T´omasd´ottir, K. F.; Aniche, M.; and Van Deursen, A. 2018.
The adoption of javascript linters in practice: A case study on
eslint. IEEE Transactions on Software Engineering, 46(8):
863–891.
Wexelblat, R. L. 1976. Maxims for malfeasant designers, or
how to design languages to make programming as difﬁcult as
possible. In Proceedings of the 2nd international conference
on Software engineering, 331–336.
Yasunaga, M.; and Liang, P. 2020. Graph-based, self-
supervised program repair from diagnostic feedback. In In-
ternational Conference on Machine Learning, 10799–10808.
PMLR.
Yasunaga, M.; and Liang, P. 2021. Break-it-ﬁx-it: Unsuper-
vised learning for program repair. In International Confer-
ence on Machine Learning, 11941–11952. PMLR.
Zhong, H.; and Su, Z. 2015. An Empirical Study on Real Bug
Fixes. 2015 IEEE/ACM 37th IEEE International Conference
on Software Engineering, 1: 913–923.

Appendix

Program Normalization
In this section we detail normalization steps taken for can-
didate dedeuplication and matching to ground truth. We do
not perform normalization for Powershell and PowerFx. For
generated candidates, we ﬁlter out those which exactly match
the buggy code. We carry out these normalization steps for
all systems, both RING and baselines.

Excel
• Excel tokens are case sensitive, therefore, we capitalize

all the cell references and identiﬁer.

• Remove whitespace tokens

Python
• Remove comments
• Remove redundant new lines

Javascript
• Remove whitespace
• Remove new line
• Remove comments

C
• Remove comments
• Remove whitespace
• Remove new lines

Number of Tokens vs Number of repairs
In Figure 10, we observe that for most domains, programs
that RING successfully repairs tend to be shorter than those
it fails to repair. Interestingly, for Javascript the gap between
proportion of programs decreases as number of tokens in-
crease. In Excel, there is no clear distinction, likely due to
smaller program length in the dataset compared to other do-
mains.

In Figure 11, we observe similar trend for Python, but in

C we ﬁnd some overlap.

Average log probability
We observe that for all domains except Powershell, RING is
more conﬁdent about correct repairs, as shown in main paper
Figure 7 and, Figure, 12a, 12b, 12c.

No token edit limit
In the results section of the main paper, we report RING
results with a token edit distance threshold of 4. In Table 9,
we present results without the token edit distance. We observe
that RING still outperforms BIFI (Yasunaga and Liang 2021)
and Dr Repair (Yasunaga and Liang 2020) in Python and C,
respectively.

Different Conﬁgurations of RING
From Table 3 to 8, we present results of RING with dif-
ferent conﬁgurations. For Excel and PowerFx, we see that
Abstracted compiler message gives better performance, likely
due to shorter (usually, 1 line) programs.

n
o
i
t
r
o
p
o
r
P

1

0.8

0.6

0.4

0.2

0

n
o
i
t
c
a
r
F
e
v
i
t
a
l
u
m
u
C

1

0.8

0.6

0.4

0.2

0

n
o
i
t
r
o
p
o
r
P

n
o
i
t
r
o
p
o
r
P

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

False
True

20

40
60
Number of tokens

80

(a) Excel

Exact Matches@1

True
False

True
False

0

50

100

Number of tokens

(b) PowerFx

True
False

0

200

400

Number of tokens

(c) Javascript

0

100

200

Number of tokens

(d) Powershell

Figure 10: Cumulative fraction of programs, grouped by
pass@1 outcome, by number of program tokens in the origi-
nal buggy program. Languages with groundtruth repair avail-
able.

n
o
i
t
c
a
r
F
e
v
i
t
a
l
u
m
u
C

1

0.8

0.6

0.4

0.2

0

n
o
i
t
r
o
p
o
r
P

1

0.8

0.6

0.4

0.2

0

Passes Critic@1

True
False

True
False

0

1,000

2,000

100

200

300

400

Number of tokens

(a) Python

Number of tokens

(b) C

Figure 11: Cumulative fraction of programs, grouped by
pass@1 outcome, by number of program tokens in the origi-
nal buggy program. Languages without groundtruth repair.

Table 3: Excel: Results with different conﬁgurations.

Localization

Transformation

None
None
Fixed Example
Smart Selection

None
Compiler
Compiler
Compiler
Abstracted Compiler None
Abstracted Compiler
Abstracted Compiler

Fixed Example
Smart Selection

Metrics: Exact Match
P@50
P@3
P@1
0.88
0.77
0.60
0.90
0.83
0.70
0.93
0.89
0.76
0.92
0.88
0.81
0.91
0.84
0.66
0.93
0.89
0.76
0.92
0.89
0.82

True
False

y
t
i
s
n
e
D

40

20

0

y
t
i
s
n
e
D

20

10

0

True
False

-0.08 -0.04

0

log prob

−1

−0.5
log prob

0

(a) Python: Average token log
probabilities density

(b) JS: Average token log proba-
bilities density

True
False

y
t
i
s
n
e
D

l
e
n
r
e
K

60

40

20

0

-0.06

-0.03

0

avg. log probs

(c) C: Average token log probabilities density

Table 4: PowerFx: Results with different conﬁgurations.

Localization

Transformation

None
None
Fixed Example
Smart Selection

None
Compiler
Compiler
Compiler
Abstracted Compiler None
Abstracted Compiler
Abstracted Compiler

Fixed Example
Smart Selection

Metrics: Exact Match
P@50
P@3
P@1
0.84
0.68
0.47
0.87
0.75
0.53
0.89
0.82
0.70
0.87
0.85
0.71
0.86
0.77
0.53
0.89
0.85
0.71
0.87
0.84
0.68

Table 6: Python: Results with different conﬁgurations.

Localization

Transformation

None
None
Fixed Example
Smart Selection

None
Compiler
Compiler
Compiler
Abstracted Compiler None
Abstracted Compiler
Abstracted Compiler

Fixed Example
Smart Selection

Metric: Passes Parser
P@50
P@3
P@1
0.98
0.94
0.87
0.97
0.94
0.88
0.96
0.95
0.91
0.97
0.97
0.94
0.97
0.95
0.90
0.96
0.95
0.90
0.95
0.94
0.89

Table 7: Powershell: Results with different conﬁgurations.

Localization

Transformation

None
None
Fixed Example
Smart Selection

None
Compiler
Compiler
Compiler
Abstracted Compiler None
Abstracted Compiler
Abstracted Compiler

Fixed Example
Smart Selection

Metric: Exact Matches
P@50
P@3
P@1
0.18
0.11
0.06
0.21
0.12
0.04
0.22
0.18
0.09
0.27
0.19
0.10
0.18
0.11
0.06
0.23
0.17
0.11
0.22
0.16
0.09

Table 8: C: Results with different conﬁgurations.

Table 5: Javascript: Results with different conﬁgurations.

Localization

Transformation

Localization

Transformation

None
None
Fixed Example
Smart Selection

None
Compiler
Compiler
Compiler
Abstracted Compiler None
Abstracted Compiler
Abstracted Compiler

Fixed Example
Smart Selection

Metric: Exact Match
P@50
P@3
P@1
0.39
0.28
0.19
0.52
0.45
0.35
0.60
0.55
0.44
0.64
0.59
0.46
0.52
0.45
0.37
0.60
0.53
0.43
0.65
0.58
0.47

None
None
Fixed Example
Smart Selection

None
Compiler
Compiler
Compiler
Abstracted Compiler None
Abstracted Compiler
Abstracted Compiler

Fixed Example
Smart Selection

Metric: Passes Parser
P@50
P@3
P@1
0.61
0.56
0.40
0.71
0.63
0.52
0.66
0.65
0.53
0.70
0.69
0.64
0.68
0.64
0.53
0.68
0.67
0.56
0.69
0.68
0.63

Table 9: Results for two languages which do not have a ground truth – C and Python, without token edit distance threshold.

Language Approach Localization

RING

Compiler Message

Dr Repair

Transformation
Smart Selection

RING

Compiler Message

Smart Selection

BIFI

C

Python

Pass@1

Pass@3

164
125
189
185

0.82
0.63
0.95
0.93

175
-
193
190

0.875
-
0.97
0.95

Pass@50
0.94
188
-
-
0.97
194
0.96
192

Evaluation

Passes Parser

Passes Parser

